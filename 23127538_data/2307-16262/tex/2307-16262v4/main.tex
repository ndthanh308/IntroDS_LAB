\documentclass[times,twocolumn,preprint, longtitle]{elsarticle}
\usepackage{medima}
\usepackage{framed,multirow}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{url}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{booktabs}
\usepackage{xspace}
\usepackage{svg}
\usepackage{soul}
\usepackage{amsmath}
\usepackage{footnote}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{array}
\usepackage[nolist,nohyperlinks]{acronym}
\usepackage{subcaption}
\makesavenoteenv{tabular}
\usepackage{caption, subcaption}
\usepackage[figuresleft]{rotating}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage[acronym]{glossaries}
\makeglossaries
\acrodef{GI}{Gastrointestinal}
\acrodef{CRC}{Colorectal Cancer}
\acrodef{CADx}{Computer-Aided Diagnosis}
\acrodef{CNN}{Convolutional Neural Network}
\acrodef{DNN}{Deep Neural Network}
\acrodef{DL}{Deep Learning}
\acrodef{ML}{Machine Learning}
\acrodef{RNN}{Residual Neural Network}
\acrodef{GAN}{Genearative Adversarial Network}
\acrodef{Medico}{Multimedia  for  Medicine  Task}
\acrodef{AI}{Artificial Intelligence}
\acrodef{FPS}{Frame Per Second}
\acrodef{CGAN}{Conditional Generative Adversarial Network}
\acrodef{CADe}{Computer Aided Detection}
\acrodef{BLI}{Blue laser imaging}
\acrodef{COD}{Camouflaged Object Detection}
\acrodef{GRU}{Gated Recurrent Neural Network}
\acrodef{FICE}{Flexible spectral Imaging Color Enhancement}
\acrodef{DSC}{Dice coefficient}
\acrodef{FPS}{Frames per second}
\acrodef{mIoU}{mean Intersection over union}
\acrodef{SOTA}{state-of-the-art}
\definecolor{newcolor}{rgb}{.8,.349,.1}
\usepackage{hyperref}

\journal{Medical Image Analysis}

\begin{document}

\begin{frontmatter}
%\title{Comparative validation of polyp and instrument segmentation methods in colonoscopy: results from Medico 2020 polyp segmentation and  MedAI 2021 transparency challenge}
%\title{An objective validation of polyp and instrument segmentation  methods in colonoscopy through the Medico 2020 polyp segmentation and MedAI 2021 transparency challenges}

%\title{Validating polyp and instrument segmentation methods in colonoscopy through Medico 2020 polyp segmentation and MedAI 2021 transparency challenges}

\title{Validating polyp and instrument segmentation methods in colonoscopy through Medico 2020 and MedAI 2021 Challenges}

\author[1]{Debesh Jha}
\cortext[cor1]{Corresponding author}
\ead{debesh.jha@northwestern.edu}
\author[4]{Vanshali Sharma}
\author[5]{Debapriya Banik}
\author[7]{Debayan Bhattacharya}
\author[5]{Kaushiki Roy}
\author[2]{Steven A. Hicks}
\author[1]{Nikhil Kumar Tomar}
\author[2]{Vajira Thambawita}
\author[11]{Adrian Krenzer}
\author[17]{Ge-Peng Ji}
\author[15]{Sahadev Poudel}
\author[19]{George Batchkala}
\author[26]{Saruar Alam}
\author[9]{Awadelrahman M. A. Ahmed}
\author[21]{Quoc-Huy Trinh}
\author[20]{Zeshan Khan}
\author[12]{Tien-Phat Nguyen}
\author[23]{Shruti Shrestha}
\author[24]{Sabari Nathan}
\author[25]{Jeonghwan Gwak}
\author[1]{Ritika K. Jha}
\author[1]{Zheyuan Zhang}
\author[7]{Alexander Schlaefer}
\author[5]{Debotosh Bhattacharjee}
\author[4]{M.K. Bhuyan}
\author[4]{Pradip K. Das}
\author[29]{Deng-Ping Fan}
\author[21]{Sravanthi Parasa}
\author[16]{Sharib Ali}
\author[2,3]{Michael A. Riegler}
\author[2,3]{P{\aa}l Halvorsen}
\author[27,28]{Thomas de Lange}
\author[1]{Ulas Bagci}

\address[1]{Machine \& Hybrid Intelligence Lab, Department of Radiology, Northwestern University, Chicago, USA}
\address[2]{SimulaMet, Oslo, Norway}
\address[3]{Oslo Metropolitan University, Oslo, Norway}
\address[5]{Jadavpur University, Kolkata, India}
\address[4]{Indian Institute of Technology, Guwahati, India}
\address[7]{Institute of Medical Technology and Intelligent Systems, Technische Universität Hamburg, Germany}
\address[8]{Jaeyong Kang, Information Systems Technology and Design, Singapore University of Technology and Design, Singapore}
\address[9]{University of Oslo, Norway}
\address[11]{Julius-Maximilian University of Würzburg, Germany}
\address[12]{Faculty of Information Technology, University of Science, VNU-HCM, Vietnam}
\address[13]{Vietnam National University, Ho Chi Minh City, Vietnam}
\address[14]{Department of Colorectal Surgery, the Second Affiliated Hospital of Zhejiang University School of Medicine, Zhejiang, China} 
\address[15]{Department of IT Convergence Engineering, Gachon University, Seongnam 13120, South Korea}
\address[16]{School of Computing, University of Leeds, LS2 9JT, Leeds, United Kingdom}
\address[17]{College of Engineering, Australian National University, Canberra, Australia}
\address[18]{School of Computer Science, Wuhan University, Hubei, China}
\address[19]{Department of Engineering Science, University of Oxford, Oxford, UK}
\address[20]{National University of Computer and Emerging Sciences, Karachi Campus, Pakistan} 
\address[21]{Swedish Medical Center, Seattle, USA}
\address[22]{Vietnam National University, Ho Chi Minh City, Vietnam}
\address[23]{NepAL Applied Mathematics and Informatics Institute for Research (NAAMII), Kathmandu, Nepal} 
\address[24]{Couger Inc, Tokyo, Japan}
\address[25]{Department of Software, Korea National University of Transportation, Chungju-si, South Korea}
\address[26]{University of Bergen, Bergen, Norway} 
\address[27]{Department of Medicine and Emergencies - Mölndal Sahlgrenska University Hospital, Region Västra Götaland, Sweden} 
\address[28]{Department of Molecular and Clinical Medicin, Sahlgrenska Academy, University of Gothenburg, Sweden}
\address[29]{Computer Vision Lab (CVL), ETH Zurich, Zurich, Switzerland}
%\address[@]{joint senior authors}


\received{x xxxx xxxx}
\finalform{x xxxx xxxx}
\accepted{x xxxx xxxx}
\availableonline{x xxxx xxxx}
\communicated{xxxx xxxx}

\begin{abstract}
Automatic analysis of colonoscopy images has been an active field of research motivated by the importance of early detection of precancerous polyps. However, detecting polyps during the live examination can be challenging due to various factors such as variation of skills and experience among the endoscopists,  lack of attentiveness, and fatigue leading to a high polyp miss-rate. Therefore, there is a need for an automated system that can flag missed polyps during the examination and improve patient care. Deep learning has emerged as a promising solution to this challenge as it can assist endoscopists in detecting and classifying overlooked polyps and abnormalities in real time, improving the accuracy of diagnosis and enhancing treatment. In addition to the algorithm's accuracy, transparency and interpretability are crucial to explaining the whys and hows of the algorithm's prediction. Further, conclusions based on incorrect decisions may be fatal, especially in medicine. Despite these pitfalls,  most algorithms are developed in private data, closed source, or proprietary software, and methods lack reproducibility. Therefore, to promote the development of efficient and transparent methods, we have organized the \textit{{``Medico automatic polyp segmentation (Medico 2020)''}} and \textit{``MedAI: Transparency in Medical Image Segmentation (MedAI 2021)"} competitions. {The Medico 2020 challenge received submissions from 17 teams, while the MedAI 2021 challenge also gathered submissions from another 17 distinct teams in the following year.} We present a comprehensive summary and analyze each contribution, highlight the strength of the best-performing methods, and discuss the possibility of clinical translations of such methods into the clinic. Our analysis revealed that the participants improved dice coefficient metrics from 0.8607 in 2020 to 0.8993 in 2021 despite adding diverse and challenging frames (containing irregular, smaller, sessile, or flat polyps), which are frequently missed during a routine clinical examination. For the instrument segmentation task, the best team obtained a mean Intersection over union metric of 0.9364. For the transparency task, a multi-disciplinary team, including expert gastroenterologists, accessed each submission and evaluated the team based on open-source practices, failure case analysis, ablation studies, usability and understandability of evaluations to gain a deeper understanding of the models’ credibility for clinical deployment. The best team obtained a final transparency score of 21 out of 25. Through the comprehensive analysis of the challenge, we not only highlight the advancements in polyp and surgical instrument segmentation but also encourage {subjective} evaluation for building more transparent and understandable AI-based colonoscopy systems. Moreover, we discuss the need for multi-center and out-of-distribution testing to address the current limitations of the methods to reduce the cancer burden and improve patient care.  

\end{abstract}

\begin{keyword}
\vspace{-15pt}
\KWD Colonoscopy  \sep polyp segmentation  \sep Transparency \sep polyp challenge \sep computer-aided diagnosis  \sep medicine
%\sep instrument segmentation 
 %\sep Efficient processing
\end{keyword}
\end{frontmatter}


\section{Introduction}
\label{section:introduction}
{Gastrointestinal (GI)} cancer is a very important global health problem and the second most common cause of mortality in the United States. According to the recent 2023 estimates, there will be approximately  1,958,310 new cancer incidences and 609,820 cancer deaths in the United States~\citep{siegel2023cancer}. Among various types of cancer, the highest number of deaths occur from lung, prostate, and colorectum in men and lung, breast, and colorectum cancer in women. As colorectal cancer is prevalent among both men and women, it is the second leading cause of cancer related death overall. One of the key indicators of colon cancer is the development of polyps in the colon and rectum. The 5-year survival rate for colon cancer is 68\%, and 44\% for stomach cancer~\citep{asplund2018survival}. If colorectal polyps are detected and removed early, the survival is close to 100.~\citep{levin2008screening}. Thus, regular screening is crucial for early detection of these polyps, as it allows for earlier diagnosis and prompt treatment. 

Endoscopic procedures, such as colonoscopy, are considered the gold standard for detecting and treating mucosal abnormalities in the GI tract (such as polyps) and cancer~\citep{moriyama2015advanced}. However, manual screening for polyps is susceptible to error and is also time-consuming. {This has driven the development of} \ac{CADe} {and} \ac{CADx} {systems that can be integrated into the clinical workflow}~\citep{riegler2016eir} {and potentially contribute to the prevention of colorectal cancer.} In the past, traditional machine learning-based \ac{CADx} systems~\citep{inproceedings2,inproceedings3} were popular.  With the recent advancement in the hardware capabilities, such as powerful GPUs and the emergence of deep learning~\citep{article}, the research has shifted towards deep learning-based \ac{CADx} systems~\citep{fan2020pranet,jha2019resunet++}. These algorithms have shown superior performance compared to traditional \ac{CADx} solutions.


% Figure environment removed

However, despite their superior performance, deep learning-based \ac{CADx} systems are still considered a ``black box'', meaning their inner workings are not fully understood or there is a lack of transparency in understanding the predictions made by the model. Because of the complexity of multiple layers and interconnected nodes in the convolutional neural network, it is challenging to interpret the decision or understand the features contributing to the outcome. For these systems to be widely adopted in clinical settings, they must be rigorously evaluated on benchmark datasets. They must demonstrate the ability to handle patient and recording device variability, provide explainability and robustness and process data in real-time. Only by carefully evaluating these systems, we can ensure the reliability and effectiveness of detecting and diagnosing cancer and its precursors (such as polyps) in a clinical setting.


In this paper, we present a comprehensive analysis of the results of the two prominent challenges in the field of automatic polyp segmentation, namely, \textit{{``Medico automatic polyp segmentation (Medico 2020)~\footnote{\url{https://multimediaeval.github.io/editions/2020/tasks/medico/}}''}} challenge and the \textit{``MedAI: Transparency in Medical Image Segmentation (MedAI 2021)''~\footnote{\url{https://github.com/Nordic-Machine-Intelligence/MedAI-Transparency-in-Medical-Image-Segmentation}}} challenge. {These challenges aimed to explore the potential of \ac{CADx} solutions on the same shared datasets, focusing on developing novel \ac{SOTA} methods in terms of high-performance metrics, efficiency,  transparency, and explainability, aiming to evaluate the relevance of such algorithms in clinical workflows.} The challenges  posed four distinct tasks:

% TODO: A critical part that is missing is why instrument segmentation - reviewers will pick on that! No motivation provided for this part of the challenge MedAI2021

\begin{itemize}
    \item \textbf{Accurate polyp segmentation task} to develop novel algorithms to enhance the early detection and treatment of colon cancer (Medico 2020, MedAI 2021).
    \item \textbf{Algorithm efficiency task} to develop efficient methods with the {highest} frames-per-second (FPS) on predetermined hardware (Medico 2020).
    \item \textbf{Surgical instruments segmentation task} to enable tracking and localization of essential tools in endoscopy and help to improve targeted biopsies and surgeries in complex GI tract organs (MedAI 2021).
    \item \textbf{Transparency task} to {evaluate different models from a transparency perspective, focusing} on explanations of the training procedure, {failure analysis, and model's predictions interpretation by interdisciplinary team}) (MedAI 2021).
\end{itemize}

These tasks were focused on the development of \ac{SOTA} algorithms for polyp and instrument segmentation in a variety of settings, including performance evaluation, resource utilization (efficiency), and transparency. By analyzing the results of these challenges, we can better understand the field's current state, identify the strength and weaknesses of different methods and find the most effective method for our problem.  It is also useful to identify the research gap and areas for future innovation in the field of polyp, instrument and medical image segmentation. Figure~\ref{GIChallengeoverviewfigure} provides an overview of both challenges along with the total number of images used for training and testing in each task. Ground truth samples with their corresponding original images are also presented for the segmentation tasks. In addition, task-specific metrics are presented (for example, FPS for ``Algorithm efficiency''). 

In short, the main contributions are the following: (i) We present a comprehensive and detailed analysis of all participant results; (ii) we provide an overview and comparative analysis of the developed methods; (iii) we obtain and discuss new insights into the current state of AI in the field of GI endoscopy including open challenges and future directions; and (iv) finally, we provide a detailed discussion of issues such as {trust, safety, interpretability, transparency,} generalizability issues and multi-center in context to current limitations of CADx systems.

%NOTE: CADe and CADx are just detection and diagnostic systems where you have to identify benign or malignant lesion - here your challenge is not identification or characterisation - its  a computer-aided polyp segmentation! You could use CADx but some reviewers might oppose it especially if they are from clinical background - you could write CADx system for polyp segmentation


\section{Challenge description}
%Here, we briefly describe both the challenge and tasks. 
\subsection{{Medico 2020 Automatic Polyp Segmentation Challenge}}
The ``Medico Automatic Polyp Segmentation Challenge" was an international benchmarking challenge hosted through the MediaEval platform {(Multimedia Evaluation Workshop). Medico 2020 is the fourth iteration of the Medico Multimedia Tasks series, following the pattern established in previous years.} This challenge aimed to benchmark automated polyp segmentation algorithms using the same dataset and develop methods to detect difficult-to-detect polyps (such as flat, sessile, and small or diminutive polyps). Researchers from medical image analysis, machine learning, multimedia, and computer vision were invited to submit their results for this challenge, which included two tasks. {The members from the organizer's institute were allowed to participate but were ineligible for receiving any recognition certificates. Participants could use any method, focusing on creating automated solutions. Below, we provide the task description of each sub-task}. 

a) \textbf{Automatic Polyp Segmentation Task}: In this task, the participants were asked to develop innovative algorithms for segmenting polyps in colonoscopic images. The focus was on developing efficient systems that could accurately segment the maximum polyp area in a frame while being fast enough for practical use in a clinical setting. This task addresses the need for robust \ac{CADx} solutions for colonoscopy.

To participate in the challenge, participants were required to train their segmentation models on an available training dataset. Once the test dataset was released, participants could test their models and submit their predicted segmentation maps to the organizers in a zip file with the name of each segmentation map image matching the colonoscopy image in the test dataset. 

{b) \textbf{Algorithmic Efficiency Task}}: CADx systems for polyp segmentation that operate in real-time can provide valuable feedback to clinicians during colonoscopy examinations, potentially reducing the risk of missing polyps and incomplete removal. However, real-time deep learning-based CADx solutions often have fewer parameters and may therefore have lower segmentation accuracy compared to more computationally intensive CADx solutions. In order to address this trade-off between accuracy and speed, the efficiency task of the challenge was designed to encourage the development of lightweight segmentation models that are both accurate and fast.

To participate in this task, participants were asked to submit docker images of their proposed algorithms. These algorithms were then evaluated on a dedicated Nvidia GeForce GTX 1080 graphics card, and the results were used to rank the teams. A \acf{mIoU} threshold was set for considering a solution to be a valid efficient segmentation solution, and teams were ranked according to their \ac{FPS}. By focusing on developing efficient CAD solutions, this task aimed to foster the creation of real-time systems that can provide valuable feedback to clinicians while maintaining high accuracy. A detailed description of the challenge, tasks, and evaluation metrics can be found in~\citep{jha2020medico}. 

\subsection{{MedAI: Transparency in Medical Image Segmentation Challenge}}

MedAI: Transparency in Medical Image Segmentation challenge (MedAI 2021) was held for the first time at the Nordic AI Meet~\footnote{\url{https://nordicaimeet.com}} 2021 {(Nordic young researchers symposium)} that focused on medical image segmentation and transparency in \ac{ML} based \ac{CADx} systems. This challenge proposed three tasks to address specific endoscopic GI image segmentation challenges, including two separate segmentation scenarios and one scenario on transparent ML systems. The latter task emphasized the need for explainable and interpretable \ac{ML} algorithms in the field of medical image analysis. {Similar to the other challenge, participants were granted the flexibility to use any method, focusing on developing automated solutions. The members from the organizer's institute were permitted to participate but were not considered for awards.}

{To participate in this challenge, participants were given a training dataset to use for their {ML} models}. These models were then tested on a concealed test dataset, allowing participants to evaluate their performance. The focus on transparency underscores the importance of developing \ac{ML} algorithms that provide not only accurate and efficient results but also provide interpretable and explainable predictions. By addressing these specific challenges, this challenge aimed to foster the development of innovative and effective \ac{CADx} solutions for GI endoscopy. {Below, we present each challenge sub-task}. 

\textbf{a) Automatic Polyp Segmentation Task}: In this task, participants were invited to submit segmentation masks of polyps from colonoscopic images of the large bowel. They were provided with a training dataset to develop their models, and a hidden test dataset was later released to them without the ground truth segmentation masks. Participants were required to submit a zip file containing their predicted masks in the same resolution as the input images, with the filenames of each mask matching the corresponding input image and using the ``.png" file format. The objective of this task was similar to Medico 2020. By using a hidden test dataset, the results of this task were reliable and provided a valuable benchmark for the field.


\input{tables/Table_overview_challenges.tex}


\textbf{b) Automatic Instrument Segmentation Task}: The instrument segmentation task required the development of algorithms that could generate segmentation masks for GI accessory instruments such as biopsy forceps or polyp snares used during live endoscopy procedures. This task aimed to create segmentation models that enable tracking and localization of essential tools in endoscopy that could aid endoscopists during interventions (such as polypectomies) by providing a precise and dense map of the instrument. Like the polyp segmentation task, participants were given a training dataset to develop their models. The submission procedure for this task was similar to that of the polyp segmentation task, with participants required to submit zip files containing their predicted masks in the same resolution as the input images and with filenames matching the corresponding input images. %By focusing on the segmentation of instruments during live endoscopy procedures, this task aimed to encourage the development of innovative solutions that could enhance the accuracy and efficiency of these procedures. 

% %\subsection{Transparency}
% {One of the main aims of the MedAI 2021 challenge was to promote transparency in polyp and surgical instrument segmentation. In addition to the metric-based evaluation, we also evaluated submissions based on the transparency of their work. We encouraged participants to share how their models were trained, how datasets were used, and the insight on the interpretation of the model prediction. We allowed the participants to submit, considering the transparency and left them to decide what to deliver for the task. We gave some suggestions, such as providing rigorous analysis of the failing cases and a detailed GitHub repository with clear steps to run the code for reproducibility. We encouraged the participants to list package dependencies, provide the code for the architecture (guiding through building, compiling, and training of the proposed architecture), provide documentation for the code, and share trained model weights in a standardized format.
% Additionally, we encouraged participants to include the code for model evaluation and provide repository licensing information to enable others to use the code and the trained model responsibly. Moreover, we suggested that the participants explain model predictions using intermediate heatmaps and statistical analysis and provide a detailed ablation study to show the contributing blocks in the network. Although heatmaps might not be the best choice, other alternatives, such as SHapley Additive exPlanations (SHAP), should be explored.} 

% {The submissions of the transparency task were evaluated using a more qualitative approach. A multi-disciplinary team accessed each submission and evaluated the transparency of the proposed solution. Then, we provided a report on the transparency of their submission and highlighted the details about which parts were good and which required more clarity to achieve transparency. Using this approach, we hope that the participant will be more responsible in making their submission in a public repository and including transparency parameters in their future work.}

% Figure environment removed



\textbf{c) Transparency Task}:
The transparency task focused on the importance of transparent research in medical artificial intelligence (AI). The main goal of this task was to evaluate systems from a transparency perspective, which included detailing the training procedure of the algorithms, the dataset used for training, the interpretation of the model's predictions, the use of explainable AI methods, etc. To participate in this task, researchers were encouraged to perform ablation studies, conduct a thorough failure analysis, and share their code in a GitHub repository with clear steps for reproducing the results. {We allowed the participants to submit, considering the transparency and left them to decide what to deliver for the task.} 



In addition, participants were required to submit a one-page document summarizing their findings from the transparency task. {We encouraged the participants to list package dependencies and architecture code (with instruction for building, compiling, and training) and share trained model weights in a standardized format. Additionally, we encouraged participants to include the code for model evaluation and provide repository licensing information to enable others to use the code and the trained model responsibly. Moreover, we suggested that the participants explain model predictions using intermediate heatmaps, statistical analysis and alternatives, such as SHapley Additive exPlanations~\citep{lundberg2017unified}.} By promoting transparency in AI research, this task aimed to foster the development of reliable, interpretable, and trustworthy algorithms for use in medical image segmentation. A detailed description of the challenge can also be found in~\citep{medaireview}. 



\section{Related Work}
Polyp detection and segmentation using \ac{ML} has been an active field of research for over a decade but have been previously limited by hand-crafted features ~\citep{bernal2012towards, hwang2007polyp}. Previous methods had limitations in sub-optimal performance, poor generalization to unseen images, and complexity that limited real-world applicability. However, in the recent 5-6 years, with the success of Convolutional Neural Networks (CNNs), the polyp segmentation task has seen a tremendous performance boost, including the winning model in the MICCAI challenge~\citep{bernal2017comparative}. The widespread use of CNNs, particularly the U-Net ~\citep{ronneberger2015u} and its variants, have been successfully applied on several polyp segmentation datasets and discussed in challenge reports. In addition, recent advances in CNN architectures for polyp segmentation have focused on improving convolution operations~\citep{alam2020automatic}, adding attention blocks~\citep{jha2019resunet++, oktay2018attention}, incorporating feature aggregation blocks(~\citep{mahmud2021polypsegnet}) and using self-supervised learning techniques~\citep{bhattacharya2021self}. These modifications and learning strategies have proven effective in improving the accuracy and reliability of polyp segmentation using CNNs. Apart from the contributions of individual research groups, several challenges~\citep{bernal2017comparative,ali2021deep} have been organized to improve the detection and classification of mucosal abnormalities in the GI tract from either single image frames or videos. However, the dataset provided in the challenge and the details of the proposed algorithms are often not publicly available, making it difficult to reproduce and build upon them. Hence, there is a need for open-access benchmarking datasets and reproducible algorithms to facilitate progress in this field. %Shin.2018



Table~\ref{tab:challengeoverview} provides an overview of {GI image analysis challenges} held in the past eight years. The challenge was conducted using images from different modalities with a specific focus on polyp segmentation, detection,  localization and wireless capsule endoscopy lesion detection and localization. In 2015, Bernal et al.~\citep{bernal2017comparative} organized the ``Automatic Polyp Detection in colonoscopy videos'' challenge. Likewise, they organized the GIANA challenge in 2017 and 2018\footnote{\url{https://giana.grand-challenge.org/}} focused on colonoscopy data and included tasks such as detection of lesions in Video capsule Endoscopy (VCE), polyp detection, and polyp segmentation. Recent challenges attempted to address generalizability in polyp detection and segmentation~\citep{ali2022assessing} with single frames and sequence colonoscopy datasets. They demonstrated how variability in images can affect algorithm performances.  Altogether, these challenges have led to many algorithmic innovations in detecting and classifying GI abnormalities {(especially polyp segmentation and detection)}.

%{Similarly, Medico 2021~\citep{hicks2021medico} focused on polyp segmentation, emphasizing algorithmic performance, efficiency, and transparency. Kindly note that this challenge was held after MedAI 2021. We note this study to make our overview table comprehensive}.

Additionally, past challenges have not emphasized on the explainability and reliability of deep learning model predictions. Most challenges also do not focus on open source codes for research and development, making it difficult for proposed algorithms to be adopted in clinical settings due to a lack of transparency. Moreover, the reported methods are not reproducible, which hinders further algorithmic advancement. Thus, we lose track of what are best practices and where we are heading in this field. Through our challenges in Medico 2020 and MedAI 2021, we address reproducibility and open science which are the two most important aspects that can enable experienced and new ML scientists to build upon and advance the field.

%Medico 2020~\citep{jha2020medico} focused on promoting new algorithmic innovations in polyp segmentation and assessing algorithmic efficiency, while MedAI 2021 ~\citep{medaireview} emphasizes innovations in both polyp segmentation and instrument segmentation. In addition, the MedAI 2021 challenge also introduces a transparency task that encourages and validates reproducible research and a focus on the explainability of model predictions. Through these two challenges, we aimed to address some of the key challenges in GI endoscopy, including benchmarking of datasets, reproducibility of algorithms, and explainability of model predictions. In this paper, we comprehensively analyze the outcomes of both challenges. 


%The EndoCV2020 challenge\footnote{\url{https://endocv.grand-challenge.org}} included a sub-challenge on ``Endoscopy disease detection (EDD2020)" with multi-organ and multi-modal endoscopy data, but only 386 annotated frames and 5 class categories were included. 

%Several challenges have been organized in the past eight years to compare and improve computer vision classification methods and benchmark GI endoscopy image datasets. 

% Note/Comment: Most reviewers knows endocv2021 - they may think the information is not upto date - I recommend using all information and develop from there - truth to be told but see the brighter side of what is done here!

%Overall, data science challenges in this field of research have been useful in benchmarking and generating datasets which is encouraging and positive in the development of innovative algorithms for detecting abnormalities in GI endoscopy. However, there is still a need for more comprehensive benchmarking of various datasets in GI endoscopy under several task categories that have not been addressed in other challenges. Furthermore, most of these datasets are not publicly available, making it difficult to reproduce results and develop new algorithms based on them. 

%These challenges have limitations such as small datasets~\citep{ali2020endoscopy} or datasets that are not used consistently across different challenges (for example, EndoVis2015 challenge on Early Barrett's cancer detection\footnote{\url{https://endovissub-barrett.grand-challenge.org}}). 



\section{Challenge datasets and evaluation metrics}

% Figure environment removed

\subsection{Medico 2020 dataset}
The training dataset contains 1,000 polyp images and their corresponding ground truth mask taken from Kvasir-SEG~\citep{jha2020kvasir}. {Kvasir-SEG consists of diverse images varying in appearance, such as sizes (for example, diminutive, regular or large), colors (same color as mucosa, or different colors such as reddish), textures (smooth or granular), locations (anywhere in large intestine such as left colon, sigmoid colon or rectum), numbers of polyp per images (for example, one or many), image quality (illumination, artifacts) and shapes (flat, pedunculated, and sessile). The variation ensures that the algorithms trained on this dataset can handle real-world variations in clinical settings. Some samples are shown in Figure~\ref{fig:polyp20_21}.}



{The datasets were acquired from real routine clinical examinations at Vestre Viken Health Trust (VV) in Norway by a team of expert gastroenterologists. The VV is the collaboration of the four hospitals that provide healthcare services to 470,000 people. The resolution of images varies from $332\times487$ to $1920\times1072$ pixels. Some images contain green thumbnails in the lower-left corner of the images showing the position marking from the ScopeGuide (Olympus). After data acquisition, our team categorized the dataset into a polyp class. To extend the dataset to the segmentation class, a team of one experienced engineer, a medical doctor, and an expert gastroenterologist annotated the polyp images using the label box tool. After annotation, we extract the corresponding ground truth and bounding box information. Once the ground truth was created, the images and ground truths were combined to facilitate the review process. These images were sent to a team of expert gastroenterologists for validation through a web-based interface. After validation, we compiled them into training and test datasets. The data proportion for each set followed the general split ratio used in the literature.}

The training dataset has been made publicly available as open access and is widely available at\footnote{\url{https://datasets.simula.no/kvasir-seg/}}. {The test dataset contains unique polyp images encompassing a wide range of diverse clinical scenarios with different polyp characteristics, varying lighting conditions and image resolution, low-quality images, as well as complex polyp images (for example, with instruments and residual stool) that the model has never encountered before. {Only the organizers had access to the test case labels. Currently, the test data can be downloaded from}\footnote{\url{https://drive.google.com/file/d/1uP2W2g0iCCS3T6Cf7TPmNdSX4gayOrv2}}}. %The dataset splits were designed to follow the general ratios used in the segmentation research literature.

%The performance of the participating teams was evaluated using our diverse dataset, which promotes generalization.


%Additionally, in both challenges, we carefully chose the test data with different polyp characteristics, varying lighting conditions, and image resolution, low-quality images as well as complex polyp images (for example, with instruments and residual stool) that the model has never encountered before. The performance of the participating teams was evaluated using our diverse dataset, which promotes generalization. 


\subsection{MedAI Transparency challenge 2021 dataset}
{We utilize our Kvasir-SEG}~\citep{jha2020kvasir} {as the development dataset for the polyp segmentation task. Similarly, Kvasir-Instrument~\citep{jha2021kvasir} was used as the training dataset for the instrument segmentation task. It can be downloaded from} \footnote{\url{https://datasets.simula.no/kvasir-instrument/}}. {We followed the same data acquisition and annotation protocol for test dataset creation as the Medico 2020 challenge.} Some sample images for polyp segmentation and instrument segmentation tasks are presented in Figure~\ref{fig:polyp20_21} and Figure~\ref{fig:inst21}. Figure~\ref{fig:datadistributuion} shows the data distribution of the train and test datasets used in Medico 2020 and MedAI 2021. {We have categorized the images into ``small", ``medium'' and ``large'' according to the size of regions of interest using a randomly selected threshold of 0.3 and 0.1 and plotted the normalized height versus normalized width of each data point.} This is to visualize the dimension of each data point and observe the diversity and complexity of the dataset used in the study. The information about the size categories and the dataset's dimensions is crucial for assessing the performance and robustness of the proposed algorithms. 


%\section{Evaluation Metrics}
\subsection{Metrics for polyp and instrument segmentation tasks}

{We used mean Intersection over Union ({mIoU}) as a primary evaluation metric for the polyp and instrument segmentation tasks. If the teams achieved the same mIoU values, their ranking was further evaluated based on the higher value of the \ac{DSC}.}  We also recommend calculating other important standard evaluation metrics that hold significant relevance in clinical settings such as Accuracy (Acc), Recall (Rec),  Precision (Pre), F-2 score, and Frames per second ({FPS}) to ensure a detailed evaluation.

\subsection{Metrics for efficiency tasks}   
{Efficiency is crucial in colonoscopy as it directly impacts the models' feasibility and practicality in real-world scenarios. Endoscopists often need to analyze numerous frames in real-time during routine colonoscopy, and lag (latency) in the analysis could lead to suboptimal results. Our approach to FPS calculation was based on the time taken to process a single image, averaged over the entire dataset, and then extrapolated to a per-second rate.} Therefore, we strongly recommend calculating processing speed in terms of \ac{FPS}.


\subsection{Metrics for transparency tasks}
{The transparency task aimed to assess the transparency and understandability of algorithms for medical AI by utilizing a qualitative approach in the evaluation metrics. We evaluated transparency tasks using a more quantitative approach than polyp and instrument segmentation. A multi-disciplinary team assessed each submission and evaluated the transparency and understandability of the proposed solutions. Each team was scored based on the three criteria: open source code, model evaluation and clinical evaluation. The open source code was evaluated based on the presence of a publicly available repository, code quality and quality of the readme file. The model evaluation included failure analysis, ablation study, explainability of the method, and metrics used. Evaluation by clinical experts considered the usefulness of the technique and its understandability. With these three criteria, we aimed to measure the transparency of the provided solutions. A detailed score distribution under different criteria is shown in Table~\ref{tab:transscore}, which was part of our Task 3. Ultimately, this task aimed to promote the development of more transparent and interpretable AI systems.}

%The evaluation team, comprising multiple experts from diverse fields, evaluated the submissions based on various attributes such as the availability of code, the depth of evaluation, reproducibility, and the implementation of explainable AI techniques. In addition, participants are expected to provide detailed information about their solution, including rigorous failure analysis, thorough ablation studies, and a comprehensive GitHub repository with clear reproducibility steps. The transparency evaluation is divided into three categories: open source, model evaluation, and clinical evaluation, with corresponding scores outlined in Table~\ref{tab:transscore}.  % Here, submissions that did not participate in {task 3 are not included in the table}.
\vspace{-4mm}

\section{Participating Research Teams}
\vspace{-2mm}
\subsection{Methods used in Medico 2020}
Table~\ref{table:challenge_summary_medico} summarizes all the teams participating in the ``Medico 2020" challenge. {It can be seen from Table}~\ref{table:challenge_summary_medico}   {that all 17 teams participated in Task 1, whereas only 9 teams participated in Task 2.}

\begin{table} [!t]
\caption{Summary information of participating teams in Medico 2020. Here, `$\surd$'  = Team participated, `--' = No participation, \textbf{Task 1 =} Polyp segmentation task and \textbf{Task 2 =} Algorithm efficiency task.}
\label{table:challenge_summary_medico}
\centering
\begin{tabular}{p{0.7cm}|p{3.5cm}|p{1cm}p{1cm}}
\toprule
\textbf{Chal.} & \centering \textbf{Team Name} & \textbf{Task 1} &\textbf{Task 2}\\
\midrule

\multirow{17}{*}{\rotatebox{90}{\textbf{Medico 2020}}}
              & FAST-NU-DS &$\surd$  &  $\surd$   \\ %\hline
              & AI-TCE & $\surd$  & -- \\ %\hline
              & ML-MMIVSARUAR & $\surd$ &--    \\ %\hline
              & UiO-Zero  & $\surd$ &--\\ %\hline
              & HBKU\_UNITN\_SIMULA & $\surd$ &--  \\ %\hline
              & AI-JMU & $\surd$ &$\surd$  \\ %\hline
              & SBS & $\surd$ & $\surd$  \\ %\hline
              & AMI Lab & $\surd$ &$\surd$ \\ %\hline
              & UNITRK & $\surd$& $\surd$  \\ %\hline
              & MedSeg\_JU & $\surd$ &-- \\ %\hline
              & IIAI-Med & $\surd$ &-- \\ %\hline
              & HGV-HCMUS & $\surd$ & $\surd$   \\ %\hline
              & GeorgeBatch & $\surd$& $\surd$ \\ 
              & PRML2020GU & $\surd$ & $\surd$  \\ %\hline
              & VT & $\surd$ &  -- \\ %\hline
              & IRIS-NSYSU & $\surd$ & -- \\ %\hline
              & NKT & $\surd$& $\surd$  \\ %\hline
              \bottomrule
\end{tabular}
\vspace{-6mm}
\end{table}
\input{tables/Summary_Team_Contributions}

\textbf{FAST-NU-DS:} 
Team FAST-NU-DS~\citep{ali2020depth} explored the advantage of using depth-wise separable convolution in the atrous convolution of the ResUNet++\citep{jha2019resunet++} architecture.  Modifications were made to get the lightweight image segmentation.  Deep atrous spatial pyramid pooling was also implemented on the ResUNet++ architecture.  The purpose of this architectural design was to provide good performance on the image segmentation evaluation metrics and inference time.  {To get the lightweight model architecture, changes were made to the atrous bridge in ResUNet++ architecture.  The convolution layer in the atrous bridge was replaced with depthwise separable convolution.  Depth-wise separable convolution first applies channel-wise filters, followed by a $1\times1$ pointwise convolution, to maintain performance while streamlining computations. } The implementation of depth-wise separable convolution resulted in less number of parameters and giga-floating point operations (GFLOPs).  
%Modifications in model architecture were compared against UNet and ResUNet++.  All models were trained on custom mean intersection over union loss.
%To get the lightweight model architecture the convolution layer in the atrous bridge was replaced with depth-wise separable convolution and the atrous bridge was also replaced with a deep atrous bridge.  The comparison of modification in model architecture was made against UNet~\citep{ronneberger2015u} and ResUNet++. All models were trained on custom mean intersection over union loss.


\textbf{AI-TCE:} 
Team AI-TCE~\citep{nathan2020efficient} proposed an efficient supervision network that uses EfficientNet~\citep{tan2019efficientnet} and an attention Unit. The proposed network had the properties of an encoder-decoder structure with supervision layers. An EfficientNet-B4 was used as a pre-trained architecture in the encoder block. The decoder block combined dense block and Concurrent Spatial and Channel Attention  block. Both the encoder and decoder were connected by Convolution Block Attention Module (CBAM). All the outputs of the decoder layer were supervised, i.e., individual decoder output was taken and upsampled with the output layer and supervised by the loss function. Also, all upsampled outputs were concatenated and fed into CBAM. In the upsampling, the convolution transpose layer was used.  %(CSCA)


\textbf{ML-MMIV SARUAR}: Team ML-MMIV SARUAR~\citep{alam2020automatic} used the U-Net with pre-trained ResNet50 on the ImageNet dataset as the encoder for the polyp segmentation task. The use of a pre-trained encoder helped the model to converge easily. The input image was fed into the pre-trained ResNet50 encoder, consisting of a series of residual blocks as their main component. These residual blocks helped the encoder extract the important features from the input image, which were then passed to the decoder. Skip connections between the encoder and decoder branch help the model to get all the low-level semantic information from the encoder, which allowed the decoder to generate the desired feature maps.


\textbf{UiO-Zero}: Team UiO-Zero~\citep{ahmed2020generative}  used the generative adversarial networks (GAN) framework for solving the automatic segmentation problem. Perceiving the problem as an image-to-image translation task, conditional GANs were utilized to generate masks conditioned by the images as inputs. The polyp segmentation GAN-based model consists of two networks, namely a generator and discriminator, that were based on convolution neural networks. A generator takes the images as input and tries to produce realistic-looking masks conditioned by this input and a discriminator, which was basically a classifier that had access to the ground truth masks and tried to classify whether the generated masks was real or not. To stabilize the training, the images were concatenated with the masks (generated or real) before being fed to the discriminator.


\textbf{HGV-HCMUS}: The {HGV-HCMUS}~\citep{trinh2020hcmus} team proposed methods combining the Residual module, Inception module, Adaptive CNN with U-Net~\citep{ronneberger2015u} model, and PraNet~\citep{fan2020pranet} for semantic segmentation of various types of polyps in endoscopic images. The team submitted five different runs considering five different solutions. In the first approach, a simple U-Net architecture was adopted to parse masks of polyps. Second, the regular ReLU was replaced with Leaky ReLU to deal with dead neurons. Third, to further boost the result, an Inception module was introduced to extract better features. Fourth, a pre-trained model with the ResNet-50 backbone was used to build ResUNet, yielding better obtained results. Last, PraNet was employed for polyp segmentation in colonoscopy images.  {This solution provided the best outcome and was used to generate the results.}

\textbf{AI-JMU:} Team AI-JMU~\citep{krenzer2020bigger} explored various image segmentation models, specifically the Cascade Mask R-CNN~\citep{cai2019cascade} and Mask R-CNN~\citep{he2017mask} with ResNet~\citep{he2016deep} as well as the ResNeSt~\citep{zhang2022resnest} architectures was used as the backbone. Additionally, the team investigated the effect of varying the depth of both the ResNet and ResNeSt architectures. Depths of 50, 101, and 200 were evaluated for the ResNeSt model, and depths of 50 and 101 for the ResNet model. {They reported that the best outcome was obtained using ResNeSt-101 when combined with Cascade Mask R-CNN.}



\textbf{SBS}: Team SBS~\citep{shrestha2020ensemble} exploited ResNet 34~\citep{he2016deep} and EfficientNet-B2~\citep{tan2019efficientnet} backbones in the U-Net. The team introduced two different models: Single Model and Ensemble Model. The ResNet-34 was used in the single model. The weights saved after the training phase was loaded in the network, and test data were fed to get the predicted polyp masks. However, in the case of the ensemble model, both ResNet-34 and EfficientNetB2 were used to predict the masks. Then the individual prediction was ensembled using bitwise multiplication between the two predicted masks. The ensemble model provided better evaluation results as compared to the single model, as when multiple algorithms were ensembled predictive power increases and error rate decreases. {Hence, the final results are reported using the ensemble model using ResNet-34 and EffiecientNetB2 as backbones in the U-Net architecture.}

%We proposed an ensemble model, for which the best results are reported. In this ensemble method, we used two models, ResNet-34 and EfficientNet-B2, to predict our masks. The predicted masks from these two different models are then combined using bitwise multiplication to obtain the final output. A transfer learning-based approach, utilizing a pre-trained mechanism using the ImageNet dataset, was implemented. The Adam optimizer was used with a learning rate of 1e-3 and default beta values of β1 = 0.9 and β2 = 0.99."




\textbf{AMI Lab:} Team AMI Lab~\citep{kang2020kd} utilized the knowledge distillation technique to improve ResUNet++~~\citep{jha2019resunet++}, which performs well on automatic polyp segmentation. First, the data augmentation module was used to generate augmented images for the input. Second, the obtained augmented images were fed to both the student model and the teacher model. Third, the distillation loss between the outputs of student and teacher models was calculated. Similarly, the loss between the output of the student model and the ground truth label was computed to train the student model.

\textbf{UNITRK:} Team UNITRK~\citep{khadka2020transfer} employed the UNet model pre-trained on the brain MRI dataset. The notion of knowledge transfer has been the key motivating factor to choose a simple pre-trained model. The model was fine-tuned with the polyp dataset. The fine-tuning of the pre-trained model helped to converge faster without the requirement of a large number of training examples. The additive soft attention mechanism was integrated with the pre-trained UNet architecture. The key benefit of this attention UNet structure in comparison to multi-stage CNNs was that it does not require training of multiple models to deal with object localization and thus reduces the number of model parameters. It helps to focus on relevant regions in the input images.


\textbf{MedSeg\_JU}: Team MedSeg\_JU~\citep{banik2020deep} proposed an approach for polyp segmentation based on deep conditional adversarial learning. The proposed framework consists of two interdependent modules: a generator network and a discriminator network. The generator was an encoder-decoder network responsible to predict the polyp mask while the discriminator enforces the segmentation to be as similar to the ground truth segmented mask.  The training process of the network alternates between training the generator and the discriminator, with the generator trained to produce a predicted synthetic mask by freezing the discriminator and the discriminator trained while freezing the generator. 


% Figure environment removed


\textbf{IIAI-Med}: Team IIAI-Med team~\citep{ji2020automatic} presented a novel deep neural network, called the Parallel Reverse Attention Network (PraNet)~\citep{fan2020pranet}, for the task of automatic polyp segmentation at MediaEval 2020. The network first aggregated features in high-level layers using a parallel partial decoder (PPD). This combined feature was then used to generate a global map as the initial guidance area for the following components. Additionally, the network mines boundary cues using a reverse attention (RA) module which establishes the relationship between areas and boundary cues. Thanks to the recurrent cooperation mechanism between areas and boundaries, the PraNet was able to calibrate misaligned predictions, improving segmentation accuracy and achieving real-time efficiency (nearly 30fps). The code and results are available at https://github.com/GewelsJI/MediaEval2020-IIAI-Med.

%HGV-HCMUS:

{\textbf{HBKU\_UNITN\_SIMULA}} Team HBKU\_UNITN\_SIMULA~\citep{nguyen2020hcmus} proposed two different approaches leveraging the advantages of either ResUNet++ or PraNet model to efficiently segment polyps in colonoscopy images, with modifications on the network structure, parameters, and training strategies to tackle various observed characteristics of the given dataset. For the first approach, PraNet was used, which is a parallel reverse attention network that helps to analyze and use the relationship between areas and boundary cues for accurate polyp segmentation. The PraNet with Training Signal Annealing strategy was used to improve segmentation accuracy and effectively train from scratch on the given small dataset.  For the second approach, ResUNet++ was used, which takes advantage of residual blocks, squeeze and excitation blocks, atrous spatial pyramid pooling, and attention blocks. The input path was modified and integrates a guided mask layer to the original structure for better segmentation accuracy.  {They used the two approaches to experiment with different runs. The best polyp segmentation outcome was achieved when the results from three PraNet and five ResUNet++ models, trained on different train-val dataset splits, were averaged. }


\textbf{GeorgeBatch}: Team GeorgeBatch~\citep{batchkala2020real}  used the standard U-Net architecture for the binary segmentation task, and experiments were conducted using the intersection-over-union loss (IoU loss) instead of the commonly used binary cross-entropy (BCE) loss. They also experiment with a combination of both losses in the training process. The motivation behind this approach was to strike a balance between accuracy and speed for using automated systems during colon cancer surveillance and surgical removal of polyps. This balance is considered while experimenting with other parameters like loss function and data augmentation to boost performance. The reported outcomes show that using IoU loss results in enhanced segmentation performance, with a nearly 3\% improvement on the \ac{DSC} metric while maintaining real-time performance (close to 200 FPS). The code and results are available at https://github.com/GeorgeBatch/kvasir-seg.


\textbf{PRML2020GU:} An overview of the approach proposed by team PRML2020GU~\citep{poudel2020automatic} is shown in Figure~\ref{bestmethod2020}. The team employed an EfficientNetB3 as an encoder backbone with a U-Net decoder and leveraged the concept of U-Net++ of redesigning the skip connections to use multi-scale semantic details. The densely connected skip connections to the decoder side enable flexible multi-scale feature fusion both horizontally and vertically at the same resolution. Besides, the proposed method is powered by deep supervision, where all the outputs after deep supervision is averaged, and the final mask is generated. Further, channel-spatial attention enables significantly better performance and fast convergence. Moreover, integrating the channel and spatial attention modules restrains irrelevant features and allows only useful spatial details.

%The model is trained for 35 epochs using the Adam optimizer with a learning rate of 1e-4 and a batch size of 12, and is implemented on a system with an Intel Xeon Processor, 128 GB RAM and NVIDIA Quadro P5000 GPU of 16GB.

\textbf{VT:} Team VT~\citep{thambawita2020pyramid} proposed a simple but efficient idea of using an augmentation method called pyramid focus-augmentation (PYRA) that uses grids in a pyramid-like manner (large to small) for polyp segmentation. The method has two main steps: data augmentation with PYRA using pre-defined grid sizes followed by training of a DL model with the resulting augmented data. PYRA can be used to improve the performance of segmentation tasks when there is a small dataset to train the DL models or if the number of positive findings is small. The method shows a large benefit in the medical diagnosis use case by focusing the clinician’s attention on regions with findings step-by-step.


\textbf{IRISNSYSU:} Team IRISNSYSU~\citep{maxwell2020temporal} proposed a local region model with attentive temporal-spatial pathways for automatically learning various target structures. The attentive spatial pathway highlights the salient region to generate bounding boxes and ignores irrelevant regions in an input image. The proposed attention mechanism allows efficient object localization, and the overall predictive performance is increased because there are fewer false positives for the object detection task for medical images with manual annotations.


\textbf{NKT:} Team NKT~\citep{tomar2021automatic} proposed a full convolution network following an encoder-decoder approach. It combines the strength of residual learning and the attention mechanism of the squeeze and excitation (SE) network. The encoding network consists of 4 encoder blocks with 32, 64, 128, and 256 filters. The decoding network also consists of 4 decoder blocks with 128, 64, 32, and 16 filters. Both the encoder and decoder block consist of a residual block as their core component. The residual block helps in building deep neural networks by solving the vanishing gradient and exploding gradient problem.

Additionally, in Table~\ref{table:challenge_summary2020}, we provide an elaborate summary of all the research teams who participated in the ``Medico 2020" challenge. It gives a detailed overview of the algorithms, backbone, nature, choice basis,  data augmentation used, loss function, and optimizer used by each participating teams. 

\begin{table} [!t]
\caption{Summary information of participating teams in MedAI 2021. Here, `$\surd$'  = Team participated, `--' = No participation,  \textbf{Task 1 =} Polyp segmentation task, \textbf{Task 2 =} Instrument segmentation task, and \textbf{Task 3 =} Transparency task.  {A total of 16 teams participated in polyp segmentation and instrument segmentation and 14 teams participated in the Transparency tasks in the challenge.}}
\label{table:challenge_summary_MedAI}
\centering
\begin{tabular}{p{0.7cm}|p{3.2cm}|p{1cm}p{1cm}p{1cm}}
\toprule
\textbf{Chal.} & \centering \textbf{Team Name} & \textbf{Task 1}  &\textbf{Task 2} &\textbf{Task 3}\\
\midrule
\multirow{16}{*}{\rotatebox{90}{\textbf{MedAI 2021}}}
              & The Segmentors & $\surd$   & $\surd$ &$\surd$\\ %\hline
              & The Arctic & $\surd$    & $\surd$ &$\surd$ \\ %\hline
              & mTEC & $\surd$ &$\surd$ &$\surd$ \\ %\hline
              & MedSeg\_JU  &-- &  $\surd$ &--\\ %\hline
              & MAHUNM & $\surd$ &  $\surd$ &$\surd$ \\ %\hline
              & IIAI-CV\&Med & $\surd$   &  $\surd$  &$\surd$  \\ %\hline
              & NYCity & $\surd$ &  $\surd$ &$\surd$ \\ %\hline
              & PRML & $\surd$ &   $\surd$ &$\surd$ \\ %\hline
              & leen & $\surd$ &   $\surd$ &$\surd$ \\ %\hline
              & CV\&Med IIAI & $\surd$ &  $\surd$ &$\surd$ \\ %\hline
              & Polypixel & $\surd$ &  $\surd$ &$\surd$ \\ %\hline
              & agaldran & $\surd$ &   $\surd$ &$\surd$ \\ %\hline
              & TeamAIKitchen & $\surd$ &$\surd$  & $\surd$  \\ %\hline
              & CamAI& $\surd$ &  $\surd$ &$\surd$ \\ %\hline
              & OXGastroVision & $\surd$ &  $\surd$ &$\surd$ \\
              & Vyobotics & $\surd$  & -- & --\\ %\hline
              & NAAMII & $\surd$ &  $\surd$ & --\\ %\hline     
\bottomrule
\end{tabular}
\end{table}




\input{tables/medai2021summary}

\subsection{Methods used in MedAI 2021}
In this subsection, we briefly summarize the methods used by the participating teams in the MedAI 2021 challenge.
%The challenge has three subtasks: polyp segmentation task, accessory instrument (eg. biopsy forceps or polyp snare) segmentation task, and transparency task. 
In Table~\ref{table:challenge_summary_MedAI}, we present the research teams who have participated in each of these three tasks. It can be seen from this table that most of the teams participated in all three tasks except for {three} teams, which participated in either one or two of the sub-tasks. {All participating teams have used the same architecture in their submission for polyp segmentation and instrument segmentation subtasks.} However, two teams, namely \textit{Vyobotics}~\citep{Rauniyar2021} and  \textit{MedSeg\_JU}~\citep{Banik2021} have participated in only one of the subtasks. The team \textit{Vyobotics}~\citep{Rauniyar2021} has participated in the polyp segmentation task whereas the team \textit{MedSeg\_JU}~\citep{Banik2021} has participated in the surgical instrument segmentation task. %{Here, we briefly describe the method used by each participating team.}


%\subsubsection{Methods for polyp segmentation}

\textbf{The Segmentors}: Team Segmentors~\citep{Mirza2021} %Yeung2021
proposed solution is a UNet-based algorithm designed for segmenting polyps in images taken from endoscopies. The primary focus of this approach was to achieve high segmentation metrics on the supplied test dataset, which was a crucial requirement for accurate and reliable polyp segmentation. To this end, they experimented with data augmentation and model tuning to achieve satisfactory results on the test sets.

%presented a deep learning pipeline that is specifically developed to accurately segment colorectal polyps and various instruments used during endoscopic procedures. To improve transparency and interpretability, the pipeline leveraged the Attention U-Net architecture, which enables visualization of the attention coefficients to identify the most salient regions of the input images. This allowed for a better understanding of the model's decision-making process and facilitated the identification of potential errors. To further improve performance, the pipeline incorporated transfer learning using a pre-trained encoder. Additionally, test-time augmentation, softmax averaging, softmax thresholding and connected component labeling were used to further refine predictions and boost performance.

%\textbf{The Arctic}: The team Arctic~\citep{Somani2021}%Chou2021
%proposed a novel Dual Model Filtering (DMF) strategy, which effectively removed false positive predictions in negative samples through the use of a metrics-based threshold setting. To better adapt to high-resolution input with various distributions, the PVTv2~\citep{wang2022pvt} backbone was embedded into the SINetV2~\citep{fan2021concealed} framework. The SINetV2 framework with Camouflaged Object Detection (COD) was used for better identification ability, as polyp segmentation is a downstream task. Additionally, extensive experiments have been conducted to study the effectiveness of DMF, and it was found that the method performs well under different data distributions, making it a favorable solution for problems where the training dataset had a different distribution of negative samples compared to the testing dataset.

\textbf{The Arctic}: Team Arctic~\citep{Somani2021}
utilized a unique hybrid optimization technique that combined the power of DeepLabV3+~\citep{chen2018encoder} and ResNet101~\citep{he2016deep} to address the specific challenges of GI image segmentation effectively. In order to ensure the accuracy of their results, the team employed a 5-fold cross-validation approach, with a learning rate of 0.0001 and a batch size of 12. Additionally, towards transparency, they proposed a method of rendering feature attention maps to visualize the attention of the network on individual pixels within the image.

\textbf{mTEC}: Team mTEC~\citep{bhattacharya_betz_eggert_schlaefer_2021} introduced a new architecture called Dual Parallel Reverse Attention Edge Network (DPRA-EdgeNet) for joint segmentation of polyp masks and polyp edge masks. This architecture utilizes the reverse attention module from PraNet~\citep{fan2020pranet} to perform the segmentation tasks. The team implemented two parallel decoder blocks, with one focused on extracting features for polyp segmentation and the other focused on extracting features for polyp edge segmentation. The polyp mask decoder leverages the features from the edge decoder block to improve the accuracy of the segmentation. Additionally, the team employed deep supervision of both edge and polyp features to stabilize the optimization process of the model.


\textbf{MedSeg\_JU}: Team MedSeg\_JU~\citep{Banik2021} proposed EM-Net, encoder-decoder-based architecture inspired by the M-Net~\citep{7950555} architecture. In their approach, the encoder branch of the network utilized  EfficientNet-B3~\citep{DBLP:journals/corr/abs-1905-11946} as its backbone. The network also employed a multi-scale input method, where the input image was downsampled at rates of 2, 4, and 8 at each level of the encoder branch, providing a multi-level receptive field. The decoder branch was a mirror structure of the encoder, where upsampling was used to increase the size of the feature maps at each level. Skip connections were used to enhance the flow of spatial information lost during downsampling. The final feature maps underwent point-wise convolution and sigmoid activation and were then upsampled to provide deep supervision and a local pixel-level prediction map for each scale of the input image. These maps were then fused to generate the final segmentation mask. 


\textbf{MAHUNM}: Team MAHUNM~\citep{Haithami2021} presented an approach for enhancing the segmentation capabilities of DeeplabV3 by incorporating Gated Recurrent Neural Network (GRU). In their approach, the team replaced the 1-by-1 convolution in DeeplabV3 with GRU after the ASSP layer to combine input feature maps. While the convolution and GRU had sharable parameters, the latter had gates that enabled or disabled the contribution of each input feature map. The experimental evaluation conducted on unseen test sets demonstrated that using GRU instead of convolution produced better segmentation results.


\textbf{IIAI-CV\&Med}: Team IIAI-CV\&Med~\citep{Dong2021} 
developed an ensemble of three sub-models, namely Polyp-PVT \citep{dong2021polyp}, Sinv2-PVT, and Transfuse-PVT. The official Polyp-PVT, as designed for polyp segmentation, was adopted without modification and achieved state-of-the-art segmentation capability and generalization performance. Transfuse, also designed for polyp segmentation, was improved by replacing the transformer part with the pyramid vision transformer (PVT)~\citep{wang2022pvt} to enhance its performance. The official Sinv2~\citep{fan2021concealed}, which proposes an end-to-end network for searching and recognizing concealed objects, was employed and its original backbone of Res2Net was replaced with a stronger PVT transformer~\citep{wang2022pvt} to extract more meaningful features. 



\textbf{NYCity}: Team NYCity~\citep{Chen2021} presented a novel multi-model ensemble framework. The team first collected a set of SOTA models in this field and further improved them through a series of {refinements. These models include TransFuse~\citep{zhang2021transfuse} and HarDNet-MSEG~\citep{huang2021hardnet}. They improvised TransFuse by replacing its backbone with HarDNet-85~\citep{chao2019hardnet} and placing an additional BiFuse layer. They further modified HarDNet-MSEG by using HarDNet-85 and ResNet-101~\citep{he2016deep} as the backbone. Additionally, they made modifications to the decoder and adopted different receptive fields.} By integrating those fine-tuned models into a more powerful ensemble framework, they were able to achieve improved performance. 
%The proposed multi-model ensemble framework was tested on polyp and instrument datasets, and the experiment results showed that it performed satisfactorily.

\textbf{PRML}: Team PRML~\citep{Poudel2021} introduced Ef-UNet, a segmentation model that is composed of two main components. First, a U-Net encoder that utilizes EfficientNet~\citep{DBLP:journals/corr/abs-1905-11946} as a backbone, which allows the generation of different semantic details in multiple stages. Second, a decoder integrates spatial information from different stages to generate a final precise segmentation mask. Using EfficientNet as the encoder backbone provides Ef-UNet with the ability to efficiently extract high-level features from the input images while the decoder component effectively integrates these features to produce accurate segmentation results.


\textbf{leen}: Team leen~\citep{ahmed2021explainable} utilized the GANs framework to produce corresponding masks that locate the polyps or instruments on GI polyp images. To ensure transparency and explainability of their models, the team leen adopted the layer-wise relevance propagation (LRP) approach~\citep{bach2015pixel}, which is one of the most widely used methods in explainable artificial intelligence. This approach generated relevant maps that display the contribution of each pixel of the input image in the final decision of the model.


\textbf{CV\&Med IIAI}: Team CV\&Med IIAI~\citep{Chou2021}
proposed a novel dual model filtering (DMF) strategy, which effectively removed false positive predictions in negative samples through the use of a metrics-based threshold setting. To better adapt to high-resolution input with various distributions, the PVTv2~\citep{wang2022pvt} backbone was embedded into the SINetV2~\citep{fan2021concealed} framework. The SINetV2 framework with camouflaged object detection was used for better identification ability, as polyp segmentation is a downstream task. Additionally, extensive experiments have been conducted to study the effectiveness of DMF, and it was found that the method performs well under different data distributions, making it a favorable solution for problems where the training dataset had a different distribution of negative samples compared to the testing dataset.

\textbf{Polypixel}: Team Polypixel~\citep{Tzavara2021} presented a study in which they used both pretrained and non-pretrained segmentation models for the polyp and instrument segmentation task. The team trained and validated both models on the dataset. The model architectures were retrieved from a Python library, ``Segmentation Models"~\url{https://github.com/qubvel/segmentation_models}, that contained different CNN architectures. This library offered models with both untrained and pre-trained weights, which were trained on the ImageNet dataset. To find the optimal fit for their datasets, they experimented and tested their results using EfficientNet, MobileNet, SE-ResNet, Inception, ResNet, and VGG. They achieved the best results with EfficientNetB1 for the polyp segmentation task. 


\textbf{agaldran}: Team agaldran~\citep{galdran2021polyp} utilized a double encoder-decoder structure for polyp and instrument segmentation, which consists of two U-Net like structures arranged sequentially as shown in Figure~\ref{fig:bestmethodmedAI2021}. The first encoder-decoder network processes the original image and produces output fed into the second encoder-decoder network. According to the authors, this setup allows the first network to highlight the important features of the image for segmentation, while the second network further improves the predictions of the first network. {For the architectural design of a double encoder-decoder network, they incorporate Feature Pyramid Network (FPN)~\citep{lin2017feature} architecture as the decoder mechanism, along with Resnext101 that serves as the pretrained decoder~\citep{kolesnikov2020big}. This is done to optimize the feature extraction. To further refine the model's optimization process, they used Sharpness-Aware Minimization (SAM)  along with the ADAM optimizer~\citep{foret2020sharpness}.} The team employed a 4-fold cross-validation approach to train their models, training with four separate models and using temperature sharpening across the ensemble model to produce the final segmentation maps. 


% Figure environment removed


\textbf{TeamAIKitchen}: Team TeamAIKitchen~\citep{Keprate2021} presented a methodology for developing, fine-tuning, and analyzing a U-Net-based model for generating segmentation masks for the polyp segmentation task. { They modified the original U-Net architecture to extend it to work with less training samples and to generate the output mask of the same size as the input. ReLU activation function was used in the hidden layers. They further experimented with different batch sizes and selected 8 as the best. Same architecture was used for polyp and instrument segmentation with early stopping criteria.} %The evaluation using the unseen testing dataset resulted in an IOU of 0.29 and a DSC of 0.41 for the polyp segmentation task. 



\textbf{CamAI}: Team CamAI~\citep{Yeung2021} presented a deep learning pipeline that is specifically developed to accurately segment colorectal polyps and various instruments used during endoscopic procedures. To improve transparency and interpretability, the pipeline leveraged the Attention U-Net architecture, which enables visualization of the attention coefficients to identify the most salient regions of the input images. This allowed for a better understanding of the model's decision-making process and facilitated the identification of potential errors. To further improve performance, the pipeline incorporated transfer learning using a pre-trained encoder. Additionally, test-time augmentation, softmax averaging, softmax thresholding and connected component labeling were used to further refine predictions and boost performance.

\textbf{OXGastroVision}: Team OXGastroVision~\citep{ali2021iterative} presented a novel solution that utilizes two state-of-the-art deep learning models, namely the iterative FANet~\citep{tomar2022fanet} architecture and DDANet~\citep{tomar2021ddanet}. The FANet is based on a feedback attention network that allows rectifying predictions iteratively. It consists of four encoder and four decoder layers. Similarly, DDANet is based on a dual decoder attention network with one shared encoder at each layer. While the iterative mechanism in the full FANet architecture can lead to larger computational time, DDANet has real-time performance (70 FPS) but sub-optimal output. To overcome these limitations, the team proposes to use the segmentation maps from the DDANet output as input for the FANet iterative network for pruning. This approach aims to achieve a balance between computational efficiency and segmentation accuracy.


\textbf{Vyobotics}: Team Vyobotics~\citep{Rauniyar2021}  presented a solution based on dual decoder attention network (DDANet)~\citep{tomar2021ddanet}, a deep learning model that has been specifically designed to achieve decent performance and real-time speed. The team performed data augmentation and trained a smaller network. This smaller network has a lower number of trainable parameters, which resulted in lower GPU training time. The ultimate goal of this approach was to achieve decent evaluation metrics while maintaining a decent FPS speed, which is crucial for real-time applications.

\textbf{NAAMII}: The team participated in polyp and instrument segmentation tasks. They employed $U^2Net$~\citep{qin2020u2} as the base network. They added a separate learnable CNN network on the decoder part of the U2Net to regress the HoG features of the input images. The output from each decoder block was fed into the HoG regressor and learned the parameters to predict the HoG correctly. They jointly minimized Mean Squared Error (MSE) loss for HoG features and CrossEntropy loss for Segmentation. {However, they only submitted their method description to the organizer and did not publish it as a research paper.}

\input{tables/Summary_Team_Results}
\input{tables/Summary_Team_Efficiency}


\section{Results}
In this section, we present a summary of the evaluated results obtained on the test dataset by all the participating teams in the two challenges: ``Medico 2020” and ``MedAI 2021”. Each challenge consists of tasks with a specific focus and evaluation metrics. There were two tasks for the Medico 2020 challenge, namely \textit{polyp segmentation} and \textit{algorithm efficiency} tasks. In the MedAI 2021, there were three tasks, namely \textit{polyp segmentation}, \textit{endoscopic accessory instrument segmentation} and \textit{transparency task}. The teams were evaluated based on standard evaluation metrics such as mIoU, DSC, Rec, Pre, Acc, F1, F2, and FPS. {We emphasized mIoU, DSC, and FPS more, whereas we also acknowledge the importance of recall and precision as they are useful metrics in clinical settings.} We have highlighted the best and the second-best scores in boldface and red color, respectively, for all the tasks in the two challenges.


\subsection{Medico 2020 results}
\subsubsection{{Polyp segmentation task}}
% Figure environment removed

In Table~\ref{table:medicochallengeresults}, we provide the results for the \textit{polyp segmentation} task. It can be observed that Team ``PRML2020GU'' outperforms other participating teams in the polyp segmentation task. It achieves a mIoU of 0.7897, DSC of 0.8607, recall of 0.9031, precision of 0.8673, and F2 of 0.8748. Team ``HBKU\_UNITN\_SIMULA'' was the second best performing team with mIoU of 0.7773. similarly, ``AI-TCE'' was the third best performing team with mIoU of 0.7773. The best-performing team, ``PRML2020GU,'' used an encoder-decoder structure with EfficientNet as the backbone and a U-Net decoder with channel-spatial attention and deep supervision. This architecture had an improvement of 1.23\% and 1.30\% over the mIoU and DSC achieved by the Team ``HBKU\_UNITN\_SIMULA'', which used an average of three PraNet and five ResUNet++ trained on different training and validation datasets. 

%This architecture had an improvement of 1.24% and 1.31% over the mIoU and DSC achieved by the Team “AI-TCE”, which used PraNet and Res2Net backbone

%and Table~\ref{table:Algorithm_efficiency_task}
%and \textit{algorithm efficiency} task for the challenge ``Medico 2020'' %A total number of 17 teams participated in the first task and 9 teams participated in the second task. The teams were ranked based on the mIoU metric for the first task, and for the second task, the teams were ranked based on the FPS metric.


\subsubsection{{Algorithm efficiency task}}
For the second task, as in Table~\ref{table:Algorithm_efficiency_task}, team ``PRML2020GU'' has poor speed performance with a processing speed of only 2.25 fps, which is not desirable for a real-time efficient model. An interesting observation is that Team ``GeorgeBatch'' outperforms other participating teams in the algorithm efficiency task with a processing speed of 196.79 fps, as seen from Table~\ref{table:Algorithm_efficiency_task}. However, it is worth noting that the team obtained a low mIoU of 0.6351 for the polyp segmentation task, even though we are considering it as the winner in this task. {Team ``UNITRK" obtained a second-best fps of 116.79. Similarly, team ``NKT" obtained a balanced mIoU of 0.6847 and a high speed of 80.60 fps, and was ranked third for this task. Despite the two teams, ``UNITRK" and ``GeorgeBatch'', achieving the highest evaluation fps values, there is a trade-off between speed and mIoU.} Low FPS cannot be used for real-time medical processing applications, and low overlap evaluation metrics cannot generate precise segmentation masks. To provide further insight, we have included the qualitative results of all the teams participating in the Medico 2020 challenge in Figure~\ref{fig:medico2020results}. We can see that none of the teams came close to the ground truth mask. Achieving a balance between these metrics is crucial for developing an efficient polyp segmentation model. 

 

\subsection{MedAI 2021 challenge results}




% Figure environment removed

% Figure environment removed

\subsubsection{{Polyp Segmentation Task}}

In Tables~\ref{table:medai}, we tabulated the evaluation results of all the participating teams in MedAI 2021 for  polyp segmentation task. From Table, it can be observed that team ``agaldran” outperforms other teams in the polyp segmentation task with mIoU of 0.8522, {and}  DSC of 0.8965. Team {``CV\&Med IIAI”} also showed good performance and was ranked $2^{nd}$ in the polyp segmentation task with {a mIoU of 0.8484}, a very small difference from the best-performing team. In Figure~\ref{fig:medaipolypchallenge2011}, we present the qualitative results of the participating teams for the polyp segmentation task of MedAI~2021. None of the methods performed well on this challenging image, emphasizing the need for more robust polyp segmentation methods. However, in the overall test set, the predicted segmentation masks from most of the team performed well on regular polyps (see Supplementary materials). Overall, the qualitative masks produced by teams ``agaldran”  and {``CV\&Med IIAI”} were better as compared to the other teams. 

%and~\ref{table:medai2}

%A total of 15 teams participated in both tasks of the MedAI2021 challenge. However, two teams, namely``Vyobotics~\citep{Rauniyar2021}” and ``MedSeg\_JU~\citep{Banik2021}” participated in only one task. Almost all the teams have used the same architecture for both tasks. The participating teams for the two tasks were ranked based on the mIoU metric.
%, recall of 0.9009 and precision of 0.9242

%We present the results of the accessory instrument segmentation task in Table~\ref{table:medai2}. 



\input{tables/Table_medai1.tex}
\input{tables/Table_medai2.tex}

\subsubsection{{Instrument Segmentation Task}}

From Table~\ref{table:medai2}, it can be observed that the same team, ``agaldran” also outperforms other participating teams in the instrument segmentation task with a high mIoU of 0.9364 and DSC of 0.9635.  Team ``NYCity” was ranked $2^{nd}$ in this task with a mIoU of 0.9326 and DSC of 0.9586. However, Team ``NYCity” obtained the highest recall of 0.9712, which signifies it has low false negative (FN) regions in the predicted segmentation mask compared to team ``agaldran”. Another interesting observation is the team ``agaldran” also achieved higher metric values for the instrument segmentation task as compared to the polyp segmentation task, as instrument segmentation is relatively easier than polyp extraction due to the greater variability of the latter regarding color and appearance. In Figure~\ref{fig:medAI_ins_2021}, we also present the qualitative results of the research teams who participated in the instrument segmentation challenge of MedAI2021. From the qualitative results, it can be observed that the ground truth prediction made by team ``agaldran” is also superior to the other team.  Therefore, it can be concluded from the obtained evaluation metrics for the two tasks that team ``agaldran” proposed a more robust algorithm and was accurately able to segment polyp and instrument at high accuracy.


\begin{table*} [!t]
\caption{Evaluation of the \textbf{`Transparency tasks'} for MedAI 2021 Challenge. For this task, a team of experts accessed the submission based on several criteria and provided a score based on the availability and quality of the source code (for e.g., open access, public availability, and documentation for reproducibility), model evaluation (for e.g., failure analysis, ablation study, explainability, and metrics used) and qualitative evaluation from clinical experts (e.g., usefulness and understandability of the results). {Here, `0' refers to no submissions for the transparency task. Doctor evaluation was only calculated for the team which manuscript were accepted. }}
\label{tab:transscore}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{|l|ccc|cccc|ll|c|}
\hline
\multicolumn{1}{|c|}{}  & \multicolumn{3}{c}{\textbf{Open Source}}  & \multicolumn{4}{|c}{\textbf{Model Evaluation}}  & \multicolumn{2}{|c|}{\textbf{Doctor Evaluation}}  & \\ \cline{2-10}
\multicolumn{1}{|c|}{\multirow{-2}{*}{\textbf{Team Name}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}l@{}}Publicly \\ available \\ (0 or 1) \end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}l@{}}Code \\ Quality \\ (0-3) \end{tabular}}} & \textbf{\begin{tabular}[c]{@{}l@{}}Readme \\ (0-3)\end{tabular}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}l@{}}Failure \\ Analysis \\ (0-3)\end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}l@{}}Ablation \\ Study \\ (0-3)\end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}l@{}}Explainability \\ (0-3) \end{tabular}}} & \textbf{\begin{tabular}[c]{@{}l@{}}Metrics \\ Used \\ (0 or 1)\end{tabular}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}l@{}}Usefulness \\ (0-3)\end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}l@{}}Understandable \\ (0-5)\end{tabular}}} & \multirow{-2}{*}{\textbf{Final Score}} \\ \hline


agaldran   & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{2} & 3 & \multicolumn{1}{c|}{3}    & \multicolumn{1}{c|}{3}  & \multicolumn{1}{c|}{3}   & 1 & \multicolumn{1}{c|}{2}  & \multicolumn{1}{c|}{3}  & \cellcolor[HTML]{57BB8A}21 \\ \hline


mTEC  & \multicolumn{1}{c|}{1}   & \multicolumn{1}{c|}{1}  & 3  & \multicolumn{1}{c|}{3}   & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{0}  & 1   & \multicolumn{1}{c|}{3} & \multicolumn{1}{c|}{4}  & \cellcolor[HTML]{7ECBA5}17 \\ \hline



CamAI  & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{1}  & 1   & \multicolumn{1}{c|}{2} & \multicolumn{1}{c|}{1}   & \multicolumn{1}{c|}{2}   & 1  & \multicolumn{1}{c|}{2}  & \multicolumn{1}{c|}{5}  & \cellcolor[HTML]{87CFAC}16 \\ \hline

The Arctic  & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{2}  & 1  & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{0}  & \multicolumn{1}{c|}{3}   & 1   & \multicolumn{1}{c|}{1}  & \multicolumn{1}{c|}{3}  & \cellcolor[HTML]{A4DBC0}13 \\ \hline

IIAI-CV\&Med   & \multicolumn{1}{c|}{1}  & \multicolumn{1}{c|}{1}  & 2  & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0}  & \multicolumn{1}{c|}{0} & 1     & \multicolumn{1}{c|}{1}                  & \multicolumn{1}{c|}{4}   & \cellcolor[HTML]{C1E6D4}10 \\ \hline

Polypixel  & \multicolumn{1}{c|}{1}  & \multicolumn{1}{c|}{1}  & 2  & \multicolumn{1}{c|}{0}  & \multicolumn{1}{c|}{0}  & \multicolumn{1}{c|}{0}  & 1 & \multicolumn{1}{c|}{0}   &\multicolumn{1}{c|}{0}  & \cellcolor[HTML]{F1FAF5}5   \\ \hline




leen  & \multicolumn{1}{c|}{0}  & \multicolumn{1}{c|}{1}  & 0  & \multicolumn{1}{c|}{0}  & \multicolumn{1}{c|}{0}   & \multicolumn{1}{c|}{2}  & 1  &\multicolumn{1}{c|}{0} &\multicolumn{1}{c|}{0} & \cellcolor[HTML]{FBFEFC}4  \\ \hline


MAHUNM  & \multicolumn{1}{c|}{1} & \multicolumn{1}{c|}{1} & 0  & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & 1 & \multicolumn{1}{c|}{0}  &\multicolumn{1}{c|}{0}   & \cellcolor[HTML]{FBECEB}3  \\ \hline

%NAAMII  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &            & \cellcolor[HTML]{E67C73}0   \\ \hline

OXGastroVision  & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{2} & 0 & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0}  & 1  & \multicolumn{1}{c|}{0} &    \multicolumn{1}{c|}{0}                  & \cellcolor[HTML]{FBECEB}3                                      \\ \hline

CV\&Med IIAI  & \multicolumn{1}{c|}{0}    & \multicolumn{1}{c|}{1}  & 0  & \multicolumn{1}{c|}{1}  & \multicolumn{1}{c|}{0}  & \multicolumn{1}{c|}{0} & 1  & \multicolumn{1}{c|}{0}  & \multicolumn{1}{c|}{0}          & \cellcolor[HTML]{FBECEB}3  \\ \hline



PRML  & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1}  & 0   & \multicolumn{1}{c|}{0}  & \multicolumn{1}{c|}{0}  & \multicolumn{1}{c|}{0} & 1 & \multicolumn{1}{c|}{0} &\multicolumn{1}{c|}{0}   & \cellcolor[HTML]{F4C6C3}2  \\ \hline



TeamAIKitchen  & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{1}  & 0  & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0} & 1 & \multicolumn{1}{c|}{0}  &  \multicolumn{1}{c|}{0}   & \cellcolor[HTML]{F4C6C3}2  \\ \hline

The Segmentors   & \multicolumn{1}{c|}{0}    & \multicolumn{1}{c|}{0}  & 0   & \multicolumn{1}{c|}{0}  & \multicolumn{1}{c|}{0} & \multicolumn{1}{c|}{0}    & 1    &\multicolumn{1}{c|}{0}       &\multicolumn{1}{c|}{0}   & \cellcolor[HTML]{FBFEFC}1      \\ \hline

NYCity   & \multicolumn{1}{c|}{0}   & \multicolumn{1}{c|}{0}  & 0 & \multicolumn{1}{c|}{0}  & \multicolumn{1}{c|}{0}  & \multicolumn{1}{c|}{0}  & 1  & \multicolumn{1}{c|}{0}  &         \multicolumn{1}{c|}{0}   & \cellcolor[HTML]{EDA19B}1  \\ \hline


%Vyobotics & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{}   & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}& \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{} &  & \cellcolor[HTML]{E67C73}0 \\ \hline
%{MedSeg\_JU} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{}   & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{}& \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{}  & \multicolumn{1}{l|}{} &  & \cellcolor[HTML]{E67C73}0 \\
\hline
\end{tabular}
}

\end{table*}



% Figure environment removed

% Figure environment removed




%A detailed score distribution under different criteria is shown in Table~\ref{tab:transscore}, which was part of our \textbf{\textit{Task 3 (transparency task)}}. Here, submissions that did not participate in {task 3 are not included in the table}. We have evaluated transparency tasks using a more quantitative approach compared to polyp and instrument segmentation. A multi-disciplinary team assessed each submission and evaluated the transparency and understandability of the proposed solutions. Each team was scored based on the three criteria: open source code, model evaluation and clinical evaluation. The open source code was evaluated based on the presence of a publicly available repository, code quality and quality of the readme file. The model evaluation included failure analysis, ablation study, explainability of the method, and metrics used.  Evaluation by clinical experts consider the usefulness of the method and its interpretability. With these three criteria, we aim to measure the transparency of the provided solutions.

\subsubsection{{Transparency Task}}
We present the transparency results in Table~\ref{tab:transscore}. Team ``agaldran'' outperformed other competitors with a final score of 21 out of 25. Similarly,  ``mTEC'' obtained a score of 17 out of 25 and was ranked $2^{nd}$. Likewise, team ``CamAI'' obtained a score of 16 out of 25 and was ranked third in the transparency task. There were also efforts from teams such as ``The Arctic'', which obtained a score of 13, and ``IIAI-CV\&Med'', which obtained a score of 10. These scores show their effort to provide a transparent solution to the polyp and instrument segmentation tasks. We provide the final ranking and task-wise scores in Figure~\ref{fig:medaipolypchallenge2021}. Notably, team \textit{\textbf{``agaldran''}} outperformed others in all three tasks and overall challenge and emerged as the winner of the MedAI 2021 challenge. Overall, \textit{\textbf{``mTec''}} secured the second position. Following closely behind, \textit{\textbf{``CamAI''}} showcased the third-best solution. {The overall rank was computed by combining the mIoU scores of polyp and instrument segmentation tasks and the Transparency score.} 


Figure~\ref{fig:avg_score} illustrates the plot of {mIoU} reported by each team in their submissions in the two challenges with three different tasks. It can be observed that the \textit{polyp segmentation task} from 2020 to 2021 gained improvement with a larger number of submissions achieving a {mIoU of more than 0.80 and the best-performing team with a mIoU of around 0.85}. Similar progress can be observed in Figure~\ref{fig:avg_graph} where an overall {mIoU increased by 4.93\%  when an average score is computed over all participating teams' individual best mIoU in the 2021 polyp segmentation challenge.} We further compared all segmentation metrics, including DSC, recall, precision, mIoU score, accuracy, and F2 score, as shown in Figure~\ref{fig:comparison_graph}.  Notably, the different evaluation metrics scores are consistent with instrument segmentation tasks in the MedAI challenge. However, there is a high variation in the mIoU between the different teams in the polyp segmentation tasks of Medico 2020 and MedAI 2021 challenges. 


These values pertain to the best score corresponding to a particular metric the individual team obtained in different executions. It is to be noted that each team was given the opportunity to submit five different submissions, and the best results for the best submission are reported in the Tables here. From here, it can be observed that most teams in the MedAI 2021 challenge reported overall high scores in terms of various segmentation metrics when compared to Medico 2020 outcomes, thus highlighting the improved performance trends in automated systems over time. Furthermore, it can also be visualized that unlike the high variations shown by teams' scores in the polyp segmentation task, better performance and smaller deviations in scores are reported in the instrument segmentation task. The high variations in the polyp segmentation results also show that polyp segmentation is more challenging because of the presence of variations in the size, structure and appearance of the polyps, and the presence of the artifacts and lighting conditions deteriorate the algorithm's performance. 


%\section{Overall score for the MedAI 2021 challenge.}



\section{Discussions}
The rapid advancement in the AI-based techniques that support CADe and CADx systems has resulted in the introduction of numerous algorithms in the domain of medical image analysis, including colonoscopy. To assess the performance of these algorithms, it is important to benchmark on the particular set of datasets. It enables the comparison and analysis of different techniques and assists in identifying challenging cases that need to be targeted using improved methodologies. This also includes cases that are misled by the presence of artifacts and occlusion by surgical instruments~\citep{ali2020objective}. Besides developing and analyzing AI-based algorithms, it is crucial to include explainability and interpretability to infuse trust and reliance during the adoption of automated systems in clinical settings. Therefore, the challenges discussed in this paper not only focus on lesion and instrument segmentation but also emphasize the importance of transparency in medical image analysis. This section covers the findings, { limitations, analysis of failing cases, trust, safety and interpretability of the methods, future steps and strategies covering both challenges}, Medico 2020 and {MedAI 2021}.



\subsection{Medico 2020 challenge methods} %Polyp segmentation
 Most of the methods reported in the Medico 2020 challenge focus on encoder-decoder architecture {(for example, U-Net, ResUNet++, PraNet, Efficient UNet, etc)}. Other networks used include {conditional GAN and Faster R-CNN}. The overview of the methods is provided in Table~\ref{table:challenge_summary2020}. For more detailed architectural information, we have also included the backbone and algorithm used by each team. Further, we also report the nature of the algorithm and the choice basis of evaluation, such as mIoU, DSC or FPS. Additionally, we provide information about the augmentation and hyperparameters, such as loss function and optimizers. {It is noteworthy that all the top three teams ``PRML2020GU", ``HBKU\_UNITN\_SIMULA" and ``AI-TCE"  used the encoder-decoder architecture}. Out of 17 participating teams, only three teams adopted some other architectures. Comparative analysis shows that the highest-scoring encoder-decoder network outperforms the GAN-based approach by a significant margin of 0.3517 in mIoU and 0.2989 in DSC score. Similarly, compared to the R-CNN-inspired networks (team ``IRIS-NSYSU''), the best approach (team ``PRML2020GU'') achieves an improvement of 0.2863 in mIoU score and 0.2191 DSC score.


Medico 2020 challenges provide valuable insight and trends for the {polyp segmentation and biomedical image analysis challenges. Most deep learning frameworks submitted for the challenge used the Adam optimizer to optimize their network. However, a handful of teams used other optimizers, such as SGD or RMSProp. Additionally, most of the teams used data augmentation to boost the number of training samples prior to training their frameworks to improve the performance of their architecture. There have been different preferences in loss function where most of the team used ``BCE + DSC loss", ``binary cross-entropy,"  IoU loss, etc. However, from the results of the top three teams, it can be concluded that ``BCE + DSC loss" is best for this dataset. Similarly, in terms of the backbone for the model architecture, the EfficientNet variant (selected by PRML2020GU) or EfficientNetB4 (selected by AI-TCE) were most favorable.} 



%All the participants in both challenges used deep learning-based frameworks for segmentation tasks. The findings from Medico 2020 and MedAI 2021 challenges provide valuable insight and trends for the current biomedical image analysis challenge.  It can be observed that most of the deep learning frameworks submitted for the challenge used Adam optimizer for optimizing their network. However, a handful of teams used other optimizers such as stochastic gradient descent (SGD) and RMSProp. We also observed that most of the deep learning frameworks used by participants in both challenges used the encoder-decoder framework as the backbone network.  For the MedAI 2021 challenge, excluding two teams, all the others participated in both polyp and instrument segmentation tasks and used the same framework.   


\subsection{MedAI 2021 challenge methods}
The summary of the different approaches adopted by the participating teams of the MedAI2021 Challenge is presented in Table~\ref{table:challenge_summary2021}. To provide a brief overview of the general techniques adopted by the different teams, they can be categorized based on the nature of the approach followed, such as ensemble models, encoder-decoder based architectures, CNN, and hybrid CNN models. Almost all the teams presented the same model for both the tasks proposed in the challenge. Most teams explored ensemble modeling, encoder-decoder networks, or a combination of both in the polyp segmentation task. Another criterion of categorization could be CNN or transformed-based approaches. It is observed that the top-ranked team ``agaldran'' utilized two encoder-decoder networks and reported a mIoU score of 0.8522. {Similarly, {``CV\&Med IIAI''} was ranked second, and Team ``NYCity" was ranked third in the polyp segmentation task with a competitive mIoU value of 0.8484 and 0.8418, respectively. Similar to the Medico 2020 polyp segmentation challenge, where GAN-based methods were adopted by teams (for example, Team ``leen") failed to perform well in this challenge for polyp and instrument segmentation tasks. It is to be noted that the winning team, ``agaldran'' used a double encoder-decoder structure with two U-Net, where they incorporated FPN and Resnext101 as the pretrained decoder. They also use SAM and Adam optimizer to optimize the model further. The other competitive team {``CV\&Med IIAI''} used the SINetv2 algorithm with PVTv2 as the backbone, and NYCity used the combination of HarDNet-85 ResNet101.}


In the MedAI2021 instrument challenge, participants mainly focused on either ensemble models or encoder-decoder networks similar to the polyp segmentation task. As the majority of the teams utilized the same model that they proposed for the polyp segmentation problem in this task, the categorization of overall methods remains the same as that of the first task described above. The top rank is secured by Team ``agaldran'', with encoder-decoder architecture, pyramid network as the decoder, and Resnext101 as the pre-trained decoder. The second-ranked model by Team ``NYCity'' is the CNN and transformer based ensemble model, which achieved only a slight difference in the scores from the leading model. {mTec was ranked third in the challenge, which used dual parallel reverse attention edge network (DPRA-EdgeNet)~\citep{bhattacharya_betz_eggert_schlaefer_2021}. The architecture used HardNet~\citep{chao2019hardnet} as the backbone.}

The challenge shows that most of the teams were reluctant to share their method (refer to Table~\ref{tab:transscore}). {From the table, it can be seen that only five teams were qualified for the doctor evaluation.} Additionally, the quality of the code submitted by most of the team was not satisfactory. Most of the participants did not put much effort into the readme file. Additionally, most teams neglected the failure analysis, ablation study and explainability in their submission. {Moreover, based on the doctor's evaluation, only the solution provided by a few teams (for example, ``agaldran'', ``mTEC''  ``CamAI'',  ``The Arctic,"  and ``IIAI-CV\&Med'') was considered useful and understandable.} 

%This approach is observed to be a close competitor to the best performer, with a slight difference of 0.90\% in accuracy and 0.38\% in the mIoU score. Also, this GAN-based approach performed better than the best method in terms of dice score and recall. The next close competitor in the list is a CNN and transformer based ensemble model.

%The findings from Medico 2020 and MedAI 2021 challenges provide valuable insight and trends for the current biomedical image analysis challenge. All the participants in both challenges used deep learning-based frameworks for segmentation tasks. It can be observed that most of the deep learning frameworks submitted for the challenge used Adam optimizer for optimizing their network. However, a handful of teams used other optimizers such as stochastic gradient descent (SGD) and RMSProp. We also observed that most of the deep learning frameworks used by participants in both challenges used the encoder-decoder framework as the backbone network. Additionally, most of the teams used data augmentation to boost the number of training samples prior to training their frameworks to improve the performance of their architecture. For the MedAI 2021 challenge, excluding two teams, all the others participated in both polyp and instrument segmentation tasks and used the same framework.   

%NOTE: This is good! 
% For reference see here: https://www.sciencedirect.com/science/article/pii/S1361841521000487?via%3Dihub#sec0023

\subsection{Analysis of the failed cases}
We have analyzed the regular and failing cases in polyp and surgical tool segmentation to highlight the limitations of the current methods so that these cases can be considered during further algorithm development. Figure~\ref{fig:medico2020results} and Figure~\ref{fig:medaipolypchallenge2011} show examples of instances where the models fail for most cases. From the results on the test dataset, it was observed that most of the algorithms failed on diminutive and flat polyps located in the left colon. These are the challenging classes in the colon and require effective detection and diagnosis system. Similarly, although most of the methods performed well on the diagnostic and therapeutic surgical tool, there were issues with the images having caps and forceps. Similarly, the performance on the challenging images for polyps and instruments (see Figures~\ref{fig:medico2020results}, \ref{fig:medaipolypchallenge2011}, ~\ref{fig:medAI_ins_2021} {\textbf{and supplementary material}}) {as algorithms could still struggle with difficult and rare cases like sessile polyps, even if they perform well on overall quantitative metrics. Therefore, investigating the cause for misclassification for such samples in the dataset and failure analysis will be critical to focus for future research. This can include generalization performance evaluation on unseen test data from different hospitals. Such investigations can reduce the chances of underperformance on rare cases.} 




\subsection{Trust, safety, and interpretability of methods}
Integrating CADe or CADx in clinical settings necessitates addressing factors such as trust, safety, and interpretability to ensure its adoption. The {high variations and potential bias} in the curated datasets used to train such models and the actual scenarios in which they are adopted create a high chance of biases, impacting the generalizability of the method. Such bias ultimately makes it challenging to infuse trust while adopting CADe or CADx tools and questions the safety of patients. {To tackle this issue, we introduced a transparency task in the MedAI2021 challenge that underscores the need for interpretability, reproducibility, and explainability in medical AI research, including polyp and instrument segmentation.}


{Our initiative aimed to light the potential risk that can arise from wrong decisions based on model and algorithmic bias. Our dataset contained polyp cases with varied appearances in terms of shapes, sizes, the presence of artifacts, lightning conditions, textures, and the different numbers of polyps per image that are encountered in real-world clinical settings. Additionally, we have included frames containing surgical instruments to support the cases of occluded endoluminal elements or polyps that could arise in general. Some of the methods adopted by the participating teams include the submission of intermediate heatmaps using approaches like layer-wise relevance propagation that showed visual explanation and highlighted the model decision-making process. Team ``agaldran"  provided detailed ablation studies in support of the predictions obtained. By promoting transparency through subjective analysis and addressing potential biases, the MedAI challenge aimed to foster trust in the presented solution and ensure safety in adopting such methods in the clinic.}

\subsection{Limitation of the Medico 2020 and MedAI 2021}
In our study, we aimed to standardize the challenge of polyp and instrument segmentation by providing the same test sets and evaluation metrics to all participants. To achieve this, we introduced variable polyp cases, including polyps with different sizes, noisy frames with artifacts, blurry images, and occlusion. We also added regular frames to the test set to ensure that participants drew the ground truth manually and did not cheat. However, our study has some limitations. Although we used datasets collected from four medical centers in Norway, these images are from a single country, limiting the ethnicity variance though there is very limited differences if any in the mucosal appearance between ethnicities. Nevertheless, there is a need for a more diverse dataset that includes multiple ethnicities and countries also because the prevalence of various diseases varies between regions. Moreover, the current models should be tested on multi-center datasets to assess their generalization ability.

There was no online leaderboard in our challenge due to the Mediaeval policy. Therefore, we manually calculated the predictions submitted by each team. Each team had limitations of 5 submissions for each task, which restricted further optimization opportunities. Although we have also introduced normal findings from the GI tract to trick the participants and models, our challenge only used still frames and did not incorporate video sequence datasets. {Even when the best performing algorithms are tested on a temporal video sequence dataset, it is possible that the performance can drop.} Most of the images are only from white light imaging. Although our dataset was annotated by one annotator and checked by two gastroenterologists, there is still a possibility of bias in the labels. In the accessory instrument challenge, we had more images from the stomach class than accessory instruments such as biopsy forceps or snares due to the lack of availability of datasets. Finally, despite including diverse cases in the polyp and instrument segmentation challenge, we still had limited flat and sessile polyps, frequently missed during routine colonoscopy examinations. Incorporating multi-center data, video sequences data and addressing label biases will lead to more comprehensive and reliable evaluations of AI-based colonoscopy systems.


%Therefore, investigating the cause for misclassification for each sample in the dataset and failure analysis will be critical to focus for future research.}


\subsection{Future steps and strategies}
In our study, we aimed to promote transparency and interpretability in machine learning models for the GI tract setting. However, more work is needed to understand how decisions are made and identify potential biases or errors in a quantitative manner to build trust in such systems in a clinical setting. To achieve this, we plan to test the best-performing algorithms on large-scale datasets to observe their scalability. {We will consider using more quantitative metrics, such as statistical mixed models, bootstrapping analysis and estimate confidence intervals.} Additionally, we will also include metrics such as Hausdorff distance and normalized surface distance.  %Furthermore, we will do a reliability test on the best-performing methods on easy, medium, and challenging real-world cases. % (examples shown in supplementary material). We will organize a competition on multi-center datasets, including images from various modalities such as Flexible spectral Imaging Color Enhancement (FICE) and Blue laser imaging (BLI), and challenging cases.

We will emphasize more transparent decision-making methods and visualize interpretability results while focusing on clinical relevance rated by expert clinicians instead of just one objective metric. To achieve this, we have already started collecting large-scale datasets and plan to build a tool if the algorithms are robust enough and verified by our gastroenterologists. {Next, we will propose a challenge to polyp video sequences analysis. We will explore the integration of state space models, such as Video Vision Mamba-based framework~\citep{yang2024vivim}, to capture the temporal information in video sequences that affect the efficiency and accuracy of segmentation tasks. It is worth noting that there has been innovation within hardware (colonoscope) for safer medical colonoscopy devices, such as developing fully flexible automated colonoscopes to offer expanded fields of view rather than 120-170° visualization, which can capture dead spots, improving the lesions' miss-rate. These scopes are currently in the final stage of development. This hardware would require high processing speed to locate potential lesions in real time for a smooth workflow. We believe these solutions from our challenge could help address the complexities with the improved hardware and improved image quality.}


%, we have organized Medical Visual Question Answering for GI Task - MEDVQA-GI challenge in 2023~\citep{ionescu2023overview}.  

%~\footnote{https://www.imageclef.org/2023/medical/vqa}
\vspace{-3mm}

\section{Conclusion}
\label{section:conclusion}
Our study aimed to provide a comprehensive analysis of the methods used by participants in the Medico 2020 and MedAI 2021 competitions for different medical image analysis tasks. We designed the tasks and datasets to demonstrate that the best-performing approaches were relatively robust and efficient for automatic polyp and instrument segmentation. We evaluated the challenge based on several standard metrics. In MedAI 2021, we also used a quantitative approach, where a multi-disciplinary team, including gastroenterologists, accessed each submission and evaluated the usefulness and understandability of their results. {Through the qualitative results, we found that even the best-performing method underperforms in rare cases. This highlights the need for further investigation to understand the cause of misclassification.} During the ``performance task'' and ``algorithm efficiency'' tasks, we observed a trade-off between mIoU and inference time when tested across unseen still frames. {For the instrument segmentation challenge, we observed that almost all teams performed relatively well, as segmenting instruments is easier than polyp segmentation.} From the transparency task, we observed that more effort is required from the community to enhance the transparency of the proposed model. Overall, we also observed that several teams demonstrated the use of data augmentation and optimization techniques to improve performance on specific tasks. Our study highlights the need for multi-center dataset collection from larger and more diverse populations, including experts from various clinics worldwide. {More competitions should be held on polyp video sequences to observe the efficiency difference in still frames and video sequences.} Further research should investigate multiple polyp classes that typically fail in clinical settings, multi-center clinical trials, and the emphasis on real-time systems. Additionally, research on transparency and interpretability should be emphasized as it could help build clinically relevant and trustworthy systems.


\section*{Acknowledgment}
D. Jha is supported by the NIH funding: R01-CA246704 and R01-CA240639. V. Sharma is supported by the INSPIRE fellowship (IF190362), DST, Govt. of India. D. Bhattacharya is funded partially by the i$^3$ initiative of the Hamburg University of Technology and by the Free and Hanseatic City of Hamburg (Interdisciplinary Graduate School “Innovative Technologies in Cancer Diagnostics and Therapy”). K. Roy is thankful to DST Inspire Ph.D fellowship (IF170366).

\section*{Authors contribution}
D. Jha conceptualized, initiated, and coordinated the work. He also led the data collection, curation, and annotation processes for Medico 2020 and evaluated the Medico 2020 Challenge. S. Hicks and M.A. Riegler initiated the MedAI 2021 Challenge, organized the data collection together with D. Jha and conducted all evaluations for the challenges, organized the reviews and coordinated with all the authors. V. Sharma analyzed the results and prepared most graphs for technical validation along with N. K. Tomar. She also wrote a part of the results and discussion. D. Banik, D. Bhattacharya and  K. Roy wrote part of the introduction, related work, and participants' methods and provided subsequent feedback on the method's tables. M.A. Riegler and P. Halvorsen facilitated the data and organization for both competitions. Our gastroenterologists, T. de Lange and S. Parasa, reviewed the annotations and provided the required feedback during dataset preparation and evaluation. Challenge participants provided the method details for Medico 2020. All authors read the manuscript, provided substantial input, and agreed to the submission.

\bibliographystyle{vendor/model2-names.bst}\biboptions{authoryear}
\bibliography{refs}
\end{document}