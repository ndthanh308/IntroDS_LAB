\section{Introduction}

In the digital world, the credibility of the famous saying ``seeing is believing"  is largely at risk since nowadays people can easily manipulate critical content within an image and redistribute the fabricated version via Online Social Networks (OSN).
% In many cases, attackers edit images to make them more eye-catching and impactful. 
Owing to the fact that readers are more susceptible to well-crafted misleading material, fabricating stories by image manipulation can be a means for some politicians to influence public opinion.
In more severe cases, those fraudulent images can be utilized as a supplement to fake news or criminal investigation. 

In the past decades, image manipulation detection has aroused extensive research interest to protect image authenticity as well as ownership.
The aim is to not only distinguish manipulated images from authentic ones but also locate the manipulated areas, therefore blocking the spreading of fake information and unveiling the purpose of forgery. 
The uprising of deep networks has greatly strengthened the capability of  representation learning to find traces left by manipulation~\cite{dong2021mvss,wu2019mantra,hu2020span}. 
% Figure environment removed
% Lossy image post-processing attacks downgrades the performance of passive image forgery detection schemes. Our RAW protection scheme realizes robust image forgery detection with the help of imperceptible signal injection.
% For example, 
% There are also many schemes for universal tampering detection~\cite{dong2021mvss,wu2019mantra,hu2020span} that exploit universal noise artifact left by manipulation.
% However, image forgery detection schemes are faced with enormous types of situations in the real world, where any critical object in an image can be removed, a.k.a., the \textit{inpainting} attack, or replaced with an image patch from the same image, a.k.a, the \textit{copy-move} attack.
% Or a non-existing object can be added, a.k.a., the \textit{splicing} attack.
However, attackers often perform a chain of image post-processing operations on the forged images to conceal their manipulation behavior.
Previous works are reported to have poor generalization against image post-processing or on JPEG-format images~\cite{SSIM,islam2020doa}, where the learned clues can be easily erased.
Though Wu et al.~\cite{RIML} proposed a Robust Image Manipulation Localization (RIML) scheme against lossy online transmission, its performances against well-crafted forgeries such as \textit{inpainting} and heavy post-processing attacks such as Gaussian blurring are still limited. 
The main difficulty of image manipulation detection is whether the subtle manipulation traces learned by forensics methods can always survive image post-processing operations in real-world applications.

In this paper, we rethink the way towards achieving more robust image manipulation detection. 
Instead of proposing a more advanced architecture or mechanism for finding \textit{irremovable} traces, we are interested in how to make the original image files ``\textit{read-only}". 
Inspired by the fact that natural RGB images are rendered from RAW data, we propose DRAW, a RAW protection scheme against image manipulation.
We introduce imperceptible protective signal into RAW files for protection, which can not only be transfered in the rendered RGB images but also survive possible image post-processing operations such as JPEG compression, rescaling or contrast enhancement. 
On the recipient's side, the detection networks can authenticate the images where the manipulated areas can be localized.
% In other words, DRAW adds digital \textit{locks} onto the pixels and image manipulation detection is conducted by observing the integrity of these \textit{locks}.
As shown in Fig~\ref{fig:teaser}, the performances of state-of-the-art image forgery detection schemes~\cite{dong2021mvss,RIML} are severely downgraded due to lossy OSN transmission. 
However, if the same attacks are performed on the images rendered from the protected RAWs, the manipulation localization results are largely improved.
Besides, a novel Multi-frequency Partial Fusion Network (MPF-Net) is proposed to implement RAW protection, which adopts frequency learning and cross-frequency partial feature fusion to largely lower the computational complexity.
% making it possible to be integrated into cameras in the future

% 1) Benefits and Motivation of RAW-domain protection.  
We also conducted experiments of image protection in the RGB domain in Section xx, and summarize
the benefits and motivation of RAW-domain protection as follows.
\highlight{\textbf{Quality.}} RAWs are the source of digital images with larger dynamic range.
Compared to RGB-domain protection, DRAW enjoys a much larger searching space that the RGBs rendered from the protected RAWs have higher fidelity \& sampling probabilistic density. 
% In the supplement, we show that we can also apply DRAW on existed RGBs by RGB2RAW reversion.
% RGB-domain image protection methods so far leave noticeable artifacts (PSNR\textless~35dB) that might fall short in practical application. 
% We explore image protection in the RAW domain and transfer the protection from RAW to RGB, with averaged PSNR\textgreater~40dB. 
% , and the forgery detection results on these protected RGBs are also promising.
% \highlight{2) Larger searching space.}
% RGBs are subject to the restriction of 8-bit integer decimal (255). 
% It can inspire futher works.
% \highlight{3) Promoting robustness via ISP}.
\highlight{\textbf{Robustness.}} 
% Apart from post-processing operations, DRAW requires the protective signal to successfully travel through different ISP pipelines and reside on the RGBs. 
% The demand of protective signal traveling through ISP adds randomness and improves robustness.
Apart from RGB-domain attacks, 
DRAW can additionally resist RAW-domain ISP-based post-processing.
Extensive experiments on several famous RAW datasets, e.g., RAISE, FiveK and SIDD, prove the robustness and transferability of our method against image manipulation.
We also verify that MPF-Net provides superior performance compared to classical U-Net architecture with only 20.9\% of its memory cost and 0.95\% of its parameters. 
\highlight{\textbf{Practicality.}} The lightweight MPF-Net is designed to be integrated into cameras for image protection in the shooting stage, thereby changing the current situation where digital images can be freely manipulated.
% The proposed method can be in the near future integrated into cameras, thereby changing the current situation where digital images can be freely manipulated.

The main contributions of this paper are as follows:
\begin{enumerate}
\item DRAW is the first to propose
RAW protection against image manipulation. The corresponding RGB images will carry imperceptible protective signal even though various types of imaging pipelines or image post-processing operations are applied. 
\item Comprehensive experiments prove that image forgery detection networks with the help of RAW protection can better resist lossy image operations such as JPEG compression, blurring and rescaling.
\item  A novel lightweight MPF-Net is proposed for integrating RAW protection into cameras in the future, thereby potentially changing the current situation where digital images can be freely manipulated.
% The architecture only requires 1/10 parameters of that in the traditional U-Net while the performances are better.
% while little visible difference between the protected and non-protected RGB pairs can be observed.
\end{enumerate}

\section{Proposed Method}
% We present the methodology of DRAW in Section 3.1, and the novel hierarchical partial network for network implementation is specified in Section 3.2. The loss functions are shown in Section 3.3 and more training details are enclosed \redmarker{in the supplement}.
\subsection{Approach}
Fig.~\ref{image_framework} depicts the pipeline of DRAW, which contains two trainable networks, i.e., a protection network $\mathcal{R}$ and a detection network $\mathcal{D}$.
We also include an image signal processing layer $\mathcal{S}$ for RAW rendering, as well as a hybrid attack layer $\mathcal{A}$ for image post-processing simulation.

\noindent\textbf{Protection Network.}
We denote the captured RAW data as $\mathbf{R}$, and use a protection network $\mathcal{P}$ to transform $\mathbf{R}$ into the protected RAW, i.e., $\hat{\mathbf{R}}$.
The functionality of $\mathcal{P}$ is to adaptively insert a protective signal into the RAW file for more robust and accurate image manipulation detection while minimizing the possible impact on the visual quality of $\hat{\mathbf{R}}$ and the corresponding rendered RGB images. 
Considering the computational limitation of existing imaging equipment, we implement $\mathcal{P}$ using our novel lightweight MPF-Net specified in Section~\ref{section_network}. 

\noindent\textbf{Image Signal Processing Layer.}
We use the ISP layer $\mathcal{S}$ to render $\hat{\mathbf{R}}$ into an RGB image $\hat{\mathbf{I}}$.
We include an off-the-shelf differentiable network to enable gradient flow, and a non-differentiable conventional ISP algorithm to enhance transferability. 
For diversifying the distribution of the output RGB, 
interpolation is conducted on the network-rendered RGB $\hat{\mathbf{I}}_{\emph{net}}$ and conventional algorithm generated RGB $\hat{\mathbf{I}}_{\emph{conv}}$ to produce $\hat{\mathbf{I}}$.
\begin{equation}
\label{eqn_mixup}
\hat{\mathbf{I}}=\omega\cdot\hat{\mathbf{I}}_{\emph{conv}}+(1-\omega)\cdot\hat{\mathbf{I}}_{\emph{net}},
\end{equation}
where $\omega\in[0,1]$.
We use the popular open-source LibRaw~\footnote{https://github.com/LibRaw/LibRaw} library as the conventional ISP algorithm.
% , which contains Bayer demosaicing, color balancing, white balancing, colorspace conversion, etc. 
As for deep-network-based ISP, we iteratively use CycleISP~\cite{zamir2020cycleisp} and InvISP~\cite{InvISP} as the surrogate model.

% Figure environment removed
% Figure environment removed
\noindent\textbf{Hybrid Attack Layer.}
The hybrid attack layer $\mathcal{A}$ includes both simulated and real-world attacks. 
We use three sub-layers for attack simulation, i.e., tampering, color adjustment and distortion.
Given the protected image $\hat{\mathbf{I}}$, we process it using different combinations of these sub-layers in arbitrary orders, subject to the activation possibilities specified in Fig.~\ref{image_framework} (b). 
We denote the manipulated images and authentic images respectively as $\hat{\mathbf{I}}_{\emph{t}}$ and $\hat{\mathbf{I}}_{\emph{nt}}$, where the purpose of including authentic images is to minimize the false alarm rate.
% \noindent\textbf{Color Adjustment and Image Distortion.}
% Many users would readjust the overall color using image augmentation such as brightening or contrast enhancing, and during data transmission, some details of the images might be altered due to format conversion or noise addition. 
% Though color adjustment and image-level distortion are less likely to cause misinformation, previous studies~\cite{dong2021mvss,wu2019mantra} have shown that they can easily erase the manipulation trace, and therefore weaken the performance of forgery detection.
% Accordingly, we train the scheme in the fashion of adversarial training. 
% We process $\mathbf{I}^{\emph{mani}}$ using a collection of typical data augmentation methods $\emph{Aug}(\cdot)$ as well as image-level distortion methods $\emph{Dis}(\cdot)$.
% Therefore, the post-processed image $\mathbf{I}^{\emph{post}}$ can be produced by one of the following: 
% $\mathbf{I}^{\emph{post}}=\emph{Aug}(\mathbf{I}^{\emph{mani}})$,
% $\mathbf{I}^{\emph{post}}=\emph{Dis}(\mathbf{I}^{\emph{mani}})$,
% $\mathbf{I}^{\emph{post}}=\emph{Aug}(\emph{Dis}(\mathbf{I}^{\emph{mani}}))$, or $\mathbf{I}^{\emph{post}}=\emph{Dis}(\emph{Aug}(\mathbf{I}^{\emph{mani}}))$. 
For image inpainting, we adopt two most recent schemes for simulation, which include ZITS~\cite{ZITS}
% ~\footnote{https://github.com/DQiaole/ZITS\_inpainting} 
and LAMA~\cite{LAMA}.
% ~\footnote{https://github.com/saic-mdal/lama}.
Other implementation details for $\mathcal{A}$ are specified \redmarker{in the supplement}.
% To encourage randomness during training, we arbitrarily shuffle the order of the two networks. The two modules can also be skipped subject to a skipping probability (15\%), respectively.
% Note that cropping can cause desynchronization in discovering the JPEG checkerboard artifact as well as the introduced signal by DRAW. 
% Thus, we opt to independently conduct image cropping attack apart from image distortion.
Additionally, we enable more realistic and diversified simulated results.
We perform real-world non-differentiable attacks with the same setting synced from the simulation and calculate the difference between $\mathbf{I}_{\emph{rw}}$ and $\mathbf{I}_{\emph{syn}}$, where $d=\mathbf{I}_{\emph{rw}}-\mathbf{I}_{\emph{syn}}$.
Then, we update the attacked image as $\mathbf{I}_{\emph{syn}}+d$, which is numerically identical to  $\mathbf{I}_{\emph{rw}}$ but with gradient flow.

 
% After performing the above attacks, we use a differentiable image quantization method \cite{bengio2013estimating} to convert the floating-point values of $\mathbf{X}$ to 8-bit integer and store the corresponding image into the disk in PNG or JPEG format. 

\noindent\textbf{Detection network.}
We use the detection network $\mathcal{D}$ to estimate the manipulated region.
For manipulated image $\mathbf{I}_{\emph{t}}$, $\hat{\mathbf{M}}_t$ should be close to the ground-truth $\mathbf{M}$.
For authentic image $\mathbf{I}_{\emph{nt}}$, $\hat{\mathbf{M}}_{\emph{nt}}$ should be close to a zero matrix $\mathbf{O}$ whose size is identical to that of $\mathbf{M}$.
We implement $\mathcal{D}$ using a popular HRNet~\cite{hrnet} architecture, while other image manipulation detectors~\cite{dong2021mvss,RIML} can also be applied.

\noindent\textbf{Loss Functions.}
% \label{section_loss}
% DRAW should not severely alter the contents of the original scene in sacrifice for anti-manipulation protection. 
We include two fidelity terms $\mathcal{L}_{\emph{r}}$ and $\mathcal{L}_{\emph{i}}$ to ensure imperceptible RAW protection using the $\ell_1$ distance.
\begin{equation}
\label{loss_protect_RAW}
\mathcal{L}_{\emph{r}}=\left|\mathbf{R}-\hat{\mathbf{R}}\right|_{1},
\mathcal{L}_{\emph{i}}=\left|\mathbf{I}-\hat{\mathbf{I}}\right|_{1}.
\end{equation}
% \begin{equation}
% \label{loss_precept}
% \mathcal{L}_{\emph{VGG}}=\sum_{i=1}^N \frac{1}{H'W'C'}\left|\phi_{\emph{pool}_i}-\hat{\phi}_{\emph{pool}_i}\right|_1,
% \end{equation}
% We also find that merely using the perceptual loss will also cause checkerboard artifact, as reported by \cite{distill}. 
% So we use a hyper-parameter $\alpha$ to carefully balance the two terms.
% In Eq.~(\ref{loss_precept}),

We include a localization term $\mathcal{L}_{\emph{c}}$ and a regularization term $\mathcal{L}_{e}$
to minimize the Binary Cross Entropy (BCE) loss between $[\hat{\mathbf{M}}_t,\mathbf{M}]$ and $[\hat{\mathbf{M}}_{\emph{nt}},\mathbf{O}]$.
% penalizes the networks if the estimation on non-tampered images are not zero matrices.
\begin{equation}
\begin{gathered}
\mathcal{L}_{c}=\emph{BCE}(\hat{\mathbf{M}},\mathbf{M}), 
\mathcal{L}_{e}=\emph{BCE}(\tilde{\mathbf{M}},\mathbf{O}), \\ 
\emph{BCE}(x,y)=-\sum_{i,j}\left(x_{i,j}\log{y_{i,j}} + \left(1-{x_{i,j}}\right)\log{(1-y_{i,j}})\right).
\end{gathered}
\end{equation}
In summary, the total loss for DRAW is shown in Eq.~(\ref{eqn_loss_sum}), where $\alpha, \beta, \gamma, \epsilon$ are hyper-parameters.
\begin{equation}
\label{eqn_loss_sum}
\mathcal{L}=\alpha\cdot\mathcal{L}_{r}+\beta\cdot\mathcal{L}_{\emph{i}}+\gamma\cdot\mathcal{L}_{\emph{c}}+\epsilon\cdot\mathcal{L}_{\emph{e}},
\end{equation}

\subsection{Multi-frequency Partial Fusion Network}
\label{section_network}
% The protection module is located within small devices such as cameras, and therefore $\mathcal{P}$ must be lightweight enough to be embedded. But it also needs to ensure that modifications made on images rendered by the generated protected RAW can be easily localized, regardless of what types of image post-processings or ISP methods are applied.
% Considering that the protection network must be lightweight and effective in RAW protection with transferable and robust forgery detection.
% Naturally, the network requires not only global information to determine which parts of the image contain different statistical characteristics, e.g., JPEG compression pattern, with those of the whole image, but also local information to preserve the contextual consistency of the image after RAW rendering.
% Since RAW protection happens within cameras, it requires that the network must efficiently extract both local, or contextual, and global, or holistic, information with limited amount of trainable parameters. 
% We find that many heuristical method such as DFT and wavelet transform, e.g., Haar Transform or DT-CWT, require very few computational resources for global or disentangled feature extraction and are compatible in mobile devices.
% Therefore, we propose a novel CNN architecture named MPFNet that jointly process the image in the combined DT-CWT and DFT domain and with partial feature fusion.

Fig.~\ref{image_hpn} illustrates the network design of MPF-Net.
The network first uses a three-level DT-CWT transform to decompose the input into a low-frequency image and three levels of higher-frequency subbands. 
Each level contains six subbands in complex forms that represent different degrees of wavelet information.
We concatenate the real and imaginary parts of the subbands as the input features of each level.
Fig.~\ref{DT-CWT_example} shows an example of a comparison of the U-Net feature pyramid and that of DT-CWT.
Vanilla convolutions usually suffer from multi-layer cascading and feature redundancy.
The introduction of DT-CWT can alleviate this issue which only requires one layer of separable convolution and the mined features show richer patterns. 
% transformation made on the input can be therefore represented as follows: $\mathbb{R}^{H\cdot~W\cdot~C} \rightarrow$ $[
%     \mathbb{R}^{\frac{H}{4}\cdot\frac{W}{4}\cdot~C},$
%     $\mathbb{R}^{\frac{H}{2}\cdot\frac{W}{2}\cdot~C\cdot~12},$
%     $\mathbb{R}^{\frac{H}{4}\cdot\frac{W}{4}\cdot~C\cdot~12},$
%     $\mathbb{R}^{\frac{H}{8}\cdot\frac{W}{8}\cdot~C\cdot~12}
%     ]$.
Next, the hierarchical features are respectively fed into independent ``DSConv-LN-GELU" layers, which is in short for depth-wise separable convolution, Layer Normalization~\cite{LN} and GELU activation~\cite{GELU}. 
% It projects the feature pyramid into refined features with the same amount of channels.

Afterward, we use several cascaded multi-frequency partial fusion blocks for further feature refinement and inter-frequency fusion. 
Each block contains a Half Fourier Convolution (HFC) layer and a Partial Feature Fusion (PFF) layer.
% In Fig.~\ref{image_hpn}, $F_{j}^1$ and $F_{j}^2$ respectively denote the output of HFC and PFF at $j_{\emph{th}}$ scale, and the subscripts for block number are omitted for simplicity.
These blocks do not alter the resolution and channel number of the features. 
Next, another ``DSConv-LN-GELU" layer projects the resultant features respectively into a low-frequency image and three levels of higher-frequency subbands, which are identical in shape to that of the input.
Finally, the inverse three-level DT-CWT transforms these features into the output.
% The benefit of replacing traditional convolutional feature pyramid mining with DT-CWT transformation is feature redundancy reduction and lower computational cost.
% Besides, U-Net architecture requires layers of convolution operations where the channel number gets larger in the deeper layers. However, DT-CWT only require one layer of separable convolution.
% Therefore, substituting DT-CWT for traditional convolutional feature pyramid extractor promotes our goal of lowering computational cost.

% We implement the protection network using MPF-Net. 
% Besides, though the ISP network and detection network do not have limitation on model size like the protection network does, we additionally implement $\emph{ISP}^1$ and $\mathcal{D}$ using the proposed architecture to verify the generalizability of the architecture on image-to-image translation and segmentation tasks.

% Figure environment removed
\noindent\textbf{Half Fourier Convolution Layer (HFC).}
% DT-CWT transformation 
% provides hierarchical and disentangled representations. 
We observe that features provided by DT-CWT
provide a rich local pattern, whereas the global information representation is lacking.
% To mitigate this, 
We include both vanilla \textit{Conv} layer and Fast Fourier Transform (FFT) in each HFC to enable simultaneous global and local feature mining. For the HFC layer at level $i$:
\begin{equation}
\begin{gathered}
\emph{HFC}_{i}: \emph{output}=[\emph{GB}(\emph{input}_1), \emph{LB}(\emph{input}_2)],\\ \emph{input}=[\emph{input}_1,\emph{input}_2],
\end{gathered}
\end{equation}
where we evenly split the input tensor by half, send them respectively into the Global Branch (GB) and Local Branch (LB) of the HFC layer, and concatenate the resultant features. 
GB contains FFT, \textit{Conv} layer and inverse FFT.
LB is composed of a cascade of two vanilla \textit{Conv} layers.

\noindent\textbf{Partial Feature Fusion Layer (PFF).}
% After performing HFC layers for global and local feature extraction, 
We fuse the processed hierarchical features for cross-frequency information exchange.
PFF mainly contains two differences compared to previous arts for fusion.
First, we only pass higher-frequency subbands into lower levels, unlike fully-connected feature fusions in \cite{cho2021rethinking,hrnet}. 
It encourages each level to process different combinations of frequencies to avoid repetition.
Second, the input features are split into two halves subject to a ratio $s$, which is 0.25 in default. 
We only engage the first half in feature fusion while directly concatenating the second half with the resultant fused features. 
For cross-frequency feature fusion, we first rescale the features from different levels into the size of the current level.
Next, we reweight each group of features according to independent Channel Attentions (\textit{CA}) and do a summation.
Therefore, the operations in PFF at level $i$ can be mathematically defined as follows.
\begin{equation}
\begin{gathered}
\emph{PFF}_{i}: \emph{output}=[\emph{input}_{i,1}, \sum_{i}\emph{CA}(\emph{Rescale}(\emph{input}_{i,2}))],\\ 
\forall i, \emph{input}_i=[\emph{input}_{i,1},\emph{input}_{i,2}], 
\end{gathered}
\end{equation}
where $\emph{CA}$ is composed of a global average pooling layer and a $1\times1$ bottleneck convolution.
Using partial feature fusion, we always reserve a portion of features within the current level to avoid relying solely on a single frequency.

% \noindent\textbf{Calibration Module for Detection Network.}
% We additionally propose a calibration module for $\mathcal{D}$. 
% Sometimes the predicted mask $\hat{M}$ provided by iDT-CWT can be blurry and inconsistent.
% Our intuition is that given a mostly correct prediction and the input image as reference, human viewers will naturally conceive that the whole item within the ambient area should be considered a complete forgery. 
% Therefore, we concatenate the output and the input of the detection network and additionally use two layer of ``DSConv-LN-GELU" to conduction mask calibration.

% % Figure environment removed

\section{Experiments}
% We provide experimental results to verify DRAW regarding the robustness and transferability of image protection. 
% We compare our scheme with several state-of-the-art image forgery detection scheme. 
% Also, ablation studies are conducted to highlight the Impact of each element in the pipeline and network design. More results are given in the supplement.
% \subsection{Experimental Setting}
% \redmarker{\noindent\textbf{Datasets. }}
\ormarker{We use several famous RAW datasets to evaluate DRAW.}
% , namely, RAISE~\cite{dang2015raise}, MIT-Adobe FiveK dataset\cite{fivek}.
We use RAISE~\cite{dang2015raise} dataset (8156 image pairs) and Canon subset (2997 image pairs) from the FiveK dataset as the training set.
Meanwhile, RAISE, Canon subset and Nikon subset (1600 image pairs) from FiveK as well as SIDD dataset~\cite{SIDD_2018_CVPR} are used to evaluate DRAW.
We divide them into training sets and test sets at a ratio of 85: 15.
We crop each RAW image into non-overlapping sub-images sized $512\times~512$, which is in line with \cite{schwartz2018deepisp,InvISP}.
% Implementation details are specified \redmarker{in the supplement.}

We empirically set the hyper-parameters as $\alpha=10, \beta=1,\gamma=0.02, \epsilon=0.01$. 
We train DRAW on four NVIDIA RTX 3090 GPU. 
For gradient descent, we use Adam optimizer with the default hyper-parameters.
The learning rate is $1\times10^{-4}$. 
All models are trained with batch size 4, and we train the networks for 10 epochs. The training finishes in roughly one day\redmarker{. (and xxx)}
%% 测试的设定要在这里说一下
% We train all network-based ISP pipelines using RGB images rendered by the libraw library as supervision and these pre-trained ISPs will be frozen when training the RAW protection network.

\begin{table}[!t]
\footnotesize
\renewcommand{\arraystretch}{1}
    \setlength{\tabcolsep}{1.5mm}
	\caption{\textbf{Quantitative analysis on the imperceptibility of RAW protection.} $[\mathbf{R},\hat{\mathbf{R}}]$: RAW file before and after protection. $[\mathbf{I},\hat{\mathbf{I}}]$: RGB file rendered respectively from $\mathbf{R}$ and $\hat{\mathbf{R}}$ using different ISP pipelines. Dataset: RAISE and Canon.}
	\label{table_different_resolution_protected}
	\begin{center}
	\begin{tabular}{c|cc|cc|cc}
		\hline
		\multirow{2}{*}{Process} & \multicolumn{2}{c|}{$512\times512$} & \multicolumn{2}{c|}{$256\times256$} & \multicolumn{2}{c}{$1024\times1024$} \\
		& PSNR & SSIM & PSNR & SSIM & PSNR & SSIM
		\\
        \hline
        [$\mathbf{R}$,$\hat{\mathbf{R}}$] & 58.43 & - & 61.67 & - & 56.41 & - \\
    
        [$\mathbf{I}$,$\hat{\mathbf{I}}$] (InvISP) & 45.13 & 0.977 & 46.20 & 0.985 & 45.60 & 0.983 \\
    
        % [$\mathbf{I}$,$\hat{\mathbf{I}}$] (DeepISP) & 44.37 & 0.971 & 44.69 & 0.973 & 43.56 & 0.967 \\
	
        [$\mathbf{I}$,$\hat{\mathbf{I}}$] (LibRaw) & 41.25 & 0.960 & 41.97 & 0.967 & 41.07 & 0.957 \\
        
        [$\mathbf{I}$,$\hat{\mathbf{I}}$] (Restormer) & 45.75 & 0.980 & 46.24 & 0.984 & 45.03 & 0.977 \\
        
        [$\mathbf{I}$,$\hat{\mathbf{I}}$] (TradISP) & 40.52 & 0.960 & 41.95 & 0.966 & 40.34 & 0.955 \\
		\hline
	\end{tabular}
	\end{center}
\end{table}

\setlength{\tabcolsep}{1.1mm}{
\begin{table*}
\footnotesize
\renewcommand{\arraystretch}{1}
    \caption{\textbf{Average performance of different methods on forgery localization.} Dataset: RAISE. The best performances are highlighted in bold type. *: open-source pretrained models finetuned on RAISE with \textit{copy-move}, \textit{splicing} and \textit{inpainting}.}
    	\label{table_comparison}
    \centering
    
	\begin{tabular}{c|c|ccc|ccc|ccc|ccc|ccc|ccc|ccc}
		\hline
		 &
		\multirow{2}{*}{Models} & 
		 \multicolumn{3}{c}{No attack} &
		 \multicolumn{3}{c}{Rescaling} &
		 \multicolumn{3}{c}{AWGN} & \multicolumn{3}{c}{JPEG90} & \multicolumn{3}{c}{JPEG70} & \multicolumn{3}{c}{Med. Blur} & \multicolumn{3}{c}{GBlur} \\
        \cline{3-23}
        & & Rec. & F1 & IoU 
        & Rec. & F1 & IoU 
        & Rec. & F1 & IoU 
        & Rec. & F1 & IoU 
        & Rec. & F1 & IoU  
        & Rec. & F1 & IoU 
        & Rec. & F1 & IoU \\
        \hline
        \multirow{5}{*}{\rotatebox[origin=c]{90}{\textit{splicing}}} 
        % & Resfcn${^{*}}$
        % & .440 & .528 & .408 & .064 & .096 & .062 & .396 & .489 & .371 & .181 & .250 & .172 & .457 & .554 & .430 & .443 & .535 & .413 & .443 & .533 & .412 \\
        & MVSS${^{*}}$
        & .908 & .725 & .597 & .715 & .609 & .470 & \textbf{.954} & .688 & .547 & \textbf{.944} & .627 & .481 & \textbf{.915} & .565 & .415 & .869 & .695 & .561 & .181 & .211 & .138  \\
        & RIML${^{*}}$
        & \textbf{.941} & \textbf{.949} & \textbf{.908} & .732 & .795 & .702 & .900 & .918 & .863 & .869 & .892 & .821 & .777 & .818 & .721 & .900 & .918 & .857 & .096 & .142 & .094  \\
        & DRAW-MVSS 
        & .867 & .874 & .793 & .553 & .636 & .514 & .886 & .854 & .764 & .878 & .856 & .767 & .820 & .789 & .680 & .732 & .770 & .658 & .320 & .419 & .301 \\
        & DRAW-RIML 
        & .897 & .926 & .876 & .877 & .910 & .856 & .928 & \textbf{.946} & \textbf{.905} & .913 & .932 & .884 & .889 & \textbf{.909} & \textbf{.849} & .917 & .939 & \textbf{.893} & \textbf{.556} & \textbf{.639} & \textbf{.544}  \\
        & DRAW-HRNet 
        & .936 & .947 & .903 & \textbf{.922} & \textbf{.934} & \textbf{.884} & .929 & .934 & .883 & .933 & \textbf{.935} & \textbf{.885} & .902 & .861 & .776 & \textbf{.927} & \textbf{.940} & .891 & .552 & .638 & .523  \\
		\hline
% 		copy move\cite{kwon2022learning}
		\multirow{5}{*}{\rotatebox[origin= c]{90}{\textit{copy-move}}} 
	    % & Resfcn${^{*}}$
     %    & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - \\
        & MVSS${^{*}}$
        & .833 & .781 & .703 & .677 & .636 & .544 & .861 & .755 & .668 & .771 & .627 & .527 & .653 & .471 & .366 & .795 & .731 & .640 & .339 & .336 & .258 \\
        & RIML${^{*}}$  
        & .888 & .889 & .856 & .774 & .793 & .737 & .896 & .895 & .861 & .829 & .835 & .788 & .694 & .719 & .657 & .850 & .856 & .811 & .557 & .572 & .493 \\
        & DRAW-MVSS 
        & .901 & .893 & .857 & .839 & .836 & .780 & .915 & .890 & .850 & .862 & .842 & .793 & .804 & .767 & .706 & .871 & .851 & .803 & .631 & .657 & .582 \\
        & DRAW-RIML 
        & .915 & .925 & .910 & .875 & .895 & .868 & .906 & .918 & .899 & .884 & .899 & .874 & .845 & .866 & .829 & .897 & .910 & .888 & .774 & .811 & .768  \\
        & DRAW-HRNet 
        & \textbf{.969} & \textbf{.970} & \textbf{.959} & \textbf{.960} & \textbf{.956} & \textbf{.937} & \textbf{.962} & \textbf{.957} & \textbf{.943} & \textbf{.955} & \textbf{.951} & \textbf{.932} & \textbf{.916} & \textbf{.884} & \textbf{.839} & \textbf{.958} & \textbf{.955} & \textbf{.939} & \textbf{.915} & \textbf{.920} & \textbf{.885}  \\
		\hline
		\multirow{5}{*}{\rotatebox[origin= c]{90}{\textit{inpainting}}} 
	    % & Resfcn${^{*}}$
     %    & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - \\
        & MVSS${^{*}}$
        & .259 & .229 & .172 & .101 & .062 & .039 & .404 & .360 & .263 & .180 & .090 & .054 & .212 & .097 & .058 & .088 & .050 & .030 & .085 & .043 & .026  \\
        & RIML${^{*}}$
        & .126 & .140 & .097 & .035 & .047 & .030 & .132 & .155 & .113 & .014 & .020 & .013 & .001 & .001 & .001 & .037 & .043 & .026 & .068 & .077 & .048  \\
        & DRAW-MVSS 
       & .737 & .752 & .672 & .657 & .682 & .588 & .771 & .756 & .667 & .617 & .645 & .546 & \textbf{.515} & \textbf{.536} & \textbf{.434} & .567 & .595 & .497 & .514 & .561 & .463   \\
        & DRAW-RIML 
        & .663 & .716 & .656 & .457 & .518 & .452 & .667 & .718 & .654 & .348 & .411 & .342 & .091 & .121 & .089 & .366 & .423 & .360 & .284 & .338 & .281 \\
        % & .611 & .666 & .604 & .374 & .434 & .368 & .591 & .649 & .578 & .264 & .323 & .259 & .057 & .077 & .056 & .299 & .354 & .293 & .233 & .282 & .230  \\
        & DRAW-HRNet 
        & \textbf{.776} & \textbf{.791} & \textbf{.735} & \textbf{.754} & \textbf{.760} & \textbf{.685} & \textbf{.788} & \textbf{.771} & \textbf{.697} & \textbf{.719} & \textbf{.714} & \textbf{.625} & .468 & .454 & .346 & \textbf{.732} & \textbf{.735} & \textbf{.647} & \textbf{.686} & \textbf{.704} & \textbf{.618}  \\
        \hline
	\end{tabular}
\end{table*}
}

% Figure environment removed


% In other words, the baseline tries to aid robust forgery detection by slightly modifying the RGB image. 
% For fair comparison, we regulate that the PSNR between $\mathbf{I}$ and $\hat{\mathbf{I}}$ should be within 1dB range with that of DRAW.
 
% \noindent\textbf{Evaluation metric. }
% We employ the Peak Signal-to-Noise Ratio (PSNR), the Structural SIMilarity (SSIM)~\cite{wang2004image} to evaluate the image fidelity.
% We employ recall, F1 score and intersection-over-union(IoU) to measure the accuracy of image forgery localization. 

% F1 score reports the overall precision and sensitivity of the scheme, where forgery localization can be considered as a binary classification problem, i.e., the pixels within a targeted image can be distinguished into either \textit{original} or \textit{forged}. 

\subsection{Performances}
\noindent\textbf{Quality Assessment.}
In Fig.~\ref{image_isp_samples}, we arbitrarily show three test RAW images from the RAISE dataset for qualitative evaluation on the imperceptibility of the protection.
We can observe little artifact from the protected version of RAW data and RGB. 
From the augmented difference, we see that DRAW tends to slightly introduce some local patterns for transferrable forgery detection.
% It can be explained that the network alters the distribution of the collection of protected RGB images where natural images are supposed to not carry similar tiny pattern. 
% The purpose is to make the new distribution more easily distinguishable with the natural image distribution.
Besides, we test the overall image quality of protected images using unseen ISP network, namely, Restormer~\cite{zamir2022restormer}, and another conventional ISP, namely, TradISP.
Restormer is originally proposed for image restoration, but we find that the transformer-based architecture also shows excellent performance on RGB image rendering.
TradISP is another popular open-source ISP pipeline apart from LibRaw, and we customize the pipeline by applying the most essential modules shown in Fig.~\ref{demo_RAWtoRGB}.
Table~\ref{table_different_resolution_protected} shows the quantitative result, where the overall image quality of the rendered RGB images is also not affected.
% \footnote{https://github.com/mushfiqulalam/isp}

% Figure environment removed
\noindent\textbf{Robustness and Accuracy of Image Manipulation Detection.}
We conduct comprehensive experiments on RAISE and Canon dataset under different lossy operations.
The qualitative and quantitative comparisons in terms of the Recall, F1 and IoU in the pixel domain are reported in Fig.~\ref{image_comparison}, Table~\ref{table_comparison} and Table~\ref{table_comparison_CANON}. 
The results under image color adjustment operations and combined attacks are included \redmarker{in the supplement}.
Note that we train $\mathcal{P}$ together with HRNet~\cite{hrnet} as the detector (DRAW-HRNet) as our benchmark model, and we extend DRAW by fixing $\mathcal{P}$ and training the MVSS~\cite{dong2021mvss} detector (DRAW-MVSS) and the RIML~\cite{RIML} detector (DRAW-RIML) on top of the protected RGB images.
To enable practical application, we always consider random image cropping attack with survival rate $r\in[0.7^2,1^2]$ on top of other manipulations.
We find that for DRAW-HRNet, although the images are manipulated by a variety of lossy operations, we succeed in localizing the tampered areas. 
If there is no post-processing operations, the F1 scores are in most cases above 0.8.
Fig.~\ref{image_detection_hrnet} further provides exampled image manipulation detection results of DRAW-HRNet, where multiple image lossy operations are involved.

% ~\footnote{https://github.com/dong03/MVSS-Net}
% ~\footnote{https://github.com/highwaywu/imageforensicsosn}
Next, for fair comparison with previous arts, we finetune the state-of-the-art passive image forgery detection schemes, i.e., MVSS~\cite{dong2021mvss} and RIML~\cite{RIML}, on RAISE and Canon dataset using the mechanisms proposed in the corresponding papers yet additionally considering \textit{splicing}, \textit{copy-move} and \textit{inpainting}.
We see that MVSS fails to detect the tampered content if there are heavy image lossy operations.
Though RIML shows stronger robustness compared to MVSS owing to OSN transmission simulation, its performances under blurring or inpainting attacks are not satisfactory.
In contrast, if we train these detectors upon protected RGB images, the robustness issue is largely alleviated where even for JPEG70 and Gaussian blurring attacks, the F1 scores are noticeably increased.
Besides, while MVSS and RIML are originally weak in detecting inpainting forgeries, DRAW can also help improve their accuracy and robustness against inpainting. 
% ~\footnote{https://github.com/dong03/MVSS-Net}
% ~\footnote{https://github.com/highwaywu/imageforensicsosn}

\setlength{\tabcolsep}{1.1mm}{
\begin{table}
\footnotesize
\renewcommand{\arraystretch}{1}
    \caption{\textbf{Average performance of different methods on forgery localization.} Dataset: CANON.}
    	\label{table_comparison_CANON}
    \centering
    
	\begin{tabular}{c|c|cc|cc|cc|cc}
		\hline
		 &
		\multirow{2}{*}{Models} & 
		 \multicolumn{2}{c}{No attack} &
		 \multicolumn{2}{c}{Rescaling} &
            \multicolumn{2}{c}{JPEG70} &  \multicolumn{2}{c}{GBlur} \\
        \cline{3-10}
        & & F1 & IoU 
        & F1 & IoU 
        & F1 & IoU 
        & F1 & IoU \\
        \hline
        \multirow{5}{*}{\rotatebox[origin=c]{90}{\textit{splicing}}} 
        % & Resfcn${^{*}}$
        % & .440 & .528 & .408 & .064 & .096 & .062 & .396 & .489 & .371 & .181 & .250 & .172 & .457 & .554 & .430 & .443 & .535 & .413 & .443 & .533 & .412 \\
        & MVSS${^{*}}$
        & .610 & .465 & .530 & .390 & .503 & .354 & .210 & .135 \\
        & RIML${^{*}}$
        & .925 & \textbf{.872} & .716 & .609 & .783 & .675 & .136 & .094 \\
        & DRAW-MVSS 
        & .841 & .738 & .875 & .789 & .842 & .739 & .829 & .731  \\
        & DRAW-RIML 
        & .887 & .818 & .925 & .870 & .855 & .769 & .906 & .843 \\
        & DRAW-HRNet 
        & \textbf{.926} & .869 & \textbf{.939} & \textbf{.889} & \textbf{.921} & \textbf{.861} & \textbf{.939} & \textbf{.891}  \\
		\hline
% 		copy move\cite{kwon2022learning}
		\multirow{5}{*}{\rotatebox[origin= c]{90}{\textit{copy-move}}} 
	    % & Resfcn${^{*}}$
     %    & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - \\
        & MVSS${^{*}}$
        & .727 & .628 & .624 & .526 & .455 & .341 & .371 & .283\\
        & RIML${^{*}}$  
        & .892 & .852 & .789 & .733 & .702 & .619 & .569 & .494  \\
        & DRAW-MVSS 
        & .912 & .879 & .868 & .828 & .832 & .785 & .826 & .776 \\
        & DRAW-RIML 
        & \textbf{.969} & \textbf{.957} & \textbf{.962} & \textbf{.947} & .911 & .882 & .952 & .933\\
        & DRAW-HRNet 
        & .968 & .956 & .960 & .945 & \textbf{.922} & \textbf{.895} & \textbf{.952} & \textbf{.934} \\
		\hline
		\multirow{5}{*}{\rotatebox[origin= c]{90}{\textit{inpainting}}} 
	    % & Resfcn${^{*}}$
     %    & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - \\
        & MVSS${^{*}}$
        & .215 & .150 & .100 & .064 & .136 & .083 & .073 & .045 \\
        & RIML${^{*}}$
        & .102 & .070 & .033 & .021 & .003 & .001 & .012 & .007 \\
        & DRAW-MVSS 
       & .829 & .753 & .676 & .574 & .115 & .076 & .513 & .407 \\
        & DRAW-RIML & \textbf{.949} & \textbf{.918} & \textbf{.889} & \textbf{.836} & \textbf{.406} & \textbf{.326} & \textbf{.849} & \textbf{.784}   \\
        & DRAW-HRNet 
        & .934 & .894 & .867 & .811 & .360 & .284 & .761 & .691 \\
        \hline
	\end{tabular}
\end{table}
}
\begin{table}
\footnotesize
\renewcommand{\arraystretch}{1}
    \caption{\textbf{Transferability of DRAW.} Our protection network trained on RAISE is tested 1) using unseen imagine pipelines on RAISE and 2) on Canon and SIDD for cross-dataset validation.}
    	\label{table_transfer}
    \centering
	\begin{tabular}{c|c|c|ccccc}
		\hline
		\multicolumn{2}{c|}{Test Item} & {Forgery} & 
		NoAtk &
		Rescale & JPEG70 & M-Blur & G-Blur\\
        \hline
        \multirow{6}{*}{\rotatebox[origin=c]{90}{\textit{ISP}}} & \multirow{3}{*}{TradISP}
        & Spli. & .929 & .910 & .837 & .933 & .620 \\
        & & Copy. & .941 & .919 & .843 & .941 & .880  \\
        & & Inpa. & .850 & .820 & .451 & .765 & .756 \\
		\cline{2-8}
		& \multirow{3}{*}{Restormer}
        & Spli. & .946 & .936 & .863 & .941 & .648 \\
        & & Copy. & .961 & .947 & .871 & .948 & .904 \\
        & & Inpa. & .906 & .833 & .487 & .789 & .759 \\
		\hline
% 	\end{tabular}
% \end{table}

% \begin{table}
% \footnotesize
% %\renewcommand{\arraystretch}{1}
%     \caption{Transferability of the proposed scheme using different dataset(cameras). (F1 score)}
%     	\label{table_transfer_dataset}
%     \centering
% 	\begin{tabular}{c|c|c|c|c|c|c}
% 		\hline
		% {Dataset} & {Forgery} & 
		% Identity &
		% JPEG 70 & Scaling &  M-Blur & Bright.\\
  %       \hline
        \multirow{6}{*}{\rotatebox[origin=c]{90}{\textit{Dataset}}} & \multirow{3}{*}{Canon}
        & Spli.& .936 & .925 & .845 & .931 & .596 \\
        & & Copy. & .957 & .930 & .859 & .946 & .881  \\
        & & Inpa. & .805 & .732 & .486 & .710 & .706   \\
		\cline{2-8}
		& \multirow{3}{*}{SIDD}
        & Spli. & .928 & .909 & .832 & .911 & .574  \\
        & & Copy. & .967 & .965 & .891 & .954 & .880 \\
        & & Inpa. & .686 & .628 & .400 & .574 & .554\\
		\hline
	\end{tabular}
\end{table}

\begin{table}[!t]
\footnotesize
\renewcommand{\arraystretch}{1}
    % \setlength{\tabcolsep}{1.5mm}
	\caption{\textbf{Comparison of computational cost} among lightweight image-to-image-translation or segmentation networks.}
	\begin{center}
	\begin{tabular}{c|c|c|c|c|c}
		\hline
	    & SegNet~\cite{segnet} & LED~\cite{LED}  & U-Net~\cite{Unet} & ENet~\cite{Enet} & MPF-Net   \\
        \hline
        Params & 29.5M & 0.94M & 26.35M & 0.36M & 0.25M \\
        FLOPS & 0.56T & 22.9G & 0.22T & 2.34G & 7.39G \\
        Memory & 465MB & 390MB & 767MB & 46MB & 160MB\\
		\hline
	\end{tabular}
	\label{table_complexity}
	\end{center}
\end{table}
 \setlength{\tabcolsep}{1.1mm}{
\begin{table}
\footnotesize
\renewcommand{\arraystretch}{1}
\caption{\textbf{Comparison with baseline methods on RAISE.}} We verify the importance of RAW protection by comparing the results with those of pure robust training using $\mathcal{A}$ and direct RGB protection. $^*$: using $\mathcal{P}$ for RGB protection.
    	\label{table_comparison_baseline}
    \centering
    
	\begin{tabular}{c|cccc|cc|cc|cc|cc}
		\hline
		 \multirow{2}{*}{} &
		\multicolumn{4}{c|}{Used Modules} & 
		 \multicolumn{2}{c}{Rescaling} &
		 \multicolumn{2}{c}{JPEG70} &
            \multicolumn{2}{c}{Med. Blur} &  \multicolumn{2}{c}{GBlur} \\
        \cline{2-13}
        & ${\mathcal{P}}$ & ${\mathcal{P}^*}$ & ${\mathcal{A}}$ & ${\mathcal{D}}$
        & F1 & IoU 
        & F1 & IoU 
        & F1 & IoU 
        & F1 & IoU \\
        \hline
        \multirow{4}{*}{\rotatebox[origin=c]{90}{\textit{splicing}}} 
        % & Resfcn${^{*}}$
        % & .440 & .528 & .408 & .064 & .096 & .062 & .396 & .489 & .371 & .181 & .250 & .172 & .457 & .554 & .430 & .443 & .535 & .413 & .443 & .533 & .412 \\
        & & & & \checkmark
        & .609 & .470 & .565 & .415 & .695 & .561 & .211 & .138  \\
        & & & \checkmark& \checkmark
       & \textbf{.668} & \textbf{.534} & .725 & .590 & .762 & .635 & .303 & .207  \\
        & & \checkmark& \checkmark& \checkmark
      & .358 & .253 & .438 & .317 & .487 & .361 & .149 & .097  \\
        & \checkmark & & \checkmark& \checkmark
        & .636 & .514 & \textbf{.789} & \textbf{.680} & \textbf{.770} & \textbf{.658} & \textbf{.419} & \textbf{.301}  \\
		\hline
% 		copy move\cite{kwon2022learning}
		\multirow{4}{*}{\rotatebox[origin= c]{90}{\textit{copy-move}}} 
	    % & Resfcn${^{*}}$
     %    & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - \\
        & & & & \checkmark
        & .636 & .544 & .471 & .366 & .731 & .640 & .336 & .258   \\
        & & & \checkmark& \checkmark
        & \textbf{.859} & \textbf{.816} & .648 & .582 & .782 & .728 & .528 & .456   \\
        & &\checkmark & \checkmark& \checkmark
         & .490 & .412 & .467 & .382 & .626 & .548 & .268 & .208   \\
        & \checkmark& & \checkmark& \checkmark
        & .836 & .780 & \textbf{.767} & \textbf{.706} & \textbf{.851} & \textbf{.803} & \textbf{.657} & \textbf{.582} \\
		\hline
		\multirow{4}{*}{\rotatebox[origin= c]{90}{\textit{inpainting}}} 
	    % & Resfcn${^{*}}$
     %    & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - \\
        & & & & \checkmark
        & .062 & .039 & .097 & .058 & .050 & .030 & .043 & .026 \\
        & & & \checkmark& \checkmark
        & .605 & .494 & .231 & .159 & .398 & .297 & .342 & .249 \\
        & & \checkmark& \checkmark& \checkmark
         & .387 & .291 & .480 & .371 & .381 & .288 & .374 & .279   \\
        & \checkmark& & \checkmark& \checkmark
        & \textbf{.682} & \textbf{.588} & \textbf{.536} & \textbf{.434} & \textbf{.595} & \textbf{.497} & \textbf{.561} & \textbf{.463}  \\
        \hline
	\end{tabular}
\end{table}
}


\noindent\textbf{Transferability.}
To verify the transferability of our method, in Table~\ref{table_transfer}, we conduct additional experiments where our protection
network trained on RAISE dataset is applied on different RAW datasets, i.e., Canon and SIDD) and with unseen ISP pipelines, i.e., TradISP and Restormer.
The results show that image protection can also be transferred from RAW data into RGB images, and the image manipulation detection results are promising.
For \textit{copy-move} and \textit{splicing} attack, the F1 scores under JPEG70 attack given the new ISPs are above 0.7, representing successful manipulation localization. 
Therefore, our method can adapt to unseen ISP pipelines.
% and can be transferred to a variety of cameras.

\noindent\textbf{Computational Complexity.}
In Table~\ref{table_complexity}, we compare the proposed MPF-Net with SegNet~\cite{segnet}, LED~\cite{LED}, U-Net~\cite{Unet} and ENet~\cite{Enet}, which are lightweight models for image segmentation. 
MPF-Net requires low computing resources, e.g,  only 20.9\% in memory cost and 0.95\% in parameters compared to the classical U-Net architecture.
Therefore, with MPF-Net, we can potentially change the current situation where digital images can be freely manipulated.


\subsection{Baseline Comparisons}
\label{section_baseline}
There is no existing work that utilizes RAW protection against image manipulation.
We build two baseline methods which respectively apply pure robust training using our proposed $\mathcal{A}$ and apply image protection on RGB.
The MVSS detector is employed since RIML contains OSN simulation during training.
Table~\ref{table_comparison_baseline} reports the comparison results.
Detailed experimental settings of the two baselines are reported \redmarker{in the supplement}.

\noindent\textbf{RAW Protection vs Pure Robust Training.}
Our proposed robust training mechanism reflected in $\mathcal{A}$ is different from that in RIML. 
We render the unprotected RAW files $\mathbf{R}$ using $\mathcal{S}$, which are then attacked by $\mathcal{A}$.
We see that the introduction of robust training can help boosting the performance of MVSS.
However, the overall performance is still worse than further applying DRAW for RAW protection. 
For instance, for blurring and inpainting attacks where the manipulation traces are comparatively subtle, the performance of using pure robust training drops more than ten percent in average.

\noindent\textbf{RAW protection vs RGB protection.}
\redmarker{Previous works~\cite{ying2021image} only embed signal to classify faulty images.}
We implement an RGB protection scheme where $\mathcal{P}$ slightly changes $\mathbf{I}$ to combat image manipulation.
For fair comparison, we regulate that the overall PSNR on RGB images before and after protection should be above 40 dB, in line with the criterion in Table~\ref{table_different_resolution_protected}. 
The results show that RGB protection cannot aid robust forgery detection if we limit the magnitude of RGB modification.
The reason is that compared to 8-bit RGB images, RAW contains a much larger dynamic range that allows more effective image protection.

\subsection{Ablation Studies}
% We explore the impact of the key components in our method by evaluating the performance of the networks with varied setups. 
Table~\ref{table_ablation} and Fig.~\ref{image_ablation} respectively show the quantitative and qualitative results of ablation studies. 
In each test, we regulate that the averaged PSNR between $\mathbf{I}$ and $\hat{\mathbf{I}}$, with ISP pipelines evenly applied, should be within the range of 41-43 dB, to ensure imperceptible image protection.

\noindent\textbf{Substituting the architecture of $\mathcal{P}$.}
We first test if using U-Net with a similar amount of parameters or ENet~\cite{Enet} as $\mathcal{P}$ can achieve similar performance on splicing detection.
First, though ENet contains similar amount of parameters compared to MPF-Net, the performance of image forgery detection using ENet as $\mathcal{P}$ is not satisfactory.
Second, though U-Net with $\textit{DSConv}$ provides much better result, because the channel numbers within each layer are restricted within 48 to save computational complexity, the performance is still worse than our benchmark.

\noindent\textbf{Impact of components in MPF-Net.}
The most noticeable difference between MPFNet with previous U-shaped networks is that feature disentanglement can be better ensured even with less parameters.
To verify this, we respectively replace the HFC layer and PFF layer with typical alternatives, i.e., vanilla convolution and channel-wise concatenation.
The performances are nearly 5-10 points weaker compared to the MPF-Net setup.
First, DT-CWT is a shift-invariant wavelet transform that comes with limited redundancy.
Second, partial feature fusion and partial connection are more flexible.
The design explicitly keeps some of the features extracted from the current level and directly feed them into the subsequent block. Therefore, for different levels, the input features will be different, which encourages feature disentanglement.

% Figure environment removed
\begin{table}[!t]
\footnotesize
\renewcommand{\arraystretch}{1}
\centering
  \caption{\textbf{Ablation study on DRAW on Nikon using splicing attack.} $^1$: replacing \textit{Conv} layers with \textit{DSConv}.}
\begin{tabular}{cccc}
\hline
\multirow{2}{*}{Test} & \multicolumn{3}{c}{F1} \\
  \cline{2-4}  & NoAtk  & JPEG70 & Mblur \\

\hline
% $\mathcal{P}$(Baseline)+$\mathcal{D}$(HrNet)  & .093  & .176   & .143   \\
% $\mathcal{P}$(Baseline)+$\mathcal{D}$(RIML)  & .093  & .176   & .143   \\
% $\mathcal{P}$(Baseline)+$\mathcal{D}$(MVSS)  & .093  & .176   & .143   \\
% \hline
$\mathcal{P}$ using U-Net$^1$  & .877  & .769  & .535   \\
$\mathcal{P}$ using ENet   & .324  & .137   & .092   \\
\hline
MPF-Net w/o HFC & .844  & .710 & .602\\
MPF-Net w/o DT-CWT & .852 & .751 & .626\\
MPF-Net w/o PFF & .827 & .712 & .667\\
% $\mathcal{D}$ w/o Cali. Module & .893 & .784 & .652\\
\hline
w/o diff from real attack & .842  & .566 & .502\\
using only one ISP Surrogate & .648  & .455 & .267\\
w/o Image Distortion Module & .929  & .245  & .116\\
w/o Color Ajustment Module & .814  & .759  & .641\\
\hline
Full implementation of \textbf{DRAW}  & .929  & \textbf{.838} & \textbf{.696} \\
\hline
\end{tabular}
\label{table_ablation}
\end{table}
\noindent\textbf{Impact of pipeline design.}
We also tested the setting of not using the image distortion module or color adjustment module in the pipeline during training.
The result is as expected that the scheme will therefore lack generalizability in overall robustness due to the fact that there are not enough random processes that can simulate the real-world situation.
Besides, not introducing the difference between the real-world and simulated attacks or using only one ISP surrogate model will also impair the overall performance.
