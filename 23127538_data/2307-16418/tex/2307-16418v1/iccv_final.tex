\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[T1]{fontenc}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{3696} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}
\input{command}
%%%%%%%%% TITLE
\title{DRAW: Defending Camera-shooted RAW against Image Manipulation}

\author{Xiaoxiao Hu\textsuperscript{1,2*}, Qichao Ying\textsuperscript{1,2*}, Zhenxing Qian\textsuperscript{1,2$\dagger$}, Sheng Li\textsuperscript{1,2}, Xinpeng Zhang\textsuperscript{1,2}\\
\textsuperscript{1}School of Computer Science, Fudan University\\
\textsuperscript{2}Key Laboratory of Culture \& Tourism Intelligent Computing, Fudan University\\
}
% Key Laboratory of Culture \& Tourism Intelligent Computing of Ministry of Culture \& Tourism,
% \author{Xiaoxiao Hu \\
% Fudan University\\
% {\tt\small huxx21@m.fudan.edu.cn}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Qichao Ying \\
% Fudan University\\
% {\tt\small shinydotcom@163.com}
% \and
% Zhenxing Qian\\
% Fudan University\\
% {\tt\small zxqian@fudan.edu.cn}
% \and 
% Sheng Li\\
% Fudan University\\
% {\tt\small lisheng@fudan.edu.cn}
% \and 
% Xinpeng Zhang \\
% Fudan University\\
% {\tt\small zhangxinpeng@fudan.edu.cn}
% }


\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi
% \thispagestyle{empty}

\newcommand\blfootnote[1]{%
\begingroup
\renewcommand\thefootnote{}\footnote{#1}%
\addtocounter{footnote}{-1}%
\endgroup
}

%%%%%%%%% ABSTRACT
\blfootnote{*Xiaoxiao Hu and Qichao Ying contribute equally to this work.}
\blfootnote{$\dagger$Corresponding author: Zhenxing Qian (zxqian@fudan.edu.cn)}
\begin{abstract}
RAW files are the initial measurement of scene radiance widely used in most cameras, and the ubiquitously-used RGB images are converted from RAW data through Image Signal Processing (ISP) pipelines. Nowadays, digital images are risky of being nefariously manipulated. Inspired by the fact that innate immunity is the first line of body defense, we propose DRAW, a novel scheme of defending images against manipulation by protecting their sources, i.e., camera-shooted RAWs. Specifically, we design a lightweight Multi-frequency Partial Fusion Network (MPF-Net) friendly to devices with limited computing resources by frequency learning and partial feature fusion. It introduces invisible watermarks as protective signal into the RAW data. The protection capability can not only be transferred into the rendered RGB images regardless of the applied ISP pipeline, but also is resilient to post-processing operations such as blurring or compression. Once the image is manipulated, we can accurately identify the forged areas with a localization network. Extensive experiments on several famous RAW datasets, e.g., RAISE, FiveK and SIDD, indicate the effectiveness of our method. We hope that this technique can be used in future cameras as an option for image protection, which could effectively restrict image manipulation at the source.
% thereby changing the current situation where digital images can be freely manipulated.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

In the digital world, the credibility of the famous saying ``seeing is believing"  is largely at risk since nowadays people can easily manipulate critical content within an image and redistribute the fabricated version via the Internet.
% In many cases, attackers edit images to make them more eye-catching and impactful. 
% Digital images are vulnerable to nefarious manipulation attacks.
% The proliferation of image editing software 
% % has led to the rampant manipulation of images, 
% causes the authenticity of digital images doubtful.
Owing to the fact that readers are more susceptible to well-crafted misleading material, 
fabricated images can be a means for some politicians to sway public opinion.
In more severe cases, those fraudulent images can be used to bolster fake news or criminal investigation. 

Image manipulation detection~\cite{chen2008determining,popescu2005exposing} and localization~\cite{dong2021mvss, RIML} has become a critical area of research for decades, with the goal of 
% protecting image authenticity and ownership.
% In the past decades, image manipulation detection has aroused extensive research interest to protect image authenticity as well as ownership.
% The objective is not only to 
distinguishing manipulated images from authentic ones and locating the manipulated areas.
% , in order to prevent the spreading and reveal the purpose of forgery.
% The aim is to not only distinguish manipulated images from authentic ones but also locate the manipulated areas, therefore blocking the spreading of fake information and unveiling the purpose of forgery. 
While early methods mainly check the integrity of the images from statistical aspects, e.g., the Photo-Response Non-Uniformity (PRNU) noise~\cite{chen2008determining} and the fixed pattern noise (FPN)~\cite{kurosawa1999ccd},
% and the Color Filter Array (CFA) interpolation patterns ~\cite{popescu2005exposing}. 
the uprising of deep networks has greatly strengthened the capability to find traces left by a variety of manipulation~\cite{dong2021mvss,wu2019mantra,hu2020span}. 
% Figure environment removed
% Lossy image post-processing attacks downgrades the performance of passive image forgery detection schemes. Our RAW protection scheme realizes robust image forgery detection with the help of imperceptible signal injection.
% For example, 
% There are also many schemes for universal tampering detection~\cite{dong2021mvss,wu2019mantra,hu2020span} that exploit universal noise artifact left by manipulation.
% However, image forgery detection schemes are faced with enormous types of situations in the real world, where any critical object in an image can be removed, a.k.a., the \textit{inpainting} attack, or replaced with an image patch from the same image, a.k.a, the \textit{copy-moving} attack.
% Or a non-existing object can be added, a.k.a., the \textit{splicing} attack.
% However, attackers often perform a chain of image post-processing operations on the forged images to conceal their manipulation behavior.
However, the adversary is also continuously evolving both in strength and diversity.
For example, recent deep-network-based image editing algorithms~\cite{LAMA, ZITS} are reported to produce highly realistic images with almost no visible artifacts near the edges.
Therefore, it remains a big issue whether the learned subtle forensics traces can always be present in the newly forged images.
% many of these works cannot combat common image lossy operations such as compression or rescaling, where the learned clues can be erased.
Also, though some works~\cite{RIML, wu2022robust} explicitly handle lossy online transmission scenarios,
they still face limited performance against well-crafted forgeries, e.g., inpainting, or lossy image operations, e.g., Gaussian blurring. 

Inspired by the fact that innate immunity is the first line of body defense and the best weapon to mitigate diseases, safeguarding images against manipulations is an alternative and promising way of deterring malicious attackers.
Indeed, the ubiquitous 8-bit RGB images are not the pristine format for reflecting how we perceive the world.
They are converted from RAW files via ISP pipelines.
% Normally, RAW data are further converted into RGB images through Image Signal Processing (ISP) pipelines. 
% We notice that RAW files serve as the primary recording relative to the scene radiance and are widely utilized in most cameras. And RGB images are generated from RAW data through ISPs, i.e., RAW data is the source of RGB images. 
Therefore, we propose DRAW, a proactive image protection scheme that defends camera-shooted RAW data against malicious manipulation on the RGB domain. 
% Typically, Attackers rarely manipulate RAW data, as it cannot be directly observed. Therefore, after protecting RAW data within the camera, all attackers are limited to tampering with the protected RGB images, which are monitored by our localization network.
Specifically, we propose to introduce imperceptible protective signal into the RAW data, which can be transferred into the rendered RGB images, even though various types of ISP pipelines are applied. 
Once these images are manipulated, the localization networks can exactly localize the forged areas regardless of image post-processing operations such as blurring, compression or color jittering.
% It serves as a digital lock which can be used to detect and localize manipulation.
% On the recipient's side, a detection network estimates the mask of the manipulated areas regardless of the presence of image post-processing operations such as compression or rescaling.
% a detection network can then locate manipulated areas within the image, even in the presence of image post-processing operations such as compression or rescaling.
% In other words, DRAW adds digital \textit{locks} onto the pixels and image manipulation detection is conducted by observing the integrity of these \textit{locks}.
Besides, a novel Multi-frequency Partial Fusion Network (MPF-Net) is proposed to implement RAW protection, which adopts frequency learning and cross-frequency partial feature fusion to significantly decrease the computational complexity.
We illustrate the functionality of DRAW in Fig.~\ref{fig:teaser}, which promotes accurate manipulation \greenmarker{localization} without affecting the visual quality.

Extensive experiments on several famous RAW datasets, e.g., RAISE, FiveK and SIDD, prove the imperceptibility, robustness and generalizability of our method.
Besides, to compare RAW-domain protection with previous works, we tempt to borrow the success of RGB-domain protection~\cite{asnani2022proactive,zhao2023proactive,ying2021image} as the baseline method for proactive manipulation localization.
% However, we find it hard to ensure high localization accuracy without compromising the visual quality.
The results show that DRAW hosts a noticeable performance gain and a nontrivial benefit of content-related adaptive embedding.
% with the help of content-related procedures, e.g., demosaicing and noise reduction (Fig.~\ref{demo_RAWtoRGB}), within the subsequent ISP algorithms that suppress unwanted artifacts.
% (Section~\ref{section_baseline}).
In addition, MPF-Net provides superior performance compared to classical U-Net~\cite{Unet} architecture with only 20.9\% of its memory cost and 0.95\% of its parameters. 
% The lightweightness makes possible image protection within cameras in the shooting stage, thereby changing the current situation where digital images can be freely manipulated.
The novel lightweight architecture makes it possible to be integrated into cameras in the future, thereby changing the current situation where digital images can be freely manipulated.


% Alternatively, there are also watermarking-based proactive forensics techniques that embed deterministic signals or patterns into original images to bypass the challenge of passively detecting the fragile traces.
% With this comes an additional requirement that users usually prefer non-intrusive image protection methods that leave little distortion.
%%% RAW 保护的好处
% In compared with RGB-domain protection, DRAW can adaptively introduce protection with the help of content-related procedures, e.g., demosaicing and noise reduction, within the subsequent ISP algorithms that suppress unwanted artifacts.
% Besides, RAW data modification theoretically enjoys a much larger searching space that allows transformations from the original image into another image also with high density upon sampling. 
% Recently, Asnani et al.~\cite{asnani2022proactive} propose to embed templates into images for more accurate manipulation detection.
% Zhao et al.~\cite{zhao2023proactive} embed watermarks as anti-Deepfake labels into the facial identity features.
% While these approaches have been proven effective in forged image classification, they 
% existing works do not address the challenge of forgery localization,
% These methods are effective in forged image classification, but do not address forgery localization.
% \ormarker{Rather than focusing on developing} a more advanced architecture or mechanism for finding \textit{irremovable} traces, we are interested in \ormarker{exploring} how to make the original image files ``\textit{read-only}". 
% and though extending the idea of RGB-domain protection for robust forgery localization can be straight-forward, 
% Besides, a novel Multi-frequency Partial Fusion Network (MPF-Net) is proposed to implement RAW protection, which adopts frequency learning and cross-frequency partial feature fusion to largely lower the computational complexity.
% making it possible to be integrated into cameras in the future
% and we find that directly extending these RGB-domain protection methods for robust image forgery detection provides unsatisfactory results.
% One key issue is that mild modification in the RGB logits can hardly provide embedding capacity large enough to hold all information required for forgery detection (see Section~\ref{section_baseline}).

% One possible reason is that the densely-predicting task requires hiding more information, making it hard to maintain high fidelity of the original image (Section~\ref{section_baseline}). 
% and there lacks effective \textit{subjective image prior} to ensure high detection accuracy without leaving noticeable artifacts (Section~\ref{section_baseline}). 
% The lightweight nature might enable image protection in the shooting stage in smartphones or cameras, which effectively restrict image manipulation at the source.
% \highlight{\textbf{Practicality.}} The lightweight MPF-Net is designed to be integrated into cameras for image protection in the shooting stage, thereby changing the current situation where digital images can be freely manipulated.
% The proposed method can be in the near future integrated into cameras, thereby changing the current situation where digital images can be freely manipulated.
The contributions of this paper are three-folded, namely:
\begin{enumerate}
\item DRAW is the first to propose
RAW protection against image manipulation. The corresponding RGB images will carry imperceptible protective signal even though various types of imaging pipelines or lossy image operations are applied. 
\item With RAW protection, image manipulation localization networks can better resist lossy image operations such as JPEG compression, blurring and rescaling.
\item  A novel lightweight MPF-Net is proposed for integrating RAW protection into cameras in the future, thereby potentially changing the current situation where digital images can be freely manipulated.
% The architecture only requires 1/10 parameters of that in the traditional U-Net while the performances are better.
% while little visible difference between the protected and non-protected RGB pairs can be observed.
\end{enumerate}

%------------------------------------------------------------------------
\section{Related Works}
\noindent\textbf{Passive Image Manipulation Localization.}
% Image tampering localization schemes aim at finding traces to unveil the behavior of image forgery.
% Splicing, copy-moving, and inpainting are three common types of image manipulation methods. 
Many existing image forensics schemes are designed to detect special kinds of attacks, e.g., splicing detection~\cite{RIML,salloum2018image}, copy-moving detection~\cite{islam2020doa,li2018fast} and inpainting detection~\cite{zhu2018deep,li2019localization}.
In addition, some universal tampering detection schemes~\cite{dong2021mvss,wu2019mantra,hu2020span} exploit universal noise artifacts left by manipulation. Mantra-Net~\cite{wu2019mantra} uses fully convolutional networks, Z-Pooling and long short-term memory cells for pixel-wise anomaly detection.
MVSS-Net~\cite{dong2021mvss} jointly exploits the noise view and the boundary artifact using multi-view feature learning and multi-scale supervision.
% Li et al.~\cite{li2019localization} proposes to implement an FCN’s first convolutional layer with trainable high-pass filters and apply their HP-FCN for inpainting detection. 
% CAT-Net~\cite{kwon2022learning} models quantized DCT coefficient distribution to trace compression artifacts in splicing attacks.
% DOA-GAN~\cite{islam2020doa} proposes two attention modules for copy-moving detection, where the first is from an affinity matrix based on the extracted feature vectors at every pixel, and the second is to further capture more precise patch inter-dependency. 
% SPAN~\cite{hu2020span} model the relationship between image patches at multiple scales by constructing a pyramid of local self-attention blocks. 
RGB-N~\cite{zhou2018learning} additionally utilizes auto-generated data augmentation for training. 
% Despite the existence of these well-designed works,
% However, real-world image tampering localization is still an open issue. 
% Unfortunately, various lossy operations adopted by OSNs, e.g., compression and resizing, impose great challenges for implementing the robust image forgery detection.
% But these methods usually neglect the fact that attackers often use a chain of image post-processing methods to hide their behaviors.
% RIML~\cite{RIML} includes adversarial training to manipulation localization schemes
RIML~\cite{RIML} includes adversarial training, where the lossy Online Social Network (OSN) transmission is simulated by modeling noise from different sources. 
However, these works are still limited in generalization to well-crafted manipulations or heavy lossy operations. 
% These challenges can result in significant deviations from typical image characteristics and may not be fully captured by existing detection methods, leading to reduced accuracy and reliability.
% We introduce imperceptible protective signal via RAW protection to combat image manipulation.
% Figure environment removed


% Figure environment removed
\noindent\textbf{Watermarking for Image Protection.}
% Image steganography~\cite{lu2021large,jing2021hinet,fridrich2012rich,ying2019robust} aims at hiding secret information into the host images for covert purposes.
Many image protection schemes based on watermarking~\cite{lu2021large,jing2021hinet,fridrich2012rich,ying2019robust} have been proposed.
% where the extracted secret information is utilized respectively for forged image identification, fragile self-recovery, or disruption of potential threat models.
% % RAW files contain everything that passes through your camera’s image sensor. Enjoy a greater color range and depth to create vibrant images.
% % When the light is reflected by the scene, the photons travel through the camera optics, fall onto the image sensors and were converted into electrons subject to the Color Filter Array (CFA).
% % , and before comprising a RAW image, cameras usually conduct black light subtraction, defective pixel masking on the signal.
% RAW files record the amount of photons that passes through camera's image sensor subject to the Color Filter Array (CFA). 
% % Enjoy a greater color range and depth to create vibrant images.
% Image Signal Processing (ISP) pipeline usually includes several individual stages, as shown in Fig.~\ref{demo_RAWtoRGB}, to convert RAW data into visually appealing RGB images.
% Recently, many learning-based methods have shown satisfactory RAW rendering results, e.g., DeepISP~\cite{schwartz2018deepisp}, CameraNet\cite{liang2021cameranet}, CycleISP~\cite{zamir2020cycleisp} and InvISP~\cite{InvISP}.
% % Many of the rendered images can achieve close or even better visual quality compared to those by some expensive manufacturerISPs.
% For image protection, 
% Zhang et al. propose several fragile watermarking schemes~\cite{zhang2008fragile,zhang2009fragile,zhang2010reference} for tampering localization and self-recovery. 
% Ying et al.~\cite{ying2021image} improve the robustness of these works using deep networks,
% Most recently, RAW data generation and rendering has been applied into many downstream applications. 
% For example, CycleISP~\cite{zamir2020cycleisp} train a paired network that estimates the RAW version of a given RGB and vice versa. 
% InvISP~\cite{InvISP} efficiently store RAW files in JPEG format using an invertible network.
% Wu et al.~\cite{zhang2021all} propose to convert adversarial images into RAWs and transform them back as data purification.
% but the visual quality of the protected images is not satisfactory.
% Asnani et al.~\cite{asnani2022proactive} recently propose to embed templates into images for more accurate DeepFake detection, but the method cannot localize the manipulated areas.
% Zhang et al. propose several fragile self-embedding schemes~\cite{zhang2008fragile,zhang2009fragile,zhang2010reference} that alter least significant bits for sanity checking and self-recovery.
Asnani et al.~\cite{asnani2022proactive} propose to embed templates into images for more accurate manipulation detection.
Zhao et al.~\cite{zhao2023proactive} embed watermarks as anti-Deepfake labels into the facial identity features.
FakeTagger~\cite{wang2021faketagger} embeds the identity information into the whole facial image, which can be recovered after illegal face swapping.
% , but these methods are non-robust and therefore limited in practical application.
% Some works use steganography to prevent the images from being manipulated by \ormarker{disturbing}  generative models.
Khachaturov et al.~\cite{khachaturov2021markpainting} and Yin et al~\cite{yin2018deep} respectively propose to attack inpainting or Super-Resolution (SR) models by forcing them to work abnormally on the targeted images. 
% As a result, the method can prevent images from being inpainted only with the cost of embedding trivial perturbations. 
% Similarly, Yin et al~\cite{yin2018deep} proposes a defensive method based on data hiding to defeat Super-Resolution (SR) models.
% The hidden information mainly resides in higher-band details of the targeted images, which are often analyzed or augmented by many schemes that employ deep networks for image restoration.
\greenmarker{However, these approaches do not tackle the issue of forgery localization, and many of them cannot combat lossy image operations.}
% In contrast, We use RAW protection to complete robust image manipulation localization.
We alternatively introduce imperceptible protective signal into RAW data and transfer it into RGB images to aid robust manipulation localization.
% In this paper, we use two learning-based methods\cite{schwartz2018deepisp, InvISP} and one traditional method for our training.

\noindent\textbf{Models for Limited Computing Resources.}
% Neural networks are shown to be gradually stronger and more  effective in representation learning provided with modern architecture design. 
% ResNet~\cite{resnet} introduces shortcut learning to achieve a richer gradient combination for training deep networks.
% DenseNet~\cite{densenet} applys dense connections to fully utilize mined features from each layer.
% For instance, ELASTIC~\cite{elastic}, SE-Net~\cite{senet} and non-local network~\cite{non_local} further introduces additional branches or attention to enhance learning representation.
% However, 
% Heavy networks can hardly be executable on small devices such as cameras or cellphones. Thus,
Classical network architectures for segmentation-based tasks, e.g., U-Net~\cite{Unet} or FPN~\cite{FPN}, usually require non-affordable computing resources for many small devices.
% Researchers have made substantial efforts to improve the efficiency of deep networks. 
MobileNet~\cite{mobile} and ShuffleNet~\cite{ma2018shufflenet} are early works on addressing this issue respectively via Depth-wise Separable Convolution (DSConv) and channel split \& shuffle.
% that divides convolutional kernels into groups with sparse connections.
ENet~\cite{Enet} proposes an asymmetric encoder-decoder architecture with early downsampling.
SegNet~\cite{segnet} only stores the max-pooling indices of
the feature maps and uses them in its decoder network to
achieve good performance.
% Octave Convolution~\cite{octave} and CSPNet~\cite{CSPNet} save computational complexity by storing low-frequency features in low-resolution tensors or only feeding half of the channels into each convolutional calculation. 
% LED~\cite{LED} utilizes channel split and shuffle in the residual blocks to greatly reduce computation cost.
% PSP~\cite{PSP} includes multi-scale pooling layers ahead of convolutions to quickly enlarge receptive field, and therefore reduces the amount of layers required. 
% Besides, Binary Neural Networks (BNN)~\cite{BNN} replace heavy matrix multiplication operations with lightweight bit-wise operations.
% Though a number of well-designed CNN architectures with enhanced learning capacity~\cite{FPN,PSP} or lower computational complexity~\cite{mobile,elastic} can be employed to implement the networks, 
% However, very few of them can simultaneously ensure efficiency and performance in image-to-image translation tasks. 
Despite substantial efforts made, these networks are either still computationally demanding or sacrifice performance for model size shrinkage.
% Motivated by recent success of frequency learning~\cite{fnet,Flearning} and , 
We propose MPF-Net that contains only 20.9\% of memory cost and 0.95\% of parameters of U-Net yet provides surpassing performance in our task.

%------------------------------------------------------------------------
\section{Proposed Method}
% We present the methodology of DRAW in Section 3.1, and the novel hierarchical partial network for network implementation is specified in Section 3.2. The loss functions are shown in Section 3.3 and more training details are enclosed \redmarker{in the supplement}.
% Fig.~\ref{image_framework} depicts the pipeline design of DRAW, which contains two trainable networks, i.e., a protection network \ormarker{$\mathcal{P}$} and a localization network $\mathcal{D}$.
% We also include an image signal processing layer $\mathcal{S}$ for image rendering, as well as a hybrid attack layer $\mathcal{A}$ to simulate image \ormarker{manipulation and} post-processing operations.
\subsection{Approach}
% \noindent\textbf{Protection Network.}
Fig.~\ref{image_framework} depicts the pipeline design of DRAW. 
We denote the captured RAW data as $\mathbf{R}$, and use a protection network $\mathcal{P}$ to transform $\mathbf{R}$ into the protected RAW, i.e., $\hat{\mathbf{R}}$.
The functionality of $\mathcal{P}$ is to adaptively \ormarker{embed} a transferrable protective signal into $\hat{\mathbf{R}}$ for robust and accurate image manipulation localization in the RGB domain.
% while minimizing the distortion towards both $\mathbf{R}$ and the rendered RGB images. 
Considering the computational limitation of imaging equipment, we \ormarker{use a} novel lightweight MPF-Net specified in Section~\ref{section_network} \ormarker{to implement $\mathcal{P}$}. 
% \noindent\textbf{Image Signal Processing Layer.}
Next, we use the ISP layer $\mathcal{S}$ to render $\hat{\mathbf{R}}$ into the protected RGB image $\hat{\mathbf{I}}$.
% We include off-the-shelf differentiable networks to enable gradient flow, and non-differentiable conventional ISP algorithms to enhance transferability.
Provided with a number of off-the-shelf deep-network-based ISP algorithms and non-differentiable conventional ISP algorithms, during training,
we include a popular conventional method, i.e., LibRaw~\cite{libraw} and two deep-learning methods, i.e.,
% , which contains Bayer demosaicing, color balancing, white balancing, colorspace conversion, etc. 
CycleISP~\cite{zamir2020cycleisp} and InvISP~\cite{InvISP}, and leave other ISP algorithms~\cite{zamir2022restormer,tradISP} for evaluation.
To improve generalizability,
% For diversifying the distribution of the output RGB, 
interpolation is conducted on one network-rendered RGB $\hat{\mathbf{I}}_{\emph{net}}$ and one conventional-algorithm-generated RGB $\hat{\mathbf{I}}_{\emph{conv}}$ to produce $\hat{\mathbf{I}}$, i.e.,
$\hat{\mathbf{I}}=\omega\cdot\hat{\mathbf{I}}_{\emph{conv}}+(1-\omega)\cdot\hat{\mathbf{I}}_{\emph{net}}$,
where ${\omega}$ is uniformly within $[0,1]$. 
% $\hat{\mathbf{I}}$ is then distributed by the owner.

% Figure environment removed
% Figure environment removed
% \noindent\textbf{Hybrid Attack Layer.}
% The hybrid attack layer $\mathcal{A}$ includes both simulated and real-world attacks. 
Afterward, to simulate image redistribution of $\hat{\mathbf{I}}$, we include the hybrid attack layer $\mathcal{A}$ to perform manipulation and lossy operations on $\hat{\mathbf{I}}$.
% Unlike existing datasets for fogery detection, we need to 
It comprises of modules for tampering, color adjustments, distortions (lossy operations) and cropping.
\greenmarker{
In line with typical forgery detection works~\cite{dong2021mvss,RIML}, we consider inpainting, splicing and copy-moving as the most common three types of tampering, which often alter the underlying meaning of an image.
In contrast, color adjustment and distortion are often considered benign yet can potentially erase traces for manipulation localization.
% For mask generation, we randomly generate them with a rate, which is unlike existing datasets for forgery localization. 
}
During training, these modules can be conditionally performed according to the empirical \textit{activation possibilities} (85\%) and in any arbitrary ordering to encourage diversity, e.g., tampering then distorting, cropping then tampering, etc.
% $\hat{\mathbf{I}}$, we process it using different combinations of these sub-layers in arbitrary orders, subject to the \textit{activation possibilities} marked in Fig.~\ref{image_framework} (b). 
We respectively denote the attacked images as $\hat{\mathbf{I}}_{\emph{t}}$ if the tampering module is activated or $\hat{\mathbf{I}}_{\emph{nt}}$ if otherwise. 
The latter is identified as authentic images, whose introduction is to explicitly minimize the false alarm rate of DRAW.
Detailed implementations of the modules are specified \redmarker{in the supplement}.
% \noindent\textbf{Color Adjustment and Image Distortion.}
% Many users would readjust the overall color using image augmentation such as brightening or contrast enhancing, and during data transmission, some details of the images might be altered due to format conversion or noise addition. 
% Though color adjustment and image-level distortion are less likely to cause misinformation, previous studies~\cite{dong2021mvss,wu2019mantra} have shown that they can easily erase the manipulation trace, and therefore weaken the performance of forgery detection.
% Accordingly, we train the scheme in the fashion of adversarial training. 
% We process $\mathbf{I}^{\emph{mani}}$ using a collection of typical data augmentation methods $\emph{Aug}(\cdot)$ as well as image-level distortion methods $\emph{Dis}(\cdot)$.
% Therefore, the post-processed image $\mathbf{I}^{\emph{post}}$ can be produced by one of the following: 
% $\mathbf{I}^{\emph{post}}=\emph{Aug}(\mathbf{I}^{\emph{mani}})$,
% $\mathbf{I}^{\emph{post}}=\emph{Dis}(\mathbf{I}^{\emph{mani}})$,
% $\mathbf{I}^{\emph{post}}=\emph{Aug}(\emph{Dis}(\mathbf{I}^{\emph{mani}}))$, or $\mathbf{I}^{\emph{post}}=\emph{Dis}(\emph{Aug}(\mathbf{I}^{\emph{mani}}))$. 
% For image inpainting, we adopt two most recent schemes for simulation, which include ZITS~\cite{ZITS}
% % ~\footnote{https://github.com/DQiaole/ZITS\_inpainting} 
% and LAMA~\cite{LAMA}.
% ~\footnote{https://github.com/saic-mdal/lama}.
% To encourage randomness during training, we arbitrarily shuffle the order of the two networks. The two modules can also be skipped subject to a skipping probability (15\%), respectively.
% Note that cropping can cause desynchronization in discovering the JPEG checkerboard artifact as well as the introduced signal by DRAW. 
% Thus, we opt to independently conduct image cropping attack apart from image distortion.
Besides, to closer the gap between real and simulated lossy operations and color jittering operations, 
% we are inspired by the straight-through estimation~\cite{bengio2013estimating, esser2021taming} that 
we add the difference between $\hat{\mathbf{I}}_{\emph{syn}}$ and $\hat{\mathbf{I}}_{\emph{rw}}$ on to $\hat{\mathbf{I}}_{\emph{syn}}$, where
$\hat{\mathbf{I}}_{\emph{syn}}$ and $\hat{\mathbf{I}}_{\emph{rw}}$ respectively denote synthetic and real-world processed image using the same setting.
% between $\mathbf{I}_{\emph{rw}}$ and $\mathbf{I}_{\emph{syn}}$, where as 
% $d=\mathbf{I}_{\emph{rw}}-\mathbf{I}_{\emph{syn}}$
$x=\hat{\mathbf{I}}_{\emph{syn}}+\emph{sg}(\hat{\mathbf{I}}_{\emph{rw}}-\hat{\mathbf{I}}_{\emph{syn}}), x\in\{\hat{\mathbf{I}}_{\emph{t}},\hat{\mathbf{I}}_{\emph{nt}}\}$,
where $\emph{sg}$ stands for the stop-gradient operator~\cite{bengio2013estimating}.
% The attacked image is then updated as $\mathbf{I}_{\emph{syn}}+d$, numerically identical to  $\mathbf{I}_{\emph{rw}}$ but with gradient.
 
% After performing the above attacks, we use a differentiable image quantization method \cite{bengio2013estimating} to convert the floating-point values of $\mathbf{X}$ to 8-bit integer and store the corresponding image into the disk in PNG or JPEG format. 

% \noindent\textbf{Detection network.}
On the recipient's side, we use the localization network $\mathcal{D}$ to estimate the manipulated region given a doubted image that could be one of $\hat{\mathbf{I}}_{\emph{t}}$ or $\hat{\mathbf{I}}_{\emph{nt}}$.
If it's an manipulated image $\hat{\mathbf{I}}_{\emph{t}}$, the predicted mask $\hat{\mathbf{M}}_t$ should be close to the ground-truth $\mathbf{M}$. 
Otherwise, it should be close to a zero matrix.
DRAW is flexible on the selection of $\mathcal{D}$, where many off-the-shelf networks can be applied, e.g., DRAW-HRNet~\cite{hrnet}, DRAW-MVSS~\cite{dong2021mvss} or DRAW-RIML~\cite{RIML}.
% Figure environment removed

\noindent\textbf{Objective Loss Functions.}
% \label{section_loss}
% DRAW should not severely alter the contents of the original scene in sacrifice for anti-manipulation protection. 
We need to include fidelity terms $\mathcal{L}_{\mathcal{P}}^{\emph{RAW}}$ and $\mathcal{L}_{\mathcal{P}}^{\emph{RGB}}$ to ensure imperceptible protection.
We find that the $\ell_1$ distance is the best in practice to minimize modification compared to many advanced deep-network-based terms, e.g., Lpips loss~\cite{zhang2018unreasonable} and contextual loss~\cite{zhang2019zoom}.
% We define $\mathcal{L}_{\mathcal{P}}^{\emph{RAW}}$ and $\mathcal{L}_{\mathcal{P}}^{\emph{RGB}}$ as follows.
\begin{equation}
\label{loss_protect_RAW}
\begin{gathered}
\mathcal{L}_{\mathcal{P}}^{\emph{RAW}}=\mathbb{E}_{\mathbf{R}}\left[\left\|\mathbf{R}-\mathcal{P}\left(\mathbf{R}\right)\right\|_1\right], \\
\mathcal{L}_{\mathcal{P}}^{\emph{RGB}}=\mathbb{E}_{\mathbf{R}}\left[\left\|\mathcal{S}\left(\mathbf{R}\right)-\mathcal{S}\left(\mathcal{P}\left(\mathbf{R}\right)\right)\right\|_1\right].
\end{gathered}
\end{equation}
% \begin{equation}
% \label{loss_precept}
% \mathcal{L}_{\emph{VGG}}=\sum_{i=1}^N \frac{1}{H'W'C'}\left|\phi_{\emph{pool}_i}-\hat{\phi}_{\emph{pool}_i}\right|_1,
% \end{equation}
% We also find that merely using the perceptual loss will also cause checkerboard artifact, as reported by \cite{distill}. 
% So we use a hyper-parameter $\alpha$ to carefully balance the two terms.
% In Eq.~(\ref{loss_precept}),
Next, we include localization terms
to minimize the Binary Cross Entropy (BCE) losses that respectively compare $\hat{\mathbf{M}}_t$ with $\mathbf{M}$, and $\hat{\mathbf{M}}_{\emph{nt}}$ with a zero matrix. 
% The latter term penalizes the networks if the estimation on non-tampered images are not zero matrices.
% \begin{equation}
% \ormarker{
% \begin{gathered}
% \mathcal{L}_{c}=\emph{BCE}(\hat{\mathbf{M}}_t,\mathbf{M}), 
% \mathcal{L}_{e}=\emph{BCE}(\hat{\mathbf{M}}_{\emph{nt}},\mathbf{O})
% % , \\ 
% % \emph{BCE}(x,y)=-\sum_{i,j}\left(x_{i,j}\log{y_{i,j}} + \left(1-{x_{i,j}}\right)\log{(1-y_{i,j}})\right).
% \end{gathered}
% }
% \end{equation}
%%%%% borrowed from "Reality Transform Adversarial Generators for Image Splicing Forgery Detection and Localization" ICCV 2021
%% Eq. 1
% \begin{aligned}
% \min _{G_M} V\left(G_M\right) & =\frac{1}{3} \mathbb{E}_{x \sim \mathcal{X}}\left[\left(G_M(x)-m\right)^2\right] \\
% & +\frac{1}{3} \mathbb{E}_{x \sim \mathcal{X}}\left[\left(G_M\left(G_T(x)\right)-m\right)^2\right] \\
% & +\frac{1}{3} \mathbb{E}_{y \sim \mathcal{Y}}\left[\left(G_M(y)-0_{W, H}\right)^2\right], \\
% \min _{G_T} V\left(G_T\right) & =\mathbb{E}_{x \sim \mathcal{X}}\left[\left(G_M\left(G_T(x)\right)-0_{W, H}\right)^2\right] .
% \end{aligned}
%% Eq. 8
\begin{equation}
\begin{gathered}
L_{\mathcal{D}}^{\emph{T}}= -\mathbb{E}_{\hat{\mathbf{I}}_{\emph{t}}}\left[\mathbf{M}\log \left(\mathcal{D}(\hat{\mathbf{I}}_{\emph{t}})\right)+\left(1-\mathbf{M}\right)\log\left(1-\mathcal{D}(\hat{\mathbf{I}}_{\emph{t}})\right)\right],\\
L_{\mathcal{D}}^{\emph{NT}}= -\mathbb{E}_{\hat{\mathbf{I}}_{\emph{nt}}}\left[\log\left(1-\mathcal{D}(\hat{\mathbf{I}}_{\emph{nt}})\right)\right].
\end{gathered}
\end{equation}
The total loss for DRAW is shown in Eq.~(\ref{eqn_loss_sum}), where $\alpha, \beta, \gamma, \epsilon$ are empirically-set hyper-parameters.
\begin{equation}
\begin{gathered}
\label{eqn_loss_sum}
\mathcal{L}=\alpha\cdot\mathcal{L}_{\mathcal{P}}^{\emph{RAW}}+\beta\cdot\mathcal{L}_{\mathcal{P}}^{\emph{RGB}}+\gamma\cdot\mathcal{L}_{\mathcal{D}}^{\emph{T}}+\epsilon\cdot\mathcal{L}_{\mathcal{D}}^{\emph{NT}},\\
\alpha=10, \beta=1,\gamma=0.02, \epsilon=0.01.
\end{gathered}
\end{equation}

\subsection{Multi-frequency Partial Fusion Network}
\label{section_network}
% The protection module is located within small devices such as cameras, and therefore $\mathcal{P}$ must be lightweight enough to be embedded. But it also needs to ensure that modifications made on images rendered by the generated protected RAW can be easily localized, regardless of what types of image post-processings or ISP methods are applied.
% Considering that the protection network must be lightweight and effective in RAW protection with transferable and robust forgery detection.
% Naturally, the network requires not only global information to determine which parts of the image contain different statistical characteristics, e.g., JPEG compression pattern, with those of the whole image, but also local information to preserve the contextual consistency of the image after RAW rendering.
% Since RAW protection happens within cameras, it requires that the network must efficiently extract both local, or contextual, and global, or holistic, information with limited amount of trainable parameters. 
% We find that many heuristical method such as DFT and wavelet transform, e.g., Haar Transform or DT-CWT, require very few computational resources for global or disentangled feature extraction and are compatible in mobile devices.
% Therefore, we propose a novel CNN architecture named MPFNet that jointly process the image in the combined DT-CWT and DFT domain and with partial feature fusion.
In order to combat sophisticated image manipulation within resource-limited environments such as cellphones and cameras, 
it is essential to deploy a lightweight architecture yet with rich feature extraction capabilities.
% lightweight architecture and diverse feature extraction are two main requirements for MPF-Net.
Fig.~\ref{image_hpn} illustrates the network design, where we first use a three-level DT-CWT transform to decompose the input into a low-frequency main component and three levels of higher-frequency subbands. 
Each level consists of six subbands in complex forms, representing different degrees of wavelet information. The real and imaginary parts of the subbands are then concatenated.
% to form the input features for each level.
In Fig.~\ref{DT-CWT_example}, we compare the feature pyramid of U-Net to that of DT-CWT.
Vanilla convolutions can be less efficient due to the restriction of receptive field, feature redundancy, and repetition during training.
In contrast, DT-CWT provides a strong prior for mitigating these issues, requiring only one layer of separable convolution and yielding richer patterns within representations.
% The introduction of DT-CWT are strong prior for alleviating this issue which only requires one layer of separable convolution and the mined features show richer patterns. 
% transformation made on the input can be therefore represented as follows: $\mathbb{R}^{H\cdot~W\cdot~C} \rightarrow$ $[
%     \mathbb{R}^{\frac{H}{4}\cdot\frac{W}{4}\cdot~C},$
%     $\mathbb{R}^{\frac{H}{2}\cdot\frac{W}{2}\cdot~C\cdot~12},$
%     $\mathbb{R}^{\frac{H}{4}\cdot\frac{W}{4}\cdot~C\cdot~12},$
%     $\mathbb{R}^{\frac{H}{8}\cdot\frac{W}{8}\cdot~C\cdot~12}
%     ]$.

Following the initial feature extraction, 
% the hierarchical features are respectively fed into independent ``DSConv-LN-GELU" layers, which is in short for depth-wise separable convolution~\cite{mobile}, Layer Normalization~\cite{LN} and GELU activation~\cite{GELU}. 
% It projects the feature pyramid into refined features with the same amount of channels.
we apply a ``DSConv-LN-GELU" layer to further refine the extracted features, which is in short for depth-wise separable convolution~\cite{mobile}, Layer Normalization~\cite{LN} and GELU activation~\cite{GELU}.
Next, we cascade sixteen multi-frequency partial fusion blocks in each level as feature refinement and fusion.
% Afterward, we use several cascaded multi-frequency partial fusion blocks for further feature refinement and inter-frequency fusion. 
Each block contains a Half Fourier Convolution (HFC) layer and a Partial Feature Fusion (PFF) layer.
% In Fig.~\ref{image_hpn}, $F_{j}^1$ and $F_{j}^2$ respectively denote the output of HFC and PFF at $j_{\emph{th}}$ scale, and the subscripts for block number are omitted for simplicity.
Notably, these blocks do not alter either the resolution or channel number of the features. 
Then we project the features back into the main components and three levels of subbands using another “DSConv-LN-GELU” layer, 
% These subbands have the same shape as their initial inputs.
which are then transformed back into the RGB domain via iDT-CWT.
% Next, another ``DSConv-LN-GELU" layer projects the resultant features respectively into a low-frequency image and three levels of higher-frequency subbands, which are identical in shape to that of the input.
% Finally, the inverse three-level DT-CWT transforms these features into the output.
% The benefit of replacing traditional convolutional feature pyramid mining with DT-CWT transformation is feature redundancy reduction and lower computational cost.
% Besides, U-Net architecture requires layers of convolution operations where the channel number gets larger in the deeper layers. However, DT-CWT only require one layer of separable convolution.
% Therefore, substituting DT-CWT for traditional convolutional feature pyramid extractor promotes our goal of lowering computational cost.

% We implement the protection network using MPF-Net. 
% Besides, though the ISP network and detection network do not have limitation on model size like the protection network does, we additionally implement $\emph{ISP}^1$ and $\mathcal{D}$ using the proposed architecture to verify the generalizability of the architecture on image-to-image translation and segmentation tasks.


\noindent\textbf{Half Fourier Convolution Layer (HFC).}
% , without any image lossy operations applied
% DT-CWT transformation 
% provides hierarchical and disentangled representations. 
We observe that features provided by DT-CWT
provide a rich local pattern, whereas the global information representation is lacking.
Considering that Fast Fourier Transform (FFT) is efficient in giving global information about the frequency components of an image~\cite{zhou2022fedformer,lee2021fnet},
we include both vanilla \textit{Conv} layer and Fast Fourier Transform (FFT) in each HFC to enable simultaneous global and local feature mining. For the HFC layer at level $i$:
\begin{equation}
\begin{gathered}
\emph{HFC}_{i}: \emph{output}=[\emph{GB}(\emph{input}_1), \emph{LB}(\emph{input}_2)],\\ \emph{input}=[\emph{input}_1,\emph{input}_2],
\end{gathered}
\end{equation}
where we evenly split the input tensor by half, send them respectively into the Global Branch (GB) and Local Branch (LB) of the HFC layer, and concatenate the resultant features. 
GB contains FFT, \textit{Conv} layer and inverse FFT.
LB is composed of a cascade of two vanilla \textit{Conv} layers.

\noindent\textbf{Partial Feature Fusion Layer (PFF).}
% After performing HFC layers for global and local feature extraction, 
% on fusing multi-scale multi-frequency features from each level.
On fusing different groups of features, two most commonly-accepted ways are ``concatenate-and-reduce"~\cite{cho2021rethinking,hrnet} or ``attend-to-aggregate"~\cite{dual_attention,SKFF}.
% Though residual learning save computation drastically, .
% Other advanced feature fusion techniques such as dual attention or 
% techniques, the PFF module mainly include two novel designs.
We propose a novel paradigm of ``reserve-attend-and-assemble". Specifically, we split the input features into two halves based on a predetermined ratio $s$ (default 0.25), i.e., $\emph{input}_i=[\emph{input}_{i,1},\emph{input}_{i,2}]$ for PFF at level i.
The first half of the multi-level features ($C_f\cdot~s$) are resized into the size of the current level, and then separately reweighed using channel attention (CA).
% channel attention (CA)~\cite{hu2018squeeze}.
Next, ``assemble" is done by pixel-wisely aggregating all groups of reweighed features and concatenating them with the reserved second half ($C_f\cdot~(1-s)$). 
Our paradigm can potentially mitigate the issue of over-attention on certain frequencies or covariance drift of the preserved representation, especially from shallow layers, caused by residual learning.
% reserves features from all frequencies, and therefore diversify representations in deeper layers.
Furthermore, we only pass higher-frequency subbands into lower levels, which also encourages each level to process unique combinations of frequencies which reduces redundancy.
The operations in PFF at level $i$ can be mathematically defined as follows.
\begin{equation}
\begin{gathered}
\emph{PFF}_{i}: \emph{output}=[\emph{input}_{i,2}, \sum_{j\leq~i}\emph{CA}(\emph{Resize}(\emph{input}_{j,1}))]
\end{gathered}
\end{equation}
% ,\\ 
% \forall j\leq~i, \emph{input}_i=[\emph{input}_{i,1},\emph{input}_{i,2}], 
where $\emph{CA}$ is composed of a global average pooling layer and a $1\times1$ bottleneck convolution.
% Note that HFC blocks process on all channels, so we do not require any additional shuffling operation in ShuffleNet~\cite{ma2018shufflenet}.
% Using partial feature fusion, we always reserve a portion of features within the current level to avoid relying solely on a single frequency.
% We prove the effectiveness of the two novel mechanisms in Section~\ref{section_ablation}.

% \noindent\textbf{Calibration Module for Detection Network.}
% We additionally propose a calibration module for $\mathcal{D}$. 
% Sometimes the predicted mask $\hat{M}$ provided by iDT-CWT can be blurry and inconsistent.
% Our intuition is that given a mostly correct prediction and the input image as reference, human viewers will naturally conceive that the whole item within the ambient area should be considered a complete forgery. 
% Therefore, we concatenate the output and the input of the detection network and additionally use two layer of ``DSConv-LN-GELU" to conduction mask calibration.

% % Figure environment removed


%------------------------------------------------------------------------
\section{Experiments}
\subsection{Experimental Setups}
% We provide experimental results to verify DRAW regarding the robustness and transferability of image protection. 
% We compare our scheme with several state-of-the-art image forgery detection scheme. 
% Also, ablation studies are conducted to highlight the Impact of each element in the pipeline and network design. More results are given in the supplement.
% \subsection{Experimental Setting}
% \redmarker{\noindent\textbf{Datasets. }}
% We use several famous RAW datasets to evaluate DRAW.
% , namely, RAISE~\cite{dang2015raise}, MIT-Adobe FiveK dataset\cite{fivek}.
We use RAISE~\cite{dang2015raise} dataset (8156 image pairs) and Canon subset (2997 image pairs) from the FiveK~\cite{bychkovsky2011learning} dataset as the training set.
Meanwhile, RAISE, Canon subset and Nikon subset (1600 image pairs) from FiveK as well as SIDD dataset~\cite{SIDD_2018_CVPR} are used to evaluate DRAW.
We divide them into training sets and test sets at a ratio of 85: 15.
We crop each RAW image into non-overlapping sub-images sized $512\times~512$. 
% , which is in line with \cite{schwartz2018deepisp, InvISP}.
% \ormarker{For quantitative analysis, manually manipulating all protected images requires unaffordable effort. Inspired by \cite{RIML,zhou2018learning}, we arbitrarily select regions for copy-moving and inpainting and borrow segmentation masks and the sources from MS-COCO~\cite{lin2014microsoft} dataset for splicing. 
For quantitative analysis, inspired by ~\cite{zhou2018learning, RIML}, we opt to arbitrarily select regions for copy-moving and inpainting and borrow segmentation masks and the sources from MS-COCO~\cite{lin2014microsoft} dataset for splicing.
For qualitative analysis, we also manually manipulate over one hundred protected images and show some of the representative examples in the figures. 
% \greenmarker{Existing manipulated RGB datasets can also be protected through raw reconstruction (see supplement for more details).}
%%%%%%%%% MOVE TO SUPPLEMENT
% \greenmarker{Moreover, we extend RAW protection for existing images from popular datasets, e.g., CASIA~\cite{CASIA}, and the results are in the supplement.}
% The survival rate of cropping for each image is $r\in[0.7^2,1^2]$.
% These visual demonstrations serve as a complementary means to the objective evaluation metrics and aim to offer additional insights into the practical applicability of DRAW.
% Implementation details are specified \redmarker{in the supplement.}

We train our benchmark model by jointly training $\mathcal{P}$ with HRNet~\cite{hrnet} as $\mathcal{D}$. 
We then fix $\mathcal{P}$ and respectively training MVSS~\cite{dong2021mvss} and RIML~\cite{RIML} as $\mathcal{D}$ on top of the protected RGB images.
All models are trained with batch size 16 on four distributed NVIDIA RTX 3090 GPUs, and we train the networks for 10 epochs in roughly one day.
For gradient descent, we use Adam optimizer with the default hyper-parameters.
The learning rate is $1\times10^{-4}$.  
\begin{table}[!t]
\footnotesize
\renewcommand{\arraystretch}{1}
    \setlength{\tabcolsep}{1.5mm}
	\caption{\textbf{Quantitative analysis on the imperceptibility of RAW protection.} $[\mathbf{R},\hat{\mathbf{R}}]$: RAW file before and after protection. $[\mathbf{I},\hat{\mathbf{I}}]$: RGB file rendered respectively from $\mathbf{R}$ and $\hat{\mathbf{R}}$ using different ISP pipelines. Dataset: RAISE and Canon.}
	\label{table_different_resolution_protected}
	\centering
	\begin{tabular}{c|cc|cc|cc}
		\hline
		\multirow{2}{*}{Process} & \multicolumn{2}{c|}{$512\times512$} & \multicolumn{2}{c|}{$256\times256$} & \multicolumn{2}{c}{$1024\times1024$} \\
		& PSNR & SSIM & PSNR & SSIM & PSNR & SSIM
		\\
        \hline
        [$\mathbf{R}$,$\hat{\mathbf{R}}$] & 58.43 & - & 61.67 & - & 56.41 & - \\
    
        [$\mathbf{I}$,$\hat{\mathbf{I}}$] (InvISP) & 45.13 & 0.977 & 46.20 & 0.985 & 45.60 & 0.983 \\
    
        % [$\mathbf{I}$,$\hat{\mathbf{I}}$] (DeepISP) & 44.37 & 0.971 & 44.69 & 0.973 & 43.56 & 0.967 \\
	
        [$\mathbf{I}$,$\hat{\mathbf{I}}$] (LibRaw) & 41.25 & 0.960 & 41.97 & 0.967 & 41.07 & 0.957 \\
        
        [$\mathbf{I}$,$\hat{\mathbf{I}}$] (Restormer) & 45.75 & 0.980 & 46.24 & 0.984 & 45.03 & 0.977 \\
        
        [$\mathbf{I}$,$\hat{\mathbf{I}}$] (OpenISP) & 40.52 & 0.960 & 41.95 & 0.966 & 40.34 & 0.955 \\
		\hline
	\end{tabular}
\end{table}
\setlength{\tabcolsep}{1.1mm}{
\begin{table*}
\footnotesize
\renewcommand{\arraystretch}{1}
    \caption{\textbf{Average performance of different methods on forgery localization.} Dataset: RAISE. The best performances are highlighted in bold type. *: open-source pretrained models finetuned on original RAISE images with \textit{copy-moving}, \textit{splicing} and \textit{inpainting}.}
    	\label{table_comparison}
    \centering
    
	\begin{tabular}{c|c|ccc|ccc|ccc|ccc|ccc|ccc|ccc}
		\hline
		 &
		\multirow{2}{*}{Models} & 
		 \multicolumn{3}{c}{No attack} &
		 \multicolumn{3}{c}{Rescaling} &
		 \multicolumn{3}{c}{AWGN} & \multicolumn{3}{c}{JPEG90} & \multicolumn{3}{c}{JPEG70} & \multicolumn{3}{c}{Med. Blur} & \multicolumn{3}{c}{GBlur} \\
        \cline{3-23}
        & & Rec. & F1 & IoU 
        & Rec. & F1 & IoU 
        & Rec. & F1 & IoU 
        & Rec. & F1 & IoU 
        & Rec. & F1 & IoU  
        & Rec. & F1 & IoU 
        & Rec. & F1 & IoU \\
        \hline
        \multirow{5}{*}{\rotatebox[origin=c]{90}{\textit{splicing}}} 
        % & Resfcn${^{*}}$
        % & .440 & .528 & .408 & .064 & .096 & .062 & .396 & .489 & .371 & .181 & .250 & .172 & .457 & .554 & .430 & .443 & .535 & .413 & .443 & .533 & .412 \\
        & MVSS${^{*}}$
        & .908 & .725 & .597 & .715 & .609 & .470 & \textbf{.954} & .688 & .547 & \textbf{.944} & .627 & .481 & \textbf{.915} & .565 & .415 & .869 & .695 & .561 & .181 & .211 & .138  \\
        & RIML${^{*}}$
        & \textbf{.941} & \textbf{.949} & \textbf{.908} & .732 & .795 & .702 & .900 & .918 & .863 & .869 & .892 & .821 & .777 & .818 & .721 & .900 & .918 & .857 & .096 & .142 & .094  \\
        & DRAW-MVSS 
        & .867 & .874 & .793 & .553 & .636 & .514 & .886 & .854 & .764 & .878 & .856 & .767 & .820 & .789 & .680 & .732 & .770 & .658 & .320 & .419 & .301 \\
        & DRAW-RIML 
        & .897 & .926 & .876 & .877 & .910 & .856 & .928 & \textbf{.946} & \textbf{.905} & .913 & .932 & .884 & .889 & \textbf{.909} & \textbf{.849} & .917 & .939 & \textbf{.893} & \textbf{.556} & \textbf{.639} & \textbf{.544}  \\
        & DRAW-HRNet 
        & .936 & .947 & .903 & \textbf{.922} & \textbf{.934} & \textbf{.884} & .929 & .934 & .883 & .933 & \textbf{.935} & \textbf{.885} & .902 & .861 & .776 & \textbf{.927} & \textbf{.940} & .891 & .552 & .638 & .523  \\
		\hline
% 		copy move\cite{kwon2022learning}
		\multirow{5}{*}{\rotatebox[origin= c]{90}{\textit{copy-moving}}} 
	    % & Resfcn${^{*}}$
     %    & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - \\
        & MVSS${^{*}}$
        & .833 & .781 & .703 & .677 & .636 & .544 & .861 & .755 & .668 & .771 & .627 & .527 & .653 & .471 & .366 & .795 & .731 & .640 & .339 & .336 & .258 \\
        & RIML${^{*}}$  
        & .888 & .889 & .856 & .774 & .793 & .737 & .896 & .895 & .861 & .829 & .835 & .788 & .694 & .719 & .657 & .850 & .856 & .811 & .557 & .572 & .493 \\
        & DRAW-MVSS 
        & .901 & .893 & .857 & .839 & .836 & .780 & .915 & .890 & .850 & .862 & .842 & .793 & .804 & .767 & .706 & .871 & .851 & .803 & .631 & .657 & .582 \\
        & DRAW-RIML 
        & .915 & .925 & .910 & .875 & .895 & .868 & .906 & .918 & .899 & .884 & .899 & .874 & .845 & .866 & .829 & .897 & .910 & .888 & .774 & .811 & .768  \\
        & DRAW-HRNet 
        & \textbf{.969} & \textbf{.970} & \textbf{.959} & \textbf{.960} & \textbf{.956} & \textbf{.937} & \textbf{.962} & \textbf{.957} & \textbf{.943} & \textbf{.955} & \textbf{.951} & \textbf{.932} & \textbf{.916} & \textbf{.884} & \textbf{.839} & \textbf{.958} & \textbf{.955} & \textbf{.939} & \textbf{.915} & \textbf{.920} & \textbf{.885}  \\
		\hline
		\multirow{5}{*}{\rotatebox[origin= c]{90}{\textit{inpainting}}} 
	    % & Resfcn${^{*}}$
     %    & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - \\
        & MVSS${^{*}}$
        & .259 & .229 & .172 & .101 & .062 & .039 & .404 & .360 & .263 & .180 & .090 & .054 & .212 & .097 & .058 & .088 & .050 & .030 & .085 & .043 & .026  \\
        & RIML${^{*}}$
        & .126 & .140 & .097 & .035 & .047 & .030 & .132 & .155 & .113 & .014 & .020 & .013 & .001 & .001 & .001 & .037 & .043 & .026 & .068 & .077 & .048  \\
        & DRAW-MVSS 
       & .737 & .752 & .672 & .657 & .682 & .588 & .771 & .756 & .667 & .617 & .645 & .546 & \textbf{.515} & \textbf{.536} & \textbf{.434} & .567 & .595 & .497 & .514 & .561 & .463   \\
        & DRAW-RIML 
        & .663 & .716 & .656 & .457 & .518 & .452 & .667 & .718 & .654 & .348 & .411 & .342 & .091 & .121 & .089 & .366 & .423 & .360 & .284 & .338 & .281 \\
        % & .611 & .666 & .604 & .374 & .434 & .368 & .591 & .649 & .578 & .264 & .323 & .259 & .057 & .077 & .056 & .299 & .354 & .293 & .233 & .282 & .230  \\
        & DRAW-HRNet 
        & \textbf{.776} & \textbf{.791} & \textbf{.735} & \textbf{.754} & \textbf{.760} & \textbf{.685} & \textbf{.788} & \textbf{.771} & \textbf{.697} & \textbf{.719} & \textbf{.714} & \textbf{.625} & .468 & .454 & .346 & \textbf{.732} & \textbf{.735} & \textbf{.647} & \textbf{.686} & \textbf{.704} & \textbf{.618}  \\
        \hline
	\end{tabular}
\end{table*}
}
%% 测试的设定要在这里说一下
% We train all network-based ISP pipelines using RGB images rendered by the libraw library as supervision and these pre-trained ISPs will be frozen when training the RAW protection network.
% In other words, the baseline tries to aid robust forgery detection by slightly modifying the RGB image. 
% For fair comparison, we regulate that the PSNR between $\mathbf{I}$ and $\hat{\mathbf{I}}$ should be within 1dB range with that of DRAW.

% \noindent\textbf{Evaluation metric. }
% We employ the Peak Signal-to-Noise Ratio (PSNR), the Structural SIMilarity (SSIM)~\cite{wang2004image} to evaluate the image fidelity.
% We employ recall, F1 score and intersection-over-union(IoU) to measure the accuracy of image forgery localization. 

% F1 score reports the overall precision and sensitivity of the scheme, where forgery localization can be considered as a binary classification problem, i.e., the pixels within a targeted image can be distinguished into either \textit{original} or \textit{forged}. 

\subsection{Performances}
\label{section_performance}
\noindent\textbf{Image Quality Assessment.}
Fig.~\ref{image_isp_samples} and Table~\ref{table_different_resolution_protected} respectively show the qualitative and quantitative results on the imperceptibility of the protection.
% It can be explained that the network alters the distribution of the collection of protected RGB images where natural images are supposed to not carry similar tiny pattern. 
% The purpose is to make the new distribution more easily distinguishable with the natural image distribution.
Besides, we test the overall image quality of protected images using untrained ISP network, namely, Restormer~\cite{zamir2022restormer}, and another conventional ISP, namely, OpenISP~\cite{tradISP}.
Restormer is originally proposed for image restoration, but we find that the transformer-based architecture also shows excellent performance on RGB image rendering.
OpenISP is another popular open-source ISP pipeline apart from LibRaw, and we customize the pipeline by applying the most essential modules.
We can observe little artifact from the protected version of RAW data and RGB. 
From the augmented difference, DRAW imperceptibly introduce content-related local patterns, which function like digital \textit{locks} onto the pixels and forgery localization is conducted by observing the integrity of these \textit{locks}.
%%%%%% 
% \footnote{https://github.com/mushfiqulalam/isp}

% Figure environment removed
\noindent\textbf{Robustness and Accuracy of Manipulation Localization.}
We conduct comprehensive experiments on RAISE and Canon datasets under different lossy operations.
The qualitative and quantitative comparisons in terms of the Recall, F1 and IoU in the pixel domain are reported in Fig.~\ref{image_detection_hrnet}, Fig.~\ref{image_comparison}, Table~\ref{table_comparison} and Table~\ref{table_comparison_CANON}. 
The results under image color adjustment operations and combined attacks are included \redmarker{in the supplement}.
We find that for DRAW-HRNet, although the images are manipulated by diverse lossy operations, we succeed in localizing the tampered areas. 
If there are no lossy operations, the F1 scores are in most cases above 0.8.
Fig.~\ref{image_detection_hrnet} further provides exampled image manipulation localization results of DRAW-HRNet under different lossy operations.

% ~\footnote{https://github.com/dong03/MVSS-Net}
% ~\footnote{https://github.com/highwaywu/imageforensicsosn}
Next, for fair comparison with previous arts, we finetune MVSS and RIML on RAISE and Canon dataset using the mechanisms proposed in the corresponding papers yet additionally considering \textit{splicing}, \textit{copy-moving} and \textit{inpainting}.
When heavy image lossy operations are present, MVSS fails to detect the tampered content. While RIML exhibits better robustness due to OSN transmission simulation, its performances under blurring or inpainting attacks are still restricted. 
However, training these detectors based on the protected images significantly improves their robustness.
% e.g., the F1 scores are noticeably increased under \textit{inpainting}.
% Besides, while MVSS and RIML are originally weak in detecting inpainting forgeries, DRAW can also help improve their accuracy and robustness against inpainting. 
% ~\footnote{https://github.com/dong03/MVSS-Net}
% ~\footnote{https://github.com/highwaywu/imageforensicsosn}

\setlength{\tabcolsep}{1.1mm}{
\begin{table}
\footnotesize
\renewcommand{\arraystretch}{1}
    \caption{\textbf{Average performance of different methods on forgery localization.} Dataset: CANON.}
    	\label{table_comparison_CANON}
    \centering
    
	\begin{tabular}{c|c|cc|cc|cc|cc}
		\hline
		 &
		\multirow{2}{*}{Models} & 
		 \multicolumn{2}{c}{No attack} &
		 \multicolumn{2}{c}{Rescaling} &
            \multicolumn{2}{c}{JPEG70} &  \multicolumn{2}{c}{GBlur} \\
        \cline{3-10}
        & & F1 & IoU 
        & F1 & IoU 
        & F1 & IoU 
        & F1 & IoU \\
        \hline
        \multirow{5}{*}{\rotatebox[origin=c]{90}{\textit{splicing}}} 
        % & Resfcn${^{*}}$
        % & .440 & .528 & .408 & .064 & .096 & .062 & .396 & .489 & .371 & .181 & .250 & .172 & .457 & .554 & .430 & .443 & .535 & .413 & .443 & .533 & .412 \\
        & MVSS${^{*}}$
        & .610 & .465 & .530 & .390 & .503 & .354 & .210 & .135 \\
        & RIML${^{*}}$
        & .925 & \textbf{.872} & .716 & .609 & .783 & .675 & .136 & .094 \\
        & DRAW-MVSS 
        & .841 & .738 & .875 & .789 & .842 & .739 & .829 & .731  \\
        & DRAW-RIML 
        & .887 & .818 & .925 & .870 & .855 & .769 & .906 & .843 \\
        & DRAW-HRNet 
        & \textbf{.926} & .869 & \textbf{.939} & \textbf{.889} & \textbf{.921} & \textbf{.861} & \textbf{.939} & \textbf{.891}  \\
		\hline
% 		copy move\cite{kwon2022learning}
		\multirow{5}{*}{\rotatebox[origin= c]{90}{\textit{copy-moving}}} 
	    % & Resfcn${^{*}}$
     %    & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - \\
        & MVSS${^{*}}$
        & .727 & .628 & .624 & .526 & .455 & .341 & .371 & .283\\
        & RIML${^{*}}$  
        & .892 & .852 & .789 & .733 & .702 & .619 & .569 & .494  \\
        & DRAW-MVSS 
        & .912 & .879 & .868 & .828 & .832 & .785 & .826 & .776 \\
        & DRAW-RIML 
        & \textbf{.969} & \textbf{.957} & \textbf{.962} & \textbf{.947} & .911 & .882 & .952 & .933\\
        & DRAW-HRNet 
        & .968 & .956 & .960 & .945 & \textbf{.922} & \textbf{.895} & \textbf{.952} & \textbf{.934} \\
		\hline
		\multirow{5}{*}{\rotatebox[origin= c]{90}{\textit{inpainting}}} 
	    % & Resfcn${^{*}}$
     %    & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - \\
        & MVSS${^{*}}$
        & .215 & .150 & .100 & .064 & .136 & .083 & .073 & .045 \\
        & RIML${^{*}}$
        & .102 & .070 & .033 & .021 & .003 & .001 & .012 & .007 \\
        & DRAW-MVSS 
       & .829 & .753 & .676 & .574 & .115 & .076 & .513 & .407 \\
        & DRAW-RIML & \textbf{.949} & \textbf{.918} & \textbf{.889} & \textbf{.836} & \textbf{.406} & \textbf{.326} & \textbf{.849} & \textbf{.784}   \\
        & DRAW-HRNet 
        & .934 & .894 & .867 & .811 & .360 & .284 & .761 & .691 \\
        \hline
	\end{tabular}
\end{table}
}
\begin{table}
\footnotesize
\renewcommand{\arraystretch}{1}
    \caption{\textbf{Generalizability to untrained ISP pipelines or datasets}. ${\mathcal{P}}$ and ${\mathcal{D}}$ are trained on RAISE.}
    	\label{table_transfer}
    \centering
	\begin{tabular}{c|c|c|ccccc}
		\hline
		\multicolumn{2}{c|}{Test Item} & {Forgery} & 
		NoAtk &
		Rescale & JPEG70 & M-Blur & G-Blur\\
        \hline
        \multirow{6}{*}{\rotatebox[origin=c]{90}{\textit{ISP}}} & \multirow{3}{*}{OpenISP}
        & Spli. & .929 & .910 & .837 & .933 & .620 \\
        & & Copy. & .941 & .919 & .843 & .941 & .880  \\
        & & Inpa. & .850 & .820 & .451 & .765 & .756 \\
		\cline{2-8}
		& \multirow{3}{*}{Restormer}
        & Spli. & .946 & .936 & .863 & .941 & .648 \\
        & & Copy. & .961 & .947 & .871 & .948 & .904 \\
        & & Inpa. & .906 & .833 & .487 & .789 & .759 \\
		\hline
% 	\end{tabular}
% \end{table}

% \begin{table}
% \footnotesize
% %\renewcommand{\arraystretch}{1}
%     \caption{Transferability of the proposed scheme using different dataset(cameras). (F1 score)}
%     	\label{table_transfer_dataset}
%     \centering
% 	\begin{tabular}{c|c|c|c|c|c|c}
% 		\hline
		% {Dataset} & {Forgery} & 
		% Identity &
		% JPEG 70 & Scaling &  M-Blur & Bright.\\
  %       \hline
        \multirow{6}{*}{\rotatebox[origin=c]{90}{\textit{Dataset}}} & \multirow{3}{*}{Canon}
        & Spli.& .936 & .925 & .845 & .931 & .596 \\
        & & Copy. & .957 & .930 & .859 & .946 & .881  \\
        & & Inpa. & .805 & .732 & .486 & .710 & .706   \\
		\cline{2-8}
		& \multirow{3}{*}{SIDD}
        & Spli. & .928 & .909 & .832 & .911 & .574  \\
        & & Copy. & .967 & .965 & .891 & .954 & .880 \\
        & & Inpa. & .686 & .628 & .400 & .574 & .554\\
		\hline
	\end{tabular}
\end{table}

% Figure environment removed
 
\noindent\textbf{Generalizability.}
We conduct additional experiments where $\mathcal{P}$ trained on RAISE dataset is applied on different RAW datasets, i.e., Canon and SIDD, and untrained ISP pipelines, i.e., OpenISP and Restormer. 
Table~\ref{table_transfer} shows that raw protection can generalize to untrained cameras and ISP pipelines while preserving promising detection capacity. 
% For instance, given the new ISPs, the F1 scores under JPEG70 attack for \textit{copy-moving} and \textit{splicing} detection are above 0.7, representing successful manipulation localization. 
% Therefore, our method can adapt to untrained ISP pipelines. 

To justify the generalizability to lossy transmission, we randomly handcraft 150 manipulated images, upload them onto several famous OSNs and download them for detection.
% The setting is close to RIML [24].
% We find that OSN transmission attack mainly comprise of lossy JPEG compression.
Also, we test the performance against dual JPEG and salt \& pepper attack ($p=5\%$) which are untrained types for DRAW.
As shown in Table~\ref{table_comparison_hybrid}, DRAW can effectively resist lossy OSN transmission, and its protection remains valuable against unknown lossy operations.

% as its performance does not drop remarkably.

\noindent\textbf{Computational Complexity.}
We compare the computational requirements of MPF-Net in Table~\ref{table_complexity} with SegNet~\cite{segnet}, ShuffleNet~\cite{ma2018shufflenet}, U-Net~\cite{Unet} and ENet~\cite{Enet}, which are famous lightweight models for image segmentation. 
MPF-Net requires lower computing resources, e.g,  only 20.9\% in memory cost and 0.95\% in parameters compared to the classical U-Net.
\subsection{Baseline Comparisons}
\label{section_baseline}
% There is no existing work that utilizes either RAW protection or RGB protection for robust image manipulation localization.
Previous techniques in proactive image forgery detection, e.g., tag retrieval~\cite{wang2021faketagger} or template matching~\cite{asnani2022proactive}, are not suitable for image manipulation localization.
Moreover, Ying et al.~\cite{ying2021image}'s method additionally considers image self-recovery, which inevitably includes much heavier protective signal.
Therefore, we alternatively build two baseline methods that respectively apply pure robust training using our proposed attack layer and apply RGB-domain protection.
In the tests, MVSS is employed as localization network. 
% since RIML has contained OSN simulation during training. 
The quantitative comparison results are reported in Table~\ref{table_comparison_baseline}. Further details regarding the experimental settings for the two baseline methods are included \redmarker{in the supplement}. 
\setlength{\tabcolsep}{1.05mm}{
\begin{table}[!t]
\footnotesize
%\renewcommand{\arraystretch}{1.1}
    \caption{\textbf{Generalizability to lossy transmission and untrained perturbations.} Dataset: RAISE.}
    	\label{table_comparison_hybrid}
    \centering
	\begin{tabular}{c|cc|cc|cc|cc|cc}
		\hline
		\multirow{2}{*}{Forgery} & 
		 \multicolumn{2}{c|}{S\&P} &
		 \multicolumn{2}{c|}{Dual JPEG} &
		 \multicolumn{2}{c|}{Facebook} &
		 \multicolumn{2}{c|}{Weibo} &
		 \multicolumn{2}{c}{WeChat}\\
        \cline{2-11}
        & F1 & IoU 
        & F1 & IoU 
        & F1 & IoU
        & F1 & IoU
        & F1 & IoU\\
        \hline
        splicing
        & .839 & .855 & .657 & .683 & .917 & .920 & .902 & .897 & .763 & .728\\
        copymove
        & .854 & .850 & .692 & .729 & .905 & .910 & .859 & .870 & .637 & .688\\
        inpainting
        & .687 & .711 & .377 & .423 & .665 & .598 & .623 & .577 & .410 & .355\\
		\hline
  
	\end{tabular}
\end{table}
}
\setlength{\tabcolsep}{1.05mm}{
\begin{table}[!t]
\footnotesize
\renewcommand{\arraystretch}{1}
    % \setlength{\tabcolsep}{1.5mm}
	\caption{\textbf{Comparison of computational cost} among lightweight image-to-image-translation or segmentation networks.}
	\centering
	\begin{tabular}{c|c|c|c|c|c}
		\hline
	    & SegNet~\cite{segnet} & ShuffleNet~\cite{ma2018shufflenet}  & U-Net~\cite{Unet} & ENet~\cite{Enet} & MPF-Net   \\
        \hline
        Params & 29.5M & 0.94M & 26.35M & 0.36M & 0.25M \\
        FLOPS & 0.56T & 22.9G & 0.22T & 2.34G & 7.39G \\
        Memory & 465MB & 390MB & 767MB & 46MB & 160MB\\
		\hline
	\end{tabular}
	\label{table_complexity}
\end{table}
}

\noindent\textbf{RAW Protection vs Pure Robust Training.}
Our proposed robust training mechanism reflected in the attack layer is different from that proposed in RIML. 
\ormarker{Specifically, }we render the unprotected RAW files $\mathbf{R}$ using $\mathcal{S}$, which are then attacked by $\mathcal{A}$.
We see that the introduction of robust training can help boost the performance of MVSS.
However, the overall performance is still worse than further applying RAW protection to aid localization. 
In severe degrading cases such as blurring, the performance gap between RAW protection and robust training without protection regarding F1 score is more than ten percent.

\noindent\textbf{RAW protection vs RGB protection.}
% We also use MPF-Net to implement RGB protection scheme.
For fair comparison, we regulate that the overall PSNR on RGB images before and after RGB protection should be above 40 dB, in line with the criterion in Table~\ref{table_different_resolution_protected}. 
We conduct qualitative experiment in Fig~\ref{image_comparison} to evaluate the effectiveness of image protection. 
% Without additional robust training of MVSS and RIML, we test their detection results under three training scenarios: without image protection, with RGB protection, and with RAW protection. 
% Besides, the quantitative results are reported in Table~\ref{table_comparison_baseline}.
According to the experimental results, RGB protection cannot aid robust manipulation localization if the magnitude of RGB modification is restricted.
We also grayscale the augmented injected signal for better visualization and found
% The signal injected by RGB protection is close to a uniform pattern, whereas
that signal injected by RAW protection is more adaptive in magnitude to the image contents. 
One possible reason is that the densely-predicting task requires hiding more information than binary image forgery classification task, making it struggle to maintain high fidelity of the original image.
% , making it hard to maintain high fidelity of the original image.
% A potential reason for this could be that robust localization tasks necessitate more concealment, which makes protection networks struggle to maintain the original image’s high fidelity without strong image prior knowledge.
% The possible reason is that the densely-predicting task requires greater information concealment, and maintaining high fidelity of original images is challenging for protection network without strong image prior knowledge.
% In contrast, RAW protection can improve the detection performance especially in hard situations such as inpainting attack and Gaussian blurring. 
%%% RAW 保护的好处
In comparison, RAW protection can adaptively introduce protection with the help of content-related procedures, e.g., demosaicing and noise reduction, within the subsequent ISP algorithms that suppress unwanted artifacts and biases.
Theoretically, RAW data modification enjoys a much larger search space that allows transformations from the original image into another image with high density upon sampling. 
% In contrast, the injection of RAW protective signal is restricted by subsequent ISP algorithms, such as content-related demosaic and denoising modules, which suppress the possibility of the network being biased towards sacrificing image quality.
% It explains why the overall performance is improved noticeably.
% In addition, RAW images have a larger dynamic range. Therefore, RAW data modification theoretically enjoys a much larger searching space that allows transformations from the original image into another image with high sampling probabilistic density.
% After comparing the predict masks under different attacks in Fig.~\ref{image_comparison}, we can summarize that
% Besides, RGB protection is still limited against heavy post-processing operations such as Gaussian blurring, even with robust schemes, i.e., RIML. 
% However, the introduction of RAW protection greatly enhances the model’s robustness to common lossy operations.
% We can observe that even with certain robust training strategies for RIML, it is difficult to handle post-processing operations with heavier losses. Adding RGB protection can help improve their detection performance, but it still has limited effectiveness against attacks such as Gaussian blurring. The introduction of RAW protection greatly enhances their robustness to common lossy operations.
\setlength{\tabcolsep}{1.1mm}{
\begin{table}
\footnotesize
\renewcommand{\arraystretch}{1}
\caption{\textbf{Comparison with baseline methods on RAISE.}} We verify the importance of RAW protection by comparing the results with those of pure robust training using $\mathcal{A}$ and direct RGB protection. $\mathcal{P}^-$: using $\mathcal{P}$ for RGB protection. $\mathcal{D}$: MVSS$^*$
    	\label{table_comparison_baseline}
    \centering
	\begin{tabular}{c|cccc|cc|cc|cc|cc}
		\hline
		 \multirow{2}{*}{} &
		\multicolumn{4}{c|}{Used Modules} & 
		 \multicolumn{2}{c}{Rescaling} &
		 \multicolumn{2}{c}{JPEG70} &
            \multicolumn{2}{c}{Med. Blur} &  \multicolumn{2}{c}{GBlur} \\
        \cline{2-13}
        & ${\mathcal{P}}$ & ${\mathcal{P}^-}$ & ${\mathcal{A}}$ & ${\mathcal{D}}$
        & F1 & IoU 
        & F1 & IoU 
        & F1 & IoU 
        & F1 & IoU \\
        \hline
        \multirow{4}{*}{\rotatebox[origin=c]{90}{\textit{splicing}}} 
        % & Resfcn${^{*}}$
        % & .440 & .528 & .408 & .064 & .096 & .062 & .396 & .489 & .371 & .181 & .250 & .172 & .457 & .554 & .430 & .443 & .535 & .413 & .443 & .533 & .412 \\
        & & & & \checkmark
        & .609 & .470 & .565 & .415 & .695 & .561 & .211 & .138  \\
        & & & \checkmark& \checkmark
       & \textbf{.668} & \textbf{.534} & .725 & .590 & .762 & .635 & .303 & .207  \\
        & & \checkmark& \checkmark& \checkmark
      & .358 & .253 & .438 & .317 & .487 & .361 & .149 & .097  \\
        & \checkmark & & \checkmark& \checkmark
        & .636 & .514 & \textbf{.789} & \textbf{.680} & \textbf{.770} & \textbf{.658} & \textbf{.419} & \textbf{.301}  \\
		\hline
% 		copy move\cite{kwon2022learning}
		\multirow{4}{*}{\rotatebox[origin= c]{90}{\textit{copy-moving}}} 
	    % & Resfcn${^{*}}$
     %    & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - \\
        & & & & \checkmark
        & .636 & .544 & .471 & .366 & .731 & .640 & .336 & .258   \\
        & & & \checkmark& \checkmark
        & \textbf{.859} & \textbf{.816} & .648 & .582 & .782 & .728 & .528 & .456   \\
        & &\checkmark & \checkmark& \checkmark
         & .490 & .412 & .467 & .382 & .626 & .548 & .268 & .208   \\
        & \checkmark& & \checkmark& \checkmark
        & .836 & .780 & \textbf{.767} & \textbf{.706} & \textbf{.851} & \textbf{.803} & \textbf{.657} & \textbf{.582} \\
		\hline
		\multirow{4}{*}{\rotatebox[origin= c]{90}{\textit{inpainting}}} 
	    % & Resfcn${^{*}}$
     %    & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - \\
        & & & & \checkmark
        & .062 & .039 & .097 & .058 & .050 & .030 & .043 & .026 \\
        & & & \checkmark& \checkmark
        & .605 & .494 & .231 & .159 & .398 & .297 & .342 & .249 \\
        & & \checkmark& \checkmark& \checkmark
         & .387 & .291 & .480 & .371 & .381 & .288 & .374 & .279   \\
        & \checkmark& & \checkmark& \checkmark
        & \textbf{.682} & \textbf{.588} & \textbf{.536} & \textbf{.434} & \textbf{.595} & \textbf{.497} & \textbf{.561} & \textbf{.463}  \\
        \hline
	\end{tabular}
\end{table}
}
\subsection{Ablation Studies}
\label{section_ablation}
% We explore the impact of the key components in our method by evaluating the performance of the networks with varied setups. 
Table~\ref{table_ablation} and Fig.~\ref{image_ablation} respectively show the quantitative and qualitative results of ablation studies. 
In each test, we regulate that the averaged PSNR between $\mathbf{I}$ and $\hat{\mathbf{I}}$, with ISP pipelines evenly applied, should be within the range of 41-43 dB, to ensure imperceptible image protection.

\noindent\textbf{Substituting the architecture of $\mathcal{P}$.}
We first test if using U-Net with a similar amount of parameters or ENet~\cite{Enet} as $\mathcal{P}$ can achieve similar performance on splicing detection.
First, though ENet contains similar amount of parameters compared to MPF-Net, the performance of image manipulation localization using ENet as $\mathcal{P}$ is not satisfactory.
Second, though U-Net with $\textit{DSConv}$ provides much better result, because the channel numbers within each layer are restricted within 48 to save computational complexity, the performance is still worse than our benchmark.
% Figure environment removed
\begin{table}[!t]
\footnotesize
\renewcommand{\arraystretch}{1}
\centering
  \caption{\textbf{Ablation study on DRAW on Nikon using splicing attack.} $^1$: replacing \textit{Conv} layers with \textit{DSConv}.}
\begin{tabular}{cccc}
\hline
\multirow{2}{*}{Test} & \multicolumn{3}{c}{F1} \\
  \cline{2-4}  & NoAtk  & JPEG70 & Mblur \\

\hline
% $\mathcal{P}$(Baseline)+$\mathcal{D}$(HrNet)  & .093  & .176   & .143   \\
% $\mathcal{P}$(Baseline)+$\mathcal{D}$(RIML)  & .093  & .176   & .143   \\
% $\mathcal{P}$(Baseline)+$\mathcal{D}$(MVSS)  & .093  & .176   & .143   \\
% \hline
$\mathcal{P}$ using U-Net$^1$  & .877  & .769  & .535   \\
$\mathcal{P}$ using ENet   & .324  & .137   & .092   \\
\hline
MPF-Net w/o HFC & .844  & .710 & .602\\
MPF-Net w/o DT-CWT & .852 & .751 & .626\\
MPF-Net w/o PFF & .827 & .712 & .667\\
% $\mathcal{D}$ w/o Cali. Module & .893 & .784 & .652\\
\hline
w/o diff from real attack & .842  & .566 & .502\\
using only one ISP Surrogate & .648  & .455 & .267\\
w/o Image Distortion Module & .929  & .245  & .116\\
w/o Color Ajustment Module & .814  & .759  & .641\\
\hline
Full implementation of \textbf{DRAW}  & .929  & \textbf{.838} & \textbf{.696} \\
\hline
\end{tabular}
\label{table_ablation}
\end{table}

\noindent\textbf{Impact of components in MPF-Net.}
The most noticeable difference between MPFNet with previous U-shaped networks is that feature disentanglement can be better ensured even with fewer parameters.
To verify this, we respectively replace the HFC layer and PFF layer with typical alternatives, i.e., vanilla convolution and channel-wise concatenation.
The performances are nearly 5-10 points weaker compared to the MPF-Net setup.
First, DT-CWT is a shift-invariant wavelet transform that comes with limited redundancy.
Second, partial feature fusion and partial connection are more flexible.
The design explicitly keeps some of the features extracted from the current level and directly feeds them into the subsequent block. Therefore, for different levels, the input features will be different, which encourages feature disentanglement.


\noindent\textbf{Impact of pipeline design.}
We also tested the setting of not using the image distortion module or color adjustment module in the pipeline during training.
The result is as expected that the scheme will therefore lack generalizability in overall robustness due to the fact that there are not enough random processes that can simulate the real-world situation.
Besides, not introducing the difference between the real-world and simulated attacks or using only one ISP surrogate model will also impair the overall performance.

%------------------------------------------------------------------------

\section{Conclusions}
We present DRAW that adds imperceptible protective signal to the RAW data against image manipulation. The protection can be transferred into RGB images and resist lossy operations.
% We train a detection network to accurately localize despite the presence of image post-processing operations.
Extensive experiments on typical RAW datasets prove the effectiveness of DRAW.
% We also verify that our novel MPF-Net provides superior performance compared to previous lightweight models for our task.

\medskip
\noindent\textbf{Acknowledgment}. This work was supported by National Natural Science Foundation of China under Grant U20B2051, U1936214, U22B2047, 62072114 and U20A20178. 

%------------------------------------------------------------------------
{\small
\bibliographystyle{ieee_fullname}
\bibliography{iccv_final}
}

\end{document}