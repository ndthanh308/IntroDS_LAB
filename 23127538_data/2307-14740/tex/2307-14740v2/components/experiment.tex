\section{Experiments}

\subsection{Overview of Evaluation Setup}
We assess SmartonAI within the KiCad environment across a diverse set of representative Electronic Design Automation (EDA) workflows. Experiments are run on a macOS Ventura 13.6 system equipped with a 16-core Apple M1 Pro CPU and 32 GB of RAM. Inference is powered by a hybrid backend using vLLM to support Qwen2.5-0.5B for efficient, memory-optimized decoding, and LLaMA3-8B for enhanced reasoning capacity in more complex tasks. 

Our retrieval stack leverages DocHelper, which indexes HTML-structured design manuals, plugin documentation, and scripting guides using FAISS with dense vector representations. The retrieval system enables paragraph-level grounding, allowing the language model to cite exact documentation snippets during multi-turn interactions. Evaluation dimensions span natural language understanding (NLU), procedural toolchain reasoning, zero-shot plugin selection, and multi-step planning for command synthesis and execution.

\subsection{Use Case 1: Multi-Turn Task Decomposition in Chat Plugin}
Figure~\ref{fig:chat_plugin_demo} illustrates the Chat Plugin’s interactive agent-based planning pipeline. Upon receiving a high-level user query (e.g., “assign footprints to components”), SmartonAI’s MainGPT performs intent classification and workflow segmentation. SubGPT then generates a task graph by decomposing the query into actionable subtasks, which are further refined through user confirmation prompts.

Throughout this interaction, QA-GPT continuously grounds clarifications and generated plans in the retrieved design documentation. The web-based interface dynamically renders the most relevant documentation spans, aiding users in decision-making. This pipeline demonstrates the ability of SmartonAI to maintain dialogue state, validate user goals, and adaptively refine its planning strategy.

% Figure environment removed

% Figure environment removed

\subsection{Use Case 2: Plugin Recommendation and Execution in OneCommandLine Plugin}
Figures~\ref{fig:plugin_selection} and~\ref{fig:plugin_execution} demonstrate SmartonAI’s OneCommandLine Plugin, designed for low-friction, single-turn plugin invocation. Given a natural language command (e.g., “rotate a footprint by 90 degrees”), the system performs intent classification, ranks available KiCad plugins using semantic similarity and metadata priors, and elicits argument fields interactively.

The final command is composed and dispatched via KiCad’s embedded Python API. Execution feedback is shown in-line, and any argument correction or re-dispatch is supported through lightweight dialogue repair strategies. The plugin recommendation module employs a combination of retrieval-augmented ranking and few-shot prompting to achieve robustness across diverse user phrasing.

% Figure environment removed

% Figure environment removed

\subsection{Qualitative Feedback and Usability Insights}
Qualitative logs indicate that SmartonAI maintains coherent task threads across multi-turn sessions and exhibits strong generalization to diverse EDA intents. In particular, the Chat Plugin effectively disambiguates user goals and scaffolds complex workflows with minimal guidance. Internal user feedback collected during pilot deployments highlighted reductions in search latency, improved tool discoverability, and better alignment with user mental models of design processes.

Additionally, SmartonAI's document rendering and snippet grounding significantly reduced reliance on external browsing, enabling in-context learning without cognitive context switching.

\subsection{Limitations and Future Evaluation Plans}
Despite its capabilities, SmartonAI currently focuses on KiCad-specific workflows. Scaling to other EDA tools (e.g., Altium Designer, Cadence Allegro) will require modular backend extensions and retraining of retrieval pipelines on domain-specific corpora. Furthermore, occasional hallucinations during synthesis of undocumented plugin commands indicate the need for constrained decoding or retrieval-aware decoding heads.

To further quantify system performance, we plan controlled usability studies using the System Usability Scale (SUS) and NASA-TLX metrics, along with task success rate and dialogue turn count as operational metrics. Additionally, we aim to fine-tune SmartonAI components using collected user traces under a curriculum learning regime to optimize performance across difficulty tiers.
