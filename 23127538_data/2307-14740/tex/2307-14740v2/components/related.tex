% related.tex
\section{Related Work}

Our work builds upon and extends multiple lines of research at the intersection of human-AI interaction, large language model (LLM) orchestration, retrieval-augmented generation (RAG), and computer-aided design automation.

\textbf{Natural language interfaces for software engineering.} The emergence of LLM-powered assistants has catalyzed a paradigm shift in software tooling. GitHub Copilot, based on OpenAI Codex and later GPT-4, exemplifies the integration of natural language programming into mainstream development workflows by translating user intent into executable code \cite{Li_2022}. Similarly, tools such as ChatGPT, Copilot for Office, Notion AI, and CodeWhisperer have demonstrated that natural language interfaces can effectively support complex, multi-modal tasks spanning code generation, document editing, and data analysis \cite{Nam2024LLMCodeUnderstanding,Wang2023NL2Code}. These systems rely heavily on few-shot prompting, in-context learning, and tool use APIs. More recent models such as Claude 3 Opus, Gemini 2.5, and open-source models like Qwen2.5 and LLaMA-3 continue to push the boundaries of long-context reasoning, tool calling, and multi-agent collaboration, suggesting even greater potential for domain-specific adaptation.

\textbf{LLM-based task orchestration and autonomous agents.} HuggingGPT \cite{Shen2023HuggingGPT} and similar agentic frameworks such as LangChain, Auto-GPT, and MetaGPT demonstrate how LLMs can coordinate subtasks across a suite of tools and models, effectively functioning as high-level controllers \cite{Schick2023Toolformer,Yao2023ReAct}. These systems emphasize modularity and dynamic reasoning, enabling LLMs to decompose user instructions and interact with APIs, search tools, plug-ins, or even simulators. Several efforts have extended these frameworks to encompass planning, memory, and reflection modules, further enhancing robustness for real-world deployments. However, despite their generalizability, such orchestration pipelines are rarely applied in engineering tools with graphical user interfaces (GUIs), especially where stateful, real-time manipulation of design artifacts is required.

\textbf{Retrieval-Augmented Generation (RAG).} RAG architectures improve LLM performance by retrieving relevant external knowledge to supplement the model’s context window. Notable systems such as REALM \cite{Guu2020REALM}, RAG \cite{Lewis2020RAG}, Atlas \cite{izacard2022atlasfewshotlearningretrieval}, and RETRO \cite{Borgeaud2022RETRO} have demonstrated superior performance on question-answering and knowledge-intensive tasks. These methods integrate neural retrievers with generative models, allowing users to issue natural language queries whose responses are grounded in structured corpora. In addition, recent applications of RAG in software documentation alignment, legal reasoning, and multi-step scientific QA highlight its effectiveness in structured, domain-specific scenarios.

Within the domain of EDA tools, SmartonAI applies RAG techniques through a module named \textit{DocHelper}, which indexes and embeds tool-specific documentation—including official manuals, plugin descriptions, and usage examples. When users pose natural language queries, DocHelper retrieves the most semantically relevant content using dense retrieval, and conditions the LLM response on this retrieved evidence. This allows SmartonAI to deliver grounded, context-aware explanations and actionable suggestions, tailored to the user’s current task and environment. While prior RAG research has mostly focused on open-domain settings, our work demonstrates its value in GUI-driven, real-time engineering workflows, where fine-grained tool usage guidance is critical but hard to encode manually.

\textbf{Intelligent automation in EDA tools.} The EDA community has historically emphasized automation through domain-specific algorithms, with major progress in placement, routing, logic synthesis, and verification. Tools such as DeepPCB, AutoDMP, and DreamPlace have leveraged ML to enhance design quality and efficiency in backend workflows \cite{Mirhoseini2021ChipPlacement,Cheng2022ChipPG,Pei2023AlphaSyn}. Concurrently, efforts in human-in-the-loop EDA remain nascent. GUI scripting, design wizards, and parameterized templates offer partial support for user guidance, but these approaches are often brittle, inflexible, and non-interactive. Furthermore, the majority of prior work overlooks the early-phase design exploration and iterative debugging stages, where human-AI collaboration could have the greatest impact.

\textbf{Positioning of SmartonAI.} In contrast to prior work, SmartonAI introduces a human-centric AI interface tailored for front-end EDA interaction. It is the first, to our knowledge, to tightly integrate multi-turn conversational LLMs with real-time KiCad tool invocation, plugin discovery, RAG-based document retrieval, and adaptive HTML documentation assembly. Unlike generic code generators or rule-based scripting, SmartonAI dynamically interprets user queries, breaks them down into actionable tasks, and orchestrates both explanation and execution flows through a unified natural language interface. This approach not only enhances usability and accessibility for novice users, but also provides experienced designers with a scalable, language-driven control layer for rapid prototyping and design iteration.
