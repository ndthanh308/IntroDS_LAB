% method.tex
\section{Method}

\subsection{System Overview}

SmartonAI is designed as a modular AI-assistant system for EDA tools, integrating large language models (LLMs), retrieval-based document grounding, and plugin execution interfaces. It comprises two main components: the Chat Plugin and the OneCommandLine Plugin. Both modules are optimized for task decomposition, contextual guidance, and direct interaction with KiCad through a unified conversational interface.

Figure~\ref{fig:chat_plugin} illustrates the architecture of the Chat Plugin. The system begins with language selection and routes user requests through a hierarchical reasoning flow involving a main task selector (MainGPT) and a sub-task selector (SubGPT). The chosen task guides the retrieval and generation of tailored documentation, which is then passed to a QA module. The QA GPT enables iterative dialogue grounded on the generated content, supporting incremental learning and refinement across multiple user turns.

% Figure environment removed

Figure~\ref{fig:commandline_plugin} shows the architecture of the OneCommandLine Plugin. Users input requirements in natural language, and the system recommends appropriate KiCad plugins via semantic matching. After gathering parameter inputs, the plugin is executed with minimal manual intervention. The interaction loop supports multilingual input, plugin auto-completion, and feedback-based correction.

% Figure environment removed

These figures provide a conceptual overview of the SmartonAI system and highlight its conversational structure, dynamic decision routing, and seamless integration with domain-specific tooling. The following subsections detail each module’s mechanism.

\subsection{Chat Plugin: Interactive Task Decomposition}
\label{sec:chat_plugin}

The Chat Plugin facilitates context-aware, multi-turn interactions by decomposing vague user intents into concrete, actionable design steps. It comprises two cascaded LLM components: \textbf{Main-Sub GPT} and \textbf{QA GPT}.

The \textbf{Main-Sub GPT} module performs hierarchical task classification and planning. Given a natural language query, the MainGPT model classifies the user’s intent into one of 20 predefined macro-task categories (e.g., ``netlist verification,'' ``footprint adjustment''). This is implemented using instruction-tuned LLMs (e.g., Qwen2.5, LLaMA-3) augmented with custom prompt templates and task ontologies. The predicted macro-task is passed to a SubGPT, which selects one or more domain-specific subtasks via dense retrieval over a curated task database. This modular decomposition enables interpretable routing, few-shot prompt specialization, and plug-and-play extensibility.

The selected subtasks trigger document synthesis routines and context gathering from the DocHelper. These outputs, along with the user query, are formatted into structured prompt templates and passed to the \textbf{QA GPT}, which maintains conversational state across multiple user rounds. The QA GPT answers queries using a mixture of retrieval-based prompting, RAG grounding, and constrained decoding to improve factuality and clarity.

To improve robustness, the plugin supports dynamic feedback injection, where users can mark responses as ``unsatisfactory'' and provide clarifying constraints. These are encoded using structured system prompts that modify retrieval or generation pipelines. In practice, this allows SmartonAI to support both guided learning and exploratory design workflows.

\subsection{DocHelper: Retrieval-Augmented Document Grounding}

The DocHelper subsystem provides retrieval-augmented grounding for task-aware question answering. It maintains an index over segmented documentation sources, including official KiCad manuals, plugin metadata, code examples, and community Q\&A posts.

Documents are preprocessed using a hybrid pipeline: HTML and Markdown files are chunked into overlapping spans with dynamic window sizes based on semantic boundaries. Each chunk is embedded using a transformer-based encoder model (e.g., BGE-M3, E5-large) and stored in a FAISS vector store with metadata tags (e.g., tool version, component type).

During interaction, each subtask predicted by the Chat Plugin issues a query to the DocHelper index. These queries are constructed via a learned retriever-query generator pipeline or statically templated from subtask descriptors. Retrieved chunks are ranked with a hybrid BM25 and dense similarity score, filtered by task type and user context, and assembled into a single context block for prompt injection.

To enable high accuracy and controllability, DocHelper supports:
\begin{itemize}
  \item \textbf{Context distillation}: Concise prompt-level summarization using GPT-4 or compression-augmented LLMs;
  \item \textbf{Source attribution}: Inline citation or reference links to raw HTML sources;
  \item \textbf{Incremental retrieval}: Feedback-based query rewriting to overcome failures in initial retrieval.
\end{itemize}

Overall, the DocHelper enables SmartonAI to deliver grounded, context-sensitive answers that are both accurate and traceable—critical in domains like EDA where users rely on tool-specific semantics and fine-grained usage details.

\subsection{OneCommandLine Plugin: Plugin Recommendation and Execution}

The OneCommandLine Plugin enables zero-shot plugin invocation from natural language by integrating semantic parsing, plugin metadata grounding, and parameter validation into a coherent workflow. It is particularly suited for users who prefer action-oriented interactions without navigating nested menus or documentation.

The workflow begins by parsing the user input into structured intents using an LLM-based semantic parser. This parser maps the user's natural language request to a latent representation of task goals and expected arguments, leveraging a combination of slot-filling techniques and prompt-conditioned generation (e.g., using in-context few-shot examples).

Next, a plugin retriever ranks available KiCad plugins using dense-sparse hybrid retrieval. Plugin metadata—consisting of descriptions, function signatures, input constraints, and usage examples—is indexed offline using SBERT and BM25. At runtime, a multi-vector MaxSim scoring function is used to match user intents against the plugin corpus.

Upon selecting the most relevant plugin, the system dynamically generates parameter input forms or argument templates. These templates include auto-suggested values based on plugin schema definitions and previous user sessions, enhancing usability and reducing invalid configurations.

Execution is handled by a backend KiCad bridge layer, which abstracts plugin calling into JSON-RPC requests. This layer verifies argument types, applies necessary data normalization, and invokes the plugin in the native environment. Any returned results or error traces are passed back to the LLM layer for summarization and user display.

The OneCommandLine Plugin further supports:
\begin{itemize}
  \item \textbf{Interactive clarification}: When ambiguous inputs are detected, the system engages the user with disambiguation prompts;
  \item \textbf{Multilingual compatibility}: Prompts and plugin metadata are translated using a multilingual embedding model (e.g., LaBSE);
  \item \textbf{User feedback loop}: Plugin invocation results are logged and reused to fine-tune recommendation heuristics.
\end{itemize}

Together, this module operationalizes a command-driven interface for non-expert users and enhances the productivity of experienced engineers by minimizing context switches and manual plugin lookups.

\subsection{Implementation Details}

The SmartonAI system is implemented as a modular, production-grade application that integrates LLM inference, document retrieval, plugin execution, and frontend rendering in a unified architecture.

\paragraph{Frontend Infrastructure.} The desktop interface is built using PyQt5, providing a native, responsive GUI that supports multilingual input, documentation rendering, and real-time chat display. To enable fluid user experience and minimize response latency, the system leverages Server-Sent Events (SSE) for streaming LLM responses, allowing partial completions to be shown token-by-token in the chat window. This improves interactivity during long-form responses and enables early interruption or correction. The frontend also includes dynamically generated form components for plugin execution, parameter filling, and trace display.

\paragraph{Backend Runtime.} The backend consists of three service layers:
\begin{itemize}
    \item \textbf{LLM Layer:} We employ Qwen2.5-0.5B and LLaMA-3-8B as our core models, deployed via vLLM for high-throughput, low-latency serving. Instruction tuning is applied using LoRA adapters fine-tuned on domain-specific EDA queries. Generation is managed via a sliding context window with ChatML-style prompts. In addition to local model inference, we support cloud-based APIs including OpenAI's GPT-4.0 and Gemini 1.5 Pro, which are used for fallback, comparative evaluation, and specific tasks such as summarization or zero-shot classification. For open-source models like Mistral and Deepseek, we utilize Ollama as a lightweight containerized serving layer, enabling model swapping without modifying the orchestration code.

    \item \textbf{Retrieval Layer:} The DocHelper subsystem uses FAISS (HNSW index) to serve vector-based search queries over HTML-parsed KiCad documentation. Embeddings are computed using BGE-M3 or E5-large models and augmented with sparse signals (BM25) for hybrid ranking. When documents include images (e.g., annotated schematics, UI screenshots), we extract the images and compute vision embeddings using a CLIP-like encoder. These embeddings are linked to the corresponding text chunks, allowing the retriever to match queries to visual content. This multimodal indexing improves the ability to answer questions involving visual UI layout, button locations, or design schematics.

    \item \textbf{Plugin Execution Layer:} A JSON-RPC bridge abstracts access to KiCad's internal Python API. Plugins are described in YAML-based schemas, which drive auto-generated UI components and parameter validation. The bridge is robust to runtime exceptions and provides structured feedback for LLM summarization.
\end{itemize}

\paragraph{Asynchronous Orchestration.} The backend services are connected using asynchronous FastAPI routes, with SSE channels for token streaming and WebSocket fallback. Tasks such as plugin recommendation, document retrieval, and chat response are coordinated using asyncio event loops to ensure non-blocking I/O and modular debugging.

\paragraph{Monitoring and Logging.} All user interactions are logged for quality improvement and debugging. Logs include LLM prompts, retrieval hits, selected plugin metadata, execution success/failure, and user feedback ratings. These logs are used for downstream fine-tuning, error tracing, and evaluation benchmarking.

This architecture ensures that SmartonAI is not only effective in guiding users through complex EDA workflows but also robust, extensible, and suitable for continuous deployment in production environments.

