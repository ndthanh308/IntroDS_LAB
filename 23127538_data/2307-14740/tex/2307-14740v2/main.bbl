\begin{thebibliography}{25}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[alt(2025)]{altium-official}
{Altium Designer – PCB Design Software \& Tools}.
\newblock \url{https://www.altium.com/}, 2025.
\newblock Accessed May 2025.

\bibitem[kic(2025)]{kicad-official}
{KiCad EDA - Schematic Capture \& PCB Design Software}.
\newblock \url{https://www.kicad.org/}, 2025.
\newblock Accessed May 2025.

\bibitem[{Anthropic}(2024)]{claude3-anthropic}
{Anthropic}.
\newblock Introducing the next generation of claude.
\newblock \url{https://www.anthropic.com/news/claude-3-family}, 2024.
\newblock Anthropic Blog, March 4, 2024.

\bibitem[Borgeaud et~al.(2022)Borgeaud, Mensch, Hoffmann, Cai, Rutherford, Millican, van~den Driessche, Lespiau, Damoc, Clark, de~Las~Casas, Guy, Menick, Ring, Hennigan, Huang, Maggiore, Jones, Cassirer, Brock, Paganini, Irving, Vinyals, Osindero, Simonyan, Rae, Elsen, and Sifre]{Borgeaud2022RETRO}
Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., van~den Driessche, G., Lespiau, J., Damoc, B., Clark, A., de~Las~Casas, D., Guy, A., Menick, J., Ring, R., Hennigan, T., Huang, S., Maggiore, L., Jones, C., Cassirer, A., Brock, A., Paganini, M., Irving, G., Vinyals, O., Osindero, S., Simonyan, K., Rae, J.~W., Elsen, E., and Sifre, L.
\newblock Improving language models by retrieving from trillions of tokens.
\newblock In \emph{Proceedings of the 39th International Conference on Machine Learning ({ICML})}, 2022.

\bibitem[{Cadence Design Systems}(2025)]{cadence-official}
{Cadence Design Systems}.
\newblock Cadence – computational software for intelligent system design.
\newblock \url{https://www.cadence.com/}, 2025.
\newblock Accessed May 2025.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, et~al.]{codex-chen2021}
Chen, M., Tworek, J., Jun, H., et~al.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}, 2021.

\bibitem[Cheng et~al.(2022)Cheng, Lyu, Li, Ye, Hao, and Yan]{Cheng2022ChipPG}
Cheng, R., Lyu, X., Li, Y., Ye, J., Hao, J., and Yan, J.
\newblock The policy-gradient placement and generative routing neural networks for chip design.
\newblock In \emph{Advances in Neural Information Processing Systems ({NeurIPS})}, volume~35, 2022.

\bibitem[Grattafiori et~al.(2024)]{llama3-grattafiori2024}
Grattafiori, A. et~al.
\newblock The llama 3 herd of models.
\newblock \emph{arXiv preprint arXiv:2407.21783}, 2024.

\bibitem[Guu et~al.(2020)Guu, Lee, Tung, Pasupat, and Chang]{Guu2020REALM}
Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M.
\newblock {REALM}: Retrieval-augmented language model pre-training.
\newblock In \emph{Proceedings of the 37th International Conference on Machine Learning ({ICML})}, pp.\  3929--3938, 2020.

\bibitem[Izacard et~al.(2022)Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, and Grave]{izacard2022atlasfewshotlearningretrieval}
Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and Grave, E.
\newblock Atlas: Few-shot learning with retrieval augmented language models, 2022.
\newblock URL \url{https://arxiv.org/abs/2208.03299}.

\bibitem[Kavukcuoglu(2025)]{gemini25-deepmind}
Kavukcuoglu, K.
\newblock Gemini 2.5: Our most intelligent ai model.
\newblock \url{https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/}, 2025.
\newblock Google DeepMind Blog, Mar 25, 2025.

\bibitem[Lewis et~al.(2020{\natexlab{a}})Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal, K{\"u}ttler, Lewis, Yih, Rockt{\"a}schel, Riedel, and Kiela]{Lewis2020RAG}
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., K{\"u}ttler, H., Lewis, M., Yih, W., Rockt{\"a}schel, T., Riedel, S., and Kiela, D.
\newblock Retrieval-augmented generation for knowledge-intensive {NLP} tasks.
\newblock In \emph{Advances in Neural Information Processing Systems ({NeurIPS})}, volume~33, pp.\  9459--9474, 2020{\natexlab{a}}.

\bibitem[Lewis et~al.(2020{\natexlab{b}})Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal, K{\"u}ttler, Lewis, Yih, Rockt{\"a}schel, Riedel, and Kiela]{rag-lewis2020}
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., K{\"u}ttler, H., Lewis, M., Yih, W.-t., Rockt{\"a}schel, T., Riedel, S., and Kiela, D.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks.
\newblock In \emph{Advances in Neural Information Processing Systems, Vol. 33 (NeurIPS 2020)}, pp.\  9459--9474, 2020{\natexlab{b}}.

\bibitem[Li et~al.(2022)Li, Choi, Chung, Kushman, Schrittwieser, Leblond, Eccles, Keeling, Gimeno, Dal~Lago, Hubert, Choy, de~Masson~d’Autume, Babuschkin, Chen, Huang, Welbl, Gowal, Cherepanov, Molloy, Mankowitz, Sutherland~Robson, Kohli, de~Freitas, Kavukcuoglu, and Vinyals]{Li_2022}
Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles, T., Keeling, J., Gimeno, F., Dal~Lago, A., Hubert, T., Choy, P., de~Masson~d’Autume, C., Babuschkin, I., Chen, X., Huang, P.-S., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J., Mankowitz, D.~J., Sutherland~Robson, E., Kohli, P., de~Freitas, N., Kavukcuoglu, K., and Vinyals, O.
\newblock Competition-level code generation with alphacode.
\newblock \emph{Science}, 378\penalty0 (6624):\penalty0 1092–1097, December 2022.
\newblock ISSN 1095-9203.
\newblock \doi{10.1126/science.abq1158}.
\newblock URL \url{http://dx.doi.org/10.1126/science.abq1158}.

\bibitem[Mirhoseini et~al.(2021)Mirhoseini, Goldie, Yazgan, Jiang, Songhori, Wang, Lee, Johnson, Pathak, Nova, Pak, Tong, Srinivasa, Hang, Tuncer, Le, Laudon, Ho, Carpenter, and Dean]{Mirhoseini2021ChipPlacement}
Mirhoseini, A., Goldie, A., Yazgan, M., Jiang, J.~W., Songhori, E., Wang, S., Lee, Y., Johnson, E., Pathak, O., Nova, A., Pak, J., Tong, A., Srinivasa, K., Hang, W., Tuncer, E., Le, Q.~V., Laudon, J., Ho, R., Carpenter, R., and Dean, J.
\newblock A graph placement methodology for fast chip design.
\newblock \emph{Nature}, 594\penalty0 (7862):\penalty0 207--212, 2021.

\bibitem[Nam et~al.(2024)Nam, Macvean, Hellendoorn, Vasilescu, and Myers]{Nam2024LLMCodeUnderstanding}
Nam, D., Macvean, A., Hellendoorn, V.~J., Vasilescu, B., and Myers, B.~A.
\newblock Using an llm to help with code understanding.
\newblock In \emph{Proceedings of the 46th ACM/IEEE International Conference on Software Engineering (ICSE)}, 2024.

\bibitem[{OpenAI}(2023)]{gpt4-openai}
{OpenAI}.
\newblock {GPT-4 Technical Report}.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Pei et~al.(2023)Pei, Liu, He, Chen, Zheng, Zhu, and Yu]{Pei2023AlphaSyn}
Pei, Z., Liu, F., He, Z., Chen, G., Zheng, H., Zhu, K., and Yu, B.
\newblock {AlphaSyn}: Logic synthesis optimization with efficient monte carlo tree search.
\newblock In \emph{Proceedings of the IEEE/ACM International Conference on Computer-Aided Design ({ICCAD})}, 2023.

\bibitem[{Qwen Team}(2024)]{qwen25-alibaba}
{Qwen Team}.
\newblock {Qwen2.5: A Party of Foundation Models!}
\newblock \url{https://qwenlm.github.io/blog/qwen2.5/}, 2024.
\newblock Alibaba Cloud AI Team Blog, Sept 19, 2024.

\bibitem[Schick et~al.(2023)Schick, Dwivedi{-}Yu, Dess{\`{\i}}, Raileanu, Lomeli, Hambro, Zettlemoyer, Cancedda, and Scialom]{Schick2023Toolformer}
Schick, T., Dwivedi{-}Yu, J., Dess{\`{\i}}, R., Raileanu, R., Lomeli, M., Hambro, E., Zettlemoyer, L., Cancedda, N., and Scialom, T.
\newblock {Toolformer}: Language models can teach themselves to use tools.
\newblock In \emph{Advances in Neural Information Processing Systems ({NeurIPS})}, volume~36, 2023.

\bibitem[Shen et~al.(2023{\natexlab{a}})Shen, Song, Tan, Li, Lu, and Zhuang]{Shen2023HuggingGPT}
Shen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang, Y.
\newblock {HuggingGPT}: Solving {AI} tasks with {ChatGPT} and its friends in {Hugging Face}.
\newblock In \emph{Advances in Neural Information Processing Systems ({NeurIPS})}, volume~36, 2023{\natexlab{a}}.

\bibitem[Shen et~al.(2023{\natexlab{b}})Shen, Song, Tan, Li, Lu, and Zhuang]{hugginggpt-shen2023}
Shen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang, Y.
\newblock Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface.
\newblock \emph{arXiv preprint arXiv:2303.17580}, 2023{\natexlab{b}}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, L., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems, Vol. 30 (NeurIPS 2017)}, pp.\  5998--6008, 2017.

\bibitem[Wang et~al.(2023)Wang, Geng, Lin, Sun, Wen, Liu, Li, Bissyand{\'e}, and Mao]{Wang2023NL2Code}
Wang, S., Geng, M., Lin, B., Sun, Z., Wen, M., Liu, Y., Li, L., Bissyand{\'e}, T.~F., and Mao, X.
\newblock Natural language to code: How far are we?
\newblock In \emph{Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE)}, 2023.

\bibitem[Yao et~al.(2023)Yao, Zhao, Yu, Du, Shafran, Narasimhan, and Cao]{Yao2023ReAct}
Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y.
\newblock React: Synergizing reasoning and acting in language models.
\newblock In \emph{Proceedings of the 11th International Conference on Learning Representations ({ICLR})}, 2023.

\end{thebibliography}
