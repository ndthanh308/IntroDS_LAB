\section{Related Work}
\label{sec:related-work}

\subsection{Depth Estimation}
As opposed to some 3D perception tasks that rely on the LiDAR sensor, \textit{e.g.} LiDAR segmentation \cite{behley2019semanticKITTI,caesar2020nuScenes,kong2023conDA,kong2023rethinking,liu2023segment} and 3D object detection \cite{lang2019pointpillars,second,centerpoint,kong2023robo3d}, monocular depth estimation aims to predict 3D structural information from a single image, which is a more affordable solution in existing perception systems. Based on the source of supervision signals, this task can be further categorized into supervised \cite{silberman2012nyu2,bhat2021adabins}, self-supervised \cite{geiger2012kitti,godard2017unsupervised}, and semi-supervised \cite{kuznietsov2017semi,ji2019semi} depth estimation. Ever since the seminar works \cite{eigen2014depth,garg2016unsupervised,zhou2017sfm,godard2019monodepth2,lee2019big} in this topic, a diverse range of ideas has been proposed, including new designs on network architectures \cite{ranftl2021dpt,zhao2021monovit,zhang2023litemono,johnston2020self,li2022depthformer,li2022binsformer}, optimization functions \cite{zhang2022dynadepth,chen2023tridepth,xie2023revealing,schellevis2019maskocc}, internal feature constraints \cite{yan2021cadepth,zhou2021diffnet,ning2023ait,xue2020dnet}, semantic-aided learning \cite{wang2020sdc,jung2021fsre,li2022simipu}, geometry constraint \cite{watson2021manydepth,tosi2019learning}, mixing-source of depth supervisions \cite{ranftl2022towards,sun2022scdepthv3,li2018megadepth}, and unsupervised model pre-training \cite{caron2021dino,oquab2023dinov2}. Following the conventional ``training-testing'' paradigm, current depth estimation methods are often trained and tested on datasets within similar distributions, while neglecting the natural corruptions that commonly occur in real-world situations. This challenge aims to fill this gap: we introduce the first academic competition for robust out-of-distribution (OoD) depth estimation under corruptions. By shedding light on this new perspective of depth estimation, we hope this challenge could enlighten follow-up research in designing novel network architectures and techniques that improve the reliability of depth estimation systems to meet safety-critical requirements.


\subsection{Out-of-Distribution Perception}
The ability to be generalized across unseen domains and scenarios is crucial for a learning-based system \cite{wang2023survey_robustness}. To pursue superior OoD performance under commonly occurring data corruptions, various benchmarks affiliated with different perception tasks have been established. ImageNet-C \cite{ImageNet-C} represented the first attempt at OoD image classification; the proposed corruption types, such as blurs, illumination changes, perspective transformations, and noise contamination, have been widely adopted by following works in OoD dataset construction. Michaelis \textit{et al.} \cite{michaelis2019dragon} built the large-scale Robust Detection Benchmark upon PASCAL VOC \cite{VOC}, COCO \cite{COCO}, and Cityscapes \cite{cordts2016cityscapes} for OoD object detection. Subsequent works adopt a similar paradigm in benchmarking and analyzing OoD semantic segmentation \cite{Cityscapes-C}, video classification \cite{Kinetics-C}, pose estimation \cite{AdvMix}, point cloud perception \cite{ren2022modelnet-c,PointCloud-C}, LiDAR perception \cite{kong2022laserMix,kong2023robo3d,fong2022panoptic-nuScenes,liu2023segment_any_point_cloud}, bird's eye view perception \cite{xie2023robobev,xie2023robobev_codebase}, and robot navigation \cite{RobustNav}. All the above works have incorporated task-specific corruptions that mimic real-world situations, facilitating the development of robust algorithms for their corresponding tasks. To achieve a similar goal, in this challenge, we resort to the newly-established \textit{KITTI-C} and \textit{NYUDepth2-C} benchmarks \cite{kong2023robodepth_benchmark} to construct our OoD depth estimation datasets. We form two stand-alone tracks, with an emphasis on robust self-supervised and robust fully-supervised depth estimation, respectively, to encourage novel designs for robust and reliable OoD depth estimation.



\subsection{Relevant Competitions}
It is worth mentioning that several previous depth estimation competitions have been successfully held to facilitate their related research areas. The Robust Vision Challenge (RVC) \cite{RVC} aimed to explore cross-domain visual perception across different scene understanding tasks, including reconstruction, optical flow estimation, semantic segmentation, single image depth prediction, \textit{etc}. The Dense Depth for Autonomous Driving (DDAD) Challenge \cite{DDAD} targeted long-range and dense depth estimation from diverse urban conditions. The Mobile AI Challenge \cite{MobileAI} focused on real-time depth estimation on smartphones and IoT platforms. The SeasonDepth Depth Prediction Challenge \cite{hu2022seasondepth} was specialized for estimating accurate depth information of scenes under different illumination and season conditions. The Monocular Depth Estimation Challenge (MDEC) \cite{MDEC,MDEC2} attracted broad attention from researchers and was tailored to tackle monocular depth estimation from complex natural environments, such as forests and fields. The Argoverse Stereo Competition \cite{ArgoverseStereo} encouraged real-time stereo depth estimation under self-driving scenarios. The NTIRE 2023 Challenge on HR Depth from Images of Specular and Transparent Surfaces \cite{NTIRE} mainly aimed at handling depth estimation of non-Lambertian surfaces characterizing specular and transparent materials. Different from previous pursuits, our RoboDepth Challenge is tailored to facilitate robust OoD depth estimation against real-world corruptions. A total of eighteen corruption types are considered, ranging from adverse weather conditions, sensor failure, and noise contamination. We believe this research topic is of great importance to the practical deployment of depth estimation algorithms, especially for safety-critical applications.
