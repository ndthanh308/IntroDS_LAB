\documentclass{article}

% ready for submission
\usepackage[final,nonatbib]{neurips_2021}

% IMPORTANT: if you are submitting attention track, please add the attention option:
% \usepackage[attention]{neurips_2021}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[pagebackref=true,colorlinks,urlcolor=magenta]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage[pdftex]{graphicx}
\usepackage{microtype}      % microtypography
\usepackage[table,dvipsnames,math]{xcolor}         % colors
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{multirow}

\usepackage{caption}
\usepackage{marvosym}
\usepackage{framed}
\RequirePackage{pdflscape} % landscape environment

\definecolor{robo_blue}{RGB}{99, 113, 250}
\definecolor{robo_red}{RGB}{239, 99, 75}
\definecolor{robo_green}{RGB}{0, 180, 139}


\title{
The \textcolor{robo_blue}{Robo}\textcolor{robo_red}{Depth} Challenge: Methods and Advancements Towards Robust Depth Estimation
}

\author{%
  Lingdong Kong$^{1,2,\color{robo_green}\spadesuit}$\quad Yaru Niu$^{3,\color{robo_green}\spadesuit}$\quad Shaoyuan Xie$^{4,5,\color{robo_green}\spadesuit}$\quad Hanjiang Hu$^{3,\color{robo_green}\spadesuit}$\quad Lai Xing Ng$^{6,\color{robo_green}\spadesuit}$\\
  \textbf{Benoit R. Cottereau}$^{7,8,\color{robo_green}\spadesuit}$\quad \textbf{Ding Zhao}$^{3,\color{robo_green}\spadesuit}$\quad \textbf{Liangjun Zhang}$^{9,\color{robo_green}\spadesuit}$\quad \textbf{Hesheng Wang}$^{10,\color{robo_green}\spadesuit}$\\
  \textbf{Wei Tsang Ooi}$^{1,\color{robo_green}\spadesuit}$\quad \textbf{Ruijie Zhu}$^{11}$\quad \textbf{Ziyang Song}$^{11}$\quad \textbf{Li Liu}$^{11}$\quad \textbf{Tianzhu Zhang}$^{11,12}$\\ \textbf{Jun Yu}$^{11}$\quad \textbf{Mohan Jing}$^{11}$\quad \textbf{Pengwei Li}$^{11}$\quad \textbf{Xiaohua Qi}$^{11}$\quad \textbf{Cheng Jin}$^{13}$\quad \textbf{Yingfeng Chen}$^{13}$\\
  \textbf{Jie Hou}$^{13}$\quad \textbf{Jie Zhang}$^{14}$\quad \textbf{Zhen Kan}$^{11}$\quad \textbf{Qiang Ling}$^{11}$\quad \textbf{Liang Peng}$^{15}$\quad \textbf{Minglei Li}$^{15}$\\
  \textbf{Di Xu}$^{15}$\quad \textbf{Changpeng Yang}$^{15}$\quad \textbf{Yuanqi Yao}$^{16}$\quad \textbf{Gang Wu}$^{16}$\quad \textbf{Jian Kuai}$^{16}$\\
  \textbf{Xianming Liu}$^{16}$\quad \textbf{Junjun Jiang}$^{16}$\quad \textbf{Jiamian Huang}$^{17}$\quad \textbf{Baojun Li}$^{17}$\quad \textbf{Jiale Chen}$^{18}$\\
  \textbf{Shuang Zhang}$^{18}$\quad \textbf{Sun Ao}$^{16}$\quad \textbf{Zhenyu Li}$^{16}$\quad \textbf{Runze Chen}$^{19,20}$\quad \textbf{Haiyong Luo}$^{19}$\\
  \textbf{Fang Zhao}$^{20}$\quad \textbf{Jingze Yu}$^{19,20}$
  \\\\
  $^{\color{robo_green}\spadesuit}$The Organizing Team\quad $^1$National University of Singapore\quad $^2$CNRS@CREATE\\
  $^3$Carnegie Mellon University\quad $^4$Huazhong University of Science and Technology\\ $^5$University of California, Irvine\quad $^6$Institute for Infocomm Research, A*STAR\\
  $^7$IPAL, CNRS IRL 2955, Singapore\quad $^8$CerCo, CNRS UMR 5549, Universit√© Toulouse III\\ $^9$Baidu Research\quad $^{10}$Shanghai Jiao Tong University\\
  $^{11}$University of Science and Technology of China\\
  $^{12}$Deep Space Exploration Lab\quad $^{13}$NetEase Fuxi\quad $^{14}$Central South University\\
  $^{15}$Huawei Cloud Computing Technology\quad 
  $^{16}$Harbin Institute of Technology\\
  $^{17}$Individual Researcher\quad $^{18}$Tsinghua University\\
  $^{19}$Beijing University of Posts and Telecommunications\\
  $^{20}$Institute of Computing Technology, Chinese Academy of Sciences
  \\
}


\begin{document}
\maketitle

\begin{abstract}
  Accurate depth estimation under out-of-distribution (OoD) scenarios, such as adverse weather conditions, sensor failure, and noise contamination, is desirable for safety-critical applications. Existing depth estimation systems, however, suffer inevitably from real-world corruptions and perturbations and are struggled to provide reliable depth predictions under such cases. In this paper, we summarize the winning solutions from the RoboDepth Challenge -- an academic competition designed to facilitate and advance robust OoD depth estimation. This challenge was developed based on the newly established KITTI-C and NYUDepth2-C benchmarks. We hosted two stand-alone tracks, with an emphasis on robust self-supervised and robust fully-supervised depth estimation, respectively. Out of more than two hundred participants, nine unique and top-performing solutions have appeared, with novel designs ranging from the following aspects: spatial- and frequency-domain augmentations, masked image modeling, image restoration and super-resolution, adversarial training, diffusion-based noise suppression, vision-language pre-training, learned model ensembling, and hierarchical feature enhancement. Extensive experimental analyses along with insightful observations are drawn to better understand the rationale behind each design. We hope this challenge could lay a solid foundation for future research on robust and reliable depth estimation and beyond. The datasets, competition toolkit, workshop recordings, and source code from the winning teams are publicly available on the challenge website\footnote{The RoboDepth Challenge: \url{https://robodepth.github.io}.}.
\end{abstract}

\input{sections/01-introduction}
\input{sections/02-related-word}
\input{sections/03-summary}
\input{sections/04-results}
\input{sections/05-track1}
\input{sections/06-track2}
\input{sections/07-conclusion}
\input{sections/08-acknowledgements}
\input{sections/09-appendix}

\clearpage

\bibliographystyle{plain}
\bibliography{ref}
\end{document}