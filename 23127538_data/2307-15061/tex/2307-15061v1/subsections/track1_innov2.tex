\subsection{The Innovation Prize Solution: Scent-Depth}
\noindent\textbf{Authors:} \textcolor{gray}{Runze Chen, Haiyong Luo, Fang Zhao, and Jingze Yu.}

\begin{framed}
    \textbf{Summary} - The lack of structural awareness in existing depth estimation systems can lead to significant performance degradation when faced with OoD situations. The \texttt{Scent-Depth} team resorts to structural knowledge distillation to tackle this challenge. A novel graph-based knowledge distillation framework is built, which is able to transfer structural knowledge from a large-scale semantic model to a monocular depth estimation model. Followed by an ensemble between semantic and depth models, the robustness of depth estimation is largely enhanced.
\end{framed}

\subsubsection{Overview}

Single-image depth estimation, also known as monocular depth estimation, is a popular area of research in computer vision due to its diverse range of applications in robotics, augmented reality, and autonomous driving \cite{dong2022survey,ming2021survey,zhao2020survey}. Despite considerable efforts made, accurately estimating the depth of objects from a single 2D image remains a challenging task due to the inherent ill-posed nature of this problem \cite{eigen2014depth}. Models that rely solely on pixel-level features struggle to capture the critical structural information of objects in a scene, which negatively impacts their performance in complex and noisy real-world environments. This lack of structural awareness can lead to significant performance degradation when faced with external disturbances such as occlusion, adverse weather, equipment malfunction, and varying lighting conditions. Therefore, effectively integrating structural information into the model becomes a crucial aspect of enhancing its depth estimation performance in various practical scenarios.

Recovering the 3D structure of a scene from just a single 2D image is difficult. However, researchers have developed unsupervised learning methods that leverage image reconstruction and view synthesis. Through the use of a warping-based view synthesis technique with either monocular image sequences or stereo pairs, the model can learn fundamental 3D object structural features, providing an elegant solution for monocular depth estimation. This approach has been previously described in academic literature \cite{zhou2017sfm,godard2017unsupervised}. Recent research has shown that the combination of Vision Transformers (ViT) \cite{dosovitskiy2020vit} and convolutional features can significantly enhance the modeling capacity for long-range structural features \cite{zhang2023litemono,zhao2021monovit}. The fusion approach leverages the strengths of both ViT \cite{dosovitskiy2020vit} and convolutional features in effectively capturing structural information from images. By incorporating both features, the model can leverage the benefits of the long-range attention mechanism and convolutional features’ ability to extract local features. This approach has shown promising results in improving the accuracy of monocular depth estimation models in complex and noisy real-world environments.

Currently, large-scale vision models demonstrate impressive generalization capabilities \cite{kirillov2023segment}, enabling the effective extraction of scene structural information in various visual contexts. Transferring scene structural knowledge through knowledge distillation from these vision models possesses significant research value. Building upon the RoboDepth Challenge, we aim to design a robust single-image depth estimation method based on knowledge distillation. Specifically, we have leveraged the ample scene structural knowledge provided by large-scale vision models to overcome the limitations of prior techniques. By incorporating these insights, our approach enhances robustness to OoD situations and improves overall performance in practical scenarios.

\subsubsection{Technical Approach}
\noindent\textbf{Task Formulation}.
The main objective of monocular depth estimation is to develop a learning-based model capable of accurately estimating the corresponding depth $\hat{D}_t$ from a monocular image frame $I_t$, within the context of a monocular image sequence $I = \{..., I_t\in\mathbb{R}^{W\times H}, ...\}$ with camera intrinsic determined by $K$. However, the challenge lies in obtaining the ground truth depth measurement $D_t$, which is both difficult and expensive to acquire.

To overcome this, we rely on unsupervised learning methods, which require our monocular depth estimation approach to leverage additional scene structural information in a single image to obtain more accurate results. In monocular depth estimation, our ultimate goal is to synthesize the view $I_{t^\prime \rightarrow t}$ by using the estimated relative pose $\hat{T}_{t \rightarrow t^\prime}$ and the estimated depth map $\hat{D}_t$ with respect to the source frame $I_t^\prime$ and the target frame $I_t$. This synthesis operation can be expressed as follows:
\begin{equation}
\label{track1_innov2_eq1}
I_{t^\prime \rightarrow t} = I_{t^\prime} < \texttt{proj}(\hat{D}_t, \hat{T}_{t \rightarrow t^\prime}, K) >~,
\end{equation}
where \texttt{proj}$(\cdot)$ projects the depth $D_t$ onto the image $I_{t^\prime}$ to obtain the two-dimensional positions, while $<\cdot>$ upsamples the estimation to match the shape of $I_{t^\prime}$ and is the approximation of $I_t$ obtained by projecting $I_{t^\prime}$. The crux of monocular depth estimation is the depth structure consistency; we need to leverage the consistency of depth structure between adjacent frames to accomplish view synthesis tasks. To achieve this, we refer to \cite{zhou2017sfm,zhao2016loss} and utilize $\mathcal{L}_p$ to impose constraints on the quality of re-projected views. This learning objective is defined as follows:
\begin{equation}
\label{track1_innov2_eq2}
\mathcal{L}_p^{u,v}(I_t, I_{t^\prime \rightarrow t}) = \frac{\alpha}{2}(1 - \texttt{ssim}(I_t, I_{t^\prime \rightarrow t})) + (1-\alpha)||I_t - I_{t^\prime \rightarrow t}||_1~,
\end{equation}
\begin{equation}
\label{track1_innov2_eq3}
\mathcal{L}_p = \sum_\mu\mathcal{L}_p^{u,v}(I_t, I_{t^\prime \rightarrow t})~,
\end{equation}
where \texttt{ssim}$(\cdot)$ computes the structural similarity index measure (SSIM) between $I_t$ and $I_{t^\prime \rightarrow t}$, $\mu$ is the auto-mask of dynamic pixels \cite{godard2019monodepth2}, and $\mathcal{L}_p$ calculates the distance measurement by taking the weighted sum of
$\sum_\mu\mathcal{L}_p^{u,v}$ over all pixels $(u, v)$.

Textures on object surfaces can vary greatly and are often not directly related to their three-dimensional structure. As a result, local textures within images have limited correlation with overall scene structure, and our depth estimation model must instead focus on higher-level, global structural features. To overcome this, we adopt the method proposed in \cite{ranjan2019competitive} to model local texture independence by
utilizing an edge-aware smoothness loss, denoted as follows:
\begin{equation}
\label{track1_innov2_eq4}
\mathcal{L}_e = \sum|| \mathbf{e}^{-\nabla I_t} \cdot \nabla \hat{D}_t ||~,
\end{equation}
where $\nabla$ denotes the spatial derivative. By incorporating $\mathcal{L}_e$, our model can better learn and utilize the overall scene structural information, irrespective of local texture variations in the object
surfaces.

\noindent\textbf{Structural Knowledge Distillation}.
The visual scene structural information is vital for a wide range of visual tasks. However, feature representations of different models for distinct tasks exhibit a certain degree of structural correlation in different channels, which is not necessarily one-to-one due to task specificity. We define $A(E, F)$ as the correlation between feature channels of $E$ and $F$, where \texttt{vec} flattens a 2D matrix into a 1D vector as follows:
\begin{equation}
\label{track1_innov2_eq5}
A(E, F) = \frac{|\texttt{vec}(E) \cdot \texttt{vec}(F)^\mathbf{T}|}{||\texttt{vec}(E)||_2 \cdot ||\texttt{vec}(F)^\mathbf{T}||_2}~.
\end{equation}
Here, $A(E, F)$ represents the edge adjacency matrix for state transitions from $E$ to $F$ in the graph space, where all $C$ channels are the nodes.

To leverage this correlation between features, we propose a structure distillation loss $\mathcal{S}$ based on isomorphic graph convolutions. We use graph isomorphic networks based on convolution operations to extract features from $E$ and $F$, resulting in $F^\prime$ and $E^\prime$, respectively. We then calculate the cosine distance between $E$ and $E^\prime$, as well as between $F$ and $F^\prime$, and include these calculations in $\mathcal{S}$ as:
\begin{equation}
\label{track1_innov2_eq6}
F^\prime = \texttt{gin}(\theta^{E\rightarrow F}, F, A(F, E))~,
\end{equation}
\begin{equation}
\label{track1_innov2_eq7}
E^\prime = \texttt{gin}(\theta^{F\rightarrow E}, E, A(E, F))~,
\end{equation}
\begin{equation}
\label{track1_innov2_eq8}
\mathcal{S}(E,F) = \texttt{cosdist}(E, E^\prime) + \texttt{cosdist}(F, F^\prime)~,
\end{equation}
where \texttt{gin}$(\cdot)$ represents the graph isomorphic network function and $\theta$ refers to the parameters of the
graph isomorphic network. This approach aggregates structured information across different tasks, which enables the transfer of such structural information to aid depth estimation.

Semantic objects in a scene carry crucial structural information; the depth of a semantic object in an image exhibits a degree of continuity. To extract image $I_t$’s encoding, we use separate depth and semantic expert encoders to obtain $F^{(d)}_t$ and $E^{(s)}_t$, respectively. The depth feature $F^{(d)}_t\in\mathbb{R}^{C^\prime \times W^\prime \times H^\prime}$ and the semantic feature $E^{(s)}_t\in\mathbb{R}^{C\times L}$ of frame $t$ exhibit a structural correlation that demonstrates graph-like characteristics in the feature embedding.

We align the depth feature $F^{(d)}_t$ with the semantic feature $E^{(s)}_t$ using the alignment function \texttt{align}$(\cdot)$, which is satisfying $E^{(s)}_t = \texttt{align}(F^{(d)}_t)$. To ensure consistent node feature dimensions before constructing the graph structure, we implement the alignment mapping \texttt{align}$(\cdot)$ using bilinear interpolation and convolution layers.

To distill the feature structural information of a powerful expert semantic model to the deep depth estimation model, we employ the structure graph distillation loss $\mathcal{L}_g$, which links the structural correlation between semantic embedding and depth embedding as follows:
\begin{equation}
\label{track1_innov2_eq9}
\mathcal{L}_g = \mathcal{S}(F^{(d)}_t, E^{(s)}_t)~.
\end{equation}
It is worth noting that $\mathcal{L}_g$ enables the cross-domain distillation of semantic structural information from the semantic expert model to the depth estimation model.

\noindent\textbf{Total Loss}.
We propose a method to train a monocular depth model using semantic and structural correlation of visual scenarios. To achieve this goal, we incorporate the idea of knowledge distillation into the design of training constraints for monocular depth estimation. The overall training objective is defined as follows:
\begin{equation}
\label{track1_innov2_eq10}
\min_{\theta}\mathcal{L} = \lambda_p\mathcal{L}_p + \lambda_e\mathcal{L}_e + \lambda_g\mathcal{L}_g~,
\end{equation}
where $\{\lambda_p, \lambda_e, \lambda_g\}$ are loss weights that balance the various constraints during training. We use these weights to determine the level of importance assigned to each constraint.

\noindent\textbf{Model Ensembling}.
To further improve the overall robustness, we use different single-stage monocular depth estimation backbones to train multiple models, resulting in different model configurations $C$ and corresponding depth estimations $D^{(C)}_t$. We then employ a model ensembling approach to improve the robustness of the overall depth estimation $D_t$. The ensembling process involves combining the predictions of each individual model $D^{(C)}_t$ with equal weight, resulting in an ensemble prediction $D_t$, which we use as the final prediction. This approach leverages the diversity of the individual models and improves the overall robustness of the depth estimation. we combine these depth maps using equal weights and obtain the ensemble depth map $D_t$ as follows:
\begin{equation}
\label{track1_innov2_eq11}
D_t = \frac{1}{N}\sum\frac{D^{(C)}_t}{\texttt{median}(D^{(C)}_t)}~,
\end{equation}
where $N$ denotes the total number of configurations, and the \texttt{median}$(\cdot)$ calculates the median value of each depth map $D^{(C)}_t$.

\subsubsection{Experimental Analysis}
\noindent\textbf{Implementation Details}.
We use the publicly accessible KITTI dataset \cite{geiger2012kitti} as the training dataset. This dataset consists of images with a resolution of $1242\times375$. We down-sample each image to $640\times192$ during training. To avoid introducing any corruptions as data augmentation during the training process, we use only raw images from KITTI \cite{geiger2012kitti} for training. During training, we use a batch size of 8 and train the model on two NVIDIA V100 GPUs. We employ a cosine annealing learning rate adjustment strategy with a period of $100$ iterations, setting the minimum and maximum learning rates to $1$e-$5$ and $1$e-$4$, respectively.

\begin{table*}[t]
\caption{Quantitative results of the baselines and our proposed approaches on the RoboDepth competition leaderboard (Track \# 1). The \textbf{best} and \underline{second best} scores of each metric are highlighted in \textbf{bold} and \underline{underline}, respectively.}
\centering\scalebox{0.78}{
\begin{tabular}{l|cccc|cccc}
    \toprule
    \textbf{Method} & \cellcolor{blue!10}\textbf{Abs Rel~$\downarrow$} & \cellcolor{blue!10}\textbf{Sq Rel~$\downarrow$} & \cellcolor{blue!10}\textbf{RMSE~$\downarrow$} & \cellcolor{blue!10}\textbf{log RMSE~$\downarrow$} & \cellcolor{red!10}$\delta<1.25$~$\uparrow$ & \cellcolor{red!10}$\delta<1.25^2$~$\uparrow$ & \cellcolor{red!10}$\delta<1.25^3$~$\uparrow$
    \\\midrule\midrule
    MonoDepth2 \cite{godard2019monodepth2} & $0.221$ & $1.988$ & $7.117$ & $0.312$ & $0.654$ & $0.859$ & $0.938$
    \\
    DPT \cite{ranftl2021dpt} & $0.151$ & $1.073$ & $5.988$ & $0.237$ & $0.782$ & $0.928$ & $0.970$
    \\\midrule
    Ours (MonoDepth2) & $0.156$ & $1.185$ & $5.587$ & $0.235$ & $0.787$ & $0.932$ & $0.973$
    \\
    Ours (MonoDepth2-E) & $0.151$ & $1.058$ & \underline{$5.359$} & \underline{$0.226$} & \underline{$0.794$} & \underline{$0.935$} & \underline{$0.976$}
    \\
    Ours (MonoViT) & \underline{$0.148$} & \underline{$1.030$} & $5.582$ & $0.230$ & $0.790$ & $0.930$ & $0.974$
    \\
    Ours (MonoViT-E) & $\mathbf{0.137}$ & $\mathbf{0.904}$ & $\mathbf{5.276}$ & $\mathbf{0.214}$ & $\mathbf{0.813}$ & $\mathbf{0.941}$ & $\mathbf{0.979}$
    \\\bottomrule
    \end{tabular}
}
\label{tab:track1_innov2_results}
\end{table*}

\noindent\textbf{Comparative \& Ablation Study}.
We adopt MonoDepth2 \cite{godard2019monodepth2}, DPT \cite{ranftl2021dpt}, and MonoViT \cite{zhao2021monovit} as the baseline models in our experiments and compare them with our own models trained in different ways. The quantitative results of different methods are shown in Table~\ref{tab:track1_innov2_results}. As can be seen from the comparative results, our knowledge distillation strategy can significantly improve the depth estimation performance over the original MonoDepth2 \cite{godard2019monodepth2} and MonoViT \cite{zhao2021monovit} under corrupted scenes. We also enable the performance of MonoViT \cite{zhao2021monovit} to exceed that of the large-scale depth estimation model, DPT \cite{ranftl2021dpt}, in the challenging OoD scenarios. Additionally, the model ensembling strategy also improves the performance of depth estimation, with all indicators achieving higher performance than those of a single model setting in our robustness evaluation.

\subsubsection{Solution Summary}
In this work, we explored robust single-image depth estimation and proposed a novel knowledge distillation approach based on graph convolutional networks. We integrated information from different visual tasks for robustness enhancement and showed that the proposed knowledge distillation strategy can effectively improve the performance of MonoDepth and MonoViT and surpassed that of DPT in corrupted scenes. Additionally, the fusion of multiple models further improved the depth estimation results. Our team achieved the innovative prize in the first track of the RoboDepth Challenge.

