\subsection{The \textcolor{robo_green}{3rd} Place Solution: \textcolor{robo_green}{YYQ}}
\noindent\textbf{Authors:} \textcolor{gray}{Yuanqi Yao, GangWu, Jian Kuai, Xianming Liu, and Junjun Jiang.}

\begin{framed}
    \textbf{Summary} - The \texttt{YYQ} team proposes to enhance the OoD robustness of self-supervised depth estimation models via joint adversarial training. Adversarial samples are introduced during training to reduce the sensitivity of depth prediction models to minimal perturbations in the corrupted input data. This approach also ensures the depth estimation models maintain their performance on the in-distribution scenarios while being more robust to different types of data corruptions. Extensive ablation results showcase the effectiveness of the proposed approach.
\end{framed}

\subsubsection{Overview}

Self-supervised depth estimation has emerged as a crucial technique in visual perception tasks, enabling the inference of depth information from 2D images without the use of expensive 3D sensors. However, like conventional depth estimation algorithms, self-supervised depth estimation models trained on ``clean'' datasets often lack robustness and generalization ability when faced with naturally corrupted data. This issue is particularly relevant in real-world scenarios where it is often difficult to ensure that the input data at test time matches the ideal image distribution of the training dataset. Additionally, adversarial attacks can also lead to incorrect depth estimation results, posing safety hazards in applications such as autonomous driving.

To address the above challenges, we propose a method for enhancing the robustness of existing self-supervised depth estimation models via adversarial training. Specifically, adversarial samples are introduced during training to force the depth estimation model to process modified inputs that aim to deceive the discriminator model. By doing so, we can reduce the sensitivity of the self-supervised depth estimation model to minimal perturbations in the input data, ensuring that the model can be trained on a ``clean'' dataset while maintaining a certain degree of robustness to common types of corruptions in the real world.

We believe that our approach will play a significant role in future vision perception applications, providing more reliable depth estimation algorithms for various fields, including autonomous driving \cite{geiger2012kitti}, augmented reality \cite{yucel2021real}, and robot navigation \cite{RobustNav}.  Furthermore, our approach also provides a new aspect to improve the robustness of learning-based models in other self-supervised learning tasks with no extra cost. Experimental results in the RoboDepth competition leaderboard demonstrate that our proposed method can improve the depth estimation scores over existing models by $23\%$ on average while still maintaining their original performance on the ``clean'' KITTI dataset \cite{geiger2012kitti}. These results verify the effectiveness and practicality of our proposed approach.

\subsubsection{Technical Approach}
Our approach can be divided into two main parts as shown in Figure~\ref{fig:3rd_framework}. In the first part, we propose a constrained adversarial training method for self-supervised depth estimation, which allows us to jointly train the depth estimation model and the adversarial noise generator. The adversarial noise generator is designed to produce spatially uncorrelated noises with adversarial properties to counter a specified depth estimation network. The second part is a model ensemble, where we improve the robustness of individual models by fusing between different training settings and model sizes.

% Figure environment removed

\noindent\textbf{Joint Adversarial Training}.
In the joint adversarial training stage, we use a simple method to jointly train an adversarial noise generator and the depth estimation model, making it useful to enhance the robustness of any existing self-supervised depth estimation model. 

Specifically, we first initialize an adversarial noise generator for adding adversarial noise to the depth estimation model, and then jointly train the depth estimation model with the adversarial noise generator. This encourages the trained depth estimation model to be robust to adversarial noise perturbations. In actual implementations, we use the common reprojection loss in self-supervised depth estimation as the supervision loss for optimizing the adversarial noise generator.

To facilitate robustness during feature learning, we now train the depth estimation model $f_\theta$ to minimize the risk under adversarial noise distributions jointly with the noise generator as follows:
\begin{equation}
\label{eq:track1_3rd_eq1}
\min _{\theta} \max _{\phi} \mathbb{E}_{x, y \sim D} \mathbb{E}_{\delta}\left[\mathcal{L}\left(f_{\theta}\left(\operatorname{clip}_{\epsilon}(\boldsymbol{x}+\boldsymbol{\delta})\right), y\right)\right]~,
\end{equation}
where $\boldsymbol{x}+\boldsymbol{\delta} \in[0,1]^{N}$ and $\|\boldsymbol{\delta}\|_{2}=\epsilon$. Here $\mathcal{L}$ represents the photometric reprojection error $L_p$ in MonoDepth2 \cite{godard2019monodepth2}, which can be formulated as follows:
\begin{equation}
\label{eq:track1_3rd_eq2}
L_{p}=\sum_{t^{\prime}} p e\left(I_{t}, I_{t^{\prime} \rightarrow t}\right)~.
\end{equation}
The noise generator $g_\phi$ consists of four $1\times1$ convolutional layers that use Rectified Linear Unit (ReLU) activations and include a residual connection that connects the input directly to the output. To ensure accurate depth estimation on ``clean'' KITTI images, we adopt a strategy that samples mini-batches comprising $50\%$ ``clean'' data and $50\%$ perturbed data. Out of the perturbed data, we use the current state of the noise generator to perturb $30\%$ of images from this source, while the remaining $20\%$ is augmented with samples from previous distributions selected randomly. To facilitate this process, we save the noise generator's states at regular intervals.

The overall framework of our approach is shown in Figure~\ref{fig:3rd_framework}. The network architecture we adopted remained consistent with MonoViT \cite{zhao2021monovit} except for the adversarial network. Firstly, we use ``clean'' multi-frame images as the input to the adversarial noise generator to obtain adversarial multi-frame images. Next, we feed the adversarial and ``clean'' images with a certain proportion into the encoder, decoder, and pose network without changing the original model architecture. We use the image reprojection loss as a constraint for optimizing the corresponding adversarial noise generator. 

\noindent\textbf{Model Ensemble}.
To further enhance the robustness of individual depth estimation models, we use a model ensemble strategy separately on both the small and base variants of MonoViT \cite{zhao2021monovit}, \textit{i.e.} MonoViT-S and MonoViT-B. Specifically, we verify the performance of MonoViT-S with and without a model ensemble, as well as MonoViT-B, which are improved by $3\%$ and $6\%$, respectively. Finally, considering that different model sizes could also affect the model's representation learning by focusing on different features, we ensemble the MonoViT-S and MonoViT-B models to achieve the best possible performance in our final submission. 

\subsubsection{Experimental Analysis}

\noindent\textbf{Implementation Details}.
We implement our proposed approach using PyTorch. The MonoViT-S model is trained on a single NVIDIA GeForce RTX 3090 GPU, while the MonoViT-B model is trained on a single NVIDIA A100 GPU. During training, only images from the training split of the KITTI depth estimation dataset \cite{geiger2012kitti} are used.

\noindent\textbf{Comparative Study}.
As shown in Table~\ref{tab:track1_3rd_at}, Table~\ref{tab:track1_3rd_at2}, and Table~\ref{tab:track1_3rd_ensemble}, our proposed approach improves the performance of existing self-supervised depth estimation models by $23\%$ on average under corrupted scenarios, while still maintaining good performance on the ``clean'' testing dataset.

\noindent\textbf{Joint Adversarial Training}.
Table~\ref{tab:track1_3rd_at} shows the evaluation results of the proposed joint adversarial training. It can be seen that such a training enhancement approach significantly improves the robustness of existing depth estimation models under OoD corruptions. The results from Table~\ref{tab:track1_3rd_at2} further validate that our method not only brings a positive impact on OoD settings but also maintains excellent performance on the ``clean'' testing set. We believe this advantage ensures the accurate estimation of depth information for images in any scenario.

\begin{table*}
  \caption{Quantitative results of the baseline and our proposed joint adversarial training approach on the RoboDepth competition leaderboard (Track \# 1). The \textbf{best} and \underline{second best} scores of each metric are highlighted in \textbf{bold} and \underline{underline}, respectively.}
  \centering\scalebox{0.78}{
  \begin{tabular}{l|cccc|ccc}
    \toprule
    \textbf{Method} & \cellcolor{blue!10}\textbf{Abs Rel~$\downarrow$} & \cellcolor{blue!10}\textbf{Sq Rel~$\downarrow$} & \cellcolor{blue!10}\textbf{RMSE~$\downarrow$} & \cellcolor{blue!10}\textbf{log RMSE~$\downarrow$} & \cellcolor{red!10}$\delta<1.25$~$\uparrow$ & \cellcolor{red!10}$\delta<1.25^2$~$\uparrow$ & \cellcolor{red!10}$\delta<1.25^3$~$\uparrow$
    \\\midrule\midrule
    MonoViT-S & \multirow{2}{*}{$0.160$} & \multirow{2}{*}{$1.238$} & \multirow{2}{*}{$5.935$} & \multirow{2}{*}{$0.245$} & \multirow{2}{*}{$0.768$} & \multirow{2}{*}{$0.920$} & \multirow{2}{*}{$0.967$}
    \\
    (Baseline) & & & & & & 
    \\\midrule
    MonoViT-S & \multirow{2}{*}{\underline{$0.135$}} & \multirow{2}{*}{\underline{$1.066$}} & \multirow{2}{*}{$\mathbf{5.258}$} & \multirow{2}{*}{$0.215$} & \multirow{2}{*}{$0.829$} & \multirow{2}{*}{\underline{$0.942$}} & \multirow{2}{*}{$\mathbf{0.976}$}
    \\
    + Adversarial Training & & & & & & & 
    \\\midrule
    MonoViT-B & \multirow{2}{*}{$\mathbf{0.130}$} & \multirow{2}{*}{$\mathbf{1.027}$} & \multirow{2}{*}{\underline{$5.281$}} & \multirow{2}{*}{$\mathbf{0.213}$} & \multirow{2}{*}{$\mathbf{0.839}$} & \multirow{2}{*}{$\mathbf{0.945}$} & \multirow{2}{*}{\underline{$0.975$}}
    \\
    + Adversarial Training & & & & & & & 
    \\\bottomrule
  \end{tabular}
}
\label{tab:track1_3rd_at}
\end{table*}

\begin{table*}
  \caption{Quantitative results f the baseline and our proposed joint adversarial training approach on the testing set of the KITTI dataset \cite{geiger2012kitti}. The \textbf{best} and \underline{second best} scores of each metric are highlighted in \textbf{bold} and \underline{underline}, respectively.}
  \centering\scalebox{0.78}{
  \begin{tabular}{l|cccc|ccc}
    \toprule
    \textbf{Method} & \cellcolor{blue!10}\textbf{Abs Rel~$\downarrow$} & \cellcolor{blue!10}\textbf{Sq Rel~$\downarrow$} & \cellcolor{blue!10}\textbf{RMSE~$\downarrow$} & \cellcolor{blue!10}\textbf{log RMSE~$\downarrow$} & \cellcolor{red!10}$\delta<1.25$~$\uparrow$ & \cellcolor{red!10}$\delta<1.25^2$~$\uparrow$ & \cellcolor{red!10}$\delta<1.25^3$~$\uparrow$
    \\\midrule\midrule
    MonoViT-S & \multirow{2}{*}{$0.104$} & \multirow{2}{*}{\underline{$0.747$}} & \multirow{2}{*}{$4.461$} & \multirow{2}{*}{$0.177$} & \multirow{2}{*}{$0.897$} & \multirow{2}{*}{$\mathbf{0.966}$} & \multirow{2}{*}{\underline{$0.983$}}
    \\
    + Adversarial Training & & & & & & & 
    \\\midrule
    MonoViT-B & \multirow{2}{*}{\underline{$0.100$}} & \multirow{2}{*}{\underline{$0.747$}} & \multirow{2}{*}{\underline{$4.427$}} & \multirow{2}{*}{\underline{$0.176$}} & \multirow{2}{*}{\underline{$0.901$}} & \multirow{2}{*}{$\mathbf{0.966}$} & \multirow{2}{*}{$\mathbf{0.984}$}
    \\
    (Baseline) & & & & & & & 
    \\\midrule
    MonoViT-B & \multirow{2}{*}{$\mathbf{0.099}$} & \multirow{2}{*}{$\mathbf{0.725}$} & \multirow{2}{*}{$\mathbf{4.356}$} & \multirow{2}{*}{$\mathbf{0.175}$} & \multirow{2}{*}{$\mathbf{0.902}$} & \multirow{2}{*}{$\mathbf{0.966}$} & \multirow{2}{*}{$\mathbf{0.984}$}
    \\
    + Adversarial Training & & & & & & & 
    \\\bottomrule
  \end{tabular}
}
\label{tab:track1_3rd_at2}
\end{table*}

\begin{table*}[t]
  \caption{Quantitative results of the baseline and the model ensemble strategy on the RoboDepth competition leaderboard (Track \# 1). Here AT denotes models trained with the proposed joint adversarial training approach. The \textbf{best} and \underline{second best} scores of each metric are highlighted in \textbf{bold} and \underline{underline}, respectively.}
  \label{table3}
  \centering\scalebox{0.78}{
  \begin{tabular}{l|cccc|ccc}
    \toprule
    \textbf{Method} & \cellcolor{blue!10}\textbf{Abs Rel~$\downarrow$} & \cellcolor{blue!10}\textbf{Sq Rel~$\downarrow$} & \cellcolor{blue!10}\textbf{RMSE~$\downarrow$} & \cellcolor{blue!10}\textbf{log RMSE~$\downarrow$} & \cellcolor{red!10}$\delta<1.25$~$\uparrow$ & \cellcolor{red!10}$\delta<1.25^2$~$\uparrow$ & \cellcolor{red!10}$\delta<1.25^3$~$\uparrow$
    \\\midrule\midrule
    \rowcolor{gray!10}\multicolumn{8}{c}{MonoViT-S} 
    \\\midrule
    + AT & $0.135$ & $1.066$ & $5.258$ & $0.215$ & $0.829$ & $0.942$ & $0.976$
    \\
    + AT + Ensemble & $0.127$ & $0.942$ & \underline{$5.043$} & \underline{$0.205$} & \underline{$0.844$} & \underline{$0.948$} & $\mathbf{0.979}$
    \\\midrule\midrule
    \rowcolor{gray!10}\multicolumn{8}{c}{MonoViT-B}
    \\\midrule
    + AT & $0.130$ & $1.027$  & $5.281$ & $0.213$ & $0.839$ & $0.945$ & $0.975$
    \\
    + AT + Ensemble & \underline{$0.126$} & \underline{$0.917$} & $5.115$ & $0.206$ & $0.842$ & \underline{$0.948$} & \underline{$0.978$}
    \\\midrule\midrule
    \rowcolor{gray!10}\multicolumn{8}{c}{MonoViT-S + MonoViT-B}
    \\\midrule
    + AT + Ensemble & $\mathbf{0.123}$ & $\mathbf{0.885}$ & $\mathbf{4.983}$ & $\mathbf{0.201}$ & $\mathbf{0.848}$ & $\mathbf{0.950}$ & $\mathbf{0.979}$
    \\\bottomrule
  \end{tabular}
}
\label{tab:track1_3rd_ensemble}
\end{table*}

\noindent\textbf{Model Ensemble}.
We evaluate the performance of MonoViT-S and MonoViT-B with and without model ensemble and show the results in Table~\ref{tab:track1_3rd_ensemble}. We observe that such a simple model fusion strategy introduces depth prediction improvements of $3\%$ and $6\%$, respectively. Furthermore, given that different model sizes could cause a model to focus on different features, we combined MonoViT-S and MonoViT-B through ensemble learning to achieve the best possible performance. This validates the effectiveness of the model ensemble in improving the robustness of depth estimation models. 

\subsubsection{Solution Summary}
In this work, we proposed a joint adversarial training approach along with a model ensemble strategy for enhancing the robustness of self-supervised depth estimation models. The adversarial samples introduced during training help reduce the sensitivity of the model to minimal perturbations in the input data, thereby improving the model performance on corrupted scenarios while still maintaining its original performance on ``clean'' datasets. Built upon the strong MonoViT baselines, our approaches achieved promising depth estimation results in this challenging competition. Our team ranked third in the first track of the RoboDepth Challenge.