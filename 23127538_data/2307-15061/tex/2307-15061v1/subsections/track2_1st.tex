\subsection{The \textcolor{robo_blue}{1st} Place Solution: \textcolor{robo_blue}{USTCxNetEaseFuxi}}
\noindent\textbf{Authors:} \textcolor{gray}{Jun Yu, Mohan Jing, Pengwei Li, Xiaohua Qi, Cheng Jin, Yingfeng Chen, and Jie Hou.}

\begin{framed}
    \textbf{Summary} - Most existing depth estimation models are trained solely on ``clean'' data, thereby lacking resilience against real-world interference. To address this limitation, the \texttt{USTCxNetEaseFuxi} team incorporates CutFlip and MAEMix as augmentations to enhance the modelâ€™s generalization capabilities during training. Additionally, appropriate inpainting methods, such as image restoration and super-resolution, are selected and tailored to handle specific types of corruptions during testing. Furthermore, a new classification-based fusion approach is proposed to leverage advantages from different backbones for robustness enhancement.
\end{framed}

\subsubsection{Overview}

To fulfill the needs of real-world perception tasks such as robot vision and robot autonomous driving, significant progress has been made in the field of image depth estimation in recent years. Many high-quality datasets have been constructed by using high-performance sensing elements, such as depth cameras for depth imaging \cite{silberman2012nyu2} and LiDAR sensors for 3D perception \cite{geiger2012kitti}.

However, the current learning-based depth estimation paradigm might become too ideal. Most existing models are trained and tested on clean datasets, without considering the fact that image acquisition often happens in real-world scenes. Even high-performance sensing devices are often affected by factors such as different lighting conditions, lens jittering, and noise perturbations. These factors can disrupt the contour information of objects in the image and interfere with the determination of relative depth. Traditional methods such as filtering cannot effectively eliminate these noise interference, and existing models often lack sufficient robustness to effectively overcome these problems.

In this work, to pursue robust depth estimation against corruptions, we propose an effective solution with specific contributions as follows. Firstly, we conducted experiments on various high-performance models to compare their robustness and ultimately selected the models with the best possible robustness at present \cite{zhao2023unleashing,bhat2023zero,ning2023ait,liu2022swin-v2}. Secondly, we attempted to find a group of non-pixel-level operational data enhancement methods without simulating noise in real conditions. Next, we have chosen some new and effective image restoration methods for reconstructing corrupted images \cite{zamir2022restormer}. Finally, our proposed approach achieved first place in the second track of the RoboDepth Challenge, which proves the effectiveness of our designs.

% Figure environment removed


\subsubsection{Technical Approach}
Our proposed solution consists of three main components: 1) the overall framework, 2) training augmentations, and 3) post-processing techniques. We first elaborate on the overall architecture of our solution. We then describe the two data augmentation techniques involved in our solution. The third component, post-processing, is introduced, including a test-time data processing method and a model fusion approach.

\noindent\textbf{Robust Depth Estimation Framework}.
Our overall model architecture is depicted in Figure~\ref{fig:track2_1st_framework}. We select four existing depth estimation models that have demonstrated outstanding performance on the NYU Depth V2 \cite{silberman2012nyu2} dataset for comparative experiments. Among them, AiT \cite{ning2023ait} exhibited the highest robustness and was found to be highly suitable for tackling this challenging task. MIM-Depth \cite{xie2023revealing} also exhibited relatively superior performance. Consequently, these two models were chosen for subsequent experiments. VPD \cite{zhao2023unleashing} focused on learning advanced semantic information about the scene during training, which was lacking fine-grained capabilities in handling the corrupted images. On the other hand, Zoe \cite{bhat2023zero} emphasized learning relative depth, but the presence of certain corruption types, such as zoom blur, inevitably resulted in significant errors.

During training, we employed two data augmentation techniques: CutFlip and MAEMix. CutFlip involves horizontally cutting the input image and swapping the upper and lower halves. MAEMix, on the other hand, entails simple image blending between the original image and the reconstructed image using a mask. In the testing phase, we applied denoising, deblurring, and super-resolution techniques to the corrupted images to enhance their quality and mitigate the effects brought by noises and interference.

\noindent\textbf{CutFlip}.
We adopt a simple CutFlip operation during training to enhance data diversity. We vertically split the input images into the upper and lower halves, and then flip these two parts along the vertical axis. This process helps to weaken the correlation between the depth and the vertical position of the image. The probability of applying CutFlip is set to $0.5$ and the vertical splitting position is randomly sampled, allowing the model to adapt well to various types of training data.

\noindent\textbf{MAEMix}.
In fact, MAE-based data processing can serve as a powerful data augmentation technology \cite{he2022mae}. The realization of MAE is simple: masking out random patches on the input images and reconstructing the masked regions based on the remaining visual cues. Empirically, masking out most of the input images (such as $75\%$) will form an important and meaningful self-supervised learning task. Strictly speaking, the MAE method belongs to a denoising autoencoder (DAE). The denoising operation in DAE belongs to a kind of representation learning, which destroys the input signal and learns to reconstruct the original and undamaged signals. The encoder and decoder structures of MAE are different and asymmetric. The encoder often encodes the input as a latent representation, while the decoder reconstructs the original signal from this latent representation.

The reconstructed image will have a decrease in clarity compared to the original image, and the image content will also undergo certain changes, which to some extent aligns with our idea of enhancing the model's robustness. An effective approach is to mix the reconstructed image with the original image, thereby transferring the disturbance introduced by MAE reconstruction to the original image. This process helps to incorporate the variations and distortions captured by MAE into the original input, resulting in enhanced feature learning and improved overall robustness of the depth estimation model.

\noindent\textbf{Post-Processing}.
For the test time augmentation, our research focus lies on image restoration operations. The testing set comprises heavily interfered and damaged images. Observing the test set, it was found that noises and blurs accounted for a significant proportion of corruptions, while weather-related corruptions were rarely seen, with only a small number of images showing corruption effects similar to fog. Indeed, as the NYU Depth V2 dataset \cite{silberman2012nyu2} is mainly constructed for indoor scenes, such indoor environments are rarely affected by adverse weather conditions in practical situations. Hence we focus on noise corruptions and blur corruptions during the post-processing.

Before performing image reconstruction, we pre-classified the test set, categorizing different noises and blurs into pre-defined categories, while the remaining images were mainly compressed image quality and color corruptions, which were all classified into another category together.

For images with various types of noises and blurs, we utilized Restormer \cite{zamir2022restormer} for repairing. Restormer \cite{zamir2022restormer} has achieved state-of-the-art results in multiple image restoration tasks, including image de-snowing, single image motion de-blurring, defocus de-blurring (single image and dual pixel data), and image de-noising, outperforming networks such as SwinIR \cite{liang2021swin-ir} and IPT \cite{chen2021ipt}.

\begin{table*}[t]
\caption{Quantitative results of the baselines and different data augmentation techniques on the RoboDepth competition leaderboard (Track \# 2). The \textbf{best} and \underline{second best} scores of each metric are highlighted in \textbf{bold} and \underline{underline}, respectively.}
\centering\scalebox{0.78}{
\begin{tabular}{c|c|c|c|c}
    \toprule
    \textbf{Method} & \textbf{Ref} & \textbf{Data Augmentation} & \cellcolor{red!10}$\delta<1.25$~$\uparrow$ & $\Delta$
    \\\midrule\midrule
    VPD & \cite{zhao2023unleashing} & \textcolor{gray}{None} & $0.743$ & -
    \\\midrule
    ZoeD-M12-N & \cite{bhat2023zero} & \textcolor{gray}{None} & $0.875$ & -
    \\\midrule
    \multirow{4}{*}{AiT-P} & \multirow{4}{*}{\cite{ning2023ait}} & \textcolor{gray}{None} & $0.903$ & \textcolor{gray}{$+0.000$}
    \\
    & & CutFlip & $0.900$ & \textcolor{robo_blue}{$-0.003$}
    \\
    & & MAEMix & $0.902$ & \textcolor{robo_blue}{$-0.001$}
    \\
    & & CutFlip + MAEMix & $0.903$ & \textcolor{gray}{$+0.000$}
    \\\midrule
    \multirow{4}{*}{SwinV2-L 1K-MIM-Depth} & \multirow{4}{*}{\cite{liu2022swin-v2}} & \textcolor{gray}{None} & $0.887$ & \textcolor{gray}{$+0.000$}
    \\
    & & CutFlip & $0.897$ & \textcolor{robo_red}{$+0.010$}
    \\
    & & MAEMix & $\mathbf{0.915}$ & \textcolor{robo_red}{$+0.028$}
    \\
    & & CutFlip + MAEMix & \underline{$0.905$} & \textcolor{robo_red}{$+0.018$}
    \\\bottomrule
    \end{tabular}
}
\label{tab:track2_1st_augmentation}
\end{table*}

On the other hand, for other images, we employed SwinIR \cite{liang2021swin-ir} for super-resolution processing. SwinIR \cite{liang2021swin-ir} has exhibited excellent performance in dealing with image compression and corruption, which can significantly improve image quality. However, color destruction, due to its inherent difficulty in recovery, can only receive a small amount of improvement.

Furthermore, our attempts to utilize Mean Absolute Error for image reconstruction during inference yielded unsatisfactory results. Similarly, we conducted multi-scale testing using super-resolution techniques, but the outcomes were sub-optimal. We speculate that the underwhelming performance of the Mean Absolute Error metric and super-resolution techniques may be attributed to their reliance on algorithmic assumptions to generate image features, rather than capturing genuine content. We conjecture that the discrepancy between algorithmic assumptions and real content could have contributed to the cause of these sub-optimal results.

\subsubsection{Experimental Analysis}
\noindent\textbf{Baselines}.
We selected four state-of-the-art depth estimation models \cite{zhao2023unleashing,bhat2023zero,ning2023ait,liu2022swin-v2} in our experiments. The inference results of these models on the RoboDepth Challenge testing set are presented in Table~\ref{tab:track2_1st_augmentation}. Among all models tested, AiT \cite{ning2023ait} and MIM-Depth \cite{xie2023revealing} demonstrated relatively superior performance, making them highly suitable for handling the challenging OoD depth estimation task. AiT \cite{ning2023ait} itself adopts mask authentication techniques similar to that in natural language processing hence it has exhibited a certain degree of robustness. MIM-Depth \cite{xie2023revealing} balances the global attention and local attention well, making it perform more evenly on the entire image.

\noindent\textbf{Data Augmentation Techniques}.
The results of our ablation experiments on the proposed MAEMix and CutFlip are presented in Table~\ref{tab:track2_1st_augmentation}. We observe that the optimal data augmentation combinations differ for AiT \cite{ning2023ait} and MIM-Depth \cite{xie2023revealing}. After applying data augmentation, AiT \cite{ning2023ait} achieved a depth estimation performance (in terms of the $\delta_1$ score) of $90.3\%$, while MIM-Depth \cite{xie2023revealing} reached a $\delta_1$ accuracy of $91.5\%$.

\begin{table*}[t]
\caption{Quantitative results of the baselines and different post-processing techniques on the RoboDepth competition leaderboard (Track \# 2). The \textbf{best} and \underline{second best} scores of each metric are highlighted in \textbf{bold} and \underline{underline}, respectively.}
\centering\scalebox{0.78}{
\begin{tabular}{c|c|c|c|c}
    \toprule
    \textbf{Method} & \textbf{Ref} & \textbf{Post-Processing} & \cellcolor{red!10}$\delta<1.25$~$\uparrow$ & $\Delta$
    \\\midrule\midrule
    \multirow{3}{*}{AiT-P} & \multirow{3}{*}{\cite{ning2023ait}} & \textcolor{gray}{None} & $0.903$ & \textcolor{gray}{$+0.000$}
    \\
    & & Restormer & $0.921$ & \textcolor{robo_red}{$+0.018$}
    \\
    & & Restormer + SwinIR & $0.922$ & \textcolor{robo_red}{$+0.019$}
    \\\midrule
    \multirow{3}{*}{SwinV2-L 1K-MIM-Depth} & \multirow{3}{*}{\cite{liu2022swin-v2}} & \textcolor{gray}{None} & $0.887$ & \textcolor{gray}{$+0.000$}
    \\
    & & Restormer & \underline{$0.924$} & \textcolor{robo_red}{$+0.037$}
    \\
    & & Restormer + SwinIR & $\mathbf{0.929}$ & \textcolor{robo_red}{$+0.042$}
    \\\bottomrule
    \end{tabular}
}
\label{tab:track2_1st_post}
\end{table*}

\begin{table*}[t]
\caption{Quantitative results of the baselines and different model ensemble techniques on the RoboDepth competition leaderboard (Track \# 2). The \textbf{best} and \underline{second best} scores of each metric are highlighted in \textbf{bold} and \underline{underline}, respectively.}
\centering\scalebox{0.78}{
\begin{tabular}{c|c|c|c|c}
    \toprule
    \textbf{Method} & \textbf{Ref} & \textbf{Model Ensemble} & \cellcolor{red!10}$\delta<1.25$~$\uparrow$ & $\Delta$
    \\\midrule\midrule
    AiT-P & \cite{ning2023ait} & \textcolor{gray}{None} & $0.903$ & $+0.000$
    \\\midrule
    \multirow{2}{*}{AiT-P + MIM-Depth} & \multirow{2}{*}{\cite{xie2023revealing}} & Weighted Average Ensemble & \underline{$0.933$} & $+0.030$
    \\
    & & Classification Ensemble & $\mathbf{0.940}$ & $+0.037$
    \\\bottomrule
    \end{tabular}
}
\label{tab:track2_1st_ensemble}
\end{table*}

\noindent\textbf{Post-Processing Techniques}.
We found that the post-processing for interference reduction using image restoration and super-resolution techniques \cite{zamir2022restormer,liang2021swin-ir} led to an additional improvement in performance. As can be seen from Table~\ref{tab:track2_1st_post}, these two post-processing operations help improve the depth estimation accuracy (in terms of the $\delta_1$ score) with scores reaching $92.20\%$ and $92.87\%$ for AiT \cite{ning2023ait} and MIM-Depth \cite{xie2023revealing}, respectively.

\noindent\textbf{Model Ensemble Techniques}.
Finally, we perform model fusion on the previously obtained results from AiT \cite{ning2023ait} and MIM-Depth \cite{xie2023revealing} using the following strategies: 1) Weighted Average Ensemble, and 2) Classification Ensemble approaches. As can be seen from Table~\ref{tab:track2_1st_ensemble}, the former ensemble strategy achieved a final $\delta_1$ score of $93.3\%$ while the latter one yielded an accuracy $94.0\%$, which is $0.0037$ higher than the baseline.

\subsubsection{Solution Summary}
In this work, we have demonstrated through extensive experiments that incorporating the CutFlip and MAEMix data augmentation techniques during the training process brings a positive effect on enhancing the depth estimation model's robustness. Additionally, the simple and effective inpainting approaches, \textit{i.e.} Restormer and SwinIR, directly improve the depth prediction results under OoD corruptions. Our classification-based model fusion method fully considers the advantages of different depth estimation modes and achieves better results than the traditional fusion approaches, which helped us achieve satisfactory results in the challenge competition. Our team ranked first in the second track of the RoboDepth Challenge.