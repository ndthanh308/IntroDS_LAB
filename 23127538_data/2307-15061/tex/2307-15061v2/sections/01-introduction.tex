\section{Introduction}
\label{sec:introduction}

The robustness of a learning-based visual perception system is among the most important factors that practitioners pursue \cite{wang2023survey_robustness}. In the context of depth estimation, the robustness of a depth prediction algorithm is often coped with its ability to maintain satisfactory performance under perturbation and degradation. Indeed, since most depth estimation systems target estimating structural information from real-world scenes \cite{silberman2012nyu2,geiger2012kitti,cordts2016cityscapes}, it is inevitable for them to deal with unseen data that are distribution-shifted from those seen during training. 

% Figure environment removed

Data distribution shifts often take various forms, such as adversarial attack \cite{cheng2022physical,duan2021advdrop,xie2023adv} and common corruptions \cite{ImageNet-C,geirhos2018imagenet-trained,kar20223d}. While the former aims to trick learning-based models by providing deceptive input, the latter cases -- which are caused by noises, blurs, illumination changes, perspective transformations, \textit{etc}. -- are more inclined to occur in practice. Recently, the RoboDepth benchmark \cite{kong2023robodepth_benchmark} established the first comprehensive study on the out-of-distribution (OoD) robustness of monocular depth estimation models under common corruptions. Specifically, a total of eighteen corruption types are defined, ranging from three main categories: 1) adverse weather and lighting conditions, 2) motion and sensor failure, and 3) noises during data processing. Following the taxonomy, two robustness probing datasets are constructed by simulating realistic data corruptions on images from the KITTI \cite{geiger2012kitti} and NYU Depth V2 \cite{silberman2012nyu2} datasets, respectively. More than forty depth estimation models are benchmarked and analyzed. The results show that existing depth estimation algorithms, albeit achieved promising performance on ``clean'' benchmarks, are at risk of being vulnerable to common corruptions. This study also showcases the importance of considering scenarios that are both in-distribution and OoD, especially for safety-critical applications.

The RoboDepth Challenge has been successfully hosted at the 40th IEEE Conference on Robotics and Automation (ICRA 2023), London, UK. This academic competition aims to facilitate and advance robust monocular depth estimation under OoD corruptions and perturbations. Specifically, based on the newly established \textit{KITTI-C} and \textit{NYUDepth2-C} benchmarks \cite{kong2023robodepth_benchmark}, this competition provides a venue for researchers from both industry and academia to explore novel ideas on: 1) designing network structures that are robust against OoD corruptions, 2) proposing operations and techniques that improve the generalizability of existing depth estimation algorithms, and 3) rethinking potential detrimental components from data corruptions occur under depth estimation scenarios. We formed two stand-alone tracks: one focused on robust self-supervised depth estimation from outdoor scenes and another focused on robust fully-supervised depth estimation from indoor scenes. The evaluation servers of these two tracks were built upon the CodaLab platform \cite{Codalab}. To ensure fair evaluations, we set the following rules and required all participants to obey during this challenge:
\begin{itemize}
    \item All participants must follow the exact same data configuration when training and evaluating their depth estimation algorithms. The use of public or private datasets other than those specified for model training is prohibited.
    \item Since the theme of this challenge is to probe the out-of-distribution robustness of depth estimation models, any use of the eighteen corruption types designed in the RoboDepth benchmark \cite{kong2023robodepth_benchmark} is strictly prohibited, including any atomic operation that is comprising any one of the mentioned corruptions.
    \item To ensure the above rules are followed, each participant was requested to submit the code with reproducible results; the code was for examination purposes only and we manually verified the training and evaluation of each participant's model.
\end{itemize}

We are glad to have more than two hundred teams registered on the challenge servers. Among them, $66$ teams made a total of $1137$ valid submissions; $684$ attempts are from the first track, while the remaining $453$ attempts are from the second track. More detailed statistics are included in Section~\ref{sec:challenge_summary}. In this report, we present solutions from nine teams that have achieved top performance in this challenge. Our participants proposed novel network structures and pre-processing and post-processing techniques, ranging from the following topics:
\begin{itemize}
    \item \textit{Spatial- and frequency-domain augmentations}: Observing that the common data corruptions like blurs and noises contain distinct representations in both spatial and frequency domains \cite{li2022uncertainty,chen2021apr}, new data augmentation techniques are proposed to enhance the feature learning.
    \item \textit{Masked image modeling}: The masking-based image reconstruction approach \cite{he2022mae} exhibits potential for improving OoD robustness; this simple operation encourages the model to learn more robust representations by decoding masked signals from remaining ones.
    \item \textit{Image restoration and super-resolution}: The off-the-shelf restoration and super-resolution networks \cite{zamir2022restormer,liang2021swin-ir,chen2021ipt} can be leveraged to handle degradation during the test time, such as noise contamination,  illumination changes, and image compression.
    \item \textit{Adversarial training}: The joint adversarial objectives \cite{madry2018adversarial} between the depth estimation and a noise generator facilitate robust feature learning; such an approach also maintains the performance on in-distribution scenarios while tackling OoD cases.
    \item \textit{Diffusion-based noise suppression}: The denoising capability of diffusion is naturally suitable for handling OoD situations \cite{rombach2022stable}; direct use of the denoising step in the pre-trained diffusion model could help suppress the noises introduced by different data corruptions. 
    \item \textit{Vision-language pre-training}: Leveraging the pre-trained text features \cite{zhao2023unleashing} and aligning them to the extracted image features via an adapter is popular among recent studies and is proven helpful to improve the performance of various visual perception tasks \cite{chen2023clip2Scene,chen2023towards}.
    \item \textit{Learned model ensembling}: The fusion among multiple models is commonly used in academic competitions; an efficient, proper, and simple model ensembling strategy often combines the advantages of different models and largely improves the performance.
    \item \textit{Hierarchical feature enhancement}: Designing network architectures that are robust against common corruptions is of great value; it has been constantly verified that the CNN-Transformer hybrid structures \cite{zhao2021monovit,zhang2023litemono} are superior in handling OoD corruptions. 
\end{itemize}

The remainder of this paper is organized as follows: Section~\ref{sec:related-work} reviews recent advancements in depth estimation and out-of-distribution perception and summarizes relevant challenges and competitions. Section~\ref{sec:challenge_summary} elaborates on the key statistics, public resources, and terms and conditions of this challenge. Section~\ref{sec:challenge_results} provides the notable results from our participants that are better than the baselines. The detailed solutions of top-performing teams from the first track and the second track of this challenge are presented in Section~\ref{sec:track1} and Section~\ref{sec:track2}, respectively. Section~\ref{sec:conclusion} draws concluding remarks and points out some future directions. Section~\ref{sec:acknowledgements} and Section~\ref{sec:appendix} are acknowledgments and appendix.




