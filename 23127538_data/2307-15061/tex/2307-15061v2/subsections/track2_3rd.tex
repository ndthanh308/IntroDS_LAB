\subsection{The \textcolor{robo_green}{3rd} Place Solution: \textcolor{robo_green}{GANCV}}
\noindent\textbf{Authors:} \textcolor{gray}{Jiamian Huang and Baojun Li.}

\begin{framed}
    \textbf{Summary} - To better handle depth estimation under real-world corruptions, the \texttt{GANCV} team proposes a joint depth estimation solution that combines AiT with masked image modeling depth estimation (MIM-Depth). New techniques related to data augmentation and model ensemble are incorporated to further improve the depth estimation robustness. By combining the advantages of AiT and MIM-Depth, this solution achieves promising OoD depth prediction results and ranks third in the second track of the RoboDepth Challenge.
\end{framed}

\subsubsection{Overview}
Depth estimation plays a crucial role as one of the vital components in visual systems that capture 3D scene structure. Depth estimation models have been widely deployed in practical applications, such as the 3D reconstruction of e-commerce products, mobile robotics, and autonomous driving \cite{geiger2012kitti,silberman2012nyu2,dong2022survey,laga2020survey}. Compared to expensive and power-hungry LiDAR sensors that provide high-precision but sparse depth information, the unique advantages of low-cost and low-power cameras have made monocular depth estimation techniques a relatively popular choice.

Although promising depth estimation results have been achieved, the current learning-based models are trained and tested on datasets within the same distribution. These approaches often ignore the more commonly occurring OoD situations in the real world. The RoboDepth Challenge was recently established to raise attention among the community for robust depth estimation. To investigate the latest advancements in monocular depth estimation, we propose a solution that combines the AiT \cite{ning2023ait}  and masked image modeling (MIM) depth estimation \cite{xie2023revealing}.

AiT \cite{ning2023ait} consists of three components: the tokenizer, detokenizer, and
task solver, as shown in Figure~\ref{fig:track2_3rd_framework}. The tokenizer and detokenizer form a VQ-VAE \cite{oord2017vqvae}, which is primarily used for the automatic encoding and decoding of tokens. The task solver is implemented as an auto-regressive encoder-decoder network, where both the encoder and decoder components combine  Transformer blocks to generate
soft tokens. In summary, the task solver model takes images as inputs, predicts token sequences through autoregressive decoding, and employs VQ-VAE’s decoder to transform the predicted tokens
into the desired output results.

MIM is a sub-task of masked signal prediction, where a portion of input images is masked, and deep networks are employed to predict the masked signals conditioned on the visible ones. In this work, we utilized SimMIM \cite{xie2022simmim} model deep estimation training. SimMIM \cite{xie2022simmim} consists of four major components with simple designs: 1) random masking with a large masked patch size; 2) the masked tokens and image tokens are fed to the encoder together; 3) the prediction head is as light as a linear layer; 4) predicting raw pixels of RGB values as the target with the L1 loss of the direct regression. With these simple designs, SimMIM \cite{xie2022simmim} can achieve state-of-the-art performance on different downstream tasks.

In addition to the network architecture, we also explore model ensemble -- a commonly used technique in competitions, aiming to combine the strengths and compensate for the weaknesses of multiple models by integrating their results. For depth estimation, utilizing a model ensemble can effectively balance the estimated results, especially when there are significant differences in the estimated depth values. It can help mitigate the disparities and harmonize the variations among them.

Lastly, we investigate the choice of different backbones in the depth estimation model. The backbone refers to the selection of the underlying architecture or network as the foundational framework for monocular depth estimation. Currently, in the field of computer vision, the most commonly used backbones are Vision Transformers (ViT) \cite{dosovitskiy2020vit} and Swin Transformers \cite{liu2021swin}. The choice between ViT \cite{dosovitskiy2020vit} and Swin Transformers \cite{liu2021swin} depends on various factors. We use Swin Transformers \cite{liu2021swin} as the backbone of our framework due to its general-purpose nature.

% Figure environment removed

\subsubsection{Technical Approach}
We propose a multi-model fusion approach with a primary focus on integrating the results of two depth estimation models: AiT \cite{ning2023ait} and MIM-Depth \cite{xie2023revealing}. Since the overall framework involves combining the results of two models, the training process is conducted in multiple stages. Therefore, we will first discuss the training of the AiT \cite{ning2023ait} algorithm, followed by the training of MIM-Depth \cite{xie2023revealing}. Finally, we will explain how to integrate the results of these two models together. When presenting the training stages of both models, we will also provide details related to training tricks and the relevant training parameters employed. 

\noindent\textbf{AiT Training}.
As per the competition organizers’ requirements, we train AiT \cite{ning2023ait} on the official training split of NYU Depth V2 \cite{silberman2012nyu2}. Moreover, we have strictly limited the utilization of various data augmentation techniques as specified. We initially employed only two data augmentation techniques: random horizontal flipping and random cropping. These methods were used to augment the training data while ensuring adherence to the specified guidelines. In addition to the aforementioned data augmentation techniques, we also employed two additional data augmentation methods: random brightness and random gamma. These augmentation techniques were sourced from the Monocular-Depth-Estimation-Toolbox \cite{lidepthtoolbox2022} and were implemented in compliance with the competition guidelines. We set all the probability of random augmentations to $0.5$.

As can be observed from Figure \ref{fig:track2_3rd_framework}, the training process of AiT \cite{ning2023ait} can be divided into two stages. In the first stage, we focus on training the VQ-VAE \cite{oord2017vqvae} network, which is a token encoder-decoder model. To enhance the robustness of the VQ-VAE \cite{oord2017vqvae}, we apply random masks to the original ground truth depth maps as inputs. The mask is performed with a masking ratio of $0.5$ and a patch size of $16$, allowing the model to better estimate monocular depth information in real-world scenarios. For monocular depth estimation, the input image size adopted is $480\times480$, with a batch size of $512$ in the first training stage. The AdamW \cite{loshchilov2018adamw} optimizer is used with the base learning rate of $3$e-$4$. During the pre-training process, we utilized a server equipped with eight A100 GPUs and trained the VQ-VAE model \cite{oord2017vqvae} for a total of $100$ epochs. In the fine-tuning process, we continued training VQ-VAE \cite{oord2017vqvae} for at least $50$ additional epochs with a learning rate of $1$e-$5$. This fine-tuning process aims to further optimize the model’s performance on the validation set of the NYU Depth V2 dataset \cite{silberman2012nyu2}. Ultimately, the validation loss of the VQ-VAE \cite{oord2017vqvae} model reduced to around $0.021$. For the data augmentation in this stage, we just use random horizontal flipping and random cropping.

In the second stage, we use the Swin Transformer V2 Large \cite{liu2022swin-v2} as the backbone, which is pre-trained with SimMIM \cite{xie2022simmim}. In the training process, we use the AdamW \cite{loshchilov2018adamw} optimizer with a base learning rate of $2$e-$4$; the weight decay is set to $0.075$. Furthermore, we set the layer decay value of the learning rate to $0.9$ in order to prevent the model from overfitting. This value helps to control the learning rate decay rate for different layers of the depth estimation model, ensuring a balanced optimization process during training. We also set the drop path rate to $0.1$. The total training steps are $15150$ with a batch size of $80$. The step learning rate schedule is used and the learning rate dropped to $2$e-$5$ and $2$e-$6$ at the $7575$-th step and the $12120$-th step, respectively. Regarding data augmentation, in addition to the conventional ones used in VQ-VAE \cite{oord2017vqvae}, we also append random brightness with a limit value from $0.75$ to $1.25$ and a random gamma.

% Figure environment removed

\noindent\textbf{MIM-Depth-Estimation Training}.
In addition to training AiT \cite{ning2023ait}, we also explored the application of MIM-Depth \cite{xie2023revealing}. Unlike the training strategy used for AiT \cite{ning2023ait}, the training of MIM-Depth \cite{xie2023revealing} is performed in an end-to-end manner, which makes the training process relatively simpler. The network architecture of MIM-Depth \cite{xie2023revealing} is depicted in Figure~\ref{fig:track2_3rd_mim}. As mentioned earlier, we select SimMIM \cite{xie2022simmim} as the backbone architecture.

During the training of MIM-Depth \cite{xie2023revealing}, we apply five data augmentation techniques to enhance the model performance. These methods include random masking, random horizontal flipping, random cropping, random brightness adjustment, and random gamma adjustment. For all the random augmentation, a probability of $0.5$ is employed.

\noindent\textbf{Integrating Both Models}.
After completing the training of both models, we experiment with various ensemble strategies to integrate the results of AiT \cite{ning2023ait} and MIM-Depth \cite{xie2023revealing}, aiming to achieve better performance than each individual model. Two ensemble strategies we used include plain averaging and weighted averaging. After conducting comparative experiments, we decide to opt for weighted averaging of the depth estimation results, with weights assigned to the AiT model’s result and the MIM-Depth-Estimation model’s result as $0.6$ and $0.4$, respectively.

\subsubsection{Experimental Analysis}
\noindent\textbf{Implementation Details}.
For the training of MIM-Depth \cite{xie2023revealing}, we select Swin Transformer V2 Large \cite{liu2022swin-v2} as the backbone architecture. Additionally, we apply a trained weight of Swin Transformer V2 Large \cite{liu2022swin-v2} pre-trained on the ImageNet classification dataset as the pre-trained model for MIM-Depth \cite{xie2023revealing}. For monocular depth estimation training, we maintain the same input image size as AiT \cite{ning2023ait}, which consists of $480\times480$ pixels. This consistency in input image size ensures compatibility and facilitates the comparison and integration of results between AiT \cite{ning2023ait} and MIM-Depth \cite{xie2023revealing}. We also apply layer decay during the training, but unlike AiT \cite{ning2023ait}, we set the value to $0.85$. For the drop path rate setting, we apply it with a value of $0.5$. Regarding the data augmentation for masking, we select the mask patch size of $32$ and the mask ratio of $0.1$. In terms of the optimizer, we use AdamW \cite{loshchilov2018adamw} with a learning rate of $5$e-$4$. We use the linear learning rate schedule and set a minimum learning rate to prevent the learning rate from decreasing too quickly. We train the entire model for approximately $25$ epochs on an $8$ V100 GPUs server. The batch size we set during training is $24$.

\begin{table*}[t]
\caption{Quantitative results of the candidate models \cite{ning2023ait,xie2023revealing} with different data augmentation strategies on the RoboDepth competition leaderboard (Track \# 2). \texttt{Aug1} indicates that only random horizontal flipping and random cropping are used during the training. \texttt{Aug2} refers to the addition of random brightness and random gamma as data augmentations on top of \texttt{Aug1}. The \textbf{best} and \underline{second best} scores of each metric are highlighted in \textbf{bold} and \underline{underline}, respectively.}
\centering\scalebox{0.78}{
    \begin{tabular}{l|cccc|ccc}
    \toprule
    \textbf{Method} & \cellcolor{blue!10}\textbf{Abs Rel~$\downarrow$} & \cellcolor{blue!10}\textbf{Sq Rel~$\downarrow$} & \cellcolor{blue!10}\textbf{RMSE~$\downarrow$} & \cellcolor{blue!10}\textbf{log RMSE~$\downarrow$} & \cellcolor{red!10}$\delta<1.25$~$\uparrow$ & \cellcolor{red!10}$\delta<1.25^2$~$\uparrow$ & \cellcolor{red!10}$\delta<1.25^3$~$\uparrow$
    \\\midrule\midrule
    MIM-Depth \cite{xie2023revealing} \textit{w/} \texttt{Aug1} & $0.132$ & $0.091$ & $0.458$ & $0.157$ & $0.849$ & $0.967$ & \underline{$0.990$}
    \\
    MIM-Depth \cite{xie2023revealing} \textit{w/} \texttt{Aug2} & \underline{$0.115$} & \underline{$0.070$} & \underline{$0.414$} & \underline{$0.141$} & \underline{$0.883$} & \underline{$0.976$} & $\mathbf{0.994}$
    \\\midrule
    AiT \cite{ning2023ait} \textit{w/} \texttt{Aug1} & \underline{$0.115$} & $0.076$ & $0.435$ & $0.146$ & $0.871$ & $0.973$ & \underline{$0.990$}
    \\
    AiT \cite{ning2023ait} \textit{w/} \texttt{Aug2} & $\mathbf{0.104}$ & $\mathbf{0.062}$ & $\mathbf{0.405}$ & $\mathbf{0.134}$ & $\mathbf{0.891}$ & $\mathbf{0.981}$ & $\mathbf{0.994}$
    \\\bottomrule
\end{tabular}
}
\label{tab:track2_3rd_aug}
\end{table*}

\begin{table*}[t]
\caption{Quantitative results of MIM-Depth \cite{xie2023revealing} with different masking ratios and patch sizes on the RoboDepth competition leaderboard (Track \# 2). \texttt{p} indicates patch size and \texttt{r} denotes masking ratio. The \textbf{best} and \underline{second best} scores of each metric are highlighted in \textbf{bold} and \underline{underline}, respectively.}
\centering\scalebox{0.78}{
    \begin{tabular}{l|cccc|ccc}
    \toprule
    \textbf{Method} & \cellcolor{blue!10}\textbf{Abs Rel~$\downarrow$} & \cellcolor{blue!10}\textbf{Sq Rel~$\downarrow$} & \cellcolor{blue!10}\textbf{RMSE~$\downarrow$} & \cellcolor{blue!10}\textbf{log RMSE~$\downarrow$} & \cellcolor{red!10}$\delta<1.25$~$\uparrow$ & \cellcolor{red!10}$\delta<1.25^2$~$\uparrow$ & \cellcolor{red!10}$\delta<1.25^3$~$\uparrow$
    \\\midrule\midrule
    MIM-Depth \textit{w/} \texttt{p16-r0.1} & $\mathbf{0.115}$ & $\mathbf{0.070}$ & \underline{$0.418$} & $\mathbf{0.141}$ & \underline{$0.881$} & \underline{$0.971$} & \underline{$0.990$}
    \\
    MIM-Depth \textit{w/} \texttt{p32-r0.0} & \underline{$0.169$} & \underline{$0.157$} & $0.535$ & \underline{$0.186$} & $0.794$ & $0.940$ & $0.978$
    \\
    MIM-Depth \textit{w/} \texttt{p32-r0.1} & $\mathbf{0.115}$ & $\mathbf{0.070}$ & $\mathbf{0.414}$ & $\mathbf{0.141}$ & $\mathbf{0.883}$ & $\mathbf{0.976}$ & $\mathbf{0.994}$
    \\\bottomrule
\end{tabular}
}
\label{tab:track2_3rd_mask}
\end{table*}

\begin{table*}[t]
\caption{Quantitative results of MIM-Depth \cite{xie2023revealing}, AiT \cite{ning2023ait}, our ensemble model, and other participants on the RoboDepth competition leaderboard (Track \# 2). The \textbf{best} and \underline{second best} scores of each metric are highlighted in \textbf{bold} and \underline{underline}, respectively.}
\centering\scalebox{0.78}{
    \begin{tabular}{l|cccc|ccc}
    \toprule
    \textbf{Method} & \cellcolor{blue!10}\textbf{Abs Rel~$\downarrow$} & \cellcolor{blue!10}\textbf{Sq Rel~$\downarrow$} & \cellcolor{blue!10}\textbf{RMSE~$\downarrow$} & \cellcolor{blue!10}\textbf{log RMSE~$\downarrow$} & \cellcolor{red!10}$\delta<1.25$~$\uparrow$ & \cellcolor{red!10}$\delta<1.25^2$~$\uparrow$ & \cellcolor{red!10}$\delta<1.25^3$~$\uparrow$
    \\\midrule\midrule
    USTCxNetEaseFuxi & $\mathbf{0.088}$ & \underline{$0.046$} & \underline{$0.347$} & $\mathbf{0.115}$ & $\mathbf{0.940}$ & \underline{$0.985$} & \underline{$0.996$}
    \\
    OpenSpaceAI & \underline{$0.095$} & $\mathbf{0.045}$ & $\mathbf{0.341}$ & \underline{$0.117$} & \underline{$0.928$} & $\mathbf{0.990}$ & $\mathbf{0.998}$
    \\
    AIIA-RDepth & $0.123$ & $0.088$ & $0.480$ & $0.162$ & $0.861$ & $0.975$ & $0.993$
    \\\midrule
    \textbf{MIM-Depth~(Ours)} & $0.115$ & $0.070$ & $0.414$ & $0.141$ & $0.883$ & $0.976$ & $0.994$
    \\
    \textbf{AiT~(Ours)} & $0.104$ & $0.062$ & $0.405$ & $0.134$ & $0.891$ & $0.981$ & $0.994$
    \\
    \textbf{Ensemble~(Ours)} & $0.104$ & $0.060$ & $0.391$ & $0.131$ & $0.898$ & $0.982$ & $0.995$
    \\\bottomrule
\end{tabular}
}
\label{tab:track2_3rd_leaderboard}
\end{table*}

We conduct several comparative experiments focusing on the selection of data augmentation methods, masking strategies, and ensemble strategies. All experimental results are obtained using the test set of the second track of the RoboDepth competition.

\noindent\textbf{Data Augmentations}.
Regarding the use of data augmentations, we compare multiple combinations and present the results in Table~\ref{tab:track2_3rd_aug}. We first establish a data augmentation combination that includes random horizontal flipping and random cropping, both with a probability of $0.5$, dubbed \texttt{Aug1}. We apply this combination to preprocess the training data for both MIM-Depth \cite{xie2023revealing} and AiT \cite{ning2023ait}. We also form another data augmentation combination by adding random brightness variation and random gamma adjustment to the previous combination; we denote this strategy as \texttt{Aug2}. Both of these augmentations are applied with a probability value of $0.5$.

\noindent\textbf{Masking Strategy}.
We conduct experiments with two sets of masking strategies on the MIM-Depth\cite{xie2023revealing}. In the first set, we select the patch size to $32$, while in the second set, the patch size is $16$. Both sets have a mask ratio of $0.1$. As shown in Table~\ref{tab:track2_3rd_mask}, we observe that the two different mask patch sizes have a minimal impact on the final depth estimation results. Additionally, we compare the scenarios where no masking ratio is set (baseline). It can be seen that masking-based modeling has a significant impact on the model's robustness.

\noindent\textbf{Ensemble Strategy}.
In terms of ensemble strategies, we compare the plain averaging and weighted averaging methods. As shown in Table~\ref{tab:track2_3rd_leaderboard}, we can see that the weighted averaging method outperforms the simple averaging method. We also observe that the optimal weights for the ensemble are $0.6$ for AiT \cite{ning2023ait} and $0.4$ for MIM-Depth \cite{xie2023revealing}. Furthermore, the ensemble approach achieves better performance compared to individual models.

\subsubsection{Solution Summary}
In this work, we presented a collaborative model ensemble solution for robust monocular depth estimation and conducted various experimental analyses to demonstrate its effectiveness. The proposed solution primarily consists of combining AiT and MIM-Depth, with an in-depth discussion on the use of data augmentation and model ensemble techniques. This solution is employed in the second track of the RoboDepth Challenge. Ultimately, we achieved a third-place ranking in the second track of this competition.