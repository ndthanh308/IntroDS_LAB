\subsection{The \textcolor{robo_red}{2nd} Place Solution: \textcolor{robo_red}{OpenSpaceAI}}
\noindent\textbf{Authors:} \textcolor{gray}{Li Liu, Ruijie Zhu, Ziyang Song, and Tianzhu Zhang.}

\begin{framed}
    \textbf{Summary} - The \texttt{OpenSpaceAI} team proposes a Robust Diffusion model for Depth estimation (RDDepth) to address the problem of single-image depth estimation on OoD datasets. RDDepth takes the use of VPD as the baseline for utilizing the denoising capability of the diffusion model, which is naturally suitable for handling such a problem. Additionally, the high-level scene priors provided by the text-to-image diffusion model are leveraged for robust predictions. Furthermore, the AugMix data augmentation is incorporated to further enhance the model’s robustness.
\end{framed}

\subsubsection{Overview}
Monocular depth estimation is a fundamental task in computer vision and is crucial for scene understanding and other downstream applications. In real practice, there are inevitably some corruptions (\textit{e.g.} rain), which hinder safety-critical applications. Many learning-based monocular depth estimation methods \cite{bhat2021adabins,li2022binsformer,yuan2022newcrfs,li2022depthformer,tang2021transdepth,liu2015deep} train and evaluate in the subsets of an individual benchmark. Therefore, they tend to overfit a specific dataset, which leads to poor performance on OoD datasets. The second track of the RoboDepth Challenge provides the necessary data and toolkit for the supervised learning-based model to handle OoD depth estimation. The objective is to accurately estimate the depth information while training only on the clean NYU Depth V2 \cite{silberman2012nyu2} dataset. Our goal is to improve the model’s generalization ability across real-world OoD scenarios.

To address this issue, we propose a Robust Diffusion model for Depth estimation (RDDepth). RDDepth takes VPD \cite{zhao2023unleashing} as the baseline, which aims to leverage the high-level knowledge learned in the text-to-image diffusion model for visual perception. We believe the knowledge from VPD \cite{zhao2023unleashing} can also benefit the robustness of depth predictors since the prior of scenes is given. Moreover, the denoising capability of diffusion is naturally suitable for handling OoD situations.

Instead of using the step-by-step diffusion pipeline, we simply employ the autoencoder as a backbone model to directly consume the natural images without noise and perform a single extra denoising step with proper prompts to extract the semantic information. Specifically, RDDepth takes the RGB image as input and extracts features by the pre-trained encoder of VQGAN \cite{esser2021vqgan}, which projects the image into the latent space. The text input is defined by the template of \textit{``a photo of a [CLS]''}, and then the CLIP \cite{radford2021clip} text encoder is applied to obtain text features.

To solve the domain gap when transferring the text encoder to depth estimation, we adopt an adapter to refine the text features obtained by the CLIP \cite{radford2021clip}. The latent feature map and the refined text features are then fed into UNet \cite{ronneberger2015unet} to obtain hierarchical features, which are used by the depth decoder to generate the final depth map. In addition, we employed the AugMix \cite{hendrycks2020augmix} data augmentation, which does not include any of the $18$ types of corruption and their atomic operations in the original RoboDepth benchmark. We find that, within a certain range, more complex data augmentation enables the model to learn more robust scene priors, thereby enhancing its generalization when tested on corrupted data.

% Figure environment removed

\subsubsection{Technical Approach}
In this section, we present RDDepth, a framework that achieves promising robustness in OoD depth estimation scenarios. RDDepth is built upon VPD \cite{zhao2023unleashing} -- a framework that exploits the semantic information of a pre-trained text-to-image diffusion model in visual perception tasks. The key idea of VPD \cite{zhao2023unleashing} is to investigate how to fully extract the pre-trained high-level knowledge in a pre-trained diffusion model. We find such knowledge also benefits the prediction of corrupted images thanks to their semantic information. The overall framework of our RDDepth is illustrated in Figire~\ref{fig:track2_2nd_framework}.

\noindent\textbf{Framework Overview}.
Our RDDepth is based on VPD \cite{zhao2023unleashing}, a model that builds upon the foundation of the popular Stable Diffusion \cite{rombach2022stable} and conducts the denoising process in a learned latent space with a UNet architecture. As for Stable Diffusion \cite{rombach2022stable}, there is adequate high-level knowledge due to the weak supervision of the natural language during pre-training. We believe that this high-level knowledge can, to some extent, mitigate the influence of the corruptions in the feature space, thereby guiding the recovery of more accurate depth maps in the depth prediction head. Therefore, the key idea is to investigate how to effectively leverage the advanced knowledge of the diffusion model to steer subsequent models in monocular depth estimation.

Specifically, RDDepth firstly uses encoder $\epsilon$ in VQGAN \cite{esser2021vqgan} to extract image features and obtain the representation of latent space. Then we hope to extract corresponding text features from class names by the simple template \textit{``a photo of a [CLS]''}. Moreover, We align the text features to the image features by an adapter. This design enables us to retain the pre-trained knowledge of the text encoder to the fullest extent while reducing the domain discrepancy between the pre-training task and the depth estimation task. After that, we feed the latent feature map and the conditioning inputs to the pre-trained network (usually implemented as a UNet \cite{ronneberger2015unet}). We do not use the step-by-step diffusion pipeline, which is common in other works. Instead, we simply consider it as a backbone. In other words, no noise is added to the latent feature map during the denoising process since we set $t = 0$. Then, we use only one denoising step by UNet \cite{ronneberger2015unet} to obtain the features.

The hierarchical feature $\mathcal{F}$ can be easily obtained from the last layer of each output block in different resolutions. Typically, the size of the input image is $512\times512$; the hierarchical feature maps $\mathcal{F}$  contain four sets, where the $i$-th feature map $F_i$ has the spatial size of $H_i = W_i = 2^{i+2}$, with $i = 1,2,3,4$. The final depth map is then generated by a depth decoder, which is implemented as a semantic FPN \cite{kirillov2019fpn}.

\noindent\textbf{Data Augmentation Module}.
We exploit new data augmentation designs that are overtly different from conventional ones. In general circumstances, models can only memorize the specific corruptions seen during training, which results in poor generalization ability against corruptions. AugMix \cite{hendrycks2020augmix} is proposed for helping models withstand unforeseen corruptions. Specifically, AugMix \cite{hendrycks2020augmix} involves blending the outputs obtained by applying chains or combinations of multiple augmentation operations. Inspired by it, we investigate the effect of different data augmentation on indoor scene corruptions in our work. The augmentation operations include rotation, translation, shear, \textit{etc}. Next, we randomly sample three augmentation chains; each augmentation chain is constructed by composing from one to three randomly selected augmentation operations. This operation can prevent the augmented image from veering too far from the original image.

\noindent\textbf{Loss Function}.
We adopt the Scale-Invariant Logarithmic (SILog) loss introduced in \cite{eigen2014depth} and denote it as $L$. We first calculate the logarithm difference between the predicted depth map and the ground-truth depth as follows:
\begin{equation}
\label{track2_2nd_eq1}
\Delta d_i = \log d^{\prime}_i - \log d^{*}_i~,
\end{equation}
where $d^{\prime}_i$ and $d^{*}_i$ are the predicted depth and ground-true depth, respectively, at pixel $i$. The SIlog loss is computed as:
\begin{equation}
\label{track2_2nd_eq2}
L = \sqrt{\frac{1}{K} \sum_i \Delta d^2_i - \frac{\lambda}{K}(\sum_i \Delta d_i)^2}~,
\end{equation}
where $K$ is the number of pixels with valid depth and $\lambda$ is a variance-minimizing factor. Following previous works \cite{bhat2021adabins,li2022binsformer}, we set $\lambda = 0.5$ in our experiments.

\subsubsection{Experimental Analysis}
\noindent\textbf{Implementation Details}.
We provide the common configurations of our baseline. We fix the VQGAN \cite{esser2021vqgan} encoder $\epsilon$ and the CLIP \cite{radford2021clip} text encoder during training. To fully preserve the pre-trained knowledge, we always set the learning rate $\epsilon_\theta$ of as $1$/$10$ of the base learning rate.

Our work is implemented using PyTorch on eight NVIDIA RTX 3090 GPUs. The network is optimized end-to-end with the Adam optimizer ($\beta_1 = 0.9$, $\beta_2 = 0.999$). We set the learning rate to $5$e-$4$ and train the model for $30$ epochs with a batch size of $24$. During training, we randomly crop the images to $480\times480$ and then use AugMix \cite{hendrycks2020augmix} to obtain the augmented image. We freeze the CLIP \cite{radford2021clip} text decoder and VQGAN \cite{esser2021vqgan} encoder. The version of stable diffusion is v1-5 by default. The decoder head and other experimental settings are the same as \cite{xie2023revealing}. We use the flip and sliding windows during testing.

\begin{table*}[t]
\caption{Quantitative results on the RoboDepth competition leaderboard (Track \# 2). The \textbf{best} and \underline{second best} scores of each metric are highlighted in \textbf{bold} and \underline{underline}, respectively.}
\centering\scalebox{0.78}{
\begin{tabular}{l|cccc|cccc}
    \toprule
    \textbf{Method} & \cellcolor{blue!10}\textbf{Abs Rel~$\downarrow$} & \cellcolor{blue!10}\textbf{Sq Rel~$\downarrow$} & \cellcolor{blue!10}\textbf{RMSE~$\downarrow$} & \cellcolor{blue!10}\textbf{log RMSE~$\downarrow$} & \cellcolor{red!10}$\delta<1.25$~$\uparrow$ & \cellcolor{red!10}$\delta<1.25^2$~$\uparrow$ & \cellcolor{red!10}$\delta<1.25^3$~$\uparrow$
    \\\midrule\midrule
    YYQ & $0.125$ & $0.085$ & $0.470$ & $0.159$ & $0.851$ & $0.970$ & $0.989$
    \\
    AIIA-RDepth & $0.123$ & $0.080$ & $0.450$ & $ 0.153$ & $0.861$ & $0.975$ & $0.993$
    \\
    GANCV & $0.104$ & $0.060$ & $0.391$ & $0.131$ & $0.898$ & $0.982$ & $0.995$
    \\
    USTCxNetEaseFuxi & $\mathbf{0.088}$ & \underline{$0.046$} & \underline{$0.347$} & $\mathbf{0.115}$ & $\mathbf{0.940}$ & \underline{$0.985$} & \underline{$0.996$}
    \\\midrule
    \textbf{OpenSpaceAI~(Ours)} & \underline{$0.095$} & $\mathbf{0.045}$ & $\mathbf{0.341}$ & \underline{$0.117$} & \underline{$0.928$} & $\mathbf{0.990}$ & $\mathbf{0.998}$
    \\\bottomrule
    \end{tabular}
}
\label{tab:track2_2nd_results}
\end{table*}

\noindent\textbf{Main Results}.
The results of RDDepth are presented in Table~\ref{tab:track2_2nd_results}. Our RDDepth framework ranks second in terms of the a1 metric and outperforms many other participants in other metrics, such as \texttt{Sq Rel}, \texttt{RMSE}, a2, and a3. The results demonstrate the superior robustness of our approach.

Additionally, we show some visual examples under different corruption scenarios in Figure~\ref{fig:track2_2nd_qualitative}. It can be seen that our proposed RDDepth can predict reasonable and accurate depth maps under various corruptions, such as color quantization, low light, motion blur, zoom blur, \textit{etc}.

% Figure environment removed

\subsubsection{Solution Summary}
In this work, we proposed RDDepth, a Robust Diffusion model for Depth estimation. With the VPD serving as the baseline, we exploited how to leverage the diffusion model’s high-level scene knowledge to guide depth estimation head to counteract the effects of corruptions, thus generalizing well in unseen OoD data. Furthermore, we found that proper data augmentations can benefit the model’s generalization ability. RDDepth achieved state-of-the-art performance on the RoboDepth benchmark and ranked second in the second track of the RoboDepth Challenge.