\subsection{The \textcolor{robo_blue}{1st} Place Solution: \textcolor{robo_blue}{OpenSpaceAI}}
\noindent\textbf{Authors:} \textcolor{gray}{Ruijie Zhu, Ziyang Song, Li Liu, and Tianzhu Zhang.}

\begin{framed}
    \textbf{Summary} - Though existing self-supervised monocular depth estimation methods achieved high accuracy on standard benchmarks, few works focused on their OoD generalizability under real-world corruptions. The \texttt{OpenSpaceAI} team proposes IRUDepth to improve the robustness and uncertainty estimation of depth estimation systems. It takes a CNN-Transformer hybrid architecture as the baseline and applies simple yet effective data augmentation chains to enforce consistent depth predictions under diverse corruption scenarios.
\end{framed}

\input{tables/track2_results}

\subsubsection{Overview}
Depth estimation is a fundamental task in 3D vision with vital applications, such as autonomous driving \cite{schon2021mgnet}, augmented reality \cite{yucel2021real}, virtual reality \cite{li2021panodepth}, and 3D reconstruction \cite{yang2020mobile3d}. Though many specialized depth sensors, \textit{e.g.} LiDAR and Time-of-Flight (ToF) cameras, can generate accurate raw depth data, they have certain limitations compared to the learning-based monocular depth estimation systems, such as higher hardware cost and limited usage scenarios.

To meet the high requirement of the challenging OoD depth estimation, we propose IRUDepth, a novel framework that focuses on improving the robustness and uncertainty of current self-supervised monocular depth estimation systems. Following MonoViT \cite{zhao2021monovit}, we use MPViT \cite{lee2022mpvit} as the depth encoder, which is a CNN-Transformer hybrid architecture that fuses multi-scale image features. We use PoseNet \cite{xiang2018posecnn} to jointly optimize the camera parameters and predicted depth maps.

To improve the robustness of the self-supervised monocular depth estimation model under OoD situations, we design an image augmentation module and a triplet loss function motivated by AugMix \cite{hendrycks2020augmix}. For the image augmentation module, we utilize stochastic and diverse augmentations to generate random augmented pairs for input images. After predicting the corresponding depth maps, a triplet loss is applied to constrain the Jensen-Shannon divergence between the predicted depth of the clean image and its augmented version.

The proposed IRUDepth ranks first in the first track of the RoboDepth Challenge. Extensive experimental results on the KITTI-C benchmark also demonstrate that IRUDepth significantly outperforms state-of-the-art methods and exhibits satisfactory OoD robustness.

\subsubsection{Technical Approach}

Given an RGB image $I_t\in\mathbb{R}^{H\times W\times 3}$, the IRUDepth framework aims to predict its corresponding depth map $D_t\in\mathbb{R}^{H\times W}$. To improve the robustness and uncertainty estimation, we use random image augmentation inspired by AugMix \cite{hendrycks2020augmix} to generate two augmented views of an image. Then both clean and augmented images are used as the input to train the depth network.

Following MonoViT \cite{zhao2021monovit}, we use the MPViT module \cite{lee2022mpvit} as the encoder of the depth network to extract the local and global context from images. This module is a multi-path CNN-Transformer hybrid architecture. With a disparity head, we generate pixel-aligned depth maps for all input images. The triplet loss is proposed to encourage consistency between the predicted depth maps of the clean and augmented images. 

During training, to obtain supervisory signals from adjacent image frames, we input the adjacent image frames $I_t^\prime=\{I_{t-1}, I_{t+1}\}$ and feed them into the pose estimation network together with $I_t$ to estimate the relative camera pose $T_{t\rightarrow t^\prime}$. The synthesized counterpart to $I_t$ is then generated by:
\begin{equation}
\label{eq:track1_1st_1}
I_{t\rightarrow t^\prime} = T_{t^\prime}<\texttt{proj}(D_t, T_{t\rightarrow t^\prime}, K)>~,
\end{equation}
where the $\texttt{proj}(\cdot)$ operator returns the 2D coordinates when reprojecting the point cloud generated using $D_t$ onto $I_{t^\prime}$; $K$ denotes the camera parameter matrix.

% Figure environment removed

\noindent\textbf{Augmentation Module}.
We believe a proper data augmentation technique can significantly improve the generalizability of monocular depth estimation models. Various data augmentation methods \cite{devries2017cutout,yun2019cutmix,zhang2018mixup} have been proposed to enhance the robustness of the model during training. Recently, adding adversarial losses during training has been shown to be an effective way of improving model robustness \cite{madry2018adversarial}. Training with these approaches, however, often greatly increases the training time and GPU memory consumption. It is thus desirable to design a cost-effective augmentation that can be easily plugged into the training pipeline to balance model performance and training consumption. In particular, IRUDepth combines operations from AutoAugment \cite{cubuk2019autoaugment} and AugMix \cite{hendrycks2020augmix} as the main components of our augmentation chain.

\noindent\textbf{Augmentation Protocol}.
To ensure the designed augmentations are disjoint with simulated evaluation data, we exclude operations that constitute or are similar to the $18$ corruption types in KITTI-C. Specifically, we remove the \textit{`contrast'}, \textit{`color'}, \textit{`brightness'}, \textit{`sharpness'}, and \textit{`cutout'} operations from the original augmentation types in \cite{cubuk2019autoaugment,hendrycks2020augmix}. Also, to avoid any potential overlap with the KITTI-C testing set, we do not use any image noising or image blurring operations.

\noindent\textbf{Augmentation Chain}. We randomly sample $k=3$ augmentation chains to combine different augmentation operations. Following AugMix \cite{hendrycks2020augmix}, we mix the resulting images from these augmentation chains via element-wise convex combinations. In particular, we sample convex coefficients from a Dirichlet distribution for the first stage mixing on augmentation chains. Next, we use a second stage mixing sampled from a Beta distribution to mix the clean and the augmented images. In this way, we can obtain final images generated by an arbitrary combination of data augmentation operations with
random mixing weights. We use such images in the training phase of IRUDepth.

\noindent\textbf{Loss Function}.
Following MonoDepth2 \cite{godard2019monodepth2}, we minimize the photometric reprojection error $L_p$. This loss can be calculated as follows:
\begin{equation}
\label{eq:track1_1st_2}
L_p=\min_{t^\prime}\texttt{pe}(I_t, I_{t^\prime\rightarrow t})~,
\end{equation}
\begin{equation}
\label{eq:track1_1st_3}
\texttt{pe}(I_a, I_b)=\frac{\alpha}{2}(1 - \texttt{SSIM}(I_a, I_b))+(1-\alpha)||I_a, I_b||_1~.
\end{equation}
Here we set $\alpha = 0.85$. Additionally, as in \cite{godard2017unsupervised}, we apply the following smoothness loss:
\begin{equation}
\label{eq:track1_1st_4}
L_s=|\partial_x d^*_t|e^{-|\partial_x I_t|} + |\partial_y d^*_t|e^{-|\partial_y I_t|}~,
\end{equation}
where $d^*_t = dt/\bar{d}t$ is the normalized inverse depth as proposed in \cite{wang2018learning}.

To constrain the consistency between the predicted depth maps of the clean and augmented images, we apply the Jensen-Shannon divergence consistency loss used in \cite{hendrycks2020augmix}. This loss aims to enforce smoother neural network responses. Firstly, we mix the depth result as the mixed depth center:
\begin{equation}
\label{eq:track1_1st_5}
D_{mix} = \frac{1}{3}(D_t + D_t^{aug1} + D_t^{aug2})~,
\end{equation}
where $D_t$, $D_t^{aug1}$, and $D_t^{aug2}$ are the depth maps of the clean and the two augmented images, respectively. Next, we compute the triplet loss listed as follows:
\begin{equation}
\label{eq:track1_1st_6}
L_{mix} = \frac{1}{3}\big(\texttt{KL}(D_t || D_{mix}) + \texttt{KL}(D_t^{aug1} || D_{mix}) + \texttt{KL}(D_t^{aug2} || D_{mix})\big)~,
\end{equation}
where the KL divergence (\texttt{KL}) is used to measure the degree of difference between two depth distributions. Note that we use the mixed depth instead of the depth map of clean images in \texttt{KL}, which is proven to perform better in experiments. As in \cite{kannan2018pairing,hendrycks2020augmix}, the triplet loss function in the form of the Jensen-Shannon divergence impels models to be stable, consistent, and insensitive across the input images from diverse scenarios.

Finally, during training, the total loss sums up the above three losses computed from outputs at the
scale $s\in\{1, \frac{1}{2}, \frac{1}{4}, \frac{1}{8}\}$, as computed in the following form:
\begin{equation}
\label{eq:track1_1st_7}
L_{total} = \frac{1}{N}\sum^N_{s=i}(\alpha L_p + \beta L_s + \gamma L_{mix})~,
\end{equation}
where $\alpha$, $\beta$, and $\gamma$ are loss coefficients sampled from the scale set $s$.

\subsubsection{Experimental Analysis}

\noindent\textbf{Implementation Details}.
The IRUDepth framework is implemented using Pytorch. Four NVIDIA GTX 3090 GPUs are used for model training, each with a batch size of $6$. We take MPViT \cite{lee2022mpvit} as the backbone, which is pre-trained on ImageNet-1K, and further fine-tuned on low-resolution ($640\times192$) images from the KITTI dataset \cite{geiger2012kitti} following splits and
data processing in \cite{eigen2015predicting,zhou2017sfm}. The overall framework is optimized end-to-end with the AdamW optimizer \cite{loshchilov2018adamw} for $30$ epochs. The learning rate of the pose network and depth decoder is initially set as $1$e-$4$, while the initial learning rate of the MPViT module is set as $1$e-$5$. Both the learning rates decay by a factor of $10$ for the final $5$ epochs.

\begin{table*}[t]
\caption{Quantitative results on the Robodepth competition leaderboard (Track \# 1). The \textbf{best} and \underline{second best} scores of each metric are highlighted in \textbf{bold} and \underline{underline}, respectively.}
\centering\scalebox{0.78}{
\begin{tabular}{l|cccc|cccc}
    \toprule
    \textbf{Method} & \cellcolor{blue!10}\textbf{Abs Rel~$\downarrow$} & \cellcolor{blue!10}\textbf{Sq Rel~$\downarrow$} & \cellcolor{blue!10}\textbf{RMSE~$\downarrow$} & \cellcolor{blue!10}\textbf{log RMSE~$\downarrow$} & \cellcolor{red!10}$\delta<1.25$~$\uparrow$ & \cellcolor{red!10}$\delta<1.25^2$~$\uparrow$ & \cellcolor{red!10}$\delta<1.25^3$~$\uparrow$
    \\\midrule\midrule
    Ensemble & $0.124$ & $0.899$ & $4.938$ & $0.203$ & \underline{$0.852$} & $0.950$ & \underline{$0.979$}
    \\
    UMCV & $0.124$ & $\mathbf{0.845}$ & \underline{$4.883$} & $0.202$ & $0.847$ & $0.950$ & $\mathbf{0.980}$
    \\
    YYQ & \underline{$0.123$} & \underline{$0.885$} & $4.983$ & \underline{$0.201$} & $0.848$ & $0.950$ & \underline{$0.979$}
    \\
    USTC-IAT-United & \underline{$0.123$} & $0.932$ & $\mathbf{4.873}$ & $0.202$ & $\mathbf{0.861}$ & $\mathbf{0.954}$ & \underline{$0.979$}
    \\\midrule
    \textbf{IRUDepth~(Ours)} & $\mathbf{0.121}$ & $0.919$ & $4.981$ & $\mathbf{0.200}$ & $\mathbf{0.861}$ & \underline{$0.953$} & $\mathbf{0.980}$
    \\\bottomrule
    \end{tabular}
}
\vspace{0.1cm}
\label{tab:track1_1st_comparative}
\end{table*}

\begin{table*}[t]
\caption{Ablation results of IRUDepth on the RoboDepth competition leaderboard. Notations: \texttt{Aug} denotes the proposed image augmentations; $L_{mix}$ denotes the proposed triplet loss. For methods only with \texttt{Aug}, we use augmented images instead of clean images as the input. The \textbf{best} and \underline{second best} scores of each metric are highlighted in \textbf{bold} and \underline{underline}, respectively.}
\centering\scalebox{0.78}{
\begin{tabular}{l|cccc|cccc}
    \toprule
    \textbf{Method} & \cellcolor{blue!10}\textbf{Abs Rel~$\downarrow$} & \cellcolor{blue!10}\textbf{Sq Rel~$\downarrow$} & \cellcolor{blue!10}\textbf{RMSE~$\downarrow$} & \cellcolor{blue!10}\textbf{log RMSE~$\downarrow$} & \cellcolor{red!10}$\delta<1.25$~$\uparrow$ & \cellcolor{red!10}$\delta<1.25^2$~$\uparrow$ & \cellcolor{red!10}$\delta<1.25^3$~$\uparrow$
    \\\midrule\midrule
    MPViT-S & $0.172$ & $1.340$ & $6.177$ & $0.258$ & $0.743$ & $0.910$ & $0.963$
    \\
    MPViT-S + \texttt{Aug} + $L_{mix}$ & \underline{$0.123$} & \underline{$0.946$} & \underline{$5.011$} & \underline{$0.203$} & \underline{$0.855$} & \underline{$0.950$} & \underline{$0.979$}
    \\
    MPViT-B & $0.170$ & $1.212$ & $5.816$ & $0.319$ & $0.753$ & $0.912$ & $0.961$
    \\
    MPViT-B + \texttt{Aug} & $0.146$ & $1.166$ & $5.549$ & $0.226$ & $0.806$ & $0.936$ & $0.974$
    \\\midrule
    MPViT-B + \texttt{Aug} + $L_{mix}$ & $\mathbf{0.121}$ & $\mathbf{0.919}$ & $\mathbf{4.981}$ & $\mathbf{0.200}$ & $\mathbf{0.861}$ & $\mathbf{0.953}$ & $\mathbf{0.980}$
    \\\bottomrule
    \end{tabular}
}
\label{tab:track1_1st_ablation}
\end{table*}

% Figure environment removed

\noindent\textbf{Comparative Study}.
The benchmark takes the depth maps of clean images as the ground truth for evaluation. Table~\ref{tab:track1_1st_comparative} compares the model performance with other methods on the RoboDepth competition leaderboard. Our method ranks first on the leaderboard and outperforms other methods across four depth evaluation metrics. Figure~\ref{fig:track1_1st_qualitative} reports some challenging examples from the RoboDepth benchmark. Even for severely corrupted images, our approach could predict accurate and consistent depth maps, which demonstrates the strong robustness of the proposed IRUDepth.

\noindent\textbf{Ablation Study}.
To further validate the effectiveness of the proposed data augmentation module and the triplet loss, we report some ablation results in Table~\ref{tab:track1_1st_ablation}. We assess the impact of different model backbones, image augmentation techniques, and loss functions. As shown in this table, the proposed data augmentation and triplet loss function play an important role in improving the model's performance under OoD corruptions.

\subsubsection{Solution Summary}
In this work, we proposed IRUDepth, a method that aims to improve the robustness and uncertainty estimation of self-supervised monocular depth estimation. With the novel image augmentation and proposed triplet loss function, IRUDepth achieved better generalization performance than state-of-the-art methods for self-supervised depth estimation on the KITTI-C dataset. Moreover, our IRUDepth ranked first in the first track of the RoboDepth Challenge, which demonstrates its superior robustness under different kinds of OoD situations.