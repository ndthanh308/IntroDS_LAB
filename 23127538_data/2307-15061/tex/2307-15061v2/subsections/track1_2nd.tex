\subsection{The \textcolor{robo_red}{2nd} Place Solution: \textcolor{robo_red}{USTC-IAT-United}}
\noindent\textbf{Authors:} \textcolor{gray}{Jun Yu, Xiaohua Qi, Jie Zhang, Mohan Jing, Pengwei Li, Zhen Kan, Qiang Ling, Liang Peng, Minglei Li, Di Xu, and Changpeng Yang.}

\begin{framed}
    \textbf{Summary} - Although current self-supervised depth estimation methods have achieved satisfactory results on ``clean'' data, their performance often disappoints when encountering damaged or unseen data, which are cases that frequently occur in the real world. To address these limitations, the \texttt{USTC-IAT-United} team proposes a solution that includes an MAE mixing augmentation during training and an image restoration module during testing. Both comparative and ablation results verify the effectiveness and superiority of the proposed techniques in handling various types of corruptions that a depth estimation system has to encounter in practice.
\end{framed}

\subsubsection{Overview}
Self-supervised depth estimation aims to estimate the depth map of a given image without the need for explicit supervision. This task is of great importance in computer vision and robotics, as it enables machines to perceive the 3D structure of the environment without the need for expensive depth sensors. Various self-supervised learning methods have been proposed to achieve this task, such as monocular, stereo, and multi-view depth estimation. These methods leverage the geometric and photometric constraints among multiple views of the same scene to learn depth representation.

In recent years, deep learning techniques have been widely adopted in self-supervised depth estimation tasks. Garg \textit{et al.} \cite{garg2016unsupervised} reformulated depth estimation into a view synthesis problem and proposed a photometric loss across stereo pairs to enforce view consistency. Godard \textit{et al.} \cite{godard2017unsupervised} proposed to leverage differentiable bilinear interpolation \cite{jaderberg2015spatial}, virtual stereo prediction, and an SSIM $+$ L1 reconstruction loss to better encourage the left-right consistency. Utilizing solely supervision signals from monocular video sequences, SfM-Learner \cite{zhou2017sfm} relaxed the stereo constraint by replacing the known stereo transform with a pose estimation network. These techniques have shown promising results in various visual perception applications, such as autonomous driving \cite{geiger2012kitti}, augmented reality \cite{yucel2021real}, and robotics \cite{ming2021survey}.

Despite the significant role that monocular and stereo depth estimation play in real-world visual perception systems and the remarkable achievements that have been made, current deep learning-based self-supervised monocular depth estimation models are mostly trained and tested on ``clean'' datasets, neglecting OoD scenarios. Common corruptions, however, often occur in practical scenes, which are crucial for the safety of applications such as autonomous driving and robot navigation. In response to this concern, recent research has focused on developing robust self-supervised depth estimation models that can handle OoD scenarios. The challenge of artifacts arising from dynamic objects has been addressed by integrating uncertainty estimation \cite{klodt2018supervising,poggi2020uncertainty,yang2020d3vo}, motion masks \cite{casser2019depth}, optical flow \cite{luo2019every}, or the minimum reconstruction loss. Simultaneously, to enhance robustness against unreliable photometric appearance, strategies such as feature-based reconstructions \cite{spencer2020general} and proxy-depth supervision \cite{klodt2018supervising} have been introduced.

In recent advancements of network architecture design, several techniques have been incorporated, such as 3D packing and unpacking blocks, positional encoding \cite{zhao2021monovit}, sub-pixel convolution for depth super-resolution \cite{pillai2019superdepth}, progressive skip connections, and self-attention decoders \cite{johnston2020self}. Moreover, some researchers have proposed to use synthetic data to augment the training dataset and improve the model’s generalization ability to OoD scenarios. For example, domain randomization techniques are used to generate diverse synthetic data with various levels of perturbations, which can help the model learn to handle different types of corruption.

In this work, to address this challenging task, we propose a solution with novel designs ranging from the following aspects: 1) an augmented training process and 2) a more stable testing pipeline. For the former stage, we resort to masked autoencoders (MAE) \cite{he2022mae} and image mixing techniques to enhance representation learning of self-supervised depth estimation models. For the latter, we explore off-the-shelf image restoration networks for obtaining images with better visual cues at the test time. Through comparative and ablation experiments, we demonstrate and verify the effectiveness and satisfactory performance of the proposed techniques under challenging OoD scenarios.

% Figure environment removed

\subsubsection{Technical Approach}
Our proposed solution consists of the following major components: 1) the training pipeline, 2) the test pipeline, 3) an MAE mixing operation, and 4) an image restoration module.

\noindent\textbf{Training Pipeline}. Our model training pipeline, as shown in Figure~\ref{fig:track1_2nd_training}, is composed of four main parts: 1) the input of image sequences or image pairs, 2) an MAE reconstruction with image mixing, 3) a data augmentation module, and 4) the overall model training objectives. The input of our model follows MonoDepth2 \cite{godard2019monodepth2}, where image sequences are used for single-view training and left-right image pairs are used for stereo training. The MAE mixing operation will be elaborated on later. We also adopt the data augmentation settings of MonoDepth2 \cite{godard2019monodepth2} during training. We use the MonoViT \cite{zhao2021monovit} architecture as our backbone model, which consists of MPViT \cite{lee2022mpvit} encoder blocks and a self-attention decoder. The training is conducted in a self-supervised manner on the KITTI depth estimation dataset \cite{geiger2012kitti}, using the photometric loss \cite{godard2019monodepth2} for both monocular frames and stereo pairs, as well as a proxy depth regression objective. The regularization is achieved by incorporating edge-aware disparity smoothness \cite{godard2017unsupervised} and depth gradient consistency with respect to the proxy labels.

% Figure environment removed

\noindent\textbf{Testing Pipeline}.
The proposed solution consists of five components in the testing phase, as shown in Figure~\ref{fig:track1_2nd_testing}: 1) the input image for inference, 2) an image restoration module, 3) model inference, 4) a test-time augmentation (TTA) technique, and 5) the final depth prediction result. The specific process is as follows: we use a single image as the input for inference, which is then enhanced with the image restoration module to be described later. The restored image is then fed into the depth estimation model for feature extraction and prediction. Finally, a TTA approach based on MonoDepth2 \cite{godard2019monodepth2} is applied as a post-processing technique to produce the final result. The entire process is mathematically and academically rigorous.

\noindent\textbf{MAE Reconstruction}.
The masking-based image reconstruction method aims for reconstructing masked regions in an image by minimizing the mean absolute error between the original input and its reconstruction. Mathematically, given an image $x$ and its reconstruction $\hat{x}$, the MAE reconstruction process can be formulated as follows:
\begin{equation}
\label{eq:track1_2nd_1}
\hat{x} = \arg\min_{\hat{x}} \frac{1}{n}\sum^{n}_{i=1}|x_i - \Tilde{x}_i|~,
\end{equation}
where $n$ is the number of pixels in the image, and $x_i$ and $\Tilde{x}_i$ represent the $i$-th pixel of the original image and its reconstruction, respectively.

MAE is a type of network that can be used for unsupervised learning of visual features, which is particularly well-suited for learning from large-scale datasets as they can be trained efficiently on distributed computing systems. The basic idea of MAE is to learn a compressed representation of an image by encoding it into a lower-dimensional space and then decoding it back to its original size. Unlike traditional autoencoders which use fully connected layers for both the encoder and decoder, MAE uses convolutional layers to capture spatial information and reduce the number of parameters.

The MAE reconstruction process not only preserves semantic information similar to the original image but also introduces blurriness and distortion, making it a suitable method for enhancing robustness under various OoD corruptions. In this challenge, we directly load a pre-trained MAE model \cite{he2022mae} for image reconstruction of the input image $x$. Specifically, the pre-trained model $f$ can be represented as a function that maps the input image $x$ to its reconstructed image $\hat{x}$, \textit{i.e.}, $\hat{x}=f(x)$.

\noindent\textbf{Image Mixing}.
Blending different images is a commonly-used data augmentation technique. It can be used to generate new training samples by mixing two or more images together. The basic idea is to combine the content of two or more images in a way that preserves the semantic information while introducing some degree of variability. This can help the model learn to be more robust to changes in the input data and improve its generalization performance.

One common approach for image mixing is to conduct a weighted sum of the pixel values from different input images. Given two images $I_A$ and $I_B$, we can generate a mixed image $I_C$ as follows:
\begin{equation}
\label{eq:track1_2nd_2}
I_C = (1-\alpha)I_A + \alpha I_B~,
\end{equation}
where $\alpha$ is a mixing coefficient that controls the degree of influence of each of the two images. For example, when $\alpha = 0.5$, the resulting image is an equal blend of the two inputs. When $\alpha$ is closer to $0$ or $1$, the resulting image is more similar to one of these two candidate input images.

To introduce a certain degree of randomness and diversity into the mixing process, we can use different values of $\alpha$ for each pair of images. This can further increase the variability of the generated samples and improve the model’s ability to handle different types of input data. Image mixing has been shown to be an effective data augmentation technique for various computer vision tasks, including image classification, object detection, and semantic segmentation. It can help the model learn to be more robust to changes in the input data and improve its generalization performance.

\noindent\textbf{MAE Mixing}.
Different from the aforementioned image mixing, our MAE mixing operation refers to the mixing of the MAE-reconstructed image and the original image. This mixing process can be mathematically described as follows:
\begin{equation}
\label{eq:track1_2nd_3}
x_{mix} = (1-\alpha)x + \alpha \hat{x}~,
\end{equation}
where $x$ and $\hat{x}$ represent the original image and the MAE-reconstructed image, respectively, and $\alpha$ is a hyperparameter representing the mixing ratio.

% Figure environment removed

% Figure environment removed

By combining the reconstructed images with the original ones, the diversity of the training data can be greatly enriched, thereby enhancing the robustness of the depth estimation model. Without the need for altering the supervision signal, we achieve such mixing and control its degree using weighted image interpolation, as described earlier. The resulting mixed image $x_{mix}$ can be used as the input to the depth estimation model, thereby increasing the diversity of the training data and improving the model’s ability to generalize to unseen data.

\noindent\textbf{Image Restoration}.
The goal of image restoration is to recover a blurred or noisy image without changing its size and content. To perform such a restoration, we use an efficient image restoration network called Restormer \cite{zamir2022restormer}. This model is based on the Transformer backbone to restore damaged images. In this challenge, we did not further fine-tune the network but directly loaded the pre-trained Restormer \cite{zamir2022restormer} checkpoint to restore the corrupted images.

As shown in Figure~\ref{fig:track1_2nd_testing}, before feeding the test images into the depth estimation model, we perform image restoration to enhance the image quality. Specifically, we first restore the damaged images using the Restormer network, which is pre-trained on various restoration tasks including `image de-raining', `single-image motion de-blurring', `defocus de-blurring', and `image de-noising'. After the restoration process, we use the restored images as the input of our depth estimation model for further processing. Mathematically, the restoration process can be formulated as follows:
\begin{equation}
\label{eq:track1_2nd_4}
\hat{I} = \texttt{Restormer}(I)~,
\end{equation}
where $I$ denotes the input image of the image restoration network and $\hat{I}$ denotes the restored image. Subsequently, the depth estimation process can be formulated as follows:
\begin{equation}
\label{eq:track1_2nd_5}
D = \texttt{DepthEstimate}(\hat{I})~,
\end{equation}
where $D$ denotes the estimated depth map. Figure~\ref{eq:track1_2nd_restore_snow} to Figure~\ref{eq:track1_2nd_restore_denoise} provide representative results of various types of corrupted images and their restored versions from Restormer \cite{zamir2022restormer}.

% Figure environment removed

Specifically, Figure~\ref{eq:track1_2nd_restore_snow} displays the restoration results of images degraded by \textit{`snow'}; while Figure~\ref{eq:track1_2nd_restore_motion} shows the restoration results of images degraded by \textit{`motion blur'}. Figure~\ref{eq:track1_2nd_restore_defocus} and Figure~\ref{eq:track1_2nd_restore_denoise} present the restoration results of images degraded by \textit{`defocus blur'} and by \textit{`noises'}, respectively. In each figure, the left-hand-side images represent the inputs that are degraded by different kinds of real-world corruptions, while the right-hand-side images are the restored outputs. The results demonstrate the effectiveness of the Restormer network in restoring images degraded by various types of distortions.

\subsubsection{Experimental Analysis}

\noindent\textbf{Implementation Details}.
We use the standard Eigen split \cite{eigen2015predicting} of the KITTI depth estimation dataset \cite{geiger2012kitti} as our training dataset and trained our models with the corresponding hyperparameters specified in the MonoDepth2 paper \cite{godard2019monodepth2}. We then fine-tuned the pre-trained models using our MAE mixing data augmentation, with the starting learning rate being one-fifth of the original learning rate.

% Figure environment removed

\noindent\textbf{Baselines}.
We evaluated the robustness of multiple self-supervised depth estimation models on the corrupted dataset and identified four models with superior depth performance under OoD scenarios: CADepth \cite{yan2021cadepth}, MonoDepth2 \cite{godard2019monodepth2}, Lite-Mono \cite{zhang2023litemono}, and MonoViT \cite{zhao2021monovit}. Their depth estimation results are shown in Table~\ref{tab:track1_2nd_baselines}. We can observe from this table that MonoViT \cite{zhao2021monovit} achieves the best OoD depth estimation performance when trained with the Mono+Stereo modality and with an input resolution of $640\times192$. Therefore, all subsequent experiments are conducted with this configuration.

\noindent\textbf{MAE Mixing}.
We conducted experiments to investigate the impact of the mixing ratio hyperparameter $\alpha$ in our MAE mixing data augmentation. The ablation results are shown in Table~\ref{tab:track1_2nd_mixing_ratio}. Specifically, we varied the mixing ratio $\alpha$ between the original image $x$ and the MAE-reconstructed image $\hat{x}$, where the mixed image is given by Eq.~\ref{eq:track1_2nd_3}. As can be seen from the results, the mixing ratio $\alpha$ should be carefully selected for our MAE mixing data augmentation, as excessively high or low values of $\alpha$ can negatively impact the model’s performance.


\begin{table*}[t]
\caption{The performance of multiple models trained on the standard Eigen split of the KITTI dataset.}
\centering\scalebox{0.78}{
\begin{tabular}{c|c|c|c|c|c}
    \toprule
    \textbf{Method} & \textbf{Ref} & \textbf{Input Modality} & \textbf{Input Resolution} & \cellcolor{blue!10}\textbf{Abs Rel~$\downarrow$} & $\Delta$
    \\\midrule\midrule
    \multirow{6}{*}{MonoDepth2} & \multirow{6}{*}{\cite{godard2019monodepth2}} & Mono & $640\times192$ & $0.149$ & \textcolor{gray}{$+0.000$}
    \\
    & & Stereo & $640\times192$ & $0.153$ & \textcolor{robo_red}{$+0.004$}
    \\
    & & Mono+Stereo & $640\times192$ & $0.146$ & \textcolor{robo_blue}{$-0.003$}
    \\
    & & Mono & $1024\times320$ & $0.153$ & \textcolor{robo_red}{$+0.004$}
    \\
    & & Stereo & $1024\times320$ & $0.154$ & \textcolor{robo_red}{$+0.005$}
    \\
    & & Mono+Stereo & $1024\times320$ & $0.240$ & \textcolor{robo_red}{$+0.091$}
    \\\midrule
    \multirow{5}{*}{CADepth} & \multirow{5}{*}{\cite{yan2021cadepth}} & Mono & $640\times192$ & $0.149$ & \textcolor{gray}{$+0.000$}
    \\
    & & Mono & $1024\times320$ & $0.151$ & \textcolor{robo_red}{$+0.002$}
    \\
    & & Mono & $1280\times384$ & $0.157$ & \textcolor{robo_red}{$+0.008$}
    \\
    & & Mono+Stereo & $640\times192$ & $0.147$ & \textcolor{robo_blue}{$-0.002$}
    \\
    & & Mono+Stereo & $1024\times320$ & $0.143$ & \textcolor{robo_blue}{$-0.006$}
    \\\midrule
    Lite-Mono-L & \cite{zhang2023litemono} & Mono & $1024\times320$ & $0.148$ & \textcolor{robo_blue}{$-0.001$}
    \\\midrule
    \multirow{5}{*}{MonoViT} & \multirow{5}{*}{\cite{zhao2021monovit}} & Mono & $640\times192$ & $0.143$ & \textcolor{robo_blue}{$-0.006$}
    \\
    & & Mono+Stereo & $640\times192$ & $0.134$ & \textcolor{robo_blue}{$-0.015$}
    \\
    & & Mono & $1024\times320$ & $0.149$ & \textcolor{gray}{$+0.000$}
    \\
    & & Mono+Stereo & $1024\times320$ & $0.138$ & \textcolor{robo_blue}{$-0.011$}
    \\
    & & Mono & $1280\times384$ & $0.147$ & \textcolor{robo_blue}{$-0.002$}
    \\\bottomrule
    \end{tabular}
}
\label{tab:track1_2nd_baselines}
\end{table*}

\begin{table*}[t]
\caption{Ablation results of MAE mixing ratio $\alpha$ on the Robodepth competition leaderboard (Track \# 1). The \textbf{best} and \underline{second best} scores of each metric are highlighted in \textbf{bold} and \underline{underline}, respectively.}
\centering\scalebox{0.78}{
\begin{tabular}{c|c|c|c|c|c}
    \toprule
    Mixing Ratio & $\alpha=0.1$ & $\alpha=0.3$ & $\alpha=0.5$ & $\alpha=0.7$ & $\alpha=0.9$
    \\\midrule
    \cellcolor{blue!10}\textbf{Abs Rel~$\downarrow$} & \underline{$0.125$} & $\mathbf{0.123}$ & $0.128$ & $0.132$ & $0.137$
    \\\bottomrule
    \end{tabular}
}
\vspace{0.1cm}
\label{tab:track1_2nd_mixing_ratio}
\end{table*}

Without loss of generalizability, our experimental results indicate that a mixing ratio of $\alpha = 0.3$ achieves the optimal performance on the OoD testing set and better enhances the model’s generalization ability. This suggests that a balanced mixture of the original image and the MAE-reconstructed image is beneficial for the model’s representation learning. On the other hand, excessively high values of $\alpha$ can lead to overfitting issues, where the model becomes too specialized to the training data and performs poorly on new data. Conversely, excessively low values of $\alpha$ may not provide enough variation in the augmented data, leading to underfitting and poor performance. One possible reason for the sensitivity of the MAE mixing method to the mixing ratio hyperparameter $\alpha$ is the use of image restoration techniques. The restoration algorithm may introduce artifacts or distortions in the reconstructed image, which can affect the performance of the MAE mixing method. Furthermore, the restoration algorithm only operates on the testing set, while the MAE mixing data augmentation is used during training, making it necessary to carefully tune the hyperparameter $\alpha$.

To address this issue, an end-to-end training approach can be explored in future work. This would involve jointly training the restoration algorithm and the downstream task model, allowing for better integration of the restoration and augmentation processes. By incorporating the restoration algorithm into the training process, the sensitivity of the MAE mixing method to the mixing ratio hyperparameter $\alpha$ can potentially be reduced, leading to improved performance and generalization ability.

\noindent\textbf{Image Restoration}.
In the final stage of our experiments, we applied the image restoration process described in previous sections to the testing images before depth inference. This resulted in an improved absolute relative error (in terms of the \texttt{Abs Rel} score) of $0.123$. The image restoration process helps to reduce the negative impact of artifacts and distortions in corrupted images, leading to more accurate predictions by the depth estimation model. By incorporating this step into the testing pipeline, we are able to achieve better performance over the baselines. Furthermore, the use of image restoration techniques can also improve the generalization ability of the depth estimation model, as it helps to reduce the impact of variations and imperfections across a wide range of test images.

\subsubsection{Solution Summary}
In this work, we have attempted various strategies to address the challenging OoD self-supervised monocular depth estimation. We first demonstrated that the CNN-Transformer hybrid networks exhibit excellent robustness over plain CNN-based ones. We designed and employed an efficient data augmentation method -- MAE mixing -- which can serve as a strong enhancement for depth estimation. Additionally, we have shown that the image restoration network can effectively handle common distortions at test time, such as blur, noise, rain, and snow, and can significantly improve depth prediction scores. Ultimately, our solution achieved an absolute relative error of $0.123$ and ranked second in the first track of the RoboDepth Challenge.