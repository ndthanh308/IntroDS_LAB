\subsection{The Innovation Prize Solution: Ensemble}
\noindent\textbf{Authors:} \textcolor{gray}{Jiale Chen and Shuang Zhang.}

\begin{framed}
    \textbf{Summary} - Observing distinct behaviors of OoD corruptions in the frequency domain, the \texttt{Ensemble} team proposes two stand-alone models for robust depth estimation. The main idea is to improve the OoD generalizability of depth estimation models from two aspects: normalization and augmentation. To incorporate this, amplitude-phase recombination and feature interaction modules are proposed. The effectiveness of each model has been verified and analyzed. A further combination of both models contributes to an enhanced depth estimation robustness.
\end{framed}

\subsubsection{Overview}
Performing self-supervised depth estimation under common corruptions and sensor failure is of great value in practical applications. In this work, we propose two model variants built respectively upon MonoViT \cite{zhao2021monovit} and Lite-Mono \cite{zhang2023litemono} and improve their robustness to tackle OoD scenarios. We further propose a simple yet effective approach for the model ensemble to meet better performance on the challenging OoD depth estimation benchmark. It is worth noting that our method is the only one that trained without an extra pre-trained model; we also do not use any image pre-processing or post-processing operations in this competition.

\subsubsection{Technical Approach}

We contribute two stand-alone solutions for robust self-supervised depth estimation: \texttt{Model-I} and \texttt{Model-II}. The first model adopts MonoViT \cite{zhao2021monovit} as the backbone and is enhanced with a better normalization technique and an amplitude-phase recombination augmentation. The second model, on the other hand, is built upon Lite-Mono \cite{zhang2023litemono} and has been integrated with a double-path architecture for better feature extraction, a median-normalization for better OoD generalization, and a channel perturbation for stronger augmentation.

\noindent\textbf{Normalization}.
For \texttt{Model-I}, we change the normalization mechanism of the conventional CNN layers for robustness enhancement. The original network uses batch normalization (\texttt{BN}), which includes parameters containing information related to the batch dimension. Such in-distribution parameters, however, can have an impact on OoD testing. Inspired by AdaIN \cite{huang2017arbitrary}, which performs style transfer by controlling the normalization process at the channel level of the feature maps, we replace batch normalization with instance normalization (\texttt{IN}). It has been proven that \texttt{IN} performs individual normalization for each channel, thus achieving stable domain information for OoD testing.

\noindent\textbf{Amplitude-Phase Recombination}.
We employ the amplitude-phase recombination (APR) \cite{chen2021amplitude} as data augmentation for enhancing the model's robustness. Figure~\ref{fig:track1_innov1_apr} provides representative examples of this APR operation on the KITTI dataset \cite{geiger2012kitti}. By exchanging the magnitude and phase spectra between different style images and performing inverse Fourier transform, we discover that the phase spectra contain more shape information, while the magnitude spectra contain more texture and style information. We utilize this single-image transformation and magnitude-phase exchange to construct ARP samples during training.

% Figure environment removed


\noindent\textbf{Lite Backbone}.
For \texttt{Model-II}, we aim at utilizing a lightweight model for robust depth estimation. Backbones with fewer parameters have lower capacity and weaker fitting capabilities. However, they may exhibit greater robustness and perform better in handling unknown data distributions. Our second model variant selects Lite-Mono \cite{zhang2023litemono} as the basic backbone and we make further changes to it to improve the overall robustness.

\noindent\textbf{Double-Path Architecture}.
CNNs have exhibited a heightened sensitivity towards local information, whereas visual Transformers demonstrate a greater aptitude for capturing global information. It is widely observed that various types of corruptions manifest significant dissimilarities in their frequency domain distributions. Consequently, a deliberate selection has been made to adopt a double-path architecture whereby distinct CNN and Transformer pathways are employed to extract features independently, followed by a subsequent feature aggregation step. Figure~\ref{fig:track1_innov1_framework}~(a) provides an example of the dual-path structure used in our network.

\noindent\textbf{Median-Normalization for OoD Generalization}.
In our framework, we propose a simple median-normalization method to facilitate better OoD generalizability. The feature map from the CNN layer is first divided into $4 \times 4$  patches, and the median value of each patch is selected for computing the mean and variance values of the channel. 


% Figure environment removed


\noindent\textbf{Domain \& Style Perturbation in Channel}.
For CNNs, the mean and variance of each channel represent domain and style information. Following DSU \cite{li2022uncertainty}, in the training process, we resample the mean and variance of the feature maps' channels outputted by the CNN. This allows the depth estimation model to utilize different domain and style distributions during training.

\noindent\textbf{Feature Interaction from Semantics to Texture}.
For a pyramid-shaped network architecture, shallow features contain more texture information, while deep features contain more semantic information. In the case of OoD corruptions, shallow texture features are often heavily affected, while deep semantic features exhibit higher robustness degrees. Therefore, we propose modules for aggregating information from semantics to texture before feeding the features into the depth decoder. Figure~\ref{fig:track1_innov1_framework}~(b) provides an example of our feature interaction modules. The high-level feature map is upsampled bilinearly and concatenated with the low-level one. Channel attention from CBAM \cite{woo2018cbam} and $1\times1$ convolution is adopted for fusion and channel squeeze.

\noindent\textbf{Model Ensemble}.
As will be discussed in the following section, \texttt{Model-II} is of a more stable performance compared with \texttt{Model-I}. To leverage the advantages of both models, we propose a simple yet effective approach for the model ensemble. The final depth prediction $D_{\text{final}}$ is the aggregation of predictions ($D_1$ and $D_2$) from both models, with fusion coefficients $\alpha$, $\beta$, and $\eta$. Specifically, the ensemble adopts the following formulation:
\begin{align}
D_{\text{final}}= \begin{cases}\frac{1}{\alpha \frac{1}{D_1}+\beta \frac{1}{D_2}}, & \text{where}~ \left|\frac{1}{D_1}-\frac{1}{D_2}\right| / \frac{1}{D_2}<\eta \\ D_2, & \text{where}~ \left|\frac{1}{D_1}-\frac{1}{D_2}\right| / \frac{1}{D_2} \geq \eta\end{cases}~.
\end{align}


\noindent\textbf{Training Loss}.
In addition to the conventional monocular self-supervised losses used in MonoDepth2 \cite{godard2019monodepth2}, our overall framework is trained with the proposed APR loss. The APR loss measures the L-1 distance between the disparities estimated from the raw image ($D$) and the augmented image ($D_{A P R}$) as follows:
\begin{align}
\mathcal{L}_{A P R}=\left\|\frac{1}{D}-\frac{1}{D_{A P R}}\right\|_1~.
\end{align}

\subsubsection{Experimental Analysis}

\noindent\textbf{Implementation Details}.
Our model is trained on four NVIDIA Tesla V100 GPUs. The AdamW optimizer \cite{loshchilov2018adamw} is adopted and the learning rate is set to $1$e-$4$. \texttt{Model-I} is fine-tuned for $40$ epochs with the pre-trained weights from MonoViT \cite{zhao2021monovit}. \texttt{Model-II} is trained without APR loss for $83$ epochs and a further $244$ epochs with APR loss. The parameters of model ensemble are set as $\alpha = \frac{2}{3}$,  $\beta = \frac{1}{3}$, and $\eta = 0.45$, respectively.

\begin{table*}[t]
\caption{Quantitative results of the baselines and our proposed approaches on the RoboDepth competition leaderboard (Track \# 1). The \textbf{best} and \underline{second best} scores of each metric are highlighted in \textbf{bold} and \underline{underline}, respectively.}
\centering\scalebox{0.78}{
    \begin{tabular}{l|cccc|ccc}
    \toprule
    \textbf{Method} & \cellcolor{blue!10}\textbf{Abs Rel~$\downarrow$} & \cellcolor{blue!10}\textbf{Sq Rel~$\downarrow$} & \cellcolor{blue!10}\textbf{RMSE~$\downarrow$} & \cellcolor{blue!10}\textbf{log RMSE~$\downarrow$} & \cellcolor{red!10}$\delta<1.25$~$\uparrow$ & \cellcolor{red!10}$\delta<1.25^2$~$\uparrow$ & \cellcolor{red!10}$\delta<1.25^3$~$\uparrow$
    \\\midrule\midrule
    \rowcolor{gray!10}\multicolumn{8}{c}{\texttt{Model-I}}
    \\\midrule
    MonoViT & $0.172$ & $1.340$ & $6.177$ & $0.258$ & $0.743$ & $0.910$ & $0.963$
    \\
    + APR & $0.140$ & $1.216$ & $5.448$ & $0.221$ & $0.830$ & $0.939$ & $0.974$
    \\
    + APR + BN $\rightarrow$ IN	& \underline{$0.129$} & $1.007$ & \underline{$5.066$} & \underline{$0.208$} & \underline{$0.849$} & \underline{$0.948$} & $0.977$
    \\\midrule\midrule
    \rowcolor{gray!10}\multicolumn{8}{c}{\texttt{Model-II}}
    \\\midrule
    Lite-Mono & $0.199$ & $1.642$ & $6.937$ & $0.293$ & $0.681$ & $0.880$ & $0.948$
    \\
    Lite-Mono-8m & $0.196$ & $1.569$ & $6.708$ & $0.287$ & $0.684$ & $0.884$ & $0.952$
    \\
    + Interact + Perturb & $0.133$ & \underline{$0.942$} & $5.115$ & $0.212$ & $0.832$ & $0.944$ & \underline{$0.978$}
    \\\midrule\midrule
    \rowcolor{gray!10}\multicolumn{8}{c}{\texttt{Model-I} \& \texttt{Model-II}}
    \\\midrule
    \textbf{Ensemble} & $\mathbf{0.124}$ & $\mathbf{0.871}$ & $\mathbf{4.904}$ & $\mathbf{0.202}$ & $\mathbf{0.851}$ & $\mathbf{0.951}$ & $\mathbf{0.980}$
    \\\bottomrule
\end{tabular}
}
\label{tab:track1_innov1_results}
\end{table*}

\noindent\textbf{Main Results}.
Table~\ref{tab:track1_innov1_results} shows the comparative and ablation results of \texttt{Model-I}, \texttt{Model-II}, and the fusion between them. For \texttt{Model-I}, we observe that the amplitude-phase recombination operation and statistical normalization help improve the depth estimation performance over the baseline MonoViT \cite{zhao2021monovit}. For \texttt{Model-II}, we can see that the double-path feature interaction and median-normalization modules are conducive to enhancing Lite-Mono \cite{zhang2023litemono} under OoD scenarios. Finally, an ensemble of both \texttt{Model-I} and \texttt{Model-II} brings a significantly positive impact on the robustness of self-supervised depth estimation models.

\subsubsection{Solution Summary}
In this work, we proposed two stand-alone models for robustness enhancement: \texttt{Model-I} adopted an amplitude-phase recombination operation and instance normalization for noise suppression; \texttt{Model-II} are equipped with a dual-path architecture with median-normalization, channel perturbation, and feature interaction for OoD generalization enhancement. As a result, our team achieved the innovative prize in the first track of the RoboDepth Challenge.
