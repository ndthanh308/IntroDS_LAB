@article{abadie2016matching,
	title = {Matching on the {Estimated} {Propensity} {Score}},
	volume = {84},
	issn = {0012-9682},
	url = {https://www.jstor.org/stable/43866448},
	abstract = {Propensity score matching estimators (Rosenbaum and Rubin (1983)) are widely used in evaluation research to estimate average treatment effects. In this article, we derive the large sample distribution of propensity score matching estimators. Our derivations take into account that the propensity score is itself estimated in a first step, prior to matching. We prove that first step estimation of the propensity score affects the large sample distribution of propensity score matching estimators, and derive adjustments to the large sample variances of propensity score matching estimators of the average treatment effect (ATE) and the average treatment effect on the treated (ATET). The adjustment for the ATE estimator is negative (or zero in some special cases), implying that matching on the estimated propensity score is more efficient than matching on the true propensity score in large samples. However, for the ATET estimator, the sign of the adjustment term depends on the data generating process, and ignoring the estimation error in the propensity score may lead to confidence intervals that are either too large or too small.},
	number = {2},
	urldate = {2024-01-16},
	journal = {Econometrica},
	author = {Abadie, Alberto and Imbens, Guido W.},
	year = {2016},
	note = {Publisher: [Wiley, The Econometric Society]},
	pages = {781--807},
}

@article{abadie2012martingale,
	title = {A {Martingale} {Representation} for {Matching} {Estimators}},
	volume = {107},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2012.682537},
	doi = {10.1080/01621459.2012.682537},
	abstract = {Matching estimators are widely used in statistical data analysis. However, the large sample distribution of matching estimators has been derived only for particular cases. This article establishes a martingale representation for matching estimators. This representation allows the use of martingale limit theorems to derive the large sample distribution of matching estimators. As an illustration of the applicability of the theory, we derive the asymptotic distribution of a matching estimator when matching is carried out without replacement, a result previously unavailable in the literature. In addition, we apply the techniques proposed in this article to derive a correction to the standard error of a sample mean when missing data are imputed using the “hot deck,” a matching imputation method widely used in the Current Population Survey (CPS) and other large surveys in the social sciences. We demonstrate the empirical relevance of our methods using two Monte Carlo designs based on actual datasets. In these Monte Carlo exercises, the large sample distribution of matching estimators derived in this article provides an accurate approximation to the small sample behavior of these estimators. In addition, our simulations show that standard errors that do not take into account hot-deck imputation of missing data may be severely downward biased, while standard errors that incorporate the correction for hot-deck imputation perform extremely well. This article has online supplementary materials.},
	number = {498},
	urldate = {2024-01-16},
	journal = {Journal of the American Statistical Association},
	author = {Abadie, Alberto and Imbens, Guido W.},
	month = jun,
	year = {2012},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01621459.2012.682537},
	keywords = {Hot-deck imputation, Martingales, Overt bias, Treatment effects},
	pages = {833--843},
}

@misc{cai2022performance,
	title = {On the {Performance} of the {Neyman} {Allocation} with {Small} {Pilots}},
	url = {http://arxiv.org/abs/2206.04643},
	doi = {10.48550/arXiv.2206.04643},
	abstract = {The Neyman Allocation and its conditional counterpart are used in many papers on experiment design, which typically assume that researchers have access to large pilot studies. This may not be realistic. To understand the properties of the Neyman Allocation with small pilots, we study its behavior in a novel asymptotic framework for two-wave experiments in which the pilot size is assumed to be fixed while the main wave sample size grows. Our analysis shows that the Neyman Allocation can lead to estimates of the ATE with higher asymptotic variance than with (non-adaptive) balanced randomization. In particular, this happens when the outcome variable is relatively homoskedastic with respect to treatment status or when it exhibits high kurtosis. We also provide a series of empirical examples showing that these situations arise frequently in practice. Our results suggest that researchers should not use the Neyman Allocation with small pilots, especially in such instances.},
	urldate = {2024-01-16},
	publisher = {arXiv},
	author = {Cai, Yong and Rafi, Ahnaf},
	month = aug,
	year = {2022},
	note = {arXiv:2206.04643 [econ]},
	keywords = {Economics - Econometrics},
}

@article{abadie2008failure,
	title = {On the {Failure} of the {Bootstrap} for {Matching} {Estimators}},
	volume = {76},
	issn = {0012-9682},
	url = {https://www.jstor.org/stable/40056514},
	abstract = {Matching estimators are widely used in empirical economics for the evaluation of programs or treatments. Researchers using matching methods often apply the bootstrap to calculate the standard errors. However, no formal justification has been provided for the use of the bootstrap in this setting. In this article, we show that the standard bootstrap is, in general, not valid for matching estimators, even in the simple case with a single continuous covariate where the estimator is root-N consistent and asymptotically normally distributed with zero asymptotic bias. Valid inferential methods in this setting are the analytic asymptotic variance estimator of Abadie and Imbens (2006a) as well as certain modifications of the standard bootstrap, like the subsampling methods in Politis and Romanoǐ (1994).},
	number = {6},
	urldate = {2023-04-14},
	journal = {Econometrica},
	author = {Abadie, Alberto and Imbens, Guido W.},
	year = {2008},
	note = {Publisher: [Wiley, Econometric Society]},
	pages = {1537--1557},
}

@article{abadie2006large,
	title = {Large {Sample} {Properties} of {Matching} {Estimators} for {Average} {Treatment} {Effects}},
	volume = {74},
	issn = {0012-9682},
	url = {https://www.jstor.org/stable/3598929},
	abstract = {Matching estimators for average treatment effects are widely used in evaluation research despite the fact that their large sample properties have not been established in many cases. The absence of formal results in this area may be partly due to the fact that standard asymptotic expansions do not apply to matching estimators with a fixed number of matches because such estimators are highly nonsmooth functionals of the data. In this article we develop new methods for analyzing the large sample properties of matching estimators and establish a number of new results. We focus on matching with replacement with a fixed number of matches. First, we show that matching estimators are not N1/2-consistent in general and describe conditions under which matching estimators do attain N1/2-consistency. Second, we show that even in settings where matching estimators are N1/2-consistent, simple matching estimators with a fixed number of matches do not attain the semiparametric efficiency bound. Third, we provide a consistent estimator for the large sample variance that does not require consistent nonparametric estimation of unknown functions. Software for implementing these methods is available in Matlab, Stata, and R.},
	number = {1},
	urldate = {2023-04-14},
	journal = {Econometrica},
	author = {Abadie, Alberto and Imbens, Guido W.},
	year = {2006},
	note = {Publisher: [Wiley, Econometric Society]},
	pages = {235--267},
}

@article{imbens2004nonparametric,
	title = {Nonparametric {Estimation} of {Average} {Treatment} {Effects} {Under} {Exogeneity}: {A} {Review}},
	volume = {86},
	issn = {0034-6535},
	shorttitle = {Nonparametric {Estimation} of {Average} {Treatment} {Effects} {Under} {Exogeneity}},
	url = {https://doi.org/10.1162/003465304323023651},
	doi = {10.1162/003465304323023651},
	abstract = {Recently there has been a surge in econometric work focusing on estimating average treatment effects under various sets of assumptions. One strand of this literature has developed methods for estimating average treatment effects for a binary treatment under assumptions variously described as exogeneity, unconfoundedness, or selection on observables. The implication of these assumptions is that systematic (for example, average or distributional) differences in outcomes between treated and control units with the same values for the covariates are attributable to the treatment. Recent analysis has considered estimation and inference for average treatment effects under weaker assumptions than typical of the earlier literature by avoiding distributional and functional-form assumptions. Various methods of semiparametric estimation have been proposed, including estimating the unknown regression functions, matching, methods using the propensity score such as weighting and blocking,
and combinations of these approaches. In this paper I review the state of this literature and discuss some of its unanswered questions, focusing in particular on the practical implementation of these methods, the plausibility of this exogeneity assumption in economic applications, the relative performance of the various semiparametric estimators when the key assumptions (unconfoundedness and overlap) are satisfied, alternative estimands such as quantile treatment effects, and alternate methods such as Bayesian inference.},
	number = {1},
	urldate = {2024-01-16},
	journal = {The Review of Economics and Statistics},
	author = {Imbens, Guido W.},
	month = feb,
	year = {2004},
	pages = {4--29},
}


@article{hirano2001estimation,
	title = {Estimation of {Causal} {Effects} using {Propensity} {Score} {Weighting}: {An} {Application} to {Data} on {Right} {Heart} {Catheterization}},
	volume = {2},
	issn = {1572-9400},
	shorttitle = {Estimation of {Causal} {Effects} using {Propensity} {Score} {Weighting}},
	url = {https://doi.org/10.1023/A:1020371312283},
	doi = {10.1023/A:1020371312283},
	abstract = {We consider methods for estimating causal effects of treatments when treatment assignment is unconfounded with outcomes conditional on a possibly large set of covariates. Robins and Rotnitzky (1995) suggested combining regression adjustment with weighting based on the propensity score (Rosenbaum and Rubin, 1983). We adopt this approach, allowing for a flexible specification of both the propensity score and the regression function. We apply these methods to data on the effects of right heart catheterization (RHC) studied in Connors et al (1996), and we find that our estimator gives stable estimates over a wide range of values for the two parameters governing the selection of variables.},
	language = {en},
	number = {3},
	urldate = {2021-01-31},
	journal = {Health Services and Outcomes Research Methodology},
	author = {Hirano, Keisuke and Imbens, Guido W.},
	month = dec,
	year = {2001},
	pages = {259--278},
}

@article{imbens2007mean-squared,
	title = {Mean-Squared-Error Calculations for Average Treatment Effects},
	year = {2007},
	author = {Guido Imbens and Whitney Newey and Geert Ridder},
    journal = {working paper}
}

@article{newey1994large,
  title={Large sample estimation and hypothesis testing},
  author={Newey, Whitney K and McFadden, Daniel},
  journal={Handbook of econometrics},
  volume={4},
  pages={2111--2245},
  year={1994},
  publisher={Elsevier}
}

@article{duflo2007using,
  title={Using randomization in development economics research: A toolkit},
  author={Duflo, Esther and Glennerster, Rachel and Kremer, Michael},
  journal={Handbook of development economics},
  volume={4},
  pages={3895--3962},
  year={2007},
  publisher={Elsevier}
}

@book{rosenberger2015randomization,
  title={Randomization in clinical trials: theory and practice},
  author={Rosenberger, William F and Lachin, John M},
  year={2015},
  publisher={John Wiley \& Sons}
}

@book{vershynin2018high-dimensional,
	address = {Cambridge},
	series = {Cambridge {Series} in {Statistical} and {Probabilistic} {Mathematics}},
	title = {High-{Dimensional} {Probability}: {An} {Introduction} with {Applications} in {Data} {Science}},
	isbn = {978-1-108-41519-4},
	shorttitle = {High-{Dimensional} {Probability}},
	url = {https://www.cambridge.org/core/books/highdimensional-probability/797C466DA29743D2C8213493BD2D2102},
	abstract = {High-dimensional probability offers insight into the behavior of random vectors, random matrices, random subspaces, and objects used to quantify uncertainty in high dimensions. Drawing on ideas from probability, analysis, and geometry, it lends itself to applications in mathematics, statistics, theoretical computer science, signal processing, optimization, and more. It is the first to integrate theory, key tools, and modern applications of high-dimensional probability. Concentration inequalities form the core, and it covers both classical results such as Hoeffding's and Chernoff's inequalities and modern developments such as the matrix Bernstein's inequality. It then introduces the powerful methods based on stochastic processes, including such tools as Slepian's, Sudakov's, and Dudley's inequalities, as well as generic chaining and bounds based on VC dimension. A broad range of illustrations is embedded throughout, including classical and modern results for covariance estimation, clustering, networks, semidefinite programming, coding, dimension reduction, matrix completion, machine learning, compressed sensing, and sparse regression.},
	urldate = {2023-06-22},
	publisher = {Cambridge University Press},
	author = {Vershynin, Roman},
	year = {2018},
	doi = {10.1017/9781108231596},
}

@book{dudley2014uniform,
	address = {Cambridge},
	edition = {2},
	series = {Cambridge {Studies} in {Advanced} {Mathematics}},
	title = {Uniform {Central} {Limit} {Theorems}},
	isbn = {978-0-521-49884-5},
	urldate = {2020-09-12},
	publisher = {Cambridge University Press},
	author = {Dudley, R. M.},
	year = {2014},
}

@article{van_der_vaart1991differentiable,
	title = {On {Differentiable} {Functionals}},
	volume = {19},
	issn = {0090-5364},
	url = {https://www.jstor.org/stable/2241850},
	abstract = {Given a sample of size n from a distribution Pλ, one wants to estimate a functional ψ(λ) of the (typically infinite-dimensional) parameter λ. Lower bounds on the performance of estimators can be based on the concept of a differentiable functional Pλ → ψ(λ). In this paper we relate a suitable definition of differentiable functional to differentiability of α → dP1/2 λ and λ → ψ(λ). Moreover, we show that regular estimability of a functional implies its differentiability.},
	number = {1},
	urldate = {2021-03-30},
	journal = {The Annals of Statistics},
	author = {van der Vaart, Aad},
	year = {1991},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {178--204},
	file = {Full Text:/Users/yuehaob/Zotero/storage/3R9LAU3K/van der Vaart - 1991 - On Differentiable Functionals.pdf:application/pdf},
}

@article{newey1994asymptotic,
	title = {The {Asymptotic} {Variance} of {Semiparametric} {Estimators}},
	volume = {62},
	issn = {0012-9682},
	url = {https://www.jstor.org/stable/2951752},
	doi = {10.2307/2951752},
	abstract = {The purpose of this paper is the presentation of a general formula for the asymptotic variance of a semiparametric estimator. A particularly important feature of this formula is a way of accounting for the presence of nonparametric estimates of nuisance functions. The general form of an adjustment factor for nonparametric estimates is derived and analyzed. The usefulness of the formula is illustrated by deriving propositions on invariance of the limiting distribution with respect to the nonparametric estimator, conditions for nonparametric estimation to have no effect on the asymptotic distribution, and the form of a correction term for the presence of nonparametric projection and density estimators. Examples discussed are quasi-maximum likelihood estimation of index models, panel probit with semiparametric individual effects, average derivatives, and inverse density weighted least squares. The paper also develops a set of regularity conditions for the validity of the asymptotic variance formula. Primitive regularity conditions are derived for {\textless}tex-math{\textgreater}\${\textbackslash}sqrt\{n\}{\textbackslash}text\{-consistency\}\${\textless}/tex-math{\textgreater} and asymptotic normality for functions of series estimators of projections. Specific examples are polynomial estimators of average derivative and semiparametric panel probit models.},
	number = {6},
	urldate = {2021-04-28},
	journal = {Econometrica},
	author = {Newey, Whitney K.},
	year = {1994},
	note = {Publisher: [Wiley, Econometric Society]},
	pages = {1349--1382},
	file = {Full Text:/Users/yuehaob/Zotero/storage/ER65SGJ8/Newey - 1994 - The Asymptotic Variance of Semiparametric Estimato.pdf:application/pdf},
}

@article{van_der_vaart1991asymptotic,
	title = {An {Asymptotic} {Representation} {Theorem}},
	volume = {59},
	issn = {0306-7734},
	url = {https://www.jstor.org/stable/1403577},
	doi = {10.2307/1403577},
	abstract = {Let a sequence of statistical experiments converge to a limit experiment in the sense of Le Cam (1972). Furthermore assume that a sequence of statistics possesses a limit distribution under every statistical parameter. Then its set of limit distributions is also the set of distributions of some randomized estimator in the limit experiment. This is a simple way of saying that the limit experiment is a 'lower bound' for the converging sequence of experiments. Moreover, convolution and minimax theorems can be obtained as corollaries. /// Soit une suite d'expériences stochastiques convergeant vers une expérience limite au sens de Le Cam (1972). Supposons aussi qu'une suite de statistiques a une distribution limite pour toute valeur possible du paramètre, alors l'ensemble des distributions limites est l'ensemble des distributions d'un estimateur randomisé dans l'expérience limite. Ceci exprime de façon simple que l'expérience limite est une 'borne inférieure' pour la suite convergeante d'expériences. De plus, l'on peut obtenir comme corollaires des résultats minimax et des théorèmes de convolution.},
	number = {1},
	urldate = {2021-03-30},
	journal = {International Statistical Review / Revue Internationale de Statistique},
	author = {van der Vaart, Aad},
	year = {1991},
	note = {Publisher: [Wiley, International Statistical Institute (ISI)]},
	pages = {97--121},
	file = {Full Text:/Users/yuehaob/Zotero/storage/J8KG2QAH/van der Vaart - 1991 - An Asymptotic Representation Theorem.pdf:application/pdf},
}

@article{robins1997toward,
	title = {Toward a {Curse} of {Dimensionality} {Appropriate} (coda) {Asymptotic} {Theory} for {Semi}-{Parametric} {Models}},
	volume = {16},
	copyright = {Copyright © 1997 John Wiley \& Sons, Ltd.},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-0258%2819970215%2916%3A3%3C285%3A%3AAID-SIM535%3E3.0.CO%3B2-%23},
	doi = {https://doi.org/10.1002/(SICI)1097-0258(19970215)16:3<285::AID-SIM535>3.0.CO;2-#},
	abstract = {We argue, that due to the curse of dimensionality, there are major difficulties with any pure or smoothed likelihood-based method of inference in designed studies with randomly missing data when missingness depends on a high-dimensional vector of variables. We study in detail a semi-parametric superpopulation version of continuously stratified random sampling. We show that all estimators of the population mean that are uniformly consistent or that achieve an algebraic rate of convergence, no matter how slow, require the use of the selection (randomization) probabilities. We argue that, in contrast to likelihood methods which ignore these probabilities, inverse selection probability weighted estimators continue to perform well achieving uniform n1/2-rates of convergence. We propose a curse of dimensionality appropriate (CODA) asymptotic theory for inference in non- and semi-parametric models in an attempt to formalize our arguments. We discuss whether our results constitute a fatal blow to the likelihood principle and study the attitude toward these that a committed subjective Bayesian would adopt. Finally, we apply our CODA theory to analyse the effect of the ‘curse of dimensionality’ in several interesting semi-parametric models, including a model for a two-armed randomized trial with randomization probabilities depending on a vector of continuous pre-treatment covariates X. We provide substantive settings under which a subjective Bayesian would ignore the randomization probabilities in analysing the trial data. We then show that any statistician who ignores the randomization probabilities is unable to construct nominal 95 per cent confidence intervals for the true treatment effect that have both: (i) an expected length which goes to zero with increasing sample size; and (ii) a guaranteed expected actual coverage rate of at least 95 per cent over the ensemble of trials analysed by the statistician during his or her lifetime. However, we derive a new interval estimator, depending on the Randomization probabilities, that satisfies (i) and (ii). © 1997 by John Wiley \& Sons, Ltd.},
	language = {en},
	number = {3},
	urldate = {2021-02-02},
	journal = {Statistics in Medicine},
	author = {Robins, James M. and Ritov, Ya'acov},
	year = {1997},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/\%28SICI\%291097-0258\%2819970215\%2916\%3A3\%3C285\%3A\%3AAID-SIM535\%3E3.0.CO\%3B2-\%23},
	pages = {285--319},
}

@article{hirano2003efficient,
	title = {Efficient {Estimation} of {Average} {Treatment} {Effects} {Using} the {Estimated} {Propensity} {Score}},
	volume = {71},
	issn = {1468-0262},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-0262.00442},
	doi = {https://doi.org/10.1111/1468-0262.00442},
	abstract = {We are interested in estimating the average effect of a binary treatment on a scalar outcome. If assignment to the treatment is exogenous or unconfounded, that is, independent of the potential outcomes given covariates, biases associated with simple treatment-control average comparisons can be removed by adjusting for differences in the covariates. Rosenbaum and Rubin (1983) show that adjusting solely for differences between treated and control units in the propensity score removes all biases associated with differences in covariates. Although adjusting for differences in the propensity score removes all the bias, this can come at the expense of efficiency, as shown by Hahn (1998), Heckman, Ichimura, and Todd (1998), and Robins, Mark, and Newey (1992). We show that weighting by the inverse of a nonparametric estimate of the propensity score, rather than the true propensity score, leads to an efficient estimate of the average treatment effect. We provide intuition for this result by showing that this estimator can be interpreted as an empirical likelihood estimator that efficiently incorporates the information about the propensity score.},
	language = {en},
	number = {4},
	urldate = {2021-02-02},
	journal = {Econometrica},
	author = {Hirano, Keisuke and Imbens, Guido W. and Ridder, Geert},
	year = {2003},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1468-0262.00442},
	pages = {1161--1189},
}

@book{van_der_vaart1996weak,
	address = {New York},
	series = {Springer {Series} in {Statistics}},
	title = {Weak {Convergence} and {Empirical} {Processes}: {With} {Applications} to {Statistics}},
	isbn = {978-0-387-94640-5},
	shorttitle = {Weak {Convergence} and {Empirical} {Processes}},
	url = {https://www.springer.com/gp/book/9780387946405},
	abstract = {This book tries to do three things. The first goal is to give an exposition of certain modes of stochastic convergence, in particular convergence in distribution. The classical theory of this subject was developed mostly in the 1950s and is well summarized in Billingsley (1968). During the last 15 years, the need for a more general theory allowing random elements that are not Borel measurable has become well established, particularly in developing the theory of empirical processes. Part 1 of the book, Stochastic Convergence, gives an exposition of such a theory following the ideas of J. Hoffmann-J!1Jrgensen and R. M. Dudley. A second goal is to use the weak convergence theory background devel­ oped in Part 1 to present an account of major components of the modern theory of empirical processes indexed by classes of sets and functions. The weak convergence theory developed in Part 1 is important for this, simply because the empirical processes studied in Part 2, Empirical Processes, are naturally viewed as taking values in nonseparable Banach spaces, even in the most elementary cases, and are typically not Borel measurable. Much of the theory presented in Part 2 has previously been scattered in the journal literature and has, as a result, been accessible only to a relatively small number of specialists. In view of the importance of this theory for statis­ tics, we hope that the presentation given here will make this theory more accessible to statisticians as well as to probabilists interested in statistical applications.},
	language = {en},
	urldate = {2020-12-03},
	publisher = {Springer-Verlag},
	author = {van der Vaart, A. W. and Wellner, Jon},
	year = {1996},
	doi = {10.1007/978-1-4757-2545-2},
}

@book{lehmann2005testing,
	address = {New York},
	edition = {3},
	series = {Springer {Texts} in {Statistics}},
	title = {Testing {Statistical} {Hypotheses}},
	isbn = {978-0-387-98864-1},
	url = {https://www.springer.com/gp/book/9780387988641},
	abstract = {The third edition of Testing Statistical Hypotheses updates and expands upon the classic graduate text, emphasizing optimality theory for hypothesis testing and confidence sets. The principal additions include a rigorous treatment of large sample optimality, together with the requisite tools. In addition, an introduction to the theory of resampling methods such as the bootstrap is developed. The sections on multiple testing and goodness of fit testing are expanded. The text is suitable for Ph.D. students in statistics and includes over 300 new problems out of a total of more than 760. E.L. Lehmann is Professor of Statistics Emeritus at the University of California, Berkeley. He is a member of the National Academy of Sciences and the American Academy of Arts and Sciences, and the recipient of honorary degrees from the University of Leiden, The Netherlands and the University of Chicago. He is the author of Elements of Large-Sample Theory and (with George Casella) he is also the author of Theory of Point Estimation, Second Edition. Joseph P. Romano is Professor of Statistics at Stanford University. He is a recipient of a Presidential Young Investigator Award and a Fellow of the Institute of Mathematical Statistics. He has coauthored two other books, Subsampling with Dimitris Politis and Michael Wolf, and Counterexamples in Probability and Statistics with Andrew Siegel.},
	language = {en},
	urldate = {2020-12-03},
	publisher = {Springer-Verlag},
	author = {Lehmann, Erich L. and Romano, Joseph P.},
	year = {2005},
	doi = {10.1007/0-387-27605-X},
}

@book{ledoux1991probability,
	address = {Berlin Heidelberg},
	series = {Classics in {Mathematics}},
	title = {Probability in {Banach} {Spaces}: {Isoperimetry} and {Processes}},
	isbn = {978-3-642-20211-7},
	shorttitle = {Probability in {Banach} {Spaces}},
	url = {https://www.springer.com/gp/book/9783642202117},
	abstract = {Isoperimetric, measure concentration and random process techniques appear at the basis of the modern understanding of Probability in Banach spaces. Based on these tools, the book presents a complete treatment of the main aspects of Probability in Banach spaces (integrability and limit theorems for vector valued random variables, boundedness and continuity of random processes) and of some of their links to Geometry of Banach spaces (via the type and cotype properties). Its purpose is to present some of the main aspects of this theory, from the foundations to the most important achievements. The main features of the investigation are the systematic use of isoperimetry and concentration of measure and abstract random process techniques (entropy and majorizing measures). Examples of these probabilistic tools and ideas to classical Banach space theory are further developed.},
	language = {en},
	urldate = {2020-12-03},
	publisher = {Springer-Verlag},
	author = {Ledoux, Michel and Talagrand, Michel},
	year = {1991},
	doi = {10.1007/978-3-642-20212-4},
}

@book{kosorok2008introduction,
	address = {New York},
	series = {Springer {Series} in {Statistics}},
	title = {Introduction to {Empirical} {Processes} and {Semiparametric} {Inference}},
	isbn = {978-0-387-74977-8},
	url = {https://www.springer.com/gp/book/9780387749778},
	abstract = {This book provides a self-contained, linear, and unified introduction to empirical processes and semiparametric inference. These powerful research techniques are surprisingly useful for developing methods of statistical inference for complex models and in understanding the properties of such methods. The targeted audience includes statisticians, biostatisticians, and other researchers with a background in mathematical statistics who have an interest in learning about and doing research in empirical processes and semiparametric inference but who would like to have a friendly and gradual introduction to the area. The book can be used either as a research reference or as a textbook. The level of the book is suitable for a second year graduate course in statistics or biostatistics, provided the students have had a year of graduate level mathematical statistics and a semester of probability.The book consists of three parts. The first part is a concise overview of all of the main concepts covered in the book with a minimum of technicalities. The second and third parts cover the two respective main topics of empirical processes and semiparametric inference in depth. The connections between these two topics is also demonstrated and emphasized throughout the text. Each part has a final chapter with several case studies that use concrete examples to illustrate the concepts developed so far. The last two parts also each include a chapter which covers the needed mathematical preliminaries. Each main idea is introduced with a non-technical motivation, and examples are given throughout to illustrate important concepts. Homework problems are also included at the end of each chapter to help the reader gain additional insights.Michael R. Kosorok is Professor and Chair, Department of Biostatistics, and Professor, Department of Statistics and Operations Research, at the University of North Carolina at Chapel Hill. His research has focused on the application of empirical processes and semiparametric inference to statistics and biostatistics. He is a Fellow of both the American Statistical Association and the Institute of Mathematical Statistics. He is an Associate Editor of the Annals of Statistics, Electronic Journal of Statistics, International Journal of Biostatistics, Statistics and Probability Letters, and Statistics Surveys.},
	language = {en},
	urldate = {2020-12-03},
	publisher = {Springer-Verlag},
	author = {Kosorok, Michael R.},
	year = {2008},
	doi = {10.1007/978-0-387-74978-5},
}

@book{van_der_vaart1998asymptotic,
	series = {Cambridge {Series} in {Statistical} and {Probabilistic} {Mathematics}},
	title = {Asymptotic statistics},
	volume = {3},
	isbn = {0-521-49603-9 0-521-78450-6},
	url = {https://doi-org.proxy.lib.umich.edu/10.1017/CBO9780511802256},
	publisher = {Cambridge University Press, Cambridge},
	author = {van der Vaart, A. W.},
	year = {1998},
	mrnumber = {1652247},
	doi = {10.1017/CBO9780511802256},
}

@article{bugni2018inference,
	title = {Inference {Under} {Covariate}-{Adaptive} {Randomization}},
	volume = {113},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2017.1375934},
	doi = {10.1080/01621459.2017.1375934},
	abstract = {This article studies inference for the average treatment effect in randomized controlled trials with covariate-adaptive randomization. Here, by covariate-adaptive randomization, we mean randomization schemes that first stratify according to baseline covariates and then assign treatment status so as to achieve “balance” within each stratum. Our main requirement is that the randomization scheme assigns treatment status within each stratum so that the fraction of units being assigned to treatment within each stratum has a well behaved distribution centered around a proportion π as the sample size tends to infinity. Such schemes include, for example, Efron’s biased-coin design and stratified block randomization. When testing the null hypothesis that the average treatment effect equals a prespecified value in such settings, we first show the usual two-sample t-test is conservative in the sense that it has limiting rejection probability under the null hypothesis no greater than and typically strictly less than the nominal level. We show, however, that a simple adjustment to the usual standard error of the two-sample t-test leads to a test that is exact in the sense that its limiting rejection probability under the null hypothesis equals the nominal level. Next, we consider the usual t-test (on the coefficient on treatment assignment) in a linear regression of outcomes on treatment assignment and indicators for each of the strata. We show that this test is exact for the important special case of randomization schemes with π=12, but is otherwise conservative. We again provide a simple adjustment to the standard errors that yields an exact test more generally. Finally, we study the behavior of a modified version of a permutation test, which we refer to as the covariate-adaptive permutation test, that only permutes treatment status for units within the same stratum. When applied to the usual two-sample t-statistic, we show that this test is exact for randomization schemes with π=12 and that additionally achieve what we refer to as “strong balance.” For randomization schemes with π≠12, this test may have limiting rejection probability under the null hypothesis strictly greater than the nominal level. When applied to a suitably adjusted version of the two-sample t-statistic, however, we show that this test is exact for all randomization schemes that achieve “strong balance,” including those with π≠12. A simulation study confirms the practical relevance of our theoretical results. We conclude with recommendations for empirical practice and an empirical illustration. Supplementary materials for this article are available online.},
	number = {524},
	urldate = {2020-12-03},
	journal = {Journal of the American Statistical Association},
	author = {Bugni, Federico A. and Canay, Ivan A. and Shaikh, Azeem M.},
	month = oct,
	year = {2018},
	pmid = {30906087},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01621459.2017.1375934},
	pages = {1784--1796},
}

@article{bugni2019inference,
	title = {Inference under covariate-adaptive randomization with multiple treatments},
	volume = {10},
	copyright = {Copyright © 2019 The Authors.},
	issn = {1759-7331},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/QE1150},
	doi = {https://doi.org/10.3982/QE1150},
	abstract = {This paper studies inference in randomized controlled trials with covariate-adaptive randomization when there are multiple treatments. More specifically, we study in this setting inference about the average effect of one or more treatments relative to other treatments or a control. As in Bugni, Canay, and Shaikh (2018), covariate-adaptive randomization refers to randomization schemes that first stratify according to baseline covariates and then assign treatment status so as to achieve “balance” within each stratum. Importantly, in contrast to Bugni, Canay, and Shaikh (2018), we not only allow for multiple treatments, but further allow for the proportion of units being assigned to each of the treatments to vary across strata. We first study the properties of estimators derived from a “fully saturated” linear regression, that is, a linear regression of the outcome on all interactions between indicators for each of the treatments and indicators for each of the strata. We show that tests based on these estimators using the usual heteroskedasticity-consistent estimator of the asymptotic variance are invalid in the sense that they may have limiting rejection probability under the null hypothesis strictly greater than the nominal level; on the other hand, tests based on these estimators and suitable estimators of the asymptotic variance that we provide are exact in the sense that they have limiting rejection probability under the null hypothesis equal to the nominal level. For the special case in which the target proportion of units being assigned to each of the treatments does not vary across strata, we additionally consider tests based on estimators derived from a linear regression with “strata fixed effects,” that is, a linear regression of the outcome on indicators for each of the treatments and indicators for each of the strata. We show that tests based on these estimators using the usual heteroskedasticity-consistent estimator of the asymptotic variance are conservative in the sense that they have limiting rejection probability under the null hypothesis no greater than and typically strictly less than the nominal level, but tests based on these estimators and suitable estimators of the asymptotic variance that we provide are exact, thereby generalizing results in Bugni, Canay, and Shaikh (2018) for the case of a single treatment to multiple treatments. A simulation study and an empirical application illustrate the practical relevance of our theoretical results.},
	language = {en},
	number = {4},
	urldate = {2020-12-03},
	journal = {Quantitative Economics},
	author = {Bugni, Federico A. and Canay, Ivan A. and Shaikh, Azeem M.},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.3982/QE1150},
	keywords = {C12, C14, Covariate-adaptive randomization, Efron's biased-coin design, multiple treatments, randomized controlled trial, saturated regression, strata fixed effects, stratified block randomization, treatment assignment},
	pages = {1747--1785},
}

@article{chernozhukov2017doubledebiasedneyman,
	title = {Double/{Debiased}/{Neyman} {Machine} {Learning} of {Treatment} {Effects}},
	volume = {107},
	issn = {0002-8282},
	url = {http://www.aeaweb.org/articles?id=10.1257/aer.p20171038},
	doi = {10.1257/aer.p20171038},
	abstract = {Chernozhukov et al. (2016) provide a generic double/de-biased machine learning (ML) approach for obtaining valid inferential statements about focal parameters, using Neyman-orthogonal scores and cross-fitting, in settings where nuisance parameters are estimated using ML methods. In this note, we illustrate the application of this method in the context of estimating average treatment effects and average treatment effects on the treated using observational data.},
	language = {en},
	number = {5},
	urldate = {2020-12-03},
	journal = {American Economic Review},
	author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney},
	month = may,
	year = {2017},
	keywords = {Quantile Regressions, Quantile Regressions, Multiple or Simultaneous Equation Models: Cross-Sectional Models, Single Equation Models, Single Variables: Cross-Sectional Models, Social Interaction Models, Spatial Models, Treatment Effect Models},
	pages = {261--265},
}

@article{hahn1998role,
	title = {On the {Role} of the {Propensity} {Score} in {Efficient} {Semiparametric} {Estimation} of {Average} {Treatment} {Effects}},
	volume = {66},
	issn = {0012-9682},
	url = {https://www.jstor.org/stable/2998560},
	doi = {10.2307/2998560},
	abstract = {In this paper, the role of the propensity score in the efficient estimation of average treatment effects is examined. Under the assumption that the treatment is ignorable given some observed characteristics, it is shown that the propensity score is ancillary for estimation of the average treatment effects. The propensity score is not ancillary for estimation of average treatment effects on the treated. It is suggested that the marginal value of the propensity score lies entirely in the "dimension reduction." Efficient semiparametric estimators of average treatment effects and average treatment effects on the treated are shown to take the form of relevant sample averages of the data completed by the nonparametric imputation method. It is shown that the projection on the propensity score is not necessary for efficient semiparametric estimation of average treatment effects on the treated even if the propensity score is known. An application to the experimental data reveals that conditioning on the propensity score may even result in a loss of efficiency.},
	number = {2},
	urldate = {2020-12-03},
	journal = {Econometrica},
	author = {Hahn, Jinyong},
	year = {1998},
	note = {Publisher: [Wiley, Econometric Society]},
	pages = {315--331},
	file = {Full Text:/Users/yuehaob/Zotero/storage/L9SWDBJT/Hahn - 1998 - On the Role of the Propensity Score in Efficient S.pdf:application/pdf},
}

@misc{bugni2022inference,
	title = {Inference for {Cluster} {Randomized} {Experiments} with {Non}-ignorable {Cluster} {Sizes}},
	url = {http://arxiv.org/abs/2204.08356},
	doi = {10.48550/arXiv.2204.08356},
	abstract = {This paper considers the problem of inference in cluster randomized experiments when cluster sizes are non-ignorable. Here, by a cluster randomized experiment, we mean one in which treatment is assigned at the level of the cluster; by non-ignorable cluster sizes we mean that "large" clusters and "small" clusters may be heterogeneous, and, in particular, the effects of the treatment may vary across clusters of differing sizes. In order to permit this sort of flexibility, we consider a sampling framework in which cluster sizes themselves are random. In this way, our analysis departs from earlier analyses of cluster randomized experiments in which cluster sizes are treated as non-random. We distinguish between two different parameters of interest: the equally-weighted cluster-level average treatment effect, and the size-weighted cluster-level average treatment effect. For each parameter, we provide methods for inference in an asymptotic framework where the number of clusters tends to infinity and treatment is assigned using simple random sampling. We additionally permit the experimenter to sample only a subset of the units within each cluster rather than the entire cluster and demonstrate the implications of such sampling for some commonly used estimators. A small simulation study shows the practical relevance of our theoretical results.},
	urldate = {2022-10-14},
	publisher = {arXiv},
	author = {Bugni, Federico and Canay, Ivan and Shaikh, Azeem and Tabord-Meehan, Max},
	month = jun,
	year = {2022},
	note = {arXiv:2204.08356 [econ, stat]},
	keywords = {Economics - Econometrics, Statistics - Methodology},
}

@article{robins1995analysis,
	title = {Analysis of {Semiparametric} {Regression} {Models} for {Repeated} {Outcomes} in the {Presence} of {Missing} {Data}},
	volume = {90},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2291134},
	doi = {10.2307/2291134},
	abstract = {We propose a class of inverse probability of censoring weighted estimators for the parameters of models for the dependence of the mean of a vector of correlated response variables on a vector of explanatory variables in the presence of missing response data. The proposed estimators do not require full specification of the likelihood. They can be viewed as an extension of generalized estimating equations estimators that allow for the data to be missing at random but not missing completely at random. These estimators can be used to correct for dependent censoring and nonrandom noncompliance in randomized clinical trials studying the effect of a treatment on the evolution over time of the mean of a response variable. The likelihood-based parametric G-computation algorithm estimator may also be used to attempt to correct for dependent censoring and nonrandom noncompliance. But because of possible model misspecification, the parametric G-computation algorithm estimator, in contrast with the proposed weighted estimators, may be inconsistent for the difference in treatment-arm-specific means, even when compliance is completely at random and censoring is independent. We illustrate our methods with the analysis of the effect of zidovudine (AZT) treatment on the evolution of mean CD4 count with data from an AIDS clinical trial.},
	number = {429},
	urldate = {2022-10-24},
	journal = {Journal of the American Statistical Association},
	author = {Robins, James M. and Rotnitzky, Andrea and Zhao, Lue Ping},
	year = {1995},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {106--121},
}

@article{bai2022optimality,
	title = {Optimality of {Matched}-{Pair} {Designs} in {Randomized} {Controlled} {Trials}},
	volume = {112},
	issn = {0002-8282},
	url = {https://www.aeaweb.org/articles?id=10.1257/aer.20201856},
	doi = {10.1257/aer.20201856},
	abstract = {In randomized controlled trials, treatment is often assigned by stratified randomization. I show that among all stratified randomization schemes that treat all units with probability one half, a certain matched-pair design achieves the maximum statistical precision for estimating the average treatment effect. In an important special case, the optimal design pairs units according to the baseline outcome. In a simulation study based on datasets from ten randomized controlled trials, this design lowers the standard error for the estimator of the average treatment effect by 10 percent on average, and by up to 34 percent, relative to the original designs.},
	language = {en},
	number = {12},
	urldate = {2022-11-28},
	journal = {American Economic Review},
	author = {Bai, Yuehao},
	month = dec,
	year = {2022},
	keywords = {Economics - Econometrics, ex-post bias, matched pairs, Matched-pair design, Mathematics - Statistics Theory, pilot experiment, randomized controlled trial, Statistics - Methodology, stratification, stratified randomization, treat- ment effect},
	pages = {3911--3940},
}

@article{bai2022inference,
	title = {Inference in {Experiments} {With} {Matched} {Pairs}},
	volume = {117},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2021.1883437},
	doi = {10.1080/01621459.2021.1883437},
	abstract = {This article studies inference for the average treatment effect in randomized controlled trials where treatment status is determined according to a “matched pairs” design. By a “matched pairs” design, we mean that units are sampled iid from the population of interest, paired according to observed, baseline covariates and finally, within each pair, one unit is selected at random for treatment. This type of design is used routinely throughout the sciences, but fundamental questions about its implications for inference about the average treatment effect remain. The main requirement underlying our analysis is that pairs are formed so that units within pairs are suitably “close” in terms of the baseline covariates, and we develop novel results to ensure that pairs are formed in a way that satisfies this condition. Under this assumption, we show that, for the problem of testing the null hypothesis that the average treatment effect equals a prespecified value in such settings, the commonly used two-sample t-test and “matched pairs” t-test are conservative in the sense that these tests have limiting rejection probability under the null hypothesis no greater than and typically strictly less than the nominal level. We show, however, that a simple adjustment to the standard errors of these tests leads to a test that is asymptotically exact in the sense that its limiting rejection probability under the null hypothesis equals the nominal level. We also study the behavior of randomization tests that arise naturally in these types of settings. When implemented appropriately, we show that this approach also leads to a test that is asymptotically exact in the sense described previously, but additionally has finite-sample rejection probability no greater than the nominal level for certain distributions satisfying the null hypothesis. A simulation study and empirical application confirm the practical relevance of our theoretical results.},
	number = {540},
	urldate = {2023-01-30},
	journal = {Journal of the American Statistical Association},
	author = {Bai, Yuehao and Romano, Joseph P. and Shaikh, Azeem M.},
	month = oct,
	year = {2022},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01621459.2021.1883437},
	pages = {1726--1737},
}

@article{tsiatis2008covariate,
	title = {Covariate adjustment for two-sample treatment comparisons in randomized clinical trials: a principled yet flexible approach},
	volume = {27},
	issn = {0277-6715},
	shorttitle = {Covariate adjustment for two-sample treatment comparisons in randomized clinical trials},
	doi = {10.1002/sim.3113},
	abstract = {There is considerable debate regarding whether and how covariate-adjusted analyses should be used in the comparison of treatments in randomized clinical trials. Substantial baseline covariate information is routinely collected in such trials, and one goal of adjustment is to exploit covariates associated with outcome to increase precision of estimation of the treatment effect. However, concerns are routinely raised over the potential for bias when the covariates used are selected post hoc and the potential for adjustment based on a model of the relationship between outcome, covariates, and treatment to invite a 'fishing expedition' for that leading to the most dramatic effect estimate. By appealing to the theory of semiparametrics, we are led naturally to a characterization of all treatment effect estimators and to principled, practically feasible methods for covariate adjustment that yield the desired gains in efficiency and that allow covariate relationships to be identified and exploited while circumventing the usual concerns. The methods and strategies for their implementation in practice are presented. Simulation studies and an application to data from an HIV clinical trial demonstrate the performance of the techniques relative to the existing methods.},
	language = {eng},
	number = {23},
	journal = {Statistics in Medicine},
	author = {Tsiatis, Anastasios A. and Davidian, Marie and Zhang, Min and Lu, Xiaomin},
	month = oct,
	year = {2008},
	pmid = {17960577},
	pmcid = {PMC2562926},
	keywords = {Algorithms, Data Interpretation, Statistical, Humans, Randomized Controlled Trials as Topic, Sampling Studies, Statistics, Nonparametric, Treatment Outcome},
	pages = {4658--4677},
}

@article{van_der_vaart1989asymptotic,
	title = {On the {Asymptotic} {Information} {Bound}},
	volume = {17},
	issn = {0090-5364},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-17/issue-4/On-the-Asymptotic-Information-Bound/10.1214/aos/1176347377.full},
	doi = {10.1214/aos/1176347377},
	language = {en},
	number = {4},
	urldate = {2023-04-04},
	journal = {The Annals of Statistics},
	author = {van der Vaart, Aad},
	month = dec,
	year = {1989},
	file = {Full Text:/Users/yuehaob/Zotero/storage/5NFEKDQ9/van der Vaart - 1989 - On the Asymptotic Information Bound.pdf:application/pdf},
}

@misc{bai2023revisiting,
	title = {Revisiting the {Analysis} of {Matched}-{Pair} and {Stratified} {Experiments} in the {Presence} of {Attrition}},
	url = {http://arxiv.org/abs/2209.11840},
	doi = {10.48550/arXiv.2209.11840},
	abstract = {In this paper we revisit some common recommendations regarding the analysis of matched-pair and stratified experimental designs in the presence of attrition. Our main objective is to clarify a number of well-known claims about the practice of dropping pairs with an attrited unit when analyzing matched-pair designs. Contradictory advice appears in the literature about whether or not dropping pairs is beneficial or harmful, and stratifying into larger groups has been recommended as a resolution to the issue. To address these claims, we derive the estimands obtained from the difference-in-means estimator in a matched-pair design both when the observations from pairs with an attrited unit are retained and when they are dropped. We find limited evidence to support the claims that dropping pairs helps recover the average treatment effect, but we find that it may potentially help in recovering a convex weighted average of conditional average treatment effects. We report similar findings for stratified designs when studying the estimands obtained from a regression of outcomes on treatment with and without strata fixed effects.},
	urldate = {2023-04-04},
	publisher = {arXiv},
	author = {Bai, Yuehao and Hsieh, Meng Hsuan and Liu, Jizhou and Tabord-Meehan, Max},
	month = feb,
	year = {2023},
	note = {arXiv:2209.11840 [econ, stat]},
}

@article{frolich2007nonparametric,
	title = {Nonparametric {IV} estimation of local average treatment effects with covariates},
	volume = {139},
	issn = {03044076},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304407606001023},
	doi = {10.1016/j.jeconom.2006.06.004},
	abstract = {In this paper nonparametric instrumental variable estimation of local average treatment effects (LATE) is extended to incorporate covariates. Estimation of LATE is appealing since identiﬁcation relies on much weaker assumptions than the identiﬁcation of average treatment effects in other nonparametric instrumental variable models. Including covariates in the estimation of LATE is necessary when the instrumental variable itself is confounded, such that the IV assumptions are valid only conditional on covariates. Previous approaches to handle covariates in the estimation of LATE relied on parametric or semiparametric methods. In this paper, a nonparametric estimator for the estimation of LATE with covariates is suggested that is root-n asymptotically normal and efﬁcient. r 2006 Elsevier B.V. All rights reserved.},
	language = {en},
	number = {1},
	urldate = {2023-04-04},
	journal = {Journal of Econometrics},
	author = {Frolich, Markus},
	month = jul,
	year = {2007},
	pages = {35--75},
	file = {Full Text:/Users/yuehaob/Zotero/storage/BBUPPLYP/Frölich - 2007 - Nonparametric IV estimation of local average treat.pdf:application/pdf},
}

@article{robins1995semiparametric,
	title = {Semiparametric {Efficiency} in {Multivariate} {Regression} {Models} with {Missing} {Data}},
	volume = {90},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1995.10476494},
	doi = {10.1080/01621459.1995.10476494},
	language = {en},
	number = {429},
	urldate = {2023-04-04},
	journal = {Journal of the American Statistical Association},
	author = {Robins, James M. and Rotnitzky, Andrea},
	month = mar,
	year = {1995},
	pages = {122--129},
}

@book{lehmann2022testing,
	address = {Cham},
	series = {Springer {Texts} in {Statistics}},
	title = {Testing {Statistical} {Hypotheses}},
	isbn = {978-3-030-70577-0 978-3-030-70578-7},
	url = {https://link.springer.com/10.1007/978-3-030-70578-7},
	language = {en},
	urldate = {2023-04-13},
	publisher = {Springer International Publishing},
	author = {Lehmann, E.L. and Romano, Joseph P.},
	year = {2022},
	doi = {10.1007/978-3-030-70578-7},
	keywords = {best fit, Excel, Resampling, Statistical Hypotheses, Statistical Theory},
}

@article{newey1990semiparametric,
	title = {Semiparametric {Efficiency} {Bounds}},
	volume = {5},
	issn = {0883-7252},
	url = {https://www.jstor.org/stable/2096601},
	abstract = {Semiparametric models are those where the functional form of some components is unknown. Efficiency bounds are of fundamental importance for such models. They provide a guide to estimation methods and give an asymptotic efficiency standard. The purpose of this paper is to provide an introduction to research methods and problems for semiparametric efficiency bounds. The nature of the bounds is discussed, as well as ways of calculating them. Their uses in solving estimation problems are outlined, including construction of semiparametric estimators and calculation of their limiting distribution. The paper includes new results as well as survey material.},
	number = {2},
	urldate = {2023-04-14},
	journal = {Journal of Applied Econometrics},
	author = {Newey, Whitney K.},
	year = {1990},
	note = {Publisher: Wiley},
	pages = {99--135},
	file = {Full Text:/Users/yuehaob/Zotero/storage/T4REXG77/Newey - 1990 - Semiparametric Efficiency Bounds.pdf:application/pdf},
}

@book{tsiatis2006semiparametric,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Semiparametric {Theory} and {Missing} {Data}},
	isbn = {978-0-387-32448-7},
	url = {http://link.springer.com/10.1007/0-387-37345-4},
	language = {en},
	urldate = {2023-04-15},
	publisher = {Springer},
	author = {Tsiatis, Anastasios A.},
	year = {2006},
	doi = {10.1007/0-387-37345-4},
	keywords = {average, estimator, likelihood, probability, semiparametric},
}

@article{firpo2007efficient,
	title = {Efficient {Semiparametric} {Estimation} of {Quantile} {Treatment} {Effects}},
	volume = {75},
	issn = {0012-9682},
	url = {https://www.jstor.org/stable/4123114},
	abstract = {This paper develops estimators for quantile treatment effects under the identifying restriction that selection to treatment is based on observable characteristics. Identification is achieved without requiring computation of the conditional quantiles of the potential outcomes. Instead, the identification results for the marginal quantiles lead to an estimation procedure for the quantile treatment effect parameters that has two steps: nonparametric estimation of the propensity score and computation of the difference between the solutions of two separate minimization problems. Root-N consistency, asymptotic normality, and achievement of the semiparametric efficiency bound are shown for that estimator. A consistent estimation procedure for the variance is also presented. Finally, the method developed here is applied to evaluation of a job training program and to a Monte Carlo exercise. Results from the empirical application indicate that the method works relatively well even for a data set with limited overlap between treated and controls in the support of covariates. The Monte Carlo study shows that, for a relatively small sample size, the method produces estimates with good precision and low bias, especially for middle quantiles.},
	number = {1},
	urldate = {2023-04-18},
	journal = {Econometrica},
	author = {Firpo, Sergio},
	year = {2007},
	note = {Publisher: [Wiley, Econometric Society]},
	pages = {259--276},
	file = {Full Text:/Users/yuehaob/Zotero/storage/BR68LUY5/Firpo - 2007 - Efficient Semiparametric Estimation of Quantile Tr.pdf:application/pdf},
}

@article{van_der_vaart1994bracketing,
	title = {Bracketing smooth functions},
	volume = {52},
	issn = {0304-4149},
	url = {https://www.sciencedirect.com/science/article/pii/0304414994901023},
	doi = {10.1016/0304-4149(94)90102-3},
	abstract = {Let F be the class of functions f :Rd→R which are α times differentiable with derivatives bounded by numbers Mj on each given set Ij in a partition of Rd = ⋃jIj. We obtain upper and lower bounds on the Lr(P)-bracketing entropy of F.},
	language = {en},
	number = {1},
	urldate = {2023-05-02},
	journal = {Stochastic Processes and their Applications},
	author = {van der Vaart, Aad},
	month = aug,
	year = {1994},
	keywords = {Bracketing number, Covering number, Donsker class, Empirical central limit theorem, Entropy, Glivenko-Cantelli class},
	pages = {93--105},
	file = {1-s2.0-0304414994901023-main.pdf:/Users/yuehaob/Zotero/storage/M6MTHI68/1-s2.0-0304414994901023-main.pdf:application/pdf},
}

@book{krantz2013implicit,
	address = {New York, NY},
	title = {The {Implicit} {Function} {Theorem}: {History}, {Theory}, and {Applications}},
	isbn = {978-1-4614-5980-4 978-1-4614-5981-1},
	shorttitle = {The {Implicit} {Function} {Theorem}},
	url = {https://link.springer.com/10.1007/978-1-4614-5981-1},
	language = {en},
	urldate = {2023-05-18},
	publisher = {Springer},
	author = {Krantz, Steven G. and Parks, Harold R.},
	year = {2013},
	doi = {10.1007/978-1-4614-5981-1},
	keywords = {Implicit Function Theorem, Inverse Function Theorem, Numerical Homotopy Methods, ordinary differential equations, partial differential equations, Real Analysis, Smooth Functions},
}


@article{abadie2003semiparametric,
	title = {Semiparametric instrumental variable estimation of treatment response models},
	volume = {113},
	issn = {0304-4076},
	url = {https://www.sciencedirect.com/science/article/pii/S0304407602002014},
	doi = {10.1016/S0304-4076(02)00201-4},
	abstract = {This article introduces a new class of instrumental variable (IV) estimators for linear and nonlinear treatment response models with covariates. The rationale for focusing on nonlinear models is that, if the dependent variable is binary or limited, or if the effect of the treatment varies with covariates, a nonlinear model is appropriate. In the spirit of Roehrig (Econometrica 56 (1988) 433), identification is attained nonparametrically and does not depend on the choice of the parametric specification for the response function of interest. One virtue of this approach is that it allows the researcher to construct estimators that can be interpreted as the parameters of a well-defined approximation to a treatment response function under functional form misspecification. In contrast to some usual IV models, heterogeneity of treatment effects is not restricted by the identification conditions. The ideas and estimators in this article are illustrated using IV to estimate the effects of 401(k) retirement programs on savings.},
	language = {en},
	number = {2},
	urldate = {2021-02-13},
	journal = {Journal of Econometrics},
	author = {Abadie, Alberto},
	month = apr,
	year = {2003},
	pages = {231--263},
}

@misc{armstrong2022asymptotic,
	title = {Asymptotic {Efficiency} {Bounds} for a {Class} of {Experimental} {Designs}},
	url = {http://arxiv.org/abs/2205.02726},
	doi = {10.48550/arXiv.2205.02726},
	abstract = {We consider an experimental design setting in which units are assigned to treatment after being sampled sequentially from an infinite population. We derive asymptotic efficiency bounds that apply to data from any experiment that assigns treatment as a (possibly randomized) function of covariates and past outcome data, including stratification on covariates and adaptive designs. For estimating the average treatment effect of a binary treatment, our results show that no further first order asymptotic efficiency improvement is possible relative to an estimator that achieves the Hahn (1998) bound in an experimental design where the propensity score is chosen to minimize this bound. Our results also apply to settings with multiple treatments with possible constraints on treatment, as well as covariate based sampling of a single outcome.},
	urldate = {2023-01-27},
	publisher = {arXiv},
	author = {Armstrong, Timothy B.},
	month = may,
	year = {2022},
	note = {arXiv:2205.02726 [stat]},
	keywords = {Statistics - Methodology},
}


@misc{jiang2022regression-adjusted,
	title = {Regression-{Adjusted} {Estimation} of {Quantile} {Treatment} {Effects} under {Covariate}-{Adaptive} {Randomizations}},
	url = {http://arxiv.org/abs/2105.14752},
	doi = {10.48550/arXiv.2105.14752},
	abstract = {Datasets from field experiments with covariate-adaptive randomizations (CARs) usually contain extra covariates in addition to the strata indicators. We propose to incorporate these additional covariates via auxiliary regressions in the estimation and inference of unconditional quantile treatment effects (QTEs) under CARs. We establish the consistency and limit distribution of the regression-adjusted QTE estimator and prove that the use of multiplier bootstrap inference is non-conservative under CARs. The auxiliary regression may be estimated parametrically, nonparametrically, or via regularization when the data are high-dimensional. Even when the auxiliary regression is misspecified, the proposed bootstrap inferential procedure still achieves the nominal rejection probability in the limit under the null. When the auxiliary regression is correctly specified, the regression-adjusted estimator achieves the minimum asymptotic variance. We also discuss forms of adjustments that can improve the efficiency of the QTE estimators. The finite sample performance of the new estimation and inferential methods is studied in simulations and an empirical application to a well-known dataset concerned with expanding access to basic bank accounts on savings is reported.},
	urldate = {2023-06-13},
	publisher = {arXiv},
	author = {Jiang, Liang and Phillips, Peter C. B. and Tao, Yubo and Zhang, Yichong},
	month = sep,
	year = {2022},
	note = {arXiv:2105.14752 [econ, stat]},
	keywords = {Economics - Econometrics, Statistics - Methodology},
}

@misc{jiang2022improving,
	title = {Improving {Estimation} {Efficiency} via {Regression}-{Adjustment} in {Covariate}-{Adaptive} {Randomizations} with {Imperfect} {Compliance}},
	url = {http://arxiv.org/abs/2201.13004},
	doi = {10.48550/arXiv.2201.13004},
	abstract = {We study how to improve efficiency via regression adjustments with additional covariates under covariate-adaptive randomizations (CARs) when subject compliance is imperfect. We first establish the semiparametric efficiency bound for the local average treatment effect (LATE) under CARs. Second, we develop a general regression-adjusted LATE estimator which allows for parametric, nonparametric, and regularized adjustments. Even when the adjustments are misspecified, our proposed estimator is still consistent and asymptotically normal, and their inference method still achieves the exact asymptotic size under the null. When the adjustments are correctly specified, our estimator achieves the semiparametric efficiency bound. Third, we derive the optimal linear adjustment that leads to the smallest asymptotic variance among all linear adjustments. We then show the commonly used two stage least squares estimator is not optimal in the class of LATE estimators with linear adjustments while Ansel, Hong, and Li's (2018) estimator is. Fourth, we show how to construct a LATE estimator with nonlinear adjustments which is more efficient than those with the optimal linear adjustment. Fifth, we give conditions under which LATE estimators with nonparametric and regularized adjustments achieve the semiparametric efficiency bound. Last, simulation evidence and empirical application confirm efficiency gains achieved by regression adjustments relative to both the estimator without adjustment and the standard two-stage least squares estimator.},
	urldate = {2023-06-13},
	publisher = {arXiv},
	author = {Jiang, Liang and Linton, Oliver B. and Tang, Haihan and Zhang, Yichong},
	month = sep,
	year = {2022},
	note = {arXiv:2201.13004 [econ, stat]},
	keywords = {Economics - Econometrics, Statistics - Methodology},
}


@article{imbens1994identification,
	title = {Identification and {Estimation} of {Local} {Average} {Treatment} {Effects}},
	volume = {62},
	issn = {0012-9682},
	url = {https://www.jstor.org/stable/2951620},
	doi = {10.2307/2951620},
	number = {2},
	urldate = {2020-12-03},
	journal = {Econometrica},
	author = {Imbens, Guido W. and Angrist, Joshua D.},
	year = {1994},
	note = {Publisher: [Wiley, Econometric Society]},
	pages = {467--475},
}

@article{zhang2008improving,
  title={Improving efficiency of inferences in randomized clinical trials using auxiliary covariates},
  author={Zhang, Min and Tsiatis, Anastasios A and Davidian, Marie},
  journal={Biometrics},
  volume={64},
  number={3},
  pages={707--715},
  year={2008},
  publisher={Wiley Online Library}
}

@article{rafi2023efficient,
  title={Efficient Semiparametric Estimation of Average Treatment Effects Under Covariate Adaptive Randomization},
  author={Rafi, Ahnaf},
  journal={arXiv preprint arXiv:2305.08340},
  year={2023}
}

@article{jiang2021bootstrap,
  title={Bootstrap inference for quantile treatment effects in randomized experiments with matched pairs},
  author={Jiang, Liang and Liu, Xiaobin and Phillips, Peter CB and Zhang, Yichong},
  journal={Review of Economics and Statistics},
  pages={1--47},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{negi2021revisiting,
  title={Revisiting regression adjustment in experiments with heterogeneous treatment effects},
  author={Negi, Akanksha and Wooldridge, Jeffrey M},
  journal={Econometric Reviews},
  volume={40},
  number={5},
  pages={504--534},
  year={2021},
  publisher={Taylor \& Francis}
}


@article{lin2013agnostic,
	title = {Agnostic notes on regression adjustments to experimental data: {Reexamining} {Freedman}’s critique},
	volume = {7},
	issn = {1932-6157, 1941-7330},
	shorttitle = {Agnostic notes on regression adjustments to experimental data},
	url = {https://projecteuclid.org/euclid.aoas/1365527200},
	doi = {10.1214/12-AOAS583},
	abstract = {Freedman [Adv. in Appl. Math. 40 (2008) 180–193; Ann. Appl. Stat. 2 (2008) 176–196] critiqued ordinary least squares regression adjustment of estimated treatment effects in randomized experiments, using Neyman’s model for randomization inference. Contrary to conventional wisdom, he argued that adjustment can lead to worsened asymptotic precision, invalid measures of precision, and small-sample bias. This paper shows that in sufficiently large samples, those problems are either minor or easily fixed. OLS adjustment cannot hurt asymptotic precision when a full set of treatment–covariate interactions is included. Asymptotically valid confidence intervals can be constructed with the Huber–White sandwich standard error estimator. Checks on the asymptotic approximations are illustrated with data from Angrist, Lang, and Oreopoulos’s [Am. Econ. J.: Appl. Econ. 1:1 (2009) 136–163] evaluation of strategies to improve college students’ achievement. The strongest reasons to support Freedman’s preference for unadjusted estimates are transparency and the dangers of specification search.},
	language = {EN},
	number = {1},
	urldate = {2020-12-03},
	journal = {Annals of Applied Statistics},
	author = {Lin, Winston},
	month = mar,
	year = {2013},
	mrnumber = {MR3086420},
	zmnumber = {06171273},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {295--318},
}


@article{zhao2023covariate,
  title={Covariate adjustment in multiarmed, possibly factorial experiments},
  author={Zhao, Anqi and Ding, Peng},
  journal={Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume={85},
  number={1},
  pages={1--23},
  year={2023},
  publisher={Oxford University Press US}
}

@article{su2021model,
  title={Model-assisted analyses of cluster-randomized experiments},
  author={Su, Fangzhou and Ding, Peng},
  journal={arXiv preprint arXiv:2104.04647},
  year={2021}
}


@book{bickel1998efficient,
	title = {Efficient and {Adaptive} {Estimation} for {Semiparametric} {Models}},
	isbn = {978-0-387-98473-5},
	abstract = {This book deals with estimation in situations in which there is believed to be enough information to model parametrically some, but not all of the features of a data set. Such models have arisen in a wide context in recent years, and involve new nonlinear estimation procedures. Statistical models of this type are directly applicable to fields such as economics, epidemiology, and astronomy.},
	language = {en},
	publisher = {Springer New York},
	author = {Bickel, Peter J. and Klaassen, Chris A. J. and Ritov, Ya'acov and Wellner, Jon A.},
	month = jun,
	year = {1998},
	note = {Google-Books-ID: lSnTm6SC\_SMC},
	keywords = {Mathematics / Applied, Mathematics / General, Mathematics / Probability \& Statistics / General},
}


@article{chen2018overidentification,
	title = {Overidentification in {Regular} {Models}},
	volume = {86},
	issn = {0012-9682},
	url = {https://www.jstor.org/stable/44955258},
	abstract = {In the unconditional moment restriction model of Hansen (1982), specification tests and more efficient estimators are both available whenever the number of moment restrictions exceeds the number of parameters of interest. We show that a similar relationship between potential refutability of a model and existence of more efficient estimators is present in much broader settings. Specifically, a condition we name local overidentification is shown to be equivalent to both the existence of specification tests with nontrivial local power and the existence of more efficient estimators of some "smooth" parameters in general semi/nonparametric models. Under our notion of local overidentification, various locally nontrivial specification tests such as Hausman tests, incremental Sargan tests (or optimally weighted quasi likelihood ratio tests) naturally extend to general semi/nonparametric settings. We further obtain simple characterizations of local overidentification for general models of nonparametric conditional moment restrictions with possibly different conditioning sets. The results are applied to determining when semi/nonparametric models with endogeneity are locally testable, and when nonparametric plug-in and semiparametric two-step GMM estimators are semiparametrically efficient. Examples of empirically relevant semi/nonparametric structural models are presented.},
	number = {5},
	urldate = {2023-04-14},
	journal = {Econometrica},
	author = {Chen, Xiaohong and Santos, Andres},
	year = {2018},
	note = {Publisher: [Wiley, The Econometric Society]},
	pages = {1771--1817},
	file = {Full Text:/Users/yuehaob/Zotero/storage/DPAMSKYZ/Econometrica - 2018 - Chen - Overidentification in Regular Models.pdf:application/pdf;Supplement:/Users/yuehaob/Zotero/storage/B7PEEK48/overid_appendix.pdf:application/pdf},
}


@article{cytrynbaum2023designing,
  title={Designing representative and balanced experiments by local randomization},
  author={Cytrynbaum, Max},
  journal={arXiv preprint arXiv:2111.08157},
  year={2023}
}


@article{abadie2008estimation,
	title = {Estimation of the {Conditional} {Variance} in {Paired} {Experiments}},
    volume = {1},
	issn = {0769-489X},
	doi = {10.2307/27917244},
	abstract = {In paired randomized experiments units are grouped in pairs, often based on covariate information, with random assignment within the pairs. Average treatment effects are then estimated by averaging the within-pair differences in outcomes. Typically the variance of the average treatment effect estimator is estimated using the sample variance of the within-pair differences. However, conditional on the covariates the variance of the average treatment effect estimator may be substantially smaller. Here we propose a simple way of estimating the conditional variance of the average treatment effect estimator by forming pairs-of-pairs with similar covariate values and estimating the variances within these pairs-of-pairs. Even though these within-pairs-of-pairs variance estimators are not consistent, their average is consistent for the conditional variance of the average treatment effect estimator and leads to asymptotically valid confidence intervals. Dans les expériences aléatoires d'appariement les unités sont regroupées par paires, souvent basées sur des caractéristiques explicatives, et avec appariement aléatoire. Les effets de traitement moyens sont alors estimés en faisant la moyenne des différences intra-paires dans les résultats. Typiquement, la variance de l'estimateur de l'effet de traitement moyen est estimée en utilisant la variance des différences intra-paires dans l'échantillon. Cependant, conditionnellement aux variables explicatives, l'estimateur de l'effet de traitement moyen peut être substantiellement plus petit. Nous proposons ici une manière simple d'estimer la variance conditionnelle de l'estimateur de l'effet de traitement moyen en formant des paires de paires avec des valeurs de variables explicatives similaires et en estimant les variances entre ces paires de paires. Même si ces estimateurs fondés sur les paires de paires ne sont pas convergents, leur moyenne est convergente pour la variance conditionnelle de l'estimateur de l'effet de traitement moyen et conduit à des intervalles de confiance asymptotiquement valides.},
	number = {91/92},
	urldate = {2020-06-15},
	journal = {Annales d'Économie et de Statistique},
	author = {Abadie, Alberto and Imbens, Guido W.},
	year = {2008},
	pages = {175--187},
}


@article{chen2008semiparametric,
	title = {Semiparametric {Efficiency} in {GMM} {Models} with {Auxiliary} {Data}},
	volume = {36},
	issn = {0090-5364},
	url = {https://www.jstor.org/stable/25464647},
	abstract = {We study semiparametric efficiency bounds and efficient estimation of parameters defined through general moment restrictions with missing data. Identification relies on auxiliary data containing information about the distribution of the missing variables conditional on proxy variables that are observed in both the primary and the auxiliary database, when such distribution is common to the two data sets. The auxiliary sample can be independent of the primary sample, or can be a subset of it. For both cases, we derive bounds when the probability of missing data given the proxy variables is unknown, or known, or belongs to a correctly specified parametric family. We find that the conditional probability is not ancillary when the two samples are independent. For all cases, we discuss efficient semiparametric estimators. An estimator based on a conditional expectation projection is shown to require milder regularity conditions than one based on inverse probability weighting.},
	number = {2},
	urldate = {2023-06-29},
	journal = {The Annals of Statistics},
	author = {Chen, Xiaohong and Hong, Han and Tarozzi, Alessandro},
	year = {2008},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {808--843},
}


@article{tabord-meehan2022stratification,
	title = {Stratification {Trees} for {Adaptive} {Randomization} in {Randomized} {Controlled} {Trials}},
	urldate = {2022-07-28},
	journal = {The Review of Economic Studies},
	author = {Tabord-Meehan, Max},
	year = {2022},
	note = {Forthcoming},
}

@article{dizon2019parents,
  title={Parents' beliefs about their children's academic ability: Implications for educational investments},
  author={Dizon-Ross, Rebecca},
  journal={American Economic Review},
  volume={109},
  number={8},
  pages={2728--65},
  year={2019}
}

@article{duflo2015education,
  title={Education, HIV, and early fertility: Experimental evidence from Kenya},
  author={Duflo, Esther and Dupas, Pascaline and Kremer, Michael},
  journal={American Economic Review},
  volume={105},
  number={9},
  pages={2757--2797},
  year={2015},
  publisher={American Economic Association}
}


@article{angrist2009effects,
	title = {The {Effects} of {High} {Stakes} {High} {School} {Achievement} {Awards}: {Evidence} from a {Randomized} {Trial}},
	volume = {99},
	issn = {0002-8282},
	shorttitle = {The {Effects} of {High} {Stakes} {High} {School} {Achievement} {Awards}},
	url = {https://www.aeaweb.org/articles?id=10.1257/aer.99.4.1384},
	doi = {10.1257/aer.99.4.1384},
	abstract = {The Israeli matriculation certificate is a prerequisite for most postsecondary
schooling. In a randomized trial, we attempted to increase certification rates
among low-achievers with cash incentives. The experiment used a school-based
randomization design offering awards to all who passed their exams in treated
schools. This led to a substantial increase in certification rates for girls but had
no effect on boys. Affected girls had a relatively high ex ante chance of certification.
The increase in girls' matriculation rates translated into an increased
likelihood of college attendance. Female matriculation rates increased partly
because treated girls devoted extra time to exam preparation. (JEL I21, I28,
J16)},
	language = {en},
	number = {4},
	urldate = {2020-12-03},
	journal = {American Economic Review},
	author = {Angrist, Joshua and Lavy, Victor},
	month = sep,
	year = {2009},
	pages = {1384--1414},
}

@article{banerjee2015miracle,
	title = {The {Miracle} of {Microfinance}? {Evidence} from a {Randomized} {Evaluation}},
	volume = {7},
	issn = {1945-7782},
	shorttitle = {The {Miracle} of {Microfinance}?},
	url = {https://www.aeaweb.org/articles?id=10.1257/app.20130533},
	doi = {10.1257/app.20130533},
	abstract = {This paper reports results from the randomized evaluation of a group-lending microcredit program in Hyderabad, India. A lender worked in 52 randomly selected neighborhoods, leading to an 8.4 percentage point increase in takeup of microcredit. Small business investment and profits of preexisting businesses increased, but consumption did not significantly increase. Durable goods expenditure increased, while "temptation goods" expenditure declined. We found no significant changes in health, education, or women's empowerment. Two years later, after control areas had gained access to microcredit but households in treatment area had borrowed for longer and in larger amounts, very few significant differences persist. (JEL G21, G31, O16, O12, L25, I38)},
	language = {en},
	number = {1},
	urldate = {2020-12-03},
	journal = {American Economic Journal: Applied Economics},
	author = {Banerjee, Abhijit and Duflo, Esther and Glennerster, Rachel and Kinnan, Cynthia},
	month = jan,
	year = {2015},
	pages = {22--53},
}

@article{bruhn2016impact,
	title = {The {Impact} of {High} {School} {Financial} {Education}: {Evidence} from a {Large}-{Scale} {Evaluation} in {Brazil}},
	volume = {8},
	issn = {1945-7782},
	shorttitle = {The {Impact} of {High} {School} {Financial} {Education}},
	url = {https://www.aeaweb.org/articles?id=10.1257/app.20150149},
	doi = {10.1257/app.20150149},
	abstract = {We study the impact of a comprehensive high school financial education program spanning 6 states, 892 schools, and approximately 25,000 students in Brazil through a randomized control trial. The program increased student financial proficiency by a quarter of a standard deviation and raised grade-level passing rates. Short-term financial behaviors, however, show mixed results with significant improvements in students' savings and budgeting as well as positive spillovers to parents, but also an increase in students' use of expensive credit to make consumer purchases.},
	language = {en},
	number = {4},
	urldate = {2020-12-03},
	journal = {American Economic Journal: Applied Economics},
	author = {Bruhn, Miriam and Leão, Luciana de Souza and Legovini, Arianna and Marchetti, Rogelio and Zia, Bilal},
	month = oct,
	year = {2016},
	keywords = {Corporate Finance and Governance, Household Saving, Personal Finance, Analysis of Education, Microeconomic Analyses of Economic Development, Economic Development: Financial Markets, Saving and Capital Investment},
	pages = {256--295},
}


@article{chernozhukov2018doubledebiased,
	title = {Double/debiased machine learning for treatment and structural parameters},
	volume = {21},
	copyright = {© 2017 Royal Economic Society.},
	issn = {1368-423X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ectj.12097},
	doi = {10.1111/ectj.12097},
	abstract = {We revisit the classic semi-parametric problem of inference on a low-dimensional parameter θ0 in the presence of high-dimensional nuisance parameters η0. We depart from the classical setting by allowing for η0 to be so high-dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate η0, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high-dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating η0 cause a heavy bias in estimators of θ0 that are obtained by naively plugging ML estimators of η0 into estimating equations for θ0. This bias results in the naive estimator failing to be consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest θ0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman-orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate θ0; (2) making use of cross-fitting, which provides an efficient form of data-splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an -neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.},
	language = {en},
	number = {1},
	urldate = {2023-07-03},
	journal = {The Econometrics Journal},
	author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ectj.12097},
	pages = {C1--C68},
}


@article{casaburi2022using,
	title = {Using {Individual}-{Level} {Randomized} {Treatment} to {Learn} about {Market} {Structure}},
	volume = {14},
	issn = {1945-7782},
	url = {https://www.aeaweb.org/articles?id=10.1257/app.20200306},
	doi = {10.1257/app.20200306},
	abstract = {Interference across competing firms in RCTs can be informative about market structure. An experiment that subsidizes a random subset of traders who buy cocoa from farmers in Sierra Leone illustrates this idea. Interpreting treatment-control differences in prices and quantities purchased from farmers through a model of Cournot competition reveals differentiation between traders is low. Combining this result with quasi-experimental variation in world prices shows that the number of traders competing is 50 percent higher than the number operating in a village. Own-price and cross-price supply elasticities are high. Farmers face a competitive market in this first stage of the value chain.},
	language = {en},
	number = {4},
	urldate = {2023-07-03},
	journal = {American Economic Journal: Applied Economics},
	author = {Casaburi, Lorenzo and Reed, Tristan},
	month = oct,
	year = {2022},
	keywords = {Agribusiness, Contracts and Reputation, Cooperatives, Energy, Environment, Natural Resources, Networks, Economic Development: Agriculture, Oligopoly and Other Imperfect Markets, Transactional Relationships, Other Primary Products, Micro Analysis of Farm Firms, Farm Households, and Farm Input Markets, Agricultural Markets and Marketing},
	pages = {58--90},
}


@article{farrell2015robust,
  title={Robust inference on average treatment effects with possibly more covariates than observations},
  author={Farrell, Max H},
  journal={Journal of Econometrics},
  volume={189},
  number={1},
  pages={1--23},
  year={2015},
  publisher={Elsevier}
}


@article{belloni2017program,
  title={Program evaluation and causal inference with high-dimensional data},
  author={Belloni, Alexandre and Chernozhukov, Victor and Fernandez-Val, Ivan and Hansen, Christian},
  journal={Econometrica},
  volume={85},
  number={1},
  pages={233--298},
  year={2017},
  publisher={Wiley Online Library}
}

@article{bruhn2009pursuit,
  title={In pursuit of balance: Randomization in practice in development field experiments},
  author={Bruhn, Miriam and McKenzie, David},
  journal={American economic journal: applied economics},
  volume={1},
  number={4},
  pages={200--232},
  year={2009},
  publisher={American Economic Association}
}


@misc{cytrynbaum2023covariate,
	title = {Covariate {Adjustment} in {Stratified} {Experiments}},
	url = {http://arxiv.org/abs/2302.03687},
	doi = {10.48550/arXiv.2302.03687},
	abstract = {This paper studies covariate adjusted estimation of the average treatment effect in stratified experiments. We work in a general framework that includes matched tuples designs, coarse stratification, and complete randomization as special cases. Regression adjustment with treatment-covariate interactions is known to weakly improve efficiency for completely randomized designs. By contrast, we show that for stratified designs such regression estimators are generically inefficient, potentially even increasing estimator variance relative to the unadjusted benchmark. Motivated by this result, we derive the asymptotically optimal linear covariate adjustment for a given stratification. We construct several feasible estimators that implement this efficient adjustment in large samples. In the special case of matched pairs, for example, the regression including treatment, covariates, and pair fixed effects is asymptotically optimal. We also provide novel asymptotically exact inference methods that allow researchers to report smaller confidence intervals, fully reflecting the efficiency gains from both stratification and adjustment. Simulations and an empirical application demonstrate the value of our proposed methods.},
	urldate = {2024-02-08},
	publisher = {arXiv},
	author = {Cytrynbaum, Max},
	month = sep,
	year = {2023},
	note = {arXiv:2302.03687 [econ, stat]},
}

@article{neyman1934two,
  title={On the Two Different Aspects of the Representative Method: The Method of Stratified Sampling and the Method of Purposive Selection},
  author={Neyman, Jerzy},
  journal={Journal of the Royal Statistical Society Series A: Statistics in Society},
  volume={97},
  number={4},
  pages={558--606},
  year={1934},
  publisher={Oxford University Press}
}

@article{heckman1998matching,
  title={Matching as an econometric evaluation estimator},
  author={Heckman, James J and Ichimura, Hidehiko and Todd, Petra},
  journal={The review of economic studies},
  volume={65},
  number={2},
  pages={261--294},
  year={1998},
  publisher={Wiley-Blackwell}
}

@article{freedman2008regression,
  title={On regression adjustments to experimental data},
  author={Freedman, David A},
  journal={Advances in Applied Mathematics},
  volume={40},
  number={2},
  pages={180--193},
  year={2008},
  publisher={Elsevier}
}

@misc{chernozhukov2024long,
	title = {Long {Story} {Short}: {Omitted} {Variable} {Bias} in {Causal} {Machine} {Learning}},
	shorttitle = {Long {Story} {Short}},
	url = {http://arxiv.org/abs/2112.13398},
	doi = {10.48550/arXiv.2112.13398},
	abstract = {We develop a general theory of omitted variable bias for a wide range of common causal parameters, including (but not limited to) averages of potential outcomes, average treatment effects, average causal derivatives, and policy effects from covariate shifts. Our theory applies to nonparametric models, while naturally allowing for (semi-)parametric restrictions (such as partial linearity) when such assumptions are made. We show how simple plausibility judgments on the maximum explanatory power of omitted variables are sufficient to bound the magnitude of the bias, thus facilitating sensitivity analysis in otherwise complex, nonlinear models. Finally, we provide flexible and efficient statistical inference methods for the bounds, which can leverage modern machine learning algorithms for estimation. These results allow empirical researchers to perform sensitivity analyses in a flexible class of machine-learned causal models using very simple, and interpretable, tools. We demonstrate the utility of our approach with two empirical examples.},
	urldate = {2024-08-14},
	publisher = {arXiv},
	author = {Chernozhukov, Victor and Cinelli, Carlos and Newey, Whitney and Sharma, Amit and Syrgkanis, Vasilis},
	month = may,
	year = {2024},
	note = {arXiv:2112.13398 [cs, econ, stat]},
	keywords = {62G, Computer Science - Machine Learning, Economics - Econometrics, Statistics - Machine Learning, Statistics - Methodology},
}


@article{bai2024inference,
	title = {Inference for {Matched} {Tuples} and {Fully} {Blocked} {Factorial} {Designs}},
	volume = {15},
	abstract = {This paper studies inference in randomized controlled trials with multiple treatments, where treatment status is determined according to a "matched tuples" design. Here, by a matched tuples design, we mean an experimental design where units are sampled i.i.d. from the population of interest, grouped into "homogeneous" blocks with cardinality equal to the number of treatments, and finally, within each block, each treatment is assigned exactly once uniformly at random. We first study estimation and inference for matched tuples designs in the general setting where the parameter of interest is a vector of linear contrasts over the collection of average potential outcomes for each treatment. Parameters of this form include standard average treatment effects used to compare one treatment relative to another, but also include parameters which may be of interest in the analysis of factorial designs. We first establish conditions under which a sample analogue estimator is asymptotically normal and construct a consistent estimator of its corresponding asymptotic variance. Combining these results establishes the asymptotic exactness of tests based on these estimators. In contrast, we show that, for two common testing procedures based on t-tests constructed from linear regressions, one test is generally conservative while the other generally invalid. We go on to apply our results to study the asymptotic properties of what we call "fully-blocked" 2{\textasciicircum}K factorial designs, which are simply matched tuples designs applied to a full factorial experiment. Leveraging our previous results, we establish that our estimator achieves a lower asymptotic variance under the fully-blocked design than that under any stratified factorial design which stratifies the experimental sample into a finite number of "large" strata. A simulation study and empirical application illustrate the practical relevance of our results.},
	number = {2},
	journal = {Quantitative Economics},
	author = {Bai, Yuehao and Liu, Jizhou and Tabord-Meehan, Max},
	year = {2024},
	keywords = {Economics - Econometrics, Mathematics - Statistics Theory, Statistics - Methodology},
	pages = {279--330},
}

@article{bai2024covariate,
	title = {Covariate adjustment in experiments with matched pairs},
	volume = {241},
	issn = {0304-4076},
	url = {https://www.sciencedirect.com/science/article/pii/S0304407624000861},
	doi = {10.1016/j.jeconom.2024.105740},
	abstract = {This paper studies inference for the average treatment effect (ATE) in experiments in which treatment status is determined according to “matched pairs” and it is additionally desired to adjust for observed, baseline covariates to gain further precision. By a “matched pairs” design, we mean that units are sampled i.i.d. from the population of interest, paired according to observed, baseline covariates, and finally, within each pair, one unit is selected at random for treatment. Importantly, we presume that not all observed, baseline covariates are used in determining treatment assignment. We study a broad class of estimators based on a “doubly robust” moment condition that permits us to study estimators with both finite-dimensional and high-dimensional forms of covariate adjustment. We find that estimators with finite-dimensional, linear adjustments need not lead to improvements in precision relative to the unadjusted difference-in-means estimator. This phenomenon persists even if the adjustments interact with treatment; in fact, doing so leads to no changes in precision. However, gains in precision can be ensured by including fixed effects for each of the pairs. Indeed, we show that this adjustment leads to the minimum asymptotic variance of the corresponding ATE estimator among all finite-dimensional, linear adjustments. We additionally study an estimator with a regularized adjustment, which can accommodate high-dimensional covariates. We show that this estimator leads to improvements in precision relative to the unadjusted difference-in-means estimator and also provides conditions under which it leads to the “optimal” nonparametric, covariate adjustment. A simulation study confirms the practical relevance of our theoretical analysis, and the methods are employed to reanalyze data from an experiment using a “matched pairs” design to study the effect of macroinsurance on microenterprise.},
	number = {1},
	urldate = {2024-05-15},
	journal = {Journal of Econometrics},
	author = {Bai, Yuehao and Jiang, Liang and Romano, Joseph P. and Shaikh, Azeem M. and Zhang, Yichong},
	month = apr,
	year = {2024},
	keywords = {Economics - Econometrics, Randomized controlled trial, Experiment, Covariate adjustment, LASSO, Matched pairs, Treatment assignment},
	pages = {105740},
}

@misc{bai2024inference-1,
	title = {Inference in {Cluster} {Randomized} {Trials} with {Matched} {Pairs}},
	url = {http://arxiv.org/abs/2211.14903},
	doi = {10.48550/arXiv.2211.14903},
	abstract = {This paper studies inference in cluster randomized trials where treatment status is determined according to a "matched pairs" design. Here, by a cluster randomized experiment, we mean one in which treatment is assigned at the level of the cluster; by a "matched pairs" design, we mean that a sample of clusters is paired according to baseline, cluster-level covariates and, within each pair, one cluster is selected at random for treatment. We study the large-sample behavior of a weighted difference-in-means estimator and derive two distinct sets of results depending on if the matching procedure does or does not match on cluster size. We then propose a single variance estimator which is consistent in either regime. Combining these results establishes the asymptotic exactness of tests based on these estimators. Next, we consider the properties of two common testing procedures based on t-tests constructed from linear regressions, and argue that both are generally conservative in our framework. We additionally study the behavior of a randomization test which permutes the treatment status for clusters within pairs, and establish its finite-sample and asymptotic validity for testing specific null hypotheses. Finally, we propose a covariate-adjusted estimator which adjusts for additional baseline covariates not used for treatment assignment, and establish conditions under which such an estimator leads to strict improvements in precision. A simulation study confirms the practical relevance of our theoretical results.},
	urldate = {2024-08-19},
	publisher = {arXiv},
	author = {Bai, Yuehao and Liu, Jizhou and Shaikh, Azeem M. and Tabord-Meehan, Max},
	month = aug,
	year = {2024},
	note = {arXiv:2211.14903 [econ, stat]},
	keywords = {Economics - Econometrics, Statistics - Methodology},
}

@misc{bai2024inference-2,
	title = {Inference in {Experiments} with {Matched} {Pairs} and {Imperfect} {Compliance}},
	url = {http://arxiv.org/abs/2307.13094},
	doi = {10.48550/arXiv.2307.13094},
	abstract = {This paper studies inference for the local average treatment effect in randomized controlled trials with imperfect compliance where treatment status is determined according to "matched pairs." By "matched pairs," we mean that units are sampled i.i.d. from the population of interest, paired according to observed, baseline covariates and finally, within each pair, one unit is selected at random for treatment. Under weak assumptions governing the quality of the pairings, we first derive the limit distribution of the usual Wald (i.e., two-stage least squares) estimator of the local average treatment effect. We show further that conventional heteroskedasticity-robust estimators of the Wald estimator's limiting variance are generally conservative, in that their probability limits are (typically strictly) larger than the limiting variance. We therefore provide an alternative estimator of the limiting variance that is consistent. Finally, we consider the use of additional observed, baseline covariates not used in pairing units to increase the precision with which we can estimate the local average treatment effect. To this end, we derive the limiting behavior of a two-stage least squares estimator of the local average treatment effect which includes both the additional covariates in addition to pair fixed effects, and show that its limiting variance is always less than or equal to that of the Wald estimator. To complete our analysis, we provide a consistent estimator of this limiting variance. A simulation study confirms the practical relevance of our theoretical results. Finally, we apply our results to revisit a prominent experiment studying the effect of macroinsurance on microenterprise in Egypt.},
	urldate = {2024-08-19},
	publisher = {arXiv},
	author = {Bai, Yuehao and Guo, Hongchang and Shaikh, Azeem M. and Tabord-Meehan, Max},
	month = jun,
	year = {2024},
	note = {arXiv:2307.13094 [econ, math, stat]},
	keywords = {Economics - Econometrics, Mathematics - Statistics Theory},
}
