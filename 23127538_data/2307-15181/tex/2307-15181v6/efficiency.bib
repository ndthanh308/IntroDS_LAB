@article{hanneke2021universal,
	title = {Universal {Bayes} consistency in metric spaces},
	volume = {49},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-49/issue-4/Universal-Bayes-consistency-in-metric-spaces/10.1214/20-AOS2029.full},
	doi = {10.1214/20-AOS2029},
	abstract = {We extend a recently proposed 1-nearest-neighbor based multiclass learning algorithm and prove that our modification is universally strongly Bayes consistent in all metric spaces admitting any such learner, making it an “optimistically universal” Bayes-consistent learner. This is the first learning algorithm known to enjoy this property; by comparison, the k-NN classifier and its variants are not generally universally Bayes consistent, except under additional structural assumptions, such as an inner product, a norm, finite dimension or a Besicovitch-type property. The metric spaces in which universal Bayes consistency is possible are the “essentially separable” ones—a notion that we define, which is more general than standard separability. The existence of metric spaces that are not essentially separable is widely believed to be independent of the ZFC axioms of set theory. We prove that essential separability exactly characterizes the existence of a universal Bayes-consistent learner for the given metric space. In particular, this yields the first impossibility result for universal Bayes consistency. Taken together, our results completely characterize strong and weak universal Bayes consistency in metric spaces.},
	number = {4},
	urldate = {2025-02-28},
	journal = {The Annals of Statistics},
	author = {Hanneke, Steve and Kontorovich, Aryeh and Sabato, Sivan and Weiss, Roi},
	month = aug,
	year = {2021},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {03E17, 03E55, 54E70, 62C12, 97K80, Bayes consistency, ‎classification‎, metric space, nearest neighbor},
	pages = {2129--2150},
}

@book{hu2006theory,
  title={The theory of response-adaptive randomization in clinical trials},
  author={Hu, Feifang and Rosenberger, William F},
  year={2006},
  publisher={John Wiley \& Sons}
}

@article{liu2023lasso-adjusted,
	title = {Lasso-adjusted treatment effect estimation under covariate-adaptive randomization},
	volume = {110},
	issn = {1464-3510},
	url = {https://doi.org/10.1093/biomet/asac036},
	doi = {10.1093/biomet/asac036},
	abstract = {We consider the problem of estimating and inferring treatment effects in randomized experiments. In practice, stratified randomization, or more generally, covariate-adaptive randomization, is routinely used in the design stage to balance treatment allocations with respect to a few variables that are most relevant to the outcomes. Then, regression is performed in the analysis stage to adjust the remaining imbalances to yield more efficient treatment effect estimators. Building upon and unifying recent results obtained for ordinary-least-squares adjusted estimators under covariate-adaptive randomization, this paper presents a general theory of regression adjustment that allows for model mis-specification and the presence of a large number of baseline covariates. We exemplify the theory on two lasso-adjusted treatment effect estimators, both of which are optimal in their respective classes. In addition, nonparametric consistent variance estimators are proposed to facilitate valid inferences, which work irrespective of the specific randomization methods used. The robustness and improved efficiency of the proposed estimators are demonstrated through numerical studies.},
	number = {2},
	urldate = {2025-02-19},
	journal = {Biometrika},
	author = {Liu, Hanzhong and Tu, Fuyi and Ma, Wei},
	month = jun,
	year = {2023},
	pages = {431--447},
}

@article{liu2023impacts,
	title = {The impacts of unobserved covariates on covariate-adaptive randomized experiments},
	volume = {51},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-51/issue-5/The-impacts-of-unobserved-covariates-on-covariate-adaptive-randomized-experiments/10.1214/23-AOS2308.full},
	doi = {10.1214/23-AOS2308},
	abstract = {Covariate-adaptive randomization (CAR) is commonly implemented in clinical trials to balance observed covariates. Recent studies have demonstrated the advantages of CAR procedures in balancing covariates and improving the subsequent statistical analysis. Covariate balance is crucial, but it is not a panacea for the valid statistical inferences. If the response to a treatment interacts with some unobserved covariates, the conclusion drawn from a CAR experiment may be affected, and thus, be inconsistent with other evidence. This paper aims to demonstrate the relationships between unobserved covariates and the analysis of treatment and covariate effects in CAR experiments. We first derive the asymptotic properties of the statistical methods based on a linear model framework with interactions between the treatment and an unobserved covariate. We also provide sufficient conditions for the identifiability of the treatment and covariate effects. Our results theoretically explain how inconsistent estimations are generated in CAR experiments when some important covariates are unobserved. Under these sufficient conditions, we show that the tests for the treatment and covariate effects can have reduced Type I errors under CAR procedures. A residual-based adjusted test is proposed to recover the Type I error when the effect can be correctly estimated. Numerical studies are conducted to evaluate the performance of our proposed procedure and theoretical findings.},
	number = {5},
	urldate = {2025-02-19},
	journal = {The Annals of Statistics},
	author = {Liu, Yang and Hu, Feifang},
	month = oct,
	year = {2023},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {60G42, 62F03, 62G99, 62L05, CAR procedures, conservativeness of test, covariate effect, Hypothesis tests, Identifiability, inconsistency, treatment effect},
	pages = {1895--1920},
}


@article{fogarty2018mitigating,
	title = {On mitigating the analytical limitations of finely stratified experiments},
	volume = {80},
	copyright = {© 2018 Royal Statistical Society},
	issn = {1467-9868},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12290},
	doi = {https://doi.org/10.1111/rssb.12290},
	abstract = {Although attractive from a theoretical perspective, finely stratified experiments such as paired designs suffer from certain analytical limitations that are not present in block-randomized experiments with multiple treated and control individuals in each block. In short, when using a weighted difference in means to estimate the sample average treatment effect, the traditional variance estimator in a paired experiment is conservative unless the pairwise average treatment effects are constant across pairs; however, in more coarsely stratified experiments, the corresponding variance estimator is unbiased if treatment effects are constant within blocks, even if they vary across blocks. Using insights from classical least squares theory, we present an improved variance estimator that is appropriate in finely stratified experiments. The variance estimator remains conservative in expectation but is asymptotically no more conservative than the classical estimator and can be considerably less conservative. The magnitude of the improvement depends on the extent to which effect heterogeneity can be explained by observed covariates. Aided by this estimator, a new test for the null hypothesis of a constant treatment effect is proposed. These findings extend to some, but not all, superpopulation models, depending on whether the covariates are viewed as fixed across samples.},
	language = {en},
	number = {5},
	urldate = {2020-12-03},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Fogarty, Colin B.},
	year = {2018},
	note = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12290},
	pages = {1035--1056},
	file = {Main Text:/Users/yuehaob/Zotero/storage/N23NWHPL/Journal of the Royal Statistical Society  Series B  Statistical Methodology - 2018 - Fogarty - On mitigating the analytical.pdf:application/pdf},
}

@article{fogarty2018regression-assisted,
	title = {Regression-assisted inference for the average treatment effect in paired experiments},
	volume = {105},
	issn = {0006-3444},
	url = {https://academic.oup.com/biomet/article/105/4/994/5047363},
	doi = {10.1093/biomet/asy034},
	abstract = {SUMMARY. In paired randomized experiments, individuals in a given matched pair may differ on prognostically important covariates despite the best efforts of pra},
	language = {en},
	number = {4},
	urldate = {2020-12-03},
	journal = {Biometrika},
	author = {Fogarty, Colin B.},
	month = dec,
	year = {2018},
	note = {Publisher: Oxford Academic},
	pages = {994--1000},
}


@article{liu2022balancing,
	title = {Balancing {Unobserved} {Covariates} {With} {Covariate}-{Adaptive} {Randomized} {Experiments}},
	volume = {117},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2020.1825450},
	doi = {10.1080/01621459.2020.1825450},
	abstract = {Balancing important covariates is often critical in clinical trials and causal inference. Stratified permuted block (STR-PB) and covariate-adaptive randomization (CAR) procedures are widely used to balance observed covariates in practice. The balance properties of these procedures with respect to the observed covariates have been well studied. However, it has been questioned whether these methods will also yield a good balance for the unobserved covariates. In this article, we develop a general framework for the analysis of the unobserved covariates imbalance. These results are applicable to develop and compare the balance properties of complete randomization (CR), STR-PB, and CAR procedures with respect to the unobserved covariates. To quantify the improvement obtained by using STR-PB and CAR procedures rather than CR, we introduce the percentage reduction in variance of the unobserved covariates imbalance and compare these quantities. Our results demonstrate the benefits of using CAR or STR-PB (when the number of strata is small relative to the sample size) in terms of balancing unobserved covariates. These results also pave the way for future research into the effect of unobserved covariates in covariate-adaptive randomized experiments in clinical trials, as well as many other applications. Supplementary materials for this article are available online.},
	number = {538},
	urldate = {2025-02-19},
	journal = {Journal of the American Statistical Association},
	author = {Liu, Yang and Hu, Feifang},
	month = apr,
	year = {2022},
	note = {Publisher: ASA Website
\_eprint: https://doi.org/10.1080/01621459.2020.1825450},
	keywords = {Balancing covariates, Correlation, Hu and Hu’s procedure, Pocock and Simon’s procedure, Stratified permuted block design, Unobserved covariates imbalance},
	pages = {875--886},
}

@article{imai2008variance,
	title = {Variance identification and efficiency analysis in randomized experiments under the matched-pair design},
	volume = {27},
	number = {24},
	journal = {Statistics in medicine},
	author = {Imai, Kosuke},
	year = {2008},
	note = {Publisher: Wiley Online Library},
	pages = {4857--4873},
}

@article{imai2009essential,
	title = {The {Essential} {Role} of {Pair} {Matching} in {Cluster}-{Randomized} {Experiments}, with {Application} to the {Mexican} {Universal} {Health} {Insurance} {Evaluation}},
	volume = {24},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid.org/euclid.ss/1255009008},
	doi = {10.1214/08-STS274},
	abstract = {A basic feature of many field experiments is that investigators are only able to randomize clusters of individuals—such as households, communities, firms, medical practices, schools or classrooms—even when the individual is the unit of interest. To recoup the resulting efficiency loss, some studies pair similar clusters and randomize treatment within pairs. However, many other studies avoid pairing, in part because of claims in the literature, echoed by clinical trials standards organizations, that this matched-pair, cluster-randomization design has serious problems. We argue that all such claims are unfounded. We also prove that the estimator recommended for this design in the literature is unbiased only in situations when matching is unnecessary; its standard error is also invalid. To overcome this problem without modeling assumptions, we develop a simple design-based estimator with much improved statistical properties. We also propose a model-based approach that includes some of the benefits of our design-based estimator as well as the estimator in the literature. Our methods also address individual-level noncompliance, which is common in applications but not allowed for in most existing methods. We show that from the perspective of bias, efficiency, power, robustness or research costs, and in large or small samples, pairing should be used in cluster-randomized experiments whenever feasible; failing to do so is equivalent to discarding a considerable fraction of one’s data. We develop these techniques in the context of a randomized evaluation we are conducting of the Mexican Universal Health Insurance Program.},
	language = {EN},
	number = {1},
	urldate = {2020-12-03},
	journal = {Statistical Science},
	author = {Imai, Kosuke and King, Gary and Nall, Clayton},
	month = feb,
	year = {2009},
	mrnumber = {MR2561126},
	zmnumber = {1327.62061},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Causal inference, community intervention trials, field experiments, group-randomized trials, health policy, matched-pair design, noncompliance, place-randomized trials, power},
	pages = {29--53},
}

@article{zelen1974randomization,
	title = {The randomization and stratification of patients to clinical trials},
	volume = {27},
	issn = {0021-9681},
	url = {https://www.sciencedirect.com/science/article/pii/0021968174900150},
	doi = {10.1016/0021-9681(74)90015-0},
	number = {7},
	urldate = {2025-02-19},
	journal = {Journal of Chronic Diseases},
	author = {Zelen, M.},
	month = sep,
	year = {1974},
	pages = {365--375},
}


@article{li2017general,
	title = {General {Forms} of {Finite} {Population} {Central} {Limit} {Theorems} with {Applications} to {Causal} {Inference}},
	volume = {112},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2017.1295865},
	doi = {10.1080/01621459.2017.1295865},
	abstract = {Frequentists’ inference often delivers point estimators associated with confidence intervals or sets for parameters of interest. Constructing the confidence intervals or sets requires understanding the sampling distributions of the point estimators, which, in many but not all cases, are related to asymptotic Normal distributions ensured by central limit theorems. Although previous literature has established various forms of central limit theorems for statistical inference in super population models, we still need general and convenient forms of central limit theorems for some randomization-based causal analyses of experimental data, where the parameters of interests are functions of a finite population and randomness comes solely from the treatment assignment. We use central limit theorems for sample surveys and rank statistics to establish general forms of the finite population central limit theorems that are particularly useful for proving asymptotic distributions of randomization tests under the sharp null hypothesis of zero individual causal effects, and for obtaining the asymptotic repeated sampling distributions of the causal effect estimators. The new central limit theorems hold for general experimental designs with multiple treatment levels, multiple treatment factors and vector outcomes, and are immediately applicable for studying the asymptotic properties of many methods in causal inference, including instrumental variable, regression adjustment, rerandomization, cluster-randomized experiments, and so on. Previously, the asymptotic properties of these problems are often based on heuristic arguments, which in fact rely on general forms of finite population central limit theorems that have not been established before. Our new theorems fill this gap by providing more solid theoretical foundation for asymptotic randomization-based causal inference. Supplementary materials for this article are available online.},
	number = {520},
	urldate = {2025-02-19},
	journal = {Journal of the American Statistical Association},
	author = {Li, Xinran and Ding, Peng},
	month = oct,
	year = {2017},
	note = {Publisher: ASA Website
\_eprint: https://doi.org/10.1080/01621459.2017.1295865},
	keywords = {Conservative confidence set, Fisher randomization test, Potential outcome, Randomization inference, Repeated sampling property, Sharp null hypothesis},
	pages = {1759--1769},
}


@article{hu2012asymptotic,
	title = {Asymptotic properties of covariate-adaptive randomization},
	volume = {40},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-40/issue-3/Asymptotic-properties-of-covariate-adaptive-randomization/10.1214/12-AOS983.full},
	doi = {10.1214/12-AOS983},
	abstract = {Balancing treatment allocation for influential covariates is critical in clinical trials. This has become increasingly important as more and more biomarkers are found to be associated with different diseases in translational research (genomics, proteomics and metabolomics). Stratified permuted block randomization and minimization methods [Pocock and Simon Biometrics 31 (1975) 103–115, etc.] are the two most popular approaches in practice. However, stratified permuted block randomization fails to achieve good overall balance when the number of strata is large, whereas traditional minimization methods also suffer from the potential drawback of large within-stratum imbalances. Moreover, the theoretical bases of minimization methods remain largely elusive. In this paper, we propose a new covariate-adaptive design that is able to control various types of imbalances. We show that the joint process of within-stratum imbalances is a positive recurrent Markov chain under certain conditions. Therefore, this new procedure yields more balanced allocation. The advantages of the proposed procedure are also demonstrated by extensive simulation studies. Our work provides a theoretical tool for future research in this area.},
	number = {3},
	urldate = {2025-02-19},
	journal = {The Annals of Statistics},
	author = {Hu, Yanqing and Hu, Feifang},
	month = jun,
	year = {2012},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {60F05, 60F10, 60F15, 62G10, Balancing covariates, clinical trial, marginal balance, Markov chain, Pocock and Simon’s design, stratified permuted block},
	pages = {1794--1815},
}



@article{li2020rerandomization,
	title = {Rerandomization and {Regression} {Adjustment}},
	volume = {82},
	issn = {1369-7412},
	url = {https://doi.org/10.1111/rssb.12353},
	doi = {10.1111/rssb.12353},
	abstract = {Randomization is a basis for the statistical inference of treatment effects without strong assumptions on the outcome-generating process. Appropriately using covariates further yields more precise estimators in randomized experiments. R. A. Fisher suggested blocking on discrete covariates in the design stage or conducting analysis of covariance in the analysis stage. We can embed blocking in a wider class of experimental design called rerandomization, and extend the classical analysis of covariance to more general regression adjustment. Rerandomization trumps complete randomization in the design stage, and regression adjustment trumps the simple difference-in-means estimator in the analysis stage. It is then intuitive to use both rerandomization and regression adjustment. Under the randomization inference framework, we establish a unified theory allowing the designer and analyser to have access to different sets of covariates. We find that asymptotically, for any given estimator with or without regression adjustment, rerandomization never hurts either the sampling precision or the estimated precision, and, for any given design with or without rerandomization, our regression-adjusted estimator never hurts the estimated precision. Therefore, combining rerandomization and regression adjustment yields better coverage properties and thus improves statistical inference. To quantify these statements theoretically, we discuss optimal regression-adjusted estimators in terms of the sampling precision and the estimated precision, and then measure the additional gains of the designer and the analyser. We finally suggest the use of rerandomization in the design and regression adjustment in the analysis followed by the Huber–White robust standard error.},
	number = {1},
	urldate = {2025-02-19},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Li, Xinran and Ding, Peng},
	month = feb,
	year = {2020},
	pages = {241--268},
}

@article{li2020rerandomization-1,
	title = {Rerandomization in \$2{\textasciicircum}\{{K}\}\$ factorial experiments},
	volume = {48},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-48/issue-1/Rerandomization-in-2K-factorial-experiments/10.1214/18-AOS1790.full},
	doi = {10.1214/18-AOS1790},
	abstract = {With many pretreatment covariates and treatment factors, the classical factorial experiment often fails to balance covariates across multiple factorial effects simultaneously. Therefore, it is intuitive to restrict the randomization of the treatment factors to satisfy certain covariate balance criteria, possibly conforming to the tiers of factorial effects and covariates based on their relative importances. This is rerandomization in factorial experiments. We study the asymptotic properties of this experimental design under the randomization inference framework without imposing any distributional or modeling assumptions of the covariates and outcomes. We derive the joint asymptotic sampling distribution of the usual estimators of the factorial effects, and show that it is symmetric, unimodal and more “concentrated” at the true factorial effects under rerandomization than under the classical factorial experiment. We quantify this advantage of rerandomization using the notions of “central convex unimodality” and “peakedness” of the joint asymptotic sampling distribution. We also construct conservative large-sample confidence sets for the factorial effects.},
	number = {1},
	urldate = {2025-02-19},
	journal = {The Annals of Statistics},
	author = {Li, Xinran and Ding, Peng and Rubin, Donald B.},
	month = feb,
	year = {2020},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62K05, 62K10, 62K15, covariate balance, tiers of covariates, tiers of factorial effects},
	pages = {43--63},
}


@article{li2018asymptotic,
	title = {Asymptotic theory of rerandomization in treatment–control experiments},
	volume = {115},
	copyright = {© 2018 . http://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/115/37/9157},
	doi = {10.1073/pnas.1808191115},
	abstract = {Although complete randomization ensures covariate balance on average, the chance of observing significant differences between treatment and control covariate distributions increases with many covariates. Rerandomization discards randomizations that do not satisfy a predetermined covariate balance criterion, generally resulting in better covariate balance and more precise estimates of causal effects. Previous theory has derived finite sample theory for rerandomization under the assumptions of equal treatment group sizes, Gaussian covariate and outcome distributions, or additive causal effects, but not for the general sampling distribution of the difference-in-means estimator for the average causal effect. We develop asymptotic theory for rerandomization without these assumptions, which reveals a non-Gaussian asymptotic distribution for this estimator, specifically a linear combination of a Gaussian random variable and truncated Gaussian random variables. This distribution follows because rerandomization affects only the projection of potential outcomes onto the covariate space but does not affect the corresponding orthogonal residuals. We demonstrate that, compared with complete randomization, rerandomization reduces the asymptotic quantile ranges of the difference-in-means estimator. Moreover, our work constructs accurate large-sample confidence intervals for the average causal effect.},
	language = {en},
	number = {37},
	urldate = {2020-12-03},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Li, Xinran and Ding, Peng and Rubin, Donald B.},
	month = sep,
	year = {2018},
	pmid = {30150408},
	note = {Publisher: National Academy of Sciences
Section: Physical Sciences},
	pages = {9157--9162},
}

\
@article{ding2017bridging,
	title = {Bridging {Finite} and {Super} {Population} {Causal} {Inference}},
	volume = {5},
	copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
	issn = {2193-3685},
	url = {https://www.degruyter.com/document/doi/10.1515/jci-2016-0027/html?lang=en&srsltid=AfmBOooct3iPrGlJnqHEP6duEVwbdrgpcHw6wkPQN-cc2GAefFDqx7QX},
	doi = {10.1515/jci-2016-0027},
	abstract = {There are two general views in causal analysis of experimental data: the super population view that the units are an independent sample from some hypothetical infinite population, and the finite population view that the potential outcomes of the experimental units are fixed and the randomness comes solely from the treatment assignment. These two views differs conceptually and mathematically, resulting in different sampling variances of the usual difference-in-means estimator of the average causal effect. Practically, however, these two views result in identical variance estimators. By recalling a variance decomposition and exploiting a completeness-type argument, we establish a connection between these two views in completely randomized experiments. This alternative formulation could serve as a template for bridging finite and super population causal inference in other scenarios.},
	language = {en},
	number = {2},
	urldate = {2025-02-19},
	journal = {Journal of Causal Inference},
	author = {Ding, Peng and Li, Xinran and Miratrix, Luke W.},
	month = sep,
	year = {2017},
	note = {Publisher: De Gruyter},
	keywords = {completeness, finite population correction, potential outcomes, simple random sample, variance of individual causal effects},
}

@article{su2024treatment,
	title = {Treatment effect quantiles in stratified randomized experiments and matched observational studies},
	volume = {111},
	issn = {1464-3510},
	url = {https://doi.org/10.1093/biomet/asad030},
	doi = {10.1093/biomet/asad030},
	abstract = {Evaluating the treatment effect has become an important topic for many applications. However, most existing literature focuses mainly on average treatment effects. When the individual effects are heavy tailed or have outlier values, not only may the average effect not be appropriate for summarizing treatment effects, but also the conventional inference for it can be sensitive and possibly invalid due to poor large-sample approximations. In this paper we focus on quantiles of individual treatment effects, which can be more robust in the presence of extreme individual effects. Moreover, our inference for them is purely randomization based, avoiding any distributional assumptions on the units. We first consider inference in stratified randomized experiments, extending the recent work by Caughey et al. (2021). We show that the computation of valid p-values for testing null hypotheses on quantiles of individual effects can be transformed into instances of the multiple-choice knapsack problem, which can be efficiently solved exactly or slightly conservatively. We then extend our approach to matched observational studies and propose a sensitivity analysis to investigate to what extent our inference on quantiles of individual effects is robust to unmeasured confounding. The proposed randomization inference and sensitivity analysis are simul- taneously valid for all quantiles of individual effects, noting that the analysis for the maximum or minimum individual effect coincides with the conventional analysis assuming constant treatment effects.},
	number = {1},
	urldate = {2025-02-19},
	journal = {Biometrika},
	author = {Su, Yongchang and Li, Xinran},
	month = mar,
	year = {2024},
	pages = {235--254},
}


@article{pashley2021insights,
  title={Insights on variance estimation for blocked and matched pairs designs},
  author={Pashley, Nicole E and Miratrix, Luke W},
  journal={Journal of Educational and Behavioral Statistics},
  volume={46},
  number={3},
  pages={271--296},
  year={2021},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{wang2023model-robust,
	title = {Model-{Robust} {Inference} for {Clinical} {Trials} that {Improve} {Precision} by {Stratified} {Randomization} and {Covariate} {Adjustment}},
	volume = {118},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2021.1981338},
	doi = {10.1080/01621459.2021.1981338},
	abstract = {Two commonly used methods for improving precision and power in clinical trials are stratified randomization and covariate adjustment. However, many trials do not fully capitalize on the combined precision gains from these two methods, which can lead to wasted resources in terms of sample size and trial duration. We derive consistency and asymptotic normality of model-robust estimators that combine these two methods, and show that these estimators can lead to substantial gains in precision and power. Our theorems cover a class of estimators that handle continuous, binary, and time-to-event outcomes; missing outcomes under the missing at random assumption are handled as well. For each estimator, we give a formula for a consistent variance estimator that is model-robust and that fully captures variance reductions from stratified randomization and covariate adjustment. Also, we give the first proof (to the best of our knowledge) of consistency and asymptotic normality of the Kaplan–Meier estimator under stratified randomization, and we derive its asymptotic variance. The above results also hold for the biased-coin covariate-adaptive design. We demonstrate our results using data from three trials of substance use disorder treatments, where the variance reduction due to stratified randomization and covariate adjustment ranges from 1\% to 36\%. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
	number = {542},
	urldate = {2025-02-19},
	journal = {Journal of the American Statistical Association},
	author = {Wang, Bingkai and Susukida, Ryoko and Mojtabai, Ramin and Amin-Esmaeili, Masoumeh and Rosenblum, Michael},
	month = apr,
	year = {2023},
	note = {Publisher: ASA Website
\_eprint: https://doi.org/10.1080/01621459.2021.1981338},
	keywords = {Covariate-adaptive randomization, Generalized linear model, Robustness},
	pages = {1152--1163},
}

@article{ye2022inference,
	title = {Inference on the average treatment effect under minimization and other covariate-adaptive randomization methods},
	volume = {109},
	issn = {1464-3510},
	url = {https://doi.org/10.1093/biomet/asab015},
	doi = {10.1093/biomet/asab015},
	abstract = {Covariate-adaptive randomization schemes such as minimization and stratified permuted blocks are often applied in clinical trials to balance treatment assignments across prognostic factors. The existing theory for inference after covariate-adaptive randomization is mostly limited to situations where a correct model between the response and covariates can be specified or the randomization method has well-understood properties. Based on stratification with covariate levels utilized in randomization and a further adjustment for covariates not used in randomization, we propose several model-free estimators of the average treatment effect. We establish the asymptotic normality of the proposed estimators under all popular covariate-adaptive randomization schemes, including the minimization method, and we show that the asymptotic distributions are invariant with respect to covariate-adaptive randomization methods. Consistent variance estimators are constructed for asymptotic inference. Asymptotic relative efficiencies and finite-sample properties of estimators are also studied. We recommend using one of our proposed estimators for valid and model-free inference after covariate-adaptive randomization.},
	number = {1},
	urldate = {2025-02-19},
	journal = {Biometrika},
	author = {Ye, Ting and Yi, Yanyao and Shao, Jun},
	month = mar,
	year = {2022},
	pages = {33--47},
}

@article{ye2023toward,
	title = {Toward {Better} {Practice} of {Covariate} {Adjustment} in {Analyzing} {Randomized} {Clinical} {Trials}},
	volume = {118},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2022.2049278},
	doi = {10.1080/01621459.2022.2049278},
	abstract = {In randomized clinical trials, adjustments for baseline covariates at both design and analysis stages are highly encouraged by regulatory agencies. A recent trend is to use a model-assisted approach for covariate adjustment to gain credibility and efficiency while producing asymptotically valid inference even when the model is incorrect. In this article we present three considerations for better practice when model-assisted inference is applied to adjust for covariates under simple or covariate-adaptive randomized trials: (a) guaranteed efficiency gain: a model-assisted method should often gain but never hurt efficiency; (b) wide applicability: a valid procedure should be applicable, and preferably universally applicable, to all commonly used randomization schemes; (c) robust standard error: variance estimation should be robust to model misspecification and heteroscedasticity. To achieve these, we recommend a model-assisted estimator under an analysis of heterogeneous covariance working model that includes all covariates used in randomization. Our conclusions are based on an asymptotic theory that provides a clear picture of how covariate-adaptive randomization and regression adjustment alter statistical efficiency. Our theory is more general than the existing ones in terms of studying arbitrary functions of response means (including linear contrasts, ratios, and odds ratios), multiple arms, guaranteed efficiency gain, optimality, and universal applicability. Supplementary materials for this article are available online.},
	number = {544},
	urldate = {2025-02-19},
	journal = {Journal of the American Statistical Association},
	author = {Ye, Ting and Shao, Jun and Yi, Yanyao and Zhao, Qingyuan},
	month = oct,
	year = {2023},
	note = {Publisher: ASA Website
\_eprint: https://doi.org/10.1080/01621459.2022.2049278},
	keywords = {Analysis of covariance, Covariate-adaptive randomization, Efficiency, Heteroscedasticity, Model-assisted, Multiple treatment arms, Treatment-by-covariate interaction},
	pages = {2370--2382},
}

@article{liu2020regression-adjusted,
	title = {Regression-adjusted average treatment effect estimates in stratified randomized experiments},
	volume = {107},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/asaa038},
	doi = {10.1093/biomet/asaa038},
	abstract = {Linear regression is often used in the analysis of randomized experiments to improve treatment effect estimation by adjusting for imbalances of covariates in the treatment and control groups. This article proposes a randomization-based inference framework for regression adjustment in stratified randomized experiments. We re-establish, under mild conditions, the finite-population central limit theorem for a stratified experiment, and we prove that both the stratified difference-in-means estimator and the regression-adjusted average treatment effect estimator are consistent and asymptotically normal; the asymptotic variance of the latter is no greater and typically less than that of the former. We also provide conservative variance estimators that can be used to construct large-sample confidence intervals for the average treatment effect.},
	number = {4},
	urldate = {2025-02-19},
	journal = {Biometrika},
	author = {Liu, Hanzhong and Yang, Yuehan},
	month = dec,
	year = {2020},
	pages = {935--948},
}

@article{ma2015testing,
	title = {Testing {Hypotheses} of {Covariate}-{Adaptive} {Randomized} {Clinical} {Trials}},
	volume = {110},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2014.922469},
	doi = {10.1080/01621459.2014.922469},
	abstract = {Covariate-adaptive designs are often implemented to balance important covariates in clinical trials. However, the theoretical properties of conventional testing hypotheses are usually unknown under covariate-adaptive randomized clinical trials. In the literature, most studies are based on simulations. In this article, we provide theoretical foundation of hypothesis testing under covariate-adaptive designs based on linear models. We derive the asymptotic distributions of the test statistics of testing both treatment effects and the significance of covariates under null and alternative hypotheses. Under a large class of covariate-adaptive designs, (i) the hypothesis testing to compare treatment effects is usually conservative in terms of small Type I error; (ii) the hypothesis testing to compare treatment effects is usually more powerful than complete randomization; and (iii) the hypothesis testing for significance of covariates is still valid. The class includes most of the covariate-adaptive designs in the literature; for example, Pocock and Simon’s marginal procedure, stratified permuted block design, etc. Numerical studies are also performed to assess their corresponding finite sample properties. Supplementary material for this article is available online.},
	number = {510},
	urldate = {2025-02-19},
	journal = {Journal of the American Statistical Association},
	author = {Ma, Wei and Hu, Feifang and Zhang, Lixin},
	month = apr,
	year = {2015},
	note = {Publisher: ASA Website
\_eprint: https://doi.org/10.1080/01621459.2014.922469},
	keywords = {Conservative tests, Linear models, Pocock and Simon’s marginal procedure, Power, Stratified permuted block design, Type I error},
	pages = {669--680},
}

@article{ma2020statistical,
	title = {Statistical {Inference} for {Covariate}-{Adaptive} {Randomization} {Procedures}},
	volume = {115},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2019.1635483},
	doi = {10.1080/01621459.2019.1635483},
	abstract = {Covariate-adaptive randomization (CAR) procedures are frequently used in comparative studies to increase the covariate balance across treatment groups. However, because randomization inevitably uses the covariate information when forming balanced treatment groups, the validity of classical statistical methods after such randomization is often unclear. In this article, we derive the theoretical properties of statistical methods based on general CAR under the linear model framework. More importantly, we explicitly unveil the relationship between covariate-adaptive and inference properties by deriving the asymptotic representations of the corresponding estimators. We apply the proposed general theory to various randomization procedures such as complete randomization, rerandomization, pairwise sequential randomization, and Atkinson’s DA-biased coin design and compare their performance analytically. Based on the theoretical results, we then propose a new approach to obtain valid and more powerful tests. These results open a door to understand and analyze experiments based on CAR. Simulation studies provide further evidence of the advantages of the proposed framework and the theoretical results. Supplementary materials for this article are available online.},
	number = {531},
	urldate = {2025-02-19},
	journal = {Journal of the American Statistical Association},
	author = {Ma, Wei and Qin, Yichen and Li, Yang and Hu, Feifang},
	month = jul,
	year = {2020},
	note = {Publisher: ASA Website
\_eprint: https://doi.org/10.1080/01621459.2019.1635483},
	keywords = {Asymptotic normality, Balancing covariates, Conservative tests, Power, Sequential analysis},
	pages = {1488--1497},
}

@article{ma2024new,
	title = {A {New} and {Unified} {Family} of {Covariate} {Adaptive} {Randomization} {Procedures} and {Their} {Properties}},
	volume = {119},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2022.2102986},
	doi = {10.1080/01621459.2022.2102986},
	abstract = {In clinical trials and other comparative studies, covariate balance is crucial for credible and efficient assessment of treatment effects. Covariate adaptive randomization (CAR) procedures are extensively used to reduce the likelihood of covariate imbalances occurring. In the literature, most studies have focused on balancing of discrete covariates. Applications of CAR with continuous covariates remain rare, especially when the interest goes beyond balancing only the first moment. In this article, we propose a family of CAR procedures that can balance general covariate features, such as quadratic and interaction terms. Our framework not only unifies many existing methods, but also introduces a much broader class of new and useful CAR procedures. We show that the proposed procedures have superior balancing properties; in particular, the convergence rate of imbalance vectors is OP(nϵ) for any ϵ{\textgreater}0 if all of the moments are finite for the covariate features, relative to OP(n) under complete randomization, where n is the sample size. Both the resulting convergence rate and its proof are novel. These favorable balancing properties lead to increased precision of treatment effect estimation in the presence of nonlinear covariate effects. The framework is applied to balance covariate means and covariance matrices simultaneously. Simulation and empirical studies demonstrate the excellent and robust performance of the proposed procedures. Supplementary materials for this article are available online.},
	number = {545},
	urldate = {2025-02-14},
	journal = {Journal of the American Statistical Association},
	author = {Ma, Wei and Li, Ping and Zhang, Li-Xin and Hu, Feifang},
	month = jan,
	year = {2024},
	note = {Publisher: ASA Website
\_eprint: https://doi.org/10.1080/01621459.2022.2102986},
	keywords = {Covariate adaptive randomization, Covariate balance, Imbalance vector, Markov chain, Treatment effect estimation},
	pages = {151--162},
	file = {Main Text:/Users/yuehaob/Zotero/storage/5S55ZRVV/A New and Unified Family of Covariate Adaptive Randomization Procedures and Their Properties.pdf:application/pdf;Supplement:/Users/yuehaob/Zotero/storage/GY2RI9DY/appx.pdf:application/pdf},
}

@book{le2022measure,
  title={Measure theory, probability, and stochastic processes},
  author={Le Gall, Jean-Fran{\c{c}}ois},
  year={2022},
  publisher={Springer}
}

@article{han2021complex,
	title = {Complex sampling designs: {Uniform} limit theorems and applications},
	volume = {49},
	issn = {0090-5364, 2168-8966},
	shorttitle = {Complex sampling designs},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-49/issue-1/Complex-sampling-designs-Uniform-limit-theorems-and-applications/10.1214/20-AOS1964.full},
	doi = {10.1214/20-AOS1964},
	abstract = {In this paper, we develop a general approach to proving global and local uniform limit theorems for the Horvitz–Thompson empirical process arising from complex sampling designs. Global theorems such as Glivenko–Cantelli and Donsker theorems, and local theorems such as local asymptotic modulus and related ratio-type limit theorems are proved for both the Horvitz–Thompson empirical process, and its calibrated version. Limit theorems of other variants and their conditional versions are also established. Our approach reveals an interesting feature: the problem of deriving uniform limit theorems for the Horvitz–Thompson empirical process is essentially no harder than the problem of establishing the corresponding finite-dimensional limit theorems, once the usual complexity conditions on the function class are satisfied. These global and local uniform limit theorems are then applied to important statistical problems including (i) \$M\$-estimation, (ii) \$Z\$-estimation and (iii) frequentist theory of pseudo-Bayes procedures, all with weighted likelihood, to illustrate their wide applicability.},
	number = {1},
	urldate = {2025-02-03},
	journal = {The Annals of Statistics},
	author = {Han, Qiyang and Wellner, Jon A.},
	month = feb,
	year = {2021},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {60E15, 62G05, Complex sampling design, empirical process, uniform limit theorems},
	pages = {459--485},
}

@article{abadie2016matching,
	title = {Matching on the {Estimated} {Propensity} {Score}},
	volume = {84},
	issn = {0012-9682},
	url = {https://www.jstor.org/stable/43866448},
	abstract = {Propensity score matching estimators (Rosenbaum and Rubin (1983)) are widely used in evaluation research to estimate average treatment effects. In this article, we derive the large sample distribution of propensity score matching estimators. Our derivations take into account that the propensity score is itself estimated in a first step, prior to matching. We prove that first step estimation of the propensity score affects the large sample distribution of propensity score matching estimators, and derive adjustments to the large sample variances of propensity score matching estimators of the average treatment effect (ATE) and the average treatment effect on the treated (ATET). The adjustment for the ATE estimator is negative (or zero in some special cases), implying that matching on the estimated propensity score is more efficient than matching on the true propensity score in large samples. However, for the ATET estimator, the sign of the adjustment term depends on the data generating process, and ignoring the estimation error in the propensity score may lead to confidence intervals that are either too large or too small.},
	number = {2},
	urldate = {2024-01-16},
	journal = {Econometrica},
	author = {Abadie, Alberto and Imbens, Guido W.},
	year = {2016},
	note = {Publisher: [Wiley, The Econometric Society]},
	pages = {781--807},
}

@article{abadie2012martingale,
	title = {A {Martingale} {Representation} for {Matching} {Estimators}},
	volume = {107},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2012.682537},
	doi = {10.1080/01621459.2012.682537},
	abstract = {Matching estimators are widely used in statistical data analysis. However, the large sample distribution of matching estimators has been derived only for particular cases. This article establishes a martingale representation for matching estimators. This representation allows the use of martingale limit theorems to derive the large sample distribution of matching estimators. As an illustration of the applicability of the theory, we derive the asymptotic distribution of a matching estimator when matching is carried out without replacement, a result previously unavailable in the literature. In addition, we apply the techniques proposed in this article to derive a correction to the standard error of a sample mean when missing data are imputed using the “hot deck,” a matching imputation method widely used in the Current Population Survey (CPS) and other large surveys in the social sciences. We demonstrate the empirical relevance of our methods using two Monte Carlo designs based on actual datasets. In these Monte Carlo exercises, the large sample distribution of matching estimators derived in this article provides an accurate approximation to the small sample behavior of these estimators. In addition, our simulations show that standard errors that do not take into account hot-deck imputation of missing data may be severely downward biased, while standard errors that incorporate the correction for hot-deck imputation perform extremely well. This article has online supplementary materials.},
	number = {498},
	urldate = {2024-01-16},
	journal = {Journal of the American Statistical Association},
	author = {Abadie, Alberto and Imbens, Guido W.},
	month = jun,
	year = {2012},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01621459.2012.682537},
	keywords = {Hot-deck imputation, Martingales, Overt bias, Treatment effects},
	pages = {833--843},
}

@misc{cai2022performance,
	title = {On the {Performance} of the {Neyman} {Allocation} with {Small} {Pilots}},
	url = {http://arxiv.org/abs/2206.04643},
	doi = {10.48550/arXiv.2206.04643},
	abstract = {The Neyman Allocation and its conditional counterpart are used in many papers on experiment design, which typically assume that researchers have access to large pilot studies. This may not be realistic. To understand the properties of the Neyman Allocation with small pilots, we study its behavior in a novel asymptotic framework for two-wave experiments in which the pilot size is assumed to be fixed while the main wave sample size grows. Our analysis shows that the Neyman Allocation can lead to estimates of the ATE with higher asymptotic variance than with (non-adaptive) balanced randomization. In particular, this happens when the outcome variable is relatively homoskedastic with respect to treatment status or when it exhibits high kurtosis. We also provide a series of empirical examples showing that these situations arise frequently in practice. Our results suggest that researchers should not use the Neyman Allocation with small pilots, especially in such instances.},
	urldate = {2024-01-16},
	publisher = {arXiv},
	author = {Cai, Yong and Rafi, Ahnaf},
	month = aug,
	year = {2022},
	note = {arXiv:2206.04643 [econ]},
	keywords = {Economics - Econometrics},
}

@article{abadie2008failure,
	title = {On the {Failure} of the {Bootstrap} for {Matching} {Estimators}},
	volume = {76},
	issn = {0012-9682},
	url = {https://www.jstor.org/stable/40056514},
	abstract = {Matching estimators are widely used in empirical economics for the evaluation of programs or treatments. Researchers using matching methods often apply the bootstrap to calculate the standard errors. However, no formal justification has been provided for the use of the bootstrap in this setting. In this article, we show that the standard bootstrap is, in general, not valid for matching estimators, even in the simple case with a single continuous covariate where the estimator is root-N consistent and asymptotically normally distributed with zero asymptotic bias. Valid inferential methods in this setting are the analytic asymptotic variance estimator of Abadie and Imbens (2006a) as well as certain modifications of the standard bootstrap, like the subsampling methods in Politis and Romanoǐ (1994).},
	number = {6},
	urldate = {2023-04-14},
	journal = {Econometrica},
	author = {Abadie, Alberto and Imbens, Guido W.},
	year = {2008},
	note = {Publisher: [Wiley, Econometric Society]},
	pages = {1537--1557},
}

@article{abadie2006large,
	title = {Large {Sample} {Properties} of {Matching} {Estimators} for {Average} {Treatment} {Effects}},
	volume = {74},
	issn = {0012-9682},
	url = {https://www.jstor.org/stable/3598929},
	abstract = {Matching estimators for average treatment effects are widely used in evaluation research despite the fact that their large sample properties have not been established in many cases. The absence of formal results in this area may be partly due to the fact that standard asymptotic expansions do not apply to matching estimators with a fixed number of matches because such estimators are highly nonsmooth functionals of the data. In this article we develop new methods for analyzing the large sample properties of matching estimators and establish a number of new results. We focus on matching with replacement with a fixed number of matches. First, we show that matching estimators are not N1/2-consistent in general and describe conditions under which matching estimators do attain N1/2-consistency. Second, we show that even in settings where matching estimators are N1/2-consistent, simple matching estimators with a fixed number of matches do not attain the semiparametric efficiency bound. Third, we provide a consistent estimator for the large sample variance that does not require consistent nonparametric estimation of unknown functions. Software for implementing these methods is available in Matlab, Stata, and R.},
	number = {1},
	urldate = {2023-04-14},
	journal = {Econometrica},
	author = {Abadie, Alberto and Imbens, Guido W.},
	year = {2006},
	note = {Publisher: [Wiley, Econometric Society]},
	pages = {235--267},
}

@article{imbens2004nonparametric,
	title = {Nonparametric {Estimation} of {Average} {Treatment} {Effects} {Under} {Exogeneity}: {A} {Review}},
	volume = {86},
	issn = {0034-6535},
	shorttitle = {Nonparametric {Estimation} of {Average} {Treatment} {Effects} {Under} {Exogeneity}},
	url = {https://doi.org/10.1162/003465304323023651},
	doi = {10.1162/003465304323023651},
	abstract = {Recently there has been a surge in econometric work focusing on estimating average treatment effects under various sets of assumptions. One strand of this literature has developed methods for estimating average treatment effects for a binary treatment under assumptions variously described as exogeneity, unconfoundedness, or selection on observables. The implication of these assumptions is that systematic (for example, average or distributional) differences in outcomes between treated and control units with the same values for the covariates are attributable to the treatment. Recent analysis has considered estimation and inference for average treatment effects under weaker assumptions than typical of the earlier literature by avoiding distributional and functional-form assumptions. Various methods of semiparametric estimation have been proposed, including estimating the unknown regression functions, matching, methods using the propensity score such as weighting and blocking,
and combinations of these approaches. In this paper I review the state of this literature and discuss some of its unanswered questions, focusing in particular on the practical implementation of these methods, the plausibility of this exogeneity assumption in economic applications, the relative performance of the various semiparametric estimators when the key assumptions (unconfoundedness and overlap) are satisfied, alternative estimands such as quantile treatment effects, and alternate methods such as Bayesian inference.},
	number = {1},
	urldate = {2024-01-16},
	journal = {The Review of Economics and Statistics},
	author = {Imbens, Guido W.},
	month = feb,
	year = {2004},
	pages = {4--29},
}


@article{hirano2001estimation,
	title = {Estimation of {Causal} {Effects} using {Propensity} {Score} {Weighting}: {An} {Application} to {Data} on {Right} {Heart} {Catheterization}},
	volume = {2},
	issn = {1572-9400},
	shorttitle = {Estimation of {Causal} {Effects} using {Propensity} {Score} {Weighting}},
	url = {https://doi.org/10.1023/A:1020371312283},
	doi = {10.1023/A:1020371312283},
	abstract = {We consider methods for estimating causal effects of treatments when treatment assignment is unconfounded with outcomes conditional on a possibly large set of covariates. Robins and Rotnitzky (1995) suggested combining regression adjustment with weighting based on the propensity score (Rosenbaum and Rubin, 1983). We adopt this approach, allowing for a flexible specification of both the propensity score and the regression function. We apply these methods to data on the effects of right heart catheterization (RHC) studied in Connors et al (1996), and we find that our estimator gives stable estimates over a wide range of values for the two parameters governing the selection of variables.},
	language = {en},
	number = {3},
	urldate = {2021-01-31},
	journal = {Health Services and Outcomes Research Methodology},
	author = {Hirano, Keisuke and Imbens, Guido W.},
	month = dec,
	year = {2001},
	pages = {259--278},
}

@article{imbens2007mean-squared,
	title = {Mean-Squared-Error Calculations for Average Treatment Effects},
	year = {2007},
	author = {Guido Imbens and Whitney Newey and Geert Ridder},
    journal = {working paper}
}

@article{newey1994large,
  title={Large sample estimation and hypothesis testing},
  author={Newey, Whitney K and McFadden, Daniel},
  journal={Handbook of econometrics},
  volume={4},
  pages={2111--2245},
  year={1994},
  publisher={Elsevier}
}

@article{duflo2007using,
  title={Using randomization in development economics research: A toolkit},
  author={Duflo, Esther and Glennerster, Rachel and Kremer, Michael},
  journal={Handbook of development economics},
  volume={4},
  pages={3895--3962},
  year={2007},
  publisher={Elsevier}
}

@book{rosenberger2015randomization,
  title={Randomization in clinical trials: theory and practice},
  author={Rosenberger, William F and Lachin, John M},
  year={2015},
  publisher={John Wiley \& Sons}
}

@book{vershynin2018high-dimensional,
	address = {Cambridge},
	series = {Cambridge {Series} in {Statistical} and {Probabilistic} {Mathematics}},
	title = {High-{Dimensional} {Probability}: {An} {Introduction} with {Applications} in {Data} {Science}},
	isbn = {978-1-108-41519-4},
	shorttitle = {High-{Dimensional} {Probability}},
	url = {https://www.cambridge.org/core/books/highdimensional-probability/797C466DA29743D2C8213493BD2D2102},
	abstract = {High-dimensional probability offers insight into the behavior of random vectors, random matrices, random subspaces, and objects used to quantify uncertainty in high dimensions. Drawing on ideas from probability, analysis, and geometry, it lends itself to applications in mathematics, statistics, theoretical computer science, signal processing, optimization, and more. It is the first to integrate theory, key tools, and modern applications of high-dimensional probability. Concentration inequalities form the core, and it covers both classical results such as Hoeffding's and Chernoff's inequalities and modern developments such as the matrix Bernstein's inequality. It then introduces the powerful methods based on stochastic processes, including such tools as Slepian's, Sudakov's, and Dudley's inequalities, as well as generic chaining and bounds based on VC dimension. A broad range of illustrations is embedded throughout, including classical and modern results for covariance estimation, clustering, networks, semidefinite programming, coding, dimension reduction, matrix completion, machine learning, compressed sensing, and sparse regression.},
	urldate = {2023-06-22},
	publisher = {Cambridge University Press},
	author = {Vershynin, Roman},
	year = {2018},
	doi = {10.1017/9781108231596},
}

@book{dudley2014uniform,
	address = {Cambridge},
	edition = {2},
	series = {Cambridge {Studies} in {Advanced} {Mathematics}},
	title = {Uniform {Central} {Limit} {Theorems}},
	isbn = {978-0-521-49884-5},
	urldate = {2020-09-12},
	publisher = {Cambridge University Press},
	author = {Dudley, R. M.},
	year = {2014},
}

@article{van_der_vaart1991differentiable,
	title = {On {Differentiable} {Functionals}},
	volume = {19},
	issn = {0090-5364},
	url = {https://www.jstor.org/stable/2241850},
	abstract = {Given a sample of size n from a distribution Pλ, one wants to estimate a functional ψ(λ) of the (typically infinite-dimensional) parameter λ. Lower bounds on the performance of estimators can be based on the concept of a differentiable functional Pλ → ψ(λ). In this paper we relate a suitable definition of differentiable functional to differentiability of α → dP1/2 λ and λ → ψ(λ). Moreover, we show that regular estimability of a functional implies its differentiability.},
	number = {1},
	urldate = {2021-03-30},
	journal = {The Annals of Statistics},
	author = {van der Vaart, Aad},
	year = {1991},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {178--204},
	file = {Full Text:/Users/yuehaob/Zotero/storage/3R9LAU3K/van der Vaart - 1991 - On Differentiable Functionals.pdf:application/pdf},
}

@article{newey1994asymptotic,
	title = {The {Asymptotic} {Variance} of {Semiparametric} {Estimators}},
	volume = {62},
	issn = {0012-9682},
	url = {https://www.jstor.org/stable/2951752},
	doi = {10.2307/2951752},
	abstract = {The purpose of this paper is the presentation of a general formula for the asymptotic variance of a semiparametric estimator. A particularly important feature of this formula is a way of accounting for the presence of nonparametric estimates of nuisance functions. The general form of an adjustment factor for nonparametric estimates is derived and analyzed. The usefulness of the formula is illustrated by deriving propositions on invariance of the limiting distribution with respect to the nonparametric estimator, conditions for nonparametric estimation to have no effect on the asymptotic distribution, and the form of a correction term for the presence of nonparametric projection and density estimators. Examples discussed are quasi-maximum likelihood estimation of index models, panel probit with semiparametric individual effects, average derivatives, and inverse density weighted least squares. The paper also develops a set of regularity conditions for the validity of the asymptotic variance formula. Primitive regularity conditions are derived for {\textless}tex-math{\textgreater}\${\textbackslash}sqrt\{n\}{\textbackslash}text\{-consistency\}\${\textless}/tex-math{\textgreater} and asymptotic normality for functions of series estimators of projections. Specific examples are polynomial estimators of average derivative and semiparametric panel probit models.},
	number = {6},
	urldate = {2021-04-28},
	journal = {Econometrica},
	author = {Newey, Whitney K.},
	year = {1994},
	note = {Publisher: [Wiley, Econometric Society]},
	pages = {1349--1382},
	file = {Full Text:/Users/yuehaob/Zotero/storage/ER65SGJ8/Newey - 1994 - The Asymptotic Variance of Semiparametric Estimato.pdf:application/pdf},
}

@article{van_der_vaart1991asymptotic,
	title = {An {Asymptotic} {Representation} {Theorem}},
	volume = {59},
	issn = {0306-7734},
	url = {https://www.jstor.org/stable/1403577},
	doi = {10.2307/1403577},
	abstract = {Let a sequence of statistical experiments converge to a limit experiment in the sense of Le Cam (1972). Furthermore assume that a sequence of statistics possesses a limit distribution under every statistical parameter. Then its set of limit distributions is also the set of distributions of some randomized estimator in the limit experiment. This is a simple way of saying that the limit experiment is a 'lower bound' for the converging sequence of experiments. Moreover, convolution and minimax theorems can be obtained as corollaries. /// Soit une suite d'expériences stochastiques convergeant vers une expérience limite au sens de Le Cam (1972). Supposons aussi qu'une suite de statistiques a une distribution limite pour toute valeur possible du paramètre, alors l'ensemble des distributions limites est l'ensemble des distributions d'un estimateur randomisé dans l'expérience limite. Ceci exprime de façon simple que l'expérience limite est une 'borne inférieure' pour la suite convergeante d'expériences. De plus, l'on peut obtenir comme corollaires des résultats minimax et des théorèmes de convolution.},
	number = {1},
	urldate = {2021-03-30},
	journal = {International Statistical Review / Revue Internationale de Statistique},
	author = {van der Vaart, Aad},
	year = {1991},
	note = {Publisher: [Wiley, International Statistical Institute (ISI)]},
	pages = {97--121},
	file = {Full Text:/Users/yuehaob/Zotero/storage/J8KG2QAH/van der Vaart - 1991 - An Asymptotic Representation Theorem.pdf:application/pdf},
}

@article{robins1997toward,
	title = {Toward a {Curse} of {Dimensionality} {Appropriate} (coda) {Asymptotic} {Theory} for {Semi}-{Parametric} {Models}},
	volume = {16},
	copyright = {Copyright © 1997 John Wiley \& Sons, Ltd.},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-0258%2819970215%2916%3A3%3C285%3A%3AAID-SIM535%3E3.0.CO%3B2-%23},
	doi = {https://doi.org/10.1002/(SICI)1097-0258(19970215)16:3<285::AID-SIM535>3.0.CO;2-#},
	abstract = {We argue, that due to the curse of dimensionality, there are major difficulties with any pure or smoothed likelihood-based method of inference in designed studies with randomly missing data when missingness depends on a high-dimensional vector of variables. We study in detail a semi-parametric superpopulation version of continuously stratified random sampling. We show that all estimators of the population mean that are uniformly consistent or that achieve an algebraic rate of convergence, no matter how slow, require the use of the selection (randomization) probabilities. We argue that, in contrast to likelihood methods which ignore these probabilities, inverse selection probability weighted estimators continue to perform well achieving uniform n1/2-rates of convergence. We propose a curse of dimensionality appropriate (CODA) asymptotic theory for inference in non- and semi-parametric models in an attempt to formalize our arguments. We discuss whether our results constitute a fatal blow to the likelihood principle and study the attitude toward these that a committed subjective Bayesian would adopt. Finally, we apply our CODA theory to analyse the effect of the ‘curse of dimensionality’ in several interesting semi-parametric models, including a model for a two-armed randomized trial with randomization probabilities depending on a vector of continuous pre-treatment covariates X. We provide substantive settings under which a subjective Bayesian would ignore the randomization probabilities in analysing the trial data. We then show that any statistician who ignores the randomization probabilities is unable to construct nominal 95 per cent confidence intervals for the true treatment effect that have both: (i) an expected length which goes to zero with increasing sample size; and (ii) a guaranteed expected actual coverage rate of at least 95 per cent over the ensemble of trials analysed by the statistician during his or her lifetime. However, we derive a new interval estimator, depending on the Randomization probabilities, that satisfies (i) and (ii). © 1997 by John Wiley \& Sons, Ltd.},
	language = {en},
	number = {3},
	urldate = {2021-02-02},
	journal = {Statistics in Medicine},
	author = {Robins, James M. and Ritov, Ya'acov},
	year = {1997},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/\%28SICI\%291097-0258\%2819970215\%2916\%3A3\%3C285\%3A\%3AAID-SIM535\%3E3.0.CO\%3B2-\%23},
	pages = {285--319},
}

@article{hirano2003efficient,
	title = {Efficient {Estimation} of {Average} {Treatment} {Effects} {Using} the {Estimated} {Propensity} {Score}},
	volume = {71},
	issn = {1468-0262},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-0262.00442},
	doi = {https://doi.org/10.1111/1468-0262.00442},
	abstract = {We are interested in estimating the average effect of a binary treatment on a scalar outcome. If assignment to the treatment is exogenous or unconfounded, that is, independent of the potential outcomes given covariates, biases associated with simple treatment-control average comparisons can be removed by adjusting for differences in the covariates. Rosenbaum and Rubin (1983) show that adjusting solely for differences between treated and control units in the propensity score removes all biases associated with differences in covariates. Although adjusting for differences in the propensity score removes all the bias, this can come at the expense of efficiency, as shown by Hahn (1998), Heckman, Ichimura, and Todd (1998), and Robins, Mark, and Newey (1992). We show that weighting by the inverse of a nonparametric estimate of the propensity score, rather than the true propensity score, leads to an efficient estimate of the average treatment effect. We provide intuition for this result by showing that this estimator can be interpreted as an empirical likelihood estimator that efficiently incorporates the information about the propensity score.},
	language = {en},
	number = {4},
	urldate = {2021-02-02},
	journal = {Econometrica},
	author = {Hirano, Keisuke and Imbens, Guido W. and Ridder, Geert},
	year = {2003},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1468-0262.00442},
	pages = {1161--1189},
}

@book{van_der_vaart1996weak,
	address = {New York},
	series = {Springer {Series} in {Statistics}},
	title = {Weak {Convergence} and {Empirical} {Processes}: {With} {Applications} to {Statistics}},
	isbn = {978-0-387-94640-5},
	shorttitle = {Weak {Convergence} and {Empirical} {Processes}},
	url = {https://www.springer.com/gp/book/9780387946405},
	abstract = {This book tries to do three things. The first goal is to give an exposition of certain modes of stochastic convergence, in particular convergence in distribution. The classical theory of this subject was developed mostly in the 1950s and is well summarized in Billingsley (1968). During the last 15 years, the need for a more general theory allowing random elements that are not Borel measurable has become well established, particularly in developing the theory of empirical processes. Part 1 of the book, Stochastic Convergence, gives an exposition of such a theory following the ideas of J. Hoffmann-J!1Jrgensen and R. M. Dudley. A second goal is to use the weak convergence theory background devel­ oped in Part 1 to present an account of major components of the modern theory of empirical processes indexed by classes of sets and functions. The weak convergence theory developed in Part 1 is important for this, simply because the empirical processes studied in Part 2, Empirical Processes, are naturally viewed as taking values in nonseparable Banach spaces, even in the most elementary cases, and are typically not Borel measurable. Much of the theory presented in Part 2 has previously been scattered in the journal literature and has, as a result, been accessible only to a relatively small number of specialists. In view of the importance of this theory for statis­ tics, we hope that the presentation given here will make this theory more accessible to statisticians as well as to probabilists interested in statistical applications.},
	language = {en},
	urldate = {2020-12-03},
	publisher = {Springer-Verlag},
	author = {van der Vaart, A. W. and Wellner, Jon},
	year = {1996},
	doi = {10.1007/978-1-4757-2545-2},
}

@book{lehmann2005testing,
	address = {New York},
	edition = {3},
	series = {Springer {Texts} in {Statistics}},
	title = {Testing {Statistical} {Hypotheses}},
	isbn = {978-0-387-98864-1},
	url = {https://www.springer.com/gp/book/9780387988641},
	abstract = {The third edition of Testing Statistical Hypotheses updates and expands upon the classic graduate text, emphasizing optimality theory for hypothesis testing and confidence sets. The principal additions include a rigorous treatment of large sample optimality, together with the requisite tools. In addition, an introduction to the theory of resampling methods such as the bootstrap is developed. The sections on multiple testing and goodness of fit testing are expanded. The text is suitable for Ph.D. students in statistics and includes over 300 new problems out of a total of more than 760. E.L. Lehmann is Professor of Statistics Emeritus at the University of California, Berkeley. He is a member of the National Academy of Sciences and the American Academy of Arts and Sciences, and the recipient of honorary degrees from the University of Leiden, The Netherlands and the University of Chicago. He is the author of Elements of Large-Sample Theory and (with George Casella) he is also the author of Theory of Point Estimation, Second Edition. Joseph P. Romano is Professor of Statistics at Stanford University. He is a recipient of a Presidential Young Investigator Award and a Fellow of the Institute of Mathematical Statistics. He has coauthored two other books, Subsampling with Dimitris Politis and Michael Wolf, and Counterexamples in Probability and Statistics with Andrew Siegel.},
	language = {en},
	urldate = {2020-12-03},
	publisher = {Springer-Verlag},
	author = {Lehmann, Erich L. and Romano, Joseph P.},
	year = {2005},
	doi = {10.1007/0-387-27605-X},
}

@book{ledoux1991probability,
	address = {Berlin Heidelberg},
	series = {Classics in {Mathematics}},
	title = {Probability in {Banach} {Spaces}: {Isoperimetry} and {Processes}},
	isbn = {978-3-642-20211-7},
	shorttitle = {Probability in {Banach} {Spaces}},
	url = {https://www.springer.com/gp/book/9783642202117},
	abstract = {Isoperimetric, measure concentration and random process techniques appear at the basis of the modern understanding of Probability in Banach spaces. Based on these tools, the book presents a complete treatment of the main aspects of Probability in Banach spaces (integrability and limit theorems for vector valued random variables, boundedness and continuity of random processes) and of some of their links to Geometry of Banach spaces (via the type and cotype properties). Its purpose is to present some of the main aspects of this theory, from the foundations to the most important achievements. The main features of the investigation are the systematic use of isoperimetry and concentration of measure and abstract random process techniques (entropy and majorizing measures). Examples of these probabilistic tools and ideas to classical Banach space theory are further developed.},
	language = {en},
	urldate = {2020-12-03},
	publisher = {Springer-Verlag},
	author = {Ledoux, Michel and Talagrand, Michel},
	year = {1991},
	doi = {10.1007/978-3-642-20212-4},
}

@book{kosorok2008introduction,
	address = {New York},
	series = {Springer {Series} in {Statistics}},
	title = {Introduction to {Empirical} {Processes} and {Semiparametric} {Inference}},
	isbn = {978-0-387-74977-8},
	url = {https://www.springer.com/gp/book/9780387749778},
	abstract = {This book provides a self-contained, linear, and unified introduction to empirical processes and semiparametric inference. These powerful research techniques are surprisingly useful for developing methods of statistical inference for complex models and in understanding the properties of such methods. The targeted audience includes statisticians, biostatisticians, and other researchers with a background in mathematical statistics who have an interest in learning about and doing research in empirical processes and semiparametric inference but who would like to have a friendly and gradual introduction to the area. The book can be used either as a research reference or as a textbook. The level of the book is suitable for a second year graduate course in statistics or biostatistics, provided the students have had a year of graduate level mathematical statistics and a semester of probability.The book consists of three parts. The first part is a concise overview of all of the main concepts covered in the book with a minimum of technicalities. The second and third parts cover the two respective main topics of empirical processes and semiparametric inference in depth. The connections between these two topics is also demonstrated and emphasized throughout the text. Each part has a final chapter with several case studies that use concrete examples to illustrate the concepts developed so far. The last two parts also each include a chapter which covers the needed mathematical preliminaries. Each main idea is introduced with a non-technical motivation, and examples are given throughout to illustrate important concepts. Homework problems are also included at the end of each chapter to help the reader gain additional insights.Michael R. Kosorok is Professor and Chair, Department of Biostatistics, and Professor, Department of Statistics and Operations Research, at the University of North Carolina at Chapel Hill. His research has focused on the application of empirical processes and semiparametric inference to statistics and biostatistics. He is a Fellow of both the American Statistical Association and the Institute of Mathematical Statistics. He is an Associate Editor of the Annals of Statistics, Electronic Journal of Statistics, International Journal of Biostatistics, Statistics and Probability Letters, and Statistics Surveys.},
	language = {en},
	urldate = {2020-12-03},
	publisher = {Springer-Verlag},
	author = {Kosorok, Michael R.},
	year = {2008},
	doi = {10.1007/978-0-387-74978-5},
}

@book{van_der_vaart1998asymptotic,
	series = {Cambridge {Series} in {Statistical} and {Probabilistic} {Mathematics}},
	title = {Asymptotic statistics},
	volume = {3},
	isbn = {0-521-49603-9 0-521-78450-6},
	url = {https://doi-org.proxy.lib.umich.edu/10.1017/CBO9780511802256},
	publisher = {Cambridge University Press, Cambridge},
	author = {van der Vaart, A. W.},
	year = {1998},
	mrnumber = {1652247},
	doi = {10.1017/CBO9780511802256},
}

@article{bugni2018inference,
	title = {Inference {Under} {Covariate}-{Adaptive} {Randomization}},
	volume = {113},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2017.1375934},
	doi = {10.1080/01621459.2017.1375934},
	abstract = {This article studies inference for the average treatment effect in randomized controlled trials with covariate-adaptive randomization. Here, by covariate-adaptive randomization, we mean randomization schemes that first stratify according to baseline covariates and then assign treatment status so as to achieve “balance” within each stratum. Our main requirement is that the randomization scheme assigns treatment status within each stratum so that the fraction of units being assigned to treatment within each stratum has a well behaved distribution centered around a proportion π as the sample size tends to infinity. Such schemes include, for example, Efron’s biased-coin design and stratified block randomization. When testing the null hypothesis that the average treatment effect equals a prespecified value in such settings, we first show the usual two-sample t-test is conservative in the sense that it has limiting rejection probability under the null hypothesis no greater than and typically strictly less than the nominal level. We show, however, that a simple adjustment to the usual standard error of the two-sample t-test leads to a test that is exact in the sense that its limiting rejection probability under the null hypothesis equals the nominal level. Next, we consider the usual t-test (on the coefficient on treatment assignment) in a linear regression of outcomes on treatment assignment and indicators for each of the strata. We show that this test is exact for the important special case of randomization schemes with π=12, but is otherwise conservative. We again provide a simple adjustment to the standard errors that yields an exact test more generally. Finally, we study the behavior of a modified version of a permutation test, which we refer to as the covariate-adaptive permutation test, that only permutes treatment status for units within the same stratum. When applied to the usual two-sample t-statistic, we show that this test is exact for randomization schemes with π=12 and that additionally achieve what we refer to as “strong balance.” For randomization schemes with π≠12, this test may have limiting rejection probability under the null hypothesis strictly greater than the nominal level. When applied to a suitably adjusted version of the two-sample t-statistic, however, we show that this test is exact for all randomization schemes that achieve “strong balance,” including those with π≠12. A simulation study confirms the practical relevance of our theoretical results. We conclude with recommendations for empirical practice and an empirical illustration. Supplementary materials for this article are available online.},
	number = {524},
	urldate = {2020-12-03},
	journal = {Journal of the American Statistical Association},
	author = {Bugni, Federico A. and Canay, Ivan A. and Shaikh, Azeem M.},
	month = oct,
	year = {2018},
	pmid = {30906087},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01621459.2017.1375934},
	pages = {1784--1796},
}

@article{bugni2019inference,
	title = {Inference under covariate-adaptive randomization with multiple treatments},
	volume = {10},
	copyright = {Copyright © 2019 The Authors.},
	issn = {1759-7331},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/QE1150},
	doi = {https://doi.org/10.3982/QE1150},
	abstract = {This paper studies inference in randomized controlled trials with covariate-adaptive randomization when there are multiple treatments. More specifically, we study in this setting inference about the average effect of one or more treatments relative to other treatments or a control. As in Bugni, Canay, and Shaikh (2018), covariate-adaptive randomization refers to randomization schemes that first stratify according to baseline covariates and then assign treatment status so as to achieve “balance” within each stratum. Importantly, in contrast to Bugni, Canay, and Shaikh (2018), we not only allow for multiple treatments, but further allow for the proportion of units being assigned to each of the treatments to vary across strata. We first study the properties of estimators derived from a “fully saturated” linear regression, that is, a linear regression of the outcome on all interactions between indicators for each of the treatments and indicators for each of the strata. We show that tests based on these estimators using the usual heteroskedasticity-consistent estimator of the asymptotic variance are invalid in the sense that they may have limiting rejection probability under the null hypothesis strictly greater than the nominal level; on the other hand, tests based on these estimators and suitable estimators of the asymptotic variance that we provide are exact in the sense that they have limiting rejection probability under the null hypothesis equal to the nominal level. For the special case in which the target proportion of units being assigned to each of the treatments does not vary across strata, we additionally consider tests based on estimators derived from a linear regression with “strata fixed effects,” that is, a linear regression of the outcome on indicators for each of the treatments and indicators for each of the strata. We show that tests based on these estimators using the usual heteroskedasticity-consistent estimator of the asymptotic variance are conservative in the sense that they have limiting rejection probability under the null hypothesis no greater than and typically strictly less than the nominal level, but tests based on these estimators and suitable estimators of the asymptotic variance that we provide are exact, thereby generalizing results in Bugni, Canay, and Shaikh (2018) for the case of a single treatment to multiple treatments. A simulation study and an empirical application illustrate the practical relevance of our theoretical results.},
	language = {en},
	number = {4},
	urldate = {2020-12-03},
	journal = {Quantitative Economics},
	author = {Bugni, Federico A. and Canay, Ivan A. and Shaikh, Azeem M.},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.3982/QE1150},
	keywords = {C12, C14, Covariate-adaptive randomization, Efron's biased-coin design, multiple treatments, randomized controlled trial, saturated regression, strata fixed effects, stratified block randomization, treatment assignment},
	pages = {1747--1785},
}

@article{chernozhukov2017doubledebiasedneyman,
	title = {Double/{Debiased}/{Neyman} {Machine} {Learning} of {Treatment} {Effects}},
	volume = {107},
	issn = {0002-8282},
	url = {http://www.aeaweb.org/articles?id=10.1257/aer.p20171038},
	doi = {10.1257/aer.p20171038},
	abstract = {Chernozhukov et al. (2016) provide a generic double/de-biased machine learning (ML) approach for obtaining valid inferential statements about focal parameters, using Neyman-orthogonal scores and cross-fitting, in settings where nuisance parameters are estimated using ML methods. In this note, we illustrate the application of this method in the context of estimating average treatment effects and average treatment effects on the treated using observational data.},
	language = {en},
	number = {5},
	urldate = {2020-12-03},
	journal = {American Economic Review},
	author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney},
	month = may,
	year = {2017},
	keywords = {Quantile Regressions, Quantile Regressions, Multiple or Simultaneous Equation Models: Cross-Sectional Models, Single Equation Models, Single Variables: Cross-Sectional Models, Social Interaction Models, Spatial Models, Treatment Effect Models},
	pages = {261--265},
}

@article{hahn1998role,
	title = {On the {Role} of the {Propensity} {Score} in {Efficient} {Semiparametric} {Estimation} of {Average} {Treatment} {Effects}},
	volume = {66},
	issn = {0012-9682},
	url = {https://www.jstor.org/stable/2998560},
	doi = {10.2307/2998560},
	abstract = {In this paper, the role of the propensity score in the efficient estimation of average treatment effects is examined. Under the assumption that the treatment is ignorable given some observed characteristics, it is shown that the propensity score is ancillary for estimation of the average treatment effects. The propensity score is not ancillary for estimation of average treatment effects on the treated. It is suggested that the marginal value of the propensity score lies entirely in the "dimension reduction." Efficient semiparametric estimators of average treatment effects and average treatment effects on the treated are shown to take the form of relevant sample averages of the data completed by the nonparametric imputation method. It is shown that the projection on the propensity score is not necessary for efficient semiparametric estimation of average treatment effects on the treated even if the propensity score is known. An application to the experimental data reveals that conditioning on the propensity score may even result in a loss of efficiency.},
	number = {2},
	urldate = {2020-12-03},
	journal = {Econometrica},
	author = {Hahn, Jinyong},
	year = {1998},
	note = {Publisher: [Wiley, Econometric Society]},
	pages = {315--331},
	file = {Full Text:/Users/yuehaob/Zotero/storage/L9SWDBJT/Hahn - 1998 - On the Role of the Propensity Score in Efficient S.pdf:application/pdf},
}

@misc{bugni2022inference,
	title = {Inference for {Cluster} {Randomized} {Experiments} with {Non}-ignorable {Cluster} {Sizes}},
	url = {http://arxiv.org/abs/2204.08356},
	doi = {10.48550/arXiv.2204.08356},
	abstract = {This paper considers the problem of inference in cluster randomized experiments when cluster sizes are non-ignorable. Here, by a cluster randomized experiment, we mean one in which treatment is assigned at the level of the cluster; by non-ignorable cluster sizes we mean that "large" clusters and "small" clusters may be heterogeneous, and, in particular, the effects of the treatment may vary across clusters of differing sizes. In order to permit this sort of flexibility, we consider a sampling framework in which cluster sizes themselves are random. In this way, our analysis departs from earlier analyses of cluster randomized experiments in which cluster sizes are treated as non-random. We distinguish between two different parameters of interest: the equally-weighted cluster-level average treatment effect, and the size-weighted cluster-level average treatment effect. For each parameter, we provide methods for inference in an asymptotic framework where the number of clusters tends to infinity and treatment is assigned using simple random sampling. We additionally permit the experimenter to sample only a subset of the units within each cluster rather than the entire cluster and demonstrate the implications of such sampling for some commonly used estimators. A small simulation study shows the practical relevance of our theoretical results.},
	urldate = {2022-10-14},
	publisher = {arXiv},
	author = {Bugni, Federico and Canay, Ivan and Shaikh, Azeem and Tabord-Meehan, Max},
	month = jun,
	year = {2022},
	note = {arXiv:2204.08356 [econ, stat]},
	keywords = {Economics - Econometrics, Statistics - Methodology},
}

@article{robins1995analysis,
	title = {Analysis of {Semiparametric} {Regression} {Models} for {Repeated} {Outcomes} in the {Presence} of {Missing} {Data}},
	volume = {90},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2291134},
	doi = {10.2307/2291134},
	abstract = {We propose a class of inverse probability of censoring weighted estimators for the parameters of models for the dependence of the mean of a vector of correlated response variables on a vector of explanatory variables in the presence of missing response data. The proposed estimators do not require full specification of the likelihood. They can be viewed as an extension of generalized estimating equations estimators that allow for the data to be missing at random but not missing completely at random. These estimators can be used to correct for dependent censoring and nonrandom noncompliance in randomized clinical trials studying the effect of a treatment on the evolution over time of the mean of a response variable. The likelihood-based parametric G-computation algorithm estimator may also be used to attempt to correct for dependent censoring and nonrandom noncompliance. But because of possible model misspecification, the parametric G-computation algorithm estimator, in contrast with the proposed weighted estimators, may be inconsistent for the difference in treatment-arm-specific means, even when compliance is completely at random and censoring is independent. We illustrate our methods with the analysis of the effect of zidovudine (AZT) treatment on the evolution of mean CD4 count with data from an AIDS clinical trial.},
	number = {429},
	urldate = {2022-10-24},
	journal = {Journal of the American Statistical Association},
	author = {Robins, James M. and Rotnitzky, Andrea and Zhao, Lue Ping},
	year = {1995},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {106--121},
}

@article{bai2022optimality,
	title = {Optimality of {Matched}-{Pair} {Designs} in {Randomized} {Controlled} {Trials}},
	volume = {112},
	issn = {0002-8282},
	url = {https://www.aeaweb.org/articles?id=10.1257/aer.20201856},
	doi = {10.1257/aer.20201856},
	abstract = {In randomized controlled trials, treatment is often assigned by stratified randomization. I show that among all stratified randomization schemes that treat all units with probability one half, a certain matched-pair design achieves the maximum statistical precision for estimating the average treatment effect. In an important special case, the optimal design pairs units according to the baseline outcome. In a simulation study based on datasets from ten randomized controlled trials, this design lowers the standard error for the estimator of the average treatment effect by 10 percent on average, and by up to 34 percent, relative to the original designs.},
	language = {en},
	number = {12},
	urldate = {2022-11-28},
	journal = {American Economic Review},
	author = {Bai, Yuehao},
	month = dec,
	year = {2022},
	keywords = {Economics - Econometrics, ex-post bias, matched pairs, Matched-pair design, Mathematics - Statistics Theory, pilot experiment, randomized controlled trial, Statistics - Methodology, stratification, stratified randomization, treat- ment effect},
	pages = {3911--3940},
}

@article{bai2022inference,
	title = {Inference in {Experiments} {With} {Matched} {Pairs}},
	volume = {117},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2021.1883437},
	doi = {10.1080/01621459.2021.1883437},
	abstract = {This article studies inference for the average treatment effect in randomized controlled trials where treatment status is determined according to a “matched pairs” design. By a “matched pairs” design, we mean that units are sampled iid from the population of interest, paired according to observed, baseline covariates and finally, within each pair, one unit is selected at random for treatment. This type of design is used routinely throughout the sciences, but fundamental questions about its implications for inference about the average treatment effect remain. The main requirement underlying our analysis is that pairs are formed so that units within pairs are suitably “close” in terms of the baseline covariates, and we develop novel results to ensure that pairs are formed in a way that satisfies this condition. Under this assumption, we show that, for the problem of testing the null hypothesis that the average treatment effect equals a prespecified value in such settings, the commonly used two-sample t-test and “matched pairs” t-test are conservative in the sense that these tests have limiting rejection probability under the null hypothesis no greater than and typically strictly less than the nominal level. We show, however, that a simple adjustment to the standard errors of these tests leads to a test that is asymptotically exact in the sense that its limiting rejection probability under the null hypothesis equals the nominal level. We also study the behavior of randomization tests that arise naturally in these types of settings. When implemented appropriately, we show that this approach also leads to a test that is asymptotically exact in the sense described previously, but additionally has finite-sample rejection probability no greater than the nominal level for certain distributions satisfying the null hypothesis. A simulation study and empirical application confirm the practical relevance of our theoretical results.},
	number = {540},
	urldate = {2023-01-30},
	journal = {Journal of the American Statistical Association},
	author = {Bai, Yuehao and Romano, Joseph P. and Shaikh, Azeem M.},
	month = oct,
	year = {2022},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01621459.2021.1883437},
	pages = {1726--1737},
}

@article{tsiatis2008covariate,
	title = {Covariate adjustment for two-sample treatment comparisons in randomized clinical trials: a principled yet flexible approach},
	volume = {27},
	issn = {0277-6715},
	shorttitle = {Covariate adjustment for two-sample treatment comparisons in randomized clinical trials},
	doi = {10.1002/sim.3113},
	abstract = {There is considerable debate regarding whether and how covariate-adjusted analyses should be used in the comparison of treatments in randomized clinical trials. Substantial baseline covariate information is routinely collected in such trials, and one goal of adjustment is to exploit covariates associated with outcome to increase precision of estimation of the treatment effect. However, concerns are routinely raised over the potential for bias when the covariates used are selected post hoc and the potential for adjustment based on a model of the relationship between outcome, covariates, and treatment to invite a 'fishing expedition' for that leading to the most dramatic effect estimate. By appealing to the theory of semiparametrics, we are led naturally to a characterization of all treatment effect estimators and to principled, practically feasible methods for covariate adjustment that yield the desired gains in efficiency and that allow covariate relationships to be identified and exploited while circumventing the usual concerns. The methods and strategies for their implementation in practice are presented. Simulation studies and an application to data from an HIV clinical trial demonstrate the performance of the techniques relative to the existing methods.},
	language = {eng},
	number = {23},
	journal = {Statistics in Medicine},
	author = {Tsiatis, Anastasios A. and Davidian, Marie and Zhang, Min and Lu, Xiaomin},
	month = oct,
	year = {2008},
	pmid = {17960577},
	pmcid = {PMC2562926},
	keywords = {Algorithms, Data Interpretation, Statistical, Humans, Randomized Controlled Trials as Topic, Sampling Studies, Statistics, Nonparametric, Treatment Outcome},
	pages = {4658--4677},
}

@article{van_der_vaart1989asymptotic,
	title = {On the {Asymptotic} {Information} {Bound}},
	volume = {17},
	issn = {0090-5364},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-17/issue-4/On-the-Asymptotic-Information-Bound/10.1214/aos/1176347377.full},
	doi = {10.1214/aos/1176347377},
	language = {en},
	number = {4},
	urldate = {2023-04-04},
	journal = {The Annals of Statistics},
	author = {van der Vaart, Aad},
	month = dec,
	year = {1989},
	file = {Full Text:/Users/yuehaob/Zotero/storage/5NFEKDQ9/van der Vaart - 1989 - On the Asymptotic Information Bound.pdf:application/pdf},
}

@misc{bai2023revisiting,
	title = {Revisiting the {Analysis} of {Matched}-{Pair} and {Stratified} {Experiments} in the {Presence} of {Attrition}},
	url = {http://arxiv.org/abs/2209.11840},
	doi = {10.48550/arXiv.2209.11840},
	abstract = {In this paper we revisit some common recommendations regarding the analysis of matched-pair and stratified experimental designs in the presence of attrition. Our main objective is to clarify a number of well-known claims about the practice of dropping pairs with an attrited unit when analyzing matched-pair designs. Contradictory advice appears in the literature about whether or not dropping pairs is beneficial or harmful, and stratifying into larger groups has been recommended as a resolution to the issue. To address these claims, we derive the estimands obtained from the difference-in-means estimator in a matched-pair design both when the observations from pairs with an attrited unit are retained and when they are dropped. We find limited evidence to support the claims that dropping pairs helps recover the average treatment effect, but we find that it may potentially help in recovering a convex weighted average of conditional average treatment effects. We report similar findings for stratified designs when studying the estimands obtained from a regression of outcomes on treatment with and without strata fixed effects.},
	urldate = {2023-04-04},
	publisher = {arXiv},
	author = {Bai, Yuehao and Hsieh, Meng Hsuan and Liu, Jizhou and Tabord-Meehan, Max},
	month = feb,
	year = {2023},
	note = {arXiv:2209.11840 [econ, stat]},
}

@article{frolich2007nonparametric,
	title = {Nonparametric {IV} estimation of local average treatment effects with covariates},
	volume = {139},
	issn = {03044076},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304407606001023},
	doi = {10.1016/j.jeconom.2006.06.004},
	abstract = {In this paper nonparametric instrumental variable estimation of local average treatment effects (LATE) is extended to incorporate covariates. Estimation of LATE is appealing since identiﬁcation relies on much weaker assumptions than the identiﬁcation of average treatment effects in other nonparametric instrumental variable models. Including covariates in the estimation of LATE is necessary when the instrumental variable itself is confounded, such that the IV assumptions are valid only conditional on covariates. Previous approaches to handle covariates in the estimation of LATE relied on parametric or semiparametric methods. In this paper, a nonparametric estimator for the estimation of LATE with covariates is suggested that is root-n asymptotically normal and efﬁcient. r 2006 Elsevier B.V. All rights reserved.},
	language = {en},
	number = {1},
	urldate = {2023-04-04},
	journal = {Journal of Econometrics},
	author = {Frolich, Markus},
	month = jul,
	year = {2007},
	pages = {35--75},
	file = {Full Text:/Users/yuehaob/Zotero/storage/BBUPPLYP/Frölich - 2007 - Nonparametric IV estimation of local average treat.pdf:application/pdf},
}

@article{robins1995semiparametric,
	title = {Semiparametric {Efficiency} in {Multivariate} {Regression} {Models} with {Missing} {Data}},
	volume = {90},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1995.10476494},
	doi = {10.1080/01621459.1995.10476494},
	language = {en},
	number = {429},
	urldate = {2023-04-04},
	journal = {Journal of the American Statistical Association},
	author = {Robins, James M. and Rotnitzky, Andrea},
	month = mar,
	year = {1995},
	pages = {122--129},
}

@book{lehmann2022testing,
	address = {Cham},
	series = {Springer {Texts} in {Statistics}},
	title = {Testing {Statistical} {Hypotheses}},
	isbn = {978-3-030-70577-0 978-3-030-70578-7},
	url = {https://link.springer.com/10.1007/978-3-030-70578-7},
	language = {en},
	urldate = {2023-04-13},
	publisher = {Springer International Publishing},
	author = {Lehmann, E.L. and Romano, Joseph P.},
	year = {2022},
	doi = {10.1007/978-3-030-70578-7},
	keywords = {best fit, Excel, Resampling, Statistical Hypotheses, Statistical Theory},
}

@article{newey1990semiparametric,
	title = {Semiparametric {Efficiency} {Bounds}},
	volume = {5},
	issn = {0883-7252},
	url = {https://www.jstor.org/stable/2096601},
	abstract = {Semiparametric models are those where the functional form of some components is unknown. Efficiency bounds are of fundamental importance for such models. They provide a guide to estimation methods and give an asymptotic efficiency standard. The purpose of this paper is to provide an introduction to research methods and problems for semiparametric efficiency bounds. The nature of the bounds is discussed, as well as ways of calculating them. Their uses in solving estimation problems are outlined, including construction of semiparametric estimators and calculation of their limiting distribution. The paper includes new results as well as survey material.},
	number = {2},
	urldate = {2023-04-14},
	journal = {Journal of Applied Econometrics},
	author = {Newey, Whitney K.},
	year = {1990},
	note = {Publisher: Wiley},
	pages = {99--135},
	file = {Full Text:/Users/yuehaob/Zotero/storage/T4REXG77/Newey - 1990 - Semiparametric Efficiency Bounds.pdf:application/pdf},
}

@book{tsiatis2006semiparametric,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Semiparametric {Theory} and {Missing} {Data}},
	isbn = {978-0-387-32448-7},
	url = {http://link.springer.com/10.1007/0-387-37345-4},
	language = {en},
	urldate = {2023-04-15},
	publisher = {Springer},
	author = {Tsiatis, Anastasios A.},
	year = {2006},
	doi = {10.1007/0-387-37345-4},
	keywords = {average, estimator, likelihood, probability, semiparametric},
}

@article{firpo2007efficient,
	title = {Efficient {Semiparametric} {Estimation} of {Quantile} {Treatment} {Effects}},
	volume = {75},
	issn = {0012-9682},
	url = {https://www.jstor.org/stable/4123114},
	abstract = {This paper develops estimators for quantile treatment effects under the identifying restriction that selection to treatment is based on observable characteristics. Identification is achieved without requiring computation of the conditional quantiles of the potential outcomes. Instead, the identification results for the marginal quantiles lead to an estimation procedure for the quantile treatment effect parameters that has two steps: nonparametric estimation of the propensity score and computation of the difference between the solutions of two separate minimization problems. Root-N consistency, asymptotic normality, and achievement of the semiparametric efficiency bound are shown for that estimator. A consistent estimation procedure for the variance is also presented. Finally, the method developed here is applied to evaluation of a job training program and to a Monte Carlo exercise. Results from the empirical application indicate that the method works relatively well even for a data set with limited overlap between treated and controls in the support of covariates. The Monte Carlo study shows that, for a relatively small sample size, the method produces estimates with good precision and low bias, especially for middle quantiles.},
	number = {1},
	urldate = {2023-04-18},
	journal = {Econometrica},
	author = {Firpo, Sergio},
	year = {2007},
	note = {Publisher: [Wiley, Econometric Society]},
	pages = {259--276},
	file = {Full Text:/Users/yuehaob/Zotero/storage/BR68LUY5/Firpo - 2007 - Efficient Semiparametric Estimation of Quantile Tr.pdf:application/pdf},
}

@article{van_der_vaart1994bracketing,
	title = {Bracketing smooth functions},
	volume = {52},
	issn = {0304-4149},
	url = {https://www.sciencedirect.com/science/article/pii/0304414994901023},
	doi = {10.1016/0304-4149(94)90102-3},
	abstract = {Let F be the class of functions f :Rd→R which are α times differentiable with derivatives bounded by numbers Mj on each given set Ij in a partition of Rd = ⋃jIj. We obtain upper and lower bounds on the Lr(P)-bracketing entropy of F.},
	language = {en},
	number = {1},
	urldate = {2023-05-02},
	journal = {Stochastic Processes and their Applications},
	author = {van der Vaart, Aad},
	month = aug,
	year = {1994},
	keywords = {Bracketing number, Covering number, Donsker class, Empirical central limit theorem, Entropy, Glivenko-Cantelli class},
	pages = {93--105},
	file = {1-s2.0-0304414994901023-main.pdf:/Users/yuehaob/Zotero/storage/M6MTHI68/1-s2.0-0304414994901023-main.pdf:application/pdf},
}

@book{krantz2013implicit,
	address = {New York, NY},
	title = {The {Implicit} {Function} {Theorem}: {History}, {Theory}, and {Applications}},
	isbn = {978-1-4614-5980-4 978-1-4614-5981-1},
	shorttitle = {The {Implicit} {Function} {Theorem}},
	url = {https://link.springer.com/10.1007/978-1-4614-5981-1},
	language = {en},
	urldate = {2023-05-18},
	publisher = {Springer},
	author = {Krantz, Steven G. and Parks, Harold R.},
	year = {2013},
	doi = {10.1007/978-1-4614-5981-1},
	keywords = {Implicit Function Theorem, Inverse Function Theorem, Numerical Homotopy Methods, ordinary differential equations, partial differential equations, Real Analysis, Smooth Functions},
}


@article{abadie2003semiparametric,
	title = {Semiparametric instrumental variable estimation of treatment response models},
	volume = {113},
	issn = {0304-4076},
	url = {https://www.sciencedirect.com/science/article/pii/S0304407602002014},
	doi = {10.1016/S0304-4076(02)00201-4},
	abstract = {This article introduces a new class of instrumental variable (IV) estimators for linear and nonlinear treatment response models with covariates. The rationale for focusing on nonlinear models is that, if the dependent variable is binary or limited, or if the effect of the treatment varies with covariates, a nonlinear model is appropriate. In the spirit of Roehrig (Econometrica 56 (1988) 433), identification is attained nonparametrically and does not depend on the choice of the parametric specification for the response function of interest. One virtue of this approach is that it allows the researcher to construct estimators that can be interpreted as the parameters of a well-defined approximation to a treatment response function under functional form misspecification. In contrast to some usual IV models, heterogeneity of treatment effects is not restricted by the identification conditions. The ideas and estimators in this article are illustrated using IV to estimate the effects of 401(k) retirement programs on savings.},
	language = {en},
	number = {2},
	urldate = {2021-02-13},
	journal = {Journal of Econometrics},
	author = {Abadie, Alberto},
	month = apr,
	year = {2003},
	pages = {231--263},
}

@misc{armstrong2022asymptotic,
	title = {Asymptotic {Efficiency} {Bounds} for a {Class} of {Experimental} {Designs}},
	url = {http://arxiv.org/abs/2205.02726},
	doi = {10.48550/arXiv.2205.02726},
	abstract = {We consider an experimental design setting in which units are assigned to treatment after being sampled sequentially from an infinite population. We derive asymptotic efficiency bounds that apply to data from any experiment that assigns treatment as a (possibly randomized) function of covariates and past outcome data, including stratification on covariates and adaptive designs. For estimating the average treatment effect of a binary treatment, our results show that no further first order asymptotic efficiency improvement is possible relative to an estimator that achieves the Hahn (1998) bound in an experimental design where the propensity score is chosen to minimize this bound. Our results also apply to settings with multiple treatments with possible constraints on treatment, as well as covariate based sampling of a single outcome.},
	urldate = {2023-01-27},
	publisher = {arXiv},
	author = {Armstrong, Timothy B.},
	month = may,
	year = {2022},
	note = {arXiv:2205.02726 [stat]},
	keywords = {Statistics - Methodology},
}


@misc{jiang2022regression-adjusted,
	title = {Regression-{Adjusted} {Estimation} of {Quantile} {Treatment} {Effects} under {Covariate}-{Adaptive} {Randomizations}},
	url = {http://arxiv.org/abs/2105.14752},
	doi = {10.48550/arXiv.2105.14752},
	abstract = {Datasets from field experiments with covariate-adaptive randomizations (CARs) usually contain extra covariates in addition to the strata indicators. We propose to incorporate these additional covariates via auxiliary regressions in the estimation and inference of unconditional quantile treatment effects (QTEs) under CARs. We establish the consistency and limit distribution of the regression-adjusted QTE estimator and prove that the use of multiplier bootstrap inference is non-conservative under CARs. The auxiliary regression may be estimated parametrically, nonparametrically, or via regularization when the data are high-dimensional. Even when the auxiliary regression is misspecified, the proposed bootstrap inferential procedure still achieves the nominal rejection probability in the limit under the null. When the auxiliary regression is correctly specified, the regression-adjusted estimator achieves the minimum asymptotic variance. We also discuss forms of adjustments that can improve the efficiency of the QTE estimators. The finite sample performance of the new estimation and inferential methods is studied in simulations and an empirical application to a well-known dataset concerned with expanding access to basic bank accounts on savings is reported.},
	urldate = {2023-06-13},
	publisher = {arXiv},
	author = {Jiang, Liang and Phillips, Peter C. B. and Tao, Yubo and Zhang, Yichong},
	month = sep,
	year = {2022},
	note = {arXiv:2105.14752 [econ, stat]},
	keywords = {Economics - Econometrics, Statistics - Methodology},
}

@misc{jiang2022improving,
	title = {Improving {Estimation} {Efficiency} via {Regression}-{Adjustment} in {Covariate}-{Adaptive} {Randomizations} with {Imperfect} {Compliance}},
	url = {http://arxiv.org/abs/2201.13004},
	doi = {10.48550/arXiv.2201.13004},
	abstract = {We study how to improve efficiency via regression adjustments with additional covariates under covariate-adaptive randomizations (CARs) when subject compliance is imperfect. We first establish the semiparametric efficiency bound for the local average treatment effect (LATE) under CARs. Second, we develop a general regression-adjusted LATE estimator which allows for parametric, nonparametric, and regularized adjustments. Even when the adjustments are misspecified, our proposed estimator is still consistent and asymptotically normal, and their inference method still achieves the exact asymptotic size under the null. When the adjustments are correctly specified, our estimator achieves the semiparametric efficiency bound. Third, we derive the optimal linear adjustment that leads to the smallest asymptotic variance among all linear adjustments. We then show the commonly used two stage least squares estimator is not optimal in the class of LATE estimators with linear adjustments while Ansel, Hong, and Li's (2018) estimator is. Fourth, we show how to construct a LATE estimator with nonlinear adjustments which is more efficient than those with the optimal linear adjustment. Fifth, we give conditions under which LATE estimators with nonparametric and regularized adjustments achieve the semiparametric efficiency bound. Last, simulation evidence and empirical application confirm efficiency gains achieved by regression adjustments relative to both the estimator without adjustment and the standard two-stage least squares estimator.},
	urldate = {2023-06-13},
	publisher = {arXiv},
	author = {Jiang, Liang and Linton, Oliver B. and Tang, Haihan and Zhang, Yichong},
	month = sep,
	year = {2022},
	note = {arXiv:2201.13004 [econ, stat]},
	keywords = {Economics - Econometrics, Statistics - Methodology},
}


@article{imbens1994identification,
	title = {Identification and {Estimation} of {Local} {Average} {Treatment} {Effects}},
	volume = {62},
	issn = {0012-9682},
	url = {https://www.jstor.org/stable/2951620},
	doi = {10.2307/2951620},
	number = {2},
	urldate = {2020-12-03},
	journal = {Econometrica},
	author = {Imbens, Guido W. and Angrist, Joshua D.},
	year = {1994},
	note = {Publisher: [Wiley, Econometric Society]},
	pages = {467--475},
}

@article{zhang2008improving,
  title={Improving efficiency of inferences in randomized clinical trials using auxiliary covariates},
  author={Zhang, Min and Tsiatis, Anastasios A and Davidian, Marie},
  journal={Biometrics},
  volume={64},
  number={3},
  pages={707--715},
  year={2008},
  publisher={Wiley Online Library}
}

@article{rafi2023efficient,
  title={Efficient Semiparametric Estimation of Average Treatment Effects Under Covariate Adaptive Randomization},
  author={Rafi, Ahnaf},
  journal={arXiv preprint arXiv:2305.08340},
  year={2023}
}

@article{jiang2021bootstrap,
  title={Bootstrap inference for quantile treatment effects in randomized experiments with matched pairs},
  author={Jiang, Liang and Liu, Xiaobin and Phillips, Peter CB and Zhang, Yichong},
  journal={Review of Economics and Statistics},
  pages={1--47},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{negi2021revisiting,
  title={Revisiting regression adjustment in experiments with heterogeneous treatment effects},
  author={Negi, Akanksha and Wooldridge, Jeffrey M},
  journal={Econometric Reviews},
  volume={40},
  number={5},
  pages={504--534},
  year={2021},
  publisher={Taylor \& Francis}
}


@article{lin2013agnostic,
	title = {Agnostic notes on regression adjustments to experimental data: {Reexamining} {Freedman}’s critique},
	volume = {7},
	issn = {1932-6157, 1941-7330},
	shorttitle = {Agnostic notes on regression adjustments to experimental data},
	url = {https://projecteuclid.org/euclid.aoas/1365527200},
	doi = {10.1214/12-AOAS583},
	abstract = {Freedman [Adv. in Appl. Math. 40 (2008) 180–193; Ann. Appl. Stat. 2 (2008) 176–196] critiqued ordinary least squares regression adjustment of estimated treatment effects in randomized experiments, using Neyman’s model for randomization inference. Contrary to conventional wisdom, he argued that adjustment can lead to worsened asymptotic precision, invalid measures of precision, and small-sample bias. This paper shows that in sufficiently large samples, those problems are either minor or easily fixed. OLS adjustment cannot hurt asymptotic precision when a full set of treatment–covariate interactions is included. Asymptotically valid confidence intervals can be constructed with the Huber–White sandwich standard error estimator. Checks on the asymptotic approximations are illustrated with data from Angrist, Lang, and Oreopoulos’s [Am. Econ. J.: Appl. Econ. 1:1 (2009) 136–163] evaluation of strategies to improve college students’ achievement. The strongest reasons to support Freedman’s preference for unadjusted estimates are transparency and the dangers of specification search.},
	language = {EN},
	number = {1},
	urldate = {2020-12-03},
	journal = {Annals of Applied Statistics},
	author = {Lin, Winston},
	month = mar,
	year = {2013},
	mrnumber = {MR3086420},
	zmnumber = {06171273},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {295--318},
}


@article{zhao2023covariate,
  title={Covariate adjustment in multiarmed, possibly factorial experiments},
  author={Zhao, Anqi and Ding, Peng},
  journal={Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume={85},
  number={1},
  pages={1--23},
  year={2023},
  publisher={Oxford University Press US}
}

@article{su2021model,
  title={Model-assisted analyses of cluster-randomized experiments},
  author={Su, Fangzhou and Ding, Peng},
  journal={arXiv preprint arXiv:2104.04647},
  year={2021}
}


@book{bickel1998efficient,
	title = {Efficient and {Adaptive} {Estimation} for {Semiparametric} {Models}},
	isbn = {978-0-387-98473-5},
	abstract = {This book deals with estimation in situations in which there is believed to be enough information to model parametrically some, but not all of the features of a data set. Such models have arisen in a wide context in recent years, and involve new nonlinear estimation procedures. Statistical models of this type are directly applicable to fields such as economics, epidemiology, and astronomy.},
	language = {en},
	publisher = {Springer New York},
	author = {Bickel, Peter J. and Klaassen, Chris A. J. and Ritov, Ya'acov and Wellner, Jon A.},
	month = jun,
	year = {1998},
	note = {Google-Books-ID: lSnTm6SC\_SMC},
	keywords = {Mathematics / Applied, Mathematics / General, Mathematics / Probability \& Statistics / General},
}


@article{chen2018overidentification,
	title = {Overidentification in {Regular} {Models}},
	volume = {86},
	issn = {0012-9682},
	url = {https://www.jstor.org/stable/44955258},
	abstract = {In the unconditional moment restriction model of Hansen (1982), specification tests and more efficient estimators are both available whenever the number of moment restrictions exceeds the number of parameters of interest. We show that a similar relationship between potential refutability of a model and existence of more efficient estimators is present in much broader settings. Specifically, a condition we name local overidentification is shown to be equivalent to both the existence of specification tests with nontrivial local power and the existence of more efficient estimators of some "smooth" parameters in general semi/nonparametric models. Under our notion of local overidentification, various locally nontrivial specification tests such as Hausman tests, incremental Sargan tests (or optimally weighted quasi likelihood ratio tests) naturally extend to general semi/nonparametric settings. We further obtain simple characterizations of local overidentification for general models of nonparametric conditional moment restrictions with possibly different conditioning sets. The results are applied to determining when semi/nonparametric models with endogeneity are locally testable, and when nonparametric plug-in and semiparametric two-step GMM estimators are semiparametrically efficient. Examples of empirically relevant semi/nonparametric structural models are presented.},
	number = {5},
	urldate = {2023-04-14},
	journal = {Econometrica},
	author = {Chen, Xiaohong and Santos, Andres},
	year = {2018},
	note = {Publisher: [Wiley, The Econometric Society]},
	pages = {1771--1817},
	file = {Full Text:/Users/yuehaob/Zotero/storage/DPAMSKYZ/Econometrica - 2018 - Chen - Overidentification in Regular Models.pdf:application/pdf;Supplement:/Users/yuehaob/Zotero/storage/B7PEEK48/overid_appendix.pdf:application/pdf},
}


@article{cytrynbaum2023designing,
  title={Designing representative and balanced experiments by local randomization},
  author={Cytrynbaum, Max},
  journal={arXiv preprint arXiv:2111.08157},
  year={2023}
}


@article{abadie2008estimation,
	title = {Estimation of the {Conditional} {Variance} in {Paired} {Experiments}},
    volume = {1},
	issn = {0769-489X},
	doi = {10.2307/27917244},
	abstract = {In paired randomized experiments units are grouped in pairs, often based on covariate information, with random assignment within the pairs. Average treatment effects are then estimated by averaging the within-pair differences in outcomes. Typically the variance of the average treatment effect estimator is estimated using the sample variance of the within-pair differences. However, conditional on the covariates the variance of the average treatment effect estimator may be substantially smaller. Here we propose a simple way of estimating the conditional variance of the average treatment effect estimator by forming pairs-of-pairs with similar covariate values and estimating the variances within these pairs-of-pairs. Even though these within-pairs-of-pairs variance estimators are not consistent, their average is consistent for the conditional variance of the average treatment effect estimator and leads to asymptotically valid confidence intervals. Dans les expériences aléatoires d'appariement les unités sont regroupées par paires, souvent basées sur des caractéristiques explicatives, et avec appariement aléatoire. Les effets de traitement moyens sont alors estimés en faisant la moyenne des différences intra-paires dans les résultats. Typiquement, la variance de l'estimateur de l'effet de traitement moyen est estimée en utilisant la variance des différences intra-paires dans l'échantillon. Cependant, conditionnellement aux variables explicatives, l'estimateur de l'effet de traitement moyen peut être substantiellement plus petit. Nous proposons ici une manière simple d'estimer la variance conditionnelle de l'estimateur de l'effet de traitement moyen en formant des paires de paires avec des valeurs de variables explicatives similaires et en estimant les variances entre ces paires de paires. Même si ces estimateurs fondés sur les paires de paires ne sont pas convergents, leur moyenne est convergente pour la variance conditionnelle de l'estimateur de l'effet de traitement moyen et conduit à des intervalles de confiance asymptotiquement valides.},
	number = {91/92},
	urldate = {2020-06-15},
	journal = {Annales d'Économie et de Statistique},
	author = {Abadie, Alberto and Imbens, Guido W.},
	year = {2008},
	pages = {175--187},
}


@article{chen2008semiparametric,
	title = {Semiparametric {Efficiency} in {GMM} {Models} with {Auxiliary} {Data}},
	volume = {36},
	issn = {0090-5364},
	url = {https://www.jstor.org/stable/25464647},
	abstract = {We study semiparametric efficiency bounds and efficient estimation of parameters defined through general moment restrictions with missing data. Identification relies on auxiliary data containing information about the distribution of the missing variables conditional on proxy variables that are observed in both the primary and the auxiliary database, when such distribution is common to the two data sets. The auxiliary sample can be independent of the primary sample, or can be a subset of it. For both cases, we derive bounds when the probability of missing data given the proxy variables is unknown, or known, or belongs to a correctly specified parametric family. We find that the conditional probability is not ancillary when the two samples are independent. For all cases, we discuss efficient semiparametric estimators. An estimator based on a conditional expectation projection is shown to require milder regularity conditions than one based on inverse probability weighting.},
	number = {2},
	urldate = {2023-06-29},
	journal = {The Annals of Statistics},
	author = {Chen, Xiaohong and Hong, Han and Tarozzi, Alessandro},
	year = {2008},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {808--843},
}


@article{tabord-meehan2022stratification,
	title = {Stratification {Trees} for {Adaptive} {Randomization} in {Randomized} {Controlled} {Trials}},
	urldate = {2022-07-28},
	journal = {The Review of Economic Studies},
	author = {Tabord-Meehan, Max},
	year = {2022},
	note = {Forthcoming},
}

@article{dizon2019parents,
  title={Parents' beliefs about their children's academic ability: Implications for educational investments},
  author={Dizon-Ross, Rebecca},
  journal={American Economic Review},
  volume={109},
  number={8},
  pages={2728--65},
  year={2019}
}

@article{duflo2015education,
  title={Education, HIV, and early fertility: Experimental evidence from Kenya},
  author={Duflo, Esther and Dupas, Pascaline and Kremer, Michael},
  journal={American Economic Review},
  volume={105},
  number={9},
  pages={2757--2797},
  year={2015},
  publisher={American Economic Association}
}


@article{angrist2009effects,
	title = {The {Effects} of {High} {Stakes} {High} {School} {Achievement} {Awards}: {Evidence} from a {Randomized} {Trial}},
	volume = {99},
	issn = {0002-8282},
	shorttitle = {The {Effects} of {High} {Stakes} {High} {School} {Achievement} {Awards}},
	url = {https://www.aeaweb.org/articles?id=10.1257/aer.99.4.1384},
	doi = {10.1257/aer.99.4.1384},
	abstract = {The Israeli matriculation certificate is a prerequisite for most postsecondary
schooling. In a randomized trial, we attempted to increase certification rates
among low-achievers with cash incentives. The experiment used a school-based
randomization design offering awards to all who passed their exams in treated
schools. This led to a substantial increase in certification rates for girls but had
no effect on boys. Affected girls had a relatively high ex ante chance of certification.
The increase in girls' matriculation rates translated into an increased
likelihood of college attendance. Female matriculation rates increased partly
because treated girls devoted extra time to exam preparation. (JEL I21, I28,
J16)},
	language = {en},
	number = {4},
	urldate = {2020-12-03},
	journal = {American Economic Review},
	author = {Angrist, Joshua and Lavy, Victor},
	month = sep,
	year = {2009},
	pages = {1384--1414},
}

@article{banerjee2015miracle,
	title = {The {Miracle} of {Microfinance}? {Evidence} from a {Randomized} {Evaluation}},
	volume = {7},
	issn = {1945-7782},
	shorttitle = {The {Miracle} of {Microfinance}?},
	url = {https://www.aeaweb.org/articles?id=10.1257/app.20130533},
	doi = {10.1257/app.20130533},
	abstract = {This paper reports results from the randomized evaluation of a group-lending microcredit program in Hyderabad, India. A lender worked in 52 randomly selected neighborhoods, leading to an 8.4 percentage point increase in takeup of microcredit. Small business investment and profits of preexisting businesses increased, but consumption did not significantly increase. Durable goods expenditure increased, while "temptation goods" expenditure declined. We found no significant changes in health, education, or women's empowerment. Two years later, after control areas had gained access to microcredit but households in treatment area had borrowed for longer and in larger amounts, very few significant differences persist. (JEL G21, G31, O16, O12, L25, I38)},
	language = {en},
	number = {1},
	urldate = {2020-12-03},
	journal = {American Economic Journal: Applied Economics},
	author = {Banerjee, Abhijit and Duflo, Esther and Glennerster, Rachel and Kinnan, Cynthia},
	month = jan,
	year = {2015},
	pages = {22--53},
}

@article{bruhn2016impact,
	title = {The {Impact} of {High} {School} {Financial} {Education}: {Evidence} from a {Large}-{Scale} {Evaluation} in {Brazil}},
	volume = {8},
	issn = {1945-7782},
	shorttitle = {The {Impact} of {High} {School} {Financial} {Education}},
	url = {https://www.aeaweb.org/articles?id=10.1257/app.20150149},
	doi = {10.1257/app.20150149},
	abstract = {We study the impact of a comprehensive high school financial education program spanning 6 states, 892 schools, and approximately 25,000 students in Brazil through a randomized control trial. The program increased student financial proficiency by a quarter of a standard deviation and raised grade-level passing rates. Short-term financial behaviors, however, show mixed results with significant improvements in students' savings and budgeting as well as positive spillovers to parents, but also an increase in students' use of expensive credit to make consumer purchases.},
	language = {en},
	number = {4},
	urldate = {2020-12-03},
	journal = {American Economic Journal: Applied Economics},
	author = {Bruhn, Miriam and Leão, Luciana de Souza and Legovini, Arianna and Marchetti, Rogelio and Zia, Bilal},
	month = oct,
	year = {2016},
	keywords = {Corporate Finance and Governance, Household Saving, Personal Finance, Analysis of Education, Microeconomic Analyses of Economic Development, Economic Development: Financial Markets, Saving and Capital Investment},
	pages = {256--295},
}


@article{chernozhukov2018doubledebiased,
	title = {Double/debiased machine learning for treatment and structural parameters},
	volume = {21},
	copyright = {© 2017 Royal Economic Society.},
	issn = {1368-423X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ectj.12097},
	doi = {10.1111/ectj.12097},
	abstract = {We revisit the classic semi-parametric problem of inference on a low-dimensional parameter θ0 in the presence of high-dimensional nuisance parameters η0. We depart from the classical setting by allowing for η0 to be so high-dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate η0, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high-dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating η0 cause a heavy bias in estimators of θ0 that are obtained by naively plugging ML estimators of η0 into estimating equations for θ0. This bias results in the naive estimator failing to be consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest θ0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman-orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate θ0; (2) making use of cross-fitting, which provides an efficient form of data-splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an -neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.},
	language = {en},
	number = {1},
	urldate = {2023-07-03},
	journal = {The Econometrics Journal},
	author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ectj.12097},
	pages = {C1--C68},
}


@article{casaburi2022using,
	title = {Using {Individual}-{Level} {Randomized} {Treatment} to {Learn} about {Market} {Structure}},
	volume = {14},
	issn = {1945-7782},
	url = {https://www.aeaweb.org/articles?id=10.1257/app.20200306},
	doi = {10.1257/app.20200306},
	abstract = {Interference across competing firms in RCTs can be informative about market structure. An experiment that subsidizes a random subset of traders who buy cocoa from farmers in Sierra Leone illustrates this idea. Interpreting treatment-control differences in prices and quantities purchased from farmers through a model of Cournot competition reveals differentiation between traders is low. Combining this result with quasi-experimental variation in world prices shows that the number of traders competing is 50 percent higher than the number operating in a village. Own-price and cross-price supply elasticities are high. Farmers face a competitive market in this first stage of the value chain.},
	language = {en},
	number = {4},
	urldate = {2023-07-03},
	journal = {American Economic Journal: Applied Economics},
	author = {Casaburi, Lorenzo and Reed, Tristan},
	month = oct,
	year = {2022},
	keywords = {Agribusiness, Contracts and Reputation, Cooperatives, Energy, Environment, Natural Resources, Networks, Economic Development: Agriculture, Oligopoly and Other Imperfect Markets, Transactional Relationships, Other Primary Products, Micro Analysis of Farm Firms, Farm Households, and Farm Input Markets, Agricultural Markets and Marketing},
	pages = {58--90},
}


@article{farrell2015robust,
  title={Robust inference on average treatment effects with possibly more covariates than observations},
  author={Farrell, Max H},
  journal={Journal of Econometrics},
  volume={189},
  number={1},
  pages={1--23},
  year={2015},
  publisher={Elsevier}
}


@article{belloni2017program,
  title={Program evaluation and causal inference with high-dimensional data},
  author={Belloni, Alexandre and Chernozhukov, Victor and Fernandez-Val, Ivan and Hansen, Christian},
  journal={Econometrica},
  volume={85},
  number={1},
  pages={233--298},
  year={2017},
  publisher={Wiley Online Library}
}

@article{bruhn2009pursuit,
  title={In pursuit of balance: Randomization in practice in development field experiments},
  author={Bruhn, Miriam and McKenzie, David},
  journal={American economic journal: applied economics},
  volume={1},
  number={4},
  pages={200--232},
  year={2009},
  publisher={American Economic Association}
}


@misc{cytrynbaum2023covariate,
	title = {Covariate {Adjustment} in {Stratified} {Experiments}},
	url = {http://arxiv.org/abs/2302.03687},
	doi = {10.48550/arXiv.2302.03687},
	abstract = {This paper studies covariate adjusted estimation of the average treatment effect in stratified experiments. We work in a general framework that includes matched tuples designs, coarse stratification, and complete randomization as special cases. Regression adjustment with treatment-covariate interactions is known to weakly improve efficiency for completely randomized designs. By contrast, we show that for stratified designs such regression estimators are generically inefficient, potentially even increasing estimator variance relative to the unadjusted benchmark. Motivated by this result, we derive the asymptotically optimal linear covariate adjustment for a given stratification. We construct several feasible estimators that implement this efficient adjustment in large samples. In the special case of matched pairs, for example, the regression including treatment, covariates, and pair fixed effects is asymptotically optimal. We also provide novel asymptotically exact inference methods that allow researchers to report smaller confidence intervals, fully reflecting the efficiency gains from both stratification and adjustment. Simulations and an empirical application demonstrate the value of our proposed methods.},
	urldate = {2024-02-08},
	publisher = {arXiv},
	author = {Cytrynbaum, Max},
	month = sep,
	year = {2023},
	note = {arXiv:2302.03687 [econ, stat]},
}

@article{neyman1934two,
  title={On the Two Different Aspects of the Representative Method: The Method of Stratified Sampling and the Method of Purposive Selection},
  author={Neyman, Jerzy},
  journal={Journal of the Royal Statistical Society Series A: Statistics in Society},
  volume={97},
  number={4},
  pages={558--606},
  year={1934},
  publisher={Oxford University Press}
}

@article{heckman1998matching,
  title={Matching as an econometric evaluation estimator},
  author={Heckman, James J and Ichimura, Hidehiko and Todd, Petra},
  journal={The review of economic studies},
  volume={65},
  number={2},
  pages={261--294},
  year={1998},
  publisher={Wiley-Blackwell}
}

@article{freedman2008regression,
  title={On regression adjustments to experimental data},
  author={Freedman, David A},
  journal={Advances in Applied Mathematics},
  volume={40},
  number={2},
  pages={180--193},
  year={2008},
  publisher={Elsevier}
}

@misc{chernozhukov2024long,
	title = {Long {Story} {Short}: {Omitted} {Variable} {Bias} in {Causal} {Machine} {Learning}},
	shorttitle = {Long {Story} {Short}},
	url = {http://arxiv.org/abs/2112.13398},
	doi = {10.48550/arXiv.2112.13398},
	abstract = {We develop a general theory of omitted variable bias for a wide range of common causal parameters, including (but not limited to) averages of potential outcomes, average treatment effects, average causal derivatives, and policy effects from covariate shifts. Our theory applies to nonparametric models, while naturally allowing for (semi-)parametric restrictions (such as partial linearity) when such assumptions are made. We show how simple plausibility judgments on the maximum explanatory power of omitted variables are sufficient to bound the magnitude of the bias, thus facilitating sensitivity analysis in otherwise complex, nonlinear models. Finally, we provide flexible and efficient statistical inference methods for the bounds, which can leverage modern machine learning algorithms for estimation. These results allow empirical researchers to perform sensitivity analyses in a flexible class of machine-learned causal models using very simple, and interpretable, tools. We demonstrate the utility of our approach with two empirical examples.},
	urldate = {2024-08-14},
	publisher = {arXiv},
	author = {Chernozhukov, Victor and Cinelli, Carlos and Newey, Whitney and Sharma, Amit and Syrgkanis, Vasilis},
	month = may,
	year = {2024},
	note = {arXiv:2112.13398 [cs, econ, stat]},
	keywords = {62G, Computer Science - Machine Learning, Economics - Econometrics, Statistics - Machine Learning, Statistics - Methodology},
}


@article{bai2024inference,
	title = {Inference for {Matched} {Tuples} and {Fully} {Blocked} {Factorial} {Designs}},
	volume = {15},
	abstract = {This paper studies inference in randomized controlled trials with multiple treatments, where treatment status is determined according to a "matched tuples" design. Here, by a matched tuples design, we mean an experimental design where units are sampled i.i.d. from the population of interest, grouped into "homogeneous" blocks with cardinality equal to the number of treatments, and finally, within each block, each treatment is assigned exactly once uniformly at random. We first study estimation and inference for matched tuples designs in the general setting where the parameter of interest is a vector of linear contrasts over the collection of average potential outcomes for each treatment. Parameters of this form include standard average treatment effects used to compare one treatment relative to another, but also include parameters which may be of interest in the analysis of factorial designs. We first establish conditions under which a sample analogue estimator is asymptotically normal and construct a consistent estimator of its corresponding asymptotic variance. Combining these results establishes the asymptotic exactness of tests based on these estimators. In contrast, we show that, for two common testing procedures based on t-tests constructed from linear regressions, one test is generally conservative while the other generally invalid. We go on to apply our results to study the asymptotic properties of what we call "fully-blocked" 2{\textasciicircum}K factorial designs, which are simply matched tuples designs applied to a full factorial experiment. Leveraging our previous results, we establish that our estimator achieves a lower asymptotic variance under the fully-blocked design than that under any stratified factorial design which stratifies the experimental sample into a finite number of "large" strata. A simulation study and empirical application illustrate the practical relevance of our results.},
	number = {2},
	journal = {Quantitative Economics},
	author = {Bai, Yuehao and Liu, Jizhou and Tabord-Meehan, Max},
	year = {2024},
	keywords = {Economics - Econometrics, Mathematics - Statistics Theory, Statistics - Methodology},
	pages = {279--330},
}

@article{bai2024covariate,
	title = {Covariate adjustment in experiments with matched pairs},
	volume = {241},
	issn = {0304-4076},
	url = {https://www.sciencedirect.com/science/article/pii/S0304407624000861},
	doi = {10.1016/j.jeconom.2024.105740},
	abstract = {This paper studies inference for the average treatment effect (ATE) in experiments in which treatment status is determined according to “matched pairs” and it is additionally desired to adjust for observed, baseline covariates to gain further precision. By a “matched pairs” design, we mean that units are sampled i.i.d. from the population of interest, paired according to observed, baseline covariates, and finally, within each pair, one unit is selected at random for treatment. Importantly, we presume that not all observed, baseline covariates are used in determining treatment assignment. We study a broad class of estimators based on a “doubly robust” moment condition that permits us to study estimators with both finite-dimensional and high-dimensional forms of covariate adjustment. We find that estimators with finite-dimensional, linear adjustments need not lead to improvements in precision relative to the unadjusted difference-in-means estimator. This phenomenon persists even if the adjustments interact with treatment; in fact, doing so leads to no changes in precision. However, gains in precision can be ensured by including fixed effects for each of the pairs. Indeed, we show that this adjustment leads to the minimum asymptotic variance of the corresponding ATE estimator among all finite-dimensional, linear adjustments. We additionally study an estimator with a regularized adjustment, which can accommodate high-dimensional covariates. We show that this estimator leads to improvements in precision relative to the unadjusted difference-in-means estimator and also provides conditions under which it leads to the “optimal” nonparametric, covariate adjustment. A simulation study confirms the practical relevance of our theoretical analysis, and the methods are employed to reanalyze data from an experiment using a “matched pairs” design to study the effect of macroinsurance on microenterprise.},
	number = {1},
	urldate = {2024-05-15},
	journal = {Journal of Econometrics},
	author = {Bai, Yuehao and Jiang, Liang and Romano, Joseph P. and Shaikh, Azeem M. and Zhang, Yichong},
	month = apr,
	year = {2024},
	keywords = {Economics - Econometrics, Randomized controlled trial, Experiment, Covariate adjustment, LASSO, Matched pairs, Treatment assignment},
	pages = {105740},
}

@misc{bai2024inference-1,
	title = {Inference in {Cluster} {Randomized} {Trials} with {Matched} {Pairs}},
	url = {http://arxiv.org/abs/2211.14903},
	doi = {10.48550/arXiv.2211.14903},
	abstract = {This paper studies inference in cluster randomized trials where treatment status is determined according to a "matched pairs" design. Here, by a cluster randomized experiment, we mean one in which treatment is assigned at the level of the cluster; by a "matched pairs" design, we mean that a sample of clusters is paired according to baseline, cluster-level covariates and, within each pair, one cluster is selected at random for treatment. We study the large-sample behavior of a weighted difference-in-means estimator and derive two distinct sets of results depending on if the matching procedure does or does not match on cluster size. We then propose a single variance estimator which is consistent in either regime. Combining these results establishes the asymptotic exactness of tests based on these estimators. Next, we consider the properties of two common testing procedures based on t-tests constructed from linear regressions, and argue that both are generally conservative in our framework. We additionally study the behavior of a randomization test which permutes the treatment status for clusters within pairs, and establish its finite-sample and asymptotic validity for testing specific null hypotheses. Finally, we propose a covariate-adjusted estimator which adjusts for additional baseline covariates not used for treatment assignment, and establish conditions under which such an estimator leads to strict improvements in precision. A simulation study confirms the practical relevance of our theoretical results.},
	urldate = {2024-08-19},
	publisher = {arXiv},
	author = {Bai, Yuehao and Liu, Jizhou and Shaikh, Azeem M. and Tabord-Meehan, Max},
	month = aug,
	year = {2024},
	note = {arXiv:2211.14903 [econ, stat]},
	keywords = {Economics - Econometrics, Statistics - Methodology},
}

@misc{bai2024inference-2,
	title = {Inference in {Experiments} with {Matched} {Pairs} and {Imperfect} {Compliance}},
	url = {http://arxiv.org/abs/2307.13094},
	doi = {10.48550/arXiv.2307.13094},
	abstract = {This paper studies inference for the local average treatment effect in randomized controlled trials with imperfect compliance where treatment status is determined according to "matched pairs." By "matched pairs," we mean that units are sampled i.i.d. from the population of interest, paired according to observed, baseline covariates and finally, within each pair, one unit is selected at random for treatment. Under weak assumptions governing the quality of the pairings, we first derive the limit distribution of the usual Wald (i.e., two-stage least squares) estimator of the local average treatment effect. We show further that conventional heteroskedasticity-robust estimators of the Wald estimator's limiting variance are generally conservative, in that their probability limits are (typically strictly) larger than the limiting variance. We therefore provide an alternative estimator of the limiting variance that is consistent. Finally, we consider the use of additional observed, baseline covariates not used in pairing units to increase the precision with which we can estimate the local average treatment effect. To this end, we derive the limiting behavior of a two-stage least squares estimator of the local average treatment effect which includes both the additional covariates in addition to pair fixed effects, and show that its limiting variance is always less than or equal to that of the Wald estimator. To complete our analysis, we provide a consistent estimator of this limiting variance. A simulation study confirms the practical relevance of our theoretical results. Finally, we apply our results to revisit a prominent experiment studying the effect of macroinsurance on microenterprise in Egypt.},
	urldate = {2024-08-19},
	publisher = {arXiv},
	author = {Bai, Yuehao and Guo, Hongchang and Shaikh, Azeem M. and Tabord-Meehan, Max},
	month = jun,
	year = {2024},
	note = {arXiv:2307.13094 [econ, math, stat]},
	keywords = {Economics - Econometrics, Mathematics - Statistics Theory},
}

@article{cattaneo2010efficient,
  title={Efficient semiparametric estimation of multi-valued treatment effects under ignorability},
  author={Cattaneo, Matias D},
  journal={Journal of Econometrics},
  volume={155},
  number={2},
  pages={138--154},
  year={2010},
  publisher={Elsevier}
}

@article{efron1971forcing,
  title={Forcing a sequential experiment to be balanced},
  author={Efron, Bradley},
  journal={Biometrika},
  volume={58},
  number={3},
  pages={403--417},
  year={1971},
  publisher={Oxford University Press}
}

@article{wei1978adaptive,
  title={The adaptive biased coin design for sequential experiments},
  author={Wei, Lee-Jen},
  journal={The Annals of Statistics},
  volume={6},
  number={1},
  pages={92--100},
  year={1978},
  publisher={Institute of Mathematical Statistics}
}

@article{cytrynbaum2024finely,
  title={Finely Stratified Rerandomization Designs},
  author={Cytrynbaum, Max},
  journal={arXiv preprint arXiv:2407.03279},
  year={2024}
}



@article{pocock1975sequential,
  title={Sequential treatment assignment with balancing for prognostic factors in the controlled clinical trial},
  author={Pocock, Stuart J and Simon, Richard},
  journal={Biometrics},
  pages={103--115},
  year={1975},
  publisher={JSTOR}
}

