\documentclass[10pt]{article}
\usepackage[hmargin=1in,vmargin=1in]{geometry}
\usepackage{setspace}
\usepackage{amsfonts,amsmath,amssymb,amsthm}
\usepackage[colorlinks,citecolor=blue]{hyperref}
\usepackage{natbib}
\usepackage[pdftex]{graphicx}
\usepackage{enumerate}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage[toc,title,titletoc,header]{appendix}
\usepackage{multirow}
\usepackage{threeparttable}
\usepackage{etoolbox}
\bibliographystyle{ims}
% \usepackage{showlabels}
\allowdisplaybreaks

\renewcommand{\qed}{\rule{2mm}{2mm}}
\renewcommand*{\proofname}{\textsc{Proof}}
\newcommand{\indep}{\perp \!\!\! \perp}

\parskip = 1.5ex plus 0.5 ex minus0.2 ex

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{algorithm}{Algorithm}[section]
\newtheorem{condition}{Condition}[section]

\AtEndEnvironment{remark}{~\qed}
\AtEndEnvironment{example}{~\qed}

\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\diag}{diag}

\renewcommand{\baselinestretch}{1.3}

\begin{document}

\author{
Yuehao Bai \\
Department of Economics\\
University of Southern California \\
\url{yuehao.bai@usc.edu}
\and
Jizhou Liu \\
Booth School of Business \\
\textcolor{white}{111} University of Chicago \textcolor{white}{11}\\
\url{jliu32@chicagobooth.edu}
\and
Azeem M.\ Shaikh\\
Department of Economics\\
University of Chicago \\
\url{amshaikh@uchicago.edu}
\and
Max Tabord-Meehan\\
Department of Economics\\
University of Chicago \\
\url{maxtm@uchicago.edu}
}

\bigskip

\title{On the Efficiency of Finely Stratified Experiments \thanks{We thank Xinran Li, Wei Ma, Kirill Ponomarev, Joseph Romano, Andres Santos, Panos Toulis, Ke-Li Xu, and the seminar participants at the 2024 CEME Young Econometricians Conference, City University of Hong Kong, UC Riverside, UC Irvine, Cornell University, Syracuse University, the University of Toronto, the University of Wisconsin, and Vanderbilt University for helpful comments. The third author acknowledges support from the National Science Foundation through grant SES-2419008. The fourth author acknowledges support from the National Science Foundation through grant SES-2149408.}}

\maketitle

% \vspace{-0.3in}

\begin{spacing}{1.1}
\begin{abstract}
This paper studies the use of finely stratified designs for the efficient estimation of a large class of treatment effect parameters that arise in the analysis of experiments. By a ``finely stratified'' design, we mean experiments in which units are divided into groups of a fixed size and a proportion within each group is assigned to a binary treatment uniformly at random. The class of parameters considered are those that can be expressed as the solution to a set of moment conditions constructed using a known function of the observed data. They include, among other things, average treatment effects, quantile treatment effects, and local average treatment effects as well as the counterparts to these quantities in experiments in which the unit is itself a cluster. In this setting, we establish three results. First, we show that under a finely stratified design, the na\"ive method of moments estimator achieves the same asymptotic variance as what could typically be attained under alternative treatment assignment mechanisms only through {\it ex post} covariate adjustment. Second, we argue that the na\"ive method of moments estimator under a finely stratified design is asymptotically efficient by deriving a lower bound on the asymptotic variance of regular estimators of the parameter of interest in the form of a convolution theorem. In this sense, finely stratified experiments are attractive because they lead to efficient estimators of treatment effect parameters ``by design.'' Finally, we strengthen this conclusion by establishing conditions under which a ``fast-balancing'' property of finely stratified designs is in fact necessary for the na\"ive method of moments estimator to attain the efficiency bound.
\end{abstract}
\end{spacing}

\noindent KEYWORDS: Convolution theorem, Efficiency, Experiment, Experimental design, Finely stratified experiment, Matched pairs, Randomized controlled trial

\noindent JEL classification codes: C12, C14

\thispagestyle{empty} 
\newpage
\setcounter{page}{1}

\section{Introduction}

This paper studies the use of finely stratified designs for the efficient estimation of a large class of treatment effect parameters that arise in the analysis of experiments. By a ``finely stratified'' design, we mean experiments in which units are divided into groups of a fixed size based on their covariate values and a proportion within each group is assigned to a binary treatment uniformly at random. For example, when the fixed size equals two and the marginal probability of treatment assignment is specified to be one half, such a design is simply a matched pairs design. The class of parameters considered are those that can be expressed as the solution to an exactly identified set of moment conditions constructed using a known function of the observed data.  This class of parameters includes many treatment effect parameters of interest: average treatment effects (ATEs), quantile treatment effects, and local average treatment effects as well as the counterparts to these quantities in experiments in which the unit is itself a cluster. 

In the setting described above, we establish three results. First, we study the asymptotic properties of a na\"ive method of moments estimator under a finely stratified design. Here, by a na\"ive method of moments estimator, we mean an estimator constructed using a direct sample analog of the moment conditions. For example, in the case of the ATE, such an estimator is given by the Horvitz-Thompson estimator for the difference in means. We show that under a finely stratified design, the na\"ive method of moments estimator achieves the same asymptotic variance as what could typically be attained under alternative treatment assignment mechanisms only through {\it ex post} covariate adjustment using the same set of covariates. Such adjustment strategies frequently involve the nonparametric estimation of conditional expectations or similar quantities; see, for example, \cite{zhang2008improving}, \cite{tsiatis2008covariate}, \cite{jiang2022improving}, \cite{jiang2022regression-adjusted} and \cite{rafi2023efficient}. We further illustrate that this feature of finely stratified experiments stems from the way in which finely stratified designs balance treatment status across covariate values, a property we define formally below and refer to as ``fast-balancing.'' Second, we derive a lower bound on the asymptotic variance of regular estimators of the parameter of interest in the form of a convolution theorem. This convolution theorem accommodates a large class of possible treatment assignment mechanisms, including covariate adaptive randomization \citep[][]{efron1971forcing, wei1978adaptive,zelen1974randomization, pocock1975sequential,hu2012asymptotic,bugni2018inference,ye2022inference,ma2020statistical,ma2024new}, re-randomization \citep[][]{li2017general,li2018asymptotic,li2020rerandomization,li2020rerandomization-1, cytrynbaum2024finely}, and fine stratification \citep[][]{jiang2021bootstrap, bai2022inference, cytrynbaum2023designing}.  We show that the lower bound is attained by the na\"ive method of moments estimator under a finely stratified design. In this sense, the na\"ive method of moments estimator under a finely stratified design is asymptotically efficient. More succinctly, we say that finely stratified experiments lead to efficient estimators ``by design.'' Finally, we strengthen this conclusion by characterizing all regular asymptotically linear estimators for a large class of treatment assignment mechanisms and use this characterization to establish conditions under which the fast-balancing property of finely stratified experiments is in fact a necessary condition for the na\"ive method of moments estimator to attain the efficiency bound.

Together, these results demonstrate that finely stratified experiments lead to efficient estimators that prioritize transparency in that they preclude the researcher from ``data snooping'' associated with {\it ex post} nonparametric covariate adjustment.  Importantly, concerns with this type of data snooping are not completely eliminated by typical pre-analysis plans because such adjustments involve choices, such as the choice of nonparametric estimator or tuning parameters, that are often not pre-registered prior to the experiment.\footnote{We emphasize that parametric covariate adjustment would, in general, not lead to efficiency.  Furthermore, it too would require choices of covariates and functional form that are also often not pre-registered prior to the experiment.}  The estimators are therefore attractive because they avoid performing nonparametric regression adjustment in order to achieve efficiency and thereby remain ``hands above the table'' \citep[][]{freedman2008regression, lin2013agnostic}. 

Our paper builds upon two strands of literature. The first strand of literature concerns the analysis of finely stratified experiments.  Within this literature, our analysis is most closely related to \cite{bai2022inference}, who derive the asymptotic behavior of the difference-in-means estimator of the ATE when treatment is assigned according to a matched pairs design, and \cite{cytrynbaum2023designing}, who develops related results for an experimental design referred to as ``local randomization'' that permits the proportion of units assigned to treatment to vary with the baseline covariates. Beyond settings that study estimation of the ATE, \cite{bai2024inference-1} develops results for the analysis of different cluster-level average treatment effects and \cite{jiang2021bootstrap} develop results analogous to those in \cite{bai2022inference} for suitable estimators of the quantile treatment effect. This paper, like those just mentioned, operates in a ``super-population'' framework, in which the outcomes and covariates are assumed to be drawn as an i.i.d.\ sample from a population distribution. This is in contrast to an alternative strand of the literature which studies finely stratified experiments from the design-based perspective \citep[see, in particular,][]{imai2008variance, imai2009essential, fogarty2018mitigating, fogarty2018regression-assisted, liu2020regression-adjusted, pashley2021insights}. To our knowledge, our paper is the first to analyze the properties of finely stratified experiments in a general framework which accommodates any parameter that can be characterized as the solutions to a set of moment conditions involving a known function of the observed data. Moreover, we emphasize that none of the above papers formally establish the asymptotic efficiency of finely stratified experiments. The second strand of literature concerns bounds on the efficiency with which treatment effect parameters can be estimated in experiments. We note that, due to the potential for dependence in treatment assignments across individuals, we cannot immediately appeal to standard semi-parametric efficiency results \citep[see, for example,][]{van_der_vaart1998asymptotic, chen2008semiparametric}. Two important recent papers in this literature studying efficiency bounds in the special case of estimating the ATE are \cite{armstrong2022asymptotic} and \cite{rafi2023efficient}. Even in this special case, their results differ from ours in important and empirically relevant ways; Remark \ref{rem:other_SPEB} provides an in-depth discussion of the connection between these results and ours.  See also \cite{bai2022optimality} for some finite-sample optimality properties of matched pairs designs for estimation of the ATE.

The remainder of this paper is organized as follows.  In Section \ref{sec:setup}, we describe our setup and notation.  We emphasize in particular the way in which our framework can accommodate various treatment effect parameters of interest.  Section \ref{sec:variance} derives the asymptotic behavior of the na\"ive method of moments estimator of our parameter of interest when treatment is assigned using a finely stratified design. In Section \ref{sec:semi}, we develop our lower bound on the asymptotic variance of regular estimators of these parameters and show that it is achieved by the the na\"ive method of moments estimator in a finely stratified design. In Section \ref{sec:regular}, we characterize all regular asymptotically linear estimators and argue that the fast-balancing property is necessary for the na\"ive method of moments estimator to attain the efficiency bound. In Section \ref{sec:sims}, we illustrate the practical relevance of our theoretical results through a simulation study. Finally, we conclude in Section \ref{sec:recs} with some recommendations for empirical practice guided by both these simulations and our theoretical results. Proofs of all results can be found in the Appendix.

\section{Setup and Motivation}\label{sec:setup}

Let $A_i \in \{0, 1\}$ denote the treatment status of the $i$th unit, and let $X_i \in \mathbf R^{d_x}$ denote their observed, baseline covariates. For $a \in \{0, 1\}$, let $R_i(a) \in \mathbf R^{d_r}$ denote a vector of potential responses. As we illustrate below, considering a vector of responses allows us to accommodate certain parameters of interest. Let $R_i \in \mathbf R^{d_r}$ denote the vector of observed responses obtained from $R_i(a)$ once treatment is assigned. As usual, the observed responses and potential responses are related to treatment status by the relationship
\begin{equation}\label{eq:PO}
R_i = R_i(1) A_i + R_i(0) (1 - A_i)~. 
\end{equation}
We assume throughout that our sample consists of $n$ units. For any random vector indexed by $i$, for example $A_i$, we define $A^{(n)} = (A_1, \ldots, A_n)$. Let $P_n$ denote the distribution of the observed data $(R^{(n)}, A^{(n)}, X^{(n)})$, and $Q_n$ the distribution of $(R^{(n)}(1), R^{(n)}(0), X^{(n)})$. We assume $Q_n = Q^n$, where $Q$ is the marginal distribution of $(R_i(1), R_i(0), X_i)$. Given $Q_n$, $P_n$ is then determined by \eqref{eq:PO} and the mechanism for determining treatment assignment. We assume that treatment assignment is performed such that a standard unconfoundedness assumptions holds and such that the probability of assignment given $X$ is some known constant for every $1 \le i \le n$, as is often the case in most experiments:

\begin{assumption} \label{ass:unconfounded}
Treatment status is assigned so that
\begin{equation} \label{eq:unconfounded}
    (R^{(n)}(1), R^{(n)}(0)) \indep A^{(n)} | X^{(n)}~,
\end{equation}
and such that $P\{A_i = 1|X_i=x\} = \eta$, for some $\eta \in (0, 1)$ for all $1 \le i \le n$.
\end{assumption}

Assumption \ref{ass:unconfounded} restricts the probability of assignment to be the fixed fraction $\eta$ across the entire experimental sample, but this restriction can be weakened so that $\eta$ is replaced by $\eta(X_i)$ for many of our subsequent results: see, in particular, Remarks \ref{rem:generaleta-attained}, \ref{rem:generaleta}, and \ref{rem:other_SPEB}. Given Assumption \ref{ass:unconfounded}, it can be shown that $(X_i, A_i, R_i)$ are identically distributed for $1 \le i \le n$, and their marginal distribution does not change with $n$ (see Lemma \ref{lem:marginal} in the Appendix). As a consequence, we denote the marginal distribution of $(X_i, A_i, R_i)$ by $P$. We consider parameters $\theta_0 \in \Theta \subset \mathbf R^{d_\theta}$ that can be defined as the solution to a set of moment equalities. In particular, let $m: \mathbf{R}^{d_x} \times \{0, 1\} \times \mathbf R^{d_r} \to \mathbf R^{d_\theta}$ be a known measurable function, then we consider parameters $\theta_0$ that uniquely solve the moment equality
\begin{equation} \label{eq:moments}
E_P[m(X_i, A_i, R_i, \theta_0)] = 0~.
\end{equation}
We emphasize that $m(\cdot)$ is not a function of any unknown nuisance parameters, but may depend on the known value of $\eta$ in Assumption \ref{ass:unconfounded}. We present five examples of well-known parameters that can be described as (functions of) solutions to a set of moment conditions as in \eqref{eq:moments}.

\begin{example}[Average Treatment Effect]\label{ex:ATE}
Let $Y_i(a) = R_i(a)$ denote a scalar potential outcome for the $i$th unit under treatment $a \in \{0, 1\}$, and let $Y_i = R_i$ denote the observed outcome. Let $\theta_0 = E_Q[Y_i(1) - Y_i(0)]$ denote the average treatment effect (ATE). Under Assumption \ref{ass:unconfounded}, $\theta_0$ solves the moment condition in \eqref{eq:moments} with
\begin{equation} \label{eq:moments-ate}
   m(X_i, A_i, R_i, \theta) = \frac{Y_i A_i}{\eta} - \frac{Y_i (1 - A_i)}{1 - \eta} - \theta~. 
\end{equation}
For a list of papers which consider estimators based on \eqref{eq:moments-ate}, see \cite{hirano2001estimation} and \cite{hirano2003efficient}.
\end{example}

\begin{example}[Quantile Treatment Effect]\label{ex:QTE}
Let $Y_i(a) = R_i(a)$ denote a scalar potential outcome for the $i$th unit under treatment $a \in \{0, 1\}$, and let $Y_i = R_i$ denote the observed outcome. Let $\tau \in (0, 1)$ and $\theta_0 = (\theta_0(1), \theta_0(0))' = (q_{Y(1)}(\tau), q_{Y(0)}(\tau))'$, where
\[ q_{Y(a)}(\tau) = \inf \{\lambda \in \mathbf R: Q \{Y_i(a) \leq \lambda \} \geq \tau\}~. \]
In other words, $\theta_0$ is defined to be the vector of $\tau$th quantiles of the marginal distributions of $Y_i(1)$ and $Y_i(0)$. If we assume $q_{Y(a)}(\tau)$ is unique for $a \in \{0, 1\}$ in the sense that $Q\{Y(a) \leq q_{Y(a)}(\tau) + \epsilon\} > Q\{Y(a) \leq q_{Y(a)}(\tau)\}$ for all $\epsilon > 0$, then it follows from Assumption \ref{ass:unconfounded} and Lemma 1 in \cite{firpo2007efficient} that $\theta_0$ solves the moment condition in \eqref{eq:moments} with
\[ m(X_i, A_i, R_i, \theta) = \begin{pmatrix}
\displaystyle \frac{A_i (\tau - I \{Y_i \leq \theta(1)\})}{\eta} \\
\displaystyle \frac{(1 - A_i) (\tau - I \{Y_i \leq \theta(0)\})}{1 - \eta}
\end{pmatrix}~, \]
for $\theta = (\theta^{(1)}, \theta^{(0)})'$. Note that the quantile treatment effect $q_{Y(1)}(\tau) - q_{Y(0)}(\tau)$ can then be defined as $h(\theta_0)$ where $h:\mathbf{R}^2 \to \mathbf{R}$ is given by $h(s,t) = s - t$.
\end{example}

\begin{example}[Local Average Treatment Effect]\label{ex:LATE}
Let $(\tilde{Y}_i(a), D_i(a)) = R_i(a)$ denote the vector of potential outcomes and treatment take-up under treatment $a \in \{0, 1\}$, and let $(Y_i, D_i) = R_i$ denote the vector of observed outcomes and treatment take-up. Note here that $\tilde{Y}_i(a)$ corresponds to the potential outcome under assignment $a \in \{0,1\}$ and not to the potential outcome for a given take-up $D_i = d$. Suppose $E_Q[D_i(1) - D_i(0)] \ne 0$ and let
\[ \theta_0 = \frac{E_Q[\tilde{Y}_i(1) - \tilde{Y}_i(0)]}{E_Q[D_i(1) - D_i(0)]}~. \]
It then follows from Assumption \ref{ass:unconfounded} that $\theta_0$ solves the moment condition in \eqref{eq:moments} with
\begin{equation} \label{eq:moments-late}
    m(X_i, A_i, R_i, \theta) = \frac{Y_i A_i}{\eta} - \frac{Y_i (1 - A_i)}{1 - \eta} - \theta \left ( \frac{D_i A_i}{\eta} - \frac{D_i (1 - A_i)}{1 - \eta} \right )~.
\end{equation}
If we further assume instrument monotonicity (i.e., $P\{D_i(1) \ge D_i(0)\} = 1$) and instrument exclusion, then $\theta_0$ could be re-interpreted as the local average treatment effect (LATE) in the sense of \cite{imbens1994identification}.
\end{example}

\begin{example}[Weighted Average Treatment Effect]\label{ex:Clust_ATE}
Let $Y_i(a) = R_i(a)$ denote a scalar potential outcome for the $i$th unit under treatment $a \in \{0, 1\}$, and let $Y_i = R_i$ denote the observed outcome. 
Let 
\[\theta_0 = E_Q\left[\frac{\omega(X_i)}{E_Q[\omega(X_i)]}\left(Y_i(1) - Y_i(0)\right)\right]~,\]
for some known function $\omega: \mathbf{R}^{d_x} \to \mathbf{R}$.
It then follows from Assumption \ref{ass:unconfounded} that $\theta_0$ solves the moment condition in \eqref{eq:moments} with
\[ m(X_i, A_i, R_i, \theta) = \omega(X_i)\left(\frac{Y_i A_i}{\eta} - \frac{Y_i (1 - A_i)}{1 - \eta}\right) - \omega(X_i)\theta~.\] 
Note that $\theta_0$ defined in this way can accommodate the (cluster) size-weighted and equally-weighted average treatment effects considered in \cite{bugni2022inference} and \cite{bai2024inference-1} in the context of cluster-level randomized controlled trials.
\end{example}

\begin{example}[Log-Odds Ratio]\label{ex:log_odds}
Let $Y_i(a) = R_i(a) \in \{0, 1\}$ denote a binary potential outcome for the $i$th unit under treatment $a \in \{0, 1\}$, and let $Y_i = R_i$ denote the observed outcome.
Suppose $0 < P\{Y_i(a) = 0\} < 1$ for $a \in \{0, 1\}$, and let $\theta_0 = (\theta_0(1), \theta_0(2))'$, where
\[\theta_0(1) = \text{logit}(E_Q[Y_i(0)])~,\]
\[\theta_0(2) = \text{logit}(E_Q[Y_i(1)]) - \text{logit}(E_Q[Y_i(0)])~,\]
with $\text{logit}(z) = \log(\frac{z}{1-z})$, so that $\theta_0(2)$ denotes the log-odds ratio of treatment $1$ relative to treatment $0$. It follows from Assumption \ref{ass:unconfounded} that $\theta_0$ solves the moment condition in \eqref{eq:moments} with
\[m(X_i, A_i, R_i, \theta) =  \begin{pmatrix}
\displaystyle 1 - A_i \\
\displaystyle A_i
\end{pmatrix}\left(Y_i - \text{expit}(\theta(1) + \theta(2)A_i)\right)~,\]
where $\text{expit}(z) = \frac{\exp(z)}{1 + \exp(z)}$. The log-odds ratio can then be defined as $h(\theta_0)$ where $h: \mathbf{R}^2 \to \mathbf{R}$ is given by $h(s,t) = t$. This parameter appears in, for example, \cite{zhang2008improving}.
\end{example}
Additional examples could be obtained by considering combinations of Examples \ref{ex:ATE}--\ref{ex:log_odds}. For instance, combining the moment functions from Examples \ref{ex:LATE} and \ref{ex:Clust_ATE} would result in a weighted LATE parameter. Beyond these examples, certain treatment effect contrasts could also be related to the structural parameters in, for instance, an economic model of supply in demand: see, for example, the model estimated in \cite{casaburi2022using}.

Throughout the rest of the paper we consider the asymptotic properties of the method  of moments estimator $\hat{\theta}_n$ for $\theta_0$ which is constructed as a solution to the sample analogue of \eqref{eq:moments}:
\begin{equation} \label{eq:est}
\frac{1}{n} \sum_{1 \leq i \leq n} m(X_i, A_i, R_i, \hat \theta_n) = 0~.
\end{equation}
Because $\hat{\theta}_n$ is constructed directly using the moment function $m(\cdot)$, we call $\hat{\theta}_n$ the na\"ive method of moments estimator. Note that $\hat \theta_n$ as defined in \eqref{eq:est} is closely related to standard estimators of the parameter $\theta_0$ in specific examples. For instance, in Example \ref{ex:ATE}, \[\hat{\theta}_n = \frac{1}{\eta}\sum_{1 \le i \le n}Y_iA_i - \frac{1}{1 - \eta}\sum_{1 \le i \le n}Y_i(1 - A_i)~,\]
so that $\hat{\theta}_n$ is a Horvitz-Thompson analogue of the standard difference-in-means estimator for the ATE. In Example \ref{ex:LATE},
\[\hat{\theta}_n = \frac{\frac{1}{\eta}\sum_{1 \le i \le n}Y_iA_i - \frac{1}{1 - \eta}\sum_{1 \le i \le n}Y_i(1 - A_i)}{\frac{1}{\eta}\sum_{1 \le i \le n}D_iA_i - \frac{1}{1 - \eta}\sum_{1 \le i \le n}D_i(1 - A_i)}~,\]
so that $\hat{\theta}_n$ is a Horvitz-Thompson analogue of the standard Wald estimator for the local average treatment effect. 

Before proceeding, in the remainder of this section, we provide a more detailed summary of the main contributions of our paper.  To this end, first note that if $A^{(n)}$ were assigned i.i.d., independently of $X^{(n)}$, then it can be shown under mild conditions on $m(\cdot)$ \citep[see, for instance, Theorem 5.1 in][]{van_der_vaart1998asymptotic} that the na\"ive method of moments estimator satisfies 
\[\sqrt{n}(\hat{\theta}_n - \theta_0) \overset{d}{\to} N(0, \mathbb{V})~,\]
where
\begin{equation}\label{eq:naive_variance}
\mathbb V = M^{-1}E_P[m(X_i, A_i, R_i, \theta_0)m(X_i, A_i, R_i, \theta_0)'](M^{-1})^{\prime}~,
\end{equation}
with $M = \frac{\partial}{\partial \theta'} E_P[m(X, A, R, \theta)] \Big |_{\theta = \theta_0}$. In Section \ref{sec:variance}, we show that if we assign $A^{(n)}$ using a finely stratified design (i.e., a treatment assignment mechanism which uses the covariates $X^{(n)}$ to block units into groups of fixed size: see Assumption \ref{ass:a} below for a formal definition), then 
\[\sqrt{n}(\hat{\theta}_n - \theta_0) \overset{d}{\to} N(0, \mathbb{V}_*)~,\]
where $\mathbb{V} \ge \mathbb{V}_*$ (see Theorem \ref{thm:normal}). Under i.i.d.\ assignment, the na\"ive method of moment estimator $\hat \theta_n$ cannot generally attain $\mathbb V_\ast$, but an estimator that attains $\mathbb{V}_*$  could instead be constructed by appropriately ``augmenting'' the moment function, and then considering an estimator which solves the augmented moment equation. For instance, if we consider the ATE in Example \ref{ex:ATE}, then it is straightforward to show that the following augmented moment function identifies $\theta_0$:
\begin{equation}\label{eq:augment_m}
m^*(X_i, A_i, R_i, \theta) = \left(\frac{A_i(Y_i - \mu_1(X_i))}{\eta} - \frac{(1 - A_i)(Y_i - \mu_0(X_i))}{1 - \eta} + \mu_1(X_i) - \mu_0(X_i)\right) - \theta~,
\end{equation}
where $\mu_a(X_i) = E_Q[Y_i(a)|X_i]$. 
This choice of $m^*(\cdot)$ produces the well known doubly-robust moment condition for estimating the ATE \citep{robins1995analysis,hahn1998role}. It can then be shown that an appropriately constructed two-step estimator, where $\mu_1(\cdot)$ and $\mu_0(\cdot)$ are non-parametrically estimated in a first step, attains $\mathbb{V}_*$  \citep{tsiatis2008covariate,farrell2015robust,chernozhukov2017doubledebiasedneyman, rafi2023efficient}. Intuitively, the estimator obtained from the augmented moment function $m^*(\cdot)$ performs nonparametric regression adjustment by exploiting the information contained in $X^{(n)}$ that may not have been captured in the original moment function $m(\cdot)$.  Similar nonparametric regression adjustments based on augmented moment equations have been developed for other parameters of interest \citep{zhang2008improving, belloni2017program,jiang2022improving,jiang2022regression-adjusted}.   In this sense, we show that fine stratification can perform nonparametric regression adjustment ``by design'' for the large class of parameters that can be expressed in terms of moment conditions of the form given in \eqref{eq:moments}, thus generalizing similar observations made in \cite{bai2022inference}, \cite{bai2022optimality}, and \cite{cytrynbaum2023designing} in the special case of estimating the ATE. As we explain in the discussion following Theorem \ref{thm:normal}, this feature of finely stratified experiments is possible because fine stratification leads to fast-balancing of the treatment across covariate values, as defined formally in \eqref{eq:fast-m} below.

Earlier work on efficient treatment effect estimation has noted that the variance $\mathbb{V}_*$ is in fact the efficiency bound for estimating $\theta_0$ under i.i.d.\ assignment \citep[see, for instance,][]{cattaneo2010efficient}. A natural follow-up question is whether or not $\mathbb{V}_*$ continues to be the efficiency bound for estimating $\theta_0$ under a finely stratified design, or more generally for complex experimental designs which induce dependence in the treatment assignments across individuals in the experiment. In Section \ref{sec:semi}, we show that $\mathbb V_\ast$ continues to be the efficiency bound for estimating $\theta_0$ for a large class of treatment assignment mechanisms with a fixed marginal probability of treatment assignment, which includes finely stratified designs as a special case. We can thus conclude that, from the perspective of asymptotic efficiency, finely stratified designs are optimal experimental designs for a broad range of treatment effect estimation problems. In Section \ref{sec:regular} we build on this result and establish conditions under which efficient estimation of $\theta_0$ using the na\"ive method of moments estimator can be achieved only if the experimental design leads to ``fast-balancing'' of the treatment across covariate values. In this sense, we show that the fast-balancing property of finely stratified experiments is in fact a \emph{necessary} condition for achieving efficient estimation of $\theta_0$ ``by design.''

\section{The Asymptotic Variance of Finely Stratified Experiments}\label{sec:variance}
In this section, we derive the limiting distribution of the method of moments estimator $\hat \theta_n$ when treatment is assigned by fine stratification over the baseline covariates $X^{(n)}$. Such assignment mechanisms use the covariates $X^{(n)}$ to group units with similar covariate values into blocks of fixed size, and then assign treatment completely at random within each block.  In order to describe this assignment mechanism formally, we require some further notation to define the blocks of units. Let $\ell$ and $k$ be arbitrary positive integers with $\ell \le k$ and set $\eta = \ell/k$. For simplicity, assume that $n$ is divisible by $k$. We then represent blocks of units using a partition of $\{1, \ldots, n\}$ given by
\[\left\{\lambda_j = \lambda_j(X^{(n)}) \subseteq \{1, \ldots, n\}, 1 \le j \le n/k\right\}~,\]
with $|\lambda_j| = k$.  Because of its possible dependence on $X^{(n)}$, $\{\lambda_j: 1 \le j \le n/k\}$ encompasses a variety of different ways of blocking the $n$ units according to the observed, baseline covariates.  Given such a partition, we assume that treatment status is assigned as described in the following assumption:
\begin{assumption} \label{ass:a}
Treatment status is assigned so that $(R^{(n)}(1), R^{(n)}(0)) \indep A^{(n)} \big | X^{(n)}$ and, conditional on $X^{(n)}$, 
\[\{(A_i: i \in \lambda_j): 1 \le j \le n/k\}\]
are i.i.d.\ and each uniformly distributed over all permutations of $(\underbrace{0, 0, \ldots, 0}_{k - \ell}, \underbrace{1, 1, \ldots, 1}_{\ell})$.
\end{assumption}

The assignment mechanism described in Assumptions \ref{ass:a} generalizes the definition of a matched pairs design. In particular, we recover a matched pairs design if we set $(\ell, k) = (1, 2)$, with $\eta = 1/2$. Indeed, suppose $n$ is even and consider pairing the experimental units into $n / 2$ pairs, represented by the sets
\[ \{\pi(2j - 1), \pi(2j)\} \text{ for } j = 1, \ldots, n / 2~, \]
where $\pi = \pi_n(X^{(n)})$ is a permutation of $n$ elements.  Because of its possible dependence on $X^{(n)}$, $\pi$ encompasses a broad variety of ways of pairing the $n$ units according to the observed, baseline covariates $X^{(n)}$. Given such a $\pi$, we assume that treatment status is assigned so that Assumption \ref{ass:a} holds and, conditional on $X^{(n)}$, $(A_{\pi(2j-1)}, A_{\pi(2j)}), j = 1, \ldots, n / 2$ are i.i.d.\ and each uniformly distributed over the values in $\{(0,1), (1,0)\}$. For some examples of such an assignment mechanism being used in practice, see, for instance, \cite{angrist2009effects}, \cite{banerjee2015miracle}, and \cite{bruhn2016impact}.

\begin{remark}\label{rem:matched_pairs}
Note that Assumption \ref{ass:a} generalizes matched pairs designs along two dimensions: first, it allows for treatment fractions other than $\eta = 1/2$. Second, it allows for choices of $\ell$ and $k$ which are not relatively prime. For instance, if we set $(\ell, k) = (2, 4)$, then $\eta = 1/2$ as in matched pairs, but now the assignment mechanism blocks units into groups of size $4$ and assigns two units to treatment, two units to control. Although Theorem \ref{thm:normal} below establishes that allowing for this level of flexibility has no effect on the asymptotic properties of our estimator, in our experience we have found that designs which employ these treatment ``replicates'' in each block can simplify the construction of variance estimators in practice.  See Appendix \ref{sec:var_est} for further discussion.
\end{remark}

Our analysis will require some discipline on the way in which the blocks are formed.  In particular, we will require that the units in each block be close in terms of their baseline covariates in the sense described by the following assumption:
\begin{assumption} \label{ass:pair}
The blocks used in determining treatment status satisfy
\[ \frac{1}{n} \sum_{1 \leq j \leq n/k} \max_{i, i' \in \lambda_j} \|X_{i} - X_{i'}\|^2 \stackrel{P}{\rightarrow} 0~. \]
\end{assumption}

\cite{bai2022inference} and \cite{cytrynbaum2023designing} discuss blocking algorithms that satisfy Assumption \ref{ass:pair}. When $X_i \in \mathbf{R}$ and $E_Q[X_i^2] < \infty$, a simple algorithm that satisfies Assumption \ref{ass:pair} is to simply order units from smallest to largest and then block adjacent units into blocks of size $k$. In the case of matched pairs, if $\mathrm{dim}(X_i) > 1$ and $E_Q[\|X_i\|^d] < \infty$ for $d \geq \mathrm{dim}(X_i) + 1$, then Assumption \ref{ass:pair} is satisfied by the \texttt{nbpmatching} algorithm in \texttt{R} that minimizes the sum of squared distances of $X$ within pairs. See Appendix A of \cite{bai2024inference-1} for details.

Finally, we impose the following assumptions to derive the large-sample properties of $\hat{\theta}_n$. In what follows, when writing expectations and variances, we suppress the subscripts $P$ and $Q$ whenever doing so does not lead to confusion.

\begin{assumption} \label{ass:normal}
Let $m(\cdot) = (m_s(\cdot): 1 \le s \le d_{\theta})'$. The moment functions are such that
\begin{enumerate}[\rm (a)]
    \item For every $\epsilon > 0$, $\inf\limits_{\theta \in \Theta: \|\theta - \theta_0\| > \epsilon} \| E[m(X_i, A_i, R_i, \theta)] \| > 0$.
    \item $E[m(X_i, A_i, R_i, \theta)]$ is differentiable at $\theta_0$ with a nonsingular derivative $M = \frac{\partial}{\partial \theta'} E[m(X, A, R, \theta)] \Big |_{\theta = \theta_0}$.
    \item For $1 \leq s \leq d_\theta$, $E[((m_s(X, a, R(a), \theta) - m_s(X, a, R(a), \theta_0))^2] \to 0$ as $\theta \to \theta_0$ for $a \in \{0, 1\}$.
    \item For $1 \leq s \leq d_\theta$, $\{m_s(x, a, r, \theta): \theta \in \Theta\}$ is pointwise measurable in the sense that there exists a countable set $\Theta^\ast$ such that for each $\theta \in \Theta$, there exists a sequence $\{\theta_m\} \subset \Theta^\ast$ such that $m_s(x, a, r, \theta_m) \to m_s(x, a, r, \theta)$ as $m \to \infty$ for all $x, a, r$.
    \item (i) $\sup_{\theta \in \Theta} E[\|m(X, a, R(a), \theta)\|] < \infty$ for $a \in \{0, 1\}$. (ii) $\{m(x, 1, r, \theta): \theta \in \Theta^\ast\}$ and $\{m(x, 0, r, \theta): \theta \in \Theta^\ast\}$ are $Q$-Donsker.
    \item For $a \in \{0, 1\}$, $E[ m(X, a, R(a), \theta_0) | X = x]$ is Lipschitz.
\end{enumerate}
\end{assumption}

Assumption \ref{ass:normal}(a) is a standard assumption to ensure the solution to \eqref{eq:moments} is ``well separated.'' It appears as a condition, for instance, in Theorem 5.9 in \cite{van_der_vaart1998asymptotic}. Assumption \ref{ass:normal}(b) is a standard assumption used when deriving the properties of $Z$-estimators. See, for instance, Theorem 3.1 in \cite{newey1994large} and Theorem 5.21 in \cite{van_der_vaart1998asymptotic}. Because differentiability is imposed on their expectations instead of the moment functions themselves, the moment functions are allowed to be nonsmooth as in Example \ref{ex:QTE}. Assumption \ref{ass:normal}(c) requires the moment function to be mean-square continuous in $\theta$. Assumption \ref{ass:normal}(d) is a standard condition to guarantee the measurability of the supremum of a suitable class of functions. In particular, it allows us to define expectations of suprema without invoking outer expectations. See Example 2.3.4 in \cite{van_der_vaart1996weak} for details. Assumption \ref{ass:normal}(e) is a standard assumption which guarantees the existence of an integrable envelope and allows us to invoke a uniform law of large numbers and a uniform central limit theorem. In particular, this assumption can be verified for Examples \ref{ex:ATE}--\ref{ex:log_odds}. Assumption \ref{ass:normal}(f) is a common assumption which simplifies some arguments when studying finely stratified designs, and ensures units that are close in terms of the baseline covariates are also close in terms of their moments. Note that Assumption \ref{ass:normal}(f) could be dropped following the approximation arguments in Lemma C.5 of \cite{cytrynbaum2023designing}; see also Examples \ref{ex:MP} and \ref{ex:MP2} for further discussion.

The following theorem establishes the asymptotic variance of the na\"ive method of moments estimator when the treatment assignment mechanism is finely stratified in the sense of satisfying Assumptions \ref{ass:a}--\ref{ass:pair}. Its proof relies on a crucial technical result in \cite{han2021complex}, which allows us to compare the empirical process that depends on the treatment assignments with the empirical process that only depends on i.i.d.\ quantities. By leveraging this result, we note that Assumption \ref{ass:normal} is comparable to the typical assumptions imposed to study the properties of method of moments estimators using i.i.d.\ data.

\begin{theorem} \label{thm:normal}
Suppose the treatment assignment mechanism satisfies Assumptions \ref{ass:a}--\ref{ass:pair} and the moment functions satisfy Assumption \ref{ass:normal}. Let $\hat{\theta}_n$ be defined as in \eqref{eq:est}. Then,
\begin{equation} \label{eq:normal}
\sqrt n(\hat \theta_n -  \theta_0) = \frac{1}{\sqrt{n}} \sum_{1 \leq i \leq n} \psi^\ast(X_i, A_i, R_i, \theta_0) + o_P(1)~.
\end{equation}
where
\begin{align*}
& \psi^\ast(X_i, A_i, R_i, \theta_0) \\
& = - M^{-1} \Big ( I\{A_i = 1\} (m(X_i, 1, R_i, \theta_0) - E[m(X_i, 1, R_i(1), \theta_0) | X_i]) \\
& \hspace{3.5em} + I\{A_i = 0\} (m(X_i, 0, R_i, \theta_0) - E[m(X_i, 0, R_i(0), \theta_0) | X_i]) \\
& \hspace{3.5em} + \eta E[m(X_i, 1, R_i(1), \theta_0) | X_i] + (1 - \eta) E[m(X_i, 0, R_i(0), \theta_0) | X_i] \Big )~.
\end{align*}
Further, we have that
\begin{equation} \label{eq:normal_convergence}
\sqrt n(\hat \theta_n -  \theta_0) \stackrel{d}{\to} N(0, \mathbb V_\ast)~,
\end{equation}
where
\begin{equation} \label{eq:Vstar}
    \mathbb V_\ast = \var[\psi^\ast(X_i, A_i, R_i, \theta_0)]~.
\end{equation}
\end{theorem}

In order to make Theorem \ref{thm:normal} useful for inference about $\theta_0$, we describe in Appendix \ref{sec:var_est} an estimator $\hat{\mathbb{V}}_n$ of $\mathbb{V}_*$ and sketch a proof of its consistency. We now sketch an argument of the proof of Theorem \ref{thm:normal} to highlight the fundamental role played by a ``fast-balancing'' property of finely stratified designs (see \eqref{eq:fast-m} below). In the proof of Theorem \ref{thm:normal}, we first establish that
\begin{equation} \label{eq:vdv}
\sqrt n(\hat \theta_n -  \theta_0) = -M^{-1}\frac{1}{\sqrt{n}}\sum_{1 \le i \le n}m(X_i,A_i,R_i,\theta_0) + o_P(1)~.
\end{equation}
To further establish \eqref{eq:normal}, it thus suffices to show that, under a finely stratified design,
\begin{equation}\label{eq:by_design}
-M^{-1}\frac{1}{\sqrt{n}}\sum_{1 \le i \le n}m(X_i,A_i,R_i,\theta_0) = \frac{1}{\sqrt{n}} \sum_{1 \leq i \leq n} \psi^\ast(X_i, A_i, R_i, \theta_0) + o_P(1)~.
\end{equation}
To obtain this equivalence, consider the following decomposition of $m(\cdot)$:
\begin{align*}
& m(X_i, A_i, R_i, \theta_0) \\
& = \eta E[m(X_i, 1, R_i(1), \theta_0) | X_i] + (1 - \eta) E[m(X_i, 0, R_i(0), \theta_0) | X_i] \\
& \hspace{3.5em} + I\{A_i = 1\} (m(X_i, 1, R_i, \theta_0) - E[m(X_i, 1, R_i(1), \theta_0) | X_i]) \\
& \hspace{3.5em} + I\{A_i = 0\} (m(X_i, 0, R_i, \theta_0) - E[m(X_i, 0, R_i(0), \theta_0) | X_i]) \\
& \hspace{3.5em} + (A_i - \eta) (E[m(X_i, 1, R_i(1), \theta_0) - m(X_i, 0, R_i(0), \theta_0) | X_i]) ~.
\end{align*}
Then the equivalence follows if we can show that
\begin{equation}\label{eq:fast-m}\frac{1}{\sqrt{n}}\sum_{1 \le i \le n}(A_i - \eta)\left(E[m(X_i, 1,R_i(1),\theta_0) - m(X_i, 0,R_i(0),\theta_0)|X_i]\right) = o_P(1)~.
\end{equation}
We call \eqref{eq:fast-m} the fast-balancing condition for the function $m(\cdot)$. Intuitively, the fast-balancing condition imposes that the experimental design should balance the treatment across covariate values at a rate which is faster than sampling variation. To see why \eqref{eq:fast-m} holds for a finely stratified design, let $\Omega(X_i) = E[m(X_i, 1,R_i(1),\theta_0) - m(X_i, 0,R_i(0),\theta_0)|X_i]$ and first note that, by Assumption \ref{eq:unconfounded},
\[E\bigg[\frac{1}{\sqrt{n}}\sum_{1 \le i \le n}(A_i - \eta)\Omega(X_i) \bigg \vert X^{(n)}\bigg] = 0~.\]
Next, for $1 \leq \ell \leq d_\theta$, let $\Omega^{(\ell)}(X_i)$ denote the $\ell$th component of $\Omega(X_i)$. Then it can be shown using Assumption \ref{ass:a} and \ref{ass:normal}(f) that for $1 \leq \ell \leq d_\theta$,
\[\var\bigg[\frac{1}{\sqrt{n}}\sum_{1 \le i \le n}(A_i - \eta)\Omega^{(\ell)}(X_i) \bigg \vert X^{(n)}\bigg] \lesssim \frac{1}{n}\sum_{1 \le j \le n/k} \max_{i, i' \in \lambda_j} \|X_i - X_{i'}\|^2~,\]
and so the conditional variance converges in probability to zero under Assumption \ref{ass:pair}. The fast balacning condition \eqref{eq:fast-m} then follows by an application of Chebyshev's inequality conditional on $X^{(n)}$ and the dominated convergence theorem. In Section \ref{sec:regular}, we further argue that the fast-balancing condition \eqref{eq:fast-m} is in fact a \emph{necessary} condition which a given experimental design must satisfy to ensure \eqref{eq:by_design}. 

\begin{remark} \label{rem:V_compare}
By comparing the variance expression in \eqref{eq:naive_variance} to the variance expression for $\mathbb{V}_*$, we obtain
\begin{multline*}
    \mathbb{V} - \mathbb{V}_* = \eta (1 - \eta) M^{-1} E[(E[m(X_i, 1, R_i(1), \theta_0) | X_i] - E[m(X_i, 0, R_i(0), \theta_0) | X_i]) \\
    \times (E[m(X_i, 1, R_i(1), \theta_0) | X_i] - E[m(X_i, 0, R_i(0), \theta_0) | X_i])'](M^{-1})^{\prime}~,
\end{multline*}
which is positive semidefinite. From this, we conclude that the asymptotic variance of the naive method of moments estimator $\hat{\theta}_n$ is lower in a finely stratified design compared to i.i.d.\ assignment. In Section \ref{sec:semi}, we will further show that $\mathbb V_\ast$ is the lowest possible asymptotic variance among regular estimators for $\theta_0$ in a large class of treatment assignment mechanisms, including both i.i.d.\ assignment and finely stratified designs. When $d_\theta = 1$, we may express $\mathbb V - \mathbb V_\ast$ in terms of the ``nonparametric $R^2$.''  In particular, $\mathbb V - \mathbb V_\ast$ is proportional to $E[R_{g, X}^2(g_i, X_i) \var[g_i]]$, where 
\begin{equation} \label{eq:R2}
R_{g, X}^2(g_i, X_i) = \frac{\var[E[g_i|X_i]]}{\var[g_i]}~,
\end{equation}
and $g_i = m(X_i, 1, R_i(1), \theta_0) - m(X_i, 0, R_i(0), \theta_0)$.  The quantity in \eqref{eq:R2} measures how much of the variation in $g_i$ can be explained nonparametrically by $X_i$. See, for instance, \cite{chernozhukov2024long}.
\end{remark}

\begin{remark}\label{rem:psi_examples_ate}
Note it follows from \eqref{eq:moments} that
\begin{equation} \label{eq:meanzero}
\eta E_Q[m(X_i, 1, R_i(1), \theta_0)] + (1 - \eta) E_Q[m(X_i, 0, R_i(0), \theta_0)] = E_P[m(X_i, A_i, R_i, \theta_0)] = 0~,     
\end{equation}
so that $E[\psi^\ast(X_i, A_i, R_i, \theta_0)] = 0$. It is further straightforward to show using Assumption \ref{ass:unconfounded} that
\begin{align}
    \label{eq:var} \mathbb V_\ast & = \var[\psi^\ast(X_i, A_i,R_i, \theta_0)] \\
    \nonumber & = M^{-1} \big ( E \big [ \eta \var[m(X_i, 1, R_i(1), \theta_0) | X_i] + (1 - \eta) \var[m(X_i, 0, R_i(0), \theta_0) | X_i] \big ] \\
    \nonumber & \hspace{5em} + \var \big [ \eta E[m(X_i, 1, R_i(1), \theta_0) | X_i] + (1 - \eta) E[m(X_i, 0, R_i(0), \theta_0) | X_i] \big ] \big ) (M^{-1})'
\end{align}
For instance, in the special case of the ATE (Example \ref{ex:ATE}) we obtain that 
\begin{align}
     \nonumber \var[\psi^\ast(X_i, A_i,R_i, \theta_0)] & = E\left[\frac{\var[Y_i(1)|X_i]}{\eta} + \frac{\var[Y_i(0)|X_i]}{1 - \eta} \right. \\
    \label{eq:var-ate} & \hspace{5em} + \left. \left(E[Y_i(1)- Y_i(0)|X_i] - E[Y_i(1) - Y_i(0)]\right)^2\right]~,
\end{align}
which matches the asymptotic variance derived in \cite{bai2022inference} for matched pairs. Theorem \ref{thm:normal} however accommodates a much larger class of parameters including those introduced in Examples \ref{ex:QTE}--\ref{ex:log_odds}.
\end{remark}

\begin{remark} \label{rem:generaleta-attained}
    Although Theorem \ref{thm:normal} is focused on the case where $\eta(X_i) = \eta$ is a constant, straightforward modifications of the treatment assignment mechanism described in Assumptions \ref{ass:a}--\ref{ass:pair} can be applied in more general settings. For instance, suppose $\eta(X_i)$ takes on a finite set of values $\{\eta_1, \dots, \eta_S\}$, we could then simply implement a finely stratified experiment over each set $\{i: \eta(X_i) = \eta_s\}$ for $1 \leq s \leq S$. In other words, separately within each stratum defined by the units for which $\eta(X_i) = \eta_s$, employ the assignment mechanism described in Assumptions \ref{ass:a}--\ref{ass:pair} with $\ell/k = \eta_s$. For more general functions $\eta(X_i)$, we conjecture that we could employ the local randomization procedure proposed in \cite{cytrynbaum2023designing}.
\end{remark}

\section{An Efficiency Bound and the Necessity of ``Fast-Balancing''}\label{sec:semi-reg}
In Section \ref{sec:semi} we establish that $\mathbb{V}_{\ast}$ is the efficiency bound for a large class of experimental designs. As a consequence, we can conclude that finely stratified designs are asymptotically efficient ``by design.'' Building on this result, in Section \ref{sec:regular} we establish that a necessary condition for achieving the bound $\mathbb{V}_{\ast}$ when estimating $\theta_0$ using the na\"ive method of moments estimator is that the experimental design be fast-balancing, in the sense of \eqref{eq:fast-m}.

\subsection{Efficiency Bound}\label{sec:semi}
An inspection of the asymptotic variance in \eqref{eq:Vstar} reveals that $\mathbb V_\ast$ in fact coincides with the classical efficiency bound for estimating $\theta_0$ with i.i.d.\ assignment. For example, the variance derived in \eqref{eq:var-ate} coincides with the efficiency bound derived in \cite{hahn1998role} for estimating the ATE with a known marginal treatment probability $\eta$.   Therefore, another way to interpret our result in Theorem \ref{thm:normal} is that the standard i.i.d.\ efficiency bound can be attained by a na\"ive method of moments estimator under a finely stratified design. On the other hand, because treatment status is not independent in a finely stratified design, a natural follow-up question is whether or not the efficiency bound for estimating $\theta_0$ changes relative to what can be obtained under i.i.d.\ assignment once we allow for more general assignment mechanisms. In this section, we show that $\mathbb{V}_*$ continues to be the efficiency bound for the class of parameters introduced in Section \ref{sec:setup}, while allowing for a more general class of treatment assignment mechanisms.  The main restriction on treatment assignment is given by Assumption \ref{ass:unconfounded}, which requires the marginal treatment probability to be known and equal to $\eta$.  As mentioned earlier and explained in Remark \ref{rem:generaleta} below, it is possible to relax this requirement so that $\eta$ can be replaced by a known function $\eta(X_i)$.  For a discussion of how our efficiency bound compares with other results in the literature, see Remark \ref{rem:other_SPEB}. 

We impose the following high-level assumption on the assignment mechanism:

\begin{assumption} \label{ass:LLN}
The treatment assignment mechanism is such that for any integrable function $\gamma: \mathbf{R}^{d_x} \to \mathbf{R}$,
\[ \frac{1}{n} \sum_{1 \leq i \leq n} A_i \gamma(X_i) \stackrel{P}{\to} \eta E[\gamma(X_i)]~. \]
\end{assumption}
\noindent In words, Assumption \ref{ass:LLN} requires that the assignment mechanism admits a law of large numbers for integrable functions of the covariate values. Examples \ref{ex:iid}--\ref{ex:rerand} illustrate that the assumption holds for common treatment assignment mechanisms used in practice.

\begin{example}[i.i.d.\ assignment]\label{ex:iid}
Let $A^{(n)}$ be assigned i.i.d., independently of $X^{(n)}$, such that $P\{A_i = 1\} = \eta$. Then it follows immediately by the law of large numbers that Assumption \ref{ass:LLN} is satisfied.
\end{example}

\begin{example}[Covariate-adaptive randomization (CAR)]\label{ex:CAR}
Let $S: \mathbf R^{d_x} \to \mathcal S = \{1, \ldots, |\mathcal S|\}$ be a function that maps the covariates into a set of discrete strata. Define $S_i = S(X_i)$ and assume that treatment status is assigned so that
\[ (R^{(n)}(1), R^{(n)}(0), X^{(n)}) \indep A^{(n)} \big | S^{(n)}~, \]
and that for $s \in \mathcal S$,
\[ \frac{\sum_{1 \leq i \leq n} I \{S_i = s, A_i = 1\}}{\sum_{1 \leq i \leq n} I \{S_i = s\}} \stackrel{P}{\to} \eta~. \]
This high-level assumption accommodates a large class of stratified assignment mechanisms, including stratified biased coin designs \citep[]{efron1971forcing, wei1978adaptive}, minimization methods \citep[][]{pocock1975sequential,hu2012asymptotic} and stratified block randomization \citep[see][for an early discussion]{zelen1974randomization}. It follows from Lemma C.4 in \cite{bugni2019inference} that for any integrable function $\gamma(\cdot)$,
\[ \frac{1}{n} \sum_{1 \leq i \leq n} A_i \gamma(X_i) \stackrel{P}{\to} 
\eta \sum_{s \in \mathcal S} P \{S_i = s\} E[\gamma(X_i)| S_i = s] = \eta E[\gamma(X_i)]~. \]
Therefore, Assumption \ref{ass:LLN} is satisfied.
\end{example}

\begin{example}[CAR with general covariate features] \label{ex:ma}
\cite{ma2024new} propose a family of covariate adaptive randomization procedures which assign treatment sequentially based on an imbalance metric defined by feature maps of (potentially continuous) covariates. It follows by Theorem 3.5 of their paper that Assumption \ref{ass:LLN} is satisfied under appropriate conditions.
\end{example}

\begin{example}[Matched pairs] \label{ex:MP}
Suppose $n$ is even and we assign treatment using a finely stratified design with $(\ell, k) = (1,2)$. As discussed at the beginning of Section \ref{sec:variance}, such a design is also known as a matched pairs design. Assume that the pairing algorithm $\pi_n(X^{(n)})$ results in pairs that are close in the sense of Assumption \ref{ass:pair}. It then follows from the same argument used to establish \eqref{eq:fast-m} in the discussion following Theorem \ref{thm:normal} that for any Lipschitz integrable function $\gamma(\cdot)$,
\[ \frac{1}{n} \sum_{1 \leq i \leq n} A_i \gamma(X_i) \stackrel{P}{\to} \frac{1}{2} E[\gamma(X_i)]~. \]
By approximating integrable functions by Lipschitz integrable functions as in Lemma A.1 in \cite{hanneke2021universal}, it can be shown that the convergence holds for any integrable function $\gamma(\cdot)$. Therefore, Assumption \ref{ass:LLN} is satisfied.
\end{example}

\begin{example}[Re-randomization]\label{ex:rerand}
Re-randomization is an assignment mechanism in which researchers specify a balance criterion for the covariates, and then repeatedly generate assignments using a completely randomized design until an assignment is found which achieves an acceptable covariate distribution according to the balance criterion. The properties of re-randomization procedures have been studied in \cite{li2017general,li2020rerandomization}, \cite{li2018asymptotic,li2020rerandomization-1}, and \cite{cytrynbaum2024finely}. It follows from Corollary 3.7 in \cite{cytrynbaum2024finely} that Assumption \ref{ass:LLN} holds for re-randomization designs, under appropriate assumptions.
\end{example}

We now present an efficiency bound for the parameter $\theta_0$ introduced in Section \ref{sec:setup}. Formally, we characterize the bound via a convolution theorem that applies to all regular estimators of the parameter $\theta_0$. In stating our theorem we leave the precise definition of regularity and related assumptions to Appendix \ref{sec:SPEB}. In the paragraph following the statement of the theorem we provide some more details on the nature of our result.

\begin{theorem} \label{thm:efficiencybound}
Suppose Assumptions \ref{ass:unconfounded}, \ref{ass:normal}(b), and \ref{ass:LLN} hold, as well as Assumption \ref{ass:bounded_path} described in Appendix \ref{sec:SPEB}. Further suppose $\mathbb V_\ast < \infty$. Let $\tilde{\theta}_n$ be any regular estimator of the parameter $\theta_0$ in the sense of \eqref{eq:regular} in Appendix \ref{sec:SPEB}. Then,  
\[\sqrt{n}(\tilde{\theta}_n - \theta_0) \xrightarrow{d} L~,\]
where
\[L = N(0, \mathbb V_\ast) \ast B~, \]
for $\mathbb V_\ast$ in \eqref{eq:Vstar} and some fixed probability measure $B$ which is specific to the estimator $\tilde{\theta}_n$.
\end{theorem}

Given Theorem \ref{thm:efficiencybound} we call $\mathbb V_\ast = \var[\psi^\ast(X_i, A_i,R_i, \theta_0)]$ the efficiency bound for $\theta_0$, since our result shows that this is the lowest asymptotic variance attainable by any regular estimator under our assumptions. We note that our assumptions on the assignment mechanism preclude us from immediately appealing to standard semi-parametric convolution theorems \citep[see, for instance, Theorem~25.20 in][]{van_der_vaart1989asymptotic}. Instead, we proceed by justifying an application of Theorem 3.1 in \cite{armstrong2022asymptotic} combined with the convolution Theorem 3.11.2 in \cite{van_der_vaart1996weak} to each $d_\theta$-dimensional parametric submodel separately, and then arguing that the supremum over all such submodels is attained by $\var[\psi^{\ast}]$. A key observation is that in order to apply Theorem 3.1 in \cite{armstrong2022asymptotic} to argue that the likelihood ratio process is locally asymptotically normal, the conditional information needs to settle down in the limit, which is guaranteed as long as Assumption \ref{ass:LLN} is satisfied.

\begin{remark}\label{rem:psi_examples}
Following similar arguments as those in Remark \ref{rem:psi_examples_ate}, we can deduce that our efficiency bound recovers well-known bounds for common parameters (like those presented in Examples \ref{ex:ATE}--\ref{ex:LATE}) in the setting of i.i.d.\ assignment. For example, we have noted in the case of the ATE (Example \ref{ex:ATE}) that \eqref{eq:var-ate} matches the efficiency bound under i.i.d.\ assignment derived in \cite{hahn1998role}. See \cite{rafi2023efficient} and \cite{armstrong2022asymptotic} for related results in the context of stratified and response-adaptive experiments.  Straightforward calculation also implies that, for the quantile treatment effect (Example \ref{ex:QTE}), the efficiency bound is given by
\begin{multline*}
    E \bigg [ \frac{1}{\eta} \frac{F_1 \big (\theta_0(1) | X_i \big ) \big (1 - F_1 \big (\theta_0(1) | X_i \big ) \big )}{f_1 \big (\theta_0(1) \big )^2} + \frac{1}{1- \eta} \frac{F_0 \big (\theta_0(0) | X_i \big ) \big (1 - F_0 \big (\theta_0(0) | X_i \big ) \big )}{f_0 \big (\theta_0(0) \big )^2} \\
    + \bigg ( \frac{F_1 \big (\theta_0(1) | X_i \big ) - \tau}{f_1 \big (\theta_0(1) \big )} - \frac{F_0 \big (\theta_0(0) | X_i \big ) - \tau}{f_0 \big (\theta_0(0) \big )} \bigg)^2 \bigg ]~,
\end{multline*}
which matches the efficiency bound under i.i.d.\ assignment derived in \cite{firpo2007efficient} when the propensity score is set to $\eta$. 
\end{remark}

\begin{remark} \label{rem:adj}
The efficiency bound in Theorem \ref{thm:efficiencybound} is attained by finely stratified experiments as in Theorem \ref{thm:normal} if no additional covariates are available for estimation beyond the set of covariates $X_i$ used in the design. In practice, researchers may consider adjusting for additional baseline covariates in order to improve efficiency. Suppose additional covariates $W^{(n)}$ are available and Assumption \ref{ass:a} is modified such that
\[ (R^{(n)}(1), R^{(n)}(0), W^{(n)}) \indep A^{(n)} \big | X^{(n)}~. \]
When $d_\theta = 1$, it can be shown that the efficiency bound, allowing for additional covariate adjustment based on $X_i$ and $W_i$, is
\begin{equation} \label{eq:adj}
    \mathbb V_\ast - \eta(1 - \eta) M^{-1} E[\var[E[g_i|X_i,W_i]|X_i]] (M^{-1})'~,
\end{equation}
where $g_i = m(X_i, 1, R_i(1), \theta_0) - m(X_i, 0, R_i(0), \theta_0)$. Then, as in Remark \ref{rem:V_compare}, the gain in efficiency obtained by adjusting for $X_i$ and $W_i$ in equation \eqref{eq:adj} is proportional to
\[ E[R_{g, X, W}^2(g_i, X_i, W_i) \var[g_i | X_i]]~, \]
where
\begin{equation*}
R_{g, X, W}^2(g_i, X_i, W_i) = \frac{\var[E[g_i|X_i,W_i]|X_i]}{\var[g_i | X_i]}
\end{equation*}
is the nonparametric $R^2$ from regressing $g_i$ on $X_i$ and $W_i$, when matching on the variables $X_i$. As a result, the scope for improving efficiency by adjusting for additional covariates is limited if $R_{g, X, W}^2$ is small. In the case of estimating the ATE,
\[ g_i = \frac{Y_i(1)}{\eta} + \frac{Y_i(0)}{1 - \eta}~, \]
so the scope for improvement depends on how much additional variation in the weighted potential outcomes can be explained by $W_i$ beyond $X_i$. If researchers select matching variables for which they believe $R_{g, X, W}^2$ to be small, then the additional gain from adjusting for the remaining covariates will necessarily be limited.
\end{remark}

\begin{remark} \label{rem:generaleta}
Although we focus on the case where $\eta_i(X_i) = P\{A_i = 1|X_i\} = \eta$ is a constant, the proof of Theorem \ref{thm:efficiencybound} holds when $\eta_i(x) = \eta(x)$ for $1 \leq i \leq n$, where $\eta(x)$ is an arbitrary known and fixed function. In these settings, Lemma \ref{lem:differentiability} shows that the efficiency bound equals
\begin{align}
    \label{eq:var-generaleta} \mathbb V_\ast & = \var[\psi^\ast(X_i, A_i,R_i, \theta_0)] \\
    \nonumber & = M^{-1} \big ( E \big [ \eta(X_i) \var[m(X_i, 1, R_i(1), \theta_0) | X_i] + (1 - \eta(X_i)) \var[m(X_i, 0, R_i(0), \theta_0) | X_i] \big ] \\
    \nonumber & \hspace{5em} + \var \big [ \eta(X_i) E[m(X_i, 1, R_i(1), \theta_0) | X_i] + (1 - \eta(X_i)) E[m(X_i, 0, R_i(0), \theta_0) | X_i] \big ] \big ) (M^{-1})'~,
\end{align}
so that the only difference from \eqref{eq:var} is that $\eta$ is replaced by $\eta(X_i)$.  Consider Example \ref{ex:ATE} and note the moment condition for the ATE is now given by
\begin{equation} \label{eq:moments-ate-generaleta}
   m(X_i, A_i, R_i, \theta) = \frac{Y_i A_i}{\eta(X_i)} - \frac{Y_i(1 - A_i)}{1 - \eta(X_i)} - \theta~. 
\end{equation}
Straightforward calculation implies that in this example, the efficiency bound in \eqref{eq:var-generaleta} becomes
\begin{equation} \label{eq:ATE-bound-generaleta}
   E\left[\frac{\var[Y_i(1)|X_i]}{\eta(X_i)} + \frac{\var[Y_i(0)|X_i]}{1 - \eta(X_i)} + \left(E[Y_i(1)- Y_i(0)|X_i] - E[Y_i(1) - Y_i(0)]\right)^2\right]~,
\end{equation}
which again matches the efficiency bound under i.i.d.\ assignment in \cite{hahn1998role}. If we additionally impose that $\eta(X_i) = \eta(S(X_i))$ for $S$ taking on finitely many values as in Example \ref{ex:CAR}, then the bound could be achieved by employing the modified design described in Remark \ref{rem:generaleta-attained}.
\end{remark}

\begin{remark}\label{rem:other_SPEB}
Here, we comment on how Theorem \ref{thm:efficiencybound} relates to prior efficiency bounds in experiments with general assignment mechanisms. For the case of estimating the ATE, \cite{armstrong2022asymptotic} derives an efficiency bound over a very large class of assignment mechanisms, including even response-adaptive designs, and shows that the bound is attained when units are assigned to treatment (control) with conditional probability proportional to the conditional standard deviation of the potential outcome under treatment (control).  This type of assignment is sometimes referred to as the Neyman allocation.  On the other hand, our results show that his bound may be quite loose whenever the assignment proportions are restricted to be anything not equal to the Neyman allocation, which is, of course, unknown.  For example, his bound is not informative about what can be achieved if the assignment proportions were set to one half regardless of whether or not the conditional outcome variances across treatment and control are equal.  Such settings frequently arise in practice due to logistical constraints or the absence of pilot data with which to estimate conditional variances of potential outcomes under treatment and control.  Furthermore, as argued in  \cite{cai2022performance}, even if pilot data is available, these quantities may be estimated so poorly that exogenously constraining the assignment proportions to one half leads to more efficient estimates of the ATE in practice.  Motivated by such concerns, \cite{rafi2023efficient} derives an efficiency bound for the ATE over the class of ``coarsely-stratified'' assignment mechanisms studied in \cite{bugni2019inference}, where the stratum-level assignment proportions are restricted {\it a priori} by the experimenter. This framework, however, rules out finely stratified designs. Finally, we once again emphasize that our analysis, unlike these other papers, applies to a general class of treatment effect parameters, including the ATE as a special case.
\end{remark}

\subsection{The Necessity of ``Fast-Balancing''} \label{sec:regular}
In this subsection, we provide conditions under which the fast-balancing condition described in \eqref{eq:fast-m} is a necessary condition for efficient estimation of $\theta_0$ ``by design,'' when using the na\"ive method of moments estimator. As a supplement, we also provide necessary and sufficient conditions for an asymptotically linear estimator to be regular (in the sense of \eqref{eq:regular} in Appendix \ref{sec:SPEB}) for a large class of treatment assignment mechanisms. Concretely, given an assignment mechanism, suppose $\tilde \theta_n$ is an asymptotically linear estimator for $\theta_0$ in the sense that
\begin{equation} \label{eq:linear}
\sqrt n (\tilde \theta_n - \theta_0) = \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} \psi(X_i, A_i, R_i, \theta_0) + o_P(1)~,
\end{equation}
where $E[\psi(X_i, A_i, R_i, \theta_0)] = 0$ and $\var[\psi(X_i, A_i, R_i, \theta_0)] < \infty$. The results in this section derive necessary and sufficient conditions for $\tilde \theta_n$ to be regular, and further demonstrate that in order for $\tilde{\theta}_n$ to be regular and efficient, either $\psi = \psi^\ast$ or a fast-balancing condition involving $\psi(\cdot)$ needs to be satisfied. Therefore, finely stratified designs are not only sufficient to guarantee efficiency when estimating $\theta_0$ using the na\"ive method of moments estimator, but their fast-balancing property is also necessary.

In order to study the behavior of the estimator under local alternatives, we will impose the following high-level assumption on the ``imbalance'' of the treatment assignments. To describe the assumption, let $\rho$ denote any metric that metrizes weak convergence.
\begin{assumption} \label{ass:imbalance}
The treatment assignment mechanism is such that for any square-integrable function $\gamma: \mathbf R^{d_x} \to \mathbf R^{d_\theta}$ with $E[\gamma(X_i)] = 0$ ,
\[ \rho \bigg ( \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} (A_i - \eta) \gamma(X_i), ~N(0, V_\gamma^{\rm imb}) \bigg \vert X^{(n)} \bigg ) \xrightarrow{P} 0 \]
for some deterministic variance $V_\gamma^{\rm imb}$.
\end{assumption}

In the following examples, we verify Assumption \ref{ass:imbalance} for some common treatment assignment mechanisms.

\begin{example}
Revisiting Example \ref{ex:iid}, let $A^{(n)}$ be assigned i.i.d., independently of $X^{(n)}$, such that $P \{A_i = 1\} = \eta$. Then by verifying the conditions of the Lindeberg-Feller CLT conditional on $X^{(n)}$, it can be shown that Assumption \ref{ass:imbalance} is satisfied with $V^{\rm imb}_\gamma = \eta (1 - \eta) \var[\gamma(X_i)]$.
\end{example}

\begin{example}
Revisiting Example \ref{ex:CAR}, suppose treatment status is assigned using stratified block randomization, which is a special case of covariate-adaptive randomization where $A^{(n)}$ is such that \[\sum_{1 \leq i \leq n} A_i I \{S_i = s\} = \bigg\lfloor \eta \sum_{1 \leq i \leq n} I \{S_i = s\} \bigg\rfloor~,\] with all such assignments being drawn uniformly at random and independently across strata. It follows from Theorem 12.2.1 in \cite{lehmann2022testing} combined with a subsequencing argument that Assumption \ref{ass:imbalance} is satisfied with $V_\gamma^{\rm imb} = \eta (1 - \eta) E[\var[\gamma(X_i) | S_i]]$.
\end{example}

\begin{example}
Revisiting Example \ref{ex:ma}, suppose treatment is assigned using the covariate adaptive randomization procedure described in \cite{ma2024new}. Then it follows from Theorem 3.6 in their paper that, under appropriate assumptions,
\[\frac{1}{\sqrt{n}}\sum_{1 \le i \le n}(A_i - \eta)\gamma(X_i) \xrightarrow{d} N(0, \tilde{V})~,\]
for some limiting variance $\tilde{V}$. Note, however, that this result is \emph{not} conditional on $X^{(n)}$ and thus does not immediately imply Assumption \ref{ass:imbalance}. We conjecture that a similar result could be established conditional on $X^{(n)}$ and thus Assumption \ref{ass:imbalance} would be satisfied.\footnote{We thank Wei Ma for discussing this conjecture with us.}
\end{example}

\begin{example} \label{ex:MP2}
Revisiting Example \ref{ex:MP}, suppose $n$ is even and we assign treatment using a matched pairs design. It then follows by arguing as in the discussion following Theorem \ref{thm:normal} that for any square-integrable Lipschitz function $\gamma(\cdot)$,
\[ \var\bigg[\frac{1}{\sqrt{n}} \sum_{1 \leq i \leq n}(A_i - \eta) \gamma(X_i)\bigg\vert X^{(n)}\bigg] \stackrel{P}{\to} 0~. \]
Therefore, by Markov's inequality, Assumption \ref{ass:imbalance} is satisfied with
$V_\gamma^{\rm imb} = 0$. By approximating square-integrable functions by square-integrable Lipschitz functions as in Lemma C.5 in \cite{cytrynbaum2023designing}, it can be shown that the convergence holds for any square-integrable function $\gamma(\cdot)$. 
\end{example}

\begin{example}\label{ex:rerand2}
Revisiting Example \ref{ex:rerand}, we note that, following Corollary 3.7 in \cite{cytrynbaum2024finely}, we do not expect Assumption \ref{ass:imbalance} to hold for re-randomization designs in general.
\end{example}

We are now ready to state a theorem that characterizes all regular asymptotically linear estimators and establishes the necessity of the fast-balancing condition for efficient estimation using $\hat{\theta}_n$.

\begin{theorem} \label{thm:regular}
Suppose the treatment assignment mechanism satisfies Assumptions \ref{ass:unconfounded} and \ref{ass:LLN}--\ref{ass:imbalance}. Suppose $\tilde \theta_n$ is an asymptotically linear estimator for $\theta_0$ in the sense of \eqref{eq:linear}. Then, $\tilde \theta_n$ is regular if and only if
\[ \psi(x, a, r, \theta_0) = \psi^\ast(x, a, r, \theta_0) + \psi^\perp(x, a, \theta_0)~, \]
for some function $\psi^\perp$ such that $E[\psi^\perp(X_i, A_i, \theta_0) | X_i] = \eta \psi^\perp(X_i, 1, \theta_0) + (1 - \eta) \psi^\perp(X_i, 0, \theta_0) = 0$. Furthermore, if $\tilde \theta_n$ is regular, it attains the efficiency bound if and only if
\begin{equation} \label{eq:fast}
\frac{1}{\sqrt n} \sum_{1 \leq i \leq n} (A_i - \eta) E[\psi(X_i, 1, R_i(1), \theta_0) - \psi(X_i, 0, R_i(0), \theta_0) | X_i] = o_P(1)~.
\end{equation}
\end{theorem}

To further understand condition \eqref{eq:fast}, note that $E[\psi^\ast(X_i, 1, R_i(1)) | X_i] = E[\psi^\ast(X_i, 0, R_i(0)) | X_i]$, so
\begin{align*}
& \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} (A_i - \eta) E[\psi(X_i, 1, R_i(1), \theta_0) - \psi(X_i, 0, R_i(0), \theta_0) | X_i] \\
& = \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} (A_i - \eta) (\psi^\perp(X_i, 1, \theta_0) - \psi^\perp(X_i, 0, \theta_0)) \\
& = \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} \psi^\perp(X_i, A_i, \theta_0)~,
\end{align*}
where the last equality follows from the fact that $E[\psi^\perp(X_i, A_i, \theta_0) | X_i] = 0$. As a result, \eqref{eq:fast} holds either when the estimator is based on the efficient influence function, so that $\psi^\perp = 0$, or when the treatment assignment mechanism groups units with similar values of $\psi^\perp(X_i, 1, \theta_0) - \psi^\perp(X_i, 0, \theta_0)$. Recall that as an intermediate step in the proof of Theorem \ref{thm:normal}, we showed in \eqref{eq:vdv} that for the na\"ive method of moments estimator $\hat \theta_n$, $\psi(\cdot) = - M^{-1} m(\cdot)$, so \eqref{eq:fast} coincides with the fast-balancing condition in \eqref{eq:fast-m}. We can thus conclude from Theorem \ref{thm:regular} that the fast-balancing condition is necessary to achieve efficient estimation based on the na\"ive method of moments estimator, when the class of assignment mechanisms satisfy Assumption \ref{ass:imbalance}.

\begin{example}
Revisiting Example \ref{ex:ATE}, recall $\hat \theta_n$ estimates the ATE based on the moment conditions in \eqref{eq:moments-ate}. Direct calculation shows that for $\hat \theta_n$,
\[ \psi^\perp(x, a, \theta_0) = (a - \eta) \Big ( \frac{\mu_1(x)}{\eta} + \frac{\mu_0(x)}{1 - \eta} \Big )~. \]
As a result, Theorem \ref{thm:regular} demonstrates that $\hat{\theta}_n$ does not achieve the efficiency bound, unless
\[ \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} (A_i - \eta) \left ( \frac{\mu_1(X_i)}{\eta} + \frac{\mu_0(X_i)}{1 - \eta} \right ) = o_P(1)~, \]
which is indeed the case in finely stratified experiments when the treatment assignment mechanism satisfies Assumption \ref{ass:pair}. \end{example}

\section{Simulations}\label{sec:sims}
In this section, we illustrate the results in Sections \ref{sec:variance} and \ref{sec:semi} with a simulation study. Specifically, we set $\eta = 1/2$, and compare the mean-squared errors (MSE) obtained from the naive estimator $\hat{\theta}_n$ and various adjusted estimators, for i.i.d.\ treatment assignment versus matched pairs assignment (see Remark \ref{rem:matched_pairs} and  Example \ref{ex:MP}).  In Section \ref{sec:sims_ATE}, we present the model specifications and estimators for estimating the ATE as in Example \ref{ex:ATE}. In Section \ref{sec:sims_LATE}, we present the model specifications and estimators for estimating the LATE as in Example \ref{ex:LATE}. Section \ref{sec:sims_results} reports the simulation results.
 
\subsection{Average Treatment Effect}\label{sec:sims_ATE}
In this section, we present model specifications and estimators for estimating the ATE as in Example \ref{ex:ATE}. Recall that in this case the moment function we consider is given by  
\begin{equation*}
    m(X_i, R_i, A_i, \theta) = \frac{Y_i A_i}{\eta} - \frac{Y_i (1 - A_i)}{1 - \eta} - \theta~,
\end{equation*}
with $R_i = Y_i$. For $a \in \{0, 1\}$ and $1\leq i \leq n$, the potential outcomes are generated according to the equation:
\begin{equation*}
Y_i(a) = \mu_a(X_i) + \sigma_a(X_i) \epsilon_{i}~.
\end{equation*}
In each of the specifications, $((X_i, \epsilon_{i}): 1\leq i \leq n)$ are i.i.d; for $1 \leq i \leq n$, $X_i$ and $\epsilon_{i}$ are independent.
\begin{enumerate}[{\bf Model} 1:]
	\item $\mu_{0}(X_i) = X_i + (X_i^2 - 1)/3$, $\mu_{1}(X_i) = 0.2 + \mu_0(X_i)$, $\epsilon_{i} \sim N(0, 1)$, $X_i \sim N(0, 1)$ and $\sigma_a(X_i) = 2$.
	\item As in Model 1, but $\mu_a(X_i) = 0.2I\{a = 1\} + \gamma_a(\sin(X_i) +  X_i) + (X_i^2 - 1)/3$ where $\gamma_1 = 1$ and $\gamma_0 = -1$, and $\sigma_a(X_i) = (1 + a)X_i^2$.
    \item As in Model 2, but $\mu_1(X_i) = 0.2 + 3(X_i^2-1)$ and $\mu_0(X_i) = 0$.
\end{enumerate}

We consider the following three estimators for the ATE:

\begin{enumerate}
    \item[] \textbf{Unadjusted Estimator}:
    \begin{equation*}
        \hat\theta_n^{\rm unadj} = \frac{1}{n / 2} \sum_{1\leq i \leq n} ( Y_i A_i - Y_i(1-A_i) )~.
    \end{equation*}
    \item[] \textbf{Adjusted Estimator 1}:
    \begin{equation*}
        \hat\theta_n^{\rm adj, 1} = \frac{1}{n} \sum_{1 \leq i \leq n} \big ( 2 A_i(Y_i - \hat\mu_1^Y(X_i)) - 2(1- A_i)(Y_i - \hat\mu_0^Y(X_i)) + \hat\mu_1^Y(X_i) - \hat\mu_0^Y(X_i) \big )~,
    \end{equation*}
    where $\hat\mu_a^Y(X_i)$ is constructed by running a least squares regression of $Y_i$ on $(X, 1_i, X_i^2)$ using the sample from $A_i=a$.
    \item[] \textbf{Adjusted Estimator 2}: 
     \begin{equation*}
        \hat\theta_n^{\rm adj, 2} = \frac{1}{n} \sum_{1 \leq i \leq n} \big ( 2 A_i(Y_i - \hat\mu_1^Y(X_i)) - 2(1- A_i)(Y_i - \hat\mu_0^Y(X_i)) + \hat\mu_1^Y(X_i) - \hat\mu_0^Y(X_i) \big )~,
    \end{equation*}
    where $\hat\mu_a^Y(X_i)$ is constructed by running a least squares regression of $Y_i$ on $(X, 1_i, X_i^2, X_i 1\{X_i > t\})$ where $t$ is the sample median using the sample from $A_i = a$.
\end{enumerate}

The first estimator $\hat\theta_n^{\rm unadj}$ is the method of moments estimator given by the solution to \eqref{eq:est}. The second and third estimators $\hat\theta_n^{\rm adj,1}$ and $\hat\theta_n^{\rm adj,2}$  are covariate-adjusted estimators which can be obtained as two-step method of moments estimators from solving the ``augmented'' moment equation \eqref{eq:augment_m} described in the discussion at the end of Section \ref{sec:setup}. $\hat\theta_n^{\rm adj,1}$ and $\hat\theta_n^{\rm adj,2}$ differ in the choice of basis functions used in the construction of the estimators $\hat{\mu}_a(x)$. Note that by the double-robustness property of the augmented estimating equation \eqref{eq:augment_m}, it can be shown that the adjusted estimators $\hat{\theta}_n^{\rm adj,1}$, $\hat{\theta}_n^{\rm adj,2}$ are consistent and asymptotically normal regardless of the choice of estimators $\hat{\mu}_a(x)$, but consistency of $\hat{\mu}_a(x)$ to $\mu_a(x)$ would ensure that $\hat{\theta}_n^{\rm adj,1}$, $\hat{\theta}_n^{\rm adj,2}$ are efficient under i.i.d. assignment \citep[][]{robins1995analysis, tsiatis2008covariate, chernozhukov2017doubledebiasedneyman}.

\subsection{Local Average Treatment Effect}\label{sec:sims_LATE}
In this section, we present the model specifications and estimators for estimating the LATE as in Example \ref{ex:LATE}. Recall that in this case the moment condition we consider is given by 
\begin{equation*}
    m(X_i, A_i, R_i, \theta) = \frac{Y_i A_i}{\eta} - \frac{Y_i (1 - A_i)}{1 - \eta} - \theta \left ( \frac{D_i A_i}{\eta} - \frac{D_i (1 - A_i)}{1 - \eta} \right )~,
\end{equation*}
with $R_i = (Y_i, D_i)$. The outcome is determined by the relationship $Y_i = D_i Y_i(1) + (1-D_i)Y_i(0)$, where $Y_i(d) = \mu_d(X_i) + \sigma_a(X_i) \epsilon_{i}$ follows the same outcome model as in the ATE setup of Section \ref{sec:sims_ATE}. In addition, we have $D_i = A_i D_i(1) + (1-A_i)D_i(0)$, where
\begin{align*}
    D_i(0)&=I\left\{\alpha_0+\alpha\left(X_i\right)> \varepsilon_{1, i}\right\}~, \\
    D_i(1)&=\begin{cases}
I\left\{\alpha_1+\alpha\left(X_i\right)> \varepsilon_{2, i}\right\} & \text { if } D_i(0)=0 \\
1 & \text { otherwise }
\end{cases}~.
\end{align*}
For each outcome model, we set $\alpha_0 = 0.5$, $\alpha_1 = 1$, $\alpha(X_i) = X_i + (X_i^2 - 1)/3$ and $\varepsilon_{1, i}, \varepsilon_{2, i} \sim N(0, 4)$.

We consider the following three estimators for the LATE:
\begin{enumerate}
    \item[] \textbf{Unadjusted Estimator}:
    \begin{equation*}
        \hat\theta_n^{\rm unadj} = \frac{\sum_{1\leq i \leq n} \left ( Y_i A_i - Y_i (1 - A_i) \right )}{\sum_{1\leq i \leq n} \left ( D_i A_i - D_i (1 - A_i) \right )}~.
    \end{equation*}
    \item[] \textbf{Adjusted Estimator 1}:
    \begin{equation*}
        \hat\theta_n^{\rm adj,1} = \frac{\sum_{1 \leq i \leq n} \left ( 2 A_i(Y_i - \hat\mu_1^Y(X_i)) - 2(1- A_i)(Y_i - \hat\mu_0^Y(X_i)) + \hat\mu_1^Y(X_i) - \hat\mu_0^Y(X_i) \right )}{\sum_{1 \leq i \leq n} \left ( 2 A_i(D_i - \hat\mu_1^Y(X_i)) - 2(1- A_i)(D_i - \hat\mu_0^Y(X_i)) + \hat\mu_1^D(X_i) - \hat\mu_0^D(X_i) \right )}~,
    \end{equation*}
    where $\hat\mu_a^Y(X_i)$ is estimated by running a least squares regression of $Y_i$ on $(X, 1_i, X_i^2)$ using the sample from $A_i=a$, and $\hat\mu_a^D(X_i)$ is estimated using logistic regressions using the same set of regressors using the sample from $A_i = a$.
    \item[] \textbf{Adjusted Estimator 2}: 
    As in Adjusted Estimator 1, but $\hat\mu_a^Y(X_i)$ and $\hat\mu_a^D(X_i)$ are estimated respectively by running a least squares and logistic regressions of $Y_i$ on $(X, 1_i, X_i^2, X_i 1\{X_i > t\})$ where $t$ is the sample median. 
\end{enumerate}
Similarly to Section \ref{sec:sims_ATE}, $\hat{\theta}_n^{\rm unadj}$ solves \eqref{eq:est} for the moment condition given in \eqref{eq:moments-late}. The second and third estimators are covariate adjusted estimators which can be obtained as two-step method of moments estimators from solving an ``augmented'' version of the moment condition \eqref{eq:moments-late} \citep[see, for instance,][]{chernozhukov2018doubledebiased, jiang2022improving}.

\subsection{Simulation Results}\label{sec:sims_results}
Table \ref{table:sims} displays the ratio of the MSE for each design/estimator pair relative to the MSE of the unadjusted estimator under i.i.d. assignment, computed across $2000$ Monte Carlo replications. As expected given our theoretical results, we find that the empirical MSEs of the naive unadjusted estimator under a matched pairs design closely match the empirical MSEs of the covariate adjusted estimators under i.i.d. assignment. 

\begin{table}[ht!]
\centering
\begin{threeparttable}
\caption{MSE ratios relative to unadjusted estimator under i.i.d.\ assignment}
\begin{tabular}{cccccccc}
\toprule
& & & \multicolumn{3}{c}{\textbf{I.I.D.\ assignment}} & & \textbf{Matched pairs} \\ \cmidrule{4-6} \cmidrule{8-8}
& & Model & $\boldsymbol{\text{Unadjusted}}^{}$ & $\text{Adjusted 1}$ & $\text{Adjusted 2}$ & & $\text{Unadjusted}$ \\ \midrule
\multirow{6}{*}{$n=200$} & \multirow{3}{*}{ATE} &  1 & 1.0000 & 0.4580 & 0.4637 & & 0.4530 \\
& &  2 & 1.0000 & 0.9836 & 1.0090 & & 1.0291 \\
& &  3 & 1.0000 & 0.7473 & 0.7615 & & 0.7415 \\ \addlinespace[0.25em]
& \multirow{3}{*}{LATE} &  1 & 1.0000 & 0.4391 & 0.4175 & & 0.4115 \\
& &  2 & 1.0000 & 0.8967 & 0.9724 & & 0.8813 \\
& &  3 & 1.0000 & 0.5191 & 0.5002 & & 0.4936 \\ \addlinespace[1em]
\multirow{6}{*}{$n=400$} & \multirow{3}{*}{ATE} &  1 & 1.0000 & 0.4616 & 0.4640 & & 0.4471 \\
& &  2 & 1.0000 & 0.9778 & 1.0470 & & 1.0042 \\
& &  3 & 1.0000 & 0.7535 & 0.7364 & & 0.7293 \\ \addlinespace[0.25em]
& \multirow{3}{*}{LATE} &  1 & 1.0000 & 0.4267 & 0.4583 & & 0.4327 \\
& &  2 & 1.0000 & 0.8754 & 0.9570 & & 0.9375 \\
& &  3 & 1.0000 & 0.5671 & 0.5349 & & 0.5240 \\ \addlinespace[1em]
\multirow{6}{*}{$n=1000$} & \multirow{3}{*}{ATE} &  1 & 1.0000 & 0.4518 & 0.4568 & & 0.4453 \\
& &  2 & 1.0000 & 0.9874 & 0.9730 & & 0.9186 \\
& &  3 & 1.0000 & 0.7374 & 0.7099 & & 0.7018 \\ \addlinespace[0.25em]
& \multirow{3}{*}{LATE} &  1 & 1.0000 & 0.4437 & 0.4301 & & 0.4259 \\
& &  2 & 1.0000 & 0.8415 & 0.8735 & & 0.8618 \\
& &  3 & 1.0000 & 0.5408 & 0.4879 & & 0.4884 \\ \addlinespace[1em]
\multirow{6}{*}{$n=2000$} & \multirow{3}{*}{ATE} &  1 & 1.0000 & 0.4677 & 0.4835 & & 0.4867 \\
& &  2 & 1.0000 & 0.9780 & 0.9163 & & 0.9105 \\
& &  3 & 1.0000 & 0.7272 & 0.7552 & & 0.7611 \\ \addlinespace[0.25em]
& \multirow{3}{*}{LATE} &  1 & 1.0000 & 0.4702 & 0.4473 & & 0.4871 \\
& &  2 & 1.0000 & 0.8692 & 0.8545 & & 0.8202 \\
& &  3 & 1.0000 & 0.4973 & 0.5044 & & 0.5079 \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\footnotesize \item Note: For each model, the MSE of the unadjusted estimator under i.i.d.\ assignment are normalized to one and the other columns contain the ratios of the MSEs against that of the unadjusted estimator under i.i.d.\ assignment. MSEs are calculated across 2000 replications.
\end{tablenotes}
\end{threeparttable}
\label{table:sims}
\end{table}

\section{Recommendations for Empirical Practice}\label{sec:recs}
We conclude with some recommendations for empirical practice based on our theoretical results. Overall, our findings highlight the general benefit of fine stratification for designing efficient experiments: finely stratified experiments ``automatically'' perform fully-efficient regression adjustment for a large class of interesting parameters. This generalizes similar observations made by \cite{bai2022inference}, \cite{bai2022optimality} and \cite{cytrynbaum2023designing} for the special case of estimating the ATE. 

Simulation evidence in \cite{bai2024inference}, however, suggests that our asymptotic approximations may be poor when the the dimension of $X_i$ is large relative to the sample size.  For this reason, we recommend that practitioners construct their blocks using a subset of the baseline covariates that they believe have the highest explanatory power in terms of the nonparametric $R^2$ in \eqref{eq:R2}; the baseline level of the experimental outcomes, for example, is typically believed to be one such covariate \citep[see, in particular,][]{bruhn2009pursuit}. The experimental data can then be analyzed efficiently using an unadjusted method-of-moments estimator. 

If one wishes to perform regression adjustment with additional covariates beyond those used for blocking, then this can be done {\it ex post}. As discussed in Remark \ref{rem:adj}, the scope for improvement from covariate adjustment is limited by the nonparametric $R^2$ from the regression of the moment functions on the additional covariates, conditional on the ones used for matching; if one has already matched on the covariates with the highest explanatory power, then the potential gain in efficiency from adjusting for these additional covariates may be limited. We further caution that care must be taken to ensure that the adjustment is performed in such a way that it guarantees a gain in efficiency: see \cite{bai2024covariate} and \cite{cytrynbaum2023covariate} for related discussions. Recent work has developed such methods of covariate adjustment for specific parameters of interest \citep[see, for instance,][]{bai2024inference-2,bai2024covariate, bai2024inference-1, cytrynbaum2023covariate}, but we leave the development of a method of covariate adjustment which applies at the level of generality considered in this paper to future work. 

\clearpage

\appendix

% \begin{center} \Large
%     Supplemental Appendix (For Online Publication)
% \end{center}

\section{Construction of the Variance Estimator}\label{sec:var_est}
For notational convenience we focus on the leading case in which $d_\theta = 1$. Note that a similar construction is valid when $d_\theta > 1$ simply by replacing all quantities with their matrix counterparts. First note that in certain examples (including Examples \ref{ex:ATE} and \ref{ex:LATE}--\ref{ex:log_odds}), the analog principle suggests that a natural estimator for $M$ is given by
    \[ \widehat M_n = \frac{1}{n} \sum_{1 \leq i \leq n} \frac{\partial}{\partial \theta'} m(X_i, A_i, R_i, \theta)\bigg|_{\theta = \hat \theta_n}~. \]
    Under suitable conditions, $\widehat M_n \stackrel{P}{\to} M$.\footnote{In examples including Example \ref{ex:QTE} where $m$ is nonsmooth in $\theta$, $M$ may consist of components which require nonparametric estimators, and in such cases bootstrap procedures may be preferable. See, for instance, \cite{jiang2021bootstrap}.} Therefore, it suffices to construct a consistent estimator for the ``meat'' in \eqref{eq:var}. By the law of total variance, this middle component equals $\Sigma_1 + \Sigma_2$, where
    \begin{align*}
       \Sigma_1 & = \eta \var[m(X_i, 1, R_i(1), \theta_0)] + (1 - \eta) \var[m(X_i, 0, R_i(0), \theta_0)] \\
       \Sigma_2 & = - \eta (1 - \eta) E \big [ \big ( E[m(X_i, 1, R_i(1), \theta_0) | X_i] - E[m(X_i, 1, R_i(1), \theta_0)] \\
       & \hspace{5em} - (E[m(X_i, 0, R_i(0), \theta_0) | X_i] - E[m(X_i, 0, R_i(0), \theta_0)]) \big )^2 \big ] \\
       & = - \eta (1 - \eta) \Big (E[E[m(X_i, 1, R_i(1), \theta_0) | X_i]^2] + E[E[m(X_i, 0, R_i(0), \theta_0) | X_i]^2] \\
       & \hspace{7em} - 2E[E[m(X_i, 1, R_i(1), \theta_0) | X_i] E[m(X_i, 0, R_i(0), \theta_0) | X_i]] \\
       & \hspace{7em} - (E[m(X_i, 1, R_i(1), \theta_0)] - E[m(X_i, 0, R_i(0), \theta_0)])^2  \Big )
    \end{align*}
    For $a \in \{0, 1\}$, define
    \[ \hat \mu_n(a) = \frac{1}{\eta_a n} \sum_{1 \leq i \leq n} I \{A_i = a\} m(X_i, A_i, R_i, \hat \theta_n)~, \]
    where $\eta_1 = \eta$ and $\eta_0 = 1 - \eta$.
    The analog principle again suggests that a natural estimator for $\Sigma_1$ is
    \begin{align*}
        \hat \Sigma_{1, n} = \frac{1}{n} \sum_{1 \leq i \leq n} I \{A_i = 1\} (m(X_i, A_i, R_i, \hat \theta_n) - \hat \mu_n(1))^2 + \frac{1}{n} \sum_{1 \leq i \leq n} I \{A_i = 0\} (m(X_i, A_i, R_i, \hat \theta_n) - \hat \mu_n(0))^2~.
    \end{align*}
    To estimate $\Sigma_2$, we first define
    \begin{align*}
        \hat \varsigma_n(0, 1) & = \frac{k}{n} \sum_{1 \leq j \leq n / k} \frac{1}{\ell(k - \ell)} \sum_{i, i' \in \lambda_j: A_i = 1, A_{i'} = 0} m(X_i, A_i, R_i, \hat \theta_n) m(X_{i'}, A_{i'}, R_{i'}, \hat \theta_n)~.
    \end{align*}
    Next, define
    \[ \hat \varsigma_n(1, 1) = \begin{cases}
        \frac{k}{n} \sum\limits_{1 \leq j \leq n / k} \frac{1}{\binom{\ell}{2}} \sum\limits_{i < i' \in \lambda_j: A_i = A_{i'} = 1} m(X_i, A_i, R_i, \hat \theta_n) m(X_{i'}, A_{i'}, R_{i'}, \hat \theta_n) & \text{ if } \ell > 1 \\
        \frac{2k}{n} \sum\limits_{1 \leq j \leq \frac{n}{2k}} {\sum\limits_{i \in \lambda_{2j}, i' \in \lambda_{2j - 1}: A_i = A_{i'} = 1}} m(X_i, A_i, R_i, \hat \theta_n) m(X_{i'}, A_{i'}, R_{i'}, \hat \theta_n) & \text{ if } \ell = 1~.
    \end{cases} \]
    Similarly, define
    \[ \hat \varsigma_n(0, 0) = \begin{cases}
        \frac{k}{n} \sum\limits_{1 \leq j \leq n / k} \frac{1}{\binom{k - \ell}{2}} \sum\limits_{i < i' \in \lambda_j: A_i = A_{i'} = 0} m(X_i, A_i, R_i, \hat \theta_n) m(X_{i'}, A_{i'}, R_{i'}, \hat \theta_n) & \text{ if } k - \ell > 1 \\
        \frac{2k}{n} \sum\limits_{1 \leq j \leq \frac{n}{2k}} {\sum\limits_{i \in \lambda_{2j}, i' \in \lambda_{2j - 1}: A_i = A_{i'} = 0}} m(X_i, A_i, R_i, \hat \theta_n) m(X_{i'}, A_{i'}, R_{i'}, \hat \theta_n) & \text{ if } k - \ell = 1~.
    \end{cases} \]
Finally, define
\[ \hat \Sigma_{2, n} = - \eta(1 - \eta) (\hat \varsigma_n(1, 1) + \hat \varsigma_n(0, 0) - 2 \hat \varsigma_n(0, 1) - (\hat \mu_n(1) - \hat \mu_n(0))^2)~. \]
The estimator $\hat \varsigma_n(1, 1)$ is constructed in one of two ways depending on the number of treated units in each block. If more than one unit in each block is treated, then we take the averages of all pairwise products of the treated units in each block, and average them across all blocks. We call this a ``within block'' estimator. If instead only one unit in each block is treated, then we take the product of two treated units in \emph{adjacent} blocks. We call this a ``between block'' estimator, and note that similar constructions have been used previously in \cite{abadie2008estimation}, \cite{bai2022inference}, \cite{bai2024inference-1}, and \cite{cytrynbaum2023designing}. The estimator $\hat \varsigma_n(0, 0)$ is constructed similarly. \cite{bai2024inference} compare the finite-sample properties of the ``within block'' and ``between block'' variance estimators via simulation. Their findings are that experimental designs which allow for a ``within block'' variance estimator have better small-sample inferential performance, at the cost of slightly increasing the mean-squared error of the estimator $\hat{\theta}_n$, relative to experimental designs which require the use of the ``between block'' variance estimator. Under suitable assumptions, it follows from similar arguments to those in \cite{bai2022optimality} and \cite{bai2024inference} that $\hat \Sigma_{1, n} \stackrel{P}{\to} \Sigma_1$ and $\hat \Sigma_{2, n} \stackrel{P}{\to} \Sigma_2$.  A natural estimator for $\mathbb{V}_\ast$ is therefore given by
\[\hat{\mathbb{V}}_n = \widehat{M}_n^{-2}\left(\hat{\Sigma}_{1,n} + \hat{\Sigma}_{2,n}\right)~. \]
Thus, provided $M$ is invertible, we have that $\hat{\mathbb{V}}_n \stackrel{P}{\to} \mathbb V_\ast$.
    
\section{Proofs of Main Results}
\subsection{Proof of Theorem \ref{thm:normal}}
First note \eqref{eq:normal_convergence} follows from \eqref{eq:normal} and Lemma \ref{lem:clt}. In particular, the second component of the decomposition therein is zero because $E[\psi^\ast | X, A] = E[\psi^\ast | X]$. To show \eqref{eq:normal}, we first establish \eqref{eq:vdv}, i.e.,
\[ \sqrt n(\hat \theta_n -  \theta_0) = -M^{-1}\frac{1}{\sqrt{n}}\sum_{1 \le i \le n}m(X_i,A_i,R_i,\theta_0) + o_P(1)~. \]
By the proof of Theorem 5.21 in \cite{van_der_vaart1998asymptotic}, to show \eqref{eq:vdv}, it suffices to show
\begin{equation}\label{eq:se}
\mathbb L_n(\hat{\theta}_n) \stackrel{P}{\to} 0~,
\end{equation}
where $\mathbb L_n(\theta) = (\mathbb L^{(1)}_{n}(\theta), \dots, \mathbb L^{(d_\theta)}_{n}(\theta))'$ for
\begin{align*} 
\mathbb L_n^{(s)}(\theta) & = \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} (m_s(X_i, A_i, R_i, \theta) - E_P[m_s(X_i, A_i, R_i, \theta)]) \\
& \hspace{1em} - \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} (m_s(X_i, A_i, R_i, \theta_0) - E_P[m_s(X_i, A_i, R_i, \theta_0)])~.
\end{align*}
To accomplish this, we study $\mathbb L_n^{(s)}(\theta)$ for $1 \leq s \leq d_\theta$ separately. It follows from Assumption \ref{ass:normal}(c)--(d), Proposition 8.11 in \cite{kosorok2008introduction}, and the arguments to establish \eqref{eq:countable} that
\[ \sup_{\theta \in \Theta: \|\theta - \theta_0\| < \delta} |\mathbb L_n^{(s)}(\theta)| = \sup_{\theta \in \Theta^\ast: \|\theta - \theta_0\| < \delta} |\mathbb 
L_n^{(s)}(\theta)|~. \]
Therefore, since $\hat{\theta}_n \stackrel{P}{\to} \theta_0$ by Lemma \ref{lem:consistency}, to show \eqref{eq:se} it suffices to argue that for every $\epsilon > 0$ and every sequence $\delta_n \downarrow 0$ \citep[p.89 of][]{van_der_vaart1996weak},
\begin{equation}\label{eq:L_equicont}
\lim_{n \to \infty}P\left\{\sup_{\theta \in \Theta^\ast:\|\theta - \theta_0\|<\delta_n}\big|\mathbb{L}^{(s)}_n(\theta)\big| > \epsilon\right\} = 0~.
\end{equation}
Following the arguments in the proof of Lemma \ref{lem:consistency}, we decompose $\mathbb L_n^{(s)}(\theta) = \mathbb L_{n, 1}^{(s)}(\theta) + \mathbb L_{n, 0}^{(s)}(\theta)$, where
\begin{align*}
\mathbb L_{n, 1}^{(s)}(\theta) & = \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} A_i (m_s(X_i, 1, R_i(1), \theta) - m_s(X_i, 1, R_i(1), \theta_0) \\
& \hspace{7.5em} - E[m(X_i, 1, R_i(1), \theta) - m_s(X_i, 1, R_i(1), \theta_0)]) \\
\mathbb L_{n, 0}^{(s)}(\theta) & = \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} (1 - A_i) (m_s(X_i, 0, R_i(0), \theta) - m_s(X_i, 0, R_i(0), \theta_0) \\
& \hspace{7.5em} - E[m_s(X_i, 0, R_i(0), \theta) - m_s(X_i, 0, R_i(0), \theta_0)])~.
\end{align*}
Define
\[ \rho_Q(\theta,\theta_0) = E_Q[(m_s(X, a, R(a), \theta) - m_s(X, a, R(a), \theta_0) - E_Q[m_s(X, a, R(a), \theta) - m_s(X, a, R(a), \theta_0)])^2]^{1/2}~. \]
Note by Assumption \ref{ass:normal}(c) that $\rho_Q(\theta, \theta_0)$ is continuous in $\theta$, i.e., as $\|\theta - \theta_0\| \to 0$,
\[ \rho_Q(\theta,\theta_0)\leq E_Q[(m_s(X, a, R(a), \theta) - m_s(X, a, R(a), \theta_0))^2]^{1/2} \to 0~. \]
Fix any sequence $\tilde \delta_n \downarrow 0$. For every $n$, there exists $n'$ such that $\{\theta \in \Theta^\ast: \|\theta - \theta_0\| < \delta_{n'}\} \subseteq \{\theta \in \Theta^\ast: \rho_Q(\theta, \theta_0) < \tilde \delta_n\}$. By Proposition C.1 in \cite{han2021complex},
\begin{align*}
E \bigg [ \sup_{\rho_Q(\theta, \theta_0) < \tilde \delta_n} |\mathbb L_{n, a}^{(s)}(\theta)| \bigg ] & \lesssim E \bigg [ \sup_{\rho_Q(\theta, \theta_0) < \tilde \delta_n} \bigg | \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} (m_s(X_i, 1, R_i(1), \theta) - m_s(X_i, 1, R_i(1), \theta_0) \\
& \hspace{5em} - E[(m_s(X_i, 1, R_i(1), \theta) - m_s(X_i, 1, R_i(1), \theta_0)]) \bigg | \bigg ] \to 0~.
\end{align*}
where the convergence follows from Assumption \ref{ass:normal}(e) and Corollary 2.3.12 in \cite{van_der_vaart1996weak}. We then obtain \eqref{eq:L_equicont} by Markov's inequality.

Finally, we derive \eqref{eq:normal} from \eqref{eq:vdv}. Note that
\begin{align*}
& \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} m(X_i, A_i, R_i, \theta_0) \\
& = \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} \Big ( \eta E[m(X_i, 1, R_i(1), \theta_0) | X_i] + (1 - \eta) E[m(X_i, 0, R_i(0), \theta_0) | X_i] \\
& \hspace{3.5em} + I\{A_i = 1\} (m(X_i, 1, R_i, \theta_0) - E[m(X_i, 1, R_i(1), \theta_0) | X_i]) \\
& \hspace{3.5em} + I\{A_i = 0\} (m(X_i, 0, R_i, \theta_0) - E[m(X_i, 0, R_i(0), \theta_0) | X_i]) \\
& \hspace{3.5em} + (A_i - \eta) (E[m(X_i, 1, R_i(1), \theta_0) - m(X_i, 0, R_i(0), \theta_0) | X_i]) \Big )~.
\end{align*}
Let $\Omega(X_i) = E[m(X_i, 1,R_i(1),\theta_0) - m(X_i, 0,R_i(0),\theta_0)|X_i]$ and note that by Assumption \ref{eq:unconfounded},
\[E\bigg[\frac{1}{\sqrt{n}}\sum_{1 \le i \le n}(A_i - \eta)\Omega(X_i) \bigg \vert X^{(n)}\bigg] = 0~.\]
Recall $\Omega^{(\ell)}(X_i)$ is the $\ell$th component of $\Omega(X_i)$. Next, it can be shown using Assumption \ref{ass:a} and \ref{ass:normal}(f) that for $1 \leq \ell \leq d_\theta$,
\[\var\bigg[\frac{1}{\sqrt{n}}\sum_{1 \le i \le n}(A_i - \eta)\Omega^{(\ell)}(X_i) \bigg \vert X^{(n)}\bigg] \propto \frac{1}{n} \sum_{1 \le j \le n/k} \sum_{i \in \lambda_j} (\Omega_i^{(\ell)} - \overline \Omega_j^{(\ell)})^2 \lesssim \frac{1}{n}\sum_{1 \le j \le n/k} \max_{i, i' \in \lambda_j} \|X_i - X_{i'}\|^2~,\]
where $\overline \Omega_j^{(\ell)} = \frac{1}{k} \sum_{i \in \lambda_j} \Omega^{(\ell)}(X_i)$, and so the conditional variance converges in probability to zero under Assumption \ref{ass:pair}. It then follows from Markov's inequality and the fact that probabilities are bounded and hence uniformly integrable that
\[ \frac{1}{\sqrt{n}}\sum_{1 \le i \le n}(A_i - \eta)\Omega(X_i) = o_P(1)~. \]
Therefore,
\[ - M^{-1} \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} m(X_i, A_i, R_i, \theta_0) = - M^{-1} \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} m^\ast(X_i, A_i, R_i, \theta_0) + o_P(1)~, \]
which, together with \eqref{eq:vdv}, implies the desired result in \eqref{eq:normal}.
\qed

\subsection{Proofs for Section \ref{sec:semi}}\label{sec:SPEB}
Recall that $P_n$ denotes the distribution of the observed data $(X^{(n)}, A^{(n)}, R^{(n)})$, and $Q$ denotes the marginal distribution of the vector $(R_i(1), R_i(0), X_i)$. Note that any treatment assignment mechanism $A^{(n)}$ satisfying Assumption \ref{ass:unconfounded} can be represented as a function of $X^{(n)}$ and some additional exogenous randomization device $U_n \in \mathbf{R}$. Let $p_n^{U_n}$ denote the density function for $U_n$ with respect to a dominating measure $\mu^U$. In what follows, we consider a family $\{Q_{t}: t \in \mathbf R^{d_{\theta}}\}$ of marginal distributions indexed by $t$, and let $q_t^X$ denote the density function for $X_i$ with respect to a dominating measure $\mu^X$, $q_t^{R(a) | X}(r | x)$ denote the conditional density of $R_i(a)$ given $X_i$ with respect to a dominating measure $\mu^R$. With some abuse of notation, continue letting $P_{t, n}$ denote the distribution of $(U_n, X^{(n)}, R^{(n)})$. We require that $Q_0 = Q$ and $P_{0, n} = P_n$ and define $q^X = q_0^X$ and $q^{R(a) | X} = q_0^{R(a) | X}$. As a consequence, the density function of $P_{t,n}$ is given by
\begin{equation} \label{eq:density}
\ell_n = p_n^U(U_n) \prod_{1 \leq i \leq n} q_{t}^X(X_i) \prod_{1 \leq i \leq n} \prod_{a \in \{0, 1\}} q_{t}^{R(a) | X}(R_i | X_i)^{I \{A_i = a\}}~.
\end{equation}
Because the density $p_n^{U_n}$ does not depend on $t$, and in general we will only concern ourselves with the ratio of likelihoods at different values of $t$ (so that $p_n^{U_n}$ in the ratio will cancel), in what follows we suppress the dependence on $n$ and simply denote the distribution $P_{t,n}$ by $P_t$.

We consider parametric submodels $\{P_t: t \in \mathbf R^{d_\theta}\}$, where $P_0 = P$, such that the following holds for some $g = (g^X, g^{R(1) | X}, g^{R(0) | X})$, each component of which is a $d_\theta$-dimensional function:
\begin{enumerate}[\rm (a)]
    \item As $t \to 0$,
    \begin{equation} \label{eq:qmd-1}
    \int \frac{1}{\|t\|^2} \Big ( q_{t}^X(x)^{1/2} - q^X(x)^{1/2}  - \frac{1}{2} q^X(x)^{1/2} t' g^X(x) \Big )^2 d \mu^X(x) \to 0~.
    \end{equation}
    \item For $a \in \{0, 1\}$, as $t \to 0$,
    \begin{multline} \label{eq:qmd-2}
    \frac{1}{\|t\|^2} \int\!\!\!\int \Big ( q_{t}^{R(a) | X}(r | x)^{1/2} - q^{R(a) | X}(r | x)^{1/2} - \frac{1}{2} q^{R(a) | X}(r | x)^{1/2} t' g^{R(a) | X}(r | x) \Big )^2 \\
    d \mu^R(r) q^X(x) d \mu^X(x) \to 0~.
    \end{multline}
\end{enumerate}
In what follows, we will index a parametric submodel by its associated function $g$, denoted by $P_{t, g}$, to emphasize the role of $g$. Similarly we denote the density of $Q_{t,g}$ by $q_{t, g}$. When writing expectations and variances, we suppress the subscripts $P$ and $Q$ whenever doing so does not lead to confusion. For completeness, we document the following properties of score functions which satisfy \eqref{eq:qmd-1}--\eqref{eq:qmd-2}:

\begin{lemma} \label{lem:qmd}
For a parametric submodel $\{P_{t, g}: t \in \mathbf R^{d_\theta}\}$ with $P_{0, g} = P$ that satisfies \eqref{eq:qmd-1}--\eqref{eq:qmd-2},
\begin{enumerate}[\rm (a)]
    \item $E[g^X(X) g^X(X)'] < \infty$.
    \item $E[g^X(X)] = 0$.
    \item $E[g^{R(a) | X}(R(a) | X) g^{R(a) | X}(R(a) | X)'] < \infty$ and hence $I^{R(a) | X}(X) < \infty$ with probability one under $Q$.
    \item $E[g^{R(a) | X}(R(a) | X) | X] = 0$ with probability one under $Q$.
\end{enumerate}
\end{lemma}

\begin{proof}
(a) and (b) follow from Lemma 14.2.1 in \cite{lehmann2022testing}. (c) follows from the same lemma. In order to show (d), fix $t_n \to 0$. Note \eqref{eq:qmd-2} and Markov's inequality imply that along a subsequence $t_{n_k}$,
\[ \frac{1}{\|t_{n_k}\|^2} \int \Big ( q_{t_{n_k}}^{R(a) | X}(r | x)^{1/2} - q^{R(a) | X}(r | x)^{1/2} - \frac{1}{2} q^{R(a) | X}(r | x)^{1/2} t_{n_k}' g^{R(a) | X}(r | x) \Big )^2 d \mu^R(r) \to 0 \]
for $Q$-almost every $x$. Along that subsequence, another application of Lemma 14.2.1 in \cite{lehmann2022testing} implies (d).
\end{proof}

Define the information of $X$ as $I^X = E[g^X(X) g^X(X)']$. Define the conditional information of $R(a)$ given $X=x$ as
\[ I^{R(a) | X}(x) = E[g^{R(a) | X}(R(a) | X) g^{R(a) | X}(R(a) | X)' | X = x]~. \]
Further define $I = I^X + \eta E[I^{R(1) | X}(X)] + (1 - \eta) E[I^{R(0) | X}(X)]$. We restrict ourselves to parametric submodels that satisfy \eqref{eq:qmd-1}--\eqref{eq:qmd-2} for a $g$ that satisfies the following conditions. These submodels exist by Lemma \ref{lem:path} below.

\begin{condition} \label{cond:path}
The function $g$ satisfies that
\begin{enumerate}[\rm (a)]
    \item $E[g^X(X)] = 0$ and $\var[g^X(X)] < \infty$.
    \item For $a \in \{0, 1\}$, $E[g^{R(a) | X}(R(a) | X) | X] = 0$, and $\var[g^{R(a) | X}(R(a) | X)] < \infty$ with probability one.
    \item $I$ is nonsingular.
\end{enumerate}
\end{condition}

\begin{lemma} \label{lem:path}
For any $g$ that satisfies Condition \ref{cond:path}, there exists a parametric submodel $\{P_{t, g}: t \in \mathbf R^{d_\theta}\}$ such that \eqref{eq:qmd-1}--\eqref{eq:qmd-2} hold.  
\end{lemma}

\begin{proof}
We use a vector version of the construction in in Example 25.16 in \cite{van_der_vaart1998asymptotic}. Let $k(x)$ be any strictly positive function that is bounded from above and away from zero with a bounded derivative such that $k(0) = k'(0) = 1$; for example, take $k(x) = 2 (1 + e^{-2x})^{-1}$. Define
\[ q_t^X(x) = C(t) q^X(x) k(t' g^X(x))~, \]
where $C(t) = \big ( \int q^X(x) k(t' g^X(x)) d \mu^X(x) \big )^{-1}$, so that $q_t^X(x)$ is a probability density function. Differentiating both sides of $C(t) \int q^X(x) k(t' g^X(x)) d \mu^X(x) = 1$ at $t = 0$, we get that $\frac{\partial}{\partial t} \big \vert_{t = 0} C(t) = 0$. It can then be verified through direct calculation that
\[ \frac{\partial}{\partial t} \bigg\vert_{t = 0} \log q_t^X(x) = g^X(x)~. \]
The quadratic mean differentiability requirement in \eqref{eq:qmd-1} follows from Lemma 7.6 in \cite{van_der_vaart1998asymptotic}. Next, for each $x \in \mathbf R^{d_x}$, we define
\[ q_t^{R(1) | X}(r | x) = C(t) q^{R(1) | X}(r | x) k(t' g^{R(1) | X}(r | x))~. \]
As above, it can be verified through direct calculation that
\[ \frac{\partial}{\partial t} \bigg\vert_{t = 0} \log q_t^{R(1) | X}(r | x) = g^{R(1) | X}(r | x)~. \]
To show \eqref{eq:qmd-2} for $a = 1$ (and symmetric arguments apply for $a = 0$), we modify the arguments in the proof of Lemma 7.6 in \cite{van_der_vaart1998asymptotic}. Define $s_t(r | x) = q_t^{R(1) | X}(r | x)^{1/2}$ and denote the $\ell$th component by $s_t^{(\ell)}(r | x)$ for $1 \leq \ell \leq d_\theta$. By the mean-value theorem, we have $s_t(r | x) - s(r | x) = \int_0^1 t' \dot s_{ut}(r | x) d u$, where $\dot s_t = \frac{\partial}{\partial t} s_t$, so that it follows from Jensen's inequality that
\begin{align}
\nonumber & \frac{1}{\|t\|^2} \int\!\!\!\int (s_t(r | x) - s_0(r | x) - t' \dot s_0(r | x))^2\, d \mu^R(r) q^X(x) d \mu^X(x) \\
\nonumber & \leq \frac{1}{\|t\|^2} \int\!\!\!\int\!\!\!\int_0^1 \big ( t' (\dot s_{ut}(r | x) - \dot s_0(r | x) ) \big )^2\,d u d \mu^R(r) q^X(x) d \mu^X(x)  \\
\label{eq:vitali} & \leq \int\!\!\!\int\!\!\!\int_0^1 \|\dot s_{ut}(r | x) - \dot s_0(r | x)\|^2 d u d \mu^R(r) q^X(x) d \mu^X(x)
\end{align}
where the first inequality follows from Jensen's inequality and the second by Cauchy-Schwarz. It then suffices to show \eqref{eq:vitali} goes to zero as $t \to 0$. Analyzing componentwise, it suffices to show that for $1 \leq \ell \leq d_\theta$, as $t \to 0$,
\begin{equation} \label{eq:vitali-l}
\int\!\!\!\int\!\!\!\int_0^1 (\dot s_{ut}^{(\ell)}(r | x) - \dot s_0^{(\ell)}(r | x))^2 d u d \mu^R(r) q^X(x) d \mu^X(x) \to 0~.
\end{equation}
The integrand in \eqref{eq:vitali-l} obviously converges to zero as $t \to 0$ by continuous differentiability of $q_t$. Recall that $\int \dot s_{ut}^{(\ell)}(r | x)^2 d \mu^R(r) = \frac{1}{4} [I_{ut}^{R(1) | X}(x)]_{(\ell, \ell)}$, where $I_{ut}^{R(1) | X}(x)$ is the conditional information for $a = 1$ given $x$ at $P_{ut, g}$. Therefore, it follows from Fubini's theorem that
\begin{align*}
\int\!\!\!\int\!\!\!\int_0^1 \dot s_0^{(\ell)}(r | x)^2 d u d \mu^R(r) q^X(x) d \mu^X(x) & = \frac{1}{4} \int [I_0^{R(1) | X}(x)]_{(\ell, \ell)} q^X(x) d \mu^X(x) \\
\int\!\!\!\int\!\!\!\int_0^1 \dot s_{ut}^{(\ell)}(r | x)^2 d u d \mu^R(r) q^X(x) d \mu^X(x) & = \frac{1}{4} \int\!\!\!\int_0^1 [I_{ut}^{R(1) | X}(x)]_{(\ell, \ell)} du q^X(x) d \mu^X(x)~.    
\end{align*}
To apply Vitali's theorem \citep[Proposition 2.29 in][]{van_der_vaart1998asymptotic}, it suffices to show that
\[ \int\!\!\!\int_0^1 [I_{ut}^{R(1) | X}(x)]_{(\ell, \ell)} du q^X(x) d \mu^X(x) \to \int [I_0^{R(1) | X}(x)]_{(\ell, \ell)} q^X(x) d \mu^X(x) \]
as $t \to 0$. To do so, we fix any arbitrarily small $\delta > 0$ and note that at least for $t$ small enough, $\|ut\| \leq \delta$ for $u \in [0, 1]$, so we can apply the dominated convergence theorem with
\[ [I_{ut}^{R(1) | X}(x)]_{(\ell, \ell)} \leq \sup_{\|h\| \leq \delta} [I_h^{R(1) | X}(x)]_{(\ell, \ell)}~, \]
as long as
\begin{equation} \label{eq:dom}
\int\!\!\!\int_0^1 \sup_{\|h\| \leq \delta} [I_h^{R(1) | X}(x)]_{(\ell, \ell)} du q^X(x) d \mu^X(x) = \int \sup_{\|h\| \leq \delta} [I_h^{R(1) | X}(x)]_{(\ell, \ell)} q^X(x) d \mu^X(x) < \infty~.   
\end{equation}
To show \eqref{eq:dom}, we calculate the conditional information as
\[ E \bigg [ \bigg ( \frac{\frac{\partial}{\partial h_\ell} C(h)}{C(h)} + \frac{k'(h' g^{R(1) | X}(R(1) | X)}{k(h' g^{R(1) | X}(R(1) | X))} g_\ell^{R(1) | X}(R(1) | X) \bigg )^2 \bigg \vert X = x \bigg ]~. \]
Note that $k'$ is bounded above and $k$ is bounded below, $C(h)$ is continuously differentiable with $C(0) = 1$, and so is bounded for $\|h\| \leq \delta$. Therefore, an application of the Cauchy-Schwarz inequality implies the previous expectation is bounded by a constant plus a constant multiple of $[I_0^{R(1) | X}(x)]_{(\ell, \ell)}$. The desired conclusion in \eqref{eq:dom} then follows because $E[I_0^{R(1) | X}(X)] < \infty$, and the proof is complete.
\end{proof}

For $t \in \mathbf R^{d_\theta}$, the log-likelihood ratio between $P_{t/\sqrt n, g}$ and $P_0 = P$ is
\[ L_{t, n}(g) = \frac{1}{n} \sum_{1 \leq i \leq n} \log \frac{q_{t / \sqrt n, g}^X(X_i)}{q^X(X_i)} + \frac{1}{n} \sum_{1 \leq i \leq n} \sum_{a \in \{0, 1\}} I \{A_i = a\} \log \frac{q_{t / \sqrt n, g}^{R(a) | X}(R_i | X_i)}{q^{R(a) | X}(R_i | X_i)}~. \]
The following lemma establishes an expansion of the log-likelihood ratio and local asymptotic normality of $\{P_{t/\sqrt n, g}\}$.

\begin{lemma} \label{lem:lan}
Suppose the treatment assignment mechanism satisfies Assumption \ref{ass:unconfounded} and the path satisfies \eqref{eq:qmd-1}--\eqref{eq:qmd-2} for $g$ satisfying Condition \ref{cond:path}. Then,
\[ L_{t, n}(g) = \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} t' s_g(X_i, A_i, R_i) - \frac{1}{2} t' I^X t - \frac{1}{2n} \sum_{1 \leq i \leq n} \sum_{a \in \{0, 1\}} I \{A_i = a\} t' I^{R(a) | X}(X_i) t + o_P(1)~, \]
where
\begin{equation} \label{eq:score}
s_g(x, a, r) = g^X(x) + I \{a = 1\} g^{R(1) | X} (r | x) + I \{a = 0\} g^{R(0) | X} (r | x)~
\end{equation}
and $I = I^X + \eta E_Q[I^{R(1)|X}(X_i)] + (1 - \eta)E_Q[I^{R(0)|X}(X_i)]$.
If in addition the assignment mechanism satisfies Assumption \ref{ass:LLN}, then, under $P_0$,
\[ L_{t, n}(g) \stackrel{d}{\to} N \Big ( - \frac{1}{2} t' I t, t' I t \Big )~, \]
\end{lemma}

\begin{proof}
The first result follows from Theorem 3.1 of \cite{armstrong2022asymptotic}. The second result follows from Lemma \ref{lem:clt} given Assumption \ref{ass:LLN} and the assumption that each component of $I^{R(a)|X}(x)$ is integrable, noting that $E[s_g(X, 1, R(1)) - s_g(X, 0, R(0)) | X] = 0$.
\end{proof}

We emphasize that Lemma \ref{lem:marginal} implies
\[ \sum_{1 \leq i \leq n} s_g(X_i, A_i, R_i)\]
is the sum of $n$ identically distributed, although possibly dependent, random variables. Therefore, in what follows, quantities like $E_P[s_g]$ are well defined.

Let $\theta(P) \in \mathbf R^{d_\theta}$ be a parameter of interest. Further suppose that there exists a $d_\theta \times 1$ vector of functions $\psi^\ast \in L^2(P)$ such that for each $g$ satisfying Condition \ref{cond:path}, for all $t \in \mathbf R^{d_\theta}$, as $n \to \infty$,
\begin{equation} \label{eq:pathwise}
    \sqrt n(\theta(P_{t / \sqrt n, g}) - \theta(P)) \to E_P[\psi^\ast s_g' t]~.
\end{equation}
In Lemma \ref{lem:differentiability} below, we provide explicit conditions which guarantee this is possible when $\theta(P)$ is defined by \eqref{eq:moments}.

We call an estimator $\tilde \theta_n$ for $\theta(P)$ regular if for all $g$ satisfying Condition \ref{cond:path} and $t \in \mathbf R^{d_\theta}$,
\begin{equation} \label{eq:regular}
\sqrt n(\tilde \theta_n - \theta(P_{t/\sqrt n, g})) \xrightarrow{P_{t/\sqrt n, g}} L 
\end{equation}
for a fixed probability measure $L$.

The following lemma establishes a convolution theorem for regular estimators:

\begin{lemma} \label{lem:convolution}
Suppose $\theta$ satisfies \eqref{eq:pathwise}. Let $\tilde \theta_n$ be a regular estimator for $\theta$. Further suppose that $\psi^\ast = s_g$ for some function $g$ satisfying Condition \ref{cond:path}. Then,
\[ L = N(0, E_P[\psi^\ast \psi^{\ast\prime}]) \ast B~, \]
where $B$ is a fixed probability measure.
% In addition, $\psi^\ast$ is equal to the efficient influence function in the setting in which treatment assignment is determined by i.i.d.\ coin flips with probability $\eta$.
\end{lemma}

\begin{proof}
In what follows, for each $g$ satisfying Condition \ref{cond:path}, we consider the linear subspace given by 
\[ \mathcal{M}_g = \{t' s_g: t \in \mathbf R^{d_\theta}\}~. \]
Note that $t' s_g$ appears in the expansion of the log-likelihood ratio between $P_{t/\sqrt n, g}$ and $P$. To align our setting with Theorem 3.11.2 in \cite{van_der_vaart1996weak}, we first characterize the adjoint map (viewed as a mapping into $\mathcal{M}_g$) of the function $v \mapsto E[\psi^* v] \in \mathbb{R}^{d_\theta}$, where $v \in \mathcal{M}_g$. To that end, implicitly identifying each $b \in \mathbf R^{d_\theta}$ with the functional $b^*:\mathbb{R}^{d_\theta} \rightarrow \mathbb{R}$ given by $x \mapsto b'x$, we construct a $w(b) \in \mathbf R^{d_\theta}$ such that
\[ b' E_P[\psi^\ast s_g' t] = E_P[w(b)' s_g s_g' t] \]
for all $t\in \mathbf R^{d_\theta}$, where we note $w(b)' s_g \in \mathcal{M}_g$, and is thus the output of the adjoint map when applied to the functional $b^*$. Because $E[s_g s_g']$ is invertible, we immediately obtain
\[ w(b) = E[s_g s_g']^{-1} E[s_g \psi^{\ast\prime}] b~. \]
Here we use the assumption that $I$ is nonsingular. It then follows from the local asymptotic normality established in Lemma \ref{lem:lan} and Theorem 3.11.2 in \cite{van_der_vaart1996weak} that
\[ L = N(0, V_g) \ast B_g~, \]
where $B_g$ is a fixed probability measure and $b' V_g b = E[(w(b)' s_g)^2]$, so that we have
\[ V_g = E_P[\psi^\ast s_g'] E_P[s_g s_g']^{-1} E[s_g \psi^{\ast\prime}]~. \]
Furthermore, by a standard projection argument, in particular the fact that the second moment of $\psi^\ast - E_P[\psi^\ast s_g'] E_P[s_g s_g']^{-1} s_g$ is positive semi-definite, it can be shown that $V_g$ is maximized in the matrix sense when $s_g = \psi^\ast$. Note this maximum is attained by our assumption that $\psi^\ast = s_g$ for some $g$ satisfying Condition \ref{cond:path}. The conclusion then follows.
\end{proof}

To apply Lemma \ref{lem:convolution} to the setting in Section \ref{sec:semi}, we we establish the form of $\psi^\ast$ in \eqref{eq:pathwise} for the parameter $\theta_0 = \theta(P)$ defined by \eqref{eq:moments}. Define $\eta(X_i) = P \{A_i = 1 | X_i\}$. Note that
\begin{equation} \label{eq:moments-P}
0 = E_P[m(X_i, A_i, R_i, \theta(P))] = E_Q[m(X, 1, R(1), \theta(P)) \eta(X)] + E_Q[m(X, 0, R(0), \theta(P)) (1 - \eta(X))]~.
\end{equation}

\begin{lemma} \label{lem:differentiability}
Suppose the treatment assignment mechanism satisfies Assumptions \ref{ass:unconfounded} and \ref{ass:LLN}. Fix a function $g$ that satisfies Condition \ref{cond:path}. Suppose \eqref{eq:qmd-1}--\eqref{eq:qmd-2} holds. Fix $t \in \mathbf R^{d_\theta}$ and consider a one-dimensional submodel $\{P_{t / \sqrt n, g}\}$ such that
\begin{equation} \label{eq:bounded_path}
\begin{split}
   E_{Q_{t / \sqrt n}}[m(X, a, R(a), \theta(P))^2] & = O(1) \\
   E_{Q^X}[E_{Q_{t / \sqrt n}^{R(a) | X}}[m(X, a, R(a), \theta(P))^2 | X]] & = O(1) \\
   E_{Q_{t / \sqrt n}^X}[E_{Q^{R(a) | X}}[m(X, a, R(a), \theta(P))^2 | X]] & = O(1)
\end{split}    
\end{equation}
as $n \to \infty$ and $\theta(P_{t / \sqrt n, g})$ is uniquely determined by \eqref{eq:moments-P}. Then, $\theta(P_{t / \sqrt n, g})$ defined by \eqref{eq:moments-P} satisfies
\begin{align*}
& \sqrt n ( \theta(P_{t / \sqrt n, g}) - \theta(P)) \\
& \to M^{-1} E_P[m(X_i, A_i, R_i, \theta(P))(g^X(X_i) + I \{A_i = 1\}g^{R(1) | X}(R_i | X_i) + I \{A_i = 0\}g^{R(0) | X}(R_i | X_i))'] t \\
& = E_P[\psi^\ast(X_i, A_i, R_i, \theta(P)) (g^X(X_i) + I \{A_i = 1\}g^{R(1) | X}(R_i | X_i) + I \{A_i = 0\} g^{R(0) | X}(R_i | X_i))'] t~,
\end{align*}
where
\begin{align*}
& \psi^\ast(X_i, A_i, R_i, \theta(P)) \\
& = - M^{-1} \Big ( \eta(X_i) E_Q[m(X_i, 1, R_i(1), \theta(P)) | X_i] + (1 - \eta(X_i)) E_Q[m(X_i, 0, R_i(0), \theta(P)) | X_i] \\
& \hspace{5em} + I\{A_i = 1\} (m(X_i, 1, R_i, \theta(P)) - E_Q[m(X_i, 1, R_i(1), \theta(P)) | X_i]) \\
& \hspace{5em} + I\{A_i = 0\} (m(X_i, 0, R_i, \theta(P)) - E_Q[m(X_i, 0, R_i(0), \theta(P)) | X_i]) \Big )~.
\end{align*}
\end{lemma}

\begin{proof}
In what follows, we only use the property that the quadratic mean derivative of $P_{t / \sqrt n, g}$ is given by $s_g' t$. Therefore, for ease of notation we consider a generic one-dimensional submodel $\{P_\nu: \nu \in [-\epsilon, \epsilon]\}$ that satisfies \eqref{eq:qmd-1}--\eqref{eq:qmd-2} for some $g = (g^X, g^{R(1) | X}, g^{R(0) | X})$, each component of which is a one-dimensional function. \eqref{eq:moments-P} implies
\begin{multline*}
0 = \int m(x, 1, r, \theta(P_\nu)) q_\nu^{R(1) | X}(r | x) d\mu^R(r) \eta(x) q_\nu^X(x) d\mu^X(x) \\
+ \int m(x, 0, r, \theta(P_\nu)) q_\nu^{R(0) | X}(r | x) d\mu^R(r) (1 - \eta(x)) q_\nu^X(x) d\mu^X(x)
\end{multline*}
Note that
\begin{align*}
& \int m(x, 1, r, \theta(P)) q_\nu^{R(1) | X}(r | x) d\mu^R(r) \eta(x) q_\nu^X(x) d\mu^X(x) \\
& \hspace{2em} - \int m(x, 1, r, \theta(P)) q^{R(1) | X}(r | x) d\mu^R(r) \eta(x) q^X(x) d\mu^X(x) = \gamma_1(\nu) + \gamma_2(\nu) + \gamma_3(\nu) + \gamma_4(\nu)~,
\end{align*}
where
\begin{align*}
\gamma_1(\nu) & = \int m(x, 1, r, \theta(P)) \big ( q_\nu^{R(1) | X}(r | x)^{1/2} - q^{R(1) | X}(r | x)^{1/2} \big ) q_\nu^{R(1) | X}(r | x)^{1/2} d\mu^R(r) \eta(x) q_\nu^X(x) d\mu^X(x) \\
\gamma_2(\nu) & = \int m(x, 1, r, \theta(P)) \big ( q_\nu^{R(1) | X}(r | x)^{1/2} - q^{R(1) | X}(r | x)^{1/2} \big ) q^{R(1) | X}(r | x)^{1/2} d\mu^R(r) \eta(x) q_\nu^X(x) d\mu^X(x) \\
\gamma_3(\nu) & = \int m(x, 1, r, \theta(P)) q^{R(1) | X}(r | x) d\mu^R(r) \eta(x) \big ( q_\nu^X(x)^{1/2} - q^X(x)^{1/2} \big ) q_\nu^X(x)^{1/2} d\mu^X(x) \\
\gamma_4(\nu) & = \int m(x, 1, r, \theta(P)) q^{R(1) | X}(r | x) d\mu^R(r) \eta(x) \big ( q_\nu^X(x)^{1/2} - q^X(x)^{1/2} \big ) q^X(x)^{1/2} d\mu^X(x)~.
\end{align*}
It follows from the Cauchy-Schwarz inequality that
\begin{align*}
& \frac{1}{\nu} \gamma_4(\nu) - \int m(x, 1, r, \theta(P)) q^{R(1) | X}(r | x) d\mu^R(r) \eta(x) \frac{1}{2} g^X(x) q^X(x)^{1/2} \times q^X(x)^{1/2}  d\mu^X(x) \\
& \leq \int \bigg ( m(x, 1, r, \theta(P))^2 q^{R(1) | X}(r | x) d\mu^R(r) \eta(x)^2 q^X(x) d \mu^X(x) \bigg )^{1/2} \\
& \hspace{2em} \times \bigg ( \int q^{R(1) | X}(r | x) d\mu^R(r) \Big ( \frac{1}{\nu}(q_\nu^X(x)^{1/2} - q^X(x)^{1/2}) - \frac{1}{2} g^X(x) q^X(x)^{1/2} \Big )^2 d \mu^X(x) \bigg )^{1/2}
\to 0
\end{align*}
by the assumption that $E_P[m(X, a, R(a), \theta(P))^2] < \infty$, the facts that $0 \leq \eta(x) \leq 1$, $\int q^{R(1) | X}(r | x) d\mu^R(r) = 1$, and \eqref{eq:qmd-1}. Similar arguments implies as $\nu \to 0$,
\[ \frac{1}{\nu} \gamma_1(\nu) - \int m(x, 1, r, \theta(P)) \frac{1}{2} g^{R(1) | X}(r | x) q^{R(1) | X}(r | x) d\mu^R(r) \eta(x) q^X(x) d\mu^X(x) \to 0 \]
because $E_{P_\nu}[m(X, a, R(a), \theta(P))^2] = O(1)$ as $\nu \to 0$. The limits of $\gamma_2(\nu)$ and $\gamma_3(\nu)$ can be derived following similar arguments using the last two conditions in \eqref{eq:bounded_path}. Combining all previous results yields
\begin{align*}
& \frac{\partial}{\partial \nu} E_{P_\nu}[m(X, A, R, \theta(P))] \Big |_{\nu = 0} \\
& = E_Q[m(X, 1, R(1), \theta(P)) (g^X(X) + g^{R(1) | X}(R | X)) \eta(X)] \\
& \hspace{3em} + E_Q[m(X, 0, R(0), \theta(P)) (g^X(X) + g^{R(0) | X}(R | X)) (1 - \eta(X))] \\
& = E_P[m(X, A, R, \theta(P))(g^X(X) + I \{A = 1\}g^{R(1) | X}(R) + I \{A = 0\}g^{R(0) | X}(R))]~.
\end{align*}
On the other hand, by definition
\[ M = \frac{\partial}{\partial \theta'} E_P[m(X, A, R, \theta)] \Big |_{\theta = \theta(P)}~. \]
The formula for the derivative therefore follows from the implicit function theorem (in particular, because we have assumed the existence of $\theta(P_\nu)$ along the path, it follows from the last part of the proof of Theorem 3.2.1 in \cite{krantz2013implicit}). The second equality follows from Lemma \ref{lem:marginal} together with Condition \ref{cond:path}.
\end{proof}

Finally, to prove Theorem \ref{thm:efficiencybound} we require the following additional regularity condition:

\begin{assumption}\label{ass:bounded_path}
For every function $g$ satisfying Condition \ref{cond:path} and every $t \in \mathbf R^{d_\theta}$ there exists a submodel $P_{t / \sqrt n,g}$ for which \eqref{eq:bounded_path} holds as $n \to \infty$, and $\theta(P_{t / \sqrt n, g})$ is uniquely determined by \eqref{eq:moments-P}.
\end{assumption}

This assumption guarantees that every element satisfying Condition \ref{cond:path} has a corresponding path for which we can apply Lemma \ref{lem:differentiability}. A similar assumption appears in \cite{chen2018overidentification} (see their Assumption 4.1(iv)). Note that a simple sufficient condition for the first part of Assumption \ref{ass:bounded_path} is that $m(x, a, r, \theta(P))$ is a bounded function in $(x, r)$ on the support of $(X, R(a))$. The second part of Assumption \ref{ass:bounded_path} can be verified easily in specific examples (see, for instance, Examples \ref{ex:ATE}--\ref{ex:log_odds} in the main text). Alternatively, Assumption \ref{ass:bounded_path} could be avoided by assuming that we can differentiate under the integral in the final step of the proof of Lemma \ref{lem:differentiability}, from which we would immediately obtain the expression for the pathwise derivative. See, for instance, \cite{newey1994asymptotic} and \cite{chen2008semiparametric}.

\begin{proof}[\sc Proof of Theorem \ref{thm:efficiencybound}]
    First note $\theta$ satisfies \eqref{eq:pathwise} because of Lemma \ref{lem:differentiability} and Assumption \ref{ass:bounded_path}. The result then follows from Lemma \ref{lem:convolution} upon noting that $\psi^\ast = s_g$ for some $g$ that satisfies Condition \ref{cond:path}.
\end{proof}

To study regular estimators, we need the following lemma.

\begin{lemma} \label{lem:clt}
Suppose the treatment assignment mechanism satisfies Assumptions \ref{ass:unconfounded}, \ref{ass:LLN}, and \ref{ass:imbalance}. Let $f(x, a, r)$ be a vector-valued function such that $E[f(X, A, R)] = 0$ and $E[f^2(X, a, R(a))] < \infty$ for $a \in \{0, 1\}$. Then,
\[ \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} f(X_i, A_i, R_i) \xrightarrow{d} N(0, V_f)~, \]
where $V_f = V_{1, f} + V_{2, f} + V_{3, f}$ for
\begin{align*}
    V_{1, f} & = \var[E[f(X, A, R) | X]] = \var[\eta E[f(X, 1, R(1)) | X] + (1 - \eta) E[f(X, 0, R(0)) | X]] \\
    V_{2, f} & = V_{E[f(X, 1, R(1)) - f(X, 0, R(0)) | X]}^{\rm imb} \\
    V_{3, f} & = E[\eta \var[f(X, 1, R(1)) | X] + (1 - \eta) \var[f(X, 0, R(0)) | X]]~.
\end{align*}
\end{lemma}

\begin{proof}
Note that
\[ \mathbb C_n := \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} f(X_i, A_i, R_i) = \mathbb C_{1, n} + \mathbb C_{2, n} + \mathbb C_{3, n}~, \]
where
\begin{align*}
\mathbb C_{1, n} & = \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} E[f(X_i, A_i, R_i) | X_i] \\
& = \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} (\eta E[f(X_i, 1, R_i(1)) | X_i] + (1 - \eta) E[f(X_i, 0, R_i(0)) | X_i]) \\
\mathbb C_{2, n} & = \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} (A_i - \eta) E[f(X_i, 1, R_i(1)) - f(X_i, 0, R_i(0)) | X_i] \\
\mathbb C_{3, n} & = \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} \big ( A_i (f(X_i, 1, R_i(1)) - E[f(X_i, 1, R_i(1)) | X_i]) \\
& \hspace{5em} + (1 - A_i) (f(X_i, 0, R_i(0)) - E[f(X_i, 0, R_i(0)) | X_i]) \big )~.
\end{align*}
Note that $\mathbb C_{1, n}$ has zero mean because $E[E[f(X_i, A_i, R_i) | X_i]] = E[f(X_i, A_i, R_i)] = 0$. Further note that $\mathbb C_{1, n} = E[\mathbb C_n | X^{(n)}]$, $\mathbb C_{2, n} = E[\mathbb C_n | X^{(n)}, A^{(n)}] - E[\mathbb C_n | X^{(n)}]$, and $\mathbb C_{3, n} = \mathbb C_n - E[\mathbb C_n | X^{(n)}, A^{(n)}]$. It follows from the central limit theorem and $E[f^2(X, a, R(a))] < \infty$ that
\[ \mathbb C_{1, n} \xrightarrow{d} N(0, V_{1, f})~. \]
Next, it follows from Assumption \ref{ass:imbalance} that
\[ \rho(\mathbb C_{2, n}, N(0, V_{2, f}) | X^{(n)}) \xrightarrow{P} 0~. \]
For $\mathbb C_{3, n}$, it follows from Assumption \ref{ass:LLN} that
\[ \var[\mathbb C_{3, n} | X^{(n)}, A^{(n)}] \xrightarrow{P} V_{3, f}~. \]
As a result, one can verify the Lindeberg condition conditional on $X^{(n)}$ and $A^{(n)}$ as in the proof of Lemma S.1.4 of \cite{bai2022inference}, and obtain
\[ \rho(\mathbb C_{3, n}, N(0, V_{3, f}) | X^{(n)}, A^{(n)}) \xrightarrow{P} 0~. \]
The proof can then be completed by the subsequencing argument at the end of the proof of Lemma S.1.4 of \cite{bai2022inference}.
\end{proof}

\begin{proof}[\sc Proof of Theorem \ref{thm:regular}]
Recall from Lemma \ref{lem:lan} that $L_{t, n}(g)$ is also asymptotically linear. Because the treatment assignment mechanism satisfies Assumptions \ref{ass:unconfounded}, \ref{ass:LLN}, and \ref{ass:imbalance}, the path satisfies Condition \ref{cond:path}, and hence $\psi$ and $s_g$ jointly satisfy the conditions in Lemma \ref{lem:clt} (in particular, note that $E[s_g(X, 1, R(1)) - s_g(X, 0, R(0)) | X] = 0$),
\[ \begin{pmatrix}
    \sqrt n (\tilde \theta_n - \theta(P)) \\
    L_{t, n}(g)
\end{pmatrix} = \begin{pmatrix}
    \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} \psi(X_i, A_i, R_i, \theta(P)) \\
    \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} t' s_g(X_i, A_i, R_i)
\end{pmatrix} - \begin{pmatrix}
    0 \\
    - \frac{1}{2} t' I t
\end{pmatrix} + o_P(1) \]
are jointly asymptotically normal. Because $E[s_g(X, 1, R(1)) - s_g(X, 0, R(0)) | X] = 0$, the covariance in the second term in the joint variance in Lemma \ref{lem:clt} vanishes, and thus the overall covariance is given by
\begin{align*}
& E[E[\psi | X] g^X(X)'] t + \eta E[(\psi(X, 1, R(1), \theta(P)) - E[\psi(X, 1, R(1), \theta(P)) | X]) g^{R(1) | X}(R(1) | X)'] t \\
& \hspace{3em} + (1 - \eta) E[(\psi(X, 0, R(0), \theta(P)) - E[\psi(X, 0, R(0), \theta(P)) | X]) g^{R(0) | X}(R(0) | X)'] t \\
& = E[\psi s_g'] t~.
\end{align*}
It then follows from Le Cam's third lemma that under $P_{t / \sqrt n, g}$, $\sqrt n (\tilde \theta_n - \theta_0)$ converges in distribution to a normal distribution with mean $E[\psi s_g'] t$ and the same variance as in the limit under $P$. At the same time,
\[ \sqrt n(\tilde \theta_n - \theta(P_{t / \sqrt n, g})) = \sqrt n(\tilde \theta_n - \theta(P)) - \sqrt n(\theta(P_{t / \sqrt n, g} - \theta(P))~. \]
Therefore, \eqref{eq:regular} holds if and only if
\[ E[\psi s_g'] t = E[\psi^\ast s_g'] t~. \]
Furthermore, $\tilde \theta_n$ is regular if the equality holds for all $t$ and all $g$ satisfying Condition \ref{cond:path}, if and only if
\[ E[(\psi - \psi^\ast) s_g'] = 0 \]
for all $g$ satisfying Condition \ref{cond:path}. Note that
\begin{align*}
& E[(\psi - \psi^\ast) s_g'] \\
& = E[E[\psi(X, A, R, \theta(P)) - \psi^\ast(X, A, R, \theta(P)) | X] g^X(X)'] \\
& \hspace{3em} + E[A_i (\psi(X, 1, R(1), \theta(P)) - \psi^\ast(X, 1, R(1), \theta(P)) \\
& \hspace{6em} - E[\psi(X, 1, R(1), \theta(P)) - \psi^\ast(X, 1, R(1), \theta(P)) | X]) g^{R(1) | X}(R(1) | X)'] \\
& \hspace{3em} + E[(1 - A_i) (\psi(X, 0, R(0), \theta(P)) - \psi^\ast(X, 0, R(0), \theta(P)) \\
& \hspace{6em} - E[\psi(X, 0, R(0), \theta(P)) - \psi^\ast(X, 0, R(0), \theta(P)) | X]) g^{R(0) | X}(R(0) | X)']~.
\end{align*}
By setting $g^{R(1) | X} = 0 = g^{R(0) | X}$ and $g^X(X) = E[\psi(X, A, R, \theta(P)) - \psi^\ast(X, A, R, \theta(P)) | X]$, we get that
\begin{equation} \label{eq:psi-psi*}
E[\psi(X, A, R, \theta(P)) | X] = E[\psi^\ast(X, A, R, \theta(P)) | X]~.
\end{equation}
Setting $g^X(X) = 0 = g^{R(0) | X}$ and $g^{R(1) | X} = \psi(X, 1, R(1), \theta(P)) - \psi^\ast(X, 1, R(1), \theta(P)) - E[\psi(X, 1, R(1), \theta(P)) - \psi^\ast(X, 1, R(1), \theta(P)) | X]$, we get
\[ \psi(X, 1, R(1), \theta(P)) - \psi^\ast(X, 1, R(1), \theta(P)) - E[\psi(X, 1, R(1), \theta(P)) - \psi^\ast(X, 1, R(1), \theta(P)) | X] = 0~, \]
which implies that $\psi(X, 1, R(1), \theta(P)) - \psi^\ast(X, 1, R(1), \theta(P))$ can only be a function of $X$. Denote it by $\psi^\perp(X, 1)$. Similarly, $\psi(X, 0, R(0), \theta(P)) - \psi^\ast(X, 0, R(0), \theta(P))$ can only be a function of $X$. Denote it by $\psi^\perp(X, 0)$. We have
\begin{align*}
\psi(X, A, R, \theta(P)) & = A \psi(X, 1, R(1), \theta(P)) + (1 - A) \psi(X, 0, R(0), \theta(P)) \\
& = A (\psi^\ast(X, 1, R(1), \theta(P)) + \psi^\perp(X, 1, \theta(P))) + (1 - A) (\psi^\ast(X, 0, R(0), \theta(P)) + \psi^\perp(X, 0, \theta(P))) \\
& = \psi^\ast(X, A, R, \theta(P)) + \psi^\perp(X, A, \theta(P))~,
\end{align*}
and it follows from \eqref{eq:psi-psi*} that $E[\psi^\perp(X, A, \theta(P)) | X] = 0$. Finally, to show that $\tilde\theta_n$ is efficient if and only if \eqref{eq:fast} holds, we apply Lemma \ref{lem:clt} with $f = \psi$. If \eqref{eq:fast} holds, then $V_\psi = V_{1, \psi} + V_{3, \psi} = V_{\psi^\ast}$ because $V_{1, \psi} = V_{1, \psi^\ast}$, $V_{3, \psi} = V_{3, \psi^\ast}$ and $V_{2, \psi^\ast} = 0$. On the other hand, if $\tilde \theta_n$ is efficient, then $V_{2, \psi} = 0$, so \eqref{eq:fast} holds by Markov's inequality combined with the fact that probabilities are bounded and thus uniformly integrable.
\end{proof}

\subsection{Auxiliary Lemmas}
\begin{lemma} \label{lem:marginal}
Suppose \eqref{eq:unconfounded} holds and $\mathrm{Pr} \{A_i = 1 | X_i = x\}$ as a function is identical across $1 \leq i \leq n$. Then,
\begin{equation} \label{eq:marginal-indep}
(R_i(1), R_i(0)) \indep A_i | X_i~.
\end{equation}
Moreover, $(X_i, A_i, R_i)$ is identically distributed across $1 \leq i \leq n$.
\end{lemma}

\begin{proof}
Fix $a \in \{0, 1\}$ and any Borel sets $B \in \mathbf R^{d_r} \times \mathbf R^{d_r}$ and $C \in \mathbf R^{d_x}$.
\begin{align*}
& E[\mathrm{Pr} \{(R_i(1), R_i(0)) \in B, A_i = a | X_i\} I \{X_i \in C\}] \\
& = E[E[\mathrm{Pr} \{(R_i(1), R_i(0)) \in B, A_i = a | X^{(n)}\} | X_i] I \{X_i \in C\}] \\
& = E[E[\mathrm{Pr} \{(R_i(1), R_i(0)) \in B | X^{(n)}\} \mathrm{Pr} \{A_i = a | X^{(n)}\} | X_i] I \{X_i \in C\}] \\
& = E[\mathrm{Pr} \{(R_i(1), R_i(0)) \in B | X_i\} \mathrm{Pr} \{A_i = a | X_i\} I \{X_i \in C\}]~,
\end{align*}
where the first equality follows from the law of iterated expectations, the second equality follows from \eqref{eq:unconfounded}, the third equality follows from the law of iterated expectations as well as the fact that $Q_n = Q^n$. The first statement of the lemma then follows from the definition of a conditional expectation.

To prove the second statement, fix units $i$ and $i'$. Clearly $X_i$ and $X_{i'}$ are identically distributed. Conditional on $X_i$, for any Borel set $C \in \mathbf R^{d_r}$ and $a \in \{0, 1\}$, it follows (a) that
\[ \mathrm{Pr} \{R_i \in C, A_i = a | X_i\} = \mathrm{Pr} \{A_i = a | X_i\} \mathrm{Pr} \{R_i(a) \in C | X_i\}~. \]
The conclusion then follows because $\mathrm{Pr} \{A_i = 1 | X_i = x\}$ is identical across $1 \leq i \leq n$ and $Q_n = Q^n$.
\end{proof}

\begin{lemma} \label{lem:consistency}
Suppose the treatment assignment mechanism satisfies Assumptions \ref{ass:a}--\ref{ass:pair} and the moment functions satisfy Assumption \ref{ass:normal}. Then, $\hat \theta_n \stackrel{P}{\to} \theta_0$.
\end{lemma}
\begin{proof}
It follows from Assumption \ref{ass:normal}(a) and Theorem 5.9 in \cite{van_der_vaart1998asymptotic} that we only need to establish for each $1 \leq s \leq d_\theta$,
\begin{equation} \label{eq:uc}
\sup_{\theta \in \Theta}~ \bigg | \frac{1}{n} \sum_{1 \leq i \leq n} (m_s(X_i, A_i, R_i, \theta) - E_P[m_s(X_i, A_i, R_i, \theta)]) \bigg | \stackrel{P}{\to} 0~.
\end{equation}
To begin, note it follows from Assumption \ref{ass:normal}(d) and the dominated convergence theorem that if $m_s(x, a, r, \theta_m) \allowbreak \to m_s(x, a, r, \theta)$ as $m \to \infty$ for $\{\theta_m\} \subset \Theta^\ast$, then $E_P[m_s(X_i, A_i, R_i, \theta_m)] \to E_P[m_s(X_i, A_i, R_i, \theta)]$. Here, the dominating function exists by Problem 2.4.1 in \cite{van_der_vaart1996weak}. Assumption \ref{ass:normal}(c) then implies
\begin{multline} \label{eq:countable}
     \sup_{\theta \in \Theta}~ \bigg | \frac{1}{n} \sum_{1 \leq i \leq n} (m_s(X_i, A_i, R_i, \theta) - E_P[m_s(X_i, A_i, R_i, \theta)]) \bigg | \\
     = \sup_{\theta \in \Theta^\ast}~ \bigg | \frac{1}{n} \sum_{1 \leq i \leq n} (m_s(X_i, A_i, R_i, \theta) - E_P[m_s(X_i, A_i, R_i, \theta)]) \bigg |~,
\end{multline}
which is measurable. Next, note that
\begin{equation} \label{eq:half-m}
m(X_i, A_i, R_i, \theta) = A_i m(X_i, 1, R_i(1), \theta) + (1 - A_i) m(X_i, 0, R_i(0), \theta)~.
\end{equation}
and it follows from Lemma \ref{lem:marginal} that
\begin{equation} \label{eq:half-E}
E_P[m(X_i, A_i, R_i, \theta)] = \eta E_Q[m(X_i, 1, R_i(1), \theta)] + (1 - \eta) E_Q[m(X_i, 0, R_i(0), \theta)]~,
\end{equation}
from which we obtain that
\begin{align*}
& E \bigg [ \sup_{\theta \in \Theta}~ \bigg | \frac{1}{n} \sum_{1 \leq i \leq n} m_s(X_i, A_i, R_i, \theta) - E_P[m_s(X_i, A_i, R_i, \theta)] \bigg | \bigg ] \\
& = E \bigg [ \sup_{\theta \in \Theta^\ast}~ \bigg | \frac{1}{n} \sum_{1 \leq i \leq n} m_s(X_i, A_i, R_i, \theta) - E_P[m_s(X_i, A_i, R_i, \theta)] \bigg | \bigg ] \\
& \le E \bigg [ \sup_{\theta \in \Theta^\ast}~ \bigg | \frac{1}{n} \sum_{1 \leq i \leq n} A_i (m_s(X_i, 1, R_i(1), \theta) - \eta E[m_s(X_i, 1, R_i(1), \theta)]) \bigg | \bigg ] \\
& \hspace{5em} + E \bigg [ \sup_{\theta \in \Theta^\ast}~ \bigg | \frac{1}{n} \sum_{1 \leq i \leq n} (1 - A_i) (m_s(X_i, 0, R_i(0), \theta) - (1 - \eta) E[m_s(X_i, 0, R_i(0), \theta)]) \bigg | \bigg ] \\
& = E \bigg [ \sup_{\theta \in \Theta^\ast}~ \bigg | \frac{1}{n} \sum_{1 \leq i \leq n} (A_i (m_s(X_i, 1, R_i(1), \theta) - E[m_s(X_i, 1, R_i(1), \theta)]) \bigg | \bigg ] \\
& \hspace{5em} + E \bigg [ \sup_{\theta \in \Theta^\ast}~ \bigg |  \frac{1}{n} \sum_{1 \leq i \leq n}(1 - A_i) (m_s(X_i, 0, R_i(0), \theta) -  E[m_s(X_i, 0, R_i(0), \theta)]) \bigg | \bigg ] \\
& \lesssim E \bigg [ \sup_{\theta \in \Theta^\ast}~ \bigg | \frac{1}{n} \sum_{1 \leq i \leq n} (m_s(X_i, 1, R_i(1), \theta) - E[m_s(X_i, 1, R_i(1), \theta)]) \bigg | \bigg ] \\
& \hspace{5em} + E \bigg [ \sup_{\theta \in \Theta^\ast}~ \bigg | \frac{1}{n} \sum_{1 \leq i \leq n} (m_s(X_i, 0, R_i(0), \theta) - E[m_s(X_i, 0, R_i(0), \theta)]) \bigg | \bigg ] \rightarrow 0~.
\end{align*}
where the first equality follows from \eqref{eq:countable}, the first inequality follows from the triangle inequality, the second equality follows because $\sum_{1 \leq i \leq n} A_i = n \eta$ and $\sum_{1 \leq i \leq n} (1 - A_i) = n (1 - \eta)$, and the last inequality follows from Proposition C.1 in \cite{han2021complex}. The convergence follows from Assumption \ref{ass:normal}(e) and an application of the backward submartingale convergence theorem \citep[see, for instance, Theorem 12.30 in][]{le2022measure}, as detailed in the proof of Theorem 3.1 in \cite{han2021complex}. The desired result in \eqref{eq:uc} then follows by Markov's inequality.
\end{proof}

% \subsection{Details for Examples \ref{ex:MP} and \ref{ex:MP2}}
% We suppose $\gamma$ takes values in $\mathbf R$ for simplicity. We first show the claim in Example \ref{ex:MP}. Lemma A.1 in \cite{hanneke2021universal} implies that for any $\epsilon > 0$, implies there exists an integrable and Lipschitz function $\tilde \gamma$ such that $E[|\gamma(X_i) - \tilde \gamma(X_i)|] < \epsilon$. We have
% \[ \frac{1}{n} \sum_{1 \leq i \leq n} A_i \tilde \gamma(X_i) \xrightarrow{P} \eta E[\tilde \gamma(X_i)] \in [\eta (E[\gamma(X_i) - \epsilon), \eta (E[\gamma(X_i) + \epsilon)]~. \]
% Moreover, because $A_i \in \{0, 1\}$, the triangle inequality and the weak law of large numbers imply
% \[ \Big | \frac{1}{n} \sum_{1 \leq i \leq n} A_i \gamma(X_i) - \frac{1}{n} \sum_{1 \leq i \leq n} A_i \tilde \gamma(X_i) \Big | \leq \frac{1}{n} \sum_{1 \leq i \leq n} |\gamma(X_i) - \tilde \gamma(X_i)| \xrightarrow{P} E[|\gamma(X_i) - \tilde \gamma(X_i)|] < \epsilon~. \]
% Suppose $\frac{1}{n} \sum_{1 \leq i \leq n} A_i \gamma(X_i)$ does not converge in probability to $E[\gamma(X_i)]$. Then, there exists $\epsilon' > 0$ and a subsequence, which with abuse of notation we still denote by $\{n\}$, along which
% \[ P \Big \{ \Big | \frac{1}{n} \sum_{1 \leq i \leq n} A_i \gamma(X_i) - \eta E[\gamma(X_i)] \Big | > \epsilon' \Big \} \to \delta > 0~. \]
% Along this subsequence, pick $\tilde \gamma$ associated with $\epsilon = \epsilon' / 3$, and get a contradiction.

% Next we show the claim in Example \ref{ex:MP2}. Lemma C.5 in \cite{cytrynbaum2023designing} implies that for every $\epsilon > 0$, there exists a square-integrable and Lipschitz function $\tilde \gamma$ such that $E[|\gamma(X_i) - \tilde \gamma(X_i)|^2] < \epsilon$. For $\tilde \gamma$ we already know that
% \[ \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} (A_i - \eta) \tilde \gamma(X_i) \xrightarrow{P} 0~. \]
% We also know from the triangle inequality and $(a + b)^2 \leq 2(a^2 + b^2)$ that
% \[ E \bigg [ \Big | \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} (A_i - \eta) \gamma(X_i) - \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} (A_i - \eta) \tilde \gamma(X_i) \Big |^2 \bigg | X^{(n)} \bigg ] \lesssim \frac{1}{n} \sum_{1 \leq i \leq n} (\gamma(X_i) - \tilde \gamma(X_i))^2~, \]
% so
% \[ E \bigg [ \Big | \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} (A_i - \eta) \gamma(X_i) - \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} (A_i - \eta) \tilde \gamma(X_i) \Big |^2 \bigg ] \lesssim E \Big [ \frac{1}{n} \sum_{1 \leq i \leq n} (\gamma(X_i) - \tilde \gamma(X_i))^2 \Big ] = \epsilon~. \]
% A subsequencing argument as above allows us to show
% \[ \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} (A_i - \eta) \gamma(X_i) \xrightarrow{P} 0~. \]
% Because the limit is 0, the desired convergence conditional on $X^{(n)}$ also holds. \qed


\clearpage

\bibliography{efficiency}

\end{document}