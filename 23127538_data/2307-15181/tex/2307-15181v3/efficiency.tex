\documentclass[10pt]{article}
\usepackage[hmargin=1in,vmargin=1in]{geometry}
\usepackage{setspace}
\usepackage{amsfonts,amsmath,amssymb,amsthm}
\usepackage[colorlinks,citecolor=blue]{hyperref}
\usepackage{natbib}
\usepackage[pdftex]{graphicx}
\usepackage{enumerate}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage[toc,title,titletoc,header]{appendix}
\usepackage{multirow}
\usepackage{threeparttable}
\usepackage{etoolbox} % This package goes here to add \qed to remarks automatically.
\bibliographystyle{ims}
% \usepackage{showlabels}
\allowdisplaybreaks

\renewcommand{\qed}{\rule{2mm}{2mm}}
\renewcommand*{\proofname}{\textsc{Proof}}
\newcommand{\indep}{\perp \!\!\! \perp}

\parskip = 1.5ex plus 0.5 ex minus0.2 ex

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{algorithm}{Algorithm}[section]
\newtheorem{condition}{Condition}[section]

\AtEndEnvironment{remark}{~\qed}
\AtEndEnvironment{example}{~\qed}

\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\diag}{diag}

\newcommand{\mycomment}[1]{}
\renewcommand{\baselinestretch}{1.35}

\begin{document}

\author{
Yuehao Bai \\
Department of Economics\\
University of Southern California \\
\url{yuehao.bai@usc.edu}
\and
Jizhou Liu \\
Booth School of Business \\
\textcolor{white}{111} University of Chicago \textcolor{white}{11}\\
\url{jliu32@chicagobooth.edu}
\and
Azeem M.\ Shaikh\\
Department of Economics\\
University of Chicago \\
\url{amshaikh@uchicago.edu}
\and
Max Tabord-Meehan\\
Department of Economics\\
University of Chicago \\
\url{maxtm@uchicago.edu}
}

\bigskip

\title{On the Efficiency of Finely Stratified Experiments \thanks{We thank the seminar participants at UC Riverside, UC Irvine, Cornell University, and Syracuse University for helpful comments. The fourth author acknowledges support from NSF grant SES-2149408.}}

\maketitle

\vspace{-0.3in}

\begin{spacing}{1.2}
\begin{abstract}
This paper examines finely stratified designs for the efficient estimation of treatment effect parameters in randomized experiments. In such designs, units are divided into groups of fixed size, with a proportion within each group randomly assigned to a binary treatment. We focus on parameters defined using moment conditions constructed from known functions of the observed data. We establish that the na\"ive method of moments estimator under a finely stratified design achieves the same asymptotic variance as that obtained using ex post covariate adjustment in i.i.d.\ designs, and further that this variance achieves the efficiency bound in a large class of designs.
%This paper studies the efficient estimation of a large class of treatment effect parameters that arise in the analysis of experiments.  Here, efficiency is understood to be with respect to a broad class of treatment assignment schemes for which the marginal probability that any unit is assigned to treatment equals a pre-specified value, e.g., one half.  Importantly, we do not require that treatment status is assigned in an i.i.d.\ fashion, thereby accommodating complicated treatment assignment schemes that are used in practice, such as stratified block randomization and matched pairs.  The class of parameters considered are those that can be expressed as the solution to a set of moment conditions involving a known function of the observed data, including possibly the pre-specified value for the marginal probability of treatment assignment.  We show that this class of parameters includes, among other things, average treatment effects, quantile treatment effects, local average treatment effects as well as the counterparts to these quantities in experiments in which the unit is itself a cluster.  In this setting, we establish two results.  First, we derive a lower bound on the asymptotic variance of estimators of the parameter of interest in the form of a convolution theorem.  Second, we show that the na\"ive method of moments estimator achieves this bound on the asymptotic variance quite generally if treatment is assigned using a ``finely stratified'' design.  By a ``finely stratified'' design, we mean experiments in which units are divided into groups of a fixed size and a proportion within each group is assigned to treatment uniformly at random so that it respects the restriction on the marginal probability of treatment assignment.  In this sense, ``finely stratified'' experiments lead to efficient estimators of treatment effect parameters ``by design'' rather than through {\it ex post} covariate adjustment.
%This paper studies the use of finely stratified designs for the efficient estimation of a large class of treatment effect parameters that arise in the analysis of experiments. By a ``finely stratified'' design, we mean experiments in which units are divided into groups of a fixed size and a proportion within each group is assigned to a binary treatment uniformly at random. The class of parameters considered are those that can be expressed as the solution to a set of moment conditions constructed using a known function of the observed data. They include, among other things, average treatment effects, quantile treatment effects, and local average treatment effects as well as the counterparts to these quantities in experiments in which the unit is itself a cluster. In this setting, we establish two results. First, we show that under a finely stratified design, the na\"ive method of moments estimator achieves the same asymptotic variance as what could typically be attained under alternative treatment assignment schemes only through {\it ex post} covariate adjustment. Second, we argue that in fact the na\"ive method of moments estimator under a finely stratified design is asymptotically efficient by deriving a lower bound on the asymptotic variance of ``regular'' estimators of the parameter of interest in the form of a convolution theorem. This result accommodates a large class of possible treatment assignment schemes that are used routinely throughout the sciences, such as stratified block randomization and matched pairs. In this sense, ``finely stratified'' experiments lead to efficient estimators of treatment effect parameters ``by design'' rather than through {\it ex post} covariate adjustment.
\end{abstract}
\end{spacing}

%CHAT GPT ABSTRACT
% This paper examines finely stratified designs for the efficient estimation of treatment effect parameters in randomized experiments. By a ``finely stratified'' design, we mean experiments in which units are divided into groups of a fixed size and a proportion within each group is assigned to a binary treatment uniformly at random. We focus on parameters defined using moment conditions constructed from known functions of observed data, including average, quantile, and local average treatment effects. We establish two key results: (1) the naÃ¯ve method of moments estimator under finely stratified designs achieves the same asymptotic variance as alternative schemes with ex post covariate adjustment, and (2) this estimator is asymptotically efficient, making finely stratified experiments inherently efficient.


\noindent KEYWORDS: Convolution Theorem, Efficiency, Experiment, Experimental design, Finely stratified experiment, Matched pairs, Randomized controlled trial

\noindent JEL classification codes: C12, C14

\thispagestyle{empty} 
\newpage
\setcounter{page}{1}

\section{Introduction}

This paper studies the use of finely stratified designs for the efficient estimation of a large class of treatment effect parameters that arise in the analysis of experiments. By a ``finely stratified'' design, we mean experiments in which units are divided into groups of a fixed size and a proportion within each group is assigned to a binary treatment uniformly at random. For example, when the fixed size equals two and the marginal probability of treatment assignment is specified to be one half, such a design is simply a matched pairs design. The class of parameters considered are those that can be expressed as the solution to a set of moment conditions constructed using a known function of the observed data.  This class of parameters includes many treatment effect parameters of interest: average treatment effects (ATEs), quantile treatment effects, and local average treatment effects as well as the counterparts to these quantities in experiments in which the unit is itself a cluster. 

In the setting described above, we establish two results. First, we study the asymptotic properties of a na\"ive method of moments estimator under a finely stratified design. Here, by a na\"ive method of moments estimator, we mean an estimator constructed using a direct sample analog of the moment conditions. For example, in the case of the ATE, such an estimator is given by a difference-in-means. We show that under a finely stratified design, the na\"ive method of moments estimator achieves the same asymptotic variance as what could typically be attained under alternative treatment assignment schemes only through {\it ex post} covariate adjustment. Such adjustment strategies frequently involve the nonparametric estimation of conditional expectations or similar quantities; see, for example, \cite{zhang2008improving}, \cite{tsiatis2008covariate}, \cite{jiang2022improving}, \cite{jiang2022regression-adjusted} and \cite{rafi2023efficient}.\footnote{For related results in the context of observational data see \cite{newey1994asymptotic}, \cite{hahn1998role}, \cite{heckman1998matching}, \cite{imbens2004nonparametric}, \cite{frolich2007nonparametric}, \cite{firpo2007efficient}, \cite{imbens2007mean-squared}, \cite{farrell2015robust}, \cite{abadie2016matching}, and \cite{chernozhukov2017doubledebiasedneyman}.} Second, we argue that in fact the na\"ive method of moments estimator under a finely stratified design is asymptotically efficient by deriving a lower bound on the asymptotic variance of ``regular'' estimators of the parameter of interest in the form of a convolution theorem. This result accommodates a large class of possible treatment assignment schemes that are used routinely throughout the sciences, such as stratified block randomization and matched pairs.\footnote{For a discussion of such treatment assignment schemes focused on clinical trials, see \cite{rosenberger2015randomization}; for reviews focused on development economics, see \cite{duflo2007using} and \cite{bruhn2009pursuit}.} We emphasize that, since treatment assignment schemes used in experiments are generally not i.i.d., classical results in, for example, \cite{van_der_vaart1998asymptotic}, do not directly apply. Putting these two results together, we show that finely stratified experiments remarkably lead to efficient estimators for a large class of treatment effect parameters while avoiding {\it ex post} covariate adjustment. In other words, finely stratified experiments can achieve full efficiency while employing a simple ``hands above the table" estimator \citep[][]{freedman2008regression, lin2013agnostic} in that they avoid specification search and prioritize transparency; borrowing the terminology of \cite{cytrynbaum2023designing}, finely stratified experiments effectively perform nonparametric regression adjustment ``by design.''

Our paper builds upon two strands of literature. The first strand of literature concerns the analysis of finely stratified experiments.  Within this literature, our analysis is most closely related to \cite{bai2022inference}, who derives the asymptotic behavior of the difference-in-means estimator of the ATE when treatment is assigned according to a matched pairs design, and  %Various extensions of this result are developed in \cite{bai2023covariate}, \cite{cytrynbaum2023covariate}, \cite{bai2023inference}, \cite{bai2023inference-1}, and  \cite{cytrynbaum2023designing}.
\cite{cytrynbaum2023designing}, who develops related results for an experimental design which he calls ``local randomization," which permits the proportion of units assigned to treatment to vary with the baseline covariates. Beyond settings which study estimation of the ATE, \cite{bai2023inference-1} develops results for the analysis of different cluster-level average treatment effects and \cite{jiang2021bootstrap} develop results analogous to those in \cite{bai2022inference} for suitable estimators of the quantile treatment effect. To our knowledge, our paper is the first to analyze the properties of finely stratified experiments in a general framework which accommodates all of the above parameters as well as any parameter that can be characterized as the solutions to a set of moment conditions involving a known function of the observed data. Moreover, none of the above papers formally establish the asymptotic efficiency of finely stratified experiments. However, we note that some finite-sample optimality properties of matched pairs designs for estimation of the ATE are developed in \cite{bai2022optimality}. The second strand of literature concerns bounds on the efficiency with which treatment effect parameters can be estimated in experiments. Two important recent papers in this literature studying efficiency bounds in the special case of estimating the ATE are \cite{armstrong2022asymptotic} and \cite{rafi2023efficient}. Even in this special case, their results do not apply to our setting; Remark \ref{rem:other_SPEB} provides an in-depth discussion of the connection between these results and ours.

The remainder of this paper is organized as follows.  In Section \ref{sec:setup}, we describe our setup and notation.  We emphasize in particular the way in which our framework can accommodate various treatment effect parameters of interest.  Section \ref{sec:variance} derives the asymptotic behavior of the na\"ive method of moments estimator of our parameter of interest when treatment is assigned using a finely stratified design. In Section \ref{sec:semi}, we develop our lower bound on the asymptotic variance of ``regular'' estimators of these parameters and show that it is achieved by the the na\"ive method of moments estimator in a finely stratified design.  In Section \ref{sec:sims}, we illustrate the practical relevance of our theoretical results by comparing the mean-squared errors of the na\"ive method of moments estimators for the ATE and local average treatment effect when treatment status is assigned according to a matched pairs design versus that of an estimator using {\it ex post} covariate adjustment when treatment status is assigned in an i.i.d.\ fashion. Finally, we conclude in Section \ref{sec:recs} with some recommendations for empirical practice guided by both these simulations and our theoretical results. Proofs of all results can be found in the Appendix.

\section{Setup and Motivation}\label{sec:setup}

Let $A_i \in \{0, 1\}$ denote the treatment status of the $i$th unit, and let $X_i \in \mathbf R^{d_x}$ denote their observed, baseline covariates. For $a \in \{0, 1\}$, let $R_i(a) \in \mathbf R^{d_r}$ denote a \emph{vector} of potential responses. As we illustrate below, considering a vector of responses allows us to accommodate certain parameters of interest. Let $R_i \in \mathbf R^{d_r}$ denote the vector of observed responses obtained from $R_i(a)$ once treatment is assigned. As usual, the observed responses and potential responses are related to treatment status by the relationship
\begin{equation}\label{eq:PO}
R_i = R_i(1) A_i + R_i(0) (1 - A_i)~. 
\end{equation}
We assume throughout that our sample consists of $n$ units. For any random vector indexed by $i$, for example $A_i$, we define $A^{(n)} = (A_1, \ldots, A_n)$. Let $P_n$ denote the distribution of the observed data $(R^{(n)}, A^{(n)}, X^{(n)})$, and $Q_n$ the distribution of $(R^{(n)}(1), R^{(n)}(0), X^{(n)})$. We assume $Q_n = Q^n$, where $Q$ is the marginal distribution of $(R_i(1), R_i(0), X_i)$. Given $Q_n$, $P_n$ is then determined by \eqref{eq:PO} and the mechanism for determining treatment assignment. We assume that treatment assignment is performed such that a standard unconfoundedness assumptions holds and such that the probability of assignment given $X$ is some known constant for every $1 \le i \le n$:

\begin{assumption} \label{ass:unconfounded}
Treatment status is assigned so that
\begin{equation} \label{eq:unconfounded}
    (R^{(n)}(1), R^{(n)}(0)) \indep A^{(n)} | X^{(n)}~,
\end{equation}
and such that $P\{A_i = 1|X_i=x\} = \eta$, for some $\eta \in (0, 1)$ for all $1 \le i \le n$.
\end{assumption}

We note that although Assumption \ref{ass:unconfounded} restricts the probability of assignment to be the fixed fraction $\eta$ across the entire experimental sample, this is largely for expositional purposes. This restriction can be weakened for many of our subsequent results: see in particular Remarks \ref{rem:generaleta-attained} and \ref{rem:generaleta}. Given Assumption \ref{ass:unconfounded}, it can be shown that $(X_i, A_i, R_i)$ are identically distributed for $1 \le i \le n$, and their marginal distribution does not change with $n$ (see Lemma \ref{lem:marginal} in the Appendix). As a consequence, we denote the marginal distribution of $(X_i, A_i, R_i)$ by $P$. We consider parameters $\theta_0 \in \Theta \subset \mathbf R^{d_\theta}$ which can be defined as the solution to a set of moment equalities. In particular, let $m: \mathbf{R}^{d_x} \times \{0, 1\} \times \mathbf R^{d_r} \to \mathbf R^{d_\theta}$ be a known measurable function, then we consider parameters $\theta_0$ which uniquely solve the moment equality
\begin{equation} \label{eq:moments}
E_P[m(X_i, A_i, R_i, \theta_0)] = 0~,
\end{equation}
 where we emphasize that $m(\cdot)$ is not a function of any unknown nuisance parameters, but may depend on the known value of $\eta$ in Assumption \ref{ass:unconfounded}. We present five examples of well-known parameters which can be described as (functions of) solutions to a set of moment conditions as in \eqref{eq:moments}.
 %, and note that further examples are provided in \cite{zhang2008improving}.

\begin{example}[Average Treatment Effect]\label{ex:ATE}
Let $Y_i(a) = R_i(a)$ denote a scalar potential outcome for the $i$th unit under treatment $a \in \{0, 1\}$, and let $Y_i = R_i$ denote the observed outcome. Let $\theta_0 = E_Q[Y_i(1) - Y_i(0)]$ denote the average treatment effect (ATE). Under Assumption \ref{ass:unconfounded}, $\theta_0$ solves the moment condition in \eqref{eq:moments} with
\begin{equation} \label{eq:moments-ate}
   m(X_i, A_i, R_i, \theta) = \frac{Y_i A_i}{\eta} - \frac{Y_i (1 - A_i)}{1 - \eta} - \theta~. 
\end{equation}
For a list of papers which consider estimators based on \eqref{eq:moments-ate}, see \cite{hirano2001estimation} and \cite{hirano2003efficient}.
\end{example}

\begin{example}[Quantile Treatment Effect]\label{ex:QTE}
Let $Y_i(a) = R_i(a)$ denote a scalar potential outcome for the $i$th unit under treatment $a \in \{0, 1\}$, and let $Y_i = R_i$ denote the observed outcome. Let $\tau \in (0, 1)$ and $\theta_0 = (\theta_0(1), \theta_0(0))' = (q_{Y(1)}(\tau), q_{Y(0)}(\tau))'$, where
\[ q_{Y(a)}(\tau) = \inf \{\lambda \in \mathbf R: Q \{Y_i(a) \leq \lambda \} \geq \tau\}~. \]
In other words, $\theta_0$ is defined to be the vector of $\tau$th quantiles of the marginal distributions of $Y_i(1)$ and $Y_i(0)$. If we assume $q_{Y(a)}(\tau)$ is unique for $a \in \{0, 1\}$ in the sense that $Q\{Y(a) \leq q_{Y(a)}(\tau) + \epsilon\} > Q\{Y(a) \leq q_{Y(a)}(\tau)\}$ for all $\epsilon > 0$, then it follows from Assumption \ref{ass:unconfounded} and Lemma 1 in \cite{firpo2007efficient} that $\theta_0$ solves the moment condition in \eqref{eq:moments} with
\[ m(X_i, A_i, R_i, \theta) = \begin{pmatrix}
\displaystyle \frac{A_i (\tau - I \{Y_i \leq \theta(1)\})}{\eta} \\
\displaystyle \frac{(1 - A_i) (\tau - I \{Y_i \leq \theta(0)\})}{1 - \eta}
\end{pmatrix}~, \]
for $\theta = (\theta^{(1)}, \theta^{(0)})'$. Note that the quantile treatment effect $q_{Y(1)}(\tau) - q_{Y(0)}(\tau)$ can then be defined as $h(\theta_0)$ where $h:\mathbf{R}^2 \to \mathbf{R}$ is given by $h(s,t) = s - t$.
\end{example}

\begin{example}[Local Average Treatment Effect]\label{ex:LATE}
Let $(\tilde{Y}_i(a), D_i(a)) = R_i(a)$ denote the vector of potential outcomes and treatment take-up under treatment $a \in \{0, 1\}$, and let $(Y_i, D_i) = R_i$ denote the vector of observed outcomes and treatment take-up. Note here that $\tilde{Y}_i(a)$ corresponds to the potential outcome under assignment $a \in \{0,1\}$ and not to the potential outcome for a given take-up $D_i = d$. Suppose $E_Q[D_i(1) - D_i(0)] \ne 0$ and let
\[ \theta_0 = \frac{E_Q[\tilde{Y}_i(1) - \tilde{Y}_i(0)]}{E_Q[D_i(1) - D_i(0)]}~. \]
It then follows from Assumption \ref{ass:unconfounded} that $\theta_0$ solves the moment condition in \eqref{eq:moments} with
\begin{equation} \label{eq:moments-late}
    m(X_i, A_i, R_i, \theta) = \frac{Y_i A_i}{\eta} - \frac{Y_i (1 - A_i)}{1 - \eta} - \theta \left ( \frac{D_i A_i}{\eta} - \frac{D_i (1 - A_i)}{1 - \eta} \right )~.
\end{equation}
If we further assume instrument monotonicity (i.e., $P\{D_i(1) \ge D_i(0)\} = 1$) and instrument exclusion, then $\theta_0$ could be re-interpreted as the local average treatment effect (LATE) in the sense of \cite{imbens1994identification}.
\end{example}

\begin{example}[Weighted Average Treatment Effect]\label{ex:Clust_ATE}
Let $Y_i(a) = R_i(a)$ denote a scalar potential outcome for the $i$th unit under treatment $a \in \{0, 1\}$, and let $Y_i = R_i$ denote the observed outcome. 
Let 
\[\theta_0 = E_Q\left[\frac{\omega(X_i)}{E_Q[\omega(X_i)]}\left(Y_i(1) - Y_i(0)\right)\right]~,\]
for some known function $\omega: \mathbf{R}^{d_x} \to \mathbf{R}$.
It then follows from Assumption \ref{ass:unconfounded} that $\theta_0$ solves the moment condition in \eqref{eq:moments} with
\[ m(X_i, A_i, R_i, \theta) = \omega(X_i)\left(\frac{Y_i A_i}{\eta} - \frac{Y_i (1 - A_i)}{1 - \eta}\right) - \omega(X_i)\theta~.\] 
Note that $\theta_0$ defined in this way can accommodate the (cluster) size-weighted and equally-weighted average treatment effects considered in \cite{bugni2022inference} and \cite{bai2023inference-1} in the context of cluster-level randomized controlled trials. 
% Note that $\theta_0$ is directly related to the average treatment effects defined in the context of cluster randomized experiments considered in \cite{bugni2022inference} and \cite{bai2023inference-1}.
\end{example}

\begin{example}[Log-Odds Ratio]\label{ex:log_odds}
Let $Y_i(a) = R_i(a) \in \{0, 1\}$ denote a binary potential outcome for the $i$th unit under treatment $a \in \{0, 1\}$, and let $Y_i = R_i$ denote the observed outcome.
Suppose $0 < P\{Y_i(a) = 0\} < 1$ for $a \in \{0, 1\}$, and let $\theta_0 = (\theta_0(1), \theta_0(2))'$, where
\[\theta_0(1) = \text{logit}(E_Q[Y_i(0)])~,\]
\[\theta_0(2) = \text{logit}(E_Q[Y_i(1)]) - \text{logit}(E_Q[Y_i(0)])~,\]
with $\text{logit}(z) = \log(\frac{z}{1-z})$, so that $\theta_0(2)$ denotes the log-odds ratio of treatment $1$ relative to treatment $0$. It follows from Assumption \ref{ass:unconfounded} that $\theta_0$ solves the moment condition in \eqref{eq:moments} with
\[m(X_i, A_i, R_i, \theta) =  \begin{pmatrix}
\displaystyle 1 - A_i \\
\displaystyle A_i
\end{pmatrix}\left(Y_i - \text{expit}(\theta(1) + \theta(2)A_i)\right)~,\]
where $\text{expit}(z) = \frac{\exp(z)}{1 + \exp(z)}$. The log-odds ratio can then be defined as $h(\theta_0)$ where $h: \mathbf{R}^2 \to \mathbf{R}$ is given by $h(s,t) = t$. 
%{\bf [discussion]}
%\cite{zhang2008improving}
% \cite{casaburi2022using} [...]
% Let (P_i(a),Q_i(a)) = R_i(a) denote the vector of potential prices and quantities under treatment $a \in \{0, 1\}$. Let $\Gamma$ be a parameter that measures the differentiation of traders in the market, and $\beta$ be the slope of the supply curve faced by traders. Then in the model of Cournot competition posited by \cite{casaburi2022using}, these parameters are identified by the equations
% \[\]
\end{example}
Additional examples could be obtained by considering combinations of Examples \ref{ex:ATE}--\ref{ex:log_odds}. For instance, combining the moment functions from Examples \ref{ex:LATE} and \ref{ex:Clust_ATE} would result in a weighted LATE parameter. Beyond these examples, certain treatment effect contrasts could also be related to the structural parameters in, for instance, a model of supply in demand: see, for example, the model estimated in \cite{casaburi2022using}.
% \[ \begin{pmatrix}
%     A_g (\bar Y_g - N_g \theta(1)) \\
%     (1 - A_g) (\bar Y_g - N_g \theta(0))
% \end{pmatrix} \]

Throughout the rest of the paper we consider the asymptotic properties of the method  of moments estimator $\hat{\theta}_n$ for $\theta_0$ which is constructed as a solution to the sample analogue of \eqref{eq:moments}:
\begin{equation} \label{eq:est}
\frac{1}{n} \sum_{1 \leq i \leq n} m(X_i, A_i, R_i, \hat \theta_n) = 0~.
\end{equation}
Note that $\hat \theta_n$ as defined in \eqref{eq:est} is closely related to standard estimators of the parameter $\theta_0$ in specific examples. For instance, in Example \ref{ex:ATE}, \[\hat{\theta}_n = \frac{1}{\eta}\sum_{1 \le i \le n}Y_iA_i - \frac{1}{1 - \eta}\sum_{1 \le i \le n}Y_i(1 - A_i)~,\]
so that $\hat{\theta}_n$ is a Horvitz-Thompson analogue of the standard difference-in-means estimator for the ATE. In Example \ref{ex:LATE},
\[\hat{\theta}_n = \frac{\frac{1}{\eta}\sum_{1 \le i \le n}Y_iA_i - \frac{1}{1 - \eta}\sum_{1 \le i \le n}Y_i(1 - A_i)}{\frac{1}{\eta}\sum_{1 \le i \le n}D_iA_i - \frac{1}{1 - \eta}\sum_{1 \le i \le n}D_i(1 - A_i)}~,\]
so that $\hat{\theta}_n$ is a Horvitz-Thompson analogue of the standard Wald estimator for the local average treatment effect. 

To illustrate the key contribution of our paper, note that if $A^{(n)}$ were assigned i.i.d., independently of $X^{(n)}$, then it can be shown under mild conditions on $m(\cdot)$ \citep[see, for instance, Theorem 5.1 in][]{van_der_vaart1998asymptotic} that the na\"ive method of moments estimator satisfies 
\[\sqrt{n}(\hat{\theta}_n - \theta_0) \overset{d}{\to} N(0, \mathbb{V})~,\]
where
\begin{equation}\label{eq:naive_variance}
\mathbb V = M^{-1}E_P[m(X_i, A_i, R_i, \theta_0)m(X_i, A_i, R_i, \theta_0)'](M^{-1})^{\prime}~,
\end{equation}
with $M = \frac{\partial}{\partial \theta} E_P[m(X, A, R, \theta)] \Big |_{\theta = \theta_0}$. In Section \ref{sec:variance}, we show that if we assign $A^{(n)}$ using a finely-stratified design (i.e., a treatment assignment scheme which uses the covariates $X^{(n)}$ to block units into groups of fixed size: see Assumption \ref{ass:a} below for a formal definition), then 
\[\sqrt{n}(\hat{\theta}_n - \theta_0) \overset{d}{\to} N(0, \mathbb{V}_*)~,\]
where $\mathbb{V} \ge \mathbb{V}_*$ (see Theorem \ref{thm:normal}). Under i.i.d.\ assignment, the na\"ive method of moment esimator $\hat \theta_n$ cannot generally attain $\mathbb V_\ast$, but an estimator which attains $\mathbb{V}_*$  could instead be constructed by appropriately ``augmenting" the moment function, and then considering an estimator which solves the augmented moment equation. For instance, if we consider the ATE in Example \ref{ex:ATE}, then it is straightforward to show that the following augmented moment function identifies $\theta_0$:
\begin{equation}\label{eq:augment_m}
m^*(X_i, A_i, R_i, \theta) = \left(\frac{A_i(Y_i - \mu_1(X_i))}{\eta} - \frac{(1 - A_i)(Y_i - \mu_0(X_i))}{1 - \eta} + \mu_1(X_i) - \mu_0(X_i)\right) - \theta~,
\end{equation}
where $\mu_a(X_i) = E_Q[Y_i(a)|X_i]$. 
This choice of $m^*(\cdot)$ produces the well known doubly-robust moment condition for estimating the ATE \citep{robins1995analysis,hahn1998role}. It can then be shown that an appropriately constructed two-step estimator, where $\mu_1(\cdot)$ and $\mu_0(\cdot)$ are non-parametrically estimated in a first step, attains $\mathbb{V}_*$  \citep{tsiatis2008covariate,farrell2015robust,chernozhukov2017doubledebiasedneyman, rafi2023efficient}. Intuitively, the estimator obtained from the augmented moment function $m^*(\cdot)$ performs nonparametric regression adjustment by exploiting the information contained in $X^{(n)}$ which may not have been captured in the original moment function $m(\cdot)$.  Similar nonparametric regression adjustments based on augmented moment equations have been developed for other parameters of interest \citep{zhang2008improving, belloni2017program,jiang2022improving,jiang2022regression-adjusted}.   In this sense, we show that ``fine stratification'' can perform nonparametric regression adjustment ``by design'' for the large class of parameters that can be expressed in terms of moment conditions of the form given in \eqref{eq:moments}, thus generalizing similar observations made in \cite{bai2022inference}, \cite{bai2022optimality}, and \cite{cytrynbaum2023designing} in the special case of estimating the ATE. 

Earlier work on efficient treatment effect estimation has noted that the variance $\mathbb{V}_*$ is in fact the efficiency bound for estimating $\theta_0$ under i.i.d.\ assignment. A natural follow-up question is whether or not $\mathbb{V}_*$ continues to be the efficiency bound for estimating $\theta_0$ under a finely stratified design, or more generally for complex experimental designs which induce dependence in the treatment assignments across individuals in the experiment. In Section \ref{sec:semi}, we show that $\mathbb V_\ast$ continues to be the efficiency bound for estimating $\theta_0$ for a large class of treatment assignment schemes with a fixed marginal probability of treatment assignment, which includes finely stratified designs as a special case. We can thus conclude that, from the perspective of asymptotic efficiency, finely stratified designs are optimal experimental designs for a broad range of treatment effect estimation problems. 
%Moreover, our results show that finely stratified experiments can achieve full efficiency while employing a simple ``hands above the table" estimator \citep[][]{freedman2008regression, lin2013agnostic} in that they avoid specification search and prioritize transparency.



\section{The Asymptotic Variance of Finely Stratified Experiments}\label{sec:variance}

In this section, we derive the limiting distribution of the method of moments estimator $\hat \theta_n$ when treatment is assigned by fine stratification over the baseline covariates $X^{(n)}$. Such assignment mechanisms use the covariates $X^{(n)}$ to group units with ``similar'' covariate values into blocks of fixed size, and then assign treatment completely at random within each block.  In order to describe this assignment mechanism formally, we require some further notation to define the blocks of units. Let $\ell$ and $k$ be arbitrary positive integers with $\ell \le k$ and set $\eta = \ell/k$. For simplicity, assume that $n$ is divisible by $k$. We then represent blocks of units using a partition of $\{1, \ldots, n\}$ given by
\[\left\{\lambda_j = \lambda_j(X^{(n)}) \subseteq \{1, \ldots, n\}, 1 \le j \le n/k\right\}~,\]
with $|\lambda_j| = k$.  Because of its possible dependence on $X^{(n)}$, $\{\lambda_j: 1 \le j \le n/k\}$ encompasses a variety of different ways of blocking the $n$ units according to the observed, baseline covariates.  Given such a partition, we assume that treatment status is assigned as described in the following assumption:
\begin{assumption} \label{ass:a}
Treatment status is assigned so that $(R^{(n)}(1), R^{(n)}(0)) \indep A^{(n)} \big | X^{(n)}$ and, conditional on $X^{(n)}$, 
\[\{(A_i: i \in \lambda_j): 1 \le j \le n/k\}\]
are i.i.d.\ and each uniformly distributed over all permutations of $(\underbrace{0, 0, \ldots, 0}_{k - \ell}, \underbrace{1, 1, \ldots, 1}_{\ell})$.
\end{assumption}

The assignment mechanism described in Assumptions \ref{ass:a} generalizes the definition of a matched pairs design. In particular, we recover a matched pairs design if we set $(\ell, k) = (1, 2)$, with $\eta = 1/2$. Indeed, suppose $n$ is even and consider pairing the experimental units into $n / 2$ pairs, represented by the sets
\[ \{\pi(2j - 1), \pi(2j)\} \text{ for } j = 1, \ldots, n / 2~, \]
where $\pi = \pi_n(X^{(n)})$ is a permutation of $n$ elements.  Because of its possible dependence on $X^{(n)}$, $\pi$ encompasses a broad variety of ways of pairing the $n$ units according to the observed, baseline covariates $X^{(n)}$. Given such a $\pi$, we assume that treatment status is assigned so that Assumption \ref{ass:a} holds and, conditional on $X^{(n)}$, $(A_{\pi(2j-1)}, A_{\pi(2j)}), j = 1, \ldots, n / 2$ are i.i.d.\ and each uniformly distributed over the values in $\{(0,1), (1,0)\}$. For some examples of such an assignment mechanism being used in practice, see, for instance, \cite{angrist2009effects}, \cite{banerjee2015miracle}, and \cite{bruhn2016impact}.

\begin{remark}\label{rem:matched_pairs}
Note that Assumption \ref{ass:a} generalizes matched pairs designs along two dimensions: first, it allows for treatment fractions other than $\eta = 1/2$. Second, it allows for choices of $\ell$ and $k$ which are not relatively prime. For instance, if we set $(\ell, k) = (2, 4)$, then $\eta = 1/2$ as in matched pairs, but now the assignment mechanism blocks units into groups of size $4$ and assigns two units to treatment, two units to control. Although Theorem \ref{thm:normal} below establishes that allowing for this level of flexibility has no effect on the asymptotic properties of our estimator, in our experience we have found that designs which employ these treatment ``replicates" in each block can simplify the construction of variance estimators in practice.  See Remark \ref{rem:var_est} below for further discussion.
\end{remark}

Our analysis will require some discipline on the way in which the blocks are formed.  In particular, we will require that the units in each block be ``close'' in terms of their baseline covariates in the sense described by the following assumption:
\begin{assumption} \label{ass:pair}
The blocks used in determining treatment status satisfy
\[ \frac{1}{n} \sum_{1 \leq j \leq n/k} \max_{i, i' \in \lambda_j} \|X_{i} - X_{i'}\|^2 \stackrel{P}{\rightarrow} 0~. \]
\end{assumption}
\cite{bai2022inference} and \cite{cytrynbaum2023designing} discuss blocking algorithms which satisfy Assumption \ref{ass:pair}. When $X_i \in \mathbf{R}$ and $E[X_i^2] < \infty$, a simple algorithm which satisfies Assumption \ref{ass:pair} is to simply order units from smallest to largest and then block adjacent units into blocks of size $k$.

The next two sets of assumptions allow us to derive the large sample properties of $\hat{\theta}_n$. We impose Assumption \ref{ass:consistency} to establish the consistency of $\hat{\theta}_n$, and we further impose Assumption \ref{ass:normal} to establish its limiting distribution. In what follows, when writing expectations and variances, we suppress the subscripts $P$ and $Q$ whenever doing so does not lead to confusion.

\begin{assumption} \label{ass:consistency}
Let $m(\cdot) = (m_s(\cdot): 1 \le s \le d_{\theta})'$. Then the moment functions are such that
\begin{enumerate}[\rm (a)]
    \item For every $\epsilon > 0$, $\inf\limits_{\theta \in \Theta: \|\theta - \theta_0\| > \epsilon} \| E[m(X_i, A_i, R_i, \theta)] \| > 0$.
    \item For $1 \le s \le d_{\theta}$, $\{m_s(x, a, r, \theta): \theta \in \Theta\}$ with $a \in \{0, 1\}$ fixed is a VC-class of functions in $(x, r)$.
     \item For $1 \leq s \leq d_\theta$, $\{m_s(x, a, r, \theta): \theta \in \Theta\}$ is pointwise measurable in the sense that there exists a countable set $\Theta^\ast$ such that for each $\theta \in \Theta$, there exists a sequence $\{\theta_m\} \subset \Theta^\ast$ such that $m_s(x, a, r, \theta_m) \to m_s(x, a, r, \theta)$ as $m \to \infty$ for all $x, a, r$.
    \item $E \bigg [ \sup\limits_{\theta \in \Theta^\ast} \| m(X, a, R(a), \theta) \| \bigg ] < \infty$ for $a \in \{0, 1\}$.
    \item For some $K < \infty$,
    % \[ \sup_{\theta \in \Theta^\ast} \|E[m(X, a, R(a), \theta) | X = x]\| \leq K \]
    % and
    \[ \sup_{\theta \in \Theta^\ast}~ \| E[m(X, a, R(a), \theta) | X = x] - E[m(X, a, R(a), \theta) | X = x'] \| \leq K \|x - x'\| \]
    for all $x, x' \in \mathbf R^{d_x}$.
    \item $E \bigg [ \sup\limits_{\theta \in \Theta^\ast}~ \big \| m(X, a, R(a), \theta) \big \| \Big | X = x \bigg ]$ is Lipschitz for $a \in \{0, 1\}$.
\end{enumerate}
\end{assumption}

Assumption \ref{ass:consistency}(a) is a standard assumption to ensure the solution to \eqref{eq:moments} is ``well separated.'' It appears as a condition, for instance, in Theorem 5.9 in \cite{van_der_vaart1998asymptotic} . Assumption \ref{ass:consistency}(b) can be readily verified in Examples \ref{ex:ATE}--\ref{ex:log_odds} because the moment conditions are either constructed as linear functions in $\theta$ (multiplied or composed with fixed functions), or dependent on $\theta$ through indicator functions. Assumption \ref{ass:consistency}(c) is a standard condition to guarantee the measurability of the supremum of a suitable class of functions. In particular, it allows us to define expectations of suprema without invoking outer expectations. See Example 2.3.4 in \cite{van_der_vaart1996weak} for details. Assumption \ref{ass:consistency}(d) guarantees the existence of an envelope function needed to establish a uniform law of large numbers. Assumptions \ref{ass:consistency}(e)--(f) mirror common assumptions used when studying matched pairs designs to ensure units that are close in terms of the baseline covariates are also close in terms of their moments. 

\begin{assumption} \label{ass:normal}
Let $m(\cdot) = (m_s(\cdot): 1 \le s \le d_{\theta})'$. The moment functions are such that
\begin{enumerate}[\rm (a)]
    \item $E[m(X_i, A_i, R_i, \theta)]$ is differentiable at $\theta_0$ with a nonsingular derivative $M$.
    \item For $\Theta^\ast$ in Assumption \ref{ass:consistency}(c), $E \bigg [ \sup\limits_{\theta \in \Theta^\ast}~ \big \| m(X, a, R(a), \theta) \big \|^2 \bigg ] < \infty$ for $a \in \{0, 1\}$.
    \item For $1 \leq s \leq d_\theta$, $E[((m_s(X, a, R(a), \theta) - m_s(X, a, R(a), \theta_0))^2] \to 0$ as $\theta \to \theta_0$ for $a \in \{0, 1\}$.
    \item For $1 \leq s \leq d_\theta$, $\{E[(m_s(X, a, R(a), \theta) | X = x]: \theta \in \Theta\}$ is a VC-class of functions for $a \in \{0, 1\}$.
    %\item For $1 \leq s \leq d_\theta$, $\{h(x_1, x_2, \theta) = E_Q[(m_s(X, a, R(a), \theta) | X = x_1] - E_Q[(m_s(X, a, R(a), \theta) | X = x_2]: \theta \in \Theta\}$ is a VC-class of functions.
    \item For $\Theta^\ast$ in Assumption \ref{ass:consistency}(c) and some $K < \infty$, for $1 \leq s \leq d_\theta$,
    \begin{align*}
        % & \sup_{\theta \in \Theta^\ast} |E_Q[m_s^2(X, a, R(a), \theta) | X = x]| \leq K \\
        & \sup_{\theta \in \Theta^\ast}~ | E[m_s^2(X, a, R(a), \theta) | X = x] - E[m_s^2(X, a, R(a), \theta) | X = x'] | \leq K \|x - x'\| \\
        & \sup_{\theta \in \Theta^\ast}~ | E[m_s(X, a, R(a), \theta) m_s(X, a, R(a), \theta_0) | X = x] \\
        & \hspace{5em} - E[m_s(X, a, R(a), \theta) m_s(X, a, R(a), \theta_0) | X = x'] | \leq K \|x - x'\|~,  
    \end{align*}
    for all $x, x' \in \mathbf R^{d_x}$, $a \in \{0, 1\}$
    \item For $\Theta^{\ast}$ in Assumption \ref{ass:consistency}(c), $E \bigg [ \sup\limits_{\theta \in \Theta^\ast}~ \big \| m(X, a, R(a), \theta) \big \|^2 \Big | X = x \bigg ]$ is Lipschitz for $a \in \{0, 1\}$.
\end{enumerate}
\end{assumption}

Assumption \ref{ass:normal}(a) is a standard assumption used when deriving the properties of $Z$-estimators. See, for instance, Theorem 3.1 in \cite{newey1994large} and Theorem 5.21 in \cite{van_der_vaart1998asymptotic}. Because differentiability is imposed on their expectations instead of the moment functions themselves, the moment functions are allowed to be nonsmooth as in Example \ref{ex:QTE}. Assumption \ref{ass:normal}(b) guarantees the existence of an envelope function needed to establish a uniform law of large numbers. Assumption \ref{ass:normal}(c) implies the moment functions are mean-square continuous in $\theta$. Assumptions \ref{ass:normal}(e)--(f) again mirror common assumptions used when studying matched pairs to ensure units that are close in terms of the baseline covariates are also close in terms of their moments. Assumption \ref{ass:normal}(d) is again readily verified in Examples \ref{ex:ATE}, \ref{ex:LATE}--\ref{ex:log_odds} because $\theta$ enters separably in these examples. To verify the assumption for Example \ref{ex:QTE}, note that for any random variables $Y(a), X$, the subgraphs $\{(x, t): t < P \{Y(a) \leq \theta(a) | X = x\}\}$ are linearly ordered in $\theta(a)$ because the conditional distribution function is increasing in $\theta(a)$. Therefore, the class of subgraphs is VC with index 2 \citep[see, for instance, the last sentence of the proof of Lemma 2.6.16 in][]{van_der_vaart1996weak}.

\begin{remark}\label{rem:assumption_strength}
We note that some of the assumptions imposed in Assumptions \ref{ass:consistency} and \ref{ass:normal} are seemingly more stringent than the low-level conditions considered in previous papers which study inference for certain specific parameters of interest under matched pairs designs \citep[][]{bai2022inference,jiang2021bootstrap,cytrynbaum2023designing,bai2023late}. We suspect that, with more delicate arguments, some of these assumptions could be weakened for specific parameters of interest: for example, following an approximation argument in \cite{cytrynbaum2023designing} in the special case of the ATE, we expect Assumptions \ref{ass:consistency}(e) and \ref{ass:normal}(e) could be dropped whenever the moment functions $m_s(\cdot)$ are linear in the parameter $\theta$. However, to accommodate more general, possibly nonlinear moment functions, we do not pursue this further in the paper.
\end{remark}

The following theorem establishes the asymptotic variance of the ``na\"ive'' method of moments estimator when the treatment assignment mechanism is finely stratified in the sense of satisfying Assumptions \ref{ass:a}--\ref{ass:pair}.

\begin{theorem} \label{thm:normal}
Suppose the treatment assignment mechanism satisfies Assumptions \ref{ass:a}--\ref{ass:pair} and the moment functions satisfy Assumptions \ref{ass:consistency}--\ref{ass:normal}. Let $\hat{\theta}_n$ be defined as in \eqref{eq:est}. Then,
\begin{equation} \label{eq:normal}
\sqrt n(\hat \theta_n -  \theta_0) = \frac{1}{\sqrt{n}} \sum_{1 \leq i \leq n} \psi^\ast(X_i, A_i, R_i, \theta_0) + o_P(1)~.
\end{equation}
where
\begin{align*}
& \psi^\ast(X_i, A_i, R_i, \theta_0) \\
& = - M^{-1} \Big ( I\{A_i = 1\} (m(X_i, 1, R_i, \theta_0) - E[m(X_i, 1, R_i(1), \theta_0) | X_i]) \\
& \hspace{3.5em} + I\{A_i = 0\} (m(X_i, 0, R_i, \theta_0) - E[m(X_i, 0, R_i(0), \theta_0) | X_i]) \\
& \hspace{3.5em} + \eta E[m(X_i, 1, R_i(1), \theta_0) | X_i] + (1 - \eta) E[m(X_i, 0, R_i(0), \theta_0) | X_i] \Big )~,
\end{align*}
and $M = \frac{\partial}{\partial \theta} E[m(X, A, R, \theta)] \Big |_{\theta = \theta_0}$.
%  = - M^{-1} \\
% \times \bigg ( \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} \sum_{a \in \{0, 1\}} I \{A_i = a\} (m(X_i, a, R_i(a), \theta_0) - E_Q[m(X_i, a, R_i(a), \theta_0) | X_i]) \\
% + \frac{\eta}{\sqrt n} \sum_{1 \leq i \leq n} (E_Q[m(X_i, 1, R_i(1), \theta_0) | X_i] - E_Q[m(X, 1, R(1), \theta_0)]) \\
% + \frac{1 - \eta}{\sqrt n} \sum_{1 \leq i \leq n} (E_Q[m(X_i, 0, R_i(0), \theta_0) | X_i] - E_Q[m(X, 0, R(0), \theta_0)]) \bigg ) + o_P(1) \\
Further, we have that
\begin{equation} \label{eq:normal_convergence}
\sqrt n(\hat \theta_n -  \theta_0) \stackrel{d}{\to} N(0, \mathbb V_\ast)~,
\end{equation}
where
\begin{equation} \label{eq:Vstar}
    \mathbb V_\ast = \var[\psi^\ast(X_i, A_i, R_i, \theta_0)]~.
\end{equation}
\end{theorem}

\begin{remark} \label{rem:V_compare}
Note that by comparing the variance expression in \eqref{eq:naive_variance} to the variance expression for $\mathbb{V}_*$, we obtain
\begin{multline*}
    \mathbb{V} - \mathbb{V}_* = \eta (1 - \eta) M^{-1} E[(E[m(X_i, 1, R_i(1) | X_i] - E[m(X_i, 0, R_i(0) | X_i]) \\
    \times (E[m(X_i, 1, R_i(1) | X_i] - E[m(X_i, 0, R_i(0) | X_i])'](M^{-1})^{\prime}~,
\end{multline*}
which is positive semidefinite. From this we conclude that the asymptotic variance of the ``naive" method of moments estimator $\hat{\theta}_n$ is lower in a finely stratified design compared to i.i.d.\ assignment. In Section \ref{sec:semi}, we will further show that $\mathbb V_\ast$ is the lowest possible asymptotic variance among ``regular'' estimators for $\theta_0$ in a large class of treatment assignment schemes, including both i.i.d.\ assignment and finely stratified designs.
%On the other hand, Theorems \ref{thm:normal} and \ref{thm:efficiencybound} establish that the ``na\"ive'' method of moments estimator attains the efficiency bound $\mathbb{V}_*$ when the treatment assignment mechanism is finely stratified in the sense of satisfying Assumptions \ref{ass:a}--\ref{ass:pair}. 
\end{remark}

\begin{remark}\label{rem:psi_examples_ate}
Note it follows from \eqref{eq:moments} that
\[ \eta E_Q[m(X_i, 1, R_i(1), \theta_0)] + (1 - \eta) E_Q[m(X_i, 0, R_i(0), \theta_0)] = E_P[m(X_i, A_i, R_i, \theta_0)] = 0~, \]
so that $E[\psi^\ast(X_i, A_i, R_i, \theta_0)] = 0$. It is further straightforward to show using Assumption \ref{ass:unconfounded} that
\begin{align}
    \label{eq:var} \mathbb V_\ast & = \var[\psi^\ast(X_i, A_i,R_i, \theta_0)] \\
    \nonumber & = M^{-1} \big ( E \big [ \eta \var[m(X_i, 1, R_i(1), \theta_0) | X_i] + (1 - \eta) \var[m(X_i, 0, R_i(0), \theta_0) | X_i] \big ] \\
    \nonumber & \hspace{5em} + \var \big [ \eta E[m(X_i, 1, R_i(1), \theta_0) | X_i] + (1 - \eta) E[m(X_i, 0, R_i(0), \theta_0) | X_i] \big ] \big ) (M^{-1})'
\end{align}
For instance, in the special case of the ATE (Example \ref{ex:ATE}) we obtain that 
\begin{equation} \label{eq:var-ate}
    \var[\psi^\ast(X_i, A_i,R_i, \theta_0)] = E\left[\frac{\var[Y_i(1)|X_i]}{\eta} + \frac{\var[Y_i(0)|X_i]}{1 - \eta} + \left(E[Y_i(1)- Y_i(0)|X_i] - E[Y_i(1) - Y_i(0)]\right)^2\right]~,
\end{equation}
which matches the asymptotic variance derived in \cite{bai2022inference} for matched pairs. Theorem \ref{thm:normal} however accommodates a much larger class of parameters including those introduced in Examples \ref{ex:QTE}--\ref{ex:log_odds}.
\end{remark}

%    Consider in particular Example \ref{ex:ATE}, then the estimator defined by \eqref{eq:est} and \eqref{eq:moments-ate-generaleta} then becomes 
%    \begin{align*}
%        \hat \theta_n & = \frac{1}{n}\sum_{1 \le s \le S}\sum_{1 \le i \le n}\left(\frac{Y_iA_i}{\eta_s} - \frac{Y_i(1 - A_i)}{1 - \eta_s}\right)I\{\eta(X_i) = \eta_s\} \\
%        & = \frac{1}{S} \sum_{1 \leq s \leq S} \frac{n_s}{n} (\hat \mu_{n, s}(1) - \hat \mu_{n, s}(0))~,
%    \end{align*}
%    where
%    \begin{align*}
%        n_s & = |\{i: \eta(X_i) = \eta_S\}| \\
%        n_s(a) & = |\{i: \eta(X_i) = \eta_S, A_i = a\}| \\
%        \hat \mu_{n, s}(a) & = \frac{1}{n_s(a)} \sum_{1 \leq i \leq n: \eta(X_i) = \eta_s, A_i = a} Y_i~.
%    \end{align*}
%    Note that this estimator is simply a weighted average of the within-strata difference in means estimators. Arguing as in Theorem C.3 in \cite{bai2022optimality}, it can be shown that such a treatment assignment mechanism combined with the above estimator attains the efficiency bound in \eqref{eq:var-generaleta}.

\begin{remark} \label{rem:generaleta-attained}
    Although Theorem \ref{thm:normal} is focused on the case where $\eta(X_i) = \eta$ is a constant, straightforward modifications of the treatment assignment mechanism described in Assumptions \ref{ass:a}--\ref{ass:pair} can be applied in more general settings. For instance, suppose $\eta(X_i)$ takes on a fixed discrete set of values $\{\eta_1, \dots, \eta_S\}$, we could then simply implement a finely stratified experiment over each set $\{i: \eta(X_i) = \eta_s\}$ for $1 \leq s \leq S$. In other words, separately within each stratum defined by the units for which $\eta(X_i) = \eta_s$, employ the assignment mechanism described in Assumptions \ref{ass:a}--\ref{ass:pair} with $\ell/k = \eta_s$. For more general functions $\eta(X_i)$, we conjecture that we could employ the local randomization procedure proposed in \cite{cytrynbaum2023designing}.
\end{remark}

\begin{remark}\label{rem:var_est}
    It is possible to construct a consistent estimator for $\mathbb{V}_*$ based on the variance formula in \eqref{eq:var}. Here, we briefly describe a construction when $\theta_0$ is a scalar and refer interested readers to \cite{bai2022inference}, \cite{bai2022optimality}, and \cite{bai2023inference} for formal arguments which could be used to prove its validity. First note that in certain examples (including Examples \ref{ex:ATE} and \ref{ex:LATE}--\ref{ex:log_odds}), the analog principle suggests that a natural estimator for $M$ is given by
    \[ \widehat M_n = \frac{1}{n} \sum_{1 \leq i \leq n} \frac{\partial}{\partial \theta} m(X_i, A_i, R_i, \theta)\bigg|_{\theta = \hat \theta_n}~. \]
    Under suitable conditions, it follows directly from following arguments in each of the papers mentioned above that $\widehat M_n \stackrel{P}{\to} M$.\footnote{In examples including Example \ref{ex:QTE} where $m$ is nonsmooth in $\theta$, $M$ may consist of components which require nonparametric estimators, and in such cases bootstrap procedures may be preferable. See, for instance, \cite{jiang2021bootstrap}.} Therefore, it suffices to construct a consistent estimator for the ``meat" in \eqref{eq:var}. By the law of total variance, this middle component equals $\Sigma_1 + \Sigma_2$, where
    \begin{align*}
       \Sigma_1 & = \eta \var[m(X_i, 1, R_i(1), \theta_0)] + (1 - \eta) \var[m(X_i, 0, R_i(0), \theta_0)] \\
       \Sigma_2 & = - \eta (1 - \eta) E \big [ \big ( E[m(X_i, 1, R_i(1), \theta_0) | X_i] - E[m(X_i, 1, R_i(1), \theta_0)] \\
       & \hspace{5em} - (E[m(X_i, 0, R_i(0), \theta_0) | X_i] - E[m(X_i, 0, R_i(0), \theta_0)]) \big )^2 \big ] \\
       & = - \eta (1 - \eta) \Big (E[E[m(X_i, 1, R_i(1), \theta_0) | X_i]^2] + E[E[m(X_i, 0, R_i(0), \theta_0) | X_i]^2] \\
       & \hspace{7em} - 2E[E[m(X_i, 1, R_i(1), \theta_0) | X_i] E[m(X_i, 0, R_i(0), \theta_0) | X_i]] \\
       & \hspace{7em} - (E[m(X_i, 1, R_i(1), \theta_0)] - E[m(X_i, 0, R_i(0), \theta_0)])^2  \Big )
    \end{align*}
    For $a \in \{0, 1\}$, define
    \[ \hat \mu_n(a) = \frac{1}{\eta_a n} \sum_{1 \leq i \leq n} I \{A_i = a\} m(X_i, A_i, R_i, \hat \theta_n)~, \]
    where $\eta_1 = \eta$ and $\eta_0 = 1 - \eta$.
    The analog principle again suggests that a natural estimator for $\Sigma_1$ is
    \begin{align*}
        \hat \Sigma_{1, n} = \frac{1}{n} \sum_{1 \leq i \leq n} I \{A_i = 1\} (m(X_i, A_i, R_i, \hat \theta_n) - \hat \mu_n(1))^2 + \frac{1}{n} \sum_{1 \leq i \leq n} I \{A_i = 0\} (m(X_i, A_i, R_i, \hat \theta_n) - \hat \mu_n(0))^2~.
    \end{align*}
    To estimate $\Sigma_2$, we first define
    \begin{align*}
        \hat \varsigma_n(0, 1) & = \frac{k}{n} \sum_{1 \leq j \leq n / k} \frac{1}{\ell(k - \ell)} \sum_{i, i' \in \lambda_j: A_i = 1, A_{i'} = 0} m(X_i, A_i, R_i, \hat \theta_n) m(X_{i'}, A_{i'}, R_{i'}, \hat \theta_n)~.
    \end{align*}
    Next, define
    \[ \hat \varsigma_n(1, 1) = \begin{cases}
        \frac{k}{n} \sum\limits_{1 \leq j \leq n / k} \frac{1}{\binom{\ell}{2}} \sum\limits_{i < i' \in \lambda_j: A_i = A_{i'} = 1} m(X_i, A_i, R_i, \hat \theta_n) m(X_{i'}, A_{i'}, R_{i'}, \hat \theta_n) & \text{ if } \ell > 1 \\
        \frac{2k}{n} \sum\limits_{1 \leq j \leq \frac{n}{2k}} {\sum\limits_{i \in \lambda_{2j}, i' \in \lambda_{2j - 1}: A_i = A_{i'} = 1}} m(X_i, A_i, R_i, \hat \theta_n) m(X_{i'}, A_{i'}, R_{i'}, \hat \theta_n) & \text{ if } \ell = 1~.
    \end{cases} \]
    Similarly, define
    \[ \hat \varsigma_n(0, 0) = \begin{cases}
        \frac{k}{n} \sum\limits_{1 \leq j \leq n / k} \frac{1}{\binom{k - \ell}{2}} \sum\limits_{i < i' \in \lambda_j: A_i = A_{i'} = 0} m(X_i, A_i, R_i, \hat \theta_n) m(X_{i'}, A_{i'}, R_{i'}, \hat \theta_n) & \text{ if } k - \ell > 1 \\
        \frac{2k}{n} \sum\limits_{1 \leq j \leq \frac{n}{2k}} {\sum\limits_{i \in \lambda_{2j}, i' \in \lambda_{2j - 1}: A_i = A_{i'} = 0}} m(X_i, A_i, R_i, \hat \theta_n) m(X_{i'}, A_{i'}, R_{i'}, \hat \theta_n) & \text{ if } k - \ell = 1~.
    \end{cases} \]
    Finally, define
    \[ \hat \Sigma_{2, n} = - \eta(1 - \eta) (\hat \varsigma_n(1, 1) + \hat \varsigma_n(0, 0) - 2 \hat \varsigma_n(0, 1) - (\hat \mu_n(1) - \hat \mu_n(0))^2)~. \]
    The estimator $\hat \varsigma_n(1, 1)$ is constructed in one of two ways depending on the number of treated units in each block. If more than one unit in each block is treated, then we take the averages of all pairwise products of the treated units in each block, and average them across all blocks. We call this a ``within block" estimator. If instead only one unit in each block is treated, then we take the product of two treated units in \emph{adjacent} blocks. We call this a ``between block" estimator, and note that similar constructions have been used previously in \cite{abadie2008estimation}, \cite{bai2022inference}, \cite{bai2023inference-1}, and \cite{cytrynbaum2023designing}. The estimator $\hat \varsigma_n(0, 0)$ is constructed similarly. \cite{bai2023inference} compare the finite-sample properties of the ``within block" and ``between block" variance estimators via simulation. Their findings are that experimental designs which allow for a ``within block" variance estimator have better small sample inferential performance, at the cost of slightly increasing the mean-squared error of the estimator $\hat{\theta}_n$, relative to experimental designs which require the use of the ``between block" variance estimator. Under suitable assumptions, it follows from similar arguments to those in \cite{bai2022optimality} and \cite{bai2023inference} that $\hat \Sigma_{1, n} \stackrel{P}{\to} \Sigma_1$ and $\hat \Sigma_{2, n} \stackrel{P}{\to} \Sigma_2$.  A natural estimator for $\mathbb{V}_\ast$ is therefore given by
    \[\hat{\mathbb{V}}_n = \widehat{M}_n^{-2}\left(\hat{\Sigma}_{1,n} + \hat{\Sigma}_{2,n}\right)~. \]
    Thus, provided $M$ is invertible, we have that $\hat{\mathbb{V}}_n \stackrel{P}{\to} \mathbb V_\ast$.
\end{remark}

\section{Efficiency Bound}\label{sec:semi}
An inspection of the asymptotic variance in \eqref{eq:Vstar} reveals that $\mathbb V_\ast$ in fact coincides with the classical efficiency bound for estimating $\theta_0$ with i.i.d.\ assignment. For example, the variance of the difference-in-means estimator in \eqref{eq:var-ate} equals the efficiency bound derived in \cite{hahn1998role} for estimating the ATE with a known marginal treatment probability $\eta$. Therefore, another way to interpret our result in Theorem \ref{thm:normal} is that the standard i.i.d.\ efficiency bound can be attained by a na\"ive method of moments estimator under a finely stratified design. On the other hand, because treatment status is not independent in a finely stratified design, a natural follow-up question is whether or not the efficiency bound for estimating $\theta_0$ changes relative to what can be obtained under i.i.d.\ assignment once we allow for more general assignment mechanisms. In this section, we show that $\mathbb{V}_*$ continues to be the efficiency bound for the class of parameters introduced in Section \ref{sec:setup} under a more general class of treatment assignment mechanisms. We impose the following high-level assumption on the assignment mechanism:

%\begin{assumption} \label{ass:id}
%For $i \neq i'$, $(W_i, A_i, R_i)$ and $(W_{i'}, A_{i'}, R_{i'})$ are identically distributed.
%\end{assumption}


\begin{assumption} \label{ass:info}
The treatment assignment mechanism is such that for any integrable Lipschitz functions $\gamma_0, \gamma_1: \mathbf{R}^{d_x} \to \mathbf{R}$,
\[ \frac{1}{n} \sum_{1 \leq i \leq n} \sum_{a \in \{0, 1\}} I \{A_i = a\} \gamma_a(X_i) \stackrel{P}{\to} \eta E[\gamma_1(X_i)] + (1 - \eta)E[\gamma_0(X_i)]~. \]
\end{assumption}


In other words, Assumption \ref{ass:info} requires that the assignment mechanism admits a law of large numbers for ``well-behaved" functions of the covariate values. Examples \ref{ex:CAR}--\ref{ex:MP} illustrate that the assumption holds for a large class of treatment assignment mechanisms used in practice. 


\begin{example}[Covariate-adaptive randomization]\label{ex:CAR}
Let $S: \mathbf R^{d_x} \to \mathcal S = \{1, \ldots, |\mathcal S|\}$ be a function that maps the covariates into a set of discrete ``strata.'' Assume that treatment status is assigned so that $(R^{(n)}(1), R^{(n)}(0), X^{(n)}) \indep A^{(n)} \big | S^{(n)}$, and that for $s \in \mathcal S$,
\[ \frac{\sum_{1 \leq i \leq n} I \{S_i = s, A_i = 1\}}{\sum_{1 \leq i \leq n} I \{S_i = s\}} \stackrel{P}{\to} \eta~. \]
This high-level assumption accommodates stratified assignment mechanisms commonly used in empirical practice \citep[see, for instance,][]{duflo2015education, dizon2019parents}. It follows from Lemma C.4 in \cite{bugni2019inference} that for any integrable functions $\gamma_0, \gamma_1$,
\[ \frac{1}{n} \sum_{1 \leq i \leq n} \sum_{a \in \{0, 1\}} I \{A_i = a\} \gamma_a(X_i) \stackrel{P}{\to} 
\sum_{s \in \mathcal S} P \{S_i = s\} (\eta E[\gamma_1(X_i)| S_i = s] + (1 - \eta)E[\gamma_0(X_i) | S_i = s])~. \]
Therefore, Assumption \ref{ass:info} is satisfied.
\end{example}


\begin{example}[Matched pairs] \label{ex:MP}
Suppose $n$ is even and we assign treatment using a finely stratified design with $(\ell, k) = (1,2)$. As discussed at the beginning of Section \ref{sec:variance}, such a design is also known as a matched pairs design. Assume that the pairing algorithm $\pi_n(X^{(n)})$ results in pairs that are ``close'' in the sense of Assumption \ref{ass:pair}. It then follows from the proof of Lemma S.1.5 of of \cite{bai2022inference} that
\[ \frac{1}{n} \sum_{1 \leq i \leq n} \sum_{a \in \{0, 1\}} I \{A_i = a\} \gamma_a(X_i) \stackrel{P}{\to} \frac{1}{2} E[\gamma_0(X_i)] + \frac{1}{2} E[\gamma_1(X_i)]~. \]
Therefore, Assumption \ref{ass:info} is satisfied. 
%We note that by following an approximation argument in \cite{cytrynbaum2023designing}, we expect that a similar result holds when $\gamma_0$, $\gamma_1$ are simply assumed to be integrable functions.
\end{example}

Next, we impose the following high-level assumption on the distributions $Q$ and $P$:

\begin{assumption}\label{ass:var_lipschitz} The distributions $Q$ and $P$ are such that 
\begin{enumerate}[\rm (a)]
\item $\var[m(X_i, a, R_i(a), \theta_0)|X_i=x]$ is a Lipschitz function.
\item $\theta_0$ is uniquely determined by \eqref{eq:moments} and $M = \frac{\partial}{\partial \theta} E[m(X_i, A_i, R_i, \theta)] \Big |_{\theta = \theta_0}$ is invertible.
\end{enumerate}
\end{assumption}

Assumption \ref{ass:var_lipschitz}(a) is a smoothness condition that is required in settings where $X_i$ is continuous to ensure that the function $\psi^*(\cdot)$ we derive in Theorem \ref{thm:efficiencybound} below is in fact the efficient influence function. Note that, if we strengthen Assumption \ref{ass:info} to hold for all integrable functions $\gamma_0$, $\gamma_1$ then Assumption \ref{ass:var_lipschitz}(a) could be dropped, with no change to the resulting bound derived below. However, our argument for attaining the bound using finely stratified designs in Section \ref{sec:variance} (see in particular Remark \ref{rem:assumption_strength}) requires similar smoothness conditions, and so we maintain it here as well. Assumption \ref{ass:var_lipschitz}(b) is a standard assumption used when deriving the properties of $Z$-estimators and repeats Assumption \ref{ass:normal}(a).

We now present an efficiency bound for the parameter $\theta_0$ introduced in Section \ref{sec:setup}. Formally, we characterize the bound via a convolution theorem which applies to all ``regular" estimators of the parameter $\theta_0$, where ``regular" here should be understood in the standard sense necessary to rule out, for instance, super-efficient estimators \citep[see, for instance, Example 8.1 in][]{van_der_vaart1998asymptotic}. In stating our theorem we leave the precise definition of ``regular" and related assumptions to Appendix \ref{sec:SPEB}. In the paragraph following the statement of the theorem we provide some more details on the nature of our result.

\begin{theorem} \label{thm:efficiencybound}
Suppose Assumptions \ref{ass:unconfounded} and \ref{ass:info}--\ref{ass:var_lipschitz} hold, and maintain the additional regularity conditions \eqref{eq:qmd-1}, \eqref{eq:qmd-2} and Assumption \ref{ass:bounded_path} described in Appendix \ref{sec:SPEB}. Let $\tilde{\theta}_n$ be any ``regular" estimator of the parameter $\theta_0$ in the sense of \eqref{eq:regular} in Appendix \ref{sec:SPEB}. Then,  
\[\sqrt{n}(\tilde{\theta}_n - \theta_0) \xrightarrow{d} L~,\]
where
\[L = N(0, \mathbb V_\ast) \ast B~, \]
for $\mathbb V_\ast$ in \eqref{eq:Vstar} and some fixed probability measure $B$ which is specific to the estimator $\tilde{\theta}_n$.
\end{theorem}

Given Theorem \ref{thm:efficiencybound} we call $\mathbb V_\ast = \var[\psi^\ast(X_i, A_i,R_i, \theta_0)]$ the efficiency bound for $\theta_0$, since our result shows that this is the lowest asymptotic variance attainable by any regular estimator under our assumptions. We note that our assumptions on the assignment mechanism preclude us from applying results based on ``standard" arguments \citep[see, for instance,][]{van_der_vaart1998asymptotic}. Specifically, if we define a tangent set as the collection of score functions of ``smooth" one-dimensional parametric sub-models in an appropriate sense, then we are not able to guarantee that the resulting tangent set is linear (or even a convex cone) while \emph{simultaneously} verifying that the likelihood ratio process is locally asymptotically normal for arbitrary assignment mechanisms which satisfy Assumption \ref{ass:info}. Instead, we proceed by justifying an application of Corollary 3.1 in \cite{armstrong2022asymptotic} combined with the convolution Theorem 3.11.2 in \cite{van_der_vaart1996weak} to each $d_\theta$-dimensional parametric submodel separately, and then arguing that the supremum over all such submodels is attained by $\var[\psi^{\ast}]$ under Assumption \ref{ass:var_lipschitz}. 

\begin{remark}\label{rem:psi_examples}
Following same arguments as those in Remark \ref{rem:psi_examples_ate}, we can deduce that our efficiency bound recovers well-known bounds for common parameters (like those presented in Examples \ref{ex:ATE}--\ref{ex:LATE}) in the setting of i.i.d.\ assignment. We have noted in the case of the ATE (Example \ref{ex:ATE}) that \eqref{eq:var-ate} matches the efficiency bound under i.i.d.\ assignment derived in \cite{hahn1998role}. See \cite{armstrong2022asymptotic} and \cite{rafi2023efficient} for related results in the context of stratified and adaptive experiments.  Straightforward calculation also implies for the quantile treatment effect (Example \ref{ex:QTE}) that
\begin{multline*}
    \var[\psi^\ast(X_i, A_i, R_i, \theta_0)] = E \bigg [ \frac{1}{\eta} \frac{F_1 \big (\theta_0(1) | X \big ) \big (1 - F_1 \big (\theta_0(1) | X \big ) \big )}{f_1 \big (\theta_0(1) \big )^2} + \frac{1}{1- \eta} \frac{F_0 \big (\theta_0(0) | X \big ) \big (1 - F_0 \big (\theta_0(0) | X \big ) \big )}{f_0 \big (\theta_0(0) \big )^2} \\
    + \bigg ( \frac{F_1 \big (\theta_0(1) | X \big ) - \tau}{f_1 \big (\theta_0(1) \big )} - \frac{F_0 \big (\theta_0(0) | X \big ) - \tau}{f_0 \big (\theta_0(0) \big )} \bigg)^2 \bigg ]~,
\end{multline*}
which matches the efficiency bound under i.i.d.\ assignment derived in \cite{firpo2007efficient} when the propensity score is set to $\eta$. 
\end{remark}

\begin{remark} \label{rem:generaleta}
Although we focus on the case where $\eta_i(X_i) = P\{A_i = 1|X_i\} = \eta$ is a constant, the proof of Theorem \ref{thm:efficiencybound} holds when $\eta_i(x) = \eta(x)$ for $1 \leq i \leq n$, where $\eta(x)$ is an arbitrary known and fixed function. In these settings, Lemma \ref{lem:differentiability} shows that the efficiency bound equals
\begin{align}
    \label{eq:var-generaleta} \mathbb V_\ast & = \var[\psi^\ast(X_i, A_i,R_i, \theta_0)] \\
    \nonumber & = M^{-1} \big ( E \big [ \eta(X_i) \var[m(X_i, 1, R_i(1), \theta_0) | X_i] + (1 - \eta(X_i)) \var[m(X_i, 0, R_i(0), \theta_0) | X_i] \big ] \\
    \nonumber & \hspace{5em} + \var \big [ \eta(X_i) E[m(X_i, 1, R_i(1), \theta_0) | X_i] + (1 - \eta(X_i)) E[m(X_i, 0, R_i(0), \theta_0) | X_i] \big ] \big ) (M^{-1})'~,
\end{align}
so that the only difference from \eqref{eq:var} is that $\eta$ is replaced by $\eta(X_i)$.  Consider Example \ref{ex:ATE} and note the moment condition for the ATE is now given by
\begin{equation} \label{eq:moments-ate-generaleta}
   m(X_i, A_i, R_i, \theta) = \frac{Y_i A_i}{\eta(X_i)} - \frac{Y_i(1 - A_i)}{1 - \eta(X_i)} - \theta~. 
\end{equation}
Straightforward calculation implies that in this example, the efficiency bound in \eqref{eq:var-generaleta} becomes
\begin{equation} \label{eq:ATE-bound-generaleta}
   E\left[\frac{\var[Y_i(1)|X_i]}{\eta(X_i)} + \frac{\var[Y_i(0)|X_i]}{1 - \eta(X_i)} + \left(E[Y_i(1)- Y_i(0)|X_i] - E[Y_i(1) - Y_i(0)]\right)^2\right]~,
\end{equation}
which again matches the efficiency bound under i.i.d.\ assignment in \cite{hahn1998role}. If we additionally impose that $\eta(X_i) = \eta(S(X_i))$ for $S$ taking on finitely many values as in Example \ref{ex:CAR}, then the bound could be achieved by employing the modified design described in Remark \ref{rem:generaleta-attained}.
%I then this expression matches the bound derived in Theorem 3.1 in \cite{rafi2023efficient}.
\end{remark}

\begin{remark}\label{rem:other_SPEB}
Here, we comment on how Theorem \ref{thm:efficiencybound} relates to prior efficiency bounds in experiments with general assignment mechanisms. For the case of estimating the ATE, \cite{armstrong2022asymptotic} derives an efficiency bound over a very large class of assignment mechanisms, which includes for instance response-adaptive designs. However, his bound is only valid when the assignment proportions are allowed to be arbitrarily specified. As a consequence, his bound is necessarily loose whenever the assignment proportions are exogenously constrained away from the Neyman allocation, in which units are assigned to treatment (control) with conditional probability proportional to the conditional variance of the potential outcome under treatment (control). For example, his bound does not apply if the assignment proportions were set to one half regardless of whether or not the conditional outcome variances across treatment and control are equal; such settings frequently arise in practice due to logistical constraints, or as argued in \cite{cai2022performance}, because the conditional variances of potential outcomes under treatment and control cannot be precisely estimated prior to the experiment. Motivated by such concerns, \cite{rafi2023efficient} derives an efficiency bound for the ATE over the class of ``coarsely-stratified" assignment mechanisms as studied in \cite{bugni2019inference}, where the stratum-level assignment proportions are restricted {\it a priori} by the experimenter. However, this rules out finely stratified designs. Finally, we once again emphasize that our analysis applies to a general class of treatment effect parameters, including the ATE as a special case.
\end{remark}

\section{Simulations}\label{sec:sims}
In this section, we illustrate the results in Sections \ref{sec:variance} and \ref{sec:semi} with a simulation study. Specifically, we set $\eta = 1/2$, and compare the mean-squared errors (MSE) obtained from the ``naive" estimator $\hat{\theta}_n$ and various adjusted estimators, for i.i.d.\ treatment assignment versus matched pairs assignment (see Remark \ref{rem:matched_pairs} and  Example \ref{ex:MP}).  In Section \ref{sec:sims_ATE}, we present the model specifications and estimators for estimating the ATE as in Example \ref{ex:ATE}. In Section \ref{sec:sims_LATE}, we present the model specifications and estimators for estimating the LATE as in Example \ref{ex:LATE}. Section \ref{sec:sims_results} reports the simulation results.
 
%$n=1000$ and 2000 Monte Carlo Trials. We compute the true LATE effect using Monte Carlo simulations and the number of Monte Carlo simulations being 1,000,000. Monte Carlo simulations were used to compute every expectations in the following equation:
%\begin{align*}
%    \theta &= \frac{E[Y_i\mid A_i=1] - E[Y_i\mid A_i=0]}{E[D_i\mid A_i=1] - E[D_i\mid A_i=0]}\\
%    &= \frac{ E[Y_i(1)D_i(1)] - E[Y_i(0)(1-D_i(1))] - (E[Y_i(1)D_i(0)] - E[Y_i(0)(1-D_i(0))]) }{E[D_i(1)] - E[D_i(0)]}
%\end{align*}
\subsection{Average Treatment Effect}\label{sec:sims_ATE}
In this section, we present model specifications and estimators for estimating the ATE as in Example \ref{ex:ATE}. Recall that in this case the moment function we consider is given by  
\begin{equation*}
    m(X_i, R_i, A_i, \theta) = \frac{Y_i A_i}{\eta} - \frac{Y_i (1 - A_i)}{1 - \eta} - \theta~,
\end{equation*}
with $R_i = Y_i$. For $a \in \{0, 1\}$ and $1\leq i \leq n$, the potential outcomes are generated according to the equation:
\begin{equation*}
Y_i(a) = \mu_a(X_i) + \sigma_a(X_i) \epsilon_{i}~.
\end{equation*}
In each of the specifications, $((X_i, \epsilon_{i}): 1\leq i \leq n)$ are i.i.d; for $1 \leq i \leq n$, $X_i$ and $\epsilon_{i}$ are independent.
\begin{enumerate}[{\bf Model} 1:]
	\item $\mu_{0}(X_i) = X_i + (X_i^2 - 1)/3$, $\mu_{1}(X_i) = 0.2 + \mu_0(X_i)$, $\epsilon_{i} \sim N(0, 1)$, $X_i \sim N(0, 1)$ and $\sigma_a(X_i) = 2$.
	\item As in Model 1, but $\mu_a(X_i) = 0.2I\{a = 1\} + \gamma_a(\sin(X_i) +  X_i) + (X_i^2 - 1)/3$ where $\gamma_1 = 1$ and $\gamma_0 = -1$, and $\sigma_a(X_i) = (1 + a)X_i^2$.
    \item As in Model 2, but $\mu_1(X_i) = 0.2 + 3(X_i^2-1)$ and $\mu_0(X_i) = 0$.
\end{enumerate}

We consider the following three estimators for the ATE:

\begin{enumerate}
    \item[] \textbf{Unadjusted Estimator}:
    \begin{equation*}
        \hat\theta_n^{\rm unadj} = \frac{1}{n / 2} \sum_{1\leq i \leq n} ( Y_i A_i - Y_i(1-A_i) )~.
    \end{equation*}
    \item[] \textbf{Adjusted Estimator 1}:
    \begin{equation*}
        \hat\theta_n^{\rm adj, 1} = \frac{1}{n} \sum_{1 \leq i \leq n} \big ( 2 A_i(Y_i - \hat\mu_1^Y(X_i)) - 2(1- A_i)(Y_i - \hat\mu_0^Y(X_i)) + \hat\mu_1^Y(X_i) - \hat\mu_0^Y(X_i) \big )~,
    \end{equation*}
    where $\hat\mu_a^Y(X_i)$ is constructed by running a least squares regression of $Y_i$ on $(1, X_i, X_i^2)$ using the sample from $A_i=a$.
    \item[] \textbf{Adjusted Estimator 2}: 
     \begin{equation*}
        \hat\theta_n^{\rm adj, 2} = \frac{1}{n} \sum_{1 \leq i \leq n} \big ( 2 A_i(Y_i - \hat\mu_1^Y(X_i)) - 2(1- A_i)(Y_i - \hat\mu_0^Y(X_i)) + \hat\mu_1^Y(X_i) - \hat\mu_0^Y(X_i) \big )~,
    \end{equation*}
    where $\hat\mu_a^Y(X_i)$ is constructed by running a least squares regression of $Y_i$ on $(1, X_i, X_i^2, X_i 1\{X_i > t\})$ where $t$ is the sample median using the sample from $A_i = a$.
\end{enumerate}

The first estimator $\hat\theta_n^{\rm unadj}$ is the method of moments estimator given by the solution to \eqref{eq:est}. The second and third estimators $\hat\theta_n^{\rm adj,1}$ and $\hat\theta_n^{\rm adj,2}$  are covariate-adjusted estimators which can be obtained as two-step method of moments estimators from solving the ``augmented" moment equation \eqref{eq:augment_m} described in the discussion at the end of Section \ref{sec:setup}. $\hat\theta_n^{\rm adj,1}$ and $\hat\theta_n^{\rm adj,2}$ differ in the choice of basis functions used in the construction of the estimators $\hat{\mu}_a(x)$. Note that by the double-robustness property of the augmented estimating equation \eqref{eq:augment_m}, it can be shown that the adjusted estimators $\hat{\theta}_n^{\rm adj,1}$, $\hat{\theta}_n^{\rm adj,2}$ are consistent and asymptotically normal regardless of the choice of estimators $\hat{\mu}_a(x)$, but consistency of $\hat{\mu}_a(x)$ to $\mu_a(x)$ would ensure that $\hat{\theta}_n^{\rm adj,1}$, $\hat{\theta}_n^{\rm adj,2}$ are efficient under i.i.d. assignment \citep[][]{robins1995analysis, tsiatis2008covariate, chernozhukov2017doubledebiasedneyman}.

\subsection{Local Average Treatment Effect}\label{sec:sims_LATE}
In this section, we present the model specifications and estimators for estimating the LATE as in Example \ref{ex:LATE}. Recall that in this case the moment condition we consider is given by 
\begin{equation*}
    m(X_i, A_i, R_i, \theta) = \frac{Y_i A_i}{\eta} - \frac{Y_i (1 - A_i)}{1 - \eta} - \theta \left ( \frac{D_i A_i}{\eta} - \frac{D_i (1 - A_i)}{1 - \eta} \right )~,
\end{equation*}
with $R_i = (Y_i, D_i)$. The outcome is determined by the relationship $Y_i = D_i Y_i(1) + (1-D_i)Y_i(0)$, where $Y_i(d) = \mu_d(X_i) + \sigma_a(X_i) \epsilon_{i}$ follows the same outcome model as in the ATE setup of Section \ref{sec:sims_ATE}. In addition, we have $D_i = A_i D_i(1) + (1-A_i)D_i(0)$, where
\begin{align*}
    D_i(0)&=I\left\{\alpha_0+\alpha\left(X_i\right)> \varepsilon_{1, i}\right\}~, \\
    D_i(1)&=\begin{cases}
I\left\{\alpha_1+\alpha\left(X_i\right)> \varepsilon_{2, i}\right\} & \text { if } D_i(0)=0 \\
1 & \text { otherwise }
\end{cases}~.
\end{align*}
For each outcome model, we set $\alpha_0 = 0.5$, $\alpha_1 = 1$, $\alpha(X_i) = X_i + (X_i^2 - 1)/3$ and $\varepsilon_{1, i}, \varepsilon_{2, i} \sim N(0, 4)$.

We consider the following three estimators for the LATE:
\begin{enumerate}
    \item[] \textbf{Unadjusted Estimator}:
    \begin{equation*}
        \hat\theta_n^{\rm unadj} = \frac{\sum_{1\leq i \leq n} \left ( Y_i A_i - Y_i (1 - A_i) \right )}{\sum_{1\leq i \leq n} \left ( D_i A_i - D_i (1 - A_i) \right )}~.
    \end{equation*}
    \item[] \textbf{Adjusted Estimator 1}:
    \begin{equation*}
        \hat\theta_n^{\rm adj,1} = \frac{\sum_{1 \leq i \leq n} \left ( 2 A_i(Y_i - \hat\mu_1^Y(X_i)) - 2(1- A_i)(Y_i - \hat\mu_0^Y(X_i)) + \hat\mu_1^Y(X_i) - \hat\mu_0^Y(X_i) \right )}{\sum_{1 \leq i \leq n} \left ( 2 A_i(D_i - \hat\mu_1^Y(X_i)) - 2(1- A_i)(D_i - \hat\mu_0^Y(X_i)) + \hat\mu_1^D(X_i) - \hat\mu_0^D(X_i) \right )}~,
    \end{equation*}
    where $\hat\mu_a^Y(X_i)$ is estimated by running a least squares regression of $Y_i$ on $(1, X_i, X_i^2)$ using the sample from $A_i=a$, and $\hat\mu_a^D(X_i)$ is estimated using logistic regressions using the same set of regressors using the sample from $A_i = a$.
    \item[] \textbf{Adjusted Estimator 2}: 
    As in Adjusted Estimator 1, but $\hat\mu_a^Y(X_i)$ and $\hat\mu_a^D(X_i)$ are estimated respectively by running a least squares and logistic regressions of $Y_i$ on $(1, X_i, X_i^2, X_i 1\{X_i > t\})$ where $t$ is the sample median. 
\end{enumerate}
Similarly to Section \ref{sec:sims_ATE}, $\hat{\theta}_n^{\rm unadj}$ solves \eqref{eq:est} for the moment condition given in \eqref{eq:moments-late}. The second and third estimators are covariate adjusted estimators which can be obtained as two-step method of moments estimators from solving an ``augmented" version of the moment condition \eqref{eq:moments-late} \citep[see, for instance,][]{chernozhukov2018doubledebiased, jiang2022improving}.

\subsection{Simulation Results}\label{sec:sims_results}
Table \ref{table:sims} displays the ratio of the MSE for each design/estimator pair relative to the MSE of the unadjusted estimator under i.i.d. assignment, computed across $2000$ Monte Carlo replications. As expected given our theoretical results, we find that the empirical MSEs of the naive unadjusted estimator under a matched pairs design closely match the empirical MSEs of the covariate adjusted estimators under i.i.d. assignment. 

\mycomment{
\begin{table}[ht!]
\centering
\begin{threeparttable}
\caption{MSE ratios relative to unadjusted estimator under i.i.d.\ assignment}
\begin{tabular}{cccccccc}
\toprule
& & & \multicolumn{3}{c}{\textbf{I.I.D.\ assignment}} & & \textbf{Matched pairs} \\ \cmidrule{4-6} \cmidrule{8-8}
& & Model & $\boldsymbol{\text{Unadjusted}}^{}$ & $\text{Adjusted 1}$ & $\text{Adjusted 2}$ & & $\text{Unadjusted}$ \\ \midrule
\multirow{6}{*}{$n=200$} & \multirow{3}{*}{ATE} &  1 & 1.0 & 0.4574 & 0.4629 & & 0.4760 \\
& &  2 & 1.0 & 1.0046 & 0.9688 & & 0.8629 \\
& &  3 & 1.0 & 0.7299 & 0.7217 & & 0.7542 \\ \addlinespace[0.25em]
& \multirow{3}{*}{LATE} &  1 & 1.0 & 0.3866 & 0.3970 & & 0.3753 \\
& &  2 & 1.0 & 0.8065 & 0.8758 & & 0.7767 \\
& &  3 & 1.0 & 0.5175 & 0.5216 & & 0.5212 \\ \addlinespace[1em]
\multirow{6}{*}{$n=400$} & \multirow{3}{*}{ATE} &  1 & 1.0 & 0.4465 & 0.4699 & & 0.4532 \\
& &  2 & 1.0 & 0.9732 & 0.9780 & & 0.9697 \\
& &  3 & 1.0 & 0.7156 & 0.6834 & & 0.6949 \\ \addlinespace[0.25em]
& \multirow{3}{*}{LATE} &  1 & 1.0 & 0.4389 & 0.4402 & & 0.4184 \\
& &  2 & 1.0 & 0.8710 & 0.8683 & & 0.7195 \\
& &  3 & 1.0 & 0.5328 & 0.5173 & & 0.5357 \\ \addlinespace[1em]
\multirow{6}{*}{$n=1000$} & \multirow{3}{*}{ATE} &  1 & 1.0 & 0.4341 & 0.4703 & & 0.4670 \\
& &  2 & 1.0 & 0.9787 & 0.9132 & & 0.8977 \\
& &  3 & 1.0 & 0.7223 & 0.7147 & & 0.7195 \\ \addlinespace[0.25em]
& \multirow{3}{*}{LATE} &  1 & 1.0 & 0.4307 & 0.4459 & & 0.4369 \\
& &  2 & 1.0 & 0.8583 & 0.8536 & & 0.8567 \\
& &  3 & 1.0 & 0.5147 & 0.5093 & & 0.4901 \\ \addlinespace[1em]
\multirow{6}{*}{$n=2000$} & \multirow{3}{*}{ATE} &  1 & 1.0 & 0.4379 & 0.4444 & & 0.4357 \\
& &  2 & 1.0 & 0.9805 & 0.9659 & & 0.9854 \\
& &  3 & 1.0 & 0.6912 & 0.7235 & & 0.6941 \\ \addlinespace[0.25em]
& \multirow{3}{*}{LATE} &  1 & 1.0 & 0.4392 & 0.4378 & & 0.4378 \\
& &  2 & 1.0 & 0.8329 & 0.8499 & & 0.8717 \\
& &  3 & 1.0 & 0.5386 & 0.5913 & & 0.5453 \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\footnotesize \item Note: For each model, the MSE of the unadjusted estimator under i.i.d.\ assignment are normalized to one and the other columns contain the ratios of the MSEs against that of the unadjusted estimator under i.i.d.\ assignment. MSEs are calculated across 2000 replications.
\end{tablenotes}
\end{threeparttable}
\label{table:sims}
\end{table}
}

\begin{table}[ht!]
\centering
\begin{threeparttable}
\caption{MSE ratios relative to unadjusted estimator under i.i.d.\ assignment}
\begin{tabular}{cccccccc}
\toprule
& & & \multicolumn{3}{c}{\textbf{I.I.D.\ assignment}} & & \textbf{Matched pairs} \\ \cmidrule{4-6} \cmidrule{8-8}
& & Model & $\boldsymbol{\text{Unadjusted}}^{}$ & $\text{Adjusted 1}$ & $\text{Adjusted 2}$ & & $\text{Unadjusted}$ \\ \midrule
\multirow{6}{*}{$n=200$} & \multirow{3}{*}{ATE} &  1 & 1.0000 & 0.4580 & 0.4637 & & 0.4530 \\
& &  2 & 1.0000 & 0.9836 & 1.0090 & & 1.0291 \\
& &  3 & 1.0000 & 0.7473 & 0.7615 & & 0.7415 \\ \addlinespace[0.25em]
& \multirow{3}{*}{LATE} &  1 & 1.0000 & 0.4391 & 0.4175 & & 0.4115 \\
& &  2 & 1.0000 & 0.8967 & 0.9724 & & 0.8813 \\
& &  3 & 1.0000 & 0.5191 & 0.5002 & & 0.4936 \\ \addlinespace[1em]
\multirow{6}{*}{$n=400$} & \multirow{3}{*}{ATE} &  1 & 1.0000 & 0.4616 & 0.4640 & & 0.4471 \\
& &  2 & 1.0000 & 0.9778 & 1.0470 & & 1.0042 \\
& &  3 & 1.0000 & 0.7535 & 0.7364 & & 0.7293 \\ \addlinespace[0.25em]
& \multirow{3}{*}{LATE} &  1 & 1.0000 & 0.4267 & 0.4583 & & 0.4327 \\
& &  2 & 1.0000 & 0.8754 & 0.9570 & & 0.9375 \\
& &  3 & 1.0000 & 0.5671 & 0.5349 & & 0.5240 \\ \addlinespace[1em]
\multirow{6}{*}{$n=1000$} & \multirow{3}{*}{ATE} &  1 & 1.0000 & 0.4518 & 0.4568 & & 0.4453 \\
& &  2 & 1.0000 & 0.9874 & 0.9730 & & 0.9186 \\
& &  3 & 1.0000 & 0.7374 & 0.7099 & & 0.7018 \\ \addlinespace[0.25em]
& \multirow{3}{*}{LATE} &  1 & 1.0000 & 0.4437 & 0.4301 & & 0.4259 \\
& &  2 & 1.0000 & 0.8415 & 0.8735 & & 0.8618 \\
& &  3 & 1.0000 & 0.5408 & 0.4879 & & 0.4884 \\ \addlinespace[1em]
\multirow{6}{*}{$n=2000$} & \multirow{3}{*}{ATE} &  1 & 1.0000 & 0.4677 & 0.4835 & & 0.4867 \\
& &  2 & 1.0000 & 0.9780 & 0.9163 & & 0.9105 \\
& &  3 & 1.0000 & 0.7272 & 0.7552 & & 0.7611 \\ \addlinespace[0.25em]
& \multirow{3}{*}{LATE} &  1 & 1.0000 & 0.4702 & 0.4473 & & 0.4871 \\
& &  2 & 1.0000 & 0.8692 & 0.8545 & & 0.8202 \\
& &  3 & 1.0000 & 0.4973 & 0.5044 & & 0.5079 \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\footnotesize \item Note: For each model, the MSE of the unadjusted estimator under i.i.d.\ assignment are normalized to one and the other columns contain the ratios of the MSEs against that of the unadjusted estimator under i.i.d.\ assignment. MSEs are calculated across 2000 replications.
\end{tablenotes}
\end{threeparttable}
\label{table:sims}
\end{table}




\section{Recommendations for Empirical Practice}\label{sec:recs}
We conclude with some recommendations for empirical practice based on our theoretical results. Overall, our findings highlight the general benefit of fine stratification for designing efficient experiments: finely stratified experiments ``automatically" perform fully-efficient regression adjustment for a large class of interesting parameters. This generalizes similar observations made by \cite{bai2022inference}, \cite{bai2022optimality} and \cite{cytrynbaum2023designing} for the special case of estimating the ATE. 

One caveat to this result, however, is that it crucially hinges on the assumption that units within each block are sufficiently ``close" (Assumption \ref{ass:pair}), and such a condition becomes difficult to satisfy as the dimension of $X_i$ increases. For this reason, we recommend that practitioners construct their blocks using a small subset of the baseline covariates which they feel are the most relevant \citep[for instance, the baseline level of the experimental outcome, as suggested by][]{bruhn2009pursuit}. The experimental data can then be analyzed efficiently using an unadjusted method-of-moments estimator. 

If one wishes to perform regression adjustment with additional covariates beyond those used for blocking, then this can be done {\it ex post}, although we caution that care must be taken to ensure that the adjustment is performed in such a way that it guarantees a gain in efficiency: see \cite{bai2023covariate} and \cite{cytrynbaum2023covariate} for related discussions. Recent work has developed such methods of covariate adjustment for specific parameters of interest \citep[see, for instance,][]{bai2023covariate, bai2023inference-1, bai2023late, cytrynbaum2023covariate}, but we leave the development of a method of covariate adjustment which applies at the level of generality considered in this paper to future work. 

\clearpage

\appendix
% \begin{small}

\begin{center} \Large
    Supplemental Appendix (For Online Publication)
\end{center}

\section{Proofs of Main Results}
\subsection{Proofs for Section \ref{sec:semi}}\label{sec:SPEB}
Recall that $P_n$ denotes the distribution of the observed data $(X^{(n)}, A^{(n)}, R^{(n)})$, and $Q$ denotes the marginal distribution of the vector $(R_i(1), R_i(0), X_i)$. Note that any treatment assignment mechanism satisfying Assumption \ref{ass:unconfounded} can be represented as a function of $X^{(n)}$ and some additional randomization device $U_n \in \mathbf{R}$. Let $p_n^{U_n}$ denote the density function for $U_n$ with respect to a dominating measure $\mu^U$. In what follows, we consider a family $\{Q_{t}: t \in \mathbb{R}^{d_{\theta}}\}$ of marginal distributions indexed by $t$, and let $q_t^X$ denote the density function for $X_i$ with respect to a dominating measure $\mu^X$, $q_t^{R(a) | X}(r | x)$ denote the conditional density of $R_i(a)$ given $X_i$ with respect to a dominating measure $\mu^R$. Further let $P_{t, n}$ denote the distribution of $Z^{(n)}$ and note it is jointly determined by $Q_t$ and the distribution of $U_n$. We require that $Q_0 = Q$ and $P_{0, n} = P_n$ and define $q^X = q_0^X$ and $q^{R(a) | X} = q_0^{R(a) | X}$. As a consequence, the density function of $P_{t,n}$ is given by
\begin{equation} \label{eq:density}
\ell_n = p_n^U(U_n) \prod_{1 \leq i \leq n} q_{t}^X(X_i) \prod_{1 \leq i \leq n} \prod_{a \in \{0, 1\}} q_{t}^{R(a) | X}(R_i | X_i)^{I \{A_i = a\}}~.
\end{equation}
Because the density $p_n^{U_n}$ does not depend on $t$, and in general we will only concern ourselves with the ratio of likelihoods at different values of $t$ (so that $p_n^{U_n}$ in the ratio will cancel), in what follows we suppress the dependence on $n$ and simply identify the distribution $P_{t,n}$ with its corresponding marginal distribution $P_t$.
%Since the only term in this expression which depends on $n$ is the density 
We consider a parametric submodel $\{P_t: t \in \mathbf R^{d_\theta}\}$, where $P_0 = P$, such that the following holds for $g = (g^X, g^{R(1) | X}, g^{R(0) | X})$, each component of which is a $d_\theta$-dimensional function:
\begin{enumerate}[\rm (a)]
    \item As $t \to 0$,
    \begin{equation} \label{eq:qmd-1}
    \int \frac{1}{\|t\|^2} \Big ( q_{t}^X(x)^{1/2} - q^X(x)^{1/2}  - \frac{1}{2} q^X(x)^{1/2} t' g^X(x) \Big )^2 d \mu^X(x) \to 0~.
    \end{equation}
    \item For $a \in \{0, 1\}$, $E_Q[g^{R(a) | X}(R(a) | X) g^{R(a) | X}(R(a) | X)' | X = x]$ is Lipschitz and for $Q$-almost every $x$, as $t \to 0$,
    \begin{multline} \label{eq:qmd-2}
    \frac{1}{\|t\|^2} \int\!\!\!\int \Big ( q_{t}^{R(a) | X}(r | x)^{1/2} - q^{R(a) | X}(r | x)^{1/2} - \frac{1}{2} q^{R(a) | X}(r | x)^{1/2} t' g^{R(a) | X}(r | x) \Big )^2 \\
    d \mu^R(r) q^X(x) d \mu^X(x) \to 0~.
    \end{multline}
\end{enumerate}
In what follows, we will further index a parametric submodel by its associated function $g$, denoted by $P_{t, g}$, to emphasize the role of $g$. Similarly we denote the density of $Q_{t,g}$ by $q_{t, g}$.

Define the information of $X$ as $I^X = E_Q[g^X(X) g^X(X)']$. Define the conditional information of $R(a)$ given $X=x$ as
\[ I^{R(a) | X}(x) = E_Q[g^{R(a) | X}(R(a) | X) g^{R(a) | X}(R(a) | X)' | X = x]~. \]

\begin{lemma} \label{lem:qmd}
For a parametric submodel $\{P_{t, g}: t \in \mathbf R^{d_\theta}\}$ with $P_{0, g} = P$ that satisfies \eqref{eq:qmd-1}--\eqref{eq:qmd-2},
\begin{enumerate}[\rm (a)]
    \item $I^X < \infty$.
    \item $E_Q[g^X(X)] = 0$.
    \item $E_Q[g^{R(a) | X}(R(a) | X) g^{R(a) | X}(R(a) | X)'] < \infty$ and hence $I^{R(a) | X}(X) < \infty$ with probability one under $Q$.
    \item $E_Q[g^{R(a) | X}(R(a) | X) | X] = 0$ with probability one under $Q$.
\end{enumerate}
\end{lemma}

\begin{proof}
(a) and (b) follow from Lemma 14.2.1 in \cite{lehmann2022testing}. (c) follows from the same lemma. In order to show (d), fix $t_n \to 0$. Note \eqref{eq:qmd-2} and Markov's inequality imply that along a subsequence $t_{n_k}$,
\[ \frac{1}{\|t_{n_k}\|^2} \int \Big ( q_{t_{n_k}}^{R(a) | X}(r | x)^{1/2} - q^{R(a) | X}(r | x)^{1/2} - \frac{1}{2} q^{R(a) | X}(r | x)^{1/2} t_{n_k}' g^{R(a) | X}(r | x) \Big )^2 d \mu^R(r) \to 0 \]
for $Q$-almost every $x$. Along that subsequence, another application of Lemma 14.2.1 in \cite{lehmann2022testing} implies (d).
\end{proof}

For $t \in \mathbf R^{d_\theta}$, the log-likelihood ratio between $P_{t/\sqrt n, g}$ and $P_0 = P$ is
\[ L_{n, t}(g) = \frac{1}{n} \sum_{1 \leq i \leq n} \log \frac{q_{t / \sqrt n, g}^X(X_i)}{q^X(X_i)} + \frac{1}{n} \sum_{1 \leq i \leq n} \sum_{a \in \{0, 1\}} I \{A_i = a\} \log \frac{q_{t / \sqrt n, g}^{R(a) | X}(R_i | X_i)}{q^{R(a) | X}(R_i | X_i)}~. \]
The following lemma establishes an expansion of the log-likelihood ratio and local asymptotic normality of $\{P_{t/\sqrt n, g}\}$.

\begin{lemma} \label{lem:ll}
Suppose the treatment assignment mechanism satisfies Assumption \ref{ass:unconfounded} and $g$ satisfies \eqref{eq:qmd-1}--\eqref{eq:qmd-2}. Then,
\[ L_{n, t}(g) = \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} t' s_g(X_i, A_i, R_i) - \frac{1}{2} t' I^X t - \frac{1}{2n} \sum_{1 \leq i \leq n} \sum_{a \in \{0, 1\}} I \{A_i = a\} t' I^{R(a) | X}(X_i) t + o_P(1)~, \]
where
\begin{equation} \label{eq:score}
s_g(x, a, r) = g^X(x) + I \{a = 1\} g^{R(1) | X} (r | x) + I \{a = 0\} g^{R(0) | X} (r | x)~
\end{equation}
and $I = I^X + \eta E_Q[I^{R(1)|X}(X_i)] + (1 - \eta)E_Q[I^{R(0)|X}(X_i)]$.
If in addition the assignment mechanism satisfies Assumption \ref{ass:info}, then, under $P_0$,
\[ L_{n, t}(g) \stackrel{d}{\to} N \Big ( - \frac{1}{2} t' I t, t' I t \Big )~, \]
\end{lemma}

\begin{proof}
The first result follows from Theorem 3.1 of \cite{armstrong2022asymptotic}. The second result follows from Corollary 3.1 of \cite{armstrong2022asymptotic} given Assumption \ref{ass:info} and the assumption that $I^{R(a)|X}(x)$ is Lipschitz.
\end{proof}

We emphasize that Lemma \ref{lem:marginal} implies
\[ \sum_{1 \leq i \leq n} s_g(X_i, A_i, R_i)\]
is the sum of $n$ identically distributed, despite possibly dependent, random variables. Therefore, in what follows, quantities like $E_P[s_g]$ are well defined.

Let the following condition collect the properties of the functions $g$ that are of interest to us: 

\begin{condition} \label{cond:A1}
The function $g$ satisfies that $E_P[g^X(X)] = 0$, $E_P[g^X(X) g^X(X)'] < \infty$, $E_P[g^{R(a) | X}(R | X) \allowbreak g^{R(a) | X}(R | X)'] < \infty$, $E_P[g^{R(a) | X}(R | X) | X] = 0$ with probability one, and $E_P[g^{R(a) | X}(R | X) g^{R(a) | X}(R | X)' | X = x]$ is Lipschitz for $a \in \{0, 1\}$. In addition, $I$ is nonsingular.
\end{condition}

\noindent We note that for any $g$ that satisfies Condition \ref{cond:A1}, there exists a parametric submodel $\{P_{t, g}: t \in \mathbf R^{d_\theta}\}$ such that \eqref{eq:qmd-1}--\eqref{eq:qmd-2} hold. Such a construction follows from the construction on p.69 in \cite{tsiatis2006semiparametric} and can be done separately for $g_1(x)$ and $g_2^a(r | x)$ for each $x$ separately so that they satisfy \eqref{eq:qmd-1}--\eqref{eq:qmd-2}.

Let $\theta(P) \in \mathbf R^{d_\theta}$ be a parameter of interest. Further suppose that for each $g$ satisfying Condition \ref{cond:A1}, there exists a $d_\theta \times 1$ vector of functions $\psi^\ast \in L^2(P)$ such that for all $t \in \mathbf R^{d_\theta}$, as $n \to \infty$,
\begin{equation} \label{eq:pathwise}
    \sqrt n(\theta(P_{t / \sqrt n, g}) - \theta(P)) \to E_P[\psi^\ast s_g' t]~.
\end{equation}
We provide explicit conditions which guarantee this is possible when $\theta(P)$ is defined by \eqref{eq:moments}, in Lemma \ref{lem:differentiability} below.

We recall an estimator $\tilde \theta_n$ for $\theta(P)$ is regular if for all $g$ and $t \in \mathbf R^{d_\theta}$,
\begin{equation} \label{eq:regular}
\sqrt n(\tilde \theta_n - \theta(P_{t/\sqrt n, g})) \stackrel{P_{t/\sqrt n, g}}{\xrightarrow{\hspace*{3em}}} L 
\end{equation}
for a fixed probability measure $L$.

% We call an estimator $\hat \theta_n$ for $\theta(P)$ regular along $\{\{P_{t,cg}: t \in [-\epsilon, \epsilon]\}: c \in \mathbf R\}$ if, for all $c \in \mathbf R$,
% \[ \sqrt n(\hat \theta_n - \theta(P_{1/\sqrt n, cg})) \stackrel{P_{1/\sqrt n, cg}}{\xrightarrow{\hspace*{3em}}} L_g \]
% for a fixed probability measure $L_g$. 

The following lemma establishes a convolution theorem for regular estimators:

%Fix a function $g$ that satisfies Assumption \ref{ass:score}. Along the set of paths $\{\{P_{t, cg}: t \in [-\epsilon, \epsilon]\}: c \in \mathbf R\}$, recall an estimator The following version of convolution theorem follows directly.

%Note that by Lemma \ref{lem:marginal}, the score of the density is the sum of $n$ identically distributed, despite possibly dependent, random variables. Note that if $s_g$ is the score function of the path $\{P_{t, g}: t \in [-\epsilon, \epsilon]\}$, then $c s_g$ is the score function of the path $\{P_{t, cg}: t \in [-\epsilon, \epsilon]\}$ and satisfies \eqref{eq:qmd-1}--\eqref{eq:qmd-2}. In what follows, for each fixed $g = (g^X, g^{R(1) | X}, g^{R(0) | X})$, we study the set of paths
%\[ \{\{P_{t, cg}: t \in [-\epsilon, \epsilon]\}: c \in \mathbf R\} \]
%and note the corresponding set of scores is

\begin{lemma} \label{lem:convolution}
Suppose $\theta$ satisfies \eqref{eq:pathwise}. Let $\tilde \theta_n$ be a regular estimator for $\theta$. Further suppose that $\psi^\ast = s_g$ for some function $g$ satisfying Condition \ref{cond:A1}. Then,
\[ L = N(0, E_P[\psi^\ast \psi^{\ast\prime}]) \ast B~, \]
where $B$ is a fixed probability measure.
% In addition, $\psi^\ast$ is equal to the efficient influence function in the setting in which treatment assignment is determined by i.i.d.\ coin flips with probability $\eta$.
\end{lemma}

\begin{proof}
In what follows, for each $g$ satisfying Condition \ref{cond:A1}, we consider the linear subspace given by 
\[ \mathcal{M}_g = \{t' s_g: t \in \mathbf R^{d_\theta}\}~. \]
Note that $t' s_g$ appears in the expansion of the log-likelihood ratio between $P_{t/\sqrt n, g}$ and $P$. We first derive the Riesz representer along the parametric subspace $\mathcal M_g$. In particular, for each $b \in \mathbf R^{d_\theta}$, we solve for $w(b) \in \mathbf R^{d_\theta}$ via the property that,
\[ b' E_P[\psi^\ast s_g' t] = E_P[w(b)' s_g s_g' t] \]
needs to hold for all $t\in \mathbf R^{d_\theta}$ and get 
\[ w(b) = E[s_g s_g']^{-1} E[s_g \psi^{\ast\prime}] b~. \]
% \[ \dot \kappa^\ast(b) = w(b)' s_g\]
Therefore, the Riesz representer is
\[ E_P[\psi^\ast s_g'] E[s_g s_g']^{-1} s_g~. \]
It then follows from the local asymptotic normality established in Lemma \ref{lem:ll} and Theorem 3.11.2 in \cite{van_der_vaart1996weak} that
\[ L = N(0, V_g) \ast B_g~, \]
where
\[ V_g = E_P[\psi^\ast s_g'] E_P[s_g s_g']^{-1} E[s_g \psi^{\ast\prime}]  \]
and $B_g$ is a fixed probability measure. Furthermore, by a standard projection argument, in particular the fact that the second moment of $\psi^\ast - E_P[\psi^\ast s_g'] E_P[s_g s_g']^{-1} s_g$ is positive semi-definite, it can be shown that $V_g$ is maximized in the matrix sense when $s_g = \psi^\ast$. Note this maximum is attained by our assumption that $\psi^\ast = s_g$ for some $g$ satisfying Condition \ref{cond:A1}. The conclusion then follows.
% The last statement follows by noting that the tangent set only relies on the marginal distribution of $(X_i, A_i, R_i)$ instead of the joint distribution, and therefore the result is the same as when treatment assignment is determined by i.i.d.\ coin flips.
\end{proof}

To apply Lemma \ref{lem:convolution} to the setting in Section \ref{sec:semi}, we we establish the form of $\psi^\ast$ in \eqref{eq:pathwise} for the parameter $\theta_0 = \theta(P)$ defined by \eqref{eq:moments}. Define $\eta(X_i) = P \{A_i = 1 | X_i\}$. Note that
\begin{equation} \label{eq:moments-P}
0 = E_P[m(X_i, A_i, R_i, \theta(P))] \\
= E_Q[m(X, 1, R(1), \theta(P)) \eta(X)] + E_Q[m(X, 0, R(0), \theta(P)) (1 - \eta(X))]~.
\end{equation}

\begin{lemma} \label{lem:differentiability}
Suppose the treatment assignment mechanism satisfies Assumptions \ref{ass:unconfounded} and \ref{ass:info}. Fix a function $g$ that satisfies Condition \ref{cond:A1}. Suppose \eqref{eq:qmd-1}--\eqref{eq:qmd-2} holds. Fix $t \in \mathbf R^{d_\theta}$ and consider a one-dimensional submodel $\{P_{t / \sqrt n, g}\}$ such that
\begin{equation} \label{eq:bounded_path}
\begin{split}
   E_{Q_{t / \sqrt n}}[m(X, a, R(a), \theta(P))^2] & = O(1) \\
   E_{Q^X}[E_{Q_{t / \sqrt n}^{R(a) | X}}[m(X, a, R(a), \theta(P))^2 | X]] & = O(1) \\
   E_{Q_{t / \sqrt n}^X}[E_{Q^{R(a) | X}}[m(X, a, R(a), \theta(P))^2 | X]] & = O(1)
\end{split}    
\end{equation}
as $n \to \infty$ and $\theta(P_{t / \sqrt n, g})$ is uniquely determined by \eqref{eq:moments-P}. Then, $\theta(P_{t / \sqrt n, g})$ defined by \eqref{eq:moments-P} satisfies
\begin{align*}
& \sqrt n ( \theta(P_{t / \sqrt n, g}) - \theta(P)) \\
& \to M^{-1} E_P[m(X_i, A_i, R_i, \theta(P))(g^X(X_i) + I \{A_i = 1\}g^{R(1) | X}(R_i | X_i) + I \{A_i = 0\}g^{R(0) | X}(R_i | X_i))'] t \\
& = E_P[\psi^\ast(X_i, A_i, R_i, \theta(P)) (g^X(X_i) + I \{A_i = 1\}g^{R(1) | X}(R_i | X_i) + I \{A_i = 0\} g^{R(0) | X}(R_i | X_i))'] t~,
\end{align*}
where
\begin{align*}
& \psi^\ast(X_i, A_i, R_i, \theta(P)) \\
& = M^{-1} \Big ( \eta(X_i) E_Q[m(X_i, 1, R_i(1), \theta(P)) | X_i] + (1 - \eta(X_i)) E_Q[m(X_i, 0, R_i(0), \theta(P)) | X_i] \\
& \hspace{5em} + I\{A_i = 1\} (m(X_i, 1, R_i, \theta(P)) - E_Q[m(X_i, 1, R_i(1), \theta(P)) | X_i]) \\
& \hspace{5em} + I\{A_i = 0\} (m(X_i, 0, R_i, \theta(P)) - E_Q[m(X_i, 0, R_i(0), \theta(P)) | X_i]) \Big )~.
\end{align*}
\end{lemma}

\begin{proof}
In what follows, we only use the property that the quadratic mean derivative of $P_{t / \sqrt n, g}$ is given by $s_g' t$. Therefore, for ease of notation we consider a generic one-dimensional submodel $\{P_\nu: \nu \in [-\epsilon, \epsilon]\}$ that satisfies \eqref{eq:qmd-1}--\eqref{eq:qmd-2} for some $g = (g^X, g^{R(1) | X}, g^{R(0) | X})$, each component of which is a one-dimensional function. \eqref{eq:moments-P} implies
\begin{multline*}
0 = \int m(x, 1, r, \theta(P_\nu)) q_\nu^{R(1) | X}(r | x) d\mu^R(r) \eta(x) q_\nu^X(x) d\mu^X(x) \\
+ \int m(x, 0, r, \theta(P_\nu)) q_\nu^{R(0) | X}(r | x) d\mu^R(r) (1 - \eta(x)) q_\nu^X(x) d\mu^X(x)
\end{multline*}
Note that
\begin{align*}
& \int m(x, 1, r, \theta(P)) q_\nu^{R(1) | X}(r | x) d\mu^R(r) \eta(x) q_\nu^X(x) d\mu^X(x) \\
& \hspace{2em} - \int m(x, 1, r, \theta(P)) q^{R(1) | X}(r | x) d\mu^R(r) \eta(x) q^X(x) d\mu^X(x) = \gamma_1(\nu) + \gamma_2(\nu) + \gamma_3(\nu) + \gamma_4(\nu)~,
\end{align*}
where
\begin{align*}
\gamma_1(\nu) & = \int m(x, 1, r, \theta(P)) \big ( q_\nu^{R(1) | X}(r | x)^{1/2} - q^{R(1) | X}(r | x)^{1/2} \big ) q_\nu^{R(1) | X}(r | x)^{1/2} d\mu^R(r) \eta(x) q_\nu^X(x) d\mu^X(x) \\
\gamma_2(\nu) & = \int m(x, 1, r, \theta(P)) \big ( q_\nu^{R(1) | X}(r | x)^{1/2} - q^{R(1) | X}(r | x)^{1/2} \big ) q^{R(1) | X}(r | x)^{1/2} d\mu^R(r) \eta(x) q_\nu^X(x) d\mu^X(x) \\
\gamma_3(\nu) & = \int m(x, 1, r, \theta(P)) q^{R(1) | X}(r | x) d\mu^R(r) \eta(x) \big ( q_\nu^X(x)^{1/2} - q^X(x)^{1/2} \big ) q_\nu^X(x)^{1/2} d\mu^X(x) \\
\gamma_4(\nu) & = \int m(x, 1, r, \theta(P)) q^{R(1) | X}(r | x) d\mu^R(r) \eta(x) \big ( q_\nu^X(x)^{1/2} - q^X(x)^{1/2} \big ) q^X(x)^{1/2} d\mu^X(x)~.
% & + \int m(x, 1, r, \theta(P)) \big ( q_\nu^{R(1) | X}(r | x)^{1/2} - p^{R(1) | X}(r | x)^{1/2} \big ) p^{R(1) | X}(r | x)^{1/2} d\mu^R(r) \\
% & \hspace{5em} \times \eta(X) \big ( q_\nu^X(x)^{1/2} - p^X(x)^{1/2} \big ) q_\nu^X(x)^{1/2} d\mu^X(x) \\
% & + \int m(x, 1, r, \theta(P)) \big ( q_\nu^{R(1) | X}(r | x)^{1/2} - p^{R(1) | X}(r | x)^{1/2} \big ) q_\nu^{R(1) | X}(r | x)^{1/2} d\mu^R(r) \\
% & \hspace{5em} \times \eta(X) \big ( q_\nu^X(x)^{1/2} - p^X(x)^{1/2} \big ) p^X(x)^{1/2} d\mu^X(x) \\
% & + \int m(x, 1, r, \theta(P)) \big ( q_\nu^{R(1) | X}(r | x)^{1/2} - p^{R(1) | X}(r | x)^{1/2} \big ) p^{R(1) | X}(r | x)^{1/2} d\mu^R(r) \\
% & \hspace{5em} \times \eta(X) \big ( q_\nu^X(x)^{1/2} - p^X(x)^{1/2} \big ) p^X(x)^{1/2} d\mu^X(x)
\end{align*}
It follows from the Cauchy-Schwarz inequality that
\begin{align*}
& \frac{1}{\nu} \gamma_4(\nu) - \int m(x, 1, r, \theta(P)) q^{R(1) | X}(r | x) d\mu^R(r) \eta(x) \frac{1}{2} g^X(x) q^X(x)^{1/2} \times q^X(x)^{1/2}  d\mu^X(x) \\
& \leq \int \bigg ( m(x, 1, r, \theta(P))^2 q^{R(1) | X}(r | x) d\mu^R(r) \eta(x)^2 q^X(x) d \mu^X(x) \bigg )^{1/2} \\
& \hspace{2em} \times \bigg ( \int q^{R(1) | X}(r | x) d\mu^R(r) \Big ( \frac{1}{\nu}(q_\nu^X(x)^{1/2} - q^X(x)^{1/2}) - \frac{1}{2} g^X(x) q^X(x)^{1/2} \Big )^2 d \mu^X(x) \bigg )^{1/2}
\to 0
\end{align*}
by the assumption that $E_P[m(X, a, R(a), \theta(P))^2] < \infty$, the facts that $0 \leq \eta(x) \leq 1$, $\int q^{R(1) | X}(r | x) d\mu^R(r) = 1$, and \eqref{eq:qmd-1}. Similar arguments implies as $\nu \to 0$,
\[ \frac{1}{\nu} \gamma_1(\nu) - \int m(x, 1, r, \theta(P)) \frac{1}{2} g^{R(1) | X}(r | x) q^{R(1) | X}(r | x) d\mu^R(r) \eta(x) q^X(x) d\mu^X(x) \to 0 \]
because $E_{P_\nu}[m(X, a, R(a), \theta(P))^2] = O(1)$ as $\nu \to 0$. The limits of $\gamma_2(\nu)$ and $\gamma_3(\nu)$ can be derived following similar arguments using the last two conditions in \eqref{eq:bounded_path}. Combining all previous results yields
\begin{align*}
& \frac{\partial}{\partial \nu} E_{P_\nu}[m(X, A, R, \theta(P))] \Big |_{\nu = 0} \\
& = E_Q[m(X, 1, R(1), \theta(P)) (g^X(X) + g^{R(1) | X}(R | X)) \eta(X)] \\
& \hspace{3em} + E_Q[m(X, 0, R(0), \theta(P)) (g^X(X) + g^{R(0) | X}(R | X)) (1 - \eta(X))] \\
& = E_P[m(X, A, R, \theta(P))(g^X(X) + I \{A = 1\}g^{R(1) | X}(R) + I \{A = 0\}g^{R(0) | X}(R))]~.
\end{align*}
On the other hand, by definition
\[ M_{\theta(P)} = \frac{\partial}{\partial \theta} E_P[m(X, A, R, \theta)] \Big |_{\theta = \theta(P)}~. \]
The formula for the derivative therefore follows from the implicit function theorem (in particular, because we have assumed the existence of $\theta(P_\nu)$ along the path, it follows from the last part of the proof of Theorem 3.2.1 in \cite{krantz2013implicit}). The second equality follows from Lemma \ref{lem:marginal} together with Condition \ref{cond:A1}.
\end{proof}

Finally, to prove Theorem \ref{thm:efficiencybound} we require the following additional regularity condition:

\begin{assumption}\label{ass:bounded_path}
For every function $g$ satisfying Condition \ref{cond:A1} and every $t \in \mathbf R^{d_\theta}$ there exists a submodel $P_{t / \sqrt n,g}$ for which \eqref{eq:bounded_path} holds as $n \to \infty$, and $\theta(P_{t / \sqrt n, g})$ is uniquely determined by \eqref{eq:moments-P}.
\end{assumption}

This assumption guarantees that every element satisfying Condition \ref{cond:A1} has a corresponding path for which we can apply Lemma \ref{lem:differentiability}. A similar assumption appears in \cite{chen2018overidentification} (see their Assumption 4.1(iv)). Note that a simple sufficient condition for the first part of Assumption \ref{ass:bounded_path} is that $m(x, a, r, \theta(P))$ is a bounded function in $(x, r)$ on the support of $(X, R(a))$. The second part of Assumption \ref{ass:bounded_path} can be verified easily in specific examples (see, for instance, Examples \ref{ex:ATE}--\ref{ex:log_odds} in the main text). Alternatively, Assumption \ref{ass:bounded_path} could be avoided by assuming that we can differentiate under the integral in the final step of the proof of Lemma \ref{lem:differentiability}, from which we would immediately obtain the expression for the pathwise derivative. See, for instance, \cite{newey1994asymptotic} and \cite{chen2008semiparametric}.

\begin{proof}[\sc Proof of Theorem \ref{thm:efficiencybound}]
    First note $\theta$ satisfies \eqref{eq:pathwise} because of Lemma \ref{lem:differentiability} and Assumption \ref{ass:bounded_path}. The result then follows from Lemma \ref{lem:convolution} upon noting that $\psi^\ast = s_g$ for some $g$ that satisfies Condition \ref{cond:A1} because of Assumption \ref{ass:var_lipschitz}.
\end{proof}

\subsection{Proof of Theorem \ref{thm:normal}}
First note \eqref{eq:normal_convergence} follows from \eqref{eq:normal} and the same proof as that of Lemma B.3 in \cite{bai2022optimality}. To establish \eqref{eq:normal}, we follow the proof of Theorem 5.21 in \cite{van_der_vaart1998asymptotic}. We start by noting that because Assumptions \ref{ass:a}--\ref{ass:pair}, \ref{ass:consistency}(e), and \ref{ass:normal}(b) hold, it follows from the same proof as that of Lemma B.3 in \cite{bai2022optimality} that
\begin{align*}
\frac{1}{\sqrt n} \sum_{1 \leq i \leq n} m(X_i, A_i, R_i, \theta_0) & = \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} \sum_{a \in \{0, 1\}} I \{A_i = a\} (m(X_i, a, R_i(a), \theta_0) - E_Q[m(X_i, a, R_i(a), \theta_0) | X_i]) \\
& + \frac{\eta}{\sqrt n} \sum_{1 \leq i \leq n} (E_Q[m(X_i, 1, R_i(1), \theta_0) | X_i] - E_Q[m(X_i, 1, R_i(1), \theta_0)]) \\
& + \frac{(1 - \eta)}{\sqrt n} \sum_{1 \leq i \leq n} (E_Q[m(X_i, 0, R_i(0), \theta_0) | X_i] - E_Q[m(X_i, 0, R_i(0), \theta_0)]) + o_P(1)~.
\end{align*}
where we note $\eta E_Q[m(X_i, 1, R_i(1), \theta_0)] + (1 - \eta) E_Q[m(X_i, 0, R_i(0), \theta_0)] =  E_P[m(X_i, A_i, R_i, \theta_0)] = 0$ by \eqref{eq:moments}. Therefore, by the proof of Theorem 5.21 in \cite{van_der_vaart1998asymptotic}, it suffices to show
\begin{equation}\label{eq:se}
\mathbb L_n(\hat{\theta}_n) \stackrel{P}{\to} 0~,
\end{equation}
where
\begin{multline*} 
\mathbb L_n(\theta) = \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} (m(X_i, A_i, R_i, \theta) - E_P[m(X_i, A_i, R_i, \theta)]) \\
- \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} (m(X_i, A_i, R_i, \theta_0) - E_P[m(X_i, A_i, R_i, \theta_0)])~.
\end{multline*}
To accomplish this, we study $m_s$ for $1 \leq s \leq d_\theta$ separately. It follows from Assumption \ref{ass:consistency}(c), (d), and the proof of Proposition 8.11 in \cite{kosorok2008introduction} that
\begin{align*}
& \sup_{\theta \in \Theta: \|\theta - \theta_0\| < \delta} \Big | \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} ( m_s(X_i, A_i, R_i, \theta) - E_P[m_s(X_i, A_i, R_i, \theta)]) \\
& \hspace{3em} - \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} (m_s(X_i, A_i, R_i, \theta_0) - E_P[m_s(X_i, A_i, R_i, \theta_0)]) \Big | \\
& = \sup_{\theta \in \Theta^\ast: \|\theta - \theta_0\| < \delta} \Big | \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} ( m_s(X_i, A_i, R_i, \theta) - E_P[m_s(X_i, A_i, R_i, \theta)]) \\
& \hspace{3em} - \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} (m_s(X_i, A_i, R_i, \theta_0) - E_P[m_s(X_i, A_i, R_i, \theta_0)]) \Big |~,
\end{align*}
and thus since $\hat{\theta}_n \stackrel{P}{\to} \theta_0$ by Lemma \ref{lem:consistency}, to show \eqref{eq:se} it suffices to argue that for every $\epsilon > 0$,
\begin{equation}\label{eq:L_equicont}\lim_{\delta \downarrow 0}\limsup_{n \to \infty}P\left\{\sup_{\theta \in \Theta^\ast:\|\theta - \theta_0\|<\delta}\big|\mathbb{L}^{(s)}_n(\theta)\big| > \epsilon\right\} = 0~,
\end{equation}
% \[ \bigg \{ \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} m(X_i, A_i, R_i, \theta): \theta \in \Theta \bigg \} \]
% is stochastically equicontinuous at $\theta_0$, i.e., for every $\epsilon > 0$,
% \begin{multline} \label{eq:se}
% \lim_{\delta \to 0} \limsup_{n \to \infty} \mathrm{Pr}^\ast \bigg \{ \sup_{\|\theta - \theta_0\| < \delta} \Big | \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} (m(X_i, A_i, R_i, \theta) - E_P[m(X_i, A_i, R_i, \theta)]) \\
% - \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} (m(X_i, A_i, R_i, \theta_0) - E_P[m(X_i, A_i, R_i, \theta_0)]) \Big | > \epsilon \bigg \} = 0~.
% \end{multline}
where 
\begin{multline*}
\mathbb{L}^{(s)}_n(\theta) = \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} ( m_s(X_i, A_i, R_i, \theta) - E_P[m_s(X_i, A_i, R_i, \theta)]) \\
- \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} (m_s(X_i, A_i, R_i, \theta_0) - E_P[m_s(X_i, A_i, R_i, \theta_0)])~.
\end{multline*}
Consider the following decomposition:
\[\big|\mathbb{L}^{(s)}_n(\theta)\big| \le \sum_{a \in \{0, 1\}} \big ( \mathbb L_{1, a, n}^{(s)}(\theta) + \mathbb L_{2, a, n}^{(s)}(\theta) +\mathbb L_{3, a, n}^{(s)}(\theta) \big )~,\]
%\begin{multline*}
%\Big | \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} ( m_s(X_i, A_i, R_i, \theta) - E_P[m_s(X_i, A_i, R_i, \theta)]) \\
%- \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} (m_s(X_i, A_i, R_i, \theta_0) - E_P[m_s(X_i, A_i, R_i, \theta_0)]) \Big | \\
%\leq \sum_{a \in \{0, 1\}} (\mathbb L_{1, a, n}^{(s)}(\theta) + \mathbb L_{2, a, n}^{(s)}(\theta) + \mathbb L_{3, a, n}^{(s)}(\theta))~,
%\end{multline*}
where
\begin{align*}
\mathbb L_{1, a, n}^{(s)}(\theta) & = \Big | \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} I \{A_i = a\} (m_s(X_i, a, R_i(a), \theta) - E_Q[m_s(X_i, a, R_i(a), \theta) | X_i]) \\
& \hspace{5em} - \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} I \{A_i = a\} (m_s(X_i, a, R_i(a), \theta_0) - E_Q[m_s(X_i, a, R_i(a), \theta_0) | X_i]) \Big | \\
\mathbb L_{2, a, n}^{(s)}(\theta) & = \Big | \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} \Big ( I \{A_i = a\} - \eta_a \Big ) \\
& \hspace{5em} \times (E_Q[m_s(X_i, a, R_i(a), \theta) | X_i] - E_Q[m_s(X_i, a, R_i(a), \theta_0) | X_i]) \Big | \\
\mathbb L_{3, a, n}^{(s)}(\theta) & = \Big | \frac{\eta_a}{\sqrt n} \sum_{1 \leq i \leq n} \big ( E_Q[m_s(X_i, a, R_i(a), \theta) | X_i] - E_Q[m_s(X, a, R(a), \theta)]) \\
& \hspace{5em} - (E_Q[m_s(X_i, a, R_i(a), \theta_0) | X_i] - E_Q[m_s(X, a, R(a), \theta_0)]) \big ) \Big |~,
\end{align*}
where $\eta_1 = \eta$ and $\eta_0 = 1 - \eta$. Then to establish \eqref{eq:L_equicont}, it suffices to establish that
\begin{equation}\label{eq:Ls_equicont}
\lim_{\delta \downarrow 0}\limsup_{n \to \infty}P\left\{\sup_{\theta \in \Theta^\ast:\|\theta - \theta_0\|<\delta}\mathbb{L}^{(s)}_{\ell, a, n}(\theta) > \epsilon\right\} = 0~.
\end{equation}
for $\ell \in \{1, 2, 3\}$ and $a \in \{0, 1\}$.

\noindent \textbf{Step 1}. First we consider $\mathbb L_{3, a, n}^{(s)}$. It follows from Assumption \ref{ass:normal}(d) and Theorems 2.5.2 and 2.6.7 in \cite{van_der_vaart1996weak} that the class of functions 
\[\{E_Q[m_s(X, a, R(a), \theta)|X=x]: \theta \in \Theta^\ast\}~,\]
%\[ \bigg \{ \frac{1}{2 \sqrt n} \sum_{1 \leq i \leq n} (E_Q[m_s(X_i, a, R_i(a), \theta) | X_i] - E_Q[m_s(X, a, R(a), \theta)]): \theta \in \Theta \bigg \} \]
is Donsker, and thus we obtain by Theorem 3.34 in \cite{dudley2014uniform} that 
\[\lim_{\delta \downarrow 0}\limsup_{n \rightarrow \infty}P\left\{\sup_{\theta \in \Theta^\ast:\rho_Q(\theta, \theta_0)<\delta}\mathbb{L}^{(s)}_{3, a, n}(\theta) > \epsilon\right\} = 0 ~,\]
where $\rho_Q(\theta, \theta_0) = E_Q[(E_Q[m_s(X, a, R(a), \theta) | X] - E_Q[m_s(X, a, R(a), \theta_0) | X])^2]$. We then obtain \eqref{eq:Ls_equicont} for $\ell = 3$ since, by Assumption \ref{ass:normal}(c) as $\theta \to \theta_0$,
\begin{multline} \label{eq:L2}
\rho_Q(\theta,\theta_0) = E_Q[(E_Q[m_s(X, a, R(a), \theta) | X] - E_Q[m_s(X, a, R(a), \theta_0) | X])^2] \\
\leq E_Q[(m_s(X, a, R(a), \theta) - m_s(X, a, R(a), \theta_0))^2] \to 0~.
\end{multline}

%The result then follows from Lemma 19.24 in \cite{van_der_vaart1998asymptotic}, 
% For $\mathbb L_{2, a, n}(\theta)$, note that for every fixed $\theta$,
% \begin{align*}
% & \var[\mathbb L_{2, a, n}^{(s)}(\theta) | X^{(n)}] \\
% & = \frac{1}{2n} \sum_{1 \leq j \leq \frac{n}{2}}(E_Q[m_s(X_{\pi(2j - 1)}, a, R_{\pi(2j - 1)}(a), \theta) | X_{\pi(2j - 1)}] - E_Q[m_s(X_{\pi(2j - 1)}, a, R_{\pi(2j - 1)}(a), \theta_0) | X_{\pi(2j - 1)}] \\
% & - (E_Q[m_s(X_{\pi(2j)}, a, R_{\pi(2j)}(a), \theta) | X_{\pi(2j)}] - E_Q[m_s(X_{\pi(2j)}, a, R_{\pi(2j)}(a), \theta_0) | X_{\pi(2j)}]))^2 \\
% & \lesssim \frac{1}{n} \sum_{1 \leq j \leq \frac{n}{2}} (E_Q[m_s(X_{\pi(2j - 1)}, a, R_{\pi(2j - 1)}(a), \theta) | X_{\pi(2j - 1)}] - E_Q[m_s(X_{\pi(2j)}, a, R_{\pi(2j)}(a), \theta) | X_{\pi(2j)}])^2 \\
% & \hspace{3em} + \frac{1}{n} \sum_{1 \leq j \leq \frac{n}{2}} (E_Q[m_s(X_{\pi(2j - 1)}, a, R_{\pi(2j - 1)}(a), \theta_0) | X_{\pi(2j - 1)}] - E_Q[m_s(X_{\pi(2j)}, a, R_{\pi(2j)}(a), \theta_0) | X_{\pi(2j)}])^2 \\
% & \lesssim \frac{1}{n} \sum_{1 \leq j \leq \frac{n}{2}} \|X_{\pi(2j - 1)} - X_{\pi(2j)}\|^2 \stackrel{P}{\to} 0~.
% \end{align*}
% Therefore, $\mathbb L_{2, a, n}^{(s)}(\theta) \stackrel{P}{\to} 0$ by Markov's inequality applied conditional on $X^{(n)}$ and the dominated convergence theorem. 

\noindent \textbf{Step 2}. Next, we study $\mathbb L_{2, a, n}^{(s)}$. 
Define
\[ f(X, \theta) = E[m_s(X, a, R(a), \theta) | X] - E[m_s(X, a, R(a), \theta_0) | X]~. \]
Note
\[\mathbb L_{2, a, n}^{(s)}(\theta) = C\Big | \frac{1}{\sqrt{n / k}} \sum_{1 \leq j \leq n / k} \alpha_j(\theta) \Big |~,\]
for some constant $C > 0 $, where $\alpha_j(\theta) \in \{\frac{1}{\ell}\sum_{i \in I}f(X_i, \theta) - \frac{1}{k - \ell}\sum_{i \in \lambda_j \backslash I}f(X_i, \theta): I \subset \lambda_j, |I| = \ell\}$, $E[\alpha_j(\theta) | X^{(n)}] = 0$, and $\alpha_j(\theta), 1 \leq j \leq n / k$ are independent conditional on $X^{(n)}$.

Define
\begin{align*}
    h(x_1, \dots, x_k, \theta) & = \sup_{I \subset \{1, \dots, k\}, |I| = \ell} \Big ( \frac{1}{\ell}\sum_{i \in I}f(x_i, \theta) - \frac{1}{k - \ell}\sum_{i \in \{1, \dots, k\} \backslash I}f(x_i, \theta) \Big ) \\
    & \hspace{3em} - \inf_{I \subset \{1, \dots, k\}, |I| = \ell} \Big ( \frac{1}{\ell}\sum_{i \in I}f(x_i, \theta) - \frac{1}{k - \ell}\sum_{i \in \{1, \dots, k\} \backslash I}f(x_i, \theta) \Big )
\end{align*}
and the classes of functions
\begin{align*}
    \mathcal H_{\delta} & = \{h(x_1, \dots, x_k, \theta): \theta \in \Theta^\ast, \|\theta - \theta_0\| < \delta\} \\
    \mathcal H_{\infty} & = \{h(x_1, \dots, x_k, \theta): \theta \in \Theta^\ast\}~.
\end{align*}
Let $P_n^\dagger$ denote a measure that puts mass $\frac{k}{n}$ on each of $(X_i: i \in \lambda_j), 1 \leq j \leq n / k$. It follows from a generalized version of Hoeffding's inequality \citep[see, for instance, Theorem 2.2.6 in][]{vershynin2018high-dimensional} that conditional on $X^{(n)}$,
\[ \bigg \{ \frac{1}{\sqrt{n / k}} \sum_{1 \leq j \leq n / k} \alpha_j(\theta): \theta \in \Theta^\ast, \|\theta - \theta_0\| < \delta \bigg \} \]
is sub-Gaussian for the seminorm
\[ \|h\|_{P_n^\dagger} = \Big ( \int h^2 d P_n^\dagger \Big )^{1/2}~. \]
Let $N(\epsilon, \mathcal H_{\delta}, L_2(P_n^\dagger))$ denote the covering number of $\mathcal H_{\delta}$ with respect to $\|\cdot\|_{P_n^\dagger}$. Let $\delta_n \downarrow 0$ be an arbitrary decreasing sequence. It follows from the maximal inequality in Corollary 2.2.8 in \cite{van_der_vaart1996weak} (note $\alpha_j(\theta_0) = 0$) that
\begin{equation} \label{eq:br-integral1}
E \bigg [ \sup_{\theta \in \Theta^\ast: \|\theta - \theta_0\| < \delta_n}~ \mathbb L_{2, a, n}^{(s)}(\theta) \bigg | X^{(n)} \bigg ] \lesssim \int_0^\infty \sqrt{\log N(\epsilon, \mathcal H_{\delta_n}, L_2(P_n^\dagger))} d \epsilon~.
\end{equation}
The upper limit of the integral is in fact $c_n$, where
\begin{align}
\label{eq:cn} c_n^2 & = \sup_{\theta \in \Theta^\ast: \|\theta - \theta_0\| < \delta_n}~ \frac{4k}{n} \sum_{1 \leq j \leq n / k} \sup_{I \in \lambda_j, |I| = \ell} \Big ( \frac{1}{\ell}\sum_{i \in I}f(X_i, \theta) - \frac{1}{k - \ell}\sum_{i \in \lambda_j \backslash I}f(X_i, \theta) \Big )^2 \\
\nonumber & \lesssim \frac{1}{n} \sum_{1 \leq j \leq n / k} \max_{i, i' \in \lambda_j} \|X_i - X_{i'}\|^2 \stackrel{P}{\to} 0   
\end{align}
by Assumptions \ref{ass:pair} and \ref{ass:consistency}(e) and the inequality $(a + b)^2 \leq 2(a^2 + b^2)$. Moreover,
\[ H(x_1, \dots, x_k) = \sum_{1 \leq i \leq k} E \bigg [ \sup_{\theta \in \Theta^\ast}~ |m_s(X, a, R(a), \theta)| + |m_s(X, a, R(a), \theta_0)| \bigg | X = x_i \bigg ] \]
is an envelope function for $\mathcal H_{\infty}$ (and thus $\mathcal H_{\delta}$ for all $\delta$) and $E[H^2] < \infty$ by Assumption \ref{ass:normal}(b). A change of variable in \eqref{eq:br-integral1} implies
\begin{multline*}
E \bigg [ \sup_{\theta \in \Theta^\ast: \|\theta - \theta_0\| < \delta_n}~ \mathbb L_{2, a, n}^{(s)}(\theta) \bigg | X^{(n)} \bigg ] \lesssim \int_0^{\frac{c_n}{\|H\|_{P_n^\dagger}}} \sqrt{\log N(\epsilon \|H\|_{P_n^\dagger}, \mathcal H_{\delta_n}, L_2(P_n^\dagger))} d \epsilon \|H\|_{P_n^\dagger} \\
\leq \int_0^{\frac{c_n}{\|H\|_{P_n^\dagger}}} \sup_\nu \sqrt{\log N(\epsilon \|H\|_\nu, \mathcal H_{\infty}, L_2(\nu))} d \epsilon \|H\|_{P_n^\dagger}~,
\end{multline*}
where the supremum for $\nu$ is over probability measures with discrete support such that $\|H\|_\nu > 0$. Also note that if $\|H\|_{P_n^\dagger} = 0$ then the conditional expectation on the left-hand side is trivially zero, so we can without loss of generality assume $\|H\|_{P_n^\dagger} > 0$. The Cauchy-Schwarz inequality implies
\begin{multline*}
E \bigg [ \int_0^{\frac{c_n}{\|H\|_{P_n^\dagger}}} \sup_\nu \sqrt{\log N(\epsilon \|H\|_\nu, \mathcal H_{\infty}, L_2(\nu))} d \epsilon \|H\|_{P_n^\dagger} \bigg ] \\
\leq E \bigg [ \Big ( \int_0^{\frac{c_n}{\|H\|_{P_n^\dagger}}} \sup_\nu \sqrt{\log N(\epsilon \|H\|_\nu, \mathcal H_{\infty}, L_2(\nu))} d \epsilon \Big )^2 \bigg ]^{1/2} E \big [ \|H\|_{P_n^\dagger}^2 \big ]^{1/2}~,
\end{multline*}
where the supremum is over all measures $\nu$ with discrete support such that $\|H\|_\nu > 0$. It follows from Assumption \ref{ass:normal}(b), the inequality $(a + b)^2 \leq 2(a^2 + b^2)$, and the conditional Jensen's inequality that
\[ E \big [ \|H\|_{P_n^\dagger}^2 \big ] \lesssim E \bigg [ \frac{1}{n} \sum_{1 \leq i \leq n} E \Big [ \sup_{\theta \in \Theta^\ast}~ |m_s(X_i, a, R_i(a), \theta)| \Big | X_i \Big ]^2 \bigg ] \leq E \bigg [ \sup_{\theta \in \Theta^\ast}~ |m_s(X, a, R(a), \theta)|^2 \bigg ] < \infty~. \]
On the other hand,
\begin{equation} \label{eq:Hlowerbound}
\|H\|_{P_n^\dagger}^2 \geq \frac{1}{n} \sum_{1 \leq i \leq n} E \bigg [ \sup_{\theta \in \Theta^\ast}~ |m_s(X_i, a, R_i(a), \theta)| \bigg | X_i \bigg ]^2 \stackrel{P}{\to} E \bigg [ E \Big [ \sup_{\theta \in \Theta^\ast}~ |m_s(X, a, R(a), \theta)| \Big | X_i \Big ]^2 \bigg ]~,
\end{equation}
the right-hand side of which can be assumed to be strictly positive, because otherwise $\sup_{\theta \in \Theta^\ast: \|\theta - \theta_0\| < \delta_n} \mathbb L_{2, a, n}^{(s)}(\theta) = 0$. Therefore, it follows from \eqref{eq:cn} and \eqref{eq:Hlowerbound} that
\begin{equation} \label{eq:upperlimit}
    \frac{c_n}{\|H\|_{P_n^\dagger}} \stackrel{P}{\to} 0~.
\end{equation}
From Assumption \ref{ass:normal}(d), Lemma \ref{lem:buei}, Theorem 2.6.7 in \cite{van_der_vaart1996weak}, and Lemma 9.13 in \cite{kosorok2008introduction},  we know
\[ \int_0^1 \sup_\nu \sqrt{\log N(\epsilon \|H\|_\nu, \mathcal H_{\infty}, L_2(\nu))} d \epsilon < \infty~. \]
% \textcolor{red}{[\bf technically BUEI means that this integral is finite with upper limit 1. Shouldn't matter for our purposes though?]}
Therefore,
\[ E \bigg [ \Big ( \int_0^{\frac{c_n}{\|H\|_{P_n^\dagger}}} \sup_\nu \sqrt{\log N(\epsilon \|H\|_\nu, \mathcal H_{\infty}, L_2(\nu))} d \epsilon \Big )^2 \bigg ] \to 0 \]
by Lemma \ref{lem:pdom} combined with \eqref{eq:upperlimit} and the continuous mapping theorem. Therefore, it follows from Markov's inequality that
\[ \sup_{\theta \in \Theta^\ast: \|\theta - \theta_0\| < \delta_n} \mathbb L_{2, a, n}^{(s)}(\theta) \stackrel{P}{\to} 0~, \]
as $n \rightarrow \infty$, from which \eqref{eq:Ls_equicont} follows \citep[see, for instance, Section 2.1.2 in][]{van_der_vaart1996weak}.

\noindent \textbf{Step 3}. Finally, we study $\mathbb L_{1, a, n}^{(s)}(\theta)$. Define
\[ \mathbb B_n(\theta) = \frac{1}{\sqrt {\eta_a n}} \sum_{1 \leq i \leq n} I \{A_i = a\} (m_s(X_i, a, R_i(a), \theta) - E_Q[m_s(X_i, a, R_i(a), \theta) | X_i])~, \]
% the finite-dimensional distributions given $X^{(n)}$ and $A^{(n)}$ converge in distribution to nonrandom limits by the same arguments as in the proof of Lemma A.1 in \cite{bai2022inference}.
Let $\delta_n \downarrow 0$ be an arbitrary decreasing sequence. To establish our result we will show
%\begin{equation} \label{eq:l1-se}
%\lim_{\delta \to 0} \limsup_{n \to \infty} P \bigg \{ \sup_{\theta \in \Theta^\ast: \|\theta - \theta_0\| < \delta} |\mathbb B_n(\theta) - \mathbb B_n(\theta_0)| > \epsilon \bigg \} = 0~.
%\end{equation}
\begin{equation} \label{eq:l1-se}\sup_{\theta \in \Theta^\ast: \|\theta - \theta_0\| < \delta_n} |\mathbb B_n(\theta) - \mathbb B_n(\theta_0)| \overset{P}{\to} 0~,
\end{equation}
as $n \to \infty$. As in the proof of Lemma \ref{lem:consistency}, we define
\[ \tilde P_n = \frac{1}{\eta_a n} \sum_{1 \leq i \leq n: A_i = a} \delta_{(X_i, R_i(a))}~. \]
Define the classes of functions
% \[ \mathcal F_{\theta_0, \delta} = \{m_s(x, a, r(a), \theta): \theta \in \Theta^\ast, \|\theta - \theta_0\| < \delta\}~, \]
\[ \mathcal{F}_{\theta_0, \infty} = \{m_s(x, a, r(a), \theta): \theta \in \Theta^\ast\}~.\]
% {\bf [did we decide we didn't need the delta balls anymore? I can't remember... will need to go back and check them]}
Pick an envelope function for $\mathcal F_{\theta_0, \infty}$
% (and thus for $\mathcal F_{\theta_0, \delta}$ for all $\delta > 0$)
as
\[ F = \sup_{\theta \in \Theta^\ast}~ |m_s(X, a, R(a), \theta)|~. \]
and define
\[ \zeta_n^2 = \sup_{\theta \in \Theta^\ast: \|\theta - \theta_0\| < \delta_n}~ \int (m_s(X, a, R(a), \theta) - m_s(X, a, R(a), \theta_0))^2 d \tilde P_n~. \]

\noindent \textbf{Step 3(a)}. Our next goal is to show for every $\xi > 0$,
\begin{equation} \label{eq:zeta}
    P \big \{ \zeta_n^2 > \xi | X^{(n)}, A^{(n)} \big \} \stackrel{P}{\to} 0~.
\end{equation}
To do so, first note by triangle inequality that $\zeta_n^2 \leq \mathbb C_{1, n} + \mathbb C_{2, n} + \mathbb C_{3, n}$, where
\begin{align*}
   \mathbb C_{1, n} & = \sup_{\theta \in \Theta^\ast} \Big| \int (m_s(X, a, R(a), \theta) - m_s(X, a, R(a), \theta_0))^2 d \tilde P_n \\
   &- E \bigg [ \int (m_s(X, a, R(a), \theta) - m_s(X, a, R(a), \theta_0))^2 d \tilde P_n \bigg | X^{(n)}, A^{(n)} \bigg ] \bigg | \\
   \mathbb C_{2, n} & = \sup_{\theta \in \Theta^\ast}~ \bigg | E \bigg [ \int (m_s(X, a, R(a), \theta) - m_s(X, a, R(a), \theta_0))^2 d \tilde P_n \bigg | X^{(n)}, A^{(n)} \bigg ] \\
   & \hspace{3em} - E[(m_s(X, a, R(a), \theta) - m_s(X, a, R(a), \theta_0))^2] \bigg | \\
   \mathbb C_{3, n} & = \sup_{\theta \in \Theta^\ast: \|\theta - \theta_0\| < \delta_n} E[(m_s(X, a, R(a), \theta) - m_s(X, a, R(a), \theta_0))^2]~.
\end{align*}
Assumption \ref{ass:normal}(c) implies
\begin{equation} \label{eq:E-small}
\mathbb C_{3, n} = \sup_{\theta \in \Theta^\ast: \|\theta - \theta_0\| < \delta_n} E[(m_s(X, a, R(a), \theta) - m_s(X, a, R(a), \theta_0))^2] \to 0~.
\end{equation}
Next, Assumption \ref{ass:normal}(b), (f) and similar arguments to those used to show \eqref{eq:gc-2} and \eqref{eq:gc-3} are $o_P(1)$ imply
\begin{multline} \label{eq:gcsquare-uncond}
   \mathbb C_{2, n} = \sup_{\theta \in \Theta^\ast}~ \bigg | E \bigg [ \int (m_s(X, a, R(a), \theta) - m_s(X, a, R(a), \theta_0))^2 d \tilde P_n \bigg | X^{(n)}, A^{(n)} \bigg ] \\
   - E[(m_s(X, a, R(a), \theta) - m_s(X, a, R(a), \theta_0))^2] \bigg | \stackrel{P}{\to} 0~.
\end{multline}
Further define
\[ \mathcal G = \{(m_s(x, a, r(a), \theta) - m_s(x, a, r(a), \theta_0))^2:\theta \in \Theta^\ast\}~. \]
We then study $\mathbb C_{1, n}$. We will establish for every $\xi > 0$,
\begin{equation} \label{eq:gcsquare-cond}
    P \bigg \{ \sup_{f \in \mathcal G} \Big| \int f d \tilde P_n - E \Big [ \int f d \tilde P_n \Big | X^{(n)}, A^{(n)} \Big ] \Big | > \xi \bigg | X^{(n)}, A^{(n)} \bigg \} \stackrel{P}{\to} 0
\end{equation}
as $n \to \infty$. It follows the symmetrization Lemma 6.2 in \cite{ledoux1991probability} applied conditional on $X^{(n)}, A^{(n)}$ for the distribution
\[ \bigotimes_{1 \leq i \leq n: A_i = 1} P \{X_i, R_i(1) | X_i\}~ \]
that
\begin{multline} \label{eq:symsquare}
E \bigg [ \sup_{f \in \mathcal G} \Big| \int f d \tilde P_n - E \Big [ \int f d \tilde P_n \Big | X^{(n)}, A^{(n)} \Big ] \Big | \bigg | X^{(n)}, A^{(n)} \bigg ] \\
\leq 2E_P \bigg [ E_\tau \bigg [ \sup_{f \in \mathcal G}~ \Big | \frac{1}{\eta_a n} \sum_{1 \leq i \leq n} \tau_i f(X_i, R_i(a)) \Big | \bigg ] \bigg | X^{(n)}, A^{(n)} \bigg ]~,
\end{multline}
where $E_\tau[\cdot]$ should be understood as the expectation with respect to $(\tau_i, 1 \le i \le n)$, holding all else fixed. Note Assumption \ref{ass:consistency}(b) and Theorem 2.6.7 in \cite{van_der_vaart1996weak} imply $\mathcal F_{\theta_0, \infty}$ is totally bounded in $L_2(\tilde P_n)$. Accordingly, for $\epsilon > 0$, let $N(\epsilon, \mathcal F_{\theta_0, \infty}, L_2(\tilde P_n))$ denote the covering number of $\mathcal F_{\theta_0, \infty}$ with respect to $L_2(\tilde P_n)$. Let $f_1, f_2$ be any pair of functions in $\mathcal G$, where we denote
\[ f_j = (m_s(x, a, r(a), \theta_j) - m_s(x, a, r(a), \theta_0))^2, ~ j = 1, 2~, \]
then the Cauchy-Schwarz inequality implies
\[ \int |f_1 - f_2| d \tilde P_n \leq \int |m_s(x, a, r(a), \theta_1) - m_s(x, a, r(a), \theta_2)| 2 F d \tilde P_n \leq \|m_s(\cdot, \theta_1) - m_s(\cdot, \theta_2)\|_{\tilde P_n} 2 \|F\|_{\tilde P_n} \]
where $\|\cdot\|_{\tilde P_n}$ denotes the $L_2(\tilde P_n)$-norm. Therefore
\begin{equation} \label{eq:squarecovering}
    N(2 \epsilon \|F\|_{\tilde P_n}^2, \mathcal G, L_1(\tilde P_n)) \leq N(\epsilon \|F\|_{\tilde P_n}, \mathcal F_{\theta_0, \infty}, L_2(\tilde P_n)).
\end{equation}
For every $\epsilon > 0$, the right-hand side is uniformly bounded across $n$ by Assumption \ref{ass:consistency}(b) and Theorem 2.6.7 in \cite{van_der_vaart1996weak}. Note it follows from Assumptions \ref{ass:a}--\ref{ass:pair}, \ref{ass:normal}(b), (f), and similar arguments to those in the first part of the proof of Lemma S.1.5 in \cite{bai2022inference} that
\begin{equation} \label{eq:Flb}
    E \big [ \|F\|_{\tilde P_n}^2 | X^{(n)}, A^{(n)} \big ] = \frac{1}{\eta_a n} \sum_{1 \leq i \leq n} I \{A_i = a\} E[F^2 | X_i] \stackrel{P}{\to} E[F^2]~.
\end{equation}
We can assume without loss of generality $E[F^2] > 0$ because otherwise $m_s(x, a, r(a), \theta) \equiv 0$. Therefore,
\begin{equation} \label{eq:Flb-P}
    P \Big \{ E \big [ \|F\|_{\tilde P_n}^2 | X^{(n)}, A^{(n)} \big ] \leq \frac{1}{2} E[F^2] \Big \} \to 0~.
\end{equation}
On the other hand, Assumptions \ref{ass:a}--\ref{ass:pair}, \ref{ass:normal}(b), (f), and similar arguments to those in the last part of the proof of Lemma S.1.5 in \cite{bai2022inference} that
\begin{equation} \label{eq:Flb-condP}
    P \Big \{ \Big | \|F\|_{\tilde P_n}^2 - E \big [ \|F\|_{\tilde P_n}^2 | X^{(n)}, A^{(n)} \big ] \Big | > \frac{1}{4} E[F^2] \Big | X^{(n)}, A^{(n)} \Big \} \stackrel{P}{\to} 0~.
\end{equation}
\eqref{eq:gcsquare-cond} now follows from \eqref{eq:Flb-P}--\eqref{eq:Flb-condP} and similar arguments to those used in the last step of the proof of Lemma \ref{lem:consistency}.

To conclude \eqref{eq:zeta} holds, note $\mathbb C_{3, n}$ is a sequence of constants and $\mathbb C_{2, n}$ is a function of $X^{(n)}, A^{(n)}$, and hence
\begin{align*}
    & P \big \{ P \big \{ \zeta_n^2 > \xi | X^{(n)}, A^{(n)} \big \} > \epsilon \big \} \\
    & \leq P \Big \{ \mathbb C_{2, n} > \frac{\xi}{3} \Big \} + P \Big \{ \mathbb C_{3, n} > \frac{\xi}{3} \Big \} \\
    & \hspace{3em} + P \Big \{ P \big \{ \mathbb C_{1, n} + \mathbb C_{2, n} + \mathbb C_{3, n} > \xi | X^{(n)}, A^{(n)} \big \} > \epsilon, \mathbb C_{2, n} \leq \frac{\xi}{3}, \mathbb C_{3, n} \leq \frac{\xi}{3} \Big \} \\
    & \leq P \Big \{ \mathbb C_{2, n} > \frac{\xi}{3} \Big \} + P \Big \{ \mathbb C_{3, n} > \frac{\xi}{3} \Big \} + P \Big \{ P \Big \{ \mathbb C_{1, n} > \frac{\xi}{3} \Big | X^{(n)}, A^{(n)} \Big \} > \epsilon \Big \} \\
    & \stackrel{P}{\to} 0~,
\end{align*}
where the convergence follows from \eqref{eq:E-small}, \eqref{eq:gcsquare-uncond}, and \eqref{eq:gcsquare-cond}.

\noindent \textbf{Step 3(b)}. Next, we show for every $\xi > 0$,
\begin{equation} \label{eq:integral-limit}
    P \bigg \{\frac{\zeta_n^2}{\|F\|_{\tilde P_n}^2} > \xi \bigg| X^{(n)}, A^{(n)} \bigg \} \overset{P}{\to} 0~.
\end{equation}
For every $\epsilon > 0$,
\begin{align*}
    & P \bigg \{ P \bigg \{\frac{\zeta_n^2}{\|F\|_{\tilde P_n}^2} > \xi \bigg| X^{(n)}, A^{(n)} \bigg \} > \epsilon \bigg \} \\
    & \leq P \Big \{ E \big [ \|F\|_{\tilde P_n}^2 | X^{(n)}, A^{(n)} \big ] \leq \frac{1}{2} E[F^2] \Big \} \\
    & \hspace{3em} + P \bigg \{ P \bigg \{\frac{\zeta_n^2}{\|F\|_{\tilde P_n}^2} > \xi \bigg| X^{(n)}, A^{(n)} \bigg \} > \epsilon, E \big [ \|F\|_{\tilde P_n}^2 | X^{(n)}, A^{(n)} \big ] > \frac{1}{2} E[F^2] \bigg \} \\
    & \leq P \Big \{ E \big [ \|F\|_{\tilde P_n}^2 | X^{(n)}, A^{(n)} \big ] \leq \frac{1}{2} E[F^2] \Big \} \\
    & + P \bigg \{ P \bigg \{ \{\zeta_n^2 > \frac{1}{4} \xi E[F^2]\} \cup \{| \|F\|_{\tilde P_n}^2 - E \big [ \|F\|_{\tilde P_n}^2 | X^{(n)}, A^{(n)} \big ] | > \frac{1}{4} E[F^2]\} \bigg| X^{(n)}, A^{(n)} \bigg \} > \epsilon~, \\
    & \hspace{5em} E \big [ \|F\|_{\tilde P_n}^2 | X^{(n)}, A^{(n)} \big ] > \frac{1}{2} E[F^2] \bigg \} \\
    & \leq P \Big \{ E \big [ \|F\|_{\tilde P_n}^2 | X^{(n)}, A^{(n)} \big ] \leq \frac{1}{2} E[F^2] \Big \} \\
    & \hspace{3em} +  P \Big \{ P \Big \{ \zeta_n^2 > \frac{1}{4} \xi E[F^2] \Big | X^{(n)}, A^{(n)} \Big \} > \frac{\epsilon}{2} \Big \} \\
    & \hspace{3em} + P \Big \{ P \Big \{ \Big | \|F\|_{\tilde P_n}^2 - E \big [ \|F\|_{\tilde P_n}^2 | X^{(n)}, A^{(n)} \big ] \Big | > \frac{1}{4} E[F^2] \Big | X^{(n)}, A^{(n)} \Big \} > \frac{\epsilon}{2} \Big \}
    \stackrel{P}{\to} 0~,
\end{align*}
where we use the fact that $E \big [ \|F\|_{\tilde P_n}^2 | X^{(n)}, A^{(n)} \big ]$ is a function of $X^{(n)}, A^{(n)}$ and the convergence follows from \eqref{eq:zeta} and \eqref{eq:Flb-P}--\eqref{eq:Flb-condP}.

\noindent \textbf{Step 3(c)}. Fix $\epsilon > 0$. Following almost verbatim the first part of the proof of Theorem 2.5.2 in \cite{van_der_vaart1996weak}, with $\tilde P_n$ replacing the empirical measure, we obtain
\begin{multline} \label{eq:l1-cs}
P \bigg \{\sup_{\theta \in \Theta^\ast: \|\theta - \theta_0\| < \delta_n} |\mathbb B_n(\theta) - \mathbb B_n(\theta_0)| > \epsilon \bigg | X^{(n)}, A^{(n)} \bigg \} \\
\leq \frac{1}{\epsilon} E \bigg [ \Big ( \int_0^{\frac{\zeta_n}{\|F\|_{\tilde P_n}}} \sup_\nu \sqrt{\log N(\epsilon \|F\|_\nu, \mathcal F_{\theta_0, \infty}, L_2(\nu))} d \epsilon \Big )^2 \bigg | X^{(n)}, A^{(n)} \bigg ]^{1/2} E \big [ \|F\|_{\tilde P_n}^2 | | X^{(n)}, A^{(n)} \big ]^{1/2}~,
\end{multline}
where the supremum for $\nu$ is over probability measures with discrete supports. Also note that if $\|F\|_{\tilde P_n} = 0$ then the conditional expectation on the left-hand side is trivially zero, so we can without loss of generality assume $\|F\|_{\tilde P_n} > 0$. Assumption \ref{ass:consistency}(b) implies
\begin{multline} \label{eq:l1-condE-finite}
E \bigg [ \Big ( \int_0^{\frac{\zeta_n}{\|F\|_{\tilde P_n}}} \sup_\nu \sqrt{\log N(\epsilon \|F\|_\nu, \mathcal F_{\theta_0, \infty}, L_2(\nu))} d \epsilon \Big )^2 \bigg | X^{(n)}, A^{(n)} \bigg ] \\
\leq \Big ( \int_0^\infty \sup_\nu \sqrt{\log N(\epsilon \|F\|_\nu, \mathcal F_{\theta_0, \infty}, L_2(\nu))} d \epsilon \Big )^2 < \infty~.
\end{multline}
%We now argue by contradiction that for every $\eta > 0$,
%\begin{equation} \label{eq:l1-condE-conv}
%    \lim_{n \to \infty} P \bigg \{ E \bigg [ \Big ( \int_0^{\frac{\zeta_n}{\|F\|_{\tilde P_n}}} \sup_\nu \sqrt{\log N(\epsilon \|F\|_\nu, \mathcal F_{\theta_0, \infty}, L_2(\nu))} d \epsilon \Big )^2 \bigg | X^{(n)}, A^{(n)} \bigg ] > \eta \bigg \} = 0
%\end{equation}


We now argue
\begin{equation} \label{eq:l1-condE-conv}
E \bigg [ \Big ( \int_0^{\frac{\zeta_n}{\|F\|_{\tilde P_n}}} \sup_\nu \sqrt{\log N(\epsilon \|F\|_\nu, \mathcal F_{\theta_0, \infty}, L_2(\nu))} d \epsilon \Big )^2 \bigg | X^{(n)}, A^{(n)} \bigg ] \overset{P}{\to} 0 ~.
\end{equation}
Note the last inequality in \eqref{eq:l1-condE-finite} and the dominated convergence theorem implies that for every $\epsilon > 0$, there exists a $\xi > 0$ such that
\begin{equation} \label{eq:integral-small}
    \Big ( \int_0^\xi \sup_\nu \sqrt{\log N(\epsilon \|F\|_\nu, \mathcal F_{\theta_0, \infty}, L_2(\nu))} d \epsilon \Big )^2 < \epsilon~.
\end{equation}
Then consider the following decomposition:
\begin{align*}
    & E \bigg [ \Big ( \int_0^{\frac{\zeta_n}{\|F\|_{\tilde P_n}}} \sup_\nu \sqrt{\log N(\epsilon \|F\|_\nu, \mathcal F_{\theta_0, \infty}, L_2(\nu))} d \epsilon \Big )^2 \bigg | X^{(n)}, A^{(n)} \bigg ] \\
    & = E \bigg [ \Big ( \int_0^{\frac{\zeta_n}{\|F\|_{\tilde P_n}}} \sup_\nu \sqrt{\log N(\epsilon \|F\|_\nu, \mathcal F_{\theta_0, \infty}, L_2(\nu))} d \epsilon \Big )^2 I \Big \{ \frac{\zeta_n}{\|F\|_{\tilde P_n}} \leq \xi \Big \} \bigg | X^{(n)}, A^{(n)} \bigg ] \\
    & \hspace{3em} + E \bigg [ \Big ( \int_0^{\frac{\zeta_n}{\|F\|_{\tilde P_n}}} \sup_\nu \sqrt{\log N(\epsilon \|F\|_\nu, \mathcal F_{\theta_0, \infty}, L_2(\nu))} d \epsilon \Big )^2 I \Big \{ \frac{\zeta_n}{\|F\|_{\tilde P_n}} > \xi \Big \} \bigg | X^{(n)}, A^{(n)} \bigg ]\\
    & \lesssim \epsilon + P \bigg \{\frac{\zeta_n}{\|F\|_{\tilde P_n}} > \xi \bigg| X^{(n)}, A^{(n)} \bigg \} \overset{P}{\to} \epsilon~,
\end{align*}
where the inequality follows from \eqref{eq:l1-condE-finite} and \eqref{eq:integral-small} and the convergence follows from \eqref{eq:integral-limit}. Because $\epsilon > 0$ was arbitrary, \eqref{eq:l1-condE-conv} follows.

It thus follows from \eqref{eq:Flb}, \eqref{eq:l1-cs}, and \eqref{eq:l1-condE-conv} that 
\[P \bigg \{\sup_{\theta \in \Theta^\ast: \|\theta - \theta_0\| < \delta_n} |\mathbb B_n(\theta) - \mathbb B_n(\theta_0)| > \epsilon \bigg | X^{(n)}, A^{(n)} \bigg \} \overset{P}{\to} 0~.\]
By the law of iterated expectations and the dominated convergence theorem we thus obtain 
\[P \bigg \{\sup_{\theta \in \Theta^\ast: \|\theta - \theta_0\| < \delta_n} |\mathbb B_n(\theta) - \mathbb B_n(\theta_0)| > \epsilon \bigg\} \to 0~,\]
as desired. \qed
%for every $\eta > 0$,
%\begin{equation} \label{eq:l1-se-P}
%    \lim_{n \to \infty} P \bigg \{ P \bigg \{\sup_{\theta \in \Theta^\ast: \|\theta - \theta_0\| < \delta_n} |\mathbb B_n(\theta) - \mathbb B_n(\theta_0)| > \epsilon \bigg | X^{(n)}, A^{(n)} \bigg \} > \eta \bigg \} = 0~.
%\end{equation}
%To conclude the proof, we argue by contradiction that \eqref{eq:l1-se} holds. Suppose not, then there exists exists a subsequence $n_k$ along which, for some $\epsilon > 0$ and $\gamma > 0$,
%\begin{equation} \label{eq:l1-contra}
%    P \bigg \{ \sup_{\theta \in \Theta^\ast: \|\theta - \theta_0\| < \delta_{n_k}} |\mathbb B_{n_k}(\theta) - \mathbb B_{n_k}(\theta_0)| > \epsilon \bigg \} \to \gamma~.
%\end{equation}
%However, it follows from \eqref{eq:l1-se-P} that for $k$ large enough,
%\[ P \bigg \{ P \bigg \{\sup_{\theta \in \Theta^\ast: \|\theta - \theta_0\| < \delta_{n_k}} |\mathbb B_{n_k}(\theta) - \mathbb B_{n_k}(\theta_0)| > \epsilon \bigg | X^{(n_k)}, A^{(n_k)} \bigg \} > \frac{\gamma}{3} \bigg \} < \frac{\gamma}{3}~. \]
%An application of the law of total expectation then implies that for $k$ large %enough
%\[ P \bigg \{ \sup_{\theta \in \Theta^\ast: \|\theta - \theta_0\| < \delta_{n_k}} |\mathbb B_{n_k}(\theta) - \mathbb B_{n_k}(\theta_0)| > \epsilon %\bigg \} < \frac{2}{3} \gamma~, \]
%a contradiction to \eqref{eq:l1-contra}. Therefore, \eqref{eq:l1-se} holds.
% It follows from Lemma G.1 in \cite{jiang2021bootstrap} that
% \begin{align*}
% & E \bigg [ \sup_{\|\theta - \theta_0\| < \delta} \Big | \frac{1}{\sqrt n} \sum_{1 \leq i \leq n} I \{A_i = a\} (m(X_i, a, R_i(a), \theta) - E_Q[m(X_i, a, R_i(a), \theta) | X_i]) \Big | \bigg | X^{(n)}, A^{(n)} \bigg ] \\
% & \lesssim \sqrt{v \zeta_n^2 \log(a \|F\|_{\tilde P_n} / \zeta_n)} + \frac{v E[\max_{1 \leq i \leq n} F^2(X_i)]^{1/2}}{\sqrt n} \log(a \|F\|_{\tilde P_n} / \zeta_n)
% \end{align*}

\subsection{Auxiliary Lemmas}
\begin{lemma} \label{lem:marginal}
Suppose \eqref{eq:unconfounded} holds and $\mathrm{Pr} \{A_i = 1 | X_i = x\}$ as a function is identical across $1 \leq i \leq n$. Then,
\begin{equation} \label{eq:marginal-indep}
(R_i(1), R_i(0)) \indep A_i | X_i~.
\end{equation}
Moreover, $(X_i, A_i, R_i)$ is identically distributed across $1 \leq i \leq n$.
\end{lemma}

\begin{proof}
Fix $a \in \{0, 1\}$ and any Borel sets $B \in \mathbf R^{d_r} \times \mathbf R^{d_r}$ and $C \in \mathbf R^{d_x}$.
\begin{align*}
& E[\mathrm{Pr} \{(R_i(1), R_i(0)) \in B, A_i = a | X_i\} I \{X_i \in C\}] \\
& = E[E[\mathrm{Pr} \{(R_i(1), R_i(0)) \in B, A_i = a | X^{(n)}\} | X_i] I \{X_i \in C\}] \\
& = E[E[\mathrm{Pr} \{(R_i(1), R_i(0)) \in B | X^{(n)}\} \mathrm{Pr} \{A_i = a | X^{(n)}\} | X_i] I \{X_i \in C\}] \\
& = E[\mathrm{Pr} \{(R_i(1), R_i(0)) \in B | X_i\} \mathrm{Pr} \{A_i = a | X_i\} I \{X_i \in C\}]~,
\end{align*}
where the first equality follows from the law of iterated expectations, the second equality follows from \eqref{eq:unconfounded}, the third equality follows from the law of iterated expectation as well as the facts that $Q_n = Q^n$ and $\mathrm{Pr} \{A_i = 1 | X_i = x\}$ as a function is identical across $1 \leq i \leq n$ . The first statement of the lemma then follows from the definition of a conditional expectation.

To prove the second statement, fix units $i$ and $i'$. Clearly $X_i$ and $X_{i'}$ are identically distributed. Conditional on $X_i$, for any Borel set $C \in \mathbf R^{d_r}$ and $a \in \{0, 1\}$, it follows (a) that
\[ \mathrm{Pr} \{R_i \in C, A_i = a | X_i\} = \mathrm{Pr} \{A_i = a | X_i\} \mathrm{Pr} \{R_i(a) \in C | X_i\}~. \]
The conclusion then follows because $\mathrm{Pr} \{A_i = 1 | X_i = x\}$ is identical across $1 \leq i \leq n$ and $Q_n = Q^n$.
\end{proof}

\begin{lemma} \label{lem:consistency}
Suppose the treatment assignment mechanism satisfies Assumptions \ref{ass:a}--\ref{ass:pair} and the moment functions satisfy Assumption \ref{ass:consistency}. Then, $\hat \theta_n \stackrel{P}{\to} \theta_0$.
\end{lemma}
\begin{proof}[\sc Proof of Lemma \ref{lem:consistency}]
It follows from Assumption \ref{ass:consistency}(a) and Theorem 5.9 in \cite{van_der_vaart1998asymptotic} that we only need to establish for each $1 \leq s \leq d_\theta$,
\begin{equation} \label{eq:uc}
\sup_{\theta \in \Theta}~ \bigg | \frac{1}{n} \sum_{1 \leq i \leq n} m_s(X_i, A_i, R_i, \theta) - E_P[m_s(X_i, A_i, R_i, \theta)] \bigg | \stackrel{P}{\to} 0~.
\end{equation}
To begin, note it follows from Assumption \ref{ass:consistency}(d) and the dominated convergence theorem that if $m_s(x, a, r, \theta_m) \allowbreak \to m_s(x, a, r, \theta)$ as $m \to \infty$ for $\{\theta_m\} \subset \Theta^\ast$, then $E_P[m_s(X_i, A_i, R_i, \theta_m)] \to E_P[m_s(X_i, A_i, R_i, \theta)]$. Assumption \ref{ass:consistency}(c) then implies
\begin{multline} \label{eq:countable}
     \sup_{\theta \in \Theta}~ \bigg | \frac{1}{n} \sum_{1 \leq i \leq n} m_s(X_i, A_i, R_i, \theta) - E_P[m_s(X_i, A_i, R_i, \theta)] \bigg | \\
     = \sup_{\theta \in \Theta^\ast}~ \bigg | \frac{1}{n} \sum_{1 \leq i \leq n} m_s(X_i, A_i, R_i, \theta) - E_P[m_s(X_i, A_i, R_i, \theta)] \bigg |~,
\end{multline}
which is measurable. Next, note that
\begin{equation} \label{eq:half-m}
m(X_i, A_i, R_i, \theta) = A_i m(X_i, 1, R_i(1), \theta) + (1 - A_i) m(X_i, 0, R_i(0), \theta)~.
\end{equation}
and it follows from Lemma \ref{lem:marginal} that
\begin{equation} \label{eq:half-E}
E_P[m(X_i, A_i, R_i, \theta)] = \frac{\ell}{k} E_Q[m(X_i, 1, R_i(1), \theta)] + \frac{k - \ell}{k} E_Q[m(X_i, 0, R_i(0), \theta)]~,
\end{equation} which implies that
\begin{align*}
& \sup_{\theta \in \Theta^\ast}~ \bigg | \frac{1}{n} \sum_{1 \leq i \leq n} m_s(X_i, A_i, R_i, \theta) - E_P[m_s(X_i, A_i, R_i, \theta)] \bigg | \\
& \le \sup_{\theta \in \Theta^\ast}~ \bigg | \frac{1}{n} \sum_{1 \leq i \leq n} A_i m_s(X_i, 1, R_i(1), \theta) - \frac{\ell}{k} E[m_s(X_i, 1, R_i(1), \theta)] \bigg | \\
& \hspace{5em} + \sup_{\theta \in \Theta^\ast}~ \bigg | \frac{1}{n} \sum_{1 \leq i \leq n} (1 - A_i) m_s(X_i, 0, R_i(0), \theta) - \frac{k - \ell}{k} E[m_s(X_i, 0, R_i(0), \theta)] \bigg |~.
\end{align*}
We study the first term on the right-hand side and similar arguments apply to the second term.
\begin{align}
\nonumber & \sup_{\theta \in \Theta^\ast}~ \bigg | \frac{1}{n} \sum_{1 \leq i \leq n} A_i m_s(X_i, 1, R_i(1), \theta) - \frac{\ell}{k} E[m_s(X_i, 1, R_i(1), \theta)] \bigg | \\
\label{eq:gc-1} & \leq \sup_{\theta \in \Theta^\ast}~ \bigg | \frac{1}{n} \sum_{1 \leq i \leq n} A_i (m_s(X_i, 1, R_i(1), \theta) - E[m_s(X_i, 1, R_i(1), \theta) | X_i]) \bigg | \\
\label{eq:gc-2} & \hspace{5em} + \sup_{\theta \in \Theta^\ast}~ \bigg | \frac{1}{n} \sum_{1 \leq i \leq n} (A_i - \frac{\ell}{k}) E[m_s(X_i, 1, R_i(1), \theta) | X_i] \bigg | \\
\label{eq:gc-3} & \hspace{5em} + \sup_{\theta \in \Theta^\ast}~ \bigg | \frac{\ell}{kn} \sum_{1 \leq i \leq n} (E[m_s(X_i, 1, R_i(1), \theta) | X_i] - E[m_s(X_i, 1, R_i(1), \theta)]) \bigg |~.
\end{align}
We study each term separately. First note \eqref{eq:gc-2} is bounded by
\[ \sup_{\theta \in \Theta^\ast}~ \frac{1}{n} \sum_{1 \leq j \leq n / k} |\zeta^\ast_j(\theta)|~,\]
where 
\[\zeta^\ast_j(\theta) = \sup_{I \subset \lambda_j}\left\{\frac{k - \ell}{k}\sum_{i \in I}E[m_s(X_i, 1, R_i(1), \theta)|X_i] - \frac{\ell}{k}\sum_{i\in\lambda_j \setminus I}E[m_s(X_i, 1, R_i(1), \theta)|X_i]: |I| = \ell\right\}~.\]
By Assumption \ref{ass:consistency}(e) and Assumption \ref{ass:pair} we then obtain
\[\sup_{\theta \in \Theta^\ast}~ \frac{1}{n} \sum_{1 \leq j \leq n / k} |\zeta^\ast_j(\theta)| \lesssim \frac{1}{n}\sum_{1 \leq j \leq n / k}\max_{i, i' \in \lambda_j} \|X_i - X_{i'}\| \stackrel{P}{\to} 0~.\]
For \eqref{eq:gc-3}, note the class of functions 
\[ \{E[m_s(X, 1, R(1), \theta) | X = x]: \theta \in \Theta^\ast\} \]
are Lipschitz continuous in $x$ with a uniform Lipschitz constant. It therefore follows from Corollary 4.1 in \cite{van_der_vaart1994bracketing}, applied with $I_j$s being hypercubes, that \eqref{eq:gc-3} converges in probability to $0$.

To analyze \eqref{eq:gc-1}, we apply the arguments in the proof of Theorem 2.4.3 in \cite{van_der_vaart1996weak} conditional on $X^{(n)}, A^{(n)}$. Define $F = \sup_{\theta \in \Theta^\ast} m_s(X, 1, R(1), \theta)$, which is measurable because $\Theta^\ast$ is countable by Assumption \ref{ass:consistency}(c). Define for any $K > 0$
\[ \mathcal F_s^K(1) = \{m_s(X, 1, R(1), \theta) I \{F \leq K\}: \theta \in \Theta^\ast\}~, \]
and 
\[ \mathcal F_s(1) = \{m_s(X, 1, R(1), \theta) : \theta \in \Theta^\ast\}~.\]
Next, let $\tau_i, 1 \leq i \leq n$ be a sequence of i.i.d.\ Rademacher random variables independent of all other variables. It follows from Markov's inequality and the symmetrization Lemma 6.3 in \cite{ledoux1991probability} applied conditional on $X^{(n)}, A^{(n)}$ for the distribution
\[ \bigotimes_{1 \leq i \leq n: A_i = 1} P \{X_i, R_i(1) | X_i\}~ \]
that for every $\epsilon > 0$,
\begin{align}
& P \bigg \{ \sup_{\theta \in \Theta^\ast}~ \Big | \frac{1}{n} \sum_{1 \leq i \leq n} A_i (m_s(X_i, 1, R_i(1), \theta) - E[m_s(X_i, 1, R_i(1), \theta) | X_i]) \Big | > \epsilon \bigg | X^{(n)}, A^{(n)} \bigg \} \\
\nonumber & \leq \frac{1}{\epsilon} E \bigg [ \sup_{\theta \in \Theta^\ast}~ \Big | \frac{1}{n} \sum_{1 \leq i \leq n} A_i (m_s(X_i, 1, R_i(1), \theta) - E[m_s(X_i, 1, R_i(1), \theta) | X_i]) \Big |~ \bigg | X^{(n)}, A^{(n)} \bigg ] \\
\nonumber & \leq \frac{2}{\epsilon} E_P \bigg [ E_\tau \bigg [ \sup_{\theta \in \Theta^\ast}~ \Big | \frac{1}{n} \sum_{1 \leq i \leq n} \tau_i A_i m_s(X_i, 1, R_i(1), \theta) \Big | \bigg ] \bigg | X^{(n)}, A^{(n)} \bigg ] \\
\nonumber & \leq \frac{2}{\epsilon} E_P \bigg [ E_\tau \bigg [ \sup_{\theta \in \Theta^\ast}~ \Big | \frac{1}{n} \sum_{1 \leq i \leq n} \tau_i A_i \min\{m_s(X_i, 1, R_i(1), \theta), K\} \Big | \bigg ] \bigg | X^{(n)}, A^{(n)} \bigg ] \\
\label{eq:gc-sym} & \hspace{5em} + 2 E[F I \{F > K\}]~,
\end{align}
where $E_\tau[\cdot]$ should be understood as the expectation with respect to $(\tau_i, 1 \le i \le n)$, holding all else fixed. The last term could be made as small as possible by choosing $K$ large because of Assumption \ref{ass:consistency}(d). Next, define
\[ \tilde P_n = \frac{1}{\eta n} \sum_{1 \leq i \leq n: A_i = 1} \delta_{(X_i, R_i(1))}~, \]
where $\delta$ denotes the Dirac measure. Note that $\mathcal{F}_s(1)$ is a VC class by Assumption \ref{ass:consistency}(b) and so $\mathcal{F}_s^K(1)$ is a VC-class by Lemma 2.6.18(vi) in \cite{van_der_vaart1996weak}, and thus both totally bounded in $L_1(\tilde{P}_n)$ by Theorem 2.6.7 in \cite{van_der_vaart1996weak} (note that if $\|F\|_{L_1(\tilde P_n)} = 0$ then the conditional expectation immediately below is trivially zero, so we can without loss of generality assume $\|F\|_{L_1(\tilde P_n)} > 0$). Accordingly, define $\mathcal{G}$ to be an $\epsilon$-net in $L_1(\tilde P_n)$ for $\mathcal F_s^K(1)$ with cardinality $N(\epsilon, \mathcal{F}_s^K(1), L_1(\tilde P_n))$. We have
\begin{align}
\nonumber & E_\tau \bigg [ \sup_{\theta \in \Theta^\ast}~ \Big | \frac{1}{\eta n} \sum_{1 \leq i \leq n} \tau_i A_i \min\{m_s(X_i, 1, R_i(1), \theta), K\} \Big | \bigg ] \\
\nonumber & \leq E_\tau \bigg [ \sup_{f \in \mathcal G} \Big | \frac{1}{\eta n} \sum_{1 \leq i \leq n} \tau_i A_i f(X_i, R_i(1)) \Big | \bigg ] + \epsilon \\
\nonumber & \leq \sqrt{1 + \log N(\epsilon, \mathcal{F}_s^K(1), L_1(\tilde P_n))} \sqrt{\frac{12}{n}} \sup_{f \in \mathcal G} \left ( \int f^2 d \tilde P_n \right )^{1/2} + \epsilon \\
\nonumber & \leq ( \sqrt{1 + \log N(\epsilon, \mathcal{F}_s^K(1), L_1(\tilde P_n))} \sqrt{\frac{12}{n}} K + \epsilon \\
\nonumber & \leq ( \sqrt{1 + \log N(\epsilon, \mathcal{F}_s(1), L_1(\tilde P_n))} \sqrt{\frac{12}{n}} K + \epsilon \\
\label{eq:gc-max} & \lesssim \left ( \sqrt{1 + (V - 1)\log \bigg ( \frac{\|F\|_{L_1(\tilde P_n)}}{\epsilon} \bigg )} \sqrt{\frac{12}{n}} K + \epsilon \right ) \wedge K~,
\end{align}
where the second inequality follows from Lemma 2.2.2 in \cite{van_der_vaart1996weak} applied with $\exp(x^2) - 1$ and Hoeffding's lemma, the third follows because $|f| \leq K$ for $f \in \mathcal G$, the fourth inequality follows from the fact that $N(\epsilon, \mathcal F_s^K(1), L_1(\tilde P_n)) \leq N(\epsilon, \mathcal F_s(1), L_1(\tilde P_n))$ because
\begin{equation} \label{eq:covering-K}
    \int |f_1 I \{F \leq K\}  - f_2 I \{F \leq K\}| d \tilde P_n = \int |f_1 - f_2| I \{F \leq K\} d \tilde P_n \leq \int |f_1 - f_2| d \tilde P_n~, 
\end{equation}
and the last inequality follows from Theorem 2.6.7 in \cite{van_der_vaart1996weak} and Assumption \ref{ass:consistency}(b).

Note it follows from Assumptions \ref{ass:a}--\ref{ass:pair}, \ref{ass:consistency}(d)--(f), and similar arguments to those in the first part of the proof of Lemma S.1.5 in \cite{bai2022inference} that 
\begin{equation} \label{eq:Fl1E}
    E \big [ \|F\|_{L_1(\tilde P_n)} | X^{(n)}, A^{(n)} \big ] = \frac{1}{\eta n} \sum_{1 \leq i \leq n} I \{A_i = 1\} E[|F| | X_i] \stackrel{P}{\to} E[|F|]~.
\end{equation}
We can assume without loss of generality $E[|F|] > 0$ because otherwise $m_s(x, 1, r(1), \theta) \equiv 0$. Therefore,
\begin{equation} \label{eq:Fl1P}
    P \Big \{ E \big [ \|F\|_{L_1(\tilde P_n)} | X^{(n)}, A^{(n)} \big ] > E[|F|] + \frac{E[|F|]}{2} \Big \} \to 0~.
\end{equation}
On the other hand, Assumptions \ref{ass:a}--\ref{ass:pair}, \ref{ass:consistency}(d)--(f) and similar arguments to those in the last part of the proof of Lemma S.1.5 in \cite{bai2022inference} that
\begin{equation} \label{eq:Fl1cond}
    P \Big \{ \Big | \|F\|_{L_1(\tilde P_n)} - E \big [ \|F\|_{L_1(\tilde P_n)} | X^{(n)}, A^{(n)} \big ] \Big | > \frac{E[|F|]}{2}\Big | X^{(n)}, A^{(n)} \Big \} \stackrel{P}{\to} 0~.
\end{equation}
Let 
\[\mathbb L_n = P\left\{\sup_{\theta \in \Theta^\ast}~ \bigg | \frac{1}{n} \sum_{1 \leq i \leq n} A_i (m_s(X_i, 1, R_i(1), \theta) - E[m_s(X_i, 1, R_i(1), \theta) | X_i]) \bigg| > \epsilon \Big |X^{(n)}, A^{(n)}\right\}~.\] 
To conclude the proof, note for every $\eta > 0$, for $n$ large enough, 
\begin{align*}
    & P \left \{ \mathbb L_n > \eta \right \} \\
    & \leq P \Big \{ E \big [ \|F\|_{L_1(\tilde P_n)} | X^{(n)}, A^{(n)} \big ] > \frac{3}{2}E[|F|] \Big \} \\
    & \hspace{5em} + P \Big \{ \mathbb L_n > \eta, E \big [ \|F\|_{L_1(\tilde P_n)} | X^{(n)}, A^{(n)} \big ] \leq \frac{3}{2}E[|F|]  \Big \} \\
    & \leq P \Big \{ E \big [ \|F\|_{L_1(\tilde P_n)} | X^{(n)}, A^{(n)} \big ] > \frac{3}{2}E[|F|]  \Big \} \\
    & \hspace{5em} + P \Bigg \{ E \left [ \left ( \sqrt{1 + (V - 1)\log \bigg ( \frac{\|F\|_{L_1(\tilde P_n)}}{\epsilon} \bigg )} \sqrt{\frac{12}{n}} K + \epsilon \right ) \wedge K \bigg | X^{(n)}, A^{(n)} \right ] > \eta'~, \\
    & \hspace{10em} E \big [ \|F\|_{L_1(\tilde P_n)} | X^{(n)}, A^{(n)} \big ] \leq \frac{3}{2}E[|F|]  \Bigg \} \\
    & \leq P \Big \{ E \big [ \|F\|_{L_1(\tilde P_n)} | X^{(n)}, A^{(n)} \big ] >  \frac{3}{2}E[|F|]  \Big \}  \\
    & \hspace{5em} + P \Big \{ P \Big \{ \Big | \|F\|_{L_1(\tilde P_n)} - E \big [ \|F\|_{L_1(\tilde P_n)} | X^{(n)}, A^{(n)} \big ] \Big | > \frac{E[|F|]}{2} \Big | X^{(n)}, A^{(n)} \Big \} > \eta'', \\ 
    & \hspace{10em} E \big [ \|F\|_{L_1(\tilde P_n)} | X^{(n)}, A^{(n)} \big ] \leq \frac{3}{2}E[|F|] \Big \} \stackrel{P}{\to} 0~,
\end{align*}
where $\eta', \eta''$ are suitably chosen constants, the last line follows from the law of total expectation combined with the fact that the quantity in the expectation is bounded by $K$, and the convergence follows from \eqref{eq:Fl1P}--\eqref{eq:Fl1cond}.
\end{proof}

\begin{lemma} \label{lem:buei}
For $f: \mathbf R^{d_x} \to \mathbf R$, define
\begin{align*}
    h_f(x_1, \dots, x_k) & = \sup_{I \subset \{1, \dots, k\}, |I| = \ell} \Big ( \frac{1}{\ell}\sum_{i \in I}f(x_i) - \frac{1}{k - \ell}\sum_{i \in \{1, \dots, k\} \backslash I}f(x_i) \Big ) \\
    & \hspace{3em} - \inf_{I \subset \{1, \dots, k\}, |I| = \ell} \Big ( \frac{1}{\ell}\sum_{i \in I}f(x_i) - \frac{1}{k - \ell}\sum_{i \in \{1, \dots, k\} \backslash I}f(x_i) \Big )~.
\end{align*}
Then,
\[ |h_f(x_1, \dots, x_k) - h_g(x_1, \dots, x_k)|^2 \lesssim \sum_{1 \leq i \leq k} |f(x_i) - g(x_i)|^2 \]
\end{lemma}

\begin{proof}
Suppose the supremum and infimum in the definition of $h_f$ are attained at $I^\ast$ and $I_\ast$. Then,
\begin{align*}
    h_f - h_g & \leq \Big ( \frac{1}{\ell}\sum_{i \in I^\ast}f(x_i) - \frac{1}{k - \ell}\sum_{i \in \{1, \dots, k\} \backslash I^\ast}f(x_i) \Big ) - \Big ( \frac{1}{\ell}\sum_{i \in I^\ast}g(x_i) - \frac{1}{k - \ell}\sum_{i \in \{1, \dots, k\} \backslash I^\ast}g(x_i) \Big ) \\
    & \hspace{3em} + \Big ( \frac{1}{\ell}\sum_{i \in I_\ast}f(x_i) - \frac{1}{k - \ell}\sum_{i \in \{1, \dots, k\} \backslash I_\ast}f(x_i) \Big ) - \Big ( \frac{1}{\ell}\sum_{i \in I_\ast}g(x_i) - \frac{1}{k - \ell}\sum_{i \in \{1, \dots, k\} \backslash I_\ast}g(x_i) \Big )~,
\end{align*}
and the result follows from repeated applications of the inequality $(a + b)^2 \leq 2(a^2 + b^2)$.
\end{proof}

\begin{lemma} \label{lem:pdom}
If $X_n \stackrel{P}{\to} 0$ and $|X_n| \leq X$ with $E[X] < \infty$, then $E[X_n] \to 0$.
\end{lemma}

\begin{proof}
Suppose not. Then along a subsequence $\{n_k\}$, $E[|X_{n_k}|] \to \delta > 0$. Because $X_n \stackrel{P}{\to} 0$, there exists a further subsequence along which $X_{n_{k_\ell}} \to 0$ with probability one, and by the dominated convergence theorem $E[X_{n_{k_\ell}}] \to 0$, a contradiction.
\end{proof}

% \end{small}

\clearpage

\bibliography{efficiency}

\end{document}