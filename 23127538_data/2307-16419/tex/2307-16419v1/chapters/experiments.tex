\section{Experiments}
\label{sec:experiments}

We start this section by describing the datasets, architectures, and implementation details used in our experiments for both continual image classification and semantic segmentation. %
For classification problem we focus on class-incremental and task-incremental settings while in case of continual semantic segmentation problem, we only follow the class-incremental setting. %

\paragraph{\textbf{Continual Learning for Classification}}

We evaluate our proposed method on 3 different benchmark datasets: MNIST~\cite{lecun1998gradient},
CIFAR-10~\cite{krizhevsky2009learning}, %
Tiny Imagenet~\cite{tinyimagenet}. 
Following DER settings, To quantitatively demonstrate the effectiveness of our proposed subspace distillation method in different tasks settings, we split the MNIST, and CIFAR-10 datasets into sequences of 5 tasks having 2 classes per task. Tiny Imagenet is split in 10 tasks with equal number of classes in all sequential tasks (20 classes per task).
In our comparative analysis we consider eight state-of-the-art regularization and distillation methods including LwF, oEWC, SI, iCARL, A-GEM, ER, DER and DER++. In our experiment, we follow class-incremental (CI) and task-incremental (TI) protocols described in~\cite{van2019three}. The identity of task is provided along with input sample in TI setting while in case of CI setting, task identity is absent. In our experiments, we partition data into distinct sets of non-overlapping classes, hence applicable in both class-incremental and task-incremental scenarios. 
Furthermore, we maintain a consistent ordering of all classes across all algorithms, guaranteeing that each algorithm receives identical data for every task. %
Compared to the class-incremental scenario, where task identity is missing, task-incremental learning benefits from having access to task identifiers, which in return helps in selecting appropriate classifiers, rendering it a comparatively easier scenario. Conversely, the class-incremental scenario poses a challenge due to the absence of task identity.

\paragraph{\textbf{Implementation Details}}
Task-incremental learning can be implemented using either a multi-head or single-head classifier, depending on the specific implementation. Following the implementation of DER~\cite{buzzega2020dark}, in our approach, we employed a single-head classifier model to learn in class-incremental scenarios. However, in task-incremental settings, we relied on the output masking technique of the single-head classifier to identify task-specific classes, leveraging the availability of task identity during inference. The output masking technique is used to selectively mask specific outputs of the single-head classifier, depending on the task at hand. This approach allows the single-head classifier to effectively prioritize the relevant outputs for the current task while disregarding the irrelevant ones.
Following the setting described in~\cite{buzzega2020dark, riemer2018learning}, a neural network with 2 fully connected layers of 100 neurons %
are used to extract latent feature for MNIST dataset. %
For CIFAR and Tiny Imagenet datasets, a modified Resnet18-like~\cite{rebuffi2017icarl}  structure is used for  feature extraction. Finally, a single head linear classifier is used for classification across the experiments on MNIST, CIFAR and Tiny Imagenet datasets. We augment both stream and memory samples by applying random crop and horizontal flip for both CIFAR10 and Tiny-Imagenet datasets~\cite{buzzega2020dark}.%

The SGD optimizer is used for training DNN model throughout the learning experiences with keeping flexibility in selection of batch size and mini-batch size for different task setting. Please refer to the appendix for the task-specific values of hyperparameters. Following DER training scheme, we train our model for one epoch at each learning step on MNIST dataset while for relatively complex dataset such as CIFAR10 and Tiny-Imagenet we use 50 and 100 epochs respectively for training. 


\paragraph{\textbf{Continual Semantic Segmentation}}

We benchmark our proposed method against state-of-the-art methods with different task settings on Pascal-VOC 2012~\cite{everingham2015pascal} %
dataset. We follow the experimental setup used in~\cite{cermelli2020modeling} for VOC dataset, baseline implementation and metric. Precisely, in our comparative study, we consider eight methods, elastic weight consolidation (EWC)~\cite{kirkpatrick2017overcoming}, learning without forgetting (LwF)~\cite{li2017learning}, Riemannian walk (RW)~\cite{chaudhry2018riemannian}, ILT~\cite{michieli2019incremental}, MiB~\cite{cermelli2020modeling}, SDR~\cite{michieli2021continual}, GIFS~\cite{cermelli2020few} and PLOP~\cite{douillard2021plop}, 

The \textbf{Pascal-VOC 2012 dataset} has 10582 training images and 1449 validation images which is used for testing. Each pixel belongs to 20 foreground classes against background. We use three different tasks settings: 19-1, 15-5 and 15-5s in the experiments. In the first two settings we incrementally add 1 and 5 novel classes on the base model trained on 19 and 15 classes respectively. However, in the 15-5 setting we add 1 class in 5 consecutive tasks while base model remains similar to the 15-5 setting.
In our evaluation of continual semantic segmentation methods, we follow class overlapped setting where task consists of images that may contain classes belonging to future task with a label of background.


\paragraph{\textbf{Implementation Details}} In our implementation, we use a Deeplab-V3~\cite{chen2017rethinking} architecture with  ResNet-101~\cite{he2016deep} as a backbone that is pretrained on Imagenet~\cite{deng2009imagenet}. Following~\cite{bulo2018place}, to reduce required memory, we use inplace activated batch normalization for training our model. We use Stochastic Gradient Decent (SGD) with a momentum of 0.9 and learning decay of 1e-4 to train our model. Following ~\cite{douillard2021plop}, we crop the images to $512 \times 512$ followed by applying random horizontal flip while training our model at each step on Pascal VOC dataset. 
While computing subspace distillation, we construct subspaces at intermediate layers using a group of 32 channels and we consider top 5 subspaces in our distillation strategy. We train our model with a learning rate of .001 from the second task while first task is trained with a higher learning rate of .01 and each task model is trained for 30 epochs with a batch size of 48 distributed over 4 GPU. We use $\alpha=10$ and $\beta=0.01$ while combining output distillation and KD with pixel-wise cross-entropy loss to compute overall loss. For computational efficiency, we employ O1 optimization from Nvidia APEX library\footnote{\url{https://github.com/NVIDIA/apex}} to train with half precision. Finally, we use the validation image set to evaluate our model. Below,  we present the experimental results of our proposed subspace distillation method for both continual learning with classification and continual semantic segmentation problem.
