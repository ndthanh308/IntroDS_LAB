\paragraph{\textbf{Major Changes:}}
\begin{enumerate}
    \item Based on \textcolor{violet}{\textbf{R1}}'s concern, 
    we have included further details regarding our proposed subspace distance presented in Equation (5) and the gradient computation presented in Equation (A.13), and
    expanded the description of class incremental and task incremental settings.
    \item To address the concerns expressed by \textcolor{violet}{\textbf{R1}} regarding the experimental results,
     we have conducted an ablation study of our subspace distillation method for continual semantic segmentation and included the result on page \textcolor{teal}{24} of the updated manuscript. We have also provided full details of the hyperparameters settings used in the experiments in appendix \textit{C.2.}.%
    
    \item The codes of this article will be available upon acceptance at \url{https://github.com/csiro-robotics/SDCL} per \textcolor{violet}{\textbf{R1}}'s comment.
    \item Based on \textcolor{violet}{\textbf{R2}}'s suggestion, we have expanded the discussion on PODNet~\cite{douillard2020podnet} and compared and contrasted it against our proposed subspace distillation method.
    \item Per the suggestion by \textcolor{violet}{\textbf{R2}}, we have added the computational complexity of subspace distillation method.
    \item We also corrected typos upon proofreading the revised manuscript. 
\end{enumerate}




\newpage
\section*{\textbf{Responses To Reviewer 1}}
\vspace{4mm}


\par{\noindent \textcolor{violet}{\textbf{R1}} - \textcolor{purple}{\textbf{Q1}} [Formulation.]}

\paragraph{\textbf{Reviewer's Comment:}} \emph{Equation (5) should be $\left\|\vec{P}_i^t - \vec{P}_i^{t-1} \right\|$.}
\paragraph{\textbf{Author's Response:}} %



\textcolor{blue}{
Let $\mathcal{S}_i$ and $\mathcal{S}_j$ be two subspaces. A valid distance between $\mathcal{S}_i$ and $\mathcal{S}_j$ is a distance that is invariant to the choice of 
the basis of the subspace. To be more specific, assume $\vec{P}_i \in \mathbb{R}^{d \times m}$ and $\vec{P}_j \in \mathbb{R}^{d \times m}$ are the basis for $\mathcal{S}_i$ and $\mathcal{S}_j$, \ie,  $\vec{P}_i\vec{P}_i^\top = \vec{P}_j\vec{P}_j^\top = \mathbf{I}_m$. Then a distance between 
$\mathcal{S}_i$ and $\mathcal{S}_j$ should meet : 
\[
d(\mathcal{S}_i, \mathcal{S}_j) = g(\vec{P}_i,\vec{P}_j) = g(\vec{P}_i\vec{R}_i,\vec{P}_j\vec{R}_j)\;,
\]
where $g:\mathbb{R}^{d \times m} \times \mathbb{R}^{d \times m} \to \mathbb{R}_+$ is the distance function and $\vec{R}_i, \vec{R}_j \in \mathcal{O}_m$
with $\mathcal{O}_m$ denoting the orthogonal group. For example, consider the XY plane in $\mathbb{R}^3$ as a 2D subspace in 3D space. Both }

\textcolor{blue}{
\begin{minipage}{0.4\textwidth}
\begin{align*}
    \vec{P}_1 & = 
\begin{pmatrix}
1 &0\\
0 &1\\
0 &0\\
\end{pmatrix}
\end{align*}
\end{minipage}%
\begin{minipage}{0.1\textwidth}
and
\end{minipage}
\begin{minipage}{0.5\textwidth}
\begin{align*}
\vec{P}_2 &= 
\begin{pmatrix}
\cos(\pi/4) &-\sin(\pi/4)\\
\sin(\pi/4) &\cos(\pi/4)\\
0 &0\\
\end{pmatrix}
\end{align*}
\end{minipage}
}



\textcolor{blue}{
represent the XY plane; hence their distance should be zero. The distance used in our work, \ie, $\left\| \vec{P}_i^{t\top}\vec{P}_i^t - \vec{P}_i^{t-1\top}\vec{P}_i^{t-1} \right\|^2_F$ not only satisfies the required conditions, but also closely related to the geodesics on the Grassmannian~\cite{harandi2015extrinsic}.} 

On the contrary, the form $\left\|\vec{P}_i^t - \vec{P}_i^{t-1} \right\|$ as mentioned by this reviewer's comment, does not meet the condition of distances between the subspaces, as the distance will change based on the choice of the subspace and even on the order of how the basis of the subspace is encoded.



\textcolor{blue}{
In equation (5), the mapping $f:\mathbb{R}^{d \times m} \to \mathbb{R}^{m \times m}; f(\vec{P}) =\vec{P}\vec{P}^\top$ is a diffeomorphism between Grassmannian and the space of  symmetric matrices (positive semidefinite to be adequate). The induced distance $\delta_p^2(\vec{P}_i,\vec{P}_j) = \left\|\vec{P}_i\vec{P}_i^\top - \vec{P}_j\vec{P}_j^\top\right\|_F^2$ is invariant to the action of the orthogonal group, satisfying the requirement of having a Grassmannian distance. 
Furthermore, since $\vec{P}^\top\vec{P}=\mathbf{I}_m$, the distance can be simplified to $\delta_p^2(\vec{P}_i,\vec{P}_j) = 2m-2\left\|\vec{P}_i^\top\vec{P}_j\right\|_F^2$, which is computationally very attractive in comparison to geodesics on Grassmannian that require computing SVD. We note that the gradient of our proposed subspace distillation loss is 
$\nabla_{\vec{P}_i} \triangleq \frac{\partial}{\partial \vec{P}_i} \delta_{p}^2\big(\vec{P}_i,\vec{P}_j\big) = -4 \vec{P}_j\vec{P}_j^{\top}\vec{P}_i$.\\}
Please note that in our work, we consider the distance between $\vec{P}_i^{t}$ and $\vec{P}_i^{t-1}$, the basis of subspaces constructed using new embedding and old embedding, respectively. %
Furthermore, to enhance the clarity of the paper, we have revised section ``Subspace Distillation" on page \textcolor{teal}{10}, incorporating the aforementioned discussion as per the reviewer's comment.\\

 


\par{\noindent \textcolor{violet}{\textbf{R1}} - \textcolor{purple}{\textbf{Q2}} [Formulation.]}

\paragraph{\textbf{Reviewer's Comment:}} \emph{In the RHS of Equation (A.13), it should be dF but dP is used.}
\paragraph{\textbf{Author's Response:}} %
Thanks for spotting this typo. We have updated the equation A.13 as follows:

\begin{align}
    \label{eqn:a_svd_diff_d}
    \begin{split}
    \nabla_{\mat{P}}:\dd\mat{P} & = \Big(\nabla_{\mat{P}}\Big)_{1}\mat{\Sigma}_d^{-1}:\mat{C} \\
    & = \underbrace{\Big(\nabla_{\mat{P}}\Big)_{1}\mat{\Sigma}_d^{-1}}_{\mat{D}} : \Big( \dd \mat{F} \mat{Q} - \mat{P} \dd \mat{\Sigma} - \mat{P}\mat{\Sigma}\dd \mat{Q}^{\top}\mat{Q} \Big)  \\
    & = \mat{D}:\dd \mat{F} \mat{Q} ~-~ \mat{D}:\mat{P} \dd \mat{\Sigma} ~-~ \mat{D}:\mat{P}\mat{\Sigma}\dd \mat{Q}^{\top}\mat{Q} ,~ \mat{D}=\Big(\nabla_{\mat{P}}\Big)_{1}\mat{\Sigma}_d^{-1}\\
    & = \mat{D}\mat{Q}^{\top}:\textcolor{blue}{\dd \mat{F}} ~-~ \mat{P}^{\top}\mat{D}: \dd \mat{\Sigma}~-~ \mat{\Sigma}\mat{P}^{\top}\mat{D}\mat{Q}^{\top}:\dd \mat{Q}^{\top} \\
    & = \mat{D}\mat{Q}^{\top}:\textcolor{blue}{\dd \mat{F}} ~-~ \mat{P}^{\top}\mat{D}: \dd \mat{\Sigma} ~-~ \mat{Q}\mat{D}^{\top}\mat{P}\mat{\Sigma}:\dd \mat{Q} \\
    & = \mat{D}\mat{Q}^{\top}:\textcolor{blue}{\dd \mat{F}} ~-~ \mat{P}^{\top}\mat{D}: \Big(\mat{P}^{\top}\dd\mat{P}\mat{Q}\Big)_{\text{diag}} ~-~ \mat{Q}\mat{D}^{\top}\mat{P}\mat{\Sigma}:2\mat{Q}~\Biggl(\mat{K}^{\top} \circ \Big(\mat{\Sigma}^{\top}\mat{P}^{\top}\dd\mat{P}\mat{Q}\Big)_{\text{sym}}~\Biggl) \\
    & = \mat{D}\mat{Q}^{\top}:\textcolor{blue}{\dd \mat{F}} ~-~ \Big(\mat{P}^{\top}\mat{D}\Big)_{\text{diag}}: \mat{P}^{\top}\dd\mat{P}\mat{Q} ~-~ 2\mat{Q}^{\top}\mat{Q}\mat{D}^{\top}\mat{P}\mat{\Sigma}:~\Biggl(\mat{K}^{\top} \circ \Big(\mat{\Sigma}^{\top}\mat{P}^{\top}\dd\mat{P}\mat{Q}\Big)_{\text{sym}}~\Biggl) \\
    & = \mat{D}\mat{Q}^{\top}:\textcolor{blue}{\dd \mat{F}} ~-~ \mat{P}\Big(\mat{P}^{\top}\mat{D}\Big)_{\text{diag}}\mat{Q}^{\top}: \dd\mat{P} ~-~ 2\Big(\mat{K}^{\top} \circ \mat{D}^{\top}\mat{P}\mat{\Sigma}\Big)_{\text{sym}}:~ \Big(\mat{\Sigma}^{\top}\mat{P}^{\top}\dd\mat{P}\mat{Q}\Big)\\
    & = \mat{D}\mat{Q}^{\top}:\textcolor{blue}{\dd \mat{F}} ~-~ \mat{P}\Big(\mat{P}^{\top}\mat{D}\Big)_{\text{diag}}\mat{Q}^{\top}: \dd\mat{P} ~-~ 2\mat{P}\mat{\Sigma}\Big(\mat{K}^{\top} \circ \mat{D}^{\top}\mat{P}\mat{\Sigma}\Big)_{\text{sym}}\mat{Q}^{\top}: \dd\mat{P}
    \end{split}
\end{align}
\\

\par{\noindent \textcolor{violet}{\textbf{R1}} - \textcolor{purple}{\textbf{Q3}} [Formulation.]}

\paragraph{\textbf{Reviewer's Comment:}} \emph{In Equation (19), "background and label" with respect to which task (current or previous task) is not clear.}
\paragraph{\textbf{Author's Response:}} To tackle the background shift problem, following MiB~\cite{cermelli2020modeling} method, we employ masked cross entropy by relating the old model's prediction to the new model's prediction after combining new classes probability with background. Therefore, the adapted output distillation loss is defined as follows:

\begin{align}
    \label{eqn:kd_loss}
    \ell_{\mathrm{KD}}^{CSS}(\mat{X}, \vec{Y}) =
    -\dfrac{1}{|\mat{X}|} \sum_{x \in \mat{X}}^{} \sum_{c \in {C^{1:t-1}}}^{} y^{t-1}_{x, c}~\log(\tilde{y}^{t}_{x, c}),
\end{align}

where $y^{t-1}_{x, c}$ is the predicted probability using $f_{\theta^{t-1}}$ for class $c$ at pixel $x$ in image $\mat{X}$. We define  $\tilde{y}^{t}_{x, c}$ as follows:

\begin{equation}
    \label{eqn:predicted_lbl}
    \tilde{y}^{t}_{x, c} =
    \begin{cases}
    y^{t}_{x, c} &\text{if label \textcolor{blue}{$c \in {C^{1:t-1}}$} is not background} \\
    \sum_{c^{'} \in {C^{t}}}^{}\limits y^{t}_{x, c^{'}} &\text{if label \textcolor{blue}{$c \in {C^{1:t-1}}$} is background}\\
    \end{cases}
\end{equation}
\textcolor{blue}{
Since the modified distillation loss does not directly match the predicted background class probability of old model to the new model, the distillation loss plays a vital role in tackling the background label shift problem in class-incremental learning for semantic segmentation.}

We have revised the section 4.4.2. ``Knowledge Distillation", incorporating the aforementioned discussion based on the reviewer's comment. 
\\

\par{\noindent \textcolor{violet}{\textbf{R1}} - \textcolor{purple}{\textbf{Q4}} [Explanation.]}

\paragraph{\textbf{Reviewer's Comment:}} \emph{It would be clearer if the authors provided a description for the class incremental setting and task incremental setting. Is the task incremental setting a multi-head setting?}
\paragraph{\textbf{Author's Response:}} 




\textcolor{blue}{
In our experiments, we partition data into distinct sets of non-overlapping classes, hence applicable in both class-incremental and task-incremental scenarios. 
Furthermore, we maintain a consistent ordering of all classes across all algorithms, guaranteeing that each algorithm receives identical data for every task. %
Compared to the class-incremental scenario, where task identity is missing, task-incremental learning benefits from having access to task identifiers, which in return helps in selecting appropriate classifiers, rendering it a comparatively easier scenario. Conversely, the class-incremental scenario poses a challenge due to the absence of task identity. \\
Task-incremental learning can be implemented using either a multi-head or single-head classifier, depending on the specific implementation. Following the implementation of DER~\cite{buzzega2020dark}, in our approach, we employed a single-head classifier model to learn in class-incremental scenarios. However, in task-incremental settings, we relied on the output masking technique of the single-head classifier to identify task-specific classes, leveraging the availability of task identity during inference. The output masking technique is used to selectively mask specific outputs of the single-head classifier, depending on the task at hand. This approach allows the single-head classifier to effectively prioritize the relevant outputs for the current task while disregarding the irrelevant ones.
}

Per your comment, we have revised subsection 2.1 titled "Continual Learning" in section 2, "Related Work". We have also incorporated a detailed discussion regarding the task-incremental and class-incremental settings mentioned earlier. \\




\par{\noindent \textcolor{violet}{\textbf{R1}} - \textcolor{purple}{\textbf{Q5}} [Presentation.]}

\paragraph{\textbf{Reviewer's Comment:}} \emph{Authors did not provide the ablation study for the losses in Equation (15) and Equation (20). (CE and SD loss, KD and SD loss).}
\paragraph{\textbf{Author's Response:}} 
To address your comment, we have included the following ablation study on page
\textcolor{teal}{24} of the updated manuscript. 

\textcolor{blue}{
In order to evaluate the contribution of each regularization term in our proposed subspace distillation method for continual semantic segmentation and class-incremental learning, we conducted experiments on the Pascal VOC dataset with varying numbers of tasks and S-CIFAR10 dataset (see~\cref{tab:iou_benchmark_voc_}). Results in \cref{tab:iou_benchmark_voc_1} suggest that both regularizers, $\ell_{\mathrm{KD}}^{CSS}$ and $\ell_{\mathrm{SD}}^{CSS}$, contribute to improving overall performance across the different settings. For example, in the (19-1) 2-task setting, combining regularizer KD with the baseline using CE led to an increase in performance of around 9\%. We also noted an additional 4\% improvement on both previously observed classes and overall performance when regularizer SD was combined with KD in our proposed subspace distillation method.
At the same time, the experimental findings displayed in \cref{tab:iou_benchmark_voc_2} demonstrate that the combination of subspace distillation (SD), $\ell_{\mathrm{SD}}^{CL}$, and memory replay, $\ell_{\mathrm{CE}}^{CL}$ yields substantial improvements in overall accuracy and forgetting on the S-CIFAR10 dataset. Specifically, there is an approximate increase of 9\% in accuracy and a reduction of 15\% in forgetting.
}
\\










\begin{table}[htbp]
    \centering
    \begin{subtable}{0.44\textwidth}
        \centering
        \resizebox{\textwidth}{!}{\begin{tabular}{ccccccc}
        \hline
        \multicolumn{3}{c}{Method} & \multicolumn{4}{c}{19-1 (2 tasks)} \\ \cline{4-7} 
                CE    &  KD  &  SD  & \multicolumn{2}{c}{0-19 (Old Classes)} & \multicolumn{2}{c}{Overall} \\ \hline
                \cmark   &  \xmark   &   \xmark & \multicolumn{2}{c}{62.30} & \multicolumn{2}{c}{59.85} \\
                \cmark   &  \cmark   &   \xmark &  \multicolumn{2}{c}{71.43 (\textcolor{green}{+9.13})}    &  \multicolumn{2}{c}{69.15 (\textcolor{green}{+9.30})} \\
                    \cmark   &  \cmark   &   \cmark & \multicolumn{2}{c}{\textbf{76.09 (\textcolor{green}{+4.66})}}   & \multicolumn{2}{c}{\textbf{73.43 (\textcolor{green}{+4.28})}}    \\ \hline
        \end{tabular}}
    \caption{\label{tab:iou_benchmark_voc_1} Experimental results of Continual Learning (CL) methods on Pascal VOC dataset for continual (19-1) 2 tasks \textbf{overlap} class learning settings.}
    \end{subtable}
    \hfill
    \begin{subtable}{0.53\textwidth}
        \centering
        \resizebox{\textwidth}{!}{\begin{tabular}{ccccccc}
        \hline
        \multicolumn{3}{c}{Method} & \multicolumn{4}{c}{S-CIFAR10 (5 tasks; C-IL setting) } \\ \cline{4-7} 
        CE    &  Memory Replay  &  SD  & \multicolumn{2}{c}{Avg. Accuracy} & \multicolumn{2}{c}{Forgetting} \\ \hline
        \cmark   &  \xmark   &   \xmark &  \multicolumn{2}{c}{17.32}    & \multicolumn{2}{c}{80.65} \\
        \cmark   &  \cmark   &   \xmark &  \multicolumn{2}{c}{38.97 (\textcolor{green}{+21.65})}    & \multicolumn{2}{c}{43.34 (\textcolor{green}{-37.31})} \\
        \cmark   &  \cmark   &   \cmark &  \multicolumn{2}{c}{\textbf{47.68 (\textcolor{green}{+8.71})}}     &  \multicolumn{2}{c}{\textbf{27.86 (\textcolor{green}{-15.48})}}    \\ \hline
        \end{tabular}}
    \caption{\label{tab:iou_benchmark_voc_2} Experimental results of CL methods trained for one epoch with 500 memory exemplars on the S-CIFAR10 dataset.}
    \end{subtable}
    \vspace{-.25cm}\caption{\label{tab:iou_benchmark_voc_}Experimental results of (a) Continual Semantic Segmentation on Pascal VOC, and (b) Class-Incremental learning on S-CIFAR10 indicate that incorporating subspace distillation with memory replay significantly improves the performance of the CL method. }
\end{table}





\par{\noindent \textcolor{violet}{\textbf{R1}} - \textcolor{purple}{\textbf{Q6}} [Explanation.]}

\paragraph{\textbf{Reviewer's Comment:}} \emph{In Table 1, why do some methods give lower results than the lower bound?}
\paragraph{\textbf{Author's Response:}} 
The lower bound refers to the performance achieved by training a continual learning model without any specific forgetting countermeasures, typically using stochastic gradient descent (SGD) as the optimization algorithm. A continual learning method can result in complete failure and perform even worse than training with vanilla SGD (\ie, lower bound). On the other hand, the upper bound is attained by jointly training all tasks (\ie, the model sees all tasks at once). However, a continual learning method may still surpass this upper bound by employing strategies that allow for better adaptation to new tasks over time. For example, an agent that incrementally learns and adapts to new data may outperform an agent that is trained on all data at once. Therefore, in order to enhance clarity, we have chosen to replace the potentially misleading terms ``upper-bound" and ``lower-bound" with ``JOINT" and "SGD" in the revised manuscript. This modification is intended to offer more accurate and understandable descriptions. \\
Furthermore, regularization-based methods such as oEWC and SI have been developed for task-incremental learning, but their performance degrades significantly in class-incremental settings where task identifiers are not available during inference. \textcolor{blue}{Experimental results presented in Table 1 suggest that replay-based distillation methods also suffer from poor performance when there are insufficient exemplars available, especially for previously observed classes where no memory exemplars exist. For instance, in S-Tiny Imagenet, during the final task, with a memory budget of 100 exemplars, only one example is stored for the 100 previously seen classes out of a total of 180 prior classes. This leads to a drop in performance for replay-based distillation methods. }

We have revised the ``Experiments" section on page \textcolor{teal}{19} and  incorporated the above mentioned discussion regarding the subpar performance of continual learning methods with insufficient memory.\\




\par{\noindent \textcolor{violet}{\textbf{R1}} - \textcolor{purple}{\textbf{Q7}} [Presentation.]}

\paragraph{\textbf{Reviewer's Comment:}} \emph{What will be the lower bound and upper bound for continual semantic segmentation?}
\paragraph{\textbf{Author's Response:}} Please find the upper bound and lower bound for continual semantic segmentation in Table \ref{tab:iou_benchmark_voc_bound}. We have updated Table 2 in our revised  manuscript with the upper bound and lower bound for continual semantic segmentation on the Pascal VOC dataset per your comment.
\\
\begin{table*}[ht]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{cccccccccc}
\hline
\multirow{2}{*}{Method} & \multicolumn{3}{c}{19-1 (2 tasks)}                   & \multicolumn{3}{c}{15-5 (2 tasks)}                      & \multicolumn{3}{c}{15-1 (6 tasks)} \\ \cline{2-10} 
                        & 0-19 & 20 & \multicolumn{1}{c|}{All} & 0-15 & 16-20 & \multicolumn{1}{c|}{All} & 0-15 & 16-20 & All \\ \hline
\textcolor{blue}{JOINT (Upper Bound)}                       & 77.60     & 76.60   & 77.50     & 79.0     & 72.80      & 77.50      &  79.00    & 72.80      & 77.50 \\

\textcolor{blue}{SGD (Lower Bound)}                       & 6.80     & 12.90   & 7.10     & 2.10     & 33.10      & 9.80      &  0.20    & 1.80      & 0.60 \\ \hline
\end{tabular}}
\caption{\label{tab:iou_benchmark_voc_bound}Performance of different continual semantic segmentation method in IOU on Pascal VOC dataset for three continual \textbf{overlap} class learning settings: (i) 19-1 (ii) 15-5 and (iii) 15-1 tasks.}

\end{table*} 

\par{\noindent \textcolor{violet}{\textbf{R1}} - \textcolor{purple}{\textbf{Q8}} [Presentation.]}

\paragraph{\textbf{Reviewer's Comment:}} \emph{The number of samples used in memory is to be mentioned.}
\paragraph{\textbf{Author's Response:}} We refer to Tiny Memory, Small Memory, Medium Memory and Large Memory as the buffer with the capacity of storing 100, 200, 500 and 5120 exemplars, respectively. We have updated the experimental results presented in Table 1 with the size of memory.\\


\par{\noindent \textcolor{violet}{\textbf{R1}} - \textcolor{purple}{\textbf{Q9}} [Presentation.]}

\paragraph{\textbf{Reviewer's Comment:}} \emph{Hyperparameters for continual semantic segmentation are to be provided. }
\paragraph{\textbf{Author's Response:}} 
The updated manuscript's appendix includes details on hyperparameters for continual semantic segmentation (CSS). Furthermore, the hyperparameters utilized on Pascal VOC for CSS are provided in Table \ref{tab:hyperparam}.

\begin{table*}[htbp]
\centering
\resizebox{.85\textwidth}{!}{\begin{tabular}{c|cc|cc|cc}
\hline
Method & \multicolumn{2}{c|}{19-1 (2-Tasks)} & \multicolumn{2}{c|}{15-5 (2-Tasks)} & \multicolumn{2}{c}{15-1 (5-Tasks)} \\ \hline
SD           & \multicolumn{2}{c|}{lr: .001, $\alpha: 10$~, $\beta: .01$ }    &  \multicolumn{2}{c|}{lr: .001, $\alpha: 10$~, $\beta: .01$ } & \multicolumn{2}{c}{ lr:.001 , $\alpha:10 $~, $\beta:.01 $ }              \\

                        \hline

\end{tabular}}
\caption{\label{tab:hyperparam}Hyperparameter selected for SD method for Continual Semantic Segmentation.
}
\end{table*}

\par{\noindent \textcolor{violet}{\textbf{R1}} - \textcolor{purple}{\textbf{Q10}} [Presentation.]}

\paragraph{\textbf{Reviewer's Comment:}} \emph{The authors need to provide the code for reproducibility.}
\paragraph{\textbf{Author's Response:}}
We are planning to release our codes upon acceptance at\\ \url{https://github.com/csiro-robotics/SDCL}.
\\


\section*{\textbf{Responses To Reviewer 2}}
\vspace{5mm}

\par{\noindent \textcolor{violet}{\textbf{R2}} - \textcolor{purple}{\textbf{Q1}} [Explanation]}

\paragraph{\textbf{Reviewer's Comment:}} \emph{One of the main improvements required to the manuscript, is clear elaboration and explanation of similar training techniques such as PODNet~\cite{douillard2020podnet}. While the authors have cited this paper, they have not explained it thoroughly, and more importantly, have not provided the main differences between their work and the above mentioned work. In particular, PODNet also uses knowledge distillation over intermediate layers of the network for CL tasks and hence differences between the two works should be clearly explained.}

\paragraph{\textbf{Author's Response:}} Per your recommendation, we have expanded the distillation-based continual learning methods in the related work section of the updated manuscript. Precisely, we have explained the LUCIR~\cite{hou2019learning}, semantic-aware knowledge distillation~\cite{cheraghian2021semantic}, and PODNet~\cite{douillard2020podnet} and compared them with our subspace distillation method as follows: %


\textcolor{blue}{ Hou \etal in~\cite{hou2019learning} proposed cosine normalization to combat catastrophic forgetting and facilitate the seamless integration of new and prior knowledge within a continual learning model. By adjusting the magnitudes of the model's weights, cosine normalization provides precise control over the influence of novel tasks. This mechanism ensures that the model does not prioritize the new task at the expense of previously learned tasks, thereby preserving valuable knowledge while accommodating novel knowledge. This normalization approach helps to mitigate the negative impact of catastrophic forgetting and enhances the model's ability to generalize across multiple tasks.  %
Cheraghian~\etal in~\cite{cheraghian2021semantic} introduced a semantic-aware distillation loss for few-shot class incremental learning that takes into account the semantic structure of the data. The distillation strategy utilizes semantic embeddings associated with each class to guide the distillation process. The integration of semantic information fosters the preservation of prior knowledge in the continual learning model by facilitating learning not only from the representations of the previous model but also from the semantic relationships among the classes. %
PODNet~\cite{douillard2020podnet} is a pioneering work in small-tasks incremental learning that proposes a novel distillation-based method to prevent catastrophic forgetting. During the incremental learning process, PODNet uses a Pooled Outputs Distillation (POD) mechanism to transfer knowledge from the previously learned tasks to the current task. Specifically, the outputs of the intermediate layers of the previous model are pooled and distilled into the corresponding layer of the current model by minimizing the discrepancies using Euclidean distance of L2-normalized features. Mathematically, POD can be expressed as 
$L_{POD} = \left\|f_i^{old}(\vec{x}) - f_i^{new}(\vec{x})\right\|^2$,
where $f_i^{old}(\vec{x})$ and $f_i^{new}(\vec{x})$ are the pooled output of the $i^{th}$ spatial position for input $\vec{x}$ using old and new model respectively.
One of the limitations of PODNet is the absence of an explicit mechanism to effectively preserve the underlying latent structure, as it primarily relies on imposing constraints on the pooled features. Consequently, the presence of outliers or noisy attributes can potentially compromise the effectiveness of distillation in PODNet. In contrast, our proposed subspace distillation approach addresses this concern by imposing constraints on the low-dimensional subspaces derived from the latent features of both the old and current models. This not only enhances robustness to noise but also ensures the preservation of latent structure in continual learning scenarios.
}


\par{\noindent \textcolor{violet}{\textbf{R2}} - \textcolor{purple}{\textbf{Q2}} [Explanation]}

\paragraph{\textbf{Reviewer's Comment:}} \emph{A section should also be added discussing the extra computational steps as well as the memory required to conduct the whole SD process which includes Singular Value Decomposition (SVD), keeping the teacher and student network in memory and calculation of the similarity measures and loss.}






\paragraph{\textbf{Author's Response:}} 
Based on your recommendation, we have included the computational complexity analysis of our proposed subspace distillation method on page \textcolor{teal}{24} and \textcolor{teal}{25} of the updated manuscript, as follows.

\textcolor{blue}{
Our proposed distillation loss requires computing the subspace basis employing SVD. For an input feature vector $\vec{F} \in \mathbb{R}^{d \times m}; d> m$, computational complexity of SVD is $\mathcal{O}\big(d \times m \times \min(d, m)\big) = \mathcal{O}(dm^2)$. Assuming, $\vec{P}^{t} \in \mathbb{R}^{d \times n}$ and $\vec{P}^{t-1} \in \mathbb{R}^{d \times n}$ be the basis of top $n$ subspaces used in our computation, the computation cost of ${\vec{P}^{t}}^\top\vec{P}^{t-1}$ is $\mathcal{O}(n \times d \times n) = \mathcal{O}(dn^2)$. Therefore, the total computation cost of our proposed subspace distillation loss can be expressed as  $\mathcal{O}(dm^2 + dn^2)$. During our experiments on the 5-tasks CIFAR10 dataset, we found that one iteration of our replay-based subspace distillation (SD) method requires approximately 60ms on Tesla P100-SXM2 GPU for a batch of 32 samples, while the replay-based vanilla SGD method requires about 37ms. We rely on the assumption that a limited buffer memory is available throughout the learning process to store exemplars from prior tasks that might restrict the use of our method in privacy-focused applications.
}










