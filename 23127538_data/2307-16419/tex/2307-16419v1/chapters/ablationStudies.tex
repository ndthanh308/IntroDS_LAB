\section{Ablation Studies}
\label{sec:ablation_studies}
In this section, we evaluate our model in terms of changes in activation map, and feature representation in different learning steps in class incremental learning setting on CIFAR10 dataset. Furthermore, in case of continual semantic segmentation on Pascal VOC dataset, we investigate the contribution of each regularizer in subspace distillation strategy and analyze our proposed method with (i) varying dimensionality of subspace and (ii) different no. of channels to construct the subspace.


{\centering
\begin{table*}[ht!]
\addtolength{\tabcolsep}{-5pt}
\resizebox{\textwidth}{!}{
\begin{tabular}{cccccc}
\hline
Input & Method & Task-2 & Task-3 & Task-4 & Task-5 \\
\hline
\multirow{10.65}{*}{\begin{subfigure}{0.2\textwidth}\centering% Figure removed\label{fig:cat}\end{subfigure}} 

& DER &
\begin{subfigure}{0.2\textwidth}\centering% Figure removed\label{fig:der_cat1}\end{subfigure}&
\begin{subfigure}{0.2\textwidth}\centering% Figure removed\label{fig:der_cat2}\end{subfigure}&
\begin{subfigure}{0.2\textwidth}\centering% Figure removed\label{fig:der_cat3}\end{subfigure}&
\begin{subfigure}{0.2\textwidth}\centering% Figure removed\label{fig:der_cat4}\end{subfigure}\\ \cline{2-6}
\newline
& DER++ &
\begin{subfigure}{0.2\textwidth}\centering% Figure removed\label{fig:derpp_cat1}\end{subfigure}&
\begin{subfigure}{0.2\textwidth}\centering% Figure removed\label{fig:derpp_cat2}\end{subfigure}&
\begin{subfigure}{0.2\textwidth}\centering% Figure removed\label{fig:derpp_cat3}\end{subfigure}&
\begin{subfigure}{0.2\textwidth}\centering% Figure removed\label{fig:derpp_cat4}\end{subfigure}\\ \cline{2-6}
\newline
& SD &
\begin{subfigure}{0.2\textwidth}\centering% Figure removed\label{fig:sd_cat1}\end{subfigure}&
\begin{subfigure}{0.2\textwidth}\centering% Figure removed\label{fig:sd_cat2}\end{subfigure}&
\begin{subfigure}{0.2\textwidth}\centering% Figure removed\label{fig:sd_cat3}\end{subfigure}&
\begin{subfigure}{0.2\textwidth}\centering% Figure removed\label{fig:sd_cat4}\end{subfigure}\\ \cline{2-6}

\hline
\multirow{10.65}{*}{\begin{subfigure}{0.2\textwidth}\centering% Figure removed\label{fig:bird}\end{subfigure}} 

& DER &
\begin{subfigure}{0.2\textwidth}\centering% Figure removed\label{fig:der_bird1}\end{subfigure}&
\begin{subfigure}{0.2\textwidth}\centering% Figure removed\label{fig:der_bird2}\end{subfigure}&
\begin{subfigure}{0.2\textwidth}\centering% Figure removed\label{fig:der_bird3}\end{subfigure}&
\begin{subfigure}{0.2\textwidth}\centering% Figure removed\label{fig:der_bird4}\end{subfigure}\\ \cline{2-6}
\newline
& DER++ &
\begin{subfigure}{0.2\textwidth}\centering% Figure removed\label{fig:derpp_bird1}\end{subfigure}&
\begin{subfigure}{0.2\textwidth}\centering% Figure removed\label{fig:derpp_bird2}\end{subfigure}&
\begin{subfigure}{0.2\textwidth}\centering% Figure removed\label{fig:derpp_bird3}\end{subfigure}&
\begin{subfigure}{0.2\textwidth}\centering% Figure removed\label{fig:derpp_bird4}\end{subfigure}\\ \cline{2-6}
\newline
& SD &
\begin{subfigure}{0.2\textwidth}\centering% Figure removed\label{fig:sd_bird1}\end{subfigure}&
\begin{subfigure}{0.2\textwidth}\centering% Figure removed\label{fig:sd_bird2}\end{subfigure}&
\begin{subfigure}{0.2\textwidth}\centering% Figure removed\label{fig:sd_bird3}\end{subfigure}&
\begin{subfigure}{0.2\textwidth}\centering% Figure removed\label{fig:sd_bird4}\end{subfigure}\\

\hline
\end{tabular}
}
\addtolength{\tabcolsep}{1pt} 
\caption{The transition of what neural network is looking at in different learning tasks on CIFAR10 dataset using GradCAM~\cite{jacobgilpytorchcam}. The model trained using our subspace distillation method consistently attends in the regions of the images where the feature of the cat or bird (\ie., around the head or body) is located while the activated regions shift significantly for state-of-the-art DER++ and DER. %
}
\label{tab:grad_cam}
\end{table*}
}


\noindent
\textbf{Changes in Activation Map.}
We examine our model in terms of where neural network is looking at the input images for decision making and whether the activated region has been changed over time in continual learning setting. To do so, We first train a neural network on 5 tasks CIFAR10 datasets and, at the end of learning different tasks, we feed the model with an image that is presented to the model at second step followed by computing heatmap images using GradCAM~\cite{selvaraju2017grad}. We present the result in Figure \ref{tab:grad_cam} showing the evolution of the important regions in the images. The result suggests that DER method activated the body part of cat while our method looks at face region of cat image. Furthermore subspace distillation consistently activates the same regions and the changes in the activation map is lower than the state-of-the-art of DER++ method. Similarly, both DER and DER++ show inconsistency in activating the interested region where bird is located in the image while SD method exhibits robustness in activating the body part of bird in consecutive tasks.

% Figure environment removed

\noindent
\textbf{Similarity in Representation.}
We analyze our models capability of retaining previously learned knowledge by maintaining similar feature representation in the intermediate layer of neural network while learning a series of tasks. Precisely, we employ Centered Kernel Alignment (CKA) metric~\cite{kornblith2019similarity} to compute similarity between intermediate feature representation generated by neural network at task $t$ and $t+1$ on test dataset of CIFAR10 and report the comparative result in Figure \ref{fig:cka_sim}. Overall we observe that the CKA similarity steadily increases as the model faces more tasks and we think that is because the model becomes more stable as it gets adapted incrementally on novel dataset in presence of samples from prior tasks in memory buffer. We notice that our proposed subspace distillation method consistently maintains higher CKA similarity of feature representation throughout the learning experiences. For instance,  SD method outperforms baseline method, ER, on CKA similarity metric by 8\% in the first task while the gap reduces to 3\% in the last task.



\begin{table}[htbh!]
\centering
\resizebox{.65\columnwidth}{!}{\begin{tabular}{ccccccc}
\hline
\multirow{2}{*}{No. of Channels} & \multicolumn{3}{c}{15-5s (6 tasks)}  & \multicolumn{3}{c}{19-1 (2 tasks)} \\ \cline{2-7} 

                        & 0-15 & 16-20 & \multicolumn{1}{c}{All} & 0-19 & 20 & \multicolumn{1}{c}{All}\\ \hline

16             & 66.73     & 21.57   & 55.97  & 76.47     & 46.38   & 75.04\\
32             & 66.83     & 21.56   & 56.05  & 76.50     & 46.39   & 75.07\\
64             & 66.75    & 21.84   & 56.06   & 76.48    & 46.23   & 75.04\\ \hline
\end{tabular}}
\caption{\label{tab:iou_cdim}Mean IOU for 15-5s and 19-1 incremental task setting on Pascal-VOC overlapped dataset with different no. of channels to compute subspace using SVD. %
}
\end{table}

\noindent
\textbf{Varying no. of channels for computing subspace.}
 We evaluate the effect of dimensionality of channels being used to construct the subspace on Pascal VOC dataset for 15-5s and 19-1 tasks setting by combining SD with PLOP. With the increasing no. of channels for constructing subspace, we do not observe any considerable changes in both stability and plasticity as the IOU for new classes and mean IOU for old classes remains stable. As reported in~\ref{tab:iou_cdim}, subspace distillation exhibits robustness against the varying no. of channels used to construct subspace. 
 


\begin{table}[htbh!]
\centering
\resizebox{.5\columnwidth}{!}{\begin{tabular}{cccc}
\hline
\multirow{2}{*}{Dimension of Subspace} & \multicolumn{3}{c}{19-1 (2 tasks)} \\ \cline{2-4} 

                        & 0-19 & 20 & \multicolumn{1}{c}{All}\\ \hline
1             &  76.45    & 46.19   & 75.00\\
3             & 76.49     & 46.43  & 75.06\\
5             & 76.46    & 46.32   & 75.03\\
7             & 76.46    & 46.33   & 75.03\\ \hline
\end{tabular}}
\caption{\label{tab:iou_SD_dim}Mean IOU for 19-1 overlap incremental task setting on Pascal-VOC dataset with different subspace dimension.}
\end{table}

\noindent
\textbf{Subspace Dimensionality.}
To also examine the effect of dimensionality of subspace used in distilling structure for continual semantic segmentation, we combine subspace distillation with PLOP method and report the experimental result on Pascal VOC dataset for 19-1 overlap tasks setting in~\ref{tab:iou_SD_dim}. The results suggest that, with the increase of subspace dimension, the performance on the already observed classes remains similar and subspace dimensionality exhibits less impact on the overall performance.







 





\noindent
\textbf{Contribution of each Regularizer.} In order to evaluate the contribution of each regularization term in our proposed subspace distillation method for continual semantic segmentation and class-incremental learning, we conducted experiments on the Pascal VOC dataset with varying numbers of tasks and S-CIFAR10 dataset (see~\cref{tab:iou_benchmark_voc_}). Results in \cref{tab:iou_benchmark_voc_1} suggest that both regularizers, $\ell_{\mathrm{KD}}^{CSS}$ and $\ell_{\mathrm{SD}}^{CSS}$, contribute to improving overall performance across the different settings. For example, in the (19-1) 2-task setting, combining regularizer KD with the baseline using CE led to an increase in performance of around 9\%. We also noted an additional 4\% improvement on both previously observed classes and overall performance when regularizer SD was combined with KD in our proposed subspace distillation method.
At the same time, the experimental findings displayed in \cref{tab:iou_benchmark_voc_2} demonstrate that the combination of subspace distillation (SD), $\ell_{\mathrm{SD}}^{CL}$, and memory replay, $\ell_{\mathrm{CE}}^{CL}$ yields substantial improvements in overall accuracy and forgetting on the S-CIFAR10 dataset. Specifically, there is an approximate increase of 9\% in accuracy and a reduction of 15\% in forgetting.

\begin{table}[htbp]
    \centering
    \begin{subtable}{0.44\textwidth}
        \centering
        \resizebox{\textwidth}{!}{\begin{tabular}{ccccccc}
        \hline
        \multicolumn{3}{c}{Method} & \multicolumn{4}{c}{19-1 (2 tasks)} \\ \cline{4-7} 
                CE    &  KD  &  SD  & \multicolumn{2}{c}{0-19 (Old Classes)} & \multicolumn{2}{c}{Overall} \\ \hline
                \cmark   &  \xmark   &   \xmark & \multicolumn{2}{c}{62.30} & \multicolumn{2}{c}{59.85} \\
                \cmark   &  \cmark   &   \xmark &  \multicolumn{2}{c}{71.43 (\textcolor{green}{+9.13})}    &  \multicolumn{2}{c}{69.15 (\textcolor{green}{+9.30})} \\
                    \cmark   &  \cmark   &   \cmark & \multicolumn{2}{c}{\textbf{76.09 (\textcolor{green}{+4.66})}}   & \multicolumn{2}{c}{\textbf{73.43 (\textcolor{green}{+4.28})}}    \\ \hline
        \end{tabular}}
    \caption{\label{tab:iou_benchmark_voc_1} Experimental results of Continual Learning (CL) methods on Pascal VOC dataset for continual (19-1) 2 tasks \textbf{overlap} class learning settings.}
    \end{subtable}
    \hfill
    \begin{subtable}{0.53\textwidth}
        \centering
        \resizebox{\textwidth}{!}{\begin{tabular}{ccccccc}
        \hline
        \multicolumn{3}{c}{Method} & \multicolumn{4}{c}{S-CIFAR10 (5 tasks; C-IL setting) } \\ \cline{4-7} 
        CE    &  Memory Replay  &  SD  & \multicolumn{2}{c}{Avg. Accuracy} & \multicolumn{2}{c}{Forgetting} \\ \hline
        \cmark   &  \xmark   &   \xmark &  \multicolumn{2}{c}{17.32}    & \multicolumn{2}{c}{80.65} \\
        \cmark   &  \cmark   &   \xmark &  \multicolumn{2}{c}{38.97 (\textcolor{green}{+21.65})}    & \multicolumn{2}{c}{43.34 (\textcolor{green}{-37.31})} \\
        \cmark   &  \cmark   &   \cmark &  \multicolumn{2}{c}{\textbf{47.68 (\textcolor{green}{+8.71})}}     &  \multicolumn{2}{c}{\textbf{27.86 (\textcolor{green}{-15.48})}}    \\ \hline
        \end{tabular}}
    \caption{\label{tab:iou_benchmark_voc_2} Experimental results of CL methods trained for one epoch with 500 memory exemplars on the S-CIFAR10 dataset.}
    \end{subtable}
    \vspace{-.25cm}\caption{\label{tab:iou_benchmark_voc_}Experimental results of (a) Continual Semantic Segmentation on Pascal VOC, and (b) Class-Incremenal laerning on S-CIFAR10 indicate that incorporating subspace distillation with memory replay significantly improves the performance of the CL method. }
\end{table}


\noindent
\textbf{Computational Complexity.} Our proposed distillation loss requires computing the subspace basis employing SVD. For an input feature vector $\vec{F} \in \mathbb{R}^{d \times m}; d> m$, computational complexity of SVD is $\mathcal{O}\big(d \times m \times \min(d, m)\big) = \mathcal{O}(dm^2)$. Assuming, $\vec{P}^{t} \in \mathbb{R}^{d \times n}$ and $\vec{P}^{t-1} \in \mathbb{R}^{d \times n}$ be the basis of top $n$ subspaces used in our computation, the computation cost of ${\vec{P}^{t}}^\top\vec{P}^{t-1}$ is $\mathcal{O}(n \times d \times n) = \mathcal{O}(dn^2)$. Therefore, the total computation cost of our proposed subspace distillation loss can be expressed as  $\mathcal{O}(dm^2 + dn^2)$. During our experiments on the 5-tasks CIFAR10 dataset, we found that one iteration of our replay-based subspace distillation (SD) method requires approximately 60ms on Tesla P100-SXM2 GPU for a batch of 32 samples, while the replay-based vanilla SGD method requires about 37ms. We rely on the assumption that a limited buffer memory is available throughout the learning process to store exemplars from prior tasks that might restrict the use of our method in privacy-focused applications.
