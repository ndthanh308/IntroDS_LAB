\section{Preliminaries}
\label{sec:preliminaries}

In continual learning, a model needs to learn from a series of tasks. Each learning task is represented by its training set, often samples from novel classes or concepts. 
Let $\mathds{T} = \{\mathcal{T}_{1}, \mathcal{T}_{2},\cdots,\mathcal{T}_{T} \}$ be a sequence of $T$ tasks. In the setup we are interested in this work, every task comprises of 
\[\mathcal{D}^t = \Big\{\big(\mat{X}_i,y_i\big)\Big\}_{i=1}^{n_t}\]
where $\mat{X}_i \in \mathcal{X}$ denotes a training image of size $W \times H$ and $y_i \in \mathcal{Y}$ is the corresponding class belonging to current task $t$. We also maintain a fixed size memory $\mathcal{M}$ to retain a small subset of samples from old tasks to better tackle the catastrophic forgetting. 
The goal of our knowledge distillation based approach is to apply constraints on updating weights of model so that the model generates similar latent representation and prediction in future tasks (\ie, $t, t+1, \cdots$) for what it has learned previously (\ie, $1, 2, \cdots, t-1$).

There is a subtle difference in setup when CL is considered for  semantic segmentation. For the problem of continual semantic segmentation, the training set consists of input samples and their corresponding segmentation mask. 
We denote the training set of CSS by
\[\mathcal{D}^t = \Big\{\big(\mat{X}_i,\vec{Y}_i\big)\Big\}_{i=1}^{n_t}\]
where $\vec{Y}_i \in \mathcal{Y}$ is the corresponding segmentation mask for the input image $\mat{X}_i$. Each pixel of the image belongs to a set of classes given by the current task $C^t$. Previously observed classes (\ie, $\mathbb{C}^{t-1}$) and future classes (\eg,  $\mathbb{C}^{t+1}$) are all labeled as the background class $c_{bg}$ for task $t$. In both continual semantic segmentation and classification tasks, at step $t$, the model should be able to predict all the observed classes, $\mathbb{C}^{1:t}$, throughout the learning experience.

\subsection{Evaluating an Incremental Learning Model}
\paragraph{\textbf{Class-Incremental Learning} The performance of continual learning methods are measured using the average task accuracy~\cite{chaudhry2018riemannian} in experiments.
Average accuracy is computed by average performance across all the previously observed and current tasks after training on current task $t$ and is defined as:
\begin{equation}
\mathrm{Acc}_{t} = \frac{1}{t}\sum _{i=1}^{t} \mathrm{Acc}_{t, i},
\label{eq:accuracy}
\end{equation}
where $Acc_{t, i}$ is the accuracy of task $i$ after learning task $t$.
}
\paragraph{\textbf{Continual Semantic Segmentation} Intersection Over Union (IOU)\cite{everingham2010pascal} metric is commonly used to evaluate the robustness towards catastrophic forgetting, in other words stability, and the ability of learning new class (\ie, plasticity) of CSS methods. IOU is computed at the end of learning all task at step $t$ for \textbf{(i)} initial set of classes $C^1$ at first task, (\textbf{ii)} incrementally leaned classes $C^{2:t}$, and \textbf{(iii) }all classes $C^{1 : t}$. IOU is defined as follows
\begin{align}
    \label{eqn:iou}
    \mathrm{IOU} =
    \dfrac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FP} + \mathrm{FN}}
\end{align}
where $TP$, $FP$ and $FN$ refers to true-positive, false-positive and false-negative, respectively. %
}

\subsection{Regularization with Knowledge Distillation}
Distilling knowledge from old model to the current model has shown promises to mitigate catastrophic forgetting of neural network in a CL setting~\cite{li2017learning, douillard2020podnet}. Here, the old model with the knowledge of already observed tasks acts as a teacher model and the purpose is to distill knowledge from teacher model to student model (\ie, current model) such that prediction of these two models matches for previously seen tasks samples. Knowledge distillation is often performed on the probability space to relate the temperature smoothed probability distribution of old to the new model~\cite{li2017learning, cermelli2020modeling}. Feature distillation on the other hand is performed on the feature space extracted from the intermediate layers of neural network to match corresponding local or global statistics of feature maps between old and new model~\cite{michieli2021continual, michieli2019incremental, douillard2020podnet, douillard2021plop}.

Assume that the extracted feature and the prediction using teacher model from step $t-1$ for input $\mat{X}$ are $\vec{f}^{t-1}$ and $\hat{y}^{t-1}$, respectively. Similarly, let $\vec{f}^{t}$ and $\hat{y}^{t}$ be the feature and prediction respectively for the same input using the student model at step $t$. The knowledge distillation is performed between teacher and student model by minimizing the following loss function
\begin{align}
    \mathrm{L}_{\text{kd}}(\mat{X};\Theta^t)
    \coloneqq - \sum_{c \in {\mathbb{C}^{1:t}}}^{} \hat{y}^{t-1}_c~\log\,(\hat{y}^{t}_c)\;.
    \label{eqn:kd_loss_cl}
\end{align}
Feature distillation strategy applies constraint on the similarity between old and new feature representation and is performed by minimizing a notion of distance between corresponding features of $\vec{f}^{t-1}$ and $\vec{f}^{t}$, such as the distance induced by the $\ell_1$ norm as
\begin{align}
    \text{L}_{\ell{1}}(\mat{X};\Theta^t) \coloneqq  \big\|{\vec{f}^{t-1} - \vec{f}^{t}} \big\|_1\;.
    \label{eqn:l1_loss_kd}
\end{align}



