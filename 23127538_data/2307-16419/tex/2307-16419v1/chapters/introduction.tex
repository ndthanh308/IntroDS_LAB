\section{Introduction}
\label{sec:intro}




\underline{C}ontinual \underline{L}earning (CL) is the process of robust, efficient and gradual learning in non-stationary environments. A fundamental aspect of intelligence is the capability of incrementally learning from sequential experiences. 
Equipping neural networks with CL capability requires the model to preserve its previously learned experiences while acquiring novel knowledge. Neural networks, trained in an offline mode%
, are currently the method of choice in a wide spectrum of problems in AI and machine learning. The underlying assumption here is that the model has the knowledge about all the decisions it should take in the future apriori. For example, all classes a model will encounter in future are known in an image classification task.
Furthermore, in offline training, data used for training the model in future steps should be i.i.d, otherwise internal representations learned by the model are hardly useful. 

In this paper, our focus is to design a mechanism that enables neural network model to learn continually in a dynamic environment.
One may wonder \emph{is it advantageous for a model to learn sequentially like humans?} Continual learning techniques will endow our machines to learn potentially  over a lifetime, as does a human. Furthermore, having visual understanding and semantic segmentation in mind, continual adaptation to a changing target specification enables the model to learn a diverse, and growing set of  classes. This aspect of continual learning is commonly considered as a necessity towards human-level artificial general intelligence~\cite{hadsell2020embracing}. Also, we note that  continual learning methods could offer profound advantages for models even in stationary settings, by enabling them to improve their efficacy without the need to train from scratch upon availability of new data.

In a continual learning setting, current \underline{D}eep \underline{N}eural \underline{N}etworks (DNNs) exhibits a drastic fall in the overall performance when the model is trained on a series of tasks. Precisely, in absence of samples from old tasks, the performance degrades on previously encountered tasks after the model is trained on a novel tasks. In other words, the knowledge from previous tasks gets overwritten thoroughly and DNNs forget  previously learned tasks abruptly once information relevant to novel tasks is presented~\cite{parisi2019continual}. This phenomenon of forgetting prior tasks because of the changes on critical weights related to previously observed tasks is often referred to as \textbf{Catastrophic Forgetting}~\cite{goodfellow2013empirical, kirkpatrick2017overcoming, robins1995catastrophic, french1999catastrophic, thrun1998lifelong}. Therefore, to design DNNs for \underline{C}ontinual \underline{L}earning, one needs to address Catastrophic Forgetting.



% Figure environment removed

Another problem of interest in this paper is \underline{C}ontinual \underline{S}emantic \underline{S}egmentation (CSS). Semantic segmentation~\cite{chen2018encoder, long2015fully, badrinarayanan2017segnet} is the task of  assigning a category label such as ``person'' or ``vehicle'' to  every single pixel of an image. In class-incremental CSS problem, a model is sequentially exposed to learn a set of novel classes. At the end of each training step, the CSS model is supposed to classify a pixel with all the seen classes until current task for evaluation. Aside from catastrophic forgetting, in CSS, we need to tackle another fundamental problem, namely \textbf{Background Shift}~\cite{cermelli2020modeling}.

In a conventional semantic segmentation setup, all object categories are predefined, and the class ``background'' encapsulates all other object categories that are not relevant to the problem at hand.
In contrast and in CSS, at each learning step, the class background merely corresponds to categories that do not belong to any of the classes at the current step (see Fig.~\ref{fig:bg_shift}).
As a result, the class \emph{background} contains not only pixels from \emph{unseen} and \emph{future classes} but also pixels from previously seen and old classes. 
This setting can be considered as a dense prediction task with noisy labels as the future unseen classes or old seen ones are grouped under a super-class named \emph{background}. If certain measures are not taken, the background shift could exacerbate the catastrophic forgetting even further.





A common way of addressing Catastrophic Forgetting and Background Shift is to distill the knowledge from old model to current one (see Fig.~\ref{fig:distillation}). Distillation methods such as LwF~\cite{li2017learning}, PODnet~\cite{douillard2020podnet}), often match the output/latent representation of a network, and hence ensuring that the prior knowledge remains unchanged in current model and the performance remains consistent on old tasks. %












In this paper, we introduce a structured form of knowledge distillation~\cite{hinton2015distilling} that is suitable for both \underline{C}ontinual \underline{L}earning with class-incremental setting and  \underline{C}ontinual \underline{S}emantic \underline{S}egmentation (CSS)~\cite{michieli2019incremental, cermelli2020modeling}. Precisely, we propose to distill the structure of the feature space in the intermediate layers of neural network to preserve the previously learned knowledge in the current model. 
% Figure environment removed
Our proposed method encodes the structure via low-dimensional subspaces. Subspaces have been used in a broad range of problems in computer vision to model the data manifold locally. Subspaces are robust to perturbation, and can be computed for high-dimensional data easily, hence has been employed with success for adapting neural networks~\cite{simon2020adaptive, zhang2019neural}. %
Therefore, to mitigate catastrophic forgetting in CL, we propose to maintain geometric structure of the feature space through encoding subspaces between models from sequential learning steps.
Our approach starts with decomposing extracted feature map in the intermediate layers of deep neural network followed by constructing structures of it through selecting prominent subspaces that approximate the data manifold to the first-order. This enables us to impose constraint to maintain similar subspace structures between old and new models. 
In our method, we formulate the constraints using the geometry of Grassmannian~\cite{edelman1998geometry}, and propose to minimize the distance between corresponding feature subspaces of old and current model.

As a motivating example, to examine the ability of subspace distillation in improving catastrophic forgetting by preserving prior knowledge, %
we evaluate the performance of subspace distillation by adding it to existing state-of-the-art CSS methods, \eg. ILT~\cite{michieli2019incremental}, MiB\cite{cermelli2020modeling}, and PLOP~\cite{douillard2021plop} and report the result in Fig.~\ref{fig:iou_improvement}. The result shows that the average IOU for all of the three methods improves significantly on the Pascal VOC dataset. %
Furthermore, our proposed subspace distillation algorithm can be merged with other distillation techniques seamlessly and provides them with complementary constraints to enforce structural similarities.

Overall, our contributions in this paper are as follows:
\begin{itemize}
  \item We propose a robust feature distillation strategy, namely Subspace Distillation (SD) to tackle catastrophic forgetting in CL through applying constraint on maintaining similar feature structure between old and new model.
  \item We present a generalized end-to-end continual learning framework using our proposed Subspace Distillation (SD) strategy in presence of a small subset of already observed samples from past tasks. %
  \item Our proposed Subspace Distillation (SD) strategy requires backpropagation through Singular Value Decomposition (SVD) method as it relies on SVD to compute the basis of subspaces. We show how this can be done in closed form solution by using partial derivative.
  \item Our proposed approach outperforms state-of-the-art continual learning methods on MNIST, CIFAR10 and Tiny-Imagenet datasets with varying memory size.
  \item We also show that a significant improvement can be achieved by combining subspace distillation strategy with existing methods for CSS on Pascal VOC dataset for different short and long task settings.
\end{itemize}


