\documentclass[journal, a4paper, draftclsnofoot, onecolumn, twoside, 12pt]{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[left=2.8cm,right=2.8cm,top=2.1cm,bottom=2.1cm]{geometry}
\usepackage{graphicx}
\usepackage{pst-grad}
\usepackage{pst-plot}
\usepackage{pst-text}
\usepackage{subfigure}
\usepackage{color}
\usepackage{indentfirst}
\usepackage{ulem}
\usepackage{subeqnarray}
\usepackage{cases}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{amsmath}
\usepackage{CJK}
\usepackage{amsmath}
\usepackage{multirow}

\usepackage{xcolor,colortbl}
\definecolor{Gray}{gray}{0.9}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}  %
\renewcommand{\algorithmicensure}{\textbf{Output:}} %
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proof}{Proof}
\newtheorem{proposition}{Proposition}
\newtheorem{collary}{Collary}
\newtheorem{example}{Example}
\newtheorem{problem}{Problem}
\newtheorem{assumption}{Assumption}
\newtheorem{property}{Property}
\newtheorem{remark}{Remark}


\def\eg{\emph{e.g}\onedot} 
\def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} 
\def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} 
\def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} 
\def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} 
\def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother

\def\Vec#1{{\boldsymbol{#1}}}
\def\Mat#1{{\boldsymbol{#1}}}
\newcommand{\st}{{\rm s.t.}\xspace}
\def\SPD#1{\mathcal{S}_{++}^{#1}}
\def\SYM#1{\operatorname{Sym}({#1})}
\def\GRASS#1#2{\mathcal{G}({#2},{#1})}
\def\ST#1#2{\mathrm{St}({#2},{#1})}
\newcommand{\DIAG}{\mbox{Diag\@\xspace}}


\def\TODO#1{{\color{red}{\bf [TODO:} {\it{#1}}{\bf ]}}}
\def\NOTE#1{{\bf [NOTE:} {\it\color{blue}{#1}}{\bf ]}.}
\def\CHK#1{{\bf [CHECK:} {\it\color{red} {#1}}{\bf ]}.}
\def\REFINE#1{{\color{violet}{\bf [REFINE:} {\it{#1}}{\bf ]}}}


\def\PF#1{{\color{magenta}{\bf [Pengfei:} {\it{#1}}{\bf ]}}}
\def\MH#1{{\color{purple}{\bf [Mehrtash:} {\it{#1}}{\bf ]}}}
\def\LP#1{{\color{blue}{\bf [Lars:} {\it{#1}}{\bf ]}}}
\def\SR#1{{\color{blue}{\bf [SR:} {\it{#1}}{\bf ]}}}

\title{\textbf{Cover Letter}}
\begin{document}

\maketitle


\noindent Dear Editorial team of Neural Networks,

~\\
\noindent We are submitting the accompanying manuscript entitled: \textbf{``Subspace Distillation for Continual Learning''} to be considered for publication in \textit{Neural Networks}.

~\\
\noindent In this document, we aim to provide brief explanations and highlight the contributions of our manuscript. This original work proposes a generic knowledge distillation strategy through encoding the low dimentional feature space through subspaces and ensuring similarities of latent manifold structure in consecutive two neural network models. The effectiveness of the proposed distillation strategy is evaluated on several challenging continual learning settings for both classification and semantic segmentation. 
In the image classification experiments with continual learning setting, our proposed distillation strategy outperforms baseline (Experience Replay) method in significant magin as shown in Table~\ref{tab:tab_cl}

\begin{table*}[htbp]
\centering
\resizebox{.9\textwidth}{!}{\begin{tabular}{ccccccc}
\hline
\multirow{2}{*}{Method} & \multicolumn{2}{c}{S-MNIST} & \multicolumn{2}{c}{S-CIFAR-10} & \multicolumn{2}{c}{S-Tiny Imagenet} \\ \cline{2-7} 
                        &        Task-IL           & Class-IL          & Task-IL       & Class-IL      & Task-IL          & Class-IL          \\ \hline
    & \multicolumn{5}{c}{Data Stream with Tiny Memory} & \\ \cline{3-5}
Baseline              & 97.72      & 73.80     & 77.85  & 32.87  & 28.07    & 5.85  \\
\rowcolor{Gray}
\textbf{Ours}   & \textbf{98.35}      &  \textbf{79.37}    &  \textbf{81.65 }   & \textbf{35.1}   &   \textbf{30.11}    & \textbf{6.05} \\ \hline
    &  \multicolumn{5}{c}{Small Memory} & \\ \cline{3-5}
Baseline              & 97.86      &  80.43     & 91.19  & 44.79  & 38.17   & 8.49  \\
\rowcolor{Gray}
\textbf{Ours}   & \textbf{97.71}      &  \textbf{85.28}    &  \textbf{92.88 }             & \textbf{61.85}      &   \textbf{39.52}      & \textbf{8.54} \\ \hline
    &  \multicolumn{5}{c}{Medium Memory} & \\ \cline{3-5}
Baseline        & 98.89      &  86.57     & 93.60  & 57.74  & 48.67     & 9.96  \\
\rowcolor{Gray}
\textbf{Ours}   & \textbf{ 99.00 }     &  \textbf{ 89.00}    &  \textbf{94.86 }             & \textbf{71.85}    &   \textbf{ 48.60}     & \textbf{ 10.03}   \\ \hline
    &  \multicolumn{5}{c}{Large Memory Buffer} & \\ \cline{3-5}
Baseline        &  99.33      &  93.40     & 96.97  & 82.43  & 67.29     & 27.37    \\ \rowcolor{Gray}
\textbf{Ours}   &  \textbf{ 99.69}    &  \textbf{ 95.90} & \textbf{ 97.18}      &  \textbf{84.75}    &   \textbf{69.13}    &   \textbf{29.32} \\ \hline
\end{tabular}}
\caption{\label{tab:tab_cl}Average top-1 accuracy on splited MNIST, CIFAR-10 and Tiny-Imagenet for 5, 5 and 10 tasks respectively for standard L3 benchmarks with a fixed memory samples that are randomly stored from the data stream using reservoir sampling strategy.}
\end{table*}

Subspace Distillation method also improves tackling catastrophic forgetting, mean IOU of last task, and average IOU across the tasks of baseline method (MiB) for continual semantic segmentation as presented in Table~\ref{tab:tab_css}.

\begin{table*}[htbp]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{cccccccccc}
\hline
\multirow{2}{*}{Method} & \multicolumn{3}{c}{19-1 (2 tasks)}                   & \multicolumn{3}{c}{15-5 (2 tasks)}                      & \multicolumn{3}{c}{15-1 (6 tasks)} \\ \cline{2-10} 
                        & 0-19 & mIOU & \multicolumn{1}{c|}{AvgIOU} & 0-15 & mIOU & \multicolumn{1}{c|}{AvgIOU} & 0-15 & mIOU & AvgIOU \\ \hline

Baseline                       & 71.43   & 69.15    & 73.28                         & 76.37     & 70.08    & 75.12                         & 34.22      & 29.29    & 54.19    \\
\rowcolor{Gray} Ours                      &  \textbf{76.09}      & \textbf{73.43}    & \textbf{75.82}              & \textbf{78.10}    & \textbf{71.70}    & \textbf{75.98}                     & \textbf{48.90}      &  \textbf{39.23}   & \textbf{59.77}    \\ \hline
\end{tabular}}
\caption{\label{tab:tab_css}Performance of different continual semantic segmentation method in IOU on Pascal VOC dataset for three continual \textbf{overlap} class learning settings: (i) 19-1 (ii) 15-5 and (iii) 15-1 tasks. }
\end{table*}

Furthermore, we derive a close form solution to backpropagate through SVD using chain rule and partial derivative as our distillation method uses SVD for computing basis of subspaces and requites to backpropagate through SVD. \\ 
~\\
\noindent Given all the above, we feel that the current manuscript presents novel and integrated contributions to the field. Along with scientific contributions, we also believe that the spirit of our work, exploiting kernel machines in learning solutions, is ideally in line with the topic of the special issue (i.e., \textit{Lifelong Learning}). Therefore, we would like to request that our manuscript be considered for publication at \textit{Neural Networks}.

~\\
\noindent All co-authors have seen and agreed with the contents of this manuscript. We have no relevant conflicts of interest, and this manuscript has not been copyrighted, published, submitted, or accepted for a journal publication elsewhere.   


~\\
\noindent Sincerely,

~\\
\noindent Kaushik Roy

\noindent Ph.D candidate

\noindent Department of Electrical and Computer Systems Engineering

\noindent Monash University

\noindent Ph: +61 426776437

\noindent Email: kaushik.roy@monash.edu





 
\end{document}
