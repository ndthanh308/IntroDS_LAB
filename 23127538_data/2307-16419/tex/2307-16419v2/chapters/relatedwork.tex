

% Figure environment removed

\section{Related Work}
\label{sec:related_work}





In this section we discuss related works in class-incremental learning and continual semantic segmentation as the targeted application. 


\subsection{Continual Learning}

A variety of methods has been proposed to alleviate catastrophic forgetting for class-incremental classification problems~\cite{kirkpatrick2017overcoming, li2017learning, douillard2020podnet, rolnick2019experience, mallya2018packnet, yoon2017lifelong}. %
These methods for continual learning are classified primarily into three categories, such as regularization, dynamic architecture and memory replay based methods ~\cite{ebrahimi2019uncertainty}.


\noindent \textbf{Regularization based methods} preserve already learned information by imposing constraints on the update of weight~\cite{aljundi2018memory, chaudhry2018riemannian, kirkpatrick2017overcoming, zenke2017continual}, intermediate feature representation~\cite{douillard2020podnet, hou2019learning, dhar2019learning}, prediction~\cite{li2017learning, rebuffi2017icarl}, gradient~\cite{lopez2017gradient, chaudhry2018efficient}, or combination thereof.  Li \etal{}~\cite{li2017learning} in Learning without Forgetting (LwF) introduced knowledge distillation strategy in the output layer to minimize the dissimilarity between old task and new one. The significance of each synapses is measured to penalize update on most influential synapses and a surrogate loss function to estimate loss for old tasks is used with modified cost function in SI~\cite{zenke2017continual}. %
Hou \etal in~\cite{hou2019learning} proposed cosine normalization to combat catastrophic forgetting and facilitate the seamless integration of new and prior knowledge within a continual learning model. By adjusting the magnitudes of the model's weights, cosine normalization provides precise control over the influence of novel tasks. This mechanism ensures that the model does not prioritize the new task at the expense of previously learned tasks, thereby preserving valuable knowledge while accommodating novel knowledge. This normalization approach helps to mitigate the negative impact of catastrophic forgetting and enhances the model's ability to generalize across multiple tasks. Cheraghian~\etal in~\cite{cheraghian2021semantic} introduced a semantic-aware distillation loss for few-shot class incremental learning that takes into account the semantic structure of the data. The distillation strategy utilizes semantic embeddings associated with each class to guide the distillation process. The integration of semantic information fosters the preservation of prior knowledge in the continual learning (CL) model by facilitating learning not only from the representations of the previous model but also from the semantic relationships among the classes. During the incremental learning process, PODNet in~\cite{douillard2020podnet} used a Pooled Outputs Distillation (POD) mechanism to transfer knowledge from the previously learned tasks to the current task. Specifically, the outputs of the intermediate layers of the previous model are pooled and distilled into the corresponding layer of current model through minimizing the discrepancies using Euclidean distance of L2-normalized features. Mathematically, POD can be expressed as 
$L_{POD} = \left\|f_i^{old}(\vec{x}) - f_i^{new}(\vec{x})\right\|^2$,
where $f_i^{old}(\vec{x})$ and $f_i^{new}(\vec{x})$ are the pooled output of the $i^{th}$ spatial position for input $\vec{x}$ using old and new model respectively.
One of the limitations of PODNet is the absence of an explicit mechanism to effectively preserve the underlying latent structure, as it primarily relies on imposing constraints on the pooled features. Consequently, the presence of outliers or noisy attributes can potentially compromise the effectiveness of distillation in PODNet. In contrast, our proposed subspace distillation approach addresses this concern by imposing constraints on the low-dimensional subspaces derived from the latent features of both the old and current models. This not only enhances robustness to noise but also ensures the preservation of latent structure in continual learning scenarios.



\noindent \textbf{Dynamic architecture based methods} allocate new neurons to adapt to novel task. Andrei \etal{}~\cite{rusu2016progressive} introduced progressive network where old knowledge remains unchanged by keeping previously trained model frozen and a novel sub-network with fixed resources is allocated to learn new knowledge. Yoon \etal{}~\cite{yoon2017lifelong} proposed dynamically extendable network (DEN) that learns a compact representation by selective training and expanding neural network capacity by optimal number of units when new task arrives. Recently, Douillard~\etal in~\cite{douillard2021dytox} proposed first transformer based architecture where dynamic expansion of task specific token is used.

\noindent \textbf{Memory-based methods} partially stores the previous data and train model by replaying stored old data together with new data~\cite{rebuffi2017icarl, gepperth2016bio}. Rebuffi~\etal{}~\cite{rebuffi2017icarl} introduced iCARL method where herding based sample selection was used to keep a small portion of previous dataset in the memory and replayed interleaved with new samples.
Many recent approaches have extended iCARL to bias correction problem for classifier~\cite{javed2018revisiting}, a metric learning model for imbalance dataset~\cite{hou2019learning}, a memory sampling method~\cite{aljundi2019gradient, aljundi2019online, isele2018selective}. %
Javed \etal{}~\cite{javed2018revisiting} introduced a dynamic threshold moving method to address the classifier bias generated by the knowledge distillation approach in iCARL. Hou~\etal~\cite{hou2019learning} in LUCIR proposed to combine inter-class separation constraint, old classes geometric structure preserving constraint and cosine normalization to tackle imbalance dataset problem. Aljundi~\etal~\cite{aljundi2019gradient} claimed that memory sampling method is crucial, and accuse random memory sample selection for sub-optimal performance on old tasks. In~\cite{aljundi2019gradient} replay memory sampling is defined as a constrained optimization problem and formulated as solid angle minimization problem to maximize the diversity in the replay memory. While in iCARL, the memory constitutes samples randomly chosen based on the cluster centers, in~\cite{aljundi2019online} and~\cite{isele2018selective}, diverse sample selection mechanisms, including most interfered sample retrieval and global distribution matching based sampling are utilized. %

To address the security concern, few recent approaches employed generative model to produce samples belonging to old classes~\cite{shin2017continual} instead of storing subset of old samples in memory. Deep Generative Replay (DGR)~\cite{shin2017continual} proposed a dual-model architecture, one for generating pseudo samples and another for solving tasks by replaying pseudo samples together with new samples. To reduce the memory footprint of storing real samples, compressed feature have been stored in~\cite{hayes2020remind, iscen2020memory}. REMIND~\cite{hayes2020remind} used product quantization method to quantize latent representation and stored indices in memory that were used later to decode the representation for reply. However, because of incremental update in model, stored latent representation also requires adaptation. To fit the stored representation into current latent space, Iscen~\etal ~\cite{iscen2020memory} employed a multi-layer perceptron to map corresponding old and new feature map generated for images of current task.%

Recently, Dark Experience Replay (DER++)~\cite{douillard2021tackling} proposed to store both logit, and label for corresponding sample and used the reservoir sampling strategy to select samples from data stream for memory buffer. In DER++, knowledge distillation is performed by mapping output logit from current model with corresponding memory logit.



    

\subsection{Continual Semantic Segmentation}

In recent time, a growing number of works have emerged into continual semantic segmentation. In the literature, CSS methods primarily fall into two major categories: (i) regularization and (ii) Replay based model.

Following the success of regularization methods in CL, several works have proposed mechanism for controlling update of neuron weights to mitigate catastrophic forgetting in CSS~\cite{michieli2019incremental, cermelli2020modeling, douillard2021plop, michieli2021continual}. ILT~\cite{michieli2019incremental} investigates a \textbf{regularization-based technique} by freezing weights of the encoder network after learning the first task and applying knowledge distillations for the upcoming tasks. %
ILT is also equipped with the masked cross-entropy loss and masks on the output of the current model to consider only the seen classes. However, this approach doesn't resolve the background shift problem of CSS properly. %
Since the background in CSS may include previously seen objects, MiB~\cite{cermelli2020modeling} proposed to preserve the knowledge of an old model with knowledge distillation. However, the difference compared to standard knowledge distillation is that the probabilities of the background and the novel classes are appended such that the non-overlapping prediction of a novel class as the output of a current model can be aligned with the output of a previous model.
As knowledge distillation shows a promising direction for CSS, PLOP~\cite{douillard2021plop} introduced pseudo-labelling to tackle the background shift problem and adapt Pooled Out Distillation (POD)~\cite{douillard2020podnet} to preserve the previously learned knowledge. Since POD is developed for a classification problem in classical continual learning settings with reliance on global statistics, thus %
PLOP uses a multi-scale version of POD to integrate local and global statistics at different intermediate layers. %
SDR~\cite{michieli2021continual} leveraged contrastive learning together with novel sparsity constraint and prototype matching strategy to efficiently learn novel tasks and mitigate forgetting of prior knowledge. In SDR, to organize geometric structure of feature representation, clusters of data are described using prototypes that are forced to be closer in consecutive learning steps and far apart from one another by using prototype matching and repulsive force. Additional sparsity constraint imposed on feature representation of same classes helped to construct well-separated and tight cluster as well as create space for accommodating novel classes. %

\noindent
\textbf{Replay based methods} tackle the catastrophic forgetting in CSS by either storing real images from past tasks~\cite{cha2021ssul} or generating synthetic data of prior classes~\cite{maracani2021recall}. Cha~\etal in SSUL~\cite{cha2021ssul} proposed to extract future classes from background that are defined as unknown classes to facilitate learning novel classes. SSUL used a subset of old samples  for the first time in the literature of CSS to improve stability and plasticity of model. Additionally, freezing weights of encoder and old classifiers with binary cross entropy loss, and pseudo-labeling techniques helped to improve catastrophic forgetting in~\cite{cha2021ssul}. Maracani~\etal~\cite{maracani2021recall} employed generative model to produce samples from previously seen tasks together with newly proposed background impainting method for pseudo-labeling to tackle background shift and catastrophic forgetting.

