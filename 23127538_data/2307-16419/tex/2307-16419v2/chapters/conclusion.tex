\section{Conclusion}
\label{sec:conclusion}
In this work, we propose a generalised end-to-end continual learning framework where subspace distillation is at the core of it. Here, we model low dimensional intermediate feature representations %
using subspaces. By imposing constraint on maintaining similar subspace between old and new model, we ensure robustness in the model towards catastrophic forgetting. Proposed subspace distillation is equally effective for classification and semantic segmentation problem in continual learning scenarios. Empirical analysis shows that our proposed framework with subspace distillation achieves state-of-the-art performance in multiple settings on Pascal-VOC dataset for continual semantic segmentation and MNIST, CIFAR10, and Tiny-Imagenet datasets for class-incremental classification problem. In future, we would like to investigate more about the efficient use of subspace distillation for long tasks setting for CSS as well as for complex continual learning setting when model is trained on data stream that is presented to model once at training time.
