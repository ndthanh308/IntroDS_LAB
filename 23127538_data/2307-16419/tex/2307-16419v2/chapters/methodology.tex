\section{Methodology}
\label{sec:methodology}


In this section, we first discuss our main contribution, \emph{the \underline{S}ubspace \underline{D}istillation (SD)} , and its properties. Next, we will elaborate on how the SD will be used for classification and segmentation problems. 


\paragraph{\textbf{Subspace Distillation}}



Let $\vec{F}_{i}^{t} \in \mathbb{R}^{d \times p}$ and $\vec{F}_{i}^{t-1} \in \mathbb{R}^{d \times p}$ be the extracted features from layer $i$ of DNNs at step $t$ and $t-1$, respectively. The goal of feature distillation is %
to ensure that the learned knowledge at step $t-1$ remain unchanged at step $t$ while training DNNs on novel dataset $\mathcal{D}^{t}$. To avoid cluttering equations, from now on we drop the layer index, unless it is not clear from the context. In conventional feature distillation approaches, old feature maps or statistics are directly matched with corresponding new one.
That is, in order to obtain $\vec{f}_{i}^{t-1}, \vec{f}_{i}^{t}$ from $\vec{F}_{i}^{t-1}, \vec{F}_{i}^{t}$ for distillation per~\cref{eqn:l1_loss_kd}, different pooling strategies (\ie., channel, height or width pooling) have been discussed in PODnet~\cite{douillard2020podnet}. %

Our hypothesis here is that distilling geometric structure of feature distribution can enrich the distillation process and will help mitigating the catastrophic forgetting in continual learning. In doing so, we propose to model feature maps $\vec{F}^{t}$ and $\vec{F}^{t-1}$ with a set of subspaces, $\mathbb{S} = \{\mathcal{S}_{j}\}_{j=1}^\tau$. Soon, we will discuss how the set of subspaces will be constructed for the classification and segmentation problems, but for now, we focus on the main idea. Each subspace $\mathcal{S} \in \mathbb{S}$ is represented by its basis as 
$\mathbb{R}^{d \times m} \ni \vec{P}; m \ll d$
with $\vec{P}^\top \vec{P}=\mathbf{I}_m$. Note that, $m \leq p$. Our goal here is to preserve the subspace structure of the intermediate feature maps extracted from different layers of old model to the new one. We argue that, robust knowledge distillation through maintaining similarity across subspaces will be advantageous to improve continual classification/segmentation paradigms. This is achieved by enforcing a constraint on subspace similarity between the old and new model.
To do so, we propose to minimize the distance between corresponding subspace constructed from feature maps of the old and new model. \\
A valid distance between $\mathcal{S}_i$ and $\mathcal{S}_j$ is a distance that is invariant to the choice of 
the basis of the subspace. To be more specific, assume $\vec{P}_i \in \mathbb{R}^{d \times m}$ and $\vec{P}_j \in \mathbb{R}^{d \times m}$ are the basis for $\mathcal{S}_i$ and $\mathcal{S}_j$, \ie,  $\vec{P}_i\vec{P}_i^\top = \vec{P}_j\vec{P}_j^\top = \mathbf{I}_m$. Then a distance between 
$\mathcal{S}_i$ and $\mathcal{S}_j$ should meet : 
\[
d(\mathcal{S}_i, \mathcal{S}_j) = g(\vec{P}_i,\vec{P}_j) = g(\vec{P}_i\vec{R}_i,\vec{P}_j\vec{R}_j)\;,
\]
where $g:\mathbb{R}^{d \times m} \times \mathbb{R}^{d \times m} \to \mathbb{R}_+$ is the distance function and $\vec{R}_i, \vec{R}_j \in \mathcal{O}_m$
with $\mathcal{O}_m$ denoting the orthogonal group.

\noindent
To be precise, given $\vec{P}_i^t$ and $\vec{P}_i^{t-1}$, we opt to minimize the projection metric~\cite{harandi2015extrinsic} as
\begin{align}
    \label{eqn:sd_metric}
    \delta_{p}^2(\vec{P}_i^{t},\vec{P}_i^{t-1}) \coloneqq 
    \left\| \vec{P}_i^{t\top}\vec{P}_i^t - \vec{P}_i^{t-1\top}\vec{P}_i^{t-1} \right\|^2_F = 2m - 2\left\| \vec{P}_i^{t\top}\vec{P}_i^{t-1}\right\|^2_F\;.
\end{align}
The projection metric is a proper distance on Grassmannian and endows intriguing properties, among them, the length of curves on Grassmannian obtained by  $\delta_{p}(\cdot,\cdot)$ 
is simply related to the length obtained by the geodesic distance via a fixed constant~\cite{harandi2015extrinsic}. 
In order to illustrate the rationale behind computing subspace distance using Eq.\eqref{eqn:sd_metric}, we present a trivial example involving the XY plane residing within three-dimensional space ($\mathbb{R}^3$) as a two-dimensional subspace. Consider the XY plane in $\mathbb{R}^3$ as a 2D subspace in 3D space. Both 
 
\begin{minipage}{0.4\textwidth}
\begin{align*}
    \vec{P}_1 & = 
\begin{pmatrix}
1 &0\\
0 &1\\
0 &0\\
\end{pmatrix}
\end{align*}
\end{minipage}%
\begin{minipage}{0.1\textwidth}
and
\end{minipage}
\begin{minipage}{0.5\textwidth}
\begin{align*}
\vec{P}_2 &= 
\begin{pmatrix}
\cos(\pi/4) &-\sin(\pi/4)\\
\sin(\pi/4) &\cos(\pi/4)\\
0 &0\\
\end{pmatrix}
\end{align*}
\end{minipage}
represent the XY plane; hence their distance should be zero. The distance used in Eq.\eqref{eqn:sd_metric}, \ie, $\left\| \vec{P}_i^{t\top}\vec{P}_i^t - \vec{P}_i^{t-1\top}\vec{P}_i^{t-1} \right\|^2_F$ not only satisfies the required conditions, but also closely related to the geodesics on the Grassmannian~\cite{harandi2015extrinsic}.\\
In Eq. ~\eqref{eqn:sd_metric}, the mapping $f:\mathbb{R}^{d \times m} \to \mathbb{R}^{m \times m}; f(\vec{P}) =\vec{P}\vec{P}^\top$ is a diffeomorphism between Grassmannian and the space of  symmetric matrices (positive semidefinite to be adequate). The induced distance $\delta_{p}^2(\vec{P}_i^{t},\vec{P}_i^{t-1}) = \left\| \vec{P}_i^{t\top}\vec{P}_i^t - \vec{P}_i^{t-1\top}\vec{P}_i^{t-1} \right\|^2_F$ is invariant to the action of the orthogonal group, satisfying the requirement of having a Grassmannian distance. 
Furthermore, since $\vec{P}^\top\vec{P}=\mathbf{I}_m$, the distance can be simplified to $\delta_{p}^2(\vec{P}_i^{t},\vec{P}_i^{t-1}) = 2m - 2\left\| \vec{P}_i^{t\top}\vec{P}_i^{t-1}\right\|^2_F$, which is computationally very attractive in comparison to geodesics on Grassmannian that require computing SVD.

Before formulating the SD loss for continual learning, note that in practice and in order to construct the subspace representing feature map, we rely on  matrix decomposition techniques, and in particular on Singular Value Decomposition (SVD). In other words, given $\vec{F}_{i}^{t}$ and $\vec{F}_{i}^{t-1}$, we first apply SVD to attain  
$\vec{P}_i$ and $\vec{P}_i^{t-1}$ and then use them accordingly for distillation. This operation differs from many common operations in deep learning, in the sense that a somewhat complicated matrix operation is involved, hence one may wonder how backpropagation will work in this case. 


\noindent {\textbf{Backpropagation through SVD.}}
First note that for $\delta_{p}^2(\vec{P}_i^{t},\vec{P}_i^{t-1})$, we have  
\begin{align}
    \nabla_{\vec{P}} \triangleq \frac{\partial}{\partial \vec{P}_i^{t}} \delta_{p}^2\big(\vec{P}_i^{t},\vec{P}_i^{t-1}\big)
     = \frac{\partial}{\partial \vec{P}_i^{t}} \bigg\{ 2m -2\left\| \vec{P}_i^{t\top}\vec{P}_i^{t-1}\right \|^2_F \bigg\} = 
     -2 \frac{\partial}{\partial \vec{P}_i^{t}} 
    \Tr\Bigg(\Big(\vec{P}_i^{t\top}\vec{P}_i^{t-1}\Big)^{\top}\Big(\vec{P}_i^{t\top}\vec{P}_i^{t-1}\Big)\Biggl) 
    = -4 \vec{P}^{t-1}\Big(\vec{P}^{t-1}\Big)^{\top}\vec{P}^{t}\;.
    \label{eqn:deriv_P_proj_metric}
\end{align}

Please note that the subspace from previous model acts as a teacher for distillation and we only need  the gradient with respect to $\vec{P}_i^{t}$ to update our current model, hence the above derivation. The next step to update the weights of the current model is to obtain the gradient of the loss with respect to $\vec{F}_{i}^{t}$, which requires us to backpropagate $\nabla_{\vec{P}}$ through the SVD operation. 
By applying chain rule (see~\cite{ionescu2015matrix} for backpropagation through matrix operations) ,
\begin{align}
    \Tr \Big( \nabla_{\vec{P}}^\top \vec{P}_i^{t} \Big) =
    \Tr \Big( \nabla_{\vec{F}}^\top \vec{F}_i^{t}\Big)\;,
    \label{eqn:chain_rule_mbp}
\end{align}
with 
\begin{align}
    \nabla_{\vec{F}} \triangleq \frac{\partial}{\partial \vec{F}_i^{t}} \delta_{p}^2\big(\vec{P}_i^{t},\vec{P}_i^{t-1}\big)\;.
\end{align}
Feature map $\vec{F}_i^{t}$ is decomposed as $\vec{F}_i^{t}=\vec{P}_i^{t}\vec{\Sigma}\vec{Q}^{\top}$, where $\vec{F}_i^t \in \mathbb{R}^{d \times p}$, $\vec{P}_i^t \in \mathbb{R}^{d \times m}$, $\vec{\Sigma} \in \mathbb{R}^{m \times p}$, $\vec{Q} \in \mathbb{R}^{p \times p}$ and $m \le p$ with the constraints $\big(\vec{P}_i^t\big)^\top\vec{P}_i^t = \vec{Q}^{\top}\vec{Q} = \mathbf{I} $. It can be shown that (see \ref{app:full_derivation} for the derivation)

\begin{align}
    \label{eqn:svd_diff_final_form}
    \nabla_{\vec{F}} = \vec{D}\vec{Q^{\top}} - \vec{P}_i^t \Big( \big(\vec{P}_i^t\big)^{\top}\vec{D}\Big)_{\text{diag}}\vec{Q}^{\top} - 2\vec{P}_i^t\vec{\Sigma}\Big(\vec{K}^{\top} \circ \big(\vec{D}^{\top}\vec{P}_i^t\vec{\Sigma}\big)\Big)_{\text{sym}}\vec{Q}^{\top}\;,
\end{align}
where    $\vec{K} \in \mathbb{R}^{p \times p}$ is defined as follows
 
\begin{equation}
    \label{eqn:kij}
    \vec{K}_{ij} =    \begin{cases}
    \dfrac{1}{\sigma_i^2 - \sigma_j^2}, &i \neq j \\
    0, &i=j
    \end{cases}
\end{equation}

\begin{align}
    \label{eqn:svd_diff_accompany_matrix_D}
    \vec{D} = \nabla_{\vec{F}}\vec{\Sigma}^{-1}_{m}\;.
\end{align}
Here, $\vec{\Sigma}_{m} \in \mathbb{R}^{m \times m}$ consists of top $m$ rows and first $m$ columns of $\vec{\Sigma}$.
Note that, $\mat{A}_{\text{diag}}$ is the diagonal part of $\mat{A}$ (\ie, all off-diagonal elements are set to $0$).

\subsection{Subspace Distillation for Continual Classification}

For the task of continual classification, we distill the subspace constructed across samples in a mini-batch. For a mini-batch 
$\big(\mathcal{X}_B, \mathcal{Y}_B\big)$ of size $b$, let $\vec{f}_{j} \in \mathbb{R}^d$ be the latent representation for input $\vec{X}_{j} \in \mathcal{X}_B$. The formulation below can be applied to any layer in a DNN and hence we drop the layer index for the sake of simplicity. 
Assume there are $p = \lfloor b/|\mathbb{C}^t|\rfloor$ samples per class in the mini-batch. In SD, we propose to compute \underline{class-wise subspaces} using latent features generated from both old and new model. 
More specifically, from the mini-batch, we form $\{\vec{F}^{t-1}_{k}\}_{k=1}^{|\mathbb{C}^t|}$ and $\{\vec{F}^t_{k}\}_{k=1}^{|\mathbb{C}^t|}$ with $\vec{F}^{t-1}_{k},\vec{F}^t_{k} \in \mathbb{R}^{d\times p}$ by stacking corresponding samples in class $k$ into a matrix (\ie, $\vec{F}^t_{k} = \big[\vec{f}_{k,1},\cdots,  \vec{f}_{k,p}\big]$, where $\mathcal{Y}_B \ni y_{k,i} = k$). We note that stacking ordering is not important in forming $\vec{F}^{t-1}_{k}, \vec{F}^t_{k}$ as we are merely interested in subspace spanned by the samples.
Next, we represent each class $k$ from the teacher and the student model by its low-dimensional subspace $\vec{P}^{t-1}_{k}, \vec{P}^t_{k} \in \mathbb{R}^{d \times m}, m \leq p$. This is achieved by applying thin SVD to $\vec{F}^{t-1}_{k}, \vec{F}^t_{k}$ and picking up the top left singular vectors. With the above, the SD loss is defined as 


\begin{align}
    \label{eqn:sd_loss_cl}
    \ell_{\text{SD}}^{\text{CL}}\big(\mathcal{X}_B, \mathcal{Y}_B \big) \coloneqq
    \dfrac{1}{|\mathbb{C}^t|}\sum_{k=1}^{|\mathbb{C}^t|} \Big( 2m -2\left\Vert \vec{P}_k^{t\top}\vec{P}_k^{t-1}\right\Vert^2_F\Big)\;.
\end{align}



\subsection{Subspace Distillation for Continual Semantic Segmentation}

For class-incremental semantic segmentation problem, we propose to compute \underline{subspace across the channel dimension} of intermediate feature map for each sample in a batch. To reduce the memory overhead while performing subspace distillation, instead of computing subspace from all feature maps at a layer, we split the full feature maps into several smaller group $G$ with $p$ channels. To compute a subspace from each  group, we need to form a matrix representation from the  feature map. To do so, for a particular input, we form $\{\vec{F}^{t-1}_{g}\}_{g=1}^{G}$ and $\{\vec{F}^t_{g}\}_{g=1}^{G}$ with $\vec{F}^{t-1}_{g},\vec{F}^t_{g} \in \mathbb{R}^{d\times p}$ from the feature maps. Here, $d=wh$, where $h$ and $w$ denote the height and width of the feature map. 
We encode the geometry of $\vec{F}^{t-1}_{j},\vec{F}^t_{j}$ by low-dimensional subspaces  
$\vec{P}^{t-1}_{g},\vec{P}^t_{g} \in \mathbb{R}^{d \times m}$ via SVD. The subspace distillation loss is defined as 



\begin{align}
    \label{eqn:sd_loss_css}
    \ell_{\text{SD}}^{\text{CSS}}\big(\vec{X}\big) \coloneqq
    \dfrac{1}{G}\sum_{g=1}^{G} \Big( 2m -2\left\Vert \vec{P}_g^{t\top}\vec{P}_g^{t-1}\right\Vert^2_F\Big)\;.
\end{align}











\subsection{Class-Incremental Continual Learning using Subspace Distillation}
In class-incremental learning, we maintain a fixed memory to store a subset of samples using reservoir sampling strategy~\cite{reservoir} from prior tasks. The samples in the memory are subsequently used during training the model on a novel task. We compute distillation loss between subspace constructed from latent feature maps extracted by feeding memory samples to old and new model. Afterwards, by minimizing subspace distillation loss combined with classification loss on novel and memory samples, we adapt DNNs model for class-incremental learning.
The  classification loss used in conjunction with the subspace distillation loss is the cross entropy defined  as: 
\begin{align}
    \label{eqn:ce_loss_cl}
    \ell_{\text{CE}}^{\text{CL}}(\mat{X}, y) =
    - \sum_{c \in {C^{1:t}}}^{} y_c~\log~(\tilde{y}^{t}_{c})\;,
\end{align}
where, $y_c$, and $\tilde{y}^{t}_c$ are the true label and predicted probability for class $c$ respectively for the input $X$ and $\tilde{y}^{t} = f_{\Theta^t}(\mat{X})$.
Putting everything together, the overall  loss used to train our model to tackle catastrophic forgetting in continual class incremental learning is
\begin{align}
    \label{eqn:overall_loss_cl}
    \mathcal{L}_{\mathrm{CL}}(\mathcal{D}^t;\Theta^t) \coloneqq 
    \E\nolimits_{(\mat{X},y) \sim \mathcal{D}^{t},~(\mat{X'},y') \sim \mathcal{M}} \Big[
    \ell_{\mathrm{CE}}^{\text{CL}}\big(\mat{X}, y\big)
    + \alpha \ell_{\mathrm{CE}}^{\text{CL}}\big(\mat{X'}, y'\big)
    + \beta \ell_{\mathrm{SD}}^{\text{CL}}\big(\mat{X'},y'\big)
    \Big]\;,
\end{align}
\noindent
where, $\vec{X}$ represents the images belonging to novel task while $\vec{X^{'}}$ represents the examples from the memory buffer. $\alpha$ and $\beta$ are hyper-parameter used to control the contribution of the second loss term and subspace distillation, respectively. We present the overall steps of training a continual learning model using subspace distillation in \cref{alg:cl}.

\begin{algorithm}[H] 
\caption{Class-Incremental Learning using Subspace Distillation}
\label{alg:cl}
\begin{algorithmic}[1]
\Require{$\text{Dataset}~\mathcal{D}^t,~\text{Memory}~\mathcal{M},~\text{and Model from step}~t-1,~h_{\Theta^{t-1}} = h_{\text{feat}}^{t-1} \circ h_{\text{cls}}^{t-1}$}
\Ensure{The new model at time $t$ with parameters $\Theta^t$}
    \State {$\text{Initialize }\Theta^t~\text{ with }\Theta^{t-1}$}
    \For {iteration $1$  to  max\_iter}
        \State {Sample a mini batch $(\mathcal{X}_B, \mathcal{Y}_B)$ from ${\mathcal{D}^t}$}
        \State {Sample a mini batch $(\mat{X}', {y}')$ from the memory ${\mathcal{M}}$}
        
        
        \State {$ \mathcal{\tilde{Y}}_B \leftarrow h_{\Theta^t}(\mathcal{X}_B)$}
        
        \State $\vec{f}^t \leftarrow h_{\text{feat}}^t(\mat{X}')$
        \State {$\tilde{y}' \leftarrow h_{\text{cls}}^{t}(\vec{f}^t)$}

        \State $\vec{f}^{t-1} \leftarrow h_{\text{feat}}^{t-1}(\mat{X}')$
        
        
        \State {Compute Cross Entropy loss, $\ell_{\mathrm{CE}}$ between ground truth $\mathcal{Y}_B$ and prediction $\mathcal{\tilde{Y}}_B$ using Eq. \eqref{eqn:ce_loss_cl}}
        \State {Compute Cross Entropy loss, $\ell_{\mathrm{CE}}$ between ground truth $y'$ and prediction $\tilde{y'}$ using Eq. \eqref{eqn:ce_loss_cl}} 
        \State {Compute Subspace Distillation loss, $\ell_{\text{SD}}^{\text{CL}}$ between $\vec{f}^{t-1}$ and $\vec{f}^{t}$ with Eq. \eqref{eqn:sd_loss_cl}}

        \State {Update $\Theta^t$ by minimizing the overall loss defined based on two cross entropy loss, $\ell_{\mathrm{CE}}$ computed for $\mathcal{X}_B$, and $\mat{X'}$ respectively and subspace distillation loss, $\ell_{\mathrm{SD}}$ as in Eq.~\eqref{eqn:overall_loss_cl}}
    \EndFor
\end{algorithmic}
\end{algorithm}


\subsection{Continual Semantic Segmentation using Subspace Distillation}

Since image pixels belonging to prior classes are labeled as background in CSS, old model is employed for distilling knowledge. The idea of knowledge distillation is a crucial step and widely adapted in preserving prior knowledge for CSS. In CSS, we apply subspace distillation loss at intermediate layers of model 
to maintain consistency in geometric structure of latent features. Additionally, we use output distillation to ensure that the current model mimics the output of prior model. In other words, subspace distillation is combined with classification loss and output distillation loss to train CSS model at any step $t$. %
Bellow, we briefly discuss the classification loss and output distillation loss for CSS.



\subsubsection{Classification Loss}
In semantic segmentation, the cross entropy loss is often applied at each pixel of the output generated by the DNN. However, in CSS and to train the model at time $t$, the background class may include prior classes from already seen tasks. Hence, keeping background shift problem of CSS in mind, we define the cross entropy loss as follows:

\begin{align}
    \label{eqn:ce_loss}
    \ell_{\mathrm{CE}}^{CSS}(\mat{X}, \vec{Y};\Theta^t) =
    -\dfrac{1}{|\mat{X}|} \sum_{(x, y) \in (\mat{X}, \mat{Y})}^{} \sum_{c \in {C^{t}}}~{y}^{t}_{x, c} \log~(\tilde{y}^{t}_{x, c})\;,
\end{align}
\noindent 
where ${y}^{t}_{x, c}$ is the ground truth label at pixel $x$ for class $c$ and $\tilde{y}^{t}_{x, c}$ is defined as follows

\begin{equation}
    \label{eqn:predicted_lbl}
    \tilde{y}^{t}_{x, c} =
    \begin{cases}
    \hat{y}^{t}_{x, c}~\text{if label is not background} \\
    \sum_{c^{'} \in {C^{1:t-1}}}^{} \hat{y}^{t}_{x, c^{'}}~\text{if label is background}.\\
    \end{cases}
\end{equation}
Here, $\hat{y}^{t}_{x, c}$ is the predicted probability for class $c$ at pixel $x$ using model at task $t$. 

        





\subsubsection{Knowledge Distillation}
Distilling knowledge from the old model to current one is crucial to mitigate the catastrophic forgetting and accurate semantic segmentation in continual learning setting. Output distillation by mimicking the predicted probability of old model to the new model has been widely used in the literature of continual learning~\cite{li2017learning, buzzega2020dark}. However, because of background shift problem, conventional knowledge distillation approach cannot be applied directly in the continual semantic segmentation. In this work, similar to MiB method, we follow the masked cross entropy by relating the old model's prediction to the new model's prediction after combining new classes probability with background. Therefore, the adapted output distillation loss is defined as follows:

\begin{align}
    \label{eqn:kd_loss}
    \ell_{\mathrm{KD}}^{CSS}(\mat{X}, \vec{Y}) =
    -\dfrac{1}{|\mat{X}|} \sum_{x \in \mat{X}}^{} \sum_{c \in {C^{1:t-1}}}^{} y^{t-1}_{x, c}~\log(\tilde{y}^{t}_{x, c})
\end{align}

where $y^{t-1}_{x, c}$ is the predicted probability using $f_{\theta^{t-1}}$ for class $c$ at pixel $x$ in image $\mat{X}$. We define  $\tilde{y}^{t}_{x, c}$ as follows:

\begin{equation}
    \label{eqn:predicted_lbl}
    \tilde{y}^{t}_{x, c} =
    \begin{cases}
    y^{t}_{x, c} &\text{if label $c \in {C^{1:t-1}}$ is not background} \\
    \sum_{c^{'} \in {C^{t}}}^{}\limits y^{t}_{x, c^{'}} &\text{if label $c \in {C^{1:t-1}}$ is background}.\\
    \end{cases}
\end{equation}
Since the modified distillation loss does not directly match the predicted background class probability of old model to the new model, the distillation loss plays a vital role in tackling the background label shift problem in class-incremental learning for semantic segmentation.



Finally, the combined loss of our end-to-end continual semantic segmentation method can be written as the linear combination of classification loss, $\ell_{\mathrm{CE}}$, feature distillation using newly proposed subspace distillation loss,$\ell_{\mathrm{SD}}$ , and knowledge distillation loss, $\ell_{\mathrm{KD}}$

\begin{align}
    \label{eqn:overall_loss_css}
    \mathcal{L}_{\mathrm{CSS}}\big(\mathcal{D}^t;\Theta^t\big) \coloneqq 
    \E\nolimits_{(\mat{X}, \vec{Y}) \sim \mathcal{D}^t} \Big[
    \ell_{\mathrm{CE}}^{CSS}\big(\mat{X}, \vec{Y}\big)
    + \alpha \ell_{\mathrm{KD}}^{CSS}\big(\mat{X}, \vec{Y}\big)
    + \beta \ell_{\mathrm{SD}}^{CSS}\big(\mat{X}\big)
    \Big]
\end{align}
Here, $\alpha$ and $\beta$ are the predefined hyperparameter used to control the contribution of $\ell_{\mathrm{KD}}^{CSS}$ and $\ell_{\mathrm{SD}}^{CSS}$ respectively. \cref{alg:css} summarizes the steps needed to be taken to train a new model for CSS.







\begin{algorithm}[H] 
\caption{Subspace Distillation for CSS}
\label{alg:css}
\begin{algorithmic}[1]
\Require{Dataset~$\mathcal{D}^t$,~Model~from~previous~step~$t-1$,~ $h_{\Theta^{t-1}} = h_{\text{encoder}}^{t-1} \circ h_{\text{decoder}}^{t-1}$}
\Ensure{New model at step $t$ with parameter $\Theta^t$}
\State {$\text{Initialize }\Theta^t~\text{ with }\Theta^{t-1}$}

    \For {iteration $1$  to  max\_iter}
        \State {Sample a mini batch $(\mathcal{X}_B, \mathcal{Y}_B)$ from ${\mathcal{D}^t}$}
        
        \State $\vec{F}^t, \hat{\mathcal{Y}_B}^t \leftarrow h_{\Theta^{t}}(\mathcal{X}_B)$
        \State $\vec{F}^{t-1}, \hat{\mathcal{Y}_B}^{t-1} \leftarrow h_{\Theta^{t-1}}(\mathcal{X}_B)$
        
        \State {Compute Cross Entropy loss, $\ell_{\mathrm{CE}}$ between ground truth $\mathcal{Y}_B$ and prediction $\mathcal{\tilde{Y}}_B^t$ using Eq. \eqref{eqn:ce_loss}}

        \State {Compute Output Distillation loss, $\ell_{\mathrm{KD}}$ between the prediction from current and old model,  $\mathcal{\tilde{Y}}_B^{t}$ and $\mathcal{\tilde{Y}}_B^{t-1}$ respectively using Eq. \eqref{eqn:kd_loss}}
        
        \For{$l \gets 1$ to $L$}
            \State {Split $\vec{F}^t[l]$ and $\vec{F}^{t-1}[l]$ into sub group}
            \State {Compute Layer-wise Subspace Distillation loss, $\ell_{\mathrm{SD}}$ between $\vec{F}^{t-1}[l]$ and $\vec{F}^{t}[l]$} using Eq. \eqref{eqn:sd_loss_css}
        \EndFor
        
        \State {Update $\Theta^t$ by minimizing the linear combination of $\ell_{\mathrm{CE}}$, $\ell_{\mathrm{KD}}$ and $\ell_{\mathrm{SD}}$ with Eq.~\eqref{eqn:overall_loss_css}}
    \EndFor
\end{algorithmic}
\end{algorithm}