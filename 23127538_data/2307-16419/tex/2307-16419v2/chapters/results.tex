




\subsection{Continual Learning}

\begin{table*}[ht]
\vspace{-10mm}
\centering
\resizebox{.85\textwidth}{!}{\begin{tabular}{ccccccc}
\hline
\multirow{2}{*}{Method} & \multicolumn{2}{c}{S-MNIST} & \multicolumn{2}{c}{S-CIFAR-10} & \multicolumn{2}{c}{S-Tiny Imagenet} \\ \cline{2-7} 
      & Task-IL           & Class-IL          & Task-IL       & Class-IL      & Task-IL          & Class-IL          \\ \hline
JOINT     & 99.65      &  97.92     & 98.29  & 92.20  & 82.04     & 59.87      \\
SGD     & 87.15      &  19.90     & 61.02 & 19.61  & 17.93     & 7.79      \\ \hline
LwF~\cite{li2017learning}             & 99.25      &  20.07     & 63.28 & 19.59  & 15.79     & 8.46      \\
oEWC~\cite{schwarz2018progress}            & 99.10      &  20.00     & 68.27 & 19.47  & 19.20     & 7.56      \\
SI~\cite{zenke2017continual}    & 99.07      &  19.97     & 68.05  & 19.46  & 35.97     & 6.58      \\ \hline
    & \multicolumn{5}{c}{Online Data Stream Setting with Tiny Memory (Buffer Size: 100)} & \\ \cline{2-7}

ER~\cite{rolnick2019experience}              & 97.72      &  73.80     & 77.85  & 32.87  & 28.07                  & 5.85                   \\
DER~\cite{buzzega2020dark}             & 98.48      &  77.12    & 80.72  & 32.43 &  27.73                 &  4.26                  \\
 \rowcolor{Gray}    \textbf{Ours (SD)}   & 98.35      &  \textbf{79.37}    &  \textbf{81.65 }             & \textbf{35.1}                 &   \textbf{30.11}               & \textbf{6.05}              \\ 
                        \hline
    &  \multicolumn{5}{c}{Small Memory (Buffer Size: 200)} & \\ \cline{2-7}
iCARL~\cite{rebuffi2017icarl}           & 98.28     &  70.51     & 88.99 & 49.02 &  28.19                &   7.53                \\
ER~\cite{rolnick2019experience}              & 97.86      &  80.43     & 91.19  & 44.79  & 38.17                  & 8.49                   \\
DER~\cite{buzzega2020dark}             & 98.80      &  84.55    & 91.40  & {61.93} &  40.22                 &  11.87                  \\
    \rowcolor{Gray}     \textbf{Ours (SD)}   & 97.71      &  {85.28}    &  \textbf{92.88 }             & 61.85                 &   39.52               & 8.54              \\ 
    \rowcolor{Gray}     DER~\cite{buzzega2020dark} + \textbf{SD}          & \textbf{98.86}      &  \textbf{86.54}    &  {92.07}    &  \textbf{66.12}    &  \textbf{42.63}                 &  \textbf{12.26}       \\ 
                        \hline
    &  \multicolumn{5}{c}{Medium Memory (Buffer Size: 500)} & \\ \cline{2-7}

iCARL~\cite{rebuffi2017icarl}           & 98.81     &  74.55     & 88.22 & 47.55 &  31.55                &   9.38                \\
ER~\cite{rolnick2019experience}              & 98.89      &  86.57     & 93.61  & 57.74  & 48.64                  & 9.99                   \\
DER~\cite{buzzega2020dark}             & 98.84      &  90.54    & 93.40 & 70.51 &  51.78    &  17.75                  \\
     \rowcolor{Gray}    \textbf{Ours (SD)}   & \textbf{ 99.00 }     &  { 89.00}    &  \textbf{94.86 }             & {71.85}                 &   48.60               & 10.03              \\ 
     \rowcolor{Gray}    DER~\cite{buzzega2020dark} + \textbf{SD}            & {98.98}      & \textbf{91.47}      & {94.68}    & \textbf{75.96}     & \textbf{52.74}                  &  \textbf{19.43}      \\


                        \hline
    &  \multicolumn{5}{c}{Large Memory (Buffer Size: 5120)} & \\ \cline{2-7}
iCARL~\cite{rebuffi2017icarl}           & 98.32      &  70.60     & 92.23  & 55.07  & 40.83   &  14.08                  \\
ER~\cite{rolnick2019experience}              &  99.33      &  93.40     & 96.98  & 82.47  & 67.29     & 27.40       \\
DER~\cite{buzzega2020dark}            & 99.29      & 94.9         & 95.43  & 83.81  & 69.50     & 36.73       \\
     \rowcolor{Gray}    \textbf{Ours (SD)}   &  \textbf{ 99.69}    &  \textbf{ 95.90} & \textbf{ 97.18}              &  {84.75}   &   69.13                &   29.32 \\
     \rowcolor{Gray}    DER~\cite{buzzega2020dark} + \textbf{SD}             & 99.44      & 95.33         &  96.77 &  \textbf{86.32}  & 69.47     & \textbf{37.27}       \\
\hline
\end{tabular}}
\caption{\label{tab:acc_benchmark_cl}Average top-1 accuracy on splited MNIST, CIFAR-10 and Tiny Imagenet for 5, 5 and 10 tasks respectively for standard L3 benchmarks. Best values are represented in bold. %
Higher is better. Results for LwF, oEWC, SI, iCARL, ER, and DER are  from~\cite{buzzega2020dark}.%
}
\end{table*}




We analyze the efficacy of subspace distillation in continual learning setting for classification problem and present the result in Table \ref{tab:acc_benchmark_cl}. 
In comparison, we consider two knowledge distillation based methods (LwF~\cite{li2017learning}, iCARL~\cite{rebuffi2017icarl}), two regularization methods (SI~\cite{zenke2017continual}, oEWC~\cite{schwarz2018progress}) and three memory replay based methods (ER~\cite{rolnick2019experience}, DER~\cite{buzzega2020dark}, DER++~\cite{buzzega2020dark}). Additionally, we provide upper and lower bounds, that reflect to jointly training on tasks one by one or on all tasks with Stochastic Gradient Descent (SGD) without any specialized strategy designed for CL. 


The results suggest that regularization methods performs poorly on class incremental learning setting as those methods are developed focusing on task incremental learning setting. This observation indicates that regularization towards the set of old parameters is not suitable for tackling catastrophic forgetting because of the local information modeling weight importance~\cite{buzzega2020dark}. Overall, replay based methods outperform regularization methods with big margin across the datasets.
Regularization methods performs good on MNIST datasets for task incremental settings while they perform notably worse on comparatively complex datasets, \eg CIFAR10, Tiny-Imagenet. We observe that our method outperforms state-of-the-art replay base methods in task incremental learning setting on all datasets. Noticeably, for class incremental learning setting on split CIFAR-10 dataset our method performs noticeably better by 15\% and 8\% percentage points compared to iCARL and ER methods, respectively. However, DER method performs slightly better than subspace distillation method as DER relies on storing additional information  (\ie, logits) together with corresponding image and label from past tasks in the memory for the distillation that requires additional memory requirements. We observe that by combining dark experience replay method DER, we can further improve our result. For instance, we can achieve about 2\% and 5\% performance gain on Tiny-Imagenet and CIFAR10 datasets respectively with medium size memory buffer.

We evaluate our model in a more constrained setting where each sample at a task is presented to model once. In other terms, we train our model for a single epoch at each task given a tiny buffer size of 100 samples. Overall, because of the complex setting, continual learning model faces underfitting problem and struggles to mitigate catastrophic forgetting. Replay-based distillation methods also suffer from poor performance when there are insufficient exemplars available, especially for previously observed classes where no memory exemplars exist. For instance, in S-Tiny Imagenet, during the final task, with just 100 memory exemplars, only one example is stored for the 100 previously seen classes out of a total of 180 prior classes, leading to a subpar performance of replay-based distillation methods. We observe that our method considerably improves on baseline ER method across the datasets and performs significantly better than DER. For example, our method enjoys around 2\% improvement on both CIFAR-10 and Tiny-Imagenet datasets than state-of-the-art DER method for class incremental setting. However, we think that this scenario deserves further investigation because of its complex nature.


In task-incremental setting, we notice better performance compared to class-incremental setting for all methods across the datasets. The reason behind such observation is the presence of task identifier at the test time in task-incremental learning that makes the problem easier. We see that SI and oEWC perform badly on relatively complex datasets such as CIFAR10, and Tiny-Imagenet though regularization based methods are designed particularly for task-incremental scenario and our method outperforms both methods with unbridgeable gap in performance. Subspace distillation method performs remarkably better than iCARL across the settings with different memory size. For example, our method outperforms iCARL by 15\% percentage points  with large memory buffer on Tiny-Imagenet, and by 24\% percentage points with medium size memory on CIFAR10 dataset. We also note that our method shows competitive performance with other memory replay based methods. We see around 2\% improvement on ER for task incremental setting on Tiny Imagenet dataset while the performance gain is around 15\% on CIFAR10 dataset for both small and medium size memory. Furthermore, by combining our method with DER, we see 4\%, 5\% and 3\% performance improvements on DER with small, medium and large size memory buffer for task-incremental setting on CIFAR10 dataset. 





\subsection{Continual Semantic Segmentation}

Table~\ref{tab:iou_benchmark_voc} reports the IOU of different baseline strategies and subspace distillation method for three scenarios: (i) 19-1, (ii) 15-5 and (iii) 15-1 overlap scenarios on Pascal-VOC dataset. The results suggest that our proposed subspace distillation based method outperforms state-of-the-art methods in 19-1 and 15-5 task settings by a significant margin. Though our method does not match the performance of PLOP in 15-5s task setting, we observe about 13\% IOU improvement on MiB at last task. Furthermore our method outperforms ILT by around 30\% in IOU of last task. We note that subspace distillation shows consistency in retaining prior knowledge across the settings. To show the efficacy of subspace distillation method in tacking catastrophic forgetting, we combine SD with ILT and PLOP methods, and see improvements on IOU in different settings. We observe that by combining SD with ILT the overall performance (\ie, mIOU) of ILT method improves by about 3\% and 2\% for 19-1 and 15-5 task settings respectively while performance remains similar for 15-5s setting. Similarly, we notice 2\% performance improvement of PLOP methods after combining SD for 19-1 and 15-5s settings which suggests that SD imposes additional constraint on distillation strategy using multi-scale POD that helps learning complimentary information and tackling catastrophic forgetting better together. Though our method shows little lower plasticity in case of learning new classes, subspace distillation shows promises in retaining prior knowledge and tackling forgetting old classes.

\begin{table*}[ht]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{cccccccccc}
\hline
\multirow{2}{*}{Method} & \multicolumn{3}{c}{19-1 (2 tasks)}                   & \multicolumn{3}{c}{15-5 (2 tasks)}                      & \multicolumn{3}{c}{15-1 (6 tasks)} \\ \cline{2-10} 
                        & 0-19 & 20 & \multicolumn{1}{c|}{All} & 0-15 & 16-20 & \multicolumn{1}{c|}{All} & 0-15 & 16-20 & All \\ \hline
 JOINT                       & 77.60     & 76.60   & 77.50     & 79.0     & 72.80      & 77.50      &  79.00    & 72.80      & 77.50 \\


 SGD                       & 6.80     & 12.90   & 7.10     & 2.10     & 33.10      & 9.80      &  0.20    & 1.80      & 0.60 \\
EWC~\cite{kirkpatrick2017overcoming}                       & 26.90     & 14.00   & 26.30     & 24.30     & 35.50     & 27.10  & 0.30      & 4.30      & 1.30    \\
LwF~\cite{li2017learning}                       & 51.20    & 8.50   & 49.10      & 58.90     & 36.60      & 53.30       & 1.00   & 3.90      & 1.80   \\
RW~\cite{chaudhry2018riemannian}                       &  23.30    & 14.20   & 22.90       & 16.60     & 34.90      & 21.20      & 0.00     & 5.20      & 1.30    \\
SDR~\cite{michieli2021continual}                       & 71.30      & 23.40    & 69.0        & 76.30    & 50.20      & 70.10     &  47.30    & 14.70      & 39.50   \\
GIFS~\cite{cermelli2020few}                       & 57.88    & 32.82   & 56.69      & 23.61    & 16.43      & 21.90      & 59.36     & 13.89      & 48.53      \\ \hline
ILT~\cite{michieli2019incremental}                       & 67.75     & 10.88   & 65.05       & 67.08    & 39.23      & 60.45       & 8.75     & 7.99      & 8.56    \\
\rowcolor{Gray} ILT~\cite{michieli2019incremental} + SD                      & \textbf{72.18}     & \textbf{28.02}   & \textbf{70.05}     & \textbf{70.28}    & \textbf{42.63}      &  \textbf{63.70}      &  6.52    &  \textbf{9.03}     & 7.12   \\ \hline
PLOP~\cite{douillard2021plop}                       & 75.35     & 37.35   & 73.54   & 75.73     & 51.71      & 70.09     & 65.12     & 21.11      & 54.64   \\
\rowcolor{Gray} PLOP~\cite{douillard2021plop} + SD                       & \textbf{76.50}     & \textbf{46.39}   & \textbf{75.07}     & 75.63     & 50.17      & 69.57      & \textbf{66.83}     & \textbf{21.56}  & \textbf{56.05}  \\ \hline

MiB~\cite{cermelli2020modeling}                       & 71.43     & 23.59   & 69.15    & 76.37     & 49.97     & 70.08      & 34.22     & 13.50      & 29.29   \\
\rowcolor{Gray} Ours~(MiB~\cite{cermelli2020modeling} + SD)                       &  \textbf{76.09}     & 20.16      & \textbf{73.43}   & \textbf{78.10}       & \textbf{51.21}    & \textbf{71.70}   & \textbf{50.62}     & \textbf{14.41}      &  \textbf{42.00}    \\ \hline


\end{tabular}}
\caption{\label{tab:iou_benchmark_voc}Performance of different continual semantic segmentation method in IOU on Pascal VOC dataset for three continual \textbf{overlap} class learning settings: (i) 19-1 (ii) 15-5 and (iii) 15-1 tasks. Best values are represented in bold. Results for EWC, LwF, RW, ILT, PLOP and MiB are extracted from PLOP~\cite{douillard2021plop} paper while the result for SDR and GIFS are collected from the corresponding original paper. Higher is better. SD: Subspace Distillation.}
\vspace{3mm}

\end{table*}















% Figure environment removed


Fig.~\ref{fig:iou_evolution} reports the continual evolution of mean IOU for 15-5s (6 tasks) overlap setting on Pascal-VOC dataset. As depicted in the figure, the performance of MiB drops significantly throughout the learning steps while PLOP shows strength in preserving already learned knowledge and at each learning step PLOP performs significantly better than MiB. We observe considerably improved performance across the learning steps when subspace distillation is added with MiB. SD facilitates MiB to preserve previously learned knowledge and therefore we note about 9\%, and 12\% improvement of mean IOU at task 5, and 6 respectively after adding SD with MiB. Similarly, SD helps PLOP tackling catastrophic forgetting better and we see  improvements on  mean IOU at the end of all learning tasks.
