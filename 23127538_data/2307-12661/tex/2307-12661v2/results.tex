%% =============================================================================
\section{Our algorithmic procedure and associated results}
\label{s:results}
%% =============================================================================

This section contains our technical results. \secref{s:results:stability} and \secref{s:results:asystability} document our results for Lyapunov stability and asymptotic stability, respectively, and \secref{s:results:discussion} contains a detailed discussion of various aspects --- theoretical and numerical --- of our results. A consolidated presentation of the proofs of our results may be found in \secref{s:results:proofs}. A schematic of the algorithmic procedure driving our results is provided in Figure \ref{fig:proc}.

% Figure environment removed


%% -----------------------------------------------------------------------------
\subsection{Procedure for Lyapunov stability}
\label{s:results:stability}
%% -----------------------------------------------------------------------------

We begin with the problem of constructing a Lyapunov function for checking \embf{Lyapunov stability} of the (isolated) equilibrium point \(0\) of the given continuous vector field \(\vecfld\). The first step is to pick a candidate Lyapunov triplet \(\bigl( \nbhd, (\lowerBound, \upperBound, \marginBound), (\basisDict, \marginDict) \bigr)\) satisfying \ref{d:nbhd}--\ref{d:dictionary}. Since our interest is in the property of Lyapunov stability, \(\marginDict = \set{0}\) and \(\marginBound \equiv 0\).\footnote{Stricly speaking, the function \(\marginBound\) cannot be in \(\classK\) once it is set to \(0\), but we shall ignore this technicality in the interest of a cogent presentation.}

Let \(\basisDim \Let \size{\basisDict}\), and fix a strictly convex, smooth, and near-monotone function \(\objective:\R[\basisDim]\lra\lcro{0}{+\infty}\).\footnote{Recall that \emph{near-monotonicity} of $\objective$ means $\lim_{\norm{y}\to+\infty} \objective(y) = +\infty$.} For instance, the convex quadratic function \(\R[\basisDim]\ni z\mapsto \objective(z) \Let \norm{z - p}^2\) for some fixed nonzero \(p\in\R[\basisDim]\) is perfectly fine. The preceding properties on the objective \(\objective\) are designed to aid the numerical routines in our procedure.

Let \(\lyapCoeff[] \Let \pmat{\lyapCoeff[1], \ldots, \lyapCoeff[\basisDim]} \in \R[\basisDim]\) denote the coefficients of a candidate Lyapunov function in \(\linspan \basisDict\). Given the data \ref{d:nbhd}--\ref{d:dictionary}, we recast the problem of finding a feasible candidate \(\lyapfn \in \linspan \basisDict\) satisfying \eqref{e:Lyapunov stability} into the following convex semi-infinite program on the coefficients \(\lyapCoeff[]\):
\begin{equation}
    \label{e:stability csip}
    \begin{aligned}
        & \minimize_{\lyapCoeff[] \in \R[\basisDim]} && \objective(\lyapCoeff[])\\
        & \sbjto && \begin{dcases}
            \lowerBound(\norm{y}) \le \sum_{i=1}^{\basisDim} \basisFunc(y) \lyapCoeff \le \upperBound(\norm{y}) & \text{for all } y \in \nbhd,\\
            \sum_{i=1}^{\basisDim} \inprod{\pdv{\basisFunc}{x}(y)}{\vecfld(y)} \lyapCoeff \le 0 & \text{for all } y \in \nbhd.
        \end{dcases}
    \end{aligned}
\end{equation}
The problem \eqref{e:stability csip}, depending on the various objects in the selected Lyapunov triplet, may or may not be feasible. However, if it \emph{is} feasible, then a solution to \eqref{e:stability csip}, say \(\lyapCoeff[\ast]\in\R[\basisDim]\), provides the Lyapunov function 
\[
	\lyapfn(y) \Let \sum_{i=1}^{\basisDim} \basisFunc(y) \lyapCoeff[\ast, i]\quad\text{for \(y\in\nbhd\)}
\]
corresponding to the equilibrium point \(0\) that verifies its stability. If \eqref{e:stability csip} is \emph{strictly} feasible (a certain Slater's condition mentioned below holds), then we shall obtain a numerical solution to \eqref{e:stability csip} in the following fashion:

Define the map \(\relObjective : \nbhd^{\basisDim} \lra\R\) given by
\begin{equation}
    \label{e:stability optimization}
	\begin{aligned}
		& \nbhd^{\basisDim}\ni (\sample[1], \ldots, \sample[\basisDim]) \mapsto \relObjective(\sample[1], \ldots, \sample[\basisDim]) \Let \\
		& \quad\begin{aligned}
				& \inf_{\lyapCoeff[] \in \R[\basisDim]} && \objective(\lyapCoeff[])\\
		        & \sbjto && \begin{dcases}
            		\lowerBound(\norm{\sample}) \le \sum_{i=1}^{\basisDim} \basisFunc(\sample) \lyapCoeff \le \upperBound(\norm{\sample}) &  \text{for all } k = 1, \ldots, \basisDim,\\
		            \sum_{i=1}^{\basisDim} \inprod{\pdv{\basisFunc}{x}(\sample)}{\vecfld(\sample)} \lyapCoeff  \le 0 & \text{for all } k = 1, \ldots, \basisDim
       			\end{dcases}
			\end{aligned}
    \end{aligned}
\end{equation}
Observe that \eqref{e:stability optimization} contains a relaxed version of \eqref{e:stability csip} --- the number of constraints in \eqref{e:stability optimization} is \(\basisDim\) in contrast to an uncountable compact family of constraints in \eqref{e:stability csip}.

This brings us to our first main result, a proof of which is deferred to \secref{s:results:proofs}.
\begin{theorem}
	\label{t:stability}
	Let \(\domain\subset\R[\sysDim]\) be an open set containing \(0\in\R[\sysDim]\), and let a continuous vector field \(\vecfld:\domain\lra\R[\sysDim]\) be given. Suppose that \(0\) is an isolated equilibrium point of \(\vecfld\), and pick a candidate Lyapunov triplet \(\bigl( \nbhd, (\lowerBound, \upperBound), (\basisDict, \marginDict) \bigr)\) satisfying \ref{d:nbhd}--\ref{d:dictionary}. Consider the problem \eqref{e:stability csip} and suppose that the interior of its admissible set is non-empty. For \(\relObjective\) defined in \eqref{e:stability optimization}, suppose that \(\pmat{\sample[1] \opt, \ldots, \sample[\basisDim]\opt} \in \nbhd^{\basisDim}\) solves the global optimization problem
	\begin{equation}
		\label{e:stability global max}
		\sup_{(\sample[1], \ldots, \sample[\basisDim])\in\nbhd^{\basisDim}} \relObjective(\sample[1], \ldots, \sample[\basisDim]),
	\end{equation}
	and let \(\lyapCoeff[\ast]\) solve the finitely constrained convex optimization problem
    \begin{equation}
        \label{e:stability MSA}
        \begin{aligned}
            & \minimize_{\lyapCoeff[] \in \R[\basisDim]} && \objective(\lyapCoeff[])\\
            & \sbjto && \begin{dcases}
                \lowerBound \bigl(\norm{\sample \opt} \bigr) \le \sum_{i=1}^{\basisDim} \basisFunc(\sample \opt) \lyapCoeff \le \upperBound \bigl(\norm{\sample \opt} \bigr) & \text{for all } k = 1, \ldots, \basisDim,\\
                \sum_{i=1}^{\basisDim} \inprod{\pdv{\basisFunc}{x}(\sample \opt)}{\vecfld(\sample \opt)} \lyapCoeff  \le 0 & \text{for all } k = 1, \ldots, \basisDim.
            \end{dcases}
        \end{aligned}
    \end{equation}
	Then 
	\begin{equation}
		\label{e:stability lyapfn}
		\nbhd\ni z\mapsto \lyapfn(z) \Let \sum_{i=1}^{\basisDim} \basisFunc(z) \lyapCoeff[\ast, i]\in\R[]
	\end{equation}
	is a Lyapunov function for \(0\in\R[\sysDim]\) and \(0\) is Lyapunov stable.
\end{theorem}


%% -----------------------------------------------------------------------------
\subsection{Procedure for asymptotic stability}
\label{s:results:asystability}
%% -----------------------------------------------------------------------------

The procedure for constructing a Lyapunov function to check \embf{asymptotic stability} of the (isolated) equilibrium point \(0\) of the given continuous vector field \(\vecfld\) follows analogously as above. Pick a candidate Lyapunov triplet \(\bigl( \nbhd, (\lowerBound, \upperBound, \marginBound), (\basisDict, \marginDict) \bigr)\) satisfying \ref{d:nbhd}--\ref{d:dictionary}.

Define \(\basisDim \Let \size{\basisDict}, \marginDim \Let \size{\marginDict}\), and choose a strictly convex, smooth, and near-monotone function \(\objective:\R[\basisDim] \times \R[\marginDim] \lra \lcro{0}{+\infty}\). As before, the convex quadratic function \(\R[\basisDim] \times \R[\marginDim] \ni (u, v) \mapsto \objective(u, v) \Let \norm{u - p}^2 + \norm{v - q}^{2}\), for some fixed nonzero \(p\in\R[\basisDim], q \in \R[\marginDim]\), is a suitable choice.

Let \(\lyapCoeff[] \Let \pmat{\lyapCoeff[1], \ldots, \lyapCoeff[\basisDim]}, \marginCoeff[] \Let \pmat{\marginCoeff[1], \ldots, \marginCoeff[\marginDim]}\) denote the coefficients of the candidate functions \(\lyapfn \in \linspan \basisDict\) and \(\stabilityMargin \in \linspan \marginDict\) respectively. The feasibility problem of the requirements \eqref{e:asymptotic stability} can be equivalently phrased as solving the following convex semi-infinite program on the coefficients \(\lyapCoeff[], \marginCoeff[]\):
\begin{equation}
    \label{e:asymptotic csip}
    \begin{aligned}
		& \minimize_{(\lyapCoeff[], \marginCoeff[])} && \objective(\lyapCoeff[], \marginCoeff[])\\
        & \sbjto && \begin{dcases}
            \lowerBound(\norm{y}) \le \sum_{i=1}^{\basisDim} \basisFunc(y) \lyapCoeff \le \upperBound(\norm{y}) & \text{for all } y \in \nbhd,\\
            \marginBound(\norm{y}) \le \sum_{j=1}^{\marginDim} \marginFunc(y) \marginCoeff & \text{for all } y \in \nbhd,\\
            \sum_{i=1}^{\basisDim} \inprod{\pdv{\basisFunc}{x}(y)}{\vecfld(y)} \lyapCoeff + \sum_{j=1}^{\marginDim} \marginFunc(y) \marginCoeff \le 0 \quad & \text{for all } y \in \nbhd,\\
			(\lyapCoeff[], \marginCoeff[]) \in \R[\basisDim]\times\R[\marginDim].
        \end{dcases}
    \end{aligned}
\end{equation}

Feasibility of the problem \eqref{e:asymptotic csip} is subject to the choice of the Lyapunov triplet (see Remark \ref{r:choice} for a discussion). However, the solution \((\lyapCoeff[\ast], \marginCoeff[\ast]) \in \R[\basisDim] \times \R[\marginDim]\) to \eqref{e:asymptotic csip}, if it exists, readily produces a Lyapunov function
\[
	\lyapfn(y) \Let \sum_{i=1}^{\basisDim} \basisFunc(y) \lyapCoeff[\ast, i] \quad\text{for \(y\in\nbhd\)}
\]
which verifies the asymptotic stability of the equilibrium point \(0\) with the stability margin given by the positive definite function
\[
    \stabilityMargin(y) \Let \sum_{j=1}^{\marginDim} \marginFunc(y) \marginCoeff \quad \text{for } y \in \nbhd.
\]
In addition, if \eqref{e:asymptotic csip} satisfies a certain Slater's condition (to be mentioned below), then the solution \((\lyapCoeff[\ast], \marginCoeff[\ast])\) maximizes the map \(\relObjective : \nbhd^{\basisDim + \marginDim} \lra\R\) defined by
\begin{equation}
    \label{e:asym stability optimization}
	\begin{aligned}
		& \nbhd^{\basisDim + \marginDim} \ni (\sample[1], \ldots, \sample[\basisDim + \marginDim]) \mapsto \relObjective(\sample[1], \ldots, \sample[\basisDim + \marginDim]) \Let \\
		& \quad\begin{aligned}
			& \inf_{(\lyapCoeff[], \marginCoeff[])} && \objective(\lyapCoeff[], \marginCoeff[])\\
		        & \sbjto && \begin{dcases}
            		\lowerBound(\norm{\sample}) \le \sum_{i=1}^{\basisDim} \basisFunc(\sample) \lyapCoeff \le \upperBound(\norm{\sample}) &\\ % \text{for all } k = 1, \ldots, \basisDim + \marginDim,\\
                    \marginBound(\norm{\sample}) \le \sum_{j=1}^{\marginDim} \marginFunc(\sample) \marginCoeff & \text{for all } k = 1, \ldots, \basisDim + \marginDim,\\
                    \sum_{i=1}^{\basisDim} \inprod{\pdv{\basisFunc}{x}(y)}{\vecfld(\sample)} \lyapCoeff + \sum_{j=1}^{\marginDim} \marginFunc(\sample) \marginCoeff \le 0 & \\% \text{for all } k = 1, \ldots, \basisDim + \marginDim,
					(\lyapCoeff[], \marginCoeff[]) \in \R[\basisDim]\times\R[\marginDim].
       			\end{dcases}
			\end{aligned}
    \end{aligned}
\end{equation}
% The map \(\relObjective\) in \eqref{e:asym stability optimization} is a finitely constrained minimization problem 

\begin{theorem}
	\label{t:asym stability}
	Let \(\domain\subset\R[\sysDim]\) be an open set containing \(0\in\R[\sysDim]\), and let a continuous vector field \(\vecfld:\domain\lra\R[\sysDim]\) be given. Suppose that \(0\) is an isolated equilibrium point of \(\vecfld\), and pick a candidate Lyapunov triplet \(\bigl( \nbhd, (\lowerBound, \upperBound, \marginBound), (\basisDict, \marginDict) \bigr)\) satisfying \ref{d:nbhd}--\ref{d:dictionary}. Consider the problem \eqref{e:asymptotic csip} and suppose that the interior of its admissible set is non-empty. For \(\relObjective\) defined in \eqref{e:asym stability optimization}, suppose that \(\pmat{\sample[1] \opt, \ldots, \sample[\basisDim + \marginDim]\opt} \in \nbhd^{\basisDim + \marginDim}\) solves the global optimization problem
	\begin{equation}
		\label{e:asym global max}
		\sup_{(\sample[1], \ldots, \sample[\basisDim + \marginDim])\in\nbhd^{\basisDim+\marginDim}} \relObjective(\sample[1], \ldots, \sample[\basisDim + \marginDim]),
	\end{equation}
    and let \((\lyapCoeff[\ast], \marginCoeff[\ast])\) solve the finitely constrained convex optimization problem
    \begin{equation}
        \label{e:asym MSA}
        \begin{aligned}
			& \minimize_{(\lyapCoeff[], \marginCoeff[])} && \objective(\lyapCoeff[], \marginCoeff[])\\
            & \sbjto && \begin{dcases}
            		\lowerBound(\norm{\sample \opt}) \le \sum_{i=1}^{\basisDim} \basisFunc(\sample \opt) \lyapCoeff \le \upperBound(\norm{\sample \opt}) &\\ % \text{for all } k = 1, \ldots, \basisDim + \marginDim,\\
                    \marginBound(\norm{\sample \opt}) \le \sum_{j=1}^{\marginDim} \marginFunc(\sample \opt) \marginCoeff & \text{for all } k = 1, \ldots, \basisDim + \marginDim,\\
                    \sum_{i=1}^{\basisDim} \inprod{\pdv{\basisFunc}{x}(y)}{\vecfld(\sample \opt)} \lyapCoeff + \sum_{j=1}^{\marginDim} \marginFunc(\sample \opt) \marginCoeff \le 0 & \\ % \text{for all } k = 1, \ldots, \basisDim + \marginDim,
					(\lyapCoeff[], \marginCoeff[])\in\R[\basisDim]\times\R[\marginDim].
            \end{dcases}
        \end{aligned}
    \end{equation}
	Then 
	\begin{equation}
		\label{e:asym lyapfn}
		\nbhd \ni z \mapsto \lyapfn(z) \Let \sum_{i=1}^{\basisDim} \basisFunc(z) \lyapCoeff[\ast, i] \in \R[]
	\end{equation}
	is a Lyapunov function for \(0 \in \R[\sysDim]\) and \(0\) is asymptotically Lyapunov stable with the stability margin characterized by
    \begin{equation}
        \label{e:asym marginfn}
		\nbhd \ni z \mapsto \stabilityMargin(z) \Let \sum_{i=1}^{\marginDim} \marginFunc(z) \marginCoeff[\ast, i] \in \R[].
    \end{equation}
\end{theorem}


%% -----------------------------------------------------------------------------
\subsection{Discussion}
\label{s:results:discussion}
%% -----------------------------------------------------------------------------

The sequence of remarks given below documents a number of distinctive features of the results presented above.

\begin{remark}[Steps to solve the convex semi-infinite programs]
	\label{r:steps}
	In abstract terms, a convex semi-infinite program is
	\begin{equation}
		\label{e:generic csip}
		\begin{aligned}
			& \minimize_\xi && \objective(\xi)\\
			& \sbjto && 
			\begin{cases}
				\constrfn(\xi, \theta) \le 0 \quad \text{for all }\theta,\\
				\xi\in\Xi\subset\R[\nu],\; \theta\in\Theta\subset\R[\mu],
			\end{cases}
		\end{aligned}
	\end{equation}
	with continuous objective \(\objective:\Xi\lra\R[]\) and continuous constraint \(\constrfn:\Xi\times\Theta\lra\R[]\) functions such that \(\objective(\cdot)\) is convex and \(\constrfn(\cdot, \theta)\) is convex for each \(\theta\), and the sets \(\Xi\) and \(\Theta\) are convex and compact, respectively. Both \eqref{e:stability csip} and \eqref{e:asymptotic csip} are of the form \eqref{e:generic csip}; in particular, the role of the set \(\Theta\) in \eqref{e:generic csip} is played by the compact neighborhood \(\nbhd\) in \eqref{e:stability csip} and \eqref{e:asymptotic csip}. Our procedure to solve \eqref{e:generic csip} passes through numerical solutions to
	\begin{equation}
		\label{e:maxmin}
		\maximize_{(\theta_1, \ldots, \theta_\nu)\in\Theta^\nu} \; \minimize_{\xi\in\Xi} \; \set[\Big]{ \objective(\xi) \suchthat \constrfn(\xi, \theta_i) \le 0 \;\text{for each }i = 1, \ldots, \nu }.
	\end{equation}
	\cite[Theorem 1]{ref:DasAraCheCha-22} ensures that the values of \eqref{e:generic csip} and \eqref{e:maxmin} are identical, and our choice of the strictly convex and near-monotone objective function \(\objective\) also leads to the extraction of optimizers of \eqref{e:generic csip} in view of \cite[Proposition 2]{ref:DasAraCheCha-22}. Both \eqref{e:stability global max} and \eqref{e:asym global max} are of the form \eqref{e:maxmin}. The inner minimization problem in \eqref{e:maxmin} is a standard convex optimization problem, and a large number of numerical routines are available to solve it. Solutions to \eqref{e:maxmin} require the employment of global maximization routines that do not require analytical expressions of the reward functions because such formulae are not available, in general, for the value of the inner minimization as a function of \((\theta_1, \ldots, \theta_\nu)\) (encoded in the function \(\relObjective\) defined in \eqref{e:stability optimization} and \eqref{e:asym stability optimization} above). We employed off-the-shelf simulated annealing routines in our numerical experiments documented in \secref{s:numerics} partially because this technique behaves well for high-dimensional problems, but our overall approach is not contingent on the employment of simulated annealing, nor do we advocate it in all cases; see \cite[Remarks 11 and 13]{ref:DasAraCheCha-22} in this context.
\end{remark}

\begin{remark}[Nature of the theorems]
	Theorem \ref{t:stability} and Theorem \ref{t:asym stability} assume that \eqref{e:stability csip} and \eqref{e:asymptotic csip} are, respectively, feasible. This is necessary to deploy the numerical routines to solve, in the abstract language of Remark \ref{r:steps}, the max-min problem \eqref{e:maxmin}. In practice, the success of our procedure is contingent on the selection of `good' candidate Lyapunov triplets (see Remark \ref{r:choice} for a discussion), which involves an educated guess concerning the class of functions in which a Lyapunov function may exist. The two convex semi-infinite programs \eqref{e:stability csip} and \eqref{e:asymptotic csip} become feasible (if \(0\) is Lyapunov stable and asymptotically stable, respectively,) under a judicious choice of candidate Lyapunov triplets, and then the procedures in Theorems \ref{t:stability} and \ref{t:asym stability} may be deployed to arrive at corresponding Lyapunov functions. On the one hand, finding \emph{a} Lyapunov function for a set of given data (continuous vector field and its equilibrium point) involves finding \emph{a function}, and is challenging. On the other hand, making an educated guess about a class of functions --- encoded by the candidate Lyapunov triplets --- in which a Lyapunov function may exist for the given data is a simpler task (by definition!). If this guess turns out to be valid, then our algorithmic procedure finds a Lyapunov function from the selected class.
\end{remark}

\begin{remark}[Feasibility and optimization]
	Finding Lyapunov functions is, strictly speaking, a \emph{feasibility problem}. The presence of the function \(\objective\) is, therefore, theoretically unnecessary, but we include it for the ease of numerical algorithms involved in our procedure and the extraction of the correct set of optimizers of \eqref{e:stability csip} and \eqref{e:asymptotic csip}; Remark \ref{r:steps} has a discussion about the relevance of \cite[Proposition 2]{ref:DasAraCheCha-22} in this connection. The spirit of our results, therefore, mimics those involving \texttt{SOSTOOLS}. However, while \texttt{SOSTOOLS} provides certificates of the infeasibility of the original problems \eqref{e:stability csip} and \eqref{e:asymptotic csip}, our approach may not be able to detect infeasibility because it relies on relaxed (finitely constrained) versions of these problems at its core.
\end{remark}

\begin{remark}[Choice of candidate Lyapunov triplets]
	\label{r:choice}
	A judicious selection of the various components of a candidate Lyapunov triplet plays a key role in the success of the numerical technique involved herein. The pair \((\basisDict, \marginDict)\) of dictionaries in \ref{d:dictionary} should be sufficiently rich in order for the numerical routine to succeed. One typically fixes a finite number of `basis' functions from families that are \emph{dense} (relative to, e.g., the uniform norm) in the set of continuously differentiable functions on a suitable compact neighborhood of \(0\). The sizes of these dictionaries are limited by the resources at one's disposal in view of the fact that the global optimization problem scales linearly with these sizes. Moreover, the vector field \(\vecfld\) also plays an important role in the selection of \((\basisDict, \marginDict)\); indeed, equilibrium points of certain vector fields do not admit polynomial Lyapunov functions, and naturally, our procedure with polynomial dictionaries would not lead to correct solutions. The Stone-Weierstrass theorem \cite[Chapter III, \S1]{ref:Lan-93} plays a subliminal but crucial part here and motivates the linear approximation viewpoint at the heart of our technique. The neighborhood \(\nbhd\) in \ref{d:nbhd} of \(0\) should not be too large in general; indeed, for verifying the stability of \(0\), selecting a neighborhood containing any other equilibrium point of the vector field \(\vecfld\) will trivially lead to failure of the numerical procedures.
\end{remark}

\begin{remark}[Convex semi-infinite programs]
	\label{r:constant memory}
	Convex semi-infinite programs have traditionally been attacked by means of i.i.d.\ randomized sampling of the so-called ``uncertainty set'' and solving the resulting convex finitary optimization problem in the hope of getting a reasonable facsimile of the optimal solutions. Such an approach suffers from at least two difficulties:
	\begin{itemize}[label=\(\circ\), leftmargin=*]
		\item A large number of samples from the ``uncertainty set'' may be needed to achieve reasonable accuracy even in low dimensions, each of which introduces one constraint into the finitary convex optimization problem. Consequently, the complexity of the finitary convex optimization problem gets inflated in the process (and unnecessarily so, it turns out), leading to large memory requirements.
		\item Moreover, simple examples \cite[\S1.5]{ref:MisChaBan-20}, \cite[\S4]{ref:DasAraCheCha-22} demonstrate that solutions via such i.i.d.\ sampling (employed, e.g., in the scenario approach \cite{ref:CamGar-18}) could be far away from optimality with high probability, especially in high dimensions; this situation arises due to the effect known as \emph{concentration of measures}. Since, in the context of our problem, the ``uncertainty set'' is a neighborhood of an equilibrium point under consideration, such an approach involving i.i.d.\ sampling of the constraints would lead to acute difficulties for high-dimensional vector fields.
	\end{itemize}
	In contrast, our algorithmic apparatus needs \emph{constant runtime memory} if appropriate global optimization methods (such as Markov chain Monte Carlo-based simulated annealing) are adopted, and then the size of the associated global optimization scales \emph{linearly} with the number of state dimensions (which, in turn, means that the procedure performs equally well for high-dimensional vector fields); both of these are key positive features.
\end{remark}

\begin{remark}[Absence of analytic formulae]
	\label{r:no expression}
	A remarkable feature of our approach is the technique is not contingent on the availability of an analytical expression of the vector field \(\vecfld\). Situations where such analytical expressions do not naturally become available are several, and we list three important ones:
	\begin{itemize}[label=\(\circ\), leftmargin=*]
		\item Deep neural networks have had strong connections with control theory in the past \cite{ref:Son-93, ref:SonSus-97}, and their recent proliferation in feedback control synthesis is being boosted by the availability of fast computational tools. Typically, a deep neural network (with continuous activation functions) is trained via optimization routines to suit pre-specified control objectives such as asymptotic stabilization, and the outcome of the training is a feedback map in terms of the activation functions that is difficult, if not impossible to write down due to the sheer complexity of the composition of the activation maps involved. The closed-loop system, however, is a continuous vector field, and tests for the stability of the equilibria of such systems are important in practice.
		\item Model predictive control is one of the most applicable constrained control techniques at present, and it is typically implemented by means of an optimization-based oracle. An optimization routine takes the current state and outputs a corresponding control action at each stage of time. The functional form inside the oracle that provides the control action (output) for each state (input) is not available in closed form, but assessing the qualitative behavior of the associated closed-loop vector field is nonetheless important.
		\item One of the key techniques in numerical optimal control is the so-called \emph{indirect method}. It employs a set of first-order necessary conditions for optimality that is furnished, e.g., by the Pontryagin maximum principle \cite[Chapter 4]{ref:Lib-12}, and effectively transfers the task of finding an extremal pair of state and co-state trajectories to finding a root of a certain multi-dimensional continuous map \(F:\R[\sysDim]\lra\R[\sysDim]\) known as the \emph{shooting function}. An essential feature of this shooting function \(F\) is that (apart from relatively trivial cases) it can be regarded as a vector field on \(\R[\sysDim]\) (as demonstrated in \cite{ref:KumSriChaNag-22}, for instance,) and it is possible to evaluate \(F\) at points picked at will by means of numerical methods (typically involving integration), but no explicit functional formula of \(F\) can be given. This is an important example of situations in which the vector field does not admit an explicit formula.
	\end{itemize}
\end{remark}


%% -----------------------------------------------------------------------------
\subsection{Proofs of Theorem \ref{t:stability} and Theorem \ref{t:asym stability}}
\label{s:results:proofs}
%% -----------------------------------------------------------------------------

\begin{proof}[Proof of Theorem \ref{t:stability}]
    We begin by showing that the semi-infinite program \eqref{e:stability csip} satisfies the hypotheses of \cite[Theorem 1]{ref:DasAraCheCha-22}.
    \begin{enumerate}[label=(\roman*), align=left, widest=B, leftmargin=*]
		\item The neighborhood \(\nbhd\) plays the role of the constraint index set and is compact (in view of the condition \ref{d:nbhd} of the Lyapunov triplet).
        \item The assumption of the feasible set having a non-empty interior satisfies Slater's condition on \eqref{e:stability csip}.% guaranteeing that its feasible set has a non-empty interior.
        \item By construction, the cost function \(\objective\) is strictly convex and continuous. 
        \item The constraint function in \eqref{e:stability csip} can be written as an inequality of the form
            \[
                \constraintMap(\lyapCoeff[], y) \le 0 \quad \text{for all } y \in \nbhd,
            \]
            where the map \(\constraintMap: \R[\basisDim] \times \domain \lra \R[2]\) is defined by
            \[
                \constraintMap (\lyapCoeff[], y) \Let \pmat{\lowerBound(\norm{y}) - \sum_{i=1}^{\basisDim} \basisFunc(y) \lyapCoeff \\ \sum_{i=1}^{\basisDim} \basisFunc(y) \lyapCoeff - \upperBound(\norm{y}) \\ \sum_{i=1}^{\basisDim} \inprod{\pdv{\basisFunc}{x}(\sample)}{\vecfld(\sample)} \lyapCoeff}
            \]
			Clearly, for each fixed \(y \in \nbhd\) the map \(\constraintMap\) is affine in the decision variable \(\lyapCoeff[]\) and hence convex in \(\lyapCoeff[]\). Further, since the vector field \(\vecfld\) is Lipschitz continuous and the functions \(\basisFunc \in \basisDict\) are continuously differentiable, \(\constraintMap\) is also jointly continuous in \(\lyapCoeff[]\) and \(y\).
    \end{enumerate}
    Hence, from \cite[Theorem 1]{ref:DasAraCheCha-22}, the solution (optimal value) of \eqref{e:stability csip} is obtained by maximizing the function \(\relObjective\) defined in \eqref{e:stability optimization}, that is by solving \eqref{e:stability global max}.

    In addition, choosing \(\objective\) to be strictly convex ensures that the relaxed minimization problem \eqref{e:stability MSA} has a unique solution. Thus, by \cite[Proposition 2]{ref:DasAraCheCha-22}, the minimizer \(\lyapCoeff[\ast]\) of the relaxed finitely constrained optimization problem \eqref{e:stability MSA} is also the solution of the SIP \eqref{e:stability csip}. In particular, \(\lyapCoeff[\ast]\) is a feasible point for \eqref{e:stability csip}. This implies that the function \(\lyapfn\) defined in \eqref{e:stability lyapfn} corresponding to \(\lyapCoeff[\ast]\) is a valid Lyapunov function that satisfies the Lyapunov stability criterion \eqref{e:Lyapunov stability} on the neighborhood \(\nbhd\) of \(0\). Consequently, stability of the equilibrium point \(0\) follows.
\end{proof}

\begin{proof}[Proof of Theorem \ref{t:asym stability}]
    % \todo[inline]{Please indicate the changes relative to the preceding proof, the rest follows verbatim.}
    The proof of Theorem \ref{t:asym stability} follows along the lines of the proof of Theorem \ref{t:stability} presented above. Under the assumption that the convex SIP \eqref{e:asymptotic csip} under consideration is strictly feasible, it also satisfies the hypotheses of \cite[Theorem 1]{ref:DasAraCheCha-22}.

    Moreover, by choosing the objective in \eqref{e:asymptotic csip} to be strictly convex in the decision variables \((\lyapCoeff[], \marginCoeff[])\), \cite[Proposition 2]{ref:DasAraCheCha-22} applies, and consequently, the solution \((\lyapCoeff[\ast], \marginCoeff[\ast])\) of \eqref{e:asym MSA} is feasible for the convex SIP \eqref{e:asymptotic csip}. This implies that the pair of functions \(\lyapfn\) and \(\stabilityMargin\) defined in \eqref{e:asym lyapfn} and \eqref{e:asym marginfn} respectively satisfy the asymptotic stability criterion \eqref{e:asymptotic stability} on the neighborhood \(\nbhd\), thereby guaranteeing asymptotic stability of the equilibrium point \(0\).
\end{proof}

