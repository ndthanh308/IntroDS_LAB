We specify probability distributions 
on a double of networks $(\bX, \bY)$, 
where $\bY$ will represent the network formation process,
which we will call the {\it basis network},  
and $\bX$ will represent the realized multilayer network. 
We assume that $\bY \in \mbY \coloneqq \{0, 1\}^{\binom{N}{2}}$ 
is an undirected, single-layer network defined on the set of nodes $\mN$ 
where,
for all $ \{i,j\} \subset \mN$,  
\beno
Y_{i,j}
\= \begin{cases}
1 & \mbox{nodes } i \mbox{ and } j \mbox{ are connected in the basis network} \\ 
0 & \mbox{otherwise} 
\end{cases},
\ee 
making the usual conventions for undirected networks mentioned previously.
For $(\bX, \bY)$, we consider semi-parametric families of probability distributions $\sepfam \coloneqq \{\sepmodel : \nat \in \mbR^p\}$ which are absolutely continuous with respect to a $\sigma$-finite measure $\nu$ defined on $\mP(\mbX \times \mbY)$,
where $\mP(\mbX \times \mbY)$ is the power set of $\mbX \times \mbY$. 
Typically,
$\nu$ will be the counting measure.  
%however sparsity inducing reference measures are also admissible and have found application in network data applications
%in order to model sparse networks \citep{butts:jms:2018,StSc21}. 
We say the probability mass function $\sepmodel \in \sepfam$ defines a {\it separable multilayer network model} if $\sepmodel$ admits the form: 
\be
\label{general_model}
\sepmodel(\{(\bx, \by)\}) 
\= f(\bx,\nat) \; g(\by) \; h(\bx, \by) \; \psi(\nat, \by),
&&  (\bx, \by) \in \mbX \times \mbY, 
\ee
where 
\bi
\item $f : \mbX \times \mbR^p \mapsto (0, 1)$ is given by 
\beno
f(\bx, \nat)
&= \, \dprod_{\{i,j\} \subset \mN} \,
\exp&\left(\dsum_{k=1}^K\,\theta_{k}\,x_{i,j}^{(k)} +  \dsum_{\substack{k < l}}^{K}\, \theta_{k,l} \, x_{i,j}^{(k)} \, x_{i,j}^{(l)} + \ldots \right.\\[20pt]
&& \left.+ \dsum_{k_1 < \,\ldots\,< k_H}^{K}\, \theta_{k_1,k_2,\ldots,k_H} \, x_{i,j}^{(k_1)} \cdots\, x_{i,j}^{(k_H)} \right),
\ee
where $H \le K$ is the highest order of cross-layer interactions included in the model.
We write $\theta_{k_1,k_2,\ldots,k_h}$ to reference the $h$-order interaction parameter 
for the interaction term among layers $\{k_1, \ldots, k_h\} \subseteq \{1, \ldots, K\}$. \s  
\item $g : \mbY \mapsto (0, 1)$ is the marginal probability mass function of $\bY$ 
and is assumed to be strictly positive on $\mbY$. \s
\item $h : \mbX \times \mbY \mapsto \{0, 1\}$ is given by 
\beno
h(\bx, \by) 
\= \dprod_{\{i,j\} \subset \mN} \, 
\one(\norm{\bx_{i,j}}_1 > 0)^{y_{i,j}} \; \one(\norm{\bx_{i,j}}_1 = 0)^{1 - y_{i,j}}, 
\ee 
where $\bx_{i,j} = (x_{i,j}^{(1)}, \ldots, x_{i,j}^{(K)}) \in \mbX_{i,j}$ ($\{i,j\} \subset \mN$). \s   
\item $\psi : \bTheta \times \mbY \mapsto (0, \infty)$ is defined by 
\beno 
\psi(\nat, \by) 
\= \left[ \, \dsum_{\bx \in \mbX} \, f(\bx, \nat) \, h(\bx, \by) \right]^{-1}, 
\ee
ensuring \eqref{general_model}
will be a valid probability mass function for $(\bX, \bY)$.
\ei

The notation $\sepmodel(\{(\bx, \by)\})$ is well-defined for each pair $(\bx, \by) \in \mbX \times \mbY$, 
as $\sepmodel$ is a probability measure defined on $\mP(\mbX \times \mbY)$. 
Frequently, 
we will  write the probability expressions
$\sepmodel(\bX = \bx, \bY = \by)$ for the joint probability of $\{(\bx, \by)\}$, 
and $\sepmodel(\bX = \bx \,|\, \bY = \by)$ for the conditional probability of the event $\bX = \bx$ 
conditional on the event $\bY = \by$. 
We denote the data-generating parameter vector by $\truth \in \mbR^p$,
and the corresponding probability measure and expectation operator  
by $\mbP \equiv \mbP_{\truth}$ and $\mbE \equiv \mbE_{\truth}$, respectively.

The specification in equation \eqref{general_model} 
separates the network formation process $\bY$,
specified by $g(\by)$, 
from the layer formation process, 
specified by $f(\bx, \nat)$. 
The two are joined by the function $h(\bx, \by)$,
which ensures $\norm{\bx_{i,j}}_1 = 0$ 
whenever $Y_{i,j} = 0$
and $\norm{\bx_{i,j}}_1 > 0$ whenever $Y_{i,j} = 1$, 
as we allow edges between nodes $i \in \mN$ and $j \in \mN$ in $\bX$ if and only if $Y_{i,j} = 1$.
We call dyads $\{i, j\} \subset \mN$ with $Y_{i,j} = 1$ {\it activated dyads}, and a pair $(\bx, \by) \in \mbX \times \mbY$ that satisfies $h(\bx, \by) =1$ is said to be a {\it concordant} pair.
We will only focus on concordant pairs of multilayer networks since our modeling framework guarantees the recovery of the basis network $\bY$ given an observation of $\bX$, a point that will be made clear shortly in Proposition \ref{prop:inference}.
The function $\psi(\nat, \by)$ ensures the resulting product of functions will be a valid probability mass function, 
and it has less of a direct role in modeling the cross-layer dependence,
essentially fulfilling the role of a normalizing constant for the conditional probability distribution of $\bX$ given $\bY$,
as derived in Proposition \ref{prop:inference}. 
Such specifications have the advantage of being able to specify the network formation process separately 
from the process that populates the layers of activated dyads,
thus modeling the cross-layer dependence conditional on $\bY$. 
% Figure environment removed
To illustrate the flexibility and generality of \eqref{general_model},
observe that $g(\by)$ is allowed to  be any probability mass function for a single-layer network $\bY$ 
(e.g., exponential-family random graph model, stochastic block model, latent space model),
provided $g(\by) > 0$ for all $\by \in \mbY$. 
To illustrate this point, 
Figure \ref{fig:nets} displays various multilayer networks with $K = 3$ layers where the basis network 
is specified via three different models,
demonstrating that our modeling framework is capable of ensuring that the multilayer network 
respects structural properties of the underlying basis network. 
\hide{
The basis network can be thought of as a network describing the fundamental connection of two nodes,
which then propagates into specific realizations of connections in the multilayer network.
}
We view our framework as semi-parametric as $g(\by)$ need not assume a specific parametric form. 
Moreover, 
our framework can be viewed as non-parametric 
when the maximal possible order of interaction terms are included in \eqref{general_model},
a point on which we further elaborate later. 
An important feature of our framework lies in the fact that 
the choice of the probability distribution for the network formation process does not directly 
influence the estimation for the cross-layer dependence structure,
i.e.,
the choice of $g(\by)$ does not directly influence estimation for $\truth$. 
Proposition \ref{prop:inference} demonstrates this point in the case of likelihood-based inference. 

 
