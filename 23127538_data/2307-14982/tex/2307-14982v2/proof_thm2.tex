In order to demonstrate the feasibility of the normal approximation for maximum likelihood estimators $\mle$  of $\truth$,
we first start with a standard Taylor expansion of the negative score equation:
\be
\label{eq:expansion} 
-\nabla_{\nat} \, \ell(\mle; \bx, \by)
\= -\nabla_{\nat} \,  \ell(\truth; \bx, \by) - \nabla_{\nat}^2 \, \ell(\truth; \bx, \by) \, (\mle - \truth) - \bR,
\ee
where $\bR \in \mbR^p$ is the vector of remainders given in the Lagrange form. 
Denoting by $R_i$, $(\mle - \truth)_i$, and $(\nabla_{\nat} \, \ell(\nat;\bx,\by))_i$ the $i^{\text{th}}$ component of $\bR$, $(\mle - \truth)$,
and the score function 
$\nabla_{\nat} \, \ell(\nat;\bx,\by)$,
respectively.
The remainder term $R_i$ ($i = 1, \ldots, p$) is given by 
\beno
R_i 
\=  \dsum_{j=1}^p \, \dfrac{1}{2} \, 
\dfrac{\partial^2\,(\nabla_{\nat} \,\ell(\dot\nat_i;\bx,\by))_i}{\partial \, \theta_j^2} 
(\mle - \truth)_j^2 \s \\
&& + \; \dsum_{1 \le j < k \le p} \, \dfrac{\partial^2 \, 
(\nabla_{\nat} \ell(\dot\nat_i; \bx,\by))_i}{\partial\, \theta_j \, \partial\, \theta_k} \, 
(\mle - \truth)_j \, (\mle - \truth)_k,
\ee
where $\dot\nat_i = t_i \, \mle + (1 - t_i) \, \truth$ (for some $t_i \in [0, 1]$).  
By Proposition \ref{prop:inference}, 
\beno
\ell(\nat; \bx, \by) 
\= \log \, \mbP_{\nat}(\bX = \bx \,|\, \bY = \by) + \log \, g(\by). 
\ee
By Lemma \ref{lem:s_hetero}, 
the probability mass function $\mbP_{\nat}(\bX = \bx \,|\, \bY = \by)$ 
belongs to a minimal exponential family with the sufficient statistic vector $\bs(\bx)$ 
given by equation \eqref{eq:suff} in Lemma \ref{lem:s_hetero}.  
We then have,
\beno
& - \nabla_{\nat} \, \ell(\nat;\bx, \by) 
\= -(\bs(\bx) - \mbE_{\nat}^{\by} \, \bs(\bX)) \s \\ 
& - \nabla_{\nat}^2 \, \ell(\nat;\bx,\by)
\= \var_{\nat}^{\by} \,  \bs(\bX)
\quad = \quad \mcI(\truth) \, \norm{\by}_1,
\ee
where $\mbE_{\nat}^{\by}$ and $\var_{\nat}^{\by}$ are the 
conditional expectation and variance operators,
respectively, 
of the conditional distribution of $\bX$ given $\bY = \by$,
and by using standard formulas for exponential families 
\citep[e.g., Proposition 3.8, pp. 29,][]{Su19}
and the results of Lemma \ref{lem:min-eig}.  
Note $\nabla_{\nat} \,  \ell(\mle; \bx,\by) = 0$, 
as the maximum likelihood estimator $\mle$ solves the score equation by definition. 
We re-arrange \eqref{eq:expansion} and multiply both sides by $(\mcI(\truth) \, \norm{\bY}_1)^{-1/2}$ to obtain
\be
\label{eq:bridge}
&& (\mcI(\truth) \, \norm{\bY}_1)^{1/2} \, (\mle - \truth) - \; (\mcI(\truth) \, \norm{\bY}_1)^{-1/2} \, \bR\s \\ 
\= (\mcI(\truth) \, \norm{\bY}_1)^{-1/2} \, (s(\bX) - \mbE^{\bY} \, s(\bX)). 
\ee
Define $\Delta \coloneqq  (\mcI(\truth) \, \norm{\bY}_1)^{-1/2} \, \bR$.
Let $\mA \subset \mbR^p$ be any measurable convex subset of $\mbR^p$ 
and $\bZ \sim \text{MvtNorm}(\bm{0}_p, \, \bI_p)$.
We are interested in bounding the quantity  
\beno
\left|\mbP((\mcI(\truth) \, \norm{\bY}_1)^{1/2} \, (\mle - \truth) - \Delta \in \mA) - \Phi(\bZ \in \mA)\right|.
\ee 
Then from \eqref{eq:bridge}, 
\beno
\mbP\left((\mcI(\truth) \, \norm{\bY}_1)^{1/2} \, (\mle - \truth) - \Delta \in \mA\right) \s \\
 = \quad \mbP\big((\mcI(\truth) \, \norm{\bY}_1)^{-1/2} \, (s(\bX) - \mbE^{\bY} \, s(\bX)) \in \mA\big).  
\ee
Applying Proposition \ref{prop:suff_norm},
for all measurable convex sets $\mA \subseteq \mbR^p$, 
\beno
&& \left| \mbP\left((\mcI(\truth) \, \norm{\bY}_1)^{-1/2} \, (s(\bX) - \mbE^{\bY} \, s(\bX)) \in \mA\right) 
- \Phi(\bZ \in \mA) \right|
\ee
is bounded above by
\beno
\dfrac{83}{(\widetilde{\lambda}_{\min}^{\epsilon})^{3/2}} \,
\sqrt{\dfrac{p^{7/2}}{\mbE \, \norm{\bY}_1}}
+  \dfrac{4}{\mbE \, \norm{\bY}_1} + \dfrac{8 \, \left[D_{g}\right]^{+}}{\left(\mbE \, \norm{\bY}_1 \right)^2}. 
\ee
Hence, 
\beno
\left|\mbP((\mcI(\truth) \, \norm{\bY}_1)^{1/2} \, (\mle - \truth) - \Delta \in \mA) - \Phi(\bZ \in \mA)\right| 
\ee
is bounded above by 
\beno
\dfrac{83}{(\widetilde{\lambda}_{\min}^{\epsilon})^{3/2}} \,
\sqrt{\dfrac{p^{7/2}}{\mbE \, \norm{\bY}_1}}
+  \dfrac{4}{\mbE \, \norm{\bY}_1} + \dfrac{8 \, \left[D_{g}\right]^{+}}{\left(\mbE \, \norm{\bY}_1 \right)^2}. \s 
\ee
We lastly handle the term $\Delta$
by showing that $\norm{\Delta}_2$ is small with high probability. 
We first use standard vector/matrix norm inequalities to bound 
\beno
\norm{\Delta}_2
\= \norm{(\mcI(\truth) \, \norm{\bY}_1)^{-1/2} \, \bR}_2 
&\leq& \dfrac{\mnorm{\mcI(\truth)^{-1/2}}_2}{\sqrt{\norm{\bY}_1}} \; \norm{\bR}_2
&\leq& \dfrac{\norm{\bR}_2}{\sqrt{\widetilde{\lambda}_{\min}^{\epsilon}  \norm{\bY}_1}},  
\ee
noting that the spectral norm $\mnorm{\mcI(\truth)^{-1/2}}_2$ 
is equal to the largest eigenvalue of $\mcI(\truth)^{-1/2}$ which will be the 
reciprocal of the smallest eigenvalue of $\mcI(\truth)^{1/2}$,
which is bounded below by $\sqrt{\widetilde{\lambda}_{\min}^{\epsilon}}$. 
Using a standard result from the Taylor theorem for functions with multiple variables,
if for each $i = 1, \ldots, p$,
there exists constants $M_i > 0$ such that
\beno
\sup\limits_{\nat \in \mbR^p \,:\, \norm{\nat - \truth}_1 \leq \norm{\mle - \truth}_1} \,
\left| \, \dfrac{\partial^2 (\nabla_{\nat} \, \ell(\nat;\bx,\by))_i}
{\partial\, \theta_j \, \partial\, \theta_k} \, \right|
&\leq& M_i,
&& 1 \leq j \leq k \leq p, 
\ee
then the Lagrange remainder is bounded above by
\beno
R_i & \le & \dfrac{M_i}{2} \, \norm{\mle - \truth}_1^2
\ee
on the set $\{\nat \in \mbR^p : \norm{\nat - \truth}_1 \leq \norm{\mle - \truth}_2\}$. 
By Lemma \ref{lem:Taylor},
conditional on $\bY = \by$,
we have,
for all $i = 1, \ldots, p$, 
the bound $M_i \le 2 \, \norm{\by}_1$. 
Hence,
\be
\label{eq:Rbound1}
\norm{\Delta}_2
&\leq& \scalebox{0.9}{$\dfrac{1}{\sqrt{\widetilde{\lambda}_{\min}^{\epsilon} \, \norm{\by}_1}} \; 
\sqrt{\dsum_{i=1}^{p} \, R_i^2}  
\quad\leq\quad \dfrac{1}{\sqrt{\widetilde{\lambda}_{\min}^{\epsilon} \, \norm{\by}_1}} \, 
\sqrt{\dsum_{i=1}^{p} \, \norm{\by}_1^2 \, \norm{\mle - \truth}_1^4}$} \s\s\\
&\leq& \scalebox{0.9}{$\dfrac{1}{\sqrt{\widetilde{\lambda}_{\min}^{\epsilon} \, \norm{\by}_1}} \,
\sqrt{p \, \norm{\by}_1^2 \, \norm{\mle - \truth}_1^4}  
\quad\leq\quad \dfrac{\sqrt{p} \, \norm{\by}_1 \, \norm{\mle - \truth}_1^2}{\sqrt{\widetilde{\lambda}_{\min}^{\epsilon} \, \norm{\by}_1}}$} \s\s\\
&\leq& \dfrac{\sqrt{p} \, \sqrt{\norm{\by}_1} \, p \, \norm{\mle - \truth}_2^2}{\sqrt{\widetilde{\lambda}_{\min}^{\epsilon}}} 
\quad\leq\quad \dfrac{p^{3/2}\, \sqrt{\norm{\by}_1} \, \norm{\mle - \truth}_2^2}{\sqrt{\widetilde{\lambda}_{\min}^{\epsilon}}}. 
\ee
By Chebyshev's inequality---as in the proof of Lemma \ref{lem:concentration_likelihood}---we can establish that
\be
\label{eq:event1}
\mbP\left(\left|\norm{\bY}_1 - \mbE \, \norm{\bY}_1\right| > \dfrac{1}{2} \, \mbE \, \norm{\bY}_1\right)
&\leq& \dfrac{4}{\mbE \, \norm{\bY}_1} + \dfrac{8 \, [D_{g}]^{+}}{(\mbE \, \norm{\bY}_1)^2}.
\ee
Under Assumptions \ref{assump1}, \ref{assump2} and \ref{assump3}, Theorem \ref{thm1} established that there exist constants $C>0$ and $N_0 \geq 3$ such that,
for all $N \geq N_0$, 
the event 
\be
\label{eq:event2}
\norm{\mle - \truth}_2 &\leq&
C \; \dfrac{\sqrt{\widetilde{\lambda}_{\max}^{\star}} }{\widetilde{\lambda}_{\min}^{\epsilon}} \; \sqrt{\dfrac{p}{\mbE \norm{\bY}_1}}
\ee
occurs with probability at least $1 - \exp\,(-2\,p) \, - \, (\mbE \, \norm{\bY}_1)^{-1}$.
Define $\mE_1$ to be the event
\beno 
|\norm{\bY}_1 - \mbE \, \norm{\bY}_1| 
&\leq& \dfrac{1}{2} \, \mbE \, \norm{\bY}_1
\ee
and $\mE_2$ to be the event in \eqref{eq:event2},
and define $\mcR$ to be the corresponding values of $\Delta$ in
the event $(\bX, \bY) \in \mE_1 \cap \mE_2$,
under which we have the bound
\be
\label{eq:Rbound2}
\norm{\Delta}_2
&\leq& \dfrac{p^{3/2}\, \sqrt{\norm{\by}_1}}{\sqrt{\widetilde{\lambda}_{\min}^{\epsilon}}} \, 
C^2 \; \dfrac{\widetilde{\lambda}_{\max}^{\star} }{(\widetilde{\lambda}_{\min}^{\epsilon})^2} \; \dfrac{p}{\mbE \norm{\bY}_1} \s\s\\
&\leq& \dfrac{C^2 \, p^{5/2} \, \sqrt{2 \, \mbE \, \norm{\bY}_1}}{\mbE \norm{\bY}_1} \, 
\dfrac{\widetilde{\lambda}_{\max}^{\star} }{(\widetilde{\lambda}_{\min}^{\epsilon})^{5/2}} \s \\
\= \dfrac{\sqrt{2} \, C^2 \, p^{5/2}}{\sqrt{\mbE \norm{\bY}_1}} \, 
\dfrac{\widetilde{\lambda}_{\max}^{\star} }{(\widetilde{\lambda}_{\min}^{\epsilon})^{5/2}}.
\ee
The first inequality in \eqref{eq:Rbound2} is obtained by combining the bounds in \eqref{eq:Rbound1} and \eqref{eq:event2}. The second inequality in \eqref{eq:Rbound2} is using the fact that 
\beno
\norm{\by}_1 
&\leq&  \mbE \, \norm{\bY}_1 + \dfrac{1}{2} \, \mbE \, \norm{\bY}_1 
&\leq& 2 \, \mbE \, \norm{\bY}_1
\ee 
in the event $\by \in \mE_1$.  
Moreover,
a union bound shows that
\beno
\mbP(\Delta \not\in \mcR) &\leq&
\mbP(\mE_1^c) \, + \, \mbP(\mE_2^c)
\s \\
&\leq & \exp\,(-2\,p) \, + \, \dfrac{5}{\mbE \, \norm{\bY}_1} + \dfrac{8 \, [D_{g}]^{+}}{(\mbE \, \norm{\bY}_1)^2} \s \\
&\leq& \exp\,(-2\,p) \, + \, \dfrac{5+ 8 \, C_0}{\mbE \, \norm{\bY}_1}, 
\ee
where the constant $C_0$ and the last inequality follow from Assumption \ref{assump1}.
Hence, 
\beno
\label{eq:R_bound}
\mbP\left(\norm{\Delta}_2 \leq 
\dfrac{\sqrt{2} \, C^2 \, p^{5/2}}{\sqrt{\mbE \norm{\bY}_1}} \, 
\dfrac{\widetilde{\lambda}_{\max}^{\star} }{(\widetilde{\lambda}_{\min}^{\epsilon})^{5/2}} \right)
&\geq& 1 - \exp\,(-2\,p) \, - \, \dfrac{5+ 8 \, C_0}{\mbE \, \norm{\bY}_1}. 
\ee
Taken together, 
we have shown under the assumptions of Theorem \ref{thm1} that there exists $N_0 \geq 3$ such that,
for all $N \geq N_0$,
the error of the multivariate normal approximation 
\beno
\left|\mbP((\mcI(\truth) \, \norm{\bY}_1)^{1/2} \, (\mle - \truth) - \Delta \in \mA) - \Phi(\bZ \in \mA)\right|
\ee
is bounded above by
\beno 
\dfrac{83}{(\widetilde{\lambda}_{\min}^{\epsilon})^{3/2}} \,
\sqrt{\dfrac{p^{7/2}}{\mbE \, \norm{\bY}_1}}
+  \dfrac{4}{\mbE \, \norm{\bY}_1} + \dfrac{8 \, \left[D_{g}\right]^{+}}{\left(\mbE \, \norm{\bY}_1 \right)^2}
\ee
where $\Delta$ satisfies 
\beno
\mbP\left(\norm{\Delta}_2 \leq 
\dfrac{\sqrt{2} \, C^2 \, p^{5/2}}{\sqrt{\mbE \norm{\bY}_1}} \, 
\dfrac{\widetilde{\lambda}_{\max}^{\star} }{(\widetilde{\lambda}_{\min}^{\epsilon})^{5/2}} \right)
&\geq& 1 - \exp\,(-2\,p) \, - \, \dfrac{5+ 8 \, C_0}{\mbE \, \norm{\bY}_1}.
\ee

\qed







\subsection{Auxiliary results for proof of Theorem \ref{thm2}} 
\begin{lemma}
\label{lem:Taylor}
Consider a separable multilayer network model following the form of equation \eqref{general_model} and is 
defined on a set of $N \geq 3$ 
and $K \geq 1$ layers and denote by $\ell(\nat; \bx,\by)$ the log-likelihood function.
Then,
for each  $i = 1, \ldots, p$ and $\epsilon > 0$,
\beno
\sup\limits_{\nat \in \mbR^p \,:\, \norm{\nat - \truth}_2 \leq \epsilon} \quad  
\left| \dfrac{\partial^2 \, (\nabla_{\nat} \, \ell(\nat;\bx,\by))_i}{\partial\, \theta_j \, \partial\, \theta_k} \right|
&\leq& 2 \, \norm{\by}_1,
\ee 
where $(\nabla_{\nat} \, \ell(\nat;\bx,\by))_i$ is the $i^{th}$ component of the score function 
$\nabla_{\nat} \, \ell(\nat;\bx,\by)$. 
\end{lemma}

\s 

\llproof \ref{lem:Taylor}. 
By Proposition \ref{prop:inference}, 
given the observation $\bx$ of $\bX$ (i.e., observation of the event $\bX = \bx$), 
$\bY$ is predictable with unique value $\by \in \mbY$ 
given by the formula in Proposition \ref{prop:inference},
and $(\bx, \by)$ is network concordant. 
Further, 
by Proposition \ref{prop:inference}  
\beno
\ell(\nat; \bx, \by)
\= \log \, \mbP_{\nat}(\bX = \bx \mid \bY = \by) 
+ \log \, g(\by), 
\ee
where $\log \, \mbP_{\nat}(\bX = \bx | \bY = \by)$ 
is the log-likelihood of a minimal, full, and regular 
exponential family. 
Thus, 
the second order derivative of 
$\ell(\nat; \bx, \by)$ with respect to the $i^{\text{th}}$ and $j^{\text{th}}$ 
components of $\nat$ correspond to the variance (in the case $i = j$) 
or covariance (in the case of $i \neq j$) 
of corresponding sufficient statistic(s) of the exponential family \citep[e.g., Proposition 3.8, p. 29,][]{Su19},
with sufficient statistics given in Lemma \ref{lem:s_hetero}.  
For $\{i, j\} \subseteq \{1, \ldots,p\}$, 
\beno
\dfrac{\partial \, (\nabla_{\nat} \, \ell(\nat; \bx, \by))_i}{\partial\, \theta_j} 
\= \dfrac{\partial^2 \, \ell(\nat;\bx,\by)}{\partial\, \theta_i \; \partial\, \theta_j}
\= \cov_{\nat}(s_i(\bX),s_j(\bX) \,|\, \bY = \by),
\ee
and when $i = j \in \{1, \ldots, p\}$,
\beno
\dfrac{\partial \, (\nabla_{\nat} \, \ell(\nat;\bx,\by))_i}{\partial\, \theta_i} 
\= \dfrac{\partial^2 \, \ell(\nat; \bx, \by)}{\partial \, \theta_i^2}
\= \var_{\nat}(s_i(\bX) \,|\, \bY = \by).
\ee
As a result, for $\{i,j\} \subseteq \{1,\ldots,p\}$ and $k \in \{1, \ldots, p\}$,
\beno
\left| \, \dfrac{\partial^2 \, (\nabla_{\nat} \, \ell(\nat;\bx,\by))_i}{\partial\, \theta_j \, \partial\, \theta_k} \, \right| 
\=
\left| \, \dfrac{\partial \,\cov_{\nat}(s_i(\bX),s_j(\bX) \,|\, \bY = \by)}{\partial \, \theta_k} \, \right|,
\ee
and when $i = j \in \{1, \ldots, p\}$ and $k \in \{1, \ldots, p\}$,
\beno
\left| \, \dfrac{\partial^2 \, (\nabla_{\nat} \, \ell(\nat;\bx,\by))_i}{\partial\, \theta_i \, \partial\, \theta_k} \, \right| 
\=
\left| \, \dfrac{\partial \,\var_{\nat}(s_i(\bX) \,|\, \bY = \by)}{\partial \, \theta_k} \, \right|.
\ee
By Lemma \ref{lem:s_hetero} equation \eqref{eq:suff}, 
conditional on $\bY = \by$, 
each sufficient statistic $s_i(\bX)$ ($i \in \{1, \ldots, p\}$) 
can be decomposed into the sum of conditionally independent statistics of each dyad 
$\bX_{v,w}$, for $\{ v,w \} \subseteq \mN$. 
We can then write 
\beno
\scalebox{0.95}{$\cov_{\nat}(s_i(\bX),s_j(\bX) \,|\, \bY = \by)$} 
\=  \scalebox{0.95}{$\dsum_{\{v,w\} \subset \mN} \, \cov_{\nat}(s_{i,v,w}(\bX_{v,w}), \, s_{j,v,w}(\bX_{v,w}) \,|\, \bY = \by)$}, 
\ee
noting that by conditional independence 
$\cov_{\nat}(s_{i,v,w}(\bX_{v,w}),s_{j,r,t}(\bX_{r,t}) \,|\, \bY = \by) = 0$
whenever $\{r,t\} \neq \{v,w\}$, 
and when $i = j$, we can write
\beno
\var_{\nat}(s_i(\bX) \,|\, \bY = \by) 
\= \dsum_{\{v,w\} \subset \mN} \,  \var_{\nat}(s_{i,v,w}(\bX_{v,w}) \,|\, \bY = \by),
\ee
again appealing to the conditional independence given $\bY$ of the random variables 
$s_{i,v,w}(\bX_{v,w})$ ($\{v,w\} \subset \mN$). 
As a result, for $k \in \{1, \ldots, p\}$, it suffices to show that, 
\beno
\left|\,\dfrac{\partial \,\cov_{\nat}(s_{i,v,w}(\bX_{v,w}), \, s_{j,v,w}(\bX_{v,w}) \,|\, \bY = \by )}{\partial \, \theta_k} \,\right|
& \leq & 2,
\ee
and 
\beno
\left| \,\dfrac{\partial \,\var_{\nat}(s_{i,v,w}(\bX_{v,w}) \,|\, \bY = \by)}{\partial \, \theta_k} \, \right|
& \leq & 1.
\ee
Recall that the sufficient statistic $s_{i,v,w}(\bX)$ ($i = 1, \ldots ,p$) 
is defined in Lemma \ref{lem:s_hetero} by
\beno
s_{i,v,w}(\bX_{v,w}) \= \dprod_{t=1}^{h} \, X_{v,w}^{(k_{t})},  
&& \{v,w\} \subset \mN, 
\ee
for some $h \in \{1, \ldots, H\}$ 
and $\{k_1, \ldots, k_h\} \subseteq \{1, \ldots, K\}$. 
Define the set $S_{i,v,w}$ of components of the sufficient statistic vector $\bs_{v,w}(\bX)$ for $\{v,w\} \subset \mN$ and $i = 1,\ldots,p$ by
\beno
S_{i,v,w} \; \coloneqq \; \left\{ \dprod_{t=1}^{h^\prime} \, X_{v,w}^{(l_{t})} \; : \; \{l_1, \ldots, l_{h^\prime} \}  \subset \{k_1,\ldots,k_h\}, \; h^\prime < h \right\},
\ee
where $h \in \{1, \ldots, H\}$ and $\{k_1, \ldots, k_h\} \subseteq \{1, \ldots, K\}$.
The set $S_{i,v,w}$ is the set of components of the sufficient statistic vector $\bs_{v,w}(\bX)$ of dyad $\{v,w\} \subset \mN$ that have a value of 1 when $s_{i,v,w}(\bX) = 1$.
For the ease of notation, 
let $I_{S_{i,v,w}}$ be the set of dimension indices whose corresponding components of the sufficient statistic vector $\bs_{v,w}(\bX)$ belong to the set $S_{i,v,w}$:
\beno
I_{S_{i,v,w}} \; \coloneqq \; \{ j \in \{1,\ldots,p\} \; : \; s_{j,v,w}(\bX) \in S_{i,v,w} \}.
\ee
Define the conditional expectation of $s_{i,v,w}(\bX)$ given $\bY = \by$ for any $i \in \{1,\ldots,p\}$ and $\{v,w\} \subset \mN$ by
\beno
P_{i,v,w}(\nat ; \bX, \by) \; \coloneqq  \; \mbP_{\nat} \, (s_{i,v,w}(\bX) = 1 \, | \, \bY = \by). 
\ee
Denote by $L_i$ the set of layer indices $\{k_1, \ldots, k_h\} \subseteq \{1, \ldots, K\}$ that define the $i^{th}$ component $s_{i,v,w}(\bX_{v,w})$ of the sufficient statistic vector $\bs_{v,w}(\bX)$ 
for any $\{v,w\} \subset \mN$, $j \in \{1,\ldots,p\}$, and some $h \in \{1, \ldots, H\}$.
We then define
\beno
\bX_{v,w }^{(L_i)} & \coloneqq & \left\{X_{v,w}^{(k_1)}, \ldots, X_{v,w}^{(k_h)} \right\}, & \; &
\bX_{v,w }^{(-L_i)} \; \coloneqq \; \bX_{v,w} \setminus \bX_{v,w }^{(L_i)}, 
\ee
and the corresponding sample space
\beno
\mbX_{v,w }^{(L_i)} & \coloneqq & \{0,1\}^h, & \; &
\mbX_{v,w }^{(-L_i)} \; \coloneqq \; \{0,1\}^{H-h}, 
\ee
for some $h \in \{1, \ldots, H\}$.
Then we can write
\beno
P_{i,v,w}(\nat ; \bX, \by) 
\= \mbP_{\nat}\left(\dprod_{l \in L_i} \, X_{v,w}^{(l)} = 1 \, | \, \bY = \by \right) \s \\
\= \dfrac{\dsum_{\mbX_{v,w }^{(-L_i)}} \, \exp\,\left(\sum_{j \in I_{S_{i,v,w}}}\,\theta_j + \sum_{j \in I_{S_{i,v,w}}^c}\, \theta_j\, s_{j,v,w}(\bx)\right)}{\dsum_{\mbX_{v,w}} \, \exp\left( \sum_{j = 1}^p \, \theta_j \, s_{j,v,w}(\bx) \right)}.
\ee
Let 
\beno
Z(\nat) & \coloneqq & \dsum_{\mbX_{v,w}} \, \exp\left( \sum_{j = 1}^p \, \theta_j \, s_{j,v,w}(\bx) \right),
\ee
and take the derivative of $P_{i,v,w}(\nat; \bx,\by)$ with respect to $\theta_k$ for $k = 1,\ldots,p$. We have
\be
\label{cov_ineq}
\dfrac{\partial\, P_{i,v,w}(\nat ; \bX, \by) }{\partial\,\theta_k} \s \\
 \leq \quad
 \scalebox{0.9}{$\dfrac{\dsum_{\mbX_{v,w }^{(-L_i)}} \, \exp\,\left(\sum_{j \in I_{S_{i,v,w}}}\,\theta_j + \sum_{j \in I_{S_{i,v,w}}^c}\, \theta_j\, s_{j,v,w}(\bx)\right) \, \left(Z(\nat) - \dfrac{\partial \, Z(\nat) }{ \partial \, \theta_k}\right)}{Z(\nat)^2}$} \s \\
 =  \scalebox{0.7}{$\dfrac{\dsum_{\mbX_{v,w }^{(-L_i)}} \, \exp\,\left(\sum_{j \in I_{S_{i,v,w}}}\,\theta_j + \sum_{j \in I_{S_{i,v,w}}^c}\, \theta_j\, s_{j,v,w}(\bx)\right) \, \left(\dsum_{\mbX_{v,w}} \, \exp\left( \sum_{j = 1}^p \, \theta_j \, s_{j,v,w}(\bx)\right) \, (1-s_{k,v,w}(\bx))\right)}{Z(\nat)^2}$} \s \\
 \leq \quad 
 \scalebox{0.9}{$\dfrac{\dsum_{\mbX_{v,w }^{(-L_i)}} \, \exp\,\left(\sum_{j \in I_{S_{i,v,w}}}\,\theta_j + \sum_{j \in I_{S_{i,v,w}}^c}\, \theta_j\, s_{j,v,w}(\bx)\right) \, Z(\nat)}{Z(\nat)^2}$} \s \\
 \leq \quad
1.
\ee
The first inequality is obtained because $s_{k,v,w} (\bx) \leq 1$, and the last inequality is due to the fact that
\beno
\dsum_{\mbX_{v,w }^{(-L_i)}} \, \exp\,\left(\sum_{j \in I_{S_{i,v,w}}}\,\theta_j + \sum_{j \in I_{S_{i,v,w}}^c}\, \theta_j\, s_{j,v,w}(\bx)\right) 
\ee
is bounded above by 
\beno
\dsum_{\mbX_{v,w}} \, \exp\left( \sum_{j = 1}^p \, \theta_j \, s_{j,v,w}(\bx) \right).
\ee
Now we turn to show the derivative of the conditional variance and covariance of the sufficient statistics of each dyad are bounded. Given $\bY = \by$, for all $\{i\} \subset \{ 1,\ldots, p\}$, $s_{i,v,w} (\bX)$ are conditionally independent across $\{v,w\} \subseteq \mN$.
Then we have 
\beno
& \cov_{\nat}(s_{i,v,w}(\bX_{v,w}), \, s_{j,v,w}(\bX_{v,w}) \,|\, \bY = \by) \s \\
 = & \scalebox{0.95}{$\mbE \,[ s_{i,v,w}(\bX) \, s_{j,v,w}(\bX) \,|\, \bY = \by] - \mbE\, [s_{i,v,w}(\bX)\,|\, \bY = \by] \, \mbE \, [s_{j,v,w}(\bX)\,|\, \bY = \by]$} \s \\
 = & \mbP_{\nat} \, (s_{i,v,w}(\bX)  = 1, s_{j,v,w}(\bX) =1 \,|\, \bY = \by) - P_{i,v,w} (\nat;\bX,\by) \, P_{j,v,w} (\nat;\bX,\by) \s \\
 = & \mbP_{\nat} \left( \dprod_{l \in L_i \cup L_j} \bX_{v,w}^{(l)} = 1  \, | \, \bY = \by \right) - P_{i,v,w} (\nat;\bX,\by) \, P_{j,v,w} (\nat;\bX,\by).
\ee
Using the inequality derived in \eqref{cov_ineq} and suppressing the notation of $\{v,w\}$ and $(\bX, \by)$ in $P_{i,v,w} (\nat;\bX,\by)$, the derivative of the covariance with respect to $\theta_k$, $k = 1,\ldots, p$ is given by
\beno
& \left| \, \dfrac{\partial \,\cov_{\nat}(s_{i,v,w}(\bX_{v,w}), \, s_{j,v,w}(\bX_{v,w}) \,|\, \bY = \by)}{\partial \, \theta_k} \, \right| \s \\
= & \dfrac{\partial \, \mbP_{\nat} \left(\dprod_{l \in L_i \cup L_j} \bX_{v,w}^{(l)} = 1  \, | \, \bY = \by \right)}{\partial \, \theta_k} - \dfrac{\partial \, P_i(\nat)}{\partial \, \theta_k} \, P_j(\nat) - P_i(\nat) \, \dfrac{\partial \, P_j(\nat)}{\partial \, \theta_k}  \s \\
 \leq & 2.
\ee
Using the same inequality and notation in \eqref{cov_ineq}, the derivative of the variance of a Bernoulli random variable $s_{i,v,w} (\bX)$ is given by
\beno
\left| \,\dfrac{\partial \,\var_{\nat}(s_{i,v,w}(\bX_{v,w}) \,|\, \bY = \by)}{\partial \, \theta_k} \, \right| 
\= \left|\,\left(1 - 2 \, P_i(\nat) \right) \, \dfrac{\partial \, P_i(\nat)}{\partial \, \theta_k} \,\right|
& \leq & 
1.
\ee
Finally, for $\{i,j\} \subseteq \{1,\ldots,p\}$ and $k \in \{1, \ldots, p\}$, we obtain
\beno
\left| \, \dfrac{\partial^2 \, (\nabla_{\nat} \, \ell(\nat;\bx,\by))_i}{\partial\, \theta_j \, \partial\, \theta_k} \, \right| 
\=
\left| \, \dfrac{\partial \,\cov_{\nat}(s_i(\bX),s_j(\bX) \,|\, \bY = \by)}{\partial \, \theta_k} \, \right| \s \\
& \leq & \scalebox{0.95}{$\dsum_{\{v,w\} \subset \mN} \, \left| \dfrac{\partial \, \cov_{\nat}(s_{i,v,w}(\bX_{v,w}), \, s_{j,v,w}(\bX_{v,w}) \,|\, \bY = \by)}{\partial \, \theta_k} \, \right|$} \s \\
& \leq & 2 \, \norm{\by}_1
\ee
due to the fact that $\cov_{\nat}(s_{i,v,w}(\bX_{v,w}), \, s_{j,v,w}(\bX_{v,w}) \,|\, \bY = \by) = 0$ when $Y_{v,w} = 0$ for $\{v,w\} \subset \mN$.
Similarly, $\var_{\nat}(s_{i,v,w}(\bX_{i,v,w}) \,|\, \bY = \by) = 0$ when $Y_{v,w} = 0$ for $\{v,w\} \subset \mN$, 
and when $i = j \in \{1, \ldots, p\}$ and $k \in \{1, \ldots, p\}$, we have
\beno
\left| \, \dfrac{\partial^2 \, (\nabla_{\nat} \, \ell(\nat;\bx,\by))_i}{\partial\, \theta_i \, \partial\, \theta_k} \, \right| 
\=
\left| \, \dfrac{\partial \,\var_{\nat}(s_i(\bX) \,|\, \bY = \by)}{\partial \, \theta_k} \, \right| \s \\
& \leq & \norm{\by}_1.
\ee

\qed
