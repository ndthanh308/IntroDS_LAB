
\begin{theorem}
\label{thm1}
Consider a multilayer network model following the form of equation \eqref{general_model} and is 
defined on a set of $N \geq 3$ nodes. If Assumptions \ref{assump1}, \ref{assump2}, and \ref{assump3} are satisfied,
there exists $N_0 \geq 3$ such that, 
for all $N \geq N_0$,
with probability at least $1 - \exp\,(-2\,p) \, - \, (\mbE \, \norm{\bY}_1)^{-1}$,
the set $\Mle$ is non-empty and the unique element $\mle \in \Mle$ satisfies  
\be
\label{consistency}
\norm{\mle - \truth}_2 &\leq&
C \; \dfrac{\sqrt{\widetilde{\lambda}_{\max}^{\star}} }{\widetilde{\lambda}_{\min}^{\epsilon}} \; \sqrt{\dfrac{p}{\mbE \norm{\bY}_1}},
\ee
where $C > 0$ is a constant independent of $N$ and $p$. 

\end{theorem}

The results of Theorem \ref{thm1} establish a few key facts concerning statistical estimation 
of the data-generating parameter vector $\truth$. 
First, 
we can view the quantity $\widetilde{\lambda}_{\min}^{\epsilon}\, \sqrt{\mbE \, \norm{\bY}_1 \, / \, \widetilde{\lambda}_{\max}^{\star}}$ 
as the effective sample size in order to compare our results to 
classical settings with independent and identically distributed data. 
The effective sample size, 
together with the dimension of the model $p$, 
helps to determine the rate of convergence (with respect to the Euclidean distance) 
of maximum likelihood estimators. 
As previously mentioned, 
the quantity $\mbE \norm{\bY}_1$ 
is determined by properties of $g(\by)$,
the marginal probability mass function of $\bY$. 
While the specification of $g(\by)$ does not directly influence the estimation algorithm, 
the statistical guarantees of estimators will depend on $g(\by)$ producing enough activated dyads and not possessing overly strong 
dependence among edges in the single-layer basis network $\bY$ (Assumption \ref{assump1}).
The requirement (Assumption \ref{assump3}) that the right-hand side of the bounds in Theorem \ref{thm1} tends to $0$ as $N \to \infty$ 
ensures that all regularity assumptions remain valid. 
Namely, 
key to our approach lies in the ability to control minimum eigenvalues of matrices 
$\mcI(\nat)$
in a neighborhood of the data-generating parameter vector $\truth$. 
The condition that the bounds tend to $0$ ensures that it is sufficient to control the smallest eigenvalue
in a bounded set,
i.e.,
we may let $\epsilon$ be fixed independent of $N$ and $p$, 
and moreover, 
to ensure consistency in the sense that $\norm{\mle - \truth}_2 \to 0$ with probability approaching $1$ as $N, p \to \infty$.  


\begin{corollary}
\label{corollary}
Under the assumptions of Theorem \ref{thm1}, and in the case that the parameter dimension $p$ is fixed, there exists $N_0 \geq 3$ such that, 
for all $N \geq N_0$ and $\alpha_N \in (2\,(\mbE\norm{\bY}_1)^{-1} \, , \, 1/2)$,
with probability at least $1 \, - \, \alpha_N$,
the set $\Mle$ is non-empty and the unique element $\mle \in \Mle$ satisfies  
\beno
\norm{\mle - \truth}_2 &\leq&
C \; \left| \, \log\, \left(\dfrac{\alpha_N}{2} \right) \, \right|  \; \dfrac{\sqrt{\widetilde{\lambda}_{\max}^{\star}} }{\widetilde{\lambda}_{\min}^{\epsilon}} \; \sqrt{\dfrac{1}{\mbE \norm{\bY}_1}},
\ee
where $C>0$ is a constant independent of $N$.
\end{corollary}

The corollary builds on the consistency result established in Theorem \ref{thm1} by providing a similar bound in the situation that the parameter dimension $p$ remains fixed. This simplifies the convergence rate by removing the dependence of $p$ in the error term, which can yield sharper asymptotic guarantees. The introduction of the probability bound $\alpha_N$ offers an explicit control over the confidence level for the estimate's accuracy, which improves interpretability and practical applicability in finite samples. The bound on $\norm{\mle - \truth}_2$ in Corollary \ref{corollary} now depends logarithmically on $\alpha_N$, introducing a trade-off between the confidence level and the convergence rate. While the key dependencies remain on the effective sample size $\widetilde{\lambda}_{\min}^{\epsilon}\, \sqrt{\mbE \, \norm{\bY}_1 \, / \, \widetilde{\lambda}_{\max}^{\star}}$ as in Theorem \ref{thm1}, Corollary \ref{corollary} provides a useful refinement of the consistency result when the model's dimensionality is constrained.

We next present that the upper bound in Theorem \ref{thm1} is minimax optimal up to a constant. Define the minimax risk to be
\beno
\mcR_N &\coloneqq& \inf\limits_{\mle}\;\sup\limits_{\nat \in \mbR^p} \; \mbE_{\nat}\,\norm{\mle - \nat}_2,
\ee
and
\beno
\widetilde{\lambda}_{\max}^{\epsilon}
&\coloneqq & \sup\limits_{\nat \in \mB_2(\truth, \epsilon)} \, \lambda_{\max} \, (\mcI(\nat)),
\ee
where $\lambda_{\max}(\bA)$ is the largest eigenvalue of matrix $\bA \in \mbR^{p \times p}$.

\begin{theorem}
\label{thm:minimax}
Consider a separable multilayer network model following the form of equation \eqref{general_model} and is 
defined on a set of $N \geq 3$ nodes. If Assumptions \ref{assump1}, \ref{assump2} and \ref{assump3} are satisfied,
there exists a constant $C > 0$ independent of $N$ and $p$, such that, 
the lower bound of the minimax risk $\mcR_N$ satisfies
\beno
\mcR_N
& \geq &
C \, \dfrac{\widetilde{\lambda}_{\min}^{\epsilon}}{\widetilde\lambda_{\max}^{\epsilon}} \, \dfrac{\sqrt{\widetilde{\lambda}_{\max}^{\star}}}{\widetilde{\lambda}_{\min}^{\epsilon}} \, \sqrt{\dfrac{p}{\mbE\,\norm{\bY}_1} }.
\ee
\end{theorem}

Theorem \ref{thm:minimax} establishes a lower bound for the minimax risk $\mcR_N$, differing from the upper bound of the $\ell_2$-error for the maximum likelihood estimator in Theorem \ref{thm1} by a factor of $\widetilde{\lambda}_{\min}^{\epsilon} \, / \, \widetilde\lambda_{\max}^{\epsilon}$. Building on this result, we establish conditions for the minimax optimality of the maximum likelihood estimators in Corollary \ref{cor:minimax}.

\begin{corollary}
    \label{cor:minimax}
    Under the assumptions of Theorem \ref{thm:minimax} and the assumption that
    \be
    \label{assump:minimax}
    \widetilde\lambda_{\max}^{\epsilon}= O\, \left(\widetilde{\lambda}_{\min}^{\epsilon}\right),
    \ee
    the maximum likelihood estimator $\mle$ achieves the minimax rate of convergence, in the sense that the upper bound on the $\ell_2$-error of $\mle$ given in Theorem \ref{thm1} matches the lower bound of the minimax risk $\mcR_N$ in Theorem \ref{thm:minimax}, up to a constant.
\end{corollary}

The condition in \eqref{assump:minimax} ensures that the rate of convergence for the maximum likelihood estimator achieves the minimax optimality by imposing a more direct and stringent relationship between the minimum and maximum eigenvalues than that required by Assumption \ref{assump3}. The control on the minimum and maximum eigenvalues for high-dimensional graphical models are common \citep[e.g.,][]{witten14, Zhao06, RaWaLa10}, ensuring that the minimum and maximum eigenvalues of the information matrices within a neighborhood of the data-generating parameter are bounded away from 0 and bounded from above, respectively, and do not diverge relative to one another. 


\hide{
\begin{corollary}[version 2]
Under the assumptions of Theorem \ref{thm1}, and in the case that the parameter dimension $p$ is fixed, there exists $N_0 \geq 3$ such that, 
for all $N \geq N_0$ and $\gamma > 0$,
with probability at least $1 \, - \, \exp \, (-\gamma \, p ) \, - \, (\mbE \, \norm{\bY}_1)^{-1}$,
the set $\Mle$ is non-empty and the unique element $\mle \in \Mle$ satisfies  
\beno
\norm{\mle - \truth}_2 &\leq&
\beta \, (\gamma) \; \dfrac{\sqrt{\widetilde{\lambda}_{\max}^{\star}} }{\widetilde{\lambda}_{\min}^{\epsilon^\star}} \; \sqrt{\dfrac{p}{\mbE \norm{\bY}_1}},
\ee
where $\beta : (0 \, , \, \infty) \mapsto (C_2 \; + \; \sqrt{C_2^2 \; + \; 72 \, C_2} \; / \; 2 \, , \,  \infty)$ is an unbounded increasing function of $\gamma$ given by
\beno
\beta\, (\gamma) \= \dfrac{C_2 \; + \;  C_2 \, \gamma \; + \; \sqrt{(C_2\; + \;  C_2 \, \gamma )^2 \; + \; 72\,(C_2\; + \; C_2 \, \gamma )}}{2},
\ee
where $C_2 > 0$ is a constant independent of $N$ and $\gamma$.
\end{corollary}
}
