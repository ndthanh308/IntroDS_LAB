

\pagebreak

\appendix

\makeatletter

\setcounter{page}{1}

\setcounter{section}{0}

\setcounter{com}{0}



\begin{center}
\Large\bf\textsc\bf
Supplement:\\
Learning cross-layer dependence structure in multilayer networks \s\s

\normalsize
{\normalfont\textsc{By Jiaheng Li and Jonathan R. Stewart}} \s 
\\
{\normalfont\em Department of Statistics, Florida State University} \s\s
\end{center}

\s\s


\noindent
{Appendix \ref{sec:prop_proof}: Proof of Proposition \ref{prop:inference}}\dotfill\pageref{sec:prop_proof}\s\\ 
{Appendix \ref{sec:lem1_proof}: Proof of Lemma \ref{lem:min-eig}}\dotfill\pageref{sec:lem1_proof}\s\\
{Appendix \ref{sec:concentration}: Concentration inequalities for multilayer networks}\dotfill\pageref{sec:concentration}\s\\
{Appendix \ref{sec:pf_thm1}: Proof of Theorem \ref{thm1} and Corollary \ref{corollary}}\dotfill\pageref{sec:pf_thm1}\s\\
{Appendix \ref{sec:pf_minimax}: Proof of Theorem \ref{thm:minimax} and Corollary \ref{cor:minimax}}\dotfill\pageref{sec:pf_thm1}\s\\
{Appendix \ref{sec:pf_prop2}: Proposition \ref{prop:suff_norm} and proof}\dotfill\pageref{sec:pf_prop2}\s\\
{Appendix \ref{sec:pf_thm2}: Proof of Theorem \ref{thm2}}\dotfill\pageref{sec:pf_thm2}\s\\
{Appendix \ref{sec:add_sim}: Additional simulation results}\dotfill\pageref{sec:add_sim}\s\\ 


\section{Proof of Proposition \ref{prop:inference}} 
\label{sec:prop_proof} 

We prove Proposition \ref{prop:inference} from Section \ref{sec2}. 
\input{proof_prop_inference}
\s\s


\section{Proof of Lemma \ref{lem:min-eig}}
\label{sec:lem1_proof}

We prove Lemma \ref{lem:min-eig} from Section \ref{sec2}.
\input{proof_min_eig_lemma}
\s


\section{Concentration inequalities for multilayer networks} 
\label{sec:concentration}


We establish the concentration inequality of gradients of log-likelihood functions 
of multilayer networks in Lemma \ref{lem:concentration_likelihood}.
Recall the definition $[D_{g}]^{+} \coloneqq \max\{0, \, D_{g}\}$, 
where  
\beno 
D_{g}
&\coloneqq& \dsum_{\{i,j\} \prec \{v,w\} \subset \mN} \, \cov(Y_{i,j}, \, Y_{v,w}), 
\ee 
with $\{i,j\} \prec \{v,w\}$ implying the sum is taken with respect to the lexicographical ordering 
of pairs of nodes, 
and where $g : \mbY \mapsto (0, 1)$ is the marginal probability mass function of $\bY$.  

\s\s

\input{concentration_likelihood}

%\input{concentration_PL}

%\llproof \ref{lem:concentration_pl}. 
%\input{proof_concentration_PL}


\s\s
\subsection{Auxiliary results}
\input{suff_exp}


\section{Proof of Theorem \ref{thm1}}
\label{sec:pf_thm1}
We prove Theorem \ref{thm1} from Section \ref{sec3}.
\input{proof_thm1}


\s\s
\section{Proposition \ref{prop:suff_norm} and proof} 
\label{sec:pf_prop2}

In order to establish a bound on the error of the multivariate normal approximation 
for estimators of data-generating parameters, 
we first establish an error bound on the multivariate normal approximation 
of a standardization of the 
sufficient statistic vector $\bs(\bX)$ of the exponential family distribution of $\bX$ given $\bY$,
derived in Lemma \ref{lem:s_hetero},
in Proposition \ref{prop:suff_norm} 
using a Lyapunov type bound presented in \citet{Raic19}.
Proposition \ref{prop:suff_norm} provides the basis for our normality proof for estimators
which we present in Theorem \ref{thm2}.
\input{prop2_normal}

\input{proof_prop2}
\s\s
\section{Proof of Theorem \ref{thm2}}
\label{sec:pf_thm2}

\input{proof_thm2}


\section{Additional simulation results} 
\label{sec:add_sim} 

Additional simulation results that enhance those contained in Section \ref{sec:sim} are provided in this section.


\subsection{Normal approximation with different basis networks}
\label{subsec:norm_sim}

The multivariate normality of $\mple$ is tested by Zhou-Shao's multivariate normal test \citep{Zhou13}, and the p-values are provided in tabel \ref{ZS-test}. Q-Q plots of $\mple$ estimated from 6 different model-generating parameters with a dense Bernoulli basis network, a sparse Bernoulli basis network, a stochastic block model (SBM) generated basis network, and a latent space model (LSM) generated basis network are shown in Figure \ref{qqplot_dense}, \ref{qqplot_sparse}, \ref{qqplot_SBM} and \ref{qqplot_LSM}, respectively.

\begin{table}[t]
\begin{center}
\caption{\label{ZS-test} P-values of the Zhou-Shao's test for multivariate normality of $\mple$ for 6 model-generating parameters ($\truth_1$, $\truth_2$, $\truth_3$, $\truth_4$, $\truth_5$, $\truth_6$) estimated from 250 network samples at size 1000 on four basis network structures. All p-values are larger than .1. \s} 
\begin{tabular}{ l  r r r  r r  r  } 
\hline
 Basis network model  & $\truth_1$ & $\truth_2$  & $\truth_3$ & $\truth_4$ & $\truth_5$ & $\truth_6$ \\ 
\hline
  Dense Bernoulli & .138 & .473 & .053 & .699 & .587 & .983  \\

 Sparse Bernoulli & .554 & .132 & .232 & .634 & .904 & .373  \\

 SBM & .650 & .891 & .982 & .975 & .871 & .674 \\

 LSM  & .859 & .831 & .500 & .227 & .613 & .409  \\
\hline
\end{tabular}
\end{center}
\end{table}

\vspace{.25in}

\begin{center}
% Figure removed 
\captionof{figure}{Q-Q plots and p-values of six components of $\mple$ estimated from 250 multilayer network samples at size 1000 on the dense Bernoulli basis network for 6 model-generating parameters on each row.}\label{qqplot_dense}
\end{center}

\begin{center}
% Figure removed 
\captionof{figure}{Q-Q plots and p-values of six components of $\mple$ estimated from 250 multilayer network samples at size 1000 on the sparse Bernoulli basis network for 6 model-generating parameters on each row.}\label{qqplot_sparse}
\end{center}

\begin{center}
% Figure removed
\captionof{figure}{Q-Q plots and p-values of six components of $\mple$ estimated from 250 multilayer network samples at size 1000 on the SBM generated basis network for 6 model-generating parameters on each row.}\label{qqplot_SBM}
\end{center}


\begin{center}
% Figure removed
\captionof{figure}{Q-Q plots and p-values of six components of $\mple$ estimated from 250 multilayer network samples at size 1000 on the LSM generated basis network for 6 model-generating parameters on each row.}\label{qqplot_LSM}
\end{center}



\subsection{Additional results on the false discovery rate}
\label{more fdr}
The false discovery rate (FDR) of the multiple testing correction procedures of
Bonferroni, Benjamini-Hochberg, Hochberg, and Holm to detect non-zero components of $\truth$ at a family-wise significance level of $\alpha = 0.05$ with a sparse Bernoulli basis network, an SBM generated basis network and an LSM generated basis network are provided in Table \ref{fdr_sparse}, \ref{fdr_SBM} and \ref{fdr_LSM}, respectively (recall that components $\theta_{1,3}^\star$ and $\theta_{3}^\star$ of $\truth$ are set to 0). The receiver operating characteristic (ROC) curves for $\mple$ of 6 selected model-generating parameters on four basis network structures are provided in each of the subplot of Figure \ref{ROC}.


\begin{table}[t]
\begin{center}
\caption{\label{fdr_sparse} False discovery rates of four procedures for detecting non-zero effects of six model-generating parameters ($\truth_1$, $\truth_2$, $\truth_3$, $\truth_4$, $\truth_5$, $\truth_6$) estimated from 250 multilayer network samples at size 1000 on the sparse Bernoulli basis network. All FDRs are smaller than 0.05.} 
\begin{tabular}{l  r r r  r r  r } 
\hline
  Procedure & $\truth_1$ & $\truth_2$  & $\truth_3$ & $\truth_4$ & $\truth_5$ & $\truth_6$ \\ 
\hline
  Bonferroni    & .002  & .003 & .003 & .003 & .003 & .011  \\

  Benjamini-Hochberg   & .020 & .011 & .022 & .022 & .014 & .017  \\

 Hochberg's &    .009 & .008 & .012 & .010 & .010 & .014 \\

 Holm's  &   .007 & .008 & .011 & .009 & .006 &  .014 \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}%[b]
\begin{center}
\caption{\label{fdr_SBM} False discovery rates of four procedures for detecting non-zero effects of six model-generating parameters ($\truth_1$, $\truth_2$, $\truth_3$, $\truth_4$, $\truth_5$, $\truth_6$) estimated from 250 multilayer network samples at size 1000 on the SBM generated basis network. All FDRs are smaller than 0.05.} 
\begin{tabular}{l  r r r  r r  r } 
\hline
  Procedure & $\truth_1$ & $\truth_2$  & $\truth_3$ & $\truth_4$ & $\truth_5$ & $\truth_6$ \\ 
\hline
  Bonferroni    & .002  & .002 & .003 & .001 & .001 & .004  \\

  Benjamini-Hochberg   & .022 & .013 & .014 & .015 & .015 & .018  \\

 Hochberg's &    .009 & .014 & .01 & .008 & .011 & .014 \\

 Holm's  &   .009 & .013 & .005 & .009 & .009 &  .011 \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}%[b]
\begin{center}
\caption{\label{fdr_LSM} False discovery rates of four procedures for detecting non-zero effects of six model-generating parameters ($\truth_1$, $\truth_2$, $\truth_3$, $\truth_4$, $\truth_5$, $\truth_6$) estimated from 250 multilayer network samples at size 1000 on the LSM generated basis network. All FDRs are smaller than 0.05.} 
\begin{tabular}{l  r r r  r r  r } 
\hline
  Procedure & $\truth_1$ & $\truth_2$  & $\truth_3$ & $\truth_4$ & $\truth_5$ & $\truth_6$ \\ 
\hline
  Bonferroni    & .004  & .006 & .000 & .005 & .003 & .004  \\

  Benjamini-Hochberg   & .016 & .013 & .011 & .015 & .016 & .017  \\

 Hochberg's &    .009 & .014 & .009 & .011 & .010 & .011 \\

 Holm's  &   .008 & .014 & .009 & .011 & .007 &  .010 \\
\hline
\end{tabular}
\end{center}
\end{table}

% Figure environment removed

\pagebreak

\input{new-submission.bbl}



 




