
We outline a procedure for model selection that controls the false discovery rate,
leveraging the results of Theorems \ref{thm1} and \ref{thm2}. 
Hotelling's $T$-squared statistic can be used to conduct a global test for 
$H_0: \truth = \bmu$ versus $H_1:\truth \neq \bmu$, where $\bmu \in \mbR^p$ is the value of $\nat$ we want to test.
%\citep[Chapter 5,][]{multivariate_test}.
We will mostly be interested in the case when $\bmu = \bm{0}_p$,
i.e.,
the zero vector of dimension $p$. 
If the global test is rejected, 
or is not of interest,  
we can perform model selection by leveraging the multivariate normal approximation 
to obtain univariate normal approximation results for the components of $\mle$
and proceed to test each component: $ H_{i,0} : \theta^\star_i=\mu_i$ 
versus $H_{i,1}: \theta^\star_i\neq\mu_i$,
for $i = 1, \ldots p$ and $\mu_i \in \mbR$. 
In general, 
$\mu_i = 0$ will allow us to test whether the estimated effect $\widehat\theta_i$ is present in the model
(i.e., whether $\theta^\star_i \neq 0$).  
One challenge in this approach lies in the fact that the model selection procedure 
is sensitive to multiple testing error. 

To ensure a more reliable procedure for identifying cross-layer dependence effects in multilayer networks while mitigating the risk of spurious discoveries, we elaborate a model selection algorithm that employs suitable multiple testing adjustments to control the false discovery rate. 
We provide simulation examples of four different univariate testing procedures including Bonferroni, Benjamini-Hochberg, 
Hochberg, 
and Holm procedures in Section \ref{sec:sim_norm}. 
In simulation studies, 
all four univariate testing procedures exhibit strong statistical power 
for detecting non-zero parameters,
while controlling the false discovery rate at a preset family-wise significance level. 



