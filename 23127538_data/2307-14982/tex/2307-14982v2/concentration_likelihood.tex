
\begin{lemma}
\label{lem:concentration_likelihood}
Consider a multilayer network model following the form of equation \eqref{general_model} and is 
defined on a set of $N \geq 3$ nodes and $K \geq 1$ layers. 
Define 
$\gradL \coloneqq - \nabla_{\nat} \, \ell(\nat; \bx, \by)$,  
where $\ell(\nat; \bx, \by)$ is the log-likelihood function. 
Then,
for all $t > 0$ and $\nat \in \mbR^p$, the probability
\beno
\mbP\left(\norm{\GradL - \mbE \, \GradL}_2 \geq t \right) 
\ee
is bounded above by
\beno
 \exp\left( -\dfrac{t^2}{36 \, \widetilde{\lambda}_{\max}^{\star} \, (\mbE \, \norm{\bY}_1 + [D_{g}]^{+}) \,  + 2 \, \sqrt{p} \, t} + \log \, p \right) + \dfrac{1}{\mbE \norm{\bY}_1}.
\ee
\end{lemma}

\llproof \ref{lem:concentration_likelihood}. 
By Proposition \ref{prop:inference},
\beno 
\ell(\nat;\bx,\by)
\= \log \, \mbP_{\nat}(\bX = \bx \,|\, \bY = \by)
+ \log \, g(\by).  
\ee
Thus, 
\be
\label{eq:887}
-\gradL
\= \nabla_{\nat} \, \log \, \mbP_{\nat}(\bX = \bx \,|\, \bY = \by)
 + \nabla_{\nat} \, \log \, g(\by) \s \\
\= s(\bx) - \mbE_{\nat} \, s(\bX),  
\ee
as $g(\by) = \mbP_{\nat}(\bY = \by)$ is assumed to not be a function of $\nat$. 
The last equation in \eqref{eq:887} follows from 
Lemma \ref{lem:s_hetero},
which showed that 
$\mbP_{\nat}(\bX = \bx \,|\, \bY = \by)$ is a minimal exponential family
with the natural parameter vector $\nat \in \mbR^p$ and the sufficient statistic vector $\bs(\bx)$ defined in Lemma \ref{lem:s_hetero},
inserting the familiar form of the score equation of an exponential family with respect to the natural parameter vector 
\citep[e.g., Proposition 3.10, p. 32,][]{Su19}.
Thus, 
\beno
- (\GradL   - \mbE \, \GradL ) 
\= \bs(\bX) - \mbE_{\nat} \, \bs(\bX) 
- \mbE \left[ \bs(\bX) - \mbE_{\nat} \, \bs(\bX)\right] \s \\
\= \bs(\bX) - \mbE \, \bs(\bX). 
\ee
Let $t > 0$ and $\nat \in \mbR^p$ be arbitrary and fixed 
and define  
$\mD_{2}(\nat,t)$ to be the event that 
$\norm{\GradL - \mbE \, \GradL}_{2}  \geq t$, i.e., 
\beno
\mD_{2}(\nat,t) \= \left\{ \bx \in \mbX \,:\, \norm{\bs(\bx)-\mbE  \, \bs(\bX)}_{2} \geq t \right\}.
\ee
Let $\epsilon > 0$ and define  
$\mE(\epsilon)$ to be the event that $|\norm{\bY}_1 - \mbE \norm{\bY}_1| \le \epsilon$,
i.e.,
\beno
\mE(\epsilon)
\= \left\{ \by \in \mbY \,:\, \left|\norm{\by}_1 - \mbE \norm{\bY}_1\right| \leq \epsilon \right\}. 
\ee 
We assume that $\epsilon > 0$ is chosen so that $\mE(\epsilon)$ is not empty, 
which implies $\mbP(\mE(\epsilon)) > 0$ 
as $g(\by)$ is assumed to be strictly positive on $\mbY$.  
By the law of total probability, 
\be
\label{divide and conquer}
\mbP\left(\mD_{2}(\nat,t) \right) 
&=& \mbP\left( \mD_{2}(\nat, t) \,|\, \mE(\epsilon) \right)\,
\mbP\left(\mE(\epsilon) \right) + 
\mbP\left( \mD_{2}(\nat,t) \,|\, \mE(\epsilon)^c \right) \, \mbP\left( \mE(\epsilon)^c \right) \s \\
&\leq& \mbP\left( \mD_{2}(\nat,t) \,|\, \mE(\epsilon) \right) + \mbP\left( \mE(\epsilon)^c \right).
\ee
Note that we have not necessarily guaranteed that $\mbP\left( \mE(\epsilon)^c \right) > 0$.  
However, 
if $\mbP\left( \mE(\epsilon)^c \right) = 0$ the non-conditional form of the law of total probability would yield the bound   
\beno
\mbP\left(\mD_{2}(\nat,t) \right)
&\leq&  \mbP\left( \mD_{2}(\nat,t) \,|\, \mE(\epsilon) \right),
\ee
which is strictly sharper than the bound we give in \eqref{divide and conquer}. 
We will use a divide-and-conquer strategy to bound each probability in \eqref{divide and conquer} in turn. 

To bound the first term in \eqref{divide and conquer}, let $\mU \coloneqq \{\bu \in \mbR^p \, : \, \norm{\bu}_2 \leq 1\}$ be a closed unit ball in $\mbR^p$. Define an $\epsilon$-net $\mV_\epsilon$ of $\mU \subset \mbR^p$. By Corollary 4.2.13 of \citep{Vershynin18}, there exists an $\epsilon$-net $\mV_\epsilon \subset \mU$ such that its cardinality satisfies $\log\, |\mV_\epsilon| \leq p \, \log \, (2\,\epsilon^{-1} + 1)$. Taking $\epsilon = 1/2$, for each $\bu \in \mU$, there exists a $\bv \in \mV_{1/2}$ such that $\norm{\bu - \bv}_2 \leq 1/2$, and by the Cauchy-Schwarz inequality, 
\be
\label{inner product of score eq}
\langle \, \bu \, , \, \nabla_{\nat} \, \ell(\nat;\bx,\by) \, \rangle \= \langle \, \bv \, ,\, \nabla_{\nat} \, \ell(\nat;\bx,\by) \,\rangle \, + \, \langle \, \bu - \bv \, ,\, \nabla_{\nat} \, \ell(\nat;\bx,\by)\,\rangle \s \\
& \leq & \langle \, \bu \, , \, \nabla_{\nat} \, \ell(\nat;\bx,\by) \, \rangle \, + \, \norm{\bu - \bv}_2 \, \norm{\nabla_{\nat} \, \ell(\nat;\bx,\by)}_2 \s \\
& \leq & \langle \, \bu \, , \, \nabla_{\nat} \, \ell(\nat;\bx,\by) \, \rangle \, + \, \dfrac{1}{2} \, \norm{\nabla_{\nat} \, \ell(\nat;\bx,\by) }_2.
\ee
If $\norm{\nabla_{\nat} \, \ell(\nat;\bx,\by) }_2 \neq 0$, we can choose 
\beno
u_i \= \dfrac{\nabla_{\nat} \, \ell(\nat;\bx,\by)_i}{\norm{\nabla_{\nat} \, \ell(\nat;\bx,\by) }_2},
\ee
so that $\norm{\bu}_2 \leq 1$ and $\bu \in \mU$. Next, re-write 
\beno
\langle \, \bu \, , \, \nabla_{\nat} \, \ell(\nat;\bx,\by) \, \rangle \= \dfrac{1}{\norm{\langle \, \bu \, , \, \nabla_{\nat} \, \ell(\nat;\bx,\by) \, \rangle }_2} \, \dsum_{i=1}^p \, (\nabla_{\nat} \, \ell(\nat;\bx,\by)_i)^2 \s \\
\= \norm{\nabla_{\nat} \, \ell(\nat;\bx,\by)}_2,
\ee
and together with \eqref{inner product of score eq}, we have
\be
\label{score ineq}
\norm{\nabla_{\nat} \, \ell(\nat;\bx,\by)}_2 \, \leq \, 2 \, \max\limits_{\bv \in \mV_{1/2}} \, \langle \, \bv \, , \, \nabla_{\nat} \, \ell(\nat;\bx,\by) \, \rangle. 
\ee
If $\norm{\nabla_{\nat} \, \ell(\nat;\bx,\by) }_2 = 0$, the inequality \eqref{score ineq} holds trivially. As a result of \eqref{score ineq}, for any $t > 0$,
\beno
\mbP\left(\mD_{2}(\nat,t) \,|\, \mE(\epsilon) \right) & \leq & \mbP \left(  2 \, \max\limits_{\bv \in \mV_{1/2}}  \, \langle \, \bv \, , \, \nabla_{\nat} \, \ell(\nat;\bx,\by) \, \rangle \, \ge \, t \right) \s \\
& \leq & \dsum_{\bv \in \mV_{1/2}} \, \mbP \left( \,\langle \, \bv \, , \, \nabla_{\nat} \, \ell(\nat;\bx,\by) \, \rangle \, \ge \,\dfrac{t}{2} \,  \right) \s \\
& \leq & \exp \, \left( p\, \log \, 5\right) \max\limits_{\bv \in \mV_{1/2}} \, \mbP \left( \,\langle \, \bv \, , \, \nabla_{\nat} \, \ell(\nat;\bx,\by) \, \rangle \, \ge \,\dfrac{t}{2} \,  \right). 
\ee
The last inequality is true because $\log\, |\mV_{1/2}| \leq p \, \log \, 5$. Note that
\be
\label{sum_over_dim}
\langle \, \bv \, , \, \nabla_{\nat} \, \ell(\nat;\bx,\by) \, \rangle  \= \dsum_{l=1}^p \, v_l \, [\nabla_{\nat} \, \ell(\nat;\bx,\by)]_l \s \\
\= \dsum_{l=1}^p \, v_l \, [s_l(\bx) - \mbE \, s_l(\bX)].
\ee
The form of \eqref{general_model} implies, 
through factorization principles,
that the dyad-based vectors $\bX_{i,j}$ ($\{i,j\} \subset \mN$) are conditionally independent given $\bY$ 
\citep[e.g.,][p. 11--13]{graphical_model_handbook}. 
Hence,
using Lemma \ref{lem:s_hetero},  
the components of the sufficient statistic vector decompose into the sum  
\beno
s_l(\bX) 
\= \dsum_{\{i,j\} \subset \mN} \, s_{l,i,j}(\bX_{i,j}),
&& l \in \{1, \ldots, p\},
\ee
so that the components of $\bs(\bX)$ are sums of bounded 
conditionally independent random variables given $\bY$.
As a result, equation \eqref{sum_over_dim} can be further decomposed into sums of independent random variables:
\beno
\langle \, \bv \, , \, \nabla_{\nat} \, \ell(\nat;\bx,\by) \, \rangle  \= \dsum_{\{i,j\} \subset \mN} \, \dsum_{l=1}^p \, v_l \, [s_{l,i,j}(\bx_{i,j}) \, - \, \mbE \, s_{l,i,j}(\bX_{i,j})].
\ee
Using the forms for $ s_{l,i,j}(\bX_{i,j})$ given in Lemma \ref{lem:s_hetero},
we have $0 \le s_{l,i,j}(\bX_{i,j}) \le Y_{i,j}$ $\mbP$-almost surely,
because $s_{l,i,j}(\bX_{i,j}) \in \{0, 1\}$ and $s_{l,i,j}(\bX_{i,j}) = 0$ if $Y_{i,j} = 0$ $\mbP$-almost surely. Then for each $\{i,j\} \subset \mN$, we have
\beno
\mbE \, \dsum_{l=1}^p \, v_l \, [s_{l,i,j}(\bx_{i,j}) \, - \, \mbE \, s_{l,i,j}(\bX_{i,j})] \= 0,
\ee
and by the Cauchy-Schwarz inequality, we obtain
\beno
\left|\,\dsum_{l=1}^p \, v_l \, [s_{l,i,j}(\bx_{i,j}) \, - \, \mbE \, s_{l,i,j}(\bX_{i,j})] \, \right| &\leq & \scalebox{0.97}{$\norm{\bv}_2 \, \sqrt{p} \, \norm{\bs_{l,i,j}(\bx_{i,j}) - \mbE\,\bs_{l,i,j}(\bX_{i,j})}_{\infty}$} \s \\
& \leq & \dfrac{3}{2} \, \sqrt{p}.
\ee
The last inequality follows from
\beno
\norm{\bv}_2 &\leq & \norm{\bu}_2 \, + \, \norm{\bu - \bv}_2 &\leq &1 \, + \, \dfrac{1}{2} &\leq & \dfrac{3}{2}.
\ee
The inequality is true because the construction of the $\epsilon$-net $\mV_{1/2} \subset \mU$ with $\epsilon=1/2$ ensures that such a $\bu \in \mU$ exists. We next bound the variance by 
\beno
\var \, \dsum_{\{i,j\}\subset \mN} \, \dsum_{l=1}^p \, v_l \, [s_{l,i,j}(\bx_{i,j}) \, - \, \mbE \, s_{l,i,j}(\bX_{i,j})] \s \\
= \dsum_{\{i,j\}\subset \mN} \, \dsum_{m=1}^p \, \dsum_{n=1}^p \, \cov \, (v_m \, s_{m,i,j} (\bX_{i,j}) \, ,\, v_n \, s_{n,i,j(\bX_{i,j})}) \s \\ 
= \dsum_{\{i,j\}\subset \mN}\, \dsum_{m=1}^p \, \dsum_{n=1}^p \, v_m \, v_n \, \cov(s_{m,i,j} (\bX_{i,j})\, s_{n,i,j} (\bX_{i,j})) \s \\
= \langle \, \bv \, , \, \norm{\by}_1 \, \mcI(\truth) \, \bv \,  \rangle \s \\
\leq  \norm{\bv}_2^2 \, \norm{\by}_1 \, \widetilde{\lambda}_{\max}^{\star} \s \\
 \leq  \dfrac{9}{4} \, \norm{\by}_1 \, \widetilde{\lambda}_{\max}^{\star},
\ee
where $\widetilde{\lambda}_{\max}^{\star}$ is the largest eigenvalue of the Fisher information of individual activated dyad defined in Lemma \ref{lem:min-eig} evaluated at the data-generating parameter $\truth$.
We then apply the one-sided Bernstein's inequality to obtain the upper bound for the conditional probability of $\mD_{2}(\nat,t)$ as follows: \cite[e.g.,][Theorem 2.8.4]{Vershynin18}
\be
\label{eq:hoef_lik}
\scalebox{0.9}{$\mbP\left( \mD_{2}(\nat,t) \given \bY = \by \right)$}
&\leq& \scalebox{0.9}{$\exp \, \left( p\, \log \, 5\right) \max\limits_{\bv \in \mV_{1/2}} \, \mbP \left( \,\langle \, \bv \, , \, \nabla_{\nat} \, \ell(\nat;\bx,\by) \, \rangle \, \ge \,\dfrac{t}{2} \,  \right)$} \s \\
& \leq & \scalebox{0.9}{$\exp\left(\dfrac{- \dfrac{(t/2)^2}{2}}{\dfrac{9}{4} \, \norm{\by}_1 \,\widetilde{\lambda}_{\max}^{\star} \, + \, \dfrac{1}{3}\, \dfrac{3}{2} \, \sqrt{p} \, \dfrac{t}{2} } \, + \, p \, \log \, 5 \right)$} \s \\
\= \scalebox{0.9}{$\exp\left(\dfrac{- t^2}{18 \, \norm{\by}_1 \,\widetilde{\lambda}_{\max}^{\star} \, + \, 2\, \sqrt{p} \, t } \, + \, p \, \log \, 5 \right)$}.
\ee
Using the law of total probability, 
we bound $\mbP\left( \mD_{2}(\nat, t) \given \mE(\epsilon) \right)$ as follows: 
\be
\label{divide_conquer}
\scalebox{0.9}{$\mbP\left( \mD_{2}(\nat, t) \,|\, \mE(\epsilon) \right)$}
&=& \scalebox{0.9}{$\dsum_{\by \in \mbY} \, \mbP\left( \mD_{2}(\nat,t) \cap [\bY = \by] \,|\, \mE(\epsilon) \right)$}   \s \\
\= \scalebox{0.9}{$\dsum_{\by \in \mE(\epsilon)} \, \mbP\left( \mD_{2}(\nat,t) \cap [\bY = \by] \,|\, \mE(\epsilon) \right)$}   \s \\
\= \scalebox{0.9}{$\dsum_{\by \in \mE(\epsilon)} \, \mbP\left( \mD_{2}(\nat,t) \,|\, [\bY = \by] \cap \mE(\epsilon)\right) \, 
\mbP(\bY = \by \,|\, \mE(\epsilon))$} \s \\ 
\= \scalebox{0.9}{$\dsum_{\by \in \mE(\epsilon)} \, \mbP(\mD_{2}(\nat,t) \,|\, \bY = \by) \, 
\dfrac{\mbP(\bY = \by)}{\mbP(\mE(\epsilon))}$},
\ee
noting that $[\bY = \by] \cap \mE(\epsilon) = [\bY = \by]$ whenever $\by \in \mE(\epsilon)$
and in the case when $\by \not\in \mE(\epsilon)$,
the intersection is empty,
implying 
\beno
\mbP(\bY = \by \,|\, \mE(\epsilon))
\= \dfrac{\mbP([\bY = \by] \cap \mE(\epsilon))}{\mbP(\mE(\epsilon))}
\= \begin{cases} 
\dfrac{\mbP(\bY = \by)}{\mbP(\mE(\epsilon))} & \by \in \mE(\epsilon) \\ 
0 & \by \not\in \mE(\epsilon)
\end{cases}. 
\ee
We now bound \eqref{divide_conquer} using the bound in \eqref{eq:hoef_lik}:  
\beno
& \dsum_{\by \in \mE(\epsilon)} \, \mbP(\mD_{2}(\nat,t) \,|\, \bY = \by) \,
\dfrac{\mbP(\bY = \by)}{\mbP(\mE(\epsilon))}  
\s \\ 
& \leq \,  \dsum_{\by \in \mE(\epsilon)} \, \exp\left(\dfrac{- t^2}{18 \, \norm{\by}_1 \,\widetilde{\lambda}_{\max}^{\star} \, + \, 2\, \sqrt{p} \, t } \, + \, p \, \log \, 5 \right) \, 
\dfrac{\mbP(\bY = \by)}{\mbP(\mE(\epsilon))} \s\\
&\leq  \, \exp\left(\dfrac{- t^2}{18 \, (\mbE\,\norm{\bY}_1 \, + \, \epsilon )\,\widetilde{\lambda}_{\max}^{\star} \, + \, 2\, \sqrt{p} \, t }  \, + \, p \, \log \, 5\right)  \, 
\dsum_{\by \in \mE(\epsilon)} \, \dfrac{\mbP(\bY = \by)}{\mbP(\mE(\epsilon))} \s\\ 
& = \exp\left(\dfrac{- t^2}{18 \, (\mbE\,\norm{\bY}_1 \, + \, \epsilon )\,\widetilde{\lambda}_{\max}^{\star} \, + \, 2\, \sqrt{p} \, t }  \, + \, p \, \log \, 5\right),
\ee
showing 
\beno
\mbP\left( \mD_{2}(\nat, t) \,|\, \mE(\epsilon) \right)
&\leq& \exp\left(\dfrac{- t^2}{18 \, (\mbE\,\norm{\bY}_1 \, + \, \epsilon )\,\widetilde{\lambda}_{\max}^{\star} \, + \, 2\, \sqrt{p} \, t }  \, + \, p \, \log \, 5\right). 
\ee
The replacement of $\norm{\by}_1$ by $\mbE\norm{\bY}_1 + \epsilon$ follows because 
$\norm{\by}_1 \le \mbE\norm{\bY}_1 + \epsilon$ for $\by \in \mE(\epsilon)$, 
resulting in the upper bound above.  
We bound the second term in the inequality \eqref{divide and conquer} 
using Chebyshev's inequality:
\beno
\mbP(\mE(\epsilon)^c) 
\= \mbP(\left| \norm{\bY}_1 - \mbE \, \norm{\bY}_1 \right| > \epsilon) \s \\
&\leq& \mbP(\left| \norm{\bY}_1 - \mbE \, \norm{\bY}_1 \right| \geq \epsilon) \s \\
&\leq&\dfrac{\var(\norm{\bY}_1)}{\epsilon^2}.
\ee
We bound the variance $\var(\norm{\bY}_1)$ as follows:
\beno
\var(\norm{\bY}_1)
\= \dsum_{\{i,j\} \subset \mN} \, \var \, Y_{i,j} 
+ 2 \, \dsum_{\{i,j\} \prec \{v,w\} \subset \mN} \, \cov(Y_{i,j}, \, Y_{v,w})\s\\
&\leq& \mbE \, \norm{\bY}_1 + 2 \, \dsum_{\{i,j\} \prec \{v,w\} \subset \mN} \, \cov(Y_{i,j}, \, Y_{v,w}),  
\ee
noting $Y_{i,j} \in \{0,1\}$ so that 
$\var \, Y_{i,j} = \mbP(Y_{i,j} = 1) \, \mbP(Y_{i,j} = 0) \leq \mbE \, Y_{i,j}$. 
Hence, 
\be
\label{ineq:var}
\mbP(\mE(\epsilon)^c)  
&\leq& \dfrac{\mbE \, \norm{\bY}_1 + 2 \, \sum_{\{i,j\} \prec \{v,w\} \subset \mN} \, \cov(Y_{i,j}, \, Y_{v,w})}{\epsilon^2}  \s \\
\= \dfrac{\mbE \, \norm{\bY}_1 + 2 \, \left[ D_{g} \right]^{+}}{\epsilon^2}.  
\ee 
Taking $\epsilon = \mbE \, \norm{\bY}_1 + 2 \, \left[ D_{g} \right]^{+} > 0$
shows that 
$\mbP(\mE(\epsilon)^c) \leq (\mbE \, \norm{\bY}_1)^{-1}$ and 
\beno
\mbP\left( \mD_{2}(\nat, t) \,|\, \mE(\epsilon) \right)
&\leq& \exp\left(\dfrac{-t^2}{36\,\widetilde{\lambda}_{\max}^{\star} \, (\mbE \, \norm{\bY}_1 + [D_{g}]^{+}) \, + \, 2\, \sqrt{p} \, t } \, + \, p \, \log \, 5 \right).
\ee
Combining all results shows that 
\beno
\mbP\left(\norm{\GradL - \mbE \, \GradL}_{2} \geq t \right)
\ee
is bounded above by 
\beno
\exp\left(\dfrac{-t^2}{36\,\widetilde{\lambda}_{\max}^{\star} \, (\mbE \, \norm{\bY}_1 + [D_{g}]^{+}) \, + \, 2\, \sqrt{p} \, t } \, + \, p \, \log \, 5 \right) \, + \, \dfrac{1}{\mbE \norm{\bY}_1}. 
\ee
As a final matter, 
note that this choice of $\epsilon > 0$ ensures $\mE(\epsilon)$ contains all $\by \in \mbY$ 
with $\norm{\by}_1 \in [0, \, 2 (\mbE \, \norm{\bY} + [D_g]^{+})]$
as the empty graph is an element of $\mbY$ with $0$ edges. 

\qed

\s 

\hide{
We next prove a related result for gradients of log-pseudolikelihood functions 
of multilayer networks in Lemma \ref{lem:concentration_pl}.  
The proof of Lemma \ref{lem:concentration_pl} essentially follows the same proof of Lemma \ref{lem:concentration_likelihood},
and as a result we do not repeat key arguments, instead opting to only outline the changes in the proof.  
}

