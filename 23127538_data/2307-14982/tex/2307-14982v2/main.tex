\documentclass{imsart}
%% Packages
\RequirePackage{amsthm,amsmath,amsfonts,amssymb}
\RequirePackage[numbers]{natbib}
%\RequirePackage[authoryear]{natbib}%% uncomment this for author-year citations
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\RequirePackage{graphicx}

%\arxiv{2010.00000}
\startlocaldefs
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Uncomment next line to change            %%
%% the type of equation numbering           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\numberwithin{equation}{section}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Axiom, Claim, Corollary, Hypothesis, %%
%% Lemma, Theorem, Proposition              %%
%% use \theoremstyle{plain}                 %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{axiom}{Axiom}
\newtheorem{claim}[axiom]{Claim}
\newtheorem{theorem}{Theorem}[]
\newtheorem{lemma}[]{Lemma}
\newtheorem{corollary}{Corollary}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Assumption, Definition, Example,     %%
%% Notation, Property, Remark, Fact         %%
%% use \theoremstyle{definition}            %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem*{example}{Example}
\newtheorem*{fact}{Fact}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Case use \theoremstyle{remark}       %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{remark}
\newtheorem{case}{Case}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please put your definitions here:        %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{appendix}
\input{preamble.tex}
\endlocaldefs



% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{enumerate}
% \usepackage{natbib}
% \usepackage{url} % not crucial - just used below for the URL 
% \usepackage{amsfonts}
% \usepackage{tikz}
% \usepackage{graphicx}
% \usepackage{mathtools}
% \usepackage{enumitem}
% \usepackage{bigints}
% \usepackage{mathrsfs}
% \usepackage{fancybox}
% \usepackage{pdfpages}

% \usetikzlibrary{arrows}
% \usetikzlibrary{positioning}
% \usepackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
% \usepackage{appendix}
% \usepackage{etoc}
% \usepackage[english]{babel}
% \newtheorem{theorem}{Theorem}
% \newtheorem{corollary}{Corollary}
% \newtheorem{lemma}{Lemma}
% \newtheorem{assumption}{Assumption}

% \usepackage{titlesec}

% \titleformat*{\section}{\large\bfseries}

% \usepackage[pagewise]{lineno}
% %\linenumbers

% \input{preamble.tex}
% \newcommand{\qed}{$\hfill\blacksquare$}

% %\pdfminorversion=4
% % NOTE: To produce blinded version, replace "0" with "1" below.
% \newcommand{\blind}{1}

% % DON'T change margins - should be 1 inch all around.
% \addtolength{\oddsidemargin}{-.5in}%
% \addtolength{\evensidemargin}{-1in}%
% \addtolength{\textwidth}{1in}%
% \addtolength{\textheight}{1.7in}%
% \addtolength{\topmargin}{-1in}%


% \pdfminorversion=4


\begin{document}
\begin{frontmatter}
\title{Learning cross-layer dependence structure in multilayer networks}
%\title{A sample article title with some additional note\thanksref{t1}}
\runtitle{Learning cross-layer dependence structure in multilayer networks}
%\thankstext{T1}{A sample additional note to the title.}

\begin{aug}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Only one address is permitted per author. %%
%% Only division, organization and e-mail is %%
%% included in the address.                  %%
%% Additional information can be included in %%
%% the Acknowledgments section if necessary. %%
%% ORCID can be inserted by command:         %%
%% \orcid{0000-0000-0000-0000}               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\author[A]{\fnms{Jiaheng}~\snm{Li}\ead[label=e1]{jl20gx@fsu.edu}},
\author[A]{\fnms{Jonathan}~\snm{Stewart}\ead[label=e2]{jrstewart@fsu.edu}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Addresses                                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\address[A]{Department of Statistics,
Florida State University\printead[presep={,\ }]{e1,e2}}
\runauthor{J. Li et al.}
\end{aug}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \if1\blind
% {
%   \title{\bf Learning cross-layer dependence structure in multilayer networks} 
%   \author{
%     Jiaheng Li\\ 
%     Department of Statistics, Florida State University \\\\ 
%     and \\\\
%     Jonathan R. Stewart\footnote{Corresponding author: Department of Statistics, Florida State University,
% 117 N Woodward Ave,
% Tallahassee, FL 32306-4330,
% E-mail:\ jrstewart@fsu.edu.} \\ Department of Statistics, Florida State University}
% %\thanks{Supported by the Test Resource Management Center (TRMC) within the Office of the Secretary of Defense (OSD), contract \#FA807518D0002.}
%   \maketitle
% } \fi

% \if0\blind
% {
%   \bigskip
%   \bigskip
%   \bigskip
%   \begin{center}
%     {\LARGE\bf Learning cross-layer dependence structure in multilayer networks}
% \end{center}
%   \medskip
% } \fi
\begin{abstract}
We propose a novel class of separable multilayer network models to capture cross-layer dependencies in multilayer networks,
enabling the analysis of how interactions in one or more layers may influence interactions in other layers.
Our approach separates the network formation process from the layer formation process,
and is able to extend existing single-layer network models to multilayer network models
that accommodate cross-layer dependence.
We establish non-asymptotic and minimax-optimal error bounds for maximum likelihood estimators
and demonstrate the convergence rate
in scenarios of increasing parameter dimension.
Additionally, we establish non-asymptotic error bounds for multivariate normal approximations and propose a model selection method that controls the false discovery rate. 
Simulation studies and an application to the Lazega lawyers network show that our framework and method perform well in realistic settings.
\end{abstract}

\begin{keyword}
\kwd{Multilayer networks}
\kwd{statistical network analysis}
\kwd{social network analysis}
\kwd{network data}
\kwd{Markov random fields}
\kwd{graphical models}
\end{keyword}

%\spacingset{1.8} % DON'T change the spacing!
%\spacingset{1.25}
\end{frontmatter}


\section{Introduction} 

\input{introduction_ml.tex}

The rest of the paper is organized as follows. 
Section \ref{sec2} introduces our modeling framework and includes illustrative examples.
The consistency and the minimax optimal results are contained in Section \ref{sec3}.
The multivariate normal approximation theory is presented in Section \ref{sec:normal}. 
The results of simulation studies are provided in Section \ref{sec:sim}, 
together with different testing procedures for model selection
which control the false discovery rate.
An application of our developed framework and methodology is given in Section \ref{sec:app},
concluding with a discussion presented in Section \ref{sec:disc}.
%The R package we developed for the simulation analysis in Section \ref{sec:sim} and the application analysis of Lazega's corporate law partnership data in Section \ref{sec:app} is available on GitHub: 
The code and data to reproduce the simulations and analyses can be found in our package online.\footnote{
\url{https://github.com/jiaheng-li/mlyrnetwork}}

\section{Modeling cross-layer dependence in multilayer networks} 
\label{sec2}

A multilayer network can be represented as 
a sequence of $1 \leq K < \infty$ random graphs $\bX^{(1)}, \ldots, \bX^{(K)}$
each defined on a common set of $N \geq 3$ nodes, 
which we take without loss to be the set $\mN = \{1,\ldots,N\}$. 
We call the graphs $\bX^{(1)}, \ldots, \bX^{(K)}$ the {\it layers} of the network,
and represent the multilayer network as the quantity $\bX = (\bX^{(1)}, \ldots, \bX^{(K)})$. 

Connections between pairs of nodes $\{i,j\} \subset \mN$ in each layer $k \in \{1, \ldots, K\}$
are modeled by random variables  
\beno
X_{i,j}^{(k)}
\= \begin{cases}
1 & \mbox{nodes } i \mbox{ and } j \mbox{ are connected in layer } k \\
0 & \mbox{otherwise}
\end{cases}. 
\ee
We refer to all connections of a pair of nodes $\{i,j\} \subset \mN$ across the $K$ layers 
as a {\it  dyad} which we denote by  
$\bX_{i,j} = (X_{i,j}^{(1)}, \ldots, X_{i,j}^{(K)})\in \{0,1\}^{K}$. 
A multilayer network can be represented by a collection of dyads as $\bX = (\bX_{i,j})_{\{i,j\} \subset \mN}$ alternatively.

For notational ease, 
we will consider undirected multilayer networks, 
which imply that the network layers $\bX^{(1)}, \ldots, \bX^{(K)}$ are undirected random graphs; 
extensions to directed multilayer networks or mixed multilayer networks with both directed and undirected 
layers will typically be straightforward, 
involving only notational adaptations in subscripts in most cases. 
We adopt the usual conventions for undirected networks, 
i.e., 
we assume that $X_{i,j}^{(k)} = X_{j,i}^{(k)}$ (all $\{i,j\} \subset \mN$, $1 \leq k \leq K$) 
and $X_{i,i}^{(k)} = 0$ (all $i \in \mN$, $1 \leq k \leq K$). 
The sample space of each layer $\bX^{(k)}$ is therefore 
the product space $\mbX^{(k)} \coloneqq \{0, 1\}^{\binom{N}{2}}$ ($k = 1, \ldots, K$), 
and the sample space $\mbX$ of $\bX$ is the product space of the sample spaces of the individual layers, 
i.e.,
$\mbX \coloneqq \mbX^{(1)} \times \cdots \times \mbX^{(K)}$. 
The sample space of dyad $\{i,j\} \subset \mN$ is the product space $\mbX_{i,j} \coloneqq \{0,1\}^{K}$. 

%A natural starting point for constructing models of cross-layer dependence is to use Markov random field specifications. 
A challenge in the statistical modeling of network data lies in the fact that 
networks have many distinguishing properties, 
including:  
\ben
\item {\bf Sparsity.} Many real-world networks are sparse, 
in the sense that the expected number of edges in the network grows at a rate slower than $\binom{N}{2}$. 
The phenomena of network sparsity manifests in a variety of different applications, 
usually due to constraints, 
such as time or financial constraints, 
which can limit the number of connections any node can maintain at a given point in time
\citep[][]{KrHaMo11,butts:jms:2018}.
\item {\bf Node heterogeneity.} Different actors in a social network will have different properties,
called node covariates,
which can lead to different propensities to form edges. 
A key example is assortative and disassortative mixing patterns in networks 
\citep{McSmCo01,KrHaRaHo07},
as well as differences in structural patterns in the network \citep{Albert02, LiXu12}.  
\item {\bf Edge dependence.} In addition to node-based effects that give rise to 
heterogeneity 
in propensities for nodes to form edges, 
scientific and statistical evidence suggests edges are dependent in many applications 
\citep{HpLs72,Fo80,block2015reciprocity},
and modeling a single system of multiple binary random variables  without replication is a challenging statistical problem
inherent to many statistical network analysis applications. 
\een
Each of the above gives rise to distinct challenges for modeling network data and performing statistical inference
in statistical network analysis applications,  
and it is not straightforward to construct
models that due justice to each of these and more.  
To address these challenges, 
a plethora of statistical models have been proposed to model network data, 
which for single-layer networks have included  
exponential-families of random graph models 
\citep[e.g.,][]{ergm.book,ScKrBu17},
stochastic block models
\citep[e.g.,][]{HoLaLe83},
latent metric space models 
\citep[e.g.,][]{HpRaHm01},
random dot product graphs \citep[e.g.,][]{Athreya2018}, 
exchangeable random graph models \citep[e.g.,][]{CaFo17,CrDe16}, 
and more. 
In this work, 
we build upon the many classes of single-layer network data models by introducing a separable multilayer network modeling framework. This framework enables existing single-layer network models to be extended to the multilayer setting and simultaneously enables learning cross-layer dependence and interactions across different layers in the multilayer network.

\subsection{Separable multilayer network models}  
\label{sec:2.1}
Multilayer networks are subject to the same forces and phenomena as single layer networks, as multiple modes of relation or interaction do not remove constraints or properties of nodes which are fundamental to network data applications. 
The same set of nodes is defined across all layers in a multilayer network, and because all layers share the same set of nodes, the dyadic connections among these nodes fundamentally define the network formation process. By specifying a single-layer network as the foundational structure reference, we can separate the network formation process from the layer formation process. In doing so, the single-layer network serves as the baseline for establishing dyadic relationships that represent the relational structure across all layers of the multilayer network. 
As a result, the network formation process determines which dyads have the potential to form connections, i.e., which pair of nodes may exhibit at least one edge in any of the layers. 
In contrast,
The layer formation process dictates the particular layers in which these  connections appear. 
To learn the effects of cross-layer dependence in multilayer networks, we propose the class of separable multilayer network models, 
which extend the broad literature on single-layer network models into the multilayer realm.
These models can incorporate an arbitrary single-layer network structure as the foundational baseline and ensures that the underlying single-layer network can be recovered from observations of the multilayer network. We illustrate this approach and its advantages through our proposed modeling framework.
\input{general_model}
\input{prop_inference}

%We prove Proposition \ref{prop:inference} in Appendix \ref{sec:prop_proof}. 
Proposition \ref{prop:inference} establishes a few key facts for the inference of cross-layer dependence structures 
in multilayer networks. 
First, 
we are able to observe $\bY$ through $\bX$,
as given any observation $\bx \in \mbX$ of the multilayer network $\bX$,
$\sepmodel(\bY = \by \,|\, \bX = \bx) = 1$ for one, and only one, $\by \in \mbY$. 
In other words, 
through the observation of $\bx$,
we can infer with probability $1$ the corresponding $\by$
due to the form of \eqref{general_model}.  
The significance of this result is that we do not need to treat the basis network $\bY$ as a latent network, 
which would require additional statistical and computational methodology to handle the latent missing network data.  
Second, 
we see that the inference for $\truth$ is unaffected by the choice of $g(\by)$; 
although, the statistical guarantees for estimators of $\truth$ will be indirectly influenced by the choice of $g(\by)$,
a point which we discuss in later sections.
Moreover, 
the above choice for $f(\bx, \nat)$
and the functional form of $\sepmodel(\bX = \bx \,|\, \bY = \by)$ derived in Proposition \ref{prop:inference}
establishes that $\log \, \sepmodel(\bX = \bx \,|\, \bY = \by)$
corresponds to the log-likelihood of a minimal exponential family, 
accessing a broad literature of statistical methodology and theory \citep[e.g.,][]{Su19}.
We note that other specifications for $f(\bx, \nat)$ are possible, 
but that Markov random field specifications provide a powerful class of models for dependent data 
\citep[e.g.,][]{WaJo08},
and in the case of the saturated model with maximal interaction term $H = K$, it
completely specifies all possible probabilities of outcomes $\bx_{i,j} \in \{0, 1\}^K$,
presenting a non-parametric model class for multilayer networks. 





\subsection{Example of a multilayer network with pairwise interactions} 
\input{example_ml2}


\section{Estimation of cross-layer dependence structure} 
\label{sec3}
\input{estimation_setup}

\input{min_eig_lemma}

\s 

In classical settings with independent and identically distributed observations, 
the expected negative Hessian of the log-likelihood function 
is the Fisher information matrix and 
is expected to scale with the number of observations.  
In such cases, 
standard matrix theory indicates that the smallest eigenvalue of this expected negative Hessian matrix 
will scale with the sample size,
provided the smallest eigenvalue of the Fisher information matrix is bounded from below.
Lemma \ref{lem:min-eig} extends this notion by establishing similar scaling behavior concerning the expected number of activated dyads $\mbE \, \norm{\bY}_1$,
proxying as an effective sample size. 
Analogously,  $\mcI(\nat)$ can be seen as the Fisher information of the population distribution governing individual activated dyads in $\bY$, mirroring the role of Fisher information for population distributions in classical independent and identically distributed scenarios.  

Before we present our theoretical guarantees for maximum likelihood estimators in Theorem \ref{thm1}, we define some notations and outline some regularity assumptions for our theorem to follow.  
As we will show in Theorem \ref{thm1}, the choice of $g(\by)$ influences the estimation error through the expected 
number of edges in $\bY$ and through the covariances of edge variables in $\bY$. 
Define
\beno
D_{g}
&\coloneqq& \dsum_{\{i,j\} \prec \{v,w\} \subset \mN} \, \cov(Y_{i,j}, \, Y_{v,w}),
\ee
where $\{i,j\} \prec \{v,w\}$ implies the sum is taken with respect to the lexicographical ordering of pairs of nodes. 
Define $[D_{g}]^{+} \coloneqq \max\{0, \, D_{g}\}$ to be the positive part of $D_{g}$.
Let $\epsilon > 0$ be fixed independent of $N$ and $p$, and denote the $\epsilon$-ball of the data-generating parameter $\truth$ by $\mB_2(\truth, \epsilon) = \{\nat \in \mbR^p : \norm{\truth - \nat}_2 \leq \epsilon\}$. Define 
\beno
\widetilde{\lambda}_{\min}^{\epsilon}
\;\coloneqq\; \inf\limits_{\nat \in \mB_2(\truth, \epsilon)} \, \lambda_{\min}(\mcI(\nat)) 
&&\mbox{and}&&
\widetilde{\lambda}_{\max}^{\star}
\;\coloneqq\; \lambda_{\max}(\mcI(\truth)),  
\ee
where $\lambda_{\min}(\bA)$ and $\lambda_{\max}(\bA)$ are the smallest and the largest eigenvalue of matrix $\bA \in \mbR^{p \times p}$, respectively. 
\begin{assumption}
\label{assump1}
 Assume there exists a $C_0 > 0$ such that $\mbE\,\norm{\bY}_1 \ge 1$ and 
\beno
\dfrac{[D_{g}]^{+}}{\mbE\, \norm{\bY}_1} &\leq& C_0,
\ee
for all network sizes $N$. 
\end{assumption}

\begin{assumption}
\label{assump2}
Assume the parameter dimension $p$ satisfies 
\beno
p 
&\leq& \sqrt{\widetilde{\lambda}_{\max}^{\star} \, \mbE\, \norm{\bY}_1},
\ee
for all network sizes $N$. 
\end{assumption}

\begin{assumption}
\label{assump3}
Assume that 
$\widetilde{\lambda}_{\max}^{\star}$ and $\widetilde{\lambda}_{\min}^{\epsilon}$ satisfy,
as a function of the network size $N$,  
 \beno
    \dfrac{\sqrt{\widetilde{\lambda}_{\max}^{\star}} }{\widetilde{\lambda}_{\min}^{\epsilon}} \= o \;\left(\sqrt{\dfrac{\mbE \norm{\bY}_1}{p}} \right).
 \ee
\end{assumption}

Assumptions \ref{assump1}â€“\ref{assump3} provide a foundation for Theorem \ref{thm1} to establish the consistency result of the maximum likelihood estimator in large network settings. Assumption \ref{assump1} imposes a lower bound on the expected number of activated dyads relative to the covariance as the network size \(N\) grows. Assumption \ref{assump2} restricts the growth rate of \(p\) in relation to the network size and the largest eigenvalue of the Fisher information $\mcI(\truth)$. Finally, Assumption \ref{assump3} sets a constraint on the ratio between $\sqrt{\widetilde{\lambda}_{\max}^{\star}}$ and $\widetilde{\lambda}_{\min}^{\epsilon}$, balancing eigenvalue magnitudes in a way that preserves estimator consistency under increasing network size. 


\input{thm1}



\section{Error of the normal approximation and model selection} 
\label{sec:normal}
In this section, 
we establish the asymptotic multivariate normality of the maximum likelihood estimator (MLE) for the data-generating parameter vector $\truth$ as its dimension grows.
Specifically,
we derive a non-asymptotic bound on the quality of the multivariate normal approximation and exhibit scaling conditions on both the model dimension $p$ 
and the expected number of activated dyads $\mbE \, \norm{\bY}_1$---under which the approximation error vanishes as the network size tends to infinity.  
Based on this result, 
we present a model selection method using multiple hypothesis testing procedures that control the false discovery rate. 
The main result is presented in Theorem \ref{thm2},
the proof of which 
is based on a Taylor expansion of the log-likelihood function  
and through the application of a Lyapunov type bound presented in \citet{Raic19}. 

\input{thm2}
%We prove Theorem \ref{thm2} in Appendix \ref{sec:pf_thm2}.

\subsection{Model selection via univariate testing with FDR control} 

\input{global_test}


\section{Simulation studies}
\label{sec:sim}
Directly simulating maximum likelihood estimators for network data with dependent edges is challenging because the normalizing constants are often computationally intractable. Computing the normalizing constant requires enumerating all $2^{\binom{N}{2}}$ possible edge combinations for each layer to maximize the true likelihood function. Additionally, dependencies among network dyads prevent factorization of the likelihood, which further complicates direct maximization. As a result, direct maximization of likelihood functions is generally infeasible in these cases.
Two predominant methods of approximating the maximum likelihood estimator $\truth$ when the likelihood function is computationally intractable 
have emerged in the literature. 
Monte Carlo maximum likelihood estimation (MCMLE) \citep{GeTh92},
which constructs a simulation-based approximation to the likelihood function 
in order to approximate the maximum likelihood estimator,
is an established method for approximating maximum likelihood estimators 
in the statistical network analysis literature \citep{HuHa06}.
While able to provide accurate estimates of maximum likelihood estimators for complex models 
\citep[e.g.,][]{StScBoMo19,ScKrBu17}, 
a drawback of MCMLE,
and other simulation-based estimation methodology, 
is the computational burden which can scale with both the complexity of the model and the size of the network \citep{BaBrSl11}.  
In settings where the computation of the MCMLE is impractical, 
a computationally efficient alternative is provided via the maximum pseudolikelihood estimator (MPLE) \citep{Bj74},
whose application to social network analysis and to statistical network analysis dates back to \citet{StIk90}. 
Pseudolikelihood-based estimators have the following computational advantages:
\ben
\item Algorithms are generally deterministic and do not require simulation-based approximation schemes,
which aids in reproducibility of results; 
\item Algorithms are generally more scalable, 
relative to alternatives such as MCMLE and other simulation-based approximations, 
and are able to be parallelized to take advantage of larger  multicore computing infrastructures which are becoming increasingly common.  
\een


\hide{
Proposition \ref{prop:inference} establishes that $\bY$ is observable through $\bX$,
i.e., 
\beno
\mbP(Y_{i,j} = y_{i,j} \,|\, \bX = \bx, \bY_{-\{i,j\}} = \by_{-\{i,j\}})
\= 1,
\ee
when $y_{i,j} = \one(\norm{\bx_{i,j}}_1 \,>\, 0)$ and $\bY_{-\{i,j\}}$ is defined to be the 
($\binom{N}{2}$-$1$)-dimensional vector of edge variables in $\bY$ which excludes $Y_{i,j}$. 
As a result, 
if $(\bx, \by)$ is network concordant,
then  
\beno
\log \, \mbP(Y_{i,j} = y_{i,j} \,|\, \bX = \bx, \bY_{-\{i,j\}} = \by_{-\{i,j\}}) \= 0, 
&& \mbox{for all } \{i,j\} \subset \mN.
\ee 

The log-pseudolikelihood of \eqref{general model} can then be written down as 
\be
\label{eq:log-pseudo}
\pl(\nat; \bx, \by) 
\,\coloneqq \dsum_{\{i,j\} \subset \mN}  \dsum_{k=1}^K \, \log  
\sepmodel(X_{i,j}^{(k)} = x_{i,j}^{(k)} \, |\, \bX_{i,j}^{(-k)} = \bx_{i,j}^{(-k)}, \bY = \by), 
\ee
provided $(\bx, \by)$ is network concordant
and by exploiting the conditional independence properties implied by \eqref{general model}. 
We denote the set of maximum pseudolikelihood estimators of the data-generating parameter vector $\truth$ by 
\beno
\Mple &\coloneqq& \left\{ \nat \in \mbR^p \,: \, \pl(\nat; \bx, \by) = \sup\limits_{\nat^{\prime} \in \mbR^p} \, \pl(\nat^{\prime}; \bx, \by) \right\}.
\ee
Individual elements are referenced by $\mple \in \Mple$.
When it exists, 
the maximum pseudolikelihood estimator may or may not be unique.
However,
our theoretical results establish that all elements $\mple \in \Mple$ will all be within the same Euclidean distance to $\truth$.
The assumption that $(\bx, \by)$ is network concordant comes at no cost,
since $\bY$ is predictable through $\bX$,
as discussed above. 
%meaning that given an observation $\bx$ of $\bX$,
%we can find the unique network concordant pair $(\bx, \by)$ with probability one.  
The advantage of \eqref{eq:log-pseudo} is that 
the conditional probabilities 
$\sepmodel(X_{i,j}^{(k)} = x_{i,j}^{(k)} \, |\, \bX_{i,j}^{(-k)} = \bx_{i,j}^{(-k)}, \bY = \by)$ 
of edges in the multilayer network 
are often computationally tractable 
since the conditional distribution is a Bernoulli distribution when $Y_{i,j} = 1$,
and is a degenerate point mass at $0$ when $Y_{i,j} = 0$.

}

In this simulation, 
we consider the maximum pseudolikelihood estimator, denoted by $\mple$.
We conduct simulation studies to investigate the performance of the maximum pseudolikelihood estimator $\mple$ (MPLE),
supplementing the theoretical results established in Sections \ref{sec3} and \ref{sec:normal} for maximum likelihood estimators. 
As will be discussed later in Section \ref{sec:app}, we successfully reproduced the sufficient statistics using the MPLE in the application, suggesting that the MPLE for the multilayer network model solves a score equation similar to that of the MLE. This indicates that the MPLE serves as a close approximation and can be a good proxy for the MLE.
In section \ref{sec:sim_con}, 
we demonstrate the consistency results of Theorem \ref{thm1} 
in settings of different data-generating parameters and increasing model dimensions.
We conduct simulation studies of the multivariate normal approximation established by Theorem \ref{thm2} 
in Section \ref{sec:sim_norm}.  
Lastly, 
we discuss several testing procedures for selecting non-zero effects 
while controlling the false discovery rate (FDR) at a given family-wise significance level $\alpha$. 
 
In all simulation studies, 
we sample concordant multilayer networks $(\bX,\bY)$ from \eqref{general_model} 
with the maximum order of corss-layer interaction $H=2$:
\be
\label{eq:sim_model}
f(\bx, \nat) = \dprod_{\{i,j\} \subset \mN} \, \exp\left( \dsum_{k=1}^K\theta_{k}\,x_{i,j}^{(k)} + \dsum_{\substack{k < l}}^{K}\, \theta_{k,l} \, x_{i,j}^{(k)} \, x_{i,j}^{(l)} \right).
\ee
Unless otherwise specified, the basis network $\bY$ is generated from the Bernoulli random graph model.

% Figure environment removed

% Figure environment removed


\subsection{Consistency} 
\label{sec:sim_con}
The consistency is demonstrated through the decay of the relative $\ell_2$-errors between $\mple$ and the data-generating parameter $\truth$ as the expected number of activated dyads $\mbE\norm{\bY}_1$ increases. 
We generated $M = 250$ multilayer networks with $N=300$ nodes, using $M$ different data-generating parameters. We created these networks for each of ten evenly spaced numbers of activated dyads increasing from $3000$ to $30000$, and for four different numbers of layers increasing from $K=3$ to $6$. The model dimension increases from 6 to 21 as $K$ increases from 3 to 6.
For each number of activated dyads, 
number of layers $K$, 
and replicate, 
we sample a multilayer network $\bX$ from \eqref{general_model} using the specification in \eqref{eq:sim_model}
with the data-generating parameter vector $\truth$ populated by randomly selecting each component from the uniform distribution on $(-1,1)$.
We make the exception that components $\theta_{3}^\star$ and $\theta_{1,3}^\star$ are set to $0$.
In each replicate, 
we compute the maximum pseudolikelihood estimator. 
The results of this simulation study are given in Figure \ref{M_theta_error}, 
which shows the decay of the relative $\ell_2$-errors between $\mple$ and $\truth$ as the number of activated dyads increases in networks with different number of layers. 
The broad selection of data-generating parameter values on networks with increasing number of layers verifies that Theorem \ref{thm1} holds in many practical settings with increasing model dimensions. 





\hide{
The decay rate of the relative $\ell_2$-error as the network size increases can be estimated by the slope of the OLS fitted line in the $\log$-$\log$ plot as shown in Figure \ref{Figure1}(b). The slope of the OLS line is $-1$, indicating a decay rate of order $1/N$, which follows the result in Theorem \ref{thm1}. We report the MPLE $\mple$ averaged from $M = 500$ samples with the selected data-generating parameter $\truth$ at network size 1000 in table \ref{t1}.

\begin{table}[t]
\begin{center}
\caption{\label{t1}Values of the data-generating parameter $\truth$ and mean of MPLE $\mple$ from 500 replications at network size 1000. Standard errors are in the parenthesis.}
\begin{tabular}{| c | c | c | c | c | c | c |} 
\hline
  & $\theta_{1}$&$\theta_{2}$&$\theta_{3}$& $\theta_{1,2}$ & $\theta_{1,3}$ & $\theta_{2,3}$ \\ 
\hline
$\truth$ & $-3$ & $-2$ & $-1$ & $.5$ & $0$ & $0$  \\
\hline
$\mple$ & $-2.999 \, (.02)$ & $-1.998 \, (.02)$ & $-.999 \, (.02)$ & $.499 \, (.02)$ & $-.001 \, (.02)$ & $-.002 \, (.02)$\\
\hline
\end{tabular}
\end{center}
\end{table}

}

% % Figure environment removed



\subsection{Multivariate normality and model selection} 
\label{sec:sim_norm}


\begin{table}[t]
\begin{center}
\caption{\label{fdr} False discovery rates of four procedures for detecting non-zero effects of 6 data-generating parameters 
($\truth_1$, $\truth_2$, $\truth_3$, $\truth_{4}$, $\truth_{5}$, $\truth_{6}$) estimated from 250 multilayer network samples at size 1000 on the dense Bernoulli basis network. All FDRs are smaller than $.05$.} 
\begin{tabular}{ l  r  r  r  r  r  r  } 
\hline
  Procedure & $\truth_1$ & $\truth_2$  & $\truth_3$ & $\truth_4$ & $\truth_5$ & $\truth_6$ \\ 
\hline
  Bonferroni    & .004  & .002 & .001 & .002 & .001 & .005  \\

  Benjamini-Hochberg   & .014 & .014 & .014 & .011 & .017 & .020  \\

 Hochberg &    .012 & .008 & .009 & .008 & .011 & .016 \\

 Holm  &   .010 & .008 & .006 & .008 & .007 &  .013 \\
\hline
\end{tabular}
\end{center}
\end{table}



As stated in Section \ref{sec:normal} and Theorem \ref{thm2}, 
the distribution of the maximum likelihood estimator $\mle$ 
converges in distribution to a multivariate normal distribution asymptotically. 
In order to study the quality of the normal approximation---especially for univariate testing 
which would be used for the false discovery rate control and model selection---we 
randomly select $6$ of the $250$ data-generating parameter vectors $\truth$
used to study the consistency results of Theorem \ref{thm1} 
in the simulation study conducted in Section \ref{sec:sim_con}.
We then generate $250$ replicates of multilayer network samples by each of these $6$ parameter vectors, using specification \eqref{eq:sim_model} on four basis network structures with the number of layers $K=3$:
the Bernoulli random graph model (dense and sparse), 
the stochastic block model, 
and the latent space model. 

The multivariate normality of $\mple$ passed Zhou-Shao's multivariate normal test \citep{Zhou13}, 
with $p$-values provided in the Appendix \ref{subsec:norm_sim} in the supplement to this paper.  
We visualize the marginal normality of individual component in $\mple$ with a dense Bernoulli basis network
in Figure \ref{qqplot_denseBer},
through Q-Q plots of the simulated maximum pseudolikelihood estimators. 
Univariate tests for normality failed to reject the null hypothesis that
each component of $\mple$ is marginally normal at a significance level of $.05$. 
Additional results studying the multivariate normality of $\mple$ on different basis network structures 
are provided in Appendix \ref{subsec:norm_sim} in the supplement to this paper.
 

 

\hide{
We first demonstrate through Q-Q plots in Figure \ref{Figure3} that each component of the MPLE $\mple$ follows a marginal normal distribution. 
% We constructed 95\% confidence ellipses for bivariate components of $\mple$ in figure \ref{Figure4}. The two axes in each sub-plot of figure \ref{Figure4} correspond to two different components of $\mple$. The red ellipse is the 95\% confidence ellipse generated by the mean and covariance matrix of the MPLEs in the plot under the bivariate normal assumption. All 15 bivariate confidence ellipses cover most of estimates points, suggesting probable bivariate normal distributions of $\mple$. 


\begin{table}%[t]
\begin{center}
\caption{\label{ZS-test} $p$-values of the Zhou-Shao's test for multivariate normality of $\mple$ for 6 data-generating parameters ($\truth_1$, $\truth_2$, $\truth_3$, $\truth_4$, $\truth_5$, $\truth_6$) estimated from 250 network samples at size 1000 on four basis network structures. All $p$-values are larger than .05. \s} 
\begin{tabular}{| c | c | c | c | c | c | c | } 
\hline
 Basis network model  & $\truth_1$ & $\truth_2$  & $\truth_3$ & $\truth_4$ & $\truth_5$ & $\truth_6$ \\ 
\hline
  Dense Bernoulli & .138 & .473 & .053 & .699 & .587 & .983  \\
\hline
 Sparse Bernoulli & .554 & .132 & .232 & .634 & .904 & .373  \\
\hline
 SBM & .65 & .891 & .982 & .975 & .871 & .674 \\
\hline
 LSM  & .859 & .831 & .5 & .227 & .613 & .409  \\
\hline
\end{tabular}
\end{center}
\end{table}
}
\hide{
We additionally performed marginal tests for normality for each component. In each case, the marginal test failed to reject the null hypotheses that $\mple_i$ is marginal normal at the same significance level, for each component $i = 1,\ldots, p$, the result of which is consistent with the $Q$-$Q$ plots. 
}


\hide{
% Figure environment removed


\begin{table}[t]
\begin{center}
\caption{\label{t3} Empirical FDR and power at significance level $.05$ for multiple testing of MPLE $\mple$ 
estimated from $500$ networks of size $1000$.  } 
\begin{tabular}{| c | c | c | c |} 
\hline
 Procedure  & Empirical FDR & FDR 95\% one-sided CI  & Empirical power \\ 
\hline
  Bonfferoni & .138 & 0.473 & .038 & .699 & .587 & .983  \\
\hline
  Benjamini-Hochberg's &  .018 & (0, .134) & 1  \\
\hline
 Hochberg's  & .016 & (0, .127) & 1 \\
\hline
 Holm's  & .013 &  (0, .114) & 1  \\
\hline
\end{tabular}
\end{center}
\end{table}
}

We then implement the multiple testing correction procedures of
Bonferroni, Benjamini-Hochberg, Hochberg, and Holm, for the 6 selected data-generating parameter vectors $\truth$ with 250 replicates to detect components that are significantly different from $0$ 
while controlling the false discovery rate (FDR) at a family-wise significance level of $\alpha = .05$---recall 
that $\theta_{1,3}^\star$ and $\theta_{3}^\star$ of $\truth$ are set to $0$ 
in each simulation replicate.  
We estimate the FDR of the four procedures by averaging the false discovery proportions from 250 replicates of each of the 6 randomly selected data-generating parameters $\truth$.
We provide the estimated FDRs for $\truth$ on a dense Bernoulli basis network in Table \ref{fdr}. 
In addition, we show the receiver operating characteristic (ROC) curves for $\mple$ estimating the 6 selected data-generating parameters in each of the subplot of Figure \ref{ROC}, on four basis network structures in Appendix \ref{more fdr} in the supplement to the paper. 
Simulation results suggest that the false discovery rate is controlled below the preset threshold $\alpha$. Different data-generating parameter values affect the trade-off between the sensitivity and the specificity of the model selection. In general, multilayer networks with a larger effective sample size lead to a larger area under the ROC curve which offers a tool to choose appropriate correction procedures and thresholds for model selection in different scenarios. 
Additional results on the false discovery rate with different basis network structures are provided in Appendix \ref{more fdr} in the supplement to the paper. 



\section{Application} 
\label{sec:app}

\input{Lazega}

\section{Discussion} 
\label{sec:disc} 

In this work, 
we introduced a flexible class of statistical models for multilayer networks. 
Key to our approach lies in the integrative nature by which we establish our framework, 
extending arbitrary strictly positive probability distributions for single-layer networks 
to multilayer-network models through a novel separable framework with Markov random field specifications. 
We established the foundations for statistical inference through consistency and multivariate normality results,
the results of which have been demonstrated in simulation studies and in an application.  
The key assumption to our approach lies in the network separability assumption, 
which necessitates network dyads be conditionally independent given the basis network. 
This assumption may or may not be valid in practice,
which would necessitate the development of generalizations of the framework we established in this work 
through the relaxation of the conditional independence assumption.  
Such relaxations would result in more complex dependence structures,
requiring 
new and careful theoretical treatment in order to establish similar statistical foundations of models 
to the ones we have developed here,  
representing potential avenues for future research. 

\section*{Acknowledgements} 

Jonathan R. Stewart was supported by 
NSF award SES-2345043 and 
the Department of Defense Test Resource Management Center under contracts FA8075-18-D-0002 and FA8075-21-F-0074.


\bibliographystyle{agsm}
\bibliography{base} 




\newpage

\begin{appendices}


\input{supplement.tex}
\end{appendices}

\label{last.page}



\end{document}
