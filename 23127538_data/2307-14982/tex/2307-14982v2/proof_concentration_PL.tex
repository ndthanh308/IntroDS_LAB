From \eqref{eq:log-pseudo},
the log-pseudolikelihood function is given by
\beno
\pl(\nat; \bx, \by) = \dsum_{\{i,j\} \subseteq \mN} \, \dsum_{k=1}^{K} \, 
\log \, \sepmodel(X_{i,j}^{(k)} = x_{i,j}^{(k)} \,|\, \bX_{i,j}^{(-k)} = \bx_{i,j}^{(-k)}, \bY = \by).
\ee
By Lemma \ref{lem:exp_pseudo}, 
$\sepmodel(X_{i,j}^{(k)} = x_{i,j}^{(k)} \,|\, \bX_{i,j}^{(-k)} = \bx_{i,j}^{(-k)}, \bY = \by)$ 
is an exponential family with sufficient statistic vector $s : \mbX \mapsto \mbR^p$ 
defined in Lemma \ref{lem:s_hetero} and natural parameter vector $\nat \in \mbR^p$.
Hence,  
\beno 
\nabla_{\nat} \, \pl(\nat;\bx,\by)
\=  \dsum_{\{i,j\} \subseteq \mN} \, \dsum_{k=1}^{K}  
\nabla_{\nat} \log \, \sepmodel(X_{i,j}^{(k)} = x_{i,j}^{(k)} \,|\, \bX_{i,j}^{(-k)} = \bx_{i,j}^{(-k)}, \bY = \by) \s \\
\= \dsum_{\{i,j\} \subseteq \mN} \, \dsum_{k=1}^{K} \, 
\left[ \, \bs(\bx) - \mbE_{\nat}\left[ \bs(\bX) \, | \, \bX_{i,j}^{-(k)}  = \bx_{i,j}^{(-k)}, \bY = \by \right]  \, \right],
\ee
where $\bX_{i,j}^{-(k)}$ denotes the ($K$-$1$)-dimensional 
vector of edge variables of dyad $\{i,j\}$ in $\bX_{i,j}$ which excludes the single edge variable $X_{i,j}^{(k)}$,
and by inserting the familiar form of the score equation of an exponential family 
with respect to the natural parameter vector 
\citep[e.g., Proposition 3.10, p. 32,][]{Su19}. 
Note that $\sepmodel(X_{i,j}^{(k)} = x_{i,j}^{(k)} \,|\, \bX_{i,j}^{(-k)} = \bx_{i,j}^{(-k)}, \bY = \by)$
may not belong to a minimal exponential family. 
This presents no issues as we do not require the conditional probability distributions of individual edge variables
belong to a minimal exponential family. 
Under the assumption that $(\bX, \bY)$ follow \eqref{general model}, 
the vectors $\bX_{i,j}$ ($\{i,j\} \subset \mN$) are conditionally independent given $\bY$ 
(as discussed in the proof of Lemma \ref{lem:concentration_likelihood}). 
Therefore,
the $l^{th}$ component $s_l(\bX)$ decomposes into the sum of conditionally independent Bernoulli random variables:
\beno
s_l(\bX)
\= \dsum_{\{i,j\} \subset \mN} \, s_{l,i,j}(\bX_{i,j}),
&& l \in \{1, \ldots, p\},
\ee
so that the components of $\bs(\bX)$ are sums of bounded conditionally independent random variables given $\bY$. 
Thus,
\beno
\nabla_{\nat} \, \pl(\nat;\bx,\by)
\= \dsum_{\{i,j\} \subseteq \mN} \, \dsum_{k=1}^{K} \,
\left( s_{l,i,j}(\bX_{i,j}) - E_{l,i,j}(\nat, \bx_{i,j}, \by) \right),
\ee
where
\beno
E_{l,i,j}(\nat, \bx_{i,j}, \by)
&\coloneqq& 
\mbE_{\nat}\left[s_{l,i,j}(\bX_{i,j}) \,|\, \bX_{i,j}^{(-k)} = \bx_{i,j}^{(-k)}, \bY = \by \right].
\ee
Using the form of $s_l(\bX)$ and $s_{l,i,j}(\bX_{i,j})$ outlined in Lemma \ref{lem:s_hetero},
we have $0 \le s_{l,i,j}(\bX_{i,j}) \le Y_{i,j}$ $\mbP$-almost surely,
because $s_{l,i,j}(\bX_{i,j}) \in \{0, 1\}$ and $s_{l,i,j}(\bX_{i,j}) = 0$ if $Y_{i,j} = 0$ $\mbP$-almost surely.
This also implies 
$0 \leq E_{l,i,j}(\nat, \bx_{i,j}, \by) \leq Y_{i,j}$ $\mbP$-almost surely. 
Taken together, 
\beno
0 
\,\leq\, \left| \dsum_{k=1}^{K} \,
\left( s_{l,i,j}(\bX_{i,j}) - E_{l,i,j}(\nat, \bx_{i,j}, \by) \right) \right| 
\,\leq\, \dsum_{k=1}^{K} \left| s_{l,i,j}(\bX_{i,j}) - E_{l,i,j}(\nat, \bx_{i,j}, \by) \right|  
\,\leq\, K \, Y_{i,j}, 
\ee
$\mbP$-almost surely. 
From here,
the remainder of the proof follows the proof of Lemma \ref{lem:concentration_likelihood},
with the sole exception using 
the bound $K \, Y_{i,j}$ in the application of Hoeffding's inequality. 
Reiterating the proof of Lemma \ref{lem:concentration_likelihood} with this change will yield  
\beno
\mbP\left(\norm{\GradPL - \mbE \, \GradPL}_{\infty} \geq t \right)
&\leq&
2 \, \exp\left( -\dfrac{t^2}{K^2 \, (\mbE\, \norm{\bY}_1 + [D_{g}]^{+})} + \log p \right) + \dfrac{1}{\mbE \norm{\bY}_1}.  
\ee

\qed 




