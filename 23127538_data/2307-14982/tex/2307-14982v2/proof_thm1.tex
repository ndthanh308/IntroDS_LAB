By Proposition \ref{prop:inference}, 
observing $\bX = \bx$ implies we observe $\bY = \by$,
as for each given $\bx \in \mbX$,
$\bY = \by$ ($\mbP$-a.s.) for one and only one $\by \in \mbY$ given by 
\beno
y_{i,j} 
\= \one\left( \norm{\bx_{i,j}}_1 \,>\, 0 \right),
&& \{i,j\} \subset \mN. 
\ee 
Denote the gradient of $-\ell(\nat; \bx, \by)$ by 
\beno
\gradL 
&\coloneqq& -\nabla_{\nat} \, \ell(\nat; \bx, \by)
\ee
and the expected Hessian matrix of the negative log-likelihood by 
\beno
\bH(\nat) 
&\coloneqq& -\mbE \, \nabla_{\nat}^2 \, \ell(\nat; \bX, \bY). 
\ee 
Theorem 6.3.4 of \citet{OrRh20} states that if 
\beno 
\label{eq:ex_cond} 
(\nat - \truth)^{\top} \, \gradL
&\geq& 0 
\ee
for all $\nat \in \partial \, \mB_2(\truth, \epsilon)$, 
where $\partial \, \mB_2(\truth, \epsilon)$ is the boundary of the set 
\beno
\mB_2(\truth, \epsilon) 
\= \{ \nat \in \mbR^p \,:\, \norm{\nat - \truth}_2 < \epsilon\},
\ee 
then $\gradL$ 
has a root in $\mB_2(\truth, \epsilon) \cup \partial \mB_2(\truth, \epsilon)$,
i.e.,
$\mle$ exists and satisfies $\norm{\mle - \truth}_2 \leq \epsilon$.
Note that a root of $\gradL$ is also a root of $-\gradL$;
in what follows, 
we consider finding a maximizer of $\ell(\nat;\bx,\by)$ by finding a minimizer of $-\ell(\nat;\bx,\by)$. 
The classification of roots as maximizers/minimizers 
is justified from the fact that that $\ell(\nat; \bx, \by)$ is concave in $\nat$,
a fact which follows from Proposition \ref{prop:inference},
as $g(\by)$ is constant in $\nat$ and $\log \, \mbP_{\nat}(\bX = \bx \,|\, \bY = \by)$ 
is the log-likelihood of a minimal, full, and regular exponential family with natural parameter vector $\nat$
and thus is strictly concave in $\nat$ \citep[Proposition 3.10, p. 32,][]{Su19}. 
By the multivariate mean-value theorem \citep[][Theorem 5]{FuMa91}, 
\beno
(\nat - \truth)^{\top}  \mbE \, \GradL 
\= (\nat - \truth)^{\top}  \mbE \nabla_{\truth}(\bX, \bY) 
+ (\nat - \truth)^{\top}  \bH(\dot\nat)  (\nat - \truth) \s \\ 
\=(\nat - \truth)^{\top}  \bH(\dot\nat)  (\nat - \truth), 
\ee
where $\dot\nat = t \, \nat + (1 - t) \, \truth$ (some $t \in [0, 1]$) 
and by invoking Lemma 2 of \citet{StSc21},
which shows that both the expected log-likelihood and log-pseudolikelihood 
of a minimal exponential family is uniquely maximized at the data-generating parameter vector $\truth$, 
implying $\mbE \, \nabla_{\truth}(\bX, \bY) = 0$. 
Let $\gamma \in (0, \epsilon)$
and arbitrarily take $\nat \in \partial \mB_2(\truth, \gamma)$. 
Then 
\beno
(\nat - \truth)^{\top} \bH(\dot\nat) (\nat - \truth)
\= \dfrac{(\nat - \truth)^{\top}  \bH(\dot\nat)  (\nat - \truth)}{(\nat - \truth)^{\top} (\nat- \truth)}  
\norm{\nat - \truth}_2^2\s \\
&\geq& \gamma^2 \, \lambda_{\min}(\bH(\dot\nat)),  
\ee
since $\norm{\nat - \truth}_2 = \gamma$ as $\nat \in \partial \mB_2(\truth, \gamma)$ 
and because the Rayleigh quotient of a matrix is bounded below by the smallest eigenvalue of that matrix so that
\beno
\dfrac{(\nat - \truth)^{\top}  \bH(\dot\nat)  (\nat - \truth)}{(\nat - \truth)^{\top} (\nat- \truth)}
\;\geq\; \lambda_{\min}(\bH(\dot\nat))
\;\geq\; \inf\limits_{\nat \in \mB_2(\truth, \epsilon)} \, \lambda_{\min}(\bH(\nat)),
\ee
where $\lambda_{\min}(\bH(\dot\nat))$ is the smallest eigenvalue of $\bH(\dot\nat)$,
noting that 
\beno
\norm{\dot\nat - \truth}_2 
\= \norm{t \, \nat + (1 - t) \, \truth - \truth}_2
\= t \, \norm{\nat - \truth}_2 
&\leq& \epsilon,  
\ee
since $t \in [0, 1]$. 
Lemma \ref{lem:min-eig} showed that 
\beno
\lambda_{\min}(\bH(\nat)) 
\= \lambda_{\min}(\mcI(\nat)) \; \mbE \, \norm{\bY}_1, 
\ee
which in turn implies 
\beno
\inf\limits_{\nat \in \mB_2(\truth, \epsilon)} \, \lambda_{\min}(\bH(\nat))
\= \widetilde{\lambda}_{\min}^{\epsilon} \; \mbE \, \norm{\bY}_1,
\ee
where 
\beno
\widetilde{\lambda}_{\min}^{\epsilon}
\;\coloneqq\; \inf\limits_{\nat \in \mB_2(\truth, \epsilon)} \, \lambda_{\min}(\mcI(\nat)) ,
\ee
with $\mcI(\nat)$ defined in Lemma \ref{lem:min-eig}. 
Hence,
for $\nat \in \partial \mB_2(\truth, \gamma)$ ($\gamma \in (0, \epsilon)$),  
\be
(\nat - \truth)^{\top}  \mbE \, \GradL
&\geq& \gamma^2 \;\widetilde{\lambda}_{\min}^{\epsilon} \; \mbE \, \norm{\bY}_1 \; \ge \; 0. 
\ee
We next turn to showing  
\beno
\mbP\left( \inf\limits_{\nat \in \mB_2(\truth,\gamma)} \, 
(\nat - \truth)^{\top} \, \GradL \,\geq\, 0 \right) 
&\geq& 1 - \, \exp \, (-2\,p) \, - \, (\mbE \norm{\bY}_1)^{-1},
\ee
by showing that the event  
\beno
\sup\limits_{\nat \in \mB_2(\nat^\star, \gamma)} \, 
|(\nat - \truth)^{\top} \, ( \mbE \, \GradL - \GradL)|
\;<\; \gamma^2 \; \Lam
\ee
occurs with probability at least $1 - \, \exp \, (-2\,p) \, - \, (\mbE \norm{\bY}_1)^{-1}$. This will 
in turn imply that the event that   
$\norm{\mle - \truth}_2 \; \leq \; \gamma$ will
happen with probability at least $1 - \, \exp \, (-2\,p) \, - \, (\mbE \norm{\bY}_1)^{-1}$. 
Applying the Cauchy-Schwarz inequality and utilizing standard vector norm inequalities, for $\nat \in \partial \mB_2(\truth, \gamma)$, we have
\beno
|(\nat - \truth)^{\top} \, ( \mbE \, \GradL - \GradL)| \s \\
\leq \norm{\nat - \truth}_2 \, \norm{\GradL  - \mbE \, \GradL}_2 \s \\ 
=  \gamma \; \norm{\GradL - \mbE \, \GradL}_{2}.
\ee
Therefore, it suffices to demonstrate,
for all $\nat \in \partial \mB_2(\truth, \gamma)$,
\beno
\mbP\left(\norm{\GradL - \mbE \, \GradL}_{2} \,<\, \gamma \, \Lam \right)
\ee
is bounded below by
\beno
1 - \, \exp \, (-2\,p) \, - \, (\mbE \norm{\bY}_1)^{-1}. 
\ee
For ease of presentation,
we define $\mD_{N,\gamma,p}$ to be the event 
\beno
\norm{\GradL - \mbE \, \GradL}_{2} 
&\geq& \gamma \, \Lam. 
\ee
Applying Lemma \ref{lem:concentration_likelihood}, the probability $\mbP\left(\mD_{N,\gamma,p} \right)$ is bounded above by
\be
\label{lem2_bound}
\scalebox{0.9}{$\exp\left( -\dfrac{(\gamma \, \Lam)^2}{36 \, \widetilde{\lambda}_{\max}^{\star} \, (\mbE \, \norm{\bY}_1 + [D_{g}]^{+}) \,  + 2 \, \sqrt{p} \, \gamma \, \Lam} \, + \, p \, \log \, 5 \right) + \dfrac{1}{\mbE \norm{\bY}_1}$},
\ee
recalling $\widetilde{\lambda}_{\max}^{\star} = \lambda_{\max}(\mcI(\truth))$, 
$[D_{g}]^{+} \coloneqq \max\{0, \, D_{g}\}$,
and 
\beno
D_{g}
&\coloneqq& \dsum_{\{i,j\} \prec \{v,w\} \subset \mN} \, \cov(Y_{i,j}, \, Y_{v,w}),
\ee
where $\{i,j\} \prec \{v,w\}$ implies the sum is taken with respect to the lexicographical ordering
of pairs of nodes.
Choose
\beno
\gamma 
\= \beta \, \sqrt{\dfrac{ p \, \widetilde{\lambda}_{\max}^{\star}}{\mbE \norm{\bY}_1}} \; \dfrac{1}{\widetilde{\lambda}_{\min}^{\epsilon}},
\ee
where $\beta > 0$ is a positive constant independent of $N$ and $p$ whose value will be determined later.
If 
\beno
\lim\limits_{N \to \infty} \, 
\beta \, \sqrt{\dfrac{ p \, \widetilde{\lambda}_{\max}^{\star}}{\mbE \norm{\bY}_1}} \; \dfrac{1}{\widetilde{\lambda}_{\min}^{\epsilon}}
\= 0,
\ee
then for $N$ sufficiently large,
we will have $\gamma < \epsilon$,
which ensures $\epsilon$ may be chosen independent of $N$ and $p$.  
While $\epsilon$ can be chosen independent of $N$ and $p$, 
note that $p$ is expected to be a function of $N$ and thus $\widetilde{\lambda}_{\min}^{\epsilon}$ 
will not (in general) be independent of $N$,
possibly holding implications for how fast $p$ may grow with $N$ for certain $\truth$ and $\epsilon$. 
This choice of $\gamma$ in turn implies that the first term of the exponent in \eqref{lem2_bound} becomes
\be
\label{reduced_exp}
 \exp\left( -\dfrac{\beta^2\, \widetilde{\lambda}_{\max}^{\star} \, \mbE\,\norm{\bY}_1 \, p }{36 \,\widetilde{\lambda}_{\max}^{\star}\, (\mbE\,\norm{\bY}_1+[D_g]^+) \, + \, 2 \,  \beta \, p \, \sqrt{\mbE\,\norm{\bY}_1\,\widetilde{\lambda}_{\max}^{\star}}} \right).
\ee
Canceling $\mbE\,\norm{\bY}_1$ in \eqref{reduced_exp} gives
\beno
\exp\left( -\dfrac{\beta^2\, \widetilde{\lambda}_{\max}^{\star}  \, p }{36 \,\widetilde{\lambda}_{\max}^{\star} \, \left(1+\dfrac{[D_g]^+}{\mbE\,\norm{\bY}_1} \right) \, + \, 2 \,  \beta \,p \, \sqrt{\dfrac{\widetilde{\lambda}_{\max}^{\star}}{\mbE\,\norm{\bY}_1} }} \right).
\ee
By Assumption \ref{assump2},
\beno
p \leq \sqrt{\widetilde{\lambda}_{\max}^{\star} \, \mbE\, \norm{\bY}_1} \, \left(1+\dfrac{[D_g]^+}{\mbE\,\norm{\bY}_1} \right),
\ee
and by Assumption \ref{assump1},
\beno
\dfrac{[D_g]^+}{\mbE\,\norm{\bY}_1}  \; \leq \; C_0,
\ee
where $C_0 > 0$ is a positive constant independent of $N$ and $p$,
we have 
\beno
p \,\sqrt{\dfrac{\widetilde{\lambda}_{\max}^{\star}}{\mbE\,\norm{\bY}_1} }
\; \leq \; 
\widetilde{\lambda}_{\max}^{\star} \, \left(1+\dfrac{[D_g]^+}{\mbE\,\norm{\bY}_1} \right)
\; \leq \; \widetilde{\lambda}_{\max}^{\star} \, \left(1+C_0 \right).
\ee
As a result, the upper bound in \eqref{lem2_bound} reduces to 
\be
\label{reduced_upper_bound}
\mbP\left(\mD_{N,\gamma,p} \right)
\;\leq\; \exp \left( \left(\dfrac{-\beta^2}{36(1+C_0)+2(1+C_0)\beta}  + \log 5 \right) p\right) + \dfrac{1}{\mbE\,\norm{\bY}_1}.
\ee
To obtain the desired convergence rate, require
\be
\label{quad_eq}
\dfrac{-\beta^2}{18\,C+C \, \beta}  + \log 5 = -2,
\ee
where $C = 2(1+C_0)$. 
The constant $\beta$ can be solved by using the quadratic formula and the positive root is given by 
\beno
\beta \= \dfrac{C\,\log\,5 \, -2\, C \; + \; \sqrt{(C\,\log\,5 \, -2 \, C )^2 \; + \; 72\,(C\,\log\,5 - 2\, C )}}{2},
\ee
which ensures $\mbP\left(\mD_{N,\gamma,p} \right) \leq \exp \, (-2\,p) + (\mbE\norm{\bY}_1)^{-1}$.  
We have thus shown,
for all $\nat \in  \partial \mB_2(\truth, \gamma)$, 
that 
\beno
\mbP\left(\norm{\GradL - \mbE \, \GradL}_{2} \,\leq\, \gamma \, \Lam \right)
\ee
is bounded below by
\beno
1 - \exp \, (-2\,p)  \, - \, (\mbE \, \norm{\bY}_1)^{-1}, 
\ee
under the above conditions. 
As a result, 
there exists $N_0 \geq 3$ such that,
for all $N \geq N_0$  
and with probability at least $1 - \exp \, (-2\,p)  \, - \, (\mbE \, \norm{\bY}_1)^{-1}$, 
the set $\Mle$ is non-empty and the unique element of the set $\mle \in \Mle$ 
satisfies (uniqueness following from minimality, as discussed in Section \ref{sec3}) 
\beno
\norm{\mle - \truth}_2 &\leq& 
C \; \dfrac{\sqrt{\widetilde{\lambda}_{\max}^{\star}} }{\widetilde{\lambda}_{\min}^{\epsilon}} \; \sqrt{\dfrac{p}{\mbE \norm{\bY}_1}}.
\ee

\s\s
\subsection{Proof of Corollary \ref{corollary}}
We prove Corollary \ref{corollary} from Section \ref{sec3}. Under the same assumptions as Theorem \ref{thm1} and in the case that the parameter dimension $p$ is fixed, the proof of Corollary \ref{corollary} remains unchanged from that of Theorem \ref{thm1} except that the exponent in equation \eqref{reduced_upper_bound} scales with $N$ as opposed to $p$ in Theorem \ref{thm1}. Following the same notations in the proof of Theorem \ref{thm1}, we rewrite equation \eqref{quad_eq} as
\be
\label{beta_gamma}
 \dfrac{-\beta^2}{18\,C+C \, \beta}  + \log 5 \= -\eta \, (N),
\ee
where $\eta : \mbN^+ \mapsto \mbR^+ $ is an increasing function of $N$. For the ease of notation, we write $\eta_N$ instead of $\eta(N)$ in the rest of the proof.
For any $\alpha_N \in (2\,(\mbE\,\norm{\bY}_1)^{-1}, 1/2)$, let
\beno
\exp \, \left( -\eta_N \, p\right) \= \dfrac{\alpha_N}{2}.
\ee
Note that as $N$ goes to infinity, $\alpha_N$ is allowed to approach 0 through the increasing of $\eta_N$.
Then the upper bound of $\mbP\left(\mD_{N,\gamma,p} \right)$ given in \eqref{reduced_upper_bound} becomes
\beno
\mbP\left(\mD_{N,\gamma,p} \right)
&\leq&  \dfrac{\alpha_N}{2} \; + \;\dfrac{1}{\mbE\,\norm{\bY}_1} &\leq& \alpha_N,
\ee
where the last inequality follows from $ \alpha_N \ge 2\,(\mbE\,\norm{\bY}_1)^{-1}$.
To obtain the desired result,
solve the positive root of $\beta$ in terms of $\eta_N$ from \eqref{beta_gamma}:
\beno
\beta \= \dfrac{C\,\log\,5 \; + \; \eta_N\, C \; + \; \sqrt{(C\,\log\,5 \; + \;\eta_N \, C )^2 \; + \; 72\,(C\,\log\,5 \;+\; \eta_N\, C )}}{2},
\ee
and write $\eta_N$ in terms of $\alpha_N$, where $\alpha_N \to 0$ and $\eta_N \to \infty$ as $N \to \infty$:
\beno
\eta_N \= -\,\dfrac{\log\,(\alpha_N \, / \,2)}{p}.
\ee
Let 
\beno
A_1 \= C\, \log\,5, && A_2 \= \dfrac{C}{p}.
\ee
Then for $\alpha_N \in (2\,(\mbE\,\norm{\bY}_1)^{-1}, 1/2)$,
\beno
\beta \= \scalebox{0.82}{$\dfrac{A_1 \; - \; A_2 \, \log \, (\alpha_N \, / \,2) \; + \; \sqrt{(A_1 \; - \; A_2 \, \log \, (\alpha_N \, / \,2))^2 \; + \; 72\,(A_1 \; - \; A_2\,\log\,(\alpha_N \, / \,2))}}{2}$} \s \\
\= \scalebox{0.66}{$\dfrac{\log\,\left(\dfrac{\alpha_N}{2}\right) \; \left(\dfrac{A_1}{\log\,(\alpha_N \, / \,2)} \; - \; A_2 \;+ \; \sqrt{\left(\dfrac{A_1}{\log\,(\alpha_N \, / \,2)} \; - \; A_2 \right)^2 \; + \; 72\,\left(\dfrac{A_1}{(\log\,(\alpha_N \, / \,2))^2} \; - \; \dfrac{A_2}{\log\,(\alpha_N \, / \,2)} \right)}\right) }{2}$} \s \\
& \leq & 
\scalebox{0.73}{$\dfrac{\log\,\left(\dfrac{\alpha_N}{2}\right)\; \left( \dfrac{A_1}{\log\,0.25} \; - \; A_2 \;+ \; \sqrt{\left(\dfrac{A_1}{\log\,0.25} \; - \; A_2 \right)^2 \; + \; 72\,\left(\dfrac{A_1}{(\log\,(0.25))^2} \; - \; \dfrac{A_2}{\log\,(0.25)} \right)}\right) }{2}$} \s \\
\= A \, \left| \log \, \left(\dfrac{\alpha_N}{2}\right) \right|,
\ee
where 
\beno
A \=\scalebox{0.8}{$\dfrac{ \left| \;\dfrac{A_1}{\log\,0.25} \; - \; A_2 \;+ \; \sqrt{\left(\dfrac{A_1}{\log\,0.25} \; - \; A_2 \right)^2 \; + \; 72\,\left(\dfrac{A_1}{(\log\,(0.25))^2} \; - \; \dfrac{A_2}{\log\,(0.25)} \right)}\;\right| }{2}$}.
\ee
As a result, when $p$ is fixed, we showed that for $\alpha_N \in (2\,(\mbE\,\norm{\bY}_1)^{-1}, 1/2)$, with probability at least $1-\alpha_N$,
\beno
\norm{\mle - \truth}_2 &\leq&
A^\prime \; | \, \log\, (\alpha_N \, / \,2) \, |  \; \dfrac{\sqrt{\widetilde{\lambda}_{\max}^{\star}} }{\widetilde{\lambda}_{\min}^{\epsilon}} \; \sqrt{\dfrac{1}{\mbE \norm{\bY}_1}},
\ee
where $A^\prime = A \, \sqrt{p}$ is a positive constant independent of $N$.
\s \s

\section{Proof of Theorem \ref{thm:minimax} and Corollary \ref{cor:minimax}}
\label{sec:pf_minimax}
We prove Theorem \ref{thm:minimax} and Corollary \ref{cor:minimax} from Section \ref{sec3} in one chapter. We first use Fano's method outlined in Chapter 15.3 of \citet{high-dim-stat} and the Kullback-Leibler divergence to derive the lower bound of the minimax risk for multilayer network models specified in \eqref{general_model}.
Let $\epsilon > 0$ be fixed and consider $\gamma \in (0,\epsilon)$. For $M\ge2$ and some $\delta > 0$, let $\{\nat_1, \ldots, \nat_M\} \subset \mB_2(\truth,\gamma)$ be a $2\delta$-separated set. We then have $\norm{\nat_i-\nat_j}_2 \ge 2\delta$ for any pair $\{i,j\} \subseteq \{1,\ldots,M\}$. Define the Kullback-Leibler divergence of $\nat_i$ and $\nat_j$ by
\beno
\text{KL}(\nat_i,\nat_j) &\coloneqq& \dsum_{\bx \in \mbX}\, \varphi_{\nat_i}(\bx) \, \log \, \dfrac{\varphi_{\nat_i}(\bx)}{\varphi_{\nat_j}(\bx)}, && \{i,j\} \subseteq \{1,\ldots,M\},
\ee
where $\varphi_{\nat}(\bx)$ belongs to a minimal exponential family defined in Proposition \ref{prop:inference}:
\beno
\varphi_{\nat}(\bx) & \coloneqq &\mbP_{\nat}(\bX = \bx \mid \bY = \by)
\= \exp\,(\log f(\bx, \nat) + \log \psi(\nat, \by)),
\ee
recalling $ f(\bx, \nat)$ and $\psi(\nat, \by)$ follow the same form of \eqref{general_model}.
For $\nat \in \mbR^p$, denote by $\bs(\bX) \in \mbR^p$ the sufficient statistic vector of the exponential family $\varphi_{\nat}(\bx)$. Then the Kullback-Leibler divergence can be written as
\be
\label{KL}
\scalebox{0.9}{$\text{KL}(\nat_i,\nat_j)$} \= \scalebox{0.9}{$\dsum_{\bx \in \mbX} \, \varphi_{\nat_i}(\bx) \, \left[ \langle\,\nat_i - \nat_j, \; \bs(\bx) \, \rangle + \log\,\psi(\nat_i,\by) - \log\,\psi(\nat_j,\by) \right]$} \s \\
\= \scalebox{0.9}{$\mbE_{\nat_i}\,\langle\,\nat_i - \nat_j, \; \bs(\bX) \, \rangle  + \log\,\psi(\nat_i,\by) - \log\,\psi(\nat_j,\by)$} \s \\
\= \scalebox{0.9}{$\langle\,\nat_i - \nat_j, \; \bmu(\nat_i) \, \rangle  + \log\,\psi(\nat_i,\by) - \log\,\psi(\nat_j,\by)$},
\ee
where $\bmu(\nat) \coloneqq \mbE_{\nat}\,\bs(\bX)$ is the mean-value parameter map of the exponential family. By Corollary 2.3 of \citet{Br86},
\be
\label{log_norm_const}
\scalebox{0.95}{$\log\,\psi(\nat_j)$} \= \scalebox{0.85}{$\log\,\psi(\nat_i) + \langle\,\nat_j - \nat_i, \; -\bmu(\nat_i) \, \rangle - \dfrac{1}{2}\, \langle\,\nat_j - \nat_i, \; \mcI_{\bX}(\dot\nat) \, (\nat_j - \nat_i) \, \rangle$} \s \\
\= \scalebox{0.85}{$\log\,\psi(\nat_i) + \langle\,\nat_i - \nat_j, \; \bmu(\nat_i) \, \rangle - \dfrac{1}{2}\, \langle\,\nat_i - \nat_j, \; \mcI_{\bX}(\dot\nat)\, (\nat_i - \nat_j) \, \rangle$}, 
\ee
where $\dot\nat = t\nat_i + (1-t)\nat_j$ for some $t \in (0,1)$, and $\mcI_{\bX}(\dot\nat)$ is the Fisher information matrix at $\dot\nat$ for $\bX \in \mbX$. For a fixed $\epsilon > 0$ such that $\gamma \in (0,\epsilon)$ and $\{\nat_1, \ldots, \nat_M\} \subset \mB_2(\truth,\gamma)$, define
\beno
\widetilde{\lambda}_{\max}^{\epsilon}
&\coloneqq & 
\sup\limits_{\nat \in \mB_2(\truth, \epsilon)} \,\dfrac{\lambda_{\max}(\mcI_{\bX}(\nat)) }{\mbE\,\norm{\bY}_1} \= \sup\limits_{\nat \in \mB_2(\truth, \epsilon)} \, \lambda_{\max} \, (\mcI(\nat)),
\ee
where $\lambda_{\max}(\bA)$ is the maximum eigenvalue of matrix $\bA$ and $\mcI(\nat)$ is the Fisher information matrix for an activated dyad defined in Lemma $\ref{lem:min-eig}$.  
Combining \eqref{KL} and \eqref{log_norm_const} and using the standard matrix norm inequality and the triangle inequality, we have
\beno
\text{KL}(\nat_i,\nat_j) \= \dfrac{1}{2} \, \langle\,\nat_i - \nat_j, \; \mcI_{\bX}(\dot\nat) \, (\nat_i - \nat_j) \, \rangle \s \\
&\leq& \dfrac{1}{2} \, \mbE\,\norm{\bY}_1\,\widetilde{\lambda}_{\max}^{\epsilon} \, \norm{\nat_i - \nat_j}_2^2 \s \\
&\leq& \dfrac{1}{2} \, \mbE\,\norm{\bY}_1\,\widetilde{\lambda}_{\max}^{\epsilon} \, (\norm{\nat_i - \truth}_2 + \norm{\nat_j - \truth}_2)^2  \s \\ 
&\leq & 2 \, \epsilon^2\,  \mbE\,\norm{\bY}_1\,\widetilde{\lambda}_{\max}^{\epsilon}.
\ee
Note that the size $M$ of the largest possible $2\,\delta$-separated set $\{\nat_1,\ldots,\nat_M\} \subset \mB_2(\truth,\gamma) \subset \mbR^p$ is the packing number of $\mB_2(\truth,\gamma)$. By Lemma 4.2.8 and Corollary 4.2.13 of \citet{Vershynin18}, we have
\beno
M &\geq& \left(\dfrac{\gamma}{2\,\delta}\right)^p,
\ee
and
\beno
\log \, M &\geq& p \,\log \, \left(\dfrac{\gamma}{2\,\delta}\right).
\ee
By Proposition 15.12 of \citet{high-dim-stat}, the minimax risk $\mcR_N$ has the lower bound
\beno
\mcR_N &\geq& \delta \, \left[ 1-\dfrac{\mF + \log\,2}{\log\,M}\right],
\ee
where 
\beno
\mF &\coloneqq& \max\limits_{\{i,j\} \subseteq \{1,\ldots,M\}} \; \text{KL}(\nat_i,\nat_j).
\ee
Since $\gamma \in (0,\epsilon)$, the lower bound for $\mcR_N$ can be written as
\beno
\mcR_N &\geq& \delta \, \left[ 1-\dfrac{2 \, \gamma^2\, \mbE\,\norm{\bY}_1\, \widetilde{\lambda}_{\max}^{\epsilon} + \log\,2}{p \,\log \, \left(\gamma / 2\,\delta\right)}\right].
\ee
To obtain the desired lower bound $\mcR_N \ge \delta/2$, we need
\beno
\dfrac{2 \, \gamma^2\, \mbE\,\norm{\bY}_1\, \widetilde{\lambda}_{\max}^{\epsilon} + \log\,2}{p \,\log \, \left(\gamma / 2\,\delta\right)} & \leq & \dfrac{1}{2},
\ee
which implies
\beno
\dfrac{4\,\gamma^2\,\mbE\,\norm{\bY}_1\,\widetilde\lambda_{\max}^{\epsilon}}{p} + \dfrac{2\,\log\,(2)}{p} &\leq& \log(\gamma/2) - \log\,(\delta).
\ee
Exponentiating both sides we have
\beno
\exp\,\left( \dfrac{4\,\gamma^2\,\mbE\,\norm{\bY}_1\,\widetilde\lambda_{\max}^{\epsilon}}{p} +  \dfrac{2\,\log\,(2)}{p}\right) &\leq& \dfrac{\gamma/2}{\delta}.
\ee
This leads us to the following inequality
\beno
\delta &\leq& \dfrac{\gamma}{2} \, \exp\,\left( -\,\dfrac{4\,\gamma^2 \, \mbE\,\norm{\bY}_1\,\widetilde\lambda_{\max}^{\epsilon} }{p} -\dfrac{2\,\log\,(2)}{p} \right).
\ee
Choosing 
\beno
\gamma \= 2 \, C\,\sqrt{\dfrac{p}{\widetilde\lambda_{\max}^{\epsilon} \, \mbE\,\norm{\bY}_1}}
\ee
for some $C>0$, we obtain the bound
\be
\label{delta_upper_bound}
\delta &\leq& C \, \exp \left(- 16 \, C^2 - \dfrac{2\,\log\,(2)}{p} \right) \, \sqrt{\dfrac{p}{\widetilde\lambda_{\max}^{\epsilon} \, \mbE\,\norm{\bY}_1}}.
\ee
As long as $p = O\,(\widetilde\lambda_{\max}^{\epsilon} \,\mbE\,\norm{\bY}_1)$, we can choose $C$ to ensure $\gamma \in (0,\epsilon)$. Finally, for all $\delta>0$ satisfying \eqref{delta_upper_bound}, we have $\mcR_N$ lower bounded by
\beno
\mcR_N &\ge& \dfrac{\delta}{2}.
\ee
Note that as $p \ge 1$,
\beno
\exp \left(- 16 \, C^2 - \dfrac{2\,\log\,(2)}{p} \right) & \ge & \exp \left(- 16 \, C^2 - 2\,\log\,(2) \right),
\ee
we may choose 
\beno
\delta \= C \, \exp \left(- 16 \, C^2 - 2\,\log\,(2) \right) \, \sqrt{\dfrac{p}{\widetilde\lambda_{\max}^{\epsilon} \, \mbE\,\norm{\bY}_1}} \s \\
\= A^\prime \, \sqrt{\dfrac{p}{\widetilde\lambda_{\max}^{\epsilon} \, \mbE\,\norm{\bY}_1}}, 
\ee
where $A^\prime = C \, \exp \left(- 16 \, C^2 - 2\,\log\,(2) \right)$.
Then we obtain the desired lower bound for the minimax risk
\be
\label{minimax_lower}
\mcR_N &\ge& \dfrac{\delta}{2} \= \dfrac{A^\prime}{2} \, \sqrt{\dfrac{p}{\widetilde\lambda_{\max}^{\epsilon} \, \mbE\,\norm{\bY}_1}}.
\ee
Next, we show the lower bound in \eqref{minimax_lower} matches with the upper bound of the $\ell_2$-error of the maximum likelihood estimator $\mle$ provided in Theorem \ref{thm1}. Let $A = A^{\prime}/2$ be an unknown constant independent of $N$ and $p$. 
We have
\beno
\mcR_N &\ge& A \, \sqrt{\dfrac{p}{\widetilde\lambda_{\max}^{\epsilon} \, \mbE\,\norm{\bY}_1}} \s \\
\= A \, \sqrt{\dfrac{\widetilde{\lambda}_{\max}^{\star}}{\widetilde{\lambda}_{\max}^{\star}} } \, \left(\dfrac{\widetilde{\lambda}_{\min}^{\epsilon}}{\widetilde{\lambda}_{\min}^{\epsilon}}\right) \, \dfrac{1}{\sqrt{\widetilde\lambda_{\max}^{\epsilon}}} \, \sqrt{\dfrac{p}{\mbE\,\norm{\bY}_1}}\s \\
\= A \,\dfrac{1}{\sqrt{\widetilde\lambda_{\max}^{\epsilon}}} \, \dfrac{\widetilde{\lambda}_{\min}^{\epsilon}}{\sqrt{\widetilde{\lambda}_{\max}^{\star}}} \, \dfrac{\sqrt{\widetilde{\lambda}_{\max}^{\star}} }{\widetilde{\lambda}_{\min}^{\epsilon}} \; \sqrt{\dfrac{p}{\mbE \norm{\bY}_1}} \s \\
&\geq & A \,\left(\dfrac{\widetilde{\lambda}_{\min}^{\epsilon}}{\widetilde\lambda_{\max}^{\epsilon}} \right)\, \dfrac{\sqrt{\widetilde{\lambda}_{\max}^{\star}} }{\widetilde{\lambda}_{\min}^{\epsilon}} \; \sqrt{\dfrac{p}{\mbE \norm{\bY}_1}},
\ee
where the last inequality holds because $\widetilde\lambda_{\max}^{\epsilon} \,\ge \,\widetilde{\lambda}_{\max}^{\star}$.
Under the assumption that
\beno
\widetilde\lambda_{\max}^{\epsilon}= O\, \left(\widetilde{\lambda}_{\min}^{\epsilon}\right),
\ee
we showed that the lower bound of the minimax risk $\mcR_N$ and the upper bound of the $\ell_2$-error of the maximum likelihood estimator presented in Theorem \ref{thm1} match up to an unknown constant independent of $N$ and $p$.
\qed 
