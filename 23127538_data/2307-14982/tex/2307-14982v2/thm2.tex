
In the following, 
$\bZ$ will denote a standard multivariate normal random vector,
i.e.,
with mean vector equal to the zero vector and covariance matrix equal to the identity matrix
(each of appropriate dimension), 
and $\Phi$ will denote the corresponding probability measure. 
 

\begin{theorem}
\label{thm2}
Consider a separable multilayer network model following the form of equation \eqref{general_model} and is 
defined on a set of $N \geq 3$ nodes.
There exists $N_0 \geq 3$ such that,
for all $N \geq N_0$ and any measurable convex set $\mA \subseteq \mbR^p$, 
the error of the multivariate normal approximation
\beno
\left|\mbP((I(\truth) \, \norm{\bY}_1)^{1/2} \, (\mle - \truth) - \Delta \in \mA) - \Phi(\bZ \in \mA)\right|
\ee
is bounded above by
\beno
\dfrac{83}{(\widetilde{\lambda}_{\min}^{\epsilon})^{3/2}} \,
\sqrt{\dfrac{p^{7/2}}{\mbE \, \norm{\bY}_1}}
+  \dfrac{4}{\mbE \, \norm{\bY}_1} + \dfrac{8 \, \left[D_{g}\right]^{+}}{\left(\mbE \, \norm{\bY}_1 \right)^2}
\ee
and $\Delta$ satisfies
\beno
\mbP\left(\norm{\Delta}_2 \leq 
\dfrac{\sqrt{2} \, C^2 \, p^{5/2}}{\sqrt{\mbE \norm{\bY}_1}} \, 
\dfrac{\widetilde{\lambda}_{\max}^{\star} }{(\widetilde{\lambda}_{\min}^{\epsilon})^{5/2}} \right)
&\geq& 1 - \exp\,(-2\,p) \, - \, \dfrac{5+ 8 \, C_0}{\mbE \, \norm{\bY}_1},
\ee
where $C>0$ is the constant given in Theorem \ref{thm1} and $C_0>0$ is the constant given in Assumption \ref{assump1}, both independent of $N$ and $p$.
\end{theorem}

\s


Theorem \ref{thm2} serves as a foundation for establishing the asymptotic normality of the maximum likelihood estimator $\mle$. 
If 
\beno
\lim\limits_{N \to \infty} \; 
\left[ \dfrac{83}{(\widetilde{\lambda}_{\min}^{\epsilon})^{3/2}} \,
\sqrt{\dfrac{p^{7/2}}{\mbE \, \norm{\bY}_1}}
+  \dfrac{4}{\mbE \, \norm{\bY}_1} + \dfrac{8 \, \left[D_{g}\right]^{+}}{\left(\mbE \, \norm{\bY}_1 \right)^2} \right] 
\= 0,  
%\;\,\to\,\; 0
\ee
Theorem \ref{thm2} implies 
$(I(\truth) \, \norm{\bY}_1)^{1/2} \, (\mle - \truth) - \Delta$ will converge
in distribution
to a standard multivariate normal random vector,
as the error bound on the multivariate normal approximation will vanish in this case.  
The term $\Delta$ can be viewed as an error term,
resulting from the fact that the normal approximation in Theorem \ref{thm2} is obtained via a multivariate Taylor approximation 
in order to bridge the distributional gap between key statistics which admit forms amenable to existing 
theorems for the normal approximation 
and the parameter vectors of interest,
thus introducing an additional source of error in the normal approximation. 

While involved, 
the above condition for asymptotic multivariate normality essentially places restrictions 
on the dependence induced through the single-layer basis network $\bY$ 
measured by $[D_{g}]^{+}$,
as well as the smallest eigenvalue of the dyad-based information matrix $\mcI(\nat)$ 
in a neighborhood of the data-generating parameter vector $\truth$ as measured by $\widetilde{\lambda}_{\min}^{\epsilon}$, 
and the model dimension $p$. 
As a result, 
if the information matrix $\mcI(\nat)$ is nearly singular at $\truth$, 
in which case $\widetilde{\lambda}_{\min}^{\epsilon}$ will be small,
the error of the normal approximation will be uniformly larger (all else equal).
Likewise, 
if the edge dependence in $\bY$ is large as measured by $[D_{g}]^{+}$,
we may not have sufficient activated dyads to ensure the error bound is small,
as $\norm{\bY}_1$ may not be tightly concentrated around $\mbE \, \norm{\bY}_1$. 
The dependence of the error approximation on the dimension of the random vector is a known challenge 
in establishing multivariate normality \citep[e.g.,][]{Raic19}.  
All quantities which are not explicit constants can increase or decrease with $N$,
with the rates of these increases or decreases having implications for the rate of convergence in distribution. 
Theorem \ref{thm2} demonstrates 
that the allowable scaling for most of quantities is with respect to the expected number of activated dyads 
$\mbE \, \norm{\bY}_1$.

We further examine Theorem \ref{thm2} through an example where  
$\bY$ is a Bernoulli random graph model,
which assumes edge variables are independent Bernoulli random variables with probability $\pi \in (0, 1)$. 
Under this model, 
$[D_{g}]^{+} = 0$ owing to the independence of edge variables
and $\mbE \norm{\bY}_1 = \pi \, \binom{N}{2}$. 
Under this scenario, 
we can show that 
\beno
\left|\mbP((I(\truth) \, \norm{\bY}_1)^{1/2} (\mle - \truth) - \Delta \in \mA) - \Phi(\bZ \in \mA)\right|
\ee
is bounded above by
\beno
\dfrac{166}{\sqrt{\pi \, (\widetilde{\lambda}_{\min}^{\epsilon})^{3}}} \; \dfrac{p^{1.75}}{N} + \dfrac{16}{\pi  N^2}, 
\ee
with the additional bound   
\beno
\mbP\left(\norm{\Delta}_2 \leq 
\dfrac{\sqrt{2} \, C^2 \, p^{2.5}}{\sqrt{\pi\,\binom{N}{2}}} \, 
\dfrac{\widetilde{\lambda}_{\max}^{\star} }{(\widetilde{\lambda}_{\min}^{\epsilon})^{2.5}} \right)
&\geq& 1 - \exp\,(-2\,p) \, - \, \dfrac{5+ 8 \, C_0}{\pi\,\binom{N}{2}},
\ee
where $C>0$ is the constant given in Theorem \ref{thm1} and $C_0>0$ is the constant given in Assumption \ref{assump1}, both independent of $N$ and $p$.
If $\widetilde{\lambda}_{\min}^{\epsilon}$ and $\pi$ are both bounded away from $0$,
then the error of the normal approximation will convergence to $0$ 
provided $(p^{2.5} \, \widetilde{\lambda}_{\max}^{\star}) \,/\, N \to 0$ as $N \to \infty$,
which is sufficient to ensure $\norm{\Delta}_2$ converges in probability to $0$. 
Under the fully saturated model specification for \eqref{general_model} ($H = K$), 
the Binomial theorem shows that $p = 2^{K} - 1 \leq 2^K$. 
Hence, 
the dimension restriction on $p$ in turn implies a restriction on the allowable rate of growth of the number of layers $K$ with $N$,
where a sufficient condition for $(p^{2.5} \, \widetilde{\lambda}_{\max}^{\star}) \,/\, N \to 0$ 
is for $K \leq .5 \, \log N$. 
In other words, 
the number of layers $K$ can grow at most logarithmically with $N$ in the fully saturated model. 
In cases when the number of interaction terms included in the cross-layer dependence probability model 
is fixed, 
$K$ may admit a sublinear scaling  with $N$. 





