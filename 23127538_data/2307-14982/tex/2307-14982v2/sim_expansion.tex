\subsection{Simulation expansion (Under update)}

\subsubsection{Consistency of MPLE for random data-generating parameters} 
We took 250 multilayer network samples from four different model-generating parameters uniformly sampled from $[-1,1]$ at increasing network sizes from $N = 200$ to $1000$. Two density settings of the Bernoulli basis network $\bY$ are simulated: $1)$ fixed density with $\mbP(Y_{i,j} = 1) = 0.8$, and $2)$ varying densities as a function of network size $N$, i.e., $\mbP(Y_{i,j} = 1) = 20/N$. Figure \ref{consistency_dense} shows the consistency of MPLE for four different data-generating parameters in the setting of a denser basis network while Figure \ref{f_varyspars} shows the same consistency result in the setting of a sparse basis network. Another example of decay rate change due to density change will be added as well.
% Figure environment removed


% Figure environment removed



\hide{
The MPLE $\mple$ of the ten parameter settings are displayed in Figure \ref{f_hm1} and Figure \ref{f_hm2} with different arrangements. In Figure \ref{f_hm1}, $x$-axis is ordered by the $\ell_2$-norm of corresponding model-generating parameters while in Figure \ref{f_hm2}, the $x$-axis is ordered by the element-wise sum of each parameter, where negative or smaller values suggest a more sparse network compared to larger values. In general, networks with more edges tend to be better estimated at a finite network size. Note that, the $\ell_2$ errors in Figure \ref{f_hm1} and \ref{f_hm2} are truncated at 0.3 because some parameters do not have a converged MPLE.
% Figure environment removed
% Figure environment removed
}


\subsubsection{Estimation errors and the number of layers}
% Figure environment removed
Figure \ref{f_hm3} is the heat map of the relative $\ell_2$ errors of $\mple$ at $\truth= (-3,  0.5, 0, -2, 0, -1)$, arranged by the number of layers on the $y$-axis and the network size on the $x$-axis. When the network size $N$ is small (50), more layers lead to larger estimation error. When the network size $N$ is large, the increase in the number of layers from 3 to 6 does not play a significant role in MPLE as shown in the graph.

