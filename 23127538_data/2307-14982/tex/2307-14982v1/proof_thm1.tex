

\ttproof \ref{thm1}. 
We first prove the theorem for maximum likelihood estimators,
and then discuss extensions and changes necessary to prove the result for maximum pseudolikelihood estimators. 
By Proposition \ref{prop:inference}, 
observing $\bX = \bx$ implies we observe $\bY = \by$,
as for each given $\bx \in \mbX$,
$\bY = \by$ ($\mbP$-a.s.) for one and only one $\by \in \mbY$ given by 
\beno
y_{i,j} 
\= \one\left( \norm{\bx_{i,j}}_1 \,>\, 0 \right),
&& \{i,j\} \subset \mN. 
\ee 
Denote the gradient of $-\ell(\nat; \bx, \by)$ by 
\beno
\gradL 
&\coloneqq& -\nabla_{\nat} \, \ell(\nat; \bx, \by)
\ee
and the expected Hessian matrix of the negative log-likelihood by 
\beno
\bH(\nat) 
&\coloneqq& -\mbE \, \nabla_{\nat}^2 \, \ell(\nat; \bX, \bY). 
\ee 
Theorem 6.3.4 of \citet{OrRh20} states that if 
\beno 
\label{eq:ex_cond} 
(\nat - \truth)^{\top} \, \gradL
&\geq& 0 
\ee
for all $\nat \in \partial \, \mB_2(\truth, \epsilon)$, 
where $\partial \, \mB_2(\truth, \epsilon)$ is the boundary of the set 
\beno
\mB_2(\truth, \epsilon) 
\= \{ \nat \in \mbR^p \,:\, \norm{\nat - \truth}_2 < \epsilon\},
\ee 
then $\gradL$ 
has a root in $\mB_2(\truth, \epsilon) \cup \partial \mB_2(\truth, \epsilon)$,
i.e.,
$\mle$ exists and satisfies $\norm{\mle - \truth}_2 \leq \epsilon$.
Note that a root of $\gradL$ is also a root of $-\gradL$;
in what follows, 
we consider finding a maximizer of $\ell(\nat;\bx,\by)$ by finding a minimizer of $-\ell(\nat;\bx,\by)$. 
The classification of roots as maximizers/minimizers 
is justified from the fact that that $\ell(\nat; \bx, \by)$ is concave in $\nat$,
a fact which follows from Proposition \ref{prop:inference},
as $g(\by)$ is constant in $\nat$ and $\log \, \mbP_{\nat}(\bX = \bx \,|\, \bY = \by)$ 
is the log-likelihood of a minimal, full, and regular exponential family with natural parameter vector $\nat$
and thus is strictly concave in $\nat$ \citep[Proposition 3.10, p. 32,][]{Su19}. 
By the multivariate mean-value theorem \citep[][Theorem 5]{FuMa91}, 
\beno
(\nat - \truth)^{\top}  \mbE \, \GradL 
\= (\nat - \truth)^{\top}  \mbE \, \gamma_{\truth}(\bX, \bY) 
+ (\nat - \truth)^{\top}  \bH(\dot\nat)  (\nat - \truth) \s \\ 
\=(\nat - \truth)^{\top}  \bH(\dot\nat)  (\nat - \truth), 
\ee
where $\dot\nat = t \, \nat + (1 - t) \, \truth$ (some $t \in [0, 1]$) 
and by invoking Lemma 2 of \citet{StSc21},
which shows that both the expected log-likelihood and log-pseudolikelihood 
of a minimal exponential family is uniquely maximized at the data-generating parameter vector $\truth$, 
implying $\mbE \, \gamma_{\truth}(\bX, \bY) = 0$. 
Let $\epsilon \in (0, \epsilon^\star)$
and arbitrarily take $\nat \in \partial \mB_2(\truth, \epsilon)$. 
Then 
\beno
(\nat - \truth)^{\top} \bH(\dot\nat) (\nat - \truth)
= \dfrac{(\nat - \truth)^{\top}  \bH(\dot\nat)  (\nat - \truth)}{(\nat - \truth)^{\top} (\nat- \truth)}  
\norm{\nat - \truth}_2^2
\,\geq\, \epsilon^2 \, \lambda_{\min}(\bH(\dot\nat)),  
\ee
since $\norm{\nat - \truth}_2 = \epsilon$ as $\nat \in \partial \mB_2(\truth, \epsilon)$ 
and because the Rayleigh quotient of a matrix is bounded below by the smallest eigenvalue of that matrix so that
\beno
\dfrac{(\nat - \truth)^{\top}  \bH(\dot\nat)  (\nat - \truth)}{(\nat - \truth)^{\top} (\nat- \truth)}
\;\geq\; \lambda_{\min}(\bH(\dot\nat))
\;\geq\; \inf\limits_{\nat \in \mB_2(\truth, \epsilon^\star)} \, \lambda_{\min}(\bH(\nat)),
\ee
where $\lambda_{\min}(\bH(\dot\nat))$ is the smallest eigenvalue of $\bH(\dot\nat)$,
noting that 
\beno
\norm{\dot\nat - \truth}_2 
\= \norm{t \, \nat + (1 - t) \, \truth - \truth}_2
\= t \, \norm{\nat - \truth}_2 
&\leq& \norm{\nat - \truth}_2
&\leq& \epsilon^\star,  
\ee
since $t \in [0, 1]$. 
Lemma \ref{lem:min-eig} showed that 
\beno
\lambda_{\min}(\bH(\nat)) 
\= \lambda_{\min}(\mcI(\nat)) \; \mbE \, \norm{\bY}_1, 
\ee
which in turn implies 
\beno
\inf\limits_{\nat \in \mB_2(\truth, \epsilon^\star)} \, \lambda_{\min}(\bH(\nat))
\= \xi_{\epsilon^\star} \; \mbE \, \norm{\bY}_1,
\ee
where 
\beno
\xi_{\epsilon^\star}
&\coloneqq& \inf\limits_{\nat \in \mB_2(\truth, \epsilon^\star)} \, \lambda_{\min}(\mcI(\nat)),
\ee
with $\mcI(\nat)$ defined in Lemma \ref{lem:min-eig}. 
Hence,
for $\nat \in \partial \mB_2(\truth, \epsilon)$ ($\epsilon \in (0, \epsilon^\star)$),  
\beno
(\nat - \truth)^{\top}  \mbE \, \GradL
&\geq& \epsilon^2 \; \xi_{\epsilon^\star} \; \mbE \, \norm{\bY}_1. 
\ee
We next turn to showing  
\beno
\mbP\left( \inf\limits_{\nat \in \mB_2(\truth,\epsilon)} \, 
(\nat - \truth)^{\top} \, \GradL \,\geq\, 0 \right) 
&\geq& 1 - 2 \, (\mbE \norm{\bY}_1)^{-1},
\ee
by showing that the event  
\beno
\sup\limits_{\nat \in \mB_2(\nat^\star, \epsilon)} \, 
|(\nat - \truth)^{\top} \, ( \mbE \, \GradL - \GradL)|
\;<\; \epsilon^2 \; \Lam
\ee
occurs with probability at least $1 - 2 \,  (\mbE \norm{\bY}_1)^{-1}$, 
in turn implying that the event   
$\norm{\mle - \truth}_2 \leq \epsilon$ 
happens with probability at least $1 - 2 \,  (\mbE \norm{\bY}_1)^{-1}$. 
Applying the Cauchy-Schwarz inequality and utilizing standard vector norm inequalities,
\beno
|(\nat - \truth)^{\top} \, ( \mbE \, \GradL - \GradL)| 
&\leq& \norm{\nat - \truth}_2 \, \norm{\GradL  - \mbE \, \GradL}_2 \s \\ 
&\leq&  \epsilon \; \sqrt{p} \; \norm{\GradL - \mbE \, \GradL}_{\infty}, 
\ee
noting $\nat \in \partial \mB_2(\truth, \epsilon)$.
It suffices to demonstrate,
for all $\nat \in \partial \mB_2(\truth, \epsilon)$, 
that  
\beno
\mbP\left(\norm{\GradL - \mbE \, \GradL}_{\infty} \,<\, \epsilon \, p^{-\frac{1}{2}} \, \Lam \right)
&\geq& 1 - 2 \, (\mbE \norm{\bY}_1)^{-1}. 
\ee
For ease of presentation,
we define $\mD_{N,\epsilon,p}$ to be the event 
\beno
\norm{\GradL - \mbE \, \GradL}_{\infty} 
&\geq& \epsilon \, p^{-\frac{1}{2}} \, \Lam. 
\ee
Applying Lemma \ref{lem:concentration_likelihood}, 
\beno
\mbP\left(\mD_{N,\epsilon,p} \right)
\;\leq\;
2 \, \exp\left( -\dfrac{(\epsilon \, \xi_{\epsilon^\star} \; \mbE \, \norm{\bY}_1)^2}
{p \, (\mbE \, \norm{\bY}_1 + [D_{g}]^{+})} + \log \, p \right) + \dfrac{1}{\mbE \norm{\bY}_1}, 
\ee
where 
$[D_{g}]^{+} \coloneqq \max\{0, \, D_{g}\}$,  
recalling 
\beno
D_{g}
&\coloneqq& \dsum_{\{i,j\} \prec \{v,w\} \subset \mN} \, \cov(Y_{i,j}, \, Y_{v,w}),
\ee
where $\{i,j\} \prec \{v,w\}$ implies the sum is taken with respect to the lexicographical ordering
of pairs of nodes.
Under the assumption that $\mbE \norm{\bY}_1 \geq 1$,
\beno
2 \, \exp\left( -\dfrac{\epsilon^2\, \xi_{\epsilon^\star}^2 \, (\mbE \norm{\bY}_1)^2}
{p \, (\mbE \, \norm{\bY}_1 + [D_g]^{+})} + \log \, p \right)
&\leq& 2 \, \exp\left( -\dfrac{\epsilon^2\, \xi_{\epsilon^\star}^2 \, \mbE \norm{\bY}_1}{p \, (1 + [D_{g}]^{+})} + \log \, p \right).
\ee
Take 
\beno
\epsilon 
\= \sqrt{\dfrac{3 \, p \, \log N}{\mbE \norm{\bY}_1}} \; \dfrac{\sqrt{1 + [D_{g}]^{+}}}{\xi_{\epsilon^\star}}.  
\ee
If 
\beno
\lim\limits_{N \to \infty} \, 
\sqrt{\dfrac{3 \, p \, \log N}{\mbE \norm{\bY}_1}} \; \dfrac{\sqrt{1 + [D_{g}]^{+}}}{\xi_{\epsilon^\star}}
\= 0,
\ee
then for $N$ sufficiently large,
we will have $\epsilon < \epsilon^\star$,
which ensures $\epsilon^\star$ may be chosen independent of $N$ and $p$.  
While $\epsilon^\star$ can be chosen independent of $N$ and $p$, 
note that $p$ is expected to be a function of $N$ and thus $\xi_{\epsilon^\star}$ 
will not (in general) be independent of $N$,
possibly holding implications for how fast $p$ may grow with $N$ for certain $\truth$ and $\epsilon^\star$.  
This choice of $\epsilon$ in turn implies 
\beno
2 \, \exp\left( -\dfrac{\epsilon^2\, \xi_{\epsilon^\star}^2 \, \mbE \norm{\bY}_1}{p \, (1 + [D_{g}]^{+})} + \log \, p \right)
\= 2 \, \exp\left(- 3 \log N + \log p \right)
&\leq& 2 \, N^{-2},
\ee
under the assumption that $p \leq N$,
which ensures $-3 \log N + \log p \leq -2 \, \log N$.  
Note that $\mbE \norm{\bY}_1 \leq \binom{N}{2} \leq N^{2}$.
We have thus shown,
for all $\nat \in  \partial \mB_2(\truth, \epsilon)$, 
that 
\beno
\mbP\left(\norm{\GradL - \mbE \, \GradL}_{\infty} \,\leq\, \epsilon \, p^{-\frac{1}{2}} \, \Lam \right)
&\geq& 1 - 3 \, (\mbE \, \norm{\bY}_1)^{-1}, 
\ee
under the above conditions. 
As a result, 
there exists $N_0 \geq 3$ such that,
for all $N \geq N_0$  
and with probability at least $1 - 3 \, (\mbE \, \norm{\bY}_1)^{-1}$, 
the set $\Mle$ is non-empty and the unique element of the set $\mle \in \Mle$ 
satisfies (uniqueness following from minimality, as discussed in Section \ref{sec3}) 
\beno
\norm{\mle - \truth}_2 &\leq& 
\sqrt{\dfrac{3 \, p \, \log N}{\mbE \norm{\bY}_1}} \; \dfrac{\sqrt{1 + [D_{g}]^{+}}}{\xi_{\epsilon^\star}}.  
\ee
The above proof can be extended to maximum pseudolikelihood estimators by substituting 
the relevant quantities (e.g., $\widetilde\xi_{\epsilon^\star}$ for $\xi_{\epsilon^\star}$, etc.). 
The one change of note is that instead of applying the concentration inequality in Lemma \ref{lem:concentration_likelihood},
we apply the concentration inequality in Lemma \ref{lem:concentration_pl},
which includes an additional factor of $K^2$. 
Following these steps and repeating the above proof will show that
there exists $N_0 \geq 3$ such that,
for all $N \geq N_0$ and 
with probability at least $1 - 3 \, (\mbE \, \norm{\bY}_1)^{-1}$, 
the set $\Mple$ is non-empty and each $\mple \in \Mple$
satisfies 
\beno
\norm{\mple - \truth}_2 &\leq&
\sqrt{\dfrac{3 \, p \, K^2 \, \log N}{\mbE \norm{\bY}_1}} \; \dfrac{\sqrt{1 + [D_{g}]^{+}}}{\widetilde\xi_{\epsilon^\star}}, 
\ee
where
\beno
\widetilde\xi_{\epsilon^\star}
&\coloneqq& \inf\limits_{\nat \in \mB_2(\truth, \epsilon^\star)} \, \lambda_{\min}(\widetilde\mcI(\nat)).
\ee
\qed 
