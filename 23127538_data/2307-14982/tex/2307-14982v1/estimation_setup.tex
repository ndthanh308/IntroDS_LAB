
Maximum likelihood estimation for network data with dependent edges faces significant computational challenges,
as the normalizing constants for such models are often computationally intractable,
which makes direct maximization of likelihood functions infeasible in general cases.
For network separable multilayer networks satisfying  \eqref{general model}, 
Proposition \ref{prop:inference} establishes that the log-likelihood function takes the form 
\be
\label{loglikelihood}
\ell(\nat; \bx, \by) 
&\coloneqq& \log \,\sepmodel(\bX = \bx, \bY = \by) 
\= \log \, \mbP_{\nat}(\bX = \bx \,|\, \bY = \by) + \log \, g(\by).
\ee
Given an observation $\bx \in \mbX$ of the multilayer network $\bX$,  
and therefore an observation $\by \in \mbY$ of $\bY$ by Proposition \ref{prop:inference},
we denote the set of maximum likelihood estimators by 
\beno
\Mle &\coloneqq& \left\{ \nat \in \mbR^p 
\,:\, \ell(\nat; \bx, \by) = \sup\limits_{\nat^{\prime} \in \mbR^p} \, \ell(\nat^{\prime}; \bx, \by) \right\},
\ee 
and reference individual elements of the set by $\mle \in \Mle$.
As Proposition \ref{prop:inference} establishes 
$\log \, \mbP_{\nat}(\bX = \bx \,|\, \bY = \by)$ to be a minimal, 
and by construction regular, 
exponential family, 
$|\Mle| \in \{0, 1\}$, 
i.e., 
when the maximum likelihood estimator exists, 
the set $\Mle$ will contain a unique element when non-empty
\citep[Proposition 3.11, pp. 32--33,][]{Su19}.


Two predominant methods of approximating $\truth$ when the likelihood function is computationally intractable 
have emerged in the literature. 
Monte Carlo maximum likelihood estimation (MCMLE) \citep{GeTh92},
which constructs a simulation-based approximation to the likelihood function 
in order to approximate the maximum likelihood estimator,
is an established method for approximating maximum likelihood estimators 
in the statistical network analysis literature \citep{HuHa06}.
While able to provide accurate estimates of maximum likelihood estimators for complex models 
\citep[e.g.,][]{StScBoMo19,ScKrBu17}, 
a drawback of MCMLE,
and other simulation-based estimation methodology, 
is the computational burden which can scale with both the complexity of the model and the size of the network \citep{BaBrSl11}.  
In settings where the computation of the MCMLE is impractical, 
a computationally efficient alternative is provided via the maximum pseudolikelihood estimator (MPLE) \citep{Bj74},
whose application to social network analysis and to statistical network analysis dates back to \citet{StIk90}.  
As Proposition \ref{prop:inference} establishes that $\bY$ is observable through $\bX$,
\beno
\mbP(Y_{i,j} = y_{i,j} \,|\, \bX = \bx, \bY_{-\{i,j\}} = \by_{-\{i,j\}})
\= 1,
\ee
when $y_{i,j} = \one(\norm{\bx_{i,j}}_1 \,>\, 0)$ and $\bY_{-\{i,j\}}$ is defined to be the 
($\binom{N}{2}$-$1$)-dimensional vector of edge variables in $\bY$ which excludes $Y_{i,j}$. 
As a result, 
if $(\bx, \by)$ is network concordant,
then  
\beno
\log \, \mbP(Y_{i,j} = y_{i,j} \,|\, \bX = \bx, \bY_{-\{i,j\}} = \by_{-\{i,j\}}) \= 0, 
&& \mbox{for all } \{i,j\} \subset \mN.
\ee 
The log-pseudolikelihood of \eqref{general model} can then be written down as 
\be
\label{eq:log-pseudo}
\pl(\nat; \bx, \by) 
\,\coloneqq \dsum_{\{i,j\} \subset \mN}  \dsum_{k=1}^K \, \log  
\sepmodel(X_{i,j}^{(k)} = x_{i,j}^{(k)} \, |\, \bX_{i,j}^{(-k)} = \bx_{i,j}^{(-k)}, \bY = \by), 
\ee
provided $(\bx, \by)$ is network concordant
and by exploiting the conditional independence properties implied by \eqref{general model}. 
We denote the set of maximum pseudolikelihood estimators of the data-generating parameter vector $\truth$ by 
\beno
\Mple &\coloneqq& \left\{ \nat \in \mbR^p \,: \, \pl(\nat; \bx, \by) = \sup\limits_{\nat^{\prime} \in \mbR^p} \, \pl(\nat^{\prime}; \bx, \by) \right\}.
\ee
Individual elements are referenced by $\mple \in \Mple$. 
Uniqueness of maximum pseudolikelihood estimators for exponential families is more complicated than 
for maximum likelihood estimators. 
However, 
our theoretical results establish that all elements $\mple \in \Mple$ will all be within the same Euclidean distance to $\truth$.
The assumption that $(\bx, \by)$ is network concordant comes at no cost 
since $\bY$ is predictable through $\bX$,
as discussed above,
meaning that given an observation $\bx$ of $\bX$,
we can find the unique network concordant pair $(\bx, \by)$ with probability one.  
The advantage of \eqref{eq:log-pseudo} is that 
the conditional probabilities 
$\sepmodel(X_{i,j}^{(k)} = x_{i,j}^{(k)} \, |\, \bX_{i,j}^{(-k)} = \bx_{i,j}^{(-k)}, \bY = \by)$ 
of edges in the multilayer network 
are often computationally tractable 
since the conditional distribution is a Bernoulli distribution when $Y_{i,j} = 1$,
and is a degenerate point mass at $0$ when $Y_{i,j} = 0$.

\hide{
Pseudolikelihood-based estimators have the following computational advantages:
\ben
\item Algorithms are generally deterministic and do not require simulation-based approximation schemes,
which aids in reproducibility of results; 
\item Algorithms are generally more scalable, 
relative to alternatives such as MCMLE and other simulation-based approximations, 
and are able to be parallelized to take advantage of larger  multicore computing infrastructures which are becoming increasingly common.  
\een
}

In this work, 
we consider both maximum likelihood estimators and maximum pseudolikelihood estimators. 
As seen from the forms of $\ell(\nat; \bx, \by)$ and $\pl(\nat; \bx, \by)$ given above, 
the gradients and Hessians of the log-likelihood and log-pseudolikelihood equations 
do not directly depend on $g(\by)$, 
echoed by the results in Proposition \ref{prop:inference}.
However, 
as mentioned in the previous section, 
theoretical guarantees for estimators of $\truth$ will be indirectly influenced by the choice of $g(\by)$,
a point supported by the following lemma. 


