
We introduce a network separable model for multilayer networks by specifying probability distributions 
on a double of networks $(\bX, \bY)$, 
where $\bY$ will represent the network formation process,
which we will call the {\it basis network},  
and $\bX$ will represent the realized multilayer network. 
We assume that $\bY \in \mbY \coloneqq \{0, 1\}^{\binom{N}{2}}$ 
is an undirected, single-layer network defined on the set of nodes $\mN$ 
where 
\beno
Y_{i,j}
\= \begin{cases}
1 & \mbox{nodes } i \mbox{ and } j \mbox{ are connected in the basis network} \s \\ 
0 & \mbox{otherwise} 
\end{cases},
\ee 
for each $\{i,j\} \subset \mN$, 
making the usual conventions for undirected networks mentioned previously.
We consider semi-parametric families of probability distributions 
$\{\sepmodel : \nat \in \mbR^p\}$ for $(\bX, \bY)$ 
which are absolutely continuous with respect to a $\sigma$-finite measure $\nu$ defined on $\mP(\mbX \times \mbY)$,
where $\mP(\mbX \times \mbY)$ is the power set of $\mbX \times \mbY$. 
Typically,
$\nu$ will be the counting measure, 
however sparsity inducing reference measures are also admissible and have found application in network data applications
in order to model sparse networks \citep{butts:jms:2018,StSc21}. 
We say the family $\sepfam \coloneqq \{\sepmodel : \nat \in \mbR^p\}$ is {\it network separable} if 
each $\sepmodel \in \sepfam$ admits the form: 
\be
\label{general model}
\sepmodel(\{(\bx, \by)\}) 
\= f(\bx,\nat) \; g(\by) \; h(\bx, \by) \; \psi(\nat, \by),
&&  (\bx, \by) \in \mbX \times \mbY, 
\ee
where 
\bi
\item $h : \mbX \times \mbY \mapsto \{0, 1\}$ is given by 
\beno
h(\bx, \by) 
\= \dprod_{\{i,j\} \subset \mN} \, 
\one(\norm{\bx_{i,j}}_1 > 0)^{y_{i,j}} \; \one(\norm{\bx_{i,j}}_1 = 0)^{1 - y_{i,j}}, 
\ee 
where $\bx_{i,j} = (x_{i,j}^{(1)}, \ldots, x_{i,j}^{(K)}) \in \mbX_{i,j}$ ($\{i,j\} \subset \mN$). \s   
\item $f : \mbX \times \mbR^p \mapsto (0, 1)$ is given by 
\beno
f(\bx, \nat)
&= \, \dprod_{\{i,j\} \subset \mN} \,
\exp&\left(\dsum_{k=1}^K\,\theta_{k}\,x_{i,j}^{(k)} +  \dsum_{\substack{k < l}}^{K}\, \theta_{k,l} \, x_{i,j}^{(k)} \, x_{i,j}^{(l)} + \ldots \right.\\[20pt]
&& \left.+ \dsum_{k_1 < \,\ldots\,< k_H}^{K}\, \theta_{k_1,k_2,\ldots,k_H} \, x_{i,j}^{(k_1)} \cdots\, x_{i,j}^{(k_H)} \right),
\ee
where $H \le K$ is the highest order of cross-layer interactions included in the model.
We write $\theta_{k_1,k_2,\ldots,k_h}$ to reference the $h$-order interaction parameter 
for the interaction term among layers $\{k_1, \ldots, k_h\} \subseteq \{1, \ldots, K\}$. \s  
\item $\psi : \bTheta \times \mbY \mapsto (0, \infty)$ is defined by 
\beno 
\psi(\nat, \by) 
\= \left[ \, \dsum_{\bx \in \mbX} \, f(\bx, \nat) \, h(\bx, \by) \right]^{-1}, 
\ee
and functions to ensure summation to one so that the specification in \eqref{general model}
will be a valid probability mass function for $(\bX, \bY)$. 
\item $g : \mbY \mapsto (0, 1)$ is the marginal probability mass function of $\bY$ 
and is assumed to be strictly positive on $\mbY$. 
\ei

The notation $\sepmodel(\{(\bx, \by)\})$ is well-defined for each $(\bx, \by) \in \mbX \times \mbY$, 
as $\sepmodel$ is a probability measure defined on $\mP(\mbX \times \mbY)$. 
In an abuse of notation, 
we will frequently write probability expressions 
$\sepmodel(\bX = \bx, \bY = \by)$ for the joint probability of $\{(\bx, \by)\}$, 
and $\sepmodel(\bX = \bx \,|\, \bY = \by)$ for the conditional probability of the event $\bX = \bx$ 
conditional on the event $\bY = \by$. 
We denote the data-generating parameter vector by $\truth \in \mbR^p$,
and the corresponding probability measure and expectation operator  
by $\mbP \equiv \mbP_{\truth}$ and $\mbE \equiv \mbE_{\truth}$, respectively.

The terminology {\it network separable} is motivated by the fact that the specification in \eqref{general model} 
separates the network formation process $\bY$,
specified by $g(\by)$, 
from the layer formation process, 
specified by $f(\bx, \nat)$. 
The two are joined by the function $h(\bx, \by)$,
which ensures $\norm{\bx_{i,j}}_1 = 0$ 
whenever $Y_{i,j} = 0$
and $\norm{\bx_{i,j}}_1 > 0$ whenever $Y_{i,j} = 1$,
and by $\psi(\nat, \by)$ which ensures the resulting product of functions will be a valid probability mass function. 
The latter has less of a direct role in modeling the cross-layer dependence and interaction between $\bX$ and $\bY$,
essentially fulfilling the role of a normalizing constant for the conditional probability distribution of $\bX$ given $\bY$,
as derived in Proposition \ref{prop:inference}. 
We call dyads $\{i, j\} \subset \mN$ 
with $Y_{i,j} = 1$ {\it activated dyads},
as we allow edges between nodes $i \in \mN$ and $j \in \mN$ in $\bX$ if and only if $\{i,j\}$ 
is an activated dyad.
Such specifications have the advantage of being able to specify the network formation process separately 
from the process that populates the layers of activated dyads,
thus modeling the cross-layer dependence conditional on the network $\bY$. 
A pair $(\bx, \by) \in \mbX \times \mbY$ that satisfies $h(\bx, \by) =1$ is said to be {\it network concordant}.

To illustrate the flexibility and generality of \eqref{general model},
observe that $g(\by)$ is allowed to  be any probability mass function for a single layer network $\bY$ 
(e.g., exponential-family random graph model, stochastic block model, latent space model),
provided $g(\by) > 0$ for all $\by \in \mbY$. 
We therefore view our framework as semi-parametric as $g(\by)$ need not assume a specific parametric form.  
Moreover, 
our framework can be viewed as non-parametric 
within the family of network separable multilayer networks 
when the maximal possible order interaction terms are included in \eqref{general model},
a point on which we further elaborate later. 
An important feature of our framework lies in the fact that 
the choice of the probability distribution for the network formation process does not directly 
influence inference for the cross-layer dependence structure,
i.e.,
the choice of $g(\by)$ does not directly influence inference for $\truth$. 
Proposition \ref{prop:inference} demonstrates this point in the case of likelihood-based inference.  
