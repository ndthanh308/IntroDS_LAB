
\begin{lemma}
\label{lem:concentration_likelihood}
Consider multilayer networks satisfying \eqref{general model}
which are defined on a set of $N \geq 3$ nodes and $K \geq 1$ layers. 
Define 
$\gradL \coloneqq - \nabla_{\nat} \, \ell(\nat; \bx, \by)$,  
where $\ell(\nat; \bx, \by)$ is the log-likelihood function. 
Then,
for all $t > 0$ and $\nat \in \mbR^p$,
\beno
\mbP\left(\norm{\GradL - \mbE \, \GradL}_{\infty} \geq t \right) 
\;\leq\; 
2 \, \exp\left( -\dfrac{t^2}{\mbE \, \norm{\bY}_1 + [D_{g}]^{+}} + \log \, p \right) + \dfrac{1}{\mbE \norm{\bY}_1}. 
\ee
\end{lemma}

\llproof \ref{lem:concentration_likelihood}. 
By Proposition \ref{prop:inference},
\beno 
\ell(\nat;\bx,\by)
\= \log \, \mbP_{\nat}(\bX = \bx \,|\, \bY = \by)
+ \log \, g(\by).  
\ee
Thus, 
\be
\label{eq:887}
\nabla_{\nat} \, \ell(\nat;\bx,\by)
\= \nabla_{\nat} \, \log \, \mbP_{\nat}(\bX = \bx \,|\, \bY = \by)
 + \nabla_{\nat} \, \log \, g(\by) 
\= s(\bx) - \mbE_{\nat} \, s(\bX),  
\ee
as $g(\by) = \mbP_{\nat}(\bY = \by)$ is assumed to not be a function of $\nat$. 
The last equality in \eqref{eq:887} follows from 
Lemma \ref{lem:s_hetero},
which showed that 
$\mbP_{\nat}(\bX = \bx \,|\, \bY = \by)$ is a minimal exponential family
with sufficient statistic vector $\bs(\bx)$ defined in Lemma \ref{lem:s_hetero} 
and natural parameter vector $\nat \in \mbR^p$,
inserting the familiar form of the score equation of an exponential family with respect to the natural parameter vector 
\citep[e.g., Proposition 3.10, p. 32,][]{Su19}.
Thus, 
\beno
- (\GradL   - \mbE \, \GradL ) 
\= s(\bX) - \mbE_{\nat} \, s(\bX) 
- \mbE \left[ s(\bX) - \mbE_{\nat} \, s(\bX)\right] \= s(\bX) - \mbE \, s(\bX). 
\ee
Let $t > 0$ and $\nat \in \mbR^p$ be arbitrary and fixed 
and define  
$\mD_{\infty}(\nat,t)$ to be the event that \linebreak 
$\norm{\GradL - \mbE \, \GradL}_{\infty} = \norm{\bs(\bX)-\mbE  \, \bs(\bX)}_{\infty} \geq t$.  
By a union bound, 
\beno
\mbP(\norm{\bs(\bX)-\mbE \, \bs(\bX)}_{\infty} \geq t)
\= \mbP\left( \, \bigcup\limits_{l=1}^{p} \, \Big[|s_l(\bX)-\mbE \, s_l(\bX)| \geq t \Big]\right) \s \\ 
&\leq & \dsum_{l=1}^{p} \, \mbP\left(|s_l(\bX)-\mbE \, s_l(\bX)| \geq t\right). 
\ee
%The union bound reduces the infinity norm to the absolute value as the $s_l(\bX)$ is single dimension. 
For each $l \in \{1, \ldots, p\}$,
define $\mD_{l}(\nat, t)$ to be the event $|s_l(\bX) - \mbE \, s_l(\bX)| \geq t$.
Let $\epsilon > 0$ and define  
$\mE(\epsilon)$ to be the event that $|\norm{\bY}_1 - \mbE \norm{\bY}_1| \le \epsilon$,
i.e.,
\beno
\mE(\epsilon)
\= \left\{ \by \in \mbY \,:\, \left|\norm{\by}_1 - \mbE \norm{\bY}_1\right| \leq \epsilon \right\}. 
\ee 
We assume that $\epsilon > 0$ is chosen so that $\mE(\epsilon)$ is not empty, 
which implies $\mbP(\mE(\epsilon)) > 0$ 
as $g(\by)$ is assumed to be strictly positive on $\mbY$.  
By the law of total probability, 
\be
\label{divide and conquer}
\mbP\left(\mD_{\infty}(\nat,t) \right) 
&=& \mbP\left( \mD_{\infty}(\nat, t) \,|\, \mE(\epsilon) \right)\,
\mbP\left(\mE(\epsilon) \right) + 
\mbP\left( \mD_{\infty}(\nat,t) \,|\, \mE(\epsilon)^c \right) \, \mbP\left( \mE(\epsilon)^c \right) \s \\
&\leq& \mbP\left( \mD_{\infty}(\nat,t) \,|\, \mE(\epsilon) \right) + \mbP\left( \mE(\epsilon)^c \right) \s \\
&\leq&  \dsum_{l = 1}^p\,\mbP\left( \mD_{l}(\nat, t) \,|\, \mE(\epsilon) \right) + \mbP\left( \mE(\epsilon)^c \right).
\ee
Note that we have not necessarily guaranteed that $\mbP\left( \mE(\epsilon)^c \right) > 0$.  
However, 
if $\mbP\left( \mE(\epsilon)^c \right) = 0$ the non-conditional form of the law of total probability would yield the bound   
\beno
\mbP\left(\mD_{\infty}(\nat,t) \right)
&\leq& \dsum_{l = 1}^p\,\mbP\left( \mD_{l}(\nat, t) \,|\, \mE(\epsilon) \right),
\ee
which is strictly sharper than the bound we give in \eqref{divide and conquer}. 
We will use a divide and conquer strategy to bound each probability in \eqref{divide and conquer} in turn.  
The form of \eqref{general model} implies, 
through factorization principles,
that the dyad-based vectors $\bX_{i,j}$ ($\{i,j\} \subset \mN$) are conditionally independent given $\bY$ 
\citep[e.g.,][p. 11--13]{graphical_model_handbook}. 
Hence,
using Lemma \ref{lem:s_hetero},  
the components of the sufficient statistic vector decompose into the sum  
\beno
s_l(\bX) 
\= \dsum_{\{i,j\} \subset \mN} \, s_{l,i,j}(\bX_{i,j}),
&& l \in \{1, \ldots, p\},
\ee
so that the components of $\bs(\bX)$ are sums of bounded 
conditionally independent random variables given $\bY$.  
Using the forms for $s_l(\bX)$ and $ s_{l,i,j}(\bX_{i,j})$ outlined in Lemma \ref{lem:s_hetero},
we have $0 \le s_{l,i,j}(\bX_{i,j}) \le Y_{i,j}$ $\mbP$-almost surely,
because $s_{l,i,j}(\bX_{i,j}) \in \{0, 1\}$ and $s_{l,i,j}(\bX_{i,j}) = 0$ if $Y_{i,j} = 0$ $\mbP$-almost surely.   
We may then apply  Hoeffding's inequality to obtain 
\be 
\label{eq:hoef_lik}
\mbP\left( \mD_{l}(\nat,t) \given \bY = \by \right)
&\leq& 2 \exp\left(- \dfrac{2 \, t^2}{\norm{\by}_1} \right),
\ee
where the denominator follows because $\sum_{\{i,j\} \subset \mN} \, y_{i,j}^2 = \norm{\by}_1$. 
Using the law of total probability, 
we bound $\mbP\left( \mD_{l}(\nat, t) \given \mE(\epsilon) \right)$ as follows: 
\be
\label{divide_conquer}
\mbP\left( \mD_{l}(\nat, t) \,|\, \mE(\epsilon) \right) 
&=& \dsum_{\by \in \mbY} \, \mbP\left( \mD_{l}(\nat,t) \cap [\bY = \by] \,|\, \mE(\epsilon) \right)   \s \\
\= \dsum_{\by \in \mE(\epsilon)} \, \mbP\left( \mD_{l}(\nat,t) \cap [\bY = \by] \,|\, \mE(\epsilon) \right)   \s \\
\= \dsum_{\by \in \mE(\epsilon)} \, \mbP\left( \mD_{l}(\nat,t) \,|\, [\bY = \by] \cap \mE(\epsilon)\right) \, 
\mbP(\bY = \by \,|\, \mE(\epsilon)) \s \\ 
\= \dsum_{\by \in \mE(\epsilon)} \, \mbP(\mD_{l}(\nat,t) \,|\, \bY = \by) \, 
\dfrac{\mbP(\bY = \by)}{\mbP(\mE(\epsilon))},
\ee
noting that $[\bY = \by] \cap \mE(\epsilon) = [\bY = \by]$ whenever $\by \in \mE(\epsilon)$
and in the case when $\by \not\in \mE(\epsilon)$,
the intersection is empty,
implying 
\beno
\mbP(\bY = \by \,|\, \mE(\epsilon))
\= \dfrac{\mbP([\bY = \by] \cap \mE(\epsilon))}{\mbP(\mE(\epsilon))}
\= \begin{cases} 
\dfrac{\mbP(\bY = \by)}{\mbP(\mE(\epsilon))} & \by \in \mE(\epsilon) \\ 
0 & \by \not\in \mE(\epsilon)
\end{cases}. 
\ee
We now bound \eqref{divide_conquer} using the bound in \eqref{eq:hoef_lik}:  
\beno
\dsum_{\by \in \mE(\epsilon)} \, \mbP(\mD_{l}(\nat,t) \,|\, \bY = \by) \,
\dfrac{\mbP(\bY = \by)}{\mbP(\mE(\epsilon))}  
&\leq& \dsum_{\by \in \mE(\epsilon)} \, 2 \, \exp\left(-\dfrac{2 \, t^2}{\norm{\by}_1} \right) \, 
\dfrac{\mbP(\bY = \by)}{\mbP(\mE(\epsilon))} \s\\
&\leq& 2 \, \exp\left(-\dfrac{2 \, t^2}{\mbE \, \norm{\bY}_1 + \epsilon}\right) \, 
\dsum_{\by \in \mE(\epsilon)} \, \dfrac{\mbP(\bY = \by)}{\mbP(\mE(\epsilon))} \s\\ 
\= 2 \, \exp\left(-\dfrac{2 \, t^2}{\mbE \, \norm{\bY}_1 + \epsilon}\right),
\ee
showing 
\beno
\mbP\left( \mD_{l}(\nat, t) \,|\, \mE(\epsilon) \right)
&\leq& 2 \, \exp\left(-\dfrac{2 \, t^2}{\mbE \, \norm{\bY}_1 + \epsilon}\right). 
\ee
The replacement of $\norm{\by}_1$ by $\mbE\norm{\bY}_1 + \epsilon$ follows because 
$\norm{\by}_1 \le \mbE\norm{\bY}_1 + \epsilon$ for $\by \in \mE(\epsilon)$, 
resulting in the upper bound above.  
We bound the second term in the inequality \eqref{divide and conquer} 
using Chebyshev's inequality:
\beno
\mbP(\mE(\epsilon)^c) 
\= \mbP(\left| \norm{\bY}_1 - \mbE \, \norm{\bY}_1 \right| > \epsilon)
&\leq& \mbP(\left| \norm{\bY}_1 - \mbE \, \norm{\bY}_1 \right| \geq \epsilon)
&\leq&\dfrac{\var(\norm{\bY}_1)}{\epsilon^2}.
\ee
We bound the variance $\var(\norm{\bY}_1)$ as follows:
\beno
\var(\norm{\bY}_1)
\= \dsum_{\{i,j\} \subset \mN} \, \var \, Y_{i,j} 
+ 2 \, \dsum_{\{i,j\} \prec \{v,w\} \subset \mN} \, \cov(Y_{i,j}, \, Y_{v,w})\s\\
&\leq& \mbE \, \norm{\bY}_1 + 2 \, \dsum_{\{i,j\} \prec \{v,w\} \subset \mN} \, \cov(Y_{i,j}, \, Y_{v,w}),  
\ee
noting $Y_{i,j} \in \{0,1\}$ so that 
$\var \, Y_{i,j} = \mbP(Y_{i,j} = 1) \, \mbP(Y_{i,j} = 0) \leq \mbE \, Y_{i,j}$. 
Hence, 
\be
\label{ineq:var}
\mbP(\mE(\epsilon)^c)  
&\leq& \dfrac{\mbE \, \norm{\bY}_1 + 2 \, \sum_{\{i,j\} \prec \{v,w\} \subset \mN} \, \cov(Y_{i,j}, \, Y_{v,w})}{\epsilon^2} 
\= \dfrac{\mbE \, \norm{\bY}_1 + 2 \, \left[ D_{g} \right]^{+}}{\epsilon^2}.  
\ee 
Taking $\epsilon = \mbE \, \norm{\bY}_1 + 2 \, \left[ D_{g} \right]^{+} > 0$
shows that 
$\mbP(\mE(\epsilon)^c) \leq (\mbE \, \norm{\bY}_1)^{-1}$ and 
\beno
\mbP\left( \mD_{l}(\nat, t) \,|\, \mE(\epsilon) \right)
&\leq& 2 \, \exp\left(-\dfrac{2 \, t^2}{2 (\mbE \, \norm{\bY}_1 + [D_{g}]^{+})} \right) 
\= 2 \, \exp\left(- \dfrac{t^2}{\mbE \, \norm{\bY}_1 + [D_{g}]^{+}} \right). 
\ee
Combining all results shows that 
\beno
\mbP\left(\norm{\GradL - \mbE \, \GradL}_{\infty} \geq t \right)
\;\leq\; 2 \, \exp\left( -\dfrac{t^2}{\mbE \, \norm{\bY}_1 + [D_{g}]^{+}} + \log \, p \right)
+ \dfrac{1}{\mbE \norm{\bY}_1}. 
\ee
As a final matter, 
note that this choice of $\epsilon > 0$ ensures $\mE(\epsilon)$ contains all $\by \in \mbY$ 
with $\norm{\by}_1 \in [0, \, 2 (\mbE \, \norm{\bY} + [D_g]^{+})]$
as the empty graph is an element of $\mbY$ with $0$ edges. 

\qed

\s 

We next prove a related result for gradients of log-pseudolikelihood functions 
of network separable multilayer networks in Lemma \ref{lem:concentration_pl}.  
The proof of Lemma \ref{lem:concentration_pl} essentially follows the same proof of Lemma \ref{lem:concentration_likelihood},
and as a result we do not repeat key arguments, instead opting to only outline the changes in the proof.  


