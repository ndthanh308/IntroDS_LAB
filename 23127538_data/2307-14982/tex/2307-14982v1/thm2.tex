
In the following, 
$\bZ$ will denote a standard multivariate normal random vector,
i.e.,
with mean vector equal to the zero vector and covariance matrix equal to the identity matrix
(each of appropriate dimension), 
and $\Phi$ will denote the corresponding probability measure. 
 

\begin{theorem}
\label{thm2}
Under the assumptions of Theorem \ref{thm1}, 
there exists $N_0 \geq 3$ such that,
for all $N \geq N_0$ and any measurable convex set $\mA \subseteq \mbR^p$, 
the error of the multivariate normal approximation
\beno
\left|\mbP((I(\truth) \, \norm{\bY}_1)^{1/2} \, (\mle - \truth) - \tilde\bR \in \mA) - \Phi(\bZ \in \mA)\right|
\ee
is bounded above by
\beno
\dfrac{83}{\xi_{\epsilon^\star}^{3/2}} \,
\sqrt{\dfrac{p^{7/2}}{\mbE \, \norm{\bY}_1}}
+  \dfrac{4}{\mbE \, \norm{\bY}_1} + \dfrac{8 \, \left[D_{g}\right]^{+}}{\left(\mbE \, \norm{\bY}_1 \right)^2}
\ee
where $\tilde\bR$ satisfies
\beno
\mbP\left(\norm{\tilde\bR}_2 \leq 
\dfrac{3 \, \sqrt{2} \, (1 + [D_{g}]^{+})}{\xi_{\epsilon^\star}^2} \;
\dfrac{p^{5/2} \, \log N}{\sqrt{\mbE \, \norm{\bY}_1}} \right)
%\dfrac{3 \, \sqrt{2} \, (1 + [D_{g}]^{+})}{\xi_{\epsilon^\star}^{5/2}} \,
%\dfrac{p^3\, \log N}{(\mbE \, \norm{\bY}_1)^{3/2}} \right)
&\geq& 1 - \dfrac{7}{\mbE \, \norm{\bY}_1} - \dfrac{8 \, [D_{g}]^{+}}{(\mbE \, \norm{\bY}_1)^2}.
\ee
\end{theorem}

\s


Theorem \ref{thm2} serves as a foundation for establishing the asymptotic normality of maximum likelihood estimators $\mle$
and maximum pseudolikelihood estimators $\mple$, 
noting Theorem \ref{thm1} established conditions under which both $\mle$ and $\mple$ are consistent estimators of 
$\truth$ (with respect to the Euclidean distance metric),
assumptions which are met by Theorem \ref{thm2}.   
If 
\beno
\lim\limits_{N \to \infty} \; 
\left[ \dfrac{83}{\xi_{\epsilon^\star}^{3/2}} \,
\sqrt{\dfrac{p^{7/2}}{\mbE \, \norm{\bY}_1}}
+  \dfrac{4}{\mbE \, \norm{\bY}_1} + \dfrac{8 \, \left[D_{g}\right]^{+}}{\left(\mbE \, \norm{\bY}_1 \right)^2} \right] 
\= 0,  
%\;\,\to\,\; 0
\ee
Theorem \ref{thm2} implies 
$(I(\truth) \, \norm{\bY}_1)^{1/2} \, (\mle - \truth) - \tilde\bR$ will converge
in distribution
to a standard multivariate normal random vector,
as error bound on the multivariate normal approximation will vanish in this case.  
The term $\tilde\bR$ can be viewed as an error term,
resulting from the fact that the normal approximation in Theorem \ref{thm2} is obtained via a multivariate Taylor approximation 
in order to bridge the distributional gap between key statistics which admit forms amenable to existing 
theorems for the normal approximation 
and the parameter vectors of interest,
thus introducing an additional source of error in the normal approximation. 
The same theory may be exported to the case of maximum pseudolikelihood estimators 
by exploiting the consistency of both $\mle$ and $\mple$ (with respect to the Euclidean distance metric)
implied via Theorem \ref{thm1} as the triangle inequality implies  
$\norm{\mle - \mple}_2 
\leq \norm{\mle - \truth}_2 + \norm{\mple - \truth}_2$.  

While involved, 
the above condition for asymptotic multivariate normality essentially places restrictions 
on the dependence induced through the single-layer network $\bY$ 
measured by $[D_{g}]^{+}$,
as well as the smallest eigenvalue of the dyad-based information matrix $\mcI(\nat)$ 
in a neighborhood of the data-generating parameter vector $\truth$ as measured by $\xi_{\epsilon^\star}$, 
and the dimension of the model $p$. 
As a result, 
if the information matrix $\mcI(\nat)$ is nearly singular at $\truth$, 
in which case $\xi_{\epsilon^\star}$ will be small,
the error of the normal approximation will be uniformly larger (all else equal).
Likewise, 
if the edge dependence in $\bY$ is large as measured by $[D_{g}]^{+}$,
we may not have sufficient activated dyads to ensure the error bound is small,
as $\norm{\bY}_1$ may not be tightly concentrated around $\mbE \, \norm{\bY}_1$. 
The dependence of the error approximation on the dimension of the random vector is a known challenge 
in establishing multivariate normality \citep[see, e.g.,][]{Raic19}.  
All quantities which are not explicit constants can increase or decrease with $N$,
with the rates of these increases or decreases having implications for the rate of convergence in distribution. 
Theorem \ref{thm2} demonstrates 
that the allowable scaling for most of quantities is with respect to the expected number of activated dyads 
$\mbE \, \norm{\bY}_1$.

We further examine Theorem \ref{thm2} through an example where  
$\bY$ is a Bernoulli random graph model,
which assumes edge variables are independent Bernoulli random variables with probability $\pi \in (0, 1)$. 
Under this model, 
$[D_{g}]^{+} = 0$ owing to the independence of edge variables
and $\mbE \norm{\bY}_1 = \pi \, \binom{N}{2}$. 
Under this scenario, 
we can show that 
\beno
\left|\mbP((I(\truth) \, \norm{\bY}_1)^{1/2} (\mle - \truth) - \tilde\bR \in \mA) - \Phi(\bZ \in \mA)\right|
&\leq& \dfrac{166}{\sqrt{\pi \, \xi_{\epsilon^\star}^{3}}} \; \dfrac{p^{1.75}}{N} + \dfrac{16}{\pi  N^2}, 
\ee
with the additional bound   
\beno
\mbP\left( \norm{\tilde\bR}_2 \,\leq\, \dfrac{6 \sqrt{2}}{\xi_{\epsilon^\star}^2} \, \dfrac{p^{2.5} \, \log N}{\pi \, N} \right) 
&\geq& 1 - \dfrac{28}{\pi \, N^2}. 
\ee 
If $\xi_{\epsilon^\star}$ and $\pi$ are both bounded away from $0$,
then the error of the normal approximation will convergence to $0$ 
provided $(p^{2.5} \log N) \,/\, N \to 0$ as $N \to \infty$,
which is sufficient to ensure $\norm{\tilde\bR}_2$ converges in probability to $0$. 
Under the fully saturated model specification for \eqref{general model} ($H = K$), 
the Binomial theorem shows that $p = 2^{K} - 1 \leq 2^K$. 
Hence, 
the dimension restriction on $p$ in turn implies a restriction on the allowable rate of growth of the number of layers $K$ with $N$,
where a sufficient condition for $(p^{2.5} \log N) \,/\, N \to 0$ 
is for $K \leq .5 \, \log N$. 
In other words, 
the number of layers $K$ can grow at most logarithmically with $N$ in the fully saturated model. 
In cases when the number of interaction terms included in the cross-layer dependence probability model 
is fixed, 
$K$ may admit a sublinear scaling  with $N$. 





