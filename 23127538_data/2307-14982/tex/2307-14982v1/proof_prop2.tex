
Before we prove Proposition \ref{prop:suff_norm}, we introduce a Lyapunov type bound in Lemma \ref{lem:raic} provided by Theorem 1 of Raic \citep{Raic19}. 

\begin{lemma}
\label{lem:raic}
Consider a sequence of $n \geq 1$ independent random vectors $\bW_i \in \mbR^p$. 
Assume that $\mbE \, \bW_i = \bm{0}_p$ and $\sum_{i=1}^{n} \, \var \, \bW_i = \bI_p$  
where $\bm{0}_p$ is the $p$-dimensional vector of zeros and $\bI_p$ is the $p \times p$ identity matrix.
Define 
\beno
\bS_n 
\= \dsum_{i=1}^{n} \, \bW_i
\ee
and let $\bZ$ be the standard multivariate normal variable, i.e., $\bZ \sim \text{MvtNorm}(\bm{0}_p, \bI_p)$.  
Then,
for all measurable convex sets $\mA \subset \mbR^p$,  
\beno
\left| \mbP(\bS_n \in \mA) - \Phi(\bZ \in \mA) \right|
&\leq& (42 \, p^{1/4} + 16) \, \dsum_{i=1}^{n} \, \mbE \, \norm{\bW_i}_2^3,
\ee 
where $\Phi$ is the standard multivariate normal measure.
\end{lemma}

\s\s

We now turn to proving Proposition \ref{prop:suff_norm}. 

\s

\pproof \ref{prop:suff_norm}. 
By Proposition \ref{prop:inference} and Lemma \ref{lem:s_hetero},   
the conditional distribution of the multilayer network $\bX$ given $\bY$ 
follows an exponential family with sufficient statistic vector that can be decomposed 
into the sum of conditionally independent dyad-based statistics: 
\beno
\bs(\bX) 
\= \dsum_{\{i,j\} \subset \mN} \, \bs_{i,j}(\bX),
\ee
with the precise formula for $\bs_{i,j}(\bX)$ given in Lemma \ref{lem:s_hetero}. 
Define 
\beno
\bS_{\mN} 
&\coloneqq& 
(I(\truth) \, \norm{\bY}_1)^{-1/2} \, (\bs(\bX) - \mbE^{\bY} \bs(\bX)) \s \\ 
\= \dsum_{\{i,j\} \subset \mN} \, (I(\truth) \, \norm{\bY}_1)^{-1/2} \, (\bs_{i,j}(\bX) - \mbE^{\bY} \bs_{i,j}(\bX)), 
\ee
where $I(\truth)$ is the Fisher information matrix of a single dyad $X_{i,j}$ for $\{i,j\} \subset \mN$ 
satisfying 
$Y_{i,j} = 1$ (i.e., the subset of activated dyads) evaluated at $\truth$ 
per Lemma \ref{lem:min-eig} 
and where 
$\mbE^{\bY}$ is the random conditional expectation operator with respect to the distribution of $\bX$ conditional on $\bY$. 
For $\epsilon > 0$ satisfying $\epsilon < \mbE \, \norm{\bY}_1$, define the event $\mE(\epsilon)$ by
\beno
\mE(\epsilon) & \coloneqq &  \left\{\, \by \in \mbY \,:\, 
\norm{\by}_1 \geq \mbE \norm{\bY}_1 - \epsilon \right\}. 
\ee
In words, 
$\mE(\epsilon)$ is the subset of configurations of the single-layer network $\bY$ 
which have number of edges equal to at least the  expected number of activated dyads 
$\mbE \, \norm{\bY}_1$ minus $\epsilon > 0$. 
The restrictions placed on $\epsilon$ ensure that $\mbE \, \norm{\bY}_1 - \epsilon > 0$ 
which implies that $\mE(\epsilon)$ will not contain the empty graph which has no edges
and that $\mE(\epsilon)$ will contain the complete graph with $\binom{N}{2}$ edges 
as $\mbE \, \norm{\bY}_1 < \binom{N}{2}$ (strict inequality following from the fact that 
$g(\by)$, the marginal probability mass function of $\bY$, 
is assumed to be strictly positive on $\mbY$).
Hence, 
$\mbP(\mE(\epsilon)) > 0$ and $\mbP(\mE(\epsilon)^c) > 0$.   
Let $\mA \subset \mbR^p$ be a measurable convex set.  
By the law of total probability and the triangle inequality, 
we have
\be
\label{tri_ineq}
\left|\, \mbP(\bS_\mN \in \mA) - \Phi(\bZ \in \mA) \, \right| 
&\leq&
\left|\mbP(\bS_n \in \mA \, | \, \mE(\epsilon))  - \Phi(\bZ \in \mA)\right| \, \mbP(\mE(\epsilon)) \s \\
 && +  \; \left|\mbP(\bS_n \in \mA \, | \, \mE^c(\epsilon))  - \Phi(\bZ \in \mA)\right| \, \mbP(\mE^c(\epsilon)) \s \\ 
 &\leq & \sup\limits_{\by \in \mE(\epsilon)} \,\left| \, \mbP(\bS_{\mN} \in \mA \, | \, \bY = \by) - \Phi(\bZ \in \mA)\,  \right| \; + \; \mbP(\mE^c(\epsilon)),
\ee
noting $\left|\mbP(\bS_n \in \mA \, | \, \mE^c(\epsilon))  - \Phi(\bZ \in \mA)\right| \leq 1$ 
and $\mbP(\mE(\epsilon)) \leq 1$. 
Taking
\beno
\bW_{i,j} & = & (I(\truth) \, \norm{\bY}_1)^{-1/2} \, (\bs_{i,j}(\bX) - \mbE^{\bY} \, \bs_{i,j}(\bX)),
\ee
we have
\beno
\mbE \, [\bW_{i,j} \, | \, \bY = \by ] \= 0, 
\ee
a result of the tower property of conditional expectation, 
and 
\beno 
\var \, \left[\sum_{\{i,j\} \subset \mN} \, \bW_{i,j}  \, | \, \bY = \by \right] \= \bI_p,  
\ee 
which follows from Lemma \ref{lem:min-eig} which establishes that 
$\var[s_{i,j}(\bX) \,|\, \bY = \by] = I(\truth)$ when $Y_{i,j} = 1$, 
recalling the form of the Fisher information matrix of exponential families to be the 
covariance matrix of the sufficient statistic vector 
\citep[e.g., Proposition 3.10, pp. 32,][]{Su19}, 
and due to the fact that 
$\var[s_{i,j}(\bX) \,|\, \bY = \by] = \bm{0}_{p,p}$ when $Y_{i,j} = 0$. 
Applying Lemma \ref{lem:raic} to the first term of the summation of \eqref{tri_ineq}, for any measurable convex set 
$\mA \subset \mbR^p$,
\beno
\left| \mbP(\bS_{\mN} \in \mA \, | \, \bY = \by) - \Phi(\bZ \in \mA) \right|
&\leq& (42 \, p^{1/4} + 16) \, \dsum_{\{i,j\} \subset \mN} \, \mbE \, \left[\norm{\bW_{i,j}}_2^3 \, | \, \bY =\by \right].
\ee
Given $\bY = \by$, using standard matrix and vector norm inequalities,
%and given $\bY = \by$, using the standard matrix and vector norm inequalities, 
\beno
\norm{\bW_{i,j}}_2
\= \norm{(I(\truth) \, \norm{\by}_1)^{-1/2} \, (\bs_{i,j}(\bX) - \mbE \, \bs_{i,j}(\bX))}_2 \s \\ 
& \leq & \norm{\by}_1^{-1/2} \,\mnorm{I(\truth)^{-1/2}}_2 \, \norm{\bs_{i,j}(\bX) - \mbE \, \bs_{i,j}(\bX)}_2 \s \\ 
& \leq & (\norm{\by}_1 \, \xi_{\epsilon^\star})^{-1/2} \, p^{1/2} \, y_{i,j}, 
\ee
where $\mnorm{\cdot}_2$ denotes the spectral norm of a $p \times p$ matrix and 
\beno
\xi_{\epsilon^\star}
&\coloneqq& \inf\limits_{\nat \in \mB_2(\truth, \epsilon^\star)} \, \lambda_{\min}(I(\nat)),  
\ee
for a given and fixed $\epsilon^\star > 0$, 
as defined in Section \ref{sec3}.
From proofs of Lemma \ref{lem:concentration_likelihood}
and \ref{lem:concentration_pl}, 
\beno
0 &\leq& s_{l,i,j}(\bx) &\leq& 1,
&& \mbox{all } l = 1, \ldots, p, \, \{i,j\} \subset \mN, 
\ee
$\mbP$-almost surely. 
Hence, 
\beno
\mbP(\norm{\bs_{i,j}(\bX) - \mbE^{\bY} \bs_{i,j}(\bX)}_\infty \leq y_{i,j} \,|\, \bY = \by) 
\= 1,
\ee 
implying (conditional on $\bY = \by$) 
\beno
\norm{\bs_{i,j}(\bX) - \mbE^{\bY} \, \bs_{i,j}(\bX)}_2 & \leq & (p \, y_{i,j})^{1/2} 
\= p^{1/2} \, y_{i,j},
\ee
$\mbP$-almost surely. 
As a result,
\beno
\mbE \, \left[\norm{\bW_{i,j}}_2^3 \, | \, \bY =\by \right]
&\leq &  (\norm{\by}_1  \, \xi_{\epsilon^\star})^{-3/2} \, p^{3/2} \, y_{i,j},
\ee
noting that $y_{i,j}^3 = y_{i,j} \in \{0, 1\}$. 
Using the fact that $42 \, p^{1/4} + 16 \leq 58 \, p^{1/4}$ ($p \geq 1$), we have
\beno
(42 \, p^{1/4} + 16) \, \dsum_{\{i,j\} \subset \mN} \,\mbE \, \left[\norm{\bW_{i,j}}_2^3 \, | \, \bY =\by \right]
&\leq& 58 \,  p^{7/4} \, \dsum_{\{i,j\} \subset \mN} \, y_{i,j} \, (\norm{\by}_1 \, \xi_{\epsilon^\star})^{-3/2} \s \\
& = & 58 \, p^{7/4} \,\norm{\by}_1^{-1/2} \,\xi_{\epsilon_\star}^{-3/2} \s \\
& \leq & 58 \, p^{7/4} \,(\mbE \, \norm{\bY}_1 - \epsilon)^{-1/2} \,\xi_{\epsilon_\star}^{-3/2},
\ee
as the conditioning event
$\mE(\epsilon)$ and choice of $\epsilon$
ensure that $\norm{\by}_1 \geq \mbE \norm{\bY}_1 - \epsilon > 0$.
We bound the second term in \eqref{tri_ineq} by Chebyshev's inequality using equation \eqref{ineq:var} in the proof of Lemma \ref{lem:concentration_likelihood}:
\beno
\mbP(\mE^c(\epsilon))  
&\leq& \dfrac{\mbE \, \norm{\bY}_1 + 2 \, \left[ D_{g} \right]^{+}}{\epsilon^2}.  
\ee 
Taking $\epsilon = 2^{-1} \, \mbE \, \norm{\bY}_1 > 0$,
we have  
\beno
\mbP(\mE^c(\epsilon))  
&\leq&  \dfrac{4}{\mbE \, \norm{\bY}_1} + \dfrac{8 \, \left[D_{g}\right]^{+}}{\left(\mbE \, \norm{\bY}_1 \right)^2}.   
\ee
Combining terms, 
we obtain the bound
\beno 
\left| \mbP(\bS_\mN \in \mA) - \Phi(\bZ \in \mA) \right|
&\leq& \dfrac{83}{\xi_{\epsilon^\star}^{3/2}} \, 
\sqrt{\dfrac{p^{7/2}}{\mbE \, \norm{\bY}_1}}
+  \dfrac{4}{\mbE \, \norm{\bY}_1} + \dfrac{8 \, \left[D_{g}\right]^{+}}{\left(\mbE \, \norm{\bY}_1 \right)^2}.
%(\mbE\,\norm{\bY}_1 - \epsilon)^{-1/2}  \,\xi_{\epsilon_\star}^{-3/2} & + & \dfrac{1}{(\mbE \, \norm{\bY}_1)^{1/2}}.
\ee
\qed
\s \s


Note that the asymptotic multivariate normality can be established provided 
\beno
\lim\limits_{N \to \infty} \, \left[
\dfrac{83}{\xi_{\epsilon^\star}^{3/2}} \,
\sqrt{\dfrac{p^{7/2}}{\mbE \, \norm{\bY}_1}}
+  \dfrac{4}{\mbE \, \norm{\bY}_1} + \dfrac{8 \, \left[D_{g}\right]^{+}}{\left(\mbE \, \norm{\bY}_1 \right)^2} 
\right]
\= 0,  
\ee
resulting in following the asymptotic convergence in distribution:  
%We assume $\mbE \, \norm{\bY}_1  > \epsilon$, and when $\mbE \, \norm{\bY}_1$ scales with the number of observations and $\left[ D_{g} \right]^{+} = o(\mbE \, \norm{\bY}_1)$, the vector of sufficient statistics $\bs(\bX)$ of the multilayer network $\bX$ given $\bY$ is asymptotically multivariate normal, and
\beno
\bS_\mN &\overset{D}{\longrightarrow}& \bZ &\sim& \text{MvtNorm}\left(\textbf{0}, \, \bI_p \right). 
\ee
