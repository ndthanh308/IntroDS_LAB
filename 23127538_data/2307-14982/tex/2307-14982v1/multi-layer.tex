\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{bigints}
\usepackage{mathrsfs}
\usepackage{fancybox}
\usepackage{pdfpages}

\usetikzlibrary{arrows}
\usetikzlibrary{positioning}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{appendix}
\usepackage{etoc}
\usepackage[english]{babel}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}

\usepackage{titlesec}

\titleformat*{\section}{\large\bfseries}

\usepackage[pagewise]{lineno}
%\linenumbers

\input{preamble.tex}
\newcommand{\qed}{$\hfill\blacksquare$}

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{1}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
\addtolength{\topmargin}{-1in}%



\begin{document}

\bibliographystyle{plainnat}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if1\blind
{
  \title{\bf Learning cross-layer dependence structure in multilayer networks} 
  \author{
    Jiaheng Li\\ 
    Department of Statistics, Florida State University \\\\ 
    and \\\\
    Jonathan R. Stewart\thanks{This research was supported by the Test Resource Management Center (TRMC) within the Office of the Secretary of Defense (OSD), contract \#FA807518D0002.} \\ 
    Department of Statistics, Florida State University
  }
  \maketitle
} \fi

\if0\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Learning cross-layer dependence structure in multilayer networks}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
Multilayer networks are a network data structure in which elements in a population of interest have multiple modes of interaction or relation,
represented by multiple networks called layers.
We propose a novel class of models for cross-layer dependence in multilayer networks,
aiming to learn how interactions in one or more layers may influence interactions in other layers of the multilayer network,
by developing a class of network separable models which separate the network formation process from the layer formation process.
In our framework,
we are able to extend existing single layer network models to a multilayer network model with cross-layer dependence.
We establish non-asymptotic bounds on the error of estimators
and demonstrate rates of convergence for
both maximum likelihood estimators and maximum pseudolikelihood estimators
in scenarios of increasing parameter dimension.
We additionally establish non-asymptotic error bounds on the multivariate normal approximation and elaborate a method for model selection which controls the false discovery rate.
We conduct simulation studies which demonstrate that our framework and method work well in realistic settings
which might be encountered in applications.
Lastly,
we illustrate the utility of our method through an application to the Lazega lawyers network.
\end{abstract}

\noindent%
{\it Keywords:} multilayer networks, statistical network analysis, social network analysis, network data, Markov random fields, graphical models
\vfill

\newpage
\spacingset{1.8} % DON'T change the spacing!

\hide{
%% Test to check line spacing -- 26 lines of text per page
\newpage
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
{\bf With this spacing we have 26 lines per page.}
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.

The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.

The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.


The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.
The quick brown fox jumped over the lazy dog.

\newpage
}


\section{Introduction} 

\input{introduction_ml.tex}

The rest of the paper is organized as follows. 
Section \ref{sec2} introduces our modeling framework and includes illustrative examples.
Our main consistency results are contained in Section \ref{sec3},
and our multivariate normal approximation theory is presented in Section \ref{sec:normal}. 
We provide simulation results in Section \ref{sec:sim} together with different testing procedures for model selection
which control the false discovery rate. 
An application of our developed framework and methodology is given in Section \ref{sec:app},
with a discussion presented in Section \ref{sec:disc}.  


\section{Modeling cross-layer dependence in multilayer networks} 
\label{sec2}

A multilayer network can be represented as 
a sequence of $1 \leq K < \infty$ random graphs $\bX^{(1)}, \ldots, \bX^{(K)}$
each defined on a common set of $N \geq 3$ nodes, 
which we take without loss to be the set $\mN = \{1,\ldots,N\}$. 
We call the graphs $\bX^{(1)}, \ldots, \bX^{(K)}$ the {\it layers} of the network,
and represent the multilayer network as the quantity $\bX = (\bX^{(1)}, \ldots, \bX^{(K)})$. 

Connections between pairs of nodes $\{i,j\} \subset \mN$ in each layer $k \in \{1, \ldots, K\}$
are modeled by random variables  
\beno
X_{i,j}^{(k)}
\= \begin{cases}
1 & \mbox{nodes } i \mbox{ and } j \mbox{ are connected in layer } k \\
0 & \mbox{otherwise}
\end{cases}. 
\ee
We refer to all connections across the $K$ layers of a pair of nodes $\{i,j\} \subset \mN$ 
as a {\it  dyad} which we denote by  
$\bX_{i,j} = (X_{i,j}^{(1)}, \ldots, X_{i,j}^{(K)})\in \{0,1\}^{K}$. 
A multilayer network can alternatively be represented by a collection of dyads where $\bX = (\bX_{i,j})_{\{i,j\} \subset \mN}$.

For notational ease, 
we will consider undirected multilayer networks, 
which imply that the network layers $\bX^{(1)}, \ldots, \bX^{(K)}$ are undirected random graphs; 
extensions to directed multilayer networks or mixed multilayer networks with both directed and undirected 
layers will typically be straightforward, 
involving only notational adaptations in subscripts in most cases. 
We adopt the usual conventions for undirected networks, 
i.e., 
we assume that $X_{i,j}^{(k)} = X_{j,i}^{(k)}$ (all $\{i,j\} \subset \mN$, $1 \leq k \leq K$) 
and $X_{i,i}^{(k)} = 0$ (all $i \in \mN$, $1 \leq k \leq K$). 
The sample space of each layer $\bX^{(k)}$ is therefore 
the product space $\mbX^{(k)} \coloneqq \{0, 1\}^{\binom{N}{2}}$ ($k = 1, \ldots, K$), 
and the sample space $\mbX$ of $\bX$ is the product space of the sample spaces of the individual layers, 
i.e.,
$\mbX \coloneqq \mbX^{(1)} \times \cdots \times \mbX^{(K)}$. 
The sample space of dyad $\{i,j\} \subset \mN$ is the product space $\mbX_{i,j} \coloneqq \{0,1\}^{K}$. 

%A natural starting point for constructing models of cross-layer dependence is to use Markov random field specifications. 
A challenge in the statistical modeling of network data lies in the fact that 
networks have many distinguishing properties, 
including:  
\ben
\item {\bf Sparsity.} Many real-world networks are sparse, 
in the sense that the expected number of edges in the network grows at a rate slower than $\binom{N}{2}$. 
The phenomena of network sparsity manifests in a variety of different applications, 
usually due to constraints, 
such as time or financial constraints, 
which can limit the number of connections any node can maintain at a given point in time
\citep[][]{KrHaMo11,KrKo14,butts:jms:2018}.
\item {\bf Node heterogeneity.} Different actors in a social network will have different properties,
called node covariates,
which can lead to differing propensities to form edges. 
A key example is assortative and disassortative mixing patterns in networks 
\citep{McSmCo01,KrHaRaHo07},
as well as differences in structural patterns in the network \citep{Albert02, LiXu12}.  
\item {\bf Edge dependence.} In addition to node-based effects that give rise to 
heterogeneity 
in propensities for nodes to form edges, 
scientific and statistical evidence suggests edges are dependent in many applications 
\citep{HpLs72,Fo80,GoKiMo2009,block2015reciprocity},
and modeling single system of multiple binary random variables  without replication is a challenging statistical problem
inherent to many statistical network analysis applications. 
\een
Each of the above gives rise to distinct challenges for modeling network data and performing statistical inference
in statistical network analysis applications,  
and it is not straightforward to construct
models that due justice to each of these and more.  
To address these challenges, 
a plethora of statistical models have been proposed to model network data, 
which for single-layer networks have included  
exponential-families of random graph models 
\citep[e.g.,][]{ergm.book,HpLs81,SnPaRoHa04,ScKrBu17},
stochastic block models
\citep[e.g.,][]{HoLaLe83,ABFX08,RoChYu11},
latent metric space models 
\citep[e.g.,][]{HpRaHm01,TaSuPr13,sewell2015latent},
random dot product graphs \citep[e.g.,][]{Athreya2018,SuTaPr14}, 
exchangeable random graph models \citep[e.g.,][]{CaFo17,CrDe16,CaCaBr16}, 
and more. 
In this work, 
we build on the many classes of network data models  
for single layer networks by establishing a new framework for modeling multilayer networks that is capable of extending 
existing single layer network models to a multilayer network models 
which are capable of modeling cross-layer dependence and interactions.  

\subsection{Network separable models of multilayer networks}  

Multilayer networks are subject to the same forces and phenomena as single layer networks, 
as multiple modes of relation or interaction do not remove 
constraints or properties of nodes which are fundamental to network data applications.  
In order to develop a novel class of models for cross-layer dependence in multilayer networks, 
we extend the broad literature of single-layer network models 
by proposing a class of network separable multilayer networks 
which separates the network formation process from the layer formation process. 
We explain this distinction through the introduction of our modeling framework. 

\input{general_model}
\input{prop_inference}

%We prove Proposition \ref{prop:inference} in Appendix \ref{sec:prop_proof}. 
Proposition \ref{prop:inference} establishes a few key facts for inference of cross-layer dependence structures 
in network separable multilayer networks. 
First, 
we are able to observe $\bY$ through $\bX$,
as given any observation $\bx \in \mbX$ of the multilayer network $\bX$,
$\sepmodel(\bY = \by \,|\, \bX = \bx) = 1$ for one, and only one, $\by \in \mbY$. 
In other words, 
through the observation of $\bx$,
we can infer with probability $1$ the corresponding $\by$
due to the form of \eqref{general model}.  
The significance of this result is that we do not need to treat the basis network $\bY$ as a latent network, 
which would require additional statistical and computational methodology to handle the latent missing data network.  
Second, 
we see that inference for $\truth$ is unaffected by the choice of $g(\by)$; 
although, the statistical guarantees for estimators of $\truth$ will be indirectly influenced by the choice of $g(\by)$,
a point which we discuss in later sections.
Moreover, 
the above choice for $f(\bx, \nat)$
and the functional form of $\sepmodel(\bX = \bx \,|\, \bY = \by)$ derived in Proposition \ref{prop:inference}
establishes that $\log \, \sepmodel(\bX = \bx \,|\, \bY = \by)$
corresponds to the log-likelihood of a minimal exponential family, 
accessing a broad literature of statistical methodology and theory \citep[e.g.,][]{Su19}.
We note that other specifications for $f(\bx, \nat)$ are possible, 
but that Markov random field specifications provide a powerful class of models for dependent data 
\citep[e.g.,][]{WaJo08},
and in the case of the saturated model with maximal interaction term $H = K$, it
completely specifies all possible probabilities of outcomes $\bx_{i,j} \in \{0, 1\}^K$,
presenting a non-parametric model class for network separable multilayer networks. 



\subsection{Example of a multilayer network with pairwise interactions} 
\input{example_ml2}


\section{Estimation of cross-layer dependence structure} 
\label{sec3}
\input{estimation_setup}

\input{min_eig_lemma}

\s 

In classical scenarios with independent and identically distributed observations, 
the expected negative Hessian of the log-likelihood function 
is the Fisher information matrix and 
is expected to scale with the number of observations.  
In such cases, 
standard matrix theory shows the smallest eigenvalue of the expected negative Hessian of the log-likelihood function 
will scale with the number of observations,
provided the smallest eigenvalue of the Fisher information matrix of the population from which 
observations are drawn is bounded below.
With regards to network separable multilayer networks,
Lemma \ref{lem:min-eig} demonstrates such a scaling with respect 
to the expected number of activated dyads $\mbE \, \norm{\bY}_1$,
proxying the effective sample size. 
In similar fashion,  $\mcI(\nat)$ is analogous to the Fisher information of the population distribution
from which observations would be sampled in classical scenarios with independent and identically distributed observations,
and may be regarded as the Fisher information of the population distribution for activated dyads in $\bY$.  
With regards to pseudolikelihood-based estimation,
we have a similar interpretation.  

We next present our theoretical guarantees for maximum likelihood and maximum pseudolikelihood estimators in Theorem \ref{thm1}. 
As we will show in Theorem \ref{thm1}, the choice of $g(\by)$ influences the estimation error though the expected 
number of edges in $\bY$  and through the covariances of edge variables in $\bY$. 
Define $[D_{g}]^{+} \coloneqq \max\{0, \, D_{g}\}$, 
where 
\beno
D_{g}
&\coloneqq& \dsum_{\{i,j\} \prec \{v,w\} \subset \mN} \, \cov(Y_{i,j}, \, Y_{v,w}),
\ee
and where $\{i,j\} \prec \{v,w\}$ implies the sum is taken with respect to the lexicographical ordering of pairs of nodes. 
Let $\epsilon^\star > 0$ be fixed independent of $N$ and $p$ and define 
\beno
\xi_{\epsilon^\star}
\;\coloneqq\; \inf\limits_{\nat \in \mB_2(\truth, \epsilon^\star)} \, \lambda_{\min}(\mcI(\nat)) 
&&\mbox{and}&&
\widetilde\xi_{\epsilon^\star}
\;\coloneqq\; \inf\limits_{\nat \in \mB_2(\truth, \epsilon^\star)} \, \lambda_{\min}(\widetilde\mcI(\nat)),  
\ee
where $\mB_2(\truth, \epsilon^\star) = \{\nat \in \mbR^p : \norm{\truth - \nat}_2 \leq \epsilon^\star\}$. 

\input{thm1}



\section{Error of the normal approximation and model selection} 
\label{sec:normal}


In this section, 
we show that a standardization of the maximum likelihood estimator (MLE) of the data-generating parameter vector $\truth$ 
of increasing dimension is asymptotically multivariate normal,
i.e.,
we demonstrate a non-asymptotic bound on the error of the multivariate normal approximation and exhibit conditions on the 
scaling of relevant model quantities---namely the dimension of the model $p$ 
together with the scaling of the expected number of activated dyads $\mbE \, \norm{\bY}_1$---under which the error bound 
on the multivariate normal approximation vanishes in the limit.  
Leveraging the consistency result in Theorem \ref{thm1}, 
we may additionally exhibit the asymptotic normality of maximum pseudolikelihood estimators (MPLE). 
Based on this result, 
we present a model selection method using multiple hypothesis testing procedures that control the false discovery rate. 
The main result is presented in Theorem \ref{thm2},
the proof of which 
is based on a Taylor expansion of the log-likelihood function  
and through the application of a Lyapunov type bound presented in \citet{Raic19}. 

\input{thm2}
%We prove Theorem \ref{thm2} in Appendix \ref{sec:pf_thm2}.

\subsection{Model selection via univariate testing with FDR control} 

\input{global_test}


\section{Simulation studies}
\label{sec:sim}

We conduct simulation studies to investigate the performance of the maximum pseudolikelihood estimation (MPLE) 
in realistic settings that could be encountered in application in order to study the realized outcomes 
of the theoretical results established in Sections \ref{sec3} and \ref{sec:normal}.  
In section \ref{sec:sim_con}, 
we demonstrate the consistency results of Theorem \ref{thm1} 
for pseudolikelihood estimators $\mple$ in settings of different model-generating parameters and different basis network structures of $\bY$. 
We study the multivariate normal approximation of $\mple$ established by Theorem \ref{thm2} 
(and by additionally leveraging the consistency result of Theorem \ref{thm1}) 
in the simulation study conducted in Section \ref{sec:sim_norm}.  
Lastly, 
we discuss several testing procedures for selecting non-zero effects 
while controlling the false discovery rate (FDR) at a given family-wise significance level $\alpha$. 
 




In all simulation studies, 
we sample network concordant multilayer networks $(\bX,\bY)$ from \eqref{general model} 
with the number of nodes varying from 
$N = 200$ to $1000$ and $K = 3$ layers.
The basis network $\bY$ is generated from three different models: 
the Bernoulli random graph model, 
the stochastic block model, 
and the latent space model. 
The layer mechanism of the multilayer network is given by 
\be
\label{eq:sim_model}
f(\bx, \nat) = \dprod_{\{i,j\} \subset \mN} \, \exp\left( \dsum_{k=1}^3\theta_{k}\,x_{i,j}^{(k)} + \dsum_{\substack{k < l}}^{3}\, \theta_{k,l} \, x_{i,j}^{(k)} \, x_{i,j}^{(l)} \right).
\ee


% Figure environment removed

% Figure environment removed


\subsection{Consistency of MPLE} 
\label{sec:sim_con}
The consistency of the maximum pseudolikelihood estimator $\mple$ is demonstrated through the decay of the relative $\ell_2$-errors between $\mple$ and the data-generating parameter $\truth$. 
We generate $M = 250$ multilayer networks by $M$ different model-generating parameters at five network sizes from $N = 200$ to $1000$. 
For each network size $N$, 
type of basis network $\bY$, 
and replicate, 
we sample a network separable multilayer network $\bX$ from \eqref{general model} using the specification in \eqref{eq:sim_model}
with the data-generating parameter vector $\truth$ populated by randomly selecting each component from the uniform distribution on $(-1,1)$.
We make the exception that the third and the sixth components $\theta_{3}^\star$ and $\theta_{1,3}^\star$ are set to $0$.
In each replicate, 
we compute the maximum pseudolikelihood estimator. 
The results of this simulation study are given in Figure \ref{M_theta_error}, 
which shows the decay of the relative $\ell_2$-errors between $\mple$ and $\truth$ as the network size increases in four different basis network structures. 
The broad selection of model-generating parameter values on different basis network structures verifies that Theorem \ref{thm1} holds in many practical settings. 

The top-left subplot of Figure \ref{M_theta_error} shows a baseline result for a dense Bernoulli basis network with $\mbP(Y_{i,j} = 1) = 0.8$ for all $\{i,j\} \subset \mN$. 
As the network size increases from $200$ to $1000$, the relative $\ell_2$-errors decrease to $0$.  
The performance of MPLE on different structures of the basis network $\bY$ is also studied with results 
being shown in the rest of the subplots of Figure \ref{M_theta_error}.  
Basis networks $\bY$ are generated by a sparse Bernoulli random graph (top-right), by a stochastic block model (SBM, bottom-left), and by a latent space model (LSM, bottom-right). 
In contrast to the baseline result of the dense Bernoulli random graph, 
where the basis network is populated with more dyadic connections owing to the fact that the expected number of activated 
dyads $\mbE \norm{\bY}_1$ is equal to $.8 \, \binom{N}{2}$,  
the three different basis structures possess fewer connections. 
A key example is the sparse Bernoulli basis network
which has a varying density of $\mbP(Y_{i,j} = 1) = 20/N$ for all $\{i,j\} \subset \mN$, 
admitting a reciprocal scaling with the network size $N$ which results in a sparse network with bounded average node degree.  
The SBM generated networks have 5 blocks where the within-block density is $.5$ and the between-block density is $.05$ for all network sizes simulated. 
The LSM generated networks follow the specifications of \citet{HpRaHm01} 
with a fixed density parameter of $.6$. We simulate node positions on the plane in $\mbR^2$, where coordinates of the position of each node are randomly generated from the standard normal distribution. In order for SBM and LSM generated basis networks to have a comparable number of effective sample size, the parameters of the SBM and the LSM are chosen so that the expected number of activated dyads $\mbE \norm{\bY}_1$ in both basis networks is approximately $.24 \, \binom{N}{2}$.




\hide{
The decay rate of the relative $\ell_2$-error as the network size increases can be estimated by the slope of the OLS fitted line in the $\log$-$\log$ plot as shown in Figure \ref{Figure1}(b). The slope of the OLS line is $-1$, indicating a decay rate of order $1/N$, which follows the result in Theorem \ref{thm1}. We report the MPLE $\mple$ averaged from $M = 500$ samples with the selected model-generating parameter $\truth$ at network size 1000 in table \ref{t1}.

\begin{table}[t]
\begin{center}
\caption{\label{t1}Values of the model-generating parameter $\truth$ and mean of MPLE $\mple$ from 500 replications at network size 1000. Standard errors are in the parenthesis.}
\begin{tabular}{| c | c | c | c | c | c | c |} 
\hline
  & $\theta_{1}$&$\theta_{2}$&$\theta_{3}$& $\theta_{1,2}$ & $\theta_{1,3}$ & $\theta_{2,3}$ \\ 
\hline
$\truth$ & $-3$ & $-2$ & $-1$ & $.5$ & $0$ & $0$  \\
\hline
$\mple$ & $-2.999 \, (.02)$ & $-1.998 \, (.02)$ & $-.999 \, (.02)$ & $.499 \, (.02)$ & $-.001 \, (.02)$ & $-.002 \, (.02)$\\
\hline
\end{tabular}
\end{center}
\end{table}

}

% % Figure environment removed



\subsection{Multivariate normality of MPLE and model selection} 
\label{sec:sim_norm}


\begin{table}%[t]
\begin{center}
\caption{\label{fdr} False discovery rates of four procedures for detecting non-zero effects of 6 model-generating parameters 
($\truth_1$, $\truth_2$, $\truth_3$, $\truth_{4}$, $\truth_{5}$, $\truth_{6}$) estimated from 250 multilayer network samples at size 1000 on the dense Bernoulli basis network. All FDRs are smaller than $.05$.} 
\begin{tabular}{| c | c | c | c | c | c | c | } 
\hline
  Procedure & $\truth_1$ & $\truth_2$  & $\truth_3$ & $\truth_4$ & $\truth_5$ & $\truth_6$ \\ 
\hline\hline 
  Bonferroni    & .004  & .002 & .001 & .002 & .001 & .005  \\
\hline
  Benjamini-Hochberg   & .014 & .014 & .014 & .011 & .017 & .020  \\
\hline
 Hochberg &    .012 & .008 & .009 & .008 & .011 & .016 \\
\hline
 Holm  &   .010 & .008 & .006 & .008 & .007 &  .013 \\
\hline
\end{tabular}
\end{center}
\end{table}



As stated in Section \ref{sec:normal} and Theorem \ref{thm2}, 
the distribution of the maximum likelihood estimator $\mle$ 
and the maximum pseudolikelihood estimator $\mple$ 
converge in distribution to a multivariate normal distribution asymptotically. 
In order to study the quality of the normal approximation---especially for univariate testing 
which would be used for false discovery control and model selection---we 
randomly select $6$ of the $250$ data-generating parameter vectors $\truth$
used to study the consistency results of Theorem \ref{thm1} 
in the simulation study conducted in Section \ref{sec:sim_con}.
We then generate $250$ replicates of multilayer network samples by each of these $6$ parameter vectors, using specification \eqref{eq:sim_model} on different basis network structures.
The multivariate normality of $\mple$ passed Zhou-Shao's multivariate normal test \citep{Zhou13}, 
with $p$-values provided in the Appendix \ref{subsec:norm_sim} in the supplement to this paper.  
We visualize the marginal normality of individual component in $\mple$ with a dense Bernoulli basis network
in Figure \ref{qqplot_denseBer},
through Q-Q plots of the simulated maximum pseudolikelihood estimators. 
Univariate tests for normality failed to reject the null hypothesis that
each component of $\mple$ is marginally normal at a significance level of $.05$. 
Additional results studying the multivariate normality of $\mple$ on different basis network structures 
are provided in Appendix \ref{subsec:norm_sim} in the supplement to this paper.
 

 

\hide{
We first demonstrate through Q-Q plots in Figure \ref{Figure3} that each component of the MPLE $\mple$ follows a marginal normal distribution. 
% We constructed 95\% confidence ellipses for bivariate components of $\mple$ in figure \ref{Figure4}. The two axes in each sub-plot of figure \ref{Figure4} correspond to two different components of $\mple$. The red ellipse is the 95\% confidence ellipse generated by the mean and covariance matrix of the MPLEs in the plot under the bivariate normal assumption. All 15 bivariate confidence ellipses cover most of estimates points, suggesting probable bivariate normal distributions of $\mple$. 


\begin{table}%[t]
\begin{center}
\caption{\label{ZS-test} $p$-values of the Zhou-Shao's test for multivariate normality of $\mple$ for 6 model-generating parameters ($\truth_1$, $\truth_2$, $\truth_3$, $\truth_4$, $\truth_5$, $\truth_6$) estimated from 250 network samples at size 1000 on four basis network structures. All $p$-values are larger than .05. \s} 
\begin{tabular}{| c | c | c | c | c | c | c | } 
\hline
 Basis network model  & $\truth_1$ & $\truth_2$  & $\truth_3$ & $\truth_4$ & $\truth_5$ & $\truth_6$ \\ 
\hline
  Dense Bernoulli & .138 & .473 & .053 & .699 & .587 & .983  \\
\hline
 Sparse Bernoulli & .554 & .132 & .232 & .634 & .904 & .373  \\
\hline
 SBM & .65 & .891 & .982 & .975 & .871 & .674 \\
\hline
 LSM  & .859 & .831 & .5 & .227 & .613 & .409  \\
\hline
\end{tabular}
\end{center}
\end{table}
}
\hide{
We additionally performed marginal tests for normality for each component. In each case, the marginal test failed to reject the null hypotheses that $\mple_i$ is marginal normal at the same significance level, for each component $i = 1,\ldots, p$, the result of which is consistent with the $Q$-$Q$ plots. 
}


\hide{
% Figure environment removed


\begin{table}[t]
\begin{center}
\caption{\label{t3} Empirical FDR and power at significance level $.05$ for multiple testing of MPLE $\mple$ 
estimated from $500$ networks of size $1000$.  } 
\begin{tabular}{| c | c | c | c |} 
\hline
 Procedure  & Empirical FDR & FDR 95\% one-sided CI  & Empirical power \\ 
\hline
  Bonfferoni & .138 & 0.473 & .038 & .699 & .587 & .983  \\
\hline
  Benjamini-Hochberg's &  .018 & (0, .134) & 1  \\
\hline
 Hochberg's  & .016 & (0, .127) & 1 \\
\hline
 Holm's  & .013 &  (0, .114) & 1  \\
\hline
\end{tabular}
\end{center}
\end{table}
}

We then implement the multiple testing correction procedures of
Bonferroni, Benjamini-Hochberg, Hochberg, and Holm, for the 6 selected model-generating parameter vectors $\truth$ with 250 replicates to detect components that are significantly different from $0$ 
while controlling the false discovery rate (FDR) at a family-wise significance level of $\alpha = .05$---recall 
the third and the sixth component $\theta_{1,3}^\star$ and $\theta_{3}^\star$ of $\truth$ are set to $0$ 
in each simulation replicate.  
We estimate the FDR of the four procedures by averaging the false discovery proportions from 250 replicates of each of the 6 randomly selected model-generating parameters $\truth$.
We provide the estimated FDRs for $\truth$ on a dense Bernoulli basis network in Table \ref{fdr}. 
In addition, we show the receiver operating characteristic (ROC) curves for $\mple$ estimating the 6 selected model-generating parameters in each of the subplot of Figure \ref{ROC}, on four basis network structures.
Simulation results suggest that the false discovery rate is controlled below the preset threshold $\alpha$. Different model-generating parameter values affect the trade-off between the sensitivity and the specificity of the model selection. In general, multilayer networks with a larger effective sample size lead to a larger area under the ROC curve which offers a tool to choose appropriate correction procedures and thresholds for model selection in different scenarios. 
Additional results on the false discovery rate with different basis network structures are provided in Appendix \ref{more fdr} in the supplement to the paper. 

% Figure environment removed

%\input{sim_expansion.tex}

% Figure environment removed



\section{Application} 
\label{sec:app}

\input{Lazega}

\section{Discussion} 
\label{sec:disc} 

In this work, 
we introduced a flexible class of statistical models for multilayer networks. 
Key to our approach lies in the integrative nature by which we establish our framework, 
extending arbitrary strictly positive probability distributions for single-layer networks 
to multilayer-network models through a network separable framework with Markov random field specifications. 
We established the foundations for statistical inference through consistency and multivariate normality results,
the results of which have been demonstrated in simulation studies and in an application.  
The key assumption to our approach lies in the network separability assumption, 
which necessitates network dyads be conditionally independent given the basis network. 
This assumption may or may not be valid in practice,
which would necessitate the development of generalizations of the framework we established in this work 
through the relaxation of the conditional independence assumption.  
Such relaxations would result in more complex dependence structures,
requiring 
new and careful theoretical treatment in order to establish similar statistical foundations of models 
to the ones we have developed here,  
representing potential avenues for future research. 


\bibliography{base} 


\newpage

\begin{appendices}


\input{supplement.tex}
\end{appendices}

\label{last.page}



\end{document}
