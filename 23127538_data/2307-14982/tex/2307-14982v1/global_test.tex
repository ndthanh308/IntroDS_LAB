Provided with the consistency results and the multivariate normal approximation of $\mle$
through Theorems \ref{thm1} and \ref{thm2},
we outline a procedure for model selection that controls the false discovery rate.
Hotelling's $T$-squared statistic can be used to conduct a global test for 
$H_0: \truth = \bmu$ versus $H_1:\truth \neq \bmu$, where $\bmu \in \mbR^p$ is the value of $\nat$ we want to test \citep[Chapter 5,][]{multivariate_test}.
We will mostly be interested in the case when $\bmu = \bm{0}_p$,
i.e.,
the zero vector of dimension $p$. 

If the global test is rejected, 
or if the global test is not of interest,  
we can perform model selection by leveraging the multivariate normal approximation 
to obtain univariate normal approximation results for the components of $\mle$
and proceed to test each component: $ H_{i,0} : \theta^\star_i=\mu_i$ 
versus $H_{i,1}: \theta^\star_i\neq\mu_i$,
for $i = 1, \ldots p$ and $\mu_i \in \mbR$. 
%\begin{equation*}
%\begin{array}{lllllllllllllllll}
%    H_{i,0} : \theta^\star_i=\mu_i & \text{versus} & H_{i,1}: \theta^\star_i\neq\mu_i,
%&&& \mbox{for } i = 1, \ldots, p,
%& \mbox{with } \mu_i \in \mbR.  
%\end{array}
%\end{equation*}
In general, 
$\mu_i = 0$ will allow us to test whether the estimated effect $\widehat\theta_i$ is present in the model
(i.e., whether $\theta^\star_i \neq 0$).  
One challenge in this approach lies in the fact that the model selection procedure 
is sensitive to multiple testing error. 
We propose to control the multiple testing error by appropriate multiple testing adjustments
by elaborating a model selection algorithm which will control the false discovery rate
in order to accurately learn the cross-layer dependence effects present in the multilayer network,
and in effect learning the cross-layer dependence structure of the multilayer network.   
We provide simulation examples of four different univariate testing procedures including Bonferroni, Benjamini-Hochberg, 
Hochberg, 
and Holm procedures in Section \ref{sec:sim_norm}. 
In simulation studies, 
all four univariate testing procedures exhibit strong statistical power 
for detecting non-zero parameters while controlling the false discovery rate at a preset family-wise significance level. 
As Theorem \ref{thm1} establishes the consistency of both $\mle$ and $\mple$,
the above procedure remains justifiable for performing model selection with maximum pseudolikelihood estimators as well,  
as it is straightforward to prove a corollary to Theorem \ref{thm1} 
which establishes that $\norm{\mle - \mple}_2$ converges in probability to $0$ under the assumptions of Theorem \ref{thm1},
further obtaining convergence in distribution. 





