\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{media/}}     % organize your images and other figures under media/ folder


% \usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
% \usepackage{algorithmic}

\usepackage{graphicx}
\usepackage{textcomp}

%% Personal pakages
\usepackage{ulem}
\usepackage{comment}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{xcolor}

% personal
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}



%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{Heng Zhang and Danilo Vasconcellos Vargas}
% \fancyhead[RE]{Heng Zhang and Danilo Vasconcellos Vargas} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
\title{A Survey on Reservoir Computing and its Interdisciplinary Applications Beyond Traditional Machine Learning
%%%% Cite as
%%%% Update your official citation here when published 
\thanks{\textit{\underline{Citation}}: 
\textbf{Heng Zhang and Danilo Vasconcellos Vargas. DOI:10.1109/ACCESS.2023.3299296}} 
}

\author{
  Heng Zhang, Danilo Vasconcellos Vargas \\
  Department of Information Science and Technology \\
  Kyushu University \\
  Fukuoka, Japan \\
  % Affiliation \\
  % Univ \\
  % City\\
  % \texttt{\{Author1, Author2\}email@email} \\
  rogerzhangheng@gmail.com, vargas@inf.kyushu-u.ac.jp
  %% examples of more authors
  %  \And
  % Danilo Vasconcellos Vargas \\
  % Department of Information Science and Technology \\
  % Kyushu University \\
  % Fukuoka, Japan \\
  % vargas@inf.kyushu-u.ac.jp \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}


\begin{document}
\maketitle


\begin{abstract}
Reservoir computing (RC), first applied to temporal signal processing, is a recurrent neural network in which neurons are randomly connected. Once initialized, the connection strengths remain unchanged. 
Such a simple structure turns RC into a non-linear dynamical system that maps low-dimensional inputs into a high-dimensional space. The model's rich dynamics, linear separability, and memory capacity then enable a simple linear readout to generate adequate responses for various applications.
RC spans areas far beyond machine learning, since it has been shown that the complex dynamics can be realized in various physical hardware implementations and biological devices. This yields greater flexibility and shorter computation time.
Moreover, the neuronal responses triggered by the model's dynamics shed light on understanding brain mechanisms that also exploit similar dynamical processes.
While the literature on RC is vast and fragmented, here we conduct a unified review of RC's recent developments from machine learning to physics, biology, and neuroscience. 
We first review the early RC models, and then survey the state-of-the-art models and their applications. 
%revision_mode_2306 
\textcolor{black}{We further introduce studies on modeling the brain's mechanisms by RC.
% Finally, we give some remarks and discuss the challenges for future research in RC.
Finally, we offer new perspectives on RC development, including reservoir design, coding frameworks unification, physical RC implementations, and interaction between RC, cognitive neuroscience and evolution. 
% We also highlight the critical challenges and bottlenecks that impede the full realization of RC's potential. 
}

\end{abstract}



% keywords can be removed
\keywords{Reservoir computing \and Neural networks \and Recurrent neural networks \and Nonlinear dynamical systems \and Cognitive neuroscience}

\section{Introduction}
\label{sec:introduction}
Artificial neural networks (ANNs) attract attention in the fields of artificial intelligence, neuroscience, computer science, and machine learning. 
These ANNs can be mainly divided into two architectures: (1) feed-forward neural networks (FFNNs) and (2) recurrent neural networks (RNNs) \cite{lecun2015deep}.
%revision_mode_230609 
\textcolor{black}{
In the field of neuroscience, it has been realized that the convergent feed-forward circuit observed in the cerebral cortex of mammals is a method used to encode relations, allowing cognitive objects to be represented through multi-layered feed-forward architectures \cite{singer2021cerebral}.
% Regarding FFNNs, it has been realized in the field of neuroscience that the convergent feed-forward circuit found in the cerebral cortex of mammals is one of the strategies for encoding relations in which cognitive objects can be represented by multi-layered feed-forward architectures \cite{singer2021cerebral}.
}
In machine learning, training FFNNs is a process that usually involves the optimization of a highly non-convex problem using gradient descent based methods to find the optimum. 
One of the biggest advantages of FFNNs is their ability to deal with static (non-temporal) data processing tasks such as image recognition \cite{rawat2017deep}, object detection \cite{zhao2019object} and semantic segmentation \cite{garcia2017review}. 
However, samples are normally independently processed in FFNNs, making it hard to handle temporally correlated events without memory.

On the other hand, RNNs are models where neurons are recurrently coupled with feedback connections. 
The recurrent connections provide rich non-linear dynamics and memory, which are essential for temporal data and sequential processing.
%revision_mode_230609 
\textcolor{black}{
% Although RNNs are known to be universal approximators, they are hard to train.  
However, RNNs can be challenging to train.  
This is mainly because they must deal with vanishing and exploding gradient problems, along with other problems such as longer training time and the need for careful weight initialization \cite{ribeiro2020beyond, hanin2018neural}.}
% Back-propagation-through-time (BPTT) \cite{werbos1990backpropagation} and Long Short-Term Memory (LSTM) \cite{hochreiter1997long} networks are two of the solutions to some problems above. However, the learning difficulty is still existent. % megumi - This is grammatically correct, but I would change it to, "However, the learning difficultly still exists."
Back-propagation-through-time (BPTT) \cite{werbos1990backpropagation} and Long Short-Term Memory (LSTM) \cite{hochreiter1997long} networks are two solutions to some of the problems mentioned above. However, the learning difficulty still exists.

% Figure environment removed 

Reservoir computing (RC) is a bio-inspired, RNN based computational framework mainly originated from two independent models in the early 2000s: 
the Echo State Network (ESN) \cite{jaeger2001echo} and the Liquid State Machine (LSM) \cite{maass2002real}. 
Similar and related concepts include backpropagation-decorrelation (BPDC) learning algorithm \cite{steil2004backpropagation} and the cortico-striatal models realized in prefrontal cortex \cite{dominey1995complex}. 
%revision_mode_230609 
These proposed models were then unified as the RC computational framework \textcolor{black}{by Verstraeten et, al. \cite{verstraeten2007experimental}, and were summarized in two reviews \cite{lukovsevivcius2009reservoir, schrauwen2007overview}. }
% Early research of RC is summarized in \cite{lukovsevivcius2009reservoir, schrauwen2007overview}.

%revision_mode_230609 
A typical RC network includes a \textit{reservoir} and a \textit{readout} layer. 
\textcolor{black}{Specifically, the reservoir is a network with randomly connected neurons. It can generate complex and high-dimensional transient responses to input. }
Such responses are considered as reservoir's states, forming unique trajectories for each input. 
These high-dimensional reservoir's states are then processed with a simple readout layer, generating the output.
During RC training, the weight connections between neurons in the reservoir are \textcolor{black}{usually} fixed to their initial random values (i.e., remain untrained), and only the readout layer is trained by relatively simple learning algorithms \textcolor{black}{(e.g., linear regression)}. 
% A typical RC network includes a reservoir and a readout layer. 
% Specifically, the reservoir (i.e., a network with randomly connected neurons) can generate complex and high-dimensional transient responses to a single input. 
% Such responses are considered as reservoir's states, forming unique trajectories for each input. 
% These high-dimensional reservoir's states, are then processed with a simple readout layer, generating the output.
% During RC training, usually the weight connections between neurons in the reservoir are fixed to their initial random values (i.e., remain untrained), with only the readout layer being trained by relatively simple learning algorithms (e.g., ridge regression). 


% \textbf{RC PROs and widely known}
RC has several advantages, such as a fast training process, simplicity of implementation, reduction of computational cost, and no vanishing or exploding gradient problems, among others. 
Moreover, they do have memory and, for this reason, are capable of tackling temporal and sequential problems.
These advantages have attracted increasing interest in RC research and many of its applications since its conception.
However, due to the impressive wide-range results of gradient descent based neural networks—the deep learning revolution, RC research receded into a niche for a few years after the 2010s.
%revision_mode_230609
% Recently, RC has regained interest as a plausible biological model of neural networks, and its physical implementations are making it increasingly more scalable.
\textcolor{black}{Recently, there has been a resurgence of interest in RC as a compelling biological model of neural networks, driven by its potential for scalability through physical implementations.}
Additionally, problems in deep learning such as adversarial attacks and robustness/adaptiveness issues have also contributed to an increase in interest in alternative paradigms \cite{goodfellow2014explaining, moosavi2017universal, tsipras2018robustness, su2019one, kotyan2020evolving, kotyan2022adversarial}. 

\subsection{Aims}
The complex dynamics in the reservoir indicate that RC is not only just a machine learning framework, but also a concept that highly correlates to physics, biology, and neuroscience.
% Having a complex dynamic in the reservoir, it is shown that the RC model is not only just a machine learning framework, but is highly correlated to physics, biology and neuroscience (see Fig. \ref{fig:RC related fields and tasks}). % megumi - i feel like this sentence is a little confusing/ clunky, maybe ask sensei for his thoughts
In 2019, a review in physical RC (PRC) comprehensively summarizes various physical materials and hardware components that can be used for PRC implementations \cite{tanaka2019recent}. 
In biology and neuroscience, it was reported in 2013 that parts of the brain's mechanisms are similar to those of reservoirs, including mixed selectivity \cite{rigotti2013importance} and neuronal oscillations \cite{singer2021cerebral}.
%revision_mode_230609 
\textcolor{black}{Although valuable surveys exist in specific research branches, there is still a notable gap in the literature regarding understanding the intricate interplay between RC and its connections to physics, biology, and neuroscience. The absence of such a comprehensive review hinders the advancement of our knowledge and the exploration of RC's potential across diverse domains. Addressing this need and providing a holistic overview would significantly contribute to bridging this gap, facilitating further research, and unlocking new insights into the various aspects of RC.}


% megumi - maybe change "is recently reported" to "was recently reported" 

%%%% PRISMA
% In this paper, a systematic literature review (SLR) method based on Preferred Reporting Items for Systematic Reviews and Meta Analysis (PRISMA) \cite{page2021prisma} is performed to provide a systematic review of RC. SLR is a formal method to identify scientific evidence about a
% research topic, which can review state-of-the-art RC literature without bias. Please refer to Section \ref{prisma} for detailed literature selection procedures.

%revision_mode_230609 
\textcolor{black}{
This survey provides a clear exposition of recent developments in RC. 
While the literature on RC is vast and fragmented, we aim to provide a uniform introduction to RC.
We begin in \textbf{Section \ref{Basic framework of RC}} by introducing the fundamental concepts of RC.
In \textbf{Section \ref{Training a RC model}}, we explore different network architectures and optimization techniques that can enhance reservoir performance.
\textbf{Section \ref{Recent approaches in RC}} further highlights the recent trends of RC, facilitated by improved training schemes. Various physical hardware solutions are also reviewed, covering electronic, optical, and biophysical approaches, among others. 
Furthermore, we demonstrate the wide range of applications of RC in \textbf{Section \ref{Recent Applications of Reservoir Computing}}, including practical engineering, natural science, and social/data science, etc. These applications often involve time-dependent data, requiring memory and memory-related processing.
Next, by discussing the cortico-striatal models and coupled oscillator networks in \textbf{Section \ref{RC with brain mechanisms and cognitive science}}, we show that high-dimensional responses triggered by the reservoir's dynamics offer insights into brain mechanisms that also exploit a high-dimensional dynamical process.
Additionally, our contributions to RC reside in \textbf{Section \ref{perspectives}}, where we present fresh perspectives on the development of RC and pinpoint open problems that require further research. 
% We first propose several directions that have not been extensively explored in the past. In the end of the paper, we bring into focus some of the critical challenges that currently slow down the full realization of RC's potential.
}

\begin{comment}
This survey provides a clear exposition of recent developments in RC. 
While the literature on RC is vast and fragmented, we aim to provide a uniform introduction to RC.
% The extensive and fragmented literature on RC makes it challenging for researchers and practitioners to keep up with the latest developments, and our paper serves as a valuable resource for this purpose.
In \textbf{Section \ref{Basic framework of RC}}, we first introduce the fundamental concepts of RC. 
%revision_mode_230609 
\textcolor{black}{
% We then review various new network architectures and optimization techniques that have been proposed to improve the performances of reservoirs in Section \ref{Training a RC model}, where it is shown that a randomly initialized reservoir may not be the optimal solution in many tasks. 
We discuss different network architectures and optimization techniques that have been suggested to enhance the performance of reservoirs in \textbf{Section \ref{Training a RC model}}. }
Moreover, we investigate a wide range of hardware RC implementations in \textbf{Section \ref{Recent approaches in RC}} (e.g., electronic, optics, and biophysics).
With better training schemes and optimized hardware implementations, RC witnesses rapid development and has a wide range of real-world applications, including but not limited to 
% megumi - maybe change to"...RC witnesses rapid development and has A wide RANGE of real-world applications INCLUDING but NOT limited to..."
(1) practical engineering involving digital signal processing and control, robotics, sensors, and communication technologies; (2) natural science, including biomedical signal processing, chemical reactions, and environmental problems; and (3) social and data science, such as network traffic analysis and financial system modelling.
The common point of the above applications, as discussed in \textbf{Section \ref{Recent Applications of Reservoir Computing}}, is that the data are normally time-dependent, which requires memory and memory related processing.
In addition to memory, high-dimensional responses triggered by the reservoir's dynamics also shed light on understanding brain mechanisms that also exploit a high-dimensional dynamical
process.  % megumi - maybe change "which also" to "that also"
We review the two main brain mechanisms in \textbf{Section \ref{RC with brain mechanisms and cognitive science}}, including cortico-striatal models and the coupled oscillator networks.
Finally, we discuss different perspectives on RC developments and open problems in \textbf{Section \ref{perspectives}}.
\end{comment}

%=============
%=====OVERVIEW PIC========
%=============
% % Figure environment removed 


%=============
%=============
%=============

\begin{comment}
\section{Method}
\label{prisma}
% A systematic literature review (SLR) was carried out, following the principles and phases of the PRISMA model \cite{}. Although a meta-analytical study would be the most desirable, the systematic review methodology seems to be the most appropriate in this case due to the heterogeneity of the selected articles.
Systematic literature review (SLR) method is applied for literature search and selection in this paper in order to provide an objective summary and evaluation. A brief workflow of SLR is as follows:
(1) defining research objectives,
(2) establishing a specific method for carrying out the review process,
(3) identifying and screening papers in the target databases.
(4) defining inclusion and exclusion criteria, 
and (5) reading the chosen publications that are relevant to the defined research objectives.

\subsection{Research objectives}
The SLR and the creation of inclusion and exclusion criteria are driven by research objectives. The research objectives of this paper include: 
(1) summary of the early and fundamental works of RC,
(2) review of the training techniques of RC,
(3) review of recent advanced approaches and applications of RC,
and (4) introduction of brain mechanisms and cognitive science related to RC.

\subsection{SLR method}
A commonly-used PRISMA-based SLR method is adopted for literature search and selection in this paper \cite{page2021prisma}. While a meta-analysis would have been preferable, the systematic review method was deemed more appropriate. Therefore, the systematic review method was conducted due to the heterogeneity of the chosen articles, following the appropriate principles and phases. See PRISMA checklist in S\ref{s2}.

% % Figure environment removed 

\subsection{Inclusion and exclusion criteria}
The studies had to meet the following criteria to be included: 
(1) to have baselines or benchmark tests, 
(2) proposes new reservoir computing network structures, 
(3) proposes new hardware or biological device implementations,
(4) to use reservoir computing as new applications
(5) available as a full-text, 
(6) the publisher of the paper exists in either Core database (for conference papers) or Scopus (for journal papers).
(7) published in English.

The exclusion criteria were: 
(1) literature for which complete data and duplicates are not available;
(2) literature that do not include benchmark tests or applications of RC,
(3) literature for which case studies, animal studies, abstracts, etc. are available.
(4) literature that is an older version of another study already included.

\subsection{Procedure}
A search was launched through databases including Core and Scopus from May 1, 2022 to Jan 3, 2023, while an informal search was also carried out on Google Scholar throughout the period. 
The following search criteria were entered into the databases anywhere to the title and keywords, filtered by journal paper, conference paper in the fields of computer science, engineering, mathematics, neuroscience, physics, chemical engineering and chemistry, multidisciplinary, biochemistry, genetics and molecular biology that are written in English. For conference papers, we checked the conference by using Core portal and selected most of the paper from conferences ranked higher than B+. For conciseness, ``reservoir computing" is represented as ``RC": 
``
[Echo state network]
OR
[Liquid state machine]
OR
[FORCE learning AND RC]
OR
[Chaotic dynamical systems]
OR
[Echo state property]
OR
[RC AND training]
OR
[Deep ESNs AND Multiple AND Multilayered reservoir]
OR
[Evolutionary algorithm AND RC]
OR
[Nonlinear AND RC]
OR
[Dynamical Systems AND RC]
OR
[Physical RC]
OR
[Bio AND RC]
OR
[Clustering AND Chunking AND RC]
OR
[Security AND Communications AND RC]
OR
[Chemistry AND Environmental AND RC]
OR
[Reservoir AND Cerebral cortex]
OR
[Neural Oscillations]
OR
[Memory AND RC]
"
.
The process of eliminating non-relevant papers following PRISMA guidelines \cite{page2021prisma} can be seen in the flowchart (Fig. \ref{fig:PRISMA}). 
When the criteria were not clear, the inclusion or exclusion of the article was decided among all two authors. 
% S-XX Table shows the ranking of publisher of the ESN and LSM papers.
S\ref{s1} Table summarizes the publisher that has more than 2 paper selected in this review.



\end{comment}

\section{Basic Framework of RC} \label{Basic framework of RC}
% In this section, all basic and standard frameworks will be reviewed.
%revision_mode_230612 
\textcolor{black}{In this section, we first review the historical developments in RC. 
By tracing its trajectory from initial theoretical foundations to the present state, our emphasis will then be on highlighting significant works that have played a crucial role in shaping the current landscape of RC.
}
\subsection{History of RC Developments}

\textbf{The first prototype of reservoir computing.} 
Perhaps the birth of the RC framework was in the 1980s-90s. % megumi - I'm not sure how it is usually used but it might sound better to say "The birth of THE RC framework", and also change to  "... framework was in the 1980s-90s"
During that period, some researchers were focusing on the characterization of the fast eye movements (i.e., the oculomotor saccade) in the corticostriatal system—the interactions between cortex and basal ganglia \cite{bruce1985primate}. 
In a pioneering work 
%revision_mode_230612 
\textcolor{black}{(1989)}, 
Barone and Joseph \cite{barone1989prefrontal} examined the function of the corticostriatal system by carrying saccade experiments on macaque monkeys. 
%revision_mode_230612 
\textcolor{black}{They found that some neurons have a preferred spatial saccade amplitude and direction, resulting in selective responses to a particular sequential order. 
}
This finding was further characterized as \textit{mixed selectivity} by \cite{rigotti2013importance} in 2013, which is one of the important principles in both RC and cognitive science (see Section \ref{RC with brain mechanisms and cognitive science}).

Taking inspiration from the experiments of the corticostriatal saccade system \cite{barone1989prefrontal,dominey1992cortico}, Dominey \cite{dominey1995complex,dominey1995model} developed the very first prototype of the reservoir network %revision_mode_230612 
\textcolor{black}{in 1995 
(see Section \ref{RC with brain mechanisms and cognitive science} and Fig. \ref{fig:cog_PFC} for detail). }
Specifically, the model was built based on a recurrent prefrontal cortex (PFC) system (the reservoir), and a reward-related learning method in PFC-to-caudate connections (the readout). 
Since they found that the modifications of the recurrent connections are considerably computationally costing, they decided to initialize the PFC layer with a mixture of fixed inhibitory and excitatory recurrent connections (i.e., a reservoir with fixed connections between neurons). % megumi - change "costing" to "costly"
The reservoir was then connected to the caudate or striatum to obtain the readout. 
% This pioneering RC model, as the author claimed, can be seen as a dedicated temporal recurrent network (TRN), which shows the inherent capabilities and sensitivity to temporal and sequential structure by providing a rich spatio-temporal dynamic.

\textbf{Early research and problems identified in general RNNs.} 
%revision_mode_230612 
\textcolor{black}{The main branch of RC was originated} in the fields of temporal and sequential pattern recognition using RNNs. 
In contrast to FFNNs that aim to approximate non-linear input-output \textit{functions}, RNNs are capable of representing
\textit{dynamical systems} and processing sequential inputs with recurrent connections \cite{tanaka2019recent,lukovsevivcius2009reservoir}. 

Early studies of RNNs include a well-known model called Hopfield network \cite{hopfield1982neural} 
%revision_mode_230612 
\textcolor{black}{in the 1980s}. 
The network topologies were specifically formulated with symmetrical weights connections and were trained in unsupervised ways. 
This special type of network normally experiences chaotic or stochastic dynamics with the mathematical background of statistical physics \cite{lukovsevivcius2009reservoir}.  
% another main-stream of RNNs is \textit{gradient descent} based non-linear dynamical systems.
Another type of RNN features a deterministic update dynamics and directed weighted connections. Systems from this type of RNN are usually made of high dimensional hidden states with non-linear dynamics, resulting in a transformation from an input sequence into an output sequence.
% megumi - inconsistent singular/ plural, maybe change "another" to "other" if you are referring to multiple things, or change the sentence to "... another main-stream of RNNs is gradient descent based non-linear dynamical systems" if you are referring to one category of things
Two standard examples are (1) back-propagation-through-time (BPTT) by \cite{werbos1990backpropagation,rumelhart1985learning}; and (2) real-time recurrent learning (RTRL) by \cite{williams1989learning}. 
Even though these learning methods showed great potential in complex sequential processing, they struggled to tackle real-world problems due to the high computational costs and difficulties of training, especially the vanishing and exploding gradient problems that make them hard to capture long-term dependencies \cite{ribeiro2020beyond, hanin2018neural}.
%revision_mode_230612 
\textcolor{black}{In 1997}, 
a well-known architecture, Long Short Term Memory (LSTM) \cite{hochreiter1997long}, was then proposed to address these problems. For more details on the gradient-based RNNs, please refer to an early review \cite{atiya2000new}. 

\textbf{Unification of reservoir computing.} 
%revision_mode_230612 
\textcolor{black}{In 2000}, 
\cite{atiya2000new} proposed a new algorithm based on error gradient approximation, which efficiently reduces the computational complexity and shows faster convergence in recurrent network training. 
This work, referred to Atiya and Parlos Recurrent Learning (APRL) in later literature, identified that an RNN can be divided into two parts: 
the quickly changing output weights, and the slowly adapting hidden weights.
Therefore, APRL is considered the algorithm to bridge between general RNNs and reservoir computing \cite{lukovsevivcius2009reservoir}. Besides, another predecessor of RC, the backpropagation-decorrelation (BPDC) learning algorithm, further simplified APRL and made it an online learning algorithm \cite{steil2004backpropagation}.

%revision_mode_230612 
\textcolor{black}{Later in the early 2000s}, two types of fundamental reservoir computing algorithms were independently invented by Maass et al. \cite{maass2002real} as Liquid State Machine (LSM), and by Jaeger \cite{jaeger2001echo} as Echo State Network (ESN). The two algorithms, as well as other related works such as BPDC and works in neuroscience fields such as Dominey's research \cite{dominey1995complex}, were unified as a computational framework called \textit{reservoir computing} (RC) \cite{verstraeten2007experimental}. 
In this unified framework, the low-dimensional input data is transformed into spatio-temporal patterns in a high-dimensional space by the \textit{reservoir}—an RNN with fixed topologies and unchanged weights. 
The high-dimensional responses generated by the reservoir are then processed by the \textit{readout}—an output layer which can be trained with simple learning algorithms such as linear regression. 
In other words, during training, the values of the weight connections within the reservoir remain unchanged while only the readout weights are trained based on specific tasks. 
Before going deep into recent advances in RC, we will first introduce the basic concepts of ESN and LSM in the following subsections. 
% We then review several temporal recurrent networks and other types of reservoirs to show how those learning approach correlated to reservoir computing.


%%%%==================
%%%%==================
%%%%==================
\subsection{Echo State Networks}
% Figure environment removed 

Echo State Network (ESN) was first proposed by \cite{jaeger2001echo}. This pioneering work is based on the fact that training only the readout layer of an RNN can achieve acceptable performance, if the network has sufficiently rich dynamics. 
%revision_mode_230612 
\textcolor{black}{
ESN is normally implemented with leaky-integrated, non-spiking, discrete-time and continuous-value artificial neurons (see Fig. \ref{fig:ESN archi} the network structure).
To illustrate the technical details, here we use the notations by \cite{lukovsevivcius2012practical}. 
Consider a temporal processing task, where the input signal is $\mathbf{u}(n) \in \mathbb{R}^{N_{u}}$ and the desired target signal is $\mathbf{y}^{target}(n) \in \mathbb{R}^{N_{y}}$, given $n=1,...,T$ with $T$ being the total number of discrete data points. 
The goal is to generate an output signal $\mathbf{y}(n) \in \mathbb{R}^{N_{y}}$ that matches $\mathbf{y}^{target}(n)$ as optimally as possible by minimizing the error between the two signals (e.g., Mean-Square Error, MSE).
The simplified update equations of the reservoir part in ESNs are given by:
}

% \begin{equation}
%     \label{eq1}
%     \mathbf{x}(n)=\mathbf{f}(\mathbf{W}^{in}\mathbf{u}(n)+
%     \mathbf{W}\mathbf{x}(n-1)),
% \end{equation}
%revision_mode_230612 
\textcolor{black}{
\begin{equation}
    \label{eq:esn1}
    \mathbf{\Tilde{x}}(n)=tanh(\mathbf{W}^{in}\mathbf{u}(n)+
    \mathbf{W}\mathbf{x}(n-1)),
\end{equation}
\begin{equation}
    \label{eq:esn2}
    \mathbf{x}(n)=(1-\alpha)\mathbf{x}(n-1) + \alpha\mathbf{\Tilde{x}}(n),
\end{equation}
}

\noindent where 
%revision_mode_230612 
\textcolor{black}{
$\mathbf{\Tilde{x}}(n) \in \mathbb{R}^{N_{x}}$ is the update at time step $n$,
$\mathbf{x}(n) \in \mathbb{R}^{N_{x}}$ is the state vector of the reservoir neurons (also known as the resulting states or the \textit{echo} of its input history \cite{lukovsevivcius2009reservoir}), 
$\mathbf{W}^{in} \in \mathbb{R}^{N_{x} \times N_{u}}$ and $\mathbf{W} \in \mathbb{R}^{N_{x} \times N_{x}}$ are the weight matrices of the input-reservoir connections and the recurrent connections inside the reservoir, respectively. $tanh()$ is the non-linear activation function applied element-wise. $\alpha$ is the leaking rate that mainly controls the speed of the dynamics. 
}

The readout layer is normally linearly defined as:
\begin{equation}
    \label{eq:esn3}
    \mathbf{y}(n)=\mathbf{W}^{out}\mathbf{x}(n),
\end{equation}

\noindent where
%revision_mode_230612 
\textcolor{black}{$\mathbf{y}(n) \in \mathbb{R}^{N_{y}}$ is the output vector and $\mathbf{W}^{out} \in \mathbb{R}^{N_{y} \times N_{x}}$ is the weight matrix of the reservoir-readout connections.
Alternatively, one can also introduce a bias value in both reservoir and readout, as well as integrate the input signal directly to the readout layer. In this case, $\mathbf{u}(n)$ in Eq. \ref{eq:esn1} becomes $[1;\mathbf{u}(n)]$ and $\mathbf{x}(n)$ in Eq. \ref{eq:esn3} becomes $[1;\mathbf{u}(n);\mathbf{x}(n)]$, where $[\cdot;\cdot]$ represents concatenation. A brief training procedure is shown in Alg. \ref{alg1}.
}


%revision_mode_230612 
\textcolor{black}{
\begin{center}
\scalebox{0.95}{
\begin{minipage}{1\linewidth}
\begin{algorithm}[H]
\caption{Simplified procedure of ESN training.} \label{alg1}
\begin{algorithmic}[1]
\State \textcolor{black}{Initialize the network by generating random $\mathbf{W}^{in}$, $\mathbf{W}$. It is common to use uniform distributed randomization $U(-1,1)$.}
\State \textcolor{black}{Run the model with input signal $\mathbf{u}(n)$, $n=1,...T$, it will generate the same length of the reservoir states $\mathbf{x}(n)$ by Eq. \ref{eq:esn1}-\ref{eq:esn2}.}
\State \textcolor{black}{Collect all $\mathbf{x}(n)$, then calculate and get the output signal $\mathbf{y}(n)$ by Eq. \ref{eq:esn3}.}
\State \textcolor{black}{Minimize the MSE between $\mathbf{y}(n)$ 
 and $\mathbf{y}^{target}(n)$ using techniques such as linear regression. This should obtain a well-trained $\mathbf{W}^{out}$.}
\State \textcolor{black}{Take unseen data $\mathbf{u}_{test}(n)$ and obtain the predicted and/or generated output $\mathbf{y}_{test}(n)$.}
\end{algorithmic}
\end{algorithm}
\end{minipage}
}
\end{center}
}



During conventional ESNs training, $\mathbf{W}^{in}$ and $\mathbf{W}$ remain unchanged, and only $\mathbf{W}^{out}$ is trained in order to minimize the error between the network output and the target output (teacher signal), usually by using linear regression such as ridge regression \cite{hoerl1970ridge}.
Alternatively, many new proposed learning rules for ESN training exist,
% Alternatively, there exists many new proposed learning rules for ESN training,
including but not limited to online FORCE learning \cite{sussillo2009generating}, weights pre-training \cite{zhong2017genetic}, gradient-based training \cite{thiede2019gradient}, and evolutionary learning \cite{wang2015optimizing}. % megumi - singular/plural inconsistency, could change it to "there EXIST many proposed learning rules", but I think "many new proposed learning rules for ESN training EXIST" would sound even better
For details on training a RC model, please refer to section \ref{Training a RC model}.


\textbf{Echo state property.} 
An essential condition (requisite) that a standard ESN must meet is the echo state property (ESP), which ensures a condition of asymptotic state convergence of the reservoir.
This property is under the influence of both the reservoir and the given input. 
On one hand, ESP is an algebraic property that is controlled by the reservoir's weight matrix $\mathbf{W}$.
% This algebraic property ensures a condition of asymptotic state convergence of the reservoir, which eliminates the effect of initial network condition, and allows the approximation of the target signal to take place \cite{jaeger2001echo} [ESN orig]. 
It has been mathematically analyzed in \cite{lukovsevivcius2009reservoir, yildiz2012re, lukovsevivcius2012practical} that the spectral radius ($SR$, i.e., the maximum eigenvalue of $\mathbf{W}$) smaller than unity ensures ESP in most situations. As a result, many RNN-based RCs in literature consider $SR<1$ as a necessary condition to make the models work (see the introduction section in \cite{schrauwen2007overview}). However, it had been proved in \cite{yildiz2012re} that $SR<1$ is neither sufficient nor necessary for the ESP. The author claim that 
%revision_mode_230609 
\textcolor{black}{
``it is not required to scale the spectral radius below 1, and there is no general benefit in scaling the spectral radius toward the Edge of Chaos''.} The same paper also proposed new sufficient conditions for the ESP. Please refer to \cite{yildiz2012re} for mathematical details.
On the other hand, ESP has been empirically studied in the presence of driving inputs of varied strength % megumi - change to either "ESPs have recently..." or "ESP has recently.."
\cite{gallicchio2018chasing}, without looking at the mathematics. \cite{gallicchio2018chasing} shows that for an input-driven reservoir and a proper input scaling, the actual range of ESP validity (i.e., $SR$), is much wider than what is covered by the above literature conditions.

\textbf{Memory capacity and edge of chaos.}
% \sout{The capacity of keeping the memory of the previous inputs is an important function of the reservoir, since memory is essential to a dynamical system.}
As a special type of RNNs, ESN also has the characteristic of short-term memory.
Analytical results that characterize the dynamical short-term memory capacity of reservoirs were discussed in \cite{lukovsevicius2012reservoir, jaeger2002short}. 
Meanwhile, it can be found in a good deal of literature that reservoirs are claimed to work best when they are tuned to operate at the so-called ``edge of chaos'' \cite{bertschinger2004real, legenstein2007edge}. Here, the edge of chaos refers to a region of parameter settings which makes the dynamical system operates at the boundary between the chaotic and non-chaotic behavior. However, this is also a misnomer, as claimed by Jaeger in \cite{nakajima2021reservoir, yildiz2012re} that the ``edge'' in question here is the edge of the ESP, not the edge of chaos. For a detailed discussion, please refer to \cite{cramer2020control}.

\subsection{Liquid State Machines}
% Figure environment removed 

Liquid State Machine (LSM) was proposed by \cite{maass2002real} from the perspective of computational neuroscience in order to study the brain mechanisms and model the neural microcircuits. % megumi - did you mean "perspective"?
In contrast to ESN that uses non-spiking artificial neurons, LSM is more biologically plausible as it is based on the Spiking Neural Networks (SNNs) with recurrent reservoir structures.
Inside the reservoir, usually a 3D structured and locally connected network of spiking integrate-and-fire (IF) neurons is randomly created and stimulated by external input spike train signals (see Fig. \ref{fig:LSM archi}). Intuitively, the reservoir in LSM is often called liquid,
since they follow a metaphor of excited states as ripples on the surface of a pool of water \cite{maass2002real}.
%revision_mode_230609 
\textcolor{black}{
Similar to ESNs in its form, here we use $\mathbf{u}(t)$, $\mathbf{x}(t)$ and $\mathbf{y}(t)$ to represent the input, reservoir state and output, respectively.
The reservoir dynamic of LSM is given by}:

\begin{equation}
    \label{eq3}
    \mathbf{x^M}(t)=\mathbf{L^M u}(t),
\end{equation}

\noindent where $t$ represents continuous time, $\mathbf{x^M}$ is the reservoir state, $\mathbf{u}$ represents the input spikes, and $\mathbf{L^M}$ is the \textit{liquid filter} for input-reservoir state transformation. The readout is given by:

\begin{equation}
    \label{eq4}
    \mathbf{y}(t)=\mathbf{f^M}(\mathbf{x^M}(t)),
\end{equation}

\noindent where $\mathbf{y}(t)$ is the output vector and $\mathbf{f^M}$ is a ``memory-less'' \textit{readout map}. The readout here can also be trained using simple algorithms.

\textbf{Separation and approximation properties.} LSMs have two mathematical preconditions, namely separation property (SP) and approximation property (AP). 
These two properties ensure that the network has fading memory (i.e., echo state property in ESN). % megumi - "ensure that the network has"
Specifically, SP addresses the degree of separation between different internal states $\mathbf{x}$ caused by different input $\mathbf{u}$ (condition is met if the \textit{liquid filter} $\mathbf{L^M}$ satisfies the point-wise separation property), 
whereas AP addresses the capability of the readout layer to produce the target outputs given different liquid states $\mathbf{x}$ (condition is met if the \textit{readout map} $\mathbf{f^M}$ satisfies the approximation property). 
For the mathematical basis of SP and AP, please refer to \cite{maass2002real}. Overall, these two properties in LSMs, together with the ESP and memory capacity in ESNs literature, ensure the RNN-based reservoirs function properly.

\subsection{Comparison of ESNs \& LSMs}
ESNs and LSMs are two similar RNNs with reservoir structures. The main difference between them is that LSM used spiking IF neurons, while ESN is based on non-spiking neurons. 
% Specifically, ESNs generally use \textit{tanh} or \textit{sigmoid} as activation function, whereas in LSM LIF neurons are used in general,
% \textcolor{black}{\textbf{[Not just LIF yes, with many transformations.]}}
% Besides, neurons in LSM act as an accumulating memory cell where the value would be added to the neuron itself when updating. Once the threshold is reached, it releases its energy to other neighborhood neurons.
% Typically, two neurons which are connected is usually given with a certain probability [2].
% \sout{Therefore, unlike ESNs which are consists of artificial neurons, LSM is more biologically motivated and plausible.}
This makes LSMs more biologically plausible to be used in investigating biological mechanisms of information processing in the brain.
In terms of model implementation, although both models show noticeable advantages in reducing computation cost and training time, LSM, with its biologically inspired characteristics and spiking implementation, becomes more suitable for new types of hardware
such as neuromorphic chips \cite{zhang2015digital}. Therefore, LSM is reported with several hardware designs and applications.

ESNs show better flexibility in model modifications, as many variants of ESNs were proposed to enhance the network performances (see Section \ref{Training a RC model}). 
% As mentioned earlier, the core of ESNs is a large-scale reservoir, which creates a non-linear dynamical system that maps the input data to a high-dimensional space. 
These modifications are mainly to overcome the disadvantages of conventional ESNs. The first drawback is that the fixed connection weights could limit the performance of ESNs, since they are randomly initialized without the process of tuning or optimizing. % megumi "without tuning or optimizing the process" or "without tuning or the optimizing process" or "without tuning or optimizing processes" -sorry i'm not sure which is best to show the meaning accurately
Moreover, it is shown that the improvement of ESNs performance will reach a saturation as the size of the reservoir increases to a certain amount. This means that only increasing reservoir size may not result in a better performance. 
As a result, researchers have been focusing on constructing multi-reservoir ESNs, such as multi-layered ESNs \cite{zhang2019deep, gallicchio2017deep, long2019evolving} and parallel reservoir computing \cite{alomar2020efficient}. 
Meanwhile, several optimization approaches have been proposed to fine-tune the networks weights and hyperparameters by using either evolutionary algorithms \cite{long2019evolving, chouikhi2017pso} or gradient based optimization techniques \cite{thiede2019gradient}.
Detailed discussions of recent ESN and LSM models are covered in section \ref{Recent approaches in RC}.

%=======================
%=======================

\section{Training a RC Model} \label{Training a RC model}

\begin{table*}[]
\centering
\caption{Various training techniques of RC.}

\renewcommand{\arraystretch}{1.0}
\resizebox{0.9\textwidth}{!}{
\centering
% \resizebox{0.6\textwidth}{!}{%
\begin{tabular}{lllll}
\hline \hline
 & \textbf{Category of techniques}                &  & \textbf{Training approaches}                                                          &  \\ \cline{2-2} \cline{4-4}
 & Classic readout training &  & \begin{tabular}[c]{@{}l@{}}Linear regression\\ Ridge regression \cite{hoerl1970ridge} \end{tabular} &  \\ \cline{2-2} \cline{4-4}
 &
  Online learning techniques &
   &
  \begin{tabular}[c]{@{}l@{}}Least mean squares (LMS) \cite{jaeger2002adaptive, jaeger2004harnessing} \\ Recursive least squares (RLS) \cite{tamura2021transfer} \\ FORCE learning \cite{sussillo2009generating} \\ FORCE variations \cite{triefenbach2010phoneme, sussillo2012transferring, laje2013robust, depasquale2018full, nicola2017supervised, tamura2020two, tamura2021transfer, zheng2020r}\end{tabular} &
   \\ \cline{2-2} \cline{4-4}
 &
  Online gradient based training &
   &
  \begin{tabular}[c]{@{}l@{}}LSM method \cite{jaeger2002adaptive, jaeger2004harnessing} \\ GD-based optimizer \cite{thiede2019gradient} \\ BackPropagation-DeCorrelation (BPDC) \cite{lukovsevivcius2009reservoir} \\ Back-propagation Through Time (BPTT)\\ ACTRNN  \cite{heinrich2020learning}\end{tabular} &
   \\ \cline{2-2} \cline{4-4}
 &
  Evolutionary learning techniques &
   &
  \begin{tabular}[c]{@{}l@{}}Genetic algorithm (GA) \cite{sharma2017optimized} \\ Particle swarm optimization (PSO) \cite{basterrech2014experimental, chouikhi2017pso} \\ Competitive swarm optimizer (CSO) \cite{long2019evolving} \\ Evolino \cite{schmidhuber2007training} \end{tabular} &
   \\ \cline{2-2} \cline{4-4}
 &
  Biologically plausible learning techniques &
   &
  \begin{tabular}[c]{@{}l@{}}Hebbian learning \cite{jaeger2005reservoir} \\ Spike Timing Dependent Plasticity (STDP) \cite{norton2006preparing, jin2016sso, luo2018improving} \\ Intrinsic Plasticity (IP) \cite{steil2007online, schrauwen2008improving, xue2017reservoir, li2011model} \end{tabular} &
   \\ \hline \hline
\end{tabular}
}
\label{table: RC training}
\end{table*}

% \subsection{Overview}
%revision_mode_230609 \textcolor{black}{% RCs have attracted much attention in recent years. 
% Various training and optimizing approaches have been proposed (see Table \ref{table: RC training}). % megumi "RCs have attracted much attention" or "RC has attracted much attention" 
%revision_mode_230609 
\textcolor{black}{
In this section, we will delve into an array of methodologies for training and optimizing RC models (see Table \ref{table: RC training}). Beyond simply training the readout, we will explore diverse techniques that aspire to enhance the construction of reservoirs from various aspects, thereby enabling more effective and efficient models.
}
These include but does not limit to (1) classical readout training such as ridge regression \cite{hoerl1970ridge}; 
(2) online learning such as least mean square method and FORCE learning \cite{sussillo2009generating};
(3) pre-training such as particle swarm optimization \cite{chouikhi2017pso};
(4) online gradient based learning with back-propagation \cite{bellec2019biologically, bellec2020solution};
(5) evolutionary learning such as Evolino \cite{schmidhuber2007training};
% and Stochastic RNN \cite{han2020self}.
and (6) biologically plausible learning techniques such as Hebbian learning \cite{hebb2005organization}.

\subsection{Classical Readout Training}
The original works of ESN and LSM state that the readout of a reservoir with rich dynamics can be trained by using any statistical classification or regression methods \cite{schrauwen2007overview}. 
%revision_mode_230609 
\textcolor{black}{Using ESNs for example, it is recommended to apply simple linear regression technique to single-layer readout. Again, we use the notations by \cite{lukovsevivcius2012practical} for illustration. First, notice that Eq. \ref{eq:esn3} can be rewritten and extended in a matrix form as:
}

%revision_mode_230609 
\textcolor{black}{
\begin{equation}
    \label{eq:esn4}
    \mathbf{Y} = \mathbf{W}^{out}\mathbf{X} \approx \mathbf{Y}^{target},
\end{equation}
}

\noindent where 
%revision_mode_230609 
\textcolor{black}{$\mathbf{Y} \in \mathbb{R}^{N_y \times T}$ stands for the collection of all $\mathbf{y}(n)$. Similar notation goes to $\mathbf{X}$ and $\mathbf{Y}^{target}$. It is clear that $\mathbf{W}^{out}$ needs to be optimized to minimize the difference between $\mathbf{Y}$ and $\mathbf{Y}^{target}$. The most common technique is the ridge regression \cite{hoerl1970ridge}, where $\mathbf{W}^{out}$ is obtained by: % megumi - "rich dynamics" OR "a rich dynamic", and "a simple linear regression technique" OR "simple linear regression techniques"}
}


%revision_mode_230609
\textcolor{black}{
\begin{equation}
    \label{eq:esn5}
    \mathbf{W}^{out} = \mathbf{Y}^{target}\mathbf{X}^{T}{(\mathbf{X}\mathbf{X}^{T} + \beta\mathbf{I})}^{-1},
\end{equation}
}

\noindent where $\mathbf{I}$ is the identity matrix with $\beta$ being the regularization factor. Detailed implementation can be found in a practical guide of ESN training in \cite{lukovsevivcius2012practical}. Once $\mathbf{X}$ is obtained in an off-line way, one can tune $\beta$ to reach the best performance without any model retraining.
Ridge regression often shows sufficiency in many concrete tasks, when the reservoir provides rich non-linear dynamics. 
% megumi - "non-linear dynamics" OR "a non-linear dynamic"
Moreover, it is easy and fast to train, which attracts many researchers coming from non-machine learning backgrounds. 
% It is the simplicity of the training that make RC recognized and applied in various interdisciplinary tasks. 

\subsection{Online Learning Techniques}
\cite{lukovsevivcius2012practical} provides many empirical solutions on how to produce a reservoir with good initialization. However, problems still exist, as one cannot guarantee that the reservoir is always well-initialized. Another effective way to solve this problem is online adaptation. In this manner, a feedback loop between the reservoir and readout is usually introduced. 
% Several online learning methods are reviewed in the following. 

\subsubsection{Least Mean Squares (LMS) and Recursive Least Squares (RLS) Methods} 
Originated in the area of adaptive signal processing, LMS and RLS are the two standard online learning methods for reservoir computing models \cite{farhang2013adaptive}. 
The mathematical description of LMS and RLS are presented in \cite{jaeger2002adaptive, tamura2021transfer}. 
To illustrate, LMS is a gradient-based error minimization method in which an error is exponentially discounted propagating back through time, yet this method might be unstable because of the large eigenvalue spreads of the cross-correlation matrix (i.e., $\mathbf{X}\mathbf{X}^{T}$). 
Moreover, it is reported that LMS struggles to capture the history-dependent temporal data \cite{beer2019one}. Compared to LMS, RLS is more popular due to its robustness/insensitivity to the effect of eigenvalue spreads mentioned above, as well as its faster second-order convergence speed. 
Therefore, RLS method for RC has been widely studied in \cite{nicola2017supervised, depasquale2018full, tamura2021transfer}. 
Although RLS has advantages over LMS, it is more computationally costly with $O(N^2)$ time complexity, while LMS only requires $O(N)$ in most situations \cite{lukovsevicius2012reservoir}, where $N$ is the number of variables. 
Having said that, RLS is employed by FORCE learning, which creates a new branch of reservoir computing with regard to cognitive science and brain mechanisms. 

% Figure environment removed 

\subsubsection{FORCE Learning} 
It has been shown that RNNs often experience spontaneous chaotic activity, and algorithms such as BPTT \cite{rumelhart1985learning} are usually not able to converge if the network exhibits chaotic activity. ESN models address the chaotic activity by ensuring echo state property (ESP), so that the models do not operate in a chaotic manner. 
% Recall here that the echo state memory (ESP) property mentioned in section \ref{Basic framework of RC} is to ensure ESNs do not operate in a chaotic manner. Furthermore, to address the chaotic activity, the author of ESNs proposed initializing the networks in the absence of the input.
% In fact 
Instead of avoiding spontaneous activity like ESNs, \textit{Fisrt-Order Reduced and Controlled Error} (FORCE) learning, which is perhaps the most popular online learning method of RC, reveals that the results are better when the reservoirs exhibit chaotic behavior before training \cite{sussillo2009generating}.  
% This method is inspired by the observation that 
% Specifically, recurrent networks can produce a motor output or respond to a stimulus, while they also show activity that is irregular and exponentially sensitive to initial condition \cite{abarbanel2008estimation}. 
% By modifying synaptic strengths of the reservoir (either internal or external), models trained with FORCE learning show the effectiveness of suppressing autonomous chaotic activity while turns it into a wide variety of desired output patterns.
By modifying the synaptic strengths of the reservoir (either internal or external), models trained with FORCE learning show the effectiveness of suppressing autonomous chaotic activity while turning it into a wide variety of desired output patterns.
%revision_mode_230609 
\textcolor{black}{
Since the original mathematics are quite complex, here we aim to provide a simplified description of the online readout training for completeness \cite{duane2017force}. Please refer to \cite{sussillo2009generating} for details. 
%We slightly modify the notation used in the original work .
% Simply put, the method introduces an additional output feedback $\mathbf{W}^{ofb}$, together with three different reservoir architectures. 
}

%revision_mode_230609 
\textcolor{black}{
Consider the reservoir state as $\mathbf{r}(t)$ (i.e., $\mathbf{x}(n)$ in ESNs), the network output $\mathbf{z}(t)$ (i.e., $\mathbf{y}(n)$ in ESNs) is defined as:
}

%revision_mode_230609
\textcolor{black}{
\begin{equation}
    \label{eq:force1}
    \mathbf{z}(t) = \mathbf{w}^{T}\mathbf{r}(t),
\end{equation}
}

\noindent where 
%revision_mode_230609
\textcolor{black}{$\mathbf{w}$ is the weights connecting reservoir and readout. Note that here the output dimension is restricted to one, while it can be easily generalized to multidimensional. Training of $\mathbf{w}$ happens at every time interval $\Delta t$. Before updating at time $t$, the error is denoted by:
}

%revision_mode_230609
\textcolor{black}{
\begin{equation}
    \label{eq:force2}
    {e}_{-}(t) = \mathbf{w}^{T}(t-\Delta t)\mathbf{r}(t) - f(t),
\end{equation}
}

\noindent where 
%revision_mode_230609
\textcolor{black}{$f(t)$ is the predefined target function (i.e., $\mathbf{y}^{target}(n)$ in ESNs). The FORCE algorithm uses a modified RLS method to update the weights by:
}

%revision_mode_230609
\textcolor{black}{
\begin{equation}
    \label{eq:force3}
    \mathbf{w}(t) = \mathbf{w}(t-\Delta t) - {e}_{-}(t)\mathbf{P}(t)\mathbf{r}(t),
\end{equation}
}

\noindent where 
%revision_mode_230609
\textcolor{black}{$\mathbf{P}(t)$ is a square matrix that is updated at the same time as the weights according to
}

%revision_mode_230609
\textcolor{black}{
\begin{equation}
    \label{eq:force4}
    \mathbf{P}(t) = \mathbf{P}(t-\Delta t) - \frac{\mathbf{P}(t-\Delta t)\mathbf{r}(t)\mathbf{r}^{T}(t)\mathbf{P}(t-\Delta t)}{1+\mathbf{r}^{T}(t)\mathbf{P}(t-\Delta t)\mathbf{r}(t)},
\end{equation}
}

\noindent and
%revision_mode_230609
\textcolor{black}{is initialized as:
}

%revision_mode_230609
\textcolor{black}{
\begin{equation}
    \label{eq:force5}
    \mathbf{P}(0) = \frac{\mathbf{I}}{\alpha},
\end{equation}
}

\noindent where 
%revision_mode_230609
\textcolor{black}{$\mathbf{I}$ is the identity matrix with $\alpha$ as constant. After training, the error becomes
}

%revision_mode_230609
\textcolor{black}{
\begin{equation}
    \label{eq:force6}
    {e}_{+}(t) = {e}_{-}(t)(1-\mathbf{r}^{T}(t)\mathbf{P}(t)\mathbf{r}(t)).
\end{equation}
}

\noindent Finally, %revision_mode_230609
\textcolor{black}{the training will end when it reaches
}

%revision_mode_230609
\textcolor{black}{
\begin{equation}
    \label{eq:force7}
     \frac{{e}_{+}(t)}{{e}_{-}(t)} \approx 1.
\end{equation}
}

%revision_mode_230609
\textcolor{black}{The above modified RLS method is} applied to suppress the output errors and frequently adapt the weight matrices in the reservoir or readout (see Fig. \ref{fig:FORCE}). 
This makes FORCE learning disparate from other traditional iterative training schemes—the errors
%revision_mode_230609
\textcolor{black}{in FORCE learning}
are always small during training, even at the beginning, suggesting that the aim is not to reduce errors but rather to keep the errors small. When training is done, the network will autonomously generate the desired output. 
As the author claimed, FORCE learning helps to construct machine learning based RNNs that ``generate complex and controllable patterns of activity either in the absence of or in response to input''. 
% These findings make FORCE learning a bridge between computational and biological neuroscience, as it can be viewed as 
% % (1) a training-induced modification model or (2) a functioning circuit models building method \cite{sussillo2009generating}.
% (1) a reservoir training method or (2) a functional model of biological neural circuits \cite{sussillo2009generating}.
%revision_mode_2306
\textcolor{black}{It provides an interesting link between computational and biological neuroscience. In short, FORCE learning can be seen as a useful tool for optimizing RC, and simultaneously, it presents a potential model that could help understand biological neural circuits \cite{sussillo2009generating}.
}

% Figure environment removed 

\subsubsection{FORCE Learning Variations and Implementations} 
% \textit{Disadvantages first. NEED REVISION. Copied From [full-FORCE 2018].} 
One of the disadvantages of FORCE learning is that the trained network is too complex to analyze the neuron activities. Also, for complex real-world problems such as speech recognition, networks trained by FORCE require many more units to match the performance of gradient based networks, as reported in  \cite{triefenbach2010phoneme}.
% Although the FORCE approach greatly expands the capabilities of trained recurrent networks, it does not take advantage of the full recurrent connectivity because of the restrictions on the rank and form of the modifications it implements. Some studies have found that networks trained by FORCE to perform complex “real-world” problems such as speech recognition require many more units to match the performance of networks trained by gradient based methods \cite{triefenbach2010phoneme} [12]. In addition, because of the reliance of FORCE on random connectivity, the activity of the resulting trained networks can be overly complex compared, for example, to experimental recordings \cite{sussillo2015neural} [13].

Therefore, some studies have been published aiming at improving FORCE learning,
% . It is worth noting that these algorithms are very interdisciplinary, 
ranging from neuroscience (e.g., spiking networks) to physical and hardware implementations.  % megumi - this phrasing isn't quite grammatically correct, but I'm not sure about the best way to change it. the problem is that "spiking networks" is out of place. if spiking networks is related to neuroscience, maybe change to "ranging from neuroscience (spiking networks)", if it is different, use "ranging from neuroscience to spiking networks to physical..."
%revision_mode_2306
\textcolor{black}{An ``extended'' FORCE \cite{sussillo2012transferring}, for example, was proposed for more general internal learning by using the desired output to generate targets for every internal neuron in the network. 
}
This so-called ``target-generating'' network is then improved by \cite{laje2013robust}, in which the reservoir network can preserve time information and generate complex and high-dimensional trajectories even under high levels of noise (see Fig. \ref{fig:laje2013robust}).  
%revision_mode_2306
\textcolor{black}{To this end, the FORCE variations are still infeasible to be implemented as a spiking network. 
% it is a non-spiking neuron model that cannot be implemented as a spiking network. Inspired by the previous work,
Later, \cite{depasquale2018full} proposed a full-FORCE algorithm. 
Compared with FORCE learning, it requires fewer neurons, achieves significantly better performance in noisy environments, and can also be applied to SNNs (see implementation in \cite{nicola2017supervised}). 
For hands-on implementation, the full-FORCE has been realized using a Python spiking network framework called Nengo \cite{bekolay2014nengo}.}
Other recent improvements of FORCE learning include (1) Two-Step FORCE that converges faster than the original work \cite{tamura2020two}; 
(2) Transfer-FORCE learning which takes the advantages of both LMS and RLS methods for better learning performance \cite{tamura2021transfer}; (3) R-FORCE that aims to model multidimensional sequences \cite{zheng2020r}. 
%revision_mode_2306
\textcolor{black}{
Very recently, \cite{liu2022tension} built an object-oriented, open-source Python package that implements a TensorFlow / Keras API for FORCE.
}


\subsection{Online Gradient Based Training}
Reservoir based on non-spiking artificial neurons (e.g., ESNs) can be trained by using gradient descent (GD) approaches. The LMS method mentioned earlier is one of the candidates which is first proposed in \cite{jaeger2004harnessing}, yet using such approach shows poor stability even when trying to stabilize the network by adding noise. \cite{thiede2019gradient} proposed a more stable version of GD-based reservoir to optimize four hyperparameters: the input scaling, spectral radius, leaking rate, and regularization parameter.
% \textbf{BackPropagation-DeCorrelation.} 
Besides, BackPropagation-DeCorrelation (BPDC) algorithm is another powerful method for online training of single-layer readouts with feedback connected back to reservoirs. This algorithm is robust to the random initialization of the reservoir weights, and it is also capable of tracking quickly changing signals.
Detailed discussions of BPDC are presented in a survey \cite{lukovsevivcius2009reservoir}. 
Meanwhile, the classical Back-propagation Through Time (BPTT) approach for RNN training can also be applied to RC model. It is worth noting that a network architecture called Adaptive Continuous Time Recurrent Neural Network (ACTRNN) \cite{heinrich2020learning} shows some similarity to GD-based RC. 
%revision_mode_2306
\textcolor{black}{Please refer to Section \ref{RC with brain mechanisms and cognitive science} for details.}


\subsection{Evolutionary Learning Techniques}
\textbf{Evolutionary algorithm.} 
One of the disadvantages of the traditional RC models is that the performance is highly reliant on the random initialization of the weights and hyperparameters. 
While the optimal hyperparameters can be found by grid-search techniques, using such techniques to find the optimal weights' initialization is nearly infeasible \cite{lukovsevivcius2012practical}, given a concrete task. 
Therefore, instead of applying online learning rules, another possible direction, taking inspiration from above, is to train (or pre-train) the reservoirs using evolutionary algorithms (EA). When EAs are applied to evolve any type of neural networks (including reservoirs) they usually receive the name of neuroevolution.

Various types of EA can be used to evolve a reservoir, including (1) genetic algorithms (GA) \cite{sharma2017optimized}; (2) particle swarm optimization (PSO) \cite{basterrech2014experimental, chouikhi2017pso} and its variants \cite{long2019evolving}; and (3) artificial bee colony \cite{badem2017new}. 
For example, a GA was applied to a double-reservoir ESN for parameter optimization, yet without optimizing weights of input and the reservoirs \cite{zhong2017genetic}. 
% \textit{NEED MORE PAPERS FROM RELATED WORKS.} 
Inspired by LSTM \cite{hochreiter1997long}, another EA for RC was proposed called
Evolino \cite{schmidhuber2007training}.
Evolino constructs units that are capable of preserving memory for long periods of time, in which the weights of the reservoir are trained using evolutionary methods. 
A performance comparison of several EAs for RC are presented in \cite{ferreira2011comparing}. 


% \textbf{Particle swarm optimization.} 
Particle swarm optimization (PSO), which is an efficient and widely used technique for finding optimal regions on complex spaces, has also applied to reservoir weight optimization. 
The first two attempts of using PSO technique include using a binary PSO to find the optimal reservoir-readout connections \cite{wang2015optimizing}, as well as a supervised PSO algorithm by \cite{basterrech2014experimental} for better initializing the input weights of RC. 
However, only a subset of the weights was tuned in the latter model, due to the high computational cost.
\cite{chouikhi2017pso} further developed the PSO algorithm for RC, where a portion of fixed weights in an ESN is pre-trained via PSO. 
The results show improvements on model generation as well as a faster convergence time, yet the network architecture is rather simple, and some hyperparameters should be selected empirically and carefully. 
The latest version of PSO based RC is the competitive swarm optimizer (CSO) for fault diagnosis problems \cite{long2019evolving}, which is a hybrid evolutionary algorithm combining both a variant of PSO and local search (LS).


\subsection{Biologically Plausible Learning Techniques}
\textbf{Hebbian learning and Spike-timing-dependent plasticity.}
Reservoir computing takes inspiration partially from biological systems. LSM-based reservoirs, for example, are implemented using spiking neurons. This indicates that some biologically plausible adaptation methods can be applied for reservoir training. 
% megumi - should this be "reservoir training"?
Inspired by synaptic plasticity in human brains, the first attempts would be to use Hebbian and anti-Hebbian learning to try to decrease the eigenvalue spread in ESNs but failed \cite{jaeger2005reservoir}.
% , hebb2005organization}.
Later, it is reported in \cite{lukovsevicius2012reservoir} that the reservoir trained by Hebbian learning ``makes neurons prefer inputs that are easy to predict and weaken connections from those that carry more information''.
% the author failed that these learning rules are not sufficient to make significant improvements, since the Hebbian rules of correlation or anti-correlation are too limited to improve the reservoir dynamics.
In terms of LSM, the spike-time-dependent plasticity (STDP), which is based on Hebbian learning and is often integrated with SNNs, 
% megumi - integrated "into" or "with", depending on which is more accurate
is reported to improve the separation property (SP) in some real-world speech data \cite{norton2006preparing}. STDP was further developed to reduce memory storage load to make RC more hardware-friendly \cite{jin2016sso, luo2018improving}.

\textbf{Intrinsic plasticity.} 
Another biologically plausible way of adaptation is based on Intrinsic Plasticity (IP), which is an unsupervised learning rule used for adapting the intrinsic excitability of the reservoir neurons. 
Here, intrinsic excitability refers to a phenomenon called long-term potentiation, in which brief and high-frequent stimulation tends to produce an increased ability to generate spikes \cite{zhang2003other}.
Early research of the integration of RC and IP mainly focuses on reservoir pre-training and global optimizations \cite{steil2007online, schrauwen2008improving, xue2017reservoir}.
In 2019, an IP with a local search scheme was proposed to improve the flexibility of the IP rule by allowing hyperparameters such as learning rate to be different \cite{wang2019echo}.
%revision_mode_2306 
\textcolor{black}{
In 2022, \cite{nakada2022information} applied IP learning to sucessfully tune the parameter in MEMS-based RC.
}
As a side note, experiments on intrinsic plasticity have shown that the output distributions of real biological neurons may have different forms in different brain regions among various species \cite{li2011model}. 

% \textcolor{black}{Q: what is intrinsic excitability? [WILL BE DELETED] In addition to synaptic plasticity, which confers neurons with the ability to modify the strength of individual synapses, nerve cells also possess forms of intrinsic plasticity (changes in intrinsic excitability), which affect largest ensembles of synapses and might affect the whole cell. This form of plasticity might endow neurons with an additional capacity to store information.}

% ip+local Echo state networks regulated by local intrinsic plasticity rules for regression

% \textbf{Other bio-inspired learning. (Under construction)}
% Mentioned the following
% 1. [RCbook Maass L2L p64] Hochreiter etal.(2001) Wang et al. (2018). Perich et al. (2018)
% 2. small-world t
% 3. neural oscilla









%=======================
%=======================

\section{Recent Approaches in RC} \label{Recent approaches in RC}

\subsection{Overview}
RC has witnessed a significant development in recent years. 
On one hand, traditional RC models such as ESN and LSM have been improved by many new proposed models. 
Some of these models are built on top of the original ones to achieve better performances, while several new architectures of RC have been proposed to solve problems with increasing difficulties. 
On the other hand, recent studies has demonstrated that the idea of reservoir (i.e., a dynamical system that can generate high-dimensional and non-linear responses) can be implemented by using various materials, such as electronic devices, physical systems, and biological realizations.

In this section, we aim to cover recent approaches in RC from several perspectives up until 2023, including ESN-based RC, LSM-based RC, dynamical systems, and physical RC.
These approaches are highly interdisciplinary and are usually tested in several benchmark tasks. 
To introduce some, benchmark tasks for pattern classification includes spoken digit recognition \cite{verstraeten2005isolated}, waveform \cite{paquot2012optoelectronic} and handwritten digit image recognition \cite{jalalvand2015real}. Besides, the non-linear Autoregressive Moving Average (NARMA) time series \cite{jaeger2002adaptive} was widely used in time series forecasting, while a channel equalization benchmark \cite{jaeger2004harnessing} was introduced to evaluate the RC performance on adaptive filtering and control. In addition, temporal XOR task \cite{bertschinger2004real} and memory capacity task \cite{jaeger2002short} were also commonly used in studies that focus on system approximation and short-term memory.


% % Figure environment removed 
% \textit{[COPYING FROM PhyRCrv]} Most of RC studies are involved in machine learning applications, such as pattern classification, time series forecasting, pattern generation, adaptive filtering and control, and system approximation. In particular, RC meets the demands for low training cost and real-time processing in these applications. Some benchmark tasks related to these applications are listed in Table below. The input and output information for a RC system are determined depending on the task. In pattern classification tasks, the input is a time series and the output is a discrete value (label) representing a pattern class. More specifically, in a spoken digit recognition task, the input is a sound signal corresponding to one of ten different utterances of the digits from zero to nine and the output is one of the ten digits. The goal of this task is to output the correct digit number from a sound signal of an unknown digit. To remove unnecessary information and noise, the original sound signal is typically transformed into a feature value such as mel-frequency cepstrum coeﬃcient (MFCC) in a preprocessing step and then given to the reservoir. Although RC is suited for temporal pattern recognition, it can be applied to image recognition by transforming an image into a sequence of pixel values. As RC can deal with any sequential data in principle, further expansion of its application fields is widely expected.

\subsection{Recent Trends of ESN-based RC}
%revision_mode_2306 
\textcolor{black}{
ESNs represent one of the foundational RC models. The simplicity of implementation makes them an approachable entry point of RC.
Therefore, improving and extending ESNs is not only a key pursuit within the RC community, but its impacts extend far beyond, proving particularly influential for researchers outside the field, and thus allowing more interdisciplinary research. 
In the following, we review recent trends of ESN-based RC models.
}

\subsubsection{Multiple Reservoirs}
% Figure environment removed 

\par
\textbf{Deep ESN.} Apart from the performance saturation problems, there is another limitation in the conventional ESN, i.e., the single large-scale reservoir is poor in simultaneously dealing with different timescales \cite{chitsazan2019wind}. In this concern, some studies started to investigate multiple timescale dynamics of reservoir structure, as it has been found that stacking recurrent networks with different topologies can generate multiple timescales at different layers \cite{gallicchio2017deep}.

In 2016, \cite{gallicchio2017deep} proposed a deep reservoir computing model to achieve hierarchical timescale representation. This model, called deep Echo State Network (DeepESN), stacks multiple reservoirs one on top of the other, as shown in Fig. \ref{fig:DeepESN}. 
%revision_mode_2306
\textcolor{black}{Mathematically, consider $N$ the number of reservoir layers. The update equations, extended from Eq. \ref{eq:esn1}-\ref{eq:esn2}, is given by:
}

%revision_mode_2306
\textcolor{black}{
\begin{equation}
    \label{eq:deepesn1}
    \mathbf{\Tilde{x}}^{(i)}(n)=tanh(\mathbf{W}^{(i)}_{fwd}\mathbf{x}^{(i-1)}(n)+
        \mathbf{W}^{(i)}_{rec}\mathbf{x}^{(i)}(n-1)),
\end{equation}
}

\textcolor{black}{
\begin{equation}         
\label{eq:deepesn2}
% \begin{split}
% \begin{aligned}[t]
        \mathbf{x}^{(i)}(n) =(1-\alpha^{(i)})\mathbf{x}^{(i)}(n-1) + \\ \alpha^{(i)}\mathbf{\Tilde{x}}^{(i)}(n),
        % \end{split}
% \end{aligned}
\end{equation}
}

\noindent where 
%revision_mode_230609
\textcolor{black}{
$\mathbf{\Tilde{x}}^{(i)}(n)$, $\mathbf{x}^{(i)}(n)$ and $\alpha^{(i)}$ are the update, the reservoir state vector, and the leaking rate at layer $i$, respectively. $\mathbf{W}^{(i)}_{fwd}$ is the weight matrix connecting layers $i$-1 and $i$, and $\mathbf{W}^{(i)}_{rec}$ is the weight matrix of the reservoir at layer $i$, $i=1...N$. The number of input layer is denoted by $i=0$ and $\mathbf{x}^{(0)}(n) = \mathbf{u}(n)$. At each time step $n$, the composition of the states in all the reservoir layers $\mathbf{x}(n)$ is given by: 
}

\textcolor{black}{
\begin{equation}         
\label{eq:deepesn3}
    \mathbf{x}(n) = \langle\mathbf{x}^{(1)}(n), ..., \mathbf{x}^{(N)}(n) \rangle.
\end{equation}
}


Experiments show that the deep RC structure can achieve
(1) multiple timescale representation, where the timescales are ordered along the network's hierarchy;
(2) multiple frequency representation, where progressively higher layers focus on progressively lower frequencies.
Additionally, when there is a perturbation at the input, the effects of this perturbation last longer for higher layers in the stack, and this differentiation is drastically attenuated when input is provided to every layer.
Therefore, having deeper layers while increasing distance from the input is a key architectural factor for obtaining a time-scales separation. 

DeepESN shows potential for designing more efficient RC learning algorithms used for sequential and temporal data processing.
% Multi-Reservoir Echo State Networks with Hodrick–Prescott Filter for nonlinear time-series prediction
% Multi-reservoir echo state networks with sequence resampling for nonlinear time-series prediction
From 2022, A branch of deep ESN is recently studied by the research team of Tanaka et al. modified the deep network architectures were proposed, combining with other techniques such as sequence resampling \cite{li2022multi} in 2022 and Hodrick–Prescott filter \cite{li2023multi} in 2023. These model are claimed to have high prediction performance in time-series prediction tasks with relatively low training cost.

\textbf{Deep Fuzzy ESN.}
In 2019, \cite{zhang2019deep} proposed a novel deep ESN model with fuzzy tuning called Deep Fuzzy ESN (DFESN). Here, two reservoirs are stacked, where the first reservoir is applied for feature extraction and dimensional reduction, and the second one is used for feature reinforcement based on fuzzy clustering. 
In other words, the output of the previous reservoir was extracted as features for the next reservoir input, followed by a feature reinforcing process performed by fuzzy clustering for classification enhancement. 
In DFESN, back propagation is no longer necessary, since the feature reinforcement process can be considered as a layer-wise fuzzy tuning that replaces the back propagation algorithm with lower computational costs.
% \sout{Furthermore, the fuzzy tuning can not only overcome the vanishing gradient problem in traditional deep architectures, but also save computational cost.} 
As claimed by the author, input samples are clustered more easily, thus improving the final classification performance. 
% Nevertheless, one of the problems of DFESN is the more complicated network structures compared with the traditional deep networks.



\subsubsection{ESN with Evolutionary Algorithms}
\textbf{Multi-layered echo state network autoencoder.}
% \textcolor{black}{Autoencoder (AE) is a commonly used feed-forward network where the outputs are ``equal'' to the inputs. Inside each hidden layer, undergoes progressive non-linear transformations are performed to create new data representations from the original ones.}
% Autoencoder (AE) is a commonly used feed-forward network. In each hidden layer, undergoes progressive non-linear transformations are performed to regenerate a new effective data representation from the original ones.
Autoencoder (AE) is a type of common feed-forward network for dimensionality reduction and feature detection, in which non-linear transformations are performed in each hidden layer to regenerate a new effective data representation from the originals.
% megumi did you mean "new data representations" or "a new data representation"
This technique was introduced to RC area by \cite{chouikhi2019bi} as the first recurrent and non-gradient descent-based AE in the literature. In \cite{chouikhi2019bi} an autoencoder was implemented by using multilayered ESN, with a bi-level evolutionary algorithms for optimizing the network architecture and weights. 
Particularly, PSO was applied for the bi-level optimization, where the first level is the architecture determination and the second one is the weights optimization. Classification results on various benchmarks showed that the performance of the evolved model is improved compared with the conventional ESN as well as other CNN or SVM based models.


\textbf{Competitive swarm optimizer.}
Pre-training an ESN using PSO introduces some extra hyperparameters, which are usually determined empirically.
% bring a problem that  
% Although PSO significantly improves the performance of ESNs compared with randomly weights initialization, one of the problems is     that some extra hyperparameters of PSO are introduced for ESN pre-training, and their values are usually determined empirically. This issue becomes more critical as the network gets bigger in size. 
Furthermore, when dealing with high-dimensional optimization tasks, PSO is likely to experience stagnation or premature convergence \cite{long2019evolving}.
To address this, \cite{long2019evolving} designed a deep ESN model with a competitive swarm optimizer (CSO) and used it for fault diagnosis—a precise classification task. 
CSO avoids the problem of optimizing too many parameters at once in PSO with its powerful particle update rule: the particles are updated by evaluating a pre-defined fitness function, and the winner particle will go straight into the next iteration.
% CSO differs from PSO in its powerful particle update rule, where the particles are updated in swarm iteration to find out the optimal solution by evaluating a pre-defined fitness function. 
% After the fitness of all particles are calculated, CSO randomly separates particles into two groups and then employs competition to obtain a winner. 
% Based on the calculated fitness, particles are randomly separated into two groups and then employs competition to obtain a winner. 
% Other particles will be updated except the winner which will go straightly into the next iteration. 
% CSO avoids the problem of optimizing too many parameters at once. 
For the implementation, CSO is combined with a local search technique to further optimize the deep ESN structure. 
% This approach was tested on four datasets and always obtained the highest accuracy, compared to the original ESN, SVM, CNN, etc. 
The work shows that deep reservoir networks based on evolutionary algorithms are suitable not only for time series prediction but can also be used to deal with classification problems with adequate results. 
% megumi maybe change to "evolutionary algorithms" if it makes sense


% \begin{comment}
% \subsubsection{Gradient Descent Based ESN}
% Evolutionary algorithm is not the only way to optimize the hyperparameters in ESNs. Generally, the hyperparameters includes spectral radius, leaking rate, input scaling, reservoir size and ridge regression penalty \cite{jaeger2001echo} [1]. In an early study, Jaeger \cite{jaeger2004harnessing} already proposed a variation of gradient descent (GD) learning rule, where they tried to obtain the optimal hyper-parameters that minimize the MSE at each time step separately. However, the authors concluded that this naïve GD approach may suffer from poor stability properties.

% Thiede et al. (2019) \cite{thiede2019gradient} [9] presented a gradient based optimization algorithm for parameters tuning. The proposed algorithm has three key changes. First, the readout weight $\mathbf{W}^{out}$ is considered a function of the hyper-parameter, taking its derivative into account. Second, the authors used the penalty in the ridge regression in GD instead of noise, as the penalty is also differential. Moreover, unlike \cite{jaeger2004harnessing} where GD is computed every time step, the proposed algorithm update the parameters on a big enough batch. Although a slower convergence is expected, this avoids the problem that parameters are updated wrongly at a particular step.

% Additionally, the authors suggested that the proposed algorithm can be combined with grid search to reduce the computational cost, and this may be the best approach to optimize the parameters while avoiding local minima.
% \end{comment}

\subsubsection{Other Types of ESNs}
\textbf{Non-linear functions readout.}
As mentioned earlier, single reservoir ESN may not be able to create rich enough non-linear dynamics. % megumi - "a rich enough non-linear dynamic" or "rich enough non-linear dynamics"
\cite{chitsazan2019wind} proposed a new method called Non-linear ESN based on non-linear functions and successfully decreased the internal states of the network while increasing dynamic complexity, thus reducing the computational load. 
%revision_mode_230609
\textcolor{black}{
Specifically, recall that $\mathbf{x}(n)$ is the internal reservoir state vector ($\mathbf{x}$ for short),} while in this method, it is replaced by a non-linear function:

%revision_mode_230609
\textcolor{black}{
\begin{equation}
    \label{eq:nesn}
    \mathbf{x}_{NESN}=\mathbf{f}(\mathbf{x})=
    a_0 + a_1 \mathbf{x} + a_2 \mathbf{x}^2+\cdots+ a_n\mathbf{x}^n.
    % (\mathbf{W}^{in}\mathbf{u}(n)+
    % \mathbf{W}\mathbf{x}(n-1))
\end{equation}
}

\noindent where 
%revision_mode_230609
\textcolor{black}{$a_n$ being the constant.
With this modification, }non-linear complexity and the learning capability are increased, which results in a higher accuracy in time series forecasting. Moreover, as this method remains simple structure design, it does not require extensive training, parameter tuning or complex optimization process.

\textbf{Small-world topology.}
Small-world (SW) network was first proposed by \cite{watts1998collective}. 
For a regular topological network, each node is usually connected to its neighboring nodes. 
For the connection to other randomly chosen nodes (not adjacent), we denote the connection probability as $p$, where $p = 0$ remain regular topology, $p = 1$ remain random topology and $p \approx 0.1$ as the SW topology (see Fig. \ref{fig:TRNN and SW topology}).

To further investigate the echo state property and learning performance of ESNs, \cite{kawai2019small} presented an SW based ESN (SW-ESN). In this study, the input and output nodes are segregated, and the reservoir remains as an SW topology; that is, neurons connected to the input are different from neurons connected to the output. 
% In this manner, the ESN acts more like a human brain, and the echo state property was further examined. 
Experiments showed that the SW topology enables the input to flow to the output nodes, and the cluster organizations of the topology guarantee a larger range of echo state property, thus improving the robustness and learning performance of the ESN.


\subsection{Recent Trends of LSM-based RC}
% In this section, recent trends of LSM-based RC models will be reviewed.
\textbf{Spike-timing-dependent plasticity (STDP).}
STDP is a local unsupervised self-organizing learning rule based on Hebbian learning \cite{jin2016sso}. The main idea of STDP is that if the firing neuron $A$ tends to induce/inhibit spikes from another neuron $B$, the synaptic connection $\mathbf{w}$ from $A$ to $B$ is likely to be potentiated/depressed \cite{zhang2015digital}. In other words, the synaptic connection $\mathbf{w}$ from $A$ to $B$ is potentiated if a causal order (i.e., the presynaptic neuron fires before the postsynaptic neuron) is observed, or depressed otherwise. 
% \textcolor{black}{A general update rule can be simplified and described as [do we need equation here? \textit{Considering delete this equation}.]}:
% \begin{equation}
%     % \label{eq6}
%     \mathbf{w}_{t+1} = \mathbf{w}_{t} \pm \Delta\mathbf{w}
% \end{equation}
% where $\Delta\mathbf{w}$ is the weight modification value.
As a spiking neural network, LSM was shown able to be trained by this adopted learning rule in an online learning manner, and therefore significantly reducing memory storage load and computational cost, as well as making LSM becomes more hardware-friendly for physical implementations \cite{jin2016sso, luo2018improving}.
%revision_mode_2306 
Specially, a recent hands-on implementation of LSM based on \cite{kaiser2017scaling} is realized using NEST simulator \cite{gewaltig2007NEST}.



\textbf{Evolutionary algorithms.}
Similarly to the small world topology, the percentage connectivity indicates the connection probability between neurons within the liquid. Finding a proper percentage of connectivity is then an important factor for improving the accuracy of LSM. Too high/low connectivity will harm the performance, which also suggests that there is an optimal connectivity for a given task.
Particularly, \cite{reynolds2019intelligent} proposed an evolutionary algorithm to optimize the number of neurons and percentage connectivity on a single liquid. Meanwhile, \cite{zhou2019evolutionary} used a covariance matrix adaptation evolution strategy to optimize three parameters, i.e., percentage connectivity, weight distribution and membrane time constant in one liquid. However, \cite{tian2021neural} pointed out that the above algorithms ``only perform parameter optimization in a single liquid and do not optimize the architectures of LSM,” and proposed a Neural Architecture Search (NAS) based framework to optimize both architecture and parameters of LSM model. Furthermore, the presented framework introduced a three-steps search for LSM, where the first step is architecture optimization, the second step is the search for the number of neurons and the final step is parameters optimization. Experimental results show that the proposed model achieves comparable accuracy on classification tasks of the three datasets (i.e., MNIST, Noisy MNIST and FSDD). 

%===============
%===============
%===============

\subsection{Dynamical Systems}
%revision_mode_2306 
\textcolor{black}{
The key essence of RC lies in its approach to use large, fixed random networks, i.e., the \textit{reservoirs}, exhibiting a rich set of dynamical behaviors. 
These reservoirs provide high-dimensionality and memory in which input data can be transformed and stored, making it easier to model complex temporal patterns and perform machine learning tasks. On the other hand, dynamical systems, characterized by their temporal evolution and behavior, offer crucial insights into the working of these reservoirs, shaping how we understand and optimize them. 
In the following subsections, we show how RC provides a practical framework for studying dynamical systems, while the theories underlying dynamical systems give a solid mathematical foundation to the operation of RC. 
}


% Figure environment removed

\subsubsection{Single-node Time-delayed Feedback Reservoir}
\label{sec:sntdfbrc}
\textbf{Definition.} Classical RC models process a low-dimensional temporal input through a high-dimensional reservoir state space. This high-dimensional state space is achieved by creating many randomly connected artificial (ESN) or spiking (LSM) neurons as a reservoir, so as to receive input data coupled into the reservoir with weight (synaptic) connections. 
%Information processing using a single dynamical node as complex system
It turns out that RC can be implemented by using only a single hardware node. In 2011, the concept of using a single dynamical node as a reservoir to generate a high-dimensional space, was introduced in \cite{appeltant2011information}. 
In contrast to network-based reservoirs consisting of many neurons such as ESN and LSM, single node RC shows great simplicity especially for physical implementation, 
Theoretically, the proposed delay system refers to non-linear delayed feedback system, which is a type of dynamical system described by delay differential equations given by \cite{lepri1994high}:
\begin{equation}
    % \label{eq6}
    \frac{dx(t)}{dx}=F(t,x(t),x(t-\tau)),
    % \mathbf{w}_{t+1} = \mathbf{w}_{t} \pm \Delta\mathbf{w}
\end{equation}

\noindent where $t$ and $x$ are continuous time and state variables, respectively. $F$ is the function of this system with $\tau>0$ the delay factor. This system is usually employed by using electronic circuits with a feedback loop (see Fig. \ref{fig:dynamical systems}A). 
%revision_mode_2306 
\textcolor{black}{
The workflow is as follows:
\begin{itemize}
    \item A low-dimensional input signal (e.g., one-dimensional temporal signal) is first processed using a time-multiplexing masking function, and modulates the state of the node.
    \item The single node samples the pre-processed input states and holds them for a delay period of $\tau$. 
    \item Meanwhile, $N$ virtual nodes are set that equally divide $\tau$ with the time interval of $\theta=\tau/N$, forming a delay line.
    \item When the signal reaches the end of the delay line, it is fed back into the node, influencing the node's future states.
    \item The current state of the node and the states stored in the delay line are then fed to the readout layer with trainable weighted connections.
\end{itemize}
}
% The node's output is sent along the delay line, effectively storing the current state for later use.
% When the signal reaches the end of the delay line, it is fed back into the node, influencing the node's future states.
% The current state of the node and the states stored in the delay line can be used to perform computations.
% To illustrate the workflow, a low-dimensional signal (e.g., one-dimensional temporal signal) is first processed using a time-multiplexing masking function. 
% Next, the single node samples the pre-processed input states and holds them for a delay period of $\tau$. 
% Meanwhile, $N$ virtual nodes are set that equally divide $\tau$ with the time interval of $\theta=\tau/N$, forming a delay line. 
% The virtual reservoir states, created by the virtual nodes, are then fed to the readout layer with trainable weighted connections.

\textbf{Extensions.} The single-node delayed feedback RC was experimentally investigated on spoken digit recognition task and NARMA time series prediction task \cite{verstraeten2005isolated, jaeger2002short}. 
Some variations of delayed feedback structure were proposed, such as
%Reservoir Computing with an Ensemble of Time-Delay Reservoirs {NO ACCESS}
(1) Different ensembles of delay-based RC with several delayed neurons by \cite{ortin2017reservoir}. 
and (2) two circular connected time-delayed based reservoirs with a longer delay line by \cite{brunner2018tutorial}.

\subsubsection{Cellular Automaton}
Another type of dynamical system that can be used as an RC is the cellular automaton (CA). 
CA is a collection of a cell-grid of specified shape that evolves (interacts with its neighbors and changes its state) through a number of discrete time steps \cite{wolfram2018cellular}. CA's new state is determined by a pre-defined set of update rules and neighboring cells, resulting in a rich dynamic (see Fig. \ref{fig:dynamical systems}B).

%Reservoir Computing using Cellular Automata
The concept of CA system was further extended to RC. A series of work on building CA-based RC was made by \cite{yilmaz2014reservoir, yilmaz2015machine}, where an evolution rule was introduced to create a space-time volume in the automaton state space (i.e., the reservoir). 
%revision_mode_2306 
\textcolor{black}{
The proposed CA system was reported to be suitable for combining other types of discrete dynamical systems such as 
% Computational capabilities of random automata networks for reservoir computing
Boolean logic and symbolic processing \cite{snyder2013computational}.
}
Recent studies on CA-based RC also include modifications and extensions of network architecture \cite{nichele2017reservoir, nichele2017deep}; 
as well as improvements of evolution rules in CA \cite{mcdonald2017reservoir}.
%revision_mode_2306
\textcolor{black}{More recently, \cite{uragami2022universal} explored the advantages of critical spacetime patterns generated by elementary cellular automata (ECAs) in reservoir computing, specifically focusing on the distractor's length in time series data and proposing asynchronously tuned ECAs (AT-ECAs) to generate universally critical spacetime patterns.}


\subsubsection{Coupled Oscillators}
\textbf{Definition.} 
RC models have been successfully applied in
% megumi - maybe "have been proven to apply to"
research areas such as machinery, chemistry, biology and physical systems (see section \ref{Recent Applications of Reservoir Computing}). Among these implementations, many RCs are built using coupled oscillators. A general representation of the dynamics of coupled oscillators is given by an ordinary differential equation:

%revision_mode_230609
\textcolor{black}{
\begin{equation}
    \frac{dx_{i}(t)}{dt} = F(x_{i}(t))+
    G(x_{1}(t),...,x_{N}(t)),
\end{equation}
}

\noindent where $i=1,...,N$ is the index of a total of $N$ coupled oscillators, $x_{i}(t)$ is the state of $i^{th}$ the oscillator at time $t$, $F$ and $G$ are an isolated function and a coupling function. The rich dynamics are provided by each oscillator and the interactions between them \cite{tanaka2019recent}.
%revision_mode_2306 
\textcolor{black}{
In the following, we aim to review studies that utilize coupled oscillators as a building block of RC. Additionally, we will discuss oscillation mechanism of brain in Section \ref{RC with brain mechanisms and cognitive science}.
}

\textbf{Mechanical oscillators.} 
The first category of RCs using coupled oscillators is based on mechanical oscillators. 
%Computing with networks of non-linear mechanical oscillators
\cite{coulombe2017computing} built a network with \textit{anharmonic} (i.e., non-linear) oscillators, where the components include masses coupled linear or non-linear springs, making the system power-efficient to solve a bit-stream computation task and a spoken words classification task.

\textbf{DNA oscillators.} 
%============
%A Novel Molecular Computing Approach: DNA Reservoir Computing
In the field of molecular computing, a deoxyribonucleic oscillator (DNA) reservoir was first proposed in \cite{goudarzi2013dna}. 
This RC consists of coupled deoxyribozyme based oscillators. 
Specifically, a microfluidic reaction \textit{chamber} was used to construct a reservoir, since different DNA species can interact (see Fig. \ref{fig:dynamical systems}C).
%revision_mode_2306 
\textcolor{black}{
Here, the microfluidic reaction chamber is a specific kind of chamber used to carry out chemical or biological reactions under well-controlled conditions.
}
Rich transient dynamics were then generated in the reservoir, where the reservoir state consists of the time-varying concentration of various species inside the chamber. 
A signal-tracking task was then performed by using a reservoir with three DNA species that exhibits oscillatory behavior.
Recent developments of DNA oscillators include a random chemical RC model \cite{nguyen2020reservoir}, where the random chemical circuits (i.e., DNA strand displacement) provide complex non-linear dynamics, making them suitable for RC implementation. 
% The proposed model outperforms the previous deoxyribozyme oscillator RC by Yahiro et al. [CRN yahiro 2018] in short and long-term memory tasks. 
%Reservoir Computing Using DNA Oscillators
A novel RC using DNA oscillators was reported in \cite{liu2022reservoir} which solves the problem of the lack of readout layer in the previous work \cite{goudarzi2013dna}, and was then demonstrated for the handwritten digit recognition and a second-order non-linear prediction.

\textbf{Chemical reaction networks (CRNs).}
Related to the DNA oscillators, the chemical reaction networks also show the capabilities to RC implementations.
%Biochemical Reservoir Computing
One of the initial studies on CRN-based RC model was presented in a presentation \cite{nguyen2018biochemical}, from which the reservoir dynamic is given by a set of ordinary differential equations (ODEs), while the readout layer is to learn the Hamming distance between input bit-streams. 
%SERS-based ssDNA composition analysis with inhomogeneous peak broadening and reservoir computing
In 2022, the author further proposed a chemical RC for single stranded DNA (ssDNA) analysis \cite{nguyen2022sers}.
%A reservoir computing approach for molecular computing
Additionally, \cite{yahiro2018reservoir} used a modular framework to implement a RC model. 
The main advantage of this work, compared with the previous DNA oscillators \cite{goudarzi2013dna}, is that molecular computing allows changing the size of CRNs on-the-fly.
%Physical Implementation of Reservoir Computing through Electrochemical Reaction
Another new chemical RC architecture was proposed by \cite{kan2021physical}, where the reservoir was implemented through electrochemical reactions.
% suggesting that the electrochemical reactions can be used to create a . 
Also, it is reported that the Polyoxometalate molecule (POM) in this chemical RC increases the diversity of the response current and thus improves their abilities to predict periodic signals. 
%Performance of reservoir computing in a random network of single-walled carbon nanotubes complexed with polyoxometalate
POM-based RC was further integrated with the so-called \textit{single-walled carbon nanotubes} as a random dense network \cite{akai2022performance}. Adequate results were obtained in tasks including waveform reconstruction, non-linear autoregressive modelling and memory capacity testing.


\textbf{Other RCs with oscillators.}
%Wave-based reservoir computing by synchronization of coupled oscillators. NO access
It is reported that oscillatory behavior can be restricted to the phase domain \cite{nakao2016phase}. This makes it possible to apply phase oscillators that exhibit rich dynamics to RC \cite{yamane2015wave}.
%Reservoir Computing using High Order Synchronization of Coupled Oscillators
A RC using two coupled relaxation oscillators built on $VO_2$ switches was reported \cite{velichko2020reservoir}, where the oscillators show high order synchronization that allows simulating the XOR operation.
% %Reservoir Computing using High Order Synchronization of Coupled Oscillators
% Another RC based on the high-order synchronization effect and special synchronization metrics was reported in \cite{} [Velichko et al. 2020] that can solve XOR operation.
%Neuromorphic Computing through TimeMultiplexing with a Spin-Torque Nano-Oscillator 
Besides, RC can also be implemented by using spin-torque nano-oscillators in neuromorphic computing \cite{riou2017neuromorphic}. See Section \ref{Physical_RC} for detail of spintronic RC.


% \subsubsection{}
%=============
%=============
%=============
%=============
%=====OVERVIEW PIC========
%=============



\begin{table*}[]
\centering
\renewcommand{\arraystretch}{1.0}
\caption{Types of physical implementations of RC: components and applications.}
\resizebox{0.99\textwidth}{!}{%
\begin{tabular}{llll}
\hline \hline
\textbf{RC Type} &
  \textbf{Name} &
  \textbf{Components / Methods} &
  \textbf{Benchmark tasks / Applications}
     \\ \hline \hline
\multirow{3}{*}{\textbf{Electronic RC}} &
  Analog Circuits &
  \begin{tabular}[c]{@{}l@{}}1. Various electronic elements \\ with digital hardware.\\ 2. Spiking circuit implementations.\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}Spoken digit recognition \cite{appeltant2014constructing}. \\ Memory capacity estimation \cite{appeltant2014constructing}.\\ Time-series prediction \cite{li2018deep}. \\ ECG signal processing \cite{li2018deep}. \\ Non-temporal non-linear task \cite{jensen2017reservoir}. \\ Efficient spiking implementation \cite{zhao2016novel, li2017analog}. \end{tabular} 
  \\ \cline{2-4} 
 &
  FPGAs &
  \begin{tabular}[c]{@{}l@{}}1. FPGAs board with stochastic logic.\\ 2. Recurrent SNN on FPGA.\\ 3. Parallel neuromorphic hardwares.\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}Channel equalization problems \cite{antonik2015fpga}. \\ Image and isolated digit recognition \cite{schrauwen2008compact, wang2016liquid}. \\ Short input and waveform- \\ patterns classification \cite{haynes2015reservoir, alomar2015digital}. \end{tabular}
  \\ \cline{2-4} 
 &
  Memristor &
  \begin{tabular}[c]{@{}l@{}}Neuromemristive components:\\ 1. Double crossbar arrays.\\ Memristive devices without neurons:\\ 1. Random memristor networks.\\ 2. Memristor with volatility.\\ 3. Memcapacitors. \\ 4. Atomic switch networks.\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}Time-series prediction \cite{yang2016investigations}. \\ Waveform pattern generation/classification \cite{kulkarni2012memristor}. \\ Associative memory task \cite{kulkarni2012memristor}. \\ Image recognition \cite{du2017reservoir}.\end{tabular} 
  \\ \hline
\multirow{2}{*}{\textbf{Photonic RC}} &
  \begin{tabular}[c]{@{}l@{}}Spatially distributed \\ Optical nodes\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}1. Semiconductor optical amplifiers \\ (SOAs) with digital masking. \\ 2. Photonic crystal platform.\\ 3. Nodes with free-space optics principles.\end{tabular} &
  \begin{tabular}[c]{@{}l@{}} Optical packet header identification \cite{vandoorne2014experimental}. \\ Spoken digit classification \cite{katumba2018low}.\\ Logical function prediction \cite{dong2018scaling}. \\ Waveform prediction \cite{fiers2014nanophotonic}. \\ Memory capacity estimation \cite{laporte2018numerical}.\end{tabular} \\ \cline{2-4} 
 &
  \begin{tabular}[c]{@{}l@{}}Time-delayed \\ feedback loop\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}1. Opto-electronic feedback loop. \\ 2. All-optical reservoir. \\ 3. Coherently driven passive cavity.\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}Non-linear channel equalization \cite{vinckier2015high}. \\ Spoken digit recognition \cite{vinckier2015high}.\end{tabular} \\ \hline
\multirow{4}{*}{\textbf{Spintronic RC}} &
  Spin-torque oscillators &
  Magnetic tunnel junction. &
  \begin{tabular}[c]{@{}l@{}}Short-term memory task \cite{taniguchi2021reservoir}. \\ Macro-magnetic simulation \cite{furuta2018macromagnetic}.\end{tabular} \\ \cline{2-4} 
 &
  Spin wave &
  \begin{tabular}[c]{@{}l@{}}Thin Yttrium iron garnet film between \\ a magneto-electric coupling layer \\ and a conductive substrate.\end{tabular} &
  Temporal XOR problems \cite{nakane2021spin}. 
   \\ \cline{2-4} 
&
  Magnetic skyrmions &
  \begin{tabular}[c]{@{}l@{}}1. Nanoscale magnetic vortex. \\ 2. Skyrmion fabrics.\\ 3. Magnetic skyrmion memristor.\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}Image classification task \cite{jiang2019physical}. \\ Handwritten digit recognition \cite{jiang2019physical}.\end{tabular}
   \\ \cline{2-4} 
 &
  \begin{tabular}[c]{@{}l@{}}Dipole-Coupled\\ Nanomagnets\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}1. Magnetic nonodots array. \\ 2. Magnetic random access memory \\ (MRAM) technology.\end{tabular} &
    \begin{tabular}[c]{@{}l@{}} NARMA10 task \cite{nomura2021reservoir}. \\ large-scale RC implementation \cite{nomura2021reservoir}. \end{tabular}   \\ \hline
\multirow{3}{*}{\textbf{Mechanical RC}} &
  \begin{tabular}[c]{@{}l@{}}Mass-spring-damper \\ systems\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}1. Mass-spring-damper simulating\\  softbodied robots.\\ 2. Soft robotic arm (octopus). \\ 3. Pneumatically driven robotic arm.\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}Active shape discrimination \cite{johnson2014active}. \\ Learning to emulate timers- \\ delays and parity \cite{nakajima2014exploiting, nakajima2018exploiting}.\\ Robot crawling \cite{bhovad2021physical}.\\ Foraging learning task \cite{yada2021physical}.\end{tabular} \\ \cline{2-4} 
 &
  Sensors &
  \begin{tabular}[c]{@{}l@{}}State Weaving Environment Echo \\ Tracker (SWEET) sensing.\end{tabular} &
  Ion concentration analysis \cite{konkoli2018developing}. 
   \\ \cline{2-4} 
 &
  \begin{tabular}[c]{@{}l@{}}Tensegrity robots and \\ Central pattern generator\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}1. Tensegrity-based structure with tensile \\ elements and compressive elements. \\ 2. Spiking-based reservoir acting as CPG.\end{tabular} &
  Locomotion and sensing tasks \cite{caluwaerts2011body}. \\ \hline
\textbf{Quantum RC} &
  \begin{tabular}[c]{@{}l@{}}Quantum circuits / \\ computers\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}1. Analog quantum dynamics under \\ a time-dependent Hamiltonian. \\ 2. Nuclear magnetic resonance (NMR).\\ 3. Quantum circuits with quantum gates.\end{tabular} & 
  \begin{tabular}[c]{@{}l@{}} Quantum-chaotic system- \\ implementation \cite{ghosh2021realising, govia2021quantum, martinez2021dynamical}. \end{tabular}
 \\ \hline
\end{tabular}%
}

\end{table*}


%=============
%=============
%=============
% \newpage

% \subsection{Neuromorphic Computing}

\subsection{Physical RC}
\label{Physical_RC}
Recall that the key feature of RC models is to transform sequential/temporal inputs into a high-dimensional non-linear dynamical space (i.e., the reservoir). If the reservoir provides rich enough dynamics, the desired output can be read out by using simple learning methods such as linear regression (see section \ref{Training a RC model} for details). Therefore, in principle, any kind of “non-linear, high-dimensional dynamical systems which satisfies some conditions”, has the potential to be a reservoir.

In particular, RC has become popular in a wide range of research fields focusing on hardware design; that is, recent trend of RC implementation have shifted to many domains of physical reservoir computing (PRC) such as optical systems, neuromorphic devices, chemical reactions, quantum computing, to name a few. % megumi - maybe change to "recent trends of RC implementation have shifted"

Several reviews have tried to organize this highly interdisciplinary topic of PRC. 
%
A comprehensive overview of recent PRC implementations was reported in \cite{tanaka2019recent}, while 
%Advances in photonic reservoir computing (2016)
\cite{van2017advances} focuses on the recent advances in photonic RC. 
%RC book
A book series with detailed and special issues on designing PRC was published \cite{nakajima2021reservoir}. 
In the following sections, we briefly outline the recently proposed PRC models for completeness. Readers may refer to the articles above for a more systematic and theoretical understanding of PRC.

%=======
% 1: what is it
% 2: how to do
% 3: why it is good
% 4: where it applies to

\subsubsection{Electronic RC}
% Figure environment removed

%======PhyRCrv=========
%Due to the low computational costs, simple configurations and
\textbf{Analog circuits.} 
% 1: what is it
Various electronic circuits are potential components for hardware RC. Previously, we discuss the single-node time-delayed feedback RC that was first proposed by \cite{appeltant2011information, appeltant2014constructing}. 
% 2: how to do
In fact, this type of RC can be formed by electronic circuits with other digital hardware elements \cite{soriano2014delay, soriano2015minimal}.  Additionally, a single-node RC implemented by using spiking circuits was also reported \cite{zhao2016novel, li2017analog}. 
% 3: why it is good
The advantages of analog circuit based RC include (1) less hardware requirements and (2) power efficiency in spiking implementations.
% 4: where it applies to
Therefore, RCs based on analog circuits were successfully applied to tasks including
(1) spoken digit recognition and memory capacity estimation  \cite{appeltant2014constructing};
(2) time series prediction and ECG signal processing \cite{li2018deep};
and (3) non-temporal non-linear tasks \cite{jensen2017reservoir}. 


\textbf{Field-programmable gate array (FPGA).}
% 1: what is it
FPGA board, as a hardware friendly element, has been proven to be suitable for RC implementations.
% 2: how to do
An early attempt at the combination of RC and FPGA board was reported by \cite{verstraeten2005reservoir}. % megumi - "An early attempt at" OR "Early attempts at... were reported by"
In 2014, an FPGA board with stochastic logic was used to implement RC for non-linear time series prediction task \cite{alomar2014low}. 
The offline learning in the work above was further modified to an online learning scheme \cite{antonik2015fpga}, where the units in the reservoir exhibit sigmoid activation and learn with a gradient descent algorithm.
In terms of spiking implementation, the conventional LSM models were successfully built on FPGAs in an early research \cite{schrauwen2008compact} (see Fig. \ref{fig:Electronic RC}A), as well as some models featured with parallel processing \cite{wang2016liquid} and STDP learning rule \cite{liu2018online}.
% 3: why it is good
In fact, FPGA-based RC have shown 
(1) better re-configurability;
(2) much faster processing speed;
(3) less energy costs compared to general CPUs;
and (4) more biologically plausible (SNN-based LSM).
% 4: where it applies to
Several benchmark tests were taken, indicating FPGAs based RC can be applied to
(1) channel equalization problems \cite{antonik2015fpga};
(2) image and isolated digit recognition \cite{schrauwen2008compact, wang2016liquid};
and (3) short input and waveform patterns classification \cite{haynes2015reservoir, alomar2015digital}.

\textbf{Memristive RC.}
%revision_mode_2306 
\textcolor{black}{A memristive device, or memristor, is a type of passive circuit element that maintains a relationship between the time integrals of current and voltage across a device.}
% 1: what is it
Some studies of RC focus on using memristive elements.
Here, the property of memristive elements is different from other circuit ones, since they vary the resistance depending on the current flow at different times. 
% These physical RCs are successfully applied to time-series prediction, waveform pattern generation and classification, associative memory task and image recognition.
%% 2: how to do
There are two main types of memristive components suitable for RC. 
The first type is based on neuromemristive circuits.
Specifically, memristors are used to model the synaptic plasticity between neurons.
\cite{yang2016investigations} successfully built an ESN based on memristor (see Fig. \ref{fig:Electronic RC}B), yet the performance was worse than the conventional ESN in terms of time-series prediction task.
Other studies include using double crossbar arrays as reservoir in ESNs \cite{hassan2017hardware} and LSMs \cite{soures2017robustness}.

Another branch of studies has shown that memristive devices without neurons can also generate rich non-linear dynamics for RC implementations.
% The second type is memristive devices without neurons. Without using neuron units, some memristive devices can also generate rich non-linear dynamics for RC implementations. 
The first attempt was made by \cite{kulkarni2012memristor} for a wave pattern classification task, where a memritstive topology was applied as a reservoir (Fig. \ref{fig:Electronic RC}C). In 2017, 
%revision_mode_2306 
\textcolor{black}{another} RC model using dynamic memristors for hard digits recognition was reported \cite{du2017reservoir}.
Other memristive networks were also explored to have potential of constructing RC, such as 
(1) random memristor networks \cite{burger2015computational};
(2) memristor with volatility \cite{carbajal2015memristor};
(3) memcapacitors \cite{tran2017memcapacitive} and its hierarchy extension \cite{tran2019hierarchical};
and (4) atomic switch networks \cite{stieg2012emergent}.

% 3: why it is good
% NO ADV????


% 1: what is it
% 2: how to do
% 3: why it is good
% 4: where it applies to
%======RC BOOK---------


\subsubsection{Photonic RC}
%There is an another review specially in Photonic
Optical computing is another paradigm suitable for RC implementations, where the complex non-linear and high-dimensional dynamics can be achieved in the intensity and phase of the optical field. 
A wide range of studies aim to uncover this specific area of RC. 
In principle, there are two main directions of photonic RC implementations: 
(1) spatially distributed optical nodes, and 
(2) time-delayed based photonic RC.
For more detailed investigation and discussion, please refer to two comprehensive reviews in \cite{van2017advances, tanaka2019recent}.

\textbf{Spatially distributed optical nodes.} 
%======PhyRCrv=========
% 1: what is it
% Conventional RC models such as ESN and LSM are the special type of RNNs where neuron nodes are spatially located with fixed and random connections, resulting in non-linear dynamics as reservoirs. 
It has been realized that the fixed and randomly connected topologies in the conventional RC models (e.g., ESN and LSM) can be implemented by spatially extended photonic networks using spatially distributed optical nodes.
% 2: how to do
Perhaps the first optical RC was proposed and subsequently developed in  \cite{vandoorne2008toward, vandoorne2014experimental}. 
Here, an on-chip network of semiconductor optical amplifiers (SOAs) was constructed to efficiently compute the \textit{tanh} function in the reservoir.
%Using digital masks to enhance the bandwidth tolerance and improve the performance of on-chip reservoir computing systems.
Later, a digital masking approach was proposed to overcome the short time delay and high operation rate in the previous photonic RC \cite{schneider2016using}.
From 2014 on, several techniques based on optical nodes were reported, including
(1) photonic crystal platform \cite{fiers2014nanophotonic, laporte2018numerical};
and (2) nodes with free-space optics principles \cite{mesaritakis2015high}.
% 3: why it is good
In fact, optical nodes RC are not only operating with lower power consumption, but more importantly, they are extremely fast in computation.
% 4: where it applies to
In terms of benchmark tasks and applications, RC with optical nodes were simulated and applied to 
(1) optical packet header identificationand spoken digit classification \cite{vandoorne2014experimental, katumba2018low};
(2) logical function prediction \cite{dong2018scaling};
(3) waveform prediction task \cite{fiers2014nanophotonic};
and (4) memory capacity task \cite{laporte2018numerical}.

%======RC BOOK---------

\textbf{Time-delayed feedback RC.} 
% 1: what is it
In section \ref{sec:sntdfbrc}, we review RC with single-node time-delayed feedback loop. After the first electronic time-delayed reservoir proposed by \cite{appeltant2011information}, optical devices were quickly used to implement such systems \cite{larger2012photonic, paquot2012optoelectronic}. 
% 2: how to do
In specific, these pioneering systems applied opto-electronic feedback loops, where the optical parts with long fiber provide non-linearity and time-delay while the electronic parts play the role of input processing and output extraction \cite{van2017advances}.
Later, the electronic parts of the opto-electronic based RC were replaced by active optical devices (i.e., SOAs or fiber coupler) \cite{duport2012all}, forming an all-optical delayed based RC.
% 3: why it is good
Moreover, it is reported that a significant improvement of high-speed, low-consumption can be achieved by using passive devices (see Fig. \ref{fig:phtonic RC}D). An example is a RC with a coherently driven passive cavity proposed in \cite{vinckier2015high},
% 4: where it applies to
in which a simple linear fiber cavity was used as a reservoir to solve tasks such as non-linear channel equalization and spoken digit recognition with remarkable performance.


% Figure environment removed 

\subsubsection{Spintronic RC}
% In the area of condensed matter physics, a new paradigm of building RC has been proposed by using spin electronics technology. 
%revision_mode_2306 
\textcolor{black}{Spintronics, or spin electronics, is a branch of physics (particularly condensed matter physics) and nanotechnology that uses the intrinsic spin of the electron and its associated magnetic moment, in addition to its fundamental electronic charge, in solid-state devices.}
It is reported independently that several spintronic elements can be candidates for RC implementations, including but not limited to 
spin-torque oscillators, 
spin waves,
magnetic skyrmion;
and Dipole-coupled nano-magnets.
Spintronic RC were reviewed in \cite{tanaka2019recent}, as well as in \cite{taniguchi2021reservoir} specially for spin-torque oscillators.

\textbf{Spin-torque oscillators.}
%======PhyRCrv=========
%======RC BOOK---------
% 1: what is it
The first experiments of building a spintronic RC can be found in \cite{torrejon2017neuromorphic, riou2021reservoir}, where the spin-torque oscillators were used to provide non-linearity. 
% 2: how to do
Here, the so-called magnetic tunnel junction is the key component and is used as a reservoir.
% 3: why it is good
The advantage of spin-torque oscillators is that the whole neural network can be emulated by the fast magnetization dynamics generated by simple components. % megumi - should this be "generated by"
% 4: where it applies to
The performances of spin oscillators based RC were evaluated with short-term memory estimation experiments, and were quantified by macro-magnetic simulation \cite{taniguchi2021reservoir, furuta2018macromagnetic}.

\textbf{Spin waves RC.}
%======PhyRCrv=========
%======RC BOOK---------
% 1: what is it
Another branch of spintronic RC is using spin waves as a reservoir. 
%Reservoir Computing With Spin Waves Excited in a Garnet Film
The first RC based on spin waves was proposed by \cite{nakane2018reservoir}. A bit sequence input was used and a stripe magnetic domain structure is introduced in a continuous magnetic garnet film where spin waves propagate, thus generating spatially distributed rich dynamics.
In 2021, spin waves RC was numerically evaluated in \cite{nakane2021spin}, 
% 3: why it is good
and was further shown to be low power consuming \cite{taniguchi2022spintronic}.
% 4: where it applies to
For the applications, spin waves RC in the above studies shows state-of-the-art performances in numerical experiments of temporal XOR problems and memory capacity tasks, once the model is properly tuned.

\textbf{Magnetic skyrmions.} 
% 1: what is it
%Magnetic skyrmions: advances in physics and potential applications 
Magnetic skyrmions are small swirling topological defects in the magnetization texture, in which the stabilization and dynamics depend strongly on the topological properties of skyrmions \cite{fert2017magnetic}. 
% This magnetic matter consists of nanoscale magnetic vortex.
% 2: how to do
%Magnetic Skyrmion as a non-linear Resistive Element: A Potential Building Block for Reservoir Computing
In 2018, the first prototype of RC based on skyrmion fabrics was proposed in \cite{bourianoff2018potential, prychynenko2018magnetic}. Here, the skyrmion fabrics refer to the phases that interpolate between single skyrmions, skyrmion crystals and magnetic domain walls. Owing to their random phase structures, they are claimed to be suitable for RC implementation. 
% 3: why it is good
% 4: where it applies to
%Physical reservoir computing built by spintronic devices for temporal information processing
Another application on RC based on magnetic skyrmion is to implement physical RC based on a single magnetic skyrmion memristor (MSM) for image classification task (i.e., handwritten digit recognition) \cite{jiang2019physical}.
% \textit{[This paper has a non-arXiv version but no access] \textit{[arXiv MSM 2019]}}

\textbf{Dipole-coupled nanomagnets.}
%Reservoir Computing with Dipole-Coupled Nanomagnets
% 1: what is it
A novel magnetic nanodots array was reported for a new way of RC implementations \cite{nomura2021reservoir}. The proposed system is a static magnetic system, in which the dynamics are enriched by increasing the number of nanomagnets. 
% 2: how to do
Specifically, the reservoir is formed by dipole-coupled nanomagnets (nodes). Similarly to the conventional RNN-based RC, some nodes were connected to the input, while all nodes were connected to the output and were then read out by the magnetic random access memory (MRAM) technology.
% 3: why it is good
A good aspect of nanomagnets based RC is that the magnetic interconnections solve the wiring problem of hardware RNN implementations, showing a great potential to build a large-scale RC system.
% 4: where it applies to
The system performance was evaluated in the NARMA10 task with adequate results.

%======RC BOOK---------
\subsubsection{Mechanical RC}
\textbf{Mass-spring-damper systems.} 
Several mechanical RC models have been proposed, including but not limited to soft robots and sensors networks. 
The idea of employing robot's body and its dynamics as a computational resource for RC, is originated from the early works \cite{hauser2011towards, hauser2012role}.
% two paper by Hauser 2011-2012
% \textbf{[non-linear mechanical oscilltor?]} 
The so-called mass-spring-damper systems are used to replace the conventional neurons in the reservoir (e.g., artificial nodes in ESNs). Specifically, mass-spring-damper systems serve as good models to simulate the biological bodies and soft-bodied robots, where both systems show rich non-linear dynamics that can be used in physical RC implementations. % megumi- "a physical RC implementation" or "physical RC implementations"
%Active shape discrimination with physical reservoir computers 2014
Inspired by the pioneering works above, \cite{johnson2014active} built a mass-spring-damper system for active shape discrimination.
%A soft body as a reservoir: case studies in a dynamic model of octopus-inspired soft robotic arm 2013
From a more biologically plausible perspective, \cite{nakajima2013soft} created a soft robotic arm based on an octopus. 
The work implies that control can partially be outsourced to the physical body and the interaction with the environment without being processed by the brain or a controller.
The authors further made a series of work on the octopus-inspired robotic RC, showing that the implementations can learn to emulate timers, delays and parity \cite{nakajima2014exploiting, nakajima2018exploiting}.
For more applications of mechanical RC, please see Section \ref{Recent Applications of Reservoir Computing}.

% %Morphological computation-based control of a modular, pneumatically driven, soft robotic arm
% In addition, recent applications of RC on robotic includes a new RC-based soft robot system with a highly complex pneumatically driven robotic arm \cite{eder2018morphological}, which can learn and reproduce various end point trajectories. 
% %Physical reservoir computing with origami and its application to robotic crawling
% For robotic crawling, \cite{bhovad2021physical} designed an origami based physical RC for soft robotic controller in dealing with earthworm-like peristaltic crawling.
% %Physical reservoir computing with FORCE learning in a living neuronal culture
% Another PRC model with FORCE learning was embedded into a robot to learn in a living neuronal culture \cite{yada2021physical}. In this work, a robot was placed on square fields with various obstacles and was directed toward the target objects.

\textbf{Sensors.}
%On developing theory of reservoir computing for sensing applications: the state weaving environment echo tracker (SWEET) algorithm
RC had been shown potential for processing sensor data (see section \ref{Recent Applications of Reservoir Computing}). Yet some researchers like \cite{konkoli2018developing} argued that those reservoirs focusing on sensing are often exploited in a somewhat passive manner, being a separated post-processing component that receives data from sensors. 
Therefore, \cite{konkoli2018developing} proposed the State Weaving Environment Echo Tracker (SWEET) sensing approach, where the RC is considered the sensing element itself for novel sensing applications such as ion concentration analysis.

\textbf{Tensegrity robots and central pattern generator (CPG).}
For completeness, tensegrity based robots and CPGs are briefly reviewed here. 
%revision_mode_2306 
\textcolor{black}{
%Environmental and Structural Effects on Physical Reservoir Computing with Tensegrity
To illustrate, tensegrity refers to a stable structure that consists of tensile elements connected by additional compressive elements \cite{fujita2018environmental}, which
is considered to be as adaptive and resilient as the biological systems. 
Another concept, the central pattern generator (CPG), is a neural network that can produce rhythmic patterned outputs without relying on rhythmic sensory or central inputs \cite{cpg1, cpg2, cpg3, cpg4}. }
%The body as a reservoir: locomotion and sensing with linear feedback
On top of the mass-spring systems, \cite{caluwaerts2011body} first developed a RC based on a tensegrity based structure and applied it to locomotion and sensing tasks. 
%Environmental and Structural Effects on Physical Reservoir Computing with Tensegrity
% Specifically, tensegrity refers to a stable structure that consists of tensile elements connected by additional compressive elements \cite{fujita2018environmental}, which
% is considered to be as adaptive and resilient as the biological systems.
Tensegrity structure can be further used as computational resources for modelling biological structures like human bodies and cells due to its stability. % megumi - "like human bodies and cells" or "like the human body and cell"
For example, CPG signals were generated by a tensegrity based RC for locomotion 
\cite{fujita2018physical, caluwaerts2014design}.
%Populations of Spiking Neurons for Reservoir Computing: Closed Loop Control of a Compliant Quadruped.
Another CPG-related RC was reported in \cite{vandesompele2019populations}, where the FORCE learning algorithm was used to train a spiking-based reservoir that acts as a CPG. 
%Behavioral Diversity Generated From Body–Environment Interactions in a Simulated Tensegrity Robot
\cite{terajima2021behavioral} further showed that the biologically plausible tensegrity robots are capable of adaptation to environmental changes.

\subsubsection{Quantum RC}
% 1: what is it
Quantum reservoir computing (QRC) is an intersection of quantum computing and RC. Reviews on this topic can be found in 
\cite{mujal2021opportunities, fujii2021quantum, ghosh2021quantum}.
%Opportunities in Quantum Reservoir Computing and Extreme Learning Machines
%Quantum Reservoir Computing: A Reservoir Approach Toward Quantum Machine Learning on Near-Term Quantum Devices [RC book]
%Quantum Neuromorphic Computing with Reservoir Computing Networks
% 2: how to do
The platform of QRC was first proposed by \cite{fujii2017harnessing}.
The idea is to use analog quantum dynamics under a time-dependent Hamiltonian, where the parameters are randomly chosen without tuning. %revision_mode_2306 
\textcolor{black}{
Here, the  Hamiltonian is a mathematical operator used to describe the total energy of a quantum system. It plays a central role in the Schrödinger equation, which describes how a quantum state evolves over time. 
}
Several improvements of QRC have been explored, including
%Boosting Computational Power through Spatial Multiplexing in Quantum Reservoir Computing
(1) boosting computing power \cite{nakajima2019boosting};
%optimizing a quantum reservoir computer for time series prediction
(2) enhancing memory capacity \cite{kutvonen2020optimizing};
%Toward NMR Quantum Reservoir Computing [RC book]
and (3) using Nuclear Magnetic Resonance (NMR) \cite{negoro2021toward}.
% 3: why it is good
Taking the advantage that any quantum-chaotic system can be used for implementations, 
% 4: where it applies to
%Realising and compressing quantum circuits with quantum reservoir computing
QRC are investigated in many other studies, including
(1) using quantum circuits with quantum gates \cite{ghosh2021realising};
% Quantum reservoir computing with a single non-linear oscillator
(2) single non-linear oscillator \cite{govia2021quantum};
%Dynamical Phase Transitions in Quantum Reservoir Computing 
and (3) dynamical phase transitions \cite{martinez2021dynamical}.
%revision_mode_2306 
% \textcolor{black}{[MORE WORKS NEEDED]}

%======PhyRCrv=========

%======RC BOOK---------



\subsection{Other RC models}
% 1: what is it
% 2: how to do
% 3: why it is good
% 4: where it applies to


\textbf{RC with non-linear vector autoregression (NVAR).}
% 1: what is it
% RC models with random weight connections usually perform well only in a particular family of tasks, and they are hard to select a good initialization of the random connections. 
It turns out that RC can be realized as a general, universal approximator of dynamical systems, in which the RNN part contains non-linear activation neurons while the readout layer is a weighted linear sum of the reservoir states.
A novel concept was proposed by \cite{gonon2019reservoir} and \cite{hart2021echo}, that RC with linear activation of neurons followed by a non-linear readout is equivalent to a universal approximator. 
In this case, such a RC would become mathematically identical to a non-linear vector autoregression (NVAR) machine \cite{bollt2021explaining}.
By identifying the limitations of random reservoir and taking inspiration from \cite{bollt2021explaining}, \cite{gauthier2021next} proposed a so-called next generation reservoir computing (NG-RC) model based on NVAR.
% 2: how to do
The proposed model was built without the requirements of random matrices and many meta-parameters, and the feature vector of the NVAR was introduced equivalent to the readout of RC
% consists of $k$ time-delay observations of the dynamical system to be learned and non-linear functions of these observations. 
For mathematical details, please refer to \cite{gauthier2021next}.
% 3: why it is good
By applying NG-RC to three RC benchmark tasks including Lorenz attractor prediction, the model showed faster computational time while at the same time requiring only a small number of sample and few meta-parameters for training. % megumi either "very few datasets" or "very small datasets" depending on which is more accurate
% 4: where it applies to
A possible application would be using NG-RC to create a digital twin for dynamical systems.



%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%

\section{Recent applications of reservoir computing} \label{Recent Applications of Reservoir Computing}

As a special type of recurrent neural network, RC avoids the main problem of a difficult, unstable and resource-consuming training process. In the past decade, however, various deep learning algorithms, took advantage of the intricacies of gradient based RNN training with greater computational power and finally became main-stream. This led reservoir computing research into a niche for a few years. 
%revision_mode_2306 
\textcolor{black}{As scientists in various research fields have found many new ways of RC implementations and applications (see Table \ref{tab:app}), RC have prompted renewed interest among researchers from disparate domains.
This section presents a comprehensive review of these recent trends, showcasing the widespread applicability of RC from the realms of engineering and computer science to the diverse fields of physical and social science.
}


\begin{table*}[]
\renewcommand{\arraystretch}{1.0}
\centering
\caption{Recent applications of RC in various research fields.}

\resizebox{0.85\textwidth}{!}{%
\begin{tabular}{lclll}
\hline \hline
 & \textbf{Research Fields}  &  & \multicolumn{1}{c}{\textbf{Applications}}                                                                                                                           &  \\ \cline{2-2} \cline{4-4}
 & Biomedical       &  & \begin{tabular}[c]{@{}l@{}}ECG, EMG and MCG signal processing \cite{hadaeghi2019reservoir, mastoi2019reservoir, n2021ecg, donati2018processing, sakib2021noise, mohsen2020ai}.\\ Medical images segmentation and classification \cite{hadaeghi2021spatio}.\end{tabular}         &  \\ \cline{2-2} \cline{4-4}
 &
  Machinery &
   &
  \begin{tabular}[c]{@{}l@{}}Active shape discrimination \cite{johnson2014active}.\\ Temporal information processing \cite{jiang2019physical}.\\ Robotic crawling \cite{bhovad2021physical}.\\ UAVs and sensors control \cite{chen2017caching, chen2019liquid, challita2019interference, bacciu2014experimental, palumbo2016human, sun2021sensor, konkoli2018developing}.\\ Fault diagnosis \cite{zheng2017brain, zheng2018fault, long2019evolving, zhang2019deep, zhang2021pre}.\end{tabular} &
   \\ \cline{2-2} \cline{4-4}
 & Data Science     &  & \begin{tabular}[c]{@{}l@{}}Series chunking and clustering \cite{atencia2019dynamic, atencia2020time, asabuki2018interactive}.\\ Similarity learning \cite{krishnagopal2018similarity}.\end{tabular}                                        &  \\ \cline{2-2} \cline{4-4}
 & Security         &  & \begin{tabular}[c]{@{}l@{}}Attack detection \cite{ozay2015machine, hamedani2017reservoir, hamedani2019detecting, chandrasekaran2021real}.\\ Specific emitter identification \cite{kokalj2020deep, kokalj2021reservoir}.\end{tabular}                                          &  \\ \cline{2-2} \cline{4-4}
 & Communications   &  & \begin{tabular}[c]{@{}l@{}}Optical communications \cite{wang2021signal}.\\ Network traffic flows analysis \cite{yamane2019application, ando2019road}.\\ Symbol detection in MIMO systems \cite{mosleh2018brain, zhou2020deep}.\end{tabular} &  \\ \cline{2-2} \cline{4-4}
 &
  Chemistry &
   &
  \begin{tabular}[c]{@{}l@{}}Remaining useful lifetime prediction of PEMFC \cite{morando2013fuel, morando2017anova, mezzi2018multi, li2020adaptive}.\\ Real-time gas concentration prediction- \\ using chemosensors \cite{fonollosa2015reservoir}.\\ DNA oscillators and Magnetic skyrmions \cite{goudarzi2013dna, yahiro2018reservoir, nguyen2020reservoir, liu2022reservoir}.\\ Chemical reaction networks \cite{yahiro2018reservoir, kan2021physical, nguyen2022sers}.\end{tabular} &
   \\ \cline{2-2} \cline{4-4}
 & Environmental    &  & \begin{tabular}[c]{@{}l@{}}Wind forecasting \cite{chitsazan2019wind, wang2020novel}.\\ Wind power generation \cite{hamedani2017reservoir}.\end{tabular}                                                    &  \\ \cline{2-2} \cline{4-4}
 & Audio and Speech &  & \begin{tabular}[c]{@{}l@{}}Speech recognition \cite{verstraeten2005isolated, ghani2010neuro, alalshekmubarak2014improving, zhang2015digital, abreu2020role}.\\ Music classification \cite{pons2019randomly}.\end{tabular}                                                   &  \\ \cline{2-2} \cline{4-4}
 & Finance          &  & \begin{tabular}[c]{@{}l@{}}Stock market prediction \cite{wang2021stock}.\\ Financial system modelling \cite{budhiraja2021reservoir}.\end{tabular}                                        &  \\ \hline \hline
\end{tabular}%
}
\label{tab:app}
\end{table*}


\subsection{Biomedical}

\textbf{Electrocardiogram (ECG).} 
%Reservoir Computing Models for Patient-Adaptable ECG Monitoring in Wearable Devices (2019)
A modified ESN is applied to \textit{cardiac monitoring} \cite{hadaeghi2019reservoir}. 
Specifically, the experimental data includes two classes of ECG signals from MIT-BIH databases with highly imbalanced number of instances. %revision_mode_2306 
\textcolor{black}{The reservoirs are used as patient-adaptable classifiers. 
}
% megumi - "from an MIT-BIH database with a highly imbalanced..." or "from MIT-BIH databases with highly imbalanced numbers" 
These classifiers can not only produce accurate results, but also show the potential to implement ECG classifiers by using neuromorphic hardware with spiking neural networks. 
%Reservoir Computing Based Echo State Networks for Ventricular Heart Beat Classification (2019)
Similarly, \cite{mastoi2019reservoir} used an ESN for a\textit{bnormal cardiac activity detection} (Fig. \ref{fig:app_biomedical}A). The main objective is to apply an ECG monitoring model in Medical Internet of Things (MIoT) devices with fast speed and low power consumption. The proposed RC model shows better performance and generalization in AHA and MIT-BIH-SVDM datasets than patient-adaptable methods, and also suggests that RC can be implemented in wearable wireless devices. 
%ECG Denoising using a Single-Node Dynamic Reservoir Computing Architecture
Besides, in the problem of ECG signal \textit{denoising}, a single-node RC is applied to solve this problem by minimizing the EMG signal that impairs the ECG signal \cite{n2021ecg}.

%Reservoir computing with biocompatible organic electrochemical networks for brain-inspired biosignal classification (2021)
In 2021, \cite{cucchi2021reservoir} reported a new type of reservoir for \textit{arrhythmic heartbeats} classification. Inspired by the pioneering work on the combination of organic electrochemical transistors (OECTs) and reservoir computing, they implemented dendritic networks using OECTs for real-time classification. In detail, the reservoir is created by using coupled dendritic fibers. 
Once these fibers are excited through the electrolyte, they create a strong enough non-linear dynamic of the input signals.
The proposed networks show the potential use for \textit{biofluid monitoring} and biosignal analysis with high accuracy, indicating that bio-compatible computational platforms can interact with body and biological analysis.

\textbf{Electromyography (EMG).}
Some studies on EMG also apply RC models.
%Design and Analysis of a Neuromemristive Reservoir Computing Architecture for Biosignal Processing (2016)
\cite{kudithipudi2016design} proposed a scalable and reconfigurable neuromemristive reservoirs architecture for EEG and EMG signal analysis. 
%Processing EMG signals using reservoir computing on an event-based neuromorphic system
A further EMG application using LSM model was proposed by \cite{donati2018processing}, where the EMG signal collected by the surface EMG (sEMG) sensors are classified by an LSM-based neuromorphic hardware. 
%Signals to Spikes for Neuromorphic Regulated Reservoir Computing and EMG Hand Gesture Recognition
In 2021, another spiking RC model that applies CRITICAL plasticity rule \cite{brodeur2012regulation} for synaptic connection optimization was proposed for hand gesture recognition \cite{garg2021signals} (Fig. \ref{fig:app_biomedical}B). The author also proposed a novel approach to evaluate and convert the raw EMG signals to spikes encoding. 

\textbf{Magnetocardiography (MCG).} 
% MCG
Checking the ECG is not possible for everyone. 
Alternatively, magnetocardiography (MCG) signals can be detected by measuring the magnetic field produced by the electrical currents in the heart and can be converted into ECG signals. 
%Noise-Removal from Spectrally-Similar Signals Using Reservoir Computing for MCG Monitoring
\cite{sakib2021noise} built the first physical RC model for MCG monitoring. 
Specifically, the noisy sensed MCG signals take as input to the reservoir (i.e., the spintronic sensors, see Section \ref{Physical_RC}) and the output is the predicted ECG signals (Fig. \ref{fig:app_biomedical}C). 
This lightweight RC model is claimed to be much power-saving and lower memory-required while achieving comparable performance with a deep learning based filtering approach \cite{mohsen2020ai}.
%A Circuit-embedded Reservoir Computer for Smart Noise Reduction of MCG Signals
A similar model was later proposed by \cite{shakya2021circuit} to extract ECG signals from MCG signals.

\textbf{Other biomedical applications of RC.}
In early research of molecular reservoir computing, the coupled deoxyribozyme oscillators is shown to be a type of reservoir \cite{goudarzi2013dna}. This refers to DNA reservoir computing (see Section \ref{Recent approaches in RC}).
%Modality classification of medical images with distributed representations based on cellular automata reservoir computing
In addition to DNA based RC, a medical image classification with distributed representations on cellular automata RC was reported by \cite{kleyko2017modality} (see cellular automata RC in Section \ref{Recent approaches in RC}).
%Spatio‑temporal feature learningwith reservoir computing forT‑cell segmentation in live‑cell Ca2+ fuorescence microscopy
Besides, a recent study on spatio-temporal feature learning used RC model for T‑cell consistent segmentation % megumi - either change to "recent research on" or "a recent paper/ study on",
\cite{hadaeghi2021spatio}. Instead of only applying a single reservoir, the model used multiple reservoirs for image segmentation and classification, where each reservoir focuses on a specific area of the image to obtain local interactions.

% Figure environment removed 


\subsection{Machinery}
% Figure environment removed 

\textbf{Robotics.} 
In section \ref{Recent approaches in RC}, we gave a brief review of mechanical RC. 
% A comprehensive survey of theoretical background and recent advances in robotic RC was reported by \cite{hauser2021physical}. 
% two paper by Hauser 2011-2012
The so-called mass-spring-damper systems were built as the first type of mechanical RC by \cite{hauser2011towards, hauser2012role} (see Fig. \ref{fig:app_machinery}A).
This new type of RC was then used for \textit{active shape discrimination}  \cite{johnson2014active}.
%A soft body as a reservoir: case studies in a dynamic model of octopus-inspired soft robotic arm 2013
Meanwhile, \cite{nakajima2013soft} made a series of works on the octopus-inspired soft robotic RC (Fig. \ref{fig:app_machinery}B), showing that the implementations can learn to emulate timers, delays, and parity \cite{nakajima2013soft, nakajima2014exploiting, nakajima2018exploiting}.
Another application of robotic RC was reported to learn and reproduce various end point trajectories by using a new RC-based soft robot system with a highly complex \textit{pneumatically driven} robotic arm \cite{eder2018morphological}.
%Physical reservoir computing with origami and its application to robotic crawling
%revision_mode_2306 
\textcolor{black}{
RC has also been applied in robotic crawling by using the \textit{origami} -- a traditional play of folding paper into sophisticated and 3D shapes. \cite{bhovad2021physical} shows that an origami structure based PRC can be designed to build a soft robotic controller for earthworm-like peristaltic crawling. 
}
%Physical reservoir computing with FORCE learning in a living neuronal culture
\cite{yada2021physical} embedded FORCE learning into a robot to learn in a living neuronal culture (i.e., foraging learning task), in which the robot was placed on square fields with various obstacles and was directed toward the target objects (see Fig. \ref{fig:app_machinery}C).

\textbf{Unmanned Aerial Vehicles (UAVs).} 
RC can be applied to UAV systems. 
Particularly in telecommunications, RC models were used in the so-called cache-enabled UAVs for optimizing \textit{resource allocation} over the LTE licensed and unlicensed bands.
%Caching in the Sky: Proactive Deployment of Cache-Enabled Unmanned Aerial Vehicles for Optimized Quality-of-Experience
The first attempt was to use ESN in such a system to predict each user’s content request distribution and its mobility pattern when limited information on the states of users and the network is available \cite{chen2017caching}.
An LSM-based model was further proposed, which can predict more context information of the users and thus improves the prediction accuracy \cite{chen2019liquid}.

%Interference Management for Cellular-Connected UAVs: A Deep Reinforcement Learning Approach
Another branch of UAV applications includes one that uses a deep ESN based reinforcement learning algorithm for UAV path planning by \cite{challita2019interference}. In this system, each UAV uses an ESN to optimize paths and learns transmission power at different locations.
%Swing-Free-Manoeuvre-Controller-for-Rotorcraft-Unmanned-Aerial-Vehicle-Slung-Load-System-Using-Echo-State-Networks.pdf
Besides, ESN can also be used to control rotorcraft UAVs, which outperforms linear models in robustness to disturbances \cite{vargas2015swing}.
%Flapping-Wing Dynamics as a Natural Detector of Wind Direction
In 2021, \cite{tanaka2021flapping} proposed a method for controlling flapping-wing UAVs in different wind directions, where strain sensors are applied to measure the wind movements, and a physical RC is used as a classifier to recognize the wind stream from the sensor data (see Fig. \ref{fig:app_datasci}A).


\textbf{Sensors.}
RC models can also be integrated into wireless sensor networks (WSNs). % megumi - integrated "into" or "with"
%An experimental characterization of reservoir computing in ambient assisted living applications
Generally, the sensor devices in WSNs are distributed and computationally constrained, and the collected data usually consist of temporal information, which makes RC inherently suitable for embedding on the WSN devices \cite{bacciu2014experimental}.
%Human Activity Recognition using Multisensor Data Fusion based on Reservoir Computing
One of the real-world WSN applications using RC is activity recognition in Ambient Assisted Living (AAL) tasks \cite{palumbo2016human}.
Specifically, RC-based multi-sensors were used for feature collection and extraction. 
The sensed data were then further processed by an ESN which provides a good activity recognition accuracy with low computational costs. 
%In-sensor reservoir computing for language learning via two-dimensional memristors
In 2021, a bio-inspired in-sensor RC was demonstrated to be effective for classifying short sentences of language \cite{sun2021sensor}. 

%On developing theory of reservoir computing for sensing applications: the state weaving environment echo tracker (SWEET) algorithm
It is worth noting that although RC had been shown potential for processing sensor data, some researchers like \cite{konkoli2018developing} argued that
those reservoirs focusing on sensing are often exploited in a somewhat passive manner, being a separated post-processing component that receives data from sensors. 
Therefore, \cite{konkoli2018developing} further proposed the State Weaving Environment Echo Tracker (SWEET) sensing approach. Here, RC was considered as the sensing element itself for novel sensing applications such as \textit{ion} concentration analysis. 

\textbf{Fault Diagnosis.} 
%[WEB]Fundamental Series on Building Analytics: What is Fault Detection and Diagnostics?
Fault diagnosis generally refers to the process of detecting errors in physical systems while attempting to identify the source of the problems.
%Evolving Deep Echo State Networks for Intelligent Fault Diagnosis
Built on the deep ESN architecture suggested by \cite{gallicchio2017deep}, \cite{long2019evolving} proposed evolving deep ESN models for 3-D printer fault diagnosis, with a developed version of particle swarm optimization (i.e., competitive swarm optimizer, CSO). This RC model uses evolutionary optimization and is shown to be state-of-the-art and computationally economic, which is a complement to deep learning algorithms, rather than a competitor. % megumi - "which is a complement to" or "which is complementary to"
%Deep Fuzzy Echo State Networks for Machinery Fault Diagnosis
Meanwhile, the same research group proposed another solution for 3-D printer fault diagnosis \cite{zhang2019deep, zhang2021pre}. Specifically, deep ESNs were used to improve feature extraction performance, where the features were reinforced throughout the hidden layers by using fuzzy clustering as a tuning step.
This low computational costing model also provides the optimal solution in all experiments, with a total of 26 different condition patterns in fault diagnosis data.

In addition, RC can also be applied to chemical fault diagnosis in %revision_mode_2306 
the \textit{proton exchange membrane fuel cell} (PENFC) system. 
%Brain-inspired computational paradigm dedicated to fault diagnosis of PEM fuel cell stack
The first RC application in PEMFC system diagnosis was made by \cite{zheng2017brain}, where a delayed feedback RC was used to detect four fault types yet in the static operating conditions only. 
%Fault Diagnosis of PEMFC Systems In The Model Space Using Reservoir Computing
Instead of processing voltage signal in the original data space, a newer variant based on previous models was proposed, which performs abnormal detection in the reservoir computing based model space (current-voltage model) without requiring additional feature extraction \cite{zheng2018fault}.


% Figure environment removed


\subsection{Data Science}

\textbf{Image recognition.}
Image recognition stands as a prominent field within computer vision and machine learning.
While RC shows impressive performance in finding and generating temporal features, it has also been adapted for image recognition tasks, either alone or integrated with other techniques such as deep networks with convolutional layers.
Table \ref{tab:performance} shows the performance comparison between different RC models.
To illustrate, when using RC alone to deal with 2D or 3D inputs (i.e., images or videos), data are usually flattened into 1D signals prior to feeding them into the reservoir, as discussed in \cite{lukovsevivcius2012practical}. 
When combining with convolutional layers, these layers preprocess images and videos, transforming them into intermediate representations which the reservoir can process temporally
\cite{jalalvand2018application, tong2018reservoir, tanaka2022reservoir, chang2020reinforcement, chang2019convolutional, yoshihiro2020}.
Although RC models demonstrate competitive performances with other machine learning methods like CNNs on simpler datasets such as MNIST, their performance significantly declines when faced with datasets exhibiting higher spatial complexity, such as CIFAR-10. 
This performance gap highlights the inherent limitations of RC in dealing with complex spatial correlations, and underscores the need for further exploration and investigations in this field.
On the other hand, studies have proved that convolutional neural networks are likely to misclassify even if small perturbations are added to original samples \cite{goodfellow2014explaining, moosavi2017universal, tsipras2018robustness, su2019one, kotyan2020evolving, kotyan2022adversarial}. 
Thus, this also highlights a strong incentive for RC-based methods to tackle high-dimensional inputs with strong 2D/3D correlations, as it was shown that higher degrees of nonlinearity in the model are related to more robust neural networks, and nonlinearity is where the RC really shines.


\begin{table}[]
\renewcommand{\arraystretch}{1.0}
\centering
\caption{%revision_mode_2306 
\textcolor{black}{Comparison of the \textit{Accuracy} (\%) of recent RC models in image recognition benchmarks.}
}
\resizebox{0.6\linewidth}{!}{%
\begin{tabular}{lclrrr}
\hline \hline
\multicolumn{1}{c}{\textbf{Model Type}} &
  \textbf{Model} &
   &
  \multicolumn{1}{c}{\textbf{MNIST}} &
  \multicolumn{1}{c}{\textbf{Fashion MNIST}} &
  \multicolumn{1}{c}{\textbf{CIFAR-10}} \\ \hline \hline
\textbf{Standard RC}                       & \cite{schaetti2016echo} &  & 99.07\% & -       & -       \\ \cline{1-2} \cline{4-6} 
\multirow{4}{*}{\textbf{Hybrid (CNN+RC)}}  & \cite{jalalvand2018application} &  & 99.19\% & -       & -       \\
                                           & \cite{tong2018reservoir} &  & 99.25\% & -       & -       \\
                                           & \cite{yoshihiro2020} &  & 98.71\% & 86.27\% & -       \\
                                           & \cite{tanaka2022reservoir} &  & 98.38\% & 91.04\% & 64.49\% \\ \cline{1-2} \cline{4-6} 
\multirow{4}{*}{\textbf{Physical RC}}              & \cite{jacobson2021image} &  & 97.70\% & -       & -       \\
                                           & \cite{tran2019hierarchical} &  & 73.86\% & -       & 12.96\% \\
                                           & \cite{yang2022optical} &  & 98.20\% & 89.90\% & -       \\
                                           & \cite{moran2018reservoir} &  & 98.08\% & -       & -       \\ \cline{1-2} \cline{4-6} 
\multirow{2}{*}{\textbf{Hybrid (CNN+PRC)}} & \cite{jacobson2021hybrid} &  & 98.90\% & -       & -       \\
                                           & \cite{an2020unified} &  & 99.03\% & -       & 60.57\% \\ \hline \hline
\end{tabular}
}
\label{tab:performance}
\end{table}

\textbf{Clustering.} 
One of the applications of RC in data science is \textit{clustering}. As a special case of clustering, \textit{time-series clustering} introduces several additional issues when compared with static data clustering. 
For example, the lengths of time-series usually vary, and some of them may be infinite (e.g., video and audio sequences collected from CCTV cameras). 
Moreover, temporal dependencies in different parts % megumi - "a different part" or "different parts"
of a particular time-series cannot be captured by making a fixed detecting window (i.e., the dynamical behaviors in sequences contain to both short- and long-term correlation).
As a result, similarity measurement techniques, such as calculating Euclidean distance among temporal data, are not inherently suitable for time-series clustering. 

%Dynamic Clustering of Time Series with Echo State Networks
\cite{atencia2019dynamic} proposed the first dynamic clustering algorithm using conventional ESNs. The idea is to apply a clustering method inside every step of the reservoir's state update, where the author claimed that any unsupervised clustering methods can be used in principle (e.g., k-means or any other iterative, partitioning clustering methods). 
The proposed method overcomes the above-mentioned issues and produces more compact clusters when applied to a hard classification problem of detecting patients with eye disease in eye movements datasets (saccades). % megumi - "eye movement datasets" or "an eye movement dataset" 
%Time Series Clustering with Deep Reservoir Computing
In 2020, deep reservoir structure was introduced in time-series clustering \cite{atencia2020time}. 
The proposed algorithm was applied to more common benchmark datasets and showed better clustering quality than the previous algorithm and static clustering methods.


\textbf{Chunking.} 
Related to clustering problems, some studies have been proposed for sequence \textit{chunking} \cite{vargas2021continual, asabuki2020somatodendritic}. Here, the main difference between time-series clustering and sequence chunking is that chunking finds the temporal correlation between state variables, instead of clustering homogenous time-series together based on a certain similarity measure (see Fig. \ref{fig:app_datasci}B in the upper-left panel).
%Interactive reservoir computing for chunking information streams
\cite{asabuki2018interactive} used dual-reservoir networks that supervise each other to mimic the partner's responses to the given input. Here, a challenge of chunking sequences with uniform transition probabilities, which can be easily processed by humans in basal ganglia, was successfully solved by the proposed model while conventional statistical approaches fail to chunk (see Fig. \ref{fig:app_datasci}B). This suggests that reservoirs can predict dynamical response patterns to sequence input other than to directly learn transition patterns. % megumi- "the reservoir" or "reservoirs"

\textbf{Similarity learning.} 
%Similarity Learning and Generalization with Limited Data: A Reservoir Computing Approach
\cite{krishnagopal2018similarity} applied RC to learn the similarity between image pairs with limited data.
The reservoir here acts as a non-linear filter that projects the images into a high-dimensional state space, in which the state trajectories represent different dynamical patterns that reflect the corresponding relationship of given image pairs. 
The proposed model was tested on MNIST dataset and images taken from a moving camera. Compared to deep Siamese Neural Networks, this RC model showed significantly better performance in generalization tasks. 
The generalized combinations of relationships provide robust and effective image pair classification.

% \subsection{Engineering}

\subsection{Security}
% Figure environment removed 


\textbf{Attack detection in Smart Grid.} 
RC has been proven to efficiently solve \textit{false data injection} (FDI) and to improve the reliability in smart grid systems \cite{ozay2015machine}. 
%Reservoir Computing Meets Smart Grids: Attack Detection Using Delayed Feedback Networks
The first attempt was to use a modified delayed feedback network (i.e., single-node time-delayed RC) as a reservoir combined with a multilayer perceptron (MLP) as a readout for \textit{single-period attack detection} \cite{hamedani2017reservoir}. 
This RC model with MLP architecture produces a high attack detection rate (99\%) and shows strong robustness in various attack types.
%Detecting Dynamic Attacks in Smart Grids Using Reservoir Computing: A Spiking Delayed Feedback Reservoir Based Approach 
Later, the author extended the pioneering work to the more challenging dynamic attack detection in smart grid \cite{hamedani2019detecting}, where a bio-inspired learning rule called \textit{precise-spike-detection} (PSD) \cite{yu2013precise} is used for spiking reservoir training.

%Real-Time Hardware-Based Malware and Micro-Architectural Attack Detection Utilizing CMOS Reservoir Computing
Regarding the attack detection, recent applications of RC include detecting malware and micro-architectural attack, which is reported in \cite{chandrasekaran2021real} using a CMOS-based RC neural network embedded in a 65nm CMOS chip.

\textbf{Specific Emitter Identification (SEI).}
%Reservoir-Based Distributed Machine Learning for Edge Operation of Emitter Identification
SEI is capable of extracting rich non-linear characteristics of internal components within a transmitter to distinguish one transmitter from another. 
Since the fingerprint of SEI cannot be emulated, it is widely used in IoT devices to prevent MAC address spoofing attacks.
%Deep Delay Loop Reservoir Computing for Specific Emitter Identification
A reservoir with delay loops for SEI was first proposed by \cite{kokalj2020deep} and further adapted 
% megumi - should "adopted" be "adapted"?
to edge computing \cite{kokalj2021reservoir} where the RC architectures include a digital loop (FPGA) and a photonic one (Fig. \ref{fig:app_security}A).

\subsection{Communications}
\textbf{Optical communications.} 
%Signal recovery based on optoelectronic reservoir computing for high speed optical fiber communication system
In the high speed optical fiber communication systems, RC was applied for \textit{digital equalization}. \cite{wang2021signal} quantified the equalization performance of the optoelectronics RC. Experiment results show that the optoelectronics RC outperforms traditional equalizers under the same transmission conditions, taking the advantage of its ring topology for better correlation between adjacent data as well as its lower complexity and computational cost.  

\textbf{Network traffic.}
%Application Identification of Network Traffic by Reservoir Computing
% \textit{[No access]} 
\cite{yamane2019application} proposed a method for application identification for network traffic by physical RC, which processes traffic flows as dynamical time series data and enables fast and real-time identification.
%Road traffic reservoir computing
Another RC application is reported by \cite{ando2019road} for road traffic analysis.

\textbf{Symbol detection in MIMO-OFDM systems.} 
In wireless communication domains, multiple-input multiple-output with orthogonal frequency division multiplexing (MIMO-OFDM) is a key enabling technology in the 5G cellular network. 
Symbol detection is an important technique due to the severe non-linear distortion during transmission (Fig. \ref{fig:app_security}B). 
Thus, an accurate estimation of MIMO-OFDM channel is usually required.
%Brain-Inspired Wireless Communications: Where Reservoir Computing Meets MIMO-OFDM
The first integration of RC and MIMO-OFDM systems was proposed by \cite{mosleh2018brain}. Specifically, an ESN was used for system modelling and predicting non-linear dynamics, where the MIMO-OFDM channel estimation is no longer necessary. 
%Deep Reservoir Computing Meets 5G MIMO-OFDM Systems in Symbol Detection
Further, inspired by deep RC architectures \cite{gallicchio2017deep}, \cite{zhou2020deep} extended the existing shallow RC to form a deep neural network \cite{zhou2020deep, hamedani2019novel}, which significantly mitigates the frequency distortion.


\subsection{Chemistry}

% Figure environment removed 

\textbf{Fuel Cells (FC).} 
%Morando
ESNs have shown their effectiveness on \textit{remaining useful lifetime} (RUL) prediction for \textit{proton exchange membrane fuel cell} (PEMFC). \cite{morando2013fuel} developed the first RC model for FC prognostics using conventional ESN. 
Later, several variants of RC model have been proposed for improving the prediction performance. 
These include an ESN combined with ANOVA method \cite{morando2017anova}, as well as a multi-reservoir ESN \cite{mezzi2018multi}. 
%Adaptive Prognostic of Fuel Cells by Implementing Ensemble Echo State Networks in Time-Varying Model Space
However, recent research states that the above proposals assume that FCs are operated in constant nominal operating conditions \cite{li2020adaptive}; that is, only the degradation is considered the factor of the deviation of stack voltage. 
Another open problem is that the prognostic results in long-term experiments show that prediction will become inaccurate when disturbances occur.

\textbf{Chemosensor.} 
Metal oxide (MOX) based sensors are a common choice for tasks of chemical detection, yet the time response of these chemical sensors is usually excessively slow. 
It is stated that algorithms based on batch or sequential measurements are not suitable for continuous sensing scenarios. 
%Reservoir computing compensates slow response of chemosensor arrays exposed to fast varying gas concentrations in continuous monitoring
\cite{fonollosa2015reservoir} used RC algorithms to overcome the slow temporal dynamics of the chemosensors and applied RC for real-time gas concentration prediction by observing the sensors’ time series in response to the changes in the composition of a gas sample. 
Still, problems were reported, such as the drift of sensor response over time.

\textbf{Magnetic skyrmions.} 
%Magnetic skyrmions: advances in physics and potential applications 
RC using magnetic \textit{skyrmions} are reviewed previously in Section \ref{Recent approaches in RC}, where the random phase structures of the skyrmion fabrics are suitable for RC implementation \cite{prychynenko2018magnetic}. 
%Physical reservoir computing built by spintronic devices for temporal information processing
Recent applications in this field is to implement RC based on a single magnetic skyrmion memristor (MSM) for image classification task (i.e., handwritten digit recognition) \cite{jiang2019physical}.

\textbf{Coupled deoxyribozyme oscillators and DNA oscillators.}
Taking the inspiration of DNA reservoir computing approach \cite{goudarzi2013dna}, \cite{nguyen2020reservoir} proposed a random chemical RC model, where the random chemical circuits (i.e., DNA strand displacement) provide complex non-linear dynamics, making them suitable for RC implementation (Fig. \ref{fig:app_chem}B). 
The proposed model outperforms the previous deoxyribozyme oscillator RC \cite{yahiro2018reservoir} in short and long-term memory tasks. 
%Reservoir Computing Using DNA Oscillators
Another recent RC using DNA oscillators was reported in \cite{liu2022reservoir} which solves the problem of the lack of readout layer \cite{goudarzi2013dna}, 
and then applied to a handwritten digit recognition and a second-order non-linear prediction task.

\textbf{Chemical Reaction Networks (CRNs).}
% Related to DNA oscillators, the chemical reaction networks also show the capabilities to RC implementations.
%Biochemical Reservoir Computing
As mentioned earlier, reservoir's dynamic can be generated by a set of ordinary differential equations (ODEs).
An extension was proposed for single stranded DNA (ssDNA) analysis \cite{nguyen2022sers}.
%A reservoir computing approach for molecular computing
Besides, \cite{yahiro2018reservoir} used a modular framework for molecular computing to implement a RC model. 
The main advantage of this work, compared with previous DNA oscillators \cite{goudarzi2013dna}, is that molecular computing allows tuning the size of CRNs.
%Physical Implementation of Reservoir Computing through Electrochemical Reaction
Another new chemical RC architecture was proposed by \cite{kan2021physical}, where the reservoir is implemented through electrochemical reactions since the chemical dynamic is shown to be computing resources (Fig. \ref{fig:app_chem}C). 
As claimed by the author, the Polyoxometalate molecule in the solution ``increases the diversity of the response current and thus improves their abilities to predict periodic signals''.

\subsection{Environmental}
\textbf{Wind forecasting and wind power generation.}
\cite{chitsazan2019wind} proposed a RC based wind speed and wind direction forecasting model. In fact, the proposed model is a new type of non-linear echo state network, which is discussed in Section \ref{Recent approaches in RC}. 
Instead of deterministic forecasting, a recent study % megumi - "forecasting, a recent study" or "forecasting, recent research" 
focuses on probabilistic wind power forecasting \cite{wang2020novel} by using a time warping invariant echo state network \cite{lukovsevicius2006time}.
In addition, the wind turbines were used as a major source of power generation for smart grids, where the delayed feedback RC was applied for attack detection \cite{hamedani2017reservoir}.

% \textbf{Wind power generation.}


\subsection{Audio and Speech}

\textbf{Audio processing.} 
Audio signals cover a wide range of temporal sequences such as speech, sounds and music. 
%RESERVOIR COMPUTING: A POWERFUL BLACK-BOX FRAMEWORK FOR non-linear AUDIO PROCESSING
In 2009, RC was first applied as a general framework for non-linear audio processing by \cite{holzmann2009reservoir}. 
Three main potential applications were proposed with simulations, including tube amplifier plugin identification, non-linear audio prediction and music information retrieval (MIR). 
RC was claimed suitable for non-linear audio processing because of the inherently temporal processing capability. 
% megumi - should this be "perspective" or "prospect"?
%Real-time audio processing with a cascade of discrete-time delay line based reservoir computers
In terms of real-time audio processing, a cascaded discrete-time RC was proposed for black-box system identification \cite{keuninckx2017real}. Albeit much effort was made to reduce computation consumption, the cascaded structure is stated considerably more complex to tune than the conventional RC.
%Randomly Weighted CNNs for (Music) Audio Classification
In 2019, \cite{pons2019randomly} proposed randomly weighted CNNs for music classification. 
The proposed model shares similarity to RC, where weight connections remain untrained during training.

% % Figure environment removed

\textbf{Speech recognition.} 
%Isolated word recognition with the Liquid State Machine: a case study
The earliest RC application of speech recognition was presented in \cite{verstraeten2005isolated}. Here, an LSM-based RC with spiking integrate-and-fire neurons was implemented recognizing isolated digits, where the readout is trained using ridge regression. 
The problem of the model is that it requires intermediate data storage for offline learning.
%Neuro-Inspired Speech Recognition Based on Reservoir Computing 
Similarly to the work above, \cite{ghani2010neuro} trained output neurons using back-propagation based MLPs.
%A Digital Liquid State Machine With Biologically Inspired Learning and Its Application to Speech Recognition
Inspired by Hebbian learning, \cite{zhang2015digital} further proposed a variant of Hebbian online learning rule to train an LSM without requiring data storage for speech recognition. 
In detail, the analog input speech signal is pre-processed by the \textit{Lyon passive ear} model and further converted into spikes by BSA algorithm \cite{schrauwen2003bsa} before feeding into LSM. 
% (Fig. \ref{fig:app_audio}A). 
% The proposed variant of Hebbian learning rule is shown in Fig. \ref{fig:app_audio}B.
%On Improving the Classfication Capability of Reservoir Computing for Arabic Speech Recognition
Other types of RC can also be applied to speech recognition. 
For example, an ESN combined with extreme kernel machines was used for Arabic speech recognition \cite{alalshekmubarak2014improving}. 
%Role of non-linear data processing on speechrecognition task in the framework of reservoir computing
In 2020, a RC based on nano-oscillators was also applied to TI-46 database \cite{abreu2020role}.

\subsection{Finance}
\textbf{Stock market prediction.}
A successful prediction of a stock's future price could yield significant profit.
%Short-term stock price prediction based on echo state networks
An early attempt of short-term stock price prediction was reported in \cite{lin2009short}, who used an ESN as a basic network with the \textit{Hurst exponent} to select a persistent subseries with the greatest predictability for training from the original training set.
% Short-term stock price prediction based on echo state networks
Instead of using basic ESN, three RC network structures were investigated in stock price prediction \cite{wang2021stock}, including the small-world topology discussed in an earlier section.

\textbf{Financial System modelling.} 
%xA reservoir computing approach for forecasting and regenerating both dynamical and time-delay controlled financial system behavior
\cite{budhiraja2021reservoir} used RC for financial system modelling. In this study, an ESN was first applied to predict a pre-defined financial system behavior. The model was further proved to effectively re-generate only the required data based on limited known information.




% \subsection{Social}


% =============
% =============
% =============

\section{RC with Brain Mechanisms and Cognitive Science}
\label{RC with brain mechanisms and cognitive science}

\subsection{Reservoir in the Cerebral Cortex}
RNNs have been shown to have rich, complex, non-linear and high-dimensional dynamics. 
In the cerebral cortex, especially the prefrontal cortex (PFC), massive recurrent connections of neurons were found, 
% megumi - "(PFC), a massive recurrent connection" or "(PFC), massive recurrent connections" 
and it is progressively recognized that some parts of the brain operate as reservoirs \cite{mante2013context}. 
Moreover, the cortex is shown able to extract the desired outputs (readout) from the high-dimensional neural representations (reservoir). 
In this section, we review studies on using RC to model the cerebral cortex.

\subsubsection{Dominey’s Decade-long Research: the Birth of RC} 
Dominey et al. developed the first RC prototype in a series of neurocognitive studies on corticostriatal systems \cite{dominey1995complex, dominey1995model}. 
During the period of 80s-90s, many researchers were focusing on the characterization of the fast eye movements (i.e., the oculomotor saccade) in the corticostriatal system, which refers to the interactions between cortex and basal ganglia \cite{bruce1985primate}. 
Particularly, \cite{barone1989prefrontal} examined the function of the corticostriatal system by carrying saccade experiments on macaque monkeys.
The experiments showed that some neurons 
(1) have a preferred spatial saccade amplitude and direction; 
(2) selective to response to a particular sequential order. 
In 2013, \cite{rigotti2013importance} characterized this finding as \textit{mixed selectivity}, which became one of the important principles in RC and cognitive science.

Suggested by the two experiments of the \textit{corticostriatal} saccade system \cite{barone1989prefrontal, dominey1992cortico},
the first corticostriatal RC model was built based on (1) a recurrent prefrontal cortex (PFC) system (i.e., the reservoir), and (2) the reward-related learning in PFC-to-caudate connections (i.e., the readout).
Fig. \ref{fig:cog_PFC} shows the architecture of the model.
Since they found that the modification of the recurrent connections are considerably computational costing, they decided to initialize PFC layer (the reservoir) with a mixture of fixed inhibitory and excitatory recurrent connections. 
The reservoir layer was then connected to the \textit{caudate} or \textit{striatum} to obtain the readout. This pioneering RC model, as the author claimed, can be seen as a dedicated temporal recurrent network (TRN), which shows the inherent capabilities and sensitivity to temporal and sequential structure by providing a rich spatio-temporal dynamic \cite{dominey2000neural}. 

% Figure environment removed 

\textbf{Recent extension of TRN.} Dominey et al. further proposed a series of works on the previous corticostriatal RC model. These include a combination of RC and neuro-physiological models of language processing \cite{dominey2009cortico, dominey2009neural}, as well as a performance improvement of the RC learning algorithm \cite{hinaut2013real}. Reader may refer to a more detailed review of the corticostriatal RC model and its historical developments in a review paper by \cite{dominey2013recurrent}.
In 2013, one of the important cortical activities was obtained in randomly connected recurrent networks (e.g., reservoir) and was then characterized as \textit{mixed selectivity} \cite{rigotti2013importance} (see the following section). 
In 2016, the representational power and dynamical properties of mixed selectivity were investigated by training a RC model to perform a complex cognitive explore-exploit task initially developed for monkeys \cite{enel2016reservoir}. By comparing the neural activity of the reservoir and the primate dACC neurons, it is found that not only mixed selectivity was observed in the two types of neurons, but more strikingly, the distributions of neurons were quite similar in terms of the epoch (explore/exploit), the task phase, and the target choice, which strongly supports the argument that the cortex behaves computationally as a reservoir (see Figures 3 and 4 in \cite{enel2016reservoir}).


\subsubsection{Recent Studies of the Cortex}
%revision_mode_2306 
\textcolor{black}{\textbf{Dimensionality implies selectivity.}} 
One of the complex neural activity phenomenons in PFC and in the model by \cite{dominey1995complex} is that the firing rates of some neuron populations were modulated by the combinations of conditions such as spatial location and sequential order \cite{dominey2021cortico}. 
This cortical activity was then characterized as mixed selectivity by \cite{rigotti2013importance} in an object sequence memory task.
Specifically, pure selectivity refers to neurons whose responses are selective only to an individual task-relevant aspect, whereas mixed selectivity refers to neurons whose responses are explained by a non-linear superposition of responses to the individual task-relevant aspects. 
In the object sequence memory task, monkeys were required to watch a sequence of two subsequently displayed images. After that, they had to 
(1) recognize the two images under distraction (recognition task), or
(2) recall the order of the two images (recall task). 
The \textit{dimensionality} of the neural spaces was then estimated (i.e., the minimal number of coordinate axes needed to specify the position of all points in neurons' firing rate space).
It is observed that the dimensionality was higher if neurons having mixed selectivity were included. 
More importantly, the neural population was estimated to have a higher dimensionality when the monkeys performed correctly on a trial. That is, in the error trials, a collapse in dimensionality was observed, which impairs the ability of downstream readout neurons to produce the correct response. 
Moreover, \cite{rigotti2013importance} and \cite{fusi2016neurons} showed that RC models can be compared with a randomly connected recurrent structure in the monkey prefrontal cortex, which can generate high-dimensional mixed selective dynamics to assure the separability in the downstream readout units. 
The higher the dimensionality of the population coding, the better the performance on the task \cite{dominey2021cortico}. 
% non-linear mixed selectivity supports reliable neural computation
Regarding the readout, it was reported that the brain implements mixed selectivity even when it does not enable behaviorally useful linear decoding (i.e., simple linear readout), suggesting that mixed selectivity may be the key of population encoding for reliable and efficient neural representations \cite{johnston2020nonlinear}.

Recent studies have shown that mixed selectivity not only plays an important role in PFC, but in other parts of the brain. 
% Task-dependent mixed selectivity in the subiculum
\cite{ledergerber2021task} found strong mixed selectivity in the subiculum (i.e., the area between the entorhinal cortex and the CA1 subfield), where individual neurons respond conjunctively to task-related aspects including position, head direction, and speed.
%Mixed selectivity coding of sensory and motor social signals in the thalamus of a weakly electric ﬁsh
In 2022, mixed selectivity was observed in the thalamus of a weakly electric fish \cite{wallach2022mixed}. Here, the mixed selectivity strategy was implemented to encode interactions in the recurrent networks in pallium, which is related to courtship and rivalry in terms of dominance in male-male competition and female-mate selection.

% As a side note, % megumi - should this be "note"?
% the finding of mixed selectivity is an important fact that supports the Hopfieldian approach, a new way to understand the brain. 
% % It is shown in the abovementioned works that 
% In the Hopfieldian view, the dimensionality of the activity of neuronal assemblies can be used to explain the complex cognitive behaviors, in which non-linear mixed-selectivity neurons allow more complex representations to take place in PFC.
% A detailed history of the Hopfieldian approach and its developments is summarized in a review paper \cite{barack2021two}.

% \textbf{Reservoir processing of Explore-Exploit task (2016)}

% \textbf{Working memory}

\subsection{Neuronal Oscillations}
% \subsection{Delay-coupled recurrent oscillators}
Neuronal oscillations refer to the temporally structured activity generated in mammalian brains, where neurons undergo periodic changes in excitability. These oscillations had been found in neuron assemblies, a concept to describe the behaviors by a population of neurons. 
In this section, we first present the cognitive science research on neuronal oscillations, and then we discuss the relationship between oscillations and RC models, followed by several examples of the existing research. Contents are partially from \cite{singer2021cerebral}.

\subsubsection{Neuron Assemblies}
\textbf{Feed-forward circuits.}
It is widely believed that there are two frameworks of processing in natural systems in the cortex \cite{singer2021cerebral}: 
(1) convergent feed-forward circuits and 
(2) neuronal assemblies. 
In the framework of feed-forward circuits, specific neurons fire to particular features, and the information is propagated from the former layer to the next higher layer. 
In this way, higher-level features (e.g., cognitive objects) are extracted through a multi-layered structure. 
The encoding scheme here refers to spatial encoding, which is well-suited for simultaneously presenting features such as images. 
However, due to the lack of short-term memory functions, feed-forward circuits are less apt to tackle the relations among temporally segregated events.

\textbf{Neuron assemblies.}
On the other hand, a complementary framework in cognitive brains is the neuronal assemblies. 
Unlike the feed-forward networks which include explicit layered structures, the assemblies of neurons usually form coupled recurrent networks with non-linear, high-dimensional and self-organizing dynamic
\cite{singer2021cerebral}.
Relations among cognitive objects are translated into the weighted connections between neurons; in other words, high-level features are represented by the amplified reverberations (echoing) of neuronal assemblies.
With the reverberating responses, the rich dynamics provided by the coupled recurrent connections have short-term memory (fading memory), and become efficient to handle temporally related sequential events.

\subsubsection{Blinding Problem}
\textbf{Problem of neuronal assemblies.}
One of the challenging problems of the neuron assemblies is the blinding problem, which refers to the segregation of simultaneously active assemblies.
According to \cite{hebb2005organization}, if assemblies were
solely distinguished by enhanced activity (i.e., discharge rate) of the constituting neurons,
it becomes difficult to distinguish which of the more active neurons actually belong to which assembly. Moreover, if the given objects share some common features and overlap in space (e.g., blind source separation and cocktail party problem), the corresponding feature-selective nodes would have to be shared by several assemblies \cite{singer2021cerebral}.

\textbf{Solutions to blinding problem.}
A possible solution is multiplexing, in which various active assemblies are segregated in time. Because of the discharge rate of cortical neurons is relatively low (i.e., the integration needs time), multiplexing becomes problematic if only discharge rate is considered for distinguishing assemblies \cite{tovee1992functional}.
Therefore, it is only capable in a slow timescale \cite{vanrullen2005spike}.

In the 1990s, Gray and Singer proposed that ``neurons temporarily bound into assemblies are distinguished not only by an increase of their discharge rate, but also by the precise synchronization of their action potentials'' \cite{gray1989oscillatory, singer1995visual, singer1999neuronal}. 
They also predicted that neurons that respond to the same sensory object might fire in temporal synchrony, with a precision in the millisecond range. Synchronization by oscillation is briefly introduced in the following section.

\subsubsection{Synchronization by Oscillation}

% % Figure environment removed 

% Figure environment removed 

% Figure environment removed 

\textbf{Definition of synchronization.}
The periodic changes of excitability of the neurons are considered neuronal oscillations among different brain areas \cite{singer2021cerebral}. It has been identified that these oscillations vary in terms of the frequency, ranging from approximately 0.05 Hz to 500 Hz \cite{buzsaki2004neuronal}.
According to \cite{fries2015rhythms}, synchronization by neuronal oscillations has been widely detected across various natural systems, in which different oscillations can ``coexist and often synchronized to each other or nested into each other''.
% \textbf{The spread of synchrony.}
This observation is called the spread of synchrony. 
As an example shown in Fig. \ref{fig:fig_cog_spreadOfSync}, the synchronized larger-scale populations can entrain other smaller local assemblies with different oscillations to be overall synchronized. 
This spread of synchronized activity was then believed to be a reinterpretation of the represented objects \cite{engel2001dynamic}.

\textbf{Arnold tongue regime.}
One of the interesting observations of synchronization behavior in coupled oscillators is the Arnold tongue regime. 
An early experiment by Van Huygens revealed that the beats of pendulum clocks can be synchronized when having the same timber; that is, if the preferred frequencies of the oscillators are similar, weak mutual interactions are enough for oscillatory synchronization \cite{singer2018neuronal}.
This observation was then summarized as Arnold tongue regime by \cite{glass1994periodic}.
As shown in Fig. \ref{fig:fig_cog_arnoldTongue}, the coupling strength should increase in order to assure a stable synchronization when the preferred frequencies between coupled oscillators become increasingly different, thus resulting in a tongue-shaped pattern.
Synchronization would become unstable if the difference between preferred frequencies exceeds a critical point \cite{singer2018neuronal}.

% \subsubsection{\textcolor{black}{A new proposal of natural recurrent networks (Will be deleted very soon, just for references atm)}}
% \begin{quote}
%     A hallmark of natural recurrent networks such as the cerebral cortex is that they are spontaneously active. The dynamics of this resting activity must reflect the weight distributions of the structured network and hence must harbor the entirety of the stored “knowledge” about the statistics of feature contingencies, i.e. the latent priors used for the interpretation of sensory evidence. This predicts that resting activity is high dimensional and represents a vast but constrained manifold inside the universe of all theoretically possible dynamical states. Once input signals become available they are likely to trigger a cascade of effects: They drive in a graded way a subset of feature-sensitive nodes and thereby perturb the network dynamics. If the evidence provided by the input patterns matches well the priors stored in the network architecture, the network dynamics will collapse to a specific substate that provides the best match with the corresponding sensory evidence. Such a substate is expected to have a lower dimensionality and to exhibit less variance than the resting activity, to possess a specific correlation structure and be metastable due to reverberation among nodes supporting the respective substate. Because these processes occur within a very high-dimensional state space, substates induced by different input patterns are likely to be well segregated and therefore easy to classify. As the transition from the high-dimensional resting activity to substates is likely to follow stimulus-specific trajectories, classification of stimulus-specific patterns should be possible once trajectories have sufficiently diverged and before they reach a fixed point. % megumi - should this be "fixed point"?
%     These points of divergence should be reached faster for input patterns that match particularly well with the internal priors.
% \end{quote}


\subsubsection{How RC Relates to Oscillations?}
% \textbf{RC properties.}
Recall that in most of the existing literature, RC models should normally meet several requirements to be efficient and functional for various tasks.
These are also considered the properties of a RC model \cite{tanaka2019recent}, which include
(1) High-dimensionality: low-dimensional inputs are mapped into a high-dimensional space, which allows originally inseparable or temporal inputs to be linearly separable as shown by the Cover Theorem \cite{cover1965geometrical}.
(2) Non-linearity: non-linear mapping transforms the input into linearly separable reservoir states which can be read out by readout layer.
(3) Separation property: a RC model should be capable of separating different inputs into different classes, under small fluctuations or in noisy environments \cite{maass2002real}.
(4) Fading memory: also known as short-term memory or echo state property. This algebraic property eliminates the effect of initial network condition. In other words, it ensures that the reservoir state is dependent on recent-past inputs (reverberating responses), but not distant-past inputs (responses faded).

In a RC review, \cite{singer2021cerebral} made a proposal to link the concept of RC in machine learning and that of neuronal behaviors in the cognitive brain. Readers may refer to the article for details.
Based on the proposal, here we aim to discuss how the dynamics of coupled oscillators in mammalian brains could be exploited to accomplish the abovementioned four characteristics in RC.

% \textbf{Oscillation perspectives.}
% \textbf{Long-term memory.}
% \begin{comment}
% ============ High-dimensionality & non-linear ity ============
\textbf{High-dimensionality and non-linearity.}
The first fact is that the cortex is reported to have consistent and random high-dimensional oscillations, which refers to the ``resting activity'' \cite{buzsaki2006Brain1}. 
%revision_mode_2306 
\textcolor{black}{Meanwhile, it is believed that brains are likely to have an internal model of the external world (i.e., prior knowledge, which can be updated by learning).
}
When the input comes in, the input stimuli “activate” some feature-sensitive neurons, thus making the dynamics of the network collapse into a stimuli-specific substate (e.g., oscillatory synchronization). 
%revision_mode_2306 
\textcolor{black}{
All of these may suggest that once a reservoir enters a substate (i.e., synchronization), it is likely that the dynamics can be tuned selectively to specific stimuli or generating specific output signals.
}

% Figure environment removed 

% ============ Separation property ============
\textbf{Separation property.}
As mentioned above, the stimuli-specific substate with rhythmic oscillations, according to \cite{singer2021cerebral}, ``would have a lower dimensionality and to exhibit less variance than the resting activity, to possess a specific correlation structure and be metastable due to reverberation (rhythmic oscillations) among nodes supporting the respective substate''. 
Note that all these activities, including the stimuli-specific substates, are happening in a high-dimensional state space; thus, different inputs can be well-separated and classified even linearly, similar to the readout of RC models.

% ============ Fading memory ============
\textbf{Fading memory.}
Moreover, fading memory refers to the short-term memory in RC literature, in which the reservoir state should depend on recent inputs but not distant-past inputs. 
From an oscillation point of view, there exist several experiments in the cat’s visual cortex, which can support the mechanism of fading memory  \cite{nikolic2009distributed}. 
As stated in \cite{singer2021cerebral}, these experiments show that ``
% \textit{[COPYING]} 
(1) the information about a particular stimulus persists in the activity of the network for up to a second after the end of the stimulus (fading memory).
(2) Two subsequent stimuli and the order of their presentation can be correctly classified with a linear classifier sometime after the end of the second stimulus, suggesting that the network is capable of performing non-linear XOR operations and
(3) Stimulus identity is distributed across many neurons ($>$30) and encoded both in the rate vector and the temporal correlation structure of the responses''.
The above evidence
% megumi - evidence can't be plural, could change to "the above evidence" or "the three reasons above" if you want to keep the 3
may explain the fading memory from the perspective of neuronal oscillations, suggesting that short-term memory is not only a property of the networks, but also a consequence of the oscillations and reverberations.

In addition, long-term memory is also important, and it is a more complex one. 
Early experiments showed that the “default” state of the unperturbed, sleeping brain is a complex system of numerous self-governed oscillations, particularly in the thalamocortical system \cite{buzsaki2004neuronal, buzsaki2006Brain1}. 
The content of these oscillations reflects spike sequence patterns created by prior waking experience. 
Moreover, these oscillations are spontaneously replayed (e.g., during sleeping), leading to an “off-line” synaptic modification. Such replays might be the way to the formation of long-term memory.
Overall, a reason why short-term memory rather than long-term memory is one of the necessary requirements for building a RC model might be that we usually
keep the random connections in the reservoir fixed without modifications. As a result, RC models generally struggle to form long-term memory, since replays in terms of synaptic modifications are required (i.e., investigating long-term memory is a more challenging task).
In fact, some learning algorithms, such as STDP \cite{caporale2008spike} and FORCE learning \cite{sussillo2009generating}, are trying to modify the synaptic connections, and therefore they are likely to be able to possess long-term memory.

\subsubsection{Examples of Synchronization in RC}
\textbf{Multiple reservoirs.}
Deep reservoir computing was proposed in \cite{gallicchio2017deep}, in which multiple reservoirs are concatenated together to form a hierarchical network structure. Detailed model descriptions are discussed in Section \ref{Recent approaches in RC}. It has been proved that the deep RC structure can achieve
(1) multiple timescale representation, ordered along the network's hierarchy;
(2) multiple frequency representation, where progressively higher layers focus on progressively lower frequencies.
According to the Arnold tongue regime shown in Fig. \ref{fig:fig_cog_arnoldTongue}, if we keep the coupling strengths at a low level (week synaptic links, or even zero weight connections), neurons with similar preferred frequencies can be synchronized, at different frequency bands. Therefore, this may explain the reason why multiple reservoirs can achieve these while the conventional ESN cannot, from the perspective of neuronal oscillations.

\textbf{GACTRNN.}
Gating Adaptive Continuous Time Recurrent Neural Network (GACTRNN), is another research taking the inspirations from neuronal oscillation proposed in machine learning by \cite{heinrich2020learning} in 2020.
It extended the classic RNN to adaptive timescales RNN, which shares some similarities to reservoir computing models. 
% The main difference is that the proposed model is operating in continuous time. 
GACTRNN is claimed to be able to learn to gate its timescale characteristic during activation and thus dynamically change the timescales in processing sequences; in other words, by changing their timescales during processing, neurons can learn to simultaneously represent temporally different primitives (Fig. \ref{fig:TRNN and SW topology}A).
% \textbf{Equations pending..}

\textbf{Small-world topology.} An ESN based on the topology of small-world (SW) wiring was proposed by \cite{kawai2019small} (Fig. \ref{fig:TRNN and SW topology}B). The model incorporated SW structure with RC and further investigated echo state property. It was found that the SW topology plays the roles of both efficient signal propagation and enhancement of the ESP in neural computation.
In fact, this idea partially originated from the cortical anatomical connectivity of the human brain.
According to \cite{buzsaki2004neuronal}, ``complex brains have developed specialized mechanisms for the grouping of principal cells into temporal coalitions''.
In order to reduce the complexity of the connections without excessive wiring, the number of long-range connections between neurons decreases in growing brains; in other words, the synaptic path lengths between distant cell assemblies are reduced, keeping the path lengths short and maintaining fundamental functions.

%%%%=======================
%%%%=======================
%%%%=======================


% \section{Discussion}

\section{Perspectives and Future Research} \label{perspectives}

% \textbf{General overview.}
Reservoir computing is becoming increasingly popular due to its simple network structure, hardware-friendly features, low computational cost, and fast training process.
These benefits enable RC to extend far beyond machine learning into a wide range of research fields.
In this paper, we provide a thorough overview of RC's history, strengths and weaknesses from the perspectives of machine learning, dynamical systems, physics, biology, and neuroscience. We also summarize recent advanced approaches and architectures for RC optimizations and implementations. 
Besides, applications of RC are reviewed, from which we have seen how this interdisciplinary idea can be applied in various research areas.

%revision_mode_2306 
\textcolor{black}{
While RC still remains an unconventional computational framework compared to other machine learning techniques like deep learning, its impact can be enhanced by addressing various challenges. Recent developments have unveiled new directions and perspectives for RC, indicating its untapped potential and promising prospects that may even surpass those of mainstream methods. In this section, we present perspectives and discuss the open problems that motivate further research in this field.
}

\subsection{Reservoir Design and Optimization}
A consensus view of conventional RC is that initializing a random RNN as a reservoir is not the optimal solution, and that connecting a linear readout with the reservoir limits the generation of the downstream responses. 
It is also known that neurons in cortical networks in the brain are not randomly connected, while their structures and synapses exploit an evolutionary and developmental process \cite{subramoney2021reservoirs}.
Recent research, especially on ESNs and LSM, mainly focuses on network structure designs (e.g., deep reservoir), parameter optimizations (e.g., particle swarm optimization) and training rule determinations (e.g., STDP and Hebbian learning).
Even if the optimal synaptic weights were discovered, the performance of various concrete tasks would still vary.
As pointed out by Jaeger \cite{nakajima2021reservoir}, ``currently available insights are mostly distilled from experimental studies of timescale profiles or frequency spectra in input data and provide no comprehensive guides for optimizing reservoir designs''.
In other words, one should find a way to analyze and abstract both the characteristics of input/output and task specifications, which can be used to design the reservoir dynamics.
One possible solution is called reservoir Learning-to-learn (L2L) \cite{subramoney2021reservoirs}, in which a set of (hyper)parameters of the reservoir are optimized by BPTT for a whole family of learning tasks; note that this shares some similarity to meta learning in machine learning and neuroscience. 
This L2L method was investigated on LSM models and it can also be implemented by other RC architectures. 
%revision_mode_2306 
\textcolor{black}{Moreover, to have better and faster learning, it is possible to train the reservoir by the L2L method even without changing synaptic weights to readout neurons.
Nevertheless, whether the performance takes advantage of other reservoirs is still an open question.}

%revision_mode_2306 
\textcolor{black}{
\subsection{Easy-access Tools, Coding frameworks and Recipes}
One pressing open problem in RC is the relative lack of user-friendly coding environments, libraries, and computation frameworks. 
Unlike the well-developed infrastructure supporting deep learning and other scientific computational paradigms, the coding ecosystem for RC remains relatively underdeveloped and fragmented. 
As shown in this paper that although numerous models and architectures have been proposed for RC, there is a notable dearth of unified frameworks that researchers can leverage with ease. 
This poses a significant challenge as it lowers the speed and efficiency of research, requiring additional time and effort to navigate through a variety of individual tools and frameworks, and often necessitates the development of custom code for each research project. 
}

%revision_mode_2306 
\textcolor{black}{
However, this does not imply that no progress is being made towards building a more unified and accessible coding infrastructure for RC. 
Indeed, we have seen some promising developments over the years.
Back in the early 2010s, \cite{lukovsevivcius2012practical} provided a comprehensive guide on how to implement an ESN, which widely impacts future studies. 
The author also released a demonstration of coding ESN from scratch in Julia, Matlab, Octave, Python, and R language. 
In 2012, a toolbox was developed for RC called \textit{Oger} (OrGanic Environment for Reservoir computing) to train and evaluate recurrent neural networks, particularly ESNs and LSMs \cite{verstraeten2012oger, oger2012}.
From 2017 on, several tools were released in terms of different RC models, such as ESNs, LSMs, and FORCE-based algorithms.
The \textit{easyesn} library was released \cite{easyesn2017, thiede2017easyesn}, providing a more easy-to-use API for automatic gradient based hyperparameter tuning (of ridge regression penalty, spectral radius, leaking rate and feedback scaling), as well as transient time estimation. 
Meanwhile, a hands-on LSM implementation using \textit{NEST} simulator in Python was proposed \cite{kaiser2017scaling, gewaltig2007NEST}, which is considered to be the starting point for new researchers who are interested in spiking-based RC models.
%https://arvoelke.github.io/nengolib-docs/master/notebooks/examples/full_force_learning.html
Later, another open-source spiking model framework, \textit{Nengo}, was developed for FORCE learning and its variation implementations \cite{bekolay2014nengo}.
%https://nschaetti.github.io/echotorch.github.io/
In 2018, \textit{EchoTorch} was proposed, and perhaps it is the first Python package to simplify the evaluation and implementation of ESNs and RC \cite{echotorch2018}.
%https://www.mathworks.com/matlabcentral/fileexchange/69402-deepesn
In 2019, a Matlab toolbox for DeepESNs \cite{gallicchio2017deep} was released that extends the RC paradigm towards deep networks. 
One of the most common deep learning frameworks, TensorFlow, also supports the ESN layer in 2020 (see TF Addons). 
% tension: A Python package for FORCE learning
More recently, in 2022, \cite{liu2022tension} present \textit{tension}, an object-oriented, open-source Python package that implements a TensorFlow / Keras API for FORCE learning. 
%ReservoirComputing.jl https://github.com/SciML/ReservoirComputing.jl
Another Julia package for RC is \textit{ReservoirComputing.jl} \cite{martinuzzi2022reservoircomputing}. It aims to provide a simple and flexible framework to work with ESNs and other models.
%https://github.com/reservoirpy/publications/tree/main/2022-SAB Create Efficient and Complex Reservoir Computing Architectures with ReservoirPy
Additionally, \cite{trouvain2022create} present a Python library that facilitates the creation of RC architectures, from ESNs and FORCE learning, to complex networks such as DeepESNs and other advanced architectures with complex connectivity between multiple reservoirs with feedback loops.
}

%revision_mode_2306 
\textcolor{black}{
Despite progress, the full potential of RC is yet to be realized, and the goal of a unified and accessible environment for RC still eludes us. There is a need for more work in this area to ensure that the full potential of RC can be explored and utilized effectively.
}

% Figure environment removed 


\subsection{Physical RC and Extremely Efficient Hardware}
As mentioned earlier, RC has reawakened and gained attention because of the fast development of PRC designs.
Unlike conventional RCs that suffer from the information processing speed limit, PRCs can overcome this limit and process massive amounts of data in real-time. 
We review PRC models that use different physical materials from different areas such as electronics, optics, chemistry, and quantum. 
% The PRC is thoroughly discussed in \cite{tanaka2019recent, nakajima2021reservoir}.
As more and more PRC approaches are being proposed, there are several open problems to be solved. 
For example, some PRCs get rid of the massive recurrent connections in RNNs, yet they are hard to design and tune (e.g., implementing a delayed feedback loop of the single node reservoir is quite a challenging task). 
Similarly, setting hyperparameters for PRC is not straightforward.
% %Reservoir Computing in Material Substrates
In 2021, \cite{dale2021reservoir} proposed a framework to evaluate what makes a good computing substrate, providing a new perspective on how to build and compare physical reservoir computers.

%revision_mode_2306 
\textcolor{black}{
Another challenge lies in harnessing the full potential of RC for machine learning applications and achieving highly efficient hardware implementations. Very recently, this issue has been explored and discussed in \cite{tanaka2022guest}, where the future directions of inquiry are segmented into three categories.
Simply put, the first category focuses on the theoretical aspects of RC, aiming to drive efficient model design and ensure reliable RC applications. This includes works like reservoir memory machines \cite{paassen2022reservoir}, consistency capacity \cite{jungling2022consistency} and curve fitting abilities \cite{manjunath2022echo} analysis.
The second category delves into the exploration of novel model designs and applications of RC, with an aim to enhance computational performance and efficiency in tasks related to pattern recognition. Possible solutions include (1) industrial applications such as adaptive practical nonlinear model predictive control \cite{schwedersky2022adaptive} and digital twins \cite{kong2023reservoir}; (2) integrating RC with deep learning methods such as convolutional and graph neural networks \cite{jalalvand2022real, pasa2022multiresolution, pedrelli2022hierarchical, tong2018reservoir, tanaka2022reservoir}.
Lastly, the third category is to keep investigating new architectures and mechanisms in physical hardware that are suitable for RC implementations. Latest research includes a new FPGA-based RC for low-power pattern recognition \cite{gupta2022neuromorphic}, networks based on the Schrödinger equation \cite{nakajima2022neural}, and a new cellular automata implementing rule (CA90) \cite{kleyko2022cellular}.
Overall, these studies use a wide range of new hardware, showcasing efficient RC-based methods and stimulating further growth in this research domain.
}




\subsection{RC with Cognitive Science and Neuroscience}
In terms of modelling RC in cognitive science and neuroscience, we have reviewed several mechanisms of cognitive brains and tried to bridge the concept of neuron population to RC, such as \textit{mixed selectivity} \cite{rigotti2013importance} which reveals that the prefrontal cortex can generate high-dimensional mixed selective dynamics to assure the separability in the downstream readout units.
% Future research directions might be to keep investigating the biological characteristics of the reservoir to get a better understanding of human brains.
Future research directions include finding new analogies for biological characteristics in RC, allowing for a deeper understanding of brain's and bodies' mechanisms.


%revision_mode_2306 
\textcolor{black}{
The domain of oscillation with synchronization, as previously discussed, is one area warranting further investigation. 
In fact, progress has been made in using ESNs to produce oscillatory outputs without any inputs, by mimicking the central pattern generators (CPGs) shown to be involved in rhythmic human movement \cite{cpg1, cpg2, cpg3, cpg4}.
CPGs are important circuits present in the neural system of live beings. In fact, vertebrates have a spinal cord composed of many of CPG circuits, and it was shown that the spinal cord and mostly CPGs are sufficient for complex locomotion (e.g., walking and running in cats) \cite{yuste2005cortex, duysens1998neural}.
In 2023, Tham and Vargas \cite{foong2023generating} show that even the most basic ESN can be trained to reproduce the trajectory of dynamical systems from simple sinusoidal and square waves to complex Lorenz chaotic time series with high precision, without any external excitation. }
\textcolor{black}{
Fig. \ref{fig:soesn} depicts the different probabilities of having a reservoir that triggers oscillations in terms of both leaking rate and spectral radius (i.e., the echo state property, ESP), where pure yellow represents the highest probability. 
Here, it is important to note that the ESP, by design, typically restricts spontaneous oscillation, as it ensures that the reservoir’s internal state should eventually lose memory of its initial conditions, hence creating a fading ``echo'' of past inputs. 
This can be seen in the cases of Fig. \ref{fig:soesn}C-D, that the reservoirs' states converge to 0 and remain stable afterward (refer to  damped oscillation in \cite{foong2023generating}). 
In contrast, as shown in Fig. \ref{fig:soesn}E, oscillations occur when the ESP is most likely not to be guaranteed.
Therefore, this study clearly indicates the necessity for additional research, particularly regarding the outcomes and implications when the ESP is not strictly adhered to.
}


On the other hand, high-dimensionality is another feature shared by the encoding of neuron populations and RC models. 
Recent studies have also proposed other approaches to understanding the role of high-dimensionality in biological and artificial neural networks.
% Neural population geometry: An approach for understanding biological and artificial neural networks
Neural population geometry, for example, is an approach that provides a useful population-level mechanistic descriptor underlying task implementation.
% Interpreting neural computations by examining the intrinsic and embedding dimensionality of neural activity
Here, the geometry of representation can be represented by high-dimensional neural activity, and it is further observed that the neural activity lies on lower-dimensional subspaces, i.e., the so-called intrinsic dimensionality or neural manifolds \cite{jazayeri2021interpreting}.
Similar to the reservoir responses, these lower-dimensional subspaces can then be well-separated by using simple classifiers.

Although all of these remain in an early stage of development,
% It will certainly make a big breakthrough once we clearly find out the biological mechanisms in the reservoirs, for not only artificial but also natural intelligence.
the investigations look promising for getting deeper insights into both artificial and natural intelligence.




%revision_mode_2306 
% \textcolor{black}{
% \subsection{RC from an evolutionary perspective}
% The existing literature predominantly concentrates on the applications of RC in artificial intelligence and neuroscience, elucidating its strengths in these areas. 
% Recently, however, some researchers are exploring RC through an evolutionary lens, which brings a fresh angle to the discussion. 
% \cite{seoane2019evolutionary} post several critical questions about RC's utilization, prominence, and evolutionary stability. They also query why, despite RC's power and simplicity, it is not more commonly utilized in biological organisms or more prominent in engineering. As suggested, potential reasons, from an evolutionary perspective, might include the need for systems to specialize or scale up, which could make the generalized properties of the RC less useful. For example, when signals need to travel longer distances, accommodate wildly varying timescales, or perform highly specific functions, the RC paradigm might be less efficient compared to other options. Consequently, the system might evolve into fine-tuned, dedicated circuits using the raw material that reservoirs provide (i.e., basic components of constructing reservoirs).
% As a key hypothesis, these could be explored computationally through simulations, thus posing open research questions at the intersection of computation and evolution.
% }

%revision_mode_2307
\subsection{RC from an Evolutionary Perspective}
In addition to artificial intelligence and neuroscience, some researchers are exploring RC through an evolutionary lens, which brings a fresh angle to the discussion.
Natural systems exhibiting reservoir-like behaviors are prevalent in nature (e.g., nonlinearities in liquids \cite{maass2002real}, soft robotic in muscles \cite{nakajima2013soft}, electric and chemical dynamics in neural networks \cite{maass2004methods, nguyen2020reservoir}, and brain mechanisms \cite{dominey1995complex, dominey1995model, rigotti2013importance, singer2021cerebral}). 
This suggests that such reservoirs might have \textit{evolved} as advantageous structures for processing complex information, similarly to other computational approaches such as feed-forward networks \cite{lecun2015deep}, attractor networks \cite{hopfield1982neural}, and self-organized maps \cite{kohonen1982self} that have been influenced by and have influenced biology. 
% There is some evidence indicating that RC may be employed by neural circuits and various body parts. 
As stated by \cite{seoane2019evolutionary}, however, the current evidence is not as compelling as the well-established similarities between, for example, the structure of the human visual system and deep convolutional neural networks. 
% As such, some open questions remain: 
% is RC rare in biological systems, or just simply under-explored? 
From an evolutionary perspective, \cite{seoane2019evolutionary} argue that although reservoir-like systems may initially emerge due to their simplicity, their long-term persistence could be hindered by evolutionary trends towards specialization and scaling. 
In other word, as the reservoir evolves to specialize, integrate diverse sensory information, or scale up, the generalizing properties of the reservoir may become less advantageous compared to highly specialized circuits.
This leads to another question: under what conditions might reservoirs maintain their original architecture with redundant dynamics, and when might they evolve towards more specialized configurations? 
Overall, understanding the evolutionary constraints is essential in evaluating its potential and limitations both in biological systems and engineering applications.




\begin{comment}
\subsection{The Bottleneck in RC}



% Scale, spatial corr, complexity, parameters, problems that cannot solved, feature quality / diversity, how to get deeper in terms of features.
%revision_mode_2306 
\textcolor{black}{
Although RC overcomes several crucial issues in RNNs such as gradient explosion or vanishing, it still encounters inherent challenges.
Thus, a discussion on the bottlenecks in RC is essential in paving the way for future improvements in this area.
}


%revision_mode_2306 
\textcolor{black}{
\textbf{Solutions to parallelizing sequential processing.} The first issue is the difficulty in parallelizing time sequence processing, particularly in ESN-based RC, given its intrinsic dependence on temporal data structures. 
Currently, the sequential nature of time-series data limits the acceleration benefits derived from parallel computing, thus posing a challenge for the scalability of RC models in time-critical applications. 
}

%revision_mode_2306 
\textcolor{black}{
\textbf{Limitation of physical RC.} For physical RC, the large computational amount due to the massive recurrent connections are not the issue, as the central aim of using a hardware device is to address this. However, the computation speed of a PRC model is generally limited by the readout, given that many models still rely on software-based readout computation. 
To overcome this, one potential solution is to train the PRC readout directly in non-digital mediums rather than transferring data back to a standard digital computer.
Jaeger also pointed out that ``physical reservoirs may only become practically useful when appropriate auto-calibration or homeostatic regulatory mechanisms are realized in combination with numerically robust and swiftly self-adapting readout processes'', \cite{nakajima2021reservoir}.
}

%revision_mode_2306 
\textcolor{black}{
\textbf{Finding a better readout.}
Related to the readout layer, the second challenge involves how to expand the ways to build the downstream layers of the reservoirs.
The conventional approach to read out the high-dimensional responses from the reservoir is linear regression. 
Although this strategy aligns with the principle that an efficient reservoir should exhibit linear separability, it limits the capability of RC models in exploring more complex correlations within the reservoir's responses. 
Some studies have proposed integrating other learning techniques, such as multi-layer perceptrons, to achieve better separability. 
However, these techniques do not significantly increase the non-linearity. 
Thus, there is a critical need for future research to investigate novel approaches to interpret the reservoir's responses. 
For instance, monitoring the correlations among the neurons' activations may provide valuable insights. 
This strategy is inspired by the effective practice in neuroscience, where the identification of similar neuronal activity aids in estimating the correlations of different parts of the brain \cite{senden2018task}. 
}

%revision_mode_2306 
\textcolor{black}{
\textbf{Challenges of capturing spatial correlations alone.}
A key drawback of RC lies in its limited capacity to capture spatial correlations, despite its impressive performance in finding and generating temporal features. 
When analyzing prior works, such as \cite{jalalvand2018application, tong2018reservoir, tanaka2022reservoir, chang2020reinforcement, chang2019convolutional, yoshihiro2020}, that utilize RC for image recognition tasks, it is evident that the spatial feature maps are mostly extracted by convolutional layers that link the input image to the reservoir (see Table \ref{tab:performance}). 
These convolutional layers pre-process the image and render it as intermediate representations that the reservoir can interpret temporally. 
As suggested by \cite{jalalvand2018application}, ``in tasks such as CIFAR-10, intermediate representations obtained/learned via 2-dimensional (2D) convolution seem to be crucial in obtaining state-of-the-art results. Hence, we do expect that on tasks such as CIFAR-10, RCNs (i.e., reservoir computing networks) will need good intermediate representations based on some form of convolution as well in order to obtain competitive results''.
However, it has been shown that CNNs (1) need to fine-tune a lot of hyperparameters; (2) are likely to misclassify if small perturbations are added to original samples \cite{su2019one, kotyan2020evolving, kotyan2022adversarial}. This implies a significant challenge in the development of RC that can extract features with higher quality, diversity and depth, without using alternative approaches for intermediate representations (pre-processing).
}



\begin{table}[]
\renewcommand{\arraystretch}{1.0}
\centering
\caption{%revision_mode_2306 
\textcolor{black}{Comparison of the \textit{Accuracy} (\%) of recent RC models in image recognition benchmarks. Although RC models demonstrate competitive performances with other machine learning methods like CNNs on simpler datasets such as MNIST, their performance significantly declines when faced with datasets exhibiting higher spatial complexity, such as CIFAR-10. 
This performance gap highlights the inherent limitations of RC in dealing with complex spatial correlations, and underscores the need for further exploration and investigations in computer vision.}
}
\resizebox{0.99\linewidth}{!}{%
\begin{tabular}{lclrrr}
\hline \hline
\multicolumn{1}{c}{\textbf{Model Type}} &
  \textbf{Model} &
   &
  \multicolumn{1}{c}{\textbf{MNIST}} &
  \multicolumn{1}{c}{\textbf{Fashion MNIST}} &
  \multicolumn{1}{c}{\textbf{CIFAR-10}} \\ \hline \hline
\textbf{Standard RC}                       & \cite{schaetti2016echo} &  & 99.07\% & -       & -       \\ \cline{1-2} \cline{4-6} 
\multirow{4}{*}{\textbf{Hybrid (CNN+RC)}}  & \cite{jalalvand2018application} &  & 99.19\% & -       & -       \\
                                           & \cite{tong2018reservoir} &  & 99.25\% & -       & -       \\
                                           & \cite{yoshihiro2020} &  & 98.71\% & 86.27\% & -       \\
                                           & \cite{tanaka2022reservoir} &  & 98.38\% & 91.04\% & 64.49\% \\ \cline{1-2} \cline{4-6} 
\multirow{4}{*}{\textbf{Physical RC}}              & \cite{jacobson2021image} &  & 97.70\% & -       & -       \\
                                           & \cite{tran2019hierarchical} &  & 73.86\% & -       & 12.96\% \\
                                           & \cite{yang2022optical} &  & 98.20\% & 89.90\% & -       \\
                                           & \cite{moran2018reservoir} &  & 98.08\% & -       & -       \\ \cline{1-2} \cline{4-6} 
\multirow{2}{*}{\textbf{Hybrid (CNN+PRC)}} & \cite{jacobson2021hybrid} &  & 98.90\% & -       & -       \\
                                           & \cite{an2020unified} &  & 99.03\% & -       & 60.57\% \\ \hline \hline
\end{tabular}
}
\label{tab:performance}
\end{table}




%revision_mode_2306 
\textcolor{black}{
\textbf{Retinotopy and spatiotopy.}
In conjunction with the difficulty in capturing spatial correlations, RC models encounter an issue in handling \textit{retinotopic} signals and \textit{spatiotopic} representations \cite{zimmermann2016spatiotopic, turi2012spatiotopic}.
To illustrate, \textit{retinotopy} refers to the mapping of spatial information based on the location of a stimulus on the retina, whereas \textit{spatiotopy} represents spatial information in world-based coordinates \cite{cavanagh2010visual}. 
In the context of RC, two-dimensional image inputs are typically flattened into one-dimensional signals before being fed into the reservoir, and herein lies the issue: unlike CNNs that use sliding window for convolution, in RC, different positions or scales of the same object within an image can yield vastly different responses from the reservoir.
This inherent spatial and scale sensitivity of RC becomes a crucial limitation when compared to the relative insensitivity of CNNs.
In other word, what the reservoir responds to is the \textit{retinotopic} signal, and it is hard to interpret the responses of \textit{retinotopic} signal into \textit{spatiotopic} representations. 
This distinction becomes crucial when we consider datasets with complex images, where RC struggles to form a unified consensus on the spatial information. 
As shown in Table \ref{tab:performance}, only very few RC models can solve datasets that are more complex than MNIST, from which two out of three used hybrid structures that integrate convolutional layers to RC.
This contrasts with datasets similar to MNIST, where \textit{spatiotopy} is ensured, as the receptive fields, either spatially or temporally, are fixed. 
See also \cite{chang2020reinforcement, chang2019convolutional} in which the RC only gets image input with relative content changes and continuous transitions. 
% These attributes contribute to the reliable and consistent responses of RC models. 
}

%revision_mode_2306 
\textcolor{black}{In short, as the complexity of image data increases, RC models encounter difficulties in generating reliable responses similarly to those of recurrent networks in the cerebral cortex.
Hence, future research should address the \textit{retinotopic-to-spatiotopic} representations, so as to better handle spatial correlations.
}





% %revision_mode_2307 
% \textcolor{black}{
% Although RC overcomes several crucial issues in RNNs such as gradient explosion or vanishing, it still encounters inherent challenges.
% Thus, a discussion on the bottlenecks in RC is essential in paving the way for future improvements in this area. In this section, we highlight two issues associated with the input (capturing spatial correlation) and output (readout) of the reservoir.
% }


% %revision_mode_23067
% \textcolor{black}{
% \textbf{New way to readout.}
% % One challenge involves how to expand the ways to build the downstream layers of the reservoirs.
% The conventional approach to read out the high-dimensional responses from the reservoir is linear regression. 
% Although this strategy aligns with the principle that an efficient reservoir should exhibit linear separability, it may limit the capability of RC in exploring more complex neurons' correlations in different timescales. 
% % Some studies have proposed integrating more complicated learning techniques, such as multi-layer perceptrons and support vector machines, to achieve better separability, yet these techniques do not show a relevant performance increase. 
% One possible solution is monitoring the correlations among the neurons' activations over time.
% % Thus, there is a critical need for future research to investigate novel approaches to interpret the reservoir's responses. 
% % For instance, monitoring the correlations among the neurons' activations may provide valuable insights. 
% It is inspired by the effective practice in neuroscience, where the identification of similar neuronal activity aids in estimating the correlations of different parts of the brain \cite{senden2018task, fornito2019bridging, seidlitz2018morphometric}. 
% To illustrate, during training, neurons' activations can be recorded over time for different input signals. 
% From these recorded activations, the correlation matrix is computed. 
% Each element of the matrix represents how the activity of one neuron correlates with another. 
% This correlation matrix can be thresholded and then utilized as a readout mechanism (e.g, see Fig. 6 in \cite{vavsa2022null}).
% Compared with simple linear mappings, it can temporally capture the higher-order relationships between different neurons’ activations, which may be more robust and representative of the underlying dynamics of the system.
% }

% %revision_mode_23067
% \textcolor{black}{
% Regarding the physical RC, the computation speed is generally limited by the readout, given that many models still rely on software-based readout computation. 
% To overcome this, one potential solution is to train the physical RC readout directly in non-digital mediums rather than transferring data back to a standard digital computer.
% Jaeger also pointed out that ``physical reservoirs may only become practically useful when appropriate auto-calibration or homeostatic regulatory mechanisms are realized in combination with numerically robust and swiftly self-adapting readout processes'' \cite{nakajima2021reservoir}.
% }

%revision_mode_2307 
\textcolor{black}{
\textbf{Challenges of capturing spatial correlations alone.}
Despite RC's impressive performance in finding and generating temporal features, a bottleneck lies in its limited capacity to capture spatial correlations. When dealing with 2D or 3D inputs, such as images or videos, data are usually flattened into 1D signals prior to feeding them into the reservoir. However, this approach faces challenges, as variations in position or scale of the same object within an image may lead to vastly different responses from the reservoir. This inherent spatial and scale sensitivity becomes a significant limitation, which further leads to excessive activity in capturing input features that might never be actually used \cite{seoane2019evolutionary} (see Table \ref{tab:performance} the performance comparison between different RC models with increasing data complexity). When dealing with 2D or 3D inputs, such as images or videos, convolutional layers are employed as a common approach to obtain the intermediate representation which the reservoir can process temporally \cite{jalalvand2018application, tong2018reservoir, tanaka2022reservoir, chang2020reinforcement, chang2019convolutional, yoshihiro2020} (see Table \ref{tab:performance}). However, studies have proved that deep networks are likely to misclassify even if small perturbations are added to original samples \cite{goodfellow2014explaining, moosavi2017universal, tsipras2018robustness, su2019one, kotyan2020evolving, kotyan2022adversarial}. Thus, this highlights a strong incentive for RC-based methods to tackle high-dimensional inputs with strong 2D/3D correlations, as it was shown that higher degrees of nonlinearity in the model are related to more robust neural networks, and nonlinearity is where the RC really shines. Would it be possible to create high accuracy methods solely based on RC?  Moreover, would these methods show a path to the solution of multiple vulnerabilities that plague much of the deep learning research for years? These are all issues that need further exploration and investigations in the fields of both RC and computer vision.
}





\end{comment}

\subsection{Hybrids and New Foundations}
Last but not least, some recent works from 2019 have merged RC with other systems or paradigms such as deep learning \cite{gallicchio2017deep, long2019evolving, challita2019interference, zhou2020deep, li2022multi, liu2022reservoir, jalalvand2022real, pasa2022multiresolution, pedrelli2022hierarchical, tong2018reservoir, tanaka2022reservoir}. Moreover, some authors have started investigating modifying the foundations (e.g., NG-RC \cite{gauthier2021next}).
In fact, research on random networks \cite{carroll2019network, kawai2019small, cucchi2021reservoir, berner2021patterns} shows that there is a wide range of methods taking inspiration from some core mechanics of RC to build novel approaches that might challenge reservoir computing's current foundations in the near future.



\section*{Acknowledgment}
This work was supported by JSPS Grant-in-Aid for Challenging Exploratory Research—Grant Number JP22534665, JST Strategic Basic Research Promotion Program (AIP Accelerated Research)—Grant Number 22584686, JSPS Research on Academic Transformation Areas (A)—Grant Number 22572551, JST SPRING—Grant Number JPMJSP2136.

\begin{comment}
\begin{thebibliography}{100}

\bibitem{lecun2015deep}
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
\newblock Deep learning.
\newblock {\em nature}, 521(7553):436--444, 2015.

\bibitem{singer2021cerebral}
Wolf Singer.
\newblock The cerebral cortex: A delay-coupled recurrent oscillator network?
\newblock In {\em Reservoir Computing}, pages 3--28. Springer, 2021.

\bibitem{rawat2017deep}
Waseem Rawat and Zenghui Wang.
\newblock Deep convolutional neural networks for image classification: A
  comprehensive review.
\newblock {\em Neural computation}, 29(9):2352--2449, 2017.

\bibitem{zhao2019object}
Zhong-Qiu Zhao, Peng Zheng, Shou-tao Xu, and Xindong Wu.
\newblock Object detection with deep learning: A review.
\newblock {\em IEEE transactions on neural networks and learning systems},
  30(11):3212--3232, 2019.

\bibitem{garcia2017review}
Alberto Garcia-Garcia, Sergio Orts-Escolano, Sergiu Oprea, Victor
  Villena-Martinez, and Jose Garcia-Rodriguez.
\newblock A review on deep learning techniques applied to semantic
  segmentation.
\newblock {\em arXiv preprint arXiv:1704.06857}, 2017.

\bibitem{ribeiro2020beyond}
Ant{\^o}nio~H Ribeiro, Koen Tiels, Luis~A Aguirre, and Thomas Sch{\"o}n.
\newblock Beyond exploding and vanishing gradients: analysing rnn training
  using attractors and smoothness.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 2370--2380. PMLR, 2020.

\bibitem{hanin2018neural}
Boris Hanin.
\newblock Which neural net architectures give rise to exploding and vanishing
  gradients?
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{werbos1990backpropagation}
Paul~J Werbos.
\newblock Backpropagation through time: what it does and how to do it.
\newblock {\em Proceedings of the IEEE}, 78(10):1550--1560, 1990.

\bibitem{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural computation}, 9(8):1735--1780, 1997.

\bibitem{jaeger2001echo}
Herbert Jaeger.
\newblock The “echo state” approach to analysing and training recurrent
  neural networks-with an erratum note.
\newblock {\em Bonn, Germany: German National Research Center for Information
  Technology GMD Technical Report}, 148(34):13, 2001.

\bibitem{maass2002real}
Wolfgang Maass, Thomas Natschl{\"a}ger, and Henry Markram.
\newblock Real-time computing without stable states: A new framework for neural
  computation based on perturbations.
\newblock {\em Neural computation}, 14(11):2531--2560, 2002.

\bibitem{steil2004backpropagation}
Jochen~J Steil.
\newblock Backpropagation-decorrelation: online recurrent learning with o (n)
  complexity.
\newblock In {\em 2004 IEEE international joint conference on neural networks
  (IEEE Cat. No. 04CH37541)}, volume~2, pages 843--848. IEEE, 2004.

\bibitem{dominey1995complex}
Peter~F Dominey.
\newblock Complex sensory-motor sequence learning based on recurrent state
  representation and reinforcement learning.
\newblock {\em Biological cybernetics}, 73(3):265--274, 1995.

\bibitem{verstraeten2007experimental}
David Verstraeten, Benjamin Schrauwen, Michiel d’Haene, and Dirk Stroobandt.
\newblock An experimental unification of reservoir computing methods.
\newblock {\em Neural networks}, 20(3):391--403, 2007.

\bibitem{lukovsevivcius2009reservoir}
Mantas Luko{\v{s}}evi{\v{c}}ius and Herbert Jaeger.
\newblock Reservoir computing approaches to recurrent neural network training.
\newblock {\em Computer Science Review}, 3(3):127--149, 2009.

\bibitem{schrauwen2007overview}
Benjamin Schrauwen, David Verstraeten, and Jan Van~Campenhout.
\newblock An overview of reservoir computing: theory, applications and
  implementations.
\newblock In {\em Proceedings of the 15th european symposium on artificial
  neural networks. p. 471-482 2007}, pages 471--482, 2007.

\bibitem{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock {\em arXiv preprint arXiv:1412.6572}, 2014.

\bibitem{moosavi2017universal}
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal
  Frossard.
\newblock Universal adversarial perturbations.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1765--1773, 2017.

\bibitem{tsipras2018robustness}
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and
  Aleksander Madry.
\newblock Robustness may be at odds with accuracy.
\newblock {\em arXiv preprint arXiv:1805.12152}, 2018.

\bibitem{su2019one}
Jiawei Su, Danilo~Vasconcellos Vargas, and Kouichi Sakurai.
\newblock One pixel attack for fooling deep neural networks.
\newblock {\em IEEE Transactions on Evolutionary Computation}, 23(5):828--841,
  2019.

\bibitem{kotyan2020evolving}
Shashank Kotyan and Danilo~Vasconcellos Vargas.
\newblock Evolving robust neural architectures to defend from adversarial
  attacks, 2020.

\bibitem{kotyan2022adversarial}
Shashank Kotyan and Danilo~Vasconcellos Vargas.
\newblock Adversarial robustness assessment: Why in evaluation both {L$_0$} and
  {L$_\infty$} attacks are necessary.
\newblock {\em Plos one}, 17(4):e0265723, 2022.

\bibitem{tanaka2019recent}
Gouhei Tanaka, Toshiyuki Yamane, Jean~Benoit H{\'e}roux, Ryosho Nakane, Naoki
  Kanazawa, Seiji Takeda, Hidetoshi Numata, Daiju Nakano, and Akira Hirose.
\newblock Recent advances in physical reservoir computing: A review.
\newblock {\em Neural Networks}, 115:100--123, 2019.

\bibitem{rigotti2013importance}
Mattia Rigotti, Omri Barak, Melissa~R Warden, Xiao-Jing Wang, Nathaniel~D Daw,
  Earl~K Miller, and Stefano Fusi.
\newblock The importance of mixed selectivity in complex cognitive tasks.
\newblock {\em Nature}, 497(7451):585--590, 2013.

\bibitem{bruce1985primate}
Charles~J Bruce and Michael~E Goldberg.
\newblock Primate frontal eye fields. i. single neurons discharging before
  saccades.
\newblock {\em Journal of neurophysiology}, 53(3):603--635, 1985.

\bibitem{barone1989prefrontal}
P~Barone and J-P Joseph.
\newblock Prefrontal cortex and spatial sequencing in macaque monkey.
\newblock {\em Experimental brain research}, 78(3):447--464, 1989.

\bibitem{dominey1992cortico}
Peter~F Dominey and Michael~A Arbib.
\newblock A cortico-subcortical model for generation of spatially accurate
  sequential saccades.
\newblock {\em Cerebral cortex}, 2(2):153--175, 1992.

\bibitem{dominey1995model}
Peter Dominey, Michael Arbib, and Jean-Paul Joseph.
\newblock A model of corticostriatal plasticity for learning oculomotor
  associations and sequences.
\newblock {\em Journal of cognitive neuroscience}, 7(3):311--336, 1995.

\bibitem{hopfield1982neural}
John~J Hopfield.
\newblock Neural networks and physical systems with emergent collective
  computational abilities.
\newblock {\em Proceedings of the national academy of sciences},
  79(8):2554--2558, 1982.

\bibitem{rumelhart1985learning}
David~E Rumelhart, Geoffrey~E Hinton, and Ronald~J Williams.
\newblock Learning internal representations by error propagation.
\newblock Technical report, California Univ San Diego La Jolla Inst for
  Cognitive Science, 1985.

\bibitem{williams1989learning}
Ronald~J Williams and David Zipser.
\newblock A learning algorithm for continually running fully recurrent neural
  networks.
\newblock {\em Neural computation}, 1(2):270--280, 1989.

\bibitem{atiya2000new}
Amir~F Atiya and Alexander~G Parlos.
\newblock New results on recurrent network training: unifying the algorithms
  and accelerating convergence.
\newblock {\em IEEE transactions on neural networks}, 11(3):697--709, 2000.

\bibitem{lukovsevivcius2012practical}
Mantas Luko{\v{s}}evi{\v{c}}ius.
\newblock A practical guide to applying echo state networks.
\newblock In {\em Neural networks: Tricks of the trade}, pages 659--686.
  Springer, 2012.

\bibitem{hoerl1970ridge}
Arthur~E Hoerl and Robert~W Kennard.
\newblock Ridge regression: Biased estimation for nonorthogonal problems.
\newblock {\em Technometrics}, 12(1):55--67, 1970.

\bibitem{sussillo2009generating}
David Sussillo and Larry~F Abbott.
\newblock Generating coherent patterns of activity from chaotic neural
  networks.
\newblock {\em Neuron}, 63(4):544--557, 2009.

\bibitem{zhong2017genetic}
Shisheng Zhong, Xiaolong Xie, Lin Lin, and Fang Wang.
\newblock Genetic algorithm optimized double-reservoir echo state network for
  multi-regime time series prediction.
\newblock {\em Neurocomputing}, 238:191--204, 2017.

\bibitem{thiede2019gradient}
Luca~Anthony Thiede and Ulrich Parlitz.
\newblock Gradient based hyperparameter optimization in echo state networks.
\newblock {\em Neural Networks}, 115:23--29, 2019.

\bibitem{wang2015optimizing}
Heshan Wang and Xuefeng Yan.
\newblock Optimizing the echo state network with a binary particle swarm
  optimization algorithm.
\newblock {\em Knowledge-Based Systems}, 86:182--193, 2015.

\bibitem{yildiz2012re}
Izzet~B Yildiz, Herbert Jaeger, and Stefan~J Kiebel.
\newblock Re-visiting the echo state property.
\newblock {\em Neural networks}, 35:1--9, 2012.

\bibitem{gallicchio2018chasing}
Claudio Gallicchio.
\newblock Chasing the echo state property.
\newblock {\em arXiv preprint arXiv:1811.10892}, 2018.

\bibitem{lukovsevicius2012reservoir}
Mantas Luko{\v{s}}evicius.
\newblock Reservoir computing and self-organized neural hierarchies.
\newblock {\em Jacobs University, Bremen}, 2012.

\bibitem{jaeger2002short}
Herbert Jaeger.
\newblock Short term memory in echo state networks. gmd-report 152.
\newblock In {\em GMD-German National Research Institute for Computer Science
  (2002), http://www. faculty. jacobs-university.
  de/hjaeger/pubs/STMEchoStatesTechRep. pdf}. Citeseer, 2002.

\bibitem{bertschinger2004real}
Nils Bertschinger and Thomas Natschl{\"a}ger.
\newblock Real-time computation at the edge of chaos in recurrent neural
  networks.
\newblock {\em Neural computation}, 16(7):1413--1436, 2004.

\bibitem{legenstein2007edge}
Robert Legenstein and Wolfgang Maass.
\newblock Edge of chaos and prediction of computational performance for neural
  circuit models.
\newblock {\em Neural networks}, 20(3):323--334, 2007.

\bibitem{nakajima2021reservoir}
Kohei Nakajima and Ingo Fischer.
\newblock {\em Reservoir Computing}.
\newblock Springer, 2021.

\bibitem{cramer2020control}
Benjamin Cramer, David St{\"o}ckel, Markus Kreft, Michael Wibral, Johannes
  Schemmel, Karlheinz Meier, and Viola Priesemann.
\newblock Control of criticality and computation in spiking neuromorphic
  networks with plasticity.
\newblock {\em Nature communications}, 11(1):1--11, 2020.

\bibitem{zhang2015digital}
Yong Zhang, Peng Li, Yingyezhe Jin, and Yoonsuck Choe.
\newblock A digital liquid state machine with biologically inspired learning
  and its application to speech recognition.
\newblock {\em IEEE transactions on neural networks and learning systems},
  26(11):2635--2649, 2015.

\bibitem{zhang2019deep}
Shaohui Zhang, Zhenzhong Sun, Man Wang, Jianyu Long, Yun Bai, and Chuan Li.
\newblock Deep fuzzy echo state networks for machinery fault diagnosis.
\newblock {\em IEEE Transactions on Fuzzy Systems}, 28(7):1205--1218, 2019.

\bibitem{gallicchio2017deep}
Claudio Gallicchio, Alessio Micheli, and Luca Pedrelli.
\newblock Deep reservoir computing: A critical experimental analysis.
\newblock {\em Neurocomputing}, 268:87--99, 2017.

\bibitem{long2019evolving}
Jianyu Long, Shaohui Zhang, and Chuan Li.
\newblock Evolving deep echo state networks for intelligent fault diagnosis.
\newblock {\em IEEE Transactions on Industrial Informatics}, 16(7):4928--4937,
  2019.

\bibitem{alomar2020efficient}
Miquel~L Alomar, Erik~S Skibinsky-Gitlin, Christiam~F Frasser, Vincent Canals,
  Eugeni Isern, Miquel Roca, and Josep~L Rossell{\'o}.
\newblock Efficient parallel implementation of reservoir computing systems.
\newblock {\em Neural Computing and Applications}, 32(7):2299--2313, 2020.

\bibitem{chouikhi2017pso}
Naima Chouikhi, Boudour Ammar, Nizar Rokbani, and Adel~M Alimi.
\newblock Pso-based analysis of echo state network parameters for time series
  forecasting.
\newblock {\em Applied Soft Computing}, 55:211--225, 2017.

\bibitem{jaeger2002adaptive}
Herbert Jaeger.
\newblock Adaptive nonlinear system identification with echo state networks.
\newblock {\em Advances in neural information processing systems}, 15, 2002.

\bibitem{jaeger2004harnessing}
Herbert Jaeger and Harald Haas.
\newblock Harnessing nonlinearity: Predicting chaotic systems and saving energy
  in wireless communication.
\newblock {\em science}, 2004.

\bibitem{tamura2021transfer}
Hiroto Tamura and Gouhei Tanaka.
\newblock Transfer-rls method and transfer-force learning for simple and fast
  training of reservoir computing models.
\newblock {\em Neural Networks}, 143:550--563, 2021.

\bibitem{triefenbach2010phoneme}
Fabian Triefenbach, Azarakhsh Jalalvand, Benjamin Schrauwen, and Jean-Pierre
  Martens.
\newblock Phoneme recognition with large hierarchical reservoirs.
\newblock {\em Advances in neural information processing systems}, 23, 2010.

\bibitem{sussillo2012transferring}
David Sussillo and LF~Abbott.
\newblock Transferring learning from external to internal weights in echo-state
  networks with sparse connectivity.
\newblock {\em PLoS One}, 7(5):e37372, 2012.

\bibitem{laje2013robust}
Rodrigo Laje and Dean~V Buonomano.
\newblock Robust timing and motor patterns by taming chaos in recurrent neural
  networks.
\newblock {\em Nature neuroscience}, 16(7):925--933, 2013.

\bibitem{depasquale2018full}
Brian DePasquale, Christopher~J Cueva, Kanaka Rajan, G~Sean Escola, and
  LF~Abbott.
\newblock full-force: A target-based method for training recurrent networks.
\newblock {\em PloS one}, 13(2):e0191527, 2018.

\bibitem{nicola2017supervised}
Wilten Nicola and Claudia Clopath.
\newblock Supervised learning in spiking neural networks with force training.
\newblock {\em Nature communications}, 8(1):1--15, 2017.

\bibitem{tamura2020two}
Hiroto Tamura and Gouhei Tanaka.
\newblock Two-step force learning algorithm for fast convergence in reservoir
  computing.
\newblock In {\em International Conference on Artificial Neural Networks},
  pages 459--469. Springer, 2020.

\bibitem{zheng2020r}
Yang Zheng and Eli Shlizerman.
\newblock R-force: Robust learning for random recurrent neural networks.
\newblock {\em arXiv preprint arXiv:2003.11660}, 2020.

\bibitem{heinrich2020learning}
Stefan Heinrich, Tayfun Alpay, and Yukie Nagai.
\newblock Learning timescales in gated and adaptive continuous time recurrent
  neural networks.
\newblock In {\em 2020 IEEE International Conference on Systems, Man, and
  Cybernetics (SMC)}, pages 2662--2667. IEEE, 2020.

\bibitem{sharma2017optimized}
Aman Sharma and Rinkle Rani.
\newblock An optimized framework for cancer classification using deep learning
  and genetic algorithm.
\newblock {\em Journal of medical imaging and health informatics},
  7(8):1851--1856, 2017.

\bibitem{basterrech2014experimental}
Sebasti{\'a}n Basterrech, Enrique Alba, and V{\'a}clav Sn{\'a}{\v{s}}el.
\newblock An experimental analysis of the echo state network initialization
  using the particle swarm optimization.
\newblock In {\em 2014 Sixth World Congress on Nature and Biologically Inspired
  Computing (NaBIC 2014)}, pages 214--219. IEEE, 2014.

\bibitem{schmidhuber2007training}
J{\"u}rgen Schmidhuber, Daan Wierstra, Matteo Gagliolo, and Faustino Gomez.
\newblock Training recurrent networks by evolino.
\newblock {\em Neural computation}, 19(3):757--779, 2007.

\bibitem{jaeger2005reservoir}
Herbert Jaeger.
\newblock Reservoir riddles: Suggestions for echo state network research.
\newblock In {\em Proceedings. 2005 IEEE International Joint Conference on
  Neural Networks, 2005.}, volume~3, pages 1460--1462. IEEE, 2005.

\bibitem{norton2006preparing}
David Norton and Dan Ventura.
\newblock Preparing more effective liquid state machines using hebbian
  learning.
\newblock In {\em The 2006 IEEE International Joint Conference on Neural
  Network Proceedings}, pages 4243--4248. IEEE, 2006.

\bibitem{jin2016sso}
Yingyezhe Jin, Yu~Liu, and Peng Li.
\newblock Sso-lsm: A sparse and self-organizing architecture for liquid state
  machine based neural processors.
\newblock In {\em 2016 IEEE/ACM International Symposium on Nanoscale
  Architectures (NANOARCH)}, pages 55--60. IEEE, 2016.

\bibitem{luo2018improving}
Shengyuan Luo, Hang Guan, Xiumin Li, Fangzheng Xue, and Hongjun Zhou.
\newblock Improving liquid state machine in temporal pattern classification.
\newblock In {\em 2018 15th International Conference on Control, Automation,
  Robotics and Vision (ICARCV)}, pages 88--91. IEEE, 2018.

\bibitem{steil2007online}
Jochen~J Steil.
\newblock Online reservoir adaptation by intrinsic plasticity for
  backpropagation--decorrelation and echo state learning.
\newblock {\em Neural networks}, 20(3):353--364, 2007.

\bibitem{schrauwen2008improving}
Benjamin Schrauwen, Marion Wardermann, David Verstraeten, Jochen~J Steil, and
  Dirk Stroobandt.
\newblock Improving reservoirs using intrinsic plasticity.
\newblock {\em Neurocomputing}, 71(7-9):1159--1171, 2008.

\bibitem{xue2017reservoir}
Fangzheng Xue, Qian Li, Hongjun Zhou, and Xiumin Li.
\newblock Reservoir computing with both neuronal intrinsic plasticity and
  multi-clustered structure.
\newblock {\em Cognitive Computation}, 9(3):400--410, 2017.

\bibitem{li2011model}
Chunguang Li.
\newblock A model of neuronal intrinsic plasticity.
\newblock {\em IEEE Transactions on Autonomous Mental Development},
  3(4):277--284, 2011.

\bibitem{bellec2019biologically}
Guillaume Bellec, Franz Scherr, Elias Hajek, Darjan Salaj, Robert Legenstein,
  and Wolfgang Maass.
\newblock Biologically inspired alternatives to backpropagation through time
  for learning in recurrent neural nets.
\newblock {\em arXiv preprint arXiv:1901.09049}, 2019.

\bibitem{bellec2020solution}
Guillaume Bellec, Franz Scherr, Anand Subramoney, Elias Hajek, Darjan Salaj,
  Robert Legenstein, and Wolfgang Maass.
\newblock A solution to the learning dilemma for recurrent networks of spiking
  neurons.
\newblock {\em Nature communications}, 11(1):1--15, 2020.

\bibitem{hebb2005organization}
Donald~Olding Hebb.
\newblock {\em The organization of behavior: A neuropsychological theory}.
\newblock Psychology Press, 2005.

\bibitem{farhang2013adaptive}
Behrouz Farhang-Boroujeny.
\newblock {\em Adaptive filters: theory and applications}.
\newblock John Wiley \& Sons, 2013.

\bibitem{beer2019one}
Chen Beer and Omri Barak.
\newblock One step back, two steps forward: interference and learning in
  recurrent neural networks.
\newblock {\em Neural Computation}, 31(10):1985--2003, 2019.

\bibitem{duane2017force}
Gregory~S Duane.
\newblock “force” learning in recurrent neural networks as data
  assimilation.
\newblock {\em Chaos: An Interdisciplinary Journal of Nonlinear Science},
  27(12):126804, 2017.

\bibitem{bekolay2014nengo}
Trevor Bekolay, James Bergstra, Eric Hunsberger, Travis DeWolf, Terrence~C
  Stewart, Daniel Rasmussen, Xuan Choo, Aaron~Russell Voelker, and Chris
  Eliasmith.
\newblock Nengo: a python tool for building large-scale functional brain
  models.
\newblock {\em Frontiers in neuroinformatics}, 7:48, 2014.

\bibitem{liu2022tension}
Lu~Bin Liu, Attila Losonczy, and Zhenrui Liao.
\newblock tension: A python package for force learning.
\newblock {\em PLOS Computational Biology}, 18(12):e1010722, 2022.

\bibitem{badem2017new}
Hasan Badem, Alper Basturk, Abdullah Caliskan, and Mehmet~Emin Yuksel.
\newblock A new efficient training strategy for deep neural networks by
  hybridization of artificial bee colony and limited--memory bfgs optimization
  algorithms.
\newblock {\em Neurocomputing}, 266:506--526, 2017.

\bibitem{ferreira2011comparing}
Aida~A Ferreira and Teresa~B Ludermir.
\newblock Comparing evolutionary methods for reservoir computing pre-training.
\newblock In {\em The 2011 International Joint Conference on Neural Networks},
  pages 283--290. IEEE, 2011.

\bibitem{zhang2003other}
Wei Zhang and David~J Linden.
\newblock The other side of the engram: experience-driven changes in neuronal
  intrinsic excitability.
\newblock {\em Nature Reviews Neuroscience}, 4(11):885--900, 2003.

\bibitem{wang2019echo}
Xinjie Wang, Yaochu Jin, and Kuangrong Hao.
\newblock Echo state networks regulated by local intrinsic plasticity rules for
  regression.
\newblock {\em Neurocomputing}, 351:111--122, 2019.

\bibitem{nakada2022information}
Kazuki Nakada, Shunya Suzuki, Eiji Suzuki, Yukio Terasaki, Tetsuya Asai, and
  Tomoyuki Sasaki.
\newblock An information theoretic parameter tuning for mems-based reservoir
  computing.
\newblock {\em Nonlinear Theory and Its Applications, IEICE}, 13(2):459--464,
  2022.

\bibitem{verstraeten2005isolated}
David Verstraeten, Benjamin Schrauwen, Dirk Stroobandt, and Jan Van~Campenhout.
\newblock Isolated word recognition with the liquid state machine: a case
  study.
\newblock {\em Information Processing Letters}, 95(6):521--528, 2005.

\bibitem{paquot2012optoelectronic}
Yvan Paquot, Francois Duport, Antoneo Smerieri, Joni Dambre, Benjamin
  Schrauwen, Marc Haelterman, and Serge Massar.
\newblock Optoelectronic reservoir computing.
\newblock {\em Scientific reports}, 2(1):1--6, 2012.

\bibitem{jalalvand2015real}
Azarakhsh Jalalvand, Glenn Van~Wallendael, and Rik Van~de Walle.
\newblock Real-time reservoir computing network-based systems for detection
  tasks on visual contents.
\newblock In {\em 2015 7th International Conference on Computational
  Intelligence, Communication Systems and Networks}, pages 146--151. IEEE,
  2015.

\bibitem{chitsazan2019wind}
Mohammad~Amin Chitsazan, M~Sami Fadali, and Andrzej~M Trzynadlowski.
\newblock Wind speed and wind direction forecasting using echo state network
  with nonlinear functions.
\newblock {\em Renewable energy}, 131:879--889, 2019.

\bibitem{li2022multi}
Ziqiang Li and Gouhei Tanaka.
\newblock Multi-reservoir echo state networks with sequence resampling for
  nonlinear time-series prediction.
\newblock {\em Neurocomputing}, 467:115--129, 2022.

\bibitem{li2023multi}
Ziqiang Li, Yun Liu, and Gouhei Tanaka.
\newblock Multi-reservoir echo state networks with hodrick--prescott filter for
  nonlinear time-series prediction.
\newblock {\em Applied Soft Computing}, page 110021, 2023.

\bibitem{chouikhi2019bi}
Naima Chouikhi, Boudour Ammar, Amir Hussain, and Adel~M Alimi.
\newblock Bi-level multi-objective evolution of a multi-layered echo-state
  network autoencoder for data representations.
\newblock {\em Neurocomputing}, 341:195--211, 2019.

\bibitem{watts1998collective}
Duncan~J Watts and Steven~H Strogatz.
\newblock Collective dynamics of ‘small-world’networks.
\newblock {\em nature}, 393(6684):440--442, 1998.

\bibitem{kawai2019small}
Yuji Kawai, Jihoon Park, and Minoru Asada.
\newblock A small-world topology enhances the echo state property and signal
  propagation in reservoir computing.
\newblock {\em Neural Networks}, 112:15--23, 2019.

\bibitem{kaiser2017scaling}
Jacques Kaiser, Rainer Stal, Anand Subramoney, Arne Roennau, and R{\"u}diger
  Dillmann.
\newblock Scaling up liquid state machines to predict over address events from
  dynamic vision sensors.
\newblock {\em Bioinspiration \& biomimetics}, 12(5):055001, 2017.

\bibitem{gewaltig2007NEST}
Marc-Oliver Gewaltig and Markus Diesmann.
\newblock Nest (neural simulation tool).
\newblock {\em Scholarpedia}, 2(4):1430, 2007.

\bibitem{reynolds2019intelligent}
John~JM Reynolds, James~S Plank, and Catherine~D Schuman.
\newblock Intelligent reservoir generation for liquid state machines using
  evolutionary optimization.
\newblock In {\em 2019 International Joint Conference on Neural Networks
  (IJCNN)}, pages 1--8. IEEE, 2019.

\bibitem{zhou2019evolutionary}
Yan Zhou, Yaochu Jin, and Jinliang Ding.
\newblock Evolutionary optimization of liquid state machines for robust
  learning.
\newblock In {\em International Symposium on Neural Networks}, pages 389--398.
  Springer, 2019.

\bibitem{tian2021neural}
Shuo Tian, Lianhua Qu, Lei Wang, Kai Hu, Nan Li, and Weixia Xu.
\newblock A neural architecture search based framework for liquid state machine
  design.
\newblock {\em Neurocomputing}, 443:174--182, 2021.

\bibitem{appeltant2011information}
Lennert Appeltant, Miguel~Cornelles Soriano, Guy Van~der Sande, Jan Danckaert,
  Serge Massar, Joni Dambre, Benjamin Schrauwen, Claudio~R Mirasso, and Ingo
  Fischer.
\newblock Information processing using a single dynamical node as complex
  system.
\newblock {\em Nature communications}, 2(1):1--6, 2011.

\bibitem{yilmaz2015machine}
Ozgur Yilmaz.
\newblock Machine learning using cellular automata based feature expansion and
  reservoir computing.
\newblock {\em Journal of Cellular Automata}, 10, 2015.

\bibitem{goudarzi2013dna}
Alireza Goudarzi, Matthew~R Lakin, and Darko Stefanovic.
\newblock Dna reservoir computing: a novel molecular computing approach.
\newblock In {\em International Workshop on DNA-Based Computers}, pages 76--89.
  Springer, 2013.

\bibitem{lepri1994high}
S~Lepri, G~Giacomelli, A~Politi, and FT~Arecchi.
\newblock High-dimensional chaos in delayed dynamical systems.
\newblock {\em Physica D: Nonlinear Phenomena}, 70(3):235--249, 1994.

\bibitem{ortin2017reservoir}
Silvia Ort{\'\i}n and Luis Pesquera.
\newblock Reservoir computing with an ensemble of time-delay reservoirs.
\newblock {\em Cognitive Computation}, 9(3):327--336, 2017.

\bibitem{brunner2018tutorial}
Daniel Brunner, Bogdan Penkovsky, Bicky~A Marquez, Maxime Jacquot, Ingo
  Fischer, and Laurent Larger.
\newblock Tutorial: Photonic neural networks in delay systems.
\newblock {\em Journal of Applied Physics}, 124(15):152004, 2018.

\bibitem{wolfram2018cellular}
Stephen Wolfram.
\newblock {\em Cellular automata and complexity: collected papers}.
\newblock crc Press, 2018.

\bibitem{yilmaz2014reservoir}
Ozgur Yilmaz.
\newblock Reservoir computing using cellular automata.
\newblock {\em arXiv preprint arXiv:1410.0162}, 2014.

\bibitem{snyder2013computational}
David Snyder, Alireza Goudarzi, and Christof Teuscher.
\newblock Computational capabilities of random automata networks for reservoir
  computing.
\newblock {\em Physical Review E}, 87(4):042808, 2013.

\bibitem{nichele2017reservoir}
Stefano Nichele and Magnus~S Gundersen.
\newblock Reservoir computing using non-uniform binary cellular automata.
\newblock {\em arXiv preprint arXiv:1702.03812}, 2017.

\bibitem{nichele2017deep}
Stefano Nichele and Andreas Molund.
\newblock Deep reservoir computing using cellular automata.
\newblock {\em arXiv preprint arXiv:1703.02806}, 2017.

\bibitem{mcdonald2017reservoir}
Nathan McDonald.
\newblock Reservoir computing \& extreme learning machines using pairs of
  cellular automata rules.
\newblock In {\em 2017 International Joint Conference on Neural Networks
  (IJCNN)}, pages 2429--2436. IEEE, 2017.

\bibitem{uragami2022universal}
Daisuke Uragami and Yukio-Pegio Gunji.
\newblock Universal criticality in reservoir computing using asynchronous
  cellular automata.
\newblock {\em Complex Systems}, 31(1):103--121, 2022.

\bibitem{coulombe2017computing}
Jean~C Coulombe, Mark~CA York, and Julien Sylvestre.
\newblock Computing with networks of nonlinear mechanical oscillators.
\newblock {\em PloS one}, 12(6):e0178663, 2017.

\bibitem{nguyen2020reservoir}
Hoang Nguyen, Peter Banda, Darko Stefanovic, and Christof Teuscher.
\newblock Reservoir computing with random chemical systems.
\newblock In {\em ALIFE 2020: The 2020 Conference on Artificial Life}, pages
  491--499. MIT Press, 2020.

\bibitem{liu2022reservoir}
Xingyi Liu and Keshab~K Parhi.
\newblock Reservoir computing using dna oscillators.
\newblock {\em ACS Synthetic Biology}, 2022.

\bibitem{nguyen2018biochemical}
Hoang Nguyen and Christof Teuscher.
\newblock Biochemical reservoir computing, 2018.

\bibitem{nguyen2022sers}
Phuong~HL Nguyen, Shimon Rubin, Pulak Sarangi, Piya Pal, and Yeshaiahu Fainman.
\newblock Sers-based ssdna composition analysis with inhomogeneous peak
  broadening and reservoir computing.
\newblock {\em Applied Physics Letters}, 120(2):023701, 2022.

\bibitem{yahiro2018reservoir}
Wataru Yahiro, Nathanael Aubert-Kato, and Masami Hagiya.
\newblock A reservoir computing approach for molecular computing.
\newblock In {\em ALIFE 2018: The 2018 Conference on Artificial Life}, pages
  31--38. MIT Press, 2018.

\bibitem{kan2021physical}
Shaohua Kan, Kohei Nakajima, Tetsuya Asai, and Megumi Akai-Kasaya.
\newblock Physical implementation of reservoir computing through
  electrochemical reaction.
\newblock {\em Advanced Science}, page 2104076, 2021.

\bibitem{akai2022performance}
Megumi Akai-Kasaya, Yuki Takeshima, Shaohua Kan, Kohei Nakajima, Takahide Oya,
  and Tetsuya Asai.
\newblock Performance of reservoir computing in a random network of
  single-walled carbon nanotubes complexed with polyoxometalate.
\newblock {\em Neuromorphic Computing and Engineering}, 2(1):014003, 2022.

\bibitem{nakao2016phase}
Hiroya Nakao.
\newblock Phase reduction approach to synchronisation of nonlinear oscillators.
\newblock {\em Contemporary Physics}, 57(2):188--214, 2016.

\bibitem{yamane2015wave}
Toshiyuki Yamane, Yasunao Katayama, Ryosho Nakane, Gouhei Tanaka, and Daiju
  Nakano.
\newblock Wave-based reservoir computing by synchronization of coupled
  oscillators.
\newblock In {\em International Conference on Neural Information Processing},
  pages 198--205. Springer, 2015.

\bibitem{velichko2020reservoir}
AA~Velichko, DV~Ryabokon, SD~Khanin, AV~Sidorenko, and AG~Rikkiev.
\newblock Reservoir computing using high order synchronization of coupled
  oscillators.
\newblock In {\em IOP Conference Series: Materials Science and Engineering},
  volume 862, page 052062. IOP Publishing, 2020.

\bibitem{riou2017neuromorphic}
Mathieu Riou, F~Abreu Araujo, Jacob Torrejon, Sumito Tsunegi, Guru Khalsa,
  Damien Querlioz, Paolo Bortolotti, Vincent Cros, Kay Yakushiji, Akio
  Fukushima, et~al.
\newblock Neuromorphic computing through time-multiplexing with a spin-torque
  nano-oscillator.
\newblock In {\em 2017 IEEE International Electron Devices Meeting (IEDM)},
  pages 36--3. IEEE, 2017.

\bibitem{appeltant2014constructing}
Lennert Appeltant, Guy Van~der Sande, Jan Danckaert, and Ingo Fischer.
\newblock Constructing optimized binary masks for reservoir computing with
  delay systems.
\newblock {\em Scientific reports}, 4(1):1--5, 2014.

\bibitem{li2018deep}
Jialing Li, Kangjun Bai, Lingjia Liu, and Yang Yi.
\newblock A deep learning based approach for analog hardware implementation of
  delayed feedback reservoir computing system.
\newblock In {\em 2018 19th International Symposium on Quality Electronic
  Design (ISQED)}, pages 308--313. IEEE, 2018.

\bibitem{jensen2017reservoir}
Johannes Jensen and Gunnar Tufte.
\newblock Reservoir computing with a chaotic circuit.
\newblock In {\em Proceedings of the European Conference on Artificial Life
  2017}. MIT Press, 2017.

\bibitem{zhao2016novel}
Chenyuan Zhao, Jialing Li, Lingjia Liu, Lakshmi~Sravanthi Koutha, Jian Liu, and
  Yang Yi.
\newblock Novel spike based reservoir node design with high performance spike
  delay loop.
\newblock In {\em Proceedings of the 3rd ACM International Conference on
  Nanoscale Computing and Communication}, pages 1--5, 2016.

\bibitem{li2017analog}
Jialing Li, Chenyuan Zhao, Kian Hamedani, and Yang Yi.
\newblock Analog hardware implementation of spike-based delayed feedback
  reservoir computing system.
\newblock In {\em 2017 International Joint Conference on Neural Networks
  (IJCNN)}, pages 3439--3446. IEEE, 2017.

\bibitem{antonik2015fpga}
Piotr Antonik, Anteo Smerieri, Fran{\c{c}}ois Duport, Marc Haelterman, and
  Serge Massar.
\newblock Fpga implementation of reservoir computing with online learning.
\newblock In {\em 24th Belgian-Dutch Conference on Machine Learning}, 2015.

\bibitem{schrauwen2008compact}
Benjamin Schrauwen, Michiel D’Haene, David Verstraeten, and Jan
  Van~Campenhout.
\newblock Compact hardware liquid state machines on fpga for real-time speech
  recognition.
\newblock {\em Neural networks}, 21(2-3):511--523, 2008.

\bibitem{wang2016liquid}
Qian Wang, Youjie Li, and Peng Li.
\newblock Liquid state machine based pattern recognition on fpga with
  firing-activity dependent power gating and approximate computing.
\newblock In {\em 2016 IEEE International Symposium on Circuits and Systems
  (ISCAS)}, pages 361--364. IEEE, 2016.

\bibitem{haynes2015reservoir}
Nicholas~D Haynes, Miguel~C Soriano, David~P Rosin, Ingo Fischer, and Daniel~J
  Gauthier.
\newblock Reservoir computing with a single time-delay autonomous boolean node.
\newblock {\em Physical Review E}, 91(2):020801, 2015.

\bibitem{alomar2015digital}
Miquel~L Alomar, Miguel~C Soriano, Miguel Escalona-Mor{\'a}n, Vincent Canals,
  Ingo Fischer, Claudio~R Mirasso, and Jose~L Rossell{\'o}.
\newblock Digital implementation of a single dynamical node reservoir computer.
\newblock {\em IEEE Transactions on Circuits and Systems II: Express Briefs},
  62(10):977--981, 2015.

\bibitem{yang2016investigations}
Xiao Yang, Wanlong Chen, and Frank~Z Wang.
\newblock Investigations of the staircase memristor model and applications of
  memristor-based local connections.
\newblock {\em Analog Integrated Circuits and Signal Processing},
  87(2):263--273, 2016.

\bibitem{kulkarni2012memristor}
Manjari~S Kulkarni and Christof Teuscher.
\newblock Memristor-based reservoir computing.
\newblock In {\em 2012 IEEE/ACM international symposium on nanoscale
  architectures (NANOARCH)}, pages 226--232. IEEE, 2012.

\bibitem{du2017reservoir}
Chao Du, Fuxi Cai, Mohammed~A Zidan, Wen Ma, Seung~Hwan Lee, and Wei~D Lu.
\newblock Reservoir computing using dynamic memristors for temporal information
  processing.
\newblock {\em Nature communications}, 8(1):1--10, 2017.

\bibitem{vandoorne2014experimental}
Kristof Vandoorne, Pauline Mechet, Thomas Van~Vaerenbergh, Martin Fiers, Geert
  Morthier, David Verstraeten, Benjamin Schrauwen, Joni Dambre, and Peter
  Bienstman.
\newblock Experimental demonstration of reservoir computing on a silicon
  photonics chip.
\newblock {\em Nature communications}, 5(1):1--6, 2014.

\bibitem{katumba2018low}
Andrew Katumba, Jelle Heyvaert, Bendix Schneider, Sarah Uvin, Joni Dambre, and
  Peter Bienstman.
\newblock Low-loss photonic reservoir computing with multimode photonic
  integrated circuits.
\newblock {\em Scientific reports}, 8(1):1--10, 2018.

\bibitem{dong2018scaling}
Jonathan Dong, Sylvain Gigan, Florent Krzakala, and Gilles Wainrib.
\newblock Scaling up echo-state networks with multiple light scattering.
\newblock In {\em 2018 IEEE Statistical Signal Processing Workshop (SSP)},
  pages 448--452. IEEE, 2018.

\bibitem{fiers2014nanophotonic}
Martin Andre~Agnes Fiers, Thomas Van~Vaerenbergh, Francis Wyffels, David
  Verstraeten, Benjamin Schrauwen, Joni Dambre, and Peter Bienstman.
\newblock Nanophotonic reservoir computing with photonic crystal cavities to
  generate periodic patterns.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  25(2):344--355, 2014.

\bibitem{laporte2018numerical}
Floris Laporte, Andrew Katumba, Joni Dambre, and Peter Bienstman.
\newblock Numerical demonstration of neuromorphic computing with photonic
  crystal cavities.
\newblock {\em Optics express}, 26(7):7955--7964, 2018.

\bibitem{vinckier2015high}
Quentin Vinckier, Fran{\c{c}}ois Duport, Anteo Smerieri, Kristof Vandoorne,
  Peter Bienstman, Marc Haelterman, and Serge Massar.
\newblock High-performance photonic reservoir computer based on a coherently
  driven passive cavity.
\newblock {\em Optica}, 2(5):438--446, 2015.

\bibitem{taniguchi2021reservoir}
Tomohiro Taniguchi, Sumito Tsunegi, Shinji Miwa, Keisuke Fujii, Hitoshi Kubota,
  and Kohei Nakajima.
\newblock Reservoir computing based on spintronics technology.
\newblock In {\em Reservoir Computing}, pages 331--360. Springer, 2021.

\bibitem{furuta2018macromagnetic}
Taishi Furuta, Keisuke Fujii, Kohei Nakajima, Sumito Tsunegi, Hitoshi Kubota,
  Yoshishige Suzuki, and Shinji Miwa.
\newblock Macromagnetic simulation for reservoir computing utilizing spin
  dynamics in magnetic tunnel junctions.
\newblock {\em Physical Review Applied}, 10(3):034063, 2018.

\bibitem{nakane2021spin}
Ryosho Nakane, Akira Hirose, and Gouhei Tanaka.
\newblock Spin waves propagating through a stripe magnetic domain structure and
  their applications to reservoir computing.
\newblock {\em Physical Review Research}, 3(3):033243, 2021.

\bibitem{jiang2019physical}
Wencong Jiang, Lina Chen, Kaiyuan Zhou, Liyuan Li, Qingwei Fu, Youwei Du, and
  Ronghua Liu.
\newblock Physical reservoir computing built by spintronic devices for temporal
  information processing.
\newblock {\em arXiv preprint arXiv:1901.07879}, 2019.

\bibitem{nomura2021reservoir}
Hikaru Nomura, Hitoshi Kubota, and Yoshishige Suzuki.
\newblock Reservoir computing with dipole-coupled nanomagnets.
\newblock In {\em Reservoir Computing}, pages 361--374. Springer, 2021.

\bibitem{johnson2014active}
Christopher Johnson, Andrew Philippides, and Philip Husbands.
\newblock Active shape discrimination with physical reservoir computers.
\newblock In {\em ALIFE 14: The Fourteenth International Conference on the
  Synthesis and Simulation of Living Systems}, pages 176--183. MIT Press, 2014.

\bibitem{nakajima2014exploiting}
Kohei Nakajima, Tao Li, Helmut Hauser, and Rolf Pfeifer.
\newblock Exploiting short-term memory in soft body dynamics as a computational
  resource.
\newblock {\em Journal of The Royal Society Interface}, 11(100):20140437, 2014.

\bibitem{nakajima2018exploiting}
Kohei Nakajima, Helmut Hauser, Tao Li, and Rolf Pfeifer.
\newblock Exploiting the dynamics of soft materials for machine learning.
\newblock {\em Soft robotics}, 5(3):339--347, 2018.

\bibitem{bhovad2021physical}
Priyanka Bhovad and Suyi Li.
\newblock Physical reservoir computing with origami and its application to
  robotic crawling.
\newblock {\em Scientific Reports}, 11(1):1--18, 2021.

\bibitem{yada2021physical}
Yuichiro Yada, Shusaku Yasuda, and Hirokazu Takahashi.
\newblock Physical reservoir computing with force learning in a living neuronal
  culture.
\newblock {\em Applied Physics Letters}, 119(17):173701, 2021.

\bibitem{konkoli2018developing}
Zoran Konkoli.
\newblock On developing theory of reservoir computing for sensing applications:
  the state weaving environment echo tracker (sweet) algorithm.
\newblock {\em International Journal of Parallel, Emergent and Distributed
  Systems}, 33(2):121--143, 2018.

\bibitem{caluwaerts2011body}
Ken Caluwaerts and Benjamin Schrauwen.
\newblock The body as a reservoir: locomotion and sensing with linear feedback.
\newblock In {\em 2nd International conference on Morphological Computation
  (ICMC 2011)}, 2011.

\bibitem{ghosh2021realising}
Sanjib Ghosh, Tanjung Krisnanda, Tomasz Paterek, and Timothy~CH Liew.
\newblock Realising and compressing quantum circuits with quantum reservoir
  computing.
\newblock {\em Communications Physics}, 4(1):1--7, 2021.

\bibitem{govia2021quantum}
LCG Govia, GJ~Ribeill, GE~Rowlands, HK~Krovi, and TA~Ohki.
\newblock Quantum reservoir computing with a single nonlinear oscillator.
\newblock {\em Physical Review Research}, 3(1):013077, 2021.

\bibitem{martinez2021dynamical}
Rodrigo Mart{\'\i}nez-Pe{\~n}a, Gian~Luca Giorgi, Johannes Nokkala, Miguel~C
  Soriano, and Roberta Zambrini.
\newblock Dynamical phase transitions in quantum reservoir computing.
\newblock {\em Physical Review Letters}, 127(10):100502, 2021.

\bibitem{van2017advances}
Guy Van~der Sande, Daniel Brunner, and Miguel~C Soriano.
\newblock Advances in photonic reservoir computing.
\newblock {\em Nanophotonics}, 6(3):561--576, 2017.

\bibitem{soriano2014delay}
Miguel~C Soriano, Silvia Ort{\'\i}n, Lars Keuninckx, Lennert Appeltant, Jan
  Danckaert, Luis Pesquera, and Guy Van~der Sande.
\newblock Delay-based reservoir computing: noise effects in a combined analog
  and digital implementation.
\newblock {\em IEEE transactions on neural networks and learning systems},
  26(2):388--393, 2014.

\bibitem{soriano2015minimal}
Miguel~C Soriano, Daniel Brunner, Miguel Escalona-Mor{\'a}n, Claudio~R Mirasso,
  and Ingo Fischer.
\newblock Minimal approach to neuro-inspired information processing.
\newblock {\em Frontiers in computational neuroscience}, 9:68, 2015.

\bibitem{verstraeten2005reservoir}
David Verstraeten, Benjamin Schrauwen, and Dirk Stroobandt.
\newblock Reservoir computing with stochastic bitstream neurons.
\newblock In {\em Proceedings of the 16th annual Prorisc workshop}, pages
  454--459, 2005.

\bibitem{alomar2014low}
Miquel~L Alomar, Vicent Canals, V{\'\i}ctor Mart{\'\i}nez-Moll, and
  Jos{\'e}~Luis Rossell{\'o}.
\newblock Low-cost hardware implementation of reservoir computers.
\newblock In {\em 2014 24th International Workshop on Power and Timing
  Modeling, Optimization and Simulation (PATMOS)}, pages 1--5. IEEE, 2014.

\bibitem{liu2018online}
Yu~Liu, Yingyezhe Jin, and Peng Li.
\newblock Online adaptation and energy minimization for hardware recurrent
  spiking neural networks.
\newblock {\em ACM Journal on Emerging Technologies in Computing Systems
  (JETC)}, 14(1):1--21, 2018.

\bibitem{hassan2017hardware}
Amr~M Hassan, Hai~Helen Li, and Yiran Chen.
\newblock Hardware implementation of echo state networks using memristor double
  crossbar arrays.
\newblock In {\em 2017 International Joint Conference on Neural Networks
  (IJCNN)}, pages 2171--2177. IEEE, 2017.

\bibitem{soures2017robustness}
Nicholas Soures, Lydia Hays, and Dhireesha Kudithipudi.
\newblock Robustness of a memristor based liquid state machine.
\newblock In {\em 2017 international joint conference on neural networks
  (ijcnn)}, pages 2414--2420. IEEE, 2017.

\bibitem{burger2015computational}
Jens Burger, Alireza Goudarzi, Darko Stefanovic, and Christof Teuscher.
\newblock Computational capacity and energy consumption of complex resistive
  switch networks.
\newblock {\em arXiv preprint arXiv:1507.03716}, 2015.

\bibitem{carbajal2015memristor}
Juan~Pablo Carbajal, Joni Dambre, Michiel Hermans, and Benjamin Schrauwen.
\newblock Memristor models for machine learning.
\newblock {\em Neural computation}, 27(3):725--747, 2015.

\bibitem{tran2017memcapacitive}
SJ~Dat Tran and Christof Teuscher.
\newblock Memcapacitive reservoir computing.
\newblock In {\em 2017 IEEE/ACM International Symposium on Nanoscale
  Architectures (NANOARCH)}, pages 115--116. IEEE, 2017.

\bibitem{tran2019hierarchical}
SJ~Dat Tran and Christof Teuscher.
\newblock Hierarchical memcapacitive reservoir computing architecture.
\newblock In {\em 2019 IEEE International Conference on Rebooting Computing
  (ICRC)}, pages 1--6. IEEE, 2019.

\bibitem{stieg2012emergent}
Adam~Z Stieg, Audrius~V Avizienis, Henry~O Sillin, Cristina Martin-Olmos,
  Masakazu Aono, and James~K Gimzewski.
\newblock Emergent criticality in complex turing b-type atomic switch networks,
  2012.

\bibitem{vandoorne2008toward}
Kristof Vandoorne, Wouter Dierckx, Benjamin Schrauwen, David Verstraeten, Roel
  Baets, Peter Bienstman, and Jan Van~Campenhout.
\newblock Toward optical signal processing using photonic reservoir computing.
\newblock {\em Optics express}, 16(15):11182--11192, 2008.

\bibitem{schneider2016using}
Bendix Schneider, Joni Dambre, and Peter Bienstman.
\newblock Using digital masks to enhance the bandwidth tolerance and improve
  the performance of on-chip reservoir computing systems.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  27(12):2748--2753, 2016.

\bibitem{mesaritakis2015high}
Charis Mesaritakis, Adonis Bogris, Alexandros Kapsalis, and Dimitris Syvridis.
\newblock High-speed all-optical pattern recognition of dispersive fourier
  images through a photonic reservoir computing subsystem.
\newblock {\em Optics letters}, 40(14):3416--3419, 2015.

\bibitem{larger2012photonic}
Laurent Larger, Miguel~C Soriano, Daniel Brunner, Lennert Appeltant, Jose~M
  Guti{\'e}rrez, Luis Pesquera, Claudio~R Mirasso, and Ingo Fischer.
\newblock Photonic information processing beyond turing: an optoelectronic
  implementation of reservoir computing.
\newblock {\em Optics express}, 20(3):3241--3249, 2012.

\bibitem{duport2012all}
Fran{\c{c}}ois Duport, Bendix Schneider, Anteo Smerieri, Marc Haelterman, and
  Serge Massar.
\newblock All-optical reservoir computing.
\newblock {\em Optics express}, 20(20):22783--22795, 2012.

\bibitem{torrejon2017neuromorphic}
Jacob Torrejon, Mathieu Riou, Flavio~Abreu Araujo, Sumito Tsunegi, Guru Khalsa,
  Damien Querlioz, Paolo Bortolotti, Vincent Cros, Kay Yakushiji, Akio
  Fukushima, et~al.
\newblock Neuromorphic computing with nanoscale spintronic oscillators.
\newblock {\em Nature}, 547(7664):428--431, 2017.

\bibitem{riou2021reservoir}
Mathieu Riou, Jacob Torrejon, Flavio Abreu~Araujo, Sumito Tsunegi, Guru Khalsa,
  Damien Querlioz, Paolo Bortolotti, Nathan Leroux, Danijela Markovi{\'c},
  Vincent Cros, et~al.
\newblock Reservoir computing leveraging the transient non-linear dynamics of
  spin-torque nano-oscillators.
\newblock In {\em Reservoir Computing}, pages 307--329. Springer, 2021.

\bibitem{nakane2018reservoir}
Ryosho Nakane, Gouhei Tanaka, and Akira Hirose.
\newblock Reservoir computing with spin waves excited in a garnet film.
\newblock {\em IEEE access}, 6:4462--4469, 2018.

\bibitem{taniguchi2022spintronic}
Tomohiro Taniguchi, Amon Ogihara, Yasuhiro Utsumi, and Sumito Tsunegi.
\newblock Spintronic reservoir computing without driving current or magnetic
  field.
\newblock {\em Scientific Reports}, 12(1):1--11, 2022.

\bibitem{fert2017magnetic}
Albert Fert, Nicolas Reyren, and Vincent Cros.
\newblock Magnetic skyrmions: advances in physics and potential applications.
\newblock {\em Nature Reviews Materials}, 2(7):1--15, 2017.

\bibitem{bourianoff2018potential}
George Bourianoff, Daniele Pinna, Matthias Sitte, and Karin Everschor-Sitte.
\newblock Potential implementation of reservoir computing models based on
  magnetic skyrmions.
\newblock {\em Aip Advances}, 8(5):055602, 2018.

\bibitem{prychynenko2018magnetic}
Diana Prychynenko, Matthias Sitte, Kai Litzius, Benjamin Kr{\"u}ger, George
  Bourianoff, Mathias Kl{\"a}ui, Jairo Sinova, and Karin Everschor-Sitte.
\newblock Magnetic skyrmion as a nonlinear resistive element: a potential
  building block for reservoir computing.
\newblock {\em Physical Review Applied}, 9(1):014034, 2018.

\bibitem{hauser2011towards}
Helmut Hauser, Auke~J Ijspeert, Rudolf~M F{\"u}chslin, Rolf Pfeifer, and
  Wolfgang Maass.
\newblock Towards a theoretical foundation for morphological computation with
  compliant bodies.
\newblock {\em Biological cybernetics}, 105(5):355--370, 2011.

\bibitem{hauser2012role}
Helmut Hauser, Auke~J Ijspeert, Rudolf~M F{\"u}chslin, Rolf Pfeifer, and
  Wolfgang Maass.
\newblock The role of feedback in morphological computation with compliant
  bodies.
\newblock {\em Biological cybernetics}, 106(10):595--613, 2012.

\bibitem{nakajima2013soft}
Kohei Nakajima, Helmut Hauser, Rongjie Kang, Emanuele Guglielmino, Darwin~G
  Caldwell, and Rolf Pfeifer.
\newblock A soft body as a reservoir: case studies in a dynamic model of
  octopus-inspired soft robotic arm.
\newblock {\em Frontiers in computational neuroscience}, 7:91, 2013.

\bibitem{fujita2018environmental}
Kenichi Fujita, Syogo Yonekura, Satoshi Nishikawa, Ryuma Niiyama, and Yasuo
  Kuniyoshi.
\newblock Environmental and structural effects on physical reservoir computing
  with tensegrity.
\newblock {\em Journal of the Institute of Industrial Applications Engineers},
  6(2):92, 2018.

\bibitem{cpg1}
Eve Marder and Dirk Bucher.
\newblock Central pattern generators and the control of rhythmic movements.
\newblock {\em Current biology}, 11(23):R986--R996, 2001.

\bibitem{cpg2}
Ronald~M Harris-Warrick.
\newblock Neuromodulation and flexibility in central pattern generator
  networks.
\newblock {\em Current opinion in neurobiology}, 21(5):685--692, 2011.

\bibitem{cpg3}
Auke~Jan Ijspeert.
\newblock Central pattern generators for locomotion control in animals and
  robots: a review.
\newblock {\em Neural networks}, 21(4):642--653, 2008.

\bibitem{cpg4}
David~W Sims, Nicolas~E Humphries, Nan Hu, Violeta Medan, and Jimena Berni.
\newblock Optimal searching behaviour generated intrinsically by the central
  pattern generator for locomotion.
\newblock {\em Elife}, 8:e50316, 2019.

\bibitem{fujita2018physical}
Kenichi FUJITA, Shogo YONEKURA, Satoshi NISHIKAWA, Ryuma NIIYAMA, and Yasuo
  KUNIYOSHI.
\newblock Physical reservoir computing in tensegrity with structural softness
  and ground collision dynamics.
\newblock {\em Journal of the Institute of Industrial Applications Engineers
  (Web)}, 6(2):92--99, 2018.

\bibitem{caluwaerts2014design}
Ken Caluwaerts, J{\'e}r{\'e}mie Despraz, At{\i}l I{\c{s}}{\c{c}}en, Andrew~P
  Sabelhaus, Jonathan Bruce, Benjamin Schrauwen, and Vytas SunSpiral.
\newblock Design and control of compliant tensegrity robots through simulation
  and hardware validation.
\newblock {\em Journal of the royal society interface}, 11(98):20140520, 2014.

\bibitem{vandesompele2019populations}
Alexander Vandesompele, Gabriel Urbain, Joni Dambre, et~al.
\newblock Populations of spiking neurons for reservoir computing: Closed loop
  control of a compliant quadruped.
\newblock {\em Cognitive Systems Research}, 58:317--323, 2019.

\bibitem{terajima2021behavioral}
Ryo Terajima, Katsuma Inoue, Shogo Yonekura, Kohei Nakajima, and Yasuo
  Kuniyoshi.
\newblock Behavioral diversity generated from bodyenvironment interactions in a
  simulated tensegrity robot.
\newblock {\em IEEE Robotics and Automation Letters}, 2021.

\bibitem{mujal2021opportunities}
Pere Mujal, Rodrigo Mart{\'\i}nez-Pe{\~n}a, Johannes Nokkala, Jorge
  Garc{\'\i}a-Beni, Gian~Luca Giorgi, Miguel~C Soriano, and Roberta Zambrini.
\newblock Opportunities in quantum reservoir computing and extreme learning
  machines.
\newblock {\em Advanced Quantum Technologies}, 4(8):2100027, 2021.

\bibitem{fujii2021quantum}
Keisuke Fujii and Kohei Nakajima.
\newblock Quantum reservoir computing: a reservoir approach toward quantum
  machine learning on near-term quantum devices.
\newblock In {\em Reservoir Computing}, pages 423--450. Springer, 2021.

\bibitem{ghosh2021quantum}
Sanjib Ghosh, Kohei Nakajima, Tanjung Krisnanda, Keisuke Fujii, and Timothy~CH
  Liew.
\newblock Quantum neuromorphic computing with reservoir computing networks.
\newblock {\em Advanced Quantum Technologies}, 4(9):2100053, 2021.

\bibitem{fujii2017harnessing}
Keisuke Fujii and Kohei Nakajima.
\newblock Harnessing disordered-ensemble quantum dynamics for machine learning.
\newblock {\em Physical Review Applied}, 8(2):024030, 2017.

\bibitem{nakajima2019boosting}
Kohei Nakajima, Keisuke Fujii, Makoto Negoro, Kosuke Mitarai, and Masahiro
  Kitagawa.
\newblock Boosting computational power through spatial multiplexing in quantum
  reservoir computing.
\newblock {\em Physical Review Applied}, 11(3):034021, 2019.

\bibitem{kutvonen2020optimizing}
Aki Kutvonen, Keisuke Fujii, and Takahiro Sagawa.
\newblock Optimizing a quantum reservoir computer for time series prediction.
\newblock {\em Scientific reports}, 10(1):1--7, 2020.

\bibitem{negoro2021toward}
Makoto Negoro, Kosuke Mitarai, Kohei Nakajima, and Keisuke Fujii.
\newblock Toward nmr quantum reservoir computing.
\newblock In {\em Reservoir Computing}, pages 451--458. Springer, 2021.

\bibitem{gonon2019reservoir}
Lukas Gonon and Juan-Pablo Ortega.
\newblock Reservoir computing universality with stochastic inputs.
\newblock {\em IEEE transactions on neural networks and learning systems},
  31(1):100--112, 2019.

\bibitem{hart2021echo}
Allen~G Hart, James~L Hook, and Jonathan~HP Dawes.
\newblock Echo state networks trained by tikhonov least squares are l2 ($\mu$)
  approximators of ergodic dynamical systems.
\newblock {\em Physica D: Nonlinear Phenomena}, 421:132882, 2021.

\bibitem{bollt2021explaining}
Erik Bollt.
\newblock On explaining the surprising success of reservoir computing
  forecaster of chaos? the universal machine learning dynamical system with
  contrast to var and dmd<? a3b2 show [feature]?>.
\newblock {\em Chaos: An Interdisciplinary Journal of Nonlinear Science},
  31(1):013108, 2021.

\bibitem{gauthier2021next}
Daniel~J Gauthier, Erik Bollt, Aaron Griffith, and Wendson~AS Barbosa.
\newblock Next generation reservoir computing.
\newblock {\em Nature communications}, 12(1):1--8, 2021.

\bibitem{hadaeghi2019reservoir}
Fatemeh Hadaeghi.
\newblock Reservoir computing models for patient-adaptable ecg monitoring in
  wearable devices.
\newblock {\em arXiv preprint arXiv:1907.09504}, 2019.

\bibitem{mastoi2019reservoir}
Qurat-ul-ain Mastoi, Teh~Ying Wah, and Ram Gopal~Raj.
\newblock Reservoir computing based echo state networks for ventricular heart
  beat classification.
\newblock {\em Applied Sciences}, 9(4):702, 2019.

\bibitem{n2021ecg}
Aya N~Elbedwehy, Mohy Eldin~Ahmed Abo-Elsoud, and Ahmed Elnakib.
\newblock Ecg denoising using a single-node dynamic reservoir computing
  architecture.(dept. e).
\newblock {\em MEJ. Mansoura Engineering Journal}, 46(4):47--52, 2021.

\bibitem{donati2018processing}
Elisa Donati, Melika Payvand, Nicoletta Risi, Renate Krause, Karla Burelo,
  Giacomo Indiveri, Thomas Dalgaty, and Elisa Vianello.
\newblock Processing emg signals using reservoir computing on an event-based
  neuromorphic system.
\newblock In {\em 2018 IEEE Biomedical Circuits and Systems Conference
  (BioCAS)}, pages 1--4. IEEE, 2018.

\bibitem{sakib2021noise}
Sadman Sakib, Mostafa~M Fouda, Muftah Al-Mahdawi, Attayeb Mohsen, Mikihiko
  Oogane, Yasuo Ando, and Zubair~Md Fadlullah.
\newblock Noise-removal from spectrally-similar signals using reservoir
  computing for mcg monitoring.
\newblock In {\em ICC 2021-IEEE International Conference on Communications},
  pages 1--6. IEEE, 2021.

\bibitem{mohsen2020ai}
Attayeb Mohsen, Muftah Al-Mahdawi, Mostafa~M Fouda, Mikihiko Oogane, Yasuo
  Ando, and Zubair~Md Fadlullah.
\newblock Ai aided noise processing of spintronic based iot sensor for
  magnetocardiography application.
\newblock In {\em ICC 2020-2020 IEEE International Conference on Communications
  (ICC)}, pages 1--6. IEEE, 2020.

\bibitem{hadaeghi2021spatio}
Fatemeh Hadaeghi, Bj{\"o}rn-Philipp Diercks, Daniel Schetelig, Fabrizio
  Damicelli, Insa Wolf, and Ren{\'e} Werner.
\newblock Spatio-temporal feature learning with reservoir computing for t-cell
  segmentation in live-cell ca2+ fluorescence microscopy.
\newblock {\em Scientific reports}, 11(1):1--12, 2021.

\bibitem{chen2017caching}
Mingzhe Chen, Mohammad Mozaffari, Walid Saad, Changchuan Yin, M{\'e}rouane
  Debbah, and Choong~Seon Hong.
\newblock Caching in the sky: Proactive deployment of cache-enabled unmanned
  aerial vehicles for optimized quality-of-experience.
\newblock {\em IEEE Journal on Selected Areas in Communications},
  35(5):1046--1061, 2017.

\bibitem{chen2019liquid}
Mingzhe Chen, Walid Saad, and Changchuan Yin.
\newblock Liquid state machine learning for resource and cache management in
  lte-u unmanned aerial vehicle (uav) networks.
\newblock {\em IEEE Transactions on Wireless Communications}, 18(3):1504--1517,
  2019.

\bibitem{challita2019interference}
Ursula Challita, Walid Saad, and Christian Bettstetter.
\newblock Interference management for cellular-connected uavs: A deep
  reinforcement learning approach.
\newblock {\em IEEE Transactions on Wireless Communications}, 18(4):2125--2140,
  2019.

\bibitem{bacciu2014experimental}
Davide Bacciu, Paolo Barsocchi, Stefano Chessa, Claudio Gallicchio, and Alessio
  Micheli.
\newblock An experimental characterization of reservoir computing in ambient
  assisted living applications.
\newblock {\em Neural Computing and Applications}, 24(6):1451--1464, 2014.

\bibitem{palumbo2016human}
Filippo Palumbo, Claudio Gallicchio, Rita Pucci, and Alessio Micheli.
\newblock Human activity recognition using multisensor data fusion based on
  reservoir computing.
\newblock {\em Journal of Ambient Intelligence and Smart Environments},
  8(2):87--107, 2016.

\bibitem{sun2021sensor}
Linfeng Sun, Zhongrui Wang, Jinbao Jiang, Yeji Kim, Bomin Joo, Shoujun Zheng,
  Seungyeon Lee, Woo~Jong Yu, Bai-Sun Kong, and Heejun Yang.
\newblock In-sensor reservoir computing for language learning via
  two-dimensional memristors.
\newblock {\em Science advances}, 7(20):eabg1455, 2021.

\bibitem{zheng2017brain}
Zhixue Zheng, Simon Morando, Marie-C{\'e}cile Pera, Daniel Hissel, Laurent
  Larger, Romain Martinenghi, and Antonio~Baylon Fuentes.
\newblock Brain-inspired computational paradigm dedicated to fault diagnosis of
  pem fuel cell stack.
\newblock {\em international journal of hydrogen energy}, 42(8):5410--5425,
  2017.

\bibitem{zheng2018fault}
Zhixue Zheng, Marie-C{\'e}cile P{\'e}ra, Daniel Hissel, Laurent Larger,
  Nadia~Yusfi Steiner, and Samir Jemei.
\newblock Fault diagnosis of pemfc systems in the model space using reservoir
  computing.
\newblock In {\em 2018 IEEE Vehicle Power and Propulsion Conference (VPPC)},
  pages 1--5. IEEE, 2018.

\bibitem{zhang2021pre}
Shaohui Zhang, Xiang Duan, Chuan Li, and Ming Liang.
\newblock Pre-classified reservoir computing for the fault diagnosis of 3d
  printers.
\newblock {\em Mechanical Systems and Signal Processing}, 146:106961, 2021.

\bibitem{atencia2019dynamic}
Miguel Atencia, Catalin Stoean, Ruxandra Stoean, Roberto Rodr{\'i}guez-Labrada,
  and Gonzalo Joya.
\newblock Dynamic clustering of time series with echo state networks.
\newblock In Ignacio Rojas, Gonzalo Joya, and Andreu Catala, editors, {\em
  Advances in Computational Intelligence}, pages 73--83, Cham, 2019. Springer
  International Publishing.

\bibitem{atencia2020time}
Miguel Atencia, Claudio Gallicchio, Gonzalo Joya, and Alessio Micheli.
\newblock Time series clustering with deep reservoir computing.
\newblock In {\em International Conference on Artificial Neural Networks},
  pages 482--493. Springer, 2020.

\bibitem{asabuki2018interactive}
Toshitake Asabuki, Naoki Hiratani, and Tomoki Fukai.
\newblock Interactive reservoir computing for chunking information streams.
\newblock {\em PLoS Computational Biology}, 14(10):e1006400, 2018.

\bibitem{krishnagopal2018similarity}
Sanjukta Krishnagopal, Yiannis Aloimonos, and Michelle Girvan.
\newblock Similarity learning and generalization with limited data: A reservoir
  computing approach.
\newblock {\em Complexity}, 2018, 2018.

\bibitem{ozay2015machine}
Mete Ozay, Inaki Esnaola, Fatos Tunay~Yarman Vural, Sanjeev~R Kulkarni, and
  H~Vincent Poor.
\newblock Machine learning methods for attack detection in the smart grid.
\newblock {\em IEEE transactions on neural networks and learning systems},
  27(8):1773--1786, 2015.

\bibitem{hamedani2017reservoir}
Kian Hamedani, Lingjia Liu, Rachad Atat, Jinsong Wu, and Yang Yi.
\newblock Reservoir computing meets smart grids: Attack detection using delayed
  feedback networks.
\newblock {\em IEEE Transactions on Industrial Informatics}, 14(2):734--743,
  2017.

\bibitem{hamedani2019detecting}
Kian Hamedani, Lingjia Liu, Shiyan Hu, Jonathan Ashdown, Jinsong Wu, and Yang
  Yi.
\newblock Detecting dynamic attacks in smart grids using reservoir computing: A
  spiking delayed feedback reservoir based approach.
\newblock {\em IEEE Transactions on Emerging Topics in Computational
  Intelligence}, 4(3):253--264, 2020.

\bibitem{chandrasekaran2021real}
Sanjeev Tannirkulam~Chandrasekaran, Abraham~Peedikayil Kuruvila, Kanad Basu,
  and Arindam Sanyal.
\newblock Real-time hardware-based malware and micro-architectural attack
  detection utilizing cmos reservoir computing.
\newblock {\em IEEE Transactions on Circuits and Systems II: Express Briefs},
  69(2):349--353, 2022.

\bibitem{kokalj2020deep}
Silvija Kokalj-Filipovic, Paul Toliver, William Johnson, Raymond~R Hoare~II,
  and Joseph~J Jezak.
\newblock Deep delay loop reservoir computing for specific emitter
  identification.
\newblock {\em arXiv preprint arXiv:2010.06649}, 2020.

\bibitem{kokalj2021reservoir}
Silvija Kokalj-Filipovic, Paul Toliver, William Johnson, and Rob Miller.
\newblock Reservoir-based distributed machine learning for edge operation of
  emitter identification.
\newblock In {\em MILCOM 2021 - 2021 IEEE Military Communications Conference
  (MILCOM)}, pages 96--101, 2021.

\bibitem{wang2021signal}
Shuai Wang, Nian Fang, and Lutang Wang.
\newblock Signal recovery based on optoelectronic reservoir computing for high
  speed optical fiber communication system.
\newblock {\em Optics Communications}, 495:127082, 2021.

\bibitem{yamane2019application}
Toshiyuki Yamane, Jean~Benoit H{\'e}roux, Hidetoshi Numata, Gouhei Tanaka,
  Ryosho Nakane, and Akira Hirose.
\newblock Application identification of network traffic by reservoir computing.
\newblock In {\em International Conference on Neural Information Processing},
  pages 389--396. Springer, 2019.

\bibitem{ando2019road}
Hiroyasu Ando and Hanten Chang.
\newblock Road traffic reservoir computing.
\newblock {\em arXiv preprint arXiv:1912.00554}, 2019.

\bibitem{mosleh2018brain}
Somayeh~Susanna Mosleh, Lingjia Liu, Cenk Sahin, Yahong~Rosa Zheng, and Yang
  Yi.
\newblock Brain-inspired wireless communications: Where reservoir computing
  meets mimo-ofdm.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  29(10):4694--4708, 2018.

\bibitem{zhou2020deep}
Zhou Zhou, Lingjia Liu, Vikram Chandrasekhar, Jianzhong Zhang, and Yang Yi.
\newblock Deep reservoir computing meets 5g mimo-ofdm systems in symbol
  detection.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 1266--1273, 2020.

\bibitem{morando2013fuel}
Simon Morando, Samir Jemei, Rafael Gouriveau, Noureddine Zerhouni, and Daniel
  Hissel.
\newblock Fuel cells prognostics using echo state network.
\newblock In {\em IECON 2013-39th annual conference of the IEEE industrial
  electronics society}, pages 1632--1637. IEEE, 2013.

\bibitem{morando2017anova}
Simon Morando, Samir Jemei, Daniel Hissel, Rafael Gouriveau, and Noureddine
  Zerhouni.
\newblock Anova method applied to proton exchange membrane fuel cell ageing
  forecasting using an echo state network.
\newblock {\em Mathematics and Computers in Simulation}, 131:283--294, 2017.

\bibitem{mezzi2018multi}
Rania Mezzi, Simon Morando, Nadia~Yousfi Steiner, Marie~Cécile Péra, Daniel
  Hissel, and Laurent Larger.
\newblock Multi-reservoir echo state network for proton exchange membrane fuel
  cell remaining useful life prediction.
\newblock In {\em IECON 2018 - 44th Annual Conference of the IEEE Industrial
  Electronics Society}, pages 1872--1877, 2018.

\bibitem{li2020adaptive}
Zhongliang Li, Zhixue Zheng, and Rachid Outbib.
\newblock Adaptive prognostic of fuel cells by implementing ensemble echo state
  networks in time-varying model space.
\newblock {\em IEEE Transactions on Industrial Electronics}, 67(1):379--389,
  2020.

\bibitem{fonollosa2015reservoir}
Jordi Fonollosa, Sadique Sheik, Ram{\'o}n Huerta, and Santiago Marco.
\newblock Reservoir computing compensates slow response of chemosensor arrays
  exposed to fast varying gas concentrations in continuous monitoring.
\newblock {\em Sensors and Actuators B: Chemical}, 215:618--629, 2015.

\bibitem{wang2020novel}
Jianzhou Wang, Tong Niu, Haiyan Lu, Wendong Yang, and Pei Du.
\newblock A novel framework of reservoir computing for deterministic and
  probabilistic wind power forecasting.
\newblock {\em IEEE Transactions on Sustainable Energy}, 11(1):337--349, 2020.

\bibitem{ghani2010neuro}
Arfan Ghani, TM~McGinnity, Liam Maguire, Liam McDaid, Ammar Belatreche, and
  N~Shabtai.
\newblock Neuro-inspired speech recognition based on reservoir computing.
\newblock {\em Advances in Speech Recognition}, 164, 2010.

\bibitem{alalshekmubarak2014improving}
Abdulrahman Alalshekmubarak and Leslie~S Smith.
\newblock On improving the classification capability of reservoir computing for
  arabic speech recognition.
\newblock In {\em International Conference on Artificial Neural Networks},
  pages 225--232. Springer, 2014.

\bibitem{abreu2020role}
Flavio Abreu~Araujo, Mathieu Riou, Jacob Torrejon, Sumito Tsunegi, Damien
  Querlioz, Kay Yakushiji, Akio Fukushima, Hitoshi Kubota, Shinji Yuasa, Mark~D
  Stiles, et~al.
\newblock Role of non-linear data processing on speech recognition task in the
  framework of reservoir computing.
\newblock {\em Scientific reports}, 10(1):1--11, 2020.

\bibitem{pons2019randomly}
Jordi Pons and Xavier Serra.
\newblock Randomly weighted cnns for (music) audio classification.
\newblock In {\em ICASSP 2019-2019 IEEE international conference on acoustics,
  speech and signal processing (ICASSP)}, pages 336--340. IEEE, 2019.

\bibitem{wang2021stock}
Wei-Jia Wang, Yong Tang, Jason Xiong, and Yi-Cheng Zhang.
\newblock Stock market index prediction based on reservoir computing models.
\newblock {\em Expert Systems with Applications}, 178:115022, 2021.

\bibitem{budhiraja2021reservoir}
Rajat Budhiraja, Manish Kumar, Mrinal~K Das, Anil~Singh Bafila, and Sanjeev
  Singh.
\newblock A reservoir computing approach for forecasting and regenerating both
  dynamical and time-delay controlled financial system behavior.
\newblock {\em Plos one}, 16(2):e0246737, 2021.

\bibitem{cucchi2021reservoir}
Matteo Cucchi, Christopher Gruener, Lautaro Petrauskas, Peter Steiner, Hsin
  Tseng, Axel Fischer, Bogdan Penkovsky, Christian Matthus, Peter Birkholz,
  Hans Kleemann, et~al.
\newblock Reservoir computing with biocompatible organic electrochemical
  networks for brain-inspired biosignal classification.
\newblock {\em Science Advances}, 7(34):eabh0693, 2021.

\bibitem{kudithipudi2016design}
Dhireesha Kudithipudi, Qutaiba Saleh, Cory Merkel, James Thesing, and Bryant
  Wysocki.
\newblock Design and analysis of a neuromemristive reservoir computing
  architecture for biosignal processing.
\newblock {\em Frontiers in neuroscience}, 9:502, 2016.

\bibitem{brodeur2012regulation}
Simon Brodeur and Jean Rouat.
\newblock Regulation toward self-organized criticality in a recurrent spiking
  neural reservoir.
\newblock In {\em International Conference on Artificial Neural Networks},
  pages 547--554. Springer, 2012.

\bibitem{garg2021signals}
Nikhil Garg, Ismael Balafrej, Yann Beilliard, Dominique Drouin, Fabien Alibart,
  and Jean Rouat.
\newblock Signals to spikes for neuromorphic regulated reservoir computing and
  emg hand gesture recognition.
\newblock In {\em International Conference on Neuromorphic Systems 2021}, pages
  1--8, 2021.

\bibitem{shakya2021circuit}
Biraj Shakya, Mostafa~M Fouda, Steve~C Chiu, and Zubair~Md Fadlullah.
\newblock A circuit-embedded reservoir computer for smart noise reduction of
  mcg signals.
\newblock In {\em 2021 IEEE International Conference on Internet of Things and
  Intelligence Systems (IoTaIS)}, pages 56--61. IEEE, 2021.

\bibitem{kleyko2017modality}
Denis Kleyko, Sumeer Khan, Evgeny Osipov, and Suet-Peng Yong.
\newblock Modality classification of medical images with distributed
  representations based on cellular automata reservoir computing.
\newblock In {\em 2017 IEEE 14th International Symposium on Biomedical Imaging
  (ISBI 2017)}, pages 1053--1056. IEEE, 2017.

\bibitem{eder2018morphological}
M~Eder, Florian Hisch, and Helmut Hauser.
\newblock Morphological computation-based control of a modular, pneumatically
  driven, soft robotic arm.
\newblock {\em Advanced Robotics}, 32(7):375--385, 2018.

\bibitem{vargas2015swing}
Aldo Vargas, Murray~L Ireland, and David Anderson.
\newblock Swing-free manoeuvre controller for rotorcraft unmanned aerial
  vehicle slung-load system using echo state networks.
\newblock {\em International Journal of Unmanned Systems Engineering.},
  3(1):26, 2015.

\bibitem{tanaka2021flapping}
Kazutoshi Tanaka, Shih-Hsin Yang, Yuji Tokudome, Yuna Minami, Yuyao Lu,
  Takayuki Arie, Seiji Akita, Kuniharu Takei, and Kohei Nakajima.
\newblock Flapping-wing dynamics as a natural detector of wind direction.
\newblock {\em Advanced Intelligent Systems}, 3(2):2000174, 2021.

\bibitem{jalalvand2018application}
Azarakhsh Jalalvand, Kris Demuynck, Wesley De~Neve, and Jean-Pierre Martens.
\newblock On the application of reservoir computing networks for noisy image
  recognition.
\newblock {\em Neurocomputing}, 277:237--248, 2018.

\bibitem{tong2018reservoir}
Zhiqiang Tong and Gouhei Tanaka.
\newblock Reservoir computing with untrained convolutional neural networks for
  image recognition.
\newblock In {\em 2018 24th International Conference on Pattern Recognition
  (ICPR)}, pages 1289--1294. IEEE, 2018.

\bibitem{tanaka2022reservoir}
Yuichiro Tanaka and Hakaru Tamukoh.
\newblock Reservoir-based convolution.
\newblock {\em Nonlinear Theory and Its Applications, IEICE}, 13(2):397--402,
  2022.

\bibitem{chang2020reinforcement}
Hanten Chang and Katsuya Futagami.
\newblock Reinforcement learning with convolutional reservoir computing.
\newblock {\em Applied Intelligence}, 50:2400--2410, 2020.

\bibitem{chang2019convolutional}
Hanten Chang and Katsuya Futagami.
\newblock Convolutional reservoir computing for world models.
\newblock {\em arXiv preprint arXiv:1907.08040}, 2019.

\bibitem{yoshihiro2020}
Yonemura Yoshihiro and Katori Yuichi.
\newblock Image recognition model based on convolutional reservoir computing.
\newblock In {\em The 34th Annual Conference of the Japanese Society for
  Artificial Intelligence}, 2020.

\bibitem{schaetti2016echo}
Nils Schaetti, Michel Salomon, and Rapha{\"e}l Couturier.
\newblock Echo state networks-based reservoir computing for mnist handwritten
  digits recognition.
\newblock In {\em 2016 IEEE Intl Conference on Computational Science and
  Engineering (CSE) and IEEE Intl Conference on Embedded and Ubiquitous
  Computing (EUC) and 15th Intl Symposium on Distributed Computing and
  Applications for Business Engineering (DCABES)}, pages 484--491. IEEE, 2016.

\bibitem{jacobson2021image}
Philip Jacobson, Mizuki Shirao, Kerry Yu, Guan-Lin Su, and Ming~C Wu.
\newblock Image classification using delay-based optoelectronic reservoir
  computing.
\newblock In {\em AI and Optical Data Sciences II}, volume 11703, pages
  120--126. SPIE, 2021.

\bibitem{yang2022optical}
Yigong Yang, Pei Zhou, Taiyi Chen, Yu~Huang, and Nianqiang Li.
\newblock Optical neuromorphic computing based on a large-scale laterally
  coupled laser array.
\newblock {\em Optics Communications}, 521:128599, 2022.

\bibitem{moran2018reservoir}
Alejandro Mor{\'a}n, Christiam~F Frasser, and Josep~L Rossell{\'o}.
\newblock Reservoir computing hardware with cellular automata.
\newblock {\em arXiv preprint arXiv:1806.04932}, 2018.

\bibitem{jacobson2021hybrid}
Philip Jacobson, Mizuki Shirao, Kerry Yu, Guan-Lin Su, and Ming~C Wu.
\newblock Hybrid convolutional optoelectronic reservoir computing for image
  recognition.
\newblock {\em Journal of Lightwave Technology}, 40(3):692--699, 2021.

\bibitem{an2020unified}
Qiyuan An, Kangjun Bai, Lingjia Liu, Fangyang Shen, and Yang Yi.
\newblock A unified information perceptron using deep reservoir computing.
\newblock {\em Computers \& Electrical Engineering}, 85:106705, 2020.

\bibitem{vargas2021continual}
Danilo~Vasconcellos Vargas and Toshitake Asabuki.
\newblock Continual general chunking problem and syncmap.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 10006--10014, 2021.

\bibitem{asabuki2020somatodendritic}
Toshitake Asabuki and Tomoki Fukai.
\newblock Somatodendritic consistency check for temporal feature segmentation.
\newblock {\em Nature communications}, 11(1):1--13, 2020.

\bibitem{hamedani2019novel}
Kian Hamedani, Zhou Zhou, Kangjun Bai, and Lingjia Liu.
\newblock The novel applications of deep reservoir computing in cyber-security
  and wireless communication.
\newblock In {\em Intelligent System and Computing}. IntechOpen, 2019.

\bibitem{yu2013precise}
Qiang Yu, Huajin Tang, Kay~Chen Tan, and Haizhou Li.
\newblock Precise-spike-driven synaptic plasticity: Learning hetero-association
  of spatiotemporal spike patterns.
\newblock {\em Plos one}, 8(11):e78318, 2013.

\bibitem{lukovsevicius2006time}
Mantas Luko{\v{s}}evicius, Dan Popovici, Herbert Jaeger, Udo Siewert, and
  Residence Park.
\newblock Time warping invariant echo state networks.
\newblock {\em International University Bremen, Technical Report}, 2006.

\bibitem{holzmann2009reservoir}
Georg Holzmann.
\newblock Reservoir computing: a powerful black-box framework for nonlinear
  audio processing.
\newblock In {\em International Conference on Digital Audio Effects (DAFx)}.
  Citeseer, 2009.

\bibitem{keuninckx2017real}
Lars Keuninckx, Jan Danckaert, and Guy Van~der Sande.
\newblock Real-time audio processing with a cascade of discrete-time delay
  line-based reservoir computers.
\newblock {\em Cognitive Computation}, 9(3):315--326, 2017.

\bibitem{schrauwen2003bsa}
Benjamin Schrauwen and Jan Van~Campenhout.
\newblock Bsa, a fast and accurate spike train encoding scheme.
\newblock In {\em Proceedings of the International Joint Conference on Neural
  Networks, 2003.}, volume~4, pages 2825--2830. IEEE, 2003.

\bibitem{lin2009short}
Xiaowei Lin, Zehong Yang, and Yixu Song.
\newblock Short-term stock price prediction based on echo state networks.
\newblock {\em Expert systems with applications}, 36(3):7313--7317, 2009.

\bibitem{mante2013context}
Valerio Mante, David Sussillo, Krishna~V Shenoy, and William~T Newsome.
\newblock Context-dependent computation by recurrent dynamics in prefrontal
  cortex.
\newblock {\em nature}, 503(7474):78--84, 2013.

\bibitem{dominey2000neural}
Peter~Ford Dominey and Franck Ramus.
\newblock Neural network processing of natural language: I. sensitivity to
  serial, temporal and abstract structure of language in the infant.
\newblock {\em Language and Cognitive Processes}, 15(1):87--127, 2000.

\bibitem{dominey2009cortico}
Peter~F Dominey and Toshio Inui.
\newblock Cortico-striatal function in sentence comprehension: Insights from
  neurophysiology and modeling.
\newblock {\em Cortex}, 45(8):1012--1018, 2009.

\bibitem{dominey2009neural}
Peter~Ford Dominey, Toshio Inui, and Michel Hoen.
\newblock Neural network processing of natural language: Ii. towards a unified
  model of corticostriatal function in learning sentence comprehension and
  non-linguistic sequencing.
\newblock {\em Brain and language}, 109(2-3):80--92, 2009.

\bibitem{hinaut2013real}
Xavier Hinaut and Peter~Ford Dominey.
\newblock Real-time parallel processing of grammatical structure in the
  fronto-striatal system: A recurrent network simulation study using reservoir
  computing.
\newblock {\em PloS one}, 8(2):e52946, 2013.

\bibitem{dominey2013recurrent}
Peter~F Dominey.
\newblock Recurrent temporal networks and language acquisition—from
  corticostriatal neurophysiology to reservoir computing.
\newblock {\em Frontiers in psychology}, 4:500, 2013.

\bibitem{enel2016reservoir}
Pierre Enel, Emmanuel Procyk, Ren{\'e} Quilodran, and Peter~Ford Dominey.
\newblock Reservoir computing properties of neural dynamics in prefrontal
  cortex.
\newblock {\em PLoS computational biology}, 12(6):e1004967, 2016.

\bibitem{dominey2021cortico}
Peter~Ford Dominey.
\newblock Cortico-striatal origins of reservoir computing, mixed selectivity,
  and higher cognitive function.
\newblock In {\em Reservoir Computing}, pages 29--58. Springer, 2021.

\bibitem{fusi2016neurons}
Stefano Fusi, Earl~K Miller, and Mattia Rigotti.
\newblock Why neurons mix: high dimensionality for higher cognition.
\newblock {\em Current opinion in neurobiology}, 37:66--74, 2016.

\bibitem{johnston2020nonlinear}
W~Jeffrey Johnston, Stephanie~E Palmer, and David~J Freedman.
\newblock Nonlinear mixed selectivity supports reliable neural computation.
\newblock {\em PLOS computational biology}, 16(2):e1007544, 2020.

\bibitem{ledergerber2021task}
Debora Ledergerber, Claudia Battistin, Jan~Sigurd Blackstad, Richard~J Gardner,
  Menno~P Witter, May-Britt Moser, Yasser Roudi, and Edvard~I Moser.
\newblock Task-dependent mixed selectivity in the subiculum.
\newblock {\em Cell reports}, 35(8):109175, 2021.

\bibitem{wallach2022mixed}
Avner Wallach, Alexandre Melanson, Andr{\'e} Longtin, and Leonard Maler.
\newblock Mixed selectivity coding of sensory and motor social signals in the
  thalamus of a weakly electric fish.
\newblock {\em Current Biology}, 32(1):51--63, 2022.

\bibitem{tovee1992functional}
Martin~J Tov{\'e}e and Edmund~T Rolls.
\newblock The functional nature of neuronal oscillations.
\newblock {\em Trends in neurosciences}, 15(10):387, 1992.

\bibitem{vanrullen2005spike}
Rufin VanRullen, Rudy Guyonneau, and Simon~J Thorpe.
\newblock Spike times make sense.
\newblock {\em Trends in neurosciences}, 28(1):1--4, 2005.

\bibitem{gray1989oscillatory}
Charles~M Gray, Peter K{\"o}nig, Andreas~K Engel, and Wolf Singer.
\newblock Oscillatory responses in cat visual cortex exhibit inter-columnar
  synchronization which reflects global stimulus properties.
\newblock {\em Nature}, 338(6213):334--337, 1989.

\bibitem{singer1995visual}
Wolf Singer and Charles~M Gray.
\newblock Visual feature integration and the temporal correlation hypothesis.
\newblock {\em Annual review of neuroscience}, 18(1):555--586, 1995.

\bibitem{singer1999neuronal}
Wolf Singer.
\newblock Neuronal synchrony: a versatile code for the definition of relations?
\newblock {\em Neuron}, 24(1):49--65, 1999.

\bibitem{engel2001dynamic}
Andreas~K Engel, Pascal Fries, and Wolf Singer.
\newblock Dynamic predictions: oscillations and synchrony in top--down
  processing.
\newblock {\em Nature Reviews Neuroscience}, 2(10):704--716, 2001.

\bibitem{singer2018neuronal}
Wolf Singer.
\newblock Neuronal oscillations: unavoidable and useful?
\newblock {\em European Journal of Neuroscience}, 48(7):2389--2398, 2018.

\bibitem{buzsaki2004neuronal}
Gyorgy Buzsaki and Andreas Draguhn.
\newblock Neuronal oscillations in cortical networks.
\newblock {\em science}, 304(5679):1926--1929, 2004.

\bibitem{fries2015rhythms}
Pascal Fries.
\newblock Rhythms for cognition: communication through coherence.
\newblock {\em Neuron}, 88(1):220--235, 2015.

\bibitem{glass1994periodic}
Leon Glass and Jiong Sun.
\newblock Periodic forcing of a limit-cycle oscillator: Fixed points, arnold
  tongues, and the global organization of bifurcations.
\newblock {\em Physical Review E}, 50(6):5077, 1994.

\bibitem{cover1965geometrical}
Thomas~M Cover.
\newblock Geometrical and statistical properties of systems of linear
  inequalities with applications in pattern recognition.
\newblock {\em IEEE transactions on electronic computers}, (3):326--334, 1965.

\bibitem{buzsaki2006Brain1}
Buzsáki György.
\newblock {175The Brain’s Default State: Self-Organized Oscillations in Rest
  and Sleep}.
\newblock In {\em {Rhythms of the Brain}}. Oxford University Press, 10 2006.

\bibitem{nikolic2009distributed}
Danko Nikoli{\'c}, Stefan H{\"a}usler, Wolf Singer, and Wolfgang Maass.
\newblock Distributed fading memory for stimulus properties in the primary
  visual cortex.
\newblock {\em PLoS biology}, 7(12):e1000260, 2009.

\bibitem{caporale2008spike}
Natalia Caporale and Yang Dan.
\newblock Spike timing--dependent plasticity: a hebbian learning rule.
\newblock {\em Annu. Rev. Neurosci.}, 31:25--46, 2008.

\bibitem{subramoney2021reservoirs}
Anand Subramoney, Franz Scherr, and Wolfgang Maass.
\newblock Reservoirs learn to learn.
\newblock In {\em Reservoir Computing}, pages 59--76. Springer, 2021.

\bibitem{verstraeten2012oger}
David Verstraeten, Benjamin Schrauwen, Sander Dieleman, Philemon Brakel, Pieter
  Buteneers, and Dejan Pecevski.
\newblock Oger: modular learning architectures for large-scale sequential
  processing.
\newblock {\em The Journal of Machine Learning Research}, 13(1):2995--2998,
  2012.

\bibitem{oger2012}
Oger.
\newblock \url{https://github.com/neuronalX/Oger}, 2018.

\bibitem{easyesn2017}
easyesn.
\newblock \url{https://github.com/kalekiu/easyesn}, 2017.

\bibitem{thiede2017easyesn}
LA~Thiede and RS~Zimmermann.
\newblock Easyesn: a library for recurrent neural networks using echo state
  networks.
\newblock {\em Easyesn: a library for recurrent neural networks using echo
  state networks}, 2017.

\bibitem{echotorch2018}
Nils Schaetti.
\newblock Echotorch: Reservoir computing with pytorch.
\newblock \url{https://github.com/nschaetti/EchoTorch}, 2018.

\bibitem{martinuzzi2022reservoircomputing}
Francesco Martinuzzi, Chris Rackauckas, Anas Abdelrehim, Miguel~D Mahecha, and
  Karin Mora.
\newblock Reservoircomputing. jl: An efficient and modular library for
  reservoir computing models.
\newblock {\em Journal of Machine Learning Research}, 23(288):1--8, 2022.

\bibitem{trouvain2022create}
Nathan Trouvain, Nicolas Rougier, and Xavier Hinaut.
\newblock Create efficient and complex reservoir computing architectures with
  reservoirpy.
\newblock In {\em From Animals to Animats 16: 16th International Conference on
  Simulation of Adaptive Behavior, SAB 2022, Cergy-Pontoise, France, September
  20--23, 2022, Proceedings}, pages 91--102. Springer, 2022.

\bibitem{foong2023generating}
Tham~Yik Foong and Danilo~Vasconcellos Vargas.
\newblock Generating oscillation activity with echo state network to mimic the
  behavior of a simple central pattern generator.
\newblock {\em arXiv preprint arXiv:2306.10927}, 2023.

\bibitem{dale2021reservoir}
Matthew Dale, Julian~F Miller, Susan Stepney, and Martin~A Trefzer.
\newblock Reservoir computing in material substrates.
\newblock In {\em Reservoir Computing}, pages 141--166. Springer, 2021.

\bibitem{tanaka2022guest}
Gouhei Tanaka, Claudio Gallicchio, Alessio Micheli, Juan-Pablo Ortega, and
  Akira Hirose.
\newblock Guest editorial special issue on new frontiers in extremely efficient
  reservoir computing.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  33(6):2571--2574, 2022.

\bibitem{paassen2022reservoir}
Benjamin Paaßen, Alexander Schulz, Terrence~C. Stewart, and Barbara Hammer.
\newblock Reservoir memory machines as neural computers.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  33(6):2575--2585, 2022.

\bibitem{jungling2022consistency}
Thomas Jüngling, Thomas Lymburn, and Michael Small.
\newblock Consistency hierarchy of reservoir computers.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  33(6):2586--2595, 2022.

\bibitem{manjunath2022echo}
G.~Manjunath.
\newblock An echo state network imparts a curve fitting.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  33(6):2596--2604, 2022.

\bibitem{schwedersky2022adaptive}
Bernardo~Barancelli Schwedersky, Rodolfo César~Costa Flesch, and Samuel~Bahu
  Rovea.
\newblock Adaptive practical nonlinear model predictive control for echo state
  network models.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  33(6):2605--2614, 2022.

\bibitem{kong2023reservoir}
Ling-Wei Kong, Yang Weng, Bryan Glaz, Mulugeta Haile, and Ying-Cheng Lai.
\newblock Reservoir computing as digital twins for nonlinear dynamical systems.
\newblock {\em Chaos: An Interdisciplinary Journal of Nonlinear Science},
  33(3), 2023.

\bibitem{jalalvand2022real}
Azarakhsh Jalalvand, Joseph Abbate, Rory Conlin, Geert Verdoolaege, and Egemen
  Kolemen.
\newblock Real-time and adaptive reservoir computing with application to
  profile prediction in fusion plasma.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  33(6):2630--2641, 2022.

\bibitem{pasa2022multiresolution}
Luca Pasa, Nicolò Navarin, and Alessandro Sperduti.
\newblock Multiresolution reservoir graph neural network.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  33(6):2642--2653, 2022.

\bibitem{pedrelli2022hierarchical}
Luca Pedrelli and Xavier Hinaut.
\newblock Hierarchical-task reservoir for online semantic analysis from
  continuous speech.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  33(6):2654--2663, 2022.

\bibitem{gupta2022neuromorphic}
Sarthak Gupta, Satrajit Chakraborty, and Chetan~Singh Thakur.
\newblock Neuromorphic time-multiplexed reservoir computing with on-the-fly
  weight generation for edge devices.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  33(6):2676--2685, 2022.

\bibitem{nakajima2022neural}
Mitsumasa Nakajima, Kenji Tanaka, and Toshikazu Hashimoto.
\newblock Neural schrödinger equation: Physical law as deep neural network.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  33(6):2686--2700, 2022.

\bibitem{kleyko2022cellular}
Denis Kleyko, Edward~Paxon Frady, and Friedrich~T. Sommer.
\newblock Cellular automata can reduce memory requirements of collective-state
  computing.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  33(6):2701--2713, 2022.

\bibitem{yuste2005cortex}
Rafael Yuste, Jason~N MacLean, Jeffrey Smith, and Anders Lansner.
\newblock The cortex as a central pattern generator.
\newblock {\em Nature Reviews Neuroscience}, 6(6):477--483, 2005.

\bibitem{duysens1998neural}
Jacques Duysens and Henry~WAA Van~de Crommert.
\newblock Neural control of locomotion; part 1: The central pattern generator
  from cats to humans.
\newblock {\em Gait \& posture}, 7(2):131--141, 1998.

\bibitem{jazayeri2021interpreting}
Mehrdad Jazayeri and Srdjan Ostojic.
\newblock Interpreting neural computations by examining intrinsic and embedding
  dimensionality of neural activity.
\newblock {\em Current opinion in neurobiology}, 70:113--120, 2021.

\bibitem{maass2004methods}
Wolfgang Maass, Robert Legenstein, and Nils Bertschinger.
\newblock Methods for estimating the computational power and generalization
  capability of neural microcircuits.
\newblock {\em Advances in neural information processing systems}, 17, 2004.

\bibitem{kohonen1982self}
Teuvo Kohonen.
\newblock Self-organized formation of topologically correct feature maps.
\newblock {\em Biological cybernetics}, 43(1):59--69, 1982.

\bibitem{seoane2019evolutionary}
Lu{\'\i}s~F Seoane.
\newblock Evolutionary aspects of reservoir computing.
\newblock {\em Philosophical Transactions of the Royal Society B},
  374(1774):20180377, 2019.

\bibitem{carroll2019network}
Thomas~L Carroll and Louis~M Pecora.
\newblock Network structure effects in reservoir computers.
\newblock {\em Chaos: An Interdisciplinary Journal of Nonlinear Science},
  29(8):083130, 2019.

\bibitem{berner2021patterns}
Rico Berner.
\newblock {\em Patterns of synchrony in complex networks of adaptively coupled
  oscillators}.
\newblock Springer Nature, 2021.

\end{thebibliography}
\end{comment}


\bibliographystyle{unsrt}
\bibliography{main_arxiv.bbl}

\begin{comment}
\section{Introduction}
\lipsum[2]
\lipsum[3]


\section{Headings: first level}
\label{sec:headings}

\lipsum[4] See Section \ref{sec:headings}.

\subsection{Headings: second level}
\lipsum[5]
\begin{equation}
\xi _{ij}(t)=P(x_{t}=i,x_{t+1}=j|y,v,w;\theta)= {\frac {\alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}{\sum _{i=1}^{N} \sum _{j=1}^{N} \alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}}
\end{equation}

\subsubsection{Headings: third level}
\lipsum[6]

\paragraph{Paragraph}
\lipsum[7]

\section{Examples of citations, figures, tables, references}
\label{sec:others}
\lipsum[8] \cite{kour2014real,kour2014fast} and see \cite{hadash2018estimate}.

The documentation for \verb+natbib+ may be found at
\begin{center}
  \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}
Of note is the command \verb+\citet+, which produces citations
appropriate for use in inline text.  For example,
\begin{verbatim}
   \citet{hasselmo} investigated\dots
\end{verbatim}
produces
\begin{quote}
  Hasselmo, et al.\ (1995) investigated\dots
\end{quote}

\begin{center}
  \url{https://www.ctan.org/pkg/booktabs}
\end{center}


\subsection{Figures}
\lipsum[10] 
See Figure \ref{fig:fig1}. Here is how you add footnotes. \footnote{Sample of the first footnote.}
\lipsum[11] 

% Figure environment removed

\subsection{Tables}
\lipsum[12]
See awesome Table~\ref{tab:table}.

\begin{table}
 \caption{Sample table title}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule(r){1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
  \label{tab:table}
\end{table}

\subsection{Lists}
\begin{itemize}
\item Lorem ipsum dolor sit amet
\item consectetur adipiscing elit. 
\item Aliquam dignissim blandit est, in dictum tortor gravida eget. In ac rutrum magna.
\end{itemize}


\section{Conclusion}
Your conclusion here

\section*{Acknowledgments}
This was was supported in part by......

%Bibliography
\bibliographystyle{unsrt}  
\bibliography{references}  
\end{comment}


\end{document}
