\section{Introduction and Motivation}
\label{s:introduction}

Task-oriented dialog (\tod), where a human user engages in a conversation with a system agent
with the aim of completing a concrete task, is one of the central objectives, hallmarks, and applications of machine intelligence \cite[\textit{inter alia}]{Gupta:2006,Tur:2010,Young:2010}. \tod technology has been proven useful across  a wide spectrum of application sectors such as hospitality industry \cite{Henderson:2014dstc3,Henderson:2019poly}, healthcare \cite{Laranjo:2018healthcare}, online shopping \cite{Yan:2017aaai}, banking \cite{Altinok:2018arxiv}, and travel \cite{Raux:2005letsgo,el-asri-etal-2017-frames}, among others.

Wider developments in \tod have been hampered by the two conflicting requirements: \textbf{1)} large-scale in-domain datasets are crucially required in order to unlock the potential of deep learning-based \tod components and systems to handle complex dialog patterns \cite{Budzianowski:2018multiwoz,Lin:2021bitod}; at the same time \textbf{2)} data collection for \tod is known to be notoriously difficult as it is extremely time-consuming, expensive, and requires expert and domain knowledge \cite{Shah:2018naacl,Larson:2022arxiv}. Put simply, the creation of \tod datasets for new domains and languages incurs significantly higher time and budget costs than for most other NLP tasks \cite{Casanueva:2022nlupp}. Consequently, the progress in \tod until recently has been limited only to a small number of high-resource languages such as English and Chinese \cite{Razumovskaia:2022survey}.





Recent work has recognized the need to expand the reach of multilingual \tod technology to more languages via collecting multilingual \tod data \cite{Razumovskaia:2022survey}. Yet, as discussed in more detail later in \S\ref{s:rw}, all the currently available multilingual \tod datasets suffer from one or several serious limitations: (i) the predominant reliance on translation-based data creation that introduces issues with `translationese' and artificial performance inflation \cite{xu-etal-2020-end,Zuo:2021allwoz}; (ii) lack of \textit{cultural adaptation} also results in artificial dialogs that are not localized nor adapted to real-world data and to cultural specificities of each target language and culture; (iii) small scale and lack of sufficient training data prevents truly equitable multilingual development and in-depth comparative cross-language analyses \cite{ding-etal-2022-globalwoz,hung-etal-2022-multi2woz}; (iv) lack of coherent and multi-parallel dialogs in all the represented languages, which are typically not created and corrected by native speakers, hinders meaningful cross-language comparisons and analyses \cite{ding-etal-2022-globalwoz}; (v) some datasets focus on a single component of a full \tod system, typically Natural Language Understanding (NLU), which prevents training and evaluation of other crucial tasks such as Dialog State Tracking (DST), or Natural Language Generation (NLG) in multilingual and transfer setups.



In this work, we address all the aforementioned limitations of current multilingual \tod datasets and present a large-scale data collection process that resulted in a novel large-scale multilingual dataset for \tod: \textbf{\dataset}. The departure point of our data collection is the established \textit{multi-domain} English MultiWOZ dataset \cite{Budzianowski:2018multiwoz}, that is, its cleaned version 2.3 in particular \cite{Han:2021multiwoz23}. \dataset is then created via adapting a recent \textit{bottom-up outline-based} approach of \newcite{Majewska:2023cod} which bypasses (the issues of) the translation-based design and discerns between language-agnostic \textit{abstract dialog schemata} (i.e., \textit{outlines}) and adapted, language-specific \textit{surface realizations} of the underlying schemata (i.e, the actual user and system \textit{utterances}). We validate the usefulness and feasibility of the outline-based approach to multilingual \tod data creation for the first time on a large scale, and prove its feasibility for such large-scale endeavors: the dataset contains a total of 494,116 dialog turns created manually by human subjects.




Guided by the need to tackle the present limitations, \dataset is the first multilingual \tod dataset with the following crucial properties; see also Table~\ref{tab:summary_dataset} for an overview.
First, \dataset is \textit{large-scale} with the equal number of training (7,440 dialogs per language), development (860), and test dialogs (860) offered in 4 different languages: English, Arabic, French, and Turkish. It is more versatile than all prior multilingual \tod datasets as it allows for training and evaluation in monolingual, multilingual, and cross-lingual setups, and in zero-shot, few-shot, and `many'-shot \textit{cross-lingual} and \textit{cross-domain} transfer scenarios. Second, \dataset offers \textit{multi-parallel} dialogs, conveying comparable information over exactly the same conversational flows across all four languages. This property allows for cross-language studies and comparative analyses. Third, \dataset enables \textit{both} (monolingual and multilingual) training and evaluation over different constituent \tod tasks such as NLU (intent detection and slot filling), DST, NLG, as well as full-fledged end-to-end (E2E) learning. Fourth, \dataset is \textit{localized} and \textit{culturally adapted} to the actual existing entities from the cultures in which the target languages are spoken. Finally, created in a bottom-up fashion by native speakers of the target languages, hence \textit{linguistically adapted} to the target language, it offers natural and native dialogs in all target languages, avoiding `translationese' and preventing over-inflation of transfer performance~\cite{Majewska:2023cod}.

Furthermore, to guide future research, we set reference scores across different \tod tasks in all the languages of \dataset, running a representative set of standard baselines in each relevant \tod task. The results clearly indicate the challenging nature of the dataset; we also outline the differences in performance across different languages.











 




