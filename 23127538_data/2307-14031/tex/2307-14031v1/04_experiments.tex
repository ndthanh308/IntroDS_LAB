
\section{\dataset as a \tod Benchmark}
\dataset establishes a multilingual and cross-lingual benchmark for \tod systems and their sub-modules. We now present a first `benchmarking study' on the dataset, evaluating representative models for NLU, DST, NLG, and E2E tasks in \tod, merely scratching the surface of possible experimental work now enabled by \dataset.







\begin{table*}[!t]
\centering
\def\arraystretch{0.67}
{\scriptsize
\resizebox{0.77\textwidth}{!}{%
\begin{tabular}{@{}lccccccccccc@{}}
\toprule
                           &  & \multicolumn{2}{c}{Intent Detection} &  & \multicolumn{3}{c}{Slot Labeling} &  & \multicolumn{3}{c}{Dialog State Tracking} \\ \cmidrule(lr){3-4} \cmidrule(lr){6-8} \cmidrule(l){10-12} 
\multirow{-2}{*}{Language} &  & Accuracy           & F1              &  & Precision    & Recall    & F1      &  & JGA         & Turn Acc.      & F1         \\ \midrule
\multicolumn{12}{c}{\cellcolor[HTML]{EFEFEF}\textbf{Fully Supervised (Monolingual)}}                                                                                                \\ \midrule
ENG                        &  & 92.71              & 95.77           &  & 95.92        & 94.08     & 94.99   &  & 59.90       & 97.87          & 93.67      \\
ARA                        &  & 92.20              & 94.59           &  & 48.44        & 42.47     & 45.26   &  & 47.72       & 96.85          & 89.26      \\
FRA                        &  & 88.92              & 92.93           &  & 79.57        & 77.76     & 78.65   &  & 49.77       & 97.02          & 89.93      \\
TUR                        &  & 91.50              & 94.52           &  & 87.25        & 86.72     & 86.94   &  & 53.59       & 97.28          & 91.04      \\
AVG.                       &  & 91.33              & 94.45           &  & 77.80        & 75.26     & 76.46   &  & 52.74       & 97.26          & 90.97      \\ \midrule
\multicolumn{12}{c}{\cellcolor[HTML]{EFEFEF}\textbf{Zero-Shot Cross-lingual Transfer} (from English)}                                                                               \\ \midrule
ARA                        &  & 60.28              & 71.06           &  & 17.56        & 27.74     & 21.47   &  & 1.47        & 80.73          & 5.80       \\
FRA                        &  & 72.88              & 81.71           &  & 48.53        & 60.52     & 53.86   &  & 3.66        & 85.08          & 32.83      \\
TUR                        &  & 69.52              & 79.09           &  & 48.47        & 66.80     & 56.18   &  & 1.30        & 82.05          & 15.22      \\
AVG.                       &  & 67.56              & 77.29           &  & 38.19        & 51.69     & 43.84   &  & 2.14        & 82.62          & 17.95      \\ 




\bottomrule


\end{tabular}
}
}%
\caption{Fully supervised and zero-shot cross-lingual transfer from English ($\mathbbm{D}^{\eng}$ as the source) for ID, SL, and DST tasks on \dataset. AVG. shows the mean average of the evaluation scores across all four languages. The reported scores are averaged over 3 random runs.}
\label{tab:nlu-results}
\end{table*}




\begin{table*}[t!]
\centering
\def\arraystretch{0.67}
{\scriptsize
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llccccccccccc@{}}
\toprule
\multirow{2}{*}{Language} &  & \multicolumn{3}{c}{Surface Realization} &  & \multicolumn{3}{c}{Language Modeling} &  & \multicolumn{3}{c}{Language Modeling with Oracle} \\ \cmidrule(lr){3-5} \cmidrule(lr){7-9} \cmidrule(l){11-13} 
                          &  & BLEU        & ROUGE       & METEOR      &  & BLEU       & ROUGE       & METEOR      &  & BLEU            & ROUGE          & METEOR          \\ \midrule
ENG                       &  & 20.67       & 47.76       & 44.16       &  & 8.66       & 27.95       & 25.18       &  & 21.20           & 48.52          & 44.31           \\
ARA                       &  & 9.57        & 14.04       & 21.92       &  & 7.22       & 20.77       & 18.11       &  & 17.56           & 15.99          & 35.22           \\
FRA                       &  & 9.96        & 35.31       & 29.17       &  & 6.19       & 24.47       & 19.78       &  & 13.61           & 40.69          & 34.87           \\
TUR                       &  & 13.59       & 39.29       & 33.99       &  & 9.87       & 30.07       & 26.84       &  & 24.23           & 53.76          & 48.49           \\
AVG.                      &  & 13.45       & 34.10       & 32.31       &  & 7.98       & 21.14       & 22.48       &  & 19.15           & 39.74          & 40.72           \\ \bottomrule
\end{tabular}%
}
}%
\caption{Fully supervised NLG performance for mT5$_\textit{small}$. AVG. shows the mean average of the evaluation scores across all four languages. The reported scores are averaged over 3 random runs.}
\label{tab:nlg_results}
\end{table*}




\rparagraph{Natural Language Understanding}
NLU is typically decomposed into two established tasks: intent detection (ID) and slot labeling (SL). ID can be cast as a multi-class classification task that identifies the presence of a domain-intent pair $d$-$i$ (e.g., \texttt{Restaurant-Inform}) from the user's utterance, where the set of intents is predefined in the ontology. SL is a sequence tagging task that identifies the presence of a value $v$ and its corresponding slot $s$ within the utterance.


We evaluate ID and SL methods backed by XLM-R$_\textit{base}$ \citep{conneau-etal-2020-unsupervised}.
Precisely, at each dialog turn $t$, the model encodes the concatenation of the previous two utterances ($\mathbf{u}_{t-2}$ and $\mathbf{u}_{t-1}$) along with the current utterance ($\mathbf{u}_{t}$) to provide embedding vectors at both the sequence and token levels.
To implement the intent detector, for each domain-intent pair $d$-$i$ defined by the ontology, the representation of the ``\texttt{<s>}'' token is subsequently projected down to two logits and passed through a Sigmoid layer to form a Bernoulli distribution indicating if $d$-$i$ appears in the $\mathbf{u}_{t}$. Performance is evaluated by measuring its accuracy in identifying the exact presence of all domain-intent pairs in a dialog act, as well as its F1 score. For SL, we adopt the widely-used BIO labeling scheme to annotate each token in the user's utterance.\footnote{Specifically, each token is labeled with either B-$d$-$i$-$s$ (e.g., \textit{B-Restaurant-Inform-Food}), denoting the beginning of a slot-value pair with the corresponding slot name, I-$d$-$i$-$s$ indicating it is inside the slot-value, or O indicating that the token is not associated with any slot-value pair.}
\footnote{We conducted all NLU experiments on a single RTX 24 GiB GPU with a batch size of 64 and a learning rate of $2e-5$. We trained each model for 10 epochs and selected the model with the best F1 score on the validation set as the final model.}



In Table~\ref{tab:nlu-results}, we observe that the fully supervised ID model achieves similarly high accuracy across all languages, and we also observe a large cross-lingual transfer gap~\cite{Hu:2020xtreme} for both tasks. Further, there is a substantial decrease in performance for Arabic SL.
Note that in \dataset the slot-value spans are annotated at the character level, and we only consider a span to be correctly identified if there is an exact match. At the same time, \citet{rust-etal-2021-good} observed that the sub-optimal performance of the tokenizers for the multilingual models may yield degraded downstream performance. To investigate the limitations of tokenization, we then aligned the slot boundaries with the token boundaries. Specifically, we defined the slot span as the minimal token span that covered the entire slot in the utterance. With this approach, the identical model achieved F1 of 78.44 ($\uparrow$30.00) for Arabic SL, confirming that the suboptimal XLM-R's tokenization was the primary contributor to the original performance degradation in Arabic.









\rparagraph{Dialog State Tracking}
For DST, we follow the standard MultiWOZ preprocessing and evaluation setups \citep{wu-etal-2019-transferable}, excluding the `hospital' and `police' domains due to the absence of test dialogs in these domains. We report the Joint Goal Accuracy (JGA), Turn Accuracy, and Joint F1.


We adapt T5DST \citep{lin-etal-2021-leveraging} as a strong baseline that reformulates the DST as a QA task with slot descriptions. The DST model is back-boned with mT5$_\textit{small}$ \citep{xue-etal-2021-mt5} (as very similar scores were obtained with mT5$_\textit{base}$).
Regarding the model and training details, readers are referred to the original work \citep{lin-etal-2021-leveraging}.\footnote{The experiments were run on a single RTX 24 GiB GPU, a batch size of 4 and a learning rate of $1e-4$; 5 epochs.}

Fully supervised DST scores provide a strong benchmark with the multilingual T5DST model over all languages in \dataset. We observe the highest performance in English ($59.9\%$ JGA), followed by Turkish, French, and Arabic, indicating the levels of difficulty of DST for each language. Table~\ref{tab:nlu-results} presents the zero-shot cross-lingual transfer-from-English results, revealing poor transferability of the DST models across languages (all below $4\%$ JGA). This indicates the limitations of current multilingual models in zero-shot setups and the challenge of transfer learning for culturally adapted dialogs in \dataset.






\rparagraph{Natural Language Generation}
We approach the NLG task as a sequence-to-sequence problem, again supported by mT5$_\textit{small}$. Specifically, at each dialog turn $t$, the model takes the input of its dialog context, and generates a system response $\mathbf{u}_{t}$. Traditionally, NLG in \tod systems is defined as the task of converting a dialog act into a natural language utterance~\cite{williams2007partially}. In our study, we evaluate NLG performance in both a traditional setup, where the goal is to realize the {surface form} of the dialog act, and an end-to-end LM setup, where we model response generation as a transduction problem from the dialog history to a natural response. Third, we consider the setup where both the dialog history and the `oracle' dialog act are available, serving as a performance upper bound. %
For the \textit{surface realization} setup, we convert the dialog act $\mathbf{a}_{t}$ into a flattened string format (e.g., \textit{[inform][restaurant]([price range][expensive],[area][center]}) to serve as the input. For the \textit{language modeling} setup, the model generates a response $\mathbf{u}_{t}$ solely based on the preceding dialog history $\mathbf{u}_{t-2}$ and $\mathbf{u}_{t-1}$. In this setup, the generation model does not have any knowledge about the system's ontology and database. In the \textit{language modeling with oracle} setup, the model takes the concatenation of the two preceding utterances $\mathbf{u}_{t-2}$ and $\mathbf{u}_{t-1}$, as well as $\mathbf{a}_{t}$ as input.

Following MultiWOZ, we evaluate with the corpus BLEU score~\cite{papineni2002bleu}; we evaluate lexicalized utterances without performing delexicalization.
We also report ROUGE-L~\cite{lin-2004-rouge} and METEOR~\cite{banerjee2005meteor}.%
\footnote{All NLG experiments were run on a single A100 80 GiB GPU; batch size of 32, a learning rate of $1e-3$; 10 epochs.}




The results are summarized in Table~\ref{tab:nlg_results}. We observe that the performance of English is significantly higher than other languages in the first setup. This disparity can be attributed to the fact that dialog acts are considered a formal language for the system to process internally and, except for culturally adapted values, they are provided in English. Therefore, it is more challenging for a model to learn how to generate natural language utterances in other languages. Furthermore, by incorporating the dialog history and the oracle dialog act, the performance of all three languages improved significantly, indicating that modeling the dialog history contributes to more coherent responses. Lastly, in the absence of database information, the performance for all languages is considerably low. This highlights the challenge of modeling \tod, and underlines the necessity of incorporating databases into the \tod models in future work.


\rparagraph{End-to-End Modeling}
Finally, E2E modeling performance serves as an even more comprehensive, challenging and arguably more important indicator for assessing the progress of \tod research, garnering intensified research attention~\cite[\textit{inter alia}]{10.5555/3495724.3497418, lin-etal-2020-mintl, peng-etal-2021-soloist, su-etal-2022-multi, wu2023using}. Developing an E2E system offers several advantages 
over focusing on individual sub-components like NLU modules or dialog state trackers. The E2E approach achieves increased applicability, enabling the development of practical real-world applications.
Moreover, it reduces vulnerability to error propagation across sub-components and offers a simpler system design compared to the traditional pipelined approaches.





To the best of our knowledge, no previous publicly available implementation of a multilingual E2E \tod system exists that would be compatible with the MultiWOZ dataset and its derivatives. Other available multilingual \tod benchmarks either lack E2E results~\cite{hung-etal-2022-multi2woz, ding-etal-2022-globalwoz}, or do not release their implementation~\cite{Zuo:2021allwoz}. The only exception is BiToD~\cite{Lin:2021bitod}; however, the BiToD dataset and the associated system use a different annotation schema, which is incompatible with MultiWOZ. Therefore, we present the first publicly available implementation of a multilingual E2E system compatible with the MultiWOZ-related datasets. We release this implementation as a baseline for further research and experimentation on \dataset.


Our system is composed of three key components: a Dialog State Tracking (DST) model, a Database (DB) Interface component, and a Response Generation (RG) model. First, the DST model is a sequence-to-sequence model, which takes the concatenated lexicalized form of all the historical utterances as input and generates a linearized dialog state (e.g., \textit{hotel price range = cheap ; type = hotel}).
Then, the DB Interface transforms the predicted dialog state into an SQL query. This query is executed, resulting in a list of entities that satisfy the specified constraints, which are then returned to the system. Finally, the RG model, also implemented as a seq2seq model, takes as input the concatenation of historical utterances, predicted dialog state, and a database summary that indicates the number of entities returned for each active domain (e.g., \textit{restaurant more than five}). It generates a delexicalized response, which can be further lexicalized using the values in the predicted dialog state and the returned entities from the database.

In our implementation, we utilize two separate mT5$_\textit{large}$ models as the backbone for the DST model and the RG model.  As discussed later, we opt for the large model because it demonstrates a substantial performance advantage over its smaller counterpart.
The data preprocessing, including the linearization of dialog state annotations for training, and the evaluation protocol are based on the established implementation of the SOLOIST system~\cite{peng-etal-2021-soloist}. To ensure up-to-date functionality, our implementation is based on the most recent version 4.30 of the Huggingface transformers repository. Our system is designed to prioritize simplicity and efficiency, with the primary goal of minimizing the complexity and effort required for training, evaluation, and future development. We report the standard evaluation metrics for the E2E task, including the Inform Rate, Success Rate, and the delexicalized corpus BLEU score.\footnote{All E2E experiments were run on a single A100 80 GiB GPU; batch size of 4 and a learning rate of $5e-5$; 5 epochs.}


\begin{table}[]
\centering
\def\arraystretch{0.85}
{\footnotesize
\begin{tabularx}{\linewidth}{X lccc@{}}
\toprule
\multirow{2}{*}{\bf Language} &  & \multicolumn{3}{c}{\bf End-to-End Modeling} \\ \cmidrule(l){2-5} 
                          &  & Inform       & Success      & BLEU      \\ \midrule
ENG                       &  & 67.9         & 39.0         & 15.7      \\
ARA                       &  & 66.8         & 36.7         & 14.0      \\
FRA                       &  & 47.9         & 22.2         & 12.0      \\
TUR                       &  & 45.9         & 21.2         & 16.7      \\
AVG.                      &  & 57.1         & 29.8         & 14.6      \\ \bottomrule
\end{tabularx}%
}%
\caption{Fully supervised E2E performance for mT5$_\textit{large}$. AVG. shows the mean average of the evaluation scores across all four languages. The reported scores are averaged over 3 random runs.}
\label{tab:table_e2e}
\vspace{-1mm}
\end{table}

Table \ref{tab:table_e2e} presents the results of the fully supervised E2E experiments. As anticipated, we observe noticeable performance disparities across languages, particularly in comparison to English. Furthermore, we find that the size of the pretrained language model significantly impacts system performance. Specifically, the mT5$_\textit{large}$ model exhibits a substantial (mean average) performance improvement of 16.4 Inform Rate, 17.2 Success Rate, and 4.6 BLEU points, compared to mT5$_\textit{small}$.
