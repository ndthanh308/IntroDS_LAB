% !TEX root = paper.tex

\section{Conclusion}
\label{sec:conclusion}

We have shown how to incorporate asynchrony within 
algebraic effects, by decoupling 
the execution of operation calls into signalling that an operation's implementation 
needs to be executed, and interrupting a running computation with the operation's result, 
to which it can react by installing interrupt handlers.
We have shown that our approach is flexible enough that not all signals have to have a matching 
interrupt, and vice versa, allowing us to also model spontaneous behaviour, such as a user 
clicking a button or the environment pre-empting a thread. We have formalised these ideas in a small 
calculus, called \lambdaAEff, and demonstrated its flexibility on a number of examples.
We have also accompanied the paper with an \pl{Agda} formalisation of \lambdaAEff's type 
safety and a prototype implementation of \lambdaAEff.

Compared to our previous work~\cite{Ahman:POPL}, in this extended version we 
have simplified the meta-theory of \lambdaAEff, removed the reliance on general recursion 
for reinstalling interrupt handlers, added a notion of state to reinstallable interrupt handlers, 
and extended \lambdaAEff~with higher-order signal and interrupt 
payloads, and with dynamic process creation.
%
However, various future work directions still remain. We discuss these and related work below.

\paragraph{Asynchronous Effects}
As asynchrony is desired in practice, it is no surprise that \pl{Koka} \cite{Leijen:AsyncAwait} 
and \pl{OCaml} \cite{Dolan:MulticoreOCaml,DBLP:conf/pldi/Sivaramakrishnan21}, the two 
largest implementations of algebraic effects and effect handlers, have been extended accordingly. 
In \pl{Koka}, algebraic operations  
reify their continuation into an explicit callback structure that is then dispatched to a primitive 
such as \lstinline{setTimeout} in its \pl{Node.JS} backend. In \pl{OCaml}, one writes effectful 
operations in a direct style, but then uses handlers to access the actual asynchronous I/O through 
calls to an external library such as \pl{libuv}. Both approaches thus \emph{delegate} the actual 
asynchrony to existing concepts in their backends. In contrast, using \lambdaAEff, we 
can express such backend features solely within the core calculus and the prototype implementation of it.

Further, in \lambdaAEff, we avoid having to manually use (un)masking to 
disable asynchronous effects in unwanted places, which can be a very 
tricky business to get right~\cite{Dolan:MulticoreOCaml}.
Instead, by design, interrupts in \lambdaAEff~\emph{never} 
influence running code unless the code has an explicit interrupt handler installed, 
and they \emph{always} wait for any potential handler to present itself during
execution (recall that they get discarded only when reaching a $\tmkw{return}$).

Finally, it is also worth discussing how signals and interrupts in
\lambdaAEff~compare to asynchronous exceptions, e.g., as found in
Haskell~\cite{Marlow:AsyncExceptions}. The two mechanisms are similar in that
both are issued outside of the running process. While asynchronous exceptions
are thrown to a specific thread, we can simulate this in our broadcast-based
semantics by carrying extra identifying information in signal and interrupt
payloads, as discussed in \autoref{sec:applications:guarded-handlers}. There is
however a crucial difference between the two approaches: while interrupts only
affect a given computation when a matching interrupt handler is installed, and
they get always discarded when they reach the program's $\tmkw{return}$ clause,
then asynchronous exceptions behave in the exact opposite way, causing the
program to stop with a thrown exception unless the asynchronous exception is
caught and handled away by the programmer.

\paragraph{Message-Passing}
While in this paper we have focussed on the foundations of asynchrony in the 
context of programming with algebraic effects, the ideas we propose have also many common 
traits with concurrency models based on \emph{message-passing}, 
such as the Actor model \cite{Hewitt:Actors}, the $\pi$-calculus \cite{Milner:PiCalculus}, 
and the join-calculus \cite{FournetGonthier:JoinCalculus}, just to name a few.
%
Namely, one can view the issuing of a signal $\tmopout{op}{V}{M}$ as sending a message, 
and handling an interrupt $\tmopin{op}{W}{M}$ as receiving a message, both along a channel
named $\op$. 
%
In fact, we believe that in our prototype implementation we could replace the semantics 
presented in the paper with an equivalent one based on shared channels
(one for each $\op$), to which the installed interrupt handlers could subscribe to.
Instead of propagating signals first out of and then back into processes, they would then 
be sent directly to channels where interrupt handlers immediately receive them, 
drastically reducing the cost of communication.

Comparing \lambdaAEff~to the Actor model, we see that 
the $\tmrun M$ processes evolve in their own bubbles, and only communicate with other
processes via signals and interrupts, similarly to actors.
However, in contrast to messages not being required to be ordered 
in the Actor model, in our parallel composition operation $\tmpar P Q$, the process $Q$ receives 
interrupts in the same order as the respective signals are issued by $P$ 
(and vice versa). This communication ordering could be relaxed by allowing 
signals to be hoisted out of computations from deeper than just the top level, or by
extending the operational semantics of \lambdaAEff~with commutativity rules for signals.
Another difference with actors is that by default \lambdaAEff-computations react to interrupts  
sequentially---this difference can be remedied by writing programs in a style in which 
interrupts are handled in parallel in dynamically spawned dedicated processes.

It is worth noting that our interrupt handlers are similar to the message receiving construct 
in the $\pi$-calculus, in that they both synchronise with matching incoming
interrupts or messages. However, the two constructs are also different, in that interrupt handlers allow
reductions to take place under them and non-matching interrupts to propagate past them.
%
Further, our interrupt handlers are also similar to join definitions in the join-calculus, describing
how to react when a corresponding interrupt arrives or join pattern appears, where in both cases
the reaction could involve effectful code. To this end, our interrupt handlers resemble join definitions 
with simple one-channel join patterns. However, where the two constructs differ is that join definitions 
additionally serve to define new (local) channels, similarly to the restriction operator in the $\pi$-calculus, 
whereas we assume a fixed global set of channels (i.e., signal and interrupt names $\op$). 
%
We expect that extending \lambdaAEff~with local algebraic effects 
\cite{Staton:Instances,Biernacki:AbstractingAlgEffects}
could help us fill this gap between the formalisms.

\paragraph{Scoped Operations}
As noted in \autoref{sec:basic-calculus:semantics:computations}, despite their name, interrupt handlers
behave like algebraic operations, not like effect handlers. However, one should also note 
that they are not conventional operations as they carry computational data that sequential 
composition does not interact with, and that executes only when a corresponding interrupt is received. 

Such generalised operations are known in the literature as \emph{scoped operations}~\cite{Pirog:ScopedOperations},  
a leading example of which is $\tmspawn{M}{N}$.
Further recalling \autoref{sec:basic-calculus:semantics:computations}, despite their appearance, 
incoming interrupts behave computationally like effect handling, not like algebraic operations. 
In fact, it turns out they correspond to effect handling 
induced by an instance of \emph{scoped effect handlers} \cite{Pirog:ScopedOperations}.
Compared to ordinary effect handlers, scoped effect handlers explain both 
how to interpret operations and their scopes. In our setting, this 
corresponds to triggering interrupt handlers and executing the corresponding handler code.

It would be interesting to extend \lambdaAEff~both with 
scoped operations having more general signatures, and with effect handlers 
for them, e.g., to allow
preventing the propagation of incoming interrupts into continuations, discarding the continuation 
of a cancelled remote call, and techniques such as masking or reordering interrupts
according to priority levels.

\paragraph{Denotational Semantics}
In this paper we study only the operational side of \lambdaAEff, 
and leave developing its denotational semantics for the future.
%
In light of how we have motivated the \lambdaAEff-specific programming 
constructs, and based on the above discussion, we expect the denotational semantics 
to take the form of an algebraically natural \emph{monadic semantics}, where the monad would 
be given by an instance of the one studied in the case of scoped operations~\cite{Pirog:ScopedOperations}
(quotiented by the commutativity of signals and interrupt handlers, 
and extended with nondeterminism to model different evaluation
outcomes). Incoming interrupts would be modelled as homomorphisms
induced by scoped algebras, while for parallel composition, we could
consider all nondeterministic interleavings of (the outgoing signals of) individual computations,
similarly to how it can be done in the context of general effect handlers~\cite{Plotkin:BinaryHandlers, Lindley:DoBeDoBeDo}.
%
Finally, we expect to be able to take inspiration for the denotational semantics of the 
promise type from that of modal logics and modal types.


\paragraph{Reasoning About Asynchronous Effects}
In addition to using \lambdaAEff's type-and-effect system only for specification purposes (such as specifying 
that $M : \tycomp{X}{(\emptyset,\{\})}$ raises no signals and installs no interrupt handlers), 
we wish to make further use of it for validating \emph{effect-dependent optimisations} \cite{Kammar:Optimisations}. 
For instance, whenever $M : \tycomp{X}{(\o,\i)}$ and $\i\, (\op) = \bot$, we would like to know  
that $\tmopin{\op}{V}{M} \reduces^* M$. One way to validate such optimisations 
is to develop an adequate denotational semantics, 
and then use a semantic \emph{computational induction} principle \cite{Bauer:EffectSystem,Plotkin:Logic}.
For \lambdaAEff, this would amount to only having to prove the optimisations for return values, signals, 
and interrupt handlers. Another way to validate effect-dependent optimisations would 
be to define a suitable logical 
relation for \lambdaAEff~\cite{Benton:AbstractEffects}.

In addition to optimisations based on \lambdaAEff's existing effect system, 
we plan to refine the current ``broadcast everything everywhere'' communication strategy
by e.g. extending process types with \emph{communication protocols} inspired by session types \cite{Honda:LangPrimitives}, or
adding \emph{restriction operations} like ones in CCS~\cite{DBLP:books/sp/Milner80} and suitably reflecting their use in the effect annotations.

\paragraph{Strong Normalisation for Computations}
In addition to getting an overall more principled core calculus, one of the motivations for introducing 
reinstallable interrupt handlers to \lambdaAEff~and removing general recursion~(compared 
to our previous work~\cite{Ahman:POPL}) was that the sequential part of the resulting calculus 
ought to be strongly normalising, i.e., there should be no infinite reduction sequences for 
computations. Intuitively, strong normalisation should follow from interrupt handlers getting 
reinstalled only when a corresponding interrupt is propagated to the computation, and no 
single interrupt can reinstall a particular interrupt handler more than once. We leave making 
this argument formal for future work. We expect to be able to build on $\top\top$-lifting style
logical relation proofs of strong normalisation~\cite{Lindley:TopTopLifting}.

However, even after making the above-mentioned changes to \lambdaAEff, its parallel 
part of course remains non-terminating---simply consider two parallel processes built from 
reinstallable interrupt handlers that indefinitely exchange ping-pong signals with each other, e.g., as
\begin{lstlisting}
run (send ping (); promise (pong _ r -> send ping (); r ()))
||
run (send pong (); promise (ping _ r -> send pong (); r ()))
\end{lstlisting}


\section*{Acknowledgements}

We thank the anonymous reviewers, Otterlo IFIP WG 2.1 meeting participants, 
and Andrej Bauer, Gavin Bierman, Žiga Lukšič, Janez Radešček, and Alex Simpson for their useful feedback.
