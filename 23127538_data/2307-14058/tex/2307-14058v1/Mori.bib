% This file was created with Citavi 6.7.0.0

@article{Metwaly.2021,
 author = {Metwaly, Kareem M. and Kim, Aerin and Branson, Elliot and Monga, Vishal},
 year = {2021},
 title = {CAR - Cityscapes Attributes Recognition A Multi-category Attributes Dataset for Autonomous Vehicles},
 url = {https://arxiv.org/abs/2111.08243},
 urldate = {02.11.2022},
 volume = {abs/2111.08243},
 journal = {ArXiv}
}


@article{Metzger.2021,
 author = {Metzger, Kai A. and Mortimer, Peter and Wuensche, H.},
 year = {2021},
 title = {A Fine-Grained Dataset and its Efficient Semantic Segmentation for Unstructured Driving Scenarios},
 pages = {7892--7899},
 journal = {2020 25th International Conference on Pattern Recognition (ICPR)},
 file = {https://arxiv.org/pdf/2103.13109.pdf}
}


@inproceedings{Fregin.2018,
 author = {Fregin, Andreas and Muller, Julian and Krebel, Ulrich and Dietmayer, Klaus},
 title = {The DriveU Traffic Light Dataset: Introduction and Comparison with Existing Datasets},
 pages = {3376--3383},
 booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
 year = {2018},
 doi = {10.1109/ICRA.2018.8460737}
}


@misc{CounciloftheEuropeanUnion.27.11.2019,
 author = {{Council of the European Union} and {European Parliament}},
 title = {Regulation (EU) 2019/2144 of the European Parliament and of the Council of 27 November 2019 on type-approval requirements for motor vehicles and their trailers, and systems, components and separate technical units intended for such vehicles, as regards their general safety and the protection of vehicle occupants and vulnerable road users, amending Regulation (EU) 2018/858 of the European Parliament and of the Council and repealing Regulations (EC) No 78/2009, (EC) No 79/2009 and (EC) No 661/2009 of the European Parliament and of the Council and Commission Regulations (EC) No 631/2009, (EU) No 406/2010, (EU) No 672/2010, (EU) No 1003/2010, (EU) No 1005/2010, (EU) No 1008/2010, (EU) No 1009/2010, (EU) No 19/2011, (EU) No 109/2011, (EU) No 458/2011, (EU) No 65/2012, (EU) No 130/2012, (EU) No 347/2012, (EU) No 351/2012, (EU) No 1230/2012 and (EU) 2015/166 (Text with EEA relevance): Regulation (EU) 2019/2144},
 year = {27.11.2019},
 url = {https://op.europa.eu/en/publication-detail/-/publication/bfd5eba8-2058-11ea-95ab-01aa75ed71a1/language-en},
 urldate = {09.09.2022},
 pages = {1--40},
 number = {L325},
 booktitle = {Official Journal of the European Union}
}


@article{Glatzki.2021,
 author = {Glatzki, Felix and Lippert, Moritz and Winner, Hermann},
 year = {2021},
 title = {Behavioral Attributes for a Behavior-Semantic Scenery Description (BSSD) for the Development of Automated Driving Functions},
 pages = {667--672},
 journal = {2021 IEEE International Intelligent Transportation Systems Conference (ITSC)}
}


@misc{InternationalOrganizationforStandardization.122018,
 year = {12/2018},
 title = {ISO 26262-1:2018: Road vehicles - functional safety - Part 1: Vocabulary},
 url = {https://www.iso.org/standard/68383.html},
 urldate = {09.09.2022},
 number = {ISO 26262-1:2018(E)},
 author = {{International Organization for Standardization}}
}


@article{Chan.2021,
 abstract = {State-of-the-art semantic or instance segmentation deep neural networks (DNNs) are usually trained on a closed set of semantic classes. As such, they are ill-equipped to handle previously-unseen objects. However, detecting and localizing such objects is crucial for safety-critical applications such as perception for automated driving, especially if they appear on the road ahead. While some methods have tackled the tasks of anomalous or out-of-distribution object segmentation, progress remains slow, in large part due to the lack of solid benchmarks; existing datasets either consist of synthetic data, or suffer from label inconsistencies. In this paper, we bridge this gap by introducing the {\textquotedbl}SegmentMeIfYouCan{\textquotedbl} benchmark. Our benchmark addresses two tasks: Anomalous object segmentation, which considers any previously-unseen object category; and road obstacle segmentation, which focuses on any object on the road, may it be known or unknown. We provide two corresponding datasets together with a test suite performing an in-depth method analysis, considering both established pixel-wise performance metrics and recent component-wise ones, which are insensitive to object sizes. We empirically evaluate multiple state-of-the-art baseline methods, including several specifically designed for anomaly / obstacle segmentation, on our datasets as well as on public ones, using our benchmark suite. The anomaly and obstacle segmentation results show that our datasets contribute to the diversity and challengingness of both dataset landscapes.},
 author = {Chan, Robin and Lis, Krzysztof and Uhlemeyer, Svenja and Blum, Hermann and Honari, Sina and Siegwart, Roland and Salzmann, Mathieu and Fua, Pascal and Rottmann, Matthias},
 year = {2021},
 title = {SegmentMeIfYouCan: A Benchmark for Anomaly Segmentation},
 url = {https://arxiv.org/pdf/2104.14812},
 urldate = {02.11.2022},
 volume = {abs/2104.14812},
 journal = {ArXiv},
 file = {Chan, Lis et al. 30.04.2021 - SegmentMeIfYouCan.pdf},
 file = {https://arxiv.org/pdf/2104.14812v1.pdf}
}


@inproceedings{Zendel.2022,
 author = {Zendel, Oliver and Sch{\"o}rghuber, Matthias and Rainer, Bernhard and Murschitz, Markus and Beleznai, Csaba},
 title = {Unifying Panoptic Segmentation for Autonomous Driving},
 pages = {21319--21328},
 booktitle = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 year = {2022},
 doi = {10.1109/CVPR52688.2022.02066}
}


@inproceedings{Karimi.2020,
 abstract = {One of the challenges in designing autonomous vehicles (AV's) is driving around humans (i.e. drivers, cyclists, pedestrians, etc.) In particular, the AV's and the humans must have a common set of traffic rules to follow. In this paper, we present a new approach to formalize and implement traffic rules. We use California's DMV driver handbook as a working example. Our approach provides a straightforward mapping from the rules in the handbook to its formal model, and from the model to its implementation. To demonstrate the efficiency of this approach, we formally model the traffic rules in the logic programming paradigm of Answer Set Programming (ASP) using a programming language called Clingo. We then integrate these rules into CARLA, a virtual test bed environment for autonomous vehicles. We simulate the behavior of autonomous vehicles at four way and three way uncontrolled intersections by correct reasoning of right-of-way rules for autonomous vehicles in real time. As a result, the behaviors of autonomous vehicles under our controller are more realistic compared to CARLA's default FIFO controller. This also improves the throughput of the traffic through the intersection.},
 author = {Karimi, A. and Duggirala, P. S.},
 title = {Formalizing traffic rules for uncontrolled intersections},
 pages = {41--50},
 isbn = {2642-9500},
 booktitle = {2020 ACM/IEEE 11th International Conference on Cyber-Physical Systems (ICCPS)},
 year = {2020},
 doi = {10.1109/ICCPS48487.2020.00012}
}


@misc{EuropeanCommissionDirectorateGeneralforCommunicationsNetworksContentandTechnology.2021,
 author = {{European Commission, Directorate-General for Communications Networks, Content and Technology}},
 year = {2021},
 title = {Proposal for a REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL LAYING DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE (ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION LEGISLATIVE ACTS},
 url = {https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206},
 urldate = {09.09.2022},
 edition = {COM(2021) 206 final},
 number = {2021/0106(COD)}
}


@misc{BundesministeriumfurJustizundVerbraucherschutz.14.03.2013,
 author = {{Bundesministerium f{\"u}r Justiz und Verbraucherschutz}},
 title = {Verordnung {\"u}ber die Erteilung einer Verwarnung, Regels{\"a}tze f{\"u}r Geldbu{\ss}en und die Anordnung eines Fahrverbotes wegen Ordnungswidrigkeiten im Stra{\ss}enverkehr (Bu{\ss}geldkatalog-Verordnung - BKatV): BKatV},
 year = {14.03.2013},
 url = {https://www.gesetze-im-internet.de/bkatv_2013/BKatV.pdf},
 urldate = {12.08.2022},
 pages = {498--546},
 volume = {2013 Teil I},
 number = {Nr. 14},
 booktitle = {Bundesgesetzblatt},
 journal = {Bundesgesetzblatt}
}


@article{BundesministeriumfurJustizundVerbraucherschutz.05.03.2003,
 author = {{Bundesministerium f{\"u}r Justiz und Verbraucherschutz}},
 title = {Stra{\ss}enverkehrsgesetz: StVG},
 year = {05.03.2003},
 url = {https://www.gesetze-im-internet.de/stvg/STVG.pdf},
 urldate = {12.08.2022},
 pages = {310--344},
 volume = {2003 Teil I},
 number = {Nr. 10},
 booktitle = {Bundesgesetzblatt},
 journal = {Bundesgesetzblatt}
}


@article{BundesministeriumfurJustizundVerbraucherschutz.13.11.1998,
 author = {{Bundesministerium f{\"u}r Justiz und Verbraucherschutz}},
 title = {Strafgesetzbuch: StGB},
 year = {13.11.1998},
 url = {\url{https://www.gesetze-im-internet.de/stgb/StGB.pdf}},
 urldate = {120.08.2022},
 pages = {3322--3410},
 volume = {1998 Teil I},
 number = {Nr. 75},
 booktitle = {Bundesgesetzblatt},
 journal = {Bundesgesetzblatt}
}



@inproceedings{Buechel.2017,
 abstract = {This paper presents a modular framework for traffic regulations based decision-making of automated vehicles. It builds on a semantic traffic scene representation formulated as ontology and includes knowledge about traffic regulations. The semantic representation supports traffic situation classification by reasoning, providing improved situational awareness for the automated vehicle. Decision-making rules are directly derived from traffic regulations and concepts used in the ontology are harmonized with concepts used in traffic regulations. Due to the modular structure of the developed ontology, switching between different sets of national traffic regulations becomes a simple process. The methodology is evaluated for a variety of traffic scenarios, building up from basic to complex urban scenarios containing intersections, traffic regulating police officers and crossing street railways.},
 author = {Buechel, M. and Hinz, G. and Ruehl, F. and Schroth, H. and Gyoeri, C. and Knoll, A.},
 title = {Ontology-based traffic scene modeling, traffic regulations dependent situational awareness and decision-making for automated vehicles},
 pages = {1471--1476},
 booktitle = {2017 IEEE Intelligent Vehicles Symposium (IV)},
 year = {2017},
 doi = {10.1109/IVS.2017.7995917}
}


@misc{Bottcher.2018,
 author = {B{\"o}ttcher, Lorenz},
 editor = {Dannemann, Gerhard},
 year = {2018},
 title = {Road Traffic Regulations (Stra{\ss}enverkehrs-Ordnung, StVO) with Annexes},
 url = {https://germanlawarchive.iuscomp.org/?p=1290},
 note = {https://germanlawarchive.iuscomp.org/?p=1290}
}


@inproceedings{Blum.2019,
 author = {Blum, Hermann and Sarlin, Paul-Edouard and Nieto, Juan and Siegwart, Roland and Cadena, Cesar},
 title = {Fishyscapes: A Benchmark for Safe Semantic Segmentation in Autonomous Driving},
 booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops},
 year = {2019}
}


@article{Bevandic.2022,
 author = {Bevandic, Petra and Orsic, Marin and Grubisic, Ivan and Saric, Josip and Segvic, Sinisa},
 year = {2022},
 title = {Multi-domain semantic segmentation with overlapping labels},
 pages = {2422--2431},
 journal = {2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}
}


@inproceedings{Pinggera.2016,
 author = {Pinggera, Peter and Ramos, Sebastian and Gehrig, Stefan and Franke, Uwe and Rother, Carsten and Mester, Rudolf},
 title = {Lost and Found: detecting small road hazards for self-driving vehicles},
 pages = {1099--1106},
 booktitle = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 year = {2016},
 doi = {10.1109/IROS.2016.7759186}
}


@inproceedings{Nolte.2017,
 author = {Nolte, Marcus and Bagschik, Gerrit and Jatzkowski, Inga and Stolte, Torben and Reschka, Andreas and Maurer, Markus},
 title = {Towards a skill- and ability-based development process for self-aware automated road vehicles},
 pages = {1--6},
 publisher = {IEEE},
 isbn = {978-1-5386-1526-3},
 booktitle = {IEEE ITSC 2017},
 year = {2017},
 address = {Piscataway, NJ},
 doi = {10.1109/ITSC.2017.8317814},
 file = {dcd56684-04d6-4116-97ea-a97f1d3936d8:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Citavi Attachments\\dcd56684-04d6-4116-97ea-a97f1d3936d8.pdf:pdf}
}


@article{Xiao.2021,
 author = {Xiao, Pengchuan and Shao, Zhenlei and Hao, Steven and Zhang, Zishuo and Chai, Xiaolin and Jiao, Judy and Li, Zesong and Wu, Jian and Sun, Kai and Jiang, Kun and Wang, Yunlong and Yang, Diange},
 year = {2021},
 title = {PandaSet: Advanced Sensor Suite Dataset for Autonomous Driving},
 pages = {3095--3101},
 journal = {2021 IEEE International Intelligent Transportation Systems Conference (ITSC)}
}


@misc{EuropeanCommission.2022,
 author = {{European Commission}},
 year = {2022},
 title = {ANNEXES to the Commission Implementing Regulation laying down rules for the application of Regulation (EU) 2019/2144 of the European Parliament and of the Council as regards uniform procedures and technical specifications for the type-approval of the automated driving system (ADS) of fully automated vehicles: Draft act: Annex 2},
 url = {https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/12152-Automated-cars-technical-specifications_en},
 urldate = {09.09.2022}
}


@article{Ahmed.2019,
 author = {Ahmed, Sarfraz and Huda, M. Nazmul and Rajbhandari, Sujan and Saha, Chitta and Elshaw, Mark and Kanarachos, Stratis},
 year = {2019},
 title = {Pedestrian and Cyclist Detection and Intent Estimation for Autonomous Vehicles: A Survey},
 pages = {38},
 volume = {9},
 number = {11},
 journal = {Applied Sciences},
 doi = {10.3390/app9112335},
 file = {2677896c-95ae-4357-89e2-6e6e99d58a05:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Citavi Attachments\\2677896c-95ae-4357-89e2-6e6e99d58a05.pdf:pdf}
}


@article{Ashmore.10.05.2019,
 abstract = {Machine learning has evolved into an enabling technology for a wide range of highly successful applications. The potential for this success to continue and accelerate has placed machine learning (ML) at the top of research, economic and political agendas. Such unprecedented interest is fuelled by a vision of ML applicability extending to healthcare, transportation, defence and other domains of great societal importance. Achieving this vision requires the use of ML in safety-critical applications that demand levels of assurance beyond those needed for current ML applications. Our paper provides a comprehensive survey of the state-of-the-art in the assurance of ML, i.e. in the generation of evidence that ML is sufficiently safe for its intended use. The survey covers the methods capable of providing such evidence at different stages of the machine learning lifecycle, i.e. of the complex, iterative process that starts with the collection of the data used to train an ML component for a system, and ends with the deployment of that component within the system. The paper begins with a systematic presentation of the ML lifecycle and its stages. We then define assurance desiderata for each stage, review existing methods that contribute to achieving these desiderata, and identify open challenges that require further research.},
 author = {Ashmore, Rob and Calinescu, Radu and Paterson, Colin},
 year = {2022},
 title = {Assuring the Machine Learning Lifecycle: Desiderata, Methods, and Challenges},
 url = {https://dl.acm.org/doi/fullHtml/10.1145/3453444},
 keywords = {robustness},
 urldate = {20.01.2023},
 pages = {1--39},
 volume = {54},
 number = {5},
 issn = {0360-0300},
 journal = {ACM Computing Surveys},
 doi = {10.1145/3453444},
 file = {Ashmore, Calinescu et al. - Assuring the Machine Learning Lifecycle.pdf},
 file = {https://arxiv.org/pdf/1905.04223v1.pdf}
}


@inproceedings{Bagschik.29.03.2017,
 abstract = {The introduction of automated vehicles without permanent human supervision demands a functional system description, including functional system boundaries and a comprehensive safety analysis. These inputs to the technical development can be identified and analyzed by a scenario-based approach. Furthermore, to establish an economical test and release process, a large number of scenarios must be identified to obtain meaningful test results. Experts are doing well to identify scenarios that are difficult to handle or unlikely to happen. However, experts are unlikely to identify all scenarios possible based on the knowledge they have on hand. Expert knowledge modeled for computer aided processing may help for the purpose of providing a wide range of scenarios. This contribution reviews ontologies as knowledge-based systems in the field of automated vehicles, and proposes a generation of traffic scenes in natural language as a basis for a scenario creation.},
 author = {Bagschik, Gerrit and Menzel, Till and Maurer, Markus},
 title = {Ontology based Scene Creation for the Development of Automated Vehicles},
 url = {https://arxiv.org/pdf/1704.01006},
 keywords = {definition;ontology;scenario;testing;validation;verification},
 booktitle = {2018 IEEE Intelligent Vehicles Symposium (IV)},
 year = {2018},
 file = {1704.01006v5.pdf},
 file = {e9facd72-ebe5-4d3c-b747-d9dc649911c1:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Citavi Attachments\\e9facd72-ebe5-4d3c-b747-d9dc649911c1.pdf:pdf}
}


@misc{BundesministeriumfurJustizundVerbraucherschutz.06.03.2013,
 author = {{Bundesministerium f{\"u}r Justiz und Verbraucherschutz}},
 title = {Stra{\ss}enverkehrs-Ordnung: StVO},
 year = {06.03.2013},
 url = {https://www.gesetze-im-internet.de/stvo_2013/StVO.pdf},
 urldate = {25.06.2021},
 pages = {1--80},
 volume = {2021 Teil I},
 number = {Nr. 48},
 booktitle = {Bundesgesetzblatt}
}


@incollection{Caesar.12.12.2016,
 abstract = {Semantic classes can be either things (objects with a well-defined shape, e.g. car, person) or stuff (amorphous background regions, e.g. grass, sky). While lots of classification and detection works focus on thing classes, less attention has been given to stuff classes. Nonetheless, stuff classes are important as they allow to explain important aspects of an image, including (1) scene type; (2) which thing classes are likely to be present and their location (through contextual reasoning); (3) physical attributes, material types and geometric properties of the scene. To understand stuff and things in context we introduce COCO-Stuff, which augments all 164K images of the COCO 2017 dataset with pixel-wise annotations for 91 stuff classes. We introduce an efficient stuff annotation protocol based on superpixels, which leverages the original thing annotations. We quantify the speed versus quality trade-off of our protocol and explore the relation between annotation time and boundary complexity. Furthermore, we use COCO-Stuff to analyze: (a) the importance of stuff and thing classes in terms of their surface cover and how frequently they are mentioned in image captions; (b) the spatial relations between stuff and things, highlighting the rich contextual relations that make our dataset unique; (c) the performance of a modern semantic segmentation method on stuff and thing classes, and whether stuff is easier to segment than things.},
 author = {Caesar, Holger and Uijlings, Jasper and Ferrari, Vittorio},
 title = {COCO-Stuff: Thing and Stuff Classes in Context},
 url = {https://arxiv.org/pdf/1612.03716},
 booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
 file = {https://arxiv.org/pdf/1612.03716v4.pdf}
}


@inproceedings{Caesar.26.03.2019b,
 abstract = {Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online.},
 author = {Caesar, Holger and Bankiti, Varun and Lang, Alex H. and Vora, Sourabh and Liong, Venice Erin and Xu, Qiang and Krishnan, Anush and Pan, Yu and Baldan, Giancarlo and Beijbom, Oscar},
 title = {nuScenes: A multimodal dataset for autonomous driving},
 url = {https://arxiv.org/pdf/1903.11027},
 keywords = {dataset;evaluation;metric;object detection;real world;tracking},
 booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 year = {2020},
 file = {18e5e15c-3537-4924-906d-9b09a56a80fb:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Citavi Attachments\\18e5e15c-3537-4924-906d-9b09a56a80fb.pdf:pdf;b84136f5-6a62-487d-ad98-06aebca03690:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Remote Attachments\\b84136f5-6a62-487d-ad98-06aebca03690.pdf:pdf}
}


@inproceedings{Chang.06.11.2019,
 abstract = {We present Argoverse -- two datasets designed to support autonomous vehicle machine learning tasks such as 3D tracking and motion forecasting. Argoverse was collected by a fleet of autonomous vehicles in Pittsburgh and Miami. The Argoverse 3D Tracking dataset includes 360 degree images from 7 cameras with overlapping fields of view, 3D point clouds from long range LiDAR, 6-DOF pose, and 3D track annotations. Notably, it is the only modern AV dataset that provides forward-facing stereo imagery. The Argoverse Motion Forecasting dataset includes more than 300,000 5-second tracked scenarios with a particular vehicle identified for trajectory forecasting. Argoverse is the first autonomous vehicle dataset to include {\textquotedbl}HD maps{\textquotedbl} with 290 km of mapped lanes with geometric and semantic metadata. All data is released under a Creative Commons license at www.argoverse.org. In our baseline experiments, we illustrate how detailed map information such as lane direction, driveable area, and ground height improves the accuracy of 3D object tracking and motion forecasting. Our tracking and forecasting experiments represent only an initial exploration of the use of rich maps in robotic perception. We hope that Argoverse will enable the research community to explore these problems in greater depth.},
 author = {Chang, Ming-Fang and Lambert, John and Sangkloy, Patsorn and Singh, Jagjeet and Bak, Slawomir and Hartnett, Andrew and Wang and Carr, Peter and Lucey, Simon and Ramanan, Deva and Hays, James},
 title = {Argoverse: 3D Tracking and Forecasting with Rich Maps},
 keywords = {dataset;real world},
 pages = {8740--8749},
 booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 year = {2019},
 doi = {10.1109/CVPR.2019.00895},
 file = {0927b880-fd3a-41fa-8d64-e4a9584dd0b3:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Citavi Attachments\\0927b880-fd3a-41fa-8d64-e4a9584dd0b3.pdf:pdf}
}


@incollection{Cordts.06.04.2016,
 abstract = {Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes.  To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations; 20000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.},
 author = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
 title = {The Cityscapes Dataset for Semantic Urban Scene Understanding},
 url = {https://arxiv.org/pdf/1604.01685},
 booktitle = {Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 file = {76184239-db55-44b4-9bc0-244b910d6778:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Citavi Attachments\\76184239-db55-44b4-9bc0-244b910d6778.pdf:pdf;70b8fe9c-4078-48b5-8311-a61933b914a0:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Remote Attachments\\70b8fe9c-4078-48b5-8311-a61933b914a0.pdf:pdf}
}


@misc{DepartmentofTransport.2021,
 editor = {{Department of Transport}},
 year = {2021},
 title = {Reported road causalties Great Britain, annual report: 2020: National statistics},
 url = {https://www.gov.uk/government/statistics/reported-road-casualties-great-britain-annual-report-2020/reported-road-casualties-great-britain-annual-report-2020#overall-casualties},
 urldate = {05.08.2022}, 
 note = {https://www.gov.uk/government/statistics/reported-road-casualties-great-britain-annual-report-2020/reported-road-casualties-great-britain-annual-report-2020\#overall-casualties},
}

@incollection{Dietmayer.2016,
 abstract = {In the case of highly-automated and fully-automated driving it is necessary for the vehicle itself to recognize the limitations of its machine perception, as well as the functional limitations of processing modules based on this perception to react adequately.},
 author = {Dietmayer, Klaus},
 title = {Predicting of Machine Perception for Automated Driving},
 pages = {407--424},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-662-48847-8},
 editor = {Maurer, Markus and Gerdes, J. Christian and Lenz, Barbara and Winner, Hermann},
 booktitle = {Autonomous Driving: Technical, Legal and Social Aspects},
 year = {2016},
 address = {Berlin, Heidelberg},
 doi = {10.1007/978-3-662-48847-8{\textunderscore }20}
}


@incollection{Ertler.10.09.2019,
 abstract = {Traffic signs are essential map features globally in the era of autonomous driving and smart cities. To develop accurate and robust algorithms for traffic sign detection and classification, a large-scale and diverse benchmark dataset is required. In this paper, we introduce a traffic sign benchmark dataset of 100K street-level images around the world that encapsulates diverse scenes, wide coverage of geographical locations, and varying weather and lighting conditions and covers more than 300 manually annotated traffic sign classes. The dataset includes 52K images that are fully annotated and 48K images that are partially annotated. This is the largest and the most diverse traffic sign dataset consisting of images from all over world with fine-grained annotations of traffic sign classes. We have run extensive experiments to establish strong baselines for both the detection and the classification tasks. In addition, we have verified that the diversity of this dataset enables effective transfer learning for existing large-scale benchmark datasets on traffic sign detection and classification. The dataset is freely available for academic research: https://www.mapillary.com/dataset/trafficsign.},
 author = {Ertler, Christian and Mislej, Jerneja and Ollmann, Tobias and Porzi, Lorenzo and Neuhold, Gerhard and Kuang, Yubin},
 title = {The Mapillary Traffic Sign Dataset for Detection and Classification on a  Global Scale},
 url = {https://arxiv.org/pdf/1909.04422},
 booktitle = {Computer Vision - ECCV 2020},
 file = {Ertler, Mislej et al. 10.09.2019 - The Mapillary Traffic Sign Dataset.pdf},
 file = {https://arxiv.org/pdf/1909.04422v2.pdf}
}


@article{Everingham.2010,
 author = {Everingham, M. and {van Gool}, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.},
 year = {2010},
 title = {The Pascal Visual Object Classes (VOC) Challenge},
 pages = {303--338},
 volume = {88},
 number = {2},
 issn = {0920-5691},
 journal = {International Journal of Computer Vision},
 file = {https://homepages.inf.ed.ac.uk/ckiw/postscript/ijcv_voc09.pdf},
 file = {https://link.springer.com/content/pdf/10.1007/s11263-009-0275-4.pdf}
}


@article{Feng.2020,
 abstract = {Recent advancements in perception for autonomous driving are driven by deep learning. In order to achieve robust and accurate scene understanding, autonomous vehicles are usually equipped with different sensors (e.g. cameras, LiDARs, Radars), and multiple sensing modalities can be fused to exploit their complementary properties. In this context, many methods have been proposed for deep multi-modal perception problems. However, there is no general guideline for network architecture design, and questions of ``what to fuse'', ``when to fuse'', and ``how to fuse'' remain open. This review paper attempts to systematically summarize methodologies and discuss challenges for deep multi-modal object detection and semantic segmentation in autonomous driving. To this end, we first provide an overview of on-board sensors on test vehicles, open datasets, and background information for object detection and semantic segmentation in autonomous driving research. We then summarize the fusion methodologies and discuss challenges and open questions. In the appendix, we provide tables that summarize topics and methods. We also provide an interactive online platform to navigate each reference: https://boschresearch.github.io/multimodalperception/.},
 author = {Feng, D. and Haase-Sch{\"u}tz, C. and Rosenbaum, L. and Hertlein, H. and Gl{\"a}ser, C. and Timm, F. and Wiesbeck, W. and Dietmayer, K.},
 year = {2020},
 title = {Deep Multi-Modal Object Detection and Semantic Segmentation for Autonomous Driving: Datasets, Methods, and Challenges},
 keywords = {multimodal;object detection;semantic segmentation;sensor fusion},
 pages = {1--20},
 issn = {1558-0016},
 journal = {IEEE Transactions on Intelligent Transportation Systems},
 doi = {10.1109/TITS.2020.2972974},
 file = {dc50e8e7-09f4-414c-a1d4-42f74599e7f8:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Citavi Attachments\\dc50e8e7-09f4-414c-a1d4-42f74599e7f8.pdf:pdf}
}


@phdthesis{G.Nilsson.2004,
 abstract = {The power model, which estimates the relationship between speed and safety, is not a new tool as the model has been used both in theory and pracise in several countries for many years and is tested and validated im a cross-sectional study and shows that the power model is valid with regard to injury accidents, fatal accidents and the number of injured but not for theNumber of fatalities. Traffic safety work needs different methods and tools in order to choose and evsaluate traffic safety measures. The thesis contributes to this problem by presenting and visualizing a method which describes the traffic safety situation in several dimensions. The method used to describe the traffic safety problem shows the potential of a simultanous presentation and evaluation of these dimensions and demonstrates that the method can be expanded to several dimensions or ratios estimating the exposure, the risk and the consequence. This is illustrated in describing the traffic safety situation for different road user groups and age groups. The power model, which estimates the relationship between speed and safety, is not a new tool as the model has been used both in theory and pracise in several countries for many years. In the thesis the theoretical and practical background are presented. The power model is here also tested and validated im a cross-sectional study. These analyses show that the power model is valid with regard to injury accidents, fatal accidents and the number of injured but not for the number of fatalities. The power model underestimates the effect on fatalities. (Less)},
 author = {{G. Nilsson}},
 year = {2004},
 title = {Traffic Safety Dimensions and the Power Model to Describe the Effect of Speed on Safety},
 url = {https://www.semanticscholar.org/paper/Traffic-Safety-Dimensions-and-the-Power-Model-to-of-Nilsson/5d9b35acb6fac92a7fa995c1849fd304ef0a4cca},
 publisher = {Traffic Engineering},
 school = {{Lund Institute of Technology}},
 type = {Doctoral Thesis}
}


@inproceedings{Geiger.2012,
 author = {Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
 title = {Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite},
 keywords = {evaluation;metric;object detection},
 pages = {3354--3361},
 booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
 year = {2012}
}


@article{Geyer.14.04.2020,
 abstract = {Research in machine learning, mobile robotics, and autonomous driving is accelerated by the availability of high quality annotated data. To this end, we release the Audi Autonomous Driving Dataset (A2D2). Our dataset consists of simultaneously recorded images and 3D point clouds, together with 3D bounding boxes, semantic segmentation, instance segmentation, and data extracted from the automotive bus. Our sensor suite consists of six cameras and five LiDAR units, providing full 360 degree coverage. The recorded data is time synchronized and mutually registered. Annotations are for non-sequential frames: 41,277 frames with semantic segmentation image and point cloud labels, of which 12,497 frames also have 3D bounding box annotations for objects within the field of view of the front camera. In addition, we provide 392,556 sequential frames of unannotated sensor data for recordings in three cities in the south of Germany. These sequences contain several loops. Faces and vehicle number plates are blurred due to GDPR legislation and to preserve anonymity. A2D2 is made available under the CC BY-ND 4.0 license, permitting commercial use subject to the terms of the license. Data and further information are available at http://www.a2d2.audi.},
 author = {Geyer, Jakob and Kassahun, Yohannes and Mahmudi, Mentar and Ricou, Xavier and Durgesh, Rupesh and Chung, Andrew S. and Hauswald, Lorenz and Pham, Viet Hoang and M{\"u}hlegg, Maximilian and Dorn, Sebastian and Fernandez, Tiffany and J{\"a}nicke, Martin and Mirashi, Sudesh and Savani, Chiragkumar and Sturm, Martin and Vorobiov, Oleksandr and Oelker, Martin and Garreis, Sebastian and Schuberth, Peter},
 year = {2020},
 title = {A2D2: Audi Autonomous Driving Dataset},
 url = {https://arxiv.org/pdf/2004.06320},
 urldate = {02.11.2022},
 volume = {abs/2004.06320},
 journal = {ArXiv},
 file = {b978d3b9-10a2-4824-a6b6-21dfa93650c3:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Citavi Attachments\\b978d3b9-10a2-4824-a6b6-21dfa93650c3.pdf:pdf}
}


@incollection{Guo.30.05.2019,
 abstract = {Generic object detection is one of the most fundamental problems in computer vision, yet it is difficult to provide all the bounding-box-level annotations aiming at large-scale object detection for thousands of categories. In this paper, we present a novel cross-supervised learning pipeline for large-scale object detection, denoted as CS-R-FCN. First, we propose to utilize the data flow of image-level annotated images in the fully-supervised two-stage object detection framework, leading to cross-supervised learning combining bounding-box-level annotated data and image-level annotated data. Second, we introduce a semantic aggregation strategy utilizing the relationships among the cross-supervised categories to reduce the unreasonable mutual inhibition effects during the feature learning. Experimental results show that the proposed CS-R-FCN improves the mAP by a large margin compared to previous related works.},
 author = {Guo, Ye and Li, Yali and Wang, Shengjin},
 title = {CS-R-FCN: Cross-supervised Learning for Large-Scale Object Detection},
 url = {https://arxiv.org/pdf/1905.12863},
 keywords = {classification;evaluation},
 booktitle = {ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
 file = {2515e8cf-6cb5-4deb-ab56-7f0fcc041707:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Citavi Attachments\\2515e8cf-6cb5-4deb-ab56-7f0fcc041707.pdf:pdf;85845e2b-bfd3-4c0e-82a4-d0d1d20ef69a:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Remote Attachments\\85845e2b-bfd3-4c0e-82a4-d0d1d20ef69a.pdf:pdf}
}


@article{Han.2012,
 abstract = {OBJECTIVE

This study aimed at investigating the effects of vehicle impact velocity, vehicle front-end shape, and pedestrian size on injury risk to pedestrians in collisions with passenger vehicles with various frontal shapes.

METHOD

A series of parametric studies was carried out using 2 total human model for safety (THUMS) pedestrian models (177 and 165 cm) and 4 vehicle finite element (FE) models with different front-end shapes (medium-size sedan, minicar, one-box vehicle, and sport utility vehicle [SUV]). The effects of the impact velocity on pedestrian injury risk were analyzed at velocities of 20, 30, 40, and 50 km/h. The dynamic response of the pedestrian was investigated, and the injury risk to the head, chest, pelvis, and lower extremities was compared in terms of the injury parameters head injury criteria (HIC), chest deflection, and von Mises stress distribution of the rib cage, pelvis force, and bending moment diagram of the lower extremities.

RESULT

Vehicle impact velocity has the most significant influence on injury severity for adult pedestrians. All injury parameters can be reduced in severity by decreasing vehicle impact velocities. The head and lower extremities are at greater risk of injury in medium-size sedan and SUV collisions. The chest injury risk was particularly high in one-box vehicle impacts. The fracture risk of the pelvis was also high in one-box vehicle and SUV collisions. In minicar collisions, the injury risk was the smallest if the head did not make contact with the A-pillar.

CONCLUSION

The vehicle impact velocity and vehicle front-end shape are 2 dominant factors that influence the pedestrian kinematics and injury severity. A significant reduction of all injuries can be achieved for all vehicle types when the vehicle impact velocity is less than 30 km/h. Vehicle designs consisting of a short front-end and a wide windshield area can protect pedestrians from fatalities. The results also could be valuable in the design of a pedestrian-friendly vehicle front-end shape. [Supplementary materials are available for this article. Go to the publisher's online edition of Traffic Injury Prevention for the following free supplemental resource: Head impact conditions and injury parameters in four-type vehicle collisions and validation result of the finite element model of one-box vehicle and minicar. ].},
 author = {Han, Yong and Yang, Jikuang and Mizuno, Koji and Matsui, Yasuhiro},
 year = {2012},
 title = {Effects of vehicle impact velocity, vehicle front-end shapes on pedestrian injury risk},
 pages = {507--518},
 volume = {13},
 number = {5},
 journal = {Traffic injury prevention},
 doi = {10.1080/15389588.2012.661111},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/22931181}
}


@article{Hendy.2020,
 author = {Hendy, Noureldin and Sloan, Cooper and Tian, F. and Duan, Pengfei and Charchut, Nick and Xie, Y. and Wang, C. and Philbin, J.},
 year = {2020},
 title = {FISHING Net: Future Inference of Semantic Heatmaps In Grids},
 url = {https://arxiv.org/abs/2006.09917},
 keywords = {middle fusion;sensor fusion;voxel},
 urldate = {02.11.2022},
 volume = {abs/2006.09917},
 journal = {ArXiv},
 file = {322ee7fd-d7c7-49f2-9d90-576865da426d:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Citavi Attachments\\322ee7fd-d7c7-49f2-9d90-576865da426d.pdf:pdf;8c3bd844-7106-4873-b9b6-607e67f52e5d:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Remote Attachments\\8c3bd844-7106-4873-b9b6-607e67f52e5d.pdf:pdf}
}


@article{Houston.25.06.2020,
 abstract = {Motivated by the impact of large-scale datasets on ML systems we present the largest self-driving dataset for motion prediction to date, containing over 1,000 hours of data. This was collected by a fleet of 20 autonomous vehicles along a fixed route in Palo Alto, California, over a four-month period. It consists of 170,000 scenes, where each scene is 25 seconds long and captures the perception output of the self-driving system, which encodes the precise positions and motions of nearby vehicles, cyclists, and pedestrians over time. On top of this, the dataset contains a high-definition semantic map with 15,242 labelled elements and a high-definition aerial view over the area. We show that using a dataset of this size dramatically improves performance for key self-driving problems. Combined with the provided software kit, this collection forms the largest and most detailed dataset to date for the development of self-driving machine learning tasks, such as motion forecasting, motion planning and simulation. The full dataset is available at http://level5.lyft.com/.},
 author = {Houston, John and Zuidhof, Guido and Bergamini, Luca and Ye, Yawei and Chen, Long and Jain, Ashesh and Omari, Sammy and Iglovikov, Vladimir and Ondruska, Peter},
 year = {2020},
 title = {One Thousand and One Hours: Self-driving Motion Prediction Dataset},
 url = {http://arxiv.org/pdf/2006.14480v2},
 urldate = {02.11.2022},
 volume = {abs/2006.14480},
 journal = {ArXiv},
 file = {a105c349-66c3-4dc9-bd47-2d9cb1c08d45:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Citavi Attachments\\a105c349-66c3-4dc9-bd47-2d9cb1c08d45.pdf:pdf}
}


@article{Huang.2020,
 author = {Huang, Yu and Chen, Y.},
 year = {2020},
 title = {Autonomous Driving with Deep Learning: A Survey of State-of-Art Technologies},
 url = {https://arxiv.org/abs/2006.06091},
 keywords = {planning},
 urldate = {05.10.2022},
 volume = {abs/2006.06091},
 journal = {ArXiv},
 file = {8a7b529a-fb8e-4435-961a-d5dac52b8afd:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Citavi Attachments\\8a7b529a-fb8e-4435-961a-d5dac52b8afd.pdf:pdf}
}


@article{Huang.2020c,
 abstract = {Autonomous driving has attracted tremendous attention especially in the past few years. The key techniques for a self-driving car include solving tasks like 3D map construction, self-localization, parsing the driving road and understanding objects, which enable vehicles to reason and act. However, large scale data set for training and system evaluation is still a bottleneck for developing robust perception models. In this paper, we present the ApolloScape dataset [1] and its applications for autonomous driving. Compared with existing public datasets from real scenes, e.g., KITTI [2] or Cityscapes [3] , ApolloScape contains much large and richer labelling including holistic semantic dense point cloud for each site, stereo, per-pixel semantic labelling, lanemark labelling, instance segmentation, 3D car instance, high accurate location for every frame in various driving videos from multiple sites, cities and daytimes. For each task, it contains at lease 15x larger amount of images than SOTA datasets. To label such a complete dataset, we develop various tools and algorithms specified for each task to accelerate the labelling process, such as joint 3D-2D segment labeling, active labelling in videos etc. Depend on ApolloScape, we are able to develop algorithms jointly consider the learning and inference of multiple tasks. In this paper, we provide a sensor fusion scheme integrating camera videos, consumer-grade motion sensors (GPS/IMU), and a 3D semantic map in order to achieve robust self-localization and semantic segmentation for autonomous driving. We show that practically, sensor fusion and joint learning of multiple tasks are beneficial to achieve a more robust and accurate system. We expect our dataset and proposed relevant algorithms can support and motivate researchers for further development of multi-sensor fusion and multi-task learning in the field of computer vision.},
 author = {Huang, Xinyu and Wang, Peng and Cheng, Xinjing and Zhou, Dingfu and Geng, Qichuan and Yang, Ruigang},
 year = {2020},
 title = {The ApolloScape Open Dataset for Autonomous Driving and Its Application},
 pages = {2702--2719},
 volume = {42},
 number = {10},
 issn = {1939-3539},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 doi = {10.1109/TPAMI.2019.2926463},
 file = {5529bdf3-71d6-4079-a557-89bd50c9b0e4:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Citavi Attachments\\5529bdf3-71d6-4079-a557-89bd50c9b0e4.pdf:pdf}
}


@misc{Huijser.082008,
 author = {Huijser, M. P. and McGowen, P. and Fuller, J. and Hardy, A. and Kociolek, A. and Clevenger, A. P. and Smith, D. and Ament, R.},
 date = {08/2008},
 year = {2008},
 title = {Wildlife-Vehicle Collision Reduction Study: Report to Congress},
 url = {https://www.fhwa.dot.gov/publications/research/safety/08034/02.cfm},
 urldate = {06.08.2022},
 note = {https://www.fhwa.dot.gov/publications/research/safety/08034/02.cfm},
 number = {FHWA-HRT-08-034},
 institution = {{Western Transporation Institute Montana State University}}
}
 

@inproceedings{J.Deng.2009,
 abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ``ImageNet'', a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500--1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
 author = {{J. Deng} and {W. Dong} and {R. Socher} and {L. -J. Li} and {Kai Li} and {Li Fei-Fei}},
 title = {ImageNet: A large-scale hierarchical image database},
 pages = {248--255},
 isbn = {1063-6919},
 booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
 year = {2009},
 doi = {10.1109/CVPR.2009.5206848},
 file = {https://arxiv.org/abs/1409.0575}
}


@article{Kang.2019,
 abstract = {Many companies aim for delivering systems for autonomous driving reaching out for SAE Level 5. As these systems run much more complex software than typical premium cars of today, a thorough testing strategy is needed. Early prototyping of such systems can be supported using recorded data from onboard and surrounding sensors as long as open-loop testing is applicable; later, though, closed-loop testing is necessary-either by testing on the real vehicle or by using a virtual testing environment. This paper is a substantial extension of our work presented at the 2017 IEEE International Conference on Intelligent Transportation Systems (ITSC) that was surveying the area of publicly available driving datasets. Our previous results are extended by additional datasets and complemented with a summary of publicly available virtual testing environments to support closed-loop testing. As such, a steadily growing number of 37 datasets for open-loop testing and 22 virtual testing environments for closed-loop testing have been surveyed in detailed. Thus, conducting research toward autonomous driving is significantly supported from complementary community efforts: A growing number of publicly accessible datasets allow for experiments with perception approaches or training and testing machine-learning-based algorithms, while virtual testing environments enable end-to-end simulations.},
 author = {Kang, Y. and Yin, H. and Berger, C.},
 year = {2019},
 title = {Test Your Self-Driving Algorithm: An Overview of Publicly Available Driving Datasets and Virtual Testing Environments},
 keywords = {dataset;real world},
 pages = {171--185},
 volume = {4},
 number = {2},
 journal = {IEEE Transactions on Intelligent Vehicles},
 doi = {10.1109/TIV.2018.2886678}
}


@inproceedings{Klueck.2018,
 abstract = {In this paper, we outline a general automated testing approach to be applied for verification and validation of automated and autonomous driving functions. The approach makes use of ontologies of environment the system under test is interacting with. Ontologies are automatically converted into input models for combinatorial testing, which are used to generate test cases. The obtained abstract test cases are used to generate concrete test scenarios that provide the basis for simulation used to verify the functionality of the system under test. We discuss the general approach including its potential for automation in the automotive domain where there is growing need for sophisticated verification based on simulation in case of automated and autonomous vehicles.},
 author = {Klueck, F. and Li, Y. and Nica, M. and Tao, J. and Wotawa, F.},
 title = {Using Ontologies for Test Suites Generation for Automated and Autonomous Driving Functions},
 pages = {118--123},
 booktitle = {2018 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)},
 year = {2018},
 doi = {10.1109/ISSREW.2018.00-20}
}


@inproceedings{Kobs.2020,
 author = {Kobs, Konstantin and Steininger, M. and Zehe, Albin and Lautenschlager, Florian and Hotho, A.},
 title = {SimLoss: Class Similarities in Cross Entropy},
 keywords = {classification;evaluation},
 pages = {431--439},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-030-59491-6},
 editor = {Helic, Denis and Leitner, Gerhard and Stettinger, Martin and Felfernig, Alexander and Ra{\'s}, Zbigniew W.},
 booktitle = {Foundations of Intelligent Systems},
 year = {2020},
 address = {Cham},
 doi = {10.1007/978-3-030-59491-6{\textunderscore }41}
}


@article{Kuznetsova.2020,
 author = {Kuznetsova, Alina and Rom, Hassan and Alldrin, Neil and Uijlings, Jasper and Krasin, Ivan and Pont-Tuset, Jordi and Kamali, Shahab and Popov, Stefan and Malloci, Matteo and Kolesnikov, Alexander and Duerig, Tom and Ferrari, Vittorio},
 year = {2020},
 title = {The Open Images Dataset V4},
 pages = {1956--1981},
 volume = {128},
 number = {7},
 issn = {0920-5691},
 journal = {International Journal of Computer Vision},
 doi = {10.1007/s11263-020-01316-z},
 file = {f7d25d6e-fac6-4004-917a-d2d8634c9cd1:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Citavi Attachments\\f7d25d6e-fac6-4004-917a-d2d8634c9cd1.pdf:pdf}
}


@inproceedings{Lambert.2020,
 abstract = {We present MSeg, a composite dataset that unifies se- mantic segmentation datasets from different domains. A naive merge of the constituent datasets yields poor performance due to inconsistent taxonomies and annotation practices. We reconcile the taxonomies and bring the pixel-level annotations into alignment by relabeling more than 220,000 object masks in more than 80,000 images. The resulting composite dataset enables training a single semantic segmentation model that functions effectively across domains and generalizes to datasets that were not seen during training. We adopt zero-shot cross-dataset transfer as a benchmark to systematically evaluate a model's robustness and show that MSeg training yields substantially more robust models in comparison to training on individual datasets or naive mixing of datasets without the presented contributions. A model trained on MSeg ranks first on the WildDash leaderboard for robust semantic segmentation, with no exposure to WildDash data during training.},
 author = {Lambert, J. and Liu, Z. and Sener, O. and Hays, J. and Koltun, V.},
 title = {MSeg: A Composite Dataset for Multi-Domain Semantic Segmentation},
 keywords = {classification;evaluation;metric},
 pages = {2876--2885},
 booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 year = {2020},
 doi = {10.1109/CVPR42600.2020.00295},
 file = {3c34ebd3-3b79-4f05-ae54-864481e56ec0:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Citavi Attachments\\3c34ebd3-3b79-4f05-ae54-864481e56ec0.pdf:pdf}
}


@misc{Lee.1999,
 author = {Lee, Jinsun and Mannering, Fred},
 date = {1999},
 year = {1999},
 title = {Analysis of roadside accident frequency and severity and roadside safety management},
 url = {https://depts.washington.edu/trac/bulkdisk/pdf/475.1.pdf},
 address = {Seattle, Washington},
 urldate = {05.08.2022},
 note = {https://depts.washington.edu/trac/bulkdisk/pdf/475.1.pdf}, 
 number = {WA-RD 475.1},
 editor = {{Washington State Transportation Center}},
 institution = {{Washington State Transportation Commission, U.S. Department of Transportation}}
}


@article{Lefevre.2014,
 abstract = {With the objective to improve road safety, the automotive industry is moving toward more ``intelligent'' vehicles. One of the major challenges is to detect dangerous situations and react accordingly in order to avoid or mitigate accidents. This requires predicting the likely evolution of the current traffic situation, and assessing how dangerous that future situation might be. This paper is a survey of existing methods for motion prediction and risk assessment for intelligent vehicles. The proposed classification is based on the semantics used to define motion and risk. We point out the tradeoff between model completeness and real-time constraints, and the fact that the choice of a risk assessment method is influenced by the selected motion model.},
 author = {Lef{\`e}vre, St{\'e}phanie and Vasquez, Dizan and Laugier, Christian},
 year = {2014},
 title = {A survey on motion prediction and risk assessment for intelligent vehicles},
 url = {https://robomechjournal.springeropen.com/articles/10.1186/s40648-014-0001-z},
 pages = {1--14},
 volume = {1},
 number = {1},
 issn = {2197-4225},
 journal = {ROBOMECH Journal},
 doi = {10.1186/s40648-014-0001-z},
 file = {e70b7036-d943-45a2-bbc6-3e0c315bd3ea:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Citavi Attachments\\e70b7036-d943-45a2-bbc6-3e0c315bd3ea.pdf:pdf}
}


@inproceedings{Lin.01.05.2014,
 abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
 author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Doll{\'a}r, Piotr},
 title = {Microsoft COCO: Common Objects in Context},
 url = {https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48},
 urldate = {20.01.2023},
 pages = {740--755},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-319-10602-1},
 editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
 booktitle = {Computer Vision -- ECCV 2014},
 year = {2014},
 address = {Cham},
 file = {1c9274ce-4894-4483-9512-b59aff250e72:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Citavi Attachments\\1c9274ce-4894-4483-9512-b59aff250e72.pdf:pdf;5477155e-f318-4fd1-b967-d31efb6e9d80:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Remote Attachments\\5477155e-f318-4fd1-b967-d31efb6e9d80.pdf:pdf}
}


@article{Liu.17.03.2020,
 abstract = {This article introduces the solutions of the two champion teams, `MMfruit' for the detection track and `MMfruitSeg' for the segmentation track, in OpenImage Challenge 2019. It is commonly known that for an object detector, the shared feature at the end of the backbone is not appropriate for both classification and regression, which greatly limits the performance of both single stage detector and Faster RCNN \cite{ren2015faster} based detector. In this competition, we observe that even with a shared feature, different locations in one object has completely inconsistent performances for the two tasks. \textit{E.g. the features of salient locations are usually good for classification, while those around the object edge are good for regression.} Inspired by this, we propose the Decoupling Head (DH) to disentangle the object classification and regression via the self-learned optimal feature extraction, which leads to a great improvement. Furthermore, we adjust the soft-NMS algorithm to adj-NMS to obtain stable performance improvement. Finally, a well-designed ensemble strategy via voting the bounding box location and confidence is proposed. We will also introduce several training/inferencing strategies and a bag of tricks that give minor improvement. Given those masses of details, we train and aggregate 28 global models with various backbones, heads and 3+2 expert models, and achieves the 1st place on the OpenImage 2019 Object Detection Challenge on the both public and private leadboards. Given such good instance bounding box, we further design a simple instance-level semantic segmentation pipeline and achieve the 1st place on the segmentation challenge.},
 author = {Liu, Yu and Song, Guanglu and Zang, Yuhang and Gao, Yan and Xie, Enze and Yan, Junjie and Loy, Chen Change and Wang, Xiaogang},
 year = {2020},
 title = {1st Place Solutions for OpenImage2019 -- Object Detection and Instance  Segmentation},
 url = {https://arxiv.org/pdf/2003.07557},
 keywords = {ensemble;image;object detection},
 urldate = {02.11.2022},
 volume = {abs/2003.07557},
 journal = {ArXiv},
 file = {Liu, Song et al. 17.03.2020 - 1st Place Solutions for OpenImage2019.pdf},
 file = {https://arxiv.org/pdf/2003.07557v1.pdf}
}


@inproceedings{Liu.2020b,
 author = {Liu, Xiao-Feng and Zhang, Yimeng and Liu, Xiongchang and Bai, Song and Li, Site and You, J.},
 title = {Reinforced Wasserstein Training for Severity-Aware Semantic Segmentation in Autonomous Driving},
 keywords = {classification;evaluation;metric},
 volume = {abs/2008.04751},
 booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 year = {2020},
 file = {fb065f8e-f70f-4711-831f-1ca128f83c33:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Citavi Attachments\\fb065f8e-f70f-4711-831f-1ca128f83c33.pdf:pdf}
}


@article{Mao.21.06.2021,
 abstract = {Current perception models in autonomous driving have become notorious for greatly relying on a mass of annotated data to cover unseen cases and address the long-tail problem. On the other hand, learning from unlabeled large-scale collected data and incrementally self-training powerful recognition models have received increasing attention and may become the solutions of next-generation industry-level powerful and robust perception models in autonomous driving. However, the research community generally suffered from data inadequacy of those essential real-world scene data, which hampers the future exploration of fully/semi/self-supervised methods for 3D perception. In this paper, we introduce the ONCE (One millioN sCenEs) dataset for 3D object detection in the autonomous driving scenario. The ONCE dataset consists of 1 million LiDAR scenes and 7 million corresponding camera images. The data is selected from 144 driving hours, which is 20x longer than the largest 3D autonomous driving dataset available (e.g. nuScenes and Waymo), and it is collected across a range of different areas, periods and weather conditions. To facilitate future research on exploiting unlabeled data for 3D detection, we additionally provide a benchmark in which we reproduce and evaluate a variety of self-supervised and semi-supervised methods on the ONCE dataset. We conduct extensive analyses on those methods and provide valuable observations on their performance related to the scale of used data. Data, code, and more information are available at https://once-for-auto-driving.github.io/index.html.},
 author = {Mao, Jiageng and Niu, Minzhe and Jiang, Chenhan and Liang, Hanxue and Chen, Jingheng and Liang, Xiaodan and Li, Yamin and Ye, Chaoqiang and Zhang, Wei and Li, Zhenguo and Yu, Jie and Xu, Hang and Xu, Chunjing},
 year = {2021},
 title = {One Million Scenes for Autonomous Driving: ONCE Dataset},
 url = {http://arxiv.org/pdf/2106.11037v3},
 urldate = {02.11.2022},
 volume = {abs/2106.11037},
 journal = {ArXiv},
 file = {548dc9de-e915-4e15-8128-4aecc38a3c2c:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Citavi Attachments\\548dc9de-e915-4e15-8128-4aecc38a3c2c.pdf:pdf}
}


@inproceedings{Meletis.15.03.2018,
 abstract = {We propose a convolutional network with hierarchical classifiers for per-pixel semantic segmentation, which is able to be trained on multiple, heterogeneous datasets and exploit their semantic hierarchy. Our network is the first to be simultaneously trained on three different datasets from the intelligent vehicles domain, i.e. Cityscapes, GTSDB and Mapillary Vistas, and is able to handle different semantic level-of-detail, class imbalances, and different annotation types, i.e. dense per-pixel and sparse bounding-box labels. We assess our hierarchical approach, by comparing against flat, non-hierarchical classifiers and we show improvements in mean pixel accuracy of 13.0{\%} for Cityscapes classes and 2.4{\%} for Vistas classes and 32.3{\%} for GTSDB classes. Our implementation achieves inference rates of 17 fps at a resolution of 520x706 for 108 classes running on a GPU.},
 author = {Meletis, Panagiotis and Dubbelman, Gijs},
 title = {Training of Convolutional Networks on Multiple Heterogeneous Datasets  for Street Scene Semantic Segmentation},
 url = {https://arxiv.org/pdf/1803.05675},
 pages = {1045--1050},
 publisher = {IEEE},
 isbn = {978-1-5386-4452-2},
 booktitle = {2018 IEEE Intelligent Vehicles Symposium (IV)},
 year = {2018},
 address = {Piscataway, NJ},
 doi = {10.1109/IVS.2018.8500398},
 file = {Meletis, Dubbelman 15.03.2018 - Training of Convolutional Networks.pdf},
 file = {https://arxiv.org/pdf/1803.05675v2.pdf}
}


@article{Miller.1995,
 author = {Miller, George A.},
 year = {1995},
 title = {WordNet: a lexical database for English},
 pages = {39--41},
 volume = {38},
 number = {11},
 issn = {0001-0782},
 journal = {Communications of the ACM},
 doi = {10.1145/219717.219748}
}


@inproceedings{Neuhold.2017,
 author = {Neuhold, Gerhard and Ollmann, Tobias and Bul{\`o}, Samuel Rota and Kontschieder, Peter},
 title = {The Mapillary Vistas Dataset for Semantic Understanding of Street Scenes},
 pages = {5000--5009},
 booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
 year = {2017},
 doi = {10.1109/ICCV.2017.534}
}


@article{Niitani.25.10.2019,
 abstract = {We present the instance segmentation and the object detection method used by team PFDet for Open Images Challenge 2019. We tackle a massive dataset size, huge class imbalance and federated annotations. Using this method, the team PFDet achieved 3rd and 4th place in the instance segmentation and the object detection track, respectively.},
 author = {Niitani, Yusuke and Ogawa, Toru and Suzuki, Shuji and Akiba, Takuya and Kerola, Tommi and Ozaki, Kohei and Sano, Shotaro},
 year = {2019},
 title = {Team PFDet's Methods for Open Images Challenge 2019},
 url = {https://arxiv.org/pdf/1910.11534},
 urldate = {02.11.2022},
 volume = {abs/1910.11534},
 journal = {ArXiv},
 file = {Niitani, Ogawa et al. 25.10.2019 - Team PFDet's Methods for Open.pdf},
 file = {https://arxiv.org/pdf/1910.11534v1.pdf}
}


@article{Orsic.02.09.2020,
 abstract = {We present our submission to the semantic segmentation contest of the Robust Vision Challenge held at ECCV 2020. The contest requires submitting the same model to seven benchmarks from three different domains. Our approach is based on the SwiftNet architecture with pyramidal fusion. We address inconsistent taxonomies with a single-level 193-dimensional softmax output. We strive to train with large batches in order to stabilize optimization of a hard recognition problem, and to favour smooth evolution of batchnorm statistics. We achieve this by implementing a custom backward step through log-sum-prob loss, and by using small crops before freezing the population statistics. Our model ranks first on the RVC semantic segmentation challenge as well as on the WildDash 2 leaderboard. This suggests that pyramidal fusion is competitive not only for efficient inference with lightweight backbones, but also in large-scale setups for multi-domain application.},
 author = {Or{\v{s}}i{\'c}, Marin and Bevandi{\'c}, Petra and Grubi{\v{s}}i{\'c}, Ivan and {\v{S}}ari{\'c}, Josip and {\v{S}}egvi{\'c}, Sini{\v{s}}a},
 year = {2020},
 title = {Multi-domain semantic segmentation with pyramidal fusion},
 url = {https://arxiv.org/pdf/2009.01636},
 keywords = {classification;evaluation;metric},
 urldate = {02.11.2022},
 volume = {abs/2009.01636},
 journal = {ArXiv},
 file = {Ori, Bevandi et al. 02.09.2020 - Multi-domain semantic segmentation with pyramidal.pdf},
 file = {https://arxiv.org/pdf/2009.01636v3.pdf}
}


@article{Pan.2020,
 abstract = {Sensing surroundings plays a crucial role in human spatial perception, as it extracts the spatial configuration of objects as well as the free space from the observations. To facilitate the robot perception with such a surrounding sensing capability, we introduce a novel visual task called Cross-view Semantic Segmentation as well as a framework named View Parsing Network (VPN) to address it. In the cross-view semantic segmentation task, the agent is trained to parse the first-view observations into a top-down-view semantic map indicating the spatial location of all the objects at pixel-level. The main issue of this task is that we lack the real-world annotations of top-down-view data. To mitigate this, we train the VPN in 3D graphics environment and utilize the domain adaptation technique to transfer it to handle real-world data. We evaluate our VPN on both synthetic and real-world agents. The experimental results show that our model can effectively make use of the information from different views and multi-modalities to understanding spatial information. Our further experiment on a LoCoBot robot shows that our model enables the surrounding sensing capability from 2D image input. Code and demo videos can be found at \url{https://view-parsing-network.github.io}.},
 author = {Pan, Bowen and Sun, Jiankai and Leung, Ho Yin Tiga and Andonian, Alex and Zhou, Bolei},
 year = {2020},
 title = {Cross-view Semantic Segmentation for Sensing Surroundings},
 url = {https://arxiv.org/pdf/1906.03560},
 keywords = {middle fusion;sensor fusion;voxel},
 pages = {4867--4873},
 volume = {5},
 number = {3},
 issn = {2377-3766},
 journal = {IEEE Robotics and Automation Letters},
 doi = {10.1109/LRA.2020.3004325},
 file = {952be018-37dd-409f-94c3-cc3e4d37629b:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Citavi Attachments\\952be018-37dd-409f-94c3-cc3e4d37629b.pdf:pdf}
}


@incollection{Redmon.25.12.2016,
 abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
 author = {Redmon, Joseph and Farhadi, Ali},
 title = {YOLO9000: Better, Faster, Stronger},
 url = {https://arxiv.org/pdf/1612.08242},
 keywords = {anchor;classification;evaluation;image;object detection;single stage},
 pages = {6517--6525},
 booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition},
 doi = {10.1109/CVPR.2017.690},
 file = {Redmon, Farhadi 25.12.2016 - YOLO9000 Better.pdf},
 file = {810be7b2-4ba2-46d2-8aa7-673deb1e22d3:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Remote Attachments\\810be7b2-4ba2-46d2-8aa7-673deb1e22d3.pdf:pdf}
}


@misc{Richards.2010,
 author = {Richards, D. C.},
 date = {2010},
 year = {2020},
 title = {Relationship between Speed and Risk of Fatal Injury: Pedestrians and Car Occupants},
 url = {https://nacto.org/docs/usdg/relationship_between_speed_risk_fatal_injury_pedestrians_and_car_occupants_richards.pdf},
 address = {London},
 urldate = {29.07.2022},
 number = {No. 16},
 series = {Road Safety Web Publication},
 institution = {{Transport Research Laboratory}}
}


@article{Russakovsky.02.09.2014,
 abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions.  This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
 author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
 year = {2015},
 title = {ImageNet Large Scale Visual Recognition Challenge},
 pages = {211--252},
 volume = {115},
 number = {3},
 issn = {0920-5691},
 journal = {International Journal of Computer Vision},
 doi = {10.1007/s11263-015-0816-y},
 file = {78951616-7690-4c81-b715-01f9de9c9b4e:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Citavi Attachments\\78951616-7690-4c81-b715-01f9de9c9b4e.pdf:pdf}
}


@article{ShalevShwartz.21.08.2017,
 abstract = {In recent years, car makers and tech companies have been racing towards self driving cars. It seems that the main parameter in this race is who will have the first car on the road. The goal of this paper is to add to the equation two additional crucial parameters. The first is standardization of safety assurance --- what are the minimal requirements that every self-driving car must satisfy, and how can we verify these requirements. The second parameter is scalability --- engineering solutions that lead to unleashed costs will not scale to millions of cars, which will push interest in this field into a niche academic corner, and drive the entire field into a {\textquotedbl}winter of autonomous driving{\textquotedbl}. In the first part of the paper we propose a white-box, interpretable, mathematical model for safety assurance, which we call Responsibility-Sensitive Safety (RSS). In the second part we describe a design of a system that adheres to our safety assurance requirements and is scalable to millions of cars.},
 author = {Shalev-Shwartz, Shai and Shammah, Shaked and Shashua, Amnon},
 year = {2017},
 title = {On a Formal Model of Safe and Scalable Self-driving Cars},
 url = {https://arxiv.org/pdf/1708.06374},
 urldate = {02.11.2022},
 volume = {abs/1708.06374},
 journal = {ArXiv},
 file = {Shalev-Shwartz, Shammah et al. 21.08.2017 - On a Formal Model.pdf},
 file = {https://arxiv.org/pdf/1708.06374v6.pdf}
}


@inproceedings{Singh.05.12.2017,
 abstract = {We present R-FCN-3000, a large-scale real-time object detector in which objectness detection and classification are decoupled. To obtain the detection score for an RoI, we multiply the objectness score with the fine-grained classification score. Our approach is a modification of the R-FCN architecture in which position-sensitive filters are shared across different object classes for performing localization. For fine-grained classification, these position-sensitive filters are not needed. R-FCN-3000 obtains an mAP of 34.9{\%} on the ImageNet detection dataset and outperforms YOLO-9000 by 18{\%} while processing 30 images per second. We also show that the objectness learned by R-FCN-3000 generalizes to novel classes and the performance increases with the number of training object classes - supporting the hypothesis that it is possible to learn a universal objectness detector. Code will be made available.},
 author = {Singh, Bharat and Li, Hengduo and Sharma, Abhishek and Davis, Larry S.},
 title = {R-FCN-3000 at 30fps: Decoupling Detection and Classification},
 url = {https://ieeexplore.ieee.org/document/8578217},
 urldate = {20.01.2023},
 pages = {1081--1090},
 publisher = {IEEE},
 isbn = {978-1-5386-6420-9},
 booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2018)},
 year = {2018},
 address = {Piscataway, NJ},
 doi = {10.1109/CVPR.2018.00119},
 file = {https://arxiv.org/pdf/1712.01802v1.pdf}
}


@misc{StatistischesBundesamt.25.07.2022,
 author = {{Statistisches Bundesamt}},
 date = {25.07.2022},
 title = {Verkehr: Verkehrsunf{\"a}lle: April 2022},
 url = {https://www.destatis.de/DE/Themen/Gesellschaft-Umwelt/Verkehrsunfaelle/Publikationen/Downloads-Verkehrsunfaelle/verkehrsunfaelle-monat-2080700221044.pdf?__blob=publicationFile},
 urldate = {05.08.2022},
 note = {https://www.destatis.de/DE/Themen/Gesellschaft-Umwelt/Verkehrsunfaelle/Publikationen/Downloads-Verkehrsunfaelle/verkehrsunfaelle-monat-2080700221044.pdf?\_\_blob\=publicationFile}, 
 edition = {Fachserie 8  Reihe 7},
 number = {2080700221044},
 editor = {{Statistisches Bundesamt}}
}


@article{Sun.10.12.2019,
 abstract = {The research community has increasing interest in autonomous driving research, despite the resource intensity of obtaining representative real world data. Existing self-driving datasets are limited in the scale and variation of the environments they capture, even though generalization within and between operating regions is crucial to the overall viability of the technology. In an effort to help align the research community's contributions with real-world self-driving problems, we introduce a new large scale, high quality, diverse dataset. Our new dataset consists of 1150 scenes that each span 20 seconds, consisting of well synchronized and calibrated high quality LiDAR and camera data captured across a range of urban and suburban geographies. It is 15x more diverse than the largest camera+LiDAR dataset available based on our proposed diversity metric. We exhaustively annotated this data with 2D (camera image) and 3D (LiDAR) bounding boxes, with consistent identifiers across frames. Finally, we provide strong baselines for 2D as well as 3D detection and tracking tasks. We further study the effects of dataset size and generalization across geographies on 3D detection methods. Find data, code and more up-to-date information at http://www.waymo.com/open.},
 author = {Sun, Pei and Kretzschmar, Henrik and Dotiwalla, Xerxes and Chouard, Aurelien and Patnaik, Vijaysai and Tsui, Paul and Guo, James and Zhou, Yin and Chai, Yuning and Caine, Benjamin and Vasudevan, Vijay and Han, Wei and Ngiam, Jiquan and Zhao, Hang and Timofeev, Aleksei and Ettinger, Scott and Krivokon, Maxim and Gao, Amy and Joshi, Aditya and Zhao, Sheng and Cheng, Shuyang and Zhang, Yu and Shlens, Jonathon and Chen, Zhifeng and Anguelov, Dragomir},
 year = {2019},
 title = {Scalability in Perception for Autonomous Driving: Waymo Open Dataset},
 url = {https://arxiv.org/pdf/1912.04838},
 keywords = {dataset;evaluation;metric;object detection;real world;tracking},
 urldate = {21.09.2022},
 volume = {abs/1912.04838},
 journal = {ArXiv},
 file = {Sun, Kretzschmar et al. 10.12.2019 - Scalability in Perception for Autonomous.pdf},
 file = {https://arxiv.org/pdf/1912.04838v7.pdf}
}


@article{Volk.2020,
 author = {Volk, Georg and Gamerdinger, J{\"o}rg and Betnuth, Alexander von and Bringmann, O.},
 year = {2020},
 title = {A Comprehensive Safety Metric to Evaluate Perception in Autonomous Systems},
 keywords = {evaluation;metric;object detection;safety;tracking},
 pages = {1--8},
 journal = {2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)},
 file = {76cda630-4ceb-4321-822b-a595345ebe66:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Citavi Attachments\\76cda630-4ceb-4321-822b-a595345ebe66.pdf:pdf;1857f36b-83a7-40e0-b10e-f9a2ae40bbba:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Remote Attachments\\1857f36b-83a7-40e0-b10e-f9a2ae40bbba.pdf:pdf}
}


@article{Wilson.2021,
 author = {Wilson, Benjamin and Qi, William and Agarwal, Tanmay and Lambert, John and Singh, Jagjeet and Khandelwal, Siddhesh and Pan, Bowen and Kumar, Ratnesh and Hartnett, Andrew and {Kaesemodel Pontes}, Jhony and Ramanan, Deva and Carr, Peter and Hays, James},
 year = {2021},
 title = {Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting},
 volume = {1},
 journal = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},
 file = {Wilson, Qi et al. 2021 - Argoverse 2.pdf},
 file = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/4734ba6f3de83d861c3176a6273cac6d-Abstract-round2.html}
}


@article{Wu.02.09.2017,
 abstract = {Failing to distinguish between a sheepdog and a skyscraper should be worse and penalized more than failing to distinguish between a sheepdog and a poodle; after all, sheepdogs and poodles are both breeds of dogs. However, existing metrics of failure (so-called {\textquotedbl}loss{\textquotedbl} or {\textquotedbl}win{\textquotedbl}) used in textual or visual classification/recognition via neural networks seldom leverage a-priori information, such as a sheepdog being more similar to a poodle than to a skyscraper. We define a metric that, inter alia, can penalize failure to distinguish between a sheepdog and a skyscraper more than failure to distinguish between a sheepdog and a poodle. Unlike previously employed possibilities, this metric is based on an ultrametric tree associated with any given tree organization into a semantically meaningful hierarchy of a classifier's classes. An ultrametric tree is a tree with a so-called ultrametric distance metric such that all leaves are at the same distance from the root. Unfortunately, extensive numerical experiments indicate that the standard practice of training neural networks via stochastic gradient descent with random starting points often drives down the hierarchical loss nearly as much when minimizing the standard cross-entropy loss as when trying to minimize the hierarchical loss directly. Thus, this hierarchical loss is unreliable as an objective for plain, randomly started stochastic gradient descent to minimize; the main value of the hierarchical loss may be merely as a meaningful metric of success of a classifier.},
 author = {Wu, Cinna and Tygert, Mark and LeCun, Yann},
 year = {2019},
 title = {A hierarchical loss and its problems when classifying non-hierarchically},
 keywords = {classification;evaluation;metric},
 volume = {14},
 number = {12},
 journal = {PloS one},
 file = {9fc0648f-fa5a-4db4-9608-6d50794bde7a:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Citavi Attachments\\9fc0648f-fa5a-4db4-9608-6d50794bde7a.pdf:pdf;70778842-ed7b-45d5-9399-ca919ec02db2:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Remote Attachments\\70778842-ed7b-45d5-9399-ca919ec02db2.pdf:pdf}
}


@article{Wu.10.08.2019,
 abstract = {Object detection is a fundamental visual recognition problem in computer vision and has been widely studied in the past decades. Visual object detection aims to find objects of certain target classes with precise localization in a given image and assign each object instance a corresponding class label. Due to the tremendous successes of deep learning based image classification, object detection techniques using deep learning have been actively studied in recent years. In this paper, we give a comprehensive survey of recent advances in visual object detection with deep learning. By reviewing a large body of recent related work in literature, we systematically analyze the existing object detection frameworks and organize the survey into three major parts: (i) detection components, (ii) learning strategies, and (iii) applications {\&} benchmarks. In the survey, we cover a variety of factors affecting the detection performance in detail, such as detector architectures, feature learning, proposal generation, sampling strategies, etc. Finally, we discuss several future directions to facilitate and spur future research for visual object detection with deep learning. Keywords: Object Detection, Deep Learning, Deep Convolutional Neural Networks},
 author = {Wu, Xiongwei and Sahoo, Doyen and Hoi, Steven C. H.},
 year = {2020},
 title = {Recent Advances in Deep Learning for Object Detection},
 url = {https://https://www.sciencedirect.com/science/article/pii/S0925231220301430.org/pdf/1908.03673},
 pages = {39--64},
 volume = {396},
 issn = {0925-2312},
 journal = {Neurocomputing},
 file = {4a9e5c44-a6b8-43e5-ada1-91623bff2072:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Citavi Attachments\\4a9e5c44-a6b8-43e5-ada1-91623bff2072.pdf:pdf;8127754d-fdba-4eab-bf1a-e3c70f13227c:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Remote Attachments\\8127754d-fdba-4eab-bf1a-e3c70f13227c.pdf:pdf}
}


@inproceedings{Yu.2020,
 abstract = {Datasets drive vision progress, yet existing driving datasets are impoverished in terms of visual content and supported tasks to study multitask learning for autonomous driving. Researchers are usually constrained to study a small set of problems on one dataset, while real-world computer vision applications require performing tasks of various complexities. We construct BDD100K, the largest driving video dataset with 100K videos and 10 tasks to evaluate the exciting progress of image recognition algorithms on autonomous driving. The dataset possesses geographic, environmental, and weather diversity, which is useful for training models that are less likely to be surprised by new conditions. Based on this diverse dataset, we build a benchmark for heterogeneous multitask learning and study how to solve the tasks together. Our experiments show that special training strategies are needed for existing models to perform such heterogeneous tasks. BDD100K opens the door for future studies in this important venue.},
 author = {Yu, Fisher and Chen, Haofeng and Wang, Xin and Xian, Wenqi and Chen, Yingying and Liu, Fangchen and Madhavan, Vashisht and Darrell, Trevor},
 title = {BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning},
 url = {https://arxiv.org/pdf/1805.04687},
 keywords = {dataset;image;map;object detection;representation;tracking},
 booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 year = {2020},
 file = {Yu, Chen et al. 12.05.2018 - BDD100K A Diverse Driving Dataset.pdf},
 file = {https://arxiv.org/pdf/1805.04687v2.pdf}
}


@inproceedings{Zendel.2018,
 author = {Zendel, Oliver and Honauer, Katrin and Murschitz, Markus and Steininger, Daniel and Dominguez, Gustavo Fernandez},
 title = {WildDash - Creating Hazard-Aware Benchmarks},
 keywords = {camera;dataset;real world;robustness;testing},
 booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
 year = {2018}
}


@article{Zhou.12.03.2021,
 abstract = {We develop a probabilistic interpretation of two-stage object detection. We show that this probabilistic interpretation motivates a number of common empirical training practices. It also suggests changes to two-stage detection pipelines. Specifically, the first stage should infer proper object-vs-background likelihoods, which should then inform the overall score of the detector. A standard region proposal network (RPN) cannot infer this likelihood sufficiently well, but many one-stage detectors can. We show how to build a probabilistic two-stage detector from any state-of-the-art one-stage detector. The resulting detectors are faster and more accurate than both their one- and two-stage precursors. Our detector achieves 56.4 mAP on COCO test-dev with single-scale testing, outperforming all published results. Using a lightweight backbone, our detector achieves 49.2 mAP on COCO at 33 fps on a Titan Xp, outperforming the popular YOLOv4 model.},
 author = {Zhou, Xingyi and Koltun, Vladlen and Kr{\"a}henb{\"u}hl, Philipp},
 year = {2021},
 title = {Probabilistic two-stage detection},
 url = {https://arxiv.org/pdf/2103.07461},
 volume = {abs/2103.07461},
 journal = {ArXiv},
 file = {7b75db6a-7bc2-4f9a-b555-03b34a9fb61c:C\:\\Users\\user\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\r8r7296ofnqmplkgurv7echwhs86cjl9841zyabhf9hd0h4ag\\Citavi Attachments\\7b75db6a-7bc2-4f9a-b555-03b34a9fb61c.pdf:pdf}
}


@article{Zhu.26.08.2019,
 abstract = {This report presents our method which wins the nuScenes3D Detection Challenge [17] held in Workshop on Autonomous Driving(WAD, CVPR 2019). Generally, we utilize sparse 3D convolution to extract rich semantic features, which are then fed into a class-balanced multi-head network to perform 3D object detection. To handle the severe class imbalance problem inherent in the autonomous driving scenarios, we design a class-balanced sampling and augmentation strategy to generate a more balanced data distribution. Furthermore, we propose a balanced group-ing head to boost the performance for the categories withsimilar shapes. Based on the Challenge results, our methodoutperforms the PointPillars [14] baseline by a large mar-gin across all metrics, achieving state-of-the-art detection performance on the nuScenes dataset. Code will be released at CBGS.},
 author = {Zhu, Benjin and Jiang, Zhengkai and Zhou, Xiangxin and Li, Zeming and Yu, Gang},
 year = {2019},
 title = {Class-balanced Grouping and Sampling for Point Cloud 3D Object Detection},
 url = {https://arxiv.org/pdf/1908.09492},
 keywords = {evaluation;grid;lidar;loss function;object detection;voxel},
 urldate = {02.11.2022},
 volume = {abs/1908.09492},
 journal = {ArXiv},
 file = {Zhu, Jiang et al. 26.08.2019 - Class-balanced Grouping and Sampling.pdf},
 file = {https://arxiv.org/pdf/1908.09492v1.pdf}
}


