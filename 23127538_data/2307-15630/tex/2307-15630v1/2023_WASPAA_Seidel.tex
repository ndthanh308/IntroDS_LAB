% --------------------------------------------------------------------------
% Template for WASPAA-2023 paper; to be used with:
%          waspaa23.sty  - WASPAA 2019 LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
%
% --------------------------------------------------------------------------

\documentclass{article}
\usepackage[preprint]{spconfa4}
\usepackage{amsmath,graphicx,url,times}
%\usepackage{waspaa19,amssymb,amsmath,graphicx,times,url}
\usepackage{color}
\usepackage{xcolor,colortbl}
% \usepackage{soul}
% \sethlcolor{green}

\usepackage[mode=buildnew]{standalone}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{etoolbox, siunitx}
\usepackage{tabularx}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{pgfplots} 

\usetikzlibrary{dsp}
\usetikzlibrary{chains}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{calc}

% Example definitions.
% --------------------
\def\defeqn{\stackrel{\triangle}{=}}
\newcommand{\symvec}[1]{{\mbox{\boldmath $#1$}}}
\newcommand{\symmat}[1]{{\mbox{\boldmath $#1$}}}
\newcommand\M{M'}

\newcommand{\circled}[1]{\textcircled{\raisebox{-0.5pt}{\scriptsize #1}}}

\definecolor{Gray}{gray}{0.85}

% Title.
% --------------------
\title{EFFICIENT DEEP ACOUSTIC ECHO SUPPRESSION \\ WITH CONDITION-AWARE TRAINING}

%% Single addresses (uncomment and modify for single-address case).
%% --------------------
%\name{Author(s) Name(s)\thanks{Thanks to XYZ agency for funding.}}
%\address{Author Affiliation(s)}
%%
%% For example:
%% ------------
%%\address{School\\
%%       Department\\
%%       Address}

% Two addresses
% --------------------
%\twoauthors
%  {John Doe\sthanks{Thanks to ABC agency for funding.}}
%    {Fictional University\\
%Computer Science Dept., 2133 Long Road\\
%     Gotham, NY 10027, USA \\
%     john@fictional.edu}
%  {Maria Ortega\sthanks{Thanks to XYZ agency for funding.}}
%    {University of the Imagination \\
%     Big Engineering Building, 8765 Dream Blvd. \\
%     New Chicago, IL 60626, USA \\
%     maria@imagination.edu}

% Many authors with many addresses
% --------------------
\name{Ernst Seidel$^{\ast}$, Pejman Mowlaee$^{\circ}$, Tim Fingscheidt$^{\ast}$}
\address{$^{\ast}$Institute for Communications Technology,
	Technische Universität Braunschweig\\
	Schleinitzstraße 22,
	38106 Braunschweig, Germany\\ $^{\circ}$GN Audio A/S,
	Lautrupbjerg 7,
	2750 Ballerup, Denmark}
% \name{John Doe,$^{1}$\sthanks{Thanks to ABC agency for funding.}
%       Maria Ortega,$^{2}$\sthanks{Thanks to XYZ agency for funding.}
%       Third Author,$^{3}$ \sthanks{Also many thanks.}
%       Fourth Author$^{2}$}
% \address{$^1$ Fictional University, Computer Science Dept., 2133 Long Road, Gotham, NY 10027, USA\\ john@fictional.edu\\              
%          $^2$ University of the Imagination, Big Engineering Building, 8765 Dream Blvd., New Chicago, IL 60626, USA\\
%           maria@imagination.edu, fourthAuthor@imagination.edu\\
%          $^3$ Important Laboratory, 123 Street, City, Country, thirdAuthor@test.edu\\       
% }

\begin{document}

\ninept
\maketitle

\begin{sloppy}

\begin{abstract}
  The topic of deep acoustic echo control (DAEC) has seen many approaches with various model topologies in recent years. Convolutional recurrent networks (CRNs), consisting of a convolutional encoder and decoder encompassing a recurrent bottleneck, are repeatedly employed due to their ability to preserve nearend speech even in double-talk (DT) condition. However, past architectures are either computationally complex or trade off smaller model sizes with a decrease in performance.
  We propose an improved CRN topology which, compared to other realizations of this class of architectures, not only saves parameters and computational complexity, but also shows improved performance in DT, outperforming both baseline architectures FCRN and CRUSE. Striving for a condition-aware training, we also demonstrate the importance of a high proportion of double-talk and the missing value of nearend-only speech in DAEC training data. Finally, we show how to control the trade-off between aggressive echo suppression and near-end speech preservation by fine-tuning with condition-aware component loss functions.
\end{abstract}

\begin{keywords}
acoustic echo suppression, machine learning, convolutional recurrent network
\end{keywords}

\section{Introduction}
\label{sec:intro}

% \begin{itemize}
%     \item General Intro AEC + DAEC
%     \item Architectures -\> CRN
%     \item Efficient Networks
%     \item Condition-Aware
% \end{itemize}

The topic of acoustic echo control (AEC) has seen many scientific advancements in the recent years. While AEC is usually still conducted using classical approaches such as linear filters \cite{SpeexDSP,enzner_vary_fdaf}, more and more works have shown the tremendous advantage of employing deep neural networks, either as postfilters \cite{Valin_AEC,Zhang2022a}, hybrid systems \cite{Revach2021}, or as fully machine-learned echo cancellers \cite{westhausen21_icassp, seidel21_interspeech, Braun2022}.
% ,Peng2021

One of the more prominent architectural choices for deep AEC is the convolutional recurrent network (CRN) \cite{seidel21_interspeech, Wang2022, Braun2022}. These models consist of a convolutional encoder and decoder encompassing a recurrent bottleneck. Such models are capable at suppressing undesired components while preserving good speech quality \cite{Strake2020}. This makes them a great choice for AEC, as nearend (NE) speech preservation can be as important as aggressive echo suppression, wherever a postfilter is employed to remove residual echo alongside noise \cite{seidel21_interspeech, Braun2022}.
While CRNs, especially their fully convolutional variants \cite{seidel21_interspeech}, are great at preserving NE speech, they often exhibit high computational complexity and large network size. Recent proposals of more efficient CRNs \cite{Braun2022} report a less than optimal performance of the trained AEC as a stand-alone system.

% Another important aspect of AEC is the proportion of the three conditions double-talk (DT), farend single-talk (STFE), and nearend single-talk (STNE) in the training data. While a higher focus on DT as the most complex condition is common, the overall distribution varies and publications in the field of AEC rarely motivate their overall condition distribution.
The proportion of the three conditions double-talk (DT), farend single-talk (STFE), and nearend single-talk (STNE) in the AEC training data is also important. Many models are trained on data from the Microsoft AEC Challenge~\cite{cutler2022AEC} or on datasets of varying condition proportions \cite{Wang2022, Braun2022}, but their specific proportion choice is rarely motivated. Some authors propose losses sensitive to the microphone signal, e.g., using echo-aware loss weights per time-frequency bin \cite{Zhang2022a} or separate condition-specific losses \cite{pfeifenberger21_interspeech}.
% Some authors also propose condition-aware loss formulations, e.g., echo-aware loss weighting \cite{Zhang2022a}, to enhance their model's overall performance.

In this work, we adapt the {\tt FCRN15} noise reduction model~\cite{Strake2022} to acoustic echo suppression (AES) and propose modifications, {\it greatly decreasing computational complexity and parameter count}. We furthermore conduct ablation studies on the impact of the {\it minibatch condition splits (MCSs)} between the different conditions DT, STFE, and STNE. To improve the condition-aware training further, we propose {\it a single} condition-aware loss allowing to trade off between NE speech preservation and echo suppression {\it on a file basis}.

% The remainder of this paper will provide model and method descriptions in Section 2, experimental setup and evaluation in Section 3 and conclusions in Section 4.
The remainder of this paper is structured as follows: Section 2 introduces the processing framework, baseline architectures, proposed modifications and the condition-aware training concept. The datasets, training details and conducted experiments are presented in Section 3. Section 4 provides conclusions.

\section{System Overview and Proposed Method}
\label{sec:method}

\subsection{Processing Framework and Baseline Models}
\label{ssec:subhead}

All models described in this paper are implemented in the same processing framework to ensure a fair performance comparison. Our available input signals are the reference FE signal $x(n)$ and the microphone signal $y(n) = s(n) + n(n) + d(n)$ with NE speech $s(n)$, background noise $n(n)$, and echo $d(n) = f_\mathrm{NL}\left(x(n)\right) * h(n)$, with loudspeaker nonlinearity $f_\mathrm{NL}(\cdot)$ and room impulse response $h(n)$. All signals are sampled at $16$\,kHz. The signals $x(n)$ and $y(n)$ are split into frames of $N_T = 424$ samples and a frame shift of $50$\,\%, which are then subject to a square root Hann window and zero-padded so that a $K=512$-point DFT can be applied. The resulting frequency-domain representations $X_\ell(k)$ and $Y_\ell(k)$ with frame index $\ell$ and frequency bin index \mbox{$k \in \mathcal{K} = \{0,1,...,K/2\}$} are used as inputs to our DNN models. Both real and imaginary part form separate input channels and are zero-padded to $L\ge K/2+1$ to create a feature count divisible by the product of all encoder strides. The models are trained to estimate a mask $M_\ell(k)$ which is applied to $Y_\ell(k)$ after gain amplitude compression according to
\vspace{-1mm}
\begin{equation}
    E_\ell(k) = Y_\ell(k) \cdot \tanh(|M_\ell(k)|) \cdot\frac{M_\ell(k)}{|M_\ell(k)|}.
    \label{eq:mask}
    \vspace{-1mm}
\end{equation}
The enhanced signal $E_\ell(k)$ is transformed back into the time domain using a $K$-point IDFT, square-root Hann-windowed ($N_T$ samples window length), and subject to overlap-add (OLA), resulting in the time sequence $e(n)$.

We compare our proposed model to two baselines architectures: The {\tt FCRN} \cite{seidel21_interspeech} as a large, high-quality model, and the {\tt CRUSE DAEC-64} architecture \cite{Braun2022} including input amplitude compression (further simply {\tt CRUSE}), as an efficient model with low parameter count and computational complexity.

\subsection{Proposed Efficient Architecture}
\label{ssec:architecture}

% Figure environment removed

% Our proposed model, created in the {\tt Tensorflow2} framework \cite{}, is based on the FCRN model \cite{}. 
While the {\tt FCRN} yields good quality, it has a high parameter count and computational complexity. As it is used in various fields of speech enhancement, more efficient variants have been developed, but not yet evaluated on the task of AES. We adapt the {\tt FCRN15} structure described in \cite{Strake2022, Strake2020a}, shown in Fig.\,\ref{fig:xCRN}. Every other encoder layer features a stride of 2 (labelled as $_{/2}$) to reduce the feature map size, which is restored using deconvolutional layers in the decoder. Depth-wise convolutions are employed in the skip connection paths. The recurrent bottleneck (grey box) is represented by two sequential ConvLSTM layers of kernel size $(N \times 1)$ and kernel count $F$, resulting in $C_\mathrm{rb}=F$.
For the use in AES, we concatenate real and imaginary parts of both input signals $X_\ell(k)$ and $Y_\ell(k)$ before passing them to the first layer ($C_\mathrm{in}=4$). We choose $L=264$, $N=12$, and $F=32$. This model already decreases the required floating point operations per second (FLOPS) by a factor of six compared to the {\tt FCRN}, but is still quite slow in training and inference. We propose further improvements to the {\tt FCRN15}:

\circled{1} \textbf{Grouped GRUs}: Replacing the \mbox{ConvLSTM} layers in the bottleneck of the {\tt FCRN15} with GRU layers massively improves training speed, but considerably increases the parameter count due to the fully connected nature of GRUs. To counteract this, we add a convolutional layer of kernel count $F$ in the beginning of the \mbox{bottleneck} and split its output feature maps between multiple parallel GRUs. This first model uses two consecutive layers of $8$ and $6$ grouped GRUs (gGRUs), respectively, and representation rearrangement~\cite{Tan2020}. A convolutional layer with kernel count $3F$ after the gGRU layers restores the input dimensions.

\circled{2} \textbf{Kernel count}: We found that increasing the kernel count from $F = 32$ to $F = 40$ improved the model's performance notably at the cost of a moderately increased computational complexity. To split our channels in the recurrent bottleneck uniformly, the first gGRU layer now consists of $10$ (instead of $8$) GRUs.

\circled{3} \textbf{Kernel size}: We found that a reduction of the kernel size did not negatively affect the models performance down to $N=3$, which reduces the computational complexity of our encoder and decoder by $75$\,\%.

\circled{4} \textbf{Input compression:} We adopt the input compression from \cite{Braun2022} defined as 
$Y^\mathrm{compressed}_\ell(k) = |Y_\ell(k)|^c \cdot e^{j\varphi_{Y,\ell}(k)}$
with $c=0.3$ for both model inputs $Y_\ell(k)$ and $X_\ell(k)$. Note that this is only applied to the encoder inputs, and that we do not compress or decompress any signals of the masking operation (\ref{eq:mask}). We found this practice to improve our model's performance in double-talk.

\circled{5} \textbf{GRU layer simplification}: As a last step to create our proposed model, further labelled as {\tt gGCRN16}, we remove the second gGRU layer, causing only slightly reduced performance. The resulting recurrent bottleneck is depicted in Fig.\,\ref{fig:gGCRN16}. This step greatly reduces our model's parameter count. Compared to the {\tt FCRN15}, we reduce FLOPS by 70\%. Compared to {\tt CRUSE}, we require 30\% less parameters and 15\% less FLOPS.

% Figure environment removed

\subsection{Proposed Condition-Aware Training and Losses}
\label{ssec:condition}

To implement a condition-aware training, we use specific minibatch condition splits (MCSs). This means that each minibatch contains a given number of sequences for each of the conditions double-talk (DT), farend single-talk (STFE), and nearend single-talk (STNE). By changing the proportion of each condition in a batch, we can better control our model's training behaviour. The representation of all used training conditions in each batch mitigates the danger of detrimental weight updates caused by an imbalanced MCS, e.g., a minibatch only containing STFE and no echo.

The knowledge of each minibatch entry's specific condition can also be leveraged to apply a condition-aware loss formulation which makes use of the white-box speech component \mbox{$\tilde{S}_\ell(k) = G_\ell(k) \cdot S_\ell(k)$} and residual echo component \mbox{$\tilde{D}_\ell(k) = G_\ell(k) \cdot D_\ell(k)$} by applying our estimated mask $G_\ell(k)$ to the respective microphone signal components.
% \begin{align}
% \tilde{S}_\ell(k) & = G_\ell(k) \cdot S_\ell(k) \\
% \tilde{D}_\ell(k) & = G_\ell(k) \cdot D_\ell(k).
% \label{eq:wb_comp}
% \end{align}
Using these components in the time domain, the condition-aware loss for each minibatch entry $b$ is computed as
\begin{align}
\begin{split}
J^\mathrm{CA}_b = (1-\alpha_b-\beta_b) J^{\mathrm{logMSE}}_b \big({e}(n), {s}_b(n) + {n}_b(n)\big) \\  
\ \ + \alpha_b J^{\mathrm{logMSE}}_b \big(\tilde{s}_b(n), {s}_b(n)\big) + \beta_b J^{\mathrm{logMSE}}_b \big(\tilde{d}_b(n), {0}\big),
\end{split}
\label{eq:cda_loss}
\end{align}
where $\alpha_b$ and $\beta_b$ are weighting factors of their respective loss terms. These weights are specific to each individual batch entry based on its condition (DT, STFE, STNE). The loss
% \vspace{-1mm}
\begin{align}
    {J}^{\mathrm{logMSE}}_b(\hat{{z}}_b(n),\,\, \overline{z}_b(n)) & = 10\!\cdot\!\log \left(\sum_{n \in \mathcal{N}} \big(\hat{{z}}_b(n),\,\, \overline{z}_b(n)\big)^2  \right) \label{eq:tlmse}
    % \vspace{-1mm}
\end{align}
computes the log MSE over the entire time sequence $(n \in \mathcal{N})$ with target $\overline{z}_b(n)$ and its estimate $\hat{z}_b(n)$. Note that the target for the last term in (\ref{eq:cda_loss}) is a sequence of zeros, as we want our residual echo power to be minimized. Such a loss formulation can be used either directly in the training or alternatively in a fine-tuning step. We focus on the application in fine-tuning, since both methods lead to similar results, but fine-tuning allows for faster ablations and proved more stable for higher values of $\alpha_b$ and $\beta_b$.
 
\section{Experimental Evaluation and Discussion}
\label{sec:setup}

\subsection{Dataset and Metrics}
\label{ssec:dataset}

The \textit{training dataset} $\mathcal{D}_{\mathrm{train}}$ uses speakers from the CSTR-VCTK corpus~\cite{VCTK} to generate NE and FE utterances. Each speaker can appear as NE or FE to discourage the trained model from overfitting to speakers. To generate the echo signal, first a scaled error function (SEF)~\cite{Zhang2020c,Klippel2006} defined as 
$f_\mathrm{SEF}(x(n)) = \int_{0}^{x} exp\left({\frac{x^2(n)}{2\mu^2}}\right) dx$
is applied to the FE reference signal $x(n)$, with $\mu$ randomly chosen from $\{0.5, 1, 10, 999\}$. This simulates possible non-linear distortions caused by the loudspeaker. The resulting distorted signal is convolved with an impulse response (IR) generated via the image method \cite{image_method}. 
The signal-to-echo ratio (SER) of each audio file is randomly sampled from a continuous range of $[-12.4,22.4]$\,dB. For background noise we use random cuts from the DEMAND~\cite{Thiemann2013} and QUT-NOISE~\cite{QUT} databases at a signal-to-noise ratio (SNR) in the range of $[-2.4, 32.4]$\,dB, with a $10$\% chance of a file to be noiseless. We generate $9500$ files of $10$\,s length, of which $1000$ files are only used for validation and learning rate control during training. Between training epochs, the speech, noise, and echo components of $\mathcal{D}_{\mathrm{train}}$ (except the validation split) are reshuffled with new SER and SNR values.

For preliminary evaluations we use a \textit{development dataset} $\mathcal{D}_{\mathrm{dev}}$ of 200 files. Speakers are drawn from a speaker subset of the CSTR-VCTK corpus disjoint to the training speakers. Nonlinear distortions are again simulated via $f_\mathrm{SEF}(x(n))$, but with distinct $\alpha$ values sampled from $\{0.2, 0.4, 1.5, 12, 999\}$. The image method is used for IR generation with the same parameter setup as in $\mathcal{D}_{\mathrm{train}}$, but different random seeding. SER values are sampled from discrete values in $\{-10, -5, ..., 20\}$\,dB. Noise is cut from the ETSI database~\cite{ETSI2008}. The SNR is chosen from $\{0, 5, ..., 30\}$\,dB.
Contrary to $\mathcal{D}_{\mathrm{train}}$, each file contains distinct sections of all three conditions in the order \mbox{STFE, STNE, DT}. All sections are of $8$ to $12$\,s length.

The \textit{test dataset} $\mathcal{D}_{\mathrm{test}}$ is created from speakers of the TIMIT speech corpus~\cite{TIMIT}. The arctan nonlinearity function~\cite{Jung2013, Zoellzer2003} defined as
$f_\mathrm{arctan}(x(n)) = {\arctan{(\alpha \cdot x(n))}}/{\alpha}$ models the loudspeaker nonlinearites, with \mbox{$\alpha=10^{-4}$}. For IR generation we use the image method, but adjust its parameters to include bigger room dimensions, more variance in speaker / loudspeaker distance and position, and different reflection coefficients. Noise is cut from the remaining ETSI files of environments unseen in the training. The SER is chosen from $\{-9, -6, ..., 9\}$\,dB, the SNR from $\{5, 8, ..., 20\}$\,dB. We generate 200 files in the same fashion as for $\mathcal{D}_{\mathrm{dev}}$.

All three conditions are evaluated on their own subset of metrics. Metrics used in this work include PESQ~\cite{ITU_P862.2_Corr}, the ERLE metric 
% \cite{Vary2006}
% defined as $ERLE(n)=10\mathrm{log}_{10}(d^2(n)/(d^2(n)-\hat{d}(n))^2)$,
% for which a first-order IIR smoothing filter with factor $\alpha=0.99$ is applied to the  signals $d^2(n)$ and $d_\Delta^2(n)=(e(n)-n(n))^2$,
after \cite{Vary2006} with a first-order IIR smoothing filter ($\alpha=0.99$) and the STFE echo estimate defined as $\hat{d}(n)=e(n)-n(n)$,
and the \mbox{AECMOS} metrics~\cite{purin2021aecmos}.
To observe the trade-off between NE speech preservation and echo suppression in the fine-tuning ablations, we use the black-box component metrics PESQ$_\mathrm{BB}$ and ERLE$_\mathrm{BB}$ as described in \cite{fingscheidt_signalseparation, seidel21_interspeech}.

\begin{table}[t]
	\setlength{\tabcolsep}{.35em}
    \vspace{-2mm}
	\caption{Effects of \textbf{condition-aware} \textbf{training} on {specific minibatch condition splits (MCSs)} DT/STFE/STNE, evaluated on the \textbf{dev set} $\mathcal{D}_{\mathrm{dev}}$. Best results are {bold}, second best {underlined}.}
	\vspace{0.5mm}
    \centering
	\input{tables/TABLE_dist_dev.tex}
    \vspace{-3mm}
	\label{tab:results_dist_dev}
\end{table}


\begin{table*}[t]
	\setlength{\tabcolsep}{.35em}
	\caption{Proposed modifications to the {\tt FCRN} architecture up to the {\tt gGCRN16} as described in Section \ref{ssec:architecture}, evaluated on the \textbf{dev set} $\mathcal{D}_{\mathrm{dev}}$. All models are trained on \mbox{MCS (16/0/0)}. Best results are {bold}, second best {underlined}. \colorbox{Gray}{Our proposed model architecture is highlighted in grey.}}
	\vspace{0.5mm}
    \centering
	\input{tables/TABLE_arch_dev.tex}
    \vspace{-3mm}
	\label{tab:results_arch_dev}
\end{table*}

\subsection{Training Details}
\label{ssec:training}

% Publications on the field of learned AEC use various often not clearly motivated compositions of training conditions and usually combine the training task with joint noise reduction (NR). There are only few examples where an AEC-only model is explicitly trained \cite{,}. We choose a random MCS concerning DT / STFE / STNE as starting point for our experiments. Although we are aware that such an equal distribution of the three distributions is not standard in literature, it serves as a good reference for the performance difference achieved through condition-aware training.

We use the Adam optimizer~\cite{adam_optimizer} in its standard configuration for model training. Apart from the fine-tuning ablations on condition-aware losses, all models are trained on loss (\ref{eq:cda_loss}) with $\alpha_b = \beta_b = 0$. The batch size is set to $16$ with a backpropagation-through-time (BPTT) unrolling sequence length of $200$ frames. 
%Between epochs, the components of $\mathcal{D}_{\mathrm{train}}$ are reshuffled with new SER and SNR values.

The initial learning rate (LR) is set to $10^{-4}$, which is halved after 4 epochs without loss improvement on the validation split of $\mathcal{D}_{\mathrm{train}}$. The training is stopped after 100 epochs, if the validation loss does not improve for 10 consecutive epochs, or if the LR drops below $10^{-5}$. The fine-tuning ablations are trained for 30 epochs using the same LR strategy with an initial LR of $2.5\cdot10^{-5}$ and minimal LR of $2.5\cdot10^{-6}$.
All models are trained in {\tt TensorFlow2}~\cite{Abadi2016}.
% All models are trained on a {\tt GTX 1080 Ti} and training runs are implemented in a deterministic fashion to avoid performance variations, e.g., from inconsistent data randomization or non-deterministic CUDA operations.

% \section{Results and Discussions}
% \label{sec:results}

% \begin{table}[t]
% 	\setlength{\tabcolsep}{.35em}
%     \vspace{-2mm}
% 	\caption{Evaluation of compared model architectures on the \mbox{\textbf{test set} $\mathcal{D}_{\mathrm{test}}$} with MCS (16/0/0). Best network results are {bold}, second best {underlined}.}
% 	\vspace{0.5mm}
% 	\input{tables/TABLE_comp_test.tex}
%     \vspace{-3mm}
% 	\label{tab:results_comp_test}
% \end{table}

% \subsection{Evaluation Metrics}
% \label{ssec:metrics}

% As explained in Section \ref{ssec:dataset}, the DT, STFE and STNE sections in our dev and test set files are evaluated independently on their own subset of metrics. For evaluation of echo suppression effectiveness, we use the AECMOS \cite{} metrics DT\,Echo and ST\,Echo, and the mean echo return loss enhancement (ERLE) defined as $ERLE(n)=10\mathrm{log}_{10}(d(n)/e(n))$, for which a first-order IIR smoothing filter with factor $\alpha=0.99$ is applied to the  signals $d(n)$ and $e(n)$. The NE speech preservation is measured through PESQ \cite{} and the AECMOS \cite{} metrics DT\,Other and ST\,Other. Note that the upper achievable score limit on these 3 metrics is dependent on the present noise level since our models are not trained to remove any noise. For example, perfect echo suppression on the dev set would yield a DT PESQ score of \textcolor{red}{2.XX} MOS points.
% For the fine-tuning ablations, we use the black-box component metrics PESQ$_\mathrm{BB}$ and ERLE$_\mathrm{BB}$ as published in \cite{,}, which allow us to better determine the trade-off between NE speech preservation and echo suppression.

\subsection{Development Set Ablations}
\label{ssec:dev}

Table \ref{tab:results_dist_dev} shows the effects of different MCSs as the specific number of files for each processed minibatch (DT / STFE / STNE), evaluated on the {\tt gGCRN16} and for selected MCSs also on both baseline models {\tt FCRN}~\cite{seidel21_interspeech} and {\tt CRUSE}~\cite{Braun2022}. We slowly shift the MCS towards DT-only (16/0/0), as DT is the most challenging condition and therefore the most valuable in training. One additional experiment is conducted with MCS (8/8/0). This resembles the distribution of the synthetic dataset provided for the INTERSPEECH 2021 AEC Challenge, which used files consisting of $10$\,s of FE audio and zero-padded NE audio of $3$-$7$\,s length \cite{cutler2022AEC}.

{\it We can see that the DT condition metrics on $\mathcal{D}_{\mathrm{dev}}$ mostly improve with higher proportion of DT in the training set, while the STFE condition performance only slightly benefits from more STFE representation in training} (e.g., ST Echo from $4.44$ to $4.50$ MOS points on the {\tt gGCRN16}). While no model shows particularly good performance on the random MCS training, the {\tt gGCRN16} with its low computational complexity seems to be more sensible to this training configuration. Note that these findings also hold true on $\mathcal{D}_{\mathrm{test}}$. {\it Interestingly, the lack of STNE training data in (n/m/0) MCSs did not lead to any considerable degradation of the near-end speech during single-talk on neither $\mathcal{D}_{\mathrm{dev}}$ nor $\mathcal{D}_{\mathrm{test}}$.} 

Table \ref{tab:results_arch_dev} shows the proposed modifications to the {\tt FCRN} model as detailed in Section \ref{ssec:architecture}. We also include our implementation of the {\tt CRUSE} as a reference for an efficient architecture. Based on the findings of Table \ref{tab:results_dist_dev}, we train all models on MCS (16/0/0).

These results show the effectiveness of our architectural improvements. The performance stays largely consistent to the original architecture. Our proposed steps repair the initial performance drop of the {\tt FCRN15} architecture in DT while further reducing computational complexity (70\% less). We observe a slight performance degradation caused by the removal of the second gGRU layer in our final proposed {\tt gGCRN16} model, which we see as a reasonable trade-off for the huge savings in parameter count. Furthermore, we outperform the {\tt CRUSE} architecture in model size (-30\%), computational complexity (-15\%), and performance. Again, STNE performance remains at a high level with negligible variations.

\subsection{Test Set Results and Discussion}
\label{ssec:test}

% Figure environment removed

Fig.\,\ref{fig:complexity_plot} shows various tested model architectures evaluated by the AECMOS metrics. As before, we only compare models trained on the same MCS (16/0/0). We include the frequency domain Kalman filter (FDKF) after \cite{Franzen2018a} without their postfilter as a classical AEC system of negligible computational complexity and model size.

We can see that compared to the baseline {\tt FCRN}, the {\tt gGCRN16} architecture greatly reduces the parameter count (65\% less) and FLOPS (95\% less) while delivering high DT performance. While ST\,Echo degrades a bit, the STFE echo suppression still stays on a high level where the remaining residual echo should be easily removable by a postfilter. {\it We again outperform or are on par with the {\tt CRUSE} architecture in all metrics.} As with the dev set, all models show next-to-perfect performance on the STNE condition with only marginal deviations on $\mathcal{D}_\mathrm{test}$ as well. All DAEC models considerably outperform the FDKF in DT.

Table \ref{tab:results_tune_test} shows the exemplary impact of two fine-tunings with condition-aware component losses after (\ref{eq:cda_loss}). All shown models are fine-tuned, either using the standard training loss ($\alpha_b = \beta_b = 0$) or context-aware losses (marked by the identifier CA). The specific weight values for context aware-losses are defined as follows: For (15/1/0), we choose $\alpha_\mathrm{DT}=0.2$, $\beta_{\mathrm{DT}}=0.2$, $\alpha_{\mathrm{STFE}}=0.2$, $\beta_{\mathrm{STFE}}=0$, for (16/0/0) we choose $\alpha_{\mathrm{DT}}=0.33$, $\beta_{\mathrm{DT}}=0.0$. 

\begin{table}[t]
	\setlength{\tabcolsep}{.35em}
    \vspace{-3.5mm}
	\caption{Evaluation of fine-tuning {\tt gGCRN16} on two MCSs, using either the standard training loss defined in Section \ref{ssec:training} or condition-aware component losses (CA) defined in (\ref{eq:cda_loss}) on the \textbf{test set} $\mathcal{D}_{\mathrm{test}}$. Best results are {bold}, second best {underlined}.}
	\vspace{0.5mm}
    \centering
	\input{tables/TABLE_tune_test.tex}
    \vspace{-2.5mm}
	\label{tab:results_tune_test}
\end{table}

We see that the main distinction of the models' behaviour can be observed in the component metrics PESQ$_\mathrm{BB}$ and ERLE$_\mathrm{BB}$. The fine-tuned model of (15/1/0) leans more towards aggressive echo suppression, while the fine-tuned (16/0/0) model preserves more NE speech at the cost of a weaker AES. This trade-off control can be leveraged to adjust the model performance towards a specific task. For example, we could tune a model to remove just as much echo as necessary for a postfilter to reliably suppress residual echo without sacrificing NE speech too much.


% \begin{table}[t]
% 	\setlength{\tabcolsep}{.35em}
% 	\caption{Files per batch drawn from the respective condition for trained gGCRN16 variations. Model 1 draws conditions evenly from each condition at random.}
% 	\vspace{0.5mm}
%     \centering
% 	\input{tables/TABLE_distDEF.tex}
%     \vspace{-3mm}
% 	\label{tab:distDef}
% \end{table}

% \begin{table}[t]
% 	\setlength{\tabcolsep}{.35em}
% 	\caption{Components loss weights used for gGCRN16 fine-tuning.}
% 	\vspace{0.5mm}
%     \centering
% 	\input{tables/TABLE_weights.tex}
%     \vspace{-3mm}
% 	\label{tab:weights}
% \end{table}

% \begin{table}[b]
% 	\setlength{\tabcolsep}{.35em}
% 	\caption{Research: on $\mathcal{D}_{\mathrm{test}}$}
% 	\vspace{0.5mm}
% 	\input{tables/TABLE_dist_test.tex}
%     \vspace{-3mm}
% 	\label{tab:TABLE_dist_test}
% \end{table}

\section{Conclusions}
\label{sec:results}

This work demonstrates the importance of a high DT proportion in the DAEC training data.
We further present modifications to the {\tt FCRN15} architecture that not only reduce computational complexity by more than 70\%, but also improve DT performance. The final model also outperforms the {\tt CRUSE DAEC-64} model, although it has 30\% less parameters and 15\% less computational complexity. 
The proposed condition-aware loss allows further control on the trade-off between NE preservation and echo suppression.


\newpage



% \begin{equation}
%   \label{eqn:wave_equation}
%     \Delta^2p(x,y,z,t)-
%     \displaystyle\frac{1}{c^2}\frac{\partial^2p(x,y,z,t)}{\partial t^2}=0,
% \end{equation}


% -------------------------------------------------------------------------
\bibliographystyle{IEEEtran}
\bibliography{refs, ifn_spaml_bibliography}

\end{sloppy}
\end{document}
