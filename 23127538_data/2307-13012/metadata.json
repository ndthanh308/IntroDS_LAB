{
  "title": "Joint speech and overlap detection: a benchmark over multiple audio setup and speech domains",
  "authors": [
    "Martin Lebourdais",
    "Th√©o Mariotte",
    "Marie Tahon",
    "Anthony Larcher",
    "Antoine Laurent",
    "Silvio Montresor",
    "Sylvain Meignier",
    "Jean-Hugh Thomas"
  ],
  "submission_date": "2023-07-24T14:29:21+00:00",
  "revised_dates": [],
  "abstract": "Voice activity and overlapped speech detection (respectively VAD and OSD) are key pre-processing tasks for speaker diarization. The final segmentation performance highly relies on the robustness of these sub-tasks. Recent studies have shown VAD and OSD can be trained jointly using a multi-class classification model. However, these works are often restricted to a specific speech domain, lacking information about the generalization capacities of the systems. This paper proposes a complete and new benchmark of different VAD and OSD models, on multiple audio setups (single/multi-channel) and speech domains (e.g. media, meeting...). Our 2/3-class systems, which combine a Temporal Convolutional Network with speech representations adapted to the setup, outperform state-of-the-art results. We show that the joint training of these two tasks offers similar performances in terms of F1-score to two dedicated VAD and OSD systems while reducing the training cost. This unique architecture can also be used for single and multichannel speech processing.",
  "categories": [
    "cs.SD",
    "cs.AI",
    "cs.NE",
    "eess.AS",
    "eess.SP"
  ],
  "primary_category": "cs.SD",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.13012",
  "pdf_url": null,
  "comment": null,
  "num_versions": null,
  "size_before_bytes": 382327,
  "size_after_bytes": 123318
}