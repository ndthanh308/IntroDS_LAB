
\section{Analysis}
\label{sect:5_analysis}

%The previous section has shown that a joint VAD+OSD system can reach similar or better performance than two dedicated systems on several speech domains.
This section evaluates the benefits of such an approach in terms of training time, speech domains, and spatial information in the multi-channel scenario.
%The error distribution according to speech domain and insights about the spatial information used in the multiple channel scenario are also presented.
\vspace{-5pt}
\subsection{Training time}

In order to assess the value of training a joint VAD+OSD system against two dedicated models, we compare the training time required for each approach.
Each system is trained on an RTX6000 GPU card until it reaches its best F1-score on the validation set.
Figure \ref{fig:train_time_chart} presents the elapsed time to obtain the best-performing model.

% Figure environment removed

2-class OSD task clearly requires more resources than VAD.
%, probably because of the difficulty of the OSD task. 
Indeed the discrimination of the spectral information between the presence of one speaker or several speakers is more difficult than between speech and non-speech signals.
Multi-channel VAD+OSD system converges as fast as the 2-class VAD system, as observed on the AMI$^\star$ and CHiME-5$^\star$ datasets.
In the single channel scenario, the gain is less significant (and no gain at all with DIHARD), probably because the spatial information helps to detect multiple speakers.

\subsection{Influence of the speech domain on performance}

In order to study the influence of the speech domain on OSD, we analyze the OSD F1-score distributions for each of the DIHARD evaluation files, manually separated into 7 domains (see Fig.~\ref{fig:domains}). \textit{Clinical} contains conversations between a clinician and a child, \textit{facetoface} contain interviews, \textit{phone} contains phone conversations, \textit{map task} contains a game in which someone guides a person remotely on a map, \textit{group chat} contains spontaneous conversations, \textit{court} contains court recordings and \textit{audiobook} contains read speech.

% Figure environment removed

Fig.~\ref{fig:domains} shows that the F1-score is globally better for \textit{phone} conversations than for \textit{clinical} and \textit{face-to-face} conversations, despite the fact that the three domains are dyadic interactions. 
We can then hypothesize that the absence of visual cues in phone conversations limits the diversity of overlaps contained in the audio files. 
%The \textit{map task} has a similar distribution as \textit{phone} conversations while showing slightly better results. 
%This shows that the acquisition modality influences the type of overlaps distribution. %created and recognized. 
Another difference between domains is the quality of the recordings. 
For example, \textit{group chat} and \textit{face-to-face} files 
%all have a lot of background noises and poorly recorded segments.
feature strong background noise and low-quality recordings, which could explain the low performance obtained in these domains.
This analysis concludes that the speech domain is of major importance for OSD.
The presence of noise, the diversity of overlaps, and the differences in turn-taking driven by the speech domain is clearly a major issue for OSD.\\ 

\vspace{-10pt}
\subsection{Spatialisation}

In the CHiME-5 dataset, the rooms where participants are located are annotated for each utterance in the evaluation set.
We can thus study which microphone the SACC feature extractor activates as a function of the speakers' positions.
Since the VAD+OSD system is trained using one microphone per array in the CHiME data, we can visualize the combination weights for each array in each room.
Two arrays are located in the kitchen, two are located in the dining area and two are in the living room.
Figure~\ref{fig:spat} shows the SACC combination weights of each channel depending on the location of the speakers.
On these utterances, the SACC system mostly activates the channels placed in the areas where speakers are located.
The system seems able to select microphones with the most information for the VAD+OSD task.
An in-depth study should however be conducted to better assess the information used by the system in the multiple-channel scenario.

% Figure environment removed

