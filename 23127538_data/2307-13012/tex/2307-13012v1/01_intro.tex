\section{Introduction}
Speaker diarization answers the question \textit{Who spoke and when?} in an audio stream. 
Today, this task remains difficult as shown by the numerous challenges recently organized \cite{ryant19_interspeech,yu2022m2met_short,iberspeech}.

Given an audio stream, speaker diarization pipelines generally address speech segmentation and speaker clustering in two distinct stages~\cite{ryant19_interspeech}.

Therefore, robust speech segmentation - mainly Voice Activity Detection (VAD) and Overlapped Speech Detection (OSD) - is essential to improve speaker diarization performance as shown in previous studies~\cite{garcia_perera_speaker_2020_short,bullock_overlap-aware_2020}.
VAD consists in segmenting an audio signal into speech and non-speech segments.

Several approaches have been proposed in the literature such as signal processing methods \cite{ghaemmaghami2010noise}, statistical models \cite{nemer2001robust}, and neural-based approaches \cite{lavechin_end--end_2020}.
OSD detects segments in which at least two speakers are simultaneously active.
Early studies mostly focus on statistical models \cite{boakye_improved_2011,yella2014overlapping} while recent approaches are mostly based on neural networks 
\cite{bullock_overlap-aware_2020,andrei2017detecting,lebourdais22_interspeech} and show promising results. 

While VAD and OSD have mainly been considered as two independent binary classification tasks,
they can be addressed jointly by considering three classes -- non-speech, single speaker, and overlapped speech -- according to the number of present speakers in each speech segment.
%recent studies have shown that they can be jointly trained as a 3-class classification problem. 
In \cite{jung21_interspeech}, such a 3-class problem is solved by training a recurrent convolutional network.
The use of far-field microphones and a Self-Attention Channel Combinator (SACC) feature extractor~\cite{mariotte22_interspeech} revealed the potential of spatial information for OSD.
\cite{Cornell2022} demonstrated that Temporal Convolutional Network (TCN) is well adapted for multiple-speaker activity detection with far-field microphones.
%Their following work \cite{Cornell2022} showed that joint VAD+OSD can be performed using this architecture under distant speech conditions.

%However, to the best of our knowledge, these approaches have not been tested on various speech domains including multi-channel recordings.

%These two tasks can be formulated as a classification problem with different targets. 
%it must then be possible to classify these 2 tasks at the same time.
%As presented earlier, the joint training presented, provided that the results obtained are on par with the single task model can be use, as well as an adaptation to multiple domains.
%Moreover, these pre-processing tasks have to be trained from scratch or at least fine-tuned on each dataset used to obtain optimal results. 
%To reduce the time cost of such training, the joint-training of multiple of theses tasks~\cite{jung21_interspeech,malek2020voice} as well as models trained on multiple speech domains can be used to get a robust pre-processing pipeline.\\


%In this paper, we propose to extend the OSD systems developed for mono \cite{lebourdais22_interspeech} and multi-channel \cite{mariotte22_interspeech} with a 2-class VAD and a 3-class VAD+OSD classifier.
In this paper, we propose two 2-class VAD and OSD and 3-class VAD+OSD for mono and multi-channel signals.
We evaluate how beneficial is the 3-class approach in comparison to the use of two independent VAD and OSD models in terms of F1-score and training resources.
%The performance of the 3-class system is compared to the separated 2-class VAD and OSD models.
Each system is trained and evaluated on four different datasets covering various speech domains including both single and multiple microphone scenarios.
%To the best of our knowledge, no work in the literature has benchmarked these approaches across various speech domains and recording setups (multi/mono channel).
To the best of our knowledge, no benchmark has been conducted on these approaches across various speech domains and recording setups (multi/mono channel) in the literature.
%are considered for our experiments.
%
%A detailed analysis of the VAD+OSD system is also proposed showing the benefits of joint training in terms of training resources.
%
%Experiments are conducted over different speech domains such as media or meetings.
%This paper presents a benchmark of speaker diarization preprocessing tasks with a robust joint VAD and OSD system trained and evaluated on multiple speech domains. Using this system we intend to investigate a potential time save on the training of these system, as well as cross-domain models.
%Single-channel and multiple-microphone scenarios are considered depending on the dataset.
%
%The paper is organized as follows: Sec.~\ref{sect:2_datasets} and \ref{sect:3_system} present the different benchmark datasets and the architecture of each neural system. 
%Sec.~\ref{sect:4_results} presents classification performances of each system while the last Section~\ref{sect:5_analysis} proposes an in-depth analysis of the different systems.

This paper presents several contributions: It first claims a new state of the art for OSD on multiple corpora, it introduces a benchmark on 4 different datasets covering various speech domains in multi/mono channel scenarios and presents a reduction of the training cost using a 3-class approach with a detailed analysis of the benefits of this system.




%Since VAD is a crucial task for most automatic speech processing systems, many research works have been conducted on this task.
%While first methods were focused on signal processing \cite{chengalvarayan1999robust,woo2000robust,ghaemmaghami2010noise} and statistical methods \cite{sohn1999statistical,pfau2001multispeaker,ng2012developing}, neural-based systems have shown impressive results on VAD \cite{}


%The side effect of these pipeline architectures is the error propagation from the first processing stages %(i.e. VAD,OSD...) 
%to the latter subtasks.
%Thus, segmentation errors may have a direct impact on the diarization accuracy and performances. %Diarization Error Rate (DER). 
%This pipeline architecture has as side effect the propagation of errors, inherent to the first processing into latter stages.
%A mistake in the original segmentation might directly impact the Diarization Error Rate (DER). 

%Pipeline-based speaker diarization architectures rely on automatic segmentation and speaker clustering. 
%Robust speech segmentation is thus needed to improve speaker diarization.
%To retrieve the speaker information at a frame level, automatic speech processing task mostly rely on speech segmentation and the clustering of these segments in order to group them by speaker.
%Many diarization systems rely on pipeline architectures \cite{}. 
%A segmentation system extracts the single speaker segments from the audio stream.
%Then, speaker clustering is applied on speaker embeddings extracted from speech segments. 
%Robust segmentation is thus needed to improve speaker diarization performance.


%In basic scenarios, i.e. single speaker speech segments with a studio audio quality, this task is nearly solved, as very accurate systems already exist~\cite{}. 
%The problem lies in muddier situation, with lots of background noise or multiple speakers at the same time.
%However, speaker diarization performances are mitigated under realistic conditions where background noise, variable number of speakers or music may appear.
%For these reasons, speaker diarization is still considered as a challenging task as shown by numerous challenges organized about it \cite{ryant2019second,ryant2021dihard,yu2022m2met,iberspeech}.

%Several approaches have been proposed in the literature such as signal processing methods \cite{chengalvarayan1999robust,ghaemmaghami2010noise}, statistical models \cite{sohn1999statistical,nemer2001robust} and recently neural-based approaches \cite{gelly2017optimization,lavechin2019end}.