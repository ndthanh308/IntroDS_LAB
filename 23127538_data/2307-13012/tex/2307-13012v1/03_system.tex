\vspace{-5pt}
\section{System overview}
\label{sect:3_system}
%The proposed segmentation is composed of a feature extraction layer preceding a sequence modeling network.
%The feature extraction is adapted accordingly with the number of channels in the input signal.
Figure~\ref{fig:archi} depicts an overview of VAD, OSD, and VAD+OSD systems.
While the feature extractor (in blue) is adapted with respect to the number of input channels, the sequence modeling network (in purple) processes the sequence of features before the frame classification.
The frame classification is done at a rate of 100~Hz, while the raw waveform is sampled at 16~kHz.
%In the target sequences each class is given with a frequency of 100~Hz, while the raw waveform is sampled at 16~kHz.
\vspace{-10pt}

% Figure environment removed
\vspace{-10pt}

\subsection{Single channel features ($M=1$)}
The single channel feature extractor is based on the WavLM pre-trained model~\cite{chen_wavlm_2022_short}.
This choice is motivated by the performance obtained on the diarization task according to the SUPERB benchmark~\cite{yang_superb_2021_short}.
Furthermore, WavLM has been trained using simulated overlapped speech and is then more robust to this type of data. 
WavLM outputs speech representations every 20~ms. 
In order to align this representation with the target sequence, we decide to add a linear layer on top of the frozen WavLM.
%Our feature extractor is composed of a freezed WavLM model followed by a linear layer. Our target is sampled with a frequency of 100~Hz, WavLM only output features every 20~ms, a transformation is then needed. 
The linear layer aims to transform a segment of 99 features extracted with WavLM over a 2~s window, into a 200-frame vector, supposedly aligned with our target.


\subsection{Multiple channel features ($M>1$)}

When multiple channels are available, feature extraction is performed using the Self-Attention Channel Combinator (SACC) \cite{gong_self-attention_2021_short}.
This architecture has previously shown its efficiency for OSD under distant speech conditions \cite{mariotte22_interspeech}.
The algorithm consists of a self-attention module \cite{vaswani_attention_2017_short} which computes per-channel weights from the multi-channel Short-Time Fourier Transform (STFT) of the input signal.
The channels are then weighted and combined in order to get a single-channel representation.
Combination weights are computed from the multichannel STFT calculated on 25~ms segments with 10~ms shift.
The attention module is composed of a single attention head of size $d=256$.
The combined representation is converted to the log-mel scale using $N_f=64$ filters.
Global Mean and Variance Normalization (MVN) is also applied before feeding the sequence modeling network.

\vspace{-5pt}

\begin{table*}[ht]
\centering
\caption{Overview of the F1-score (\%) for each system on the evaluation set of several corpora covering various domains, $^\star$ indicates multi-microphone data, $\dagger$ indicates that the results are taken from the original article.}
\setlength\tabcolsep{4.5pt}
\begin{tabular}{@{}rlcccccccccc@{}}

\toprule
            && \multicolumn{5}{c}{VAD} & \multicolumn{5}{c}{OSD} \\ 
            \cmidrule(l){3-7} \cmidrule(l){8-12}\\
            && DIHARD    & ALLIES    & AMI   & AMI$^{\star}$   & CHiME$^{\star}$    & DIHARD    & ALLIES   & AMI   & AMI$^{\star}$ & CHiME$^{\star}$\\ \midrule
%            \cmidrule(l){2-6} \cmidrule(l){7-11}\\
%Baselines   & 63.4   & /      & /      & /           &  \\
\multirow{4}{*}{\rotatebox[origin=c]{90}{2-class}}&
VAD (ours)    & 97.0      &  99.8     & 97.4     & 96.4      & 99.8          & -         & -         & -         & -         & -\\
&OSD (ours)    & -         & -         & -        & -         & -          & \textbf{66.2}      & \textbf{71.6}  & \textbf{79.6}   & \textbf{72.2}      & \textbf{75.9}   \\
& Mel+CRNN~\cite{Cornell2022} & - & - & - & - & - & 51.3 & - & 66.0 & 57.2 & -\\ 
& Mel+TCN~\cite{cornell_detecting_2020} & - & - & - & - & - & 54.7 & - & 73.4 & 65.8 & -\\ \midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{3-class}}&
VAD+OSD (ours) & 97.0      & 89.2     &  97.2     & 96.6     &   99.3      & \textbf{66.8} & \textbf{75.4}      & \textbf{80.4}     & \textbf{71.8}      & \textbf{75.5} \\ 
& Mel+CRNN~\cite{Cornell2022} & - & - & - & - & - & 50.8 & - & 69.6 & 61.2 & -\\ 
& Mel+TCN~\cite{cornell_detecting_2020} & - & - & - & - & - & 54.5 & - & 73.8 & 67.9 & -\\ 
& SincNet+BLSTM~\cite{bredin:hal-03257524}$\dagger$ & - & - & - & - & - & 59.9 & - & 75.3 & - & -\\ \bottomrule
\end{tabular}
\label{tab:overview}
\end{table*}

\subsection{Sequence modeling and classification}

%VAD and OSD tasks rely on the classification of frames.
The sequence modeling network (in purple in Fig.~\ref{fig:archi}) takes as input a sequence $\boldsymbol{x}$ of single or multi-channel features and assigns a class to each frame of this sequence.
%The temporal representations extracted either by single- or multichannel feature extractors are then processed by a sequence modeling network.
%This network learns a representation of the sequence of features and classifies each frame.
This task is performed using a TCN \cite{bai_empirical_2018} since this architecture has shown noticeable results on both VAD and OSD tasks \cite{lebourdais22_interspeech,mariotte22_interspeech,Cornell2022,cornell_detecting_2020}.
It is composed of 5 residual convolutional blocks repeated 3 times.
Classification is performed by a 1-d convolutional layer followed by a $\mathrm{softmax}$ activation function. %to compute classification pseudo-probabilities.
%The global architecture with each configuration is presented in Figure \ref{fig:archi}.

For each frame in the output sequence, the VAD outputs the pseudo-probability to contain at least one speaker $p(N_{spk} \geq 1|\boldsymbol{x})$. 
The OSD outputs the pseudo-probability to contain speech from more than one speaker $p(N_{spk} \geq 2|\boldsymbol{x})$. 
Both VAD and OSD are then binary classifiers.
The joint VAD+OSD system outputs the pseudo-probability of either containing any speech $p(N_{spk}=0|\boldsymbol{x})$, speech from a single speaker $p(N_{spk}=1|\boldsymbol{x})$, or speech from more than one speaker $p(N_{spk}\geq2|\boldsymbol{x})$. 
The 3-class approach is then converted to 2-class VAD and OSD by merging the relevant classes.
%One output is then dedicated to VAD while another one is dedicated to OSD.

\vspace{-5pt}
\subsection{Training and Evaluation}
In order to estimate the robustness over different speech domains, the three systems are trained and evaluated independently on the 5 datasets aforementioned. 
%to evaluate their performance on different speech domains.
%The evaluation in the cross-dataset scenario is not considered and will take part of a future study.
%Segmentation systems are trained on 2 seconds audio segments with associated frame-level labels.
To counteract the small number of overlap segments, 50\% of the training segments are augmented on-the-fly by summing them to another randomly sampled training segment. Associated labels of each segment are also combined \cite{bredin_pyannoteaudio_2020_short}.
The loss function is a cross-entropy, and we used the ADAM optimizer with a learning rate of $lr=10^{-3}$.
Single-channel audio data is augmented with noise extracted from MUSAN~\cite{snyder2015musan} and additional reverberation using simulated room impulse responses.
Preliminary experiments have shown that data augmentation did not bring significant improvement in the far-field scenario. 

%To ensure an adequate proportion of overlap in the training data, artificial overlap is added as a weighted sum of two speech segments.
% the input of each of the channel have also been augmented with noise extracted from MUSAN~\cite{Snyder2015MUSANAM} and reverb by simulated room impulse responses. 

%Each system is evaluated on the test subset of the considered corpus. 
Following the DIHARD evaluation plan, we use the F1-score obtained on the evaluation set as a performance metric.
%The evaluation procedure slightly differs between the 2-class systems -- VAD and OSD -- and joint VAD+OSD.
In the 2-class approach, only the positive class output ($N_{spk} \geq 1$ for VAD, and $N_{spk} \geq 2$ for OSD) is used for prediction and two detection thresholds are applied to predict binary labels \cite{bredin_pyannoteaudio_2020_short}.
In the 3-class approach, the class associated with the maximum $\mathrm{softmax}$ output is selected at the frame level. A working version of the code will soon be released\footnote{Hidden link for anonymous submission}. %\url{https://git-lium.univ-lemans.fr/speaker}
