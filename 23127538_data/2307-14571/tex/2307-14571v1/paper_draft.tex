%% 
%% Copyright 2007-2020 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 

%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%%
%% 
%%
%% $Id: elsarticle-template-num.tex 190 2020-11-23 11:12:32Z rishi $
%%
%%
% \documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
\documentclass[times]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{commath}
%\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\journal{Pattern Recognition}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%% \fntext[label3]{}

\title{Robust Detection, Assocation, \& Localization of Vehicle Lights: A Context-Based Cascaded CNN Approach \& Evaluations}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \affiliation[label1]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%%
%% \affiliation[label2]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}

\author[1]{Akshay Gopalkrishnan\corref{cor1}} 

\cortext[cor1]{Corresponding author:}
  %Tel.: +0-000-000-0000;  
  %fax: +0-000-000-0000;}
\ead{agopalkr@ucsd.edu}

\author[1]{Ross Greer}
\author[1]{Maitrayee Keskar}
\author[1]{Mohan M. Trivedi}



\affiliation[1]{organization={University of California San Diego},
                addressline={Laboratory for Intelligent \& Safe Automobiles}, 
                city={La Jolla}, 
                %postcode={92092}, 
                state={California},
                country={USA}}


\begin{abstract}
%% Text of abstract
Vehicle light detection is required for important downstream safe autonomous driving tasks, such as predicting a vehicle's light state to determine if the vehicle is making a lane change or turning. Currently, many vehicle light detectors use single-stage detectors which predict bounding boxes to identify a vehicle light, in a manner decoupled from vehicle instances. In this paper, we present a method for detecting a vehicle light given an upstream vehicle detection and approximation of a visible light's center. Our method predicts four approximate corners associated with each vehicle light. We experiment with CNN architectures, data augmentation, and contextual preprocessing methods designed to reduce surrounding-vehicle confusion. We achieve an average distance error from the ground truth corner of 5.09 pixels, about 17.24\% of the size of the vehicle light on average. We train and evaluate our model on the LISA Lights dataset, allowing us to thoroughly evaluate our vehicle light corner detection model on a large variety of vehicle light shapes and lighting conditions. We propose that this model can be integrated into a pipeline with vehicle detection and vehicle light center detection to make a fully-formed vehicle light detection network, valuable to identifying trajectory-informative signals in driving scenes.  
\end{abstract}

%%Graphical abstract
% \begin{graphicalabstract}
% %% Figure removed
% \end{graphicalabstract}

%%Research highlights
% \begin{highlights}
% \item Research highlight 1
% \item Research highlight 2
% \end{highlights}

\begin{keyword}

pattern detection \sep vehicle lights \sep pose models \sep neural networks \sep machine learning \sep autonomous driving 
%% keywords here, in the form: keyword \sep keyword

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%% \linenumbers

\section{Introduction}
\label{sec:intro}

Detecting car lights is a critical task for autonomous and safe driving, as vehicle lights are key indicators for the future motion of the vehicle. With an accurate car detection model, intelligent systems (typically onboard the ego-vehicle) can analyze the surround car light state to learn the future surrounding traffic actions. As a result, a vehicle light can be used as a cue for models that perform vehicle maneuver classification and trajectory predictions \cite{deo2018would} \cite{messaoud2021trajectory} \cite{deo2020trajectory} or combined with driver monitoring data to set up a looking-in and looking-out system \cite{tawari2014looking} to predict driver take-over time\cite{rangesh2021autonomous} \cite{rangesh2021predicting}.  

%https://docs.google.com/presentation/d/1LVk6aE6Ubgk1wrj7qZ3ps3aAii4lVuj77ycwCDsCaL0/edit?usp=sharing
% Figure environment removed

Accordingly, in this paper, we propose a cascaded model approach to vehicle light detection in which each model performs the following:
\begin{enumerate}
  \item Detect vehicles (2D) in the traffic scene.
  \item From bounding boxes of these detected vehicles, estimate the center of each visible vehicle light.
  \item Using the bounding boxes of vehicles and centers of each visible vehicle light, predict the location of the 4 ``corners" associated with each vehicle light. Note that not all vehicles have a strict ``corner", so this can be taken to refer to a boundary point of two intersecting geometric curves comprising visual edges of the taillight. 
\end{enumerate}
In this work, we focus on developing a model that can solve the third problem defined in the list above. We formulate this problem as a regression task: Given the center coordinate of a car light in an image, we output 4 $(x,y)$ coordinates which regress from the center of the light to each of the 4 corners of the vehicle light. We use a CNN to learn features from a cropped image centered on the vehicle light to predict these 4 $(x,y)$ regression coordinates. Such a model can be cascaded with the preceding model layers to form a complete taillight detector, with the added benefit of implicit association of a detected taillight to its respective vehicle. 

% Our contribution is summarized as follows:
% \begin{itemize} 
%     \item With the LISA Lights dataset \cite{lisalightsdataset}, we train models using a masked regression loss function to predict the four taillight corners, and evaluate performance using a metric which represents the percent error relative to the vehicle light size. 
% \end{itemize}

% Figure environment removed

\section{Related Research}

% We present approaches to both provide common methods of vehicle light detection, and to highlight the ability of current models to perform 2D detection of vehicles under a variety of lighting conditions, since this is an important first step in our cascaded structure proposed above. While it may seem redundant to detect lights in order to detect cars in order to again detect lights, our overall structure learns to detect lights using relative information from the vehicle structure (i.e. intermediately determining the position of the light centers prior to regression), such that the vehicle-body-informed prediction of the lights should be stronger than a dissociated detection of a light.  

\subsection{Approaches to Vehicle light Detection}
 Detecting vehicle lights is especially critical during the night time, as the most visible feature of vehicles in front are the tail and brake lights in such conditions. Malley et al. \cite{o2010vehicle} take advantage of this and use a image processing technique with color thresholding to detect the vehicle tail lights, helping them detect the vehicle itself during night time conditions. Satzoda et al. \cite{satzoda2016looking} similarly aim to detect vehicles at night time using the rear lights as a main cue. To do so, they use an Ada-boost classifier on gray-scale images to detect the vehicle region, segment two sub-regions from the vehicle detection, and then perform red-channel thresholding for taillight detection. Pillai et. al \cite{pillai2016detecting} perform taillight detection for nighttime vehicle detection by using HSV thresholding and connected component analysis to detect and group a taillight pair. Instead of using a thresholding technique, Kosaka \& Ohashi \cite{kosaka2015vision} use an approach coined ``Center Surround Extremas" \cite{agrawal2008censure} for detecting taillights at night. Center Surround Extremas uses an integral image to detect light blobs like taillights moving at high speed.

While the techniques described above achieve high performance for detecting vehicle lights in nighttime conditions,
it is still critical to detect vehicle lights in daytime scenarios as well. Therefore, making vehicle light detectors robust to lighting conditions such as night and day is crucial. Ming \& Kang-Hyun \cite{ming2011vehicle} show that detecting the tail lights can still help for detecting vehicles in the daytime. Their approach uses color segmentation to find horizontal tail light pairs. Cui et al. \cite{cui2015vision} also address this problem of previous approaches, developing a taillight detection framework that works under different illumination circumstances. This framework first detects vehicles with a Deformable Part Model and then extract the taillight candidates by converting the pixel color space from RGB to HSV to perform color thresholding. Cao et. al \cite{cao2021application} implement a CNN for vehicle detection and then with this cropped vehicle image use the RGB and CMY color spaces for taillight recognition. 

Often, such image processing techniques for day-time conditions are not robust to conditions such as lighting or the distance to the vehicle of interest. For example, in cases where the redness of a taillight is lessened by a shadow, image processing and hand-crafted filters may not provide accurate taillight detections. To address this, deep learning object detection methods can be used to provide robustness to adverse lighting conditions. Rather than just predicting bounding boxes for tail lights, Vancea et. al \cite{vancea2017vehicle} use a FCN based on the VGG16 architecture \cite{simonyan2014very} to perform light segmentation. Since this segmentation network can output more than two taillight clusters, they also add a taillight pair identification step that uses distance and 3-D histogram test to match red regions from the segmentation output representing the taillights. Vancea et. al mention that when detecting taillights from a far distance, the segmentation produces worse results. Our taillight detection approach is invariant to vehicle distance and in fact uses the vehicle size to constrain the vehicle light prediction sizes. Rampavan \& Ijjina \cite{rampavan2023genetic} focus on brake light detection for motorcycle vehicles using a Mask-RCNN network. They note that two-stage object detectors achieve better performance for smaller objects like brake lights, an important finding in support of similar cascaded approaches to learning component vehicle features. Jeon et. al \cite{jeon2022deep} propose a deep learning cascaded model with a lane detector, car detector, and taillight detector. For specifically the taillight detector module, they use a Recurrent Rolling Convolution architecture to find the taillight regions of a vehicle. 

We note that this paper as well as the other papers that ignore the detection of the front lights, which are still an important feature to detect in many driving scenarios. For example, consider a four-way intersection with stop signs on each side. Detecting the front lights of the incoming vehicles would be critical to see if they are turning and therefore potentially interfering with an ego-vehicles trajectory. Our approach addresses this issue and presents a model that can detect both front and tail lights. Moreover, similar to other traffic detection problems that involve non-rectangular objects such as traffic lights and signs \cite{mogelmose2015detection, philipsen2015traffic}, previous research treats vehicle light detection as predicting a bounding box even though almost all vehicle lights are not a perfect rectangle. In this paper, we introduce a more precise vehicle light detection model that can predict the most common vehicle light shapes. 

As stated by Rapson et al. \cite{rapson2019performance}, the task of detecting car lights is difficult for a variety of reasons including: 
\begin{itemize}
    \item variety of car light shapes and brightness,
    \item variety of occlusions and orientations, and 
    \item environment lighting conditions (one of the most common problems in camera related Automated Driving Systems). 
\end{itemize} 
Deep learning methods have been effective in solving image recognition and detection tasks where the objects of interest may appear with such variation, in particular through the use of CNNs. Many vehicle light detection models use end-to-end object detection approach to predict a bounding box around the head or taillight \cite{li2020highly, vancea2017vehicle, rampavan2023genetic}. However, for certain tasks, data preprocessing and feature extraction can provide stronger performance by relieving the learning algorithm of the challenge of discovering features which human experts already know to be important to the task at hand \cite{dziezyc2020can}. Further, explicitly engineered (versus implicitly learned) features assist towards AI system explainability \cite{gosiewska2021simpler, shevskaya2021explainable}, a growing concern for safety-critical systems such as autonomous vehicles \cite{zablocki2022explainability}. 

% \subsection{Vehicle Light Datasets}
% As deep learning approaches become the common technique for vehicle light detection, there is a need for substantial public vehicle light datasets that can be used to train and evaluate such detection models. However, despite the rapid growth in autonomous driving datasets \cite{bogdoll2023impact}, there is still a lack in vehicle light detection datasets. Unlike some autnomous driving data that can be collected and automatically processed from sensor data \cite{satzoda2014drive}, a vehicle light detection dataset is time-consuming to curate since it requires manually annotating vehicle lights in traffic scenes. To expand such datasets, an automatic labeling tool such as 3d bat \cite{zimmer20193d} can be used on large autonomous vehicle datasets (KITTI \cite{geiger2012we}, A9-Dataset \cite{cress2022a9}, nuScenes \cite{caesar2020nuscenes}) to label vehicle lights. Rapson et al. \cite{rapson2019performance} construct the Vehicle Light Datasets, which contains manual segmentation labels of vehicle lights from a variety of datasets such as KITTI \cite{geiger2012we}, Berkeley Drive \cite{xu2017end}, and Cityscapes \cite{cordts2016cityscapes}. These segmentation labels of vehicle lights are not necessarily helpful for detection methods that treat vehicle lights as bounding boxes, requiring to convert these segmentation labels to bounding boxes through clustering algorithms that can produce inaccurate ground truths. Hsu et al. \cite{hsu2017learning} introduce Vehicle Rear Signal dataset, which contains consecutive video frames of the back of a vehicle to recognize taillight signals. However, this dataset is not useful for vehicle light detection as there is no data related to the location and size of the taillights. Vancea et al. \cite{vancea2017vehicle} note that to train their vehicle light detection model they manually annotate 677 images from the CompCars dataset \cite{yang2015large}. Such a dataset size may not be sufficient enought to train a robust vehicle light detector. The Apollo-Car3D dataset \cite{song2019apollocar3d} contains 5,277 driving images and over 60K car instances, where each vehicle is provided with annotations of labeled keypoints of various parts of the car including the vehicle lights. This dataset offers the largest amount of vehicle light data but needs to be filtered to remove irrelevant data related to vehicle lights and only include vehicle instances with a visible vehicle light. 

\subsection{Corner-based 2D Object Detection}
Most 2D object detection modules predict a bounding box around the object of interest. There are a few ways to encode a 2D box with four pieces of information, and the most typical encodings utilize (1) a 2D origin point, a height, and a width, or (2) two 2D points. One benefit of the prediction of keypoints (e.g. Law \& Deng's CornerNet \cite{law2018cornernet}) is that the boundary features of the object may be easier to identify as singular points, rather than learning the span of an object through anchor-box learning in methods like Faster RCNN \cite{girshick2015fast} or YOLO \cite{redmon2016you}. DÃ¶rr et al. \cite{dorr2022tetrapacknet} base their approach off of CornerNet with TetraPackNet, but instead of predicting a top-left corner and bottom-right corner an object is represented as four arbitrary vertices. Zhou et al. \cite{zhou2019objects} take an entirely different approach to object detection compared to 2D bounding boxes, instead viewing an object as collection of interconnected keypoints, developing CenterNet, which models an object by a single center point (presumed to be akin to the center of a traditional 2D bounding box). Using this center point, the detector can regress to other properties like the object corners, orientation, or even pose. We can see, then, that prediction of four corners of a bounding box (e.g. \cite{dorr2022tetrapacknet} and this research, in which we predict the four corners of the light as four $(x,y)$ distances from the known light center) represents a hybrid method, where specific, fixed landmark points of interest are being detected (rather than a broad parameterization of a box as height, width, and origin, for which many possible encodings exist which define the same box). Zhao et al. \cite{zhao2022corner} improve CenterNet by introducing CenternessNet. This detector adds box-edge length constraints to CenterNet which improves its ability to differentiate the corners of objects in the same category and reduces the computational expenses of CenterNet as well. %These center and corner based object detection modules is how we would envision a vehicle light detector to work, with a network like CenterNet to detect the centers of the vehicle light and then a regressor model to find the corners of the vehicle light. 

CenterNet and its derivatives have further utility upstream in the proposed cascaded model, as identifying the center of the vehicle would allow for a variety of regressions to points of interest (light centers, light corners, and other key features). While we limit the scope of this particular research to the identification of light corners from light centers, we highlight the utility of this center-based detection approach toward similar associated tasks in vehicle detection. 

\section{Methods}
% Figure environment removed

% Figure environment removed
% \subsection{Creating Vehicle Light Regression Dataset}
% \begin{table*}[hbt!]
% \caption{Comparison of different public datasets used to train vehicle light detection models. We note that Vancea et al.    \cite{vancea2017vehicle} manually annotate 677 taillight examples from the CompCars dataset \cite{yang2015large} to train their tail light segmentation model.}
% \centering
% \begin{tabular}{|C{3.5cm}|C{3cm}|C{8.5cm}|}
% \hline
% \textbf{Dataset Name} & \textbf{\# of Training Traffic Scene Images} & \textbf{Key Features} \\
% \hline \hline
% \href{https://cerv.aut.ac.nz/vehicle-lights-dataset/}{Vehicle Lights Dataset} \cite{rapson2019performance} & 1,232 & segmentation labels, data from different continents, variety of lighting and weather conditions \\
% \hline
% \href{http://mmlab.ie.cuhk.edu.hk/ datasets/comp_cars/index.html}{CompCars dataset} \cite{yang2015large} & 677 & images focused on head and taillights, variety of vehicle orientations\\
% \hline
% \href{https://cvrr.ucsd.edu/vehicle-lights-dataset}{Specialized version of ApolloCar3D dataset} &  \textbf{3,634} & corner labels of keypoints, variety of lighting, vehicle light shapes, and orientations, vehicle-only context images with vehicle light centered in image \\
% \hline
% \end{tabular}
% \label{table:datasetcomp}
% \end{table*}

% % Figure environment removed

% % Figure environment removed

% We first filter the ApolloCar3D dataset \cite{song2019apollocar3d} to only use the keypoint annotations of any visible front and tail lights, which provide us with the $(x,y)$ coordinates in respect to the the full traffic scene image for the four corners of a vehicle light and its center. Table \ref{table:datasetcomp} highlights our curated dataset in comparison to other public datasets used for vehicle light detection. This specialized version of the ApolloCar3D dataset we use has nearly three times the amount of traffic scene images in comparison to other public vehicle light datasets, providing an abundance of vehicle light examples and a variety of lighting conditions, vehicle light shapes, and orientations. Figure \ref{histogram} shows a histogram of the heights and widths of the vehicle lights in this dataset. 

% For each vehicle light annotation, we first crop the full traffic image to only focus on the vehicle of interest using the corresponding bounding box of the vehicle. With this cropped image of the vehicle, we create another $128\times 128$ cropped image that centers the center-coordinate of the taillight. To do this, we first calculate the translated coordinates $(x',y')$ of the vehicle light center so that they are in respect to the cropped vehicle image as follows: 
% \begin{equation}
% \begin{aligned}
%     &x' = x - x_{bbox}\\
%     &y' = y - y_{bbox}
% \end{aligned}
% \end{equation}
% $(x,y)$ define the coordinates of the center of vehicle light from the traffic scene image and $(x_{bbox}, y_{bbox})$ represent the upper-left coordinates of the vehicle in the traffic scene image. Using this coordinate, we find the upper left and bottom right coordinates of the vehicle light cropped image by subtracting and adding 64 respectively to $(x',y')$. Once we know the upper left and bottom right coordinates, we crop the vehicle image around this region. In case these crops exceed the cropped vehicle image boundaries, we add black padding on the top, bottom, left, and right if necessary. Placing the center of the vehicle light in the middle of the image simplifies the learning process for our model, as it will know consistently that the vehicle light of interest is located at the center of the image. We define this approach of cropping around and the vehicle and then the vehicle light as the "Vehicle-Only Context Approach". We also use a second approach we coin the "Vehicle with Scene Context Approach" that includes further traffic context. In this second approach, we still place the center of the vehicle light in the middle of the image, but include image content from the entire traffic scene rather than only the cropped vehicle region. Figure \ref{fig2} shows an example of each approach. 

% These cropped images around the vehicle light act as our inputs to the CNN regression model. We note that cropping around the vehicle and then the vehicle light (i.e. Approach 1) improves performance versus just taking the original traffic scene image and cropping around the vehicle light. 
% % The images from the second approach bring unnecessary information and potential other vehicles into the image which can confuse the model, especially if other vehicles are within the frame. If already given the bounding box of the vehicle, we can filter out such unwanted noise from our image so our model can focus on the vehicle light of interest. 
% To create the targets, we compute the difference of the $x$ and $y$ coordinates between the vehicle light corner and the vehicle light center coordinate $(x_{center}, y_{center})$. This computation gives us a list 8 offsets: $[x'_1, y'_1, x'_2, y'_2,  x'_3, y'_3,  x'_4, y'_4]$, where $(x'_1, y'_1)$, $(x'_2, y'_2)$, $(x'_3, y'_3)$, $(x'_4, y'_4)$ represent the offset from the center coordinate of the vehicle light to the upper left, upper right, bottom left, and bottom right corners respectively. We normalize these values from [-1, 1], where a value of -1 corresponds to a corner being 64 pixels to the left/above the light center and a value of 1 corresponds to a corner being 64 pixels to the right/below the right center. In some cases, a corner may not be visible in an image. If so, then we set $(x'_i, y'_i) = (0,0)$, where $(x'_i, y'_i)$ are the x and y pixel distances from the $i$th corner to the center of the vehicle light, a signal that such a corner should be ignored in training. 

% % Figure environment removed

% From this process, we collect 44,728 total cropped images around a vehicle light. In particular, there are 9,322 cropped images of the front-left light, 17,527 left-rear light images, 13,477 right-rear light images, and 4,432 right-front light images. Figure \ref{datapipeline} outlines the process done described in this paragraphs to curate the LISA lights dataset. Furthermore, data augmentation has improved performance in many autonomous driving tasks such as vehicle detection \cite{pokrywka2022yolo} and can increase the vehicle light dataset size. This propelled us to curate a separate dataset that horizontally reflects each cropped vehicle light image and corresponding regression labels, doubling our dataset size to have 89,456 examples. For both datasets we use a 80\%/10\%/10\% split to create a training, validation, and test set.
 To train a model to predict the corners of a vehicle light, we use the LISA Lights Dataset \cite{lisalightsdataset}. This dataset contains over 40,000 images of the four different vehicle lights with special images cropped around the center of the vehicle light. In addition, the (x,y) locations of each visible corner for a vehicle light, allow us to train a CNN that can extract features from these cropped vehicle light images to predict corner points. We modify standard CNN architectures (Resnet \cite{he2016deep}, Densenet \cite{huang2017densely}) by reducing the output size of the final layer to 8 values and using the tanh activation function to normalize predictions to a scale of -1 to 1. We test our network on pretrained versions of Resnet-18, Resnet-34, Resnet-50, Resnet-101 and DenseNet-121 to evaluate which model performs best for this task. We begin each model from pretrained weights optimized to the ImageNet classification task \cite{deng2009imagenet}. The CNN regression model is then trained using a custom regression loss that we define below:



\begin{equation}
    \mathcal{L} = \frac{1}{N}\sum_{i=1}^N\frac{1}{V_i}\sum_{j=1}^{4} \norm{p_{ij}M_{ij}-t_{ij}}_2
\end{equation}



In this equation, N represents the number of examples, $V_i$ represents the number of visible corners for the \emph{ith} example, and $p_{ij}$ and $t_{ij}$ are the (x,y) regression predictions and targets for the \emph{jth} corner of the \emph{ith} example. $M_{ij}$ is a boolean mask which is $10^{-8}$ if $t_{ij}=0$ and 1 otherwise. We avoid setting $M_{ij}=0$ if $t_{ij}=0$ because this would cause the weights to infinitely increase their value during backpropagation. This alteration of regression loss avoids penalizing the corner regression predictions for non-visible corners. While we originally used one CNN regression model to predict the corners for all vehicle lights, we found that training four separate models to learn corner prediction for the front left light, front right light, rear left light, and rear right light improved performance. As shown by Figure \ref{rear_lights} and \ref{front_lights}, the rear and front lights have significantly different shapes and colors, so it would be more difficult for a single model to generalize corner predictions for these type different vehicle lights. Instead, a model trained on a singular type of vehicle light type will be able to learn the features solely pertaining to this light and make more accurate corner predictions. Figure \ref{fig3} highlights the process defined above to make these corner predictions. We use the Adam optimizer \cite{kingma2014adam} to update our weights, Stochastic Weighting Averaging with a learning rate decay, a learning rate of $10^{-3}$, weight decay of $10^{-4}$, and train for 25 epochs for each vehicle light model. 

% Figure environment removed

\section{Experimental Results}

We evaluate the performance of our separate vehicle light models based on three metrics: the custom regression loss defined in the previous section, the average distance error (ADE) between a regression prediction and target, and the average percent error of a corner prediction. To calculate the average distance error we use the following the equation: 
\begin{equation}
    ADE = \frac{1}{4}\sum_{i=1}^4\frac{1}{V_i}\sum_{j=1}^{N} 64*\norm{p_{ij}M_{ij}-t_{ij}}_2
\end{equation}

The Average Distance Error uses the same variables defined in Equation 1. We multiply the distance between the predictions and targets by 64 since both are normalized from -1 to 1, so multiplying by 64 will give us the actual pixel distance in the image. This metric represents the average pixel distance for every corner label in our dataset. The average percent error metric is calculated as following:
\begin{equation}
    \text{\% Error} = \frac{1}{N}\sum_{i=1}^N\frac{1}{V_i}\sum_{j=1}^{4} \frac{\norm{p_{ij}M_{ij}-t_{ij}}_2}{\sqrt{W_{ij}^2 + H_{ij}^2}}
\end{equation}
We use similar variables to Equation 1 but introduce $W_{ij}$ and $H_{ij}$, which represent the width and height for the \emph{jth} corner of the \emph{ith} example respectively. $\sqrt{W_{ij}^2 + H_{ij}^2}$ represents the maximum distance a corner prediction can be while staying inside of the vehicle light box. Therefore, a percent error greater than 100\% means a prediction is out of range of the taillight corners. Some autonomous driving detection tasks such as detecting signs emphasize detection on important objects relative to the driver \cite{greer2022salience, greer2023salient, ohn2017all}. Even though there are four different different vehicle lights on a car that have varying impact to the ego-vehicle, our vehicle light detection approaches treats each vehicle light with equal importance. Therefore, to determine the overall performance of our approach, we average the performance on the test set for each of the separate vehicle models.

\begin{table}
\centering
\caption{Comparative performance of various CNN architectures on the vehicle light corner regression task.}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Model Type} & \textbf{Regression Loss} & \textbf{ADE} & \textbf{\% Error}\\
\hline \hline
ResNet-18 & 0.090 & 5.56 & 21.24 \\
\hline
ResNet-34 & 0.089 & 5.46 & 19.84 \\
\hline
ResNet-50 & 0.085 & 5.16 & \textbf{17.49} \\
\hline
ResNet-101 & \textbf{0.084} & \textbf{5.12} & 17.52 \\
\hline
DenseNet-121 & 0.094 & 5.81 & 21.69\\
\hline
DenseNet-169 & 0.095 & 6.10 & 22.78 \\
\hline
Vision Transformer (ViT) & 0.089 & 5.49 & 18.65 \\
\hline
\end{tabular}
\label{CNNStructures}
\end{table}

Table \ref{CNNStructures} highlights the performance of various CNN architectures trained for our designed task. Resnet-50 achieves the best percent error with 17.49\% while Resnet-101 produces the best average distance error (ADE) with 5.12 and a regression loss of 0.0838. As we increase model complexity up until ResNet-101, our performance increasingly becomes better. However, as we use even more complex models such as DenseNet-121, the performance becomes worse. This suggests that demonstrates that Resnet-50 or Resnet-101 offers the best model complexity for our vehicle light corner regression task. In addition, we are also interested in current state-of-the art models such as a Vision Transformer \cite{dosovitskiy2020image} would perform for this task. From these results, we can see that the Vision Transformer (ViT) leads to a 1\% error increase in comparison to the best models such as ResNet-50 and ResNet-101. 

%However, the Resnet-50 and Resnet-101 performances are nearly identical, but after running additional trials and taking the average performance Resnet-50 achieves better regression loss and average distance error. 

\begin{table}[hbt!]
\centering
\caption{Regression performance for different vehicle light cropping approaches.}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Cropping Vehicle Light Approach } & \textbf{Regression Loss} & \textbf{ADE} & \textbf{\% Error} \\
\hline \hline
Vehicle with Scene Context Approach & 0.095 & 5.87 & 20.07 \\
\hline
\textbf{Vehicle-Only Context Approach} & \textbf{0.085} & \textbf{5.16} & \textbf{17.49} \\
\hline
\end{tabular}
\label{table:vehcontext}
\end{table}
We also evaluate the performance of the two different images the LISA Lights Dataset provides us: Vehicle with Scene Context Approach and Vehicle-Only Context Approach \cite{lisalightsdataset}. For this experiment, we use ResNet-50 for each of our vehicle light corner regression models. We can see that the approach which uses a degree of meaningful feature extraction (i.e. using only the vehicle in the crop) achieves a better regression loss and average distance error than a more ``end-to-end" approach cropping from the full scene visual context. Knowing the image of the vehicle helps detect the vehicle light since using the vehicle image filters out noise from the traffic scene and surrounding vehicles. In addition, the black padding can constrain the regression predictions of the model so it can learn to never make predictions that will make the taillight corner locations outside the vehicle region; the vehicle-context-only model contains a more strongly constrained foreground. This simplifies the learning task for the CNN regression model further and allows for more accurate vehicle light predictions. 

\begin{table}[hbt!]
\caption{Regression performance for the regular vehicle light dataset and the expanded vehicle light dataset with augmentations (horizontal reflections).}
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Dataset Type} & \textbf{Regression Loss} & \textbf{ADE} & \textbf{\% Error}\\
\hline \hline
LISA Lights Dataset & 0.085 & \textbf{5.16} & 17.49  \\
\hline
LISA Lights Dataset with Augmentations & 0.085 & 5.19 & \textbf{17.27} \\
\hline
\end{tabular}
\label{table:augmentation}
\end{table}
To analyze if data augmentation through horizontal reflections improves the model accuracy, we use a Resnet-50 and trained it on the regular LISA lights dataset and LISA vehicle lights dataset with augmentations. Both datasets use the ``Vehicle-Only Context Approach`` as this was shown to improve the vehicle light corner regression performance. As shown in Table \ref{table:augmentation}, the expanded dataset with a horizontal reflection of each vehicle light achieves similar performance to the regular dataset. While it is true that lights have a chirality (the left-hand light cannot be superimposed onto the right-hand light), the visual features that distinguish a light from a non-light background would be expected to be similar regardless of this reflection. However, it is important to note that reflecting does strongly influence the imagined ``perspective" from which the camera views the vehicle. So, it could be the case that these reflections create artificial viewing angles which are not common to driving patterns where such data is collected, essentially creating virtual car orientations that the machine has no practical use in learning (leading to unimproved performance on the real-world-only test dataset). 
\begin{table}[hbt!]
\centering
\caption{Regression performance for each of the single vehicle light models.}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Vehicle Light} & \textbf{Regression Loss} & \textbf{ADE} & \textbf{\% Error} \\
\hline \hline
Front-Left & 0.080 & 4.93 & \textbf{14.11} \\
\hline
Rear-Left & \textbf{0.077} & \textbf{4.69} & 16.75 \\
\hline
Front-Right & 0.092 & 5.61 & 20.44 \\
\hline
Rear-Right &  0.084 & 5.14 & 17.69\\
\hline
\end{tabular}
\label{table:lights}
\end{table}

We also analyze the performance of each vehicle model as shown in Table \ref{table:lights}. For all statistics, the left-front and left-rear corner prediction models perform the best. There is a significant amount of data for both of these lights (17,794 left-rear light images and 9,578 left-front light images), which provided the light models enough examples to make accurate corner predictions. We can attest the lower performance of the right-front light to the smaller dataset we have for this light (4,452 images). Collecting more examples for the right-front light so there is a similar amount of images as the other lights would significantly improve performance and model generalization.

\begin{table*}[hbt!]
\centering 
\caption{Comparison of vehicle light detection performance of our vehicle light detection method to other taillight detection approaches}
\resizebox{\textwidth}{!}{%
%\begin{tabular}{|C{3cm}|C{3.75cm}|C{3.25cm}|C{2.25cm}|C{2.5cm}|}
\begin{tabular}{|c|c|c|c|c|}

\hline

\textbf{\footnotesize{Vehicle Light Detection Approach}} & \textbf{\footnotesize{Evaluation Dataset}} & \textbf{\footnotesize{\# of Evaluated Images}} & \textbf{\footnotesize{Performance Metric/s}} & \textbf{\footnotesize{Performance Metric Results}} \\
\hline \hline

Rapson et al. \cite{rapson2019performance} & \href{https://cerv.aut.ac.nz/vehicle-lights-dataset/}{\footnotesize{Vehicle Lights Dataset}} & 1,869 & mAP@25 (\%), mAP@50 (\%) & 18, 5\\
\hline
Vancea et al. \cite{vancea2017vehicle} & \footnotesize{Subset of KITTI Tracking} & 3 videos, unknown lengths & Segmentation Accuracy (\%) & 95.8 \\
\hline
Jeon et al. \cite{jeon2022deep} & \footnotesize{Subset of KITTI Tracking} & 3 videos, unknown lengths & mAP@50 (\%) & 100.0 \\
\hline
\textbf{Our Approach} & \href{https://cvrr.ucsd.edu/vehicle-lights-dataset}{\footnotesize{Specialized version of ApolloCar3d}} &  \textbf{2,464} & mAP@25 (\%), mAP@50(\%) & 97.65, 80.97 \\
\hline
\end{tabular}
}
\label{table:iou}
\end{table*}
Furthermore, we compare the performance of our vehicle light detection approach to other taillight detection methods in research. We apply the same metrics Rapson et al. use, mAP@25 and mAP@50, to evaluate the performance of our model on the dataset we use. To do so, we treat our corner predictions and ground truths as a bounding box and calculate the IOU between the prediction and ground truth box. If the IOU was greater than a threshold $\alpha$, which is 0.25 for mAP@25 and 0.5 for mAP@50, then we would count this prediction as a correct prediction. We note that Vancea et al. \cite{vancea2017vehicle} and Jeon et al. \cite{jeon2022deep} report these metrics while evaluating on three different videos from the KITTI tracking dataset. Our approach achieves similar results to current taillight detection methods and also includes a more thorough evaluation of our model given the \# of traffic scene images we evaluated on. In addition, we also include detection of front lights, while these other methods solely focus on taillight detection. While we are not performing a full-scale vehicle light detection process, these results demonstrate that our model can be integrated with a vehicle detector and vehicle light center detector model and still achieve strong performance in these metrics.
% Figure environment removed

Figure \ref{fig:qual} shows some arbitrarily selected predictions of our model, meant to be representative of the spectrum of performance. Overall, it predicts shapes similar to the ground truth and is robust to different types of vehicle lights. Furthermore, the light detector is still effective in a variety of adverse conditions such as irregular lighting and far distances from the vehicle of interest. However, for some irregular tail light shapes or occluded vehicle lights (such as the images in the 1st and 2nd row of the 3rd column of Figure \ref{fig:qual}), our model does not predicts a shape similar to the ground truth. This is expected as some of these cases of lights are irregular and offer less examples for the model to learn from. %To address this, we could manually search for these examples in our dataset and then upsample them during training time. 


%------------------------------------------------------------------------
\section{Concluding Remarks}
In this paper, we have presented a corner-based method of 2D vehicle light detection using a CNN model that takes as input a given vehicle detection and visible light center point, and predicts as output the corners of the vehicle light. Importantly, this method implicitly learns to associate detected lights with a particular vehicle due to the cascaded model approach which integrates prior information of vehicle detections (while also voiding non-vehicle context from the scene image).

Why are these vehicle lights important and relevant to a nearby autonomous vehicle? Beyond the role of vehicle component detection for obstacle awareness in AEB systems, for an autonomous vehicle to safely navigate in traffic, learning how to detect surround vehicle lights is a crucial component toward understanding meaningful information (like turn signals and brake indications) that assist in predicting the future actions of other traffic agents (such as lane or speed changes). From these detected tail lights, we suggest that future research should seek to derive these temporal signals (such as turn and brake indications), ultimately serving the task of safe path planning. 

%For further improvement of this model, we aim to upsample lest common taillight shapes during training. This will expose the CNN regression model a better variety of taillight shapes so it can learn to properly predict the corners of these vehicle lights. 


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{elsarticle-num.bst}
\bibliography{refs}
}

\end{document}
\endinput
%%
%% End of file `elsarticle-template-num.tex'.
