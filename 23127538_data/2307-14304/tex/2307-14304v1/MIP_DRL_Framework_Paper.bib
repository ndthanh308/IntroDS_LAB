%% safe reinforcement learning 
@article{benchmark_SRL_review,
  title={Benchmarking safe exploration in deep reinforcement learning},
  author={Ray, Alex and Achiam, Joshua and Amodei, Dario},
  journal={arXiv preprint arXiv:1910.01708},
  volume={7},
  pages={1},
  year={2019}
}
@article{comprehensive_survey_SRL_review,
  title={A comprehensive survey on safe reinforcement learning},
  author={Garc{\i}a, Javier and Fern{\'a}ndez, Fernando},
  journal={J. of Machine Learning Research},
  volume={16},
  number={1},
  pages={1437--1480},
  year={2015}
}

%% reference for safe reinforcement learning based OESS 
@inproceedings{srl_ed,
  title={Contingency-constrained economic dispatch with safe reinforcement learning},
  author={Eichelbeck, Michael and Markgraf, Hannah and Althoff, Matthias},
  booktitle={2022 21st IEEE International Conference on Machine Learning and Applications (ICMLA)},
  pages={597--602},
  year={2022},
  organization={IEEE}
}
@article{srl_projection_performance_test,
  title={Safe reinforcement learning via projection on a safe set: How to achieve optimality?},
  author={Gros, Sebastien and Zanon, Mario and Bemporad, Alberto},
  journal={IFAC-PapersOnLine},
  volume={53},
  number={2},
  pages={8076--8081},
  year={2020},
  publisher={Elsevier}
}



% primal dual DDPG
@article{em_pd_DDPG,
  title={A safe reinforcement learning approach for multi-energy management of smart home},
  author={Ding, Hongyuan and Xu, Yan and Hao, Benjamin Chew Si and Li, Qiaoqiao and Lentzakis, Antonis},
  journal={Electric Power Systems Research},
  volume={210},
  pages={108120},
  year={2022},
  publisher={Elsevier}
}
% gurobi 
@misc{gurobi_paper,
	title = {What’s {New} – {Gurobi} 10.0},
	url = {https://www.gurobi.com/whats-new-gurobi-10-0/},
	abstract = {Blazing-fast speed, innovative data science integration, and enterprise-level development and deployment.},
	language = {en-US},
	urldate = {2023-06-07},
	journal = {Gurobi Optimization},
}
% DDPG
@article{lillicrap_ddpg,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}
% soft actor critic 
@inproceedings{haarnoja2018softactor,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1861--1870},
  year={2018},
  organization={PMLR}
}
%model based approaches
@inproceedings{liu2021deep,
  title={Deep Reinforcement Learning for Stochastic Dynamic Microgrid Energy Management},
  author={Liu, Linpeng and Zhu, Jianquan and Chen, Jiajun and Ye, Hanfang},
  booktitle={2021 IEEE 4th International Electrical and Energy Conference (CIEEC)},
  pages={1--6},
  year={2021},
  organization={IEEE}
}
@article{chen2022robust,
  title={A robust optimization framework for energy management of CCHP users with integrated demand response in electricity market},
  author={Chen, Lingmin and Tang, Huiling and Wu, Jiekang and Li, Changjie and Wang, Yanan},
  journal={International Journal of Electrical Power \& Energy Systems},
  volume={141},
  pages={108181},
  year={2022},
  publisher={Elsevier}
}
@article{su2022energy,
  title={Energy management for active distribution network incorporating office buildings based on chance-constrained programming},
  author={Su, Su and Li, Zening and Jin, Xiaolong and Yamashita, Koji and Xia, Mingchao and Chen, Qifang},
  journal={International Journal of Electrical Power \& Energy Systems},
  volume={134},
  pages={107360},
  year={2022},
  publisher={Elsevier}
}
%% Pedro MILP
@ARTICLE{VergaraLopez2019,
  author={Vergara, Pedro P. and López, Juan C. and Rider, Marcos J. and da Silva, Luiz C. P.},
  journal={IEEE Trans. Smart Grid}, 
  title={Optimal Operation of Unbalanced Three-Phase Islanded Droop-Based Microgrids}, 
  year={2019},
  volume={10},
  number={1},
  pages={928-940},
  doi={10.1109/TSG.2017.2756021}}
  
@article{VergaraLopezStch2020,
  title={A stochastic programming model for the optimal operation of unbalanced three-phase islanded microgrids},
  author={Vergara, Pedro P and L{\'o}pez, Juan Camilo and Rider, Marcos J and Shaker, Hamid R and da Silva, Luiz CP and J{\o}rgensen, Bo N},
  journal={International Journal of Electrical Power \& Energy Systems},
  volume={115},
  pages={105446},
  year={2020},
  publisher={Elsevier}
}
%aihui paper 
@article{aihui2022distributed,
  author={Fu, Aihui and Cvetković, Miloš and Palensky, Peter},
  journal={IEEE Trans. Smart Grid}, 
  title={Distributed Cooperation for Voltage Regulation in Future Distribution Networks}, 
  year={2022},
  volume={13},
  number={6},
  pages={4483-4493},
}
% cpo 
@inproceedings{achiam2017cpo,
  title={Constrained policy optimization},
  author={Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
  booktitle={International conference on machine learning},
  pages={22--31},
  year={2017},
  organization={PMLR}
}

@ARTICLE{Macedo2015,
  author={Macedo, Leonardo H. and Franco, John F. and Rider, Marcos J. and Romero, Rubén},
  journal={IEEE Trans. Smart Grid}, 
  title={Optimal Operation of Distribution Networks Considering Energy Storage Devices}, 
  year={2015},
  volume={6},
  number={6},
  pages={2825-2836},
  doi={10.1109/TSG.2015.2419134}}

% cpo distribution network
@ARTICLE{cpo_distirbution_network,  author={Li, Hepeng and He, Haibo},  journal={IEEE Trans. Smart Grid},   title={Learning to Operate Distribution Networks With Safe Deep Reinforcement Learning},   year={2022},  volume={13},  number={3},  pages={1860-1872},  doi={10.1109/TSG.2022.3142961}}
%cpo EV
@article{li_cpo_EV,
  title={Constrained {EV} charging scheduling based on safe deep reinforcement learning},
  author={Li, Hepeng and Wan, Zhiqiang and He, Haibo},
  journal={IEEE Trans. on Smart Grid},
  volume={11},
  number={3},
  pages={2427--2439},
  year={2019},
  publisher={IEEE}
}

% cpo control 
@article{cpo_overload_relief,
  title={Online Preventive Control for Transmission Overload Relief Using Safe Reinforcement Learning with Enhanced Spatial-Temporal Awareness},
  author={Cui, Han and Ye, Yujian and Hu, Jianxiong and Tang, Yi and Lin, Zizhao and Strbac, Goran},
  journal={IEEE Trans. Power Systems},
  year={2023},
  publisher={IEEE}
}


% safe DDPG
@article{safe_ddpg,
  title={Safe exploration in continuous action spaces},
  author={Dalal, Gal and Dvijotham, Krishnamurthy and Vecerik, Matej and Hester, Todd and Paduraru, Cosmin and Tassa, Yuval},
  journal={arXiv preprint arXiv:1801.08757},
  year={2018}
}

% computation time 
@inproceedings{wei2022safe,
  title={Safe control with neural network dynamic models},
  author={Wei, Tianhao and Liu, Changliu},
  booktitle={Learning for Dynamics and Control Conference},
  pages={739--750},
  year={2022},
  organization={PMLR}
}
%% state wise 
@article{zhao2023state,
  title={State-wise Safe Reinforcement Learning: A Survey},
  author={Zhao, Weiye and He, Tairan and Chen, Rui and Wei, Tianhao and Liu, Changliu},
  journal={arXiv preprint arXiv:2302.03122},
  year={2023}
}
%% safe ddpg voltage 
@article{kou2020safe,
  title={Safe deep reinforcement learning-based constrained optimal control scheme for active distribution networks},
  author={Kou, Peng and Liang, Deliang and Wang, Chen and Wu, Zihao and Gao, Lin},
  journal={Applied energy},
  volume={264},
  pages={114772},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{fischettiJo2018,
  title={Deep neural networks and mixed integer linear optimization},
  author={Fischetti, Matteo and Jo, Jason},
  booktitle={Constraints},
  volume={23},
  pages={296--309},
  year={2018}
}

%% actor expert 
@article{actor_expert,
  title={Actor-expert: A framework for using q-learning in continuous action spaces},
  author={Lim, Sungsu and Joseph, Ajin and Le, Lei and Pan, Yangchen and White, Martha},
  journal={arXiv preprint arXiv:1810.09103},
  year={2018}
}

%% sutton 
@book{sutton_reinforcement_2018,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

%% pedro rl control 
@article{pedro2022_rl_votlage_control,
  title={Optimal dispatch of {PV} inverters in unbalanced distribution systems using Reinforcement Learning},
  author={Vergara, Pedro P and Salazar, Mauricio and Giraldo, Juan S and Palensky, Peter},
  journal={Int. J. of Elec. Power \& Energy Systems},
  volume={136},
  pages={107628},
  year={2022},
  publisher={Elsevier}
}

%% mauricio
@article{mauricio2022eligibility,
title = {Community energy storage operation via reinforcement learning with eligibility traces},
journal = {Electric Power Systems Research},
volume = {212},
pages = {108515},
year = {2022},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2022.108515},
author = {Edgar Mauricio {Salazar Duque} and Juan S. Giraldo and Pedro P. Vergara and Phuong Nguyen and Anne {van der Molen} and Han Slootweg},
keywords = {Battery management, Reinforcement learning, Operation under uncertainty, Temporal difference learning, Eligibility traces},
}

%multi-agent voltage control 
@article{wang2021multi,
  title={Multi-agent reinforcement learning for active voltage control on power distribution networks},
  author={Wang, Jianhong and Xu, Wangkun and Gu, Yunjie and Song, Wenbin and Green, Tim C},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={3271--3284},
  year={2021}
}

%% DQN
@article{mnih2015DQN,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}
%% qlearning 
@article{qlearning_watkins,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3},
  pages={279--292},
  year={1992},
  publisher={Springer}
}
%%caql 
@inproceedings{Ryu2020CAQL,
title={{CAQL}: Continuous Action {Q}-Learning},
author={Moonkyung Ryu and Yinlam Chow and Ross Anderson and Christian Tjandraatmadja and Craig Boutilier},
booktitle={International Conference on Learning Representations},
year={2020},
}
%% omlt 
@article{ceccon2022omlt,
  title={OMLT: Optimization \& machine learning toolkit},
  author={Ceccon, Francesco and Jalving, Jordan and Haddad, Joshua and Thebelt, Alexander and Tsay, Calvin and Laird, Carl D and Misener, Ruth},
  journal={The Journal of Machine Learning Research},
  volume={23},
  number={1},
  pages={15829--15836},
  year={2022},
  publisher={JMLRORG}
}

%% pyomo 
@book{hart_pyomo_2017,
  title={Pyomo-optimization modeling in python},
  author={Hart, William E and Laird, Carl D and Watson, Jean-Paul and Woodruff, David L and Hackebeil, Gabriel A and Nicholson, Bethany L and Siirola, John D and others},
  volume={67},
  year={2017},
  publisher={Springer}
}

%% sac
@inproceedings{haarnoja_sac_2018,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1861--1870},
  year={2018},
  organization={PMLR}
}
%%td3
@inproceedings{fujimoto_td3_2018,
  title={Addressing function approximation error in actor-critic methods},
  author={Fujimoto, Scott and Hoof, Herke and Meger, David},
  booktitle={International conference on machine learning},
  pages={1587--1596},
  year={2018},
  organization={PMLR}
}
%%ppo
@article{schulman_ppo_2017,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

%% state-wise safety
@article{zhao2023state_wise,
  title={State-wise Safe Reinforcement Learning: A Survey},
  author={Zhao, Weiye and He, Tairan and Chen, Rui and Wei, Tianhao and Liu, Changliu},
  journal={arXiv preprint arXiv:2302.03122},
  year={2023}
}
%% Pedro MILP
@ARTICLE{pedro2019_voltage_control,
  author={Vergara, Pedro P. and López, Juan C. and Rider, Marcos J. and da Silva, Luiz C. P.},
  journal={IEEE Trans. Smart Grid}, 
  title={Optimal Operation of Unbalanced Three-Phase Islanded Droop-Based Microgrids}, 
  year={2019},
  volume={10},
  number={1},
  pages={928-940},
  doi={10.1109/TSG.2017.2756021}}


%% performance comparision 
@inproceedings{shengren2022performance,
  title={Performance comparison of deep RL algorithms for energy systems optimal scheduling},
  author={Shengren, Hou and Salazar, Edgar Mauricio and Vergara, Pedro P and Palensky, Peter},
  booktitle={2022 IEEE PES Innovative Smart Grid Technologies Conference Europe (ISGT-Europe)},
  pages={1--6},
  year={2022},
  organization={IEEE}
}

%%github
@misc{
    Shengren,
    author = "Hou Shengren and Pedro Vergara",
    title = "",
    year = {2022},
    note = {\url{https://github.com/ShengrenHou/Energy-management-MIP-Deep-Reinforcement-Learning}}
    }
@inproceedings{Ryu2020CAQL,
title={{CAQL}: Continuous Action {Q}-Learning},
author={Moonkyung Ryu and Yinlam Chow and Ross Anderson and Christian Tjandraatmadja and Craig Boutilier},
booktitle={International Conference on Learning Representations},
year={2020},
}
%% count linear region 
@article{montufar2014count_linear_region,
  title={On the number of linear regions of deep neural networks},
  author={Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}
%% expressiveness 
@InProceedings{ReLU_expressiveness,
  title = 	 {Expressiveness of Rectifier Networks},
  author = 	 {Pan, Xingyuan and Srikumar, Vivek},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2427--2435},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/panb16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/panb16.html},
  abstract = 	 {Rectified Linear Units (ReLUs) have been shown to ameliorate the vanishing gradient problem, allow for efficient backpropagation, and empirically promote sparsity in the learned parameters. They have led to state-of-the-art results in a variety of applications. However, unlike threshold and sigmoid networks, ReLU networks are less explored from the perspective of their expressiveness. This paper studies the expressiveness of ReLU networks. We characterize the decision boundary of two-layer ReLU networks by constructing functionally equivalent threshold networks. We show that while the decision boundary of a two-layer ReLU network can be captured by a threshold network, the latter may require an exponentially larger number of hidden units. We also formulate sufficient conditions for a corresponding logarithmic reduction in the number of hidden units to represent a sign network as a ReLU network. Finally, we experimentally compare threshold networks and their much smaller ReLU counterparts with respect to their ability to learn from synthetically generated data.}
}
%%bounding for linear region 
@inproceedings{serra2018bounding_linear_region,
  title={Bounding and counting linear regions of deep neural networks},
  author={Serra, Thiago and Tjandraatmadja, Christian and Ramalingam, Srikumar},
  booktitle={International Conference on Machine Learning},
  pages={4558--4566},
  year={2018},
  organization={PMLR}
}
%% shengren paper 
@article{shengren2023optimal,
  title={Optimal energy system scheduling using a constraint-aware reinforcement learning algorithm},
  author={Shengren, Hou and Vergara, Pedro P and Duque, Edgar Mauricio Salazar and Palensky, Peter},
  journal={Int. J. of Electrical Power \& Energy Systems},
  volume={152},
  pages={109230},
  year={2023},
  publisher={Elsevier}
}