% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{aihui2022distributed}
A.~Fu, M.~Cvetković, and P.~Palensky, ``Distributed cooperation for voltage
  regulation in future distribution networks,'' \emph{IEEE Trans. Smart Grid},
  vol.~13, no.~6, pp. 4483--4493, 2022.

\bibitem{Macedo2015}
L.~H. Macedo, J.~F. Franco, M.~J. Rider, and R.~Romero, ``Optimal operation of
  distribution networks considering energy storage devices,'' \emph{IEEE Trans.
  Smart Grid}, vol.~6, no.~6, pp. 2825--2836, 2015.

\bibitem{VergaraLopez2019}
P.~P. Vergara, J.~C. López, M.~J. Rider, and L.~C.~P. da~Silva, ``Optimal
  operation of unbalanced three-phase islanded droop-based microgrids,''
  \emph{IEEE Trans. Smart Grid}, vol.~10, no.~1, pp. 928--940, 2019.

\bibitem{chen2022robust}
L.~Chen, H.~Tang, J.~Wu, C.~Li, and Y.~Wang, ``A robust optimization framework
  for energy management of cchp users with integrated demand response in
  electricity market,'' \emph{International Journal of Electrical Power \&
  Energy Systems}, vol. 141, p. 108181, 2022.

\bibitem{wang2021multi}
J.~Wang, W.~Xu, Y.~Gu, W.~Song, and T.~C. Green, ``Multi-agent reinforcement
  learning for active voltage control on power distribution networks,''
  \emph{Advances in Neural Information Processing Systems}, vol.~34, pp.
  3271--3284, 2021.

\bibitem{sutton_reinforcement_2018}
R.~S. Sutton and A.~G. Barto, \emph{Reinforcement learning: An
  introduction}.\hskip 1em plus 0.5em minus 0.4em\relax MIT press, 2018.

\bibitem{mnih2015DQN}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~A. Rusu, J.~Veness, M.~G. Bellemare,
  A.~Graves, M.~Riedmiller, A.~K. Fidjeland, G.~Ostrovski \emph{et~al.},
  ``Human-level control through deep reinforcement learning,'' \emph{Nature},
  vol. 518, no. 7540, pp. 529--533, 2015.

\bibitem{comprehensive_survey_SRL_review}
J.~Garc{\i}a and F.~Fern{\'a}ndez, ``A comprehensive survey on safe
  reinforcement learning,'' \emph{J. of Machine Learning Research}, vol.~16,
  no.~1, pp. 1437--1480, 2015.

\bibitem{benchmark_SRL_review}
A.~Ray, J.~Achiam, and D.~Amodei, ``Benchmarking safe exploration in deep
  reinforcement learning,'' \emph{arXiv preprint arXiv:1910.01708}, vol.~7,
  p.~1, 2019.

\bibitem{cpo_overload_relief}
H.~Cui, Y.~Ye, J.~Hu, Y.~Tang, Z.~Lin, and G.~Strbac, ``Online preventive
  control for transmission overload relief using safe reinforcement learning
  with enhanced spatial-temporal awareness,'' \emph{IEEE Trans. Power Systems},
  2023.

\bibitem{achiam2017cpo}
J.~Achiam, D.~Held, A.~Tamar, and P.~Abbeel, ``Constrained policy
  optimization,'' in \emph{International conference on machine learning}.\hskip
  1em plus 0.5em minus 0.4em\relax PMLR, 2017, pp. 22--31.

\bibitem{cpo_distirbution_network}
H.~Li and H.~He, ``Learning to operate distribution networks with safe deep
  reinforcement learning,'' \emph{IEEE Trans. Smart Grid}, vol.~13, no.~3, pp.
  1860--1872, 2022.

\bibitem{safe_ddpg}
G.~Dalal, K.~Dvijotham, M.~Vecerik, T.~Hester, C.~Paduraru, and Y.~Tassa,
  ``Safe exploration in continuous action spaces,'' \emph{arXiv preprint
  arXiv:1801.08757}, 2018.

\bibitem{kou2020safe}
P.~Kou, D.~Liang, C.~Wang, Z.~Wu, and L.~Gao, ``Safe deep reinforcement
  learning-based constrained optimal control scheme for active distribution
  networks,'' \emph{Applied energy}, vol. 264, p. 114772, 2020.

\bibitem{srl_ed}
M.~Eichelbeck, H.~Markgraf, and M.~Althoff, ``Contingency-constrained economic
  dispatch with safe reinforcement learning,'' in \emph{2022 21st IEEE
  International Conference on Machine Learning and Applications (ICMLA)}.\hskip
  1em plus 0.5em minus 0.4em\relax IEEE, 2022, pp. 597--602.

\bibitem{srl_projection_performance_test}
S.~Gros, M.~Zanon, and A.~Bemporad, ``Safe reinforcement learning via
  projection on a safe set: How to achieve optimality?''
  \emph{IFAC-PapersOnLine}, vol.~53, no.~2, pp. 8076--8081, 2020.

\bibitem{em_pd_DDPG}
H.~Ding, Y.~Xu, B.~C.~S. Hao, Q.~Li, and A.~Lentzakis, ``A safe reinforcement
  learning approach for multi-energy management of smart home,'' \emph{Electric
  Power Systems Research}, vol. 210, p. 108120, 2022.

\bibitem{pedro2022_rl_votlage_control}
P.~P. Vergara, M.~Salazar, J.~S. Giraldo, and P.~Palensky, ``Optimal dispatch
  of {PV} inverters in unbalanced distribution systems using reinforcement
  learning,'' \emph{Int. J. of Elec. Power \& Energy Systems}, vol. 136, p.
  107628, 2022.

\bibitem{mauricio2022eligibility}
E.~M. {Salazar Duque}, J.~S. Giraldo, P.~P. Vergara, P.~Nguyen, A.~{van der
  Molen}, and H.~Slootweg, ``Community energy storage operation via
  reinforcement learning with eligibility traces,'' \emph{Electric Power
  Systems Research}, vol. 212, p. 108515, 2022.

\bibitem{shengren2022performance}
H.~Shengren, E.~M. Salazar, P.~P. Vergara, and P.~Palensky, ``Performance
  comparison of deep rl algorithms for energy systems optimal scheduling,'' in
  \emph{2022 IEEE PES Innovative Smart Grid Technologies Conference Europe
  (ISGT-Europe)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2022, pp. 1--6.

\bibitem{shengren2023optimal}
H.~Shengren, P.~P. Vergara, E.~M.~S. Duque, and P.~Palensky, ``Optimal energy
  system scheduling using a constraint-aware reinforcement learning
  algorithm,'' \emph{Int. J. of Electrical Power \& Energy Systems}, vol. 152,
  p. 109230, 2023.

\bibitem{qlearning_watkins}
C.~J. Watkins and P.~Dayan, ``Q-learning,'' \emph{Machine learning}, vol.~8,
  no.~3, pp. 279--292, 1992.

\bibitem{actor_expert}
S.~Lim, A.~Joseph, L.~Le, Y.~Pan, and M.~White, ``Actor-expert: A framework for
  using q-learning in continuous action spaces,'' \emph{arXiv preprint
  arXiv:1810.09103}, 2018.

\bibitem{lillicrap_ddpg}
T.~P. Lillicrap, J.~J. Hunt, A.~Pritzel, N.~Heess, T.~Erez, Y.~Tassa,
  D.~Silver, and D.~Wierstra, ``Continuous control with deep reinforcement
  learning,'' \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem{fujimoto_td3_2018}
S.~Fujimoto, H.~Hoof, and D.~Meger, ``Addressing function approximation error
  in actor-critic methods,'' in \emph{International conference on machine
  learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2018, pp. 1587--1596.

\bibitem{haarnoja2018softactor}
T.~Haarnoja, A.~Zhou, P.~Abbeel, and S.~Levine, ``Soft actor-critic: Off-policy
  maximum entropy deep reinforcement learning with a stochastic actor,'' in
  \emph{International conference on machine learning}.\hskip 1em plus 0.5em
  minus 0.4em\relax PMLR, 2018, pp. 1861--1870.

\bibitem{fischettiJo2018}
M.~Fischetti and J.~Jo, ``Deep neural networks and mixed integer linear
  optimization,'' in \emph{Constraints}, vol.~23, 2018, pp. 296--309.

\bibitem{ceccon2022omlt}
F.~Ceccon, J.~Jalving, J.~Haddad, A.~Thebelt, C.~Tsay, C.~D. Laird, and
  R.~Misener, ``Omlt: Optimization \& machine learning toolkit,'' \emph{The
  Journal of Machine Learning Research}, vol.~23, no.~1, pp. 15\,829--15\,836,
  2022.

\bibitem{Shengren}
H.~Shengren and P.~Vergara, 2022,
  \url{https://github.com/ShengrenHou/Energy-management-MIP-Deep-Reinforcement-Learning}.

\end{thebibliography}
