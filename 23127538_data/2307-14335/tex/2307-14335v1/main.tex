
\documentclass{article} % For LaTeX2e
\usepackage{iclr2023_conference,times}

\input{math_commands.tex}
\usepackage[colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue,
            citecolor = blue,
            anchorcolor = blue]{hyperref}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{hyperref}
\usepackage{url}
\usepackage{enumitem}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{listings}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{comment}
\usepackage{xcolor}
\lstdefinelanguage{json}{
    basicstyle=\normalfont\ttfamily,
    numbers=left,
    numberstyle=\scriptsize,
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    frame=lines,
    backgroundcolor=\color{white},
    stringstyle=\color{black},
    keywordstyle=\color{black},
    commentstyle=\color{gray},
    string=[s]{"}{"},
    comment=[l]{//},
    morecomment=[s]{/*}{*/},
    keywords={:, {, }, [, ]}
}


\iclrfinalcopy

\title{\parbox{\linewidth}{\centering WavJourney: Compositional Audio Creation with Large Language Models}}

\author{     
    \parbox{\linewidth}{\centering 
    Xubo Liu\textsuperscript{1,$\dagger$},
    \textbf{Zhongkai Zhu\textsuperscript{2}},
    \textbf{Haohe Liu\textsuperscript{1,*}}, 
    \textbf{Yi Yuan\textsuperscript{1,*}},
    \textbf{Meng Cui\textsuperscript{1}}, 
    \textbf{Qiushi Huang\textsuperscript{1}},} \\
    \parbox{\linewidth}{\centering 
    ~\textbf{Jinhua Liang\textsuperscript{3}}, 
    \textbf{Yin Cao\textsuperscript{4}}, 
    \textbf{Qiuqiang Kong\textsuperscript{5}},
    \textbf{Mark D. Plumbley\textsuperscript{1}},
    \textbf{Wenwu Wang\textsuperscript{1}}} \\\\
    \parbox{\linewidth}{\centering 
    ~\textsuperscript{1} University of Surrey ~\textsuperscript{2} Independent Researcher ~\textsuperscript{3} Queen Mary University of London}  \\
    \parbox{\linewidth}{\centering 
    ~\textsuperscript{4} Xiâ€™an Jiaotong Liverpool University ~\textsuperscript{5} The Chinese University of Hong Kong} \\\\
    \parbox{\linewidth}{\centering 
    ~\textsuperscript{$\dagger$} Project Lead ~\textsuperscript{*} Equal Contribution}\\\\
    \parbox{\linewidth}{\centering \url{https://Audio-AGI.github.io/WavJourney_demopage}}
}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
Large Language Models (LLMs) have shown great promise in integrating diverse expert models to tackle intricate language and vision tasks. Despite their significance in advancing the field of Artificial Intelligence Generated Content (AIGC), their potential in intelligent audio content creation remains unexplored. In this work, we tackle the problem of creating audio content with storylines encompassing speech, music, and sound effects, guided by text instructions. We present WavJourney, a system that leverages LLMs to connect various audio models for audio content generation. Given a text description of an auditory scene, WavJourney first prompts LLMs to generate a structured script dedicated to audio storytelling. The audio script incorporates diverse audio elements, organized based on their spatio-temporal relationships. As a conceptual representation of audio, the audio script provides an interactive and interpretable rationale for human engagement. Afterward, the audio script is fed into a script compiler, converting it into a computer program. Each line of the program calls a task-specific audio generation model or computational operation function (e.g., concatenate, mix). The computer program is then executed to obtain an explainable solution for audio generation. We demonstrate the practicality of WavJourney across diverse real-world scenarios, including science fiction, education, and radio play. The explainable and interactive design of WavJourney fosters human-machine co-creation in multi-round dialogues, enhancing creative control and adaptability in audio production. WavJourney audiolizes the human imagination, opening up new avenues for creativity in multimedia content creation.
\end{abstract}
% Figure environment removed

\section{Introduction}
The emerging field of multi-modal artificial intelligence (AI), a realm where visual, auditory, and textual data converge, opens up fascinating possibilities in our day-to-day life, ranging from personalized entertainment to advanced accessibility features. As a powerful intermediary, natural language shows great potential to enhance understanding and facilitate communication across multiple sensory domains. Large Language Models (LLMs), which are designed to understand and interact with human language, have demonstrated remarkable capabilities in acting as agents \citep{HuggingGPT, VisualProgramming}, engaging with a broad range of AI models to address various multi-modal challenges. While LLMs are regarded as effective multi-modal task solvers, an open question remains: can these models also become \textit{creators} of rich, dynamic multimedia content? 

Multimedia content creation involves digital media production in multiple forms, such as text, images, and audio. As a vital element of multimedia, audio not only provides context and conveys emotions but also fosters immersive experiences, guiding auditory perception and engagement. In this work, we address a novel problem of \textit{compositional audio creation with language instructions}, which aims to automatically design an audio storyline based on a given text instruction and further create corresponding audio content using various sound elements such as speech, music, and sound effects. Prior works have leveraged generative models to synthesize audio context that aligns with task-specific conditions, such as speech transcriptions \citep{Librispeech}, music descriptions \citep{MusicLM}, and audio captions \citep{AudioCaps}. However, the capabilities of these models to generate audio beyond such conditions are often limited, falling short of the demand for audio creation in real-world scenarios. In light of the integrative and collaborative capabilities of LLMs, it is intuitive to ask: can we leverage the potential of LLMs and these expert audio generation models for compositional audio creation?

Compositional audio creation presents inherent challenges due to the complexities of synthesizing intricate and dynamic auditory scenes. Harnessing LLMs for compositional audio creation presents a range of challenges: 
1) \textit{Contextual comprehension and design}: 
 using LLMs for compositional audio creation requires them not only to comprehend textually described auditory scenes but also to design audio scripts featuring speech, music, and sound effects. How to expand the capabilities of LLMs in text generation to audio storytelling is a challenge;
2) \textit{Audio production and composition}: unlike for vision and language data, an audio signal is characterized by dynamic spatio-temporal relationships among its constituent sound elements. Leveraging LLMs for integrating various audio generation models to produce sound elements and further compose them into a harmonious whole presents additional challenges;
3) \textit{Interactive and interpretable creation}:
Establishing an interpretable pipeline to facilitate human engagement is critical in automated audio production, as it enhances creative control and adaptability, fostering human-machine collaboration. However, designing such an interactive and interpretable creation pipeline with LLMs remains an ongoing question.

We introduce \textit{WavJourney}, a system that leverages LLMs for compositional audio creation guided by language instructions. Specifically, WavJourney first prompts LLMs to generate an audio script, which follows the format of a pre-defined structure. These audio scripts encompass the contexts of speech, music, and sound effects while considering their spatio-temporal relationship. To handle intricate auditory scenes, WavJourney decomposes them into individual acoustic elements along with their acoustic layout. By inputting the audio script into a script compiler, WavJourney generates a computer program where each line of code invokes a task-specific audio generation model, audio I/O functions, or computational operation functions. The computer program is subsequently executed to create the audio content. The overview of WavJourney with an example illustrates a science fiction scenario is shown in Figure \ref{fig:main}.

The design of WavJourney offers multiple benefits: as it is 
1) \textit{Creative}, by leveraging the understanding capability and generalizable knowledge of LLMs, WavJourney can design audio scripts with audio storyline, diverse sound elements, and intricate acoustic relationships; 
2) \textit{Compositional}, benefiting from its composable design, WavJourney can automatically decompose complex auditory scenes into independent sound elements, enabling the integration of various task-specific audio generation models to create audio content. Our approach differs from previous methods~\citep{AudioLDM, huang2023make}, where end-to-end generation often fails to fully present elements described in text descriptions;
3) \textit{Training-free}, as WavJourney eliminates the need for training audio models or fine-tuning LLMs, making it resource-efficient;
4) \textit{Interactive}, The interpretability offered by both the audio script and computer program facilitates audio producers with varying expertise to engage with WavJourney, fostering human-machine co-creation in real-world audio production. WavJourney advances the research of audio creation beyond traditional task-specific conditions and opens up new avenues for creativity in AIGC.


Our contributions are summarized as follows:
\begin{itemize}
    \item We present WavJourney, a system that leverages LLMs for compositional audio creation. Given text instructions, WavJourney can create audio content with storylines encompassing speech, music, and sound effects, without the need for additional training.
    \item Through its decomposition paradigm, WavJourney is capable of synthesizing audio content aligned with complex text descriptions. The decomposed components presented in both the audio script and computer program provide WavJourney with interactive and interpretable rationale, facilitating human-machine co-creation.
    \item WavJourney demonstrates practicality across diverse real-world scenarios, including Science Fiction, education, radio play, and the AudioCaps \citep{AudioCaps} dataset. To foster further research in AIGC for audio, we will release the code and software at: \url{https://github.com/Audio-AGI/WavJourney}.

\end{itemize}

\section{Related work}

\textbf{Large Language Models (LLMs)}.
LLMs such as GPT-3 \citep{GPT3}, LLaMA \citep{llama}, and ChatGPT \citep{ChatGPT} have advanced the research area in natural language processing (NLP) due to their capability to generate human-like text. Recently, LLMs have emerged as agents, showcasing their capability to address intricate AI tasks by integrating a range of domain-specific AI models. ViperGPT \citep{ViperGPT} and VisProg \citep{VisualProgramming} have demonstrated the significant promise of LLMs in decomposing complex vision-language tasks such as visual reasoning and text-to-image generation. These methods can generate a computer program (e.g., Python code) for each decomposed sub-task, which is executed sequentially to offer an explainable task solution. HuggingGPT \citep{HuggingGPT} leverages ChatGPT \citep{ChatGPT} as a controller to manage existing AI models in HuggingFace \citep{HuggingFace} for solving AI tasks in the domain of language, vision, and speech. Similar to HuggingGPT, AudioGPT \citep{AudioGPT} connects multiple audio foundation models for solving tasks with speech, music, sound understanding, and generation in multi-round dialogues. In the context of existing research, considerable efforts have been dedicated to leveraging LLMs' integrative and collaborative capabilities to solve multi-modal tasks effectively. However, there remains a relatively unexplored area concerning the potential of LLMs in audio content creation, despite its significance in advancing AIGC.


\textbf{Audio Content Creation}. The process of audio creation is complex and dynamic, involving various components such as content design, music composition, audio engineering, and audio synthesis. Traditional methods have relied on human-involved approaches such as field recording \citep{gallagher2015field}, Foley art \citep{wright2014footsteps}, and music composition \citep{tokui2000music} co-existing with digital signal processing modules \citep{reiss2011intelligent}. In recent years, the intersection of AI and audio creation has gained significant attention. AI-driven approaches, particularly generative models, have demonstrated remarkable capabilities in synthesizing audio content for speech \citep{NaturalSpeech, wang2017tacotron, wu2023speechgen}, music \citep{MusicLM, MusicGen}, sound effects \citep{AudioLDM, huang2023make, liu2021conditional, yuan2023text}, or specific types of sounds, such as footsteps or violin \citep{bresin2010expressive, engel2020ddsp}. Existing audio generation models primarily focus on synthesizing audio content based on a particular type of condition, such as speech transcriptions \citep{Librispeech}, music descriptions \citep{MusicLM}, or audio captions \citep{AudioCaps},  and they are not designed to generate compositional audio content with speech, music, and sound effects. Leveraging data-driven approaches for addressing compositional audio creation is resource-intensive and impractical. It demands the collection of a sophisticated audio dataset with corresponding text annotations, as well as the training of powerful audio models.


\section{WavJourney}
WavJourney is a collaborative system composed of an audio script writer utilizing LLMs, a script compiler and a set of audio generation models such as zero-shot text-to-speech\footnote{Zero-shot text-to-speech (TTS) refers to the ability of a TTS system to generate speech in a voice that has not been explicitly trained on, given an unseen voice preset as a condition. }, text-to-music, and text-to-audio generation models. The overall architecture is illustrated in Figure \ref{fig:main}. The pipeline of WavJourney can be deconstructed into two major steps: 1) \textbf{audio script generation}: given a text instruction, the audio script writer initiates the process by warping the input instruction with specific prompts. Then, the LLM is engaged with these prompts, which directs it to generate an audio script conforming to the structured format. 2) \textbf{script compiling and program execution}: Subsequently, the script compiler transcribes the audio scripts into a computer program. The computer program is further executed by calling the APIs of expert audio generation models to create audio content. We describe the details of these two steps in the following sections.

\subsection{Audio Script Generation}\label{sec:3-1}
The first challenge in harnessing LLMs for audio content creation lies in generating an audio narrative script based on the input text instructions that often only contain conceptual and abstract descriptions. Recognizing that LLMs have internalized generalizable text knowledge, we utilize LLMs to expand input text instructions into audio scripts, including detailed descriptions of decomposed acoustic contexts such as speech, music, and sound effects. To handle the  spatio-temporal acoustic relationships, we prompt LLMs to output the audio script in a structured format composed of a list of JSON nodes. Each JSON node symbolizes an audio element, including acoustic attributes (e.g., duration and volume). In this way, a complex auditory scene can be decomposed into a series of single acoustic components. Then, we can create the desired audio content by leveraging diverse domain-specific audio generation models. We introduce these details in the following paragraphs.

\textbf{Format of Audio Script.} We define three types of audio elements: speech, music, and sound effects. For each audio element, there are two types of layouts: foreground and background. Foreground audio components are concatenated sequentially, i.e. no overlap with each other. Background audio on the other hand can only be played along with some foreground elements, i.e. they can not be played independently (overlaps of background audio are allowed). Sound effects and music can either be foreground or background, while speech can only be foreground. The concepts above are reflected in the format of a list consisting of a series of JSON nodes, with each node embodying a unique audio component. Each node is supplemented with a textual description of its content. To enhance the auditory experience, each audio component is assigned a volume attribute. Sound effects and musical elements are furnished with an additional attribute pertaining to length, facilitating control over their durations. For speech components, a character attribute is assigned. This attribute enables the synthesis of personalized voices in the later stage, thereby enhancing the narrative thread of the audio storyline. This layered structuring and detailing of audio elements contribute to creating rich and dynamic audio content. We observe that the outlined format is able to cover a wide variety of auditory scenarios and its simplified list-like structure facilitates the LLM's understanding of complex auditory scenes. An example of the audio script is shown in the Listing \ref{1st:json}.

\textbf{Personalized Voice Setting.} The advances in zero-shot TTS have facilitated the synthesis of personalized speech based on specific voice presets. In WavJourney, we leverage zero-shot TTS to amplify the narrative depth of audio storytelling, utilizing a set of voice presets tailored for diverse scenarios. More specifically, each voice preset is manually annotated with descriptions of its characteristics and appropriate application scenarios. In subsequent stages, we can leverage LLM to allocate a suitable voice from the preset to each character outlined in the audio script. We utilize a simple prompt design to facilitate the voice allocation process, as discussed in the next paragraph. This design enables WavJourney to create personalized auditory experiences.

\textbf{Prompt Strategy.}
To generate rich and formatted audio scripts from given text instructions, we wrap the text instruction within a prompt template. The prompt template contains the specifications of each JSON node type, including audio type, layout, and attributes. The prompt template is shown in Table \ref{tab:prompt1} in Appendix \ref{apx-1}. The instructions listed in the prompt template facilitate formatting the audio script generated by LLMs. In the next step, following the receipt of the generated audio scripts, we instruct the LLM to parse the characters outlined in each speech node into personalized speech presets. The prompt used for voice parsing is shown in Table \ref{tab:prompt2} in Appendix \ref{apx-1}. The audio script with parsed voice mapping is further processed through the script compiler, generating the code for subsequent stages.  

\subsection{Script Compiling and Program Execution}\label{sec:3-2}
WavJourney employs a script compiler to automatically transcribe the audio script into a computer program. The pseudo-code of the script compiler is shown in Algorithm \ref{algo1}. Each line of code in the program invokes a task-specific audio generation model, audio I/O function, or computational operation function (e.g., mix, concatenate). The program is subsequently executed, resulting in an explainable solution for compositional audio creation. Diverging from previous studies that utilize LLMs to generate code \citep{ViperGPT, VisualProgramming}, WavJourney prompts LLMs to generate textual audio scripts, which foster improved comprehension for audio producers without programming expertise. Additionally, the process of crafting a computer program to compose intricate auditory scenes requires an elaborate series of procedures within the code to manage audio length calculations and the mixing or concatenation operations of audio clips. Given the unpredictability of LLMs, there are occasions when they may fail to adhere to specified instructions during the generation process \citep{HuggingFace}. By introducing a script compiler, we can mitigate the potential exceptions in the program workflow arising from the instability of LLMs, thereby reducing the uncertainty during inference.


\subsection{Human-Machine Co-Creation}\label{Sec:HCI}
The natural interface design of the audio script enables audio producers to engage in automated audio creation actively. By leveraging the communication capabilities of LLMs, WavJourney allows users to customize the audio content through multi-round dialogues. We perform several case studies on the AudioCaps dataset. The results are shown in Appendix \ref{apx:co-creation}. Initially, WavJourney was requested to synthesize the audio content conditioned on the provided text descriptions, followed by dynamic changes to the audio elements through multi-round dialogues, such as adding new elements or modifying acoustic attributes. The successful execution of these tasks by WavJourney demonstrates its great promise in interactive audio creation. WavJourney enhances creative control and adaptability, fostering a promising human-machine co-creation in real-world audio production.

\section{Experiments}
\subsection{Experimental Setup}
We utilize the GPT-4 model \citep{ChatGPT} as the LLMs for WavJourney. For text-to-music and text-to-audio generation, we adopt the publicly-available state-of-the-art models MusicGen \citep{MusicGen} and AudioLDM \citep{AudioLDM}, respectively. As for text-to-speech synthesis, we leverage the Bark \citep{Bark} model, which can generate realistic speech and is able to match the tone, pitch, emotion, and prosody of a given voice preset. To enhance the quality of synthesized speech, we apply the speech restoration model VoiceFixer \citep{liu2022voicefixer} after the Bark model. We use \num{32} kHz sampling rate for processing audio signals. We implement the computer program in the Python language. For the volume control of the generated audio content, we adopt the Loudness Unit Full Scale (LUFS) standard \citep{steinmetz2021pyloudnorm}.

\subsection{Case Study on AudioCaps Benchmark}
We performed a case study on the AudioCaps~\citep{AudioCaps} dataset, which contains $46,000$ audio-text pairs. We randomly select several cases with complex captions (e.g., more than 15 words, containing at least three sound events) to demonstrate the effectiveness of WavJourney, compared with state-of-the-art text-to-audio generation systems AudioLDM~\citep{AudioLDM} and Tango~\citep{ghosal2023text}. The results and comparison are shown in Appendix \ref{apx-2}. We highlight each sound event in the spectrogram using the colored line underneath per spectrogram. The output events of the WavJourney-generated-audio are in the chronological order depicted by the corresponding text (e.g., two events happen simultaneously when instructed by the word ``as" or one sound is behind another when the word ``follow" appears). Compared to AudioLDM and Tango systems, WavJourney succeeds in generating all sound events mentioned in the text description and scheduling the sound events in the proper temporal relationship, indicating its capacity for controllable audio generation.

\subsection{Case Study on Real-world Scenarios}
We conducted additional case studies using WavJourney in various scenarios, including Science Fiction, education, and radio play. The Appendix \ref{apx-3} shows examples of the generated audio script and audio content. The outcomes achieved demonstrate the practicality of WavJourney in real-world audio production applications and indicate its potential in facilitating audio content creation.


\section{Limitations}
Although WavJourney can create audio content with text instructions, limitations could be observed as follows: 1) Extensibility: WavJourney relies on structured audio scripts to represent auditory scenes and script compilers to generate computer programs,  which is inflexible to expand their capabilities; 2) Artificial composition: The process of decomposing and re-mixing audio may result in synthetic audio that deviates from real-world sound distributions, particularly concerning music composition, which requires the alignment of beats, chord progression, and melody in multiple music tracks;  3) Efficiency: The reliance of WavJourney on LLMs and multiple audio generation models introduces significant time costs when generating complex audio content. Improving the efficiency of WavJourney could facilitate its practicality for real-world applications.

\section{Conclusion}
In this work, we presented WavJourney, which connects LLMs with diverse expert audio generation models, enabling compositional audio creation via natural language instructions. WavJourney can schedule various expert audio generation models to create audio content by decomposing auditory scenes into individual audio elements with their acoustic relationships. Case studies conducted on the AudioCaps benchmark and several real-world scenarios have demonstrated the practicality of WavJourney in audio production applications.  WavJourney opens up new avenues for advancing AIGC in the realm of audio content creation.

\subsubsection*{Acknowledgments}
This work is partly supported by UK Engineering and Physical Sciences Research Council (EPSRC) Grant EP/T019751/1 ``AI for Sound'' and Grant EP/T518086/1, British Broadcasting Corporation Research and Development~(BBC R\&D), a PhD scholarship from the Centre for Vision, Speech and Signal Processing, Faculty of Engineering and Physical Science, University of Surrey and a Grant ``XJTLU RDF-22-01-084''. For the purpose of open access, the authors have applied a Creative Commons Attribution (CC BY) license to any Author Accepted Manuscript version arising.

\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}

\appendix
\newpage
\section{Appendix}
\subsection{Prompt Templates}\label{apx-1}

\begin{table*}[ht]
\begin{tabular}{@{}l@{}}
\toprule
\multicolumn{1}{c}{Prompt \#1}                                          \\ \midrule
\begin{tabular}[c]{@{}l@{}}I want you to act as an audio script writer. I'll give you an instruction which is a general idea \\ and you will make it an audio script in List format containing a series of JSON nodes. \\ \\ The script must follow the rules below:\\ \\ Each line represents an audio JSON node. There are three types of audio: sound effects, music, \\ and speech. For each audio, there are two types of layouts: foreground and background. Fore-\\ ground audios are played sequentially, and background audios are sound effects or music which \\ are played while the foreground audio is being played. \\ \\ Sound effects can be foreground or background. For sound effects, you must provide its layout, \\ volume (dB, LUFS standard), length (in seconds), and detailed description of the sound effect. \\ Example: \{``audio\_type": ``sound\_effect", ``layout": ``foreground", ``vol": -35, ``len": 2, ``desc": \\ ``Airport beeping sound"\}\\ \\ Music can be foreground or background. For music, you must provide its layout, volume (dB, \\ LUFS standard), length (in seconds), and detailed description of the music. \\ Example: \{``audio\_type": ``music", ``layout": ``foreground", ``vol": -35, ``len": 10, ``desc": ``Up-\\ lifting newsroom music"\}\\ \\ Speech can only be foreground. For speech, you must provide the character, volume (dB, LUFS \\ standard), and the character's line. You do not need to specify the length of the speech. \\ Example: \{``audio\_type": ``speech", ``layout": ``foreground", ``character":``"News Anchor", \\ ``vol": -15, ``text": ``Good evening, this is BBC News"\}\\ \\ For background sound effects, you must specify the id of the background sound effect, and you \\ must specify the beginning and the end of a background sound effect in separate lines, hence \\ you do not need to specify the length of the audio. \\ Example: \{``audio\_type": ``sound\_effect", ``layout": ``background", ``id":1, ``action": ``begin", \\ ``vol": -35, ``desc": "Airport ambiance"\} ... \{``audio\_type": ``sound\_effect", ``layout": ``back-\\ ground", ``id":1, ``action": "end"\}\\ \\ For background music, it's the same as background sound effects.\\ \\ The output format must be a list of the root node containing all the audio JSON nodes.\end{tabular} \\ \bottomrule
\end{tabular}
\caption{Prompt template used for generating audio scripts.}
\label{tab:prompt1}
\end{table*}

\begin{table*}[ht]
\begin{tabular}{@{}l@{}}
\toprule
\multicolumn{1}{c}{Prompt \#2}                                                          \\ \midrule
\begin{tabular}[c]{@{}l@{}}
Given an audio script in json format, for each character that appeared in the ``character" \\attribute, you should map the character to a ``voice type" according to their lines and\\ the ``voice types" features. Each character must be mapped to a different voice type, and\\ each ``voice type" must be from one of the following:\\\\ - Female1: a normal female adult voice, British accent \\- Female2: a normal female adult voice, American accent \\- Female3: a female adult voice, suitable for commentary and narrator \\- Male1: a normal male adult voice, British accent \\- Male2: a normal male adult voice, American accent \\- Male3: a male adult voice, suitable for commentary and narrator \\- BBC\_Female: a female voice of a news anchor, suitable for news scenarios \\- BBC\_Female\_Out: a female voice of an off-site news reporter, suitable for news scenario \\- BBC\_Male: a male voice of a news anchor, suitable for news scenarios \\ \\\\ Output should be in the format of CSV, like:\\ ```\\ {[}character 1{]}, {[}voice type 1{]}\\ {[}character 2{]}, {[}voice type 2{]}\\ ...\\ '''\end{tabular} \\ \bottomrule
\end{tabular}
\caption{Prompt template used for voice parsing.}
\label{tab:prompt2}
\end{table*}


\newpage
\subsection{Example of Audio Script}
\begin{lstlisting}[language=json,caption={Example audio script in the list form.},captionpos=b,label=1st:json]
AudioScript = [
    {"audio_type": "music", "layout": "background", "id":1, "action": "begin", "vol": -30, "desc": "Dramatic orchestral news theme"},
    {"audio_type": "speech", "layout": "foreground", "character": "News Anchor", "vol": -15, "text": "Welcome to Mars News ..."},
    {"audio_type": "music", "layout": "background", "id":1, "action": "end"},
    {"audio_type": "sound_effect", "layout": "foreground", "vol": -35, "len": 1, "desc": "Transition swoosh"},
    {"audio_type": "speech", "layout": "foreground", "character": "Reporter", "vol": -15, "text": "We're here at the ..."},
    ...
    {"audio_type": "speech", "layout": "foreground", "character": "News Anchor", "vol": -15, "text": "... Stay tuned to Mars News for the latest updates."},
    {"audio_type": "music", "layout": "foreground", "vol": -30, "len": 5, "desc": "orchestral news outro music"}
]
\end{lstlisting}

\newpage
\subsection{Script Compiler}
\begin{algorithm}[H]\label{algo1}
\DontPrintSemicolon
\SetKwInOut{Initialize}{Initialize}
\Initialize{foregroundAudioList as an empty list, backgroundAudioList as an empty list, nextForegroundNodeID as 0}
\ForAll{node in the root list of the json file}{
    \eIf{node.type is "foreground"}{
        Add node to foregroundAudioList\;
        Increment nextForegroundNodeID by 1\;
    }{
        \eIf{node.isBeginning is True}{
            Set node.beginForegroundID as nextForegroundNodeID\;
            Add node to backgroundAudioList\;
        }{
            Set backgroundNode in backgroundAudioList with id node.id's endForegroundID as nextForegroundNodeID\;
        }
    }
}
\Initialize{fgAudioLengths as an empty list}
\ForAll{foregroundAudio in foregroundAudioList}{
    Generate the audio based on foregroundAudio\;
    Calculate the generated length, append it to fgAudioLengths\;
}
Concatenate all generated foreground audio to create finalForegroundAudio\;

\ForAll{backgroundAudio in backgroundAudioList}{
    Calculate targetLength by using backgroundAudio's beginForegroundID, endForegroundID and fgAudioLengths\;
    Generate the audio of targetLength based on backgroundAudio\;
    Calculate the offset from the beginning of the finalForegroundAudio using beginForegroundID and fgAudioLengths, set as offset\;
}

\Initialize{finalAudio as finalForegroundAudio}
\ForAll{backgroundAudio in backgroundAudioList}{
    Mix finalAudio with backgroundAudio generated audio, with offset according to backgroundAudio.offset\;
}
Output finalAudio\;
\caption{Pseudo code of Script Compiler}
\end{algorithm}


\newpage
\subsection{Human-Machine Co-Creation}
\label{apx:co-creation}
% Figure environment removed

\newpage
\subsection{Case Study on AudioCaps Benchmark}\label{apx-2}
\subsubsection{Case One}
% Figure environment removed

\newpage
\subsubsection{Case Two}
% Figure environment removed


\newpage
\subsection{Case Study on Real-World Scenarios}\label{apx-3}
\subsubsection{Case One - Radio Play}
Text instruction: \textit{``write a radio drama for a love comedy: a couple is on a date in a fine restaurant, and suddenly, then suddenly something happened, and the atmosphere was totally ruined."}

The generated audio script is presented in Figure \ref{fig:radio_play}.

The generated audio clip is available at \url{https://github.com/Audio-AGI/WavJourney/examples/radio\_play.wav}

% Figure environment removed
\newpage
\subsubsection{Case Two - Education}
Text description: \textit{``Generate a one-minute introduction to quantum mechanics by a professor."}

The generated audio script is presented in Figure \ref{fig:education}.

The generated audio clip is available at \url{https://github.com/Audio-AGI/WavJourney/examples/education.wav}
% Figure environment removed



\end{document}



