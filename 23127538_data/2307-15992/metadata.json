{
  "title": "Towards Codable Watermarking for Injecting Multi-bits Information to LLMs",
  "authors": [
    "Lean Wang",
    "Wenkai Yang",
    "Deli Chen",
    "Hao Zhou",
    "Yankai Lin",
    "Fandong Meng",
    "Jie Zhou",
    "Xu Sun"
  ],
  "submission_date": "2023-07-29T14:11:15+00:00",
  "revised_dates": [
    "2023-11-27T08:30:00+00:00",
    "2024-04-03T04:36:40+00:00"
  ],
  "abstract": "As large language models (LLMs) generate texts with increasing fluency and realism, there is a growing need to identify the source of texts to prevent the abuse of LLMs. Text watermarking techniques have proven reliable in distinguishing whether a text is generated by LLMs by injecting hidden patterns. However, we argue that existing LLM watermarking methods are encoding-inefficient and cannot flexibly meet the diverse information encoding needs (such as encoding model version, generation time, user id, etc.). In this work, we conduct the first systematic study on the topic of Codable Text Watermarking for LLMs (CTWL) that allows text watermarks to carry multi-bit customizable information. First of all, we study the taxonomy of LLM watermarking technologies and give a mathematical formulation for CTWL. Additionally, we provide a comprehensive evaluation system for CTWL: (1) watermarking success rate, (2) robustness against various corruptions, (3) coding rate of payload information, (4) encoding and decoding efficiency, (5) impacts on the quality of the generated text. To meet the requirements of these non-Pareto-improving metrics, we follow the most prominent vocabulary partition-based watermarking direction, and devise an advanced CTWL method named Balance-Marking. The core idea of our method is to use a proxy language model to split the vocabulary into probability-balanced parts, thereby effectively maintaining the quality of the watermarked text. Our code is available at https://github.com/lancopku/codable-watermarking-for-llm.",
  "categories": [
    "cs.CL"
  ],
  "primary_category": "cs.CL",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.15992",
  "pdf_url": "https://arxiv.org/pdf/2307.15992v3",
  "comment": "ICLR 2024 poster",
  "num_versions": null,
  "size_before_bytes": 4791843,
  "size_after_bytes": 814739
}