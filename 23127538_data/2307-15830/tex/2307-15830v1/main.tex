%% 
%% Copyright 2007-2020 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 

%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%%
%% 
%%
%% $Id: elsarticle-template-num.tex 190 2020-11-23 11:12:32Z rishi $
%%
%%
\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%%\documentclass[final,1p,times]{elsarticle}
%%\documentclass[final,1p,times,twocolumn]{elsarticle}
%%\documentclass[final,3p,times]{elsarticle}
%%\documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}

\usepackage{caption}
\usepackage{subcaption}
\usepackage{soul,color}

%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

%\journal{Neurocomputing}

\begin{document}

\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%% \fntext[label3]{}

\title{A Distance Correlation-Based Approach to Characterize the Effectiveness of Recurrent Neural Networks for Time Series Forecasting}

% Alternative Titles
%How Distance Correlation Can Enhance Understanding of RNN-Based %Networks for Time Series Forecasting

%Limitations of RNN-Based Networks for Time Series Forecasting Tasks: Insights from Distance Correlation Analysis

\author[1]{Christopher Salazar}
\author[1,2]{Ashis G. Banerjee \corref{cor1}}

\address[1]{Department of Industrial \& Systems Engineering, University of Washington, Seattle, WA 98195, USA}
\address[2]{Department of Mechanical Engineering, University of Washington, Seattle, WA 98195, USA}

\cortext[cor1]{Corresponding author; Email address: ashisb@uw.edu}

\begin{abstract}
%% Text of abstract
Time series forecasting has received a lot of attention %seen several modeling methods 
with recurrent neural networks (RNNs) being one of the widely used models 
%among the most popular 
due to their ability to handle sequential data. Prior studies of 
%Researchers have systematically studied 
RNNs for time series forecasting 
%and found widely 
yield inconsistent results with 
%. Their conclusions offer 
limited insights as to why the performance varies for different datasets.
%, indicating that an improved analysis tool is needed.} 
In this paper, we provide an approach to link the characteristics of time series with the components of RNNs via the versatile metric of distance correlation. %With distance correlation, we are able 
This metric allows us to %use it as an information flow measure that helps us 
examine the information flow through the RNN activation layers to be able to interpret and explain their performance. 
%Using this general framework, 
We empirically show that the RNN activation layers learn the lag structures of time series well. However, they gradually lose this information over a span of a few consecutive layers, 
%as \hl{it progresses through the network}, 
thereby worsening the forecast quality for series with large lag structures. We also show that the activation layers cannot adequately model moving average and heteroskedastic time series processes. Last, we 
%re-purpose distance correlation to 
generate heatmaps for visual comparisons of the activation layers for different choices of the network hyperparameters to identify which of them affect the forecast performance. Our findings can, therefore, aid practitioners in assessing the effectiveness of RNNs for given time series data without actually training and evaluating the networks. %\hl{Further, %we believe that employing 
%our framework %with deep learning models 
%can play a crucial role in developing a fundamental understanding of deep learning forecasting tasks.} 

\end{abstract}

%%Graphical abstract
%\begin{graphicalabstract}
%% Figure removed
%\end{graphicalabstract}

%%Research highlights
%\begin{highlights}
%\item %Systematic studies of RNNs and time series forecasting have yielded inconsistent results, whose insights are limited to explainability of chosen hyperparameters or domain specific hypotheses
%Prior studies of RNNs for time series forecasting yield inconsistent results
%\item We use distance correlation
%%is a metric that 
%to measure information flow 
%%of time series data 
%through RNN activation layers
%%, allowing us to interpret RNN behavior
%\item Activation layers learn time series lag structures, but gradually forget this information
%\item Activation layers struggle with modeling moving average and heteroskedastic processes
%\item Forecasts depend on input size, but not on activation function or layer output size
%%Distance correlation heatmaps can be used to visualize RNN activation layer comparisons. This helped us determine that number of hidden units or activation function type is less sensitive to performance compared to RNN input size. 
%\end{highlights}

\begin{keyword}
Recurrent Neural Network, Time Series Forecasting, Distance Correlation
\end{keyword}

\end{frontmatter}

%% \linenumbers

%% main text
\section{Introduction}
\label{sec: Intro}

%% Intro about time Series forecasting
Time series forecasting is a long-standing subject that continues to challenge both academic researchers and industry professionals. Accurate forecasting of real-world operations and processes, such as weather conditions, stock market prices, 
%climate change, 
supply chain demands and deliveries, and so on, %other domains that include time series related tasks remains an 
are still open problems that would benefit from reliable and interpretable methods.
%open-ended question that lacks reliable methods.
%for obtaining accurate forecasts.} 
The challenges include 
%It is a complex subject mired in undesirable characteristics such as 
data non-stationarity and seasonality, limited data availability with missing observations, and the inherent uncertainty of real-world events. These challenges require practitioners to have a thorough understanding of both the time series characteristics and the techniques to develop high performance models.
%subject/topic; question/problem?

%% Current methods are plentiful, but poorly understood
As the practitioners grapple with these challenges, several modeling (forecasting) techniques have been developed, which include classical statistical prediction models, such as autoregressive integrated moving average (ARIMA), statistical learning models, and deep neural networks. 
%While all these techniques exhibit varying degrees of success,  
%Given the recent advances in computer vision and natural language processing, using deep learning architectures for forecasting is an active space. 
As might be expected, there has been a surge of interest in the use of deep learning models for forecasting. This is illustrated in the recent work by Zhang \textit{et al.} \cite{ZHANG2023}, where they bridge the idea of localized stochastic sensitivity to recurrent neural networks (RNNs) to induce robustness in their prediction tasks. Other works, such as the information-aware attention dynamic synergetic network \cite{HE2022143}, use multi-dimensional attention to create a novel multivariate forecasting model. The rapid emergence of diverse and complex deep learning models for forecasting has provided practitioners with numerous options. However, it has also left many of them unsure of which models to choose and how to interpret their successes and limitations.
%model/architecture?

%% RNNs are appropriate, but only evaluated based on accuracy and no internal study has been conducted
Despite the lack of agreement in deep learning model selection, RNNs are popular candidates for time series forecasting. This is due to their ability to handle sequential data, which is a fundamental characteristic of time series. Yet, through a series of several experiments, RNNs and their adaptations \cite{lai2018modeling, huang2019dsanet, zhou2021informer, lim2021time} have shown a wide range of forecasting results, making it difficult to evaluate their effectiveness. Experimental reviews and surveys have highlighted both the scenarios, one in which RNNs have achieved highly accurate results \cite{lara2021experimental, sezer2020financial, gasparin2022deep, yan2018financial, wegayehu2022short}, and another in which they have been surpassed by traditional statistical regression models such as ARIMA \cite{bousqaoui2021comparative} and gradient boosting trees \cite{elsayed2021we}. Despite the breadth of these studies, their conclusions offer limited insights on the variations in RNN performance, often deferring to the explainability of the chosen hyperparameters or domain-specific hypotheses. These outlooks can be attributed to the restricted perspective offered by generalization error, which is the primary way of evaluating these models. While the generalization error is important to track, it does not delve into the internal mechanism of how time series inputs evolve through the RNN layers, limiting our understanding of this subject. As it stands, an evaluation tool that allows us to comprehend the inner workings of RNNs on a layer-by-layer basis is needed. 

%% Distance correlation and its advantage to help understand RNN
While there are several evaluation metrics to measure mathematical and statistical relationships, many lack the flexibility that is needed for studying the components of RNNs. A recently proposed statistical dependency metric, called distance correlation \cite{szekely2013distance}, 
has a few key advantages %that make it useful 
in this regard. First, it has the ability to capture the non-linear relationships 
%which are 
present in typical RNN operations. Additionally, it can compare random variables of different dimensions which can vary in RNN architectures. It can also be used as a proximal metric that measures the information stored in the RNN activation layers through its training cycle \cite{zhen2022versatile, vepakomma2020nopeek}.
These factors make distance correlation well suited for a detailed characterization of RNN models based on their forecast performance.
%interpretability-based studies of deep learning models. 
%Further, distance correlation allows us to examine RNN architectures in the context of time series forecasting from a layer-by-layer perspective, going beyond the limited viewpoint of generalization errors.

%  Contributions 
Given this context, we make the following contributions to the deep learning-based time series forecasting literature in this paper.
%propose the following contributions to the deep learning and time series forecasting communities. 
First, we outline a distance correlation-based approach for evaluating and understanding time series modeling %RNN behavior 
at the RNN activation layer level. 
%with time series. 
%Using this framework, 
Consequently, we demonstrate that the activation layers %can 
identify time series lag structures well. However, they quickly lose this important information over a sequence of a few layers, posing difficulties in modeling time series with large lag structures.  
%needed to generate accurate forecasts. This loss of information is worsened with increasing lag structure. 
Further, our experiments show that the activation layers cannot adequately model moving average and heteroskedastic processes.
%time series and data exhibiting volatility. 
Last, we employ distance correlation to visualize heatmaps that compare RNNs with varying hyperparameters. These heatmaps show that certain hyperparameters, such as the number of hidden units and activation function, do not influence the activation layer outputs as much as the RNN input size. A visual overview of our work is shown in Figure \ref{fig:overall_meth}. 
%Ashis: This paragraph probably needs to be improved in the revised version. 

% Figure environment removed

\section{Related Works}
\label{sec: rela_work}


%% Our current theoretical understanding of NN and RNNs
Neural networks, despite their tremendous success, are not fully understood from a theoretical perspective. Some strides have been made, such as the work by Telgarsky \cite{telgarsky2016benefits}, where the greater approximation power of deeper networks is proved for a class of nodes that includes ReLU and piecewise polynominal functions. Rolnick's work \cite{rolnick2018towards} extended this rigorous understanding of neural networks by establishing the essential properties of expressivity, learnability, and robustness. Bahri \textit{et al.} \cite{bahri2020statistical} recently reviewed a breadth of research 
%aimed at understanding 
on the connections between deep neural networks and several principles of statistical mechanics to obtain a better conceptual understanding of deep learning capabilities. To estimate the overall complexity and learning ability of deep neural networks without actual training and testing, 
%Other lines of research focus on characterizing deep learning architectures to measure their overall complexity and ability to learn given problems.} For example, 
Badias and Banerjee proposed a new layer algebra-based framework to measure the intrinsic capacity and compression of networks, respectively \cite{badias2023neural}. These measurements enabled us to analyze the performance (accuracy) of state-of-the art networks on benchmark computer vision datasets. 
%Based on their concept of layer algebra, they measured the global complexity properties of many neural networks that are advantageous over the widely used Vapnik–Chervonenkis (VC) dimension. 

Specifically for RNNs, one of the more well-known drawbacks is their inability to retain information from long term sequences, as found by Bengio \textit{et al.} while using gradient-based algorithms for training RNNs \cite{bengio1994learning}. 
%\hl{found that gradient-based algorithms for training RNNs of long span inputs lead to inefficiencies and difficulties latching on to the long term behavior.} 
This finding led to the use of alternate RNN architectures, such as the Long Short-Term Memory (LSTM) network \cite{hochreiter1997long}, despite their increased computational complexity. 
%More related towards RNNs, 
More recently, Chen \textit{et al.} \cite{chen2019generalization} established generalization bounds for RNNs and their variants based on the spectral norms of the weight matrices and the network size.  %% RNN short term memory
A different approach %centered around RNNs 
focused on studying the implicit regularization effects of RNN by injecting noise into the hidden states, which resulted in promoting flatter minima and overall global stability of RNNs \cite{lim2021noisy}. While all these approaches are theoretically rigorous, 
%Alhough many of these rigorous approaches theoretically ground deep learning models, 
they are typically formulated under rigid constraints, many of which are not necessarily satisfied in real-world applications. 
%impractical to satisfy during applied problems.}

%% Other research involving interpretability of RNN
%\hl{The examples above show that developing an understanding of RNNs from a theoretical perspective is nuanced and inflexibile to accommodate the advancements of novel architectures}. 
Therefore, several researchers have turned to interpretation based methods to develop a principled yet practically useful understanding of RNNs. In this context, since RNNs are common for natural language processing (NLP) tasks, many studies have focused on tracking the hidden memories of RNNs and their expected responses to the input text \cite{ming2017understanding, strobelt2017lstmvis}. For time series, Shen \textit{et al.} \cite{shen2020visual} developed a visual analytics system to interpret RNNs for high dimensional (multivariate) forecasting tasks. While this system is very useful, we aim to approach RNN interpretability from a different perspective of understanding performance (forecast) generalizability by tracking how the hidden memories respond to specific 
%observing particular 
time series characteristics.
%and tracking how hidden memories respond to them. 
%\hl{This allows us to make certain generalizations of RNN behavior given specific time series attributes. 
For this purpose, we look at alternate information-theoretic approaches. 


% The promising method of using MI for understanding RNNs
One of the most promising works on information-theoretic analysis 
%of research in deep neural networks 
was conducted by Schwartz and Tishby \cite{shwartz2017opening}. They used mutual information (MI) 
%of information theory 
to represent a deep neural network as a series of encoder-decoder networks, enabling analysis at the activation layer level. Such analysis goes beyond the typical generalization errors used for model assessment, and allows us to closely observe the information flow through deep learning networks such as RNNs.
% \hl{In general, they showed how input information is compressed in early training epochs, how stochastic gradient descent (SGD) evolves into a random diffusion process for representation compression, and lastly they recovered the information plane inequalities (or data processing inequality) by representing a deep neural network into a series of encoder-decoder networks.} Such \hl{findings provides} a feasible pathway for understanding neural networks beyond their typical generalization errors or outputs, as we are able to observe how information flows from layer to layer. Although this is a promising source of research within the deep learning community, issues arise quickly as we try to apply it towards a time series. 
%% Using MI is impractical, but we can build from this method
Although this approach is novel, %it is predicated on 
MI estimation is challenging for real-world data. 
%Despite this challenge, some efforts are pursued to estimate mutual information, 
To address this challenge, researchers have investigated different approaches, such as %developed most notably with 
a $k$-nearest neighbor method \cite{kraskov2004estimating} and a semi-parametric local Gaussian method \cite{gao2015efficient}. While these attempts %advances in estimating mutual information 
seem promising, there are questions regarding their practical usefulness. 
%others find them such methods unreliable. 
For example, McAllester and Stratos \cite{mcallester2020formal} argue that without any known probability characteristics of the data, any distribution-free, high-confidence lower bound on mutual information would require an exponential amount of samples. %As it pertains to time series, 
It is quite rare to find real world datasets with known (or well-fitting) distributions, 
%are quite rare 
and this is generally 
%unwieldy 
true for time series applications \cite{brockwell_1996}. Nevertheless, 
%while estimation of mutual information is challenging for real-world time series data, 
the underlying framework of analyzing RNNs from a layer-by-layer perspective is a useful concept %and something 
that we can build upon using an alternative metric, as discussed next. 


%% Distance correlation and its previous uses in time series forecasting
As we search for an approach 
%tool or method 
that affords the flexibility of analyzing the components of RNNs, Szekely \textit{et al.'s} \cite{szekely2007measuring} distance correlation comes to the forefront as a dependency measure. It is closely related to Pearson's correlation with the advantage of measuring non-linear relationships among the random variables. Naturally, some researchers directly apply it toward time series forecasting. Zhou's \cite{zhou2012measuring} work re-purposes distance correlation by proposing the auto-distance correlation function (ADCF), which is an extension of the auto-correlation function (ACF) \cite{brockwell_1996}. Yousuf and Feng \cite{yousuf2022targeting} use distance correlation as a screening method for deciding which lagged variables should be retained in a model. 

% how distance correlation is used for deep learning networks
Beyond time series applications, distance correlation has other practical uses in the deep learning space. Zhen \textit{et al.} \cite{zhen2022versatile} outline a process that uses distance correlation and partial distance correlation to compare deep learning models from a network layer level. They use it as a measure of information lost or gained in the network layers, where high distance correlation values indicate more information is stored in the activation layers. In a separate study, NoPeek \cite{vepakomma2020nopeek} uses a distance correlation method that decorrelates the raw input information within the activation layers during training, which is essential for preventing the leakage of sensitive data. These studies show the viability of distance correlation as an analysis tool in time series applications and deep learning, separately. Our work aims to connect the two topics to further our 
%undamental 
understanding of time series forecasting tasks.

\section{Methodology}

%Several concepts need to be defined before discussing the experimental results. 
In this section, we first characterize the time series forecasting problem before describing the components of an RNN and providing an overview of distance correlation. We then propose a method that links these core topics to further our understanding of time series forecasting using RNNs.

 \subsection{Time Series Forecasting}\label{sec:TS_Forecast}

% The general time series forecasting problem
As we define the forecasting problem, we adopt the notation used by Lara-Benitez et al. \cite{lara2021experimental} in their experimental review of deep learning models with forecasting tasks. We let $\textbf{Z} = z_{1}, z_{2}, \dots, z_{L}$ be the historical time series data of length $L$ and $H$ be the prediction horizon. The goal of a time series forecasting problem is to use $\textbf{Z}$ to accurately predict the next horizon values $z_{L+1}, z_{L+2}, \dots, z_{L+H}$. The time series data can either be univariate or multivariate; we focus this paper on univariate analysis where single and sequential observations are recorded over time.

% Why real datasets are difficult to use
One of the challenges in working with time series research is the lack of standardized datasets for forecasting. Many time series experimental reviews \cite{lara2021experimental, yan2018financial, wegayehu2022short} choose datasets from several domains with various characteristics. This variance across data makes it difficult to generalize the behavior of time series models since it is not clear whether each dataset represents a class of time series processes or is uniquely domain specific. This complicates the potential interpretability of the experiments and their corresponding conclusions since the underlying time series process is unknown. Instead, we choose to generate synthetic data to have control of time series structures and their expected outcomes.

%% Using ACF to establish baseline for time series data
Upon generating this synthetic data, it is important for us to establish a baseline of the time series structure. One of the primary methods for analyzing time series forecasting problems is by using the auto-correlation function (ACF) \cite{brockwell_1996}. This function looks at the linear correlations between $z_{l}$ and its previous values $z_{l-h}$ where $l = 1,2 \ldots, L$ at lag $h$. Formally, the auto-correlation function, $\rho_z(h)$, is defined as 
%the following: 
\begin{align}\label{eq: pcor}
    \rho_{z}(h) = Cor(z_{l}, z_{l-h})
\end{align}
\noindent where $Cor$ represents the Pearson's correlation function. Using the ACF and plotting each lag of interest is a visual method for understanding the most important lagged time steps. An example of this auto-correlation plot is shown in Figure \ref{fig:acf_ex} for the sun spot time series data \cite{center_2009}. 

% Figure environment removed

A final key aspect to consider is the size of the prediction horizon $H$. In this study, we stick with single-step ahead forecasting ($H = 1$) to create a simple experimental environment. This choice allows us to minimize the model complexity for multi-step forecasting and reduces the error accumulation with having a larger $H$. 

\subsection{RNN Architecture} 
\label{sec: RNN_Arch}

%% Describing the basic RNN cell
Considering that we are 
%focusing our study on 
investigating recurrent neural networks (RNNs), we must define all its components. RNN was originally proposed by Elman \cite{elman1990finding}, where they altered the classical feedforward network layer structure into a recurrent layer. This change in architecture allowed the outputs of a previous layer to become the inputs of the current layer, thereby creating a compact model representation for sequential input data. Figure \ref{fig:unfolded_rnn} shows an unfolded RNN displaying the inputs, an activation function operation (i.e., tanh), and outputs (hidden state). While the Elman RNN is relatively simple as compared to other model architectures, focusing on this network provides a baseline understanding of how recurrent networks learn time series structures.

A necessary step for modeling time series with an RNN is preparing the full univariate time series history into samples that purposefully generate the input-output sequences. We adopt the moving window procedure described by \cite{lara2021experimental} to create the input and output samples. Given a full time history $\textbf{Z}$, 
%we implement the moving window procedure where 
a window of fixed size $T$ slides over the 
%time history 
data to create samples of input $\textbf{x}_{i}$ in $\mathbb{R}^{T}$ and a corresponding output $\textbf{y}_{i}$ in $\mathbb{R}^{H}$, where $i = 1, 2, \ldots, n$. A visual example of generating these input-output samples 
%from a time series history $Z$ 
is shown in Figure \ref{fig:ts_samp} for $T = 5$ and $H = 1$.

% Figure environment removed

%% Expand more clearly on global representation and sample representation.

The input-output sample vectors, $(\textbf{x}_{i},\textbf{y}_{i})$, are currently globally represented in terms of their $z_{l}$ values. However, it is beneficial to convert our vectors into a sample representation that serve as local inputs for our RNN model. To do this, we %instead 
denote our samples as $\textbf{x}_{i} = [x_{1,i}, x_{2,i}, ... , x_{T,i}] $, which represents the \textit{i}th sample of a sequence of $T$ time series values. For each component of $\textbf{x}_{i}$, we recover its relation to the time series history using $x_{t,i} = z_{t+i-1}$, where $t = 0, 1, \ldots, T$. Similarly, the output of the RNN is a vector $\hat{\textbf{y}}_{i} = [\hat{x}_{T+1,i}]$, which represents the \textit{i}th forecasting output for a single-step ahead value beyond $T$, or $H = 1$. We measure the output error with reference to the ground truth single-step ahead vector $\textbf{y}_{i} = [x_{T+1,i}]$. 
%this is typically used for assessing training, validation, and testing errors.
% Ashis: Somewhat repetitive; can be condensed 

%% Notation for activation layers
Lastly, we define the activation outputs (or hidden states) in the context of our time series data and the RNN architecture. Considering an input-output sample $(\textbf{x}_{i}, \textbf{y}_{i})$, the hidden state $\textbf{a}_{t,i}^{(p)}$ represents the $i$th sample output of an activation layer at time step $t$ and epoch $p$, where $p = 1, 2, \dots, P$. The activation layer output $\textbf{a}_{t,i}^{(p)}$ is a vector in $\mathbb{R}^{b}$, where $b$ specifies the width of the hidden state or the number of hidden units. Each activation layer output is produced from tensor operations with sample inputs that pass through some activation function $f$. The time step $t$ for the activation layer output $\textbf{a}_{t,i}^{(p)}$ doubles as an indicator for the layer number. For example, $t = 2$ corresponds to the input $x_{2,i}$ and $\textbf{a}_{2,i}^{(p)}$, the second activation layer for training epoch $p$. In order to output a single prediction value, we must collapse the dimension of the final activation layer output, $\textbf{a}_{T,i}^{(p)}$, with a dense layer. A visualization of this unrolled RNN setup with the time series forecasting problem is shown in Figure \ref{fig:unfolded_rnn}.

% Figure environment removed
%Ashis: Mismatch in parentheses for $\textbf{x}_{i}$ and $\hat{\textbf{y}}_{i}; $T$ indices are different; is $f$ defined anywhere in the text? 

\subsection{Distance Correlation}
\label{sec: EnergyStat}

% Change Dcor notation to R_hat
% Introducing energy statistics and distance correlation
An approach for examining the relationships among random variables is through the use of energy statistics \cite{szekely2013energy}. Energy statistics comprise a set of functions derived from the distances of statistical observations, inspired by the gravitational potential energy between two bodies. We primarily use the  distance correlation metric from energy statistics \cite{szekely2007measuring} to measure the statistical dependence between random variables. Distance correlation is tailored to measure the non-linear relationships between two random variables, not necessarily of equal sizes. We build the empirical definition of distance correlation by relating it some of our previously defined time series components. We start with our input-output samples as $\mathbf{X} \in \mathbb{R}^{T \times n}$  and $\mathbf{Y} \in \mathbb{R}^{H \times n}$: 
\[
\mathbf{X} = 
\left[
  \begin{array}{cccc}
    \vertbar & \vertbar &        & \vertbar \\
    \mathbf{x}_{1}    & \mathbf{x}_{2}    & \ldots & \mathbf{x}_{n}    \\
    \vertbar & \vertbar &        & \vertbar 
  \end{array}
\right], 
\mathbf{Y} = 
\left[
  \begin{array}{cccc}
    \vertbar & \vertbar &        & \vertbar \\
    \mathbf{y}_{1}    & \mathbf{y}_{2}    & \ldots & \mathbf{y}_{n}    \\
    \vertbar & \vertbar &        & \vertbar 
  \end{array}
\right]
\] 
\noindent where the columns $\mathbf{x}_{i} \in \mathbb{R}^{T}, \mathbf{y}_{i} \in \mathbb{R}^{H}$ are the input-output sample vectors defined in section \ref{sec: RNN_Arch}. From the observed samples $\mathbf{X}, \mathbf{Y}$, define 
\[
b_{ij} = \|\mathbf{x}_{i} - \mathbf{x}_{j}\| \quad \overline{b}_{i.} = \left(\sum_{j = 1}^{n}b_{ij}\right)/n \quad \overline{b}_{.j} = \left(\sum_{i = 1}^{n}b_{ij}\right)/n
\]
\[
\overline{b}_{..} = \left(\sum_{i,j = 1}^{n}b_{ij}\right)/n^{2} \quad B_{ij} = b_{ij} - \overline{b}_{i.} - \overline{b}_{.j} + \overline{b}_{..}
\]
\noindent These are similarly defined for $\overline{c}_{i.}$, $\overline{c}_{.j}$, $\overline{c}_{..}$ where $c_{ij} = \|\mathbf{y}_{i} - \mathbf{y}_{j}\|$ and $C_{ij} = c_{ij} - \overline{c}_{i.} - \overline{c}_{.j} + \overline{c}_{..}$. Given this set of double centered matrices, the sample distance covariance function \cite{szekely2007measuring} is defined as
%We adopt the notation of Sz'ekely et al. \cite{szekely2007measuring}, where random variables $\mathbf{X}$ in $\mathbb{R}^{p}$ and $\mathbf{Y}$ in %$\mathbb{R}^{q}$. Formally, the distance correlation between $\mathbf{X}$ and $\mathbf{Y}$ is defined as
%\begin{align}\label{eq: dist_corr}
%    R^{2}(\mathbf{X},\mathbf{Y}) =  
%\begin{cases}
%    \frac{V^{2}(\mathbf{X},\mathbf{Y})}{\sqrt{V^{2}(\mathbf{X})V^{2}(\mathbf{Y})}},& V^{2}(\mathbf{X})V^{2}(\mathbf{Y}) > 0\\
%    0,              & V^{2}(\mathbf{X})V^{2}(\mathbf{Y}) = 0
%\end{cases}
%\end{align}

%\noindent where $V$ is the distance covariance function defined by \cite{szekely2007measuring}. Distance covariance is analogous to a product-moment %covariance. %Ashis: I think we want to define V too. 
%Distance correlation $R$ possesses the following properties:

%\begin{itemize}
%  \item $R(\mathbf{X},\mathbf{Y})$ is defined for any dimension of $\mathbf{X}$ and $\mathbf{Y}$
%  \item $R(\mathbf{X},\mathbf{Y}) = 0$ denotes the independence of $\mathbf{X}$ and $\mathbf{Y}$
%  \item %Distance correlation satisfies 
%  $0 \leq R \leq 1$
%\end{itemize}
%% Distance correlation estimator
\begin{align}
    \hat{V}^{2}(\mathbf{X}, \mathbf{Y}) = \frac{1}{n^{2}} \sum_{i,j = 1}^{n} B_{ij}C_{ij} \quad.
\end{align}

\noindent Finally, the empirical distance correlation is defined as 
\begin{align}\label{eq:emp_dist}
    \hat{R}^{2}(\mathbf{X},\mathbf{Y}) =  
\begin{cases}
    \frac{\hat{V}^{2}(\mathbf{X},\mathbf{Y})}{\sqrt{\hat{V}^{2}(\mathbf{X})\hat{V}^{2}(\mathbf{Y})}},& \hat{V}^{2}(\mathbf{X})\hat{V}^{2}(\mathbf{Y}) > 0\\
    0,              & \hat{V}^{2}(\mathbf{X})\hat{V}^{2}(\mathbf{Y}) = 0
\end{cases}
\end{align}

\noindent The empirical distance correlation function has the following properties \cite{edelmann2019updated}: 
\begin{itemize}
  \item $\hat{R}(\mathbf{X},\mathbf{Y})$ converges almost assuredly to the theoretical counterpart \cite{szekely2007measuring} provided $n \rightarrow \infty$ and $E[\|\mathbf{X}\|] < \infty$ and $E[\|\mathbf{Y}\|] < \infty$. 
  \item $\hat{R}(\mathbf{X},\mathbf{Y}) = 0$ denotes the independence of $\mathbf{X}$ and $\mathbf{Y}$ 
  \item %Distance correlation satisfies 
  $0 \leq \hat{R} \leq 1$
\end{itemize}
\noindent Additionally, another key property shown by \cite{szekely2013distance}, for a fixed $n$
\begin{align}\label{eq: dcor_inf}
    \lim_{T,H \to \infty} \hat{R}(\mathbf{X}, \mathbf{Y}) = 1    
\end{align}
\noindent provided that $\mathbf{X}$ and $\mathbf{Y}$ are independently and identically distributed (i.i.d.) and their corresponding second moments exist. 


% Energy distance information
% Specifically, the energy distance between two random variables of the same dimension, serves as a measure to determine whether they originate from identical distributions. It is derived from the Cramér von Mises Smirnov distance \cite{anderson_1962}, with the advantage of not requiring predefined distributions. Given two independent random samples $\mathbf{U} \in \mathbb{R}^{m \times n}$  and $\mathbf{V} \in \mathbb{R}^{m \times n}$: 
% \[
% \mathbf{U} = 
% \left[
%   \begin{array}{cccc}
%     \vertbar & \vertbar &        & \vertbar \\
%     \mathbf{u}_{1}    & \mathbf{u}_{2}    & \ldots & \mathbf{u}_{n}    \\
%     \vertbar & \vertbar &        & \vertbar 
%   \end{array}
% \right], 
% \mathbf{V} = 
% \left[
%   \begin{array}{cccc}
%     \vertbar & \vertbar &        & \vertbar \\
%     \mathbf{v}_{1}    & \mathbf{v}_{2}    & \ldots & \mathbf{v}_{n}    \\
%     \vertbar & \vertbar &        & \vertbar 
%   \end{array}
% \right]
% \] 
% \noindent where the columns $\mathbf{u}_{k} \in \mathbb{R}^{m}, \mathbf{v}_{k} \in \mathbb{R}^{m}$ represent independent samples of a statistical process, the estimate of energy distance $\varepsilon(\textbf{U},\textbf{V})$, is defined as
% \begin{align}\label{eq: Estat}
%     \varepsilon(\textbf{U},\textbf{V}) = \frac{2}{n^{2}}\sum_{i=1}^{n}\sum_{m=1}^{n} \|\mathbf{u}_{i} - \mathbf{v}_{m}\| - \frac{1}{n^{2}}\sum_{i=1}^{n}\sum_{j=1}^{n}\|\mathbf{u}_{i} - \mathbf{u}_{j}\|-  \frac{1}{n^{2}}\sum_{l=1}^{n}\sum_{m=1}^{n}\|\mathbf{v}_{l} - \mathbf{v}_{m}\|
% \end{align}
% \noindent Effectively, $\varepsilon(\mathbf{U},\mathbf{V}) \geq 0$, where the equality holds if and only if $\mathbf{U}$ and $\mathbf{V}$ are identically distributed. Although there is no upper bound on $\varepsilon(\mathbf{U},\mathbf{V})$, it can still be used in a relative manner to compare their distribution origins. 

\subsection{Analysis of RNN Activation Layers}\label{sec: meas_dist_RNN}

% How we used analyze time series, RNN and distance correlation combined. 
%Finally, given the definitions listed in the sections above, 
We now provide the framework for using distance correlation to study how the RNN activation layer outputs learn time series structures that eventually translate to 
%and how that ultimately translates to 
forecasting outputs. We start with a set of samples $(\textbf{x}_{1}, \textbf{y}_{1})$, $(\textbf{x}_{2}, \textbf{y}_{2})$, ... , $(\textbf{x}_{n}, \textbf{y}_{n})$ where each $\textbf{x}_{i}$ is an RNN input to produce a forecasting value $\hat{\textbf{y}}_{i}$. This is compared with the ground truth output $\textbf{y}_{i}$ using a loss function. For each sample fed into the RNN during training, a unique activation layer output $\textbf{a}_{t,i}^{(p)}$ is produced depending on the sample $i$, time step $t$, and epoch $p$. The accumulation of all the samples for a particular activation layer output at time step $t$ and epoch $p$ is represented as a matrix
\[
\textbf{A}_{t}^{(p)} = 
\left[
  \begin{array}{cccc}
    \vertbar & \vertbar &        & \vertbar \\
    \textbf{a}_{t,1}^{(p)}    & \textbf{a}_{t,2}^{(p)}    & \ldots & \textbf{a}_{t,n}^{(p)}    \\
    \vertbar & \vertbar &        & \vertbar 
  \end{array}
\right]
\]
\noindent With our representation of sample ground truth outputs $\mathbf{Y}$, we estimate the distance correlation between the activation layer output at time $t$ and epoch $p$, and the ground truth outputs. This is denoted by $\hat{R}(\textbf{A}_{t}^{(p)})$, and is calculated using (\ref{eq:emp_dist}). We also estimate the dependency between two activation layers, $\textbf{A}_{v}^{(p)}, \textbf{A}_{m}^{(p)}$, at different time steps using $\hat{R}(\textbf{A}_{v}^{(p)}, \textbf{A}_{m}^{(p)})$, which is the distance correlation between the activation layer outputs at time steps $v$ and $m$ at epoch $p$. We note that for both the scenarios, the dimensions between the two random variables need not be the same. This framework guides the experiments 
%that we carry out 
in section \ref{sec: Experiments}. 

\section{Experiments} \label{sec: Experiments}

%With the framework described in the previous section, 
We %are now equipped to 
now conduct a series of experiments to showcase the usefulness of our method for assessing time series forecasting using RNN models. Using such method, we make note of certain limitations of RNN forecasting and compare the models (network architectures) using heatmap visualizations.  

\subsection{Generating Time Series Data}\label{sec: Gen_TS}

% Defining AR and MA processes
Before we begin our experiments, we must select the type of time series data we would like to analyze. As discussed in section \ref{sec:TS_Forecast}, we generate synthetic time series data for each time step $z_{l}$, according to a few well established processes. Perhaps the simplest case is the auto-regressive process, signified by AR($p$), where the value of the current time step $l$ value is a linear combination of the previous $p$ time steps. Formally, the auto-regressive process is defined as \cite{brockwell_1996}
\begin{align}\label{eq: AR}
    z_{l} = \sum_{i = 1}^{p} c_{i}z_{l-i} + \epsilon_{l} \quad.
\end{align}
\noindent Here, $c_{i}$ are the coefficients of the lagged variables and $\epsilon_{l}$ is white noise which follows a normal distribution $N(\mu, \sigma^{2})$ at time step $l$. We also consider the moving average model, MA($q$), which is similarly defined as \cite{brockwell_1996}
\begin{align}\label{eq: MA}
    z_{l} =  \delta + \sum_{i = 1}^{q} \theta_{i}\epsilon_{l-i}
\end{align}
\noindent where $\theta_{i}$ are the coefficients of the lagged white noised $\epsilon_{l-i}$ and $\delta$ is a predetermined average value. This moving average process generates the value of the current time step $l$ based on an average value $\delta$ and the previous $q$ lagged white noise. When we combine the processes from (\ref{eq: AR}) and (\ref{eq: MA}), we arrive at the auto-regressive moving average, or the ARMA($p,q$) model. 

%% Define GARCH processes
Beyond these simple models, we consider another time series model called the generalized autoregressive conditional heteroskedasticity (GARCH) model. Full definition of the GARCH($p,q$) process is presented in \cite{brockwell_1996}, where this process allows the time series to exhibit variance spikes during any given time step. It is generally defined as: %by the following: 
\begin{align}\label{eq: GARCH_Zt}
    z_{l} = \sqrt{h_{l}}\epsilon_{l}
\end{align}
\noindent where $h_{l}$ is a positive function that represents the variance. The variance $h_{l}$ is defined by: 
\begin{align}\label{eq: GARCH_ht}
    h_{l} = \alpha_{0} + \sum^{p}_{i = 1}\alpha_{i}z^{2}_{l-i} + \sum^{q}_{j = 1}\beta_{j}h^{2}_{l-j} \quad.
\end{align}
\noindent In this GARCH($p,q$) definition, 
%e_{l}$ represents the white noise, 
$\alpha_{i}$ are the coefficients of the $p$ lagged variables, and $\beta_{j}$ are the coefficients of the $q$ lagged variances. This process induces volatility in the time series data that is often present in real world datasets.

Last, one simple but effective way to characterize these times series processes is by observing the magnitudes of their lag structures. For large values of $p,q$, we consider the lag structure to be high (and correspondingly low for small values of $p,q$). %This characterization helps us describe certain time series processes.

\subsection{RNN behavior under various time series processes}\label{sec: RNN_limits}

%% Describe the objective of the experiments and comparisons to ACF
The focus of this section is to use distance correlation to determine the characteristics of RNNs for various time series processes. To investigate this, we carry out a series of experiments that track the outputs of each activation layer and compare it to the ground truth output via distance correlation upon the completion of model training. In effect, we are calculating $\hat{R}(\textbf{A}_{t}^{(P)}, \textbf{Y})$ for $t = 1,2, ..., T$ at the final epoch, $P$. $\hat{R}(\textbf{A}_{t}^{(P)}, \textbf{Y})$ values near one show that the activation layer at $t$ has a strong dependency to $\textbf{Y}$, indicating it is a highly important layer in the training process. Conversely, values close to zero imply that the activation layer at $t$ does not contribute to learning the output $\textbf{Y}$. 

One particular component of interest is layer number $T$, $\textbf{A}_{T}^{(P)}$, which is the final activation layer before the RNN generates an output. Since all the previous inputs flow to this activation layer, tracking $\hat{R}(\textbf{A}_{T}^{(P)}, \textbf{Y})$ serves as a measure of what information is retained or lost during training.

%% How it compares to ACF
Since we are tracking the dependency structure of each layer $\textbf{A}_{t}^{(P)}$ and ground truth $\textbf{Y}$, we can make a parallel comparison to the auto-correlation function (ACF) defined by 
%equation 
(\ref{eq: pcor}). If we are interested in understanding the linear structure between a single-step ahead time series value and a corresponding lag $h$, we would calculate  $Cor(z_{l+1}, z_{l+1-h})$. The analogous comparison to this would be calculating the distance correlation, $\hat{R}(\textbf{A}_{T+1-h}^{(P)}, \textbf{Y})$. For example, for $T = 20$, $h = 5$, and activation layer number 16, $\textbf{A}_{16}^{(P)}$ can be directly compared to the ACF value at the 5th lag. Note that this inverse relationship between the layer number and lag (e.g., layer 20 and lag 1 or layer 1 and lag 20) is used for direct comparisons. We make this comparison to establish a baseline analysis of the time series structure.


\subsubsection{Implementation Details}
%% Defining the remaining parameters of the experiment                      
Before presenting the results, we fix some parameters specific to this experiment. We begin by generating a time series history $Z$ of length $L = 4,000$ using one of the processes defined in Section \ref{sec: Gen_TS}. We then split our time series history such that the first 80$\%$ points form the training set and the last 20$\%$ points are used for evaluation (test set), as shown in Figure \ref{fig:ts_samp}. We only consider positive coefficients in our time series process to establish a closed set of outcomes for each experiment. 
%Ashis: Is this training-testing procedure standard?

For the Elman RNN setup, we fix the input window size $T = 20$, the number of hidden units as $b = 64$, use the ReLU activation function, batch size of 64, learning rate of 0.0001 and train the model until 35 epochs. We also initialize the networks weights via a Kaiming He initialization procedure \cite{he2015delving}, as it has been shown to be a stable and efficient initialization scheme \cite{rolnick2018towards}. We choose these parameters with the aim of creating a stable environment where the RNN has a high likelihood of converging. Overall, we want to ensure an experiment that captures how RNNs learn time series structures, not why they have known instabilities. In terms of the computational resources, we use TensorFlow 2.1 for model training and activation layer extraction. This is done with a single GTX 970 GPU.  

 \subsubsection{Time Series Process Results}
%% Description of simulation and plots
%In the set of figures, 
We now report the results of a series of simulations where a total of 50 runs are conducted for a given time series process. For each run, we 
%generate a fixed time series process, 
train an RNN from scratch.
%, and extract the relevant distance correlation values described above. Along with each of these runs, we calculate the corresponding ACF values for each lag for comparison. 
We begin by studying various auto-regressive time series processes. Figure \ref{fig:AR_1_5} shows two plots each for AR(1) and AR(5) time series processes: 1) A time series plot with the original and forecast values of the trained RNN; and 2) the mean correlation values between the ground truth and each activation layer using both distance correlation and ACF metrics. Figure \ref{fig:AR_10_20} shows the same plots for AR(5) and AR(10) time series processes. In addition, the figures also display the corresponding mean squared errors (MSEs) and the coefficients for the time series processes.

% Figure environment removed

% Figure environment removed

%% AR results 
Figure \ref{fig:AR_1_5}(a) shows that for an AR(1) process, the distance correlation values closely resemble the pattern produced by the ACF values for all the layers and lags. The similarity between ACF and distance correlation is likely due to the recursive nature of the AR(1) process, where all the time steps are recursively dependent on each other. While there is a gradual increase in correlation as the layer numbers increase, they still remain relatively high (above 0.7 for all the layers). Noting that layer number 20 is the activation layer prior to generating a forecast value, it follows that a high distance correlation in this layer yields accurate forecasts. This finding is supported by the low MSE and well fitting time series forecasting plots. 

However, the corresponding plots for AR(5), AR(10) and AR(20) indicate that the forecasting accuracy of RNN decreases for increasing time series lag structures. Figure \ref{fig:AR_1_5}(b) shows the correctly identified AR(5) lag structure by displaying high correlation values at layers 1, 6, 11 and 16. Recalling the inverse relationship that lags have with the layer numbers, this aligns with the ACF values whose corresponding correlations are high at lags 20, 15, 10 and 5. However, this identification diminishes quickly as the distance correlation values decrease to less than 0.5 at layer 20. This is further evident in Figure \ref{fig:AR_10_20} where the correct lags are identified for both distance correlation and ACF, but the correlation reduces to less than 0.4 when it reaches layer 20. This reduction in distance correlation  with increasing layer numbers is synonymous to the RNN losing memory of important previous inputs. As a result, the MSE score increases for the AR(10) and AR(20) time series, which is supported by the poor fitting forecasting plots. 


% For instance, Figure \ref{fig:AR_1_5}(a) displays an AR(1) process with a seemingly linear decrease in both the correlation values from lag 1 to 10, which is the expected result with ACF. Figure \ref{fig:AR_1_5}(b) shows a similar pattern for both correlation metrics and all lag values. For all the 
% %figures that incorporate the 
% AR($p$) processes, the time series plots show a relatively well-fitting prediction to the test set, with overall small MSE values. These observations suggest that RNNs learn auto-regressive forecasting tasks similar to how the auto-correlation function unveil time series structures.

% MA results
We extend these experiments to the moving average process by looking at MA(1) and MA(20) structures. Figure \ref{fig:MA_1_20} shows the same mean correlation and time series forecasting plots as before. In Figure \ref{fig:MA_1_20}(a), the mean correlation plot displays the correct layer number (or lag structure) for MA(1); however, the value is relatively low (approximately 0.4). Although the distance correlation value is highest at layer number 20, the MSE score is larger than its AR(1) counterpart. The relatively low distance correlation value at the final activation layer combined with worse MSE scores indicate that the RNN cannot model the error lag structures well.

Figure \ref{fig:MA_1_20}(b) shows the results for the MA(20) structure. Although the proper lag is identified at layer number 1, it is also relatively low with a value of approximately 0.4. However, since the lag structure is high (larger values of $p,q$) and far removed from the forecast horizon, we see an asymptotic decrease in correlation values near layer number 20, exhibiting values less than 0.1. The RNN appears to compensate for this lack of information being transmitted to the final layer by modeling values near the average (of zero). This trend is noticeable in the time series forecasting plot where the amplitude is truncated and centered around zero. 

% We extend this notion to two other time series processes, which include the moving average (MA) and auto-regressive moving average (ARMA). Figure \ref{fig:MA_1_7} 
% %and \ref{fig:MA_5_9} (appendix) 
% shows two representative time series forecasting and mean correlation plots for the MA process. In essence, we observe fairly similar findings to the AR processes with a few differences. We notice a general dip in both the distance correlation and ACF values, which are generally less than 0.3. Despite this, the relevant lags corresponding to the lagged coefficients that describe the MA process are recognized. For example, Figure \ref{fig:MA_1_7}(a) is highlighted by a noticeable increase in the correlations for the first lag, while all the other lags are close to zero for both the metrics. This is also the case in Figure \ref{fig:MA_1_7}(b) where the lags 2, 5, 7 are recognized by both the correlation metrics.
% %This lag recognition is also apparent for the remaining MA processes shown. 
% The corresponding time series RNN predictions %for each figure 
% show fairly close adherence to the original values with low MSEs, although we notice a slight increase in MSE values with increasing lag structure. 

% ARMA results
We also look at the combined effects of AR($p$) and MA($q$) processes by modeling ARMA($p,q$) time series structures. Figure \ref{fig:ARMA_1_10_x2} shows the same set of plots for the ARMA(1,10) and ARMA(10,1) processes. Figure \ref{fig:ARMA_1_10_x2}(a) shows the results for ARMA(1,10), which exhibit a similar correlation behavior to that of AR(1). The distance correlation values are high for all the layers with the highest occurring at layer number 20. As in the AR(1) plots, we see that the ACF values closely resemble the distance correlation values, which can again be explained by the single step recursive nature of an AR(1) term. This results in highly accurate forecasts with low MSE and well fitting time series plots. Figure \ref{fig:ARMA_1_10_x2}(b) shows the results for ARMA(10,1), which exhibits similar correlation patterns to that of AR(10). However, the correlations do not decrease as much when approaching layer number 20 (0.5 instead of 0.4). The inclusion of the MA terms, therefore, seems to provide additional pertinent information on the underlying time series characteristics to the RNN. %which suggest it retains more information. 
This leads to lower MSE scores for both the ARMA processes as compared to their AR counterparts.


% The relationships between distance correlation and the corresponding ACF values are perhaps strongest for the ARMA plots in Figure \ref{fig:ARMA_11_53}. %and \ref{fig:ARMA_77_99}. 
% Both the correlation methods have similar values at each lagged value. The time series plot also demonstrates well-fitting RNN forecasts and some of the lowest MSE values among all the experiments. %The added results from the MA and ARMA experiments further provides evidence that 
% These results indicate that RNNs have an internal mechanism that is similar to the auto-correlation function, which makes it favorable for predicting ARMA processes.

% Figure environment removed

% Figure environment removed

%\subsubsection{GARCH results}
% GARCH results
We also explore how RNNs internally learn time series processes exhibiting heteroscedasticity with GARCH processes. We report the same time series forecasts and mean value correlation plots in Figure \ref{fig:GARCH_22_44}, where we note %it is noticeable 
that the distance correlation values are quite low (less than 0.25) for all the layers. The low correlation values indicate that none of the RNN activation layers make strong contributions to learning the ground truth. This is corroborated by the poor fitting forecasting plots for both the GARCH(2,2) and GARCH(4,4) processes. In essence, the RNN activation layers are not able to effectively model the heteroscedasticity and volatility present in the GARCH process, at least without additional pre-processing.

% First, it is noticeable that both the correlation metrics are fairly close to zero at all the lags. This is expected for the ACF plot with GARCH processes \cite{brockwell_1996}, where no relevant lags can be typically identified. The distance correlation mean values also see low values for all the lags (less than 0.25). The non-zero values may be explained by the RNN's ability to recognize some non-linear relationships of the time series structure, although they are close to negligible. The low values for distance correlation in all the activation layers indicates that the none of the RNN layers make strong contributions to learning the \hl{ground truth horizon}. 

% % Short discussion of poor GARCH results
% Further, it is visually evident that RNNs cannot easily forecast the nature of a GARCH process by looking at the time series plots with relatively high MSE values in Figures \ref{fig:GARCH_22_44}(a), \ref{fig:GARCH_22_44}(b). The combined results of low mean correlations values and high MSE indicate that the RNN is unsuccessful in learning the heteroscedasticity and volatility present in the GARCH process. Nonetheless, the similarities in values for both the correlation metrics provide evidence that RNNs learn GARCH process analogous to the auto-correlation function, thereby limiting their forecasting accuracy.

% Figure environment removed

\subsubsection{Temporal information loss in RNNs}

We recall the studies that used distance correlation as a metric of the information stored in neural network activation layers \cite{zhen2022versatile, vepakomma2020nopeek}, and apply the same principle in our study. From our experiments, we observe 
that the distance correlation values change as the time series data moves from one activation layer to the next, analogous to a measurable change in information content across the RNN.
%a decrease in the distance correlation values as we approach the final activation layer; this is analogous to the loss of information in RNNs.} 
%To emphasize this loss of information, 
Table \ref{table: All_Exp} reports the key values that exemplify this behavior. The key indicator %for this 
is the percentage change between the max distance correlation 
%$\hat{R}(\textbf{A}_{t}^{(P)}, \textbf{Y})$ 
at any layer 
%number $t$ 
and the distance correlation at the final layer. 
%number 20, 
This is calculated by $(\hat{R}(\textbf{A}_{t}^{(P)}, \textbf{Y})_{max} - \hat{R}(\textbf{A}_{T}^{(P)}, \textbf{Y})) / \hat{R}(\textbf{A}_{t}^{(P)}, \textbf{Y})_{max}$.  %The percentage change is calculated to show 
It summarizes how much information is lost by the time the data reaches the final activation layer before the RNN produces a forecasting value. 

\begin{table}[h!]
\caption{MSE and distance correlation results for all the simulation experiments}
\begin{tabular}{||p{0.2\textwidth} | p{0.2\textwidth} | p{0.15\textwidth} | p{0.15\textwidth} | p{0.15\textwidth}||} 
 \hline
 Time Series Process & Mean Squared Error & $\hat{R}(\textbf{A}_{t}^{(P)}, \textbf{Y})$ (max) & $\hat{R}(\textbf{A}_{T}^{(P)}, \textbf{Y})$ & Percent Change(\%)\\ [0.5ex] 
 \hline\hline
 AR(1) & 0.025 $\pm$ 0.006 & 0.927 & 0.927 &  0 \\ 
 \hline
 AR(5) & 0.100 $\pm$ 0.039 & 0.916 &  0.436 & 52 \\ 
 \hline
 AR(10) & 0.672 $\pm$ 0.344 & 0.949 & 0.391 & 59  \\ 
 \hline
 AR(20) & 1.281 $\pm$ 0.441 & 0.964 & 0.356 & 63  \\ 
 \hline
  MA(1) & 0.602 $\pm$ 0.037 & 0.418 & 0.418 & 0  \\ 
 \hline
 MA(5) & 0.783 $\pm$ 0.057 & 0.427 & 0.131 & 69  \\ 
 \hline
 MA(10) & 1.008 $\pm$ 0.081 & 0.422 & 0.098 & 77  \\ 
 \hline
 MA(20) & 1.056 $\pm$ 0.063 & 0.435 & 0.093 & 79  \\
 \hline
 ARMA(1,1) & 0.009 $\pm$ 0.003 & 0.935 & 0.935 & 0  \\ 
 \hline
 ARMA(1,10) & 0.013 $\pm$ 0.003 & 0.951 & 0.951 & 0 \\ 
 \hline
 ARMA(10,1) & 0.341 $\pm$ 0.195 & 0.947 & 0.496 & 48  \\ 
 \hline
 GARCH(2,2) & 1.037 $\pm$ 0.830 & 0.264 & 0.259 & 2  \\ 
 \hline
 GARCH(4,4) & 1.080 $\pm$ 0.751 & 0.215 & 0.202 & 6  \\ 
 \hline
\end{tabular}\label{table: All_Exp}
\begin{minipage}{13cm}
\vspace{2mm}
\small Notes: %Results for all time series experiments. 
Mean squared errors are calculated with standard deviations for 50 simulation runs. $\hat{R}(\textbf{A}_{t}^{(P)}, \textbf{Y})$ (max) represents the max distance correlation for any layer number $t$. $\hat{R}(\textbf{A}_{T}^{(P)}, \textbf{Y})$ is the distance correlation value at layer number $T = 20$. The percent change from the peak distance correlation values in any activation layer to the final layer measures how much information is lost during RNN training. In general, we see that larger lag structures tend to lose more information during training, leading to higher MSE scores. 
\end{minipage}
\end{table}

From Table \ref{table: All_Exp}, we see that for the higher AR lag structures, the RNN is able to identify the proper lags. However, higher lag structure AR processes lose a larger percentage of the information as they reach the final activation layer. For the MA lag structures, the proper lags are also identified, although they exhibit smaller distance correlation values. Nonetheless, the higher lag structures still lose more information as they reach the final activation layer. This general loss of memory in the final activation layer leads to higher MSE scores. This is also exemplified by comparing AR(10) to ARMA(10,1), where they are similar in distance correlation patterns as seen in Figures \ref{fig:AR_10_20}(a) and \ref{fig:ARMA_1_10_x2}(b). However, ARMA(10,1) loses 48\% of the input memory as compared to 59\% for AR(1), resulting in a lower MSE. The exception to this behavior are the GARCH results, where no RNN activation layers learn the ground truth values well, leading to poor overall results.

These experiments and outcomes were replicated for different RNN hyperparameters to show that this behavior is consistent despite changes in the modeling parameters. See Appendix A for a summary of the additional results. 


% \subsubsection{Comparing ACF and distance correlation methods with energy distance}
% % Make known MSE values are for predicitions
% %% Describtion of data preparation for energy distance calculation
% In observing many results, there is clearly some relationship between the distance correlation of the activation layers/output and the corresponding lags of the auto-correlation function. The similarity in trends and values observed between both metrics suggest that they may be generated by the same distribution. To formally measure these similarities, we use the energy distance between the collective values. We collect the independent correlation outcomes for the $M = 50$ simulations of both methods. For the ACF, we consider the random samples as
% \[
% \textbf{U}_{ACF} = 
% \left[
%   \begin{array}{cccc}
%     \vertbar & \vertbar &        & \vertbar \\
%     \textbf{u}_{ACF,1}    & \textbf{u}_{ACF,2}    & \ldots & \textbf{u}_{ACF,M}    \\
%     \vertbar & \vertbar &        & \vertbar 
%   \end{array}
% \right]
% \]
% \noindent where 
% \[\mathbf{u}_{ACF, j} = [Cor(z_{l}, z_{l-1}), Cor(z_{l}, z_{l-2}), ... , Cor(z_{l}, z_{l-T})] \] 
% \noindent where $j$ represents the $j$th simulation run and $j = 1, 2, \ldots, M$. Similarly for the distance correlation method, we construct and analogous set of samples which are defined by  
% \[
% \textbf{U}_{Dcor} = 
% \left[
%   \begin{array}{cccc}
%     \vertbar & \vertbar &        & \vertbar \\
%     \textbf{u}_{Dcor,1}    & \textbf{u}_{Dcor,2}    & \ldots & \textbf{u}_{Dcor,M}    \\
%     \vertbar & \vertbar &        & \vertbar 
%   \end{array}
% \right]
% \]
% \noindent where 
% \[\mathbf{u}_{Dcor, j} = [\hat{R}(\textbf{A}_{T}^{(P)}, Y), \hat{R}(\textbf{A}_{T-1}^{(P)}, Y), ... , \hat{R}(\textbf{A}_{1}^{(P)}, Y)]\]

% %% Description of scaled energy distance
% We use the independent random samples $\textbf{U}_{ACF}$ and $\textbf{U}_{Dcor}$ to calculate the energy distance $\varepsilon(\textbf{U}_{ACF}, \textbf{U}_{Dcor})$ using (\ref{eq: Estat}). As noted in section \ref{sec: EnergyStat}, $\varepsilon(\textbf{U}_{ACF}, \textbf{U}_{Dcor}) \geq 0$ with energy distance values close to zero, indicate that both random variables come from identical distributions. However, since energy distance does not have an upper bound, it can be difficult to interpret the absolute raw values. Luckily, our set of experiments has a finite set of values in which the worst case occurs when all the ACF lag values equal zero and all the distance correlation values equal one (or vice versa). For our RNN with an input size of 10, the max energy distance $\varepsilon_{max}(\textbf{U}_{ACF}, \textbf{U}_{Dcor}) = 6.32$ under this worst case scenario. We subsequently calculate the scaled energy distance, $\varepsilon_{scaled}$, as

% \[\varepsilon_{scaled}(\textbf{U}_{ACF}, \textbf{U}_{Dcor})=\varepsilon(\textbf{U}_{ACF}, \textbf{U}_{Dcor})/\varepsilon_{max}(\textbf{U}_{ACF}, \textbf{U}_{Dcor})\]

% \noindent which lies between values 0 and 1.  

% %% Discuss the results of energy distance for ARMA processes
% Table \ref{table: All_Exp} shows the results of the simulation experiments for each time series process, their corresponding mean squared error and energy distance (with scaled energy distance). The correlation plots for the additional time series processes not discussed in this section can be found in the appendix. For many of AR and ARMA processes,  $\varepsilon_{scaled}$ is less than 0.04 with the exception of lag structures exceeding 9. The remaining MA and ARMA processes, $\varepsilon_{scaled}$ is consistently around 0.10. Having these $\varepsilon_{scaled}$ values near zero provides evidence that the two methods generate results from the same distribution. Such identical distributions suggest that RNNs try to emulate the linear ACF process for solving time series that have ARMA type structures.

% \begin{table}[h!]
% \caption{MSE and Energy Distance results for all simulation experiments.}
% \begin{tabular}{||c | c| c||} 
%  \hline
%  Time Series Process & Mean Squared Error & Energy Distance (Scaled) \\ [0.5ex] 
%  \hline\hline
%  AR(1) & 0.218 $\pm$ 0.026 & 0.29 (0.04)  \\ 
%  \hline
%  AR(5) & 0.470 $\pm$ 0.048 & 0.19 (0.03)  \\ 
%  \hline
%  AR(7) & 0.580 $\pm$ 0.087 & 0.16 (0.03)  \\ 
%  \hline
%  AR(9) & 0.587 $\pm$ 0.066 & 0.57 (0.09)  \\ 
%  \hline
%   MA(1) & 0.647 $\pm$ 0.056 & 0.605 (0.096)  \\ 
%  \hline
%  MA(5) & 0.867 $\pm$ 0.055 & 0.661 (0.104)  \\ 
%  \hline
%  MA(7) & 0.915 $\pm$ 0.061 & 0.59 (0.09)  \\ 
%  \hline
%  MA(9) & 0.951 $\pm$ 0.068 & 0.68 (0.11)  \\
%  \hline
%  ARMA(1,1) & 0.084 $\pm$ 0.014 & 0.28 (0.04)  \\ 
%  \hline
%  ARMA(5,3) & 0.323 $\pm$ 0.037 & 0.27 (0.04)  \\ 
%  \hline
%  ARMA(7,6) & 0.333 $\pm$ 0.058 & 0.20 (0.03)  \\ 
%  \hline
%  ARMA(9,9) & 0.484 $\pm$ 0.071 & 0.61(0.10)  \\ 
%  \hline
%  GARCH(2,2) & 1.117 $\pm$ 1.006 & 1.09 (0.17)  \\ 
%  \hline
%  GARCH(4,3) & 1.404 $\pm$ 1.086 & 0.82 (0.13)  \\ 
%  \hline
%  GARCH(6,5) & 1.158 $\pm$ 0.824 & 0.75 (0.12)  \\   
%  \hline
% \end{tabular}\label{table: All_Exp}
% \begin{minipage}{13cm}
% \small Notes: Results for all time series experiments. Mean squared errors are calculated with standard deviations for 50 simulation runs. Energy distance is shown with the scaled number in parentheses, where 0 signifies that the ACF and distance correlation methods are from the same distribution. A scaled energy distance value of 1 indicates a complete dissimilarity of distribution from both metrics according to our experiments. In general we see relatively small scaled energy distance values for all time series processes. However, we see a relatively large increase in MSE value for the GARCH process despite still having a relatively low scaled energy distance. 
% \end{minipage}
% \end{table}

% % Discuss results for GARCH
% Observing the remaining results of the GARCH process, we begin to see a significant decrease in accuracy from the RNN forecast with high MSE values. Even though $\varepsilon_{scaled}$ values between the methods are beginning to increase up to 0.17, it is still close to 0 and it is not unreasonable that each correlation method generate values from similar distributions. This indicates that neither method is effective for learning the volatile characteristics of the GARCH process due to the similarities between to the RNN and ACF method of solving time series forecasts. 

% %% Results for other simulation experiments under different hyperparameteres
% Lastly, we consider changing some hyperparameters in our series of simulation experiments to see if the general findings remain consistent. \hl{We provide additional experiments and tables in the appendix.} 

% Sell the heatmaps
\subsection{Visualizing differences in time series RNN models}

% Motivating why we use heatmaps to compare networks
As we try to gain a better understanding of RNN capabilities with time series forecasting, we leverage distance correlation to generate heatmaps as a visual way of investigating the activation layers; similar to the comparison of computer vision models \cite{zhen2022versatile}. We motivate this visualization method by considering which RNN hyperparameters lead to accurate forecasts. These include input size, activation functions, the number of hidden units, and even the RNN output configurations (i.e., many-to-one or many-to-many). For example, Figure \ref{fig:Heat_same} shows a distance correlation heatmap between a trained RNN with 10 inputs (activation layers) and itself, where we see a complete symmetry in the heatmap. This baseline heatmap can help us visualize the similarities between the activation layers at different time steps and reveal characteristics such as the lag structure. We expand such comparisons with some of the hyperparameters mentioned above. 

% Figure environment removed

%% Comparing RNNs with different hidden units and activation functions
Distance correlation heatmaps can show whether two networks with different hyperparameters are similar at each activation layer. Two examples of this are displayed in Figure \ref{fig:Sim_Network}, where we test networks with different activation functions and number of hidden units. In the case of Figure \ref{fig:Sim_Network}(a), the symmetric nature of the heatmap suggests that using either ReLU or Tanh activation function makes minimal difference in its evaluation of an AR(1) process. Extending this to Figure \ref{fig:Sim_Network}(b), we see a similar symmetry in the heatmap for networks having 8 and 128 hidden units in the activation layer for an ARMA(1,2) process. Note that all the networks were allowed to train until convergence, which exceeded 100 epochs for the RNN with 8 hidden units. Both these results show that the different hyperparameter choices do not have any noticeable effect for a given time series process, provided the computational cost is not an issue. 

% Figure environment removed

%% Comparing networks of undersized networks
Another crucial choice with RNN time series forecasting is the proper window input size. While some heuristics can be applied in choosing this parameter, we can also use distance correlation heatmaps to understand the impact. We exemplify this with a heatmap that compares an RNN with 6 inputs and 10 inputs under an AR(8) process which is shown in Figure \ref{fig:6_10_Net}. In this scenario, the smaller RNN shows that it can learn similar activation outputs to the larger 10 input RNN as shown by the similarities in the diagonal elements between activation layers 1-6 and 5-10, respectively. However, it seems that without having access to at least 8 inputs for the AR(8) process, suggesting that the smaller network cannot adjust its learning weights via a backpropagation through time algorithm \cite{werbos1990backpropagation} to produce accurate forecasts. This is supported by the lack of good fit in the time series forecast plot for the 6 input RNN and the better fit in the 10 input RNN. 

% Figure environment removed

%% Comparing networks of oversized networks
We can then adopt a conservative strategy of %Perhaps a strategy in 
choosing a window input size %is being conservative and choosing a 
that is larger than necessary. Putting aside the known issues with long term dependency and exploding gradient problems in RNNs, we use distance correlation heatmaps to identify how the RNNs learn time series when the networks inputs are oversized. Figure \ref{fig:10_20_Net} simulates this scenario where both the RNN input sizes (10 and 20, respectively) are greater than the AR(6) process lag structure. We notice that the diagonal streaks of similarities are 6 activation layers apart, which confirms that both the RNNs can detect the 6 lag structure of the time series. Further, the 20 input RNN encounters the lag structure multiple times. This redundancy likely aids in the backpropagation through time algorithm, where the adjusted layer weights are updated to have more accurate forecasts. 

% Figure environment removed

\section{Discussion}

% Quick recap of main results and implications
%From our experiments, 
We use distance correlation to show that the RNN activation layers can identify lag structures but have issues in transmitting that information to the final activation layer. This loss in information seems to occur over a span of 5-6 activation layers before the distance correlation values converge. The critical implication 
%of this 
is that the RNN forecasts of 
%modeling 
univariate time series processes with large lag structures are likely to be poor 
%perform poorly 
since they are subject to more information loss. This can affect design decisions such as the sampling rate. For example, %choosing 
a sampling rate of hours, as compared to days, may subject the RNN to larger lag structures, even though a higher resolution input sequence is obtained. 
%analysis is gained. 
Conversely, lower lag structures make RNNs a preferred model with %that requires quick 
fast forecast results as they are relatively computationally inexpensive. Additionally, low distance correlation values of the activation layers in MA processes show that the error lags are difficult to model for RNNs. This lower accuracy is exacerbated by the information loss of RNNs. We also observe that the low distance correlation values in all the activation layers for GARCH processes lead to poor performance. Thus, an RNN model of any heteroskedastic time series 
%that exhibits heteroscedasticity 
is not likely to perform well. 

These results can also help the practitioners with \textit{a priori} assessments of the suitability of RNN forecasts given the characteristics of their time series data. 
%about time series characteristics that are likely to perform well with an RNN, aiding them in pre-modeling assessments. 
For example, ACF plots, indicating whether the lag structures are high, can determine whether the corresponding time series processes are suitable for RNN forecasting. 
%to determine lag structures reveals whether a lag structure is high or low, with the former being undesirable for RNN modeling. 
The presence of AR, MA, ARMA, and GARCH processes are also indicated in the ACF plots, providing cues on whether RNNs would be suitable for the corresponding forecast problems. 
%same plots may also reveal whether an AR or MA process is present. Simply visually inspecting the raw time series values can show whether volatility is present, which fares poorly for RNN modeling. 
This knowledge is valuable in reducing unnecessary and time-consuming model building and exploration steps. 
%Knowing this is valuable as it can reduce the amount of model building and explorations required, which can have high costs. 
Coupling these observations with the %internal 
hyperparameter-based comparisons provided by the distance correlation heatmaps, analysts are now better equipped %with a set of interpretation tools that enhance their ability 
to both interpret and explain RNN modeling of time series.

% Why using Distance correlation is beneficial for RNN time
% Given these findings, we can use distance correlation values at each RNN activation layer and determine its forecasting performance without having any other information. For example, the correlation plots from Figures \ref{fig:AR_10_20}(b) (for an AR(20) process), \ref{fig:MA_1_20}(c) (MA(20) process) and \ref{fig:GARCH_22_44}(a) (GARCH(2,2) process) show that the embedded input information is gradually lost (or never present) upon reaching the final activation layer, allowing us to conclude that the forecasting accuracy will be low. In contrast, the increasing correlations until the final activation layer as in the AR(1) or ARMA(1,10) processes allow us to conclude that the RNN will perform well. Coupling these observations with the internal comparisons provided by the distance correlation heatmaps, analysts are now equipped with a set of interpretation tools that enhance the ability to understand RNN modeling of time series. 

% Our study is limited to simple cases
%While we \hl{believe this level of analysis is useful for interpreting RNN time series forecasting}, 
We acknowledge some ways to improve and expand our findings. First, we consider only well-established time series processes. This was done intentionally to create a controlled experiment, where the inputs and outputs of the time series processes are well defined. However, real-world time series data rarely adhere to the simplified nature of ARMA type processes. Instead, they are often more complex, volatile, and exhibit non-stationarities. We also only consider the most basic form of time series forecasting, which is univariate, single-step prediction. Multivariate and multiple horizon forecasting are expected to reveal more insights into the effectiveness of RNNs, and we believe that distance correlation is powerful and generic enough to be adapted to such problems.
%flexible enough to accommodate these potential improvements. 

% Cannot make theoretical guarantees
% We also want to recognize that our findings are not claiming any theoretical guarantees for RNN performance with time series forecasting. Though we found evidence that RNNs try to solve time series structure that follows a linear process like the auto-correlation function, our experiments do not make this claim true in an absolute sense. Instead, we view our proposed framework of using distance correlation as a measure that helps break down the complex components of deep learning architectures into ways that can simplify how they learn time series structures. This is essential, especially in the current era of exponential development of sophisticated deep learning models that do not have well established theoretical foundations. 

% Potential experimental expansions of our work with other RNN variants
We also recognize that our distance correlation-based analysis is done with only Elman RNNs. However, we contend that it can be %easily 
expanded to other widely-used RNN architectures, such as %The other more popular RNN variants include 
Long-Short Term Memory (LSTM) and Gated Recurrent Unit (GRU) models, which have more complex cell operations to alleviate some of the common pitfalls of RNNs. 
%We can use distance correlation to repeat the experiments of this paper with these variant architectures. We can also visualize the similarities and differences in the hidden and additional cell states of LSTMs via distance correlation heatmaps. 
It may be particularly interesting to investigate how the information is partitioned among 
%each of 
the regulatory gates and additional recurrent cell states of the LSTMs.
% Expansion of work beyond just RNNs
This analysis tool can also be extended to many other deep learning networks that attempt to address time series forecasting. This includes observing how a time series input evolves through every major component in 
%the architecture of 
a transformer or another hybrid architecture. Overall, we feel that distance correlation is a flexible metric that can potentially unlock the unexplained nature of deep learning models for time series forecasting tasks. 

\section{Conclusions}

In this paper, 
%To enhance our understanding of time series forecasting with RNNs, 
we develop a distance correlation framework 
%that allows us 
to study the effectiveness of RNNs for time series forecasting.
%this architecture. 
Specifically, we leverage the versatility of distance correlation to track the outputs of the RNN activation layers and examine how well they learn specific time series processes. Empirically, we find that the activation layers detect time series lag structures well, but tend to lose this information over a sequence of five-six layers, thereby affecting the forecast accuracy of series with large lag structures. Further, the activation layers have difficulty modeling the error lag terms in both moving average and heteroscedastic %(GARCH) 
processes. Last, our distance correlation heatmap comparisons reveal that certain network hyperparameters, such as the number of hidden units and activation function, matter less as compared to the input window size of an RNN for accurate forecasts. %Using these well supported results, practitioners will be able to inspect the important characteristics of their univariate time series history, and quickly decide if an RNN is a viable model for their problem. 
We, therefore, believe our framework is a foundational step toward improving our understanding of the ability of deep learning models %and their ability 
to handle time series forecasting tasks. 

% As the research community continues to explore solutions for time series forecasting tasks beyond classical methods, it is easy to understand why deep learning models like RNNs have gained popularity due to their ability to approximate highly complex functions. However, the use of RNNs (and other similar networks) also comes with a lack of understanding of how these networks learn to predict the \hl{time series horizon} values. 

% In this paper, we propose a general framework that connects time series characteristics and RNNs with the versatile metric of distance correlation
% %, a measure that can 
% to reveal the information flow \hl{of} activation layers, enhancing our ability to interpret RNNs. \hl{We use this framework to design a set of experiments, which reveals that the RNN activation layers reliably detect lag structure patterns of a given time series, yet partially loses that information upon generating a forecast, resulting in poor performance.} %Ashis: Split this sentence.
% \hl{It also revealed} that RNNs have difficulty modeling error lag terms in moving average processes and poorly model heteroscedasticity of GARCH processes. These \hl{limitation} explain why RNNs cannot properly learn time series with volatile variances, which are often found in real-world datasets.

% Further, we leverage distance correlation as a visualization tool with heatmaps. These distance correlation heatmaps allow us to compare networks quickly, identify certain lag structures, or visually evaluate the differences among the networks with varying hyperparameters. \hl{Such heatmaps can simplify the interpretability of the time series forecasting tasks with RNNs. By using our proposed method of distance correlation to assess time series deep RNN models, we are confident that our method can be used to gain a better understanding of deep learning models' capabilities and the roles they play in solving complex time series forecasting tasks.}


%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections

\appendix

\clearpage 

% \section{Additional Mean Correlation Plots}\label{Add_mean_Corrs}

% The following figures provide further tests of the mean correlation comparisons between the distance correlation method and the auto-correlation function. Figures \ref{fig:AR_7_9}, \ref{fig:MA_5_9}, \ref{fig:ARMA_77_99} and \ref{fig:GARCH_6_5} are in reference to AR, MA, ARMA, and GARCH processes, respectively. Reference to the interpretation of these results are discussed in section \ref{sec: RNN_limits}. 

% We do this for the AR(7) and AR(9) time series in figure \ref{fig:AR_7_9} in the appendix. 
% In Figure \ref{fig:AR_7_9}(b), \hl{the visual pattern is less apparent. However, the mean correlations at lags 5 and 9 exhibit relatively high values for both the methods, which aligns with the time series lag structure.}

% % Figure environment removed

% % Figure environment removed

% % Figure environment removed

% % Figure environment removed

\section{Additional Experimental Results}\label{Add_Exp_Results}

The following tables \ref{table: HN_128}, \ref{table: LR_001} display experiments in addition to the one discussed in table \ref{table: All_Exp}. These tables expand the RNN architecture to having a larger input hidden units to 128 (Table \ref{table: HN_128}) and a learning rate of 0.001 (Table \ref{table: LR_001}). The purpose of these additional tables is to show consistency in the results from section \ref{sec: RNN_limits}, even under a different set of hyperparameters.

\begin{table}[h!]
\caption{MSE and Distance Correlation results for all simulation experiments with hidden units 128}
\begin{tabular}{||p{0.2\textwidth} | p{0.2\textwidth} | p{0.15\textwidth} | p{0.15\textwidth} | p{0.15\textwidth}||} 
 \hline
 Time Series Process & Mean Squared Error & $\hat{R}(\textbf{A}_{t}^{(P)}, \textbf{Y})$ (max) & $\hat{R}(\textbf{A}_{T}^{(P)}, \textbf{Y})$ & Percent Change(\%)\\ [0.5ex] 
 \hline\hline
 AR(1) & 0.024 $\pm$ 0.005 & 0.952 & 0.952 &  0 \\ 
 \hline
 AR(5) & 0.070 $\pm$ 0.029 & 0.913 &  0.449 & 45 \\ 
 \hline
 AR(10) & 0.404 $\pm$ 0.197 & 0.947 & 0.402 & 58  \\ 
 \hline
 AR(20) & 1.112 $\pm$ 0.333 & 0.966 & 0.378 & 61  \\ 
 \hline
  MA(1) & 0.612 $\pm$ 0.034 & 0.434 & 0.434 & 0  \\ 
 \hline
 MA(5) & 0.772 $\pm$ 0.050 & 0.431 & 0.134 & 69  \\ 
 \hline
 MA(10) & 0.920 $\pm$ 0.059 & 0.431 & 0.105 & 76  \\ 
 \hline
 MA(20) & 1.080 $\pm$ 0.064 & 0.428 & 0.096 & 78  \\
 \hline
 ARMA(1,1) & 0.008 $\pm$ 0.002 & 0.947 & 0.947 & 0  \\ 
 \hline
 ARMA(1,10) & 0.012 $\pm$ 0.003 & 0.966 & 0.966 & 0 \\ 
 \hline
 ARMA(10,1) & 0.235 $\pm$ 0.137 & 0.946 & 0.482 & 49  \\ 
 \hline
 GARCH(2,2) & 0.85 $\pm$ 0.674 & 0.272 & 0.267 & 2  \\ 
 \hline
 GARCH(4,4) & 1.186 $\pm$ 1.081 & 0.218 & 0.208 & 5  \\ 
 \hline
\end{tabular}\label{table: HN_128}
\begin{minipage}{13cm}
%\small Notes: Results for all time series experiments with 128 units for the hidden units of the RNN. Mean squared errors are calculated with standard deviations for 50 simulation runs. $\hat{R}(\textbf{A}_{t}^{(P)}, \textbf{Y})$ (max) represents the max distance correlation for any layer number $t$. $\hat{R}(\textbf{A}_{T}^{(P)}, \textbf{Y})$ is the distance correlation value at layer number 20. The percent change is calculated as $(\hat{R}(\textbf{A}_{t}^{(P)}, \textbf{Y})_{max} - \hat{R}(\textbf{A}_{T}^{(P)}, \textbf{Y})) / \hat{R}(\textbf{A}_{t}^{(P)}, \textbf{Y})_{max}$ which measures how much information is lost during RNN training. In general, we see that larger lag structures tend to lose more information during training, leading to higher MSE scores. 
\end{minipage}
\end{table}


\begin{table}[h!]
\caption{MSE and Distance Correlation results for all simulation experiments with learning rate = 0.001}
\begin{tabular}{||p{0.2\textwidth} | p{0.2\textwidth} | p{0.15\textwidth} | p{0.15\textwidth} | p{0.15\textwidth}||} 
 \hline
 Time Series Process & Mean Squared Error & $\hat{R}(\textbf{A}_{t}^{(P)}, \textbf{Y})$ (max) & $\hat{R}(\textbf{A}_{T}^{(P)}, \textbf{Y})$ & Percent Change(\%)\\ [0.5ex] 
 \hline\hline
 AR(1) & 0.028 $\pm$ 0.007 & 0.964 & 0.9964 &  0 \\ 
 \hline
 AR(5) & 0.070 $\pm$ 0.036 & 0.917 &  0.416 & 55 \\ 
 \hline
 AR(10) & 0.438 $\pm$ 0.184 & 0.952 & 0.426 & 55  \\ 
 \hline
 AR(20) & 1.111 $\pm$ 0.250 & 0.965 & 0.346 & 64  \\ 
 \hline
  MA(1) & 0.826 $\pm$ 0.060 & 0.472 & 0.472 & 0  \\ 
 \hline
 MA(5) & 1.011 $\pm$ 0.078 & 0.450 & 0.170 & 62  \\ 
 \hline
 MA(10) & 1.213 $\pm$ 0.092 & 0.439 & 0.115 & 74  \\ 
 \hline
 MA(20) & 1.534 $\pm$ 0.130 & 0.431 & 0.106 & 75  \\
 \hline
 ARMA(1,1) & 0.008 $\pm$ 0.002 & 0.967 & 0.967 & 0  \\ 
 \hline
 ARMA(1,10) & 0.012 $\pm$ 0.004 & 0.972 & 0.972 & 0 \\ 
 \hline
 ARMA(10,1) & 0.193 $\pm$ 0.097 & 0.945 & 0.508 & 46  \\ 
 \hline
 GARCH(2,2) & 1.201 $\pm$ 1.034 & 0.279 & 0.270 & 27  \\ 
 \hline
 GARCH(4,4) & 1.235 $\pm$ 1.201 & 0.209 & 0.068 & 20  \\ 
 \hline
\end{tabular}\label{table: LR_001}
\begin{minipage}{13cm}
%\small Notes: Results for all time series experiments using an RNN learning rate of 0.001. The experimental setup as well as the overall outcomes are the same from Table \ref{table: All_Exp}.
\end{minipage}
\end{table}

% The following tables \ref{table: AddExp_T_15}, \ref{table: AddExp_HN128} display experiments in addition to the one discussed in \ref{sec: RNN_limits}. These tables provide a comparison in how RNNs and ACF methods solve the listed time series processes for forecasting via the energy distance score. These tables expand the RNN architecture to having a larger input window of 15 (Table \ref{table: AddExp_T_15}) and larger number of hidden units, 128, in the activation layers (Table \ref{table: AddExp_HN128}). The purpose of these additional tables is to show consistency in the results from table \ref{table: All_Exp}, even under a different set of hyperparameters.

% In a separate set of two additional experiments, we alter the RNN input size to 15 and change the number of hidden units in the activation layer to 128 while keep all other hyperparameters the same. The results for each additional experiment are shown in tables \ref{table: AddExp_T_15}, \ref{table: AddExp_HN128}. In general, we see that the results closely resemble our findings from table \ref{table: All_Exp}. The correlation values generated by the ACF and distance correlation seem to come from similar distributions as evident by the low scaled energy distance values for ARMA time series processes. For GARCH processes, the the combination of relatively high MSE values and low scaled energy distance values, showcases that RNN learning GARCH processes via a linear ACF method is not suitable for forecasting.

% \begin{table}[h!]
% \caption{MSE and energy distance results for all the simulation experiments.}
% \begin{tabular}{||c | c| c||} 
%  \hline
%  Time Series Process & Mean Squared Error & Energy Distance (Scaled) \\ [0.5ex] 
%  \hline\hline
%  AR(1) & 0.227 $\pm$ 0.030 & 0.22 (0.03)  \\ 
%  \hline
%  AR(5) & 0.467 $\pm$ 0.062 & 0.24 (0.03)  \\ 
%  \hline
%  AR(7) & 0.578 $\pm$ 0.091 & 0.19 (0.02)  \\ 
%  \hline
%  AR(9) & 0.605 $\pm$ 0.066 & 0.77 (0.10)  \\ 
%  \hline
% MA(1) & 0.665 $\pm$ 0.055 & 0.77 (0.10)  \\ 
%  \hline
%  MA(5) & 0.880 $\pm$ 0.073 & 0.79 (0.10)  \\ 
%  \hline
%  MA(7) & 0.925 $\pm$ 0.083 & 0.77 (0.10)  \\ 
%  \hline
%  MA(9) & 0.942 $\pm$ 0.077 & 0.84 (0.11)  \\
%  \hline
%  ARMA(1,1) & 0.080 $\pm$ 0.017 & 0.33 (0.04)  \\ 
%  \hline
%  ARMA(5,3) & 0.335 $\pm$ 0.048 & 0.25 (0.03)  \\ 
%  \hline
%  ARMA(7,6) & 0.367 $\pm$ 0.054 & 0.18 (0.02)  \\ 
%  \hline
%  ARMA(9,9) & 0.489 $\pm$ 0.075 & 0.68 (0.09)  \\ 
%  \hline
%  GARCH(2,2) & 1.125 $\pm$ 0.89 & 1.17 (0.15)  \\ 
%  \hline
%  GARCH(4,3) & 1.171 $\pm$ 0.852 & 0.95 (0.12)  \\ 
%  \hline
%  GARCH(6,5) & 1.392 $\pm$ 1.226 & 0.897 (0.12)  \\   
%  \hline
% \end{tabular}
% \label{table: AddExp_T_15}
% \begin{minipage}{13cm}
% \vspace{2mm}
% \small Notes: 
% %Results for all time series experiments. 
% All the hyperparameters are the same as listed in section \ref{sec: RNN_limits} except for an increase window size of 15. Mean squared errors are calculated with standard deviations for 50 simulation runs. Energy distance is shown with the scaled number in parentheses, where 0 signifies that the ACF and distance correlation methods are from the same distribution. A scaled energy distance value of 1 indicates a complete dissimilarity of distribution from both metrics according to our experiments. In general, we see relatively small scaled energy distance values for all the time series processes. However, we see a relatively large increase in MSE values for the GARCH processes. 
% \end{minipage}
% \end{table}

% % Do not need to repeat all details for notes on subsequent table
% \begin{table}[h!]
% \caption{MSE and energy distance results for all the simulation experiments.}
% \begin{tabular}{||c | c| c||} 
%  \hline
%  Time Series Process & Mean Squared Error & Energy Distance (Scaled) \\ [0.5ex] 
%  \hline\hline
%  AR(1) & 0.221 $\pm$ 0.027 & 0.24 (0.04)  \\ 
%  \hline
%  AR(5) & 0.462 $\pm$ 0.055 & 0.14 (0.02)  \\ 
%  \hline
%  AR(7) & 0.581 $\pm$ 0.078 & 0.15 (0.02)  \\ 
%  \hline
%  AR(9) & 0.568 $\pm$ 0.075 & 0.63 (0.10)  \\ 
%  \hline
% MA(1) & 0.655 $\pm$ 0.053 & 0.59 (0.09)  \\ 
%  \hline
%  MA(5) & 0.876 $\pm$ 0.067 & 0.65 (0.10)  \\ 
%  \hline
%  MA(7) & 0.931 $\pm$ 0.059 & 0.63 (0.10)  \\ 
%  \hline
%  MA(9) & 0.934 $\pm$ 0.073 & 0.71 (0.11)  \\
%  \hline
%  ARMA(1,1) & 0.073 $\pm$ 0.010 & 0.28 (0.04)  \\ 
%  \hline
%  ARMA(5,3) & 0.318 $\pm$ 0.036 & 0.28 (0.04)  \\ 
%  \hline
%  ARMA(7,6) & 0.333 $\pm$ 0.062 & 0.18 (0.03)  \\ 
%  \hline
%  ARMA(9,9) & 0.408 $\pm$ 0.067 & 0.71 (0.11)  \\ 
%  \hline
%  GARCH(2,2) & 1.25 $\pm$ 1.059 & 1.06 (0.17)  \\ 
%  \hline
%  GARCH(4,3) & 1.334 $\pm$ 0.943 & 0.876 (0.14)  \\ 
%  \hline
%  GARCH(6,5) & 1.420 $\pm$ 1.063 & 0.742 (0.12)  \\   
%  \hline
% \end{tabular}
% \label{table: AddExp_HN128}
% \begin{minipage}{13cm}
% \vspace{2mm}
% \small Notes: 
% %Results for all time series experiments. 
% All the hyperparameters are the same as listed in section \ref{sec: RNN_limits} except for an increase in the number of hidden units of activation layers to 128. Mean squared errors are calculated with standard deviations for 50 simulation runs. Energy distance is shown with the scaled number in parentheses, where 0 signifies that the ACF and distance correlation methods are from the same distribution. A scaled energy distance value of 1 indicates a complete dissimilarity of distribution from both metrics according to our experiments. In general, we see relatively small scaled energy distance values for all the time series processes. However, we see a relatively large increase in MSE values for the GARCH processes. 
% \end{minipage}
% \end{table}


%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
%%  \bibliographystyle{elsarticle-num} 
%%  \bibliography{<your bibdatabase>}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

\clearpage

\bibliographystyle{elsarticle-num} 
\bibliography{refs}

\end{document}
\endinput
%%
%% End of file `elsarticle-template-num.tex'.
