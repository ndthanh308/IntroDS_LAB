\begin{thebibliography}{105}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2021)Agarwal, Schwarzer, Castro, Courville, and
  Bellemare]{rliable_agarwal2021}
Rishabh Agarwal, Max Schwarzer, Pablo~Samuel Castro, Aaron~C. Courville, and
  Marc~G. Bellemare.
\newblock Deep reinforcement learning at the edge of the statistical precipice.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem[Ajay et~al.(2021)Ajay, Kumar, Agrawal, Levine, and
  Nachum]{opal_ajay2021}
Anurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and Ofir Nachum.
\newblock Opal: Offline primitive discovery for accelerating offline
  reinforcement learning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[An et~al.(2021)An, Moon, Kim, and Song]{edac_an2021}
Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun~Oh Song.
\newblock Uncertainty-based offline reinforcement learning with diversified
  q-ensemble.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray, Schneider, Fong,
  Welinder, McGrew, Tobin, Pieter~Abbeel, and Zaremba]{her_andrychowicz2017}
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong,
  Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter~Abbeel, and Wojciech
  Zaremba.
\newblock Hindsight experience replay.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2017.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ln_ba2016}
Jimmy Ba, Jamie~Ryan Kiros, and Geoffrey~E. Hinton.
\newblock Layer normalization.
\newblock \emph{ArXiv}, abs/1607.06450, 2016.

\bibitem[Bacon et~al.(2017)Bacon, Harb, and Precup]{oc_bacon2017}
Pierre-Luc Bacon, Jean Harb, and Doina Precup.
\newblock The option-critic architecture.
\newblock In \emph{AAAI Conference on Artificial Intelligence (AAAI)}, 2017.

\bibitem[Baker et~al.(2022)Baker, Akkaya, Zhokhov, Huizinga, Tang, Ecoffet,
  Houghton, Sampedro, and Clune]{vpt_baker2022}
Bowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien
  Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune.
\newblock Video pretraining (vpt): Learning to act by watching unlabeled online
  videos.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2022.

\bibitem[Brandfonbrener et~al.(2021)Brandfonbrener, Whitney, Ranganath, and
  Bruna]{onestep_brandfonbrener2021}
David Brandfonbrener, William~F. Whitney, Rajesh Ranganath, and Joan Bruna.
\newblock Offline rl without off-policy evaluation.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{openaigym_brockman2016}
G.~Brockman, Vicki Cheung, Ludwig Pettersson, J.~Schneider, John Schulman, Jie
  Tang, and W.~Zaremba.
\newblock {OpenAI Gym}.
\newblock \emph{ArXiv}, abs/1606.01540, 2016.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{gpt3_brown2020}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T.~J.
  Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeff Wu, Clemens
  Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
  Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
  Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Chane-Sane et~al.(2021)Chane-Sane, Schmid, and
  Laptev]{ris_chanesane2021}
Elliot Chane-Sane, Cordelia Schmid, and Ivan Laptev.
\newblock Goal-conditioned reinforcement learning with imagined subgoals.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Chang et~al.(2022)Chang, Gupta, and Gupta]{laq_chang2022}
Matthew Chang, Arjun Gupta, and Saurabh Gupta.
\newblock Learning value functions from undirected state-only experience.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2022.

\bibitem[Chebotar et~al.(2021)Chebotar, Hausman, Lu, Xiao, Kalashnikov, Varley,
  Irpan, Eysenbach, Julian, Finn, and Levine]{am_chebotar2021}
Yevgen Chebotar, Karol Hausman, Yao Lu, Ted Xiao, Dmitry Kalashnikov, Jacob
  Varley, Alex Irpan, Benjamin Eysenbach, Ryan~C. Julian, Chelsea Finn, and
  Sergey Levine.
\newblock Actionable models: Unsupervised offline reinforcement learning of
  robotic skills.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Chen et~al.(2021)Chen, Lu, Rajeswaran, Lee, Grover, Laskin, Abbeel,
  Srinivas, and Mordatch]{dt_chen2021}
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael
  Laskin, P.~Abbeel, A.~Srinivas, and Igor Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and
  Hinton]{simclr_chen2020}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey~E. Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Cobbe et~al.(2020)Cobbe, Hesse, Hilton, and
  Schulman]{procgen_cobbe2019}
Karl Cobbe, Christopher Hesse, Jacob Hilton, and John Schulman.
\newblock Leveraging procedural generation to benchmark reinforcement learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Dayan and Hinton(1992)]{feudal_dayan1992}
Peter Dayan and Geoffrey~E. Hinton.
\newblock Feudal reinforcement learning.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 1992.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{bert_devlin2019}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Conference of the North American Chapter of the Association
  for Computational Linguistics: Human Language Technologies (NAACL-HLT)},
  2019.

\bibitem[Ding et~al.(2019)Ding, Florensa, Phielipp, and
  Abbeel]{goalgail_ding2019}
Yiming Ding, Carlos Florensa, Mariano Phielipp, and P.~Abbeel.
\newblock Goal-conditioned imitation learning.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2019.

\bibitem[Durugkar et~al.(2021)Durugkar, Tec, Niekum, and
  Stone]{aim_durugkar2021}
Ishan Durugkar, Mauricio Tec, Scott Niekum, and Peter Stone.
\newblock Adversarial intrinsic motivation for reinforcement learning.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem[Espeholt et~al.(2018)Espeholt, Soyer, Munos, Simonyan, Mnih, Ward,
  Doron, Firoiu, Harley, Dunning, Legg, and Kavukcuoglu]{impala_espeholt2018}
Lasse Espeholt, Hubert Soyer, R{\'e}mi Munos, Karen Simonyan, Volodymyr Mnih,
  Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and
  Koray Kavukcuoglu.
\newblock Impala: Scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Eysenbach et~al.(2019)Eysenbach, Salakhutdinov, and
  Levine]{sorb_eysenbach2019}
Benjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine.
\newblock Search on the replay buffer: Bridging planning and reinforcement
  learning.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2019.

\bibitem[Eysenbach et~al.(2021)Eysenbach, Salakhutdinov, and
  Levine]{cl_eysenbach2021}
Benjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine.
\newblock C-learning: Learning to achieve goals via recursive classification.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[Eysenbach et~al.(2022)Eysenbach, Zhang, Salakhutdinov, and
  Levine]{contrastive_eysenbach2022}
Benjamin Eysenbach, Tianjun Zhang, Ruslan Salakhutdinov, and Sergey Levine.
\newblock Contrastive learning as goal-conditioned reinforcement learning.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2022.

\bibitem[Fang et~al.(2022{\natexlab{a}})Fang, Yin, Nair, and
  Levine]{ptp_fang2022}
Kuan Fang, Patrick Yin, Ashvin Nair, and Sergey Levine.
\newblock Planning to practice: Efficient online fine-tuning by composing goals
  in latent space.
\newblock In \emph{IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS)}, 2022{\natexlab{a}}.

\bibitem[Fang et~al.(2022{\natexlab{b}})Fang, Yin, Nair, Walke, Yan, and
  Levine]{flap_fang2022}
Kuan Fang, Patrick Yin, Ashvin Nair, Homer Walke, Gengchen Yan, and Sergey
  Levine.
\newblock Generalization with lossy affordances: Leveraging broad offline data
  for learning visuomotor tasks.
\newblock In \emph{Conference on Robot Learning (CoRL)}, 2022{\natexlab{b}}.

\bibitem[Fang et~al.(2019)Fang, Zhou, Shi, Gong, Xu, and Zhang]{dher_fang2019}
Meng Fang, Cheng Zhou, Bei Shi, Boqing Gong, Jia Xu, and Tong Zhang.
\newblock Dher: Hindsight experience replay for dynamic goals.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{d4rl_fu2020}
Justin Fu, Aviral Kumar, Ofir Nachum, G.~Tucker, and Sergey Levine.
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock \emph{ArXiv}, abs/2004.07219, 2020.

\bibitem[Fujimoto et~al.(2019)Fujimoto, Meger, and Precup]{bcq_fujimoto2019}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Garg et~al.(2023)Garg, Hejna, Geist, and Ermon]{xql_garg2023}
Divyansh Garg, Joey Hejna, Matthieu Geist, and Stefano Ermon.
\newblock Extreme q-learning: Maxent rl without entropy.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2023.

\bibitem[Ghasemipour et~al.(2021)Ghasemipour, Schuurmans, and
  Gu]{emaq_ghasemipour2021}
Seyed Kamyar~Seyed Ghasemipour, Dale Schuurmans, and Shixiang~Shane Gu.
\newblock Emaq: Expected-max q-learning operator for simple yet effective
  offline and online rl.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Ghosh(2023)]{jaxrlm_ghosh2023}
Dibya Ghosh.
\newblock dibyaghosh/jaxrl\_m, 2023.
\newblock URL \url{https://github.com/dibyaghosh/jaxrl_m}.

\bibitem[Ghosh et~al.(2021)Ghosh, Gupta, Reddy, Fu, Devin, Eysenbach, and
  Levine]{gcsl_ghosh2019}
Dibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Coline Devin, Benjamin
  Eysenbach, and Sergey Levine.
\newblock Learning to reach goals via iterated supervised learning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[Ghosh et~al.(2023)Ghosh, Bhateja, and Levine]{icvf_ghosh2023}
Dibya Ghosh, Chethan Bhateja, and Sergey Levine.
\newblock Reinforcement learning from passive data via latent intentions.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2023.

\bibitem[Gupta et~al.(2019)Gupta, Kumar, Lynch, Levine, and
  Hausman]{ril_gupta2019}
Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman.
\newblock Relay policy learning: Solving long-horizon tasks via imitation and
  reinforcement learning.
\newblock In \emph{Conference on Robot Learning (CoRL)}, 2019.

\bibitem[He et~al.(2022)He, Chen, Xie, Li, Doll'ar, and Girshick]{mae_he2022}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll'ar, and Ross~B.
  Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock In \emph{IEEE/CVF Computer Vision and Pattern Recognition Conference
  (CVPR)}, 2022.

\bibitem[Hendrycks and Gimpel(2016)]{gelu_hendrycks2016}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units (gelus).
\newblock \emph{ArXiv}, abs/1606.08415, 2016.

\bibitem[Hoang et~al.(2021)Hoang, Sohn, Choi, Carvalho, and Lee]{sfl_hoang2021}
Christopher Hoang, Sungryull Sohn, Jongwook Choi, Wilka Carvalho, and Honglak
  Lee.
\newblock Successor feature landmarks for long-horizon goal-conditioned
  reinforcement learning.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem[Hong et~al.(2022)Hong, Yang, and Agrawal]{bvn_hong2022}
Zhang-Wei Hong, Ge~Yang, and Pulkit Agrawal.
\newblock Bilinear value networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2022.

\bibitem[Huang et~al.(2019)Huang, Liu, and Su]{mss_huang2019}
Zhiao Huang, Fangchen Liu, and Hao Su.
\newblock Mapping state space using landmarks for universal goal reaching.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2019.

\bibitem[Janner et~al.(2021)Janner, Li, and Levine]{tt_janner2021}
Michael Janner, Qiyang Li, and Sergey Levine.
\newblock Reinforcement learning as one big sequence modeling problem.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem[Janner et~al.(2022)Janner, Du, Tenenbaum, and
  Levine]{diffuser_janner2022}
Michael Janner, Yilun Du, Joshua~B. Tenenbaum, and Sergey Levine.
\newblock Planning with diffusion for flexible behavior synthesis.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2022.

\bibitem[Jiang et~al.(2023)Jiang, Zhang, Janner, Li, Rocktaschel, Grefenstette,
  and Tian]{tap_jiang2022}
Zhengyao Jiang, Tianjun Zhang, Michael Janner, Yueying Li, Tim Rocktaschel,
  Edward Grefenstette, and Yuandong Tian.
\newblock Efficient planning in a compact latent action space.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2023.

\bibitem[Kaelbling(1993)]{gcrl_kaelbling1993}
Leslie~Pack Kaelbling.
\newblock Learning to achieve goals.
\newblock In \emph{International Joint Conference on Artificial Intelligence
  (IJCAI)}, 1993.

\bibitem[Kim et~al.(2021)Kim, Seo, and Shin]{higl_kim2021}
Junsu Kim, Younggyo Seo, and Jinwoo Shin.
\newblock Landmark-guided subgoal generation in hierarchical reinforcement
  learning.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem[Kim et~al.(2023)Kim, Seo, Ahn, Son, and Shin]{pig_kim2023}
Junsu Kim, Younggyo Seo, Sungsoo Ahn, Kyunghwan Son, and Jinwoo Shin.
\newblock Imitating graph-based planning with goal-conditioned policies.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2023.

\bibitem[Kingma and Ba(2015)]{adam_kingma2014}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2015.

\bibitem[Kostrikov et~al.(2021)Kostrikov, Yarats, and
  Fergus]{drq_kostrikov2021}
Ilya Kostrikov, Denis Yarats, and Rob Fergus.
\newblock Image augmentation is all you need: Regularizing deep reinforcement
  learning from pixels.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[Kostrikov et~al.(2022)Kostrikov, Nair, and Levine]{iql_kostrikov2021}
Ilya Kostrikov, Ashvin Nair, and Sergey Levine.
\newblock Offline reinforcement learning with implicit q-learning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2022.

\bibitem[Krishnan et~al.(2017)Krishnan, Fox, Stoica, and
  Goldberg]{ddco_krishnan2017}
Sanjay Krishnan, Roy Fox, Ion Stoica, and Ken Goldberg.
\newblock Ddco: Discovery of deep continuous options for robot learning from
  demonstrations.
\newblock In \emph{Conference on Robot Learning (CoRL)}, 2017.

\bibitem[Kulkarni et~al.(2016)Kulkarni, Narasimhan, Saeedi, and
  Tenenbaum]{hdqn_kulkarni2016}
Tejas~D. Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Joshua~B. Tenenbaum.
\newblock Hierarchical deep reinforcement learning: Integrating temporal
  abstraction and intrinsic motivation.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2016.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and Levine]{cql_kumar2020}
Aviral Kumar, Aurick Zhou, G.~Tucker, and Sergey Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Kumar et~al.(2022)Kumar, Agarwal, Ma, Courville, Tucker, and
  Levine]{dr3_kumar2022}
Aviral Kumar, Rishabh Agarwal, Tengyu Ma, Aaron~C. Courville, G.~Tucker, and
  Sergey Levine.
\newblock Dr3: Value-based deep reinforcement learning requires explicit
  regularization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2022.

\bibitem[Lange et~al.(2012)Lange, Gabel, and Riedmiller]{batch_lange2012}
Sascha Lange, Thomas Gabel, and Martin Riedmiller.
\newblock Batch reinforcement learning.
\newblock In \emph{Reinforcement learning: State-of-the-art}, pages 45--73.
  Springer, 2012.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{offline_levine2020}
Sergey Levine, Aviral Kumar, G.~Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{ArXiv}, abs/2005.01643, 2020.

\bibitem[Levy et~al.(2019)Levy, Konidaris, Platt, and Saenko]{hac_levy2019}
Andrew Levy, George~Dimitri Konidaris, Robert~W. Platt, and Kate Saenko.
\newblock Learning multi-level hierarchies with hindsight.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Li et~al.(2020)Li, Pinto, and Abbeel]{gh_li2020}
Alexander~C. Li, Lerrel Pinto, and P.~Abbeel.
\newblock Generalized hindsight for reinforcement learning.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Li et~al.(2022)Li, Tang, Tomizuka, and Zhan]{higoc_li2022}
Jinning Li, Chen Tang, Masayoshi Tomizuka, and Wei Zhan.
\newblock Hierarchical planning through goal-conditioned offline reinforcement
  learning.
\newblock \emph{IEEE Robotics and Automation Letters (RA-L)}, 7\penalty0
  (4):\penalty0 10216--10223, 2022.

\bibitem[Lynch et~al.(2019)Lynch, Khansari, Xiao, Kumar, Tompson, Levine, and
  Sermanet]{play_lynch2019}
Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey
  Levine, and Pierre Sermanet.
\newblock Learning latent plans from play.
\newblock In \emph{Conference on Robot Learning (CoRL)}, 2019.

\bibitem[Ma et~al.(2022)Ma, Yan, Jayaraman, and Bastani]{gofar_ma2022}
Yecheng~Jason Ma, Jason Yan, Dinesh Jayaraman, and Osbert Bastani.
\newblock How far i'll go: Offline goal-conditioned reinforcement learning via
  f-advantage regression.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2022.

\bibitem[Ma et~al.(2023)Ma, Sodhani, Jayaraman, Bastani, Kumar, and
  Zhang]{vip_ma2023}
Yecheng~Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash
  Kumar, and Amy Zhang.
\newblock Vip: Towards universal visual reward and representation via
  value-implicit pre-training.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2023.

\bibitem[Machado et~al.(2017)Machado, Bellemare, and
  Bowling]{eigen_machado2017}
Marlos~C. Machado, Marc~G. Bellemare, and Michael Bowling.
\newblock A laplacian framework for option discovery in reinforcement learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2017.

\bibitem[Mees et~al.(2022)Mees, Hermann, Rosete-Beas, and
  Burgard]{calvin_mees2022}
Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Wolfram Burgard.
\newblock Calvin: A benchmark for language-conditioned policy learning for
  long-horizon robot manipulation tasks.
\newblock \emph{IEEE Robotics and Automation Letters (RA-L)}, 7\penalty0
  (3):\penalty0 7327--7334, 2022.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{dqn_mnih2013}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin~A. Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{ArXiv}, abs/1312.5602, 2013.

\bibitem[Nachum et~al.(2018)Nachum, Gu, Lee, and Levine]{hiro_nachum2018}
Ofir Nachum, Shixiang~Shane Gu, Honglak Lee, and Sergey Levine.
\newblock Data-efficient hierarchical reinforcement learning.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2018.

\bibitem[Nachum et~al.(2019)Nachum, Gu, Lee, and
  Levine]{nearoptimal_nachum2019}
Ofir Nachum, Shixiang~Shane Gu, Honglak Lee, and Sergey Levine.
\newblock Near-optimal representation learning for hierarchical reinforcement
  learning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Nair et~al.(2020)Nair, Dalal, Gupta, and Levine]{awac_nair2020}
Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine.
\newblock Accelerating online reinforcement learning with offline datasets.
\newblock \emph{ArXiv}, abs/2006.09359, 2020.

\bibitem[Nair et~al.(2022)Nair, Rajeswaran, Kumar, Finn, and
  Gupta]{r3m_nair2022}
Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhi Gupta.
\newblock R3m: A universal visual representation for robot manipulation.
\newblock In \emph{Conference on Robot Learning (CoRL)}, 2022.

\bibitem[Nasiriany et~al.(2019)Nasiriany, Pong, Lin, and
  Levine]{leap_nasiriany2019}
Soroush Nasiriany, Vitchyr~H. Pong, Steven Lin, and Sergey Levine.
\newblock Planning with goal-conditioned policies.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2019.

\bibitem[Neumann and Peters(2008)]{lawer_neumann2008}
Gerhard Neumann and Jan Peters.
\newblock Fitted q-iteration by advantage weighted regression.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2008.

\bibitem[Peng et~al.(2019)Peng, Kumar, Zhang, and Levine]{awr_peng2019}
Xue~Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine.
\newblock Advantage-weighted regression: Simple and scalable off-policy
  reinforcement learning.
\newblock \emph{ArXiv}, abs/1910.00177, 2019.

\bibitem[Pertsch et~al.(2020)Pertsch, Lee, and Lim]{spirl_pertsch2020}
Karl Pertsch, Youngwoon Lee, and Joseph~J. Lim.
\newblock Accelerating reinforcement learning with learned skill priors.
\newblock In \emph{Conference on Robot Learning (CoRL)}, 2020.

\bibitem[Peters and Schaal(2007)]{rwr_peters2007}
Jan Peters and Stefan Schaal.
\newblock Reinforcement learning by reward-weighted regression for operational
  space control.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2007.

\bibitem[Peters et~al.(2010)Peters, Muelling, and Altun]{reps_peters2010}
Jan Peters, Katharina Muelling, and Yasemin Altun.
\newblock Relative entropy policy search.
\newblock In \emph{AAAI Conference on Artificial Intelligence (AAAI)}, 2010.

\bibitem[Pong et~al.(2018)Pong, Gu, Dalal, and Levine]{tdm_pong2018}
Vitchyr~H. Pong, Shixiang~Shane Gu, Murtaza Dalal, and Sergey Levine.
\newblock Temporal difference models: Model-free deep rl for model-based
  control.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Rosete-Beas et~al.(2022)Rosete-Beas, Mees, Kalweit, Boedecker, and
  Burgard]{tacorl_rosetebeas2022}
Erick Rosete-Beas, Oier Mees, Gabriel Kalweit, Joschka Boedecker, and Wolfram
  Burgard.
\newblock Latent plans for task-agnostic offline reinforcement learning.
\newblock In \emph{Conference on Robot Learning (CoRL)}, 2022.

\bibitem[Salter et~al.(2022)Salter, Wulfmeier, Tirumala, Heess, Riedmiller,
  Hadsell, and Rao]{mo2_salter2022}
Sasha Salter, Markus Wulfmeier, Dhruva Tirumala, Nicolas Manfred~Otto Heess,
  Martin~A. Riedmiller, Raia Hadsell, and Dushyant Rao.
\newblock Mo2: Model-based offline options.
\newblock In \emph{Conference on Lifelong Learning Agents (CoLLAs)}, 2022.

\bibitem[Savinov et~al.(2018)Savinov, Dosovitskiy, and
  Koltun]{sptm_savinov2018}
Nikolay Savinov, Alexey Dosovitskiy, and Vladlen Koltun.
\newblock Semi-parametric topological memory for navigation.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Schaul et~al.(2015)Schaul, Horgan, Gregor, and
  Silver]{uvfa_schaul2015}
Tom Schaul, Dan Horgan, Karol Gregor, and David Silver.
\newblock Universal value function approximators.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2015.

\bibitem[Schmeckpeper et~al.(2020)Schmeckpeper, Rybkin, Daniilidis, Levine, and
  Finn]{rlv_schmeckpeper2020}
Karl Schmeckpeper, Oleh Rybkin, Kostas Daniilidis, Sergey Levine, and Chelsea
  Finn.
\newblock Reinforcement learning with videos: Combining offline observations
  with interaction.
\newblock In \emph{Conference on Robot Learning (CoRL)}, 2020.

\bibitem[Schmidhuber(1991)]{subgoal_schmidhuber1991}
J{\"u}rgen Schmidhuber.
\newblock Learning to generate sub-goals for action sequences.
\newblock In \emph{Artificial neural networks}, 1991.

\bibitem[Seo et~al.(2022)Seo, Lee, James, and Abbeel]{apv_seo2022}
Younggyo Seo, Kimin Lee, Stephen James, and P.~Abbeel.
\newblock Reinforcement learning with action-free pre-training from videos.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2022.

\bibitem[Shah et~al.(2021)Shah, Eysenbach, Kahn, Rhinehart, and
  Levine]{recon_shah2021}
Dhruv Shah, Benjamin Eysenbach, Gregory Kahn, Nicholas Rhinehart, and Sergey
  Levine.
\newblock Recon: Rapid exploration for open-world navigation with latent goal
  models.
\newblock In \emph{Conference on Robot Learning (CoRL)}, 2021.

\bibitem[Shi et~al.(2022)Shi, Lim, and Lee]{skimo_shi2022}
Lu~Shi, Joseph~J. Lim, and Youngwoon Lee.
\newblock Skill-based model-based reinforcement learning.
\newblock In \emph{Conference on Robot Learning (CoRL)}, 2022.

\bibitem[Stolle and Precup(2002)]{option_stolle2002}
Martin Stolle and Doina Precup.
\newblock Learning options in reinforcement learning.
\newblock In \emph{Symposium on Abstraction, Reformulation and Approximation},
  2002.

\bibitem[Sutton et~al.(1999)Sutton, Precup, and Singh]{option_sutton1999}
Richard~S Sutton, Doina Precup, and Satinder Singh.
\newblock Between mdps and semi-mdps: A framework for temporal abstraction in
  reinforcement learning.
\newblock \emph{Artificial intelligence}, 112\penalty0 (1-2):\penalty0
  181--211, 1999.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{mujoco_todorov2012}
Emanuel Todorov, Tom Erez, and Yuval Tassa.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS)}, 2012.

\bibitem[Torabi et~al.(2018)Torabi, Warnell, and Stone]{bco_torabi2018}
Faraz Torabi, Garrett Warnell, and Peter Stone.
\newblock Behavioral cloning from observation.
\newblock In \emph{International Joint Conference on Artificial Intelligence
  (IJCAI)}, 2018.

\bibitem[van~den Oord et~al.(2017)van~den Oord, Vinyals, and
  Kavukcuoglu]{vqvae_oord2017}
A{\"a}ron van~den Oord, Oriol Vinyals, and Koray Kavukcuoglu.
\newblock Neural discrete representation learning.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2017.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{transformer_vaswani2017}
Ashish Vaswani, Noam~M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2017.

\bibitem[Vezhnevets et~al.(2017)Vezhnevets, Osindero, Schaul, Heess, Jaderberg,
  Silver, and Kavukcuoglu]{fun_vezhnevets2017}
Alexander~Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Manfred~Otto
  Heess, Max Jaderberg, David Silver, and Koray Kavukcuoglu.
\newblock Feudal networks for hierarchical reinforcement learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2017.

\bibitem[Villaflor et~al.(2022)Villaflor, Huang, Pande, Dolan, and
  Schneider]{optimism_villaflor2022}
Adam~R. Villaflor, Zheng Huang, Swapnil Pande, John~M. Dolan, and Jeff~G.
  Schneider.
\newblock Addressing optimism bias in sequence modeling for reinforcement
  learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2022.

\bibitem[Wang et~al.(2023)Wang, Torralba, Isola, and Zhang]{quasi_wang2023}
Tongzhou Wang, Antonio Torralba, Phillip Isola, and Amy Zhang.
\newblock Optimal goal-reaching reinforcement learning via quasimetric
  learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2023.

\bibitem[Wang et~al.(2020)Wang, Novikov, Zolna, Springenberg, Reed, Shahriari,
  Siegel, Merel, Gulcehre, Heess, and de~Freitas]{crr_wang2020}
Ziyun Wang, Alexander Novikov, Konrad Zolna, Jost~Tobias Springenberg, Scott~E.
  Reed, Bobak Shahriari, Noah Siegel, Josh Merel, Caglar Gulcehre, Nicolas
  Manfred~Otto Heess, and Nando de~Freitas.
\newblock Critic regularized regression.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Wu et~al.(2019)Wu, Tucker, and Nachum]{brac_wu2019}
Yifan Wu, G.~Tucker, and Ofir Nachum.
\newblock Behavior regularized offline reinforcement learning.
\newblock \emph{ArXiv}, abs/1911.11361, 2019.

\bibitem[Wulfmeier et~al.(2021)Wulfmeier, Rao, Hafner, Lampe, Abdolmaleki,
  Hertweck, Neunert, Tirumala, Siegel, Heess, and
  Riedmiller]{ho2_wulfmeier2021}
Markus Wulfmeier, Dushyant Rao, Roland Hafner, Thomas Lampe, Abbas Abdolmaleki,
  Tim Hertweck, Michael Neunert, Dhruva Tirumala, Noah Siegel, Nicolas
  Manfred~Otto Heess, and Martin~A. Riedmiller.
\newblock Data-efficient hindsight off-policy option learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Xu et~al.(2022)Xu, Jiang, Li, and Zhan]{por_xu2022}
Haoran Xu, Li~Jiang, Jianxiong Li, and Xianyuan Zhan.
\newblock A policy-guided imitation approach for offline reinforcement
  learning.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2022.

\bibitem[Xu et~al.(2023)Xu, Jiang, Li, Yang, Wang, Chan, and Zhan]{sql_xu2023}
Haoran Xu, Li~Jiang, Jianxiong Li, Zhuoran Yang, Zhaoran Wang, Victor Chan, and
  Xianyuan Zhan.
\newblock Offline rl with no ood actions: In-sample learning via implicit value
  regularization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2023.

\bibitem[Yang et~al.(2023)Yang, Schuurmans, Abbeel, and Nachum]{doc_yang2022}
Mengjiao Yang, Dale Schuurmans, P.~Abbeel, and Ofir Nachum.
\newblock Dichotomy of control: Separating what you can control from what you
  cannot.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2023.

\bibitem[Yang et~al.(2022)Yang, Lu, Li, Sun, Fang, Du, Li, Han, and
  Zhang]{wgcsl_yang2022}
Rui Yang, Yiming Lu, Wenzhe Li, Hao Sun, Meng Fang, Yali Du, Xiu Li, Lei Han,
  and Chongjie Zhang.
\newblock Rethinking goal-conditioned supervised learning and its connection to
  offline rl.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2022.

\bibitem[Zhang et~al.(2021)Zhang, Yang, and Stadie]{l3p_zhang2021}
Lunjun Zhang, Ge~Yang, and Bradly~C. Stadie.
\newblock World model as a graph: Learning latent landmarks for planning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Zhang et~al.(2022)Zhang, Eysenbach, Salakhutdinov, Levine, and
  Gonzalez]{cp_zhang2022}
Tianjun Zhang, Benjamin Eysenbach, Ruslan Salakhutdinov, Sergey Levine, and
  Joseph Gonzalez.
\newblock C-planning: An automatic curriculum for learning goal-reaching tasks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2022.

\bibitem[Zhang et~al.(2020)Zhang, Guo, Tan, Hu, and Chen]{hrac_zhang2020}
Tianren Zhang, Shangqi Guo, Tian Tan, Xiaolin Hu, and Feng Chen.
\newblock Generating adjacency-constrained subgoals in hierarchical
  reinforcement learning.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Zheng et~al.(2023{\natexlab{a}})Zheng, Eysenbach, Walke, Yin, Fang,
  Salakhutdinov, and Levine]{stablecrl_zheng2023}
Chongyi Zheng, Benjamin Eysenbach, Homer Walke, Patrick Yin, Kuan Fang, Ruslan
  Salakhutdinov, and Sergey Levine.
\newblock Stabilizing contrastive rl: Techniques for offline goal reaching.
\newblock \emph{ArXiv}, abs/2306.03346, 2023{\natexlab{a}}.

\bibitem[Zheng et~al.(2023{\natexlab{b}})Zheng, Henaff, Amos, and
  Grover]{ssorl_zheng2023}
Qinqing Zheng, Mikael Henaff, Brandon Amos, and Aditya Grover.
\newblock Semi-supervised offline reinforcement learning with action-free
  trajectories.
\newblock In \emph{International Conference on Machine Learning (ICML)},
  2023{\natexlab{b}}.

\end{thebibliography}
