\documentclass{article}


%
%
%
%
\PassOptionsToPackage{comma,numbers,sort,compress}{natbib}


%
%


%
%
%


%
\usepackage[final]{neurips_2023}


%
%
\usepackage{basic_commands}

\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{hyperref}       %
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{xcolor}         %
\usepackage{dsfont}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{amsthm}
\usepackage{cleveref}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\captionsetup[table]{skip=5pt}

\usepackage{enumitem}
\input{math_commands}

\RequirePackage{algorithm}
\RequirePackage{algorithmic}
%


\definecolor{candypink}{rgb}{0.89, 0.44, 0.48}          %
\definecolor{mediumaquamarine}{rgb}{0.4, 0.8, 0.67}     %
\definecolor{azure}{rgb}{0.0, 0.5, 1.0}                 %
\definecolor{awesome}{rgb}{1.0, 0.13, 0.32}             %
\definecolor{myred}{HTML}{F54254}
\definecolor{myblue}{HTML}{598BE7}
\definecolor{mydarkblue}{HTML}{385492}

\newcommand{\cutsectionup}{\vspace{-5pt}}
\newcommand{\cutsectiondown}{\vspace{-7pt}}
\newcommand{\cutsubsectionup}{\vspace{-3pt}}
\newcommand{\cutsubsectiondown}{\vspace{-4pt}}

\newcommand{\sh}[1]{{\color{candypink}{\small\bf\sf [Seohong: #1]}}}
\newcommand{\dibya}[1]{{\color{myred}{[D: #1]}}}
%
\def\methodname{HIQL\xspace}

\newcommand{\hiqlcode}{{\url{https://github.com/seohongpark/HIQL}}}
\newcommand{\hiqlwebsite}{{\url{https://seohong.me/projects/hiql/}}}


%
%
%
%
%
\title{HIQL: Offline Goal-Conditioned RL \\ with Latent States as Actions}
%
%
%
%
%
%
%
%
%
%

%


%

%
%
%
%
%
%
%


\author{%
  Seohong Park$^{1}$ \quad Dibya Ghosh$^{1}$ \quad Benjamin Eysenbach$^{2}$ \quad Sergey Levine$^{1}$ \\
  $^{1}$University of California, Berkeley \quad $^{2}$Princeton University \\
  \texttt{seohong@berkeley.edu} \\
  %
  %
  %
  %
  %
  %
}


\begin{document}


\maketitle

\vspace{-5pt}
\begin{abstract}
\vspace{-3pt}

%

%
%
%
%
%
%
%
%
%
%
%
Unsupervised pre-training has recently
%
become the bedrock for computer vision and natural language processing.
In reinforcement learning (RL),
goal-conditioned RL can potentially provide an analogous self-supervised approach for making use of large quantities of unlabeled (reward-free) data.
However, building effective algorithms for goal-conditioned RL that can learn directly from diverse offline data is challenging, because
%
%
it is hard to accurately estimate the exact value function for faraway goals.
Nonetheless, goal-reaching problems exhibit structure, such that reaching distant goals entails first passing through closer subgoals.
This structure can be very useful, as assessing the quality of actions for nearby goals is typically easier than for more distant goals.
Based on this idea, we propose a hierarchical algorithm for goal-conditioned RL from offline data.
Using one action-free value function,
we learn two policies that allow us to exploit this structure:
%
%
a high-level policy that treats states as actions and predicts (a latent representation of) a subgoal
and a low-level policy that predicts the action for reaching this subgoal. 
%
Through analysis and didactic examples, we show how this hierarchical decomposition makes our method robust to noise in the estimated value function. We then apply our method to offline goal-reaching benchmarks, showing that our method can solve long-horizon tasks that stymie prior methods,
can scale to high-dimensional image observations, and can readily make use of action-free data.
Our code is available at \hiqlwebsite
%
%
%
%
\end{abstract}

\cutsectionup
\cutsectionup
\section{Introduction}
\cutsectiondown

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%

Many of the most successful machine learning systems for computer vision~\citep{simclr_chen2020,mae_he2022}
and natural language processing~\citep{bert_devlin2019,gpt3_brown2020} leverage large amounts of unlabeled or weakly-labeled data.
%
%
In the reinforcement learning (RL) setting, offline goal-conditioned RL provides an analogous way to potentially leverage large amounts of multi-task data without reward labels or video data without action labels:
offline learning~\citep{batch_lange2012,offline_levine2020} enables leveraging previously collected and passively observed data,
and goal-conditioned RL~\citep{gcrl_kaelbling1993,uvfa_schaul2015} enables learning from unlabeled, reward-free data.
%
%
However, offline goal-conditioned RL poses major challenges.
%
%
%
First, learning an accurate goal-conditioned value function for any state and goal pair is challenging
when considering very broad and long-horizon goal-reaching tasks.
This often results in a noisy value function and thus potentially an erroneous policy.
%
%
%
%
%
%
%
%
%
%
Second, while the offline setting unlocks the potential for using previously collected data,
it is not straightforward to incorporate vast quantities of existing action-free video data into standard RL methods.
%
%
%
In this work, we aim to address these challenges by developing an effective offline goal-conditioned RL method that can learn to reach distant goals, readily make use of data without reward labels, and even utilize data without actions.
%

%
%
%
%

One straightforward approach to offline goal-conditioned RL is to first train a goal-conditioned value function and then train a policy that leads to states with high values.
%
%
%
%
%
%
However, many prior papers have observed that goal-conditioned RL is very difficult,
particularly when combined with offline training and distant goals~\citep{ril_gupta2019,sfl_hoang2021,cp_zhang2022}.
We observe that part of this difficulty stems from the ``signal-to-noise'' ratio in value functions for faraway goals:
when the goal is far away, the optimal action may be only slightly better than suboptimal actions,
because a transition in the wrong direction can simply be corrected at the next time step.
Thus, when the value function is learned imperfectly and has small errors, these errors can drown out the signal for distant goals,
potentially leading to an erroneous policy.
This issue is further exacerbated with the offline RL setting, as erroneous predictions from the value function are not corrected when those actions are taken and their consequences observed.

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
% Figure environment removed


To address this challenge,
we separate policy extraction into two levels.
We first train a goal-conditioned value function from offline data
with implicit Q-learning (IQL)~\citep{iql_kostrikov2021}
and then we extract two-level policies from it.
Our high-level policy produces intermediate waypoint states, or \emph{subgoals}, as actions.
Because predicting high-dimensional states can be challenging, we will propose a method that only requires the high-level policy to product \emph{representations} of the subgoals,
with the representations learned end-to-end from the value function.
Our low-level policy takes this subgoal representation as input and produces actions to reach the subgoal (\Cref{fig:arch}).
Here, in contrast to previous hierarchical methods~\citep{hiro_nachum2018,hac_levy2019},
we extract both policies from the \emph{same} value function.
Nonetheless, this hierarchical decomposition enables the value function to provide clearer learning signals for both policies (\Cref{fig:illust}).
For the high-level policy, the value difference between various subgoals is much larger than that between different low-level actions.
For the low-level policy,
the value difference between actions becomes relatively larger because the low-level policy only needs to reach nearby subgoals.
Moreover, the value function and high-level policy do not require action labels, so this hierarchical scheme provides a way to
leverage a potentially large amount of passive, action-free data. Training the low-level policy does require some data labeled with actions.
%



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%

To summarize,
our main contribution in this paper is 
to propose \textbf{Hierarchical Implicit Q-Learning} (\textbf{\methodname}), a simple hierarchical method for offline goal-conditioned RL.
%
%
%
%
\methodname extracts all the necessary components---%
a representation function, a high-level policy, and a low-level policy---%
from a single goal-conditioned value function.
%
%
%
%
%
%
Through our experiments on six types of state-based and pixel-based offline goal-conditioned RL benchmarks, we demonstrate that
\methodname significantly outperforms previous offline goal-conditioned RL methods, especially in complex, long-horizon tasks,
scales to high-dimensional observations,
and is capable of incorporating action-free data.
%

%

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\cutsectionup
\section{Related work}
\cutsectiondown
%

%

%

Our method draws on concepts from
%
offline RL~\citep{batch_lange2012,offline_levine2020},
goal-conditioned RL~\citep{gcrl_kaelbling1993,uvfa_schaul2015,her_andrychowicz2017},
hierarchical RL~\citep{option_sutton1999,option_stolle2002,oc_bacon2017,eigen_machado2017,ho2_wulfmeier2021,mo2_salter2022},
and action-free RL~\citep{bco_torabi2018,rlv_schmeckpeper2020,laq_chang2022,vpt_baker2022,ssorl_zheng2023,icvf_ghosh2023},
providing a way to effectively train general-purpose goal-conditioned policies from previously collected offline data.
%
%
Prior work on goal-conditioned RL has introduced algorithms based on a variety of techniques,
such as hindsight relabeling
\citep{her_andrychowicz2017,tdm_pong2018,dher_fang2019,hac_levy2019,gh_li2020,am_chebotar2021,wgcsl_yang2022},
contrastive learning
\citep{cl_eysenbach2021,cp_zhang2022,contrastive_eysenbach2022},
and state-occupancy matching \citep{gofar_ma2022,aim_durugkar2021}.
%

However, directly solving goal-reaching tasks is often challenging in complex, long-horizon environments
%
\citep{hiro_nachum2018,hac_levy2019,ril_gupta2019}.
To address this issue, several goal-conditioned RL methods have been proposed based on
hierarchical RL
\citep{subgoal_schmidhuber1991,feudal_dayan1992,hdqn_kulkarni2016,fun_vezhnevets2017,hiro_nachum2018,nearoptimal_nachum2019,hac_levy2019,hrac_zhang2020,ris_chanesane2021}
or graph-based subgoal planning
\citep{sptm_savinov2018,sorb_eysenbach2019,mss_huang2019,leap_nasiriany2019,l3p_zhang2021,sfl_hoang2021,higl_kim2021,pig_kim2023}.
%
%
Like these prior methods, our algorithm will use higher-level subgoals in a hierarchical policy structure, but we will focus on solving goal-reaching tasks from \emph{offline} data.
We use an offline RL algorithm \citep{iql_kostrikov2021}
%
to train a goal-conditioned value function from the dataset,
which allows us to simply \emph{extract} the hierarchical policies in a decoupled manner
with no need for potentially complex graph-based planning procedures.
Another important difference from prior work is that
%
we only train a \emph{single} goal-conditioned value function,
unlike previous hierarchical methods that train multiple hierarchical value functions~\citep{hiro_nachum2018,hac_levy2019}.
Perhaps surprisingly, we show that this can still significantly improve the performance of the hierarchical policies, due to an improved ``signal-to-noise'' ratio (\Cref{sec:hier}). %
%
%
%
%
%
%
%
%
%

Our method is most closely related to previous works on hierarchical offline skill extraction and hierarchical offline (goal-conditioned) RL.
Offline skill extraction methods \citep{ddco_krishnan2017,spirl_pertsch2020,opal_ajay2021,skimo_shi2022,tap_jiang2022,tacorl_rosetebeas2022}
encode trajectory segments into a latent skill space, and learn to combine these skills to solve downstream tasks.
The primary challenge in this setting is deciding
how trajectories should be decomposed hierarchically,
which can be sidestepped in our goal-conditioned setting since subgoals provide a natural decomposition.
Among goal-conditioned approaches, hierarchical imitation learning \citep{play_lynch2019,ril_gupta2019}
jointly learns subgoals and low-level controllers from optimal demonstrations.
These methods have two drawbacks: they predict subgoals in the raw observation space, and they require expert trajectories;
our observation is that a value function can alleviate both challenges,
as it provides a way to use suboptimal data and stitch across trajectories,
as well as providing a latent goal representation in which subgoals may be predicted.
Another class of methods plans through a graph or model to generate subgoals \citep{recon_shah2021,ptp_fang2022,flap_fang2022,higoc_li2022};
%
our method simply extracts all levels of the hierarchy from a single unified value function,
avoiding the high computational overhead of planning.
Finally, our method is closely related to POR \citep{por_xu2022},
which predicts the immediate next state as a subgoal;
this can be seen as one extreme of our method without representations,
%
although we show that more long-horizon subgoal prediction can be advantageous both in theory and practice.


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\cutsectionup
\section{Preliminaries}
\cutsectiondown
\label{sec:prelim}
%
%

\textbf{Problem setting.}
%
We consider the problem of offline goal-conditioned RL,
defined by a Markov decision process $\gM = (\gS, \gA, \mu, p, r)$ and a dataset $\gD$,
where $\gS$ denotes the state space, $\gA$ denotes the action space,
$\mu \in \gP(\gS)$ denotes an initial state distribution,
$p \in \gS \times \gA \to \gP(\gS)$ denotes a transition dynamics distribution,
and $r(s, g)$ denotes a goal-conditioned reward function.
The dataset $\gD$ consists of trajectories $\tau = (s_0, a_0, s_1, a_1, \dots, s_T)$.
%
In some experiments,
we assume that we have an additional action-free dataset $\gD_\gS$ that consists of state-only trajectories
$\tau_s = (s_0, s_1, \dots, s_T)$.
Unlike some prior work~\citep{her_andrychowicz2017,hiro_nachum2018,mss_huang2019,l3p_zhang2021,pig_kim2023},
we assume that the goal space $\gG$ is the same as the state space (\ie, $\gG = \gS$).
%
%
Our goal is to learn from $\gD \cup \gD_\gS$ an optimal goal-conditioned policy $\pi(a|s, g)$ that maximizes
$J(\pi) = \E_{g \sim p(g), \tau \sim p^\pi(\tau)}[\sum_{t=0}^T \gamma^t r(s_t, g) ]$
with $p^\pi(\tau) = \mu(s_0)\prod_{t=0}^{T-1} \pi(a_t \mid s_t, g) p(s_{t+1} \mid s_t, a_t)$,
where $\gamma$ is a discount factor
and $p(g)$ is a goal distribution.
%
%
%
%
%
%
%

\textbf{Implicit Q-learning (IQL).}
One of the main challenges with offline RL is that a policy can exploit overestimated values for out-of-distribution actions \citep{offline_levine2020},
%
as we cannot correct erroneous policies and values via environment interactions, unlike in online RL.
To tackle this issue, \citet{iql_kostrikov2021} proposed implicit Q-learning (IQL),
which avoids querying out-of-sample actions by converting the $\max$ operator in the Bellman optimal equation into expectile regression.
%
%
Specifically, IQL trains an action-value function $Q_{\theta_Q} (s, a)$
and a state-value function $V_{\theta_V} (s)$ with the following loss:
\begin{align}
\gL_V(\theta_V) &= \E_{(s, a) \sim \gD}[L_2^\tau (Q_{\bar{\theta}_Q}(s, a) - V_{\theta_V}(s))], \label{eq:iql_v} \\
\gL_Q(\theta_Q) &= \E_{(s, a, s') \sim \gD}[(r_{\text{task}}(s, a) + \gamma V_{\theta_V}(s') - Q_{\theta_Q}(s, a))^2], \label{eq:iql_q}
\end{align}
where $r_{\text{task}}(s, a)$ denotes the task reward function,
$\bar{\theta}_Q$ denotes the parameters of the target Q network \citep{dqn_mnih2013},
and $L_2^\tau$ is the expectile loss with a parameter $\tau \in [0.5, 1)$: $L_2^\tau(x) = |\tau - \mathds{1} (x < 0)|x^2$.
Intuitively, expectile regression can be interpreted as an asymmetric square loss that penalizes positive values more than negative ones.
As a result,
when $\tau$ tends to $1$,
$V_{\theta_V}(s)$ gets closer to $\max_a Q_{\bar{\theta}_Q}(s, a)$ (\Cref{eq:iql_v}).
Thus, we can use the value function to estimate the TD target ($r_{\text{task}}(s, a) + \gamma \max_{a'} Q_{\bar{\theta}_Q}(s', a')$) as $(r_{\text{task}}(s, a) + \gamma V_{\theta_V}(s'))$ without having to sample actions $a'$.

After training the value function with \Cref{eq:iql_v,eq:iql_q},
IQL extracts the policy with advantage-weighted regression (AWR) %
\citep{rwr_peters2007,lawer_neumann2008,reps_peters2010,awr_peng2019,awac_nair2020,crr_wang2020}:
\begin{align}
J_{\pi}(\theta_\pi) &= \E_{(s, a, s') \sim \gD}[\exp(\beta \cdot (Q_{\bar{\theta}_Q}(s, a) - V_{\theta_V}(s))) \log \pi_{\theta_\pi}(a \mid s)], \label{eq:awr}
\end{align}
where $\beta \in \sR_0^+$ denotes an inverse temperature parameter.
%
%
Intuitively, \Cref{eq:awr} encourages the policy to select actions
that lead to large $Q$ values while not deviating far from the data collection policy \citep{awr_peng2019}.


%

%
%
%

%
\textbf{Action-free goal-conditioned IQL.}
The original IQL method described above requires both reward and action labels
in the offline data to train the value functions via \Cref{eq:iql_v,eq:iql_q}. 
However, in real-world scenarios,
offline data might not contain task information or action labels, as in the case of task-agnostic demonstrations or videos.
%
%
As such, we focus on the setting of offline goal-conditioned RL,
%
which does not require task rewards, and provides us with a way to incorporate state-only trajectories into value learning.
%
%
%
%
We can use the following action-free variant \citep{por_xu2022,icvf_ghosh2023} of IQL
to learn an offline goal-conditioned value function $V_{\theta_V}(s, g)$:
\begin{align}
\gL_V(\theta_V) &= \E_{(s, s') \sim \gD_\gS, g \sim p(g \mid \tau)}[L_2^\tau (r(s, g) + \gamma V_{\bar{\theta}_V}(s', g) - V_{\theta_V}(s, g))]. \label{eq:goal_iql}
\end{align}
Unlike \Cref{eq:iql_q,eq:iql_v}, this objective does not require actions when fitting the value function,
as it directly takes backups from the values of the next states.
%
%
%
%
%
%
%
%

Action-labeled data is only needed when extracting the policy.
With the goal-conditioned value function learned by \Cref{eq:goal_iql}, we can extract the policy with the following variant of AWR:
%
\begin{align}
%
J_{\pi}(\theta_\pi) &= \E_{(s, a, s') \sim \gD, g \sim p(g \mid \tau)}[\exp(\beta \cdot A(s, a, g)) \log \pi_{\theta_\pi}(a \mid s, g)], \label{eq:goal_awr}
\end{align}
where we approximate $A(s, a, g)$ as $\gamma V_{\theta_V}(s', g) + r(s, g) - V_{\theta_V}(s, g)$.
Intuitively, \Cref{eq:goal_awr} encourages the policy to select the actions that lead to the states having high values. %
With this action-free variant of IQL,
%
%
we can train an optimal goal-conditioned value function only using action-free data
and extract the policy from action-labeled data that may be different from the passive dataset.

We note that this action-free variant of IQL is unbiased when the environment dynamics are deterministic \citep{icvf_ghosh2023},
but it may overestimate values in stochastic environments. %
%
This deterministic environment assumption is inevitable for learning an unbiased value function solely from state trajectories.
The reason is subtle but important: in stochastic environments, it is impossible to tell whether a good outcome was caused by taking a good action or because of noise in the environment.
%
%
As a result, applying action-free IQL to stochastic environments will typically result in overestimating the value function, implicitly assuming that all noise is controllable.
%
While we will build our method upon \Cref{eq:goal_iql} in this work for simplicity,
in line with many prior works on offline RL that employ similar assumptions
\citep{gcsl_ghosh2019,dt_chen2021,tt_janner2021,diffuser_janner2022,por_xu2022,quasi_wang2023,icvf_ghosh2023},
we believe correctly handling stochastic environments with advanced techniques
(\eg, by identifying controllable parts of the environment \citep{doc_yang2022,optimism_villaflor2022})
is an interesting direction for future work.

%


%

%
\cutsectionup
\section{Hierarchical policy structure for offline goal-conditioned RL}
\cutsectiondown
\label{sec:hier}
%

%
%
%
%
%
Goal-conditioned offline RL provides a general framework for learning flexible policies from data, but the goal-conditioned setting also presents an especially difficult multi-task learning problem for RL algorithms, particularly for long-horizon tasks where the goal is far away.
In \Cref{sec:why}, we discuss some possible reasons for this difficulty, from the perspective of the ``signal-to-noise'' ratio in the learned goal-conditioned value function.
We then propose hierarchical policy extraction as a solution (\Cref{sec:hier_sol})
and compare the performances of hierarchical and flat policies in a didactic environment, based on our theoretical analysis (\Cref{sec:didactic}).
%

% Figure environment removed


%
\cutsubsectionup
\subsection{Motivation: why non-hierarchical policies might struggle}
\cutsubsectiondown
\label{sec:why}

%
%
One common strategy in offline RL is to first fit a value function and then extract a policy that takes actions leading to high values
\citep{bcq_fujimoto2019,awr_peng2019,brac_wu2019,cql_kumar2020,awac_nair2020,emaq_ghasemipour2021,onestep_brandfonbrener2021,edac_an2021,iql_kostrikov2021,wgcsl_yang2022,por_xu2022,xql_garg2023,sql_xu2023}.
This strategy can be directly applied to offline goal-conditioned RL
by learning a goal-conditioned policy $\pi(a \mid s_t, g)$ that aims to maximize the learned goal-conditioned value function $V(s_{t+1}, g)$,
as in \Cref{eq:goal_awr}.
%
%
%
%
However, when the goal $g$ is far from the current state $s$,
the learned goal-conditioned value function may not provide a clear learning signal for a flat, non-hierarchical policy.
%
There are two reasons for this.
First, the differences between the values of different next states ($V(s_{t+1}, g)$) may be small,
%
as bad outcomes by taking suboptimal actions may be simply corrected in the next few steps,
%
causing only relatively minor costs.
%
Second, these small differences can be overshadowed by the noise present in the learned value function (due to, for example, sampling error or approximation error),
especially when the goal is distant from the current state, in which case the magnitude of the goal-conditioned value (and thus the magnitude of its noise or errors) is large.
%
%
%
%
In other words, the ``signal-to-noise'' ratio in the next time step values $V(s_{t+1}, g)$ can be small,
not providing sufficiently clear learning signals for the flat policy.
\Cref{fig:snr} illustrates this problem.
\Cref{fig:snr}a shows the ground-truth optimal value function $V^*(s, g)$ for a given goal at each state,
which can guide the agent to reach the goal.
However, when noise is present in the learned value function $\hat{V}(s, g)$ (\Cref{fig:snr}b),
the flat policy $\pi(a \mid s, g)$ becomes erroneous, especially at states far from the goal (\Cref{fig:snr}c).
%

%
\cutsubsectionup
\subsection{Our hierarchical policy structure}
\cutsubsectiondown
\label{sec:hier_sol}

To address this issue,
our main idea in this work, which we present fully in Section~\ref{sec:full_method}, is to separate policy extraction into two levels.
Instead of directly learning a single, flat, goal-conditioned policy $\pi(a \mid s_t, g)$ that aims to maximize $V(s_{t+1}, g)$,
%
%
%
%
%
%
%
we extract both a high-level policy $\pi^h(s_{t+k} \mid s_t, g)$
and a low-level policy $\pi^\ell(a \mid s_t, s_{t+k})$,
%
which aims to maximize
$V(s_{t+k}, g)$ and $V(s_{t+1}, s_{t+k})$,
%
respectively.
Here, $s_{t+k}$ can be viewed as a waypoint or \emph{subgoal}.
The high-level policy outputs intermediate subgoal states that are $k$ steps away from $s$,
while the low-level policy produces primitive actions to reach these subgoals.
%
Although we extract both policies from the \emph{same} learned value function in this way,
%
%
%
this hierarchical scheme provides clearer learning signals for both policies.
%
%
%
%
%
%
%
Intuitively, 
the high-level policy receives a more reliable learning signal
because different subgoals lead to more dissimilar values than primitive actions.
%
The low-level policy also gets a clear signal (from the same value function)
since it queries the value function with only nearby states,
for which the value function is relatively more accurate (\Cref{fig:illust}).
As a result, the overall hierarchical policy can be more robust to noise and errors in the value function (\Cref{fig:snr}d).
%
%
%

%

%
\cutsubsectionup
\subsection{Didactic example: hierarchical policies mitigate the signal-to-noise ratio challenge}
\label{sec:didactic}
\cutsubsectiondown

\begin{wrapfigure}{r}{0.3\textwidth}
    \centering
    \raisebox{0pt}[\dimexpr\height-1.0\baselineskip\relax]{
        \begin{subfigure}[t]{1.0\linewidth}
        % Figure removed
        \end{subfigure}
    }
    \caption{
    \footnotesize
    \textbf{1-D toy environment.}
    }
    \vspace{-15pt}
    \label{fig:toy_env}
\end{wrapfigure}
%
%
%
%
To further understand the benefits of hierarchical policies,
%
we study a toy example with one-dimensional state space (\Cref{fig:toy_env}).
In this environment, the agent can move one unit to the left or right at each time step.
The agent gets a reward of $0$ when it reaches the goal; otherwise, it always gets $-1$.
The optimal goal-conditioned value function is hence given as $V^*(s, g) = -|s - g|$ (assuming $\gamma = 1$).
%
%
We assume that the noise in the learned value function $\hat V(s, g)$ is proportional to the optimal value:
\ie, $\hat V(s, g) = V^*(s, g) + \sigma z_{s,g} V^*(s, g)$,
where $z_{s,g}$ is sampled independently from the standard normal distribution and $\sigma$ is its standard deviation.
This indicates that as the goal becomes more distant, the noise generally increases,
a trend we observed in our experiments (see \Cref{fig:proc_analysis}).
%
%

%
In this scenario, we compare the probabilities of choosing incorrect actions under the flat and hierarchical policies.
We assume that the distance between $s$ and $g$ is $T$ (\ie, $g = s + T$ and $T > 1$).
Both the flat policy and the low-level policy of the hierarchical approach consider the goal-conditioned values at $s \pm 1$.
The high-level policy evaluates the values at $s \pm k$, using $k$-step away subgoals.
%
For the hierarchical approach, we query both the high- and low-level policies at every step.
%
Given these settings, we can bound the error probabilities of both approaches as follows:
%
\begin{proposition}
\label{prop:toy}
%
In the environment described in \Cref{fig:toy_env},
the probability of the flat policy $\pi$ selecting an incorrect action is given as
$\gE(\pi) = \Phi\left(-\frac{\sqrt 2}{\sigma \sqrt{T^2 + 1}}\right)$
%
%
%
%
%
%
and the probability of the hierarchical policy $\pi^\ell \circ \pi^h$ selecting an incorrect action is bounded as
$\gE(\pi^\ell \circ \pi^h) \leq \Phi\left(-\frac{\sqrt 2}{\sigma \sqrt{(T/k)^2 + 1}}\right) + \Phi\left(-\frac{\sqrt 2}{\sigma \sqrt{k^2 + 1}}\right)$,
%
%
%
%
%
%
where $\Phi$ denotes the cumulative distribution function of the standard normal distribution, $\Phi(x) = \P[z \leq x] = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{x} e^{-t^2 / 2} \de t$.
\end{proposition}

The proof can be found in \Cref{sec:proof_toy}.
We first note that each of the error terms in the hierarchical policy bound %
is always no larger than the error in the flat policy, %
implying that both the high- and low-level policies are more accurate than the flat policy.
To compare the total errors, $\gE(\pi)$ and $\gE(\pi^\ell \circ \pi^h)$,
we perform a numerical analysis.
%
%
%
\Cref{fig:toy} shows the hierarchical policy's error bound for varying subgoal steps in five different $(T, \sigma)$ settings.
The results indicate that the flat policy's error can be significantly reduced
by employing a hierarchical policy with an appropriate choice of $k$,
%
suggesting that splitting policy extraction into two levels can be beneficial.
%
%
%

% Figure environment removed


%
%
%
%
%



\cutsectionup
\section{Hierarchical Implicit Q-Learning (\methodname)}
\cutsectiondown
\label{sec:full_method}
%


%
%
%
%
%
Based on the hierarchical policy structure in \Cref{sec:hier},
we now present a practical algorithm, which we call \textbf{Hierarchical Implicit Q-Learning} (\textbf{\methodname}),
%
to extract hierarchical policies that are robust to the noise present in the learned goal-conditioned value function.
We first explain how to train a subgoal policy (\Cref{sec:hier_awr})
and then extend this policy to predict representations (learned via the value function), which will enable \methodname to scale to image-based environments (\Cref{sec:repr}).
%

\cutsubsectionup
\subsection{Hierarchical policy extraction}
\cutsubsectiondown
\label{sec:hier_awr}

%
As motivated in \Cref{sec:hier_sol},
we split policy learning into two levels,
%
with a high-level policy generating intermediate subgoals
and a low-level policy producing primitive actions to reach the subgoals.
In this way, the learned goal-conditioned value function can provide clearer signals for both policies,
effectively reducing the total policy error.
%
%
Our method, \methodname, extracts the hierarchical policies from the \emph{same} value function learned by action-free IQL (\Cref{eq:goal_iql})
using AWR-style objectives.
While we choose to use action-free IQL in this work,
we note that our hierarchical policy extraction scheme is orthogonal
to the choice of the underlying offline RL algorithm used to train a goal-conditioned value function.
%

%
\methodname trains
%
both a high-level policy $\pi^h_{\theta_h}(s_{t+k} \mid s_t, g)$, which produces optimal $k$-step subgoals $s_{t+k}$,
and a low-level policy $\pi^\ell_{\theta_\ell}(a \mid s_t, s_{t+k})$,
which outputs primitive actions,
%
with the following objectives:
%
%
%
%
%
%
%
\begin{align}
J_{\pi^h}(\theta_h) &= \E_{(s_t, s_{t+k}, g)}[\exp(\beta \cdot \tilde A^h(s_t, s_{t+k}, g)) \log \pi^h_{\theta_h}(s_{t+k} \mid s_t, g)], \label{eq:high_awr} \\
J_{\pi^\ell}(\theta_\ell) &= \E_{(s_t, a_t, s_{t+1}, s_{t+k})}[\exp(\beta \cdot \tilde A^\ell(s_t, a_t, s_{t+k}))
\log \pi^\ell_{\theta_\ell}(a_t \mid s_t, s_{t+k})], \label{eq:low_awr}
%
\end{align}
%
where $\beta$ denotes the inverse temperature hyperparameter
and we approximate $\tilde A^h(s_t, s_{t+k}, g)$ as $V_{\theta_V}(s_{t+k}, g) - V_{\theta_V}(s_t, g)$
and $\tilde A^\ell(s_t, a_t, s_{t+k})$ as $V_{\theta_V}(s_{t+1}, s_{t+k}) - V_{\theta_V}(s_t, s_{t+k})$.
We do not include rewards and discount factors in these advantage estimates for simplicity,
%
as they are (mostly) constants or can be subsumed into the temperature $\beta$ (see \Cref{sec:train_detail} for further discussion).
%
%
%
Similarly to vanilla AWR (\Cref{eq:goal_awr}),
our high-level objective (\Cref{eq:high_awr}) performs a weighted regression over subgoals to reach the goal,
and the low-level objective (\Cref{eq:low_awr}) carries out a weighted regression over primitive actions to reach the subgoals.
%
%
%

\begin{minipage}{1\linewidth}
\vspace{-20pt}
\begin{algorithm}[H]
    \algsetup{linenosize=\small}
    \small
    \caption{Hierarchical Implicit Q-Learning (\methodname)}
    \begin{algorithmic}[1]
        \STATE \textbf{Input}: offline dataset $\gD$, action-free dataset $\gD_\gS$ (optional, $\gD_\gS = \gD$ otherwise)
        \STATE Initialize value function $V_{\theta_V}(s, \phi(g))$ with built-in representation $\phi(g)$,
        high-level policy $\pi_{\theta_h}^h(z_{t+k} \mid s_t, g)$, low-level policy $\pi_{\theta_\ell}^\ell(a \mid s_t, z_{t+k})$,
        learning rates $\lambda_V,\lambda_h,\lambda_\ell$
        \WHILE{not converged}
            \STATE $\theta_V \gets \theta_V - \lambda_V \nabla_{\theta_V} \gL_V(\theta_V)$ with $(s_t, s_{t+1}, g) \sim \gD_\gS$ {\color{gray} \; \# Train value function, \Cref{eq:goal_iql}}
        \ENDWHILE
        \WHILE{not converged}
            \STATE $\theta_h \gets \theta_h + \lambda_h \nabla_{\theta_h} J_{\pi^h}(\theta_h)$ with $(s_t, s_{t+k}, g) \sim \gD_\gS$ {\color{gray} \; \# Extract high-level policy, \Cref{eq:high_awr}}
        \ENDWHILE
        \WHILE{not converged}
            \STATE \mbox{ $\theta_\ell \gets \theta_\ell + \lambda_\ell \nabla_{\theta_\ell} J_{\pi^\ell}(\theta_\ell)$ with $(s_t, a_t, s_{t+1}, s_{t+k}) \sim \gD$ {\color{gray} \; \# Extract low-level policy, \Cref{eq:low_awr}}}
        \ENDWHILE
    \end{algorithmic}
    \label{alg:algo}
\end{algorithm}
\end{minipage}

We note that \Cref{eq:high_awr} and \Cref{eq:low_awr} are completely separated from one another,
and only the low-level objective requires action labels.
%
%
As a result, we can leverage action-free data for both the value function and high-level policy of \methodname,
by further training them with a potentially large amount of additional passive data.
%
%
Moreover, the low-level policy is relatively easy to learn compared to the other components,
as it only needs to reach local subgoals without the need for learning the complete global structure.
This enables \methodname to work well even with a limited amount of action information,
as we will demonstrate in \Cref{sec:exp_passive}.
%
%

%
%

%

%
%
%
%
%
%
%
%




%
%
%
%
%
%
%
%
%
%
%

%
\cutsubsectionup
\subsection{Representations for subgoals}
\cutsubsectiondown
\label{sec:repr}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

In high-dimensional domains, such as pixel-based environments,
directly predicting subgoals can be prohibitive or infeasible for the high-level policy.
To resolve this issue,
we incorporate representation learning into \methodname,
letting the high-level policy produce more compact \emph{representations} of subgoals.
While one can employ existing action-free representation learning methods \citep{apv_seo2022,r3m_nair2022,vip_ma2023,icvf_ghosh2023}
to learn state representations,
%
%
\methodname simply uses
an intermediate layer of the value function as a goal representation,
which can be proven to be sufficient for control.
%
%
Specifically, we parameterize the goal-conditioned value function $V(s, g)$ with $V(s, \phi(g))$,
and use $\phi(g)$ as the representation of the goal.
%
Using this representation,
the high-level policy $\pi^h(z_{t+k} \mid s_t, g)$ produces $z_{t+k} = \phi(s_{t+k})$ instead of $s_{t+k}$,
%
which the low-level policy $\pi^\ell(a \mid s_t, z_{t+k})$ takes as input to output actions (\Cref{fig:arch}).
%
%
%
In this way, we can simply learn compact goal representations
that are sufficient for control with no separate training objectives or components.
%
Formally, we prove that the representations from the value function are sufficient for action selection:
\begin{proposition}[Goal representations from the value function are sufficient for action selection]
\label{prop:repr}
Let $V^*(s, g)$ be the value function for the optimal reward-maximizing policy $\pi^*(a \mid s, g)$ in a deterministic MDP. Let a representation function $\phi(g)$ be given. If this same value function can be represented in terms of goal representations $\phi(g)$, then the reward-maximizing policy can also be represented in terms of goal representations $\phi(g)$:
\begin{align*}
    & \exists \; V_\phi(s, \phi(g)) \text{ s.t. } V_\phi(s, \phi(g)) = V^*(s, g) \text{ for all }s, g \implies \\
    %
    & \qquad \qquad \exists \; \pi_\phi(a \mid s, \phi(g)) \text{ s.t. } \pi_\phi(a \mid s, \phi(g)) = \pi^*(a \mid s, g) \text{ for all }s, g.
\end{align*}
\end{proposition}

While \Cref{prop:repr} shows that the parameterized value function $V(s, \phi(g))$ provides a sufficient goal representation $\phi$,
we found that additionally concatenating $s$ to the input to $\phi$ (\ie, using $\phi([g, s])$ instead of $\phi(g)$) \citep{bvn_hong2022}
leads to better empirical performance (see \Cref{sec:train_detail} for details),
and thus we use the concatenated variant of value function parameterization in our experiments.
We provide a pseudocode for \methodname in \Cref{alg:algo} and the full training details in \Cref{sec:train_detail,sec:impl_detail}.
%
%
%
%
%


%
%
%
%
%
%



\cutsectionup
\cutsectionup
\section{Experiments}
\cutsectiondown

%
%
Our experiments will use six offline goal-conditioned tasks,
aiming to answer the following questions:
%
\begin{enumerate}[itemsep=0pt, topsep=0pt, leftmargin=15pt]
    \item How well does \methodname perform on a variety of goal-conditioned tasks, compared to prior methods?
    %
    \item Can \methodname solve image-based tasks, and are goal representations important for good performance?
    %
    %
    \item Can \methodname utilize action-free data to accelerate learning?
    %
    \item Does \methodname mitigate policy errors caused by noisy and imperfect value functions in practice?
\end{enumerate}

\cutsubsectionup
\subsection{Experimental setup}
\cutsubsectiondown

%
%
%
%
%
%
%
%
%
%
%
%
%
%
    
%
%
%
%
%
%
%
%
%
%
%
%
%


We first describe our evaluation environments, shown in \Cref{fig:state_envs} (state-based) and \Cref{fig:pixel_envs} (pixel-based).
\textbf{AntMaze} \citep{mujoco_todorov2012,openaigym_brockman2016} is a class of challenging long-horizon navigation tasks,
where the goal is to control an $8$-DoF Ant robot to reach a given goal location from the initial position.
%
%
We use the four medium and large maze datasets from the original D4RL benchmark \citep{d4rl_fu2020}. While the large mazes already present a significant challenge for long-horizon reasoning, we also include two even larger mazes (AntMaze-Ultra) proposed by \citet{tap_jiang2022}.
\textbf{Kitchen} \citep{ril_gupta2019} is a long-horizon manipulation domain,
in which the goal is to complete four subtasks (\eg, open the microwave or move the kettle) with a $9$-DoF Franka robot.
We employ two datasets consisting of diverse behaviors (`-partial' and `-mixed') from the D4RL benchmark \citep{d4rl_fu2020}.
\textbf{CALVIN} \citep{calvin_mees2022}, another long-horizon manipulation environment, also features four target subtasks similar to Kitchen.
However, the dataset accompanying CALVIN \citep{skimo_shi2022} consists of a much larger number of task-agnostic trajectories
from $34$ different subtasks,
%
which makes it challenging for the agent to learn relevant behaviors for the goal.
\textbf{Procgen Maze} \citep{procgen_cobbe2019} is a pixel-based maze navigation environment.
%
We train agents on an offline dataset consisting of $500$ or $1000$ different maze levels with a variety of sizes, colors, and difficulties,
and test them on both the same and different sets of levels to evaluate their generalization capabilities.
\textbf{Visual AntMaze} is a vision-based variant of the AntMaze-Large environment \citep{d4rl_fu2020}.
We provide only a $64 \times 64 \times 3$ camera image (as shown in the bottom row of \Cref{fig:pixel_envs}b)
and the agent's proprioceptive states, excluding the global coordinates.
As such, the agent must learn to navigate the maze based on the wall structure and floor color from the image.
\textbf{Roboverse} \citep{ptp_fang2022,stablecrl_zheng2023} is a pixel-based, goal-conditioned robotic manipulation environment.
The dataset consists of $48 \times 48 \times 3$ images of diverse sequential manipulation behaviors,
starting from randomized initial object poses.
We evaluate the agent's performance across five unseen goal-reaching tasks that require multi-stage reasoning and generalization. %
%
To train goal-conditioned policies in these benchmark environments,
during training, we replace the original rewards with a sparse goal-conditioned reward function,
$r(s, g) = 0 \ (\text{if} \ s = g), \ -1 \ (\text{otherwise})$.
%

% Figure environment removed

We compare the performance of \methodname
with six previous behavioral cloning and offline RL methods.
For behavioral cloning methods, we consider flat goal-conditioned behavioral cloning (GCBC) \citep{goalgail_ding2019,gcsl_ghosh2019}
and hierarchical goal-conditioned behavioral cloning (HGCBC) with two-level policies \citep{play_lynch2019,ril_gupta2019}.
For offline goal-conditioned RL methods, we evaluate a goal-conditioned variant of IQL \citep{iql_kostrikov2021} (``GC-IQL'') (\Cref{sec:prelim}),
which does not use hierarchy,
and POR \citep{por_xu2022} (``GC-POR''), which uses hierarchy but does not use temporal abstraction (\ie, similar to $k=1$ in \methodname) nor representation learning. In AntMaze, we additionally compare \methodname with two model-based approaches that studied this domain in prior work:
Trajectory Transformer (TT) \citep{tt_janner2021}, which models entire trajectories with a Transformer \citep{transformer_vaswani2017},
and TAP \citep{tap_jiang2022}, which encodes trajectory segments with VQ-VAE \citep{vqvae_oord2017}
and performs model-based planning over latent vectors in a hierarchical manner.
We use the performance reported by \citet{tap_jiang2022} for comparisons with TT and TAP.
In our experiments, we use $8$ random seeds
and represent $95\%$ confidence intervals with shaded regions (in figures)
or standard deviations (in tables), unless otherwise stated.
We provide full details of environments and baselines in \Cref{sec:impl_detail}.
%

\cutsubsectionup
\subsection{Results on state-based environments}
\cutsubsectiondown
\label{sec:exp_state}

%
\begin{table}[t!]
    \vspace{-20pt}
    \caption{
    \footnotesize
    \textbf{Evaluating \methodname on state-based offline goal-conditioned RL.}
    \methodname mostly outperforms six baselines on a variety of benchmark tasks, including on different types of data.
    %
    We show the standard deviations across $8$ random seeds and refer to \Cref{sec:add_plots}
    %
    for the full training curves.
    Baselines: GCBC~\citep{gcsl_ghosh2019}, HGCBC~\citep{ril_gupta2019}, GC-IQL~\citep{iql_kostrikov2021}, GC-POR~\citep{por_xu2022}, TAP~\citep{tap_jiang2022}, TT~\citep{tt_janner2021}.
    }
    \label{table:state_all}
    \centering
    \scalebox{0.7}{
    \begin{tabular}{lrrrrrrrr}
    \toprule
    \textbf{Dataset} & \textbf{GCBC} & \textbf{HGCBC} & \textbf{GC-IQL} & \textbf{GC-POR} & \textbf{TAP} & \textbf{TT} & \textbf{\methodname} (\textbf{ours}) & \textbf{\methodname} (w/o repr.) \\
    \midrule
    antmaze-medium-diverse & $67.3$ {\tiny $\pm 10.1$} & $71.6$ {\tiny $\pm 8.9$} & $63.5$ {\tiny $\pm 14.6$} & $74.8$ {\tiny $\pm 11.9$} & $85.0$ & $\mathbf{100.0}$ & $86.8$ {\tiny $\pm 4.6$} & $89.9$ {\tiny $\pm 3.5$} \\
    antmaze-medium-play & $71.9$ {\tiny $\pm 16.2$} & $66.3$ {\tiny $\pm 9.2$} & $70.9$ {\tiny $\pm 11.2$} & $71.4$ {\tiny $\pm 10.9$} & $78.0$ & $\mathbf{93.3}$ & $84.1$ {\tiny $\pm 10.8$} & $87.0$ {\tiny $\pm 8.4$} \\
    antmaze-large-diverse & $20.2$ {\tiny $\pm 9.1$} & $63.9$ {\tiny $\pm 10.4$} & $50.7$ {\tiny $\pm 18.8$} & $49.0$ {\tiny $\pm 17.2$} & $82.0$ & $60.0$ & $\mathbf{88.2}$ {\tiny $\pm 5.3$} & $87.3$ {\tiny $\pm 3.7$} \\
    antmaze-large-play & $23.1$ {\tiny $\pm 15.6$} & $64.7$ {\tiny $\pm 14.5$} & $56.5$ {\tiny $\pm 14.4$} & $63.2$ {\tiny $\pm 16.1$} & $74.0$ & $66.7$ & $\mathbf{86.1}$ {\tiny $\pm 7.5$} & $81.2$ {\tiny $\pm 6.6$} \\
    antmaze-ultra-diverse & $14.4$ {\tiny $\pm 9.7$} & $39.4$ {\tiny $\pm 20.6$} & $21.6$ {\tiny $\pm 15.2$} & $29.8$ {\tiny $\pm 13.6$} & $26.0$ & $33.3$ & $\mathbf{52.9}$ {\tiny $\pm 17.4$} & $52.6$ {\tiny $\pm 8.7$} \\
    antmaze-ultra-play & $20.7$ {\tiny $\pm 9.7$} & $38.2$ {\tiny $\pm 18.1$} & $29.8$ {\tiny $\pm 12.4$} & $31.0$ {\tiny $\pm 19.4$} & $22.0$ & $20.0$ & $39.2$ {\tiny $\pm 14.8$} & $\mathbf{56.0}$ {\tiny $\pm 12.4$} \\
    \midrule
    kitchen-partial & $38.5$ {\tiny $\pm 11.8$} & $32.0$ {\tiny $\pm 16.7$} & $39.2$ {\tiny $\pm 13.5$} & $18.4$ {\tiny $\pm 14.3$} & - & - & $\mathbf{65.0}$ {\tiny $\pm 9.2$} & $46.3$ {\tiny $\pm 8.6$} \\
    kitchen-mixed & $46.7$ {\tiny $\pm 20.1$} & $46.8$ {\tiny $\pm 17.6$} & $51.3$ {\tiny $\pm 12.8$} & $27.9$ {\tiny $\pm 17.9$} & - & - & $\mathbf{67.7}$ {\tiny $\pm 6.8$} & $36.8$ {\tiny $\pm 20.1$} \\
    drule
    calvin & $17.3$ {\tiny $\pm 14.8$} & $3.1$ {\tiny $\pm 8.8$} & $7.8$ {\tiny $\pm 17.6$} & $12.4$ {\tiny $\pm 18.6$} & - & - & $\mathbf{43.8}$ {\tiny $\pm 39.5$} & $23.4$ {\tiny $\pm 27.1$} \\
    \bottomrule
    \end{tabular}
    }
\end{table}
\begin{table}
    \centering
    \vspace{-7pt}
    \caption{
    \footnotesize
    \textbf{Evaluating \methodname on pixel-based offline goal-conditioned RL.}
    \methodname scales to high-dimensional pixel-based environments with latent subgoal representations,
    achieving the best performance across the environments.
    We refer to \Cref{sec:add_plots} for the full training curves.
    %
    }
    %
        \centering
        \scalebox{0.7}{
        \begin{tabular}{lrrrrr}
        \toprule
        \textbf{Dataset} & \textbf{GCBC} & \textbf{HGCBC} {\scriptsize (+ repr.)} & \textbf{GC-IQL} & \textbf{GC-POR} {\scriptsize (+ repr.)} & \textbf{\methodname} (\textbf{ours}) \\
        \midrule
        procgen-maze-500-train & $16.8$ {\tiny $\pm 2.8$} & $14.3$ {\tiny $\pm 4.1$} & $72.5$ {\tiny $\pm 10.0$} & $75.8$ {\tiny $\pm 12.1$} & $\mathbf{82.5}$ {\tiny $\pm 6.0$} \\
        procgen-maze-500-test & $14.5$ {\tiny $\pm 5.0$} & $11.2$ {\tiny $\pm 3.7$} & $49.5$ {\tiny $\pm 9.8$} & $53.8$ {\tiny $\pm 14.5$} & $\mathbf{64.5}$ {\tiny $\pm 13.2$} \\
        procgen-maze-1000-train & $27.2$ {\tiny $\pm 8.9$} & $15.0$ {\tiny $\pm 5.7$} & $78.2$ {\tiny $\pm 7.2$} & $82.0$ {\tiny $\pm 6.5$} & $\mathbf{87.0}$ {\tiny $\pm 13.9$} \\
        procgen-maze-1000-test & $12.0$ {\tiny $\pm 5.9$} & $14.5$ {\tiny $\pm 5.0$} & $60.0$ {\tiny $\pm 10.6$} & $69.8$ {\tiny $\pm 7.4$} & $\mathbf{78.2}$ {\tiny $\pm 17.9$} \\
        \midrule
        visual-antmaze-diverse & $71.4$ {\tiny $\pm 6.0$} & $35.1$ {\tiny $\pm 12.0$} & $72.6$ {\tiny $\pm 5.9$} & $47.4$ {\tiny $\pm 17.6$} & $\mathbf{80.5}$ {\tiny $\pm 9.4$} \\
        visual-antmaze-play & $64.4$ {\tiny $\pm 6.3$} & $23.8$ {\tiny $\pm 8.5$} & $70.4$ {\tiny $\pm 26.6$} & $57.0$ {\tiny $\pm 8.1$} & $\mathbf{78.4}$ {\tiny $\pm 4.6$} \\
        visual-antmaze-navigate & $33.2$ {\tiny $\pm 7.9$} & $21.4$ {\tiny $\pm 4.6$} & $22.1$ {\tiny $\pm 14.1$} & $16.1$ {\tiny $\pm 15.2$} & $\mathbf{45.7}$ {\tiny $\pm 18.1$} \\
        \midrule
        roboverse & $26.2$ {\tiny $\pm 4.5$} & $26.4$ {\tiny $\pm 6.4$} & $31.2$ {\tiny $\pm 8.7$} & $46.6$ {\tiny $\pm 7.4$} & $\mathbf{61.5}$ {\tiny $\pm 5.3$} \\
        \bottomrule
        \end{tabular}
        }
    %
    %
    %
    %
    %
    %
    \label{table:pixel_all}
    \vspace{-7pt}
\end{table}

% Figure environment removed



%
%
%
%

We first evaluate \methodname in the five state-based environments (AntMaze-\{Medium, Large, Ultra\}, Kitchen, and CALVIN)
using nine offline datasets.
%
We evaluate the performance of the learned policies by commanding them with
the evaluation goal state $g$ %
(\ie, the benchmark task target position in AntMaze, or the state that corresponds to completing all four subtasks in Kitchen and CALVIN),
%
and measuring the average return with respect to the original benchmark task reward function.
We test two versions of \methodname (without and with representations) in state-based environments.
%
\Cref{table:state_all} and \Cref{fig:rliable}a show the results on the nine offline datasets,
indicating that \methodname mostly achieves the best performance in our experiments. %
Notably, \methodname attains an $88\%$ success rate on AntMaze-Large
and $53\%$ on AntMaze-Ultra,
which is, to the best of our knowledge, better than any previously reported result on these datasets.
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
In manipulation domains, we find that having latent subgoal representations in \methodname is important for enabling good performance. %
In CALVIN, while other methods often fail to achieve any of the subtasks due to the high diversity in the data,
\methodname completes approximately two subtasks on average. %

\cutsubsectionup
\subsection{Results on pixel-based environments}
\cutsubsectiondown
\label{sec:exp_pixel}


Next, to verify whether \methodname can scale to high-dimensional environments using goal representations,
%
%
we evaluate our method on three pixel-based domains (Procgen Maze, Visual AntMaze, and Roboverse) with image observations.
%
%
%
%
%
%
%
%
For the prior hierarchical approaches that generate raw subgoals (HGCBC and GC-POR),
we apply \methodname's value-based representation learning scheme
to enable them to handle the high-dimensional observation space.
\Cref{table:pixel_all} and \Cref{fig:rliable}b present the results,
showing that our hierarchical policy extraction scheme,
combined with representation learning, improves performance in these image-based environments as well.
%
Notably, in Procgen Maze, \methodname exhibits larger gaps compared to the previous methods on the test sets.
This is likely because the high-level policy can generalize better than the flat policy,
as it can focus on the long-term direction toward the goal rather than the maze's detailed layout.
In Roboverse, \methodname is capable of generalizing to solve unseen robotic manipulation tasks purely from images,
achieving an average success rate of $62\%$.
%
%

%
\cutsectionup
\subsection{Results with action-free data}
\cutsectiondown
\label{sec:exp_passive}

%
%
%
%
%
%
%
%
%
%
As mentioned in \Cref{sec:hier_awr},
one of the advantages of \methodname is its ability to leverage
a potentially large amount of passive (action-free) data.
To empirically verify this capability, %
we train \methodname on action-limited datasets,
where we provide action labels for just $25\%$ of the trajectories
and use state-only trajectories for the remaining $75\%$.
%
\Cref{table:free} shows the results from six different tasks,
demonstrating that \methodname, even with a limited amount of action information,
can mostly maintain its original performance.
Notably, action-limited \methodname still outperforms previous offline RL methods (GC-IQL and GC-POR) trained with the full action-labeled data.
We believe this is because \methodname learns a majority of the knowledge through hierarchical subgoal prediction from state-only trajectories.
%




\cutsubsectionup
\subsection{Does \methodname mitigate policy errors caused by noisy value functions?}
\cutsubsectiondown
\label{sec:exp_analysis}

\begin{table}[t!]
    \vspace{-20pt}
    \caption{
    \footnotesize
    \textbf{\methodname can leverage passive, action-free data.}
    Since our method requires action information only for the low-level policy,
    which is relatively easier to learn,
    %
    \methodname mostly achieves comparable performance with just $25\%$ of action-labeled data,
    outperforming even baselines trained on full datasets.
    }
    \label{table:free}
    \centering
    \scalebox{0.67}{
    \begin{tabular}{lrrrrrr}
    \toprule
    \textbf{Dataset} & \textbf{GC-IQL} (full) & \textbf{GC-POR} (full) & \textbf{\methodname} (full) & \textbf{\methodname} (\textbf{action-limited}) & {\color{gray}\textbf{vs. HIQL} (full)} & {\color{gray}\textbf{vs. Prev. best} (full)} \\
    \midrule
    antmaze-large-diverse & $50.7$ {\tiny $\pm 18.8$} & $49.0$ {\tiny $\pm 17.2$} & $88.2$ {\tiny $\pm 5.3$} & $88.9$ {\tiny $\pm 6.4$} & {\color{myblue}$+0.7 $} & {\color{myblue}$+38.2 $} \\
    antmaze-ultra-diverse & $21.6$ {\tiny $\pm 15.2$} & $29.8$ {\tiny $\pm 13.6$} & $52.9$ {\tiny $\pm 17.4$} & $38.2$ {\tiny $\pm 15.4$} & {\color{myred}$-14.7 $} & {\color{myblue}$+8.4 $} \\
    kitchen-mixed & $51.3$ {\tiny $\pm 12.8$} & $27.9$ {\tiny $\pm 17.9$} & $67.7$ {\tiny $\pm 6.8$} & $59.1$ {\tiny $\pm 9.6$} & {\color{myred}$-8.6 $} & {\color{myblue}$+7.8 $} \\
    calvin & $7.8$ {\tiny $\pm 17.6$} & $12.4$ {\tiny $\pm 18.6$} & $43.8$ {\tiny $\pm 39.5$} & $35.8$ {\tiny $\pm 30.7$} & {\color{myred}$-8.0 $} & {\color{myblue}$+23.4 $} \\
    procgen-maze-500-train & $72.5$ {\tiny $\pm 10.0$} & $75.8$ {\tiny $\pm 12.1$} & $82.5$ {\tiny $\pm 6.0$} & $77.0$ {\tiny $\pm 12.5$} & {\color{myred}$-5.5 $} & {\color{myblue}$+1.2 $} \\
    procgen-maze-500-test & $49.5$ {\tiny $\pm 9.8$} & $53.8$ {\tiny $\pm 14.5$} & $64.5$ {\tiny $\pm 13.2$} & $65.5$ {\tiny $\pm 16.4$} & {\color{myblue}$+1.0 $} & {\color{myblue}$+11.7 $} \\
    \bottomrule
    \end{tabular}
    }
\end{table}


% Figure environment removed
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
%
To empirically verify whether our two-level policy architecture is more robust to errors in the learned value function (\ie, the ``signal-to-noise'' ratio argument in \Cref{sec:hier}),
we compare the policy accuracies of GC-IQL (flat policy), GC-POR (hierarchy without temporal abstraction),
and \methodname (ours) in Procgen Maze,
by evaluating the ratio at which the ground-truth actions match the learned actions.
We also measure the noisiness (\ie, standard deviation) of the learned value function with respect to the ground-truth distance between the state and the goal.
\Cref{fig:proc_analysis} shows the results.
We first observe that the noise in the value function generally becomes larger as the state-goal distance increases.
Consequently, \methodname achieves the best policy accuracy, especially for distant goals~($\mathrm{dist}(s, g) \geq 50$),
as its hierarchical policy extraction scheme provides the policies with clearer learning signals (\Cref{sec:hier_sol}).

We refer to \Cref{sec:abl} for further analyses, including
\textbf{subgoal visualizations} and
an \textbf{ablation study} on subgoal steps and design choices for representations.
%


%
%
%
%
%
%
%

\cutsectionup
\section{Conclusion}
\cutsectiondown

%
%
%
%
%
%
%
%
We proposed \methodname as a simple yet effective hierarchical algorithm for offline goal-conditioned RL.
%
While hierarchical RL methods tend to be complex, involving many different components and objectives,
\methodname shows that it is possible to build a method where a single value function simultaneously drives
the learning of the low-level policy, the high-level policy, and the representations in a relatively simple and easy-to-train framework.
We showed that \methodname not only exhibits strong performance in various challenging goal-conditioned tasks,
but also can leverage action-free data and enjoy the benefits of built-in representation learning for image-based tasks.
%
%

\textbf{Limitations.}
One limitation of \methodname is that the objective for its action-free value function (\Cref{eq:goal_iql})
is unbiased only when the environment dynamics are deterministic.
As discussed in \Cref{sec:prelim},
\methodname (and other prior methods that use action-free videos) may overestimate the value function
in partially observed or stochastic settings.
To mitigate the optimism bias of \methodname in stochastic environments,
we believe disentangling controllable parts from uncontrollable parts of the environment can be one possible solution
\citep{optimism_villaflor2022,doc_yang2022},
which we leave for future work.
Another limitation of our work is that
we assume the noise for each $V(s, g)$ is independent in our theoretical analysis (\Cref{prop:toy}).
%
While \Cref{fig:proc_analysis} shows that the ``signal-to-noise'' argument empirically holds in our experiments,
the independence assumption in our theorem might not hold in environments with continuous state spaces,
especially when the value function is modeled by a smooth function approximator.
%
%
%
%
%





\cutsectionup
\section*{Acknowledgments}
\cutsectiondown
We would like to thank Aviral Kumar for an informative discussion about the initial theoretical results,
Chongyi Zheng, Kuan Fang, and Fangchen Liu for helping set up the Roboverse environment,
and RAIL members and anonymous reviewers for their helpful comments.
This work was supported by the Korea Foundation for Advanced Studies (KFAS), the Fannie and John Hertz Foundation, the NSF GRFP (DGE2140739),
AFOSR (FA9550-22-1-0273), and ONR (N00014-21-1-2838).
This research used the Savio computational cluster resource provided by the Berkeley Research Computing program at UC Berkeley.
%


{\footnotesize
\bibliographystyle{plainnat}
\bibliography{neurips_2023}
}


\clearpage


\appendix

\section{Training details}
\label{sec:train_detail}

%
%

%
%
%
%
%
%
%
%
%
%
%
%
%


\paragraph{Goal distributions.}
We train our goal-conditioned value function, high-level policy, and low-level policy respectively with
\Cref{eq:goal_iql,eq:high_awr,eq:low_awr}, using different goal-sampling distributions.
For the value function (\Cref{eq:goal_iql}),
we sample the goals from either random states, futures states, or the current state
with probabilities of $0.3$, $0.5$, and $0.2$, respectively, following \citet{icvf_ghosh2023}.
We use $\mathrm{Geom}(1 - \gamma)$ for the future state distribution and the uniform distribution over the offline dataset for sampling random states.
For the hierarchical policies,
we mostly follow the sampling strategy of \citet{ril_gupta2019}.
We first sample a trajectory $(s_0, s_1, \dots, s_t, \dots, s_T)$ from the dataset $\gD_\gS$ and a state $s_t$ from the trajectory.
For the high-level policy (\Cref{eq:high_awr}),
we either (\emph{i}) sample $g$ uniformly from the future states $s_{t_g}$ ($t_g > t$) in the trajectory and set the target subgoal to $s_{\min(t+k, t_g)}$
%
or (\emph{ii}) sample $g$ uniformly from the dataset and set the target subgoal to $s_{\min(t+k, T)}$.
%
For the low-level policy (\Cref{eq:low_awr}),
we first sample a state $s_t$ from $\gD$, and set the input subgoal to $s_{\min(t+k, T)}$ in the same trajectory.

\paragraph{Advantage estimates.}
In principle, the advantage estimates for \Cref{eq:high_awr,eq:low_awr} are respectively given as
\begin{align}
    A^h(s_t, s_{t+\tilde{k}}, g) &= \gamma^{\tilde{k}} V_{\theta_V}(s_{t+\tilde{k}}, g) + \sum_{t'=t}^{\tilde{k}-1} r(s_{t'}, g) - V_{\theta_V}(s_t, g), \label{eq:correct_high_a} \\
    A^\ell(s_t, a_t, \tilde s_{t+k}) &= \gamma V_{\theta_V}(s_{t+1}, \tilde s_{t+k}) + r(s_t, \tilde s_{t+k}) - V_{\theta_V}(s_t, \tilde s_{t+k}), \label{eq:correct_low_a}
\end{align}
where we use the notations $\tilde{k}$ and $\tilde{s}_{t+k}$ to incorporate the edge cases discussed in the previous paragraph
(\ie, $\tilde{k} = \min(k, t_g - t)$ when we sample $g$ from future states,
$\tilde{k} = \min(k, T - t)$ when we sample $g$ from random states,
and $\tilde{s}_{t+k} = s_{\min(t+k, T)}$).
Here, we note that $s_{t'} \neq g$ and $s_t \neq \tilde{s}_{t+k}$ always hold except for those edge cases.
Thus, the reward terms in \Cref{eq:correct_high_a,eq:correct_low_a} are mostly constants
(under our reward function $r(s, g) = 0 \ (\text{if} \ s = g), \ -1 \ (\text{otherwise})$),
as are the third terms (with respect to the policy inputs).
As such, we practically ignore these terms for simplicity,
and this simplification further enables us to subsume the discount factors in the first terms into the temperature hyperparameter $\beta$.
We hence use the following simplified advantage estimates,
which we empirically found to lead to almost identical performances in our experiments:
\begin{align}
    \tilde A^h(s_t, s_{t+\tilde{k}}, g) &= V_{\theta_V}(s_{t+\tilde{k}}, g) - V_{\theta_V}(s_t, g), \\
    \tilde A^\ell(s_t, a_t, \tilde s_{t+k}) &= V_{\theta_V}(s_{t+1}, \tilde s_{t+k}) - V_{\theta_V}(s_t, \tilde s_{t+k}).
\end{align}


\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \raisebox{0pt}[\dimexpr\height-1.0\baselineskip\relax]{
        \begin{subfigure}[t]{1.0\linewidth}
        % Figure removed
        \end{subfigure}
    }
    \caption{
    \footnotesize
    \textbf{Full architecture of HIQL.}
    In practice, we use $V(s, \phi([g, s]))$ instead of $V(s, \phi(g))$
    as we found that the former leads to better empirical performance.
    }
    \vspace{-10pt}
    \label{fig:arch_detailed}
\end{wrapfigure}
\paragraph{State representations.}
We model the output of the representation function $\phi(g)$ in $V(s, \phi(g))$ with a $10$-dimensional latent vector
and normalize the outputs of $\phi(g)$ \citep{dr3_kumar2022}. %
Empirically, we found that concatenating $s$ to the input (\ie, using $\phi([g, s])$ instead of $\phi(g)$, \Cref{fig:arch_detailed}),
similarly to \citet{bvn_hong2022},
improves performance in our experiments.
While this might lose the sufficiency property of the representations (\ie, \Cref{prop:repr}),
we found that the representations obtained in this way generally lead to better performance in practice,
indicating that they still mostly preserve the goal information for control.
We believe this is due to the imposed bottleneck on $\phi$
by constraining its effective dimensionality to $9$ (by using normalized $10$-dimensional vectors),
which enforces $\phi$ to retain bits regarding $g$ and to reference $s$ only when necessary.
Additionally, in pixel-based environments,
we found that allowing gradient flows from the low-level policy loss (\Cref{eq:low_awr}) to $\phi$
further improves performance.
%
We ablate these choices and report the results in \Cref{sec:abl}.

\paragraph{Policy execution.}
At test time, we query both the high-level and low-level policies at every step, without temporal abstraction.
%
%
We found that fixing subgoal states for more than one step does not significantly affect performance,
so we do not use temporal abstraction for simplicity.

We provide a pseudocode for \methodname in \Cref{alg:algo}.
We note that the high- and low-level policies can be jointly trained with the value function as well, as in \citet{iql_kostrikov2021}.

%

\section{Additional Plots}
\label{sec:add_plots}

% Figure environment removed
% Figure environment removed
% Figure environment removed
% Figure environment removed

% Figure environment removed
% Figure environment removed

We include the training curves for \Cref{table:state_all,table:pixel_all,table:free}
in \Cref{fig:state_all,fig:pixel_all,fig:free}, respectively.
We also provide the training curves for each of the five tasks \citep{stablecrl_zheng2023} in Roboverse in \Cref{fig:pixel_rob}.
We include the Rliable \citep{rliable_agarwal2021} plots in \Cref{fig:state_all_rliable_full,fig:pixel_all_rliable_full}.
We note that the numbers in \Cref{table:state_all,table:pixel_all,table:free} are \emph{normalized} scores (see \Cref{sec:impl_detail}),
while the returns in the figures are unnormalized ones.

%

\section{Ablation Study}
\label{sec:abl}

% Figure environment removed

\paragraph{Subgoal steps.}
To understand how the subgoal steps $k$ affect performance,
we evaluate \methodname with six different $k \in \{1, 5, 15, 25, 50, 100\}$ on AntMaze, Kitchen, and CALVIN.
On AntMaze, we test both \methodname with and without representations (\Cref{sec:repr}).
\Cref{fig:abl_k} shows the results,
suggesting that \methodname generally achieves the best performance with $k$ between $25$ and $50$.
Also, \methodname still maintains reasonable performance even when $k$ is not within this optimal range, unless $k$ is too small.
%
%
%

% Figure environment removed

\paragraph{Representation parameterizations.}
We evaluate four different choices of the representation function $\phi$ in \methodname:
$\phi([g, s])$, $\phi(g - s)$, $\phi(g)$, and without $\phi$.
%
%
%
%
\Cref{fig:abl_rep_param} shows the results,
indicating that passing $g$ and $s$ together to $\phi$ generally improves performance.
We hypothesize that this is because $\phi$, when given both $g$ and $s$,
can capture contextualized information about the goals (or subgoals) with respect to the current state,
which is often easier to deal with for the low-level policy.
For example, in AntMaze, the agent only needs to know the relative position of the subgoal with respect to the current position.

% Figure environment removed

\paragraph{Auxiliary gradient flows for representations.}
We found that in pixel-based environments (\eg, Procgen Maze),
allowing gradient flows from the low-level policy loss to the representation function improves performance (\Cref{fig:abl_rep_grad}).
%
We believe this is because the additional gradients from the policy loss further help maintain the information necessary for control.
We also (informally) found that this additional gradient flow occasionally slightly improves performances in the other environments as well,
%
but we do not enable this feature in state-based environments
to keep our method as simple as possible.
%



\section{Implementation details}
\label{sec:impl_detail}

We implement \methodname based on JaxRL Minimal~\citep{jaxrlm_ghosh2023}.
Our implementation is available at the following repository: \hiqlcode.
We run our experiments on an internal GPU cluster composed of TITAN RTX and A5000 GPUs.
Each experiment on state-based environments takes no more than $8$ hours
and each experiment on pixel-based environments takes no more than $16$ hours.

\subsection{Environments}

\paragraph{AntMaze \citep{mujoco_todorov2012,openaigym_brockman2016}}
We use the `antmaze-medium-diverse-v2', `antmaze-medium-play-v2',
`antmaze-large-diverse-v2', and `antmaze-large-play-v2' datasets from the D4RL benchmark~\cite{d4rl_fu2020}.
For AntMaze-Ultra, we use the `antmaze-ultra-diverse-v0' and `antmaze-ultra-play-v0' datasets proposed by \citet{tap_jiang2022}.
The maze in the AntMaze-Ultra task is twice the size of the largest maze in the original D4RL dataset.
Each dataset consists of $999$ length-$1000$ trajectories,
in which
the Ant agent navigates from an arbitrary start location to another goal location, which does not necessarily correspond to the target evaluation goal.
At test time, to specify a goal $g$ for the policy,
we set the first two state dimensions (which correspond to the $x$-$y$ coordinates) to the target goal given by the environment
and the remaining proprioceptive state dimensions
to those of the first observation in the dataset.
At evaluation, the agent gets a reward of $1$ when it reaches the goal.

\paragraph{Kitchen \citep{ril_gupta2019}.}
We use the `kitchen-partial-v0' and `kitchen-mixed-v0' datasets from the D4RL benchmark~\cite{d4rl_fu2020}.
Each dataset consists of $136950$ transitions with varying trajectory lengths (approximately $227$ steps per trajectory on average).
In the `kitchen-partial-v0' task,
the goal is to achieve the four subtasks of opening the microwave, moving the kettle, turning on the light switch, and sliding the cabinet door.
The dataset contains a small number of successful trajectories that achieve the four subtasks.
In the `kitchen-mixed-v0' task,
the goal is to achieve the four subtasks of opening the microwave, moving the kettle, turning on the light switch, and turning on the bottom left burner.
The dataset does not contain any successful demonstrations,
only providing trajectories that achieve some subset of the four subtasks.
At test time, to specify a goal $g$ for the policy,
we set the proprioceptive state dimensions to those of the first observation in the dataset
and the other dimensions to the target kitchen configuration given by the environment.
At evaluation, the agent gets a reward of $1$ whenever it achieves a subtask.

\paragraph{CALVIN \citep{calvin_mees2022}.}
We use the offline dataset provided by \citet{skimo_shi2022},
which is based on the teleoperated demonstrations from \citet{calvin_mees2022}.
The task is to achieve the four subtasks of opening the drawer, turning on the lightbulb, sliding the door to the left, and turning on the LED.
The dataset consists of $1204$ length-$499$ trajectories.
In each trajectory, the agent achieves some of the $34$ subtasks in an arbitrary order,
which makes the dataset highly task-agnostic \citep{skimo_shi2022}.
At test time, to specify a goal $g$ for the policy,
we set the proprioceptive state dimensions to those of the first observation in the dataset
and the other dimensions to the target configuration.
At evaluation, the agent gets a reward of $1$ whenever it achieves a subtask.

\paragraph{Procgen Maze \citep{procgen_cobbe2019}.}
We collect an offline dataset of goal-reaching behavior on the Procgen Maze suite.
For each maze level, we pre-compute the optimal goal-reaching policy using an oracle,
and collect a trajectory of $1000$ transitions by commanding a goal,
using the goal-reaching policy to reach this goal,
then commanding a new goal and repeating henceforth.
The `procgen-maze-500' dataset consists of $500000$ transitions collected over the first $500$ levels
and `procgen-maze-1000' consists of $1000000$ transitions over the first $1000$ levels.
%
At test time, we evaluate the agent on ``challenging'' levels that contain at least $20$ leaf goal states
(\ie, states that have only one adjacent state in the maze).
We use $50$ such levels and goals for each evaluation,
where they are randomly sampled either between Level $0$ and Level $499$ for the ``-train'' tasks
or between Level $5000$ and Level $5499$ for the ``-test'' tasks.
The agent gets a reward of $1$ when it reaches the goal.

\paragraph{Visual AntMaze.}
We convert the original state-based AntMaze environment into a pixel-based environment
by providing both a $64 \times 64 \times 3$-dimensional camera image (as shown in the bottom row of \Cref{fig:pixel_envs}b)
and $27$-dimensional proprioceptive states without global coordinates.
For the datasets,
we use the converted versions of the `antmaze-large-diverse-v2' and `antmaze-large-play-v2' datasets from the D4RL benchmark~\citep{d4rl_fu2020}
as well as a newly collected dataset, `antmaze-large-navigate-v2',
which consists of diverse navigation behaviors that visit multiple goal locations within an episode.
The task and the evaluation scheme are the same as the original state-based AntMaze environment.

\paragraph{Roboverse \citep{ptp_fang2022,stablecrl_zheng2023}.}
We use the same dataset and tasks used in \citet{stablecrl_zheng2023}.
The dataset consists of $3750$ length-$300$ trajectories,\footnote{While \citet{stablecrl_zheng2023} separate each length-$300$ trajectory into four length-$75$ trajectories,
we found that using the original length-$300$ trajectories improves performance in general.}
out of which we use the first $3334$ trajectories for training (which correspond to approximately $1000000$ transitions),
while the remaining trajectories are used as a validation set.
Each trajectory in the dataset features four random primitive behaviors, such as pushing an object or opening a drawer,
starting from randomized initial object poses.
At test time, we employ the same five goal-reaching tasks used in \citet{stablecrl_zheng2023}.
We provide a precomputed goal image, and the agent gets a reward of $1$ upon successfully completing the task
by achieving the desired object poses.

In \Cref{table:state_all,table:pixel_all,table:free},
we report the normalized scores with a multiplier of $100$ (AntMaze, Procgen Maze, Visual AntMaze, and Roboverse) or $25$ (Kitchen and CALVIN).

\subsection{Hyperparameters}

We present the hyperparameters used in our experiments in \Cref{table:hyp},
where we mostly follow the network architectures and hyperparameters used by \citet{icvf_ghosh2023}.
We use layer normalization~\citep{ln_ba2016} for all MLP layers.
For pixel-based environments,
we use the Impala CNN architecture~\citep{impala_espeholt2018} to handle image inputs,
mostly with $512$-dimensional output features,
but we use normalized $10$-dimensional output features for the goal encoder of \methodname's value function
to make them easily predictable by the high-level policy,
as discussed in \Cref{sec:train_detail}.
We do not share encoders between states and goals, or between different components.
As a result, in pixel-based environments, we use a total of \emph{five} separate CNN encoders
(two for the value function, two for the high-level policy, and two for the low-level policy,
but the goal encoder for the value function is the same as the goal encoder for the low-level policy (\Cref{fig:arch})).
In Visual AntMaze and Roboverse, we apply a random crop \citep{drq_kostrikov2021} (with probability $0.5$)
to prevent overfitting, following \citet{stablecrl_zheng2023}.

During training,
we periodically evaluate the performance of the learned policy at every $100\mathrm{K}$ (state-based) or $50\mathrm{K}$ (pixel-based) steps,
%
using $52$ (AntMaze, Kitchen, CALVIN, and Visual AntMaze), $50$ (Procgen Maze), or $110$ (Roboverse, $22$ per each task) rollouts\footnote{These numbers include two additional rollouts for video logging (except for Procgen Maze).}.
At evaluation,
we use $\argmax$ actions for environments with continuous action spaces
and $\eps$-greedy actions with $\eps = 0.05$ for environments with discrete action spaces (\ie, Procgen Maze).
Following \citet{stablecrl_zheng2023}, in Roboverse, we add Gaussian noise with a standard deviation of $0.15$ to the $\argmax$ actions.

To ensure fair comparisons,
we use the same architecture for both \methodname and four baselines (GCBC, HGCBC, GC-IQL, and GC-POR).
The discount factor $\gamma$ is chosen from $\{0.99, 0.995\}$,
the AWR temperature $\beta$ from $\{1, 3, 10\}$,
the IQL expectile $\tau$ from $\{0.7, 0.9\}$ for each method.

For \methodname, we set $(\gamma, \beta, \tau) = (0.99, 1, 0.7)$ across all environments.
For GC-IQL and GC-POR,
we use $(\gamma, \beta, \tau) = (0.99, 3, 0.9)$ (AntMaze-Medium, AntMaze-Large, and Visual AntMaze),
$(\gamma, \beta, \tau) = (0.995, 1, 0.7)$ (AntMaze-Ultra),
or $(\gamma, \beta, \tau) = (0.99, 1, 0.7)$ (others).
For the subgoal steps $k$ in \methodname,
we use $k=50$ (AntMaze-Ultra), $k=3$ (Procgen Maze and Roboverse), or $k=25$ (others).
HGCBC uses the same subgoal steps as \methodname for each environment,
with the exception of AntMaze-Ultra,
where we find it performs slightly better with $k=25$.
For HIQL, GC-IQL, and GC-POR, in state-based environments and Roboverse,
%
%
we sample goals for high-level or flat policies
from either the future states in the same trajectory (with probability $0.7$)
or the random states in the dataset (with probability $0.3$).
We sample high-level goals only from the future states in the other environments (Procgen Maze and Visual AntMaze).


\begin{table}[t]
    \caption{Hyperparameters.}
    \label{table:hyp}
    %
    \begin{center}
    \begin{tabular}{ll}
        \toprule
        Hyperparameter & Value \\
        \midrule
        \# gradient steps & $1000000$ (AntMaze), $500000$ (others) \\
        Batch size & $1024$ (state-based), $256$ (pixel-based) \\
        Policy MLP dimensions & $(256, 256)$ \\
        Value MLP dimensions & $(512, 512, 512)$ \\
        Representation MLP dimensions (state-based) & $(512, 512, 512)$ \\
        Representation architecture (pixel-based) & Impala CNN~\citep{impala_espeholt2018} \\
        Nonlinearity & GELU~\citep{gelu_hendrycks2016} \\
        Optimizer & Adam \citep{adam_kingma2014} \\
        Learning rate & $0.0003$ \\
        Target network smoothing coefficient & $0.005$ \\
        \bottomrule
    \end{tabular}
    \end{center}
    %
\end{table}

%

\section{Proofs}

\subsection{Proof of \Cref{prop:toy}}
\label{sec:proof_toy}

For simplicity, we assume that $T/k$ is an integer and $k \leq T$.
%
%
%

\begin{proof}
Defining $z_1 := z_{1,T}$ and $z_2 := z_{-1,T}$,
the probability of the flat policy $\pi$ selecting an incorrect action can be computed as follows:
\begin{align}
\gE(\pi)
&= \P[\hat V(s+1, g) \leq \hat V(s-1, g)] \\
&= \P[\hat V(1, T) \leq \hat V(-1, T)] \\
&= \P[-(T-1)(1 + \sigma z_1) \leq -(T+1)(1 + \sigma z_2)] \\
&= \P[z_1 \sigma (T-1) - z_2 \sigma (T+1) \leq -2] \\
&= \P[z \sigma \sqrt{T^2 + 1} \leq - \sqrt 2] \\
&= \Phi\left(-\frac{\sqrt 2}{\sigma \sqrt{T^2 + 1}}\right),
\end{align}
where $z$ is a standard Gaussian random variable,
and we use the fact that the sum of two independent Gaussian random variables
with standard deviations of $\sigma_1$ and $\sigma_2$
follows a normal distribution with a standard deviation of $\sqrt{\sigma_1^2 + \sigma_2^2}$.

Similarly, the probability of the hierarchical policy $\pi^\ell \circ \pi^h$ selecting an incorrect action is bounded using a union bound as
\begin{align}
\gE(\pi^\ell \circ \pi^h)
&\leq \gE(\pi^h) + \gE(\pi^\ell) \\
&= \P[\hat V(s+k, g) \leq \hat V (s-k, g)] + \P[\hat V(s+1, s+k) \leq \hat V(s-1, s+k)] \\
&= \P[\hat V(k, T) \leq \hat V (-k, T)] + \P[\hat V(1, k) \leq \hat V(-1, k)] \\
&= \Phi\left(-\frac{\sqrt 2}{\sigma \sqrt{(T/k)^2 + 1}}\right) + \Phi\left(-\frac{\sqrt 2}{\sigma \sqrt{k^2 + 1}}\right).
\end{align}
\end{proof}


\subsection{Proof of \Cref{prop:repr}}
\label{sec:proof_repr}

%
We first formally define some notations.
For $s \in \gS, a \in \gA, g \in \gS$, and a representation function $\phi: \gS \to \gZ$,
we denote the goal-conditioned state-value function as $V(s, g)$,
the action-value function as $Q(s, a, g)$,
the parameterized state-value function as $V_\phi(s, z)$ with $z = \phi(g)$,
and the parameterized action-value function as $Q_\phi(s, a, z)$.
%
We assume that the environment dynamics are deterministic,
and denote the deterministic transition kernel as $p(s, a) = s'$.
Accordingly, we have $Q(s, a, g) = V(p(s, a), g) = V(s', g)$ and $Q_\phi(s, a, z) = V_\phi(p(s, a), z) = V_\phi(s', z)$.
%
We denote the optimal value functions with the superscript ``$*$'', \eg, $V^*(s, g)$.
We assume that there exists a parameterized value function,
which we denote $V_\phi^*(s, \phi(g))$,
that is the same as the true optimal value function,
\ie,  $V^*(s, g) = V_\phi^*(s, \phi(g))$ for all $s \in \gS$ and $g \in \gS$.
%
%

\begin{proof}
%
%
For $\pi^*$, we have
\begin{align}
    \pi^*(a \mid s, g) &= \argmax_{a \in \gA} Q^*(s, a, g) \\
    &= \argmax_{s' \in \gN_s} V^*(s', g) \\
    &= \argmax_{s' \in \gN_s} V_\phi^*(s', z),  \label{eq:ori_optimal}
\end{align}
where $\gN_s$ denotes the neighborhood sets of $s$, \ie, $\gN_s = \{s' \mid \exists a, p(s, a) = s'\}$.
For $\pi_\phi^*$, we have
\begin{align}
    \pi_\phi^*(a \mid s, z) &= \argmax_{a \in \gA} Q_\phi^*(s, a, z) \\
    &= \argmax_{s' \in \gN_s} V_\phi^*(s', z).  \label{eq:repr_optimal} 
\end{align}
By comparing \Cref{eq:ori_optimal} and \Cref{eq:repr_optimal},
we can see that they have the same $\argmax$ action sets for all $s$ and $g$. %
\end{proof}
%




%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%

\section{Subgoal Visualizations}

% Figure environment removed

% Figure environment removed

We visualize learned subgoals in \Cref{fig:viz_amz,fig:viz_proc} (videos are available at \hiqlwebsite).
For AntMaze-Large, we train \methodname without representations and plot the $x$-$y$ coordinates of subgoals.
For Procgen Maze, we train \methodname with $10$-dimensional representations
and find the maze positions that have the closest representations (with respect to the Euclidean distance) to the subgoals produced by the high-level policy.
The results show that \methodname learns appropriate $k$-step subgoals that lead to the target goal.





%


\end{document}