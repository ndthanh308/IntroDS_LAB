\section{User Study}
To evaluate the potential of \system{}, we conducted a user study consisting of preliminary user testing with eleven students and expert interviews with six math instructors. The purpose of the user testing was to evaluate the ease of use and user satisfaction with the system, while the expert interviews aimed to gain insights for educational use from the experts' perspectives.

\subsection{Preliminary User Evaluation}
We evaluated the usability of our system in three different conditions: the \system{} mobile AR interface, the \system{} desktop interface, and static textbook pages without AR. The same pages from a high school math textbook were used for each condition. 
We recruited 11 adult participants (8 males, 3 females, ages 18 - 39) from our local community, all of whom had at least a high school level knowledge in math and varying experiences with AR applications.
Each session lasted approximately 45 minutes, and participants were compensated with 15 CAD for their time.

\subsubsection{Method}
The study employed a within-subject evaluation design, with the three conditions randomized across participants. The AR interface was displayed on a tablet mounted on a tripod, with a pen used for interaction, while the web version ran on a laptop, with a mouse for interaction.
The study began with an introduction that included participants completing a consent form, followed by a system walkthrough. Subsequently, participants were asked to answer five math questions in each of the conditions, targeting the different features of the system: 
\begin{itemize}
\item How would the graph change if a particular constant were to be increased or decreased?
\item How would the equation change if a particular constant were to be increased or decreased?
\item How would you simplify this equation?
\item How would you expand this notation?
\item How would the variables in this equation change if we were to change the shape it represents in certain ways? 
\end{itemize}
The graphs, equations, and numbers varied slightly between the conditions. After completing all conditions, participants filled out a questionnaire that included usability questions for the AR and desktop interfaces, inquiries about each feature of the system, and an overall comparison of the three conditions. Responses were recorded on a 7-point Likert scale, and open questions allowed participants to provide additional feedback.

\subsubsection{Results} 
This section describes the results of our preliminary user study. We describe three aspects: 1) system usability between our mobile AR and desktop interfaces, 2) overall experience and engagement to compare mobile AR, desktop, and static textbooks, and 3) feedback for different features.

\subsubsection*{\textbf{Usability of Mobile AR and Desktop Interface}}
Overall, participants found both the AR interface and desktop interface easy to use, with a preference for the desktop interface. The desktop interface received an average SUS score of 81.82 (SD = 13.56), while the AR interface had an average SUS score of 75 (SD = 15.32). Many participants encountered issues with the imprecision of the AR version, including small fonts on the page that made selections difficult, especially when viewed through the camera (P1-2, P5, P9-11). This problem was exacerbated by the shakiness of participants' hands (P5) and the device itself when in use (P2, P10). In contrast, participants found the web version smoother to interact with, as the mouse provided a more precise input modality (P1, P2) and the fields were more consistently identified by the OCR (P3). However, some participants desired even greater precision, such as through text box input (P10, P11). %Overall, our results suggest that the system must improve its precision to enhance usability.

\subsubsection*{\textbf{Comparison of the Three Conditions: Mobile AR, Desktop, and Static Textbook}}
When comparing the three conditions, the desktop was the favorite among participants (M = 6.00, SD = 1.26), followed by the AR version (M = 5.18, SD = 0.98). The static textbook pages without AR were the least preferred (M = 3.40, SD = 1.81). When asked about which condition was the most engaging, participants preferred the AR interface (M = 6.09, SD = 0.83) over the desktop interface (M = 5.82, SD = 1.07) due to its novelty (P7, P10) and the intuitive physical manipulation it allowed (P11). The static textbook scored an average of 3.9 (SD = 1.97) in engagement. The results are summarized in \autoref{fig:boxplot}. 

% Figure environment removed



\subsubsection*{\textbf{Comparison of the Different Features}}
Participants generally provided positive feedback about the features. On a scale of 1 to 7, the \textit{interactive figure} received an average score of 6.55 (SD = 0.69) and was particularly popular among participants. They appreciated being able to see the cause and effect immediately on the graph as they interacted with the equation. The second favorite feature was the \textit{relationship highlight}, which scored an average of 5.81 (SD = 1.6). Participants valued how it helped them see the correlation between the shape and the equation more clearly. However, one participant (P2) did not find this feature helpful, as they believed the shape was already too abstract and disconnected from the equation. Most participants also found the \textit{step-by-step hint} helpful (M = 5.81, SD = 1.6), particularly as a convenience (P4, P7) or as a means of checking for mistakes (P9, P10). One participant (P2), however, found that it showed too many steps that they could already visualize in their head.
The \textit{exemplify} feature was well-liked (M = 5.55, SD = 1.69), as it helped participants concretize an abstract concept, and some found it easier to understand the notation through interaction (P3, P5). Some participants wanted to see it applied to more complex concepts (P7, P10), while others were more interested in the shorthand formula to solve abstracted notation rather than its detailed expansion (P2). Lastly, participants had mixed feelings about the \textit{dynamic value} feature and did not find it particularly useful compared to the other features (M = 4.91, SD = 1.76). It was more useful with long solutions (P9) or when combined with other features (P4, P7) but not as much on its own (P6). The results are summarized in \autoref{fig:feature}.

% Figure environment removed



%Similarly, P2 did not like the \textit{step-by-step hint} because it showed too many steps that they could already visualize in their head. However, most participants found this feature helpful (M = 5.81, SD = 1.6), particularly as a convenience (P4, P7) or as a means of checking for mistakes (P9, P10). 

%Our participants found the web version to be easier to use than the AR version. The web version had an average SUS score of 81.82 (SD = 13.56), while the AR version had an average SUS score of 75 (SD = 15.32). Most participants experienced issues with the imprecision of the AR prototype, such as small fonts on the page that made it difficult to make selections, especially when viewed through the camera (P1-2, P5, P9-11). This problem was exacerbated by the shakiness of participants' hands (P5) and the device itself when in use (P2, P10). In contrast, participants found the web version smoother to interact with because the mouse provided a more precise input modality (P1, P2) and the fields were more consistently identified by the OCR (P3). However, some participants desired even more precision, such as through text box input (P10, P11). Overall, our results suggest that the system must improve its precision to enhance usability.

%The participants generally gave positive feedback about the features. On a scale of 1 to 7, the interactive figure received an average score of 6.55 (SD = 0.69) and was particularly popular among participants. They liked being able to see cause and effect immediately on the graph as they interacted with the equation. The second favorite feature was the relationship highlight, which scored an average of 5.81 (SD = 1.6). Participants appreciated how it helped them see the correlation between the shape and the equation more clearly. However, one participant (P2) did not find this feature helpful as they thought the shape was already too abstract and disconnected from the equation. Similarly, P2 did not like the step-by-step hint because it showed too many steps that they could already visualize in their head. However, most participants found this feature helpful (M = 5.81, SD = 1.6), particularly as a convenience (P4, P7) or as a means of checking for mistakes (P9, P10). The exemplify feature was also popular (M = 5.55, SD = 1.69) as it helped participants concretize an abstract concept, and some found it easier to understand the notation through interaction (P3, P5). Some participants wanted to see it applied to more complex concepts (P7, P10), while others cared more about the shorthand formula to solve abstracted notation rather than its detailed expansion (P2). Lastly, the participants had mixed feelings about the dynamic value feature and did not find it particularly useful compared to the other features (M = 4.91, SD = 1.76). It was more useful with long solutions (P9) or when combined with other features (P4, P7) but not so much on its own (P6).

%Finally, we asked participants to rate the overall system on a scale of 1 to 7. The web version was the clear favorite with an average score of 6.00 (SD = 1.26), followed by the AR version (M = 5.18, SD = 0.98), and then the physical pages (M = 3.4, SD = 1.81). Most participants preferred the web version due to its precision and ease of interaction (P1, P3, P5, P6, P7, P9, P10). However, P2 found the augmentations distracting and preferred just the physical page. Participants found the AR version to be the most engaging (M = 6.09, SD = 0.83), followed by the web version (M = 5.82, SD = 1.07), and then the paper version (M = 3.9, SD = 1.97). Overall, most participants agreed that the interaction made for a more engaging experience, while the AR application edged out the web version due to novelty (P7, P10) and intuitive physical manipulation (P11).
% The feedback on the idea and implemented features were much more positive. Interactive figure 




% The web app feels less clunky/ more smooth (P1, P2, P3, P6)
%     - mouse is more precise (P1, P2)
%     - more stable and the fields were more consistently identified (p3)

% Participants want more controlled input method such as a textbox (P10, p11)

% Participants don't like being constrained by the page
%     - "the graph sizes felt a bit suffocating" P10 


% The results showed that the AR condition had a significantly higher task completion rate compared to both the web page and textbook conditions (p<0.05). The mean completion time for the AR condition was 48.3 seconds (SD=14.1), which was significantly faster than the web page (mean=58.9 seconds, SD=22.2) and textbook (mean=73.1 seconds, SD=20.9) conditions.

% For task correctness, AR condition had significantly higher task correctness (M = 95\%, SD = 3.3) than the Web Page condition (M = 82\%, SD = 7.9) and the textbook control condition (M = 80\%, SD = 8.2). There was no significant difference in task correctness between the Web Page and textbook control conditions.

% For user Satisfaction, AR condition had significantly higher ratings (M = 6.9, SD = 0.4) than the Web Page condition (M = 5.4, SD = 0.9) and the textbook control condition (M = 5.3, SD = 0.8). There was no significant difference in user satisfaction ratings between the Web Page and textbook control conditions.



\subsection{Expert Interview}
We conducted semi-structured interviews with five math instructors to evaluate the system's potential for educational use. These instructors have teaching experience ranging from 1 to 6 years (M = 3.4 years). Three experts teach at the university level (E1, E3, E4), one teaches late elementary to middle school level (E2), and the last teaches from middle school up to early junior university level (E5). During the interviews, we asked them to compare four different educational media: textbooks, videos, interactive websites, and our AR approach. Each interview lasted approximately 60 minutes, and the experts received compensation of 15 USD.

\subsubsection{Method}
The goal of the interviews was to gather feedback on the augmented textbook and identify potential areas for improvement. We began by asking the instructors about their experience using textbooks, as well as any challenges or benefits they found in the physical medium. Next, we presented an educational video and an interactive math website covering a topic familiar to the instructors. We asked them to compare these three media as educational resources.
Finally, we demonstrated our system through an example authoring walkthrough, allowing the instructors to interact with the system. We gathered their feedback on our AR approach through a series of open-ended questions, focusing on how it compares to existing methods. 

\subsubsection{Results}
Overall, all the experts expressed enthusiasm about our system. They specifically highlighted several key benefits of our approach, including enhanced student engagement, minimal authoring efforts, adaptable content, and potential use for independent learning. Below, we outline our key findings.

\subsubsection*{\textbf{Student Engagement through Interactions}}
All the instructors we interviewed concurred that integrating interactivity into learning materials can effectively encourage students to explore and experiment with concepts at their own pace. They expressed that the interactive nature of our augmented textbook has the potential to transform mundane textbooks into engaging, game-like experiences (E4). 
Particularly, E5 mentioned that \textit{ "the ones that are fascinated by it will play around and try to learn more"}. 
The experts highlighted the value of manipulating content and observing immediate results, especially in calculus, where students grapple with unintuitive concepts like limits. As one instructor noted,
\textit{"actually seeing that change happen in real time could be helpful to understand a concept"} (E1). 
Our augmented reality approach, which combines visual and motor stimuli, may also aid students in retaining information more effectively (E5). 

While the experts generally praised the potential benefits of augmented textbooks, most (E1-E4) believed that the available interactions were not always self-explanatory and required guidance. Instructors E3 and E4 specifically emphasized the importance of incorporating built-in guidance to assist them in directing students' attention and interactions. In summary, our system can enhance engagement with traditional materials, enabling seamless integration into existing educational workflows.

\subsubsection*{\textbf{Reduced Authoring Effort}} 
During the interviews, instructors revealed that they dedicate a considerable amount of time to crafting materials for their students, often drawing from online resources or textbooks. They also expressed an interest in developing interactive teaching media but frequently lack the necessary skills (E3, E4, E5). Current authoring interfaces can be varied and complex, and instructors may not be well-versed in using them. E1 specifically stated, \textit{ "If you ask me to graph a piece wise function on GeoGebra right now, I wouldn't be able to do it"}.
Our approach holds the potential to substantially reduce the authoring effort needed to create interactive materials. E1 also noted that our approach makes it especially easy to modify existing content rather than starting from scratch. All the experts we interviewed agreed that they could utilize our tool to produce relevant materials for their students. 
Overall, our AR approach has the potential to make the creation of interactive materials more accessible and time-efficient for instructors.

\subsubsection*{\textbf{On-Demand Adaptable Content}}
Teachers frequently create customized content to align better with their curriculum and teaching objectives, building upon existing materials. \textit{"I would often just use specific images or drawings from textbooks and papers to explain a concept"} (E4). Our AR approach offers significant support in this area. During the interviews, instructors appreciated the system's ability to select from various augmentation techniques, enabling them to customize the augmented textbook according to their specific needs. This adaptability allows them to tailor the content to their course's learning objectives and their students' requirements. One expert (E1) found that integrating interactive websites into teaching can be challenging due to unfamiliarity with the site's functionalities or difficulties inputting functions into the system. Furthermore, comprehending the website's content and relating it back to the course material can also be challenging (E2). In contrast, our AR approach offers instructors more control over the material they are presenting (E1, E4). This control is particularly beneficial when teaching in front of a class, as customizing the material to suit specific teaching objectives is more desirable than using pre-animated content, such as videos (E1).
Overall, our AR approach delivers an efficient and adaptable solution for instructors to create interactive materials that align with their teaching objectives and better engage their students.


\subsubsection*{\textbf{Independent Learning}}
Instructors frequently assign materials for students to review independently, such as videos or online resources (E1, E3). Our AR approach provides an accessible solution for creating customized materials that can be easily shared with students.
During the interviews, instructors expressed enthusiasm about the ability to easily share the interactive AR textbook with their students via a web address. Moreover, having access to the materials at any time empowers students to review or advance at their own pace, allowing for more in-depth discussions and knowledge refinement during class time (E5).




% %\subsubsection{\textbf{Method}} 
% We presented participants with 5 figures with small variations for each trial. The AR version is presented through a tablet that is mounted on a tripod, the web version was ran on a laptop with a mouse, and we printed out pages from a high school math textbook for the baseline. Each participant were asked to answer a simple math question for each figure, repeated over each condition for a total of 15 questions. The questions we asked them includes:
%     \begin{enumerate}
%         \item How would the graph change if a particular constant were to be increased or decreased?
%         \item How would the equation change if a particular constant were to be increased or decreased?
%         \item How would you simplify this equation?
%         \item How would you expand this notation?
%         \item How would the variables in this equation change if we were to change the shape it represents in certain ways? 
%     \end{enumerate}
% We asked variations of these questions over similar figures and equations with different values for each condition to ensure consistency. We randomized the order of the conditions, the order of questions, the figures, the equations, and the numbers that were used. We evaluate the ease of use with the system usability scale [cite]. We also collect user feedback on implemented features, their engagement with the system using a 7-point likert scale through a post-task questionnaire. Moreover, we asked the participants to rank the three conditions based on their personal preference as well. 

% Each participant were ask to answer 2 questions realated to the figure or equations that are being presented. by using a figure we prepared beforehand. This task is repeated over 5 figures, for a total of 10 questions per condition. 
% We show the participants an interactive figure that we prepared beforehand, and asked them 2 simple questions that they can answer using what we gave them. The first trial is a testrun where the participant can get used to how the interface works. We ran 5 more trials for a total of 10 questions, excluding the test run. We repeat this for each condition. We measure task completion time and correctness for each trial. 
% They were asked to complete a set of tasks using Reality Extract, such as creating an interactive visual, embedding it into a static textbook, and adding dynamic value to it. During the test, we collected various data, including task completion time, error rates, and subjective feedback from the participants.


%where 5/11 with about 50\% of our participants having 1st to second year level of math proficiency. 
%Three participants frequently use AR, six have some experience, and the rest have little to no prior AR experience. 
%We designed the study to compare the effectiveness of our AR approach against online resources with the following three conditions: \system{} mobile AR interface, \system{} desktop interface, and a static math textbook as a baseline.



% % Figure environment removed