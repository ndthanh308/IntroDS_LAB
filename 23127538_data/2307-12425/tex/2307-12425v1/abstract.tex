\begin{abstract}
A common training technique for language models is teacher forcing (TF). TF  attempts to match human language exactly, even though identical meanings can be expressed in different ways. This motivates use of sequence-level objectives for dialogue response generation. In this paper, we study the efficacy of various offline reinforcement learning (RL) methods to maximize such objectives. We present a comprehensive evaluation across multiple datasets, models, and metrics. Offline RL shows a clear performance improvement over teacher forcing while not inducing training instability or sacrificing practical training budgets.

\vspace{-1.1em}

\end{abstract}

%% Version 2

%A common training technique for language models is teacher forcing that attempts to match human language exactly. However, humans vary in their expressions which motivates the use of sequence-level objectives. In this paper, we study the efficacy of various offline reinforcement learning (RL) techniques to maximize sequence-level objectives for dialogue text generation. We present a comprehensive evaluation across multiple datasets, models and metrics and find that offline RL methods show a clear performance improvement over teacher forcing while not inducing training instability or sacrificing practical training time budgets. % We also analyze the trade-offs between various offline RL approaches. 

% common training technique is teacher forcing
% attempts to match human language exactly
% However, humans vary in their expressions which motivates the use of sequence-level objectives.

%% Version 1

% Humans often say the same things in different ways. However, we typically train dialogue models via teacher forcing to \emph{exactly} match human tokens, which can be needlessly challenging. Instead, we would like to train such models to produce utterances that are \emph{close enough}, that a human would choose if offered as a suggestion. This requires optimizing objectives that consider an entire sequence rather than individual tokens.

%We study the efficacy of various offline reinforcement learning (RL) techniques to maximize sequence-level objectives for dialogue text generation. We present a comprehensive evaluation across multiple datasets (ABCD, MultiWoz, TaskMaster), models (distilGPT2, GPT2) and metrics (\textsc{BERTScore}, \textsc{BLEURT}, \textsc{METEOR)}. We find that offline RL methods show a clear performance improvement over teacher forcing while not inducing training instability or sacrificing practical training time budgets. We also analyze the trade-offs between various offline RL approaches.
