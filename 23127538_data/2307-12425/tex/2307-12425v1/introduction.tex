\section{Introduction}
\label{introduction}

% Humans often say the same things in different ways. However, we typically train dialogue models via teacher forcing to \emph{exactly} match human tokens, which can be needlessly challenging. Instead, we would like to train such models to produce utterances that are \emph{close enough}, that a human would choose if offered as a suggestion. This requires optimizing objectives that consider an entire sequence rather than individual tokens.

Dialogue response generation is an important task in natural language processing with numerous applications such as virtual personal assistants and call center agent tools~\cite{zhou2017end, swanson2019building, jaques2020human, ramakrishnan2022long, ouyang2022training}. Historically, text generation models have typically been trained with teacher forcing (TF) \cite{williams1989learning}, which involves predicting the next token in a sequence to exactly match the human utterance in a ground truth dataset. However, this can be a needlessly challenging objective, as a human may choose to say the same thing in multiple different ways. Consider a dialogue system that provides suggestions to an agent during a conversation with a customer. These suggestions need only be \emph{close enough} for an agent to select it. This suggests a different objective, one that is defined on the entire sentence rather than individual tokens.

% Consider a dialogue system that provides suggestions to an agent during a conversation with a customer. These suggestions don't need to be exactly what the agent would say, but rather \emph{close enough} such that the agent would choose the suggestion if it were offered. 

% %This suggests a different objective, one that is defined on the entire sentence rather than individual tokens.

One way to design such a loss would be to incorporate human-in-the-loop feedback if a model generated utterance matches the meaning of the ground truth sentence. 
However, this can be expensive to collect. Instead, model-based metrics to measure utterance similarity, such as BERTScore~\cite{zhang2019bertscore} and BLEURT~\cite{sellam2020bleurt}, provide a cheaper alternative. These are automated metrics that capture semantic similarity between sentences and tend to have a high correlation with human judgment~\cite{zhang2019bertscore,sellam2020bleurt}. Given a choice of such metrics, what learning framework would allow us to maximize them for dialogue text generation?

% Rewrite
% One reason such methods have not been used because challenging to optimize for sequence-level objective
% What's attractive about TF, per token loss, back propogate easily, classification problem. sequence-level can't backpropogate.
% sentence level have delayed feedback and need RL.
% Given a choice of metrics, .... what framework?

% \kqw{I would cut this following paragraph. It feels redundant with the paragraphs before and after. Instead, I would just list these as strong features of offline RL in the next paragraph (which you already do). }
% We begin by listing a desired set of attributes for such a framework. \textbf{D1.} The framework should capture sentence-level invariances in utterances, some of which aren't evident until the entire sentence is revealed. For instance, ``A supervisor will be with you in one moment'' should be marked similar to ``Please wait one moment while I transfer you to my supervisor''. \textbf{D2.} The framework need not require expensive interactions with humans. It should be able to take an existing corpus of dialogue data and train on it directly. \textbf{D3.} It should have comparable computational efficiency to teacher forcing for it to be practically useful.

Recent works have explored online RL methods for text generation \cite{ranzato2015sequence, li2016deep, ouyang2022training, ramamurthy2022reinforcement}, leading to some exciting successes~\cite{ouyang2022training}. 
However, \textit{offline} RL has received relatively less attention~\cite{jaques2020human, pang2020text}.
We argue that offline RL~\cite{levine2020offline} does provide a framework that meets all aforementioned desiderata. Unlike teacher forcing, it can handle losses on the entire sequence as a reward function and  unlike online RL, it can leverage existing data without having to explore, matching similar training times as teacher forcing. 

In this paper, we present a comprehensive evaluation of offline RL methods for dialogue text generation and investigate best practices. 
We explore three complementary approaches. 
The first, TF Top, is to fine-tune a model on utterances that accrue high returns.
The second, Decision Transformers (DT)~\cite{chen2021decision}, is to train a conditional model that conditions on returns, and at inference time condition on a high return.
The third, ILQL \cite{kostrikov2021offline, snell2022ilql}, is an off-policy Q-learning approach that uses dynamic programming to train a  critic.
All three of these approaches are complementary and have been shown to be competitive outside of dialogue settings, making them great candidates to evaluate the efficacy of offline RL for dialogue text generation.

% We formalize dialogue text generation as an MDP, analyze three different existing offline RL methods in the context of text generation and evaluate these methods across multiple datasets, models and metrics. We find that offline RL methods show a clear performance improvement over teacher forcing and achieve a trade-off where they generate close enough text rather than trying to exactly match the human. 

% Our contributions are, (1) Formulation and broad comparison of three state-of-the-art offline RL approaches for dialogue text generation across multiple datasets, models and metrics. (2) Ablation and analysis of these approaches in dialogue settings.

To summarize our contributions, we formalize three state-of-the-art offline RL approaches for the task of dialogue text generation. We evaluate them across multiple data sets, models, and metrics and provide a thorough ablation analysis of these approaches. We find that offline RL methods show a clear performance improvement over teacher forcing and achieve a trade-off where they generate text close enough in meaning to human. Through different experiments, we demonstrate that the offline RL framework provides an ideal fit for the task of dialogue generation, and should be considered seriously by the community.

\vspace{-0.25em}