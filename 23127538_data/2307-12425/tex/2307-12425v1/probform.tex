\section{Problem Formulation}
\label{prob_form}

\subsection{Dialogue Response Generation as an MDP}
We look at the problem of dialogue text generation, \textit{i.e.}, generating response utterances in a dialogue setting. We begin with a supervised dataset of context response pairs $\{ x^i, y^i \}_{i=1}^{N}$, where context $x$ is the conversation history, and response $y = \{y_1, \dots, y_T \}$ is a target sequence of tokens. We map each data point $(x, y)$ to an episode of a Markov Decision Process (MDP), which we define below (Fig. \ref{fig:prob_form}):
\begin{itemize}[nosep]
\item \textbf{States, $s_t \in \mathcal{S}$} is the context $x$ and the partially generated sequence of tokens up to and including time step $t$, $\hat{y}_{<t}:=\{\hat{y}_1,\dots, \hat{y}_t\}$. 
\item \textbf{Actions, $a_t\in \mathcal{A}$} are the set of next tokens $\hat{y}_{t+1}$ available from the vocabulary $V$
\item \textbf{Transition function, $\mathcal{T}(s_{t+1} | s_t, a_t)$} is deterministic since every state-action pair $(\hat{y}_{<t}, \hat{y}_{t+1})$ leads to a unique state $\hat{y}_{<t+1}$ for the next step.
\item \textbf{Rewards, $r_t: \mathcal{S}\times\mathcal{A}\rightarrow [0, 1]$} is a terminal reward that computes similarity generated response $\hat{y}$ and 
target response $y$
\item \textbf{Horizon, $T$} is time horizon. Each episode ends when the current time step $t$ exceeds $T$ or an end-of-sentence (EOS) token is generated.
\end{itemize}
The goal is to learn a policy $\pi: s_t \rightarrow a_t$ maximizing \textit{return}, \emph{i.e.} the cumulative reward over an episode $\mathbb{E}_{\pi} \sum_{t=0}^T \gamma^t r_t$. We assume undiscounted cumulative rewards, \emph{i.e.} $\gamma=1$.

\subsection{Rewards for Dialogue Response Generation}
We define the reward to be a similarity metric between the generated text $\hat{y}$ and the speaker's ground truth utterance $y$. Such a metric should capture both what the speaker is trying to communicate and the relevance to the conversation. 
% Learnt / Human annotated vs automated?

One option is to collect human-in-the-loop annotations, \emph{i.e.} what the speaker would likely prefer to say. However, this requires costly human supervision. Automated metrics, such as BERTScore \cite{zhang2019bertscore}, BLEURT \cite{sellam2020bleurt}, offer a promising alternative. They are able to capture models of human preference and are cheap to evaluate. 

We use a terminal reward since the similarity can only be evaluated at the end of the utterance, and since the same content can be expressed in different styles, \emph{e.g.} \emph{The flight from New York to Boston has been confirmed.} vs. \emph{Your JFK to BOS flight has been booked.}

\subsection{Why Offline Reinforcement Learning?}
\label{sec:why_offline_rl}

In online reinforcement learning, an agent learns by interacting with an environment in real-time. This presents an explore-exploit trade-off, where the agent must balance the need to try out new actions to learn about the environment with the need to exploit its current knowledge to maximize reward. This can be particularly challenging in text generation, as action space (\emph{i.e.} vocabulary size) is often large, \emph{e.g.} of the order of 50,000 words for GPT-based models~\cite{radfordlanguage}. Another problem is that the reward landscape is sparse, hence policies during training can get stuck in local minima where reward is persistently zero. 

For text generation, we argue that an offline setting is reasonable. There exists good generation policies, e.g. policies from teacher forcing, that can generate a set of responses such that one of them is close enough to the human response. Also, once a token is generated, we \emph{deterministically} transition to next state with the additional token appended to the prefix, \emph{i.e.} no interaction is needed to learn the environment.
 
Offline RL provides a learning paradigm that combines both supervised learningâ€™s ability to leverage existing data with the general utility optimization power of online reinforcement learning methods. We collect an offline dataset of state transitions $\mathcal{D} = \{(s_t^i, a_t^i, r_t^i, s_{t+1}^i)\}_{i=1}^{N}$\footnote{We suppress superscripts when considering a single transition.} using a behavior policy $\pi_{\beta}$, typically a policy trained via supervised learning. The goal is to learn a policy $\pi$ that maximizes performance on the dataset while staying close to the behavior policy:
% \begin{equation}
    \begin{align}
        \max_{\pi} J_{\mathcal{D}}(\pi) - \alpha D(\pi, \pi_{\beta}) \, ,
    \end{align}
% \end{equation}
where $J_{\mathcal{D}}(\cdot)$ is performance on dataset $\mathcal{D}$ and $D(\cdot, \pi_{\beta})$ is distributional regularization against behavior policy $\pi_{\beta}$.