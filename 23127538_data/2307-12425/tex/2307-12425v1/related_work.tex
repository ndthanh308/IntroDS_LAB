\section{Related Work}
\label{related_work}

% Figure environment removed

% States are the context $x_i$ and partially generated response tokens up to time t, $\hat{y}_{\leq t}$. Actions are set of next tokens $\hat{y}_{t+1}$ available from vocabulary $V$. Rewards here are a terminal reward that computes similarity of the generated response $\hat{y}$ against target response $y$ once current time step $t$ exceeds $T$ or an end of sentence (EOS) token is generated.

\paragraph{RL for NLP.}  Prior work has used RL techniques to improve models in a variety of NLP applications~\cite{ranzato2015sequence, pang2020text, yang2020improving, lu2022quark, snell2022ilql, ramamurthy2022reinforcement} such as machine translation~\cite{yonghui2016bridging, wu2018adversarial, kiegeland2021revisiting}, summarization~\cite{paulus2017deep, pasunuru2018multi, stiennon2020learning}, question answering~\cite{furman2022qait}, visual reasoning ~\cite{wu2022lilgym} and instruction following ~\cite{misra2015reinforcement, ouyang2022training}. Techniques adopted range from online RL methods like REINFORCE~\cite{williams1992simple} and PPO~\cite{schulman2017proximal} to offline RL approaches like conservative Q-learning (CQL)~\cite{kumar2020conservative} and decision transformers~\cite{chen2021decision}.

\textbf{RL for Dialogue Generation.} 
Dialogue generation can be challenging as generated sequences can be long and in each turn there can be multiple acceptable responses. \citet{li2016deep} use REINFORCE to optimize a set of rewards that capture informativity, coherence, and ease of answering. \citet{zhou2017end} use a mixture of on and off-policy policy gradient to optimize a reward that captures both utterance-level and dialog-level rewards. \citet{jaques2019wayoff,jaques2020human} use offline RL to optimize a learned reward function from human responses. \citet{ouyang2022training} use PPO to optimize a learned reward model from human ranking. Our goal is to generate dialogue responses that are semantically close to ground truth utterances without having to design explicit rewards that capture dialogue success~\citet{liu2016not}. This is complementary to approaches that look at optimizing dialogue-level metrics like key values for slots \cite{lee2021dialogue, tian2021amendable, bang2023task}.

\textbf{Offline RL for NLP.} Offline RL removes the need for interaction during train time operating only on static datasets of prior human interaction, which leads to improved training stability. \citet{pang2020text} use importance weighted REINFORCE, which only trains a policy without a critic to control for variance. \citet{verma2022chai} use CQL but operate on entire utterances and not per token thus reasoning over shorter sequences. \citet{jaques2019wayoff, jaques2020human} operate at per-token level using off-policy Q-learning, but require generation at RL training time that can be expensive. \citet{snell2022ilql} propose ILQL, a variant of CQL with implicit dataset support constraints, that requires no such generation at train time. \citet{lu2022quark} propose Quark, that uses Decision Transformers by quantizing rewards. While both papers explore metrics like toxicity and sentiment, they don't optimize for similarity to human utterances in dialogue settings that we examine in this paper.