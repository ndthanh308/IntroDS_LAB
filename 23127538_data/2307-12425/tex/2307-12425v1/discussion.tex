\section{Discussion}
\label{discussion}

In this paper, we examine the effectiveness of offline RL methods for generating dialogue responses. We analyze three distinct techniques: fine-tuning on high returns (TF Top), conditioning on return (DT), and an off-policy Q-learning approach (ILQL). Our evaluation is based on three task-oriented dialogue datasets, and we conduct various analyses and ablation studies to investigate the trade-offs between these approaches. 

\paragraph{Offline RL models learn to produce good enough text that are similar to human.}

% * We saw offline RL improve upon TF (by about 5\%)
% * We learned that the improvements are in examples like (qual example insight)
% * The improvement is not just on average, but a distributional improvement. Histogram shows that improvements are spread across bins, gap sustained across multiple responses.
% * The improvement is sustained even when we go to bigger models

We hypothesized that there are multiple ways to convey the same information as a human and that a model can learn this. We constructed a reward using \textsc{BERTscore} that captures this similarity and trained various offline RL methods. 
% We tested this hypothesis by using offline reinforcement learning (RL) techniques for dialogue text generation. 
Our results show that offline RL clearly improves upon traditional methods by approximately 5\% (Table \ref{table:overall_metrics}). We found that the improvements were most significant in examples where traditional methods repeat themselves to ask for the same information or do not follow the correct flow of a human utterance even if the response is contextually relevant (Fig. \ref{fig:human_eval}). Improvements were not limited to overall averages but also seen as a distributional improvement (Fig. \ref{fig:bert_score_hist}). Additionally, the improvements were sustained across multiple responses and when using larger models (Fig. \ref{fig:topk_bert_click}, Table \ref{table:model_sizes}).

\paragraph{Decision Transformer is a practical choice.}
% * When we use all the data, DT and TF Top are comparable
% * When we use limited data, DT outperforms TF Top (and backed by theory)
% * Comparable training time to TF Top 
% * In theory, not much worse than ILQL. 
% * In practice, better generator than ILQL, does not need to rely on additional critic heads
% * For scalar rewards, selecting a threshold necessary for TF top
%We find Decision Transformer (DT) to be the practical choice. 

When working with all available data, \textbf{DT} and \textbf{TF Top} show comparable performance. However, when it comes to limited data, \textbf{DT} significantly outperforms \textbf{TF Top} (Table \ref{table:overall_metrics}, Fig. \ref{fig:subsample_dt_vs_tf_top}). This aligns with our understanding from theory that suggests that \textbf{TF Top} discards useful information while \textbf{DT} retains it. This is relevant for fine-tuning in low data regimes where we expect \textbf{DT} to be more effective. 
% Moreover, for scalar rewards, it is necessary to select a threshold that defines what constitutes \emph{top} returns. If not selected correctly, this leads to sub-optimal performance compared to DT that is independent of thresholds. 

% DT has comparable training time to TF Top and, in theory, its performance is not much worse than that of ILQL. In practice, DT proves to be a better generator than ILQL, as it does not require additional critic heads.

\paragraph{We see two potential future directions.} First, we use \textsc{BERTscore} as a proxy for whether a human would have clicked on the suggested utterance. Instead, can we learn reward functions from human feedback that is easier to optimize? Second, we consider a single turn when a dialogue has multiple turns. How do these methods compare when optimizing rewards that extend to more than 1 turn? 

% \textbf{Offline RL models learn to produce good enough text that are similar to human}
% * We hypothesized that there are multiple ways to say the same thing as a human, and that a model could learn that
% \textbf{Decision Transformer is a practical choice}
% \textbf{Future directions}