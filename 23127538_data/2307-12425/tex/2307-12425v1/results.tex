\section{Experiments}
\label{evaluation}

\subsection{Experimental Setup}

\subsubsection{Task-oriented Dialogue Datasets}

We evaluate offline RL methods using three task-oriented dialogue datasets. These are relevant for dialogue systems designed for real-world applications, where users have specific goals and tasks that they want to accomplish. Each dataset consists of conversations between two speakers: one is the system or agent, and the other is the user or customer. We optimize rewards on system or agent utterances so as to emulate applications designed to assist an agent (e.g customer service representative) in providing helpful and human-like responses to customer queries and problems.

\textbf{MultiWoz 2.2~\cite{zang2020multiwoz}} is a widely used dataset created to evaluate performance of dialogue systems in multi-domain settings. It consists of over 10k conversations spanning 8 domains like hotel, train, restaurant, etc.

\textbf{Action Based Conversations Dataset (ABCD)~\cite{chen2021action}} contains customer-agent conversations where the agent's goal is to solve a customer problem. It consists of over 10k conversations spread over 55 user intents in the retail customer service domain.

\textbf{TaskMaster-3~\cite{byrne2019taskmaster}:} contains 23,789 conversations between users and a system on movie ticketing.

\subsubsection{Baselines and Metrics}
We choose a terminal binary reward \textsc{BERTClick}, which is a thresholded \textsc{BERTScore} \cite{zhang2019bertscore} with threshold value $0.6$. We select a value of $0.6$ qualitatively such the generated response is close enough and has similar meaning to human response. We evaluate on a range of automated similarity metrics shown to have a high correlation with human judgements like \textsc{BERTScore} \cite{zhang2019bertscore}, \textsc{BLEURT} \cite{sellam2020bleurt}, \textsc{METEOR} \cite{banerjee2005meteor} and \textsc{BLEU} \cite{papineni2002bleu}. We also do human evaluation on a subset of the data where we ask humans to rate similarity and relevance on a scale of 1-3. More details on the study in Appendix~\ref{appx:human_eval}.

We evaluate across following methods and baselines: \textbf{TF}, Base model trained via teacher forcing on all conversations, \textbf{TF All}, TF model fine tuned on entire offline RL Dataset, \textbf{TF Top}, TF model fine tuned only on data points with top returns $\mathcal{D}_{top}$, \textbf{DT}, Decision Transformer, \textbf{ILQL}, Off-policy Q-learning, \textbf{PPO}, Online RL via policy gradients.

We train the TF model on all the training data (stage 1), use this trained TF model to generate an offline RL dataset (stage 2), and finally fine tune different RL models on varying percentages of generated offline RL data (stage 3). More details on training setup are in Appendix~\ref{appx:experiment_details}. Since the generation step is expensive, we would like to be able to fine tune on subsets of offline RL dataset for improved efficiency and train time budgets.

For base models we study GPT2Medium\footnote{\url{https://huggingface.co/gpt2-medium}} \cite{radfordlanguage} and DistilGPT\footnote{\url{https://huggingface.co/distilgpt2}} \cite{sanh2019distilbert} which have 355M and 82M parameters, respectively. For real-time environments, models like distilGPT2 are preferable since they have low latency (order of 100 ms) to be used in dialogue settings. We use huggingface transformers library \cite{wolf2019huggingface} to implement TF Top, DT and trlx\footnote{\url{https://github.com/CarperAI/trlx}} for ILQL, PPO.

% We Training costs about x GPU hours in total.

Finally, we evaluate models as both generators and rankers. For ranker, we score set of responses generated by base \textbf{TF} model and pick highest score. In our experiments, we found \textbf{ILQL} to be more effective as a ranker, as it trains a critic rather than an actor. Hence, we evaluate \textbf{ILQL} as a ranker. 

\subsection{Results and Analysis}

% This allow us to study the effect of offline RL for models of various sizes

%% Backup 1 %%

% \begin{table*}[t]
% \centering
% \resizebox{0.7\textwidth}{!}{
% \begin{tabular}{cccccccc}
% \toprule
% & Algorithm & \textsc{BERTClick} & \textsc{BERTScore} & \textsc{BLEURT} & \textsc{METEOR} & \textsc{BLEU} & \textsc{Perplexity} \\ 
% & & (reward) & &  &  &  &  \\ 
% \midrule
% \parbox[t]{2mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{ABCD}}} 
%  & TF      & 0.276           & 0.404           & 0.571              & 0.370               &  0.135\\
%  & TF All & 0.281           & 0.401           & 0.569              & 0.385               &  0.145 \\
%  & TF Top &\textbf{0.315}   & \textbf{0.427} & 0.579               & \textbf{0.391}      & \textbf{0.166}\\ 
%  & DT     & 0.314           &  0.425         & \textbf{0.58}       & 0.388               & 0.158 \\ 
%  % & PPO  & & & & & \\
% \midrule
% \parbox[t]{2mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{MultiWoz 2.2}}} 
%  & TF & 0.13 & 0.366 & 0.512 & 0.312 & 0.074 \\
%  & TF All & 0.156 & 0.374 & 0.518 & 0.315 & 0.085 \\
%  & TF Top & \textbf{0.178} & \textbf{0.399} & 0.537 & \textbf{0.335} & 0.09\\ 
%  & DT & 0.176 & 0.394 & 0.532 & 0.334 & \textbf{0.091}\\ 
%  & PPO & 0.147 & 0.364 & & 0.320 & 0.079 \\
% \midrule
% \parbox[t]{2mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{TaskMaster-3}}} 
%  & TF & 0.446 & 0.554 & 0.624 & 0.513 & 0.36 \\
%  & TF All & 0.446 & 0.557 & 0.632 & 0.514 & 0.36 \\
%  & TF Top & 0.462 & 0.561 & 0.629 & 0.518 & 0.36 \\ 
%  & DT & \textbf{0.465} & \textbf{0.563} & \textbf{0.633} & \textbf{0.521} & \textbf{0.364} \\ 
%  & PPO & & & & & \\
%  \midrule
% \end{tabular}
% }
% \label{table:overall_metrics}
% \caption{Comparison among all baselines on average metrics across varying datasets. }
% \end{table*}

\begin{table*}[t]
\centering
\renewcommand{\arraystretch}{1.1}
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{cccccccccccccc}
\toprule
& Algorithm & \multicolumn{2}{c}{\textsc{BERTClick}} & \multicolumn{2}{c}{\textsc{BERTScore}} & \multicolumn{2}{c}{\textsc{BLEURT}} & \multicolumn{2}{c}{\textsc{METEOR}} & \multicolumn{2}{c}{\textsc{BLEU}} & \multicolumn{2}{c}{\textsc{Perplexity}($\downarrow)$} \\ 
&  & 20\% & 80\% & 20\% & 80\% & 20\% & 80\% & 20\% & 80\% & 20\% & 80\% & 20\% & 80\%\\ 
\midrule
\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{ABCD}}} 
 & TF & \multicolumn{2}{c}{0.276} & \multicolumn{2}{c}{0.404} & \multicolumn{2}{c}{0.571} & \multicolumn{2}{c}{0.370} & \multicolumn{2}{c}{0.135} & \multicolumn{2}{c}{36.36} \\  
 & TF All & 0.269 & 0.285 & 0.390 & 0.399 & 0.559 & 0.564 & 0.365 & 0.375 & 0.134 & 0.143 & 41.76 & 45.39 \\
 & TF Top & 0.281 & 0.307 & 0.388 & 0.420 & 0.559 & 0.576 & 0.358 & 0.382 & 0.135 & \tbcolorg 0.156 & 36.82 & 34.25 \\ 
 & DT & \tbcolorg 0.299 & \tbcolorg 0.321 & \tbcolorg 0.411 & \tbcolorg 0.429 & \tbcolorg 0.572 & \tbcolorg 0.582 & \tbcolorg  0.372 & \tbcolorg  0.391 & \tbcolorg 0.144 & 0.155 & 36.22 & 36.51\\ 
\midrule
\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\small MultiWoz 2.2}}}
 & TF & \multicolumn{2}{c}{0.130} & \multicolumn{2}{c}{0.366} & \multicolumn{2}{c}{0.512} & \multicolumn{2}{c}{0.312} & \multicolumn{2}{c}{0.074} & \multicolumn{2}{c}{48.97} \\
 & TF All & 0.148 & 0.163 & 0.368 & 0.376 & 0.512 & 0.519 & 0.308 & 0.313 & 0.085 & 0.082 & 42.62 & 45.83 \\
 & TF Top & 0.150 & \tbcolorg 0.179 & 0.373 &\tbcolorg  0.394 & 0.513 & 0.530 & 0.303 & 0.325 & 0.080 & \tbcolorg 0.092 & 42.84 & 41.54\\ 
 & DT & \tbcolorg 0.170 & 0.171 & \tbcolorg 0.380 & 0.392 &\tbcolorg  0.523 & \tbcolorg 0.531 & \tbcolorg 0.316 & \tbcolorg 0.331 & \tbcolorg 0.087 & 0.088 & 44.45 & 37.77\\ 
\midrule
\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\small TaskMaster-3}}} 
 & TF & \multicolumn{2}{c}{0.446} & \multicolumn{2}{c}{0.554} & \multicolumn{2}{c}{0.624} & \multicolumn{2}{c}{0.513} & \multicolumn{2}{c}{\tbcolorg 0.360} & \multicolumn{2}{c}{77.18} \\
 & TF All & 0.438 & 0.450 & 0.450 & 0.546 & 0.621 & 0.621 & 0.501 &  0.507  & 0.347  & 0.350 & 70.93 & 69.56\\
 & TF Top & 0.431 & 0.453 & 0.533 & 0.556 & 0.612 & 0.626 & 0.487 & 0.511  & 0.328 & 0.357 & 65.24 & 70.31\\ 
 & DT & 0.436 & \tbcolorg 0.460 & 0.548 & \tbcolorg 0.562 & 0.617 & \tbcolorg 0.630 & 0.498 &  \tbcolorg 0.514  & 0.342 & 0.359 & 69.00 & 74.67 \\ 
\midrule
\end{tabular}
}
\caption{Comparison across different methods on average metrics and dataset size with distilGPT2. 20\%, 80\% refer to percentage of the data used for fine-tuning offline RL methods. For consistency, BLEU scores are in [0, 1] unlike some papers converting them to [0, 100].}
\label{table:overall_metrics}
\end{table*}

% taskmaster3 w/ distilgpt2 results (20.0% data),
% +--------+------------+------------+--------------+--------+-------+------------+
% | method | bert_click | bert_score | bleurt_score | meteor |  bleu | perplexity |
% +--------+------------+------------+--------------+--------+-------+------------+
% |   TF   |   0.446    |   0.554    |    0.624     | 0.513  |  0.36 |   77.183   |
% | TF ALL |   0.438    |   0.546    |    0.621     | 0.501  | 0.347 |   70.933   |
% | TF TOP |   0.431    |   0.533    |    0.612     | 0.487  | 0.328 |   65.238   |
% |   DT   |   0.436    |   0.548    |    0.617     | 0.498  | 0.342 |   68.998   |
% +--------+------------+------------+--------------+--------+-------+------------+
% taskmaster3 w/ distilgpt2 results (80.0% data),
% +--------+------------+------------+--------------+--------+-------+------------+
% | method | bert_click | bert_score | bleurt_score | meteor |  bleu | perplexity |
% +--------+------------+------------+--------------+--------+-------+------------+
% |   TF   |   0.446    |   0.554    |    0.624     | 0.513  |  0.36 |   77.183   |
% | TF ALL |    0.45    |   0.546    |    0.621     | 0.507  |  0.35 |   69.555   |
% | TF TOP |   0.453    |   0.556    |    0.626     | 0.511  | 0.357 |   70.307   |
% |   DT   |    0.46    |   0.562    |     0.63     | 0.513  | 0.359 |   74.665   |
% +--------+------------+------------+--------------+--------+-------+------------+

% multi_woz w/ distilgpt2 results (20.0% data),
% +--------+------------+------------+--------------+--------+-------+------------+
% | method | bert_click | bert_score | bleurt_score | meteor |  bleu | perplexity |
% +--------+------------+------------+--------------+--------+-------+------------+
% |   TF   |    0.13    |   0.366    |    0.512     | 0.312  | 0.074 |   48.965   |
% | TF ALL |   0.148    |   0.368    |    0.512     | 0.308  | 0.085 |   42.616   |
% | TF TOP |    0.15    |   0.373    |    0.513     | 0.303  |  0.08 |   42.84    |
% |   DT   |    0.17    |    0.38    |    0.523     | 0.316  | 0.087 |   44.453   |
% +--------+------------+------------+--------------+--------+-------+------------+
% multi_woz w/ distilgpt2 results (80.0% data),
% +--------+------------+------------+--------------+--------+-------+------------+
% | method | bert_click | bert_score | bleurt_score | meteor |  bleu | perplexity |
% +--------+------------+------------+--------------+--------+-------+------------+
% |   TF   |    0.13    |   0.366    |    0.512     | 0.312  | 0.074 |   48.965   |
% | TF ALL |   0.163    |   0.376    |    0.519     | 0.313  | 0.082 |   45.825   |
% | TF TOP |   0.179    |   0.394    |     0.53     | 0.325  | 0.092 |   41.542   |
% |   DT   |   0.171    |   0.392    |    0.531     | 0.331  | 0.088 |   37.773   |
% +--------+------------+------------+--------------+--------+-------+------------+

% abcd w/ distilgpt2 results (20.0% data),
% +--------+------------+------------+--------------+--------+-------+------------+
% | method | bert_click | bert_score | bleurt_score | meteor |  bleu | perplexity |
% +--------+------------+------------+--------------+--------+-------+------------+
% |   TF   |   0.276    |   0.404    |    0.571     |  0.37  | 0.135 |   36.357   |
% | TF ALL |   0.269    |    0.39    |    0.559     | 0.365  | 0.134 |   41.758   |
% | TF TOP |   0.281    |   0.388    |    0.559     | 0.358  | 0.135 |   36.816   |
% |   DT   |   0.299    |   0.411    |    0.572     | 0.372  | 0.144 |   36.215   |
% +--------+------------+------------+--------------+--------+-------+------------+
% abcd w/ distilgpt2 results (80.0% data),
% +--------+------------+------------+--------------+--------+-------+------------+
% | method | bert_click | bert_score | bleurt_score | meteor |  bleu | perplexity |
% +--------+------------+------------+--------------+--------+-------+------------+
% |   TF   |   0.276    |   0.404    |    0.571     |  0.37  | 0.135 |   36.357   |
% | TF ALL |   0.285    |   0.399    |    0.564     | 0.375  | 0.143 |   45.393   |
% | TF TOP |   0.307    |    0.42    |    0.576     | 0.382  | 0.156 |   34.246   |
% |   DT   |   0.321    |   0.429    |    0.582     | 0.391  | 0.155 |   36.511   |
% +--------+------------+------------+--------------+--------+-------+------------+

%%%%%%%%%%%%%%%%%%%%%%%

% taskmaster3 w/ distilgpt2 results (100% data),
% +--------+------------+------------+--------------+--------+-------+
% | method | bert_click | bert_score | bleurt_score | meteor |  bleu |
% +--------+------------+------------+--------------+--------+-------+
% |   TF   |   0.446    |   0.554    |    0.624     | 0.513  |  0.36 |
% | TF All |   0.446    |   0.557    |    0.632     | 0.514  |  0.36 |
% | TF TOP |   0.462    |   0.561    |    0.629     | 0.518  |  0.36 |
% |   DT   |   0.465    |   0.563    |    0.633     | 0.521  | 0.364 |
% +--------+------------+------------+--------------+--------+-------+

%%%%%%%%%%%%%%%%%%%%%%%

We analyze the  results through a series of questions.

\subsubsection{Overall Performance Gains}

\textbf{Do offline RL methods improve on average over base teacher forcing model?}
Table~\ref{table:overall_metrics} presents average metrics for \textbf{TF}, \textbf{TF All}, \textbf{TF Top}, \textbf{DT} on all datasets. We see that on all datasets the offline RL methods improve the average reward (\textsc{BERTClick}) from $1.5 \%$ (TaskMaster) to $5\%$ (ABCD, MultiWoz). Offline RL methods also improve on other metrics not part of the reward, e.g. $2\%$ to $3\%$ on \textsc{METEOR} and $2\%$ (ABCD, MultiWoz) to $3\%$  on \textsc{BLEU} (ABCD). These improvements come without sacrificing perplexity (we compute perplexity with respect to off-the-shelf gpt2 model). Finally, we also note performance gains on TaskMaster are not as large as the other datasets. 

On most datasets and metrics, \textbf{DT} outperforms the other methods. The performance of \textbf{DT} over \textbf{TF Top} is consistent when fine-tuned on 20\% of the dataset vs 80\% (analyzed later in Fig.~\ref{fig:subsample_dt_vs_tf_top}). While Table~\ref{table:overall_metrics} shows only average metrics, we also look at the distribution over \textsc{BERTscore} in Fig.~\ref{fig:bert_score_hist}. We see that offline RL methods have a higher probability mass than TF on almost all \textsc{BERTScore} bins $\geq 0.6$. This is expected as $0.6$ is the threshold for \textsc{BERTClick} used as the reward function. The results show that improvements is not limited to any one bin, but across all bins.

% On a majority of datasets and bins, \textbf{DT} outperforms \textbf{TF Top}.

\textbf{How does performance vary across multiple responses?}
An argument in favor of the base \textbf{TF} model might be that it's unfair to evaluate it on a single response. After all, it optimizes for recall, so with multiple responses, it should be able to reach the performance of offline RL methods.  

Fig.~\ref{fig:topk_bert_click} shows average \textsc{BERTClick} of the best response selected from multiple responses. We see that offline RL methods maintain a persistent gap above \textbf{TF} model on all datasets. This likely indicates that they converge on a better distribution of responses over \textbf{TF}. \textbf{DT}, \textbf{TF Top} are similar for ABCD, TaskMaster, but \textbf{DT} outperforms on MultiWoz.

% \subsubsection{Which Buckets do the Performance Improvements come from?}

% Figure environment removed

% Figure environment removed

\subsubsection{Human Evaluation}
\textbf{How do improvements look qualitatively to human evaluators?}
Fig. \ref{fig:human_eval} presents a human evaluation on 100 examples for models fine-tuned on 80\% of the data. Human evaluators were presented with a context, true human response and 3 generated responses (for each method, which are randomized and anonymized). Humans provide two ratings (1-3) -- similarity and relevance. Similarity captures how similar the response is to the true human response. Relevance captures how relevant the response is given the context (even though it may not match the human response). More details on study guidelines in Appendix~\ref{appx:human_eval}.

\textbf{DT} responses are marked the most similar ($2.36$) compared to \textbf{TF Top} ($2.27$) and \textbf{TF} ($1.98$). Interestingly, all methods do better in relevance, \textit{i.e.} \textbf{TF} ($2.62$), \textbf{TF Top} ($2.78$) and \textbf{DT} ($2.85$). This indicates while \textbf{TF} may not be producing similar utterances to humans, it is still producing relevant utterances. Offline RL fine-tunes this to prefer responses that tend to be more similar to humans.

We pick two representative example conversations in Fig. \ref{fig:human_eval}. In the first, \textbf{DT} produces both relevant and similar responses. However, both \textbf{TF} and \textbf{TF Top} produce utterances that contradict facts in the conversation, e.g, asking for account ID even though the customer said they didn't have it. 

The second example shows a case where all three methods produce relevant responses, but \textbf{TF} produces a dissimilar response, e.g. offering an extension to the customer instead of collecting personal information. More qualitative examples in Appendix~\ref{appx:qual_results}.

% Figure environment removed

\textbf{How statistically significant are the improvements of TF Top and DT over TF?}
To measure statistical significance, we conduct a two sample test on the human evaluation study and provide p-values in Table~\ref{table:p_values}. While the number of examples is limited, we find improvements of both TF Top and DT over the base TF model to be statistically significant. 

% Table
\begin{table}[!h]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{ccccccc}
\toprule
Eval Metric & TF Top $>$ TF & DT $>$ TF \\ 
\midrule
Similarity p-value (paired t-test)   & 3.96e-03	& 2.35e-04 \\
Relevance p-value (paired t-test)  &   4.26e-02 & 4.29e-03 \\
BERTClick p-value (paired t-test)  &   7.83e-04 & 3.86e-06 \\
 \bottomrule
\end{tabular}
}
\caption{Statistical significance of human evaluation in Fig.~\ref{fig:human_eval}} \vspace{-1em}
\label{table:p_values}
\end{table}

\subsubsection{Comparison between RL methods}

\paragraph{How does ILQL critic perform as a ranker?}

Table~\ref{table:scorer_metrics} presents a comparison of all methods when ranking responses produced by the base \textbf{TF} model. \textbf{ILQL} has the largest \textsc{BERTClick} improvement of $3$\% on ABCD. It outperforms both \textbf{TF Top} (0.266) and \textbf{DT} (0.257) by a large margin. One reason for this is that  \textbf{ILQL} explicitly trains a critic $V(s)$ to approximate the optimal value.

% We further ran an ablation over \textbf{ILQL} by varying the sequence length. As sequence length is increased, \textbf{ILQL} performance improves, which is expected as more information is available to the scorer.

\begin{table}[!t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{ccccccc}
\toprule
 & Algorithm & \textsc{BERTClick} & \textsc{BERTScore} & \textsc{BLEURT} & \textsc{METEOR} & \textsc{BLEU} \\ 
&   & (reward) &  &  &  &  \\ 
\midrule
 \parbox[t]{2mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{ABCD}}} 
 & TF & 0.276           & 0.404           & 0.571              & 0.370               &  0.135 \\ 
   &\specialcell{DT \\ (Offline RL)}       &  \tbcolorg{0.314}           & \tbcolorg{0.425}         &  \tbcolorg{0.580}       & \tbcolorg{0.388}               & \tbcolorg{0.158} \\ 
 &\specialcell{PPO \\ (Online RL)}   &  0.274    & 0.407 & 0.578 & 0.377 & 0.143 \\
 \midrule
 \parbox[t]{2mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{\small MultiWoz 2.2}}} 
 & TF & 0.13 & 0.366 & 0.512 & 0.312 & 0.074 \\ 
   &\specialcell{DT \\ (Offline RL)}  & \tbcolorg{0.176} & \tbcolorg{0.394} & \tbcolorg{0.532} & \tbcolorg{0.334} & \tbcolorg{0.091} \\ 
 &\specialcell{PPO \\ (Online RL)}   & 0.147 & 0.364 & 0.516 & 0.320 & 0.079  \\
 \midrule
 \parbox[t]{2mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{\small TaskMaster-3}}} 
 & TF & 0.446 & 0.554 & 0.624 & 0.513 & 0.360 \\ 
   &\specialcell{DT \\ (Offline RL)}  & \tbcolorg{0.465} & \tbcolorg{0.563} & \tbcolorg{0.633} & \tbcolorg{0.521} & \tbcolorg{0.364} \\ 
 &\specialcell{PPO \\ (Online RL)}   & 0.452  & 0.561  & 0.625  & 0.510 & 0.360 \\
 \bottomrule
\end{tabular}
}
\caption{Comparison of offline RL (DT) against online RL (PPO. While \textbf{PPO} performs better than \textbf{TF}, it still performs worse than \textbf{DT} on all datasets.} \vspace{-1em}
\label{table:ppo_comparison}
\end{table}

\begin{table}[!t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{ccccccc}
\toprule
 & Algorithm & \textsc{BERTClick} & \textsc{BERTScore} & \textsc{BLEURT} & \textsc{METEOR} & \textsc{BLEU} \\ 
&   & (reward) &  &  &  &  \\ 
\midrule
 \parbox[t]{2mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{ABCD}}} 
 & TF       & 0.251     & 0.387 & 0.571 & 0.383 & 0.13\\ 
 & TF All      & 0.244     & 0.377 & 0.566 & 0.385 & 0.13 \\
 & TF Top   & 0.266    & 0.398 & \tbcolorg 0.572 & \tbcolorg 0.399 & 0.13 \\ 
 & DT       & 0.257     & 0.388 & 0.570 & 0.392 & 0.12 \\ 
 & ILQL     & \tbcolorg 0.285     & \tbcolorg 0.403 & 0.568 & 0.366 & \tbcolorg 0.14 \\
 \midrule
 \parbox[t]{2mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{Taskmaster-3}}} 
 & TF     &  0.388  &  0.49  & 0.584 & 0.485 & 0.296 \\ 
 & TF All & 0.377 & 0.477 & 0.58 & 0.478  & 0.277 \\ 
 & TF Top & 0.426  &  0.512  &  0.598  & 0.499 & 0.303 \\ 
 & DT    & \tbcolorg 0.442  &  0.512   &  \tbcolorg 0.597  & \tbcolorg 0.496 & 0.297  \\ 
 & ILQL   & 0.439  &  \tbcolorg 0.52  & 0.593  & 0.486 & \tbcolorg 0.306 \\ 
 \bottomrule
\end{tabular}
}
\caption{Comparison when ranking responses generated by the base TF model. Offline RL methods improve over logit scoring of base TF model, with \textbf{ILQL} being most effective as a ranker.} \vspace{-1em}
\label{table:scorer_metrics}
\end{table}

\textbf{How do offline RL compare with PPO?}
Table~\ref{table:ppo_comparison} presents a comparison of \textbf{PPO} against \textbf{DT} and \textbf{TF}. While \textbf{PPO} performs better than \textbf{TF}, it still performs worse than \textbf{DT} on all datasets. During training, \textbf{PPO} reward for the model over iterations appear unstable: 0.272 (epoch=1), 0.268 (epoch=3), 0.274 (epoch=5). \textbf{DT} on the other hand shows much more stable convergence. This is consistent with the discussion in \ref{sec:why_offline_rl} that for text generation, on-policy exploration can be challenging and requires significant KL regularization to the base \textbf{TF} policy. This KL regularization serves to limit the performance gains. We see the following trend for average reward: 0.259 (KL=0.1), 0.274 (KL=0.2), 0.279 (KL=0.4). For very high KL, performance falls back to the base TF reward of 0.276. \textbf{PPO} also has much longer training times  because of calls it has to make to the model’s generate function and \textsc{BERTScore} computation. \textbf{PPO} takes 1.95 hours / epoch, while \textbf{DT} takes 1.24 hours / epoch and \textbf{TF Top} takes 0.48 hours / epoch. 

\textbf{Can online data collection help DT?}
We compare with \textbf{Quark}~\cite{lu2022quark}, which can be viewed as an online counterpart to \textbf{DT}. It introduces an outer loop on \textbf{DT} by iteratively training a model, collecting data with the model and retraining. While this requires an extra outer loop for collecting data, this can certainly improve performance by collecting more positive examples on-policy as the policy improves. We implement \textbf{Quark} by creating an outer loop where at every epoch we collect new data with the current policy. We compare this to \textbf{DT} that holds the data fixed across epochs. 

% Figure environment removed

Fig.~\ref{fig:dt_vs_quark} shows \textsc{BERTClick} of \textbf{DT} and \textbf{Quark} over iterations fine-tuned on $20\%$ ABCD dataset. While performance is comparable in the initial epochs, the online data collection seems to help \textbf{Quark} outperform \textbf{DT} at the end of epoch 5. However, the performance boost with an additional online data collection step would vary with tasks depending on how good a coverage sampling from the base TF model has.

% For our setting, since sampling from the base TF model has sufficiently good coverage, the online data gathering step doesn't seem to provide a very significant performance boost. However, this would vary for different tasks.

% We will extend these results for all the datasets and include them in the paper. Overall, we place Quark under the class of online RL methods. While a detailed comparison of offline vs online RL is an important problem, this is beyond the scope of the current paper and exciting future work. For instance, there are a different set of tradeoffs that need to be considered and empirically studied, e.g. stability, sample efficiency, training time.



%% abcd on 80% data %%
\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{cccccccc}
\toprule
& & Algorithm & \textsc{BERTClick} & \textsc{BERTScore} & \textsc{BLEURT} & \textsc{METEOR} & \textsc{BLEU} \\ 
 & &  & (reward) &  &  &  \\ 
\midrule
\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{distilGPT2}}} &
\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\tiny ~\cite{sanh2019distilbert}}}}
 & TF   & 0.276    &   0.404    &   0.571     &  0.37  & 0.135  \\
 && TF All & 0.285    &   0.399    &    0.564    & 0.375  & 0.143 \\
 && TF Top & 0.307    &    0.42    &    0.576    & 0.382  & \tbcolorg 0.156 \\ 
 && DT    & \tbcolorg 0.321    &   \tbcolorg 0.429   &   \tbcolorg 0.582    & \tbcolorg 0.391  & 0.155 \\ 
 \midrule
 \parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{GPT2 Med}}} &
 \parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\tiny 
\scalebox{.9}{~\cite{radfordlanguage}}}}} 
 & TF  &  0.278   &   0.414    &   0.577   & 0.369  & 0.139 \\ 
 && TF All & 0.309    &   0.422   &    0.581  & 0.39  & 0.157 \\ 
 && TF Top &  0.331   &  0.444    &    0.596  & \tbcolorg 0.407  & 0.162  \\ 
 && DT  & \tbcolorg 0.334   &   \tbcolorg 0.446    &   \tbcolorg 0.597  & 0.406  & \tbcolorg 0.163  \\
 % \midrule
 % \parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{GPT2 Large}}} 
 % & TF  & 0.29    &   0.427  &   0.582   & 0.385  &  0.158  &   34.478  \\ 
 % & TF All & 0.287  &  0.42  &  0.576   & 0.386  & 0.155 &  39.432  \\ 
 % & TF Top & & & & & &\\ 
 % & DT & & & & & &\\ 
 \bottomrule
\end{tabular}
}
\caption{Comparison across different model sizes. Improvements are continually sustained as we go to a larger model size.} \vspace{-1.em}
\label{table:model_sizes}
\end{table}

\subsubsection{Ablations and Analysis}

\paragraph{How do offline RL improvements vary with model size?}

As we increase the model size from distilGPT2 to GPT2 Med, we see performance of all methods improves. However, offline RL methods persistently maintain a $5$\% performance gain over \textbf{TF} across sizes. This indicates that offline RL performance gains come from the way the model is trained rather than simply having a larger model capacity. 
 
\paragraph{How does performance vary with offline RL data size?}
\label{sec:subsample_dt_vs_tf_top}
Fig. \ref{fig:subsample_dt_vs_tf_top} shows how performance of offline RL varies with increasing data. \textbf{DT} has an edge at low data size, but as data size increases \textbf{TF Top} and \textbf{DT} merge. This backs our understanding from the theory behind \textbf{DT} and \textbf{TF Top}, where \textbf{TF Top} throws away data while \textbf{DT} retains it. This advantage goes away with increasing data size. It's important to note that for fine-tuning, we will often be in the low data regime and hence \textbf{DT} is favourable from that regard. 

% Figure environment removed

%\subsubsection{How does training time vary across methods?}

\paragraph{How does TF Top performance vary with different top quantiles?}

We conducted an ablation experiment where we trained both \textbf{DT} and \textbf{TF Top} with varying \textsc{BERTClick} thresholds. Fig. \ref{fig:ablation_tf_top_thresh_and_cql_reg} shows the average \textsc{BERTScore} of the greedy response.

As we increase the quantile threshold, we see the \textbf{TF Top} performance increase, reach a peak and then drop. On one extreme, setting the threshold to be $0$ implies that we are training \textbf{TF Top} on all the data. This is suboptimal as \textbf{TF Top} trains on all of it's own responses and fails to tell the difference between good and bad responses. On the other extreme, setting the threshold to be $1$ implies that we are training \textbf{TF Top} on only the human response, which has similar performance to \textbf{TF}. 

\paragraph{How does ILQL performance change with varying regularization?}

As we increase regularization  $\alpha$, \textbf{ILQL} performance improves as it forces the critic to stay close to the data. Increasing $\alpha$ further ($>0.05$ in Fig. \ref{fig:ablation_tf_top_thresh_and_cql_reg}(b)) hurts performance as the regularization dominates other losses.  

% Figure environment removed

\paragraph{How does offline RL compare with TF on dialogue metrics?}
We analyze how various methods perform on dialogue metrics, i.e., metrics looking at whether the generated response results in a correct slot prediction. We chose a state-of-the-art approach~\cite{lee2021dialogue} to train a T5 dialogue state tracking (DST) model on MultiWoz to extract slots from generated responses. 

In MultiWoz, We took generated responses from the offline RL method and replaced “SYSTEM” utterances with the generated responses (keeping “USER” utterances the same). We then feed these to the DST model and compute different dialogue-level joint accuracy metrics: 'joint\_goal\_accuracy', 'joint\_cat\_accuracy', 'joint\_noncat\_accuracy’ in Table~\ref{table:dialogue_metrics}. 

\begin{table}[!h]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{ccccccc}
\toprule
 & \textbf{joint\_goal\_accuracy} & \textbf{joint\_cat\_accuracy} & \textbf{joint\_noncat\_accuracy} \\ 
\midrule
\textbf{Groundtruth}  &   0.565	 & 0.712 &  0.766\\
\textbf{TF Top}    & 0.474	 & 0.689 & 0.629\\
\textbf{TF} & 0.458  & 0.679 & 0.613\\
 \bottomrule
\end{tabular}
}
\caption{Dialogue metrics on MultiWoz dataset} \vspace{-1.em}
\label{table:dialogue_metrics}
\end{table}

Overall, we find that both \textbf{TF Top} and \textbf{TF} do worse than the ground truth, as expected. Ground truth utterances have access to privileged information which in turn defines the ground truth slots. For instance, a specific restaurant name that neither of the generated utterances would be able to predict ahead of time. 
Interestingly, we see \textbf{TF Top} score higher than \textbf{TF} on the slot metrics even though such metrics do not appear in the rewards. When looking at the utterances, we observe that TF makes mistakes by either making up new information or repeating information from the context (similar to the qualitative / human study examples in the paper). However, for offline RL methods to truly do better on these metrics, they must be trained on rewards that capture such dialogue-level metrics. This is an interesting direction for future work. 


% However, since slots are defined for specific key-value pairs that aren't seen by the reward model, it doesn't optimize for it. optimizing dialogue future work.
% The hallucinated hotel name in the TF response results in an incorrect slot value of "hotel-name": [ "alexander bed and breakfast" ]. We will extend this analysis for more examples and add them to the paper.
% However, since slots are defined for specific key-value pairs that aren't seen by the reward model, it doesn't optimize for it. optimizing dialogue future work.

% \subsubsection{How does ILQL performance compare with TF Top?}

% We train \textbf{ILQL} with different regularization parameters $\alpha$ and compared to \textbf{TF-Top} with varying percentile of top rewards.

% As we increase $\alpha$, \textbf{ILQL} approaches the performance of \textbf{TF}, which is similar to the trend seen with \textbf{TF-Top}.

% \begin{itemize}\itemsep0em 
%     \item Train ILQL for different regularization parameters $\alpha$. Compare to TF-Top model with varying percentile of top rewards. Is increasing $\alpha$ similar to increasing $\delta$ (TF in the limit of large $\alpha$ and infinite data)?
% \end{itemize}

% \begin{itemize}\itemsep0em 
%     \item Train a DT model on quantized rewards. Train a TF-Top model on varying \% of top rewards. Compare both. 
%     \item How does the performance change as you vary \% for TF Top? In the limit it goes from TF to training only on the true responses? Tradeoff between $\delta$ and ``coverage''?
% \end{itemize}