% \section*{Checklist}

% Have you read the publication ethics guidelines and ensured that your paper conforms to them?
% Did you discuss any potential negative societal impacts (for example, disinformation, privacy, fairness) of your work?
% For example, these could include if you see a direct path to negative applications, some stakeholders may be impacted negatively even if others are benefited, consider possible harms through intended use or misuse. 
% You may also find this unofficial guidance and other resources at the broader impacts workshop at NeurIPS 2020 helpful.
% If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
% If your work uses existing assets, did you cite the creators and the version?
% Did you mention the license of the assets?
% If you scraped data from a particular source (e.g., Twitter), you should state the copyright and terms of service of that source.
% If you are releasing assets, you should include a license, copyright information, and terms of use in the package.
% If you are using an existing dataset, check paperswithcode.com/datasets, which has curated licenses for some datasets. Use their licensing guide to determine the license of a dataset.
% If you are repackaging an existing dataset, you should state the original license as well as the one for the derived asset (if it has changed).
% Did you include any new assets either in the supplemental material or as a URL?
% For initial review, anonymize your assets. You can either create an anonymized URL or include an anonymized zip file.
% If you cannot release (e.g., the asset contains proprietary info), state why.
% Where possible, discuss whether and how consent was obtained from people whose data you're using/curating 
% Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
% This question is to encourage discussion of potentially undesirable properties and the goals of the research.
% Offensive material (as data) in your manuscript may target specific vulnerable populations, and a reviewer may be a member of that population. Please refer to the Code of Conduct when selecting materials for inclusion in the manuscript to avoid this type of targeting.
% If you used crowdsourcing or conducted research with human subjects...
% Did you include the instructions given to participants including the consent form?
% Did you describe any potential participant risks?
% Examples of risks include a crowdsourcing experiment that might show offensive content or collect personal identifying information (PII).
% If you obtained IRB approval, you should clearly state this in the paper. For initial submissions, do not include information that would break anonymity.
% Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
% Next, we review best practices for paper writing and structure…

% Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
% Did you describe the limitations of your work?
% For more guidance, please refer to this 2022 FAccT paper by Smith, et al.
% If you are including theoretical results...
% Did you state the full set of assumptions of all theoretical results?
% Did you include complete proofs of all theoretical results?
% If you ran experiments...
% Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL, in an anonymized way at the review time)?
% Please include a README about how to reproduce the results in the paper using the provided code. 
% While we encourage release of code and data, we understand that this might not be possible, so "no because the code is proprietary" is acceptable. 
% Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
% Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
% Did you include the amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?

% \newpage


\section{Limitations}
In this paper, we study small and medium size models due to the limited computation resources. It is possible that our findings do not generalize to large scale models with billions of parameters. We plan to extend our study to large language models in a future work.

\section{Potential Negative Social Impacts}
It is possible that dialogue language models are used to generate malicious text or produce harmful conversations with humans if they are trained on biased data. Directly applying a language model to a product without any safeguard is risky. The models trained and released with this paper should not be used in any products.

\section{Human Evaluation Details}
\label{appx:human_eval}

We gather evaluations from humans to assess the quality of response utterances generated given a conversational context. We use two measures to evaluate the generated utterances: (a) how similar they are to the actual response, and (b) how relevant they are to the context. We obtain annotations from 5 different annotators on 100 examples from ABCD dataset, each annotator evaluating 20 examples. We provided the following guidelines to the annotators:

\subsection{Similarity to True Response?}

On a scale of 1 to 3 (1=not similar, 3=similar), how similar is the generated response to the true response?

\subsubsection*{Scale}

\textbf{1 = not similar}

Not similar at all or even opposite in meaning. This can even include sentences that have a lot of string overlap, e.g., "I booked that for you", "I didn’t book that for you".

\textbf{2 = somewhat similar}

Overlap in meaning, but some errors or missing / added information. E.g., ‘You need to bring your passport’ and ‘You need to bring your passport and vaccination record’ or ‘You need to bring your passport’ and ‘You need to bring your identity card’

\textbf{3 = similar}

Essentially the same meaning. A human reading the two responses would come to basically the same conclusion about the agent state.

\subsubsection*{Examples}

\textbf{1 = not similar}

\textit{True Response:} Hello, the annual sale began on January 23rd and ended on January 31st.

\textit{Generated Response:} Okay, let me look into that for you.

\textbf{3 = similar}

\textit{True Response:} I would be happy to find the answer for you.

\textit{Generated Response:} I would be happy to look into that for you.

\subsection{Relevance to Context?}
On a scale of 1 to 3 (1=not relevant, 3=relevant), how relevant is the generated response given the conversation context?

\subsubsection*{Scale}
\textbf{1 = not relevant}

Has nothing or very little to do with the conversation context.

\textbf{2 = somewhat relevant}

Is an OK response to the conversation context, though maybe missing some details or superfluous in some respects.

\textbf{3 = relevant}

A good response. If a customer saw this, they could believe a human wrote this. Note that this can include specific utterances like ‘OK, I booked that flight for you’ to generic ‘you’re welcome’, ‘please wait one moment’ if they make sense in that context.

\subsubsection*{Examples}
\textbf{1 = not relevant}

\textit{Conversation Context:} [REP] Hi, how may I help you this morning? 
[CUS] Yea, I had a quick question. I was checking my email and it says my subscription was removed. Is that true?  I still want it there.
[REP] Sure, I can check that for you. What is your account ID?
[CUS] Umm, not sure.

\textit{Generated Response:} What is the shipping status of the order?

\textbf{3 = relevant}

\textit{Conversation Context:} [REP] Hi, how may I help you this morning? 
[CUS] Yea, I had a quick question. I was checking my email and it says my subscription was removed. Is that true?  I still want it there.
[REP] Sure, I can check that for you. What is your account ID?
[CUS] Umm, not sure

\textit{Generated Response:} OK, and to whom do I have the pleasure of speaking with?

\section{Experimental Details}
\label{appx:experiment_details}

\textbf{Training Details}

\textbf{Stage 1. Train base TF model} We train / finetune a distilGPT2 / GPT2 medium model on all conversations for each dataset ABCD, MultiWoz, TaskMaster (separate model for each dataset) for 10 epochs. We call this the base teacher forcing (TF) model. We conducted grid search for the learning rate in \{1e-3, 5e-4, 2e-4, 1e-4, 5e-5, 2e-5, 1e-5\} and number of epochs in \{10, 20, 40\}.

\textbf{Stage 2. Generate Offline RL data} Given a TF model, we call it to generate an offline RL dataset. Each data point in the dataset is a tuple (context, response, reward). We obtain the reward by evaluating generated response against true response using thresholded BERTScore that we call BERTClick. For each context in the dataset, we include 1 true response + 5 model generated response.

\textbf{Stage 3. Train offline RL model} Finally, we fine tune the base TF model on the offline RL dataset to get models for each of the three offline RL methods TF Top, DT, ILQL. We implement TF, TF Top using the huggingface transformers library \cite{wolf2019huggingface} and ILQL using the trlx library \footnote{\url{https://github.com/CarperAI/trlx}}. We fine tune for 5 epochs. We disable gradients on context tokens during this stage of training so as to better match the inference time setup. Moreover, the same context repeats multiple times that would bias the gradients.

Training is done on an AWS EC2 g5.12xlarge instance which has 4 Nvidia A10G GPUs. For both Stage 1, 3 we pick the best epoch checkpoint based on the validation loss. We use the same set of hyper parameters across all datasets. More hyperparameter details in Tables \ref{tab:hyperparams_tf_dt}, \ref{tab:hyperparams_ilql} and \ref{tab:hyperparams_ppo}. On abcd dataset with 10k conversations using distilgpt2 model, training base model takes $\approx$20 mins for 10 epochs. Training offline RL model with 70\% subsamples takes, TF TOP: $\approx$2.5 hours for 5 epochs, DT: $\approx$6 hours for 5 epochs.

\begin{table}[!t]
\centering
\resizebox{0.8\columnwidth}{!}{
\begin{tabular}{llll}
\toprule
\textbf{Hyperparameters} & \textbf{TF} & \textbf{TF Top} & \textbf{DT} \\
\midrule
Model & DistilGPT / GPT2 Medium & DistilGPT / GPT2 Medium & DistilGPT / GPT2 Medium \\
Batch size & 16 / 32 & 32 / 64 & 32 / 64 \\
Block size & 1024 / 1024 & 512 / 512 & 512 / 512 \\
Max number of epochs & 10 & 5 & 5 \\
Optimizer & Adam & Adam & Adam \\
Learning rate &  1e-4 & 5e-5 & 5e-5 \\
Adam $(\beta_1, \beta_2)$ & (0.9, 0.999) & (0.9, 0.999)& (0.9, 0.999) \\
Adam $\epsilon$ & 1e-8 & 1e-8 & 1e-8 \\
Learning rate scheduler & Cosine decay & Cosine decay & Cosine decay \\
% Learning rate warmup steps & 0 \\
\bottomrule
\end{tabular}
}
\caption{TF, TF Top, DT training hyperparameters for ABCD, MultiWoz 2.2, TaskMaster-3 datasets. We tune the learning rate in \{5e-4, 1e-4, 5e-5, 1e-5\}.}
\label{tab:hyperparams_tf_dt}
\end{table}

\begin{table}[!t]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Hyperparameters} & \textbf{ILQL} \\
\midrule
Model & DistilGPT\\
Batch size & 16 \\
Block size & 128 \\
Max number of iterations & 50000 \\
Optimizer & Adam \\
Learning rate &  1e-4 \\
Adam $(\beta_1, \beta_2)$ & (0.9, 0.95)\\
Adam $\epsilon$ & 1e-8 \\
Learning rate scheduler & Cosine \\
CQL Scale & 0.05 \\
$\tau$ & 0.7 \\
$\gamma$ & 0.99 \\
\bottomrule
\end{tabular}
\caption{ILQL Training hyperparameters for ABCD, MultiWoz 2.2, TaskMaster-3 datasets. We tune the learning rate in \{5e-4, 1e-4, 1e-5\} and CQL scale (regularization against base TF logits) in \{0.001, 0.005, 0.05, 0.5, 5\}}
\label{tab:hyperparams_ilql}
\end{table}

\begin{table}[!t]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Hyperparameters} & \textbf{PPO} \\
\midrule
Model & DistilGPT\\
Batch size & 16 \\
Block size & 128 \\
Max number of iterations & 50000 \\
Optimizer & AdamW \\
Learning rate &  5e-7 \\
Adam $(\beta_1, \beta_2)$ & (0.9, 0.95)\\
Adam $\epsilon$ & 1e-8 \\
Weight decay & 1e-6 \\
Learning rate scheduler & Constant \\
PPO value coefficient & 2.3 \\
PPO KL initial coefficient & 0.2 \\
% Learning rate warmup steps & 0 \\
\bottomrule
\end{tabular}
\caption{PPO Training hyperparameters for ABCD, MultiWoz 2.2, TaskMaster 3 datasets. We tune the learning rate in \{1e-4, 5e-5, 2e-5, 1e-5, 5e-6, 2e-6, 1e-6, 5e-7, 2e-7, 1e-7\}, PPO value coefficient in \{1, 2, 2.3, 3\}, and PPO KL initial coefficient in \{0.2, 1, 2\}.}
\label{tab:hyperparams_ppo}
\end{table}

\section{Qualitative Results}
\label{appx:qual_results}

We show qualitative predictions for the different methods (TF, TF Top, DT) across the ABCD, MultiWoz 2.2 and Taskmaster-3 datasets. It is worth noting that while the responses generated by TF may appear relevant to the context, they often do not match the true human utterance and typically repeat information already in context. Offline RL approaches address these limitations in the TF responses. This is consistent with what we observed in the human evaluation study where the difference between \textit{similarity} annotations was greater than the difference in \textit{relevance} annotations across the three methods.

% Figure environment removed


% Figure environment removed

% Figure environment removed

\clearpage
\section{Qualitative Results on Dialogue Metrics}

We also explored how \textbf{TF} and \textbf{TF-Top} perform against ground truth on dialogue metrics. The goal was to see whether these methods generate text that generates correct slot information. To do so, we trained a T5 dialogue state tracking (DST) model on MultiWoz. We then took generated responses and fed them to the DST model to compute slots and matched them with ground truth slots. Overall, we find that both \textbf{TF Top} and \textbf{TF} do worse than the ground truth, as expected. Ground truth utterances have access to privileged information which in turn defines the ground truth slots. For instance, a specific restaurant name that neither of the generated utterances would be able to predict ahead of time. 


We note that we are not directly optimizing dialgoue metrics, so there is no guarantee that these metrics would improve with offline RL. However, we do observe that performance is not negatively affected. In fact, we see that \textbf{TF Top} scores higher than \textbf{TF} on the slot metrics. One such example is shown below. The context here is the user is asking the system to book a reservation in Zizzi Cambridge. The hallucinated hotel name in the TF response results in an incorrect slot value of "hotel-name": [ "alexander bed and breakfast" ]. 

% Figure environment removed

\section{Additional Quantitative Results}

We also provide distribution over \textsc{BERTScores} for methods trained on 20\% of the data below. We find similar trends when models are trained on $80\%$ of the data, with slightly lower performance gains.

% Figure environment removed