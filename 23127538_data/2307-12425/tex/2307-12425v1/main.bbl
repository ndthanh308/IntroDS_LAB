\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Banerjee \& Lavie(2005)Banerjee and Lavie]{banerjee2005meteor}
Banerjee, S. and Lavie, A.
\newblock {METEOR}: An automatic metric for {MT} evaluation with improved
  correlation with human judgments.
\newblock In \emph{Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic
  Evaluation Measures for Machine Translation and/or Summarization}, pp.\
  65--72, Ann Arbor, Michigan, June 2005. Association for Computational
  Linguistics.
\newblock URL \url{https://aclanthology.org/W05-0909}.

\bibitem[Bang et~al.(2023)Bang, Lee, and Koo]{bang2023task}
Bang, N., Lee, J., and Koo, M.-W.
\newblock Task-optimized adapters for an end-to-end task-oriented dialogue
  system.
\newblock \emph{arXiv preprint arXiv:2305.02468}, 2023.

\bibitem[Brandfonbrener et~al.(2022)Brandfonbrener, Bietti, Buckman, Laroche,
  and Bruna]{brandfonbrener2022does}
Brandfonbrener, D., Bietti, A., Buckman, J., Laroche, R., and Bruna, J.
\newblock When does return-conditioned supervised learning work for offline
  reinforcement learning?
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2022.

\bibitem[Byrne et~al.(2019)Byrne, Krishnamoorthi, Sankar, Neelakantan,
  Duckworth, Yavuz, Goodrich, Dubey, Cedilnik, and Kim]{byrne2019taskmaster}
Byrne, B., Krishnamoorthi, K., Sankar, C., Neelakantan, A., Duckworth, D.,
  Yavuz, S., Goodrich, B., Dubey, A., Cedilnik, A., and Kim, K.-Y.
\newblock Taskmaster-1: Toward a realistic and diverse dialog dataset.
\newblock In \emph{Empirical Methods in Natural Language Processing (EMNLP)},
  2019.

\bibitem[Chen et~al.(2021{\natexlab{a}})Chen, Chen, Yang, Lin, and
  Yu]{chen2021action}
Chen, D., Chen, H., Yang, Y., Lin, A., and Yu, Z.
\newblock Action-based conversations dataset: A corpus for building more
  in-depth task-oriented dialogue systems.
\newblock In \emph{Proc. North American Chapter of the Association for
  Computational Linguistics (NAACL)}, pp.\  3002--3017, 2021{\natexlab{a}}.

\bibitem[Chen et~al.(2021{\natexlab{b}})Chen, Lu, Rajeswaran, Lee, Grover,
  Laskin, Abbeel, Srinivas, and Mordatch]{chen2021decision}
Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P.,
  Srinivas, A., and Mordatch, I.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021{\natexlab{b}}.

\bibitem[Furman et~al.(2022)Furman, Toledo, Shock, and Buys]{furman2022qait}
Furman, G., Toledo, E., Shock, J., and Buys, J.
\newblock A sequence modelling approach to question answering in text-based
  games.
\newblock In \emph{ACL Wordplay Workshop}, 2022.

\bibitem[Jaques et~al.(2019)Jaques, Ghandeharioun, Shen, Ferguson, Lapedriza,
  Jones, Gu, and Picard]{jaques2019wayoff}
Jaques, N., Ghandeharioun, A., Shen, J.~H., Ferguson, C., Lapedriza, A., Jones,
  N., Gu, S., and Picard, R.
\newblock Way off-policy batch deep reinforcement learning of implicit human
  preferences in dialog.
\newblock \emph{arXiv preprint arXiv:1907.00456}, 2019.

\bibitem[Jaques et~al.(2020)Jaques, Shen, Ghandeharioun, Ferguson, Lapedriza,
  Jones, Gu, and Picard]{jaques2020human}
Jaques, N., Shen, J.~H., Ghandeharioun, A., Ferguson, C., Lapedriza, A., Jones,
  N., Gu, S.~S., and Picard, R.
\newblock Human-centric dialog training via offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2010.05848}, 2020.

\bibitem[Kiegeland \& Kreutzer(2021)Kiegeland and
  Kreutzer]{kiegeland2021revisiting}
Kiegeland, S. and Kreutzer, J.
\newblock Revisiting the weaknesses of reinforcement learning for neural
  machine translation.
\newblock \emph{arXiv preprint arXiv:2106.08942}, 2021.

\bibitem[Kostrikov et~al.(2021)Kostrikov, Nair, and
  Levine]{kostrikov2021offline}
Kostrikov, I., Nair, A., and Levine, S.
\newblock Offline reinforcement learning with implicit q-learning.
\newblock \emph{arXiv preprint arXiv:2110.06169}, 2021.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
Kumar, A., Zhou, A., Tucker, G., and Levine, S.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1179--1191, 2020.

\bibitem[Lee et~al.(2021)Lee, Cheng, and Ostendorf]{lee2021dialogue}
Lee, C.-H., Cheng, H., and Ostendorf, M.
\newblock Dialogue state tracking with a language model using schema-driven
  prompting.
\newblock \emph{arXiv preprint arXiv:2109.07506}, 2021.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Levine, S., Kumar, A., Tucker, G., and Fu, J.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Li et~al.(2016)Li, Monroe, Ritter, Galley, Gao, and
  Jurafsky]{li2016deep}
Li, J., Monroe, W., Ritter, A., Galley, M., Gao, J., and Jurafsky, D.
\newblock Deep reinforcement learning for dialogue generation.
\newblock \emph{arXiv preprint arXiv:1606.01541}, 2016.

\bibitem[Liu et~al.(2016)Liu, Lowe, Serban, Noseworthy, Charlin, and
  Pineau]{liu2016not}
Liu, C.-W., Lowe, R., Serban, I.~V., Noseworthy, M., Charlin, L., and Pineau,
  J.
\newblock How not to evaluate your dialogue system: An empirical study of
  unsupervised evaluation metrics for dialogue response generation.
\newblock \emph{arXiv preprint arXiv:1603.08023}, 2016.

\bibitem[Lu et~al.(2022)Lu, Welleck, Jiang, Hessel, Qin, West, Ammanabrolu, and
  Choi]{lu2022quark}
Lu, X., Welleck, S., Jiang, L., Hessel, J., Qin, L., West, P., Ammanabrolu, P.,
  and Choi, Y.
\newblock Quark: Controllable text generation with reinforced unlearning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2022.

\bibitem[Misra \& Artzi(2015)Misra and Artzi]{misra2015reinforcement}
Misra, D. and Artzi, Y.
\newblock Reinforcement learning for mapping instructions to actions with
  reward learning.
\newblock 2015.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.~L., Mishkin, P.,
  Zhang, C., Agarwal, S., Slama, K., Ray, A., et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{arXiv preprint arXiv:2203.02155}, 2022.

\bibitem[Pang \& He(2020)Pang and He]{pang2020text}
Pang, R.~Y. and He, H.
\newblock Text generation by learning from demonstrations.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Papineni et~al.(2002)Papineni, Roukos, Ward, and
  Zhu]{papineni2002bleu}
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
\newblock Bleu: a method for automatic evaluation of machine translation.
\newblock In \emph{Proceedings of the 40th annual meeting of the Association
  for Computational Linguistics}, pp.\  311--318, 2002.

\bibitem[Pasunuru \& Bansal(2018)Pasunuru and Bansal]{pasunuru2018multi}
Pasunuru, R. and Bansal, M.
\newblock Multi-reward reinforced summarization with saliency and entailment.
\newblock \emph{arXiv preprint arXiv:1804.06451}, 2018.

\bibitem[Paulus et~al.(2017)Paulus, Xiong, and Socher]{paulus2017deep}
Paulus, R., Xiong, C., and Socher, R.
\newblock A deep reinforced model for abstractive summarization.
\newblock \emph{arXiv preprint arXiv:1705.04304}, 2017.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radfordlanguage}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem[Ramakrishnan et~al.(2022)Ramakrishnan, Narangodage, Schilman,
  Weinberger, and McDonald]{ramakrishnan2022long}
Ramakrishnan, R., Narangodage, H.~B., Schilman, M., Weinberger, K.~Q., and
  McDonald, R.
\newblock Long-term control for dialogue generation: Methods and evaluation.
\newblock In \emph{Proc. North American Chapter of the Association for
  Computational Linguistics (NAACL)}, 2022.

\bibitem[Ramamurthy et~al.(2022)Ramamurthy, Ammanabrolu, Brantley, Hessel,
  Sifa, Bauckhage, Hajishirzi, and Choi]{ramamurthy2022reinforcement}
Ramamurthy, R., Ammanabrolu, P., Brantley, K., Hessel, J., Sifa, R., Bauckhage,
  C., Hajishirzi, H., and Choi, Y.
\newblock Is reinforcement learning (not) for natural language processing?:
  Benchmarks, baselines, and building blocks for natural language policy
  optimization.
\newblock \emph{arXiv preprint arXiv:2210.01241}, 2022.

\bibitem[Ranzato et~al.(2015)Ranzato, Chopra, Auli, and
  Zaremba]{ranzato2015sequence}
Ranzato, M., Chopra, S., Auli, M., and Zaremba, W.
\newblock Sequence level training with recurrent neural networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2015.

\bibitem[Sanh et~al.(2019)Sanh, Debut, Chaumond, and Wolf]{sanh2019distilbert}
Sanh, V., Debut, L., Chaumond, J., and Wolf, T.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock In \emph{NeurIPS EMC\^2 Workshop}, 2019.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Sellam et~al.(2020)Sellam, Das, and Parikh]{sellam2020bleurt}
Sellam, T., Das, D., and Parikh, A.~P.
\newblock Bleurt: Learning robust metrics for text generation.
\newblock \emph{arXiv preprint arXiv:2004.04696}, 2020.

\bibitem[Snell et~al.(2022)Snell, Kostrikov, Su, Yang, and
  Levine]{snell2022ilql}
Snell, C., Kostrikov, I., Su, Y., Yang, M., and Levine, S.
\newblock Offline {RL} for natural language generation with implicit language
  {Q} learning.
\newblock \emph{arXiv preprint arXiv:2206.11871}, 2022.

\bibitem[Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss,
  Radford, Amodei, and Christiano]{stiennon2020learning}
Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A.,
  Amodei, D., and Christiano, P.~F.
\newblock Learning to summarize with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 3008--3021, 2020.

\bibitem[Swanson et~al.(2019)Swanson, Yu, Fox, Wohlwend, and
  Lei]{swanson2019building}
Swanson, K., Yu, L., Fox, C., Wohlwend, J., and Lei, T.
\newblock Building a production model for retrieval-based chatbots.
\newblock \emph{arXiv preprint arXiv:1906.03209}, 2019.

\bibitem[Tian et~al.(2021)Tian, Huang, Lin, Bao, He, Yang, Wu, Wang, and
  Sun]{tian2021amendable}
Tian, X., Huang, L., Lin, Y., Bao, S., He, H., Yang, Y., Wu, H., Wang, F., and
  Sun, S.
\newblock Amendable generation for dialogue state tracking.
\newblock \emph{arXiv preprint arXiv:2110.15659}, 2021.

\bibitem[Verma et~al.(2022)Verma, Fu, Yang, and Levine]{verma2022chai}
Verma, S., Fu, J., Yang, M., and Levine, S.
\newblock Chai: A chatbot ai for task-oriented dialogue with offline
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2204.08426}, 2022.

\bibitem[Watkins \& Dayan(1992)Watkins and Dayan]{watkins1992q}
Watkins, C.~J. and Dayan, P.
\newblock Q-learning.
\newblock \emph{Machine learning}, 8\penalty0 (3):\penalty0 279--292, 1992.

\bibitem[Williams(1992)]{williams1992simple}
Williams, R.~J.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 8\penalty0 (3):\penalty0 229--256, 1992.

\bibitem[Williams \& Zipser(1989)Williams and Zipser]{williams1989learning}
Williams, R.~J. and Zipser, D.
\newblock A learning algorithm for continually running fully recurrent neural
  networks.
\newblock \emph{Neural computation}, 1\penalty0 (2):\penalty0 270--280, 1989.

\bibitem[Wolf et~al.(2019)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, et~al.]{wolf2019huggingface}
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P.,
  Rault, T., Louf, R., Funtowicz, M., et~al.
\newblock Huggingface's transformers: State-of-the-art natural language
  processing.
\newblock \emph{arXiv preprint arXiv:1910.03771}, 2019.

\bibitem[Wu et~al.(2022)Wu, Brantley, Kojima, and Artzi]{wu2022lilgym}
Wu, A., Brantley, K., Kojima, N., and Artzi, Y.
\newblock lilgym: Natural language visual reasoning with reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2211.01994}, 2022.

\bibitem[Wu et~al.(2018)Wu, Xia, Tian, Zhao, Qin, Lai, and
  Liu]{wu2018adversarial}
Wu, L., Xia, Y., Tian, F., Zhao, L., Qin, T., Lai, J., and Liu, T.-Y.
\newblock Adversarial neural machine translation.
\newblock In \emph{Asian Conference on Machine Learning}, pp.\  534--549. PMLR,
  2018.

\bibitem[Yang et~al.(2020)Yang, Chen, and Narasimhan]{yang2020improving}
Yang, R., Chen, J., and Narasimhan, K.
\newblock Improving dialog systems for negotiation with personality modeling.
\newblock \emph{arXiv preprint arXiv:2010.09954}, 2020.

\bibitem[Yonghui et~al.(2016)Yonghui, Schuster, Chen, Le, Norouzi, Macherey,
  Krikun, Cao, Gao, Macherey, et~al.]{yonghui2016bridging}
Yonghui, W., Schuster, M., Chen, Z., Le, Q.~V., Norouzi, M., Macherey, W.,
  Krikun, M., Cao, Y., Gao, Q., Macherey, K., et~al.
\newblock Bridging the gap between human and machine translation.
\newblock \emph{arXiv preprint arXiv:1609.08144}, 2016.

\bibitem[Zang et~al.(2020)Zang, Rastogi, Sunkara, Gupta, Zhang, and
  Chen]{zang2020multiwoz}
Zang, X., Rastogi, A., Sunkara, S., Gupta, R., Zhang, J., and Chen, J.
\newblock {M}ulti{WOZ} 2.2 : A dialogue dataset with additional annotation
  corrections and state tracking baselines.
\newblock In \emph{Proceedings of the 2nd Workshop on Natural Language
  Processing for Conversational AI}, 2020.
\newblock URL \url{https://aclanthology.org/2020.nlp4convai-1.13}.

\bibitem[Zhang et~al.(2019)Zhang, Kishore, Wu, Weinberger, and
  Artzi]{zhang2019bertscore}
Zhang, T., Kishore, V., Wu, F., Weinberger, K.~Q., and Artzi, Y.
\newblock {BERT}score: Evaluating text generation with {BERT}.
\newblock \emph{arXiv preprint arXiv:1904.09675}, 2019.

\bibitem[Zhou et~al.(2017)Zhou, Small, Rokhlenko, and Elkan]{zhou2017end}
Zhou, L., Small, K., Rokhlenko, O., and Elkan, C.
\newblock End-to-end offline goal-oriented dialog policy learning via policy
  gradient.
\newblock \emph{arXiv preprint arXiv:1712.02838}, 2017.

\end{thebibliography}
