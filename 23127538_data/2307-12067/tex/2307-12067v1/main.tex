\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{iccv}
\usepackage{times}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xspace}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[dvipsnames]{xcolor}
\usepackage{multirow}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\usepackage[capitalise]{cleveref}
\usepackage{enumitem}

\iccvfinalcopy
\def\iccvPaperID{6874}
\ificcvfinal\pagestyle{empty}\fi

% suppress annoying warnings
\hfuzz=5.002pt
\hbadness=10000

\makeatletter
\renewcommand{\paragraph}{%
  \@startsection{paragraph}{4}%
  {\z@}{0.25em}{-1em}%
  {\normalfont\normalsize\bfseries}%
}
\makeatother

% change skip above the caption
\setlength{\abovecaptionskip}{2pt plus 3pt minus 2pt}

\newcommand{\datasetlong}{Replay\xspace}
\newcommand{\dataset}{Replay\xspace}
\newcommand{\RSnote}[1]{{\color{purple}{\bf RS$^\circ$: }#1}}

\title{\datasetlong: \\
Multi-modal Multi-view Acted Videos for Casual Holography\vspace{-1.8ex}}
\author{Roman Shapovalov$^*$
\qquad Yanir Kleiman$^*$
\qquad Ignacio Rocco$^*$
\qquad David Novotny\\
Andrea Vedaldi
\qquad Changan Chen$^\dagger$
\qquad Filippos Kokkinos
\qquad Ben Graham
\qquad Natalia Neverova\\
Meta \qquad UT Austin$^\dagger$ \qquad $^*$\textit{equal contribution}\\
%{\tt\small \{romansh, yanirk, irocco, dnovotny, vedaldi, fkokkinos, benjamingraham, nneverova\}@meta.com}
{\tt\small \url{https://replay-dataset.github.io/}}
%{\tt\small changan@cs.utexas.edu}
}

\begin{document}
\maketitle
\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}
We introduce \emph{\datasetlong}, a collection of multi-view, multi-modal videos of humans interacting socially. Each scene is filmed in high production quality, from different viewpoints with several static cameras, as well as wearable action cameras, and recorded with a large array of microphones at different positions in the room. Overall, the dataset contains over 4000 minutes of footage and over 7 million timestamped high-resolution frames annotated with camera poses and partially with foreground masks.
The \dataset dataset has many potential applications, such as novel-view synthesis, 3D reconstruction, novel-view acoustic synthesis, human body and face analysis, and training generative models. We provide a benchmark for training and evaluating novel-view synthesis, with two scenarios of different difficulty. Finally, we evaluate several baseline state-of-the-art methods on the new benchmark.
%The videos in the HDT dataset are filmed in 4K and production quality, 
%and include camera calibrations to eliminate camera distortion and color balance discrepancies between sensors. 
\end{abstract}

\input{intro}
\input{related}
\input{dataset}
\input{experiments}
\input{conclusions}

{\small\bibliographystyle{ieee_fullname}\bibliography{vedaldi_general,vedaldi_specific,local}}

\input{supplementary}
\end{document}