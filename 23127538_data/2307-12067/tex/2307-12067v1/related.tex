\section{Related Work}%
\label{s:related}

\paragraph{Datasets for dynamic new-view synthesis.}

With the explosion of neural rendering, many datasets for studying new-view synthesis of dynamic content were proposed.
Focusing on humans,
HumanEva~\cite{sigal10humaneva:},
Human3.6M~\cite{ionescu14human3.6m:},
AIST~\cite{tsuchida19aist},
AIST++~\cite{li21learn}, and
ZJU-Mocap~\cite{peng20neural}
portray a single person in isolation, without context, performing scripted motion.
In contrast, \dataset contains groups of humans acting naturally in a familiar environment.

More complex multi-view data containing dynamic humans in context include the Immersive Light Field dataset~\cite{broxton20immersive}, which contains sixteen scenes captured from approximately 46 calibrated cameras.
The NVIDIA Dynamic Scene Dataset~\cite{yoon20novel} contains eight videos captured with 12 (mostly front-facing) calibrated GoPro Black Hero 7 cameras.
The UCSD Dynamic Scene Dataset~\cite{lin21deep} contains 96 videos collected in a similar manner to~\cite{yoon20novel}, but using 10 cameras.
The Plenoptic Video dataset~\cite{li22neural} provides 6 more scenes from 21 cameras.
All such videos are complex and visually diverse, but they all capture a single short-duration activity (typically 1 or 2 minutes at most).

Some datasets are collected using domes, and therefore do not contain natural environments or moving cameras.
Panoptic Studio~\cite{joo17panoptic} contains 3 hours of recordings of humans engaged in multiple social activities captured with roughly 500 cameras and depth sensors.
NeuralDome~\cite{zhang22neuraldome:} contains videos of a single human manipulating an object captured from 76 cameras, and additional sensor-based participant tracking data.
Our \dataset focuses on long sequences with professional actors in a familiar setting.
Furthermore, the usage of head-mounts makes our dataset particularly well-suited for studying scene reconstruction from a single egocentric device, which is one of the most realistic settings for future applications in casual holography.

Finally, all datasets above focus on visual reconstruction, and are thus not multimodal like \dataset.
See also \cref{f:dataset_comparison} for a schematic summary.

\paragraph{Reconstructing 3D dynamic humans.}

Reconstructing a 4D video remains a challenging problem, so many authors have focused on special cases, such as reconstructing individual humans.
Much work has focused on modelling articulated human bodies, including
Neural volumes~\cite{lombardi19neural},
Relightables~\cite{guo19the-relightables:},
Articulated Neural Rendering~\cite{raj20anr:},
A-NeRF~\cite{su21a-nerf:},
Neural Actor~\cite{liu21Bneural},
H-NeRF~\cite{xu21h-nerf:},
Neural Performer~\cite{kwon21neural},
Deep Dynamic Character~\cite{habermann21real-time},
Human Re-rendering~\cite{sarkar21neural},
Pixel Aligned Avatars~\cite{raj21pixel-aligned},
HumanNeRF~\cite{weng22humannerf:},
HiFi Human Avatar~\cite{zhao22high-fidelity},
Generative Neural Articulated RFs~\cite{bergman22generative},
Animatable NeIS~\cite{peng22animatable}.
Most of these works approach the problem by explicitly tracking the human body, usually by using SMPL~\cite{loper15smpl:} fits, and then modelling shape and appearance in a canonical, articulation-free, space.
Other works, including
Neural Head~\cite{grassal21neural},
Dynamic Head~\cite{wang21learning},
Dynamic Neural Faces~\cite{gafni21dynamic},
MoRF~\cite{wang22morf:},
specialise in reproducing heads, and a few such as
Artemis~\cite{luo22artemis:}
explore other animals.
Since our scenes contain several interacting humans and objects, these methods are not applicable to our problem because they focus on isolated reconstructions of specific object classes.

% , which are difficult to segment and track individually.

\paragraph{Reconstructing generic 4D videos.}

Several authors have considered the problem of reconstructing generic 4D videos.
Some have proposed to capture directly the lightfield, with no or partial understanding of scene geometry.
Examples include~\cite{gortler96the-lumigraph,chai00plenoptic,buehler01unstructured,zitnick04high-quality,kanade97virtualized,bansal204d-visualization,kalantari16learning-based,srinivasan19pushing,zhang21editable,stich08view,broxton20immersive,suhail21light}.
% or the more recent Immersive Lightfield~\cite{broxton20immersive}, which attempt to compress the lightfield captured by a large number of cameras in an efficient representation, and~\cite{suhail21light}, which uses a neural representations to do so.

Other methods model shape more explicitly, often using dynamic generalizations of NeRF~\cite{mildenhall20nerf:}.
These are the most applicable to the \dataset scenarios.
Many of them, including
D-NeRF~\cite{pumarola20d-nerf:},
Deformable NeRF~\cite{park20deformable},
Dynamic NVS~\cite{yoon20novel},
Nerfies~\cite{park21nerfies:},
HyperNeRF~\cite{park21hypernerf:},
Neural Trajectory Fields~\cite{wang21neural},
NR-NeRF~\cite{tretschk21non-rigid},
NSFF~\cite{li21neural},
NeRFlow~\cite{du21neural},
STaR~\cite{yuan21star:},
NeRFPlayer~\cite{song22nerfplayer:},
Deformable Voxel Grid~\cite{guo22neural},
TiNeuVox~\cite{fang22fast},
DynIBaR~\cite{li22dynibar:},
DeVRF~\cite{liu22devrf:}
attempt to estimate a deformation field and thus explicitly model the motion in the scene, reducing the video to a single canonical reconstruction and the deformation field.
While this is statistically parsimonious, and necessary for reconstruction when the number of input viewpoints is small, estimating a correct deformation field is difficult due to the underconstrained nature of the problem.
% and also limited to model changes that can indeed be modelled by warping.

Other methods directly add time to the radiance field parametrisation (sometimes called NeRF+$t$), thus avoiding the challenge of explicitly estimating deformations.
Examples include
NERF-W~\cite{martin-brualla21nerf},
NeuralDiff~\cite{tschernezki21neuraldiff},
Video NeRF~\cite{xian21space-time},
Dynamic View Synthesis~\cite{gao21dynamic},
Fourier PlenOctrees~\cite{wang22fourier} and
DyNeRF~\cite{li22neural}.
Finally two recent concurrent works,
K-Planes~\cite{fridovich-keil23k-planes:} and
HexPlane~\cite{cao23hexplane:}, extend the voxel grid decomposition introduced in EG3D~\cite{chan22efficient} and TensoRF~\cite{chen22tensorf:} to spatio-temporal 4D grids. 

Many of these methods explicitly require a large number of viewpoints.
In some cases, this requirement is indirect~\cite{gao22monocular}, in the sense that methods may also work from a monocular camera, but only if the camera motion dominates the scene motion.
% Casual holography will require to solve the much harder problem of reconstructing a full dynamic 3D scene from a camera with very limited viewpoint diversity.

% Regularization:
% DTET~\cite{gao20learning}
% PERF~\cite{song22pref:}
% HRF-Ne~\cite{HRF-Net}

