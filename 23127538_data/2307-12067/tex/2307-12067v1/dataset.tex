\section{The \dataset Dataset}\label{s:dataset}

\newcommand{\inches}{${}^{\prime\prime}$}
\newcommand{\degrees}{${}^\circ$}
\newcommand{\numscenes}{68\xspace}

\input{table_datasets}

The full \datasetlong dataset consists of \numscenes scenes of social interactions between people, such as playing board games, exercising, or unwrapping presents.
Each scene contains about 5 minutes of acting following a few minutes of calibration stages, and is filmed with 12 cameras, static and dynamic.
Audio is captured separately by 8 binaural microphones and additional near-range microphones for each actor and for each egocentric device.
All sensors are temporally synchronized, undistorted, geometrically calibrated, and color calibrated.

In addition to the full dataset, we introduce the \textit{\dataset novel view reconstruction benchmark}, a curated subset of scenes with given training and test sets and supplemental information such as foreground/background masks.
We run several state-of-the-art novel view reconstruction methods on this benchmark and report the results in \cref{s:experiments}.

\paragraph{Content.}

The videos depict human social interaction in a large variety of indoor settings and contexts.
Examples include meeting friends, talking, sitting in a living room, making hand gestures, playing charades, exercising on a yoga mat, playing video games, playing board games, arranging ornaments, having a meal, unwrapping presents, and more.
Each scene contains up to 4 actors, with a total of 42 actors of diverse age, gender, and ethnicity across the scenes. In particular, 21 of them are white, 11 are Asian, 5 are black, and 5 are mixed race.

\paragraph{Scene setup.}

In each scene, there are three human operators wearing wearable \textit{egocentric} cameras that provide eye-level views of the scene. The focus on monocular and binocular wearable cameras and microphones is a unique feature of \dataset, which enables evaluating methods targeting AR/VR applications where a scene captured by one wearable device may have to be rendered in a world-locked manner on another device, from a new viewpoint.

In addition, the scene is shot by 8 static DSLR cameras arranged in a full circle around the action, approximately 45\degrees\,apart from one another.
Each static DSLR camera has a binaural microphone attached to it, and each actor and egocentric camera operator is equipped with a near-range microphone.
We also provide an auxiliary capture of the entire scene with a 360\degrees\ ceiling camera. This capture is not intended to be used as input, and is included to provide an overview of the scene for users of the dataset.
\Cref{fig:splash} shows a bird-eye view of the scene setup, as well as a schematic representation.

\paragraph{Phases.}

Scenes are divided into three logical phases: calibration, flyaround, and acting.
The \emph{Calibration} phase is part of the scene setup, and contains images of calibration patterns and of the digital clapper.
%
% in which the film crew is performing the following calibration actions.
% A moving checekerboard calibration pattern is shown to each camera in order to estimate their intrinsics, such as the focal length of the lens. Then, a colour checker is placed in the scene to assist with color calibration.
% Finally, either a digital or a physical clapper slate is shown, in order to temporally synchronize all the sensors using audio and visual cues.
%
The \emph{Flyaround} phase %the calibration equipment is taken out, 
shows the actors take their place in the scene and remain still.
Then, the wearable camera operators walk around the scene while looking at the central action area.
This provides a continuous 360\degrees{} view of a  scene which remains nearly static.
This stage takes 40--60 seconds.
Finally, during the \emph{Acting} phase the actors perform for about 3--5 minutes.

To reduce the amount of data necessary to process for the benchmark, we limit the fly-around part of the scene to 40 seconds and the acting part to 60 seconds.
We further segment the acting part into two 30-seconds segments, since we found that most state-of-the art-methods are incapable of handling longer sequences.
However, we strongly encourage future users of the dataset to test reconstructing at lest an entire minute.

% For the benchmark, we provide a temporal segmentation of each scene, where we clip most of the flyaround (40 seconds), and one minute of the acting. We further segment the acting part into two 30-seconds segments, since we found current state of the art methods are usually not suitable for longer sequences. However, we strongly encourage future users of the dataset to use the entire minute as a benchmark for future work.

\paragraph{Sensors.}
% Here we provide the complete list of equipment used for recording each scene.
% \begin{itemize}[noitemsep,nolistsep]
% \item Eight static DSLR cameras: Sony A7 III\@; 24\inches{} lens; 30 FPS\@; 4K resolution.
% \item Eight 3Dio binaural microphones co-located with the DSLR cameras.
% \item A ceiling camera AXIS M4308-PLE: wide angle, 30 FPS, 2880$^2$ resolution, circular frame.
% \item Three GoPro cameras: Hero 9; RAW model, 60 FPS, 4K resolution.
% \item A near-range lavalier microphone for each camera operator and actor in the scene.
% % \item The three human operators for ego-centric recording wearing either a single GoPro camera (Hero 9; RAW model, 60 FPS, 4K resolution) or a pair of those connected in a stereo rig, and a near-range microphone.
% \end{itemize}
As shown in \cref{fig:splash}, \dataset uses several sensors. These are:
Eight static DSLR cameras (Sony A7 III\@; 24\inches{} lens; 30 FPS\@; 4K resolution);
Eight 3Dio binaural microphones co-located with the DSLR cameras;
A ceiling camera (AXIS M4308-PLE: wide angle, 30 FPS, 2880$^2$ resolution, circular frame);
Three GoPro cameras (Hero 9; RAW model, 60 FPS, 4K resolution);
And a near-range lavalier microphone for each camera operator and actor in the scene.
% \item The three human operators for ego-centric recording wearing either a single GoPro camera (Hero 9; RAW model, 60 FPS, 4K resolution) or a pair of those connected in a stereo rig, and a near-range microphone.
% \end{itemize}

% - Aria glasses that provide wide-angle RGB video (1408$\times$1408 at 20-30 FPS) in addition to low-quality grayscale stereo footage (2 x 640×480 @ 10 FPS), ahttps://www.overleaf.com/project/63ff7a57e338816441ec6b1flong with IMU recordings and 7-channel audio [recording profile 15].
% - A ZED Mini camera that provides higher-quality stereo footage. The camera produces two streams of 30 FPS with HD resolution.
% - A pair of GoPro cameras (Hero 9) connected with a rigid stereo rig. The shooting mode is 60 FPS with 4K resolution.
% - Each actor is wearing a near-range microphone.

\paragraph{Publicly-available assets.}

% We believe \dataset can be extremely valuable both as training data for new methods, and as a benchmark to evaluate the success of state of the art methods. While we focus our efforts on novel view reconstruction, there are many other potential uses for high quality multi-view data.

% To facilitate further research, we provide the following deliverables, that will be made publicly available upon acceptance of the manuscript. These include the raw video and audio files from all sensors, as well as calibration and synchronisation information that are common as preprocessing steps for many current methods.

We will make the pre-processed data available to resaerchers upon publication, delivering the following assets.
For each imaging sensor $s$, we provide
(1) a collection of video frames $I_{st}$ indexed by $t$;
(2) the distortion and intrinsic calibration parameters $(\rho_s,K_s)$;
%  estimated with COLMAP \cite{schonberger16structure-from-motion};
(3) the camera pose $\pi_{st}$ with respect to the scene reference frame;
%  estimated with COLMAP \cite{schonberger16structure-from-motion};
(4) and, for 10 of the scenes, foreground segmentation masks $M_{st}$ for each frame, including furniture, actors, and objects they interact with.
%  manually annotated with XMem \cite{cheng22xmem:}.
For each audio sensor $a$, we provide
(1) a collection of audio frames $A_{at}$ indexed by $t$;
(2) the location $\pi_{at}$ of the sensor (which usually coincides with a certain imaging sensor), except for the near-range microphones, which are mounted on the actors and camera operators, whose dynamic location is therefore difficult to estimate.
All sensors are temporally synchronised; for this, we provide the time information $\tau_{st}$ and $\tau_{at}$ for each video and audio frame.

We also provide benchmark definitions (\cref{s:bench}) and corresponding evaluation code.

% The data will be made publicly available upon acceptance of the manuscript.

\section{Data collection}\label{s:collection}

Collecting a dataset such as \dataset{} is a major endeavour.
We describe the key aspects of the data collection to better understand the properties of the dataset, and because they can be helpful for other researchers that wish to engage in a similar experimental activity.

\subsection{Production}

The data was produced with the help of a vendor who took care of finding locations, ordering hardware, hiring professional actors, running the filming, quality assurance, and assigning basic metadata.
Production lasted for more than six months, and resulted in the collection of 119 scenes, of which \numscenes have been processed so far to be released.
The vendor was instructed to calibrate sensors before recording each scene, as described below.
%In the later recordings, show a calibration pattern to all static cameras to enable accurately estimating their poses (location and orientation).

To be able to diversify recording locations and keep natural backgrounds, we required a relatively mobile capture setup that (contrary to a dome) required non-trivial calibration and synchronisation from scratch before each recording.
The large amount of sensors of different types and setup phases significantly increased the likelihood of human and equipment failures, such as camera or microphone malfunction, accidental camera movement during filming, wrong focus or focal length, missing or low-quality setup step for a specific camera, \etc
Secure data storage and transfer was also a challenge due to the data volume (60 GB per scene).


\subsection{Processing}

The captured data required substantial pre-processing, including, in order: intrinsic calibration, temporal synchronisation, temporal segmentation, extrinsic calibration, photometric calibration, foreground/background segmentation.
The principal challenges and solutions are discussed next.

\paragraph{Intrinisic calibration.}

The focal length, principal point, and lens distortion of each sensor was estimated by asking the vendor to show a moving ChARuco calibration board at least once to each sensor, and for latter recordings, before recording each scene.
A ChARuco board is a commercially available checkerboard combined with ARuco tags \cite{aruco-Garrido2014}, which allows disambiguating the pose of the board in camera coordinates.
These  estimations were then used initialise the cameras in the COLMAP SfM software~\cite{schonberger16structure-from-motion}, which then refined the intrinsic parameters through joint optimisation with camera poses; see below for details.

\paragraph{Temporal synchronisation.}
% \RSnote{Rewrite it in a more paper style: focus on the details of a final solution, and then describe the motivation for it, and ehy other things did not work.}
Accurate temporal synchronisation of the various sensors is crucial for training and evaluating a dynamic scene from multiple view points.
While specific sensors support hardware-based synchronisation, this is inapplicable to our heterogeneous mix of sensors.
Instead, the sensors are synchronised using their audio signature. We take a salient segment of the audio recorded by each sensor, and match it with the entire audio of a second sensor using cross correlation. To increase robustness, we take several segments from different positions in the audio file and use a majority vote to estimate the true offset between each two sensors.
After the computation of the pairwise correspondence, we check the stability of the estimated offsets by analysing cycles of three or more sensors. That allows us to identify sensors which were not matched correctly, due to errors in the data collection, for example noisy cameras or cameras which did not record audio due to technical issues. In such cases, manual temporal synchronisation was required for specific sensors in a small portion of the shots.

% Developing a solution required several iterations.
% We first attempted to use sound correlation (because almost all sensors contain a microphone), initially using a clapper and then by simply correlating background sounds (which turned out to be more robust).
% However, this was insufficient from frame-accurate synchronisation because, while the temporal resolution of audio is far greater than that of video, different cameras have a different amount of delay between their visual and audio streams (leading to up to 5 frame discrepancy).
% We also attempted to correlate streams visually using the statistics of the optical flow, but this was not sufficiently robust to work in all cases.

\paragraph{Camera pose calibration.}

Camera poses were estimated using COLMAP~\cite{schonberger16structure-from-motion} with several improvements for robustness and scalability.
Specifically, camera intrinsics were initialising via  ChARuco calibration (see above); we then fixed the principal point, but let COLMAP refine the focal length and distortion parameters.
The environment was first reconstructed using the head-mounted camera from the fly-around phase of the capture at a low frame rate (3 FPS).
This produced a sufficiently small number of frames ($\sim$1,000) with sufficient parallax for COLMAP to run successfully.
Then, all the other frames in the capture were triangulated against this initial reconstruction after masking out image regions prone to contain dynamic objects (instances of person, cat, dog segmented using PointRend~\cite{kirillov20pointrend:}).
Finally, since the absolute (and thus relative) positions of the static cameras are constant, they are assumed to form a rig; we thus use bundle adjustment with rig constraints to reconstruct them.
For environments that did not provide sufficient texture detail to robustly triangulate the DSLR poses, we further refined calibration of the DSLR cameras by showing to pairs of them a ChARuco board.

\paragraph{Photometric Calibration.}

We bring all frames from different sensors in the same sRGB colour space, despite differences in the sensor type, factory calibration, and possible processing steps over which we had no control.
This was done by
(1) using the color model of each sensor to map colors to linear space;
(2) filming a reference colour chart from each camera and fitting a linear map to align colours between sensor pairs;
(3) moving colours back to sRGB space.

\paragraph{Foreground segmentation.}

We provide high-quality foreground masks for part of the dataset.
Fully automatic foreground segmentation in videos is still an extremely challenging research problem.
We therefore adopted a human-in-the-loop approach, leveraging the state-of-the-art XMem~\cite{cheng22xmem:} video segmenter.
The model extends scribbles annotated by an operator to generate a segmentation mask for each video frame, and then automatically propagates masks to the subsequent frames.
When an error is detected, the operator intervenes drawing additional scribbles to correct the current prediction, and then re-initiating the propagation process.
Since the definition of foreground objects is open to interpretations, we made sure the same person worked on each scene, ensuring consistency within and across its videos.
Although the process requires only a limited intervention, we still found it time consuming due to the long duration of the videos and the number of cameras per scene.
Thus, only 10 scenes are annotated with foreground masks.

% Motivation

% Most methods for training 3D reconstruction models on multiple views of the same person / object require accurate foreground masks (with the foreground definition depending on the application). That’s why we need to either annotate segmentations for all training frames or train a separate model to provide those segmentations. In the latter case, we still need to annotate some frames to be able to train/evaluate the model.

% Based on that, we came up with the following requirements for segmentation annotation:

% We need video instance segmentation, not just foreground/background. That means each person/object should have its own mask and must be tracked across frames and sensors.
% Segmentation should be consistent during the whole video and across views. There are possible variations about how annotators may decide to mask some objects, and we need to make sure these decisions are consistent.
% Segmentation should be of a good quality, don’t capture too many pixels belonging to other objects, discontinuous if needed.
% These requirements are described more formally in the annotation guidelines.

% An example of the annotation for different views of the same scene ---  GoPro (left) and DSLR (right).

% Examples of ambiguity in required annotations: game controller can be segmented with or without the cable (left), Jenga tower is falling apart ---  it could be segmented as 1 object or as multiple (right).

% Our Approach and Challenges

% We decided to use Halo for annotations. Halo is Meta’s internal tool for annotation. It supports various kinds of annotation, including image segmentation and video segmentation.

% We faced a few challenges along the way:

% Amount of labor. Video annotation is hard and labor intensive. Model-in-the-loop approaches help with that for both space and time:
% In the time dimension, once a mask has been drawn for the first frame, it can be propagated to the consecutive frames.
% Halo supports mask propagation via HALO MILO interface, and there is a model providing this functionality.
% The current model used by MILO is based on the STM method, which is outperformed by newer methods like XMem or MinVIS. We started to explore bringing in XMem to fbcode, but we need to make sure the licence allows that first.
% In order to ease object masking on each frame, we explored the Segment Everything project. With SegEv, a person / object can be annotated in a few clicks, and then the mask can be manually refined. The problem though is that SegEv isn’t available for video segmentations in Halo as of H2 2022. However, there is ongoing work between Halo team and SegEv team for that, so it may be used for our project when it’s ready.
% Cross-view consistency.
% One of our key requirements is to segment the same objects in the same way on different views. We plan to have a 2-stage annotation process for that:
% In the first stage, only one view is annotated.
% In the second stage, annotators see the new view along with previous annotation.
% Long videos.
% Our videos are 5--- 10 minutes long, which means having thousands of frames per video, depending on the frame sampling rate. Job size in Halo should be limited to 1--- 2 hours or human labor, otherwise it’s hard for annotators to finish it.
% In Halo videos are recommended to be in the order of tens of MB maximum for a smooth annotation process in the browser.
% We can’t simply split the videos into chunks and process them independently because we need to ensure mask consistency throughout the video.
% Our approach is to split the videos to overlapping chunks, so that masks from the end of one chunk would get auto-populated in the beginning of the next chunk.
% Alternative Approaches

% Don’t do the video annotation but rather process individual frames at fixed sampling rate, then propagate masks on FAIR cluster.
% The concern here is that annotations may lose consistency ---  one object may be annotated on one image while not annotated, or masked slightly differently, on another image.
% Other platforms for data annotation.
% We didn’t really consider this option in depth because Halo is a powerful tool (for example, it supports model-in-the-loop) and the Halo team is open to collaboration with clients in order to adjust their needs.

