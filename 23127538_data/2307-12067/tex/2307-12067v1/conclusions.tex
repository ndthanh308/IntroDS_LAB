\section{Conclusion}%
\label{s:conclusions}

We presented \datasetlong, a collection of scenes captured with egocentric and scene-static sensors.
We aim primarily to support research in new-view synthesis of dynamic and multi-modal content from egocentric sensors, including in particular reconstruction from a single viewpoint or a very narrow baseline.
In the future, this technology will enable breakthrough applications such as personal holography.
While this task is still too challenging for existing new-view synthesis methods, as generative AI matures, it will become possible to better hallucinate information missing in the capture, and \dataset can spur further research in this direction.
Furthermore, while in this paper we have only discussed the visual component of the data, \dataset also contains fully-calibrated and synchronised audio information for research in multi-modal new-view synthesis, which is as of today largely unexplored.


% and our data can spur further resarch in this setting.

% and 3D reconstruction by providing data that can be used for training and evaluation of new methods.
% However, there are many other potential uses for this data which we have not explored, such as video prediction, action recognition, and even generative methods. Thus, we encourage the use of this dataset outside the domains which we have explored in this manuscript.

% % We could add a paragraph about novel view synthesis methods and how much they suck for videos longer than a few seconds.

% \paragraph{Future work.} \RSnote{Extending the number of scenes for the benchmark, masks, length of the benchmark; monocular fitting.}


% \paragraph{Future work.} 
%\RSnote{Extending the number of scenes for the benchmark, masks, length of the benchmark; monocular fitting.}

%Finally, \dataset is a living platform.
%First, we have purposefully limited the data we delivered with our first release to make its processing reasonable for a wider audience of researchers (a dataset which is too large may in fact \emph{discourage} potential users).
%However, as compute and methods improve, we will be able to deliver more of the data that we have already collected, including significantly longer videos, thus maintaining the challenge fresh and approachable.
%Second, we are in the process of collecting more and more diverse videos, which we also plan to make available in future releases.

\paragraph{Limitation.} There are a few limitations of the released part of the dataset. First, we did not record stereo videos, which might become an important modality with the next generation of wearable devices. Second, the operators of dynamic cameras do not change location during acting, only moving their heads in a natural way, which represents a subset of AR/VR applications. Finally, all released scenes were filmed in the same room, albeit with various furniture and props. These limitations may be addresses in future work.

% In the near future, we will extend the dataset with a second batch of scenes that are currently in production. The second batch will include egocentric stereo rigs as well and egocentric depth cameras, as well as increasing the variety and diversity of the scene content.
% %, and the sheer amount of audio and video frames that can be used as training data.
% %Many novel view reconstruction methods require masks of foreground elements or moving objects in the scene.
% %In \dataset, we provide such masks for 10 scenes. 
% In addition, we are planning to add masks that are required by many methods for more scenes to increase the usefulness of the benchmark.
% %During the work on the benchmark, we discovered that a lot of the state of the art methods 
% %Many methods struggle with scenes which are longer than a few seconds. Therefore, 
% We intentionally left a significant amount of data outside the benchmark by cutting scenes short and excluding some sensors, to accommodate methods which struggle with longer scenes. 
% %For example, instead of providing 3 minute long segments, we only used 30 seconds long segments for the actual benchmark, and instead of using all cameras, we only included the front facing DSLRs for acting segments.
% Looking forward, we are hopeful that upcoming methods would be suitable for processing larger scenes, and we encourage further work to utilize the complete length of the scenes and all of the cameras.
% In this view, our dataset is scalable for the many challenges ahead by providing the ability to increase throughput as we develop more advanced methods and capabilities.


% \paragraph{Broader impact.} \RSnote{Actor consents, diversity, blah-blah}