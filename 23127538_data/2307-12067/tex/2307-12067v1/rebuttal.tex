\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{iccv_rebuttal}
\usepackage{times}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{xspace}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\definecolor{msgreen}{rgb}{0.19215686,  0.63921569,  0.32941176}
\definecolor{msred}{rgb}{0.87058824,  0.17647059,  0.14901961}
\definecolor{msblue}{rgb}{0.19215686,  0.50980392,  0.74117647}

\newcommand{\note}[1]{\textcolor{red}{#1}}
\newcommand{\issue}[3]{\noindent\textsf{[#1] \textbf{#2}} #3}
\newcommand{\Rone}{\textcolor{msred}{\textbf{R1}}}
\newcommand{\Rtwo}{\textcolor{msgreen}{\textbf{R2}}}
\newcommand{\Rthree}{\textcolor{msblue}{\textbf{R3}}}

\def\iccvPaperID{6874}
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\begin{document}
\title{Replay: Multi-modal Multi-view Acted Videos for Casual Holography}
\maketitle
\thispagestyle{empty}

We thank the reviewers for their helpful suggestions and questions, which we address below.
Unfortunately, \Rtwo~reviewed the paper we included in the sup.~mat. covering the audio part of the dataset instead of our submission. 

\issue{\Rone}{Only $<1$ min videos [...] released}
We will release raw 10+ minute videos containing calibrations and full acting stage alongside the processed crops used in the benchmarks.
We trained on 30â€“40 second crops for the benchmarks since SOTA methods are failing on longer sequences.

\issue{\Rone}{Lack of color in equalized images}
DSLR cameras capture a wider range of brightness and colour, with more details than GoPro footage in the bright and dark areas of the image.
Because they contain more information, they may look less vibrant after equalization for print.
``Normal''  contrast and vibrancy can easily be recovered in post-processing at the expense of losing details in bright or dark areas.
% However, these images can be transformed using simple post processing to achieve any desired look, such as increasing contrast and vibrancy at the expense of losing details in bright or dark areas.
This is demonstrated in our supplementary video (timestamp 1:15), where after the color equalization step we perform post processing to get more vibrant images that are still consistent between the two sensors.
The processed crops of videos will be released with corrected colours.
% seamlessly get the post processed images.

\issue{\Rone}{Difference from dome datasets}
While the background is not fully natural as it contains several cameras on tripods and, for rear cameras, GoPro operators, as opposed to dome datasets, we were able to fit in bulky props such as a sofa and TV, or a ping-pong table. Moreover, masked or not, lighting conditions are natural and vary noticeably with the time of day, which is not possible with a dome.

\issue{\Rone}{The ``acting'' benchmark is more simple than expected}
We agree that monocular reconstruction is the Holy Grail of AR/VR, however, the SOTA dynamic NeRFs fall short of even attempting to extrapolate beyond the convex hull of input viewpoints (reconstructing only humans could be possible due to the ability to learn strong priors but this is out of scope of our benchmarks).
Please note that we will release the data loaders which permit effortlessly slicing the data by sensor, so that researchers will be able to experiment with monocular reconstruction.

\issue{\Rthree}{Variety of data}
The data collection for this benchmark is an ongoing process. At submission time we reported results for 46 scenes, but we have since collected more and now plan to release 68 scenes.
You are right that they were filmed in the same room, however furniture changed significantly between the recordings.
Out of 42 actors participating in the scenes, 21 are white, 11 are Asian, 5 are black, and 5 are mixed race.

\issue{\Rthree}{Are any stereo cameras included in the dataset?}
We agree that stereo videos might become an important modality, depending on how consumer devices are going to evolve.
As part of our ongoing data collection, we replaced GoPros with head-mounted rigs of two GoPros each (not part of the data presented in the paper).
% for subsequent sequences, however, the scenes presented in the paper were not filmed with binocular sensors.

\issue{\Rthree}{Limited GoPro movement during the action}
It is true that during acting, GoPro operators stay in place, however they exhibit natural human movements (such as swaying from side to side or turning the head).
We acknowledge the limitation but we believe that is representative of many natural AR/VR recording / playback scenarios.

\issue{\Rthree}{Rolling-shutter artefacts}
We used the highest shutter speed as reasonably possible to not degrade the colours, and the cameras we use are either high-end (DSLR) or marketed for sports activities (GoPro), so we did not observe this problem.
We specify the models of all hardware we use in l.\,359+.

%%%%%%%%%%%%%%%
% minor issues

\issue{\Rone}{4K resolution images are probably overkill}
While they are a few years away, highly-detailed reconstructions are required in practical consumer applications of holography. This future-proofs our data.
% This is not the bottleneck as we downsample images in the data loader.
% The problem with longer scenes is limited model capacity and, for some implementations, caching the ray points in RAM.

\issue{\Rone}{What is ``foreground segmentation''?}
The definition is scene-specific and is guaranteed to be consistent within a scene.
The methods are expected to produce non-zero density only for the foreground objects.

\issue{\Rone}{Why can't NeRFies produce ``opacity masks''?}
We empirically found that learning NeRFies on masked videos produced worse results, in some cases degenerated to black renders.
Please note that we fairly evaluate all models on the foreground regions only.

\issue{\Rone}{The IOU metric is unclear}
We will describe the metric more formally.

\issue{\Rone}{Is there a bug in HexPlane for the ``flyaround'' (semi-static) scene?}
We don't think it is a bug.
We use the same code base with incremental differences between TensoRF and HexPlane.
We attempted to explain the phenomenon in l.\,477+.
Flyaround benchmark requires the methods to extrapolate more significantly, and HexPlane, having to model an additional degree of freedom (time), fails to generalise to distant viewpoints, while TensoRF has enough data assuming constancy in time.
The acting benchmark requires mostly to interpolate the viewpoints, so that methods are generally less prone to overfitting there.

\issue{\Rthree}{Dataset format}
Thank you for the suggestion.
We will provide highly configurable PyTorch data loaders, which will hopefully cover most use cases, and most users won't need to learn the underlying format.
We will distribute the metadata (including camera poses at 30 FPS) as an Sqlite database, and the undistorted images and masks re-encoded as videos (lossless or with high enough bitrate), as we found this the best balance between data-loading speed and dataset size. We will provide tutorials on using both data-loader API and low-level metadata access.


\end{document}
