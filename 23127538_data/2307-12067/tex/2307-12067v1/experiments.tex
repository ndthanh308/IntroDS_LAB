\section{Experiments}%
\label{s:experiments}

After defining two benchmarks (\cref{s:bench}), we conduct several experiments on \dataset for the purpose of demonstrating its application to the development of new-view synthesis methods, as well as to assess current state-of-the-art neural rendering techniques (\cref{s:baselines}) on this challenging data (\cref{s:results}).

\subsection{Benchmarks}%
\label{s:bench}


We define two novel-view synthesis benchmarks using \dataset:
\textit{flyaround}, with semi-static actors filmed from $360^{\circ}$ trajectory, and the more challenging \textit{acting}, where naturally behaving actors are filmed with frontal cameras.

\begin{table}[b]
\centering
% \begin{tabular}{|l|lll|lll|}
% \hline
% \multicolumn{1}{|c|}{\multirow{2}{*}{Method}} & \multicolumn{3}{c|}{flyaround (semi-static)}                                               & \multicolumn{3}{c|}{acting (dynamic)}                                                      \\ \cline{2-7} 
% \multicolumn{1}{|c|}{}                        & \multicolumn{1}{l|}{PSNR}           & \multicolumn{1}{l|}{IOU}            & LPIPS          & \multicolumn{1}{l|}{PSNR}           & \multicolumn{1}{l|}{IOU}            & LPIPS          \\ \hline
% NeRF ~\cite{mildenhall20nerf:}                & \multicolumn{1}{l|}{21.85}          & \multicolumn{1}{l|}{\textbf{0.948}} & \textbf{0.218} & \multicolumn{1}{l|}{19.85}          & \multicolumn{1}{l|}{0.921}          & 0.227          \\ \hline
% NeRF$+t$                                        & \multicolumn{1}{l|}{20.86}          & \multicolumn{1}{l|}{0.917}          & 0.254          & \multicolumn{1}{l|}{\textbf{21.15}} & \multicolumn{1}{l|}{\textbf{0.937}} & \textbf{0.215} \\ \hline
% TensoRF  ~\cite{chen22tensorf:}                & \multicolumn{1}{l|}{20.58}          & \multicolumn{1}{l|}{0.924}          & 0.223          & \multicolumn{1}{l|}{17.26}          & \multicolumn{1}{l|}{0.776}          & 0.417          \\ \hline
% HexPlane ~\cite{cao23hexplane:}                & \multicolumn{1}{l|}{15.08}       & \multicolumn{1}{l|}{0.866}          & 0.289          & \multicolumn{1}{l|}{17.66}          & \multicolumn{1}{l|}{0.723}          & 0.444          \\ \hline
% NeRFies  ~\cite{park21nerfies:}                   & \multicolumn{1}{l|}{\textbf{23.22}} & \multicolumn{1}{l|}{N/A}            & 0.613          & \multicolumn{1}{l|}{18.08}          & \multicolumn{1}{l|}{N/A}            & 0.712          \\ \hline
% \end{tabular}%
{\small
\begin{tabular}{@{}lcccccc@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{c}{Flyaround ($\approx$static)} & \multicolumn{3}{c}{Acting (dynamic)} \\ \cmidrule(lr){2-4}\cmidrule(l){5-7} 
                        & {\footnotesize PSNR}          & {\footnotesize IOU}           & {\footnotesize LPIPS}        & {\footnotesize PSNR}       & {\footnotesize IOU}       & {\footnotesize LPIPS}      \\ \midrule
NeRF~\cite{mildenhall20nerf:}            & 21.85         & \textbf{0.95}        & \textbf{0.22}        & 20.22      & 0.93      & 0.23      \\
NeRF+t                  & 20.86         & 0.92        & 0.25        & \textbf{21.28}      & \textbf{0.94}      & \textbf{0.22}      \\
TensoRF~\cite{chen22tensorf:}          & 20.58         & 0.92        & 0.22        & 17.26      & 0.78      & 0.42      \\
HexPlane~\cite{cao23hexplane:}         & 15.08         & 0.87        & 0.29        & 17.66      & 0.72      & 0.44      \\
Nerfies~\cite{park21nerfies:}         & \textbf{23.22}         & N/A          & 0.61        & 18.08      & N/A        & 0.71       \\ \bottomrule
\end{tabular}
}
\caption{Quantitative results on the two proposed benchmarks. Please note that the numbers are not comparable across benchmarks: in \textit{flyaround}, methods have to model a wider range of viewpoints. In \textit{acting}, all training and evaluation cameras are frontal, but the methods have to model the dynamic geometry of the scene.\vspace{-0.2cm}}\label{tab:results}
\end{table}

\paragraph{Flyaround.}

This simpler benchmark allows evaluating standard reconstruction approaches such as neural radiance fields that cannot model time, alongside with those that model time dependency and shape deformations.
The 40-second segments are extracted from each of the \dataset sequences (see \cref{s:dataset}).
% automatically detected based on optical-flow projections.
Training frames are extracted from one dynamic wearable camera at 30 FPS (GoPro-2 in \cref{fig:splash}), and all 8 static DSLR cameras are used for evaluation.
This amounts to 1,200 training and 64 evaluation frames per scene.
Despite the fact that there is no significant motion in the scene in these segments, the task is already quite challenging since, unlike typical NVS datasets, we require \textit{extrapolating} beyond camera the trajectory, as the DSLRs are located far away from the trajectory of the wearable camera used for capture.
%  the wider circle than the trajectory of a wearable camera.

\paragraph{Acting.}

This is the most challenging setting since the actors are allowed to move freely, while the operators of wearable cameras stand fairly still in front of them.
Monocular reconstruction in this context is still beyond the capability of state-of-the-art reconstruction methods, so in this instance we consider a multi-view reconstruction setup.
We consider a 30-second segment sampled at 30 FPS, which is significantly longer than the data used for testing modern deformable NVS methods, so it stretches their limits.
We hold out one static DSLR for evaluation, while using two relatively frontal DSLRs (DSLR-5 and DSLR-6 in \cref{fig:splash}) and 3 wearable cameras for training, resulting in 4,500 training frames and 50 randomly sampled evaluation frames per scene.
While this scenario is more challenging because of the changing geometry of the scene, all training and evaluation sensors are located in front of the actors, so that the methods do not have to generalise to a wide range of viewing angles.

% Figure environment removed

% Figure environment removed

\subsection{Metrics}

We compare the methods using a range of metrics evaluating the faithfulness of the rendering, their perceptual quality, and the quality of the opacity mask (for the methods that produce it).
To this end, we use the following metrics: Peak Signal-to-Noise Ratio (PSNR), Learned Perceptual Image-Patch Similarity (LPIPS), both evaluated in the foreground region only, and the Intersection over Union (IoU) between the produced opacity mask and ground-truth foreground mask.
We compute the metrics at a fixed resolution of $960 \times 540$, which can be handled by all the methods.
% We evaluate on relatively standard GPU hardware.

\subsection{Baselines}\label{s:baselines}

We evaluate the dataset on a range of novel-view synthesis methods, including those modelling dynamic scenes.

\paragraph{NeRF and NeRF$+t$.}

Neural Radiance Field (NeRF)~\cite{mildenhall20nerf:} fitting is a cornerstone of modern novel-view synthesis.
The method learns the radiance field of the scene through an MLP $\Psi$ that predicts the colour $c_i \in \mathbb{R}^3$ and density $o_i \in \mathbb{R}$ for points $r_i$ along the rays $\mathbf{r}$ emitted from the camera center passing through each pixel:
\begin{equation}\label{eq:nerf}
    [c_i, o_i] = \Psi(\gamma_{R_x}(r_i), \gamma_{R_d}(d_i)),
\end{equation}
where $d_i$ is a normalised vector pointing from the camera centre to $r_i$, $\gamma_{R}$ is an order-$R$ harmonic encoding, $R_x, R_d \in \mathbb{N}$ are hyperparameters.
The predicted colours and opacities are then integrated along the ray using the emission-absorption raymarching function to get the final RGB value in the corresponding pixel.
Unlike standard NeRF, we do not model view-dependent colours (\ie $R_d=0$) due to sparsity of input views:
in \textit{flyaround} setting, we noticed the model generalises poorly to DSLR camera poses that are located farther away from the scene centre than the wearable camera's trajectory;
in \textit{acting} setting, we found that 5 viewpoints per timestamp are not sufficient to fit view-directional colours reliably,
i.e. we set $R_d = 0$.
Since we are interested only in the foreground, we pre-process the images by masking out background pixels.

NeRF assumes that the scene is static and produces blurry renders even in case of a limited non-deliberate motion.
Hence, we consider the temporal extension NeRF$+t$ (used in various video reconstruction methods~\cite{martin-brualla21nerf,tschernezki21neuraldiff,xian21space-time}):
\begin{equation}\label{eq:timenerf}
    [c_i, o_i] = \Psi(\gamma_{R_x}(r_i), \gamma_{R_t}(t)),
\end{equation}
where $t$ is a frame's timestamp normalised to $[0, 1]$ range.
This model is thus tasked in modelling a 4D time-space. 

\paragraph{Nerfies.} Nerfies~\cite{park21nerfies:} extend the vanilla NeRF model to handle deformations, but does this in a different way than NeRF$+t$. Instead of treating $t$ as an additional input dimension, Nerfies model the dynamics by considering a time-invariant rigid radiance field $\Psi$ in \emph{canonical space}, and a time-dependant deformation field $\Delta$, which allows to convert points from posed to canonical space. Therefore, in order to compute the color of a pixel of an image $I_k$, the points $r_i$ along the ray $\mathbf{r}$ are first offset to canonical coordinates by applying $\Delta$, before obtaining the colors $c_i$ and opacities $o_i$ by evaluating the implicit function $\Psi$:
%
\begin{equation}\label{eq:nerfies}
\begin{split}
    \bar{r}_i &= \Delta(\gamma_{R_x}(r_i), \phi_k) \\
    [c_i, o_i] &= \Psi(\gamma_{R_x}(\bar{r}_i), \gamma_{R_d}(d_i)),
\end{split}
\end{equation}
%
where $\phi_k$ is an appearance code corresponding to image $I_k$. We found that the quality of Nerfies degrades with masking, so we train it on the unmasked videos. Note that we still report the foreground-only PSNR and LPIPS.


% \paragraph{NSFF}

\paragraph{TensoRF and HexPlane.} 
TensoRF~\cite{chen22tensorf:} shares with NeRF the underlying idea of volumetric rendering through emission-absortion ray marching. However, instead of modelling the radiance field with an MLP, TensoRF proposes to model it through a product of a set of 2D ($M^{XY}$, $M^{YZ}$, $M^{XZ}$) and 1D tensor components ($v^{X}$, $v^{Y}$, $v^{Z}$), which factorize the 3D density and color spatial fields in a memory- and computationally-efficient manner.

HexPlane~\cite{cao23hexplane:} extends TensoRF by considering the factorization of the 4D space-time density and color fields (analogousy to how NeRF$+t$ extends NeRF), and therefore including into the factorization 2D tensors $M^{Xt}$, $M^{Yt}$, $M^{Zt}$ which span the temporal axis $t$ along with each spatial axis $x$, $y$, or $z$.

\subsection{Results}%
\label{s:results}

\paragraph{Flyaround.}
The result on a flyaround benchmark are shown in Figure \ref{fig:qual-flyaround} and summarised in the left half of Table \ref{tab:results}. NeRF and NeRFies produce best results, with the latter better adapting to small movements present in the scene. While TensoRF shows results comparable to NeRF, its temporal extension, HexPlane, falls short to generalise to a wide range of viewing angles, presumably due to overfitting to the additional temporal dimension.

\paragraph{Acting.}
The results on acting benchmark are shown in Figure \ref{fig:qual-acting} and summarised in the right half of Table \ref{tab:results}. Here, time-extension of NeRF shows the best quality, being able to learn the dynamic geometry to a better extent, while time-agnostic NeRF produces the ghost-looking shapes wherever an actor changed the pose. HexPlane, on the other hand, in spite of a better ability to model deformations, does not improve much over TensoRF, and NeRFies fails to reconstruct the geometry explaining the errors away with floaters.
%relies on floaters to explain away the geometry.

