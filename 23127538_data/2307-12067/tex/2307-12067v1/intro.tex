\vspace*{-2mm}
\section{Introduction}\label{s:intro}

% Figure environment removed

A staple of science fiction is to relive past events and memories as holograms.
With technological advances in virtual, mixed and augmented reality, this vision is ever closer to become a reality.
We can now think of recording an event with a pair of AR glasses instead of a camera, and relive it later as a 360$^\circ$ re-projection in a real or virtual space.
However, there are still major technical hurdles before this can be done reliably and with sufficient quality.

High-fidelity 3D reconstruction remains one of the primary obstacles.
Given a casual recording from a single sensor like a pair of AR glasses, it is in general not possible to reconstruct the content in 3D.
Monocular data, or even data collected from cameras with a short baseline, simply does not contain sufficient information for 360$^\circ$ reconstruction.
For example, in such setup, it is not possible to observe simultaneously the front and back of an object.
Furthermore, reconstructing appearance is not enough: any engaging user experience also requires to reconstruct sounds, so the problem is inherently multi-modal.

Consumer holography requires to compensate for the intrinsic limitations of a casual data capture setup via machine learning.
% However, despite the success of modern techniques such as neural radiance fields, much work remains to be done.
However, despite the success of neural rendering~\cite{mildenhall20nerf:}, even the bests method struggle to reconstruct complex, long dynamic content from a monocular sensor.
% They also lack the ability of completing missing information in a plausible manner.
% Only now hallucination in 3D or 4D reconstruction has begun to be tackled by generative AI, but results are still far from realistic, and there is no evidence that these models could be used to aid the reconstruction of real scenes.
Furthermore, none of these approaches tackles multi-modal reconstruction yet.

% This is not to say that holography must remain the purview of dedicated enterprises and studios.
% Consumer holography is possible, but learning the prior distribution is required to compensate for the intrinsic limitations of a casual data capture.
% However, despite the success of modern techniques such as neural radiance fields, much work remains to be done.
% Current techniques do not model well complex dynamic content, particularly if the content lasts for minutes instead of a few seconds.
% They also lack the ability of completing missing information in a plausible manner.
% Only now hallucination in 3D or 4D reconstruction has begun to be tackled by generative AI, but results are still far from realistic, and there is no evidence that these models could be used to aid the reconstruction of real scenes.
% Furthermore, none of these approaches tackles multi-modal reconstruction yet.

Here, we suggest that further progress in casual holography, and in general in the reconstruction and generation of realistic 4D (3D + time) multi-modal content, is severely hampered by the lack of suitable datasets.
We address this gap by introducing \textit{\datasetlong}, a new large dataset to study the problem of multi-modal new-view synthesis for long captures of acted dynamic content.
\dataset contains long scenes in a natural indoor environment (living room, dining room, \etc), where multiple people are interacting with props and with each other and performing a variety of activities such as exercising, playing games, or chatting.
Each scene is several minutes long, and is filmed in 4K resolution with 8 static DSLR cameras and 3 head-mounted GoPro cameras that capture the scene from all view points, allowing the evaluation of scene reconstruction from the view points that significantly differ from the source video.
For each scene, we also provide a semi-static \emph{fly-around} sequence, where the actors pause and remain still while the head-mounted camera operators walk around them.
In addition, the scene is recorded with a large array of microphones to allow novel view acoustic synthesis\,\cite{chen2023nvas}.
All sensors are temporally calibrated, and cameras are also color- and view-calibrated as well. In addition, metadata such as foreground segmentation masks is provided for some of the scenes.
The data is collected with actors' consent, addressing privacy concerns, and will be public for non-commercial research.

This paper focuses on the visual component of \dataset;
the audio part of the dataset is introduced and used for novel-view \textit{acoustic} synthesis by Chen et al.\,\cite{chen2023nvas}.
The \dataset videos, in turn, constitute a notable step up compared to existing datasets for static and dynamic novel view synthesis; so far methods have been evaluated on short sequences with a limited range of view points.
For example, the popular Dynamic Scene Dataset~\cite{yoon20novel}, which is often used to evaluate dynamic new-view synthesis, contains short scenes ($\approx$5 sec) sampled at low FPS (30 frames in total), and captured by static cameras where the farthest two cameras are about one meter apart.
Other datasets such as ZJU-Mocap~\cite{omran18neural} and AIST++~\cite{li21learn}, provide longer videos with a large variety of view points, but are human-centric and contain people with an empty background and no additional objects, which makes them less useful for evaluating full scene reconstruction.
None of these datasets contain naturally-acted events with sounds.

Due to the richness of scenes, actors, sensors and modalities, \dataset can be used to define a large variety of different tasks in multi-modal new-view synthesis.
The most direct setting for casual holography is to reconstruct a scene from a single head-mounted camera; then, reconstruction quality can be assessed in a 360$^\circ$ manner by using the static DSLRs cameras or the other head-mounts for evaluation.
However, tasks of various complexity can also be defined, such as reconstruction from any combination of static and moving cameras.
Furthermore, the fly-around segments at the beginning of each sequence can be used to test reconstruction in a decidedly simpler (semi-static) setting, and to simplify the reconstruction of the dynamic part as well.

Using \dataset we define two such benchmark tasks of increasing difficulty and assess various existing techniques on them.
Specifically, we consider baselines representing different families of radiance-field models (NeRF~\cite{mildenhall20nerf:}, TensoRF~\cite{chen22tensorf:}) and their extensions dealing with dynamic scenes (NeRF+time, HexPlane~\cite{cao23hexplane:}, Nerfies~\cite{park21nerfies:}).
