% \documentclass[10pt,twocolumn,letterpaper]{article}
% \usepackage{iccv}
% \usepackage{times}

% \usepackage[T1]{fontenc}
% \usepackage{graphicx}
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{xspace}
% \usepackage{booktabs}
% \usepackage{multirow}
% \usepackage[dvipsnames]{xcolor}
% \usepackage{multirow}

% \usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
% \usepackage[capitalise]{cleveref}
% \usepackage{enumitem}

\iccvfinalcopy
\def\iccvPaperID{6874}
\ificcvfinal\pagestyle{empty}\fi

% suppress annoying warnings
\hfuzz=5.002pt
\hbadness=10000

% \makeatletter
% \renewcommand{\paragraph}{%
%   \@startsection{paragraph}{4}%
%   {\z@}{0.25em}{-1em}%
%   {\normalfont\normalsize\bfseries}%
% }
% \makeatother

% % change skip above the caption
% \setlength{\abovecaptionskip}{2pt plus 3pt minus 2pt}

% \newcommand{\datasetlong}{Replay\xspace}
% \newcommand{\dataset}{Replay\xspace}
% \newcommand{\RSnote}[1]{{\color{purple}{\bf RS$^\circ$: }#1}}
% \newcommand{\degrees}{${}^\circ$}

% \title{\datasetlong: \\
% Multi-modal Multi-view Acted Videos for Casual Holography\\
% \textit{Supplementary material}\vspace{-1.8ex}}
% \author{Roman Shapovalov$^*$
% \qquad Yanir Kleiman$^*$
% \qquad Ignacio Rocco$^*$
% \qquad David Novotny\\
% Andrea Vedaldi
% \qquad Changan Chen$^\dagger$
% \qquad Filippos Kokkinos
% \qquad Ben Graham
% \qquad Natalia Neverova\\
% Meta \qquad UT Austin$^\dagger$ \qquad $^*$\textit{equal contribution}\\
% {\tt\small \{romansh, yanirk, irocco, dnovotny, vedaldi, fkokkinos, benjamingraham, nneverova\}@meta.com}
% %{\tt\small changan@cs.utexas.edu}
% }

%\begin{document}
%\maketitle
\clearpage
\appendix
\ificcvfinal\thispagestyle{empty}\fi

\section{Scene Variety}

The \dataset dataset contains a large variety of scenes in terms of actions, number of participants, environments, and props. The scenes are acted out by a diverse cast of actors of different age, gender, and ethnicity. 
In the overview video and \cref{fig:all_scenes}, we show the representative sample of the different scenes one can find in the dataset.
In \cref{fig:all_sensors}, we show an example of a moment in time that was captured by the twelve visual sensors that we provide for each scene. 
All sensors are temporally synchronized: we provide frame timestamps in the time frame that is shared across sensors of each scene.

% Figure environment removed

% Figure environment removed

\section{Overview video and dynamic results}

We attach a video demonstrating:
\begin{itemize}
\item the diversity of the collected scenes (cf. \Cref{fig:all_scenes}),
\item results of the color calibration (cf. \Cref{fig:cc-before-after}),
\item visualisation of the estimated camera poses and static scene points,
\item the diversity of viewpoints (cf. \Cref{fig:all_sensors}) and foreground segmentation available for benchmark scenes,
\item the results of the novel-view synthesis when moving in space or time.
\end{itemize}

We show the novel-view synthesis rendered in three settings, specified in the slide titles. Below is the description of those.

\paragraph{Fly-around -- Fixed Timestamp.}

In this setting, we train a model on a fly-around stage where the camera used for training goes around the scene.
To render these videos, we generated camera poses by fitting a circle to camera centres from the training data.
The camera wearer went along an elliptical trajectory during the fly-around,
so approximating it with a circle forces the method to extrapolate the views in some parts of the trajectory.
For the models taking time as input, we fix the timestamp to the middle of the sequence.

\paragraph{Acting -- Fixed Timestamp.}

In this setting, we train the model using captures from the frontal cameras during the acting stage that contain substantial actors' motions.
Here we again fit a circle to the centres of cameras used for training and fix the timestamp to the middle of the stage, hence visualisations are static.
This setup shows how well the models are able to decouple viewpoints from motions in time.

\paragraph{Acting -- Fixed Novel Viewpoint.}

We additionally render the acting stage in fixed-viewpoint mode.
We fix the camera pose to the one used for evaluation (\ie, static DSLR-1, held out from training) and use the sub-sampled range of training timestamps.
This setting shows how well the methods can model the geometry and appearence changing in time.
In particular, time-agnostic methods (NeRF and TensoRF) are bound to produce static renders in this mode.
Please note that this is a viewpoint extrapolation scenario, since all training cameras were located at a significant distance from DLSR-1.

\input{supfigure_color_cali}

% {\small\bibliographystyle{ieee_fullname}\bibliography{vedaldi_general,vedaldi_specific,local}}

\end{document}