This section details the results of modeling pairwise comparisons using the methodologies above. We begin by presenting implementation details. Next, we present the results for each paired comparison model and the information about predicting environment perception scores based on environment characteristics.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% IMPLEMENTATION DETAILS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We begin by splitting the available pairwise comparisons into train and test sets (85-15\% split). We run a grid search for each model over tunable hyperparameters and present results for the best model. Table \ref{tab:hyperparameters} shows the best hyperparameters. To evaluate each method, we compute the negative average logarithmic loss:\vspace{-3pt}
\begin{equation}
\textsf{log loss} = -\frac{1}{N}\sum^{N}_{n=1} log(p(y^*)),
\label{eq:metrics_logloss}
\end{equation}
for pairwise comparison output $y^*$, and average accuracy. We note that a 
random predictor's accuracy would be 50\%. Log loss provides a good gauge of model calibration, heavily penalizing models for outcomes it considers improbable. We report evaluation metrics on the test set, averaged over five different seeds.
All models were implemented in Python and are publicly available online\footnote{\url{https://github.com/mncosta/scoring_pairwise}}.

\input{tables/1.hyperparameters}

\input{images/3.sequence_scores}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CYCLING ENVIRONMENT RATING
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Cycling environment rating}

\input{tables/2.models_results}

Table \ref{tab:models_results} shows each model's log loss and accuracy. LSR reveals the lowest log loss but with values close to the TS's. In turn, GP showcases the highest accuracy but with a log loss much higher than that of LSR, meaning that, while it is more accurate, its probability of choosing the winning environment is usually much lower than that of TS or LSR.
We depict the normalized predicted perceived safety scores in Figure~\ref{fig:sequence_scores} for all models, with higher values representing environments perceived as safer. All methods show similar perceived safety score trends, showcasing the lowest scores for the same environments and similar tendencies for the perceived safer ones. We highlight some characteristics by visually inspecting each environment and its predicted score. First, images with non-parked cars (Images 1 and 2) show the lowest score, indicating that the presence of these vehicles decreases the perception of safety. Image 5 has the highest perceived safety score showing a cycle lane and no cars in sight. Images 3 and 4 show average to high scores. While Image 3 shows a cycling lane, it also shows an intersection with other vehicles crossing it. In turn, Image 4 was not taken in an intersection, which was perceived as slightly safer. Additionally, lighting conditions and slight lens distortion play no role in individuals' perception, and only semantic and urban characteristics seem to influence perceptions score. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CYCLING ENVIRONMENT RATING
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Binary classification}

Lastly, we aim to understand if cycling environments can be predicted to be perceived as either safe or unsafe directly from image features. We use XGBoost to perform binary classification on cycling environments, tuning hyperparameters using grid search over a 5-fold cross-validation procedure. Optimal hyperparameters are shown in Table~\ref{tab:hyperparameters}. Given its relatively high accuracy and low log loss, we perform classification using TS scores. To decrease the impact of pictures with few comparisons, we conduct classification only on images whose certainty has reduced past $1/6$ of the initial $\sigma$ value. Images with scores within $[s_L, s_H]$ are considered neutral and removed from this analysis.

Classification accuracy is shown in Figure~\ref{fig:binary_classification_alpha}. When $\alpha=0$, the model has 61.4\% accuracy, reaching an accuracy of 89.5\% when $\alpha=1.5$. While increasing the value of $\alpha$ limits the grouping of environments being perceived as safe or unsafe, it also increases the distinction between the two classes, thus increasing the model's accuracy. For urban planners who seek to massively understand how their cities impact cyclists' perception of risk, this process can be widely adapted to analyze a city's urban form and infrastructure.

\input{images/5.binary_classification_alpha}
