\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\usepackage{multirow}


\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{stfloats}
\usepackage{xcolor}
\usepackage{algorithm}  
\usepackage{algorithmic}
\usepackage[normalem]{ulem}
\usepackage{lscape}
\usepackage{geometry}
\usepackage{caption} 
\usepackage{longtable}
\usepackage{setspace} 

% \usepackage{ulem}
\newcommand{\edit}[1]{\textcolor{red}{#1}}

% Checkmark and cross
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% \makeatletter % "@" is now a normal letter for TeX 
% \def\@makecaption#1#2{% 
%   \vskip\abovecaptionskip 
%   \sbox\@tempboxa{#1 #2}% 
%     {\bfseries #1} #2\par 
%   \vskip\belowcaptionskip} 
% \makeatother 
\usepackage[justification=centering]{caption}
% \captionsetup[figure]{labelfont={bf},labelformat={default},labelsep=period,name={Fig.}}

 \usepackage{lineno}
 


\graphicspath{{media/}}     % organize your images and other figures under media/ folder

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
% \fancyhead[LO]{Running Title for Header}
% \fancyhead[LO]{Yan Gong and Georgina Cosma} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
\title{Neural-based Cross-modal Search and Retrieval of Artwork
%%%% Cite as
%%%% Update your official citation here when published 
% \thanks{\textit{\underline{Citation}}: 
% \textbf{Authors. Title. Pages.... DOI:000000/11111.}} 
}

% \author{
  % The paper has been accepted by Elsevier Pattern Recognition.}

  \author{
  Yan Gong, Georgina Cosma, Axel Finke \\
  Loughborough University \\
  Loughborough\\
  \texttt{\{y.gong2, g.cosma, a.finke\}@lboro.ac.uk}
  }

% \author{
%   Axel Finke \\
%   Department of Mathematical Sciences \\
%   Loughborough University \\
%   Loughborough\\
%   \texttt{\{a.finke\}@lboro.ac.uk}}
  


  %% examples of more authors
%   \And
%   Author3 \\
%   Affiliation \\
%   Univ \\
%   City\\
%   \texttt{email@email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
% }


\begin{document}
\maketitle


\begin{abstract}
Creating an intelligent search and retrieval system for artwork images, particularly paintings, is crucial for documenting cultural heritage, fostering wider public engagement, and advancing artistic analysis and interpretation. Visual-Semantic Embedding (VSE) networks are deep learning models used for information retrieval, which learn joint representations of textual and visual data, enabling 1) cross-modal search and retrieval tasks, such as image-to-text and text-to-image retrieval; and 2) relation-focused retrieval to capture entity relationships and provide more contextually relevant search results. Although VSE networks have played a significant role in cross-modal information retrieval, their application to painting datasets, such as ArtUK, remains unexplored. This paper introduces BoonArt, a VSE-based cross-modal search engine that allows users to search for images using textual queries, and to obtain textual descriptions along with the corresponding images when using image queries. The performance of BoonArt was evaluated using the ArtUK dataset. Experimental evaluations revealed that BoonArt achieved 97\,\% Recall@10 for image-to-text retrieval, and 97.4\,\% Recall@10 for text-to-image Retrieval. By bridging the gap between textual and visual modalities, BoonArt provides a much-improved search performance compared to traditional search engines, such as the one provided by the ArtUK website. BoonArt can be utilised to work with other artwork datasets.

\end{abstract}


% keywords can be removed
\keywords{cross-modal, information retrieval, retrieval of artwork, visual-semantic embedding, search engine.}


\section{Introduction}
Creating a retrieval system for artwork images, particularly paintings, is of paramount importance in documenting cultural heritage, facilitating wider public engagement, and fostering advancements in art analysis and interpretation \cite{castellano2021visual, li2022intuitively}. Existing research for retrieving painting images primarily focuses on using neural networks to classify objects within painting images and match their categories with user queries for retrieval purposes \cite{crowley2014state, crowley2016art, seidenari2017deep}. However, these studies have limitations in fully comprehending the complex semantic information expressed in user queries. While research specific to information retrieval can uncover the underlying meaning of user queries, text-based information retrieval \cite{lowe1999object, zhang2013image, li2011text} lacks the ability to understand the visual content of images, relying solely on textual information such as image tags. On the other hand, cross-modal information retrieval is capable of extracting and comprehending high-level visual semantics in conjunction with textual information, enabling users to obtain more relevant and accurate results \cite{kaur2021comparative}, which is appropriate for applying to retrieval of painting images. Visual-Semantic Embedding (VSE) networks represent state-of-the-art techniques in cross-modal information retrieval. These networks aim to embed image-description pairs into a shared latent space, enabling the computation of similarity scores for image-to-text and text-to-image retrieval tasks \cite{zhang2020context, li2020unicoder}. VSE networks have demonstrated their effectiveness with real-world images in widely used benchmark datasets such as Flickr30K \cite{young2014image}, MS-COCO \cite{lin2014microsoft}, and RefCOCOg \cite{mao2016generation}. Despite their successful application in these datasets, their potential for implementing retrieval of painting images, especially in the context of being integrated into a cross-modal search engine, remains unexplored, representing an untapped area of research.

% Figure environment removed 

In recent years, there have been significant advancements in the field of VSE. 
Faghri et al.\ \cite{faghri2018vse++} introduced an architecture that embeds image region features extracted by the faster R-CNN \cite{anderson2018bottom} and their descriptions into a shared latent space using a fully connected neural network and a Gated Recurrent Units (GRU) network \cite{cho2014learning}, respectively. 
Lee et al.\ \cite{lee2018stacked} augmented VSE networks by employing the attention mechanism to enhance the alignment of image regions with their corresponding words.
Li et al.\ \cite{li2019visual} proposed the visual semantic reasoning network, which leverages the Graph Convolution Network (GCN) \cite{kipf2017semi} to extract high-level visual semantics. 
Chen et al.\ \cite{chen2021learning} unveiled a network employing a generalised pooling operator to formulate an optimal strategy for integrating image and description representations. 
Chen et al.\ \cite{chen2020uniter} introduced a pre-trained network built upon the transformer model \cite{devlin2019bert} that has been pre-trained on four large datasets \cite{lin2014microsoft, krishna2017visual, sharma2018conceptual, ordonez2011im2text}. 
Recently, Radford et al.\ \cite{radford2021learning} proposed a Contrastive Language-Image Pre-training network (CLIP), which leverages 400 million image-description pairs to enable the efficient learning of visual concepts via natural language supervision. 
To overcome the limitation of the pre-trained Vision Transformers (ViTs) for relation-focused cross-modal information retrieval, Gong et al.\ \cite{gong2023vitr} proposed a ViT-Relation-focus network (VITR), which employs a local encoder to reason about relations within image regions. 
This paper primarily focuses on implementing a VSE-based cross-modal search engine designed for retrieving painting images. Figure~\ref{ArtUKdata} showcases a variety of painting images, encompassing different styles including realistic, impressionistic, abstract, and still life paintings. The proposed search engine, named BoonArt, incorporates a VSE network, VITR, enhancing its capability in image-to-text and text-to-image retrieval tasks within the painting images, with a particular emphasis on relation-focused cross-modal information retrieval. Specifically, the contributions are as follows.
\begin{itemize} 
    \item BoonArt search engine has the capability to perform image-to-text and text-to-image retrieval for painting images; that BoonArt allows users to search for painting images using textual queries and retrieve textual descriptions along with their corresponding images using image queries; and that BoonArt benefits from a state-of-the-art VSE network, VITR, which can extract and understand high-level visual semantics to improve retrieval performance and enhance the user experience.
    
    \item BoonArt's performance is evaluated through experiments using the ArtUK dataset. The results demonstrate that BoonArt outperformes the ArtUK search system (from the ArtUK website https://artuk.org) for text-to-image tasks. In particular, BoonArt can use image queries for retrieval. In contrast, the ArtUK search system lacks this capability.
\end{itemize} 

\section{Methodology}

BoonArt excels at retrieving painting images from the ArtUK dataset \cite{elliot2021artuk}.  Users can enable text-to-image requests to search for relevant images based on their textual queries. Additionally, users can perform image-to-text requests to search for descriptions and their corresponding images using image queries.
The BoonArt engine is composed of three main components: the front-end, back-end, and a database (images, textual descriptions, and representations, i.e., embeddings). Each component functions as follows. 

\subsection{Front-end}
As depicted in Figure~\ref{FlowArtUK}, in the front-end, users are presented with two options for querying: they can either input a textual query in the provided text box to enable the text-to-image request, or they can upload an image query using the designated upload button to enable the image-to-text request. 
After entering their query, users can initiate the search by clicking the search button. 
The retrieval results are then displayed on the interface, allowing users to view and explore the painting images. To provide additional information about the retrieved paintings, users can click on the links associated with the images, which will open a new page on the source website of each painting. 
Users can input their queries with high-level semantics, such as a focus on relations, and BoonArt will provide accurate search results based on the semantics of their queries.

% Figure environment removed 

\subsection{Back-end}

The core of BoonArt's back-end is a VSE network, and the process of back-end is shown as Figure~\ref{BackEndArtUK}. 
For the text-to-image request, the VSE network compares the textual query with all the images in the dataset. It generates similarity scores for each image and ranks them accordingly. The front-end then displays the ranked images. 
Similarly, for the image-to-text request, the VSE network compares the image query with all the descriptions in the dataset. It calculates similarity scores for each description and ranks them accordingly. The front-end displays the images corresponding to the ranked descriptions.

BoonArt employs the state-of-the-art VSE network VITR \cite{gong2023vitr}. 
VITR consists of a text encoder for encoding descriptions as global and local representations, a ViT encoder for encoding images as global representations, a local encoder for encoding images as local representations for relational reasoning, and a fusion module that fuses the representations from the encoders to output the similarity score between the image and the description. 
VITR takes the text and ViT encoders from CLIP to obtain pre-trained knowledge from an extensive dataset of 400 million image-description pairs. Additionally, VITR has been fine-tuned on the RefCOCOg dataset, which enhances the network's ability to learn the reasoning relations of image regions to improve performance in relation-focused cross-modal information retrieval tasks. 

% Figure environment removed 

\subsection{Database}
The database is built upon the ArtUK dataset \cite{elliot2021artuk}, which consists of 6\,783 pairs of painting images and their descriptions, sourced from the ArtUK website (https://artuk.org).   
To optimise retrieval time, the representations for images and descriptions needed by the VSE network have been pre-encoded and stored. The following files were created and are available in the database: 
1) ArtUKimGloRp.npy (13.0\,MB) stores the global representations of images; 
2) ArtUKimLocRp.npy (1.7\,GB) stores the local representations of images; 
3) ArtUKdeGloRp.npy (13.0\,MB) stores the global representations of descriptions; 
and 4) ArtUKdeLocRp.npy (999.8\,MB) file stores the local representations of descriptions. 
By directly accessing the saved representation values from these files, the back-end eliminates the need for encoding images and descriptions during the retrieval process, resulting in faster retrieval.

\section{Experiments}
\subsection{Implementation Details}
BoonArt can function with a minimum requirement of a single NVIDIA RTX 3080 graphics card. The integrated VSE network of BoonArt is VITR, which leverages the encoder of `ViT-L/14' from CLIP. To maintain generalizability in real-world image scenarios, BoonArt's VITR employs zero-shot learning on the ArtUK dataset. 

% Figure environment removed 

% Figure environment removed 

\subsection{A Comparison of BoonArt and the ArtUK Search System for Text-to-Image Retrieval}

\begin{table}[ht]
\caption{Performance of BoonArt with the ArtUKA search system on a set of textual queries.}
\label{TableComparisonBoonArtArtUK}
\centering
% \resizebox{\linewidth}{!}{
\begin{tabular}{lcc}
\hline
% Query & \begin{tabular}[c]{@{}c@{}}BoonArt's\\Result\end{tabular} & \begin{tabular}[c]{@{}c@{}}ArtUK's\\Result\end{tabular} \\ \hline
Query & \begin{tabular}[c]{@{}c@{}}BoonArt\end{tabular} & \begin{tabular}[c]{@{}c@{}}ArtUK\end{tabular} \\ \hline
1. a cow in the room & \cmark & \xmark \\
2. a red cow in the room &  \cmark & \xmark  \\
3. airplane flying with another airplane &  \cmark & \xmark  \\
4. a man sitting with his dog &  \cmark & \xmark  \\
5. train in the town &  \cmark & \xmark  \\
6. a man riding a horse with another horse &  \cmark & \xmark  \\
7. a brown dog with a white dog &  \cmark & \xmark  \\
8. white car on the street &  \cmark & \xmark  \\
9. impression painting of river in a town &  \cmark & \xmark  \\
10. impression painting of sheep in the rain &  \cmark & \xmark  \\
11. impression painting of trees and horse &  \cmark & \xmark  \\
12. impression painting of ruin in the landscape &  \cmark & \xmark  \\
13. abstract painting of bird on the table &  \cmark & \xmark  \\
14. abstract painting of a boat behinds another &  \cmark & \xmark  \\
15. abstract painting of a person with red hair &  \cmark & \xmark  \\
16. abstract painting of a town in the dark night &  \cmark & \xmark  \\
17. still life of flower &  \cmark & \xmark  \\
18. still life of yellow flower &  \cmark & \xmark  \\
19. still life of cat on the chair &  \cmark & \xmark  \\
20. still life of fruits on the blue tablecloth &  \cmark & \xmark  \\ \hline
\end{tabular}
% }
\end{table}

To compare the performance between BoonArt and the ArtUK search system (from the ArtUK website) for text-to-image retrieval, a set of 20 textual queries was generated, as shown in Table~\ref{TableComparisonBoonArtArtUK}. These queries encompass high-level semantics, including relations. Specifically, there are four queries focused on impression paintings, four queries on abstract paintings, four queries on still life paintings, and an additional eight queries without any specific limitations. The top-ranked retrieved results by BoonArt and the ArtUK search system for these queries are presented in Figure~\ref{FigureComparisonBoonArtArtUK}. For instance, when queried with `a red cow in the room', BoonArt retrieves the relevant painting image. However, the ArtUK search system only identifies an image with the color red, which is irrelevant to the query. Similarly, when queried with `a man sitting with his dog', BoonArt accurately retrieves a relevant painting image. In contrast, the ArtUK search system only finds an image with a man and his dog, overlooking the critical relation of `sitting with' in the image. 
Figure~\ref{FigureComparisonBoonArtArtUK} highlights the performance of BoonArt in capturing high-level semantics from painting images. 

\subsection{BoonArt's Capability for Image-to-Text Retrieval}

The ArtUK search system does not support the use of image queries, therefore, only the results of BoonArt for image-to-text retrieval are presented. BoonArt utilised eight real-world image queries to retrieve the top-ranked descriptions and their corresponding images from the ArtUK dataset, as shown in Figure~\ref{FigureComparisonBoonArtArtUKi2t}. 
For example, when a query image depicting two trains in a station was provided, BoonArt successfully retrieved the relevant description `two trains in a station' and displayed the corresponding image. Similarly, when a query image of a cat on a chair was given, BoonArt retrieved the relevant description `cat on a chair' along with its corresponding image. These results demonstrate the effectiveness of BoonArt in performing image-to-text retrieval and in facilitating the exploration using visual queries. 

\subsection{Quantitative Results of Cross-modal Information Retrieval with BoonArt} \label{Quantitative Results}

To conduct experiments with quantitative results, the ArtUK dataset was randomly partitioned into three subsets: 5\,783 for training, 500 for validation, and an additional 500 for testing. The evaluation metric used is Recall@$K$ for cross-modal information retrieval experiments, which measures the percentage of relevant items in the top $K$ retrieved results \cite{gong2021limitations, saracevic1995evaluation}. The objective is to retrieve at least one relevant item from a given list, and the average Recall is calculated across all evaluated queries to assess retrieval performance.

Table~\ref{BoonArtUKResults} presents the performance of BoonArt for image-to-text and text-to-image retrieval tasks on the ArtUK test set. Additionally, to highlight the best performance, the results of BoonArt fine-tuned on the training set are also included in Table~\ref{BoonArtUKResults}.  

According to Table~\ref{BoonArtUKResults}, BoonArt (zero-shot) achieved an average Recall@10 value of 93.0\,\% for image-to-text retrieval and 94.2\,\% for text-to-image retrieval in the ArtUK test set. On the other hand, BoonArt (fine-tuned) achieved an average Recall@10 value of 97.0\,\% for image-to-text retrieval and 97.4\,\% for text-to-image retrieval. These findings highlight BoonArt's capability in achieving successful cross-modal information retrieval in the ArtUK dataset.

\begin{table}[htbp]
\caption{Results of cross-modal information retrieval networks on the ArtUK test set. Table shows average Recall@$K$ (\%) values.}
\label{BoonArtUKResults}
\centering
% \resizebox{\linewidth}{!}{
\begin{tabular}{lccccccc}
\hline
\multirow{2}{*}{Engine} & \multirow{2}{*}{Method} & \multicolumn{3}{c}{Image-to-Text [\%]} & \multicolumn{3}{c}{Text-to-Image [\%]} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8}
                         & & R@1           & R@5           & R@10          & R@1           & R@5           & R@10          \\ \hline
BoonArt           & zero-shot &  68.8  & 88.0 & 93.0 & 71.2 & 90.2 & 94.2  \\

BoonArt            &  fine-tuned & 77.4 & 93.4 & 97.0 & 80.8 & 94.6 & 97.4  \\
\hline
\end{tabular}
% }
\end{table}

\subsection{Evaluation of Retrieval Time}
The average retrieval time for each query by BoonArt was experimentally measured. For the image-to-text retrieval task, which involved processing 6\,783 textual descriptions, the average retrieval time for one query was 0.18 seconds. Similarly, for the text-to-image retrieval task, which included 6\,783 images, the average retrieval time for one query was 0.40 seconds. 

\section{Conclusion}
This paper presents BoonArt, a cross-modal search engine specifically designed for retrieving painting images. To enhance the user experience, BoonArt excels at image-to-text and text-to-image retrieval by extracting high-level visual semantics. It integrates the state-of-the-art VSE network VITR, which focuses on relation-focused cross-modal information retrieval. Extensive experiments were conducted to evaluate BoonArt's performance, demonstrating its ability to outperform the ArtUK search system in text-to-image retrieval tasks. Furthermore, BoonArt surpasses the limitations of the ArtUK search system by enabling image queries, allowing users to retrieve textual descriptions and their corresponding images. BoonArt enhances the retrieval performance of painting images by bridging the gap between textual and visual modalities, resulting in an improved user experience. Currently, the database includes the ArtUK dataset, but it can be extended to work with new datasets. In future work, the database will be expanded to incorporate various datasets of artworks, and the engine will be evaluated using these datasets.

%Bibliography
\bibliographystyle{unsrt}  
\bibliography{egbib}  
% \begin{thebibliography}{10} 

% \expandafter\ifx\csname url\endcsname\relax 

%   \def\url#1{\texttt{#1}}\fi 

% \expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi 

% \expandafter\ifx\csname href\endcsname\relax 

%   \def\href#1#2{#2} \def\path#1{#1}\fi 

 

% \bibitem{gong2021limitations} 

% Y.~Gong, G.~Cosma, H.~Fang, On the limitations of visual-semantic embedding 

%   networks for image-to-text information retrieval, Journal of Imaging 7~(8) 

%   (2021) 125. 

 

% \bibitem{shu2021scalable} 

% X.~Shu, G.~Zhao, Scalable multi-label canonical correlation analysis for 

%   cross-modal retrieval, Pattern Recognition 115 (2021) 107905. 

 

% \bibitem{faghri2018vse++} 

% F.~Faghri, D.~J. Fleet, J.~R. Kiros, S.~Fidler, {VSE++}: Improving 

%   visual-semantic embeddings with hard negatives, in: Proceedings of the 

%   British Machine Vision Conference, 2018, p.~12. 

 

% \bibitem{diao2021similarity} 

% H.~Diao, Y.~Zhang, L.~Ma, H.~Lu, Similarity reasoning and filtration for 

%   image-text matching, in: Proceedings of the AAAI Conference on Artificial 

%   Intelligence, Vol.~35, 2021, pp. 1218--1226. 

 

% \bibitem{hu2021cross} 

% P.~Hu, X.~Peng, H.~Zhu, J.~Lin, L.~Zhen, W.~Wang, D.~Peng, Cross-modal 

%   discriminant adversarial network, Pattern Recognition 112 (2021) 107734. 

 

% \bibitem{lin2014microsoft} 

% T.-Y. Lin, M.~Maire, S.~Belongie, J.~Hays, P.~Perona, D.~Ramanan, 

%   P.~Doll{\'a}r, C.~L. Zitnick, Microsoft {COCO}: Common objects in context, 

%   in: European conference on computer vision, Springer, 2014, pp. 740--755. 

 

% \bibitem{young2014image} 

% P.~Young, A.~Lai, M.~Hodosh, J.~Hockenmaier, From image descriptions to visual 

%   denotations: New similarity metrics for semantic inference over event 

%   descriptions, Transactions of the Association for Computational Linguistics 2 

%   (2014) 67--78. 

 

% \bibitem{chen2021learning} 

% J.~Chen, H.~Hu, H.~Wu, Y.~Jiang, C.~Wang, Learning the best pooling strategy 

%   for visual semantic embedding, in: Proceedings of the IEEE conference on 

%   computer vision and pattern recognition, 2021, pp. 15789--15798. 

 

% \bibitem{song2021deep} 

% G.~Song, X.~Tan, J.~Zhao, M.~Yang, Deep robust multilevel semantic hashing for 

%   multi-label cross-modal retrieval, Pattern Recognition 120 (2021) 108084. 

 

% \bibitem{wei2021universal} 

% J.~Wei, Y.~Yang, X.~Xu, X.~Zhu, H.~T. Shen, Universal weighting metric learning 

%   for cross-modal retrieval, IEEE Transactions on Pattern Analysis and Machine 

%   Intelligence (2021). 

 

% \bibitem{anderson2018bottom} 

% P.~Anderson, X.~He, C.~Buehler, D.~Teney, M.~Johnson, S.~Gould, L.~Zhang, 

%   Bottom-up and top-down attention for image captioning and visual question 

%   answering, in: Proceedings of the IEEE conference on computer vision and 

%   pattern recognition, 2018, pp. 6077--6086. 

 

% \bibitem{cho2014learning} 

% K.~Cho, B.~Van~Merri{\"e}nboer, C.~Gulcehre, D.~Bahdanau, F.~Bougares, 

%   H.~Schwenk, Y.~Bengio, Learning phrase representations using {RNN} 

%   encoder-decoder for statistical machine translation, in: Proceedings of the 

%   2014 Conference on Empirical Methods in Natural Language Processing, 2014, 

%   pp. 1724--1734. 

 

% \bibitem{li2022image} 

% K.~Li, Y.~Zhang, K.~Li, Y.~Li, Y.~Fu, Image-text embedding learning via visual 

%   and textual semantic reasoning, IEEE Transactions on Pattern Analysis and 

%   Machine Intelligence (2022). 

 

% \bibitem{zhang2019graph} 

% S.~Zhang, H.~Tong, J.~Xu, R.~Maciejewski, Graph convolutional networks: a 

%   comprehensive review, Computational Social Networks 6~(1) (2019) 1--23. 

 

% \bibitem{liu2020graph} 

% C.~Liu, Z.~Mao, T.~Zhang, H.~Xie, B.~Wang, Y.~Zhang, Graph structured network 

%   for image-text matching, in: Proceedings of the IEEE conference on computer 

%   vision and pattern recognition, 2020, pp. 10921--10930. 

 

% \bibitem{li2020unicoder} 

% G.~Li, N.~Duan, Y.~Fang, M.~Gong, D.~Jiang, {Unicoder-VL}: A universal encoder 

%   for vision and language by cross-modal pre-training, in: Proceedings of the 

%   AAAI Conference on Artificial Intelligence, Vol.~34, 2020, pp. 11336--11344. 

 

% \bibitem{duan2022ms2gah} 

% Y.~Duan, N.~Chen, P.~Zhang, N.~Kumar, L.~Chang, W.~Wen, {MS$^{2}$GAH:} Multi-label 

%   semantic supervised graph attention hashing for robust cross-modal retrieval, 

%   Pattern Recognition 128 (2022) 108676. 

 

% \bibitem{liu2019mtfh} 

% X.~Liu, Z.~Hu, H.~Ling, Y.-m. Cheung, {MTFH}: A matrix tri-factorization 

%   hashing framework for efficient cross-modal retrieval, IEEE transactions on 

%   pattern analysis and machine intelligence 43~(3) (2019) 964--981. 

 

% \bibitem{karpathy2015deep} 

% A.~Karpathy, L.~Fei-Fei, Deep visual-semantic alignments for generating image 

%   descriptions, in: Proceedings of the IEEE conference on computer vision and 

%   pattern recognition, 2015, pp. 3128--3137. 

 

% \bibitem{liu2018learning} 

% Y.~Liu, L.~Liu, Y.~Guo, M.~S. Lew, Learning visual and textual representations 

%   for multimodal matching and classification, Pattern Recognition 84 (2018) 

%   51--67. 

 

% \bibitem{zhang2022multi} 

% L.~Zhang, X.~Wu, Multi-task framework based on feature separation and 

%   reconstruction for cross-modal retrieval, Pattern Recognition 122 (2022) 

%   108217. 

 

% \bibitem{liu2019cyclematch} 

% Y.~Liu, Y.~Guo, L.~Liu, E.~M. Bakker, M.~S. Lew, {CycleMatch}: A cycle-consistent 

%   embedding network for image-text matching, Pattern Recognition 93 (2019) 

%   365--379. 

 

% \bibitem{song2019polysemous} 

% Y.~Song, M.~Soleymani, Polysemous visual-semantic embedding for cross-modal 

%   retrieval, in: Proceedings of the IEEE conference on computer vision and 

%   pattern recognition, 2019, pp. 1979--1988. 

 

% \bibitem{lee2018stacked} 

% K.-H. Lee, X.~Chen, G.~Hua, H.~Hu, X.~He, Stacked cross attention for 

%   image-text matching, in: Proceedings of the European Conference on Computer 

%   Vision (ECCV), 2018, pp. 201--216. 

 

% \bibitem{wang2020stacked} 

% H.~Wang, Z.~Ji, Z.~Lin, Y.~Pang, X.~Li, Stacked squeeze-and-excitation 

%   recurrent residual network for visual-semantic matching, Pattern Recognition 

%   105 (2020) 107359. 

 

% \bibitem{devlin2019bert} 

% J.~Devlin, M.-W. Chang, K.~Lee, K.~Toutanova, {BERT}: Pre-training of deep 

%   bidirectional transformers for language understanding, in: Proceedings of 

%   Conference of the North American Chapter of the Association for Computational 

%   Linguistics, Vol. 1: Long and Short Papers, 2019, pp. 4171--4186. 

 

% \bibitem{lipovetsky2009pca} 

% S.~Lipovetsky, {PCA} and {SVD} with nonnegative loadings, Pattern Recognition 

%   42~(1) (2009) 68--76. 

 

% \bibitem{furnas2017information} 

% G.~W. Furnas, S.~Deerwester, S.~T. Durnais, T.~K. Landauer, R.~A. Harshman, 

%   L.~A. Streeter, K.~E. Lochbaum, Information retrieval using a singular value 

%   decomposition model of latent semantic structure, in: ACM SIGIR Forum, 

%   Vol.~51, ACM New York, NY, USA, 2017, pp. 90--105. 

 

% \bibitem{maltoudoglou2022well} 

% L.~Maltoudoglou, A.~Paisios, L.~Lenc, J.~Mart{\'\i}nek, P.~Kr{\'a}l, 

%   H.~Papadopoulos, Well-calibrated confidence measures for multi-label text 

%   classification with a large number of labels, Pattern Recognition 122 (2022) 

%   108271. 

 

% \bibitem{haddi2013role} 

% E.~Haddi, X.~Liu, Y.~Shi, The role of text pre-processing in sentiment 

%   analysis, Procedia computer science 17 (2013) 26--32. 

 

% \bibitem{cosma2019feature} 

% G.~Cosma, T.~M. Mcginnity, Feature extraction and classification using leading 

%   eigenvectors: applications to biomedical and multi-modal mhealth data, IEEE 

%   Access 7 (2019) 107400--107412. 

 

% \bibitem{grubinger2006iapr} 

% M.~Grubinger, P.~Clough, H.~M{\"u}ller, T.~Deselaers, The {IAPR TC-12} 

%   benchmark: A new evaluation resource for visual information systems, in: 

%   International workshop ontoImage, Vol.~2, 2006, pp. 13--23. 

 

% \bibitem{martineau2009delta} 

% J.~Martineau, T.~Finin, Delta {TFIDF}: An improved feature space for sentiment 

%   analysis, in: Proceedings of the International AAAI Conference on Web and 

%   Social Media, Vol.~3, 2009, pp. 258--261. 

 

% \bibitem{krizhevsky2012imagenet} 

% A.~Krizhevsky, I.~Sutskever, G.~E. Hinton, Imagenet classification with deep 

%   convolutional neural networks, Advances in neural information processing 

%   systems 25 (2012) 1097--1105. 

 

% \bibitem{krishna2017visual} 

% R.~Krishna, Y.~Zhu, O.~Groth, J.~Johnson, K.~Hata, J.~Kravitz, S.~Chen, 

%   Y.~Kalantidis, L.-J. Li, D.~A. Shamma, et~al., Visual genome: Connecting 

%   language and vision using crowdsourced dense image annotations, International 

%   journal of computer vision 123~(1) (2017) 32--73. 

 

% \end{thebibliography} 




\end{document}
