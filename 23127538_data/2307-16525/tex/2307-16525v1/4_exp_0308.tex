
\section{Experiments}
We conduct extensive experiments to evaluate the performance of ViECap in diverse zero-shot image captioning settings, including 1) cross-domain captioning, 2) in-domain captioning, and 3) data-efficient captioning. The experiments are organized as follows: In Sec.~\ref{sec:sec51}, we assess the transferability of ViECap through the cross-domain setting. Here, the model is trained on a corpus from the source domain and evaluated on a target domain. In Sec.~\ref{sec:sec52}, we focus on the generalizability of ViECap under the in-domain scenario, where the model is trained and evaluated on the same dataset. We conduct data-efficient experiments to assess the applicability of our model in low-data scenarios in Sec.~\ref{sec:sec53}. In Sec.~\ref{sec:sec54}, we perform various ablation experiments to assess the effectiveness of entity-aware decoding. Furthermore, we qualitatively evaluate ViECap in Sec.~\ref{sec:sec55}.

\paragraph{Implementation Details.} We use CLIP-ViT-B/32 as our backbone. The language model is GPT-2$_{\rm base}$ implemented by Wolf et al.~\cite{hugface}. The projector comprises an 8-layer transformer with 8 attention heads and a hidden size of 768. The length of learnable soft prompts is set to 10. During training, we freeze the CLIP text encoder and only train GPT-2 and projector using AdamW~\cite{AdamW} optimizer for all experiments. For caption generation, we use beam search with a beam size of 5. Details are shown in the Appendix.

\paragraph{Datasets and Metrics.} We conduct experiments on four widely used image captioning benchmarks, \ie, NoCaps~\cite{NoCaps}, COCO~\cite{MSCOCO,MSCOCO1}, Flikcr30k~\cite{Flickr30k}, and FlickrStyle10K~\cite{StyleNet}. For COCO and Flickr30k, we follow the commonly used Karpathy split~\cite{alignmentcaption}. For NoCaps, we train our model on the COCO training set and report the results on the validation set, as suggested by OSCAR~\cite{Oscar}. As for FlickrStyle10K,  we follow MemCap~\cite{MemCap}, randomly sampling 6,000 captions as our training set and using the remaining image-text pairs for testing. We report results with common used captioning metrics BLEU@n (B@n)~\cite{BLEU}, METEOR (M)~\cite{METEOR}, CIDEr (C)~\cite{CIDEr} and SPICE (S)~\cite{SPICE}. Refer to the Appendix for details about these datasets.

\paragraph{Methods.} We include several captioning methods as follows: 1) BUTD~\cite{BUTD} and OSCAR~\cite{Oscar} as classic supervised methods, 2) ClipCap~\cite{ClipCap}, I-Tuning~\cite{ITuning}, and SmallCap~\cite{SMALLCAP} as lightweight paired captioning methods that utilize GPT-2 for CLIP-based captioning, 3) ZeroCap~\cite{ZeroCap} as a training-free method, 4) StyleNet~\cite{StyleNet} and MemCap~\cite{MemCap} as classic methods for style captioning, and 5) MAGIC \cite{MAGIC}, CapDec \cite{CapDec}, and DeCap \cite{DECAP} as text-only training methods, which are in line with our work.
%
Specifically, MAGIC employs late-guidance decoding. CapDec and DeCap adopt early-guidance decoding, which learns soft prompts for caption generation. Notably, DeCap leverages an additional memory bank, and CapDec serves as our baseline.

\input{tables/in_domain_coco}
\input{tables/in_domain_flickrstyle}

\subsection{Cross-Domain Captioning}
\label{sec:sec51}

In this section, we demonstrate the transferability of ViECap in cross-domain captioning. We evaluate ViECap's ability to describe novel entities in images by training it on the COCO training set and testing on the NoCaps validation set without any additional fine-tuning.
%
As Tab.~\ref{tab:nocaps} shows, ViECap outperforms all other text-only methods by a large margin and even achieves competitive performance compared to some supervised methods in the \textit{out-of-domain} and \textit{Overall} setting, indicating that incorporating the entities-aware hard prompt is beneficial for the model to describe unseen entities.
%
While other methods experience a notable drop in CIDEr score from the \textit{in-domain} to \textit{out-of-domain} setting in NoCaps, ViECap maintains a minimal fluctuation across different domains, showcasing the remarkable transferability of our model.
%
In real-world scenarios, the target domain of images is typically agnostic, making the evaluation based on the \textit{Overall} results a better reflection of the models' effectiveness in practical applications. Surprisingly, with text-only corpus, ViECap achieves comparable results to supervised methods, obtaining a CIDEr score of 66.2 compared to 63.8 for OSCAR and 65.8 for ClipCap on the \textit{Overall}. Furthermore, ViECap significantly outperforms the unpaired methods DeCap and CapDec by a large margin of 20.3 CIDEr, demonstrating our model can generate captions with stable quality in diverse domains.

Tab.~\ref{tab:crossdomain} showcases results in more cross-domain settings, where ViECap sets a new state-of-the-art performance on all metrics from Flickr30k to COCO and on most metrics from COCO to Flickr30k.

Both Tab.~\ref{tab:nocaps} and Tab.~\ref{tab:crossdomain} demonstrate the remarkable zero-shot transferability of our model. ViECap is capable of describing images that deviate from the distribution of the training sets, as well as those that do not, making it highly useful when applied in real-world scenarios.

% \begin{table*}
% \begin{center}
% \small
% \setlength{\tabcolsep}{3.0mm}{
% \begin{tabular}{c|c|cccc|cccc}
% \hline \hline
% % \multicolumn{13}{c}{$(A)$ \textbf{In-Domain Captioning}} \\
% % \hline
% \multirow{2}{*}{Methods} & \multirow{2}{*}{Datasets} & \multicolumn{4}{c}{MSCOCO|} & \multicolumn{4}{c}{NoCaps} \\
% ~ & ~ & B@4 & M & C & S & In & Near & Out & Overall\\
% \hline
% % \multicolumn{13}{c}{Supervised Method} \\
% % \hline
% Changpinyo et al. & CC3M & - & - & - & - & 29.2 & 27.5 & 37.3 & 29.7 \\
% Changpinyo et al. & CC12M & - & - & - & - & 20.7 & 24.1 & 41.6 & 27.1 \\
% \hline
% ZeroCap & training-free & 2.6 & 11.5 & 14.6 & 5.5 & - & - & - & - \\
% CLIPRe & CC3M & 4.6 & 13.3 & 25.6 & 9.2 & 23.3 & 26.8 & 36.5 & 28.2 \\
% DeCap & CC3M(1M extra memory) & 8.8 & 16.0 & 42.1 & 10.9 & \textbf{34.8} & 37.7 & 49.9 & 39.7 \\
% CapDec\textcolor{red}{(run)} & w/o hard prompt & - & - & - & - & - & - & - & - \\
% \hline
% Ours & CC3M & \textbf{13.2} & \textbf{18.5} & \textbf{51.8} & \textbf{13.0} & 32.4 & \textbf{40.6} & \textbf{50.5} & - \\
% % \multicolumn{13}{c}{Text-only training, zero-shot on image-text pairs} \\
% % \hline
% % Magic & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0\\
% \hline \hline
% \end{tabular}}
% \end{center}
% \caption{Results.   Ours is better.}
% \end{table*}

\subsection{In-Domain Captioning}
\label{sec:sec52}

To further assess the generalizability of ViECap, we conduct evaluations on COCO, Flickr30k, and FlickrStyle10K in the in-domain setting, where the training and testing data are from the same dataset. As shown in Tab.~\ref{tab:indomain}, our proposed model outperforms CapDec, our baseline method, in most metrics. We attribute this improvement to our entity-aware hard prompt, which explicitly emphasizes infrequent object concepts, thereby mitigating the long-tail problem associated with the existing dataset.
%
It is worth noting that DeCap utilizes a large memory bank to bridge the modality gap, which may not be practical in real-world scenarios. In contrast, our approach achieves comparable performance with an acceptable memory complexity, highlighting the effectiveness of the proposed ViECap. Tab.~\ref{tab:flickrstyle10k} shows that ViECap achieves state-of-the-art performance on FlickStyle10K. These results demonstrate that ViECap can adapt well to diverse style text data, showcasing its versatility and strong generalizability.

\begin{table}
\small
\begin{center}
\setlength{\tabcolsep}{2.0 mm}{
\begin{tabular}{r|c|c|cccc}
\toprule
\multirow{2}{*}{Data} & \multirow{2}{*}{Method} &\multicolumn{1}{c|}{\textbf{COCO}}&\multicolumn{4}{c}{\textbf{NoCaps val}} \\
 & & Test & In & Near & Out & Overall \\
\midrule
\multirow{2}{*}{0.1\%} & CapDec & 24.0 & 13.2 & 11.0 & 6.2 & 10.4 \\
% & ViECap & 36.0 & 17.1 & 14.5 & 11.4 & 14.4 \\
& ViECap & \textbf{32.3} & \textbf{20.9} & \textbf{27.6} & \textbf{34.9} & \textbf{30.2} \\
\midrule
\multirow{2}{*}{1\%} & CapDec & 55.8 & 29.6 & 20.5 & 9.8 & 18.9 \\
% & ViECap &64.2& 36.1 & 39.9 & 40.6 & 41.2\\
& ViECap & \textbf{63.9} & \textbf{34.6} & \textbf{39.9} & \textbf{39.3} & \textbf{40.4}\\
\midrule
\multirow{2}{*}{10\%} & CapDec & \textbf{83.6} & 47.3 & 39.8 & 19.1& 35.4\\
& ViECap & 83.4 & \textbf{45.9} & \textbf{51.8} & \textbf{48.7}  & \textbf{53.3} \\
\midrule
% \multirow{2}{*}{20\%} & CapDec & & & & \\
% & ViECap & \\
% \midrule
\multirow{2}{*}{100\%} & CapDec & 92.7 & 60.1 & 50.2 & 28.7 & 45.9  \\
& ViECap & \textbf{92.9} & \textbf{61.1} & \textbf{64.3} & \textbf{65.0} & \textbf{66.2} \\
\bottomrule
\end{tabular}}
\end{center}
\caption{Data-efficient captioning results.}
\label{tab:fewshot}
\end{table}

\subsection{Data-Efficient Captioning}
\label{sec:sec53}

In this section, we explore ViECap's capability to learn from low-data scenarios. Specifically, we randomly sample different scales of data from the COCO training set to train ViECap. For simplicity, we leverage In, Near, and Out to denote \textit{in-domain}, \textit{near-domain}, and \textit{out-of-domain}, respectively.
%
As shown in Tab.~\ref{tab:fewshot}, ViECap outperforms CapDec across all data scales. Even with as little as 0.1\% of the data, ViECap can still generate reasonable captions and maintain transferability (CIDEr: 32.3 on the COCO testing set vs. 30.2 on the \textit{Overall} of NoCaps validation set), suggesting ViECap is data-efficient and applicable in low-data settings.

\subsection{Ablation Studies}
\label{sec:sec54}

\input{tables/ablation}

% Figure environment removed

We conduct comprehensive ablation studies to explore the influence of entities and prompt structures on our model. Additionally, we evaluate the advantages of using a larger-scale language model for ViECap. In all these experiments, we assess the performance of our model on both the COCO testing set and the NoCaps validation set.

\paragraph{Masking Rate.} We begin by investigating the impact of entities on ViECap by randomly masking entities at different rates. As shown in Tab.~\ref{tab:masking}, incorporating entities enhances our model's ability to perceive unseen entities (from $28.7$ to $53.1$).
%
However, as mentioned before, the model may learn a shortcut from detected entities, leading to a rapid decline in ID performance (from $92.7$ to $53.2$). The CIDEr score on the COCO testing set gradually increases as the masking rate increases, indicating that entity masking can prevent ViECap from relying heavily on entities.
%
For the NoCaps validation set, the performance of ViECap first increases and then decreases, showing that a moderate entity masking rate benefits caption prediction in unseen scenarios. The results of this experiment demonstrate that the proposed entity masking strategy boosts the captioning performance of ViECap across diverse scenes.

\paragraph{Prompts.} We then explore the impact of different prompt generation methods on the performance of ViECap. As shown in Tab.~\ref{tab:prompts}, learning only soft prompts leads to overfitting to in-domain captioning, resulting in poor performance when describing novel entities. Incorporating hard prompts improves captioning performance for unseen images, but solely modeling entity information leads to reduced performance on in-domain captioning.
%
When combining soft and hard prompts, we achieve comparable ID performance with soft prompts-only methods and superior performance on OOD scenarios. Additionally, we find that the order of soft and hard prompts does not affect ViECap's performance.
%
To construct hard prompts with visual entities, we leverage CLIP-based retrieval, where the accuracy of retrieval benefits from the prompt ensemble~\footnote{The prompt engineering is released by OpenAI.~\url{https://github.com/openai/CLIP/blob/main/notebooks}}.

\input{tables/lms}

\paragraph{Scaling up ViECap.} We assess diverse pre-trained LLMs, ranging from GPT-2 to OPT~\cite{OPT}, to investigate the impact of scaling up language models on the capability of ViECap to describe novel entities in images.
%
The results are shown in Tab.~\ref{scaleup}. As the model parameters increase, the performance of ViECap continues to improve. Notably, we freeze the language model for more effective training. To our surprise, ViECap can effectively leverage the information from the language model without the need for further fine-tuning.

\subsection{Qualitative Evaluation}
\label{sec:sec55}

Fig.~\ref{fig:visual} displays ground truth captions and captions generated by CapDec and ViECap trained on COCO. Images are from the \textit{out-of-domain} set in the NoCaps validation set, which contains unseen entities. In the first image in Fig.~\ref{fig:visual}, ViECap correctly recognizes the specific entity ``jay" while CapDec mistakenly identifies a generic entity ``bird". Similar outcomes can be observed in the other instances shown in the figure.
%
CapDec exhibits object hallucination in its textual descriptions, while ViECap demonstrates the ability to generate high-quality descriptions of entities in novel scenarios, illustrating the effectiveness of using hard prompts to guide LLMs in attending to entities in images. More visualization results can be found in the Appendix.