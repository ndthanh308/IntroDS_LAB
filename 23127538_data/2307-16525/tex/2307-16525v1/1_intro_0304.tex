\section{Introduction}

% Figure environment removed

Large-scale pre-trained vision-language models (VLMs) like CLIP~\cite{CLIP} and ALIGN~\cite{ALIGN} showcase impressive zero-shot transferability in various discriminative downstream tasks (\eg, classification~\cite{CLIP}, segmentation~\cite{Lseg,GroupViT}, and detection~\cite{ViLD,GLIP}). However, effectively adapting these pre-trained VLMs into zero-shot generative tasks (\eg, text and image generation) remains an open question that requires further exploration.
%
Recently, some works~\cite{ZeroCap,MAGIC} have leveraged large language models (LLMs), \eg, GPT~\cite{GPT2,GPT3}, to achieve CLIP-based zero-shot image-to-text generation. They follow a late-guidance paradigm where visual information is injected after completing word prediction. However, the weak visual guidance in this paradigm often results in modality bias, \ie, the language prior in LLMs dominates the decoding process and therefore generates descriptions that are unrelated to the corresponding images.
%
Fig.~\ref{fig:intro}(a) shows incorrect predictions made by late-guidance decoding, \eg, even if \textit{``jump''} and \textit{``cobblestone''} are unrelated to the image, they finally appear in the predictions due to their close association with predicted words \textit{``skateboarder''} and \textit{``snowy"}.
%
Similarly, another example in Fig.~\ref{fig:intro}(b) shows that \textit{``donut''} and \textit{``rocks''} are primarily generated by language prior instead of visual guidance. 

Early-guidance methods~\cite{DECAP,CapDec,SMs} provide explicit guidance for word generation in LLMs by prefixing visual prompts to the text tokens. Typically, visual prompts are projected from the CLIP image embedding using a learnable projector.
%
This early-guidance paradigm significantly alleviates the modality bias and boosts the alignments between the image and the generated captions. 
%
However, the learnable (soft) visual prompts are prone to overfitting when trained on a limited corpus, leading to poor performance in describing a diverse range of objects (visual entities). This, in turn, may cause object hallucination in the generated captions.
%
Specifically, when transferring these models to unseen scenarios beyond the training corpus, novel entities are often misrecognized as similar entities frequently appearing in the training corpus. 
%
As Fig.~\ref{fig:intro} shows, early-guidance decoding is capable of understanding in-domain (ID) images but tends to hallucinate entities that do not actually exist in out-of-domain (OOD) images (\ie, hallucinating \textit{``sea turtle''} with \textit{``surfboard''}, where \textit{``surfboard''} frequently appears in the training corpus). Consequently, the transferability of the well-learned CLIP latent space is degraded into current decoding strategies, significantly limiting their applicability in real-world scenarios. 
%
We further validate the observed modality bias and object hallucination issues when adapting pre-trained VLMs and LLMs for image-to-text generation through experiments in Sec.~\ref{sec:sec3}.

To address the observed issues, we propose ViECap, which incorporates entity-aware hard prompts to compensate for the degradation of the CLIP latent space caused by learning soft prompts on a specific training corpus. This method is motivated by our observation that the CLIP-based entity classifier can accurately classify both ID and OOD images (\eg, \textit{``snowboard''} and \textit{``sea turtle''} in Fig.~\ref{fig:intro}).
%
The entity-aware hard prompts enable transferable language decoding from the CLIP latent space. Fig.~\ref{fig:intro} shows that the proposed entity-aware decoding approach is capable of describing both seen and unseen entities in diverse images.
%
Specifically, ViECap builds on early-guidance decoding methods, \eg, CapDec~\cite{CapDec}.
% Specifically, ViECap builds on CapDec~\cite{CapDec}, an early-guidance decoding method.
Unlike these models, which can only describe entities present in the training corpus, our model can generate captions in diverse scenarios.
%
Following CapDec, we train ViECap only using text data. The entity-aware hard prompt is the critical design enabling the transferability of our model to diverse captioning scenarios. The hard prompts, constructed by nouns extracted from texts during training or entities retrieved from images during inference, can prompt the LLMs to attend training-agnostic entities based on open vocabulary retrieval through CLIP.
%
As we find that a naive integration of entities pushes ViECap to learn a copy-then-paste shortcut (\ie, directly copying the entities to captions), we introduce a simple yet efficient entity masking strategy when incorporating the entity-aware hard prompts into early-guidance decoding.

We extensively evaluate ViECap on four benchmarks, NoCaps~\cite{NoCaps}, COCO~\cite{MSCOCO,MSCOCO1}, Flickr30k~\cite{Flickr30k}, and FlickrStyle10K~\cite{StyleNet}. The experimental results demonstrate that ViECap outperforms all other text-only methods and sets a new state-of-the-art in the cross-domain (transferable) setting while remaining competitive with them in the in-domain setting. 
%
In out-of-domain scenarios (NoCaps), we achieve a margin of 39.2 and 36.3 improvements, respectively, compared to DeCap and CapDec. We even surpass some supervised methods, indicating our model generalizes well to novel entities.
%
In the experiment on FlickrStyle10K, ViECap effectively generates captions in different styles corresponding to the styles of the training set.
%
Additionally, the data-efficient experiment shows ViECap's applicability in low-data settings, further highlighting its versatility and effectiveness across various scenarios.

To summarize, our contributions are as follows: 
% 1) We identify modality bias and object hallucination that appears in CLIP-based zero-shot captioning assisted by LLMs, providing timely insights for the generalizability problem.
1) We shed light on the observations and underlying reasons behind the degraded generalizability when adapting pre-trained VLMs and LLMs into image-to-text generation, \ie, modality bias and object hallucination, providing timely and valuable insights for pre-trained large-scale model adaptation.
%
2) We introduce entity-aware decoding to improve the transferability of zero-shot captioning. Specifically, aided by VLMs, we integrate entity-aware hard prompts with entity masking strategy into the decoding process, guiding LLMs to attend both seen and unseen entities.
%
3) Extensive experiments show the remarkable zero-shot transferability of ViECap, even in low-data scenarios.