\section{Related Work}
% Figure environment removed

\paragraph{Supervision in Image Captioning.}
%
We classify image captioning models into supervised and unsupervised methods based on whether the image-text alignment information is provided during training.
%
Supervised image captioning methods~\cite{ShowAttendTell, alignmentcaption, AoA, BUTD, MMT, CIDEr, SCA-CNN} are trained with paired (well-aligned) image-text data and typically adopt the encoder-decoder architecture. Initially, diverse vision backbones (\eg, CNN~\cite{ResNet}, ViT~\cite{ViT}) are utilized to extract visual features, which are then fed into a language decoder (\eg, LSTM~\cite{LSTM}, Transformer~\cite{Transformer}) to generate coherent sentences. Various attention mechanisms~\cite{ShowAttendTell, AoA, ICSemanticAttention, SCA-CNN, KnowingWhenToLook} are commonly designed to capture vision-language alignment cues. However, the high cost associated with collecting paired image-text data limits the applicability of these models.
%
In contrast, Unsupervised image captioning methods~\cite{laina2019towards, feng2019unsupervised} train the model using unpaired image-text data and primarily rely on visual concepts as anchor points to establish pseudo image-text alignment.
%
Our proposed approach, on the other hand, only requires text data for model training. Compared to previous methods, our method further reduces the data collection cost while exhibiting superior efficiency by eliminating the need for image encoding during training.

\paragraph{Zero-shot Image Captioning.}
%
Zero-shot image captioning aims to generate image captions without relying on human-annotated data~\cite{DECAP}. Some methods~\cite{CC12M, SimVLM} in this area pre-train the model on large-scale weak image-text pairs and then evaluate the model on target benchmarks without further fine-tuning. 
%
Another set of methods~\cite{ZeroCap, CapDec, DECAP, MAGIC, SMs} achieves zero-shot image captioning by combining large VLMs and LLMs. Specifically, VLMs provide vision-aware language guidance, which guides LLMs to generate image-related captions. We divide these methods into two paradigms: 1) late-guidance methods (ZeroCap~\cite{ZeroCap} and MAGIC~\cite{MAGIC}) inject visual guidance after word prediction, and 2) early-guidance methods (SMs~\cite{SMs}, CapDec~\cite{CapDec}, and DeCap~\cite{CapDec}) retain visual information in several tokens using VLMs, prompting LLMs to generate image-aware words.
%
Compared with these methods, we follow the early-guidance paradigm but integrate additional entity-aware hard prompts with an entity masking strategy, which significantly reduces the problem of object hallucination when describing images containing novel objects.

\paragraph{Novel Object Captioning}
%
This task aims to generate descriptions for images containing unseen objects during training~\cite{MindEye, ConstrainedBeamSearch, NoCaps, NeuralBabyTalk, DCC, NOC}. DCC~\cite{DCC} and NOC~\cite{NOC} leverage object recognition networks to recognize novel concepts. Other methods rely on object detectors (\eg, Faster R-CNN~\cite{FasterRCNN}, Mask R-CNN~\cite{MaskRCNN}) to recognize unseen entities in images~\cite{NeuralBabyTalk, Oscar, VinVL}.
%
Recently, captioning models leveraging the CLIP-based entity classifier~\cite{SMALLCAP,UniversalCaptioner} have shown even more promising performance in describing novel concepts in images. Despite their success, these methods are trained on limited image-text pairs, making data collection challenging and rendering them susceptible to overfitting to the text style of the training corpus. Consequently, their capability to generate diverse sentences is restricted.
%
In this study, we extend novel object captioning to a more data-efficient setting. Unlike the aforementioned methods, our model can seamlessly adapt to a new domain by simply fine-tuning with text-only data, thereby improving its transferability and diversity.