\section{Conclusion}

In this paper, we have comprehensively investigated the challenges of adapting pre-trained VLMs and LLMs for image-to-text generation. Our empirical findings demonstrate the existence of modality bias and object hallucination, highlighting the limited transferability when adapting pre-trained models to downstream tasks and providing valuable insight for further research in this area.
%
We propose an entity-aware decoding approach to address the observed issues. By leveraging the CLIP latent space to prompt GPT-2 during caption generation, our method, ViECap, demonstrates remarkable performance in both seen and unseen scenarios.
%
Extensive experiments showcase that our ViECap outperforms existing zero-shot methods in transferability and achieves competitive performance on zero-shot in-domain captioning. Moreover, ViECap proves to be data-efficient in low-data settings on COCO. Experiment on FlickrStyle10K shows that our model can also generate captions in different styles based on the training corpus style.

\paragraph{Acknowledgments.} This work was supported by the National Key R\&D Program of China (Grant NO. 2022YFF1202903) and the National Natural Science Foundation of China (Grant NO. 62122035).