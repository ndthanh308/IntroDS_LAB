
% misc{Authors14,
%  author = {Full Author Name},
%  title = {The Frobnicatable Foo Filter},
%  note = {Face and Gesture  submission ID 324. Supplied as additional material {\tt fg324.pdf}},
%  year = 2014
% }

% misc{Authors14b,
%  author = {Full Author Name},
%  title = {Frobnication Tutorial},
%  note = {Supplied as additional material {\tt tr.pdf}},
%  year = 2014
}

@article{ResNet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  journal={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}


@article{FasterRCNN,
  title={Faster r-cnn: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{ViT,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{LSTM,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT press}
}

@article{Transformer,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{ShowAttendTell,
  title={Show, attend and tell: Neural image caption generation with visual attention},
  author={Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
  journal={International conference on machine learning},
  pages={2048--2057},
  year={2015},
  organization={PMLR}
}

@article{ICSemanticAttention,
  title={Image captioning with semantic attention},
  author={You, Quanzeng and Jin, Hailin and Wang, Zhaowen and Fang, Chen and Luo, Jiebo},
  journal={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4651--4659},
  year={2016}
}

@article{SCA-CNN,
  title={Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning},
  author={Chen, Long and Zhang, Hanwang and Xiao, Jun and Nie, Liqiang and Shao, Jian and Liu, Wei and Chua, Tat-Seng},
  journal={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5659--5667},
  year={2017}
}

@article{KnowingWhenToLook,
  title={Knowing when to look: Adaptive attention via a visual sentinel for image captioning},
  author={Lu, Jiasen and Xiong, Caiming and Parikh, Devi and Socher, Richard},
  journal={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={375--383},
  year={2017}
}

@article{CC3M,
  title={Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning},
  author={Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
  journal={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2556--2565},
  year={2018}
}

@article{CC12M,
  title={Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts},
  author={Changpinyo, Soravit and Sharma, Piyush and Ding, Nan and Soricut, Radu},
  journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3558--3568},
  year={2021}
}

@article{Oscar,
  title={Oscar: Object-semantics aligned pre-training for vision-language tasks},
  author={Li, Xiujun and Yin, Xi and Li, Chunyuan and Zhang, Pengchuan and Hu, Xiaowei and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and others},
  journal={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XXX 16},
  pages={121--137},
  year={2020},
  organization={Springer}
}

@article{VinVL,
  title={Vinvl: Revisiting visual representations in vision-language models},
  author={Zhang, Pengchuan and Li, Xiujun and Hu, Xiaowei and Yang, Jianwei and Zhang, Lei and Wang, Lijuan and Choi, Yejin and Gao, Jianfeng},
  journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5579--5588},
  year={2021}
}

@article{CM3,
  title={Cm3: A causal masked multimodal model of the internet},
  author={Aghajanyan, Armen and Huang, Bernie and Ross, Candace and Karpukhin, Vladimir and Xu, Hu and Goyal, Naman and Okhonko, Dmytro and Joshi, Mandar and Ghosh, Gargi and Lewis, Mike and others},
  journal={arXiv preprint arXiv:2201.07520},
  year={2022}
}

@article{SimVLM,
  title={Simvlm: Simple visual language model pretraining with weak supervision},
  author={Wang, Zirui and Yu, Jiahui and Yu, Adams Wei and Dai, Zihang and Tsvetkov, Yulia and Cao, Yuan},
  journal={arXiv preprint arXiv:2108.10904},
  year={2021}
}

@article{LEMON,
  title={Scaling up vision-language pre-training for image captioning},
  author={Hu, Xiaowei and Gan, Zhe and Wang, Jianfeng and Yang, Zhengyuan and Liu, Zicheng and Lu, Yumao and Wang, Lijuan},
  journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={17980--17989},
  year={2022}
}

@article{CLIP,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  journal={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{GPT2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{GPT3,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{BART,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@article{T5,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@article{Chinchilla,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{VLAdapter,
  title={Vl-adapter: Parameter-efficient transfer learning for vision-and-language tasks},
  author={Sung, Yi-Lin and Cho, Jaemin and Bansal, Mohit},
  journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5227--5237},
  year={2022}
}

@article{Flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and others},
  journal={arXiv preprint arXiv:2204.14198},
  year={2022}
}

@article{ITuning,
  title={I-Tuning: Tuning frozen language models with image for lightweight image captioning},
  author={Luo, Ziyang and Hu, Zhipeng and Xi, Yadong and Zhang, Rongsheng and Ma, Jing},
  journal={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
}

@article{VisualGPT,
  title={Visualgpt: Data-efficient adaptation of pretrained language models for image captioning},
  author={Chen, Jun and Guo, Han and Yi, Kai and Li, Boyang and Elhoseiny, Mohamed},
  journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18030--18040},
  year={2022}
}

@article{SMALLCAP,
  title={SmallCap: lightweight image captioning prompted with retrieval augmentation},
  author={Ramos, Rita and Martins, Bruno and Elliott, Desmond and Kementchedjhieva, Yova},
  journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2840--2849},
  year={2023}
}

@article{ClipCap,
  title={Clipcap: Clip prefix for image captioning},
  author={Mokady, Ron and Hertz, Amir and Bermano, Amit H},
  journal={arXiv preprint arXiv:2111.09734},
  year={2021}
}

@article{MAPL,
  title={MAPL: Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for Vision-Language Few-Shot Prompting},
  author={Ma{\~n}as, Oscar and Rodriguez, Pau and Ahmadi, Saba and Nematzadeh, Aida and Goyal, Yash and Agrawal, Aishwarya},
  journal={arXiv preprint arXiv:2210.07179},
  year={2022}
}

@article{ZeroCap,
  title={Zero-shot image-to-text generation for visual-semantic arithmetic},
  author={Tewel, Yoad and Shalev, Yoav and Schwartz, Idan and Wolf, Lior},
  journal={arXiv preprint arXiv:2111.14447},
  year={2021}
}

@article{SMs,
  title={Socratic models: Composing zero-shot multimodal reasoning with language},
  author={Zeng, Andy and Wong, Adrian and Welker, Stefan and Choromanski, Krzysztof and Tombari, Federico and Purohit, Aveek and Ryoo, Michael and Sindhwani, Vikas and Lee, Johnny and Vanhoucke, Vincent and others},
  journal={arXiv preprint arXiv:2204.00598},
  year={2022}
}

@article{MAGIC,
  title={Language models can see: plugging visual controls in text generation},
  author={Su, Yixuan and Lan, Tian and Liu, Yahui and Liu, Fangyu and Yogatama, Dani and Wang, Yan and Kong, Lingpeng and Collier, Nigel},
  journal={arXiv preprint arXiv:2205.02655},
  year={2022}
}

@article{CapDec,
  title={Text-Only Training for Image Captioning using Noise-Injected CLIP},
  author={Nukrai, David and Mokady, Ron and Globerson, Amir},
  journal={arXiv preprint arXiv:2211.00575},
  year={2022}
}

@article{DECAP,
  title={DECAP: Decoding CLIP Latents for Zero-shot Captioning},
  author={Li, Wei and Zhu, Linchao and Wen, Longyin and Yang, Yi},
  journal={International Conference on Learning Representations},
  year={2023}
}

@article{UnifiedVLM,
  title={Unified vision-language pre-training for image captioning and vqa},
  author={Zhou, Luowei and Palangi, Hamid and Zhang, Lei and Hu, Houdong and Corso, Jason and Gao, Jianfeng},
  journal={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={07},
  pages={13041--13049},
  year={2020}
}

@article{NeuralBabyTalk,
  title={Neural baby talk},
  author={Lu, Jiasen and Yang, Jianwei and Batra, Dhruv and Parikh, Devi},
  journal={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7219--7228},
  year={2018}
}

@article{NoCaps,
  title={Nocaps: Novel object captioning at scale},
  author={Agrawal, Harsh and Desai, Karan and Wang, Yufei and Chen, Xinlei and Jain, Rishabh and Johnson, Mark and Batra, Dhruv and Parikh, Devi and Lee, Stefan and Anderson, Peter},
  journal={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={8948--8957},
  year={2019}
}

@article{DCC,
  title={Deep compositional captioning: Describing novel object categories without paired training data},
  author={Hendricks, Lisa Anne and Venugopalan, Subhashini and Rohrbach, Marcus and Mooney, Raymond and Saenko, Kate and Darrell, Trevor},
  journal={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--10},
  year={2016}
}

@article{NOC,
  title={Captioning images with diverse objects},
  author={Venugopalan, Subhashini and Anne Hendricks, Lisa and Rohrbach, Marcus and Mooney, Raymond and Darrell, Trevor and Saenko, Kate},
  journal={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5753--5761},
  year={2017}
}

@article{ConstrainedBeamSearch,
  title={Guided open vocabulary image captioning with constrained beam search},
  author={Anderson, Peter and Fernando, Basura and Johnson, Mark and Gould, Stephen},
  journal={arXiv preprint arXiv:1612.00576},
  year={2016}
}

@article{MindEye,
  title={Mind's eye: A recurrent visual representation for image caption generation},
  author={Chen, Xinlei and Lawrence Zitnick, C},
  journal={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2422--2431},
  year={2015}
}

@article{MaskRCNN,
  title={Mask r-cnn},
  author={He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  journal={Proceedings of the IEEE international conference on computer vision},
  pages={2961--2969},
  year={2017}
}

@article{ImageNet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  journal={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}



@article{ALIGN,
  title={Scaling up visual and vision-language representation learning with noisy text supervision},
  author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
  journal={International Conference on Machine Learning},
  pages={4904--4916},
  year={2021},
  organization={PMLR}
}

@article{alignmentcaption,
  title={Deep visual-semantic alignments for generating image descriptions},
  author={Karpathy, Andrej and Fei-Fei, Li},
  journal={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3128--3137},
  year={2015}
}

@article{AoA,
  title={Attention on attention for image captioning},
  author={Huang, Lun and Wang, Wenmin and Chen, Jie and Wei, Xiao-Yong},
  journal={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={4634--4643},
  year={2019}
}

@article{BUTD,
  title={Bottom-up and top-down attention for image captioning and visual question answering},
  author={Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
  journal={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6077--6086},
  year={2018}
}

@article{MMT,
  title={Meshed-memory transformer for image captioning},
  author={Cornia, Marcella and Stefanini, Matteo and Baraldi, Lorenzo and Cucchiara, Rita},
  journal={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10578--10587},
  year={2020}
}

@article{Lseg,
  title={Language-driven semantic segmentation},
  author={Li, Boyi and Weinberger, Kilian Q and Belongie, Serge and Koltun, Vladlen and Ranftl, Ren{\'e}},
  journal={arXiv preprint arXiv:2201.03546},
  year={2022}
}

@article{GroupViT,
  title={Groupvit: Semantic segmentation emerges from text supervision},
  author={Xu, Jiarui and De Mello, Shalini and Liu, Sifei and Byeon, Wonmin and Breuel, Thomas and Kautz, Jan and Wang, Xiaolong},
  journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18134--18144},
  year={2022}
}

@article{ViLD,
  title={Open-vocabulary object detection via vision and language knowledge distillation},
  author={Gu, Xiuye and Lin, Tsung-Yi and Kuo, Weicheng and Cui, Yin},
  journal={arXiv preprint arXiv:2104.13921},
  year={2021}
}

@article{GLIP,
  title={Grounded language-image pre-training},
  author={Li, Liunian Harold and Zhang, Pengchuan and Zhang, Haotian and Yang, Jianwei and Li, Chunyuan and Zhong, Yiwu and Wang, Lijuan and Yuan, Lu and Zhang, Lei and Hwang, Jenq-Neng and others},
  journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10965--10975},
  year={2022}
}

@article{DataEfficientMLM,
  title={Data efficient masked language modeling for vision and language},
  author={Bitton, Yonatan and Stanovsky, Gabriel and Elhadad, Michael and Schwartz, Roy},
  journal={arXiv preprint arXiv:2109.02040},
  year={2021}
}

@article{MSCOCO,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  journal={Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@article{MSCOCO1,
  title={Microsoft coco captions: Data collection and evaluation server},
  author={Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  journal={arXiv preprint arXiv:1504.00325},
  year={2015}
}

@article{MindTheGap,
  title={Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning},
  author={Liang, Weixin and Zhang, Yuhui and Kwon, Yongchan and Yeung, Serena and Zou, James},
  journal={arXiv preprint arXiv:2203.02053},
  year={2022}
}

@article{LAFITE,
  title={Towards language-free training for text-to-image generation},
  author={Zhou, Yufan and Zhang, Ruiyi and Chen, Changyou and Li, Chunyuan and Tensmeyer, Chris and Yu, Tong and Gu, Jiuxiang and Xu, Jinhui and Sun, Tong},
  journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={17907--17917},
  year={2022}
}

@article{CIDEr,
  title={Cider: Consensus-based image description evaluation},
  author={Vedantam, Ramakrishna and Lawrence Zitnick, C and Parikh, Devi},
  journal={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4566--4575},
  year={2015}
}

@article{SPICE,
  title={Spice: Semantic propositional image caption evaluation},
  author={Anderson, Peter and Fernando, Basura and Johnson, Mark and Gould, Stephen},
  journal={Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V 14},
  pages={382--398},
  year={2016},
  organization={Springer}
}

@article{BLEU,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  journal={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@article{METEOR,
  title={Meteor universal: Language specific translation evaluation for any target language},
  author={Denkowski, Michael and Lavie, Alon},
  journal={Proceedings of the ninth workshop on statistical machine translation},
  pages={376--380},
  year={2014}
}

@article{ROUGE_L,
  title={Automatic evaluation of machine translation quality using n-gram co-occurrence statistics},
  author={Doddington, George},
  journal={Proceedings of the second international conference on Human Language Technology Research},
  pages={138--145},
  year={2002}
}

@article{Flickr30k,
  title={From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions},
  author={Young, Peter and Lai, Alice and Hodosh, Micah and Hockenmaier, Julia},
  journal={Transactions of the Association for Computational Linguistics},
  volume={2},
  pages={67--78},
  year={2014},
  publisher={MIT Press}
}

@article{AdamW,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{OPT,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{CLIPExperiment,
  title={How much can clip benefit vision-and-language tasks?},
  author={Shen, Sheng and Li, Liunian Harold and Tan, Hao and Bansal, Mohit and Rohrbach, Anna and Chang, Kai-Wei and Yao, Zhewei and Keutzer, Kurt},
  journal={arXiv preprint arXiv:2107.06383},
  year={2021}
}

@article{StyleNet,
  title={Stylenet: Generating attractive visual captions with styles},
  author={Gan, Chuang and Gan, Zhe and He, Xiaodong and Gao, Jianfeng and Deng, Li},
  journal={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3137--3146},
  year={2017}
}

@article{MemCap,
  title={Memcap: Memorizing style knowledge for image captioning},
  author={Zhao, Wentian and Wu, Xinxiao and Zhang, Xiaoxun},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={07},
  pages={12984--12992},
  year={2020}
}

@article{hugface,
  title={Transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations},
  pages={38--45},
  year={2020}
}


@article{Yang01,
  title={Unified contrastive learning in image-text-label space},
  author={Yang, Jianwei and Li, Chunyuan and Zhang, Pengchuan and Xiao, Bin and Liu, Ce and Yuan, Lu and Gao, Jianfeng},
  journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={19163--19173},
  year={2022}
}

@inproceedings{laina2019towards,
  title={Towards unsupervised image captioning with shared multimodal embeddings},
  author={Laina, Iro and Rupprecht, Christian and Navab, Nassir},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={7414--7424},
  year={2019}
}

@inproceedings{feng2019unsupervised,
  title={Unsupervised image captioning},
  author={Feng, Yang and Ma, Lin and Liu, Wei and Luo, Jiebo},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4125--4134},
  year={2019}
}

@article{UniversalCaptioner,
  title={Universal Captioner: Inducing Content-Style Separation in Vision-and-Language Model Training},
  author={Cornia, Marcella and Baraldi, Lorenzo and Fiameni, Giuseppe and Cucchiara, Rita},
  journal={arXiv preprint arXiv:2111.12727},
  year={2021}
}

@article{CLIPExperiment,
  title={How much can clip benefit vision-and-language tasks?},
  author={Shen, Sheng and Li, Liunian Harold and Tan, Hao and Bansal, Mohit and Rohrbach, Anna and Chang, Kai-Wei and Yao, Zhewei and Keutzer, Kurt},
  journal={arXiv preprint arXiv:2107.06383},
  year={2021}
}

@article{OfficeHome,
  title={Multi-component image translation for deep domain generalization},
  author={Rahman, Mohammad Mahfujur and Fookes, Clinton and Baktashmotlagh, Mahsa and Sridharan, Sridha},
  journal={2019 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages={579--588},
  year={2019},
}