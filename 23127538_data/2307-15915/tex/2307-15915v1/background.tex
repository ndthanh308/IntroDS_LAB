\section{Background}
\label{sec:background}
In this section, we present the background knowledge of some recently advanced technologies utilized by JFinder.

\subsection{Pre-trained Models}
Pre-trained models are machine learning models that have already undergone training for certain tasks. These models can significantly reduce training time and achieve better results without requiring large amounts of data. Notable pre-trained models such as BERT and GPT exhibit exceptional performance in natural language processing (NLP) and serve as milestones in the field of artificial intelligence. Due to their complex pre-training objectives and large parameter count, pre-trained models can effectively capture knowledge from vast amounts of labeled and unlabeled data. By storing knowledge in numerous parameters and fine-tuning specific tasks, the rich knowledge encoded in these parameters can benefit various downstream tasks, as demonstrated by experimental validation and empirical analysis. Pre-training mechanisms enable models to learn generic linguistic expressions by leveraging substantial volumes of unlabeled text. Pre-trained models can be adapted to downstream tasks by adding one or two specific layers, providing a good initialization and preventing the need to train downstream models from scratch. This approach improves performance on small datasets, reducing the requirement for a large number of labeled instances. As deep learning models with many parameters tend to overfit on small datasets, pre-training serves as a form of regularization by providing a good initialization and avoiding overfitting.

\subsection{Self-Attention Mechanism}
The self-attention mechanism constitutes a crucial component of pre-trained models commonly employed for NLP tasks, such as the Transformer. This mechanism determines the relationships between words by focusing on the input data's positions, resulting in more efficient text representations. The self-attention mechanism encodes the contextual information of an entire text in each word's semantic representation. Its advantages include capturing long-distance dependencies in text and effectively handling variable-length sequences. Owing to its outstanding performance in NLP, the self-attention mechanism has become a popular research topic in the field.

In our implementation, we utilize a multi-head self-attention mechanism. This approach replicates a single attention head into multiple ones, applying them to different data positions separately to obtain more semantic information. Each head independently learns distinct contextual information, enhancing the model's generalization capabilities. This method is widely used in Transformer models and has achieved satisfactory practical results.

\subsection{Word Embedding}
Traditional machine learning methods often struggle to process textual data directly, necessitating appropriate techniques to convert text data into numerical data, which introduces the concept of word embedding. Word embedding encompasses language modeling and representation learning techniques in NLP, serving as an early pre-training technique. It refers to embedding a high-dimensional space, with dimensions equal to the total number of words, into a continuous vector space of much lower dimensionality. In this space, each word or phrase is mapped to a vector of real numbers, representing a specific concept through a distributed representation.

\subsubsection{Bag-of-words}
In information retrieval, the bag-of-words model assumes that a text is a collection of words or word combinations, disregarding word order, grammar, and syntax. The model considers the occurrence of each word in a text to be independent of the occurrences of other words. Common applications include one-hot encoding and N-gram techniques. Although the model is easy to understand and implement, it has several drawbacks. Careful vocabulary design is crucial, particularly to manage size and avoid sparse context representations. Disregarding word order neglects the context and meaning of words in a document.

\subsubsection{Context-independent with machine learning}
Context-independent machine learning approaches do not consider contextual information in the learning process. They assume input data is independent of other information, focusing solely on the current input during model training. These approaches are commonly used for text classification, with popular models including word2vec, fastText, and glove.
\subsubsection{Context-dependent and transformer-based}
Context-dependent and transformer-based methods represent context-sensitive approaches in which the same word is represented differently depending on the context. These methods obtain contextual information by using a transformer model to compute the relationships of the input data. Transformer models incorporate attention mechanisms that learn long-range dependencies, making them better suited for natural language processing (NLP) tasks.