\section{Implementation}
\label{sec:implementation}
In this section, we provide details on the implementation of JFinder, as illustrated in Fig.~\ref{fig:model}, including data preparation and module implementation.

\subsection{Generating data}
\subsubsection{Generating AST}
We employed JavaParser\footnote{http://javaparser.org/}, an open-source tool for analyzing Java code, to construct abstract syntax trees (ASTs). Using the \texttt{parse} module, we generated an AST for a given code snippet and outputted a DOT file containing edge and node information. For instance, \texttt{n0 -> n1} denotes an edge from the 0th node to the 1st node. Based on this information, we constructed the AST adjacency matrix.

\subsubsection{Generating CFG and DFG}
We extracted C++ code using tree-sitter-c~\footnote{https://tree-sitter.github.io/tree-sitter/}, a parser-generating tool and incremental parsing library that produces concrete syntax trees for source files and efficiently updates the syntax trees as the source files change. We then created a Node class to store the current node's header nodes, end node, and next node, enabling straightforward traversal of all nodes and their associated nodes. Next, we used lists to store related nodes, added directed edges between them based on conditional expressions, and created the control flow graph (CFG) adjacency matrix using these directed edges. For example, for node 4 with two child nodes 5 and 6, we set matrix entries $M(4,5)=1$ and $M(4,6)=1$.

The data flow graph (DFG) adjacency matrix generation process is similar to that of the CFG adjacency matrix. The only difference is that we added directed edges based on data flow instead of conditional expressions.

\subsubsection{Generating CSS}
We employed the HuggingFace transformer library~\cite{wolf-etal-2020-transformers}, which includes a framework of pre-trained models. We loaded the UniXcoder model~\cite{guo2022unixcoder} based on this framework, a unified cross-modal pre-trained model for programming languages that supports both code-related understanding and generation tasks. Before generating code snippet embeddings, we tokenized code snippets into token sequences. We used the UniXcoder tokenizer to process our datasets. However, the tokenizer's performance was suboptimal, as it divided \texttt{LF\_NORMAL} into \texttt{LF}, \texttt{\_}, and \texttt{NORMAL}. In our experiments, we found that incorrect code tokenization led to a significant decrease in our model's accuracy. To address this issue, we expanded the UniXcoder's vocabulary by traversing our datasets and recording words not present in the UniXcoder's vocabulary as a special word list, which we then added to the UniXcoder. For each source code snippet, the UniXcoder outputted a code semantic snippet (CSS) embedding matrix containing its semantic information. We should note that the source code length cannot exceed 512 tokens, and any source code exceeding this length must be truncated.
\subsection{module implementation}
\subsubsection{Metapath}
As shown in Fig.\ref{fig:model} and discussed in Sec.\ref{sec:metapath}, we needed to add reversed edges for each pair of nodes connected by a directed edge. To customize our metapaths, we wrote a Python program to add metapaths to the AST, CFG, and DFG adjacency matrices, as shown below:
\begin{algorithm}[]
	\caption{Metapath module}
	\LinesNumbered
	\KwIn{Matrix $x_i \in AST, CFG, DFG$}
	\KwOut{Matrix $y_i$}
	\For{i=0 i$<x_i$.1st-dimention i++}{
		\For{q=0 q$<x_i$.2nd-dimention q++}{
			\If{$x_i$(i,q)=1}{
				$x_i$(q,i)$\leftarrow$1
			}
		}
	}
\end{algorithm}
\subsubsection{Multi-View Self-Attention Encoder (MVSA)}
We implemented the MVSA using Keras~\cite{chollet2015keras} based on TensorFlow~\cite{abadi2016tensorflow}, an advanced neural network library for building and training deep learning models that offers an easy-to-use high-level interface for constructing, training, and evaluating deep learning models. We created a custom layer containing four MVSAs, named Quad Self-Attention Layer. The outputs of the Quad Self-Attention Layer were fed into a convolutional layer and a fully connected layer, which predicted whether a code snippet was vulnerable. Our model employed cross-entropy as the loss function, calculated as follows:
\begin{equation}
	\mathcal{L}_{CE}=\sum_{i=0}^{N} y_{i} \log p_{i}+\left(1-y_{i}\right) \log \left(1-p_{i}\right)\label{eq_ce},
\end{equation}
where $y_i$ represents the true label and $p_i$ is the probability of the label predicted by the model.