\section{Experiments}
\label{exp}
In this section, we conduct thorough experiments to validate the effectiveness of our approach. We first introduce the dataset and metrics used in our experiments, then followed by comparisons with other state-of-the-art methods. Lastly, we ablate some designs and potential input format of raw radar data in our method, and discuss some limitations.

\subsection{Experimental Settings}
\textbf{The RADIal Dataset} \cite{rebut2022raw} provides 8252 annotated frames with synchronized multi-sensor data. Training, validation, and test sets contain 6231, 986, and 1035 frames, respectively. Each object is annotated with a 3D center coordinate of the visible face of the vehicle. The size and orientation of the objects are pre-defined templates, i.e. they are the same for all objects. In the evaluation, precisions and recalls under ten thresholds are calculated. The average precision (AP), average recall (AR), and F1 score are obtained through the ten precisions and ten recalls as the evaluation metric. Range error (RE) and azimuth error (AE) are also reported to evaluate localization accuracy. The annotation method and metrics are quite different to those in common autonomous driving datasets \cite{caesar2020nuscenes,sun2020scalability}. Nonetheless, to compare with state-of-the-art methods, we still list our results using the metrics defined in RADIal \cite{rebut2022raw} as a reference.

\textbf{Re-annotation and more metrics.} Based on the provided LiDAR point cloud, we have refined the cluster-center-based annotation to 3D bounding box form, which includes center position, scale, and heading in 3D space. Capitalizing on these new annotations, we are able to apply metrics that are vastly used in 3D object detection tasks. Specifically, we take AP defined in Waymo dataset \cite{sun2020scalability} as our main metrics. Since the LiDAR used in RADIal is only 16-beam, we cannot label accurate height for the objects. So we mainly utilize BEV IoU for evaluation. We use the threshold of 0.7 for normal IoU and longitudinal error-tolerant (LET) IoU computation \cite{hung2022let}. 
%Under 3D evaluation, we take 0.5 as the IoU threshold due to relatively low vertical annotation accuracy. 
Considering the spatial distribution of ground-truth, the distance evaluation is further broken down into
two categories: 0 to 50 meters, 50 to 100 meters. For a more comprehensive evaluation, we also introduce the error metrics defined in nuScenes dataset \cite{caesar2020nuscenes}, including  ATE, ASE, and AOE for measuring errors of translation, scale, and orientation. 

\textbf{The KRadar Dataset} \cite{paek2022k} is a recently published 4D radar dataset, which contains 58 sequences, i.e. 35K frames of synchronized multi-modality data. However, the full data are still not available by the camera-ready. Thus, we evaluate our method with the sub-dataset stored on Google Drive, which consists of 20 sequences. This so-called KRadar-20 dataset includes 6493 training samples and 6515 test samples, of which 50 \% are nighttime data. The training samples are named trainval split, it is further split into train split of 5190 samples and val split of 1303 samples for ablation. Metrics follow KITTI \cite{geiger2012we} protocols with 40 recall positions. Different from RADIal, it only contains radar formats of range-azimuth map (RA map) and radar point cloud. But the RA map of KRadar contains an extra height dimension, and it enables us to test 3D prediction ability with 4D radar data.

\textbf{Radar Formats.} As introduced in Section \ref{preliminary}, there are multiple formats of radar raw data, such as ADC, RT, RD, READ. These formats can be converted sequentially by FFT, which is a linear operation that can be absorbed into the linear layers. So from the perspective of neural network, these formats are equivalent. However, our proposed Polar-Aligned Attention requires a range dimension for indexing key and value in cross-attention, so we choose RT as a representative in the experiments. We also conduct experiments on RA to investigate the necessity of explicit azimuth.

\textbf{Implementation Details.} All our experiments are carried out on eight 3090 GPUs with the same batch size of 8. For the image backbone, we adopt ResNet-50 with pre-trained weights provided by BEVFormer \cite{li2022bevformer}. For LiDAR and radar point cloud branch, we borrow the voxel encoder from PointPillar \cite{lang2019pointpillars} and use ResNet-50 with modified strides as the backbone. Other representations of radar data share similar modified ResNet-50 as the backbone, but with an extra 1 $\times$ 1 convolution at the beginning for channel number alignment. To prevent overfitting, the image-only variant is trained for only 12 epochs while others are trained for 24 epochs. AdamW \cite{loshchilov2017decoupled} is adopted as the optimizer and a learning rate of 5e-5 is shared for all models. 
Due to the limited size of the dataset, we find that the results are unstable across different runs. To make a fair comparison, each experiment is repeated three times. In modality ablation, we show the mean and variance of each experiment result.

% \yang{It is worth noticing that for RT data in RADIal dataset, a pre-encoder is required due to DDM applied in radar sensor \cite{rebut2022raw}. And this pre-encoder is realized by an appropriately initialized complex linear layer. More details can be found in appendix.} 

\subsection{Comparison to State-of-the-art Methods}
\subsubsection{Results on RADIal Dataset}
\begin{table}
  \caption{Detection performances on RADIal Dataset test split with original protocol \cite{rebut2022raw}. C, RD, and RT respectively refer to camera, range-Doppler spectrum, and range-time data. Our method achieves the best performance in both average precision (AP), average recall (AR), and F1-score (F1). The best result of each metric is in \textbf{bold}.}
  \label{sota}
  \centering
  \resizebox{0.85\textwidth}{!}{%
  \begin{tabular}{lllllll}
    \toprule
    Methods & Modality & AP($\%$)$\uparrow$ & AR($\%$)$\uparrow$ & F1($\%$)$\uparrow$ & RE(m)$\downarrow$ & AE($^{\circ}$)$\downarrow$ \\
    \midrule
    FFTRadNet \cite{rebut2022raw} & RD & 96.84 & 82.18 & 88.91 & \textbf{0.11} & 0.17 \\
    T-FFTRadNet \cite{giroux2023t} & ADC & 89.60 & 89.50 & 89.50 & 0.15 & 0.12 \\
    ADCNet \cite{yang2023adcnet} & ADC & 95.00 & 89.00 & 91.90 & 0.13 & \textbf{0.11} \\
    CMS \cite{jin2023cross} & RD\&C & 96.90 & 83.50 & 89.70 & 0.45 & n/a \\
    EchoFusion & RT\&C & \textbf{96.95} & \textbf{93.43} & \textbf{95.16} &0.12 & 0.18 \\
    \bottomrule
  \end{tabular}
  }
\end{table}
\begin{table}
  \caption{BEV detection performances on RADIal Dataset test split with refined 3D ground-truth bounding box. C, RD, and RT respectively refer to camera, range-Doppler spectrum, and range-time data. The best result of each metric is in bold. Our method exceeds FFTRadNet by a large margin.}
  \label{sota_newlabel}
  \centering
  \resizebox{0.99\textwidth}{!}{%
  \begin{tabular}{llllllll}
    \toprule
    & & \multicolumn{3}{c}{BEV AP@0.7($\%$)$\uparrow$}&\multicolumn{3}{c}{LET-BEV-AP@0.7($\%$)$\uparrow$}\\
    \cmidrule(r){3-5} \cmidrule(r){6-8}
     Methods & Modality & Overall & 0 - 50m & 50 - 100m & Overall & 0 - 50m & 50 - 100m \\
    \midrule
    FFTRadNet3D\cite{rebut2022raw} & RD & 57.26 & 68.66 & 52.28 & 62.81 & 75.55 & 57.98 \\
    EchoFusion & RT\&C & \textbf{84.92} & \textbf{87.56} & \textbf{91.06} & \textbf{88.86} & \textbf{92.81} & \textbf{94.81} \\
    \bottomrule
  \end{tabular}
  }
\end{table}

We first compare our method with state-of-the-art detectors on AP, AR, range error, and azimuth error defined in RADIal \cite{rebut2022raw}, with original object-level point annotations\footnote{The official implementation of these metrics are different from common implementations. Specifically, AP and AR are averaged on score thresholds from 0.1 to 0.9 with step of 0.1 and IoU threshold of 0.5. And F1 score is directly calculated from AP and AR defined above.}. The results are illustrated in Table \ref{sota}. Our method significantly improves AR and F1 performance and achieves a remarkable improvement over all existing detectors including CMS \cite{jin2023cross}, which also integrates both radar raw data and camera data. Note that RE and AE are not quite informative because these two metrics are calculated on all the recalled objects of a method, which is unfair to high-recall models like ours.

To evaluate in a more rigorous and informative way, we also conduct experiments on the refined annotations and vastly adopted metrics in 3D object detection. FFTRadNet \cite{rebut2022raw} is lifted up with additional branches to predict the scale and orientation of targets. We can only compare with this FFTRadNet variant since codes of other algorithms \cite{giroux2023t, jin2023cross, yang2023adcnet} are not available yet. The results in Table \ref{sota_newlabel} show that the proposed method outperforms FFTRadNet by a large margin. This significant improvement reveals the huge benefit of unleashing the power of radar data in multi-modality fusion.

\subsubsection{Results on KRadar Dataset}
The baseline on KRadar dataset is the official RTNH \cite{paek2022k}, which requires radar point cloud. Here, the proposed EchoFusion is input with image and RA map. To align with the baseline, we take the radius and azimuth range respectively as [0, 72] m and [-20, 20] degrees by the limit of camera field of view. The results are shown in Table \ref{kradar}. With RA map and image fusion, our EchoFusion improves over 10 points at each metric, except for BEV AP of IoU threshold 0.3. And since KRadar has higher vertical resolution and more accurate 3D groundtruth than RADIal, our method achieves more considerable improvements in 3D metrics.

\begin{table}
  \caption{BEV detection performances on KRadar Dataset test split with KITTI protocol. 0.3, 0.5, and 0.7 are IoU thresholds. The inputs of EchoFusion are image and range-azimuth map. The best result of each metric is in \textbf{bold}.}
  \label{kradar}
  \centering
  \resizebox{0.99\textwidth}{!}{%
  \begin{tabular}{llllllll}
    \toprule
    & & \multicolumn{3}{c}{BEV AP($\%$)$\uparrow$}&\multicolumn{3}{c}{3D AP($\%$)$\uparrow$}\\
    \cmidrule(r){3-5} \cmidrule(r){6-8}
     Training Set & Method & AP@0.3 & AP@0.5 & AP@0.7 & AP@0.3 & AP@0.5 & AP@0.7 \\
    \midrule
    KRadar & RTNH\cite{paek2022k} & 58.04 & 42.60 & 10.69 & 49.65 & 17.87 & 0.45 \\
    KRadar-20-trainval & RTNH\cite{paek2022k} & 61.38 & 46.47 & 10.47 & 53.05 & 17.98 & 3.03 \\
    KRadar-20-trainval & EchoFusion & \textbf{69.95} & \textbf{57.28} & \textbf{33.07} & \textbf{68.35} & \textbf{43.87} & \textbf{14.00} \\
    \bottomrule
  \end{tabular}
  }
\end{table}

\subsection{Ablation Studies}
\begin{table}[!tb]
  \caption{Ablation studies on our EchoFusion. Complex means interpreting input features as real and imaginary (IQ) or magnitude and phase (MP). Pretrain means whether to use the pre-trained image backbone. The best result of each metric is in \textbf{bold}.}
  \label{arch}
  \centering
  \resizebox{0.9\textwidth}{!}{%
  \begin{tabular}{lllllllll}
    \toprule
    \multicolumn{2}{c}{Conditions} & \multicolumn{3}{c}{BEV AP@0.7($\%$)$\uparrow$}&\multicolumn{3}{c}{LET-BEV-AP@0.7($\%$)$\uparrow$}\\
    \cmidrule(r){1-2} \cmidrule(r){3-5} \cmidrule(r){6-8}
    Complex & Pretrain & Overall & 0 - 50m & 50 - 100m & Overall & 0 - 50m & 50 - 100m \\
    \midrule
    MP & w & \textbf{84.92} & \textbf{87.56} & \textbf{91.06} & \textbf{88.86} & \textbf{92.81} & \textbf{94.81} \\
    IQ & w & 82.64 & 87.09 & 87.81 & 88.06 & 92.25 & 93.24 \\
    MP & w/o & 74.34 & 81.26 & 78.71 & 78.90 & 85.44 & 84.28 \\
    \bottomrule
  \end{tabular}
  }
\end{table}

We ablate the input format to the radar feature extraction network in Table \ref{arch}. It shows that the decomposition of complex input can improve overall detection performance, especially for long-range perception. The magnitude and phase representation encodes a non-linear transformation of the complex representation, which may relate to the final prediction target better. We also tried to remove the pre-training of the image branch. The result drops significantly, which confirms that in this relatively small dataset, a good pre-training is crucial for good performance.

\subsection{Comparison of Different Modalities}
\begin{table}
  \caption{BEV detection performances of different modality combinations of our algorithm on RADIal Dataset test split. Refined 3D bounding box annotations are applied. C, L, RD, RT, RA, RP respectively refer to the camera, LiDAR, range-Doppler spectrum, range-time data, range-azimuth map, and radar point cloud. The best result without LiDAR of each metric are in \textbf{bold}, and the results with LiDAR are on gray background.}
  \label{modalities}
  \centering
  \resizebox{0.99\textwidth}{!}{%
  \begin{tabular}{lllllllllll}
    \toprule
    \multicolumn{5}{c}{Modality} & \multicolumn{3}{c}{BEV AP@0.7($\%$)$\uparrow$}&\multicolumn{3}{c}{LET-BEV-AP@0.7($\%$)$\uparrow$}\\
    \cmidrule(r){1-5} \cmidrule(r){6-8} \cmidrule(r){9-11}
    C & RT & RA & RP & L & Overall & 0 - 50m & 50 - 100m & Overall & 0 - 50m & 50 - 100m \\
    \midrule
    \checkmark & & & & & 10.07$\pm$1.10 & 21.68$\pm$3.11 & 2.21$\pm$0.59 & 56.92$\pm$3.91 & \textbf{78.98$\pm$1.41} & 36.20$\pm$5.81 \\
    & \checkmark & & & & 48.26$\pm$0.92 & 62.33$\pm$4.00 & 45.05$\pm$0.98 & 55.04$\pm$1.25 & 68.56$\pm$4.11 & 52.30$\pm$1.82 \\
    & & \checkmark & & & 50.67$\pm$1.03 & 65.05$\pm$1.59 & 42.98$\pm$0.51 & 58.53$\pm$0.13 & 72.95$\pm$0.78 & 51.41$\pm$0.54 \\
    & & & \checkmark & & \textbf{53.55$\pm$0.70} & \textbf{66.19$\pm$1.72} & \textbf{47.41$\pm$1.06} & \textbf{62.59$\pm$1.14} & 76.45$\pm$2.06 & \textbf{56.73$\pm$1.33} \\ \rowcolor{gray!20}
    & & & & \checkmark & 84.47$\pm$0.15 & 86.92$\pm$0.61 & 91.91$\pm$0.13 & 86.07$\pm$0.44 & 88.55$\pm$0.79 & 93.17$\pm$0.10 \\
    \midrule
    \checkmark & \checkmark & & & & \textbf{84.92$\pm$0.98} & 87.56$\pm$1.58 & 91.06$\pm$1.18 & 88.86$\pm$0.62 & 92.81$\pm$1.37 & 94.81$\pm$0.52 \\
    \checkmark & & \checkmark & & & 84.77$\pm$0.65& \textbf{87.93$\pm$0.60} & \textbf{91.48$\pm$0.28} & \textbf{89.54$\pm$0.54} & \textbf{93.83$\pm$0.56} & \textbf{94.87$\pm$0.58} \\
    \checkmark & & & \checkmark & & 82.35$\pm$0.93 & 86.97$\pm$1.01 & 86.39$\pm$0.74 & 88.35$\pm$0.78 & 93.07$\pm$0.80 & 92.77$\pm$0.69 \\ \rowcolor{gray!20}
    \checkmark & & & & \checkmark & 86.35$\pm$1.15 & 88.81$\pm$1.40 & 94.25$\pm$1.68 & 88.44$\pm$0.79 & 91.63$\pm$1.34 & 95.31$\pm$1.28 \\ 
    \bottomrule
  \end{tabular}
} 
\end{table}
To further study the effects of different modalities, we change the input format indicated in Figure \ref{data_vis} and corresponding backbones, and test with and without image modality. Table \ref{modalities} presents the results, more results on KRadar can be found in our appendix, and our findings are listed as follows.

\textbf{Comparing single-modality results} It is not surprising that LiDAR ranks first while the camera is the worst in terms of AP. Though poor in depth estimation, the camera has excellent angular resolution, which is beneficial for LET-BEV-AP. It achieves comparable results with radar in LET-BEV-AP. Besides, without the guidance of image, the radar-only network gets better performance as more hand-crafted post-processing are involved. However, their metrics still lag far from those of LiDAR.

\textbf{Comparing multi-modality with single modality} The power of radar data is released by fusing with image. No matter in what data format, all the radar raw representations gain around 30 points improvement by fusing with images. By combining image and range-time data, we obtain BEV AP that is only 0.03 less than that of LiDAR only method. And the LET-BEV-AP and long-range detection ability are even better. We argue that it is mainly because images provide enough clues to decode the essential information from raw radar representation.

\textbf{Comparing multi-modality results} When integrating with images, the LiDAR-fused method still outperforms other radar-fused methods. In terms of different representations of radar data, both RT and RA outperform traditional cloud points, especially in the 50-100m range, in which the difference is as large as 4 points in AP. We owe this finding to the information loss and false alarm of radar point cloud as shown in Figure \ref{pcd}. The performance gap between RT and RA with camera modality is within the error bar, indicating that it is not necessary to explicitly solve the azimuth, nor necessary to permute the Doppler-angle dimensions as FFTRadNet \cite{rebut2022raw} does.
%On the other hand, the speed, azimuth, elevation, and range information in RT and RA data is never impaired by the hand-crafted radar data processing pipeline, which offers the best overall performance.


\subsection{Discussion and Limitation}
\label{subsec:discussion}
\begin{table}
  \caption{3D detection performances on RADIal Dataset test split with refined 3D ground-truth bounding box. C, L, and RT respectively refer to camera, LiDAR, and radar-time data. The best result of each metric is in bold.}
  \label{3DmAP}
  \centering
  \resizebox{0.7\textwidth}{!}{%
  \begin{tabular}{lllllll}
    \toprule
    & \multicolumn{3}{c}{3D AP@0.5($\%$)$\uparrow$}&\multicolumn{3}{c}{Average Error@0.5$\downarrow$}\\
    \cmidrule(r){2-4} \cmidrule(r){5-7}
     Modality & Overall & 0 - 50m & 50 - 100m & ATE & ASE & AOE\\
    \midrule
    L & 62.69 & 80.04 & 54.26 & 0.243 & \textbf{0.171} & 0.027 \\
    L\&C & \textbf{68.36} & \textbf{84.74} & \textbf{61.54} & \textbf{0.239} & 0.180 & \textbf{0.020} \\
    RT\&C & 39.81 & 56.05 & 31.24 & 0.301 & 0.173 & \textbf{0.020} \\
    \bottomrule
  \end{tabular}
  }
\end{table}
Firstly, it is worth noticing that although the overall performance of radar is improved by fusing with the camera, the performance in the short-range is lower than that in the long-range. We speculate the main reason is inaccurate annotation. The 16-beam LiDAR provided in RADIal is too sparse at a farther distance for annotators to give an accurate size for the long-range objects. In such cases, they are required to assign a template with a fixed size to these objects. These annotations make the problem much easier for farther distances, which leads to higher performance than near distances.
%However, this is not detrimental for neural networks, as predicting a fixed value is relatively straightforward for the networks.  
%the objects of large yaw are not common in the training set, and the predictions are not perfect. But in the test split, most of the objects with large rotations are concentrated in short range, thus harming the performance. 

Secondly, though the radial speed label is included in the original annotations of RADIal, radial speed is not quite useful for downstream modules like planning since they need accurate longitudinal and lateral velocity to predict whether the vehicles will interfere with the ego vehicle. However, the synchronized frames are not consecutive in this dataset, it is hard to label velocities for the objects. Considering these factors, velocity prediction is not included in our task. 

Finally, as shown in Table \ref{3DmAP}, our method still lags far from LiDAR-based methods. The primary limitation can be attributed to imperfect z localization, resulting in relatively high Average Localization Error (ATE). The inferior performance is mainly due to coarse LiDAR annotation and inferior radar elevation resolution. The low elevation distinguishing power of LiDAR makes annotation error much worse at long range. As a result, both LiDAR-based and radar-based methods experience a significant drop in 3D AP within the 50-100m interval. But when equipped with high elevation resolution and accurate 3D groundtruth provided by KRadar, our method shows excellent performance in 3D metrics, as shown in Table \ref{kradar}. This emphasizes the pressing need for a raw radar dataset of higher quality that enables better exploration of radar data usage.