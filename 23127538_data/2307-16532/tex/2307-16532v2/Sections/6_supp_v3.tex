
\setcounter{equation}{0}
\setcounter{table}{0}
\setcounter{figure}{0}
\setcounter{section}{0}
\renewcommand\theequation{S\arabic{equation}}
\renewcommand\thefigure{S\arabic{figure}}
\renewcommand\thetable{S\arabic{table}}
\renewcommand\thesection{\Alph{section}}

%In this supplementary material, we begin by providing detailed proof for section \ref{Column-wise Image Fusion} in \cref{sec:proof}, offering a detailed explanation of our methodology. Moving on to \cref{sec:data}, we present the visualization of detected results as well as the distribution of the RADIal dataset and elaborate on the rationale behind our range division. In Section \ref{sec:R&C}, we present additional experimental results that highlight the complementary nature of radar and camera modalities, providing further evidence of their synergistic effects. In \cref{sec:detail}, we offered BEV detection results when the radar-Doppler spectrum is taken as input radar format. Besides, the averaged 3D performances are also presented for a better understanding of our approach.

\section*{Supplementary Material}
\section{Proof for column and pillar correspondence}
\label{sec:proof}
In this section, we will provide a detailed proof for the correspondence between pillar in radar coordinate and column in camera coordinate as described in Section 4.2 of our main paper.

Suppose that the rotation matrix and translation vector between radar and camera are:
\begin{equation}
    R=\left( \begin{matrix}
	a_1&		b_1&		c_1\\
	a_2&		b_2&		c_2\\
	a_3&		b_3&		c_3\\
    \end{matrix} \right) , T=\left( \begin{array}{c}
    	d_1\\
    	d_2\\
    	d_3\\
    \end{array} \right). 
\end{equation}
Then for a point located at $(r_{bev}, \phi_{bev}, z)$ under radar's polar coordinate, it has cartesian coordinate $(x_R, y_R, z_R)$:
\begin{equation}
    x_R=r_{bev}\cos \left( \phi _{bev} \right) , y_R=r_{bev}\sin \left( \phi _{bev} \right) , z_R=z,
\end{equation}
and it can be projected to the camera frame using extrinsic parameters:
\begin{equation}
    \label{eq:ex}
    \left( \begin{array}{c}
	x_C\\
	y_C\\
	z_C\\
    \end{array} \right) =R\left( \begin{array}{c}
    	x_R\\
    	y_R\\
    	z_R\\
    \end{array} \right) +T=\left( \begin{array}{c}
    	a_1r_{bev}\cos \phi_{bev}+b_1r_{bev}\sin \phi_{bev}+c_1z+d_1\\
    	a_2r_{bev}\cos \phi_{bev}+b_2r_{bev}\sin \phi_{bev}+c_2z+d_2\\
    	a_3r_{bev}\cos \phi_{bev}+b_3r_{bev}\sin \phi_{bev}+c_3z+d_3\\
    \end{array} \right). 
\end{equation}
Using the intrinsic parameters, the camera coordinate $(x_C, y_C, z_C)$ can be correlated with image pixel position $(x_I, y_I)$ using:
\begin{equation}
    \label{eq:in}
    \frac{x_I-u_0}{f_x}=\frac{x_C}{z_C},\frac{y_I-v_0}{f_y}=\frac{y_C}{z_C},
\end{equation}
where $f_x$, $f_y$, $u_0$, and $v_0$ are intrinsic parameters. Our goal is to find a situation that the pillar is projected as a column on the image plane. Under this condition, $x_I$ should be irrelevant with $z$, i.e. in the equation below:
\begin{equation}
    \label{eq:col_org}
    \frac{x_I-u_0}{f_x}=\frac{x_C}{z_C}=\frac{a_1r_{bev}\cos \phi_{bev} +b_1r_{bev}\sin \phi_{bev} +c_1z+d_1}{a_3r_{bev}\cos \phi_{bev} +b_3r_{bev}\sin \phi_{bev} +c_3z+d_3}.
\end{equation}
The coefficients of z should be 0, which means  by combining the relationship between rotation matrix $R$ with roll $\alpha$, pitch $\beta$, and yaw $\gamma$ \cite{murray1994mathematical}, we can derive:
\begin{equation}
    \label{eq:c1&c3}
    \left\{ \begin{array}{l}
    	c_1=\sin \gamma \sin \alpha +\cos \gamma \sin \beta \cos \alpha =0\\
    	c_3=\cos \beta \cos \alpha =0.\\
    \end{array} \right. 
\end{equation}
Considering second row of \cref{eq:c1&c3}, there are two solutions  $\beta=\pm\frac{\pi}{2}$ or $\alpha=\pm\frac{\pi}{2}$. Considering the first solution $\beta=\pm\frac{\pi}{2}$, the first row of \cref{eq:c1&c3} is transformed to:
\begin{equation}
    \sin \gamma \sin \alpha \pm \cos \gamma \cos \alpha =0,
\end{equation}
which means:
\begin{equation}
    \cos \left( \gamma \mp \alpha \right) =0.
\end{equation}
Thus we have:
\begin{equation}
    \begin{aligned}
        \beta &=\frac{\pi}{2}, \gamma =\alpha \pm \frac{\pi}{2}, \alpha \in \left[ -\pi ,\pi \right] \,\, ,\\
	or, \,\,\beta &=-\frac{\pi}{2},\gamma =-\alpha \pm \frac{\pi}{2}, \alpha \in \left[ -\pi ,\pi \right].\\
    \end{aligned}
\end{equation}
For another solution $\alpha=\pm\frac{\pi}{2}$, the first row of \cref{eq:c1&c3} is transformed to:
\begin{equation}
    \sin \gamma \sin \alpha +\cos \gamma \sin \beta \cos \alpha =\pm \sin \gamma =0.
\end{equation}
Thus we have:
\begin{equation}
    \alpha =\pm \frac{\pi}{2}, \gamma =0 \,\,or\,\,\pi , \beta \in \left[ -\pi ,\pi \right].\\
\end{equation}
As a result, the final solution is:
\begin{equation}
    \label{eq:colsolu}
    \begin{aligned}
        \beta &=\frac{\pi}{2}, \gamma =\alpha \pm \frac{\pi}{2}, \alpha \in \left[ -\pi ,\pi \right] ,\\
    	or,\, \beta &=-\frac{\pi}{2}, \gamma =-\alpha \pm \frac{\pi}{2}, \alpha \in \left[ -\pi ,\pi \right],\\
    	or,\, \alpha &=\pm \frac{\pi}{2}, \gamma =0 \,\,or\,\,\pi , \beta \in \left[ -\pi ,\pi \right].\\
    \end{aligned}
\end{equation}

% \textbf{If the slope of the projected line is $0$}, the projected line is indeed a row, which means $y_I$ is irrelevant with $z$, i.e. in the equation below:
% \begin{equation}
%     \frac{y_I-v_0}{f_y}=\frac{y_C}{z_C}=\frac{a_2r_{bev}\cos \phi_{bev} +b_2r_{bev}\sin \phi_{bev} +c_2z+d_2}{a_3r_{bev}\cos \phi_{bev} +b_3r_{bev}\sin \phi_{bev} +c_3z+d_3}
% \end{equation}
% The coefficients of z are 0, so similarly, we have that:
% \begin{equation}
%     \begin{cases}
%     	c_2=-\cos \gamma \sin \alpha +\sin \gamma \sin \beta \cos \alpha =0\\
%     	c_3=\cos \beta \cos \alpha =0\\
%     \end{cases}
% \end{equation}
% Solving this equation set, we can get the solution:
% \begin{equation}
%     \begin{cases}
%     	\beta =\frac{\pi}{2}, \gamma =\alpha \,\,or\,\,\alpha +\pi \,\,, \alpha \in \left[ -\pi ,\pi \right]\\
%     	\beta =-\frac{\pi}{2}, \gamma =-\alpha \,\,or\,-\alpha +\pi \,\,, \alpha \in \left[ -\pi ,\pi \right]\\
%     	\alpha =\pm \frac{\pi}{2},\gamma =\pm \frac{\pi}{2}, \beta \in \left[ -\pi ,\pi \right]\\
%     \end{cases}
% \end{equation}

% \textbf{For other conditions}, the slope of the projected is irrelevant with $z$. Combining \cref{eq:ex} and \cref{eq:in}, we have that:
% \begin{equation}
%     \frac{y_I-v_0}{f_y}\frac{f_x}{x_I-u_0}=\frac{y_C}{x_C}=\frac{a_2r_{bev}\cos \phi_{bev} +b_2r_{bev}\sin \phi_{bev} +c_2z+d_2}{a_1r_{bev}\cos \phi_{bev} +b_1r_{bev}\sin \phi_{bev} +c_1z+d_1}
% \end{equation}
% and we can consequently derive that:
% \begin{equation}
%     y_I=v_0+\frac{a_2r_{bev}\cos \phi_{bev} +b_2r_{bev}\sin \phi_{bev} +c_2z+d_2}{a_1r_{bev}\cos \phi_{bev} +b_1r_{bev}\sin \phi_{bev} +c_1z+d_1}\frac{f_y}{f_x}\left( x_I-u_0 \right) 
% \end{equation}
% the slope should also be irrelevant with $z$, thus the coefficients of z are 0. Similarly, we have that:
% \begin{equation}
%     \begin{cases}
%     	c_1=\sin \gamma \sin \alpha +\cos \gamma \sin \beta \cos \alpha =0\\
%     	c_2=-\cos \gamma \sin \alpha +\sin \gamma \sin \beta \cos \alpha =0\\
%     \end{cases}
% \end{equation}
% Solving this equation set, we can get the solution:
% \begin{equation}
%     \alpha =0\,\, or\,\,\pi , \beta =0\,\,or\,\,\pi , \gamma \in \left[ -\pi ,\pi \right] 
% \end{equation}
\begin{wrapfigure}{r}{0.4\textwidth}
\centering
\vspace{-10pt}
% Figure removed
\caption{Illustration of coordinate system transformation.}
\label{fig:coord}
\vspace{-20pt}
\end{wrapfigure}

In standard ADS, the LiDAR or radar coordinate system usually has a z-axis pointing up, while the camera coordinate system has a y-axis pointing up or down. So a widely adopted practice first exchanges the y and z axes, which means a roll $\alpha = \pm \frac{\pi}{2}$. After that, a pitch $\beta$ around the vertical y-axis is executed, while further rotation around the z-axis is not required, as shown in \cref{fig:coord}. In other words, the roll $\beta$ can be any value between $-\pi$ and $\pi$, while yaw $\gamma$ is set to 0. This is exactly the situation of the third solution of \cref{eq:colsolu}. And under this condition, we have expression of $R$ according to \cite{murray1994mathematical}:
\begin{equation}
    R=\left( \begin{matrix}
    	\cos \beta&		\pm \sin \beta&		0\\
    	0&		0&		\mp 1\\
    	-\sin \beta&		\pm \cos \beta&		0\\
    \end{matrix} \right) , T=\left( \begin{array}{c}
    	d_1\\
    	d_2\\
    	d_3\\
    \end{array} \right) .
\end{equation}
Thus, using \cref{eq:ex} and \cref{eq:col_org}, we can get the expression of column index $x_I$. If $\alpha =\frac{\pi}{2}$, we have:
\begin{equation}
    x_I=u_0+f_x\frac{r_{bev}\cos \left( \beta -\phi _{bev} \right) +d_1}{-r_{bev}\sin \left( \beta -\phi _{bev} \right) +d_3}.
\end{equation}
If $\alpha =-\frac{\pi}{2}$, we have:
\begin{equation}
	x_I=u_0+f_x\frac{r_{bev}\cos \left( \beta +\phi _{bev} \right) +d_1}{-r_{bev}\sin \left( \beta +\phi _{bev} \right) +d_3}.
\end{equation}
For RADIal, the rotation matrix from the radar coordinate system to the camera coordinate system is:
\begin{equation}
    R=\left( \begin{matrix}
	0.0465&		-0.9989&		-0.0051\\
	-0.0476&		0.0029&		-0.9989\\
	0.9978&		0.0467&		-0.0474\\
    \end{matrix} \right), 
\end{equation}
which is the approximation of $(\alpha, \beta, \gamma)=(\pi/2, -\pi/2, 0)$. And thus we approximately have:
\begin{equation}
    x_I \approx u_0+f_x\frac{-r_{bev}\sin \phi _{bev}+d_1}{r_{bev}\cos \phi _{bev}+d_3}.
\end{equation}
It can be inferred that $x_I$ is almost irrelevant to $z$, which means one pillar in the polar coordinate of radar corresponds to a column in the image.

\section{Visualization and Dataset Statistics}
\label{sec:data}
% Figure environment removed
To better illustrate the performance of our method, we visualize the predictions and ground truths in \cref{fig:vis}. As can be seen from the cases in the first and the second column, our method can accurately predict the position of vehicles. But there are several cases in which the accurate prediction does not get high confidence, as shown in the upper case in the third column. On the other hand, the orientation estimation error may be significant when the yaw of the vehicle is large, shown in the inferior case in the third column. The main reason is that the samples with large yaw are relatively rare in the dataset, as shown in \cref{fig:data_stat} (b) and (c).

% Figure environment removed
As shown in \cref{fig:data_stat} (a), the samples of test data are rather rare within 30 meters and out of 80 meters. To comprehensively explore the detection performance of objects, we divide the range into 0-50m and 50-100m, which correspond to short-range and long-range detection performance respectively.

% Figure environment removed
We further visualize scenarios that are corrected by our new labels. It is worth noting that the original annotation and prediction of previous methods are centers of objects' visible faces, and they assign a fixed scale and zero orientation when calculating Average Precision (AP). But vehicles of large angles and scales are normal in daily scenarios, as shown in \cref{fig:label}. Our new annotations based on dense LiDAR points correct this, helping a more comprehensive evaluation of existing methods. We also annotate other traffic participants such as pedestrians and cyclists. But due to their limited numbers (4 cyclists, 23 pedestrians in training split, and 17 cyclists, 0 pedestrians in test split), we donâ€™t add more fine-grained class labels.

%Thus a vastly adopted range interval of 5-30m, 30-50m, and 50-80m would lead to unrepresentative short-range performance. We adopt a more suitable division, 0-50m and 50-100m, so as to more comprehensively explore the detection performance of objects.



\section{Complementary property of radar and camera}
\label{sec:R&C}
\begin{table}
  \caption{LET-BEV-AP results with different longitudinal tolerance $T_l$. The performances of different modality combinations of our algorithm on RADIal Dataset test split are presented, using refined 3D bounding box annotations. C, L, and RT respectively refer to the camera, LiDAR, and range-time data. The results with LiDAR are on gray background.}
  \label{tab:let-bev}
  \centering
  \resizebox{0.99\textwidth}{!}{%
  \begin{tabular}{lllllllll}
    \toprule
    \multicolumn{3}{c}{Modality} & \multicolumn{3}{c}{$T_l=0.1$, LET-BEV-AP@0.7($\%$)$\uparrow$}&\multicolumn{3}{c}{ $T_l=0.3$, LET-BEV-AP@0.7($\%$)$\uparrow$}\\
    \cmidrule(r){1-3} \cmidrule(r){4-6} \cmidrule(r){7-9}
    C & RT & L & Overall & 0 - 50m & 50 - 100m & Overall & 0 - 50m & 50 - 100m \\
    \midrule
    \checkmark & & & 56.92$\pm$3.91 & 78.98$\pm$1.41 & 36.20$\pm$5.81 & 81.36$\pm$1.15 & 83.30$\pm$2.01 & 77.00$\pm$0.43\\
     & \checkmark & & 55.04$\pm$1.25 & 68.56$\pm$4.11 & 52.30$\pm$1.82 & 55.34$\pm$1.91 & 69.79$\pm$4.59 & 51.02$\pm$0.07\\
    \rowcolor{gray!20} & & \checkmark & 86.07$\pm$0.44 & 88.55$\pm$0.79 & 93.17$\pm$0.10 & 86.41$\pm$0.47 & 89.72$\pm$0.03 & 93.22$\pm$0.04\\
    \midrule
    \checkmark & \checkmark & & 88.86$\pm$0.62 & 92.81$\pm$1.37 & 94.81$\pm$0.52 & 88.56$\pm$0.53 & 92.58$\pm$0.70 & 94.57$\pm$0.29\\
    %\rowcolor{gray!20}
    %\checkmark & & & \checkmark & \checkmark & 86.53$\pm$0.05 & 87.98$\pm$1.11 & 95.08$\pm$0.47 & 88.35$\pm$0.24 & 91.30$\pm$0.42 & 96.19$\pm$0.58 \\
    \bottomrule
  \end{tabular}
  }
\end{table}

\begin{table}
  \caption{Recall with different IoU thresholds. The performances of different modality combinations of our algorithm on RADIal Dataset test split are presented, using refined 3D bounding box annotations. C, L, and RT respectively refer to the camera, LiDAR, and range-time data. The results with LiDAR are on gray background.}
  \label{tab:recall}
  \centering
  \resizebox{0.99\textwidth}{!}{%
  \begin{tabular}{lllllllll}
    \toprule
    \multicolumn{3}{c}{Modality} & \multicolumn{3}{c}{BEV recall@0.7($\%$)$\uparrow$}&\multicolumn{3}{c}{BEV recall@0.3($\%$)$\uparrow$}\\
    \cmidrule(r){1-3} \cmidrule(r){4-6} \cmidrule(r){7-9}
    C & RT & L & Overall & 0 - 50m & 50 - 100m & Overall & 0 - 50m & 50 - 100m \\
    \midrule
     & \checkmark & & 62.73$\pm$1.24 & 73.89$\pm$2.11 & 61.21$\pm$0.82 & 86.27$\pm$0.43 & 92.13$\pm$0.71 & 93.40$\pm$0.54\\
    \rowcolor{gray!20} & & \checkmark & 86.38$\pm$0.00 & 89.11$\pm$0.08 & 93.35$\pm$0.08 & 91.40$\pm$0.08 & 96.13$\pm$0.17 & 96.95$\pm$0.16\\
    \midrule
    \checkmark & \checkmark & & 86.18$\pm$0.13 & 89.13$\pm$0.6 & 92.53$\pm$0.56 & 91.78$\pm$0.85 & 95.82$\pm$0.50 & 97.67$\pm$1.05\\
    \bottomrule
  \end{tabular}
  }
\end{table}

\begin{table}
  \caption{BEV detection performances of different modality combinations of our algorithm on RADIal Dataset test split. Refined 3D bounding box annotations are applied. C, L, ADC, RD, RT, RA, RP respectively refer to the camera, LiDAR, ADC data, range-Doppler spectrum, range-time data, range-azimuth map, and radar point cloud. The best result without LiDAR of each metric are in \textbf{bold}, and the results with LiDAR are on gray background.}
  \label{tab:RD}
  \centering
  \resizebox{0.99\textwidth}{!}{%
  \begin{tabular}{lllllllllllll}
    \toprule
    \multicolumn{6}{c}{Modality} & \multicolumn{3}{c}{BEV AP@0.7($\%$)$\uparrow$}&\multicolumn{3}{c}{LET-BEV-AP@0.7($\%$)$\uparrow$}\\
    \cmidrule(r){1-7} \cmidrule(r){8-10} \cmidrule(r){11-13}
    C & ADC & RT & RD & RA & RP & L & Overall & 0 - 50m & 50 - 100m & Overall & 0 - 50m & 50 - 100m \\
    \midrule
    \checkmark & & & & & & & 10.07$\pm$1.10 & 21.68$\pm$3.11 & 2.21$\pm$0.59 & 56.92$\pm$3.91 & \textbf{78.98$\pm$1.41} & 36.20$\pm$5.81 \\
    & \checkmark & & & & & & 49.64$\pm$0.43 & 61.37$\pm$1.57 & 47.92$\pm$1.48 & 56.52$\pm$2.36 & 68.93$\pm$1.87 & 54.68$\pm$1.76 \\
    & & \checkmark & & & & & 48.26$\pm$0.92 & 62.33$\pm$4.00 & 45.05$\pm$0.98 & 55.04$\pm$1.25 & 68.56$\pm$4.11 & 52.30$\pm$1.82 \\
    & & & \checkmark  & & & & 47.34$\pm$0.73 & 59.31$\pm$1.64 & 42.53$\pm$1.98 & 54.96$\pm$0.67 & 67.02$\pm$1.67 & 50.59$\pm$0.26 \\
    & & & & \checkmark & & & 50.67$\pm$1.03 & 65.05$\pm$1.59 & 42.98$\pm$0.51 & 58.53$\pm$0.13 & 72.95$\pm$0.78 & 51.41$\pm$0.54 \\
    & & & & & \checkmark & & \textbf{53.55$\pm$0.70} & \textbf{66.19$\pm$1.72} & \textbf{47.41$\pm$1.06} & \textbf{62.59$\pm$1.14} & 76.45$\pm$2.06 & \textbf{56.73$\pm$1.33} \\ \rowcolor{gray!20}
    & & & & & & \checkmark & 84.47$\pm$0.15 & 86.92$\pm$0.61 & 91.91$\pm$0.13 & 86.07$\pm$0.44 & 88.55$\pm$0.79 & 93.17$\pm$0.10 \\
    \midrule
    \checkmark & \checkmark & & & & & & 84.85$\pm$0.22 & 87.68$\pm$1.54 & 91.46$\pm$1.08 & 88.66$\pm$0.63 & 92.43$\pm$1.15 & 94.99$\pm$0.66 \\
    \checkmark & & \checkmark & & & & & \textbf{84.92$\pm$0.98} & 87.56$\pm$1.58 & 91.06$\pm$1.18 & 88.86$\pm$0.62 & 92.81$\pm$1.37 & 94.81$\pm$0.52 \\
    \checkmark & & & \checkmark & & & & 83.31$\pm$0.39 & 87.26$\pm$0.56 & 89.16$\pm$0.44 & 88.24$\pm$0.06 & 92.18$\pm$0.43 & 93.26$\pm$0.73 \\
    \checkmark & & & & \checkmark & & & 84.77$\pm$0.65& \textbf{87.93$\pm$0.60} & \textbf{91.48$\pm$0.28} & \textbf{89.54$\pm$0.54} & \textbf{93.83$\pm$0.56} & \textbf{94.87$\pm$0.58} \\
    \checkmark & & & & & \checkmark & & 82.35$\pm$0.93 & 86.97$\pm$1.01 & 86.39$\pm$0.74 & 88.35$\pm$0.78 & 93.07$\pm$0.80 & 92.77$\pm$0.69 \\ \rowcolor{gray!20}
    \checkmark & & & & & & \checkmark & 86.35$\pm$1.15 & 88.81$\pm$1.40 & 94.25$\pm$1.68 & 88.44$\pm$0.79 & 91.63$\pm$1.34 & 95.31$\pm$1.28 \\ 
    %\rowcolor{gray!20}
    %\checkmark & & & \checkmark & \checkmark & 86.53$\pm$0.05 & 87.98$\pm$1.11 & 95.08$\pm$0.47 & 88.35$\pm$0.24 & 91.30$\pm$0.42 & 96.19$\pm$0.58 \\
    \bottomrule
  \end{tabular}
  }
\end{table}

\begin{table}
  \caption{Ablation on the coordinate system. Experiments are carried on RADIal dataset, with RT data and image as input. }
  \label{tab:DDM}
  \centering
  \resizebox{0.99\textwidth}{!}{%
  \begin{tabular}{lllllll}
    \toprule
    & \multicolumn{3}{c}{BEV AP@0.7($\%$)$\uparrow$}&\multicolumn{3}{c}{LET-BEV-AP@0.7($\%$)$\uparrow$}\\
    \cmidrule(r){2-4} \cmidrule(r){5-7}
     Coordinate & Overall & 0 - 50m & 50 - 100m & Overall & 0 - 50m & 50 - 100m \\
    \midrule
    Polar & 84.92$\pm$0.98 & 87.56$\pm$1.58 & 91.06$\pm$1.18 & 88.86$\pm$0.62 & 92.81$\pm$1.37 & 94.81$\pm$0.52 \\
    Sphere & 85.02$\pm$0.60 & 88.02$\pm$0.59 & 91.76$\pm$0.98 & 89.97$\pm$0.61 & 93.35$\pm$0.32 & 96.41$\pm$1.21 \\
    \bottomrule
  \end{tabular}
  }
\end{table}

To highlight the complementary nature of radar and camera properties, we investigated the performance of LET-BEV-AP with varying longitudinal tolerance ($T_l$), as presented in Table \ref{tab:let-bev}. Notably, when using a larger $T_l$ of 0.3, the camera's performance experiences a significant improvement. This suggests that the relatively lower performance of long-range LET-BEV-AP under $T_l$ of 0.1 primarily stems from depth errors. By reducing the impact of longitudinal estimation, the camera's exceptional angular perception accuracy becomes more pronounced. Conversely, the LET-BEV-AP performance of the radar-only variant remains relatively unchanged. This finding indicates that the radar's depth estimation is quite accurate and minimally affected by the value of $T_l$. Similarly, the highly accurate LiDAR-only variant and camera and RT fusion variant don't suffer from performance fluctuation as well, which supports our conclusion.

To further investigate the primary factor affecting radar performance, we conducted a comparison of BEV recall at IoU thresholds of 0.7 and 0.3, as outlined in Table \ref{tab:recall}. It can be deduced that by using a less stringent IoU threshold, the recall of the RT-only variant experiences a significant increase. This result suggests that the RT-only variant's limitation mainly lies in its inferior angular localization ability. But by combining the strengths of camera and radar modalities, the model can leverage the advantages of both modalities, leading to a significant performance boost. 

\section{Doppler domain multiplexing and coordinate system}
% Figure environment removed

% \begin{table}
%   \caption{Ablation on preprocessing for DDM and coordinate system. Experiments are carried on RADIal dataset, with RT data and image as input. Here, Preproc. and Coord. denotes preprocessing method and coordinate system. And fixed means using \cref{eq:DDM} while learned means applying complex linear layer to resolve DDM.}
%   \label{tab:DDM}
%   \centering
%   \resizebox{0.99\textwidth}{!}{%
%   \begin{tabular}{llllllll}
%     \toprule
%     & \multicolumn{3}{c}{BEV AP@0.7($\%$)$\uparrow$}&\multicolumn{3}{c}{LET-BEV-AP@0.7($\%$)$\uparrow$}\\
%     \cmidrule(r){3-5} \cmidrule(r){6-8}
%      Preproc. & Coord. & Overall & 0 - 50m & 50 - 100m & Overall & 0 - 50m & 50 - 100m \\
%     \midrule
%     None & Polar & 83.18$\pm$0.71 & 87.17$\pm$0.78 & 88.56$\pm$1.64 & 88.04$\pm$0.58 & 92.79$\pm$1.18 & 93.96$\pm$1.41 \\
%     fixed & Polar & 84.97$\pm$0.45 & 87.78$\pm$0.32 & 90.79$\pm$0.96 & 89.60$\pm$0.43 & 93.23$\pm$0.14 & 95.32$\pm$0.42 \\
%     learned & Polar & 84.68$\pm$0.68 & 87.39$\pm$0.60 & 90.97$\pm$0.95 & 88.78$\pm$0.24 & 92.52$\pm$0.61 & 94.45$\pm$0.85 \\
%     learned & Sphere & 85.02$\pm$0.60 & 88.02$\pm$0.59 & 91.76$\pm$0.98 & 89.97$\pm$0.61 & 93.35$\pm$0.32 & 96.41$\pm$1.21 \\
%     \bottomrule
%   \end{tabular}
%   }
% \end{table}

As mentioned in \cite{rebut2022raw}, Doppler Domain Multiplexing(DDM) is used to distinguish received radar signals from different transmitters. The mechanism can be best illustrated in Range-Doppler (RD) spectrum format of data, as shown in \cref{fig:DDM}. Without DDM, the valid measurable speed interval is $[-V_{max}, V_{max}]$. Suppose we have four transmitters, Tx 1/2/3/4, and one object with speed in $[0,V_{max}/2]$. The echo signals from Tx 1/2/3/4 will respectively fall into sections C, D, A, B by using DDM, showing a periodical pattern. But now we can only measure speed in $[0,V_{max}/2]$. Similar phenomenon can be observed in the figure of Range Doppler data, i.e. the third image in \cref{data_vis} in the paper.

% Mathematically, suppose that the RD spectrum is of dimension $(B_R, B_D, N_{Rx})$, where $B_R$, $B_D$ are the numbers of discretization bins for range and Doppler respectively, and $N_{Rx}$ is the number of radar receiver. Using DDM, a specific object will periodically appear $N_{Tx}$ times, where $N_{Tx}$ is the number of transmitters. Suppose that the object has range $r$ and relative radial velocity $d$ expressed in Doppler effect. Then the signal emitted by $n_T$-th transmitter, reflected by object, and received by $n_R$-th receiver can be extracted as follows:
% \begin{equation}
%     \hat{F}\left( r, d, n_T, n_R \right) =F\left( r, d+\varDelta \left( n_T \right) , n_R \right),
% \end{equation}
% where $\varDelta \left( n_T \right)$ is the Doppler shift corresponding to $n_T$-th transmitter, and $F$ is the 3-dimensional RD spectrum. However, different transmitters can only be linearly separated in Doppler domain. But for range-time (RT) data $f(r, s, n_R)$, the second dimension is still in slow time domain. It is correlated with RD spectrum by FFT:
% \begin{equation}
%     F\left( r, d, n_R \right) =FFT_{2nd}\left( f\left( r, s, n_R \right) \right),
% \end{equation}
% where $FFT_{2nd}$ means 1D-FFT on second dimension. To resolve DDM in slow time domain, we apply translation rule of FFT and have:
% \begin{equation}
% \label{eq:DDM}
%     F\left( r,d+\varDelta \left( n_T \right) ,n_R \right) =FFT_{2nd}\left( f\left( r,s,n_R \right) \exp \left( j2\pi \frac{-\varDelta \left( n_T \right) s}{D} \right) \right).
% \end{equation}
% Thus, $\hat{f}\left( r,s,n_T \times n_R \right) =f\left( r,s,n_R \right) \exp \left( j2\pi \frac{-\varDelta \left( n_T \right) s}{D} \right) 
% $ corresponds to DDM resolved version of RT data. And therefore this transformation can be replaced by an complex linear layer initialized with corresponding coefficients, which lifts the input RT data channel from $n_R$ to $n_T \times n_R$. As can be observed in \cref{tab:DDM}, a correct DDM resolver is critical for the final performance. Besides, the learned preprocessing provides on-par performance with fixed version, while enables end-to-end training.

We also carried out experiments on coordinate system of BEV space. Though the range dimension is under sphere system, we found that a polar system can achieve on par performance, as shown in \cref{tab:DDM}. Thus we apply the polar system in the paper.


\section{More experimental results}
\label{sec:detail}
\begin{table}
  \caption{BEV detection performances of different modality combination on KRadar Dataset test split with KITTI protocol of 40 recall positions. C, RA, RP respectively refer to the camera, range-azimuth map, and radar point cloud. The best result of each metric is in \textbf{bold}.}
  \label{tab:kradar-ablate}
  \centering
  \resizebox{0.99\textwidth}{!}{%
  \begin{tabular}{llllllll}
    \toprule
    & & \multicolumn{3}{c}{BEV AP($\%$)$\uparrow$}&\multicolumn{3}{c}{3D AP($\%$)$\uparrow$}\\
    \cmidrule(r){3-5} \cmidrule(r){6-8}
     Training set & Method & AP@0.3 & AP@0.5 & AP@0.7 & AP@0.3 & AP@0.5 & AP@0.7 \\
    \midrule
    KRadar-20-train & EchoFusion(RP) & 55.74 & 43.94 & 20.56 & 53.40 & 29.21 & 2.90 \\
    KRadar-20-train & EchoFusion(RA) & 54.45 & 42.28 & 17.97 & 51.75 & 24.65 & 3.85 \\
    KRadar-20-train & EchoFusion(RP+C) & 66.46 & 54.17 & 26.39 & 58.62 & \textbf{34.67} & 3.96 \\
    KRadar-20-train & EchoFusion(RA+C) & \textbf{68.70} & \textbf{55.90} & \textbf{29.65} & \textbf{66.68} & 34.33 & \textbf{5.34} \\
    \bottomrule
  \end{tabular}
  }
\end{table}
Referring to Table \ref{tab:RD}, we have included the detection performance when our model consumes the ADC data or range-Doppler spectrum as input. It is worth noticing that we add an extra complex linear layer initialized with FFT coefficients for ADC data to replace the range FFT and the side-lobe suppression. In the case of the single modality version, the performance discrepancy between RT and these two modalities remains within the margin of error. When considering the fusion of image data, the performance of RD slightly lags behind that of RT, while that of ADC is almost the same as RT. These findings suggest that the integration of image data significantly mitigates the importance of conventional radar signal processing in accurate radar-based detection.

Besides, we offer ablation of different modality combinations on KRadar. The results are reported in \cref{tab:kradar-ablate}. Here, we train all models with training split. The RA map shows considerable superiority over radar point cloud when fusing with the monocular image, which aligns with our research motivation and results on RADIal dataset.