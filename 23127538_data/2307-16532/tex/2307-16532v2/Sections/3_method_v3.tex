\section{Preliminary}
\label{preliminary}

% Figure environment removed

In the automotive industry, multiple-input, multiple-output (MIMO) frequency-modulated continuous-wave (FMCW) radars are adopted \cite{zhou2022towards} to balance cost, size, and performance. Each transmitting antenna (Tx) transmits a sequence of FMCW waveforms, also named chirps, which will be reflected by objects and returned to the receiving antennas (Rx). Then the echo signal will be first mixed with the corresponding emitted signal and then passed through a low pass filter, so as to obtain the intermediate frequency (IF) signal. The discretely sampled IF signal, which includes the phase difference of emitted and returned signal, is called \textbf{ADC data} \cite{sun2020mimo}. The ADC data has three dimensions, fast time, slow time, and channel, which have physical correspondence with range, speed, and angle respectively.

Because of the continuous-wave nature, the IF signal of a chirp contains a frequency shift caused by the time delay of traveling between radar and objects. Then the radial distance of obstacles can be extracted by applying Fast Fourier Transform (FFT) on each chirp, i.e. fast-time dimension. This FFT operation is also called range-FFT. The resulting complex signal is denoted as \textbf{range-time (RT) data}. Next, a second FFT (Doppler FFT) along different chirps, i.e. slow-time dimension, is conducted to extract the phase shift between two chirps caused by the motion of targets. The derived data is named \textbf{range-Doppler (RD) spectrum}. We denote $N_{Tx}$ and $N_{Rx}$ respectively as the number of transmitting and receiving antennas. And each grid on the RD spectrum has $N_{Tx} \times N_{Rx}$ channels. The small distance between adjacent virtual antennas leads to phase shifts among different antennas, which is dependent on the AoA. Thus a third FFT (angle-FFT) can be applied along the channel axis to unwrap AoA. 
The AoA can be further decoded to azimuth and elevation based on the radar antenna calibration. The final 4D tensor is referred to as the \textbf{range-elevation-azimuth-Doppler (READ) tensor}. 

If point cloud is desired, the RD spectrum will be first processed by the CFAR algorithm to filter out peaks as candidate points. Then the angle-FFT will only be executed on these points. Final \textbf{radar point cloud (PCD)} consists of 3D coordinates, speed, and reflective intensity. In traditional radar signal processing, there is another data format named \textbf{range-azimuth (RA) map}, which is obtained by decoding the azimuth of a single elevation on the RD spectrum and compressing the Doppler dimension to one channel. For comprehensive analyses, we include this modality in our experiments as well. The data formats mentioned above are illustrated in Figure \ref{data_vis}, except for READ tensor because it is difficult to visualize.
\section{Methods}

In this section, we will introduce our network design. Firstly we will give a brief introduction of the whole pipeline in Section \ref{overall_structure}. Then our proposed Polar-Aligned Attention module will be elaborated in Section \ref{Column-wise Image Fusion} and Section \ref{Range-wise Radar Fusion}. Finally, the detection head and loss functions are introduced in Section \ref{head_loss}.

\label{headings}
\subsection{Overall Structure}
\label{overall_structure}
% Figure environment removed

The overall architecture of our model is shown in Figure \ref{pipeline}.  First, we use separate backbones and FPNs \cite{lin2017feature} to extract multi-level features from RT radar maps and images. Then the polar queries aggregate camera features and radar features by cross-attention. Finally, the multi-scale polar BEV map will be passed through a transformer-based decoder with object queries. The final score and bounding box predictions can be obtained by decoding the output object queries. The core of our EchoFusion is how to fuse the camera and radar slow-time data. We fuse them using a novel strategy named Polar Aligned Attention (PA-Attention). In the next two subsections, we will explain this strategy in detail.

\subsection{Column-wise Image Fusion}
\label{Column-wise Image Fusion}
After obtaining image and radar features from corresponding encoders, we first initialize the $l$-th level polar queries $\boldsymbol{Q}_{\boldsymbol{l}}\in \mathbb{R} ^{R_l\times A_l\times d}$ uniformly in polar BEV space. Here, $R_l$, $A_l$, and $d$ respectively denote the shape of range, azimuth, and channel dimensions. For brevity, we omit $l$ in the following two subsections. Given a query $q\in \mathbb{R} ^d$ located at $(r_{bev}, \phi_{bev})$, it corresponds to a pillar centered at $(r_{bev}, \phi_{bev})$ with infinite height. Taking the coordinate system defined in RADIal \cite{rebut2022raw}, $x_R$, $y_R$, and $z_R$ axes respectively points to the front, left, and upright, while subscript $R$ means radar coordinate. Similarly, the camera coordinates are denoted as $(x_C, y_C, z_C)$, which points to the right, down, and front, respectively. Thus we have correspondence in Cartesian coordinate system as follows:
\begin{equation}
    \label{w-polar}
    x_R=r_{bev}\cos \left( \phi _{bev} \right), \quad y_R=r_{bev}\sin \left( \phi _{bev} \right).
\end{equation}
Using extrinsic matrix and intrinsic matrix, we have the following formulation:
\begin{equation}
    \frac{x_I-u_0}{f_x}=\frac{x_C}{z_C},\quad \left( \begin{array}{c}
    	x_C\\
    	y_C\\
    	z_C\\
    \end{array} \right) =R\left( \begin{array}{c}
    	x_R\\
    	y_R\\
    	z_R\\
    \end{array} \right) +T, T=\left( \begin{array}{c}
    	d_1\\
    	d_2\\
    	d_3\\
    \end{array} \right) ,
\end{equation}
where $u_0$, $f_x$ are principal point offset and focal length on $x$-axis in the intrinsic matrix, $x_I$ is the image column index that corresponds to the point $(x_R, y_R, z_R)$, while $R$ and $T$ are rotation matrix and translation vector of the calibration matrix between camera and radar, respectively. 
Under mild distortion conditions and normally adopted sensor setting, $x_I$ is determined by pillar position and calibration parameters as follows:
\begin{equation}
    x_I \approx u_0+f_x\frac{-r_{bev}\sin \phi _{bev}+d_1}{r_{bev}\cos \phi _{bev}+d_3}.
\end{equation}
Limited by the number of pages, we leave the detailed proof in the appendix.
% Under mild conditions, we can derive that the pillar exactly matches a specific column in the image. Limited by the number of pages, we leave detailed proof in the appendix.
% In the standard sensor setting of ADS, the front camera, LiDAR, and radar are parallel to roads, pointing in the driving direction. This enables the rotation matrix to only involve a 90-degree rotation around the right x-axis of the radar coordinate, i.e. $|\alpha |=\pi /2$, $\beta =\gamma =0$, where $\alpha$, $\beta$, $\gamma$ are respectively Euler angles roll, pitch, and yaw correlated with rotation matrix R. And it is also the exact case in the RADIal dataset. With this precondition and assuming that $\alpha=\pi /2$, we re-express the $x_I$ using $r_{bev}$ and $\phi_{bev}$ as follows:
% \begin{equation}
%     \label{r_phi_to_x}
%     x_I=u_0+f_x\frac{r_{bev}\cos \left( \phi _{bev} \right) + t_x}{r_{bev}\sin \left( \phi _{bev} \right) +  t_z},
% \end{equation}
% which means the pillar exactly matches a specific column in the image.

However, the height is not specified for a query, while for a pixel $(x_I, y_I)$ on the monocular image, its depth is unknown as well. Thus we cannot precisely associate a query to the image feature, but we can correspond queries of the same azimuth to the same column of images. Enlightened by this observation, we use a versatile cross-attention mechanism to flexibly aggregate required features from images. %In detail, we can first collect features of all the locations in the column corresponding with the pillar. 
Namely, all the queries with the same azimuth form the query matrix, while all the features in the corresponding column in the image form the key and value matrix.
%Consequently, the cross-attention \cite{vaswani2017attention} is utilized to update query $q$. 
Formally, these two steps can be expressed as follows:
\begin{equation}
    \begin{aligned}
        F_I( x_I, \cdot) &=\mathrm{Stack}\left( [ f_I\left( x_I,y_I \right)] \right) \in \mathbb{R} ^{H_l\times d}, \quad \forall y_I \in [0,H_l-1], \\
        \hat{q}\left( r_{bev},\phi _{bev} \right) &=\mathrm{CrossAttn}\left( q\left( r_{bev},\phi _{bev} \right) , F_I\left( x_I,\cdot \right) , F_I\left( x_I,\cdot \right) \right) ,
    \end{aligned}
\end{equation}
where $f_I( x_I,y_I)$ is the image feature located at $x_I,y_I$, while details of cross attention function $\mathrm{CrossAttn}$ can be found in \cite{vaswani2017attention}.

\subsection{Range-wise Radar Fusion}
\label{Range-wise Radar Fusion}
We denote updated BEV queries as $\hat{\boldsymbol{Q}}_{\boldsymbol{l}}\in \mathbb{R} ^{R_l\times A_l\times d}
$. After that, it will be used to sample features from radar. Since the radar feature map shares the same range partition with the BEV queries, the updated query $\hat{q}\left( r_{bev},\phi _{bev} \right)$ matches the same range index of the radar feature, i.e. $r_R=r_{bev}$.
To aggregate all the features from all chirps in one frame, we perform cross attention for a certain query $\hat{q}$ with the row $r_R$ in RT map as key and value. Formally,
%However, the slow-time correspondence of $\hat{q}\left(r_{bev},\phi _{bev} \right)$ is non-trivial to determine because this dimension doesn't exist in the polar BEV query. But using a similar strategy, we can bypass this problem. With radar row index $r_R$, we can collect all the radar bins located in this row as keys and values. The cross-attention \cite{vaswani2017attention} is then applied to update query $\hat{q}$. These two steps can be formulated as follows:
\begin{equation}
    \begin{aligned}
        F_R( r_R, \cdot ) &=\mathrm{Stack}\left( [ f_R( r_R,t_R )] \right) \in \mathbb{R} ^{R_l\times d}, \quad \forall t_R \in [0,T_l-1] ,
        \\
        \tilde{q}\left( r_{bev},\phi _{bev} \right) &=\mathrm{CrossAttn}\left( \hat{q}\left( r_{bev},\phi _{bev} \right) , F_R\left( r_R, \cdot \right) , F_R\left( r_R, \cdot \right) \right) ,
        %\\
        %&=\mathrm{CrossAttn}\left( \hat{q}\left( r_{bev},\phi _{bev} \right) ,G_R\left( r_{bev} \right) , G_R\left( r_{bev} \right) \right) 
    \end{aligned}
\end{equation}
where $R_l$ and $T_l$ are the height and width of the radar feature map, and  $f_R\left( r_R,t_R \right) \in \mathbb{R} ^d$ is the radar feature of position $(r_R, t_R)$.

%For the radar echoes belonging to a specific range, their azimuth distribution can be fully described by all bins scattered in the corresponding row. And thus the query can inherently learn how to capture the required information to help locate the correct position of objects.

%By applying this cross-attention strategy, the fusion module is no longer bothered by the mismatched FoV between radar and camera. The row and column attention elaborately get around the problem of the image lacking a range dimension and the RT data of radar lacking an explicit azimuth dimension.
%, and alignment uncertainty caused by low-resolution sensing ability of specific direction. 
%Besides, this structure can be easily extended to LiDAR or other data formats of radar, which facilitates exploring the effects of different modalities.

Note that though we don't explicitly decode AoA as in RA map, the AoA has been implicitly encoded in the phase difference of the response of different virtual antenna. Consequently, it can be learned from the features in RT map indirectly.

%We also try to use RA map in ablation study. Although it is also in polar coordinate, the azimuth resolution is different from that of the BEV queries. For implementation simplicity, we still conduct the same attention mechanism as RT.

\subsection{Head and Loss}
\label{head_loss}
We follow polar BEV decoders \cite{jiang2022polarformer} to decode multi-level BEV features and make predictions from object queries. Since we don't have velocity annotation on this dataset, we remove the corresponding branch. The regression targets include ($d_\rho$, $d_\phi$, $d_z$, $\mathrm{log}\space l$, $\mathrm{log}\space w$, $\mathrm{log}\space h$, $\mathrm{sin}(\theta_{ori}-\phi)$, $\mathrm{cos}(\theta_{ori}-\phi)$), where $d_\rho$, $d_\phi$, $d_z$ are relative offset to the reference point ($\rho$, $\phi$, $z$), $l$, $w$, $h$, $\theta_{ori}$ are length, width,  height, and orientation of bounding box. The classification and regression tasks are respectively supervised by Focal loss \cite{lin2017focal} and L1 loss.