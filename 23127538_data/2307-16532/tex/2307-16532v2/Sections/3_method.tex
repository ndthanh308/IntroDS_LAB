\section{Preliminary}
\label{preliminary}
% If an object with radial speed $v$ has a radial distance $R_0$ and angle of arrival (AoA) $\theta$ to the radar, it will have a complex response on ADC data with phase:
% \begin{equation}
%     \label{phase}
%     \begin{aligned}
%         &P_{D}^{H}\left( \mathrm{m},\mathrm{n},\mathrm{k} \right) \approx 2\pi \left[ -\mu \cdot \tau _0\cdot \mathrm{m}\cdot T_s+f_d\cdot \mathrm{n}\cdot T_r \right] +\mathrm{k}\cdot \varphi _A+\varphi _0
%         \\
%         &\mu =B/T_r, \tau _0=2R_0/c, f_d=2v/c, \varphi _A=\frac{2\pi}{\lambda}d\sin \theta ,
%         \\
%         &\mathrm{m}\in \left[ 0,N_f-1 \right] , \mathrm{n}\in \left[ 0,N_s-1 \right] , \mathrm{k}\in \left[ 0,N_a-1 \right] 
%     \end{aligned}
% \end{equation}
% where $N_f$, $N_s$, and $N_a$ are the number of sampling points, chirps, and virtual antennas, respectively. $B$ and $\lambda$  denote the bandwidth and wavelength of the FMCW waveform, while $T_s$ and $T_r$ denote the sampling period and chirp period. $d$ and $c$ are respectively the space distance of adjacent virtual antenna and light speed. 

% Figure environment removed
In automotive industry, multiple-input, multiple-output (MIMO) frequency-modulated continuous-wave (FMCW) radars are adopted \cite{zhou2022towards} to balance cost, size and performance. Each transmitting anttenas(Tx) transmit a sequence of FMCW waveforms, also named chirps, will be reflected by objects and returned to the receiving antennas(Rx). Then the echo signal will be first mixed with corresponding emitted signal and then passed through a low pass filter, so as to obtain the intermediate frequency (IF) signal. The discretely sampled IF signal, which includes the phase difference of emitted and returned signal, is called \textbf{ADC data}\cite{sun2020mimo}. 

Because of the continuous-wave nature, the IF signal of a chirp contains a frequency shift caused by the time delay of traveling between radar and objects. Then the radial distance of obstacles can be extracted by applying Fast Fourier Transform (FFT) on each chirp (range-FFT). The resulting complex signal is denoted as \textbf{range-time (RT) data}. Next, a second FFT along different chirps is conducted to extract the phase shift between two chirps caused by the motion of targets. The derived data is named \textbf{range-Doppler (RD) spectrum}. We denote $N_{Tx}$ and $N_{Rx}$ respectively as the number of transmit and receive antennas. And each grid on the RD spectrum has $N_{Tx}$ by $N_{Rx}$ channels. The small distance between adjacent virtual antennas leads to phase shift among different antennas, which is dependent on the AoA. Thus a third FFT (angle-FFT) can be applied along the channel axis to unwrap AoA. 
The AoA can be further decoded to azimuth and elevation based on the radar anttena calibration. The final real 3D tensor is referred to as the \textbf{range-azimuth-doppler (RAD) tensor}. 

If point cloud is desired, the RD spectrum will be first processed by CFAR algorithm to filter out peaks as candidate points. Then the angle-FFT will only be executed on these points. Final \textbf{radar point clouds (PCD)} consists of 3D coordinates, speed, and reflective intensity. In traditional radar signal processing, there is another data format named \textbf{range azimuth (RA) map}, which is obtained by decoding the azimuth of a single elevation on the RD spectrum and compressing the Doppler dimension to one channel. For comprehensive analyses, we include this modality in our experiments as well. The data formats mentioned above are illustrated in Figure \ref{data_vis}.
\section{Methods}
\label{headings}
\subsection{Overall Structure}
% Figure environment removed

The overall architecture of our model is shown in Figure \ref{pipeline}.  First, we use separate backbones and FPNs \cite{lin2017feature} to extract multi-level features from RT radar maps and images. Then the polar queries aggregate camera features and radar features by deformable attention\footnote{cite sth}. Finally, the multi-scale polar BEV map will be passed through a transformer-based decoder. The final score and bounding box predictions can be obtained by decoding the output object queries. 

% \subsection{Radar Data Preprocessing}
% The range FFT results in a complex 3D tensor of shape ($B_R$, $B_S$, $N_{Rx}$), in which $B_R$, $B_S$, and $N_{Rx}$ are respectively numbers of discretization bins of range and slow-time dimension and the number of receiving antennas. Due to Doppler domain multiplexing (DDM), the same object will be visible $N_{Tx}$ times, one per emitting antenna \cite{rebut2022raw}.  In detail, on the RD spectrum, the object of range $R$ and relative radial velocity $D$ will appear at range-Doppler positions $\left[ \left( R, \left( D+n_{Tx}\varDelta \right) \mathrm{mod}\left( D_{\max} \right) \right) \right] _{n_{Tx}=1,..., N_{Tx}}$. $\varDelta$ is the Doppler shift generated by DDM, and $D_{\max}$ is the largest measurable Doppler values. 

% On the range-slow-time data, it is required to convert the phase shift in the Doppler domain to the slow-time domain. Based on the translation rule of DFT \cite{gonzalez2008digital}, the $n_{Tx}$-th transmitter corresponds to signal of 
% \begin{equation}
%     \hat{f}\left( b_R,b_S,n_{Rx},n_{Tx} \right) =f\left( b_R,b_S,n_{Rx} \right) \exp \left( -j2\pi \frac{n_{Tx}\varDelta \cdot b_S}{D_{\max}} \right) 
% \end{equation}
% where $f$ and $\hat{f}$ are respectively feature of input and output complex tensor. By applying this formulation and merging the last two channels, the signal intricacy will be solved, which facilitates further angle information extraction of the network. 

\subsection{Complementary Modality Fusion}
The core of our EchoFusion is how to fuse the camera and radar-slow-time data. We fuse them using a novel strategy named Polar Aligned Fusion (PAFusion). We start from multi-level features extracted by backbones and FPNs of radar and camera. Given the scale level of $l$, suppose that the corresponding image feature is in the shape of $H_l\times W_l\times C$. %where $H_l$, $W_l$, and $C$ are the height, width, and feature number of the feature map. 
And $\boldsymbol{f_{l,w}}\in \mathbb{R} ^{H_l\times C}$ denotes features  of tokens contained in $w$-th column. The initialized polar rays are in the shape of $R_l\times W_l\times C$, which has the same width and feature number, but different heights $R_l$ decided by the range span. Let $\boldsymbol{p_{l,w}}\in \mathbb{R} ^{R_l\times C}$ denote the polar ray of $w$-th column, then the azimuth-aligned cross attention can be formulated as follows:
\begin{equation}
    \begin{aligned}
        &\hat{\boldsymbol{p}}_{\boldsymbol{l},\boldsymbol{w}}=\mathrm{MultiHead}\left( \boldsymbol{p}_{\boldsymbol{l},\boldsymbol{w}},\bar{\boldsymbol{f}}_{\boldsymbol{l},\boldsymbol{w}},\bar{\boldsymbol{f}}_{\boldsymbol{l},\boldsymbol{w}} \right) =\mathrm{Concat}\left( \boldsymbol{z}^{\left( \boldsymbol{1} \right)},...,\boldsymbol{z}^{\left( \boldsymbol{n} \right)} \right) \boldsymbol{W}_{\boldsymbol{l}}^{\boldsymbol{O}}
        \\
        &\boldsymbol{z}^{\left( \boldsymbol{n} \right)}=\mathrm{Attention}\left( \boldsymbol{p}_{\boldsymbol{l},\boldsymbol{w}}\boldsymbol{W}_{\boldsymbol{l},\boldsymbol{n}}^{\boldsymbol{Q}},\bar{\boldsymbol{f}}_{\boldsymbol{l},\boldsymbol{w}}\boldsymbol{W}_{\boldsymbol{l},\boldsymbol{n}}^{\boldsymbol{K}},\bar{\boldsymbol{f}}_{\boldsymbol{l},\boldsymbol{w}}\boldsymbol{W}_{\boldsymbol{l},\boldsymbol{n}}^{\boldsymbol{V}} \right) 
    \end{aligned}
\end{equation}
where $\boldsymbol{\bar{f}_{l,w}}$ is the camera features with sine positional embedding. And $\boldsymbol{W}_{\boldsymbol{l},\boldsymbol{n}}^{\boldsymbol{Q}}\in \mathbb{R} ^{C\times C_{\mathrm{q}}}$, $\boldsymbol{W}_{\boldsymbol{l},\boldsymbol{n}}^{\boldsymbol{K}}\in \mathbb{R} ^{C\times C_{\mathrm{k}}}$, $\boldsymbol{W}_{\boldsymbol{l},\boldsymbol{n}}^{\boldsymbol{V}}\in \mathbb{R} ^{C\times C_{\mathrm{v}}}$, $\boldsymbol{W}_{\boldsymbol{l}}^{\boldsymbol{O}}\in \mathbb{R} ^{C\times C}$ are projection matrices. $C_{\mathrm{q}}=C_{\mathrm{k}}=C_{\mathrm{v}}=\mathrm{C}/n$, $n$ is the number of heads. 

The updated polar rays are stacked along the azimuth axis and form the polar feature map $\boldsymbol{\hat{P}_l}\in \mathbb{R} ^{R_l\times W_l\times C}$. In order to convert $\boldsymbol{\hat{P}_l}$ to a BEV feature map under radar coordinate system, a series of 3D points $S_{l}^{3D}=\left\{ \left( \rho _i,\phi _j,z_k \right) |i\in \left[ 1,R_l \right],j\in \left[ 1,A_l \right] ,k\in \left[ 1,Z_l \right] \right\} $ within the field of view (FoV) of the camera are generated. $A_l$ and $Z_l$ are respectively the number of points scattered in azimuth and z dimensions. Then the points are projected to the polar plane points $S_{l}^{P}=\left\{ \left( r_x,w_y \right) |x\in \left[ 1,R_l \right] ,j\in \left[ 1,W_l \right] \right\} $ using shared range index and the calibration matrix of the camera \cite{jiang2022polarformer}. After that, the features of projected points are endowed with corresponding 3D points. By aggregating features along z-axis, the final polar BEV feature map $\boldsymbol{G}_{\boldsymbol{l}}^{\boldsymbol{bev}}\in \mathbb{R} ^{R_l\times A_l\times C}$ can be obtained as follows:
\begin{equation}
    \begin{aligned}
        \boldsymbol{G_{l}^{bev}}\left( \rho _i,\phi _j \right) =\sum_{z=1}^{Z_l}{\lambda \left( \rho _i,\phi _j,z_k \right)}B\left( \boldsymbol{\hat{P}_l}, \left( r_x,w_y \right) \right) , \left( r_x,w_y \right) =\mathrm{Proj}\left( \rho _i,\phi _j,z_k|\boldsymbol{E},\boldsymbol{I},\boldsymbol{D} \right) 
    \end{aligned}
\end{equation}
where $\lambda$ is normalized weight, $B$ is the bilinear sampler, and $\mathrm{Proj}$ is the projection function utilizing extrinsic matrix $\boldsymbol{E}$, intrinsic matrix $\boldsymbol{I}$ and distortion coefficient $\boldsymbol{D}$. 

Let $\boldsymbol{D}_{\boldsymbol{l}}\in \mathbb{R} ^{R_l\times \varTheta _l\times C}$ denotes the radar BEV feature map of $l$-th level, where $\varTheta _l$ is the width. Since radar is already under the same polar coordinate system, the radar bin with the range index $\rho$ can directly interact with BEV bins with the same index. In particular, it can be formulated as follows:
\begin{equation}
    \begin{aligned}
        &\hat{\boldsymbol{g}}_{\boldsymbol{l},\boldsymbol{\rho }}=\mathrm{MultiHead}\left( \bar{\boldsymbol{g}}_{\boldsymbol{l},\boldsymbol{\rho }},\bar{\boldsymbol{d}}_{\boldsymbol{l},\boldsymbol{\rho }},\bar{\boldsymbol{d}}_{\boldsymbol{l},\boldsymbol{\rho }} \right) =\mathrm{Concat}\left( \boldsymbol{z'}^{\left( \boldsymbol{1} \right)},...,\boldsymbol{z'}^{\left( \boldsymbol{n} \right)} \right) \boldsymbol{W'}_{\boldsymbol{l}}^{\boldsymbol{O}}
        \\
        &\boldsymbol{z'}^{\left( \boldsymbol{n} \right)}=\mathrm{Attention}\left( \bar{\boldsymbol{g}}_{\boldsymbol{l},\boldsymbol{\rho }}\boldsymbol{W'}_{\boldsymbol{l},\boldsymbol{n}}^{\boldsymbol{Q}},\bar{\boldsymbol{d}}_{\boldsymbol{l},\boldsymbol{\rho }}\boldsymbol{W'}_{\boldsymbol{l},\boldsymbol{n}}^{\boldsymbol{K}},\bar{\boldsymbol{d}}_{\boldsymbol{l},\boldsymbol{\rho }}\boldsymbol{W'}_{\boldsymbol{l},\boldsymbol{n}}^{\boldsymbol{V}} \right)  
    \end{aligned}
\end{equation}
in which $\bar{\boldsymbol{g}}_{\boldsymbol{l},\boldsymbol{\rho }}\in \mathbb{R} ^{A_l\times C}
$ and $\bar{\boldsymbol{d}}_{\boldsymbol{l},\boldsymbol{\rho }}\in \mathbb{R} ^{\varTheta _l\times C}$ are respectively BEV feature and radar feature of tokens contained in $\rho$-th row. And $\boldsymbol{W'}_{\boldsymbol{l},\boldsymbol{n}}^{\boldsymbol{Q}}\in \mathbb{R} ^{C\times C_{\mathrm{q}}}$, $\boldsymbol{W'}_{\boldsymbol{l},\boldsymbol{n}}^{\boldsymbol{K}}\in \mathbb{R} ^{C\times C_{\mathrm{k}}}$, $\boldsymbol{W'}_{\boldsymbol{l},\boldsymbol{n}}^{\boldsymbol{V}}\in \mathbb{R} ^{C\times C_{\mathrm{v}}}$, $\boldsymbol{W'}_{\boldsymbol{l}}^{\boldsymbol{O}}\in \mathbb{R} ^{C\times C}$ are projection matrices. $C_{\mathrm{q}}=C_{\mathrm{k}}=C_{\mathrm{v}}=\mathrm{C}/n$, $n$ is the number of heads. After that, $\{\hat{\boldsymbol{g}}_{\boldsymbol{l},\boldsymbol{\rho }}|\rho=1,...,R_l\}$ are stacked along range dimension and form the final out put BEV feature map $\hat{\boldsymbol{G}}_{\boldsymbol{l}}\in \mathbb{R} ^{R_l\times A_l\times C}$.

By applying this cross-attention strategy, the fusion module is no longer bothered by the mismatched FoV between radar and camera. The row and column attention elaborately get around the problem of the image lacking a range dimension and the RT data of radar lacking an explicit azimuth dimension.
%, and alignment uncertainty caused by low-resolution sensing ability of specific direction. 
Besides, this structure can be easily extended to LiDAR or other data formats of radar, which facilitates exploring the effects of different modalities.

\subsection{Head and Loss}
We follow polar BEV decoders \cite{jiang2022polarformer} to decode multi-level BEV features and make predictions from object queries. Since velocity prediction is not included in our tasks, we delete the corresponding branches. The regression targets include ($d_\rho$, $d_\phi$, $d_z$, $\mathrm{log}\space l$, $\mathrm{log}\space w$, $\mathrm{log}\space h$, $\mathrm{sin}(\theta_{ori}-\phi)$, $\mathrm{cos}(\theta_{ori}-\phi)$), where $d_\rho$, $d_\phi$, $d_z$ are relative offset to the reference point ($\rho$, $\phi$, $z$), $l$, $w$, $h$, $\theta_{ori}$ are length, width,  height, and orientation of bounding box. The classification and regression tasks are respectively supervised by Focal loss \cite{lin2017focal} and F1 loss.