\section{Related Works}
\subsection{Deep Learning on Radar}
\subsubsection{Radar Data Representation and Datasets}
Traditional radar usually only outputs sparse points\cite{schumann2021radarscenes, deziel2021pixset,caesar2020nuscenes} for use. However, such points are subjective to heavy signal processing methods to reduce noisy observation and alleviate bandwidth limitation. Nevertheless, such hand-crafted pipeline makes subsequent fusion with other sensors intractable. Consequently, some attempts have been made to provide upstream data, such as range-azimuth-Doppler (RAD) tensor \cite{ouaknine2021carrada,zhang2021raddet}, range-azimuth (RA) maps \cite{sheeny2021radiate}, range-Doppler (RD) spectrums \cite{mostajabi2020high} or even raw Analog Digital Converter (ADC) 
 data\cite{lim2021radical}\cite{mostajabi2020high}. Note that the RAD, RA and RD map are generated from ADC data using several times of (almost) lossless FFT. Thus we call these all these four types of data ``radar raw data" in the sequel.

%Radar point cloud  and the  are the most prevalent formats for their fully decoded off-the-shelf attributes. The low transmission bandwidth requirement and light representation of radar point cloud make it extremely suitable for multi-modality datasets of numerous amounts such as nuScenes \cite{}. Other datasets provide as radar data formats. 


\begin{table}
  \caption{4D Radar Datasets Comparison. Data: PC, RA, ADC denote to point cloud, range azimuth map and raw ADC data; Sensors: C, $C_s$, L, O denote to camera, stereo camera, LiDAR and odometer; Scenarios: U, S, H denote to urban (city), suburban, highway; Annotations: 3D, BEV, T, $P_o$, M denote to 3D bounding box, BEV bounding box, track ID, object-level point and segmentation mask; Classes and Size denote the number of classes and number of annotated frames of each dataset.}
  \label{dataset}
  \centering
  \begin{tabular}{lllllll}
    \toprule
    Dataset & Data & Other Sensors & Scenarios & Annotations & Classes & Size \\
    \midrule
    Astyx \cite{meyer2019automotive} & PC & CL & SH & 3D & 7 & 500 \\
    View-of-Delft \cite{palffy2022multi} & PC & $C_s$LO & U & 3D,T & 13 & 8693 \\
    RADIal \cite{rebut2022raw} & ADC,RA,PC & CLO & USH & $P_o$,M & 1 & 8252 \\
    TJ4DRadSet \cite{zheng2022tj4dradset} & PC & CLO & U & 3D,T & 5 & 7757 \\
    Radatron \cite{madani2022radatron} & ADC,RA & $C_s$ & U & BEV & 1 & 16K \\
    \bottomrule
  \end{tabular}
\end{table}

%By using large arrays of virtual antennas, 4D radars successfully alleviate sparsity and low angular resolution problems of traditional ones. Several datasets have recently been released for detection tasks on 4D radar data, as shown in Table \ref{dataset}. As can be observed, most datasets provide point cloud as the only radar data format \cite{meyer2019automotive}\cite{palffy2022multi}\cite{zheng2022tj4dradset}, which leads to information loss and super sparsity at the input side. 
Since our motivation is to deeply fuse with other sensors, we only consider the datasets providing data beyond point cloud. Among them, both RADIal \cite{rebut2022raw} and Radatron \cite{madani2022radatron} provide such data. But the latter one only covers a single scenario while lacking camera calibration, LiDAR modality, and vertical\footnote{height?} annotation. Taking into account the factors mentioned above, we carried out all our experiments on the RADIal dataset. However, the annotations of RADIal only contain object-level points. For a more comprehensive and objective evaluation, we further annotate the bounding box size and heading of each object based on dense LiDAR points. The annotations will be released to the community soon. 


\subsubsection{Object Detection on Radar Data}
Traditional radar object detection method that utilizes point cloud faces challenges of super sparsity due to the low angular resolution. Practical treatments include occupancy grid mapping \cite{zhou2022towards} and separate processing for static and dynamic objects \cite{schumann2019scene}. 4D radar technology improves the azimuth resolution and provide additional elevation resolution, which enables the combination of static and dynamic object detection in a single network, using either PointPillars \cite{palffy2022multi} or tailor-made self-attention modules \cite{xu2021rpfa}\cite{bai2021radar}.

The pre-CFAR(Constant False Alarm Rate Detector) data provide rich information of both targets and backgrounds. RAD tensor is fully decoded but brings high demand of storage and computation. Hence, Zhang et al. \cite{zhang2021raddet} takes the Doppler dimension as channels and Major et al. \cite{major2019vehicle} projects RAD tensor to multiple 2D views. RTCNet \cite{palffy2020cnn} divides RAD tensor into small cubes and applies 3D CNN to reduce computation burden. Besides, \cite{zhang2020object}\cite{rebut2022raw} take complex RD spectrum as input and apply neural networks to automatically extract spatial information. Networks such as RODNet \cite{wang2021rodnet} adopt complex RA maps for detection, which avoid false alarms caused by extended Doppler proﬁle.


The fusion of different sensors provides complementary cues, leading to more robust performance. At input level, radar point clouds are usually projected as pseudo-images and concatenated with camera images \cite{nobis2019deep}\cite{chadwick2019distant}. At Region of Interest (RoI) level, the RoIs are either consecutively refined by radar and other modalities \cite{nabati2020radar}\cite{nabati2021centerfusion} or unified together from different sensors \cite{kim2020grif}\cite{kim2023crn}.\footnote{not clear. need rewrite.} At feature level, 
\cite{wu2023mvfusion}\cite{zhang2021rvdet} integrates feature maps generated from different modalities, while \cite{kim2020low}\cite{kim2022craft} use RoIs to crop and merge features across modalities. To the best of our knowledge, we are the first to deeply fuse radar using raw data with other modalities in a unified BEV perspective. %we are not only the first method that utilizes BEV queries to extract features from different modalities, but also the first to take radar data only processed by range FFT as inputs. 


\subsection{BEV 3D Object Detector}\footnote{need cite more papers like bevdepth, bevstereo, bevfusion, petrv2, etc...}
% 按照LSS正投影，query反投影
% 多模态融合，其他信息（时序，depth监督）
% 这几个角度和逻辑重新写一下覅吧
BEV object detection bursts out a rising wave of research due to its vast success in 3D perception tasks. OFT \cite{roddick2018orthographic} and VPN \cite{pan2020cross} are the first literature that project single-view and multi-view images to BEV space for 3D scene understanding, respectively. Modern BEV-based algorithms benefit from the increase of available multi-sensor data. CVT \cite{zhou2022cross} merges perspective view and BEV features by applying camera-aware positional embedding and attention mechanism. PETR \cite{liu2022petr} projects image features to 3D embedded space, which is further processed by DETR-style decoders. BEVFormer \cite{li2022bevformer} and its variant \cite{yang2022bevformer} use BEV representation to fuse both spatial and temporal information from multiple perspectives. Furthermore, simpleBEV \cite{harley2022simple} introduces a deformable multi-scale BEV fusion mechanism while BEVDet \cite{huang2021bevdet} explores image-level and BEV-level augmentation strategies. As fully transformer-based structures free BEV detectors from uniform and regular sampling grids, PolarFormer \cite{jiang2022polarformer} realizes multi-view image fusion under polar coordinates. Considering the natural polar structure of both radar and image data and  the versatile fusion ability of attention, we choose PolarFormer as our stem structure. What makes our method different is our concentration on single-view multi-modalities fusion and specifically designed attention mechanisms for sensors with different properties.