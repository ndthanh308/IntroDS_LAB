\begin{abstract}
 %   Accurate and reliable perception is crucial for autonomous driving systems, and sensor fusion is a promising approach to improve overall performance. The combination of camera and radar data is highly complementary as cameras provide high angular resolution and texture information, while radar provides accurate depth sensing ability and robustness to adverse weather conditions. However, traditional radar point cloud generation algorithms usually cause information loss and reduce the effectiveness of fusion. In this paper, we propose a novel approach that fuses camera and raw radar data using transformer-based structures. Our approach applies learnable Bird's Eye View (BEV) queries to sample radar features that correspond to the same range and camera features that share the same azimuth with queries. To evaluate our method, we use RADIal, the only 4D radar dataset that provides both raw radar data and camera data with calibrations. %Since the original dataset only provide one point for each object, we re-annotate the dataset with 3D bounding boxes to comprehensively evaluate the performance. 
   % The superiority of raw radar data in modality fusion is verified through extensive experiments. On a series of metrics, our proposed method achieves state-of-the-art results, even comparable with the performance obtained by image fusion with LiDAR. The code and refined annotations will be released soon.
Radar is ubiquitous in autonomous driving systems due to its low cost and good adaptability to bad weather. Nevertheless, the radar detection performance is usually inferior because its point cloud is sparse and not accurate due to the poor azimuth and elevation resolution. Moreover, point cloud generation algorithms already drop weak signals to reduce the false targets which may be suboptimal for the use of deep fusion. In this paper, we propose a novel method named EchoFusion to skip the existing radar signal processing pipeline and then incorporate the radar raw data with other sensors. Specifically, we first generate the Bird's Eye View (BEV) queries and then take corresponding spectrum features from radar to fuse with other sensors. By this approach, our method could utilize both rich and lossless distance and speed clues from radar echoes and rich semantic clues from images, making our method surpass all existing methods on the RADIal dataset, and approach the performance of LiDAR. Codes will be available upon acceptance.
\end{abstract}