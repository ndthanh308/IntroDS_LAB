\section{Introduction}
Robust and accurate perception is a long-standing challenge in Autonomous Driving Systems (ADS). The full perception system usually relies on multiple sensor fusion including camera, LiDAR, and radar. Camera provides rich semantic cues of objects and excellent resolution, while LiDAR is capable of capturing highly accurate spatial information. However, neither camera nor LiDAR can directly measure speed or survive in adverse weather conditions, such as fog, sandstorms, and snowstorms \cite{madani2022radatron}. Although radar can overcome the aforementioned problems, even the point cloud from the latest 4D millimeter wave (mmWave) radars suffers from severe sparsity and low angular resolution which makes it hardly discriminate objects and backgrounds solely.
%Meanwhile, 4D millimeter wave (mmWave) radars offer accurate depth and speed measurement, and it is robust to bad weather. But due to low angular resolution, objects in 4D radar can hardly be discriminated from the background by shape or response. Moreover, using 4D radar as a sole sensor may cause severe false alarms on metallic backgrounds or false negatives on static vehicles. 
Fortunately, radar and camera are highly complementary to each other. Fusing these two types of sensors becomes a promising solution.

%However, images lack depth information while LiDAR is expensive. 
%Meanwhile, neither camera nor LiDAR can directly measure speed or resist adverse weather conditions, such as fog, sandstorms, and snowstorms \cite{madani2022radatron}. 
%These drawbacks lead to potential dangers and labor-consuming design in perception systems. Fortunately, 4D millimeter wave (mmWave) radars offer complementary performance, including accurate depth, directly measured speed, and robustness to weather. 
%But due to extremely low angular resolution and severe false alarms, the research and application on radars are quite limited in the past. The emergence of 4D radar mitigates this problem by applying dense virtual antenna arrays, which provide higher resolution in both vertical and horizontal directions \cite{madani2022radatron}. 
%In academical area, more and more 4D radar datasets come forth, such as View-of-Delft \cite{palffy2022multi} and RADIal \cite{rebut2022raw}.

Radar data are usually represented as point clouds. Technically, the radar point cloud stems from raw Analog-Digital-Converter (ADC) data received by antennas. Range, speed, and angle of arrival (AoA) information of perceivable objects can be explicitly extracted in the frequency domain by consecutively applying FFT along corresponding dimensions. %Firstly, by applying side-lobe suppression and FFT along the range and Doppler axis, objects of specific range and speed would show distinguishable peaks at corresponding bins in these first two dimensions \cite{sun2020mimo}. 
%Then all these salient peaks will be filtered out as points by a constant false alarm rate (CFAR) detector \cite{richards2014fundamentals}. Finally, along the channel dimension of each point, another side-lobe suppression and FFT will be employed to unwrap the AoA. With the calibration matrix of the sensor, AoA can be further translated to azimuth and elevation\cite{rebut2022raw}. 
Among these steps, side lobe suppression and constant false alarm rate (CFAR) detector\cite{richards2014fundamentals} are usually adopted to reduce the noise and false alarms.
As a result, all the derived points are equipped with spatial coordinates, speed, and reflection intensity. Though these noise suppression operations can significantly reduce the data size and further computation costs, the resulting radar point cloud is extremely sparse, as shown in Figure \ref{pcd}. Even worse, quite a lot of useful information is lost during CFAR, which is adverse to the accurate perception of environments and subsequent fusion with other sensors.\cite{lim2019radar}. %To further explore the potential of radar data, how to utilize pre-CFAR data has raised community attention but still remains a challenging problem.
% However, how to efficiently utilize the raw data from radar, especially fusion with other sensors remains an open problem.
Raw data serves as a potential solution to alleviate these problems. However, how to efficiently utilize it, especially fusing with other sensors remains an open problem.

% Figure environment removed

Recently, Birds-Eye-View (BEV) based methods are prevalent for driving scenario perception. It integrates features from multiple views and multiple modalities into a unified 3D representation space\cite{chen2022futr3d, li2022bevformer,liu2022bevfusion}. Among the varieties of BEV-based methods, PolarFormer\cite{jiang2022polarformer} divides the 3D space in polar coordinates, which matches the format of raw radar data better. As such, we chose PolarFormer as our baseline model. %and incorporate radar modality using a tailored attention mechanism. 

%Specifically, given a polar query with a certain range and azimuth, we can find its corresponding range on radar raw data and corresponding azimuth on image. Then we apply 

%One design choice is to project different modalities to discrete BEV maps. Subsequently, the BEV maps are aligned and deeply fused \cite{liu2022bevfusion}. Another choice is to use a learnable BEV query to iteratively sample features from different modalities \cite{li2022bevformer}\cite{jiang2022polarformer}\cite{li2023fully}. 
%BEV-based methods not only provide a unified representation for different modalities, but are also freed from scale variation caused by the perspective view \cite{fan2021rangedet}. Besides, the radar raw data is de facto a dense 3d feature map under the polar coordinates, which makes it easy to be merged with other modalities through BEV representation.

In this work, we present EchoFusion, a method designate for fusing raw radar data with images. We first extract multi-level features from both radar and image, and then fuse them using a novel polar-aligned attention (PAA) technique. This technique is based on an observation that the polar coordinate is aligned with radar data on the range dimension and aligned with image data on the azimuth dimension. 
%of radar features with semantically rich and high-resolution features from the camera and LiDAR. 
With row-wise cross-attention on radar data and column-wise attention on image, the PAA could precisely aggregate essential features from both modalities while maintaining simple and efficient implementation. Finally, a polar BEV decoder refines object queries for accurate bounding box predictions.

The contribution of our paper is three-folds: 
\begin{enumerate}
    \item We are the first to fuse raw radar data with images in BEV space. Specifically, we propose a novel Polar-Aligned Attention module to guide the network learn effective radar and image features.
    %\item We offered a novel and elastic paradigm that not only sufficiently combines different advantages of radar and camera, but also overcomes problems of perception range misalignment. 
    %\item We carried out comprehensive experiments with our re-labeled 3d annotations on the RADIal dataset. Our experimental study vividly shows how lossless raw radar data can benefit accurate detection.
    \item We relabel the RADIal dataset\cite{rebut2022raw}, which is the only dataset available for raw radar and camera sensor fusion, with accurate 3D bounding box annotations. The new annotations will be public upon the paper acceptance.
    \item Extensive experiments show that our method has outperformed all the existing methods and achieved promising BEV detection performance, even approaching the LiDAR-based method.
\end{enumerate}