\section{Experiments}
In this section, we conduct thorough experiments to validate the effectiveness of our approach. We first introduce the dataset and metrics used in our experiments, then followed by comparisons with other state-of-the-art methods. Lastly, we ablate some designs and potential input format of raw radar data in our method, and discuss some limitations.
\label{exp}
\subsection{Experimental Settings}
\textbf{The RADIal Dataset} \cite{rebut2022raw} provides 8252 annotated frames with synchronized multi-sensor data. Training, validation, and test sets contain 6231, 986, and 1035 frames, respectively. Each object is annotated with a 3D center coordinate of the visible face of the vehicle. The size and orientation of the objects are pre-defined templates, i.e. they are the same for all objects. In the evaluation, precisions and recalls under ten thresholds are calculated, The average precision (AP), average recall (AR), and F1 score are obtained through the ten precisions and ten recalls as the evaluation metric. Range error (RE) and azimuth error (AE) are also reported to evaluate localization accuracy. The annotation method and metrics are quite different with common autonomous driving datasets \cite{caesar2020nuscenes,sun2020scalability}. Nonetheless, to compare with state-of-the-art methods, we still list our results using the metrics defined in RADIal \cite{rebut2022raw} as a reference.

\textbf{Re-annotation and more metrics.} Based on the provided LiDAR point cloud, we have refined the annotation to 3D bounding box form, which includes center position, scale, and heading in 3D space. Capitalizing on these new annotations, we are able to apply metrics that are vastly used in 3D object detection tasks. Specifically, we take AP defined in Waymo dataset \cite{sun2020scalability} as our main metrics. Since the LiDAR used in RADIal is only 16-beam, we cannot label accurate height for the objects. So we mainly utilize BEV IoU for evaluation. We use the threshold of 0.7 for normal IoU and longitudinal error-tolerant (LET) IoU computation\cite{hung2022let}. 
%Under 3D evaluation, we take 0.5 as the IoU threshold due to relatively low vertical annotation accuracy. 
Considering the spatial distribution of ground-truth, the distance evaluation is further broken down into
two categories: 0 to 50 meters, 50 to 100 meters. For a more comprehensive evaluation, we also introduce the error metrics defined in nuScenes dataset\cite{caesar2020nuscenes}, including  ATE, ASE, and AOE for measuring errors of translation, scale, and orientation. 

\textbf{Radar Formats.} As introduced in Section \ref{preliminary}, there are multiple formats of radar raw data, such as ADC, RT, RD, READ. These formats can be converted sequentially by FFT, which is a linear operation that can be absorbed into the linear layers. So from the perspective of neural network, these formats are equivalent. However, our proposed Polar-Aligned Attention requires a range dimension for indexing key and value in cross-attention, so we choose RT as a representative in the experiments. We also conduct experiments on RA to investigate the necessity of explicit azimuth.

\textbf{Implementation Details.} All our experiments are carried out on eight 3090 GPUs with the same batch size of 8. For the image backbone, we adopt ResNet-50 with pre-trained weights provided by BEVFormer \cite{li2022bevformer}. For LiDAR and radar point cloud branch, we borrow the voxel encoder from PointPillar \cite{lang2019pointpillars} and use ResNet-50 with modified strides as the backbone. Other representations of radar share similar modified ResNet-50 as the backbone, but with an extra 1 $\times$ 1 convolution at the beginning for channel number alignment. It is worth noticing that the RT map needs to be pre-processed to decode the echo of each virtual antenna\cite{rebut2022raw}. To prevent overfitting, the image-only variant is trained for only 12 epochs while others are trained for 24 epochs. AdamW \cite{loshchilov2017decoupled} is adopted as the optimizer and a learning rate of 5e-5 is shared for all models. 
Due to the limited size of the dataset, we find that the results are unstable across different runs. To make a fair comparison, each experiment is repeated three times. In modality ablation, we show the mean and variance of each experiment result.

\subsection{Comparison to State-of-the-art Methods}
\begin{table}
  \caption{Point-based detection performances on RADIal Dataset test split. C, RD, and RT respectively refer to camera, radar-Doppler spectrum, and radar-time data. Our method achieves the best performance in both average precision (AP), average recall (AR), and F1-score (F1). The best result of each metric is in \textbf{bold}.}
  \label{sota}
  \centering
  \resizebox{0.85\textwidth}{!}{%
  \begin{tabular}{lllllll}
    \toprule
    Methods & Modality & AP($\%$)$\uparrow$ & AR($\%$)$\uparrow$ & F1($\%$)$\uparrow$ & RE(m)$\downarrow$ & AE($^{\circ}$)$\downarrow$ \\
    \midrule
    FFTRadNet \cite{madani2022radatron} & RD & 96.84 & 82.18 & 88.91 & \textbf{0.11} & 0.17 \\
    T-FFTRadNet \cite{giroux2023t} & ADC & 89.60 & 89.50 & 89.50 & 0.15 & 0.12 \\
    ADCNet \cite{yang2023adcnet} & ADC & 95.00 & 89.00 & 91.90 & 0.13 & \textbf{0.11} \\
    CMS \cite{jin2023cross} & RD\&C & 96.90 & 83.50 & 89.70 & 0.45 & n/a \\
    EchoFusion & RT\&C & \textbf{96.95} & \textbf{93.43} & \textbf{95.16} &0.12 & 0.18 \\
    \bottomrule
  \end{tabular}
  }
\end{table}

We first compare our method with state-of-the-art detectors on AP, AR, range error, and azimuth error defined in RADIal \cite{rebut2022raw}, with original object-level point annotations\footnote{The official implementation of these metrics are different from common implementations. Specifically, AP and AR are averaged on score thresholds from 0.1 to 0.9 with step of 0.1 and IoU threshold of 0.5. And F1 score is directly calculated from AP and AR defined above.}. The results are illustrated in Table \ref{sota}. Our method significantly improves AR and F1 performance and achieves a remarkable improvement over all existing detectors including CMS \cite{jin2023cross} which also integrates both radar raw data and camera data. Note that RE and AE are not quite informative because these two metrics are calculated on all the recalled objects of a method, which is unfair to high-recall models like ours.

\begin{table}
  \caption{BEV detection performances on RADIal Dataset test split with refined 3D ground-truth bounding box. C, RD, and RT respectively refer to camera, range-Doppler spectrum, and range-time data. The best result of each metric is in bold. Our method exceeds FFTRadNet by a large margin.}
  \label{sota_newlabel}
  \centering
  \resizebox{0.99\textwidth}{!}{%
  \begin{tabular}{llllllll}
    \toprule
    & & \multicolumn{3}{c}{BEV AP@0.7($\%$)$\uparrow$}&\multicolumn{3}{c}{LET-BEV-AP@0.7($\%$)$\uparrow$}\\
    \cmidrule(r){3-5} \cmidrule(r){6-8}
     Methods & Modality & Overall & 0 - 50m & 50 - 100m & Overall & 0 - 50m & 50 - 100m \\
    \midrule
    FFTRadNet3D\cite{rebut2022raw} & RD & 57.26 & 68.66 & 52.28 & 62.81 & 75.55 & 57.98 \\
    EchoFusion & RT\&C & \textbf{84.25} & \textbf{86.67} & \textbf{90.48} & \textbf{88.54} & \textbf{92.10} & \textbf{94.45} \\
    \bottomrule
  \end{tabular}
  }
\end{table}

To evaluate in a more rigorous and informative way, we also conduct experiments on the refined annotations and vastly adopted metrics in 3D object detection. FFTRadNet\cite{rebut2022raw} is lifted up with additional branches to predict the scale and orientation of targets. We can only compare with this FFTRadNet variant since codes of other algorithms\cite{giroux2023t, jin2023cross, yang2023adcnet} are not available yet. The results in Table \ref{sota_newlabel} show that the proposed method outperforms FFTRadNet by a large margin. This significant improvement reveals the huge benefit of unleashing the power of radar data in multi-modality fusion.

\subsection{Ablation Study}
\begin{table}[!tb]
  \caption{Ablation studies on our EchoFusion. Complex means interpreting input features as real and imaginary (IQ) or magnitude and phase (MP). Pretrain means whether to use the pre-trained image backbone. The best result of each metric is in \textbf{bold}.}
  \label{arch}
  \centering
  \resizebox{0.9\textwidth}{!}{%
  \begin{tabular}{lllllllll}
    \toprule
    \multicolumn{2}{c}{Conditions} & \multicolumn{3}{c}{BEV AP@0.7($\%$)$\uparrow$}&\multicolumn{3}{c}{LET-BEV-AP@0.7($\%$)$\uparrow$}\\
    \cmidrule(r){1-2} \cmidrule(r){3-5} \cmidrule(r){6-8}
    Complex & Pretrain & Overall & 0 - 50m & 50 - 100m & Overall & 0 - 50m & 50 - 100m \\
    \midrule
    MP & w & \textbf{84.25} & 86.67 & \textbf{90.48} & \textbf{88.54} & 92.10 & \textbf{94.45} \\
    IQ & w & 82.64 & \textbf{87.09} & 87.81 & 88.06 & \textbf{92.25} & 93.24 \\
    MP & w/o & 74.34 & 81.26 & 78.71 & 78.90 & 85.44 & 84.28 \\
    \bottomrule
  \end{tabular}
  }
\end{table}

We ablate the input format to the radar feature extraction network in Table \ref{arch}. It shows that the interpretation of complex input can improve overall detection performance, especially for long-range perception. The magnitude and phase representation encodes a non-linear transformation of the complex representation, which may relate to the final prediction target better. We also tried to remove the pre-training of the image branch. The results drop significantly, which confirms that in this relatively small dataset, a good pre-training is crucial for good performance.

\subsection{Comparison of Different Modalities}
\begin{table}
  \caption{BEV detection performances of different modality combinations of our algorithm on RADIal Dataset test split. Refined 3D bounding box annotations are applied. C, L, RD, RT, RA, RP respectively refer to the camera, LiDAR, range-Doppler spectrum, range-time data, range-azimuth map, and radar point cloud. The best result without LiDAR of each metric are in \textbf{bold}, and the results with LiDAR are on gray background.}
  \label{modalities}
  \centering
  \resizebox{0.99\textwidth}{!}{%
  \begin{tabular}{lllllllllll}
    \toprule
    \multicolumn{5}{c}{Modality} & \multicolumn{3}{c}{BEV AP@0.7($\%$)$\uparrow$}&\multicolumn{3}{c}{LET-BEV-AP@0.7($\%$)$\uparrow$}\\
    \cmidrule(r){1-5} \cmidrule(r){6-8} \cmidrule(r){9-11}
    C & RT & RA & RP & L & Overall & 0 - 50m & 50 - 100m & Overall & 0 - 50m & 50 - 100m \\
    \midrule
    \checkmark & & & & & 10.85$\pm$0.01 & 23.84$\pm$0.68 & 1.83$\pm$0.29 & 54.31$\pm$1.58 & \textbf{78.67$\pm$1.64} & 32.10$\pm$0.37 \\
    & \checkmark & & & & 48.37$\pm$1.11 & 64.02$\pm$3.93 & 44.36$\pm$0.12 & 54.96$\pm$1.53 & 69.78$\pm$4.58 & 51.01$\pm$0.07 \\
    & & \checkmark & & & 51.19$\pm$0.88 & \textbf{65.83$\pm$1.41} & 43.31$\pm$0.28 & 58.49$\pm$0.15 & 73.48$\pm$0.27 & 51.16$\pm$0.52 \\
    & & & \checkmark & & \textbf{53.33$\pm$0.76} & 65.55$\pm$1.78 & \textbf{47.07$\pm$1.16} & \textbf{62.94$\pm$1.25} & 76.71$\pm$2.48 & \textbf{56.60$\pm$1.62} \\ \rowcolor{gray!20}
    & & & & \checkmark & 84.56$\pm$0.09 & 87.34$\pm$0.09 & 91.85$\pm$0.12 & 86.23$\pm$0.47 & 88.52$\pm$0.97 & 93.11$\pm$0.06 \\
    \midrule
    \checkmark & \checkmark & & & & 84.25$\pm$0.33 & 86.67$\pm$1.17 & 90.48$\pm$1.04 & 88.54$\pm$0.52 & 92.10$\pm$1.15 & 94.45$\pm$0.17 \\
    \checkmark & & \checkmark & & & \textbf{84.85$\pm$0.78}& \textbf{87.88$\pm$0.73} & \textbf{91.64$\pm$0.20} & \textbf{89.70$\pm$0.60} & \textbf{93.86$\pm$0.69} & \textbf{95.28$\pm$0.05} \\
    \checkmark & & & \checkmark & & 82.00$\pm$0.97 & 86.58$\pm$1.04 & 86.17$\pm$0.83 & 87.97$\pm$0.69 & 92.75$\pm$0.81 & 92.39$\pm$0.53 \\ \rowcolor{gray!20}
    \checkmark & & & & \checkmark & 87.05$\pm$0.71 & 88.88$\pm$1.71 & 95.43$\pm$0.23 & 88.97$\pm$0.27 & 91.55$\pm$1.64 & 96.15$\pm$0.59 \\ 
    %\rowcolor{gray!20}
    %\checkmark & & & \checkmark & \checkmark & 86.53$\pm$0.05 & 87.98$\pm$1.11 & 95.08$\pm$0.47 & 88.35$\pm$0.24 & 91.30$\pm$0.42 & 96.19$\pm$0.58 \\
    \bottomrule
  \end{tabular}
  }
\end{table}
To further study the effects of different modalities, we change the input format indicated in Figure \ref{data_vis} and corresponding backbones, and test with and without image modality. Table \ref{modalities} presents the results, and our findings are listed as follows.

\textbf{Comparing single-modality results} It is not surprising that LiDAR ranks first while the camera is the worst in terms of AP. Though poor in depth estimation, the camera has excellent angular resolution, which is beneficial for LET-BEV-AP. It achieves comparable results with radar in LET-BEV-AP. Besides, without the guidance of image, the radar-only network gets better performance as more hand-crafted post-processing is involved. However, their metrics still lag far from LiDAR.

\textbf{Comparing multi-modality with single modality} The power of radar data is released by fusing with image. No matter in what data format, all the radar raw representations gain around 30 points improvement by fusing with images. By combining image and range-time data, we obtain BEV AP that is only 0.03 less than that of LiDAR only method. And the LET-BEV-AP and long-range detection ability are even better. We argue that it is mainly because images provide enough clues to decode the essential information from raw radar representation.

\textbf{Comparing multi-modality results} When integrating with images, the LiDAR-fused method still outperforms other radar-fused methods. In terms of different representations of radar, both RT and RA outperform traditional cloud points, especially in the 50-100m range, in which the difference is as large as 4 points in AP. We owe this finding to the information loss and false alarm of radar point cloud as shown in Figure \ref{pcd}. The performance gap between RT and RA with camera modality is within the error bar, indicating that it is not necessary to explicitly solve the azimuth, nor necessary to permute the Doppler-angle dimensions as FFTRadNet\cite{rebut2022raw} does.
%On the other hand, the speed, azimuth, elevation, and range information in RT and RA data is never impaired by the hand-crafted radar data processing pipeline, which offers the best overall performance.


\subsection{Discussion and Limitations}
\label{subsec:discussion}
\begin{table}
  \caption{3D detection performances on RADIal Dataset test split with refined 3D ground-truth bounding box. C, L, and RT respectively refer to camera, LiDAR, and radar-time data. The best result of each metric is in bold.}
  \label{3DmAP}
  \centering
  \resizebox{0.75\textwidth}{!}{%
  \begin{tabular}{lllllll}
    \toprule
    & \multicolumn{3}{c}{3D AP@0.5($\%$)$\uparrow$}&\multicolumn{3}{c}{Average Error@0.5$\downarrow$}\\
    \cmidrule(r){2-4} \cmidrule(r){5-7}
     Modality & Overall & 0 - 50m & 50 - 100m & ATE & ASE & AOE\\
    \midrule
    L & 62.69 & 80.04 & 54.26 & 0.243 & \textbf{0.171} & 0.027 \\
    L\&C & \textbf{68.36} & \textbf{84.74} & \textbf{61.54} & \textbf{0.239} & 0.180 & \textbf{0.020} \\
    RT\&C & 39.81 & 56.05 & 31.24 & 0.301 & 0.173 & \textbf{0.020} \\
    \bottomrule
  \end{tabular}
  }
\end{table}
Firstly, it is worth noticing that although the overall performance of radar is improved by fusing with the camera, the performance in the short-range is lower than that in the long-range. We speculate the main reason is inaccurate annotation. The 16-beam LiDAR provided in RADIal is too sparse at a farther distance for annotators to give an accurate size for the long-range objects. In such cases, they are required to assign a template with a fixed size to these objects. These annotations make the problem much easier for farther distances, which leads to higher performance than near distances.
%However, this is not detrimental for neural networks, as predicting a fixed value is relatively straightforward for the networks.  
%the objects of large yaw are not common in the training set, and the predictions are not perfect. But in the test split, most of the objects with large rotations are concentrated in short range, thus harming the performance. 

Secondly, though the radial speed label is included in the original annotations of RADIal, radial speed is not quite useful for downstream modules like planning since they need accurate longitudinal and lateral velocity to predict whether the vehicles will interfere with the ego vehicle. However, the synchronized frames are not consecutive in this dataset, it is hard to label velocities for the objects. Considering these factors, velocity prediction is not included in our task. 

Finally, as shown in Table \ref{3DmAP}, our method still lags far from the LiDAR-only structure on 3D metrics. The reasons are two-fold. The 16-beam LiDAR 
still has considerable errors in the z-axis. From 30 meters on, the error has exceeded 0.3 meters, which makes accurate annotation infeasible. The elevation resolution of the radar is inferior to that of LiDAR, making it less favorable. As a result, the detector struggles to regress inaccurate elevation with more inaccurate measurements. This phenomenon can be alleviated by better radar antenna design.