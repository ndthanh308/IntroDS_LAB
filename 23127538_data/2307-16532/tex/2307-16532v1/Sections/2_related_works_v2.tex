\section{Related Works}
\subsection{Deep Learning on Radar}
\subsubsection{Radar Data Representation and Datasets}
Traditional radar usually only outputs sparse points\cite{caesar2020nuscenes, deziel2021pixset, schumann2021radarscenes} for use. However, such points are subjective to heavy signal processing methods to reduce noisy observation and alleviate bandwidth limitation. Nevertheless, such hand-crafted pipelines make the subsequent fusion with other sensors intractable. Consequently, some attempts have been made to provide upstream data, such as range-azimuth-Doppler (RAD) tensor \cite{ouaknine2021carrada,zhang2021raddet}, range-azimuth (RA) maps \cite{sheeny2021radiate}, range-Doppler (RD) spectrums \cite{mostajabi2020high} or even raw Analog Digital Converter (ADC) 
 data\cite{lim2021radical, mostajabi2020high}. Note that the RAD, RA, and RD tensors are generated from ADC data using several times (almost) lossless FFT. Thus we call all these four types of data "radar raw data" in the sequel.

%Radar point cloud  and the  are the most prevalent formats for their fully decoded off-the-shelf attributes. The low transmission bandwidth requirement and light representation of radar point cloud make it extremely suitable for multi-modality datasets of numerous amounts such as nuScenes \cite{}. Other datasets provide as radar data formats. 


\begin{table}
  \caption{4D Radar Datasets Comparison. Data: PC, RA, ADC denote to point cloud, range azimuth map and raw ADC data; Sensors: C, $C_s$, L, O denote to camera, stereo camera, LiDAR, and odometer; Scenarios: U, S, H denote to urban (city), suburban, highway; Annotations: 3D, BEV, T, $P_o$, M denote to 3D bounding box, BEV bounding box, track ID, object-level point and segmentation mask; Classes and Size denote the number of classes and the number of annotated frames of each dataset.}
  \label{dataset}
  \centering
  \resizebox{0.99\textwidth}{!}{%
  \begin{tabular}{lllllll}
    \toprule
    Dataset & Data & Other Sensors & Scenarios & Annotations & Classes & Size \\
    \midrule
    Astyx \cite{meyer2019automotive} & PC & CL & SH & 3D & 7 & 500 \\
    View-of-Delft \cite{palffy2022multi} & PC & $C_s$LO & U & 3D,T & 13 & 8693 \\
    RADIal \cite{rebut2022raw} & ADC,RA,PC & CLO & USH & $P_o$,M & 1 & 8252 \\
    TJ4DRadSet \cite{zheng2022tj4dradset} & PC & CLO & U & 3D,T & 5 & 7757 \\
    Radatron \cite{madani2022radatron} & ADC,RA & $C_s$ & U & BEV & 1 & 16K \\
    \bottomrule
  \end{tabular}
  }
\end{table}

%By using large arrays of virtual antennas, 4D radars successfully alleviate sparsity and low angular resolution problems of traditional ones. Several datasets have recently been released for detection tasks on 4D radar data, as shown in Table \ref{dataset}. As can be observed, most datasets provide point cloud as the only radar data format \cite{meyer2019automotive}\cite{palffy2022multi}\cite{zheng2022tj4dradset}, which leads to information loss and super sparsity at the input side. 
Since our motivation is to deeply fuse with other sensors, we only consider the datasets providing data beyond point cloud. Among them, both RADIal \cite{rebut2022raw} and Radatron \cite{madani2022radatron} provide such data. But the latter one only covers a single scenario while lacking camera calibration, LiDAR modality, and height annotation. Taking the factors mentioned above into account, we carry out all our experiments on the RADIal dataset. However, the annotations of RADIal only contain object-level points. For a more comprehensive and objective evaluation, we further annotate the bounding box size and heading of each object based on dense LiDAR points. The annotations will be released to the community soon. 


\subsubsection{Object Detection on Radar Data}
Traditional radar object detection methods that utilize point cloud face challenges of super sparsity due to the low angular resolution. Practical treatments include occupancy grid mapping \cite{zhou2022towards} and separate processing for static and dynamic objects \cite{schumann2019scene}. 4D radar technology improves the azimuth resolution and provides additional elevation resolution, which enables the combination of static and dynamic object detection in a single network, using either PointPillars \cite{palffy2022multi} or tailor-made self-attention modules \cite{bai2021radar, xu2021rpfa}.

The pre-CFAR(Constant False Alarm Rate Detector) data provide rich information of both targets and backgrounds. RAD tensor is fully decoded but brings high demand of storage and computation. Hence, Zhang et al. \cite{zhang2021raddet} takes the Doppler dimension as channels and Major et al. \cite{major2019vehicle} projects RAD tensor to multiple 2D views. RTCNet \cite{palffy2020cnn} divides RAD tensor into small cubes and applies 3D CNN to reduce computation burden. Besides, \cite{rebut2022raw,zhang2020object} take complex RD spectrum as input and apply neural networks to automatically extract spatial information. Networks such as RODNet \cite{wang2021rodnet} adopt RA maps for detection, which avoid false alarms caused by extended Doppler proÔ¨Åle.
Despite the aforementioned research works, the utilization of ADC radar data has recently gained increasing attention within the community \cite{giroux2023t, yang2023adcnet}. However, their results remain unsatisfactory.

The fusion of different sensors provides complementary cues, leading to more robust performance. At the input level, radar point clouds are usually projected as pseudo-images and concatenated with camera images \cite{chadwick2019distant,nobis2019deep}. At the Region of Interest (RoI) level, some approaches \cite{nabati2020radar,nabati2021centerfusion}consecutively refine the parameters of RoIs by radar and other modalities, while others \cite{kim2020grif, kim2023crn} unify RoIs generated independently by different sensors. At the feature level, 
\cite{wu2023mvfusion, zhang2021rvdet} integrate feature maps generated from different modalities, while \cite{kim2020low, kim2022craft} use RoIs to crop and merge features across modalities. To the best of our knowledge, we are the first to deeply fuse radar using raw data with other modalities in a unified BEV perspective. %we are not only the first method that utilizes BEV queries to extract features from different modalities, but also the first to take radar data only processed by range FFT as inputs. 


\subsection{BEV 3D Object Detector}
BEV object detection bursts out a rising wave of research due to its vast success in 3D perception tasks. One thread is to adopt transformations to project 2D image features to 3D BEV space for further detection, such as OFT\cite{roddick2018orthographic} and LSS\cite{philion2020lift}. Another thread applies initialized BEV queries \cite{zhou2022cross} or object queries \cite{liu2022petr, liu2022petrv2} to iteratively and automatically sample features from multi-view images. Based on these advanced techniques, BEVFusion \cite{liu2022bevfusion} explores the advantages of BEV representation in multi-sensor fusion and achieves impressive performances. Despite that, how to make full use of other information sources also attracts the interest of researchers. BEVFormer \cite{li2022bevformer} and its variant \cite{yang2022bevformer} utilize temporal information to enhance detection capability, while BEVStereo \cite{li2022bevstereo} and STS \cite{wang2022sts} explore how the estimated depth can benefit BEV-based detection. Besides, PolarFormer \cite{jiang2022polarformer} introduces the polar grid setting and proves its effectiveness in environment perception, which is closely related to our work.

%What makes our method different is our concentration on single-view multi-modalities fusion and specifically designed attention mechanisms for sensors with complementary properties.