\documentclass[11pt, letterpaper]{article}
\usepackage{epsfig,amssymb,amsmath, multirow, url, tcolorbox,booktabs, enumitem}

% For bibtex
%\renewcommand{\cite}{\cite*}
%\bibliographystyle{agsm}
%\citestyle{dcu}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{natbib}
\usepackage[utf8]{inputenc}
\bibliographystyle{apalike}
\citestyle{dcu}
\usepackage{authblk}
\usepackage{lscape}
\usepackage{adjustbox}
\newsavebox{\theorembox}
\newsavebox{\lemmabox}
\newsavebox{\claimbox}
\newsavebox{\factbox}
\newsavebox{\corollarybox}
\newsavebox{\examplebox}
\newsavebox{\remarkbox}
\newsavebox{\assbox}
\newsavebox{\propositionbox}
\newsavebox{\problembox}
\newsavebox{\defbox}
 
\savebox{\theorembox}{\noindent\bf Theorem}
\savebox{\lemmabox}{\noindent\bf Lemma}
\savebox{\factbox}{\noindent\bf Fact}
\savebox{\corollarybox}{\noindent\bf Corollary}
\savebox{\examplebox}{\noindent\bf Example}
\savebox{\assbox}{\noindent\bf Assumption}
\savebox{\propositionbox}{\noindent\bf Proposition}
\savebox{\problembox}{\noindent\bf Problem}
\savebox{\defbox}{\noindent\bf Definition}

\newtheorem{ass}{\usebox{\assbox}}
\newtheorem{thm}{\usebox{\theorembox}}
\newtheorem{prop}{\usebox{\propositionbox}}
\newtheorem{lem}{\usebox{\lemmabox}}
\newtheorem{fact}{\usebox{\factbox}}
\newtheorem{cor}{\usebox{\corollarybox}}
\newtheorem{problem}{\usebox{\problembox}}
\newtheorem{defn}{\usebox{\defbox}}

\def\blackslug{\hbox{\hskip 1pt \vrule width 4pt height 8pt depth 1.5pt
\hskip 1pt}}
\newtheorem{@assumption}{\bf Assumption}[section]
\newenvironment{assumption}{\begin{@assumption}\rm}{\end{@assumption}}
 \newtheorem{@remark}{\bf Remark}[section]
 \newenvironment{remark}{\begin{@remark}\rm}{\end{@remark}}


\newcommand{\qed}{\mbox{}\hspace*{\fill}\nolinebreak\mbox{$\rule{0.7em}{0.7em}$}}
%\newenvironment{proof}{\par{\noindent \bf Proof:}}{\(\qed\) \par}

\newenvironment{proof}[1][\noindent \bf Proof:]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}


\newcommand{\norm}[1]{\left\Vert#1\right\Vert}
\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\set}[1]{\left\{#1\right\}}

\newcommand{\basis}{(\Omega,  \, (\F_t)_{t \in \Real_+}, \, \prob)}
\newcommand{\indic}{\mathbb{I}}
\newcommand{\pare}[1]{\left(#1\right)}
\newcommand{\bra}[1]{\left[#1\right]}
\newcommand{\such}{\, | \, }
\newcommand{\vs}{\vspace*{3mm}}
\newcommand{\com}[1]{\textbf{\color{red} #1 }} % \textbf{#1}}}
\newcommand{\argmax}{\mathop{\rm argmax}}

%%% newly added commands %%%%%%%%%%%

\newcommand{\argmin}{\mathop{\rm argmin}}
\newcommand{\esssup}{\mathop{\rm esssup}}
\newcommand{\essinf}{\mathop{\rm essinf}}
\newcommand{\lt}{\left}
\newcommand{\rt}{\right}
\newcommand{\rd}{{\rm d}}
\newcommand{\ri}{{\rm i}}
\newcommand{\Tr}{{\rm Tr}}


%% bold face roman
%%
\def\bA{{\bf A}}
\def\ba{{\bf a}}
\def\bB{{\bf B}}
\def\bb{{\bf b}}
\def\bC{{\bf C}}
\def\bc{{\bf c}}
\def\bD{{\bf D}}
\def\bd{{\bf d}}
\def\bE{{\bf E}}
\def\be{{\bf e}}
\def\bF{{\bf F}}
\def\boldf{{\bf f}}
\def\bG{{\bf G}}
\def\bg{{\bf g}}
\def\bH{{\bf H}}
\def\bh{{\bf h}}
\def\bi{{\bf i}}
\def\bI{{\bf I}}
\def\bJ{{\bf J}}
\def\bj{{\bf j}}
\def\bK{{\bf K}}
\def\bk{{\bf k}}
\def\bL{{\bf L}}
\def\bl{{\bf l}}
\def\bM{{\bf M}}
\def\bm{{\bf m}}
\def\bN{{\bf N}}
\def\bn{{\bf n}}
\def\bO{{\bf O}}
\def\bo{{\bf o}}
\def\bP{{\bf P}}
\def\bp{{\bf p}}
\def\bQ{{\bf Q}}
\def\bq{{\bf q}}
\def\bR{{\bf R}}
\def\br{{\bf r}}
\def\bS{{\bf S}}
\def\bs{{\bf s}}
\def\bT{{\bf T}}
\def\bt{{\bf t}}
\def\bU{{\bf U}}
\def\bu{{\bf u}}
\def\bV{{\bf V}}
\def\bv{{\bf v}}
\def\bW{{\bf W}}
\def\bw{{\bf w}}
\def\bX{{\bf X}}
\def\bx{{\bf x}}
\def\bY{{\bf Y}}
\def\by{{\bf y}}
\def\bZ{{\bf Z}}
\def\bz{{\bf z}}
%%


%% bold face Greek (incomplete)
\def\balpha{\mbox{\boldmath $\alpha$}}
\def\bbeta{\mbox{\boldmath $\beta$}}
\def\bepsilon{\mbox{\boldmath $\varepsilon$}}
\def\bvarepsilon{\mbox{\boldmath $\varepsilon$}}
\def\bDelta{{\bf \Delta}}
\def\bdelta{\mbox{\boldmath $\delta$}}
%\def\boldeta{\mbox{\boldmath $\zeta$}}
\def\boldeta{\mbox{\boldmath $\eta$}}
\def\bGamma{{\bf \Gamma}}
\def\bgamma{\mbox{\boldmath $\gamma$}}
\def\bLambda{{\bf \Lambda}}
\def\blambda{\mbox{\boldmath $\lambda$}}
\def\bmu{\mbox{\boldmath $\mu$}}
\def\bOmega{{\bf \Omega}}
\def\bpi{\mbox{\boldmath $\pi$}}
\def\bphi{\mbox{\boldmath $\phi$}}
\def\bPsi{{\bf \Psi}}
\def\bpsi{\mbox{\boldmath $\psi$}}
\def\bSigma{{\bf \Sigma}}
\def\bsigma{\mbox{\boldmath $\sigma$}}
\def\bTheta{{\bf \Theta}}
\def\btheta{\mbox{\boldmath $\theta$}}
\def\bXi{{\bf \Xi}}
\def\bxi{\mbox{\boldmath $\xi$}}
\def\bzeta{\mbox{\boldmath$\zeta$}}
%%

%% caligraphy letters
\def\cA{{\cal A}}
\def\cB{{\cal B}}
\def\cC{{\cal C}}
\def\cD{{\cal D}}
\def\cE{{\cal E}}
\def\cF{{\cal F}}
\def\cG{{\cal G}}
\def\cI{{\cal I}}
\def\cL{{\cal L}}
\def\cM{{\cal M}}
\def\cO{{\cal O}}
\def\cR{{\cal R}}
\def\cS{{\cal S}}
\def\cT{{\cal T}}
\def\cY{{\cal Y}}
\def\cU{{\cal U}}
\def\cV{{\cal V}}

%% mathsf letters
%%
\def\sA{{\sf A}}
\def\sB{{\sf B}}
\def\sC{{\sf C}}
\def\sc{{\sf c}}
\def\sD{{\sf D}}
\def\sE{{\sf E}}
\def\sF{{\sf F}}
\def\sG{{\sf G}}
\def\sH{{\sf H}}
\def\sI{{\sf I}}
\def\sJ{{\sf J}}
\def\sK{{\sf K}}
\def\sL{{\sf L}}
\def\sM{{\sf M}}
\def\sN{{\sf N}}
\def\sP{{\sf P}}
\def\sQ{{\sf Q}}
\def\sR{{\sf R}}
\def\sS{{\sf S}}
\def\sT{{\sf T}}
\def\sU{{\sf U}}
\def\sV{{\sf V}}
\def\sv{{\sf v}}
\def\sW{{\sf W}}
\def\sX{{\sf X}}
\def\sY{{\sf Y}}
\def\sZ{{\sf Z}}

%% mathbb letters
%%
\def\bbA{\mathbb{A}}
\def\bbB{\mathbb{B}}
\def\bbC{\mathbb{C}}
\def\bbD{\mathbb{D}}
\def\bbE{\mathbb{E}}
\def\bbF{\mathbb{F}}
\def\bbG{\mathbb{G}}
\def\bbH{\mathbb{H}}
\def\bbI{\mathbb{I}}
\def\bbJ{\mathbb{J}}
\def\bbK{\mathbb{K}}
\def\bbL{\mathbb{L}}
\def\bbM{\mathbb{M}}
\def\bbN{\mathbb{N}}
\def\bbF{\mathbb{F}}
\def\bbP{\mathbb{P}}
\def\bbQ{\mathbb{Q}}
\def\bbR{\mathbb{R}}
\def\bbS{\mathbb{S}}
\def\bbT{\mathbb{T}}
\def\bbU{\mathbb{U}}
\def\bbV{\mathbb{V}}
\def\bbW{\mathbb{W}}
\def\bbX{\mathbb{X}}
\def\bbY{\mathbb{Y}}
\def\bbZ{\mathbb{Z}}

%% mathfrack incomplete
\newcommand{\fE}{\mathfrak{E}}
\newcommand{\fD}{\mathfrak{D}}

%%
%%
%% \blot gives a square at end of formulas and proofs
%% (thanks to Marc Posner)
\def\blot{\quad {$\vcenter{\vbox{\hrule height.4pt
             \hbox{\vrule width.4pt height.9ex \kern.9ex \vrule
width.4pt}
             \hrule height.4pt}}$}}

%% End of latexmacro

%adjusting margins
%\usepackage{geometry}
%\geometry{paper=letterpaper, top=1.25 in, inner=1.25 in, outer=1.25 in, bottom=1 in, footskip=.5 in}

\textwidth 6.5 in \hoffset -.8in \textheight 9 in \voffset -.8in

\newcommand{\calW}{{\mathcal W}}
\newcommand{\bfd}{{\bf d}}
\newcommand{\bfT}{{\bf T}}
\newcommand{\bfA}{{\bf A}}
\newcommand{\bfB}{{\bf B}}
\newcommand{\bfX}{{\bf X}}
\newcommand{\bfU}{{\bf U}}
\newcommand{\bfY}{{\bf Y}}
\newcommand{\bfy}{{\bf y}}
\newcommand{\bfV}{{\bf V}}
\newcommand{\bfW}{{\bf W}}
\newcommand{\bfZ}{{\bf Z}}
\newcommand{\bfR}{{\bf R}}
\newcommand{\bfN}{{\bf N}}
\newcommand{\bfC}{{\bf C}}
\newcommand{\bfgamma}{{\boldsymbol \gamma}}
\newcommand{\rE}{\textrm{E}}
\newcommand{\E}{\bbE}
\newcommand {\cov}{\qopname\relax n{\textrm{Cov}}}
\newcommand {\var}{\qopname\relax n{\textrm{Var}}}
\newcommand {\corr}{\qopname\relax n{\textrm{Corr}}}
\newcommand {\Le}{\mathbb{L}}
\newcommand {\B}{{\mathcal{B}}}
\newcommand {\ind} {\overset{d}{=}}
\newcommand {\inD}{\overset{\mathcal{D}}{\rightarrow}}
\newcommand {\tod} {\overset{d}{\rightarrow}}
\newcommand {\topp} {\overset{p}{\rightarrow}}
\newcommand {\xn}{\{ X_n \}}
\newcommand {\fdd}{\overset{fdd}{\longrightarrow}}
\newcommand {\indf}[1]{{1}_{\{#1 \}}}
\newcommand {\argsup}[1]{\underset{#1}{\rm{argsup}}}
\newcommand {\wpsi}{{\psi}}
\newcommand {\D}[1]{\dot{#1}}
\newcommand {\revd}{\tilde{d}}
\newcommand {\revb}{\tilde{b}}
\newcommand {\revx}{\tilde{X}}
\newcommand {\fbar}{\overline{F}}
\newcommand {\xbar}{\overline{X}}
\newcommand {\ybar}{\overline{Y}}
\newcommand {\normal}{{\mathcal N}}
\newcommand {\khat}{\what{k}}
\newcommand {\that}{\what{\tau}}
\newcommand {\xhat}{\what{X}}
\newcommand {\yhat}{\what{Y}}
\newcommand {\Rhat}{\what{R}}
\newcommand {\oml}{\omega_l}
\newcommand {\lwhat}{\what{d}_{lw}}
\newcommand {\gphhat}{\what{d}_{gph}}
\newcommand {\cusum}{\mbox{CUSUM}}
\newcommand {\adjcusum}{\mbox{adjCUSUM}}
\newcommand {\hnull}{\mbox{H}_0}
\newcommand {\halt}{\mbox{H}_1}
\newcommand {\rhohat}{ \hat{\rho}}
\newcommand {\mhat}[1]{{#1}_t^{(m)}}
\newcommand {\pc}[1]{ #1^{\scriptsize pc}  }
\newcommand{\doublebar}[1]{\bar{\bar{#1}}}
\newcommand{\doubletilde}[1]{\tilde{\tilde{#1}}}
\newcommand{\doublehat}[1]{\hat{\hat{#1}}}
\newcommand{\doublecheck}[1]{\check{\check{#1}}}

%Younghoon Kim's personal setting. It can be modified.
\usepackage{multirow}
\providecommand{\keywords}[1]
{
{  \small	
  {\textit{Keywords---}} #1}
}
\usepackage{cleveref}
\crefname{equation}{equation}{equations}
\Crefname{equation}{Equation}{Equations}% For beginning \Cref
\crefrangelabelformat{equation}{(#3#1#4--#5#2#6)}
\crefmultiformat{equation}{equations (#2#1#3}{, #2#1#3)}{#2#1#3}{#2#1#3}
\Crefmultiformat{equation}{Equations (#2#1#3}{, #2#1#3)}{#2#1#3}{#2#1#3}
\linespread{1.5} %line Space
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{rotating}
\usepackage{tikz}




\begin{document}
\title{Group integrative dynamic factor models \\ 
with application to multiple subject brain connectivity}

\author[1]{Younghoon Kim$^{*}$}
\author[2]{Zachary F. Fisher}
\author[3]{Vladas Pipiras} 

\affil[1]{Cornell University}
\affil[2]{The Pennsylvania State University}
\affil[3]{University of North Carolina at Chapel Hill}

\def\thefootnote{$*$}\footnotetext{Corresponding author. Email: yk748@cornell.edu}

\date{\today}
\maketitle

\begin{abstract}
    This work introduces a novel framework for dynamic factor model-based data integration of multiple subjects time series data, called GRoup Integrative DYnamic factor (GRIDY) models. The framework identifies and characterizes inter-subject differences between two pre-labeled groups by considering a combination of group spatial information and individual temporal dependence. Furthermore, it enables the identification of intra-subject differences over time by employing different model configurations for each subject. Methodologically, the framework combines a novel principal angle-based rank selection algorithm and a non-iterative integrative analysis framework. Inspired by simultaneous component analysis, this approach also reconstructs identifiable latent factor series with flexible covariance structures. The performance of the GRIDY models is evaluated through simulations conducted under various scenarios. An application is also presented to compare resting-state functional MRI data collected from multiple subjects in the Autism Spectrum Disorder and control groups.
\end{abstract}

\keywords{Data integration, high-dimensional time series, principal angles, dynamic factor model, multi-way analysis, fMRI.}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{se:intro}


\subsection{Statistical analysis of brain connectivity}
\label{se:connectivity}

The analysis of brain connectivity typically focuses on examining the synchronized activity of spatially distinct brain regions. Modeling such interconnected regions includes structural connectivity (a network of anatomical or structural connections linking distinct units), and functional connectivity (capturing statistical dependence between distinct units). The units can be individual neurons, neuronal populations, and anatomical regions of interest (ROIs). In addition to the statistical dependence, one may be interested in causal interactions between these units (effective connectivity) \cite[see][for further explanation]{sporns:2007}. 


To explore functional connectivity, one uses functional magnetic resonance imaging (fMRI) to capture blood-oxygen-level dependent (BOLD) signals observed over time. Each BOLD signal is mapped to pre-specified ROIs via a brain atlas. The strength of this signal is regarded as a proxy for brain activity since the blood oxygenation of the area of interest increases when the corresponding regions become active. Depending on the research goal, one may focus on establishing a statistical model to localize brain areas activated by the task, or building a prediction model about psychological or disease states. For these purposes, task-based fMRI (T-fMRI), a series of BOLD signals acquired while the subject performs a set of tasks, is often used. In making inferences about the structure of relationships among brain regions, across time points, and between subjects, resting-state fMRI (R-fMRI) is another option \cite[e.g.,][]{smith:2004}. Our motivating application will focus on the analysis of R-fMRI.


Two types of questions can be addressed with R-fMRI for statistical brain network analysis. The first question concerns the construction of a network representation that reveals spatial and temporal dependences between ROIs. The most active research related to this question involves identifying commonality and uniqueness in connectivity patterns between (group of) subjects. The other question is about establishing a mathematical model that describes effective connectivity of the brain network  \cite[e.g.,][]{lindquist:2008}.


One promising statistical tool for addressing the first question is called data integration. In this context, data integration can be used to study similarities and differences in structures across subjects. Often, this heterogeneity is due to factors either directly observable (e.g., demographic, disease status, experiments) or hidden (e.g., causal mediation), which can be revealed through statistical modeling. The observations for such studies form multi-block (or multi-view) data, and these data structures naturally motivate the simultaneous exploration of the joint and (group) individual variations across and within data blocks. However, this approach often discounts temporal dependence and shows limitations in explaining directed connections. On the other hand, the second question can be dealt with using network modeling. For example, structural equation modeling (SEM) can be used to test causal relationships between ROIs. Alternatively, statistical dependence can be formulated through graphical models. However, identifying joint and (group) individual configurations in this context is less obvious, and the number of such studies is relatively low \cite[e.g.,][and references therein]{lee:2013}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Contributions}
\label{se:contribution}


We propose GRoup Integrative DYnamic factor models, which we abbreviate as GRIDY, and their estimation procedure for multivariate time series data (e.g., R-fMRI) collected one set of common variables (e.g., ROIs) across subjects. There are several important points to note about GRIDY models. First, we can identify model configurations for spatial and temporal information. Distinguishing model parameters contributes to explaining inter-subject differences in the measurement and dynamics of the time-dependent signals. Second, we allow the factor series to evolve over time. This is crucial because the model can present dynamic temporal dependencies, thus explaining intra-subject differences over time. Third, by allowing flexible covariance structures of factor series, the model can capture the heterogeneity of the subjects through different scales and combinations of factor series. Last, we analyze R-fMRI data from multiple subjects from a time-series perspective. To the best of our knowledge, our model is the first to suggest a comparison of the brain networks estimated from R-fMRI of multiple subjects, explaining both inter-subject and intra-subject differences through time series modeling. Our model also seems to be the first to bring time dimensions into modern data integration, especially when explaining joint and individual variations, by considering multivariate time series for multiple subjects.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Related approaches}
\label{se:related}


From a methodological standpoint, two developments are related to our proposed approach. One is integrative data analysis. It has been employed in various fields such as computational biology, chemometrics, and some areas of neuroscience. It originated from partial least squares (PLS) or canonical correlations analysis (CCA) to find the maximum covariance (correlation, respectively) between sample projections on the canonical loadings and the canonical scores from different subjects \cite[e.g.,][]{witten:2009,lofstedt:2013}. A variant of principal component analysis (PCA) for finding the direction presenting a maximal variation from the center has also been leveraged for multi-block data \cite[e.g.,][]{abdi:2013}. 


The data integration approach has been developed to discover common and individual structures shared across different subjects. The recent development tends to include various block segmentations of multi-block data \cite[e.g.,][]{o:2019,park:2020}. Several data integration approaches have also been applied to neuroimaging studies. For example, \cite{yu:2017} analyzed behavioral and T-fMRI data, while \cite{murden:2022} applied their model to R-fMRI and diffusion MRI (dMRI) to find similarities between functional connectivity and structural connectivity. Their approach extracts common structures from different datasets, with the heterogeneity arising from the source of data rather than factors uniquely embedded in subjects. However, these approaches share the limitation of ignoring temporal dependence.



Another related approach is Gaussian graphical models (GGMs). Several recent works on the joint estimation of these models incorporate temporal dependencies. For example, \cite{qiu:2016} used a kernel-based method to represent smooth changes in dependence over time. Similarly, \cite{qiao:2019} introduced Gaussian random functions to model temporal dependence, and \cite{zhu:2018} introduced matrix-valued variables to represent spatial and temporal dependence simultaneously. In contrast, \cite{fan:2018} added external covariates to loadings matrices in factor models to construct graphical models from the residuals. However, it is important to note that graphical models cannot be used to understand directional (causal) relationships between variables.


Time series models offer an alternative as they easily incorporate directed and lagged relationships. Their usefulness in the analysis of fMRI has been demonstrated \cite[e.g.,][]{chen:2011,gates:2012}. Recently, \cite{skripnikov:2019}, \cite{manomaisaowapak:2022} employed combinations of different penalized estimations of vector autoregressive (VAR) models to represent subject heterogeneity by specifying both joint and individual structures. However, the joint structures in their works need not be identical, which has a limitation in explaining common dependences. \cite{fisher:2022,fisher:2023} addressed this issue by imposing additivity on the transition matrices, ensuring that the model produces identical estimates across subjects while allowing the rest of the components to be individually specified. However, VAR-based modeling requires a relatively large computational cost compared to other dimension reduction methods.


Hence, we shall employ dynamic factor models (DFMs) within the data integration framework. DFMs provide low-dimensional dynamic representations that enhance explainability and typically involve low computational costs in their estimation procedure. Our study extends the application of DFMs by fitting R-fMRI data from multiple subjects, enabling us to explore both inter- and intra-subject differences. Through the reconstruction of latent factor structures, we also investigate the heterogeneity among subjects. To the best of our knowledge, this work is the first model, even in terms of methodology, to consider DFMs in the context of multiple subjects and data integration.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Outline of study}
\label{sse:outline}

The rest of the manuscript is organized as follows. In Section \ref{se:model_procedure}, we introduce our model and summarize our procedure. In Section \ref{se:method}, we describe the segmentation of multi-block data and discuss the identification of structures. Then, we introduce the estimation method for the covariance of the latent factors and the method to reconstruct the dynamics of factor series. In Section \ref{se:illustrative}, we provide simulation results under various settings that support the approach empirically. In Section \ref{se:data_app}, a joint analysis of R-fMRI data collected from multiple individuals is conducted and leads to the functional connectivity networks of brain regions. In Section \ref{se:discussion}, we summarize the results and suggest possible extensions.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Proposed model and procedure}
\label{se:model_procedure}


\subsection{Population model}
\label{sse:model}


Assume that we divide all individuals into $G$ non-overlapping and different groups and assume there are $K_1,K_2,\ldots,K_G$ number of subjects in each group. To simplify the notation and to be consistent with our example data, we consider only two groups, $G=2$, and suppose each group contains an equal number of subjects, with the first $K$ subjects in the first group and the remaining $K$ subjects in the second group. It is possible to extend the number of groups to be more than two and handle an unequal number of subjects in each group.


The observations from the $k$th subject are as follows. Define the $T_k\times d$ dimensional observation matrix $\mathbf{X}_{k}=(X_{i,t}^{(k)})_{i=1,\ldots,d,\ t=1,\ldots,T_k}$ to represent $k$th subject, and $d$ variables with the observations collected over $T_k$ times. We assume that the temporal evolution of $k$th subject observations is explained by $r$ latent factor series, in the presence of the additive noise $\mathbf{E}_{k}\in\mathbb{R}^{T_k\times d}$. More specifically, the model is defined as
%
\begin{equation} \label{e:observation_k}
    \mathbf{X}_{k} = 
    \left\{\begin{array}{ll}
      \mathbf{F}_k\mathbf{B}_1'+\mathbf{E}_{k}, & \quad k=1,\ldots,K, \\
      \mathbf{F}_k\mathbf{B}_2'+\mathbf{E}_{k}, & \quad k=K+1,\ldots,2K,
    \end{array}\right.
\end{equation}
%
where $\mathbf{F}_k\in\mathbb{R}^{T_k \times r}$ are factor series of $k$th subject and the corresponding loadings matrices are denoted as $\mathbf{B}_1,\mathbf{B}_2\in\mathbb{R}^{d \times r}$ so that the components $\mathbf{F}_k\mathbf{B}_1'$ and $\mathbf{F}_k\mathbf{B}_2'$ construct the \textit{block signals} of $k$th subject in each group, respectively. 

Through suitable analysis, we are interested in the separation of contributions for explaining the variability of the data jointly and individually. Among the block signals of $k$th subject in $g$th group $\mathbf{F}_k\mathbf{B}_g'$, we assume that this block can be decomposed into the joint components and group individual components, 
%
\begin{equation}\label{e:augmented_block1}
        \mathbf{F}_k\mathbf{B}_g' 
        = \bar{\mathbf{F}}_k \bar{\mathbf{B}}^{'} + \widetilde{\mathbf{F}}_k \widetilde{\mathbf{B}}_g^{'},
\end{equation}
%
where $\bar{\mathbf{B}}=(\bar{B}_{i,j})_{i=1,\ldots,d,\ j=1,\ldots,r_{J}}$ is the \textit{joint loadings matrix} identical to all $2K$ subjects, $\tilde{\mathbf{B}}_{g}=(\tilde{B}_{i,j}^{(g)})_{i=1,\ldots,d,\ j=1,\ldots,r_{G}}$, $g=1,2$, are \textit{group loadings matrices} only shared through the subjects within each group, $\bar{\mathbf{F}}_k \in \mathbb{R}^{T_k \times r_J}$ are \textit{joint factor series} and $\widetilde{\mathbf{F}}_k$ are \textit{group individual factor series}. We set $r=r_J+r_G$. Note that the ranks of the group individual factor series need not be identical; one can set the numbers of individual factors for groups 1 and 2 as $r_{G_1}\neq r_{G_2}$. However, to simplify the model description, we limit these quantities to be identical. 



To construct DFMs, for each $k$, the $d-$dimensional vector of observation at time $t$, denoted as $X_{t}^{(k)}$, is explained by the joint component vector $\bar{\mathbf{B}} \bar{F}_{t}^{(k)}$ and group individual component vector $\tilde{\mathbf{B}}_{g}\tilde{F}_{t}^{(k)}$, $g=1,2$, where
%
\begin{eqnarray}
    X_{t}^{(k)} 
    &=& \mathbf{B}_{g}F_{t}^{(k)} + E_{t}^{(k)} \label{e:augmented_factor_1} \\
    &=& \begin{bmatrix} \bar{\mathbf{B}} & \tilde{\mathbf{B}}_{g} \end{bmatrix}
    \begin{bmatrix} \bar{F}_{t}^{(k)} \\ \tilde{F}_{t}^{(k)} \end{bmatrix} + E_{t}^{(k)} \\
    &=& \bar{\mathbf{B}} \bar{F}_{t}^{(k)} + \tilde{\mathbf{B}}_{g}\tilde{F}_{t}^{(k)} + E_{t}^{(k)} \label{e:augmented_factor_2}\\
    &=& \bar{X}_{t}^{(k)} + \tilde{X}_{t}^{(k)} + E_{t}^{(k)},\ g=1,2,\ k=1,\ldots,2K, \label{e:augmented_factor_3}
\end{eqnarray}
%
and $E_{t}^{(k)}=(E_{i,t}^{(k)})_{i=1,\ldots,d}$ is a noise vector with zero mean and variance $\boldsymbol{\Sigma}_{E,k}$. To connect \eqref{e:augmented_factor_1}--\eqref{e:augmented_factor_3} with \eqref{e:observation_k} and \eqref{e:augmented_block1}, we define $X_{t}^{(k)}$ as being explained by a linear combination of the $r_J-$dimensional joint factor $\bar{F}_{t}^{(k)}=(\bar{F}_{j,t}^{(k)})_{j=1,\ldots,r_{J}}$ and $r_G-$dimensional group individual factor $\tilde{F}_{t}^{(k)}=(\tilde{F}_{j,t}^{(k)})_{j=1,\ldots,r_{G}}$. In summary, the model is defined as
%
\begin{eqnarray} 
    \mathbf{X}_{k} &=& \begin{bmatrix}
                        X_1^{(k)'} \\
                        \vdots \\ 
                        X_{T_k}^{(k)'}
                        \end{bmatrix}  = \bar{\mathbf{X}}_{k} + \tilde{\mathbf{X}}_{k} + \mathbf{E}_{k}
                = \begin{bmatrix}
                        \bar{X}_1^{(k)'} \\
                        \vdots \\ 
                        \bar{X}_{T_k}^{(k)'}
                    \end{bmatrix} 
                + \begin{bmatrix}
                    \tilde{X}_1^{(k)'} \\
                     \vdots \\ 
                     \tilde{X}_{T_k}^{(k)'}
                \end{bmatrix}
               + \begin{bmatrix}
                    E_1^{(k)'} \\
                        \vdots \\ 
                    E_{T_k}^{(k)'}
                \end{bmatrix} \label{e:matrix_joint_individual1}  \\
                &=& \mathbf{F}_k\mathbf{B}_g'+\mathbf{E}_{k} 
            = \begin{bmatrix} 
                    F_1^{(k)'} \\
                    \vdots \\ 
                    F_{T_k}^{(k)'}
                \end{bmatrix}
                \begin{bmatrix}
                B_1^{(g)'} & \ldots &  B_{d}^{(g)'}
                \end{bmatrix}
                + \begin{bmatrix}
                E_1^{(k)'} \\
                \vdots \\ 
                E_{T_k}^{(k)'}
                \end{bmatrix}, \label{e:matrix_joint_individual2}
\end{eqnarray}
where the block signal for $k$th subject is 
%
\begin{equation}\label{e:augmented_block2}
        \mathbf{F}_k\mathbf{B}_g' 
        = \begin{bmatrix}
        \bar{F}_1^{(k)'} \\
        \vdots \\ 
        \bar{F}_{T}^{(k)'}
        \end{bmatrix}
        \begin{bmatrix}
        \bar{B}_1^{'} & \ldots &  \bar{B}_{d}^{'}
        \end{bmatrix} 
        + \begin{bmatrix}
        \tilde{F}_1^{(k)'} \\
        \vdots \\ 
        \tilde{F}_{T}^{(k)'}
        \end{bmatrix}
        \begin{bmatrix}
        \tilde{B}_1^{(g)'} & \ldots &  \widetilde{B}_{d}^{(g)'}
        \end{bmatrix}
        = \bar{\mathbf{F}}_k \bar{\mathbf{B}}^{'} + \widetilde{\mathbf{F}}_k \widetilde{\mathbf{B}}_g^{'}. 
\end{equation}
%


By stacking up all multi-block observations, we have a large matrix of blocks in \eqref{e:observation_k} as follows:
%
\begin{equation}\label{e:multi_block_AJIVE}
            \begin{bmatrix}
                    \mathbf{X}_1 \\
                    \vdots \\
                    \mathbf{X}_{2K} \\
                 \end{bmatrix} 
            = \begin{bmatrix}
                    \mathbf{Y}_1 \\
                    \vdots \\
                    \mathbf{Y}_{2K} \\
                 \end{bmatrix} 
+            \begin{bmatrix}
                    \mathbf{E}_1 \\
                    \vdots \\
                    \mathbf{E}_{2K} \\
                 \end{bmatrix}
              = \begin{bmatrix}
                    \bar{\mathbf{X}}_1 \\
                    \vdots \\
                    \bar{\mathbf{X}}_{2K} \\
                 \end{bmatrix} 
               + \begin{bmatrix}
                    \tilde{\mathbf{X}}_1 \\
                    \vdots \\
                    \tilde{\mathbf{X}}_{2K} \\
                 \end{bmatrix}
               + \begin{bmatrix}
                    \mathbf{E}_1 \\
                    \vdots \\
                    \mathbf{E}_{2K} \\
                 \end{bmatrix},
\end{equation}
%
where $\bar{\mathbf{X}}_k$ refers to the \textit{joint components} of $k$th subject and $\tilde{\mathbf{X}}_k$ refers to the \textit{group individual components} of $k$th subject, which form the joint structure and group individual structures, respectively. Each large matrix consists of $n=\sum_{k}T_k$ samples in time and $d$ variables, usually $n > d$. Note that the number of variables is set to be identical across all subjects while we allow the number of observations $T_k$ of each subject to be different. We also note that the way data blocks are stacked in \eqref{e:multi_block_AJIVE} is not the standard construction seen in the current literature on data integration. A typical stacking is to have subjects on the horizontal axis and variables across several datasets (blocks) on the vertical axis. The stacking used here arises because of the time dimension and is also tied to the model of interest. The number of blocks $2K$ in \eqref{e:multi_block_AJIVE} is also large, compared to what is typically considered in the literature on data integration.


To complete the construction of the latent factor series, assume that VAR($p$) model governs the joint factor series, that is,
%
\begin{equation}\label{e:filter_equation}
    \bar{\boldsymbol{\Psi}}_{k}(L)\bar{F}_{t}^{(k)} = \bar{\eta}_{t}^{(k)},\quad \left\{\bar{\eta}_{t}^{(k)}\right\} \sim \mathrm{WN}(0,\boldsymbol{\Sigma}_{\bar{\eta},k}),
\end{equation}
%
where $\bar{\boldsymbol{\Psi}}_{k}(L)=I - \bar{\boldsymbol{\Psi}}_{1,k}L - \ldots - \bar{\boldsymbol{\Psi}}_{p,k}L^p$ is a $r_J \times r_J$ operator of finite length $p$ with a lag operator $L$. The roots of $\textrm{det}(\bar{\boldsymbol{\Psi}}_{k}(z))$ lie outside the unit circle so that the series becomes stable. Note that, unlike unspecified properties of the noise at the observation level, we quantify the noise for latent factor series to be white noise. The same construction is made for the group individual factor series.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Overview of GRIDY procedure}
\label{sse:Procedure}

We summarize the estimation procedure of GRIDY. In the first step, we decide on the rank $r$ of block signals from block observations $\mathbf{X}_1,\ldots,\mathbf{X}_{2K}$ by applying rotational bootstrap described in Section \ref{ssse:initial_rank}. Next, we apply the algorithm that obtains the joint components $\bar{\mathbf{X}}_1,\ldots,\bar{\mathbf{X}}_{2K}$ and two group individual components $\tilde{\mathbf{X}}_1,\ldots,\tilde{\mathbf{X}}_{K}$ and $\tilde{\mathbf{X}}_{K+1},\ldots,\tilde{\mathbf{X}}_{2K}$ from the raw data via a principal angle-based block segmentation algorithm. This is explained in Section \ref{sse:segmentation}. Third, the algorithm that factorizes the obtained block structures into the loadings matrices $\bar{\mathbf{B}},\tilde{\mathbf{B}}_1,\tilde{\mathbf{B}}_2$ and the factor series $\bar{\mathbf{F}}_1,\ldots,\bar{\mathbf{F}}_{2K}$, $\tilde{\mathbf{F}}_1,\ldots,\tilde{\mathbf{F}}_{K}$, and $\tilde{\mathbf{F}}_{K+1},\ldots,\tilde{\mathbf{F}}_{2K}$ designed for multiple subject component models is applied to each block structure. In the final step, we refit the estimates of factor series to observation blocks by regression and complete the reconstruction of factor dynamics by the Yule-Walker equations. The details of the last two steps are included in Sections \ref{sse:reconstruction} and \ref{ssse:estimation}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Methods}
\label{se:method}


\subsection{Segmentation of joint and group individual variation}
\label{sse:segmentation}



Joint Individual Variation Explained \cite[JIVE;][]{lock:2013} first used the idea of singular value decomposition (SVD) to segment concatenated block observations into joint and individual structures using low-rank approximations. In this approach, the desired structures are found by considering the sums of squared residuals to minimize the unexplained variability. However, a permutation-based rank selection for the joint structure is not guaranteed to achieve the true rank, in particular when there is some correlation across individual structures. Common orthogonal basis extraction \cite[COBE;][]{zhou:2015} can improve the basis extraction in JIVE in a systematic way by adding orthogonality constraints for joint and individual loadings. However, the algorithm is still iterative and the rank selection is not supported theoretically.


Recently, a non-iterative integrative data analysis framework, namely Angle-Based Joint Individual Variation Explained \cite[AJIVE;][]{feng:2018}, was developed. This framework decomposes multi-block data into the desired structures. Unlike previous works that determine the joint structure by enforcing orthogonality between the joint structure and the individual structure, AJIVE employs the concept of Principal Angle Analysis (PAA). Furthermore, the estimation procedure is non-iterative, which saves computational cost when the number of blocks is substantial.


The AJIVE method applied to our problem works as follows. An initial estimate $\hat{r}_k$ of the signal block $\mathbf{Y}_k$ of $\mathbf{X}_k$ is needed for each $k$. Its selection is discussed in Section 3.1.1. Let
%
\begin{equation}\label{e:svd_subject}
    \hat{\mathbf{Y}}_k = \hat{\mathbf{U}}_k \hat{\mathbf{S}}_k \hat{\mathbf{V}}_k'
\end{equation}
%
be rank-$\hat{r}_k$ approximation of $\mathbf{Y}_k$ obtained from the SVD of $\mathbf{X}_k$, where $\hat{\mathbf{U}}_k$ and $\hat{\mathbf{V}}_k$ are $T_k \times \hat{r}_k$ and $d \times \hat{r}_k$ matrices of orthonormal columns and $\hat{\mathbf{S}}_k$ is a diagonal matrix with $\hat{r}_k$ singular values. Note that $\hat{\mathbf{V}}_k$ can be thought as an approximation of $\mathbf{B}_g$ under our model \eqref{e:matrix_joint_individual2}. Since we are looking for a joint structure among $2K$ subjects, it is natural to consider the following $( \sum \hat{r}_k ) \times d$ matrix and its SVD:
%
\begin{equation}\label{e:mat_J}
    \hat{J} 
    = \begin{bmatrix}
    \hat{\mathbf{V}}_1' \\ \vdots \\ \hat{\mathbf{V}}_{2K}' 
    \end{bmatrix} = \hat{\mathbf{U}}_J\hat{\mathbf{S}}_J\hat{\mathbf{V}}_J',
\end{equation}
%
where, in particular, the diagonal matrix $\hat{\mathbf{S}}_J$ consists of singular values $\hat{\sigma}_{J,i}$ in decreasing order. The idea then is to decide on the number $\hat{r}_J$ of singular values $\hat{\sigma}_{J,i}$ and take the corresponding columns of $\hat{\mathbf{V}}_J$ to capture a joint structure among $2K$ subjects. 


Two approaches were suggested in \cite{feng:2018} for the number of singular values $\hat{\sigma}_{J,i}$ for a joint structure: the random direction bound approach and the Wedin bound approach. In the random direction approach, one replaces $\hat{\mathbf{V}}_{k}$ by randomly generated matrices with orthonormal columns and computes the largest singular value of the resulting matrix in \eqref{e:mat_J}. The process is repeated $M$ times and the 5th percentile of the distribution of such largest singular values is taken as a threshold for $\hat{\sigma}_{J,i}$. In the other approach, the Wedin bound (in the second inequality below concerning principal angles) is used to show that
%
\begin{equation}\label{e:singular_bound}
    \hat{\sigma}_{J,i}^2 \geq 2K - \sum_{k=1}^{2K} \sin^2(\theta_{k,r_k \wedge \hat{r}_k}) \geq 2K - \sum_{k=1}^{2K} \left\{ \frac{\max ( \|\mathbf{E}_k\hat{\mathbf{V}}_k\|_2, \|\mathbf{E}_k'\hat{\mathbf{U}}_k\|_2 )}{\sigma_{\min}(\hat{\mathbf{Y}}_k)} \wedge 1 \right \}^2,
\end{equation}
%
where $\sigma_{\min}(\hat{\mathbf{Y}}_k)$ denotes the smallest singular value of $\hat{\mathbf{Y}}_k$ in \eqref{e:svd_subject} and $\theta_{k,r_k \wedge \hat{r}_k}$ is the largest principal angle between subspaces for $\mathbf{Y}_k$ and $\hat{\mathbf{Y}}_k$. In practice, $\|\mathbf{E}_k\hat{\mathbf{V}}_k\|_2$ and $\|\mathbf{E}_k'\hat{\mathbf{U}}_k\|_2$ are approximated by $\|\mathbf{X}_k\tilde{\mathbf{V}}_k\|_2$ and $\|\mathbf{X}_k'\tilde{\mathbf{U}}_k\|_2$ where $\tilde{\mathbf{V}}_k$ and $\tilde{\mathbf{U}}_k$ have the same dimensions as and are orthogonal to $\hat{\mathbf{V}}_k$ and $\hat{\mathbf{U}}_k$, respectively. Repeating this process $L(=M)$ times, one obtains $L$ approximations of the lower bound in \eqref{e:singular_bound} and takes their 95 percentile as a threshold for the squared singular values $\hat{\sigma}_{J,i}^2$. The final determination of the joint rank $\hat{r}_{J}$ is achieved by counting the number of $\hat{\sigma}_{J,i}$ that exceed both thresholds.



\begin{comment}
We elucidate the concept of PAA using the notation of GRIDY in Section \ref{se:model_procedure} as follows. Suppose that for each subject $k$, our goal is to properly approximate $\mathbf{Y}_k$, rank-$r_k$ signal block in \eqref{e:augmented_block2}, by rank-$\tilde{r}_k$ approximation $\tilde{\mathbf{Y}}_k$ under the presence of the noise $\mathbf{E}_k$. We define the principal angles between the row subspaces generated by $\mathbf{Y}_k$ and $\tilde{\mathbf{Y}}_k$ as
%
\begin{equation}\label{e:theta}
    \Theta\{\textrm{row}(\mathbf{Y}_k),\textrm{row}(\tilde{\mathbf{Y}}_k)\} = \{\theta_{k,1},\ldots,\theta_{k,r_{k} \wedge \tilde{r}_{k}} =: \tilde{\theta}_{k} \},
\end{equation}
%
where $\theta_{k,1} \leq \ldots \leq \tilde{\theta}_k$, and we place particular emphasis on the largest principal angle $\tilde{\theta}_k$ between these two subspaces. \cite{feng:2018} employ the Generalized $\sin\theta$ Theorem of \cite{wedin:1972} to bound $\tilde{\theta}_k$.


\begin{lem}[\cite{feng:2018}]\label{cor:wedin}
For $\mathbf{Y}_k$ and $\tilde{\mathbf{Y}}_k$ described above, let $\tilde{\mathbf{Y}}_{k} = \tilde{\mathbf{U}}_{k}\tilde{\mathbf{S}}_{k}\tilde{\mathbf{V}}_{k}'$ be the rank-$\tilde{r}_k$ SVD of $\mathbf{X}_k$. If $r_k = \tilde{r}_k$, then $\sin(\tilde{\theta}_{k})$, the distance between the subspaces of $\mathbf{Y}_k$ and $\tilde{\mathbf{Y}}_k$ satisfies
%
\begin{equation}\label{e:wedin_bdd}
    \sin(\tilde{\theta}_{k}) \leq \frac{\max \{\|\mathbf{E}_k\tilde{\mathbf{V}}_{k}\|_2,\|\mathbf{E}_k'\tilde{\mathbf{U}}_{k}\|_2 \} }{\sigma_{\min}(\tilde{\mathbf{Y}}_{k})} \wedge 1,
\end{equation}
%
where $\sigma_{\min}(\mathbf{A})$ is the minimum singular value of $\mathbf{A}$. 
\end{lem}
Additionally, if $r_k > \tilde{r}_k$, there is a bound similar to \eqref{e:wedin_bdd} on the distance between $\mathbf{Y}_k$ and $\mathbf{Y}_{k,1}$, where $\mathbf{Y}_{k,1}$ is the approximation of $\mathbf{Y}_{k}$ with underestimated rank-$\tilde{r}_k$. Similarly, Lemma \ref{cor:wedin} can be modified for the case of overestimated rank, $r_k < \tilde{r}_k$. In other words, the Wedin bound \eqref{e:wedin_bdd} serves as a theoretical upper bound for the largest principal angle $\bar{\theta}_k$ between two subspaces. By leveraging the fact that the larger principal angle between $\textrm{row}(\mathbf{Y}_k)$ and $\textrm{row}(\tilde{\mathbf{Y}}_k)$ implies the smaller singular values resulting from the multiplication of orthogonal matrices in those two subspaces  \cite[e.g., Theorem 1 in][]{bjorck1973numerical}, the AJIVE constructs a simulated lower bound for the smallest singular values of the orthogonal matrix described below. In practice, since the noise $\mathbf{E}_k$  cannot be observed, the algorithm produces a resampled distribution of the Wedin bound, described below. Combined with Lemma \ref{cor:wedin}, the following result determines the largest permissible principal angle for the joint structure. As in \cite{feng:2018}, we illustrate the result with the number of subjects restricted to two.


\begin{lem}[\cite{feng:2018}]\label{lem:feng2}
Let $\phi$ be the largest principal angle between two subspaces that are each a perturbation of the common row space within $\textrm{row}(\tilde{\mathbf{Y}}_1)$ and $\textrm{row}(\tilde{\mathbf{Y}}_2)$. Then,
%
\begin{equation*}
    \sin(\phi) \leq \sin(\tilde{\theta}_{1} + \tilde{\theta}_{2}),
\end{equation*}
%
where $\tilde{\theta}_{1},\tilde{\theta}_{2}$ are defined in \eqref{e:theta}.
\end{lem}
By substituting quantities derived from the Wedin bounds for $\tilde{\theta}_k$'s, one can establish criteria to determine the rank of the joint structure. However, as highlighted by \cite{feng:2018} and demonstrated in their example, the Wedin bound can be overly conservative for over-specified ranks of the approximation.  To address this issue, \cite{feng:2018} introduced the random direction bound described below. 


We summarize the AJIVE algorithm as follows. Firstly, for each $k$th observations block $\mathbf{X}_k$, take the rank-$\hat{r}_k$ SVD to obtain the approximation $\hat{\mathbf{Y}}_k = \hat{\mathbf{U}}_k\hat{\mathbf{S}}_k\hat{\mathbf{V}}_k'$ of the signal block $\mathbf{Y}_k$, where $\hat{\mathbf{U}}_k,\hat{\mathbf{V}}_k$ are matrices of orthogonal columns and $\mathbf{S}_k$ is a diagonal matrix with estimated $\hat{r}$ singular values. Then, construct the concatenation of $\hat{\mathbf{V}}_k$'s and perform rank-$\min_{k}\hat{r}_k$ SVD so that
%
\begin{equation}\label{e:mat_J}
    \hat{J} 
    = \begin{bmatrix}
    \hat{\mathbf{V}}_1 \\ \vdots \\ \hat{\mathbf{V}}_{2K} 
    \end{bmatrix} = \hat{\mathbf{U}}_J\hat{\mathbf{S}}_J\hat{\mathbf{V}}_J.
\end{equation}
%
Let $\hat{\sigma}_{J,1},\ldots,\hat{\sigma}_{J,\min_{k}\hat{r}_k}$ be the entries of the diagonal matrix $\hat{\mathbf{S}}_J$. The selection of $\hat{r}_k$'s is discussed in Section \ref{ssse:initial_rank}.


Secondly, the basis of the joint structure is determined by selecting the number of singular values $\hat{\sigma}_{J,i}$. The first criterion involves the distribution of the Wedin bound: Randomly permute all columns of $\hat{\mathbf{U}}_{k}$ without replacement and denote the outcome as $\tilde{\mathbf{U}}_{k}$. Then, compute $\|\mathbf{X}_k'\tilde{\mathbf{U}}_{k}\|_2$ as an approximation of $\|\mathbf{E}_k'\tilde{\mathbf{U}}_{k}\|_2$ in \eqref{e:wedin_bdd}. Similarly, approximate $\|\mathbf{E}_k\tilde{\mathbf{V}}_{k}\|_2$ by randomly sampling columns of $\hat{\mathbf{V}}_{k}$ without replacement. Repeat this sampling procedure $L$ times. Then, one can obtain the empirical distribution of the quantities
%
\begin{equation*}
    2K - \sum_{k=1}^{2K} \left(\frac{\max\{ \|\mathbf{X}_k\tilde{\mathbf{V}}_k^{(\ell)}\|_2, \|\mathbf{X}_k'\tilde{\mathbf{U}}_k^{(\ell)}\|_2 \}}{\sigma_{\min}(\hat{\mathbf{Y}}_k)} \wedge 1 \right)^2,\quad \ell=1,\ldots,L.
\end{equation*}
%
Following Lemma 3 in \cite{feng:2018}, utilize the 95 percentile of the $L$ quantities as the cutoff value derived from the Wedin bound. The random direction bound as the second criterion is constructed as follows. Let $\vec{\mathbf{V}}_{k}$ be the matrix generated by substituting $\hat{\mathbf{V}}_k$'s in \eqref{e:mat_J} with independently generated orthogonal matrices of the same sizes. Then, sample the maximum eigenvalue of $\begin{bmatrix} \vec{\mathbf{V}}_{1}^{(m)'} & \ldots & \vec{\mathbf{V}}_{2K}^{(m)'} \end{bmatrix}'$, $m=1,\ldots,M$. In practice, we take $M=L$. Subsequently, use the 95 percentile of the $M$ largest singular values obtained from multiple replications as the cutoff value. The final determination of the joint rank $\hat{r}_J$ is achieved by counting the number of singular values $\hat{\sigma}_{J,i}$ that surpass both of these cutoffs.
\end{comment}

Finally, define the matrix $\bar{\mathbf{V}}_J$ by retaining the $\hat{r}_J$ columns of $\hat{\mathbf{V}}_J$. Let $\bar{\mathbf{P}}_{J} = \bar{\mathbf{V}}_J\bar{\mathbf{V}}_J'$ be the projection matrix onto the estimated joint space. The estimators of $\bar{\mathbf{X}}_k$, the joint structure for each $k$, are obtained through $\mathbf{X}_k\bar{\mathbf{P}}_{J}$. Similarly, the estimation of the individual structure for each $k$, denoted as $\tilde{\mathbf{X}}_k$, is estimated by $\mathbf{X}_k(\hat{\mathbf{V}}_k\hat{\mathbf{V}}_k'-\bar{\mathbf{P}}_{J})$. We regard them as the estimators of group individual structures $\tilde{\mathbf{X}}_k$ in GRIDY model, but taking into account the following remark.


\begin{remark}\label{rem:rank}
We note that the joint structures $\mathbf{X}_k\bar{\mathbf{P}}_{J} = (\mathbf{X}_k\bar{\mathbf{V}}_J)\bar{\mathbf{V}}_J'$ are of the factor model form $\bar{\mathbf{F}}_k\bar{\mathbf{B}}'$ as in \eqref{e:augmented_block2}. This is not the case for the group individual structures $\mathbf{X}_k(\hat{\mathbf{V}}_k\hat{\mathbf{V}}_k' - \bar{\mathbf{P}}_J)$ where $k=1,\ldots,K$ for group 1 and $k=K+1,\ldots,2K$ for group 2. We nevertheless refer to the latter as group individual structures, and under our model, fit factor models of the form $\tilde{\mathbf{F}}_k\tilde{\mathbf{B}}_g'$ as in  \eqref{e:augmented_block2} through simultaneous component analysis (SCA) in Section \ref{ssse:estimation} below. The factor model is also refitted through SCA for the joint structures. We also note that in our simulation study, we consider the procedure where the joint and group individual structures are identified through double application of SCA, but this approach performs considerably worse than the suggested method based on AJIVE. It should also be stressed that only AJIVE can currently produce reasonable estimates of the various ranks needed for our model. For further discussion, see Section \ref{se:illustrative}.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsubsection{Initial rank selection}
\label{ssse:initial_rank}


As seen in \eqref{e:mat_J}, AJIVE requires an initial guess $\hat{r}_k$ of the rank $r_k$. We acknowledge that as the number of subjects increases, deciding on the initial rank using graphical tools like a scree plot becomes more challenging. Moreover, even when employing cross-validation-based rank selection algorithms \cite[e.g.,][]{bro:2003}, our model encompasses several distinct structures within the joint and two-group individual structures, with all structures being interrelated. Furthermore, our application involves a large number of subjects, making the implementation of cross-validation complex.


Instead, we opt for the rotational bootstrap algorithm developed by \cite{prothero:2022}. While this algorithm was originally designed to identify partially shared structures, it can also be used to explore the best reduced-rank approximation of the true block signals. This algorithm is also rooted in PAA, and involves bootstrap to construct an upper bound for the largest principal angle between the subspace of the unknown signal block and the low-rank approximation of the observation block. We apply the algorithm to each block observation and simplify notation by omitting the subscript $k$ for the subject number.


The first step of the algorithm is to compute shrunk singular values and imputed noise matrices. Suppose that a data block $\mathbf{X}=\mathbf{Y}+\mathbf{E}$ is given. Consider the SVD $\hat{\mathbf{X}} = \hat{\mathbf{U}}\hat{\mathbf{S}}\hat{\mathbf{V}}'$, where $\hat{\mathbf{U}},\hat{\mathbf{V}}$ consist of orthonormal columns $\hat{U}_i$ and $\hat{V}_i$, and diagonal matrix $\hat{\mathbf{S}}$ with singular values $\sigma_1 \geq \ldots \geq \sigma_{d\wedge T}$. Then, the thresholded singular values are $\check{\sigma}_i := \check{\kappa}h^{*}(\sigma_{i}/\check{\kappa})$, $i=1,\ldots,d \wedge T$, where $\check{\kappa}=\frac{\sigma_{\textrm{med}}}{\sqrt{\textrm{MP}(\beta)_{0.5}}}$ for $\sigma_{\textrm{med}} = \textrm{med}(\sigma_1,\ldots,\sigma_{d\wedge T})$, $\textrm{MP}(\beta)_{q}$ is the 100$q$ percentile (hence, the median for $\check{\kappa}$) of the Marchenko-Pastur distribution with the so-called aspect ratio $\beta=\frac{d \wedge T}{d \vee T}$ \cite[e.g.,][]{gavish:2014}, and the thresholding function used is 
%
\begin{equation*}
    h^{*}(a) = \left\{\begin{array}{ll}
      \frac{1}{\sqrt{2}}\sqrt{a^2-\beta-1+\sqrt{(a^2-\beta-1)^2-4\beta}} ,  & \textrm{if }a\geq 1+\sqrt{\beta}, \\
      0,   & \textrm{if } a<1+\sqrt{\beta}.
    \end{array}\right.
\end{equation*}
%
Naturally, the maximal allowable rank $\check{r}$ is determined by the number of non-zero $\check{\sigma}_i$'s. Thus, $\check{\mathbf{S}}_{\check{r}}$ represents a $\check{r}$-dimensional diagonal matrix with entries $\check{\sigma}_r$ for $r=1,\ldots,\check{r}$. The estimate $\check{\mathbf{E}}$ of the noise matrix $\mathbf{E}$ for bootstrapping is computed by 
%
\begin{equation*}
    \check{\mathbf{E}} 
    = \sum_{i=1}^{\check{r}}\check{\kappa}\textrm{MP}(\beta)_{u_i}\hat{U}_{i}\hat{V}_{i}' + \sum_{i=\check{r}+1}^{d\wedge T}\sigma_i\hat{U}_{i}\hat{V}_{i}',
\end{equation*}
%
where $u_{1},\ldots,u_{\check{r}}$ are i.i.d. uniform random variables on $(0,1)$. This estimate is thought of as imputed noise, which is justified in Appendix C in \cite{prothero:2022}. 


The rest of the algorithm consists of two nested loops. In the outer loop, obtain $T \times \check{r}$ and $d \times \check{r}$ independently generated orthonormal matrices, $\bar{\mathbf{U}}_{\check{r}}^{(\ell)}$ and $\bar{\mathbf{V}}_{\check{r}}^{(\ell)}$, $\ell=1,\ldots,L$, and construct $\bar{\mathbf{X}}^{(\ell)} = \bar{\mathbf{U}}_{\check{r}}^{(\ell)}\check{\mathbf{S}}_{\check{r}}\bar{\mathbf{V}}_{\check{r}}^{(\ell)'}+\check{\mathbf{E}}$. Then, compute matrices with orthonormal columns associated with $r$ largest singular values from the SVD of $\bar{\mathbf{X}}^{(\ell)}$, denoted as $\hat{\mathbf{U}}_{r}^{(\ell)}$ and $\hat{\mathbf{V}}_{r}^{(\ell)}$, $r=1,\ldots,\check{r}$. In the inner loop, compute the smallest singular values $\sigma_{\min}(\cdot)$ from SVDs of $\bar{\mathbf{U}}_{\check{r}}^{(\ell)'}\hat{\mathbf{U}}_{r}^{(\ell)}$ and $\bar{\mathbf{V}}_{\check{r}}^{(\ell)'}\hat{\mathbf{V}}_{r}^{(\ell)}$ for $r=1,\ldots,\check{r}$ and then, convert them into the largest principal angles $\theta_{U,r}^{(\ell)}$ and $\theta_{V,r}^{(\ell)}$ between $\bar{\mathbf{U}}_{\check{r}}^{(\ell)}$ and $\hat{\mathbf{U}}_{r}^{(\ell)}$, and $\bar{\mathbf{V}}_{\check{r}}^{(\ell)}$ and $\hat{\mathbf{V}}_{r}^{(\ell)}$ by $\arccos(\sigma_{\min}(\bar{\mathbf{U}}_{\check{r}}^{(\ell)'}\hat{\mathbf{U}}_{r}^{(\ell)}) )$ and $\arccos(\sigma_{\min}(\bar{\mathbf{V}}_{\check{r}}^{(\ell)'}\hat{\mathbf{V}}_{r}^{(\ell)}) )$, respectively. This leads to $L \times \check{r}$ computed principal angles $\theta_{U,r}^{(\ell)}$ and $\theta_{V,r}^{(\ell)}$ for each of the row and column spaces. After sorting those angles for each $r$, select the final initial rank estimate $\hat{r}$ by 
%
\begin{equation*}
    \hat{r} 
    = \min \left( 
    \sum_{r=1}^{\check{r}}
    1_{\{ [ \textrm{95 percentile of } \theta_{U,r}^{(\ell)}, \ \ell=1,\ldots,L] \ < \ \xi\theta_{0,U} \} } , \ 
    \sum_{r=1}^{\check{r}} 1_{\{ [ \textrm{95 percentile of } \theta_{V,r}^{(\ell)}, \ \ell=1,\ldots,L] \ < \ \xi\theta_{0,V} \} } 
    \right).
\end{equation*}
%
Here, $\theta_{0,U}$ and $\theta_{0,V}$ represent the random direction angle bounds for the row and column subspaces and are computed in parallel in the inner loop of the bootstrapping algorithm. Similarly to AJIVE, the candidates of the random direction bounds are calculated as the largest principal angles between $\vec{\mathbf{U}}_{\check{r}}^{(m)}$ and $\vec{\mathbf{U}}_{r}^{(m)}$, and between $\vec{\mathbf{V}}_{\check{r}}^{(m)}$ and $\vec{\mathbf{V}}_{r}^{(m)}$, where $\vec{\mathbf{U}}_{r}^{(m)}$ and $\vec{\mathbf{V}}_{r}^{(m)}$ are independently generated orthonormal matrices of the sizes $T \times r$ and $d \times r$, $r=1,\ldots,\check{r}$, respectively. The algorithm repeats this computation $m=1,\ldots,M(=L)$ times and selects the 5 percentile of all replications. The tuning parameter $\xi$, when multiplied by the bounds, controls the maximum allowed principal angles. An increase in the value of $\xi$ leads to larger angles falling within the criterion, typically resulting in higher estimated initial ranks. The method for fine-tuning $\xi$ may vary depending on the specific case and remains somewhat unclear. However, it is generally recommended to keep it within the range of $(0, 0.5]$. In our empirical results in Section \ref{sse:result_bootstrap} and the application illustrated in Section \ref{se:data_app}, we set $\xi = 0.5$, which has shown reasonable performance. We refer to Algorithm 1 in Section 2.1.3 in \cite{prothero:2022} for the remaining details.


In practice, the rotational bootstrap is applied to the block observation of each subject. Then, the initial rank is finalized by a majority vote, as a mode of $2K$ number of $\hat{r}$'s. We use this estimate for selecting the rank of the joint factors $\hat{r}_J$ produced by AJIVE. Naturally, this leads to $\hat{r}_G=\hat{r}-\hat{r}_J$. Potentially, one can also take different initial ranks for each group, or even different initial ranks for each subject, depending on the context. As noted in Section \ref{sse:segmentation}, while AJIVE does not require identical initial ranks across subjects and neither does the rotational bootstrap, we assume that the initial ranks are identical at least within each group. In order to apply the factor structure described below for the rest of the procedure, the ranks of the joint and the two group blocks should be specified. Furthermore, to impose determinacy conditions across subjects which are described below, it is convenient for the ranks within each group to be identical in that SCA described below can be employed for both joint and group individual structures. The identification of factor structure relies on the performance of the initial rank selection from the rotational bootstrap and the joint rank selection from AJIVE. We will assess the performance of the initial rank selection in Section \ref{sse:result_bootstrap}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsubsection{Identifiability}
\label{ssse:identifiability}


Note that the model \eqref{e:augmented_factor_1}--\eqref{e:augmented_factor_3} should ideally satisfy two kinds of identifiability conditions. The first condition is to ensure that the factor model in \eqref{e:augmented_factor_1} can be uniquely decomposed into the joint components $\bar{\mathbf{F}}_{k}\bar{\mathbf{B}}'$ and the group individual components $\tilde{\mathbf{F}}_{k}\tilde{\mathbf{B}}_g^{'}$ as in \eqref{e:augmented_factor_2}. To distinguish this condition from the next one, we call it an identifiability condition. Another important condition is to ensure that the loadings matrices $\bar{\mathbf{B}},\tilde{\mathbf{B}}_1,\tilde{\mathbf{B}}_{2}$ and the factor series $\bar{F}_{t}^{(k)},\tilde{F}_{t}^{(k)}$ can also be identified. We will see that unlike the first condition, these loadings matrices and factor scores are not identifiable until additional assumptions are imposed. We shall refer to determinacy as the condition for identifying the loadings and the factor series in the components of our decomposition. This condition will be discussed in Section \ref{ssse:determinacy}.


The identification of the first two block structures in \eqref{e:multi_block_AJIVE} can be obtained by checking suitable identifiability conditions of AJIVE. The following lemma adapted from \cite{feng:2018} to our setting shows that under certain conditions, the uniqueness of the decomposed blocks can be achieved. To discuss the condition below, we define a subspace spanned by joint loadings matrix as 
%
\begin{equation*}
    \textrm{row}(\bar{\mathbf{X}}_{1}) = \ldots = \textrm{row}(\bar{\mathbf{X}}_{2K}) =: \textrm{row}(\bar{\mathbf{X}}).
\end{equation*}
%


\begin{lem}[\cite{feng:2018}]\label{lem:feng1}
Given a set $\{\mathbf{Y}_1,\ldots,\mathbf{Y}_{2K}\}$ of matrices, there are unique sets $\{\bar{\mathbf{X}}_1,\ldots,\bar{\mathbf{X}}_{2K}\}$ and $\{\tilde{\mathbf{X}}_1,\ldots,\tilde{\mathbf{X}}_{2K}\}$ of matrices so that
\begin{enumerate}
    \item $\mathbf{Y}_k = \bar{\mathbf{X}}_k + \tilde{\mathbf{X}}_k$ for all $k=1,\ldots,2K$.
    
    \item $\mathrm{row}(\bar{\mathbf{X}}_k)=\mathrm{row}(\bar{\mathbf{X}}) \subset \mathrm{row}(\mathbf{Y}_k)$ for all $k=1,\ldots,2K$.
    
    \item $\mathrm{row}(\bar{\mathbf{X}}) \perp \mathrm{row}(\tilde{\mathbf{X}}_k)$ for all $k=1,\ldots,2K$.
    
    \item $\bigcap_{k=1}^{2K} \mathrm{row}(\tilde{\mathbf{X}}_k)=\{\mathbf{0}\}$.
\end{enumerate}
\end{lem}

Condition 1 corresponds to our setting of interest. Note that each row space $\mathrm{row}(\bar{\mathbf{X}}_k)$ is defined by the loadings, specifically by the linear combinations of entries of loading matrices multiplied by the factor scores. Also, note that the row subspaces of the joint components are spanned by the same loadings matrix, that is, $\textrm{row}(\bar{\mathbf{X}}_{k}) = \textrm{row}(\bar{\mathbf{F}}_{k}\bar{\mathbf{B}}') = \textrm{row}(\bar{\mathbf{B}}') = \textrm{row}(\bar{\mathbf{X}})$ since the subspace is represented by a linear combination of basis columns in the loadings matrix, where the coefficients are the factor scores, which rarely have the same values across different rows. Hence, condition 2 is satisfied assuming $\bar{\mathbf{X}}_k = \bar{\mathbf{F}}_k\bar{\mathbf{B}}'$ in our setting. Consider next condition 3. It assumes that for fixed $k$, $\bar{\mathbf{X}}_k \tilde{\mathbf{X}}_k^{'} = \bar{\mathbf{F}}_k\bar{\mathbf{B}}'\tilde{\mathbf{B}}_g\tilde{\mathbf{F}}_k' = \mathbf{0}_{T_k \times T_k}$, $g=1,2$, and is implied by $\bar{\mathbf{B}}'\tilde{\mathbf{B}}_1 = \bar{\mathbf{B}}'\tilde{\mathbf{B}}_2 = \mathbf{0}_{r_J \times r_G}$. This means that all columns in both of group loadings matrices are perpendicular to the corresponding columns of joint loadings matrix. The easiest way to achieve this condition is when the non-zero row entries of joint loadings matrix $\bar{\mathbf{B}}$ do not overlap with any other non-zero row entries of both group loadings matrices $\tilde{\mathbf{B}}_g$, $g=1,2$. Interestingly, this condition implies that the rows can be shared between group loadings matrices. Lastly, condition 4 states that there are no rows shared by all group loadings matrices. This is implied by our model where 
$\mathrm{row}(\tilde{\mathbf{B}}_1')\cap\mathrm{row}(\tilde{\mathbf{B}}_2')=\{0\}$. That is, the blocks span different subspaces depending on groups. We thus identified natural condition on the elements of our model $(\bar{\mathbf{B}},\tilde{\mathbf{B}}_1,\tilde{\mathbf{B}}_{2})$ so that its components are the unique components of the decomposition given in Lemma \ref{lem:feng1}.


We also note that the identifiability condition can be summarized as 
%
\begin{equation*}
    \mathbf{B}_g'\mathbf{B}_g 
    = \begin{pmatrix}
    \bar{\mathbf{B}}'\bar{\mathbf{B}} & \mathbf{0}_{r_J \times r_G } \\
    \mathbf{0}_{r_G \times r_J } & \tilde{\mathbf{B}}_g'\tilde{\mathbf{B}}_g \\
    \end{pmatrix}, \quad g=1,2.
\end{equation*}
%
As we will discuss in Section \ref{sse:reconstruction}, consideration of conditions on loadings matrices is not important to obtain the determinacy of factor series.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Reconstruction of factor series}
\label{sse:reconstruction}


After the segmentation using AJIVE, the remaining task is to reconstruct the factor structure in \eqref{e:augmented_factor_2}. A typical way to ensure the determinacy of the factor series up to the transformation with the sign changes of the columns is to assume that the factor covariance is an identity matrix \cite[e.g.,][]{lam:2011,bai:2013}. For fixed $k$, by assuming we have only $k$th subject, the condition implies that the variances within the joint factor series and the group individual factor series should be identity matrices, 
%
\begin{equation}\label{e:orthogonality_factor} 
    \mathbb{E}\bar{F}_{t}^{(k)}\bar{F}_{t}^{(k)'} 
    = \mathbf{I}_{r_{J}}, \quad 
    \mathbb{E}\tilde{F}_{t}^{(k)}\tilde{F}_{t}^{(k)'} 
    = \mathbf{I}_{r_{G}}.
\end{equation}
%
The same determinacy condition has been applied for multiple subjects \cite[e.g.,][]{fan:2018} as well. This condition, however, is quite stringent; it does not allow for the joint or group individual factor series to be correlated and forces the covariance of factors to be identical across subjects, which limits the degree of heterogeneity. One alternative suggested by \cite{bai:2015} is to make the elements of the square matrix on the top of the loadings matrix be identity, that is, 
%
\begin{equation}\label{e:pca_dynamic}
    \bar{\mathbf{B}} 
    = \begin{pmatrix}
        \mathbf{I}_{r_{J}} \\ \bullet
    \end{pmatrix}, \quad
    \tilde{\mathbf{B}}_g 
    = \begin{pmatrix}
        \mathbf{I}_{r_{G}} \\ \bullet
    \end{pmatrix},
\end{equation}
%
where $\bullet$ stands for the remaining entries of each loadings matrix. However, it may not be possible to find an invertible $r_{J} \times r_{J}$ ($r_{G} \times r_{G}$, respectively) matrix that makes the top of the loadings matrix the identity matrix.


To relax the stringent restrictions \eqref{e:orthogonality_factor}, we adopt the SCA introduced by \cite{timmerman:2003}. SCA was designed for multivariate time series from multiple subjects to study both intra- and inter-individual differences. We will demonstrate that the factor dynamics can also be reconstructed using this determinacy condition.


The idea of SCA is not different from PCA but various restrictions on the component factor scores may be imposed. There are four types of covariance structures suggested in \cite{timmerman:2003}. However, we focus on  SCA with PARAFAC2 constraints (SCA-PF2), inspired by tensor decomposition PARAFAC \citep[e.g.,][]{kiers:1999}. Additionally, we consider a more restrictive but useful covariance structure, namely SCA with INDSCAL constraints (SCA-IND). To compare the various covariance structures of the factors, consider the joint factor series. Then, the two covariance structures mentioned above are
%
\begin{eqnarray}
    \mathbb{E}\bar{F}_{t}^{(k)}\bar{F}_{t}^{(k)'} 
    &=:& \mathbb{E}\left(\bar{\mathbf{C}}_k\bar{A}_{t}^{(k)}\right)\left(\bar{\mathbf{C}}_k\bar{A}_{t}^{(k)}\right)^{'}  
    = \bar{\mathbf{C}}_k \bar{\boldsymbol{\Phi}} \bar{\mathbf{C}}_k, \quad \textrm{(SCA-PF2)} \label{e:SCA-PF2} \\
    \mathbb{E}\bar{F}_{t}^{(k)}\bar{F}_{t}^{(k)'} 
    &=:& \mathbb{E}\left(\bar{\mathbf{C}}_k\bar{A}_{t}^{(k)}\right)\left(\bar{\mathbf{C}}_k\bar{A}_{t}^{(k)}\right)^{'}  
    = \bar{\mathbf{C}}_k^2, \quad \textrm{(SCA-IND)} \label{e:SCA-IND}
\end{eqnarray}
%
where $\bar{\boldsymbol{\Phi}}=(\bar{\phi}_{i,j})$ is a $r_J \times r_J$ positive definite matrix and $\bar{\mathbf{C}}_k=(\bar{C}_{i,j}^{(k)})$ are diagonal matrices. Note that to identify matrices within their multiplications, we let the diagonal entries of $\bar{\boldsymbol{\Phi}}$ be unit-scaled so that they become correlation matrices of the scaled joint factors $\bar{A}_{t}^{(k)}$ and the absolute values of $\bar{\mathbf{C}}_k$ represent the standard deviations of the joint factors $\bar{F}_{t}^{(k)}$. Note also that \eqref{e:SCA-PF2} allows for correlations across joint factors while \eqref{e:SCA-IND} does not. Lastly, one can think of \eqref{e:SCA-IND} as a special case of \eqref{e:SCA-PF2} by letting $\bar{\boldsymbol{\Phi}}=\mathbf{I}_{r_{J}}$. Obviously, these conditions do not require any restriction on loadings matrices. The same assumption on the covariance of group individual factors can be applied, which in turn produces 
%
\begin{eqnarray*}
    \mathbb{E}\tilde{F}_{t}^{(k)}\tilde{F}_{t}^{(k)'} 
    &=:& 
    \tilde{\mathbf{C}}_k \tilde{\boldsymbol{\Phi}} \tilde{\mathbf{C}}_k, \\
    \mathbb{E}\tilde{F}_{t}^{(k)}\tilde{F}_{t}^{(k)'} 
    &=:&  
    \tilde{\mathbf{C}}_k^2,
\end{eqnarray*}
%
where $\tilde{\boldsymbol{\Phi}}$ is $r_G \times r_G$ positive definite and $\tilde{\mathbf{C}}_k=(\tilde{C}_{i,j}^{(k)})$ are diagonal as similarly defined as above. 


\begin{remark}
While the conditions as in \eqref{e:SCA-PF2}--\eqref{e:SCA-IND} enable the model to have more complex covariance structures, one can think that with more restrictions, as \eqref{e:orthogonality_factor} in an extreme case, the model becomes more parsimonious. In the other direction, the most flexible covariance structure is known as SCA with Invariant Pattern (SCA-P) \cite[see][]{timmerman:2003}. To our best knowledge, there are no tests to decide on particular structures. As long as we deal with the latent factor series and their characteristics are not known in advance, it is beneficial to allow the model to be as flexible as possible. Therefore, we do not prefer the most restrictive condition \eqref{e:orthogonality_factor}. At the same time, our hope is to incorporate the heterogeneity of subjects without losing identifiability. However, the SCA-P condition is known not to guarantee such uniqueness. Furthermore, while the condition \eqref{e:pca_dynamic} does not impose any restriction on the covariance, it loses some spatial information as mentioned above. Therefore, we prefer the SCA-PF2 condition and as a special case, the SCA-IND condition.
\end{remark}


\begin{remark}
Among studies related to SCA, using a rotational matrix to make targeted factor scores \cite[e.g.,][]{schouteden:2013,schouteden:2014} or particular structural loadings matrices \cite[e.g.,][]{helwig:2017} have been tried. However, the former requires a known target structure of factor scores in advance or requires additional steps to determine the target. It also carries relatively large computational costs since estimation of factor scores and loadings are involved in finding rotational transformation. Furthermore, the adopted determinacy condition in the approach is SCA-P, which does not guarantee the identification of factor scores. The latter also demands the pre-specified structure of loadings matrices in advance. When we do not know the relationship between factors themselves or desired loadings matrices, this blocking approach has limitations.
\end{remark}


We estimate loadings matrices $\bar{\mathbf{B}},\tilde{\mathbf{B}}_1,\tilde{\mathbf{B}}_2$ and factor series $\{\bar{\mathbf{F}}_k,\tilde{\mathbf{F}}_k\}_{k=1,\ldots,2K}$ through a direct fitting algorithm for PARAFAC2 model, which will be described in Section \ref{ssse:estimation}. Then, we refit the latent factors through subject-wise linear regressions:
%
\begin{equation}\label{e:refitting}
    \begin{pmatrix} \hat{\bar{\mathbf{F}}}_k & \hat{\tilde{\mathbf{F}}}_k \end{pmatrix} 
    = \argmin_{\mathbf{F}_k=(\bar{\mathbf{F}}_k\ \tilde{\mathbf{F}}_k)} 
    \| \mathbf{X}_k 
    - \bar{\mathbf{F}}_k\bar{\mathbf{B}}' - \tilde{\mathbf{F}}_k\tilde{\mathbf{B}}_g' \|_F^2,
\end{equation}
%
for $k=1,\ldots,2K$ and $g=1,2$ accordingly. Although there are alternative ways to refit the model from SCA, handling $\bar{\mathbf{F}}_k$ and $\tilde{\mathbf{F}}_k$ together has an advantage over treating them separately. First, \eqref{e:refitting} can be solved explicitly. Also, since the VAR in the factor models is latent, other refitting methods may be difficult to apply. Naturally, the squared error of the fitting becomes smaller after refitting the factor series by \eqref{e:refitting}. Then, we compute the empirical covariance matrix for the joint and group individual factor series, respectively. Finally, by the Yule-Walker equations \cite[e.g., Chapter 11 in][]{brockwell:2009}, we estimate the parameters in the model \eqref{e:filter_equation}, including the covariance matrix of noise $\bar{\boldsymbol{\Sigma}}_{\eta,k}$. We repeat this process for $k=1,\ldots,2K$ subjects. 

\begin{remark}
By following a two-step estimation procedure \cite[e.g.,][]{doz:2011}, one can also use Kalman recursions to take into account the heteroscedasticity of noise and dynamics of the factor series. Although it may be useful to take the second step, we exclude the usage of Kalman recursions. First, we do not assume any particular structure on the covariance of noise. The cost of the recursive computations is also substantial because observations of all subjects are required. Lastly, our model is assumed to have the joint and group loadings matrices, which are identical across (groups of) subjects, and their estimates are already fixed. Their values will change while running the algorithm.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsubsection{Determinacy of factors}
\label{ssse:determinacy}


Determinacy of factors in SCA was studied by several researchers. \cite{harshman:1996} proved the uniqueness of a general form of parameters of multi-view data which led to the rotational determinacy of SCA-PF2 models. In particular, when the rank $r$ of factors exceeds 2, the estimated components multiplied with some scaling and permutation matrices described below can be determined if the number of subjects is greater than $r(r+1)(r+2)(r+3)/24$ for rank $r$. So, although the inequality seems to require a quite larger number of subjects to obtain the determinacy, the computational experiments by  \cite{ten:1996} suggested that the determinacy can be achieved even for larger ranks if the number of subjects exceeds 4. However, a rigorous proof for this or the other minimum number of subjects remains to be found.


The proved determinacy for SCA-PF2 is as follows. If the SCA-PF2 model can be expressed through two sets $\bar{\mathbf{B}},\bar{A}_t^{(k)},\bar{\mathbf{C}}_k$ and $\doublebar{\mathbf{B}},\doublebar{A}_t^{(k)},\doublebar{\mathbf{C}}_k$ of joint loadings matrices, scaled joint factor series, and the diagonal matrices, then 
%
\begin{equation}\label{e:rotational_determinacy_joint_1}
    \doublebar{\mathbf{B}} = \bar{\mathbf{B}}\bar{\mathbf{W}}\Delta_{\bar{\mathbf{B}}}, \quad 
    \doublebar{A}_t^{(k)} = z_k\Delta_{\bar{\mathbf{A}}}\bar{\mathbf{W}}'\bar{A}_t^{(k)},\quad 
    \doublebar{\mathbf{C}}_{k} = z_k\bar{\mathbf{W}}'\bar{\mathbf{C}}_k\bar{\mathbf{W}}\Delta_{\bar{\mathbf{C}}},
\end{equation}
%
where $\bar{\mathbf{W}}$ is a permutation matrix, $z_k=-1$ or $1$, and $\Delta_{\bar{\mathbf{A}}},\Delta_{\bar{\mathbf{B}}},\Delta_{\bar{\mathbf{C}}}$ are $r_{J} \times r_{J}$ non-singular diagonal scaling matrices satisfying $\Delta_{\bar{\mathbf{A}}}\Delta_{\bar{\mathbf{B}}}\Delta_{\bar{\mathbf{C}}}=\mathbf{I}_{r_{J}}$. By convention, one can fix $z_k=1$ for all $k$ to overcome the sign indeterminacy \cite[e.g.,][]{helwig:2013}. Thus, the joint factors can be related as
%
\begin{equation}\label{e:rotational_determinacy_joint_2}
    \doublebar{F}_t^{(k)} 
    := \doublebar{\mathbf{C}}_{k}\doublebar{A}_t^{(k)},
\end{equation}
%
and the correlation matrix of the scaled joint factors is of the form $\doublebar{\boldsymbol{\Phi}} = \Delta_{\bar{\mathbf{A}}}\bar{\mathbf{W}}' \bar{\boldsymbol{\Phi}} \bar{\mathbf{W}} \Delta_{\bar{\mathbf{A}}}$ so that 
%
\begin{equation}\label{e:rotational_determinacy_joint_3}
    \mathbb{E}\doublebar{F}_t^{(k)}\doublebar{F}_t^{(k)'}
    = \doublebar{\mathbf{C}}_{k}\doublebar{\boldsymbol{\Phi}}\doublebar{\mathbf{C}}_{k} 
    =  \underbrace{\bar{\mathbf{W}}'\bar{\mathbf{C}}_k\bar{\mathbf{W}}}_{=:\bar{\mathbf{C}}_k^{\bar{\mathbf{W}}}} \Delta_{\bar{\mathbf{B}}}^{-1} \underbrace{\bar{\mathbf{W}}'\bar{\boldsymbol{\Phi}}\bar{\mathbf{W}}}_{=:\bar{\boldsymbol{\Phi}}^{\bar{\mathbf{W}}}} \Delta_{\bar{\mathbf{B}}}^{-1} \bar{\mathbf{W}}'\bar{\mathbf{C}}_k\bar{\mathbf{W}} =\bar{\mathbf{C}}_k^{\bar{\mathbf{W}}} \Delta_{\bar{\mathbf{B}}}^{-1} \bar{\boldsymbol{\Phi}}^{\bar{\mathbf{W}}} \Delta_{\bar{\mathbf{B}}}^{-1} \bar{\mathbf{C}}_k^{\bar{\mathbf{W}}},
\end{equation}
%
%https://math.stackexchange.com/questions/197243/properties-of-diagonal-and-permutation-matrices
where $\bar{\mathbf{C}}_k^{\bar{\mathbf{W}}}$ is again diagonal with permutated entries. That is, modulo the scaling matrix $\Delta_{\bar{\mathbf{B}}}$, one has the same covariance structure of the factor series up to their rearrangement. Note that from \eqref{e:rotational_determinacy_joint_1}--\eqref{e:rotational_determinacy_joint_3}, the determinacy can be achieved with the multiplication of the scalers and permutations. In the sense that although the model still has an issue of scaling, the identification up to the change of rows and columns of matrices by imposing SCA-PF2 structure is remarkable.


Similar results hold for group individual factor series. For each group $g=1,2$, the group loadings matrices $\tilde{\mathbf{B}}_g,\doubletilde{\mathbf{B}}_g$, scaled group individual factors $\tilde{A}_{t}^{(k)},\doubletilde{A}_{t}^{(k)}$, and the diagonal matrices $\tilde{\mathbf{C}}_k,\doubletilde{\mathbf{C}}_k$ are related as
%
\begin{equation*}
    \doubletilde{\mathbf{B}}_g = \tilde{\mathbf{B}}\tilde{\mathbf{W}}_g\Delta_{\tilde{\mathbf{B}}_g}, \quad 
    \doubletilde{A}_t^{(k)} = z_{g,k}\Delta_{\tilde{\mathbf{A}}_g}\tilde{\mathbf{W}}_g'\tilde{A}_t^{(k)},\quad 
    \doubletilde{\mathbf{C}}_{k} = z_{g,k}\tilde{\mathbf{W}}_g'\tilde{\mathbf{C}}_k\tilde{\mathbf{W}}_g\Delta_{\tilde{\mathbf{C}}_g}.
\end{equation*}
%
Here, $\tilde{\mathbf{W}}_g$ is a permutation matrix of $g$th group and $z_{g,k}=-1$ or $1$, so we take $z_{g,k}=1$ for all cases by convention. Obviously, the index $k$ goes from 1 to $K$ when $g=1$, and $k=K+1,\ldots,2K$ are used for $g=2$. Finally, $\Delta_{\tilde{\mathbf{A}}_g},\Delta_{\tilde{\mathbf{B}}_g},\Delta_{\tilde{\mathbf{C}}_g}$ are $r_{G} \times r_{G}$ non-singular diagonal matrices such that $\Delta_{\tilde{\mathbf{A}}_g}\Delta_{\tilde{\mathbf{B}}_g}\Delta_{\tilde{\mathbf{C}}_g}=\mathbf{I}_{r_{G}}$. Accordingly, the group individual factors are related as
%
\begin{equation*}
    \doubletilde{F}_t^{(k)} 
    = \doubletilde{\mathbf{C}}_{k}\doubletilde{A}_t^{(k)},
\end{equation*}
%
for corresponding $g$ and $k$ as above, so that the covariance of each group individual factor is 
%
\begin{equation*}
    \mathbb{E}\doubletilde{F}_t^{(k)}\doubletilde{F}_t^{(k)'}
    = \doubletilde{\mathbf{C}}_{k}\doubletilde{\boldsymbol{\Phi}}\doubletilde{\mathbf{C}}_{k} 
    =  \underbrace{\tilde{\mathbf{W}}_{g}'\tilde{\mathbf{C}}_k\tilde{\mathbf{W}_{g}}}_{:=\tilde{\mathbf{C}}_k^{\tilde{\mathbf{W}}_{g}}} \Delta_{\tilde{\mathbf{B}}}^{-1} \underbrace{\tilde{\mathbf{W}_{g}}'\tilde{\boldsymbol{\Phi}}\tilde{\mathbf{W}_{g}}}_{:=\tilde{\boldsymbol{\Phi}}^{\tilde{\mathbf{W}}_{g}}} \Delta_{\tilde{\mathbf{B}}}^{-1} \tilde{\mathbf{W}_{g}}'\tilde{\mathbf{C}}_k\tilde{\mathbf{W}_{g}} =\tilde{\mathbf{C}}_k^{\tilde{\mathbf{W}}_{g}} \Delta_{\tilde{\mathbf{B}}}^{-1} \tilde{\boldsymbol{\Phi}}^{\tilde{\mathbf{W}}_{g}} \Delta_{\tilde{\mathbf{B}}}^{-1} \tilde{\mathbf{C}}_k^{\tilde{\mathbf{W}}_{g}},
\end{equation*}
%
where $\doubletilde{\boldsymbol{\Phi}}_g := \Delta_{\tilde{\mathbf{A}}_g}\tilde{\mathbf{W}}_g' \tilde{\boldsymbol{\Phi}}_g \tilde{\mathbf{W}}_g \Delta_{\tilde{\mathbf{A}}_g}$, $g=1,2$. 

In Section \ref{se:illustrative} with simulation results, the determinacy conditions allow us to assess the estimation performance as follows. We start by permuting the columns of the estimated loadings matrices for each structure and determine the permutation orders by minimizing the mean squared errors to the true loadings. Subsequently, we rearrange the factor series in each structure based on these orders.


\begin{comment}
Consider the joint structure $\bar{\mathbf{B}}\bar{F}_{t}^{(k)}$ first. Assume that we follow the determinacy condition as \cite{doz:2011,doz:2012}. That is,
%
\begin{equation}
    \bar{\mathbf{B}}\bar{F}_{t}^{(k)} 
    = (\bar{\mathbf{B}} \Sigma_{\bar{\mathbf{F}},k}^{1/2})(\Sigma_{\bar{\mathbf{F}},k}^{-1/2} \bar{F}_{t}^{(k)}) 
    =: \doublebar{\mathbf{B}}_k\doublebar{F}_t^{(k)},
\end{equation}
%
where $\Sigma_{\bar{\mathbf{F}},k}$ is the covariance of the joint factor series of $k$th subject. Then one has a symmetric positive matrix $\Sigma_{\bar{\mathbf{F}},k}^{1/2}\bar{\mathbf{B}}'\bar{\mathbf{B}} \Sigma_{\bar{\mathbf{F}},k}^{1/2}$ so that
%
\begin{equation}
    \Sigma_{\bar{\mathbf{F}},k}^{1/2}\bar{\mathbf{B}}'\bar{\mathbf{B}} \Sigma_{\bar{\mathbf{F}},k}^{1/2} 
    =: \doublebar{\mathbf{B}}_k'\doublebar{\mathbf{B}}_k 
    = \doublebar{\mathbf{U}}_k\doublebar{\mathbf{S}}_k\doublebar{\mathbf{U}}_k',
\end{equation}
%
where $\doublebar{\mathbf{U}}_k$ is an orthogonal matrix and $\doublebar{\mathbf{S}}_k$ is diagonal. Hence, we can identify the estimates up to orthogonal transformation,
%
\begin{equation}\label{e:identifiability_transform_joint}
    \doublebar{\mathbf{B}}_k\doublebar{F}_t^{(k)} 
    =  \doublebar{\mathbf{B}}_k\doublebar{\mathbf{U}}_k\doublebar{\mathbf{U}}_k'\doublebar{F}_t^{(k)}
    =: \doublebar{\boldsymbol{\Gamma}}_k \doublebar{G}_{t}^{(k)},
\end{equation}
%
where $\mathbb{E}\doublebar{G}_{t}^{(k)}\doublebar{G}_{t}^{(k)'}=I_{r_J}$. In particular, if $\bar{\Delta}_k$ is a $r_{J} \times r_{J}$ diagonal matrix whose nonzero entries are $\pm 1$, then the $\doublebar{\mathbf{U}}_k,\doublebar{\boldsymbol{\Gamma}}_k$, and $\doublebar{G}_{t}^{(k)}$ in \eqref{e:identifiability_transform_joint} are replaced by $\doublebar{\mathbf{U}}_k\doublebar{\Delta}_k,\doublebar{\boldsymbol{\Gamma}}_k\doublebar{\Delta}_k$, and $\doublebar{\Delta}_k\doublebar{G}_{t}^{(k)}$, respectively. Note that such sign changes are minor in that it is inevitable and we can set the reference individual, say $k=1$, and fix the signs to make $\doublebar{\boldsymbol{\Gamma}}_k\doublebar{\Delta}_k$ to be identical to $\doublebar{\boldsymbol{\Gamma}}_k\doublebar{\Delta}_1$. Then the corresponding signs of the factor series are automatically determined. To fix the rotational transformation, on the other hand, we can choose the reference index, say $k=1$ again, and find the orthogonal matrix $\doublebar{\mathbf{Q}}_k$, $k=2,\ldots,2K$, such that $\doublebar{\boldsymbol{\Gamma}}_k\doublebar{\mathbf{Q}}_k = \doublebar{\boldsymbol{\Gamma}}_1$. 


Likewise, a similar argument can be applied to the group individual structure so that
%
\begin{equation}\label{e:identifiability_transform_individual}
    \doubletilde{\mathbf{B}}_g\doubletilde{F}_{t}^{(k)} = \doubletilde{\mathbf{B}}_g\doubletilde{\mathbf{U}}_{g,k}\doubletilde{\mathbf{U}}'_{g,k}\tilde{F}_{t}^{(k)} =: \doubletilde{\boldsymbol{\Gamma}}_{g,k}\doubletilde{G}_{t}^{{(g,k)}},
\end{equation}
%
where where $\mathbb{E}\doubletilde{G}_{t}^{{(g,k)}}\doubletilde{G}_{t}^{{(g,k)}'}=I_{r_G}$, $g=1$ for $k=1,\ldots,K$ and $g=2$ for $k=K+1,\ldots,2K$, with possible sign changes through $\doubletilde{\Delta}_k$, similarly defined as above. For this case, one can find the rotation matrix $\doubletilde{\mathbf{Q}}_{k}$ such that $\doubletilde{\boldsymbol{\Gamma}}_{1,k}\doubletilde{\mathbf{Q}}_k = \doubletilde{\boldsymbol{\Gamma}}_{1,1}$ for $k=1,\ldots,K$, and $\doubletilde{\boldsymbol{\Gamma}}_{2,k}\doubletilde{\mathbf{Q}}_k = \doubletilde{\boldsymbol{\Gamma}}_{2,1}$ for $k=K+1,\ldots,2K$, respectively. In Section \ref{sse:result_main}, we will illustrate the performance of the usual rotational determinacy and compare it with our model when there are potential correlations between factor series themselves in each structure.
\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Direct fitting algorithm}
\label{ssse:estimation}


For the algorithm of estimating the joint loadings matrix and joint factor series at the second stage of our procedure, we use the direct fitting algorithm developed by \cite{kiers:1999}. Direct fitting means that one estimates components of factor-type models in the least squares sense, by regressing responses on covariates. It is different from the indirect fitting. \cite{harshman:1970} suggested an indirect fitting algorithm through alternating least squares (ALS) as follows. To explain the latter procedure, we describe it on the joint structure. If we denote the covariance matrix of the joint component $\bar{\mathbf{X}}_k$ as $\bar{\boldsymbol{\Sigma}}_{k}$, the model can be estimated by minimizing
%
\begin{equation*}
    \min_{\bar{\mathbf{B}},\bar{\mathbf{C}}_{1:2K},\bar{\boldsymbol{\Phi}}} \sum_{k=1}^{2K}\|\bar{\boldsymbol{\Sigma}}_{k} - \bar{\mathbf{B}}\bar{\mathbf{C}}_k \bar{\boldsymbol{\Phi}}\bar{\mathbf{C}}_k\bar{\mathbf{B}}'\|_F^2.
\end{equation*}
%
Instead, we focus on the direct fitting algorithm. This algorithm has advantages because not only it has a fast convergence but it also produces the factor scores, which are used for constructing factor dynamics. Moreover, one can easily incorporate additional constraints such as non-negative entries in scaler matrices.  


We now describe the direct fitting algorithm. From $\bar{\mathbf{F}}_{k} = \bar{\mathbf{A}}_{k}\bar{\mathbf{C}}_{k}$ where $\bar{\mathbf{A}}_{k}$ is a $T_k \times r_J$ matrix consisting of scaled factor series $\bar{F}_{t}^{(k)}$, the goal is to solve the following problem:
%
\begin{eqnarray}
 (\hat{\mathbf{A}}_{1:2K},\hat{\mathbf{C}}_{1:2K},\hat{\mathbf{B}}) 
 &=& \argmin_{\bar{\mathbf{A}}_{1:2K},\bar{\mathbf{C}}_{1:2K},\bar{\mathbf{B}}} \sum_{k=1}^{2K} \left\| \bar{\mathbf{X}}_k - \bar{\mathbf{A}}_k\bar{\mathbf{C}}_k\bar{\mathbf{B}}' \right\|_F^2, \label{e:PARAFAC2_orginal_object} \\
    &\textrm{s.t.}& \quad \bar{\mathbf{A}}_k'\bar{\mathbf{A}}_k = \bar{\mathbf{A}}_{k'}'\bar{\mathbf{A}}_{k'}, \quad k,k'\in\{1,\ldots,2K\}. \label{e:PARAFAC2_orginal_constraint}
\end{eqnarray}
%
The key idea is to convert the problem \eqref{e:PARAFAC2_orginal_object}--\eqref{e:PARAFAC2_orginal_constraint} into $2K$ separate orthogonal Procrustes problems, which have been dealt with in psychometrics \cite[e.g.,][]{green:1952,schonemann:1966}. 


Note that the optimal solution \eqref{e:PARAFAC2_orginal_object}--\eqref{e:PARAFAC2_orginal_constraint} exists when $\bar{\mathbf{A}}_k'\bar{\mathbf{A}}_k = \bar{\mathbf{A}}'\bar{\mathbf{A}}$ for all $k$. Then, one can always find a columnwise orthonormal $T_k \times r_J$ matrix $\bar{\mathbf{P}}_k$ such that $\bar{\mathbf{A}}_k = \bar{\mathbf{P}}_k\bar{\mathbf{A}}$ \cite[e.g., Exercise 14.3.13 in][]{harville2006matrix} so that
%
\begin{equation*}
    \bar{\mathbf{A}}_k'\bar{\mathbf{A}}_k  = \bar{\mathbf{A}}'\bar{\mathbf{P}}_k'\bar{\mathbf{P}}_k\bar{\mathbf{A}} = \bar{\mathbf{A}}'\bar{\mathbf{A}}.
\end{equation*}
%
This corresponds to the necessary and sufficient conditions for the optimality described in Section 2 in \cite{kiers:1999}. Thus, the original problem \eqref{e:PARAFAC2_orginal_object}--\eqref{e:PARAFAC2_orginal_constraint} becomes
%
\begin{eqnarray}
    (\hat{\mathbf{P}}_{1:2K},\hat{\mathbf{A}},\hat{\mathbf{C}}_{1:2K},\hat{\mathbf{B}}) 
     &=& \argmin_{\bar{\mathbf{P}}_{1:2K},\bar{\mathbf{A}},\bar{\mathbf{C}}_{1:2K},\bar{\mathbf{B}}} \sum_{k=1}^{2K} \left\| \bar{\mathbf{X}}_k - \bar{\mathbf{P}}_k\bar{\mathbf{A}}\bar{\mathbf{C}}_k\bar{\mathbf{B}}' \right\|_F^2, \label{e:PARAFAC2_separate_object}\\
        &\textrm{s.t.}& \quad \bar{\mathbf{P}}_k'\bar{\mathbf{P}}_k = \mathbf{I}_{r_{J}},\quad k=1,\ldots,2K. \label{e:PARAFAC2_separate_constraint}
\end{eqnarray}
%
Note that the objective function for fixed $k$ is $\textrm{Tr}(\bar{\mathbf{X}}_k'\bar{\mathbf{X}}_k) - 2\textrm{Tr}(\bar{\mathbf{X}}_k'\bar{\mathbf{P}}_k\bar{\mathbf{A}}\bar{\mathbf{C}}_k\bar{\mathbf{B}}') + \textrm{Tr}(\bar{\mathbf{B}}\bar{\mathbf{C}}_k\bar{\mathbf{A}}'\bar{\mathbf{A}}\bar{\mathbf{C}}_k\bar{\mathbf{B}}')$, where $\textrm{Tr}(\cdot)$ is a trace operator. Hence, a minimizer of \eqref{e:PARAFAC2_separate_object}--\eqref{e:PARAFAC2_separate_constraint} over $\bar{\mathbf{P}}_k$ for fixed other components is obtained by
%
\begin{equation}\label{e:e:PARAFAC2_single}
    \hat{\mathbf{P}}_k = \argmax_{\bar{\mathbf{P}}_k'\bar{\mathbf{P}}_k = \mathbf{I}_{r_{J}}} \textrm{Tr}(\bar{\mathbf{A}}\bar{\mathbf{C}}_k\bar{\mathbf{B}}'\bar{\mathbf{X}}_k'\bar{\mathbf{P}}_k).
\end{equation}
%
Take the SVD as $\bar{\mathbf{A}}\bar{\mathbf{C}}_k\bar{\mathbf{B}}'\bar{\mathbf{X}}_k' = \bar{\mathbf{U}}_k\bar{\mathbf{S}}_k\bar{\mathbf{V}}_k'$, where $\bar{\mathbf{U}}_k$ and $\bar{\mathbf{V}}_k$ are $r_J \times r_J$ and $r_J \times T_k$ matrices with orthogonal columns and $\bar{\mathbf{S}}_{k}$ consists of singular values of $\bar{\mathbf{A}}\bar{\mathbf{C}}_k\bar{\mathbf{B}}'\bar{\mathbf{X}}_k'$. By replacing $\bar{\mathbf{A}}\bar{\mathbf{C}}_k\bar{\mathbf{B}}'\bar{\mathbf{X}}_k'$ with $\bar{\mathbf{U}}_k\bar{\mathbf{S}}_k\bar{\mathbf{V}}_k'$, the solution of \eqref{e:e:PARAFAC2_single} becomes $\bar{\mathbf{P}}_k = \bar{\mathbf{V}}_k\bar{\mathbf{U}}_k'$, which can be shown by using CauchySchwarz inequality. This completes the first step of the direct fitting algorithm. Next, substituting $\bar{\mathbf{V}}_k'\bar{\mathbf{U}}_k$ for $\bar{\mathbf{P}}_k$ in \eqref{e:PARAFAC2_separate_object} leads to 
%
\begin{equation}\label{e:PARAFAC_object}
    (\hat{\mathbf{A}},\hat{\mathbf{C}}_{1:2K},\hat{\mathbf{B}}) 
    = \argmin_{\bar{\mathbf{A}},\bar{\mathbf{C}}_{1:2K},\bar{\mathbf{B}}} \sum_{k=1}^{2K} \left\| \hat{\mathbf{P}}_k'\bar{\mathbf{X}}_k - \bar{\mathbf{A}}\bar{\mathbf{C}}_k\bar{\mathbf{B}}' \right\|_F^2.
\end{equation}
%
The problem \eqref{e:PARAFAC_object} can be solved by CANDECOMP/PARAFAC (CP) model as the ALS algorithm \cite[CP-ALS, see][for the details]{kiers:1998}. This completes the second step. The two steps are repeated until a convergence criterion is satisfied. The same algorithm is applied to the estimates $\tilde{\mathbf{A}}_{k},\tilde{\mathbf{C}}_{k}$, and $\tilde{\mathbf{B}}_g$, with group individual components $\tilde{\mathbf{X}}_k$, $k=1,\ldots,2K$, for the corresponding $g=1,2$.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Illustrative examples}
\label{se:illustrative}

\subsection{Simulation design}
\label{sse:design}

In this section, we describe the simulation design that is used to assess estimation performance under different scenarios. The results will be reported in Section \ref{sse:result_main} below. For the simulation design, we choose various combinations of large $d,T$ and $K$ as long as the sizes of the problem can be handled through the standard computational environment. R packages \texttt{ajive} of \cite{Carmichael:2020} and \texttt{multiway} of \cite{helwig:2019} have been used. 

The data generating processes (DGPs) are as follows.
%
\begin{itemize}
    \item[-] $X_{i,t}^{(k)} 
    = \left\{\begin{array}{ll}
    \sum_{j=1}^{r_J}\bar{B}_{i,j}\bar{F}_{j,t}^{(k)} + \sum_{j=1}^{r_G}\tilde{B}_{i,j}^{(1)}\tilde{F}_{j,t}^{(k)} + E_{i,t}^{(k)}, & \textrm{if }k=1,\ldots,K, \\
    \sum_{j=1}^{r_J}\bar{B}_{i,j}\bar{F}_{j,t}^{(k)} + \sum_{j=1}^{r_G}\tilde{B}_{i,j}^{(2)}\tilde{F}_{j,t}^{(k)} + E_{i,t}^{(k)}, & \textrm{if }k=K+1,\ldots,2K,
    \end{array}\right.$
    
    \item[-] Among the indices $i=1,\ldots,d$, choose 50\% at random and make $\bar{B}_{i,j}$, $j=1,\ldots,r_J$, non-zero as $\bar{B}_{i,j}\stackrel{i.i.d.}{\sim}\mathrm{Unif}(b_{\min},b_{\max})$. Otherwise, $\bar{B}_{i,j}=0$ for the non-selected indices $i$. This ensures identifiability as discussed in Section \ref{ssse:identifiability}.
    
    \item[-] Among the non-selected indices $i$ above, half of them are chosen to make $\tilde{B}_{i,j}^{(1)}$, $j=1,\ldots,r_G$, have non-zero entries. The remaining half are used for $\tilde{B}_{i,j}^{(2)}$, $j=1,\ldots,r_G$, to be non-zero. Both are generated by $\tilde{B}_{i,j}^{(1)},\tilde{B}_{i,j}^{(2)}\stackrel{i.i.d.}{\sim}\mathrm{Unif}(b_{\min},b_{\max})$. Otherwise, let $\tilde{B}_{i,j}^{(1)},\tilde{B}_{i,j}^{(2)}$ be zero for the non-selected indices of $i$.
    
    \item[-] For each $k=1,\ldots,2K$, generate the series $\bar{A}_{t}^{(k)}$ and $\tilde{A}_{t}^{(k)}$, $t=1,\ldots,T_{k}$, in the SCA-PF2 formulation \eqref{e:SCA-PF2} according to the following VAR(1) models,
    %
    \begin{eqnarray*}
        \bar{A}_{t}^{(k)} 
        = \bar{\boldsymbol{\Psi}}\bar{A}_{t-1}^{(k)} + \bar{\xi}_{t}^{(k)},\quad \{\bar{\xi}_{t}^{(k)}\} \sim \mathrm{WN}(0,\sigma_{\bar{\xi}}\mathbf{I}_{r_{J}}), \\
        \tilde{A}_{t}^{(k)} 
        = \tilde{\boldsymbol{\Psi}}\tilde{A}_{t-1}^{(k)} + \tilde{\xi}_{t}^{(k)},\quad \{\tilde{\xi}_{t}^{(k)}\} \sim \mathrm{WN}(0,\sigma_{\tilde{\xi}}\mathbf{I}_{r_{G}}).        
    \end{eqnarray*}
    %
    For simplicity, let $\sigma_{\bar{\xi}} = \sigma_{\tilde{\xi}} = \sigma_{\xi}$. Let also $\bar{\boldsymbol{\Psi}}$ and $\bar{\boldsymbol{\Phi}}=\mathbb{E}\bar{A}_t^{(k)}\bar{A}_t^{(k)'}$ have the same structures as $\tilde{\boldsymbol{\Psi}}$ and $\tilde{\boldsymbol{\Phi}}=\mathbb{E}\tilde{A}_t^{(k)}\tilde{A}_t^{(k)'}$, respectively. For stable factore series, the VAR transition matrices $\bar{\boldsymbol{\Psi}}$ and $\tilde{\boldsymbol{\Psi}}$ satisfy the following equations,
    %
    \begin{eqnarray}
        \bar{\boldsymbol{\Phi}} 
        &=& \bar{\boldsymbol{\Psi}} \bar{\boldsymbol{\Phi}} \bar{\boldsymbol{\Psi}}^{'} + \sigma_{\xi}\mathbf{I}_{r_{J}}, \label{e:riccati_joint} \\
        \tilde{\boldsymbol{\Phi}} 
        &=& \tilde{\boldsymbol{\Psi}} \tilde{\boldsymbol{\Phi}} \tilde{\boldsymbol{\Psi}}^{'} + \sigma_{\xi}\mathbf{I}_{r_{G}} \label{e:riccati_group}
    \end{eqnarray}
    %
    For identifability of \eqref{e:SCA-PF2}--\eqref{e:SCA-IND}, the PARAFAC2 algorithm automatically estimates the diagonal components of $\bar{\boldsymbol{\Phi}}$ and $\tilde{\boldsymbol{\Phi}}$ to be one. Hence, we set $\bar{\phi}_{ii}=\tilde{\phi}_{ii}=1$ at the model level. Then, we specify the structures $\bar{\boldsymbol{\Phi}}$ and $\tilde{\boldsymbol{\Phi}}$ and choose $\bar{\boldsymbol{\Psi}}$ and $\tilde{\boldsymbol{\Psi}}$ satisfying the equations \eqref{e:riccati_joint}--\eqref{e:riccati_group} for fixed $\sigma_{\xi}$. From the known fact of algebraic Riccati equations, the stable solution is uniquely determined \cite[e.g.,][]{boyd:1994}. Different structures of $\bar{\boldsymbol{\Phi}}$ and $\tilde{\boldsymbol{\Phi}}$ will be referred to as types below.
    
    \item[-] Take $\bar{F}_{t}^{(k)} = \bar{\mathbf{C}}_k\bar{A}_{t}^{(k)}$ and $\tilde{F}_{t}^{(k)} = \tilde{\mathbf{C}}_k\tilde{A}_{t}^{(k)}$, where 
    %
    \begin{equation*}
        \bar{C}_{i,i}^{(k)},\tilde{C}_{i,i}^{(k)} \stackrel{i.i.d.}{\sim} \textrm{Unif}(c_{\min},c_{\max}),\quad
        \bar{C}_{i,j}^{(k)} = \tilde{C}_{i,j}^{(k)} = 0,\ i\neq j.
    \end{equation*}
    %
    
    \item[-] $E_{t}^{(k)}\sim\mathcal{N}\left(0,\Sigma_{\mathbf{E},k} = \sigma_{\varepsilon} \mathbf{I}_{d} \right)$, $k=1,\ldots,2K$.
    
\end{itemize}
%
In summary, DGP is fully controlled by $(\textrm{type},b_{\min},b_{\max},c_{\min},c_{\max},\sigma_{\xi},\sigma_{\varepsilon})$ for fixed $(r_J,r_G)$. By changing these parameters, we vary the generated series to see how the realizations with different characteristics affect estimation performance. 


We assess estimation performance in three ways involving 5 measures of interest. First, we report $R^2$ statistic which indicates the overall fit of the model. It measures how the model explains the total variability. From \eqref{e:matrix_joint_individual1}--\eqref{e:matrix_joint_individual2} and \eqref{e:augmented_block1}, we define this measure by
%
\begin{equation*}
    R^2 = 1 - \frac{\sum_{k=1}^{2K}\|\mathbf{X}_k -  \hat{\bar{\mathbf{F}}}_k\hat{\bar{\mathbf{B}}}^{'} - \hat{\tilde{\mathbf{F}}}_k\hat{\tilde{\mathbf{B}}}_g^{'}\|_F^2}{\sum_{k=1}^{2K}\|\mathbf{X}_k\|_F^2},
\end{equation*}
%
where $\{\hat{\bar{\mathbf{F}}}_k,\hat{\tilde{\mathbf{F}}}_k\}$ are either the direct output from PARAFAC2 \eqref{e:PARAFAC2_orginal_object}--\eqref{e:PARAFAC2_orginal_constraint} or the refitted latent factor series from \eqref{e:refitting}.


Another measure of the model fit is a root mean squared error (RMSE). The difference between this measure and $R^2$ statistic is that it concerns only the fit of each block while $R^2$ statistic considers both the joint and group individual components. For the joint structure, we define
%
\begin{equation*}
    RMSE = \sqrt{\frac{1}{2dK}\sum_{k=1}^{2K}\frac{1}{T_k}\| \bar{\mathbf{X}}_k - \hat{\bar{\mathbf{F}}}_k\hat{\bar{\mathbf{B}}}^{'}\|_F^2}.
\end{equation*}
%
Replacing $\bar{\mathbf{X}}_k$, $\hat{\bar{\mathbf{F}}}_k\hat{\bar{\mathbf{B}}}^{'}$ and $2K$ by $\tilde{\mathbf{X}}_k$, $\hat{\tilde{\mathbf{F}}}_k\hat{\tilde{\mathbf{B}}}_g^{'}$ and $K$, gives the RMSE of the group individual structures for  $g=1,2$. 


Finally, at each component level, we compute three other measures on how estimation captures spatial information and temporal dependence. We introduce the Tucker congruence coefficient $(CC_M)$ defined as
%
\begin{equation*}
    CC_M = \frac{\textrm{vec}(\mathbf{M})'\textrm{vec}(\hat{\mathbf{M}})}{\sqrt{\textrm{vec}(\mathbf{M})'\textrm{vec}(\mathbf{M})}\sqrt{\textrm{vec}(\hat{\mathbf{M}})'\textrm{vec}(\hat{\mathbf{M}})}},
\end{equation*}
%
where $\mathbf{M}$ is some parameter and $\hat{\mathbf{M}}$ corresponds to its estimate. We can assess the accuracy of estimating loadings matrices for spatial information through $CC_B$, transition matrices for temporal dependence of latent factors through $CC_{\Psi}$, and factor series themselves through $CC_{F}$. For computing $CC_{\Psi}$ and $CC_F$, we use the refitted factor series. $CC_{F}$ may be inherently more varying since the measure compares two random processes. We nevertheless expect that these $CC_F$ scores are still informative. The CC scores will be reported for the joint and two groups separately.


One reason for the joint modeling of time series data is its potential to enhance the forecasting performance \cite[e.g.,][]{fisher:2022}. To evaluate the forecasting performance, from the same DGP, we generate data for additional $h$ steps. Then, the task is to forecast this true but unobserved vectors $X_{t}^{(k)}$, $t=T_k+1,\ldots,T_k+h$. From the estimated factor series and their model equations, one can obtain the $h$-step-ahead predictions by: with $g=1$ for $k=1,\ldots,K$, and $g=2$ for $k=K+1,\ldots,2K$,
%
\begin{eqnarray*}
    X_{t|T_k}^{(k)} 
    &=& \hat{\bar{\mathbf{B}}}\hat{\bar{F}}_{t|T_k}^{(k)} + \hat{\tilde{\mathbf{B}}}_g\hat{\tilde{F}}_{t|T_k}^{(k)}, \\
    \hat{\bar{F}}_{t|T_k}^{(k)} 
    &=& \hat{\bar{\boldsymbol{\Psi}}}_{k,1}\hat{\bar{F}}_{t-1|T_k}^{(k)}, \\
    \hat{\tilde{F}}_{t|T_k}^{(k)} 
    &=&
     \hat{\tilde{\boldsymbol{\Psi}}}_{k,1}\hat{\tilde{F}}_{t-1|T_k}^{(k)},
\end{eqnarray*}
%
where $\hat{\bar{F}}_{T_k|T_k}^{(k)}=\hat{\bar{F}}_{T_k}^{(k)}$, $\hat{\tilde{F}}_{T_k|T_k}^{(k)}=\hat{\tilde{F}}_{T_k}^{(k)}$. From these $h$-step-ahead predictions, we compute the root mean squared forecasting error (RMSFE) as
%
\begin{equation*}
    RMSFE = \sqrt{\frac{1}{2dK}\sum_{k=1}^{2K}\sum_{t=T_k+1}^{T_k+h}\|X_{t}^{(k)} - \hat{X}_{t|T_k}^{(k)}\|_F^2}.
\end{equation*}
%
We take up to 3-step-ahead predictions in this simulation.


For each simulation setting, we replicate estimation 100 times. In Section \ref{sse:result_main}, we let $(d,T,K)$ be:
%
\begin{itemize}
    \item[C1.] Growing numbers of variables: Fix $T=100$, $K=50$. Take $d=200,400$ and $800$.
    
    \item[C2.] Growing observation times: Fix $d=100$, $K=50$. Take $T=200,400$ and $800$.
    
    \item[C3.] Growing numbers of subjects: Fix $d=100$, $T=200$. Take $K=100,200$ and $400$.
\end{itemize}
%
For each case, we fix the ranks as $(r_J,r_G)=(2,2),(2,3),(3,3)$. As one can see from the results in Section \ref{sse:result_bootstrap}, the sums of these given ranks are mostly guaranteed to be found through the initial rank selection algorithm.


Along the problem sizes described above, we consider the different combinations of control parameters as follows. The eigenvalues of transition matrices generated by the combinations below have been checked to be outside the unit circle. This confirmation guarantees the stationarity of the factor series. We use the combinations P1 through P4 of model parameters as:
%
\begin{itemize}
    \item[P1.] SCA-PF2: $(\textrm{type},b_{\min},b_{\max},c_{\min},c_{\max},\sigma_{\xi},\sigma_{\varepsilon})=(1,0,1,5,10,0.2,1)$.

    \item[P2.] SCA-IND: $(\textrm{type},b_{\min},b_{\min},c_{\min},c_{\max},\sigma_{\xi},\sigma_{\varepsilon})=(2,0,1,5,10,0.3,1)$.
    
    \item[P3.] SCA-PF2 (Strong spatial): $(\textrm{type},b_{\min},b_{\max},c_{\min},c_{\max},\sigma_{\xi},\sigma_{\varepsilon})=(1,0,2,5,10,0.2,1)$.
    
    \item[P4.] SCA-PF2 (Strong temporal): $(\textrm{type},b_{\min},b_{\max},c_{\min},c_{\max},\sigma_{\xi},\sigma_{\varepsilon})=(1,0,1,10,20,0.2,1)$.
    
    \item[P5.] SCA-PF2 (Strong noise): $(\textrm{type},b_{\min},b_{\max},c_{\min},c_{\max},\sigma_{\xi},\sigma_{\varepsilon})=(1,0,1,5,10,0.2,4)$.
\end{itemize}
%
P5 is only used for illustrating the initial rank selection in Section \ref{ssse:initial_rank}. $\bar{\boldsymbol{\Phi}}$ and $\tilde{\boldsymbol{\Phi}}$ for type 1 are
%
\begin{equation}\label{e:covariance_example}
    \bar{\boldsymbol{\Phi}},\tilde{\boldsymbol{\Phi}} 
    = 1,\quad 
    \begin{pmatrix}
    1 & -0.6 \\
    -0.6 & 1
    \end{pmatrix},\quad
    \begin{pmatrix}
    1 & -0.6 & 0.3 \\
    -0.6 & 1 & -0.6 \\
    0.3 & -0.6 & 1
    \end{pmatrix},\quad
    \begin{pmatrix}
    1 & -0.6 & 0.3 & -0.1\\
    -0.6 & 1 & -0.6 & 0.3\\
    0.3 & -0.6 & 1 & -0.6\\
    -0.1 & 0.3 & -0.6 & 1
    \end{pmatrix},
\end{equation}
%
and the structures for type 2 are the identity matrix with the corresponding rank. 


We consider three other methods as benchmarks for comparison. The first alternative method is to fit the PARAFAC2 model at the first two stages of our approach. Recall that the first two stages are to fit the total block data by AJIVE and then fit PARAFAC2 models to three factorized data blocks (joint component block and two group individual component blocks). However, motivated by practical considerations, one can possibly fit PARAFAC2 to the whole data block first and extract the signal block. This signal naturally becomes the joint structure. Then, for the remainder of each group, one can fit PARAFAC2 again to extract the group individual structure. Then, the rest of the steps are the same as GRIDY. We refer to this benchmark as \textit{double-SCA} (\textit{DSCA}). However, we stress that this method has an issue, despite being used by researchers, since in order to fit PARAFAC2 models, one has to know the ranks for the factors in advance. We note that the AJIVE implicitly selects the joint rank and this joint rank estimation is quite reliable as shown in the literature. One can instead do permutation for the combinations of ranks, which requires a heavy computation load and is beyond our interest. For this comparison, we assume that the joint rank is explicitly known.


As another benchmark, the usual PCA is used in place of the SCA as follows. First, use PCA by imposing \eqref{e:orthogonality_factor}. Then, find matrices, say $\bar{\mathbf{Q}}_{k},\tilde{\mathbf{Q}}_{k}$, satisfying 
%
\begin{eqnarray*}
    \bar{\mathbf{B}}_{1}
    &=& \bar{\mathbf{Q}}_{k}\bar{\mathbf{B}}_{k},\quad k=2,\ldots,2K, \\
    \tilde{\mathbf{B}}_{1} 
    &=& \tilde{\mathbf{Q}}_{k}\tilde{\mathbf{B}}_{k},\quad k=2,\ldots,K,\\
    \tilde{\mathbf{B}}_{K+1} 
    &=& \tilde{\mathbf{Q}}_{k}\tilde{\mathbf{B}}_{k},\quad k=K+2,\ldots,2K,
\end{eqnarray*}
%
and multiply $\bar{\mathbf{Q}}_{k}^{-1}$ and $\tilde{\mathbf{Q}}_{k}^{-1}$ to each factor series $\bar{\mathbf{F}}_{k}$ and $\tilde{\mathbf{F}}_{k}$. The rest of the steps, using AJIVE, subject-wise linear regressions, and Yule-Walker equations, are also identical to GRIDY. We denote this method as \textit{PCA}.


Finally, one can directly use the estimated factor series from SCA at the second stage of our approach. That is, one can omit the refitting of factor series in \eqref{e:refitting}. Obviously, the first two stages for estimation remain the same. We denote this benchmark as \textit{Unfitted}.


We admit that other benchmarks can possibly be considered. For example, one can replace AJIVE with COBE in our approach. However, for this replacement, the rest of the measures are heavily relying on the performance of data integration methods. This analysis has been covered in related literature so we do not consider it here. One can also replace the Yule-Walker equations with other estimating methods, such as Kalman recursions. However, as stated in Section \ref{sse:reconstruction}, the additional assumption that the joint factors and the group individual factors are independent, for instance, should be made. Moreover, since our noise terms have no particular structure, we do not expect to have a significant enhancement in the estimation. We omit this replacement for brevity.




\subsection{Simulation results}
\label{sse:result_main}


We explain the simulation results in the order of measures described in Section \ref{sse:design}. All figures are displayed at the end of this work. Figures \ref{fig:R2_d} through \ref{fig:R2_K} present $R^2$ from 4 methods including GRIDY as the number of variables $d$, the sample length $T$, and the number of subjects $K$ are increasing, according to C1, C2, and C3 respectively, as defined in Section \ref{sse:design}. Throughout all cases, GRIDY has a slightly better result (larger $R^2$) than the other benchmarks. Interestingly, the values of $R^2$ statistics for C1 through C3 vary little. The same is also suggested for the different rank combinations. The potential differences come more from the parameter settings, P1 through P4, that control the signal-to-noise ratio. As the variability increases due to either larger variation of loadings matrices or factor series, the performance improves.


Next, Figures \ref{fig:RMSE_d} through \ref{fig:RMSE_K} present the root mean squared error of each block structure according to C1 through C3. The consecutive three panels for Joint, Group1, and Group2 in each row by rank combinations consist of the results from the same setting. As we can see, our method has the best performance in all settings. In particular, we can see that DSCA has notably the worst performance. Such deterioration becomes clear when the ranks of factors are increasing. This suggests that researchers should not be using DSCA in practice. Interestingly, our approach, even including Unfitted, seems robust along the increase in the ranks of factor series, or when the ranks are different.


The next three results are on the estimation performance of each component, loadings matrices, factor series, and transition matrices in factor VAR. Figures \ref{fig:CCB_d} through \ref{fig:CCB_K} present the measures $CC_B$ along C1 through C3, respectively. The Unfitted method produced similar results as GRIDY for $CC_B$: we exclude it from the illustrations. Interestingly, although our approach is best overall even in this measure, the results across C1 through C3 are quite different. For example, although $CC_B$ values for all cases in C1 are clearly below 1, they reach nearly 1 in some cases of C2 and C3. This reveals that as the number of variables increases, having a good fit is difficult. Despite this challenge, the results for our approach guarantee good performance, while the results for other methods deteriorate. Similar observations apply to C2 and C3, but the results look more dramatic. As with the previous measures, we can see that the benchmarks perform worse as the number of factors increases. Another interesting point is that DSCA cannot handle the independent factor setting P2, across all cases. This points to another issue when using DSCA. Working well regardless of the dependence in factors is another advantage that our approach offers.



Figures \ref{fig:CCF_d} through \ref{fig:CCF_K} present the measures $CC_F$ according to C1 through C3, respectively. Our approach scored the best in this measure as well. The overall measure values for all methods have slightly higher scores when $d$ or $T$ is increasing. As for the other measures, DSCA performs worse when the number of factors increases. In particular, as the ranks of factors are larger, the differences in the measures between methods become extreme in all parameter combinations. This supports our observation made in Section \ref{sse:design} that the measure $CC_F$ may be more varying in evaluating the estimation performance of factor series. Thus, when the sample length gets longer, the estimation is less likely to match the truth. 



Figures \ref{fig:CCPsi_d} through \ref{fig:CCPsi_K} show the result of $CC_{\Psi}$ according to C1 through C3, respectively. Our approach again gives the best results. A remarkable point is that, unlike the results for $CC_B$ and $CC_F$ where the consequences of DSCA are the most undesirable,  $CC_{\Psi}$ values from PCA deteriorate most rapidly along the increase of the number of factors. This is particularly significant for C2 and C3. 


Finally, we present the forecasting performance results for all methods. Figures \ref{fig:forecast_d} through \ref{fig:forecast_K} show the RMSFE for C1 through C3, respectively. Due to the presence of extreme outliers, we report the mean of the measures after removing the smallest 5\% and largest 5\% of values. The error bars, constructed using the 5th and 95th quantiles, represent the smallest and largest values used for the means, respectively. The results support the necessity of refitting. While our method and PCA exhibit similar forecasting errors followed by DSCA, Unfitted produces the worst results in many cases. Particularly, as the number of factor series increases, the mean performance measure significantly increases. This is because some of the outputs from Unfitted are quite extreme. As the number of subjects grows, the quantile bands for Unfitted also become larger, indicating that not only can extreme outputs be produced, but also average performance cannot be guaranteed. Therefore, only GRIDY with fully completed steps delivers better forecasting results.



\subsection{Choice of initial rank selection}
\label{sse:result_bootstrap}


We consider several DGPs in Section \ref{sse:design} with various ranks and run the bootstrap procedure in Section \ref{ssse:initial_rank} to see if it can capture the correct initial ranks. Different sizes $(d,T)$ of problems are considered while $2K=100$ is fixed. We do not consider the case of growing $K$ since the rotational bootstrap is performed for each block, and gathering multiple data blocks does not help to estimate initial ranks. Then for all control parameter cases above, we generate the DGPs along the combinations of ranks as follows:
%
\begin{itemize}
    \item[R.] $(r_J,r_G) = (1,1),(1,2),(1,3),(2,2),(1,4),(2,3),(2,4),(3,3),(3,4),(4,4)$.
\end{itemize}
%
Note that along the increase of $r_J$ and $r_G$, the corresponding transition matrices $\bar{\boldsymbol{\Phi}}$ and $\tilde{\boldsymbol{\Phi}}$ are adjusted by preserving \eqref{e:riccati_joint} and \eqref{e:riccati_group} with $\bar{\boldsymbol{\Psi}}$ and $\tilde{\boldsymbol{\Psi}}$ in \eqref{e:covariance_example} and fixed $\sigma_{\xi}$. Accordingly, we run the rotational bootstrap to evaluate the performance of the initial rank selection. For the combinations in R above, the initial ranks are $r=r_J+r_G=2,3,4,4,5,5,6,6,7,8$.


Figure \ref{fig:rank} shows several interesting results. First of all, the rank combinations used in Section \ref{sse:result_main} are purple, orange, and dark brown bars, respectively. These are correctly estimated for $(d,T)=(200,100)$ or $(d,T)=(100,200)$. This is also true for the rest of the higher initial ranks when $d$ or $T$ is increased to 400. This indicates that the large problem size seems advantageous in the same way the higher dimensions help in the usual factor models.


Furthermore, for the same initial total ranks, the same parameters and fixed problem sizes, the case where the number of joint factor series is equal to the number of group individual factors is more favorable than the case where the latter is larger. For example, the algorithm is more likely to choose the correct initial ranks in the case $(r_J,r_G)=(3,3)$ than the case $(r_J,r_G)=(2,4)$. Hence, larger ranks in the joint structure is favorable for selecting the correct ranks rather than larger ranks in the group individual structures.


As one can expect, the case P1, where the factors themselves are correlated, is less favorable than the case P2 where the factors are independent as the setting of the usual literature. The discrepancy becomes more dramatic when the true initial ranks are higher. Furthermore, as seen by cases P3 and P4, the rank selection has an advantage from the larger variability due to both the magnitude of loadings matrices and the standard deviation of the factor series. Regardless of what causes stronger signals, this turns out to be favorable for the rank selection. In the same fashion, the estimated ranks are less precise when the variance of the noise becomes larger. Although the algorithm correctly detects the number of factors in the case P5 when either $d$ or $T$ is 400, it is still delicate to choose the right initial rank when the noise is larger. 



\section{Data application}
\label{se:data_app}

\subsection{Autism Brain Imaging Data Exchange preprocessed data}
\label{sse:ABIDE}

In this section, we consider a large pre-processed dataset from the Autism Brain Imaging Data Exchange \citep[ABIDE preprocessed, e.g.,][]{craddock:2013}. ABIDE is an international collection of R-fMRI data from 20 site locations. The data consists of two different groups. Subjects with autism spectrum disorder (ASD) are marked as group 1. And the rest of the subjects are from the control group, denoted as group 2. The dataset from ABIDE is known to have been preprocessed by five teams with their preferred methods. 


We start analysis with 505 subjects from the ASD group, and 530 subjects from the control group. This dataset consists of an extensive array of BOLD signals. The Dosenbach brain atlas by \cite{dosenbach:2010} contains 160 ROIs. In our analysis, the average of the sample lengths over 1035 subjects is 193 time points. Each of 160 ROI labels consists of a combination of the networks and the number. More specifically, the network means the brain region where each ROI is measured. 6 unique regions as brain networks are identified. In the illustration in Section \ref{sse:result_application}, we set different colors for each region so that the same colors are used for representing the same unique regions.


\subsection{Connection to VAR-type model}
\label{sse:VARMA}

In this section, we briefly introduce the network construction by converting DFMs into VAR-type models. This will be useful in thinking about temporal and contemporaneous effects in the considered models. Consider the DFM in \eqref{e:augmented_factor_1}--\eqref{e:augmented_factor_2}. From \eqref{e:filter_equation}, we assume that the joint and group individual latent factor series follow VAR(1), $\bar{F}_{t}^{(k)}=\bar{\mathbf{\Psi}}_k\bar{F}_{t-1}^{(k)} + \bar{\eta}_{t}^{(k)}$ and $\tilde{F}_{t}^{(k)}=\tilde{\mathbf{\Psi}}_k\bar{F}_{t-1}^{(k)} + \tilde{\eta}_{t}^{(k)}$, respectively. Then, one can rewrite the DFM as 
%
\begin{eqnarray*}
    X_{t}^{(k)} 
    &=& \mathbf{B}_{g}F_{t}^{(k)} + E_{t}^{(k)} 
    = \begin{bmatrix} \bar{\mathbf{B}} & \tilde{\mathbf{B}}_{g} \end{bmatrix}
    \begin{bmatrix} \bar{F}_{t}^{(k)} \\ \tilde{F}_{t}^{(k)} \end{bmatrix} + E_{t}^{(k)} \\ 
    &=& \begin{bmatrix} \bar{\mathbf{B}} & \tilde{\mathbf{B}}_{g} \end{bmatrix}
    \left( \begin{pmatrix}
    \bar{\mathbf{\Psi}}_k & \boldsymbol{0}_{r_{J} \times r_{G}} \\
    \boldsymbol{0}_{r_{G} \times r_{J}} & \tilde{\mathbf{\Psi}}_k
    \end{pmatrix} \begin{bmatrix} \bar{F}_{t-1}^{(k)} \\ \tilde{F}_{t-1}^{(k)} \end{bmatrix} + \begin{bmatrix} \bar{\eta}_{t}^{(k)} \\ \tilde{\eta}_{t}^{(k)} \end{bmatrix} \right) + E_{t}^{(k)} \\
    &=:& \mathbf{B}_{g} \left( \mathbf{\Psi}_k F_{t-1}^{(k)} +\eta_{t}^{(k)} \right) + E_{t}^{(k)}.
\end{eqnarray*}
%
By using the fact that $F_{t-1}^{(k)} = (\mathbf{B}_{g}'\mathbf{B}_{g})^{-1}\mathbf{B}_{g}'(X_{t-1}^{(k)}-E_{t-1}^{(k)})$ where
%
\begin{equation*}
    (\mathbf{B}_{g}'\mathbf{B}_{g})^{-1} 
    = \begin{pmatrix}
    (\bar{\mathbf{B}}'\bar{\mathbf{B}})^{-1} & \boldsymbol{0}_{r_{J} \times r_{G}} \\
    \boldsymbol{0}_{r_{G} \times r_{J}} & (\tilde{\mathbf{B}}_g'\tilde{\mathbf{B}}_g)^{-1}
    \end{pmatrix},
\end{equation*}
%
$X_{t}^{(k)}$ can be expressed further as
%
\begin{eqnarray}
    X_{t}^{(k)} 
    &=& \left( \bar{\mathbf{B}}\bar{\mathbf{\Psi}}_k(\bar{\mathbf{B}}'\bar{\mathbf{B}})^{-1}\bar{\mathbf{B}}' + \tilde{\mathbf{B}}_g\tilde{\mathbf{\Psi}}_k(\tilde{\mathbf{B}}_g'\tilde{\mathbf{B}}_g)^{-1}\tilde{\mathbf{B}}_g' \right) X_{t-1}^{(k)} + \zeta_{t}^{(k)} \label{e:VARMA_trans1} \\
    &=:& \left( \bar{\mathbf{\Theta}}_{k} + \tilde{\mathbf{\Theta}}_{k} \right) X_{t-1}^{(k)} + \zeta_{t}^{(k)},\quad\ k=1,\ldots,2K, \label{e:VARMA_trans2}
\end{eqnarray}
%
where $\zeta_{t}^{(k)}$ is a centered noise with covariance matrix $\boldsymbol{\Sigma}_{\zeta,k}$, which is given by
%
\begin{equation}\label{e:VARMA_covariance}
    \boldsymbol{\Sigma}_{\zeta,k} = \left( \bar{\mathbf{\Theta}}_{k} + \tilde{\mathbf{\Theta}}_{k} \right)\left(\mathbf{I}_d + \boldsymbol{\Sigma}_{E,k}\right) + \bar{\mathbf{B}} \boldsymbol{\Sigma}_{\bar{\eta},k} \bar{\mathbf{B}}' + \tilde{\mathbf{B}}_g \boldsymbol{\Sigma}_{\tilde{\eta},k} \tilde{\mathbf{B}}_g',
\end{equation}
%
and $\bar{\mathbf{\Theta}}_{k} = \bar{\mathbf{B}}\bar{\mathbf{\Psi}}_k(\bar{\mathbf{B}}'\bar{\mathbf{B}})^{-1}\bar{\mathbf{B}}', \tilde{\mathbf{\Theta}}_{k}=\tilde{\mathbf{B}}_g\tilde{\mathbf{\Psi}}_k(\tilde{\mathbf{B}}_g'\tilde{\mathbf{B}}_g)^{-1}\tilde{\mathbf{B}}_g'$. Hence, for each subject $k$, the dynamics is defined by the two transition matrices $\bar{\mathbf{\Theta}}_{k},\tilde{\mathbf{\Theta}}_{k}$ in \eqref{e:VARMA_trans1}--\eqref{e:VARMA_trans2} and the error covariance matrix $\boldsymbol{\Sigma}_{\zeta,k}$ in \eqref{e:VARMA_covariance}. The equation \eqref{e:VARMA_trans2} is a high-dimensional VAR(1) model. Note that, unlike the high-dimensional VAR models where the sparsity on the transition matrices is imposed, the matrices $\bar{\mathbf{\Theta}}_{k},\tilde{\mathbf{\Theta}}_{k}$ are not sparse objects in general but have reduced rank. The model is akin to reduced-rank VAR since the rank of the transition matrix constructed by the sum of low-rank matrices cannot exceed the sum of two ranks. In the network context, the VAR transition matrix and covariance of the noise are called the directed network and contemporaneous network, respectively. From this perspective, we examine in the next section the differences between the two groups by comparing how their model network connectivities differ.


\subsection{Application results}
\label{sse:result_application}


Figure \ref{fig:rank_ABIDE} presents the results of the initial rank selection. Different colors, which are the same as used in quality assessment (QA) measures at the website \cite{abide:2016}, represent data collected from 20 different sites. The estimated initial ranks are divided into three regions: the left with lower ranks, the middle with intermediate ranks, and the right with higher ranks. The distribution of initially estimated ranks for group 1 closely resembles that of group 2. Rather the results are dependent on preprocessing differences across the various data-collecting sites. We expect that GRIDY framework can further facilitate the study of heterogeneity in R-fMRI across different multi-site differences \cite[e.g.,][]{abraham:2017,wang:2019} in future research.


We exclude data where the initially estimated ranks are zero or exceed 15. Approximately 40\% of the subjects are retained for the remaining analysis. Consequently, the majority of the estimated ranks are 2. Interestingly, although the AJIVE result for joint rank selection is 2, the ranks for the remaining individual structures within each group, as produced by AJIVE, are mostly 1. Figure \ref{fig:AJIVE_ABIDE} provides the results. This indicates that AJIVE detects systematic variation within each subject even after factoring out the joint structures. It's worth noting that, while the remaining individual structures after decomposing the estimated joint structures in AJIVE do not directly imply the existence of group individual structures, the similarity in ranks of the individual structures suggests that one can presume the existence of group structures explicitly. Therefore, for the remaining analysis, we set the joint structure as rank 2 and model the group individual structures with a rank of 1 for each group. We also exclude subjects whose ranks for the group individual structures are zero for the rest of the procedure.



In Figure \ref{fig:R2_ABIDE}, we present the box plots of the $R^2$ statistics across ROIs obtained by regressing the univariate observations $X_{i,t}^{(k)}$ on a particular factor estimate, as discussed in \cite{stock:2002} and \cite{jungbacker:2015}. These statistics indicate how much variability for each variable can be explained by each factor. All variables are reordered based on their networks, with each of the 6 regions represented by its own color. The results for both the Autism group (on the left panel) and the control group (on the right panel) are displayed separately. Since there are 2 joint factors and each group has 1 group-specific individual factor, the $R^2$ statistics from the joint factors are displayed in the top two rows. The $R^2$ statistics for each group's individual factor are displayed in the third row of the left column and the fourth row of the right column, respectively. We can observe that the patterns of the $R^2$ statistics from the joint factors are quite similar for the two groups, while those from each group individual factors are notably different, in particular, the networks in red and purple. This difference arises from the fact that their spatial information shapes the landscape through the loadings matrices, while the vertical scales are determined by the factor series, which also causes temporal dependences.


We next consider the various estimates separately. Figure \ref{fig:loadings_ABIDE} displays the estimated loadings matrices, which include both the joint and group individual structures. Interestingly, the distributions of $R^2$ statistics for each factor are quite similar to the distributions of values in the loadings matrices when ignoring their signs. Additionally, Figure \ref{fig:factor_ABIDE} presents the estimated factor series for each structure. Each plot contains 7 randomly selected subjects for better display clarity, with series in each partitioned plot of the same color belonging to the same subject. As expected from the usual PCA result, the first factor in the joint components fluctuates the most, with the magnitude being proportional to its singular values. However, while the ranges of fluctuation for the other factor series are small, their contributions to the $R^2$ statistics seem quite impactful. In particular, we observe that the deviation within the same ROI is quite large. This implies that not only the first series but also the other factor series contribute significantly to explaining the variability. Further information regarding estimated VAR factor transition matrices and covariance matrices of the noise are provided in Figure \ref{fig:Psi_Cov_ABIDE}. While the deviations of each estimate in this figure are not large, the contribution of each factor to explain the variability diverges. Therefore, the differences between subjects can be explained by the configuration of model components, and the different realizations of the factor series contribute to the differences in time evolution within each subject.


Now, let's consider the VAR construction explained in Section \ref{sse:VARMA} to illustrate network structures. Figure \ref{fig:roi_ABIDE} summarizes the network structures across all subjects. The left panel presents the averages of the directed networks and the contemporaneous networks for group 1, while the results for group 2 are displayed in the right panel. The two heatmaps in the top panel represent the directed networks, and the two in the bottom panel represent contemporaneous networks. The ROIs for the variables are the same as in the previous figures. We can clearly observe distinct block structures in both the directed networks and contemporaneous networks. Interestingly, while the sizes and locations of the identified blocks in the directed networks are quite similar, the subpatterns within each block are quite different. Although the scales of the contemporaneous networks are relatively larger, they exhibit a similar tendency as the directed networks. Hence, the brain connectivity dynamics over time for Autism patients appear to differ, on average, from the temporal network structure of the subjects in the control group. While the contemporaneous network structures for both groups are also different, they exhibit relatively high similarity. We expect that revealing such differences in the network structures may be applicable to other ABIDE studies that demonstrate differences in functional connectivity between the Autism group and the control group, as well as among each group's subjects \cite[e.g.,][]{subbaraju:2017,easson:2019}.


The forecasting performance for the two groups is similar. It may be related to that the variability explained by their signals through the directed network is also similar for both groups, as we have seen through the $R^2$ statistics. Table \ref{table:rmsfe_ABIDE} presents the forecasting errors of each group over 10-step forecasting horizons with two benchmarks used in the previous section. 


\begin{table}[t]
\centering
\resizebox{0.8\textwidth}{!}{%
\begin{tabular}{ccccccccccccc}
 &  &  & \multicolumn{10}{c}{Forcasting horizon} \\ \cline{4-13} 
 &  &  & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ \hline
\multirow{6}{*}{RMSFE} & \multirow{3}{*}{Group 1} 
 & GRIDY &  0.0377 & 0.0559 & 0.0735 & 0.0772 & 0.0794 & 0.0889 & 0.0928 & 0.0935 & 0.0950 & 0.0950 \\
 &  & DSCA &  0.0377 & 0.0553 & 0.0723 & 0.0758 & 0.0779 & 0.0872 & 0.0909 & 0.0918 & 0.0935 & 0.0935 \\
 &  & PCA &  0.0376 & 0.0558 & 0.0735 & 0.0772 & 0.0795 & 0.0890 & 0.0928 & 0.0935 & 0.0951 & 0.0951 \\ \cline{2-3}
 & \multirow{3}{*}{Group 2} 
  & GRIDY &  0.0592 & 0.0754 & 0.0792 & 0.0792 & 0.0819 & 0.0856 & 0.0880 & 0.0903 & 0.0909 & 0.0919 \\
 &  & DSCA &  0.0595 & 0.0761 & 0.0800 & 0.0801 & 0.0824 & 0.0860 & 0.0883 & 0.0905 & 0.0911 & 0.0919  \\
 &  & PCA &  0.0589 & 0.0751 & 0.0788 & 0.0789 & 0.0818 & 0.0855 & 0.0878 & 0.0902 & 0.0908 & 0.0918 \\ \hline
\end{tabular}%
}
\caption{Root mean squared forecasting errors for each group over 10-step forecasting horizon with two benchmarks.}
\label{table:rmsfe_ABIDE}
\end{table}




\section{Discussion}
\label{se:discussion}

In this study, we propose a novel approach to treat the data integration of DFMs where the resulting integrated model is considered as the summation of joint components, group individual components, and the observation noise. Each signal structure is further decomposed into loadings matrices and latent factor series, with these series assumed to follow VAR models. Unlike the usual DFMs, our model allows different factor series within the same structure to be correlated. To handle such latent and correlated structures, we introduce simultaneous component analysis. We conducted simulations to compare our modeling approach with several benchmark models suitable for the considered setting.  Using various estimation performance measures and forecasting errors, the empirical evidence suggests that our approach outperforms all possible combinations of problem scenarios. To illustrate the practical application of our approach, ABIDE preprocessed dataset is used. This demonstrates how our approach can be employed to compare configurations and network structures between Autism patients and control groups, aiding in the discovery of differences in characteristics between the groups.


There are several promising extensions of this work that could be pursued in the future. Firstly, one could consider adopting a more efficient algorithm for estimating the PARAFAC2 model, as developed by \cite{perros:2017}. This algorithm would prove indispensable in speeding up computation and improving memory efficiency, particularly as the number of subjects increases. Additionally, from the VAR representation discussed earlier, our model bears similarities to the multi-VAR model of \cite{fisher:2022} albeit with sparse VAR transition matrices. Lastly, a highly promising avenue for future research involves the consideration of multiple subgroups. In such cases, incorporating a partially-shared structure \cite[e.g.,][]{gaynanova:2019,prothero:2022} could allow for only some grouped subjects to possess shared structures when the number of groups exceeds two.





\section*{Acknowledgement}

Vladas Pipirass research was partially supported by the grants NSF DMS 1712966, DMS 2113662, and DMS 2134107.


\section*{Data availability statement}
The R code used in the simulation of Sections \ref{se:illustrative} and in the data analysis of Section \ref{se:data_app} are available on GitHub at \href{https://github.com/yk748/GRIDY}{https://github.com/yk748/GRIDY}. The ABIDE preprocessed data used in Section \ref{se:data_app} were derived from the following resource available in the public domain: \href{http://preprocessed-connectomes-project.org/abide/index.html}{http://preprocessed-connectomes-project.org/abide/index.html}. 




\clearpage
%%%%% R2 
% Figure environment removed

% Figure environment removed

% Figure environment removed


%%%%% RMSE 
% Figure environment removed

% Figure environment removed

% Figure environment removed


%%%%% CCB


% Figure environment removed

% Figure environment removed

% Figure environment removed


%%%%% CCF
% Figure environment removed

% Figure environment removed

% Figure environment removed


%%%%% CC Phi
% Figure environment removed

% Figure environment removed

% Figure environment removed




%%%%% Forecast
% Figure environment removed

% Figure environment removed

% Figure environment removed



%%%%% Rank
% Figure environment removed


%%%%% ABIDE rank
% Figure environment removed

%%%%% ABIDE AJIVE
% Figure environment removed


%%%%% ABIDE R2
% Figure environment removed


%%%%% ABIDE loadings
% Figure environment removed


%%%%% ABIDE factors
% Figure environment removed


%%%%% ABIDE factors2
% Figure environment removed


%%%%% ABIDE heatmap
% Figure environment removed


\clearpage
\small
\bibliography{multifac}

\end{document}
