\documentclass[11pt, letterpaper]{article}
\usepackage{epsfig,amssymb,amsmath, multirow, url, tcolorbox,booktabs, enumitem}

% For bibtex
%\renewcommand{\cite}{\cite*}
%\bibliographystyle{agsm}
%\citestyle{dcu}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{natbib}
\usepackage[utf8]{inputenc}
\bibliographystyle{agsm}
\citestyle{dcu}
\usepackage{authblk}
\usepackage{lscape}
\usepackage{adjustbox}
\newsavebox{\theorembox}
\newsavebox{\lemmabox}
\newsavebox{\claimbox}
\newsavebox{\factbox}
\newsavebox{\corollarybox}
\newsavebox{\examplebox}
\newsavebox{\remarkbox}
\newsavebox{\assbox}
\newsavebox{\propositionbox}
\newsavebox{\problembox}
\newsavebox{\defbox}

\savebox{\theorembox}{\noindent\bf Theorem}
\savebox{\lemmabox}{\noindent\bf Lemma}
\savebox{\factbox}{\noindent\bf Fact}
\savebox{\corollarybox}{\noindent\bf Corollary}
\savebox{\examplebox}{\noindent\bf Example}
\savebox{\assbox}{\noindent\bf Assumption}
\savebox{\propositionbox}{\noindent\bf Proposition}
\savebox{\problembox}{\noindent\bf Problem}
\savebox{\defbox}{\noindent\bf Definition}

\newtheorem{ass}{\usebox{\assbox}}
\newtheorem{thm}{\usebox{\theorembox}}
\newtheorem{prop}{\usebox{\propositionbox}}
\newtheorem{lem}{\usebox{\lemmabox}}
\newtheorem{fact}{\usebox{\factbox}}
\newtheorem{cor}{\usebox{\corollarybox}}
\newtheorem{problem}{\usebox{\problembox}}
\newtheorem{defn}{\usebox{\defbox}}

\def\blackslug{\hbox{\hskip 1pt \vrule width 4pt height 8pt depth 1.5pt
\hskip 1pt}}
\newtheorem{@assumption}{\bf Assumption}[section]
\newenvironment{assumption}{\begin{@assumption}\rm}{\end{@assumption}}
 \newtheorem{@remark}{\bf Remark}[section]
 \newenvironment{remark}{\begin{@remark}\rm}{\end{@remark}}


\newcommand{\qed}{\mbox{}\hspace*{\fill}\nolinebreak\mbox{$\rule{0.7em}{0.7em}$}}
%\newenvironment{proof}{\par{\noindent \bf Proof:}}{\(\qed\) \par}

\newenvironment{proof}[1][\noindent \bf Proof:]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}


\newcommand{\norm}[1]{\left\Vert#1\right\Vert}
\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\set}[1]{\left\{#1\right\}}

\newcommand{\basis}{(\Omega,  \, (\F_t)_{t \in \Real_+}, \, \prob)}
\newcommand{\indic}{\mathbb{I}}
\newcommand{\pare}[1]{\left(#1\right)}
\newcommand{\bra}[1]{\left[#1\right]}
\newcommand{\such}{\, | \, }
\newcommand{\vs}{\vspace*{3mm}}
\newcommand{\com}[1]{\textbf{\color{red} #1 }} % \textbf{#1}}}
\newcommand{\argmax}{\mathop{\rm argmax}}

%%% newly added commands %%%%%%%%%%%

\newcommand{\argmin}{\mathop{\rm argmin}}
\newcommand{\esssup}{\mathop{\rm esssup}}
\newcommand{\essinf}{\mathop{\rm essinf}}
\newcommand{\lt}{\left}
\newcommand{\rt}{\right}
\newcommand{\rd}{{\rm d}}
\newcommand{\ri}{{\rm i}}
\newcommand{\Tr}{{\rm Tr}}


%% bold face roman
%%
\def\bA{{\bf A}}
\def\ba{{\bf a}}
\def\bB{{\bf B}}
\def\bb{{\bf b}}
\def\bC{{\bf C}}
\def\bc{{\bf c}}
\def\bD{{\bf D}}
\def\bd{{\bf d}}
\def\bE{{\bf E}}
\def\be{{\bf e}}
\def\bF{{\bf F}}
\def\boldf{{\bf f}}
\def\bG{{\bf G}}
\def\bg{{\bf g}}
\def\bH{{\bf H}}
\def\bh{{\bf h}}
\def\bi{{\bf i}}
\def\bI{{\bf I}}
\def\bJ{{\bf J}}
\def\bj{{\bf j}}
\def\bK{{\bf K}}
\def\bk{{\bf k}}
\def\bL{{\bf L}}
\def\bl{{\bf l}}
\def\bM{{\bf M}}
\def\bm{{\bf m}}
\def\bN{{\bf N}}
\def\bn{{\bf n}}
\def\bO{{\bf O}}
\def\bo{{\bf o}}
\def\bP{{\bf P}}
\def\bp{{\bf p}}
\def\bQ{{\bf Q}}
\def\bq{{\bf q}}
\def\bR{{\bf R}}
\def\br{{\bf r}}
\def\bS{{\bf S}}
\def\bs{{\bf s}}
\def\bT{{\bf T}}
\def\bt{{\bf t}}
\def\bU{{\bf U}}
\def\bu{{\bf u}}
\def\bV{{\bf V}}
\def\bv{{\bf v}}
\def\bW{{\bf W}}
\def\bw{{\bf w}}
\def\bX{{\bf X}}
\def\bx{{\bf x}}
\def\bY{{\bf Y}}
\def\by{{\bf y}}
\def\bZ{{\bf Z}}
\def\bz{{\bf z}}
%%


%% bold face Greek (incomplete)
\def\balpha{\mbox{\boldmath $\alpha$}}
\def\bbeta{\mbox{\boldmath $\beta$}}
\def\bepsilon{\mbox{\boldmath $\varepsilon$}}
\def\bvarepsilon{\mbox{\boldmath $\varepsilon$}}
\def\bDelta{{\bf \Delta}}
\def\bdelta{\mbox{\boldmath $\delta$}}
%\def\boldeta{\mbox{\boldmath $\zeta$}}
\def\boldeta{\mbox{\boldmath $\eta$}}
\def\bGamma{{\bf \Gamma}}
\def\bgamma{\mbox{\boldmath $\gamma$}}
\def\bLambda{{\bf \Lambda}}
\def\blambda{\mbox{\boldmath $\lambda$}}
\def\bmu{\mbox{\boldmath $\mu$}}
\def\bOmega{{\bf \Omega}}
\def\bpi{\mbox{\boldmath $\pi$}}
\def\bphi{\mbox{\boldmath $\phi$}}
\def\bPsi{{\bf \Psi}}
\def\bpsi{\mbox{\boldmath $\psi$}}
\def\bSigma{{\bf \Sigma}}
\def\bsigma{\mbox{\boldmath $\sigma$}}
\def\bTheta{{\bf \Theta}}
\def\btheta{\mbox{\boldmath $\theta$}}
\def\bXi{{\bf \Xi}}
\def\bxi{\mbox{\boldmath $\xi$}}
\def\bzeta{\mbox{\boldmath$\zeta$}}
%%

%% caligraphy letters
\def\cA{{\cal A}}
\def\cB{{\cal B}}
\def\cC{{\cal C}}
\def\cD{{\cal D}}
\def\cE{{\cal E}}
\def\cF{{\cal F}}
\def\cG{{\cal G}}
\def\cI{{\cal I}}
\def\cL{{\cal L}}
\def\cM{{\cal M}}
\def\cO{{\cal O}}
\def\cR{{\cal R}}
\def\cS{{\cal S}}
\def\cT{{\cal T}}
\def\cY{{\cal Y}}
\def\cU{{\cal U}}
\def\cV{{\cal V}}

%% mathsf letters
%%
\def\sA{{\sf A}}
\def\sB{{\sf B}}
\def\sC{{\sf C}}
\def\sc{{\sf c}}
\def\sD{{\sf D}}
\def\sE{{\sf E}}
\def\sF{{\sf F}}
\def\sG{{\sf G}}
\def\sH{{\sf H}}
\def\sI{{\sf I}}
\def\sJ{{\sf J}}
\def\sK{{\sf K}}
\def\sL{{\sf L}}
\def\sM{{\sf M}}
\def\sN{{\sf N}}
\def\sP{{\sf P}}
\def\sQ{{\sf Q}}
\def\sR{{\sf R}}
\def\sS{{\sf S}}
\def\sT{{\sf T}}
\def\sU{{\sf U}}
\def\sV{{\sf V}}
\def\sv{{\sf v}}
\def\sW{{\sf W}}
\def\sX{{\sf X}}
\def\sY{{\sf Y}}
\def\sZ{{\sf Z}}

%% mathbb letters
%%
\def\bbA{\mathbb{A}}
\def\bbB{\mathbb{B}}
\def\bbC{\mathbb{C}}
\def\bbD{\mathbb{D}}
\def\bbE{\mathbb{E}}
\def\bbF{\mathbb{F}}
\def\bbG{\mathbb{G}}
\def\bbH{\mathbb{H}}
\def\bbI{\mathbb{I}}
\def\bbJ{\mathbb{J}}
\def\bbK{\mathbb{K}}
\def\bbL{\mathbb{L}}
\def\bbM{\mathbb{M}}
\def\bbN{\mathbb{N}}
\def\bbF{\mathbb{F}}
\def\bbP{\mathbb{P}}
\def\bbQ{\mathbb{Q}}
\def\bbR{\mathbb{R}}
\def\bbS{\mathbb{S}}
\def\bbT{\mathbb{T}}
\def\bbU{\mathbb{U}}
\def\bbV{\mathbb{V}}
\def\bbW{\mathbb{W}}
\def\bbX{\mathbb{X}}
\def\bbY{\mathbb{Y}}
\def\bbZ{\mathbb{Z}}

%% mathfrack incomplete
\newcommand{\fE}{\mathfrak{E}}
\newcommand{\fD}{\mathfrak{D}}

%%
%%
%% \blot gives a square at end of formulas and proofs
%% (thanks to Marc Posner)
\def\blot{\quad {$\vcenter{\vbox{\hrule height.4pt
             \hbox{\vrule width.4pt height.9ex \kern.9ex \vrule
width.4pt}
             \hrule height.4pt}}$}}

%% End of latexmacro

%adjusting margins
%\usepackage{geometry}
%\geometry{paper=letterpaper, top=1.25 in, inner=1.25 in, outer=1.25 in, bottom=1 in, footskip=.5 in}

\textwidth 6.5 in \hoffset -.8in \textheight 9 in \voffset -.8in

\newcommand{\calW}{{\mathcal W}}
\newcommand{\bfd}{{\bf d}}
\newcommand{\bfT}{{\bf T}}
\newcommand{\bfA}{{\bf A}}
\newcommand{\bfB}{{\bf B}}
\newcommand{\bfX}{{\bf X}}
\newcommand{\bfU}{{\bf U}}
\newcommand{\bfY}{{\bf Y}}
\newcommand{\bfy}{{\bf y}}
\newcommand{\bfV}{{\bf V}}
\newcommand{\bfW}{{\bf W}}
\newcommand{\bfZ}{{\bf Z}}
\newcommand{\bfR}{{\bf R}}
\newcommand{\bfN}{{\bf N}}
\newcommand{\bfC}{{\bf C}}
\newcommand{\bfgamma}{{\boldsymbol \gamma}}
\newcommand{\rE}{\textrm{E}}
\newcommand{\E}{\bbE}
\newcommand {\cov}{\qopname\relax n{\textrm{Cov}}}
\newcommand {\var}{\qopname\relax n{\textrm{Var}}}
\newcommand {\corr}{\qopname\relax n{\textrm{Corr}}}
\newcommand {\Le}{\mathbb{L}}
\newcommand {\B}{{\mathcal{B}}}
\newcommand {\ind} {\overset{d}{=}}
\newcommand {\inD}{\overset{\mathcal{D}}{\rightarrow}}
\newcommand {\tod} {\overset{d}{\rightarrow}}
\newcommand {\topp} {\overset{p}{\rightarrow}}
\newcommand {\xn}{\{ X_n \}}
\newcommand {\fdd}{\overset{fdd}{\longrightarrow}}
\newcommand {\indf}[1]{{1}_{\{#1 \}}}
\newcommand {\argsup}[1]{\underset{#1}{\rm{argsup}}}
\newcommand {\wpsi}{{\psi}}
\newcommand {\D}[1]{\dot{#1}}
\newcommand {\revd}{\tilde{d}}
\newcommand {\revb}{\tilde{b}}
\newcommand {\revx}{\tilde{X}}
\newcommand {\fbar}{\overline{F}}
\newcommand {\xbar}{\overline{X}}
\newcommand {\ybar}{\overline{Y}}
\newcommand {\normal}{{\mathcal N}}
\newcommand {\khat}{\what{k}}
\newcommand {\that}{\what{\tau}}
\newcommand {\xhat}{\what{X}}
\newcommand {\yhat}{\what{Y}}
\newcommand {\Rhat}{\what{R}}
\newcommand {\oml}{\omega_l}
\newcommand {\lwhat}{\what{d}_{lw}}
\newcommand {\gphhat}{\what{d}_{gph}}
\newcommand {\cusum}{\mbox{CUSUM}}
\newcommand {\adjcusum}{\mbox{adjCUSUM}}
\newcommand {\hnull}{\mbox{H}_0}
\newcommand {\halt}{\mbox{H}_1}
\newcommand {\rhohat}{ \hat{\rho}}
\newcommand {\mhat}[1]{{#1}_t^{(m)}}
\newcommand {\pc}[1]{ #1^{\scriptsize pc}  }
\newcommand{\doublebar}[1]{\bar{\bar{#1}}}
\newcommand{\doubletilde}[1]{\tilde{\tilde{#1}}}
\newcommand{\doublehat}[1]{\hat{\hat{#1}}}
\newcommand{\doublecheck}[1]{\check{\check{#1}}}

%Younghoon Kim's personal setting. It can be modified.
\usepackage{multirow}
\providecommand{\keywords}[1]
{
{  \small	
  {\textit{Keywords---}} #1}
}
\usepackage{cleveref}
\crefname{equation}{equation}{equations}
\Crefname{equation}{Equation}{Equations}% For beginning \Cref
\crefrangelabelformat{equation}{(#3#1#4--#5#2#6)}
\crefmultiformat{equation}{equations (#2#1#3}{, #2#1#3)}{#2#1#3}{#2#1#3}
\Crefmultiformat{equation}{Equations (#2#1#3}{, #2#1#3)}{#2#1#3}{#2#1#3}
\linespread{1.5} %line Space
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{rotating}
\usepackage{tikz}




\begin{document}
\title{Group integrative dynamic factor models \\ 
for inter- and intra-subject brain networks}

\author[1]{Younghoon Kim}
\author[2]{Zachary F. Fisher}
\author[3]{Vladas Pipiras} 

\affil[1]{Cornell University}
\affil[2]{The Pennsylvania State University}
\affil[3]{University of North Carolina at Chapel Hill}

\date{\today}
\maketitle

\begin{abstract}
    This work introduces a novel framework for dynamic factor model-based data integration of multiple subjects, called GRoup Integrative DYnamic factor models (GRIDY). The framework facilitates the determination of inter-subject differences between two pre-labeled groups by considering a combination of group spatial information and individual temporal dependence. Furthermore, it enables the identification of intra-subject differences over time by employing different model configurations for each subject. Methodologically, the framework combines a novel principal angle-based rank selection algorithm and a non-iterative integrative analysis framework. Inspired by simultaneous component analysis, this approach also reconstructs identifiable latent factor series with flexible covariance structures. The performance of the framework is evaluated through simulations conducted under various scenarios and the analysis of resting-state functional MRI data collected from multiple subjects in both the Autism Spectrum Disorder group and the control group.
\end{abstract}

\keywords{Data integration, high-dimensional time series, principal angles, dynamic factor model, multi-way analysis, fMRI.}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{se:intro}


\subsection{Statistical analysis of brain connectivity}
\label{se:connectivity}

The analysis of brain connectivity is to mainly draw a map that shows which regions of the brain are activated by specific tasks. Discovering such sets of interconnected regions includes structural connectivity (a network of anatomical or structural connections linking distinct units), and functional connectivity (capturing statistical dependence between distinct units). The unit can be individual neurons, neuronal populations, and anatomical regions of interest (ROIs). In addition to the statistical dependence, one may further look into causal interactions between these units (effective connectivity) \cite[see][for further explanation]{sporns:2007}. 


To explore functional connectivity, we use functional magnetic resonance imaging (fMRI), which consists of blood-oxygen-level dependent (BOLD) signals, images observed over time. Each BOLD level is mapped to pre-specified ROIs via the atlas. The strength of this signal is regarded as a proxy for brain activity since the blood flow of the area of interest increases when the corresponding regions become active. Depending on the research goal, one may focus on establishing a statistical model to localize brain areas activated by the task, or building a prediction model about psychological or disease states for medical purposes. For these purposes, task-based fMRI (T-fMRI), a series of BOLD signals acquired while the subject performs a set of tasks, is often used. In making inferences about the structure of relationships among brain regions, across time points, and between subjects, resting-state fMRI (R-fMRI) can be leveraged \cite[e.g.][]{smith:2004}. Our study will focus on the analysis of R-fRMI.


After the R-fMRI at each time point is realigned to a reference image to reduce the the effects of subject's movement, two kinds of questions can be cast for statistical brain network analysis. The first question is how to construct a network representation that reveals the spatial and temporal correlations between ROIs. This concerns generating a functional map called the state of connectivity. The most active research related to this question is to discover different connectivity patterns between (group of) subjects. The other question is how to describe the connectivity of ROIs by establishing a mathematical model. Mostly the task is related to discovering the effective connectivity, such as causal inference, of the brain network \cite[e.g.][]{lindquist:2008}. 


The promising statistical tool for addressing the first type of question is called data integration, which is designed to study differences in structures across different subjects. Often, heterogeneity is due to either observed factors (e.g. demographic, disease status, experiments) or be hidden, but which can be revealed through statistical modeling. The observations for such studies form multi-block data (or multi-view data), and these data structures naturally motivate the simultaneous exploration of the joint and individual variations across and within data blocks, which can result in new insight. However, this approach often discounts temporal dependence and shows limitations in explaining directed connections. On the other hand, the second question can be dealt with network modeling. For example, one may leverage structural equation modeling (SEM) that contributes to discovering the causal relationships between ROIs. Alternatively, statistical temporal dependence can be reformulated through random graphical models. However, this approach has a limitation in characterizing different signal variations across the subjects and representing different model configurations \cite[e.g.][and reference therein]{lee:2013}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Contributions}
\label{se:contribution}


We propose an estimation procedure for GRoup Integrative DYnamic factor models, called GRIDY for short. There are several important points to note about GRIDY. Firstly, we can identify model configurations for spatial and temporal information. The distinction of model parameters contributes to explaining inter-differences between subjects through combinations of various loading matrices and factors. Secondly, we allow the factor series to evolve over time. Constructing the estimated scores into the dynamic series is important because it enables modeling of dynamic temporal dependences and using them for prediction. This also explains the intra-subject differences over time. Thirdly, by allowing flexible covariance structures of factor series, the model can capture the heterogeneity of the subjects through different scales and combinations of factor series. Thus, along with loading matrices that differ for each group, the identified factor dynamics explain spatial and temporal dependences. Lastly, we analyze R-fMRI data from multiple subjects from a time series perspective. The main goal of this analysis is to characterize the dependence between and within ROIs. Our model provides the brain networks estimated from multiple subject R-fMRI that explain the inter-subject and intra-subject differences.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Related approaches}
\label{se:related}


From a methodological standpoint, two directions are related to our proposed approach. One is integrative data analysis. It has been employed in various fields such as computational biology, chemometrics, and even some areas of neuroscience. It originated from partial least squares (PLS) or canonical correlations analysis (CCA) in finding the maximum covariance (correlation, respectively) between sample projections on the canonical loadings and the canonical scores from different subjects \cite[e.g.][]{witten:2009,lofstedt:2013}. A variant of principal component analysis (PCA) for finding the direction presenting a maximal variation from the center has been also leveraged for multi-block data \cite[e.g.][]{abdi:2013}. 


The data integration approach has been developed to segment joint structures from individual structures by matrix factorizations, which will be discussed in detail in Section \ref{sse:segmentation}. The recent development tends to include the granular segment of multi-block data \cite[e.g.][]{o:2019,park:2020} and introduce external covariates added to loading matrices \cite[][]{gao:2021}. However, as pointed out in Section \ref{se:connectivity}, these approaches do not consider temporal dependence. 


A similar data integration approach has been applied to neuroimaging studies. For example, \cite{yu:2017} analyzed behavioral and T-fMRI data. \cite{murden:2022} applied their model to R-fMRI and diffusion MRI (dMRI) to find similarities between functional connectivity and structural connectivity. Their approach extracts common structures from different datasets, where the heterogeneity comes from the source of data, not the factors uniquely embedded in subjects. Furthermore, the approaches share the same limitation of disregarding temporal dependences.


Some graphical models incorporating temporal dependence have been considered for multiple subjects. For example, \cite{qiu:2016} used a kernel-based method to represent smooth changes in the dependence over time. Similarly, \cite{qiao:2019} introduced Gaussian random functions to model the temporal dependence. \cite{zhu:2018} introduced matrix-valued variables to represent the spatial and temporal dependence simultaneously. \cite{fan:2018} added external covariates to loading matrices in factor models to construct graphical models from the residuals. Even if they are model-based, the graphical models cannot be used to understand the directional (causal) relationships between variables.


The time series models have been used to present directed and lagged relationships between ROIs \cite[e.g.][]{chen:2011,gates:2012}. Recently, \cite{skripnikov:2019,manomaisaowapak:2022} used combinations of different penalized estimations of VAR models to represent the heterogeneity across subjects by specifying the joint and individual structures. Although their models incorporate temporal dependence, their joint structures need not be identical, which has limitations in explaining the observed phenomenon. \cite{fisher:2023} resolved this issue by imposing the additivity to the transition matrices, so the model is able to produce the estimates identical across subjects while having the rest of the components individually specified. But VAR-based modeling requires a relatively large computation cost compared to other dimension reduction methods. 


Hence, we use dynamic factor models (DFMs) with the data integration framework. DFMs have low-dimensional representations and enhance explainability, and their estimation procedure has usually low computation costs. Their usefulness has already been demonstrated in fields such as psychometrics \cite[e.g.][]{molenaar:2017,epskamp:2020}. Our study extends the application of DFMs to a single subject by fitting R-fMRI data from multiple subjects to present the inter- and intra-subject differences. By reconstructing the latent factor structures, we also explore the heterogeneity among the subjects.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Outline of study}
\label{sse:outline}

The rest of the manuscript is organized as follows. In Section \ref{se:model_procedure}, we introduce our model and summarize our procedure. In Section \ref{se:method}, we describe the segmentation of multi-block data and discuss the identification of structures. Then, we introduce the estimation method for the covariance of the latent factors and the method to reconstruct the dynamics of factor series. In Section \ref{se:illustrative}, we provide simulation results under various settings that support the approach empirically. In Section \ref{se:data_app}, a joint analysis of resting-state R-fMRI data collected from multiple individuals is conducted and leads to the functional connectivity networks of brain regions. In Section \ref{se:discussion}, we summarize the results and suggest possible extensions.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Proposed model and procedure}
\label{se:model_procedure}


\subsection{Population model}
\label{sse:model}


Assume that we divide all individuals into $G$ non-overlapping and different groups and assume there are $K_1,K_2,\ldots,K_G$ number of subjects in each group. To simplify the notation as well as to use the model for our purpose, we consider only two groups, $G=2$, and suppose each group contains an equal number of subjects so that the first $K$ subjects belong to the first group and the remaining $K$ subjects are in the second group. It is possible to extend the number of groups to be more than two, as well as possibly consider an unequal number of subjects in each group.


The observations from the $k$th subject are as follows. Define the $T_k\times d$ dimensional observation matrix $\mathbf{X}_{k}=(X_{i,t}^{(k)})_{i=1,\ldots,d,\ t=1,\ldots,T_k}$ to represent $k$th subject, and $d$ variables with the observations collected over $T_k$ times. We assume that the temporal evolution of $k$th subject observations is explained by $r$ latent factor series, in the presence of the additive noise $\mathbf{E}_{k}\in\mathbb{R}^{T_k\times d}$. More specifically, the model is defined as
%
\begin{equation} \label{e:observation_k}
    \mathbf{X}_{k} = 
    \left\{\begin{array}{ll}
      \mathbf{F}_k\mathbf{B}_1'+\mathbf{E}_{k}, & \quad k=1,\ldots,K, \\
      \mathbf{F}_k\mathbf{B}_2'+\mathbf{E}_{k}, & \quad k=K+1,\ldots,2K,
    \end{array}\right.
\end{equation}
%
where $\mathbf{F}_k\in\mathbb{R}^{T\times r}$ are factor series of $k$th subject and the corresponding loadings matrices are denoted as $\mathbf{B}_1,\mathbf{B}_2\in\mathbb{R}^{d \times r}$ so that the components $\mathbf{F}_k\mathbf{B}_1'$ and $\mathbf{F}_k\mathbf{B}_2'$ construct the \textit{block signals} of $k$th subject in each group, respectively. 


By stacking up all multi-block observations, we have a large matrix of blocks in \eqref{e:observation_k} as follows:
%
\begin{equation}\label{e:multi_block_AJIVE}
            \begin{bmatrix}
                    \mathbf{X}_1 \\
                    \vdots \\
                    \mathbf{X}_{2K} \\
                 \end{bmatrix} 
            = \begin{bmatrix}
                    \mathbf{Y}_1 \\
                    \vdots \\
                    \mathbf{Y}_{2K} \\
                 \end{bmatrix} 
+            \begin{bmatrix}
                    \mathbf{E}_1 \\
                    \vdots \\
                    \mathbf{E}_{2K} \\
                 \end{bmatrix}
              = \begin{bmatrix}
                    \bar{\mathbf{X}}_1 \\
                    \vdots \\
                    \bar{\mathbf{X}}_{2K} \\
                 \end{bmatrix} 
               + \begin{bmatrix}
                    \tilde{\mathbf{X}}_1 \\
                    \vdots \\
                    \tilde{\mathbf{X}}_{2K} \\
                 \end{bmatrix}
               + \begin{bmatrix}
                    \mathbf{E}_1 \\
                    \vdots \\
                    \mathbf{E}_{2K} \\
                 \end{bmatrix},
\end{equation}
%
where $\bar{\mathbf{X}}_k$ refers to the \textit{joint components} of $k$th subject and $\tilde{\mathbf{X}}_k$ refers to the \textit{group individual components} of $k$th subject, which form the joint structure and group individual structures, respectively. Each large matrix consists of $n=\sum_{k}T_k$ samples in time and $d$ variables, usually $n > d$. Note that the number of variables is set to be identical across all subjects while we allow the number of observations $T_k$ of each subject to be possibly different.


Through suitable analysis, we are also interested in the separation of contributions for explaining the variability of the data jointly and individually. Among the block signals of $k$th subject in $g$th group $\mathbf{F}_k\mathbf{B}_g'$, we assume that this block can be decomposed into the joint components and group individual components, 
%
\begin{equation}\label{e:augmented_block1}
        \mathbf{F}_k\mathbf{B}_g' 
        = \bar{\mathbf{F}}_k \bar{\mathbf{B}}^{'} + \widetilde{\mathbf{F}}_k \widetilde{\mathbf{B}}_g^{'},
\end{equation}
%
where $\bar{\mathbf{B}}=(\bar{B}_{i,j})_{i=1,\ldots,d,\ j=1,\ldots,r_{J}}$ is the \textit{joint loading matrix} identical to all $2K$ subjects, $\tilde{\mathbf{B}}_{g}=(\tilde{B}_{i,j}^{(g)})_{i=1,\ldots,d,\ j=1,\ldots,r_{G}}$, $g=1,2$, are \textit{group loadings matrices} only shared through the subjects within each group, $\bar{\mathbf{F}}_k \in \mathbb{R}^{T_k \times r_J}$ are \textit{joint factor series} and $\widetilde{\mathbf{F}}_k$ are \textit{group individual factor series}. We set $r=r_J+r_G$. Note that the ranks of each of the group individual factor series need not be identical; one can set the numbers of individual factors for groups 1 and 2 as $r_{G_1}\neq r_{G_2}$. However, to simplify the model description, we limit these quantities to be identical. 



To construct DFMs, for each $k$, the $d-$dimensional vector of observation at time $t$, denoted as $X_{t}^{(k)}$, is explained by the joint component vector $\bar{\mathbf{B}} \bar{F}_{t}^{(k)}$ and group individual component vector $\tilde{\mathbf{B}}_{g}\tilde{F}_{t}^{(k)}$, $g=1,2$, where
%
\begin{eqnarray}
    X_{t}^{(k)} 
    &=& \mathbf{B}_{g}F_{t}^{(k)} + E_{t}^{(k)} \label{e:augmented_factor_1} \\
    &=& \begin{bmatrix} \bar{\mathbf{B}} & \tilde{\mathbf{B}}_{g} \end{bmatrix}
    \begin{bmatrix} \bar{F}_{t}^{(k)} \\ \tilde{F}_{t}^{(k)} \end{bmatrix} + E_{t}^{(k)} \\
    &=& \bar{\mathbf{B}} \bar{F}_{t}^{(k)} + \tilde{\mathbf{B}}_{g}\tilde{F}_{t}^{(k)} + E_{t}^{(k)} \label{e:augmented_factor_2}\\
    &=& \bar{X}_{t}^{(k)} + \tilde{X}_{t}^{(k)} + E_{t}^{(k)},\ g=1,2,\ k=1,\ldots,2K, \label{e:augmented_factor_3}
\end{eqnarray}
%
and $E_{t}^{(k)}=(E_{i,t}^{(k)})_{i=1,\ldots,d}$ is a noise vector with zero mean and variance $\boldsymbol{\Sigma}_{E,k}$. To connect \eqref{e:augmented_factor_1}--\eqref{e:augmented_factor_3} with \eqref{e:observation_k} and \eqref{e:augmented_block1}, we define $X_{t}^{(k)}$ as being explained by a linear combination of the $r_J-$dimensional joint factor $\bar{F}_{t}^{(k)}=(\bar{F}_{j,t}^{(k)})_{j=1,\ldots,r_{J}}$ and $r_G-$dimensional group individual factor $\tilde{F}_{t}^{(k)}=(\tilde{F}_{j,t}^{(k)})_{j=1,\ldots,r_{G}}$. In summary, the model is defined as
%
\begin{eqnarray} 
    \mathbf{X}_{k} &=& \begin{bmatrix}
                        X_1^{(k)'} \\
                        \vdots \\ 
                        X_{T_k}^{(k)'}
                        \end{bmatrix}  = \bar{\mathbf{X}}_{k} + \tilde{\mathbf{X}}_{k} + \mathbf{E}_{k}
                = \begin{bmatrix}
                        \bar{X}_1^{(k)'} \\
                        \vdots \\ 
                        \bar{X}_{T_k}^{(k)'}
                    \end{bmatrix} 
                + \begin{bmatrix}
                    \tilde{X}_1^{(k)'} \\
                     \vdots \\ 
                     \tilde{X}_{T_k}^{(k)'}
                \end{bmatrix}
               + \begin{bmatrix}
                    E_1^{(k)'} \\
                        \vdots \\ 
                    E_{T_k}^{(k)'}
                \end{bmatrix} \label{e:matrix_joint_individual1} \\
                &=& \mathbf{F}_k\mathbf{B}_g'+\mathbf{E}_{k} 
            = \begin{bmatrix} 
                    F_1^{(k)'} \\
                    \vdots \\ 
                    F_{T_k}^{(k)'}
                \end{bmatrix}
                \begin{bmatrix}
                B_1^{(g)'} & \ldots &  B_{d}^{(g)'}
                \end{bmatrix}
                + \begin{bmatrix}
                E_1^{(k)'} \\
                \vdots \\ 
                E_{T_k}^{(k)'}
                \end{bmatrix},\label{e:matrix_joint_individual2}
\end{eqnarray}
where the block signal for $k$th subject is 
%
\begin{equation} \label{e:augmented_block2}
        \mathbf{F}_k\mathbf{B}_g' 
        = \begin{bmatrix}
        \bar{F}_1^{(k)'} \\
        \vdots \\ 
        \bar{F}_{T}^{(k)'}
        \end{bmatrix}
        \begin{bmatrix}
        \bar{B}_1^{'} & \ldots &  \bar{B}_{d}^{'}
        \end{bmatrix} 
        + \begin{bmatrix}
        \tilde{F}_1^{(k)'} \\
        \vdots \\ 
        \tilde{F}_{T}^{(k)'}
        \end{bmatrix}
        \begin{bmatrix}
        \tilde{B}_1^{(g)'} & \ldots &  \widetilde{B}_{d}^{(g)'}
        \end{bmatrix}. 
\end{equation}
%

 To complete the construction of the latent factor series, assume that VAR($p$) model governs the joint factor series, that is,
%
\begin{equation}\label{e:filter_equation}
    \bar{\boldsymbol{\Psi}}_{k}(L)\bar{F}_{t}^{(k)} = \bar{\eta}_{t}^{(k)},\quad \left\{\bar{\eta}_{t}^{(k)}\right\} \sim \mathrm{WN}(0,\boldsymbol{\Sigma}_{\bar{\eta},k}),
\end{equation}
%
where $\bar{\boldsymbol{\Psi}}_{k}(L)=I - \bar{\boldsymbol{\Psi}}_{1,k}L - \ldots - \bar{\boldsymbol{\Psi}}_{p,k}L^p$ is a $r_J \times r_J$ operator of finite length $p$ with a lag operator $L$. The root of $\textrm{det}(\bar{\boldsymbol{\Psi}}_{k}(z))$ lies outside the unit circle so that the series becomes stable. Note that, unlike unspecified properties of the noise at the observation level, we quantify the noise for latent factor series to be white noise. The same construction is made for the group individual factor series.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Procedure of GRIDY}
\label{sse:Procedure}

We summarize the procedure of GRIDY. In the first step, we decide on the rank $r$ of block signals from block observations $\mathbf{X}_1,\ldots,\mathbf{X}_{2K}$ by applying rotational bootstrapping described in Section \ref{ssse:initial_rank}. Next, we apply the algorithm that obtains the joint components $\bar{\mathbf{X}}_1,\ldots,\bar{\mathbf{X}}_{2K}$ and two group individual components $\tilde{\mathbf{X}}_1,\ldots,\tilde{\mathbf{X}}_{K}$ and $\tilde{\mathbf{X}}_{K+1},\ldots,\tilde{\mathbf{X}}_{2K}$ from the whole signal blocks via a principal angle-based block segmentation algorithm. This is explained in Section \ref{sse:segmentation}. Third, the algorithm that factorizes the obtained block structures into the loading matrices $\bar{\mathbf{B}},\tilde{\mathbf{B}}_1,\tilde{\mathbf{B}}_2$ and the factor series $\bar{\mathbf{F}}_1,\ldots,\bar{\mathbf{F}}_{2K}$, $\tilde{\mathbf{F}}_1,\ldots,\tilde{\mathbf{F}}_{K}$, and $\tilde{\mathbf{F}}_{K+1},\ldots,\tilde{\mathbf{F}}_{2K}$ designed for multiple subject component models is applied for each block structure. In the final step, we refit the estimate of loading matrices and factor series to observation blocks by regression and complete the reconstruction of factor dynamics by the Yule-Walker equations. The details of the last two steps are included in Sections \ref{sse:reconstruction} and \ref{ssse:estimation}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Methods}
\label{se:method}


\subsection{Segmentation of joint and individual variation}
\label{sse:segmentation}


Joint individual variation explained \cite[JIVE;][]{lock:2013} firstly used the idea of singular value decomposition (SVD) to segment the observations into the joint and individual structures through low-rank approximations. In this approach, the desired structures are found by minimizing the sums of squared residuals to minimize the unexplained variability. However, a permutation-based rank selection for the joint structure has no guarantee to achieve the true rank, in particular when there is some correlation across individual structures. Common orthogonal basis extraction \cite[COBE;][]{zhou:2015} improves the basis extraction in JIVE in a systematic way by adding orthogonality constraints of joint and individual loadings. However, the algorithm is still iterative and the rank selection is not supported theoretically.


Recently, a non-iterative integrative data analysis framework, namely angle-based joint individual variation explained \cite[AJIVE;][]{feng:2018}, was developed by decomposing the multi-block data into the desired structures. Unlike the previous work that determined joint structure by enforcing orthogonality between the joint structure and the individual structure, the idea of principal angle analysis (PAA) has been employed in AJIVE. Recall that the principal angle is measured for two subspaces. It is known that when the angle formed by two subspaces is small, the approximation of one subspace for the other has a large singular value. The AJIVE approach is to measure the largest principal angle between the subspace generated by true signal blocks and their low-rank approximations, whose rank varies from one to its attainable maximum. \cite{feng:2018} introduced two upper bounds of the measured angles. First, the Wedin bound used to theoretically justify the largest allowable principal angle of the joint structure can be estimated by using orthogonal columns and rows of SVD. Then, one can choose the rank of the joint structure by counting the number of principal angles thresholded below this bound. However, as  \cite{feng:2018} pointed out and their example showed, Wedin bound becomes too conservative for over-specified ranks of the approximation. Intuitively, by Corollary 3 in \cite{feng:2018}, the upper bound of (pseudo-)principal angle between the true signal and its approximation involves $\hat{\mathbf{U}}_k$ and $\hat{\mathbf{V}}_k$, the matrices of orthogonal columns from rank-$\hat{r}$ approximation $\hat{\mathbf{Y}}_k = \hat{\mathbf{U}}_k\hat{\mathbf{D}}_k\hat{\mathbf{V}}_k'$. A result from a related experiment is presented in Section 2.2.2 of \cite{feng:2018}. The issue becomes critical when dealing with multiple subjects' block observations, where $n$ often exceeds $d$ and the block observations are far from square. To address this issue, we introduce a second bound, called the random direction bound, which replaces the orthogonal columns and rows of the singular value decomposition (SVD) with randomly generated orthogonal matrices. If any principal angles or even the Wedin bound exceeds this random direction bound, the user must ignore that number of singular values.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsubsection{Discussion on identifiability}
\label{ssse:identifiability}


Note that the model \eqref{e:augmented_factor_1}--\eqref{e:augmented_factor_3} should ideally possess two kinds of identifiability conditions. First condition is to ensure that the factor model in \eqref{e:augmented_factor_1} can be uniquely decomposed into the joint components $\bar{\mathbf{F}}_{k}\bar{\mathbf{B}}'$ and the group individual components $\tilde{\mathbf{F}}_{k}\tilde{\mathbf{B}}_g^{'}$ as in \eqref{e:augmented_factor_2}. To distinguish this condition from the next one, we call it an identifiability condition. Another important condition is to ensure that the loadings matrices $\bar{\mathbf{B}},\tilde{\mathbf{B}}_1,\tilde{\mathbf{B}}_{2}$ and the factor series $\bar{F}_{t}^{(k)},\tilde{F}_{t}^{(k)}$ can also be identified. We will see that unlike for the first condition, these loadings matrices and factor scores are not identifiable until additional assumptions are imposed. We shall refer to determinacy as the condition for identifying the loadings and the factor series in the components of our decomposition. This condition will be discussed in Section \ref{sse:reconstruction}.


The identification of the first two block structures in \eqref{e:multi_block_AJIVE} can be obtained by checking suitable identifiability conditions of AJIVE. The following lemma adapted from \cite{feng:2018} to our setting shows that under certain conditions, the uniqueness of the decomposed blocks can be achieved. To discuss the condition below, we define a subspace spanned by joint loading matrix as 
%
\begin{equation}
    \textrm{row}(\bar{\mathbf{X}}_{1}) = \ldots = \textrm{row}(\bar{\mathbf{X}}_{2K}) =: \textrm{row}(\bar{\mathbf{X}}).
\end{equation}
%


\begin{lem}[\cite{feng:2018}]\label{lem:feng}
Given a set $\{\mathbf{Y}_1,\ldots,\mathbf{Y}_{2K}\}$ of matrices, there are unique sets $\{\bar{\mathbf{X}}_1,\ldots,\bar{\mathbf{X}}_{2K}\}$ and $\{\tilde{\mathbf{X}}_1,\ldots,\tilde{\mathbf{X}}_{2K}\}$ of matrices so that
\begin{enumerate}
    \item $\mathbf{Y}_k = \bar{\mathbf{X}}_k + \tilde{\mathbf{X}}_k$ for all $k=1,\ldots,2K$.
    
    \item $\mathrm{row}(\bar{\mathbf{X}}_k)=\mathrm{row}(\bar{\mathbf{X}}) \subset \mathrm{row}(\mathbf{Y}_k)$ for all $k=1,\ldots,2K$.
    
    \item $\mathrm{row}(\bar{\mathbf{X}}) \perp \mathrm{row}(\tilde{\mathbf{X}}_k)$ for all $k=1,\ldots,2K$.
    
    \item $\bigcap_{k=1}^{2K} \mathrm{row}(\tilde{\mathbf{X}}_k)=\{\mathbf{0}\}$.
\end{enumerate}
\end{lem}

Condition 1 corresponds to our setting of interest. Note that each row space $\mathrm{row}(\bar{\mathbf{X}}_k)$ is defined by the loadings, specifically by the linear combinations of entries of loadings matrices multiplied by the factor scores. Also, note that the row subspaces of the joint components are spanned by the same loading matrix, that is, $\textrm{row}(\bar{\mathbf{X}}_{k}) = \textrm{row}(\bar{\mathbf{F}}_{k}\bar{\mathbf{B}}') = \textrm{row}(\bar{\mathbf{B}}') = \textrm{row}(\bar{\mathbf{X}})$ since the subspace is represented by a linear combination of basis columns in the loading matrix, where the coefficients are the factor scores, which rarely have the same values across different rows. Hence, condition 2 is satisfied assuming $\bar{\mathbf{X}}_k = \bar{\mathbf{F}}_k\bar{\mathbf{B}}'$ in our setting. Consider next condition 3. It assumes that for fixed $k$, $\bar{\mathbf{X}}_k \tilde{\mathbf{X}}_k^{'} = \bar{\mathbf{F}}_k\bar{\mathbf{B}}'\tilde{\mathbf{B}}_g\tilde{\mathbf{F}}_k' = \mathbf{0}_{T_k \times T_k}$, $g=1,2$, and is implied by $\bar{\mathbf{B}}'\tilde{\mathbf{B}}_1 = \bar{\mathbf{B}}'\tilde{\mathbf{B}}_2 = \mathbf{0}_{r_J \times r_G}$. This means that all columns in both of group loadings matrices are perpendicular to the corresponding columns of joint loadings matrix. The easiest way to achieve this condition is when the non-zero rows of joint loadings matrix $\bar{\mathbf{B}}$ do not overlap with any other rows of both group loadings matrices $\tilde{\mathbf{B}}_g$, $g=1,2$. Interestingly, this condition implies that the rows can be shared between group loadings matrices. Lastly, condition 4 states that there are no rows shared by all group loadings matrices. This is implied by our model where 
$\mathrm{row}(\tilde{\mathbf{B}}_1')\cap\mathrm{row}(\tilde{\mathbf{B}}_2')=\{0\}$. That is, the blocks span different subspaces depending on groups. We thus identified natural condition on the elements of our model $(\bar{\mathbf{B}},\tilde{\mathbf{B}}_1,\tilde{\mathbf{B}}_{2})$ so that its components are the unique components of the decomposition given in Lemma \ref{lem:feng}.


We also note that the identifiability condition can be summarized as 
%
\begin{equation}\label{e:block_diagonal}
    \mathbf{B}_g'\mathbf{B}_g 
    = \begin{pmatrix}
    \bar{\mathbf{B}}'\bar{\mathbf{B}} & \mathbf{0}_{r_J \times r_G } \\
    \mathbf{0}_{r_G \times r_J } & \tilde{\mathbf{B}}_g'\tilde{\mathbf{B}}_g \\
    \end{pmatrix}, \quad g=1,2.
\end{equation}
%
As we will discuss in Section \ref{sse:reconstruction}, consideration of conditions on loading matrices is not important to obtain the determinacy of factor series.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsubsection{Discussion on initial rank selection}
\label{ssse:initial_rank}


AJIVE requies that the initial rank $r=r_J + r_G$ for each block should be given. We admit that when the number of subjects becomes larger, it is delicate to decide on the initial rank through a graphical tool such as a scree plot. Also, even if the cross-validation-based rank selection algorithm \cite[e.g.][]{bro:2003} can be applied, our model incorporates several distinct structures in the joint and two group structures, and each structure is related to the other. Furthermore, our model usually deals with a massive number of subjects. This makes the implementation of the cross-validation delicate.



Instead, we apply a bootstrapping-based algorithm by \cite{prothero:2022}. The key idea is as follows. For an arbitrary vector, if it lies in the space spanned by the true block signals, the angle between them is zero. The true subspace, however, is unobservable, but a low-rank approximation of the block signals can replace the subspace. Also, since the angle itself cannot be observed, one constructs a perturbation angle bound which defines an angle between subspaces spanned by the true signals and their approximation. 


To be specific, let $v$ be an arbitrary vector in the whole row space, say $v \in \mathbb{R}^{d}$ since we will apply this algorithm to each block $\mathbf{X}_k$. We drop the subscript $k$ for the subject number to simplify notation. A similar exploration can be made for the column space belonging to $\mathbb{R}^{T_k}$ by replacing $v$, $\mathbf{V}$, $\check{\mathbf{V}}$ with $u$, $\mathbf{U}$, $\check{\mathbf{U}}$ as well. Suppose we have two row subspaces spanned by the true signals and their approximation, denoted by $\textrm{P}(\mathbf{Y})$ and $\textrm{P}(\check{\mathbf{Y}})$. We project this vector into each subspace to have $v_{\textrm{proj}}$ and $\check{v}_{\textrm{proj}}$, respectively. Then one has two angles,
%
\begin{equation}\label{e:theta}
    \theta = \arccos\left(\frac{ \langle v,v_{\textrm{proj}} \rangle}{\|v\|_2 \|v_{\textrm{proj}}\|_2}\right),\quad 
    \check{\theta} = \arccos\left(\frac{ \langle v,\check{v}_{\textrm{proj}} \rangle}{\|v\|_2 \|\check{v}_{\textrm{proj}}\|_2}\right),
\end{equation}
%
where $v_{\textrm{proj}} = \mathbf{V}\mathbf{V}'v$, $\check{v}_{\textrm{proj}} = \check{\mathbf{V}}\check{\mathbf{V}}'v$, and $\mathbf{V},\check{\mathbf{V}}$ are the orthonormal basis matrice of $\textrm{P}(\mathbf{Y})$ and $\textrm{P}(\check{\mathbf{Y}})$, respectively. Now, one can think of additional two angles: $\theta_1$, an angle between $v_{\textrm{proj}}$ and $\textrm{P}(\check{\mathbf{Y}})$, and $\theta_2$, an angle between $\check{v}_{\textrm{proj}}$ and $\textrm{P}(\mathbf{Y})$, computed by
%
\begin{equation}
    \theta_1 = \arccos\left(\frac{ \| \check{\mathbf{V}}'\mathbf{V}\mathbf{V}'v \|_2}{\|\mathbf{V}\mathbf{V}'v\|_2 }\right),\quad 
    \theta_2 = \arccos\left(\frac{ \| \mathbf{V}'\check{\mathbf{V}}\check{\mathbf{V}}'v \|_2}{\|\check{\mathbf{V}}\check{\mathbf{V}}'v\|_2 }\right).
\end{equation}
%
Note that although both $\theta$ and $\mathbf{Y}$ are unknown so is $\mathbf{V}$, we can obtain $\check{\mathbf{V}}$ from the approximation. Thus, $\theta_2$ can be estimated when $\mathbf{V}'\check{\mathbf{V}}$ becomes known and is computed via bootstrapping. Furthermore, the result in Section 5.2 in \cite{jiang:2018} implies 
%
\begin{equation}\label{e:inequality_angle}
    \max(\check{\theta} - \theta_1,0) \leq \theta \leq \check{\theta}+\theta_2.
\end{equation}
%
Here, if $v$ is close to $\textrm{P}(\check{\mathbf{Y}})$, $\theta$ becomes similar to $\theta_2$ and $\check{\theta}$ is negligible. \eqref{e:inequality_angle} thus implies that the lower bound of $\theta$ may be less informative, but the upper bound can be tight. Therefore, we design the bootstrap algorithm described below to estimate $\check{\theta}+\theta_2$ as an upper bound of $\theta$ in \eqref{e:theta}. Accordingly, the bootstrap estimate requires the random direction angle bound, the upper bound of the estimate. The algorithm chooses a random directional angle bound, denoted as $\theta_0$, by the low percentile of the null distribution. Precisely, it is measured by the 5\% percentile of replicated angles computed with $d$-dimensional standard normal random noise and its extraction of $\check{r}$ entries, where $\check{r}$ is explained below.


Now we describe the rotational bootstrapping algorithm. The essential idea is that singular values of the observation block can contain all information required to extract the signals from the perturbation by noise \cite[see Section 2 in ][]{shabalin:2013}. First, we demonstrate how to decide on the candidate initial low ranks. We drop the subscript $k$ for the subject number again for simplicity. First, given a data block $\mathbf{X}=\mathbf{Y}+\mathbf{E}$, we obtain the full SVD result by $\mathbf{X}=\mathbf{U}\mathbf{S}\mathbf{V}'$, where $\mathbf{U},\mathbf{V}$ consists of orthogonal columns $U_i$ and $V_i$. By using singular values $\nu_1,\ldots,\nu_{d\wedge T}$ of $\mathbf{X}$, one has the estimate of signal matrix by 
%
\begin{equation}
    \check{\mathbf{Y}} 
    = \sum_{i=1}^{d \wedge T} \check{\sigma}h^{*}(\nu_{i}/\check{\sigma})U_{i}V_{i}'
    =: \sum_{i=1}^{d \wedge T} \check{\nu}_i U_{i}V_{i}',
\end{equation}
%
where $\check{\sigma}=\frac{\nu_{\textrm{med}}}{\sqrt{MP(\beta)_{0.5}}}$ for $\nu_{\textrm{med}} = \textrm{med}(\nu_1,\ldots,\nu_{d\wedge T})$, and $MP(\beta)_{q}$ is the 100$q$th percentile (hence, the median for $\check{\sigma}$) of the Marchenko-Pastur distribution with parameter $\beta$, called aspect ratio $\beta=\frac{d \wedge T}{d \vee T}$ \cite[e.g.][]{gavish:2014}. The corresponding thresholding function is defined as
%
\begin{equation}\label{e:thresholding_function}
    h^{*}(a) = \left\{\begin{array}{ll}
      \frac{1}{\sqrt{2}}\sqrt{a^2-\beta-1+\sqrt{(a^2-\beta-1)^2-4\beta}} ,  & \textrm{if }a\geq 1+\sqrt{\beta}, \\
      0,   & \textrm{if } a<1+\sqrt{\beta}.
    \end{array}\right.
\end{equation}
%
Thus, through shrinking the scaled singular values, we obtain the maximum initial rank estimate by the number of scaled non-zero singular values, denoted by $\check{r}$. Then by letting $\check{\mathbf{U}}$ and $\check{\mathbf{V}}$ be the matrices consisting of the first $\check{r}$ columns of $\mathbf{U}$ and $\mathbf{V}$, we have $\check{\mathbf{Y}} = \check{\mathbf{U}}\check{\mathbf{S}}\check{\mathbf{V}}'$, where $\check{\mathbf{S}} = \textrm{diag}(\check{\nu}_1,\ldots,\check{\nu}_{\check{r}})$ is a $\check{r}$-dimensional square and diagonal matrix. This completes the pre-step for rotational bootstrapping.


From the pre-step, we have the rank-$\check{r}$ approximation of the data block $\mathbf{X}$. If we were aware of the true signal block, we could take rank-$\check{r}$ approximation via compact SVD, $\mathbf{Y} = \hat{\mathbf{U}}\hat{\mathbf{S}}\hat{\mathbf{V}}'$ and calculate the angles between $\hat{\mathbf{U}}$ and $\check{\mathbf{U}}$, and $\hat{\mathbf{V}}$ and $\check{\mathbf{V}}$. Instead, by using the singular values obtained at the pre-step, one can generate the random replication of $\mathbf{X}$. This forms the outer loop of rotational bootstrapping. Thus, during the large number $M$ of repetitions, we generate $\doublehat{\mathbf{X}} = \doublehat{\mathbf{U}}\check{\mathbf{S}}\doublehat{\mathbf{V}}'+\check{\mathbf{E}}$ for every iteration, where $\doublehat{\mathbf{U}}$ and $\doublehat{\mathbf{V}}$ are randomly generated $T \times \check{r}$ and $d \times \check{r}$ orthogonal matrices and the estimate $\check{\mathbf{E}}$ of the noise matrix $\mathbf{E}$ is computed by 
%
\begin{equation}
    \check{\mathbf{E}} 
    = \sum_{i=1}^{\check{r}}\check{\sigma}\textrm{MP}(\beta)_{m_i}U_{i}V_{i}' + \sum_{i=\check{r}+1}^{d\wedge T}\nu_iU_{i}V_{i}',
\end{equation}
%
where $m_{1},\ldots,m_{\check{r}}$ are i.i.d. uniform random variables. This estimate is thought of as imputed noise, which is justified in Appendix C in \cite{prothero:2022}. The estimate needs to be computed once at the beginning. Similarly as dealing with $\mathbf{X}$, let $\doublecheck{\mathbf{U}}$ and $\doublecheck{\mathbf{V}}$ be $T \times \check{r}$ and $d \times \check{r}$ matrices of orthogonal columns by compact SVD on $\doublehat{\mathbf{X}}$.


Finally, for the inner loop of the bootstrapping, we obtain the smallest singular values from the SVD of $\doublehat{\mathbf{U}}'\doublecheck{\mathbf{U}}_{1:r}$ and $\doublehat{\mathbf{V}}'\doublecheck{\mathbf{V}}_{1:r}$, where $\doublecheck{\mathbf{U}}_{1:r}$ stands for $r$ columns of $\doublecheck{\mathbf{U}}$, so does $\doublecheck{\mathbf{V}}_{1:r}$, $r=1,\ldots,\check{r}$. Each smallest singular value is converted to the largest angle. Thus we have $M \times \check{r}$ angles for each of the row and column spaces. After sorting those angles, we select the final initial rank estimate $\hat{r}$ by 
%
\begin{equation}
    \hat{r} 
    = \min\left(\sum_{i=1}^{\check{r}}
    1_{\{\textrm{95\% percentile of angles made by $\doublehat{\mathbf{U}}'\doublecheck{\mathbf{U}}_{1:i}$} < \theta_0\}} ,\sum_{i=1}^{\check{r}}
    1_{\{\textrm{95\% percentile of angles made by $\doublehat{\mathbf{V}}'\doublecheck{\mathbf{V}}_{1:i}$} < \theta_0\}} \right).
\end{equation}
%
where $\theta_0$ is the random direction angle bound described above. We refer to Algorithm 1 in Section 2.1.3 in \cite{prothero:2022} for the remaining details.


In practice, the rotational bootstrapping is applied to the block observations of each subject. Then, the initial rank is finalized by a majority vote, as a mode of $2K$ number of $\hat{r}$s. We use this estimate for selecting the rank of the joint factors $\hat{r}_J$ produced by AJIVE. Naturally, this leads to $\hat{r}_G=\hat{r}-\hat{r}_J$. Potentially, one can also take the different initial ranks for each group, depending on the context. In order to apply the factor structure described below for the rest of the procedure, the ranks of the joint and the two group blocks should be specified. Hence, the identification of factor structure relies on the performance of the initial rank selection from the rotational bootstrapping and the joint rank selection from AJIVE. We will assess the performance of the initial rank selection in Section \ref{sse:result_bootstrap}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Reconstruction of factor series}
\label{sse:reconstruction}


After the segmentation through the AJIVE, the remaining task is to reconstruct the factor structure in \eqref{e:augmented_factor_2}. A typical way to impose the determinacy of factor series up to the transformation with the sign changes of the columns is to assume that the factor covariance is an identity matrix \cite[e.g.][]{lam:2011,bai:2013}. For fixed $k$, by assuming we have only $k$th subject, the condition implies that the variances within the joint factor series and the group individual factor series should be identity matrices, 
%
\begin{equation}
    \mathbb{E}\bar{F}_{t}^{(k)}\bar{F}_{t}^{(k)'} 
    = \mathbf{I}_{r_{J}}, \quad 
    \mathbb{E}\tilde{F}_{t}^{(k)}\tilde{F}_{t}^{(k)'} 
    = \mathbf{I}_{r_{G}}. \label{e:orthogonality_factor} 
\end{equation}
%
The same determinacy condition has been applied for multiple subjects \cite[e.g.][]{fan:2018} as well. This condition, however, is quite stringent; not only it does not allow for the joint or group individual factor series to be correlated but it also forces the covariance of factors to be identical across subjects, which limits the degree of heterogeneity. One alternative suggested by \cite{bai:2015} is to make the elements of the square matrix on the top of the loading matrix be identity, that is, 
%
\begin{equation}\label{e:pca_dynamic}
    \bar{\mathbf{B}} 
    = \begin{pmatrix}
        \mathbf{I}_{r_{J}} \\ \bullet
    \end{pmatrix}, \quad
    \tilde{\mathbf{B}}_g 
    = \begin{pmatrix}
        \mathbf{I}_{r_{G}} \\ \bullet
    \end{pmatrix},
\end{equation}
%
where $\bullet$ stands for the remaining entries of each loading matrix. However, this condition still loses information on the first $r_J$ (or $r_G$) spatial maps.


To relax the stringent restrictions \eqref{e:orthogonality_factor}, we adopt the simultaneous component analysis (SCA) introduced by \cite{timmerman:2003}. SCA was designed for multivariate time series from multiple subjects to study both intra- and inter-individual differences. Although SCA has been applied to factor models without considering dynamics, we will show that SCA can be applied to dynamic factor series as well.


The idea of SCA is not different from PCA but various restrictions on the component factor scores may be imposed. There are four types of covariance structures suggested in \cite{timmerman:2003}. However, we focus on  SCA with PARAFAC2 constraints (SCA-PF2), inspired by tensor decomposition PARAFAC \citep[e.g.][]{kiers:1999}. Additionally, we introduce a more restrictive but useful covariance structure, namely SCA with INDSCAL constraints (SCA-IND). To compare the various covariance structures of the factors, consider the joint factor series. Then, the two covariance structures mentioned above are
%
\begin{eqnarray}
    \mathbb{E}\bar{F}_{t}^{(k)}\bar{F}_{t}^{(k)'} 
    &=:& \mathbb{E}\left(\bar{\mathbf{C}}_k\bar{A}_{t}^{(k)}\right)\left(\bar{\mathbf{C}}_k\bar{A}_{t}^{(k)}\right)^{'}  
    = \bar{\mathbf{C}}_k \bar{\boldsymbol{\Phi}} \bar{\mathbf{C}}_k, \quad \textrm{(SCA-PF2)} \label{e:SCA-PF2} \\
    \mathbb{E}\bar{F}_{t}^{(k)}\bar{F}_{t}^{(k)'} 
    &=:& \mathbb{E}\left(\bar{\mathbf{C}}_k\bar{A}_{t}^{(k)}\right)\left(\bar{\mathbf{C}}_k\bar{A}_{t}^{(k)}\right)^{'}  
    = \bar{\mathbf{C}}_k^2, \quad \textrm{(SCA-IND)} \label{e:SCA-IND}
\end{eqnarray}
%
where $\bar{\boldsymbol{\Phi}}=(\bar{\phi}_{i,j})$ is a $r_J \times r_J$ positive definite matrix and $\bar{\mathbf{C}}_k=(\bar{C}_{i,j}^{(k)})$ are diagonal matrices. Note that to identify matrices within their multiplications, we let the diagonal entries of $\bar{\boldsymbol{\Phi}}$ be unit-scaled so that they become correlation matrices of the scaled joint factor $\bar{A}_{t}^{(k)}$ and the absolute values of $\bar{\mathbf{C}}_k$ represent the standard deviations of the joint factor $\bar{F}_{t}^{(k)}$. Note also that \eqref{e:SCA-PF2} allows for correlations across joint factors while \eqref{e:SCA-IND} does not. Lastly, one can think of \eqref{e:SCA-IND} as a special case of \eqref{e:SCA-PF2} by letting $\bar{\boldsymbol{\Phi}}=\mathbf{I}_{r_{J}}$. Obviously, these conditions do not require any restriction on loading matrices. The same assumption on the covariance of group individual factors can be applied, which in turn produces 
%
\begin{eqnarray}
    \mathbb{E}\tilde{F}_{t}^{(k)}\tilde{F}_{t}^{(k)'} 
    &=:& 
    \tilde{\mathbf{C}}_k \tilde{\boldsymbol{\Phi}} \tilde{\mathbf{C}}_k, \\
    \mathbb{E}\tilde{F}_{t}^{(k)}\tilde{F}_{t}^{(k)'} 
    &=:&  
    \tilde{\mathbf{C}}_k^2,
\end{eqnarray}
%
where $\tilde{\boldsymbol{\Phi}}$ is $r_G \times r_G$ positive definite, and $\tilde{\mathbf{C}}_k=(\tilde{C}_{i,j}^{(k)})$ are diagonal as similarly defined as above. 


\begin{remark}
While the conditions as in \eqref{e:SCA-PF2}--\eqref{e:SCA-IND} enable the model to have more complex covariance structures, one can think that with more restrictions, as \eqref{e:orthogonality_factor} in an extreme case, the model becomes more parsimonious. In the other direction, the most flexible covariance structure is known as SCA with Invariant Pattern (SCA-P) \cite[see][]{timmerman:2003}. To our best knowledge, there are no tests to decide on particular structures. As long as we deal with the latent factor series and their characteristics are not known in advance, it is beneficial to allow the model to be as flexible as possible. Therefore, we do not prefer the most restrictive condition \eqref{e:orthogonality_factor}. At the same time, our hope is to incorporate the heterogeneity of subjects without losing identifiability. However, the SCA-P condition is known not to guarantee such uniqueness. Furthermore, while the condition \eqref{e:pca_dynamic} does not impose any restriction on the covariance, it loses some spatial information as mentioned above. Therefore, we prefer the SCA-PF2 condition and as a special case, the SCA-IND condition.
\end{remark}


\begin{remark}
Among studies related to SCA, inducing rotational matrix to make targeted factor scores \cite[e.g.][]{schouteden:2013,schouteden:2014} or particular structural loading matrices \cite[e.g.][]{helwig:2017} have been tried. However, the former requires a known target structure of factor scores in advance or takes additional steps to determine the target. It also carries relatively large computational costs since whole factor scores and loadings are involved in finding rotational transformation. Furthermore, the adopted determinacy condition in the approach is SCA-P, which does not guarantee the identification of factor scores. The latter also demands the pre-specified structure of loadings matrices in advance. When we do not know the relationship between factors themselves or desired loading matrices, this blocking approach has limitations in its application.
\end{remark}


We estimate loadings matrices $\{\bar{\mathbf{B}},\tilde{\mathbf{B}}_1,\tilde{\mathbf{B}}_2\}$ and factor series $\{\bar{\mathbf{F}}_k,\tilde{\mathbf{F}}_k\}_{k=1,\ldots,2K}$ through a direct fitting algorithm for PARAFAC2 model, which will be described in Section \ref{ssse:estimation}. Then, we refit the latent factors through subject-wise linear regressions:
%
\begin{equation}\label{e:refitting}
    \begin{pmatrix} \hat{\bar{\mathbf{F}}}_k & \hat{\tilde{\mathbf{F}}}_k \end{pmatrix} 
    = \argmin_{\mathbf{F}_k=(\bar{\mathbf{F}}_k\ \tilde{\mathbf{F}}_k)} 
    \| \mathbf{X}_k 
    - \bar{\mathbf{F}}_k\bar{\mathbf{B}}' - \tilde{\mathbf{F}}_k\tilde{\mathbf{B}}_g' \|_F^2,
\end{equation}
%
for $k=1,\ldots,2K$ and $g=1,2$ accordingly. Although there are alternative ways to refit the model from SCA, handling $\bar{\mathbf{F}}_k$ and $\tilde{\mathbf{F}}_k$ together has an advantage over treating them separately. First of all, \eqref{e:refitting} can be solved explicitly. Also, since the VAR in the factor models is latent, other refitting methods may be difficult to apply. Naturally, the squared error of the fitting becomes smaller after refitting the factor series by \eqref{e:refitting}. Then, we compute the empirical covariance matrix for the joint and group individual factor series, respectively. Finally, by the Yule-Walker equations \cite[e.g. Chapter 11 in][]{brockwell:2009}, we estimate the parameters in the model \eqref{e:filter_equation}, including the covariance matrix of noise $\bar{\boldsymbol{\Sigma}}_{\eta,k}$. We repeat this process for $k=1,\ldots,2K$. 

\begin{remark}
By following a two-step estimation procedure \cite[e.g.][]{doz:2011}, one can also use Kalman recursions to take into account the heteroscedasticity of noise and dynamics of the factor series. Although it may be useful to take the second step, we exclude the usage of Kalman recursions and only use PCA and Yule-Walker equations. First, we do not assume any particular structure on the covariance of noise. The cost of the recursive computations is also substantial because observations of all subjects are required. Lastly, our model is assumed to have the joint and group loading matrices, which are identical across (groups of) subjects, and their estimates are already fixed. Their values will change while running the algorithm.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsubsection{Discussion on determinacy of factors}
\label{ssse:determinacy}


Determinacy of factors in SCA was studied by several researchers. \cite{harshman:1996} proved the uniqueness of a general form of parameters of multi-view data which led to the rotational determinacy of SCA-PF2 models. In particular, when the rank $r$ of factors exceeds 2, the estimated components multiplied with some scaling and permutation matrices described below can be determined if the number of subjects is greater than $r(r+1)(r+2)(r+3)/24$ for rank $r$. So, although the inequality seems to require a quite larger number of subjects to obtain the determinacy, the computational experiment by  \cite{ten:1996} suggested that the determinacy can be achieved even for larger ranks if the number of subjects exceeds 4. However, a rigorous proof for this or the other minimum number of subjects remains to be found.


The proved determinacy for SCA-PF2 is as follows. If the SCA-PF2 model can be expressed through two sets $\bar{\mathbf{B}},\bar{A}_t^{(k)},\bar{\mathbf{C}}_k$ and $\doublebar{\mathbf{B}},\doublebar{A}_t^{(k)},\doublebar{\mathbf{C}}_k$ of joint loading matrices, scaled joint factor series, and the diagonal matrices, then 
%
\begin{equation}\label{e:rotational_determinacy_joint_1}
    \doublebar{\mathbf{B}} = \bar{\mathbf{B}}\bar{\mathbf{W}}\Delta_{\bar{\mathbf{B}}}, \quad 
    \doublebar{A}_t^{k} = z_k\Delta_{\bar{\mathbf{A}}}\bar{\mathbf{W}}'\bar{A}_t^{k},\quad 
    \doublebar{\mathbf{C}}_{k} = z_k\bar{\mathbf{W}}'\bar{\mathbf{C}}_k\bar{\mathbf{W}}\Delta_{\bar{\mathbf{C}}},
\end{equation}
%
where $\bar{\mathbf{W}}$ is a permutation matrix, $z_k=-1$ or $1$, and $\Delta_{\bar{\mathbf{A}}},\Delta_{\bar{\mathbf{B}}},\Delta_{\bar{\mathbf{C}}}$ are $r_{J} \times r_{J}$ non-singular diagonal scaling matrices satisfying $\Delta_{\bar{\mathbf{A}}}\Delta_{\bar{\mathbf{B}}}\Delta_{\bar{\mathbf{C}}}=I_{r_{J}}$. By convention, one can fix $z_k=1$ for all $k$ to overcome the sign indeterminacy \cite[e.g.][]{helwig:2013}. Thus, the joint factors can be related as
%
\begin{equation}\label{e:rotational_determinacy_joint_2}
    \doublebar{F}_t^{(k)} 
    := \doublebar{\mathbf{C}}^{(k)}\doublebar{A}_t^{(k)} 
    = \Delta_{\bar{\mathbf{B}}}^{-1} \bar{\mathbf{W}}' \bar{F}_t^{(k)},
\end{equation}
%
and the correlation matrix of the scaled joint factors is of the form $\doublebar{\boldsymbol{\Phi}} = \Delta_{\bar{\mathbf{A}}}\bar{\mathbf{W}}' \bar{\boldsymbol{\Phi}} \bar{\mathbf{W}} \Delta_{\bar{\mathbf{A}}}$ so that 
%
\begin{equation}\label{e:rotational_determinacy_joint_3}
    \mathbb{E}\doublebar{F}_t^{(k)}\doublebar{F}_t^{(k)'}
    = \doublebar{\mathbf{C}}_{k}\doublebar{\boldsymbol{\Phi}}\doublebar{\mathbf{C}}_{k} 
    = \Delta_{\bar{\mathbf{B}}}^{-1}\bar{\mathbf{W}}'\bar{\mathbf{C}}_k\bar{\boldsymbol{\Phi}} \bar{\mathbf{C}}_k\bar{\mathbf{W}}\Delta_{\bar{\mathbf{B}}}^{-1}.
\end{equation}
%
That is, modulo the scaling matrix $\Delta_{\bar{\mathbf{B}}}$, one has the same covariance structure of the factor series up to their rearrangement. Note that from \eqref{e:rotational_determinacy_joint_1}--\eqref{e:rotational_determinacy_joint_3}, the determinacy can be achieved with the multiplication of the scalers and permutations. In the sense that although the model still has an issue of scaling, the identification up to the change of rows and columns of matrices by imposing SCA-PF2 structure is remarkable.


Similar results hold for group individual factor series. For each group $g=1,2$, the group loadings matrices $\tilde{\mathbf{B}}_g,\doubletilde{\mathbf{B}}_g$, scaled group individual factors $\tilde{A}_{t}^{(k)},\doubletilde{A}_{t}^{(k)}$, and the diagonal matrices $\tilde{\mathbf{C}}_k,\doubletilde{\mathbf{C}}_k$ are related as
%
\begin{equation}\label{e:rotational_determinacy_ind_1}
    \doubletilde{\mathbf{B}}_g = \tilde{\mathbf{B}}\tilde{\mathbf{W}}_g\Delta_{\tilde{\mathbf{B}}_g}, \quad 
    \doubletilde{A}_t^{(k)} = z_{g,k}\Delta_{\tilde{\mathbf{A}}_g}\tilde{\mathbf{W}}_g'\tilde{A}_t^{(k)},\quad 
    \doubletilde{\mathbf{C}}_{k} = z_{g,k}\tilde{\mathbf{W}}_g'\tilde{\mathbf{C}}_k\tilde{\mathbf{W}}_g\Delta_{\tilde{\mathbf{C}}_g}.
\end{equation}
%
Here, $\tilde{\mathbf{W}}_g$ is a permutation matrix of $g$th group and $z_{g,k}=-1$ or $1$, so we take $z_{g,k}=1$ for all cases by convention. Obviously, the index $k$ goes from 1 to $K$ when $g=1$, and $k=K+1,\ldots,2K$ are used for $g=2$. Finally, $\Delta_{\tilde{\mathbf{A}}_g},\Delta_{\tilde{\mathbf{B}}_g},\Delta_{\tilde{\mathbf{C}}_g}$ are $r_{G} \times r_{G}$ non-singular diagonal matrices such that $\Delta_{\tilde{\mathbf{A}}_g}\Delta_{\tilde{\mathbf{B}}_g}\Delta_{\tilde{\mathbf{C}}_g}=\mathbf{I}_{r_{G}}$. Accordingly, the group individual factors are related as
%
\begin{equation}\label{e:rotational_determinacy_ind_2}
    \doubletilde{F}_t^{(k)} 
    = \doubletilde{\mathbf{C}}_{k}\doubletilde{A}_t^{(k)} 
    = \Delta_{\tilde{\mathbf{B}}_g}^{-1} \tilde{\mathbf{W}}_g' \tilde{F}_t^{(k)},
\end{equation}
%
for corresponding $g$ and $k$ as above, so that the covariance of each group individual factor is 
%
\begin{equation}\label{e:rotational_determinacy_ind_3}
    \mathbb{E}\doubletilde{F}_t^{(k)}\doubletilde{F}_t^{(k)'}
    = \doubletilde{\mathbf{C}}_{k}\doubletilde{\boldsymbol{\Phi}}\doubletilde{\mathbf{C}}_{k} 
    = \Delta_{\tilde{\mathbf{B}}_g}^{-1}\tilde{\mathbf{W}}_g'\tilde{\mathbf{C}}_k\tilde{\boldsymbol{\Phi}}_g \tilde{\mathbf{C}}_k\tilde{\mathbf{W}}_g\Delta_{\tilde{\mathbf{B}}_g}^{-1},
\end{equation}
%
where $\doubletilde{\boldsymbol{\Phi}}_g := \Delta_{\tilde{\mathbf{A}}_g}\tilde{\mathbf{W}}_g' \tilde{\boldsymbol{\Phi}}_g \tilde{\mathbf{W}}_g \Delta_{\tilde{\mathbf{A}}_g}$, $g=1,2$. 


\begin{comment}
Consider the joint structure $\bar{\mathbf{B}}\bar{F}_{t}^{(k)}$ first. Assume that we follow the determinacy condition as \cite{doz:2011,doz:2012}. That is,
%
\begin{equation}
    \bar{\mathbf{B}}\bar{F}_{t}^{(k)} 
    = (\bar{\mathbf{B}} \Sigma_{\bar{\mathbf{F}},k}^{1/2})(\Sigma_{\bar{\mathbf{F}},k}^{-1/2} \bar{F}_{t}^{(k)}) 
    =: \doublebar{\mathbf{B}}_k\doublebar{F}_t^{(k)},
\end{equation}
%
where $\Sigma_{\bar{\mathbf{F}},k}$ is the covariance of the joint factor series of $k$th subject. Then one has a symmetric positive matrix $\Sigma_{\bar{\mathbf{F}},k}^{1/2}\bar{\mathbf{B}}'\bar{\mathbf{B}} \Sigma_{\bar{\mathbf{F}},k}^{1/2}$ so that
%
\begin{equation}
    \Sigma_{\bar{\mathbf{F}},k}^{1/2}\bar{\mathbf{B}}'\bar{\mathbf{B}} \Sigma_{\bar{\mathbf{F}},k}^{1/2} 
    =: \doublebar{\mathbf{B}}_k'\doublebar{\mathbf{B}}_k 
    = \doublebar{\mathbf{U}}_k\doublebar{\mathbf{S}}_k\doublebar{\mathbf{U}}_k',
\end{equation}
%
where $\doublebar{\mathbf{U}}_k$ is an orthogonal matrix and $\doublebar{\mathbf{S}}_k$ is diagonal. Hence, we can identify the estimates up to orthogonal transformation,
%
\begin{equation}\label{e:identifiability_transform_joint}
    \doublebar{\mathbf{B}}_k\doublebar{F}_t^{(k)} 
    =  \doublebar{\mathbf{B}}_k\doublebar{\mathbf{U}}_k\doublebar{\mathbf{U}}_k'\doublebar{F}_t^{(k)}
    =: \doublebar{\boldsymbol{\Gamma}}_k \doublebar{G}_{t}^{(k)},
\end{equation}
%
where $\mathbb{E}\doublebar{G}_{t}^{(k)}\doublebar{G}_{t}^{(k)'}=I_{r_J}$. In particular, if $\bar{\Delta}_k$ is a $r_{J} \times r_{J}$ diagonal matrix whose nonzero entries are $\pm 1$, then the $\doublebar{\mathbf{U}}_k,\doublebar{\boldsymbol{\Gamma}}_k$, and $\doublebar{G}_{t}^{(k)}$ in \eqref{e:identifiability_transform_joint} are replaced by $\doublebar{\mathbf{U}}_k\doublebar{\Delta}_k,\doublebar{\boldsymbol{\Gamma}}_k\doublebar{\Delta}_k$, and $\doublebar{\Delta}_k\doublebar{G}_{t}^{(k)}$, respectively. Note that such sign changes are minor in that it is inevitable and we can set the reference individual, say $k=1$, and fix the signs to make $\doublebar{\boldsymbol{\Gamma}}_k\doublebar{\Delta}_k$ to be identical to $\doublebar{\boldsymbol{\Gamma}}_k\doublebar{\Delta}_1$. Then the corresponding signs of the factor series are automatically determined. To fix the rotational transformation, on the other hand, we can choose the reference index, say $k=1$ again, and find the orthogonal matrix $\doublebar{\mathbf{Q}}_k$, $k=2,\ldots,2K$, such that $\doublebar{\boldsymbol{\Gamma}}_k\doublebar{\mathbf{Q}}_k = \doublebar{\boldsymbol{\Gamma}}_1$. 


Likewise, a similar argument can be applied to the group individual structure so that
%
\begin{equation}\label{e:identifiability_transform_individual}
    \doubletilde{\mathbf{B}}_g\doubletilde{F}_{t}^{(k)} = \doubletilde{\mathbf{B}}_g\doubletilde{\mathbf{U}}_{g,k}\doubletilde{\mathbf{U}}'_{g,k}\tilde{F}_{t}^{(k)} =: \doubletilde{\boldsymbol{\Gamma}}_{g,k}\doubletilde{G}_{t}^{{(g,k)}},
\end{equation}
%
where where $\mathbb{E}\doubletilde{G}_{t}^{{(g,k)}}\doubletilde{G}_{t}^{{(g,k)}'}=I_{r_G}$, $g=1$ for $k=1,\ldots,K$ and $g=2$ for $k=K+1,\ldots,2K$, with possible sign changes through $\doubletilde{\Delta}_k$, similarly defined as above. For this case, one can find the rotation matrix $\doubletilde{\mathbf{Q}}_{k}$ such that $\doubletilde{\boldsymbol{\Gamma}}_{1,k}\doubletilde{\mathbf{Q}}_k = \doubletilde{\boldsymbol{\Gamma}}_{1,1}$ for $k=1,\ldots,K$, and $\doubletilde{\boldsymbol{\Gamma}}_{2,k}\doubletilde{\mathbf{Q}}_k = \doubletilde{\boldsymbol{\Gamma}}_{2,1}$ for $k=K+1,\ldots,2K$, respectively. In Section \ref{sse:result_main}, we will illustrate the performance of the usual rotational determinacy and compare it with our model when there are potential correlations between factor series themselves in each structure.
\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Discussion on a direct fitting algorithm}
\label{ssse:estimation}


For the algorithm of estimating the joint loadings matrix and joint factor series at the second stage of our procedure, we use the direct fitting algorithm developed by \cite{kiers:1999}. Direct fitting means that one estimates components of factor-type models in the least squares sense, by regressing responses on covariates. It is different from the indirect fitting described below. Previously, \cite{harshman:1970} suggested an indirect fitting algorithm through alternating least squares (ALS) as follows. To explain the procedure, we describe it on the joint structure. If we denote the covariance matrix of the joint component $\bar{\mathbf{X}}_k$ as $\bar{\boldsymbol{\Sigma}}_{k}$, the model can be estimated by minimizing
%
\begin{equation}
    \min_{\bar{\mathbf{B}},\bar{\mathbf{C}}_{1:2K},\bar{\boldsymbol{\Phi}}} \sum_{k=1}^{2K}\|\bar{\boldsymbol{\Sigma}}_{k} - \bar{\mathbf{B}}\bar{\mathbf{C}}_k \bar{\boldsymbol{\Phi}}\bar{\mathbf{C}}_k\bar{\mathbf{B}}'\|_F^2.
\end{equation}
%
Instead, we focus on the direct fitting algorithm. This algorithm is beneficial because not only it has a fast convergence but it also produces the factor scores, which are used for constructing factor dynamics. Moreover, one can easily incorporate additional constraints such as non-negative entries in scaler matrices.  


We now describe the direct fitting algorithm. From $\bar{\mathbf{F}}_{k} = \bar{\mathbf{A}}_{k}\bar{\mathbf{C}}_{k}$ where $\bar{\mathbf{A}}_{k}$ is a $T_k \times r_J$ matrix consisting of scaled factor series $\bar{F}_{t}^{(k)}$, the goal is to solve the following problem:
%
\begin{eqnarray}
 (\hat{\mathbf{A}}_{1:2K},\hat{\mathbf{C}}_{1:2K},\hat{\mathbf{B}}) 
 &=& \argmin_{\bar{\mathbf{A}}_{1:2K},\bar{\mathbf{C}}_{1:2K},\bar{\mathbf{B}}} \sum_{k=1}^{2K} \left\| \bar{\mathbf{X}}_k - \bar{\mathbf{A}}_k\bar{\mathbf{C}}_k\bar{\mathbf{B}}' \right\|_F^2, \label{e:PARAFAC2_orginal_object} \\
    &\textrm{s.t.}& \quad \bar{\mathbf{A}}_k'\bar{\mathbf{A}}_k = \bar{\mathbf{A}}_{k'}'\bar{\mathbf{A}}_{k'}, \quad k,k'\in\{1,\ldots,2K\}. \label{e:PARAFAC2_orginal_constraint}
\end{eqnarray}
%
The key idea is to convert the problem \eqref{e:PARAFAC2_orginal_object}--\eqref{e:PARAFAC2_orginal_constraint} into $2K$ separated orthogonal Procrustes problems, which have been dealt with in psychometrics \cite[e.g.][]{green:1952,schonemann:1966}. Note that the sufficiency condition for satisfying \eqref{e:PARAFAC2_orginal_constraint} at optimality is to find a columnwise orthonormal $T_k \times r_J$ matrix $\bar{\mathbf{P}}_k$ such that $\bar{\mathbf{A}}_k = \bar{\mathbf{P}}_k\bar{\mathbf{A}}$ so that
%
\begin{equation}\label{e:constraint_relationship}
    \bar{\mathbf{A}}_k'\bar{\mathbf{A}}_k  = \bar{\mathbf{A}}'\bar{\mathbf{P}}_k'\bar{\mathbf{P}}_k\bar{\mathbf{A}} = \bar{\mathbf{A}}'\bar{\mathbf{A}}
\end{equation}
%
\cite[see Section 2 in][]{schonemann:1966}. The necessity condition of the optimality implies $\bar{\mathbf{A}}_k = \bar{\mathbf{Q}}_k\bar{\mathbf{N}}_k\bar{\mathbf{D}}_1$, where $\bar{\mathbf{Q}}_k$ is a $T_k \times r_J$ matrix with orthogonal vectors and $\bar{\mathbf{N}}_k$ is a $r_J \times r_J$ matrix satisfying $\bar{\mathbf{D}}_k'\bar{\mathbf{D}}_k = \bar{\mathbf{D}}_1'\bar{\mathbf{N}}_k'\bar{\mathbf{N}}_k\bar{\mathbf{D}}_1 = \bar{\mathbf{D}}_1'\bar{\mathbf{D}}_1$ with $\bar{\mathbf{D}}_k = \bar{\mathbf{N}}_k\bar{\mathbf{D}}_1$. Thus, by letting $\bar{\mathbf{P}}_k = \bar{\mathbf{Q}}_k\bar{\mathbf{N}}_k$ and $\bar{\mathbf{A}} = \bar{\mathbf{D}}_1$, the original problem \eqref{e:PARAFAC2_orginal_object}--\eqref{e:PARAFAC2_orginal_constraint} becomes
%
\begin{eqnarray}
    (\hat{\mathbf{P}}_{1:2K},\hat{\mathbf{A}},\hat{\mathbf{C}}_{1:2K},\hat{\mathbf{B}}) 
     &=& \argmin_{\bar{\mathbf{P}}_{1:2K},\bar{\mathbf{A}},\bar{\mathbf{C}}_{1:2K},\bar{\mathbf{B}}} \sum_{k=1}^{2K} \left\| \bar{\mathbf{X}}_k - \bar{\mathbf{P}}_k\bar{\mathbf{A}}\bar{\mathbf{C}}_k\bar{\mathbf{B}}' \right\|_F^2, \label{e:PARAFAC2_separate_object}\\
        &\textrm{s.t.}& \quad \bar{\mathbf{P}}_k'\bar{\mathbf{P}}_k = \mathbf{I}_{r_{J}},\quad k=1,\ldots,2K. \label{e:PARAFAC2_separate_constraint}
\end{eqnarray}
%
Note that the objective function for fixed $k$ is $\textrm{Tr}(\bar{\mathbf{X}}_k'\bar{\mathbf{X}}_k) - 2\textrm{Tr}(\bar{\mathbf{X}}_k'\bar{\mathbf{P}}_k\bar{\mathbf{A}}\bar{\mathbf{C}}_k\bar{\mathbf{B}}') + \textrm{Tr}(\bar{\mathbf{B}}\bar{\mathbf{C}}_k\bar{\mathbf{A}}'\bar{\mathbf{A}}\bar{\mathbf{C}}_k\bar{\mathbf{B}}')$, where $\textrm{Tr}(\cdot)$ is a trace operator. Hence, a minimizer of \eqref{e:PARAFAC2_separate_object}--\eqref{e:PARAFAC2_separate_constraint} over $\bar{\mathbf{P}}_k$ for fixed other components is obtained by
%
\begin{equation}\label{e:e:PARAFAC2_single}
    \hat{\mathbf{P}}_k = \argmax_{\bar{\mathbf{P}}_k'\bar{\mathbf{P}}_k = \mathbf{I}_{r_{J}}} \textrm{Tr}(\bar{\mathbf{A}}\bar{\mathbf{C}}_k\bar{\mathbf{B}}'\bar{\mathbf{X}}_k'\bar{\mathbf{P}}_k).
\end{equation}
%
Take the SVD as $\bar{\mathbf{A}}\bar{\mathbf{C}}_k\bar{\mathbf{B}}'\bar{\mathbf{X}}_k' = \bar{\mathbf{U}}_k\bar{\mathbf{S}}_k\bar{\mathbf{V}}_k'$, where $\bar{\mathbf{U}}_k$ and $\bar{\mathbf{V}}_k$ are $r_J \times r_J$ and $r_J \times T_k$ matrices with orthogonal columns and $\bar{\mathbf{S}}_{k}$ consists of singular values of $\bar{\mathbf{A}}\bar{\mathbf{C}}_k\bar{\mathbf{B}}'\bar{\mathbf{X}}_k'$. By replacing $\bar{\mathbf{A}}\bar{\mathbf{C}}_k\bar{\mathbf{B}}'\bar{\mathbf{X}}_k'$ with $\bar{\mathbf{U}}_k\bar{\mathbf{S}}_k\bar{\mathbf{V}}_k'$, the solution of \eqref{e:e:PARAFAC2_single} becomes $\bar{\mathbf{P}}_k = \bar{\mathbf{V}}_k\bar{\mathbf{U}}_k'$, which can be shown by using CauchySchwarz inequality. This completes the first step of the direct fitting algorithm. Next, substituting $\bar{\mathbf{V}}_k'\bar{\mathbf{U}}_k$ for $\bar{\mathbf{P}}_k$ in \eqref{e:PARAFAC2_separate_object} becomes 
%
\begin{equation}\label{e:PARAFAC_object}
    (\hat{\mathbf{A}},\hat{\mathbf{C}}_{1:2K},\hat{\mathbf{B}}) 
    = \argmin_{\bar{\mathbf{A}},\bar{\mathbf{C}}_{1:2K},\bar{\mathbf{B}}} \sum_{k=1}^{2K} \left\| \hat{\mathbf{P}}_k'\bar{\mathbf{X}}_k - \bar{\mathbf{A}}\bar{\mathbf{C}}_k\bar{\mathbf{B}}' \right\|_F^2.
\end{equation}
%
The problem \eqref{e:PARAFAC_object} can be solved by CANDECOMP/PARAFAC (CP) model as the ALS algorithm \cite[CP-ALS, see][for the details]{kiers:1998}. This completes the second step. The two steps are repeated until a convergence criterion is satisfied. The same algorithm is applied to the estimates $\tilde{\mathbf{A}}_{k},\tilde{\mathbf{C}}_{k}$, and $\tilde{\mathbf{B}}_g$, with group individual components $\tilde{\mathbf{X}}_k$, $k=1,\ldots,2K$, for the corresponding $g=1,2$.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Illustrative examples}
\label{se:illustrative}

\subsection{Simulation design}
\label{sse:design}

In this section, we describe the simulation design that is used to assess estimation performance under different scenarios. The results will be reported in Section \ref{sse:result_main} below. For the simulation design, we choose various combinations of large $d,T$, and $K$ as long as the sizes of the problem can be handled through the standard computational environment. R package \texttt{ajive} by \cite{Carmichael:2020} and another R package \texttt{multiway} showcased in \cite{helwig:2019} have been used. The data generating processes (DGPs) are as follows.
%
\begin{itemize}
    \item[-] $X_{i,t}^{(k)} 
    = \left\{\begin{array}{ll}
    \sum_{j=1}^{r_J}\bar{B}_{i,j}\bar{F}_{j,t}^{(k)} + \sum_{j=1}^{r_G}\tilde{B}_{i,j}^{(1)}\tilde{F}_{j,t}^{(k)} + E_{i,t}^{(k)}, & \textrm{if }k=1,\ldots,K, \\
    \sum_{j=1}^{r_J}\bar{B}_{i,j}\bar{F}_{j,t}^{(k)} + \sum_{j=1}^{r_G}\tilde{B}_{i,j}^{(2)}\tilde{F}_{j,t}^{(k)} + E_{i,t}^{(k)}, & \textrm{if }k=K+1,\ldots,2K,
    \end{array}\right.$
    
    \item[-] Among the indices $i=1,\ldots,d$, choose 50\% of them and make $\bar{B}_{i,j}$, $j=1,\ldots,r_J$, as non-zero by $\bar{B}_{i,j}\stackrel{i.i.d.}{\sim}\mathrm{Unif}(b_{\min},b_{\max})$. Otherwise, $\bar{B}_{i,j}=0$ for non-selected indices of $i$. 
    
    \item[-] Among the non-selected indices above, half of them are chosen to make $\tilde{B}_{i,j}^{(1)}$, $j=1,\ldots,r_G$, have non-zero entries. The remaining half are used for $\tilde{B}_{i,j}^{(2)}$, $j=1,\ldots,r_G$, to be non-zero. Both are generated by $\tilde{B}_{i,j}^{(1)},\tilde{B}_{i,j}^{(2)}\stackrel{i.i.d.}{\sim}\mathrm{Unif}(b_{\min},b_{\max})$. Otherwise, let $\tilde{B}_{i,j}^{(1)},\tilde{B}_{i,j}^{(2)}$ be zero for non-selected indices of $i$.
    
    \item[-] For each $k$, generate the series $\bar{A}_{t}^{(k)}$ and $\tilde{A}_{t}^{(k)}$, $t=1,\ldots,T_{k}$, $k=1,\ldots,2K$, by following VAR(1), respectively,
    %
    \begin{eqnarray}
        \bar{A}_{t}^{(k)} 
        = \bar{\boldsymbol{\Psi}}\bar{A}_{t-1}^{(k)} + \bar{\xi}_{t}^{(k)},\quad \{\bar{\xi}_{t}^{(k)}\} \sim \mathrm{WN}(0,\sigma_{\bar{\xi}}\mathbf{I}_{r_{J}}), \\
        \tilde{A}_{t}^{(k)} 
        = \tilde{\boldsymbol{\Psi}}\tilde{A}_{t-1}^{(k)} + \tilde{\xi}_{t}^{(k)},\quad \{\tilde{\xi}_{t}^{(k)}\} \sim \mathrm{WN}(0,\sigma_{\tilde{\xi}}\mathbf{I}_{r_{G}}).        
    \end{eqnarray}
    %
    For simplicity, let $\sigma_{\bar{\xi}} = \sigma_{\tilde{\xi}} = \sigma_{\xi}$, $\bar{\boldsymbol{\Psi}} = \tilde{\boldsymbol{\Psi}}=\boldsymbol{\Psi}$ and $\bar{\boldsymbol{\Phi}} = \tilde{\boldsymbol{\Phi}}=\boldsymbol{\Phi}$ where $\bar{\boldsymbol{\Phi}}=\mathbb{E}\bar{A}_t^{(k)}\bar{A}_t^{(k)'}$ and $\tilde{\boldsymbol{\Phi}}=\mathbb{E}\tilde{A}_t^{(k)}\tilde{A}_t^{(k)'}$. The following equation has to be satisfied for a stable factor series,
    %
    \begin{equation}\label{e:riccati}
        \boldsymbol{\Phi} 
        = \boldsymbol{\Psi} \boldsymbol{\Phi} \boldsymbol{\Psi}^{'} + \sigma_{\xi}\mathbf{I}.
    \end{equation}
    %
    By following the constraints \eqref{e:SCA-PF2} and \eqref{e:SCA-IND}, the PARAFAC2 fitting algorithm automatically estimates the diagonal components of $\boldsymbol{\Phi}$ to be unit. Hence, we set $\phi_{ii}=1$ at the model level. Then, we specify the structure $\boldsymbol{\Phi}$ and choose $\boldsymbol{\Psi}$ satisfying the equation \eqref{e:riccati} for fixed $\sigma_{\xi}$. From the known fact of algebraic Riccati equations, the stable solution is uniquely determined \cite[e.g.][]{boyd:1994}. Different structures of $\boldsymbol{\Phi}$ will be referred to as types below.
    
    \item[-] Take $\bar{F}_{t}^{(k)} = \bar{\mathbf{C}}_k\bar{A}_{t}^{(k)}$ and $\tilde{F}_{t}^{(k)} = \tilde{\mathbf{C}}_k\tilde{A}_{t}^{(k)}$, respectively, where 
    %
    \begin{equation}
        \bar{C}_{i,i}^{(k)},\tilde{C}_{i,i}^{(k)} \stackrel{i.i.d.}{\sim} \textrm{Unif}(c_{\min},c_{\max}),\quad
        \bar{C}_{i,j}^{(k)} = \tilde{C}_{i,j}^{(k)} = 0,\ i\neq j.
    \end{equation}
    %
    
    \item[-] $E_{t}^{(k)}\sim\mathcal{N}\left(0,\Sigma_{\mathbf{E},k} = \sigma_{\varepsilon} \mathbf{I}_{d} \right)$, $k=1,\ldots,2K$.
    
\end{itemize}
%
Note that the difference from the study in dynamic factor models such as \cite{doz:2012,brauning:2014} is that the correlations between the factor series are allowed. So, we leave the covariance matrices of the noise diagonal. In summary, DGP is fully controlled by $(\textrm{type},b_{\min},b_{\max},c_{\min},c_{\max},\sigma_{\xi},\sigma_{\varepsilon})$ for fixed $(r_J,r_G)$. By controlling these parameters, we vary the generated series to see how the realizations with different characteristics affect the performance. 


We assess estimation performance in three ways using 5 measures of interest. First, we report $R^2$ statistic which indicates the overall fit of the model. It measures how the model explains the total variability. From \eqref{e:matrix_joint_individual2} and \eqref{e:augmented_block1}, we define this measure by
%
\begin{equation}\label{e:measure_R2}
    R^2 = 1 - \frac{\sum_{k=1}^{2K}\|\mathbf{X}_k -  \hat{\bar{\mathbf{F}}}_k\hat{\bar{\mathbf{B}}}^{'} - \hat{\tilde{\mathbf{F}}}_k\hat{\tilde{\mathbf{B}}}_g^{'}\|_F^2}{\sum_{k=1}^{2K}\|\mathbf{X}_k\|_F^2},
\end{equation}
%
where $\{\hat{\bar{\mathbf{F}}}_k,\hat{\tilde{\mathbf{F}}}_k\}$ are either the direct output from PARAFAC2 \eqref{e:PARAFAC2_orginal_object}--\eqref{e:PARAFAC2_orginal_constraint} or the refitted latent factor series by \eqref{e:refitting}.


Another measure of the model fit is a root mean square error (RMSE). The difference between this measure and $R^2$ statistic is that it concerns only the fit of each block while $R^2$ statistic considers both the joint and group individual components. For the joint structure, we define
%
\begin{equation}\label{e:measure_RMSE}
    RMSE = \sqrt{\frac{1}{2dK}\sum_{k=1}^{2K}\frac{1}{T_k}\| \bar{\mathbf{X}}_k - \hat{\bar{\mathbf{F}}}_k\hat{\bar{\mathbf{B}}}^{'}\|_F^2}.
\end{equation}
%
Substituting $\bar{\mathbf{X}}_k$, $\hat{\bar{\mathbf{F}}}_k\hat{\bar{\mathbf{B}}}^{'}$, and $2K$ with $\tilde{\mathbf{X}}_k$, $\hat{\tilde{\mathbf{F}}}_k\hat{\tilde{\mathbf{B}}}_g^{'}$, and $K$, respectively, gives the RMSE of the group individual structures for  $g=1,2$. 


Finally, at each component level, we compute three other measures on how estimation captures spatial information and temporal dependence. We introduce the Tucker congruence coefficient $(CC_M)$ defined as
%
\begin{equation}\label{e:measure_CC}
    CC_M = \frac{\textrm{vec}(\mathbf{M})'\textrm{vec}(\hat{\mathbf{M}})}{\sqrt{\textrm{vec}(\mathbf{M})'\textrm{vec}(\mathbf{M})}\sqrt{\textrm{vec}(\hat{\mathbf{M}})'\textrm{vec}(\hat{\mathbf{M}})}},
\end{equation}
%
where $\mathbf{M}$ is some parameter and $\hat{\mathbf{M}}$ corresponds to its estimate. We can assess the accuracy of estimating loadings matrices for spatial information through $CC_B$, transition matrices as temporal dependence of latent factors through $CC_{\Psi}$, and factor series themselves through $CC_{F}$. For computing $CC_{\Psi}$ and $CC_F$, we use the refitted factor series. $CC_{F}$ may be a bit stringent since the measure compares two random processes pathwise. We nevertheless expect that these $CC$ scores are still informative. These scores will be reported for the joint and two groups separately.


One reason for joint modeling of time series data is its potential to enhance the forecasting performance \cite[e.g.][]{fisher:2022}. To evaluate the forecasting performance, from the same DGP, we generate data for extra $h$ steps. Then, the task is to forecast this true but unobserved vectors $X_{t}^{(k)}$, $t=T_k+1,\ldots,T_k+h$. From the estimated factor series and their equations, one can easily obtain the next $h$-step-ahead predictions by: with $g=1$ for $k=1,\ldots,K$, and $g=2$ for $k=K+1,\ldots,2K$,
%
\begin{eqnarray}
    X_{t|T_k}^{(k)} 
    &=& \hat{\bar{\mathbf{B}}}\hat{\bar{F}}_{t|T_k}^{(k)} + \hat{\tilde{\mathbf{B}}}_g\hat{\tilde{F}}_{t|T_k}^{(k)}, \\
    \hat{\bar{F}}_{t|T_k}^{(k)} 
    &=& \hat{\bar{\boldsymbol{\Psi}}}_{k,1}\hat{\bar{F}}_{t-1|T_k}^{(k)}, \\
    \hat{\tilde{F}}_{t|T_k}^{(k)} 
    &=&
     \hat{\tilde{\boldsymbol{\Psi}}}_{k,1}\hat{\tilde{F}}_{t-1|T_k}^{(k)},
\end{eqnarray}
%
where $\hat{\bar{F}}_{T_k|T_k}^{(k)}=\hat{\bar{F}}_{T_k}^{(k)}$, $\hat{\tilde{F}}_{T_k|T_k}^{(k)}=\hat{\tilde{F}}_{T_k}^{(k)}$. From these $h$-step-ahead predictions, we compute the root mean square forecasting error as
%
\begin{equation}\label{e:measure_rmsfe}
    RMSFE = \sqrt{\frac{1}{2dK}\sum_{k=1}^{2K}\sum_{t=T_k+1}^{T_k+h}\|X_{t}^{(k)} - \hat{X}_{t|T_k}^{(k)}\|_F^2}.
\end{equation}
%
We take up to 10-step-ahead predictions in this simulation.


For each simulation setting, we replicate estimation 100 times. In Section \ref{sse:result_main}, we let $(d,T,K)$ be:
%
\begin{itemize}
    \item[C1.] Growing numbers of variables: Fix $T=100$, $K=50$. Take $d=200,400$, and $800$.
    
    \item[C2.] Growing observation times: Fix $d=100$, $K=50$. Take $T=200,400$, and $800$.
    
    \item[C3.] Growing numbers of subjects: Fix $d=100$, $T=200$. Take $K=100,200$, and $400$.
\end{itemize}
%
For each case, we fix the ranks as $(r_J,r_G)=(2,2),(2,3),(3,3)$. As one can see from the results in Section \ref{sse:result_bootstrap}, these chosen pairs of ranks are mostly guaranteed to be found through the initial rank selection algorithm.


Along problem sizes described above, we consider the different combinations of control parameters as follows. The eigenvalues of transition matrices generated by the combinations below have been checked to be outside the unit circle. This confirmation guarantees the stationarity of the factor series. For each rank selection, we use the combinations P1 through P4 of model parameters as:
%
\begin{itemize}
    \item[P1.] SCA-PF2: $(\textrm{type},b_{\min},b_{\max},c_{\min},c_{\max},\sigma_{\xi},\sigma_{\varepsilon})=(1,0,1,5,10,0.2,1)$.

    \item[P2.] SCA-IND: $(\textrm{type},b_{\min},b_{\min},c_{\min},c_{\max},\sigma_{\xi},\sigma_{\varepsilon})=(2,0,1,5,10,0.3,1)$.
    
    \item[P3.] SCA-PF2 (Strong spatial): $(\textrm{type},b_{\min},b_{\max},c_{\min},c_{\max},\sigma_{\xi},\sigma_{\varepsilon})=(1,0,2,5,10,0.2,1)$.
    
    \item[P4.] SCA-PF2 (Strong temporal): $(\textrm{type},b_{\min},b_{\max},c_{\min},c_{\max},\sigma_{\xi},\sigma_{\varepsilon})=(1,0,1,10,20,0.2,1)$.
    
    \item[P5.] SCA-PF2 (Strong noise): $(\textrm{type},b_{\min},b_{\max},c_{\min},c_{\max},\sigma_{\xi},\sigma_{\varepsilon})=(1,0,1,5,10,0.2,4)$.
\end{itemize}
%
P5 is only used for illustrating the initial rank selection in Section \ref{ssse:initial_rank}. The covariance $\boldsymbol{\Phi}$ for type 1, along these four ranks, is
%
\begin{equation}\label{e:covariance_example}
    \boldsymbol{\Phi} 
    = 1,\quad 
    \begin{pmatrix}
    1 & -0.6 \\
    -0.6 & 1
    \end{pmatrix},\quad
    \begin{pmatrix}
    1 & -0.6 & 0.3 \\
    -0.6 & 1 & -0.6 \\
    0.3 & -0.6 & 1
    \end{pmatrix},\quad
    \begin{pmatrix}
    1 & -0.6 & 0.3 & -0.1\\
    -0.6 & 1 & -0.6 & 0.3\\
    0.3 & -0.6 & 1 & -0.6\\
    -0.1 & 0.3 & -0.6 & 1
    \end{pmatrix},
\end{equation}
%
and the covariance for type 2 is identity matrices with the corresponding ranks. We report the 5 measures for estimation accuracy (i.e., $R^2$ in \eqref{e:measure_R2}, RMSE in \eqref{e:measure_RMSE}
, $CC_B$, $CC_F$ and $CC_{\Psi}$ in \eqref{e:measure_CC}). For the last four measures, we report them for joint and two group individual structures. In addition, we provide the RMSFEs computed as \eqref{e:measure_rmsfe} along the forecasting horizon $h=1,\ldots,H(=10)$. The averages of RMSFEs are provided including the 5th and 95th largest values out of the replications.


Throughout the main simulation, we consider the three comparison methods as benchmarks. We admit that other benchmarks can possibly be considered. For example, one can replace AJIVE with COBE in our approach. However, for this replacement, the rest of the measures are heavily relying on the performance of data integration methods. This analysis has been covered in related literature so we do not consider it here. One can also replace the Yule-Walker equations with other calibrating methods, such as Kalman filtering. However, as stated in Section \ref{sse:reconstruction}, the additional assumption that the joint factors and the group individual factors are independent of each other, for instance, should be made. Moreover, since our noise terms have no particular structure, we do not expect to have a significant enhancement in the estimation. We omit this replacement for brevity.


The first alternative method is to fit the PARAFAC2 model through the two stages. Recall that through our three stages of the process, the first two stages are to fit the total block data by AJIVE and then fit PARAFAC2 models to three factorized data blocks (joint component blocks and two group individual component blocks). However, inspired by practical considerations, one can possibly fit PARAFAC2 to the whole data block first and extract the signal blocks. This signal naturally becomes the joint structure. Then, for the remainder of each group, one can fit PARAFAC2 again to extract the group individual structure. Then, the rest of the steps are the same as GRIDY. We denote this benchmark as double-SCA (DSCA). However, we stress that this method is not preferred in practice, despite being used by researchers, since in order to fit PARAFAC2 models, one has to know the ranks for the factors in advance. We point out that the AJIVE implicitly selects the joint rank and this joint rank estimation is quite reliable as shown in the literature. However, there is yet no way to extract the joint rank from the given rank of the whole series. One can do permutation for the combinations of ranks, which requires a heavy computation load. For this comparison, we assume that the joint rank is explicitly known.


We also focus on how the large number of $K$ subjects helps to recover the factor series. Note that as $K$ gets larger, the model is in favor of rotational determinacy in that they preserve the constraint \eqref{e:SCA-PF2} and \eqref{e:SCA-IND}. Related to this, we focus on fitting PARAFAC2. As a benchmark, ordinary PCA can be used in place of the SCA as follows. First, use PCA by imposing \eqref{e:orthogonality_factor}. Then find matrices, say $\bar{\mathbf{Q}}_{k},\tilde{\mathbf{Q}}_{k}$, satisfying 
%
\begin{eqnarray}
    \bar{\mathbf{B}}_{1}
    &=& \bar{\mathbf{Q}}_{k}\bar{\mathbf{B}}_{k},\quad k=2,\ldots,2K, \\
    \tilde{\mathbf{B}}_{1} 
    &=& \tilde{\mathbf{Q}}_{k}\tilde{\mathbf{B}}_{k},\quad k=2,\ldots,K,\\
    \tilde{\mathbf{B}}_{K+1} 
    &=& \tilde{\mathbf{Q}}_{k}\tilde{\mathbf{B}}_{k},\quad k=K+2,\ldots,2K,
\end{eqnarray}
%
and multiply the inverse of the matrices to factor series as well. The rest of the steps are also identical to GRIDY. We denote this method as PCA.


Finally, one can directly use the estimated factor series from SCA at the second stage. That is, one can omit the refitting of factor series in \eqref{e:refitting}. Obviously, the first two stages for estimations are identical. We denote this method as Unfitted.



\subsection{Simulation results}
\label{sse:result_main}


Due to the large volume of the plots, we explain the simulation results in the order of measures described in Section \ref{sse:design}. All figures are displayed at the end of this work. Figures \ref{fig:R2_d} through \ref{fig:R2_K} present $R^2$ from 4 methods including GRIDY as the number of variables $d$, the sample length $T$, and the number of subjects $K$ is increasing, respectively, according to C1, C2, and C3 as defined in Section \ref{sse:design}. Throughout all cases, GRIDY has a slightly better result than the other benchmarks. Interestingly, the values of $R^2$ statistics for C1 through C3 are not different from each other. The same is also suggested for the different rank combinations. The potential differences come more from the parameter settings, P1 through P4, that control the signal-to-noise ratio. Although it is delicate to define the ratio in this context, we can state that as the variability increases due to either larger variation of loadings matrices or factor series, the performance improves. Additionally, the independence between the factor series as in P2 generally leads to better performance. This result corresponds to the higher values in $CC_{\Psi}$ illustrated below.


Next, Figures \ref{fig:RMSE_d} through \ref{fig:RMSE_K} present the root mean square error of each block structure according to C1 through C3. The consecutive three panels for Joint, Group1, and Group2 in each row by rank combinations consist of the results from the same setting. As we can see, our method has the best performance in all settings. In particular, we can see that DSCA has remarkably worse performance than the other methods. The performance deteriorates when the ranks of factors are increasing for all cases. This suggests that researchers should not be using DSCA in practice, but rather adopting the GRIDY approach. Interestingly, our approach, even including Unfitted, seems robust along the increase in the ranks of factor series, or when the ranks are different.


The next three results are on the estimation performance of each component, loadings matrices, factor series, and transition matrices in factor VAR. Figures \ref{fig:CCB_d} through \ref{fig:CCB_K} present the measures $CC_B$ along C1 through C3, respectively. The Unfitted case produced similar results as GRIDY for $CC_B$: we exclude it from the illustration. Interestingly, although our approach is best overall even in this measure, the results across C1 through C3 are quite different. For example, although $CC_B$ values for all cases in C1 are clearly below 1, they reach nearly 1 in some cases of C2 and C3. This reveals that as the number of variables increases, having a good fit is difficult. Despite this challenge, the results for our approach guarantee good performance, while the results for other methods deteriorate. Similar observations apply to C2 and C3, but the results look more dramatic. As with the previous measures, we can see that the benchmarks perform worse as the number of factors increases. Another interesting point is that DSCA cannot handle the independent factor setting P2, across all cases. This points to another danger when using DSCA. Working well regardless of the dependence in factors is another advantage that our approach offers.



Figures \ref{fig:CCF_d} through \ref{fig:CCF_K} present the measures $CC_F$ according to C1 through C3, respectively. Our approach scored the best in this measure as well. The overall measure values for all methods have slightly higher scores when $d$ or $T$ is increasing. As for the other measures, DSCA performs worse when the number of factors increases. As the sample length becomes larger, the results contain more extreme values through all approaches. This supports our caution made in Section \ref{sse:design} that the measure $CC_F$ may be stringent to evaluate the estimation performance of factor series. Thus, when the sample length gets longer, the estimation is less likely to match the truth. Another interesting point is that as the number of subjects becomes larger, the performance pattern looks similar to the case of increasing $T$. 



As the last performance measure, Figures \ref{fig:CCPsi_d} through \ref{fig:CCPsi_K} show the result of $CC_{\Psi}$ according to C1 through C3, respectively. Our approach again gives the best results. A remarkable point is that, unlike the results for $CC_B$ and $CC_F$ where the consequences of DSCA are the most undesirable,  $CC_{\Psi}$ values from PCA deteriorate most rapidly along the increase of the number of factors. This is particularly significant at C2 and C3. 



Lastly, we report on the forecasting performance through all methods. Figures \ref{fig:forecast_d} through \ref{fig:forecast_K} are for the RMFSE according to C1 through C3, respectively. The results support the necessity of refitting. While our method, DSCA, and PCA have similar forecasting errors, the results from Unfitted are the worst. In particular, as the number of factor series increases, not only the error bar constructed by the 5th and 95th quantile becomes wider, the mean performance measure becomes significantly larger. This is because some of the outputs from Unfitted are so extreme. As the number of subjects gets larger, the quantile bands also become larger for Unfitted. This indicates that not only the extreme outputs can be produced, but also the performance cannot be guaranteed on average. Hence, only GRIDY with fully completed steps delivers better forecasting results.



\subsection{On the choice of initial rank selection}
\label{sse:result_bootstrap}


We consider several DGPs in Section \ref{sse:design} with various ranks and run the bootstrap procedure in Section \ref{ssse:initial_rank} if it can capture the correct initial ranks. Different sizes $(d,T)$ of problems are considered while $K$ is fixed. We do not consider the case of growing $K$ since the rotational bootstrapping is performed for each block, and gathering multiple data blocks does not help to estimate initial ranks. Then for all control parameter cases above, we generate the DGPs along the combinations of ranks as follows:
%
\begin{itemize}
    \item[R.] $(r_J,r_G) = (1,1),(1,2),(1,3),(2,2),(1,4),(2,3),(2,4),(3,3),(3,4),(4,4)$.
\end{itemize}
%
Note that along the increase of $r_J$ and $r_G$, the corresponding transition matrix $\Phi$ is adjusted by preserving \eqref{e:riccati} with $\boldsymbol{\Phi}$ in \eqref{e:covariance_example} and fixed $\sigma_{\xi}$. Accordingly, we run the rotational bootstrapping to evaluate the performance of the initial rank selection. For the combinations in R above, the initial ranks are $r=r_J+r_G=2,3,4,4,5,5,6,6,7,8$.


Figure \ref{fig:rank} shows several interesting results. First of all, the rank combinations used in Section \ref{sse:result_main} are purple, orange, and dark brown bars, respectively. These are correctly estimated for $(d,T)=(200,100)$ or $(d,T)=(100,200)$. This is also true for the rest of the higher initial ranks when $d$ or $T$ is increased to 400. This indicates that the large problem size seems advantageous in the same way the higher dimensions help in the usual factor models.


Furthermore, for the same initial total ranks within the same parameters and fixed problem sizes, the case where the number of joint factor series is equal to the number of group individual factors is more favorable than the case where the latter is larger. For example, the algorithm is more likely to choose the correct initial ranks in the case $(r_J,r_G)=(3,3)$ than the case $(r_J,r_G)=(2,4)$. Hence, larger ranks in the joint structure is favorable for selecting the correct ranks rather than larger ranks in the group individual structures.


As one can expect, the case P1, where the factors themselves are correlated, is less favorable than the case P2 where the factors are independent as the setting of the usual literature. The discrepancy becomes more dramatic when the true initial ranks are higher. 

Another interesting observation is that in cases P3 and P4, the rank selection has an advantage from the larger variability due to both the magnitude of loadings matrices and the standard deviation of the factor series. Regardless of what causes stronger signals, this turns out to be favorable for the rank selection. In the same fashion, the estimated ranks are less precise when the variance of the noise becomes larger. Although the algorithm correctly detects the number of factors in the case P5 when either $d$ or $T$ is 400, it is still delicate to choose the right initial rank when the noise is larger. 



\section{Data application}
\label{se:data_app}

\subsection{Autism Brain Imaging Data Exchange preprocessed data}
\label{sse:ABIDE}

In this section, we consider a large pre-processed dataset from the Autism Brain Imaging Data Exchange \citep[ABIDE preprocessed, e.g.][]{craddock:2013}.\footnote{The dataset can be found at http://preprocessed-connectomes-project.org/abide/\cite{abide:2016}.} ABIDE is an international collection of R-fMRI data, 20 site location marks are identified in total. The data consists of two different groups. Subjects with autism spectrum disorder (ASD) are marked as group 1. And the rest of the subjects are from the control group, denoted as group 2. The dataset from ABIDE is known to have been preprocessed by five teams with their preferred methods. 


We start analysis with 505 subjects from the ASD group, and 530 subjects from the control group. This dataset consists of an extensive array of BOLD signals. The Dosenbach brain atlas by \cite{dosenbach:2010} contains 160 ROIs. In our analysis, the average of the sample lengths over 1035 subjects is 193 time points. Each of 160 ROI labels consists of a combination of the networks and the number. More specifically, the network means the brain region where each ROI is measured. 6 unique regions as brain networks are identified. In the illustration, we set different colors for each region so that the same colors are used for representing the same unique region.


\subsection{Connection to VARMA model}
\label{sse:VARMA}

In this section, we briefly introduce the network construction by converting dynamic factor models into VAR models. Such a final form of the representation is called the vector autoregressive and moving average (VARMA) model defined as follows. We will see the usefulness of this representation to illustrate the connectivities of network structures.


Consider the dynamic factor model in \eqref{e:augmented_factor_1} through \eqref{e:augmented_factor_2}. From \eqref{e:filter_equation}, we assume that the joint and group individual latent factor series follow VAR(1), $\bar{F}_{t}^{(k)}=\bar{\mathbf{\Psi}}_k\bar{F}_{t-1}^{(k)} + \bar{\eta}_{t}^{(k)}$ and $\tilde{F}_{t}^{(k)}=\tilde{\mathbf{\Psi}}_k\bar{F}_{t-1}^{(k)} + \tilde{\eta}_{t}^{(k)}$, respectively. Then, one can rewrite the dynamic factor models as 
%
\begin{eqnarray}
    X_{t}^{(k)} 
    &=& \mathbf{B}_{g}F_{t}^{(k)} + E_{t}^{(k)} 
    = \begin{bmatrix} \bar{\mathbf{B}} & \tilde{\mathbf{B}}_{g} \end{bmatrix}
    \begin{bmatrix} \bar{F}_{t}^{(k)} \\ \tilde{F}_{t}^{(k)} \end{bmatrix} + E_{t}^{(k)} \\ 
    &=& \begin{bmatrix} \bar{\mathbf{B}} & \tilde{\mathbf{B}}_{g} \end{bmatrix}
    \left( \begin{pmatrix}
    \bar{\mathbf{\Psi}}_k & \bullet \\
    \bullet' & \tilde{\mathbf{\Psi}}_k
    \end{pmatrix} \begin{bmatrix} \bar{F}_{t-1}^{(k)} \\ \tilde{F}_{t-1}^{(k)} \end{bmatrix} + \begin{bmatrix} \bar{\eta}_{t}^{(k)} \\ \tilde{\eta}_{t}^{(k)} \end{bmatrix} \right) + E_{t}^{(k)} \\
    &=& \mathbf{B}_{g} \left( \mathbf{\Psi}_k F_{t-1}^{(k)} +\eta_{t}^{(k)} \right) + E_{t}^{(k)},
\end{eqnarray}
%
where $\bullet$ are undefined entries in transition matrix of $\{F_{t}^{(k)}\}$. A further assumption, such as no dependence between two types of factor series, is required to determine the quantities. In this illustration, we take $\bullet=\boldsymbol{0}_{r_J \times r_G}$ for all $k$. By using the fact that $F_{t}^{(k)} = (\mathbf{B}_{g}'\mathbf{B}_{g})^{-1}\mathbf{B}_{g}'(X_{t}^{(k)}-E_{t}^{(k)})$ where
%
\begin{equation}
    (\mathbf{B}_{g}'\mathbf{B}_{g})^{-1} 
    = \begin{pmatrix}
    (\bar{\mathbf{B}}'\bar{\mathbf{B}})^{-1} & \boldsymbol{0}_{r_J \times r_G} \\
    \boldsymbol{0}_{r_G \times r_J} & (\tilde{\mathbf{B}}_g'\tilde{\mathbf{B}}_g)^{-1}
    \end{pmatrix},
\end{equation}
%
$X_{t}^{(k)}$ can be expressed as
%
\begin{eqnarray}
    X_{t}^{(k)} 
    &=& \left( \bar{\mathbf{B}}\bar{\mathbf{\Psi}}_k(\bar{\mathbf{B}}'\bar{\mathbf{B}})^{-1}\bar{\mathbf{B}}' + \tilde{\mathbf{B}}_g\tilde{\mathbf{\Psi}}_k(\tilde{\mathbf{B}}_g'\tilde{\mathbf{B}}_g)^{-1}\tilde{\mathbf{B}}_g' \right) X_{t-1}^{(k)} + \zeta_{t}^{(k)} \\
    &=:& \left( \bar{\mathbf{\Theta}}_{k} + \tilde{\mathbf{\Theta}}_{k} \right) X_{t-1}^{(k)} + \zeta_{t}^{(k)},\quad\ k=1,\ldots,2K, \label{e:VARMA}
\end{eqnarray}
%
where $\zeta_{t}^{(k)}$ is a centered noise with covariance matrix $\boldsymbol{\Sigma}_{\zeta,k}$, which is given by
%
\begin{equation}\label{e:VARMA_covariance}
    \boldsymbol{\Sigma}_{\zeta,k} = \left( \bar{\mathbf{\Theta}}_{k} + \tilde{\mathbf{\Theta}}_{k} \right)\left(\mathbf{I}_d + \boldsymbol{\Sigma}_{E,k}\right) + \bar{\mathbf{B}} \boldsymbol{\Sigma}_{\bar{\eta},k} \bar{\mathbf{B}}' + \tilde{\mathbf{B}}_g \boldsymbol{\Sigma}_{\tilde{\eta},k} \tilde{\mathbf{B}}_g',
\end{equation}
%
and $\bar{\mathbf{\Theta}}_{k} = \bar{\mathbf{B}}\bar{\mathbf{\Psi}}_k(\bar{\mathbf{B}}'\bar{\mathbf{B}})^{-1}\bar{\mathbf{B}}', \tilde{\mathbf{\Theta}}_{k}=\tilde{\mathbf{B}}_g\tilde{\mathbf{\Psi}}_k(\tilde{\mathbf{B}}_g'\tilde{\mathbf{B}}_g)^{-1}\tilde{\mathbf{B}}_g'$. Hence, for each subject $k$, the dynamics is defined by the two transition matrices $\bar{\mathbf{\Theta}}_{k},\tilde{\mathbf{\Theta}}_{k}$ in \eqref{e:VARMA} and the error covariance matrix $\boldsymbol{\Sigma}_{\zeta,k}$ in \eqref{e:VARMA_covariance}. The equation \eqref{e:VARMA} is a high-dimensional VAR model (strictly speaking, it is a VARMA model because of the temporal dependence in $\zeta_{t}^{(k)}$; see \cite{bhamidi:2023}). Note that, unlike the high-dimensional VAR models where the sparsity on the transition matrices is imposed, the matrices $\bar{\mathbf{\Theta}}_{k},\tilde{\mathbf{\Theta}}_{k}$ are not sparse objects in general because of the loadings matrices. Instead, the model is akin to reduced-rank VAR since the rank of the transition matrix constructed by the sum of low-rank matrices cannot exceed the sum of two ranks.


In the network context, the VAR transition matrix and covariance of the noise are called the directed network and contemporaneous network, respectively. This is because the observation $X_t^{(k)}$ can be considered as the sum of the impact from the previous observations and instantaneous shocks and the directed network structure accounts for the temporal dependence. From this perspective, we examine the differences between the two groups by comparing how their model network connectivities differ.


\subsection{Application results}
\label{sse:result_application}


Figure \ref{fig:rank_ABIDE} presents the results of the initial rank selection. The use of different colors indicates data collected from 20 different sites. The estimated initial ranks belong to three regions: the left with lower ranks, the middle with intermediate ranks, and the right with higher ranks. Interestingly, the distribution of initially estimated ranks for group 1 closely resembles the distribution of estimates in group 2. This suggests that the estimation of the number of latent factor series is significantly influenced by the types of fMRI scanners used, which may vary across different sites. Moreover, the distribution of the sites based on the estimated initial ranks demonstrates similar patterns of anatomical quality assessment (QA) measures for all the ABIDE subjects provided by \cite{craddock:2013}. This indicates that the results are dependent on the preprocessing differences across the various data-collecting sites.


By matching QA measures displayed on their official webpage to the initial rank estimation results, the sites whose dataset is estimated as low rank are likely to have high contrast-to-noise ratio (CNR), foreground-to-background energy ratio (FBER), and signal-to-noise ratio (SNR). Such similar patterns indicate the rotational bootstrapping algorithm for initial rank selection determines the rank based on the strength of the signals in general. In particular, the algorithm does not distinguish between the true signals from the subjects and the artificially added signals during the pre-processing as we see the dataset having a high percentage of Artificial Voxels as having high estimated ranks.


For this reason, we exclude the data where the initially estimated ranks are higher than 25. Consequently, the numbers of subjects for groups 1 and 2 become 205 and 204, respectively. Then the majority of the estimated ranks is 2. However, if we think of the estimated ranks by groups 1 and 2 separately, they become 2 and 4, respectively. Interestingly, the AJIVE result for the joint rank selection is 2, which means the rank for the joint structure becomes 2. This implies that only subjects belonging to group 2 have rank 2 group individual structure while the subjects in group 1 have no group individual structure. This finding points to the first possible differences in brain functionality between the patients and control groups, but more analysis is certainly warranted. For the rest of the analysis, we set the joint structure as rank 2 and model the group individual structure only for group 2 as rank 2.


In Figure \ref{fig:R2_ABIDE}, we present the $R^2$ statistics, obtained by regressing the univariate observations $X_{i,t}^{(k)}$ on a particular factor estimate \cite[e.g.][]{stock:2002,jungbacker:2015}. All variables are reordered based on their networks, each of the 6 regions is represented by its own color. Both the Autism group (the left panel) and the control group (the right panel) results are displayed separately. Since the number of factors for each group is 2 and 4, respectively, the right panel has two more plots on the bottom, with the $R^2$ statistics for individual factors against each variable over all subjects. We can see that the patterns of the $R^2$ statistics from the joint factors are quite similar for the two groups, except for the vertical scales of the statistics. This is because their spatial information estimated through the loadings matrix is identical. The differences in vertical scales are due to the estimated factor series.


Furthermore, the $R^2$ statistics from the two individual structures are quite different from the displayed patterns in the joint structure. In particular,  not only the networks which have the larger $R^2$ are different from the results of the two joint factors but also different between individual factors. It is expected that the group individual structure of the control group can contribute to explaining more variability of the dataset, which makes the difference from the Autism group. Interestingly, if each $R^2$ statistic from all factors is aggregated, the total amount of $R^2$ statistics is quite the same for both groups. That is, the variability explained by the joint components from the Autism group is larger in general than that from the control group. At the same time, the variability explained by the group individual structure for group 2 cannot be ignored.


We next consider the various estimates separately. Figure \ref{fig:loadings_ABIDE} displays the estimated loading matrices, which include both the joint and group individual structures. Interestingly, the distributions of $R^2$ statistics for each factor are quite similar to the distributions of values of the loading matrices when ignoring their signs. By multiplying with the factor series, spatial information is projected onto the observations evolving over time. Additionally, Figure \ref{fig:factor_ABIDE} presents the estimated factor series for each structure. Each plot contains 10 randomly selected subjects for better display clarity, with the series in each partitioned plot with the same color belonging to the same subject. As expected from the usual PCA result, the first factor in the joint components fluctuates the most, with the magnitude being proportional to its singular values. However, while the ranges of oscillation for the other factor series are small, their contributions to the $R^2$ statistics seem quite impactful. In particular, we observe that the deviation within the same ROI is quite large. This implies that not only the first series but also other factor series contribute to explaining the variability. Figure \ref{fig:series_ABIDE} further supports the idea that each factor contributes to explaining the variability. While the covariance matrices of the noise for the second series are relatively small compared to those for the first factor series, the entries in the VAR transition matrices for each factor are almost equally large. In particular, the estimated off-diagonal entries are quite large, implying that their interaction cannot be ignored. Therefore, by combining the different loading matrices of the joint and group individual structures, the model can explain the difference. At the same time, the different realizations and various combinations of factor series contribute to the differences over time evolution within the subject.


Now, we consider VARMA construction explained in Section \ref{sse:VARMA} for illustrating network structures. Figure \ref{fig:roi_ABIDE} summarizes the network structures over all subjects. The left panel presents the averages of the directed networks and the contemporaneous networks for group 1, and the results for group 2 are displayed in the right panel. The two heatmaps in the top panel are about the directed networks and the two in the bottom panel are contemporaneous networks. The ROIs for the variables are the same as in the previous figures. We can see clear but different block structures in the directed networks. Interestingly, the sizes and the forms of blocks are not the same between groups 1 and 2 for the directed network, while it is more difficult to see the particular blocks in the contemporaneous networks. In addition, it is instructive to state the differences and similarities between the groups identified above in terms of our model components. We found that the directed network denoted by $\bar{\mathbf{\Theta}}_{k}$ for group 1 is quite different from the directed network for group 2 as $\bar{\mathbf{\Theta}}_{k} + \tilde{\mathbf{\Theta}}_{k}$, where $\bar{\mathbf{\Theta}}_{k}$ presents the network for the joint structure. This means that the group individual structure for group 2 denoted as $\tilde{\mathbf{\Theta}}_{k}$ makes the difference. However, the two contemporaneous networks expressed as $\bar{\mathbf{\Theta}}_{k} (\mathbf{I}_d +\boldsymbol{\Sigma}_{E,k}) + \bar{\mathbf{B}} \boldsymbol{\Sigma}_{\bar{\eta},k} \bar{\mathbf{B}}'$ for group 1 and $(\bar{\mathbf{\Theta}}_{k} + \tilde{\mathbf{\Theta}}_{k})(\mathbf{I}_d +\boldsymbol{\Sigma}_{E,k}) + \bar{\mathbf{B}} \boldsymbol{\Sigma}_{\bar{\eta},k} \bar{\mathbf{B}}' + \tilde{\mathbf{B}}_g \boldsymbol{\Sigma}_{\tilde{\eta},k} \tilde{\mathbf{B}}_g'$ for group 2 are similar.


The forecasting performance for the two groups is similar. It may be related to that the variability explained by their signals through the directed network is also similar for both groups, as we have seen through the $R^2$ statistics. Table \ref{table:rmsfe_ABIDE} presents the forecasting errors of each group over 10-step forecasting horizons with two benchmarks used in the previous section. 



In summary, the brain connectivity dynamics over time for Autism patients appear to differ, on average, from the temporal network structure of the subjects in the control group. However, the contemporaneous network structures, characterized by a mixture of idiosyncratic noise and instantaneous shocks through brain activity, exhibit high similarity. These differences in connectivity patterns and biological similarities may support other studies analyzing resting-state networks \cite[e.g.][]{yao:2016,hong:2018}. Therefore, it is crucial to consider both spatial and temporal information when examining brain connectivity, which highlights the utility of our approach. Our model can explain how the group-specific structure contributes to the variability of observations. This finding can be applied to other ABIDE studies that demonstrate differences in functional connectivity between the Autism group and the control group, as well as within each group's subjects \cite[e.g.][]{subbaraju:2017,easson:2019}. The GRIDY framework can further facilitate the study of heterogeneity in R-fMRI across different multi-site differences \cite[e.g.][]{abraham:2017,wang:2019} in future research.


\begin{table}[t]
\centering
\resizebox{0.8\textwidth}{!}{%
\begin{tabular}{ccccccccccccc}
 &  &  & \multicolumn{10}{c}{Forcasting horizon} \\ \cline{4-13} 
 &  &  & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ \hline
\multirow{6}{*}{RMSFE} & \multirow{3}{*}{Group 1} 
 & GRIDY &  0.4619 & 0.8512 & 1.1385 & 1.3821 & 1.5725 & 1.7346 & 1.9037 & 2.0642 & 2.1450 & 2.2328 \\
 &  & DSCA &  0.4617 & 0.8510 & 1.1381 & 1.3817 & 1.5718 & 1.7338 & 1.9032 & 2.0639 & 2.1445 & 2.2321  \\
 &  & PCA &  0.5141 & 0.8914 & 1.1487 & 1.3726 & 1.5531 & 1.7110 & 1.8772 & 2.0324 & 2.1133 & 2.2039  \\ \cline{2-3}
 & \multirow{3}{*}{Group 2} 
  & GRIDY &  0.4428 & 0.8307 & 1.1162 & 1.3425 & 1.5381 & 1.7112 & 1.8748 & 2.0152 & 2.0829 & 2.1559 \\
 &  & DSCA &  0.4434 & 0.8319 & 1.1179 & 1.3445 & 1.5402 & 1.7134 & 1.8754 & 2.0148 & 2.0820 & 2.1549  \\
 &  & PCA &  0.5638 & 0.9779 & 1.2435 & 1.4411 & 1.6121 & 1.7623 & 1.9090 & 2.0407 & 2.1106 & 2.1874 \\ \hline
\end{tabular}%
}
\caption{Root mean square forecasting errors for each group over 10-step forecasting horizon with two benchmarks.}
\label{table:rmsfe_ABIDE}
\end{table}




\section{Discussion}
\label{se:discussion}

In this study, we suggest a novel approach to treat the data integration of dynamic factor models where the resulting integrated model is considered as the summation of joint components, group individual components, and the observation noise. Each signal structure is further decomposed into loadings matrices and latent factor series. In particular, those series are assumed to follow VAR models. Unlike the usual dynamic factor models, our model allows the different factor series within the same structure to be correlated. In order to handle such latent and correlated structures, we introduce the simultaneous component analysis derived from tensor decomposition. Through the simulations, we compare our model with several benchmarks that can be used in the considered setting. With the various estimation performance measures and the forecasting errors, the empirical evidence suggests that our approach has the best performance over all possible combinations of problem scenarios. We use ABIDE preprocessed dataset to illustrate how our approach can be used to analyze the two network structures, directed and contemporaneous, for the patients and control groups. This turns out to be useful to discover the differences in characteristics between the two groups.


Several promising extensions of this work could be pursued in the future. First of all, one could adopt the more efficient algorithm for estimating PARAFAC2 model, developed by \cite{perros:2017}. This algorithm will be indispensable to speed up the computation and memory efficiency when both the number of subjects increases and brain images have higher resolution \cite[e.g.][and reference therein]{zhao:2018}. In addition, from the VARMA representation above, our model is akin to multi-VAR of \cite{fisher:2022} but which assumes sparse VAR transition matrices. One could seek to clarify the differences between GRIDY and multi-VAR, and when either approach should be preferred. Lastly, a highly promising avenue for future research involves considering multiple subgroups. In this case, incorporating a partially-shared structure \cite[e.g.][]{gaynanova:2019,prothero:2022} would allow for only some grouped subjects to possess common features when the number of groups exceeds two.






\section*{Acknowledgement}

Vladas Pipirass research was partially supported by the grants NSF DMS 1712966, DMS 2113662, and DMS 2134107.

\clearpage
%%%%% R2 
% Figure environment removed

% Figure environment removed

% Figure environment removed


%%%%% RMSE 
% Figure environment removed

% Figure environment removed

% Figure environment removed


%%%%% CCB


% Figure environment removed

% Figure environment removed

% Figure environment removed


%%%%% CCF
% Figure environment removed

% Figure environment removed

% Figure environment removed


%%%%% CC Phi
% Figure environment removed

% Figure environment removed

% Figure environment removed




%%%%% Forecast
% Figure environment removed

% Figure environment removed

% Figure environment removed



%%%%% Rank
% Figure environment removed


%%%%% ABIDE rank
% Figure environment removed


%%%%% ABIDE R2
% Figure environment removed


%%%%% ABIDE loadings
% Figure environment removed


%%%%% ABIDE factors
% Figure environment removed


%%%%% ABIDE factors2
% Figure environment removed


%%%%% ABIDE heatmap
% Figure environment removed


\clearpage
\small
\bibliography{multifac}

\end{document}
