\documentclass[11pt,reqno]{amsart}
\usepackage[tmargin=1in,bmargin=1in,rmargin=1in,lmargin=1in]{geometry}
\usepackage[mathscr]{eucal}
\usepackage[breaklinks=true]{hyperref}
\usepackage[dvipsnames]{xcolor}
\usepackage{enumerate}

\usepackage{amsmath,amsthm,amsfonts,amssymb,tocvsec2}
\usepackage{mathdots}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{rlt}{Theorem}
\renewcommand{\therlt}{\Alph{rlt}}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{cor}[theorem]{Corollary}
\newtheorem{utheorem}{\textrm{\textbf{Theorem}}}
\newcommand{\inc}[2]{{#1}^{#2, \uparrow}}
\newcommand{\bd}[1]{d^{[#1]}}
\newcommand{\z}[1]{z^{\langle #1 \rangle}}
\renewcommand*{\theutheorem}{\Alph{utheorem}}

\theoremstyle{definition}
\newtheorem{defn}[theorem]{Definition}
\newtheorem{obs}{Observation}
\newtheorem{rem}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}

\numberwithin{equation}{section}

\DeclareMathOperator{\adj}{adj}
\DeclareMathOperator{\omg}{\Omega}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\inte}{int}
\DeclareMathOperator{\diag}{diag}
\newcommand{\altr}[1]{\mathbb{R}^{#1}_{\rm alt}}
\newcommand{\lcp}[1]{\mathrm{LCP}(#1,q)}
\newcommand{\lcpn}[1]{\mathrm{LCP}(#1)}
\newcommand{\sol}[1]{\mathrm{SOL}(#1,q)}
\newcommand{\rn}[1]{\mathbb{R}^{#1}}
\newcommand{\altg}[1]{\mathbb{R}^{#1}_{\rm alt_{\geq}}}
%\DeclareMathOperator{\altr}{\mathfrak{a}lt^r}
%\DeclareMathOperator{\altn}{\mathbb{R}^n_{alt}}
%\DeclareMathOperator{\altm}{\mathbb{R}^m_{alt}}



\begin{document}
	\title[Sign regularity: variation diminution and linear preserver problem]{Sign regularity: Refinement of Gantmacher--Krein--Motzkin Results on Variation Diminution and the Linear Preserver Problem}
	
	\author{Projesh Nath Choudhury and Shivangi Yadav}
	\address[P.N.~Choudhury]{Department of Mathematics, Indian Institute of Technology Gandhinagar, Gujarat 382055, India}
	\email{\tt projeshnc@iitgn.ac.in}
	\address[Shivangi~Yadav]{Department of Mathematics, Indian Institute of Technology Gandhinagar, Gujarat 382055, India; Ph.D. Student}
	\email{\tt shivangi.yadav@iitgn.ac.in, shivangi97.y@gmail.com}
	
	\date{\today}
	
	\begin{abstract}
		Variation diminution (VD) is a fundamental property in total positivity theory, first studied by Fekete--P\'olya (1912) for one sided P\'olya frequency sequences, followed by Schoenberg (1930), and by Motzkin (1936) who characterized sign regular (SR) matrices using VD and some rank hypotheses. A classical theorem in 1950 by Gantmacher--Krein characterized all $m \times n$ strictly sign regular (SSR) matrices for $m>n$ using this property. In this article we strengthen their result by characterizing all  $m \times n$ SSR matrices using VD. We further characterize strict sign regularity of a given sign pattern in terms of VD together with a natural condition motivated by total positivity. We then refine Motzkin's characterization of SR matrices by omitting the rank condition and specifying the sign pattern.  More strongly, these characterizations employ single test vectors with alternating sign coordinates -- i.e., lying in an alternating bi-orthant.
		
		The second contribution of our work includes the study of linear preservers of SR and SSR matrices. The linear preserver problem is an important question in matrix theory and operator theory. We classify all linear mappings $\mathcal{L}:\mathbb{R}^{m\times n}\to\mathbb{R}^{m\times n}$ that preserve: (i) sign regularity and (ii) sign regularity with a given sign pattern, as well as strict versions of these.
	\end{abstract}
	
	\subjclass[2020]{15B48, 15A86 (primary); 15A24 (secondary)}
	
	\keywords{Strict sign regularity, sign regularity, total positivity, variation diminishing property, linear preserver problem}
	
	\maketitle
	
	\vspace*{-11mm}
	\settocdepth{section}
	\tableofcontents
	
	%%%%%%%%%%%%%%%%%%%% Section 1 %%%%%%%%%%%%%%%%%%%
	\section{Introduction and main results} \label{section introduction}
	Given integers $m,n\geq k\geq1$, an $m\times n$ real matrix $A$ is \textit{strictly sign regular of order $k$} (SSR$_k$) if there exists a sequence of signs $\epsilon_r\in\{1,-1\}$ such that every $r\times r$ minor of $A$ has sign $\epsilon_r$ for all $1\leq r\leq k$.  A SSR$_k$ matrix $A$ is \textit{strictly sign regular} (SSR) if $k=\mathrm{min}\{m,n\}$. If minors are allowed to also vanish, then $A$ is correspondingly said to be \textit{sign regular of order $k$} (SR$_k$) and \textit{sign regular} (SR). For a SSR (respectively SR) matrix $A$, if $\epsilon_r=1$ for all $r\geq1$, then A is said to be \textit{totally positive} (TP) (respectively \textit{totally non-negative} (TN)). These matrices have numerous applications in analysis, approximation theory, cluster algebras, combinatorics, differential equations, Gabor analysis, integrable systems, matrix theory, probability and statistics, Lie theory, and representation theory \cite{Ando87,BGKP20,BFZ96,Bre95,Chepuri,fallat-john,FZ00,FZ02,gantmacher-krein,GW96,GRS18,Karlin64,Karlinsplines,Khare20,KW14,Lo55,Lu94,pinkus,Ri03,Schoenberg46,S55,Whitney}.
	
	SR and SSR matrices $A$ enjoy the variation diminishing property, which states that the number of sign changes of $A\mathbf{x}$ is bounded above by the number of sign changes of $\mathbf{x}$, where  $\mathbf{x}$ is a vector. Variation diminution is considered to have originated from the famous 1883 memoir of Laguerre \cite{Laguerre}. P\'{o}lya coined the phrase ``variation diminishing" (``variationsvermindernd" in German) when Fekete showed (using P\'{o}lya frequency sequences in \cite{FP12}) the following result of Laguerre on the sign changes in the coefficients of power series: \textit{if $f(x)$ is a polynomial and $s\geq0$, then the variations var($e^{sx}f(x)$) in the Maclaurin coefficients of $e^{sx}f(x)$ do not increase in $0\leq s<\infty$ and hence are bounded above by var($f$)$<\infty$}. %In 1912, Fekete \cite{FP12} reformulated and proved this result, in correspondence with P\'{o}lya, using P\'{o}lya frequency sequence and the variation diminishing property.
	For more examples and instances of the variation diminishing property in classical analysis, we refer to \cite{PS1925}; there are also applications to the theory splines -- early paper include \cite{Curry, SW53}. We now look back on some fundamental contributions in this direction: in 1930, Schoenberg \cite{S30} initiated the study of the variation diminishing property of a matrix. He showed that SR matrices satisfy this property. In his 1936 Ph.D. thesis \cite{Mot36}, Motzkin then characterized all $m \times n$ matrices satisfying certain rank-constraints as well as the VD property, by showing that they are precisely SR matrices. Thereafter, in 1950, Gantmacher--Krein\footnote{Gantmacher--Krein stated this result for $m\geq n$; however, their proof seems to work only for $m>n$. Later in 1968, Karlin \cite{K68} stated (and proved) this result for the first time using $m>n$.}  \cite{GK50} characterized $m\times n$ SSR matrices using the variation diminishing property for $m>n$. In this article, we extend their result %our goal is to characterize SSR matrices via variation diminution 
	by removing the size restriction $m>n$.% on the size of the matrices imposed by Gantmacher--Krein.
	
	The following definitions and notations are essential in order to formulate these results.
	\begin{defn}
		Let $m,n\geq1$ be integers.
		\begin{enumerate}
			\item For $\mathbf{x}=(x_1,\ldots,x_n)^T\in\mathbb{R}^n$ we denote by $S^-(\mathbf{x})$ the number of sign changes in the sequence $x_1,\ldots,x_n$ after discarding all zero entries. We define $S^-(\mathbf{0}):=0$. Next, allocate to each zero entry of $\mathbf{x}$ a value of 1 or $-1$, and denote by $S^+(\mathbf{x})$ the maximum number of sign changes in the resulting sequence. We set $S^+(\mathbf{0}):=n$ for $\mathbf{0}\in\mathbb{R}^n.$
			\item A \textit{contiguous submatrix} is one whose rows and columns are indexed by consecutive integers.
			\item Let $\adj(A)$ denote the adjugate matrix of A. If $n=1$, we define $\adj A:=1$ to be the determinant of the empty matrix.
			\item Let $\mathbf{e}^i$ denote the unit vector in $\mathbb{R}^n$ whose $i^{th}$ entry is 1 and the rest are zero.
			\item For an SSR (SR) matrix $A \in \mathbb{R}^{m \times n}$, we define its \textit{signature} or \textit{sign pattern} to be the ordered tuple $\epsilon = (\epsilon_1, \dots, \epsilon_{\min\{m,n\}})$, with $\epsilon_r$ denoting the sign of all $r \times r$ minors of $A$. We set $\epsilon_0 := 1$
			\item Given integers $m,n\geq k\geq 1$ and a sign pattern $\epsilon=(\epsilon_1,\ldots,\epsilon_k)$, an SSR$_k$ (SR$_k$) matrix $A\in \mathbb{R}^{m\times n}$ is called SSR$_k({\epsilon})$ (SR$_k({\epsilon})$) if the sign pattern of the minors of $A$ of order at most $k$ is given by $\epsilon$. If $k=\min\{m,n\}$ then we simply write SSR$(\epsilon)$ (SR$(\epsilon)$).
			\item Let $A\begin{pmatrix}
				i_1,\ldots,i_p \\
				j_1,\ldots,j_q
			\end{pmatrix}$ denote the $p\times q$ submatrix of $m\times n$ matrix $A$ indexed by rows $1\leq i_1<\cdots<i_p\leq m$ and columns $1\leq j_1<\cdots<j_q\leq n$, where $1\leq p\leq m$ and $1\leq q\leq n$.
		\end{enumerate}
	\end{defn}
	We now state our main results. Our first result characterizes SSR matrices of arbitrary sizes using the variation diminishing property, parallel to Motzkin's 1936 result for SR matrices. %where order of matrices can bear any relation to each other. 
	%%%%%%%%%%%%%%%%%% Theorem A %%%%%%%%%%%%%%%%%%
	\begin{utheorem}\label{A}
		Given $A\in\mathbb{R}^{m\times n}$,  $A$ satisfies the variation diminishing property, $S^+(A\mathbf{x})\leq S^-(\mathbf{x})$ for all $\mathbf{0}\neq\mathbf{x}\in\mathbb{R}^n$ if and only if $A$ is SSR.
	\end{utheorem} 
	The above theorem ensures that if a matrix $A$ satisfies the variation diminishing property then it is SSR. However, it does not give any information about the in-built sign pattern of the minors of each order of $A$. Our next result aims to fill this gap, and it ensures that we can obtain an SSR matrix with a sign pattern of desired choice at the cost of imposing a natural condition along with the variation diminishing property, which we have stated in part (2) of Theorem \ref{B}. Further, we have refined part (2) below in part (3) where exactly one vector $\mathbf{x}^{A_k}$ for every contiguous submatrix $A_k$ of $A$ is sufficient to guarantee that $A$ is SSR$(\epsilon)$.
	%%%%%%%%%%%%%%%%%% Theorem B %%%%%%%%%%%%%%%%%%
	\begin{utheorem}\label{B}
		Given $A\in\mathbb{R}^{m\times n}$ and $\epsilon=(\epsilon_1,\ldots,\epsilon_{\mathrm{min}\{m,n\}})$, the following statements are equivalent.
		\begin{itemize}
			\item[(1)] $A$ is SSR$({\epsilon})$.
			\item[(2)] For all $\mathbf{0}\neq\mathbf{x}\in\mathbb{R}^n$, we have $S^+(A\mathbf{x})\leq S^-(\mathbf{x})$. Further, for all integers $0\leq r\leq \mathrm{min}\{m,n\}-1$ and $\mathbf{x}\in\mathbb{R}^n$ with $S^-(\mathbf{x})=r$, if $S^+(A\mathbf{x})=S^-(\mathbf{x})$ and $A\mathbf{x}\neq\mathbf{0}$, then the sign of the first (last) component of $A\mathbf{x}$ (if zero, the sign given in determining $S^+(A\mathbf{x})$) agrees with $\epsilon_r\epsilon_{r+1}$ times the sign of the first (last) non-zero component of $\mathbf{x}$. \smallskip
			
			In fact, these are also equivalent to the following assertion with a severely reduced test sets:\smallskip
			
			\item[(3)] For every contiguous square submatrix $A_k$ of $A$ of size $k\times k$, where $1\leq k\leq \mathrm{min}\{m,n\}$, and given any fixed vector $\mathbf{0}\neq\mathbf{v}:=(\alpha_1,-\alpha_2,\ldots,(-1)^{k-1}\alpha_k)^T\in\mathbb{R}^k$ with all $\alpha_j\geq0$, we define the vector
			\begin{equation}\label{x^A_k}
				\mathbf{x}^{A_k}:=\adj(A_k)\mathbf{v}.
			\end{equation}
			Then $S^+(A_k\mathbf{x}^{A_k})\leq S^-(\mathbf{x}^{A_k})$. Moreover, for all integers $0\leq r\leq k-1$ and $\mathbf{x}^{A_k}\in\mathbb{R}^k$ with $S^-(\mathbf{x}^{A_k})=r$, if $S^+(A_k\mathbf{x}^{A_k})=S^-(\mathbf{x}^{A_k})$, then the sign of the first (last) component of $A_k\mathbf{x}^{A_k}$ (if zero, the sign given in determining $S^+(A_k\mathbf{x}^{A_k})$) agrees with $\epsilon_{r}\epsilon_{r+1}$ times the sign of the first (last) non-zero component of $\mathbf{x}^{A_k}$.
		\end{itemize}
	\end{utheorem}
	\begin{rem}
		As a consequence of Theorem \ref{B}, one can reduce the test set in Theorem \ref{A} as well, to a single vector $	\mathbf{x}^{A_k}:=\adj(A_k)\mathbf{v}$ for each contiguous submatrix, and where $\mathbf{v}$ is as in Theorem \ref{B}, with alternate signed coordinates.
	\end{rem}
	In a sense, Theorems \ref{A} and \ref{B} are the culmination of many previous results, extending work on SR matrices (Schoenberg \cite{S30}, Motzkin \cite{Mot36} and Gantmacher--Krein \cite{GK50}) and TP/TN matrices (Gantmacher--Krein \cite{GK50}, Brown--Johnstone--MacGibbon \cite{BJM81} and Choudhury \cite{C22}).
	
	Along with variation diminution, the study of linear preserver problems is fundamental. A \textit{linear preserver problem} asks, given a subset $S$ of a vector space $V$, to characterize all linear transformations $\mathcal{L}:V\to V$ such that $\mathcal{L}(S)=S$. Such a linear transformation $\mathcal{L}$ is called a \textit{linear preserver of $S$} or \textit{$S$-preserver}. In 1887, Frobenius \cite{Frobenius1897} characterized the general form of all determinant preserving linear maps on matrix algebras, which is regarded as the first result on linear preserver problems. The linear preserver problem has since been widely studied in matrix theory and operator theory. Some examples include the problem of spectrum preserving transformations on a space of bounded linear operators over a Banach space (Jafarian--Sourour \cite{Jaf-Sour86}), linear preservers of the unitary group in $\mathbb{C}^{n\times n}$ (Marcus \cite{Marcus59}) and on arbitrary $C^*$-algebras (Russo--Dye \cite{Russo-Dye66}), linear transformations preserving operators with fixed rank (Beasley \cite{Beasley88} and Omladi\v c--\v Semrl \cite{Om-Sem93}),  linear transformations on operator algebras preserving absolute values (Radjabalipour--Seddighi--Taghavi \cite{Radja-Sedd-Tag01}),  and linear maps preserving invertibility (Sourour \cite{Sour96}). For more details about linear preserver problems and some techniques to tackle them, we refer to \cite{GLS2000,Li-Pierce01,Molnar07}. The classification of linear preservers of various forms of positivity has long been studied and is an important problem in the preserver literature -- for instance, linear preservers of positive semi-definite matrices (all of whose principal minors are non-negative). A related notion is that of total positivity. The linear preservers of TP and TN matrices had been classified for square matrices by Berman--Hershkowitz--Johnson \cite{BHJ85}.   Our next two results extend their work in several ways. First, the results below hold for arbitrary sizes. Secondly, we classify linear preservers of SSR and SR matrices (allowing all sign patterns). Third, we do this for every fixed sign pattern.
	\begin{defn}
		We need the following definitions and notations in order to state these results.
		\begin{enumerate}
			\item A square matrix is said to be an \textit{exchange matrix} if it is an antidiagonal $0$-$1$ matrix of the form
			\[P_n:=\begin{pmatrix}
				0 & 0 & \ldots & 0 & 1 \\
				0 & 0 & \ldots & 1 & 0 \\
				\vdots & \vdots & \iddots & \vdots & \vdots \\
				0 & 1 & \ldots & 0 & 0 \\
				1 & 0 & \ldots & 0 & 0 
			\end{pmatrix}_{n\times n}.\]
			\item Let $\mathcal{SR}$ denote the class of SR matrices of a given fixed size. Similarly we can define $\mathcal{SR}_2(\epsilon)$, $\mathcal{SR}(\epsilon)$, $\mathcal{SSR}$, and $\mathcal{SSR}(\epsilon)$, where $\mathcal{SR}_2(\epsilon)$ is only concerned with  the signs $\epsilon_1$ and $\epsilon_2$. 
			\item Let $P(S)$ denote the set of $S$-preservers, for $S$ among $\mathcal{SR}, \mathcal{SR}_2, \mathcal{SR}(\epsilon), \mathcal{SR}_2(\epsilon)$.
		\end{enumerate} 
	\end{defn}
	Our first theorem in this regard characterizes all linear preservers for the class of SR matrices. Moreover, we show that surprisingly, to classify the linear preservers of $\mathcal{SR}$ it suffices to examine the linear preservers of $\mathcal{SR}_2$.  %given any linear transformation $\mathcal{L}$ on $\mathbb{R}^{n\times n}$ and to show that $\mathcal{L}\in P(\mathcal{SR})$, we have to make sure that $\mathcal{L}(A)$ is SR for $A\in \mathcal{SR}$. Instead of examining this, we have proved that it is sufficient to check that $\mathcal{L}(A)$ is SR$_2$ in order to guarantee that $\mathcal{L}\in P(\mathcal{SR})$.
	%%%%%%%%%%%%%%%%%% Theorem C %%%%%%%%%%%%%%%%%%
	%	\begin{utheorem}\label{C}
		%		Let $\mathcal{L}:\mathbb{R}^{n\times n}\to\mathbb{R}^{n\times n}$ for $n\geq3$ be a linear transformation. Then the following statements are equivalent.
		%		\begin{enumerate}
			%			\item $\mathcal{L}$ maps the class of SR matrices onto itself.
			%			\item $\mathcal{L}$ maps the class of SR$_2$ matrices onto itself.
			%			\item $\mathcal{L}$ is a composition of one or more of the following types of transformations
			%			\begin{enumerate}
				%				\item $A\to FAE$, in which $E$ and $F$ are diagonal matrices with positive diagonal entries;
				%				\item $A\to A^T$;
				%				\item $A\to -A$;
				%				\item $A\to PA$; and
				%				\item $A\to AP$, in which $P$ is a exchange matrix.
				%			\end{enumerate}
			%		\end{enumerate}
		%		Moreover, the theorem is also true if SR is replaced by SSR.
		%	\end{utheorem}
	%	Since transposition, that is, the map $A\mapsto A^T$ does not preserve the order of rectangular matrices so we need to treat the class of rectangular SR matrices separately. In the next theorem, we have characterized all linear preservers for the class of SR matrices of order $m\times n$ where $m\neq n$ and $\mathrm{min}\{m,n\}\ge2$. Here also, we have showed that it is sufficient to check that $\mathcal{L}(A)$ is SR$_2$ for $A\in \mathcal{SR}$ in order to guarantee that $\mathcal{L}$ is a linear $\mathcal{SR}$-preserver of rectangular matrices.
	%%%%%%%%%%%%%%%%%% Theorem D %%%%%%%%%%%%%%%%%%
	\begin{utheorem} \label{D}
		Let $\mathcal{L}:\mathbb{R}^{m\times n}\to\mathbb{R}^{m\times n}$  be a linear transformation, where either $m=n\geq 3$ or $m \neq n$ and $m,n \geq 2$. Then the following statements are equivalent.
		\begin{itemize}
			\item[(1)] $\mathcal{L}$ maps the class of SR matrices onto itself.
			\item[(2)] $\mathcal{L}$ maps the class of SR$_2$ matrices onto itself.
			\item[(3)] $\mathcal{L}$ is a composition of one or more of the following types of transformations:
			\begin{itemize}
				\item[(a)] $A\mapsto FAE$, in which $F_{m\times m}$ and $E_{n\times n}$ are diagonal matrices with positive diagonal entries;
				\item[(b)] $A\mapsto -A$;
				\item[(c)] $A\mapsto P_mA$, in which $P_m$ is an exchange matrix;
				\item[(d)] $A\mapsto AP_n$;  and
				\item[(e)] $A\mapsto A^T$, provided $m=n$.
			\end{itemize}
		\end{itemize}
		Moreover, the theorem is also true if SR is replaced by SSR.
	\end{utheorem}
\begin{rem}
	If $m \neq n$ and either $m=1$ or $n=1$ in Theorem \ref{D}, then the problem reduces to classifying linear $\mathcal{SR}_1$-preservers. In this case Theorem \ref{D} still holds, but now the second statement is $\mathcal{L} \in P (\mathcal{SR}_1)$ in place of  $\mathcal{L} \in P (\mathcal{SR}_2)$, and $P_m, P_n$ can be any permutation matrices instead of exchange matrices in the third statement. If $m=n=2$, then there is only one $2\times 2$ minor and since we are considering  the set $S$ of all sign patterns of SR matrices in Theorem \ref{D}, the problem again reduces to classifying linear $\mathcal{SR}_1$-preservers. We consider this case in Theorem \ref{thrmSR_1}. %Hence we are assuming either $m=n\geq 3$ or $m \neq n$ and $m,n \geq 2$.
\end{rem}
	Theorem \ref{D} guarantees that if $\mathcal{L}\in P(\mathcal{SR})$ then $A\in\mathcal{SR}$ if and only if $\mathcal{L}(A)\in\mathcal{SR}$. However, it does not assure that the sign patterns of $A$ and $\mathcal{L}(A)$ will be identical. In our final main result, our objective is to characterize linear preservers for the class of SR($\epsilon$) matrices for any size $m \times n$ and any given sign pattern $\epsilon$. Further, we show that it is again sufficient to check that $\mathcal{L}(A)$ is SR$_2(\epsilon)$ if and only if $A\in \mathcal{SR}_2(\epsilon)$ in order to guarantee that $\mathcal{L}$ is a linear $\mathcal{SR}(\epsilon)$-preserver.
	\begin{utheorem} \label{F}
		Let $\epsilon$ be a given sign pattern and $\mathcal{L}:\mathbb{R}^{m\times n}\to\mathbb{R}^{m\times n}$ be a linear transformation, where  $m,n\geq2$. Then the following statements are equivalent.
		\begin{itemize}
			\item[(1)] $\mathcal{L}$ maps the class of SR$(\epsilon)$ matrices onto itself.
			\item[(2)] $\mathcal{L}$ maps the class of SR$_2(\epsilon)$ matrices onto itself.
			\item[(3)] $\mathcal{L}$ is a composition of one or more of the following types of transformations:
			\begin{itemize}
				\item[(a)] $A\mapsto FAE$, in which $F_{m\times m}$ and $E_{n\times n}$ are diagonal matrices with positive diagonal entries;
				\item[(b)] $A\mapsto P_mAP_n$, in which $P_m$ and $P_n$ are exchange matrices; and
				\item[(c)] $A\mapsto A^T$, provided $m=n$.
			\end{itemize}
		\end{itemize}
		Moreover, the theorem is also true if SR$(\epsilon)$ is replaced by SSR$(\epsilon)$.
	\end{utheorem}
	In particular, by taking $m=n$ and  $\epsilon_k=1$ for all $k$, Theorem \ref{F} gives the linear preservers for the class of TP and TN matrices of order $n$ as a special case which were characterized by Berman--Hershkowitz--Johnson \cite{BHJ85}.
	\begin{rem}
		In both Theorems \ref{D} and \ref{F}, the linear preservers of $\mathcal{SR}$ and $\mathcal{SR}_2$ (respectively $\mathcal{SR}(\epsilon)$ and $\mathcal{SR}_2(\epsilon)$) are automatically the set of all linear preservers of $\mathcal{SR}_k$ ($\mathcal{SR}_k(\epsilon)$) for $2\leq k\leq\mathrm{min}\{m,n\}$.
	\end{rem}
%		\begin{rem}
%			If $m=n=2$, then there is only one $2\times 2$ minor and since we are considering  the set $S$ of all sign pattern of SR matrices in Theorem \ref{D}, the problem reduces to classifying linear SR$_1$ preservers. We consider this case in Theorem \ref{thrmSR_1}. %Hence we are assuming either $m=n\geq 3$ or $m \neq n$ and $m,n \geq 2$.
%		\end{rem}
%\end{rem}
     \textbf{Organization of the paper:} The remaining sections of the paper are devoted to proving our main results. In Section \ref{section VDP}, we prove Theorems \ref{A} and \ref{B}. In fact, Theorem \ref{B} uses a single test vector whose coordinates alternate in sign. After proving Theorem \ref{B}, we show that strict sign regularity with a given sign pattern can not be characterized by the variation diminishing property using test vectors from any open orthant other than alternating bi-orthant -- see Theorem \ref{SSR_test_vector_not_biorthant}. In Section \ref{section SR} we prove Theorem \ref{D} which classifies all linear maps that preserve SR/SSR matrices. In the final section, we prove Theorem \ref{F}.
	
	%%%%%%%%%%%%%%%%%%%% Section 2 %%%%%%%%%%%%%%%%%%%%
	\section{Theorems \ref{A} and \ref{B}: Variation diminution for SSR} \label{section VDP} 	
	We begin by proving Theorems \ref{A} and \ref{B} for SSR matrices. In Section \ref{sec_VDSR}, we show a similar result to Theorem \ref{B} for SR, which in particular strengthens Motzkin's result also specifying the sign pattern $\epsilon$ and removing Motzkin's rank-hypothesis. For this section define $\mathbf{d}_{p}:=(1,-1,\ldots,(-1)^{p-1})^T\in\mathbb{R}^p$, for any integer $p\geq1$.
	%%%%%%%%%%%%%%%%%%%% Proof of Theorem A %%%%%%%%%%%%%%%%%%%%
	\begin{proof}[Proof of Theorem \ref{A}] 
		Let $A\in\mathbb{R}^{m\times n}$ such that $S^+(A\mathbf{x})\leq S^-(\mathbf{x})$ for all $\mathbf{0}\neq\mathbf{x}\in\mathbb{R}^n$. Our aim is to show all minors of the same size of $A$ have the same sign. The proof is by induction on the size of the minors. First we show that all $p\times p$ minors of $A$ are non-zero for $1\leq p\leq\mathrm{min}\{m,n\}$.  Assume to the contrary that $\det A\begin{pmatrix}
			i_1,\ldots,i_p \\
			j_1,\ldots,j_p
		\end{pmatrix}=0$, where (henceforth) $1\leq i_1<\cdots<i_p\leq m$ index the rows of the minor and $1\leq j_1<\cdots<j_p\leq n$ index the columns. Then there exists $\mathbf{0}\neq\mathbf{z}\in\mathbb{R}^{p}$ such that
		$A\begin{pmatrix}
			i_1,\ldots,i_p \\
			j_1,\ldots,j_p
		\end{pmatrix}\mathbf{z}=\mathbf{0}.$
		Define $\mathbf{x}\in\mathbb{R}^n$ such that the $j_l$ component of $\mathbf{x}$ is $z_{l}$ for $l=1,\ldots,p$, and all other components are zero. Then $S^-(\mathbf{x})\leq p-1$ while $S^+(A\mathbf{x})\geq p$, a contradiction. This shows the claim. 
		
		Next we show that all entries of $A$ are of the same sign. For each $j\leq n$, $S^-(\mathbf{e}^j)=0$, and thus $S^+(A\mathbf{e}^j)=0.$ Therefore, all the elements of $A$ present in the $j^{th}$ column are non-zero and further they all are of the same sign. Now, we will show that no two columns of $A$ have different signs. On the contrary, assume without loss of generality, that the $i_1$ column of $A$ is positive while the $i_2$ column is negative. We can choose positive real numbers $x_{i_1}$ and $x_{i_2}$ such that at least one entry of the vector $x_{i_1}\mathbf{a}^{i_1}+x_{i_2}\mathbf{a}^{i_2}\in\mathbb{R}^{m}$ is zero where $\mathbf{a}^{i_1}$ and $\mathbf{a}^{i_2}$ are the $i_1$ and $i_2$ columns of $A$, respectively. Take $\mathbf{0}\neq\mathbf{x}\in\mathbb{R}^n$ with $i_1$ and $i_2$ entries as $x_{i_1}$ and $x_{i_2}$, respectively and all other entries zero. Then $S^-(\mathbf{x})=0$ whereas  $S^+(A\mathbf{x})\geq1$, a contradiction. Thus $A$ is SSR$_1.$ Next, we assume that $A$ is SSR$_{p-1}$ for $1<p\leq\mathrm{min}\{m,n\}$. We will show that $A$ is SSR$_p$. 
		
		The proof strategy is broadly as follows: To prove all $p\times p$ minors of $A$ have the same sign, we will choose arbitrary $p$ columns of $A$, say $1\leq j_1<\cdots<j_p\leq n$ and then we will show that all $p\times p$ minors of submatrix of $A$ included in rows $1,\ldots,m$ and columns $j_1,\ldots,j_p$ have the same sign. To complete the proof, we will further show that the sign of all $p\times p$ minors of $A$ is independent of the particular choice of the columns $j_1,\ldots,j_p$ and depends only on $p$.
		
		Fix $1\leq j_1<\cdots<j_p\leq n$. We will first prove that $\det A\begin{pmatrix}
			i_1,\ldots,i_p \\
			j_1,\ldots,j_p
		\end{pmatrix}$ for all $1\leq i_1<\cdots<i_p\leq m$ have the same sign. If $p=m$, the conclusion is clear; if $p<m$, it suffices to show this for the $(p+1)$ minors of size $p\times p$ that can be formed from an arbitrary choice of $(p+1)$ rows, $i_1<\cdots<i_{p+1}$ (say). Consider the following $p\times p$ submatrices of $A$ which are obtained by deleting the $i_l^{th}$ row of $A\begin{pmatrix}
			i_1,\ldots,i_{p+1} \\
			j_1,\ldots,j_p
		\end{pmatrix}$ for $l=1,\ldots,p+1$:
	
		\[A_l=\begin{pmatrix}
			a_{i_1j_1} & a_{i_1j_2} & \ldots & a_{i_1j_p} \\
			\vdots    & \vdots    & \ddots & \vdots \\
			a_{i_{l-1}j_1} & a_{i_{l-1}j_2} & \ldots & a_{i_{l-1}j_p} \\
			a_{i_{l+1}j_1} & a_{i_{l+1}j_2} & \ldots & a_{i_{l+1}j_p} \\
			\vdots    & \vdots    & \ddots & \vdots \\
			a_{i_{p+1}j_1} & a_{i_{p+1}j_2} & \ldots & a_{i_{p+1}j_p} \\
		\end{pmatrix}_{p\times p}.\] We will show that $\det A_1=\det A_l$ for $l=2,\ldots,p+1$. Fix $l\neq1$, the determinant of the $p\times p$ matrix $A_l$ is given by 
		\[\det A_l= a_{i_1j_1}\alpha_{l,1}-a_{i_1j_2}\alpha_{l,2}+\cdots+(-1)^{p-1}a_{i_ 1j_p}\alpha_{l,p},\]
		where $\alpha_{l,1},\ldots,\alpha_{l,p}$ are non-zero $(p-1)\times(p-1)$ minors of $A$ and hence by the induction hypothesis they all have the same sign. Define $\mathbf{0}\neq\mathbf{x}\in\mathbb{R}^n$ such that the entries in positions $j_1,j_2,\ldots,j_p$ are $\alpha_{l,1},-\alpha_{l,2},\ldots,(-1)^{p-1}\alpha_{l,p}$, respectively and zero elsewhere. Therefore, $S^-(\mathbf{x})=p-1$. Now, the first $(p+1)$ entries of the vector $A\mathbf{x}$ are given by 
		\[\det A_l,0,\ldots,0,(-1)^l\det A_1,0,\ldots,0\]
		where $(-1)^l\det A_1$ is present in the $l^{th}$ position. Suppose that $\det A_1$ and $\det A_l$ are of opposite signs. Note that for a sequence to have alternate signs, the sign of its component in the first and odd positions should be same, respectively the opposite in the first and even positions. Therefore by our construction, the elements of the vector $A\mathbf{x}$ in positions 1 and $l$ have different signs for $l$ even and the same sign for $l$ odd. Hence $S^+(A\mathbf{x})\geq p$ which is not possible. Thus $\det A_1=\det A_l$ for all $l=2,\ldots,p+1.$ Therefore, \[\det A\begin{pmatrix}
			i_1,\ldots,i_p \\
			j_1,\ldots,j_p
		\end{pmatrix}\] for all $1\leq i_1<\cdots<i_p\leq m$ have the same sign. We denote this common sign of the minors by $\epsilon(j_1,\ldots,j_p)$. Note that if $p=n$, then we are done. To complete the proof, we assume $p<n$ and show that this sign is independent of the particular choice of $p$ columns $j_1,\ldots,j_p$ and depends only on $p$. Let us take $(p+1)$ columns of $A$, with $1\leq j_1<\cdots<j_{p+1}\leq n$, and let $\epsilon_r=\epsilon(j_1,\ldots,j_{r-1},j_{r+1},\ldots,j_{p+1}).$ It suffices to prove $\epsilon_r=\epsilon_{r+1}$ for $r=1,\ldots,p.$
		
		Define a vector $\mathbf{x}\in\mathbb{R}^{n}$ whose coordinates in position $j_k$ are given by
		\[x_{j_k}:=(-1)^{k-1}\det A\begin{pmatrix}
			1,2,\ldots \hspace{1.1cm} \ldots p-1, p \\
			j_1,\ldots,j_{k-1},j_{k+1},\ldots,j_{p+1}
		\end{pmatrix}\]
		for $k=1,\ldots,p+1$ and zero elsewhere. Therefore, we have
		\[\sum_{k=1}^{p+1}a_{ij_k}x_{j_k}=0 \quad\mathrm{for}\hspace{.1cm} i=1,\ldots,p\] 
		and hence $S^+(A\mathbf{x})\geq p$. If $\epsilon_r\epsilon_{r+1}<0$ for some $r$, then $x_{j_r}x_{j_{r+1}}>0$. Thus, $S^-(\mathbf{x})\leq p-1$ which is a contradiction. Hence, $A$ is SSR.
		
		%%%%%%%%%%%%%%%%%%%%(2) implies (1)%%%%%%%%%%%%%%%%%%%%
		To prove the converse, let $A\in\mathbb{R}^{m\times n}$ be SSR.  Let us take a non-zero vector $\mathbf{x}\in\mathbb{R}^n$ and assume that $S^-(\mathbf{x})=r\leq n-1$. Therefore, we can  partition $\mathbf{x}$ into $(r+1)$  contiguous components such that no two components having different signs belong to the same partition:
		\[(x_1,\ldots,x_{s_1}), (x_{s_1+1},\ldots,x_{s_2}), \ldots, (x_{s_r+1},\ldots,x_n).\]
		We assume without loss of generality, that the sign of the non-zero elements in the $k^{th}$ partition is given by $(-1)^{k-1}$. Also, note that there is at least one non-zero element in each partition, otherwise $S^-(\mathbf{x})<r$. We can write $A$ as $A=(\mathbf{a}^1|\ldots|\mathbf{a}^n)$ where $\mathbf{a}^i\in\mathbb{R}^m$ for all $i=1,\ldots n$. Therefore, \[A\mathbf{x}=\sum_{k=1}^{n}x_k\mathbf{a}^k =\sum_{i=1}^{r+1}(-1)^{i-1}\mathbf{y}^i\] where \[\mathbf{y}^i=\sum_{k=s_{i-1}+1}^{s_i}|x_k|\mathbf{a}^k \hspace{.2cm}\mathrm{for}\hspace{.2cm} i=1,\ldots,r+1,\hspace{.2cm}\hspace{.2cm} s_0=0 \hspace{.2cm}\mathrm{and} \hspace{.2cm} s_{r+1}=n.\] 
		Let $Y:=(\mathbf{y}^1|\ldots|\mathbf{y}^{r+1})\in\mathbb{R}^{m\times (r+1)}$. Using basic properties of determinants, we have
		\[\det Y\begin{pmatrix}
			i_1,\ldots,i_p \\
			j_1,\ldots,j_p
		\end{pmatrix}=\sum_{k_1=s_{j_1-1}+1}^{s_{j_1}}\cdots\sum_{k_p=s_{j_p-1}+1}^{s_{j_p}}|x_{k_1}|\cdots|x_{k_p}|\det A\begin{pmatrix}
			i_1,\ldots,i_p \\
			k_1,\ldots,k_p
		\end{pmatrix}. \] Since $A$ is SSR, the terms $\det A\begin{pmatrix}
			i_1,\ldots,i_p \\
			k_1,\ldots,k_p
		\end{pmatrix}$ have the same sign, for all choices of $1\leq i_1<\cdots<i_p\leq m$, $1\leq k_1<\cdots<k_p\leq n$; and $|x_{k_1}|\cdots|x_{k_p}|>0$ for some choice of admissible $\{k_1,\ldots,k_p\}$ in the above sum. Hence $Y$ is SSR. Note that the minors of $A$ and $Y$ have the same sign pattern.
		
		With the above analysis in hand, we will show that $S^+(A\mathbf{x})\leq r$. Consider the following two cases. 
		
		\noindent \textbf{Case I.} $m\leq r+1$.
		
		If $m=r+1$, then $A\mathbf{x}\neq\mathbf{0}$, since $A\mathbf{x}=Y\mathbf{d}_{r+1}$ and $Y$ is SSR. Hence $S^+(A\mathbf{x})\leq r$. Otherwise, $m\leq r$. Then clearly $S^+(A\mathbf{x})\leq m\leq S^-(\mathbf{x}).$
		
		\noindent \textbf{Case II.} $m>r+1$.
		
		Define $w:=A\mathbf{x}=Y\mathbf{d}_{r+1}$ and to the contrary assume that $S^+(A\mathbf{x})\geq r+1$. Thus there exist indices $1\leq i_1<\cdots<i_{r+2}\leq m$ and $\theta\in\{1,-1\}$ such that $\theta(-1)^{j-1}w_{i_j}\geq 0$ for $j=1,\ldots,r+2$. Note that at least two of the $w_{i_j}$ are non-zero since $Y$ has rank $(r+1)$. Now, consider the following determinant 
		\[\det\begin{pmatrix}
			w_{i_1} & y_{i_11} & \ldots & y_{i_1r+1} \\
			w_{i_2} & y_{i_21} & \ldots & y_{i_2r+1} \\
			\vdots  & \vdots   & \ddots & \vdots     \\
			w_{i_{r+2}} & y_{i_{r+2}1} & \ldots & y_{i_{r+2}r+1} 
		\end{pmatrix}.\]
		The above determinant vanishes since the first column is an alternating sum of the rest. Expanding the above determinant along the first column gives 
		\[0=\sum_{j=1}^{r+2}(-1)^{j-1}w_{i_j}\det Y\begin{pmatrix} 
			i_1,\ldots,i_{j-1},i_{j+1},\ldots,i_{r+2} \\
			1, 2,\ldots \hspace{1cm}\ldots,r, r+1	
		\end{pmatrix}.	\]
		But the right hand side is non-zero, since $Y$ is SSR, $(-1)^{j-1}w_{i_j}$ has the same sign $\theta$ for $j=1,\ldots,r+2$, and not all $w_{i_j}$ are zero. This contradiction implies that $S^+(A\mathbf{x})\leq r=S^-(\mathbf{x})$.
	\end{proof}
	To prove our next main result, we recall a 1968 seminal result by Karlin for strict sign regularity, first proved by Fekete (1912) for total positivity and later refined by Schoenberg (1955) to total positivity of order $k$.
	\begin{theorem}[Karlin \cite{K68}]\label{Karlin}
		Suppose $1\leq k\leq m,n$ are integers and matrix $A\in\mathbb{R}^{m\times n}$. Then $A$ is SSR$_k$ if and only if all contiguous minors of order $r\in\{1,\ldots,k\}$ have the same sign.
	\end{theorem}
	%%%%%%%%%%%%%%%%%%%% Proof of Theorem B %%%%%%%%%%%%%%%%%%%%
	\begin{proof}[Proof of Theorem \ref{B}] 
		%%%%%%%%%%%%%%%%%%%%(1) implies (2)%%%%%%%%%%%%%%%%%%%%
		We will show a cyclic chain of implications: $(1) \implies (2) \implies (3) \implies (1)$. In addition, we also show that $(2) \implies (1)$ (although it is not needed) in keeping with previous proofs of the equivalence of SSR and VD. This will also be the strategy in our proof of Theorem \ref{VD_SRE} below.
		
		We begin by showing (1)$\implies$(2). If $A\in\mathbb{R}^{m\times n}$ is SSR$(\epsilon)$, that $S^+(A\mathbf{x})\leq S^-(\mathbf{x})$ for all $\mathbf{0}\neq\mathbf{x}\in\mathbb{R}^n$ immediately follows from Theorem \ref{A}. It only remains to show the second part of the assertion. For $0\leq r\leq\mathrm{min}\{m,n\}-1$, let $S^+(A\mathbf{x})=S^-(\mathbf{x})=r$ with $A\mathbf{x}\neq\mathbf{0}$. To proceed, we adopt the notation in the second subcase of the preceding proof's converse part; note that now there exist $r+1$ indices instead of $r+2$. We claim that if $\theta(-1)^{j-1}w_{i_j}\geq0$ for $j=1,\ldots,r+1$, then $\theta=\epsilon_r\epsilon_{r+1}$. To show this, we use the following system of equations
		\[Y\begin{pmatrix}
			i_1,\ldots,i_{r+1} \\
			1,\ldots,r+1
		\end{pmatrix}\mathbf{d}_{r+1}=\begin{pmatrix}
			w_{i_1} \\
			\vdots \\
			w_{i_{r+1}}
		\end{pmatrix}.\]
		Since $Y\in\mathbb{R}^{m\times(r+1)}$ is SSR, every set of $(r+1)$ rows of $Y$ is linearly independent. Using Cramer's rule to solve the above system of equations for the first component gives
		\[1=\frac{\det\begin{pmatrix}
				w_{i_1} & y_{i_12} & \ldots & y_{i_1r+1} \\
				w_{i_2} & y_{i_22} & \ldots & y_{i_2r+1} \\
				\vdots  & \vdots   & \ddots & \vdots      \\
				w_{i_{r+1}} & y_{i_{r+1}2} & \ldots & y_{i_{r+1}r+1} 
		\end{pmatrix}}{\det Y\begin{pmatrix}
				i_1,\ldots,i_{r+1} \\
				1,\ldots,r+1
		\end{pmatrix}}.\]
		Expanding the numerator along the first column gives
%		\begin{equation}\label{vd_ee}
%		1=\frac{\displaystyle{\sum_{j=1}^{r+1}} (-1)^{j-1}w_{i_j}\det Y\begin{pmatrix}
%				i_1,\ldots,i_{j-1},i_{j+1},\ldots,i_{r+1} \\
%				2,3,\ldots \hspace{1cm} \ldots,r,r+1
%		\end{pmatrix}}{\det Y\begin{pmatrix}
%				i_1,\ldots,i_{r+1} \\
%				1,\ldots,r+1
%		\end{pmatrix}}.
%		\end{equation}
		\[1=\frac{\displaystyle{\sum_{j=1}^{r+1}} (-1)^{j-1}w_{i_j}\det Y\begin{pmatrix}
				i_1,\ldots,i_{j-1},i_{j+1},\ldots,i_{r+1} \\
				2,3,\ldots \hspace{1cm} \ldots,r,r+1
		\end{pmatrix}}{\det Y\begin{pmatrix}
				i_1,\ldots,i_{r+1} \\
				1,\ldots,r+1
		\end{pmatrix}}.\]
		Note that $\theta(-1)^{j-1}w_{i_j}\geq0$ for $i=1,\ldots,r+1$ and all $p\times p$ minors of $Y$ have sign $\epsilon_p$ for $p=1,\ldots,r+1$. Multiplying both sides of the above equation by $\theta$, we have $\theta=\epsilon_r\epsilon_{r+1}$. 
		
		%%%%%%%%%%%%%%%%%%%%(2) implies (1)%%%%%%%%%%%%%%%%%%%%
		We next show that (2)$\implies$(1). Our proof is by induction on the size of the minors. Let $\mathbf{e}^j\in\mathbb{R}^n$, $1\leq j\leq n$. Then
		$S^+(A\mathbf{e}^j)\leq S^-(\mathbf{e}^j)=0$, so $A\mathbf{e}^j$ can not have zero entries -- in fact all entries have sign $\epsilon_1$ by (2).
		
		We next claim that all minors of $A$ are non-zero. Indeed, suppose for contradiction that $\det A\begin{pmatrix}
			i_1,\ldots,i_p \\
			j_1,\ldots,j_p
		\end{pmatrix}=0$, where $2\leq p\leq\mathrm{min}\{m,n\}$, $1\leq i_1<\cdots<i_p\leq m$ and $1\leq j_1<\cdots<j_p\leq n$. Then there exists $\mathbf{0}\neq\mathbf{z}\in\mathbb{R}^{p}$ such that
		$A\begin{pmatrix}
			i_1,\ldots,i_p \\
			j_1,\ldots,j_p
		\end{pmatrix}\mathbf{z}=\mathbf{0}.$
		Define $\mathbf{x}\in\mathbb{R}^n$ such that the $j^{th}_l$ component of $\mathbf{x}$ is $z_{l}$ for $l=1,\ldots,p$, and all other components are zero. Then $S^-(\mathbf{x})\leq p-1$ while $S^+(A\mathbf{x})\geq p$, a contradiction. Thus all minors of $A$ are non-zero. Finally, we claim by induction on $1\leq p\leq\mathrm{min}\{m,n\}$ that $A$ is SSR$_p$ with sign pattern $\epsilon_1,\ldots,\epsilon_p$, with the base case $p=1$ shown above. For the induction step, suppose $1\leq i_1<\cdots<i_p\leq m$ and $1\leq j_1<\cdots<j_p\leq n$ as above. Since $A\begin{pmatrix}
			i_1,\ldots,i_p \\
			j_1,\ldots,j_p
		\end{pmatrix}$ is non-singular, there exists $\mathbf{z}=(z_1,\ldots,z_p)\in\mathbb{R}^p$ such that
		\begin{equation}\label{Az=d}
			A\begin{pmatrix}
				i_1,\ldots,i_p \\
				j_1,\ldots,j_p
			\end{pmatrix}\mathbf{z}=\mathbf{d}_p.
		\end{equation}
		Again, extend $\mathbf{z}$ to $\mathbf{x}\in\mathbb{R}^n$ by embedding in positions $j_1,\ldots,j_p$ and padding by zeros elsewhere. Then $p-1\leq S^-(A\mathbf{x})\leq S^+(A\mathbf{x})\leq S^-(\mathbf{x})\leq p-1$. It follows that $S^+(A\mathbf{x})=S^-(\mathbf{x})=p-1$. From this we conclude:
		(i) the coordinates of $\mathbf{z}$ alternate in sign; (ii) all coordinates of $A\mathbf{x}$ in positions $1,\ldots,i_1$ are positive; and (iii) $\epsilon_{p-1}\epsilon_pz_1>0$ by the hypothesis. We now return to equation \eqref{Az=d} and solve for $z_{1}$ using Cramer's rule to obtain 
		\[z_{1}=\frac{\displaystyle{\sum_{l=1}^{p}}\det A\begin{pmatrix}
				i_1,\ldots,i_{l-1},i_{l+1}\ldots,i_p \\
				j_2,j_3, \ldots \hspace{.2cm} \ldots, j_{p-1},j_p
		\end{pmatrix}}{\det A\begin{pmatrix}
				i_1,\ldots,i_p \\
				j_1,\ldots,j_p
		\end{pmatrix}}.\]
		Multiplying both sides by $\epsilon_{p-1}\epsilon_p$, we obtain \[0<\epsilon_{p-1}\epsilon_pz_{1}=\epsilon_{p-1}\epsilon_p\frac{\displaystyle{\sum_{l=1}^{p}}\det A\begin{pmatrix}
				i_1,\ldots,i_{l-1},i_{l+1}\ldots,i_p \\
				j_2,j_3, \ldots \hspace{.2cm} \ldots, j_{p-1},j_p
		\end{pmatrix}}{\det A\begin{pmatrix}
				i_1,\ldots,i_p \\
				j_1,\ldots,j_p
		\end{pmatrix}}.\]
		The sign of the numerator is $\epsilon_{p-1}$ by the induction hypothesis, and hence the sign of $\det A\begin{pmatrix}
			i_1,\ldots,i_p \\
			j_1,\ldots,j_p
		\end{pmatrix}$ is $\epsilon_p$. This concludes (2)$\implies$(1).
		
		%%%%%%%%%%%%%%%%%%%%(2) implies (3)%%%%%%%%%%%%%%%%%%%%
		Now we will show that (2)$\implies$(3), where $A_k$ is not necessarily required to be a contiguous submatrix of $A$ and $\mathbf{0}\neq\mathbf{x}^{A_k}\in\mathbb{R}^k$ can be arbitrary. Let $A_k=A\begin{pmatrix}
			i_1,\ldots,i_k \\
			j_1,\ldots,j_k
		\end{pmatrix}$, where $1\leq i_1<\cdots<i_k\leq m$ and $1\leq j_1<\cdots<j_k\leq n$. Define $\mathbf{x}\in\mathbb{R}^n$ whose coordinates at position $j_l$ for $l=1,\ldots,k$ are $x_l^{A_k}$ and zero elsewhere. By the hypothesis, we have 
		\[S^+(A_k\mathbf{x}^{A_k})\leq S^+(A\mathbf{x})\leq S^-(\mathbf{x})=S^-(\mathbf{x}^{A_k})\quad \implies \quad S^+(A_k\mathbf{x}^{A_k})\leq S^-(\mathbf{x}^{A_k}).\]
		Now suppose that $S^+(A_k\mathbf{x}^{A_k})=S^-(\mathbf{x}^{A_k})=r$ where $0\leq r\leq k-1$. Then $A_k\mathbf{x}^{A_k}\neq\mathbf{0}$ and \[ S^+(A\mathbf{x})=S^-(\mathbf{x})=r.\] 
		Since $A_k\mathbf{x}^{A_k}\neq\mathbf{0}$ is a sub-string of $A\mathbf{x}$ and hence $A\mathbf{x}\neq\mathbf{0}$. Also, as $S^+(A\mathbf{x})=S^+(A_k\mathbf{x}^{A_k})$ therefore by $S^+$-filling of $A\mathbf{x}$, all coordinates of $A\mathbf{x}$ in positions $1,\ldots,i_1$ (respectively $i_{k},\ldots,m$) have the same sign. Note that the first non-zero component of $\mathbf{x}^{A_k}$ is same as $\mathbf{x}$. Since $S^+(A\mathbf{x})=S^-(\mathbf{x})$ and $A\mathbf{x}\neq\mathbf{0}$, hence by (2), the sign of the first (and last) component of $A_k\mathbf{x}^{A_k}$ agrees with $\epsilon_{r}\epsilon_{r+1}$ times the sign of the first (and last) non-zero component of $\mathbf{x}^{A_k}$. 
		
		%%%%%%%%%%%%%%%%%%%%(3) implies (1)%%%%%%%%%%%%%%%%%%%%
		To complete the proof, we show that (3)$\implies$(1). By Karlin's Theorem \ref{Karlin}, it suffices to show that the sign of $\det A_r$ is $\epsilon_r$ for all $r\times r$ contiguous submatrices $A_r$ of $A$, where $1\leq r\leq \mathrm{min}\{m,n\}$. We prove this by induction on $r$. If $r=1$, then $\adj A_1=(1)_{1\times1}$ and
		\[0\leq S^+(A_1x^{A_1})\leq S^-(x^{A_1})=0.\]
		Thus $A_1x^{A_1}\neq0$ and by the hypothesis the sign of $A_1x^{A_1}$ is $\epsilon_1$ times the sign of $x^{A_1}$. Hence all the components of $A$ have sign $\epsilon_1$.
		
		For the induction step fix $1\leq r\leq\mathrm{min}\{m,n\}$, and suppose that all contiguous $p\times p$ minors of $A$ have signs $\epsilon_{p}$ for $1\leq p\leq r-1$. Let $A_r$ be an $r\times r$ contiguous submatrix of $A$. By Theorem \ref{Karlin}, $A_r$ is SSR$_{r-1}$. Therefore, all entries of $\adj A_r$ are non-zero and have a checker-board sign pattern. Now, define a vector $\mathbf{x}^{A_r}:=\adj A_r\mathbf{v}$, as in \eqref{x^A_k}. Note that all entries of $\mathbf{x}^{A_r}$ are non-zero with alternating signs. We first show that $A_r$ is non-singular. If $A_r$ is singular, then \[A_r\mathbf{x}^{A_r}=(\det A_r)\mathbf{v}=\mathbf{0}\in\mathbb{R}^r\implies S^+(A_r\mathbf{x}^{A_r})=r>r-1=S^-(\mathbf{x}^{A_r}),\] 
		a contradiction. Thus, $A_r$ is invertible. 
		
		Next, we show that $\det A_r$ has sign $\epsilon_r$. Since $A_r\mathbf{x}^{A_r}=(\det A_r)\mathbf{v}$, we have \[r-1=S^-(\mathbf{x}^{A_r})\geq S^+(A_r\mathbf{x}^{A_r})= S^+((\det A_r)\mathbf{v}).\] 
		Now, note that even if some entries of the vector $\mathbf{v}$ are zero, the conditions on the $\alpha_j$ imply that $\mathbf{v}$ can be $S^+$-completed to a vector with all non-zero entries and alternating signs. In particular, $S^+((\det A_r)\mathbf{v})=r-1$. Thus, we have
		\[S^+(A_r\mathbf{x}^{A_r})=S^-(\mathbf{x}^{A_r})=r-1.\]
		Since the sign of the first component of $A_r\mathbf{x}^{A_r}$ (or $S^+$-completion of $A_r\mathbf{x}^{A_r}$) is sign($\alpha_1\det A_r$) and therefore by (3), sign($\alpha_1\det A_r$) and the first non-zero component of $\mathbf{x}^{A_r}$ bear the following relation:
		\[\mathrm{sign}(\alpha_1\det A_r)= \epsilon_{r-1}\epsilon_r\mathrm{sign}(\sum_{j=1}^{r}\alpha_jA_r^{j1}),\]
		where $A_r^{ij}$ denotes the determinant of $(r-1)\times(r-1)$ submatrix of $A_r$ formed by deleting the $i^{th}$ row and $j^{th}$ column of $A_r$. Observe that the sign of each summand on the right of the above equation is $\epsilon_{r-1}$, since sign$(A_r^{j1})=\epsilon_{r-1}$ for $j=1,\ldots,r$, $\alpha_j\geq0$ for $j=1,\ldots,r$, and not all $\alpha_j$ are zero. Hence sign$(\det A_r)=\epsilon_r$. This completes our proof.
	\end{proof}
	%%%%%%%%%%%%%%%%%%%% Theorem %%%%%%%%%%%%%%%%%%%%
	From part (3) of Theorem \ref{B}, we have seen that given a $m\times n$ matrix $A$ and a sign pattern $\epsilon$, the variation diminishing property at a single test vector, which is drawn from the alternating bi-orthant  for each contiguous square submatrix of $A$ suffices to show the strict sign regularity of $A$ with sign pattern $\epsilon$. Our next result shows that Theorem \ref{B} is the ``best possible" in the following sense: Any $n\times n$ singular SSR$_{n-1}$ matrix also satisfies the variation diminishing property on every vector in $\mathbb{R}^n$ other than the alternating bi-orthant. Thus a characterization of strict sign regularity with a given sign pattern in terms of the variation diminishing property can not hold with test vectors in any open bi-orthant other than the alternating bi-orthant. The proof is similar to $(1)\implies(2)$ of Theorem \ref{B}.
	\begin{theorem} \label{SSR_test_vector_not_biorthant}
		Suppose all coordinates of the vector $\mathbf{x}\in\mathbb{R}^n$ are non-zero and at least two successive coordinates have the same sign. Let $A\in\mathbb{R}^{n\times n}$ be SSR$_{n-1}$. Then $A$ satisfies $S^+(A\mathbf{x})\leq S^-(\mathbf{x})$. Further, for all integers $0\leq r\leq n-1$ and $\mathbf{x}\in\mathbb{R}^n$ with $S^-(\mathbf{x})=r$, if $S^+(A\mathbf{x})=S^-(\mathbf{x})=r$, then the first (last) component of $A\mathbf{x}$ (if zero, the sign given in determining $S^+(A\mathbf{x})$) has the sign same as $\epsilon_r\epsilon_{r+1}$ times the sign of the first (last) non-zero component of $\mathbf{x}$. 
	\end{theorem}
	It remains to show that for each positive integer $n$, we can always construct a real $n\times n$ matrix $A$ such that $A$ is SSR$_{n-1}$ while $\det A=0.$ This follows from a result of Gantmacher--Krein (see Theorem \ref{G-K} below). The steps to construct such a matrix are given as follows:
	\begin{enumerate}[(i)]
		\item Take any $B\in\mathbb{R}^{(n-1)\times (n-1)}$ such that $B$ is SSR. Clearly, rank $B=n-1$.
		\item Let $B^{\prime}:=B\oplus\{0\}\in\mathbb{R}^{n\times n}$. Therefore, $B^{\prime}$ is a $n\times n$ SR matrix whose rank is $(n-1)$.
		%Let $B^{\prime}\in\mathbb{R}^{n\times n}$ such that whose leading principal submatrix of order $(n-1)$ is the above $B$ and remaining entries are zero. Therefore, $B^{\prime}\in\mathbb{R}^{n\times n}$ is a SR matrix whose rank is $(n-1)$.
		\item Define $A:=F_{\sigma}^{(n)}B^{\prime}F_{\sigma}^{(n)}$ where $F_{\sigma}^{(n)}=(\exp^{-\sigma(i-j)^2})_{i,j=1}^n$ for $\sigma>0$ is TP. Therefore, $A$ is SSR$_{n-1}$ with rank $(n-1)$ and hence $\det A=0$.
	\end{enumerate}
	%%%%%%%%%%%%%% Subsection of section 2 %%%%%%%%%%%%%%%%%%%
	\subsection{Variation Diminution for Sign Regular Matrices}\label{sec_VDSR}
	
	
	We conclude this section by characterizing all $m\times n$ sign regular matrices with a given sign pattern using the variation diminishing property. The proof requires a density theorem proved by Gantmacher--Krein in 1950 using the total positivity of the Gaussian kernel.
	% Statement of Theorem 
	\begin{theorem}[Gantmacher--Krein \cite{GK50}] \label{G-K}
		Let $m,n\geq k\geq1$ be integers. Given a sign pattern $\epsilon=(\epsilon_1,\ldots,\epsilon_k)$, the set of $m\times n$ SSR$(\epsilon)$ matrices of order $k$ is dense in the set of $m\times n$ SR$(\epsilon)$ matrices of order $k$.
	\end{theorem}
	The following basic lemma on sign changes of limits of vectors will also be needed in proving the theorem that follows it.
	%We also state the following preliminary lemma which will be helpful in proving below theorem.
	%The following preliminary lemma will be useful in proving below theorem:
	\begin{lemma}\cite[Lemma 3.2]{pinkus}\label{liminf} 
		If $\displaystyle{\lim_{k\to\infty}}\mathbf{x}_k=\mathbf{x}\neq\mathbf{0}\in\mathbb{R}^n$, then \[\varliminf_{k\to\infty}S^-(\mathbf{x}_k)\geq S^-(\mathbf{x})
		\hspace{.2cm}\mathrm{and}\hspace{.2cm} \varlimsup_{k\to\infty}S^+(\mathbf{x}_k)\leq S^+(\mathbf{x}).\]
	\end{lemma}
	%%%%%%%%%%%%%%%%%%%% Theorem %%%%%%%%%%%%%%%%%%%%
	\begin{theorem}\label{VD_SRE}
		Given $A\in\mathbb{R}^{m\times n}$ and $\epsilon=(\epsilon_1,\ldots,\epsilon_{\mathrm{min}\{m,n\}})$, the following are equivalent.
		\begin{itemize}
			\item[(1)] $A$ is SR$({\epsilon})$. 
			\item[(2)] For all $\mathbf{x}\in\mathbb{R}^n$, we have $S^-(A\mathbf{x})\leq S^-(\mathbf{x})$. Moreover, for all integers $0\leq r\leq\mathrm{min}\{m,n\}-1$ and $\mathbf{x}\in\mathbb{R}^n$ with $S^-(\mathbf{x})=r$, if $S^-(A\mathbf{x})=S^-(\mathbf{x})$ and $A\mathbf{x}\neq\mathbf{0}$, then the sign of the first (last) non-zero component of $A\mathbf{x}$ agrees with $\epsilon_r\epsilon_{r+1}$ times the sign of the first (last) non-zero component of $\mathbf{x}$. 
			\item[(3)] For every square submatrix $A_k$ of $A$ of size $k\times k$, where $1\leq k\leq\mathrm{min}\{m,n\}$, and given any fixed vector $\boldsymbol{\alpha}:=(\alpha_1,-\alpha_2,\ldots,(-1)^{k-1}\alpha_k)^T\in\mathbb{R}^k$ with all $\alpha_j>0$, we define the vector \[\mathbf{y}^{A_k}:=adj(A_k)\boldsymbol{\alpha}.\] Then $S^-(A_k\mathbf{y}^{A_k})\leq S^-(\mathbf{y}^{A_k})$. Moreover, for all integers $0\leq r\leq k-1$ and $\mathbf{y}^{A_k}\in\mathbb{R}^k$ with $S^-(\mathbf{y}^{A_k})=r$, if $S^-(A_k\mathbf{y}^{A_k})=S^-(\mathbf{y}^{A_k})$ and $A_k\mathbf{y}^{A_k}\neq\mathbf{0}$, then the sign of the first (last) non-zero component of $A_k\mathbf{y}^{A_k}$ agrees with $\epsilon_r\epsilon_{r+1}$ times the sign of the first (last) non-zero component of $\mathbf{y}^{A_k}$. 
		\end{itemize}
	\end{theorem}
	\begin{proof}
		%%%%%%%%%%%%%%%%%%%%(1) implies (2)%%%%%%%%%%%%%%%%%%%%
		We first show that (1)$\implies$(2). Since $A\in\mathbb{R}^{m\times n}$ is SR$(\epsilon)$, by the Gantmacher--Krein Theorem \ref{G-K}, there exists a sequence of $m\times n$ SSR$(\epsilon)$ matrices $(A_k)$ converging entrywise to $A$.
		For $\mathbf{0}\neq\mathbf{x}\in\mathbb{R}^n$, using Theorem \ref{B} and Lemma \ref{liminf}, we have \[S^-(A\mathbf{x})\leq\varliminf_{k\to\infty}S^-(A_k\mathbf{x})\leq \varliminf_{k\to\infty}S^+(A_k\mathbf{x})\leq \varliminf_{k\to\infty}S^-(\mathbf{x}) = S^-(\mathbf{x}).\]
		If $S^-(A\mathbf{x})=S^-(\mathbf{x})=r$ where $0\leq r\leq\mathrm{min}\{m,n\}-1$ and $A\mathbf{x}\neq\mathbf{0}$, then for all $k$ sufficiently large we necessarily have \[r=S^-(A\mathbf{x})\leq S^-(A_k\mathbf{x})\leq S^+(A_k\mathbf{x})\leq S^-(\mathbf{x})=r\]
		using Theorem \ref{B} and Lemma \ref{liminf}. Therefore, for large $k$, $S^-(A_k\mathbf{x})=S^+(A_k\mathbf{x})$, i.e. the sign patterns of $A_k\mathbf{x}$ do not depend on any zero entries. Since $S^+(A_k\mathbf{x})=S^-(\mathbf{x})$ and $A_k\mathbf{x}\neq\mathbf{0}$, we have that the sign pattern of $A_k\mathbf{x}$ agrees with $\epsilon_r\epsilon_{r+1}$ times the sign of $\mathbf{x}$ by Theorem \ref{B}. By a limiting argument the non-zero sign patterns of $A\mathbf{x}$ and $A_k\mathbf{x}$ agree.
		
		%%%%%%%%%%%%%%%%%%%%(2) implies (1)%%%%%%%%%%%%%%%%%%%%
		That (2)$\implies$(1) is shown similarly to the proof of Theorem \ref{B} by induction on size $p\times p$, where $1\leq p\leq\mathrm{min}\{m,n\}$. Again observe that $S^-(A\mathbf{e}^j)\leq S^-(\mathbf{e}^j)=0$ for $1\leq j\leq n$. Since the first non-zero component of $\mathbf{e}^j$ is positive, by the hypothesis all non-zero components of $A\mathbf{e}^j$ have  sign $\epsilon_1$ and hence all non-zero entries of $A$ are of sign $\epsilon_1$.
		
		We now assume that all $(p-1)\times(p-1)$ minors of $A$ have  sign $\epsilon_{p-1}$, where $2\leq p\leq\mathrm{min}\{m,n\}$. Consider the $p\times p$ submatrix of $A$ indexed by rows $1\leq i_1<\cdots<i_p\leq m$ and columns $1\leq j_1<\cdots<j_p\leq n$. If the determinant of this submatrix is zero, then we are done. Therefore, assume that this minor is non-zero and hence there exists $\mathbf{z}=(z_{1},\ldots,z_{p})^T\in\mathbb{R}^p$ satisfying 
		\[A\begin{pmatrix}
			i_1,\ldots,i_p\\
			j_1,\ldots,j_p
		\end{pmatrix}\mathbf{z}=\mathbf{d}_p.\]
		By repeating the corresponding part of the proof of Theorem \ref{B}, we have $S^-(A\mathbf{x})=S^-(\mathbf{x})=p-1$ and hence $A\mathbf{x}\neq\mathbf{0}$, which further implies $\epsilon_{p-1}\epsilon_pz_1>0$. Now by Cramer's rule,
		\[z_{1}=\frac{\displaystyle{\sum_{l=1}^{p}}\det A\begin{pmatrix}
				i_1,\ldots,i_{l-1},i_{l+1}\ldots,i_p \\
				j_2,j_3, \ldots \hspace{.2cm} \ldots, j_{p-1},j_p
		\end{pmatrix}}{\det A\begin{pmatrix}
				i_1,\ldots,i_p \\
				j_1,\ldots,j_p
		\end{pmatrix}}.\]
		Multiplying both sides by $\epsilon_{p-1}\epsilon_p$, we obtain \[0<\epsilon_{p-1}\epsilon_pz_{1}=\epsilon_{p-1}\epsilon_p\frac{\displaystyle{\sum_{l=1}^{p}}\det A\begin{pmatrix}
				i_1,\ldots,i_{l-1},i_{l+1}\ldots,i_p \\
				j_2,j_3, \ldots \hspace{.2cm} \ldots, j_{p-1},j_p
		\end{pmatrix}}{\det A\begin{pmatrix}
				i_1,\ldots,i_p \\
				j_1,\ldots,j_p
		\end{pmatrix}}.\]
		By the induction hypothesis,  the sign of the numerator is $\epsilon_{p-1}$. Thus the sign of $\det A\begin{pmatrix}
			i_1,\ldots,i_p \\
			j_1,\ldots,j_p
		\end{pmatrix}$ is $\epsilon_p$. This completes the induction step. 
		
		%%%%%%%%%%%%%%%%%%%%(2) implies (3)%%%%%%%%%%%%%%%%%%%%
		Now we show that (2)$\implies$(3) again for arbitrary vectors $\mathbf{0}\neq\mathbf{y}^{A_k}\in\mathbb{R}^k$. The proof is similar to that of Theorem \ref{B}. Fix $1\leq k\leq\mathrm{min}\{m,n\}$, and let $A_k$ be $k\times k$ arbitrary submatrix of $A$ whose rows and columns are indexed by $1\leq i_1<\cdots<i_k\leq m$ and $1\leq j_1<\cdots<j_k\leq n$, respectively. Let us take $\mathbf{x}\in\mathbb{R}^n$ whose coordinates at position $j_l$ for $l=1,\ldots,k$ are $y_l^{A_k}$ and zero elsewhere. Then
		\[S^-(A_k\mathbf{y}^{A_k})\leq S^-(A\mathbf{x}) \leq S^-(\mathbf{x})= S^-(\mathbf{y}^{A_k}).\] 
		Now suppose $S^-(A_k\mathbf{y}^{A_k})=S^-(\mathbf{y}^{A_k})=r$ where $0\leq r\leq k-1$ with $A_k\mathbf{y}^{A_k}\neq\mathbf{0}$. Then \[S^-(A_k\mathbf{y}^{A_k})=S^-(\mathbf{y}^{A_k})=S^-(\mathbf{x})=S^-(A\mathbf{x})=r\hspace{.2cm}\mathrm{and}\hspace{.2cm}A\mathbf{x}\neq\mathbf{0}.\] 
		Assume without loss of generality that the first and the last non-zero entries of $A_k\mathbf{y}^{A_k}$ are in positions $s,t\in\{1,\ldots,k\}$, respectively. Since $S^-(A\mathbf{x})=S^-(A_k\mathbf{y}^{A_k})=r$ and $A_k\mathbf{y}^{A_k}$ is the sub-string of $A\mathbf{x}$, hence all coordinates of $A\mathbf{x}$ in positions $1, 2,\ldots,i_s$ (respectively $i_t<\cdots<m$) have the same sign which agrees with $\epsilon_r\epsilon_{r+1}$ times the sign of the first (respectively last) non-zero component of $\mathbf{x}$ which is same as the sign of the first (respectively last) non-zero component of $\mathbf{y}^{A_k}$. 
		
		%%%%%%%%%%%%%%%%%%%%(3) implies (1)%%%%%%%%%%%%%%%%%%%%
		Finally we show that (3)$\implies$(1). We show by induction on $r$ that the sign of $\det A_r$ is $\epsilon_r$ for all $r\times r$ non-singular submatrices of $A$, where $1\leq r\leq\mathrm{min}\{m,n\}$. For the base case $r=1$, if $A_1=(0)_{1\times1}$, there is nothing to prove. Otherwise, $\adj(A_1)=(1)_{1\times1}$ which further implies $y^{A_1}=(\alpha_1)_{1\times1}$, where $\alpha_1>0$. Since $S^-(A_1y^{A_1})=S^-(y^{A_1})=0$ and $A_1y^{A_1}\neq0$, by the hypothesis the sign of $A_1y^{A_1}$ is $\epsilon_1$ times the sign of $y^{A_1}$. Therefore, all non-zero components of $A$ have sign $\epsilon_1$.
		
		For the induction step, $A_r$ is a $r\times r$ submatrix of $A$ and $A$ is SR$_{r-1}$ with sign pattern $\epsilon_1,\ldots,\epsilon_{r-1}$. If $\det A_r=0$, we are done; else we assume $\det A_r\neq0$. Let \[\boldsymbol{\alpha}=(\alpha_1,-\alpha_2,\ldots,(-1)^{r-1}\alpha_r)^T\in\mathbb{R}^r\hspace{.2cm}\mathrm{with}\hspace{.2cm}\mathrm{all} \hspace{.2cm}\alpha_j>0\] and define \[\mathbf{y}^{A_r}:=\adj(A_r)\boldsymbol{\alpha}.\] 
		Note that no row of $\adj(A_r)$ is zero as $\det A_r\neq0$. Also, since $A_{r-1}$ is SR with $\epsilon=(\epsilon_1,\ldots,\epsilon_{r-1})$, $\adj A_r$ has entries whose signs are in a checker-board pattern. Further, as all coordinates of $\boldsymbol{\alpha}$ are non-zero with alternating signs, it follows that $S^-(\mathbf{y}^{A_r})=r-1$. Since $\mathbf{0}\neq A_r\mathbf{y}^{A_r}=(\det A_r)\boldsymbol{\alpha},$ we have \[S^-(A_r\mathbf{y}^{A_r})=S^-(\mathbf{y}^{A_r})=r-1.\] 
		By the hypothesis, the sign of the first non-zero entry of $A_r\mathbf{y}^{A_r}$ and the first non-zero entry of $\mathbf{y}^{A_r}$ have the following relation:
		\[\mathrm{sign}(\alpha_1\det A_r)=\epsilon_{r-1}\epsilon_r\mathrm{sign}(\sum_{j=1}^{r}\alpha_jA_r^{j1}).\] 
		Thus sign$(\det A_r)=\epsilon_r$. This completes our proof.
	\end{proof}

	%%%%%%%%%%%%%%%%%%%% Section 3 %%%%%%%%%%%%%%%%%%%%
	\section{Theorem \ref{D}: Linear preserver problem for sign regularity}\label{section SR}
	The goal of this section is to prove Theorem \ref{D}: Classify all linear sign regularity preservers. That $(1)\implies(2)$ is immediate, while $(3)\implies(1)$ follows from straightforward calculations. To complete the proof we need to show $(2)\implies(3)$. Note that the map $A\mapsto A^T$ does not preserve the domain $\mathcal{SR} \cap\mathbb{R}^{m\times n}$ for $m\neq n$. So we need to treat the cases $m=n$ and $m\neq n$ separately. To proceed, we need some basic notations and a preliminary result.
	\begin{enumerate}[(i)]
		\item Let $E_{ij}$ denote the matrix whose $(i,j)$ entry is 1 and zero elsewhere.
		\item $S_{ij}$ is defined as $S_{ij}:=\{E_{pq}:\mathcal{L}(E_{ij})_{pq}\neq0\}$.
		\item Let $J=J_{m \times n}$ be the $m \times n$ matrix with all entries 1. We will denote $\mathcal{L}(J)$ by $\mathcal{L}(J):=Y=(y_{ij})$, and we will specify the order $m\times n$ as and when we use the matrix $J$.
		\item Let $\epsilon_i:=\epsilon_i(A)$ denote the sign of the $i\times i$ non-zero minors of $A\in\mathcal{SR}$.
	\end{enumerate}
	The following preliminary lemma will be handy in classifying linear SSR-preservers using linear SR-preservers. We provide the proof for completeness.
	\begin{lemma}\cite[Lemma 1]{BHJ85}\label{lemma_preserver}
		Let $S$ be a subset of a finite-dimensional real vector space $V$. Then $P(S)\subseteq P(\overline{S}) \subseteq P(\mathrm{span}(S))$.
	\end{lemma}
	\begin{proof}
		Let $\mathcal{L}\in P(S)$. Then $\mathcal{L}$ maps $\mathrm{span}(S)$ homeomorphically onto itself, since dim$V<\infty$. As $\overline{S}\subseteq \mathrm{span}(S)$, it follows that $\mathrm{span}(S)=\mathrm{span}(\overline{S})$, and $\mathcal{L}(\overline{S})=\overline{\mathcal{L}(S)}=\overline{S}.$
	\end{proof}
	First, we will prove certain propositions which will be used in proving Theorem \ref{D} for both the $m=n$ and $m\neq n$ case. \smallskip
	
	Let $\mathcal{L}:\mathbb{R}^{m\times n}\to\mathbb{R}^{m\times n}$ be a linear transformation such that $\mathcal{L}$ maps $\mathcal{SR}_2$ onto itself, where either $m=n\geq 3$ or $m \neq n$ and $m,n \geq 2$. If $m\neq n$, then we assume without loss of generality that $m>n$, i.e., the number of rows are more than the number of columns. The case when $m<n$ is dealt similarly by applying the arguments to the rows that we have applied to the columns and vice-versa or by pre -- or post -- composing $\mathcal{L}$ with $A\mapsto A^T$ to obtain a linear preserver of $\mathcal{SR}_2\cap\mathbb{R}^{m\times n}$ with $n>m$.
	
	Note that $E_{ij}\in \mathcal{SR}_2$ for all $1\leq i\leq m$, $1\leq j\leq n$. By Lemma \ref{lemma_preserver}, $\mathcal{L}$ maps $\mathbb{R}^{m\times n}$ onto itself. Hence $\mathcal{L}^{-1}$ exists and $\mathcal{L}^{-1}\in P(\mathcal{SR}_2)$.
	
	%Let $\mathfrak{B}=\{E_{11},\ldots,E_{nn};E_{12},\ldots,E_{mn}\}$ be an ordered basis of $\mathbb{R}^{m\times n}$. %Let $L\in\mathbb{R}^{mn\times mn}$ be the matrix representation of $\mathcal{L}$ with respect to the ordered basis $\mathfrak{B}$ above.
	%Note that for any matrix in $\mathcal{SR}$ with $\epsilon_1=-1$, the above holds trivially as $-E_{ij}\in\mathcal{SR}_2$ for all $1\leq i\leq m$, $1\leq j\leq n$.
	\begin{prop}\label{SR_square_image_same_epsilon_1_sign}
		Let $\mathfrak{B}=\{E_{11},\ldots,E_{nn};E_{12},\ldots,E_{mn}\}$ be an ordered basis of $\mathbb{R}^{m\times n}$. Then $\epsilon_1(\mathcal{L}(E_{ij}))$ has the same sign for all $E_{ij}\in \mathfrak{B}$.
	\end{prop}
	\begin{proof}
		Indeed, suppose that there exist two distinct $E_{ef}$ and $E_{kl}$ in $\mathfrak{B}$ such that $\mathcal{L}(E_{ef})=U=(u_{ij})_{i,j=1}^{m,n}$ and $\mathcal{L}(E_{kl})=V=(v_{ij})_{i,j=1}^{m,n}$ but $\epsilon_1(U)\neq\epsilon_1(V)$. Without loss of generality, assume that $\epsilon_1(U)=1$ and $\epsilon_1(V)=-1$. Therefore, $u_{ij}\geq 0\geq v_{ij}$ for all $1\leq i\leq m$, $1\leq j\leq n$. \\\\
		Note the following facts about the matrices $U$ and $V$:
		\begin{enumerate}[(i)]
			\item $U, V\neq0_{m\times n}$, since $\mathcal{L}$ is invertible.
			\item \textit{$u_{st}\neq0$ if and only if $v_{st}\neq0$ where $1\leq s\leq m,$ $1\leq t\leq n$.} Assume to the contrary that some $u_{st}>0$ but $v_{st}=0$. Note that for all $c\geq0$, $E_{ef}+cE_{kl}\in \mathcal{SR}_2$, and thus, $\mathcal{L}(E_{ef}+cE_{kl})=U+cV\in \mathcal{SR}_2$. Now, $(U+cV)_{st}=u_{st}>0$. Since $V\neq0$ therefore there exist $1\leq q\leq m,$ $1\leq r\leq n$ such that $v_{qr}\neq0$. Hence, we can choose $c>0$ such that $(U+cV)_{qr}<0$, a contradiction.
			\item \textit{There exist at least two non-zero entries in $U$ and $V$.} Suppose instead that $u_{ij}$ is the only non-zero entry in $U$. Again, we can choose $c>0$ such that $\mathcal{L}(E_{ef}+cE_{kl})=0_{m\times n}$, a contradiction.
			\item $U\neq\alpha V$ for any $\alpha\in\mathbb{R}$.
		\end{enumerate}
		From the above observations, there exist $1\leq i,s\leq m$ and $1\leq j,t\leq n$ such that $u_{ij}, v_{ij}, u_{st}, v_{st}\neq0$ and $\frac{u_{ij}}{v_{ij}}\neq\frac{u_{st}}{v_{st}}$. Without loss of generality, assume that $\frac{-u_{ij}}{v_{ij}}<\frac{-u_{st}}{v_{st}}$. Now choose $c$ such that $0<\frac{-u_{ij}}{v_{ij}}<c<\frac{-u_{st}}{v_{st}}$. For this $c>0$, $\mathcal{L}(E_{ef}+cE_{kl})\in \mathcal{SR}_2$ has a positive and a negative entry, a contradiction. Hence $\epsilon_1(\mathcal{L}(E_{ij}))$ has the same sign for all $i,j$.
	\end{proof}   
	\begin{rem}\label{Ato-Awlog}
		Note that $\mathcal{L}$ is a linear $\mathcal{SR}_2$-preserver if and only if $\mathcal{L}$ composed with the map $A\mapsto-A$ is an $\mathcal{SR}_2$-preserver. Hence, without loss of generality we assume that $\epsilon_1(\mathcal{L}(E_{ij}))=1$ for all $E_{ij}\in\mathfrak{B}$.
	\end{rem} 
	\begin{prop} \label{SR_square_monomial}
		Let $L\in\mathbb{R}^{mn\times mn}$ be the matrix representation of $\mathcal{L}$ with respect to the ordered basis $\mathfrak{B}$ above. Then $L$ is a monomial matrix, i.e. every row and column of $L$ contains exactly one non-zero entry which is positive.
	\end{prop}	
	\begin{proof}
		From Remark \ref{Ato-Awlog}, $\epsilon_1(\mathcal{L}(E_{ij}))=1$ for all $E_{ij}\in\mathfrak{B}.$ Thus, $L\geq 0$. Since $\mathcal{L}^{-1}$ is also a linear $\mathcal{SR}_2$-preserver, by Proposition \ref{SR_square_image_same_epsilon_1_sign} either $L^{-1}\geq 0$ or $L^{-1}\leq 0$. But, $L\geq 0$ and $LL^{-1}=I,$ therefore $L^{-1}\geq 0$. Hence $L$ is a  monomial matrix.
	\end{proof}	
	\begin{rem}\label{Sij_singleton}
		Since $L$ is a monomial matrix, $S_{ij}$ is a non-empty singleton set for all $1\leq i\leq m$, $1\leq j\leq n$. Also, $S_{ij}\cap S_{kl}=\emptyset$ for $(i,j)\neq(k,l)$.
	\end{rem}
	Now, we will prove Theorem \ref{D} using Proposition \ref{SR_square_monomial} for $m=n\geq3$.
	%%%%%%%%%%%%%%%%%%%% Proof of Theorem C %%%%%%%%%%%%%%%%%%%%
	\begin{proof}[Proof of Theorem \ref{D} for $m=n$]
		To show $(2)\implies(3)$, let $\mathcal{L}:\mathbb{R}^{n\times n}\to\mathbb{R}^{n\times n}$ for $n\geq3$ be a linear transformation such that $\mathcal{L}$ maps the class of SR$_2$ matrices onto itself. We claim that $\mathcal{L}$ is a composition of one or more of the transformations listed in Theorem \ref{D}(3). Note that Proposition \ref{SR_square_monomial} holds. For convenience, we split the rest of the proof into several propositions.
		\begin{prop} \label{SR_square_S11SnnS1nSn1_elements}
			For $\mathcal{L}\in P(\mathcal{SR}_2)$, the element in each of the sets $S_{11}$,  $S_{nn}$, $S_{1n}$, and $S_{n1}$ must be among the following:
			\[E_{11},\hspace{.1cm}E_{nn},\hspace{.1cm}E_{1n},\hspace{.1cm}\mathrm{or}\hspace{.1cm}E_{n1.}\]
		\end{prop}
		\begin{proof}
			%	To show this, it is sufficient to consider SR$_2$ matrices with $\epsilon$ from $(1,1)$ and $(1,-1).$ 
			First, we will prove for $S_{11}$. Suppose that \[S_{11}\neq\{E_{11}\}, \{E_{nn}\}, \{E_{1n}\},  \{E_{n1}\}.\] 
			Let $J(c)$ be the matrix of size $n\times n$ obtained by multiplying the $(1,1)$ entry of $J=J_{n\times n}$ by $c$. Then $J(c)\in \mathcal{SR}_2$ for $c>0$ and hence $\mathcal{L}(J(c)):=Y(c)\in \mathcal{SR}_2$. Note that all the entries of $Y(c)$ are non-zero by Remark \ref{Sij_singleton}. Now, consider the following cases.\smallskip
			
			
			If $S_{11}=\{E_{1k}\}$ where $k\neq1,n$, then consider the following two minors of size $2\times2$ of $Y(c)$ included in
			\begin{itemize}
				\item  rows $1$ and $n$ and columns $1$ and $k$: 
				$\det\begin{pmatrix}
					y_{11} & cy_{1k} \\
					y_{n1} & y_{nk}
				\end{pmatrix}=y_{11}y_{nk}-cy_{1k}y_{n1}$, and 
				\item rows $1$ and $n$ and columns $k$ and $n$: 
				$\det\begin{pmatrix}
					cy_{1k} & y_{1n} \\
					y_{nk} & y_{nn}
				\end{pmatrix}=cy_{1k}y_{nn}-y_{1n}y_{nk}$. 
			\end{itemize}
			It is always possible to choose $c>1$ as well as $0<c<1$ such that above two minors have opposite signs, a contradiction. 
			
			Similarly, we can show that $S_{11}\neq\{E_{k1}\}, \{E_{nk}\}, \{E_{kn}\}$ for $k\neq1,n$.
			
			If $S_{11}=\{E_{ij}\}$ where $\{i,j\}\neq\{1,n\}$, then the following two minors of size $2\times2$ of $Y(c)$ included in
			\begin{itemize}
				\item rows $1$ and $i$ and columns $1$ and $j$: 
				$\det\begin{pmatrix}
					y_{11} & y_{1j} \\
					y_{i1} & cy_{ij}
				\end{pmatrix}$, and 
				\item rows $i$ and $n$ and columns $1$ and $j$: 
				$\det\begin{pmatrix}
					y_{i1} & cy_{ij} \\
					y_{n1} & y_{nj}
				\end{pmatrix}$ 
			\end{itemize}
			give us a contradiction. \smallskip
			
			Similarly, we can show this for each of $S_{nn}$, $S_{1n}$, and $S_{n1}$ by multiplying $(n,n)$, $(1,n)$, and $(n,1)$ entry, respectively of $J=J_{n\times n}$ by $c>0$.
		\end{proof}
		\begin{prop} \label{SR_square_S11Snn_S1nSn1_combo}
			For $\mathcal{L}\in P(\mathcal{SR}_2)$, the following pairwise combinations are possible.
			\begin{itemize}
				\item[(i)] $S_{11}=\{E_{11}\} \hspace{.2cm}\mathrm{and}\hspace{.2cm} S_{nn}=\{E_{nn}\},\hspace{.2cm}\mathrm{or}\hspace{.2cm} S_{11}=\{E_{nn}\} \hspace{.2cm}\mathrm{and}\hspace{.2cm} S_{nn}=\{E_{11}\}, \hspace{.2cm}\mathrm{or}$ \\
				$S_{11}=\{E_{1n}\} \hspace{.2cm}\mathrm{and}\hspace{.2cm} S_{nn}=\{E_{n1}\}, \hspace{.2cm}\mathrm{or}\hspace{.2cm} S_{11}=\{E_{n1}\} \hspace{.2cm}\mathrm{and}\hspace{.2cm} S_{nn}=\{E_{1n}\}.$ 
				\item[(ii)] $S_{1n}=\{E_{1n}\} \hspace{.2cm}\mathrm{and}\hspace{.2cm} S_{n1}=\{E_{n1}\}, \hspace{.2cm}\mathrm{or}\hspace{.2cm} S_{1n}=\{E_{n1}\} \hspace{.2cm}\mathrm{and}\hspace{.2cm} S_{n1}=\{E_{1n}\}, \hspace{.2cm}\mathrm{or}$ \\
				$S_{1n}=\{E_{11}\} \hspace{.2cm}\mathrm{and}\hspace{.2cm} S_{n1}=\{E_{nn}\}, \hspace{.2cm}\mathrm{or}\hspace{.2cm} S_{1n}=\{E_{nn}\} \hspace{.2cm}\mathrm{and}\hspace{.2cm} S_{n1}=\{E_{11}\}.$
			\end{itemize}
		\end{prop} 
		\begin{proof} First, we prove (i). From Proposition \ref{SR_square_S11SnnS1nSn1_elements}, we have either \[S_{11}=\{E_{11}\} \hspace{.1cm}\mathrm{or}\hspace{.1cm} S_{11}=\{E_{nn}\} \hspace{.1cm}\mathrm{or}\hspace{.1cm} S_{11}=\{E_{n1}\} \hspace{.1cm}\mathrm{or}\hspace{.1cm} S_{11}=\{E_{1n}\}\] and
			\[S_{nn}=\{E_{11}\} \hspace{.1cm}\mathrm{or}\hspace{.1cm} S_{nn}=\{E_{nn}\} \hspace{.1cm}\mathrm{or}\hspace{.1cm} S_{nn}=\{E_{n1}\} \hspace{.1cm}\mathrm{or}\hspace{.1cm} S_{nn}=\{E_{1n}\}.\]
			Now, out of sixteen possible combinations of $S_{11}$ and $S_{nn}$, four are discarded straightaway because of the ``monomiality" of $L$, viz., %of monomiality of $L$, viz., 
			\[S_{11}=S_{nn}=\{E_{11}\},\hspace{.1cm} S_{11}=S_{nn}=\{E_{nn}\}, \hspace{.1cm} S_{11}=S_{nn}=\{E_{n1}\}, \hspace{.1cm}\mathrm{and}\hspace{.1cm}  S_{11}=S_{nn}=\{E_{1n}\}.\] 
			It remains to show that eight other combinations of $S_{11}$ and $S_{nn}$ mentioned below are not possible:
			\[S_{11}=\{E_{11}\} \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{nn}=\{E_{n1}\}, \quad 
			S_{11}=\{E_{11}\} \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{nn}=\{E_{1n}\},\] 
			\[S_{11}=\{E_{nn}\} \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{nn}=\{E_{n1}\}, \quad 
			S_{11}=\{E_{nn}\} \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{nn}=\{E_{1n}\}, \]
			\[S_{11}=\{E_{n1}\} \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{nn}=\{E_{11}\}, \quad
			S_{11}=\{E_{n1}\} \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{nn}=\{E_{nn}\},\] 
			\[S_{11}=\{E_{1n}\} \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{nn}=\{E_{11}\}, \quad
			S_{11}=\{E_{1n}\} \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{nn}=\{E_{nn}\}.\]
			Indeed, assume that $S_{11}=\{E_{11}\}$ and $S_{nn}=\{E_{n1}\}$. Let $J(c)$ be the matrix of size $n\times n$ obtained by multiplying the $(1,1)$ and $(n,n)$ entries of $J=J_{n\times n}$ by $c$. Then $J(c)\in \mathcal{SR}_2$ for $c>0$ and hence $\mathcal{L}(J(c)):=Y(c)\in \mathcal{SR}_2$. Now, consider the following two minors of size $2\times2$ of $Y(c)$ included in
			\begin{itemize}
				\item rows $1$ and $2$ and columns $1$ and $2$: 
				$\det\begin{pmatrix}
					cy_{11} & y_{12} \\
					y_{21} & y_{22}
				\end{pmatrix}=cy_{11}y_{22}-y_{12}y_{21}$, and
				\item rows $2$ and $n$ and columns $1$ and $2$: 
				$\det\begin{pmatrix}
					y_{21} & y_{22} \\
					cy_{n1} & y_{n2}
				\end{pmatrix}=y_{21}y_{n2}-cy_{22}y_{n1}$.
			\end{itemize}
			We can always choose $c>0$ such that the above two minors are of opposite signs, which is a contradiction. Similarly, for the remaining seven cases, we can always find two non-zero $2\times2$ minors of $Y(c)$ having opposite signs. \smallskip
			
			Adapting the same argument as in the preceding half of this proof shows that (ii) holds.
		\end{proof}
		\begin{rem}
			In part (i) of Proposition \ref{SR_square_S11Snn_S1nSn1_combo}, we can assume without loss of generality that
			\begin{equation} \label{SR_square_S11Snn_wlog}
				S_{11}=\{E_{11}\}\hspace{.1cm}\mathrm{and}\hspace{.1cm}S_{nn}=\{E_{nn}\}
			\end{equation}
			since $\mathcal{L}\in P(\mathcal{SR}_2)$ if and only if $\mathcal{L}$ composed with the maps $A\mapsto AP_n$ and $A\mapsto P_nA$ is a linear $\mathcal{SR}_2$-preserver. %Since our domain is composed of SR$_2$ matrices with all possible $\epsilon$, and $\mathcal{L}$ is preserving SR$_2$ matrices with any given $\epsilon$, thus we must take the union of the restrictions obtained in
			Now, using Proposition \ref{SR_square_monomial} and \eqref{SR_square_S11Snn_wlog} in part (ii) of Proposition \ref{SR_square_S11Snn_S1nSn1_combo}, we have either 
			\[S_{11}=\{E_{11}\},\hspace{.1cm} S_{nn}=\{E_{nn}\},\hspace{.1cm} S_{1n}=\{E_{1n}\}, \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{n1}=\{E_{n1}\}\hspace{.2cm}\mathrm{or}\] 
			\[S_{11}=\{E_{11}\},\hspace{.1cm} S_{nn}=\{E_{nn}\},\hspace{.1cm} S_{1n}=\{E_{n1}\}, \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{n1}=\{E_{1n}\}.\] 
			We henceforth assume that
			\begin{equation}\label{SR_square_S11SnnS1nSn1_wlog}
				S_{11}=\{E_{11}\},\hspace{.1cm} S_{nn}=\{E_{nn}\},\hspace{.1cm} S_{1n}=\{E_{1n}\}, \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{n1}=\{E_{n1}\}
			\end{equation}
			without loss of generality, since $\mathcal{L}\in P(\mathcal{SR}_2)$ if and only if $\mathcal{L}$ composed with the map $A\mapsto A^T$ is a linear $\mathcal{SR}_2$-preserver.
		\end{rem}
		\begin{prop} \label{SR_square_firstLast_rowTOrow_colTOcol}
			Let $\mathcal{L}\in P(\mathcal{SR}_2)$. %such that
%			\begin{equation}
%				S_{11}=\{E_{11}\},\hspace{.1cm} S_{nn}=\{E_{nn}\},\hspace{.1cm} S_{1n}=\{E_{1n}\}, \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{n1}=\{E_{n1}\}. \nonumber
%			\end{equation}
			\begin{itemize}
				\item[(i)]  Then $\mathcal{L}$ must map the first (and last) row and column of its arguments entirely to the first (and last) row and column, respectively.
				\item[(ii)] Further, $\mathcal{L}$ must map all rows and columns of its arguments entirely to some row and column, respectively.
			\end{itemize}	
		\end{prop}	
		\begin{proof}
			We will first show that $\mathcal{L}$ must map the first row of its arguments entirely to the first row. Again, let $J(c)$ be the matrix of size $n\times n$ obtained by multiplying the first row of $J=J_{n\times n}$ by $c>0$. Then $J(c)\in \mathcal{SR}_2$ with $\epsilon_1=1$ and hence $\mathcal{L}(J(c)):=Y(c)\in \mathcal{SR}_2$. Assume that $\mathcal{L}$ does not map the first row of its arguments entirely to the first row. Thus, there exists $1<k<n$ such that the $(1,k)$ position of the matrix $Y(c)$ is not occupied by the image of any element from the first row of $J(c)$. Using \eqref{SR_square_S11SnnS1nSn1_wlog}, we can obtain the following two minors of size $2\times 2$ of $Y(c)$ included in
			\begin{itemize}
				\item rows $1$ and $n$ and columns $1$ and $k$: 
				$\det\begin{pmatrix}
					cy_{11} & y_{1k} \\
					y_{n1} & \alpha y_{nk}
				\end{pmatrix}=c\alpha y_{11}y_{nk}-y_{1k}y_{n1}$, and
				\item rows $1$ and $n$ and columns $k$ and $n$: 
				$\det\begin{pmatrix}
					y_{1k} & cy_{1n} \\
					\alpha y_{nk} & y_{nn}
				\end{pmatrix}=y_{1k}y_{nn}-c\alpha y_{1n}y_{nk}$,
			\end{itemize}
			where  either $\alpha=c$ or $\alpha=1$.
			
			Choosing $c$ large enough gives the above two minors of $Y(c)$ are of opposite signs, which is a contradiction. Similarly, the remaining assertions can be established by multiplying that particular row and column of $J=J_{n\times n}$ by $c>0$. This proves (i).
			
			To prove (ii), we begin by showing $\mathcal{L}$ must map the $k^{th}$ row of its arguments entirely to some row for $1<k<n$. Again, let $J(c)$ be the matrix of size $n\times n$ obtained by multiplying the $k^{th}$ row of $J=J_{n\times n}$ by $c>0$. Then $J(c)\in \mathcal{SR}_2$ and hence $\mathcal{L}(J(c)):=Y(c)\in \mathcal{SR}_2$. By Proposition \ref{SR_square_firstLast_rowTOrow_colTOcol}(i), $\mathcal{L}$ must map the first column of its arguments entirely to the first column and hence the $(k,1)$ element of $J(c)$ will be mapped to the first column. Let us say $\mathcal{L}$ maps it to the $(s,1)$ position of $Y(c)$ where $1<s<n$. Suppose that $\mathcal{L}$ does not map the $k^{th}$ row of $J(c)$ entirely to some row. Then there exists $1<j\leq n$ such that the $(s,j)$ position of the matrix $Y(c)$ is not occupied by the image of any element from the $k^{th}$ row of $J(c)$. By Proposition \ref{SR_square_firstLast_rowTOrow_colTOcol}(i) and sufficiently large $c$, we have the following two minors of size $2\times2$ of $Y(c)$ included in
			\begin{itemize}
				\item rows $1$ and $s$ and columns $1$ and $j$: 
				$\det\begin{pmatrix}
					y_{11} & y_{1j} \\
					cy_{s1} & y_{sj}
				\end{pmatrix}<0$, and
				\item  rows $s$ and $n$ and columns $1$ and $j$: 
				$\det\begin{pmatrix}
					cy_{s1} & y_{sj} \\
					y_{n1} & y_{nj}
				\end{pmatrix}>0$.
			\end{itemize}
			Hence, $\mathcal{L}$ must map every row of its argument entirely to some row. Similarly, we can show that $\mathcal{L}$ must map every column of its argument entirely to some column by multiplying the $k^{th}$ column of the matrix $J=J_{n\times n}$ by $c>0$ where $1<k<n$.
		\end{proof} 
		To summarize, we have used the transformations $A\mapsto-A$, $A\mapsto P_nA$, $A\mapsto AP_n$, and $A\mapsto A^T$ to assume that $\mathcal{L}\in P(\mathcal{SR}_2)$ has the following properties:
		\begin{enumerate}[(i)]
			\item $\epsilon_1(\mathcal{L}(E_{ij}))=1$ for all $1\leq i,j\leq n$.
			\item $S_{11}=\{E_{11}\},\hspace{.1cm} S_{nn}=\{E_{nn}\},\hspace{.1cm} S_{1n}=\{E_{1n}\}, \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{n1}=\{E_{n1}\}$.
			\item $\mathcal{L}$ maps the entire first (and last) row and column of its arguments to the first (and last) row and column, respectively.
			\item $\mathcal{L}$ maps every other row and column to some row and column, respectively.
		\end{enumerate} 
		With the above information in hand, we are now ready to use induction on $n$. We first prove the base case, when $n=3$. Let $\mathcal{L}:\mathbb{R}^{3\times3}\to\mathbb{R}^{3\times3}$ be a linear map such that $\mathcal{L}(\mathcal{SR}_2)=\mathcal{SR}_2$. \smallskip
		
		Let $\mathfrak{B}=\{E_{11}, E_{22}, E_{33}, E_{12}, E_{13}, E_{21}, E_{23}, E_{31}, E_{32}\}$ be an ordered basis of $\mathbb{R}^{3\times 3}$. We have $S_{ij}=\{E_{ij}\}$ for all $1\leq i,j\leq 3$ using Propositions \ref{SR_square_monomial} and \ref{SR_square_firstLast_rowTOrow_colTOcol}. Thus, $\mathcal{L}(E_{11})=l_{1}E_{11},\mathcal{L}(E_{22})=l_{2}E_{22},\ldots,\mathcal{L}(E_{32})=l_{9}E_{32}\in \mathcal{SR}_2$. Note that $l_{i}>0$ for all $1\leq i\leq 9$ by Remark \ref{Ato-Awlog} and Proposition \ref{SR_square_monomial}. 
		%	and the matrix $L$ which represents $\mathcal{L}$, with respect to this basis has the following form
		%	\[L= \begin{pmatrix}
			%		l_{11} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
			%		0 & l_{22} & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
			%		0 & 0 & l_{33} & 0 & 0 & 0 & 0 & 0 & 0 \\
			%		0 & 0 & 0 & l_{44} & 0 & 0 & 0 & 0 & 0 \\
			%		0 & 0 & 0 & 0 & l_{55} & 0 & 0 & 0 & 0 \\
			%		0 & 0 & 0 & 0 & 0 & l_{66} & 0 & 0 & 0 \\
			%		0 & 0 & 0 & 0 & 0 & 0 & l_{77} & 0 & 0 \\
			%		0 & 0 & 0 & 0 & 0 & 0 & 0 & l_{88} & 0 \\
			%		0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & l_{99}
			%	\end{pmatrix}.\]
		Consider the following rank 1 matrix
		\[J=\begin{pmatrix}
			1 & 1 & 1 \\
			1 & 1 & 1 \\
			1 & 1 & 1
		\end{pmatrix}_{3\times3}\in \mathcal{SR}_2\implies \mathcal{L}(J)=\begin{pmatrix}
			l_{1} & l_{4} & l_{5} \\
			l_{6} & l_{2} & l_{7} \\
			l_{8} & l_{9} & l_{3} 
		\end{pmatrix}\in \mathcal{SR}_2.\]
		Next, we claim that the rank of $\mathcal{L}(J)$ is 1. If $\mathcal{L}(J)$ is not a rank 1 matrix, then it has at least one non-zero minor of size $2\times2$ and further all of these minors are of the same sign, say non-negative without loss of generality. 
		
		For $c>1$, let
		\[J(c):=\begin{pmatrix}
			1 & 1 & c \\
			1 & 1 & 1 \\
			1 & 1 & 1
		\end{pmatrix}\in \mathcal{SR}_2\implies \mathcal{L}(J(c))=\begin{pmatrix}
			l_{1} & l_{4} & cl_{5} \\
			l_{6} & l_{2} & l_{7} \\
			l_{8} & l_{9} & l_{3} 
		\end{pmatrix}\in \mathcal{SR}_2.\]
		But we can choose $c$ sufficiently large such that $l_{4}l_{7}-cl_{5}l_{2}<0.$ Thus, we must have that the first two columns of $\mathcal{L}(J(c))$ and hence of $\mathcal{L}(J)$ are linearly dependent. 
		
		Again for $c^{\prime}>1$, let
		\[J(c^{\prime}):=\begin{pmatrix}
			1 & 1 & 1 \\
			1 & 1 & 1 \\
			c^{\prime} & 1 & 1
		\end{pmatrix}\in \mathcal{SR}_2\implies \mathcal{L}(J(c^{\prime}))=\begin{pmatrix}
			l_{1} & l_{4} & l_{5} \\
			l_{6} & l_{2} & l_{7} \\
			c^{\prime}l_{8} & l_{9} & l_{3} 
		\end{pmatrix}\in \mathcal{SR}_2.\]
		Hence $l_{6}l_{9}-c^{\prime}l_{2}l_{8}<0$ for $c^{\prime}$ large enough. Thus, the last two columns of $\mathcal{L}(J)$ are linearly dependent. Hence, $\mathcal{L}(J)$ is a rank 1 matrix. 
		
		Now, let 
		\[B=\begin{pmatrix}
			b_{11} & b_{12} & b_{13} \\
			b_{21} & b_{22} & b_{23} \\
			b_{31} & b_{32} & b_{33}
		\end{pmatrix}\in \mathcal{SR}_2 \implies \mathcal{L}(B)=\begin{pmatrix}
			l_{1}b_{11} & l_{4}b_{12} & l_{5}b_{13} \\
			l_{6}b_{21} & l_{2}b_{22} & l_{7}b_{23} \\
			l_{8}b_{31} & l_{9}b_{32} & l_{3}b_{33}
		\end{pmatrix}\in \mathcal{SR}_2.\] 
		Since rank of $\mathcal{L}(J)$ is 1, we can write $\mathcal{L}(B)$ as 
		\[\mathcal{L}(B)=\begin{pmatrix}
			l_{1} & 0 & 0 \\
			0 & l_{6} & 0 \\
			0 & 0 & l_{8}
		\end{pmatrix}\begin{pmatrix}
			b_{11} & b_{12} & b_{13} \\
			b_{21} & b_{22} & b_{23} \\
			b_{31} & b_{32} & b_{33}
		\end{pmatrix}\begin{pmatrix}
			1 & 0 & 0 \\
			0 & l_{4}/l_{1} & 0 \\
			0 & 0 & l_{7}/l_{6}
		\end{pmatrix}\] 
		which is a positive diagonal equivalence, i.e., $\mathcal{L}$ maps $B\mapsto FBE$ where $F=\begin{pmatrix}
			l_{1} & 0 & 0 \\
			0 & l_{6} & 0 \\
			0 & 0 & l_{8}
		\end{pmatrix}$ and $E=\begin{pmatrix}
			1 & 0 & 0 \\
			0 & l_{4}/l_{1} & 0 \\
			0 & 0 & l_{7}/l_{6}
		\end{pmatrix}$. This completes the proof for the base case. \smallskip
		
		For the induction step, let $\mathcal{L}:\mathbb{R}^{n\times n}\to\mathbb{R}^{n\times n}$ with $n>3$ be a linear $\mathcal{SR}_2$-preserver. Let $A\in \mathcal{SR}_2$. By Proposition \ref{SR_square_firstLast_rowTOrow_colTOcol}(i), the leading principal submatrix of $A$ of size $(n-1)\times (n-1)$ must be transformed to the leading principal submatrix of size $(n-1)\times (n-1)$ of $\mathcal{L}(A)$. Since every SR$_2$ matrix $\widehat{A}$ of size $(n-1)\times(n-1)$ is a leading principal submatrix of the SR$_2$ matrix $\widehat{A}\oplus\{0\}$ of size $n\times n$, the natural restriction of $\mathcal{L}$ onto the leading principal submatrix of size $(n-1)\times(n-1)$ is a linear $\mathcal{SR}_2$-preserver on $\mathbb{R}^{(n-1)\times(n-1)}$. By the induction hypothesis, the restriction of $\mathcal{L}$ is a composition of one or more of the following maps: (i) $X\mapsto -X$, (ii) $X\mapsto XP$, (iii) $X\mapsto PX$, (iv) $X\mapsto X^T$, and (v) $X\mapsto FXE$. In fact, it is a positive diagonal equivalence since the first row is transformed to the first row and $\epsilon_1(\mathcal{L}(E_{ij}))=1$ for all $1\leq i,j\leq n$. Thus,
		\[S_{ij}=\{E_{ij}\}\hspace{.2cm}\mathrm{for}\hspace{.2cm}\mathrm{all}\hspace{.1cm} 1\leq i,j\leq n-1.\] 
		By Proposition \ref{SR_square_firstLast_rowTOrow_colTOcol}, $\mathcal{L}$ maps all rows and columns of its arguments entirely to some row and column, respectively, and hence 
		\begin{equation}\label{SR_sqaure_Sij=Eij}
			S_{ij}=\{E_{ij}\}\hspace{.2cm}\mathrm{for}\hspace{.2cm}\mathrm{all}\hspace{.1cm} 1\leq i,j\leq n.
		\end{equation} 
		Since we may compose the inverse positive diagonal equivalence relative to the leading principal submatrix of size $(n-1)\times(n-1)$ with $\mathcal{L}$, we may assume without loss of generality, that \[\mathcal{L}(A)\begin{pmatrix}
			1,\ldots,n-1 \\
			1,\ldots,n-1
		\end{pmatrix}=A\begin{pmatrix}
			1,\ldots,n-1 \\
			1,\ldots,n-1
		\end{pmatrix}.\] 
		By using \eqref{SR_sqaure_Sij=Eij}, we have
		\[\mathcal{L}(E_{in})=c_iE_{in},\hspace{.1cm} \mathcal{L}(E_{ni})=k_iE_{ni} \hspace{.2cm}\mathrm{for}\hspace{.2cm} 1\leq i\leq n-1, \hspace{.2cm}\mathrm{and}\hspace{.2cm} \mathcal{L}(E_{nn})=dE_{nn}\]
		for some positive scalars $c_i$, $k_i$, and $d$.
		We next claim that
		\[c_1=\cdots=c_{n-1},\hspace{.1cm} k_1=\cdots=k_{n-1},\hspace{.2cm} \mathrm{and} \hspace{.2cm} d=c_1k_1.\]
		Consider the following rank 1 matrix 
		\[J=\begin{pmatrix}
			1 & \ldots & 1 & 1 \\
			\vdots & \ddots & \vdots & \vdots \\
			1 & \ldots & 1 & 1 \\
			1 & \ldots & 1 & 1 \\
		\end{pmatrix}_{n\times n}\in \mathcal{SR}_2\implies\mathcal{L}(J)=\begin{pmatrix}
			1 & \ldots & 1 & c_1 \\
			\vdots & \ddots & \vdots & \vdots \\
			1 & \ldots & 1 & c_{n-1} \\
			k_1 & \ldots & k_{n-1} & d \\
		\end{pmatrix}\in \mathcal{SR}_2. \]
		We can show that $\mathcal{L}(J)$ is a rank 1 matrix, similar to the case when $n=3$. Hence, all $2\times2$ minors of $\mathcal{L}(J)$ are zero. Thus
		\[c_1=\cdots=c_{n-1},\hspace{.1cm} k_1=\cdots=k_{n-1},\hspace{.2cm} \mathrm{and} \hspace{.2cm} d=c_1k_1.\]
		Hence, $\mathcal{L}$ maps $A$ to $FAE$ for some positive diagonal matrices $F$ and $E$. This concludes the induction step and the proof.
	\end{proof}
To complement Theorem \ref{D}, we now classify all linear $\mathcal{SR}$-preservers on $\mathbb{R}^{2\times 2}$.
\begin{theorem}\label{thrmSR_1}
	Let $\mathcal{L}:\mathbb{R}^{2\times 2}\to\mathbb{R}^{2\times 2}$  be a linear transformation. Then the following statements are equivalent.
	\begin{itemize}
		\item[(1)] $\mathcal{L}$ maps the class of SR matrices onto itself.
		\item[(2)] $\mathcal{L}$ maps the class of SR$_1$ matrices onto itself.
		\item[(3)] $\mathcal{L}$ is a composition of one or more of the following types of transformations:
		\begin{itemize}
			\item[(a)] $A\mapsto H\circ A$, where $H$ is an entrywise positive matrix;
			\item[(b)] $A\mapsto -A$;
			\item[(c)] $A\mapsto P_2A$, in which $P_2$ is an exchange matrix;
			\item[(d)] $A\mapsto AP_2$;
			\item[(e)] $A\mapsto A^T$;
			\item[(f)] $\begin{pmatrix}
				a_{11} & a_{12} \\
				a_{21} & a_{22}
			\end{pmatrix}\mapsto\begin{pmatrix}
				a_{11} & a_{21} \\
				a_{22} & a_{12}
			\end{pmatrix}$; and
			\item[(g)] $\begin{pmatrix}
				a_{11} & a_{12} \\
				a_{21} & a_{22}
			\end{pmatrix}\mapsto\begin{pmatrix}
				a_{11} & a_{12} \\
				a_{22} & a_{21}
			\end{pmatrix}$.
		\end{itemize}
	\end{itemize}
	%Moreover, the theorem is also true if SR is replaced by SSR.
\end{theorem}
\begin{proof}
	$(1)\implies(2)$ and $(3)\implies(1)$ are trivial. It only remains to show $(2)\implies(3)$. Let $\mathcal{L}:\mathbb{R}^{2\times2}\to\mathbb{R}^{2\times2}$ be a linear map such that $\mathcal{L}(\mathcal{SR}_1)=\mathcal{SR}_1$ and let $\mathfrak{B}=\{E_{11},E_{22},E_{12},E_{21}\}$ be an ordered basis of $\mathbb{R}^{2\times2}$.  Since $E_{ij}\in\mathcal{SR}_1$ for all $1\leq i,j\leq2$ and $\mathcal{L}$ maps $\mathcal{SR}_1$ onto itself, therefore $\mathcal{L}^{-1}$ exists, and further $\mathcal{L}^{-1}\in P(\mathcal{SR}_1)$.  Notice that the proof of Proposition \ref{SR_square_image_same_epsilon_1_sign} holds for $\mathcal{L}\in P(\mathcal{SR}_1)$. Thus $\epsilon_1(\mathcal{L}(E_{ij}))$ has the same sign for all $i,j$. We can assume without loss of generality that $\epsilon_1(\mathcal{L}(E_{ij}))=1$ for all $i,j$ since $\mathcal{L}$ is a linear $\mathcal{SR}_1$-preserver if and only if $\mathcal{L}$ composed with the map $A\mapsto-A$ is a linear $\mathcal{SR}_1$-preserver. By Proposition \ref{SR_square_monomial} and Remark \ref{Sij_singleton}, $S_{ij}$ is a non-empty singleton set for all $1\leq i,j\leq 2$ and $S_{ij}\cap S_{kl}=\emptyset$ for $(i,j)\neq(k,l)$.  
	%  Note that 
	%  
	%  Let $L\in\mathbb{R}^{4\times4}$ be the matrix representation of $\mathcal{L}$ with respect to the above basis $\mathfrak{B}$. Then $L\geq0$. As $\mathcal{L}^{-1}\in P(\mathcal{SR}_1)$, thus $\epsilon_1(\mathcal{L}^{-1}(E_{ij}))$ have the same sign for all $i,j$. More precisely, either $L^{-1}\geq0$ or $L^{-1}\leq0$. As $L\geq0$, $LL^{-1}=I$, we must have that $L^{-1}\geq0$, and hence $L$ is a generalized permutation matrix.
	
	Thus the element in each of the sets $S_{11},$ $S_{12},$ $S_{21}$, and $S_{22}$ must be among the following:
	\[E_{11}, E_{12}, E_{21},\hspace{.1cm}\mathrm{and}\hspace{.1cm} E_{22}.\]
	Thus, there are twenty-four possible combinations of the sets $S_{11},$ $S_{12},$ $S_{21}$, and $S_{22}$. They are listed in the below table. %\newpage
	\begin{center}
		\begin{tabular}{ ||c|c|c|c|c|c||} 
			\hline\hline
			S.No. & $S_{11}$ & $S_{12}$ & $S_{21}$ & $S_{22}$ & $\mathcal{SR}_1$-preservers \\ 
			\hline\hline
			I. & $E_{11}$  &  $E_{12}$  &  $E_{21}$  &  $E_{22}$  & - \\
			& $E_{11}$  &  $E_{21}$  &  $E_{12}$  &  $E_{22}$  & $A\mapsto A^T$ \\
			& $E_{12}$  &  $E_{11}$  &  $E_{22}$  &  $E_{21}$  & $A\mapsto AP_2$  \\
			& $E_{12}$  &  $E_{22}$  &  $E_{11}$  &  $E_{21}$  & $A\mapsto AP_2\mapsto (AP_2)^T$ \\
			& $E_{21}$  &  $E_{11}$  &  $E_{22}$  &  $E_{12}$  & $A\mapsto P_2A\mapsto (P_2A)^T$ \\
			& $E_{21}$  &  $E_{22}$  &  $E_{11}$  &  $E_{12}$  & $A\mapsto P_2A$ \\
			& $E_{22}$  &  $E_{12}$  &  $E_{21}$  &  $E_{11}$  & $A\mapsto P_2AP_2\mapsto (P_2AP_2)^T$ \\
			& $E_{22}$  &  $E_{21}$  &  $E_{12}$  &  $E_{11}$  & $A\mapsto P_2AP_2$ \\
			\hline 
			II. & $E_{11}$  &  $E_{12}$  &  $E_{22}$  &  $E_{21}$  & - \\
			& $E_{11}$  &  $E_{21}$  &  $E_{22}$  &  $E_{12}$  & $A\mapsto A^T$ \\
			& $E_{12}$  &  $E_{11}$  &  $E_{21}$  &  $E_{22}$  & $A\mapsto AP_2$ \\
			& $E_{12}$  &  $E_{22}$  &  $E_{21}$  &  $E_{11}$  & $A\mapsto AP_2\mapsto (AP_2)^T$ \\
			& $E_{21}$  &  $E_{11}$  &  $E_{12}$  &  $E_{22}$  & $A\mapsto P_2A\mapsto (P_2A)^T$ \\
			& $E_{21}$  &  $E_{22}$  &  $E_{12}$  &  $E_{11}$  & $A\mapsto P_2A$ \\
			& $E_{22}$  &  $E_{12}$  &  $E_{11}$  &  $E_{21}$  & $A\mapsto P_2AP_2\mapsto (P_2AP_2)^T$  \\
			& $E_{22}$  &  $E_{21}$  &  $E_{11}$  &  $E_{12}$  & $A\mapsto P_2AP_2$  \\
			\hline
			III. & $E_{11}$  &  $E_{22}$  &  $E_{12}$  &  $E_{21}$  & - \\
			& $E_{11}$  &  $E_{22}$  &  $E_{21}$  &  $E_{12}$  & $A\mapsto A^T$  \\
			& $E_{12}$  &  $E_{21}$  &  $E_{11}$  &  $E_{22}$  & $A\mapsto AP_2$  \\
			& $E_{12}$  &  $E_{21}$  &  $E_{22}$  &  $E_{11}$  & $A\mapsto AP_2\mapsto (AP_2)^T$ \\
			& $E_{21}$  &  $E_{12}$  &  $E_{11}$  &  $E_{22}$  & $A\mapsto P_2A\mapsto (P_2A)^T$  \\
			& $E_{21}$  &  $E_{12}$  &  $E_{22}$  &  $E_{11}$  & $A\mapsto P_2A$  \\
			& $E_{22}$  &  $E_{11}$  &  $E_{12}$  &  $E_{21}$  & $A\mapsto P_2AP_2\mapsto (P_2AP_2)^T$  \\
			& $E_{22}$  &  $E_{11}$  &  $E_{21}$  &  $E_{12}$  & $A\mapsto P_2AP_2$ \\
			\hline\hline
		\end{tabular}
	\end{center}
	In the last column of the above table, we have listed the $\mathcal{SR}_1$-preservers that we have used to assume without loss of generality:
	\begin{equation}\label{SR_1preserver-1}
		\mathrm{I.}\hspace{.2cm} S_{11}=\{E_{11}\}, S_{12}=\{E_{12}\}, S_{21}=\{E_{21}\}, \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{22}=\{E_{22}\}; \end{equation}
	\begin{equation}\label{SR_1preserver-2}
		\mathrm{II.}\hspace{.2cm} S_{11}=\{E_{11}\}, S_{12}=\{E_{12}\}, S_{21}=\{E_{22}\}, \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{22}=\{E_{21}\}; \end{equation}
	\begin{equation}\label{SR_1preserver-3}
		\mathrm{III.}\hspace{.2cm} S_{11}=\{E_{11}\}, S_{12}=\{E_{22}\}, S_{21}=\{E_{12}\}, \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{22}=\{E_{21}\}. \end{equation}
	%  In the last column of the above table, we have listed $\mathcal{SR}_1$-preservers which are used to assume without loss of generality, that \[S_{11}=\{E_{11}\}, S_{12}=\{E_{12}\}, S_{21}=\{E_{21}\}, \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{22}=\{E_{22}\}\]
	%  since $\mathcal{L}\in P(\mathcal{SR}_1)$ if and only if $\mathcal{L}$ composed with the maps listed in the last column of the above table is a linear $\mathcal{SR}_1$-preserver.
	Further, using the maps $\begin{pmatrix}
		a_{11} & a_{12} \\
		a_{21} & a_{22}
	\end{pmatrix}\mapsto\begin{pmatrix}
		a_{11} & a_{12} \\
		a_{22} & a_{21}
	\end{pmatrix}$ in \eqref{SR_1preserver-2} and $\begin{pmatrix}
		a_{11} & a_{12} \\
		a_{21} & a_{22}
	\end{pmatrix}\mapsto\begin{pmatrix}
		a_{11} & a_{21} \\
		a_{22} & a_{12}
	\end{pmatrix}$ in \eqref{SR_1preserver-3}, we assume \eqref{SR_1preserver-1} without loss of generality, since $\mathcal{L}\in P(\mathcal{SR}_1)$ if and only if $\mathcal{L}$ composed with the above two maps is a linear $\mathcal{SR}_1$-preserver.
	
	Let $\mathcal{L}(E_{11})=l_1E_{11}$, $\mathcal{L}(E_{22})=l_2E_{22}$, $\mathcal{L}(E_{12})=l_3E_{12}$, and $\mathcal{L}(E_{21})=l_4E_{21}$, where $l_i>0$ for all $1\leq i\leq4$ as $\epsilon_1(\mathcal{L}(E_{ij}))=1$ for all $i,j$. Thus for any $2\times2$ matrix $B=\begin{pmatrix}
		b_{11} & b_{12} \\
		b_{21} & b_{22}
	\end{pmatrix}\in\mathcal{SR}_1$, $\mathcal{L}(B)=\begin{pmatrix}
		l_1b_{11} & l_3b_{12} \\
		l_4b_{21} & l_2b_{22}
	\end{pmatrix}=H\circ B$, where $H=\begin{pmatrix}
		l_1 & l_3 \\
		l_4 & l_2
	\end{pmatrix}$. Therefore, the linear $\mathcal{SR}_1$-preservers on $\mathbb{R}^{2\times2}$ are composition of one or more of the following maps: \smallskip 
	
	(i) $A\mapsto H\circ A$, where $H\in \mathbb{R}^{2\times 2}$ is entrywise positive, (ii) $A\mapsto-A$, (iii) $A\mapsto A^T$, (iv) $A\mapsto AP_2$, (v) $A\mapsto P_2A$, (vi) $\begin{pmatrix}
		a_{11} & a_{12} \\
		a_{21} & a_{22}
	\end{pmatrix}\mapsto\begin{pmatrix}
		a_{11} & a_{12} \\
		a_{22} & a_{21}
	\end{pmatrix}$, and
	(vii) $\begin{pmatrix}
		a_{11} & a_{12} \\
		a_{21} & a_{22}
	\end{pmatrix}\mapsto\begin{pmatrix}
		a_{11} & a_{21} \\
		a_{22} & a_{12}
	\end{pmatrix}$.
\end{proof}

	Next, we prove Theorem \ref{D} for $m>n\geq2$ using the propositions stated at the beginning of this section.
	%%%%%%%%%%%%%%%%%%%% Proof of Theorem D %%%%%%%%%%%%%%%%%%%%
	\begin{proof}[Proof of Theorem \ref{D} for $m\neq n$] 
		Let $m>n\geq2$ and $\mathcal{L}:\mathbb{R}^{m\times n}\to\mathbb{R}^{m\times n}$ be a linear transformation such that $\mathcal{L}$ maps $\mathcal{SR}_2$ onto itself. We claim that $\mathcal{L}$ is a composition of one or more of the transformations listed in Theorem \ref{D}(3). For convenience, again we split the rest of the proof into several propositions. The statement of these propositions are similar to the case $m=n\geq3$, but the proofs differ for the $m>n\geq2$ case.
		\begin{prop}\label{SR_rectangle_S11SmnS1nSm1_elements}
			For $\mathcal{L}\in P(\mathcal{SR}_2)$, the element in each of the sets $S_{11}$, $S_{mn}$, $S_{1n}$, and $S_{m1}$ must be either of the following:
			\[E_{11},\hspace{.1cm}E_{mn},\hspace{.1cm}E_{1n},\hspace{.1cm}\mathrm{or}\hspace{.1cm}E_{m1.}\]
		\end{prop}
		\begin{proof}
			We prove this by contradiction. Suppose that \[S_{11}\neq\{E_{11}\}, \{E_{mn}\}, \{E_{1n}\}, \{E_{m1}\}.\] 
			First, we consider the case $n=2$. Let
			\[J(c):=\begin{pmatrix}
				c & 1 \\
				1 & 1 \\
				\vdots & \vdots \\
				1 & 1
			\end{pmatrix}_{m\times2}\in\mathcal{SR}_2\implies\mathrm{either}\hspace{.2cm} \mathcal{L}(J(c))=\begin{pmatrix}
				y_{11} & y_{12} \\
				\vdots & \vdots \\
				cy_{i1} & y_{i2} \\
				\vdots & \vdots \\
				y_{m1} & y_{m2}
			\end{pmatrix}\hspace{.2cm}\mathrm{or}\hspace{.2cm} \mathcal{L}(J(c))=\begin{pmatrix}
				y_{11} & y_{12} \\
				\vdots & \vdots \\
				y_{j1} & cy_{j2} \\
				\vdots & \vdots \\
				y_{m1} & y_{m2}
			\end{pmatrix}.\]
			Since $m>2$, we can find two minors of size $2\times2$ of $\mathcal{L}(J(c))$ which are of opposite signs for large $c$, a contradiction. Now, for $m>n\geq3$, the same arguments that we have used in Proposition \ref{SR_square_S11SnnS1nSn1_elements} can be used here. 
			
			Similarly, we can show this for each of $S_{mn}$, $S_{1n}$, and $S_{m1}$ by multiplying the $(m,n)$, $(1,n)$, and $(m,1)$ entries, respectively of $J=J_{m\times n}$ by $c>0$.
		\end{proof}
		\begin{prop} \label{SR_rectangle_S11Smn_S1nSm1_combo} 
			For $\mathcal{L}\in P(\mathcal{SR}_2)$, the following pairwise combinations are possible.
			\begin{itemize}
				\item[(i)] $S_{11}=\{E_{11}\} \hspace{.2cm}\mathrm{and}\hspace{.2cm} S_{mn}=\{E_{mn}\}, \hspace{.2cm}\mathrm{or}\hspace{.2cm} S_{11}=\{E_{mn}\} \hspace{.2cm}\mathrm{and}\hspace{.2cm} S_{mn}=\{E_{11}\}, \hspace{.2cm}\mathrm{or}$ \\
				$S_{11}=\{E_{1n}\} \hspace{.2cm}\mathrm{and}\hspace{.2cm} S_{mn}=\{E_{m1}\}, \hspace{.2cm}\mathrm{or}\hspace{.2cm} S_{11}=\{E_{m1}\} \hspace{.2cm}\mathrm{and}\hspace{.2cm} S_{mn}=\{E_{1n}\}.$
				\item[(ii)] $S_{1n}=\{E_{1n}\} \hspace{.2cm}\mathrm{and}\hspace{.2cm} S_{m1}=\{E_{m1}\},\hspace{.2cm}\mathrm{or}\hspace{.2cm} S_{1n}=\{E_{m1}\} \hspace{.2cm}\mathrm{and}\hspace{.2cm} S_{m1}=\{E_{1n}\}, \hspace{.2cm}\mathrm{or}$ \\
				$S_{1n}=\{E_{11}\} \hspace{.2cm}\mathrm{and}\hspace{.2cm} S_{m1}=\{E_{mn}\},\hspace{.2cm}\mathrm{or}\hspace{.2cm} S_{1n}=\{E_{mn}\} \hspace{.2cm}\mathrm{and}\hspace{.2cm} S_{m1}=\{E_{11}\}.$
			\end{itemize}
		\end{prop}
		\begin{proof} First, we prove (i). The proof is similar to that of Proposition \ref{SR_square_S11Snn_S1nSn1_combo}, except for the case when $n=2$. From Proposition \ref{SR_rectangle_S11SmnS1nSm1_elements}, we have either  
			\[S_{11}=\{E_{11}\} \hspace{.1cm}\mathrm{or}\hspace{.1cm} S_{11}=\{E_{mn}\} \hspace{.1cm}\mathrm{or}\hspace{.1cm} S_{11}=\{E_{m1}\} \hspace{.1cm}\mathrm{or}\hspace{.1cm} S_{11}=\{E_{1n}\}\] and
			\[ S_{mn}=\{E_{11}\} \hspace{.1cm}\mathrm{or}\hspace{.1cm} S_{mn}=\{E_{mn}\} \hspace{.1cm}\mathrm{or}\hspace{.1cm} S_{mn}=\{E_{m1}\} \hspace{.1cm}\mathrm{or}\hspace{.1cm} S_{mn}=\{E_{1n}\}.\]
			Since $L$ is a  monomial matrix, out of sixteen possible combinations of $S_{11}$ and $S_{mn}$, the following four are discarded straightaway:
			\[S_{11}=S_{mn}=\{E_{11}\},\hspace{.1cm} S_{11}=S_{mn}=\{E_{mn}\}, \hspace{.1cm} S_{11}=S_{mn}=\{E_{m1}\}, \hspace{.1cm}\mathrm{and}\hspace{.1cm}  S_{11}=S_{mn}=\{E_{1n}\}.\] 
			We next show that the following eight other combinations of $S_{11}$ and $S_{mn}$ below are also not possible:
			\[S_{11}=\{E_{m1}\} \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{mn}=\{E_{mn}\}, \quad 
			S_{11}=\{E_{11}\} \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{mn}=\{E_{1n}\},\] 
			\[S_{11}=\{E_{mn}\} \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{mn}=\{E_{m1}\}, \quad 
			S_{11}=\{E_{mn}\} \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{mn}=\{E_{1n}\}, \]
			\[S_{11}=\{E_{m1}\} \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{mn}=\{E_{11}\}, \quad
			S_{11}=\{E_{11}\} \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{mn}=\{E_{m1}\},\] 
			\[S_{11}=\{E_{1n}\} \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{mn}=\{E_{11}\}, \quad
			S_{11}=\{E_{1n}\} \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{mn}=\{E_{mn}\}.\]
			Suppose $S_{11}=\{E_{m1}\}$ and $S_{mn}=\{E_{mn}\}$. Let $n=2$ and for $c>0$, define
			\[J(c):=\begin{pmatrix}
				1+c & c \\
				1   & 1 \\
				\vdots & \vdots \\
				1   & 1
			\end{pmatrix}_{m\times2}\in \mathcal{SR}\implies \mathcal{L}(J(c))\in \mathcal{SR}.\]
			By Propositions \ref{SR_square_monomial} and \ref{SR_rectangle_S11SmnS1nSm1_elements}, $\mathcal{L}(J(c))$ can be either of the following
			\[\begin{pmatrix}
				y_{11} & cy_{12} \\
				y_{21} & y_{22} \\
				\vdots & \vdots \\
				(1+c)y_{m1} & y_{m2}
			\end{pmatrix}\hspace{0.1cm}\mathrm{or}\hspace{0.1cm}\begin{pmatrix}
				cy_{11} & y_{12} \\
				y_{21} & y_{22} \\
				\vdots & \vdots \\
				(1+c)y_{m1} & y_{m2}
			\end{pmatrix}.\]
			For the first case, using the fact $\mathcal{L}(J)$ is a rank 1 matrix, the following two minors of size $2\times2$ of $Y(c)$ have opposite signs for appropriate choice of c,
			\[\det\begin{pmatrix}
				y_{11} & cy_{12} \\
				y_{21} & y_{22} 
			\end{pmatrix}>0\hspace{.2cm}\mathrm{and}\hspace{.2cm}
			\det\begin{pmatrix}
				y_{21} & y_{22} \\
				(1+c)y_{m1} & y_{m2} 
			\end{pmatrix}<0.\] 
			
			For the other case, we can choose $c$ large enough such that we obtain two minors of size $2\times2$ of $Y(c)$ having opposite signs. Hence, we conclude that $S_{11}\neq\{E_{m1}\}$ and $S_{mn}\neq\{E_{mn}\}$ for $n=2$. Now, the proof for $m>n\geq3$ follows similarly to the proof of Proposition \ref{SR_square_S11Snn_S1nSn1_combo}. \smallskip
			
			Using a similar argument as in the above proof shows that (ii) holds.
		\end{proof}
		\begin{rem}
			In Proposition \ref{SR_rectangle_S11Smn_S1nSm1_combo}(i), we can assume without loss of generality that
			\begin{equation}\label{SR_rectangle_S11Smn_wlog}
				S_{11}=\{E_{11}\}\hspace{.1cm}\mathrm{and}\hspace{.1cm}S_{mn}=\{E_{mn}\}
			\end{equation}
			since $\mathcal{L}\in P(\mathcal{SR})$ if and only if $\mathcal{L}$ composed with the maps $A\mapsto AP_n$ and $A\mapsto P_mA$ is a linear $\mathcal{SR}$-preserver. 
			%Since our domain is composed of SR$_2$ matrices with all possible $\epsilon$, and $\mathcal{L}$ is preserving SR$_2$ matrices with any given $\epsilon$, thus we must take the union of the restrictions obtained in
			Now, using Proposition \ref{SR_square_monomial} and \eqref{SR_rectangle_S11Smn_wlog} in  Proposition \ref{SR_rectangle_S11Smn_S1nSm1_combo}(ii), we have either
			\[S_{11}=\{E_{11}\},\hspace{.1cm} S_{mn}=\{E_{mn}\},\hspace{.1cm} S_{1n}=\{E_{1n}\}, \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{m1}=\{E_{m1}\}\hspace{.2cm}\mathrm{or}\] 
			\[S_{11}=\{E_{11}\},\hspace{.1cm} S_{mn}=\{E_{mn}\},\hspace{.1cm} S_{1n}=\{E_{m1}\}, \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{m1}=\{E_{1n}\}.\] 
		\end{rem}
		Next, we show that \[S_{11}=\{E_{11}\},\hspace{.1cm} S_{mn}=\{E_{mn}\},\hspace{.1cm} S_{1n}=\{E_{m1}\}, \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{m1}=\{E_{1n}\}\] does not hold. To the contrary, suppose it holds. Let $J(c)$ be the matrix obtained by multiplying the first row of $J=J_{m\times n}$ by $c>0$. Then $J(c)\in \mathcal{SR}_2$ and so $\mathcal{L}(J(c)):=Y(c)\in \mathcal{SR}_2$. Since $m>n$ and hence even if $\mathcal{L}$ maps entire elements of the first row of $J(c)$ to the first column of $Y(c)$, there exists $1<j<m$ such that the $(j,1)$ position of $Y(c)$ is not occupied by the image of any element from the first row of $J(c)$. Now, consider the following two minors of size $2\times2$ of $Y(c)$ included in
		\begin{itemize}
			\item rows $1$ and $j$ and columns $1$ and $n$: 
			$\det\begin{pmatrix}
				cy_{11} & y_{1n} \\
				y_{j1} & \alpha y_{jn}
			\end{pmatrix}$, and
			\item rows $j$ and $m$ and columns $1$ and $n$: 
			$\det\begin{pmatrix}
				y_{j1} & \alpha y_{jn} \\
				cy_{m1} & y_{mn}
			\end{pmatrix}$, 
		\end{itemize}
		where either $\alpha=c$ or $\alpha=1$. Now, it is always possible to choose $c$ large enough such that the above two minors of $Y(c)$ are of opposite signs which is a contradiction. Thus, we have
		\begin{equation} \label{SR_rectangle_S11SmnS1nSm1_wlog}
			S_{11}=\{E_{11}\},\hspace{.1cm} S_{mn}=\{E_{mn}\},\hspace{.1cm} S_{1n}=\{E_{1n}\}, \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{m1}=\{E_{m1}\}. 
		\end{equation}
		\begin{prop} \label{SR_rectangle_FirstLast_rowTorow_colTOcol}
			Let $\mathcal{L}\in P(\mathcal{SR}_2)$ such that \eqref{SR_rectangle_S11SmnS1nSm1_wlog} holds.
			\begin{itemize}
				\item[(i)] Then $\mathcal{L}$ must map the first (and last) row and column of its arguments entirely to the first (and last) row and column, respectively. 
				\item[(ii)] Moreover, $\mathcal{L}$ must map all rows and columns of its arguments entirely to some row and column, respectively.
			\end{itemize}
		\end{prop}	
		\begin{proof}
			We begin by showing (i). We will first show that $\mathcal{L}$ must map the first (and last) row of its arguments entirely to the first (and last) row. For $n=2$, this follows directly from \eqref{SR_rectangle_S11SmnS1nSm1_wlog}. For $m>n\geq3$, the proof is similar to that of Proposition \ref{SR_square_firstLast_rowTOrow_colTOcol}.
			
			Our next aim is to prove that $\mathcal{L}$ maps the entire first column of its arguments to the first column. Let $J(c)$ be the matrix obtained by multiplying the first column of $J=J_{m\times n}$ by $c>0$. Assume to the contrary that there exists $k$ where $1<k<m$ such that the $(k,1)$ position of the matrix $Y(c)$ is not occupied by the image of any element from the first column of $J(c)$. Since $\mathcal{L}$ maps the entire first and last row of its arguments to the first and the last row, respectively, the following two minors of size $2\times2$ of $Y(c)$ give us a contradiction:
			\begin{itemize}
				\item rows $1$ and $k$ and columns $1$ and $n$: 
				$\det\begin{pmatrix}
					cy_{11} & y_{1n} \\
					y_{k1} & \alpha y_{kn}
				\end{pmatrix}$, and
				\item rows $k$ and $m$ and columns $1$ and $n$: 
				$\det\begin{pmatrix}
					y_{k1} & \alpha y_{kn} \\
					cy_{m1} & y_{mn}
				\end{pmatrix}$,
			\end{itemize} where either $\alpha=c$ or $\alpha=1$. 
		
		    We can prove similarly the corresponding assertion for the last column as well. This shows (i).
			Now we will show (ii). The proof is similar to Proposition \ref{SR_square_firstLast_rowTOrow_colTOcol}(ii), except for the case when $n=2$. By Proposition \ref{SR_rectangle_FirstLast_rowTorow_colTOcol}(i), this holds trivially for columns. Next we claim that $\mathcal{L}$ maps every row of its argument to some row. Let $J(c)$ be the $m\times 2$ matrix obtained by multiplying the $k^{th}$ row of $J=J_{m\times 2}$ by $c>0$ where $1<k<m$. By Proposition \ref{SR_rectangle_FirstLast_rowTorow_colTOcol}(i), $\mathcal{L}$ maps the entire first (and last) column of its arguments to the first (and last) column and hence the $(k,1)$ element of $J(c)$ will be mapped to the first column, let us say $\mathcal{L}$ maps it to the $(p,1)$ position of $Y(c)$. To the contrary assume that $\mathcal{L}$ does not map the entire $k^{th}$ row of its argument to some row, then the $(p,2)$ position of $Y(c)$ is not occupied by the image of element of $J(c)$ present in the $(k,2)$ position. Using Proposition \ref{SR_rectangle_FirstLast_rowTorow_colTOcol}(i), we obtain the following two minors of size $2\times2$ of $Y(c)$ having opposite signs for appropriate choice of $c$, included in
			\begin{itemize}
				\item rows $1$ and $p$ and columns $1$ and $2$: 
				$\det\begin{pmatrix}
					y_{11} & y_{12} \\
					cy_{p1} & y_{p2}
				\end{pmatrix}<0$, and
				\item rows $p$ and $m$ and columns $1$ and $2$: 
				$\det\begin{pmatrix}
					cy_{p1} & y_{p2} \\
					y_{m1} & y_{m2}
				\end{pmatrix}>0$.
			\end{itemize}
			This completes the proof.
		\end{proof}
		So far, we have used the transformations $A\mapsto-A$, $A\mapsto P_mA$, and $A\mapsto AP_n$ to assume that $\mathcal{L}\in P(\mathcal{SR}_2)$ has the following properties:
		\begin{enumerate}[(i)]
			\item $\epsilon_1(\mathcal{L}(E_{ij}))=1$ for all $1\leq i\leq m$, $1\leq j\leq n$.
			\item $S_{11}=\{E_{11}\},\hspace{.1cm} S_{mn}=\{E_{mn}\},\hspace{.1cm} S_{1n}=\{E_{1n}\}, \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{m1}=\{E_{m1}\}$.
			\item  $\mathcal{L}$ maps the entire first (and last) row and column of its arguments to the first (and last) row and column, respectively.
			\item $\mathcal{L}$ maps other rows and columns to some row and column, respectively.
		\end{enumerate} 
		With the above analysis in hand, we now complete the proof by induction on the sizes of the matrices. We first apply induction on $m>2$ and show that the result holds for all $m\times2$ matrices. For the base case, let $m=3$ and let $\mathcal{L}:\mathbb{R}^{3\times2}\to\mathbb{R}^{3\times2}$ be a linear map such that $\mathcal{L}(\mathcal{SR}_2)=\mathcal{SR}_2$. 
		
		Let $\mathfrak{B}=\{E_{11}, E_{22}, E_{12}, E_{21}, E_{31}, E_{32}\}$ be an ordered basis of $\mathbb{R}^{3\times 2}$. We have $S_{ij}=\{E_{ij}\}$ for all $1\leq i\leq 3, \hspace{0.1 cm} 1\leq j\leq 2$ because of Proposition \ref{SR_rectangle_FirstLast_rowTorow_colTOcol}. Thus, $\mathcal{L}(E_{11})=l_{1}E_{11},\mathcal{L}(E_{22})=l_{2}E_{22},\ldots,\mathcal{L}(E_{32})= l_{6}E_{32}\in \mathcal{SR}_2$, where $l_{i}>0$ for all $1\leq i\leq 6$ by Remark \ref{Ato-Awlog} and Proposition \ref{SR_square_monomial}. 
		%	and the matrix $L$ which represents $\mathcal{L}$ with respect to this basis has the following form
		%	\[L= \begin{pmatrix}
			%		l_{11} & 0 & 0 & 0 & 0 & 0  \\
			%		0 & l_{22} & 0 & 0 & 0 & 0  \\
			%		0 & 0 & l_{33} & 0 & 0 & 0 \\
			%		0 & 0 & 0 & l_{44} & 0 & 0  \\
			%		0 & 0 & 0 & 0 & l_{55} & 0  \\
			%		0 & 0 & 0 & 0 & 0 & l_{66} 
			%	\end{pmatrix}.\]
		Let
		\[J=\begin{pmatrix}
			1 & 1\\
			1 & 1 \\
			1 & 1
		\end{pmatrix}_{3\times2},\hspace{.2cm}\mathrm{and} \hspace{.1cm}\mathrm{claim} \hspace{.1cm}\mathrm{that}\hspace{.2cm} \mathcal{L}(J)=\begin{pmatrix}
			l_{1} & l_{3}  \\
			l_{4} & l_{2} \\
			l_{5} & l_{6}  
		\end{pmatrix}\hspace{.1cm}\mathrm{is}\hspace{.1cm}\mathrm{a}\hspace{.1cm} \mathrm{rank}\hspace{.1cm} 1\hspace{.1cm}\mathrm{matrix}.\] 
		Indeed, if rank of $\mathcal{L}(J)$ is 2, then it has at least one non-zero minor of size $2\times2$ and further all of them are of the same sign, say non-negative without loss of generality. 
		
		For $c>1$, let
		\[J(c):=\begin{pmatrix}
			1 & c \\
			1 & 1 \\
			1 & 1 
		\end{pmatrix}\in \mathcal{SR}_2 \implies \mathcal{L}(J(c))=\begin{pmatrix}
			l_{1} & cl_{3} \\
			l_{4} & l_{2}  \\
			l_{5} & l_{6}  
		\end{pmatrix}\in \mathcal{SR}_2.\] 
		But we can choose $c$ sufficiently large such that $l_{1}l_{2}-cl_{3}l_{4}<0.$ Thus, we must have $l_{4}l_{6}-l_{2}l_{5}=0$. 
		
		Again for $c^{\prime}>1$, let		
		\[J(c^{\prime}):=\begin{pmatrix}
			1 & 1 \\
			1 & 1 \\
			c^{\prime} & 1
		\end{pmatrix}\in \mathcal{SR}_2 \implies \mathcal{L}(J(c^{\prime}))=\begin{pmatrix}
			l_{1} & l_{3} \\
			l_{4} & l_{2} \\
			c^{\prime}l_{5} & l_{6}
		\end{pmatrix}\in \mathcal{SR}_2.\] 
		As above, choose $c^{\prime}$ sufficiently large such that $l_{4}l_{6}-c^{\prime}l_{2}l_{5}<0.$ Thus, we have $l_{1}l_{2}-l_{3}l_{4}=0$. So $l_{1}l_{6}-l_{3}l_{5}=0$ and hence $\mathcal{L}(J)$ is a rank 1 matrix. 
		
		Let $B=\begin{pmatrix}
			b_{11} & b_{12} \\
			b_{21} & b_{22} \\
			b_{31} & b_{32}
		\end{pmatrix}\in \mathcal{SR}_2.$ Then $\mathcal{L}(B)=\begin{pmatrix}
			l_{1}b_{11} & l_{3}b_{12} \\
			l_{4}b_{21} & l_{2}b_{22} \\
			l_{5}b_{31} & l_{6}b_{32}
		\end{pmatrix}\in \mathcal{SR}_2.$
		Since $\mathcal{L}(J)$ is a rank 1 matrix, we can write $\mathcal{L}(B)$ as 
		\[\mathcal{L}(B)=\begin{pmatrix}
			l_{1} & 0 & 0 \\
			0 & l_{4} & 0 \\
			0 & 0 & l_{5}
		\end{pmatrix}\begin{pmatrix}
			b_{11} & b_{12} \\
			b_{21} & b_{22} \\
			b_{31} & b_{32}
		\end{pmatrix}\begin{pmatrix}
			1 & 0 \\
			0 & l_{3}/l_{1}
		\end{pmatrix}\] 
		which is a positive diagonal equivalence. This completes the base case. \smallskip
		
		Strategy used for applying induction: For the induction step, we first prove it for vector space of $m\times2$ matrices assuming it to be true for that of $(m-1)\times2$ matrices, where $m>3$. After this, we again apply induction on $m\times n$ by assuming it holds for space of $m\times (n-1)$ matrices for fixed $m$, where $m>n$. 
		
		%For the induction step, we first prove it for matrices of order $m\times2$ assuming it to be true for matrices of order $(m-1)\times2$ where $m>2$. 
		Let $A\in\mathcal{SR}_2$ have size $m\times2$. By Proposition \ref{SR_rectangle_FirstLast_rowTorow_colTOcol}(i), the submatrix of $A$ formed by the first $(m-1)$ rows and both columns must be transformed to the first $(m-1)$ rows of $\mathcal{L}(A)$. Since every SR$_2$ matrix $\widehat{A}$ of size $(m-1)\times 2$ is a submatrix of the SR$_2$ matrix $(\widehat{A}^T|0)^T\in\mathbb{R}^{m\times 2}$, the natural restriction of $\mathcal{L}$ onto the leading submatrix of size $(m-1)\times 2$ is a linear $\mathcal{SR}_2$-preserver on $\mathbb{R}^{(m-1)\times 2}$. By the induction hypothesis, the restriction of $\mathcal{L}$ is a composition of one or more of the following maps: (i) $X\mapsto -X$, (ii) $X\mapsto XP$, (iii) $X\mapsto PX$, and (iv) $X\mapsto FXE$. In fact, it is a positive diagonal equivalence since the first row and column are transformed to the first row and column, respectively with $\epsilon_1(\mathcal{L}(E_{ij}))=1$ for all $i,j$. Thus
		\[S_{ij}=\{E_{ij}\} \hspace{.2cm}\mathrm{for}\hspace{.2cm}\mathrm{all}\hspace{.1cm} 1\leq i\leq m-1,\hspace{.1cm} 1\leq j\leq 2.\] 
		By Proposition \ref{SR_rectangle_FirstLast_rowTorow_colTOcol}(i), $\mathcal{L}$ maps the first (and last) column of its arguments entirely to the first (and last) column and hence
		\begin{equation}\label{SR_rectangle_Sij=Eij}
			S_{ij}=\{E_{ij}\} \hspace{.2cm}\mathrm{for}\hspace{.2cm}\mathrm{all}\hspace{.1cm} 1\leq i\leq m,\hspace{.1cm}  1\leq j\leq 2.
		\end{equation} 
		Since we may compose the inverse positive diagonal equivalence relative to the upper left submatrix of size $(m-1)\times 2$ with $\mathcal{L}$, we may assume without loss of generality, that 
		\[\mathcal{L}(A)\begin{pmatrix}
			1,\ldots,m-1 \\
			1,2
		\end{pmatrix}=A\begin{pmatrix}
			1,\ldots,m-1 \\
			1,2
		\end{pmatrix}.\] 
		Using \eqref{SR_rectangle_Sij=Eij}, we have
		\[\mathcal{L}(E_{mi})=k_iE_{mi} \hspace{.2cm}\mathrm{for}\hspace{.2cm} 1\leq i\leq 2,\]
		for some positive scalars $k_1$ and $k_2$. We next claim that $k_1=k_{2}.$ Consider the following rank 1 matrix
		\[J=\begin{pmatrix}
			1 & 1 \\
			\vdots & \vdots   \\
			1 & 1 & \\
			1 & 1 & \\
		\end{pmatrix}_{m\times 2}\in \mathcal{SR}_2\implies\mathcal{L}(J)=\begin{pmatrix}
			1 & 1   \\
			\vdots & \vdots   \\
			1 & 1   \\
			k_1 & k_2 \\
		\end{pmatrix}\in \mathcal{SR}_2. \]
		We can show $\mathcal{L}(J)$ is a rank 1 matrix, similar to the case when $m=3$ and $n=2$. Hence, all $2\times2$ minors of $\mathcal{L}(J)$ are zero, and thus $k_1=k_{2}.$ Thus $\mathcal{L}$ is a positive diagonal equivalence. This completes the induction step for $m$. 
		
		Next, fix arbitrary $m$ and suppose the assertion holds for linear $\mathcal{SR}_2$-preservers from $\mathbb{R}^{m\times (n-1)}$ to $\mathbb{R}^{m\times (n-1)}$, where $m>n$. Let $\mathcal{L}:\mathbb{R}^{m\times n}\to\mathbb{R}^{m\times n}$ such that $\mathcal{L}\in P(\mathcal{SR}_2)$. For $A\in \mathcal{SR}_2$ of size $m\times n$, the submatrix of $A$ formed by all rows and first $(n-1)$ columns must be transformed to the first $(n-1)$ columns of $\mathcal{L}(A)$ because of Proposition \ref{SR_rectangle_FirstLast_rowTorow_colTOcol}(i). Since every SR$_2$ matrix $\widehat{A}$ of size $m\times(n-1)$ is a submatrix of the SR$_2$ matrix $(\widehat{A}|0)\in\mathbb{R}^{m\times n}$, the natural restriction of $\mathcal{L}$ onto the leading submatrix of size $m\times(n-1)$ is a linear $\mathcal{SR}_2$-preserver on $\mathbb{R}^{m\times(n-1)}$. By the induction hypothesis, it is a composition of one or more of the following maps: (i) $X\mapsto -X$, (ii) $X\mapsto XP$, (iii) $X\mapsto PX$, and (iv) $X\mapsto FXE$. By the same arguments in the preceding part, we have
		\[S_{ij}=\{E_{ij}\} \hspace{.2cm}\mathrm{for}\hspace{.2cm}\mathrm{all}\hspace{.1cm} 1\leq i\leq m,\hspace{.1cm} 1\leq j\leq n-1.\] 
		By Proposition \ref{SR_rectangle_FirstLast_rowTorow_colTOcol}, $\mathcal{L}$ maps each row of its argument entirely to some row and hence
		\begin{equation}\label{SR_rectangle_second_Sij=Eij}
			S_{ij}=\{E_{ij}\} \hspace{.2cm}\mathrm{for}\hspace{.2cm}\mathrm{all}\hspace{.1cm} 1\leq i\leq m,\hspace{.1cm} 1\leq j\leq n.
		\end{equation} 
		Since we may compose the inverse positive diagonal equivalence relative to the upper left submatrix of size $m\times(n-1)$ with $\mathcal{L}$, we may assume without loss of generality, that 
		\[\mathcal{L}(A)\begin{pmatrix}
			1,\ldots,m \\
			1,\ldots,n-1
		\end{pmatrix}=A\begin{pmatrix}
			1,\ldots,m \\
			1,\ldots,n-1
		\end{pmatrix}.\] 
		Using \eqref{SR_rectangle_second_Sij=Eij}, we have
		\[\mathcal{L}(E_{in})=c_iE_{in}\hspace{.2cm}\mathrm{for}\hspace{.2cm} 1\leq i\leq m, \hspace{.1cm}\mathrm{for} \hspace{.1cm}\mathrm{some} \hspace{.1cm}\mathrm{positive} \hspace{.1cm}\mathrm{scalar} \hspace{.1cm}c_i.\]
		Now, to complete the proof, we must show that $c_1=\cdots=c_{m}.$ 
		
		Consider the following rank 1 matrix
		\[J=\begin{pmatrix}
			1 &  \ldots & 1 & 1  \\
			1 &  \ldots & 1 & 1  \\
			\vdots & \ddots & \vdots & \vdots & \\
			1 &  \ldots & 1 & 1
		\end{pmatrix}_{m\times n}\in \mathcal{SR}\implies\mathcal{L}(J)=\begin{pmatrix}
			1 & \ldots & 1 & c_1 \\
			1 &  \ldots & 1 & c_2 \\
			\vdots & \ddots & \vdots & \vdots \\
			1 &  \ldots & 1 & c_{m} 
		\end{pmatrix}\in \mathcal{SR}. \]
		We can show that $\mathcal{L}(J)$ is a rank 1 matrix similar to the case when $m=n\geq3$. Hence, all $2\times2$ minors of $\mathcal{L}(J)$ are zero. Therefore, we have $c_1=\cdots=c_{m}.$ Thus, $\mathcal{L}$ maps $A$ to $FAE$ for some positive diagonal matrices $F$ and $E$. This concludes the induction step and the proof.
	\end{proof}
	\begin{rem}\label{SSR_proof}
		Theorem \ref{D} holds also for the class of SSR matrices. Since the five linear transformations specified in Theorem \ref{D} map $\mathcal{SRR}$ onto $\mathcal{SRR}$, so $P(\mathcal{SR})\subseteq P(\mathcal{SSR})$. By Theorem \ref{G-K}, $\mathcal{SR}=\overline{\mathcal{SSR}}$, and by Lemma \ref{lemma_preserver}, it follows that $P(\mathcal{SSR})\subseteq P(\mathcal{SR})$. Therefore, $P(\mathcal{SSR})=P(\mathcal{SR}).$
	\end{rem}
	%%%%%%%%%%%%%%%%%%%% Section 4 %%%%%%%%%%%%%%%%%%%%
	\section{Theorem \ref{F}: Linear preserver problem for sign regularity with given sign pattern} \label{section SR_epsilon}
	In the final section of this article, we show Theorem \ref{F}, i.e., we characterize all linear maps preserving sign regularity with a given sign pattern. Again, that $(1)\implies(2)$ is immediate, while $(3)\implies(1)$ is a straightforward verification. To show $(2)\implies(3)$, we must handle cases $m=n$ and $m\neq n$ separately, since $A\mapsto A^T$ does not map $m\times n$ matrices to $m\times n$ matrices. First, we will prove certain propositions which will be used in proving Theorem \ref{F} for both cases. \smallskip
	
	Let $\mathcal{L}:\mathbb{R}^{m\times n}\to\mathbb{R}^{m\times n}$ for $m,n\geq2$ be a linear transformation such that $\mathcal{L}$ maps $\mathcal{SR}_2(\epsilon)$ onto itself. If $m\neq n$, then we assume without loss of generality that $m>n$. The case $m<n$ follows similarly. Also, assume without loss of generality that $\epsilon_1>0.$ For $\epsilon_1<0$, we can take negative of the below basis elements and proceed similarly.
	
	Note that $E_{ij}\in \mathcal{SR}_2(\epsilon)$ for all $1\leq i\leq m$, $1\leq j\leq n$, thus $\mathcal{L}^{-1}$ exists and further $\mathcal{L}^{-1}\in P(\mathcal{SR}_2(\epsilon))$. Let $\mathfrak{B}=\{E_{11},E_{22},\ldots,E_{nn};E_{12},\ldots,E_{mn}\}$ be an ordered basis of $\mathbb{R}^{m\times n}$ and $L$ be the matrix which represents linear transformation $\mathcal{L}$ with respect to this basis.
	\begin{prop} \label{SR_epsilon_monomial}
		$L$ is a monomial matrix.
	\end{prop}	
	\begin{proof}
		Since $\mathcal{L}, \mathcal{L}^{-1}\in P(\mathcal{SR}_2(\epsilon))$, $L$ and $L^{-1}$ are entrywise non-negative and hence $L$ is a monomial matrix.
	\end{proof}	
    Note that Remark \ref{Sij_singleton} holds and all the entries of $\mathcal{L}(J)=Y$ are non-zero.
	\begin{prop} \label{SR_epsilon_S11Snn_S1nSn1_combo}
		For $\mathcal{L}\in P(\mathcal{SR}_2(\epsilon))$ with $\epsilon=(\epsilon_1,\epsilon_2)$, the following pairwise combinations are possible.
		\begin{itemize}
			\item[(i)] $S_{11}=\{E_{11}\}\hspace{.2cm} \mathrm{and} \hspace{.2cm} S_{mn}=\{E_{mn}\}, \hspace{.2cm} \mathrm{or} \hspace{.2cm} S_{11}=\{E_{mn}\} \hspace{.2cm} \mathrm{and} \hspace{.2cm} S_{mn}=\{E_{11}\}.$
			\item[(ii)] $S_{1n}=\{E_{1n}\} \hspace{.2cm} \mathrm{and} \hspace{.2cm} S_{m1}=\{E_{m1}\}, \hspace{.2cm} \mathrm{or} \hspace{.2cm} S_{1n}=\{E_{m1}\} \hspace{.2cm} \mathrm{and} \hspace{.2cm} S_{m1}=\{E_{1n}\}.$
		\end{itemize}
	\end{prop}
	\begin{proof}
		Let us begin by proving (i). To the contrary suppose that \[S_{11}\neq\{E_{11}\}, \{E_{mn}\}.\] 
		Let $J(c)$ be the matrix obtained by multiplying the $(1,1)$ entry of $J=J_{m\times n}$ by $c>0$. Then $J(c)\in \mathcal{SR}_2(\epsilon)$ with $\epsilon_1=\epsilon_2=1$ for $c>1$ and with $\epsilon_1=1$, $\epsilon_2=-1$ for $0<c<1$. Thus, $\mathcal{L}(J(c)):=Y(c)\in \mathcal{SR}_2(\epsilon)$. Now, consider the following cases.
		
		If $S_{11}=\{E_{1k}\}$ where $k\neq 1$, then an appropriate choice of $c>0$ gives us
		\[ 	\epsilon_2\det\begin{pmatrix}
			y_{11} & cy_{1k} \\
			y_{m1} & y_{mk}
		\end{pmatrix}<0,\hspace{.1cm}\mathrm{a}\hspace{.1cm} \mathrm{contradiction.}\]
		Similarly, we can show that $S_{11}\neq\{E_{k1}\}$ for $k\neq1$,  $S_{11}\neq\{E_{mk}\}$ for $k\neq n$, and $S_{11}\neq\{E_{kn}\}$ for $k\neq m$. 
		
		If $S_{11}=\{E_{ij}\}$ where $(i,j)\neq(1,1)$ and $(m,n)$, then an appropriate choice of $c>0$ gives us
		\[\epsilon_2\det\begin{pmatrix}
			y_{i1} & cy_{ij} \\
			y_{m1} & y_{mj}
		\end{pmatrix}<0,\hspace{.1cm}\mathrm{a}\hspace{.1cm} \mathrm{contradiction.}\]
		Hence, we conclude that either $S_{11}=\{E_{11}\}\hspace{.2cm} \mathrm{or} \hspace{.2cm} S_{11}=\{E_{mn}\}.$  \smallskip
		
		Again, to the contrary, suppose that
		\[S_{mn}\neq\{E_{11}\} \hspace{.2cm} \mathrm{and} \hspace{.2cm} \{E_{mn}\}.\] 
		In this case, let $J(c)$ be the matrix obtained by multiplying the $(m,n)$ entry of $J=J_{m\times n}$ by $c>0$. Proceed similar to the previous case to show that our assumption is not true. Hence,  \[S_{mn}=\{E_{11}\} \hspace{.2cm} \mathrm{or} \hspace{.2cm} S_{mn}=\{E_{mn}\}.\] 
		By Proposition \ref{SR_epsilon_monomial}, for any $\epsilon=(\epsilon_1,\epsilon_2)$, we have either 
		\[S_{11}=\{E_{11}\}\hspace{.2cm} \mathrm{and} \hspace{.2cm} S_{mn}=\{E_{mn}\}, \hspace{.2cm} \mathrm{or} \hspace{.2cm} S_{11}=\{E_{mn}\} \hspace{.2cm} \mathrm{and} \hspace{.2cm} S_{mn}=\{E_{11}\}.\] 
		The same argument can be adapted as in the preceding half of this proof shows that (ii) holds.
	\end{proof}
	%	\begin{rem}
		%		Now, we will prove next proposition assuming below holds.
		%		\begin{equation}\label{SR_epsilon_S11SnnSn1S1n_wlog}
			%			S_{11}=\{E_{11}\}, \hspace{.1cm} S_{mn}=\{E_{mn}\}, \hspace{.1cm} S_{1n}=\{E_{1n}\} \hspace{.2cm} \mathrm{and} \hspace{.2cm} S_{m1}=\{E_{m1}\}.
			%		\end{equation}
		%	\end{rem}
	\begin{prop} \label{SR_epsilon_FirstLast_Row_Col}
		Let $\mathcal{L}\in P(\mathcal{SR}_2(\epsilon))$ with $\epsilon=(\epsilon_1,\epsilon_2)$ such that
		\begin{equation}\label{SR_epsilon_S11SnnSn1S1n_wlog}
			S_{11}=\{E_{11}\}, \hspace{.1cm} S_{mn}=\{E_{mn}\}, \hspace{.1cm} S_{1n}=\{E_{1n}\} \hspace{.2cm} \mathrm{and} \hspace{.2cm} S_{m1}=\{E_{m1}\}.
		\end{equation}
		\begin{itemize}
			\item [(i)] Then $\mathcal{L}$ must map the first (and last) row and column of its arguments entirely to the first (and last) row and column, respectively.
			\item [(ii)] Moreover, $\mathcal{L}$ must map all rows and columns of its arguments entirely to some row and column, respectively.
		\end{itemize} 
	\end{prop}
	\begin{proof}
		First, we show that $\mathcal{L}$ must map the first row of its arguments entirely to the first row. Note that it holds trivially for $n=2$ by \eqref{SR_epsilon_S11SnnSn1S1n_wlog}. Therefore, let $n\geq3$. Let $J(c)$ be the matrix obtained by multiplying the first row of $J=J_{m\times n}$ by $c>0$. Then \[J(c)\in \mathcal{SR}_2(\epsilon)\hspace{.2cm} \mathrm{with}\hspace{.2cm} \epsilon_1=1\quad \implies \quad \mathcal{L}(J(c)):=Y(c)\in \mathcal{SR}_2(\epsilon)\hspace{.2cm}\mathrm{with}\hspace{.2cm} \epsilon_1=1. \] 
		Assume that $\mathcal{L}$ does not map the first row of its arguments entirely to the first row. Thus, there exists $1<p<n$ such that the $(1,p)$ position of the matrix $Y(c)$ is not occupied by the image of any element from the first row of $J(c)$. Using \eqref{SR_epsilon_S11SnnSn1S1n_wlog} and an appropriate choice of $c>0$ gives us
		\[\epsilon_2\det\begin{pmatrix}
			cy_{11} & y_{1p} \\
			y_{m1} & \alpha y_{mp}
		\end{pmatrix}=\epsilon_2(c\alpha y_{11}y_{mp}-y_{1p}y_{m1})<0,\]
		where either $\alpha=c$ or $\alpha=1$, a contradiction. Similarly, we can prove the other cases by multiplying that particular row and column of $J=J_{m\times n}$ by $c>0$. This shows part (i). 
		%	\end{proof}
	%	\begin{prop} \label{SR_epsilon_all_RowCol}
		%		For $\mathcal{L}\in P(\mathcal{SR}_2(\epsilon))$ with $\epsilon=(\epsilon_1,\epsilon_2)$, $\mathcal{L}$ must map all rows and columns of its arguments entirely to some row and column, respectively.
		%	\end{prop}
	%	\begin{proof}
		
		To prove part (ii), we will first show that $\mathcal{L}$ must map the $k^{th}$ row of its arguments entirely to some row where $1<k<m$. Let $J(c)$ be the matrix obtained by multiplying the $k^{th}$ row of $J=J_{m\times n}$ by $c> 0$. By Proposition \ref{SR_epsilon_FirstLast_Row_Col}(i), $\mathcal{L}$ maps the $(k,1)$ element of $J(c)$ to the first column, let us say $\mathcal{L}$ maps it to the $(s,1)$ position of $Y(c)$, where $1<s<m$. If $\mathcal{L}$ does not map the entire $k^{th}$ row of $J(c)$ to the $s^{th}$ row of $J(c)$, then there exists $1<j\leq n$ such that the $(s,j)$ position of the matrix $Y(c)$ is not occupied by the image of any element from the $k^{th}$ row of $J(c)$. Using Proposition \ref{SR_epsilon_FirstLast_Row_Col}(i) and an appropriate choice of $c>0$ gives us
		\[\epsilon_2\det\begin{pmatrix}
			cy_{s1} & y_{sj} \\
			y_{m1} & y_{mj}
		\end{pmatrix}<0, \hspace{.2cm}\mathrm{a}\hspace{.2cm}\mathrm{contradiction}.\]
		Similarly, we can prove this for columns. 
	\end{proof}
	Now, we will use the above proposition for $m=n\geq2$ to prove Theorem \ref{F}.
	%%%%%%%%%%%%%%%%%%%% Proof of Theorem E %%%%%%%%%%%%%%%%%%%%
	\begin{proof}[Proof of Theorem \ref{F} for $m=n$] 
		To prove $(2)\implies(3)$, let $n\geq2$ and $\mathcal{L}:\mathbb{R}^{n\times n}\to\mathbb{R}^{n\times n}$ be a linear transformation such that $\mathcal{L}$ maps $\mathcal{SR}_2(\epsilon)$ onto itself. By Proposition \ref{SR_epsilon_S11Snn_S1nSn1_combo}, we can assume without loss of generality, that
		\begin{equation}\label{SR_epsilon_square_S11SnnSn1S1n_wlog}
			S_{11}=\{E_{11}\}, \hspace{.1cm} S_{nn}=\{E_{nn}\}, \hspace{.1cm} S_{1n}=\{E_{1n}\}, \hspace{.2cm} \mathrm{and} \hspace{.2cm} S_{n1}=\{E_{n1}\}
		\end{equation}
		since $\mathcal{L}\in P(\mathcal{SR}(\epsilon))$ if and only if $\mathcal{L}$ composed with the maps $A\mapsto P_nAP_n$ and $A\mapsto A^T$ is a linear SR$(\epsilon)$-preserver. Hence, Proposition \ref{SR_epsilon_FirstLast_Row_Col} holds. \smallskip
		
		We now complete the proof by induction on $n$ with the base case $n=2$. Let $\mathcal{L}:\mathbb{R}^{2\times2}\to\mathbb{R}^{2\times2}$ be a linear map such that $\mathcal{L}(\mathcal{SR}_2(\epsilon))=\mathcal{SR}_2(\epsilon)$ and let $\mathfrak{B}=\{E_{11}, E_{22}, E_{12}, E_{21}\}$ be an ordered basis of $\mathbb{R}^{2\times 2}$. By Propositions \ref{SR_epsilon_monomial} and \ref{SR_epsilon_FirstLast_Row_Col}, we have $S_{ij}=\{E_{ij}\}$ for all $1\leq i,j\leq 2$. Thus, $\mathcal{L}(E_{11})=l_{1}E_{11},\ldots,\mathcal{L}(E_{21})=l_{4}E_{21}\in \mathcal{SR}_2(\epsilon)$ with $\epsilon_1=1$. By Proposition \ref{SR_epsilon_monomial}, $l_{i}>0$ for all $1\leq i\leq 4$. 
		
		%Thus, the matrix $L$ which represents $\mathcal{L}$ with respect to this basis has the following form  
		%		\[L= \begin{pmatrix}
			%			l_{11} & 0 & 0 & 0 \\
			%			0 & l_{22} & 0 & 0 \\
			%			0 & 0 & l_{33} & 0 \\
			%			0 & 0 & 0 & l_{44} 
			%		\end{pmatrix}.\]
		Again, since $J=\begin{pmatrix}
			1 & 1 \\
			1 & 1
		\end{pmatrix}_{2\times2}\in \mathcal{SR}_2(\epsilon) \hspace{.2cm}\mathrm{with}\hspace{.2cm} \epsilon_1=1,$
		we have 
		\[\mathcal{L}(J)=\begin{pmatrix}
			l_{1} & l_{3} \\
			l_{4} & l_{2}
		\end{pmatrix}\in \mathcal{SR}_2(\epsilon)\hspace{.2cm}\mathrm{and}\hspace{.2cm} \mathcal{L}^{-1}(J)=\begin{pmatrix}
			\frac{1}{l_{1}} & \frac{1}{l_{3}} \\
			\frac{1}{l_{4}} & \frac{1}{l_{2}}
		\end{pmatrix}\in \mathcal{SR}_2(\epsilon) \]
		and hence
		\begin{equation}\label{SR_epsilon_square_l11l22-l33l44}
			l_{1}l_{2}-l_{3}l_{4}=0.
		\end{equation}
		Now, let 
		\[B=\begin{pmatrix}
			b_{11} & b_{12} \\
			b_{21} & b_{22} 
		\end{pmatrix}\in \mathcal{SR}_2(\epsilon) \implies \mathcal{L}(B)=\begin{pmatrix}
			l_{1}b_{11} & l_{3}b_{12} \\
			l_{4}b_{21} & l_{2}b_{22}
		\end{pmatrix}\in \mathcal{SR}_2(\epsilon).\] 
		Using \eqref{SR_epsilon_square_l11l22-l33l44}, we can write $\mathcal{L}(B)$ as 
		\[\mathcal{L}(B)=\begin{pmatrix}
			l_{1} & 0  \\
			0 & l_{4}
		\end{pmatrix}\begin{pmatrix}
			b_{11} & b_{12} \\
			b_{21} & b_{22} 
		\end{pmatrix}\begin{pmatrix}
			1 & 0  \\
			0 & l_{3}/l_{1}
		\end{pmatrix}\] 
		which is a positive diagonal equivalence. This completes the proof for $n=2$. \smallskip
		
		For the induction step, let $\mathcal{L}:\mathbb{R}^{n\times n} \to\mathbb{R}^{n\times n}$  be a linear $\mathcal{SR}_2(\epsilon)$-preserver with $n>2$. Let $A\in\mathcal{SR}_2(\epsilon)$. By Proposition \ref{SR_epsilon_FirstLast_Row_Col}(i), the leading principal submatrix of $A$ of size $(n-1)\times (n-1)$ must be transformed to the leading principal submatrix of size $(n-1)\times (n-1)$ of $\mathcal{L}(A)$. Since every SR$_2(\epsilon)$ matrix $\widehat{A}$ of size $(n-1)\times(n-1)$ is a leading principal submatrix of the SR$_2(\epsilon)$ matrix $\widehat{A}\oplus\{0\}$ of size $n\times n$, the natural restriction of $\mathcal{L}$ onto the leading principal submatrix of size $(n-1)\times(n-1)$ is a linear $\mathcal{SR}_2(\epsilon)$-preserver on $\mathbb{R}^{(n-1)\times(n-1)}$. By the induction hypothesis, it is a composition of one or more of the following maps: (i) $X\mapsto PXP$, (ii) $X\mapsto X^T$, and (iii) $X\mapsto FXE$. In fact, it is a positive diagonal equivalence since the first row is transformed to the first row. Thus \[S_{ij}=\{E_{ij}\}\hspace{.2cm}\mathrm{for}\hspace{.2cm}\mathrm{all} \hspace{.2cm} 1\leq i,j\leq n-1.\] 
		By Proposition \ref{SR_epsilon_FirstLast_Row_Col}, $\mathcal{L}$ maps all arguments of rows and columns entirely to some row and column, respectively and hence
		\begin{equation}\label{SR_epsilon_square_Sij=Eij}
			S_{ij}=\{E_{ij}\}\hspace{.2cm}\mathrm{for}\hspace{.2cm}\mathrm{all} \hspace{.2cm} 1\leq i,j\leq n.
		\end{equation}
		Since we may compose the inverse positive diagonal equivalence relative to the upper left principal submatrix of size $(n-1)\times(n-1)$ with $\mathcal{L}$, we may assume without loss of generality that 
		\[\mathcal{L}(A)\begin{pmatrix}
			1,\ldots,n-1 \\
			1,\ldots,n-1
		\end{pmatrix}=A\begin{pmatrix}
			1,\ldots,n-1 \\
			1,\ldots,n-1
		\end{pmatrix}.\] 
		Using \eqref{SR_epsilon_square_Sij=Eij}, we have
		\[\mathcal{L}(E_{in})=c_iE_{in}, \hspace{.2cm} \mathcal{L}(E_{ni})=k_iE_{ni}, \hspace{.2cm}\mathrm{for}\hspace{.2cm} 1\leq i\leq n-1, \hspace{.2cm}\mathrm{and}\hspace{.2cm}
		\mathcal{L}(E_{nn})=dE_{nn}\] 
		for some positive scalars $c_i,k_i$, and $d$. We next claim that
		\begin{equation}\label{eqthrmE3}
			c_1=\cdots=c_{n-1}, \hspace{.1cm}k_1=\cdots=k_{n-1},\hspace{.2cm}\mathrm{and}\hspace{.2cm}d=c_1k_1.	\end{equation}
		Let $J=J_{n\times n}$. Then
		\[\mathcal{L}(J)=\begin{pmatrix}
			1 & \ldots & 1 & c_1 \\
			\vdots & \ddots & \vdots & \vdots \\
			1 & \ldots & 1 & c_{n-1} \\
			k_1 & \ldots & k_{n-1} & d
		\end{pmatrix}\in \mathcal{SR}_2(\epsilon) \hspace{.2cm}\mathrm{and}\hspace{.2cm}\mathcal{L}^{-1}(J)=\begin{pmatrix}
			1 & \ldots & 1 & \frac{1}{c_1} \\
			\vdots & \ddots & \vdots & \vdots \\
			1 & \ldots & 1 & \frac{1}{c_{n-1}} \\
			\frac{1}{k_1} & \ldots & \frac{1}{k_{n-1}} & \frac{1}{d}
		\end{pmatrix}\in \mathcal{SR}_2(\epsilon).\]
		Thus, \eqref{eqthrmE3} holds. Hence, $\mathcal{L}$ maps $A$ to $FAE$ for some positive diagonal matrices $F$ and $E$. This concludes the induction step and the proof.
	\end{proof}
	%	\begin{rem}
		%		Theorem \ref{E} holds also for the class of SSR$(\epsilon)$ matrices.
		%	\end{rem}	
	Next, we prove Theorem \ref{F} for $m>n\geq2$, using the propositions stated at the beginning of this section.
	%%%%%%%%%%%%%%%%%%%% Proof of Theorem F %%%%%%%%%%%%%%%%%%%%
	\begin{proof}[Proof of Theorem \ref{F} for $m\neq n$] 
		To show $(2)\implies(3),$ let $\mathcal{L}:\mathbb{R}^{m\times n}\to\mathbb{R}^{m\times n}$ where $m>n\geq2$ be a linear transformation such that $\mathcal{L}$ maps $\mathcal{SR}_2(\epsilon)$ onto itself. Since $\mathcal{L}$ is an $\mathcal{SR}_2(\epsilon)$-preserver if and only if $\mathcal{L}$ composed with the map $A\mapsto P_{m}AP_{n}$ is also one, by Proposition \ref{SR_epsilon_S11Snn_S1nSn1_combo}(i), we can assume without loss of generality that 
		\begin{equation}\label{SR_epsilon_rectangle_S11Smn_wlog}
			S_{11}=\{E_{11}\}\hspace{.2cm} \mathrm{and} \hspace{.2cm} S_{mn}=\{E_{mn}\}.
		\end{equation} 
		Therefore, from \eqref{SR_epsilon_rectangle_S11Smn_wlog} and part (ii) of Proposition \ref{SR_epsilon_S11Snn_S1nSn1_combo}, we have either
		\[S_{11}=\{E_{11}\}, \hspace{.1cm} S_{mn}=\{E_{mn}\}, \hspace{.1cm} S_{1n}=\{E_{1n}\}, \hspace{.2cm} \mathrm{and} \hspace{.2cm} S_{m1}=\{E_{m1}\} \hspace{.2cm} \mathrm{or} \]
		\[S_{11}=\{E_{11}\}, \hspace{.1cm} S_{mn}=\{E_{mn}\}, \hspace{.1cm} S_{1n}=\{E_{m1}\}, \hspace{.2cm} \mathrm{and} \hspace{.2cm} S_{m1}=\{E_{1n}\}.\]
		
		Next, we show that \[S_{11}=\{E_{11}\}, \hspace{.1cm} S_{mn}=\{E_{mn}\}, \hspace{.1cm} S_{1n}=\{E_{m1}\}, \hspace{.2cm} \mathrm{and} \hspace{.2cm} S_{m1}=\{E_{1n}\}\] is not possible. Assume to the contrary that it holds. Let $J(c)$ be the matrix obtained by multiplying the first row of $J=J_{m\times n}$ by $c>0$. Then $J(c)\in \mathcal{SR}_2(\epsilon)$ and hence $\mathcal{L}(J(c)):=Y(c)\in \mathcal{SR}_2(\epsilon)$ with $\epsilon_1=1$. Since $m>n$, even if $\mathcal{L}$ maps entire elements of the first row of $J(c)$ to the first column of $Y(c)$, then there exists $1<j<m$ such that the $(j,1)$ position of $Y(c)$ is not occupied by the image of any element from the first row of $J(c)$. Therefore, by our assumption and an appropriate choice of $c>0$,
		\[\epsilon_2\det\begin{pmatrix}
			cy_{11} & y_{1n} \\
			y_{j1} & \alpha y_{jn}
		\end{pmatrix}<0,\]
		where either $\alpha=c$ or  $\alpha=1$, a contradiction. Thus
		\begin{equation} \label{SR_epsilon_rectangle_S11SmnS1nSm1_wlog}
			S_{11}=\{E_{11}\},\hspace{.1cm} S_{mn}=\{E_{mn}\},\hspace{.1cm} S_{1n}=\{E_{1n}\}, \hspace{.1cm}\mathrm{and}\hspace{.1cm} S_{m1}=\{E_{m1}\}.
		\end{equation}
		Hence, Proposition \ref{SR_epsilon_FirstLast_Row_Col} holds. \smallskip
		
		We now complete the proof by induction on the size of the matrices with the base case $m=3$ and $n=2$. Let $\mathcal{L}:\mathbb{R}^{3\times2}\to\mathbb{R}^{3\times2}$ such that $\mathcal{L}(\mathcal{SR}_2(\epsilon))=\mathcal{SR}_2(\epsilon)$. 
		
		Let $\mathfrak{B}=\{E_{11}, E_{22}, E_{12}, E_{21},E_{31},E_{32}\}$ be an ordered basis of $\mathbb{R}^{3\times 2}$. We have $S_{ij}=\{E_{ij}\}$ for all $1\leq i\leq 3$,\hspace{.2cm} $1\leq j\leq2$ because of Proposition \ref{SR_epsilon_FirstLast_Row_Col}. Thus, $\mathcal{L}(E_{11})=l_{1}E_{11},\ldots,\mathcal{L}(E_{32})=l_{6}E_{32}\in \mathcal{SR}_2(\epsilon)$ with $\epsilon_1=1$. By Proposition \ref{SR_epsilon_monomial}, $l_{i}>0$ for all $1\leq i\leq 6.$ 
		
		%		Then, the matrix $L$ which represents $\mathcal{L}$ with respect to this basis has the following form  
		%		\[L= \begin{pmatrix}
			%			l_{11} & 0 & 0 & 0 & 0 & 0 \\
			%			0 & l_{22} & 0 & 0 & 0 & 0 \\
			%			0 & 0 & l_{33} & 0 & 0 & 0 \\
			%			0 & 0 & 0 & l_{44} & 0 & 0 \\
			%			0 & 0 & 0 & 0 & l_{55} & 0 \\
			%			0 & 0 & 0 & 0 & 0 & l_{66}
			%		\end{pmatrix}.\]
		Since $A_1:=\begin{pmatrix}
			1 & 1 \\
			1 & 1 \\
			0 & 0
		\end{pmatrix}\in \mathcal{SR}_2(\epsilon),$ 
		\[\mathcal{L}(A_1)=\begin{pmatrix}
			l_{1} & l_{3} \\
			l_{4} & l_{2} \\
			0 & 0
		\end{pmatrix}\in \mathcal{SR}_2(\epsilon)\hspace{.2cm} \mathrm{and}\hspace{.2cm}\mathcal{L}^{-1}(A_1)=\begin{pmatrix}
			\frac{1}{l_{1}} & \frac{1}{l_{3}} \\
			\frac{1}{l_{4}} & \frac{1}{l_{2}} \\
			0            &      0
		\end{pmatrix}\in \mathcal{SR}_2(\epsilon) \]
		and hence
		\begin{equation}\label{SR_epsilon_rectangle_l11l22-l33l44}
			l_{1}l_{2}-l_{3}l_{4}=0.
		\end{equation}	
		Again, $A_2:=\begin{pmatrix}
			1 & 1 \\
			0 & 0 \\
			1 & 1
		\end{pmatrix}\in \mathcal{SR}_2(\epsilon)$ and hence
		\[\mathcal{L}(A_2)=\begin{pmatrix}
			l_{1} & l_{3} \\
			0 & 0 \\
			l_{5} & l_{6}
		\end{pmatrix}\in \mathcal{SR}_2(\epsilon)\hspace{.2cm} \mathrm{and}\hspace{.2cm}\mathcal{L}^{-1}(A_2)=\begin{pmatrix}
			\frac{1}{l_{1}} & \frac{1}{l_{3}} \\
			0            &      0   \\
			\frac{1}{l_{5}} & \frac{1}{l_{6}}
		\end{pmatrix}\in \mathcal{SR}_2(\epsilon). \]	
		Thus
		\begin{equation}\label{SR_epsilon_rectangle_l11l66-l33l55}
			l_{1}l_{6}-l_{3}l_{5}=0.
		\end{equation}
		Now let 
		\[B=\begin{pmatrix}
			b_{11} & b_{12} \\
			b_{21} & b_{22} \\
			b_{31} & b_{32}
		\end{pmatrix}\in \mathcal{SR}_2(\epsilon) \implies \mathcal{L}(B)=\begin{pmatrix}
			l_{1}b_{11} & l_{3}b_{12} \\
			l_{4}b_{21} & l_{2}b_{22} \\
			l_{5}b_{31} & l_{6}b_{32}
		\end{pmatrix}\in \mathcal{SR}_2(\epsilon).\] 
		Using \eqref{SR_epsilon_rectangle_l11l22-l33l44} and \eqref{SR_epsilon_rectangle_l11l66-l33l55}, we can write $\mathcal{L}(B)$ as 
		\[\mathcal{L}(B)=\begin{pmatrix}
			l_{1} & 0 & 0 \\
			0 & l_{4} & 0 \\
			0 & 0 & l_{5}
		\end{pmatrix}\begin{pmatrix}
			b_{11} & b_{12} \\
			b_{21} & b_{22} \\
			b_{31} & b_{32}
		\end{pmatrix}\begin{pmatrix}
			1 & 0  \\
			0 & l_{3}/l_{1}
		\end{pmatrix}\] 
		which is a positive diagonal equivalence. This completes the proof for the base case. 
		
		For the induction step, let $\mathcal{L}:\mathbb{R}^{m\times2}\to\mathbb{R}^{m\times2}$ be a linear map such that $\mathcal{L}\in P(\mathcal{SR}_2(\epsilon))$. Let $A\in\mathcal{SR}_2(\epsilon)$ of size $m\times2$. By Proposition \ref{SR_epsilon_FirstLast_Row_Col}(i), the submatrix of $A$ formed by the first $(m-1)$ rows and both columns must be transformed to the first $(m-1)$ rows of $\mathcal{L}(A)$. Since every SR$_2(\epsilon)$ matrix $\widehat{A}$ of size $(m-1)\times 2$ is a submatrix of the SR$_2(\epsilon)$ matrix $(\widehat{A}^T|0)^T\in\mathbb{R}^{m\times 2}$, the natural restriction of $\mathcal{L}$ onto the leading submatrix of size $(m-1)\times 2$ is a linear $\mathcal{SR}_2(\epsilon)$-preserver on $\mathbb{R}^{(m-1)\times 2}$. By the induction hypothesis, the restriction of $\mathcal{L}$ is a composition of one or more of the following maps: (i) $X\mapsto PXP$ and (ii) $X\mapsto FXE$. In fact, it is a positive diagonal equivalence since the first row and column is transformed to the first row and column, respectively. Thus
		\[S_{ij}=\{E_{ij}\} \hspace{.2cm}\mathrm{for}\hspace{.2cm}\mathrm{all}\hspace{.1cm} 1\leq i\leq m-1,\hspace{.1cm} 1\leq j\leq 2.\] 
		By Proposition \ref{SR_epsilon_FirstLast_Row_Col}(i), $\mathcal{L}$ maps the first (and last) column of its arguments entirely to the first (and last) column, and hence
		\begin{equation}\label{SR_epsilon_rectangle_Sij=Eij}
			S_{ij}=\{E_{ij}\} \hspace{.2cm}\mathrm{for}\hspace{.2cm}\mathrm{all}\hspace{.1cm} 1\leq i\leq m,\hspace{.1cm}  1\leq j\leq 2.
		\end{equation} 
		Since we may compose the inverse positive diagonal equivalence relative to the upper left submatrix of size $(m-1)\times 2$ with $\mathcal{L}$, we may assume without loss of generality, that 
		\[\mathcal{L}(A)\begin{pmatrix}
			1,\ldots,m-1 \\
			1,2
		\end{pmatrix}=A\begin{pmatrix}
			1,\ldots,m-1 \\
			1,2
		\end{pmatrix}.\] 
		Using \eqref{SR_epsilon_rectangle_Sij=Eij}, we have
		\[\mathcal{L}(E_{mi})=k_iE_{mi} \hspace{.2cm}\mathrm{for}\hspace{.2cm} 1\leq i\leq 2,\]
		for some positive scalars $k_1$ and $k_2$. We next claim that $k_1=k_{2}.$ Let $J=J_{m\times2}$. Then
		\[\mathcal{L}(J)=\begin{pmatrix}
			1 & 1   \\
			\vdots & \vdots   \\
			1 & 1   \\
			k_1 & k_2 \\
		\end{pmatrix}\in \mathcal{SR}_2(\epsilon) \hspace{.2cm}\mathrm{and} \hspace{.2cm} \mathcal{L}^{-1}(J)=\begin{pmatrix}
			1 & 1   \\
			\vdots & \vdots   \\
			1 & 1   \\
			\frac{1}{k_1} & \frac{1}{k_2} \\
		\end{pmatrix}\in \mathcal{SR}_2(\epsilon). \]
		Therefore, we have $k_1=k_{2}$ and hence $\mathcal{L}$ is a positive diagonal equivalence. This completes the induction step for $m$ and the result holds for all linear $\mathcal{SR}_2(\epsilon)$-preservers from $\mathbb{R}^{m\times2}$ to $\mathbb{R}^{m\times2}$. 
		
		Now, fix arbitrary $m$ and suppose the claim holds for all linear $\mathcal{SR}_2(\epsilon)$-preservers from $\mathbb{R}^{m\times(n-1)}\to\mathbb{R}^{m\times(n-1)}$. Let $\mathcal{L}:\mathbb{R}^{m\times n}\to\mathbb{R}^{m\times n}$ be a linear $\mathcal{SR}_2(\epsilon)$-preserver. For $A\in \mathcal{SR}_2(\epsilon)$ of size $m\times n$, the submatrix of $A$ formed by all rows and the first $(n-1)$ columns must be transformed to the first $(n-1)$ columns of $\mathcal{L}(A)$ because of Proposition \ref{SR_epsilon_FirstLast_Row_Col}(i). Since every SR$_2(\epsilon)$ matrix $\widehat{A}$ of size $m\times(n-1)$ is a submatrix of the SR$_2(\epsilon)$ matrix $(\widehat{A}|0)\in\mathbb{R}^{m\times n}$, the natural reduction of $\mathcal{L}$ onto the leading submatrix of size $m\times(n-1)$ is a linear $\mathcal{SR}_2(\epsilon)$-preserver on $\mathbb{R}^{m\times(n-1)}$. By the induction hypothesis, it is a composition of one or more of the following maps: (i) $X\mapsto PXP$, and (ii) $X\mapsto FXE$. By the same argument as in the preceding part, we have
		\[S_{ij}=\{E_{ij}\} \hspace{.2cm}\mathrm{for}\hspace{.2cm}\mathrm{all}\hspace{.1cm} 1\leq i\leq m,\hspace{.1cm} 1\leq j\leq n-1.\] 
		By Proposition \ref{SR_epsilon_FirstLast_Row_Col}, $\mathcal{L}$ maps all rows of its arguments entirely to some row and hence
		\begin{equation}\label{SR_epsilon_rectangle_second_Sij=Eij}
			S_{ij}=\{E_{ij}\} \hspace{.2cm}\mathrm{for}\hspace{.2cm}\mathrm{all}\hspace{.1cm} 1\leq i\leq m,\hspace{.1cm} 1\leq j\leq n.
		\end{equation} 
		Since we may compose the inverse positive diagonal equivalence relative to the upper left submatrix of size $m\times(n-1)$ with $\mathcal{L}$, we may assume without loss of generality, that 
		\[\mathcal{L}(A)\begin{pmatrix}
			1,\ldots,m \\
			1,\ldots,n-1
		\end{pmatrix}=A\begin{pmatrix}
			1,\ldots,m \\
			1,\ldots,n-1
		\end{pmatrix}.\] 
		Using \eqref{SR_epsilon_rectangle_second_Sij=Eij}, we have
		\[\mathcal{L}(E_{in})=c_iE_{in}\hspace{.2cm}\mathrm{for}\hspace{.2cm} 1\leq i\leq m, \hspace{.1cm}\mathrm{for} \hspace{.1cm}\mathrm{some} \hspace{.1cm}\mathrm{positive} \hspace{.1cm}\mathrm{scalar} \hspace{.1cm}c_i.\]
		Now, to complete the proof, we must show that $c_1=\cdots=c_{m}.$ Let $J=J_{m\times n}$. Then
		\[\mathcal{L}(J)=\begin{pmatrix}
			1 & \ldots & 1 & c_1 \\
			1 &  \ldots & 1 & c_2 \\
			\vdots & \ddots & \vdots & \vdots \\
			1 &  \ldots & 1 & c_{m} 
		\end{pmatrix}\in \mathcal{SR}_2(\epsilon) \hspace{.2cm}\mathrm{and} \hspace{.2cm} \mathcal{L}^{-1}(J)=\begin{pmatrix}
			1 & \ldots & 1 & \frac{1}{c_1} \\
			1 &  \ldots & 1 & \frac{1}{c_2} \\
			\vdots & \ddots & \vdots & \vdots \\
			1 &  \ldots & 1 & \frac{1}{c_{m}} 
		\end{pmatrix}\in \mathcal{SR}_2(\epsilon). \]
		Therefore, we have $c_1=\cdots=c_{m}.$ Thus, $\mathcal{L}$ maps $A$ to $FAE$ for some positive diagonal matrices $F$ and $E$. This concludes the induction step and the proof.
	\end{proof}
	\begin{rem}
		Theorem \ref{F} holds also for the class of SSR$(\epsilon)$ matrices. The proof follows Remark \ref{SSR_proof} verbatim.
	\end{rem}	
	
	\section*{Acknowledgments}
	We thank Apoorva Khare for a detailed reading of an earlier draft and for providing valuable feedback. The first author was partially supported by INSPIRE Faculty Fellowship research grant DST/INSPIRE/04/2021/002620
	(DST, Govt.~of India), and IIT Gandhinagar Internal Project: IP/IITGN/MATH/PNC/2223/25.
	
	
	\begin{thebibliography}{10}
		
		\bibitem{Ando87}
		T.~Ando.
		\newblock Totally positive matrices.
		\newblock \href{https://doi.org/10.1016/0024-3795(87)90313-2}{\em Linear Algebra Appl.}, 90:165--219, 1987.
		
		
		\bibitem{Beasley88}
		L.B.~Beasley.
		\newblock Linear operators on matrices: the invariance of rank-$k$ matrices.
		\newblock \href{https://www.sciencedirect.com/science/article/pii/002437958890242X}
		{\em Linear Algebra Appl.}, 107:161--167, 1988.
		
		\bibitem{BGKP20}
		A.~Belton, D.~Guillot, A.~Khare, and M.~Putinar.
		\newblock  Totally positive kernels, P\'olya frequency functions,
		and their transforms.
		\newblock {\em J.\ d'Analyse Math.}, in press;
		\href{http://arxiv.org/abs/2006.16213}{arXiv:math.FA/2006.16213}.
		
		%		\bibitem{BGKP21}
		%		A. Belton, D. Guillot, A. Khare, and M. Putinar.
		%		\newblock  Hirschman-Widder densities. {\em Prepint,}
		%		\newblock \href{https://arxiv.org/abs/2101.02129}{ 	arXiv:2101.02129}, 2021.
		
		\bibitem{BFZ96}
		A.~Berenstein, S.~Fomin, and A.~Zelevinsky.
		\newblock Parametrizations of canonical bases and totally positive matrices.
		\newblock \href{http://dx.doi.org/10.1006/aima.1996.0057}{\em Adv.
			Math.}, 122:49--149, 1996.
		
		\bibitem{BHJ85}
		A.~Berman, D.~Hershkowitz, and C.R.~Johnson.
		\newblock Linear transformations that preserve certain positivity classes of matrices.
		\newblock \href{https://doi.org/10.1016/0024-3795(85)90205-8}{\em Linear Algebra Appl.}, 68:9--29, 1985.
		
%		\bibitem{deBoor}
%		C.~de Boor.
%		\newblock On calculating with $B$-splines.
%		\newblock \href{http://dx.doi.org/10.1016/0021-9045(72)90080-9}{\em J.\
%			Approx.\ Theory}, 6(1):50--62, 1972.
		
		\bibitem{Bre95}
		F.~Brenti.
		\newblock Combinatorics and total positivity.
		\newblock \href{http://dx.doi.org/10.1016/0097-3165(95)90000-4}%
		{\em J.\ Combin.\ Theory Ser.~A}, 71(2):175--218, 1995.
		
		\bibitem{Chepuri}
		A.~Brosowsky, S.~Chepuri, and A.~Mason.
		\newblock Parametrizations of {$k$}-non-negative matrices: cluster algebras and
		{$k$}-positivity tests.
		\newblock \href{https://doi.org/10.1016/j.jcta.2020.105217}{\em J.
			Combin. Theory Ser. A}, 174:art.~105217, 25 pp., 2020.
		
		\bibitem{BJM81}
		L.D.~Brown, I.M.~Johnstone, and K.B.~MacGibbon.
		\newblock Variation diminishing transformations: a direct approach to total positivity and its statistical applications.
		\newblock \href{https://doi.org/10.2307/2287577 }%
		{\em J. Amer. Statist. Assoc.}, 76(376):824--832, 1981.
		
		
		\bibitem{C22}
		P.N.~Choudhury.
		\newblock  Characterizing total positivity: single vector tests via Linear Complementarity, sign non-reversal, and variation diminution. \newblock \href{https://doi.org/10.1112/blms.12601}{\em Bull.\ London
			Math.\ Soc.}, 54(2):791--811, 2022.
		
		%		\bibitem{CKK21}
		%		P.N.~Choudhury, M.R.~Kannan, and A.~ Khare.
		%		\newblock Sign non-reversal property for totally non-negative and totally positive matrices, and testing total positivity of their interval hull.
		%		\newblock \href{http://dx.doi.org/10.1112/blms.12475}{\em Bull.\ London
			%			Math.\ Soc.}, published online.
		
		%	\bibitem{Co66}
		%	R.W.~Cottle.
		%	\newblock Nonlinear programs with positively bounded {J}acobians.
		%	\newblock \href{https://doi.org/10.1137/0114012}{\em SIAM J. Appl. Math.}, 14:147--158, 1966.
		
		
		%	\bibitem{Co68}
		%	R.W.~Cottle.
		%	\newblock On a problem in linear inequalities.
		%	\newblock \href{https://doi.org/10.1112/jlms/s1-43.1.378}{\em J. London Math. Soc.}, 43:378--384, 1968.
		
		%	\bibitem{CD68}
		%	R.W.~Cottle and G.B.~Dantzig.
		%	\newblock Complementary pivot theory of mathematical programming.
		%	\newblock \href{https://doi.org/10.1016/0024-3795(68)90052-9}{\em Linear Algebra Appl.}, 1(1):103--125, 1968.
		
		%	\bibitem{CPS09}
		%	R.W.~Cottle, J-S~Pang, and R.E.~Stone.
		%	\newblock {\em The linear complementarity problem}.
		%	\newblock \href{ https://doi.org/10.1137/1.9780898719000}{Classics in Applied Mathematics}, SIAM, Philadelphia, PA, 2009.
		
		%	\bibitem{Cr71}
		%	C.W.~Cryer.
		%	\newblock The solution of a quadratic programming problem using systematic overrelaxation.
		%	\newblock \href{https://epubs.siam.org/doi/pdf/10.1137/0309028}{\em SIAM J. Control}, 9:385--392, 1971.
		
		\bibitem{Curry}
		H.B.~Curry and I.J.~Schoenberg.
		\newblock On P\'olya frequency functions IV: the fundamental spline
		functions and their limits.
		\newblock \href{http://dx.doi.org/10.1007/BF02788653}{\em J.\ d'Analyse Math.}, 17:71--107, 1966.
		
		%	\bibitem{ES76}
		%	B.C.~Eaves and H.~Scarf.
		%	\newblock The solution of systems of piecewise linear equations.
		%	\newblock \href{https://doi.org/10.1287/moor.1.1.1}{\em  Math. Oper. Res.}, 1(1):1--27, 1976.
		
		\bibitem{fallat-john}
		S.M.~Fallat and C.R.~Johnson.
		\newblock {\em Totally non-negative matrices}.
		\newblock
		\href{https://press.princeton.edu/books/hardcover/9780691121574/totally-non-negative-matrices}{Princeton
			Series in Applied Mathematics}, Princeton University Press, Princeton, 2011.
		
		\bibitem{FP12}
		M.~Fekete and G.~P\'{o}lya.
		\newblock \"{U}ber ein {P}roblem von {L}aguerre.
		\newblock \href{http://dx.doi.org/10.1007/BF03015009}{\em Rend.\ Circ.\
			Mat.\ Palermo}, 34:89--120, 1912.
		
		\bibitem{FZ00}
		S.~Fomin and A.~Zelevinsky.
		\newblock Total positivity: tests and parametrizations.
		\newblock \href{http://dx.doi.org/10.1007/BF03024444}%
		{\em Math.\ Intelligencer}, 22(1):23--33, 2000.
		
		\bibitem{FZ02}
		S.~Fomin and A.~Zelevinsky.
		\newblock Cluster algebras. {I}. {F}oundations.
		\newblock \href{http://dx.doi.org/10.1090/S0894-0347-01-00385-X}{\em J.\
			Amer.\ Math.\ Soc.}, 15(2):497--529, 2002.
		
		\bibitem{Frobenius1897}
		G.~Frobenius.
		\newblock  \"{U}ber die Darstellung der endlichen Gruppen durch lineare Substitutionen.
		\newblock {\em Sitzungsber. Preuss. Akad. Wiss. Berlin}, 994--1015, 1897.
		
		%	\bibitem{gale-nikai-pmat}
		%	D.~Gale and H.~Nikaido.
		%	\newblock The {J}acobian matrix and global univalence of mappings.
		%	\newblock \href{http://dx.doi.org/10.1007/BF01360282}{\em Math.\ Ann.},
		%	159:81--93, 1965.
		
		\bibitem{gantmacher-krein}
		F.R.~Gantmacher and M.G.~Krein.
		\newblock Sur les matrices compl\`etement nonn\'egatives et oscillatoires.
		\newblock \href{http://www.numdam.org/item?id=CM_1937__4__445_0}%
		{\em Compositio Math.}, 4:445--476, 1937.
		
		\bibitem{GK50}
		F.R.~Gantmacher and M.G.~Krein.
		\newblock {\em Oscillyacionye matricy i yadra i malye kolebaniya
			mehani\v{c}eskih sistem}.
		\newblock Gosudarstv. Isdat. Tehn.-Teor. Lit., Moscow-Leningrad, 1950.
		\newblock 2d ed.
		
		\bibitem{GW96}
		J.~Garloff and D.G.~Wagner.
		\newblock Hadamard products of stable polynomials are stable.
		\newblock \href{https://doi.org/10.1006/jmaa.1996.0348}{\em J. Math. Anal. Appl.}, 202(3):797--809, 1996.
		
		\bibitem{GRS18}
		K.~Gr\"{o}chenig, J.L.~Romero, and J.~St\"{o}ckler.
		\newblock Sampling theorems for shift-invariant spaces, {G}abor frames, and
		totally positive functions.
		\newblock \href{http://dx.doi.org/10.1007/s00222-017-0760-2}{\em Invent.\
			Math.}, 211:1119--1148, 2018.
		
		\bibitem{GLS2000}
		A. Guterman, Chi-K. Li, and P. \v{S}emrl.
		\newblock Some general techniques on linear preserver problems.
		\newblock \href{https://doi.org/10.1016/S0024-3795(00)00119-1}{\em Linear Algebra Appl.}, 315(1--3):61--81, 2000.
		
		%	\bibitem{I66}
		%	A.W. Ingleton.
		%	\newblock A problem in linear inequalities.
		%	\newblock \href{https://doi.org/10.1112/plms/s3-16.1.519}{\em Proc. London Math. Soc. (3)}, 16:519--536, 1966.
		
		%	\bibitem{I70}
		%	A.W. Ingleton.
		%	\newblock The linear complementarity problem.
		%	\newblock \href{https://doi.org/10.1112/jlms/s2-2.2.330}{\em J. London Math. Soc. (2)}, 2:330--336, 1970.
		
		\bibitem{Jaf-Sour86}
		A.A.~Jafarian and A.R.~Sourour.
		\newblock Spectrum-preserving linear maps.
		\newblock \href{https://www.sciencedirect.com/science/article/pii/002212368690073X}
		{\em J. Funct. Anal.}, 66(2):255--261, 1986.
		
		\bibitem{Karlin64}
		S.~Karlin.
		\newblock Total positivity, absorption probabilities and applications.
		\newblock \href{https://doi.org/10.2307/1993667}{\em Trans. Amer. Math. Soc.}, 111:33--107, 1964.
		
		\bibitem{K68}
		S.~Karlin.
		\newblock {\em Total positivity. {V}ol. {I}}.
		\newblock Stanford University Press, Stanford, CA, 1968.
		
		\bibitem{Karlinsplines}
		S.~Karlin and Z.~Ziegler.
		\newblock Chebyshevian spline functions.
		\newblock \href{http://dx.doi.org/10.1137/0703044}{\em SIAM J.\ Numer.\
			Anal.}, 3(3):514--543, 1966.
		
		\bibitem{Khare20}
		A.~Khare.
		\newblock  Critical exponents for total positivity, individual kernel encoders, and the Jain-Karlin-Schoenberg kernel. {\em Prepint,}
		\newblock \href{https://arxiv.org/abs/2008.05121v1}{ 	 	arXiv:2008.05121}, 2020.
		
		\bibitem{KW14}
		Y.~Kodama and L.~Williams.
		\newblock K{P} solitons and total positivity for the {G}rassmannian.
		\newblock \href{http://dx.doi.org/10.1007/s00222-014-0506-3}%
		{\em Invent.\ Math.}, 198(3):637--699, 2014.
		
		\bibitem{Laguerre}
		E.~Laguerre.
		\newblock M\'emoire sur la th\'eorie des \'equations num\'eriques.
		\href{http://sites.mathdoc.fr/JMPA/PDF/JMPA_1883_3_9_A5_0.pdf}{\em J.\
			Math.\ Pures Appl.}, 9:9--146, 1883.
		
		%		\bibitem{L65}
		%		C.E.~Lemke.
		%		\newblock Bimatrix equilibrium points and mathematical programming.
		%		\newblock \href{https://doi.org/10.1016/0024-3795(68)90052-9}{\em Management Science}, 11(7):681--689, 1965.
		
		\bibitem{Li-Pierce01}
		Chi--K.~Li and S.~Pierce.
		\newblock Linear preserver problems.
		\newblock \href{https://www.tandfonline.com/doi/abs/10.1080/00029890.2001.11919790}
		{\em Amer. Math. Monthly}, 108(7):591--605, 2001.
		
		\bibitem{Lo55}
		C.~Loewner.
		\newblock On totally positive matrices.
		\newblock \href{https://doi.org/10.1007/BF01187945}{\em Math. Z.}, 63:338--340, 1955.
		
		\bibitem{Lu94}
		G.~Lusztig.
		\newblock Total positivity in reductive groups.
		\newblock In {\em Lie theory and geometry}, volume 123 of {\em Progr. Math.},
		pages 531--568. Birkh\"{a}user, Boston, MA, 1994.
		
		%	\bibitem{MS19}
		%		M.~Margaliot and E.D.~Sontag.
		%		\newblock Revisiting totally positive differential systems: a tutorial and new results.
		%			\newblock \href{https://doi.org/10.1016/j.automatica.2018.11.016}
		%		{\em Automatica J. IFAC}, 101:1--14, 2019.
		
		\bibitem{Marcus59}
		M.~Marcus.
		\newblock All linear operators leaving the unitary group invariant.
		\newblock \href{https://projecteuclid.org/journals/duke-mathematical-journal/volume-26/issue-1/All-linear-operators-leaving-the-unitary-group-invariant/10.1215/S0012-7094-59-02615-8.short}
		{\em Duke Math. J.}, 26:55--163, 1959.
		
		\bibitem{Molnar07}
		L.~Moln\'{a}r.
		\newblock {\em Selected preserver problems on algebraic structures of linear operators and on function spaces. {V}ol. {1895}}.
		\newblock Springer-Verlag, Berlin, 2007.
		
		\bibitem{Mot36}
		T.S.~Motzkin.
		\newblock {\em Beitr\"{a}ge zur Theorie der linearen Ungleichungen}.
		\newblock PhD dissert., Basel, 1933 and Jerusalem, 1936.
		
		\bibitem{Om-Sem93}
		M.~Omladi\v{c} and P.~\v{S}emrl.
		\newblock Additive mappings preserving operators of rank one.
		\newblock \href{https://www.sciencedirect.com/science/article/pii/002437959390502F}
		{\em Linear Algebra Appl.}, 182:239--256, 1993.
		
		%	\bibitem{M68}
		%	K.G. Murty.
		%	\newblock On the number of solutions to the complementary quadratic programming problem.
		%	\newblock \href{https://apps.dtic.mil/sti/pdfs/AD0675385.pdf}{ORC 68-17}, Operations Research Center, University of California, Berkeley, (June 1968).
		
		\bibitem{pinkus}
		A.~Pinkus.
		\newblock {\em Totally positive matrices}.
		\newblock \href{http://dx.doi.org/10.1017/CBO9780511691713}{Cambridge
			Tracts in Mathematics, Vol.~181}, Cambridge University Press, Cambridge,
		2010.
		
		\bibitem{PS1925}
		G. P\'{o}lya and G. Szeg\H{o}.
		\newblock {\em Problems and theorems in analysis. {II}}.
		\newblock
		\href{https://doi.org/10.1007/978-3-642-61905-2_7}{Classics in Mathematics}, Springer-Verlag, Berlin, 1998.
		
		\bibitem{Radja-Sedd-Tag01}
		M.~Radjabalipour, K.~Seddighi, and Y.~Taghavi.
		\newblock Additive mappings on operator algebras preserving absolute values.
		\newblock \href{https://www.sciencedirect.com/science/article/pii/S0024379500003384}
		{\em Linear Algebra Appl.}, 327(1-3):197--206, 2001.
		
		\bibitem{Ri03}
		K.C.~Rietsch.
		\newblock Totally positive {T}oeplitz matrices and quantum cohomology of
		partial flag varieties.
		\newblock \href{http://dx.doi.org/10.1090/S0894-0347-02-00412-5}%
		{\em J.\ Amer.\ Math.\ Soc.}, 16(2):363--392, 2003.
		
		\bibitem{Russo-Dye66}
		B.~Russo and H.A.~Dye.
		\newblock A note on unitary operators in $C^*$-algebras.
		\newblock \href{https://projecteuclid.org/journals/duke-mathematical-journal/volume-33/issue-2/A-note-on-unitary-operators-in-C-algebras/10.1215/S0012-7094-66-03346-1.short}
		{\em Duke Math. J.}, 33:413--416, 1966. 
		
		%	\bibitem{STW58}
		%	H.~Samelson, R.M.~Thrall, and O.~Wesler.
		%	\newblock A partition theorem for {E}uclidean {$n$}-space.
		%	\newblock \href{https://doi.org/10.2307/2033091}%
		%	{\em Proc. Amer. Math. Soc.}, 9:805--807, 1958.
		
		\bibitem{S30}
		I.J.~Schoenberg.
		\newblock \"{U}ber variationsvermindernde lineare {T}ransformationen.
		\newblock \href{http://dx.doi.org/10.1007/BF01194637}{\em Math.\ Z.},
		32:321--328, 1930.
		
		\bibitem{Schoenberg46}
		I.J.~Schoenberg.
		\newblock Contributions to the problem of approximation of equidistant
		data by analytic functions. Part A. On the problem of smoothing or
		graduation. A first class of analytic approximation formulae.
		\newblock \href{http://dx.doi.org/10.1090/qam/15914}{\em Quart.\ Appl.\
			Math.}, 4(1):45--99, 1946.
		
		\bibitem{S55}
		I.J.~Schoenberg.
		\newblock On the zeros of the generating functions of multiply positive
		sequences and functions.
		\newblock \href{http://dx.doi.org/10.2307/1970073}{\em Ann.\ of Math.\
			(2)}, 62(3):447--471, 1955.
		
		\bibitem{SW53}
		I.J.~Schoenberg and A.M.~Whitney.
		\newblock On P\'olya frequency functions. III. The positivity of translation determinants with an application to the interpolation problem by spline curves.
		\newblock \href{https://doi.org/10.2307/1990881 }{\em Trans. Amer. Math. Soc.}, 74:246--259, 1953.
		
		\bibitem{Sour96}
		A.R.~Sourour.
		\newblock Invertibility preserving linear maps on $\mathcal{L}(X)$.
		\newblock \href{https://www.ams.org/journals/tran/1996-348-01/S0002-9947-96-01428-6/}
		{\em Trans. Amer. Math. Soc.}, 348:13--30, 1996. 
		
		\bibitem{Whitney}
		A.M.~Whitney.
		\newblock A reduction theorem for totally positive matrices.
		\newblock \href{http://dx.doi.org/10.1007/BF02786969}%
		{\em J.\ d'Analyse Math.}, 2(1):88--92, 1952.
		
	\end{thebibliography}
\end{document}
