% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%
\usepackage{color}

\usepackage{algorithmicx}
\usepackage{algorithm, algpseudocode}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\usepackage{makecell}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{mathrsfs}
\usepackage[symbol]{footmisc}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}


% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
\renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{Pathology-and-genomics Multimodal Transformer for Survival Outcome Prediction}
%\thanks{xxxxxxxx}}
% \thanks{This study has been partially supported by fund of STCSM (19511121400)}}

\titlerunning{Pathology-and-genomics Multimodal Transformer for Survival Prediction}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Kexin Ding\inst{1}\and Mu Zhou\inst{2}\and Dimitris N. Metaxas\inst{2}\and Shaoting Zhang\inst{3}}

% 1{Ding, Kexin}
% 2{Zhou, Mu}
% 3{Metaxas, Dimitris}
% 5{Zhang, Shaoting}

\authorrunning{K. Ding et al.}
% \authorrunning{Anonymous Author(s)}

% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%

\institute{Department of Computer Science, UNC at Charlotte, North Carolina, USA \and Department of Computer Science, Rutgers University, New Jersey, USA \and Shanghai Artificial Intelligence Laboratory, Shanghai, China\\\email{zhangshaoting@pjlab.org.cn}}
% 
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%

%
\maketitle              % typeset the header of the contribution
%

\begin{abstract}

Survival outcome assessment is challenging and inherently associated with multiple clinical factors (e.g., imaging and genomics biomarkers) in cancer. Enabling multimodal analytics promises to reveal novel predictive patterns of patient outcomes. In this study, we propose a multimodal transformer (\textbf{PathOmics}) integrating pathology and genomics insights into colon-related cancer survival prediction. We emphasize the unsupervised pretraining to capture the intrinsic interaction between tissue microenvironments in gigapixel whole slide images (WSIs) and a wide range of genomics data (e.g., mRNA-sequence, copy number variant, and methylation). After the multimodal knowledge aggregation in pretraining, our task-specific model finetuning could expand the scope of data utility applicable to both multi- and single-modal data (e.g., image- or genomics-only). We evaluate our approach on both TCGA colon and rectum cancer cohorts, showing that the proposed approach is competitive and outperforms state-of-the-art studies. Finally, our approach is desirable to utilize the limited number of finetuned samples towards data-efficient analytics for survival outcome prediction. The code is available at \url{https://github.com/Cassie07/PathOmics}.


\keywords{Histopathological image analysis \and Multimodal learning \and Cancer diagnosis \and Survival prediction}



\end{abstract}


\section{Introduction}

Cancers are a group of heterogeneous diseases reflecting deep interactions between pathological and genomics variants in tumor tissue environments~\cite{ref_article22}. Different cancer genotypes are translated into pathological phenotypes that could be assessed by pathologists~\cite{ref_article22}. High-resolution pathological images have proven their unique benefits for improving prognostic biomarkers prediction via exploring the tissue microenvironmental features~\cite{ref_article2,ref_article3,ref_article4,ref_article5,ref_article6,ref_article30}. Meanwhile, genomics data (e.g., mRNA-sequence) display a high relevance to regulate cancer progression~\cite{ref_article8,ref_article9}. For instance, genome-wide molecular portraits are crucial for cancer prognostic stratification and targeted therapy~\cite{ref_article27}. Despite their importance, seldom efforts jointly exploit the multimodal value between cancer image morphology and molecular biomarkers. In a broader context, assessing cancer prognosis is essentially a multimodal task in association with pathological and genomics findings. Therefore, synergizing multimodal data could deepen a cross-scale understanding towards improved patient prognostication.

The major goal of multimodal data learning is to extract complementary contextual information across modalities~\cite{ref_article1}. Supervised studies~\cite{ref_article12,ref_article13,ref_article11} have allowed multimodal data fusion among image and non-image biomarkers. For instance, the Kronecker product is able to capture the interactions between WSIs and genomic features for survival outcome prediction~\cite{ref_article12,ref_article13}. Alternatively, the co-attention transformer~\cite{ref_article11} could capture the genotype-phenotype interactions for prognostic prediction. Yet these supervised approaches are limited by feature generalizability and have a high dependency on data labeling. To alleviate label requirement, unsupervised learning evaluates the intrinsic similarity among multimodal representations for data fusion. For example, integrating image, genomics, and clinical information can be achieved via a predefined unsupervised similarity evaluation~\cite{ref_article1}. To broaden the data utility, the study ~\cite{ref_article10} leverages the pathology and genomic knowledge from the teacher model to guide the pathology-only student model for glioma grading. From these analyses, it is increasingly recognized that the lack of flexibility on model finetuning limits the data utility of multimodal learning. Meanwhile, the size of multimodal medical datasets is not as large as natural vision-language datasets, which necessitates the need for data-efficient analytics to address the training difficulty.

To tackle above challenges, we propose a pathology-and-genomics multimodal framework (i.e., \textbf{PathOmics}) for survival prediction (Fig~\ref{fig1}). We summarized our contributions as follows. \textbf{(1) Unsupervised multimodal data fusion.} Our unsupervised pretraining exploits the intrinsic interaction between morphological and molecular biomarkers (Fig~\ref{fig1}a). To overcome the gap of modality heterogeneity between images and genomics, we project the multimodal embeddings into the same latent space by evaluating the similarity among them. Particularly, the pretrained model offers a unique means by using similarity-guided modality fusion for extracting cross-modal patterns. \textbf{(2) Flexible modality finetuning.} A key contribution of our multimodal framework is that it combines benefits from both unsupervised pretraining and supervised finetuning data fusion (Fig~\ref{fig1}b). As a result, the task-specific finetuning broadens the dataset usage (Fig~\ref{fig1}b and c), which is not limited by data modality (e.g., both single- and multi-modal data). \textbf{(3) Data efficiency with limited data size.} Our approach could achieve comparable performance even with fewer finetuned data (e.g., only use 50\% of the finetuned data) when compared with using the entire finetuning dataset. 



% Figure environment removed

\section{Methodology}

\subsubsection{Overview.} Fig~\ref{fig1} illustrates our multimodal transformer framework. Our method includes an unsupervised multimodal data fusion pretraining and a supervised flexible-modal finetuning. From Fig~\ref{fig1}a, in the pretraining, our unsupervised data fusion aims to capture the interaction pattern of image and genomics features. Overall, we formulate the objective of multimodal feature learning by converting image patches and tabular genomics data into group-wise embeddings, and then extracting multimodal patient-wise embeddings. More specifically, we construct group-wise representations for both image and genomics modalities. For image feature representation, we randomly divide image patches into groups; Meanwhile, for each type of genomics data, we construct groups of genes depending on their clinical relevance~\cite{ref_article20}. Next, as seen in Fig~\ref{fig1}b and c, our approach enables three types of finetuning modal modes (i.e., multimodal, image-only, and genomics-only) towards prognostic prediction, expanding the downstream data utility from the pretrained model.  



\subsubsection{Group-wise Image and Genomics Embedding.}
We define the group-wise genomics representation by referring to $N=8$ major functional groups obtained from~\cite{ref_article20}. Each group contains a list of well-defined molecular features related to cancer biology, including transcription factors, tumor suppression, cytokines and grow\-th factors, cell differentiation markers, homeodomain proteins, translocated cancer genes, and protein kinases. The group-wise genomics representation is defined as $G_{n} \in \mathbb{R}^{1\times d_{g}}$, where $n \in N$, $d_{g}$ is the attribute dimension in each group which could be various. To better extract high-dimensional group-wise genomics representation, we use a Self-Normalizing Network (SNN) together with scaled exponential linear units (SeLU) and Alpha Dropout for feature extraction to generate the group-wise embedding $G_{n} \in \mathbb{R}^{1\times 256}$ for each group.

For group-wise WSIs representation, we first cropped all tissue-region image tiles from the entire WSI and extracted CNN-based (e.g., ResNet50) $d_{i}$-dimensional features for each image tile k as $h_{k} \in \mathbb{R}^{1 \times d_{i}}$, where $d_{i}=1,024$, $k \in K$ and K is the number of image patches. We construct the group-wise WSIs representation by randomly splitting image tile features into N groups (i.e., the same number as genomics categories). Therefore, group-wise image representation could be defined as $I_{n} \in \mathbb{R}^{k_{n}\times1024}$, where $n \in N$ and $k_{n}$ represents tile k in group n. Then we apply an attention-based refiner (ABR)~\cite{ref_article21}, which is able to weight the feature embeddings in the group, together with a dimension deduction (e.g., fully-connected layers) to achieve the group-wise embedding. The ABR and the group-wise embedding $I_{n} \in \mathbb{R}^{1\times256}$ are defined as:

\begin{equation}
a_{k} = \frac{epx\{ w^{T}(tanh(V_{1}h_{k})\odot (sigm(V_{2}h_{k}))\}}{\sum_{j=1}^{K} epx\{ w^{T}(tanh(V_{1}h_{j})\odot (sigm(V_{2}h_{j}))\}}
\label{eq1}
\end{equation}
where w,V1 and V2 are the learnable parameters.

\begin{equation}
I_{n} = \sum_{k=1}^{K} a_{k}h_{k}
\label{eq2}
\end{equation}



\subsubsection{Patient-wise Multimodal Feature Embedding.}
To aggregate patient-wise multimodal feature embedding from the group-wise representations, as shown in Fig~\ref{fig1}a, we propose a pathology-and-genomics multimodal model containing two model streams, including a pathological image and a genomics data stream. In each stream, we use the same architecture with different weights, which is updated separately in each modality stream. In the pathological image stream, the patient-wise image representation is aggregated by N group representations as $I_{p} \in \mathbb{R}^{N\times 256}$, where $p \in P$ and P is the number of patients. Similarly, the patient-wise genomics representation is aggregated as $G_{p} \in \mathbb{R}^{N\times 256}$. After generating patient-wise representation, we utilize two transformer layers~\cite{ref_article24} to extract feature embeddings for each modality as follows:

\begin{equation}
H_{p}^{l} = MSA(H_{p})
\end{equation}

where MSA denotes Multi-head Self-attention~\cite{ref_article24} (see Appendix 1), l denotes the layer index of the transformer, and $H_{p}$ could either be $I_{p}$ or $G_{p}$. Then, we construct global attention poolings~\cite{ref_article21} as Eq.\ref{eq1} to adaptively compute a weighted sum of each modality feature embeddings to finally construct patient-wise embedding as $I_{embedding}^{p} \in \mathbb{R}^{1\times256}$ and $G_{embedding}^{p} \in \mathbb{R}^{1\times256}$ in each modality. 

\subsubsection{Multimodal Fusion in Pretraining and Finetuning.} Due to the domain gap between image and molecular feature heterogeneity, a proper design of multimodal fusion is crucial to advance integrative analysis. In the pretraining stage, we develop an unsupervised data fusion strategy by decreasing the mean square error (MSE) loss to map images and genomics embeddings into the same space. Ideally, the image and genomics embeddings belonging to the same patient should have a higher relevance between each other. MSE measures the average squared difference between multimodal embeddings. In this way, the pretrained model is trained to map the paired image and genomics embeddings to be closer in the latent space, leading to strengthen the interaction between different modalities.

\begin{equation}
\mathcal{L}_{fusion} = argmin \frac{1}{P}\sum_{p=1}^{P}((I_{embedding}^{p} - G_{embedding}^{p})^{2})
\end{equation}
In the single modality finetuning, even if we use image-only data, the model is able to produce genomic-related image feature embedding due to the multimodal knowledge aggregation already obtained from the model pretraining. As a result, our cross-modal information aggregation relaxes the modality requirement in the finetuning stage. As shown in Fig~\ref{fig1}b, for multimodal finetuning, we deploy a concatenation layer to obtain the fused multimodal feature representation and implement a risk classifier (FC layer) to achieve the final survival stratification (see Appendix 2). As for single-modality finetuning mode in Fig~\ref{fig1}c, we simply feed $I_{embedding}^{p}$ or $G_{embedding}^{p}$ into risk classifier for the final prognosis prediction. During the finetuning, we update the model parameters using a log-likelihood loss for the discrete-time survival model training~\cite{ref_article11}(see Appendix 2).


\section{Experiments and Results}
\subsubsection{Datasets.} 

All image and genomics data are publicly available. We collected WSIs from The Cancer Genome Atlas Colon Adenocarcinoma (TCGA-COAD) dataset (CC-BY-3.0)~\cite{ref_article14,ref_article29} and Rectum Adenocarcinoma (TCGA-READ) dataset (CC-BY-3.0)~\cite{ref_article16,ref_article29}, which contain 440 and 153 patients. We cropped each WSI into 512 $\times$ 512 non-overlapped patches. We also collected the corresponding tabular genomics data (e.g., mRNA sequence, copy number alteration, and methylation) with overall survival (OS) times and censorship statuses from Cbioportal~\cite{ref_article17,ref_article18}. We removed the samples without the corresponding genomics data or ground truth of survival outcomes. Finally, we included 426 patients of TCGA-COAD and 145 patients of TCGA-READ.


% Figure environment removed

\subsubsection{Experimental Settings and Implementations.} 
% Pretrain and finetune training process
We implement two types of settings that involve internal and external datasets for model pretraining and finetuning. As shown in Fig~\ref{fig2}a, we pretrain and finetune the model on the same dataset (i.e., internal setting). We split TCGA-COAD into training (80\%) and holdout testing set (20\%). Then, we implement four-fold cross-validation on the training set for pretraining, finetuning, and hyperparameter-tuning. The test set is only used for evaluating the best finetuned models from each cross-validation split. For the external setting, we implement pretraining and finetuning on the different datasets, as shown in Fig~\ref{fig2}b; we use TCGA-COAD for pretraining; Then, we only use TCGA-READ for finetuning and final evaluation. We implement a five-fold cross-validation for pretraining, and the best pretrained models are used for finetuning. We split TCGA-READ into finetuning (60\%), validation (20\%), and evaluation set (20\%). For all experiments, we calculate the average performance on the evaluation set across the best models. 

The number of epochs for pretraining and finetuning are 25, the batch size is 1, the optimizer is Adam~\cite{ref_article19}, and the learning rate is 1e-4 for pretraining and 5e-5 for finetuning. We used one 32GB Tesla V100 SXM2 GPU and Pytorch. The concordance index (C-index) is used to measure the survival prediction performance. We followed the previous studies~\cite{ref_article11,ref_article12,ref_article13} to partition the overall survival (OS) months into four non-overlapping intervals by using the quartiles of event times of uncensored patients for discretized-survival C-index calculation (see Appendix 2). For each experiment, we reported the average C-index among three-times repeated experiments. Conceptionally, our method shares a similar idea to multiple instance learning (MIL)~\cite{ref_article25,ref_article26}. Therefore, we include two types of baseline models, including the MIL-based models (DeepSet~\cite{ref_article23}, AB-MIL~\cite{ref_article21}, and TransMIL~\cite{ref_article28}) and MIL multimodal-based models (MCAT~\cite{ref_article11}, PORPOISE~\cite{ref_article13}). We follow the same data split and processing, as well as the identical training hyperparameters and supervised fusion as above. Notably, there is no need for supervised finetuning for the baselines when using TCGA-COAD (Table~\ref{tab1}), because the supervised pretraining is already applied to the training set.



\begin{table}[t!]
\centering
\caption{The comparison of C-index performance on TCGA-COAD and TCGA-READ dataset. "Methy" is used as the abbreviation of Methylation.}\label{tab1}
\begin{tabular}{c|c|c|c|c|c}
\hline

Model & \makecell{Pretrain\\data modality} & \multicolumn{2}{c|}{TCGA-COAD} & \multicolumn{2}{c}{TCGA-READ}\\
 \cline{3-6}
 &  & \makecell{Finetune\\data modality} & C-index (\%) & \makecell{Finetune\\data modality} & C-index (\%)\\
\hline
 & image+mRNA & - & $58.70\pm 1.10 $ & image+mRNA &$70.19\pm1.45$\\
 % \cline{2-6}
\makecell{DeepSets\\~\cite{ref_article23}} & image+CNA & - & $51.50\pm 2.60$& image+CNA & $62.50\pm2.52$\\
 % \cline{2-6}
 & image+Methy & - & $65.61\pm1.86$& \makecell{image+Methy} & $55.78\pm1.22$\\
\hline
 & image+mRNA &  - & $54.12\pm 2.88$& image+mRNA & $68.79\pm1.44$\\
% \cline{2-6}
\makecell{AB-MIL\\~\cite{ref_article21}} & image+CNA & - & $54.68\pm 2.44$ & image+CNA & $66.72\pm0.81$\\
 % \cline{2-6}
 & image+Methy & - & $49.66\pm 1.58$& image+Methy & $55.78\pm1.22$\\
\hline
 & image+mRNA & -  & $54.15 \pm1.02$ & image+mRNA  & $67.91\pm2.35$\\
 % \cline{2-6}
\makecell{TransMIL\\~\cite{ref_article28}} & image+CNA& -  & $59.80\pm0.98$ & image+CNA & $62.75\pm1.92$\\
% \cline{2-6}
 & image+Methy & - & $53.35\pm1.78$ & image+Methy & $53.09\pm1.46$\\
\hline
 & image+mRNA &  - & $65.02\pm 3.10$& image+mRNA & $70.27\pm2.75$\\
% \cline{2-6}
\makecell{MCAT\\~\cite{ref_article11}} & image+CNA & - &$64.66\pm 2.31$& image+CNA &$60.50\pm1.25$\\
% \cline{2-6}
 & image+Methy & - & $60.98\pm2.43$& image+Methy & $59.78\pm1.20$\\
\hline
 & image+mRNA & -  & $65.31\pm1.26$ & image+mRNA  & $68.18\pm1.62$\\
 % \cline{2-6}
\makecell{PORPOI\\-SE~\cite{ref_article13}} & image+CNA& -  & $57.32\pm1.78$ & image+CNA & $60.19\pm1.48$\\
% \cline{2-6}
 & image+Methy & - & $61.84\pm1.10$ & image+Methy & $68.80\pm0.92$\\
\hline
\hline
 \multirow{9}{*}{Ours} & \multirow{3}{*}{image+mRNA}  & image+mRNA & $\mathbf{67.32\pm 1.69}$ & image+mRNA& $74.35\pm1.15$\\
 % \cline{3-6}
 &  & image    & $63.78\pm1.22$& image& $\mathbf{74.85\pm 0.37}$\\
 % \cline{3-6}
 &  & mRNA    & $60.76\pm0.88$& mRNA & $59.61\pm1.37$\\
\cline{2-6}
 &\multirow{3}{*}{image+CNA}& image+CNA & $61.19\pm 1.03$ & image+CNA & $73.95\pm1.05$\\
 % \cline{3-6}
 &  & image    & $58.06\pm1.54$& image & $71.18\pm1.39$\\
 % \cline{3-6}
 &  & CNA    & $56.43\pm1.02$& CNA & $63.95\pm0.55$\\
\cline{2-6}
 & \multirow{3}{*}{image+Methy}& image+Methy & $67.22\pm1.67$& image+Methy & $71.80\pm2.03$\\
 % \cline{3-6}
 &  & image & $60.43\pm0.72$& image & $64.42\pm0.72$\\
 % \cline{3-6}
 &  & Methy    & $61.06\pm1.34$ & Methy & $65.42\pm0.91$\\
\hline

\end{tabular}
\end{table} 

\subsubsection{Results.}
In Table~\ref{tab1}, our approach shows improved survival prediction performance on both TCGA-COAD and TCGA-READ datasets. Compared with supervised baselines, our unsupervised data fusion is able to extract the phenotype-genotype interaction features, leading to achieving a flexible finetuning for different data settings. With the multimodal pretraining and finetuning, our method outperforms state-of-the-art models by about 2\% on TCGA-COAD and 4\% TCGA-READ. We recognize that the combination of image and mRNA sequencing data leads to reflecting distinguishing survival outcomes. Remarkably, our model achieved positive results even using a single-modal finetuning when compared with baselines (more results in Appendix 3.1). In the meantime, on the TCGA-READ, our single-modality finetuned model achieves a better performance than multimodal finetuned baseline models (e.g., with model pretraining via image and methylation data, we have only used the image data for finetuning and achieved a C-index of 74.85\%, which is about 4\% higher than the best baseline models). We show that with a single-modal finetuning strategy, the model could generate meaningful embedding to combine image- and genomic-related patterns. In addition, our model reflects its efficiency on the limited finetuning data (e.g., 75 patients are used for finetuning on TCGA-READ, which are only 22\% of TCGA-COAD finetuning data). In Table~\ref{tab1}, our method could yield better performance compared with baselines on the small dataset across the combination of images and multiple types of genomics data.

% Figure environment removed

\subsubsection{Ablation Analysis.}
We verify the model efficiency by using fewer amounts of finetuning data in finetuning. For TCGA-COAD dataset, we include 50\%, 25\%, and 10\% of the finetuning data. For the TCGA-READ dataset, as the number of uncensored patients is limited, we use 75\%, 50\%, and 25\% of the finetuning data to allow at least one uncensored patient to be included for finetuning. As shown in Fig~\ref{fig3}a, by using 50\% of TCGA-COAD finetuning data, our approach achieves the C-index of 64.80\%, which is higher than the average performance of baselines in several modalities. Similarly, in Fig~\ref{fig3}b, our model retains a good performance by using 50\% or 75\% of TCGA-READ finetuning data compared with the average of C-index across baselines (e.g., 72.32\% versus 64.23\%). For evaluating the effect of cross-modality information extraction in the pretraining, we kept supervised model training (i.e., the finetuning stage) while removing the unsupervised pretraining. The performance is lower 2\%-10\% than ours on multi- and single-modality data. For evaluating the genomics data usage, we designed two settings: (1) combining all types of genomics data and categorizing them by groups; (2) removing category information while keeping using different types of genomics data separately. Our approach outperforms the above ablation studies by 3\%-7\% on TCGA-READ and performs similarly on TCGA-COAD. In addition, we replaced our unsupervised loss with cosine similarity loss; our approach outperforms the setting of using cosine similarity loss by 3\%-6\%.


\section{Conclusion}
Developing data-efficient multimodal learning is crucial to advance the survival assessment of cancer patients in a variety of clinical data scenarios. We demonstrated that the proposed PathOmics framework is useful for improving the survival prediction of colon and rectum cancer patients. Importantly, our approach opens up perspectives for exploring the key insights of intrinsic genotype-phenotype interactions in complex cancer data across modalities. Our finetuning approach broadens the scope of dataset inclusion, particularly for model finetuning and evaluation, while enhancing model efficiency on analyzing multimodal clinical data in real-world settings. In addition, the use of synthetic data and developing a foundation model training will be helpful to improve the robustness of multimodal data fusion~\cite{ref_article31,ref_article32}.

%\textcolor{red}{We plan to improve model interpretability with a flexible pretraining to overcome the multimodal challenge.}

\subsubsection{Acknowledgements.} The results of this study are based on the data collected from the public TCGA Research Network: \url{https://www.cancer.gov/tcga}.

% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{Reference1847}

\input{Supplementary1847}

\end{document}



