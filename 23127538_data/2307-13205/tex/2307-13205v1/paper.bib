@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}
@inproceedings{tsai2019multimodal,
  title={Multimodal transformer for unaligned multimodal language sequences},
  author={Tsai, Yao-Hung Hubert and Bai, Shaojie and Liang, Paul Pu and Kolter, J Zico and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  booktitle={Proceedings of the conference. Association for Computational Linguistics. Meeting},
  volume={2019},
  pages={6558},
  year={2019},
  organization={NIH Public Access}
}
@inproceedings{hazarika2020misa,
  title={Misa: Modality-invariant and-specific representations for multimodal sentiment analysis},
  author={Hazarika, Devamanyu and Zimmermann, Roger and Poria, Soujanya},
  booktitle={Proceedings of the 28th ACM International Conference on Multimedia},
  pages={1122--1131},
  year={2020}
}
@inproceedings{yu2021learning,
  title={Learning modality-specific representations with self-supervised multi-task learning for multimodal sentiment analysis},
  author={Yu, Wenmeng and Xu, Hua and Yuan, Ziqi and Wu, Jiele},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  pages={10790--10797},
  year={2021}
}
@article{wu2023denoising,
  title={Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion},
  author={Wu, Shaoxaing and Dai, Damai and Qin, Ziwei and Liu, Tianyu and Lin, Binghuai and Cao, Yunbo and Sui, Zhifang},
  journal={arXiv preprint arXiv:2305.14652},
  year={2023}
}
@inproceedings{lv2021progressive,
  title={Progressive modality reinforcement for human multimodal emotion recognition from unaligned multimodal sequences},
  author={Lv, Fengmao and Chen, Xiang and Huang, Yanyong and Duan, Lixin and Lin, Guosheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2554--2562},
  year={2021}
}
@inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics},
  pages={2236--2246},
  year={2018}
}
@article{zadeh2016mosi,
  title={Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos},
  author={Zadeh, Amir and Zellers, Rowan and Pincus, Eli and Morency, Louis-Philippe},
  journal={arXiv preprint arXiv:1606.06259},
  year={2016}
}
@inproceedings{yang2022disentangled,
  title={Disentangled Representation Learning for Multimodal Emotion Recognition},
  author={Yang, Dingkang and Huang, Shuai and Kuang, Haopeng and Du, Yangtao and Zhang, Lihua},
  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
  pages={1642--1651},
  year={2022}
}
@inproceedings{yang2022learning, 
author = {Yang, Dingkang and Kuang, Haopeng and Huang, Shuai and Zhang, Lihua}, 
title = {Learning Modality-Specific and -Agnostic Representations for Asynchronous Multimodal Language Sequences},
booktitle={Proceedings of the 30th ACM International Conference on Multimedia}, 
year = {2022}, 
pages = {1708â€“1717}, 
numpages = {10} 
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@inproceedings{lee2019context,
  title={Context-aware emotion recognition networks},
  author={Lee, Jiyoung and Kim, Seungryong and Kim, Sunok and Park, Jungin and Sohn, Kwanghoon},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10143--10152},
  year={2019}
}
@inproceedings{mittal2020emoticon,
  title={Emoticon: Context-aware multimodal emotion recognition using frege's principle},
  author={Mittal, Trisha and Guhan, Pooja and Bhattacharya, Uttaran and Chandra, Rohan and Bera, Aniket and Manocha, Dinesh},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14234--14243},
  year={2020}
}
@article{tsai2018learning,
  title={Learning factorized multimodal representations},
  author={Tsai, Yao-Hung Hubert and Liang, Paul Pu and Zadeh, Amir and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1806.06176},
  year={2018}
}
@inproceedings{wang2019words,
  title={Words can shift: Dynamically adjusting word representations using nonverbal behaviors},
  author={Wang, Yansen and Shen, Ying and Liu, Zhun and Liang, Paul Pu and Zadeh, Amir and Morency, Louis-Philippe},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={7216--7223},
  year={2019}
}
@inproceedings{pham2019found,
  title={Found in translation: Learning robust joint representations by cyclic translations between modalities},
  author={Pham, Hai and Liang, Paul Pu and Manzini, Thomas and Morency, Louis-Philippe and P{\'o}czos, Barnab{\'a}s},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={6892--6899},
  year={2019}
}
@inproceedings{wu2021text,
  title={A text-centered shared-private framework via cross-modal prediction for multimodal sentiment analysis},
  author={Wu, Yang and Lin, Zijie and Zhao, Yanyan and Qin, Bing and Zhu, Li-Nan},
  booktitle={Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages={4730--4738},
  year={2021}
}
@article{dai2018transformer,
  title={Transformer-xl: Language modeling with longer-term dependency},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Cohen, William W and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  year={2018}
}
@article{baevski2018adaptive,
  title={Adaptive input representations for neural language modeling},
  author={Baevski, Alexei and Auli, Michael},
  journal={arXiv preprint arXiv:1809.10853},
  year={2018}
}
@article{strubell2018linguistically,
  title={Linguistically-informed self-attention for semantic role labeling},
  author={Strubell, Emma and Verga, Patrick and Andor, Daniel and Weiss, David and McCallum, Andrew},
  journal={arXiv preprint arXiv:1804.08199},
  year={2018}
}
@article{tang2018self,
  title={Why self-attention? a targeted evaluation of neural machine translation architectures},
  author={Tang, Gongbo and M{\"u}ller, Mathias and Rios, Annette and Sennrich, Rico},
  journal={arXiv preprint arXiv:1808.08946},
  year={2018}
}
@article{zadeh2017tensor,
  title={Tensor fusion network for multimodal sentiment analysis},
  author={Zadeh, Amir and Chen, Minghai and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  journal={arXiv preprint arXiv:1707.07250},
  year={2017}
}
@inproceedings{yu2020ch,
  title={Ch-sims: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality},
  author={Yu, Wenmeng and Xu, Hua and Meng, Fanyang and Zhu, Yilin and Ma, Yixiao and Wu, Jiele and Zou, Jiyun and Yang, Kaicheng},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={3718--3727},
  year={2020}
}
@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural Computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT press}
}
@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@inproceedings{liang2021attention,
  title={Attention is not enough: Mitigating the distribution discrepancy in asynchronous multimodal sequence fusion},
  author={Liang, Tao and Lin, Guosheng and Feng, Lei and Zhang, Yan and Lv, Fengmao},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={8148--8156},
  year={2021}
}
@article{sarzynska2021detecting,
  title={Detecting formal thought disorder by deep contextualized word representations},
  author={Sarzynska-Wawer, Justyna and Wawer, Aleksander and Pawlak, Aleksandra and Szymanowska, Julia and Stefaniak, Izabela and Jarkiewicz, Michal and Okruszek, Lukasz},
  journal={Psychiatry Research},
  volume={304},
  pages={114135},
  year={2021},
  publisher={Elsevier}
}
@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}
@article{liu2018efficient,
  title={Efficient low-rank multimodal fusion with modality-specific factors},
  author={Liu, Zhun and Shen, Ying and Lakshminarasimhan, Varun Bharadhwaj and Liang, Paul Pu and Zadeh, Amir and Morency, Louis-Philippe},
  journal={arXiv preprint arXiv:1806.00064},
  year={2018}
}
@inproceedings{rahman2020integrating,
  title={Integrating multimodal information in large pretrained transformers},
  author={Rahman, Wasifur and Hasan, Md Kamrul and Lee, Sangwu and Zadeh, Amir and Mao, Chengfeng and Morency, Louis-Philippe and Hoque, Ehsan},
  booktitle={Proceedings of the conference. Association for Computational Linguistics. Meeting},
  volume={2020},
  pages={2359},
  year={2020},
  organization={NIH Public Access}
}
@inproceedings{carion2020end,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={Proceedings of the European conference on computer vision},
  pages={213--229},
  year={2020},
  organization={Springer}
}
@inproceedings{chen2017multimodal,
  title={Multimodal sentiment analysis with word-level fusion and reinforcement learning},
  author={Chen, Minghai and Wang, Sen and Liang, Paul Pu and Baltru{\v{s}}aitis, Tadas and Zadeh, Amir and Morency, Louis-Philippe},
  booktitle={Proceedings of the 19th ACM International Conference on Multimodal Interaction},
  pages={163--171},
  year={2017}
}
@InProceedings{yang2023context, 
  author = {Yang, Dingkang and Chen, Zhaoyu and Wang, Yuzheng and Wang, Shunli and Li, Mingcheng and Liu, Siao and Zhao, Xiao and Huang, Shuai and Dong, Zhiyan and Zhai, Peng and Zhang, Lihua}, 
  title = {Context De-Confounded Emotion Recognition}, 
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  month = {June}, 
  year = {2023}, 
  pages = {19005--19015} }
@inproceedings{yang2022emotion, 
  author = {Dingkang Yang and Shuai Huang and Shunli Wang and Yang Liu and Peng Zhai and Liuzhen Su and Mingcheng Li and Lihua Zhang}, 
  title = {Emotion Recognition for Multiple Context Awareness}, 
  booktitle = {Proceedings of the European Conference on Computer Vision}, 
  volume = {13697}, 
  pages = {144--162}, 
  year = {2022} }
@article{yang2023target, 
  title={Target and source modality co-reinforcement for emotion understanding from asynchronous multimodal sequences}, 
  author={Yang, Dingkang and Liu, Yang and Huang, Can and Li, Mingcheng and Zhao, Xiao and Wang, Yuzheng and Yang, Kun and Wang, Yan and Zhai, Peng and Zhang, Lihua}, 
  journal={Knowledge-Based Systems}, 
  volume={265}, 
  pages={110370}, 
  year={2023}, 
  publisher={Elsevier} }
@article{yang2022contextual, 
  author={Yang, Dingkang and Huang, Shuai and Liu, Yang and Zhang, Lihua}, 
  journal={IEEE Signal Processing Letters}, 
  title={Contextual and Cross-Modal Interaction for Multi-Modal Speech Emotion Recognition}, 
  year={2022}, 
  volume={29}, 
  pages={2093--2097}}
@inproceedings{du2021learning,
author={Du, Yangtao and Yang, Dingkang and Zhai, Peng and Li, Mingchen and Zhang, Lihua},
booktitle={IEEE International Conference on Image Processing},
title={Learning Associative Representation for Facial Expression Recognition},
pages={889--893},
year={2021}
}