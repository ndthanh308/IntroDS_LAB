\begin{thebibliography}{10}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }
\providecommand{\doi}[1]{https://doi.org/#1}

\bibitem{chen2017multimodal}
Chen, M., Wang, S., Liang, P.P., Baltru{\v{s}}aitis, T., Zadeh, A., Morency,
  L.P.: Multimodal sentiment analysis with word-level fusion and reinforcement
  learning. In: Proceedings of the 19th ACM International Conference on
  Multimodal Interaction. pp. 163--171 (2017)

\bibitem{devlin2018bert}
Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep
  bidirectional transformers for language understanding. arXiv preprint
  arXiv:1810.04805  (2018)

\bibitem{du2021learning}
Du, Y., Yang, D., Zhai, P., Li, M., Zhang, L.: Learning associative
  representation for facial expression recognition. In: IEEE International
  Conference on Image Processing. pp. 889--893 (2021)

\bibitem{hazarika2020misa}
Hazarika, D., Zimmermann, R., Poria, S.: Misa: Modality-invariant and-specific
  representations for multimodal sentiment analysis. In: Proceedings of the
  28th ACM International Conference on Multimedia. pp. 1122--1131 (2020)

\bibitem{hochreiter1997long}
Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Computation
  \textbf{9}(8),  1735--1780 (1997)

\bibitem{kingma2014adam}
Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv
  preprint arXiv:1412.6980  (2014)

\bibitem{liang2021attention}
Liang, T., Lin, G., Feng, L., Zhang, Y., Lv, F.: Attention is not enough:
  Mitigating the distribution discrepancy in asynchronous multimodal sequence
  fusion. In: Proceedings of the IEEE/CVF International Conference on Computer
  Vision. pp. 8148--8156 (2021)

\bibitem{liu2018efficient}
Liu, Z., Shen, Y., Lakshminarasimhan, V.B., Liang, P.P., Zadeh, A., Morency,
  L.P.: Efficient low-rank multimodal fusion with modality-specific factors.
  arXiv preprint arXiv:1806.00064  (2018)

\bibitem{lv2021progressive}
Lv, F., Chen, X., Huang, Y., Duan, L., Lin, G.: Progressive modality
  reinforcement for human multimodal emotion recognition from unaligned
  multimodal sequences. In: Proceedings of the IEEE/CVF Conference on Computer
  Vision and Pattern Recognition. pp. 2554--2562 (2021)

\bibitem{mittal2020emoticon}
Mittal, T., Guhan, P., Bhattacharya, U., Chandra, R., Bera, A., Manocha, D.:
  Emoticon: Context-aware multimodal emotion recognition using frege's
  principle. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition. pp. 14234--14243 (2020)

\bibitem{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., et~al.: Pytorch: An imperative
  style, high-performance deep learning library. Advances in Neural Information
  Processing Systems  \textbf{32} (2019)

\bibitem{rahman2020integrating}
Rahman, W., Hasan, M.K., Lee, S., Zadeh, A., Mao, C., Morency, L.P., Hoque, E.:
  Integrating multimodal information in large pretrained transformers. In:
  Proceedings of the conference. Association for Computational Linguistics.
  Meeting. vol.~2020, p.~2359. NIH Public Access (2020)

\bibitem{strubell2018linguistically}
Strubell, E., Verga, P., Andor, D., Weiss, D., McCallum, A.:
  Linguistically-informed self-attention for semantic role labeling. arXiv
  preprint arXiv:1804.08199  (2018)

\bibitem{tang2018self}
Tang, G., M{\"u}ller, M., Rios, A., Sennrich, R.: Why self-attention? a
  targeted evaluation of neural machine translation architectures. arXiv
  preprint arXiv:1808.08946  (2018)

\bibitem{tsai2019multimodal}
Tsai, Y.H.H., Bai, S., Liang, P.P., Kolter, J.Z., Morency, L.P., Salakhutdinov,
  R.: Multimodal transformer for unaligned multimodal language sequences. In:
  Proceedings of the conference. Association for Computational Linguistics.
  Meeting. vol.~2019, p.~6558. NIH Public Access (2019)

\bibitem{tsai2018learning}
Tsai, Y.H.H., Liang, P.P., Zadeh, A., Morency, L.P., Salakhutdinov, R.:
  Learning factorized multimodal representations. arXiv preprint
  arXiv:1806.06176  (2018)

\bibitem{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,
  Kaiser, {\L}., Polosukhin, I.: Attention is all you need. Advances in Neural
  Information Processing Systems  \textbf{30} (2017)

\bibitem{wang2019words}
Wang, Y., Shen, Y., Liu, Z., Liang, P.P., Zadeh, A., Morency, L.P.: Words can
  shift: Dynamically adjusting word representations using nonverbal behaviors.
  In: Proceedings of the AAAI Conference on Artificial Intelligence. pp.
  7216--7223 (2019)

\bibitem{wu2021text}
Wu, Y., Lin, Z., Zhao, Y., Qin, B., Zhu, L.N.: A text-centered shared-private
  framework via cross-modal prediction for multimodal sentiment analysis. In:
  Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021.
  pp. 4730--4738 (2021)

\bibitem{yang2023context}
Yang, D., Chen, Z., Wang, Y., Wang, S., Li, M., Liu, S., Zhao, X., Huang, S.,
  Dong, Z., Zhai, P., Zhang, L.: Context de-confounded emotion recognition. In:
  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition. pp. 19005--19015 (June 2023)

\bibitem{yang2022disentangled}
Yang, D., Huang, S., Kuang, H., Du, Y., Zhang, L.: Disentangled representation
  learning for multimodal emotion recognition. In: Proceedings of the 30th ACM
  International Conference on Multimedia. pp. 1642--1651 (2022)

\bibitem{yang2022contextual}
Yang, D., Huang, S., Liu, Y., Zhang, L.: Contextual and cross-modal interaction
  for multi-modal speech emotion recognition. IEEE Signal Processing Letters
  \textbf{29},  2093--2097 (2022)

\bibitem{yang2022emotion}
Yang, D., Huang, S., Wang, S., Liu, Y., Zhai, P., Su, L., Li, M., Zhang, L.:
  Emotion recognition for multiple context awareness. In: Proceedings of the
  European Conference on Computer Vision. vol. 13697, pp. 144--162 (2022)

\bibitem{yang2022learning}
Yang, D., Kuang, H., Huang, S., Zhang, L.: Learning modality-specific and
  -agnostic representations for asynchronous multimodal language sequences. In:
  Proceedings of the 30th ACM International Conference on Multimedia. p.
  1708â€“1717 (2022)

\bibitem{yang2023target}
Yang, D., Liu, Y., Huang, C., Li, M., Zhao, X., Wang, Y., Yang, K., Wang, Y.,
  Zhai, P., Zhang, L.: Target and source modality co-reinforcement for emotion
  understanding from asynchronous multimodal sequences. Knowledge-Based Systems
   \textbf{265},  110370 (2023)

\bibitem{yu2020ch}
Yu, W., Xu, H., Meng, F., Zhu, Y., Ma, Y., Wu, J., Zou, J., Yang, K.: Ch-sims:
  A chinese multimodal sentiment analysis dataset with fine-grained annotation
  of modality. In: Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics. pp. 3718--3727 (2020)

\bibitem{yu2021learning}
Yu, W., Xu, H., Yuan, Z., Wu, J.: Learning modality-specific representations
  with self-supervised multi-task learning for multimodal sentiment analysis.
  In: Proceedings of the AAAI conference on artificial intelligence. pp.
  10790--10797 (2021)

\bibitem{zadeh2017tensor}
Zadeh, A., Chen, M., Poria, S., Cambria, E., Morency, L.P.: Tensor fusion
  network for multimodal sentiment analysis. arXiv preprint arXiv:1707.07250
  (2017)

\bibitem{zadeh2016mosi}
Zadeh, A., Zellers, R., Pincus, E., Morency, L.P.: Mosi: multimodal corpus of
  sentiment intensity and subjectivity analysis in online opinion videos. arXiv
  preprint arXiv:1606.06259  (2016)

\bibitem{zadeh2018multimodal}
Zadeh, A.B., Liang, P.P., Poria, S., Cambria, E., Morency, L.P.: Multimodal
  language analysis in the wild: Cmu-mosei dataset and interpretable dynamic
  fusion graph. In: Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics. pp. 2236--2246 (2018)

\end{thebibliography}
