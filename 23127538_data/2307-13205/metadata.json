{
  "title": "Text-oriented Modality Reinforcement Network for Multimodal Sentiment Analysis from Unaligned Multimodal Sequences",
  "authors": [
    "Yuxuan Lei",
    "Dingkang Yang",
    "Mingcheng Li",
    "Shunli Wang",
    "Jiawei Chen",
    "Lihua Zhang"
  ],
  "submission_date": "2023-07-25T02:08:28+00:00",
  "revised_dates": [],
  "abstract": "Multimodal Sentiment Analysis (MSA) aims to mine sentiment information from text, visual, and acoustic modalities. Previous works have focused on representation learning and feature fusion strategies. However, most of these efforts ignored the disparity in the semantic richness of different modalities and treated each modality in the same manner. That may lead to strong modalities being neglected and weak modalities being overvalued. Motivated by these observations, we propose a Text-oriented Modality Reinforcement Network (TMRN), which focuses on the dominance of the text modality in MSA. More specifically, we design a Text-Centered Cross-modal Attention (TCCA) module to make full interaction for text/acoustic and text/visual pairs, and a Text-Gated Self-Attention (TGSA) module to guide the self-reinforcement of the other two modalities. Furthermore, we present an adaptive fusion mechanism to decide the proportion of different modalities involved in the fusion process. Finally, we combine the feature matrices into vectors to get the final representation for the downstream tasks. Experimental results show that our TMRN outperforms the state-of-the-art methods on two MSA benchmarks.",
  "categories": [
    "cs.MM"
  ],
  "primary_category": "cs.MM",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.13205",
  "pdf_url": null,
  "comment": "Accepted by CICAI 2023 (Finalist of Best Student Paper Award)",
  "num_versions": null,
  "size_before_bytes": 4116947,
  "size_after_bytes": 130847
}