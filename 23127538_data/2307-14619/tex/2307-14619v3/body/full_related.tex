%!TEX root = ../neurips_main.tex

\paragraph{Imitation Learning.} Over the past few years, there has been a significant surge of interest in utilizing machine learning techniques for the execution of exceedingly intricate manipulation and control tasks.  Imitation learning, whereby a policy is trained to mimic expert demonstrations, has emerged as a highly data efficient and effective method in this domain, with application to self-driving vehicles \citep{hussein2017imitation,bojarski2016end,bansal2018chauffeurnet}, visuomotor policies \citep{finn2017one,zhang2018deep}, and navigation tasks \citep{hussein2018deep}. A widely acknowledged challenge of imitation learning is distribution shift: since the training and test time distributions are induced by the expert and trained policies respectively, compounding errors in imitating the expert at test-time can lead the trained policy to explore out-of-distribution states \citep{ross2010efficient}. This distribution shift has been shown to result in the imitator making incorrect judgements regarding observation-action causality, often with catastrophic consequences \citep{de2019causal}. Prior work in this domain has predominantly attempted to mitigate this issue in the non-stochastic setting via online data augmentation strategies, sampling new trajectories to mitigate distribution shift \citep{ross2011reduction,ross2010efficient,laskey2017dart}. Among this class of methods, the DAgger algorithm in particular has seen widespread adoption \cite{ross2010efficient,sun2023mega,kelly2019hg}. These approaches have the drawback that sampling new trajectories or performing queries on the expert is often expensive or intractable. Due to these limitations, recent developments have focused on novel algorithms and theoretical guarantees for imitation learning in an offline, non-interactive environment \citep{chang2021mitigating,pfrommer2022tasil}. Our work is similarly focused on the offline setting, but is capable of handling stochastic, non-Markovian demonstrators. Unlike \citep{pfrommer2022tasil}, we do not require our expert demonstrations to be sampled from a stabilizing expert policy, instead utilizing a synthesis oracle to stabilize around the provided demonstrations. This is a significantly weaker requirement and enables the development of high-probability guarantees for human demonstrators, where sampling new trajectories and reasoning about the stability properties is not possible.

\paragraph{Denoising Diffusion Probabilistic Models.}  Denoising Diffusion Probabilistic Models (DDPMs) \citep{sohl2015deep,ho2020denoising} and their variant, Annealed Langevin Sampling \citep{song2019generative}, have seen enourmous empirical success in recent years, especially in state-of-the-art image generation \citep{ramesh2022hierarchical,nichol2021improved,song2020denoising}.  More relevant to this paper is their application to imitation learning, where they have seen success even without the proposed data augmentation in \citet{janner2022planning,chi2023diffusion,pearce2023imitating,hansen2023idql}.  DDPMs rely on learning the score function of the target distribution, which is generally accomplished through some kind of denoised estimation \citep{hyvarinen2005estimation,vincent2011connection,song2020sliced}.  On the theoretical end, annealed Langevin sampling has been studied with score estimators under a variety of assumptions including the manifold hypothesis and some form of dissapitivity \citep{raginsky2017non,block2020generative,block2020fast}, although these works have generally suffered from an exponential dependence on ambient dimension, which is unacceptable in our setting.  Of greatest relevance to the present paper are the concurrent works of \citet{chen2022sampling,lee2023convergence} that provide polynomial guarantees on the quality of sampling using a DDPM assuming that the score functions are close in an appropriate mean squared error sense.  We take advantage of these latter two works in order to provide concrete end-to-end bounds in our setting of interest.  To our knowledge, ours is the first work to consider the application of DDPMs to imitation learning under a rigorous theoretical framework, although we emphasize that this does not constitute a strong technical contribution as opposed to an instantiation of earlier work for the sake of completeness and concreteness.

\paragraph{Smoothing Augmentations.} Data augmentation with smoothing noise has become such common practice, its adoption is essentially folklore. While augmentation of actions which noise is common practice for exploration (see, e.g. \cite{laskey2017dart}), it is widely accepted that noising actions in the learned policy is not best practice, and thus it is more common to add noise to the \emph{states} at training time, preserving target actions as fixed \cite{ke2021grasping}. Our work gives an interpretation of this decision as enforcing that the learned policy obey the distributional continuity property we term TVC (\Cref{defn:tvc}), so that the policy selects similar actions on nearby states. Previous work has interpreted noise augmentation as providing robustness. Data augmentation has been demonstrated to provide more robustness in RL from pixels \citep{kostrikov2020image}, adaptive meta-learning \citep{ajay2022distributionally}, in more traditional supervised learning as well \citep{hendrycks2020jacob}.