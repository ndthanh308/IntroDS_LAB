%!TEX root = ../main.tex

\newcommand{\qOUh}[2]{q_{#2,[#1]}^{\star}}
\section{Setting}\label{sec:setting}

\nipspar{Notation and Preliminaries.} \Cref{app:notation_and_org} gives a full review of notation. Bold lower-case (resp. upper-case) denote vectors (resp. matrices). We abbreviate the concatenation of sequences via $\bz_{1:n} = (\bz_{1},\dots,\bz_n)$. Norms $\|\cdot\|$ are Euclidean for vectors and operator norms for matrices unless otherwise noted. Rigorous probability-theoretic preliminaries are provided in  \Cref{app:prob_theory}. In short, all random variables take values in Polish spaces $\cX$ (which include real vector spaces), the space of Borel distributions on $\cX$ is denoted $\laws(\cX)$. We rely heavily on \bfemph{coupling}s from optimal transport theory: given measures $X \sim \lawP$ and $X' \sim \lawP'$ on $\cX$ and $\cX'$ respectively, $\couple(\lawP,\lawP')$ denotes the space of joint distributions $\mu \in \laws(\cX\times \cX')$ called ``couplings'' such that $(X,X') \sim \mu$ has marginals $X \sim \lawP$ and $X' \sim \lawP$. $\laws(\cX \mid \cY)$ denotes the space of \bfemph{kernels} $\lawQ: \cY \to \laws(\cX)$ ; \Cref{app:prob_theory} rigorously justifies that, in our setting, all conditional distributions can be expressed as kernels (which we do throughout the paper without comment). Finally $\I_{\infty}(\cE)$ denotes the indicator taking value $1$ if $\cE$ is true and $\infty$ otherwise.


\nipspar{Dynamics and Demonstrations.} We consider a discrete-time, control system with states $\bx_t \in \cX := \R^{\dimx}$, and inputs $\bu_t \in \cU := \R^{\dimu}$, obeying the following nonlinear dynamics 
\begin{align}
\bx_{t+1} =  f(\bx_t,\bu_t), \quad t \ge 1. \label{eq:dynamics}
\end{align}
 Given length $T \in \N$, we call sequences $\ctraj_T = (\bx_{1:T+1},\bu_{1:T}) \in \Ctraj_T := \cX^{T+1} \times \cU^{T}$ \bfemph{trajectories}.
For simplicity, we assume that \eqref{eq:dynamics}  deterministic and address stochastic dynamics in Appendix \ref{app:extensions}.  Though the dynamics are Markov and deterministic, we consider a stochastic and possibly \emph{non-Markovian} demonstrator, which allows for the multi-modality described in the \Cref{sec:intro}.
%Trajectories are still stochastic even though \eqref{eq:dynamics} is not due to  randomness in demonstrator inputs.  Our goal is to imitate a sequence of states and actions from an expert distribution.
  \begin{definition}[Expert Distribution]\label{def:expert} Let $\Dexp \in \laws(\Ctraj_T)$ denote an \bfemph{expert distribution} over trajectories to be imitated.  $\Dxone$ denotes the distribution of $\bx_1$  under $\ctraj_T = (\bx_{1:T+1},\bu_{1:T}) \sim \Dexp$.  
% We further suppose that the expert distribution is induced by a (time-inhomogeneous) \emph{expert policy} $\pist$ that maps states $\bx_t$ to distributions on actions $\bu_t$ such that if $\ctraj$ is constructed by sampling $\bx_1 \sim \Dxone$ and $\bu_t \sim \pist(\bx_t)$ for each $t$, then $\ctraj \sim \Dexp$. 
  \end{definition}
  
	%\Cref{asm:Dtarg} allows expert deonstration to be non-Markovian in control-state, provided has bounded memory $\taunot$. 
	\nipspar{Primitive Controllers and Synthesis Oracle.}
Our approach is to imitate not just actions, but simple local \emph{control policies}. In the body of this paper, we consider affine mappings $\cX \to \cU$ (redundantly) parameterized as $\bx \mapsto \bbaru + \bbarK(\bx - \bbarx)$; we call these \bfemph{primitive controllers}, denoted with $\sfk = (\bbaru,\bbarx,\bbarK) \in \cK$. We extend  our results  togeneral families of parameterized controllers to in \Cref{app:gen_controllers}. 


\newcommand{\tauc}{\tau_{\mathrm{chunk}}}
\newcommand{\taum}{\tau_{\mathrm{obs}}}
\newcommand{\dA}{d_{\cA}}
\nipspar{Chunking Policies and Indices. } The expert distribution $\Dexp$ may involve non-Markovian sequences of actions. We imititate these sequences via \bfemph{chunking policies}. Fix a \bfemph{chunk length} $\tauc \in \N$ and \bfemph{observation length} $\taum \le \tauc$, and define time indices $t_{h} = (h-1)\tauc+1$. For simplicity, we assume $\tauc$ divides $T$, and set $H = T/\tauc$. Given a $\ctraj_T \in \Ctraj_T$, define the \bfemph{trajectory-chunks} $\pathc := (\bx_{t_{h-1}:t_{h}},\bu_{t_{h-1}:t_{h}-1}) \in \Ctraj_{\tauc}$ and \bfemph{observation-chunks} $\pathm := (\bx_{t_{h}-\taum+1:t_{h}},\bu_{t_{h}-\taum+1:t_h-1}) \in \Ctraj_{\taum-1}$ for $h > 1$, and $\pathc[1] = \pathm[1] = \bx_1$ (for simplicity, we embed $\pathm[1]$ into $\Ctraj_{\taum-1}$ via zero-padding).  %\Cref{fig:abstractmdp} in \Cref{sec:analysis} explains our indexing choices.
We call $\tauc$-length sequences of primitive controllers \bfemph{composite actions} $\seqa_h = \sfk_{t_{h}:t_{h-1}} \in \cA := \cK^{\tauc}$. A \bfemph{chunking policy} $\pi = (\pi_h)$ consists of functions $\pi_h$ mapping observation-chunks $\pathm$ to distributions $\laws(\cA)$ over composite actions and interacting with the dynamics \eqref{eq:dynamics} by  $\seqa_h = \sfk_{t_{h}:t_{h-1}}\sim \pi_h(\pathm)$, and executing $\bu_t = \sfk_t(\bx_t)$. We let $\dA =  \tauc(\dimx + \dimu + \dimx\dimu)$ denote the dimension of the space $\cA$ of composite actions. The chunking scheme is represented in \Cref{fig:abstractmdp} in \Cref{sec:analysis}, alongside the abstraction we use in our analysis.





\newcommand{\pathf}[1][h]{\ctraj_{\mathrm{fut},#1}}
\newcommand{\pathp}[1][h]{\ctraj_{\mathrm{past},#1}}

%\begin{definition}[Chunking Policy] Fix a \emph{chunk window} $\tauf \in \Z_{>0}$ and \emph{past window} $\taup \in [\tauf]$. We define $t_{h} = (h-1)\tauf+1$, and given $\ctraj_T = (\bx_{1:H+1},\bu_{H}) \in \Ctraj_T$, define the future chunk and past chunks, respectively, as $\pathf := (\bx_{t_{h}:t_{h+1}},\bu_{t_h:t_{h-1}}) \in \Ctraj_{\tauf}$ and $\pathp := (\bx_{t_{h}-\taup+1:t_{h}},\bu_{t_{h}-\taup+1:t_h-1}) \in \Ctraj_{\taup}$ for $h > 1$, with $\pathp[1] = \bx_1$. 

%A $(\tauf,\taup)$-chunking policy $\pi = (\pi_1,\dots,\pi_H)$ with $\pi_h: \Ctraj_{\taup} \to \laws(\cK^{\tauf})$, which and applying \mscomment{..}. We define $\cD_{\pi}$ as \mscomment{..} \mscomment{for simplicity, we assume that stuff divides}
%\end{definition}
%\mscomment{Why do we do chunking. And in some sense thats the point}

\newcommand{\Imitloss}[1][\epsilon]{\cL_{\mathrm{imit},#1}}
\newcommand{\Imitjoint}[1][\epsilon]{\cL_{\mathrm{joint},#1}}
\newcommand{\Imitmarg}[1][\epsilon]{\cL_{\mathrm{marg},#1}}
\newcommand{\Imitfin}[1][\epsilon]{\cL_{\mathrm{fin},#1}}

\nipspar{Desideratum.} The quality of imitation of a deterministic policy is naturally measured in terms of step-wise closeness of state and action \cite{tu2022sample, pfrommer2022tasil}. With stochastic policies, however, two rollouts of even the same policy can visit different states. We propose measuring \emph{distributional closeness} via \emph{couplings} introduced in the preliminaries above. We define the following losses, focusing on the \emph{marginal distributions} between trajectories.
% three \mscomment{give them names? OTIE?} imitation losses: $\Imitjoint$ measures joint error over trajectories; $\Imitmarg$ only matching marginal distributions; and $\Imitfin$ requires matching goal-states.  \mscomment{adam edit}. 
\begin{definition}\label{def:losses}
  \iftoggle{workshop}{
    Given $\epsilon > 0$ and a (chunking) policy $\pi$, the imitation loss $\Imitmarg(\pi)$ is defined to be
    \begin{align}
      \max_{t\in [T]}\inf_{\coup} \left\{\Pr_{\coup}\left[\|\xexp_{t+1} - \bx^\pi_{t+1}\| > \epsilon\right],\, \Pr_{\coup}\left[\|\uexp_t - \bu^\pi_t\| > \epsilon\right]\right\}
    \end{align}
  }
  {Given $\epsilon > 0$ and a (chunking) policy $\pi$, the imitation loss is
\begin{align}
\Imitmarg(\pi) &:= \max_{t\in [T]} \inf_{\coup}\Pr_{\coup}\left[\max\left\{\|\xexp_{t+1} - \bx^\pi_{t+1}\|, \|\uexp_t - \bu^\pi_t\|\right\} > \epsilon\right]
\end{align}
}
where the infimum is over all couplings $\coup$ between the distribution  of $\ctraj_T$ under $\Dexp$ and that induced by the policy $\pi$ as described above, such that $\Pr_{\coup}[\xexp_{1} = \bx^\pi_{1}] = 1$.  Also define $\Imitfin(\pi) := \inf_{\coup} \Pr_{\coup}\left[\|\xexp_{T+1} - \bx^\pi_{T+1}\| > \epsilon\right]$, the loss restricted to the final states under each distribution. 
\end{definition}
Under stronger conditions (whose necessity we establish), we can also imitate joint distributions over actions (\Cref{app:end_to_end}).  Observe that $\Imitfin \le \Imitmarg$, and that both losses are equivalent to Wasserstein-type metrics on bounded domains. These losses are also equivalent to L\'evy-Prokhorov metrics \citep{strassen1965existence} under re-scaling of the Euclidean metric (even for unbounded domains), and also correspond to total variation analogues of shifted Renyi divergences \citep{altschuler2022privacy,altschuler2023faster}. While empirically evaluating these infima over couplings is challenging, $\Imitmarg$ upper bounds the difference in expectation between any bounded and Lipschitz control cost decomposing across time steps, states and inputs, and $\Imitfin$ upper bounds differences in bounded, Lipschitz final-state costs; see \Cref{app:end_to_end} for further discussion. 

%\mscomment{this should go to the appendix} While evaluating the losses in practice may be difficult, the following result demonstrates their utility:


\nipspar{Diffusion Models.} Our analysis provides imitiation guarantees when chunking policies $\pi_h$ select $\seqa_h$ via a sufficiently accurate generative model. Given their recent success, we adopt the popular Denoising Diffusion Probabilistic Models (DDPM) framework \citep{chen2022sampling,lee2023convergence} that allows the learner to sample from a density $q \in \Delta(\rr^d)$ assuming that the \emph{score} $\nabla \log q$ is known to the learner.  More precisely, suppose the learner is given an observation $\pathm$ and wishes to sample $\seqa_h \sim q(\cdot | \pathm)$ for some family of probability kernels $q(\cdot | \cdot)$.  A DDPM starts with some $\seqa_h^0$ sampled from a standard Gaussian noise and iteratively ``denoises'' for each DDPM-time step  $0 \leq \dpind < \dphorizon$:%\footnote{We use superscripts $\dpind$ for DPPM time steps to disambiguate from time steps $t$ of the dynamical system, and chunk-steps $h$. Subscripts $\qOU{t}$ denote continous-time indices of the DDPM process.}
\begin{align}\label{eq:sampling}
  \seqa_h^{\dpsup} = \seqa_h^{\dpsup[j-1]} - \dpstep \cdot \scoref_{\theta,h}(\seqa_h^{\dpsup[j-1]}, \pathm, \dpind) + 2 \cdot \cN(0, \dpstep^2 \eye),
\end{align}
where $\scoref_{\theta,h}(\seqa_h^{\dpind}, \pathm, \dpind)$ estimates the true score $\scorefsth(\seqa_h,\pathm,\dpstep \dpind)$, formally defined for any continuous argument $t \le \dphorizon \dpstep$ to be $\scorefsth(\seqa,\pathm,t) := \nabla_{\seqa} \log \qOUh{t}{h}(\seqa \mid \pathm)$, where $\qOUh{t}{h}(\cdot | \pathm)$ is the distribution of $e^{-t }\seqa_h^{(0)} + \sqrt{1 - e^{-2t}} \bgamma$ with $\seqa_h^{(0)}$ is sampled from the target distribution we which to sample from, and  $\bgamma \sim \cN(0, \eye)$ is a standard Gaussian.  We will denote by $\ddpm(\scoref_\theta, \pathm)$ the law of $\seqa_h^{\dphorizon}$ sampled according to the DDPM using $\scoref_\theta(\cdot, \pathm, \cdot)$ as a score estimator. Preliminaries on DPPMs are detailed in \Cref{app:scorematching}.




