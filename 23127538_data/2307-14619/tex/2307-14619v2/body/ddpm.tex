%!TEX root = ../main.tex

\section{Sampling and Score Matching}\label{app:scorematching}
In this section, we provide a rigorous guarantee on the quality of sampling from the learned DDPM under \Cref{ass:score_realizability}. 
We begin by recalling the basic motivation for Denoising Diffusion Probabilistic Models (DDPMs) and explain how we train them.  We then apply results from \citet{chen2022sampling} to show that if we have learned the conditional score function, then sampling can be done efficiently.  While \citet{block2020generative} demonstrated that unconditional score learning can be learned through standard statistical learning techniques, we generalize these results to the case of conditional score learning and conclude the section by proving that with sufficiently many samples, we can efficiently sample from a distribution close to our target.  




 We organize the section as follows:
\begin{itemize}
    %\item In \Cref{def:defn_of_comp} we provide the main notion of function class complexity, a vectorized Rademacher complexity that also appears in some form in \citet{block2020generative,maurer2016vector}.
    \item We then state the main result of the section, \Cref{thm:samplingguarantee}, which provides a high probability upper bound on the number of samples $n$ required in order to sample from $\ddpm$ trained on a given score estimate such that the sample is close in our optimal transport metric to the target distribution.
    \item In particular, in \eqref{eq:samplingparameters}, we give the exact polynomial dependence of the sampling parameters $\dpstep$ and $\dphorizon$ on the parameters of the problem.
    \item Before embarking on the proof, \Cref{sec:ddpm_simplify_notation} introduces simplifying notation; notably, dropping the dependence on subscript $h$, replacing the score dependence on $j$ with a class $\Theta_j$, and denoted $\mathcal D_{\sigma, h, [t]}$ as simply $q_{[t]}$.
    \item We break the proof of \Cref{thm:samplingguarantee} into two sections.  First, in \Cref{ssec:ddpms}, we recall a result of \citet{chen2022sampling,lee2023convergence} that shows that it suffices to accurately learn the score in the sense that if the score estimate is accurate in the appropriate sense, then the $\ddpm$ will adequately sample from a distribution close to the target.
    \item In \Cref{rmk:notvguarantee}, we emphasize the conditions that would be required to sample in total variation and explain why they do not hold in our setting.
    \item Then, in \Cref{ssec:scorematching}, we apply statistical learning techniques, similar to those in \citet{block2020generative}, to show that with sufficiently many samples, we can effectively learn the score.  We detail in \Cref{rmk:realizable} how the realizability part of \Cref{ass:score_realizability} can be relaxed.
    \item Finally, we conclude the proof of \Cref{thm:samplingguarantee} by combining the two intermediate results detailed above.
\end{itemize}
To begin, we \iftoggle{arxiv}{recall}{define} our notion of statistical complexity:


% \begin{definition}[Setting of $J$ and $\alpha$]\label{defn:setting_of_J_alpha} \mscomment{TODO set $J$ and $\alpha$}
% \end{definition}
% \mscomment{TODO:
% \begin{itemize}
%     \item Specify $J$ and $\alpha$
%     \item Explain how to uniformly bound score magnitude by smoothing and compactness, using bound $\Rstab$ on sampling distribution,
%     \item Invert the complexity bound $\rad_n(\Theta) \le \Ctheta n^{-1/\Cnu}$. 
%     \item Remove the convexity on the class assumption and do slow-rates.
% \end{itemize}}
We now state the main result of this section. 
\begin{theorem}\label{thm:samplingguarantee}  Fix $1 \leq h \leq H$, let $q$ denote $\Daugh$,  $d$ denote $\dA$, and suppose that $(\seqa_i, \pathm[h,i]) \sim q$ are independent for $1 \leq i \leq n$.   Suppose that the projection of $q$ onto the first coordinate has support (as defined in \Cref{def:support}) contained in the euclidean ball of radius $R \geq 1$ in $\rr^d$.  For $\epsilon > 0$,
    set
    \begin{align}
        \dphorizon = c \frac{d^3 R^4 (R + \sqrt{d})^4 \log\left( \frac{d R}{\epsilon} \right)}{\epsilon^{20}}, \qquad\qquad \dpstep = c\frac{\epsilon^8}{d^2 R^2(R + \sqrt{d})^2}.\label{eq:samplingparameters}
    \end{align}
    for some universal constant $c > 0$.  Suppose that for all $1 \leq j \leq J$, the following hold:
    \begin{itemize}
        \item There exists a function class $\Theta_j$ containing some $\thetast_j$ such that $\scorefst(\cdot, \cdot, \dpind \dpstep) = \scoref_{\thetast_j}(\cdot, \cdot, j \dpstep) = \nabla \log \qOU{j \dpstep}(\cdot | \cdot)$, where $\qOU{\cdot}$ is defined in \Cref{sec:setting}.
        \item The following holds for some $\delta > 0$:
        \begin{align}
            \sup_{\substack{\theta, \theta' \in \Theta_j \\ \norm{\seqa} \vee \norm{\seqa'} \leq R + \sqrt{d \log\left( \frac{2nd}\delta \right)} \\ \norm{\pathm} \leq R}} \norm{\scoref_\theta(\seqa , \pathm,t) - \scoref_{\theta'}(\seqa', \pathm,t)} \leq c\frac{d^2 (R + \sqrt{d \log\left( \frac {2nd}\delta \right)})^2}{\epsilon^8}.
        \end{align}
        \item \Cref{ass:score_realizability} holds and thus, for all $j \in [\dphorizon]$, it holds that $\rad_n(\Theta_j) \leq C_\Theta \alpha^{-1} n^{-1/\nu}$ for some $\nu \geq 2$ and all $n \in \bbN$ and, moreover, the linear growth condition is satisfied.
        \item The parameter $\thetahat = \thetahat_{1:\dphorizon}$ is defined to be the empirical minimizer of $\Lddpm$ from \Cref{sec:results}.
    \end{itemize}
    If 
    \begin{align}
        n \geq c \left( \frac{C_\Theta \alpha^{-1} d R (R \vee \sqrt{d}) \log(dn)}{\epsilon^4} \right)^{4\nu} \vee \left( \frac{d^6 (R^4 \vee d^2 \log^3\left( \frac{ndR}{\delta \epsilon} \right))}{\epsilon^{24}} d^2 \right)^{4\nu},
    \end{align}
    then with probability at least $1 - \delta$, it holds that
    \begin{align}
        \ee_{\pathm \sim q_{\pathm}}\left[ \inf_{\coup \in \couple(\emph{\ddpm}(\scoref_{\thetahat}, \pathm), q(\cdot | \pathm)) } \pp_{(\widehat \seqa, \seqa^\ast) \sim \coup}\left( \norm{\widehat \seqa - \seqa^*} \geq \epsilon \right) \right] \leq 3 \epsilon.
    \end{align}
  \end{theorem}
\begin{remark}
    We emphasize that the exact value of the polynomial dependence (and in particular its pessimism) stem from the guarantees of \citet{chen2022sampling,lee2023convergence} regarding the quality of sampling with DDPMs.  We remark below that the learning process itself does not incur such poor polynomial dependence except via these guarantees.  Furthermore, we do not expect the sampling guarantees of those two works to be tight in any sense and such a poor polynomial dependence is not observed in practice.  Rather, we include the bounds of \citet{chen2022sampling,lee2023convergence} so as to provide a fully rigorous end-to-end guarantee showing that polynomially many samples suffice to do imitation learning under our assumptions.
\end{remark}
\begin{remark}
    A subtle difference between the presentation in the body and that here is the dependence of the complexity of $\Theta$ on the parameter $\dpstep$.  We phrase the complexity guarantee as we did in the body in order to emphasize the dependence on the algorithmic parameter.  If we let $C_{\Theta}'$ denote a constant such that $\rad_n(\Theta) \leq C_{\Theta}' (\alpha / n)^{- 1/\nu}$, then the sample complexity above becomes
    \begin{align}
        n \geq c \left( \frac
        {C_\Theta' \log(d n)}{\alpha} \right)^{4 \nu} \vee \left( \frac{d^2 (R^2 \vee d^2 \log^3\left( \frac{n d R}{\epsilon \delta} \right))}{\alpha^2 \epsilon^{16}} \right)^{4 \nu}.
    \end{align}
\end{remark}
% \begin{remark}
%     We observe that while at first it may seem that the upper bound on the osculation of $\scoref_\theta$ is limiting, and, indeed, it is not obvious that this assumption does not contradict the realizability assumption immediately preceding it, it follows immediately from \Cref{lem:scorelipschitz} that if the preceding assumptions are satisfied, then the true score function $\scorefst$ automatically satisfies the bound on osculation.  Moreover, the boundedness of the function class is only assumed for the sake of convenience and could be substantially relaxed to an assumption requiring finiteness of moments of the envelope of the class \citep{wainwright2019high,rakhlin2017empirical}.  For the sake of simplicity, we do not further remark on this.
% \end{remark}
Critically, the guarantee of the quality of our DDPM is not in TV, but rather an optimal transport distance tailored to the problem at hand.  As remarked in \Cref{sec:asms_ctrl_body}, it is precisely this weaker guarantee that makes the problem challenging.


\subsection{Simplifying Notation}\label{sec:ddpm_simplify_notation}

We substantially simplify the notation in this appendix to suppress all dependence on $h$.  In particular, we fix some $h \in [H]$ and consider $\pathm[h] \sim \mathcal{D}_{\sigma, h, [0]}$.  We let $q$ denote $\Daugh$ and  $d$ denote $\dA$.
We further fix some $\sigma > 0$ and let $\qOU{t}$ denote the law of $\seqa \mid \pathm[h]$ according to  $\mathcal{D}_{\sigma, h, [t]}$ for the sake of notational simplicity.  Furthermore, to emphasize that our analysis of the statistical learning theory decomposes accross DDPM time steps, we denote by $\Theta_j$ the function class $\scoref_{\theta}(\cdot, \cdot, \dpstep \dpind)$.  We (redundantly) keep the dependence on $t$ in the function evaluation for the sake of clarity.  All other notation is defined \emph{in situ}.

We emphasize that while our theoretical analysis treats each $\scoref_{\theta, h}$ separately, empirically one sees better success in training the score estimates jointly; on the other hand, the focus of this paper is not on sampling and score estimation and so we make the simplifying assumption for the sake of convenience.


\subsection{Denoising Diffusion Probabilistic Models}\label{ssec:ddpms}
We begin by motivating the sampling procedure described in \eqref{eq:sampling}, which is derived by fixing a horizon $T$ and considering the continuum limit as $\dpstep \downarrow 0$ and $\dphorizon = \frac{T}{\dpstep}$.  More precisely, consider a forward process satisfying the stochastic differential equation (SDE) for $0 \leq t \leq T$:
\begin{align}
  d \seqa^t = - \seqa^t d t + \sqrt{2} d B_t, \quad \seqa^0 \sim q,
\end{align}
where $B_t$ is a Brownian motion on $\rr^d$ and $\seqa^0$ is sampled from the target density.  Applying the standard time reversal to this process results in the following SDE:
\begin{align}
  d \seqaback^{T - t} = \left( \seqaback^t + 2 \nabla \log q_{T-t}(\seqaback^t) \right) d t + \sqrt 2 d B_t, \quad \seqaback^0 \sim q_T,
\end{align}
where $q_t$ is the law of $\seqa^t$.  Because the forward process mixes exponentially quickly to a standard Gaussian, in order to approximately sample from $q$, the learner may sample $\seqabacktil^0 \sim \cN(0, \eye)$ and evolving $\seqabacktil^t$ according to the SDE above.  Note that the classical Euler-Maruyama discretization of the above procedure is exactly \eqref{eq:sampling}, but with the true score $\nabla \log q_{T-t}$ replaced by score estimates $\scoref_{\theta}(\cdot, T-t): \rr^d \to \rr^d$; we may hope that if $\scoref_\theta(\cdot, T-t) \approx \nabla \log q_{T-t}$ as functions, then the procedure in \eqref{eq:sampling} produces a sample close in law to $q$.  Indeed, the following result provides a quantitative bound:
\begin{theorem}[Corollary 4, \citet{chen2022sampling}]\label{thm:sampling}
    Suppose that a distribution $q$ on $\rr^d$ is supported on some ball of radius $R \geq 1$.  Let $C$ be a universal constant, fix $\epsilon > 0$, and let $\dpstep, \dphorizon$ be set as in \eqref{eq:samplingparameters}.  If we have a score estimator $\scoref_\theta: \rr^d \times [\tau] \to \rr^d$ such that
    \begin{align}
        \max_{\dpind \in [\dphorizon]} \ee_{\seqa \sim \qOU{\dpstep \dpind}}\left[ \norm{\scoref_\theta(\seqa, \dpind) - \nabla \log \qOU{\dpstep \dpind}(\seqa)}^2 \right] \leq \epsilon^4,
    \end{align}
    then
    \begin{align}
        \sup_{f : \, \norm{f}_\infty \vee \norm{\norm{\nabla f}}_\infty \leq 1} \ee_{\widehat{\seqa} \sim \Law(\seqa^{\dphorizon})}\left[f(\widehat \seqa)  \right] - \ee_{\seqa^* \sim q}\left[ f(\seqa^*) \right] \leq \epsilon^2,
    \end{align}
    where $\seqa^{\dphorizon}$ is sampled as in \eqref{eq:sampling}.
\end{theorem}
\begin{remark}
    As a technical aside, we note that \citet[Corollary 4]{chen2022sampling} applies to an ``early stopped'' DDPM, in the sense that the denoising is stopped in slightly fewer than $\dphorizon$ steps.  On the other hand, for the choice of $\dpstep$ given above, \citet[Lemma 20 (a)]{chen2022sampling} demonstrates that this distribution is $\epsilon^2$-close in Wasserstein distance to the sample produced by using all $\dphorizon$ steps and so by multiplying $C$ above by a factor of $2$ the guarantee is preserved.  Because in practice we do not stop the DDPM early, we phrase \Cref{thm:sampling} in the way above as opposed to the more complicated version with the early stopping.
\end{remark}

\begin{remark}\label{rmk:notvguarantee}
   While \citep{chen2022sampling,lee2023convergence} show that if $\scoref_\theta$ is close to the $\scorefsth$ in $L^2(\qOU{\dpstep \dpind})$ and $q$ satisfies mild regularity properties, then the law of $\seqa_h^{\dphorizon}$ will be close in total variation to $q$.  Unfortunately, the required regularity of $q$, that the score is Lipschitz, is too strong to hold in many of our applications, such as when the data lie close to a low-dimensional manifold.  In such cases, \citet{chen2022sampling} provided guarantees in a weaker metric on distributions.  We emphasize that even with full dimensional support, the Lipschitz constant of $\nabla \log q$ is likely large and thus the dependence on this constant appearing in \citet[Theorem 2]{chen2022sampling} is unacceptable.  In particular, this subtle point is what necessitates the intricate construction of our paper; as remarked in \Cref{sec:results}, if we could expect the score to be sufficiently regular and producing a sample close in total variation to the target distribution were feasable, the problem would be trivial.
\end{remark}
While \Cref{thm:sampling} applies to unconditional sampling, it is easy to derive conditional sampling guarantees as a corollary.
\begin{corollary}\label{cor:sampling}
    Suppose that $q$ is a joint distribution on actions $\seqa$ and observations $\pathm \in \rr^{d'}$.  Further assume that the marginals over $\rr^d$ are fully supported in a ball of radius $R \geq 1$.  Then there exists a universal constant $C$ such that for all small $\epsilon > 0$, if $\dphorizon$ and $\dpstep$ are set as in \eqref{eq:samplingparameters} and
    \begin{align}
        \ee_{\pathm \sim q_{\pathm}}\left[ \max_{\dpind \in [\dphorizon]} \ee_{\seqa \sim \qOU{\dpstep \dpind}(\cdot | \pathm)}\left[ \norm{\scoref_\theta(\seqa, \dpind, \pathm) - \nabla \log \qOU{\dpstep \dpind}(\seqa | \pathm )}^2 \right] \right] \leq \epsilon^4,\label{eq:scorecondition}
    \end{align}
    then
    \begin{align}
        \ee_{\pathm \sim q_{\pathm}}\left[ \inf_{\coup \in \couple(\emph{\ddpm}(\scoref_\theta, \pathm), q(\cdot | \pathm)) } \pp_{(\widehat \seqa, \seqa^\ast) \sim \coup}\left( \norm{\widehat \seqa - \seqa^*} \geq \epsilon \right) \right] \leq 3 \epsilon
    \end{align}
\end{corollary}
\begin{proof}
    We begin by showing an intermediate result,
    \begin{align}
        \ee_{\pathm \sim q_{\pathm}}\left[\sup_{f : \, \norm{f}_\infty \vee \norm{\norm{\nabla f}}_\infty \leq 1} \ee_{\widehat{\seqa} \sim \ddpm(\scoref_\theta, \pathm)}\left[f(\widehat \seqa)  \right] - \ee_{\seqa^* \sim q(\cdot | \pathm)}\left[ f(\seqa^*) \right]\right] \leq 3\epsilon^2 .\label{eq:intermediate}
    \end{align}
    using \Cref{thm:sampling}.  Let
    \begin{align}
        \cA = \left\{ \max_{\dpind \in [\dphorizon]} \ee_{\seqa \sim \qOU{\dpstep \dpind}(\cdot | \pathm)}\left[ \norm{\scoref_\theta(\seqa, \dpind, \pathm) - \nabla \log \qOU{\dpstep \dpind}(\seqa | \pathm )}^2  \right] \leq \epsilon^2\right\}.
    \end{align}
    By Markov's inequality and \eqref{eq:scorecondition}, it holds that
    \begin{align}
        \pp_{\pathm \sim q_{\pathm}}\left(  \cA^c \right) \leq \frac{\epsilon^4}{\epsilon^2} = \epsilon^2
    \end{align}
    and thus
    \begin{align}
        &\ee_{\pathm \sim q_{\pathm}}\left[ \sup_{f : \, \norm{f}_\infty \vee \norm{\norm{\nabla f}}_\infty \leq 1} \ee_{\widehat{\seqa} \sim \ddpm(\scoref_\theta, \pathm)}\left[f(\widehat \seqa)  \right] - \ee_{\seqa^* \sim q(\cdot | \pathm)}\left[ f(\seqa^*) \right] \right]  \\
        &= \ee_{\pathm \sim q_{\pathm}}\left[ \I[\cA] \sup_{f : \, \norm{f}_\infty \vee \norm{\norm{\nabla f}}_\infty \leq 1} \ee_{\widehat{\seqa} \sim \ddpm(\scoref_\theta, \pathm)}\left[f(\widehat \seqa)  \right] - \ee_{\seqa^* \sim q(\cdot | \pathm)}\left[ f(\seqa^*) \right] \right]\\
        &+ \ee_{\pathm \sim q_{\pathm}}\left[\I[A^c] \sup_{f : \, \norm{f}_\infty \vee \norm{\norm{\nabla f}}_\infty \leq 1} \ee_{\widehat{\seqa} \sim \ddpm(\scoref_\theta, \pathm)}\left[f(\widehat \seqa)  \right] - \ee_{\seqa^* \sim q(\cdot | \pathm)}\left[ f(\seqa^*) \right]\right] \\
        &\leq \ee_{\pathm \sim q_{\pathm}}\left[ \I[\cA]\inf_{q' \in \laws(\rr^d)} W_{2}(q(\cdot | \pathm), q') + \tv\left( q', \Law(\pi^\tau) \right) \right] + 2\epsilon^2.
    \end{align}
    For each $\pathm$, we may apply \Cref{thm:sampling} and observe that for  $\pathm \in \cA$,
    \begin{align}
        \sup_{f : \, \norm{f}_\infty \vee \norm{\norm{\nabla f}}_\infty \leq 1} \ee_{\widehat{\seqa} \sim \ddpm(\scoref_\theta, \pathm)}\left[f(\widehat \seqa)  \right] - \ee_{\seqa^* \sim q(\cdot | \pathm)}\left[ f(\seqa^*) \right] \leq \epsilon^2,
    \end{align}
    which proves \eqref{eq:intermediate}.  Now, for any fixed $\pathm$, by Markov's inequality and the definition of Wasserstein distance,
    \begin{align}
        \inf_{\coup \in \couple(\ddpm(\scoref_\theta, \pathm), q(\cdot | \pathm))} \pp_{(\widehat \seqa, \seqa^\ast) \sim \coup}\left( \norm{\widehat \seqa - \seqa^*} \geq \epsilon \right) \leq  \frac{W_{1}(\ddpm(\scoref_\theta, \pathm), q(\cdot | \pathm))}{\epsilon}.
    \end{align}
    The result follows.
\end{proof}
Note that the guarantee in \Cref{cor:sampling} is precisely what we need to control the one step imitation error in \Cref{thm:smooth_cor}; thus, the problem of conditional sampling has been reduced to estimating the score.  In the subsequent section, we will apply standard statistical learning techniques to provide a nonasymptotic bound on the quality of a score estimator.

\subsection{Score Estimation}\label{ssec:scorematching}
In the previous section we have shown that conditional sampling can be reduced to the problem of learning the conditional score.  While there exist non-asymptotic bounds for learning the unconditional score \citep{block2020generative}, they apply to a slightly different score estimator than is typically used in practice.  Here we upper bound the estimation error in terms of the complexity of the space of parameters $\Theta$.

Observe that in order to apply \Cref{cor:sampling}, we need a guarantee on the error of our score estimate in $L^2(\qOU{\dpstep \dpind})$ for all $\dpind \in [\dphorizon]$.  Ideally, then, for fixed $\pathm$ and $t  = \dpstep \dpind$, we would like to minimize $\ee_{\seqa \sim \qOU{t}}\left[ \norm{\scoref_\theta(\seqa, \pathm, t) - \nabla \log \qOU{t}(\seqa | \pathm)}^2 \right]$, where the inner norm is the Euclidean norm on $\rr^d$.  Unfortunately, because $\qOU{t}$ itself is unkown, we cannot even take an empirical version of this loss.  Instead, through a now classical integration by parts \citep{hyvarinen2005estimation,vincent2011connection,song2019generative}, this objective can be shown to be equivalent to minimizing
\begin{align}
  \Lddpm(\theta, \seqa, \pathm[], t) = \ee_{\seqa \sim \qOU{t}}\left[ \norm{\scoref_\theta\left(e^{-t} \seqa + \sqrt{1 - e^{-2t}} \bgamma , \pathm, t\right) + \frac{1}{\sqrt{1 - e^{-2t}}} \bgamma}^2\right].
\end{align}
Because we are really interested in the expectation over the joint distribution $(\seqa, \pathm)$, we may take the expectation over $\pathm$ and recover \eqref{eq:ddpmcond} as the empirical approximation.  We now prove the following result for a single time step $t$:
\begin{proposition}\label{prop:scorematchingsingletimestep}
    Suppose that $q$ is a distribution such that $q(\cdot | \pathm[i])$ is supported on a ball of radius $R$ for $q$-almost every $\pathm$.  For fixed $j \in [\dphorizon]$ and $\dpstep$ from \eqref{eq:samplingparameters}, let $t = j \dpstep$ and suppose that there is some $\thetast \in \Theta_j$ such that $\scorefst(\cdot, \cdot, t) = \scoref_{\thetast}(\cdot, \cdot, t) = \nabla \log \qOU{t}(\cdot | \cdot)$, i.e., $\scoref_\theta$ is rich enough to represent the true score at time $t$.  Suppose further that the class of functions $\left\{ \scoref_\theta | \theta \in \Theta_j \right\}$ satisfies for all $\theta \in \Theta_j$,
    \begin{align}
        \sup_{\substack{\theta, \theta' \in \Theta_j \\ \norm{\seqa} \vee \norm{\seqa'} \leq R \\ \norm{\pathm} \leq R}} \norm{\scoref_\theta(\seqa , \pathm,t) - \scoref_{\theta'}(\seqa', \pathm,t)} \leq c\frac{d^2 (R + \sqrt{d \log\left( \frac {2nd}\delta \right)})^2}{\epsilon^8}
    \end{align}
    for some universal constant $c > 0$. Recall the Rademacher term $\cR_n(\Theta_j)$ defined in \Cref{def:defn_of_comp}, and let
    \begin{align}
        \thetahat \in \argmin_{\theta \in \Theta}\sum_{i  =1}^n \Lddpm(\theta, \seqa_i, \pathm[i], t)
    \end{align}
    for independent and identically distributed $(\seqa_i, \pathm[i]) \sim q$.  Then it holds with probability at least $1- \delta$ over the data that
    \begin{align}
        &\ee_{(\seqa_t, \pathm) \sim \qOU{t}}\left[ \norm{\scoref_{\thetahat}(\seqa_t, \pathm, t) - \nabla \log \qOU{t}(\seqa_t | \pathm)}^2\right]  \\
        &\leq c \cdot \sqrt{\frac{ \log\left( dn \right)}{1 - e^{-2t}}} \left(\rad_n(\Theta) + \frac{d^2 (R + \sqrt{d \log\left( \frac {2nd}\delta \right)})^2}{\epsilon^8} \cdot \sqrt{\frac{d \log\left( \frac{4 d n}{\delta} \right)}{n}}\right).
    \end{align}
\end{proposition}
\begin{remark}
    We note that while we assume a linearly growing score function for the sake of simplicity, our analysis easily handles any polynomial growth with a mild resulting change in the constants, omitted for the sake of simplicity.
\end{remark}
Before we provide a proof, we recall the following result:
\begin{lemma}\label{lem:scorelipschitz}
    Suppose that $q$ is supported in a ball of radius $R$ and let $t \geq \dpstep$ for $\dpstep$ as in \eqref{eq:samplingparameters}.  Then $\nabla \log \qOU{t}(\cdot | \cdot)$ is $L$-Lipschitz with respect to the first parameter for
    \begin{align}
        L = \frac{dR^2 (R \vee \sqrt d)^2}{\epsilon^8}.
    \end{align}
    In particular,
    \begin{align}
        \sup_{\substack{\norm{\seqa} \vee \norm{\seqa'} \leq R \\ \pathm}} \norm{\nabla \log \qOU{t}(\seqa | \pathm) - \nabla \log \qOU{t}(\seqa' | \pathm)} \leq 2 LR
    \end{align}
    and there exists some assignment of $\Theta$ and  $\scoref_\theta$ that satisfies the boundedness condition in \Cref{prop:scorematchingsingletimestep}.
\end{lemma}
\begin{proof}
    The first statement follows from eplacing the $\epsilon$ in \citet[Lemma 20 (c)]{chen2022sampling} by $\epsilon^2$.  The second statement follows immediately from the first.
\end{proof}
\begin{remark}
    Note that a slight variation of this result is what leads to the dependence on $\alpha$ in the growth parameter in \Cref{ass:score_realizability} allowing for realizability.  Indeed, by \citet[Lemma 20]{chen2022sampling}, it holds that the true score of $\qOU{\alpha}$ is
    \begin{align}
        L = \frac{1}{1 - e^{-2\alpha}} \vee \frac{\abs{1 - e^{-2\alpha} (1 + R^2)}}{(1 - e^{-2\alpha})^2}
    \end{align}
    Lipschitz, which is $O(\alpha^{-1})$ for $\alpha \ll 1$.
\end{remark}

We also require the following standard result:
\begin{lemma}\label{lem:rad_gaussian_comparison}
    If $\rad_n(\Theta_j)$ is defined as in \Cref{def:defn_of_comp}, then
    \begin{align}
        \ee_{\bgamma_1, \dots, \bgamma_n}\left[ \sup_{\substack{\theta \in \Theta_j \\ 1 \leq j \leq J}} \frac 1n \cdot \sum_{i = 1}^n \inprod{\scoref_{\theta}(\seqa, \pathm[i], j) }{\bgamma_i} \right]  \leq \sqrt{\pi \log(dn)} \cdot \rad_n(\Theta_j)
    \end{align}
\end{lemma}
\begin{proof}
    This statement is classical and follows immediately from the fact that the norm of a Gaussian is independent from its sign as well as the fact that $\ee\left[ \max_{i,j} (\bgamma_i)_j \right] \leq \sqrt{\pi \log(dn)}$ by classical Gaussian concentration.  See \citet{van2014probability} for more details.
\end{proof}





\begin{proof}[Proof of \Cref{prop:scorematchingsingletimestep}]
    Let $P_n$ denote the empirical measure on $n$ independent samples $\left\{ \left( \seqa_i, \pathm[i], \bgamma_i \right)\right\}$ and let $\seqa_i^t = e^{-t} \seqa_i + \sqrt{1 - e^{-2t}} \bgamma_i$.  Let $C_t = \sqrt{1 - e^{-2t}}$ and observe that by definition and realizability,
    \begin{align}
         P_n\left( \norm{C_t \scoref_{\thetahat}(\seqa^t, \pathm, t) - \bgamma}^2 \right) \leq \cdot P_n\left( \norm{C_t \nabla \log \qOU{t}(\seqa^t| \pathm) - \bgamma}^2 \right).\label{eq:basicinequality}
    \end{align}
    We emhasize that by \Cref{lem:scorelipschitz}, realizability does not make the result vaccuous.  Adding and subtracting $C_t \nabla \log \qOU{t}(\seqa^t | \pathm)$ from the left hand inequality, expanding and rearranging, we see that
    \begin{align}
        C_t^2 P_n\left( \norm{\scoref_{\thetahat}(\seqa^t, \pathm, t) - \nabla \log \qOU{t}(\seqa^t| \pathm)}^2 \right) &\leq 2 C_t \cdot P_n\left( \inprod{\scoref_{\thetahat}(\seqa^t, \pathm, t) - \nabla \log \qOU{t}(\seqa^t| \pathm)}{\bgamma} \right) \\
        &\leq 2 C_t \cdot P_n\left( \sup_{\theta \in \Theta_j} \inprod{\scoref_{\theta}(\seqa^t, \pathm, t) - \nabla \log \qOU{t}(\seqa^t| \pathm)}{\bgamma} \right).
    \end{align}
    We now claim that with probability at least $1 - \delta$, it holds that
    \begin{align}
        P_n\left( \sup_{\theta \in \Theta} \inprod{\scoref_{\theta}(\seqa^t, \pathm, t) - \nabla \log \qOU{t}(\seqa^t| \pathm)}{\bgamma} \right) &\leq \ee\left[ P_n\left( \sup_{\theta \in \Theta_j} \inprod{\scoref_{\theta}(\seqa^t, \pathm, t) - \nabla \log \qOU{t}(\seqa^t| \pathm)}{\bgamma} \right) \right] \\
        &+ B\cdot \sqrt{\frac{d\log\left( \frac {2d} \delta \right)}{n}},
    \end{align}
    where
    \begin{align}\label{eq:setting_B}
        B = c\frac{d^2 (R + \sqrt{d \log\left( \frac {2nd}\delta \right)})^2}{\epsilon^8}
    \end{align}
    for some universal constant $c > 0$.  To see this, we claim that with probability at least $1 - \frac \delta 2$, it holds that $\norm{\seqa_i^t} \leq  c \left(R + \sqrt{d \log\left( \frac {2nd}\delta \right)}\right)$ for all $1 \leq i \leq n$.  Indeed, this follows by Gaussian concentration in \citet[Lemmata 1 \& 2]{jin2019short}.  We may now apply \Cref{lem:scorelipschitz} to a bound on the osculation of $\scoref_\theta - \nabla \log \qOU{t}$ in the ball of the above radius.  Conditioning on the event that $\norm{\seqa_i^t}$ is bounded by the above, we may argue as in \citet[Theorem 4.10]{wainwright2019high} that if we let the function
    \begin{align}
        G = G(\seqa_1, \pathm[1], \dots, \seqa_n, \pathm[n]) =  P_n\left( \sup_{\theta \in \Theta_j} \inprod{\scoref_{\theta}(\seqa^t, \pathm, t) - \nabla \log \qOU{t}(\seqa^t| \pathm)}{\bgamma} \right),
    \end{align}
    then for any $i$, on the event of bounded norm, replacing $(\seqa_i, \pathm[i])$ with $(\seqa_i', \pathm[i]')$ and leaving other terms unchanged changes ensures that $\abs{G - G'} \leq \frac{2B}{n} \bgamma_i$.  Thus by \citet[Corollary 7]{jin2019short}  and a union bound, the claim holds.  Because $\bgamma$ is mean zero, we have
    \begin{align}
        \ee\left[ P_n\left( \sup_{\theta \in \Theta} \inprod{\scoref_{\theta}(\seqa^t, \pathm, t) - \nabla \log \qOU{t}(\seqa^t| \pathm)}{\bgamma} \right) \right] &\leq \ee\left[ P_n\left( \sup_{\theta \in \Theta} \inprod{\scoref_{\theta}(\seqa^t, \pathm, t)}{\bgamma} \right) \right] \\
        &\leq \sqrt{\pi \log(dn)} \cdot \rad_n(\Theta_j),
    \end{align}
    where the last inequality follows by \Cref{lem:rad_gaussian_comparison} and the fact that $t = \dpind \dphorizon$.  Summing up the argument until this point and rearranging tells us that with probability at least $1 - \delta$, it holds that 
    \begin{align}
        P_n\left( \norm{\scoref_{\thetahat}(\seqa^t, \pathm, t) - \nabla \log \qOU{t}(\seqa^t| \pathm)}^2 \right) &\leq \frac{2}{C_t} \sqrt{\pi \log(nd)} \cdot \rad_n(\Theta) + \frac B{C_t} \cdot \sqrt{\frac{d \log\left( \frac{2 nd}{\delta} \right)}{n}},
    \end{align}
    with $B$ given in \eqref{eq:setting_B}.  We now use a uniform norm comparison between population and empirical norms to conclude the proof.  Indeed, it holds by \citet[Lemma 8.i \& 9]{rakhlin2017empirical} that there exists a critical radius
    \begin{align}
        r_n \leq c B \log^3(n) \rad_n(\Theta_j)^2
    \end{align}
    such that with probability at least $1 - \delta$,
    \begin{align}
        \ee_{(\seqa^t, \pathm) \sim \qOU{t}}&\left[\norm{\scoref_{\thetahat}(\seqa^t, \pathm, t) - \nabla \log \qOU{t} (\seqa^t | \pathm)}^2 \right] \\
        &\leq 2 \cdot P_n\left(\norm{\scoref_{\thetahat}(\seqa^t, \pathm, t) - \nabla \log \qOU{t} (\seqa^t | \pathm)}^2\right) + c r_n + c \frac{\log\left( \frac 1\delta \right) + \log\log n}{n},
    \end{align}
    where again $c$ is some universal constant.  Combining this with our earlier bound on the empirical distance and a union bound, after rescaling $\delta$, we have that
    \begin{align}
        \ee_{(\seqa^t, \pathm) \sim \qOU{t}}\left[\norm{\scoref_{\thetahat}(\seqa^t, \pathm, t) - \nabla \log \qOU{t} (\seqa^t | \pathm)}^2 \right] &\leq \frac{4}{C_t} \sqrt{\pi \log(nd)} \cdot \rad_n(\Theta_j) + \frac {2B}{C_t}\cdot \sqrt{\frac{d \log\left( \frac{4nd}\delta\right)}{n}} \\
        &+c B \log^3(n) \cdot \rad_n^2(\Theta_j) + c \frac{\log\left( \frac 2\delta \right) + \log\log(n)}{n}
    \end{align}
    with probability at least $1 - \delta$.  This concludes the proof.
\end{proof}
\begin{remark}
    For the sake of simplicity, in the proof of \Cref{prop:scorematchingsingletimestep} we applied uniform deviations and recovered the ``slow rate'' of $\rad_n(\Theta)$, up to logarithmic factors.  Indeed, if we were to further assume that the score function class is star-shaped around the true score, we could recover a faster rate, as was done in the case of unconditional sampling in \citet{block2020generative} with a slightly different loss.  While in our proof the appeal to \citet{rakhlin2017empirical} to control the population norm by the empirical norm could be replaced with a simpler uniform deviations argument because we have already given up on the fast rate, such an argument is necessary in the more refined analysis.  As the focus of this paper is not on the sampling portion of the end-to-end analysis, we do not include a rigorous proof of the case of fast rates for the sake of simplicity and space.
\end{remark}
\begin{remark}\label{rmk:realizable}
    While we assumed for simplicity that the score was realizable with respect to our function class for every time $t = \dpstep \dpind$, this condition can be relaxed to approximate realizability in a standard way.  In particular, if the score is $\epsilon$-far away from some function representable by our class in a pointwise sense, then we can add an $\epsilon$ to the right hand side of \eqref{eq:basicinequality} with minimal modification to the proof.
\end{remark}
With \Cref{prop:scorematchingsingletimestep}, and a union bound, we recover the following result:
\begin{proposition}\label{prop:scorematching}
    Suppose that the conditions on $\scoref_\theta$ in \Cref{prop:scorematchingsingletimestep} continue to hold.  Suppose further that $\norm{\scoref_\theta(\seqa, \pathm[h], t)} \leq \Cgrow(1 + \norm{\seqa} + \norm{\pathm[h]})$ for all $\seqa$ and some universal constant $\Cgrow>0$.  Let $\dphorizon$ and $\dpstep$ be as in \eqref{eq:samplingparameters} and suppose that $\dpstep \leq \frac 12$.  Then, with probability at least $1 - \delta$ over $\cD'$, it holds that
    \begin{align}
        &\ee_{\pathm \sim q_{\pathm}}\left[ \max_{\dpind \in [\dphorizon]} \ee_{\seqa \sim \qOU{\dpstep \dpind}(\cdot | \pathm)}\left[ \norm{\scoref_\theta(\seqa, \dpind, \pathm) - \nabla \log \qOU{\dpstep \dpind}(\seqa | \pathm)}^2 \right]\right] \\
        &\leq c \frac{d (R \vee \sqrt{d})^2 \log(dn)}{\epsilon^4} \rad_n(\Theta) + c\frac{d^3 \left( (R \vee \sqrt d)^2 + d \log\left( \frac{n dR}{\delta \epsilon} \right) \right)}{\epsilon^{12}} \sqrt{\frac{d \log\left( \frac{4 d n R}{\delta \epsilon} \right)}{n}}
    \end{align}
    In particular if
    \begin{align}
        \rad_n(\Theta_j) \leq C_\Theta n^{- 1/\nu}
    \end{align}
    for some $\nu \geq 2$ and all $j \in [\dphorizon]$, then for
    \begin{align}
        n \geq c \Cgrow \left( \frac{C_\Theta \alpha^{-1} d (R \vee \sqrt{d})^2 \log(dn)}{\epsilon^4} \right)^{4\nu} \vee \left( \frac{d^6 (R^4 \vee d^3 \log^3\left( \frac{ndR}{\delta \epsilon} \right))}{\epsilon^{24}} d^2 \right)^{4\nu}
    \end{align}
    it holds that with probability at least $1- \delta$,
    \begin{align}
        \ee_{\pathm \sim q_{\pathm}}\left[ \max_{\dpind \in [\dphorizon]} \ee_{\seqa \sim \qOU{\dpstep \dpind}(\cdot | \pathm)}\left[ \norm{\scoref_\theta(\seqa, \dpind, \pathm) - \nabla \log \qOU{\dpstep \dpind}(\seqa | \pathm)}^2 \right]\right] \leq \epsilon^4.
    \end{align}
\end{proposition}
\begin{proof}
    We begin by proving the result on the event that $\norm{\seqa}  \vee \norm{\pathm[h]} \leq C (R \vee \sqrt{d}) \log\left(\frac {J n (R \vee \sqrt d)}{\delta \epsilon}\right)$. Note that
    \begin{align}
        1 - e^{-2t} \geq 1 - e^{-2\alpha} \geq \alpha
    \end{align}
    because $2\alpha \leq 1$.  We now apply \Cref{prop:scorematchingsingletimestep} and take a union bound over $j \in [J]$.  All that remains is to demonstrate that the contribution of the event that $\seqa^j$ is outside the above defined ball is negligable.  To do this, observe that by \citet[Lemma 4.15]{lee2023convergence}, there is some $C > 0$ such that $\seqa^j$ is $C(R \vee \sqrt{d})$-subGaussian.  By the sublinearity of the growth of $\scoref_\theta$ in $\seqa$, as well as the Lipschitzness of $\qOU{\alpha j}$ following from \citet[Lemma 20]{chen2022sampling}, bounding a maximum by a sum, and the elementary computation in \Cref{lem:subgaussianintegration}, we have that the expectation of this term on this event is bounded by $\frac{C\Cgrow}{n}$. The result follows.
\end{proof}
We note that in our simplified analysis, we have assumed that $\Naug = 1$, i.e., for each sample, we take a single noise level from the path.  In practice, we use many augmentations per sample.  Again, as the focus of our paper is not on score estimation and sampling, we treat this as a simple convenience and leave open to future work the problem of rigorously demonstrating that multiple augmentations indeed help with learning.  Finally, for a discussion on bounding $\rad_n(\Theta)$, see \citet{wainwright2019high}.



\begin{proof}[Proof of \Cref{thm:samplingguarantee}]
    We note that the proof follows immediately from combining \Cref{cor:sampling} with \Cref{prop:scorematching}.
\end{proof}
We conclude the section with the following elementary computation used above:
\begin{lemma}\label{lem:subgaussianintegration}
    Suppose that $X$ is a $\sigma$-subGaussian random variable on $\rr$.  Then for any $r \geq 1$,
    \begin{align}
        \ee\left[ \abs{X} \cdot \I[\abs{X} > r] \right] \leq C\frac{\sigma}{r} \cdot e^{- \frac{r^2}{2 \sigma^2}}
    \end{align}
\end{lemma}
\begin{proof}
    This is an elementary computation.  Indeed,
    \begin{align}
        \ee\left[ \abs{X} \cdot \I[\abs{X} > r] \right] &= \int_r^\infty \pp\left( \abs{X} > t \right) d t \leq C \int_r^\infty e^{- \frac{t^2}{2 \sigma^2}} d t \\
        &\leq C \cdot \int_r^\infty \frac{t}{r} e^{- \frac{t^2}{2\sigma^2}} d t \\
        &\leq C \frac{\sigma}{r} \cdot e^{- \frac{r^2}{2 \sigma^2}}.
    \end{align}
    The result follows.
\end{proof}




