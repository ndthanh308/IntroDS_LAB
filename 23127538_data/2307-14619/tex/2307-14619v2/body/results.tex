
%!TEX root = ../main.tex

\newcommand{\Phicl}[1]{\bm{\Phi}_{\mathrm{cl},#1}}
\newcommand{\Cnu}{\nu}
\newcommand{\Ctheta}{C_{\Theta}}

\newcommand{\augmentflag}{\bm{\mathtt{augment}}}
\newcommand{\Nsample}{N_{\texttt{exp}}}
\newcommand{\Naug}{N_{\texttt{aug}}}

\newcommand{\trueflag}{\texttt{true}}
\newcommand{\falseflag}{\texttt{false}}
\newcommand{\sigaug}{\sigma}

\newcommand{\feta}{f_{\eta}}
\newcommand{\Lfp}{L_{\cV}}
\newcommand{\Ldyn}{L_{\mathrm{dyn}}}
\newcommand{\Rdyn}{R_{\mathrm{dyn}}}
\newcommand{\Mf}{M_{\mathrm{dyn}}}
\newcommand{\Mdyn}{M_{\mathrm{dyn}}}
\newcommand{\Rstab}{R_{\mathrm{stab}}}
\newcommand{\Lstab}{L_{\mathrm{stab}}}
\newcommand{\Bstab}{B_{\mathrm{stab}}}
\newcommand{\ddx}{\frac{\partial}{\partial x}}
\newcommand{\ddu}{\frac{\partial}{\partial u}}
\newcommand{\Aclk}[1][k]{\bA_{\mathrm{cl},#1}}
\newcommand{\Dexpbar}{\bar{\cD}_{\mathrm{exp}}}

\section{Algorithm and Results}\label{sec:results}
We show that trajectories of the form given in \Cref{def:expert} can be efficiently imitated if (a) we are given a synthesis oracle that locally stabilizes chunks of the trajectory with primitive controllers and (b) the score of the following conditional distributions (whose existence is guaranteed by \Cref{app:prob_theory}) lies in a class $\Theta$ of bounded statistical complexity.



\nipspar{Control-Theoretic Assumptions.}  We say trajectory $\ctraj_{\tau} = (\bx_{1:\tau+1},\bu_{1+\tau}) \in \scrP_{\tau}$ is \bfemph{feasible} if it obeys the dynamics in \eqref{eq:dynamics}. We assume that the transition map $f$ takes the form of an Euler-like discretization 
\begin{align}f(\bx_t,\bu_t) = \bx_t + \eta \feta(\bx_{t},\bu_t)
\end{align} 
for a small step size $\eta > 0$\iftoggle{arxiv}{. Let us introduce some key definitions
\begin{definition} 
We say $\ctraj_{\tau}$ is $(\Rdyn,\Ldyn,\Mdyn)$-\emph{regular} if, for any $t \in [\tau]$ and $(\bx'_t,\bu'_t) \in \R^{\dimx} \times \R^{\dimu}$ such that $\|\bx'_t-\bx_t\| \vee \|\bu_t-\bu'_t\| \le \Rdyn$, it holds that  $\| \nabla \feta(\bx'_t,\bu'_t)\|_{\op} \le \Ldyn$ and $\| \nablatwo \feta(\bx'_k,\bu'_t)\|_{\op} \le \Mf$.\footnote{Here, $ \|\nablatwo \feta(\bx'_t,\bu'_t)\|_{\op} $ denotes the operator-norm of a three-tensor.} 
\end{definition}
\begin{definition}
The \bfemph{Jacobian linearizations} along a trajectory $\ctraj_{\tau} = (\bx_{1:\tau+1},\bu_{1:\tau}) \in \scrP_\tau$ are matrices $\bA_t(\ctraj_{\tau}) := \ddx f(x_t,u_t)$ and $\bB_t(\ctraj_{\tau}) := \ddu f(x_t,u_t)$ for $t \in [\tau]$. 
\end{definition}
\begin{definition} Given $\ctraj_{\tau} \in \scrP_{\tau}$ and primitive controllers $\sfk_{1:\tau}$, expressed as $\sfk_{t}(\bx) = \bbarK_{t}(\bx - \bbarx_t) + \bbaru_t(x)$, we say $(\ctraj_{\tau},\sfk_{1:\tau})$ are \bfemph{$(\Rstab,\Bstab,\Lstab)$-Jacobian stable} if (a)  $\sfk_{1:\tau}$ is consistent with $\ctraj_{\tau}$  (b) $\max_{t \in [\tau]} \|\bbarK_{t}\| \vee \|\bbarx_t\| \vee \|\bbaru_t\| \le \Rstab$, and (c) the linearized closed-loop transition operator has exponential decay: \iftoggle{workshop}{
  \begin{align}
    &\textstyle \|\Phicl{k,j}\|_{\op} \le \Bstab(1 - \frac{\eta}{\Lstab})^{k-j} \\
    &\Phicl{k,j} := (\eye + \step \Aclk[k-1]) \cdot(\eye + \step\Aclk[k-2])  \cdots (\eye + \step \Aclk[j]),
    \end{align}
}{
\begin{align}
\textstyle \|\Phicl{k,j}\|_{\op} \le \Bstab(1 - \frac{\eta}{\Lstab})^{k-j}, \quad
\Phicl{k,j} := (\eye + \step \Aclk[k-1]) \cdot(\eye + \step\Aclk[k-2])  \cdots (\eye + \step \Aclk[j]),
\end{align}
}
where above $\Aclk[k] = \bA_k(\ctraj_{\tau}) + \bB_{k-1}(\ctraj_{\tau}) \bK_{k-1}$. 
\end{definition}
}
{
  and say $\ctraj_{\tau}$ is $(\Rdyn,\Ldyn,\Mdyn)$-\emph{regular} if, for any $t \in [\tau]$ and $(\bx'_t,\bu'_t) \in \R^{\dimx} \times \R^{\dimu}$ such that $\|\bx'_t-\bx_t\| \vee \|\bu_t-\bu'_t\| \le \Rdyn$, it holds that  $\| \nabla \feta(\bx'_t,\bu'_t)\|_{\op} \le \Ldyn$ and $\| \nablatwo \feta(\bx'_k,\bu'_t)\|_{\op} \le \Mf$.\footnote{Here, $ \|\nablatwo \feta(\bx'_t,\bu'_t)\|_{\op} $ denotes the operator-norm of a three-tensor.} The \bfemph{Jacobian linearizations} along a trajectory $\ctraj_{\tau} = (\bx_{1:\tau+1},\bu_{1:\tau}) \in \scrP_\tau$ are matrices $\bA_t(\ctraj_{\tau}) := \ddx f(x_t,u_t)$ and $\bB_t(\ctraj_{\tau}) := \ddu f(x_t,u_t)$ for $t \in [\tau]$. Given $\ctraj_{\tau} \in \scrP_{\tau}$ and primitive controllers $\sfk_{1:\tau}$, expressed as $\sfk_{t}(\bx) = \bbarK_{t}(\bx - \bbarx_t) + \bbaru_t(x)$, we say $(\ctraj_{\tau},\sfk_{1:\tau})$ are \bfemph{$(\Rstab,\Bstab,\Lstab)$-Jacobian stable} if (a)  $\sfk_{1:\tau}$ is consistent with $\ctraj_{\tau}$  (b) $\max_{t \in [\tau]} \|\bbarK_{t}\| \vee \|\bbarx_t\| \vee \|\bbaru_t\| \le \Rstab$, and (c) the linearized closed-loop transition operator has exponential decay: 
\begin{align}
\textstyle \|\Phicl{k,j}\|_{\op} \le \Bstab(1 - \frac{\eta}{\Lstab})^{k-j}, \quad
\Phicl{k,j} := (\eye + \step \Aclk[k-1]) \cdot(\eye + \step\Aclk[k-2])  \cdots (\eye + \step \Aclk[j]),
\end{align}
where above $\Aclk[k] = \bA_k(\ctraj_{\tau}) + \bB_{k-1}(\ctraj_{\tau}) \bK_{k-1}$. 
}   
Our first two assumptions are as follows.
\begin{assumption}\label{asm:traj_regular} The $\ctraj_T \sim \Dexp$ is feasible and $(\Rdyn,\Ldyn,\Mdyn)$-regular with probability $1$.
\end{assumption}
\begin{assumption}\label{asm:Jacobian_Stable} With probability 1 over $\ctraj_T \sim \Dexp$ and $\sfk_{1:T} = \synth(\ctraj_T)$, the chunk-action pairs $(\pathc[h+1],\seqa_{h})$ are $(\Rstab,\Bstab,\Lstab)$-Jacobian Stable for $1\le h \le H$.
\end{assumption}
\Cref{asm:traj_regular} enforces smoothness of the dynamics, but \emph{not} smoothness or continuity of the underlying policy. \Cref{asm:Jacobian_Stable} generalizes popular quantifications of stability (e.g. strong stability \cite{cohen2019learning}), and is satisfied when primitive controllers are synthesized via Ricatti equations of  dynamics with stabilizable linearizations (\Cref{app:control_stability}). 
\begin{definition}[Problem constants]\label{defn:prob_constants_body} Throughout, we refer to constants $c_1,c_2,c_3,c_4,c_5 > 0$, which are polynomial in the terms in \Cref{asm:traj_regular,asm:Jacobian_Stable}, and which are defined formally in  \Cref{defn:IPS_consts}.
\end{definition}


\newcommand{\Daugh}[1][h]{\cD_{\sigma,#1}}
\newcommand{\pidecsigh}[1][h]{\pi^\star_{\mathrm{dec},\sigma,#1}}
\subsection{A reduction to conditional sampling}
We begin with our main theoretical result: if one learns a chunking policy $\pihat$ that can compute the conditional distribution of abstract actions at time steps given observed memory chunks, then a smoothed version of this policy, $\pihat_{\sigma}$, has low imitation error. Because we don't assume that expert demonstrations are continuous or unimodal, these expert conditional distributions can be very poorly behaved. Therefore, we study estimating the conditionals under a popular data augmentation technique \cite{ke2021grasping}, where we smooth not the abstract actions, but the memory chunks. This induces the following joint distribution over memory chunks and composite actions.
\begin{definition}[Smoothed Data Distribution] For $h \in [H]$ and $\sigma > 0$, let $\Daugh$ denote the joint distribution of $(\seqa_h,\pathmtil)$, induced by letting  $\seqa_h  = \sfk_{t_h:t_{h+1}-1}$ and $\pathm$ have the distribution of the abstract action and memory chunk at time $h$ induced by drawing a trajectory  $\ctraj_T \sim \Dexp$, and primitive controllers $\sfk_{1:T} = \synth(\ctraj_T)$, and letting $\pathmtil \sim \cN(\pathm, \sigma^2 \eye)$ augment $\pathm$ with noise. 
\end{definition}
Because the demonstrator is stochastic, we care not about point-estimation, but rather \emph{generative modeling} of the conditional distributions of composite actions. This is measured by the following notion of discrepancy, formulated in an optimal transport error, which relaxes the L\'evy-Prokhorov metric \citep{strassen1965existence} between the conditional distribution of $\seqa_h \mid \pathmtil$ and $\seqa_h' \sim \pihat_h(\pathmtil)$ under $\Daugh$.   
\newcommand{\dmax}{\dist_{\max}}
\newcommand{\couphatsigh}{\couple_{\sigma,h}(\pihat)}
\begin{definition}[Policy Error]\label{defn:policy_error} Given two composite actions $\seqa = (\bbaru_{1:\tauc}, \bbarx_{1:\tauc}, \bbarK_{1:\tauc})$ and $\seqa' = (\bbaru_{1:\tauc}', \bbarx_{1:\tauc}', \bbarK_{1:\tauc}')$, define the distance $\dmax(\seqa,\seqa') := \max_{1\le k \le \tauc}(\|\bbaru_{k}-\bbaru_{k}'\| + \|\bbarx_{k}-\bbarx_{k}'\| +\|\bbarK_{k}-\bbarK_{k}'\|)$. For a given chunking policy $\pihat = (\pihat_h)$,  let $\couphatsigh$ denote the set of couplings of $(\pathmtil,\seqa_h,\seqa_h')$, where $(\pathmtil,\seqa_h)$ are jointly distributed according to $\Daugh$, and $\seqa_h' \mid \pathmtil$ is distributed according to $\pihat_h(\cdot \mid \pathmtil)$. Define the policy error  at scale $\epsilon > 0$ and step $h$ as
\begin{align}
\Delta_{\sigma,h}(\pihat;\epsilon) &:=  \inf_{\coup \in \couphatsigh}\Pr_{(\seqa,\seqa') \sim \coup}[\dmax(\seqa,\seqa') > \epsilon] 
\end{align}
\end{definition}



Lastly, we define a \emph{smoothed} version of $\pihat$, which ads the same noise $\sigma$ back in at inference time.
\begin{definition}[The smoothed policy]\label{defn:smoothed_policy} Let $(\pihat_h)$ be a chunking policy. We define the \emph{smoothed policy} $\pihat_{\sigma} = (\pihat_{\sigma,h})$, by letting $\pihat_{\sigma,h}(\cdot \mid \pathm)$ be distributed as $\pihat_{h}(\cdot \mid \pathmtil)$, where $\pathmtil  \sim \cN(\pathm,\sigma^2 \eye)$.
\end{definition}
We are now ready to state our main theorem. 
\begin{restatable}{theorem}{maintemp}\label{thm:main_template}
Let \Cref{asm:traj_regular,asm:Jacobian_Stable} hold, and let $c_1,\dots,c_5 > 0$ be as in \Cref{defn:prob_constants_body}. 
Suppose that the  $\epsilon,\sigma,\tauc > 0$ satisfy $\epsilon < c_2 $, and $\tauc \ge c_3/\eta$, and $ 5 \dimx + \log\left( \frac {4\sigma}{\epsilon} \right) \le c_4^2/(16\sigma^2)$. Then the marginal and final imitation loss of the \emph{smoothed }$\pihat_{\sigma}$ are bounded by
\begin{align}
    \Imitmarg[\epsilon_1](\pihat_{\sigma}) \vee \Imitfin[\epsilon_2](\pihat_{\sigma})  &\leq H\sqrt{2\taum-1}\left(\frac{2\epsilon}{\sigma} +  \iota(\epsilon) e^{-\frac{\eta(\tauc - \taum)}{\Lstab}}\right) +  \sum_{h=1}^H\Delta_{\sigma,h}(\pihat;\epsilon/c_1) \label{eq:mainguarantee}\\
    &\text{ where } \epsilon_1 =   \epsilon + \sigma \iota(\epsilon) , \quad 
    \epsilon_2 = \epsilon + \sigma e^{-\frac{\eta \tauc}{\Lstab}} \iota(\epsilon), 
    \end{align}
    and where  $\iota(\epsilon) = 6c_5\sqrt{5 \dimx + 2\log\left( \frac{4\sigma}{c_1 \epsilon} \right)}$ is logarithmic in $1/\epsilon$.
\end{restatable}

\Cref{thm:main_template} guarantees imitation of the distribution of marginals and final states of $\Dexp$. Unlike past work, it applies to the \emph{smoothed policy} $\pihat_{\sigma}$. The motivation is that adding noise at inference time \emph{removes distribution shift} coming from training on smoothed data; this simple observation is crucial for our theoretical guarantees, and differs from past work which does not smooth at inference time. 


 Each term in \eqref{eq:mainguarantee} can be made small by decreasing the amount of noise $\sigma$ in the smoothing, increasing the number of trajectories, and increasing the chunk length $\tauc$. Increasing $\tauc$ comes at the (implicit) expense of increasing the length of composite actions, thereby inducing a more challenging conditional generative modeling problem.  Decreasing $\sigma$ increases sensitivity to the tolerance $\epsilon$, and, as discuseed in \Cref{app:scorematching}, may make the underlying generative modeling problem more challenging. Note that the contribution of the additive $\sigma$-term in $\epsilon_2$, used for the final-state loss $\Imitfin$, is exponentially-in-$\tauc$ smaller than that in $\epsilon_1$. Interestingly, our theory suggest no benefit to increasing $\taum$ (corroborated empirically in \cite{chi2023diffusion}). \Cref{app:end_to_end} gives guarantees for imitating \emph{joint} trajectories under the further assumptions that (a) the  demonstrator has memory (or, more generally, a mixing time) of at most $\taum$, and (b) \emph{either} the demonstrator distribution  happens to satisfy a certain continuity property, \emph{or} $\sigma = 0$ and instead the learned $\pihat$ satisfies that same property.




%, we then guarantee imitation of the \emph{joint} trajectory distribution.


The proof of \Cref{thm:main_template} is conducted in  \Cref{sec:analysis}, via a general template for imitation learning of general stochastic policies; the arguments are based on the construction of sophisticated couplings between expert and learner trajectories, and in particular, showing that $\pihat_{\sigma}$ mimics a certain ``replica policy'' with certain favorable properties. The analysis relies heavily on continuous in total variation distance (TVC, \Cref{defn:tvc}, concretized as \Cref{defn:tvc_main} below). In \Cref{app:lbs}, we show that this  framework is essentially tight. 
The final proof is completed in \Cref{app:end_to_end}, which also lists various refinements, extensions and ancillary guarantees.
\begin{remark}[The $\epsilon=0$ case]
If we were able to bound the policy error $\Delta_{\sigma,h}(\pihat;\epsilon)$ with $\sigma = \epsilon = 0$ -- which corresponds to estimating $\seqa_h \mid \pathm$ in \emph{total variation} distance -- the imitation learning problem would be trivialized, and smoothing would not be needed  (see \Cref{app:end_to_end}).  \Cref{app:scorematching} explains that the needed assumptions for this stronger sense of approximate sampling do not hold in our setting, because expert distributions over actions typically lie on low-dimensional manifolds. 
\end{remark}


\nipspar{Stability, limitations, and future work.} We never explicitly model bifurcations; rather, we allow expert demonstrations to be sufficiently rich as to permit them. Eschewing global stability, $\tauc$ ensures that trajectories are long enough for the \emph{strictly local} stability assumptions in \Cref{asm:Jacobian_Stable} to provide benefit. Thus, non-Markovianity and multi-modality is challenging only insofar as it relates to the difficulty of local stabilization. 
%The role of $\tauc$ is best understood as ensuring that trajectories are long enough to stabilize around, which suggests the following surprising conclusion: \emph{non-Markovianity of the demonstrator is  only insofar as it a.} 
A key limitation of our work is that, to take advantage of local stability, we rely on either synthesized primitive controllers (in our analysis) or low-level stabilizing controllers built into problem environments (in our experiments). As noted in the Introduction, this obviates the needed for stronger query access to the expert (e.g. \cite{pfrommer2022tasil}), or for interactive data collection \cite{ross2010efficient}. 
Developing a more comprehensive approach to stability (perhaps one that does not require explicit gain synthesis, and extends to non-smooth systems) is an exciting direction for future work.  \Cref{app:related} compares our hierarchical approach to  stability to more standard notions, which we show rule out the possibility for bifurcated demonstrations. 


\subsubsection{The $\sigma = 0$ case: simplification via total variation continuity} We begin by providing guarantees in the special case where we use \emph{no data smoothing}, i.e. $\sigma = 0$. This requires the definition of total variation continuity (TVC), defined below.
\begin{definition}[TVC of Chunking Policies]\label{defn:tvc_main} Define the metric $\disttvc$ on memory chunks via $\disttvc(\pathm,\pathm') = \max_{t \in [t_h - \taum:t_h]} \norm{\bx_t - \bx_t'} \vee \max_{t \in [t_h - \taum:t_h - 1]} \norm{\bu_t - \bu_t'}$, where $\pathm = (\bx_{t_h-\taum:t_h},\bu_{t_h-\taum:t_h-1})$ and $\pathm' = (\bx_{t_h-\taum:t_h}',\bu_{t_h-\taum:t_h-1}')$. We say that $\pihat$ is total variation continuous with modulus $\gamma: \R_{\ge 0} \to \R_{\ge 0}$ 
 ($\gamma$-TVC) if, 
\begin{align}
\forall \pathm,\pathm', \quad \TV(\pihat(\pathm),\pihat(\pathm')) \le \gamma(\disttvc(\pathm,\pathm')).
\end{align}
\end{definition}
\begin{proposition}\label{prop:TVC_main} Suppose \Cref{asm:traj_regular,asm:Jacobian_Stable} hold, and suppose that $0 \le \epsilon < c_2 $, and $\tauc \ge c_3/\eta$. Then, for any non-decreasing non-negative $\gamma(\cdot)$ and $\gamma$-TVC chunking policy $\pihat$,
\begin{align}
\Imitmarg[\epsilon](\pihat) \le H\gamma(\epsilon) +  \sum_{h=1}^H\Delta_{\sigma=0,h}(\pihat;\epsilon/c_1) \label{eq:TVC_main}
\end{align}
In addition, suppose the expert distribution $\Dexp$ has at most $\taum$-bounded memory (defined formally in \Cref{defn:bounded_memory}). Then  $\Imitjoint[\epsilon](\pihat)$ satisfies the same upper bound \eqref{eq:TVC_main}, where $\Imitjoint[\epsilon](\pihat)$, formally defined in \Cref{def:loss_joint}, measures an optimal transport distance between the \emph{joint distribution} of the expert trajectory and the one induced by $\pihat$.
\end{proposition}
\begin{remark}\label{rem:TVC}
The proof of \Cref{prop:TVC_main} is considerably simpler than that of \Cref{thm:main_template}, and serves to build intuition for the latter; a formal proof is given  in \Cref{app:end_to_end} by combining results in \Cref{sec:analysis}. It is true than any $\pihat$ implemented as a DDPM with a Lipschitz activation with bounded-magnitude parameters is indeed TVC.  Unfortunately, these Lipschitz constants can be too large to be meaningful in practical scenarios, scaling exponentially with network depth. In addition, the absence of smoothing $\sigma$ may make the corresponding DDPM learning problem more challenging.   Hence, we require the additional sophistication of smoothing noise $\sigma > 0$ for meaningful guarantees.
\end{remark}




%\Cref{thm:main} leverages statistical learning guarantees for DPPMs to show our learned policy approximately samples from $\pidech$ in a truncated Wasserstein distance (\Cref{app:scorematching}). 


\subsection{\toda: Instantiation with DDPMs}
We now instantiate \Cref{thm:main_template} by showing that one can learn a policy $\pihat$ for which \Cref{defn:policy_error} is small  by fitting a DDPM to noise-smoothed data.



\paragraph{Algorithm.} Our proposed algorithm, $\toda$ (\Cref{alg:imitation_augmentation}) combines DDPM-learning of chunked policies as in \cite{chi2023diffusion} with a popular form of data-augmentation \citep{ke2021grasping}. We collect $\Nsample$ expert trajectories, synthesize gains, and segment trajectories into memory chunks $\pathm$ and composite actions $\seqa_h$ as described in \Cref{sec:setting}. We perturb  each $\pathm$ to form $\Naug$ chunks $\pathmtil$, as well as horizon indices $\dpind \in [\dphorizon]$ and inference noises $\bgamma \sim \cN(\seqa_h, (\dpstep \dpind_h)^2 \eye)$, 
and add these tuples $(\seqa_h, \pathmtil, \dpind_h, \bgamma_h,h)$ to our  data $\Imitdata$. We end the training phase by minimizing the standard DDPM loss \citep{song2019generative}\iftoggle{workshop}{ $\Lddpm(\theta, \Imitdata)$:
  \begin{align}
     \sum \norm{\bgamma_h - \scoref_{\theta,h}\left(e^{-\dpstep \dpind}\seqa_h + \sqrt{1 - e^{-2\dpstep \dpind}}\bgamma_h, \pathmtil, \dpind_h\right)}^2,\label{eq:ddpmcond}
  \end{align}
  where the sum is over $(\seqa_h, \pathctil,  \dpind_{h}, \bgamma_h,h) \in \Imitdata$.
}{:
\begin{align}
  \Lddpm(\theta, \Imitdata) = \sum_{(\seqa_h, \pathctil,  \dpind_{h}, \bgamma_h,h) \in \Imitdata} \norm{\bgamma_h - \scoref_{\theta,h}\left(e^{-\dpstep \dpind}\seqa_h + \sqrt{1 - e^{-2\dpstep \dpind}}\bgamma_h, \pathmtil, \dpind_h\right)}^2.\label{eq:ddpmcond}
\end{align}
}
Our algorithm differs subtly from past work in \Cref{line:test_time}: motivated by \Cref{thm:main_template}, we add smoothing noise \emph{back in} at test time. Here, the notation $\ddpm(\bs_{\theta,h}, \cdot) \circ \cN(\pathm, \sigaug^2 \eye)$ means, given $\pathm$, we perturb it to $\pathmtil \sim \cN(\pathm, \sigaug^2 \eye)$, and sample $\seqa_h \sim \ddpm(\bs_{\theta,h}, \pathmtil[h])$.  




\nipspar{Learning-Theoretic Assumptions.} We now state the assumptions required for our theoretical guarantees on \toda{}. We require access to a class of score functions rich enough to represent the following deconvolution conditionals. 
\newcommand{\Daught}[1][t]{\cD_{\sigma,h,[#1]}}
\newcommand{\aht}{\seqa_{h,[t]}}

\begin{definition}[Deconvolution Conditionals]\label{defn:dec_cond} For $h \in [H]$ and fixed $\sigma > 0$, let $\Daught$ denote the distribution of $(\pathmtil,\aht)$, where $(\pathmtil,\seqa_h) \sim \Daugh$, and where $\aht := e^{-t }\seqa_h^{(0)} + \sqrt{1 - e^{-2t}} \bgamma$ interpolates $\seqa_h$ with Gaussian noise $\bgamma \sim \cN(0,\eye)$; that is, $\Daught$ \emph{noises} the composite actions in $\Daugh$.\footnote{Note, that this noise is applied to actions, rather than \emph{smoothing} which refers to applying noise to policy inputs.} Finally, let $\pidecht \in \laws(\cA | \scrP_{\taum-1})$ denote the conditional distribution of $\aht \mid \pathmtil$ under $\Daught$, and let $\pidech = \pidecht[t=0]$ denote the conditional of $\seqa_h \mid \pathmtil$ under $\Daugh$.
\begin{comment}
\begin{itemize}
\item[(a)]$\pidech \in \laws(\cA | \scrP_{\taum-1})$ denote a conditional distribution of $\seqa_h = \sfk_{t_h:t_{h+1}-1} \mid \pathmtil$, where $\ctraj_T \sim \Dexp$, $\sfk_{1:T} = \synth(\ctraj_T)$, and $\pathm$ is the memory chunk of $\ctraj_T$ at step $h$, and $\pathmtil \sim \cN(\pathm, \sigma^2 \eye)$ augments $\pathm$ with noise
\item[(b)] $\pidecht \in \laws(\cA | \scrP_{\taum-1})$ denote the distribution of , where $\seqa_h^{(0)} \sim \pidech(\cdot | \pathm)$ and $\bgamma \sim \cN(0, \eye)$ is a standard Gaussian. \iftoggle{arxiv}{Thus, $\pidecht$ is the ``noised'' version of $\pidech$.}}{}
\end{itemize}
\end{comment}
\end{definition}
Next, we need an assumption of bounded statistical complexity. We opt for the popular \emph{Rademacher complexity} \cite{bartlett2002rademacher} In defining this quantity, recall that scores are vector-valued, necessitating the vector analogue of Rademacher complexity \cite{maurer2016vector,foster2019vector}, studied for score matching in \cite{block2020generative}.
\begin{definition}[Function Class $\Theta$ and Rademacher Complexity]\label{def:defn_of_comp} Consider a class of score functions of the form $\Theta = \left\{ (\scoref_{\theta,h})_{1 \le h \le H}  | \theta \in \Theta \right\}$, where $\scoref_{\theta,h}$ maps triples $(\seqa,\pathm,j)$ of composite actions $\seqa \in \cA$, memory chunks $\pathm$, and DDPM-steps $j \in \N$ to vectors in $\R^{\dA}$.  For each chunk $h \in [H]$, DDPM-step $j \in \N$ and discretization size $\alpha$, define the vector- Rademacher complexity of $\Theta$ as 
  \begin{align}
        \rad_{n,h,j}(\Theta;{\color{blue}\alpha}) := \ee\left[ \sup_{\substack{\theta \in \Theta}} \frac {1}{n} \sum_{k =1}^n \left\langle \bm{\epsilon}^{(k)},  \scoref_{\theta,h}\left(\seqa_{h,[j {\color{blue}\alpha}]}^{(k)}, \pathmtil^{(k)}, j\right)\right \rangle \right],
    \end{align}
    where $\bm{\epsilon}_k \in \R^d$ are i.i.d. random vectors with Rademacher coordinates, and where $(\pathmtil^{(k)},\seqa_{h,[j {\color{blue}\alpha}]}^{(k)})$ are i.i.d. samples from $\Daught[j {\color{blue}\alpha}]$.
\end{definition}
\newcommand{\Cgrow}{C_{\mathrm{grow}}}

\begin{assumption} \label{ass:score_realizability} We suppose that, for any $\sigma > 0$, we are given a class of score functions $\Theta = \Theta(\tauc,\taum,\sigma,\alpha)$ of the form in \Cref{def:defn_of_comp} which satisfies the following conditions:
\begin{itemize}
\item[(a)] \emph{Realizability:} there exists a $\theta_{\star}$ such that, for all $h \in [H]$ and $j \in \N$, $\scoref_{\theta_\star,h}\left(\seqa, \pathm, j\right) $ is the score function of $\pidecht[j\alpha](\pathm)$ at $\seqa \in \cA$.
\item[(b)] \emph{The Rademacher complexity of $\Theta$ has polynomial decay in $n$ and growth in $\alpha$:}
\begin{align}
  \sup_{j \in \N}\max_{h \in [H]}\rad_{n,h,j}(\Theta;\alpha) \le \Ctheta \alpha^{-1} n^{-\frac 1 \Cnu},
\end{align}
where $\Ctheta = \Ctheta(\sigma,\tauc,\taum) > 0$. 
\item[(c)] The scores have \emph{linear growth}; that is, there exists some $\Cgrow = \Cgrow(\sigma,\tauc,\taum) > 0$ sucht hat 
\begin{align}
\sup_{j \in \N, h \in [H]} \sup_{\theta \in \Theta}\norm{\scoref_\theta(\seqa, \pathmtil, j)} \le \Cgrow \alpha^{-1}(1+\norm{\seqa}+\norm{\pathmtil}),
\end{align} 
 As discussed in \Cref{app:scorematching}, generalizing to polynomial growth is straightforward. 
\end{itemize}
\end{assumption}

\begin{comment}
For the same $\sigma > 0$ as above, and for a given $\dpstep > 0$ and $\dpind \in \bbN$, let $\scoresthsig[\dpind]$ denote the score function of $\pidecht[\dpstep \dpind]$.  We suppose that for any $\dphorizon \in \bbN$ and $\alpha > 0$, we are given a class of scores $\Theta = \Theta(\tauc,\taum,\sigma)$ consisting of functions . 
\end{comment}
As justified in \Cref{app:scorematching}, our decay condition on the Rademacher complexity is natural for statistical learning, and holds for most common function classes (often with $\nu \le 2$ and even more benign dependence on $J,\alpha$);  our results can easily extend to approximate realizability as well. Our Rademacher bound depends  implicitly on chunk and memory lengths $\tauc,\taum > 0$ and implicitly on dimension $\dA$ via $C_{\Theta}$.  Realizability is motivated by the approximation power of deep neural networks \citep{bartlett2021deep}. Lastly, we do expect realizability to hold uniformly over $j \ge 0$ because, as $j \to 0$, the corresponding scores corresponds to a scaled identity function (i.e. the score of a standard Gaussian).



%Taken together, our assumptions effectively remove the need for Markovian, uni-modal demonstrations by appealing to two oracles. \Cref{asm:Jacobian_Stable} lets us synthesize  locally stabilizing controllers around possibly bifurcated demonstrations. Because stability in nonlinear control systems depends on dynamics along trajectories, we then require (approximate) realizability of \emph{joint distributions over sequences of primitive controllers} in \Cref{ass:score_realizability}. This latter assumption   


\begin{comment}
We further remark that the realizability assumption of the class $\Theta$ can be relaxed so that $\scoresthsig$ is $\epsilon$-misspecified in mean squared error.  Rigorous control of the quality of generated samples under \Cref{ass:score_realizability} can be realized by combining statistical learning guarantees in \citet{block2020generative} with sampling guarantees in \citet{chen2022sampling} \ab{cite holden lee's paper as well}; this analysis is conducted in \Cref{app:scorematching}.  We now abstracat away the generative modeling aspect of the analysis and consider the implications to imitation learning; indeed, our results are not specific to DDPMs but hold for any learned sampler satisfying a guarantee in an optimal transport distance defined in \Cref{sec:analysis} and any suboptimality in our final bound's dependence on the number of samples comes from looseness of the theoretical analysis of DDPMs in \citet{chen2022sampling}\ab{cite holden lee}.
\end{comment}

%; as we envision $\Theta$ as a class of deep neural-networks, our bounds rely on the capacity of these networks to model joint distributions of a certain length.  Similaly, we replace prior assumptions on policy stability \mscomment{dan}.



% \begin{corollary}\label{cor:samplingguarantee}
%   Suppose the assumptions in \Cref{prop:scorematchinginformal} hold and that $\dpstep$ and $\dpind$ are polynomial in problem parameters.\footnote{Exact dependence is given in \eqref{eq:samplingparameters}.}  Then with probability at least $1 - \delta$ for all $h \leq H$, it holds that
%   \begin{align}\label{eq:samplingguarantee}
%     \ee_{\pathm{h} \sim \Dexp}\left[ \inf_{\coup \in \couple(\emph{\ddpm}(\scoref_{\thetahat, h}, \pathm), \Dexp(\cdot | \pathm))} \pp_{(\seqahat_h, \seqast_h) \sim \coup}\left( \norm{\seqahat_h - \seqast_h} \geq \epsilon \right) \right] \leq \epsilon.
%   \end{align}
% \end{corollary}
% \Cref{cor:samplingguarantee} is proved in Appendix \ref{app:scorematching} and further discussion is deferred there for the sake of space.  


\begin{comment}
\begin{algorithm}[!t]
    \begin{algorithmic}[1]
    \State{}\textbf{Initialize} $K,K_0 \in \N$, synthesis oracle $\synth$, sample size $\Nsample,\Naug \in \N$, DDPM step size $\eta$, DDPM horizon $\tau$ \mscomment{function classes}
    \State{} $\Imitdata \gets \emptyset$.
    \For{$n =1,2,\dots \Nsample$}
    \State{}Sample $\ctraj = (x_{1:T+1},u_{1:T}) \sim \Dexp$ and set $\sfk_{1:H} = \synth(\ctraj)$.
    \State{} $\Imitdata \gets \Imitdata.\mathrm{append}(\{\ctraj,\sfk_{1:H}\})$ \qquad \algcomment{add things} \mscomment{stuff}
    \State{} Sample $k \sim \mathrm{Unif}([\tau])$ and $\epsilon \sim \cN(\sfk_{1:H}, k \eta \eye)$.
    \State{} $\Imitdata' \gets \Imitdata'.\mathrm{append}(\left\{ (\sfk_{1:H}, \ctraj,  k, \epsilon) \right\})$
    \EndFor
    \State{}Fit $\theta_h \in \argmin_{\theta \in \Theta}\Lddpm(\theta, h,\Imitdata')$, $h = 1,2,\dots,H$
    \State{} \textbf{return} $\pi = (\hat \pi_{1:H})$, where $\hat \pi_h(\bz) \sim \ddpm(\bs_{\theta_h}, \bz)$.
    \caption{Trajectory Optimization}
    \label{alg:imitation_noaugmentation}
    \end{algorithmic}
\end{algorithm}
\end{comment}
\newcommand{\ProjGain}{\mathtt{ProjGain}}
\begin{algorithm}[!t]
  \begin{algorithmic}[1]
  \State{}\textbf{Initialize} Synthesis oracle $\synth$, sample sizes $\Nsample,\Naug \in \N$, $\sigaug \ge 0$, DDPM step size $\dpstep > 0$, DDPM horizon $\dphorizon$, function class $\{\scoref_{\theta}\}_{\theta \in \Theta}$, gain magnitude $R >0$, empty data buffer $\Imitdata \gets \emptyset$. 
  \Statex{} \algcomment{For no smoothing, set $\sigaug = 0$ and $\Naug = 1$}
  \For{$n =1,2,\dots \Nsample$}
  \State{}Sample $\ctraj_T = (x_{1:T+1},u_{1:T}) \sim \Dexp$ and set $\sfk_{1:T} = \synth(\ctraj)$ 
  \Statex{}~~~~\algcomment{Segment $ \pathm[1:H]$ from $\ctraj_T$ and $ \seqa_{1:H}$ from $\sfk_{1:T}$}
  \For{$i = 1,2,\dots,\Naug$ and $h = 1,2,\dots,H$}
  \State{}Sample $\pathmtil \sim \cN(\pathm,\sigaug^2 \eye_{})$,  $\dpind_h \sim \mathrm{Unif}([\dphorizon])$ and $\bgamma_h \sim \cN(\seqa_h, (\dpind_h \alpha)^2 \eye)$.
    %
    \State{} $\Imitdata \gets \Imitdata.\mathrm{append}\left(\{ (\seqa_{h}, \pathctil[h],  \dpind_{h}, \bgamma_{h},h) \}\right)$
    \EndFor
  \EndFor
  \State{}Fit $\theta \in \argmin_{\theta \in \Theta}\Lddpm(\theta, \Imitdata)$, and let $\pihat = (\pihat_h)$ be given by $\pihat(\cdot \mid \pathm) = \ddpm(\bs_{\theta,h}, \pathm)$. \label{line:DDPM_train}
  \State{}\label{line:test_time} \textbf{return} $\pihat_{\sigma} = (\hat \pi_{\sigma,h})$, by smoothing $\pihat$ as per \Cref{defn:smoothed_policy}. 
  \caption{ \
  \textbf{H}ierarchical \textbf{I}mitation via \textbf{N}oising at Inference \textbf{T}ime ($\toda$)}
  \label{alg:imitation_augmentation}
  \end{algorithmic}
\end{algorithm}

\label{sec:asms_ctrl_body}


\paragraph{Theoretical Guarantee for \toda.} We now state our guarantee for \toda{}. Recall that $\dA$ denotes the dimension of composite actions, and that $c_1,\dots,c_5$ are as in \Cref{defn:prob_constants_body}.
\begin{theorem}\label{thm:main} 
 Consider running $\toda$ for $\sigma > 0$ with parameters $J,\alpha$ polynomial in the parameters given in \Cref{asm:traj_regular,asm:Jacobian_Stable} specified in \Cref{app:scorematching}.  Suppose that \Cref{asm:traj_regular,asm:Jacobian_Stable,ass:score_realizability} hold, and $\sigma,\epsilon,\tauc$ satisfy the conditions of \Cref{thm:main_template}. Then, $\Nsample \geq \poly\left( \Ctheta(\sigma,\tauc,\taum),1/\epsilon,  \Rstab, \dA,\log(1/\delta)\right)^{\nu} > 0$, then with probability $1-\delta$, the policy $\pihat_{\sigma}$ returned by \toda{} satisfies
 \begin{align}
 \Imitmarg[\epsilon_1](\pihat_{\sigma}) \vee \Imitfin[\epsilon_2](\pihat_{\sigma})  &\leq H\sqrt{2\taum-1}\cdot\left(\epsilon + \frac{2\epsilon}{\sigma} +  \iota(\epsilon) e^{-\frac{\eta(\tauc - \taum)}{\Lstab}}\right), \label{eq:guarantee_ddpm}
 \end{align}where $\epsilon_1 = \tilde\cO(\epsilon + \sigma)$ and $\epsilon_2 = \tilde\cO(\epsilon + \sigma e^{-\Omega(\tauc)})$ and $\iota(\epsilon) = \tilde{\cO}(1)$ are as in \Cref{thm:main_template}. 

In addition, consider running  \toda{} with $\sigma=0$, and suppose $\Ctheta(\sigma,\tauc,\taum)\big{|}_{\sigma=0}$ is finite. Then, for $\Nsample$ satisfying the same bound as above, it holds that with probability $1-\delta$, the policy $\pihat$ produced by \toda{} satisfies the guarantees of \Cref{prop:TVC_main} up to an additive factor of $H\epsilon$ on the event that $\pihat$ happens to be $\gamma$-TVC.
\end{theorem}
\Cref{thm:main} instantiates \Cref{thm:main_template}  by bounding the policy errors $\Delta_{\sigma,h}(\pihat ; \epsilon)$, where $\pihat$ is the policy learned by fitting the DDPM in \Cref{line:DDPM_train}; note that the guarantee applies to the \emph{smoothed policy} in \Cref{line:test_time}. The formal bound on $\Nsample$ is given in \eqref{eq:sample_complexity_app} in \Cref{app:end_to_end}. Making $\epsilon > 0$ smaller  comes at the expense of increasing the sample complexity $\Nsample$ via increasing $\Ctheta(\sigma,\tauc,\taum)$. Optimizing the above bound comes from trading off $\sigma$ in $\epsilon_1,\epsilon_2$ with $\epsilon/\sigma$ \eqref{eq:guarantee_ddpm}, and also increasing the sampling complexity.   In addition, while making $\tauc$ larger appears to improve the above bound, it generally increases the statistical and computational challenge of learning the DDPM itself. The guarantees for score matching are derived in \Cref{app:scorematching} by applying \cite{chen2022sampling,lee2023convergence,block2020generative}; these are applied to \Cref{thm:main_template} in \Cref{app:end_to_end}.


%\mscomment{discuss}
%a more expressive score class $\Theta$ (requiring greater $\Nsample$); similarly, as explained in \Cref{app:scorematching}, the scores $\scoresthsig$ may become harder to learn $\sigma$ decreases.


%\mscomment{to read.}
%Further, as we show in Appendix \ref{app:lowerbound} \ab{TODO: Max, do this lower bound}, the only looseness in the above bound is in the guarantee on the quality of the learned DDPM; indeed, as will be clear from the analysis, we can replace the DDPM in $\toda$ with an arbitrary generative model, as long as a certain optimal transport cost is small between the learned generative model and the expert policy.  We make two further remarks regarding \Cref{thm:main}.  First, while the result controls the marginal and final losses of the learned policy with respect to the expert $\Dexp$, in fact we can also control the a stronger ``joint'' loss with respect to the closely related ``replica'' distribution, defined below.  Second, as written, \eqref{eq:mainguarantee} does not apply to the limiting case when there is no augmentation, i.e., when $\sigaug = 0$; in Appendix \ref{app:no_augmentation}, however, we control the imitation loss for $\toda$ without augmentation under a strong continuity assumption on the learned policy $\pihat$.  Our theoretical results indicate that, outside of such situations, data augmentation will be an effective tool \ab{mention experiments?}
% 
%\begin{remark} If we were able to sample construct samples close to $\pidech$ in total variation,
%\end{remark}
%We emphasize that  
%That our guarantees only hold on the marginal distributions is a consequence of data augmentation. In \mscomment{..}, we show that if $\sigma = 0$ and our learned policy $\pihat$ happens to satisfy a certain continuity property, we do guarantee imitation of the joint trajectory distribution.
% 
 %Unfortunately, as discussed in \Cref{rmk:notvguarantee}, the assumptions under which this is possible are far too strong to hold in our setting.  Appropriately handling the weaker optimal transport metric under which we do have theoretical guarantees for sampling is one of the key challenges in our analysis.
% 
% 
% 
% 
% 
% \paragraph{Old Adam Stuff:}
% \mscomment{let's discuss}
% Building on the guarantee in \citet{block2020generative} for unconditional score estimation and that in \citet{chen2022sampling} \ab{cite the holden lee paper as well} for sampling, we provide can control the quality of the learned conditional DDPM as follows:
% \begin{proposition}
%   Let $\thetahat$ denote the $\theta$ minimizing $\Lddpm(\theta, \Imitdata)$.  Suppose that there is some $\thetast \in \Theta$ such that $\scorefst = \scoref_{\thetast}$, that $\norm{\scoref_\theta} \leq B$, and that $\Nsample \geq \poly\left( \comp(\Theta), \epsilon^{-1}, \log\left( \dphorizon/\delta \right), d,  \dpstep^{-1}\right)$, where $\comp(\Theta)$ is learning-theoretic notion of complexity.\footnote{We give a precise bound in \Cref{prop:scorematching} depending on the Rademacher complexity of the induced function class, which can, in turn be bounded by standard metric notions of complexity as in, e.g., \citet{wainwright2019high}.}  For some $\dpstep$ and $\dpind$ polynomial in all problem parameters\footnote{Exact dependence is given in \eqref{eq:samplingparameters}.}, with probability at least $1 - \delta$, it holds for all $h \leq H$ that
%   \begin{align}
%     \ee_{\pathm{h} \sim \Dexp}\left[ \inf_{\coup \in \couple(\emph{\ddpm}(\scoref_{\thetahat, h}, \pathm), \Dexp(\cdot | \pathm))} \pp_{(\seqahat_h, \seqast_h) \sim \coup}\left( \norm{\seqahat_h - \seqast_h} \geq \epsilon \right) \right] \leq \epsilon.
%   \end{align}
% \end{proposition}
% The proof of \Cref{prop:samplingguarantee} combines a standard statistical learning analysis of \citet{block2020generative} with the analysis of \citet{chen2022sampling} \ab{cite holden lee as well} and can be found in Appendix \ref{app:scorematching}.  Under strong conditions on $\Dexp$, \citet[Theorem 2]{chen2022sampling} can be generalized to ensure that the sampling distribution is close to the true distribution in TV; as discussed in \Cref{rmk:notvguarantee}, the regularity properties required are too strong to hold in our setting and we thus instead apply \citet[Corollary 4]{chen2022sampling} to recover a weaker guarantee.  Appropriately handling this weaker metric is one of the key technical challenges in our analysis.  We now suppose that the guarantee in \eqref{eq:samplingguarantee} holds and consider the implications to imitation learning; indeed, our results are not specific to DDPMs but hold for any learned sampler satisfying \eqref{eq:samplingguarantee} and any suboptimality in our final bound's dependence on the number of samples comes from looseness of the theoretical analysis of DDPMs in \citet{chen2022sampling}.  We now discuss the assumptions required to state our guarantee.