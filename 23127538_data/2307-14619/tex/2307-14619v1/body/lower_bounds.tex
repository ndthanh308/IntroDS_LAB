%!TEX root = ../neurips_main.tex

\newcommand{\Preph}[1][h]{\lawP^{\star}_{\circlearrowleft,#1}}
\newcommand{\Pdech}[1][h]{\lawP^{\star}_{\mathrm{dec},#1}}
\section{Lower Bounds}\label{app:lbs}
In this section, we establish lower bounds against the imitation results in the composite MDP. Specifically, we show that 
\begin{itemize}
	\item In \Cref{sec:lb_sharpness}  we show that \Cref{thm:smooth_cor,prop:IS_general} are sharp in the regime where $\gamipsone = \gamipstwo = 0$.
	\item In \Cref{sec:lb_diff_joints}, we show that the marginals of an expert policy $\pist$ and  replica policy $\pirep$ can coincide, but their joint distributions can be different. By considering $\pihat = \pidec$ in \Cref{thm:smooth_cor}, this establishes the necessity of considering the marginal imitation gap with respect to $\pist$. 
	\item In \Cref{sec:lb_ipstwo}, we lower bound the distance between \emph{marginal distributions} over states under $\pist$ and $\pirep$ in the regime where $\gamipstwo \ne 0$. This example demonstrates that the dependence of $\gamipstwo$ in \Cref{thm:smooth_cor} is essentially sharp. 
	\item In \Cref{sec:lb_pirep_dech}, we show that for an expert policy $\pist$ and smoothing kernel $\Wsig$, the state distributions under $\pirep$ and $\pidec$ can have different marginals (and thus different joint distributions). By considering $\pihat = \pidec$ in \Cref{thm:smooth_cor}, this explains why it is necessary to smooth $\pihat$ to $\pihat \circ \Wsig$.
\end{itemize}
Taken together, the above counterexamples show that our distinctions between joint and marginal distributions, decision to add noise at inference time, and dependence on almost all problem quantities in \Cref{sec:analysis} are sharp. We do not, however, establish necessity of $\gamipsone$ in the interest of brevity; we believe this quantity is necessary. Still, the  $\gamipsone$ term contributes a factor exponentially small in $\tauc$ in \Cref{thm:main}, so we deem lower bounds establishing its necessity of lesser importance. 

\paragraph{Commonalities of construction.}
In all but \Cref{sec:lb_ipstwo},  we take the action and state spaces to be 
\begin{align}
\cS = \cA = \R,
\end{align}
which is the archetypal Polish space \citep{durrett2019probability}. Throughout, we use $\dirac_x$ to denote the dirac-delta distribution on $x \in \R$. We let $\dists(\seqs',\seqs) = \disttvc(\seqs',\seqs) = |\seqs' - \seqs|$ and $\dista(\seqa',\seqa) = |\seqa' - \seqa|$ all be the Euclidean distance. 



\subsection{Sharpness of Proposition \ref{prop:IS_general} and Theorem \ref{thm:smooth_cor}}\label{sec:lb_sharpness}
Here, we  demonstrate that \Cref{prop:IS_general} is tight up to constant factors, and that \Cref{thm:smooth_cor} is tight up to the terms $\gamipsone,\gamipstwo$ and concentration probability $p_r$.
Consider the simple dynamics
\begin{align}
F_h(\seqs,\seqa) = \seqa.
\end{align} 
Note that, as the dynamics are state-independent, we have $\gamipsone(\cdot) = \gamipstwo(\cdot) \equiv 0$. 
Furthermore, let us assume policies do not depend on time index $h$. Let $\pist: \seqs \to \dirac_0$ be deterministic, and let $\Dinit = \dirac_0$ be an initial state distribution concentrated on $0$. Then, $\Dist_{\pist}$ is the dirac distribution on the all-zero trajectory. 

Fix parameters $0 < \epsilon < \sigma$, and $p \in (0,1)$. We consider the following smoothing-kernel 
\begin{align}
\lawW_{\epsilon,\sigma} = \begin{cases}  \dirac_{0}& \seqs \le 0\\
 (1-\frac{s}{\sigma})\dirac_{0} + \frac{s}{\sigma}\dirac_{\sigma} & \seqs \in [0,\sigma]\\
 \dirac_{\sigma} & \seqs > \sigma,
\end{cases}
\end{align}
Define the candidate policy
\begin{align}
\pihat_{\epsilon,p,\sigma}(\seqs) := \begin{cases} (1-p)\dirac_{\epsilon} +p\dirac_{\sigma} & \seqs \le \frac{\epsilon}{2}\\
\dirac_{\sigma} & \seqs > \frac{\epsilon}{2}
\end{cases}
\end{align}
\begin{proposition} For any $p \in (0,1)$, $0 < \epsilon < \sigma$, set $\bar{\pi} = \pihat_{\epsilon, p,\sigma} \circ \lawW_{\sigma,\epsilon}$. Then,
\begin{enumerate}[(a)]
\item $\pist$, $\pirep$ and $\pidec$ all map $\seqs \to \dirac_0$, $\Psth = \dirac_{0}$, and thus for any $\tilde \pi \in \{\pist,\pirep,\pidec\}$,
\begin{align}
\Exp_{\sstar_h \sim \Psth}\Exp_{s_h' \sim \Wsig(\sstar_h)}[\drob(\pihat_{\epsilon, p,\sigma}(s_h') \parallel \tilde \pi(s_h')] = \Exp_{\sstar_h \sim \Psth}[\drob(\bar \pi(\sstar_h) \parallel \tilde \pi(\sstar_h))] = p.
\end{align}
\item The kernel $\lawW_{\sigma,\epsilon}$ is $\gamma_{\sigma}$-TVC, where $\gamma_{\sigma}(u) = u/\sigma$.
\item For a universal constant $c > 0$,
\begin{align}
\gapjoint(\bar \pi \parallel \pist) = 
\gapmarg(\bar \pi \parallel \pist) &\ge c \min\{1,H(p + \epsilon/\sigma)\},
\end{align}
and the same holds with $\pist$ replaced by $\pirep$ or $\pidec$.
\end{enumerate}
\end{proposition}
In particular, 
the above proposition shows that 
\begin{align}
\gapjoint(\bar \pi \parallel \pist) = \gapmarg(\bar \pi \parallel \pist) \gtrsim H\gamma_{\sigma}(\epsilon) + \sum_{h=1}^H\Exp_{\sstar_h \sim \Psth}[\drob(\pibar(\sstar_h) \parallel  \pist(\sstar_h)],
\end{align}
verifying the sharpness of \Cref{prop:IS_general} (note that $\pibar = \pihat_{\epsilon,p,\sigma} \circ \Wsig$ is $\gamma_{\sigma}$ TVC). Similary, our above proposition shows that,
\begin{align}\label{eq:LB_smooth_cor_general}
\gapjoint(\bar \pi \parallel \pirep) = \gapmarg(\bar \pi \parallel \pist) \gtrsim H\gamma_{\sigma}(\epsilon) + \sum_{h=1}^H\Exp_{\sstar_h \sim \Psth}[\drob(\pihat_{\epsilon, p,\sigma}(\sstar_h) \parallel \pidech(\sstar_h)],
\end{align}
verying that \Cref{thm:smooth_cor} is sharp up to the additional stability terms $\gamipsone,\gamipstwo$.


\begin{proof} We begin with a computation. Define 
\begin{align}
\eta(\seqs) = 1- (1-p)(1-\frac{\seqs}{\sigma}) = p + (1-p)\frac{\seqs}{\sigma}
\end{align}
We compute
\begin{align}
\bar \pi = \pihat_{\epsilon, p,\sigma} \circ \lawW_{\sigma,\epsilon} &= \begin{cases} (1-p)\dirac_{\epsilon} + p\dirac_{\sigma} & \seqs \le \frac{\epsilon}{2}\\
\dirac_{\sigma} & \seqs > \frac{\epsilon}{2}
\end{cases} \circ  \begin{cases}  \dirac_{0}& \seqs \le 0\\
 (1-\frac{s}{\sigma})\dirac_{0} + \frac{s}{\sigma}\dirac_{\sigma} & \seqs \in [0,\sigma]\\
 \dirac_{\sigma} & \seqs \ge \sigma.
\end{cases}\\
&= \begin{cases} (1-p)\dirac_{\epsilon} + p\dirac_{\sigma} & \seqs \le 0\\
 (1-\eta(s))\dirac_{\epsilon} + \eta(s) \dirac_{\sigma} & 0 \le \seqs \le\sigma\\
 \dirac_{\sigma} & \seqs > \sigma.
\end{cases} \label{eq:pibar_comp}
\end{align}
In particular,
\begin{align}
\pihat(0) = \pi_{\epsilon, p,\sigma}(0) = (1-p)\dirac_{\epsilon} + p\dirac_{\sigma}
\end{align}
\paragraph{Part (a).} Notice that the support of the deconvolution and replica distributions are always in the support of $\Psth$, which is always $\seqs = 0$ under $\pist$. Thus, $\pist = \pirep = \pidec$. By the same token, for any policy $\pi$,
\begin{align}
\Exp_{\sstar_h \sim \Psth}[\drob(\pi(\sstar_h) \parallel \tilde \pi_{\star}(\sstar_h)] = \Pr[|\pi(0)| > \epsilon].
\end{align}
Hence, as $\bar \pi(0) = \pihat_{\epsilon, p,\sigma}(0) = (1-p)\dirac_{\epsilon} + p\dirac_{\sigma}$, and as $\sigma > \epsilon$, part (a) follows.


\paragraph{Part (b).} Consider $\seqs, \seqs' \in \cS$. We can assume, from the functional form of $\lawW_{\epsilon,\sigma}(\cdot)$, that $0 \le \seqs \le \seqs' \le \sigma$. Then, 
\begin{align}
\TV(\lawW_{\epsilon,\sigma}(\seqs), \lawW_{\epsilon,\sigma}(\seqs')) = \TV(\dirac_{0}(1-\frac \seqs \sigma) + (\frac \seqs \sigma)\dirac_{\sigma},\dirac_{0}(1-\frac{\seqs'}{\sigma}) + (\frac{\seqs'}{\sigma} )\dirac_{\sigma} = \frac{|\seqs' - \seqs|}{\sigma},
\end{align}
establishing total variation continuity. 




\paragraph{Part (c)} In view of part (a), it suffices to bound gaps relative to $\pist$.  Let $\Pr$ denote probabilities over $\seqs_{1:H+1},\seqa_{h}$ under $\bar \pi$. Let $\cA_{1,h}$ denote the event that at step $h$, $\seqa_h = \epsilon$, and let $\cA_{2,h}$ denote the event that $\seqa_h = \sigma$. As the state $\seqs_0$ is absoring and as $F_h(\seqs,\seqa) = \seqa_h$, the following events are equal
\begin{align}
\{\exists h: |\seqa_h|\vee |\seqs_{h+1}| > \epsilon\} = \cA_{2,H}.
\end{align}
Hence, 
\begin{align}
\gapjoint(\bar\pi \parallel \pist) = \Pr[\cA_{2,H}].
\end{align}
Moreover, as $\cA_{2,H}$ is measurable with respect to the marginal of $\seqa_H$, we also have that 
\begin{align}
\gapmarg(\bar \pi \parallel \pist) = \Pr[\cA_{2,H}].
\end{align}
It thus suffices to lower bound $\Pr[\cA_{2,H}]$. By definition of $\bar \pi$, the events $\cA_{1,h},\cA_{2,h}$ are exhaustive: $\cA_{1,h}^c = \cA_{2,h}$. Moreover, from \eqref{eq:pibar_comp},
\begin{align}
\Pr[\cA_{2,h+1} \mid \cA_{2,h}] = 1, \quad \Pr[\cA_{2,h+1} \mid \cA_{1,h}] = \eta(\epsilon), \quad \Pr[\cA_{1,1}] = 1-\eta(0) \ge 1-\eta(\epsilon).
\end{align}
Thus,
\begin{align}
\Pr[\cA_{2,H}] &= \Pr[\cA_{2,H} \mid \cA_{2,H-1}]\Pr[\cA_{2,H-1}] + \Pr[\cA_{2,H} \mid \cA_{1,H-1}]\Pr[\cA_{1,H-1}]\\
&= \Pr[\cA_{2,H-1}] + \eta(\epsilon)\Pr[\cA_{1,H-1}]\\
&= \Pr[\cA_{2,H-2}] + \eta(\epsilon)\left(\Pr[\cA_{1,H-1} + \Pr[\cA_{1,H-2}]\right)\\
&= \eta(\epsilon)\left(\sum_{h=1}^{H-1} \Pr[\cA_{1,h}]\right) + \Pr[\cA_{2,1}]\\
&\ge \eta(\epsilon)\left(\sum_{h=1}^{H-1} \Pr[\cA_{1,h}]\right) 
\end{align}
Moreover, as $\seqs_0$ is absorbing,
\begin{align}
 \Pr[\cA_{1,h}] =  \Pr[\cA_{1,h} \mid \cA_{1,h-1}]\Pr[\cA_{1,h-1}] = (1 - \eta(\epsilon))\Pr[\cA_{1,h-1}].
\end{align} 
Combining with $\Pr[\cA_{1,1}] = (1-p) \ge (1-\eta(0)) \ge 1-\eta(\epsilon)$, we have $\Pr[\cA_{1,h}] \ge (1 - \eta(\epsilon))^{h}$.  Hence, 
\begin{align}
\Pr[\cA_{2,H+1}] &\ge  \eta(\epsilon)\left(\sum_{h=1}^{H-1}(1 - \eta(\epsilon))^{h}\right)\\
&= \eta(\epsilon) \frac{1-\eta(\epsilon) - (1 - \eta(\epsilon))^{H}}{1-(1 - \eta(\epsilon))}\\
&= 1 - \eta(\epsilon) - (1 - \eta(\epsilon))^{H}\\
&= \Omega(\min\{1,H(\eta(\epsilon)\})
\end{align}
as $\eta(\epsilon) \downarrow 0$.  Subsituting in $\eta(\epsilon) = p + (1-p)\epsilon/\sigma = \Omega(p+\epsilon/\sigma)$ concludes.
\end{proof}


\subsection{$\pirep$ and $\pist$ induce the same marginals but different joint distributions, even with memoryless dynamics}\label{sec:lb_diff_joints}
We give a simple example where $\pirep$ and $\pist$ induce the same marginal distributions over trajectories, but different joints. As we show, this example demonstrates the necessity of measuring the marginal imitation error of a smoothed policy, $\gapmarg$, over the joint error, $\gapjoint$.  A graphical (but nonrigorous) demonstration of this issue can be seen in \Cref{fig:figure_eight} in \Cref{app:related}.


Again, let $\cS = \cA = \R$, and $F_h(\seqs,\seqa) = \seqa$. We let 
\begin{align}
\Wsig(\cdot) = \cN(\cdot,\sigma^2)
\end{align}
denote Gaussian smoothing. Fix some $\epsilon > 0$. Define 
\begin{align}
\Dinit = \frac{1}{2}(\dirac_{-\epsilon} + \dirac_{+\epsilon}), \quad 
\pist(\seqs) = \begin{cases}\dirac_{-\epsilon} & \seqs \le 0\\
\dirac_{\epsilon} & \seqs > 0\\
\end{cases}.
\end{align}
Thus, $\Dist_{\pist}$ is supported on the trajectories with $(\seqs_{1:H+1},\seqa_{1:H})$ being either all $\epsilon$ or all $-\epsilon$, and 
\begin{align}
\Psth = \Dinit = \frac{1}{2}(\dirac_{-\epsilon} + \dirac_{+\epsilon}).
\end{align}
Hence, the replica and deconvolution map to distributions supported on $\{\epsilon,-\epsilon\}$. Let $\phi_{\sigma}(\cdot)$ denote the Gaussian PDF with variance $\sigma$. Then,
\begin{align}
\Qdech(\seqs) = \frac{\dirac_{\epsilon}\phi_{\sigma}(\seqs - \epsilon) + \dirac_{-\epsilon}\phi_{\sigma}(\seqs + \epsilon)}{\phi_{\sigma}(\seqs -\epsilon) + \phi_{\sigma}(\seqs + \epsilon)}.
\end{align}
Moreover, 
\begin{align}
\Qreph(\seqs) = \Exp_{Z \sim \cN(0,\sigma^2)}\left[\frac{\dirac_{\epsilon}\phi_{\sigma}(\seqs - \epsilon + Z) + \dirac_{-\epsilon}\phi_{\sigma}(\seqs + \epsilon +Z )}{\phi_{\sigma}(\seqs -\epsilon +Z ) + \phi_{\sigma}(\seqs + \epsilon +Z)}\right].  \label{eq:Z_alternating}
\end{align}
One can check that for  $\epsilon \le \sigma$,
\begin{align}
	\Qreph(u\epsilon) = \Theta\left( \frac{(1+\frac{c \epsilon}{\sigma})\dirac_{u\epsilon} + (1-\frac{c \epsilon}{\sigma})\dirac_{-u\epsilon}}{2}\right), \quad u \in \{-1,1\}
\end{align}
for $\epsilon \ll 1$.  In particular, for $\seqs \in \{-\epsilon,\epsilon\}$
\begin{align}
\Pr_{\seqa \sim \pireph(\seqs)}[\seqa= -\seqs] \ge \Omega(1). \label{eq:opposite_lb}
\end{align}
In particular, if $(\srep_{1:H+1},\arep_{1:H}) \sim \Dist_{\pirep}$, then 
\begin{align}
\Pr[\exists h: \dist(\srep_h,\srep_{h+1}) > \epsilon] &\le \Pr[\exists h: \srep_h = -\srep_{h+1}] \\
&\le \Pr[\exists h: \srep_h = -\arep_h] = 1 - \exp(-\Omega(H)),
\end{align}
where in the last step we used \eqref{eq:opposite_lb} and the the fact that the $\pirep$ uses fresh randomness at each round. Moreover, as $\pist$ always commits to either an all-$\epsilon$ or all-$(-\epsilon)$-trajectory, we see that for any $\coup \in \couple(\Dist_{\pist},\Dist_{\pirep})$ over $(\sstar_{1:H+1},\seqa^\star_{1:H}) \sim \Dist_{\pist}$ and $(\srep_{1:H+1},\arep_{1:H}) \sim \Dist_{\pirep}$, 
\begin{align}
\gapjoint(\pirep,\pist) \ge 
\Pr_{\coup}[\exists 1\le h \le H: \dist(\sstar_{h+1},\srep_{h+1}) > \epsilon] \ge1 - \exp(-\Omega(H)),
\end{align}
That is, the replica and expert policies have different joint state distribution.
\begin{remark}
The above result demonstrates the necessity of measuring the marginal error between $\pihat \circ \Wsig$ and $\pist$ in \Cref{thm:smooth_cor}: if we apply that proposition with $\pihat = \pidec$, then for all $\epsilon$, $\Exp_{\sstartil_h \sim \Wsig(\sstar_h) } \drob( \pihat_{h}(\sstartil_h) \parallel \pidec(\sstartil_h)) = 0$. But then $ \pihat \circ \Wsig = \pirep$, and we know that $\gapjoint(\pirep,\pist) \ge 
\Pr_{\coup}[\exists 1\le h \le H: \dist(\sstar_{h+1},\srep_{h+1}) > \epsilon] \ge1 - \exp(-\Omega(H))$. Thus, we cannot hope for smoothed policies to imitate expert demonstrations in joint state distributions without additional assumptions.
\end{remark}

%Hence, with constant probability $\tilde \seqs_h \sim \Qreph(\seqs_h) = -\seqs_h$, for $\seqs_h \in \supp(\Psth)$. Hence, the trajectory induced by $\pirep$ will oscillate between $\{-\epsilon,+\epsilon\}$, whilst $\pist$ always picks one side. One can use this to verify that 
%\begin{align} = \Omega(1), \forall \epsilon' \in (0,\epsilon).
%\end{align}

\begin{remark}[Importance of chunking] Above we have shown that $\pirep$ oscillates between $\epsilon$ and $-\epsilon$ (for actions and subsequent states). 
We remark that these oscillations can have very deleterious effects on performance on real control systems. This is why it is beneficial to predict entire sequences of trajectories. Indeed, consider a modified  construction such that $\cS = \cA = \R^K$, and $F_h(\seqs,\seqa) = \seqa$. Here, we interpret $\cS$ as a sequence of $K$-control states in $\R$, and $\seqa$ as sequence of $K$-actions, denoting the $i$-th coordinate of $\seqs$ via $\seqs[i]$,
\begin{align}
\pist(\seqs) = \begin{cases}\dirac_{-\epsilon\mathbf{1}} & \seqs[1] \le 0\\
\dirac_{\epsilon \mathbf{1}} & \seqs[1] > 0,
\end{cases}
\end{align}
Then, we can view the oscillations in $\pirep$ as oscillations between length $K$ trajectories, which is essentially what happens in our analysis for $K = \tauc$.
\end{remark}



\subsection{ $\pirep$ and $\pist$ can have different marginals, implying necessity of $\gamipstwo$}\label{sec:lb_ipstwo}
Our construction lifts the construction in \Cref{sec:lb_diff_joints} to a  two-dimensional state space $\cS = \R^2$, keeping one dimensional actions $\cA = \R$. Let $\seqs = (\seqs[1],\seqs[2])$ denote coordinate of $\seqs \in \cS$. For some parameter $\nu$, the dynamics are
\begin{align}
\seqs_{h+1} = F_h(\seqs_h,\seqa_h) = (\seqa_h, \nu\cdot(\seqs_h[1] - \seqa_h))
\end{align}
We let $\dists = \disttvc = \distips$ denote the $\ell_1$ norm on $\cS = \cR^2$. Our initial state distribution is 
\begin{align}
\Dinit = \frac{1}{2}\left(\dirac_{(\epsilon,0)} + \dirac_{(-\epsilon,0)}\right)
\end{align}
We let
\begin{align}
\pist(\seqs) = \begin{cases}\dirac_{(-\epsilon,0)} & \seqs \le 0\\
\dirac_{(\epsilon,0)} & \seqs > 0\\
\end{cases}.
\end{align}
Thus, $\pist$ induces trajectories which either stay on $\dirac_{(\epsilon,0)}$ or $\dirac_{(-\epsilon,0)}$.
\begin{align}
\Psth = \frac{1}{2}\left(\dirac_{(\epsilon,0)} + \dirac_{(-\epsilon,0)}\right), \quad \forall h \ge 1.
\end{align}
Let
\begin{align}
\Wsig(\seqs) = \cN(\seqs',\sigma^2)
\end{align}
\begin{proposition}\label{prop:gamipstwo_lb} In the above construction, we can take $\gamipstwo(u) \le \nu \cdot u$ in \Cref{defn:ips_body}, and $p_r$ satisfies the conditions in \Cref{thm:smooth_cor} for $r = 2\sigma \sqrt{\log(1/p_r)}$.  Moreover, for any $\epsilon \le \sigma$, 
\begin{align}
\gapmarg[\epsilon'](\pirep \parallel \pist) \ge \Omega(1), \quad \epsilon' = \nu \epsilon
\end{align}
\end{proposition}
\begin{remark}[Sharpness of $\gamipstwo$]Before proving this proposition, we note that if we take $\epsilon = \sigma$ and $r = 2\sigma \sqrt{\log(1/p_r)}$, then $\nu \epsilon = \tilde{\Omega}(\gamips(2r))$, showing that our dependence on $\gamipstwo$ is sharp up to logarithmic factors. Moreover, the looseness up to logarithmic factors in the above point is an artifact of using the Gaussian smoothing $\Wsig$, and can be remover by replaced $\Wsig$ with a truncated-Gaussian kernel. 
\end{remark}
\begin{proof}[Proof of \Cref{prop:gamipstwo_lb}] To see $\gamipstwo(u) \le \nu \cdot u$, we have $\|F_h(\seqs,\seqa)- F_h(\seqs',\seqa)\| = \|(\seqa, \nu\cdot(\seqs[1] - \seqa)) - (\seqa, \nu\cdot(\seqs'[1] - \seqa))\| = \nu|\seqs[1] -\seqs'[1]|\le \nu\disttvc(\seqs,\seqs')$. That we can take $r = 2\sigma \sqrt{\log(1/p_r)}$ follows from Gaussian concentration.

To prove the final claim,  one can directly generalize \eqref{eq:Z_alternating} to find that, for any $b \in \R$, 
\begin{align}
\Qreph(\seqs) = \Exp_{Z \sim \cN(0,\sigma^2)}\left[\frac{\dirac_{(\epsilon,0)}\phi_{\sigma}(\seqs[1] - \epsilon + Z) + \dirac_{(-\epsilon,0)}\phi_{\sigma}(\seqs[1] + \epsilon +Z )}{\phi_{\sigma}(\seqs[1] -\epsilon +Z ) + \phi_{\sigma}(\seqs[1] + \epsilon +Z)}\right]. 
\end{align}
This follows form the observation that $\Qreph$ and $\Psth$ have the same support, and as $\Psth$ always is support on vectors with second coordinate zero, that the second coordinate of $\seqs$ in $\Qreph(\seqs)$ is uninformative. For $\epsilon \le \sigma$, we find that 
\begin{align}
\Qreph((\epsilon,b)) = c\dirac_{(\epsilon,0)} + (1-c)\dirac_{(-\epsilon,0)}, c = \Omega(1), b \in \R.
\end{align}
and $\Qreph((-\epsilon,b))$ is defined symmetrically, Hence, under $(\srep_{1:H+1},\arep_{1:H}) \sim \pirep$,
\begin{align}
\Pr[ \srep_{1} \ne \arep_1] \ge \Omega(1)
\end{align}
Moroever, when $\srep_{2} \ne \arep_h$, we have that $|\srep_{2}[2]| = \nu|\srep_1 - \arep_1|$, which as $\pist$ is supported on $\{\dirac_{(\epsilon,0)},\dirac_{(-\epsilon,0)}\}$, means, $|\srep_{2}(2)| \ge 2\nu\epsilon$. Thus, 
\begin{align} 
\Pr[ |\srep_{2}[2]| \ge 2\nu \epsilon] \ge \Omega(1)
\end{align}
On the other hand, $\sstar_2 \sim \Pst_h$ has $\sstar_2[2] = 0$ with probability one. Thus, for any coupling $\coup$ between $\Dist_{\pist},\Dist_{\pirep}$,
\begin{align}
\Pr_{\coup}[ \dists(\srep_{2},\sstar_2)| \ge 2\nu \epsilon] \ge \Omega(1)
\end{align}
Thus, 
\begin{align}
\gapmarg[\nu \epsilon](\pirep \parallel \pist) \ge \Omega(1).
\end{align}
\end{proof}


\subsection{$\pirep$ and $\pidec$ have different marginals, even with memoryless dynamics}\label{sec:lb_pirep_dech}
Here, we show how $\pirep$ and $\pidec$ have different marginals even if the dynamics are memoryless. By considering $\pihat = \pidec$ in \Cref{thm:smooth_cor}, the discussion below demonstrates why one needs to consider $\pihat \circ \Wsig$ in order to obtain small imitation gap.

For simplicity, we use a discrete smoothing kernel $\Wsig$, though the example extends to the Gaussian smoothing kernel in the previous counter example.  Again, let $\cS = \cA = \R$, and $F_h(\seqs,\seqa) = \seqa$. Take
\begin{align}
\pist(\seqs) = \begin{cases}\dirac_{-\sigma} & \seqs \le 0\\
\dirac_{\sigma} & \seqs > 0\\
\end{cases}
\end{align}
Let us consider an asymmetric initial state distribution
\begin{align}
\Dinit = \frac{1}{4}\dirac_{-\sigma} + \frac{3}{4}\dirac_{+\sigma}.
\end{align}
Note then that 
\begin{align}
\forall h, \quad \Psth = \Dinit = \frac{1}{4}\dirac_{-\sigma} + \frac{3}{4}\dirac_{\sigma}, \label{eq:Psth}
\end{align}
We consider a smoothing kernel, 
\begin{align}
\Wsig(\seqs) = \begin{cases}(\frac{1}{2}+\frac{\seqs}{4\sigma})\dirac_{\sigma} + (\frac{1}{2}-\frac{\seqs}{4\sigma})\dirac_{\sigma} & -2\sigma \le \seqs \le 2\sigma\\
\dirac_{\sigma} & \seqs \ge 2\sigma\\
\dirac_{-\sigma} & \seqs \le -2\sigma\\
\end{cases}
\end{align}
The salient part of our construction of $\Wsig$ is that 
\begin{align}
\Wsig(\sigma) =  \frac{1}{4}\dirac_{-\sigma} + \frac{3}{4}\dirac_{\sigma}, \ \Wsig(-\sigma) =  \frac{1}{4}\dirac_{\sigma} + \frac{3}{4}\dirac_{-\sigma}.
\end{align}

Denote the marginals of $\pirep$ and $\pidec$ with $\Preph$ and $\Pdech$. One can show via the lack of memory in the dynamics and the structure of $\pist$ that 
\begin{align}
\Preph[h+1] = \Qreph \circ \Preph, \quad \Qdech[h+1] = \Qdech \circ \Pdech, \label{eq:memoryless_policy}
\end{align}
By the replica property (\Cref{lem:replica_property}), $\Qreph \circ \Psth = \Psth$ for all $h$. Thus, for all $h$, \eqref{eq:Psth} and \eqref{eq:memoryless_policy} imply
\begin{align}
\Preph = \Psth =  \frac{1}{4}\dirac_{-\sigma} + \frac{3}{4}\dirac_{+\sigma}. \label{eq:Preph_Psth}
\end{align}
The following claim computes $\Pdech$.
\begin{claim}\label{claim:dec_claim} Consider any distribution of the form $\lawP = (1-p)\dirac_{\sigma} + p\dirac_{-\sigma}$. Then 
\begin{align}\Qdech \circ \lawP = (\frac{9}{10} - \frac{p}{5})\dirac_{\sigma} + (\frac{1}{10} + \frac{p}{5})\dirac_{-\sigma}.
\end{align}
Thus,
\begin{align}
\Pdech[h+1][-\sigma] &=  \frac{1}{10}\left(\sum_{i=0}^{h-1}5^{-i}\right) + \frac{1}{4} 5^{1-h}.
\end{align}
\end{claim}
Before proving the claim, let us remark on its implications. As $h \to \infty$, 
\begin{align}
\Pdech[h][-\sigma] &\to \frac{1}{10}\left(\frac{1}{1-1/5}\right) = \frac{1}{10}\cdot \frac{5}{4} = \frac{1}{8}.
\end{align}
Thus,
\begin{align}
\lim_{h\to \infty} \Pdech =  \frac{7}{8}\dirac_{\sigma} +  \frac{1}{8}\dirac_{-\sigma},
\end{align}
achieving a different stationary distribution that $\Psth = \Preph$. This shows that
\begin{align}
\lim_{H \to \infty}\gapmarg[\sigma](\pirep,\pidec) \ge \TV( \frac{7}{8}\dirac_{\sigma} +  \frac{1}{8}\dirac_{-\sigma}, \frac{3}{4}\dirac_{\sigma} +  \frac{1}{4}\dirac_{-\sigma}) = \frac{1}{8}, \quad 
\end{align}
which implies that the deconvolution policy $\pidec$ does approximate $\pirep$. From \eqref{eq:Preph_Psth}, it also follows that $\pirep$ and $\pist$ have identical marginals, so 
\begin{align}
\lim_{H \to \infty}\gapmarg[\sigma](\pist,\pidec) \ge \TV( \frac{7}{8}\dirac_{\sigma} +  \frac{1}{8}\dirac_{-\sigma}, \frac{3}{4}\dirac_{\sigma} +  \frac{1}{4}\dirac_{-\sigma}) = \frac{1}{8}
\end{align}
as well. In particular, if we take $\pihat = \pidec$ in \Cref{thm:smooth_cor}, we see that there is no hope to for bounding $\gapmarg(\pist,\pihat)$; we must bound $\gapmarg(\pist,\pihat\circ \Wsig)$ (again noting that if $\pihat = \pidec$, $\pihat\circ \Wsig = \pirep$).

\begin{proof}[Proof of \Cref{claim:dec_claim}] We have that for $\seqs' \in\{-\sigma,\sigma\}$,
\begin{align}
\Qdech[\seqs' \mid \seqs] = \frac{\Wsig(\seqs')[\seqs] \cdot \Psth(\seqs')}{\Wsig(\seqs')[\seqs] \cdot \Psth(\seqs') + \Wsig(-\seqs')[\seqs] \cdot \Psth(-\seqs')}
\end{align}
With $\seqs = \seqs' = \sigma$, the above is 
\begin{align}
\Qdech(\seqs' = \sigma \mid \seqs = \sigma) = \frac{\frac{3}{4}\cdot \frac{3}{4}}{\frac{3}{4}\cdot \frac{3}{4} + \frac{1}{4}\cdot \frac{1}{4}} = \frac{9}{10}.
\end{align}
And
\begin{align}
\Qdech(\seqs' = \sigma \mid \seqs = -\sigma) = \frac{\frac{1}{4}\cdot \frac{3}{4}}{\frac{1}{4}\cdot \frac{3}{4} + \frac{3}{4}\cdot \frac{1}{4}} = \frac{1}{2}.
\end{align}
Hence, for any $p \in [0,1]$,
\begin{align}
\Qdech(\seqs' = \sigma \mid \seqs = -\sigma)((1-p)\dirac_{\sigma} + p\dirac_{-\sigma}) &= ((1-p)\frac{9}{10} + \frac{p}{2})\dirac_{\sigma} + (1 - ((1-p)\frac{9}{10} + \frac{p}{2})))\dirac_{\sigma}\\
&= (\frac{9}{10} - \frac{p}{5})\dirac_{\sigma} + (\frac{1}{10} + \frac{p}{5})\dirac_{-\sigma}.
\end{align}
Consequently, by \eqref{eq:memoryless_policy}, we can unfold a recursion to compute
\begin{align}
\Pdech[h+1][-\sigma] &= \Qdech(\seqs' = \sigma \mid \seqs = -\sigma)\Pdech[h]\\
&= (\frac{1}{10} + \frac{\Pdech[h][\sigma]}{5})\\
&= \frac{1}{10}\sum_{i=0}^{h-1}5^{-i} + \Pdech[1][\sigma] \cdot 5^{1-h}\\
&= \frac{1}{10}\sum_{i=0}^{h-1}5^{-i} + \Psth[1][\sigma] \cdot 5^{1-h}\\
&= \frac{1}{10}\left(\sum_{i=0}^{h-1}5^{-i}\right) + \frac{1}{4} 5^{1-h}.
\end{align}
\end{proof}
