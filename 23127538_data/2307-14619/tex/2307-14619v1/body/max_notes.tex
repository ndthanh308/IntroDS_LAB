%!TEX root = ../main.tex
\section{Max Notes}


\subsection{}
We consider a finite-horizon decision process with actions $a_h \in \cA$, states $s_h \in \cS$, endowed with pseudo-metrics $\dista(\cdot,\cdot)$ and $\dists(\cdot,\cdot)$, respecitively. For simplicity, we consider deterministic transitions; extension to randomized transitions are deferred to \mscomment{...}We model the agent as selecting \emph{primitive controllers} $\sfk_h \in \cK$. We assume there exists a \emph{transition map} $F_h:\cS \times \cA \to \cS$ such that
\begin{align}
s_{h+1} = F_h(s_h,\sfk_h),  \label{eq:primitives}
\end{align} 
Here, $\sfk$ are formally equivalent to actions, but we think of them as richer controllers in our instantiation.


\paragraph{Policies.} We consider randomized policies $\pi$, which are defined as mappings $(\boldpol_h)_{h=1}^H$, where $\boldpol_h: \cS \to \laws(\cK)$ maps states to distributions on the space $\cK$ of primitive controllers. 
We assume a fixed distribution $\Dinit$ over initial states $s_1 \sim \Dinit$. We let $\Dist_{\pol}$ denote the law of the sequence $\traj = (s_{1:H},\sfk_{1:H})$, where $s_{1} \sim \Dinit$, each $\sfk_h$ is distributed according to $\boldpol_h(s_h)$, for each $1 \leq h \leq H$.  We let $\traj_{\le h} := (s_{1:h},\sfk_{1:h})$. Throughout, we consider a fixed policy $\pist$ to be imitated. We let $\Psth$ denote the marginal distribution of $s_h$ under $\Dist_{\pist}$.


\ab{We are not being consistent with bold/not bold policies.  Do we need the bold policy notation for something?}
\mscomment{my thinking is that any object which is a probability distribution/ probability kernel is bold. }

\paragraph{Multi-step control}\mscomment{Return to this much later}
 Our instantiation of the general framework focuses on smooth, discretize-time control, where primitive controllers correspond to sequence of inputs stabilized by linear gains. Specifically, let $\cX = \R^d$ and $\cU = \R^m$ denote continuous control-state and control-input spaces. We consider the process with abstract-state $\cS =\{ x_{h,1:\ell+1}\} \in \cX^{\ell+1}$, $\cA = \{u_{h,1:\ell}\} \in \cU^{\ell}$. Given a control-transition map $f:\cX \times \cU \to \cX$, the dynamic map $F$ is time-invariant, and determined by the dynamics
\begin{align}
x_{h+1,1} = x_{h,\ell+1}, \quad x_{h,i+1} = f(x_{h,i},u_{h,i}), ~ i \in [\ell].
\end{align}
We consider the space of primitives controllers $\sfk = (\bar{u}_{1:\ell},\bar{K}_{1:\ell},\bar{x}_{1:\ell}) \in  \cK :=  (\cU \times\R^{d \times m} \times \cX)^{\ell}$ consisting of sequences of control-inputs $\bar{u}$, gains $\bar{K}$, and control-states $x$. We let the action evaluation maps $\eval:\cS \times \cK$ be the sequence of maps returning $\tilde{u}_{1:\ell}$ and $\tilde{x}_{1+\ell}$ such that
\begin{align}
\tilde u_{i} = \bar{u}_{i} + \bar{K}_1(\tilde x_{h+1,i} - \bar{x}_i), \quad \tilde x_{1} = x_{h,\ell+1}, \quad \tilde{x}_{i+1} = f(\tilde x_{i}, \tilde u_{i}).
\end{align}
\ab{Add in explanation/motivation here}

A primitive controller $\sfk$ is called \emph{simple} if the gains vanish, i.e., $\bar{K}_{1:\ell} = \zero$, and is called \emph{proper} if $\bar{x}_{1:\ell} = \evalS(s_h,\sfk)$ for any state $s_h = (x_{h,1:\ell+1})$ with $x_{h,\ell+1} = \bar{x}_1$. \ab{Remark something about how we are gluing together these multi-step trajectories and that is why proper is important.  Maybe motivate this formalism here?}  The first observation is that, given access to trajectory demonstrations, one can always synthesize proper-controllers from simple ones (by using observed trajectories) \ab{TODO: Spell out this correspondence}. Moreover, with access to dynamics, we can also synthesize stabilizing \ab{finish this?}



%We let $\pol_h(s)$ denote a random variable with distribution $\sfk \sim \boldpol_h(s) \in \laws(\cK)$. 

%take in an abstract random \emph{seed} $\seed \in \zspace$, and select primitive controllers $\sfk \in \cK$.
%A (randomized, non-stationary) \emph{ policy} $\pi$ consists of mappings $(\pol_h)_{h =1}^H$, where $(s,\zeta) \mapsto \pi_h(s \mid \zeta) \in \cK $ evalues the policy at state $s$, on seed $\zeta$, at time step $h$. Throughout, we fix a random-generation distribution $\Drand \in \laws(\zspace)$. We let $\boldpol_h(\cdot): s \mapsto  \Law(\pi_h(s \mid \zeta) : \zeta \sim \Drand) \in \laws(\cA)$ maps $s$ to the distribution over actions induced by drawing the seed from $\Drand$. 



\subsection{Conditions}




\mscomment{explain TVC here?}

\newcommand{\gameval}{\gamma_{\eval}}
%\begin{definition} We say $\polst$ is $\gameval$-evaluation continuous ($\gameval$-EC) \ab{Finish.}
%\end{definition}

\newcommand{\xist}{\xi^\star}
\newcommand{\xihat}{\hat\xi}

\newcommand{\gaprob}{\Gamma_{\epsilon,\eta}}

\subsection{Main Guarantee}
We consider the difference in trajectories between a target policy $\pist$ and a learned one $\pihat$. Our notion of closeness is as follows: 
\newcommand{\Gamrob}{\Gamma_{\epsilon,\eta;\epsilon_0}}



\begin{comment}For an initial condition $\xi$ and sequence of random seeds $\seed_{1:H}$, we consider tuples of the form $\tuple = (\xi,\seed_{1:H})$, and define
\begin{align}
s_{h \mid h}^\sfk(\tuple) = \xi, \quad s_{j+1 \mid h}^\sfk(\tuple) = F_j(s_{j \mid h}^\sfk(\tuple), \sfk_j(s_{j \mid h}^\sfk(\tuple) \mid s_{j \mid h}^\sfk(\tuple),\seed_j)) 
\end{align}
We also consider a perturbed sequence as follows. As shorthand, let $\deltuple = (\xi',\Delta_{1:H}, \xi,\seed_{1:H})$. We have
\begin{align}
s_{h \mid h}^{\updelta\sfk}(\deltuple) = \xi', \quad s_{j+1 \mid h}^{\updelta\sfk}(\deltuple) = F_j(s_{j \mid h}^{\updelta\sfk}(\deltuple), \sfk_j(s_{j \mid h}^{\updelta\sfk}(\deltuple)\mid s_{j \mid h}^{\sfk}(\tuple),\seed_j)+\Delta_j).
\end{align}
In words, the perturbed sequence considers actions perturbed by $\Delta_h$, and considers varying the feedback states
\end{comment}


\begin{comment}
\paragraph{Total Variation Continuity} \ab{This is redundant given earlier remarks, no?} The next condition we require is that the imitator $\sfkhat$ is continous with respect to the total variation. Formally,


Note that this property depends only on the controller, and neither on the underlying system dynamics nor the metric structure of $\cA$. Total-variation continuity may seem rather stringent , but it is quite easy to enforce. For example, if $\dists$ is the Euclidean metric, any controller arising from a Gaussian convolution with its second argument is total-variation continuous with $\gamtvc(\epsilon) \propto \epsilon$. 
\newcommand{\Dtilrand}{\tilde{\cD}_{\mathrm{sd}}}
\newcommand{\sfktil}{\tilde{\sfk}}
\begin{example}[Gaussian smoothing] Let $\sfk$ be a an deterministic affine feedback controller. SUppose $\cS = \R^d$, and define for $s,s' \in \R^{d}$ and $w \in \R^d$
\begin{align}
\tilde{\sfk}_h(s',s  \mid w,\seed) =  \sfk_h(s',s + w  \mid \seed) 
\end{align}
When we draw $w \sim \cD_{w}$, we can view $\sfktil$ as a randomized controller with  augmented random seed $\tilde \seed := (w,\seed) \sim \Dtilrand$ as the random seed, where $\seed\sim \Drand \perp w \sim \cD_{w}$. We can then see that 
\begin{align} 
\max_{h \in [H]}\TV(\Dtilrand(\tilde\sfk_h(s',s) \parallel\Dtilrand(\tilde\sfk_h(s',\sbar)))  \le \TV(s+\cD_w \parallel \sbar + \cD_w) = \TV((s-\sbar) + \cD_w \parallel \cD_w).
\end{align}
By Pinsker's inequality (\mscomment{cite}), we have  $\TV((s-\sbar) + \cD_w \parallel \cD_w) \le \sqrt{\frac{1}{2}\KL((s-\sbar) + \cD_w \parallel \cD_w)}$. Thus, when we choose $\cD_w = \cN(0,\sigma^2 \bI_d)$ and $\dists(s,s') = \|s-s'\|$ to be the Euclidean distance, we chave $\TV((s+s') + \cD_w \parallel \cD_w) \le \frac{1}{2\sigma}\dists(s,s')$. Hence, $\sfktil$ is ($\gamtvc$)-TVC for $\gamtvc(\epsilon) = \frac{\epsilon}{2\sigma}$.

\end{example}
Note that the above does not depend at all on the form of the action space.









%We can smooth $\pi_h^\star$ inputs 


% \begin{enumerate}
%     \item 
% \end{enumerate}
\end{comment}