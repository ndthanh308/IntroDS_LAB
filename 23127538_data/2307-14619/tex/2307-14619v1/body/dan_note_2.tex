\section{Dan Notes 2}
In order to evaluate the learned controller at a particular $\hat{s}$, we do:

\begin{enumerate}
    \item Sample a primitive controller $\hat{\kappa}_s$ from the distribution $\hat{\pi}(\hat{s})$, i.e. $\hat{\kappa}_s \sim \hat{\pi}(\hat{s})$
    \item Evaluate the primitive controller $\hat{a}_s = \alpha(\hat{s}, \hat{\kappa}_s)$
\end{enumerate}
Now, in theory $\alpha(\hat{s}, \hat{\kappa}_s)$ incrementally stabilizes to a target point implicitly selected by sampling $\hat{\kappa}_s$, but in practice we only ever evaluate $\hat{\kappa}_s$ at $\hat{s}$, the same point used to sample $\hat{\kappa}_s$.

Can we simply posit that this internal decomposition of $\hat{\pi}$ exists and simply learn $\hat{\pi}$ as a distribution $\hat{a}_s \sim \hat{\pi}(\hat{s})$? 

We need the decomposition of the expert into this primitive-controller sampling form for smoothing purposes (and the expert primitive controllers need to be incrementally stable), but once that is done, the manner in which $\hat{\pi}$ is actually evaluated strongly suggests that we can throw away the primitive-controller representation. Or at least it is unclear when we would evaluate $\alpha(s, \hat{k}_s)$ \emph{not} at $\hat{s}_k$.