
 \begin{definition}[Stabilizing $\dots$]
 \end{definition} 
\mscomment{More generally, controllers could be sequences of very simple policies, and the evaluation map extends (see Appendix)}
\subsection{}
\begin{definition}\label{defn:fis}We say that $\sfk$ is $(\betais,\gamis)$-\emph{feedback incrementally input-state stable} at $\xi$ ($(\betais,\gamis)$-FIS at $\xi$) if for every $\xi' \in \cS$, $a_{1:H}' \in \cA^H$, and $\seed_{1:H}\in \zspace^H$, the dynamics $(s_{1:H},s_{1:H}')$ given by $s_1 = \xi,s_1' = \xi'$ and  
\begin{align}
s_{h+1} = F_h(s_h,a_h), \quad s_{h+1}' = F_h(s_h',a_h'), \quad a_h = \sfk_h(s_h,s_h\mid \seed_{h})
\end{align}
satisy, for all $h \in [H]$, and for $\tilde{a}_h = \sfk_h(s_h', s_h \mid \seed_h)$
\begin{align}
\dists(s_h,s_h') \le \betais( \dists(\xi,\xi')) + \gamis(\max_{j \in [h-1]}\dista(\tilde{a}_j, a_j' )).
\end{align}
%We say $\sfk$ is $(\betais,\gamis,\etaiss,\etaisa)$-locally FIS if the above only holds for all $\dists(\xi,\xi') \le \etais$ and $\dista(a_h,a_h') \le $
\end{definition}  
We say a function $\phi: \R_{\ge 0} \to \R \cup \{\infty\}$ is an \emph{valid bounding function} if $\phi(0) = 0$, $\phi(t)$ is non-decreasing. When applying the definition, we will always assume that the functions $\epsilon \mapsto \betais(\epsilon)$ and $\epsilon \mapsto \gamis(\epsilon)$ are valid bounding functions. By letting $\betais,\gamis$ take the values of $+\infty$, we may encode constraints requiring, for example, that $\dista(\tilde a_h,a'_h) \le \eta$ for all $h$. Lastly, we observe that when $\sfk_h(s',s \mid \seed)$ does not depend on $s,\seed$, we can write $\sfk_h(s',s \mid \seed) = \pi_h(s')$ for a deterministic policy $\pi$, so we recover the standard definition of incremental input-to-state stability.


%Let $\scrP(\Omega)$ denote the space of Borel measures over a topological space $\Omega$. 

We consider a finite-horizon decision process on actions $a_h \in \cA$, states $s_h \in \cS$, with possibly nonstationary, deterministic transition dynamics
\begin{align}
s_{h+1} = F_h(s_h,a_h), \quad F_h:\cS \times \cA \to \cS.
\end{align}
We assume that $\cS$ and $\cA$ are pseudometric spaces with pseudometrics $\dists$ and $\dista$, respectively. We use $\distfont{P} \in \laws(\cS)$ to denote distributions over states.


\paragraph{Unilevel and Bilevel Policies.} We consider randomized policies, which take in an abstract random \emph{seed} $\seed \in \zspace$, and we fix a random-generation distribution $\Drand \in \laws(\zspace)$. A (randomized, non-stationary) \emph{uni-level policy} $\pi$ considers of mappings $(\pol_h)_{h =1}^H$, where $(s,\zeta) \mapsto \pi_h(s \mid \zeta) \in \cA $ evalues the policy at state $s$, on seed $\zeta$, at time step $h$. We let $\boldpol_h(\cdot): s \mapsto  \Law(\pi_h(s \mid \zeta) : \zeta \sim \Drand) \in \laws(\cA)$ maps $s$ to the distribution over actions induced by drawing the seed from $\Drand$. Given $\xi \in \cS$, let $\Dist_{\pol}(\xi)$ denote the law of the sequence $\traj = (s_{1:H},a_{1:H},\zeta_{1:H})$, where $s_{1} = \xi$, $\zeta_{1:H}$ and i.i.d. drawn from $\Drand$, and $s_{h+1} = F_h(s_h,a_h)$. Lastly, given $\Dinit \in \laws(\cS)$,  $\Dist_{\pi}\circ\Dinit$ denotes the distribution $\Dist_{\pi}(\cdot)$ marginalized over $\xi \sim \Dinit$.

A randomized \emph{bi-level policy} $\sfk$ is specified by maps $\sfk = (\sfk_h)_{h=1}^H$,  where  $\sfk_h(s', s \mid \zeta ) \mapsto \cA$. Again, we let $\bsfk_h(s',s) = \Law(\sfk_h(s',s \mid \zeta): \zeta \sim \Drand)$. In our application of bilevel policies, we think of $s'$ as the \emph{low-level state}, and $s$ as the \emph{high-level state}. We will show that by \emph{augmenting} imitiation unilevel policys $\pi$ into bilevel policies $\sfk$, we can ensure effective stabilization. Accordingly, let $\pol^\sfk$ denote the policy with $\pol^\sfk_h(s \mid \zeta) = \sfk_h(s,s \mid \zeta)$, $\Dist_{\sfk}(\xi) := \Dist_{\pol^\sfk}(\xi)$, and define $\Dist_{\sfk} \circ \Dinit$ accordingly. 
  \mscomment{marginalization}



\paragraph{Motivating Example} Our motivating example is imitating in a smooth-control setting, with a control-state space $\cX = \R^d$ and control-input space $\cU = \R^{m}$, and dynamic map $f:\cX \times \cU \to \cX$. We use \emph{gains} to refer to linear maps $K \in \R^{m \times d}$.
To apply our abstraction, we introduce an \emph{abstract-state} space $\cS = (\cX)^{\ell+1}$ of all $\ell$-length sequences in $\cX \subset \R^d$; we denote these states with $s_h = x_{h,1:\ell+1}$. An \emph{abstract-action} $a_h$ is a tuple of $\ell$-length  control-inputs $u_{h,1:\ell}$.

 gains, and target-control states
\begin{align}
a_h = ( \bar u_{h+1,1:\ell}, \bar{K}_{h+1,1:\ell}, \bar{x}_{h+1,1:\ell}) \in (\R^m \times \R^{m \times d} \times \R^d)^{\ell}
\end{align} 
The \emph{abstract-transition map} is time-invariant, and given by
\begin{align}
&F(s_h,a_h)= x_{h+1,1:\ell+1}, \quad \text{where } x_{h+1,1} = x_{h,\ell+1} \text{ and }, \quad \forall i \ge 1, \\
& u_{h+1,i} = \bar u_{h+1,i} + \bar K_{h+1,i} (x_{h+1,i} - \bar x_{h+1,i}), \quad x_{h+1,i+1} = f(x_{h+1,i},u_{h+1,i}).
\end{align}
\mscomment{explain traj overlap, foreshadow why would do this}. In this context, we will use \emph{policies} to describe mappings

\paragraph{Example 1: Perfect Position Control} $\cA = \cS$, $F_h(s_h,a_h) = a_h$. In this case, we will find that there is no advantage to the bilevel phenomenon. 

\paragraph{Example 2: Fully Actuated Dynamics} Consider a control system where $s_h \in \R^d$, and where 

 The purpose of using bivariate controllers is to allow for policies which are \emph{discontinuous}, but where each individual discontinuous trajectory can be extended to a stabilizing policy. Specificially, supposet that states $s_h \in \R^d$, and let $\dists(s_h',s_h) = \|s_h - s_h\|$. For simplicitly, let us assume that our controller is deterministic . Consider a function $\phi(s)$ which assigns to each state $s$ and step $h$ an inputs $\bbaru_{1:\ell} \in (\R^{m})^\ell$, and gains $\bbarK_{1:\ell} \in (\R^{m \times d})^\ell$
\begin{align}
\sfk_h^\phi(s',s) := \bbaru_h^\phi(s) + \bbarK^\phi(s)(s'-s), \quad  \phi_h(s) = (\bbaru_h^\phi(s),\bbarK_h^\phi(s)).  \label{eq:aff_cont_nozeta}
\end{align}
Even if $\phi(s)$ is discontinuous and highly nonlinear, $\sfk^\phi(s',s) $ is \emph{linear} in $s'$. This naturaly generalizes to $h$-dependent and randomized $\phi$:
\begin{align}
\sfk_h^\phi(s',s \mid \seed) := \bbaru^\phi(s \mid \seed) + \bbarK(s \mid \seed)(s'-s), \quad  \phi(s \mid \seed) = (\bbaru^\phi(s \mid \seed),\bbarK^\phi(s \mid \seed)). \label{eq:aff_cont_zeta}
\end{align}

\begin{definition}We say $\sfk$ is a \emph{affine-feedback} controller if $\sfk$ can be represented in the form \eqref{eq:aff_cont_zeta}. We say it is a derministic affine-feedback controller if it is of the form \eqref{eq:aff_cont_nozeta}.
\end{definition}
The policy induced if proper affine-feedback $\sfk$  $\pi_h(s \mid \seed) = \sfk(s,s \mid \seed)$ is determined \emph{uniquely} by $\bbaru_h(s \mid \seed)^\phi$. Hence, we can view the gains $\bbarK^\phi(s \mid \seed)^\phi$ are abstractions used to reason about off-policy behavior. 


\subsection{Regularity Conditions}
\paragraph{Feedback Incremental Stability.} We study the difference between a target controller $\sfkst$ and an imitation $\sfkhat$. We require two regularity conditions: first, that $\sfkst$ is stabilizing in its first argument, and second, that $\sfkhat$ is continuous in its second argument with respect to the total variation distance.  The generalization of incremental input-to-state stability for this controllers is given as follows.
\begin{definition}\label{defn:fis}We say that $\sfk$ is $(\betais,\gamis)$-\emph{feedback incrementally input-state stable} at $\xi$ ($(\betais,\gamis)$-FIS at $\xi$) if for every $\xi' \in \cS$, $a_{1:H}' \in \cA^H$, and $\seed_{1:H}\in \zspace^H$, the dynamics $(s_{1:H},s_{1:H}')$ given by $s_1 = \xi,s_1' = \xi'$ and  
\begin{align}
s_{h+1} = F_h(s_h,a_h), \quad s_{h+1}' = F_h(s_h',a_h'), \quad a_h = \sfk_h(s_h,s_h\mid \seed_{h})
\end{align}
satisy, for all $h \in [H]$, and for $\tilde{a}_h = \sfk_h(s_h', s_h \mid \seed_h)$
\begin{align}
\dists(s_h,s_h') \le \betais( \dists(\xi,\xi')) + \gamis(\max_{j \in [h-1]}\dista(\tilde{a}_j, a_j' )).
\end{align}
%We say $\sfk$ is $(\betais,\gamis,\etaiss,\etaisa)$-locally FIS if the above only holds for all $\dists(\xi,\xi') \le \etais$ and $\dista(a_h,a_h') \le $
\end{definition}  
We say a function $\phi: \R_{\ge 0} \to \R \cup \{\infty\}$ is an \emph{valid bounding function} if $\phi(0) = 0$, $\phi(t)$ is non-decreasing. When applying the definition, we will always assume that the functions $\epsilon \mapsto \betais(\epsilon)$ and $\epsilon \mapsto \gamis(\epsilon)$ are valid bounding functions. By letting $\betais,\gamis$ take the values of $+\infty$, we may encode constraints requiring, for example, that $\dista(\tilde a_h,a'_h) \le \eta$ for all $h$. Lastly, we observe that when $\sfk_h(s',s \mid \seed)$ does not depend on $s,\seed$, we can write $\sfk_h(s',s \mid \seed) = \pi_h(s')$ for a deterministic policy $\pi$, so we recover the standard definition of incremental input-to-state stability.

\begin{example}\label{exmp:tis_dafffeed} Suppose that $\sfk$ is an \emph{deterministic} affine feedback controller . Then, for each $\xi$, there is a unique trajectory $s_{1:H},a_{1:H}$ and gains $\bbarK_{1:H}$ such that
\begin{align}
a_j = \sfk_h(s_j,s_j), \quad s_{j+1} = F_h(s_j,a_j), \quad \sfk_j(s',s_j) = a_j + \bbarK_{j}(s' - s_j), \quad s_1 = \xi
\end{align}
Consequently, if $\bbarK_{1:H}$ stabilize the Jacobian linearizations around $(s_{1:H},a_{1:H})$, then one can show that $\sfk$ is $(\betais,\gamis)$-FIS at $\xi$, where $\betais$ and $\gamis$ are functions of the form
\begin{align}
\betais(\epsilon) = \epsilon (c_\beta  \indinf\{\epsilon \le M_\beta\}), \quad \gamis(\eta) = \eta (c_\gamma  \indinf\{\epsilon \le M_{\gamma}\}),
\end{align}
and where $\indinf\{\mathrm{clause}\}$ is $1$ if $\mathsf{clause}$ is true, and $\infty$ otherwise. In words, in some bounded region, the functions determining incremental stability have at most linear growth.
\end{example}
\begin{example} Suppose $\sfk$ is a non-deterministic affine feedback controller. Then, it is no longer possible to ensure that the gains generated by $\sfk$ stabilize all trajectories. We can handle this, but we require considering controllers which apply \emph{sequences}. \mscomment{This is a todo, i will explain later}. 
\end{example}