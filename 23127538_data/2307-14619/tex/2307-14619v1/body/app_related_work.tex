%!TEX root = ../main.tex

\section{Complete Related Work}\label{app:related}

 \iftoggle{arxiv}
 {}
 {
\input{body/full_related}
}
\subsection{Comparison to prior notions of Stability.}

Prior work in guarantees for imitation learning focuses either on constraining the learned policy to be stable \cite{havens2021imitation,tu2022sample} or assume the expert policy is suitably stable \cite{pfrommer2022tasil}. 

The principal notion of stability used in these prior works is \emph{incremental-input-to-state} stability of the closed-loop system under a deterministic controller $\pi$: 

\begin{definition}[Incremental Input-to-State Stability]
    There exists class $\mathcal{K}$ function $\gamma$ and class $\mathcal{KL}$ function $\beta$ such that for any two initial conditions $\xi_1, \xi_2 \in \mathcal{X}$, the closed-loop dynamics under policy $\pi: \cal{X} \to \cal{U}$ given by $f_{\textrm{cl}}(x_t, \Delta_t) = f(x_t, \pi(x_t) + \Delta_t)$ satisfies:
    $$\|x_t(\xi_1; \{\Delta_s\}_{s=0}^t) - x_t(\xi_2; \{0\}_{s=0}^t)\| \leq \beta(\|\xi_1 - \xi_2\|) + \gamma\left(\max_{0 \leq s \leq t-1} \|\Delta_s\|\right),$$
    where $x_t(\xi; \{\Delta_s\}_{s=0}^{t-1})$ is the state at time $t$ under $f_{\textrm{cl}}$ with $x_0 = \xi$ and input perturbations $\{\Delta_s\}_{s=0}^{t-1}.$
\end{definition}
This notion of stability is quite restrictive, as the $\beta$-term necessitates that the dynamics converge irrespective of initial condition. Without time-varying dynamics this can only be achieved by a policy which stabilizes to an equilibrium point, as a policy which tracks a reference trajectory is unable to ``forget" the initial condition. Constraining learned policies such that they satisfy this notion of stability is also challenging. Tu et. al. \cite{tu2022sample} attempt to do so through regularization while Haven et. a. \cite{havens2021imitation} use matrix inequalities to satisfy this stability property under linear dynamics. Pfrommer et. at. \cite{pfrommer2022tasil} avoid this difficulty by relaxing the incremental stability to a local variant of stability:
\begin{definition}[$\eta$-Local Incremental Input-to-State Stability]\label{defn:related_local_stability}
    There exists class $\mathcal{K}$ function $\gamma$ such that for any $\xi \in \mathcal{X}$, the closed-loop dynamics under policy $\pi: \cal{X} \to \cal{U}$ given by $f_{\textrm{cl}}(x_t, \Delta_t) = f(x_t, \pi(x_t) + \Delta_t)$ satisfies:
    $$\|x_t(\xi; \{\Delta_s\}_{s=0}^t) - x_t(\xi; \{0\}_{s=0}^t)\| \leq \gamma\left(\max_{0 \leq s \leq t-1} \|\Delta_s\|\right),$$
    for all $\{\Delta_s\}_{s=0}^t$ where $\max_{0 \leq s \leq t} \|\Delta_s\| \leq \eta$.
\end{definition}

This weaker notion of incremental stability simply postulates the existence of a (local) input-perturbation to state-perturbation gain function $\gamma$. Since this stability property does not necessitate convergence across with different initial conditions and only under input perturbations of magnitude $\leq \eta$, this only necessitates that the expert policy can correct from small input perturbations.

% Figure environment removed

We further weaken this assumption, which we formalize in \Cref{asm:Jacobian_Stable} and abstract to the composite MDP through \Cref{defn:stability_setup}, by only requiring that a locally stabilizing controller can be synthesized per-demonstration. Through the introduction of a synthesis oracle which can generate locally stabilizing primitive controllers, we decouple the stability properties of the expert from the stabilizability of the underlying dynamical system. This allows for reasoning about generalization in the presence of bifurcations or conflicting demonstrations, which is precluded by \Cref{defn:related_local_stability} since an expert policy cannot simultaneously stabilize to multiple branches of a bifurcation.  For a concrete example, consider \Cref{fig:bifurcation}.  Indeed, continuity is the \emph{sine qua non} of stability and the example given demonstrates the necessity of augmentation to enforce the former.  In detail, the figure illustrates an example where an agent is navigating around an obstacle, providing a bifurcation.  Without augmentation, the demonstrator trajectories always navigate around the obstacle in the direction closer to their starting point, leading to a sharp discontinuity along a bisector of the obstacle.  On the other hand, the data augmentations allow for the policy to have some probability of navigating around the obstacle in the ``wrong'' direction, which leads to the notion of continuity we consider: total variation continuity.


% Figure environment removed
Because our notion of stability is applied in chunks, our theory is sufficiently flexible so as to allow for the learned policy to switch between expert demonstrations in a manner preserving the marginal distributions but not consistent with the joint distribution across the entire trajectory.  This flexibility is illustrated in \Cref{fig:figure_eight}, where we suppose that the demonstrator distribution consists both of trajectories traversing a figure ``8'' consistently in either a clockwise or counter-clockwise manner, with both orientations represented in the data set.  Due to the multi-modality at the critical point in the trajectory, there is ambiguity about which loop to traverse next; specifically, there may exist a policy that randomly select which loop to traverse each time the critical point is visited in such a way that the marginal distributions on states and actions is the same as that induced by the demonstrator.  Such a policy will, by definition, preserve the correct \emph{marginal} distributions across states and actions; at the same time, this policy has a different \emph{joint} distribution across all time steps from the demonstrator due to the possibility of traversing the same loop twice in a row.