%!TEX root = ../neurips_main.tex



\section{Measure-Theoretic Background}\label{app:prob_theory}
In this section, we introduce the prerequisite notions from probability theory that we use to formally construct the couplings  in \Cref{app:no_augmentation,sec:imit_composite}.  We begin by introducing general preliminaries, followed by kernels, regular conditional probabilities and a ``gluing'' lemma in \Cref{sec:regularconditionalprobabilities}. We then show that optimal transport costs commute in an appropriate sense with conditional probabilities (\Cref{prop:MK_RCP} in \Cref{sec:opt_trans_kernel}). We use the preliminaries in the previous sections to derive certain optimal-transport and data processing inequalities in  \Cref{sec:dpis}. We prove \Cref{prop:MK_RCP} in \Cref{sec:prop:MK_RCP}. Finally, we state a simple union bound lemma (\Cref{lem:peeling_lem} in \Cref{sec:simple_union_bound}) of use in later appendices.



\paragraph{General preliminaries.} We rely extensively on the exposition in \citet{durrett2019probability} and refer the reader there for a more thorough introduction. Throughout, we assume there is a Polish space $\Omega$ such that all random variables of interest are mappings $X:\Omega \to \cX$, where $\cX$ is also Polish. Here, the $\upsigma$-algebras are always the Borel algebras (the $\upsigma$-algebra generated by open subsets), denoted $\Borel(\Omega)$ and $\Borel(\cX)$.

The space of (Borel) probablity distributions on $\cX$ is denoted $\laws(\cX)$, and measurability is meant in the Borel sense. Given a measure $\mu$ on a space $\cX \times \cY$, we say that $X \sim \lawP_X$ under $\mu$ if, for all $A \in \Borel(\cX)$, $\mu(X \in A) = \lawP_X(A)$.

We adopt standard information theoretic notation to denote joint, marginal, and conditional distributions on vectors of random variables.  In particular, if random variables $X,Y$ are distributed according to $\lawP$, we denote by $\lawP_X$ as the marginal over $X$, $\lawP_{X|Y}$ as the conditional of $X | Y$ under $\lawP$, and $\lawP_{X,Y}$ as the joint distribution when this needs to be empasized. 




\begin{definition}[Couplings] Let $\cX,\cY$ be Polish spaces and let $\lawP_X \in \laws(\cX)$ and $\lawP_Y \in \laws(\cY)$. The set of couplings $\couple(\lawP_X,\lawP_Y)$ denotes the set of measure $\mu \in \laws(\cX \times \cY)$ such that, $(X,Y)\sim \mu$ has marginals $X \sim \lawP_X$ and $Y \sim \lawP_Y$.\footnote{More pedantically, for all Borel sets $A_1 \in \scrB(\cX)$, $\mu(A_1 \times \cY) = \lawP_X(A_1)$ all Borel sets $A_2 \in \scrB(\cX)$, $\mu(\cX \times A_2) = \lawP_2(A_2)$. } We let $\lawP_X \otimes \lawP_Y \in \couple(\lawP_X,\lawP_Y)$ denote the \emph{indepent coupling}  under which $X$ and $Y$ are independent. 
\end{definition}
It is standard that $\lawP_X \otimes \lawP_Y$ is always a valid coupling, and hence $\couple(\lawP_X,\lawP_Y)$ is nonempty. 
Couplings have the advantage that they can be used to design many probability-theoretic distances. Through the paper, we use the total variation distance.
\begin{definition}[Total Variation Distance]\label{defn:TV} Let $\lawP_1,\lawP_2 \in \laws(\cX)$. We define the total variation distance $\tvof{\lawP_1}{\lawP_2} := \sup_{A \subset \Borel(\cX)} |\lawP_1(A) - \lawP_2(A)|$
\end{definition}
The total variation distance can be expressed in terms of couplings as follows \citep{polyanskiy2022}.
\begin{lemma}\label{lem:tv_coupling_equiv} Let $\lawP_1,\lawP_2 \in \laws(\cX)$. Then, 
\begin{align}
\tvof{\lawP_1}{\lawP_2} = \inf_{\coup \in \couple(\lawP_1,\lawP_2)}\Pr_{(X_1,X_2)\sim \coup}\{X_1 \ne X_2\}.
\end{align}
Moreover, there exists a coupling $\coup_{\star}$ attaining the infinum.
\end{lemma}

\paragraph{Support and absolute continuity.} We will also require the definition of the support of a measure.

\begin{definition}\label{def:support}
    Given a measure $\mu$ on a Borel space $(\Omega, \cF)$, we define the \emph{support} $\supp(\mu)$ to be the closure in the topology given by the metric of the set $\left\{ \omega \in \Omega| \mu(\cU) > 0 \text{ for all open } U \ni \omega \right\}$.
\end{definition}
In addition, we require the definition of absolute continuinty.
\begin{definition}[Absolute Continuity]\label{defn:abs_cont} We say that $\lawP \in \laws(\cX)$ is absolutely continuous with respect to law $\lawP' \in \laws(\cX)$, written $\lawP \llac \lawP'$, if for $A \in \Borel(\cX)$, $\lawP'(A) = 0$ implies $\lawP(A) = 0$.
\end{definition}

We now go into greater detail on the kinds of couplings that we consider.

\subsection{Kernels, Regular Conditional Probabilities and Gluing}
\label{sec:regularconditionalprobabilities} One key technical challenge in proving results in the sequel is the fact that we need to ``glue'' together multiple different couplings.  Specifically, while it may be the case that there exist pairwise couplings which satisfy desired properties, there exists a coupling such that the probability of the relevant event is small, it is not obvious that there exists a \emph{single} coupling such that all of these probabilities are small \emph{simultaneously}.  There are two natural ways to due this gluing: the first, using regular conditional probabilities we provide here.  The second, involving a sophisticated construction of \citet{angel2019pairwise} requires stronger assumptions on the pseudo-metric, but generalizing beyond Polish spaces, we simply remark can be substituted with a loss of a constant factor.

% \begin{remark}\label{rmk:mastertvcoupling} \mscomment{revisit}
%     We remark that if we assume that $\phiis$ is a metric, in that it satisfies the triangle inequality, then a simpler argument would work.  Indeed, the main result of \citet{angel2019pairwise}, tells us that there exists a coupling between an arbitrary class of random variables such that for any two of these random variables, the probability that they are not equal is at most twice the total variation distance between their laws.  Thus, losing at most a factor of 2 and applying a discretization argument to prove an analogue of \eqref{eq:coupfs}, it suffices to only prove individual total variation bounds.  While this method benefits from not having to deal with regular conditional probabilities, it suffers both in the constants appearing in the bound and the resulting loss of generality that follows from requiring $\phiis$ to be a metric.
% \end{remark}

\paragraph{Kernels.} We begin by introducing the notion of a kernel.
\begin{definition}[Kernels]\label{def:regularconditionalprobabilities}
    Let $(\Omega, \pp)$ be a probability space and let $X$ denote a random variable on this space.  For a given $\upsigma$-algebra $\cG$, and map $Q: \Omega \times \cG \to [0,1]$, we say that $Q$ is a probability kernel if the following two conditions are satisfied:
    \begin{enumerate}
        \item For all measurable events $A$, the map $\omega \mapsto Q(\omega, A)$ is measurable.%almost surely equal to $\pp(X \in A \mid \cG)$.
        \item For almost every $\omega \in \Omega$, the map $A \mapsto Q(\omega, A)$ is a probability measure.
    \end{enumerate}
    %We say that $(\Omega, \pp)$ is a regular conditional probability measure if there exists a probability kernel $Q$ such that $\pp(A \mid \sigma(X))$ is almost surely equal to $Q(X, A)$.
\end{definition}


We can combine a probability kernel with a probabilty measure on $\cY$ to yield joint distributions over $\cX \times \cY$.
\begin{definition}Given an  $\lawP_Y \in \laws(\cY)$, we define the  probability measure $\lawof{\lawQ_{X \mid Y}}{\lawP_Y} \in \laws(\cX \times \cY)$ such that $\mu = \lawof{\lawQ_{X \mid Y}}{\lawP_Y}$ satisfies\footnote{Recall that $\Borel(\cX \times \cY)$ is generated by sets $A \times B \in \Borel(\cX) \times \Borel(\cY)$, so \eqref{eq:muAB_def} defines a unique probability measure} 
\begin{align}
\mu(A \times B) = \Exp_{Y \sim \lawP_Y}\left[\lawQ_{X\mid Y}(A \mid Y)\I\{Y \in B\})\right], \quad \forall A \in \Borel(\cX), B \in \Borel(\cY).\label{eq:muAB_def}
\end{align}
We let $\lawQ_{X \mid Y} \circ \lawP_Y \in \laws(\cX)$ denote the measure for which $\mu = \lawQ_{X \mid Y} \circ \lawP_Y$ satisfies
\begin{align}
\mu(A) = \Exp_{Y \sim \lawP_Y}\left[\lawQ_{X\mid Y}(A \mid Y)\right], \quad \forall A \in \Borel(\cX)
\end{align}
\end{definition}
From these, we define the space of conditional couplings as follows.
\begin{definition}[Kernel Couplings] Let $\lawP_Y \in \laws(\cY)$, and $\lawQ_{X_i \mid Y} \in \laws(\cX \mid \cY)$ for $i \in \{1,2\}$. We let $\couple_{\lawP_Y}(\lawQ_{X_1 \mid Y},\lawQ_{X_1\mid Y})$ denote the space of measures $\mu \in \laws(\cX_1 \times \cX_2 \times \cY)$ over random variables $(X_1,X_2,Y)$ such that $(X_i,Y) \sim \lawof{\lawQ_{X \mid Y}}{\lawP_Y}$ for $i \in \{1,2\}$.
\end{definition}
Note that a similar construction to the independent coupling ensures $\couple_{\lawP_Y}(\lawQ_{X_1 \mid Y},\lawQ_{X_1\mid Y})$ is nonempty, namely considering the measure $\mu(A_1 \times A_2 \times B_2) = \Exp_{Y \sim \lawP_Y}\left[\lawQ_{X_1 \mid Y}(A_1 \mid Y)\lawQ_{X_2\mid Y}(A_2)\I\{Y \in B\}\right]$. 


\paragraph{Regular Conditional Probabilities.} We now recall a standard result  that conditional probabilities can be expressed through kernels in our setting. 
\begin{theorem}[Theorem 5.1.9, \citet{durrett2019probability}]\label{thm:durrett}
    If $\Omega$ is a Polish space and $\pp$ is a probability measure on the Borel sets of $\Omega$, such that random variables $(X, Y) \sim \pp$ in spaces $\cX$ and $\cY$, then there exists a kernel $\lawQ(\cdot \mid \cdot) \in \laws(\cX \mid \cY)$ such that, for all $A \in \Borel(\cX)$ and $\Pr$-almost every $y$, the (standard) conditional probability $\Pr[X \in A \mid Y] = \lawQ(A \mid y)$. We can $\lawQ(\cdot \mid \cdot)$ the \emph{regular conditional probability measure}.
\end{theorem}
Regular conditional probabilities allow one to think of conditional probabilities in the most intuitive way, i.e., for two random variables $X, Y$, the map $Y \mapsto \pp(X \in A \mid Y)$ is a probability kernel.  This will be the essential property that we use below.  


\paragraph{Gluing.} Finally, regular conditional probabilities allow us to ``glue together'' couplings which share a common random variable.
\begin{lemma}[Gluing Lemma]\label{lem:couplinggluing}
    Suppose that $X, Y, Z$ are random variables taking value in Polish spaces $\cX,\cY,\cZ$. Let $\mu_1 \in \laws(\cX \times \cY), \mu_2 \in \laws(\cY \times \cZ)$ be couplings of $(X, Y)$ and $(Y, Z)$ respectively. Then there exists a coupling $\coup \in \laws(\cX \times \cY \times \cZ)$ on $(X, Y, Z)$ such that under $\coup$, $(X, Y) \sim \mu_1$ and $(Y, Z) \sim \mu_2$.
\end{lemma}
\begin{proof} Let $\cQ(\cdot \mid Y)$ be a regular conditional probability for $Z $ given $Y$ under $\mu_2$ (who existence is ensured by \Cref{thm:durrett}).

    We construct $\coup$ by first sampling $(X, Y) \sim \mu_1$ and then sampling $Z \sim \lawQ(\cdot \mid Y)$; observe that by the second property in \Cref{def:regularconditionalprobabilities}, this is a valid construction.  It is immediate that under $\coup$, we have $(X, Y) \sim \mu_1$ and thus we must only show that $(Y, Z) \sim \mu_2$ to conclude the proof.  Let $A, B$ be two measurable sets and we see that
    \begin{align}
        \pp_{\coup}\left( (Y, Z) \in A \times B \right) &= \ee_{Y \sim \coup}\left[ \pp_{\coup}\left( (Y, Z) \in A \times B | Y \right) \right] \\
        &= \ee_{Y \sim \coup}\left[ \ee_{(Y, Z) \sim \coup}\left[ \I[Y \in A] \cdot \I[Z \in B] | Y \right] \right] \\
        &= \ee_{Y \sim \coup}\left[ \I[Y \in A] \cdot \ee_{\coup}\left[ \I\left[ Z \in B \right]| Y \right] \right] \\
        &= \ee_{Y \sim \coup}\left[ \I[Y \in A] \cdot \pp_{\mu_2}(Z \in B | Y)  \right] \\
        &= \mu_2\left( (Y, Z) \in A \times B \right),
    \end{align}
    where the first equality follows from the tower property of expectations, the second follows by definition of conditional probability, the third follows from the definition of conditional expectation, the fourth follows by the first property from \Cref{def:regularconditionalprobabilities}, and the last follows from the fact that the marginals of $Y$ under $\coup$ and under $\mu_2$ are the same.  The result follows.
\end{proof}

\subsection{Optimal Transport and Kernel Couplings}\label{sec:opt_trans_kernel}
As shown above for the TV distance, many measures of distributional distance can be quantified in terms of \emph{optimal transport} costs; these are quantities expressed as infima, over all couplings, of the expectation of a certain lower-semicontinuous functions. We show that if the optimal transport costs between two kernels $Y \to \laws(\cX_i)$ are controlled pointwise, then for any $\lawP_Y \in \laws(\cY)$, is a there exists a joint distribution over $(X_1,X_2,Y)$ which attains the minimal transport cost.
\begin{proposition}\label{prop:MK_RCP} Let $\cX_1,\cX_2,\cY$ be Polish spaces,  and let $\lawP_Y \in \laws(\cY)$, and  $\lawQ_i \in \laws(\cX_i \mid \cY)$. for $i \in \{1,2\}$. Finally, let $\phi: \cX_1\times \cX_2 \to \R$ be lower semicontinuous and bounded below.
Then, the following function 
\begin{align}
\psi(y) := \inf_{\coup \in \couple(\lawQ_1(y),\lawQ_2(y))}\Exp_{(X_1,X_2) \sim \coup}[\phi(X_1,X_2)]
\end{align}
is a measurable function of $y$ and there exists some $\coup_{\star} \in \couple_{\lawP_Y}(\lawQ_1,\lawQ_2)$ such that 
\begin{align}
\Exp_{(X_1,X_2,Y) \sim \coup_{\star}}[\phi(X_1,X_2)] = \Exp_{Y \sim \nu_Y}\psi(Y).
\end{align}
In particular it holds $\mu_\star$-almost surely that
\begin{align}
    \ee_{\mu_\star}[\phi(X_1, X_2) | Y] = \psi(Y).
\end{align}
\end{proposition}
We prove the above proposition in \Cref{sec:prop:MK_RCP}.
One useful consequence is the following identity for the total variation distance.

\begin{corollary}\label{cor:first_TV}  Let $\cX,\cY$ be Polish spaces,  and let $\lawP_Y \in \laws(\cY)$, and  $\lawQ_i \in \laws(\cX \mid \cY)$, for $i \in \{1,2\}$. Then, there exists a coupling $\coup_{\star} \in \couple_{\lawP_Y}(\lawQ_1,\lawQ_2)$ such that
\begin{align}
\Pr_{\coup_{\star}}[X_1 \ne X_2] = \Exp_{Y \sim \lawP_Y}\TV(\lawQ_1(\cdot \mid Y),\lawQ_2(\cdot \mid Y)),
\end{align}
with the left-hand side integrand being measurable. 
\end{corollary}
\begin{proof} Using \Cref{lem:tv_coupling_equiv}, we can represent total variation as an optimal transport cost with $\phi(x_1,x_2) = \I\{x_1 \ne x_2\}$.  Note that $\phi(x_1,x_2)$ is lower semicontinuous, being the indicator of an open set. Thus, the result follows from \Cref{prop:MK_RCP} with $\cX = \cX_1 = \cX_2$, and $\phi(x_1,x_2) = \I\{x_1 \ne x_2\}$.
\end{proof}


\subsection{Data Processing Inequalities}\label{sec:dpis}

We now derive two \emph{ inequalities}. First, we  recall the classical version for the total variation distance, and check that a well-known identity holds in our setting.
% \mscomment{remark that many proofs exist; this one uses coupling machinery}
\begin{lemma}[Data Processing for Total Variation]\label{cor:tv_two}  Let $\lawP_{Y_1},\lawP_{Y_2} \in \laws(\cY)$ and let $\lawQ_X \in \laws(\cX \mid \cY)$. Then, 
\begin{align}
\TV(\lawQ_X \circ \lawP_{Y_1},\lawQ_X \circ \lawP_{Y_2})&\le \TV(\lawof{\lawQ_X}{\lawP_{Y_1}},\lawof{\lawQ_X}{\lawP_{Y_2}} ) = \TV(\lawP_{Y_1},\lawP_{Y_2}).
\end{align}
\end{lemma}
\begin{proof} 
The first inequality is just the data processing inequality \citep[Theorem 7.7]{polyanskiy2022}, which also shows that
% For the first inequality, we use the direct definition of total variation given in  \Cref{defn:TV}. From this definition, \mscomment{remark this is DPI}
% \begin{align}
% \TV(\lawQ_X \circ \lawP_{Y_1},\lawQ_X \circ \lawP_{Y_2}) &= \sup_{A \in \Borel(\cX)}|\lawQ_X \circ \lawP_{Y_1}(A) - \lawQ_X \circ \lawP_{Y_2}(A)|\\
% &= \sup_{A \in \Borel(\cX)}|\lawof{\lawQ_X}{\lawP_{Y_1}}(A\times \cY) - \lawof{\lawQ_X}{\lawP_{Y_2}}(A\times \cY)|\\
% &\le \sup_{A' \in \Borel(\cX\times \cY)}|\lawof{\lawQ_X}{\lawP_{Y_1}}(A') - \lawof{\lawQ_X}{\lawP_{Y_2}}(A')|\\
% &=\TV(\lawof{\lawQ_X}{\lawP_{Y_1}},\lawof{\lawQ_X}{\lawP_{Y_2}} ).
% \end{align}
% Now let's prove the equality. First, by a similar argument to the above one, one can check that
$\TV(\lawof{\lawQ_X}{\lawP_{Y_1}},\lawof{\lawQ_X}{\lawP_{Y_2}} ) \ge \TV(\lawP_{Y_1},\lawP_{Y_2})$. To prove the reverse inequality, we use \Cref{lem:tv_coupling_equiv} to find a coupling $\mu_Y$ such that $(\lawP_{Y_1},\lawP_{Y_2})$ such that $\Exp[\I\{Y_1 \ne Y_2\}] = \TV(Y_1,Y_2)$. 

Define a probability kernel in $\laws(\cX \times \cX \mid \cY_1 \times \cY_2)$ via defining the set $B_{=} \left\{(x_1,x_2) \in \cX \times \cX:x_1 = x_2\right\} \subset \cX \times \cX$, and define for $A \in \Borel(\cX \times \cX)$,
\begin{align}
\lawQ(A \mid y_1,y_2 ) = \begin{cases}\lawQ_X\left(\pi_1\left(A  \cap B_{=}\right) \mid y_1\right) & y_1 = y_2\\
\lawQ_X( \cdot\mid y_1) \otimes \lawQ_X(\cdot \mid y_2)(A) & \text{otherwise}\\
\end{cases}
\end{align}
In a Polish space, \Cref{lem:openness_of_prod,lem:projection_keeps_measure} imply that $A\mapsto \lawQ_X\left(\pi_1\left(A  \cap B_{=}\right) \mid y_1\right)$ for eacy $y_1$ is a valid measure, and it is standard that the product measures $\lawQ_X( \cdot\mid y_1) \otimes \lawQ_X(\cdot \mid y_2)(A)$ are valid. Moreover, this construction ensures that for $\mu = \lawof{\lawQ}{\mu_Y}$,
\begin{align}
\Pr_{\mu}[\{Y_1 = Y_2\} \text{ and } \{X_1 \ne X_2\}] = 0. \label{eq:TV_data_proc_thing_event}
\end{align}
Lastly, one can check that under $\mu = \lawof{\lawQ}{\mu_Y}$, that $(X_1,Y_1) \sim \lawof{\lawQ_X}{\lawP_{Y_1}}$ and $(X_2,Y_2) \sim \lawof{\lawQ_X}{\lawP_{Y_2}}$. Thus, $\mu$ can be regarded as an element of $\couple(\lawof{\lawQ_X}{\lawP_{Y_1}},\lawof{\lawQ_X}{\lawP_{Y_2}} )$. Hence, \Cref{lem:tv_coupling_equiv} implies that
\begin{align}
\TV(\lawof{\lawQ_X}{\lawP_{Y_1}},\lawof{\lawQ_X}{\lawP_{Y_2}} ) &\le \TV(\Pr_{\mu}[(X_1,Y_1)\ne(X_2,Y_2)] \\
&= \Pr_{\mu}[Y_1 \ne Y_2] +\Pr_{\mu}[\{Y_1 = Y_2\} \text{ and } \{X_1 \ne X_2\}]\\
&= \Pr_{\mu_{\star}}[Y_1 \ne Y_2] \tag{Eq.\eqref{eq:TV_data_proc_thing_event}}\\
&= \Pr_{(Y_1,Y_2)\sim \mu_Y}[Y_1 \ne Y_2] \\
&= \TV(\lawP_{Y_1},\lawP_{Y_2}) \tag{construction of $\mu_Y$}.
\end{align} 
\end{proof}
Next, we derive a general data processing inequality for optimal costs. This result is a corollary of \Cref{prop:MK_RCP}.
\begin{lemma}[Another Data Processing Inequality for Optimal Transport]\label{cor:opt_trans} Let $\cX_1,\cX_2,\cY$ be Polish spaces,  and let $\lawP_Y \in \laws(\cY)$, and  $\lawQ_i \in \laws(\cY \mid \cX_i)$. for $i \in \{1,2\}$. Denote by $\lawQ_i \circ \lawP_Y$ the marginal of $X_i$  under $(X_i,Y) \sim \lawof{\lawQ_i}{\lawP_Y}$. Then, 
\begin{align}
\inf_{\coup \in \couple(\lawQ_1 \circ \lawP_Y,\lawQ_2 \circ \lawP_Y)}\Exp_{X_1,X_2 \sim \coup}\phi(X_1,X_2) \le \Exp_{Y \sim \mu_Y}\left(\inf_{\coup' \in \couple(\lawQ_1(Y) \circ \lawQ_2(Y))} \Exp_{X_1,X_2 \sim \coup'}\phi(X_1,X_2)\right).
\end{align}
\end{lemma}
\begin{proof} One can check that any coupling in $\coup \in \couple(\lawQ_1 \circ \lawP_Y,\lawQ_2 \circ \lawP_Y)$ can be obtained by marginalizing $Y$ in a certain coupling of $\coup' \in \couple(\lawof{\lawQ_1}{\lawP_Y},\lawof{\lawQ_1}{\lawP_Y})$, and any coupling in the latter can be marginalized to a coupling in the former. Hence, 
\begin{align}
\inf_{\coup \in \couple(\lawQ_1 \circ \lawP_Y,\lawQ_2 \circ \lawP_Y)}\Exp_{X_1,X_2 \sim \coup}\phi(X_1,X_2) = \inf_{\coup \in \couple(\lawof{\lawQ_1}{\lawP_Y},\lawof{\lawQ_1}{\lawP_Y}}\Exp_{X_1,X_2,Y_1,Y_2 \sim \coup}\phi(X_1,X_2)
\end{align}
Moreover, to every measure $\coup \in \coup_{\lawP_Y}(\lawQ_1,\lawQ_2)$ over $(X_1,X_2,Y)$, \Cref{lem:identity_coupling} implies that there exists a coupling $\coup' \in \couple(\lawof{\lawQ_1}{\lawP_Y},\lawof{\lawQ_1}{\lawP_Y})$ over $(X_1,X_2,Y_1,Y_2)$ such  $(X_1,X_2)$ have the same marginals under $\coup$ and $\coup'$. Therefore,
\begin{align}
\inf_{\coup \in \couple(\lawof{\lawQ_1}{\lawP_Y},\lawof{\lawQ_1}{\lawP_Y}}\Exp_{X_1,X_2,Y_1,Y_2 \sim \coup}\phi(X_1,X_2) \le \inf_{\coup' \in \couple_{\lawP_Y}(\lawQ_1,\lawQ_2)}\Exp_{X_1,X_2,Y\sim \coup}\phi(X_1,X_2).
\end{align}
Finally, the right hand side is equal to  $\Exp_{Y \sim \mu_Y}\left(\inf_{\coup' \in \couple(\lawQ_1(Y) \circ \lawQ_2(Y))} \Exp_{X_1,X_2 \sim \coup'}\phi(X_1,X_2)\right)$ by \Cref{prop:MK_RCP}. 
\end{proof}






\begin{comment}
A simple consequence of the gluing lemma is the following.
\begin{lemma}\label{lem:TV_perturbation} Let $(X_1,X_2)$ be jointly distributed and let $\TV(X_i,X_i') \le \epsilon_i$ for $i \in \{1,2\}$. Then, there exists a coupling of $(X_1',X_2')$ such that $\TV((X_1,X_2),(X_1',X_2')) \le \epsilon_1 + \epsilon_2$. 
\end{lemma}
\begin{proof} Since the total variation distance satisfies the triangle inequality, suffices to show that, if $X_2 = X_2'$,  and $\TV(X_1,X_1') \le \epsilon$, then $\TV((X_1,X_2),(X_1',X_2)) \le \epsilon$.

Because the total variation coupling is attained on Polish spaces \mscomment{cite}, there exists a coupling of $(X_1,X_2)$ such that $\Pr[X_1 \ne X_1'] \le \epsilon$.  By the gluing lemma, there exists a coupling of $(X_1,X_1',X_2)$ respecting the marginals of $(X_1,X_1')$ and $(X_1,X_2)$. Then, under this coupling, $\Pr[(X_1,X_2) \ne (X_1',X_1)] = \Pr[X_1' \ne X_2] \le \epsilon$. Thus, $\TV((X,Y),(X',Y)) \le \epsilon$. 
\end{proof} 
\end{comment}

\begin{comment}
\begin{corollary}\label{cor:MK_cor} Let $(X_1,X_2,\dots,X_{n+1})$ denote a finite sequence of random variables, taking values in polish spaces $\cX_i$. Let $Y \in \cY$ be distributed according to $\nu_Y$, and let $\nu_i(\cdot \mid Y)$ be RCPs on $\cX_i$. Finally, let $\phi_i:\cX_i\times \cX_{i+1}$ be bounded. lower-semicontinous functions. Then, there exists a coupling $\coup_{\star}$ over $(X_1,\dots,X_{n+1},Y)$ with $Y \sim \nu_Y$, $X_i \mid Y \sim \nu_i(\cdot \mid Y)$, and for each $i \in [n]$, the following function is measurable
\begin{align}
\psi_i(y) := \inf_{\coup \in \couple(\nu_i(\cdot \mid Y=y),\nu_{i+1}(\cdot \mid Y=y)}\Exp_{(X_i,X_{i+1}) \sim \coup}[\phi(X_i,X_{i+1})]
\end{align}
and
\begin{align}
\Exp_{Y \sim \nu_Y}\psi_i(Y) =\Exp_{(X_1,\dots,X_{n+1},Y) \sim \coup_{\star}}[\phi_i(X_{i},X_{i+1})].
\end{align}
\end{corollary}
\begin{proof}[Proof of \Cref{cor:MK_cor}] We start with \Cref{prop:MK_RCP} and induct on $n$. Then $n = 1$ case is precisely \Cref{prop:MK_RCP}. Suppose that the result holds up to $n$, and defote te resulting coupling $\coup_{\star,n}$. Then, by the inductive set, $X_n \mid Y \sim \nu_n(\cdot \mid Y)$ under $\coup_{\star,n}$. Hence,
\begin{align}
\psi_n(y) &:= \inf_{\coup \in \couple(\nu_n(\cdot \mid Y=y),\nu_{n+1}(\cdot \mid Y=y))}\Exp_{(X_n,X_{n+1}) \sim \coup}[\phi(X_{n},X_{n+1})]\\
&:= \inf_{\coup \in \couple(\coup_{\star,n}(\cdot \mid Y=y),\nu_{n+1}(\cdot \mid Y=y))}\Exp_{(X_1,\dots,X_n,X_{n+1}) \sim \coup}[\phi(X_{n},X_{n+1})]
\end{align}
Appling \Cref{prop:MK_RCP} with $\nu_1 \gets \coup_{\star,n}(\cdot \mid Y),\nu_{n+1}(\cdot \mid Y))$ and $\nu_2(\cdot \mid Y) \gets \nu_{n+2}(\cdot \mid Y)$ gives us a coupling $\coup_{\star,n+1}$ over $(X_1,\dots,X_{n+1},Y)$ such that 
\begin{align}
\Exp_{Y \sim \nu_Y}\psi_n(Y) =\Exp_{(X_1,\dots,X_{n+1},Y) \sim \coup_{\star,n+1}}[\phi_in(X_{n},X_{n+1})].
\end{align}
Moreover, since $(X_1,\dots,X_n,Y)$ have the same distribution under $\coup_{\star,n+1}$ and $\coup_{\star,n}$, the induction hypothesis implies
\begin{align}
\Exp_{Y \sim \nu_Y}\psi_i(Y) =\Exp_{(X_1,\dots,X_{i+1},Y) \sim \coup_{\star,n+1}}[\phi_in(X_{i},X_{i+1})], \quad i < n.
\end{align}
as needed.
\end{proof}

\end{comment}

\subsubsection{Deferred lemmas for the data processing inequalities}
\begin{lemma}\label{lem:openness_of_prod} Let $\cX$ be a Polish space. Then, the set $\{(x_1,x_2) \in \cX \times \cX: x_1 \ne x_2\}$ is open in $\cX \times \cX$. 
\end{lemma}
\begin{proof}
    The diagonal is closed in any Polish space by definition of the topology.  The result follows.
\end{proof}
\begin{lemma}\label{lem:projection_keeps_measure} Let $\cX$ be a Polish space, and let $\pi_1,\pi_2: \cX \times \cX$ denote the projection mappings onto each coordinate. Then, for any $A \in \Borel(\cX \times \cX)$, $\pi_1(A)$ and $\pi_2(A)$ are in $\Borel(\cX)$. 
\end{lemma}
\begin{proof}
    The projection map is open so the result follows immediately by definition of the Borel algebra.
\end{proof}
%\begin{lemma} Let $\cX$ be a Polish space, and let $\lawP_X \in \laws(\cX)$. Then there exists a measure $\mu \in \couple(\lawP_X,\lawP_X)$satisfying $\mu(A_1 \times A_2) = \lawP_X(A_1)\lawP_X(A_2)$, and another measure $\mu'$ satisfying 
%\begin{align}
%\mu'(B) = \lawP_X(\pi_1(B \cap \{(x_1,x_2) \in \cX \times \cX: \}))
%\end{align}
\begin{lemma}\label{lem:identity_coupling} Let $\cX,\cY$ be Polish spaces, and let $\mu \in\laws(\cX \times \cY)$. Then, there is a measure $\mu' \in \laws(\cX \times \cY \times \cY)$ satisfying
\begin{align}
\mu'(A \times \cY) = \mu(A), \quad \forall A \in \Borel(\cX \times \cY)
\end{align}
and 
\begin{align}
\mu'(\cX \times \{(y_1,y_2):y_1= y_2\}) = 1
\end{align}
\end{lemma}
\begin{proof} Define the set $B_{=} = \{(y_1,y_2):y_1= y_2\}$. One can check that $\mu'(A \times B) = \mu(A\times \pi_1(B \cap B_{=})$, where $\pi_1:\cY \times \cY \to \cY$ is the projection onto the first coordinate, is a valid measure.
\end{proof}


\subsection{Proof of Proposition \ref{prop:MK_RCP}}\label{sec:prop:MK_RCP}
 In the case that $\phi(\cdot,\cdot)$ is continuous, the result follows from \citet[Corollary 5.22]{villani2009optimal}. For general lower-semicontinuous $\phi$, our argument adopts the strategy of ``Step 3'' of the proof of \citet[Theorem 1.3]{villani2021topics}. This shows that there exists a sequence $\phi_n \uparrow \phi$ pointwise, such that each $\phi_n$ is uniformly bounded. Define 
\begin{align}
\psi_n(y) &:= \inf_{\coup \in \couple(\lawQ_1(y),\lawQ_2(y))}\Exp_{(X_1,X_2) \sim \coup}[\phi_n(X_1,X_2)].
\end{align}
Then, for each $n$, the continuous case implies that there exists a measure $\coup_{\star,n} \in \couple_{\nu_Y}(\lawQ_1,\lawQ_2)$ such that
\begin{align}
\Exp_{Y \sim \nu_Y}\psi_n(Y) =\Exp_{(X_1,X_2,Y) \sim \coup_{\star,n}}[\phi_n(X_1,X_2)] \label{eq:cont_case}
\end{align}
Recall now the definition
\begin{align}
\psi(y) = \inf_{\coup \in \couple(\lawQ_1(y),\lawQ_2(y))}\Exp_{(X_1,X_2) \sim \coup}[\phi(X_1,X_2)].
\end{align}

\begin{claim}\label{claim:psi_claim}  $\psi(y)$ is measurable and satisfies $\psi_n(y)\uparrow \psi(y)$ pointwise.
\end{claim}
\begin{proof} We can write
\begin{align}
\sup_{n \ge 0}\psi_n(y) &= \sup_{n \ge 0}\inf_{\coup \in \couple(\lawQ_1(y),\lawQ_2(y))}\Exp_{(X_1,X_2) \sim \coup}[\phi_n(X_1,X_2)]\\
&\overset{(i)}{=} \inf_{\coup \in \couple(\lawQ_1(y),\lawQ_2(y))}\Exp_{(X_1,X_2) \sim \coup}[\phi(X_1,X_2)] = \psi(y).
\end{align}
Here, $(i)$ follows from the ``Step 3'' in the proof of \citet[Theorem 1.3]{villani2021topics}, which shows that any optimal transport cost $C$ of a lowersemicontinuous $\phi$ is equal to a limit of the costs $C_n$ of any bounded continuous $\phi_n \uparrow \phi$. In our case, we fix each $y$, so $C = \psi(y)$ and $C_n = \psi_n(y)$. It is clear that $\psi_n(y)$ is increasing, so for each $y$, $\psi_n(y) \uparrow \psi(y)$. As $\psi$ is the pointwise monotone limit of $\psi_n$, it is measurable. 
\end{proof}
\begin{claim}The set of couplings of $\couple_{\lawP_Y}(X_1,X_2)$ is compact in the weak topology.
\end{claim}
\begin{proof} Recall that $\laws(\cY \times \cX_1 \times \cX_2)$ denote the set of Borel measures on $\cY \times \cX_1 \times \cX_2$. This set is also a Polish space in the weak topology. The subset $\couple_{\lawP_Y}(X_1,X_2) \subset \laws(\cY \times \cX_1 \times \cX_2) $  is compact if and only if it is relatively compact and closed. 

To show relative compactness, Prokhorov's theorem means that it suffices to show that  $\coup_{\lawP_Y}(\lawQ_1,\lawQ_2)$ is tight, i.e.  for all $\epsilon > 0$, there exists a compact $\cK_{\epsilon} \subset \cY \times \cX_1 \times \cX_2$ such that for any $\coup \in\couple_{\lawP_Y}(X_1,X_2)$, $\Pr_{\coup}[(Y,X_1,X_2) \in \cK_{\epsilon}] \ge 1 - \epsilon$. This follows by setting $\cK = \cK_{Y,\epsilon} \times \cK_{X,1,\epsilon} \times \cK_{X,2,\epsilon}$, where the sets are such that $\Pr_{\lawP_Y}[Y \notin \cK_{Y,\epsilon}] \ge 1- \epsilon/3$ and $\Pr_{\lawQ_{i}}[X_i \notin \cK_{X,i,\epsilon}] \ge 1- \epsilon/3$, where $\lawQ_{i}$ is the marginal of $X_i$ given by $Y \sim \lawP_Y$, $X_i \sim \lawP_i(\cdot \mid Y)$ (such sets exist because $\cX_1,\cX_2,\cY$ are Polish).

To check that $\couple_{\lawP_Y}(\lawQ_1,\lawQ_2) \subset \laws(\cY \times \cX_1 \times \cX_2) $ is closed, it suffices to show that it is sequentially closed (as $\laws(\cY \times \cX_1 \times \cX_2)$ is Polish). To this end, consider any sequence $\mu_n \in \couple_{\lawP_Y}(\lawQ_1,\lawQ_2)$ such that $\mu_n \weakconv  \mu \in \laws(\cY \times \cX_1 \times \cX_2)$  in the weak topology. By definition, this means that for any $i \in \{1,2\}$ and any  continuous and bounded $f_i:\cY \times \cX_i \to \R$, 
\begin{align}
\lim_{n \to \infty}\Exp_{\mu_n}f_i(Y,X_i) = \Exp_{\mu}f_i(Y,X_i).
\end{align}
For all $\mu_n \in \couple_{\lawP_Y}(\lawQ_1,\lawQ_2)$, $\Exp_{\mu_n}f_i(Y,X_i) = \Exp_{Y \sim \nu_Y}\Exp_{X_i \sim \nu_i(\cdot \mid Y_i)}f_i(Y,X_i)$. Thus,
\begin{align}
\Exp_{\mu}f_i(Y,X_i) =  \Exp_{Y \sim \nu_Y}\Exp_{X_i \sim \nu_i(\cdot \mid Y_i)}f_i(Y,X_i), \quad \text{ for all continuous, bounded } f_i:\cY \times \cX \to \R.
\end{align}
Hence, the marginal distribution of $(Y,X_i)$ under $\mu$ must be equal to that of $(Y \sim \lawP_Y,X_i \sim \lawQ_i(\cdot \mid Y))$ for $i \in \{1,2\}$, which means $\mu \in \couple_{\lawP_Y}(\lawQ_1,\lawQ_2)$.
\end{proof}

By compactness, there exists (passing to a subsequence if necessary) a $\mu_{\star} \in \couple_{\lawP_Y}(\lawQ_1,\lawQ_2)$ such that $\mu_{\star,n} \weakconv \mu_{\star}$ in the weak topology. Then, as $\phi_m$ is continuous and bounded, it follows that for all $m$,
\begin{align}
\Exp_{(X_1,X_2,Y) \sim \coup_{\star}}[\phi_m(X_1,X_2)] &= \limsup_{n \to \infty}\Exp_{(X_1,X_2,Y) \sim \coup_{\star,n}}[\phi_m(X_1,X_2)] \tag{$\mu_{\star,n} \weakconv \mu_{\star}$}\\
&\le \limsup_{n \to \infty}\Exp_{(X_1,X_2,Y) \sim \coup_{\star,n}}[\phi_n(X_1,X_2)] \tag{$\phi_m \le \phi_n$ for $n \ge m$}\\
&= \limsup_{n \to \infty}\Exp_{Y}\psi_n(Y)\tag{\eqref{eq:cont_case}}\\
&= \Exp_{Y}\lim_{n \to \infty}\psi_n(Y) \tag{Monotone Convergence}\\
&= \Exp_{Y}\psi(Y) \tag{\Cref{claim:psi_claim}}.
\end{align}
Thus, by the monotone convergence theorem,
\begin{align}
\Exp_{(X_1,X_2,Y) \sim \coup_{\star}}[\phi(X_1,X_2)] &= \Exp_{(X_1,X_2,Y) \sim \coup_{\star}}\left[\lim_{m \to \infty}\phi_m(X_1,X_2)\right]\\
&= \lim_{m \to \infty}\Exp_{(X_1,X_2,Y) \sim \coup_{\star}}\left[\phi_m(X_1,X_2)\right]\\
&\le \lim_{m \to \infty}\Exp_{Y}\psi(Y) = \Exp_{Y}\psi(Y).
\end{align}
Similarly, repeating some of the above steps,
\begin{align}
\Exp_{Y}\psi(Y) &= \limsup_{n \to \infty}\Exp_{Y}\psi_n(Y)\\
&= \limsup_{n \to \infty}\Exp_{(X_1,X_2,Y) \sim \coup_{\star,n}}[\phi_n(X_1,X_2)]\\
&\le \limsup_{n \to \infty}\Exp_{(X_1,X_2,Y) \sim \coup_{\star,m}}[\phi_n(X_1,X_2)] \tag{$\mu_{\star,n}$ is the optimal coupling for $\phi_n$}\\
&\le \Exp_{(X_1,X_2,Y) \sim \coup_{\star,m}}[\lim_{n \to \infty}\phi_n(X_1,X_2)] \tag{monotone convergence}\\
&\le \Exp_{(X_1,X_2,Y) \sim \coup_{\star,m}}[\phi(X_1,X_2)].
\end{align}
Hence, $\Exp_{Y}\psi(Y) \le \liminf_{m \ge 1} \Exp_{(X_1,X_2,Y) \sim \coup_{\star,m}}[\phi(X_1,X_2)]$. By assumption, $\phi(X_1,X_2)$ is lower semicontinuous and bounded from below. Thus, the Portmanteau theorem \citep{durrett2019probability} implies that, as $\coup_{\star,m} \weakconv \coup_\star$, $\liminf_{m \ge 1} \Exp_{(X_1,X_2,Y) \sim \coup_{\star,m}}[\phi(X_1,X_2)] =  \Exp_{(X_1,X_2,Y) \sim \coup_{\star}}[\phi(X_1,X_2)]$. Hence, $\Exp_{Y}\psi(Y) \le \Exp_{(X_1,X_2,Y) \sim \coup_{\star}}[\phi(X_1,X_2)]$, proving the reverse inequality.

\paragraph{Proof of the last statement.} To prove the last statement, we observe that if $\mu_\star \in \couple_{\lawP_Y}(\lawQ_1, \lawQ_2)$ then there exists a version of $(\mu_\star)_{X,X' | Y}$ that is a regular conditional probability and such that for almost every $y$ it holds that $(\mu_\star)_{X,X' | y} \in \couple(\lawQ_1(y), \lawQ_2(y))$.  Indeed, the existence of a version that is a regular conditional probability is immediate by \Cref{thm:durrett}.  To see that this version is a valid coupling of $\lawQ_1(y)$ and $\lawQ_2(y)$, observe that under $\mu_\star$, the joint law of $(X, Y) \sim \lawQ_1$ and thus the conditional distribution under $\mu_\star$ of $X | Y$ is determined up to sets of $\lawQ_1$-measure 0.  In particular, again by \Cref{thm:durrett}, there exists a regular conditional probablity that is a version of $(\mu_\star)_{X|y}$ and this must agree almost everywhere with $(\lawQ_1)_{X|y} = \lawQ_1(y)$.  The same argument holds for $X'$ and thus $(\mu_{\star})_{X,X'|y} \in \couple(\lawQ_1(y), \lawQ_2(y))$ for almost every $y$.  Thus, by definition of $\psi$ as an infimum, it holds for almost every $y$ that
\begin{align}
    \psi(y) \leq \ee_{(X,X')\sim (\mu_{\star})_{|Y}}[\phi(X, X')].
\end{align}
By the second claim of the proposition, we also have that
\begin{align}
    \ee_{\mu_{\star}}\left[ \phi(X_1, X_2) \right] = \ee_{\mu_{\star}}[\psi(Y)].
\end{align}
Because the expectations are equal and one function is pointwise almost everywhere dominated by the other function, the two functions must be equal almost everywhere, concluding the proof. \qed


\subsection{A simple union-bound recursion.} \label{sec:simple_union_bound} Finally, we also use the following version of the union bound extensively in our recursion proofs.
\begin{lemma}\label{lem:peeling_lem} For any event $\cE$ and events $\cB_1,\cB_2,\dots,\cB_H$, it holds that
\begin{align}
\Pr[(\cQ \cap \bigcap_{h=1}^H\cB_h)^c] \le \Pr[\cQ^c] + \Pr\left[ \exists h \in [H] \text{ s.t. } \left(\cQ \cap \bigcap_{j=1}^{h-1} \cB_j \cap \cB_h^c\right) \text{ holds } \right ]
\end{align}
\end{lemma}
\begin{proof}
    Note that
    \begin{align}
        \left( \cQ \cap \bigcap_{h  =1}^H \cB_h \right)^c &= \cQ^c \cup \left(\cQ\cap \left( \bigcap_{h  =1}^H \cB_h \right)^c  \right) = \cQ^c \cup \bigcup_{h = 1}^H \cQ \cap \cB_h \cap  \bigcap_{j = 1}^{h-1} \cB_j.
    \end{align}
    The result follows by a union bound.
\end{proof}

\newcommand{\coupinit}{\coup_{\mathrm{init}}}
