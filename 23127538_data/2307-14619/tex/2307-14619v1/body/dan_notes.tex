\newcommand{\fcl}{f_{\mathrm{cl}}}
\section{Dan Notes}

\dpcomment{These definitions are for high-level informational purposes and are not super rigorous.}

\begin{definition} Let $\varphi_t^{\pi}(\xi)$ denote $x_t$ under the closed-loop dynamics from controller $\pi$ with $x_0 = \xi$.
    
\end{definition}

\begin{definition}[Incremental-Input-to-State Stability] A system $f: \mathcal{X} \times \mathcal{U} \to \mathcal{X}$ is $(\gamma,\beta)$ incrementally-input-to-state stable (i.e., $(\gamma,\beta)$-IISS) if

    \begin{align}
        \|\varphi_{t}(\xi_1; \{u_i\}_{i=1}^{t-1}) - \varphi_{t}(\xi_2; \{0\}_{i=1}^{t-1})\| \leq \gamma(\|\xi_1 - \xi_2\|, t) + \beta(\max_{k < t}\|u_k\|).
    \end{align}

We say that a policy $\pi$ is $(\gamma,\beta)$-IISS if the closed loop $\fcl^{\pi}(x_t, u_t) = f(x_t, \pi(x_t) + u_t)$ is $(\gamma,\beta)$-IISS.
\end{definition}

\begin{theorem}[TaSIL]\label{thm:tasil} Let $\pi^*$ be $(\gamma,\beta)$-IISS. Then provided there exists an $\epsilon > 0$ such that
\begin{align}
    \max_t \sup_{\|z\| < \epsilon} \|\pi(\varphi_t^{\pi_*}(\xi) + z) - \pi^*(\varphi_t^{\pi_*}(\xi) + z)\| \leq \gamma^{-1}(\epsilon)
\end{align}
it holds that
\begin{align}
    \sup_{t} \|\varphi_t^{\pi_*}(\xi) - \varphi_t^{\pi}(\xi)\| < \epsilon
\end{align}
\end{theorem}
\begin{proof} This follows by induction. The base case $t = 0$ is trivial. Note that
\begin{align}
    \|\varphi_t^{\pi_*}(\xi) - \varphi_t^{\pi}(\xi)\| \leq \gamma(\max_{k<t} \|\pi(\varphi_k^{\pi}(\xi)) - \pi_*(\varphi_k^{\pi}(\xi))\|)
\end{align}
By (strong) induction $\varphi_t^{\pi}(\xi)$ is $\epsilon$-close to $\varphi_t^{\pi_*}(\xi)$ for all $t$. By the condition $\max_{t}\|$
\end{proof}


\begin{theorem}[Informal] If $\gamma(\epsilon)$ is an $n$-th degree polynomial, the requirements for Theorem \ref{thm:tasil} can be satisfied by matching the first $n$th derivatives with sufficient accuracy.
\end{theorem}
\begin{proof}(Informal)
    Note that if $\gamma(x)$ is a degree $n$ polynomial, $\gamma^{-1}$ is a degree $n$ polynomial. Therefore, taking an $n$-th degree Talor expansion of the LHS:
\begin{align}
    \max_t \sum_{i = 0}^{n-1} \|D^i\pi(\varphi_t^{\pi_*}(\xi)) - D^i\pi^*(\varphi_t^{\pi_*}(\xi))\|\epsilon^i + L\epsilon^{n+1} \leq \gamma^{-1}(\epsilon) = \sum_{i=1}^{n}a_i\epsilon^i.
\end{align}
Provided the $D^i\pi - D^i\pi_*$ terms are $< a_i$ for all $i$, we can pick $\epsilon > 0$ sufficiently small such that $\epsilon$ is a solution to the adversarial condition.
\end{proof}


\paragraph{Using RL to synthesize stabiling trajectories.}
Collect demonstrations $\{(s_{1:T}^{(i)}, a_{1:T}^{(i)})\}_{i=1}^N$. 

In actuality we are skipping $L$ states and stacking $L$ inputs for some input generation horizon $L$, i.e $$s_h = x_{Lh},\quad\quad a_h = \begin{bmatrix}u_{h,1}, u_{h,2}, \ldots, u_{h,L}\end{bmatrix}$$
Therefore our policy is of the form $a_h = k(s_h, s'_h \mid \zeta_h)$, i.e $u_{h,1:L} = k(x_{Lh}, x_{Lh}' \mid \zeta_h)$. For a fixed $s_{h}'$, we want $k(s_{h}, s_{h}')$ to stabilize towards $\{s_{j}'\}_{j=h}^{H}$.

Rather than doing this with feedback gains, the basic idea is that we sample new expert trajectories starting at $\tilde{s}_h^{(i,\ell)} = s_{h}^{(i)} + \epsilon^{(i,\ell)} \mid \epsilon^{(i,\ell)} \sim \mathcal{N}(0, I)$ (or whatever smothing distribution we please). For each sampled $\tilde{s}_h^{(i,\ell)}$, we then use RL to learn a sequence of actions $\{\bar{a}_{j}^{(i,\ell)}\}_{j=h}^{H}$ minimizing the cost $\max_{H \leq j \geq h} \|\bar{s}_j^{(i,\ell)} - s_j^{(i)}\|$. These actions can be considered as $k(\bar{s}_j^{(i,\ell)}, s_h^{(i)})$, effectively giving us a way of sampling from an incrementally stabilized version of the expert policy.

% \begin{enumerate}
%     \item 
% \end{enumerate}