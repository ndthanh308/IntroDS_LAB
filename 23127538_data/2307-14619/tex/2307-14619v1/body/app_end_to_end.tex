%!TEX root = ../main.tex
\newcommand{\ErrTVC}{\textsc{ErrTvc}}
\section{End-to-end Guarantees and the Proof of Theorem \ref{thm:main}}\label{app:end_to_end}

In this section, we provide a number of end-to-end guarantees for the learned imitation policy under various assumptions.  The core of the section is the proof of  \Cref{thm:main_template} which provides the basis for the final proof of \Cref{thm:main} in the body by uniting the analysis in the composite MDP from \Cref{sec:imit_composite}, the control theory from \Cref{app:control_stability}, and the sampling guarantees from \Cref{app:scorematching}.  We now summarize the organisation of the appendix:
\begin{itemize}
    \item In \Cref{ssec:end_to_end_prelims}, we recall the association between the control setting and the composite MDP presented in \Cref{sec:analysis}, as well as rigorously instantiating the direct decomposition and the expert policy.
    \item In \Cref{translating:imit_composite_to_gap}, we establish the correspondence between the imitation losses studied for the composite MDP in \Cref{sec:analysis} with the disiderata in \Cref{sec:setting}.
    \item In \Cref{ssec:end_to_end_mainproof}, we provide the proof of \Cref{thm:main_template,thm:main}.
    % state a reduction from imitation learning to conditional sampling, which we then use to derive a proof of \Cref{thm:main}.
    \item In \Cref{ssec:end_to_end_demonstrator_tvc}, we demonstrate that if the demonstrator policy is assumed to be TVC, then we can recover stronger guarantees than those provided in \Cref{thm:main} without this assumption; in particular, we show that we can bound the \emph{joint} imitation loss as well as the marginal and final versions.
    \item In \Cref{app:imititation_in_tv}, we show that if we were able to produce samples from a distribution close in \emph{total variation} to the expert policy distribution, as opposed to the weaker optimal transport metric that we consider in the rest of the paper, then without any further assumptions, imitation learning is easily achievable.
    \item  In \Cref{app:imititation_in_joint_no_aug}, we show that if we remove the data augmentation from $\toda$, i.e., we set $\sigma = 0$, then we can recover similar guarantees under the assumption that the imitator policy $\pihat$ is TVC.  In this way, we show that in some sense, total variation continuity is the important property imparted by smoothing.
    \item In \Cref{ssec:consequences_for expected_costs}, we demonstrate the utility of our imitation losses, showing that for Lipschitz cost functions decomposing in natural ways, our imitation losses as defined in \Cref{def:losses} provide control over the difference in expected cost under expert and imitated distributions.
    \item Finally, in  \Cref{ssec:end_to_end_lemmata}, we collect a number of useful lemmata that we use throughout the appendix.
\end{itemize}


\subsection{Preliminaries}\label{ssec:end_to_end_prelims}

Here, we state various preliminaries to the end-to-end theorems. For simplicity, to avoid complications with the boundary effects at $h = 1$, we re-define $h=1$-memory chunks $\pathm[1]$ as elements $\scrP_{\taum-1}$ by prepending the necessary zeros -- i.e. $\pathm[1] = (0,0,\dots,0,\bx_1)$-- and similarly modifying $\pathc[1] \in \scrP_{\tauc}$ by prepending zeros.   We first recall the definitions of the composite-states and -actions from \Cref{sec:analysis}.  The prepending of zeros in the $h=1$ case is mentioned above.  For $h > 1$, recall that $\seqs_h = (\bx_{t_{h-1}:t_h}, \bu_{t_{h-1}:t_h-1})$ and that $\seqa_h = \sfk_{t_h:t_{h+1}-1}$, where we again emphasize that $\seqa_h$ begins at the same $t$ that $\seqs_{h+1}$ does.  We further recall that $\dists(\seqs_h, \seqs_h') = \max_{t \in [t_{h-1}:t_h]} \norm{\bx_t - \bx_t'} \vee \max_{t \in [t_{h-1}:t_h-1]} \norm{\bu_t - \bu_t'}$, $\disttvc(\seqs_h, \seqs_h') = \max_{t \in [t_h - \taum:t_h]} \norm{\bx_t - \bx_t'} \vee \max_{t \in [t_h - \taum:t_h - 1]} \norm{\bu_t - \bu_t'}$, and $\distips(\seqs_h, \seqs_h') = \norm{\bx_{t_h} - \bx_{t_h}'}$.  Finally, for $\seqa = (\bbaru_{1:\tauc}, \bbarx_{1:\tauc}, \bbarK_{1:\tauc})$ and $\seqa' = (\bbaru_{1:\tauc}', \bbarx_{1:\tauc}', \bbarK_{1:\tauc}')$, recall from \eqref{eq:dA_body} that
\begin{align}
    \dmax(\seqa,\seqa') &=\max_{1\le k \le \tauc}\|\bbaru_{k}-\bbaru_{k}'\| + \|\bbarx_{k}-\bbarx_{k}'\| +\|\bbarK_{k}-\bbarK_{k}'\|\\
    \distA(\seqa,\seqa') &:= c_1 \dmax(\seqa,\seqa') + 
    c_1 + \I_{0,\infty}\{\cE\},
\end{align}
where we $\cE = \{\max_{1\le k \le \tauc}\max\{\|\bbaru_{k}-\bbaru_{k}'\|,\|\bbarx_{k}-\bbarx_{k}'\|, \|\bbarK_{k}-\bbarK_{k}'\|\} \le c_2\}$,  $\I_{0,\infty}$ is the indicator taking infinite value when the event fails to hold, and $c_1$ and $c_2$ are given in \Cref{defn:constants_for_stability_stuff}.


\paragraph{Direct Decomposition and Smoothing Kernel.} This section will invoke the generalizations \Cref{thm:smooth_cor} which requires TVC only subspace of the state space. This invokes the direct decomposition explained in \Cref{sec:imit_composite}.
\begin{definition}[Direct Decomposition and Smoothing Kernel]\label{defn:smoothing_instantiation} We consider the decomposition of $\cS = \cZ \oplus \cV$, where $\cZ = \scrP_{\taum-1}$ are the coordinates of $\pathc$ corresponding to the memory chunk $\pathm$, and $\cV$ are all remaining coordinates We let $\phiZ:\cS \to \cZ$ denote the projection onto the coordinates in $\cZ$.
We instantiate the smoothing kernel $\Wsig$ as follows: For $\seqs = \pathc \in \scrP_{\tauc}$, we let
    \begin{align}
    \Wsig(\seqs) = \cN\left(\pathc, \begin{bmatrix} \sigma^2 \eye_{\cZ} & 0 \\ 
    0 & 0 \end{bmatrix}\right),
    \end{align} 
    where $\eye_{\cZ}$ denotes the identity supported on the coordinates in $\cZ$ as described above.
\end{definition}
We note that the above direct decomposition satisfies the requiste compatibility assumptions explained in \Cref{sec:imit_composite}. Note also that $\distips$ and $\Wsig$ are compatible with the above direct decomposition. 

\paragraph{Chunking Policies.} We continue by centralizing a definition of chunking policies.
\begin{definition}[Policy and Initial-State Distributions] \label{defn:Dchunking}Given an \emph{chunking policy} $\pi = (\pi_h)_{h=1}^H$ with $\pi_h:\scrP_{\taum-1} \to \laws(\cA)$, we let $\cD_{\pi}$ denote the distribution over $\ctraj_T$ and $\seqa_{1:H}$ induced by selecting $\seqa_h \sim \pi_h(\pathm)$, and rolling out the dynamics as described in \Cref{sec:setting}. We extend chunking policies to maps $\pi_h: \cS = \scrP_{\tauc} \to \laws(\cA)$ by expressing $\pi_h = \pi_h \circ \phiZ$ (i.e., projection $\pathc$ onto its $\pathm$-components). Further, we let $\Dinit$ denote the distribution of $\bx_1$ under $\ctraj_T \sim \Dexp$.
\end{definition}
\begin{remark} The notation $\cD_{\pi}$ denotes the special case of chunking policies in the control setting of \Cref{sec:setting}, whereas we reserve the seraf font $\Dist_{\pi}$ for the distribution induced by policies in the abstract MDP. For composite MDPs instantiated as in \Cref{sec:control_instant_body}, the two exactly coincide.
\end{remark}

\paragraph{Construction of $\pist$ for composite MDP.} We now explain how to extract $\pist$ from $\Dexp$ in the composite MDP.
\begin{definition}[Policies corresponding to $\Dexp$]\label{def:Dexp_policies} Define the following sequence kernels $\pist = (\pist_h)_{h=1}^H$ and $\pidec = (\pidech)_{h=1}^H$ via the following process. Let $\ctraj_T \sim \Dexp$, and let $\seqa_{1:H} = \synth(\ctraj_T)$; further, let $\pathm[1:H]$ be the corresponding memory-chunks from $\ctraj_T$. Let
\begin{itemize}
    \item $\pist_h(\cdot): \scrP_{\taum-1} \to \cA$ denote a regular conditional probability corresponding to the distribution over $\seqa_h$ given $\pathm$ in the above construction. 
    \item $\pidech(\cdot): \scrP_{\taum-1} \to \cA$ denote a regular conditional probability corresponding to the distribution over $\seqa_h$ given an augmented $\pathmtil \sim \cN(\pathm,\sigma^2 \eye)$.
\end{itemize}
Finally, for $\pist$ as constructed above, $\Psth$ denotes the distribution over $\pathc$ under $\cD_{\pist}$. By \Cref{lem:pistar_existence}, this is in fact equal to the distribution over $\pathc$ under $\Dexp$. Notice further, therefore, that $\phiZ \circ \Psth$ is precisely the distribution of $\pathm$ under $\Dexp$.
\end{definition}

\begin{remark}  We remark that by \Cref{thm:durrett}, $\pisth$ is unique up to a measure zero set of $\pathm$ as distributed as above, and $\pidech$ is unique almost surely for $\pathmtil$ distributed as above. In particular, since the latter has density with respect to the Lebesgue measure and infinite support, $\pidech$ is unique in a Lebesgue almost everywhere sense.  
\end{remark}








\subsubsection{Preliminaries for joint-distribution imitation.} This section introduces a further \emph{joint imitation gap}, which we can make small under a stronger bounded-memory assumption on $\Dexp$ stated below. 
\begin{definition}[Joint Imitation Gap]\label{def:loss_joint}
    Given a chunking polcy $\pi'$, we let
    \begin{align}\label{eq:imitjoint}
        \Imitjoint(\pi) &:= \inf_{\coup} \Pr_{\coup}\left[\max_{t\in [T]}\max\left\{\|\xexp_{t+1} - \bx^\pi_{t+1}\|,\|\uexp_t - \bu^\pi_t\|\right\} > \epsilon\right],
    \end{align}
where the infimum is over all couplings between the distribution of $\ctraj_T$ under $\Dexp$ and that induced by the policy $\pi$.
\end{definition}
Controlling $\Imitjoint(\pi)$ requires various additional stronger assumptions (\emph{which we do not require in \Cref{thm:main}}), one of which is that the demonstrator has bounded memory:
\begin{definition}\label{defn:bounded_memory} We say that the demonstration distribution, synthesis oracle pair $(\Dexp,\synth)$ have $\tau$-bounded memory if under $\ctraj_T  = (\bx_{1:T+1},\bu_{1:T})\sim \Dexp$ and $\seqa_{1:H} = \synth(\ctraj_T)$, the conditional distribution of $\seqa_h$ and $\bx_{1:t_h - \tau},\bu_{1:t_h - \tau}$ are conditionally independence given $(\bx_{t_h-\tau+1:t_h},\bu_{t_h - \tau + 1:t_{h} -1})$. 
\end{definition} 
We note that enforcing \Cref{defn:bounded_memory} can be relaxed to a mixing time assumption (see \Cref{rem:mixing_time}). Moreover, we stress that we \emph{do not} need the condition in \Cref{defn:bounded_memory} if we only seek imitation of marginal distributions (as captured by $\Imitmarg$ and $\Imitfin$), as in \Cref{thm:main}.
%In this section, we complete the proof of \Cref{thm:main} using our intermediate results which demonstrate imittion of the appropriate marginal distributions. Under stronger conditions, we also demonstrate that we can imitate the joint distributions over trajectories, i.e. ensuring the following loss is small: 
\subsection{Translating Control Imitation Losses to Composite-MDP Imitation Gaps}\label{translating:imit_composite_to_gap}
\begin{lemma}\label{lem:eq_loss_converstions} Recall the imitation losses \Cref{def:losses,def:loss_joint}, and the compsite-MDP imitation gaps \Cref{defn:imit_gaps}.
Further consider, the substitutions defined in \Cref{sec:control_instant_body}, with $\pist$ instantiated as in \Cref{def:Dexp_policies}.  Given policies $\pi = (\pi_h)$ with $\pi_h:\scrP_{\taum -1} \to \cA$, we can extend $\pi_h: \cS = \scrP_{\tauc} \to  \cA$ by the natural embedding of $\scrP_{\taum-1}$ into $\scrP_{\tauc}$. Then, for any $\epsilon > 0$, 
 \begin{align}
  \Imitmarg[\epsilon](\pi) \le \gapmarg(\pi \parallel \pist). 
  \end{align}
  If we instead consider the the substitutions defined in \Cref{sec:control_instant_body}, but set $\dists$ to equal $\distips$, which only measures distance in the final coordinate of each trajectory chunk $\pathc$, 
  \begin{align}
  \Imitfin[\epsilon](\pi) \le \gapmarg(\pi \parallel \pist), \quad \dists(\cdot,\cdot) \gets \distips(\cdot,\cdot) \label{eq:imitfin_thing}
  \end{align}
  Finally, if $\Dexp$ has $\tau \le \taum$-bounded memory,
  \begin{align}
  \Imitjoint[\epsilon](\pi) \le  \gapjoint(\pi \parallel \pist).
  \end{align}
\end{lemma}
\begin{proof} Let's start with the first bound, let superscript $\mathrm{exp}$ denote objects from $\Dexp$ and superscript $\pi$ from $\cD_{\pi}$, the distribution induced by chunking policy $\pi$. Letting $\inf_{\coup}$ denote infima over couplings between the two, we have
\begin{align}
\Imitmarg(\pi) &:= \max_{t\in [T]}\inf_{\coup} \left\{\Pr_{\coup}\left[\|\xexp_{t+1} - \bx^\pi_{t+1}\| > \epsilon\right],\, \Pr_{\coup}\left[\|\uexp_t - \bu^\pi_t\| > \epsilon\right]\right\}\\
&:= \max_{t\in [T]}\inf_{\coup} \left\{\Pr_{\coup}\left[\|\xexp_{t+1} - \bx^\pi_{t+1}\| \vee \|\uexp_t - \bu^\pi_t\| > \epsilon\right]\right\}\\
&\le \max_{h\in [H]}\inf_{\coup} \left\{\Pr_{\coup}\left[ \max_{0 \le i \le \tauc} \|\xexp_{t_h-i} - \bx^\pi_{t_h - i }\| \vee  \max_{1 \le i \le \tauc} \|\uexp_{t_h-i} - \bu^\pi_{t_h - i}\|\right]\right\}\\
&\le \max_{h\in [H]}\inf_{\coup} \left\{\Pr_{\coup}\left[ \dists(\pathc^{\mathrm{exp}},\pathc^{\pi})\right]\right\},
\end{align}
From \Cref{lem:pistar_existence}, $\pathc^{\mathrm{exp}}$ has the same marginal distribution as $\pathc^{\pist}$, the distribution induced by $\pist$ in \Cref{def:Dexp_policies}. Note the subtlety that the joint distribution of these may defer because $\pist$ has limited trajectories. Still, letting $\inf_{\coup'}$ denote infimum over couplings between $\cD_{\pi}$ and $\cD_{\pist}$, equality of marginals suffices to ensure    
\begin{align}
\Imitmarg(\pi) &= \max_{h\in [H]}\inf_{\coup} \left\{\Pr_{\coup'}\left[ \dists(\pathc^{\pist},\pathc^{\pi})\right]\right\},
\end{align}
which is at most $\gapmarg(\pi \parallel \pist)$ by definition \Cref{defn:imit_gaps}.

For the final-state imitation loss,
\begin{align}
\Imitfin(\pi) &:= \inf_{\coup} \Pr_{\coup}\left[\|\xexp_{T+1} - \bx^\pi_{T+1}\| > \epsilon\right]\\
&\le \max_{h\in [H]}\inf_{\coup} \left\{\Pr_{\coup}\left[ \distips(\pathc^{\mathrm{exp}},\pathc^{\pi})\right]\right\},
\end{align}
where again $\distips$ only measures error in the final state of $\pathc$. The corresponding bound in \eqref{eq:imitfin_thing} follows similarly. 

Finally, we have 
\begin{align}
        \Imitjoint(\pi) &:= \inf_{\coup} \Pr_{\coup}\left[\max_{t\in [T]}\max\left\{\|\xexp_{t+1} - \bx^\pi_{t+1}\|,\|\uexp_t - \bu^\pi_t\|\right\} > \epsilon\right],
    \end{align}
    When $\Dexp$ has $\tau \le \taum$-bounded memory, then, the expert and $\pist$-induced trajectories are identically distributed. Therefore, directly from this observation and  \Cref{defn:imit_gaps},
\begin{align}\label{eq:imitjoint}
        \Imitjoint(\pi) &= \inf_{\coup} \Pr_{\coup}\left[\max_{t\in [T]}\max\left\{\|\bx^{\pist}_{t+1} - \bx^\pi_{t+1}\|,\|\bu^{\pist}_t - \bu^\pi_t\|\right\} > \epsilon\right] \le \gapjoint(\pi \parallel \pist).
    \end{align}
\end{proof}

\subsection{Proof of \Cref{thm:main,thm:main_template}}\label{ssec:end_to_end_mainproof}
Before begining, we place the policy error term $\Delta_{\sigma,h}(\pihat;\epsilon)$ from \Cref{defn:policy_error} in a form closer to the right-hand side of \Cref{thm:smooth_cor}.
\begin{lemma}\label{lem:Delta_redef} Recall from \Cref{defn:dec_cond} that $\pidech$ is theconditional distribution of $\seqa_h \mid \pathmtil$ under $\Daugh$. It holds that
\begin{align}
\Delta_{\sigma,h}(\pihat;\epsilon) &:=  \inf_{\coup \in \couphatsigh}\Pr_{(\seqa,\seqa') \sim \coup}[\dmax(\seqa,\seqa') > \epsilon] \\
&=\Exp_{\pathmtil} \inf_{\coup \in \pidech(\cdot \mid \pathmtil), \pihat(\pathmtil)} \Pr_{(\seqa,\seqa') \sim \coup}[\dmax(\seqa,\seqa') > \epsilon],
\end{align}
where $\pathmtil \sim \Daugh$, and where 
\end{lemma}
\begin{proof} The proof follows directly from \Cref{prop:MK_RCP} and lower semicontinuity of the function $\I\{\dmax(\seqa,\seqa') \le \epsilon\} = 1 - \I\{\dmax(\seqa,\seqa') > \epsilon\}$.
\end{proof}

We now prove \Cref{thm:main,thm:main_template}. We recall the former. 
\begin{theorem*}[Restatement of \Cref{thm:main_template}.]
Let \Cref{asm:traj_regular,asm:Jacobian_Stable} hold, and let $c_1,\dots,c_5 > 0$ be as in \Cref{defn:prob_constants_body}. 
Suppose that the  $\epsilon,\sigma,\tau > 0$ satisfy $\epsilon < c_2 $, and $\tauc \ge c_3/\eta$, and $ 5 \dimx + \log\left( \frac {4\sigma}{\epsilon} \right) \le c_4^2/(16\sigma^2)$. Then the marginal and final imitation loss of the \emph{smoothed }$\pihat_{\sigma}$ are bounded by
\begin{align}
    \Imitmarg[\epsilon_1](\pihat_{\sigma}) \vee \Imitfin[\epsilon_2](\pihat_{\sigma})  &\leq H\left(\frac{2c_1\epsilon}{\sigma} +  \iota(\epsilon) e^{-\frac{\eta(\tauc - \taum)}{\Lstab}}\right) +  \sum_{h=1}^H\Delta_{\sigma,h}(\pihat;\epsilon/c_1) \label{eq:mainguarantee}\\
    &\text{ where } \epsilon_1 =  c_1 \epsilon + \sigma \iota(\epsilon) , \quad 
    \epsilon_2 = c_1 \epsilon + \sigma e^{-\frac{\eta \tauc}{\Lstab}} \iota(\epsilon), 
    \end{align}
    and where  $\iota(\epsilon) = 6c_5\sqrt{5 \dimx + 2\log\left( \frac{4\sigma}{c_1 \epsilon} \right)}$ is logarithmic in $1/\epsilon$.
    \end{theorem*}
\begin{proof} 

Lets begin by bounding $\Imitmarg(\pi) $. Recall the definitions of $\dists,\disttvc,\distips$ in \Cref{sec:analysis}, and let $\sstar_{1:H+1}$ and $\seqs_{1:H+1}$ denote the composite states corresponding to a trajectory $(\bx_{1:T+1}^{\pist},\bu^{\pist}_{1:T})$ under $\pist$ and $(\bx_{1:T+1}^{\pi},\bu^{\pi}_{1:T})$, respectively, under the instantiation of the composite MDP in \Cref{sec:control_instant_body}. We can view $\pist$ and $\pi$ (which depend only on memory chunks $\pathm$) as policies in the composite MDP which are compatible with the decomposition \Cref{defn:direct_decomp}. 
We make the following points:
\begin{itemize}
    \item In light of \Cref{lem:eq_loss_converstions},
    \begin{align}
    \Imitmarg[\epsilon_1](\pi \parallel \pist)  \le \gapmarg[\epsilon_1](\pi \parallel \pist).
    \end{align}\item
    By \Cref{lem:gaussian_tvc}, a consequence of Pinsker's inequality, it holds that the Gaussian kernel $\Wsig$ used in \toda{} is $\gamsig$-TVC (w.r.t. $\disttvc$) with $\gamsig(u) = \frac{u\sqrt{\taum + 1}}{2\sigma}$. 
    \item Note that $\distips(\seqs_h,\seqs_h') = \|\bx_{t_h}-\bx_{t_h}'\|$ measures Euclidean distance between the last $\bx$-coordinates of $\seqs_h,\seqs_h'$. Moreover, if $\seqs_h' \sim \Wsig(\seqs_h)$ the last coordinate $\bx_{t_h}'$ of $\seqs'$ is distributed as $\cN(\bx_{t_h},\sigma^2 I)$. By \Cref{lem:gaussian_concentration} with $d = \dimx$, that for $r = 2\sigma \cdot \sqrt{5 \dimx + 2\log\left( \frac 1p \right)} $
    \begin{align}
    \Pr_{\seqs' \sim \Wsig(\seqs)}[\distips(\seqs,\seqs') > r] \le p.
    \end{align}
    \item As (a) $\sstar_h$ corresponds to $\pathc$ from $\ctraj_T \sim \Dexp$, (b) as $\pihat,\pidec$ are functions of $\pathm$, and (c) by recalling the definition of $\drob$ in \Cref{defn:imit_gaps}, $\epsilon \le c_2$ ensures
    \begin{align}
    &\Exp_{\sstar_h \sim \Psth}\Exp_{\sstartil_h \sim \Wsig(\sstar_h) } \drob[\epsilon]( \pi_{h}(\sstartil_h) \parallel \pidec(\sstartil_h))\\
    &=  \Exp_{\pathm \sim \Dexp}\Exp_{\pathmtil \sim \cN(\pathm,\sigma^2 \eye)} \inf_{\coup \in \couple(\pidec(\pathmtil),\pihat(\pathmtil))}\Pr_{(\seqa,\seqa') \sim \coup}[\dista(\seqa,\seqa') \ge \epsilon]\\
    &\le  \Exp_{\pathm \sim \Dexp}\Exp_{\pathmtil \sim \cN(\pathm,\sigma^2 \eye)} \inf_{\coup \in \couple(\pidec(\pathmtil),\pihat(\pathmtil))}\Pr_{(\seqa,\seqa') \sim \coup}[\dmax(\seqa,\seqa') \ge \epsilon/c_1]\\
    &=  \Delta_{\sigma,h}(\pihat;\epsilon/c_1),
    \end{align}
    where the last step is by \Cref{lem:Delta_redef}, and where we also used that whenever $\epsilon \le c_2$, we have the $\distA(\seqa,\seqa') \le c_1\epsilon$ if and whenever $\dmax(\seqa,\seqa') \le \epsilon$ (because the former involves only an additional indicator function which returns $0$ for $\dmax(\seqa,\seqa') \le \epsilon \le c_2$).
    \item Finally, \Cref{prop:ips_instant} ensures that under our assumption $\tauc \ge c_3/\eta$, and let $\rips = c_4$, $\gamipsone(u) = c_5 u \exp(-\eta(\tauc - \taum)/\Lstab)$, $\gamipstwo(u) = c_5 u$ for $c_3, c_4, c_5$ given in \Cref{defn:IPS_consts}. Then, for $\dists,\disttvc,\distips$ as above, we have that $\pist$ is $(\gamipsone,\gamipstwo,\distips,\rips)$-IPS.
\end{itemize}
Consequently, for $r = 2\sigma \cdot \sqrt{5 \dimx + 2\log\left( \frac{4\sigma}{\epsilon} \right)} \in (0,\frac{1}{2}\rips)$, \Cref{thm:smooth_cor_decomp} (which, we recall, generalizes \Cref{thm:smooth_cor} to account for the direct decomposition structure) implies
\begin{align}
         \Imitmarg[\epsilon + 2rc_5](\pihat_{\sigma}) &= \Imitmarg[\epsilon + 2rc_5](\pihat_{\sigma} \parallel \pist)  \le \gapmarg[\epsilon + 2rc_5](\pihat_{\sigma} \parallel \pist) \\
         &\leq H\left(\frac{\epsilon}{2\sigma} +  \frac{3}{2\sigma}\sqrt{2\taum - 1}\left(\max\left\{\epsilon,2rc_5e^{-\frac{\eta(\tauc - \taum)}{\Lstab}}\right\}\right)\right)  \\
        &\qquad+  \sum_{h=1}^H\Exp_{\sstar_h \sim \Psth}\Exp_{\sstartil_h \sim \Wsig(\sstar_h) } \drob( \pi_{h}(\sstartil_h) \parallel \pidec(\sstartil_h))\\
        &\leq H\sqrt{2\taum - 1}\left( \frac{2\epsilon}{\sigma} +  6 \sigma c_5\sqrt{5 \dimx + 2\log\left( \frac{4\sigma}{c_1\epsilon} \right)}e^{-\frac{\eta(\tauc - \taum)}{\Lstab}}\right) +  \sum_{h=1}^H\Delta_{\sigma,h}(\pihat;\epsilon/c_1)\\
        &\leq H\sqrt{2\taum - 1}\left( \frac{2\epsilon}{\sigma} +  \sigma \iota(\epsilon)\right) +  \sum_{h=1}^H\Delta_{\sigma,h}(\pihat;\epsilon/c_1).
    \end{align}
    Substituting in $\epsilon_1 =\epsilon + 2rc_5 =  c_1\epsilon + 4c_5\sigma \cdot \sqrt{5 d + 2\log\left( \frac {4\sigma}{\epsilon} \right)} \le \epsilon + \sigma \iota(\epsilon)$, the bound on $\Imitmarg[\epsilon_1]$ is proved.
    

    To show $\Imitfin[\epsilon_2](\pihat_{\sigma})$ satisfies the same bound, we replace $\dists$ in the above argument (as defined in \Cref{sec:control_instant_body}) with $\dists(\cdot,\cdot) \gets \distips(\cdot,\cdot)$, where again we recall that $\distips(\seqs_s,\seqs_s') = \|\bx_{t_h} -\bx_{t_h}'\|$ measures differences in the final associated control state. From \Cref{cor:cor_stability_guarantee_all}, which is a a generalization of \Cref{prop:ips_instant}, it follows that we can replace $\gamipstwo(u) = c_5 u$ as used above with the considerable smaller quantity $\gamipstwo(u) = c_5 ue^{-\frac{\eta\tauc}{\Lstab}}$. Thus, we can replace $\epsilon_1$ above with $\epsilon_2 := \epsilon + 4c_5e^{-\eta \tauc/\Lstab}\sigma \cdot (5 \dimx + 2\log\left( \frac 1 \epsilon \right))^{1/2}$. This concludes the proof that 
    \begin{align}
    \Imitmarg[\epsilon_2](\pihat_{\sigma}) &\le H\sqrt{2\taum - 1}\left( 6c_5\sqrt{5 \dimx + 2\log\left( \tfrac {4\sigma}{\epsilon} \right)} e^{-\frac{\eta(\tauc - \taum)}{\Lstab}} + \frac{2\epsilon}{\sigma}\right),
    \end{align}
    which can be simplified as needed.
\end{proof}
\begin{proof}[Proof of \Cref{thm:main}]
    Adopt the shorthand $\Delta_h = \Delta_{\sigma,h}(\pihat;\epsilon/c_1)$.
    From \Cref{thm:main_template}, it suffices to show that with probability at least $1 - \delta$, it holds that $\Delta_h \leq \frac{c_1\epsilon}{\sigma}$ for all $h \in [H]$.  Note that by \Cref{asm:Jacobian_Stable} it holds $\Dexp$-almost surely that $\norm{\seqa_h} \leq \Rstab$ and thus the condition on $q$ in \Cref{thm:samplingguarantee} holds for $R = \Rstab$.  Moreover, for $d = \tauc(\dimx + \dimu + \dimx\dimu)$, we have that $\seqa \in \rr^d$.  By \Cref{ass:score_realizability}, the conditions on the score class $\scoref_\theta$ hold for us to apply \Cref{thm:samplingguarantee}.  Note that by assumption,
    \begin{align}
        \Nsample \geq c \left( \frac{C_\Theta d R (R \vee \sqrt{d}) \log(dn)}{(\epsilon / \sigma)^4} \right)^{4\nu} \vee \left( \frac{d^6 (R^4 \vee d^2 \log^3\left( \frac{HndR \sigma}{\delta \epsilon} \right))}{(\epsilon / \sigma)^{24}} d^2 \right)^{4\nu},
    \end{align}
    where we note that the right hand side is $\poly\left(C_\Theta,\epsilon/\sigma, \Rstab, d, \log(H/\delta)  \right)^\nu$, and $\dphorizon$ and $\dpstep$ are set as in \eqref{eq:samplingparameters}.  Taking a union bound over $h \in [H]$ and applying \Cref{thm:samplingguarantee} tells us that with probability at least $1- \delta$, for all $h \in [H]$, it holds that
    \begin{align}
        \ee_{\pathm \sim q_{\pathm}}\left[ \inf_{\coup \in \couple(\emph{\ddpm}(\scoref_{\thetahat}, \pathm), q(\cdot | \pathm)) } \pp_{(\widehat \seqa, \seqa^\ast) \sim \coup}\left( \norm{\widehat \seqa - \seqa^*} \geq \epsilon  \right) \right] \leq \epsilon.
    \end{align}
    Thus, using \Cref{lem:Delta_redef}, it holds that with probability at least $1 - \delta$,
    \begin{align}
        \sum_{h = 1}^H \Delta_h \leq \H \epsilon.
    \end{align}
    Plugging this in to \Cref{thm:main_template} concludes the proof.
\end{proof}
\begin{comment}
\begin{theorem}\label{thm:main_app}
    Let $\Wsig$ be the Gaussian smoothing in $\toda$ for $\sigma = \sigaug$.  Suppose that \Cref{asm:traj_regular,asm:Jacobian_Stable} hold as well as (a) there is some $\thetast \in \Theta$ such that for all $h \in [H]$, $\scoref_{\theta^\star,h}$ is the score of $\pist \circ \Wdeconvh$ and (b)  the chunk length satisfies $\tauc \geq c_3 / \eta$ and $\rips = c_4$, where $c_3$ and  $c_4$ are polynomial in problem parameters given in \Cref{defn:IPS_consts}.  For any $\delta \geq \exp(- \rips^2/(4d\sigaug^2))$ and $\epsilon > 0$, if $\Nsample \geq \poly\left( \comp(\Theta), \epsilon^{-1}, d\right)$, then for $\pihat$ the policy output by $\toda$, it holds that
    \begin{align}
        \Imitjoint[\epsilon](\pihat \parallel \pist) \vee \Imitmarg[\epsilon + \sigaug \sqrt{d \log(1/\delta)}](\pihat) \leq 3 c_5 H\left(\delta +  \exp\left( - \eta (\tauc - \taum) / \Lstab \right)\sigaug \sqrt{d \log(1/\delta)} + \epsilon / \sigaug \right).
    \end{align}
    \ab{Finish for final}
  \end{theorem}
  \end{comment}



\subsection{Imitation of the joint trajectory under total variation continuity of demonstrator policy}\label{ssec:end_to_end_demonstrator_tvc}
Here, we show that if the demonstrator policy has (a) bounded memory and (b) satisfies a certain continuity property in total variation distance, then we can imitate the \emph{joint distribution} over trajectories, not just marginals. Recall the joint imitation loss from $\Imitjoint$ from \Cref{def:loss_joint}.

\begin{theorem}\label{thm:joint_under_TVC} Consider the setting \Cref{thm:main_template}, with $\Delta_h(\epsilon)$ as in \Cref{defn:policy_error}, and suppose all the assumptions of that theorem are met. Suppose that, in addition, there is a strictly increasing function $\gamma(\cdot)$ such that for all $\pathm,\pathm' \in \scrP_{\taum -1}$,
\begin{align}
\TV(\pist(\pathm),\pist(\pathm')) \le \gamma(\|\pathm-\pathm'\|),
\end{align}
where $\pist$ is defined is the conditional in \Cref{def:Dexp_policies}. Further, suppose that $\Dexp$ has $\tau \le \taum$ bounded memory (\Cref{defn:bounded_memory}). Then,  with $\epsilon_1  :=  \epsilon + \sigma \iota(\epsilon)$ as in \Cref{thm:main_template},
\begin{align}
&\Imitjoint[\epsilon_1](\pihat_{\sigma}) \le H\cdot\ErrTVC(\sigma, \gamma) \\
&\qquad+ H\sqrt{2\taum -1}\left(\frac{2\epsilon}{\sigma} +  6c_5\sqrt{5 \dimx + 2\log\left( \frac{4\sigma}{\epsilon} \right)}e^{-\frac{\eta(\tauc - \taum)}{\Lstab}}\right) +  \sum_{h=1}^H\Delta_{\sigma,h}(\pihat;\epsilon/c_1),
\end{align}
where we define $d_0 = \taum\dimx + (\taum-1)\dimu$ and $u_0 = {\gamma}(8\sigma \sqrt{d_0\log(9)})$, and
\begin{align}
\ErrTVC(\sigma, \gamma) = \begin{cases}
2c \sigma \sqrt{d_0} & \text{ linear } \gamma(u) = c\cdot u, c > 0\\
 u_0 + \int_{u_0}^{\infty}e^{-\frac{\gamma^{-1}(u)^2}{64\sigma^2}}\rmd u & \text{ general } \gamma(\cdot)
\end{cases}\label{eq:ErrTVC}.
\end{align}
In particular, under \Cref{ass:score_realizability}, if
\begin{align}
    \Nsample \geq c \left( \frac{C_\Theta d R (R \vee \sqrt{d}) \log(dn)}{(\epsilon / \sigma)^4} \right)^{4\nu} \vee \left( \frac{d^6 (R^4 \vee d^2 \log^3\left( \frac{HndR \sigma}{\delta \epsilon} \right))}{(\epsilon / \sigma)^{24}} d^2 \right)^{4\nu},
\end{align}
then with probability at least $1 - \delta$, it holds that
\begin{align}
    &\Imitjoint[\epsilon_1](\pihat_{\sigma}) \le H\cdot\ErrTVC(\sigma, \gamma) + H\sqrt{2\taum -1}\left(\frac{3\epsilon}{\sigma} +  6c_5\sqrt{5 \dimx + 2\log\left( \frac{4\sigma}{\epsilon} \right)}e^{-\frac{\eta(\tauc - \taum)}{\Lstab}}\right).
\end{align}
\end{theorem}
\begin{remark}
The second term in our bound on $\Imitjoint(\pi)$ is identical to the bound in \Cref{thm:main_template}. The term $\ErrTVC$ captures the additional penalty we pay to strengthen for imitation of marginals to imitation of joint distributions. 
Notice that if $\lim_{u\to 0}\gamma(u)\to 0$ and $\gamma(u)$ is sufficiently integrable, then, $\lim_{\sigma \to 0}\Err(\sigma,\gamma) = 0$. This is most clear in the linear $\gamma(\cdot)$ case, where $\Err(\sigma,\gamma) = \BigOh{\sigma}$. 
\end{remark}
The proof is given in \Cref{sec:thm:joint_under_TVC}; it mirrors that of \Cref{thm:main_template}, but replaces \Cref{thm:smooth_cor} with the following imitation guarantee in the composite MDP abstraction of \Cref{sec:analysis}, which bounds the joint imitation gap relative to $\pist$ if $\pist$ is TVC.
\begin{proposition}\label{cor:pist_tvc} Consider the set-up of \Cref{sec:analysis}, and suppose that the assumptions of \Cref{thm:smooth_cor_decomp}, but that, in addition, the expert policy $\pist$ is $\tilde\gamma(\cdot)$-TVC with respect to the pseudometric $\disttvc$, where $\tilde \gamma: \R_{\ge 0} \to \R_{\ge 0}$ is strictly increasing.
Then, for all parameters as in \Cref{thm:smooth_cor}, and any $\tilde{r} > 0$,
\begin{align}
&\gapjoint (\pihat \circ \Wsig \parallel \pist) \le {\color{emphcolor} H\int_{0}^{\infty}\max_{\seqs}\Pr_{\seqs'\sim \Wsig(\seqs)}[\disttvc(\seqs,\seqs') > \tilde\gamma^{-1}(u)/2]\rmd u} \\
&\quad+ H\left( 2p_r +  3\gamma_{\sigma}(\max\{\epsilon,\gamipsone(2r)\})\right)  + \textstyle \sum_{h=1}^H\Exp_{\sstar_h \sim \Psth}\Exp_{\sstartil_h \sim \Wsig(\sstar_h) } \drob( \pihat_{h}(\sstartil_h) \parallel \pidec(\sstartil_h)), 
\end{align}
where the {\color{emphcolor} term in color} on the first line is the only term that differs from the bound in \Cref{thm:smooth_cor}. 

Moreover, in the special case where all of the distributions of $\disttvc(\seqs,\seqs')\mid \seqs' \sim \Wsig(\seqs)$ are stochastically dominated by a common random variable $Z$, and further more $\tilde{\gamma}(u) = \tilde{c}\cdot u$ for some constant $\tilde{c}$, then our bound may be simplified to
\begin{align}
&\gapjoint (\pihat \circ \Wsig \parallel \pist) \le  {\color{emphcolor}2\tilde{c} H\Exp[Z]} \\
&\quad+ H\left( 2p_r +  3\gamma_{\sigma}(\max\{\epsilon,\gamipsone(2r)\})\right)  + \textstyle \sum_{h=1}^H\Exp_{\sstar_h \sim \Psth}\Exp_{\sstartil_h \sim \Wsig(\sstar_h) } \drob( \pihat_{h}(\sstartil_h) \parallel \pidec(\sstartil_h)).
\end{align}
\end{proposition}
\begin{proof}[Proof Sketch]
\Cref{cor:pist_tvc} is derived below in \Cref{sec:cor:pist_tvc}. It is corollary of \Cref{thm:smooth_cor}, combined with adjoining the coupling constructed therein to a TV distance coupling between $\pirep$ (whose joints we \emph{can always} imitate) and $\pist$. Coupling trajectories induced by $\pirep$ and $\pist$ relies on the TVC of $\pist$, as well as concentration of $\Wsig$.
\end{proof}
Using the above proposition, we can derive the following consequences for imitation of the joint distribution.

\subsubsection{Proof of Theorem \ref{thm:joint_under_TVC}}\label{sec:thm:joint_under_TVC}
 The proof is nearly identical to that of \Cref{thm:main_template}, with the modifications that we replace our use of \Cref{thm:smooth_cor} with \Cref{cor:pist_tvc} taking $\tilde\gamma \gets \gamma$. By \Cref{lem:eq_loss_converstions} and the assumpton that $\Dexp$ has $\tau \le \taum$-bounded memory, it suffices to bound the joint-gap in the composite MDP:
 \begin{align}
\Imitjoint(\pi) \le \gapjoint(\pi \parallel \pist).
\end{align}
We bound this directly  from \Cref{cor:pist_tvc}.  The final statement follows from \Cref{thm:samplingguarantee} in the same way that it does in the proof of \Cref{thm:main}.

The only remaining modification, then, is to evaluate the additional additive terms colored in purple in \Cref{cor:pist_tvc}; we will show that $\ErrTVC$ as defined in \eqref{eq:ErrTVC} suffices as an upper bound. We have two cases. In both, let $d_0 = \taum\dimx + (\taum-1)\dimu$. As $\disttvc$ measures the distance between the chunks $\pathm = \phiZ(\seqs_h),\pathmtil = \phiZ(\seqs_h')$, which have dimension $d_0$, and since we $\phiZ \circ \Wsig(\cdot) = \cN(\cdot,\sigma^2 \eye_{d_0})$, we have 
\begin{align}
\disttvc(\phiZ\circ \seqs,\phiZ \circ \seqs') \mid \seqs'\sim \Wsig(\seqs) \overset{\mathrm{dist}}{=} \|\bm{\gamma}\|, \quad \bm{\gamma} \sim \cN(0, \sigma^2 \eye_{d_0}) \label{eq:equal_smooth_dist}
\end{align}
\paragraph{General $\gamma(\cdot)$.} Eq. \eqref{eq:equal_smooth_dist} and \Cref{lem:gaussian_concentration} imply that
\begin{align}
\Pr_{\seqs'\sim \Wsig(\seqs)}[\disttvc(\seqs,\seqs')] \le \exp(-r^2/16\sigma^2), \quad r \ge  4\sigma d_0\log(9).
\end{align}
Hence, if $u_0 = {\gamma}(8\sigma d_0\log(9))$, then 
\begin{align}
\Pr[\disttvc(\seqs,\seqs') > \gamma^{-1}(u)/2] \le \exp(-\gamma^{-1}(u)^2/64\sigma^2), \quad u \ge u_0.
\end{align}
Thus, as probabilities are at most one, 
\begin{align}
\int_{0}^{\infty}\max_{\seqs}\Pr_{\seqs'\sim \Wsig(\seqs)}[\disttvc(\seqs,\seqs') > \gamma^{-1}(u)/2]\rmd u  \le u_0 + \int_{u_0}^{\infty}e^{-\frac{\gamma^{-1}(u)^2}{64\sigma^2}}\rmd u,
\end{align}
as needed.

\paragraph{Linear $\gamma(\cdot)$.} In the special case where $\gamma(u) = c(u)$, \Cref{eq:equal_smooth_dist} implies that we can take $Z = \|\bm{\gamma}\|$ where $\bm{\gamma}\sim \cN(0,\sigma^2 \eye_{d_0})$ in the second part of \Cref{cor:pist_tvc}. The corresponding additive term is then $2Hc\Exp[\|\bm{\gamma}\|]$. By Jensen's inequality, $\Exp[\|\bm{\gamma}\|] \le \sqrt{\Exp[\|\bm{\gamma}\|^2]} = \sqrt{\sigma^2 d_0} = \sigma\sqrt{d_0}$, as needed.
\qed




\subsubsection{Proof of \Cref{cor:pist_tvc}}\label{sec:cor:pist_tvc}
 Define the shorthand 
\begin{align}
B &:= H\left( 2p_r   + 3\gamma_{\sigma}(\max\{\epsilon,\gamipsone(2r)\})\right)  + \textstyle \sum_{h=1}^H\Exp_{\sstar_h \sim \Psth}\Exp_{\sstartil_h \sim \Wsig(\sstar_h) } \drob( \pihat_{h}(\sstartil_h) \parallel \pidec(\sstartil_h)),
\end{align} and recall that \Cref{thm:smooth_cor} ensures $\gapjoint (\pihat \circ \Wsig \parallel \pirep) \le B $. Further, recall from \Cref{defn:imit_gaps} that 
\begin{align}
\gapjoint (\pihat \circ \Wsig \parallel \pirep)  &= \inf_{\coup_1}\Pr_{\coup_1}\left[\max_{h \in [H]}\max\{\dists(\srep_{h+1},\shat_{h+1}),\phia(\arep_h,\seqahat_h)\}  > \epsilon\right], 
\end{align}
where the infinum is over all couplings $\coup_1$ of $(\shat_{1:H+1},\ahat_{1:H}) \sim \Dist_{\pihat \circ \Wsig}$ and $(\srep_{1:H+1},\arep_{1:H}) \sim \Dist_{\pirep}$  with $\Pr_{\coup_{1}}[\shat_1 = \srep_1] = 1$. For any coupling $\coup_1$, we can consider another coupling $\coup_2$ of $(\sstar_{1:H+1},\astar_{1:H}) \sim \Dist_{\pist}$ and $(\srep_{1:H+1},\arep_{1:H}) \sim \Dist_{\pirep}$  with $\Pr_{\coup_{2}}[\sstar_1 = \srep_1] = 1$. By the ``gluing lemma'' (\Cref{lem:couplinggluing}), we can construct a combined coupling $\coup$ which respects the marginals of $\coup_1$ and $\coup_2$. This combined coupling induces a joint coupling $\tilde{\coup}_1$ of $\Dist_{\pihat \circ \Wsig}$ and $\Dist_{\pist}$ which, by a union bound, satisfies $\Pr_{\tilde{\coup}_1}[\shat_1 = \sstar_1] = 1$. 
Thus, by a union bound, we can bound 
\begin{align}
\gapjoint (\pihat \circ \Wsig \parallel \pist)  &\le \Pr_{\tilde \coup_1}\left[\max_{h \in [H]}\max\{\dists(\sstar_{h+1},\shat_{h+1}),\phia(\astar_h,\seqahat_h)\}  > \epsilon\right]\\
&\le \Pr_{ \coup_1}\left[\max_{h \in [H]}\max\{\dists(\srep_{h+1},\shat_{h+1}),\phia(\arep_h,\seqahat_h)\}  > \epsilon\right]\\
&+\Pr_{ \coup_2}\left[(\sstar_{1:H+1},\astar_{1:H}) \ne (\srep_{1:H+1},\arep_{1:H}) \right].
\end{align}
Passing to the infinum over $\mu_1,\mu_2$, 
\begin{align}
\gapjoint (\pihat \circ \Wsig \parallel \pist)  &\le \underbrace{\gapjoint (\pihat \circ \Wsig \parallel \pirep)}_{\le B} +\inf_{\coup_2}\Pr_{ \coup_2}\left[(\sstar_{1:H+1},\astar_{1:H}) \ne (\srep_{1:H+1},\arep_{1:H}) \right],
\end{align}
where again $\coup_2$ quantify couplines of $(\sstar_{1:H+1},\astar_{1:H}) \sim \Dist_{\pist}$ and $(\srep_{1:H+1},\arep_{1:H}) \sim \Dist_{\pirep}$  with $\Pr_{\coup_{2}}[\sstar_1 = \srep_1] = 1$. Bounding the infinum over $\coup_2$ with \Cref{prop:TV_imit}, we have
\begin{align}
\gapjoint (\pihat \circ \Wsig \parallel \pist)  &\le  B + \sum_{h=1}^H \Exp_{\sstar_h}\TV(\pist_h(\sstar_h),\pireph(\sstar_h))
\end{align}
To conclude, it suffices to show the following bound:
\begin{claim}\label{claim:TV_coup} For any $\seqs \in \cS$, $h \in [H]$, and $\tilde{r} \ge 0$, $\TV(\pist_h(\seqs),\pireph(\seqs)) \le \int_{0}^{\infty}\max_{\seqs}\max_{\seqs}\Pr_{\seqs'\sim \Wsig(\seqs)}[\disttvc(\seqs,\seqs') > \tilde\gamma^{-1}(u)/2]$.
\end{claim} 
\begin{proof}
To show this claim, we note that we can represent (via the notation in \Cref{app:smoothcor_proof}) $\pireph(\seqs) = \pist_h \circ \Qreph(\seqs)$, where $\Qreph$ is the replica-kernel defined in \Cref{defn:all_kernels}. Thus, we can construct a coupling of $\astar \sim \pist_h(\seqs)$ and $\arep \sim \pireph(\seqs)$ by introducing an intermediate state $\seqs' \sim \Qreph(\seqs)$ and $\arep \sim \pist(\seqs')$. By \Cref{cor:tv_two}, the fact that $\TV$ distance is bounded by one, and the assumption that $\pist$ is $\tilde{\gamma}$-TVC, we then have
\begin{align}
\TV(\pist_h(\seqs),\pireph(\seqs)) &\le \Exp_{\seqs' \sim \Qreph(\seqs)}\TV(\pist_h(\seqs),\pist_h(\seqs')).
\end{align}
Recall the well-known formula that, for a non-negative random variable $X$, $\Exp[X] = \int_{0}^{\infty}\Pr[X >u] \rmd u$ \citep{durrett2019probability}. From this formula, we find 
\begin{align}
\TV(\pist_h(\seqs),\pireph(\seqs)) &\le \int_{0}^{\infty}\Pr[\TV(\pist_h(\seqs),\pist_h(\seqs')) > u] \rmd u\\
&\overset{(i)}{\le} \int_{0}^{\infty}\Pr[\disttvc(\seqs,\seqs') > \tilde\gamma^{-1}(u)]\rmd u
\end{align}
where in $(i)$ we used  that $\TV(\pist_h(\seqs),\pist_h(\seqs')) \le \tilde \gamma(\disttvc(\seqs,\seqs'))$ and that, as $\tilde \gamma(\cdot)$ is strictly increasing, we have the equality of events $\{\TV(\pist_h(\seqs),\pist_h(\seqs')) > u\} = \{\disttvc(\seqs,\seqs') > \gamma^{-1}(u)\}$. Arguing as in the proof of \Cref{lem:rep_conc}, we have that $\Pr_{\seqs'\sim \Wsig(\seqs)}[\disttvc(\seqs,\seqs') > \tilde\gamma^{-1}(u)] \le \max_{\seqs}\Pr_{\seqs'\sim \Wsig(\seqs)}[\disttvc(\seqs,\seqs') > \tilde\gamma^{-1}(u)/2]$. Hence, we conclude
\begin{align}
\TV(\pist_h(\seqs),\pireph(\seqs)) &\le \int_{0}^{\infty}\max_{\seqs}\Pr_{\seqs'\sim \Wsig(\seqs)}[\disttvc(\seqs,\seqs') > \tilde\gamma^{-1}(u)/2]\rmd u
\end{align}
which proves the first guarantee.
\end{proof}
With the above claim proven, we conclude the proof of the first statement of \Cref{cor:pist_tvc}. For the second statement, we observe that under the stated stochastic domination assumption by $Z$, and if $\tilde \gamma(u) = \tilde{c}\cdot u$, then  $\max_{\seqs}\Pr_{\seqs'\sim \Wsig(\seqs)}[\disttvc(\seqs,\seqs') > \tilde\gamma^{-1}(u)/2] \le \Pr[ Z > \frac{u}{2c}]$. Hence,  by a change of variables $u = \frac{t}{2c}$, 
\begin{align}
\int_{0}^{\infty}\max_{\seqs}\Pr_{\seqs'\sim \Wsig(\seqs)}[\disttvc(\seqs,\seqs') > \tilde\gamma^{-1}(u)/2]\rmd u \le \int_{0}^{\infty}\Pr[ Z > \frac{u}{2c}] = 2c\int_{0}^{\infty}\Pr[ Z > u] = 2c\Exp[Z],
\end{align}
where again we invoke that $Z$ must be nonnegative (to stochastically dominate non-negative random variables), and thus used the expectation formula referenced above.
\qed



\subsection{Imitation in total variation distance}\label{app:imititation_in_tv}
Here, we notice that estimating the score in TV distance fascilliates estimation in the composite MDP, with no smoothing:
\begin{theorem}
For a chunking policy $\pihat$, suppose that there are terms $(\bar \Delta_h)_{1\le h \le H}$ such that
\begin{align}
\bar \Exp_{\pathm \sim \Dexp}\TV(\pist(\pathm),\pihat(\pathm)) \le \bar\Delta_h,
\end{align}
Then, under no additional assumption (not even those in \Cref{sec:results}),  we have
\begin{align}
\Imitfin[\epsilon = 0](\pihat)\le \Imitmarg[\epsilon = 0](\pihat) \le \sum_{h=1}^H \bar \Delta_h
\end{align}
In in addition $\pist$ has $\tau$-bounded memory(\Cref{defn:bounded_memory}) for $\tau \le \taum$, then for $\Imitjoint$ as in \Cref{def:loss_joint}, 
\begin{align}
\Imitjoint[\epsilon = 0](\pihat) \le \sum_{h=1}^H \bar \Delta_h
\end{align} 
\end{theorem}
The above theorem is a direct consequence of the result below in the composite MDP, together with the correct instantiations for control, and \Cref{lem:eq_loss_converstions} to convert $\Imitmarg$ and $\Imitfin$ into $\gapmarg \le \gapjoint$, and $\gapjoint$, respectively. 
\begin{proposition}\label{prop:TV_imit} Consider the composite MDP setting of \Cref{sec:analysis}. Then, there exists a coupling 
\begin{align}
\TV(\Dist_{\pihat},\Dist_{\pist}) \le \sum_{h=1}^H \Exp_{\sstar_h \sim \Psth} \TV(\pist_h(\sstar_h),\pihat_h(\sstar_h))
\end{align}
Thus, there exists a a couple $\coup \in \couple(\Dist_{\pist},\Dist_{\pihat})$ of $(\sstar_{1:H+1},\astar_{1:H}) \sim \Dist_{\pist}$ and $(\shat_{1:H+1},\ahat_{1:H}) \sim \Dist_{\pihat}$ such that $\Pr_{\coup}[(\sstar_{1:H+1},\astar_{1:H}) \ne (\shat_{1:H+1},\ahat_{1:H})]$ is bounded by the right-hand side of the above display. Moreover, this coupling can be constructed such that $\Pr_{\coup}[\sstar_{1} = \shat_1]$.
\end{proposition}
\begin{proof}[Proof of \Cref{prop:TV_imit}] This is a direct consequence of \Cref{lem:tv_telescope}, with $\lawP_1 \gets \Dinit$, and $\lawQ_{h+1}$ corresponding to the kernel for sampling $\astar_h \sim \pist(\sstar_h)$ and incrementing the dynamics $\sstar_{h+1} = F_h(\sstar_h,\astar_h)$, and $\lawQ_h'$ the same for $\ahat_h \sim \pihat(\shat_h)$, and similar incrementing of the dynamics.
\end{proof}

\subsection{Imitiation with no augmentation}\label{app:imititation_in_joint_no_aug}



\begin{theorem}\label{thm:no_augmentation} Let $\pihat$ be a learner policy, and define
 \begin{align}
 \Delta^\star_h(\epsilon) := \Delta_{\sigma = 0,h}(\pihat;\epsilon)
 \end{align}
 be the policy error specialized to no soothing. Suppose that there is a non-decreasing function $\gamma(\cdot)$ such that for all $\pathm,\pathm' \in \scrP_{\taum -1}$
\begin{align}
\TV(\pihat(\pathm),\pihat(\pathm')) \le  \gamma(\|\pathm-\pathm'\|),
\end{align}
where $\pist$ is defined is the conditional in \Cref{def:Dexp_policies}. Then, the loss of $\pihat$, without smoothing, is bounded by
\begin{align}
&\Imitmarg[\epsilon](\pihat) \le H\gamma(\epsilon\sqrt{2\taum - 1}) +  \sum_{h=1}^H\Delta_h^\star(\epsilon),
\end{align}
Further if $\Dexp$ has $\tau \le \taum$ bounded memory (\Cref{defn:bounded_memory}), then it also holds that 
\begin{align}
\Imitjoint[\epsilon](\pihat) \le H\gamma(\epsilon\sqrt{2\taum - 1}) +  \sum_{h=1}^H\Delta_h^\star(\epsilon)
\end{align}
\end{theorem}
\begin{proof} The above is a direct consequence of the following points. First, with our instantition of the composite MDP, we can bound  $\Imitmarg(\pihat) \le \gapmarg(\pihat \parallel \pist) \le \gapjoint(\pihat \parallel \pist)$ due to \Cref{lem:eq_loss_converstions}; and moreover, we have  $\Imitjoint(\pihat) \le \gapjoint(\pihat \parallel \pist)$ when $\Dexp$ has $\tau\le \taum$-bounded memory. 

Next, bounding $\|\pathm-\pathm'\| \le \sqrt{2\taum-1}\disttvc(\pathm,\pathm') $, we see $\pihat$ is $\tilde{\gamma}(\cdot)$-TVC w.r.t. $\disttvc$, where $\tilde \gamma(u) = \gamma(u \sqrt{2\taum -1})$. The bound now follows from \Cref{prop:IS_general}, and the fact that \Cref{prop:ips_instant} verifies the input-stability property.
\end{proof}





\subsection{Consequence for expected costs}\label{ssec:consequences_for expected_costs}
Finally, we prove \Cref{prop:imit_bounds_lipschitz}, which shows that it is sufficient to control the imitation losses in \Cref{def:losses} if we wish to control the difference of a Lipschitz cost function between the learned policy and the expert distribution:

\begin{proposition}\label{prop:imit_bounds_lipschitz}
Recall the marginal and final imitation losses in \Cref{def:losses}, and also the joint imitation loss in \Cref{def:loss_joint}. Consider a cost function $\cost:\Ctraj_T \to \R$ on trajectories $\ctraj_T \in \Ctraj_T$. Finally, let $\ctraj_T \sim \Dexp$, and let $\ctraj_T' \sim \cD_{\pi}$ be under the distribution induced by $\pi$ Then, 
\begin{itemize}
    \item[(a)] If $\max_{\ctraj_T}|\cost(\ctraj_T)| \le B$, and $\ctraj_T$ is $L$ Lipschitz in the Euclidean norm\footnote{Of course, Lipschitznes in other norms can be derived, albeit with different $T$ dependence} (treating $\ctraj_T$ as  Euclidean vector in $\R^{(T+1)\dimx + T\dimu}$), then
    \begin{align}
    |\Exp_{\Dexp}[\cost(\ctraj_T)] - \Exp_{\cD_{\pi}}[\cost(\ctraj_T')]| \le \sqrt{2T}L\epsilon + 2B \Imitjoint(\pi).
    \end{align}
    \item[(b)] If $\cost$ decomposes into a sum of of costs, $\cost(\ctraj) = \costinstantone[T+1](\bx_{1+T}) + \sum_{t = 1}^{T} \costinstantone(\bx_t) + \costinstanttwo(\bu_t)$,
  where $\costinstantone(\cdot),\costinstanttwo(\cdot)$ are $L$-Lipschitz and bounded in magnitude in $B$. Then, 
   \begin{align} 
   |\Exp_{\Dexp}[\cost(\ctraj_T)] - \Exp_{\cD_{\pi}}[\cost(\ctraj_T')]| \leq  4T B\Imitmarg[\epsilon](\pi) + 2TL \epsilon.
  \end{align} 
  \item[(c)] $\cost(\ctraj) = \costinstantone[T+1](\bx_{T+1})$ depends only on $\bx_{T+1}$, then 
\begin{align}
|\Exp_{\Dexp}[\cost(\ctraj_T)] - \Exp_{\cD_{\pi}}[\cost(\ctraj_T')]| \le  + 2B\Imitfin[\epsilon](\pi) + L\epsilon
\end{align}
\end{itemize}
\end{proposition}
Thus, for our imitation guarantees to apply to most natural cost functions used in practice, it suffices to control the imitation losses defined above.  
\begin{proof}[Proof of \Cref{prop:imit_bounds_lipschitz}]  
Let $\ctraj_T = (\bx_{1:T+1},\bu_{1:T})\sim \Dexp$, and let $\ctraj_T' = (\bx_{1:T+1}',\bu_{1:T}')$ be under the distribution induced by $\pi$.    
\paragraph{Part (a).} For any coupling $\coup$ between the two under which $\bx_1 = \bx_1'$, and let $\cE_{\epsilon} := \{\max_{t}\|\bx_{t+1}-\bx_{t+1}'\| \vee \|\bu_t - \bu_t'\| \le \epsilon\}$. 
\begin{align}
|\Exp[\cost(\ctraj_T)] - \Exp[\cost(\ctraj_T')]| &= |\Exp_{\coup}[\cost(\ctraj_T)-\cost(\ctraj_T')]|\\
&\le \Exp_{\coup}[|\cost(\ctraj_T)-\cost(\ctraj_T')|]\\
&\le 2B \Pr_{\coup}[ \cE_{\epsilon}^c] + \Exp_{\coup}[|\cost(\ctraj_T)-\cost(\ctraj_T')|\I\{\cE_{\epsilon}\}]
\end{align}
By passing to an infinum over couplings, $\inf_{\coup} \Pr_{\coup}[ \cE_{\epsilon}^c]  \le \Imitjoint(\pi)$. Moreover, we observe that under $\coup$, $\bx_1 = \bx_1'$, and the remaining coordinates, $(\bx_{2:T+1},\bu_{1:T})$ and $ (\bx_{2:T+1}',\bu_{1:T}')$ are the concatentation of $2T$ vectors, so the Euclidean norm of the concatenations $\|\ctraj_T - \ctraj_T'\|$ is at most $\sqrt{2T}\max_{t}\|\bx_{t+1}-\bx_{t+1}'\| \vee \|\bu_t - \bu_t'\|$, which on $\cE_{\epsilon}$ is at most $\sqrt{2T}\epsilon$. Using Lipschitz-ness of $\cost$ concludes.

\paragraph{Part (b)} Using the adaptive discomposition of the cost and the fact that $\bx_1$ and  $\bx_1'$ have the same distributions,
\begin{align}
|\Exp[\cost(\ctraj_T)] - \Exp[\cost(\ctraj_T')]| &= |\sum_{t=1}^T (\Exp[\costinstantone(\bx_{t+1})] -\Exp[\costinstantone(\bx_{t+1}'))  + (\Exp[\costinstanttwo(\bu_t)] -\Exp[\costinstanttwo(\bu_1'))|\\
&\le \sum_{t=1}^T |\Exp[\costinstantone(\bx_{t+1})] -\Exp[\costinstantone(\bx_{t+1}')|  + |\Exp[\costinstanttwo(\bu_t)] -\Exp[\costinstanttwo(\bu_1')|
\end{align}
Applying similar arguments as in part (a) to each term, we can bound
\begin{align}
\max\left\{|\Exp[\costinstantone(\bx_{t+1})] -\Exp[\costinstantone(\bx_{t+1}')|, |\Exp[\costinstanttwo(\bu_t)] -\Exp[\costinstanttwo(\bu_1')|\right\} \le 2B\Imitmarg(\pi) + L \epsilon. 
\end{align}
Summing over the $2T$ terms concludes. 

\paragraph{Part (c).} Follows similar to part (b).
\end{proof}


\subsection{Useful Lemmata}\label{ssec:end_to_end_lemmata}
\subsubsection{On the trajectories induced by $\pist$ from $\Dexp$}\label{sec:policies_Dexp}


The key step in all of our proofs is to relate the expert distribution over trajectories $\ctraj_T \sim \Dexp$ to the distribution induced by the chunking policy $\pist$ in \Cref{def:Dexp_policies}



\begin{lemma}\label{lem:pistar_existence}
There exists a sequence of probability kernels $\pist_h$ mapping $\pathm \to \laws(\cA)$ such that the chunking policy $\pist = (\pist_h)_{1 \le h \le H}$ satisfies the following:
\begin{itemize}
    \item[(a)]  $\pist_h(\pathm)$ is equal to the almost-sure conditional probability of $\seqa_h$ conditioned on $\pathm$ under $\ctraj_T \sim \Dexp$ and $\seqa_{1:H} = \synth(\ctraj_T)$.
    \item[(b)] The marginal distribution over each $\pathc$ is the same as the marginals of each $\bx_t$ and $\bu_t$ under $\ctraj_T \sim \Dexp$.
    \item[(c)] If $\Dexp$ has $\tau$-bounded memory (\Cref{defn:bounded_memory}) and if $\tau \le \taum$, then the joint distribution of $\ctraj_T$ induced by $\pist$ is equal to the joint distribution over $\ctraj_T$ under $\Dexp$.
\end{itemize}
\end{lemma}
\begin{remark}[Replacing $\tau$-bounded memory with mixing]\label{rem:mixing_time} We can replace that $\tau$-bounded memory condition to the following mixing assumption. Define the chunk $\ctraj_{i \le j} = (\bx_{i:j},\bu_{i:j-1})$. 
Define the measures
\begin{align}
\lawQ_{h}(\pathm) &= \Pr_{\seqa_{1:h-1},\ctraj_{1 : t_h - \taum-1},\seqa_{h:H},\ctraj_{t_h:T+1} \mid \pathm}\\
\lawQ_{h}^{\otimes}(\pathm) &= \Pr_{\seqa_{1:h-1},\ctraj_{1 : t_h - \taum-1} \mid \pathm} \otimes \Pr_{\seqa_{h:H},\ctraj_{t_h:T+1} \mid \pathm}.
\end{align}
which describes the conditional distribution of the whole trajectory without $\pathm$ and the product-distribution of the conditional distributions of the before-$\pathm$ part of the trajectory, and after $\pathm$-part. Under the condition
\begin{align}
&\Exp_{\pathm \text{ from } \ctraj_T \sim \Dexp}\TV\left(\lawQ_{h}(\pathm),\lawQ_{h}^\otimes(\pathm)\right) \le \epsilon_{\mathrm{mix}}(\taum),
\end{align}
which measures how close the before- and after-$\pathm$ parts of the trajectory are to being conditionally independent, one can leverage \Cref{lem:tv_telescope} to show that 
\begin{align}
\TV(\cD_{\pist},\Dexp) \le H\epsilon_{\mathrm{mix}}(\taum)
\end{align}
\Cref{lem:pistar_existence} corresponds to the special when when $\epsilon_{\mathrm{mix}} = 0$.


\end{remark}
\begin{proof}[Proof of \Cref{lem:pistar_existence}] We prove each part in sequence 

\paragraph{Part (a).} follows from the fact that all random variables are in real vector spaces, and thus Polish spaces. Hence, we can invoke the existence of regular conditional probabilities by \Cref{thm:durrett}. 

\paragraph{Part (b).} This follows by marginalization and Markovianity of the dynamics. Specifically, let $(\ctraj_T^{\star},\seqa_{1:H}^{\star}$ be a trajectory and composite actions induced by the chunking policy $\pist$, and let $(\ctraj_T,\seqa_{1:H})$ be the same induced by $\Dexp$. Let $\pathm^{\star}$ denote memory chunks of $\ctraj_T^{\star}$, and let $\pathm$ memory chunks of $\ctraj_T$ (length $\taum-1$); similarly, denote by $\pathc^{\star}$ and $\pathc$ the respective trajectory chunks (length $\tauc \ge \taum$).  

We argue inductively that the trajectory chunks $\pathc^{\star}$ and $\pathc$ are identically distribued for each $h$. For $h = 1$, $\pathc[1]^{\star}$ and $\pathc[1]$ are identically distributed according to $\Dxone$. Now assume we have show that $\pathc^{\star}$ and $\pathc$ are identically distributed. As memory chunks are sub-chunks of trajectory chunks, this means that $\pathm^{\star}$ and $\pathm$ are identically distributed. By part (a), it follows that $(\pathm^{\star},\seqa^{\star}_h)$ and $(\pathm,\seqa_h)$ are identically distributed. In particular, $(\bx_{t_h}^{\star},\seqa^\star_h)$ and $(\bx_{t_h},\seqa_h)$ are identically distributed, where $\bx_{t_h}^\star$ (resp $\bx_{t_h}$) these denote the $t_h$-th control state under $\pist$ (resp. $\Dexp$). By Markovianity of the dynamics, $\pathc[h+1]^{\star}$ and $\pathc[h+1]$ are functions of $(\bx_{t_h}^\star,\seqa_h^\star)$  and $(\bx_{t_h},\seqa_h)$, respectively, $\pathc[h+1]^{\star}$ and $\pathc[h+1]$ are identically distributed, as needed. 

\paragraph{Part (c).} When $\Dexp$ has $\tau$-bounded memory and $\tau \le \taum$, then we have the almost-sure equality
\begin{align}
\Pr_{\Dexp}[\seqa_h \in \cdot \mid \bx_{1:t_h},\bu_{1:t_h}] = \Pr_{\Dexp}[\seqa_h \in \cdot \mid \pathm] = \pist_h(\pathm)[\seqa_h \in \cdot].
\end{align}
Finally, $\bx_{t_{h}+1:t_{h+1}},\bu_{t_{h}:t_{h+1}-1}$ are determined by $\bx_{t_h}$ and $\seqa_h$, this inductively establishes equality of the joint-trajectory distributions. 

\end{proof}


\subsubsection{Concentration and TVC of Gaussian Smoothing.} We now include two easy lemmata necessary for the proof.  The first shows that $p_r$ is small when $r$ is $\Theta(\sigma)$ by elementary Gaussian concentration:
  \begin{lemma}\label{lem:gaussian_concentration}
  Suppose that $\bgamma \sim \cN(0, \sigma^2 \eye)$ is a centred Gaussian vector with covariance $\sigma^2 \eye$ in $\rr^d$ for some $\sigma > 0$.  Then for all $p > 0$, it holds with probability at least $1 - p$ that
  \begin{align}
    \norm{\bgamma} \leq 2\sigma \cdot \sqrt{2 d\log(9)  + 2\log\left( \frac 1 p \right)} \le 2\sigma \cdot \sqrt{5 d + 2\log\left( \frac 1p \right)} 
  \end{align}
  Moreover, for $r \ge 4\sigma \sqrt{d\log(9)}$, $\Pr[ \norm{\bgamma}] \ge r] \le \exp(-r^2/16\sigma^2)$. 

\end{lemma}
\begin{proof}
  We apply the standard covering based argument as in, e.g., \citet[Section 4.2]{vershynin2018high}.  Note that
  \begin{align}
    \norm{\bgamma} = \sup_{\bw \in \cS^{d-1}} \inprod{\bgamma}{\bw},
  \end{align}
  where $\cS^{d-1}$ is the unit sphere in $\rr^d$.  Let $\cU$ denote a minimal $(1/4)$-net on $\cS^{d-1}$ and observe that a simple computation tells us that
  \begin{align}
    \sup_{\bw \in \cS^{d-1}} \inprod{\bgamma}{\bw} \leq 2 \cdot \max_{\bw \in \cU} \inprod{\bw}{\bgamma}.
  \end{align}
  A classical volume argument (see for example, \citet[Section 4.2]{vershynin2018high}) tells us that $\abs{\cU} \leq 9^d$.  A classical Gaussian tail bound tells us that for any $\bw \in \cS^{d-1}$, it holds that for any $r > 0$,
  \begin{align}
    \pp\left(\inprod{\bw}{\bgamma} > r  \right) \leq e^{-\frac{r^2}{2 \sigma^2}}.
  \end{align}
  Thus by a union bound, we have
  \begin{align}
    \pp\left( \norm{\bgamma} > r \right) &\leq \abs{\cU} \cdot \max_{\bw \in \cU} \pp\left( \norm{\bgamma} > \frac{r}{2} \right) \leq 9^d \cdot e^{- \frac{r^2}{8 \sigma^2}}.
  \end{align}
Inverting concludes the proof.


\end{proof}

The second lemma shows that the relevant smoothing kernel is TVC:
\begin{lemma}\label{lem:gaussian_tvc} 
    For any $\sigma > 0$, let $\phiZ$ and $\Wsig$ be as in \Cref{defn:smoothing_instantiation} kernel, then $\Wsig$ is $\gamtvc$-TVC for with respect to $\disttvc$ (as defined in \Cref{sec:control_instant_body})
    \begin{align}
        \gamtvc(u) = \frac{u\sqrt{2\taum - 1}}{2 \sigma}.
    \end{align}
\end{lemma}
\begin{proof}
    Recall that $\phiZ$ denotes projection onto the $\cZ$-component of the direct decomposition in \Cref{defn:direct_decomp}, i.e. projects onto the memory chunk $\pathm$.   
    We apply Pinsker's inequality \citep{polyanskiy2022}: Then, for for $\seqs, \seqs' \in \rr^p$, we have 
    \begin{align}
    \tv\left( \phiZ \circ \Wsig(\seqs), \phiZ \circ \Wsig(\seqs') \right) \leq \sqrt{\frac 12 \cdot \dkl{\phiZ \circ \Wsig(\seqs)}{\phiZ \circ \Wsig(\seqs')}}.
    \end{align} Note that for $\seqs = \pathc$ with corresponding memory chunk $\pathm$ $\phiZ \circ \Wsig(\seqs) \sim \cN(\pathm,\sigma^2 \eye)$. Similarly, for $\pathm'$ corresponding to $\seqs'$, $\phiZ \circ \Wsig(\seqs') \sim \cN(\pathm',\sigma^2 \eye)$. Hence, 
    \begin{align}\dkl{\phiZ \circ \Wsig(\seqs)}{\phiZ \circ \Wsig(\seqs')} \le \frac{\norm{\pathm-\pathm'}^2}{2 \sigma^2}.
    \end{align} Thus, we conclude $\tv\left( \phiZ \circ \Wsig(\seqs), \phiZ \circ \Wsig(\seqs') \right) \le  \frac{\norm{\pathm-\pathm'}}{2 \sigma}$. Finally, we upper bound the Euclidean norm $\norm{\pathm-\pathm'}$ of vectors consistening of $2\taum - 1$ sub-vectors via $\disttvc$ (which is the maximum Euclidean norm of these subvectors) via $\norm{\pathm-\pathm'} \le \sqrt{2\taum - 1}\disttvc(\seqs,\seqs')$.
\end{proof}



\subsubsection{Total Variation Telescoping}
\begin{lemma}[Total Variation Telescoping]\label{lem:tv_telescope} Let $\cY_{1},\dots,\cY_H,\cY_{H+1}$ be Polish spaces. Let $\lawP_1 \in \laws(\cY_1)$, and let $\lawQ_h,\lawQ_h' \in \laws(\cY_h \mid \cX,\cY_{1:h-1})$, $h > 1$. Define $\lawP_1' = \lawP_1$, and recursively define
\begin{align}
\lawP_h = \lawof{\lawQ_h}{\lawP_{h-1}}, \quad \lawP_h' = \lawof{\lawQ_h'}{\lawP_{h-1}'}, \quad h > 1.
\end{align}
Then,
\begin{align}
\TV(\lawP_{H+1},\lawP_{H+1}') \le \sum_{h=1}^H \Exp_{Y_{1:h} \sim \lawP_h}\TV(\lawQ_{h+1}(\cdot \mid Y_{1:h}), \lawQ'_{h+1}(\cdot \mid Y_{1:h}))
\end{align}
Moreover, there exists a coupling of $\coup \in \couple(\lawP_{H+1},\lawP_{H+1}')$ over $Y_{1:H+1} \sim \lawP_{H+1}$ and $Y_{1:H+1}] \sim \lawP_{H+1}'$ such that
\begin{align}
\Pr_{\coup}[Y_1 = Y_1'] = 1, \quad \Pr_{\coup}[Y_{1:H+1}\ne  Y_{1:H+1}'] \le \sum_{h=1}^H \Exp_{Y_{1:h} \sim \lawP_h}\TV(\lawQ_{h+1}(\cdot \mid Y_{1:h}), \lawQ'_{h+1}(\cdot \mid Y_{1:h})).
\end{align}
\end{lemma}
\begin{proof} To prove the first part of the lemma, define $\lawQ_{i,j}'$ for $2 \le i \le j \le H+1$ by $\lawQ_{i,i}' = \lawQ_i$ define $\lawQ_{i,j}'$ by appending $\lawQ_{i,j}'$ to $\lawQ_{i,j-1}'$. and $\lawof{\lawQ_{i,j}'}{(\cdot)} = \lawof{\lawQ_{j}'}{\lawof{\lawQ_{i,j-1}}{(\cdot)}'}$. We now define
\begin{align}
\lawP^{(i)} = \lawof{Q_{i+1,H+1}'}{\lawP_{i}},
\end{align}
with the convenction $\lawof{Q_{H+2,H+1}'}{\lawP_{H+1}} = \lawP_{H+1}$.
Note that $\lawP^{(H+1)} = \lawP_{H+1}$, and $\lawP^{(1)} = \lawP_{H+1}'$.
Then, because TV distance is a metric,
\begin{align}
\TV(\lawP_{H+1},\lawP_{H+1}') &\le \sum_{h=1}^H \TV(\lawP^{(i)},\lawP^{(i+1)})
\end{align}
Moreover, we can write $\lawP^{(i)} =  \lawof{Q_{i+2,H+1}'}{\lawof{\lawQ_{i+1}'}{\lawP_{i}}}$ and $P_{i+1} = \lawof{\lawQ_{i+1}}{\lawP_i}$. Thus, 
\begin{align}
\TV(\lawP^{(i)},\lawP^{(i+1)}) &= \TV(\lawof{Q_{i+2,H+1}'}{\lawof{\lawQ_{i+1}'}{\lawP_{i}}}, \lawof{Q_{i+2,H+1}'}{\lawof{\lawQ_{i+1}}{\lawP_{i}}} \tag{\Cref{cor:tv_two}}\\
&=\TV({\lawof{\lawQ_{i+1}'}{\lawP_{i}}}, {\lawof{\lawQ_{i+1}}{\lawP_{i}}}\\
&=\Exp_{Y_{1:i} \sim \lawP_i} \TV(\lawQ'_i(Y_{1:i}),\lawQ_i(Y_{1:i})).\tag{\Cref{cor:first_TV}}
\end{align}
This completes the first part of the demonstration (noting symmetry of $\TV$). The second part follows from \Cref{cor:first_TV}, by letting $Y \gets Y_1$, and $X \gets Y_{2:H+1}$ in that lemma.
\end{proof}



