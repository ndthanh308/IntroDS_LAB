%!TEX root = ../main.tex
\section{Introduction}\label{sec:intro}

%When training an agent to perform a given task, there have been two successful paradigms in recent literature: in the first, leveraging modern Reinforcement Learning techniques, the agent actively explores the enviroment in an attempt to optimize a known reward function \ab{citations}, while in the second, the agent attempts to learn a good policy from a dataset consisting of expert examples.  While the first approach has seen recent empirical success in a number of domains \ab{citations}, 

%\mscomment{We use \emph{multi-modal} to distinguish from (even stochastic policies) which provide demonstrations that all closely track one common  trajectory. Explain why hard to move past this}


Training dynamic agents from datasets of expert examples, known as 
\emph{imitation learning}, promises to take advantage of the plentiful demonstrations available in the modern data environment, in an analogous manner to the recent successes of language models conducting unsupervised learning on enormous corpora of text \citep{thoppilan2022lamda,vaswani2017attention}.  Imitation learning is especially exciting in robotics, where mass stores of pre-recorded demonstrations on Youtube \citep{abu2016youtube} or cheaply collected simulated trajectories \citep{mandlekar2021matters, dasari2019robonet} can be converted into learned robotic policies.

What makes imitation learning more challenging than supervised learning is the problem of compounding errors, which may bring the agent to regions of state space not seen during training. Without the strong requirement that the learner can interactively query the expert at new states \citep{laskey2017dart,ross2010efficient}, it is well understood that ensuring some form of \emph{stability} in the imitation learning procedure is indeispensable \citep{tu2022sample, havens2021imitation,pfrommer2022tasil}. 
%The latter often requires other forms of stronger access to the expert policy, e.g. by querying gradients \cite{pfrommer2022tasil}. 
While many  natural notions of stability exist for simple behaviors, how to enforce stability when imitating more complex behaviors remains an open question. Multi-modal trajectories present a key example: consider a robot navigating around an obstacle; because there is no difference between navigating around the object to the right and around to the left, the dataset of expert trajectories may include examples of both options.  This bifurcation of good trajectories can make it difficult for the agent to effectively choose which direction to go, possibly even causing the robot to oscillate between directions and run into the object instead of going around it \citep{chi2023diffusion}. Moreover, human demonstrators correlate current actions with the past in order to \emph{commit} to either a right or left path, which makes even formulating the idea of an ``expert \emph{policy}'' a conceptually challenging one. Lastly, we note that bifurcations are necessarily \emph{incompatible} with previous notions of stability derived from classical control theory \cite{tu2022sample,havens2021imitation,pfrommer2022tasil}.


%demonstrator policies correlate with past actions in sophisticated ways. 
%This simple example demonstrates both that the optimal policy is often stochastic, multi-modal and non-Markovian. 

% and that imitation learning in nonsmooth or discontinuous environments can be challenging.



\newcommand{\otae}{\texttt{OTAE}}
\newcommand{\otee}{\texttt{OTEE}}


%Many candidate definitions of stability have been studied by control theorists over the past decades, but none quite transfer to the sorts of hard bifurcations given in the example above \mscomment{..}

\nipspar{Contributions. } 
This paper develops a theory of imitation learning flexible enough to imitate non-Markovian, multi-modal, and/or bifurcated demonstrations in smooth, nonlinear control systems.  As in previous work, we formalize imitation learning in two stages: at \emph{train-time}, we learn a map from observations to distributions over actions, supervised by (state, action)-pairs from expert demonstrations, while at \emph{test-time},  the learned map, or \emph{policy}, is executed on random initial states (distributed identically to initial training states).   Following the schematic of existing guarantees for imitation learning \cite{tu2022sample,pfrommer2022tasil,havens2021imitation}, we demonstrate that a policy trained by minimizing a certain supervised learning objective on expert demonstrations induces trajectories that approximate those of expert demonstrations. Unlike past works, however:
\begin{itemize}
    \item We imitate stochastic, possibly non-Markovian demonstrators. Due to stochasticity, we guarantee tracking in an appropriate optimal transport metric; due to non-Markovianity of the expert, we only guarantee matching the per-time-step \emph{marginal distributions} of the expert trajectories; still, joint distributions over trajectories can be approximated under stronger assumptions.
    \item Without Markovianity, we  reason about the conditional distribution of expert actions given recent states, but \emph{marginalized over past states.} This {conditional action distribution} can be multi-modal and exhbit bifurcations; in particular, the conditional distribution of an expert actions given the state can be discontinuous (in any natural distance metric) as a function of state.
    \item Because of the generality of possible experts, our learner's policy is trained as a \emph{generative model} of the conditional distribution of actions; it is not, as in past work, trained by $\ell_2$ regression from expert states to actions, nor via say, a logistic loss on a discrete action distribution.
    \item  We  require neither interactive data collection (e.g. \textsc{Dagger} \citep{ross2010efficient,laskey2017dart}), nor access to gradients of the expert policy (as in \textsc{TaSil}\cite{pfrommer2022tasil}). Instead, we replace these assumption with an oracle, described below,  which  synthesizes stabilizing linear gains along training demonstrations. We also apply a subtle-yet-significant modification to a popular \emph{data noising} strategy, which we show yields both theoretical and empirical beneifts.
\end{itemize}
Our main result is a reduction from imitation of complex expert trajectories to supervised generative learning of a specific conditional distribution. For concreteness, \Cref{thm:main} instantiates the generative modeling with Denoising Diffusion Probabilistic Models (DDPMs) of sufficient regularity and expressivity (as investigated empirically in \cite{chi2023diffusion,pearce2023imitating,hansen2023idql}), and establishes end-to-end guarantees for imitation of complex trajectories with sample complexity polynomial in relevant problem parameters. Our analysis framework exposes that any sufficient powerful generative learner obtains similar guarantees. Finally, we empirically validate the benefits of our proposed smoothing strategy in simulated robotic manipulation tasks. 
We now summarize the algorithmic choices and analytic ideas that  facilitate our reduction. 
    

\paragraph{A hierarchical approach. } As mentioned above, the key challenge is ensuring an appropriate notion of stability when imitating complex behaviors. We propose a hierarchical approach, both as an analysis tool and algorithmic design principle for imitation learning. During training, we consider learners that synthesize sequences of \emph{primitive controllers} - time-varying affine control policies which  locally stabilize around each demonstration trajectory. The primitive controllers can arise either from hand-coded linear feedback controllers (e.g. as in robotic position control), or, given access to a differentiable dynamics simulator, one can solve time-varying linear Riccati equation along the Jacobian linearizations of given expert demonstrations \cite{jacobson1970differential}. We break these \{demonstrator trajectory, primitive controller\} pairs into sub-trajectories we call ``chunks.''  Building on  \citep{chi2023diffusion}, we use DDPMs to estimate the conditional distribution of primitive controller chunks conditioned on recent states from the previous chunk. 

%We prove that the learner can approximate the expert's trajectory distribution provided three conditions hold:  along each expert trajectory,  (a) the dynamics are sufficiently smooth; (b) one  can synthesize primitive controllers that stabilize the Jacobian-linearized dynamics; and (c) one can approximately sample from conditional distributions over sequences of primitive controllers.  

\paragraph{A novel data noise-injection strategy.} During training, we adopt a popular noise-injection technique that corrupts trajectories (but not supervising actions) with a small amount of Gaussian noise \citep{ke2021grasping,laskey2017dart,ross2011reduction}. Unlike prior work, we propose adding noise \emph{back into the policies at inference time}, a technique which is both both provably indispensable in our analysis, and which our simulations suggest yields considerable benefit over the conventional approach of noising \emph{without} adding noise at inference time. 

\paragraph{Analysis framework, and the ``replica policy''.}  Our analysis reformulates our setting as imitation in a composite MDP, where composite states $\seqs_h$ corresponds to trajectory chunks, and composite-actions $\seqa_h$ correspond to subsequences of primitive controllers. A learner's policy maps composite-states to distributions over composite-actions, and a marginalization trick lets us represent non-Markovian demonstrator trajectories in the same format. The primitive controller sequences $\seqa_h$ provide the requisite stability, and we show that noising the learner policy at inference time ensures  continuity in the total variation distance (TVC). Whereas \emph{TVC} relates actions selected at deviating states to actions selected to states visited by the expert, appropriate definitions of \emph{stability} of the composite actions ensures errors do not compound excessively. The last key ingredient in the analysis: we show that by training with noised states, and crucially, \emph{adding that same noise distribution back at inference time}, our supervised training converges to a ``replica'' version of the expert policy which is reminiscient of \emph{replica pairs} in statistical physics  \citep{mezard2009information}. We argue that,  up to the stability of controllers, this replica policy enjoys per-time-step marginals over states and actions close to those of the expert policy, thereby (a) enjoying the TVC properties endowed by data smoothing (b) avoiding potential distribution shift induced by the standard data smoothing approach. The proof of this fact constructs a sophisticated coupling between the learned policy, replica policy, and other interpolating sequences; this construction is enabled by subtle measure-theoretic arguments demonstrating consistency of our couplings. We also establish stability guarantees for sequences of primitive controllers in non-linear control systems, which may be of independent interest. 
 
 

%\mscomment{replica?} Surprisingly, we find that augmenting training trajectories with Gaussian perturbations, and applying the same perturbation to the learner policy at test time \emph{essentially removes distribution shift.} The proof constructs a sophisticated couplings between two surrogates for the expert policiy (a teleporting and replica sequence), and the learners policy. \mscomment{mention some measure theoretic theoretic stuff on couplings nad condtionals. relies measurable selection of optimal transport couplings for continuous, which can be extended to lower-semicontinous costs (i.e. the cost which induces the total variation distance) via limiting arguments. }
\iftoggle{arxiv}
{
\subsection{Related Work}
    \input{body/full_related}
}
{
\nipspar{Abridged Related Work.} Due to space, we defer a full comparison to past work to \Cref{app:related}. 
 DDPMs, proposed in \cite{ho2020denoising,sohl2015deep}, along with their relatives have seen success in image generation \cite{song2019generative,ramesh2022hierarchical}, along with imitation learning (without data augmentation) \cite{janner2022planning,chi2023diffusion,pearce2023imitating}, which is the starting point of our work. Smoothing data augmentation is ubiquitous in modern imitation learning \cite{laskey2017dart} and our approach corresponds to that of \cite{ke2021grasping} but with noise added at inference time. Despite the benefits of adaptive data collection \cite{ross2011reduction,laskey2017dart}, adaptive demonstrations are more expensive to collect. Previous analyses of imitation learning without adaptive data collection have focused on classical control-theoretic notions of stability, notably incremental stability, \citep{tu2022sample,havens2021imitation,pfrommer2022tasil}, which require continuity, Markovianity, and often determinism, and preclude the bifurcations permitted in our setting.
 }
 {}

\iftoggle{arxiv}
{\subsection{Organization}}
{\nipspar{Organization.}} In \Cref{sec:setting} we formally introduce our setting as well as some preliminary notation and our main desideratum.  We then state our assumptions and our proposed algorithm, $\toda$ before giving our main guarantee (\Cref{thm:main}) in \Cref{sec:results}.  In \Cref{sec:analysis} we describe our proof techniques and provide a high level overview before concluding with some experiments in \Cref{sec:experiments}. The organization of our many appendices is given in \Cref{app:notation_and_org}.





