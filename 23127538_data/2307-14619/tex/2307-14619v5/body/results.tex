%!TEX root = ../neurips_main.tex

\newcommand{\Phicl}[1]{\bm{\Phi}_{\mathrm{cl},#1}}
\newcommand{\Cnu}{\nu}
\newcommand{\Ctheta}{C_{\Theta}}

\newcommand{\augmentflag}{\bm{\mathtt{augment}}}
\newcommand{\Nsample}{N_{\texttt{exp}}}
\newcommand{\Naug}{N_{\texttt{aug}}}

\newcommand{\trueflag}{\texttt{true}}
\newcommand{\falseflag}{\texttt{false}}
\newcommand{\sigaug}{\sigma}


\newcommand{\Dexpbar}{\bar{\cD}_{\mathrm{exp}}}

\section{Conditional sampling with stabilization suffices for behavior cloning}\label{sec:results}
\newcommand{\tiss}[1][T]{\mathsf{t}\text{-}\textsc{Iss}}
\newcommand{\dmax}{\dist_{\max}}
\newcommand{\couphatsigh}{\couple_{\sigma,h}(\pihat)}

\newcommand{\delu}{\updelta \bu}
\newcommand{\delx}{\updelta \bx}


We show that trajectories of the form given in \Cref{def:expert} can be efficiently imitated if (a) we are given a \emph{synthesis oracle}, described below, that produces low-level control policies that locally stabilize chunks of the trajectory with primitive controllers and (b) we can learn to generate certain appropriate distributions over composite actions, i.e. sequences of primitive controllers.   All the following results apply to affine primitive controllers introduced in \Cref{sec:setting} and assume that the system dynamics are second-order smooth and locally stabilizable. In \Cref{app:gen_controllers}, we show that our results still hold with general families of parametric primitive controllers, provided that these controllers induce the same local stability guarantee.


\nipspar{The synthesis oracle.} We say { primitive controller (cf. \Cref{sec:setting}) $\sfk_{1:T} \in \cK^T$ is  \emph{consistent with} a trajectory $\ctraj = (\bx_{1:T+1},\bu_{1:T})\in \Ctraj_T$  if $\bbarx_t = \bx_t$ and $\bbaru_t = \bu_t$ for all $t \in [T]$; note that this implies that $\sfk_t(\bx_t)=\bu_t$ for all $t$.  A \bfemph{synthesis oracle} $\synth$ maps $\Ctraj_T \to \cK^T$ such that, for all $\ctraj_T \in \Ctraj_T$, $\sfk_{1:T} = \synth(\ctraj_T)$ is consistent with $\ctraj_T$. For our theory, we assume access to a {synthesis oracle} at training time, and assume the ability to estimate conditional distributions over joint sequences of primitive controllers; \Cref{app:control_stability} explains how this can be implemented by solving Ricatti equations if dynamics are known (e.g. in a simulator), smooth, and stabilizable. In our experimental environment, control inputs are desired robot configurations, which the simulated robot executes by applying feedback gains. \iftoggle{arxiv}{Reiterating \Cref{rem:primitive_controllers_natural}, learned}{As discussed in \Cref{app:gen_controllers}, learned} or hand-coded low-level controllers are popular in practical implementations of generative behavior cloning. We discuss the merits of studying imitation learning with a synthesis oracle  in depth in \Cref{sec:merits_synthesis}.

\nipspar{Notions of distance.} While restricting ourselves to affine primitive controllers, our approximation error of generative behavior cloner is measured in terms of optimal transport distances that use the following ``maximum distance.'' Given two composite actions $\seqa = (\bbaru_{1:\tauc}, \bbarx_{1:\tauc}, \bbarK_{1:\tauc})$ and $\seqa' = (\bbaru_{1:\tauc}', \bbarx_{1:\tauc}', \bbarK_{1:\tauc}')$, we define
\begin{align}
\dmax(\seqa,\seqa') := \max_{1\le k \le \tauc}(\|\bbaru_{k}-\bbaru_{k}'\| + \|\bbarx_{k}-\bbarx_{k}'\| +\|\bbarK_{k}-\bbarK_{k}'\|). \label{eq:dmax}
\end{align}
Distances between policies are defined via natural optimal transport costs.
Given two policies $\pi = (\pi_h),\pi' = (\pi_h')$ and observation chunk $\pathm$, we define an induced optimal transport cost
\begin{align}
\Delta_{\epsilon}(\pi_h(\pathm),\pi_h'(\pathm)) := \inf_{\coup}\Pr_{(\seqa_h,\seqa_h')\sim \coup}\left[\dmax(\seqa_h,\seqa_h') > \epsilon\right],
\end{align}
where the $\inf_{\coup}$ denotes the infinum over all couplings between $\seqa_h \sim \pi_h(\pathm)$ and $\seqa_h' \sim \pi_h'(\pathm)$. $\Delta_{\epsilon}$ corresponds to a relaxed  L\'evy-Prokhorov metric \citep{strassen1965existence}, and can always be bounded, via Markov's inequality, by
\iftoggle{arxiv}
{
\begin{align}
\Delta_{\epsilon}(\pi_h,\pi_h' \mid \pathm)  \le \frac{1}{\epsilon}\cW_{1,\dmax}(\pi_h(\pathm),\pi_h'(\pathm)),
\end{align}
}
{$\Delta_{\epsilon}(\pi_h,\pi_h' \mid \pathm)  \le \frac{1}{\epsilon}\cW_{1,\dmax}(\pi_h(\pathm),\pi_h'(\pathm))$, }
where $\cW_{1,\dmax}(\pi_h(\pathm),\pi_h'(\pathm))$ denotes the $1$-Wasserstein distance between $\seqa_h \sim \pi_h(\pathm)$ and $\seqa_h' \sim \pi_h'(\pathm)$. 
 






\subsection{Incremental Stability and the Synthesis Oracle.}\label{sec:stab_defs} 
\iftoggle{arxiv}{Our key assumption is that the}{We assume that} synthesis oracle above produces \emph{incrementally stabilizing} control gains, in the sense first proposed by  \cite{angeli2002lyapunov}. Incremental stability has emerged as a natural desirable property for imitation limitation \citep{pfrommer2022tasil,tu2022sample,havens2021imitation}, because it forces the expert to be robust to small perturbations of their policy.
\iftoggle{arxiv}
{ 
% Figure environment removed
We now supply a formal definition, depicted in \Cref{fig:incremental_stab}.
}
{
  We now supply a formal definition.
}
 Given a primitive controller $\sfk: \R^{\dimx} \to \R^{\dimu}$, define the closed loop dynamic map $\fclkap(\bx,\delu) := f(\bx,\sfk(\bx)+\delu)$. Thus, composite action $\seqa$ is \emph{consistent} with a trajectory chunk $\seqs = (\bx_{1:\tauc+1},\bu_{1:\tau})$ if $\bx_{t+1} = \fclkap[t](\bx,\bzero)$ for $1 \le t \le \tauc$.\footnote{Below, we recall definitions of classes of comparison functions in nonlinear control \cite{khalil2002nonlinear} as follows: we say a univariate function $\gammaiss:\R_{\ge 0} \to \R_{\ge 0}$ is \emph{\classK}~if it is strictly increasing and satisfies $\gammaiss(0) = 0$. We say a bivariate function $\betaiss:\R_{\ge 0} \times \Z_{\ge 0} \to \R_{\ge 0}$ is \emph{\classKL}~if $x \mapsto \betaiss(x,t)$ is \classK~for each $t \ge 0$, and $t \mapsto \betaiss(x,t)$ is nonincreasing in $t$.}

\begin{restatable}[Time-Varying Incremental Stability]{definition}{tissdef}\label{defn:tiss} 
Let $\gammaiss(\cdot)$ be a \classK{} function, $\betaiss(\cdot,\cdot)$ be \classKL{} function, and let $\seqa = (\sfk_{1},\sfk_2,\dots,\sfk_{\tau})$ denote a sequence of primitive controllers (i.e. a composite action when $\tau = \tauc$). Given a sequence of input  perturbations $\delu_{1:\tau} \in (\R^{\dimu})^{\tau}$ and initial condition $\bxi \in \R^{\dimx}$, let $\xa_{i+1}(\delu_{1:\tau},\bxi) = \fclkap[i](\xa_{i}(\delu_{1:\tau},\bxi),\delu_i)$,  with $\xa_1 = \bxi$. We say that composite action $\seqa$ is time-varying incrementally input-to-state stable ($\tiss$) with moduli $\gammaiss(\cdot),\betaiss(\cdot,\cdot)$ if 
\iftoggle{arxiv}
{
\begin{align}
\forall  \bxi,\bxi' \in \R^{\dimx}, 0 \le i \le \tau,  \quad \|\xa_{i}(\bm{0}_{1:\tau},\bxi)-\xa_{i}(\delu_{1:\tau},\bxi')\| \le \betaiss(\|\bxi-\bxi'\|,\tau) + \gammaiss\left(\max_{1 \le s \le i-1}\|\delu_s\|\right) %\label{eq:tiss}
\end{align}
}{
  for all $\bxi,\bxi' \in \R^{\dimx}, 0 \le i \le \tau$,  $\|\xa_{i}(\bm{0}_{1:\tau},\bxi)-\xa_{i}(\delu_{1:\tau},\bxi')\| \le \betaiss(\|\bxi-\bxi'\|,\tau) + \gammaiss\left(\max_{1 \le s \le i-1}\|\delu_s\|\right)$. 
}
Given parameters $\cgamma,\cxi>0$ we say that $\seqa$ is local-$\tiss$ at $\bxi_{0}$ if the above holds only for all $\bxi,\bxi',\delu_{1:\tau}$ such that $\|\bxi-\bxi_0\|,\|\bxi'-\bxi_0\| \le \cxi$  and $\max_{t}\|\delu_{t}\| \le \cgamma$. 
\end{restatable}
Incremental stability implies that as the inital conditions $\|\bxi - \bxi'\| \to 0$ and $\max_{0 \le s \le i-1}\|\delu_t\| \to 0$, the trajectories induced by taking rolling out $\seqa$ from $\bxi$, and rolling out $\seqa$ from $\bxi'$ with additive input perturbations $\delu_{1:\tau}$ tend to zero in norm. This behavior needs only hold for initial conditions in a small neighborhood of a nominal state $\bxi_0$.
Importantly, the perturbations $\delu_{1:\tau}$ are fixed pertubrations of inputs, applied  to the \emph{closed loop behavior} under the controllers.
Our notion of incremental stability are similar too, but sublty different similar notions of past work. We provide an extended comparisons in \Cref{sec:comparison_to_prior}.
\iftoggle{arxiv}{
  
}{}
Our main assumption is that the synthesis oracle described above produced primitive controllers which are consistent with, and incrementally stabilizing for, the demonstrated trajectories. \Cref{fig:stab_tube} demonstrates the effect of stabilizing primitive controllers. 
\begin{assumption}\label{asm:iss_body} We assume that our synthesis oracle enjoys {the} following property. Let $\ctraj_T = (\bx_{1:T+1},\bu_{1:T}) \sim \Dexp$, and let $\sfk_{1:T} = \synth(\ctraj_T)$, partitioned into composite actions $\seqa_{1:H}$, with $\sfk_t(\bx) = \bbarK_t (\bx - \bbarx_t) + \bbaru_t$. We assume that, with probability one, $\sfk_{1:T}$ is consistent with $\ctraj_T$\footnote{Note that this implies $\bbarx_t = \bx_t$ and $\bbaru_t=\bu_t$.}, and that, for each $1 \le h \le H$, $\seqa_h = (\sfk_{t_h:t_{h}+\tauc-1})$ is local $\tiss$ at $\bx_{t_h}$ with moduli $\gammaiss,\betaiss$ and parameters $\cbeta,\cxi > 0$. We further assume that $\gammaiss$ and $\betaiss$ take the form 
\begin{align}
\gammaiss(u) = \cbargamma \cdot u , \quad \betaiss(u,k) = \cbarbeta e^{-(k-1)\lamiss}\cdot u, \quad \cbargamma,\cbarbeta > 0, \quad \lamiss \in (0,1].
\end{align} 
Lastly, we assume that for the expert trajectories and the primitive controllers drawn as above,  it holds that satisfy $\max\{\|\bx_t\|,\|\bu_t\|\} \le \Rdyn$ and $\|\bbarK_t\| \le \Rstab$ with probability one.
\end{assumption}
In \Cref{app:control_stability}, we show that \Cref{asm:iss_body} holds whenever (a) the dynamics of our system are smooth (but not necessarily linear!) (b) the affine gains are chosen to stabilize the Jacobian linearizations of the system around the nominal trajectory.

\begin{definition}[Problem constants]\label{defn:prob_constants_body} Throughout, we refer to constants $c_1,c_2,c_3,c_4,c_5 > 0$, which are polynomial in the terms in \Cref{asm:iss_body}, and which are defined formally in  \Cref{app:control_stability}.
\end{definition}





\newcommand{\cDh}{\cD_{\mathrm{exp},h}}

\newcommand{\dtraj}{\dist_{\mathrm{traj}}}
\newcommand{\Daugh}[1][h]{\cD_{\sigma,#1}}

\subsection{Simplified guarantees under total variation continuity}\label{sec:results_tvc}


This section presents our main theoretical result: if one learns a chunking policy $\pihat$ that can compute the conditional distribution of composite actions at time steps given observation-chunks, then a stochastically smoothed version of this policy, $\pihat_{\sigma}$, has low imitation error. Define, for any length $\tau \in \N$, the \emph{trajectory distance} between trajectories $\ctraj = (\bx_{1:\tau+1},\bu_{1:\tau}),\ctraj' = (\bx'_{1:\tau+1},\bu'_{1:\tau}) \in \mathscr{P}_{\tau}$ 
\begin{align}
\dtraj(\ctraj,\ctraj') := \max_{1 \le k \le \tau+1}\|\bx_{k}-\bx_k'\| \vee \max_{1 \le k \le \tau}\|\bu_k-\bu_k'\|. \label{eq:dtraj}
\end{align}
In particular, we define $\dtraj(\pathm,\pathm')$ and $\dtraj(\pathc,\pathc')$ by viewing these as trajectories of length $\taum-1$ and $\tauc$, respectively. 
Lastly, we define a per-timestep restriction of the expert distribution. In this section, we consider the case where the learner policy satisfies a total variation continuity (TVC) condition, defined below.
\begin{definition}[TVC of Chunking Policies]\label{defn:tvc_main}  We say that a chunking policy $\pi = (\pi_h)$ is total variation continuous with modulus $\gamtvc: \R_{\ge 0} \to \R_{\ge 0}$, written $\gamtvc$-TVC, if, for all $h \in [H]$ and any observation-chunks $\pathm,\pathm' \in \scrP_{\taum-1}$, \iftoggle{arxiv}
{
\begin{align}
\TV(\pi_h(\pathm),\pi_h(\pathm')) \le \gamtvc(\dtraj(\pathm,\pathm')).
\end{align}
}
{$\TV(\pi_h(\pathm),\pi_h(\pathm')) \le \gamtvc(\dtraj(\pathm,\pathm'))$. }
\end{definition}


% Figure environment removed

We depict the TVC property using our running left-right obstacle example in \Cref{fig:tvc_fig}. We stress that, in \Cref{defn:tvc_main}, the TV bound on $\TV(\pi_h(\pathm),\pi_h(\pathm'))$ applies to the \emph{composite actions} consisting of primitive controllers $\seqa_h  = \sfk_{t_h:t_h+\tauc-1} \sim \pi_h(\pathm)$; it does not upper bound the TV distance between raw control inputs. Indeed, ensuring TVC of the latter can lead to the failure modes depicted in  \Cref{fig:tvc_fig}(b). 
\iftoggle{arxiv}{
  
}{}
Next, we extract an expert ``policy'' from the expert demonstrations. 
\begin{definition}[Expert ``policy'' with synthesized controllers]\label{defn:Dexph} For $h \in [H]$, we let $\cDh$ denote the joint distribution of $(\seqa_h,\pathm)$, induced by drawing a trajectory  $\ctraj_T = (\bx_{1:T+1},\bu_{1:T}) \sim \Dexp$ from the expert distribution, $\sfk_{1:T} = \synth(\ctraj_T)$ be the associated primitive controllers, letting $\pathm = (\bx_{t_h - \taum+1:t_h},\bu_{t_h - \taum+1:t_{h}-1})$ be the associated observation-chunk at time $h$,  and $\seqa_h  = \sfk_{t_h:t_{h+1}-1}$ the associated composite action.  We let $\pist_h(\cdot):\cO \to \laws(\cA)$ denote the condition distribution of $\seqa_h \mid \pathm$ under $\cDh$.
\end{definition}

The conditional distributions $\pist_h(\cdot)$ are \iftoggle{arxiv}{precisely what is}{} estimated when training a generative model to predict $\seqa_h$ from observations $\pathm$.  Note that $\pist_h(\cdot)$ (and $\cDh$) is defined in terms of  \emph{both} expert demonstration from $\Dexp$ and the associated synthesized primitive controllers. In \Cref{lem:pistar_existence}, we show that when the synthesis oracle $\sfk_{1:T} = \synth(\ctraj_T)$ produces primitive controllers consistent with the trajectories, than $\pist = (\pist_h)$ produces the same marginals over states as $\Dexp$; that is, $\Imitmarg(\pist) = 0$.


\begin{theorem}\label{prop:TVC_main} Suppose \Cref{asm:iss_body} holds, and suppose that $0 \le \epsilon < c_2 $, and $\tauc \ge c_3$. Then, for any non-decreasing non-negative $\gamtvc(\cdot)$ and $\gamtvc$-TVC chunking policy $\pihat$,
\iftoggle{arxiv}
{
\begin{align}
\Imitmarg[\epsilon](\pihat) &\le H\gamtvc(\epsilon) +  \sum_{h=1}^H \Exp_{\pathm \sim \cDh}\Delta_{(\epsilon/c_1)}\left(\pist_h(\pathm),\pihat_h(\pathm) \right) \label{eq:TVC_main}\\
&\le H\gamtvc(\epsilon) +  \frac{c_1}{\epsilon}\sum_{h=1}^H \Exp_{\pathm \sim \cDh}\left[\cW_{1,\dmax}\left(\pist_h(\pathm), \pihat_h(\pathm) \right) \right].
\end{align}
}
{it holds that $\Imitmarg[\epsilon](\pihat) \le H\gamtvc(\epsilon) +  \sum_{h=1}^H \Exp_{\pathm \sim \cDh}\Delta_{(\epsilon/c_1)}\left(\pist_h(\pathm),\pihat_h(\pathm) \right)$, which is at most $ H\gamtvc(\epsilon) +  \frac{c_1}{\epsilon}\sum_{h=1}^H \Exp_{\pathm \sim \cDh}\left[\cW_{1,\dmax}\left(\pist_h(\pathm), \pihat_h(\pathm) \right) \right]$.}
\end{theorem}
The above result reduces the marginal imitation error of $\pihat$ to the sum over optimal transport errors between $\pihat$ and $\seqa \mid \pathm$ chosen by the expert demonstrators. Thus, if these are small, the local stabilization properties of the primitive controllers guaranteed by \Cref{asm:iss_body} ensure that errors compound at most linearly in problem horizon. The key ideas of the proof are given  \Cref{sec:analysis}, via a general template for imitation learning of general stochastic policies.  This template is instantiated with a details in \Cref{app:end_to_end}. 

\iftoggle{arxiv}
{
  \input{body/thm1_remarks}
}
{}

\newcommand{\cDhsig}{\cD_{\mathrm{exp},\sigma,h}}


\subsection{A general guarantee via data noising.}\label{sec:results_smoothing}  To circumvent assuming that the learner's policy is TVC,   we study estimating the conditionals under a popular data augmentation technique \cite{ke2021grasping}, where the learner is trained to imitate the conditional sequence of $\seqa \mid \pathmtil$, where $\pathmtil \sim \cN(\pathm,\sigma^2 \eye)$ adds $\sigma^2$-variance Gaussian noise to the true observation-chunk. To understand this better, consider the following \emph{smoothed} policy:
\begin{definition}[The smoothed policy]\label{defn:smoothed_policy} Let $\pihat = (\pihat_h)$ be a chunking policy. We define the \emph{smoothed policy} $\pihat_{\sigma} = (\pihat_{\sigma,h})$ by letting $\pihat_{\sigma,h}(\cdot \mid \pathm)$ be distributed as $\pihat_{h}(\cdot \mid \pathmtil)$, where $\pathmtil  \sim \cN(\pathm,\sigma^2 \eye)$.
\end{definition}
\Cref{sec:TVC_check} show's that Pinsker's inequality  implies noising automatically enforces TVC\iftoggle{arxiv}{:
\begin{lemma}\label{lem:tvc_body} Let $\pihat = (\pihat_h)$ be \textbf{\emph{any}} arbitrary chunking policy. Then, $\pihat_{\sigma}$ is $\gamtvc$-TVC, with $\gamtvc(u) = \frac{u\sqrt{2\taum - 1}}{2\sigma}$ being linear in $u$ and inversely proportional to $\sigma$. 
\end{lemma}
}{
}
This suggests that we can use some form of data noising to enforce the TVC property in \Cref{defn:tvc_main}. Let's now consider a related problem: trying to estimate the optimal distribution over composite actions \emph{conditioned on} a noised observation. This gives rise to a \emph{deconvolution} of the expert policy, which can be thought as an inverse operation of data noising. 

\begin{definition}[Noised Data Distribution and Deconvolution Policy]\label{defn:Dsigh} Let $\cDh$ be as in \Cref{defn:Dexph}. Define $\cDhsig$ as the distribution over $(\pathmtil,\seqa_h)$ generated by $(\pathm,\seqa_h) \sim \cDh$ and  $\pathmtil \sim \cN(\pathm,\sigma^2 \eye)$. We define the \emph{deconvolution policy} $\pidecsigh(\pathmtil)$ as the conditional distribution of $\seqa_h \mid \pathmtil$ under $\cDhsig$.
\end{definition}


%Because the demonstrator is stochastic, we care not about point-estimation, but rather \emph{generative modeling} of the conditional distributions of composite actions. This is measured by the following notion of discrepancy, formulated in an optimal transport error, which relaxes the L\'evy-Prokhorov metric \citep{strassen1965existence} between the conditional distribution of $\seqa_h \mid \pathmtil$ and $\seqa_h' \sim \pihat_h(\pathmtil)$ under $\Daugh$. 




\newcommand{\Thetatiss}{\Uptheta_{\mathrm{Iss}}}


Analogously to $\pist$, the policy $\pidecsigh$ is what a generative model trained to generate $\seqa_h $ from noised observations $\pathmtil$ of $\pathm \sim \Dexp$ learns to generate. 
Our next theorem states that, if our $\pihat$ approximates the idealized conditional distributation of composite actions given noised observations, then $\pihat_{\sigma}$, the smoothed policy, imitates the expert distribution with provable bounds on its imitation error:
\begin{theorem}[Reduction to conditional sampling under nosing]\label{thm:main_template}  Suppose \Cref{asm:iss_body} holds. Let  $c_1,\dots,c_5 > 0$, defined in \Cref{defn:prob_constants_body}, and let $\Thetatiss(x)$ denote a term which is upper and lower bounded by a $x$ times a polynomial in those constants and their inverses. Then, for  $\epsilon \le \Thetatiss(1)$, if we choose $\sigma = \epsilon/\Thetatiss(\sqrt{\dimx} + \log(1/\epsilon))$ and let $\tauc \le c_3$ and $\tauc - \taum \ge \frac{1}{\lamiss}\log(c_1/\epsilon)$, 
\iftoggle{arxiv}
{
    \begin{align}
    \Imitmarg(\pihat_{\sigma})  &\leq \Thetatiss\left(\epsilon H\sqrt{\taum}  \cdot (\sqrt{\dimx} + \log(1/\epsilon) \right) +   \sum_{h=1}^H\Exp_{\pathmtil \sim \cDhsig} \left[\Delta_{(\epsilon^2)}\left(\pidecsigh\left(\pathmtil\right),\,\pihat_h\left(\pathmtil\right) \right)\right]
 \label{eq:mainguarantee_simple}\\
  &\le  \Thetatiss\left(\epsilon H\sqrt{\taum}  \cdot (\sqrt{\dimx} + \log(1/\epsilon) \right) + \frac{1}{\epsilon^2}\sum_{h=1}^H \Exp_{\pathm \sim \cDhsig}\left[\cW_{1,\dmax}\left(\pidecsigh(\pathmtil),\,\pihat_h(\pathmtil) \right) \right],
    \end{align}
    }{
    \begin{align}\textstyle \Imitmarg(\pihat_{\sigma})  \leq \Thetatiss\left(\epsilon H\sqrt{\taum}  \cdot (\sqrt{\dimx} + \log(\frac 1 \epsilon) \right) +   \sum_{h=1}^H\Exp_{\pathmtil \sim \cDhsig} \left[\Delta_{(\epsilon^2)}\left(\pidecsigh\left(\pathmtil\right),\,\pihat_h\left(\pathmtil\right) \right)\right],\end{align} which is upper bounded by at most  $\Thetatiss\left(\epsilon H\sqrt{\taum}  \cdot (\sqrt{\dimx} + \log(1/\epsilon) \right) + \frac{1}{\epsilon^2}\sum_{h=1}^H \Exp_{\pathm \sim \cDhsig}\left[\cW_{1,\dmax}\left(\pidecsigh(\pathmtil),\,\pihat_h(\pathmtil) \right) \right]$.
    }
\end{theorem}

To reiterate, \Cref{thm:main_template} guarantees imitation of the distribution of marginals and final states of $\Dexp$ by replacing the explicit TVC assumption with noising, and the resulting guarantee applies to the \emph{smoothed policy} $\pihat_{\sigma}$ which adds smoothing noise back in. 
 \Cref{app:end_to_end} gives a number of additional results\iftoggle{arxiv}{, including: 
 \begin{itemize}
  \item A granular guarantee, \Cref{thm:main_template_precise}, that exposes the tradeoffs between parameters $\sigma$, $\epsilon$, and $\tauc$.  This result also ensures sharper bounds imitation of the marginal of the \emph{final state} $\bx_{T+1}$
  \item  Guarantees for imitating \emph{joint} trajectories under the further assumptions that (a) the  demonstrator has memory (or, more generally, a mixing time) of at most $\taum$, and (b) \emph{either} the demonstrator distribution  happens to satisfy a certain continuity property, \emph{or} $\sigma = 0$ and instead the learned $\pihat$ satisfies that same property.
\end{itemize}
}{. In \Cref{app:lbs}, we show that the proof framework, outlined in \Cref{sec:analysis}, which under lies the proofs  of \Cref{prop:TVC_main,thm:main_template}, is essentially sharp in the worst case. Moreover, in \Cref{sec:merits_synthesis}, we discuss the merits and drawbacks of our use of the synthesis oracle, and how it circumvents some of the challenges encountered in behavior cloning in past work. The key intuition behind the proof of \Cref{thm:main_template}  is depicted in \Cref{fig:replica_fig}, and full proof sketch is deferred to \Cref{sec:proof_sketch_thm2}}


\iftoggle{arxiv}
{
% Figure environment removed
}{
  \begin{SCfigure}
  % Figure removed
  \caption{ \footnotesize Multi-modal demonstrations traverse an obstacle left or right, exhibiting a pure  bifurcation.  We consider perturbing expert data on the right mode (blue circle) to a noised datum (gray circle). We show that generative behavior cloners learn to deconvolve this noise, creating a virtual ``replica'' sample (red circle) following the left mode, such that the replica and original are i.i.d. given the noised one. When the red circle's primitive controllers are rolled from from the blue circle, this leads to a trajectory (yellow circle) which  interpolates  across the bifurcations. Marginalizing over this process, the yellow trajectories probabilistically interpolate between red and blue modes, and (approximately) match the per-time-step marginal over expert distributions. } %Incremental stability ensures that the perturbed trajectory deviates from the nominal my at most $\gamma(\max_t \|\delu_t\|)$. Note that our general notion of incremental stability accomodates $\bx_1\ne \bx_1'$, and ensures that the effect of this difference in initial conditions on trajectory distance at time $t$ decays  as $t \mapsto \betaiss(\|\bx_1-\bx_1'\|,t)$. }
  \label{fig:replica_fig}
    %\iftoggle{arxiv}{}{\vspace*{-1.2em}}
\end{SCfigure}
}

\iftoggle{arxiv}
{
\begin{proof}[Proof Sketch]
\input{body/replica_proof_sketch}
\end{proof}

\begin{remark}[Sharpness of \Cref{prop:TVC_main,thm:main_template}]  In \Cref{app:lbs}, we show that the proof framework, outlined in \Cref{sec:analysis}, which under lies the proofs  of \Cref{prop:TVC_main,thm:main_template}, is essentially sharp in the worst case. Of particular importance, we show that (a) that our dependence on TVC without smoothing  is necessary in the worst case, (b) the smoothing the learned policy $\pihat$ to $\pihat_{\sigma}$ is necessary to ensure adequate imitation.   This section suggests that, up to terms polynomial in the stability parameters specified in \Cref{defn:tiss}, the results in these theorems that upper bound $\Imitmarg$ in terms of the $\Delta_{\epsilon}$ error terms are tight. We note that bounding $\Delta_{\epsilon} \le \frac{1}{\epsilon}\cW_{1,\dmax}$ may be loose in general. 
\end{remark}

  \subsection{Merits and Drawbacks of the Synthesis Oracle}\label{sec:merits_synthesis}
  \input{body/merits_of_synthesis}
}
{

  %\mscomment{TODO abridged merits of synthesis}
}



%We never explicitly model bifurcations; rather, we allow expert demonstrations to be sufficiently rich as to permit them. Eschewing global stability, $\tauc$ ensures that trajectories are long enough for the \emph{strictly local} stability assumptions in \Cref{asm:iss_body} to provide benefit. Thus, non-Markovianity and multi-modality is challenging only insofar as it relates to the difficulty of local stabilization. Indeed, in \Cref{app:gen_controllers}, we generalize \Cref{thm:main_template} and \Cref{prop:TVC_main} below to general families of parametric, incrementally stabilizing controllers.
%The role of $\tauc$ is best understood as ensuring that trajectories are long enough to stabilize around, which suggests the following surprising conclusion: \emph{non-Markovianity of the demonstrator is  only insofar as it a.} 
%A key limitation of our work is that, to take advantage of local stability, we rely on either synthesized primitive controllers (in our analysis) or low-level stabilizing controllers built into problem environments (in our experiments). As noted in the Introduction, this obviates the needed for stronger query access to the expert (e.g. \cite{pfrommer2022tasil}), or for interactive data collection \cite{ross2010efficient}. 
%\abcomment{it would be good to summarize this paragraph in like two sentences in the intro to really drive home the point that our innovation is to replace strong stability assumptions on the expert with strong (but realistic) assumptions on the oracle.} 





%\Cref{thm:main} leverages statistical learning guarantees for DPPMs to show our learned policy approximately samples from $\pidech$ in a truncated Wasserstein distance (\Cref{app:scorematching}). 

