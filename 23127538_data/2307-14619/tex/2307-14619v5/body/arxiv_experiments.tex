%!TEX root = ../main.tex


% Figure environment removed



\subsection{Experimental Results}\label{sec:experiments}
In this section, we demonstrate the benefits of diffusing low level controllers, and of our approach to data noising. We explain the environments in greater detail, along with all training and computational details in \Cref{app:exp_details}.

\paragraph{Diffusion of Gain Matrices.} \Cref{fig:quad_gains_two} compares the performance of diffusing (chunks of) raw control inputs to diffusing (chunks of) gain matrices for a canonical model of a 2-d quadrotor. Gain matrices are derived from solving an infinite horizon LQR problem at each (state, input) pair (see \Cref{app:exp_details} for details and for rationale).
We find that diffusing gain matrices yields dramatic improvements in performance, in particular allowing a \emph{single imitated trajectory} to outperform learning raw control inputs from 10 demonstrations. We stress that access to synthesized gain matrices is a stronger requirement than classical behavior cloning from raw data. Importantly, the gain matrices do not require {additional expert supervision}. Rather, they  \emph{supplement additional expert data with models of the environment}, via the synthesized gain matrices. 



\paragraph{Noise Injection: Replica v.s. Deconvolution.}
We empirically evaluate the effect on policy performance of our proposal to inject noise back into the dynamics at inference time.  We consider three challenging robotic manipulation tasks studied in prior work: PushT block-pushing \citep{chi2023diffusion}; Robomimic Can Pick-and-Place and Square Nut Assembly \citep{mandlekar2021matters}.  
% Since \Cref{alg:imitation_augmentation} produces a policy which intentionally injects noise into the observation during evaluation, we naturally wish to understand the practical impact of this noise on policy performance. We principally consider challenging robotic manipulation tasks studied in prior work, including the PushT block-pushing task \citep{chi2023diffusion}, the Robomimic Can Pick-and-Place task, and the Robomimic Square Nut Assembly task \citep{mandlekar2021matters}. See \Cref{app:exp_details} for the environment details, DDPM architecture, and training methodology. 
The learned diffusion policy generates state trajectories over a $\tau_c = 8$ chunking horizon using fixed feedback gains provided by the $\synth$ oracle to perform position-tracking of the DDPM model output. We direct the reader to \citet{chi2023diffusion} for an extensive empirical investigation into the performance of diffusion policies in the noiseless $\sigma = 0$ setting. We display the results of our experiments in \Cref{fig:noise_sweep}.  Observe that the performance degredation of the replica policy from the unnoised $\sigma = 0$ variant is minimal across all environments and even leads to a slight but noticeable improvement in the small-noise regime for PushT (and low-data Can Pick and Place). In the presence of non-negligible noise $\toda$ significantly outperforms the conventional policy $\pihat$ (obtained by noising the observations at training but not test time), as predicted by our theory.

% Figure environment removed
