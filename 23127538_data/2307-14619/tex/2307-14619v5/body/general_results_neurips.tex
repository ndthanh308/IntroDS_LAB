%!TEX root = ../neurips_main.tex
\section{Further Main Results and Discussion}
\label{app:further_main_nips}

We begin with a general remark. 
\begin{remark}[Do we need state observation or time-varying policies?]\label{rem:states_or_time_var} In practical applications, behavior cloning policies respond not to measurements of physical system state, but rather visual observations and/or tactile feedback. Additionally, learned policies to not explicit take a time index $h$ as input. This allows these policies to perform flexibily across tasks with varying time horizons, and to automatically reset after encountered obstacles.

In contrast, our formulation requires policies to be (a) functions of system state and (b) vary with the chunk index $h$. If visual observations or tactile measurements are  \emph{sufficient} to recover system state, then we can view these data are redundant states, and thus (a) is not a restriction. Moreover, as described in \Cref{rem:time-varying}, a limited portion of theoretical results do hold for policies which do not vary with $h$. In general, restrictions (a) and (b) are necessary for our analysis because they allow us to analyze the imitator behavior in a Markovian fashion. Without these restrictions, one would have to reason about (a$'$) uncertainty over state given observation, or (b$'$) variation in expert behavior across different time steps $h$. Removing these restrictions is an exciting direction for future work.  
\end{remark}

\subsection{Discussion surround \Cref{prop:TVC_main}}
\input{body/thm1_remarks}

\subsection{Proof sketch of \Cref{thm:main_template}}\label{sec:proof_sketch_thm2}
\input{body/replica_proof_sketch}
\qed
\subsection{Merits and Drawbacks of the Synthesis Oracle}\label{sec:merits_synthesis}

\input{body/merits_of_synthesis}


%%%%%%%%%%%%Formal Results for HINT
 \input{body/hint_formal_results}
