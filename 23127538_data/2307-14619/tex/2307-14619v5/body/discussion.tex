%!TEX root = ../neurips_main.tex

\section{Discussion}\label{sec:discussion}

This work considerably loosened assumptions placed on the \emph{expert distribution} by introducing a synthesis oracle responsible for stabilization. How best to achieve low-level stabilization remains an open question.
We hope that this work encourages further empirical research into improving the stability of imitation learning, either via the hierarchical route proposed in this paper or via new innovations. 


\iftoggle{arxiv}{
In addition, while our theory allows for time-varying policies, in many robotics applications, time-invariant policies are more natural; we believe extension to time-invariant policies is possible but would require further complication. 
Despite these limitations, our work presents a significant step toward understanding the imitation of complex trajectories in natural control systems.

Furthermore, as discused in \Cref{rem:time-varying,rem:states_or_time_var}, practical implementations of generative behavior cloning use time-\emph{invariant} policies conditioned on observations which may not be sufficient statistics for system states. Doing so in full generality breaks the Markovian structure of the problem, and will therefore require fundamentally new techniques to adress. We suspect that a resolution to this question should combine distribution matching with some form of ``cost-to-go'' on demonstration performance.

Lastly, as per \Cref{rem:TVC}, our guarantees \emph{can} hold for zero ($\sigma = 0$) noise-based smoothing if the learned policy $\pihat$ is total variation continuous (TVC). While uniform TVC is unrealistic for DDPMs, there might be some form of local or distributional TVC in the support of training data. It is an interesting direction for future work to investigate if this property does hold, as our experiment suggests that imitation is successful in the $\sigma = 0$ regime. 
}{}
