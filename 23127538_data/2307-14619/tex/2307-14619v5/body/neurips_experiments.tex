%!TEX root = ../neurips_main.tex


% Figure environment removed



\subsection{Experimental Results}\label{sec:experiments}
In this section, we demonstrate the benefits of diffusing low level controllers, and of our approach to data noising. We explain the environments in greater detail, along with all training and computational details in \Cref{app:exp_details}. \Cref{fig:quad_gains_two} compares the performance of diffusing (chunks of) raw control inputs to diffusing (chunks of) gain matrices for a canonical model of a 2-d quadrotor. We find that diffusing gain matrices yields dramatic improvements in performance, in particular allowing a \emph{single imitated trajectory} to outperform learning raw control inputs from 10 demonstrations.


Next, empirically evaluate the effect on policy performance of our proposal to inject noise back into the dynamics at inference time.  We consider three challenging robotic manipulation tasks studied in prior work: PushT block-pushing \citep{chi2023diffusion}; Robomimic Can Pick-and-Place and Square Nut Assembly \citep{mandlekar2021matters} (we direct the reader to \citet{chi2023diffusion} for an extensive empirical investigation into the performance of diffusion policies in the un-noised $\sigma = 0$ regime). We display the results of our experiments in \Cref{fig:noise_sweep}.  Observe that the performance degredation of the replica policy from the unnoised $\sigma = 0$ variant is minimal across all environments and even leads to a slight but noticeable improvement in the small-noise regime for PushT (and low-data Can Pick and Place). In the presence of non-negligible noise $\toda$ significantly outperforms the conventional policy $\pihat$ (obtained by noising observations at training but not test time), as predicted by our theory.
% Since \Cref{alg:imitation_augmentation} produces a policy which intentionally injects noise into the observation during evaluation, we naturally wish to understand the practical impact of this noise on policy performance. We principally consider challenging robotic manipulation tasks studied in prior work, including the PushT block-pushing task \citep{chi2023diffusion}, the Robomimic Can Pick-and-Place task, and the Robomimic Square Nut Assembly task \citep{mandlekar2021matters}. See \Cref{app:exp_details} for the environment details, DDPM architecture, and training methodology. 

% Figure environment removed
