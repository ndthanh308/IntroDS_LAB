%!TEX root = ../neurips_main.tex



\begin{remark}[The $\epsilon=0$ case]\label{rem:eps_0}
If we were able to bound the policy error $\Delta_{(\epsilon)}$ with $\epsilon = 0$ -- which corresponds to estimating $\seqa_h \mid \pathm$ in \emph{total variation} distance -- the imitation learning problem would be trivialized, and neither the TVC condition above or the noise-injection based smoothing in the section below would not be needed  (see \Cref{app:end_to_end}).  \Cref{app:scorematching} explains that the needed assumptions for this stronger sense of approximate sampling do not hold in our setting, because expert distributions over actions typically lie on low-dimensional manifolds. 
\end{remark}


\begin{remark}[On the TVC assumption]\label{rem:TVC}  It is true than any $\pihat$ implemented as a DDPM with a Lipschitz activation with bounded-magnitude parameters is indeed TVC.  Unfortunately, these Lipschitz constants can be too large to be meaningful in practical scenarios, scaling exponentially with network depth. In addition, the absence of smoothing $\sigma$ may make the corresponding DDPM learning problem more challenging.   Hence, in what follows, we shall require the additional sophistication of smoothing with Gaussian noise of variance $\sigma^2 > 0$ for meaningful guarantees. 

Furthermore, we show in \Cref{app:TVC_relax} that the TVC assumption, which measures total variation distance between nearby $\pihat_h(\cdot)$ at nearby observations, can be relaxed to variant which measures the probability (under a minimal coupling) that actions differ by some tolerance. However, this tolerance has to be quite small, and as we argue, any reasonable notion of Wasserstein continuity  is unlikely to suffice. 
%\Cref{prop:TVC_main} corresponds to the $\sigma = 0$ case of \Cref{thm:main}. The proof \Cref{prop:TVC_main} is considerably simpler than that of \Cref{thm:main_template}, and serves to build intuition for the latter; 
\end{remark}

\begin{remark}[Imitation of the joint distribution] Suppose the expert distribution $\Dexp$ has at most $\taum$-bounded memory (defined formally in \Cref{defn:bounded_memory}). Then  $\Imitjoint[\epsilon](\pihat)$ satisfies the same upper bound \eqref{eq:TVC_main}, where $\Imitjoint[\epsilon](\pihat)$, formally defined in \Cref{def:loss_joint}, measures an optimal transport distance between the \emph{joint distribution} of the expert trajectory and the one induced by $\pihat$.
\end{remark}
\begin{remark}[Is chunking necessary?] In \Cref{sec:no_min_chunk_length}, we show that we can remove the required lower bound on $\tauc$ --- allowing, in particular, the choice of $\tauc = 1$ --- under the slightly stronger condition that our synthesis oracle ensures that the entire sequence of primitive controllers $\sfk_{1:T}$ on the whole horizon $T$ are incrementally stabilizing.  However, chunking is known to yield empirical benefits \cite{zhao2023learning}, and training models to predict action-chunks of longer duration than the agent acts on is also observed to improve performance \cite{chi2023diffusion}.
\end{remark}

\begin{remark}[Are time-varying policies necessary? (continuing \Cref{rem:states_or_time_var})]\label{rem:time-varying}  In  practice, time-invariant policies $\pihat$ which do depend on the $h$-index  are preferred because they are more resilient to varying-horizon tasks, and can automatically ``reset'' when they encounter an obstacle. Here, we note that if $\pist_h(\pathm)$, the conditional distribution of $\pist_h$ given $\pathm$, is independent of $h$ --- that is, the expert is Markov and time-invariant given $\pathm$ --- then the term $\Delta_{(\epsilon/c_1)}\left(\pist_h(\pathm),\pihat_h(\pathm) \right)$ on the right-hand side of \eqref{eq:TVC_main} can be made small by choosing a time-invariant $\pihat$. Thus, certain expert behavior can indeed be imitated by time-invariant policies. However, we do require time-varying policies to imitate \emph{arbitrary experts}. And, in addition, the data smoothing strategy described below requires a time-varying $\pihat$. Extending our results  to time-invariant $\pihat$ is an interesting direction for future inquiry, and we suspect that this may require some further notion of cost-to-go to made the formulation feasible. 
%(not just the primitive controllers for each composite action individually).
\end{remark}