%!TEX root = ../main.tex

\section{Experiment Details}
\label{app:exp_details}
% Figure environment removed

\subsection{Compute and Codebase Details}

\paragraph{Code.} For our experiments we build on the existing PyTorch-based codebase and standard environment set provided by \citet{chi2023diffusion} as well as the robomimic demonstration dataset \citet{mandlekar2021matters}.
\footnote{The modified codebase with instructions for running the experiments is available at the following anonymous link: \url{https://www.dropbox.com/s/vzw0gvk1fd3yadw/diffusion_policy.zip?dl=0}. We will provide a public github repository for the final release.}
\paragraph{Compute.} We ran all experiments using 4 Nvidia V100 GPUs on an internal cluster node. For each environment running all experiments depicted in \Cref{fig:noise_sweep} took 12 hours to complete with 20 workers running simultaneously for a total of approximately 10 days worth of compute-hours. Between all 20 workers, peak system RAM consumption totaled about 500 GB.

\subsection{Environment Details}\label{sec:env_details}
For simplicity the stabilizatin oracle $\synth$ is built into the environment so that the diffusion policy effectively only performs positional control. See \cref{fig:environments} for visualizations of the environments.

\paragraph{PushT.} The PushT environment introduced in \cite{chi2023diffusion} is a 2D manipulation problem simulated using the PyMunk physics engine. It consists of pushing a T-shaped block from a randomized start position into a target position using a controllable circular agent. The synthesis oracle runs a low-level feedback controller at a 10 times higher to stabilize the agent's position towards a desired target position at each point in time via acceleration control. Similar to \citet{chi2023diffusion}, we use a position-error gain of $k_p = 100$ and velocity-error gain of $k_v = 20$. The observation provided to the DDPM model consists of the x,y oordinates of 9 keypoints on the T block in addition to the x,y coordinates of the manipulation agent, for a total observation dimensionality of 20.

For rollouts on this environment we used trajectories of length $T = 300$. Policies were scored based on the maximum coverage between the goal area and the current block position, with > 95 percent coverage considered an ``successful'' (score = 1) demonstration and the score linearly interpolating between $0$ and $1$ for less coverage. A total of 206 human demonstrations were collected, out of which we use a subset of 90 for training.

\paragraph{Can Pick-and-Place.} This environment is based on the Robomimic \citep{mandlekar2021matters} project, which in turn uses the MuJoCo physics simulator. For the low-level control synthesis we use the feedback controller provided by the Robomimic package. The position-control action space is $7$ dimensional, including the desired end manipulator position, rotation, and gripper position, while the observation space includes the object pose, rotation in addition to position and rotation of all linkages for a total of $23$ dimensions. Demonstrations are given a score of $1$ if they successfully complete the pick-and-place task and a score of $0$ otherwise. We roll out 400 timesteps during evaluation and for training use a subset of up to 90 of the 200 ``proficient human" demonstrations provided.

\paragraph{Square Nut Assembly.} For Square Nut Assembly, which is also Robomimic-based \citep{mandlekar2021matters}, we use the same setup as the Can Pick and Place task in terms of training data, demonstration scoring, and low-level positional controller. The observation, action spaces are also equivalent to the Can Pick-and-Place task with $23$ and $7$ dimensions respectively.

\paragraph{2D Quadcopter.}
The 2D quadcopter system is described by the state vector: $(x, z, \phi, \dot{x}, \dot{z}, \dot{\phi}),$ with input $u = (u_1, u_2),$ and dynamics:

\begin{align}
   \ddot{x} &= -u_1 \sin(\phi) / m, \\
   \ddot{z} &= u_1 \cos(\phi)/m - g, \\
   \ddot{\phi} &= u_2 / I_{xx}.
\end{align}
The specific constants we use are 
$m = 0.8$, $g = 9.8$, and $I_{xx} = 0.5$.
We integrate these dynamics using forward Euler with step size $\tau=0.01$.
The task is to move the quadcopter to the origin state.
The cost function we used for the MPC expert is:
$$c((x, z, \phi, \dot{x}, \dot{z}, \dot{\phi}), (u_1, u_2)) = 
    x^2 + z^2 + \phi^2 + \dot{x}^2 + \dot{z}^2 + \dot{\phi}^2 + 0.5 (u_1 - mg)^2 + 0.1u_2^2.$$
We constructed a per-timestep reward function using this cost function: $$r((x, z, \phi, \dot{x}, \dot{z}, \dot{\phi}), (u_1, u_2))) = \exp(-c((x, z, \phi, \dot{x}, \dot{z}, \dot{\phi}), (u_1, u_2))),$$
such that the MPC cost minimization corresponds to maximizing the reward used to benchmark the trained models.

\subsection{Gain Synthesis}

For the quadcopter gain-diffusion experiments, we synthesize stabilizing gains for each $(\overline{x}_t, \overline{u}_t)$ pair in our training data by analytically differentiating the dynamics $x_{t+1} = f(x_t, u_t)$ given in \ref{sec:env_details} at $\overline{x}_t, \overline{u}_t$ and applying infinite-horizon LQR to the linearized system $x_{t + 1} = A(x_t - \overline{x}_t) + B(u_t - \bar{u}_t)$ where $A = \partial_x f(\overline{x}_t, \overline{u}_t), B = \partial_u f(\overline{x}_t, \overline{u}_t)$. In particular, we solve the discrete time algebraic Ricatti equation:

$$ P = A^\top P A - (A^\top P B)(R + B^\top P B)^{-1} + Q,$$
where for simplicity we used identity matrices for $R, Q$. Using $P$ we computed the gains:
$$K = -(R + B^\top P B)^{-1}B^\top P A.$$
Since the timesteps of the simulator are small, we experimentally find that this is sufficient in order to stabilize the system over the diffused chunks and produces significantly less variance in the gains than performing time-varying discrete LQR over the chunks to synthesize the gains.

% \subsection{Additional Experimental Results}
\subsection{DDPM Model and Training Details.}

\paragraph{PushT, Can-Pick-and-Place, Square Nut Assembly.} For these experiments we use the same 1-D convolutional UNet-style \citep{ronneberger2015u} architecture employed by \citep{chi2023diffusion}, which is in turn adapted from \citet{janner2022planning}. This principally consists of 3 sets of downsampling 1-dimensional convolution operations using Mish activation functions \citep{misra2019mish}, Group Normalization (with 8 groups) \citep{wu2018group}, and skip connections with 64, 128, and 256 channels followed by transposed convolutions and activations in the reversed order. The observation and timestep were provided to the model with Feature-wise Linear Modulation (FiLM)  \citep{perez2018film}, with the timestep encoded using sin-positional encoding into a $64$ dimensional vector. 

During training and evaluation we utilize a squared cosine noise schedule \citep{nichol2021improved} with 100 timesteps across all experiments. For training we use the AdamW optimizer with linear warmup of 500 steps, followed by an initial learning rate of $1 \times 10^{-4}$ combined with cosine learning rate decay over the rest of the training horizon. For PushT models we train for 800 epochs and evaluate test trajectories every 200 epochs while for Can Pick-and-Place and Square Nut Assembly we evaluate performance every 250 epochs and train for a total of 1500 epochs.

The diffusion models are conditioned on the previous two observations trained to predict a sequence of $16$ target manipulator positions, starting at the first timestep in the conditional observation sequence. The $2$rd (corresponding to the target position for the current timestep) through $9$th generated actions are emitted as the $\tau_c = 8$ length action sequence and the rest is discarded. Extracting a subsequence of a longer prediction horizon in this manner has been shown to improve performance over just predicting the $H=8$ action sequence directly \citep{chi2023diffusion}.

\paragraph{2D Quadcopter.} For the 2D quadcopter experiments, we used a 5 layer MLP with hidden feature dimensions of $128, 128, 64$, and $64$ for all experiments, with sine-positional encoding of dimension 64 and FiLM to condition on the diffusion timestep and observation chunk.  We use the same optimizer setup as the PushT, Pick-and-Place, and Square Assembly experiments with a batch size of 64 and a total of $200$ epochs or 20,000 training iterations, whichever is larger.

We predict sequences of 8 control inputs, conditioned on two previous observations, where the 2nd control input in this sequence corresponds to the current timestep. For gain diffusion experiments, this includes a sequence of 8 control inputs, reference states, and gains. Similar to our other experiments, the 2nd through 5th generated actions of this sequence are emitted. 

\paragraph{Augmentation Procedure.} For $\sigma > 0$ we generate new perturbed observations per training iteration, effectively using $N_{\textrm{aug}} = N_{\textrm{epoch}}$ augmentations. We find this to be easier than generating and storing $N_{\textrm{aug}}$ augmentations with little impact on the training and validation error. Noise is injected after the observations have been normalized such that all components lie within $[-1, 1]$ range. Performing noise injection post normalization ensures that the magnitude of noise injected is not affected by different units or magnitudes.

