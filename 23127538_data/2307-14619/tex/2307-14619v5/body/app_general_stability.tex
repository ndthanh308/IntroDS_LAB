%!TEX root = ../main.tex

\section{Generalization to Generic Incrementally Stable Primitive Controllers}\label{app:gen_controllers}


In this section, we consider a generalization of the theory to allow for general, nonlinear primitive controllers, as long as they obey the incremental stability considered in \citet{pfrommer2022tasil}. {We consider controllers of the form:}
\begin{align}
\sfk(\bx) = \sfk(\bx;\theta), \quad \theta \in \Theta. \label{eq:sfk}
\end{align}
We assume that $\Theta \subset \R^{d_{\Theta}}$ is a measurable subset of a finite dimensional space and (2) that   $\sfk(\bx;\theta)$ is jointly piecewise-Lipschitz with at most countably many pieces. We define composite actions just as for linear primitive controllers: 
\begin{align}
\seqa = (\sfk_{1},\sfk_2,\dots,\sfk_{\tauc}).
\end{align}
We view $\cK := \{\sfk(\cdot;\theta) | \theta \in \Theta\}$ and $\cA = \cK^{\tauc}$ as Polish spaces with the Euclidean metric on the controller parameters $\theta$ (resp. sequences of control parameters).
%Lastly, 
%given a primitive controller $\sfk$, we recall the closed loop dynamic map $\fclkap(\bx,\delu) := f(\bx,\sfk(\bx)+\delu)$. Thus, composite action $\seqa$ is \emph{consistent} with $(\bx_{1:\tauc+1},\bu_{1:\tau})$ if $\bx_{t+1} = \fclkap[t](\bx,\bzero)$ for $1 \le t \le \tauc$.
 Our definition of incremental stability from \Cref{defn:tiss} applies verbatim to these more general controllers.

\begin{example}[Approximate Inverse Dynamics \& Position Control]\label{exmp:gen_control}  A natural example of the above is where $\theta$ corresponds to a sequence of position commands supplied to a robotic position controller, as in \cite{chi2023diffusion}, or where $\theta$ is a of state-command given to an inverse dynamics model, as in \cite{ajay2022distributionally}. In these settings, we can actually regard $\theta$ as the ``control action,'' and envision the closed loop system of \{system + position controller/inverse dynamics model\} as itself being incrementally stable. However, our framework is considerably more general, and allows us, for example, to diffuse other parameters governing the performance of the low level controllers as well (e.g. joint spring constants in robotic position control).
\end{example}
    

In this section, we replace \Cref{asm:iss_body} with the more general assumption that allows arbitrary forms of the incremental stability moduli:

\begin{asmmod}{asm:iss_body}{b}\label{asm:tis} Let  $\gammaiss(\cdot)$ be \classK, $\betaiss(\cdot,\cdot)$ be \classKL,  and let $\cxi,\cbeta,\cgamma > 0$ be postive constants. We assume access to a synthesis oracle $\synth: \scrP_{T} \to \cA^{H}$ such that, with probability $1$ over $\ctraj_T = (\bx_{1:T+1},\bu_{1:T})\sim \Dexp$, $\seqa_{1:H} = \synth(\ctraj_T)$ satisfies the following properties:
\begin{itemize}
    \item $\seqa_h = \sfk_{t_h:t_{h+1}-1}$ is consistent with $\pathc[h+1] = (\bx_{t_h:t_{h+1}},\bu_{t_h:t_{h+1}-1})$; equivalently, 
    \begin{align}
    \bx_{t+1} = f(\bx_t,\sfk_t(\bx_t)), \quad t \in [T].
    \end{align}
    \item $\seqa_h$ is locally $\tiss$ at $\bx_{t_h}$ with moduli $\gammaiss(\cdot),\betaiss(\cdot,\cdot)$ and constants $\cgamma,\cbeta,\cxi > 0$.
\end{itemize}
\end{asmmod}
%We note that $\tiss$ encapsulates the precise conditions we need about the dynamical system \emph{and} the synthesized controller. In particular, \emph{we do not require smoothness/regularity} of the dynamics when \Cref{asm:tis} holds. 



\subsection{Main Results for General Primitive Controllers}

To state our main results, we begin by defining distances on primitive controllers, as well as the induced imitation error of a policy $\pihat$. Throughout,  $\Daugh$  denotes the noise-smoothed expert data distribution  over $(\pathmtil,\seqa_h)$ as  in \Cref{defn:Dsigh} (with $\seqa_h$ now representing sequence of general primitive controllers, not {the linear ones considered in the main body}. 


\newcommand{\couphatsighbar}{\overline{\couple}_{\sigma,h}(\pihat)}
\newcommand{\dloc}[1][\alpha]{\dist_{\mathrm{loc},#1}}
\newcommand{\rollout}{\mathrm{rollout}}
\newcommand{\Delis}{\Delta_{\Iss,\sigma,h}}
\begin{definition}\label{defn:dloc} Define the local-distance between composite actions $\seqa = \sfk_{1:\tauc},\seqa'=\sfk'_{1:\tauc} \in \cA$ at state $\bx$ and scale $\alpha > 0$ as
\begin{align}
\dloc(\seqa, \seqa' \mid \bx) := \max_{1 \le i \le \tauc} \sup_{\delx:\|\delx\| \le \alpha} \|\sfk_i(\bx_i+\delx)-\sfk_i'(\bx_i+\delx)\|, 
\end{align}
where above $\bx_{1} = \bx$, $\bx_{t+1}=f(\bx_t,\sfk_t(\bx))$, and $\seqa =\sfk_{1:\tauc}$.
Finally, we define
\begin{align}
\Delta_{\Iss,\sigma,h}(\pihat;\epsilon,\alpha) &:=  \inf_{\coup \in \couphatsighbar}\Pr_{(\seqa_h,\seqa_h') \sim \coup}\left[\dloc(\seqa, \seqa' \mid \bx_{t_h})) > \epsilon\right], 
\end{align}
where $\couphatsighbar$ denotes the set of couplings of $(\pathm,\pathmtil,\seqa_h,\seqa_h')$, induced by drawing 
$(\pathm,\seqa_h) \sim \cDh$, $\pathmtil \sim \cN(\pathm,\sigma^2 \eye)$, and $\seqa_h' \sim \pihat_h(\pathmtil)$, and where above $\bx_{t_h}$ is the last state in $\pathm$.Note that the only degree of freedom for in selecting elements of $\couphatsighbar$.
\end{definition}

\begin{remark}[On the distance $\dloc$]
In words, $\dloc(\seqa, \seqa' \mid \bx_{1:\tauc})$ measures the supremal distance between the primitive controllers comprising $\seqa,\seqa'$, along radius-$\alpha$ neighborhoods of a given sequence $\bx_{1:\tauc}$. This supremal distance was studied in \citet{pfrommer2022tasil} and motivates their proposed algorithm TaSIL. \emph{Unlike affine primitive controllers,} the supremal distance between general primitive controllers may indeed dependence on the localizing sequence $\bx_{1:\tauc}$. 
\end{remark}

Having defined our distance, $\Delta_{\Iss,\sigma,h}(\pihat;\epsilon,\alpha)$ measures the probability that this difference exceeds some threshold $\alpha > 0$, under appropriate couplings where $\pathmtil$ is induced by noising the expert distribution, $\seqa$ is the corresponding action, $\seqa'$ is from the policy $\pihat$, and the localizing sequence follows from rolling out $\seqa$ on the last state $\tilde\bx_{t_h}$ of $\pathmtil$. $\Delta_{\Iss,\sigma,h}$ is the natural analogue of the distances consider in \Cref{thm:main_template}. One subtlety of $\Delta_{\Iss,\sigma,h}$ is that the couplings are constructed such that $\seqa_h$ is the action associated with $\pathm$. This is conceptually correct because it specifies a localizing-state for $\dloc(\seqa_h, \seqa_h' \mid \bx_{t_h})$ (which is not an issue for affine primitive controllers). When $\seqa_h$ arises from a synthesis oracle, $\bx_{t_h}$ lies on the trajectory from which $\seqa_h$ is synthesized.

\Cref{thm:main_template} generalizes as follows.

\begin{theorem}[Generalization of \Cref{thm:main_template} to general primitive controllers]\label{thm:main_template_general} Assume \Cref{asm:tis}, and let $\epsilon > 0$ satisfy
\begin{align}
\gammaiss^{-1}(\betaiss(2\gammaiss(\epsilon),\tauc) \le \epsilon \le \min\{\cgamma,\gammaiss^{-1}(\cxi/4)\} \label{eq:eps_cond_general}
\end{align} 
Define
\begin{align}
\upomega = 2 \sqrt{5 \dimx + 2\log\left( \frac{2\sigma}{\gammaiss(\epsilon)} \right)},\quad \epsilon_1 = 2\betaiss(2\gammaiss(\epsilon),0) + 2\upbeta(2\sigma \upomega,0), \quad \epsilon_2 = 2\betaiss(2\gammaiss(\epsilon),0). \label{eq:general_imit_terms}
\end{align} 
Then, if $\sigma \le \cxi/4\upomega$ and $\gammaiss(\epsilon) \le 2\sigma$, we have
\begin{align}
\Imitmarg[\epsilon_1](\pihat) \le  \frac{3H\sqrt{2\taum-1}}{2\sigma}\left(2\epsilon_2 + \betaiss(2\sigma\upomega,\tauc-\taum)\right) + \sum_{h=1}^H\Delta_{\Iss,\sigma,h}(\pihat;\epsilon,\epsilon_1).
\end{align}
\end{theorem}
Before continuing, let us remark on the parameters and the scaling. Here, $\epsilon$ parametrizes an error scale. $\epsilon_1$ captures both the imitation error, as well as the radius in which $\Delta_{\Iss,\sigma,h}$ is evaluated. $\epsilon_2$ contributes to the upper bound on the $\Imitmarg[\epsilon_1]$ normalized by $1/\sigma$. We recall that $\Imitmarg[\epsilon_1]$ measures probabilities of deviating from the marginal by a magnitude of at most $\epsilon_1$ under optimal couplings.
To drive the upper bound on $\Imitmarg$ to zero, we need (a) $\Delta_{\Iss,\sigma,h}(\pihat;\epsilon,\epsilon_1) \to 0$ , (b) $\epsilon_2/\sigma \to 0$ and (c) $\frac{1}{\sigma}\betaiss(2\sigma\upomega,\tauc-\taum) \to 0$, which requires $\tauc-\taum$ to grow and $\betaiss$ to decay in its second argument. To drive the tolerance $\epsilon_1$ to zero, we require that $2\betaiss(2\gammaiss(\epsilon),0) + 2\upbeta(2\sigma \upomega,0)$ to tend to zero, which requires $\sigma \to 0$ as well. 
\footnote{Notice that $\epsilon_1 \to \Delta_{\Iss,\sigma,h}(\pihat;\epsilon,\epsilon_1)$ is non-increasing, so making $\epsilon_1$ smaller does not increase this term (though making $\epsilon$ smaller does).} The following remark examines the typical scalings of these terms and checks that \eqref{eq:eps_cond_general} is generally easy to satisfy.
\begin{remark} Suppose that for some $c > 0$, $q_1, q_2 \in (0,1]$, we have $\gammaiss(u) = c u^{q_1}$ and $\betaiss(u,\tau) = u^{q_2} \phi(\tau)$ for some decreasing function $\phi$. This is the scaling studied in \cite{pfrommer2022tasil}, and indeed for smooth systems with stabilizable systems, our analysis essentially shows that we can take $q_1=q_2 = 1$ and $\phi(\tau)$ to decay exponentially in $\tau$ as shown in \Cref{app:control_stability}, and which reflects in \Cref{thm:main_template}. For the more general power scalings, \eqref{eq:eps_cond_general} reads
\begin{align}
c^{q_2-1}\phi(\tauc)^{1/q_1}\epsilon^{q_2} \le \epsilon \le \text{constant}.
\end{align}
If $q_2 = 1$ (e.g. the stabilizable, smooth case), then this is satisfied whenever $\epsilon$ is sufficiently small and $\tauc$ is sufficently large. Otherwise, it one has to take $\epsilon \ge \frac{1}{c}\phi(\tauc)^{1/q_1(1-q_2)}$, whch becomes increasingly permissive as $\tau$ is enlarged. Morever, for these scalings, we have $\upomega = \BigOh{\log(1+\sigma/\epsilon^{q_1})}$, $\epsilon_1 = \BigOh{\epsilon^{q_1q_2} + \sigma \upomega} = \epsilon_1 = \BigOhTil{\epsilon^{q_1q_2} + \sigma}$, and $\epsilon_2 = \BigOh{\epsilon^{q_1q_2}}$. In the regime where $q_1=q_2$, this recovers the scaling observed in \Cref{thm:main_template}.
\end{remark}




Similarly, we can generalize \Cref{prop:TVC_main} to the general controller setting.
\begin{theorem}\label{prop:TVC_main_general} Suppose \Cref{asm:tis} holds, and suppose that $\epsilon > 0$ and $\tauc \in \mathbb{N}$  satisfies \eqref{eq:eps_cond_general}.  Then, for any non-decreasing non-negative $\gamma(\cdot)$ and $\gamma$-TVC chunking policy $\pihat$,
\begin{align}
\Imitmarg[\epsilon_1](\pihat) \le H\gamma(\epsilon) +  \sum_{h=1}^H\Delta_{\Iss,\sigma,h}(\pihat;\epsilon,\epsilon_1), \quad \epsilon_1 := 2\betaiss(2\gammaiss(\epsilon),0) \label{eq:TVC_main}
\end{align}
In addition, suppose the expert distribution $\Dexp$ has at most $\taum$-bounded memory (defined formally in \Cref{defn:bounded_memory}). Then  $\Imitjoint[\alpha(\epsilon)](\pihat)$ satisfies the same upper bound \eqref{eq:TVC_main}, where $\Imitjoint[\alpha](\pihat)$, formally defined in \Cref{def:loss_joint}, measures an optimal transport distance between the \emph{joint distribution} of the expert trajectory and the one induced by $\pihat$.
\end{theorem}
The proofs of \Cref{thm:main_template_general,prop:TVC_main_general} are given in the \Cref{app:gen_controllers_proofs}, generalizing the proofs of \Cref{thm:main_template,prop:TVC_main} in the main text, respectively. As with \Cref{prop:TVC_main},  \Cref{sec:no_min_chunk_length} shows that we can replace the condition on the chunk length $\tauc$ on and on $\epsilon$ in \eqref{eq:eps_cond_control} with the condition $\epsilon \le \cgamma$ and the vacuous condition $\tauc \ge 1$, provided that the synthesis oracle produces entire primitive controllers for which the entire sequences $\sfk_{1:T}$ are incrementally stabilizing. 


\subsection{Comparison to prior notions of stability.}
\label{sec:comparison_to_prior}




Prior {theoretical} work in imitation learning focuses either on constraining the learned policy to be stable \cite{havens2021imitation,tu2022sample} or assume\abedit{s} the expert policy is suitably stable \cite{pfrommer2022tasil}. The principal notion of stability used in these prior works is \emph{incremental-input-to-state} stability of the closed-loop system under a deterministic, but possibly sophisticated time-independent controller $\pi: \cX \to \cU$. Importantly, this work considers the imitiation of a \emph{joint distribution} over \emph{sequences} of simple controlers $\sfk$ we call the ``primitive controllers''. These approach necessitate subtle differences in our choice of definitions described below. 

In what follows, we let $\gammaiss$ be a \classK{} function and $\betaiss$ be a \classKL{} function, as described above \Cref{defn:tiss}.
\begin{definition}[Incremental Input-to-State Stability]
We say a policy $\pi: \cX \to \cU$ satisfies \emph{Incremental Input-To-State Stability} ($\delIss$) with moduli $\gammaiss$ and $\betaiss$ if for any two initial conditions $\xi_1, \xi_2 \in \mathcal{X}$, the closed-loop dynamics under policy $\pi: \cal{X} \to \cal{U}$ given by $f_{\textrm{cl}}(\bx_t, \Delta_t) = f(\bx_t, \pi(\bu_t) + \Delta_t)$ satisfies:
    \begin{align}
    \|\bx_t(\xi_1; \delu_{1:\tau}) - \bx_t(\bxi_2; \mathbf{0}_{1:\tau})\| \leq \betaiss(\|\bxi_1 - \bxi_2\|) + \gammaiss\left(\max_{0 \leq s \leq t-1} \|\delu_s\|\right),
    \end{align}
    where $\bx_t(\bxi; \delu_{1:t-1})$ is the state at time $t$ under $f_{\textrm{cl}}$ with $x_0 = \bxi$ and input perturbations $\delu_{1:t-1}$. 

    We say that it satisfies $\pi$ satisfies \emph{local $\delIss$} with parameter $c$ if the above holds for all of identical initial conditions $\bxi_1 = \bxi_2$ (with $\betaiss(0) = 0$) and for $\delu_{1:t-1}$ satisfying $\max_{1 \le s \le t}\|\delu_s\| \le c$.
\end{definition}

 Notice that for $\bxi_1 \ne \bxi_2$, the $\upbeta$-term necessitates that the dynamics converge irrespective of initial condition. Without time-varying dynamics this can only be achieved by a policy which stabilizes to an equilibrium point, as a policy which tracks a reference trajectory is unable to ``forget" the initial condition. Constraining learned policies such that they satisfy this notion of stability is also challenging. Tu et. al. \cite{tu2022sample} attempt to do so through regularization while Haven et. a. \cite{havens2021imitation} use matrix inequalities to satisfy this stability property under linear dynamics.   Pfrommer et. at. \cite{pfrommer2022tasil} avoid this difficulty only requiring local incremental stability. This weaker notion of incremental stability simply postulates the existence of a (local) input-perturbation to state-perturbation gain function $\gammaiss$. Since this stability property does not necessitate convergence across with different initial conditions and only under input perturbations of magnitude $\leq c$, this only necessitates that the expert policy can correct from small input perturbations.

% Figure environment removed

\paragraph{Comparing local $\delIss$ and \Cref{defn:tiss}.}  As stated above, past work consider imitation of a fixed, but possibly complex deterministic controller $\pi$. In contrast, we imititate joint distributions over \emph{sequences} of primitive controllers $\seqa = (\sfk_{1:\tau})$.  Moreover, our ``primitive'' controllers are intended to be much simpler than the policy $\pi$ considered in past work; e.g. the affine controllers consided in the body in this work. Indeed, the real ``policy'' we try to imitate is potentially very complex expert distribution $\Dexp$, and these primitive controllers serve to stablize to this distribution. To account for these differences, we modify the local stability considered by Pfrommer et al. \cite{pfrommer2022tasil} in three respects.
\begin{itemize}
    \item Our notion of stability,  \Cref{defn:tiss}, is applied to fixed-length sequences of controllers $\seqa = (\sfk_{1},\dots,\sfk_{\tau})$; past notions of incremental stability are for time-varying controllers and are infinite horizon.
    \item \Cref{defn:tiss} only requires that our notion of incremental stability holds for initial conditions $\bxi$ in a radius $c_{\xi}$ of a nomimal initial condition $\bxi_0$. The reason for this can be seen by considering jus tthe affine primitive controllers studied in the body: time-varying feedback that stabilizes the linearization of a smooth dynamical system is only stabilizing of the actual system in a tube around the nominal trajectory. 
    \item Unlike the local notion of $\delIss$ considered in Pfrommer et al. \cite{pfrommer2022tasil}, we \emph{do} require considering stability from different initial conditions $\bxi_1 \ne \bxi_2$. This is because we re-apply incremental stability at each chunk $h$, and must account for imitation error accumulated up to that point. 
\end{itemize}

\paragraph{The power of a hierarchical approach to stability.} Through the introduction of a synthesis oracle which can generate locally stabilizing primitive controllers, we decouple the stability properties of the expert's behavior  from the stabilizability of the underlying dynamical system. 
%We further weaken this assumption, which we formalize in \Cref{asm:Jacobian_Stable} and abstract to the composite MDP through \Cref{defn:stability_setup}, by only requiring that a locally stabilizing controller can be synthesized per-demonstration. 
This allows for reasoning about generalization in the presence of bifurcations or conflicting demonstrations, which is precluded by local $\delIss$ since an expert policy cannot simultaneously stabilize to multiple branches of a bifurcation.  For a concrete example, consider \Cref{fig:bifurcation}.  Indeed, continuity is the \emph{sine qua non} of stability and the example given demonstrates the necessity of augmentation to enforce the former.  In detail, the figure illustrates an example where an agent is navigating around an obstacle, providing a bifurcation.  Without augmentation, the demonstrator trajectories always navigate around the obstacle in the direction closer to their starting point, leading to a sharp discontinuity along a bisector of the obstacle.  On the other hand, the data augmentations allow for the policy to have some probability of navigating around the obstacle in the ``wrong'' direction, which leads to the notion of continuity we consider: total variation continuity.


% Figure environment removed
Because our notion of stability is applied in chunks, our theory is sufficiently flexible so as to allow for the learned policy to switch between expert demonstrations in a manner preserving the marginal distributions but not consistent with the joint distribution across the entire trajectory.  This flexibility is illustrated in \Cref{fig:figure_eight}, where we suppose that the demonstrator distribution consists both of trajectories traversing a figure ``8'' consistently in either a clockwise or counter-clockwise manner, with both orientations represented in the data set.  Due to the multi-modality at the critical point in the trajectory, there is ambiguity about which loop to traverse next; specifically, there may exist a policy that randomly select which loop to traverse each time the critical point is visited in such a way that the marginal distributions on states and actions is the same as that induced by the demonstrator.  Such a policy will, by definition, preserve the correct \emph{marginal} distributions across states and actions; at the same time, this policy has a different \emph{joint} distribution across all time steps from the demonstrator due to the possibility of traversing the same loop twice in a row.