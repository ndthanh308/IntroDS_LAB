%!TEX root = ../main.tex






\section{Analysis Overview}\label{sec:analysis}

Our analysis abstracts away the vector-valued dynamics into a deterministic MDP -- the \bfemph{composite MDP} -- with \bfemph{composite-states} $\seqs \in \cS$ and \bfemph{composite-actions} $\seqa \in \cA$, corresponding to trajectory-chunks and composite-actions in \Cref{sec:setting}. We abstract away our dynamics as
\begin{align}
\seqs_{h+1} = F_h(\seqs_h,\seqa_h), \quad h \in \{1,2,\dots,H\} \label{eq:abstract_dynamics}
\end{align}
%We now describe the abstract MDP in detail before explaining its relationship to our control setting.
  A \bfemph{composite-policy} $\pi$ is a sequence of kernels $\pi_{1},\pi_2,\dots,\pi_H: \cS \to \laws(\Seqa)$.  We let $\Dinit$ denote the distribution of initial state $\seqs_1 $, and $\Dist_{\pi}$ denote the distribution of $(\seqs_{1:H+1},\seqa_{1:H})$ subject to $\seqs_1 \sim \Dinit$, $\seqa_h \mid \seqs_{1:h},\seqa_{1:h-1} \sim \pi_h(\seqs_h)$, and the composite-dynamics \eqref{eq:abstract_dynamics}. We assume that we have an optimal policy $\pist$ to be imitated, and define $\Psth$ as the marginal distribution of $\seqs_h$ under $\Dist_{\pist}$; ultimately, we shall take $\pist$ to be the policy defined in \Cref{defn:Dexph}. 
\subsection{Structure of the proof.} 
We begin by explaining key objects, stability and continuity properties required in the composite MDP. Then, \Cref{sec:control_instant_body} relates the composite MDP to our original setting by taking composite-states $\seqs_h = \pathc$ as chunks, and taking composite actions  as sequences of primitive controllers $\seqa_h = \sfk_{t_h:t_{h+1}-1}$  as in \Cref{sec:setting}. We also explain why relevant stability and continuity conditions are met. Finally, we derive \Cref{thm:main_template} from a generic guarantee for smoothed imitiation learning in the composite MDP, \Cref{thm:smooth_cor}, and from sampling guarantees in \Cref{app:scorematching}.


 %Finally, \mscomment{??}
%We break the proof of \Cref{thm:main} into two parts.  In the first, we relate the composite MDP described above back to the control setting of interest and demonstrate stability of the composite dynamics.  In the second, we show that under the stability assumptions proved in the first part, we can derive a bound on the imitation loss.  \Cref{thm:main} follows by combining these two results with \Cref{prop:samplingguarantee}; a detailed proof can be found in Appendix \ref{app:end_to_end}.

We consider two pseudometrics on the space $\cS$: $\dists,\disttvc: \cS^2 \to \R_{\ge 0}$, and a function $\phia: \cA^2 \to \R_{\ge 0}$. For convenience, \emph{do not require $\phia$ to satisfy the axioms of a pseudometric.} We use $\dists$ and $\phia$ to measure error between states and actions, respectively, and $\disttvc(\cdot,\cdot)$ for a probabilistic continuity property described below.  
In terms of $\dists$ and $\phia$, we consider three measures of imitation error: error on the (i) joint distribution of trajectories ($\gapjoint$) (ii) marginal distribution of trajectories ($\gapmarg$) and (iii) one-step error in actions ($\drob$). Formally: 
\begin{definition}[Imitation Errors]\label{defn:imit_gaps} Given an error parameter $\epsilon > 0 $, define the \bfemph{joint-error} \iftoggle{arxiv}{
\begin{align}
\gapjoint(\polhat \parallel \pist) := \inf_{\coup_1}\Pr_{\coup_1}\left[\max_{h \in [H]}\max\{\dists(\sstar_{h+1},\shat_{h+1}),\phia(\seqast_h,\seqahat_h)\}  > \epsilon\right],
\end{align}
}{$\gapjoint(\polhat \parallel \pist) := \inf_{\coup_1}\Pr_{\coup_1}\left[\max_{h \in [H]}\max\{\dists(\sstar_{h+1},\shat_{h+1}),\phia(\seqast_h,\seqahat_h)\}  > \epsilon\right]$,} where the first infimum is over trajectory couplings $((\shat_{1:H+1},\seqahat_{1:H}),(\sstar_{1:H+1},\seqa^\star_{1:H})) \sim \coup_1 \in \couple(\Dist_{\polhat},\Dist_{\polst})$ satisfying $\Pr_{\coup_1}[\shat_{1} = \sstar_1] = 1$.   
Define the \bfemph{marginal error} \iftoggle{arxiv}{
  \begin{align}
  \gapmarg(\polhat \parallel \pist) := \max_{h \in [H]}\max\left\{\inf_{\coup_1}\Pr_{\coup_1}[\dists(\sstar_{h+1},\shat_{h+1}) > \epsilon],\,\inf_{\coup_1}\Pr_{\coup_1}[\phia(\seqast_h,\seqahat_h) > \epsilon] \right\}
  \end{align}
}{$\gapmarg(\polhat \parallel \pist) := \max_{h \in [H]}\{\inf_{\coup_1}\Pr_{\coup_1}[\dists(\sstar_{h+1},\shat_{h+1}) > \epsilon],\inf_{\coup_1}\Pr_{\coup_1}[\phia(\seqast_h,\seqahat_h) > \epsilon] \}$} to be the same as the to joint-gap, with the ``$\max$'' outside the probability and $\inf$ over couplings. Lastly, 
define the \bfemph{one-step error}  
\iftoggle{arxiv}{
  \begin{align}
  \drob(\polhat_h(\seqs) \parallel \polst_h(\seqs)) := \inf_{\coup_2}\Pr_{\coup_2}\left[\phia(\seqahat_h,\seqast_h) \le \epsilon \right],
  \end{align}
}{$\drob(\polhat_h(\seqs) \parallel \polst_h(\seqs)) := \inf_{\coup_2}\Pr_{\coup_2}\left[\phia(\seqahat_h,\seqast_h) \le \epsilon \right]$,} where the infimum is over $(\seqast_h, \hat \seqa_h) \sim \coup_2 \in \couple( \polhat_h(\seqs),\pol_h^\star(\seqs))$. 

\end{definition}

\nipspar{Stability.}  Our hierarchical approach offloads stability of stochastic $\pist$ onto that of its composite-actions $\seqa_h$, instantiated as \emph{primitive controllers} (not raw inputs!). This allows us to circumvent more challenging incremental senses of stability (see 
\Cref{sec:comparison_to_prior} for further discussion).
%We leverage the stability of primitive controllers by assuming $\pist$ in the composite MDP is stable in the following sense (see \Cref{app:related} for extensive comparison to other notions of stability).
\begin{definition}[Input-Stability] \label{defn:fis} A trajectory $(\seqs_{1:H+1},\seqa_{1:H})$ is \bfemph{input-stable} if  all sequences $\seqs_1' = \seqs_1$ and $\seqs_{h+1}' = F_h(\seqs_h',\seqa_h')$ satisfy  \iftoggle{arxiv}{
  \begin{align}
  \dists(\seqs_{h+1}',\seqs_{h+1}) \vee \disttvc(\seqs_{h+1}',\seqs_{h+1}) \le  \max_{1 \le j \le h}\phia\left(\seqa_{j}',\seqa_j\right),~ \forall h \in [H]
  \end{align}
}{$\dists(\seqs_{h+1}',\seqs_{h+1}) \vee \disttvc(\seqs_{h+1}',\seqs_{h+1}) \le  \max_{1 \le j \le h}\phia\left(\seqa_{j}',\seqa_j\right),~ \forall h \in [H]$.} A policy $\pi$ is \bfemph{input-stable} if $(\seqs_{1:H},\seqa_{1:H}) \sim \Dist_{\pi}$ is \bfemph{input-stable} almost surely.
\end{definition}


%Recall that a probability kernel maps one space to distributions on another; in particular, for a policy $\pi$, $\pi_h:\cS \to \laws(\cA)$ are probability kernels.  


\iftoggle{arxiv}{\nipspar{Total Variation Continuity.}}{\nipspar{TVC.}} Continuity of probability kernels and policies in TV distance are measured in terms of $\disttvc$.
\begin{definition}\label{defn:tvc} For a measure-space $\cX$ and non-decreasing $\gamma: \R_{\ge 0} \to \R_{\ge 0}$, we call a probability kernel $\lawW: \cS \to \laws(\cX)$ $\gamma$-\bfemph{total variation continuous ($\gamma$-TVC)} if, for all $\seqs,\seqs' \in \cS$, \iftoggle{arxiv}{
  \begin{align}
  \tvargs{\lawW(\seqs)}{\lawW(\seqs')} \le \gamma(\disttvc(\seqs,\seqs')).
  \end{align}
}{$\tvargs{\lawW(\seqs)}{\lawW(\seqs')} \le \gamma(\disttvc(\seqs,\seqs'))$.} A policy $\pi$ is \bfemph{$\gamma$-TVC} if $\pi_h:\cS \to \laws(\cA)$ is $\gamma$-TVC \iftoggle{arxiv}{for all $h \in [H]$}{$\forall h \in [H]$.}
\end{definition}


%The following specifies the correspondences between our abstraction and the original control setting. 
%\begin{definition}[Imitation Error]
%\end{definition}

%We fix a parameter $K \ge K_0$ assuming, for simplicity, that $H = \frac{T}{K}+1 \in \N$. Define $t_h = (h-2)K+1$. We set $\cS = \cX^{K+1} \times \cU^{K}$ and $\cA = \cK^H$. Actions are given by $\seqa_h = \gain_{t_{h+1}:t_{h+2}-1}$. States are $\seqs_h = (x_{t_{h}:t_{h+1}},u_{t_h:t_{h+1}-1})$ for $h > 1$, with initial state $\seqs_1 = ((0,\dots,x_1),(0,\dots,0))$.Note that composite-states $\seqs_{h+1}$ and $\seqs_h$ overlap at control-state $x_{t_{h+1}}$, but control-inputs do not overlap. See \mscomment{??} for explanation. The dynamics $F_h(\seqs_h,\seqa_h)$ are determined by \eqref{eq:dynamics} and $u_{t} = \gain_{t}(x_{t})$. We let $\Dinit$ denote the distribution of $\seqs_1$ induced by $x_1 \sim \Dxone$.


\nipspar{Data Noising.} In Appendix \ref{app:no_augmentation}, we show that under the strong condition that the learned policy $\pihat$ is $\gamma$-TVC, then $\toda$ with no noise injection ($\sigaug = 0$) learns the distribution.  Frequently, however, $\pihat$ may not satisfy this condition, such as when the ground truth $\pist$ is not also TVC. We circumvent this by introducing a \emph{smoothing kernel} $\Wsig: \cS \to \laws(\cS)$ that corresponds to the data noising procedure; in $\toda$ we let the kernel be a Gaussian, sending $\pathm$ to $\cN(\pathm,\sigma^2 \eye) \in \laws(\scrP_{\pathm})$. We will thus be able to replace TVC of $\pihat$ with TVC of $\Wsig$. We now introduce a few key objects.

\begin{definition}\label{defn:body_replica} Given a policy $\pi$, we define its \bfemph{smoothed policy} $\pi \circ \Wsig$ via components $(\pi \circ \Wsig)_h = \pi_h \circ \Wsig: \cS \to \laws(\cA)$. For $\pist$ fixed, define the \bfemph{smoothed distibution} $\Paugh$ as the joint distribution over $(\sstar_h \sim \Pst_h, \seqa^\star_h \sim \pist_h(\sstar_h), \sstartil_h \sim \Wsig(\sstar_h))$, with $\seqa^\star_h \perp  \sstartil_h \mid \sstar_h$. The \bfemph{deconvolution policy} $\pidec$ is defined by letting $\pidech(\seqs)$ denote the distribution of $\seqa^\star_h \mid \sstartil_h = \seqs_h$, where $\seqa^\star_h,\sstartil_h$ are drawn from $\Paugh$. Finally, the  \bfemph{replica policy} is $\pistrep = \pidec \circ \Wsig$. 
\end{definition}
%\begin{definition}[Smoothed and Replica Policies] Let a smoothing kernel $\Wsig$ and optimal $\pist$ be given, inducing distributions $\Psth \in \laws(\cS)$. For any policy $\pihat$, define $\pihat_{\sigma}$ as the policy with $\pihat_{\sigma,h} = \pihat_h(\cdot) \circ \Wsig(\cdot)$. Further, define the \emph{deconvolution kernel} $\Wdeconvh(\seqs) := \Law(\tilde\seqs \mid  \tilde \seqs \sim \Psth, \tilde \seqs' \sim \Wsig(\tilde \seqs), \tilde \seqs' = \seqs)$ and \emph{replica kernel} $\Wreph = \Wdeconvh \circ \Wsig$, as well as the \emph{replica policy} $\pistrep$ via $\pistreph = \pist_h\circ \Wreph$. \end{definition}
The operator $\pi \circ \Wsig$ composes $\pi$ with the smoothing kernel. The deconvolution policy $\pidec$ captures the distribution of actions under $\pist$ given a smoothed state, and corresponds to $\pidec = (\pidech)_{h=1}^H$. We argue that if a policy $\pihat$ approximates $\pidec$ at each step, then $\pihat \circ \Wsig$ imitates $\pistrep = \pidec \circ \Wsig$. We explain the ``replica policy'', and importance of imitating it, after we state our main theorem. First, we define a notion of stability to smoothing, taking $\disttvc,\dists,\dista$ as given.%(\mscomment{again, differing probabilistic variants to the appendix}. 
%The name ``replica policy'' comes from the fact, shown in the appendix, that $\pistreph \circ \Psth = \pisth \circ \Psth$; that, is the \emph{marginal } distribution of actions obtained by applying $\pistreph$ to $\sstar_h \sim \Psth$ is the same as those obtained by applying the true policy $\pist$. 

\begin{definition}\label{defn:ips_body}
For a non-decreasing maps $\gamipsone,\gamipstwo:\R_{\ge 0} \to  \R_{\ge 0}$ a  pseudometric $\distips:\cS \times \cS \to \R$ (possibly other than $\dists$ or $\disttvc$), and $\rips > 0$, we say a policy $\pi$ is \emph{$(\gamipsone,\gamipstwo,\distips,\rips)$-input-\&-process stable (IPS)} if the following holds for any $r \in [0,\rips]$. Consider any  sequence of kernels $\lawW_1,\dots,\lawW_H:\cS \to \laws(\cS)$ satisfying $\max_{h,\seqs \in \cS}\Pr_{\tilde \seqs\sim \lawW_h(\seqs)}[\distips(\tilde \seqs,\seqs) \le r] = 1$, and define a process $\seqs_1 \sim \Dinit$, $\tilde\seqs_h \sim \lawW_h(\seqs_h),\seqa_h \sim \pi_h(\tilde \seqs_h)$, and $\seqs_{h+1} := F_h(\seqs_h,\seqa_h)$. Then, almost surely, \iftoggle{arxiv}{
\begin{itemize}
  \item[(a)] The sequence $(\seqs_{1:H+1},\seqa_{1:H})$ is input-stable.
  \item[(b)] It holds that $\max_{h \in [H]} \disttvc(F_h(\tilde\seqs_h,\seqa_h),\seqs_{h+1}) \le \gamipsone(r)$.
  \item[(c)] It holds that $\max_{h \in [H]} \dists(F_h(\tilde\seqs_h,\seqa_h),\seqs_{h+1}) \le \gamipstwo(r)$.
\end{itemize}
}{(a) the sequence $(\seqs_{1:H+1},\seqa_{1:H})$ is input-stable (b) $\max_{h \in [H]} \disttvc(F_h(\tilde\seqs_h,\seqa_h),\seqs_{h+1}) \le \gamipsone(r)$ and (c) $\max_{h \in [H]} \dists(F_h(\tilde\seqs_h,\seqa_h),\seqs_{h+1}) \le \gamipstwo(r)$.}
\end{definition}
Condition $(a)$ means that the policy  $\tilde \pi$ defined by $\tilde \pi_h = \pi_h \circ \lawW_h$ is input-stable. In the appendix, we instantiate $\lawW_{1:H}$ not as $\Wsig$, but as (a truncation of) \emph{replica kernels} $\Qreph$ for which $\pistreph = \pisth \circ \Qreph$.  We show that the replica kernel inherits any concentration satisfied by $\Wsig$, ensuring (via truncation) that $\Pr_{\tilde \seqs\sim \lawW_h(\seqs)}[\distips(\tilde \seqs,\seqs)] \le r$. Conditions (b \& c) merely require that one-step dynamics are robust to small changes in state, measured in terms of both $\disttvc$ and $\dists$.


\subsection{Instantiation for control}\label{sec:control_instant_body}
\iftoggle{workshop}{
  % Figure environment removed
}{}
\end{comment}
\subsection{A Guarantee in the Composite MDP, and the derivation of \Cref{thm:main}}\label{ssec:gen_guarantee}
With the substitutions in \Cref{sec:control_instant_body}, it suffices to prove an imitation guarantee in the composite MDP, assuming $\pist$ is IPS, and $\pihat$ is close to $\pidec$ in the appropriate sense.
\begin{theorem}\label{thm:smooth_cor}  Suppose $\pist$ is $(\gamipsone,\gamipstwo,\distips,\rips)$-IPS and $\Wsig$ is $\gamma_{\sigma}$-TVC. Let $\epsilon > 0$, $r \in (0,\frac{1}{2}\rips]$; define $p_r := \sup_{\seqs}\Pr_{\seqs' \sim \Wsig(\seqs)}[\distips(\seqs',\seqs) >  r]$ and $\epsilon' := \epsilon+\gamipstwo(2r)$. Then, for any policy $\pihat$,  both  $\gapjoint (\pihat \circ \Wsig \parallel \pistrep)$ and  $\gapmarg[\epsilon'] (\pihat \circ \Wsig \parallel \pist)$ are upper bounded by \iftoggle{workshop}{
  \begin{align}
    &H\left(2p_r +  3\gamma_{\sigma}(\max\{\epsilon,\gamipsone(2r)\})\right)  \\
    &+ \textstyle \sum_{h=1}^H\Exp_{\sstar_h \sim \Psth}\Exp_{\sstartil_h \sim \Wsig(\sstar_h) } \drob( \pihat_{h}(\sstartil_h) \parallel \pidec(\sstartil_h))  . \label{eq:smooth_ub}
    \end{align}
}{
\begin{align}
H\left(2p_r +  3\gamma_{\sigma}(\max\{\epsilon,\gamipsone(2r)\})\right)  + \sum_{h=1}^H\Exp_{\sstar_h \sim \Psth}\Exp_{\sstartil_h \sim \Wsig(\sstar_h) } \drob( \pihat_{h}(\sstartil_h) \parallel \pidec(\sstartil_h))  . \label{eq:smooth_ub}
\end{align}
}
\end{theorem}
\begin{proof}[Deriving \Cref{thm:main_template} from \Cref{thm:smooth_cor}] A full proof is given in \Cref{app:end_to_end}. The key steps are using the stability guarantee of \Cref{prop:ips_instant}, the aforementioned TVC-bound on $\Wsig$, and Gaussian concentration to bound $p_r$ with the bound in \Cref{thm:smooth_cor} to conclude.  Moreover, we can show that, with $\sstartil_h = \pathmtil$,
\begin{align}
\sum_{h=1}^H\Exp_{\sstar_h \sim \Psth}\Exp_{\sstartil_h \sim \Wsig(\sstar_h) } \drob( \pihat_{h}(\sstartil_h) \parallel \pidec(\sstartil_h)) = \sum_{h=1}^H\Exp_{\pathmtil \sim \cDhsig} \left[\Delta_{(\epsilon^2)}\left(\pidecsigh\left(\pathmtil\right),\,\pihat_h\left(\pathmtil\right) \right)\right].
\end{align}
Thus, \Cref{thm:smooth_cor} provides the desired guarantee for imitating the policy $\pist$ constructed in \Cref{sec:control_instant_body}. We conclude with a subtle but powerful observation: that $\pist$ as constructed has trajectories with the same marginals (but possibly different joint distributions) as $\ctraj_T \sim \Dexp$. 

%Recall the dimension $d$ in \Cref{thm:main}; we take $r = \sigma\sqrt{d\log(1/p)}$, which allows us to bound $ p_r \le p$ by Gaussian concentration. As noted above, we take $\gamma_{\sigma}(u) = \frac{u}{2\sigma}$. Hence, via \Cref{prop:ips_instant}, we can take $\epsilon' = \epsilon + c_5\sigma\sqrt{d\log(1/p)}$ and $\gamma_{\sigma}(\gamipsone(2r)) \le c_5\sqrt{d\log(1/p)}\exp(-\eta(\tauc - \taum)/\Lstab)$. \footnote{Notice $\gamma_{\sigma}(\gamipstwo(2r)) \sim \sqrt{d\log(1/p)}$, which is why we differentiate between $\gamipsone$ and $\gamipstwo$.} These substitutions complete the proof.
\end{proof}

\input{body/arxiv_proof_sketch}


\begin{comment}
  \begin{proof}[Proof Sketch of \Cref{thm:smooth_cor}] 
  The proof draws inspiration from the notion of replica symmetry in statistical physics (hence, the name replica) \citep{mezard2009information}. 

  

    We construct a coupling between a trajectory over $(\srep_h, \arep_h)$ sampled using the replica policy $\pistrep$, and a trajectory $(\shat_h, \seqahat_h)$  sampled from $\pihat_{\sigma}$.%To do so, we introduce a \emph{relica kernel}  $\Qreph: \cS \to \laws(\cS)$, defined so that $\pireph = \pisth \circ \Qreph$, constructed by analogy to the replica policy in \Cref{defn:body_replica}. 
  We introduce \emph{teleporting}  trajectories $\srep_{h+1} = F_h(\srep_h, \arep_h)$, and $\stel_{h+1} = F_h(\ssq_h, \atel_h)$, where $\ssq_h$ is sampled from the replica distribution of $\stel_h$ and $\atel_h \sim \pist_h(\ssq_h)$; in words, $\stel_h$ \emph{teleports} to an independent and identically distributed copy conditional on the noise agumentation, and draws an action from the expert policy evaluated on the new state.


   %$(\stel_h, \atel_h)$ and trajectories $(\stel_h, \atelinter_h)$ and $(\srep_h, \arepinter_h)$ which interpolate between the teleporting and replica trajectories.  We sample $\shat_1 = \srep_1 = \stel_1$  according to $\Dinit$ and we let $\shat_{h+1} = F_h(\shat_h, \ahat_h)$,  To define the actions, we let $\arep_h$ be sampled from $\pistreph[h](\srep_h)$, $\ahat_h$ from $\pihatsigh[h](\shat_h)$, $\atelinter_h$ from $\pihatsigh[h](\stel_h)$, and $\arepinter$ from $\pihatsigh[h](\srep_h)$.  

  The key fact of the replica distribution is that it preserves marginals, meaning that all $\stel_h$ and $\ssq_h$ both have marginals according to $\Psth$. We show that $\srep_h$ tracks the teleporting trajectories, up to the IPS terms $\gamipsi$ and concentration of the kernel, due to total variation continuity of $\Wsig$.  Because the marginals of $\stel_h$ are distributed according to $\Psth$,  we can argue that a (fictitious) action $\atelinter_h \sim  (\pihat_{h} \circ \Wsig)(\stel_h)$ is close to $\atel_h$ (by the data processing inequality, it is bounded by the closeness of $\pihat_h$ and $\pidech$ on $\ssq_h \sim \Wsig(\stel_h)$, $\stel_h \sim \Psth$). We then use total variation continuity to relate to another fictious action $\arepinter_h$ to $\arep_h$. Finally, we use input-stability and TVC again, to relate $\arepinter_h$ to actions $\seqahat_h \sim (\pihat_{h}\circ \Wsig)(\shat_h)$. Our couplings are summarized in the following diagram:\iftoggle{workshop}{
    \begin{align}
      &\underbrace{(\arep \leftrightarrow \atel)}_{\gamtvc \text{ and induction}} \to \underbrace{(\atel \leftrightarrow \atelinter)}_{\text{learning and sampling}} \\
      &\to \underbrace{(\atelinter \leftrightarrow \arepinter)}_{\gamtvc \text{ and induction}} \to \underbrace{(\arepinter \leftrightarrow \seqahat)}_{\gamtvc \text{ and induction}}. \label{eq:mainproofoutlinebody}
    \end{align}
  }{
  \begin{align}
    \underbrace{(\arep \leftrightarrow \atel)}_{\gamtvc \text{ and induction}} \to \underbrace{(\atel \leftrightarrow \atelinter)}_{\text{learning and sampling}} \to \underbrace{(\atelinter \leftrightarrow \arepinter)}_{\gamtvc \text{ and induction}} \to \underbrace{(\arepinter \leftrightarrow \seqahat)}_{\gamtvc \text{ and induction}}. \label{eq:mainproofoutlinebody}
  \end{align}
  }
  We construct conditional couplings between pairs of the aforementioned trajectories, each of which corresponds to a certain optimal transport cost. That past trajectories can be associated to optimal couplings measurably is non-trivial, and proven in \Cref{prop:MK_RCP}. To conclude, we apply a measure theoretic result (\Cref{lem:couplinggluing}) to ``glue'' the pairwise couplings together and establish the main result. The full proof is given in \Cref{sec:imit_composite}, relying on measure-theoretic details in \Cref{app:prob_theory}.
\end{proof}
\end{comment}

\vspace{-.5em}