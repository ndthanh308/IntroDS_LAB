
%!TEX root = ../main.tex

\newcommand{\Phicl}[1]{\bm{\Phi}_{\mathrm{cl},#1}}
\newcommand{\Cnu}{\nu}
\newcommand{\Ctheta}{C_{\Theta}}

\newcommand{\augmentflag}{\bm{\mathtt{augment}}}
\newcommand{\Nsample}{N_{\texttt{exp}}}
\newcommand{\Naug}{N_{\texttt{aug}}}

\newcommand{\trueflag}{\texttt{true}}
\newcommand{\falseflag}{\texttt{false}}
\newcommand{\sigaug}{\sigma}


\newcommand{\Dexpbar}{\bar{\cD}_{\mathrm{exp}}}

\section{Conditional sampling with stabilization suffices for behavior cloning}\label{sec:results}
\newcommand{\tiss}[1][T]{\mathsf{t}\text{-}\textsc{Iss}}
\newcommand{\dmax}{\dist_{\max}}
\newcommand{\couphatsigh}{\couple_{\sigma,h}(\pihat)}

\newcommand{\delu}{\updelta \bu}
\newcommand{\delx}{\updelta \bx}


We show that trajectories of the form given in \Cref{def:expert} can be efficiently imitated if (a) we are given a \emph{synthesis oracle}, described below, that produces low-level control policies that locally stabilize chunks of the trajectory with primitive controllers and (b) we can learn to generate certain appropriate distributions over composite actions, i.e. sequences of primitive controllers.   All the following results apply to affine primitive controllers introduced in \Cref{sec:setting} and assume that the system dynamics are second-order smooth and locally stabilizable. In \Cref{app:gen_controllers}, we show that our results still hold with general families of parametric primitive controllers, provided that these controllers induce the same local stability guarantee.


\paragraph{The synthesis oracle.} We say { primitive controller (cf. \Cref{sec:setting}) $\sfk_{1:T} \in \cK^T$ is  \emph{consistent with} a trajectory $\ctraj = (\bx_{1:T+1},\bu_{1:T})\in \Ctraj_T$  if $\bbarx_t = \bx_t$ and $\bbaru_t = \bu_t$ for all $t \in [T]$; note that this implies that $\sfk_t(\bx_t)=\bu_t$ for all $t$.  A \bfemph{synthesis oracle} $\synth$ maps $\Ctraj_T \to \cK^T$ such that, for all $\ctraj_T \in \Ctraj_T$, $\sfk_{1:T} = \synth(\ctraj_T)$ is consistent with $\ctraj_T$. For our theory, we assume access to a {synthesis oracle} at training time, and assume the ability to estimate conditional distributions over joint sequences of primitive controllers; \Cref{app:control_stability} explains how this can be implemented by solving Ricatti equations if dynamics are known (e.g. in a simulator), smooth, and stabilizable. In our experimental environment, control inputs are desired robot configurations, which the simulated robot executes by applying feedback gains. Reiterating \Cref{rem:primitive_controllers_natural}, learned or hand-coded low-level controllers are popular in practical implementations of generative behavior cloning. We discuss the merits of studying imitation learning with a synthesis oracle  in depth in \Cref{sec:merits_synthesis}.

\paragraph{Notions of distance.} While restricting ourselves to affine primitive controllers, our approximation error of generative behavior cloner is measured in terms of optimal transport distances that use the following ``maximum distance.'' Given two composite actions $\seqa = (\bbaru_{1:\tauc}, \bbarx_{1:\tauc}, \bbarK_{1:\tauc})$ and $\seqa' = (\bbaru_{1:\tauc}', \bbarx_{1:\tauc}', \bbarK_{1:\tauc}')$, we define
\begin{align}
\dmax(\seqa,\seqa') := \max_{1\le k \le \tauc}(\|\bbaru_{k}-\bbaru_{k}'\| + \|\bbarx_{k}-\bbarx_{k}'\| +\|\bbarK_{k}-\bbarK_{k}'\|). \label{eq:dmax}
\end{align}
Distances between policies are defined via natural optimal transport costs.
Given two policies $\pi = (\pi_h),\pi' = (\pi_h')$ and observation chunk $\pathm$, we define an induced optimal transport cost
\begin{align}
\Delta_{\epsilon}(\pi_h(\pathm),\pi_h'(\pathm)) := \inf_{\coup}\Pr_{(\seqa_h,\seqa_h')\sim \coup}\left[\dmax(\seqa_h,\seqa_h') > \epsilon\right],
\end{align}
where the $\inf_{\coup}$ denotes the infinum over all couplings between $\seqa_h \sim \pi_h(\pathm)$ and $\seqa_h' \sim \pi_h'(\pathm)$. $\Delta_{\epsilon}$ corresponds to a relaxed  L\'evy-Prokhorov metric \citep{strassen1965existence}, and can always be bounded, via Markov's inequality, by
\begin{align}
\Delta_{\epsilon}(\pi_h,\pi_h' \mid \pathm)  \le \frac{1}{\epsilon}\cW_{1,\dmax}(\pi_h(\pathm),\pi_h'(\pathm)),
\end{align}
where $\cW_{1,\dmax}(\pi_h(\pathm),\pi_h'(\pathm))$ denotes the $1$-Wasserstein distance between $\seqa_h \sim \pi_h(\pathm)$ and $\seqa_h' \sim \pi_h'(\pathm)$. 
 






\subsection{Incremental Stability and the Synthesis Oracle.}\label{sec:stab_defs} Our key assumption is that the synthesis oracle above produces \emph{incrementally stabilizing} control gains, in the sense first proposed by  \cite{angeli2002lyapunov}. Incremental stability has emerged as a natural desirable property for imitation limitation \citep{pfrommer2022tasil,tu2022sample,havens2021imitation}, because it forces the expert to be robust to small perturbations of their policy. 
% Figure environment removed

We now supply a formal definition, depicted in \Cref{fig:incremental_stab}. Given a primitive controller $\sfk: \R^{\dimx} \to \R^{\dimu}$, define the closed loop dynamic map $\fclkap(\bx,\delu) := f(\bx,\sfk(\bx)+\delu)$. Thus, composite action $\seqa$ is \emph{consistent} with a trajectory chunk $\seqs = (\bx_{1:\tauc+1},\bu_{1:\tau})$ if $\bx_{t+1} = \fclkap[t](\bx,\bzero)$ for $1 \le t \le \tauc$.\footnote{Below, we recall definitions of classes of comparison functions in nonlinear control \cite{khalil2002nonlinear} as follows: we say a univariate function $\gammaiss:\R_{\ge 0} \to \R_{\ge 0}$ is \emph{\classK}~if it is strictly increasing and satisfies $\gammaiss(0) = 0$. We say a bivariate function $\betaiss:\R_{\ge 0} \times \Z_{\ge 0} \to \R_{\ge 0}$ is \emph{\classKL}~if $x \mapsto \betaiss(x,t)$ is \classK~for each $t \ge 0$, and $t \mapsto \betaiss(x,t)$ is nonincreasing in $t$.}

\begin{restatable}[Time-Varying Incremental Stability]{definition}{tissdef}\label{defn:tiss} 
Let $\gammaiss(\cdot)$ be a \classK{} function, $\betaiss(\cdot,\cdot)$ be \classKL{} function, and let $\seqa = (\sfk_{1},\sfk_2,\dots,\sfk_{\tau})$ denote a sequence of primitive controllers (i.e. a composite action when $\tau = \tauc$). Given a sequence of input  perturbations $\delu_{1:\tau} \in (\R^{\dimu})^{\tau}$ and initial condition $\bxi \in \R^{\dimx}$, let $\xa_{i+1}(\delu_{1:\tau},\bxi) = \fclkap[i](\xa_{i}(\delu_{1:\tau},\bxi),\delu_i)$,  with $\xa_1 = \bxi$. We say that composite action $\seqa$ is time-varying incrementally input-to-state stable ($\tiss$) with moduli $\gammaiss(\cdot),\betaiss(\cdot,\cdot)$ if 
\begin{align}
\forall  \bxi,\bxi' \in \R^{\dimx}, 0 \le i \le \tau,  \quad \|\xa_{i}(\bm{0}_{1:\tau},\bxi)-\xa_{i}(\delu_{1:\tau},\bxi')\| \le \betaiss(\|\bxi-\bxi'\|,\tau) + \gammaiss\left(\max_{1 \le s \le i-1}\|\delu_s\|\right) \label{eq:tiss}
\end{align}
Given parameters $\cgamma,\cxi>0$ we say that $\seqa$ is local-$\tiss$ at $\bxi_{0}$ if the above holds only for all $\bxi,\bxi',\delu_{1:\tau}$ such that $\|\bxi-\bxi_0\|,\|\bxi'-\bxi_0\| \le \cxi$  and $\max_{t}\|\delu_{t}\| \le \cgamma$. 
\end{restatable}
Incremental stability implies that as the inital conditions $\|\bxi - \bxi'\| \to 0$ and $\max_{0 \le s \le i-1}\|\delu_t\| \to 0$, the trajectories induced by taking rolling out $\seqa$ from $\bxi$, and rolling out $\seqa$ from $\bxi'$ with additive input perturbations $\delu_{1:\tau}$ tend to zero in norm. This behavior needs only hold for initial conditions in a small neighborhood of a nominal state $\bxi_0$.
Importantly, the perturbations $\delu_{1:\tau}$ are fixed pertubrations of inputs, applied  to the \emph{closed loop behavior} under the controllers.
Our notion of incremental stability are similar too, but sublty different similar notions of past work. We provide an extended comparisons in \Cref{sec:comparison_to_prior}.


Our main assumption is that the synthesis oracle described above produced primitive controllers which are consistent with, and incrementally stabilizing for, the demonstrated trajectories. \Cref{fig:stab_tube} demonstrates the effect of stabilizing primitive controllers. 
\begin{assumption}\label{asm:iss_body} We assume that our synthesis oracle enjoys {the} following property. Let $\ctraj_T = (\bx_{1:T+1},\bu_{1:T}) \sim \Dexp$, and let $\sfk_{1:T} = \synth(\ctraj_T)$, partitioned into composite actions $\seqa_{1:H}$, with $\sfk_t(\bx) = \bbarK_t = (\bx - \bbarx_t) + \bbaru_t$. We assume that, with probability one, $\sfk_{1:T}$ is consistent with $\ctraj_T$\footnote{Note that this implies $\bbarx_t = \bx_t$ and $\bbaru_t=\bu_t$.}, and that, for each $1 \le h \le H$, $\seqa_h = (\sfk_{t_h:t_{h}+\tauc-1})$ is local $\tiss$ at $\bx_{t_h}$ with moduli $\gammaiss,\betaiss$ and parameters $\cbeta,\cxi > 0$. We further assume that $\gammaiss$ and $\betaiss$ take the form 
\begin{align}
\gammaiss(u) = \cbargamma \cdot u , \quad \betaiss(u,k) = \cbarbeta e^{-(k-1)\lamiss}\cdot u, \quad \cbargamma,\cbarbeta > 0, \quad \lamiss \in (0,1].
\end{align} 
Lastly, we assume that for the expert trajectories and the primitive controllers drawn as above,  it holds that satisfy $\max\{\|\bx_t\|,\|\bu_t\|\} \le \Rdyn$ and $\|\bbarK_t\| \le \Rstab$ with probability one.
\end{assumption}
In \Cref{app:control_stability}, we show that \Cref{asm:iss_body} holds whenever (a) the dynamics of our system are smooth (but not necessarily linear!) (b) the affine gains are chosen to stabilize the Jacobian linearizations of the system around the nominal trajectory.

\begin{definition}[Problem constants]\label{defn:prob_constants_body} Throughout, we refer to constants $c_1,c_2,c_3,c_4,c_5 > 0$, which are polynomial in the terms in \Cref{asm:iss_body}, and which are defined formally in  \Cref{app:control_stability}.
\end{definition}





\newcommand{\cDh}{\cD_{\mathrm{exp},h}}

\newcommand{\dtraj}{\dist_{\mathrm{traj}}}
\newcommand{\Daugh}[1][h]{\cD_{\sigma,#1}}

\subsection{Simplified guarantees under total variation continuity}\label{sec:results_tvc}


This section presents our main theoretical result: if one learns a chunking policy $\pihat$ that can compute the conditional distribution of composite actions at time steps given observation-chunks, then a stochastically smoothed version of this policy, $\pihat_{\sigma}$, has low imitation error. Define, for any length $\tau \in \N$, the \emph{trajectory distance} between two trajectories $\ctraj = (\bx_{1:\tau+1},\bu_{1:\tau}),\ctraj' = (\bx'_{1:\tau+1},\bu'_{1:\tau}) \in \mathscr{P}_{\tau}$ the trajectory distance
\begin{align}
\dtraj(\ctraj,\ctraj') := \max_{1 \le k \le \tau+1}\|\bx_{k}-\bx_k'\| \vee \max_{1 \le k \le \tau}\|\bu_k-\bu_k'\|. \label{eq:dtraj}
\end{align}
In particular, we define $\dtraj(\pathm,\pathm')$ and $\dtraj(\pathc,\pathc')$ by viewing these as trajectories of length $\taum-1$ and $\tauc$, respectively. 
Lastly, we define a per-timestep restriction of the expert distribution. In this section, we consider the case where the learner policy satisfies a total variation continuity (TVC) condition, defined below.
\begin{definition}[TVC of Chunking Policies]\label{defn:tvc_main}  We say that a chunking policy $\pi = (\pi_h)$ is total variation continuous with modulus $\gamtvc: \R_{\ge 0} \to \R_{\ge 0}$, written $\gamtvc$-TVC, if, for all $h \in [H]$ and any observation-chunks $\pathm,\pathm' \in \scrP_{\taum-1}$,
\begin{align}
\TV(\pi_h(\pathm),\pi_h(\pathm')) \le \gamtvc(\dtraj(\pathm,\pathm')).
\end{align}
\end{definition}


% Figure environment removed

We depict the TVC property using our running left-right obstacle example in \Cref{fig:tvc_fig}. We stress that, in \Cref{defn:tvc_main}, the TV bound on $\TV(\pi_h(\pathm),\pi_h(\pathm'))$ applies to the \emph{composite actions} consisting of primitive controllers $\seqa_h  = \sfk_{t_h:t_h+\tauc-1} \sim \pi_h(\pathm)$; it does not upper bound the TV distance between raw control inputs. Indeed, ensuring TVC of the latter can lead to the failure modes depicted in  \Cref{fig:tvc_fig}(b). 

Next, we extract an expert ``policy'' from the expert demonstrations. 
\begin{definition}[Expert ``policy'' with synthesized controllers]\label{defn:Dexph} For $h \in [H]$, we let $\cDh$ denote the joint distribution of $(\seqa_h,\pathm)$, induced by drawing a trajectory  $\ctraj_T = (\bx_{1:T+1},\bu_{1:T}) \sim \Dexp$ from the expert distribution, $\sfk_{1:T} = \synth(\ctraj_T)$ be the associated primitive controllers, letting $\pathm = (\bx_{t_h - \taum+1:t_h},\bu_{t_h - \taum+1:t_{h}-1})$ be the associated observation-chunk at time $h$,  and $\seqa_h  = \sfk_{t_h:t_{h+1}-1}$ the associated composite action.  We let $\pist_h(\cdot):\cO \to \laws(\cA)$ denote the condition distribution of $\seqa_h \mid \pathm$ under $\cDh$.
\end{definition}

The conditional distributions $\pist_h(\cdot)$ are precisely what is estimated when training a generative model to predict $\seqa_h$ from observations $\pathm$.  Note that $\pist_h(\cdot)$ (and $\cDh$) is defined in terms of  \emph{both} expert demonstration from $\Dexp$ and the associated synthesized primitive controllers. In \Cref{lem:pistar_existence}, we show that when the synthesis oracle $\sfk_{1:T} = \synth(\ctraj_T)$ produces primitive controllers consistent with the trajectories, than $\pist = (\pist_h)$ produces the same marginals over states as $\Dexp$; that is, $\Imitmarg(\pist) = 0$.



\begin{theorem}\label{prop:TVC_main} Suppose \Cref{asm:iss_body} holds, and suppose that $0 \le \epsilon < c_2 $, and $\tauc \ge c_3$. Then, for any non-decreasing non-negative $\gamtvc(\cdot)$ and $\gamtvc$-TVC chunking policy $\pihat$,
\begin{align}
\Imitmarg[\epsilon](\pihat) &\le H\gamtvc(\epsilon) +  \sum_{h=1}^H \Exp_{\pathm \sim \cDh}\Delta_{(\epsilon/c_1)}\left(\pist_h(\pathm),\pihat_h(\pathm) \right) \label{eq:TVC_main}\\
&\le H\gamtvc(\epsilon) +  \frac{c_1}{\epsilon}\sum_{h=1}^H \Exp_{\pathm \sim \cDh}\left[\cW_{1,\dmax}\left(\pist_h(\pathm), \pihat_h(\pathm) \right) \right].
\end{align}
\end{theorem}
The above result reduces the marginal imitation error of $\pihat$ to the sum over optimal transport errors between $\pihat$ and $\seqa \mid \pathm$ chosen by the expert demonstrators. Thus, if these are small, the local stabilization properties of the primitive controllers guaranteed by \Cref{asm:iss_body} ensure that errors compound at most linearly in problem horizon. The key ideas of the proof are given  \Cref{sec:analysis}, via a general template for imitation learning of general stochastic policies.  This template is instantiated with a details in \Cref{app:end_to_end}. Lastly, in \Cref{sec:no_min_chunk_length}, we show that we can remove the required lower bound on $\tauc$ under the slightly stronger condition that our synthesis oracle ensures that the entire sequence of primitive controllers $\sfk_{1:T}$ are incrementally stabilizing (not just the primitive controllers for each composite action individually).
\begin{remark}[The $\epsilon=0$ case]\label{rem:eps_0}
If we were able to bound the policy error $\Delta_{(\epsilon)}$ with $\epsilon = 0$ -- which corresponds to estimating $\seqa_h \mid \pathm$ in \emph{total variation} distance -- the imitation learning problem would be trivialized, and neither the TVC condition above or the noise-injection based smoothing in the section below would not be needed  (see \Cref{app:end_to_end}).  \Cref{app:scorematching} explains that the needed assumptions for this stronger sense of approximate sampling do not hold in our setting, because expert distributions over actions typically lie on low-dimensional manifolds. 
\end{remark}


\begin{remark}[On the TVC assumption]\label{rem:TVC}  It is true than any $\pihat$ implemented as a DDPM with a Lipschitz activation with bounded-magnitude parameters is indeed TVC.  Unfortunately, these Lipschitz constants can be too large to be meaningful in practical scenarios, scaling exponentially with network depth. In addition, the absence of smoothing $\sigma$ may make the corresponding DDPM learning problem more challenging.   Hence, in what follows, we shall require the additional sophistication of smoothing with Gaussian noise of variance $\sigma^2 > 0$ for meaningful guarantees. 
%\Cref{prop:TVC_main} corresponds to the $\sigma = 0$ case of \Cref{thm:main}. The proof \Cref{prop:TVC_main} is considerably simpler than that of \Cref{thm:main_template}, and serves to build intuition for the latter; 
\end{remark}

\begin{remark}[Imitation of the joint distribution] Suppose the expert distribution $\Dexp$ has at most $\taum$-bounded memory (defined formally in \Cref{defn:bounded_memory}). Then  $\Imitjoint[\epsilon](\pihat)$ satisfies the same upper bound \eqref{eq:TVC_main}, where $\Imitjoint[\epsilon](\pihat)$, formally defined in \Cref{def:loss_joint}, measures an optimal transport distance between the \emph{joint distribution} of the expert trajectory and the one induced by $\pihat$.
\end{remark}



\newcommand{\cDhsig}{\cD_{\mathrm{exp},\sigma,h}}


\subsection{A general guarantee via data noising.}\label{sec:results_smoothing}  To circumvent assuming that the learner's policy is TVC,   we study estimating the conditionals under a popular data augmentation technique \cite{ke2021grasping}, where the learner is trained to imitate the conditional sequence of $\seqa \mid \pathmtil$, where $\pathmtil \sim \cN(\pathm,\sigma^2 \eye)$ adds $\sigma^2$-variance Gaussian noise to the true observation-chunk. To understand this better, consider the following \emph{smoothed} policy:
\begin{definition}[The smoothed policy]\label{defn:smoothed_policy} Let $\pihat = (\pihat_h)$ be a chunking policy. We define the \emph{smoothed policy} $\pihat_{\sigma} = (\pihat_{\sigma,h})$ by letting $\pihat_{\sigma,h}(\cdot \mid \pathm)$ be distributed as $\pihat_{h}(\cdot \mid \pathmtil)$, where $\pathmtil  \sim \cN(\pathm,\sigma^2 \eye)$.
\end{definition}
In \Cref{sec:TVC_check}, we use  Pinsker's inequality given to show that smoothing automatically enforces TVC:
\begin{lemma}\label{lem:tvc_body} Let $\pihat = (\pihat_h)$ be \textbf{\emph{any}} arbitrary chunking policy. Then, $\pihat_{\sigma}$ is $\gamtvc$-TVC, with $\gamtvc(u) = \frac{u\sqrt{2\taum - 1}}{2\sigma}$ being linear in $u$ and inversely proportional to $\sigma$. 
\end{lemma}
This suggests that we can use some form of data noising to enforce the TVC property in \Cref{defn:tvc_main}. Let's now consider a related problem: trying to estimate the optimal distribution over composite actions \emph{conditioned on} a noised observation. This gives rise to a \emph{deconvolution} of the expert policy, which can be thought as an inverse operation of data noising. 

\begin{definition}[Noised Data Distribution and Deconvolution Policy]\label{defn:Dsigh} Let $\cDh$ be as in \Cref{defn:Dexph}. Define $\cDhsig$ as the distribution over $(\pathmtil,\seqa_h)$ generated by $(\pathm,\seqa_h) \sim \cDh$ and  $\pathmtil \sim \cN(\pathm,\sigma^2 \eye)$. We define the \emph{deconvolution policy} $\pidecsigh(\pathmtil)$ as the conditional distribution of $\seqa_h \mid \pathmtil$ under $\cDhsig$.
\end{definition}


%Because the demonstrator is stochastic, we care not about point-estimation, but rather \emph{generative modeling} of the conditional distributions of composite actions. This is measured by the following notion of discrepancy, formulated in an optimal transport error, which relaxes the L\'evy-Prokhorov metric \citep{strassen1965existence} between the conditional distribution of $\seqa_h \mid \pathmtil$ and $\seqa_h' \sim \pihat_h(\pathmtil)$ under $\Daugh$. 




\newcommand{\Thetatiss}{\Uptheta_{\mathrm{Iss}}}


Analogously to $\pist$, the policy $\pidecsigh$ is what a generative model trained to generate $\seqa_h $ from noised observations $\pathmtil$ of $\pathm \sim \Dexp$ learns to generate. 
Our next theorem states that, if our $\pihat$ approximates the idealized conditional distributation of composite actions given noised observations, then $\pihat_{\sigma}$, the smoothed policy, imitates the expert distribution with provable bounds on its imitation error:
\begin{theorem}[Reduction to conditional sampling under nosing]\label{thm:main_template}  Suppose \Cref{asm:iss_body} holds. Let  $c_1,\dots,c_5 > 0$, defined in \Cref{defn:prob_constants_body}, and let $\Thetatiss(x)$ denote a term which is upper and lower bounded by a $x$ times a polynomial in those constants and their inverses. Then, for  $\epsilon \le \Thetatiss(1)$, if we choose $\sigma = \epsilon/\Thetatiss(\sqrt{\dimx} + \log(1/\epsilon))$ and let $\tauc \le c_3$ and $\tauc - \taum \ge \frac{1}{\lamiss}\log(c_1/\epsilon)$, 
    \begin{align}
    \Imitmarg(\pihat_{\sigma})  &\leq \Thetatiss\left(\epsilon H\sqrt{\taum}  \cdot (\sqrt{\dimx} + \log(1/\epsilon) \right) +   \sum_{h=1}^H\Exp_{\pathmtil \sim \cDhsig} \left[\Delta_{(\epsilon^2)}\left(\pidecsigh\left(\pathmtil\right),\,\pihat_h\left(\pathmtil\right) \right)\right]
 \label{eq:mainguarantee_simple}\\
  &\le  \Thetatiss\left(\epsilon H\sqrt{\taum}  \cdot (\sqrt{\dimx} + \log(1/\epsilon) \right) + \frac{1}{\epsilon^2}\sum_{h=1}^H \Exp_{\pathm \sim \cDhsig}\left[\cW_{1,\dmax}\left(\pidecsigh(\pathmtil),\,\pihat_h(\pathmtil) \right) \right],
    \end{align}
\end{theorem}

To reiterate, \Cref{thm:main_template} guarantees imitation of the distribution of marginals and final states of $\Dexp$ by replacing the explicit TVC assumption with noising, and the resulting guarantee applies to the \emph{smoothed policy} $\pihat_{\sigma}$ which adds smoothing noise back in. 
 \Cref{app:end_to_end} gives a number of additional results, including: 
 \begin{itemize}
  \item A granular guarantee, \Cref{thm:main_template_precise}, that exposes the tradeoffs between parameters $\sigma$, $\epsilon$, and $\tauc$.  This result also ensures sharper bounds imitation of the marginal of the \emph{final state} $\bx_{T+1}$
  \item  Guarantees for imitating \emph{joint} trajectories under the further assumptions that (a) the  demonstrator has memory (or, more generally, a mixing time) of at most $\taum$, and (b) \emph{either} the demonstrator distribution  happens to satisfy a certain continuity property, \emph{or} $\sigma = 0$ and instead the learned $\pihat$ satisfies that same property.
\end{itemize}

% Figure environment removed

\begin{proof}[Proof Sketch]
As with \Cref{prop:TVC_main}, the key ideas of the proof are given  \Cref{sec:analysis}, expressed in terms of a general abstraction for behavior cloning we call the ``composite MDP''.  This template is instantiated with a details in \Cref{app:end_to_end}. Moreso than \Cref{prop:TVC_main}, the proof of \Cref{thm:main_template} requires sophisticated couplings between expert and learner trajectories, and in particular. The intuition is based on the observation that $\pihat_{\sigma,h}$ mimic $\pirepsigh := (\pidecsigh)_{\sigma}$, the smoothing of the deconvolution policy. Inspired by replica pairs in statistical physics, we call $\pirepsig$ the ``replica'' policy because actions from $\pirepsigh$ can be thought of as actions from $\pist_h$ that have been noised and deconvolved. This implies:
\begin{fact} Let $\pathm \sim \cDh$. Then, the distributions of $\seqa_h \sim \pist_h(\pathm)$, and $\seqa_h' \sim \pirepsigh(\pathm)$, marginalized over $\pathm$, are identical. 
\end{fact}
This observation can be interpreted as meaning that smoothing and deconvolution are inverse operations at the distributional level. For a moment, consider an idealized environment where $\seqa_h$, and not $\pathm$, perfectly determined the dynamics (e.g. by teleportation), then $\pirepsig = (\pirepsigh)$ and $\pist = (\pist_h)$ would induce the same dynamics (and, as remarked above, $\Imitmarg(\pist) = 0$).

For non-idealized  environments, the argument goes as follows: We  couple  $\seqa_h \sim \pist_h(\pathm)$, and $\seqa_h' \sim \pirepsigh(\pathm)$ so that $\seqa_h'$ has the distribution of $\seqa_h' \sim \pist(\pathm')$, where $\pathm'$ is distributed as $\pathm$ and, with high probability, $\pathm'$ and $\pathm$ are $\tilde{O}(\sigma)$-close in Euclidean norm. This coupling is depicted \Cref{fig:replica_fig}. We then argue that the dynamics induced by $\pirepsig$ track those of $\pist$ by roughly a similar margin. Moreover, by smoothing $\pihat_h$ and $\pidech$ and applying Jensen's inequality, 
 \begin{align}
  \Exp_{\pathm \sim \cDh} \left[\Delta_{(\epsilon^2)}\left(\pirepsigh\left(\pathm\right),\,\pihat_{\sigma,h}\left(\pathm\right) \right)\right] \le 
 \Exp_{\pathmtil \sim \cDhsig} \left[\Delta_{(\epsilon^2)}\left(\pidecsigh\left(\pathmtil\right),\,\pihat_h\left(\pathmtil\right) \right)\right].
 \end{align}
 Consequently, when the right hand side of the above inequality is small, the noised policy $\pihat_{\sigma}$ tracks the replica policy $\pirepsig$, which we have shown to be track $\pist$ (and thus track $\Dexp$).
\end{proof}



%, we then guarantee imitation of the \emph{joint} trajectory distribution.




\begin{remark}[Sharpness of \Cref{prop:TVC_main,thm:main_template}]  In \Cref{app:lbs}, we show that the proof framework, outlined in \Cref{sec:analysis}, which under lies the proofs  of \Cref{prop:TVC_main,thm:main_template}, is essentially sharp in the worst case. Of particular importance, we show that (a) that our dependence on TVC without smoothing  is necessary in the worst case, (b) the smoothing the learned policy $\pihat$ to $\pihat_{\sigma}$ is necessary to ensure adequate imitation.   This section suggests that, up to terms polynomial in the stability parameters specified in \Cref{defn:tiss}, the results in these theorems that upper bound $\Imitmarg$ in terms of the $\Delta_{\epsilon}$ error terms are tight. We note that bounding $\Delta_{\epsilon} \le \frac{1}{\epsilon}\cW_{1,\dmax}$ may be loose in general. 
\end{remark}



\subsection{Merits and Drawbacks of the Synthesis Oracle}\label{sec:merits_synthesis}

The role of a synthesis oracle satisfying \Cref{asm:iss_body} is to replace a strong assumption on the stability of an \emph{expert demonstration} with an \emph{algorithmic assumption} that allows post-hoc stabilizing of expert demonstrations.  This approach presents a natural question: in what sense is this tradeoff a sensible one?  To answer this, consider a paradigmatic case, where the demonstrations solve some complicated task in a smooth, nonlinear control system. Suppose further that the one-step dynamics of the system are known, but that the expert demonstrations come from some optimal control law which is computationally prohibitive to compute, or possibly even some mixture of different, mutually-incompatible trajectories. Assuming the Jacobian linearizations of the nonlinear system are stabilizable (see \Cref{app:control_stability} for further details), one can implement a synthesis oracle for affine gains directly by solving a Ricatti recursion on the Jacobian-linearized dynamics around each expert trajectory. Conceptually, this has the following interpretation: \textbf{Our framework reduces the problem of imitating a complex expert trajectory to (i) supervised generative modeling and (ii) solving strictly \emph{local} control problems}.

That is, we offload complex behavior of the expert being imitated, and reduce the learner's burden to solving  local control problems that are significantly simpler than global planning. For more general systems, \Cref{app:gen_controllers} addresses possibly non-affine stabilizing gains, and discusses how these may arise from standard practices of using robotic position control or inverse dynamics.  \Cref{sec:comparison_to_prior} compares our hierarchical approach to  stability to standard formulations that apply to the expert distribution, and we show how the latter rule out the possibility for complex behaviors such as bifurcated trajectories.  

\paragraph{Limitations and Future Directions.} Our above example required access to differentiable (indeed, smooth) system dynamics. Stabilizing systems with contact dynamics remains an outstanding challenge. More generally, an overtly hierarchical approach may be inefficient for many reasons, notably (1) the dimension of the primitive controller may be much higher than the dimension of raw control inputs; and (2) when the high-level and low-level controllers are parametrized by the neural networks, explicit heirarchy with separate models may preclude shared representation learning. Developing a more comprehensive approach to stability (perhaps one that does not require explicit gain synthesis, and extends to non-smooth systems) is an exciting direction for future work.  Nevertheless, we think that \textbf{our conceptual contribution of decoupling low-level stability and generative matching of demonstrated behavior will prove useful in future endeavors for reliable and performant behavior cloning.}


%We never explicitly model bifurcations; rather, we allow expert demonstrations to be sufficiently rich as to permit them. Eschewing global stability, $\tauc$ ensures that trajectories are long enough for the \emph{strictly local} stability assumptions in \Cref{asm:iss_body} to provide benefit. Thus, non-Markovianity and multi-modality is challenging only insofar as it relates to the difficulty of local stabilization. Indeed, in \Cref{app:gen_controllers}, we generalize \Cref{thm:main_template} and \Cref{prop:TVC_main} below to general families of parametric, incrementally stabilizing controllers.
%The role of $\tauc$ is best understood as ensuring that trajectories are long enough to stabilize around, which suggests the following surprising conclusion: \emph{non-Markovianity of the demonstrator is  only insofar as it a.} 
%A key limitation of our work is that, to take advantage of local stability, we rely on either synthesized primitive controllers (in our analysis) or low-level stabilizing controllers built into problem environments (in our experiments). As noted in the Introduction, this obviates the needed for stronger query access to the expert (e.g. \cite{pfrommer2022tasil}), or for interactive data collection \cite{ross2010efficient}. 
%\abcomment{it would be good to summarize this paragraph in like two sentences in the intro to really drive home the point that our innovation is to replace strong stability assumptions on the expert with strong (but realistic) assumptions on the oracle.} 





%\Cref{thm:main} leverages statistical learning guarantees for DPPMs to show our learned policy approximately samples from $\pidech$ in a truncated Wasserstein distance (\Cref{app:scorematching}). 

