%!TEX root = ../main.tex

\section{\toda: Instantiating Data Noising with DDPMs}
\label{sec:algorithm}
We now instantiate \Cref{thm:main_template} by showing that one can learn a policy $\pihat$ for which the error terms in \Cref{thm:main_template,prop:TVC_main} are small  by fitting a DDPM to noise-smoothed data.


\newcommand{\ProjGain}{\mathtt{ProjGain}}
\begin{algorithm}[!t]
  \begin{algorithmic}[1]
  \State{}\textbf{Initialize} Synthesis oracle $\synth$, sample sizes $\Nsample,\Naug \in \N$, $\sigaug \ge 0$, DDPM step size $\dpstep > 0$, DDPM horizon $\dphorizon$, function class $\{\scoref_{\theta}\}_{\theta \in \Theta}$, gain magnitude $R >0$, empty data buffer $\Imitdata \gets \emptyset$. 
  \Statex{} \algcomment{For no smoothing, set $\sigaug = 0$ and $\Naug = 1$}
  \For{$n =1,2,\dots \Nsample$}
  \State{}Sample $\ctraj_T = (x_{1:T+1},u_{1:T}) \sim \Dexp$ and set $\sfk_{1:T} = \synth(\ctraj)$ 
  \Statex{}~~~~\algcomment{Segment $ \pathm[1:H]$ from $\ctraj_T$ and $ \seqa_{1:H}$ from $\sfk_{1:T}$}
  \For{$i = 1,2,\dots,\Naug$ and $h = 1,2,\dots,H$}
  \State{}Sample $\pathmtil \sim \cN(\pathm,\sigaug^2 \eye_{})$,  $\dpind_h \sim \mathrm{Unif}([\dphorizon])$ and $\bgamma_h \sim \cN(0, (\dpind_h \alpha)^2 \eye)$.
    %
    \State{} $\Imitdata \gets \Imitdata.\mathrm{append}\left(\{ (\seqa_{h}, \pathmtil[h],  \dpind_{h}, \bgamma_{h},h) \}\right)$
    \EndFor
  \EndFor
  \State{}Fit $\theta \in \argmin_{\theta \in \Theta}\Lddpm(\theta, \Imitdata)$, and let $\pihat = (\pihat_h)$ be given by $\pihat(\cdot \mid \pathm) = \ddpm(\bs_{\theta,h}, \pathm)$. \label{line:DDPM_train}
  \State{}\label{line:test_time} \textbf{return} $\pihat_{\sigma} = (\hat \pi_{\sigma,h})$, by smoothing $\pihat$ as per \Cref{defn:smoothed_policy}. 
  \caption{ \
  \textbf{H}ierarchical \textbf{I}mitation via \textbf{N}oising at Inference \textbf{T}ime ($\toda$)}
  \label{alg:imitation_augmentation}
  \end{algorithmic}
\end{algorithm}



\paragraph{Algorithm.} Our proposed algorithm, $\toda$ (\Cref{alg:imitation_augmentation}) combines DDPM-learning of chunked policies as in \cite{chi2023diffusion} with a popular form of data-augmentation \citep{ke2021grasping}. We collect $\Nsample$ expert trajectories, synthesize gains, and segment trajectories into observation-chunks $\pathm$ and composite actions $\seqa_h$ as described in \Cref{sec:setting}. We perturb  each $\pathm$ to form $\Naug$ chunks $\pathmtil$, as well as horizon indices $\dpind \in [\dphorizon]$ and inference noises $\bgamma \sim \cN(0, (\dpstep \dpind_h)^2 \eye)$, 
and add these tuples $(\seqa_h, \pathmtil, \dpind_h, \bgamma_h,h)$ to our  data $\Imitdata$. We end the training phase by minimizing the standard DDPM loss \citep{song2019generative}\iftoggle{workshop}{ $\Lddpm(\theta, \Imitdata)$:
  \begin{align}
     \sum \norm{\bgamma_h - \scoref_{\theta,h}\left(e^{-\dpstep \dpind}\seqa_h + \sqrt{1 - e^{-2\dpstep \dpind}}\bgamma_h, \pathmtil, \dpind_h\right)}^2,\label{eq:ddpmcond}
  \end{align}
  where the sum is over $(\seqa_h, \pathctil,  \dpind_{h}, \bgamma_h,h) \in \Imitdata$.
}{:
\begin{align}
  \Lddpm(\theta, \Imitdata) = \sum_{(\seqa_h, \pathctil,  \dpind_{h}, \bgamma_h,h) \in \Imitdata} \norm{\bgamma_h - \scoref_{\theta,h}\left(e^{-\dpstep \dpind}\seqa_h + \sqrt{1 - e^{-2\dpstep \dpind}}\bgamma_h, \pathmtil, \dpind_h\right)}^2.\label{eq:ddpmcond}
\end{align}
}
Our algorithm differs subtly from past work in \Cref{line:test_time}: motivated by \Cref{thm:main_template}, we add smoothing noise \emph{back in} at test time. Here, the notation $\ddpm(\bs_{\theta,h}, \cdot) \circ \cN(\pathm, \sigaug^2 \eye)$ means, given $\pathm$, we perturb it to $\pathmtil \sim \cN(\pathm, \sigaug^2 \eye)$, and sample $\seqa_h \sim \ddpm(\bs_{\theta,h}, \pathmtil[h])$.  

%\mscomment{TODO experimental results}

% Figure environment removed

\subsection{Simulation Study of Test-Time Noise-Injection}\label{sec:experiments}

We empirically evaluate the effect on policy performance of our proposal to inject noise back into the dynamics at inference time.  We consider three challenging robotic manipulation tasks studied in prior work: PushT block-pushing \citep{chi2023diffusion}; Robomimic Can Pick-and-Place and Square Nut Assembly \citep{mandlekar2021matters}.  We explain the environments in greater detail, along with all training and computational details in \Cref{app:exp_details}.
% Since \Cref{alg:imitation_augmentation} produces a policy which intentionally injects noise into the observation during evaluation, we naturally wish to understand the practical impact of this noise on policy performance. We principally consider challenging robotic manipulation tasks studied in prior work, including the PushT block-pushing task \citep{chi2023diffusion}, the Robomimic Can Pick-and-Place task, and the Robomimic Square Nut Assembly task \citep{mandlekar2021matters}. See \Cref{app:exp_details} for the environment details, DDPM architecture, and training methodology. 
The learned diffusion policy generates state trajectories over a $\tau_c = 8$ chunking horizon using fixed feedback gains provided by the $\synth$ oracle to perform position-tracking of the DDPM model output. We direct the reader to \citet{chi2023diffusion} for an extensive empirical investigation into the performance of diffusion policies in the noiseless $\sigma = 0$ setting. We display the results of our experiments in \Cref{fig:noise_sweep}.  Observe that the performance degredation of the replica policy from the unsmoothed $\sigma = 0$ variant is minimal across all environments and even leads to a slight but noticeable improvement in the small-noise regime for PushT (and low-data Can Pick and Place). In the presence of non-negligible noise $\toda$ significantly outperforms the conventional policy $\pihat$ (obtained by adding augmentation at training but not test time), as predicted by our theory. 

\subsection{Assumptions for Analysis of \toda{}}
 We now state the assumptions required for our theoretical guarantees on \toda{}. We require access to a class of score functions rich enough to represent the following deconvolution conditionals. To define these, we introduce the following distribution




\newcommand{\Daught}[1][t]{\cD_{\sigma,h,[#1]}}
\newcommand{\aht}{\seqa_{h,[t]}}
\newcommand{\pidecsight}{\pist_{\mathrm{dec},\sigma,h,[t]}}
\begin{definition}\label{defn:dec_cond} Recall the policy $\pidecsigh$ from \Cref{defn:Dsigh}. Given $\pathm \in \cO$, let  $\pidecsight(\pathm) \in \laws(\cA)$ denote the law of  $\aht := e^{-t }\seqa_h^{(0)} + \sqrt{1 - e^{-2t}} \bgamma$, where $\seqa_h \sim \pidecsigh(\pathm)$, and $\bgamma \sim \cN(0,\eye)$ is independent whte Gaussian noise. In words, $\aht$ is generated from the Ornsteinâ€“Uhlenbeck process that interpolates $\seqa_h$ with white noise. To avoid notational clutter, we supress the dependence of $\pidecsight$ on $\sigma$ via $\pidecht = \pidecsight$ when $\sigma$ is clear from context.
\end{definition}
Next, we need an assumption of bounded statistical complexity. We opt for the popular \emph{Rademacher complexity} \cite{bartlett2002rademacher}.  In defining this quantity, recall that scores are vector-valued, necessitating the vector analogue of Rademacher complexity \cite{maurer2016vector,foster2019vector}, studied for score matching in \cite{block2020generative}.
\begin{definition}[Function Class $\Theta$ and Rademacher Complexity]\label{def:defn_of_comp} Consider a class of score functions of the form $\Theta = \left\{ (\scoref_{\theta,h})_{1 \le h \le H}  | \theta \in \Theta \right\}$, where $\scoref_{\theta,h}$ maps triples $(\seqa,\pathm,j)$ of composite actions $\seqa \in \cA$, observation-chunks $\pathm$, and DDPM-steps $j \in \N$ to vectors in $\R^{\dA}$.  For each chunk $h \in [H]$, DDPM-step $j \in \N$ and discretization size $\alpha$, define the vector- Rademacher complexity of $\Theta$ as 
  \begin{align}
        \rad_{n,h,j}(\Theta;{\color{blue}\alpha}) := \ee\left[ \sup_{\substack{\theta \in \Theta}} \frac {1}{n} \sum_{k =1}^n \left\langle \bm{\epsilon}^{(k)},  \scoref_{\theta,h}\left(\seqa_{h,[j {\color{blue}\alpha}]}^{(k)}, \pathmtil^{(k)}, j\right)\right \rangle \right],
    \end{align}
    where $\bm{\epsilon}_k \in \R^d$ are i.i.d. random vectors with Rademacher coordinates, and where $(\pathmtil^{(k)},\seqa_{h,[j {\color{blue}\alpha}]}^{(k)})$ are i.i.d. samples from $\Daught[j {\color{blue}\alpha}]$.
\end{definition}
\newcommand{\Cgrow}{C_{\mathrm{grow}}}

\begin{assumption} \label{ass:score_realizability} We suppose that, for any $\sigma > 0$, we are given a class of score functions $\Theta = \Theta(\tauc,\taum,\sigma,\alpha)$ of the form in \Cref{def:defn_of_comp} which satisfies the following conditions:
\begin{itemize}
\item[(a)] \emph{Realizability:} there exists a $\theta_{\star}$ such that, for all $h \in [H]$ and $j \in \N$, $\scoref_{\theta_\star,h}\left(\seqa, \pathm, j\right) $ is the score function of $\pidecht[j\alpha](\pathm)$ at $\seqa \in \cA$.
\item[(b)] \emph{The Rademacher complexity of $\Theta$ has polynomial decay in $n$ and growth in $\alpha$:}
\begin{align}
  \sup_{j \in \N}\max_{h \in [H]}\rad_{n,h,j}(\Theta;\alpha) \le \Ctheta \alpha^{-1} n^{-\frac 1 \Cnu},
\end{align}
where $\Ctheta = \Ctheta(\sigma,\tauc,\taum) > 0$. 
\item[(c)] The scores have \emph{linear growth}; that is, there exists some $\Cgrow = \Cgrow(\sigma,\tauc,\taum) > 0$ sucht hat 
\begin{align}
\sup_{j \in \N, h \in [H]} \sup_{\theta \in \Theta}\norm{\scoref_\theta(\seqa, \pathmtil, j)} \le \Cgrow \alpha^{-1}(1+\norm{\seqa}+\norm{\pathmtil}),
\end{align} 
 As discussed in \Cref{app:scorematching}, generalizing to polynomial growth is straightforward. 
\end{itemize}
\end{assumption}

\begin{comment}
For the same $\sigma > 0$ as above, and for a given $\dpstep > 0$ and $\dpind \in \bbN$, let $\scoresthsig[\dpind]$ denote the score function of $\pidecht[\dpstep \dpind]$.  We suppose that for any $\dphorizon \in \bbN$ and $\alpha > 0$, we are given a class of scores $\Theta = \Theta(\tauc,\taum,\sigma)$ consisting of functions . 
\end{comment}
As justified in \Cref{app:scorematching}, our decay condition on the Rademacher complexity is natural for statistical learning, and holds for most common function classes (often with $\nu \le 2$ and even more benign dependence on $J,\alpha$);  our results can easily extend to approximate realizability as well. Our Rademacher bound depends  implicitly on chunk and observation lengths $\tauc,\taum > 0$ and implicitly on dimension $\dA$ via $C_{\Theta}$.  Realizability is motivated by the approximation power of deep neural networks \citep{bartlett2021deep}. Lastly, we do expect realizability to hold uniformly over $j \ge 0$ because, as $j \to 0$, the corresponding scores corresponds to a scaled identity function (i.e. the score of a standard Gaussian).



%Taken together, our assumptions effectively remove the need for Markovian, uni-modal demonstrations by appealing to two oracles. \Cref{asm:Jacobian_Stable} lets us synthesize  locally stabilizing controllers around possibly bifurcated demonstrations. Because stability in nonlinear control systems depends on dynamics along trajectories, we then require (approximate) realizability of \emph{joint distributions over sequences of primitive controllers} in \Cref{ass:score_realizability}. This latter assumption   


\begin{comment}
We further remark that the realizability assumption of the class $\Theta$ can be relaxed so that $\scoresthsig$ is $\epsilon$-misspecified in mean squared error.  Rigorous control of the quality of generated samples under \Cref{ass:score_realizability} can be realized by combining statistical learning guarantees in \citet{block2020generative} with sampling guarantees in \citet{chen2022sampling} \ab{cite holden lee's paper as well}; this analysis is conducted in \Cref{app:scorematching}.  We now abstracat away the generative modeling aspect of the analysis and consider the implications to imitation learning; indeed, our results are not specific to DDPMs but hold for any learned sampler satisfying a guarantee in an optimal transport distance defined in \Cref{sec:analysis} and any suboptimality in our final bound's dependence on the number of samples comes from looseness of the theoretical analysis of DDPMs in \citet{chen2022sampling}\ab{cite holden lee}.
\end{comment}

%; as we envision $\Theta$ as a class of deep neural-networks, our bounds rely on the capacity of these networks to model joint distributions of a certain length.  Similaly, we replace prior assumptions on policy stability \mscomment{dan}.



% \begin{corollary}\label{cor:samplingguarantee}
%   Suppose the assumptions in \Cref{prop:scorematchinginformal} hold and that $\dpstep$ and $\dpind$ are polynomial in problem parameters.\footnote{Exact dependence is given in \eqref{eq:samplingparameters}.}  Then with probability at least $1 - \delta$ for all $h \leq H$, it holds that
%   \begin{align}\label{eq:samplingguarantee}
%     \ee_{\pathm{h} \sim \Dexp}\left[ \inf_{\coup \in \couple(\emph{\ddpm}(\scoref_{\thetahat, h}, \pathm), \Dexp(\cdot | \pathm))} \pp_{(\seqahat_h, \seqast_h) \sim \coup}\left( \norm{\seqahat_h - \seqast_h} \geq \epsilon \right) \right] \leq \epsilon.
%   \end{align}
% \end{corollary}
% \Cref{cor:samplingguarantee} is proved in Appendix \ref{app:scorematching} and further discussion is deferred there for the sake of space.  





\subsection{Theoretical Guarantee for \toda.} We now state our guarantee for \toda{}. Recall that $\dA$ denotes the dimension of composite actions, and that $c_1,\dots,c_5$ are as in \Cref{defn:prob_constants_body}.
\begin{theorem}\label{thm:main}  Suppose \Cref{asm:iss_body} holds. Let  $c_1,\dots,c_5 > 0$, defined in \Cref{defn:prob_constants_body}, and let $\Thetatiss(x)$ denote a term which is upper and lower bounded by a $x$ times a polynomial in those constants and their inverses. Let $\epsilon \le \Thetatiss(1)$, if we choose $\sigma = \epsilon/\Thetatiss(\sqrt{\dimx} + \log(1/\epsilon))$ and let $\tauc \le c_3$ and $\tauc - \taum \ge \frac{1}{\lamiss}\log(c_1/\epsilon)$. Consider running $\toda$ for $\sigma > 0$ with parameters $J,\alpha$ polynomial in the parameters given in \Cref{asm:iss_body} specified in \Cref{app:scorematching}. Then,
Then, if 
 \begin{align}
 \Nsample \geq \poly\left( \Ctheta(\sigma,\tauc,\taum),1/\epsilon,  \Rstab, \dA,\log(1/\delta)\right)^{\nu} > 0,
 \end{align} then with probability $1-\delta$, the policy $\pihat_{\sigma}$ returned by \toda{} satisfies
    \begin{align}
    \Imitmarg(\pihat_{\sigma})  &\leq \Thetatiss\left(\epsilon H\sqrt{\taum}  \cdot (\sqrt{\dimx} + \log(1/\epsilon) \right). \label{eq:guarantee_ddpm}
    \end{align}

In addition, consider running  \toda{} with $\sigma=0$, and suppose $\Ctheta(\sigma,\tauc,\taum)\big{|}_{\sigma=0}$ is finite. Then, for $\Nsample$ satisfying the same bound as above, it holds that with probability $1-\delta$, the policy $\pihat$ produced by \toda{} satisfies the guarantees of \Cref{prop:TVC_main} up to an additive factor of $H\epsilon$ on the event that $\pihat$ happens to be $\gamma$-TVC.
\end{theorem}
\Cref{thm:main} instantiates \Cref{thm:main_template}  by bounding the policy error terms $\Delta_{(\epsilon^2)}(\pidecsigh(\pathmtil),\pihat_h(\pathmtil))$  in that theorem when $\pihat$ is the policy learned by fitting the DDPM in \Cref{line:DDPM_train}. The formal bound on $\Nsample$ is given in \eqref{eq:sample_complexity_app} in \Cref{app:end_to_end}. While making $\tauc$ larger appears to improve the above bound, it generally increases the statistical and computational challenge of learning the DDPM itself. The guarantees for score matching are derived in \Cref{app:scorematching} by applying \cite{chen2022sampling,lee2023convergence,block2020generative}; these are applied to \Cref{thm:main_template} in \Cref{app:end_to_end}.


%\mscomment{discuss}
%a more expressive score class $\Theta$ (requiring greater $\Nsample$); similarly, as explained in \Cref{app:scorematching}, the scores $\scoresthsig$ may become harder to learn $\sigma$ decreases.


%\mscomment{to read.}
%Further, as we show in Appendix \ref{app:lowerbound} \ab{TODO: Max, do this lower bound}, the only looseness in the above bound is in the guarantee on the quality of the learned DDPM; indeed, as will be clear from the analysis, we can replace the DDPM in $\toda$ with an arbitrary generative model, as long as a certain optimal transport cost is small between the learned generative model and the expert policy.  We make two further remarks regarding \Cref{thm:main}.  First, while the result controls the marginal and final losses of the learned policy with respect to the expert $\Dexp$, in fact we can also control the a stronger ``joint'' loss with respect to the closely related ``replica'' distribution, defined below.  Second, as written, \eqref{eq:mainguarantee} does not apply to the limiting case when there is no augmentation, i.e., when $\sigaug = 0$; in Appendix \ref{app:no_augmentation}, however, we control the imitation loss for $\toda$ without augmentation under a strong continuity assumption on the learned policy $\pihat$.  Our theoretical results indicate that, outside of such situations, data augmentation will be an effective tool \ab{mention experiments?}
% 
%\begin{remark} If we were able to sample construct samples close to $\pidech$ in total variation,
%\end{remark}
%We emphasize that  
%That our guarantees only hold on the marginal distributions is a consequence of data augmentation. In \mscomment{..}, we show that if $\sigma = 0$ and our learned policy $\pihat$ happens to satisfy a certain continuity property, we do guarantee imitation of the joint trajectory distribution.
% 
 %Unfortunately, as discussed in \Cref{rmk:notvguarantee}, the assumptions under which this is possible are far too strong to hold in our setting.  Appropriately handling the weaker optimal transport metric under which we do have theoretical guarantees for sampling is one of the key challenges in our analysis.
% 
% 
% 
% 
% 
% \paragraph{Old Adam Stuff:}
% \mscomment{let's discuss}
% Building on the guarantee in \citet{block2020generative} for unconditional score estimation and that in \citet{chen2022sampling} \ab{cite the holden lee paper as well} for sampling, we provide can control the quality of the learned conditional DDPM as follows:
% \begin{proposition}
%   Let $\thetahat$ denote the $\theta$ minimizing $\Lddpm(\theta, \Imitdata)$.  Suppose that there is some $\thetast \in \Theta$ such that $\scorefst = \scoref_{\thetast}$, that $\norm{\scoref_\theta} \leq B$, and that $\Nsample \geq \poly\left( \comp(\Theta), \epsilon^{-1}, \log\left( \dphorizon/\delta \right), d,  \dpstep^{-1}\right)$, where $\comp(\Theta)$ is learning-theoretic notion of complexity.\footnote{We give a precise bound in \Cref{prop:scorematching} depending on the Rademacher complexity of the induced function class, which can, in turn be bounded by standard metric notions of complexity as in, e.g., \citet{wainwright2019high}.}  For some $\dpstep$ and $\dpind$ polynomial in all problem parameters\footnote{Exact dependence is given in \eqref{eq:samplingparameters}.}, with probability at least $1 - \delta$, it holds for all $h \leq H$ that
%   \begin{align}
%     \ee_{\pathm{h} \sim \Dexp}\left[ \inf_{\coup \in \couple(\emph{\ddpm}(\scoref_{\thetahat, h}, \pathm), \Dexp(\cdot | \pathm))} \pp_{(\seqahat_h, \seqast_h) \sim \coup}\left( \norm{\seqahat_h - \seqast_h} \geq \epsilon \right) \right] \leq \epsilon.
%   \end{align}
% \end{proposition}
% The proof of \Cref{prop:samplingguarantee} combines a standard statistical learning analysis of \citet{block2020generative} with the analysis of \citet{chen2022sampling} \ab{cite holden lee as well} and can be found in Appendix \ref{app:scorematching}.  Under strong conditions on $\Dexp$, \citet[Theorem 2]{chen2022sampling} can be generalized to ensure that the sampling distribution is close to the true distribution in TV; as discussed in \Cref{rmk:notvguarantee}, the regularity properties required are too strong to hold in our setting and we thus instead apply \citet[Corollary 4]{chen2022sampling} to recover a weaker guarantee.  Appropriately handling this weaker metric is one of the key technical challenges in our analysis.  We now suppose that the guarantee in \eqref{eq:samplingguarantee} holds and consider the implications to imitation learning; indeed, our results are not specific to DDPMs but hold for any learned sampler satisfying \eqref{eq:samplingguarantee} and any suboptimality in our final bound's dependence on the number of samples comes from looseness of the theoretical analysis of DDPMs in \citet{chen2022sampling}.  We now discuss the assumptions required to state our guarantee.