%!TEX root = ../main.tex
\section{Introduction}\label{sec:intro}

%When training an agent to perform a given task, there have been two successful paradigms in recent literature: in the first, leveraging modern Reinforcement Learning techniques, the agent actively explores the enviroment in an attempt to optimize a known reward function \ab{citations}, while in the second, the agent attempts to learn a good policy from a dataset consisting of expert examples.  While the first approach has seen recent empirical success in a number of domains \ab{citations}, 

%\mscomment{We use \emph{multi-modal} to distinguish from (even stochastic policies) which provide demonstrations that all closely track one common  trajectory. Explain why hard to move past this}


Training dynamic agents from datasets of expert examples, known as 
\emph{imitation learning}, promises to take advantage of the plentiful demonstrations available in the modern data environment, in an analogous manner to the recent successes of language models conducting unsupervised learning on enormous corpora of text \citep{thoppilan2022lamda,vaswani2017attention}.  Imitation learning is especially exciting in robotics, where mass stores of pre-recorded demonstrations on Youtube \citep{abu2016youtube} or cheaply collected simulated trajectories \citep{mandlekar2021matters, dasari2019robonet} can be converted into learned robotic policies. 

For imitation learning to be a viable path toward generalist robotic behavior, it needs to be able to both represent and \emph{execute} the complex behaviors exhibited in the demonstrated data. An approach that has shown tremendous promise is \emph{generative behavior cloning:} fitting generative models, {such as}  diffusion models \cite{ajay2022conditional,chi2023diffusion,janner2022planning}, to expert demonstrations  with pure supervised learning.  In this paper, we ask:

\begin{quote}
\emph{Under what conditions can generative behavior cloning imitate arbitrarily complex expert behavior?}
\end{quote}
In this paper, we are interested in how \emph{algorithmic choices} interface with the \emph{dynamics of the agent's environment} to render  imitation possible.
%
The key challenge {separating imitation learning from vanilla supervised learning} is one of \emph{compounding error}:
when the learner executes the trained behavior in its environment, small {mistakes} can accumulate into larger ones; this in turn may bring the agent to regions of state space not seen during training, leading to larger-still deviations from intended trajectories. Without the strong requirement that the learner can interactively query the expert at new states \citep{laskey2017dart,ross2010efficient}, it is well understood that ensuring some form of \emph{stability} in the imitation learning procedure is {indispensable} \citep{tu2022sample, havens2021imitation,pfrommer2022tasil}. 
%The latter often requires other forms of stronger access to the expert policy, e.g. by querying gradients \cite{pfrommer2022tasil}. 
While many  natural notions of stability exist for simple behaviors,  how to \emph{enforce} stability when imitating more complex behaviors remains an open question. Multi-modal trajectories present a key example {of this challenge}: consider a robot navigating around an obstacle; because there is no difference between navigating around the object to the right and around to the left, the dataset of expert trajectories may include examples of both options.  This bifurcation of good trajectories can make it difficult for the agent to effectively choose which direction to go, possibly even causing the robot to oscillate between directions and run into the object. \citep{chi2023diffusion}. Moreover, human demonstrators correlate current actions with the past in order to \emph{commit} to either a right or left path, which makes even formulating the idea of an ``expert \emph{policy}'' a conceptually challenging one. Lastly,  bifurcations are necessarily \emph{incompatible} with previous notions of stability derived from classical control theory \cite{tu2022sample,havens2021imitation,pfrommer2022tasil}. \textbf{In this work, we investigate how these strong and often unrealistic assumptions on the expert policy can be replaced by practical (and often realistic) assumptions on available algorithms.}

%demonstrator policies correlate with past actions in sophisticated ways. 
%This simple example demonstrates both that the optimal policy is often stochastic, multi-modal and non-Markovian. 

% and that imitation learning in nonsmooth or discontinuous environments can be challenging.




%Many candidate definitions of stability have been studied by control theorists over the past decades, but none quite transfer to the sorts of hard bifurcations given in the example above \mscomment{..}

\subsection{Contributions. } 


% Figure environment removed



%This paper develops a theory of imitation learning flexible enough to imitate non-Markovian, multi-modal, and/or bifurcated demonstrations in smooth, nonlinear control systems by appealing to generative model as behavior cloners.  
As in previous work, we formalize behavior cloning in two stages: at \emph{train-time}, we learn a map from observations to distributions over actions, supervised by (state, action)-pairs from expert demonstrations coming from $N_{\mathrm{exp}}$ independent expert trajectories, while at \emph{test-time},  the learned map, or \emph{policy}, is executed on random initial states (distributed identically to initial training states).   Following the schematic of existing theoretical analyses of behavior cloning \cite{tu2022sample,pfrommer2022tasil,havens2021imitation}, we demonstrate that a policy trained by minimizing a certain supervised learning objective on expert demonstrations induces trajectories that approximate those of expert demonstrations. Our work considers a {significantly} more general setting than past theoretical literature, and one which reflects the strength of \emph{generative models} for imitation. 
One corollary of our key contributions is summarized in the following informal statement. The main technical insights leading to the proof of the theorem are detailed in the bullet points below it, and depicted in \Cref{fig:stab_tube}.

\newcommand{\Pexpact}{\Pr_{\mathrm{expert}~\mathrm{actions}}}
\newcommand{\Pimitact}{\Pr_{\mathrm{imitator}~\mathrm{actions}}}
\newtheorem*{thm:informal}{Theorem (informal)}
\begin{thm:informal} 
Consider a generative behavior cloner $\pihat$ that learns to predict sequences of expert actions on horizon $H$, along with low-level controllers that locally stabilize the trajectories.  Then, with a suitable data noising strategy, for all times $h \leq H$, 
\begin{align}
&\Pr[\emph{\text{expert \& imitator trajectories disagree at some time $h$ by }} \ge \epsilon] \\
&\quad\le \mathcal{O}_{\textsc{Iss}}\left(H\epsilon+ \frac{1}{\epsilon^2}\sum_h \Exp_{\mathrm{expert},h}\left[\cW_1(\Pexpact,\Pimitact)\right]\right)
\end{align}
where $\Exp_{\mathrm{expert},h}[\cW_1(\Pexpact,\Pimitact)]$ denotes a $1$-Wasserstein distance in an appropriate metric between the conditional distribution over expert and imitator actions given the observation at time step $h$, and where $\mathcal{O}_{\textsc{Iss}}$ hides constants depending polynomially on the stability properties of the low-level controllers, defined formally in \Cref{sec:stab_defs}.
\end{thm:informal}
We now detail the key ingredients of our results.

\begin{itemize}
    \item We imitate stochastic demonstrators that may exhibit {both} complex correlations between actions in their trajectories (e.g. be non-Markovian) and  multi-modal behavior. %, and bifurcations 
     %Due to stochasticity, we guarantee tracking in an appropriate optimal transport metric, and for our most general results, only on the the per-time-step \emph{marginal distributions} of the expert trajectories. %; due to non-Markovianity of the expert, we only guarantee matching the per-time-step \emph{marginal distributions} of the expert trajectories; still, joint distributions over trajectories can be approximated under stronger assumptions.
    %\item 
    The natural object to imitate  in this setting is the conditional probability distribution of expert actions given recent states, but {marginalized over past states.} We require said {conditional action distribution} to be learnable by a \textbf{generative model}, but otherwise arbitrarily complex: in particular, the conditional distribution of an expert actions given the state can be discontinuous (in any natural distance metric) as a function of state, as in the bifurcation depicted in \Cref{fig:stab_tube}\emph{(right)}. %Because of the generality such distributions, our learner's policy is trained as a \textbf{generative model} of the conditional distribution of actions.%.; it is not, as in past work, trained by $\ell_2$ regression from expert states to actions, nor via say, a logistic loss on a discrete action distribution.
    \item  We  obtain \textbf{rigorous, theoretical guarantees} and {without} requir{ing} either interactive data collection (e.g. \textsc{Dagger} \citep{ross2010efficient,laskey2017dart}), or access to gradients of the expert policy (as in \textsc{TaSil}\cite{pfrommer2022tasil}). Instead, we replace these assumption with an oracle, described below,  which \textbf{ synthesizes stabilizing, low-level policies} along training demonstrations---the green arrows in \Cref{fig:stab_tube}\emph{(left)}. This mirrors recent work on generative behavior cloning that find that providing state-commands through inverse dynamics controllers \cite{janner2022planning,ajay2022conditional} or position-command controllers of end effectors \cite{chi2023diffusion} leads to substantially improved performance. 
    \item We also apply a subtle-yet-significant modification to a popular \textbf{data noising} strategy, which we show yields both theoretical and empirical beneifts. Data noising ensures a helpful property we denote \emph{total variation continuity} that interpolates between modes in probability space ({without} naively averaging their trajectories in world space). This effectively ``fills in the missing gaps'' in bifurcations, as indicated by yellow arrows in \Cref{fig:stab_tube}.
\end{itemize}
%We summarize our main results in the following informal theorem, which controls the discrepancy between marginal distributions of expert and imitator trajectories consisting of states and actions in terms of the supervised learning loss of the generative behavior cloner:

Our main results, \Cref{thm:main_template,prop:TVC_main}, are reductions from imitation of complex expert trajectories to supervised generative learning of a specific conditional distribution. For concreteness, \Cref{thm:main} instantiates the generative modeling with Denoising Diffusion Probabilistic Models (DDPMs) of sufficient regularity and expressivity (as investigated empirically in \cite{chi2023diffusion,pearce2023imitating,hansen2023idql}), and establishes end-to-end guarantees for imitation of complex trajectories with sample complexity polynomial in relevant problem parameters. Our analysis framework exposes that any sufficient powerful generative learner obtains similar guarantees. Finally, we empirically validate the benefits of our proposed smoothing strategy in simulated robotic manipulation tasks. 
We now summarize the algorithmic choices and analytic ideas that  facilitate our reduction. 
    

\paragraph{A hierarchical approach. } As mentioned above, the key challenge is ensuring an appropriate notion of stability when imitating complex behaviors. We propose a hierarchical approach, both as an analysis tool and algorithmic design principle for imitation learning. During training, we consider learners that synthesize sequences of \emph{primitive controllers} - time-varying  control policies which  locally stabilize around each demonstration trajectory. We remark that this is \emph{standard practice} in generative behavior cloning, where policies generate state sequences that are tracked with either low-level robotic controllers \cite{chi2023diffusion}, or by learned inverse dynamics models \cite{janner2022planning,ajay2022conditional}. For concreteness, we analyze the case where   primitive controllers are \emph{linear gains} which can arise either from hand-coded linear feedback controllers (e.g. as in robotic position control), or, given access to a differentiable dynamics simulator, one can solve time-varying linear Riccati equation along the Jacobian linearizations of given expert demonstrations \cite{jacobson1970differential}. We break these \{demonstrator trajectory, primitive controller\} pairs into sub-trajectories we call ``chunks.''  Building on  \citep{chi2023diffusion}, we use DDPMs to estimate the conditional distribution of primitive controller chunks conditioned on recent states from the previous chunk. Our more general analysis in \Cref{app:gen_controllers} extends to arbitrary synthesized gains which may may include, for example, inverse dynamics controllers. \emph{The essential theoretical point is that we replace a stabilizing assumption on the \textbf{expert}, with a hierarchical stabilization assumption built into the \textbf{algorithm}.}


%We prove that the learner can approximate the expert's trajectory distribution provided three conditions hold:  along each expert trajectory,  (a) the dynamics are sufficiently smooth; (b) one  can synthesize primitive controllers that stabilize the Jacobian-linearized dynamics; and (c) one can approximately sample from conditional distributions over sequences of primitive controllers.  

\paragraph{A novel data noise-injection strategy.} During training, we adopt a popular noise-injection technique that corrupts trajectories (but not supervising actions) with a small amount of Gaussian noise \citep{ke2021grasping,laskey2017dart,ross2011reduction}. Unlike prior work, we propose adding noise \emph{back into the policies at inference time}, a technique that is both both provably indispensable in {in some situations}, and that our simulations suggest yields considerable benefit over the conventional approach of noising \emph{without} adding noise at inference time. 

\paragraph{Analysis framework, and the ``replica policy''.}  Our analysis reformulates our setting as imitation in a composite MDP, where composite states $\seqs_h$ corresponds to trajectory chunks, and composite-actions $\seqa_h$ correspond to subsequences of primitive controllers. A learner's policy maps composite-states to distributions over composite-actions, and a marginalization trick lets us represent non-Markovian demonstrator trajectories in the same format. The primitive controller sequences $\seqa_h$ provide the requisite stability, and we show that noising the learner policy at inference time ensures  continuity in the total variation distance (TVC). Whereas TVC relates actions selected at deviating states to actions selected {at} states visited by the expert, appropriate definitions of \emph{stability} of the composite actions ensures errors do not compound excessively. {For} the last key ingredient in the analysis, we show that training with noised states and, crucially, \emph{adding that same noise distribution back at inference time}, {causes} supervised training to converge to a ``replica'' version of the expert policy which is reminiscent of \emph{replica pairs} in statistical physics  \citep{mezard2009information}. We argue that,  up to the stability of controllers, this replica policy enjoys per-time-step marginals over states and actions close to those of the expert policy, thereby (a) enjoying the TVC properties endowed by the data noise injection strategy {and} (b) avoiding the distribution shift induced by naive data noising. To prove this, we construct a sophisticated coupling between the learned policy, replica policy, and other interpolating sequences; this construction is enabled by subtle measure-theoretic arguments demonstrating consistency of our couplings. We also establish stability guarantees for sequences of primitive controllers in non-linear control systems, which may be of independent interest. 
 
 

%\mscomment{replica?} Surprisingly, we find that augmenting training trajectories with Gaussian perturbations, and applying the same perturbation to the learner policy at test time \emph{essentially removes distribution shift.} The proof constructs a sophisticated couplings between two surrogates for the expert policiy (a teleporting and replica sequence), and the learners policy. \mscomment{mention some measure theoretic theoretic stuff on couplings nad condtionals. relies measurable selection of optimal transport couplings for continuous, which can be extended to lower-semicontinous costs (i.e. the cost which induces the total variation distance) via limiting arguments. }
\iftoggle{arxiv}
{
\subsection{Related Work}
    \input{body/full_related}
}
{
\nipspar{Abridged Related Work.} Due to space, we defer a full comparison to past work to \Cref{app:related}. 
 DDPMs, proposed in \cite{ho2020denoising,sohl2015deep}, along with their relatives have seen success in image generation \cite{song2019generative,ramesh2022hierarchical}, along with imitation learning (without data augmentation) \cite{janner2022planning,chi2023diffusion,pearce2023imitating}, which is the starting point of our work. Smoothing data augmentation is ubiquitous in modern imitation learning \cite{laskey2017dart} and our approach corresponds to that of \cite{ke2021grasping} but with noise added at inference time. Despite the benefits of adaptive data collection \cite{ross2011reduction,laskey2017dart}, adaptive demonstrations are more expensive to collect. Previous analyses of imitation learning without adaptive data collection have focused on classical control-theoretic notions of stability, notably incremental stability, \citep{tu2022sample,havens2021imitation,pfrommer2022tasil}, which require continuity, Markovianity, and often determinism, and preclude the bifurcations permitted in our setting.
 }
 {}

\iftoggle{arxiv}
{\subsection{Organization}}
{\nipspar{Organization.}} In \Cref{sec:setting} we formally introduce our setting and our main desideratum. Here, we define the low-level ``primitive controllers'' considered throughout.  In \Cref{sec:results}, we establish a reduction from imitation to conditional sampling with low-level stabilization via the aforementioned primitive controllers. First, \Cref{sec:stab_defs} introduces a formal notion of stability. Given access to an \emph{synthesis oracle} for generating these controllers around expert demonstrations, \Cref{sec:results_tvc} provides an imitation guarantee assuming the imitator policy satisfies the \emph{total variation continuity} property.  In \Cref{sec:results_smoothing}, we achieve this property with a more sophisticated reduction based on data smoothing. \Cref{sec:merits_synthesis} describes the advantages and disadvantages of our hierarchical approach to imitation. Subsequently,
 \Cref{sec:algorithm} instantiates these results with an algorithm, \toda{}, based on DDPMs, and discusses performance of our method in simulation. 
 In \Cref{sec:analysis}, we describe our proof framework in greater detail, and provide concluding remarks in \Cref{sec:discussion}.  The organization of our many appendices is given in \Cref{app:notation_and_org}. Notably, the main text of this paper considers affine primitive controllers for concreteness, whereas \Cref{app:gen_controllers} confirms that our results extend to general controller families. 





