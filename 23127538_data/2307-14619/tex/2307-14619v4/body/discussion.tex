%!TEX root = ../main.tex

\section{Discussion}\label{sec:discussion}

This work considerably loosened assumptions placed on the \emph{expert distribution} by introducing a synthesis oracle responsible for stabilization. How best to achieve low-level stabilization remains an open question.
We hope that this work encourages further empirical research into improving the stability of imitation learning, either via the hierarchical route proposed in this paper or via new innovations. 
In addition, while our theory allows for time-varying policies, in many robotics applications, time-invariant policies are more natural; we believe extension to time-invariant policies is possible but would require further complication. 
Despite these limitations, our work presents a significant step toward understanding the imitation of complex trajectories in natural control systems.


Lastly, as per \Cref{rem:TVC}, our guarantees \emph{can} hold for zero ($\sigma = 0$) noise-based smoothing if the learned policy $\pihat$ is total variation continuous (TVC). While uniform TVC is unrealistic for DDPMs, there might be some form of local or distributional TVC in the support of training data. It is an interesting direction for future work to investigate if this property does hold, as our experiment suggests that imitation is successful in the $\sigma = 0$ regime. 

