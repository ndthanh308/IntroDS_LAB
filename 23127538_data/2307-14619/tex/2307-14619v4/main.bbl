\begin{thebibliography}{73}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abu-El-Haija et~al.(2016)Abu-El-Haija, Kothari, Lee, Natsev, Toderici,
  Varadarajan, and Vijayanarasimhan]{abu2016youtube}
S.~Abu-El-Haija, N.~Kothari, J.~Lee, P.~Natsev, G.~Toderici, B.~Varadarajan,
  and S.~Vijayanarasimhan.
\newblock Youtube-8m: A large-scale video classification benchmark.
\newblock \emph{arXiv preprint arXiv:1609.08675}, 2016.

\bibitem[Ajay et~al.(2022{\natexlab{a}})Ajay, Du, Gupta, Tenenbaum, Jaakkola,
  and Agrawal]{ajay2022conditional}
A.~Ajay, Y.~Du, A.~Gupta, J.~Tenenbaum, T.~Jaakkola, and P.~Agrawal.
\newblock Is conditional generative modeling all you need for decision-making?
\newblock \emph{arXiv preprint arXiv:2211.15657}, 2022{\natexlab{a}}.

\bibitem[Ajay et~al.(2022{\natexlab{b}})Ajay, Gupta, Ghosh, Levine, and
  Agrawal]{ajay2022distributionally}
A.~Ajay, A.~Gupta, D.~Ghosh, S.~Levine, and P.~Agrawal.
\newblock Distributionally adaptive meta reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2210.03104}, 2022{\natexlab{b}}.

\bibitem[Altschuler and Talwar(2022)]{altschuler2022privacy}
J.~Altschuler and K.~Talwar.
\newblock Privacy of noisy stochastic gradient descent: More iterations without
  more privacy loss.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 3788--3800, 2022.

\bibitem[Altschuler and Chewi(2023)]{altschuler2023faster}
J.~M. Altschuler and S.~Chewi.
\newblock Faster high-accuracy log-concave sampling via algorithmic warm
  starts.
\newblock \emph{arXiv preprint arXiv:2302.10249}, 2023.

\bibitem[Anderson and Moore(2007)]{anderson2007optimal}
B.~D. Anderson and J.~B. Moore.
\newblock \emph{Optimal control: linear quadratic methods}.
\newblock Courier Corporation, 2007.

\bibitem[Angel and Spinka(2019)]{angel2019pairwise}
O.~Angel and Y.~Spinka.
\newblock Pairwise optimal coupling of multiple random variables.
\newblock \emph{arXiv preprint arXiv:1903.00632}, 2019.

\bibitem[Angeli(2002)]{angeli2002lyapunov}
D.~Angeli.
\newblock A lyapunov approach to incremental stability properties.
\newblock \emph{IEEE Transactions on Automatic Control}, 47\penalty0
  (3):\penalty0 410--421, 2002.

\bibitem[Bansal et~al.(2018)Bansal, Krizhevsky, and
  Ogale]{bansal2018chauffeurnet}
M.~Bansal, A.~Krizhevsky, and A.~Ogale.
\newblock Chauffeurnet: Learning to drive by imitating the best and
  synthesizing the worst.
\newblock \emph{arXiv preprint arXiv:1812.03079}, 2018.

\bibitem[Bartlett and Mendelson(2002)]{bartlett2002rademacher}
P.~L. Bartlett and S.~Mendelson.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Nov):\penalty0 463--482, 2002.

\bibitem[Bartlett et~al.(2021)Bartlett, Montanari, and
  Rakhlin]{bartlett2021deep}
P.~L. Bartlett, A.~Montanari, and A.~Rakhlin.
\newblock Deep learning: a statistical viewpoint.
\newblock \emph{Acta numerica}, 30:\penalty0 87--201, 2021.

\bibitem[Block et~al.(2020{\natexlab{a}})Block, Mroueh, and
  Rakhlin]{block2020generative}
A.~Block, Y.~Mroueh, and A.~Rakhlin.
\newblock Generative modeling with denoising auto-encoders and langevin
  sampling.
\newblock \emph{arXiv preprint arXiv:2002.00107}, 2020{\natexlab{a}}.

\bibitem[Block et~al.(2020{\natexlab{b}})Block, Mroueh, Rakhlin, and
  Ross]{block2020fast}
A.~Block, Y.~Mroueh, A.~Rakhlin, and J.~Ross.
\newblock Fast mixing of multi-scale langevin dynamics under the manifold
  hypothesis.
\newblock \emph{arXiv preprint arXiv:2006.11166}, 2020{\natexlab{b}}.

\bibitem[Bojarski et~al.(2016)Bojarski, Del~Testa, Dworakowski, Firner, Flepp,
  Goyal, Jackel, Monfort, Muller, Zhang, et~al.]{bojarski2016end}
M.~Bojarski, D.~Del~Testa, D.~Dworakowski, B.~Firner, B.~Flepp, P.~Goyal, L.~D.
  Jackel, M.~Monfort, U.~Muller, J.~Zhang, et~al.
\newblock End to end learning for self-driving cars.
\newblock \emph{arXiv preprint arXiv:1604.07316}, 2016.

\bibitem[Chang et~al.(2021)Chang, Uehara, Sreenivas, Kidambi, and
  Sun]{chang2021mitigating}
J.~D. Chang, M.~Uehara, D.~Sreenivas, R.~Kidambi, and W.~Sun.
\newblock Mitigating covariate shift in imitation learning via offline data
  without great coverage.
\newblock \emph{arXiv preprint arXiv:2106.03207}, 2021.

\bibitem[Chen et~al.(2022)Chen, Chewi, Li, Li, Salim, and
  Zhang]{chen2022sampling}
S.~Chen, S.~Chewi, J.~Li, Y.~Li, A.~Salim, and A.~Zhang.
\newblock Sampling is as easy as learning the score: theory for diffusion
  models with minimal data assumptions.
\newblock In \emph{NeurIPS 2022 Workshop on Score-Based Methods}, 2022.

\bibitem[Chi et~al.(2023)Chi, Feng, Du, Xu, Cousineau, Burchfiel, and
  Song]{chi2023diffusion}
C.~Chi, S.~Feng, Y.~Du, Z.~Xu, E.~Cousineau, B.~Burchfiel, and S.~Song.
\newblock Diffusion policy: Visuomotor policy learning via action diffusion.
\newblock \emph{arXiv preprint arXiv:2303.04137}, 2023.

\bibitem[Dasari et~al.(2019)Dasari, Ebert, Tian, Nair, Bucher, Schmeckpeper,
  Singh, Levine, and Finn]{dasari2019robonet}
S.~Dasari, F.~Ebert, S.~Tian, S.~Nair, B.~Bucher, K.~Schmeckpeper, S.~Singh,
  S.~Levine, and C.~Finn.
\newblock Robonet: Large-scale multi-robot learning.
\newblock \emph{arXiv preprint arXiv:1910.11215}, 2019.

\bibitem[De~Haan et~al.(2019)De~Haan, Jayaraman, and Levine]{de2019causal}
P.~De~Haan, D.~Jayaraman, and S.~Levine.
\newblock Causal confusion in imitation learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Durrett(2019)]{durrett2019probability}
R.~Durrett.
\newblock \emph{Probability: theory and examples}, volume~49.
\newblock Cambridge university press, 2019.

\bibitem[Finn et~al.(2017)Finn, Yu, Zhang, Abbeel, and Levine]{finn2017one}
C.~Finn, T.~Yu, T.~Zhang, P.~Abbeel, and S.~Levine.
\newblock One-shot visual imitation learning via meta-learning.
\newblock In \emph{Conference on robot learning}, pages 357--368. PMLR, 2017.

\bibitem[Foster and Rakhlin(2019)]{foster2019vector}
D.~J. Foster and A.~Rakhlin.
\newblock L-infinity vector contraction for rademacher complexity.
\newblock \emph{arXiv preprint arXiv:1911.06468}, 6, 2019.

\bibitem[Hagood and Thomson(2006)]{hagood2006recovering}
J.~W. Hagood and B.~S. Thomson.
\newblock Recovering a function from a dini derivative.
\newblock \emph{The American Mathematical Monthly}, 113\penalty0 (1):\penalty0
  34--46, 2006.

\bibitem[Hansen-Estruch et~al.(2023)Hansen-Estruch, Kostrikov, Janner, Kuba,
  and Levine]{hansen2023idql}
P.~Hansen-Estruch, I.~Kostrikov, M.~Janner, J.~G. Kuba, and S.~Levine.
\newblock Idql: Implicit q-learning as an actor-critic method with diffusion
  policies.
\newblock \emph{arXiv preprint arXiv:2304.10573}, 2023.

\bibitem[Havens and Hu(2021)]{havens2021imitation}
A.~Havens and B.~Hu.
\newblock On imitation learning of linear control policies: Enforcing stability
  and robustness constraints via lmi conditions.
\newblock In \emph{2021 American Control Conference (ACC)}, pages 882--887.
  IEEE, 2021.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Basart, Mu, Kadavath, Wang, Dorundo,
  Desai, Zhu, Parajuli, Guo, et~al.]{hendrycks2020jacob}
D.~Hendrycks, S.~Basart, N.~Mu, S.~Kadavath, F.~Wang, E.~Dorundo, R.~Desai,
  T.~Zhu, S.~Parajuli, M.~Guo, et~al.
\newblock Jacob steinhardt et justin gilmer. the many faces of robustness: A
  critical analysis of out-of-distribution generalization.
\newblock \emph{arXiv preprint arXiv:2006.16241}, 2020.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
J.~Ho, A.~Jain, and P.~Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 6840--6851, 2020.

\bibitem[Hussein et~al.(2017)Hussein, Gaber, Elyan, and
  Jayne]{hussein2017imitation}
A.~Hussein, M.~M. Gaber, E.~Elyan, and C.~Jayne.
\newblock Imitation learning: A survey of learning methods.
\newblock \emph{ACM Computing Surveys (CSUR)}, 50\penalty0 (2):\penalty0 1--35,
  2017.

\bibitem[Hussein et~al.(2018)Hussein, Elyan, Gaber, and Jayne]{hussein2018deep}
A.~Hussein, E.~Elyan, M.~M. Gaber, and C.~Jayne.
\newblock Deep imitation learning for 3d navigation tasks.
\newblock \emph{Neural computing and applications}, 29:\penalty0 389--404,
  2018.

\bibitem[Hyv{\"a}rinen and Dayan(2005)]{hyvarinen2005estimation}
A.~Hyv{\"a}rinen and P.~Dayan.
\newblock Estimation of non-normalized statistical models by score matching.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0 (4), 2005.

\bibitem[Jacobson and Mayne(1970)]{jacobson1970differential}
D.~H. Jacobson and D.~Q. Mayne.
\newblock Differential dynamic programming. number 24, 1970.

\bibitem[Janner et~al.(2022)Janner, Du, Tenenbaum, and
  Levine]{janner2022planning}
M.~Janner, Y.~Du, J.~B. Tenenbaum, and S.~Levine.
\newblock Planning with diffusion for flexible behavior synthesis.
\newblock \emph{arXiv preprint arXiv:2205.09991}, 2022.

\bibitem[Jin et~al.(2019)Jin, Netrapalli, Ge, Kakade, and Jordan]{jin2019short}
C.~Jin, P.~Netrapalli, R.~Ge, S.~M. Kakade, and M.~I. Jordan.
\newblock A short note on concentration inequalities for random vectors with
  subgaussian norm.
\newblock \emph{arXiv preprint arXiv:1902.03736}, 2019.

\bibitem[Ke et~al.(2021)Ke, Wang, Bhattacharjee, Boots, and
  Srinivasa]{ke2021grasping}
L.~Ke, J.~Wang, T.~Bhattacharjee, B.~Boots, and S.~Srinivasa.
\newblock Grasping with chopsticks: Combating covariate shift in model-free
  imitation learning for fine manipulation.
\newblock In \emph{2021 IEEE International Conference on Robotics and
  Automation (ICRA)}, pages 6185--6191. IEEE, 2021.

\bibitem[Kelly et~al.(2019)Kelly, Sidrane, Driggs-Campbell, and
  Kochenderfer]{kelly2019hg}
M.~Kelly, C.~Sidrane, K.~Driggs-Campbell, and M.~J. Kochenderfer.
\newblock Hg-dagger: Interactive imitation learning with human experts.
\newblock In \emph{2019 International Conference on Robotics and Automation
  (ICRA)}, pages 8077--8083. IEEE, 2019.

\bibitem[Khalil(2002)]{khalil2002nonlinear}
H.~Khalil.
\newblock \emph{Nonlinear Systems}.
\newblock Pearson Education. Prentice Hall, 2002.
\newblock ISBN 9780130673893.
\newblock URL \url{https://books.google.com/books?id=t\_d1QgAACAAJ}.

\bibitem[Kostrikov et~al.(2020)Kostrikov, Yarats, and
  Fergus]{kostrikov2020image}
I.~Kostrikov, D.~Yarats, and R.~Fergus.
\newblock Image augmentation is all you need: Regularizing deep reinforcement
  learning from pixels.
\newblock \emph{arXiv preprint arXiv:2004.13649}, 2020.

\bibitem[Laskey et~al.(2017)Laskey, Lee, Fox, Dragan, and
  Goldberg]{laskey2017dart}
M.~Laskey, J.~Lee, R.~Fox, A.~Dragan, and K.~Goldberg.
\newblock Dart: Noise injection for robust imitation learning.
\newblock In \emph{Conference on robot learning}, pages 143--156. PMLR, 2017.

\bibitem[Lee et~al.(2023)Lee, Lu, and Tan]{lee2023convergence}
H.~Lee, J.~Lu, and Y.~Tan.
\newblock Convergence of score-based generative modeling for general data
  distributions.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  pages 946--985. PMLR, 2023.

\bibitem[Mandlekar et~al.(2021)Mandlekar, Xu, Wong, Nasiriany, Wang, Kulkarni,
  Fei-Fei, Savarese, Zhu, and Mart{\'\i}n-Mart{\'\i}n]{mandlekar2021matters}
A.~Mandlekar, D.~Xu, J.~Wong, S.~Nasiriany, C.~Wang, R.~Kulkarni, L.~Fei-Fei,
  S.~Savarese, Y.~Zhu, and R.~Mart{\'\i}n-Mart{\'\i}n.
\newblock What matters in learning from offline human demonstrations for robot
  manipulation.
\newblock \emph{arXiv preprint arXiv:2108.03298}, 2021.

\bibitem[Maurer(2016)]{maurer2016vector}
A.~Maurer.
\newblock A vector-contraction inequality for rademacher complexities.
\newblock In \emph{Algorithmic Learning Theory: 27th International Conference,
  ALT 2016, Bari, Italy, October 19-21, 2016, Proceedings 27}, pages 3--17.
  Springer, 2016.

\bibitem[Mezard and Montanari(2009)]{mezard2009information}
M.~Mezard and A.~Montanari.
\newblock \emph{Information, physics, and computation}.
\newblock Oxford University Press, 2009.

\bibitem[Misra(2019)]{misra2019mish}
D.~Misra.
\newblock Mish: A self regularized non-monotonic activation function.
\newblock \emph{arXiv preprint arXiv:1908.08681}, 2019.

\bibitem[Nichol and Dhariwal(2021)]{nichol2021improved}
A.~Q. Nichol and P.~Dhariwal.
\newblock Improved denoising diffusion probabilistic models.
\newblock In \emph{International Conference on Machine Learning}, pages
  8162--8171. PMLR, 2021.

\bibitem[Pearce et~al.(2023)Pearce, Rashid, Kanervisto, Bignell, Sun,
  Georgescu, Macua, Tan, Momennejad, Hofmann, et~al.]{pearce2023imitating}
T.~Pearce, T.~Rashid, A.~Kanervisto, D.~Bignell, M.~Sun, R.~Georgescu, S.~V.
  Macua, S.~Z. Tan, I.~Momennejad, K.~Hofmann, et~al.
\newblock Imitating human behaviour with diffusion models.
\newblock \emph{arXiv preprint arXiv:2301.10677}, 2023.

\bibitem[Perez et~al.(2018)Perez, Strub, De~Vries, Dumoulin, and
  Courville]{perez2018film}
E.~Perez, F.~Strub, H.~De~Vries, V.~Dumoulin, and A.~Courville.
\newblock Film: Visual reasoning with a general conditioning layer.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem[Pfrommer et~al.(2022)Pfrommer, Zhang, Tu, and
  Matni]{pfrommer2022tasil}
D.~Pfrommer, T.~Zhang, S.~Tu, and N.~Matni.
\newblock Tasil: Taylor series imitation learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 20162--20174, 2022.

\bibitem[Pfrommer et~al.(2023)Pfrommer, Simchowitz, Westenbroek, Matni, and
  Tu]{pfrommer2023power}
D.~Pfrommer, M.~Simchowitz, T.~Westenbroek, N.~Matni, and S.~Tu.
\newblock The power of learned locally linear models for nonlinear policy
  optimization.
\newblock \emph{arXiv preprint arXiv:2305.09619}, 2023.

\bibitem[Polyanskiy and Wu(2022+)]{polyanskiy2022}
Y.~Polyanskiy and Y.~Wu.
\newblock \emph{Information Theory: From Coding to Learning}.
\newblock Cambridge University Press, 2022+.

\bibitem[Raginsky et~al.(2017)Raginsky, Rakhlin, and
  Telgarsky]{raginsky2017non}
M.~Raginsky, A.~Rakhlin, and M.~Telgarsky.
\newblock Non-convex learning via stochastic gradient langevin dynamics: a
  nonasymptotic analysis.
\newblock In \emph{Conference on Learning Theory}, pages 1674--1703. PMLR,
  2017.

\bibitem[Rakhlin et~al.(2017)Rakhlin, Sridharan, and
  Tsybakov]{rakhlin2017empirical}
A.~Rakhlin, K.~Sridharan, and A.~B. Tsybakov.
\newblock Empirical entropy, minimax regret and minimax risk.
\newblock \emph{Bernoulli Society for Mathematical Statistics and Probability},
  2017.

\bibitem[Ramesh et~al.(2022)Ramesh, Dhariwal, Nichol, Chu, and
  Chen]{ramesh2022hierarchical}
A.~Ramesh, P.~Dhariwal, A.~Nichol, C.~Chu, and M.~Chen.
\newblock Hierarchical text-conditional image generation with clip latents.
\newblock \emph{arXiv preprint arXiv:2204.06125}, 2022.

\bibitem[Ronneberger et~al.(2015)Ronneberger, Fischer, and
  Brox]{ronneberger2015u}
O.~Ronneberger, P.~Fischer, and T.~Brox.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In \emph{Medical Image Computing and Computer-Assisted
  Intervention--MICCAI 2015: 18th International Conference, Munich, Germany,
  October 5-9, 2015, Proceedings, Part III 18}, pages 234--241. Springer, 2015.

\bibitem[Ross and Bagnell(2010)]{ross2010efficient}
S.~Ross and D.~Bagnell.
\newblock Efficient reductions for imitation learning.
\newblock In \emph{Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 661--668. JMLR Workshop and
  Conference Proceedings, 2010.

\bibitem[Ross et~al.(2011)Ross, Gordon, and Bagnell]{ross2011reduction}
S.~Ross, G.~Gordon, and D.~Bagnell.
\newblock A reduction of imitation learning and structured prediction to
  no-regret online learning.
\newblock In \emph{Proceedings of the fourteenth international conference on
  artificial intelligence and statistics}, pages 627--635. JMLR Workshop and
  Conference Proceedings, 2011.

\bibitem[Sohl-Dickstein et~al.(2015)Sohl-Dickstein, Weiss, Maheswaranathan, and
  Ganguli]{sohl2015deep}
J.~Sohl-Dickstein, E.~Weiss, N.~Maheswaranathan, and S.~Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In \emph{International Conference on Machine Learning}, pages
  2256--2265. PMLR, 2015.

\bibitem[Song et~al.(2020{\natexlab{a}})Song, Meng, and
  Ermon]{song2020denoising}
J.~Song, C.~Meng, and S.~Ermon.
\newblock Denoising diffusion implicit models.
\newblock \emph{arXiv preprint arXiv:2010.02502}, 2020{\natexlab{a}}.

\bibitem[Song and Ermon(2019)]{song2019generative}
Y.~Song and S.~Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Song et~al.(2020{\natexlab{b}})Song, Garg, Shi, and
  Ermon]{song2020sliced}
Y.~Song, S.~Garg, J.~Shi, and S.~Ermon.
\newblock Sliced score matching: A scalable approach to density and score
  estimation.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 574--584.
  PMLR, 2020{\natexlab{b}}.

\bibitem[Strassen(1965)]{strassen1965existence}
V.~Strassen.
\newblock The existence of probability measures with given marginals.
\newblock \emph{The Annals of Mathematical Statistics}, 36\penalty0
  (2):\penalty0 423--439, 1965.

\bibitem[Sun et~al.(2023)Sun, Yang, and Mangharam]{sun2023mega}
X.~Sun, S.~Yang, and R.~Mangharam.
\newblock Mega-dagger: Imitation learning with multiple imperfect experts.
\newblock \emph{arXiv preprint arXiv:2303.00638}, 2023.

\bibitem[Thoppilan et~al.(2022)Thoppilan, De~Freitas, Hall, Shazeer,
  Kulshreshtha, Cheng, Jin, Bos, Baker, Du, et~al.]{thoppilan2022lamda}
R.~Thoppilan, D.~De~Freitas, J.~Hall, N.~Shazeer, A.~Kulshreshtha, H.-T. Cheng,
  A.~Jin, T.~Bos, L.~Baker, Y.~Du, et~al.
\newblock Lamda: Language models for dialog applications.
\newblock \emph{arXiv preprint arXiv:2201.08239}, 2022.

\bibitem[Tu et~al.(2022)Tu, Robey, Zhang, and Matni]{tu2022sample}
S.~Tu, A.~Robey, T.~Zhang, and N.~Matni.
\newblock On the sample complexity of stability constrained imitation learning.
\newblock In \emph{Learning for Dynamics and Control Conference}, pages
  180--191. PMLR, 2022.

\bibitem[Van~Handel(2014)]{van2014probability}
R.~Van~Handel.
\newblock Probability in high dimension.
\newblock Technical report, PRINCETON UNIV NJ, 2014.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Vershynin(2018)]{vershynin2018high}
R.~Vershynin.
\newblock \emph{High-dimensional probability: An introduction with applications
  in data science}, volume~47.
\newblock Cambridge university press, 2018.

\bibitem[Villani(2021)]{villani2021topics}
C.~Villani.
\newblock \emph{Topics in optimal transportation}, volume~58.
\newblock American Mathematical Soc., 2021.

\bibitem[Villani et~al.(2009)]{villani2009optimal}
C.~Villani et~al.
\newblock \emph{Optimal transport: old and new}, volume 338.
\newblock Springer, 2009.

\bibitem[Vincent(2011)]{vincent2011connection}
P.~Vincent.
\newblock A connection between score matching and denoising autoencoders.
\newblock \emph{Neural computation}, 23\penalty0 (7):\penalty0 1661--1674,
  2011.

\bibitem[Wainwright(2019)]{wainwright2019high}
M.~J. Wainwright.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge university press, 2019.

\bibitem[Westenbroek et~al.(2021)Westenbroek, Simchowitz, Jordan, and
  Sastry]{westenbroek2021stability}
T.~Westenbroek, M.~Simchowitz, M.~I. Jordan, and S.~S. Sastry.
\newblock On the stability of nonlinear receding horizon control: a geometric
  perspective.
\newblock In \emph{2021 60th IEEE Conference on Decision and Control (CDC)},
  pages 742--749. IEEE, 2021.

\bibitem[Wu and He(2018)]{wu2018group}
Y.~Wu and K.~He.
\newblock Group normalization.
\newblock In \emph{Proceedings of the European conference on computer vision
  (ECCV)}, pages 3--19, 2018.

\bibitem[Zhang et~al.(2018)Zhang, McCarthy, Jow, Lee, Chen, Goldberg, and
  Abbeel]{zhang2018deep}
T.~Zhang, Z.~McCarthy, O.~Jow, D.~Lee, X.~Chen, K.~Goldberg, and P.~Abbeel.
\newblock Deep imitation learning for complex manipulation tasks from virtual
  reality teleoperation.
\newblock In \emph{2018 IEEE International Conference on Robotics and
  Automation (ICRA)}, pages 5628--5635. IEEE, 2018.

\end{thebibliography}
