\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsfonts,amsthm}
\usepackage[a4paper,total={16cm,26cm}]{geometry}
\usepackage[small]{titlesec}
% \usepackage{graphicx,color}
\usepackage{xcolor}
\usepackage{mathrsfs} % \mathscr{} 
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{tcolorbox}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[misc]{ifsym}

\usepackage{makecell}

\setlength{\parskip}{0.25cm plus4mm minus3mm} % spacing between paragraphs
\setlength{\parindent}{0.75cm}
\linespread{1.15}

\newtheorem{theorem}{Theorem}%[section]
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}

\makeatletter
\renewenvironment{proof}[1][\proofname] {\par\pushQED{\qed}\normalfont\topsep6\p@\@plus6\p@\relax\trivlist\item[\hskip\labelsep\bfseries#1\@addpunct{.}]\ignorespaces}{\popQED\endtrivlist\@endpefalse}
\makeatother

\usepackage[font={small},labelfont={small}]{caption}

\title{\Large\bf Conformal prediction for frequency-severity modeling}

\author{
{\normalsize Helton Graziadei \tt\footnote{Corresponding author \Letter \, \tt\footnotesize helton.carvalho@fgv.br}} \\
\textit{\small School of Applied Mathematics, Getulio Vargas Foundation, Rio de Janeiro, Brazil} \bigskip \\ 
{\normalsize Paulo C. Marques F.} \\
\textit{\small Insper Institute of Education and Research, São Paulo, Brazil} \bigskip \\
{\normalsize Eduardo F. L. de Melo} \\ 
\textit{\small School of Applied Mathematics, Getulio Vargas Foundation, Rio de Janeiro, Brazil} \\
\textit{\small SUSEP - Superintendence of Private Insurance, Rio de Janeiro, Brazil} \\
\textit{\small UERJ - State University of Rio de Janeiro, Brazil} \bigskip \\ 
{\normalsize Rodrigo S. Targino} \\
\textit{\small School of Applied Mathematics, Getulio Vargas Foundation, Rio de Janeiro, Brazil} \bigskip 
} 

\date{\footnotesize July 2023}

\begin{document}

\maketitle

\begin{abstract}
\bigskip 
We present a nonparametric model-agnostic framework for building prediction intervals of insurance claims, with finite sample statistical guarantees, extending the technique of split conformal prediction to the domain of two-stage frequency-severity modeling. The effectiveness of the framework is showcased with simulated and real datasets. When the underlying severity model is a random forest, we extend the two-stage split conformal prediction procedure, showing how the out-of-bag mechanism can be leveraged to eliminate the need for a calibration set and to enable the production of prediction intervals with adaptive width.
\end{abstract}
\bigskip 
\noindent {\footnotesize\textbf{Keywords:} Frequency-severity modeling; Prediction sets; Two-stage split conformal prediction; Random forests; Two-stage out-of-bag conformal prediction.}

\vspace{5cm}

\thispagestyle{empty}

\section{Introduction}\label{sec:intro}

The statistical modeling of insurance claims is a crucial task of the property and casualty insurance industry. An essential ingredient in this modeling process is the two-stage approach, encompassing a frequency model and a severity model. At the first stage, a frequency model predicts the number of claims, while, at the second stage, a severity model predicts the average financial impact or size of a claim, given that it has occurred. Together, these two models map relevant predictors such as the policyholder's age, geographical location, and claim history, to the response variables describing the frequency and severity of the claims. This classic approach, known as the frequency-severity model, has been instrumental in the process of risk categorization, premium calculation, and, in a broader context, risk quantification of business portfolios for specific industry segments \cite{frees,shi}. 

More specifically, for insurance pricing, the form of the two-stage model varies according to the characteristics of the available data\cite{frees}. A common approach is to incorporate the modeling of two assumed independent variables: the frequency of claims, representing the number of claims within a given portfolio, and the severity (cost) associated with a claim. The pure premium is then calculated multiplying the predicted frequency and severity of a claim. Another approach is to model the so called aggregated losses, defined as the sum of claims in subsets of the underlying portfolio. Our results are applicable to both contexts.

Traditionally, generalized linear models \cite{mcculloch} have been the standard choice for both frequency and severity stages. A count regression model, typically based on the Poisson or the negative binomial distributions, is employed for the frequency variable, while a continuous regression model is designed for the severity component, commonly using the gamma or the log-normal distributions. In recent years, we have witnessed the inception of a paradigm shift, with the insurance industry increasingly gravitating towards the use of modern machine learning techniques \cite{su, roel} for the two modeling stages.

However, the plethora of currently available machine learning algorithms share a common limitation: their focus is on pointwise predictions, without an appropriate accounting of the confidence in their forecasts. In light of this, the aim of this paper is to explore a model agnostic, universally applicable framework, suitable for frequency-severity modeling tasks, which is capable of producing prediction intervals with statistical guarantees.

In Section \ref{sec:freqsev} we discuss the general process of frequency-severity modeling. Section \ref{sec:tsscp} presents the key ideas behind split conformal prediction \cite{vovk1999, vovk2005, lei}, showing how this technique can be extended to the two-stage frequency-severity scenario. The framework is exemplified in Section \ref{sec:examples} using simulated and real datasets, including a novel dataset of crop insurance claims in Brazil.  In Section \ref{sec:oobext} we show how to extend the two-stage split conformal prediction procedure when the underlying predictive models are random forests, exploiting the out-of-bag mechanism as a way to eliminate the need for a calibration set, as well as a means to produce prediction intervals of adaptive width. We state our conclusions in Section \ref{sec:concl}.

\subsection*{Related work}

To the best of the authors knowledge, this is the first application of conformal prediction techniques to the process of frequency-severity modeling.

\section{Frequency-severity modeling}\label{sec:freqsev}

Let $(X_1,D_1,Y_1),\dots,(X_n,D_n,Y_n)$ be a sample of independent and identically distributed triplets, in which, for the $i$-th sample unit,  $X_i\in\mathbb{R}^p$ is a vector of predictors, $D_i\in\mathbb{N}$ denotes the corresponding number of incurred claims, and $Y_i\in\mathbb{R}$ is the severity of the corresponding claims, following the definition given in \cite{su, roel}. More specifically, in Section \ref{ssec:mtpl}, we analyse a motor third party liability dataset from the Belgium insurance market, in which each row corresponds to a policyholder. In this case, $D_i$ is the number of incurred claims, while $Y_i$ stands for the average claim cost of a claim. The structure of the data is illustrated in Table \ref{tab:datastr}.Furthermore, our results are also applicable to cases in which there are some level of data aggregation. For instance, in Section \ref{ssec:brazil}, a soybean crop insurance dataset from Brazil is analyzed, in which each row is an aggregation of all contracts at the municipality level, with $D_i$ representing the number of claims and $Y_i$ being the average claim cost of within a municipality. 

\begin{table}[b]
\centering
\begin{tabular}{cccccc}
\hline \hline 
\\ 
Policy & Claim Count ($D_i$) & Total Claim Amount & Severity ($Y_i$) & Age ($X_{i,1}$) & Sex ($X_{i,2}$)   \\
\\ 
\hline \hline 
1      & $2$                  &  $2,\!852.76$           & $1,\!426.38$   & 24 & \texttt{male} \\
2      & 0                  & $0$             & $0$   & 64 & \texttt{female} \\
3      & $1$                  & $1,\!618.00$                  & $1,\!618.00$        & $5$0 & \texttt{male} \\
\hline \hline 
\end{tabular}
\caption{Structure of the motor third party liability dataset discussed in Section \ref{ssec:mtpl}, showing the first three policies and the values of two predictors: $X_{i,1}$ is the age of the policyholder (in years), and $X_{i,2}$ is the sex of the policyholder.}\label{tab:datastr}
\end{table}

We assume a two-stage data generating process of the form
$$
  D_i \mid X_i \sim F, \qquad\qquad Y_i \mid X_i, D_i \sim G,
$$
in which the severity distribution $G$ is such that $Y_i = 0$, whenever $D_i = 0$. The total claim cost is predicted using two regression models, a first one for the frequency component and a second one for the severity component. Among the many available options, Poisson regression and gamma regression, respectively, are commonly employed to model the two referred components. Modern machine learning models, such as random forests \cite{breimanRF}, have been recently applied in this two-stage scenario, with remarkable improvements in terms of predictive performance \cite{su,roel}. 

\section{Two-stage split conformal prediction}\label{sec:tsscp}

Conformal prediction \cite{vovk1999,papadopoulos,vovk2005,shafer2008,lei,fontana} was developed in the late 1990s as a nonparametric way to quantify the confidence in the forecasts made by general predictive models through the construction of prediction intervals with finite sample coverage guarantees \footnote{The coverage guarantees refers to the probability of the prediction interval will contain the observed value of the response variable within the test set. It does not relate to  the type of coverage provided by an insurance contract.}.

% Figure environment removed

In a regression context, we have a sample of independent and identically distributed\footnote{This assumption can be weakened by assuming only exchangeability of the pairs.} pairs $(X_1,Y_1),\dots,(X_n,Y_n)$, in which $X_i\in\mathbb{R}^d$ is a vector of predictors and $Y_i\in\mathbb{R}$ is a response variable. In the split conformal prediction scheme \cite{papadopoulos,lei}, we randomly split the sample units indexes into two disjoint sets $I_1$ and $I_2$, with $I_1\cup I_2=\{1,\dots,n\}$, corresponding to training and calibration samples, with sizes $n_1$ and $n_2$, respectively. Some regression model $\hat{\mu}$ is trained using the pairs in the training sample, and this model is used to compute the conformity scores $R_i=|Y_i-\hat{\mu}(X_i)|$ in the calibration set, i.e., for $i\in I_2$. Let $R_{(1)}<R_{(2)}<\dots<R_{(n_2)}$ denote the ordered calibration conformity scores.

The key idea of split conformal prediction is that, due to the assumed distributional symmetry of the data, the conformity score $R_{n+1}=|Y_{n+1}-\hat{\mu}(X_{n+1})|$ for a future random pair $(X_{n+1},Y_{n+1})$ is ranked uniformly among the ordered calibration conformity scores, implying that (assuming that there are no ties among the conformity scores)
\[
  1 - \alpha \leq P(Y_{n+1} \in C^{(1-\alpha)}(X_{n+1})) \leq 1 - \alpha + \frac{1}{n_2+1}, \tag{$\star$}
\]
for every training and calibration sample sizes, and any prescribed nominal miscoverage level $0<\alpha<1$, in which the conformal prediction set
$$
  C^{(1-\alpha)}(X_{n+1}) = [\,\hat{\mu}(X_{n+1}) - \hat{r}, \hat{\mu}(X_{n+1}) + \hat{r}\,],
$$
with $\hat{r}=R_{(\lceil(1-\alpha)(n_2+1)\rceil)}$, where $\lceil t\rceil$ denotes the smallest integer greater than or equal to the real number $t$ (see the proof of Theorem 2 in the supplementary materials of \cite{lei} for the formal details).

It is worth emphasizing that property $(\star)$ holds for high dimensional predictors ($d\gg 1$), no matter what model or algorithm is used to construct $\hat{\mu}$. All these combined features account for the so-called universality of the split conformal prediction framework. Figure \ref{fig:scp} provides some intuition on the split conformal prediction procedure in a simple case.

\begin{algorithm}[t]
\caption{Two-stage split conformal prediction}\label{algo:tsscp}
\begin{algorithmic}[1]
  \Require Dataset $\{(x_i,d_i,y_i)\}_{i=1}^n$, training and calibration indexes $I_1$ and $I_2$, future vector of predictors $x_{n+1}\in\mathbb{R}^p$, and nominal miscoverage level $0<\alpha<1$.
  \Statex
      \Function{PredictionInterval}{$\{(x_i,d_i,y_i)\}_{i=1}^n$, $I_1$, $I_2$, $x_{n+1}$, $\alpha$}
      \State Train frequency model $\hat{\mu}$ from $\{(x_i,d_i)\}_{i\in I_1}$
      \If{$\hat{\mu}(x_{n+1}) = 0$}
          \State \Return $\emptyset$
      \EndIf
      \State $I^+_2 \gets \{i\in I_2:\hat{\mu}(x_i)>0\}$
      % \State $n^+_2 \gets \text{number of indexes in} \; I^+_2$
      \State $n^+_2 \gets |I^+_2|$
      \State Train severity model $\hat{\psi}$ from $\{(x_i,d_i,y_i):i\in I_1, d_i>0\}$
      \For{$i\in I^+_2$}
        \State $r_i \gets |y_i - \hat{\psi}(x_i,\hat{\mu}(x_i))|$
      \EndFor
      \State $\hat{r} \gets r_{(\lceil(1-\alpha)(n^+_2+1)\rceil)}$
      \State \Return{$[\,\max\,\{\hat{\psi}(x_{n+1},\hat{\mu}(x_{n+1})) - \hat{r},0\}, \hat{\psi}(x_{n+1},\hat{\mu}(x_{n+1})) + \hat{r}\,]$}
      \EndFunction
\end{algorithmic}
\end{algorithm}

Extension of split conformal prediction to the two-stage frequency-severity scenario consists in maintaining the distributional symmetries of the original procedure while accounting for the presence of the different models on each one of the two stages.

We start with the random sample $(X_1,D_1,Y_1),\dots,(X_n,D_n,Y_n)$ described in Section \ref{sec:freqsev}. The data is split into training and calibration samples as described above. A frequency model $\hat{\mu}$ is built using the information in the training sample units $(X_i,D_i)$ with $i\in I_1$. After this, a severity model $\hat{\psi}$ is built from $(X_i,D_i,Y_i)$, using only those training sample units $i\in I_1$ for which the observed frequency $D_i>0$. Using the frequency model $\hat{\mu}$, we predict the frequency $\hat{\mu}(X_i)$ for the calibration sample units $i\in I_2$. Now, the severity model $\hat{\psi}$ is used to predict the total claim cost $\hat{\psi}(X_i,\hat{\mu}(X_i))$ only for the calibration sample units $i\in I_2$ such that $\hat{\mu}(X_i)>0$. We denote the indexes of the sample units in this calibration sub-sample by $I^+_2$. After these steps, we compute the conformity scores $R_i = |Y_i - \hat{\psi}(X_i,\hat{\mu}(X_i))|$ for the calibration sample units $i\in I^+_2$.

As in the original split conformal procedure, the assumed distributional symmetry of the data sequence implies, for a future random triplet $(X_{n+1},D_{n+1},Y_{n+1})$ such that $\hat{\mu}(X_{n+1})>0$, the conformity score $R_{n+1} = |Y_{n+1} - \hat{\psi}(X_{n+1},\hat{\mu}(X_{n+1}))|$ is uniformly ranked among the ordered calibration conformity scores, and the coverage property $(\star)$ holds for the conformal prediction set
$$
  C^{(1-\alpha)}(X_{n+1}) = [\,\max\,\{\hat{\psi}(X_{n+1},\hat{\mu}(X_{n+1})) - \hat{r},0\}, \hat{\psi}(X_{n+1},\hat{\mu}(X_{n+1})) + \hat{r}\,],
$$
if in $(\star)$ we replace $n_2$ by $n^+_2$ (the number of calibration sample units in $I^+_2$), and define $\hat{r}=R_{(\lceil(1-\alpha)(n^+_2+1)\rceil)}$. A programmatic description of this two-stage split conformal prediction procedure is given in Algorithm \ref{algo:tsscp}.

\section{Synthetic and real datasets}\label{sec:examples}

\subsection{Synthetic data}\label{ssec:synth}

% Figure environment removed

% Figure environment removed

Following the structure of the two-stage data generating process described in Section \ref{sec:freqsev}, we simulate a size $10,000$ dataset with $10$ independent predictors $X_{i,1},X_{i,2},\dots X_{i,10}$, each one with $10 \times U[0,1]$ distribution. The frequency variable $D_{i}$ is drawn from a mixture, with symmetric weights, comprising a point mass at zero and a Poisson($\lambda_i$) distribution, with rate parameter $\lambda_i = e^{0.01\times X_{1,i}}$. The purpose of this mixture is to mimic the behavior of the zero-inflated frequencies commonly found in insurance claims. The severity variable $Y_i$ is sampled conditionally: if $D_i=0$, then $Y_i=0$, otherwise, we draw $Y_i$ from an exponential distribution with expectation $4\times e^{X_{i,2}} + \sin(X_{i,3}\times X_{i, 4}) + 5\times X_{i, 5}^3$. Figure \ref{fig:synthdist} depicts the distributions of claim frequency ($D_i$) and severity ($Y_i$). Claims with zero frequency ($D_i = 0$) make up a significant portion ($67.8\%$) of the data.

The synthetic dataset is partitioned into training, calibration, and test samples, with sizes $5,\!000$, $2,\!500$, and $2,\!500$, respectively. We use a random forest with $1,\!000$ trees as the predictive model for the claims frequency. For the severity model, we consider two alternatives: a gamma regression and a random forest with $1,\!000$ trees. In terms of predictive performance, considering only the positive severity predictions on the test set, the mean absolute error using the gamma regression and the random forest are $14,\!196.73$ and $6,\!189.51$, respectively.

Using a nominal miscoverage level $\alpha=5\%$, Figure \ref{fig:synthint} shows the prediction intervals for $50$ random test sample units. For the option using the gamma regression severity model, the prediction intervals width is $81,\!551.56$, while the random forest severity model gives prediction intervals with width equal to $29,\!166,26$. The observed coverages of the prediction intervals considering the full test sample are $94.93\%$ and $94.87\%$, for the gamma regression and the random forest severity models, respectively.

\subsection{Motor third party liability in Belgium}\label{ssec:mtpl}

% Figure environment removed

% Figure environment removed

In this example, we use a motor third party liability (MTPL) portfolio dataset from a Belgium insurance company, related to the year 1997, also analysed by \cite{roel,denuit,klein}. This dataset contains information about $163,\!212$ policyholders. The proportion of zero claims in this dataset is approximately $88.88\%$. Figure \ref{fig:mtpldist} depicts the distributions of claim frequency and severity. A description of the variables is given in Table \ref{tab:mtpl}.

\begin{table}[ht]
\centering
\begin{tabular}{rl}
\hline
Variable & Description \\
\hline
\texttt{Coverage} & Type of coverage \\
\texttt{Fuel} &  Type of the motor fuel of the vehicle \\ 
\texttt{Sex} & Gender of the policyholder: male or female \\
\texttt{Use} & Main use of the vehicle: private or work \\ 
\texttt{Fleet} & Indicates whether the vehicle is part of a fleet \\ 
\texttt{Ageph} & Age of the policyholder (in years) \\
\texttt{Power}  & Horsepower of the motor vehicle \\
\texttt{Bm} & Bonus malus scale (lower value indicates good claim history)  \\ 
\texttt{Lat} & Latitude of the center of the municipality where the policyholder resides \\ 
\texttt{Long} & Longitude of the center of the municipality where the policyholder resides \\ 
\texttt{NClaims} & The number of claims \\ 
\texttt{Amount} & The total amount claimed by the policyholder (in euros) \\ 
\texttt{Expo} & The duration of the contract (proportion of the year)  \\
\hline
\end{tabular}
\captionof{table}{Variables in the MTPL dataset (reproduced from \cite{roel}).}\label{tab:mtpl}
\end{table}

The MTPL dataset is randomly partitioned into training, calibration, and test samples, according to the proportions $70\%$, $15\%$, and $15\%$, respectively. Again, we use a random forest with $1,\!000$ trees as the predictive model for the claims frequency. As before, we consider two alternatives for the severity model: a gamma regression and a random forest with $1,\!000$ trees. For the severity predictions on the test set, the mean absolute error using the gamma regression and the random forest are $1,\!389.59$ and $628.57$ euros, respectively.

Using a nominal miscoverage level $\alpha=5\%$, Figure \ref{fig:mtplint} shows the prediction intervals for $50$ random test sample units. For the scenario using the gamma regression severity model, the prediction intervals average width is $3,\!153.94$ euros, while the random forest severity model gives prediction intervals with average width equal to $1,\!568.74$ euros. The observed coverages of the prediction intervals are $ 96.18\%$ and $97.13\%$, for the gamma regression and the random forest severity models, respectively.

\subsection{Crop insurance in Brazil}\label{ssec:brazil}

Now, we consider data related to crop insurance in Brazil. The data was compiled from the Brazilian Agricultural Ministry public records\footnote{The raw data is publicly available at: https://dados.agricultura.gov.br/dataset/sisser3}, encompassing crop insurance policies within a time frame ranging from March 2016 to February 2022. All policies belong to the Rural Grant Program ({\it Programa de Subvenção Rural, PSR}), which is maintained by the Brazilian federal government in order to subsidize a percentage of the insurance premiums for farmers.

In our analysis we focus on policies related to soybean crops, due to the peculiarities of its agricultural cycles, and its relevance for the Brazilian economy. The dataset is aggregated at the municipality level. We consider the sample unit as the municipality in a given year crop. Therefore, the second stage component actually refers to an aggregated loss within each municipality. Furthermore, municipalities without at least one policy during a specific agricultural year were excluded from the dataset.

The dataset incorporates relevant climate variables with monthly cumulative precipitation (in millimeters) and monthly average temperatures (in Celsius) which were obtained from version 4 of the Climatic Research Unit gridded Time Series \cite{harris2020}. The detailed list of variables of this dataset is presented in Table \ref{tab:rural}.

\begin{table}[t]
\centering
\begin{tabular}{r|l}
\hline
Variable & Description \\
\hline
\texttt{Municipality} & Name of the municipality \\
\texttt{Year} & Agricultural year, numbered from 1 to 6 \\
\texttt{Latitude}  & Latitude of the municipality \\
\texttt{Longitude} &  Longitude of the municipality \\ 
\texttt{AWC} & Available water capacity of the municipality \\
\texttt{Soil} & Predominant type of soil found in the municipality \\ 
\texttt{TempPC1} & First principal component of temperature \\ 
\texttt{TempPC2} & Second principal component of temperature \\ 
\texttt{PrecPC1} & First principal component of precipitation \\ 
\texttt{PrecPC2} & Second principal component precipitation \\ 
\texttt{PrecPC3} & Third principal component precipitation \\ 
\texttt{PrecPC4} & Fourth principal component of precipitation \\ 
\texttt{Claims}  & Number of claims \\
\texttt{RelativeLoss} &  Measure of relative loss, ratio of total claims amounts to number of claims \\ 
\hline
\end{tabular}
\captionof{table}{Variables in the crop insurance dataset.}\label{tab:rural}
\end{table}

% Figure environment removed

The distributions of frequency and relative loss by municipality is given in Figure \ref{fig:ruraldist}. The proportion of claims in this dataset is approximately $32.57\%$. This substantially high proportion is due to the fact that soybean crops are highly sensitive to climate factors, especially droughts, which happened in the considered time frame. 

We randomly split the crop insurance dataset into training, calibration, and test samples, according to the proportions $70\%$, $15\%$, and $15\%$, respectively. Again, we use a random forest with $1,\!000$ trees as the predictive model for the claims frequency. As before, we consider two alternatives for the severity model: a gamma regression and a random forest with $1,\!000$ trees. For the positive relative loss predictions on the test set, the mean absolute error using the gamma regression and the random forest are $142,\!988.00$ and $105,\!796.80$, respectively.

Using a nominal miscoverage level $\alpha=5\%$, Figure \ref{fig:ruralint} shows the prediction intervals for $50$ random test sample units. For the scenario using the gamma regression severity model, the prediction intervals average width is $719,\!859.50$, while the random forest relative loss model gives prediction intervals with width equal to $473,\!635.00$. The observed coverages of the prediction intervals are $96.40\%$ and $94.42\%$, for the gamma regression and the random forest relative loss models, respectively.

% Figure environment removed

\section{Out-of-bag extension}\label{sec:oobext}

The two-stage split conformal prediction has two undesired features. Firstly, the need to have a separate calibration sample reduces the amount of data available to train the frequency and the severity models. Secondly, the prediction intervals for future observations have exactly the same width when the lower limits are non negative numbers. When a random forest, or any other predictive model relying on bagging \cite{breimanBAG}, is used for the severity stage, the out-of-bag mechanism \cite{breimanRF,marques} can be exploited to address both issues: a calibration sample is no longer needed, and the prediction intervals begin to have adaptive width. To understand this extension of Algorithm \ref{algo:tsscp} we need to review a few details about the inner workings of a random forest, in the context of regression.

We are in the usual supervised learning regression context \cite{esl,murphy,bishop}, and a training sample $\{(x_i,y_i)\}_{i=1}^n$ is available, for which $x_i\in\mathbb{R}^p$ is a vector of predictors, and $y\in\mathbb{R}$ is the response variable. In the 1980s, Breiman and coauthors \cite{cart} developed the classification and regression trees (CART) algorithm, which recursively partitions the space of predictor variables, greedily looking for splits in the training data which minimize a quadratic cost function. If we resample the training set, drawing $n$ observations uniformly with replacement, this bootstrap sample \cite{efron} can be used to train a tall regression tree using the CART algorithm. By repeating this process $B$ times, we get the regression trees $\hat{\psi}^{(1)},\dots,\hat{\psi}^{(B)}$, which can be averaged to produce the aggregated regression function
$$
  \hat{\psi}(\;\cdot\;) = \frac{1}{B} \sum_{j=1}^B\hat{\psi}^{(j)}(\;\cdot\;).
$$
This general aggregation process of regression functions trained from bootstrap samples, known as bagging \cite{breimanBAG}, can be further optimized by uniformly drawing without replacement a random subset of predictors when deciding each split of each regression tree in the ensemble, resulting in the definition of a random forest \cite{breimanRF}.

From the classical bias-variance trade-off perspective \cite{esl}, the effectiveness of random forests in making accurate predictions can be attributed to two factors. Firstly, by averaging numerous tall regression trees, which have low bias and high variance, random forests effectively reduce variance. Secondly, Breiman's randomized split decision mechanism further enhances the performance by breaking up correlations between predictions made by individual trees in the ensemble.

It is easy to prove that this use of bootstrapping in the training process of a random forest implies that each training observation is not used when cultivating approximately $e^{-1}\approx36.8\%$ of the trees in the ensemble. Letting $\mathcal{O}_i\subset\{1,2,\dots,B\}$ denote the indexes of the trees for which the $i$-th training observation stayed out of the corresponding bootstrap samples -- stayed ``out-of-bag'', in Breiman's terminology -- the regression trees $\{\hat{\psi}^{(j)}:j\in\mathcal{O}_i\}$ form a random subforest which can be used to extend Algorithm \ref{algo:tsscp} in the following manner.

Any frequency model $\hat{\mu}$ is trained from $\{(x_i,d_i)\}_{i=1}^n$. Let $I^+$ denote the indexes of the $n^+$ training sample units whose observed frequency $d_i$ are not zero. For the severity model, grow a random forest $\hat{\psi}=\{\hat{\psi}^{(j)}\}_{j\in B}$, with $B$ trees, using the training sub-sample $\{(x_i,d_i,y_i)\}_{i\in I^+}$. Compute
$$
  \hat{y}_i = \frac{1}{|\mathcal{O}_i|} \sum_{j\in\mathcal{O}_i} \hat{\psi}^{(j)}(x_i,\hat{\mu}(x_i)) \quad\text{and}\quad \hat{\delta}_i = \frac{1}{|\mathcal{O}_i|} \sum_{j\in\mathcal{O}_i} \left| \hat{\psi}^{(j)}(x_i,\hat{\mu}(x_i)) - \hat{y}_i\right|,
$$
for $i\in I^+$, in which $|\mathcal{O}_i|$ denotes the number of trees whose indexes are in $\mathcal{O}_i$. For $i\in I^+$, compute the weighted absolute residuals $r_i = \left|y_i - \hat{y}_i\right| / \hat{\delta_i}$. For a nominal miscoverage level $0<\alpha<1$, letting $\hat{r} = r_{(\lceil(1-\alpha)(n^+ \,+\, 1)\rceil)}$, the corresponding prediction interval for a future vector of predictors $x_{n+1}$ is given by
$$
  [\,\max\,\{\hat{\psi}(x_{n+1},\hat{\mu}(x_{n+1})) - \hat{r}\times\hat{\delta}_{n+1}, 0\}, \hat{\psi}(x_{n+1},\hat{\mu}(x_{n+1})) + \hat{r}\times\hat{\delta}_{n+1}\,],
$$
in which $\hat{\delta}_{n+1} = \frac{1}{B} \sum_{j=1}^B \left| \hat{\psi}^{(j)}(x_{n+1},\hat{\mu}(x_{n+1})) - \hat{\psi}(x_{n+1},\hat{\mu}(x_{n+1}))\right|$. A more programmatic description of this two-stage out-of-bag conformal prediction procedure is given in Algorithm \ref{algo:tsoobcp}.

\begin{algorithm}[t]
\caption{Two-stage out-of-bag conformal prediction}\label{algo:tsoobcp}
\begin{algorithmic}[1]
  \Require Dataset $\{(x_i,d_i,y_i)\}_{i=1}^n$, number $B$ of trees used to train the severity random forest, future vector of predictors $x_{n+1}\in\mathbb{R}^p$, and nominal miscoverage level $0<\alpha<1$.
  \Statex
      \Function{PredictionInterval}{$\{(x_i,d_i,y_i)\}_{i=1}^n$, $B$, $x_{n+1}$, $\alpha$}
      \State Train frequency model $\hat{\mu}$ from $\{(x_i,d_i)\}_{i=1}^n$
      \If{$\hat{\mu}(x_{n+1})=0$}
          \State \Return $\emptyset$
      \EndIf
      \State $I^+ \gets \{i \in \{1,\dots,n\} : d_i>0\}$
      % \State $n^+ \gets \text{number of indexes in} \; I$
      \State $n^+ \gets |I^+|$
      \State Train severity random forest $\hat{\psi}=\{\hat{\psi}^{(j)}\}_{j\in B}$ from $\{(x_i,d_i,y_i)\}_{i\in I^+}$
      \For{$i \in I^+$}
          \State $\mathcal{O}_i \gets \emptyset$
          \For{$j \gets 1 \textrm{ to } B$}
              \If{$i$-th sample unit not in the $j$-th bootstrap sample}
                  \State $\mathcal{O}_i \gets \mathcal{O}_i \cup \{j\}$
              \EndIf
          \EndFor
          \State $\hat{y}_i \gets \frac{1}{|\mathcal{O}_i|} \sum_{j\in\mathcal{O}_i} \hat{\psi}^{(j)}(x_i,\hat{\mu}(x_i))$
          \State $\hat{\delta}_i \gets \frac{1}{|\mathcal{O}_i|} \sum_{j\in\mathcal{O}_i} \left| \hat{\psi}^{(j)}(x_i,\hat{\mu}(x_i)) - \hat{y}_i\right|$
          \State $r_i \gets \left|y_i - \hat{y}_i\right| / \hat{\delta_i}$
      \EndFor
      \State $\hat{r} \gets r_{(\lceil(1-\alpha)(n^+ \,+\, 1)\rceil)}$
      \State $\hat{\delta}_{n+1} \gets \frac{1}{B} \sum_{j=1}^B \left| \hat{\psi}^{(j)}(x_{n+1},\hat{\mu}(x_{n+1})) - \hat{\psi}(x_{n+1},\hat{\mu}(x_{n+1}))\right|$
      \State \Return{$[\,\max\,\{\hat{\psi}(x_{n+1},\hat{\mu}(x_{n+1})) - \hat{r}\times\hat{\delta}_{n+1}, 0\}, \hat{\psi}(x_{n+1},\hat{\mu}(x_{n+1})) + \hat{r}\times\hat{\delta}_{n+1}\,]$}
      \EndFunction
\end{algorithmic}
\end{algorithm}

In order to elucidate our methodology, we randomly partition the crop insurance dataset into training and test samples, according to the proportions $70\%$ and $30\%$, respectively. For both the frequency and relative loss models, we employ a random forest with $1,\!000$  trees. Setting a nominal miscoverage level $\alpha=5\%$, Figure \ref{fig:cropoobint} shows the out-of-bag conformal prediction intervals for $100$ random sample units. The median width of the prediction intervals is $581,\!607.70$, whereas the observed coverage obtained is $96.84\%$.

% Figure environment removed

\section{Concluding remarks}\label{sec:concl}

The increasing use of machine learning algorithms for the predictive modeling of frequency-severity tasks demands the development of a comprehensive framework to assess the confidence in the corresponding forecasts. It is desirable that this framework avoids relying on strong assumptions about the underlying data generation process and the specific features of the predictive algorithms selected for the modeling task. Additionally, it should generate prediction intervals with reliable finite sample coverage guarantees. This paper contributes in this direction, proposing conformal prediction procedures suitable for dealing with the two-stage nature of frequency-severity modeling, codified in the more general Algorithm \ref{algo:tsscp} and the more specialized Algorithm \ref{algo:tsoobcp}, which exhibit different features in terms of data usage and properties of the generated prediction intervals. The general performance exhibited by the procedures developed in the paper motivates the future investigation of conformal prediction ideas applied to other actuarial science problems.

\section*{Data availability}
Open source \texttt{R} \cite{R} codes  and data for all the examples in the paper are openly available at \url{https://github.com/heltongraziadei/conformal-fs}.


\bibliographystyle{ieeetr}
\bibliography{bibliography}

\end{document}
