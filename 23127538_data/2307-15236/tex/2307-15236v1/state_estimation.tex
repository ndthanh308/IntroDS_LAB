\label{sec:state_estimation}

For a mobile robot to successfully navigate through its environment, it needs to maintain an estimate of its state (pose, velocity, etc).
Such estimates are typically obtained through \textit{sensor fusion}, or the process of determining a robot's state relative to a reference frame given many noisy sensor measurements, such as IMUs, wheel encoders, pose measurements, GPS, and more.
The output of this fusion is an approximation of a robot's state variables and typically a measure of confidence regarding the estimate. 

ROS Enhancement Proposal (REP) 105 establishes the coordinate frames for common types of robotic systems to ensure interoperability of components \cite{rep105}. The transformation tree containing these coordinate frames is populated by the algorithms of this and the next sections and managed by the \textit{tf} library \cite{tf}.
\textit{tf} is utilized by a litany of applications to transform sensor data or relative vectors through time-varying reference frames - not simply for robot state tracking.

Within this formulation, the {\tt odom} to {\tt base\_link} transform can be generated by integrating a robot's wheel encoder data to provide a pose estimate that can be used for local control of the robot. That pose will drift over time, however, so REP 105 also specifies a {\tt map} to {\tt base\_link} transform\footnote{Technically, the {\tt tf} library does not permit a given coordinate frame having two parents, so we instead use the {\tt map} to {\tt base\_link} transform and the inverse of the {\tt odom} to {\tt base\_link} transform to generate a {\tt map} to {\tt odom} transform.} that represents a drift-free estimate of the robot's pose. This pose is useful for carrying out operations such as global planning. However, it is subject to discontinuities, making it a poor choice for control algorithms. The {\tt map} to {\tt base\_link} transform can be provided through a localization package, described in more detail in section \ref{sec:localization}.

Alternatively, both transforms can be provided through the use of state estimators. In the case of the {\tt odom} to {\tt base\_link} transform, this would typically mean fusing wheel encoder data with other continuous sources like angular velocity data from an IMU. For the {\tt map} to {\tt base\_link} transform, the pose estimate from the localization package might be fused with other global pose estimates or with the same sources fused in the {\tt odom} to {\tt base\_link} state estimator. This is useful if the localization package produces infrequent or irregularly-timed pose measurements, as it enables the global pose estimate to be generated at a fixed frequency.

While there are a myriad of algorithms for performing sensor fusion, two well-known and robust classes of approaches are Kalman filtering and factor graphs, both of which have implementations within the ROS~2 ecosystem.

\subsection{State Estimators}

\label{sec:rl}
\textbf{Robot Localization}
In the 60 years since its introduction, Kalman filtering has become one of the best-known and widely adopted approaches to state estimation \cite{kalman_filter}. The Kalman Filter algorithm manages the state estimate in two stages: predict and update. While the original Kalman Filter works for linear systems, the Extended Kalman Filter (EKF) addresses this shortcoming by linearizing the state transition function \textit{f} and measurement model \textit{h} around the current state. It then uses the resulting Jacobian matrices $\textbf{F}_t$ and $\textbf{H}_t$ in place of the original formulation's state transition matrix \textbf{F} and measurement model \textbf{H}, respectively in Eq. \ref{fh}.

\begin{equation}
\begin{split}
\label{fh}
\textbf{F}_t & = \frac{\partial f}{\partial \hat{\textbf{x}}_{t-1}} \\
\textbf{H}_t & = \frac{\partial h}{\partial \hat{\textbf{x}}_{t-1}}.
\end{split}
\end{equation}

A further extension known as the Unscented Kalman Filter (UKF) eschews the computation of Jacobian matrices in favor of using a set of \textit{sigma points} to carry out state projection \cite{unscented_filter}.
Each sigma point is effectively the most recent system state estimate $\hat{\textbf{x}}_{t-1}$, but with one dimension perturbed in proportion to the current covariance matrix $\hat{\textbf{P}}_{t-1}$.
Each sigma point is then projected through \textit{f} and the predicted state $\hat{\textbf{x}}_{t}$ and covariance $\hat{\textbf{P}}_{t}$ are recovered.

In ROS~2, the \textit{robot\_localization} (\textit{r\_l}) package provides implementations of both an EKF and UKF \cite{rl}. The package supports an unlimited number of sensor inputs from common ROS message formats such as {\tt Odometry}, {\tt Imu}, {\tt PoseWithCovarianceStamped}, and {\tt TwistWithCovarianceStamped}. Users also have the ability to control which dimensions from any given measurement source will be fused in the final state estimate. For example, if an IMU sensor produces faulty heading data due to magnetic interference from the magnetometer, that dimension can be excluded when fusing the IMU measurements. Pose data can also be fused \textit{differentially}, i.e., consecutive poses can be converted into velocities before being passed to the filter. This feature is useful when attempting to fuse world-referenced pose data from sources that do not agree with one another.

\textit{r\_l} uses an omnidirectional three-dimensional kinematic state transition function to estimate the 15-dimensional state vector given in Eq. \ref{rlstate}.

\begin{equation} \label{rlstate}
\textbf{x} = [x, y, z, \phi, \theta, \psi, \Dot{x}, \Dot{y}, \Dot{z}, \Dot{\phi}, \Dot{\theta}, \Dot{\psi}, \Ddot{x}, \Ddot{y}, \Ddot{z}]
\end{equation}

While the package does not support any other kinematic models, common models like the unicycle model can be obtained by clamping all three-dimensional state variables (as well as $\Dot{y}$ and $\Ddot{y}$) to 0. Furthermore, kinematic models such as Ackermann steering can be used if the measurement source provides absolute pose data and the appropriate process noise covariance matrix entries are increased to force the filter to trust measurements more than the filter.
In practice, this is sufficient for most mobile robotics applications and the most common method of sensor fusion for state estimation in ROS over the last half-decade.

The large number of parameters available to the user in \textit{r\_l} can be used to tune the filter's performance and tailor it to the user's application. In general, the following practices are recommended:
\begin{itemize}
\item If operating in a planar environment, set {\tt two\_d\_mode} to \textit{true}. While the filter will still continue to perform full 3D state estimation, dimensions outside SE2 will be set to 0.
\item For differential drive platforms, the only body-frame velocities that are typically relevant are $\Dot{x}$ and $\Dot{\theta}$. However, the 0 value for $\Dot{y}$ that is produced by the wheel encoder odometry should be fused in the filter. This prevents the filter from artificially generating non-zero $\Dot{y}$ in the state as the robot goes around turns.
\item Every linear and rotational dimension must have a reference (i.e., it must be measured by one of the user's sensor inputs). This measurement can be provided via absolute pose data \textit{or} velocity data. Prefer pose sources when fusing only a single input in that dimension. Prefer one pose and many velocity sources when fusing multiple sources for a dimension.
\item While the filter accepts linear acceleration measurements, they are generally insufficient for state estimation, as the double-integration of linear acceleration without any velocity or pose reference will lead to unbounded growth in that dimension.
\item Start with the minimum set required to provide a reference for every linear and angular dimension, and then add inputs one-at-a-time.
\item Take time to understand the coordinate frames of sensor data. Make sure that transforms are available for all data that is not provided in the principal coordinate frames.
\end{itemize}

\textbf{Fuse} The nonlinear state estimation problem can also be solved through iterative numerical optimization. It can be factored into the following least-squares form:

\begin{equation} \label{leastsquares}
\hat{\textbf{x}}_t = \arg \min_{\textbf{x}} \sum_i\left(\frac{\textbf{y}_i - g(\textbf{x})}{\Sigma^{\frac{1}{2}}}\right)^2
\end{equation}

where $\hat{\textbf{x}}_t$ is the optimal state estimate.
We obtain it by finding the state that minimizes the sum squared error between each observation $\textbf{y}_i$ and the transformed state $g(\textbf{x})$, weighted by the inverse square root of the covariance $\Sigma$.

Factor graphs represent this formulation graphically, as shown in Fig. \ref{fig:factorgraph}.
Two types of nodes are present in the graph: \textit{variable nodes} represent specific quantities that we want to estimate (e.g., the robot's pose at time \textit{t}) and \textit{factor nodes} represent constraints on those variables (e.g., kinematic constraints via a motion model).
In Fig. \ref{fig:factorgraph}, the variable nodes are the states $\textbf{x}_1$ through $\textbf{x}_4$, with the square factor nodes representing constraints between the connected states. This example shows a common use case for sensor fusion, in which asynchronous measurements arrive from different sensors. Each sensor measurement creates a constraint between two states, leading to two separate sets of vertices. In order to unify the graph, we create kinematic constraints between $\textbf{x}_1$ and $\textbf{x}_2$, $\textbf{x}_2$ and $\textbf{x}_3$, and $\textbf{x}_3$ and $\textbf{x}_4$. The graph is then optimized to find the most likely values for the states.

% Figure environment removed

The ROS~2 \textit{fuse} package provides a factor graph framework that can be used in state estimation, mapping, calibration, and more \cite{fuse, fusegh}.
Within \textit{fuse}, each factor node can represent a measurement constraint or a kinematic constraint. Measurement constraints represent the error term in Eq.~\ref{leastsquares} as $\textbf{R}^{-\frac{1}{2}}\left(\textbf{z}_i - h(\textbf{x})\right)$, where \textit{h}, the sensor model, is a function that transforms the variables in the constraint into the space of the measurement.
The covariance matrix \textbf{R} controls how much weight the error term in question carries during the optimization process.
A small covariance would result in the measurement being heavily weighted.

Kinematic constraints represent cost in the form $\textbf{P}^{-\frac{1}{2}}\left(\textbf{x}_i - f(\textbf{x}_{t-1})\right)$, where $\textbf{x}_i$ is the current value of the variables connected to the constraint and the function \textit{f} is a state transition function that projects the variable values in $\textbf{x}_{t-1}$ forward in time.

\textit{fuse} makes extensive use of dynamically loaded plugins to manage the highly configurable graph construction and optimization process.
A number of plugins are already provided for common mobile robotics needs, but users may create and utilize their own plugins for custom requirements.
Each type of plugin are detailed below.

\textit{Variables}: Definitions of the quantities to be optimized. A variable may represent a single value, e.g., $x$ position, or it may represent more complex data, such as a timestamped two-dimensional pose. For example, the {\tt Orentation3DStamped} variable comprises a 3D orientation (represented as a quaternion) with an associated time stamp.

\textit{Constraints}: Connections for variables in the factor graph. Each constraint is responsible for generating some cost given the connected variables. These costs are then minimized during optimization. For example, the {\tt RelativePose2DStampedConstraint} produces a cost for two poses in the graph that is proportional to the difference between those poses and the measured difference as provided by a sensor measurement.

\textit{Sensor Models}: Models of variables with associated constraints generated from received sensor data. The variables and constraints are then passed to the optimizer. For example, the {\tt Imu2D} sensor model produces the relevant two-dimensional orientation and angular velocity data variables to add to the graph, as well as constraints on those variables as given by the measurement itself.

\textit{Motion Models}: Models of kinematic constraints between timestamped variables. These are generated on request and passed to the optimizer. For example, the {\tt Unicycle2D} kinematic model generates a constraint between two pose variables in the graph. If those poses are not feasible given a unicycle kinematic model, the constraint will generate non-zero cost.

\textit{Optimizers}: Build and optimize the graph. The core nodes in \textit{fuse} are effectively wrappers around optimizer plugins. For example, the primary state estimation node in \textit{fuse} is known as the \textit{fixed-lag smoother}. It wraps an optimizer of the same name.

\textit{Publishers}: Extract relevant quantities from the optimizer and publish the ROS~2 messages containing results. For example, the {\tt Pose2DPublisher} publishes the most recent optimized pose and covariance from the graph as both a { \tt geometry\_msgs/PoseStamped} ROS~2 message as well as a transform to be consumed by other nodes via \textit{tf}.

Each iteration, sensor model plugins will receive sensor data and create constraints based on them.
The optimizer will receive these constraints and request the motion model plugin to generate constraints to connect the sensor model constraints to the existing graph.
These new variables and constraints are optimized over via the optimizer to compute optimal values.
Finally, the publisher will return the results over ROS~2 topics to interested nodes.

\textit{fuse} has a number of advantages over \textit{r\_l}, both due to software design and affordances provided by factor graphs:

\begin{itemize}
  \item \textit{fuse} is more flexible and extensible. In addition to \textit{r\_l}'s support of an arbitrary number of sensors, \textit{fuse}'s plugin-based architecture allows configuration of the state vector, motion, and sensor models.
  \item Relative pose measurements are better supported in \textit{fuse}.
  \item Linearization errors are reduced in \textit{fuse} at every iteration in the optimization.
  \item \textit{fuse} can be used for applications outside sensor fusion, mapping, and calibration; one such application is robot control.
\end{itemize}

However, these features come at an increased compute cost. The experiments in section \ref{sec:comparison} suggest that \textit{fuse} could require as much as 4x the CPU of \textit{r\_l}. As \textit{r\_l} is fairly lightweight, this increase is generally acceptable for the vast majority of applications.

For those particularly resource-constrained applications, there are a number of parameters that may be tuned to improve compute time.
Reducing the length of the smoothing window of the fixed-lag smoother and the frequency of update can improve the performance. 
Further, \textit{fuse} uses Google Ceres' automatic differentiation in many of the plugins to compute Jacobians \cite{ceres}.
While this is convenient, it is not as efficient as analytically derived Jacobians, which Ceres also supports.

\subsection{Comparison}
\label{sec:comparison}

To compare the two state estimation systems in ROS~2, an experiment was designed and run on a 3.8 GHz AMD Ryzen 5800x CPU.
A mobile robot equipped with wheel encoders, an IMU, and a laser scanner was driven along a 541-meter long route through a commercial environment to collect a dataset, ending within 10 cm of the starting pose.
The odometry and IMU data was collected at 25 Hz.
\textit{r\_l}'s EKF node and \textit{fuse}'s fixed-lag smoother node each fused linear and angular velocities from the wheel encoders and angular velocities from the IMU.
ROS~2's \textit{amcl} package provides the ground-truth information (Sec. \ref{sec:amcl}). The fixed lag smoothing window in \textit{fuse} was set to 0.5 seconds and the covariance was extracted from the graph at 1 Hz.

The results can be seen in Fig. \ref{fig:se_comp} and Table \ref{tab:se_comp}.
At the end of the path, the \textit{fuse} estimate was 1.44 meters closer to the ground truth position than the \textit{r\_l} estimate, though both estimates were well below 5.4 meters, or 1\%, of the ground-truth pose.
The CPU usage for \textit{fuse} was approximately 3.7x that of \textit{r\_l}, with a slightly larger standard deviation.

\begin{table*}[ht]
 \caption{State Estimators Comparison}
 \label{tab:se_comp}
 \begin{center}
 \begin{tabular}{ |c|c|c|c|c|c| }
   \hline
   State Estimator & Update Frequency (Hz) & Total Distance (m) & Total Error (m) & Error (\% Distance Traveled) & CPU \% \\
   \hline
   \hline
   robot\_localization & 30 & 542.16 & 4.25 & 0.78 & 1.38 $\pm$ 1.17 \\
   \hline
   fuse & 20 & 542.46 & 2.81 & 0.52 & 5.19 $\pm$ 1.37 \\
   \hline
 \end{tabular}
 \end{center}
\end{table*}

% Figure environment removed

