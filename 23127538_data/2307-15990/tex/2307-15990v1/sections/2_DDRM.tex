 \DM{
A DDPM is a parameterized Markov chain trained to generate \ji{synthetic} images from noise relying on variational inference~\cite{ho_denoising_2020,nichol_improved_2021,dhariwal_diffusion_2021}. 
The Markov chain consists }\YZ{of two processes: a forward fixed diffusion process and a backward learned generation process. The forward diffusion process gradually adds Gaussian noise with variance $\sigma_t^2$ ($t = 1,\ldots, T$) to the clean signal $\xv_0$ until it becomes random noise, while in the backward generation process (see Fig.\ref{fig: sub_DDPM}), the random noise $\xv_T$ undergoes a gradual denoising process until a clean $\xv_0$ is generated}.
%
% Figure environment removed

%One attractive direction in recent years is
\DM{An interesting question in model-based deep learning is how}
\YZ{ to use prior knowledge learned by generative models to solve inverse problems.} 
Denoising Diffusion Restoration Models (DDRM)~\cite{kawar_denoising_2022} were recently introduced for solving linear inverse problems, taking advantage of a pre-trained DDPM model as the learned prior. 
%\YZ{In other words, DDRM applies the influence of the measurements to each step of denoising in DDPM as in Fig.\ref{fig: sub_DDRM}}. 
\DM{Similar to a DDPM, a DDRM is also a Markov Chain but conditioned on measurements $\yv_d$ through a linear observation model $\Hv_d$  
%linking the generated images to their observations
\footnote{We use subscript $d$ to refer to the original equations of the DDRM model.}. The linear model serves as a link between an unconditioned image generator and any restoration task. In this way, DDRM makes it possible to exploit pre-trained DDPM models \yz{whose weights are assumed to generalize over tasks}. 
\dm{In this sense, DDRM is fundamentally different from previous task-specific learning paradigms requiring training with paired datasets.}
%which is fundamentally different from the previous task-specific paradigms requiring training individuals with paired datasets.} 
Relying on this principle, the original DDRM paper was shown to work on several}
natural image restoration tasks such as denoising, inpainting, and colorization.

\DM{Different from DDPMs, the Markov chain in DDRM is defined in the spectral space of the degradation operator $\Hv_d$. To this end,}
DDRM leverages the Singular Value Decomposition (SVD): %of the observation operator, i.e., 
$\Hv_d=\Uv_d \Sb_d \Vv_d^\tD$ with $\Sb_d=\Diag\left(s_1,\ldots,s_N\right)$,
%$\svd(\Hv_d)$
%to decouple 
\DM{which allows decoupling} the dependencies between the measurements. 
\DM{The original observation model 
%From
%and to apply a denoising process to the noisy data in $\yv_d$. 
%\begin{align*}
$
 \yv_d= \Hv_d \xv_d + \nv_d= \Uv_d \Sb_d \Vv^\tD _d\xv_d + \nv_d,
$
%\end{align*}
%we get 
can thus be cast as a denoising problem that can be addressed \yz{on the transformed measurements:}}
\begin{equation*}
\overline{\yv}_d= \overline{\xv}_d + \overline{\nv}_d
\end{equation*}
with $\overline{\yv}_d= \Sb_d^\dag\Uv_d^\tD\yv_d$, $ \overline{\xv}_d=\Vv_d^\tD\xv_d$, and $\overline{\nv}_d=\Sb_d^\dag\Uv_d^\tD \nv_d$, where $\Sb_d^\dag$ is the generalized inverse of $\Sb_d$. %The subscript $_d$ indicates DDRM. 
The additive noise $\nv_d$ being assumed \textit{i.i.d.} Gaussian: $\nv_d\sim \mathcal{N}\left(0, \sigma_d^2\Iv_N\right)$, with a known variance $\sigma_d^2$ and $\Iv_N$ the $N\times N$ identity matrix, we then have
$\overline{\nv}_d$ \yz{with} standard deviation $\sigma_d\Sb_d^\dag$.
%\left[\begin{array}{ccccc}
%\frac{\sigma_d^2}{s_1^2} & & &\\
%& .. & & & \\
%& & \frac{\sigma_d^2}{s_i^2} &\\
%& & & .. &\\
%& & & & \frac{\sigma_d^2}{s_N^2}\\
%\end{array}\right]
%, and the denoising process is realized as follows:

Each denoising step from $\overline{\xv}_t$ to $\overline{\xv}_{t-1}$ ($t=T,...,1$) is a linear combination of $\overline{\xv}_t$, the transformed measurements $\overline{\yv}_d$, the transformed prediction of 
$\xv_0$ at the current step $\overline{\xv}_{\theta,t}$, and random noise. To determine their coefficients which are denoted as $A$, $B$, $C$, and $D$ respectively, the condition on the noise, $(A\sigma_t)^2 + (B\sigma_d/s_i)^2 + D^2 = {\sigma_{t-1}}^2$, and on the signal, $A+B+C = 1$, are leveraged, and the two degrees of freedom are taken care of by two hyperparameters.% $\eta$ and $\eta_b$.  
\begin{comment}
In practice, a denoising step is computed by sampling from a probability distribution $p_\theta$ parameterized by a step-dependent DNN with parameters $\theta$. When $t=T$,
\begin{equation*}
p_\theta^{(T)}\left(\overline{\xv}_T^{(i)} \mid \yv_d\right)= \begin{cases}\mathcal{N}\left(\overline{\yv}_d^{(i)}, \sigma_T^2-\frac{\sigma_d^2}{s_i^2}\right) & \text{if } s_i>0 \\ \mathcal{N}\left(0, \sigma_T^2\right) & \text{if } s_i=0,\end{cases}
\end{equation*}
when $t<T$, 
\begin{equation*}
    p_\theta^{(t)}\left(\overline{\xv}_t^{(i)} \mid \xv_{t+1}, \yv_d\right)= \begin{cases}\mathcal{N}\left(\overline{\xv}_{\theta, t}^{(i)}+\sqrt{1-\eta^2} \sigma_t \frac{\overline{\xv}_{t+1}^{(i)}-\overline{\xv}_{\theta, t}^{(i)}}{\sigma_{t+1}}, \eta^2 \sigma_t^2\right) & \text{if } s_i=0 \\ \mathcal{N}\left(\overline{\xv}_{\theta, t}^{(i)}+\sqrt{1-\eta^2} \sigma_t \frac{\overline{\yv}_d^{(i)}-\overline{\xv}_{\theta, t}^{(i)}}{\sigma_{{d}} / s_i}, \eta^2 \sigma_t^2\right) & \text{if } \sigma_t<\frac{\sigma_{{d}}}{s_i} \\ \mathcal{N}\left(
    (1-\eta_b)\overline{\xv}_{\theta,t}^{(i)} + \eta_b\overline{\yv}_d^{(i)}, \sigma_t^2-\frac{\sigma_{{d}}^2}{s_i^2}\eta_b^2 \right) & \text{if } \sigma_t \geqslant  \frac{\sigma_{{d}}}{s_i},\end{cases}
\end{equation*}
where the notation $i$ indicates the element index over the singular values of $\Hv_d$.
\end{comment}

\yz{In this way, the iterative restoration is achieved by the iterative denoising, and the final restored image is $\xv_0 = \Vv_d\overline{\xv}_{0}$. %, where $\overline{\xv}_{0}$ is sampled from $p_\theta^{(0)}\left(\overline{\xv}_0^{(i)} \mid \xv_{1}, \yv_d\right)$. 
For speeding up this process, skip-sampling \ji{\cite{DDIM}} is applied in practice. We denote the number of iterations as \texttt{it}.}
\begin{comment}
The first denoising step (when $t=T$) is realized by sampling from $p_\theta^{(T)}\big(\overline{\xv}_T^{(i)} \mid \yv_d\big)$, with two cases:
%(i) $s_i>0$: since it is reasonable to assume that $\sigma_T$ is larger than any $\sigma_d / s_i$ as $\xv_T$ is a random noise, only the contribution of measurement $\overline{\yv}_d^{(i)}$ is considered; (ii) $s_i=0$: it samples from a random field.}

\begin{equation*}
p_\theta^{(T)}\left(\overline{\xv}_T^{(i)} \mid \yv_d\right)= \begin{cases}\mathcal{N}\left(\overline{\yv}_d^{(i)}, \sigma_T^2-\frac{\sigma_d^2}{s_i^2}\right) & \text{if } s_i>0 \\ \mathcal{N}\left(0, \sigma_T^2\right) & \text{if } s_i=0\end{cases}
\end{equation*}
%
\YZ{The denoising process in the next steps (when $t<T$) is achieved by sampling from $p_\theta^{(t)}\left(\overline{\xv}_t^{(i)} \mid \xv_{t+1}, \yv_d\right)$:
%, where there are three possible cases: $\sigma_d / s_i = +\infty,$ $\sigma_d / s_i > \sigma_t,$ and $\sigma_d / s_i \leqslant \sigma_t$. The synthetic output from the learned prior, $\overline{\xv}_{\theta, t}=\Vv_d^{t} \xv_{\theta, t}$, is used for the first two cases, the measurement information $\overline{\yv}_d^{(i)}$ is utilized for the last two cases, and the previous step's results, $\overline{\xv}_{t+1}^{(i)},$ are used when $\sigma_d / s_i = +\infty.$
% \begin{itemize}
%     \item if $\sigma_d / s_i = +\infty$, we assign weights to both the synthetic of the learned prior $\overline{\xv}_{\theta, t}^{(i)}$ and the results from the previous step $\overline{\xv}_{t+1}^{(i)}$;
%     \item if $\sigma_d / s_i > \sigma_t$, we assign weights to both the synthetic of the learned prior and the measurements;
%     \item if $\sigma_d / s_i \leqslant \sigma_t$, we totally believe the measurements.
% \end{itemize}
}
%
\begin{equation*}
    p_\theta^{(t)}\left(\overline{\xv}_t^{(i)} \mid \xv_{t+1}, \yv_d\right)= \begin{cases}\mathcal{N}\left(\overline{\xv}_{\theta, t}^{(i)}+\sqrt{1-\eta^2} \sigma_t \frac{\overline{\xv}_{t+1}^{(i)}-\overline{\xv}_{\theta, t}^{(i)}}{\sigma_{t+1}}, \eta^2 \sigma_t^2\right) & \text{if } s_i=0 \\ \mathcal{N}\left(\overline{\xv}_{\theta, t}^{(i)}+\sqrt{1-\eta^2} \sigma_t \frac{\overline{\yv}_d^{(i)}-\overline{\xv}_{\theta, t}^{(i)}}{\sigma_{{d}} / s_i}, \eta^2 \sigma_t^2\right) & \text{if } \sigma_t<\frac{\sigma_{{d}}}{s_i} \\ \mathcal{N}\left(\overline{\yv}_d^{(i)}, \sigma_t^2-\frac{\sigma_{{d}}^2}{s_i^2} \right) & \text{if } \sigma_t \geqslant  \frac{\sigma_{{d}}}{s_i},\end{cases}
\end{equation*}
where $\theta$ denotes the trainable parameters and ${\xv}_{\theta, t}$ represents the prediction of $\xv_0$ at step $t$ by a model, the notation $i$ indicates the element index, and $\eta$ is a hyperparameter.

In this way, the Markov chain $\mathbf{x}_T \rightarrow \mathbf{x}_{T-1} \rightarrow \ldots \rightarrow \mathbf{x}_1 \rightarrow \mathbf{x}_0$ is dependent on the inverse problem, and the final restored image is $\xv_0 = \Vv_d\overline{\xv}_{0}$, where $\overline{\xv}_{0}$ is sampled from $p_\theta^{(0)}\left(\overline{\xv}_0^{(i)} \mid \xv_{1}, \yv_d\right)$. \YZ{For speeding up this process, skip-sampling is always applied in practice. We denote the number of iterations as \texttt{it}.}
\end{comment}