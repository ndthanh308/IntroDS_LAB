\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage[colorlinks,linkcolor=blue]{hyperref}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}
\usepackage{cite}
\begin{document}
\title{Data-free Black-box Attack based on Diffusion Model}
\author{Mingwen~Shao,~\IEEEmembership{Member,~IEEE,} 
	Lingzhuang~Meng, 
	Yuanjian~Qiao, 
    Lixu~Zhang,
    and~Wangmeng~Zuo,~\IEEEmembership{Senior Member,~IEEE} 
%	and~Zhaofei~Xu
\thanks{Ming-Wen Shao, Ling-Zhuang Meng, Yuan-Jian Qiao and Li-Xu Zhang are with the School of Computer Science and Technology, China University of Petroleum (East China), Qingdao, 266580, China (e-mail: smw278@126.com; lzhmeng1688@163.com; yjqiao@s.upc.edu.cn; z2216842477@163.com;)}
\thanks{Wang-meng Zuo is with the Harbin Institute of Technology, China (e-mail: wmzuo@hit.edu.cn;}
\thanks{This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.}
}
\markboth{IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL. *, NO. *, JUNE 2023}%
{Data-free Black-box Attacks based on Diffusion Model}
\maketitle

\begin{abstract}
Since the training data for the target model in a data-free black-box attack is not available, most recent schemes utilize GANs to generate data for training substitute model. However, these GANs-based schemes suffer from low training efficiency as the generator needs to be retrained for each target model during the substitute training process, as well as low generation quality.
To overcome these limitations, we consider utilizing the diffusion model to generate data, and propose a data-free black-box attack scheme based on diffusion model to improve the efficiency and accuracy of substitute training.
Despite the data generated by the diffusion model exhibits high quality, it presents diverse domain distributions and contains many samples that do not meet the discriminative criteria of the target model.
To further facilitate the diffusion model to generate data suitable for the target model, we propose a Latent Code Augmentation (LCA) method to guide the diffusion model in generating data.
With the guidance of LCA, the data generated by the diffusion model not only meets the discriminative criteria of the target model but also exhibits high diversity. 
By utilizing this data, it is possible to train substitute model that closely resemble the target model more efficiently.
Extensive experiments demonstrate that our LCA achieves higher attack success rates and requires fewer query budgets compared to GANs-based schemes for different target models. 
\end{abstract}

%Notably, the proposed LCA approach guides the diffusion model to generate images with both high diversity and preserved essential features of the member data.
 %we propose a latent code augmentation (LCA) approach used to guide the diffusion model to generate data more suitable for the target model.
%First, 

\begin{IEEEkeywords}
Black-box Attack, Data-free Attack, Diffusion Model, Substitute Training, Latent Code Augmentation.
\end{IEEEkeywords}


\section{Introduction}
\IEEEPARstart{C}{onvolutional} Neural Networks (CNNs) currently exhibit remarkable performance and play a pivotal role in a wide range of applications in various fields. However, recent studies have shown that CNNs are vulnerable to adversarial perturbations, which may lead to incorrect decisions and pose risks in critical applications~\cite{10100731,MIM,UAP}. For example, a little imperceptible adversarial perturbations can cause autonomous vehicles to make wrong decisions, leading to severe consequences. Such adversarial perturbation reveal the vulnerability of CNNs. Therefore, there is currently an increasing focus on adversarial perturbations, adversarial attacks and their defence~\cite{9121297,AdverDefen}.

% Figure environment removed
Existing adversarial attack schemes can be categorized into white-box attacks and black-box attacks according to the availability of network information. The white-box attacks assume complete knowledge of the network structure and parameters, allowing for effective generation of adversarial samples using gradient-based methods, such as FGSM~\cite{FGSM}, BIM~\cite{BIM}, PGD~\cite{PGD}, etc. However, in realistic scenarios where networks are deployed on servers, there is no access to information about the network, making black-box attacks more practical~\cite{yue2021black,EBA,TSEA,MAZE,DaST,DST,HardLabel,ColorQuery,queries,9727149}.
The black-box attacks aim to achieve attacks on target networks using only the output (label~\cite{HardLabel} or probability~\cite{MAZE}) of the target networks, and researchers have proposed various solutions for this purpose. For example, transfer-based attacks~\cite{EBA,TSEA} exploit the transferability of adversarial samples, enabling adversarial samples generated on other models to be applied to black-box target networks. Another example is the substitute-based attack~\cite{MAZE,DaST,DST}, which requires training a substitute network similar to the target network and then using the adversarial samples generated by the substitute network to attack the black-box target network. In addition, query-based attacks~\cite{9954062,ColorQuery,queries} are also viable options for black-box attacks, which adds small noise to the image each time and keeps querying the target model output until the target result is obtained. 
In this paper, we focus on the substitute-based approach which often achieved better attack success rates than the other schemes. The essential reason is that the substitute model is very similar to the target model through effective training, making the attack on the substitute network very close to a direct attack on the white-box target model. Therefore, how to train a substitute model that is close to the target model is a key issue for the substitute-based approach.

The substitute-based schemes utilize knowledge distillation methods to make the output of the substitute model fit the output of the target model~\cite{MAZE,DaST}.
In this substitute training, if the original training data of the target model is available, it is possible to train a substitute network that closely resembles the target network. %By leveraging the substitute model, adversarial samples with higher attack success rates can be generated against the target model.
This is because deep learning models are primarily influenced by data, as indicated by their data-driven nature~\cite{TransBB}.
However, in realistic scenario, we do not have access to the original training data of the target model.
In light of this, most existing solutions use data generated by Generative Adversarial Networks (GANs) to train substitute models~\cite{MAZE,DaST,DST}. 
%However, a significant drawback of GANs-based approach is that the generator needs to be retrained for the target network during training, resulting in a less efficient substitute training. 
%However, the significant drawbacks of GANs-based approaches are the need to retrain the generator for the target network during the training process and the low quality of the generated images, which leads to less efficient substitute training.
However, these GAN-based methods suffer from low training efficiency, as the generator needs to be retrained for each target model and the quality of the generated images is relatively low.
In recent years, diffusion models~\cite{DDPM,StableDiffusion} have demonstrated superior generation quality as well as diverse generation results compared to GANs. In this paper, we consdier to \textit{utilize diffusion models to generate richer data for training substitute models.} 

Diffusion models~\cite{DDPM,StableDiffusion} have shown remarkable performance in image generation tasks, and it can directly generate images of various categories without retraining as in the case of GANs.
Nevertheless, when directly using the diffusion model for substitute training, we observed that \textbf{the generated data exhibits various domain distributions and contains many samples that do not conform to the discriminative criteria of the target model}, as shown in Fig.~\ref{fig_1} (a). 
The images generated using the diffusion model prompted by class labels cover a variety of types, including drawings, landscapes, close-ups, and text.
In addition, the data distribution reflects \textbf{significant discrepancies in the number of positive and negative samples among different classes,} as depicted in Fig.~\ref{fig_1} (b). The accuracy of some classes is less than 10\% when evaluated in the target network (ResNet50), which are generated for 100 classes according to CIFAR100~\cite{CIFAR}.
%Using such imbalanced data for substitute training can result in a notable performance gap between the substitute model and the target model, especially for classes with particularly small positive samples.

Therefore, to further facilitate diffusion models to generate data suitable for the target model, we propose a novel Latent Code Augmentation (LCA) method to guide diffusion models in generating data.
Specifically, we introduce Membership Inference (MI) to identify the images that are most likely to belong to the training data of the target model, known as member data. 
Then, we apply LCA to augment the latent codes of the member data, which serve as guidance for the diffusion model. 
Guided by augmented latent code, the diffusion model is able to generate images that are consistent with the features of the member data.
Subsequently, the generated images are used to train a substitute model that simulates the target network.
Finally, we employ the substitute model to generate adversarial samples for attacking the target model.

The main contributions of this paper are as follows:
\begin{itemize}	
	\item To the best of our knowledge, we are the first to use diffusion model to improve the performance of data-free black-box attacks, making substitute training more efficient and achieving higher attack success rates.
	\item We propose LCA to further facilitate the diffusion model to generate images that are more suitable for the target network.
	\item The experimental results demonstrate the superiority of our LCA in terms of attack success rates and query efficiency across different target networks trained on various training sets.
\end{itemize}

\section{Related Work}
\label{sec:Rela}  
\subsection{Adversarial Attacks} 
Existing adversarial attacks schemes are divided into white-box and black-box attacks according to whether the structure and parameters of the target network are available. For white-box attacks, the network gradient or data distribution is mainly utilized to generate adversarial samples and usually achieve a high success rate of attacks. Classical examples include FGSM~\cite{FGSM}, BIM~\cite{BIM}, PGD~\cite{PGD}, which generate adversarial samples by increasing the gradient of the real class from a modeling perspective. In addition, Zhu et al.~\cite{DistrWhite} reconceptualised the adversarial sample in terms of data distribution, by manipulating the distribution of the images to induce misclassifications.
For black-box attacks, the limited output information (labels~\cite{HardLabel} or probabilities~\cite{MAZE}) of the target model is required to generate adversarial samples~\cite{chen2017zoo,hu2022substitute,zhou2022adversarial}. One solution to black-box attacks is transfer-based attacks~\cite{EBA,TSEA}, which enhance the transferability of adversarial samples generated using white-box attacks on known networks to attack a black-box target model. Another solution is substitute-based attacks~\cite{MAZE,DDG,DaST,DST,cui2020substitute,park2020partial}, which focus on training a substitute model using the output of the target model. Then the adversarial samples generated using white-box attacks on the substitute model can then be used to attack the target network.
There are also other solutiones, such as query-based attacks~\cite{9954062,ColorQuery,queries,zhu2022defense}, which add small perturbations to the image at a time and then query the output of the target network until the target network makes an incorrect judgement. %Among black-box attack schemes, substitute based attacks have garnered significant attention. It can achieve good performance in targeted and non-targeted attacks when the substitute model closely approximates the target model.

\subsection{Data-free Black-box Attacks} 
In real-world scenarios, it is usually difficult to obtain the network structure, parameters, and training data of CNNs deployed on servers, which makes data-free black-box attacks more difficult to implement~\cite{yue2021black}. Most recent works~\cite{MAZE,DDG,DaST,DST} have explored generating samples with Generative Adversarial Networks (GANs) and then using these samples to train a substitute network that simulates the target network.

Among them, in order to generate samples with a more uniform distribution of classes, Zhou et al.~\cite{DaST} designed label-controlled generators for each class so that the generated samples cover all classes of the attacked model more evenly. In contrast, Wang et al.~\cite{DDG} embedded labels in the layers of the generator so that the generator learns relatively independent data distributions for each class to synthesize diversity images.
To better adapt to different target models, Wang et al.~\cite{DST} implemented a dynamic substitute network using gate structures and used graph structures to optimize the distillation process.
To better optimize the generator, Kariyappa et al.~\cite{MAZE} used a zero-gradient estimation technique to estimate the gradient of the target model so that the generator can be optimized indirectly in the black-box setting. In addition, Zhang et al.~\cite{TED} changed the game between the generator and subsitute model by optimizing the generator and the subsitute model independently instead of jointly. Despite the approach resulted in a more stable convergence of the subsitute models to the target model, it makes an inappropriate assumption that the structure of the target model network is known, which is not feasible in a black-box attack. 

However, the above recent schemes predominantly relied on GANs as the primary structure to generate data for training substitute model. Where the generators are not pre-trained, it needs to be retrained during the substitute training process and the quality of the generated images is poor, which leads to inefficient substitute training. To improve substitute training efficiency, this paper leverages a pre-trained diffusion model to generate data, which has the advantage of generating diverse and high-quality data from different domains without retraining.

\subsection{Diffusion Model} 
Diffusion models~\cite{DDPM,StableDiffusion} are trained on large-scale datasets and has gained attention for its ability to produce realistic, natural and diverse images. It is divided into two steps: forward diffusion and reverse denoising. In the diffusion process, the clean image $x$ is added with $T$ steps of noise $\epsilon$ to obtain a noisy image $x_{t}$. And in denoising process, a denoising encoder $\epsilon^{_{\theta }}$ in the form of a U-Net is used to estimate the noise added at step $t$. During the sampling process, the diffusion model can infer the noise of a given noisy image using a denoising encoder $\epsilon^{_{\theta }}$, which eventually generates a clean, natural image through $T$-step denoising.

The objective function of the network at step $t$ can be expressed as:
\begin{equation}
	L_{DM}=\mathbb{E}_{x,\epsilon\sim \mathcal{N}(0,1),t}\left [ \left \| \epsilon-\epsilon^{_{\theta }}\left ( x_{t},t \right ) \right \|_{2}^{2} \right ].
\end{equation}

To improve the efficiency of the diffusion model, Rombach et al.~\cite{StableDiffusion} extended the diffusion model from the image space to the latent space $z$, called Stable Diffusion. It can generate images with specific requirements conditioned on either text or images, where text is encoded by a pre-trained CLIP text encoder and images are encoded by a pre-trained `AutoEncoder' image encoder. The encoding of the condition $\tau _{\theta}(y)$ is integrated into the U-Net architecture through a cross-attention. The objective function of stable diffusion can be expressed as:
\begin{equation}
	L_{LDM}={\mathbb{E}}_{z,y,\epsilon\sim \mathcal{N}(0,1),t}\left [ \left \| \epsilon-\epsilon^{_{\theta }}\left ( z_{t},t,\tau _{\theta}(y) \right ) \right \|_{2}^{2} \right ].
\end{equation}

\subsection{Membership Inference} 
Membership Inference (MI) is an attack scheme in the privacy leakage category that determines whether a given sample belongs to the training data set of the model, i.e. the member data~\cite{LOGAN,GANLeaks}. MI exploits the observation that CNNs are constantly fitting the training data during training and respond differently to the training and non-training data. Therefore, using the overfitting property of the model, MI can infer membership using the output of the model. 

In the MI scheme for attacking image classification, Salem et al.~\cite{MLLeaks} utilized the prediction confidence of the model as a credential to infer membership. They argue that higher prediction confidence indicate a higher possibility of belonging to the training set. However, this approach relies on the confidence output of the model and is not suitable for black-box models that only provide label outputs.
To overcome this limitation, Choquette-Choo et al.~\cite{LabelonlyMI} presented a label-only MI scheme that inferred sample membership based on whether the model made the correct decision when faced with a slight perturbation (e.g., adversarial perturbations). The theoretical basis of the scheme lies in the fact that the CNNs continuously fits the training data during the training phase, so that the training data are contained within the decision boundary as much as possible, while the non-training data are more likely to be close to the boundary. That is, the training data has higher robustness than the non-training data, and therefore data with higher robustness is more likely to belong to the member data.

In this paper, we introduce MI to identify data that is most likely to belong to the training data of the target model, and use this member data as a reference for the diffusion model to generate data more suitable for the target model.

% Figure environment removed
\section{Methodology}     
In this section, we propose a data-free black-box attacks scheme based on diffusion model, the framework is shown in Fig.~\ref{fig_2}. We divide the process into two stages: 1) identifying member data suitable for the target model and updating the codebook, and 2) guiding diffusion model to generate data and training substitute model. 

In the Stage 1, we employ MI to identify samples that are most likely to belong to the member data of target model (Sec.~\ref{IMD}), and these samples are encoded and stored into the codebook. In the Stage 2, we augment the latent codes (Sec.~\ref{LCA}) to guide the diffusion model to generate data. Guided by latent codes conditions, the data generated by the diffusion model retains the features of the member data at the feature level and exhibits good diversity. Finally, the generated data are used to train the substitute model (Sec.~\ref{ST}). In our scheme, we use a pre-trained Stable Diffusion~\cite{StableDiffusion} that is the same in both stages differing only in the content of the guidance, the Stage 1 using class labels as guidance and the Stage 2 using image as guidance. The algorithm is described in Algorithm~\ref{alg:alg1}.
\renewcommand{\algorithmicrequire}{\textbf{Inputs:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{algorithm}[t]
	\caption{Training steps of the proposed scheme.}\label{alg:alg1}
	\begin{algorithmic}[1]
		\REQUIRE $T$: the target model; $DMs$: the diffusion model; $maxM$: Max lenght of Codebook; $K$: Max Epoch.
		\ENSURE $S$: the substitute model.
		\STATE Initializate: $K$=100;
		\STATE // Stage 1
		\FOR{$M$ $<$ $maxM$}
		\STATE Generates data using $DMs$ with class labels;
		\STATE Infer member data according to Eq. (4);
		\STATE Encode the member data and update it into Codebook;
		\ENDFOR
		\STATE // Stage 2
		\FOR{$k$ =1 to $K$}
		\STATE Extract latent codes from Codebook;
		\STATE Augmente the latent codes;
		\STATE Generate data using $DMs$ guided by augmented latent codes;
		\STATE Feed the generated data into $T$ and $S$;
		\STATE Optimize $S$ based on loss $\mathcal{L}_{sub}$;
		\ENDFOR
		\STATE \textbf{return}  $S$
	\end{algorithmic}
\end{algorithm}

\subsection{Inferring Member Data}\label{IMD}
In order to obtain data that is suitable for the target model, we introduce membership inference to infer the member data from the data generated by the diffusion model in Stage 1. As observed by Choquette-Choo et al.~\cite{LabelonlyMI}, the network exhibited different responses to small perturbations added to the data. On this basis, we directly add a small amount of noise to the samples, and distinguish between member and non-member data through the network's fluctuation or misclassification for the noisy images. The difference of our method is that we introduce decision distance to make better judgments. 

The Gaussian noise is added to the clean image $x$ to obtain the noisy image $\hat{x}$. And the decision distance between clean and noisy samples by the network is used as the inferred membership distance $dist_T$:
\begin{equation}
	\hat{x}_{i}=x+\mathcal{N}(0,\sigma ^{2}),
\end{equation}
\begin{equation}
	dist_T (x)=d(T(x)-T(\hat{x})),
\end{equation}
where, $\mathcal{N}(0,\sigma ^{2})$ denotes Gaussian noise with a mean of 0 and a variance of $\sigma ^{2}$, and $d$ is the two-sample decision distance. Specifically, in the case of models with probability outputs, we employ Mean Squared Error (MSE) to measure the distance between the two sample outputs, and samples with small distances are more likely to belong to the training data. And in the case of models with label-only outputs, we classify samples where the output remains unchanged after adding noise as member data, while samples where the output changes are classified as non-member data.
%One of the benefits of using the robustness of the network to infer member data is that the inferred data is highly robust and able to avoid data bias during latent code augmentation.

\subsection{Latent Code Augmentation}\label{LCA}
%To enhance the diversity of the generated images while maintaining the distinctive features of the member data, we introduce the latent code augmentation (LCA) approach.
To further facilitate the diffusion model to generate images suitable for the target model, we propose a novel LCA for guiding the diffusion model to generate images.
In this method, we use the inferred few member data as a reference and leverage the image-to-image generation method in the Stable Diffusion to generate images. Stable Diffusion encodes the reference image to the latent space, and then generates the image using the latent code as a guide, where the encoder is `AutoEncoderKL'.
To increase the diversity of the images generated by Stable Diffusion, we propose the LCA to augment the latent codes of member data, which then serve as guidance for Stable Diffusion. By applying LCA, the images generated by Stable Diffusion have good diversity while maintaining the characteristics of the member data.
%By augmenting latent code, we introduce diversity into latent encoding. These augmented codes are then utilized to guide the diffusion model in the generation of diverse and high-quality images. 
%The LCA approach aims to explore ways to enable diffusion models in the latent space to produce more diverse outputs while retaining the essential features of the member data.

% Figure environment removed
Similar to traditional data augmentation, we recommend using operations such as translation, rotation, cropping, affine and adding noise to augment the latent code:
\begin{equation}
	z_{ag}=\phi(z),
\end{equation}
where $z$ denotes latent code in latent space, and $\phi$ denotes an augmentation operation.
The difference is that we operate on latent code, instead of images, and the LCA process is shown in Fig.~\ref{fig_2.5}. Then, the augmented latent code is used to guide the diffusion model to generate new images. 
In addition, to further expand the diversity of generated images, latent codes $z_i$ and $z_j$ can be fused according to different scales using multivariate operations $\psi$, such as MixUp and CutMix:
\begin{equation}
	z_{ag}=\psi(z_i,z_j).
\end{equation}

This allows for the creation of diverse combinations of latent codes, resulting in a broader range of visually diverse outputs.

Furthermore, by randomly selecting and applying various augmentations from set $\{\phi_1,\dots,\phi_k,\psi_1,\dots,\psi_l\}$, the generated latent codes exhibit higher diversity.  
Guided by these latent codes, the diffusion model generates images with enhanced diversity while preserving the significant features of the member data. 

\textbf{But is the augmented latent code still a meaningful and valid latent code?}

As we know, the convolution layer maps images to feature vectors, and the translational weight tying in CNNs gives it translation equivalence~\cite{equiv4,equiv}. That is, a translation $\phi$ of the image $I$ leads to a corresponding translation $\pi$ of the feature map $f(I)$:
\begin{equation}
	f(\phi(I))=\pi(f(I)).
\end{equation}

In general, $\pi \neq \phi$, due to the presence of pooling layers.
However, similar to the generator in VAE, `AutoEncoderKL' does not have a pooling layer and therefore has equivalents such as rotation equivalence and translation equivalence~\cite{equiv1}. In the case of ignoring edge effects, the feature vector of a transformed image is equivalent to the transformed feature vector of an image by a specific scale, i.e. $\pi=\phi$.

This implies that augmenting the latent code is equivalent to encoding the augmented image. As a result, the augmented latent codes remain meaningful and valid latent code of image. These augmented codes can be used as a guide to diffusion models to generate diverse and visually appealing images while preserving the distinctive attributes of the member data.

% Figure environment removed
It is worth noting that our proposed LCA is free from problems such as image white space and unnatural transitions at stitching, which are common in traditional image augmentation. 
This is because the diffusion model considers the blank areas in the latent code as missing features. And leveraging the excellent inpainting ability of the diffusion model, regions corresponding to missing or distorted features are repaired, resulting in highly natural images.
In contrast, if an image is fed into the diffusion model after conventional image augmentation, information such as white space will be encoded into the latent code. And these white space will be considered as part of the image information by the diffusion model, resulting in a generated image containing white space, as shown in Fig.~\ref{fig_3}.
By applying LCA at the feature level, we avoid this issue and ensure that the generated images do not exhibit unnatural artifacts or artificial traces.
%By performing LCA at the feature level, the diffusion model is able to produce images with enhanced diversity while retaining the distinctive characteristics of the member data. 

% Figure environment removed
\subsection{Substitute Training}\label{ST}
The data generated by the diffusion model in Stage 2 is used to train the substitute model. In order to adapt the substitute model to different target models, in this paper, we design a substitute model with ResNet34 structure and introduce learnable parameters, and the structure of the ResBlock with learnable parameters is shown in Fig.~\ref{fig_3_5}. To make the output of the substitute model similar to that of the target model, we exploit both the cross-entropy loss and the $MSE$ loss between the two networks:
\begin{equation}
	\mathcal{L}_{sub}=\lambda_{1}CE+\lambda_{2}MSE,
\end{equation}
where $\lambda_{1}$ and $\lambda_{2}$ are hyperparameters. For the case where the target model outputs label-only, we set $\lambda_{2} = 0$. By minimizing the loss, the substitute model is optimized to be more similar to the target model.

\section{Experiments}
\label{experiment}
In this section we provide the implementation details of the experiment (Sec.~\ref{ED}), the resulting image generated by latent code augmentation (Sec.~\ref{LCAR}), the performance in different cases of black-box attacks (Sec.~\ref{BAR}), and the ablation experiments with different components and settings (Sec.~\ref{AS}). 

\subsection{Experiment Details}\label{ED}
\textbf{Datasets and models.} We evaluated the performance of our LCA on different target networks, include VGG (16, 19)~\cite{VGG}, ResNet (18, 34, 50)~\cite{ResNet}. The training datasets for the target models include CIFAR10~\cite{CIFAR} (10 classes, image size 32$\times$32), CIFAR100~\cite{CIFAR} (100 classes, image size 32$\times$32), STL10~\cite{STL10} (10 classes, image size 96$\times$96) and Tiny-ImageNet~\cite{Tinyimagenet} (200 classes, image size 64$\times$64). 

\textbf{Training setup.} All training and testing is performed on GPU: NVIDIA GeForce RTX 2080 Ti. The substitute network is optimized by $Adam$ with an initial learning rate of 0.001 and batch size set to 32.
%The substitute training on the CIFAR10 and STL10 datasets consumes 300k queries, on the CIFAR100 dataset consumes 500k queries, and on the Tiny-ImageNet datasets consumes 3M queries.

\subsection{Latent Code Augmentation Results}\label{LCAR}
\textbf{Visual effects of latent code augmentation.} We present the results of our LCA, as shown in Fig.~\ref{fig_4}. %LCA utilizes a series of manipulations to augment the latent code, which in turn produces diverse images using a diffusion model. 
It can be seen that the images obtained by operations such as translation, padding, and affine transformation do not exhibit white edges, which indicates that the image effect obtained by our LCA is smooth and seamless.
In addition, the images generated by adding noise to the latent codes are altered in texture while maintaining the natural appearance of the foreground and background. This provides an effective means of introducing variation into the image while preserving the overall integrity of the image.
%Moreover, the images generated by erasing and CutMix operations show natural and visually coherent fillings.

Moreover, it can be observed that the effect of augmentation obtained by a single transformation applied to a single image is limited. This ensures that the essential characteristics of the original image are retained throughout the augmentation process.
In contrast, when fusing two different scales of latent codes, the resulting images exhibit enhanced diversity. For example, the images obtained using MixUp and CutMix fuse latent codes of different scales to produce images with richer variation.
% Figure environment removed

% Figure environment removed
Furthermore, by applying random augmentation on a larger number of latent codes, the images generated by diffusion model exhibit higher diversity, as depicted in Fig.~\ref{fig_5}. %These generated images successfully preserve the features of the member images while providing better diversity.
It is important to emphasize that our proposed LCA outperforms traditional data augmentation in terms of naturalness and diversity. This is due to the fact that LCA exploits a wider range of combinations in the latent code and generates images with significantly greater diversity. Overall, our LCA provides a more efficient approach for task-specific image generation.

\textbf{Accuracy of latent code augmentation.} 
We present the accuracy bar graphs for each data class obtained through LCA in Fig.~\ref{fig_1}. Which is the accuracy results obtained from images generated according to CIFAR100 and classified using the ResNet50 target network. The comparison is the data generated by diffusion model using class labels as prompt, which have relatively low average accuracy and uneven distribution of positive and negative samples. In contrast, our LCA demonstrates a higher average accuracy and a more uniform distribution of positive samples across classes. These indicate that the data generated by LCA are of better quality and a more balanced number of positive samples for the target network.

\subsection{Black-box Attack Results}\label{BAR}
\textbf{ASR on different target models.}
We report the Attack Success Rates (ASR) against various target models on different datasets in Table~\ref{table1}. The experiments include both target and non-target attack scenarios. It can be seen that our approach achieves higher ASR than other data-free baseline methods~\cite{DaST,DST} on the all datasets, while having lower or comparable $L_2$ distances. In addition, the performance of our LCA remains competitive compared to the partial data scheme MAZE-PD~\cite{MAZE}. Notably, our proposed LCA achieves a significant improvement in ASR for datasets with more classes such as CIFAR100, even at smaller $L_2$ distances. This enhancement can be attributed to the fact that the distribution of the data generated by our method is very similar to that of the original training data, resulting in a more accurate substitute model and thus superior performance.
\begin{table*}[!t]
	\centering
	\caption{Comparison of the attack success rates of our method and competitors on different target models. For a fair comparison, we use PGD as the default attack method. The numbers in `()' indicate the average $L_2$ perturbation distance, with the best result being \textbf{bolded}. \label{table1}}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{c|c|ccc|cc|cc|c}
			\hline
			& Dataset       & \multicolumn{3}{c|}{CIFAR10} & \multicolumn{2}{c|}{CIFAR100} & \multicolumn{2}{c|}{STL10}   & \multicolumn{1}{c}{Tiny-ImageNet}    \\ \cline{2-10} 
			& Target Model                        & VGG16                & ResNet18             & ResNet34             & VGG19                & ResNet50             & ResNet34                     &   ResNet50                    &   ResNet50   \\ \hline
			\multirow{4}{*}{Non-Target} & DaST~\cite{DaST}    & 48.65(1.28)          & 49.40(1.08)          & 51.80(1.44)          & 30.24(4.74)          & 26.41(4.20)          & 54.41(3.55)          & 60.50(2.88)          & 27.08(4.70) \\
			& DST~\cite{DST}      & 52.21(1.27)          & 54.93(1.10)          & 62.80(1.44)          & 34.00(4.70)          & 31.63(4.57)          & 66.00(3.60)          & 69.65(2.90)           & 32.51(4.69)\\
			& MAZE-PD~\cite{MAZE} & 64.39(1.20)          & 78.99(1.11)          & 66.80(1.34)          & 40.75(4.55)          & 44.44(4.32)          & 69.86(3.35)          & 73.73(2.88)           & 45.46(4.71) \\
			& LCA (Ours)                           & \textbf{82.54(1.16)} & \textbf{90.29(0.91)} & \textbf{89.64(1.13)} & \textbf{95.15(4.18)} & \textbf{97.82(4.32)} & \textbf{96.16(3.15)} & \textbf{96.97(2.89)} & \textbf{84.05(4.66)}\\ \hline
			\multirow{4}{*}{Target}     & DaST~\cite{DaST}    & 29.29(1.24)          & 41.60(1.17)          & 33.64(2.24)          & 11.55(4.80)          & 20.04(5.12)          & 45.33(4.05)          & 50.09(3.99)          & 19.88(4.88)  \\
			& DST~\cite{DST}      & 32.50(1.23)          & 42.15(1.15)          & 35.26(2.25)          & 17.22(4.98)          & 22.54(4.98)          & 56.00(3.99)          & 66.22(4.11)    & 22.47(4.87)       \\
			& MAZE-PD~\cite{MAZE} & 33.20(1.24)          & 41.99(1.08)          & 39.05(1.20)          & \textbf{22.75(5.21)}          & 24.65(5.10)          & 56.39(4.05)          & 67.18(3.92)         & 23.55(4.90)  \\
			& LCA (Ours)                           & \textbf{35.19(1.21)} & \textbf{48.36(0.99)} & \textbf{39.15(1.20)} & 19.42(3.02) & \textbf{51.63(3.19)} & \textbf{79.93(3.97)} & \textbf{80.76(3.92)} & \textbf{26.79(4.88)}\\ \hline
		\end{tabular}
	}
\end{table*}

\textbf{ASR on different white-box adversarial sample generation schemes.} 
We present the ASR using different white-box adversarial sample generation schemes for both targeted and non-targeted attacks in Table~\ref{table2}.
It is evident that our LCA achieves higher ASR when generating adversarial samples using the BIM and PGD schemes compared to other schemes. Additionally, our proposed LCA achieves comparable performance in generating adversarial samples using the FGSM scheme.
Notably, our LCA achieves a relatively high ASR boost in non-targeted attacks compared to targeted attacks. This is due to the fact that the data generated by LCA has a high accuracy for the target model, which makes the trained substitute model more similar to the target model, and thus the non-target attack is successful. However, the generated data still has negative samples for the target model, and the trained substitute model still has a gap compared to the target model, so the target attack is relatively weak.
\begin{table*}[!t]
	\centering
	\caption{Comparison of the attack success rates of our approach and competitors on different white-box adversarial sample generation schemes. For a fair comparison, we use ResNet18 as the target model for CIFAR10, ResNet50 as the target model for CIFAR100, and ResNet34 as the target model for STL10. The number in `()' indicates the average $L_2$ perturbation distance, with the best result being \textbf{bolded}. \label{table2}}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{c|c|ccc|ccc|ccc}
			\hline
			& Dataset                             & \multicolumn{3}{c|}{CIFAR10}                                       & \multicolumn{3}{c|}{CIFAR100}                                      & \multicolumn{3}{c}{STL10}                                          \\ \cline{2-11} 
			& Target Model                        & FGSM                 & BIM                  & PGD                  & FGSM                 & BIM                  & PGD                  & FGSM                 & BIM                  & PGD                  \\ \hline
			\multirow{4}{*}{\rotatebox{90}{Non-Target}} & DaST~\cite{DaST}    & 39.12(1.54)          & 60.45(0.99)          & 49.40(1.08)          & 24.73(4.34)          & 31.10(4.34)          & 26.41(4.20)          & 40.42(3.90)          & 89.77(3.09)          & 54.41(3.55)          \\
			& DST~\cite{DST}      & 45.39(1.54)          & 78.54(0.98)          & 54.93(1.10)          & 30.42(4.53)          & 33.96(4.08)          & 31.63(4.57)          & 56.90(3.60)          & 96.84(3.14)          & 66.00(3.60)           \\
			& MAZE-PD~\cite{MAZE} & 66.57(1.53)          &90.97(0.98)          & 78.99(1.11)          & 34.07(4.34)          & 36.45(4.05)          & 44.44(4.32)          & 68.78(3.69)          & 97.68(3.41)          & 69.86(3.35)          \\
			& LCA (Ours)                          & \textbf{87.62(1.42)} & \textbf{93.33(0.94)} & \textbf{90.29(0.91)} & \textbf{95.14(4.23)} & \textbf{97.59(4.06)} & \textbf{97.82(4.32)} & \textbf{96.16(3.40)} & \textbf{98.18(3.01)} & \textbf{96.16(3.15)} \\ \hline
			\multirow{4}{*}{\rotatebox{90}{Target}}     & DaST~\cite{DaST}    & 18.23(1.24)          & 62.23(1.54)          & 41.60(1.17)          & 14.84(5.23)          & 25.07(5.12)          & 20.04(5.12)          & 22.08(4.66)          & 60.80(3.82)          & 45.33(4.05)          \\
			& DST~\cite{DST}      & 23.83(1.50)          & 65.75(1.43)          & 42.15(1.15)          & \textbf{18.43(5.21)} & 24.57(5.22)          & 22.54(4.98)          & 26.97(4.47)          & 70.00(3.92)          & 56.00(3.99)          \\
			& MAZE-PD~\cite{MAZE} & 39.35(1.50)          & 69.20(1.42)          & 41.99(1.08)          & 18.37(5.23)          & 25.02(5.01)          & 24.65(5.10)          & 43.35(5.10)          & 69.57(3.78)          & 56.39(4.05)          \\
			& LCA (Ours)                          & \textbf{31.77(1.55)} & \textbf{74.79(1.42)} & \textbf{48.36(0.99)} & 5.85(4.16)           & \textbf{50.46(3.17)} & \textbf{51.63(3.19)} & \textbf{44.14(4.62)} & \textbf{79.24(3.79)} & \textbf{79.93(3.97)} \\ \hline
		\end{tabular}
	}
\end{table*}

\textbf{ASR on label-only and probability-based cases.} 
We report the ASR for both label-only output and probability-based output of the target model, as shown in Table~\ref{table3}.
Our LCA outperforms existing schemes in terms of ASR in both label-only and probability-based output cases. Moreover, our proposed LCA demonstrates similar ASR in both label-only and probability-based cases. This is due to the fact that our LCA does not rely on probabilistic outputs for discriminating member data, such that the distribution of the generated data in both cases closely aligns with the distribution of the training data of the target model. %As a result, the target network exhibits relatively high accuracy on these data samples. 
Even when the target model has label-only outputs, the substitute model can still learn the knowledge of the target model during the substitute training process. As a result, the trained substitute model is closer to the target model in both output cases, leading to outstanding ASR performance.
\begin{table}[!t]
	\centering
	\caption{Comparison of the attack success rates of our approach and competitors in terms of label-only and probability-based scenarios. Using BIM as the default attack method and ResNet18 trained on CIFAR10 as the default target model. The best result is \textbf{bolded}. \label{table3}}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{c|c|cc|cc}
			\hline
			&                                  & \multicolumn{2}{c|}{Probability-based}       & \multicolumn{2}{c}{Label-only} \\
			& \multirow{-2}{*}{Method}         & ASR                          & distance      & ASR            & distance      \\ \hline
			& DaST~\cite{DaST} & 60.45                        & 0.99          & 44.72          & 1.19          \\
			& MAZE~\cite{MAZE} & 78.54                        & 0.98          & -              & -             \\
			& DST~\cite{DST}   & 90.97                        & 0.98          & 78.24          & 1.18          \\
			\multirow{-4}{*}{Non-Target} & LCA (Ours)                        & \textbf{93.33}               & \textbf{0.94} & \textbf{90.65} & \textbf{0.94} \\ \hline
			& DaST~\cite{DaST} & 62.23                        & 1.54          & 46.96          & 1.54          \\
			& MAZE~\cite{MAZE} & 65.75                        & 1.59          & -              & -             \\
			& DST~\cite{DST}   & 69.20                        & 1.55          & 58.04          & 1.54          \\
			\multirow{-4}{*}{Target}     & LCA (Ours)                        & \textbf{74.79} & \textbf{1.42}          & \textbf{70.09}          & \textbf{1.39}          \\ \hline
	\end{tabular}}
\end{table}

\textbf{Query budget.} 
To investigate the query budget, we analyze the number of queries in two stages. In stage 1, for each data generated by diffusion model, we need to query the target model twice: once for clean images and once for noisy images. The number of queries in stage 1, denoted as $N_1$, depends on the number of classes and the size of the codebook. In stage 2, we train the substitute model by continuously querying the output of the target model, denoted as $N_2$, which depends on the specific training process and can vary based on factors such as the number of iterations and batch size.

Therefore, the total query budget, denoted as $N_{QB}$, is the sum of the queries from both stages. It can be expressed as:
\begin{equation}
	N_{QB} = N_1 + N_2.
\end{equation}

\begin{table}[!t]
	\centering
	\caption{The query volume of the first stage of our proposed LCA. Experiments are conducted under CIFAR10, CIFAR100 and STL10 datasets, different codebook sizes as well as probability-based and label-only cases, respectively. \label{table4}}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{c|c|ccccc}
			\hline
			& \multirow{2}{*}{Dataset} & \multicolumn{5}{c}{Codebook size}  \\
			&                          & 2    & 5    & 10   & 20    & 50    \\ \hline
			\multirow{3}{*}{Probability-based} & CIFAR10                  & 90   & 206  & 389  & 692   & 1580  \\
			& CIFAR100                 & 1045 & 2548 & 5100 & 10017 & 23900 \\
			& STL10                    & 91   & 194  & 351  & 644   & 1523  \\ \hline
			\multirow{3}{*}{Label-only}        & CIFAR10                  & 63   & 146  & 269  & 525   & 1231  \\
			& CIFAR100                 & 954  & 2241 & 4521 & 8847  & 18560 \\
			& STL10                    & 75   & 169  & 319  & 598   & 1438  \\ \hline
		\end{tabular}
	}
\end{table}
Table~\ref{table4} provides the first stage query number $N_1$ for different datasets at different codebook sizes. The results demonstrate that $N_1$ primarily depends on the number of classes and the size of the codebook. A larger codebook size and more classes will result in a higher number of queries. The impact of codebook size on the ASR will be discussed in Section \ref{sec_Ab}.
%Additionally, the precision of the target model can also influence the number of queries required to obtain the full codebook. If the target model has a relatively low accuracy, more queries may be needed to cover all the classes in the codebook. But in this paper, it is assumed that the target model has a relatively high accuracy, and therefore do not consider this factor.

We discuss the ASR under different query budgets, and the results are given in Fig.~\ref{fig_6} (for ResNet18 trained on the CIFAR10 dataset) and in Fig.~\ref{fig_1} (c) (for ResNet50 trained on the CIFAR100 dataset). The codebook size factor is fixed in these experiments. In Figures~\ref{fig_1} and ~\ref{fig_6}, we give the results of substitute training using data from different scenarios, which include: data generated by diffusion model using class labels as prompt (generate by label), training data of target model (Full data), data generated by GANs (DST~\cite{DST}) scheme and our LCA.

It is worth noting that our proposed LCA achieves high ASR with very small query budgets, already reaching 90\% ASR in around 200k queries. In contrast, DST scheme that use GANs structures to generate data requires a query budget of more than 9 M to achieve a relatively good ASR.
In Fig.~\ref{fig_6}, our scheme initially exhibits a slightly lower ASR compared to the full data scheme but surpasses it in the later stages. This is because the full training data is a fixed amount, while our LCA continues to generate data during the training process, thereby providing more diverse training data.
Similarly, in the experiments conducted on CIFAR100 dataset (Fig.~\ref{fig_1} (c)), our scheme exhibits more efficient performance than the full data scheme. This is attributed to the fact that the training set of CIFAR100 contains a relatively small amount of data, whereas LCA generates a significantly larger and more diverse training data.

Furthermore, considering the number of queries for different datasets in Fig.~\ref{table4}, it can be observed that the query number $N_1$ is relatively small compared to the total query budget $N_{QB}$. This indicates that our proposed solution achieves substantial performance gains with a relatively low cost.
Overall, the results demonstrate the effectiveness and efficiency of our proposed LCA and provide superior ASR at lower query budgets compared to existing schemes.%, demonstrating its superior performance with a lower query budget compared to existing schemes.
% Figure environment removed

% Figure environment removed
\subsection{Ablation Study}\label{AS}
\label{sec_Ab}
\textbf{The efficacy of the codebook size.} 
In Fig.~\ref{fig_7}, we provide the effect of different codebook sizes on the ASR. The results show that the codebook size has an impact on the ASR and the number of queries.
When the codebook length is 2, the scheme achieves a lower ASR and requires more queries. And as the codebook length increases to 5, a higher ASR is achieved with a reduced number of queries. Similarly, the scheme becomes more effective when the codebook length is further increased to 10.
However, when the codebook length is set to 20 and 50, the increase in ASR is minimal. And more query budget is required when the codebook length is larger, as shown in Table~\ref{table4}.
Therefore, considering both the ASR and the query budget, we believe that setting the codebook size to 10 in our LCA is optimal. With this setting, LCA generates sufficiently diverse images and achieves good performance in terms of ASR and query consumption.

\textbf{The efficacy of different components.}
The ablation experiments examining the effect of different components on the ASR are presented in Table~\ref{table5}. The experiments are conducted using ResNet50 trained on the CIFAR100 dataset, and the default white-box adversarial sample generation method is BIM.
In Table~\ref{table5}, the `Baseline' refers to a scheme that uses labels as prompt for diffusion model to generate data for substitute training. The `w/o LCA' indicates that LCA is not used and training is performed with inferred member data. 
\begin{table}[!t]
	\centering
	\caption{The attack success rates of models with different settings. The experiments use ResNet50 trained in CIFAR100 as the target model and use BIM to generate the adversarial samples.`Baseline' means that the data is generated directly using the diffusion model for training the substitute model, `w/o LCA' indicates that LCA is not used and training is performed with inferred member data. The best result is \textbf{bolded}. \label{table5}}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{c|c|cc|cc}
			\hline
			& \multirow{2}{*}{Method} & \multicolumn{2}{c|}{Probability-based} & \multicolumn{2}{c}{Label-only} \\
			&                         & ASR                & distance          & ASR             & distance      \\ \hline
			\multirow{3}{*}{Non-Target} & Baseline                & 60.85              & 4.05              & 61.88           & 4.06          \\
			& w/o LCA              & 65.51              & 4.07              & 66.34           & 4.06          \\
			& LCA (Ours)              & \textbf{97.59}     & \textbf{4.06}     & \textbf{97.98}  & \textbf{4.06} \\ \hline
			\multirow{3}{*}{Target}     & Baseline                & 21.47              & 3.15              & 15.48           & 3.19          \\
			& w/o LCA              & 25.24              & 3.15              & 18.78           & 3.19          \\
			& LCA (Ours)              & \textbf{50.46}     & \textbf{3.17}     & \textbf{47.46}  & \textbf{3.18} \\ \hline
	\end{tabular}}
\end{table}

The results demonstrate that the data generated directly from the diffusion model is less accurate for the specific target model, resulting in relatively low ASR. In contrast, with the use of our proposed LCA, the ASR is significantly improved.
%Furthermore, incorporating the learnable prompt into the scheme further enhances the ASR. This is owing to the fact that prompt provides additional data knowledge, enabling the generated data to better approximate the member data of the target model.
Overall, these ablation experiments highlight the importance of latent code augmentation in improving the ASR.

\section{Conclusion}
In this paper, we propose a data-free black-box attack scheme based on diffusion model to improve the efficiency and accuracy of substitute training. Specifically, we utilize the diffusion model to generate data for substitute training in data-free scenarios, which overcoming the limitations of low generation quality and the need to retrain for each target model in GANs-based schemes.
In addition, to further facilitate the diffusion model to generate data more suitable for the target model, we propose a novel LCA to guide the diffusion model in generating data.
Subsequently, the generated data is used to train the substitute model, which allows the substitute models to simulate the target model more efficiently.
The main advantage of our LCA is the ability to guide diffusion models to generate diverse and suitable data for specific target models. This effectively improves the substitute training efficiency and ensures that the substitute training can be applied to various target models trained on different datasets.
Numerous experiments demonstrate that our approach achieves a higher attack success rate while reducing the number of queries compared to existing schemes.
% This highlights the usefulness and efficiency of our approach in conducting data-free black-box attacks.

\bibliographystyle{IEEEtran}
\bibliography{Attack23.bib}

\end{document}


