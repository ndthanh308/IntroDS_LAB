\documentclass{svjour3} % onecolumn (standard format)
\smartqed % flush right qed marks, e.g. at end of proof
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{mathtools}
\usepackage{mathrsfs} 
\usepackage{times}
\usepackage{float}
\usepackage{color}
\usepackage{setspace}
\usepackage{color,soul}
\let\proof\relax
\let\endproof\relax
\usepackage{amsmath,amsfonts,amsthm,eucal}
\usepackage{enumerate}
\usepackage[letterpaper,margin=3cm]{geometry}
\usepackage{float}
\usepackage[title]{appendix}
\usepackage{color}
\usepackage{graphics}
\usepackage{stringenc}
\usepackage[
pdfstartview=XYZ,
bookmarks=true,
colorlinks=true,
linkcolor=blue,
urlcolor=blue,
citecolor=blue,
pdftex,
bookmarks=true,
linktocpage=true,   % makes the page number as hyperlink in table of content
hyperindex=true
]{hyperref}
\usepackage{lipsum}
%\usepackage{lineno}
%\linenumbers
\usepackage[FIGBOTCAP,TABTOPCAP,bf,tight]{subfigure}
\usepackage{natbib}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf}
\usepackage{booktabs} 
\usepackage{lineno}
\usepackage{empheq}

\usepackage{bm,upgreek}
\usepackage{tikz,mathpazo}
\usetikzlibrary{shapes.geometric, arrows}

\usepackage{caption}



% Insert the name of "your journal" with
%\journalname{International Journal for Numerical Methods in Engineering}
\newcommand{\klj}[1]{\textcolor{red}{\textbf{(klj)} #1}}

\newcommand{\trans}[1]{{#1}^{\ensuremath{\mathsf{T}}}}
\newcommand{\mat}[1]{\ensuremath{\mathrm{\mathbf{#1}}}}
\newcommand{\tensor}[1]{\ensuremath{\boldsymbol{#1}}}
\newcommand{\jump}[1]{\lbrack\!\lbrack #1 \rbrack\!\rbrack}
\newcommand{\alert}[1]{\textcolor{red}{#1}}
\DeclareMathOperator{\grad}{\nabla}
\DeclareMathOperator{\diver}{\nabla\cdot}
\DeclareMathOperator{\Grad}{\nabla^{\tensor X}}
\DeclareMathOperator{\Diver}{\nabla^{\tensor X}\cdot}
\DeclareMathOperator{\sym}{sym}
\DeclareMathOperator{\skw}{skw}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\dev}{dev}
\DeclareMathOperator{\sgn}{sgn}
\newtheorem{prop}{Proposition}
\theoremstyle{remark}
\newtheorem{rmk}{Remark}
\renewcommand{\vec}[1]{\ensuremath{\boldsymbol{#1}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% \newcommand{\slbk}[1]{\textcolor{violet}{SLBK COMMENT: #1}}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{multirow}

\usepackage{CJKutf8}
\usepackage{kotex}

\renewcommand{\arraystretch}{1.125}
\setlength\arraycolsep{4pt}

%% LISTINGS
\usepackage{regexpatch}% http://ctan.org/pkg/regexpatch
\usepackage{listings}% http://ctan.org/pkg/listings

\usepackage{xcolor}

% Some definitions to use color
% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{9} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{9}  % for normal

% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{terminalblack}{rgb}{0.25,0.25,0.25}
\definecolor{f77green}{rgb}{0,0.7,0}
\definecolor{f77blue}{rgb}{0.0,0,0.7}

\newcounter{python}
\newcounter{bash}
\newcounter{fortran}

\makeatletter
% --------------------------------------- Python
\newcommand{\lstlistpythonname}{List of Python Codes}
\lst@UserCommand\lstlistofpython{\bgroup
    \let\contentsname\lstlistpythonname
    \let\lst@temp\@starttoc \def\@starttoc##1{\lst@temp{lop}}%
    \tableofcontents \egroup}
\lstnewenvironment{python}[1][]{%
  \renewcommand{\lstlistingname}{Listing}%
  \let\c@lstlisting=\c@python
  \let\thelstlisting=\thepython
  \xpatchcmd*{\lst@MakeCaption}{lol}{lop}{}{}%
  \lstset{language=Python,
  basicstyle=\small,
  otherkeywords={self},
  tabsize=1,
  keywordstyle=\ttb\color{deepblue},
  emph={__init__, update_port},
  emphstyle=\ttb\color{deepred},
  stringstyle=\ttb\color{deepgreen},
  commentstyle=\color{f77green},
  frame=single,
  showstringspaces=false,
  float=htpb,
  captionpos=b,
  numbersep=5pt,
  linewidth=0.99\linewidth,
  xleftmargin=0.01\linewidth,
  breaklines=true
  breakwhitespace=false,
  #1}}
  {}
% --------------------------------------- FORTRAN
\newcommand{\lstlistfortranname}{List of FORTRAN Codes}
\lst@UserCommand\lstlistoffortran{\bgroup
    \let\contentsname\lstlistfortranname
    \let\lst@temp\@starttoc \def\@starttoc##1{\lst@temp{lof}}%
    \tableofcontents \egroup}
\lstnewenvironment{fortran}[1][]{%
  \renewcommand{\lstlistingname}{Listing}%
  \let\c@lstlisting=\c@fortran
  \let\thelstlisting=\thefortran
  \xpatchcmd*{\lst@MakeCaption}{lol}{lof}{}{}%
  \lstset{language=[90]fortran,
  basicstyle=\ttfamily\small,
  showstringspaces=false,
  keywordstyle=\color{f77blue},
  stringstyle=\color{purple},
  commentstyle=\color{f77green},
  breakatwhitespace=false,
  frame=single,
  linewidth=0.99\linewidth,
  xleftmargin=0.01\linewidth,
  breaklines=true
  breakwhitespace=false,
  showstringspaces=false,
  captionpos=b,
  #1}}
  {}

%% ALGORITHM
\usepackage{algorithm}
\usepackage{algpseudocode}

\newcommand\StateX{\Statex\hspace{\algorithmicindent}}




\algdef{SE}[SUBALG]{Indent}{EndIndent}{}{\algorithmicend\ }%
\algtext*{Indent}
\algtext*{EndIndent}

\title{
Discovering interpretable elastoplasticity models via the neural polynomial method enabled symbolic regressions
}

\begin{document}

\titlerunning{Interpretable ML plasticity}

\author{Bahador Bahmani \and Hyoung Suk Suh \and WaiChing Sun}

\institute{Corresponding author: WaiChing Sun 
\at Associate Professor,
Department of Civil Engineering and Engineering Mechanics, Columbia University,
614 SW Mudd, Mail Code: 4709, New York, NY 10027 Tel.: 212-854-3143, Fax:
212-854-6267, 
  \email{wsun@columbia.edu}
}

%\date{Received: \today / Accepted: date}

\maketitle



\begin{abstract}
Conventional neural network elastoplasticity models are often perceived as lacking interpretability. This paper introduces a two-step machine-learning approach that returns mathematical models interpretable by human experts. In particular, we introduce a surrogate model where yield surfaces are expressed in terms of a set of single-variable feature mappings obtained from supervised learning. A postprocessing step is then used to re-interpret the set of single-variable neural network mapping functions into mathematical form through symbolic regression. This divide-and-conquer approach provides several important advantages. First, it enables us to overcome the scaling issue of symbolic regression algorithms. From a practical perspective, it enhances the portability of learned models for partial differential equation solvers written in different programming languages. Finally, it enables us to have a concrete understanding of the attributes of the materials, such as convexity and symmetries of models, through automated derivations and reasoning. Numerical examples have been provided, along with an open-source code to enable third-party validation.
%


\end{abstract}

%
\keywords{quadratic neural model; neural additive model; symbolic regression; level-set model; computational plasticity}


\section{Introduction}
\label{sec:intro}
In the last decade, the number of machine learning constitutive models has increased significantly \citet{ghaboussi1991knowledge, pernot1999application, mozaffar2019deep, liu2021review}. Among those machine learning models, neural networks trained with experimental or simulation data have been 
one of the most popular choices \citep{li2019machine, wang2018multiscale, vlassis2020geometric, vlassis2021sobolev, flaschel2022discovering}.
Despite the recent popularity of these neural network models, 
the adaptation of these models on high consequent engineering applications is slow. A potential issue could be attributed to the inherent lack of reproducibility of the neural network models \citep{suh2023publicly} and interpretability/explainability.
%
%
In the former case, the reproducibility of the performance can be achieved by open-sourcing the training codes and systematic benchmarking (e.g. \citet{wen2020convolutional}). 
%
%
%
In the latter case,  due to the complexity and nonlinearity of the utilized black-box deep neural network models, understanding input-output relationships becomes challenging, making them not interpretable.
%

To address this black-box issue,\citep{vlassis2022component} adopt a component-based design where the ingredients of elastoplasticity models are represented by individual neural networks to distinguish the properties of the elasticity models 
from the plastic responses, a departure from the recurrent neural network approach or multi-step feed-forward neural network approaches in \citet{ghaboussi1991knowledge, mozaffar2019deep}. Coincidentally, this approach has also been used in \citet{fuhg2023modular} and reintroduced as modular machine learning elastoplasticity. 
Other related efforts to introduce more interpretable models include the incorporation of knowledge graphs 
\citep{ wang2019meta, he2022thermodynamically}, causal discovery for constitutive responses \citep{sun2022data}. 
These approaches may provide relational and structural knowledge about the learned material models and facilitate the model validation against physical constraints, as such thermodynamic consistency \citep{vlassis2021sobolev}. However, the lack of analytical expression makes it difficult to provide a definite proof of basic properties such as convexity and material symmetry.

Another approach that has been attempted in the literature involves  performing symbolic regression directly to learn a portion or all of the plasticity models \citep{versino2017data, wang2022establish, bomarito2021development}. The advantage of this approach is that it may lead to a mathematical expression of the learned function that is much shorter than the neural network counterparts and hence suitable for analysis. 
However, as the symbolic regression requires solving a combinatorial optimization to find the optimal equation expressed as a tree, the number of possible combinations of symbolic expression grows rapidly with the dimensionality of the input and output; it is an NP-hard problem (cf. \citet{mundhenk2021symbolic}).
%
%

On a related note, \citep{linka2023new, linka2023automated,tacc2023benchmarking} apply a different symbolic regression approach in which a set of prior hyperelasticity models are chosen as the basis functions for biological tissues, and an optimization problem is solved to determine the coefficients of the learned models. In principle, this interpolation technique can also be used for learning yield functions or hardening laws. However, since the modeling space is spanned by the chosen basis models, the selection of the basis models and how well they correspond to the mechanical behaviors are all crucial factors that may determine the success of this model.

% Figure environment removed

\subsection{Interpretability vs. expressivity}
Multilayer perceptrons (MLP) have been demonstrated to be good candidates for supervised regression tasks where a single fully-connected deep neural network 
%is $\text{MLP}(\vec{x}; \vec{\beta}): \mathbb{R}^D \to \mathbb{R}$ 
is utilized as the model class \citep{szegedy2013intriguing}.  
MLP is also robust in learning nonlinear models that can distinguish data that is not linearly separable.
Due to the high expressivity of deep neural networks and the inherent nonlinear interactions among the input features,  a consistently high accuracy has been reported by those successfully trained MLP models. 
However, it could become challenging to interpret/extract the input-output relationships for neural network models with multivariate inputs because they may combine the input features in a highly nonlinear manner; this entanglement makes it difficult to isolate the effect of each feature on the output \citep{peng2019domain}. 

This black-box issue is not only a technical barrier to computational
 mechanics, but also critical for many other disciplines, such as 
 drug delivery where the interpretability of the solution is critical.
\citet{doran2017does}, for instance,  define interpretable systems: \textit{“A system where a user cannot only see, but also study and understand how inputs are mathematically mapped to outputs."}
\citet{gilpin2018explaining} define intelligibility as a combination of explainability (being able to provide a rationale for the results, sometimes through posthoc analysis ) and interpretability (the logic that delivers the learned results can be comprehended by humans.)
In many cases, explainability could be achieved by 
model-agnostic methods developed to explain the predictions of black-box models via the feature importance and local approximation, as pointed out by \citet{xu2022sparse}.  
Meanwhile, models that are inherently interpretable, such as decision-tree-based models, often introduce mechanisms (e.g.,  hierarchical decisions or rules) such that the rationale of the trained model can be understood. 


Nevertheless, as pointed out by \citet{agarwal2021neural},  
machine learning techniques that exhibit high interpretability 
often lack the level of expressivity (the ability to express an arbitrary function -- a necessary but not sufficient condition for accuracy) to yield accurate predictions for complex tasks. 
Fig. \ref{fig:tradeoff} (modified from \citet{agarwal2021neural}) illustrates the trade-off between interpretability and expressivity for a variety of common machine learning techniques, which include deep neural nets, boosted trees, and random forest. 
As supported by the universal approximation theorem \citet{hornik1989multilayer}, the deep neural network is often considered a machine learning tool with high expressivity but also 
difficult to interpret. Meanwhile, linear regression is easy to interpret but often lack the expressivity for more complex tasks.

Interestingly, a successful symbolic regression may achieve 
both the desirable level of expressivity and interpretability. 
However, the NP-hard nature of the symbolic regression problem 
in for the multi-dimensional data makes it difficult to predict 
the probability of yielding a successful training. 
\citet{petersen2019deep}, for instance, benchmark the performance of 
6 state-of-the-art symbolic regression software packages and found a zero percent success in recovering the following equation among all of the state-of-the-art software packages, 
\begin{equation}
(fx,y) = x^4 − x^3 + \frac{1}{2}y^{2} - y
\label{eq:difficultequation} 
\end{equation}


\subsection{Neural Additive Models: Trade-off for Interpretability and expressivity}
\label{sec:nam}
\citet{agarwal2021neural} propose the Neural Additive Model (NAM) in which a  set of independent neural networks are co-trained to generate a set of nonlinear scalar features $f_{i}(x_{i})$, one for each input $x_{i}$ where $i=1,2,...,D$ and $D$ is the number of input dimensions.  The model structure is the linear combination of these scalar features, i.e., 
%
\begin{equation}
\bar{\phi}(\vec{x}; \vec{\beta},  \vec{\omega}) = \sum_{i=1}^D w_{i} f_i(x_i; \vec{\beta}_i),
\label{eq::nam}
\end{equation}
%
where each feature function $f_i$ is parameterized by a multilayer perceptron (MLP) with parameters $\vec{\beta}_i \in \mathbb{R}^{M_i}$,  and $M_i$ is the total number of trainable parameters of the \textit{i}-th MLP.  
%Input feature dimension $x_i \in \mathbb{R}$ is only used in the \textit{i}-th MLP $f_i(x_i): \mathbb{R} \to \mathbb{R}$.  
These single-variable MLP functions are referred to as \textbf{shape functions}.
The contribution of each shape function is controlled by the trainable parameters $w_{i} \in \mathbb{R}$.  
Concatenation of neural network parameters and shape function weights are denoted by $\vec{\beta} = \{ \vec{\beta}_i \}_{i=1}^D$ and $\vec{w} \in \mathbb{R}^D$, respectively.

\citet{agarwal2021neural} argues that this approach is  interpretiable in the sense that importance of each feature 
can be ranked through examining the coefficients of the feature $w_{i}$. In other words, the NAM approach maintains the interpretability of the linear regression (in the feature space) with enhanced an expressivity afforded by the neural networks. 
However, \citet{agarwal2021neural} also points out that NAM
exhibits less expressivity of the fully connected neural network, 
especially when expressing the ground-truth function requires
bases independent of the feature basis functions. 
% 

% 

\subsection{Proposed strategy for interpretable model recovery}
Given the fact that constitutive laws are often used for high-consequent engineering applications, making the machine learning generated constitutive laws interpretable is necessary but not sufficient to make those models trustworthy and hence has the potential to have impacts on engineering applications. 

The purpose of this article is to propose a two-step machine learning approach that strikes a balance between the expressivity 
and interpretability (see Fig. \ref{fig:tradeoff}). 
We first propose a modification of the neural additive model
in which we continue to use neural networks to parametrize
a set of scalar-valued shape functions. 
However, instead of directly using these shape functions as the bases for the yield function, we introduce additional quadratic terms of the feature shape function to further enhance the expressivity and hence the accuracy of the machine learning model (see Section \ref{sec:qnm}). 
To further enhance the interpretability, we design a new symbolic regression step introduced after the neural network training. 
Here we leverage the special neural network architecture of the quadratic neural model in which the set of neural networks
that generates the feature space are all scalar-valued scalar functions. This setting enables us to break down the  NP-hard high-dimensional symbolic regression problem into a series of separated one-dimensional symbolic regressions (see Section \ref{sec:symb-regr}), which have consistently been successful in discovery yield surface of 3 to 5 dimensions in 
our numerical experiments (see Section \ref{sec:num_example}.)

\section{Method}
\label{sec:interp_ML}

We begin by introducing the general problem statement of finding yield surface as a supervised regression problem in Section \ref{sec:prob_state}. We then introduce the quadratic neural model (QNM) in Section \ref{sec:qnm}. We explain our choice of neural network architecture in Section \ref{sec:nn-model}. Additionally, in Section \ref{sec:sparse-loss}, we specify a sparsity-promoting constraint used during the training process to enhance the simplicity and interpretability of the model. For completeness, we provide details of the genetic programming algorithm that conduct the symbolic regression for the feature bases in Section \ref{sec:symb-regr}.

\subsection{Problem statement}
\label{sec:prob_state}
Our learning task is to find a mapping function $\phi(\vec{x}): \mathbb{R}^D \to \mathbb{R}$ from  any element of $D$-dimensional Euclidean space onto real number, where $\vec{x}$ 
is the state variables for the yield function, including the Cauchy stress and internal variables, $D$ is the dimension of the inputs,  and $\phi(\vec{x})$ is the yield function. 
Given $N$ data points stored as a point cloud $\mathcal{C} = \{ \vec{x}^l,  \phi^l \}_{l=1}^{N}$,  we approximate such a function by the parametric function $\bar{\phi}(\vec{x};\vec{\beta}, \vec{w})$ where the best estimator for its unknown parameters $\vec{\beta}$ and $\vec{w}$ are found via,  in the sense of least square,
%
\begin{equation}
	\vec{\beta},  \vec{w} = \underset{\vec{\beta}, \vec{w}}{\argmin} 
	\frac{1}{N}
	\sum_{l=1}^N 
	(\bar{\phi}(\vec{x}^l; \vec{\beta}, \vec{w}) - \phi^l)^2
	+
	\mathcal{L}_{\text{sparsity}}(\vec{w}),
\label{eq::prob-state}
\end{equation}
where $\mathcal{L}_{\text{sparsity}}$ is a regularization term for sparsity control (see Section \ref{sec:sparse-loss}).
%
In the conventional setting,  one may use a multivariate fully connected neural network as the model class.  A major departure of our approach is that we will instead postulate the existence of a feature space spanned by basis functions $f_{i}$ learned by univariate neural networks (see Section \ref{sec:qnm}). 
%
This configuration enables us to obtain sufficiently expressive models $\bar{\phi}(\vec{x}^{l})$  while interpretability is guaranteed via symbolic regressions that replace the trained neural networks that parametrize basis functions of the feature space. 

%

\subsection{Quadratic Neural Model for enhanced expressivity}
\label{sec:qnm}
As mentioned in Section \ref{sec:nam}, the original neural additive model enhances the interpretability of the learned model through the generation of feature basis. The resultant model then becomes linear in the feature space. This enhanced interpretability is, nevertheless, achieved at the expense of expressivity. To circumvent
this limitation, we generalize the formulation of the neural additive model to introduce additional quadratic terms (see Fig. ~\ref{fig:step1}.) As such, we refer to this revised approach as Quadratic Neural Model (QNM). 

% Figure environment removed

The resultant learned yield function expressed with the additional enhancement bases reads:
%
\begin{equation}
\bar{\phi}(\vec{x}; \vec{\beta},  \vec{\omega}) = \sum_{i=1}^D w_{i} f_i(x_i; \vec{\beta}_i)
+
\sum_{i=1}^D \sum_{j=i}^D w_{ij} f_i(x_i; \vec{\beta}_i) f_j(x_j; \vec{\beta}_j), 
\label{eq::qnm}
\end{equation}
%
where $w_{ij}$ are additional trainable parameters that control contributions of the second order interactions between $\textit{i}$-th and $\textit{j}$-th shape functions.  
$\vec{w} \in \mathbb{R}^{D + D (D-1)/2}$ denotes the concatenation of all parameters $\vec{w} = \{ w_i, w_{ij}\} $.
Notice that the same functions as those used in the first-order term are utilized for the second-order term; in total, there are $D$ numbers of different shape functions to be learned simultaneously. 


%

%
%





\subsubsection{Neural network architecture for shape functions}
\label{sec:nn-model}
The functional forms in Eqs.~\ref{eq::nam} and \ref{eq::qnm} are limited in terms of the level of interaction among different input features. Therefore, the choice of parametric function for each shape function becomes crucial to avoid any failure due to a lack of expressivity or flexibility. Previous studies have shown that neural networks with low-dimensional input spaces are "lazy learners" \citep{tancik2020fourier,rahaman2019spectral}, meaning their convergence to capture high-frequency content becomes slower or even impossible.

As our learning task involves a univariate neural network, we must ensure that each shape function can handle such scenarios. In the NAM work by \citet{agarwal2021neural}, the authors propose a new activation function to address this issue. However, in our work, we build upon recent developments in enriching classical neural networks with Fourier layers, as described in \citep{rahimi2007random,tancik2020fourier}. This approach can better capture high-frequency content and improve the overall performance of our model.


% Figure environment removed

Each shape functions $f_i(x_i)$ shown in Fig.~ \ref{fig:nn-arch} is parameterized by an MLP enriched with the Fourier layer as follows:
%
%
\begin{equation}
f_i(x_i; \vec{\beta}_i) =
h(
\vec{W}_L^i
\cdots
g(
\vec{W}_2^i
g(
\vec{W}_1^i 
\vec{\gamma}(x_i)
+ \vec{b}_1^i
)
+ \vec{b}_2^i
)
\cdots
+
\vec{b}_L^i
),
\end{equation}
%
where $\vec{W}_k^i$ and $\vec{b}_k^i$ are the weight and bias of the $k$-th hidden layer and $g(\cdot)$ and $h(\cdot)$ are hidden and output activation functions, respectively.  In this MLP function, the input layer $\vec{\gamma}(x_i)$ is the Fourier mapping of the input feature $x_i$ with random frequency vector $\vec{v} \in \mathbb{R}^{M}$ as follows:
%
\begin{equation}
\vec{\gamma}(x_i)
= 
\sin(\vec{v} x_i) 
\oplus
\cos(\vec{v} x_i)
=
[\sin(\vec{v} x_i)^T, \cos(\vec{v} x_i)^T]^T \in \mathbb{R}^{2M},
\end{equation}
%
here $\oplus$ indicates vector concatenation operation.  
Components of the random vector $\vec{v}$ are sampled from zero-mean normal distributional with standard deviation $\sigma_v$, i.e., $v_m \sim \mathcal{N}(0, \sigma_v)$. 
In this work, we keep the random Fourier features fixed during training, although they can be considered trainable parameters.  Previous research has demonstrated that optimizing them may not improve the approximation power and may increase the computational cost.  
All trainable parameters associated with the $i$-th shape function are denoted by  
$\vec{\beta}_i = 
\{ 
\vec{W}_l, \vec{b}_l
\}_{l=1}^{L}
$.


To demonstrate the effectiveness of our architecture choice, we conducted an educational example following \cite{agarwal2021neural}. We generated a training dataset with high-frequency content using purely random noise and then empirically examined whether our architecture choice could overfit the training data.

The results, shown in Fig.~\ref{fig:noise-pred}, confirm that the neural network enriched with the spectral layer is capable of overfitting the data.  We note that overfitting is generally not desirable, but in this particular example, our goal was to measure the expressivity and flexibility of the architecture by its ability to memorize the entire training dataset.

% Figure environment removed

In this framework,  the contribution of each shape functions $f_i$ in Eq.~ \ref{eq::qnm} is balanced by the associated weight $w_i$.  For example,  $w_i \gg w_j$ is intended to mean that the $i$-th shape function effect is much more important than the $j$-th shape function in the final prediction.  The necessary condition for this argument to be meaningful is that shape functions should have the same scale.  To achieve this, we apply the \texttt{tanh} activation function in the last layer of the neural network architecture,  which restricts the output to a range between -1 and 1.

\subsubsection{Regularization of polynomial function in feature space}
\label{sec:sparse-loss}
In terms of interpretability, one may prefer only the linear combination of shape functions than having higher-order terms involved in the approximation.  
Moreover,  one may prefer to have the least number of activated shape functions which is equivalent to a feature extraction task, i.e.,  removing the spurious input features not having significant contribution in the approximation. 

To incorporate such biases towards interpretability and simplicity, we have included a regularization term in the optimization statement Eq.~\ref{eq::prob-state} to promote sparsity for low-order and high-order terms. 
The so-called $L0$-norm, which simply counts the total number of nonzero elements of a vector, is known as one of the best sparsity measures \citep{natarajan1995sparse,gale2019state}.  
 However,  $L0$-norm minimization is an NP-hard problem and makes the proposed loss function in Eq.~\ref{eq::prob-state} non-differiantiable \citep{natarajan1995sparse},  which is not preferred. 
As a result,  we use the $L1$-norm as a differentiable replacement of the $L0$-norm as follows \citep{tibshirani1996regression,brunton2016discovering}, 
%
\begin{equation}
\mathcal{L}_{\text{sparsity}}(\vec{w}) = 
\alpha_{\text{lo}} \sum_{i} |w_{i}| 
+ 
\alpha_{\text{ho}} \sum_{i,j} |w_{ij}|,
\label{eq::sparse_loss}
\end{equation}
%
where $\alpha_{lo}$ and $\alpha_{ho}$ are hyperparameters to control how much low-order and higher-order terms should be sparse; a higher value penalizes more.

\subsection{Symbolic Regression of feature space for enhanced interpretability}
\label{sec:symb-regr}

Symbolic regression (SR) seeks to discover a mathematical expression that best fits a given dataset without specifying the 
form of the mathematical expression. 
Without specifying a form enables more flexibility to curve-fit the data. However, symbolic regression, particularly for multi-dimensional vector-valued or tensor-valued functions, is significantly more difficult due to the combinatoric nature of the optimization problem necessary to search the mathematical expression \citep{icke2013improving,de2018greedy}.
However, the existence of the polynomial feature space spanned by the basis $\{1, f_{i}, f_{i} f_{j}\}$ offers us an opportunity to break down the multi-dimensional symbolic regression problem 
into multiple one-dimensional problems, one for each shape function $f_{i}$. The final learned function is then expressed as the polynomial in the feature space $\text{span}(\{1, f_{i}, f_{i} f_{j}\})$ (see Fig. \ref{fig:step2}).
This setting may greatly reduce the difficulty of the symbolic regression problem at the expense of injecting the additional assumption that the learned function can be expressed in the aforementioned way. 

% Figure environment removed

% already mentioned
 The space of possible expressions is commonly defined by specifying the set of mathematical operators, functions, variables, and constants that can be used to construct the expressions represented efficiently in binary trees (see Fig. \ref{fig:bnary_tree}.)
 %
Genetic programming is one of the most popular stochastic optimization methods to search the combinatorial space of all possible mathematical expressions \citep{koza1994genetic,schmidt2009distilling,wang2019symbolic}.
Recently, methods based on deep reinforcement learning are also developed as alternative ways for conducting an efficient discrete search in the space of tree data structures \citep{petersen2019deep,landajuela2021discovering}. 

% Figure environment removed

In genetic programming, the space of possible expressions is represented as a \textit{population} of candidate solutions, which are randomly generated at the start of the algorithm. Each \textit{individual} candidate solution is represented as an expression binary tree (shown in Figure \ref{fig:bnary_tree}), where the leaves of the tree represent the input variables or constants, and the internal nodes represent the mathematical operations or functions. The genetic programming algorithm then evaluates the \textit{fitness} of each candidate solution by comparing its output to the target output values. Fitness measures how well the candidate solution approximates the data, and mean square error is a commonly used fitness function.

% Figure environment removed


The genetic programming algorithm then iteratively evolves the population of candidate solutions through \textit{selection},  \textit{crossover}, and \textit{mutation} operators, in a process similar to natural selection. Selection involves choosing the fittest individuals from the current population based on their fitness scores. Crossover, as shown in Figure \ref{fig:cross_mut_ops}(a), combines the genetic information of two individuals to create offspring with characteristics from both parents.  Mutation involves randomly changing some of the genetic material of an individual to introduce new variations in the population; see Figure \ref{fig:cross_mut_ops}(b). 

Through these operations, the genetic programming algorithm creates a new generation of candidate solutions with higher fitness than the previous generation. The process is repeated until a satisfactory mathematical expression is found that fits the data well. Once a satisfactory expression is found, it can be used to predict output values for new input values that were not used in the training dataset. At inference,  symbolic equations are more lightweight than other standard machine learning models, such as neural networks needed to store a large number of parameters, hence more transportable. Discovered equations by SR are shown to generalize well outside the train data support \citet{kim2020integration}, providing successful training.



%
%
Unlike multi-dimensional symbolic regression,  in our proposed framework,  we extract a symbolic equation for each shape function: single-variable to single-variable data.  This feature enables easy application of parallel computing; enabling SR algorithms to be executed simultaneously for each shape function.
%
In this work, we conduct symbolic regression with \texttt{PySR} open source package \citep{cranmer2020discovering,cranmer2023interpretable}, which is developed based on evolutionary-based genetic programming. In this approach, a list of unitary and binary operators can be specified to restrict the search space of tree structures. The number of adjustable constants for these operators can be specified; a gradient-based optimizer optimizes these parameters after each iteration. 
There is a tradeoff between model complexity and expressivity during SR optimization. One needs to tune and balance the complexity versus expressivity of found analytical expression to increase interpretability and reduce overfitting. Users can decide based on the desired accuracy and simplicity to choose the best equation.

\definition[Complexity score in symbolic regression]{The complexity of a symbolic equation is often assessed qualitatively rather than quantitatively, as there is no universally agreed-upon definition. In this context, we adopt the complexity measure defined in \cite{cranmer2023interpretable}, which utilizes the number of nodes in an expression tree as the complexity score. While it is possible to assign different weights to each node type, such as considering the exponential operator $\exp(\cdot)$ as more complex than the addition operator $+$, we do not incorporate such weightings in our analysis.
}

\remark[Related literature on plasticity]{
\cite{versino2017data} introduced the application of SR for learning flow stress models from data. They incorporated domain knowledge (e.g.,  via augmenting the data,  constraining the functional form, etc.) to enhance the generalization accuracy of the SR.  \cite{bomarito2021development} showed the application of SR in discovering plastic yield surface equations.
Our divide-and-conquer approach offers mechanical analysts an additional level of flexibility, allowing them to visually inspect the required equation type for SR before running computationally expensive symbolic regression. Additionally, our approach may exhibit superior performance in higher dimensions compared to direct SR \cite{cranmer2020discovering}. Furthermore, addressing physics constraints such as convexity can be achieved during differentiable QNM training in the first step, potentially reducing computational costs compared to the approach that incorporates physics constraints during the discrete search of SR.
}

%
\remark[Related literature on hybrid SR]{
Other research efforts have also utilized scalable models (e.g., neural networks and decision trees) in combination with SR algorithms to handle the curse of dimensionality while maintaining interpretability \citep{icke2013improving,cranmer2020discovering,wadekar2020modeling}.
However, like NAM, their functional form assumption does not account for potential interactions among features, which may be necessary in certain applications.  The proposed QNM approach considers such interactions in the constructed quadratic space of learned shape functions.
% 
\subsection{Implementation for third-party validation in open-source finite element models}
\label{sec:implications_yield_surf}
%
For completeness and to ensure third-party reproducibility, we outline the steps taken to implement the generated yield surface 
model into the user-defined material subroutine (UMAT) for finite element simulations. Unlike the previous approach in \citet{suh2023publicly} where yield functions are parametrized via neural network, the implementation of the analytical model is profoundly simpler.  

%


\subsubsection{Level-set plasticity modeling framework}
\label{sec:level_set}
By limiting our attention to the case where material behavior is perfectly plastic, the yield function $f_y$, which can be expressed in terms of the Cauchy stress $\tensor{\sigma} \in \mathbb{S}$, defines an elastic domain $\mathbb{E}$ as a region where $f_y(\tensor{\sigma}) < 0$ and a plastic domain $\partial \mathbb{E}$ where $f_y(\tensor{\sigma}) = 0$, such that all the admissible stresses that belong to its closure $\overline{\mathbb{E}}$:
\begin{equation}
\label{eq:elastic_region}
\overline{\mathbb{E}} = \mathbb{E} \cup \partial \mathbb{E} =
\lbrace \tensor{\sigma} \in \mathbb{S} | f_y(\tensor{\sigma}) \le 0 \rbrace.
\end{equation}
Based on the theory of elastoplasticity, this means that a dataset of stress points at the plastic regime measured either from the experiments or from the sub-scale simulations always reside on the yield surface $f_y(\tensor{\sigma}) = 0$. 
Since the data lacks the information inside and outside the yield surface, training a data-driven model directly from the collected stress states is not an easy task since the learned function may not be capable of returning positive values if the given stress is inadmissible and negative values at the elastic regime. 
Hence, this study adopts the concept of level-set modeling framework proposed by \citet{vlassis2021sobolev,vlassis2022component} that regularizes the yield function $f_y(\tensor{\sigma})$ into an implicit signed distance function $\phi(\tensor{\sigma})$ that is well-defined anywhere in the space of second-order symmetric tensors $\mathbb{S}$:
\begin{equation}
\label{eq:signed_distance}
\phi(\hat{\tensor{x}}) = 
\begin{dcases}
d(\hat{\tensor{x}})  &\text{if } f_y(\hat{\tensor{x}}) > 0, \\
0                    &\text{if } f_y(\hat{\tensor{x}}) = 0, \\
-d(\hat{\tensor{x}}) &\text{if } f_y(\hat{\tensor{x}}) < 0,
\end{dcases}
\end{equation}
where $\hat{\tensor{x}}$ is an arbitrary stress point represented in a proper parametric space, while $d(\hat{\tensor{x}})$ represents the minimum Euclidean distance between $\hat{\tensor{x}}$ and the yield surface in principal stress space. 
It should be noted that the choice of parametric space for representing the stress state $\hat{\tensor{x}}$ greatly affects the performance of the data-driven model, as pointed out in \citet{kuhn2013applied}. 
Although it's effect will be further discussed in Section \ref{sec:low_order_NAM},  this section focuses on a cylindrical coordinate system for the $\pi$-plane orthogonal to the hydrostatic axis, i.e., $\hat{\tensor{x}} = \hat{\tensor{x}}(p, \rho, \theta)$, rather than directly adopting the Cartesian coordinates ($\sigma_1$, $\sigma_2$, $\sigma_3$). 

Since the yield surface cross-section perpendicular to the hydrostatic axis forms a closed loop, one possible way to construct the signed distance field $\phi$ is to solve the Eikonal equation, i.e., 
\begin{equation}
\label{eq:Eikonal}
\| \nabla^{\hat{\tensor{x}}} \phi \| = 1,
\end{equation}
while imposing homogeneous Dirichlet boundary condition at the stresses that belongs to $\partial \mathbb{E}$ since they exhibit zero minimum Euclidean distance from the yield surface. 

Based on the obtained signed distance field, we augment the original set of stress points that satisfies $f_y(\tensor{\sigma})=0$ with  $N$ sets of points that are not necessarily located on the yield surface. These auxiliary data points are added, as the yield function is an implicit function that is defined everywhere in the parametric space. Adding these auxiliary data may help us reach the intended inductive bias with more support. The unit stress gradient also helps the learned model have a unit plastic flow, enabling the plastic multiplier to reflect the magnitude of the plastic strain for a given plastic flow direction. 

\subsubsection{Implicit integration of interpretable-ML-based constitutive relation}
\label{sec:stress_integration}
This section presents an implicit return mapping algorithm for the interpretable-ML-based constitutive equation that computes the stress tensor $\tensor{\sigma}_{\text{n+1}}$ at loading step $\text{n}+1$ for a given strain increment $\Delta \tensor{\varepsilon}$ and the previous stress state $\tensor{\sigma}_{\text{n}}$. 
Similar to the previous studies, e.g., \citep{wilkins1963calculation, hughes1984numerical, borja2013plasticity}, the stress integration consists of an elastic predictor that computes the trial stress $\tensor{\sigma^{\text{tr}}}_{\text{n+1}}$, followed by a plastic correction scheme, while the only difference is that we replace the mathematical expression of the yield criterion with the trained model $\bar{\phi}$ (i.e., either NAM, QNM, or symbolic model). 
In this case, by restricting the formulation within the infinitesimal range and assuming that the elasticity tensor $\mathbb{C}^e$ is given, the rate form of the constitutive equation based on an associative flow rule can be expressed as,
\begin{equation}
\label{eq:const_rate_form}
\dot{\tensor{\sigma}}
=
\mathbb{C}^e : \dot{\tensor{\varepsilon}}^e
=
\mathbb{C}^e : \left( \dot{\tensor{\varepsilon}} - \dot{\lambda} \frac{\partial \bar{\phi}}{\partial \tensor{\sigma}} \right),
\end{equation}
since the infinitesimal strain tensor can additively be decomposed into the elastic ($\tensor{\varepsilon}^e$) and plastic ($\tensor{\varepsilon}^p$) parts, while $\lambda$ indicates the plastic multiplier. 
Here, the incremental form of Eq.~\eqref{eq:const_rate_form} can be obtained by substituting the trial stress $\tensor{\sigma^{\text{tr}}}_{\text{n+1}} = \mathbb{C}^e : \tensor{\varepsilon}^{e,\text{tr}}_{\text{n+1}}$ (where $\tensor{\varepsilon}^{e,\text{tr}}_{\text{n+1}} = \tensor{\varepsilon}^e_{\text{n}} + \Delta \tensor{\varepsilon}$ indicates the trial elastic strain) computed via elastic predictor:
\begin{equation}
\label{eq:const_inc_form}
\tensor{\sigma}_{\text{n+1}}
=
\tensor{\sigma^{\text{tr}}}_{\text{n+1}}
-
\Delta \lambda \mathbb{C}^e : \left. \frac{\partial \bar{\phi}}{\partial \tensor{\sigma}} \right|_{\text{n+1}}.
\end{equation}
If we further limit our attention to the case where the plastic behavior of our target material is isotropic, the predictor-corrector scheme can be reduced in principal stress axes as:
\begin{equation}
\label{eq:const_principal_axes}
\sigma_A = \sigma_A^{\text{tr}} - \Delta \lambda \sum_{B=1}^3 C^e_{AB} \frac{\partial \bar{\phi}}{\partial \sigma_A}
\: \: ; \: \:
\varepsilon^e_A = \varepsilon^{e,\text{tr}}_A - \Delta \lambda \frac{\partial \bar{\phi}}{\partial \sigma_A},
\end{equation}
where we omit the subscript $\text{n}+1$ for brevity. 
Here, $\sigma_A$ ($A = \lbrace 1,2,3 \rbrace$) denotes the principal stress, and $C^e_{AB}$ indicates the elastic moduli in principal axes, where its matrix form can be expressed as,
\begin{equation}
\label{eq:matrix_elas_mod}
[C_{AB}^e]
=
\begin{bmatrix}
K + \frac{4 \mu}{3} & K - \frac{2 \mu}{3} & K - \frac{2 \mu}{3} \\[1.2ex]
K - \frac{2 \mu}{3} & K + \frac{4 \mu}{3} & K - \frac{2 \mu}{3} \\[1.2ex]
K - \frac{2 \mu}{3} & K - \frac{2 \mu}{3} & K + \frac{4 \mu}{3} 
\end{bmatrix},
\end{equation}
if the elastic behavior of the material is linear, while $K$ and $\mu$ are the bulk and shear moduli, respectively. 
Recall that either NAM or its polynomial extension (QNM) adopts total $D$ univariate MLPs assigned for each input feature, while we parameterize stresses in cylindrical coordinates, e.g., $\bar{\phi} = \bar{\phi}(\hat{\tensor{x}})$. 
Hence, the stress gradient of the trained model in Eq.~\eqref{eq:const_principal_axes} can be obtained via chain rule as,
\begin{equation}
\label{eq:stress_grad1}
\frac{\partial \bar{\phi}}{\partial \sigma_A}
=
\sum_{i=1}^3
\frac{\partial \bar{\phi}}{\partial \hat{x}_i} \frac{\partial \hat{x}_i}{\partial \sigma_A},
\end{equation}
where:
\begin{equation}
\label{eq:stress_grad2}
\frac{\partial \bar{\phi}}{\partial \hat{x}_i} 
= 
\left[ w_i + \underbrace{w_{ii} f_i(\hat{x}_i) + \sum_{j=1}^D w_{ij} f_j(\hat{x}_j)}_{\text{H.O.T.}} \right] 
\frac{\partial f_i}{\partial \hat{x}_i}
\: \: \text{(no sum)}.
\end{equation}
Note that higher order terms (H.O.T.) in Eq.~\eqref{eq:stress_grad2} only exist if we adopt QNM or QNM-based symbolic model. 


Based on Eq.~\eqref{eq:const_principal_axes} and the consistency condition $\bar{\phi} = 0$, we formulate a return mapping algorithm in the principal strain space which iteratively solves a nonlinear problem $\tilde{\vec{r}}(\tilde{\vec{x}}) = \vec{0}$ until the magnitude of the residual vector reaches an acceptable value near zero. 
Specifically, we construct the local residual vector $\tilde{\vec{r}}(\tilde{\vec{x}})$ and the unknown vector $\tilde{\vec{x}}$ as follows:
\begin{equation}
\label{eq:residual_and_solution_vec}
\tilde{\vec{r}}(\tilde{\vec{x}})
=
\begin{bmatrix}
\tensor{\varepsilon}_1^e - \tensor{\varepsilon}_1^{e,\text{tr}} + \Delta \lambda \frac{\partial \bar{\phi}}{\partial \sigma_1} \\[1.2ex]
\tensor{\varepsilon}_2^e - \tensor{\varepsilon}_2^{e,\text{tr}} + \Delta \lambda \frac{\partial \bar{\phi}}{\partial \sigma_2} \\[1.2ex]
\tensor{\varepsilon}_3^e - \tensor{\varepsilon}_3^{e,\text{tr}} + \Delta \lambda \frac{\partial \bar{\phi}}{\partial \sigma_3} \\[1.2ex]
\bar{\phi}(\hat{\tensor{x}})
\end{bmatrix}
\: \: ; \: \:
\tilde{\vec{x}}
=
\begin{bmatrix}
\tensor{\varepsilon}_1^e \\
\tensor{\varepsilon}_2^e \\
\tensor{\varepsilon}_3^e \\
\Delta \lambda
\end{bmatrix},
\end{equation} 
such that the admissible Cauchy stress tensor at loading step $\text{n}+1$ can be recovered once we obtain the converged set of solutions $\tilde{\vec{x}}$, e.g.,
\begin{equation}
\label{eq:stress_recov}
\tensor{\sigma}_{\text{n+1}}
= 
\mathbb{C}^e : \left[ \sum_{A=1}^{3} \varepsilon^e_A (\vec{n}_A \otimes \vec{n}_A) \right],
\end{equation}
where $\vec{n}_A$ ($A = \lbrace 1,2,3 \rbrace$) indicates the principal direction. 





\section{Results}
\label{sec:num_example}
%
We will discuss and validate different aspects of our introduced interpretable framework for the data-driven discovery of plastic yield surfaces. 
The first example in Section \ref{sec:low_order_NAM} focuses on a pressure-insensitive dataset and examines how the proposed method can discover the symbolic yield surface. We also examine the method's extrapolation capability compared to previous methods in the literature. 
In the second example,  Section \ref{sec:high_order_NAM}, we study the efficacy of the QNM-based symbolic model in dealing with higher-dimensional data of metal plasticity in a five-dimensional space. We also discuss the enforcement of simplicity through sparsity control. The last example in Section \ref{sec:FE_simulation} illustrates how the proposed QNM-based model can discover the correct symbolic equation for pressure-sensitive data. We demonstrate that the found symbolic equations can be readily used in classical FEM codes without significant changes compared to neural network-based plasticity models.


\remark{
In a more general setup as a supervised learning task, in Appendix \ref{appendix:toy_problem}, we compare different modeling assumptions for solving a regression task when dealing with sparse data, considering the problem dimensionality. The limitations of the proposed scheme are discussed in Appendix \ref{appendix:limit}.
}

\remark{
In this work, symbolic regression tasks are carried out using the \texttt{PySR} package \citep{cranmer2020discovering}. Therefore, readers should note that the level of symbolic regression accuracy obtained using other packages or methods may differ.
}


\subsection{Pressure-insensitive elastoplasticity for benchmark performance}
\label{sec:low_order_NAM}
In this example, we use pressure-insensitive data in the stress space, which has three dimensions. However, due to pressure insensitivity, only two dimensions are required to describe the yield surface. Our modeling framework aims to determine whether it can distinguish this independence. We also investigate the impact of spectral layer and data parameterization on training performance. Furthermore, we demonstrate that correct assumptions and inductive biases can improve generalization by conducting stress point integration via the return mapping algorithm.  


The benchmark function where we generate a set of synthetic stress points resembles the von Mises yield criterion. 
While it manifests 
cylinder shape along the hydrostatic axis, the benchmark yield surface is also dependent on the Lode's angle $\theta$ such that it exhibits a flower-shaped cross-section: 
\begin{equation}
\label{eq:benchmark_flower_shape}
f = 
\sqrt{\frac{3}{2}} \rho
\left[
1 + A_p \sin(k_p \theta)
\right] - \sigma_y,
\end{equation}
where the parameters $k_p$ and $A_p$ control the number and the size of petals, respectively, while $\sigma_y$ is the yielding stress. 
From Eq.~\eqref{eq:benchmark_flower_shape}, we choose the parameters as $k_p = 3$, $A_p = 0.325$, and $\sigma_y = 250$ MPa, and then collect a set of stress points that satisfies $f = 0$. 
Specifically, we sample 20 data points along the mean pressure axis (from $-1$ GPa to $1$ GPa) and 120 points along the Lode's angle axis (from $0$ to $2 \pi$), such that total 2,400 different admissible stress states are considered as an original dataset. 
Then the original data points are then pre-processed via the signed distance function by setting $N = 11$, such that the number of stress points in our full dataset is 26,400. 
Here, compared to the previous studies \citep{vlassis2021sobolev, vlassis2022component} where the Lode's radii of the training dataset ranges from 0 to $2 \rho$, as illustrated in Figure \ref{fig:flower_level_set}, our augmented dataset only covers a narrow band region for the original yield surface (i.e., [0.85$\rho$, 1.15 $\rho$]) in order to test the extrapolation capability of our symbolic regression model obtained from the trained NAM. 

% Figure environment removed

Our experiments show that the NAM assumption is sufficient to recover the correct yield surface in this problem. Therefore, we focus on presenting the NAM results and omit the QNM results for brevity. In Fig.\ref{fig:loss_spec_nspec_full_data}, we compare two NAM models trained with the same parameters, one with a Fourier layer and one without, using the full data set. The results show that the network with the Fourier layer achieves higher accuracy with fewer iterations. We also conduct a training process on a random data set,  and the corresponding results are shown in Fig.\ref{fig:loss_spec_nspec_less_data}. The random dataset is a subset of the entire dataset, consisting of 2,000 random points with a zero level-set and 3,000 random points with non-zero level-set values.
In this case, we observe a significant performance difference between the two methods when using a random subset of the full data set.


% Figure environment removed

In Fig.~\ref{fig:loss_parameterization}, we study the effect of input data representation on the learning task using the same neural network architecture for both cylindrical and Cartesian coordinate systems, with the spectral layer utilized. The results suggest that the cylindrical coordinate system can outperform the Cartesian coordinate system, making a difficult training process much easier. Finding an appropriate data representation may become more critical in our proposed framework based on the NAM or QNM, as we have stronger assumptions regarding feature separability compared to classical surrogate modeling methods; our approach is also less flexible than those methods. This is consistent with classical approaches in mechanics, where researchers have introduced different coordinate systems to transform a complex problem into an easier one in the new coordinate system, for example, by taking into account the underlying symmetries in the new coordinate system.

% Figure environment removed

We focus solely on the model trained with the full data set in the cylindrical coordinate system that utilized the Fourier layer. Figure~\ref{fig:flower_shape_funcs} displays the learned shape functions by NAM after training their associated symbolic equations extracted by the symbolic regression algorithm. An advantageous feature of the NAM or QNM modeling idea is its ability to find appropriate univariate, separable representations of complex, multivariate data. This allows each shape function to be visually inspected individually, and even a reasonable symbolic equation can be derived for each univariate data set. In this example, without using any symbolic regression algorithm, it is clear that constant, linear, and sinusoidal functions can describe the NAM shape functions reasonably well. This is one of the primary advantages of using a divide-and-conquer algorithm to break down complexities into more straightforward tasks that can be handled more efficiently by humans.  


The weights associated with each shape function are as follows: $w_p = 0.43$, $w_{\rho} = 5.27$, and $w_{\theta}=3.82$, where these weights are denoted as $w_i$ in Eq.\ref{eq::nam}. Notably, the weight corresponding to the pressure coordinate is about one order of magnitude less than the weights of the other shape functions. Furthermore, as seen in Fig.\ref{fig:flower_shape_funcs}(a), the pressure shape function behaves almost constantly around 1. These observations confirm that the NAM is capable of discarding the effect of pressure coordinate on the final prediction, which is expected since the data is pressure-insensitive.


% Figure environment removed


We apply the symbolic regression algorithm to determine the remaining shape functions, allowing for flexibility in equation forms. The algorithm employs binary and unary operations, including plus, multiplication, division, cosine, exponential, sine, and logarithm. We selected equations with varying complexities and displayed them in Figs.\ref{fig:flower_shape_funcs}(b,c), along with their explicit forms listed in Tables\ref{Tab::petal-symb-eqs-rho} and \ref{Tab::petal-symb-eqs-theta}. 

%

The second shape function $f_2(\bar{\theta})$ exhibits almost the same accuracy with the least complex equation, which is a linear function. It is noteworthy that the other options in Table \ref{Tab::petal-symb-eqs-rho} with higher Complexity scores include a linear term and a sinusoidal function. However, the amplitude of the sinusoidal term is two orders of magnitude smaller than that of the linear term, allowing it to be ignored.

Analyzing the third shape function $f_3(\bar{\theta})$ in Table \ref{Tab::petal-symb-eqs-theta} poses more difficulty as higher complexity levels correspond to significant changes in accuracy. In the context of plasticity applications, accuracy may be more valued than simplicity since the return mapping algorithm requires both function values and their gradients to be accurate and stable. Failure to meet these requirements can lead to the Newton-Raphson algorithm's failure in an implicit method. However, as per Occam's Razor, simplicity's generalization power suggests that simpler models may generalize better \citep{thorburn1918myth}. However, there are no extrapolation concerns for this shape function since it is only defined for the angle $\theta$, which falls within the training data range of zero to $2\pi$. Therefore, we can focus solely on finding a sufficiently accurate function in the interpolation regime. It is worth noting that we already considered Occam's Razor principle at the start by restricting the approximation function to be univariate and separable.

The divide-and-conquer scheme introduced here has an additional advantage in that, in certain cases, one can rely on their intuition to identify the appropriate equation without using the symbolic regression algorithm. This advantage stems from the one-dimensional nature of curve-fitting tasks. For instance, one may hypothesize that the third shape function is a sinusoidal function of the form $f_3(\bar{\theta}) = a \sin( b \pi \bar{\theta}) + c) + d$ and determine the unknown parameters $a$, $b$, $c$, and $d$ through a nonlinear least squares method. In this case, we obtain $f_3(\bar{\theta})= 0.89 \sin(5.95\pi \theta- 0.02) + 0.16$, which is labeled "manual curve fitting" in Fig. \ref{fig:flower_shape_funcs}(c). In terms of the trade-off between complexity and accuracy, one may prefer this equation over those obtained through symbolic regression algorithms, which are, in fact, closer to the ground truth function, Eq.~\eqref{eq:benchmark_flower_shape}. This simple exercise demonstrates that human intuition may outperform symbolic regression algorithms in some cases.

\begin{table}
  \centering
  \caption{Found symbolic shape function $f_2(\bar{\rho})$ for pressure-insensitive benchmark}\vspace{-10pt}
   %\resizebox{\columnwidth}{!}{
        \begin{tabular}{|p{10cm}|c|c|}
        \hline
        Expression & Complexity score & Loss\\[3mm]
          \hline
        & & \\
        $\begin{aligned}f_2(\bar{\rho}) = 1.0 \bar{\rho}\end{aligned}$ & 3 & 5.546e-05\\[5mm] 
        %\hline 
        $\begin{aligned}
        f_2(\bar{\rho}) = \bar{\rho} - 0.01 \sin{\left(\sin{\left(\sin{\left(\bar{\rho} + \sin{\left(\bar{\rho} \right)} \right)} \right)} \right)} \cos{\left(1.32 \bar{\rho} \right)}\end{aligned}$ & 17 & 4.908e-05 \\[5mm]
        %\hline
        $\begin{aligned}
        f_2(\bar{\rho}) =\bar{\rho} - 0.01 \sin{\left(\sin{\left(0.77 \bar{\rho} + \sin{\left(\sin{\left(\bar{\rho} \right)} \right)} + 0.29 \right)} \right)} \cos{\left(1.32 \bar{\rho} \right)}\end{aligned}$ & 21 & 4.894e-05 \\ \hline\end{tabular}
        %}
\label{Tab::petal-symb-eqs-rho} 
\end{table}



%\renewcommand{\arraystretch}{3}
\begin{table}
  \centering
  \caption{symbolic shape function $f_3(\bar{\theta})$ for pressure-insensitive benchmark}\vspace{-10pt}
   %\resizebox{\columnwidth}{!}{
        \begin{tabular}{|p{10cm}|c|c|}
        \hline
        Expression & Complexity score & Loss \\[3mm]
        \hline
      & &   \\
        $\begin{aligned}f_3(\bar{\theta}) = - \sin{\left(4.84 \bar{\theta} \right)}\end{aligned}$& 4 & 9.466e-02\\[5mm] 
        $\begin{aligned}
        f_3(\bar{\theta}) =- \frac{\sin{\left(4.83 \bar{\theta} \right)}}{\cos{\left(\sin{\left(\cos{\left(\sin{\left(\sin{\left(\cos{\left(e^{\bar{\theta}} \right)} \right)} \right)} \right)} \right)} \right)}}\end{aligned}$ & 13 & 2.367e-02\\[10mm]  
        %\hline
        $\begin{aligned}
        &f_3(\bar{\theta}) =
        -1.36 \sin{\left(4.83 \bar{\theta} \right)} + 1.36 \cos(\left(0.86 \sin{\left(4.83 \bar{\theta} \right)} \right.\\ 
        &\left. - 0.86 \cos{\left(0.81 \sin{\left(\sin{\left(4.83 \bar{\theta} \right)} \right)} \right)} + 0.69 \right)) - 1.1\end{aligned}$ & 34 & 3.094e-04 \\[5mm]  \hline\end{tabular}
        %}
\label{Tab::petal-symb-eqs-theta} 
\end{table}

Figure \ref{fig:yield_surf_flower} illustrates that our symbolic regression model (red curve) is capable of reproducing the shape of the benchmark yield function (black curve). 
Here, based on the full dataset, we also train a single multivariate MLP that consists of a number of fully connected layers and Multiply layers \citep{vlassis2021sobolev} to compare the predictive capability against our proposed framework. 
Although a multivariate MLP trained based upon the level-set augmented data can capture the yield surface (blue dots) that is similar to the benchmark, however, it fails to reproduce the stress-strain curve based upon an implicit stress integration scheme [Figure \ref{fig:stress_strain_flower}] due to its limited capacity to make predictions outside the training domain [0.85$\rho$, 1.15$\rho$]. 
On the other hand, as illustrated in Figure \ref{fig:stress_strain_flower}, the symbolic regression model results in a stress-strain curve that is identical to the benchmark, which highlights that it can make accurate predictions outside the range of the training data based upon its extrapolation capacity. 

% Figure environment removed

\subsection{Discovery of symbolic level set plasticity model from porous metal}
\label{sec:high_order_NAM}
In this section, we benchmark the application of the proposed method for finding the plastic yield surface of porous metal material. The data in this problem are in five-dimensional space, including the level-set. We will discuss equation discovery under sparsity control.  
% 

In this section, we chose a model that discovered by \citet{bomarito2021development}, which describes the plastic behavior of a porous material depending on the hydrostatic pressure $\bar{\sigma}_h$, the von Mises stress $\bar{\sigma}_{vm}$, the volume-averaged Lode parameter $\bar{L} = 3 \sqrt{3} (\sigma_1 - \bar{\sigma}_h) (\sigma_2 - \bar{\sigma}_h) (\sigma_3 - \bar{\sigma}_h) / (2 J_2^{3/2})$, and a parameter $\bar{v}$ that describes the void fraction. 
To generate the training data, we adopt the expression for the yield function that can be found in Eq. (48) in \citep{bomarito2021development}, and sample 20 data points along the $p$-axis (from 0 to 1.8 MPa), 30 points along the $\theta$-axis (from 0 to 2$\pi$) based on the cylindrical coordinate system, and 10 points along the $\bar{v}$-axis (from 0.063 to 0.065) such that total 6,000 different admissible stress states are considered. 
Here, we add uniformly distributed noise along the radial direction where its magnitude ranges from $-4$ \% to $4$ \% of Lode's radius to test the performance of the model trained by a dataset that may possibly contain noises. 
Then, similar to the previous example, the original dataset is then pre-processed via level-set augmentation by setting $N = 11$ that covers a narrow band region of [0.85$\rho$, 1.15$\rho$] such that our full dataset consists of 66,000 stress points. 


Figure~\ref{fig:bomarito_lo_vs_ho} displays the yield surfaces found at different levels of hydrostatic stress and void volume fraction using the NAM and QNM methods. Both methods accurately represent the underlying yield surfaces, but the QNM method performs slightly better, especially at higher levels of hydrostatic stress, due to its higher level of flexibility. The QNM results shown in this figure were obtained by training the model with sparsity control, with $\alpha_{\text{lo}} = 0.01$ and $\alpha_{\text{ho}} = 0.001$, see Eq.~\ref{eq::sparse_loss}. The QNM method uses four learnable shape functions, each with an associated learnable weight ($w_1$, $w_2$, $w_3$, and $w_4$). The complete quadratic approximation based on these four shape functions has ten additional terms that are controlled by trainable weights ($w_{ij}$) for $1\le i \le j \le 4$.

In Figure~\ref{fig:sparsity_control}, we can see how the weights change during training when sparsity is enforced compared to when it is not. When sparsity control is used, all of the lower-order contributions (shown by different colors) eventually diminish, with $w_i$ approaching zero in later epochs, improving the model's simplicity and interpretability. Notably, this is consistent with the benchmark equation, which features couplings between multiple features and does not involve any single-variable term.


% Figure environment removed


% Figure environment removed



%

Tables \ref{Tab::porous-symb-eqs-sparse} and \ref{Tab::porous-symb-eqs-nosparse} report the corresponding discovered symbolic equations generated by the symbolic regression algorithm. In this study, we deliberately chose the best function with the least loss function to validate our modeling performance for unseen data in the interpolation regime - where data falls inside the convex hull of the train data but was not seen during training. To this end, we plot the yield surface at two different levels in Figure \ref{fig:sparsity_control_yield_surf}. The results suggest that the modeling assumption in Equation \eqref{eq::qnm}, in terms of separability, may be sufficiently robust to avoid overfitting, at least in the interpolation regime.  



%

\begin{table}
  \centering
  \caption{symbolic shape functions found with sparsity promoting loss constraint for porous metal}\vspace{-10pt}
   %\resizebox{\columnwidth}{!}{
        \begin{tabular}{|p{10cm}|c|c|}
        \hline
        shape function & Complexity score & Loss \\[3mm]
        \hline
        & & \\
        $\begin{aligned} 
        f_1(\bar{\sigma_h}) = \bar{\sigma_h} - \sin{\left(0.07 \left(\bar{\sigma_h} + \exp({\bar{\sigma_h}})\right) \cos{\left(0.96 \bar{\sigma_h} + 0.43 \right)} \right)}
        \end{aligned}$ & 19 & 4.691e-5 \\[5mm]
        %\hline
        $\begin{aligned}
        &f_2(\bar{\sigma}_{vm}) = \bar{\sigma}_{vm} - \left(\bar{\sigma}_{vm} + \cos{\left(\sin{\left(\bar{\sigma}_{vm} \right)} - 0.07 \right)}\right)\\ &\quad \sin{\left(0.15 \sin{\left(\bar{\sigma}_{vm} - 1.01 \right)} \right)}
        \end{aligned}$ & 19 & 4.815e-5 \\[5mm]
        %\hline
        $\begin{aligned}
        &f_3(\bar{\sigma}_{L}) = \bar{\sigma}_{L} \left(\sin(\left(\sin(\left(\sin(\left(0.56 \bar{\sigma}_{L} + 0.56 \cos(\left(\sin( \right.\right.\right.\right.\right.\\
        & \left. \left.\left. \left.\left.  \left(\bar{\sigma}_{L} + \cos(\left(1.0 \sin{\left(\sin{\left(\sin{\left(\bar{\sigma}_{L} \right)} \right)} + 0.99 \right)} \right)) \right)) \right)) \right)) \right)) \right)) - 1.19\right) - 0.34
        \end{aligned}$ & 33 & 2.113e-04 \\[5mm]
        %\hline
        $f_4(\bar{v}) = \begin{aligned}1.04 \bar{v} + \sin{\left(1.41 \exp({- 0.46 \bar{v}}) \sin{\left(\cos{\left(\bar{v} \right)} \right)} \right)} - 0.61\end{aligned}$ & 33 & 5.206e-04 \\[1mm] \hline
        \end{tabular}
        %}
\label{Tab::porous-symb-eqs-sparse} 
\end{table}



\begin{table}
  \centering
  \caption{symbolic shape functions found without sparsity promoting loss constraint for porous metal}\vspace{-10pt}
   %\resizebox{\columnwidth}{!}{
        \begin{tabular}{|p{10cm}|c|c|}
        \hline
        shape function & Complexity score & Loss \\[3mm]
        \hline
        & & \\
        $f_1(\bar{\sigma_h}) = \begin{aligned}\bar{\sigma_h} - 0.29 \sin{\left(\exp({0.76 \bar{\sigma_h}}) \right)} + 0.17\end{aligned}$ & 17 & 4.364e-05 \\[5mm]
        %\hline
        $
        \begin{aligned}
        f_2( \bar{\sigma}_{vm} ) = 
        &\bar{\sigma}_{vm} + \left(1.47 \bar{\sigma}_{vm} + 0.9\right)
        \left(0.04 \cos{\left(\bar{\sigma}_{vm} \right)}
        + 0.04 \cos{\left(\bar{\sigma}_{vm} + 0.9 \right)}\right)
        \end{aligned}$        & 21 & 8.601e-05  \\[5mm] 
        %\hline
        $\begin{aligned}
        f_3(\bar{\sigma}_{L}) = 
        &\left(\bar{\sigma}_{L} \left(\sin{\left(0.48 \bar{\sigma}_{L} \right)} - 0.03 \cos{\left(\bar{\sigma}_{L} \right)} - 1.2\right) - 0.48\right)\\
        & \cos{\left(\sin{\left(\cos{\left(0.58 \bar{\sigma}_{L} + 0.03 \cos{\left(\bar{\sigma}_{L} \right)} \right)} \right)} \right)}\end{aligned}$ & 36 & 2.003e-05 \\[5mm] 
        %\hline
        $\begin{aligned}
        &f_4(\bar{v}) = 
        \bar{v} + \cos(\left(\bar{v} + \cos(\left(\bar{v} \right.\right.\\
         & \left(- 0.57 \bar{v} \cos(\left(\sin(\left(\sin(\left(\cos(\left(2 \bar{v} + \right.\right.\right.\right.\right. \\
         &\left.\left.  \left.\left. \left.\left. \left.  \cos{\left(\bar{v} \cdot \left(0.63 \bar{v} \cos{\left(\sin{\left(\bar{v} \right)} \right)} + 0.63 \cos{\left(\bar{v} \right)}\right) \right)}- \right.\right.\right.\right.\right.\right.\right.\\
          & \left. \left. \left. \left. \left. \left. \left.  0.57 \right)) \right)) \right)) \right)) - 0.57 \cos{\left(\bar{v} \right)}\right) \right)) - 0.57 \right)) - 0.57
        \end{aligned}$ 
        & 42 & 5.971e-04 \\[1mm] \hline
        \end{tabular}
        %}
\label{Tab::porous-symb-eqs-nosparse} 
\end{table}






% Figure environment removed



In Appendix \ref{appendix:symb_reg_comparison}, we conduct a comparison with the brute-force symbolic regression approach directly applied to the data, highlighting the interpretability advantages of our proposed scheme. 


\remark[Model and training setup]{
Each utilized MLP consists of one Fourier layer with 20 randomly selected frequencies, followed by three additional hidden layers with 40, 20, and 20 hidden units, respectively. The hidden and output activation layers are \texttt{ReLU} and \texttt{Tanh} layers. Penalty factors are set to $\alpha_{\text{lo}} = 0$ and $\alpha_{\text{ho}} = 0.01$. We set the initial learning rate to 0.005 and continued training for 22,000 epochs. While these hyperparameters were determined through manual trial and error, they are not necessarily optimal.
}




\subsection{Applications in finite element simulations}
\label{sec:FE_simulation}
In this problem, we illustrate how our end-to-end framework can be used to discover symbolic equations for the plastic yield surface, which can then be directly incorporated into finite element simulations. Through this example, we will demonstrate how the QNM approach, with its greater flexibility in modeling assumptions, can lead to more appropriate and simpler symbolic equations compared to the NAM approach. 


Our benchmark material model to be replicated via QNM is the Matsuoka-Nakai criterion \citep{matsuoka1974stress}:
\begin{equation}
\label{eq:benchmark_MatsuokaNakai}
f = 
- (I_1 I_2)^{1/3} + (\beta I_3)^{1/3},
\end{equation}
where the stress invariants are defined as: $I_1 = \sigma_1 + \sigma_2 + \sigma_3$, $I_2 = \sigma_1 \sigma_2 + \sigma_2 \sigma_3 + \sigma_3 \sigma_1$, and $I_3 = \sigma_1 \sigma_2 \sigma_3$, while the material parameter $\beta$ depends on the friction angle $\phi_f$:
\begin{equation}
\label{eq:benchmark_MatsuokaNakai_phi}
\beta = \frac{9 - \sin^2\phi_f}{1 - \sin^2\phi_f}.
\end{equation}
By setting the friction angle to be $\phi_f = 30^{\circ}$, we collected total 13,200 stress points as a training dataset. 
Specifically, we sampled 20 points along the $p$-axis from 0 to 1,000 MPa, 60 points along the $\theta$-axis from 0 to 2$\pi$, while choosing $N=11$ from 0.85$\rho$ to 1.15$\rho$.  

% Figure environment removed

The shape functions learned through NAM and QNM are presented in Figs.\ref{fig:MN_fp}-\ref{fig:MN_ftheta}. The yield surfaces discovered using these methods, as shown in Fig.\ref{fig:MN_yield_surf}, are in good agreement with the benchmark. However, the shape functions learned through NAM in Figs.~\ref{fig:MN_fp}-\ref{fig:MN_ftheta} exhibit greater complexity and noise, particularly for pressure and radius. This is not surprising, given that the NAM model is unable to account for interactions between input features and, therefore, may increase the complexity of each shape function to improve overall flexibility in capturing the target response.


The QNM learned is expressed as $f = 1.39 f_1(\bar{p}) + 2.18 f_2(\bar{\rho}) + 0.24 f_3(\bar{\theta}) - 0.22 f_1(\bar{p}) f_3(\bar{\theta})$. All other second-order interactions among the shape functions are nearly zero, except for $f_1(\bar{p}) f_3(\bar{\theta})$. The linear dependence found with pressure (as seen in Fig.~\ref{fig:MN_fp}) and the form of the equation obtained are consistent with the benchmark. This demonstrates the QNM's ability to uncover interpretable relationships among different features and the underlying functional form, which can be useful for the second step of the symbolic regression algorithm.  Table \ref{Tab::theta-MN-symb} summarizes the results of the symbolic regression for the shape function $f_3({\bar{\theta}})$.  The last row in this table is used for the finite element analysis.

\begin{table}
  \centering
  \caption{symbolic shape functions found for $f_3(\bar{\theta})$ in case of pressure-sensitive material}\vspace{-10pt}
        \begin{tabular}{|p{10cm}|c|c|}
        \hline
        Expression & Complexity score & Loss\\[3mm]
         \hline
        & & \\
        %\hline
        $\begin{aligned}
        f_3(\bar{\theta}) = - 1.35 \sin{\left(4.78 \bar{\theta} \right)}\end{aligned}$ & 6 & 5.286e-02\\[5mm]
        %\hline 
        $\begin{aligned}
        f_3(\bar{\theta}) = \frac{0.15 - \sin{\left(4.79\bar{\theta} \right)}}{\cos{\left(\cos{\left(2.38 \bar{\theta} - 0.87 \right)} \right)}}\end{aligned}$ & 18 & 1.143e-02 \\[5mm] 
        %\hline 
        $\begin{aligned}
        f_3(\bar{\theta}) = \frac{0.13 - \sin{\left(4.79 \bar{\theta} + 6.19 \right)}}{\cos{\left(1.04 \cos{\left(2.38 \bar{\theta} - \cos{\left(\sin{\left(\sin{\left(2.38 \bar{\theta} \right)} \right)} \right)} \right)} \right)}} + 0.03\end{aligned}$ & 31 & 2.928e-03\\[1mm] 
        \hline \end{tabular}
\label{Tab::theta-MN-symb} 
\end{table}


\noindent

We now incorporate the obtained QNM-based symbolic expressions in a boundary value problem solved via the finite element method to showcase the applicability of our proposed approach. 
Specifically, as illustrated in Figure \ref{fig:geometry_and_bcs}, we consider a 20 mm $\times$ 20 mm rectangular plate that is weakened by a circular hole of a radius of 5 mm at its center. 
For simplicity, we limit our attention to a two-dimensional case by assuming plane strain condition while only considering its upper right quarter of our problem domain. 
Our domain of interest is spatially discretized with a mesh that consists of total 871 triangular elements that have one integration point each. 
By assuming that our target material behaves linearly in the elastic regime and setting Young's modulus $E = 25$ GPa and Poisson's ratio $\nu = 0.3$, we conduct a finite element simulation under a displacement-controlled regime by prescribing a vertical displacement $\hat{\vec{u}}$ at a rate of $-0.1$ mm/sec on the top, while imposing a 100 MPa compressive traction along the inner radii and the right-hand side of the domain as confinement. 

% Figure environment removed

Figures \ref{fig:fe_von_mises} and \ref{fig:fe_plas_strain} compare the von Mises stress and the accumulated plastic strain contours obtained from the (a) benchmark and the (b) QNM-based symbolic expressions at $\hat{u}_y = -0.04$ mm, $-0.06$ mm, $-0.08$ mm, and $-0.1$ mm, respectively. 
We observe that the plastic strain first accumulates at the right-hand side of the perforation, where stresses are concentrated and evolves towards the upper right part of the domain of interest, such that it forms a localized pattern. 
Therefore, the stress history recorded at point B near the region where the accumulated plastic strain is localized exhibits a higher level of von Mises stress compared to point A, as illustrated in Figure \ref{fig:stress_evolution}. 
More importantly, the finite element analysis based upon the QNM-based symbolic regression replicates the classical finite element simulation with a benchmark material model, highlighting that our approach is not only capable of discovering the mathematical expression of the yield function from the given set of data without a priori knowledge but also easily replace the constitutive model for continuum-scale simulations. 


% Figure environment removed

% Figure environment removed

% Figure environment removed





%
\section{Discussions}
 In this section, we conduct additional numerical experiments to benchmark the performances of the proposed models against other state-of-the-art approaches. 

\subsection{Comparisons with the direct symbolic regressions}  
\label{appendix:symb_reg_comparison}
In this study, we compare the results obtained using our proposed two-step symbolic regression framework to those obtained by applying brute-force single-step symbolic regression directly to the multivariate dataset. 

The total CPU time required for training QNM is approximately 83 minutes. Each univariate symbolic regression process took around one minute. Therefore, the estimated total computational time for our two-step framework is approximately 87 minutes. In contrast, when applying the same configuration used for the univariate symbolic regressions to the direct multivariate SR, the SR algorithm finds $\phi_1 = 1.88 \sin{\tilde{\phi}_1}$ in approximately 43 minutes where, 
%
\begin{align}
\begin{split}
\tilde{\phi}_1 = 
    & \bar{\sigma}_{vm} \sin{\left(\sin{\left(\sin{\left(0.41 \bar{\sigma}_{vm} + 0.41 \cos{\left(\sin{\left(\frac{1.08 \sin{\left(0.24 \bar{\sigma}_{vm} \bar{v} \right)} \cos{\left(0.69 \bar{v} \right)}}{\bar{v}} \right)} - 0.16 \right)} \right)} \right)} \right)} \\
    & + \frac{\bar{\sigma}_h + \bar{\sigma}_{vm} + \frac{\bar{v} + \sin{\left(\bar{L} \right)}}{2.85 \bar{v} + 9.4} + \sin{\left(0.22 \sin{\left(\cos{\left(\bar{L} \cos{\left(\cos{\left(\cos{\left(\bar{\sigma}_h + \sin{\left(\bar{L} + 0.86 \right)} \right)} \right)} \right)} \right)} \right)} - 0.33 \right)}}{\cos{\left(\cos{\left(\sin{\left(\cos{\left(\bar{\sigma}_h + 0.73 \right)} \right)} \right)} \right)}}.
\end{split}
\end{align}
%complexity is 92 and loss 0.1
%
If the SR algorithm is allocated more time, it discovers $\phi_2 = 1.94 \sin{\tilde{\phi}_2}$ within approximately 3.4 hours where,
%
\begin{multline}
    \tilde{\phi}_2 = 
    1.17 (\bar{\sigma}_h + \bar{\sigma}_{vm}) + 0.19 (\bar{v} + \cos(\bar{\sigma}_{vm}) + \cos(\bar{v})) + 0.17 e^{\bar{\sigma}_h} + \sin{\left(\bar{\sigma}_{vm} - 0.95 \right)} +\\
    0.17 \sin{\left(\bar{\sigma}_{vm} + \left(\bar{L} + 1.11\right) \sin{\left(\bar{\sigma}_h + \sin{\left(\cos{\left(\bar{\sigma}_{vm} - 0.71 \right)} \right)} \right)} \right)} +0.17 \sin{\left(\bar{L} + \sin{\left(\cos{\left(\bar{L} + 0.17 \right)} \right)} \right)} - 0.14.
    \end{multline}
%
The training MSE values for $\phi_1$ and $\phi_2$ are 0.100 and 0.093, respectively.  The RMSE values for random test data (not seen in train data) in each case are 0.0144 and 0.0082, respectively.  However,  RMSE for the proposed two-step method is 0.0070, slightly better than both achieved with less execution time.


%

$\phi_2$ is more desirable compared to $\phi_1$ in terms of simplicity. However,  both of them may be less desirable in terms of interpretability compared to the proposed two-step framework.  
%
Since the contribution of each variable in the final yield surface is less apparent.  Additionally,  it is unclear how to simplify this equation and reduce its complexity, which is easily achievable in our framework, as discussed in Section~\ref{sec:low_order_NAM}. 
%

Figure \ref{fig:direct_comp_yield_surf} demonstrates that QNM-based symbolic regression provides a higher accuracy representation of the target yield surface. This may be due to the proposed divide-and-conquer approach, which has the potential to break down complex learning objectives into simpler ones, possibly resulting in improved learning outcomes.

% Figure environment removed



\remark{
Directly comparing the computational time between the proposed method and the direct SR method in this manner may not provide a comprehensive analysis. It should be noted that the proposed method utilizes both \texttt{PySR} and \texttt{PyTorch}, which are developed by different groups of developers and optimized for different purposes. On the other hand, the direct SR method solely relies on \texttt{PySR}. Thus, due to the differences in the underlying packages and their optimizations, a direct time comparison may not accurately reflect the performance of each method.
}

\subsection{Comparisons among different methods for sparse data}
\label{appendix:toy_problem}
In this example, we evaluate the effectiveness of various methods for a regression task involving input features with four dimensions, where the data is relatively sparse.

In this study, we create a regression task with predetermined shape functions, such as polynomial, exponential decay, and multiscale sinusoidal, to assess the method's effectiveness in capturing various shape functions with distinct characteristics. Additionally, we intentionally exclude one of the input features ($x_4$) in the data generation process to evaluate the method's ability to identify irrelevant features. The data is generated as follows:
\begin{align}
&f_1(x_1) = 3(x_1^3 - x_1), \\
&f_2(x_2)= \frac{1}{x_2 + 1.2}, \\
&f_3(x_3)= 1.5
\left(
-x_3^2 + 0.3 \sin(10\pi x_3) + 0.4
\right
),\\
&f(x_1, x_2, x_3, x_4) = f_1(x_1) + 0.25 f_2(x_2) f_3(x_3) + \mathcal{N}(0, 0.1),
\end{align}
where input variables $x_i$ are sampled randomly from a uniform distribution over the interval $[-1,1]$.  The size of each of the randomly generated training and test datasets is 500 data points.  Note that the shape function $f_3$ exhibits parabolic behavior at the coarse scale and sinusoidal behavior at the fine scale, as shown in Fig. \ref{fig:sparse-toy-qnm-shapes}(c).


% Figure environment removed


Figures \ref{fig:sparse-toy-errors}(b-c) display the residuals ($y_{\text{true}} - y_{\text{pred}}$) of the models' predictions using different methods. Figure \ref{fig:sparse-toy-errors}(a) reports the mean squared errors of the predictions during the training process for NAM, QNM, and the vanilla single MLP. The training error for the single MLP method is almost zero, but it performs poorly on the test data, as shown in Figure \ref{fig:sparse-toy-errors}(c). This is expected because 500 data points are too sparse for a four-dimensional response surface without any inductive bias. In contrast, NAM and QNM show better generalization than the single MLP since their model assumptions have a more appropriate bias-variance tradeoff and stronger compatibility with the underlying data generation process.
Furthermore, QNM outperforms NAM slightly in terms of residual errors and train mean squared error, which is expected due to its higher flexibility and structural assumptions fully compatible with the data.

The quadratic expression of QNM contains non-zero terms, with $w_1 \approx 0.38$ and $w_{23} \approx 0.32$. This means that the structural model discovered by QNM can be written as $\bar{f}_{QNM}(x_1, x_2, x_3) \approx 0.38 \bar{f}_1(x_1) + 0.32 \bar{f}_2(x_2) \bar{f}_3(x_3)$, where $\bar{f}_i$ are learned shape functions. QNM was able to identify the underlying data generation process and discard the irrelevant feature $x_4$.  Interestingly, QNM accurately captured even the complex, multiscale sinusoidal shape function $f_3(x_3)$. While marginal errors can be observed in $f_2(x_2)$, it effectively reflects the exponential decay behavior.

In contrast, NAM identified all terms as non-zero, resulting in the following structural equation: 
%
\begin{equation}
\bar{f}_{NAM}(x_1, x_2, x_3, x_4) = 0.43 \bar{f}_1(x_1) + 0.51 \bar{f}_2(x_2) + 0.58 \bar{f}_3(x_3) + 0.48 \bar{f}_4(x_4).
\end{equation}
%
Although NAM and QNM perform similarly in terms of train and test errors, NAM's learned structural equation is misleading. Not only does the irrelevant feature $x_4$ contribute to the model, but its effect is even higher than that of feature $x_1$ ($w_4$ is higher than $w_1$). This can lead to misinterpretation and confusion regarding causality. One possible explanation for this behavior is that, since NAM cannot incorporate interactions among features, it attempts to use $x_4$ as an additional degree of flexibility to minimize prediction loss during training. The learned shape functions are shown in Figure \ref{fig:sparse-toy-nam-shapes}, where only the polynomial shape function $f_1(x_1)$ is discovered by the model.


% Figure environment removed



% Figure environment removed


Figure \ref{fig:sparse-toy-errors} additionally shows the results for two symbolic regression models: ``SR-UniVar'' and ``SR-MultiVar''. The former corresponds to the model obtained by performing symbolic regression on the shape functions learned by the QNM, while the latter is the vanilla multivariate symbolic regression directly performed on the data. While SR-MultiVar does not have the least amount of error in the train data, its performance is comparable to that of SR-UniVar. The symbolic representation discovered by SR-MultiVar is presented below:
%
\begin{equation}
\frac{- \sin{\left(1.77 x_{1} + 0.17 \right)} + \frac{0.07 e^{- x_{2}} \cos{\left(1.4 x_{3} \right)}}{\sin{\left(\cos{\left(\cos{\left(\frac{e^{x_{3}}}{x_{3}} \right)} - 0.09 \right)} \right)}}}{\cos{\left(\cos{\left(\cos{\left(\sin{\left(x_{1} \right)} \right)} \right)} \right)}}.
\end{equation}
%
The symbolic representation discovered by SR-MultiVar is fully transparent, and it clearly discards the contribution of the irrelevant feature $x_4$. This is an essential ingredient for model interpretability. However, the equation itself needs to provide an easy way to uncover the underlying data generation process, making it less interpretable than the proposed divide-and-conquer scheme.




\section{Conclusions}
\label{sec:conc}
We introduce a framework that combines the expressivity of the neural network and the interpretability of the symbolic regression 
to yield a plasticity model that can be expressed analytically while 
(1) achieving the necessary accuracy for engineering applications
and (2) overcoming the technical barrier of symbolic regression.
In particular, we take advantage of the divide-and-conquer nature of the feature generation step of the proposed neural quadratic method such that a series of one-dimensional symbolic regression 
for the basis of the feature space may replace the NP-hard 
high-dimensional symbolic regression problem. 
The proposed machine learning tool is tested against synthetic data 
with a known analytical yield function that is not convex, as well as 
a data set for porous metal with no known analytical solution. 
In both cases, we found that the proposed method is feasible to train, and the generated model is capable of discovering yield functions with superior accuracy than those obtained from the classical neural additive model. To ensure third-party validation, the source code is open-sourced. 


\section*{Acknowledgments}
\label{sec:acknowledgement}
The authors are supported by the National Science Foundation under grant contracts CMMI-1846875 and the Dynamic Materials and Interactions Program from the Air Force Office of Scientific Research under grant contracts FA9550-21-1-0391 with the major equipment supported by FA9550-21-1-0027, with additional funding from the Department of Energy DE-NA0003962 and the UPS Foundation Visiting Professorship at Stanford. 
These supports are gratefully acknowledged. The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the sponsors, including the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein. The views and conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the sponsors, including the Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.

\section*{CRediT authorship contribution statement}
Bahador Bahmani: Conceptualization, Methodology, Software, Validation, 
Formal analysis, Investigation, Data Curation, Writing – Original Draft. Hyoung Suk Suh: Conceptualization, Methodology, Software, Validation, 
Formal analysis, Investigation, Data Curation, Writing – Original Draft. WaiChing Sun: Conceptualization, Methodology, Investigation, Validation, Resource, Writing – Original Draft, Supervision, Project administration, Funding acquisition. 


\begin{appendices}
\renewcommand{\theequation}{A\arabic{equation}}
\setcounter{equation}{0}



\section{Limitations of QNM for non-smooth cases}
\label{appendix:limit}
For completeness, we also present a regression example that demonstrates NAM and QNM's inability to achieve the desired level of accuracy.
We synthesized data using the following function:
%
\begin{equation}
f(x_1, x_2) = |x_1 - x_2| + |x_1 + x_2|,
\end{equation}
%
This function represents a pyramid, which is depicted in Figure~\ref{fig:failure-data}(a). We obtained prediction results using two models: QNM and NAM. These results are presented in Figures~\ref{fig:failure-data}(b-c). However, since both models rely on restricted modeling assumptions by controlling the amount of possible interactions among input features, they may not be able to represent complex tasks that require higher order interactions among features. To improve the expressivity necessary for this non-smooth learned function, we can extend the QNM to higher-order polynomials or introduce specific enrichment function (manually or through machine learning) in the feature space to handle the sharp gradient. 
Alternatively, one may also construct feature space locally for coordinate charts that constitute a yielding manifold \citep{xiao2022geometric}. 
These potential improvements are out of the scope of this study but will be considered in the future. 
We included the prediction residuals of these models in Figure \ref{fig:failure-err} for completeness.

% Figure environment removed

% Figure environment removed



\end{appendices}


\bibliographystyle{plainnat}

\bibliography{main}

\end{document}