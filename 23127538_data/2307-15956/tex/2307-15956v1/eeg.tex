
%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for the instructions to authors using
% the LaTeX document class 'llncs.cls' for contributions to
% the Lecture Notes in Computer Sciences series.
% http://www.springer.com/lncs       Springer Heidelberg 2006/05/04
%
% It may be used as a template for your own input - copy it
% to a new file with a new name and use it as the basis
% for your article.
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[runningheads,a4paper]{llncs}

\usepackage[T1]{fontenc}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{subfigure}
%\usepackage[caption=false]{subfig}
\usepackage{tabularx}
\usepackage{graphicx,wrapfig,lipsum}

\usepackage{epsfig}
\usepackage{subcaption}

%\setcounter{secnumdepth}{3}
%
%\renewcommand\subsubsection{\@startsection{subsubsection}{3}{\z@}%
%                       {-18\p@ \@plus -4\p@ \@minus -4\p@}%
%                       {-0.5em \@plus -0.22em \@minus -0.1em}%
%                       {\normalfont\normalsize\bfseries\boldmath}}


\usepackage{url}
\urldef{\mailsa}\path|{alfred.hofmann, ursula.barth, ingrid.haas, frank.holzwarth,|
\urldef{\mailsb}\path|anna.kramer, leonie.kunz, christine.reiss, nicole.sator,|
\urldef{\mailsc}\path|erika.siebert-cole, peter.strasser, lncs}@springer.com|    
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\begin{document}

%\mainmatter  % start of an individual contribution
%
%% first the title is needed
\title{Analyzing Cryptocurrency trends using Tweet Sentiment Data and User Meta-Data}

%% a short form should be given in case it is too long for the running head
%\titlerunning{Lecture Notes in Computer Science: Authors' Instructions}
%
%% the name(s) of the author(s) follow(s) next
%%
%% NB: Chinese authors should write their first names(s) in front of
%% their surnames. This ensures that the names appear correctly in
%% the running heads and the author index.
%%
\author{Samyak Jain \inst{1} \and Sarthak Johari \inst{1}
%\thanks{Please note that the LNCS Editorial assumes that all authors have used
%the western naming convention, with given names preceding surnames. This determines
%the structure of the names in the running heads and the author index.}%
\and Radhakrishnan Delhibabu \inst{2}}
%
\authorrunning{Samyak Jain et al.}
\titlerunning{Analyzing Cryptocurrency}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{Computing Science and Engineering, Indraprastha Institute of Information Technology, New Delhi \and
School of Computing Science and Engineering, Vellore Institute of Technology,  Vellore
%\url{http://www.springer.com/lncs}
}

%%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%
%
\toctitle{Lecture Notes in Computer Science}
%\tocauthor{Authors' Instructions}
\maketitle


\begin{abstract}
Cryptocurrency is a form of digital currency using cryptographic techniques in a decentralized system for secure peer-to-peer transactions. It is gaining much popularity over traditional methods of payments because it facilitates a very fast, easy and secure way of transactions. However, it is very volatile and is influenced by a range of factors, with social media being a major one. Thus, with over four billion active users of social media, we need to understand its influence on the crypto market and how it can lead to fluctuations in the values of these cryptocurrencies. In our work, we analyze the influence of activities on Twitter, in particular the sentiments of the tweets posted regarding cryptocurrencies and how it influences their prices. In addition, we also collect metadata related to tweets and users. We use all these features to also predict the price of cryptocurrency for which we use some regression-based models and an LSTM-based model.
\keywords{Cryptocurrency, LSTM, Tweet Sentiment Data, Linear Regression, SGD Regressor, Random Forest Regressor, Principal Component Analysis, Mean Absolute Error, Root Mean Squared Error, Maximum Percentage Error. }
\end{abstract}

\section{INTRODUCTION}
\subsection{MOTIVATION}
With the digitization of the world and the market, most of the financial operations are moving to the digital space. Cryptocurrency has emerged as a secure form of currency that allows end-to-end secured transactions. It has also emerged as a form of a financial asset just like traditional stocks in the stock market. 
However, the cryptocurrency exchange is an extremely volatile market that operates very differently compared to the traditional market. While traditional markets use technical indicators for calculating price fluctuations, the prices and valuations in cryptocurrency can be influenced by a wide range of factors ranging from the demand-supply balance, legal and regulatory factors, to the sentiments about it in news and social media. It has been clearly observed in the case of many popular virtual currencies that their prices can fluctuate heavily just on the basis of related activity on popular social media platforms like {\itshape Twitter, Facebook, etc}. 
Thus, our motivation for this research is to analyze the value fluctuations of cryptocurrencies from each bracket of market capitalization based on tweet sentiment analysis and use the user metadata for these tweets. We will analyze 2 coins from the large-cap like {\itshape Solana and Avalanche}, and 3 coins from the mid-cap range like {\itshape DogeCoin, Matic, and Shiba Inu}.

% Figure environment removed

% Figure environment removed

\subsection{PROBLEM STATEMENT}
Our project aims to capture the sentiment of the text in the tweet to analyze the correlation between the price and sentiment of the cryptocurrency, and finally use that sentiment along with tweet metadata to conclude about the fluctuation in its price. We then use this sentiment coupled with user metadata as a combined feature to then predict the price of the cryptocurrency.

Our tasks include firstly collecting the relevant twitter data for the cryptocurrencies and the corresponding cryptocurrency values. Given a set of tweets that are related to a cryptocurrency coin, we would like to find the associated sentiment for that coin by performing sentiment analysis for the tweet text and then analyze the fluctuations in their value which occur in the near future as a result of these sentiments. We map the sentiment to a sentiment score ranging between zero and one which reflects the strength of the sentiment(positive/negative/neutral) and combine this with the tweet metadata to use the whole as a combined feature in our model for concluding about the fluctuation in the cryptocurrency’s value. Also, we would separately analyze this effect for both mid-cap range and high-cap range cryptocurrencies which differ in their market values.

The problem we are trying to solve is novel compared to the work previously done in this research area as we are not only focusing on the sentiment of the tweet but also incorporating user metadata and tweet data along with it to provide a deeper insight into user data and cryptocurrency fluctuation. The metadata includes details like count of followers, verification status, the number of likes, retweets etc. Including this as features in our models can provide better results which is because in terms of social media, the influence and effect created by the Twitter activity is very much dependent on the user who performs that as well (whether they are famous or influential or not). We are also trying to find out how high-cap range and mid-cap range coins are affected by the tweets and if the impact of a tweet is similar in both these cases.

% Figure environment removed

\subsection{MAJOR CONTRIBUTIONS}

Following are some major contributions of our work on this topic :
\begin{enumerate}
    \item We have collected the tweets and related data along with the price related data for two large-cap and three mid-cap cryptocurrencies. We ourselves have curated this dataset and released it for further use.

    \item We perform sentiment analysis of the tweet text to understand the context, and opinion by analyzing the polarity of the text. For this, we use RoBERTA based pre-trained model and finetuned the same. We tried to also unfreeze some layers of the already pretrained model while finetuning and then use the model to generate sentiment scores that are further used for our prediction and analysis.

    \item We analyze the effect of the sentiment of the tweets on the corresponding cryptocurrency price.

    \item We also use the sentiment analysis based features along with collected metadata related to the tweet and user to perform the task of price prediction for these cryptocurrencies. For this, we use regression-based models (linear regression, SGD regression and Random Forest regressor) and also to include a sense of time and sequence in our model we use LSTM to model this task as a time-series forecasting.

    \item We have tried out various approaches to train the LSTM based models with and without metadata. Also, we have accommodate the metadata after calculating the sentiment for a certain period of time and then weighing the sentiment by the number of retweets and likes based so as to capture the variation of sentiment and also metadata.
\end{enumerate}

\section{Literature Review}

[1] The paper predicts changes in Bitcoin and Ethereum (the two largest cryptocurrencies) prices using Twitter  data and Google trends. As Twitter is used widely as a news source and judging popularity, they influence the purchase/sell decisions of the user. They used three models to find the correlation with the cryptocurrency's price.  First, they collected tweets from twitter’s API using tweepy. After cleaning the collected tweets, they analyzed using the VADER\\(Valence Aware Dictionary for sEntiment Reasoning) sentiment analysis. Then they analyzed if the tweets actually have a sentiment or not and then established a relation between the sentiments of tweets with the price change of cryptocurrency. They found a positive correlation of prices with the sentiments when the price was rising. To have a better model input, they also considered the tweet volumes and used it as a metric to see the price fluctuation. They concluded that the relationship is robust to periods of high variance and non-linearity. With these inputs, a multiple linear regression model accurately reflected future price changes with the addition of lagged variables.

[4] This paper explores the use of social sentiment data as a better predictor as compared to the traditional methods of using technical financial indicators and uses the non-linear relation between sentiments and bitcoin price to predict prices in the future.\\
 
 \textbf{TRMI index construction} \\
        
    For this research, they have used an index called as Thomson Reuters Marketpsych Index (TRMI). It is evaluated on news, social media, and a combination of both. TRMI is defined as the ratio of the sum of all relevant variables to the sum of absolute value sum of the TRMI constituent variables, which is defined as Buzz. These are given as :
    \\
    \begin{equation}
    Buzz(a) = \sum_{c \in C(a), v\in V} |Var_{c,v}|
    \end{equation}
    \begin{equation}
    TRMI_t(a) = \frac{\sum_{c \in C(a), v\in V(t)} (I(t, v) * PsychVar_v(C))}{Buzz(Asset)}
    \end{equation}
     For the features obtained, they use ARIMA(Autoregressive Integrationg Moving Averages) and RNN  models. ARIMA has parameters such as autoregression, moving average, and integration. They also use variations of ARIMA such as ARIMAX which has an exogenous variable along with a time series variable attached to it. RNN is an artificial neural network-based model which takes the current input data and also the previous input data for making a prediction, which allows it to perceive data at time t-1. The paper shows that sentiment analysis is a key part of data-enabled algorithmic systems for cryptocurrency investments and trading.
    
[5] This paper uses historical tweet data fetched from twitter, user meta data containing the number of followers, number of retweets of a given tweet and corresponding Bitcoin prices, XRP prices and Ethereum prices at that given time instance because of the high correlation between their prices.
    The paper broadly works up on two aspects:
    \\
    \begin{enumerate}
        \item Implementing a predictive model which uses momentum metric to predict actions of buying, selling and holding a given crypto currency. If the momentum is above 5\% they predict buying, if less than -5\% they predict selling and if in between they predict holding. The threshold is decided based on the volatility of the crypto markets.
        \begin{equation}
          momentum = \frac{Price_{close} - Price_{open}}{Price_{open}}
        \end{equation}
        
        \item Providing explainability on the above implementation by using unsupervised deep learning clustering models to determine the underlying patterns. More formally, the paper compares each tweet representation obtained from DistilBERT to the buy, sell and hold category obtained from the section 1) and then finds the maximum similarity between groups of tweets and assigns the given tweet to the group having highest cosine similarity.


    \end{enumerate}
[6] The paper aims to study the correlation between Twitter sentiment and the changes it can bring to the prices of cryptocurrencies such as Bitcoin. Their motivation was that their research could help predict the price of Bitcoin in the future using past sentiments and Bitcoin prices. They wanted to develop a time series analysis for which tweets and Bitcoin prices along with their timestamps were collected from 12th March 2018 to 12th May 2018 as there was aggressive fluctuation on the prices of Bitcoin and this helped them in making an effective model. The data was scraped using APIs and web scraping techniques. Bitcoin prices were collected from four different sources namely - "BITSTAMP" "COINBASE",  "ITBIT" and  "KRAKEN'' and only close price of Bitcoin was used. In order to get a curve that is smooth the average of prices from the four sources was taken. Sentiment analysis using VADER (Valence Aware Dictionary and Sentiment Reasoner ) was performed. VADER was chosen as it was highly used when dealing with data from social media sites. A score between -1 to 1 was given by VADER. Finally, a Random Forest Regressor was used to perform evaluations of their model, Random Forest Regression was used as it was more adaptable to inputs of various kinds. A 62.48\% accuracy was observed while making the predictions using tweet sentiments and past prices of Bitcoin.

[7] The paper covers the study of Twitter sentiment about a particular altcoin (NEO) and how it correlates with its price in the crypto exchange market. They follow a straightforward methodology in their paper. The Twitter tweet data related to this particular altcoin is collected by scraping using fourteen different variations of a related hashtag for e.g - \#neo, \$neo, \#NEO etc. The tweet text along with some other tweet related data like username, language is collected using this way. This was followed by various preprocessing steps which included filtering based on the most frequent word, negative/positive crypto terms and removal of punctuation and spaces. Also, bot account tweets were also identified and filtered based of the frequency of tweets. Using a Random Forest classifier they were able to obtain 82\%  train set and 77\% test set accuracy for the sentiment analysis task on a subset. The pre-trained BERT in comparison had an accuracy of 45\% in the test set. The tweets' sentiments are classified into three classes: positive, negative and neutral. This sentiment is aggregated for a particular day and combined with cryptocurrency price and volume data(particularly bitcoin, Ethereum and NEO). The sentiment correlation with the price is observed and it was found that tweets with neutral sentiment had the highest correlation with NEO prices. Also, a high correlation was observed between Bitcoin and NEO prices. BERT based sentiment analysis showed that positive sentiment tweets had the highest correlation with price. Basically because neutral sentiment class is the most dominating, it was concluded that that is why probably it corresponded to a high correlation.\\
   
[3] The paper aims to predict the two-hour price  of cryptocurrency, namely Bitcoin and Litecoin on the basis of social factors such as tweet sentiments with the help of a multi-linear regression model. Bitcoin and Litecoin were chosen in particular due to their heavy popularity and reach among the public.The prices of the two bitcoins are extracted with the help of CoinDesk and the tweets are extracted with the help of rest APIs. After the tweets have been extracted they are classified into 3 classes on the basis of sentiments. Those sentiments being positive, negative and neutral. Textblob sentiment polarity is used for this purpose. This gives a score to a tweet between -1 and 1. All tweets with polarity $>$ 0 are classified as positive, all with polarity = 0 are classified as neutral and all tweets with polarity $<$ 0 are classified as negative. After this all these tweets are put into different groups based on the time they were posted and each group contains tweets posted within a span of 2 hours, the count of positive, negative and neutral tweets are kept as features. The average price during these 2 hours of both bitcoin and litecoin is also calculated and used as labels of the dataset. In the next phase, real-time tweets are used for testing and their metrics, accuracy and R2-score are used for the evaluation.

The proposed multilinear regression model is able to predict the 2-hour prices of bitcoin and litecoin upto an R2 value of 44\% and 59\% respectively. 

\section{METHODOLOGY}
\subsection{DATASET}

We extracted the data for six different cryptocurrencies, 3 coins from the large-cap range: Avalanche, Ripple, Solana and 3 coins from the mid-cap range: DogeCoin, Matic and Shiba Inu. For extracting the Twitter data, the Tweepy library is used which makes access of the Twitter API. We search for hashtags containing the symbol of the coin (‘\#<Name/Symbol of Coin>’) for searching the tweets relevant for the coin. The cryptocurrency price data for these coins are collected using the CryptoCompare API which provides historical cryptocurrency price data by minute, hour and day. We collected the cryptocurrency price data by the minute i.e at the interval of one minute.\\

\textbf{Fields in the tweet data include:}
\begin{itemize}
    \item id: tweet id
    \item text: tweet text
    \item favourite\_count:  The number of times the tweet has been favourited (liked).
    \item retweet\_count: The number of times the tweet has been retweeted.
    \item created\_at: The datetime of the moment the tweet has been tweeted.
    \item User: User related data for the user that tweeted it. It includes around sixty user-related information fields like id, name, screen\_name, location, followers\_count,\\ friends\_count, favourites\_count, verification status, following\_count etc.
    \item place: The place (geographical location) from where the tweet is tweeted.\\
\end{itemize}
\textbf{Fields collected in the cryptocurrency price data include:}
\begin{itemize}
    \item time: The datetime for which the crypto data is recorded
    \item high: The highest price during that time period (here minute)
    \item low: The lowest price during that time period (here minute)
    \item open: The price at the start of the minute
    \item volume\_from: Total amount of base currency (USD) traded into the cryptocurrency during that minute.
    \item volume\_to: Total amount of cryptocurrency traded into the base currency (USD) during that minute.
    \item close: The price at the end of the minute\\
\end{itemize}

\subsubsection{Preprocessing}
The collected Twitter data included the tweet text which was preprocessed because it is needed for the part of sentiment analysis. Also, the ‘User’ data in the tweet data was present in the JSON form and the keys of the JSON were parsed into columns of our pandas dataframe. The preprocessing steps applied to the tweet text include:

\begin{itemize}
    \item Removed any user mentions present and handled retweets: Tweets tend to have user mentions “@sarthakj01” such mentions are removed from the tweets
    \item Removed any links/URLs from the tweet text: Tweets have links, all HTTP or bitly links are removed from the tweets 
    \item Separating the hashtags in a different column, these hashtags are useful as they contain very important information regarding the tweet: Hashtags such as \# bitcoin,  \# DOGECOIN are handled, they are removed from the tweets and kept in a separate column for that tweet as they can hold vital information
    \item Converted tweets to lowercase
    \item Removed punctuation
    \item Classified Emojis and Emoticons: Emoticons like :-)) is written as ‘Very\_happy’ and symbol based emojis are also appropriately expanded. Python’s emot and emoji libraries are used for these respective classifications\\
\end{itemize}    

\textbf{Outlier Detection}\\

\begin{itemize}
    \item In the Avalanche dataset, we found out that there were many tweets which contained the word Avalanche but weren't related to the cryptocurrency Avalanche
    \item Upon further research we found out that there were multiple other meanings related to the word Avalanche (such as a football team) and hence the tweets were out of context
    \item For this problem we created a list of crypto/financial terms and classified the data into crypto or non crypto category and discarded the non crypto category data because that would have led to wrong results while sentiment and further analysis\\
\end{itemize}
    

Note that we deliberately chose not to remove the stopwords because in the case of sentiment analysis they can hold important sentiment-related information.  Removing them can lead to capturing the sentiment wrongly.\\For example:
\\\\\textbf{Original sentence:} Bitcoin is not a good investment (Negative sentiment)\\
\textbf{After stopword removal:} Bitcoin good investment (Positive sentiment)\\

The cyrptocurrency price data did not need any preprocessing. The cryptocurrency price data is finally joined with the tweets data on the basis of the time of the tweet. The cryptocurrency price data of the very next minute of the time given by the  ‘created\_at’ column of the tweet is joined with that tweet’s data. A tweet made at the time 
‘hh:mm:ss’ will have the price data of that cryptocurrency at ‘hh:(mm+1):00’ joined with it.

\subsubsection{Analysis}
\begin{itemize}
    \item We generate the wordcloud of our twitter dataset for each coin. The tweet text was first preprocessed according to the preprocessing steps explained above. (Figure ~\ref{fig:wordcloudforcoins}) 
    
    % Figure environment removed
    
    
% Figure environment removed
 
     \item   Then we went for topic modelling of our tweet text. For this we used pyLDAvis library which gives a visual representation to the topic modelling performed by Latent Dirichlet Allocation. This helps us to understand about the most popular topics in our dataset, The preprocessed text is used for LDA.
    
    \item To understand if the fact of a user being verified have any impact to the way people view a tweet, we plotted a graph (Figure ~\ref{fig:userverified}) , it can be clearly seen that people engage more with a tweet when the user is verified hence there is a much greater retweet count and favourite count when compared to the tweet made by a user who is not verified.
    \item The average tweet text length is also calculated for all the different coins' tweet data (Figure ~\ref{fig:tweetlens}).
    % Figure environment removed
    
    % Figure environment removed
\end{itemize}

% \pagebreak
\subsection{Sentiment Analysis}

\begin{itemize}
    \item Sentiment Analysis as a NLP task where we identify, and categorize opinions expressed in a given text, with respect to the overall sentiment of the corpus. This means that for a review based dataset, a positive tweet will have a different emotion attached to it, as compared to a finance based dataset.
    
    \item For our problem statement the preprocessed dataset has an undefined sentiment score. Using a pretrained sentiment analysis model might not lead to accurate results for the same reasons mentioned above. Hence, we decided to fine tune a pretrained sentiment analysis model using a previously collected and labeled dataset for crypto currency tweets. This will further improve the sentiment analysis results.
    \item To this extent, we have used a BitCoin dataset, with tweets and their sentiment category (\emph{positive, neutral, negative}) and a related sentiment score. The given dataset had a total of 50,859 tweets. Out of these tweets, we use 24917 tweets for training, 15257 tweets for testing, and 10679 tweets for validation.
    \item \emph{Pre-Trained Model}: For our pre-trained model, we use the \emph{twitter-roberta-base-sentiment} model, which was previously trained on around 58M tweets, and fine tuned for sentiment analysis with the TweetEval benchmark.
    \item \emph{RoBERTa} : A futher improvement on BERT model to improve pre-training in self-supervised NLP systems. RoBERTa improves on BERT by training larger mini-batches, improved learning rates, and removing next-sentence pre-training object in a BERT model.
    \item For training the given model, we use a learning rate of 1e(-5), batch size of 8 and a total of 4 epochs. The maximum length is set to 256, which is under the average tweet text length we have. After fine tuning the model, we use it on our current dataset.
    \item In the predictions obtained from our model, we get a sentiment label and the corresponding sentiment score which is a value in the range [0,1] on the tweet text of the tweet dataset we have for all the coins.

     \item We used various models which are BERT, Roberta based and available on hugging face, we finally used the model which was Roberta based and pretrained on twitter sentiment analysis and finetuned it on our crypto dataset by splitting it into train and validation sets as it gave the best accuracy
     \item Finally we obtained our predictions which are the sentiment label (Positive, Negative, or Neutral) and the corresponding sentiment score which is a value in the range [0,1] on the dataset we had obtained by scraping tweet data and computing sentiment scores for cryptocurrencies such as Avalanche, Doge Coin, Matic, Solana and Shiba Inu.
\end{itemize}

\subsection{Prediction Models}

We use regression based machine learning models from scikit-learn to make a prediction on crypto coin prices. These models use the sentiment scores and sentiment labels of the tweet text along with tweet metadata(favourite\_count, retweet\_count) and basic user metadeta (user\_follower\_count, user\_verified) as feature set. The columns of sentiment label is one-hot encoded and user\_verified status is mapped to a binary integer value (0,1).\\\\
Regression models used for baseline price prediction include:\\
\begin{itemize}
    \item \textbf{Linear Regression (LR)}: It tries to fit a linear model with the help of coefficients $W = (W_1, W_2, W_3, W_4, ….., W_n)$ so as to reduce the residual sum of squares between the actual values and the values predicted using the linear approximation.\\
    \item \textbf{SGD Regressor (SGD-R)}: It works towards building an estimator using a regularized linear model. The regularizer adds a penalty to the loss to help shrink the model parameters. It follows a stochastic gradient descent method where gradients are computed for each sample one at a time and weights are updated using that. We use Huber loss during training of the model. It is less sensitive to outliers and is computed as:\\


% Figure environment removed

 \item \textbf{Random Forest Regressor (RF-R)}: It is an ensemble method that tries to improve the predictive power by fitting a number of classifying decision trees and averaging.\\
\end{itemize}
We split our data into training and testing sets for running through the machine learning models.We have taken a 70:30 train:test split ratio.The parameters used for training SGDRegressor include 'l2' penalty, initial learning rate of 0.01, maximum\_iter as 100 and a alpha(regularization weight) of 0.01. We take the max\_depth of the Random Forest Regressor as 5 and the Linear Regression model is trained on default settings. The metrics we use for evaluation of the performance of these predictive models are Mean Absolute Error (MAE) and Root Mean Squared Error(RMSE). In addition to this, we also use a metric called percentage error ($\delta$) for comparisons.\\

% Figure environment removed

\textbf{LSTM based model }: As the problem of price prediction has a time variable and also is related to the previous values and features in the real world, we have used LSTM based model. Although RNN models can theoretically remember about all previous occurrences but in practice that is not the case, the LSTM is an optimized version of RNNs that can in practice also remember the past occurrences and by a mechanism of hidden state, forget input and output gates. 

\begin{wrapfigure}{r}{5.5cm}
% Figure removed
\caption{LSTM based model architecture}\label{fig:modelarch}
\end{wrapfigure} 

We tried various stacked combinations and then used fully connected layers with combinations over it to finally obtain the predictions. We train for 200 epochs with a learning rate of 8e-4, hidden size 16. We also add price related features like high, low, volume\_to/from etc to the data. Also, we both standard scale the data and do min-max normalization on the price values while training. However, we also take appropriate inverse transforms while inference of the results whenever needed. The model architecture can be seen in Figure~\ref{fig:modelarch}\\.


\section{EVALUATION}
\medskip
We evaluate and analyze the result of the price prediction task and the sentiment analysis part for all the cryptocurrencies.
\subsection{Prediction Models}
The above mentioned linear models are trained on the features dataset and the corresponding cryptocurrency coin price is predicted. Following are the values of mean absolute error, root mean squared error and maximum percentage error obtained for each model:

% Figure environment removed

From the loss values we observe that learning is happening the best for ShibaInu, DogeCoin and Matic. Their predicted values are close to the actual values. Avalanche and Solana have high error/loss value which indicates the model is not learning to predict their prices properly. An important thing to note here is that the price data is time dependent i.e. the price of the cryptocurrency coin is dependent on the previous prices and the previous feature data (here sentiment data) as well. Thus here in our regression-based baselines we have not considered this information (dependency on past data and values) and there is no context of time or sequence in our current models which work considering the features as simple numerical data. This leads to loss of contextual information.

% Figure environment removed

Following are the values of mean absolute error, root mean squared error and maximum percentage error (max $\delta$) obtained for LSTM based model:

% Figure environment removed

We observe a significant improvement in the MAE and RMSE values when compared with baseline models. Most significantly, we can see an improvement in the value of maximum percentage error for all the crypto coins. Also, when we plot all the price values of the cryptocurrency (both training set and predicted values along with actual values), we can see that the model is capturing the trend of the prices and is predicting values in a good range. This holds true for both Avalanche and Solana as well that had high MAE and RMSE values in the regression based models. The fluctuations in the prices are also being properly captured. Figure ~\ref{fig:modelpriceplot} depicts this for two of the crypto coins. Also, on analyzing the effect of metadata on the predictions, we find that the error values remain almost similar but introducing metadata adds a bit of noise to the predictions i.e- we observe that there are some spikes in the predictions.

\begin{wrapfigure}{r}{3.5cm}
% Figure removed
\caption{Price value over time for Dogecoin and Avalanche}
\label{fig:modelpriceplot}
\end{wrapfigure} 

Since, we do not find any other works that use the same large-cap and mid-cap coins as used by us, we cannot make a direct comparison for our results. However, the recent work on bitcoin price prediction using twitter sentiment analysis reports a maximum percentage error of 43.83\% [6]. The actual and predicted price value's fluctuation for Bitcoin and its plot is also presented in this work [6].Another work on bitcoin price prediction report the best MAE value of 2.7526 and RMSE value 13.7033 [2].  Comparing to the cryptocurrencies we have chosen, we can see that we achieve significantly better(lower) MAE and RMSE value for all of them in our best case LSTM based model. Also, we have a notably lower value of the maximum percentage error than these. Also, our price plots are more smoother and have lesser noise and capture the tend more accurately.

\subsection{Sentiment Analysis}
% Figure environment removed
\textbf{Principal Component Analysis} : After evaluating our dataset on the fine tuned sentiment analysis model, we find the sentiment score and sentiment label for each tweet, for all the coins.  However, we need to analyze whether the text and their corresponding sentiments are actually trained well or not. To do this we can use clustering to find whether similar sentiments are clustered together. For this, we use PCA, a dimensionality reduction technique. First, we use a sentence tokenizer(BERT based) on all the tweets, and pick up 1000 points randomly from each of the sentiment classes. This results in an embedded matrix of size 3000 X 786 (no. of tweets X embedding size). We  apply PCA, reducing their total dimensions to 2. Using these dimensions we plot the points, and observe that similar sentiments get clustered together. 

% Figure environment removed


% Figure environment removed


% Figure environment removed

% Figure environment removed

% Figure environment removed

We plotted prices of crypto currency and the weighted sentiment score against created\_at time of the tweet (Figure ~\ref{fig:weightedsentimentscore} ). As we hoped, we were able to find some correlation in between the weighted sentiment score. It could be seen from the graph of almost all the currencies that the peaks and declines for both weighted sentiment score and the crypto currency coincided with time, thus showing that positive sentiments have led to an increase in the prices of cryptocurrency and and the negative sentiments have led to a decline. We also not that they do not coincide at the exact same moment rather, the effect of sentiment on price of the cryptocurrency comes after some hours. This causal relation is visible more clearly for DogeCoin and Avalanche.
The weighted sentiment score is calculated by assigning certain weights to positive, negative and neutral sentiment labels and then scaling this according to the cryptocurrency price (so that the sentiment value and prices are in comparable range for plotting). The weights are decided empirically for all coins.

\section{CONCLUSION}
We have computed the sentiment score and label of the collected tweets for the 2 large-cap crypto coins Solana and Avalanche, and 3 coins from mid-cap range like Dogecoin, Matic and Shiba Inu. It is done using RoBERTa based pretrained sentiment analysis model finetuned on a crypto-sentiment (Bitcoin-based) dataset. We observe that similar sentiments get clustered together. We combine this sentiment analysis derived features with the metadata we collect which include features like favourite\_count, retweet\_count and user metadata (follower\_count, user\_verified). 

We have used this for the task of price prediction and use three regression based models and an LSTM based model. We obtain satisfactory results in terms of the metrics like MAE, RMSE and percentage error. We also see from the price plots (Figure 10) that the model is able to capture the trend of price change of the cryptocurrency and is appropriately predicting its future value.

Now moving forward things that can be worked upon more are that our model predicts a decently well accuracy but the predictions are shaky, that is the model is not able to capture well that the price fluctuations are almost continuous in nature. More complex models might be able to capture this and as the duration of our project was less and more time was occupied by other things we could not deep dive into more complex model architectures that might be able to capture this well.

Also some mechanism to incorporate the sentiment and corresponding tweet metadata can be tinkered with as the tweets seem noisy. This can be because of the twitter bots tweeting similar kind of posts that makes the sentiment inflated to either the positive or negative side. Some more advanced and complex time series forecasting methods can also be used for this task in the future.

%\bibliographystyle{ACM-Reference-Format}
%\bibliography{sample-base}
%%\printbibliography

%\section{Introduction}
%
%Emotion recognition from EEG data has emerged as a promising area of research in the ﬁelds of affective computing, neuroscience, and human-computer interaction. The ability to accurately identify and understand human emotions based on brainwave activity offers vast potential for enhancing various applications, such as virtual reality experiences, mental health monitoring, and adaptive human-robot interactions.
%Traditionally, emotion recognition methods relied on behavioral and physiological cues, such as facial expressions, voice tone, and heart rate variability. While these approaches provided valuable insights, they often lacked the precision and direct access to the underlying neural processes responsible for generating emotions. EEG, as a non-invasive and portable neuroimaging technique, bridges this gap by directly measuring the brain's electrical activity.
%
%Over the years, researchers have explored different methods to decode emotional states from EEG data. Classical machine learning techniques, such as Support Vector Machines (SVM) and Random Forests, were initially employed to classify emotions based on EEG features. These methods often required handcrafted feature extraction, which could limit their ability to capture complex temporal dynamics inherent in EEG data.
%With the advent of deep learning, Recurrent Neural Networks (RNNs) brought signiﬁcant improvements to emotion recognition from sequential data. RNNs introduced the concept of memory cells, allowing the model to retain information over time and learn long-term dependencies within the data. This led to better emotion recognition performance compared to traditional machine learning approaches.
%
%The Gated Recurrent Unit (GRU) algorithm, a variant of RNNs, further enhanced the capabilities of emotion recognition from EEG data. GRUs introduced specialized gating mechanisms that regulate the ﬂow of information within the network. The update gate controls how much past information should inﬂuence future predictions, while the reset gate selectively resets or forgets certain knowledge. The current memory gate, often overlooked, adds non-linearity and normalization to the input, improving the model's ability to capture relevant features.
%
%The unique architecture of GRUs addresses the vanishing and exploding gradient problem encountered in standard RNNs, making them more effective in capturing long-term dependencies within EEG signals. Moreover, GRUs are computationally efﬁcient, allowing for faster training and prediction times compared to more complex models like Long Short-Term Memory (LSTM) networks.
%
%The advantage of using GRUs for emotion recognition lies in their ability to process time-series EEG data directly, avoiding the need for handcrafted feature engineering. By learning from the raw EEG signals, GRUs can automatically capture intricate temporal patterns associated with different emotional states. This self-learning capability contributes to the model's adaptability and robustness across various emotion recognition tasks.
%
%Emotion recognition from EEG data has seen signiﬁcant advancements with the introduction of deep learning techniques, particularly the Gated Recurrent Units. The ability of GRUs to capture temporal dependencies and learn from raw EEG signals has opened new possibilities for creating more empathetic and emotionally aware human-computer interfaces. As the research in this domain progresses, we can anticipate even more accurate and nuanced emotion recognition systems that truly understand and respond to human emotions. 
%
%\section{Related Work}
%
%Koelstra et al. (2012) present the DEAP database, a multimodal dataset for analyzing human affective states [1]. The dataset comprises EEG and peripheral physiological signals recorded from 32 participants while watching 40 one-minute music video excerpts. Participants rated the videos based on arousal, valence, like/dislike, dominance, and familiarity levels. The paper proposes a novel method for stimuli selection using affective tags from the last.fm website and online assessment tools. It also performs an extensive analysis of participants' ratings and investigates correlations between EEG signal frequencies and emotional ratings. Liu et al. (2011) focus on real-time EEG-based emotion recognition[2]. The study utilizes EEG signals to classify human emotions and visualize emotional states in real-time. The proposed system demonstrates the feasibility of using EEG for emotion recognition tasks and provides insights into the potential applications of EEG-based emotion analysis.Chanel et al. (2006) explore emotion assessment using EEG and peripheral physiological signals [3]. The research investigates arousal evaluation based on these modalities and demonstrates the feasibility of using EEG signals in conjunction with peripheral physiological measurements for emotion recognition.
%
%Liu et al. (2010) focus on real-time mental stress recognition using EEG data [4]. The study presents a system that can detect and recognize mental stress from EEG signals in real-time. The proposed approach demonstrates the potential of EEG-based emotion recognition in practical applications related to stress management and mental health assessment. Koelstra et al. (2012) focus on single-trial classification of emotions induced by music videos using EEG and peripheral physiological signals [5]. The study presents methods and results for classifying arousal, valence, and like/dislike ratings based on the modalities of EEG, peripheral physiological signals, and multimedia content analysis. The research highlights the potential of combining multiple modalities for more robust emotion recognition. Alarcao and Fonseca (2017) investigate emotions recognition in EEG signals during a virtual reality simulation of an emergency evacuation [6]. The study explores the potential of using EEG-based emotion recognition in virtual reality environments for assessing users' emotional states during critical scenarios.
%Shi et al. (2019) focus on real-time emotion recognition using deep learning from EEG signals [7]. The study presents a deep learning-based approach for detecting and classifying emotions in real-time based on EEG data. The proposed method demonstrates the effectiveness of deep learning in handling EEG-based emotion recognition tasks. Zhang et al. (2019) provide an overview of emotion recognition using EEG signals [8]. 
%
%The survey paper reviews various methods and approaches used in EEG-based emotion recognition, highlighting the advancements and challenges in this field. The paper also discusses potential applications and future directions for EEG-based emotion analysis. El Ayadi et al. (2011) provide valuable insights into emotion recognition using other modalities, including speech [9]. Although not focused on EEG-based emotion recognition, this survey paper reviews various features, classification schemes, and databases used in speech emotion recognition, offering valuable information for understanding multimodal emotion recognition. Guo et al. (2018) explore emotion recognition from EEG signals using multimodal deep learning [10]. The study proposes a novel approach that combines EEG data with other modalities to enhance emotion recognition accuracy. The research demonstrates the potential benefits of integrating EEG-based emotion recognition with other modalities for more robust emotion classification. Cho et al. (2014) propose a novel approach for learning phrase representations using RNN Encoder-Decoder models, which include GRUs, for statistical machine translation[11]. The research introduces an end-to-end architecture for mapping variable-length input sequences to variable-length output sequences, making it suitable for natural language processing tasks. The experimental results demonstrate the effectiveness of GRUs in capturing semantic information and generating accurate translations. Li et al. (2018) explore the use of EEG data for emotion recognition using 3D Convolutional Neural Networks (CNNs) with GRU layers[12]. The research investigates the fusion of spatial and temporal features extracted from EEG signals for improved emotion classification. The results show that the combination of 3D CNNs and GRUs outperforms traditional machine learning methods in EEG-based emotion recognition tasks. Wen et al. (2018) conduct a comparative study on deep learning methods, including GRUs, for EEG-based emotion recognition[13]. The research evaluates the performance of various deep learning architectures and explores the impact of different feature extraction techniques on emotion classification accuracy. The findings highlight the advantage of GRUs in capturing temporal dynamics and extracting meaningful features from EEG data.
%
%Zheng et al. (2020) present a comprehensive review of deep learning methods for emotion recognition from EEG signals[14]. The paper includes an analysis of different neural network architectures, including GRUs, and their application in emotion recognition tasks. The review highlights the strengths and limitations of using GRUs in EEG-based emotion recognition and discusses potential future research directions in this area. Huang et al. (2014) propose an emotion recognition approach using Hidden Markov Models (HMMs) with GRU-based feature extraction from EEG data [15]. The research investigates the use of GRUs as a feature learning method for capturing sequential patterns in EEG signals. The results demonstrate the effectiveness of the proposed method in recognizing emotions from EEG data. Li et al. (2020) introduce a hybrid network that combines GRUs and Long Short-Term Memory (LSTM) units for EEG-based emotion recognition [16]. The research investigates the complementary strengths of GRUs and LSTMs in capturing temporal dependencies and modeling long-range dependencies in EEG data. The hybrid network demonstrates improved performance in emotion classification compared to individual GRU or LSTM models. Lv et al. (2018) propose an improved LSTM-based model for EEG-based emotion recognition [17]. The research enhances the LSTM model with attention mechanisms to focus on salient EEG signal segments relevant to emotion classification. The study compares the improved LSTM model with GRUs and other LSTM variants and shows promising results in emotion recognition accuracy.
%
%Li et al. (2019) propose an improved GRU-RNN model for EEG-based emotion recognition that combines temporal-spatial domain features [18]. The research investigates the combination of spatial and temporal EEG information to enhance emotion classification accuracy. The results show that the improved GRU-RNN model outperforms other traditional machine learning methods and demonstrates the potential of GRUs in capturing both spatial and temporal patterns in EEG data. Wang et al. (2021) propose a deep convolutional neural network (CNN) with GRU layers for emotion recognition from EEG data [19]. The research combines the advantages of CNNs in spatial feature extraction with the temporal modeling capabilities of GRUs to improve emotion classification performance. The experimental results demonstrate that the CNN-GRU model achieves competitive accuracy compared to other state-of-the-art methods. Yao et al. (2020) propose a hybrid model that combines Convolutional Neural Networks (CNNs) and GRUs for EEG-based emotion recognition [20]. The research investigates the fusion of spatial and temporal information from EEG signals using CNNs and GRUs, respectively. The results demonstrate the synergy between CNNs and GRUs, leading to improved emotion classification accuracy in EEG data. These studies highlight the diverse applications of GRU in various fields, including natural language processing, image captioning, emotion recognition from EEG data, and time-series data processing. The research demonstrates the efficacy of GRUs in capturing temporal dependencies, modeling complex patterns, and improving the performance
%
%\subsection{Correlation between Emotion Recognition from EEG and GRU}
%
%The literature survey reveals that emotion recognition from EEG data has been explored using various machine learning techniques, such as support vector machines and time delay neural networks. While these methods have shown promising results, they often struggle to capture the complex temporal dependencies present in EEG signals, which are crucial for accurate emotion classification. The emergence of deep learning approaches, particularly the Gated Recurrent Unit (GRU) algorithm, has offered a solution to address the temporal modeling challenges in EEG-based emotion recognition. GRU, as a variant of Recurrent Neural Networks (RNNs), has gained attention for its ability to efficiently process time-series data and capture long-term dependencies within sequential information.
%
%The advantage of GRU lies in its gated architecture, which enables it to selectively retain relevant past information while discarding irrelevant data. This unique property makes GRU well-suited for emotion recognition tasks where temporal context is essential for understanding emotional states encoded in brainwave signals.
%When considering both emotion recognition from EEG data and the GRU algorithm together, a synergistic relationship emerges. By incorporating GRU into the emotion recognition pipeline, it becomes possible to enhance feature extraction from preprocessed EEG data and improve the prediction accuracy of emotional states.
%GRU's ability to capture context and temporal relationships within EEG signals complements the requirements of emotion recognition, where subtle changes in brainwave patterns are indicative of different emotional states. Leveraging GRU's capability, the emotion recognition system gains a deeper understanding of the underlying dynamics of emotional responses encoded in brainwave signals, leading to more accurate and robust emotion classification models.
%
%Overall, the literature survey demonstrates that combining emotion recognition from EEG data with the GRU algorithm offers promising prospects for advancing emotion recognition tasks. As more research continues to explore the correlation between EEG-based emotion recognition and deep learning techniques like GRU, we can expect further advancements in real-time emotion recognition applications across various domains.  In Table. 2.  shows the EEG-Emotion Based survey in the appendix section.
%
%The correlation between emotion recognition from EEG data and GRU lies in the 
% potential of using GRU as a deep learning algorithm to capture the temporal dynamics 
% and dependencies present in EEG signals. EEG-based emotion recognition requires 
% modeling the complex changes in brainwave patterns over time, and GRU's gated 
% architecture enables it to effectively learn and represent these temporal relationships. 
% By incorporating GRU into the emotion recognition pipeline, it becomes possible to 
% enhance feature extraction from EEG data and improve the prediction accuracy of 
% emotional states. This combination of EEG-based emotion recognition and GRU 
% provides a promising direction for advancing emotion analysis and understanding 
% affective responses encoded in brainwave signals. The publicly available multimodal 
% dataset presented in the research serves as a valuable resource for other researchers to explore and test their own affective state estimation methods. 
% 
%The papers listed in the table explore various aspects of using GRUs in emotion recognition, spanning EEG data analysis, deep learning methods, and temporal dynamics modeling. These studies contribute to the growing body of research on emotion recognition and demonstrate the versatility and effectiveness of GRUs in handling sequential data and capturing complex patterns, making them valuable tools in emotion recognition from EEG signals.
%
%\section{Dataset}
%The data used in this research was collected for a duration of 3 minutes from two  participants, consisting of one male and one female, for each emotional state: positive, neutral, and negative. The participants were exposed to specific stimuli designed to elicit the desired emotions. Additionally, six minutes of resting neutral data were also recorded. The EEG data was captured using a Muse EEG headgear equipped with dry electrodes. The following EEG placements were utilized: TP9, AF7, AF8, and TP10. These electrode placements are commonly used for capturing brainwave activity related to emotion recognition.
%
%The dataset used in this research is publicly available and can be accessed at the following link: https://www.kaggle.com/datasets/birdy654/eeg-brainwave-dataset-feeling-emotions. It provides access to the collected EEG data, which includes the recordings for each participant in different emotional states, as well as the neutral resting data.The dataset consists of time-series EEG recordings, where each sample represents the electrical activity captured at specific time intervals. The data is labeled according to the emotional states experienced by the participants during the recordings. By utilizing this dataset, the research aims to explore the effectiveness of various noise removal methods on ambulatory EEG data and assess the performance of the GRU algorithm in accurately predicting emotional states based on the recorded brainwave activity.
%
%\section{Methodology}
%\subsection{Recurrent Neural Networks (RNNs)}
%Recurrent Neural Networks (RNNs) have emerged as a powerful neural network architecture for modeling sequential data. Unlike traditional feedforward neural networks, RNNs have a unique ability to capture temporal dependencies by introducing recurrent connections that allow information to flow from one time step to another. This characteristic makes RNNs well-suited for tasks involving sequential or time-series data analysis, including natural language processing, speech recognition, and, in our case, emotion recognition from EEG signals.
%
%In standard neural networks, each input and output is treated as independent, disregarding any contextual information. However, in many real-world applications, such as sentiment analysis or language generation, understanding the context of previous inputs is crucial for accurate predictions. RNNs address this limitation by incorporating a hidden state, which acts as a memory mechanism that retains information about past inputs and influences future predictions.
%
%The hidden state in an RNN serves as a crucial element for capturing long-term dependencies within the sequential data. As the RNN processes each input in a sequence, the hidden state is updated and passed to the next time step, allowing the network to remember past information and utilize it to make informed predictions. This recursive nature of RNNs enables them to model complex temporal relationships and extract relevant features from sequential data.
%
%However, traditional RNNs suffer from the vanishing or exploding gradient problem, which poses challenges in learning long-term dependencies. To address this issue, advanced variants of RNNs, such as Gated Recurrent Units (GRUs) and Long Short-Term Memory (LSTM) networks, have been developed. These architectures introduce specialized gating mechanisms that selectively retain or forget information, enabling the network to capture long-term dependencies while mitigating the gradient vanishing or exploding problem.
%
%In our research, we focus on the GRU algorithm as a variant of RNNs for emotion recognition using EEG data. GRUs have gained attention due to their simplified architecture, which consists of three gates: the update gate, reset gate, and current memory gate. The update gate determines how much past information should be propagated to future time steps, while the reset gate controls the extent to which previous knowledge should be forgotten. The current memory gate, often overlooked in discussions of GRUs, contributes non-linearity and zero-mean normalization to the input, reducing the impact of past data on future information.
%
%By leveraging the GRU algorithm, we aim to predict emotional states by analyzing EEG data collected from individuals exposed to various movie scenes or stimuli. The utilization of GRUs offers an efficient approach to capture temporal dependencies within the EEG signals, allowing us to explore the effectiveness of this architecture in emotion recognition tasks.
%
%\subsection{Gated Recurrent Units (GRUs)}
%Gated Recurrent Units (GRUs) have gained significant attention as an alternative architecture within the realm of Recurrent Neural Networks (RNNs). GRUs address some of the limitations of traditional RNNs, such as the vanishing or exploding gradient problem, while providing an efficient and effective solution for capturing temporal dependencies in sequential data. In this section, we will delve into the specifics of GRUs and their relevance to our research on emotion recognition using EEG data.
%
%GRUs are a type of RNN architecture that incorporates gating mechanisms to regulate the flow of information within the network. Unlike the more complex Long Short-Term Memory (LSTM) networks, GRUs have a simplified structure consisting of three essential gates: the update gate, reset gate, and current memory gate. This architectural design allows GRUs to strike a balance between modeling long-term dependencies and computational efficiency.
%The update gate, denoted as z, determines the extent to which the previous hidden state is incorporated into the current state. It controls how much of the past information should be carried forward to future time steps. By selectively updating the hidden state, the GRU can adapt to different patterns in the data and retain relevant context over time.
%
%The reset gate, denoted as r, determines the extent to which the previous hidden state influences the current state. It selectively resets or forgets some of the previous knowledge, allowing the model to focus on relevant features and adapt to changing patterns within the sequence. The combination of the reset and update gates enables GRUs to capture and adapt to varying dependencies within the data.
%
%Another crucial component of GRUs is the current memory gate, often overlooked in discussions of GRUs. It is a sub-component of the reset gate and plays a vital role in introducing non-linearity and zero-mean normalization to the input. This helps in reducing the impact of previous data on the current data being propagated forward, ensuring that relevant information is preserved while minimizing the interference from irrelevant or noisy signals.
%
%Compared to a basic RNN, the workflow of a GRU is similar, with the primary distinction lying in the internal functioning of each recurrent unit. By leveraging the gating mechanisms, GRUs excel at capturing and modeling temporal dependencies in the data, making them well-suited for tasks such as emotion recognition from EEG signals.
%
%In our research, we adopt the GRU algorithm as the core architecture for predicting emotional states based on EEG data collected during exposure to various movie scenes or stimuli. The GRU's simplified yet powerful design allows us to effectively model the temporal dynamics within the EEG signals, contributing to the advancement of emotion recognition using EEG-based approaches.
%
%\subsection{Preprocessing and Data Preparation}
%The preprocessing and data preparation stage is essential to ensure the quality and suitability of the EEG data for accurate emotion recognition using the GRU algorithm. We follow standard practices in EEG-based emotion recognition to preprocess the data effectively.
%The raw EEG data collected from participants wearing the Muse EEG headgear with dry electrodes is subjected to artifact removal techniques such as independent component analysis (ICA) or template matching algorithms. This step eliminates unwanted noise and artifacts, allowing us to focus on genuine EEG signals related to emotional states.
%
%Next, bandpass filters are applied to remove unwanted frequency components while retaining the relevant frequency ranges associated with brainwave activity. Normalization techniques, such as z-score normalization or min-max scaling, are then employed to address amplitude variations between participants or electrode placements.
%
%Feature extraction techniques are utilized to capture relevant information from the preprocessed EEG data. These features, including power spectral density, signal entropy, or time-domain statistics, serve as input for the GRU algorithm, enabling it to learn meaningful patterns and associations with emotional states.
%
%To ensure unbiased evaluation, the dataset is partitioned into training, validation, and testing sets. Stratified or random partitioning techniques maintain representative distributions of emotional states across the subsets. This partitioning facilitates model training, hyperparameter optimization, and unbiased evaluation of the GRU model's generalization capabilities.
%
%By implementing these preprocessing and data preparation steps, we enhance the quality and suitability of the EEG data for subsequent analysis using the GRU algorithm. The GRU model can effectively leverage the preprocessed data to accurately predict emotional states.
%\subsection{Gated Recurrent Unit Algorithm}
%The Gated Recurrent Unit (GRU) algorithm is a variant of Recurrent Neural Networks (RNNs) that excels at capturing long-term dependencies in sequential data while mitigating the vanishing or exploding gradient problem. In this section, we provide an overview of the GRU algorithm and its relevance to our research on emotion recognition using EEG data.
%The GRU architecture is designed to have a simplified structure compared to traditional RNNs and LSTM networks. It consists of three fundamental gates: the update gate, reset gate, and current memory gate. These gating mechanisms enable the GRU algorithm to effectively model temporal dependencies within the data while maintaining computational efficiency.
%
%The update gate (z) determines the extent to which the previous hidden state influences the current state. It controls the flow of information from past time steps to future time steps, allowing the model to adapt and retain relevant context over time. By selectively updating the hidden state, the GRU algorithm can capture long-term dependencies and learn patterns within the sequential data.
%
%The reset gate (r) regulates the influence of previous knowledge on the current state. It selectively resets or forgets certain information, enabling the model to focus on relevant features and adapt to changing patterns within the sequence. The combination of the update and reset gates empowers the GRU algorithm to effectively model and adapt to varying dependencies within the data.
%
%The current memory gate, often overlooked in discussions of GRUs, plays a critical role in introducing non-linearity and zero-mean normalization to the input. By serving as a sub-component of the reset gate, it helps reduce the impact of previous data on the current data being propagated forward. This mechanism minimizes the interference of irrelevant or noisy signals and ensures the effective transfer of information to future time steps.
%
%Compared to a basic RNN, the GRU algorithm follows a similar workflow but excels in its internal functioning within each recurrent unit. Leveraging the gating mechanisms, GRUs excel at capturing temporal dependencies and facilitating the prediction of emotional states from EEG data.
%
%In our research, we employ the GRU algorithm as the core architecture for predicting emotional states based on EEG data collected during exposure to various movie scenes or stimuli. The GRU's simplified yet powerful design allows us to effectively model the temporal dynamics within the EEG signals and contribute to the advancement of emotion recognition using EEG-based approaches.
%
%\subsection{Architecture}
%The architecture in Fig 1., used in our research leverages the Gated Recurrent Unit (GRU) algorithm for emotion recognition based on EEG data. This architecture comprises an InputLayer, GRU layer, Flatten layer, and Dense layer. The configuration of this architecture is as follows:
%
%% Figure environment removed
%
%\begin{enumerate}
%\item[1] \emph{InputLayer}: The InputLayer serves as the entry point for the EEG data into the neural network. It defines the shape and format of the input data, aligning with the preprocessing steps and feature extraction performed on the EEG data. The input layer represents the initial stage of information flow in the neural network.
%
%\item[2]  \emph{GRU Layer}: The GRU layer is the core component of the architecture and employs the Gated Recurrent Unit algorithm. It processes the sequential EEG data, capturing temporal dependencies and extracting relevant features for emotion recognition. The GRU layer's hidden state retains information from previous time steps and influences the predictions made by subsequent layers
%
%\item[3] \emph{Flatten Layer}: Following the GRU layer, the Flatten layer is applied to transform the multi-dimensional output of the GRU into a one-dimensional vector. This flattening operation enables the subsequent layers to receive a flat input, facilitating compatibility with traditional fully connected layers.
%
%\item[4] \emph{Dense Layer}: The Dense layer, also known as the fully connected layer, receives the flattened output from the preceding layer. It serves as a powerful learning component, responsible for mapping the extracted features to the emotional states being predicted. The dense layer consists of multiple interconnected neurons, and each neuron contributes to the final emotion classification based on learned weights and biases.
%
%\end{enumerate}
%
%This architecture efficiently processes the preprocessed EEG data through the GRU layer, capturing temporal dynamics and learning meaningful patterns related to emotional states. The subsequent flatten and dense layers allow for feature extraction and final classification, respectively.
%
%\section{Results and Analysis}
%
%The deep learning accuracy details of the GRU model on the validation set are as follows: loss - 3.4356e-09 and accuracy - 1.0000. These impressive results indicate that the GRU model achieved perfect accuracy in predicting emotional states based on the EEG data. The model's ability to achieve such high accuracy suggests its proficiency in capturing the temporal dynamics and extracting meaningful features from the EEG signals. Additionally, we compare the performance of the GRU model with other machine learning models using their respective scores. The following table 1 summarizes the scores obtained for various models:
%% Figure environment removed
%
%From the results, it is evident that the GRU model (95.46\% accuracy) performs competitively when compared to other machine learning models. Notably, the Extreme Gradient Boosting Classifier achieves the highest score of 99.39\%, closely followed by the Random Forest Classifier with a score of 98.7\%. The Linear Support Vector Machine Classifier also demonstrates excellent performance with an accuracy of 96.57\%.
%
%% Figure environment removed
%
%However, it is important to note that the GRU model showcases the advantage of deep learning in capturing complex temporal dependencies and extracting meaningful features from EEG data. Its accuracy of 95.46\% suggests its efficacy in predicting emotional states based on the EEG signals, showcasing (Fib. 2) its potential for real-world applications.
%
%Further analysis is required to delve into the strengths and weaknesses of each model, considering factors such as computational complexity, interpretability, and generalization capabilities. Additionally, a more comprehensive evaluation using additional performance metrics like precision, recall, and F1-score could provide deeper insights into the models' overall effectiveness.
%
%Overall, the results demonstrate the effectiveness of the GRU algorithm in accurately predicting emotional responses from EEG data. Its competitive performance against other machine learning models supports the notion that deep learning approaches, such as the GRU, can significantly contribute to the advancement of emotion recognition tasks using EEG signals.
%
%% Figure environment removed
%
%The confusion matrix analysis (Fib 3)  was conducted to evaluate the performance of the machine learning models, including the GRU algorithm, in predicting emotional states based on EEG data. The confusion matrix provides insights into the model's accuracy, precision, recall, and overall performance by comparing predicted labels with actual labels. It helps identify potential biases, misclassifications, and areas for improvement in the models' predictions.
%
%\section{Discussion}
%
%The results obtained from our research demonstrate the effectiveness of the GRU algorithm and other machine learning models in predicting emotional states based on EEG data. The deep learning accuracy of the GRU model showed remarkable performance, achieving perfect accuracy on the validation set. This indicates the ability of the GRU model to capture temporal dependencies and extract meaningful features from EEG signals, making it a promising approach for emotion recognition tasks.
%
%Comparing the performance of various machine learning models, we observed competitive results across different models. The Extreme Gradient Boosting Classifier achieved the highest score, closely followed by the Random Forest Classifier, while the Linear Support Vector Machine Classifier also demonstrated excellent performance. These findings highlight the importance of choosing an appropriate model based on the specific requirements and characteristics of the dataset.
%
%The confusion matrix analysis provided valuable insights into the models' predictions, allowing us to assess their accuracy and identify potential biases or misclassifications. By examining the distribution of predicted labels across different emotional states, we gained a deeper understanding of the models' strengths and weaknesses in capturing specific emotions. This analysis can guide future improvements and refinements in the models' performance.
%
%\section{Conclusion}
%
%In conclusion,  Fib 4 shows the Mindgraph of the our study.  Our research explored the application of machine learning models, including the GRU algorithm, for emotion recognition using EEG data. The GRU model demonstrated exceptional
%
%% Figure environment removed
%
%performance, achieving perfect accuracy on the validation set, highlighting its capability to capture temporal dynamics and extract meaningful features from EEG signals. Additionally, the comparison with other machine learning models emphasized the competitive performance achieved by various approaches. The choice of model should consider factors such as interpretability, computational complexity, and generalization capabilities, based on the specific requirements of the task. The incorporation of the confusion matrix analysis provided valuable insights into the models' performance, aiding in identifying potential biases and areas for improvement. By understanding the models' strengths and weaknesses, future research can focus on enhancing their accuracy and addressing specific challenges related to emotion recognition from EEG data.
%
%Overall, our findings contribute to the advancement of emotion recognition using EEG signals and highlight the potential of the GRU algorithm and other machine learning models in this field. Further research and exploration are warranted to improve the robustness and generalizability of these models and advance the understanding of human emotions through EEG-based approaches.
%
\begin{thebibliography}{20}

\bibitem{Jethin} Jethin Abraham, Danny W. Higdon, Johnny Nelson, and Juan Ibarra. 2018. Cryp-tocurrency Price Prediction Using Tweet Volumes and Sentiment Analysis.

\bibitem{Abid} Abid Inamdar, Aarti Bhagtani, Suraj Bhatt, and Pooja M. Shetty. 2019. Predicting Cryptocurrency Value using Sentiment Analysis. In 2019 International Conference on Intelligent Computing and Control Systems (ICCS). 932–934. https://doi.org/10. 1109/ICCS45141.2019.9065838

\bibitem{Arti} Arti Jain, Shashank Tripathi, Harsh Dhar Dwivedi, and Pranav Saxena. 2018. Forecasting Price of Cryptocurrencies Using Tweets Sentiment Analysis. In 2018 Eleventh International Conference on Contemporary Computing (IC3). 1–7. https: //doi.org/10.1109/IC3.2018.8530659

\bibitem{Yan} Yan Pang, Ganeshkumar Sundararaj, and Jiewen Ren. 2019. Cryptocurrency Price Prediction using Time Series and Social Sentiment Data. Proceedings of the 6th IEEE/ACM International Conference on Big Data Computing, Applications and Technologies (2019).

\bibitem{Anna} Anna Paula Pawlicka Maule and Kristen Johnson. 2021. Cryptocurrency Day Trading and Framing Prediction in Microblog Discourse. In Proceedings of the Third Workshop on Economics and Natural Language Processing. Association for Computational Linguistics, Punta Cana, Dominican Republic, 82–92. https://doi. org/10.18653/v1/2021.econlp-1.11

\bibitem{Otabek} Otabek Sattarov, Heung Jeon, Ryumduck Oh, and Jun Lee. 2020. Forecasting Bitcoin Price Fluctuation by Twitter Sentiment Analysis. 1–4. https://doi.org/10. 1109/ICISCT50599.2020.9351527

\bibitem{Emre} Emre Şaşmaz and F. Boray Tek. 2021. Tweet Sentiment Analysis for Cryptocur-rencies. In 2021 6th International Conference on Computer Science and Engineering (UBMK). 613–618. https://doi.org/10.1109/UBMK52708.2021.9558914
\end{thebibliography}
%
%%\newpage
%
%\section*{Appendix: Survey}
%
%% Figure environment removed
%
%%
%% Figure environment removed

\end{document}