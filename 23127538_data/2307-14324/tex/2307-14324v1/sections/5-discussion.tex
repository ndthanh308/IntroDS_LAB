%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion \& Limitations} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This paper presents a case study on the process of designing, administering, and evaluating a moral belief survey on LLMs. 
The survey findings provide insights into LLM evaluation and LLM fine-tuning.
Findings in low-ambiguity setting demonstrate that although most LLMs output responses that are aligned with commonsense reasoning, variations in the prompt format can greatly influence the response distribution. This highlights the importance of using multiple prompt variations when performing model evaluations.
The findings in high-ambiguity scenarios reveal that certain LLMs reflect distinct preferences, even in situations where there is no clear answer. We identify a cluster of models that have high agreement. We hypothesize that it is because these models have been through an ``alignment with human preference" process at the fine-tuning stage. Understanding the factors that drive this consensus among the models is a crucial area for future research. \looseness-1

There are several limitations in the design and administration of the survey in this study.
One limitation of this study is that the survey scenarios lack diversity, both in terms of the task and the scenario content.
We focus on norm-violations to generate the survey scenarios. However, in practice, moral and ethical scenarios can be more convoluted. In future work, we plan on expanding to include questions related to professional conduct codes. 
In generating scenarios, we utilized both handwritten scenarios and LLM assistance. However, we recognize that we did not ensure diversity in terms of represented professions and different contexts within the survey questions.
In future work, we aim to enhance the diversity of the survey questions by initially identifying the underlying factors and subsequently integrating them into distinct scenarios.

Another limitation of the work is the lack of diversity in the question forms used for computing the question-form consistency. We only used English language prompts and three hand-curated question templates, which do not fully capture the possible variations of the model input.
In future work, we plan to develop a systematic and automatic pipeline that generates semantic-preserving prompt perturbations, allowing for a more comprehensive evaluation of the models' performance.

A third limitation of this work is the sequential administration of survey questions, with a reset of the context window for each question. Although this approach mitigates certain biases related to question ordering, it does not align with the real-world application of LLMs. In practice, individuals often base their responses on previous interactions. To address this, future research will investigate the impact of sequentially asking multiple questions on the outcome analysis.

\section*{Acknowledgments}
We thank Yookoon Park, Gemma Moran, Adri√† Garriga-Alonso, Johannes von Oswald, and the reviewers for their thoughtful comments and suggestions, which have greatly improved the paper. This
work is supported by NSF grant IIS 2127869, ONR
grants N00014-17-1-2131 and N00014-15-1-2209,
the Simons Foundation, and Open Philanthropy.
