\section{Introduction}

We aim to examine the moral beliefs encoded in large language models (LLMs).
Building on existing work on moral psychology \citep{aquino2002self, greene2009pushing, graham2009liberals, christensen2014moral, ellemers2019psychology}, we approach this question through a large-scale empirical survey, where LLMs serve as ``survey respondents''.
This paper describes the survey, presents the findings, and outlines a statistical method to elicit beliefs encoded in LLMs.


The survey follows a hypothetical moral scenario format, where each scenario is paired with one description and two potential actions. 
We design two question settings: \emph{low-ambiguity} and \emph{high-ambiguity}. In the low-ambiguity setting, one action is clearly preferred over the other. 
In the high-ambiguity setting, neither action is clearly preferred.
\Cref{fig:example} presents a randomly selected survey question from each setting.
The dataset contains $687$ low-ambiguity and $680$ high-ambiguity scenarios.


Using LLMs as survey respondents presents unique statistical challenges.
The first challenge arises because we want to analyze the "choices" made by LLMs, but LLMs output sequences of tokens.
The second challenge is that LLM responses are sensitive to the syntactic form of survey questions \citep{efrat2020turking, webson2021prompt, zhao2021calibrate, jang2022becel}. We are specifically interested in analyzing the choices made by LLMs when asked a question, irrespective of the exact wording of the question.


To address the first challenge, we define \emph{action likelihood}, which measures the ``choices" made by the model.  It uses an iterative rule-based function to map the probability of token sequences, produced by the LLM, into a distribution over actions.
For the second challenge,  we define the \emph{marginal action likelihood}, which measures the choices made by the model when a question is presented with randomly sampled question forms. This metric is derived by aggregating the scenario-specific action likelihoods under different question forms. \looseness-1


To quantify the uncertainty of the model's choices, we use entropy \citep{mackay2003information} and define \textit{action entropy} and \textit{marginal action entropy}. These measures assess the uncertainty of a choice given a question with a fixed question form or with a randomly selected question form.
To gain further insights into the sources of uncertainty, we develop two evaluation metrics. The first one is the \textit{question-form consistency} (QF-C) metric, which assesses the model's consistency to variations in question forms. QF-C is based on the Generalized Jensen-Shannon divergence~\citep{sibson1969information}.
In conjunction with QF-C, we calculate the \textit{average question-form-specific action entropy} (QF-E) as an evaluation metric. QF-E measures the average uncertainty in the model's output when we vary the question forms. \looseness-1

\begin{wrapfigure}{r}{0.31\textwidth}
    \vspace{-0.5em}
    \begin{center}
        % Figure removed
    \end{center}
    \vspace{-0.5em}
    \caption{Two random scenarios of the \texttt{MoralChoice} survey.}\label{fig:example}
\end{wrapfigure}
We administer the survey to $28$ open and closed-source LLMs. The main findings are:\
(1) In general, the responses of LLMs reflect the level of ambiguity in the survey questions. When presented with unambiguous moral scenarios, most LLMs output responses that align with commonsense. 
When presented with ambiguous moral scenarios, most LLMs are uncertain about which action is preferred.
(2) There are exceptions to the general trend. In low-ambiguity scenarios, a subset of models exhibits uncertainty in ``choosing'' the preferred action. 
Analysis suggests that some models are uncertain because of sensitivity to how a question is asked, others are uncertain regardless of how a question is asked.
(3) In high-ambiguity scenarios, a subset of models reflects a clear preference as to which action is preferred. We cluster the models' ``choices'' and find agreement patterns within the group of open-source models and within the group of 
closed-source models. We find especially strong agreement among OpenAI's \texttt{gpt-4}~\citep{openai2023gpt4}, Anthropic's \texttt{claude-v1.1}, \texttt{claude-instant-v1.1}~\citep{bai2022constitutional} and Google's \texttt{text-bison-001} (PaLM 2)~\citep{anil2023palm}.\looseness=-1



\textbf{Contributions. }
The contributions of this paper are:
\begin{itemize}[topsep=-3pt,itemsep=-6pt,leftmargin=25pt]

    \item A statistical methodology for analyzing survey responses from LLM ``respondents''. The method consists of a set of statistical measures and evaluation metrics that quantify the probability of an LLM "making a choice," the associated uncertainty, and the consistency of that choice.
    \Cref{fig:flow_chart} illustrates the application of this method to study moral beliefs encoded in LLMs.
     \item \texttt{MoralChoice}, a survey dataset containing $1767$ moral scenarios and responses from $28$ open and closed source LLMs.
    \item Survey findings on the moral beliefs encoded in the $28$ LLM ``respondents''.
\end{itemize}