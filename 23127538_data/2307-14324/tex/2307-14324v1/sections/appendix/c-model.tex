

\begin{landscape}
\vspace{-7mm}

\section{Model Cards \&  Access/Download Timestamps}
\subsection{Model Cards}
\label{app:model_cards}

\begin{table}[h]
    \centering
    \resizebox{1.2\textwidth}{!}{
    \begin{tabular}{lllrllllll}
        \toprule
        \textbf{Company} & \multicolumn{5}{c}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{Pre-Training}} & \multicolumn{2}{c}{\textbf{Fine-Tuning}}\\
        \cmidrule(l){0-0} \cmidrule(l){2-6} \cmidrule(l){7-8}  \cmidrule(l){9-10}  
                         & Family & Instance & Size & Access & Type & Technique & Corpus & Technique & Corpus\\
        \toprule
        \multirow{5}{*}{\textbf{Google}}        & \multirow{5}{*}{Flan-T5}          & \texttt{flan-T5-small}         & 80M      & HF-Hub     & Enc-Dec   & MLM (Span Corruption)   & C4        & SFT               & Flan 2022 Collec.\\
                                                &                                   & \texttt{flan-T5-base}          & 250M     & HF-Hub     & Enc-Dec   & MLM (Span Corruption)   & C4        & SFT               & Flan 2022 Collec.\\
                                                &                                   & \texttt{flan-T5-large}         & 780M     & HF-Hub     & Enc-Dec   & MLM (Span Corruption)   & C4        & SFT               & Flan 2022 Collec.\\
                                                &                                   & \texttt{flan-T5-xl}            & 3B       & HF-Hub     & Enc-Dec   & MLM (Span Corruption)   & C4        & SFT               & Flan 2022 Collec.\\
                                        
                                                \cmidrule(l){2-6} \cmidrule(l){7-8} \cmidrule(l){9-10}  
                                                & PaLM 2                       & \texttt{text-bison-001} (PaLM 2)             & Unknown      & API     & Unknown  & Mixture of Objectives    & PaLM 2 Corpus        & SFT + Unknown               & Unknown \\
        \toprule
        \multirow{2}{*}{\textbf{Meta}}          & \multirow{1}{*}{OPT-IML-Regular}  & \texttt{opt-iml-1.3B}          & 1.3B     & HF-Hub     & Dec-only  & CLM  & OPT-Mix   & SFT               & OPT-IML Bench \\
                                           
                                                \cmidrule(l){2-6} \cmidrule(l){7-8} \cmidrule(l){9-10}  
                                                & \multirow{1}{*}{OPT-IML-Max}      & \texttt{opt-iml-max-1.3B}      & 1.3B     & HF-Hub     & Dec-only  & CLM  & OPT-Mix   & SFT               & OPT-IML Bench \\
                            
        \toprule
        \multirow{6}{*}{\textbf{BigScience}}          & \multirow{2}{*}{BLOOMZ}    & \texttt{bloomz-560m}          & 560M     & HF-Hub      & Dec-only &  CLM   & BigScienceCorpus  & SFT               & xP3 \\
                                                       &                            & \texttt{bloomz-1b1}          & 1.1B     & HF-Hub      & Dec-only &  CLM   & BigScienceCorpus  & SFT               & xP3 \\
                                                       &                            & \texttt{bloomz-1b7}          & 1.7B     & HF-Hub      & Dec-only &  CLM   & BigScienceCorpus  & SFT               & xP3 \\
                                                       &                            & \texttt{bloomz-3b}            & 3B      & HF-Hub      & Dec-only &  CLM   & BigScienceCorpus  & SFT               & xP3 \\
                                                       &                            & \texttt{bloomz-7b1}          & 7.1B     & HF-Hub      & Dec-only & CLM    & BigScienceCorpus  & SFT               & xP3 \\
                                                    \cmidrule(l){2-6} \cmidrule(l){7-8} \cmidrule(l){9-10}   
                                                    & \multirow{1}{*}{BLOOMZ-MT}    & \texttt{bloomz-7b1-mt}       & 7.1B     & HF-Hub      & Dec-only &  CLM   & BigScienceCorpus  & SFT               & xP3mt \\
                                   
        \multirow{4}{*}{\textbf{OpenAI}}        & \multirow{4}{*}{InstructGPT-3}  & \texttt{text-ada-001}          & 350M$^{1}$ & API       & Dec-only       &  CLM+ &  Unknown         & FeedMe      & Unknown \\
                                                &                                   & \texttt{text-babbage-001}      & 1.0B$^{1}$ & API       & Dec-only       & CLM+  & Unknown          & FeedMe      & Unknown \\
                                                &                                   & \texttt{text-curie-001}        & 6.7B$^{1}$ & API       & Dec-only       & CLM+  & Unknown          & FeedMe      & Unknown \\
                                                &                                   & \texttt{text-davinci-001}      & 175B$^{1}$  & API       & Dec-only       & CLM+  & Unknown          & FeedMe      & Unknown \\
                                                \cmidrule(l){2-6} \cmidrule(l){7-8} \cmidrule(l){9-10}    
                                                & \multirow{4}{*}{InstructGPT-3.5}          & \texttt{text-davinci-002}      & 175B$^{1}$  & API       & Dec-only       &  Unknown   &  Unknown         & FeedMe      & Unknown \\
                                                &                                   & \texttt{text-davinci-003}      & 175B$^{1}$  & API       & Dec-only       & Unknown   & Unknown          & RLHF (PPO)              & Unknown \\ 
                                                &                                   & \texttt{gpt-3.5-turbo}         & Unknown  & API       & Dec-only       & Unknown   &  Unknown         & RLHF              & Unknown \\  
                                                \cmidrule(l){2-6} \cmidrule(l){7-8} \cmidrule(l){9-10}                                                          
                                                & GPT-4                             & \texttt{gpt-4}                 & Unknown   & API       & Unknown       & Unknown   &   Unknown        & RLHF              & Unknown \\              
        \toprule
        \multirow{2}{*}{\textbf{Cohere}}         &\multirow{2}{*}{command}          & \texttt{command-medium}        & 6.067B$^{2}$    & API       & Unknown        & Unknown    & coheretext-filtered & SFT + RLHF? & Unknown \\ 
                                                &   & \texttt{command-xlarge}        & 52.4B$^{2}$     & API       & Unknown         & Unknown   & coheretext-filtered & SFT + RLHF? & Unknown \\ 
                                              
                                                                          
        \toprule
        \multirow{3}{*}{\textbf{Anthropic}}    & \multirow{2}{*}{CAI Instant}   & \texttt{claude-instant-v1.0}  & Unknown        & API        & Unknown        & Unknown  &   Unknown    & SFT + RLAIF &  Partially Known (Constitutions)\\
                                               &                                & \texttt{claude-instant-v1.1}  & Unknown        & API        & Unknown        & Unknown  &   Unknown    & SFT + RLAIF &  Partially Known (Constitutions)\\
                    \cmidrule(l){2-6} \cmidrule(l){7-8} \cmidrule(l){9-10}   
                                
                                               & CAI                 & \texttt{claude-v1.3}               & Unknown     & API        & Unknown        & Unknown   & Unknown     & SFT + RLAIF & Partially Known (Constitutions) \\
         \toprule
         \multirow{3}{*}{\textbf{AI21 Studio}}     & \multirow{2}{*}{Jurassic2 Instruct}  & \texttt{j2-grande-instruct}  & 17B$^{3}$       & API        & Unknown        & Unknown   & Unknown & Unknown & Unknown  \\
                                                    &  & \texttt{j2-jumbo-instruct}   & 178B$^{3}$        & API        & Unknown        & Unknown  & Unknown & Unknown & Unknown \\
                                               
                                                                                     
         \bottomrule
    \end{tabular}
    }
    \vspace{-3mm}
    \caption{Model cards of evaluated LLM with information about model architecture, pre-training and fine-tuning. 
    \small $^{1}$ Estimate based on \url{https://blog.eleuther.ai/gpt3-model-sizes/}. $^{2}$ Estimate based on reported details in \url{https://crfm.stanford.edu/helm/v0.2.2/} (may have changed since then). 
    $^{3}$ Estimate based on reported details of a previous version \url{https://www.ai21.com/blog/introducing-j1-grande} (may have changed from \texttt{j1} to \texttt{j2})}
    \label{tab:models}
\end{table}

{
\small
\quad \vspace{-5mm}\\
\textbf{Abbreviations:}\vspace{-5mm}\\
\begin{itemize}[topsep=-2pt,itemsep=0pt,leftmargin=20pt]
    \item \textbf{SFT:} Supervised fine-tuning on human demonstrations
    \item \textbf{FeedME:} Supervised fine-tuning on human-written demonstrations and on model samples rated 7/7 by human labelers on an overall quality score
    \item \textbf{InstructGPT} models are initialized from GPT-3 models, whose training dataset is composed of text posted to the internet or uploaded to the internet (e.g., books). The internet data that the GPT-3 models were trained on and evaluated against includes: a version of the CommonCrawl dataset filtered based on similarity to high-quality reference corpora,
an expanded version of the Webtext dataset,x
two internet-based book corpora, and
English-language Wikipedia. (Source: \url{https://github.com/openai/following-instructions-human-feedback/blob/main/model-card.md})
\end{itemize}
}

\end{landscape}



\subsection{API Access \& Model Download Timestamps}
\label{app:api_access_times}
To ensure the reproducibility of evaluations, we have recorded timestamps (or timeframes) of API calls to models of OpenAI, Cohere, and Anthropic, and timestamps of model downloads from the HuggingFace Hub \cite{wolf2019huggingface}. In addition, we have recorded exact response timestamps (up to milliseconds) for every acquired sample and can release them upon request.

\begin{table}[h]
    \centering
    \resizebox{0.77\textwidth}{!}{
    \begin{tabular}{llll}
        \toprule
        \textbf{Company} & \textbf{Model ID} & \texttt{MoralChoice-HighAmb} & \texttt{MoralChoice-LowAmb} \\
        \toprule
        \multirow{2}{*}{AI21 Studios}    & \texttt{j2-grande-instruct}   & \texttt{2023-06-\{6,7\}} & \texttt{2023-06-08} \\
         & \texttt{j2-jumbo-instruct}   & \texttt{2023-05-\{9,10,11\}} & \texttt{2023-05-13} \\
         \midrule
        \multirow{3}{*}{Anthropic}  & \texttt{claude-instant-v1.0}   & \texttt{2023-05-\{9,10,11\}} & \texttt{2023-05-12} \\
                                    & \texttt{claude-instant-v1.1}   & \texttt{2023-06-\{7,8\}} & \texttt{2023-06-08} \\
                                    \cmidrule(l){2-4} 
                                    %& \texttt{claude-v1.2}           & \texttt{2023-05-\{\}} & \texttt{2023-05-\{\}} \\
                                    & \texttt{claude-v1.3}           & \texttt{2023-05-\{9,10,11\}} & \texttt{2023-05-12} \\
         \midrule
        \multirow{2}{*}{Cohere}     & \texttt{command-medium}    & \texttt{2023-06-06} & \texttt{2023-06-08} \\
                                    & \texttt{command-xlarge}    & \texttt{2023-05-\{9,10,11\}} & \texttt{2023-05-12} \\
                                %& \texttt{command-medium}    & \texttt{2023-05-\{\}} & \texttt{2023-05-\{\}}  \\
        \midrule
        Google                      & \texttt{text-bison-001} &  \texttt{2023-06-\{7,8\}} & \texttt{2023-06-\{8,9\}} \\
        \midrule
         \multirow{6}{*}{OpenAI}    & \texttt{text-ada-001}       & \texttt{2023-05-\{10,11,12\}} & \texttt{2023-05-13}  \\
                                    & \texttt{text-babbage-001}   & \texttt{2023-05-\{10,11,12\}} & \texttt{2023-05-13}  \\
                                    & \texttt{text-curie-001}     & \texttt{2023-05-\{10,11,12\}} & \texttt{2023-05-13}  \\
                                    \cmidrule(l){2-4} 
                                    & \texttt{text-davinci-001}   & \texttt{2023-05-\{10,11\}} & \texttt{2023-05-13}  \\
                                    & \texttt{text-davinci-002}   & \texttt{2023-05-\{10,11\}} & \texttt{2023-05-13}  \\
                                    & \texttt{text-davinci-003}   & \texttt{2023-05-\{10,11\}} & \texttt{2023-05-13}  \\
                                    & \texttt{gpt-3.5-turbo}      & \texttt{2023-05-\{9,10,11\}} & \texttt{2023-05-\{12,13\}}  \\
                                    \cmidrule(l){2-4} 
                                    & \texttt{gpt-4}             & \texttt{2023-05-\{9,10,11,12\}} & \texttt{2023-05-\{12,13\}}  \\
                                       
         \bottomrule
    \end{tabular}
    }
    \caption{API access times for models from OpenAI, Cohere, Anthropic and AI21 Labs. Timesteps for evaluations on \texttt{MoralChoice-LowAmb} and \texttt{MoralChoice-HighAmb} are shown separately. Timeframes for evaluations on \texttt{MoralChoice-HighAmb} are slightly longer as we acquired two batches of responses (5 sample per prompt variation each) iteratively.}
    \label{tab:api_accesses}
\end{table}

\begin{table}[h]
    \centering
    \resizebox{0.58\textwidth}{!}{
    \begin{tabular}{lll}
        \toprule
        \textbf{Company} & \textbf{Model ID} & \textbf{Download Timestamp} \\
        \toprule
         \multirow{4}{*}{Google} & \texttt{flan-t5-small} & \texttt{2023-05-01} \\
                                 & \texttt{flan-t5-base} & \texttt{2023-05-01} \\
                                 & \texttt{flan-t5-large} & \texttt{2023-05-01} \\
                                 & \texttt{flan-t5-xl} & \texttt{2023-05-01} \\
                         
         \midrule
         \multirow{2}{*}{Meta}  & \texttt{opt-iml-1.3b} & \texttt{2023-05-01} \\
                                \cmidrule(l){2-3} 
                                & \texttt{opt-iml-max-1.3b} & \texttt{2023-05-01} \\
         \midrule
         \multirow{6}{*}{OpenScience}  & \texttt{bloomz-560M}   & \texttt{2023-05-01} \\
                                       & \texttt{bloomz-1.1B}   & \texttt{2023-05-01}\\
                                       & \texttt{bloomz-1.7B}   & \texttt{2023-05-01} \\
                                       & \texttt{bloomz-3B}   & \texttt{2023-05-01}\\
                                       & \texttt{bloomz-7.1B}   & \texttt{2023-05-01} \\
                                       & \texttt{bloomz-7.1B-MT}   & \texttt{2023-05-01} \\
         \bottomrule 
    \end{tabular}
    }
    \caption{Timestamps of Model Downloads from HuggingFace Hub. From this time point, model weights were stored on the cluster and reloaded.}
    \label{tab:hf_downloads}
\end{table}
