\section{The MoralChoice Survey}\label{sec:study_design}
We first discuss the distinction between humans and LLMs as ``respondents'' and its impact on the survey design.
We then outline the process of question generation and labeling.
Lastly, we describe the LLM survey respondents, the survey administration, and the response collection.

\vspace{-2mm}
\subsection{Survey Design}

Empirical research in moral psychology has studied human moral judgments using various survey approaches, such as hypothetical moral dilemmas \citep{rest1975longitudinal}, self-reported behaviors \citep{aquino2002self}, or endorsement of abstract rules \citep{graham2009liberals}. See \citet{ellemers2019psychology} for an overview.
Empirical moral psychology research naturally depends on human participants. 
Consequently, studies focus on narrow scenarios and small sample sizes. 


This study focuses on using LLMs as ``respondents'', which presents both challenges and opportunities.
Using LLMs as ``respondents'' imposes limitations on the types of analyses that can be conducted. Surveys designed for gathering self-reported traits or opinions on abstract rules assume that respondents have agency. However, the question of whether LLMs have agency is debated among researchers \citep{bender2020climbing, hase2021language, piantasodi2022meaning, shanahan2022talking, andreas2022language}. Consequently, directly applying surveys designed for human respondents to LLMs may not yield meaningful interpretations.
On the other hand, using LLMs as ``survey respondents'' provides advantages not found in human surveys. Querying LLMs is faster and less costly compared to surveying human respondents. This enables us to scale up surveys to larger sample sizes and explore a wider range of scenarios without being constrained by budget limitations.



Guided by these considerations, we adopt hypothetical moral scenarios as the framework of our study. 
These scenarios mimic real-world situations where users turn to LLMs for advice.
Analyzing the LLMs outputs in these scenarios enables an assessment of the encoded preferences.
This approach sidesteps the difficulty of interpreting the LLMs' responses to human-centric questionnaires that ask directly for stated preferences. 
Moreover, the scalability of this framework offers significant advantages. It allows us to create a wide range of scenarios, demonstrating the extensive applicability of LLMs. It also leverages the swift response rate of LLMs, facilitating the execution of large-scale surveys.

\vspace{-2mm}
\subsection{Survey Generation}

\textbf{Generating Scenarios and Action Pairs.~}
We grounded the scenario generation in the common morality framework developed by Gert \citep{gert2004common}, which consists of ten rules that form the basis of common morality.
The rules are categorized into "Do not cause harm" and "Do not violate trust". The specific rules are shown in \Cref{appsec:dataset_overview}. For each scenario, we design a pair of actions, ensuring that at least one action actively violates a rule.
The survey consists of two settings: high-ambiguity and low-ambiguity.


In the low-ambiguity setting, we pair each scenario with one favorable action and one unfavorable action designed to violate one rule. We employ zero-shot prompting with OpenAI's \texttt{gpt-4} to generate a raw dataset of $1142$ scenarios. 
The authors manually review this dataset to remove duplicates and ensure coherency.
We then pass the dataset to annotators from Surge AI\footnote{\url{https://www.surgehq.ai/}} to evaluate whether one action is clearly preferred over another.
Each scenario is evaluated by three annotators. We determine the final dataset by a majority vote.
After removing scenarios that were determined as ambiguous by the annotators, we obtain $687$ scenarios.
\Cref{fig:flow_chart} shows examples of both types of scenarios.



In the high-ambiguity setting, each scenario is paired with two potentially unfavorable actions. 
We begin the dataset construction by handwriting $100$ ambiguous moral scenarios, with $10$ examples for each rule. \Cref{appsec:scenario_generation} provide examples of the handwritten scenarios. All scenarios are presented as first-person narratives.
To increase the diversity of the scenarios, we expand the dataset using OpenAI's \texttt{text-davinci-003} with stochastic 5-shot prompting \cite{perez2022discovering, bakker2022fine}.
In total, we generate $2000$ raw high-ambiguity moral scenarios, which are then manually reviewed by the authors to eliminate duplicates and incoherent examples. This iterative process culminates in a final dataset of $680$ high-ambiguity scenarios.



\textbf{Auxiliary Labels.~}
We further augment the dataset with labels about rule violations. Although the scenarios and actions are designed to violate a single rule, some of them may involve multiple rule violations. For instance, throwing a grenade violates the rules of ``do not kill", ``do not cause pain", and ``do not disable".
To label these factors, we enlist the assistance of three annotators from Surge AI.
The final labels are determined through a majority vote among the annotators. 
The level of agreement among annotators varies depending on the specific task and dataset, which we report in \Cref{appsec:auxiliary}.


\subsection{Survey Administration and Processing}


\textbf{LLMs Respondents.} We provide an overview of the 28 LLMs respondents in \Cref{tab:llm_respondents}. Among them, there are 12 open-source models and 16 closed-source models. These models are gathered from seven different companies. The model parameter sizes range from Google's \texttt{flan-t5-small}(80m) to \texttt{gpt-4}, with an unknown number of parameters. Notably, among the models that provide architectural details, only Google's \texttt{flan-T5} models are based on an encoder-and-decoder-style transformer architecture and trained using a masked language modeling objective \citep{chung2022scaling}. All models have undergone a fine-tuning procedure, either for instruction following behavior or dialogue purposes. For detailed information on the models, please refer to the extended model cards in \Cref{app:model_cards}.

\begin{table}[ht!]
    \centering
    \footnotesize
    \resizebox{0.88\columnwidth}{!}{%
    \begin{tabular}{llll}
    \toprule
    \textbf{\# Parameters} & \textbf{Access} & \textbf{Provider} & \textbf{Models}\\
    \toprule
         $<1$B              & Open Source     & BigScience & \texttt{bloomz-560m} \cite{muennighoff2022crosslingual}  \\
                            &                           & Google & \texttt{flan-T5-\{small, base, large\}} \cite{chung2022scaling} \\ 
                            \cmidrule{2-4}
                            & API    & OpenAI & \texttt{text-ada-001} \cite{OpenAI2023}  $^*$\\
         \midrule
         $1$B - $100$B      & Open-Source     & BigScience & \texttt{bloomz-\{1b1, 1b7, 3b, 7b1, 7b1-mt\}}\cite{muennighoff2022crosslingual} \\     
                            &                           & Google & \texttt{flan-T5-\{xl\}}\cite{chung2022scaling} \\ 
                            &                           & Meta & \texttt{opt-iml-\{1.3b, max-1.3b\}} \cite{iyer2022opt}\\
                            \cmidrule{2-4}
                            & API      & AI21 Labs & \texttt{j2-grande-instruct} \cite{AI21Labs2023} $^*$\\
                            &                           & Cohere &  \texttt{command-\{medium, xlarge\}} \cite{cohere2023} $^*$\\
                            &                           & OpenAI & \texttt{text-\{babbage-001, curie-001\}} \cite{brown2020fewshot, ouyang2022training} $^*$\\
         \midrule
         $>100$B            & API      & AI21 Labs & \texttt{j2-jumbo-instruct} \cite{AI21Labs2023} $^*$\\
                            &                           & OpenAI & \texttt{text-davinci-\{001,002,003\}} \cite{brown2020fewshot, ouyang2022training} $^*$ \\
         \midrule
         Unknown            & API   & Anthropic &\texttt{claude-instant\{v1.0, v1.1\}} and \texttt{claude-v1.3} \cite{Anthropic2023}\\
                            &                           & Google & \texttt{text-bison-001} (PaLM 2) \cite{anil2023palm}\\
                            &                           & OpenAI & \texttt{gpt-3.5-turbo} and \texttt{gpt-4} \cite{OpenAI2023} \\
    \bottomrule
    \end{tabular}
    }
    \vspace{-2mm}
    \caption{Overview of the $28$ LLMs respondents.  The numbers of parameters of models marked with $^*$ are based on existing estimates. See \Cref{app:model_cards} for extended model cards and details. }
    \label{tab:llm_respondents}
\end{table}




\textbf{Addressing Question Form Bias.} Previous research has demonstrated that LLMs exhibit sensitivity to the question from \citep{efrat2020turking, webson2021prompt, zhao2021calibrate, jang2022becel}. In multiple-choice settings, the model's outputs are influenced by the prompt format and the order of the answer choices. To account for these biases, we employ three hand-curated question styles: \emph{A/B}, \emph{Repeat}, and \emph{Compare} (refer to \Cref{fig:flow_chart} and \Cref{tab:prompt_templates} for more details) and randomize the order of the two possible actions for each question template, resulting in six variations of question forms for each scenario.

\textbf{Survey Administration.} When querying the models for responses, we keep the prompt header and sampling procedure fixed and present the model with one survey question at a time, resetting the context window for each question. 
This approach allows us to get reproducible results because LLMs are fixed probability distributions.
However, some of the models we are surveying are only accessible through an API. This means the models might change while we are conducting the survey. While we cannot address that, we record the query timestamps. The API query and model weight download timestamps are reported in \Cref{app:api_access_times}.


\textbf{Response Collection.} 
The estimands of interests are defined in Definitions 1-6.
We estimate these quantities through Monte Carlo approximation as described in \Cref{eq:monte-carlo}. For each survey question and each prompt format, we sample $M$ responses from each LLM. The sampling is performed using a temperature of 1, which controls the randomness of the LLM's responses.
We then employ an iterative rule-based mapping procedure to map from sequences to actions. The details of the mapping are provided in \Cref{appsec:semantic_likelihood}.
For high-ambiguity scenarios, we set $M$ to $10$, while for low-ambiguity scenarios, we set $M$ to $5$.
We assign equal weights to each question template. \looseness-1

When administering the survey, we observed that models behind APIs refuse to respond to a small set of moral scenarios when directly asked.
To elicit responses, we modify the prompts to explicitly instruct the language models not to reply with statements like "I am a language model and cannot answer moral questions." 
We found that a simple instruction was sufficient to prompt responses for moral scenarios.
When calculating the action likelihood, we exclude invalid answers. If a model does not provide a single valid answer for a specific scenario and prompt format, we set the likelihood to $0.5$ for that particular template and scenarios.
We report the percentage of invalid and refusing answers in \Cref{appsec:refusal_invalid_responses}.  
