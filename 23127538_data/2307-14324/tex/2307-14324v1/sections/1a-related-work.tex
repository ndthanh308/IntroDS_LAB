\subsection{Related Work}

\textbf{Analyzing the Encoded Preferences in LLMs.} There is a growing interest in analyzing the preferences encoded in LLMs in the context of morality, psychiatry, and politics. \citet{hartmann2023political} examines \texttt{ChatGPT} using political statements relevant to German elections. \citet{santurkar2023whose} compares LLMs' responses on political opinion surveys with US demographics. \citet{coda2023inducing} explores \texttt{GPT-3.5} through an anxiety questionnaire. Our research aligns with studies that analyze LLMs' preferences with respect to moral and social norms. \citet{fraser2022does, abdulhai2022moral} probe LLMs like \texttt{Delphi}\citep{jiang2021delphi} and \texttt{GPT-3}\citep{brown2020fewshot}, using ethics questionnaires such as the Moral Foundation Questionnaire~\citep{graham2009liberals, graham2011mapping} or Shweder’s “Big Three” Ethics~\citep{shweder2013big}. However, it's uncertain whether LLMs' responses on ethics questionnaires, which measure behavioral intentions, reflect actual preferences in context-specific decision scenarios. We differ by employing hypothetical scenarios to unveil moral preferences, rather than directly querying for moral preferences.

\textbf{LLMs in Computational Social Science.}  While we treat LLMs as independent "survey respondents", there is a growing literature treating LLMs as simulators of human agents conditioned on socio-demographic backgrounds \citep{argyle2022out, park2022social, aher2022using, horton2023large, park2023generative}. In the context of morality, \citet{simmons2022moral} found that \texttt{GPT-3} replicates moral biases when presented with political identities. In this study, we focus on the encoded moral preferences in LLMs without treating them as simulators of human agents.

\textbf{Aligning LLMs with Human Preferences.} Advances in LLMs \citep{brown2020fewshot, chowdhery2022palm, bubeck2023sparks, openai2023gpt4, anil2023palm} have sparked growing efforts to align these models with human preferences \citep{amodei2016concrete,ziegler2019fine,stiennon2020learning,solaiman2021process, askell2021general,hendrycks2021unsolved,bai2022constitutional,glaese2022improving,ganguli2023capacity,ganguli2022red}. These efforts include fine-tuning LLMs with specific moral concepts \citep{hendrycks2021ethics}, training LLMs to predict human responses to moral questions \citep{forbes2020social,emelin2021moral,lourie2021scruples,jiang2021delphi}, and employing multi-step inference techniques to improve agreement between LLMs and human responses \citep{jin2022make,nie2023moca}. In contrast, this work focuses on evaluating the beliefs encoded in LLMs, rather than aligning LLMs with specific beliefs or norms through fine-tuning or inference techniques.