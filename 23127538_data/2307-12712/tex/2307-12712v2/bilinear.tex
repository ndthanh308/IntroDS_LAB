\RequirePackage{amssymb}
\documentclass{article}
\usepackage{fullpage}
%
%
%
%
%
\usepackage{amsmath,amsthm}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amsfonts}
\usepackage{arydshln} %
\usepackage{xcolor}
\usepackage{enumitem}

%
%
%
%
%
%
%
%
%
%
%
%

%


%
%
%
%
%


%

\usepackage{xspace}
\usepackage{booktabs}
\usepackage[symbol]{footmisc}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\usepackage{algorithm}
\usepackage[noend]{algcompatible}
\usepackage{xifthen} %
\newcommand{\To}{\textbf{to}\xspace}
\newcommand{\AlgAnd}{\textbf{and}\xspace}
\newcommand{\DownTo}{\textbf{down-to}\xspace}
\newcommand{\algorithmicreturn}{\textbf{return}}
\newcommand{\RETURN}{\STATE\algorithmicreturn{}\xspace}
\algnewcommand{\IfThen}[2]{%
  \State \algorithmicif\ #1\ \algorithmicthen\ #2}
\algnewcommand{\IfThenEnd}[2]{%
  \State \algorithmicif\ #1\ \algorithmicthen\ #2\ \algorithmicend\ \algorithmicif}
\algnewcommand{\IfThenElse}[3]{%
  \State \algorithmicif\ #1\ \algorithmicthen\ #2\ \algorithmicelse\ #3}
\algnewcommand{\ForDoEnd}[3][]{%
  \ifthenelse{\isempty{#1}}%
  {\State \algorithmicfor\ #2\ \algorithmicdo\ #3\ \algorithmicend\ \algorithmicfor}
  {\State\label{#1}\algorithmicfor\ #2\ \algorithmicdo\ #3\ \algorithmicend\ \algorithmicfor}
}
\renewcommand{\algorithmicrequire}{{\textbf{Input:}}}
\renewcommand{\algorithmicensure}{{\textbf{Output:}}}
\algnewcommand\algorithmicreadonly{\textbf{Read-only:}}
\algnewcommand\READONLY{\item[\algorithmicreadonly]}%

\newcommand{\D}{\ensuremath{\mathbb{D}}\xspace}
\newcommand{\F}{\ensuremath{\mathbb{F}}\xspace}
\newcommand{\M}{\ensuremath{\mathfrak{M}}}
\newcommand{\bnd}[2]{\ensuremath{#1\mathopen{}(#2)\mathclose{}}}
\newcommand{\bnddisplay}[2]{\ensuremath{#1\mathopen{}\left(#2\right)\mathclose{}}}
\newcommand{\softoh}[1]{\bnd{\tilde{\mathcal{O}}}{#1}}
\newcommand{\bigO}[1]{\bnd{\mathcal{O}}{#1}}
\newcommand{\bigOdisplay}[1]{\bnddisplay{\mathcal{O}}{#1}}


\newcommand{\threshold}{\ensuremath{\text{Threshold}}}
\DeclareMathOperator{\me}{\,\ensuremath{\mathrel{\textrm{--}}=}\,}
\DeclareMathOperator{\pe}{\,\ensuremath{\mathrel{+}=}\,}
\DeclareMathOperator{\fe}{\,\ensuremath{\mathrel{{\ast}}=}\,}
\DeclareMathOperator{\de}{\,\ensuremath{\mathrel{/}=}\,}
\newcommand{\rev}[1]{\ensuremath{{\operatorname{rev}}\left(#1\right)}}
\newcommand{\Toeplitz}[1]{\ensuremath{\operatorname{\mathcal{T}_{#1}}}}
\newcommand{\Circulant}[1]{\ensuremath{\operatorname{\mathscr{C}_{#1}}}}
\newcommand{\random}{\stackrel{\$}{\leftarrow}}

%
\newcommand\DFT{\mathsf{DFT}}
\newcommand\brDFT{\mathsf{brDFT}}
\newcommand\brTFT{\mathsf{brTFT}}
\newcommand\partTFT{\mathsf{partTFT}}

%
\makeatletter
\DeclareRobustCommand{\cev}[1]{%
  {\mathpalette\do@cev{#1}}%
}
\newcommand{\do@cev}[2]{%
  \vbox{\offinterlineskip
    \sbox\z@{$\m@th#1 x$}%
    \ialign{##\cr
      \hidewidth\reflectbox{$\m@th#1\vec{}\mkern4mu$}\hidewidth\cr
      \noalign{\kern-\ht\z@}
      $\m@th#1#2$\cr
    }%
  }%
}
\makeatother
%
\newcommand{\mat}[1]{\textbf{\ensuremath{#1}}}
\newcommand{\Transpose}[1]{{{\mat{#1}}^{\intercal}}\xspace}
\newcommand{\gTranspose}[2]{{{\mat{\begingroup\setlength\arraycolsep{#1}#2\endgroup}}^{\intercal}}\xspace}


\usepackage{fancyvrb}



%
\newenvironment{smatrix}{\left[\begin{smallmatrix}}{\end{smallmatrix}\right]}
%
%
\def\matrixsize#1#2{{{#1}\times{#2}}}
\def\MatrixProduct#1#2{{\mat{#1}\cdot\mat{#2}}}
%
\def\triadone{brown}
\def\triadtwo{red}
\def\triadthree{blue}
\def\triadfour{green}
\def\triadfive{purple}
\def\triadsix{dimgray}
\def\triadseven{chocolate}

%
\usepackage{pifont}
\usepackage{savesym}
\savesymbol{checkmark}
\newcommand{\gcheck}{{\color{darkgreen}\checkmark}}
\usepackage{dingbat}
\newcommand{\xyes}{{\gcheck}}
\newcommand{\xno}{{\color{darkred}\ding{55}}}
\newcommand{\xquestion}{\color{darkred}{\fontfamily{cyklop}\selectfont\textit{?}}}

\newcommand{\LLTr}{\ensuremath{{\text{Low}}}}
\newcommand{\Low}[1]{\ensuremath{{\LLTr\left({\mat{#1}}\right)}}}

%

\newcommand{\MUL}{\normalfont{\textbf{MUL}}\xspace}
\newcommand{\ADD}{\normalfont{\textbf{ADD}}\xspace}
\newcommand{\SCA}{\normalfont{\textbf{SCA}}\xspace}
%

%
\title{In-place accumulation of fast multiplication formulae}

%
%
%
%
%
%
%
%

\author{Jean-Guillaume Dumas\footnote{
  {Universit\'e Grenoble Alpes}.
  {Laboratoire Jean Kuntzmann, CNRS, UMR 5224}.
  {150 place du Torrent, IMAG - CS 40700},
  {38058 Grenoble, cedex 9}
  {France}.
\href{mailto:Jean-Guillaume.Dumas@univ-grenoble-alpes.fr,Bruno.Grenet@univ-grenoble-alpes.fr}{\{firstname.lastname\}@univ-grenoble-alpes.fr}}
\and{Bruno Grenet}\footnotemark[1]}


%
\usepackage{color,svgcolor}
\usepackage[plainpages=true]{hyperref}



%

\makeatletter
\hypersetup{%
	pdftitle={\@title},
	pdfauthor={Jean-Guillaume Dumas and Bruno Grenet},
	breaklinks=true,
	colorlinks=true,
	linkcolor=darkred,
	citecolor=blue,
	urlcolor=darkgreen,
    hypertexnames=false,
}
\makeatother
%
\usepackage[capitalize]{cleveref}
\newcommand{\algabbrev}{Alg}
\newcommand{\algname}{{\algabbrev}.}
\newcommand{\algsname}{{\algabbrev}s.}
\newcommand{\Algname}{Algorithm}
\newcommand{\Algsname}{{\Algname}s}
\crefname{algorithm}{\algname}{\algsname}
\Crefname{algorithm}{\Algname}{\Algsname}
\crefname{proposition}{Prop.}{Props.}
\Crefname{proposition}{Proposition}{Propositions}

%
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{definitions}[theorem]{Definitions}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{facts}[theorem]{Facts}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
%


%

%
\begin{document}
\maketitle
%
\begin{abstract}
This paper deals with simultaneously fast and in-place algorithms
for formulae where the result has to be linearly accumulated:
some of the output variables are also input variables,
linked by a linear dependency.
Fundamental examples include the in-place accumulated multiplication
of polynomials or matrices, $C\pe{AB}$.
The difficulty is to combine in-place computations with fast
algorithms:
those usually come at the expense of (potentially large) extra
temporary space, but with accumulation the output variables are not
even available to store intermediate values.
%
We first propose a novel automatic design of fast and in-place
accumulating algorithms for any bilinear formulae (and thus for
polynomial and matrix multiplication) and then extend it to any linear
accumulation of a collection of functions.
For this, we relax the in-place model to any
algorithm allowed to modify its inputs, provided that those are
restored to their initial state afterwards.
%
%
%
%
This allows us, in fine, to derive unprecedented in-place accumulating
algorithms for fast polynomial multiplications and for Strassen-like
matrix multiplications.
\end{abstract}
%
%
%
%
\section{Introduction}
Multiplication is one of the most fundamental arithmetic operations in
computer science and in particular in computer algebra and symbolic
computation.
%
In terms of arithmetic operations, for instance, from the work
of~\cite{MoenckBorodin1972,Kung1974,Strassen:1969:GENO}, many
sub-quadratic (resp. sub-cubic) algorithms
were developed for polynomial (resp. matrix) multiplication.
%
But these fast algorithms usually come at the expense
of (potentially large) extra temporary space to perform the
computation. On the contrary, classical, quadratic (resp. cubic)
algorithms, when computed sequentially, quite often require very few
(constant) extra registers.
%
Further work then proposed simultaneously ``fast'' and ``in-place''
algorithms, for matrix or polynomial
operations~\cite{jgd:2009:WinoSchedule,Roche:2009:spacetime,Harvey:2010:issactft,Giorgi:2019:issac:reductions,Giorgi:2020:issac:inplace}.

We here extend the latter line of work for
\emph{accumulating} algorithms.
Actually, one of the main ingredient of the latter line of work is to
use the (free) space of the output as intermediate storage.
But when the result has to be accumulated, i.e., if the output is also
part of the input, this free space does not even exist.
%
To be able to design accumulating in-place algorithms we thus relax the in-place
model to allow algorithms to also modify their input, therefore to use
them as intermediate storage,
\emph{provided that they are restored to their initial state after
  completion of the procedure}.
This is in fact a natural possibility in many programming environments.
Furthermore, this restoration allows for recursive combinations of
such procedures, as the (non concurrent) recursive calls will not
mess-up the state of their callers.
%
We thus propose a generic technique transforming any bilinear
algorithm into an in-place algorithm under this model.
This then directly applies to accumulating polynomial and matrix
multiplication algorithms, including fast ones.
Further, the technique actually generalizes to any linear
accumulation, i.e. not only bilinear formulae, provided that the input
of the accumulation can be itself reversibly computed in-place
(therefore also potentially in-place of some of its own input if
needed).

Next, we give our model for in-place computations
and recall classical in-place algorithms
in~\cref{ssec:inplace}.
We then detail in~\cref{sec:linacc} our novel technique for in-place
accumulation.
Finally we apply this technique and further optimizations in order to
derive new fast and in-place algorithms for the accumulating
multiplication of matrices, \cref{sec:strassen}, and of polynomials,
\cref{sec:inpaccpol}.

\section{In-place model with mutable input}\label{ssec:inplace}
%
We here call \emph{in-place}, an algorithm using only
\textbf{the space of its inputs, its outputs, and at most $\bigO{1}$ extra
  space}; the algorithm can also modify its (mutable) inputs,
\textbf{provided that these inputs are restored to their initial
  state} afterwards.
This is a less powerful model than when the input is purely read-only,
but it turns out to be crucial in our case, especially when we have
accumulation operations.
%
Indeed, the algorithms we describe are \emph{in-place with
  accumulation}, and the
archetypical example is a multiply-accumulate operation $c \pe a\times{b}$.
For such an algorithm, the condition is that $a$ and $b$ are restored
to their initial states at the end of the computation, while $c$ (which
is also part of the input) is replaced by $c+ab$.
%
%
%
%
%
%
%
%
Also, as a variant, by \emph{over-place}, we mean an algorithm where
the output replaces (parts of) its input (e.g., like
$\vec{a}\leftarrow{b}{\cdot}\vec{a}$).
%
%
In the following we signal by a ``\algorithmicreadonly'' tag the parts
of the input that the algorithm is not allowed to modify (the other
parts are mutable as long as they are restored).
%
%

For recursive algorithms, some space may be required to store the recursive
call stack (this space is bounded by the recursion depth of the
algorithms, and managed in practice by the compiler).
In our complexity summaries (\Cref{tab:kara,tab:fft}
in~\cref{app:redpolmul}), the space complexity includes the
stack. Nonetheless, we call \emph{in-place} a recursive algorithm
whose only extra space is the call stack.
%
%
%
For more details on these models, we refer
to~\cite{Buhrman:2014:STOC:catalytic,Roche:2009:spacetime,Giorgi:2019:issac:reductions}.
%
We end this section by recalling in~\cref{alg:classicmul}, that
classical algorithms for matrix or polynomial operations can be
performed strictly in-place.

\begin{algorithm}[htbp]\caption{Quadratic/cubic in-place accumulating
    polynomial/matrix multiplication}\label{alg:classicmul}
\begin{minipage}{.475\columnwidth}
\begin{algorithmic}[1]
\REQUIRE $A$, $B$, $C$ polynomials of degrees $m$, $n$, $m+n$.
\READONLY $A,B$.
\ENSURE $C(X)\pe{A(X)B(X)}$
\FOR{$i,j$}
\STATE $C[i{+}j]\pe{A[i]B[j]}$;
\ENDFOR
\end{algorithmic}
\end{minipage}\hfill
\begin{minipage}{.475\columnwidth}
\begin{algorithmic}[1]
\REQUIRE $A$, $B$, $C$ matrices of dimensions $m{\times}\ell$,
$\ell{\times}n$, $m{\times}n$.
\READONLY $A,B$.
\ENSURE $C\pe{AB}$
\FOR{$i,j,k$}
\STATE $C_{ij}\pe{A_{ik}B_{kj}}$;
\ENDFOR
\end{algorithmic}
\end{minipage}
\end{algorithm}


%
\section{In-place linear accumulation}\label{sec:linacc}
%
Karatsuba polynomial
multiplication~\cite{Karatsuba:1963:multiplication}
and Strassen matrix multiplication~\cite{Strassen:1969:GENO}
are famous optimizations of bilinear formulae on their inputs: results
are linear combinations of products of bilinear combinations of the inputs.
%
To compute recursively such a formula in-place, we perform
each product one at a time. For each product, both factors are then
linearly combined in-place into one of the entry beforehand and
restored afterwards. The product of both entries is at that point
accumulated in one part of the output and then distributed to the
other parts.
The difficulty is to perform this distribution in-place, {\em without
recomputing the product}. Our idea is to presubtract one output from the
other, then accumulate the product to one output, and finally re-add the
newly accumulated output to the other one: overall both outputs just
have accumulated the product, in-place. Potential constant factors can
also be dealt with pre-divisions and post-multiplications.
Basically we need two kind of in-place operations, and their
combinations.
First, as shown in~\cref{eq:basemul}, an in-place accumulation of a
quantity multiplied by a (known in advance) invertible constant:
\begin{equation}\label{eq:basemul}
%
\left\lbrace{}c\de\mu;~c\pe m;~c\fe\mu;\right\rbrace~\text{computes
  in-place}~c\gets{c+\mu\cdot{m}}.
\end{equation}
Second, as shown in~\cref{eq:basedist}, an in-place distribution of
the same quantity, without recomputation, to several outputs:
\begin{equation}\label{eq:basedist}
\left\lbrace{}d\me{c};~c\pe{m};~d\pe{c};\right\rbrace~\text{computes
  in-place}~\left\{\begin{array}{@{}l@{\,}l@{}}
c&\gets{c+m};\\
d&\gets{d+m}.\\
\end{array}
\right.
\end{equation}


\Cref{ex:bilin} shows how to combine several of these operations,
while also linearly combining parts of the input.
\begin{example}\label{ex:bilin}
  Suppose, that for some inputs/outputs $a,b,c,d,r,s$ one wants to compute an
  intermediate product $p=(a+3b)*(c+d)$ only once and then distribute
  and accumulate that product to two of its outputs (or results),
  such that we have both $r\gets{r+5p}$ and $s\gets{s+2p}$.
  To perform this in-place, first accumulate $a\pe{3b}$ and $c\pe{d}$,
  then pre-divide $r$ by $5$, as in~\cref{eq:basemul}.
  Now we directly have $p=ac$ and it can be computed once,
  and then accumulated to $r$, and to $s$, if the latter is prepared:
  divide it by $2$, and pre-subtract $r$. This is
  $s\de{2}$, $s\me{r}$ followed by $r\pe{ac}$. After this, we can
  reciprocate (or unroll) the precomputations: this distributes
  the product to the other result and restores the read-only inputs to
  their initial state (for $s$, another possibility is to directly
  pre-subtract $2r$ and to post-add $2r$).
  This is summarized as:
  \fbox{\renewcommand{\baselinestretch}{0.5}
      \ensuremath{\left\lbrace\begin{aligned}
            a\pe{3b}; &\quad c\pe{d}; & r\de{5}\phantom{r};\\[-5pt]
            s\me{2r}; &\quad r\pe{ac}; & s\pe{2r}; \\[-5pt]
            r\fe{5}\phantom{r}; &\quad c\me{d}; & a\me{3b};
          \end{aligned}\right\rbrace
        \begin{array}{l}
          \text{computes in-place}\\[-0pt]
          \left\lbrace\begin{aligned}
            r& \gets{r+5(a+3b)(c+d)};\\[-5pt]
            s& \gets{s+2(a+3b)(c+d)}.\\[-1pt]
          \end{aligned}\right.
        \end{array}
      }}
\end{example}

\Cref{alg:bilin} shows how to implement this in general, taking into
account the constant (or read-only) multiplicative coefficients of all
the linear combinations. We suppose that inputs are in three distinct
sets: left-hand sides, $\vec{a}$, right-hand sides, $\vec{b}$, and those
accumulated to the results, $\vec{c}$.
We denote by $\odot$ the point-wise multiplications of
left-hand sides by right-hand sides.
Then~\cref{alg:bilin} computes $\vec{c}\pe\mat{\mu}\vec{m}$, for
$\vec{m}=(\mat{\alpha}\vec{a})\odot(\mat{\beta}\vec{b})$, with
$\mat{\alpha}$, $\mat{\beta}$ and $\mat{\mu}$ matrices of
constants.

\begin{algorithm}[htbp]
  \caption{In place bilinear formula}\label{alg:bilin}
  \begin{algorithmic}[1]\small
    \REQUIRE $\vec{a}\in\F^m$, $\vec{b}\in\F^n$, $\vec{c}\in\F^s$;
    $\mat{\alpha}\in\F^{t{\times}m}$, $\mat{\beta}\in\F^{t{\times}n}$,
    $\mat{\mu}\in\F^{s{\times}t}$, without zero-rows in $\alpha$,
    $\beta$, $\mu$.
    \READONLY$\mat{\alpha},\mat{\beta},\mat{\mu}$.
    \ENSURE $\vec{c}\pe\mat{\mu}\vec{m}$, for
    $\vec{m}=(\mat{\alpha}\vec{a})\odot(\mat{\beta}\vec{b})$.
    \FOR{$\ell=1$ \To $t$}
    \STATE\label{lin:alpha}Let $i$ s.t. $\alpha_{\ell,i}\neq{0}$;
    $a_i\fe\alpha_{\ell,i}$;
    \ForDoEnd[lin:foralpha]{$\lambda=1$ \To $m$, $\lambda\neq{i}$,
      $\alpha_{\ell,\lambda}\neq{0}$ }{ $a_i\pe\alpha_{\ell,\lambda}a_\lambda$}
    \STATE\label{lin:beta}Let $j$ s.t. $\beta_{\ell,j}\neq{0}$;
    $b_j\fe\beta_{\ell,j}$;
    \ForDoEnd[lin:forbeta]{$\lambda=1$ \To $n$, $\lambda\neq{j}$,
      $\beta_{\ell,\lambda}\neq{0}$}{ $b_j\pe\beta_{\ell,\lambda}b_\lambda$}
    \STATE\label{lin:mu}Let $k$ s.t. $\mu_{k,\ell}\neq{0}$;
    $c_k\de\mu_{k,\ell}$;
    \ForDoEnd[lin:formu]{$\lambda=1$ \To $s$, $\lambda\neq{k}$,
      $\mu_{\lambda,\ell}\neq{0}$}{$c_\lambda\me\mu_{\lambda,\ell}{c_k}$}
    \STATE\label{lin:product}$c_k\pe{a_i\cdot{b_j}}$\hfill\COMMENT{This is the product $m_\ell$, computed only once}
    \ForDoEnd[lin:distribmu]{$\lambda=1$ \To $s$, $\lambda\neq{k}$,
      $\mu_{\lambda,\ell}\neq{0}$}{$c_\lambda\pe\mu_{\lambda,\ell}{c_k}$}
    \hfill\COMMENT{undo~\ref{lin:formu}}
    \STATE\label{lin:fmuk}$c_k\fe\mu_{k,\ell}$;\hfill\COMMENT{undo~\ref{lin:mu}}
    \ForDoEnd[lin:distribbeta]{$\lambda=1$ \To $n$, $\lambda\neq{j}$,
      $\beta_{\ell,\lambda}\neq{0}$}{$b_j\me\beta_{\ell,\lambda}b_\lambda$}
    \hfill\COMMENT{undo~\ref{lin:forbeta}}
    \STATE\label{lin:dbetaj}$b_j\de\beta_{\ell,j}$;\hfill\COMMENT{undo~\ref{lin:beta}}
    \ForDoEnd[lin:distribalpha]{$\lambda=1$ \To $m$, $\lambda\neq{i}$,
      $\alpha_{\ell,\lambda}\neq{0}$}{$a_i\me\alpha_{\ell,\lambda}a_\lambda$}
    \hfill\COMMENT{undo~\ref{lin:foralpha}}
    \STATE\label{lin:dalphai}$a_i\de\alpha_{\ell,i}$;\hfill\COMMENT{undo~\ref{lin:alpha}}
    \ENDFOR
    \RETURN $\vec{c}$.
  \end{algorithmic}
\end{algorithm}

\begin{remark}\label{rk:parallelism}
  \Cref{lin:alpha,lin:foralpha,lin:beta,lin:forbeta,lin:mu,lin:formu,lin:distribmu,lin:fmuk,lin:distribbeta,lin:dbetaj,lin:distribalpha,lin:dalphai}   of~\cref{alg:bilin} are acting on independent parts of the
  input, $\vec{a}$ and $\vec{b}$, and of the output $\vec{c}$.
  If needed they could therefore be computed
  in parallel
  or in different orders,
  and even potentially grouped or factorized across the main loop (on $\ell$).
\end{remark}

To simplify the counting of operations,
we denote by \ADD both the addition or subtraction of elements, $\pe$
or $\me$; by \MUL the (tensor) product of elements, $\odot$;
and by \SCA the scaling by constants, $\fe$ or $\de$.
%
We also denote by $\#x$ (resp. $\sharp{x}$) the number of
non-zero (resp. $\not\in\{0,1,-1\}$) elements in a matrix $x$.

\begin{theorem}\label{thm:bilin}
\Cref{alg:bilin} is correct, in-place, and requires
$t$ \MUL,
$2(\#\alpha+\#\beta+\#\mu)-5t$ \ADD and
$2(\sharp\alpha+\sharp\beta+\sharp\mu)$ \SCA operations.
\end{theorem}
\begin{proof}
First, as the only used operations ($\pe$, $\me$, $\fe$, $\de$) are
in-place ones, the algorithm is in-place.
Second, the algorithm is correct both for the input and the
output:
the input is well restored, as
$(\alpha_{\ell,i}a_i+\sum\alpha_{\ell,\lambda}a_\lambda-\sum\alpha_{\ell,\lambda}a_{\lambda})/\alpha_{\ell,i}=a_i$
and
$(\beta_{\ell,j}b_j+\sum\beta_{\ell,\lambda}b_\lambda-\sum\beta_{\ell,\lambda}b_\lambda)/\beta_{\ell,j}=b_j$;
the output is correct as
$c_\lambda-\mu_{\lambda,\ell}c_k/\mu_{k,\ell}+\mu_{\lambda,\ell}(c_k/\mu_{k,\ell}+a_ib_j)=c_\lambda+\mu_{\lambda,\ell}a_ib_j$
and
$(c_k/\mu_{k,\ell}+a_ib_j)\mu_{k,\ell}=c_k+\mu_{k,\ell}a_ib_j$.
Third, for the number of operations,
\cref{lin:alpha,lin:foralpha} require one multiplication by a constant for each
non-zero element $a_{\lambda}$ in the row and one less addition.
But multiplications and divisions by $1$ are no-op, and by $-1$ can be
dealt with subtraction. This is $\#\alpha-t$ additions
and $\sharp\alpha$ constant multiplications.
\cref{lin:beta,lin:forbeta} (resp. \cref{lin:mu,lin:formu}) are
similar for each non-zero element in $b_{\lambda}$ (resp. in
$\mu$). Finally \cref{lin:product} performs $t$ multiplications of
elements and $t$ additions. The remaining lines double the number of
\ADD and \SCA.
This is $t+2(\#\alpha+\#\beta+\#\mu-3t)=2(\#\alpha+\#\beta+\#\mu)-5t$ \ADD.
\end{proof}

\begin{remark} Similarly, slightly more generic accumulation
  operations of the form
  $\vec{c}\gets\vec{\gamma}\odot\vec{c}+\mat{\mu}\vec{m}$, for a
  vector $\gamma\in\F^{s}$, can also be computed in-place: precompute
  first $\vec{c}\gets\vec{\gamma}\odot\vec{c}$, then
  call~\cref{alg:bilin}.
\end{remark}

For instance, to use~\cref{alg:bilin} with matrices or polynomials,
each product $m_\ell$ is in fact computed recursively.
Further, in an actual implementation of a fixed formula, one can
combine more efficiently the pre- and post-computations over
the main loop on $\ell$, as in~\cref{rk:parallelism}.
See~\cref{sec:strassen,sec:inpaccpol} for examples of recursive
calls, together with sequential optimizations and combinations.

%
%
In fact the method for accumulation, computing each bilinear
multiplication once is generalizable.
With the notations of~\cref{alg:bilin}, any algorithm of the form
$\vec{c}\pe\mat{\mu}\vec{m}$ can benefit from this technique,
provided that each $m_j$ can be obtained from a function
that can be computed in-place.
Let $F_j:\Omega\to\F$ be such a function on some inputs from a
space $\Omega$, for which an in-place algorithm exists.
Then we can accumulate it in-place, \emph{if it satisfies the following
constraint}, that it is not using its output space as an available
intermediary memory location.
Further, this function can be in-place in different models:
it can follow our model of~\cref{ssec:inplace}, if there is a way to
put its input back into their initial states, or some other model,
again provided that it follows the above constraint.

Then, the idea is just to keep from~\cref{alg:bilin}
the~\cref{lin:mu,lin:formu,lin:product,lin:distribmu,lin:fmuk},
replacing~\cref{lin:product} by the in-place call to $F_j$,
potentially surrounding that call by manipulations on the inputs of
$F_j$ (just like the one performed on $\vec{a}$ and $\vec{b}$
in~\cref{alg:bilin}).
We have shown:
\begin{theorem}\label{thm:general}
Let $\vec{c}\in\F^s$ and $\mat{\mu}\in\F^{s{\times}t}$, without zero-rows.
Let $\vec{F}=(F_j:\Omega\to\F)_{j=1..t}$ be a collection of functions
and $\omega\in\Omega$.
If all these functions are computable in-place, without using their
output space as an %
intermediary memory location,
then there exists an in-place algorithm computing
$\vec{c}\pe\mat{\mu}\vec{F}(\omega)$ in-place, requiring a single call to
each $F_j$,
together with
$(2\#\mu-t)$ \ADD and
$2\sharp\mu$ \SCA ops.
\end{theorem}

This is close to catalytic machines and transparent
space~\cite{Buhrman:2014:STOC:catalytic}; but here we allow
only the input and output as catalytic space, and we do preserve the
(not in-place) time complexity bound, up to a (quasi)-linear overhead.

We give examples of the application of the generalized method
of~\cref{thm:general} to non bilinear formulae in~\cref{app:aat}.

%
\section{In-place Strassen matrix multiplication with accumulation}\label{sec:strassen}

Considered as~${\matrixsize{2}{2}}$ matrices, the matrix
product with accumulation ~${\mat{C}\pe\MatrixProduct{A}{B}}$ could be computed using
Strassen-Winograd (S.-W.) algorithm by performing the computations of the following~\cref{eq:StrassenWinogradMultiplicationAlgorithm}.
\begin{gather}
\begin{array}{ll}
\mathcolor{\triadone}{\rho_{1}}\gets{\mathcolor{\triadone}{a_{11}}\mathcolor{\triadone}{b_{11}}},
\quad
\mathcolor{\triadthree}{\rho_{3}}\gets{(\mathcolor{\triadthree}{-a_{11}-a_{12}+a_{21}+a_{22}})\mathcolor{\triadthree}{b_{22}}},
\\
\mathcolor{\triadtwo}{\rho_{2}}\gets{\mathcolor{\triadtwo}{a_{12}}\mathcolor{\triadtwo}{b_{21}}},
\quad
\mathcolor{\triadfour}{\rho_{4}}\gets{\mathcolor{\triadfour}{a_{22}}(\mathcolor{\triadfour}{-b_{11}+b_{12}+b_{21}-b_{22}})},
\\
\mathcolor{\triadfive}{\rho_{5}}\gets{(\mathcolor{\triadfive}{a_{21}+a_{22}})(\mathcolor{\triadfive}{-b_{11}+b_{12}})},
\quad
\mathcolor{\triadsix}{\rho_{6}}\gets{(\mathcolor{\triadsix}{-a_{11}+a_{21}})(\mathcolor{\triadsix}{b_{12}-b_{22}})},
\\
\mathcolor{\triadseven}{\rho_{7}}\gets{(\mathcolor{\triadseven}{-a_{11}+a_{21}+a_{22}})(\mathcolor{\triadseven}{-b_{11}+b_{12}-b_{22}})}
\end{array}\nonumber\\
\label{eq:StrassenWinogradMultiplicationAlgorithm}
\begin{smatrix} c_{11} &c_{12} \\ c_{21} &c_{22} \end{smatrix}
\pe
\begin{smatrix}
\mathcolor{\triadone}{\rho_{1}} + \mathcolor{\triadtwo}{\rho_{2}} &
\mathcolor{\triadone}{\rho_{1}} - \mathcolor{\triadthree}{\rho_{3}} + \mathcolor{\triadfive}{\rho_{5}} - \mathcolor{\triadseven}{\rho_{7}}\\
\mathcolor{\triadone}{\rho_{1}} + \mathcolor{\triadfour}{\rho_{4}} +
\mathcolor{\triadsix}{\rho_{6}} - \mathcolor{\triadseven}{\rho_{7}} &
\mathcolor{\triadone}{\rho_{1}}+\mathcolor{\triadfive}{\rho_{5}} +
\mathcolor{\triadsix}{\rho_{6}} - \mathcolor{\triadseven}{\rho_{7}}
\end{smatrix}
\end{gather}
This algorithm uses $7$ multiplications of half-size matrices and $24+4$
additions (that can be factored into only $15+4$,
see~\cite{Winograd:1977:complexite}:
$4$ involving $A$, $4$ involving $B$ and $7$ involving the products,
plus $4$ for the accumulation).
This can be used recursively on matrix blocks, halved at each
iteration, to obtain a sub-cubic algorithm. To save on operations, it
is of course interesting to compute the products only once, that is
store them in extra memory chunks.
%
%
%
To date, up to our knowledge, the best versions that reduced this
extra memory space (also overwriting the
input matrices but not putting them back in place) were proposed
in~\cite{jgd:2009:WinoSchedule}:
their best sub-cubic accumulating product used $2$ temporary blocks
per recursive level, thus a total of extra memory required to
be~$\frac{2}{3}n^2$.
%
With~\cref{alg:bilin} we instead obtain a sub-cubic algorithm for
accumulating matrix multiplication with $\bigO{1}$ extra space requirement.
%
From~\cref{eq:StrassenWinogradMultiplicationAlgorithm} indeed (see
also the representation in~\cite{Bshouty:1995:minwinoadd}), we can extract
the matrices $\mu$, $\alpha$ and $\beta$ to be used in \cref{alg:bilin} as
given in~\cref{eq:alphabetamu}.
%
\begin{equation}\label{eq:alphabetamu}
\mu=\begin{smatrix}
\mathcolor{\triadone}{1}&\mathcolor{\triadtwo}{1}&\mathcolor{\triadthree}{0}&\mathcolor{\triadfour}{0}&\mathcolor{\triadfive}{0}&\mathcolor{\triadsix}{0}&\mathcolor{\triadseven}{0}\\
\mathcolor{\triadone}{1}&\mathcolor{\triadtwo}{0}&\mathcolor{\triadthree}{-1}&\mathcolor{\triadfour}{0}&\mathcolor{\triadfive}{1}&\mathcolor{\triadsix}{0}&\mathcolor{\triadseven}{-1}\\
\mathcolor{\triadone}{1}&\mathcolor{\triadtwo}{0}&\mathcolor{\triadthree}{0}&\mathcolor{\triadfour}{1}&\mathcolor{\triadfive}{0}&\mathcolor{\triadsix}{1}&\mathcolor{\triadseven}{-1}\\
\mathcolor{\triadone}{1}&\mathcolor{\triadtwo}{0}&\mathcolor{\triadthree}{0}&\mathcolor{\triadfour}{0}&\mathcolor{\triadfive}{1}&\mathcolor{\triadsix}{1}&\mathcolor{\triadseven}{-1}
\end{smatrix}\quad
\alpha=\begin{smatrix}
\mathcolor{\triadone}{1}&\mathcolor{\triadone}{0}&\mathcolor{\triadone}{0}&\mathcolor{\triadone}{0}\\
\mathcolor{\triadtwo}{0}&\mathcolor{\triadtwo}{1}&\mathcolor{\triadtwo}{0}&\mathcolor{\triadtwo}{0}\\
\mathcolor{\triadthree}{-1}&\mathcolor{\triadthree}{-1}&\mathcolor{\triadthree}{1}&\mathcolor{\triadthree}{1}\\
\mathcolor{\triadfour}{0}&\mathcolor{\triadfour}{0}&\mathcolor{\triadfour}{0}&\mathcolor{\triadfour}{1}\\
\mathcolor{\triadfive}{0}&\mathcolor{\triadfive}{0}&\mathcolor{\triadfive}{1}&\mathcolor{\triadfive}{1}\\
\mathcolor{\triadsix}{-1}&\mathcolor{\triadsix}{0}&\mathcolor{\triadsix}{1}&\mathcolor{\triadsix}{0}\\
\mathcolor{\triadseven}{-1}&\mathcolor{\triadseven}{0}&\mathcolor{\triadseven}{1}&\mathcolor{\triadseven}{1}
\end{smatrix}\quad
\beta=\begin{smatrix}
\mathcolor{\triadone}{1}&\mathcolor{\triadone}{0}&\mathcolor{\triadone}{0}&\mathcolor{\triadone}{0}\\
\mathcolor{\triadtwo}{0}&\mathcolor{\triadtwo}{0}&\mathcolor{\triadtwo}{1}&\mathcolor{\triadtwo}{0}\\
\mathcolor{\triadthree}{0}&\mathcolor{\triadthree}{0}&\mathcolor{\triadthree}{0}&\mathcolor{\triadthree}{1}\\
\mathcolor{\triadfour}{-1}&\mathcolor{\triadfour}{1}&\mathcolor{\triadfour}{1}&\mathcolor{\triadfour}{-1}\\
\mathcolor{\triadfive}{-1}&\mathcolor{\triadfive}{1}&\mathcolor{\triadfive}{0}&\mathcolor{\triadfive}{0}\\
\mathcolor{\triadsix}{0}&\mathcolor{\triadsix}{1}&\mathcolor{\triadsix}{0}&\mathcolor{\triadsix}{-1}\\
\mathcolor{\triadseven}{-1}&\mathcolor{\triadseven}{1}&\mathcolor{\triadseven}{0}&\mathcolor{\triadseven}{-1}
\end{smatrix}
\end{equation}

All coefficients being $1$ or $-1$ the resulting in-place algorithm
can of course compute the accumulation $C\pe{AB}$ without constant
multiplications.
It thus requires $7$ recursive calls and, from~\cref{thm:bilin},
at most $2(\#\alpha+\#\beta+\#\mu-3t)=2(14+14+14-3*7)=42$ block
additions.
Just like the $24$ additions
of~\cref{eq:StrassenWinogradMultiplicationAlgorithm} can be factored
into $15$, one can optimize also the in-place algorithm.
For instance, looking at $\alpha$ we see that performing the products
in the order $\rho_{6}$, $\rho_{7}$, $\rho_{3}$, $\rho_{5}$ and
accumulating in $a_{21}$ allows to perform all additions/subtractions
in $A$ with only $6$ operations (this is in fact optimal, see the
following~\cref{prop:six}).
This is similar for $\beta$ if the order
$\rho_{6}$, $\rho_{7}$, $\rho_{4}$, $\rho_{5}$ is used and
accumulation is in $b_{12}$.
Thus ordering for instance $\rho_{6}$, $\rho_{7}$, $\rho_{4}$,
$\rho_{3}$, $\rho_{5}$ will reduce the number of block additions to $26$.
Now looking at $\mu$ (more precisely at its transpose,
see~\cite{Kaminski:1988:transpose}), a similar reduction can be
obtained, e.g., if one of the orders
($\rho_{6}$, $\rho_{7}$, $\rho_{1}$, $\rho_{5}$)
or
($\rho_{5}$, $\rho_{7}$, $\rho_{1}$, $\rho_{6}$) is used
and accumulation is in $c_{22}$.

Therefore, using the ordering
$\rho_{6},\rho_{7},\rho_{1},\rho_{4},\rho_{3},\rho_{5},\rho_{2}$
requires only $18$ additions, as shown with~\cref{alg:ipsw},
in~\cref{app:inplsw}.
%
Thus, without thresholds and for powers of two, the dominant term of
the overall arithmetic cost is $8n^{\log_2(7)}$, for the
in-place version,
roughly a third more operations than the $6n^{\log_2(7)}$ dominant
term of the  cost for the version using extra temporaries.

Any bilinear algorithm for matrix multiplication (see, e.g.,
\url{https://fmm.univ-lille.fr/}) can be dealt with similarly.
%
Further, even the accumulating version of the non bilinear algorithm
of~\cite{Dumas:2023:adjoint} can
benefit from our techniques of in-place
accumulation (see~\cref{app:aat}).


%
\input{bshouty}


%
\section{In-place polynomial multiplication with
  accumulation}\label{sec:inpaccpol}
\Cref{alg:bilin} can also be used for polynomial multiplication.
An additional difficulty is that this does not completely fits the setting,
as multiplication of two size-$n$ inputs will in general span a (double)
size-$2n$ output.
This is not an issue until one has to distribute separately the two
halves of this $2n$ values (or more generally to different parts of
different outputs).
In the following we show that this can anyway always be done
for polynomial multiplications.

\subsection{In-place accumulating Karatsuba}\label{ssec:kara}
For instance, we immediately obtain an in-place Karatsuba
polynomial multiplication since it writes as
in~\cref{eq:kara}, from which we can extract the associated $\mu$,
$\alpha$, $\beta$ matrices shown in~\cref{eq:bilinkara}.
\begin{gather}
\begin{aligned}
(Ya_1 + a_0)(Yb_1 +b_0)&=a_0b_0+Y^2(a_1b_1)\\
&+Y((a_1+a_0)(b_1+b_0)-a_0b_0-a_1b_1)
\end{aligned}\label{eq:kara}\\
\mu=\begin{smatrix}
1&0&0\\
-1&1&-1\\
0&0&1
\end{smatrix}\quad\quad
\alpha=\begin{smatrix}
1&0\\
1&1\\
0&1
\end{smatrix}\quad\quad
\beta=\begin{smatrix}
1&0\\
1&1\\
0&1
\end{smatrix}\label{eq:bilinkara}
\end{gather}

Then, with $Y=X^t$ and $a_i$, $b_i$, $c_i$ polynomials in $X$ (and $a_0$,
$b_0$, $c_0$ of degree less than $t$), this is
detailed, with accumulation, in~\cref{eq:karasplit}:
%
\begin{equation}\label{eq:karasplit}
\fbox{\scalebox{.975}[0.975]{\ensuremath{\begin{aligned}
A(Y)& = Ya_1 + a_0;\quad
B(Y) = Yb_1 + b_0;\\
C(Y)& = Y^3c_{11} + Y^2c_{10} + Yc_{01} + c_{00};\\
m_0& = a_0\cdot{b_0} = m_{01}Y+m_{00};\quad
m_2 = a_1\cdot{b_1} = m_{21}Y+m_{20};\\
m_1& = (a_0 + a_1)\cdot(b_0 + b_1)= m_{11}Y+m_{10};\\
t_{00} &= c_{00}+m_{00};\quad
t_{01} = c_{01}+m_{01}+m_{10}-m_{00}-m_{20};\\
t_{10} &= c_{10}+m_{11}+m_{20}-m_{01}-m_{21};\quad
t_{11} = c_{11}+m_{21};\\
\text{\algorithmicthen}&\quad C+AB ={Y^3t_{11}+Y^2t_{10}+Yt_{01}+t_{00}}
\end{aligned}}}}
\end{equation}
%
To deal with the distributions of each half of the
products of~\cref{eq:karasplit},
each coefficient in $\mu$ in~\cref{eq:bilinkara} can be expanded into
$2{\times}2$ identity blocks, and the middle rows combined two by two,
as each tensor product actually spans
two sub-parts of the result; we obtain~\cref{eq:bilinkarasplit}:
\begin{equation}\label{eq:bilinkarasplit}
\scalebox{.95}[0.95]{\renewcommand{\baselinestretch}{0.5}
\ensuremath{\mu^{(2)}{=}
\begin{smatrix} I_2&0_2&0_2\\0_2&0_2&0_2\end{smatrix}
{+}
\begin{smatrix} 0&0&0\\-I_2&I_2&-I_2\\ 0&0&0\end{smatrix}
{+}
\begin{smatrix} 0_2&0_2&0_2\\0_2&0_2&I_2\end{smatrix}
{=}
\begin{smatrix}
1&0&0&0&0&0\\
-1&1&1&0&-1&0\\
0&-1&0&1&1&-1\\
0&0&0&0&0&1
\end{smatrix}}}
\end{equation}
%
Finally, \cref{eq:karasplit} then translates into an in-place algorithm
thanks to~\cref{alg:bilin,eq:bilinkara,eq:bilinkarasplit}.
%
The first point is that
products double the degree: this corresponds to a constraint that the
two blocks have to remain together when distributed.
%
In other words, this means that the $\mu^{(2)}$ matrix needs to be
considered two consecutive columns by two consecutive columns.
This is always possible if the two columns are of full rank $2$.
Indeed, consider a $2\times{2}$ invertible submatrix
$M=\begin{smatrix} v & w\\x & y\end{smatrix}$ of these two columns.
Then computing
$\begin{smatrix}c_i\\c_j\end{smatrix}\pe{}M\begin{smatrix}\rho_0\\\rho_1\end{smatrix}$
is equivalent to computing a $2\times{2}$ version of~\cref{eq:basemul}:
\begin{equation}\label{eq:twobytwo}
\left\lbrace\begin{smatrix}c_i\\c_j\end{smatrix} \fe M^{-1}; \quad
\begin{smatrix}c_i\\c_j\end{smatrix} \pe \begin{smatrix} \rho_0\\\rho_1\end{smatrix}; \quad
\begin{smatrix}c_i\\c_j\end{smatrix} \fe M\right\rbrace.
\end{equation}
The other rows of these two columns can be dealt with as before by
pre- and post- multiplying/dividing by a constant and pre- and post-
adding/subtracting the adequate $c_i$ and $c_j$.
Now to apply a matrix $M=\begin{smatrix}a&b\\c&d\end{smatrix}$ to a
vector of results $\begin{smatrix}\vec{u}\\\vec{v}\end{smatrix}$,
it is sufficient that one of its coefficient is invertible.
W.l.o.g suppose that its upper left element, $a$, is invertible.
Thus
$\begin{smatrix}a&b\\c&d\end{smatrix}=\begin{smatrix}1&0\\ca^{-1}&1\end{smatrix}\begin{smatrix}a&b\\0&d-ca^{-1}\end{smatrix}$.
Then the in-place evaluation of~\cref{eq:twobytwomul} performs this
application, using the two (known in advance) constants $x=ca^{-1}$ and $y=d-ca^{-1}b$:
%
\begin{equation}\label{eq:twobytwomul}
  \fbox{\ensuremath{\left.\begin{aligned}
          \vec{u}&\fe{a}\\[-5pt]
          \vec{u}&\pe{b\cdot\vec{v}}\\[-5pt]
          \vec{v}&\fe{y}\\[-5pt]
          \vec{v}&\pe{x\cdot\vec{u}}
   \end{aligned}\right\rbrace
   \quad
   \begin{array}{l}
        \text{computes in-place:}\\
        \begin{smatrix}\vec{u}\\\vec{v}\end{smatrix}
        \gets\begin{smatrix}a&b\\c&d\end{smatrix}
        \odot
        \begin{smatrix}\vec{u}\\\vec{v}\end{smatrix}
        =\begin{smatrix}a\vec{u}+b\vec{v}\\c\vec{u}+d\vec{v}\end{smatrix}\\
        \text{for}~x=ca^{-1}~\text{and}~y=d-xb
      \end{array}
    }}
\end{equation}

\begin{remark}\label{rq:zerotopleft}
  In practice for $2\times{2}$ blocks, if $a$ is not
  invertible, permuting the rows is sufficient since $c$ has to be
  invertible for the matrix to be invertible: for
  $J=\begin{smatrix}0&1\\1&0\end{smatrix}$,
  if $\tilde{M}=\begin{smatrix}c&d\\0&b\end{smatrix}=J{\cdot}{M}$, then
  $M=J{\cdot}\tilde{M}$ and $M^{-1}=\tilde{M}^{-1}{\cdot}{J}$ so
  that~\cref{eq:twobytwo} just becomes:\\
\(\begin{smatrix} c_i\\c_j\end{smatrix}\fe J;
  \begin{smatrix} c_i\\c_j\end{smatrix}\fe \tilde{M}^{-1};
  \begin{smatrix} c_i\\c_j\end{smatrix}\pe\begin{smatrix}\rho_0\\\rho_1\end{smatrix};
  \begin{smatrix} c_i\\c_j\end{smatrix}\fe \tilde{M};
  \begin{smatrix} c_i\\c_j\end{smatrix}\fe J.\)
%
%
\end{remark}

\input{doubleexpand}


For instance with $m_{00}+Ym_{01}=a_0b_0=\rho_0+Y\rho_1$,
consider the upper left $2\times{2}$ block of $\mu^{(2)}$
in~\cref{eq:bilinkarasplit}, that is
$M=\begin{smatrix} 1&0\\-1&1\end{smatrix}$, whose inverse is
$M^{-1}=\begin{smatrix} 1&0\\1&1\end{smatrix}$.
One has first to precompute
$M^{-1}\begin{smatrix}c_{00}\\c_{01}\end{smatrix}$, that is nothing on
$c_{00}$ and $c_{01}\pe{}c_{00}$ for the second coefficient.
Then, afterwards, the third row, for $c_{10}$, will just be $-m_{01}$:
for this just pre-add $c_{10}\pe{}c_{01}$, and post-subtract
$c_{10}\me{}c_{01}$ after the product actual computation.
This example is exactly lines \ref{lin:m0BEG} to \ref{lin:m0END}
of~\cref{alg:accinplmulkara} thereafter.
One could also consider instead the first and last rows, but in this
particular case without any advantage in term of computations.
To complete~\cref{eq:karasplit}, the computation of $m_2$ is dealt
with in the same manner, while that
of $m_1$ is direct in the results (and requiring pre and post
additions of its inputs). This gives then the whole
of~\cref{alg:accinplmulkara}.
%
The second point is to deal with unbalanced dimensions and degrees
for $Y=X^t$ and recursive calls: first separate the largest
polynomial in two parts, so that two sub-products are performed: a
large balanced one, and, recursively, a smaller unbalanced one.
Second, for the balanced case, the idea is to ensure that three out of
four parts of the result, $t_{00}$, $t_{01}$ and $t_{10}$, have the
same size and that the last one $t_{11}$ is smaller. This ensures that
all accumulations can be performed in-place.
Again the details can be found in~\cref{alg:accinplmulkara}.

\begin{algorithm}[ht]
\caption{In-place Karatsuba polynomial multiplication with
  accumulation}\label{alg:accinplmulkara}
\begin{algorithmic}[1]
\REQUIRE $A$, $B$, $C$ polynomials of degrees $m$, $n$, $m+n$
with $m\geq{n}$.
\ENSURE $C\pe{AB}$
\IF{$n\leq\threshold$}\hfill\COMMENT{Constant-time if $\threshold\in\bigO{1}$}
\RETURN the quadratic in-place multiplication. \hfill\COMMENT{\cref{alg:classicmul}}
\ELSIF{$m>n$}
\STATE Let $A(X)=A_0(X)+X^{n+1}A_1(X)$
\STATE $C_{0..2n}\pe{A_0B}$\hfill\COMMENT{Recursive call}
\IF{$m\geq{2n}$}\hfill\COMMENT{Recursive call}
\STATE $C_{(n+1)..(n+m)}\pe{A_1B}$
\ELSE
\STATE $C_{(n+1)..(n+m)}\pe{BA_1}$
\ENDIF
\ELSE
\STATE Let $t=\lceil(2n+1)/4\rceil$; \hfill\COMMENT{$t-1\geq{2n-3t}$ and thus $t>n-t$}
\STATE Let $A=a_0+X^ta_1$; $B=b_0+X^tb_1$;
\STATE Let $C=c_{00}+c_{01}X^t+c_{10}X^{2t}+c_{11}X^{3t}$;
\hfill\COMMENT{$d^\circ{c_{11}}=2n-3t$}
\STATE\label{lin:m0BEG}$c_{01}\pe{c_{00}}$; $c_{10}\pe{c_{01}}$;
\STATE $\begin{smatrix} c_{00} \\ c_{01}\end{smatrix} \pe a_0\cdot{b_0} $
\hfill\COMMENT{Recursive call for $m_0$}
\STATE\label{lin:m0END}$c_{10}\me{c_{01}}$;
$c_{01}\me{c_{00}}$;\hfill\COMMENT{$c_{10}-m_{01}$ and $c_{01}+m_{01}-m_{00}$}
\STATE $a_{0}\pe{a_{1}}$; $b_{0}\pe{b_{1}}$;\hfill\COMMENT{$d^\circ{a_0}=t\geq{n-t}=d^\circ{a_1}$}
\STATE $\begin{smatrix} c_{01} \\ c_{10}\end{smatrix} \pe a_0\cdot{b_0} $
\hfill\COMMENT{Recursive call for $m_1$}
\STATE $b_{0}\me{b_{1}}$; $a_{0}\me{a_{1}}$;
\STATE $c_{10}\pe{c_{11}}$; $c_{01}\pe{c_{10}}$;
\STATE $\begin{smatrix} c_{10} \\ c_{11}\end{smatrix} \pe a_1\cdot{b_1} $
\hfill\COMMENT{Recursive call for $m_2$}
\STATE $c_{01}\me{c_{10}}$; $c_{10}\me{c_{11}}$;\hfill\COMMENT{$t_{01}$ and $t_{10}$ in~\cref{eq:karasplit}}
\ENDIF
\RETURN $C$.
\end{algorithmic}
\end{algorithm}

\begin{proposition}
\cref{alg:accinplmulkara} is correct and requires
$\bigO{mn^{\log_2(3)-1}}$ operations.
\end{proposition}
\begin{proof}
With the above analysis, correctness comes from that
of~\cref{alg:doublebilin} applied
to~\cref{eq:bilinkara}.
When $m=n$, with $3$ recursive calls and \bigO{n} extra operations,
the algorithm thus requires overall $\bigO{n^{\log_2(3)}}$ operations.
Otherwise, it requires $\left\lfloor\frac{m}{n}\right\rfloor$ equal
degree calls, then a recursive call with $n$ and $(m\bmod{n})$.
Now, let $u_1=m$ and $u_2=n$ and if the Euclidean algorithm on
them requires $k$ steps, let $u_i$ for $i=1..k$ denote the
successive residues within this Euclidean algorithm (and
$u_k\in\{0,1\}$). Let $\kappa = k-1$ if $u_k = 0$ and $\kappa = k$
otherwise.
Now, \cref{alg:accinplmulkara} requires
less than
$\bigO{\sum_{i=1}^{\kappa-1}\lfloor\frac{u_i}{u_{i+1}}\rfloor{}u_{i+1}^{\log_2(3)}}
\leq \bigO{\sum_{i=1}^{\kappa-1} u_iu_{i+1}^{\log_2(3)-1}}$
operations.
But, $u_{i+1}\leq{u_2}=n$ and we let $s_i=u_i+u_{i+1}$ so that
$u_i=s_i-u_{i+1}\leq{s_i}$.
Now, from~\cite[Corollary~2.6]{Grenet:2020:euclide}, we have that
$s_i\leq s_1(2/3)^{i-1}$.
Thus the number of operations is bounded by
$\bigO{\sum_{i=1}^{\kappa-1} s_i n^{\log_2(3)-1}}
\leq \bigO{n^{\log_2(3)-1}s_1(\frac{1}{1-(2/3)}-1)}
=\bigO{n^{\log_2(3)-1}(m+n)}=\bigO{mn^{\log_2(3)-1}}$.
\end{proof}

In order to compare the complexity when $m=n$,
note that all coefficients of $\mat{\alpha}$,
$\mat{\beta}$ and $\mu^{(2)}$ being $1$ or $-1$, \cref{alg:accinplmulkara}
does compute the accumulation $C\pe{AB}$ without constant
multiplications.
Also, the de-duplication of each recursive output has enabled some
natural reuse, so in fact there is a cost of
$2(\#\alpha-t+\#\beta-t)=2(4-3+4-3)$ with $t=3$, and
$2(2(\#\mu-t)=4(5-3)=2(\#\mu^{(2)}-2t)$, for a total of $3$ recursive
accumulating calls and at most $12$ block additions.
Thus, without thresholds, for powers of two and with $m=n$, the
dominant term of the overall cost only goes from $10n^{\log_2(3)}$,
for the simple application of~\cref{eq:karasplit}, to
$14n^{\log_2(3)}$, for the fully in-place version
in~\cref{alg:accinplmulkara}.


\subsection{Further bilinear polynomial multiplications}\label{ssec:toom}

We have shown that any bilinear algorithm can be transformed into an
in-place version. This approach thus also works for any Toom-$k$
algorithm using $2k-1$ interpolations points instead of the three
points of Karatsuba (Toom-$2$).

For instance Toom-$3$ uses interpolations at
$0,1,-1,2,\infty$. Therefore, $\alpha$ and $\beta$ are the Vandermonde matrices
of these points for the $3$ parts of the input polynomials and
$\mu$ is the inverse of the Vandermonde matrix of these points for the
$5$ parts of the result, as shown in~\cref{eq:toom3} thereafter.

\begin{equation}\label{eq:toom3}
\mu=\begin{smatrix}
1 & 0 & 0 & 0 & 0 \\
1 & 1 & 1 & 1 & 1 \\
1 & -1 & 1 & -1 & 1 \\
1 & -2 & 4 & -8 & 16 \\
0 & 0 & 0 & 0 & 1
\end{smatrix}^{-1}\!\!\!\!=
\begin{smatrix}
 1 & 0 & 0 & 0 & 0 \\
 \tfrac12 & \tfrac13 & -1 & \tfrac16 & -2 \\
 -1 & \tfrac12 & \tfrac12 & 0 & -1 \\
-\tfrac12 & \tfrac16 & \tfrac12 & -\tfrac16 & 2 \\
 0 & 0 & 0 & 0 & 1
\end{smatrix}\!;~\alpha=\beta=\begin{smatrix}
1 & 0 & 0 \\
1 & 1 & 1 \\
1 & -1 & 1 \\
1 & -2 & 4 \\
0 & 0 & 1
\end{smatrix}
\end{equation}

With the same kind of duplication as in~\cref{eq:bilinkarasplit}, apart from the
recursive calls, the initially obtained operation count is
$2(11+11-2*5)+2(2(16-5))=68$ additions
and $2(2+2+2(11))=52$ scalar multiplications.
Following the optimization of \cite{Bodrato:2007:WAIFI:toomcook},
we see in $\alpha$ and $\beta$ that the evaluations at $1$ and $-1$
(second and third rows) share one addition. As they are successive in
our main loop, subtracting one at the end of the second iteration,
then followed by re-adding it at the third iteration can be optimized
out. This is $2$ less operations.
Together with shared coefficients in the rows of $\mu$,
some further optimizations of \cite{Bodrato:2007:WAIFI:toomcook} can
probably also be applied, where the same multiplicative
constants appear at successive places.


\subsection{Fast bilinear polynomial multiplication}\label{ssec:fft}


When sufficiently large roots of unity exist, polynomial
multiplications can be computed fast in our in-place model via a
discrete Fourier transform and its inverse, as shown
in~\cref{alg:fft2pow}, for power of two dimensions, and
in~\cref{alg:fftaccu}, for general dimensions.

Let $F\in\D[X]$ of degree $<n$ and $\omega$ be a principal $n$-th root of unity,
where $n = 2^p$. The discrete Fourier transform of $F$ at $\omega$ is defined as
$\DFT_n(F,\omega) = (F(\omega^0), F(\omega^1), \dotsc, F(\omega^{n-1}))$. The map is invertible, of
inverse $\DFT^{-1}_n(\cdot,\omega) = \frac{1}{n} \DFT_n(\cdot,
\omega^{-1})$. Further, the DFT can be computed over-place, replacing the input by the
output~\cite{1965:CooleyTukey:MathComp:FFT}. Actually, for over-place
algorithms and their extensions to the \emph{truncated Fourier transform}, it is
more natural to work with the \emph{bit-reversed DFT} defined by
$\brDFT_n(F,\omega) = (F(\omega^{[0]_p}), F(\omega^{[1]_p}), \dotsc, F(\omega^{[n-1]_p}))$
where $[i]_p=\sum_{j=0}^{p-1} d_j 2^{p-j}$ is the length-$p$ bit reversal of $i = \sum_{j=0}^{p-1} d_j 2^j$,
$d_j\in\{0,1\}$.
If $\pi$ denotes the
bit-reversal permutation (that is $\pi(i) = [i]_p$), we have $\brDFT_n(\cdot,\omega)
= \pi\circ\DFT_n(\cdot,\omega)$. Its inverse becomes $\brDFT_n^{-1}(\cdot,\omega) = \frac{1}{n}
\DFT_n(\cdot,\omega^{-1})\circ\pi=\frac{1}{n}\DFT_n(\pi(\cdot),\omega^{-1})$ since $\pi$ is an involution.

\begin{remark}
    The Fast Fourier Transform (FFT) algorithm has two main variants:
    \emph{decimation in time} (DIT) and \emph{decimation in frequency} (DIF).
    Both algorithms can be performed over-place, replacing the input by the
    output. Without applying any permutation to the entries of the input/output
    vector, the over-place DIF-FFT algorithm naturally computes
    $\brDFT_n(\cdot,\omega)$, while the over-place DIT-FFT algorithm on $\omega^{-1}$
    computes $n\cdot\brDFT_n^{-1}(\cdot, \omega)$.
\end{remark}


\begin{algorithm}[htbp]
\caption{In-place power of two accumulating multiplication}\label{alg:fft2pow}
\begin{algorithmic}[1]
\REQUIRE $\vec{a}$, $\vec{b}$ and $\vec{c}$ of length $2^L$, $2^L$ and $2^{L+1}$, containing the coefficients of $A$, $B$, $C\in\D[X]$ respectively; $\omega\in\D$ primitive $2^{L+1}$-th root of unity.
\ENSURE $\vec{c}$ contains the coefficients of $C+A\cdot B$.
\STATE Let $n=2^L$;
\STATE $\vec{c}\gets \brDFT_{2n}(\vec{c},\omega)$;
    \hfill\COMMENT{over-place}
\STATE\label{lin:DFTab}$\vec{a}\gets \brDFT_n(\vec{a},\omega^2)$; $\vec{b}\gets \brDFT_n(\vec{b},\omega^2)$
    \hfill\COMMENT{over-place}
\ForDoEnd{$i=0$ \To $n-1$}{$c_i\pe a_i\times b_i$}
\STATE $\vec{a}\gets \brDFT^{-1}_n(\vec{a},\omega^2)$; $\vec{b}\gets \brDFT^{-1}_n(\vec{b},\omega^2)$
    \hfill\COMMENT{Undo \ref{lin:DFTab}}
\ForDoEnd[lin:mulw]{$i=0$ \To $n-1$}{$a_i\fe \omega^i$; $b_i\fe \omega^i$}
%
\STATE\label{lin:DFTab2}$\vec{a}\gets \brDFT_n(\vec{a},\omega^2)$; $\vec{b}\gets
\brDFT_n(\vec{b},\omega^2)$
    \hfill\COMMENT{over-place}
\ForDoEnd{$i=0$ \To $n-1$}{$c_{i+n}\pe a_i\times b_i$}
\STATE $\vec{a}\gets \brDFT^{-1}_n(\vec{a},\omega^2)$; $\vec{b}\gets \brDFT^{-1}_n(\vec{b},\omega^2)$
    \hfill\COMMENT{Undo \ref{lin:DFTab2}}
\ForDoEnd{$i=0$ \To $n-1$}{$a_i\de \omega^i$; $b_i\de \omega^i$}
    \hfill\COMMENT{Undo \ref{lin:mulw}}
\RETURN $\vec{c}\gets \brDFT^{-1}_{2n}(\vec{c},\omega)$
\end{algorithmic}
\end{algorithm}

%
%
%
%
%
%
%
%
%
%
%
%
%
%

\begin{theorem}\label{thm:fft2pow}
  Using an over-place $\brDFT$ algorithm with
  complexity bounded by $\bigO{n\log n}$, \cref{alg:fft2pow} is
  correct, in-place and has complexity bounded by $\bigO{n\log n}$.
\end{theorem}
\begin{proof}
    \cref{alg:fft2pow} follows the pattern of the standard FFT-based
    multiplication
    algorithm. Our goal is to compute $\brDFT_{2n}(A,\omega)$, $\brDFT_{2n}(B,\omega)$ and
    $\brDFT_{2n}(C,\omega)$,
    then obtain $\brDFT_{2n}(C+AB,\omega)$ and finally $C+AB$ using an inverse
    $\brDFT$. Computations on $C$ and then $C+AB$ are performed over-place using
    any standard over-place $\brDFT$ algorithm. The difficulty happens for $A$ and
    $B$ that are stored in length-$n$ arrays. We use the following property of
    the bit reversed order: for $k < n/2$, $[k]_p = 2[k]_{p-1}$, and for
    $k \ge n/2$, $[k]_p = 2[k-n/2]_{p-1}+1$. Therefore, the first $n$
    coefficients of $\brDFT_{2n}(A,\omega)$ are $(A(\omega^{2[0]_{p-1}}), \dotsc
    A(\omega^{2[\frac{n}{2}-1]_{p-1}})) = \brDFT_n(A,\omega^2)$. Similarly, the next $n$
    coefficients are $\brDFT_n(A(\omega X), \omega^2)$. Therefore, one can compute
    $\brDFT(A,\omega^2)$ and $\brDFT(B,\omega^2)$ in $\vec{a}$ and $\vec{b}$ respectively,
    and update the first $n$ entries of $\vec{c}$. Next we restore $\vec{a}$
    and $\vec{b}$ using $\brDFT^{-1}_n(\cdot,\omega^2)$. We compute $A(\omega X)$ and
    $B(\omega X)$ and again $\brDFT(A(\omega X),\omega^2)$ and $\brDFT(B(\omega X),\omega^2)$ to update the
    last $n$ entries of $\vec{c}$. Finally, we restore $\vec{a}$ and
    $\vec{b}$ and perform the inverse $\brDFT$ on $\vec{c}$.
    The cost is dominated by the ten $\brDFT^{\pm1}$ computations.
\end{proof}

Since there are 2 size-$2n$ and 8 size-$n$ $\brDFT^{\pm1}$ computations,
the dominant term of the cost is $18n\log n$ ring operations, twice as large as
the dominant term for the standard (not in-place) algorithm~\cite{1965:CooleyTukey:MathComp:FFT}.

The case where $n$ is not a power of two is loosely similar, using as a routine
a truncated Fourier transform (TFT) rather than a
DFT~\cite{2004:vanderHoeven:ISSAC:TFT}: let $\omega$ be an $N$-th root of
unity for some $N = 2^p$.
The length-$n$ (bit-reversed) TFT of a polynomial $F\in\D[X]$,
$n < N$, is $\brTFT_n(F,\omega) = (F(\omega^{[0]_p}), \dotsc, F(\omega^{[n-1]}_p))$, that is
the $n$ first coefficients of $\brDFT_{N}(F,\omega)$.
As for the DFT, the (bit-reversed) TFT and its inverse
can be computed over-place~\cite{Harvey:2010:issactft, Roche:2011:waterloo,
2013:Arnold:ISSAC:TFT, Coxon:2022:JSC:inplaceTFT}.

Given inputs $A$ and $B\in\D[X]$ of respective lengths $m$ and $n$ and an output
$C\in\D[X]$ of length $m+n-1$, we aim to replace $C$ by $C+AB$.  The idea is
first to replace $C$ by $\brTFT_{m+n-1}(C,\omega)$ where $\omega$ is a $2^p$-th principal
root of unity, $2^p \ge m+n-1$. That is, the vector $\vec{c}$ now contains as
its $i$-th entry the value $C(\omega^{[i]_p})$. The goal is then to replace $C(\omega^{[i]_p})$ by
$C(\omega^{[i]_p})+A(\omega^{[i]_p})B(\omega^{[i]_p})$, for $i=0$ to $m+n-2$. We cannot compute
the length $m+n-1$ $\brTFT$'s of $A$ and $B$ since this takes too much space.
Instead, we will progressively compute some parts of these $\brTFT$'s by means
of (standard) $\brDFT$'s, and update $\vec{c}$ accordingly. The starting point
of this strategy is the following lemma.

\begin{lemma}[\cite{Harvey:2010:issactft,Roche:2011:waterloo}]\label{lem:roche}
    Let $F\in\D[X]$, $\ell$ and $s$ be two integers such that $2^\ell$ divides
    $s$ and $\omega$ be a $2^p$-th principal root of unity. Define $F_{s,\ell}(X) =
    F(\omega^{[s]_p}X)\bmod X^{2^\ell-1}$. Then
    \(\brDFT_{2^\ell}(F_{s,\ell},\omega^{2^{p-\ell}}) = (F(\omega^{[s]_p}),\dotsc,F(\omega^{[s+2^\ell-1]_p}))\).
\end{lemma}
%
\begin{proof}
    Let $\omega_\ell = \omega^{2^{p-\ell}}$. This is a principal $2^\ell$-th root of unity
    since $\omega$ is a principal $2^p$-th root of unity. In particular, for any $i <
    2^\ell$, $F_{s,\ell}(\omega_\ell^{[i]_\ell}) = F(\omega^{[s]_p}\omega_\ell^{[i]_\ell})$.
    Now, $\omega_\ell^{[i]_\ell} = \omega^{[i]_p}$ since $2^{p-\ell}[i]_\ell = [i]_p$.
    Furthermore, $[s]_p+[i]_p = [s+i]_p$ since $i < 2^\ell$ and $2^\ell$ divides
    $s$.
    Finally, $F_{s,\ell}(\omega_\ell^{[i]_\ell}) = F(\omega^{[s+i]_p})$.
\end{proof}

\begin{corollary}\label{cor:partTFT}
    Let $F\in\D[X]$ stored in an array $\vec{f}$ of length $n$. Let
    also $\ell$ and $k$ be
    two integers and $\omega$ be a $2^p$-th principal root of unity, with $2^\ell\le
    n$ and $(k+1)2^\ell \le 2^p$. There exists an algorithm,
    $\partTFT_{k,\ell}(\vec{f},\omega)$, that replaces the first $2^\ell$ entries of
    $\vec{f}$ by $F(\omega^{[k\cdot 2^\ell]_p})$, \dots,
    $F(\omega^{[(k+1)\cdot2^\ell-1]_p})$, and an inverse algorithm
    $\partTFT^{-1}_{k,\ell}$ that restores $\vec{f}$ to its initial state. Both
    algorithms use $\bigO{1}$ extra space and have complexity bounded by
    $\bigO{n+\ell\cdot2^\ell}$.
\end{corollary}
%
\begin{proof}
    Algorithm $\partTFT_{k,\ell}(\vec{f}, \omega)$ is the following:
    \begin{algorithmic}[1]
    \ForDoEnd{$i=0$ \To $n-1$}{$f_i\fe \omega^{i[k\cdot2^\ell]_p}$}
    \ForDoEnd{$i=2^\ell$ \To $n-1$}{$f_{i-2^\ell}\pe f_i$}
    \STATE $\vec{f}_{0..2^\ell-1} \gets \brDFT_{2^\ell}(\vec{f}_{0..2^\ell-1},\omega^{2^{p-\ell}})$
    \end{algorithmic}
    Its correctness is ensured by~\cref{lem:roche}. Its inverse algorithm
    $\partTFT^{-1}_{k,\ell}(\vec{f},\omega)$ does the converse:
    \begin{algorithmic}[1]
    \STATE $\vec{f}_{0..2^\ell-1} \gets\brDFT^{-1}_{2^\ell}(\vec{f}_{0..2^\ell-1},\omega^{2^{p-\ell}})$
    \ForDoEnd{$i=2^\ell$ \To $n-1$}{$f_{i-2^\ell}\me f_i$}
    \ForDoEnd{$i=0$ \To $n-1$}{$f_i\de \omega^{i[k\cdot2^\ell]_p}$}
    \end{algorithmic}
    In both algorithms, the call to $\brDFT^{\pm1}$ has cost
    $\bigO{\ell\cdot{2^\ell}}$, and the two other steps have cost~$\bigO{n}$.
\end{proof}

To implement the previously sketched strategy, we assume that $m\le n$ for
simplicity. We let $\ell$, $t$ be such that $2^\ell\le m<2^{\ell+1}$ and
$2^{\ell+t}\le n<2^{\ell+t+1}$. Using $\partTFT^{\pm 1}$, we are able to compute
$(A(\omega^{[k\cdot 2^\ell]_p}), \dotsc, A(\omega^{[(k+1)\cdot 2^{\ell}-1]_p}))$ for any
$k$, and restore $A$ in its initial state afterwards.
Similarly, it is possible to compute
$(B(\omega^{[k\cdot 2^{\ell+t}]_p}), \dotsc, B(\omega^{[(k+1)\cdot
  2^{\ell+t}-1]_p}))$ and restore $B$.

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\begin{algorithm}[htbp]
\caption{In-place fast accumulating polynomial multiplication}\label{alg:fftaccu}
\begin{algorithmic}[1]
\REQUIRE $\vec{a}$, $\vec{b}$ and $\vec{c}$ of length $m$, $n$ and $m+n-1$,
$m\le n$, containing the coefficients of $A$, $B$, $C\in\D[X]$ respectively;
$\omega\in\D$ principal $2^p$-th root of unity with $2^{p-1} < m+n-1 < 2^p$
\ENSURE $\vec{c}$ contains the coefficients of $C+A\cdot B$.
\STATE $\vec{c}\gets \brTFT_{m+n-1}(\vec{c},\omega)$;
    \hfill\COMMENT{over-place}
\STATE $r\gets m+n-1$
\WHILE{$r \ge 0$}
\STATE\label{lin:boundslt}$\ell\gets\lfloor\log_2\min\{r, m\}\rfloor$; $t\gets\lfloor\log_2\min\{r, n\}\rfloor-\ell$;
 \STATE\label{lin:boundsk}$k\gets m+n-1-r$
 \Statex\hspace{1em}\COMMENT{over-place: $B(\omega^{[k\cdot2^{\ell+t}]_p]}), \dotsc, B(\omega^{[(k+1)\cdot 2^{\ell+t}-1]_p})$}
    \STATE\label{lin:TFTb}$\vec{b}\gets\partTFT_{k,\ell+t}(\vec{b},\omega)$
        \FOR{$s=0$ \To $2^t-1$}
        \Statex\hfill\COMMENT{over-place: $A(\omega^{[(k\cdot2^t+s)2^\ell]_p]}), \dotsc, A(\omega^{[(k\cdot 2^t+s+1)2^\ell-1]_p})$}
        \STATE\label{lin:TFTa}$\vec{a}\gets\partTFT_{s+k\cdot2^t,\ell}(\vec{a},\omega)$
        \ForDoEnd{$i=0$ \To $2^\ell-1$}{$c_{i+(k\cdot 2^t+s)2^\ell} \pe  a_i b_{i+s\cdot 2^\ell}$}
        \STATE $\vec{a}\gets\partTFT^{-1}_{s+k\cdot2^t,\ell}(\vec{a},\omega)$
        \hfill\COMMENT{Undo~\ref{lin:TFTa} over-place}
        \ENDFOR
    \STATE $\vec{b}\gets\partTFT^{-1}_{k,\ell+t}(\vec{b},\omega)$
    \hfill\COMMENT{Undo~\ref{lin:TFTb} over-place}
    \STATE $r\me 2^{\ell+t}$
\ENDWHILE
\RETURN $\vec{c}\gets \brTFT^{-1}_{m+n-1}(\vec{c},\omega)$
\end{algorithmic}
\end{algorithm}

\begin{theorem}
    \Cref{alg:fftaccu} is correct and in-place.  If the algorithm
    $\brDFT$ used inside $\partTFT$ has complexity $\bigO{n\log n}$, then the running
    time of~\cref{alg:fftaccu} is $\bigO{(m+n)\log(m+n)}$.
\end{theorem}

\begin{proof}
    The fact that the algorithm is in-place comes
    from~\cref{cor:partTFT}.
    The only slight difficulty is to produce, fast
    and in-place, the relevant roots of unity. This is actually dealt with in
    the original over-place TFT algorithm~\cite{Harvey:2010:issactft} and can be
    done the same way here.

    To assess its correctness, first note that the values
    of~\cref{lin:boundslt,lin:boundsk}
    are computed so that $2^\ell\le r,m$ and
    $2^{\ell+t}\le r,n$. One iteration of the while loop updates the entries
    $c_k$ to $c_{k+2^{\ell+t}-1}$ where $k = m+n-1-r$. To this end,
    we first compute $B(\omega^{[k\cdot2^{\ell+t}]_p]})$ to $B(\omega^{[(k+1)\cdot
    2^{\ell+t}-1]_p})$ in $\vec{b}$ using $\partTFT$. Then, since $\vec{a}$ may
    be too small to store $2^{\ell+t}$ values, we compute the corresponding
    evaluations of $A$ by groups of $2^\ell$, using a smaller $\partTFT$. After
    each computation in $\vec{a}$, we update the corresponding entries in
    $\vec{c}$ and restore $\vec{a}$. Finally, at the end of the iteration,
    entries $k$ to $k+2^{\ell+t}-1$ of $\vec{c}$ have been updated and $\vec{b}$
    can be restored. This proves the correctness of the algorithm.

    We now bound its complexity. Since $m\le n$, our aim is to
    bound it by $\bigO{n\log n}$. Let us first bound the number of iterations of the
    while loop. We identify two phases, first iterations where $r \ge n$ and
    then iterations with $r < n$. During the first phase,
    $2^{\ell+t}>\frac{n}{2}$ entries of $\vec{c}$ are updated at each iteration,
    hence the first phase has at most $3$ iterations. In the second phase,
    $2^{\ell+t}>\frac{r}{2}$ entries are updated per iteration. The second phase
    starts with $r < n$ and each iteration decreases $r$ by half, hence the
    second phase has at most $\log_2{n}$ iterations.

    The cost of an iteration is dominated by the calls to $\partTFT^{\pm 1}$. The cost of
    a call to $\partTFT^{\pm 1}_{k,\ell}$ with a size-$m$ input is the sum of a
    linear term $\bigO{m}$ and a non-linear term $\bigO{\ell\cdot 2^\ell}$.
    At each iteration, there are two calls to $\partTFT^{\pm 1}$ on $\vec{b}$ and
    $2^{t+1}$ calls to $\partTFT^{\pm 1}$ on $\vec{a}$. The linear terms sum to
    $\bigO{n+m\cdot 2^t} = \bigO{n}$ since $m\cdot
    2^t < 2^{\ell+1+t} \le 2n$. Over the $\log_2{n}$ iterations, the global
    cost due to these linear terms is $\bigO{n\log n}$.

    The cost due to the non-linear terms in one iteration is
    $\bigO{(\ell+t)\cdot{2^{\ell+t}}}$. In the first iterations,
    $2^{\ell+t} \le n$ and these costs sum to $\bigO{n\log n}$.
    In the next iterations, $2^{\ell+t} \le r < n$. Since $r$ is
    halved at each iteration, the non-linear costs in these
    iterations sum to
    $\bigOdisplay{\sum_i\frac{n}{2^i}\log\frac{n}{2^i}}=\bigO{n\log{n}}$.
\end{proof}

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%



%
\section{Conclusion}
We here provide a generic technique mapping any bilinear formula
(and more generally any linear accumulation)
into an in-place algorithm.
This allows us for instance to provide the first accumulating in-place
Strassen-like matrix multiplication algorithm.
This also allows use to provide fast in-place accumulating polynomial
multiplications algorithms.

Many further accumulating algorithm can then be reduced to these fundamental
building blocks, see for instance Toeplitz, circulant, convolutions or
remaindering operations in~\cite{jgd:2023:inplrem}.

%
\clearpage
%
%

%
\bibliographystyle{plainurl}
%
\bibliography{shortbib}
%
%
\appendix
%
%
%
%
%
%
%
\section{In-place accumulating matrix-multiplication with 7 recursive
  calls and 18 additions}\label{app:inplsw}
We here give an in-place version of Strassen-Winograd algorithm for
matrix multiplication.
We first directly apply our~\cref{alg:bilin} to the classical, not
in-place Strassen-Winograd algorithm, following the specific scheduling
strategy of~\cref{sec:strassen}. This strategy allows to reduce the
number of additions obtained when calling~\cref{alg:bilin}, from $42+7$
to $18+7$: mostly remove successive additions/subtractions that are
reciprocal on either sub-matrices. This optimized version is given
in~\cref{alg:ipsw} and reaches the minimal possible number of
extra additions/subtractions, as shown in~\cref{thm:eighteen}.

\begin{algorithm}[ht]
\caption{In-place accumulating S.-W. matrix-multiplication}\label{alg:ipsw}
\begin{algorithmic}
\REQUIRE
%
%
%
$A=\begin{smatrix} a_{11} & a_{12} \\  a_{21} &a_{22}\end{smatrix}$,
$B=\begin{smatrix} b_{11} & b_{12} \\  b_{21} &b_{22}\end{smatrix}$,
$C=\begin{smatrix} c_{11} & c_{12} \\  c_{21} &c_{22}\end{smatrix}$.
\ENSURE $C\pe{AB}$.
\end{algorithmic}
\input{2x2}
\end{algorithm}

The memory footprint of~\cref{alg:ipsw} is compared to that of
previously known algorithms in~\cref{tab:inpsw}.
\begin{table}[htbp]\centering
\caption{Reduced-memory accumulating S.-W. multiplication}\label{tab:inpsw}
\begin{tabular}{lccc}
\toprule
Alg. & Temporary & inputs & accumulation \\
\midrule
\cite{HussLederman:1996:ISA} & $3$ & {\color{darkgreen} read-only} & {\xyes}\\
\cite{jgd:2009:WinoSchedule} & $2$ & {\color{darkgreen} read-only} & {\xyes}\\
\cref{alg:ipsw} & $0$ & mutable & {\xyes}\\
\bottomrule
\end{tabular}
\end{table}

To go beyond our minimality result for operations, one could
try an alternate basis of~\cite{Karstadt:JACM:2020:MMfaster}.
But an argument similar to that of~\cref{prop:six} shows
that alternate basis does not help for the in-place case.
\input{sparsify}
%
%
%




%
%
%
\section{Fast In-place Square \& Symmetric Rank-k Update}\label{app:aat}
Thanks to~\cref{alg:ipsw} and with some care on transposes, the same
technique can be adapted to, e.g., \cite[Alg.~12]{Dumas:2023:adjoint},
which performs the multiplication of a matrix by its transpose.
With an accumulation, this is a classical \emph{Symmetric Rank-k
  Update} (or SYRK): $C\leftarrow{\alpha{A}\Transpose{A}+\beta{C}}$.

Following the notations of the latter algorithm, which is not a
bilinear algorithm on its single input matrix, the in-place
accumulating version is shown in~\cref{alg:ipaat}, for $\alpha=\beta=1$,
using any (fast to apply) skew-unitary $Y\in\F^{n{\times}n}$.
It has been obtained automatically by the method
of~\cref{thm:general}, and it thus preserves the need of only $5$
multiplications $P_1$ to ${P_5}$. It has then been scheduled so as to
reduce the number of extra operations.


\Cref{alg:ipaat} requires
$3$ recursive calls,
$2$ multiplications of two independent half matrices,
$4$ multiplications by a skew-unitary half matrix,
$8$ additions (of half inputs),
$12$ semi-additions (of half triangular outputs).
Provided that the multiplication by the skew-unitary matrix can be
performed in-place in negligible time,
this gives a dominant term of the complexity bound
for~\cref{alg:ipaat} of a fraction $\frac{2}{2^\omega-3}$ of the
cost of the full in-place algorithm.
This is a factor $\frac{1}{2}$, when~\cref{alg:ipsw} is used for the
two block multiplications of independent matrices ($P4$ and $P5$).

\begin{algorithm}[ht]
\caption{In-place accumulating multiplication by its transpose}\label{alg:ipaat}
\begin{algorithmic}
\REQUIRE $A=\begin{smatrix} a_{11} & a_{12} \\  a_{21}
  &a_{22}\end{smatrix}\in\F^{m{\times}2n}$;
symmetric $C=\begin{smatrix} c_{11} & \Transpose{c_{21}} \\  c_{21}
  &c_{22}\end{smatrix}\in\F^{m{\times}m}$.
\ENSURE $\Low{C}\pe\Low{A\cdot\Transpose{A}}$. \hfill\COMMENT{update
  bottom left triangle}
\end{algorithmic}
\input{AAT_inpl}
\end{algorithm}

Now, the skew-unitary matrices used in~\cite{Dumas:2023:adjoint}, are
either a multiple of the identify matrix, or the Kronecker product of
$\begin{smatrix}a&b\\-b&a\end{smatrix}$ by the identity matrix,
for $a^2+b^2=-1$ and $a\neq{0}$.
The former is easily performed in-place in time \bigO{n^2}.
For the latter, it is sufficient to use~\cref{eq:twobytwomul}:
the multiplication $\begin{smatrix}a&b\\-b&a\end{smatrix}\vec{u}$
can be realized in place by the algorithm:
$u_1\fe{a}$; $u_1\pe{b{\cdot}u_2}$; $u_2\fe(a+b^2a^{-1})$;
$u_2\pe{\left(-ba^{-1}\right){\cdot}u_1}$.

The same technique can be used on the symmetric algorithm for the
square of matrices given in~\cite{Bodrato:ISSAC2010}. The resulting
in-place algorithm is given in~\cref{alg:square}.
\begin{algorithm}[ht]
\caption{In-place accumulating S.-W. matrix-square}\label{alg:square}
\begin{algorithmic}
\REQUIRE
$A=\begin{smatrix} a_{11} & a_{12} \\  a_{21} &a_{22}\end{smatrix}$,
$C=\begin{smatrix} c_{11} & c_{12} \\  c_{21} &c_{22}\end{smatrix}$.
\ENSURE $C\pe{A^2}$.
\end{algorithmic}
\input{square}
\end{algorithm}

%
%
%
%
%
\section{Reduced memory algorithms for polynomial multiplication}\label{app:redpolmul}

We compare in~\cref{tab:kara} the procedure given
in~\cref{alg:accinplmulkara} (obtained via the automatic application
of~\cref{alg:doublebilin}) with previous Karatsuba-like algorithms for
polynomial multiplications, designed to reduce their memory footprint
(see also \cite[Table~2.2]{Giorgi:2019:hdr}).

\begin{table}[htbp]\centering
\caption{Reduced-memory algorithms for Karatsuba polynomial
  multiplication}\label{tab:kara}
\begin{tabular}{lccc}
\toprule
Alg. & Memory & inputs & accumulation \\
\midrule
\cite{Thome:2002:karatemp} & $n+5\log{n}$ & {\color{darkgreen} read-only} & {\xno}\\
\cite{Roche:2009:spacetime,Roche:2011:waterloo} & $5\log{n}$ & {\color{darkgreen} read-only} & {\xno}\\
\cite{Giorgi:2019:issac:reductions} & \textcolor{darkgreen}{$\bigO{1}$} & {\color{darkgreen} read-only} & {\xno}\\
\cref{alg:accinplmulkara} & $5\log n$ & mutable & {\xyes}\\
\bottomrule
\end{tabular}
\end{table}


Then, \cref{alg:fftaccu} is compared with previous FFT-based
algorithms for polynomial multiplications designed to reduce their
memory footprint in~\cref{tab:fft} (see also
\cite[Table~2.2]{Giorgi:2019:hdr}).

\begin{table}[htbp]\centering
\caption{Reduced-memory algorithms for FFT polynomial
  multiplication}\label{tab:fft}
\begin{tabular}{lccc}
\toprule
Alg. & Memory & inputs & accumulation \\
\midrule
\cite{1965:CooleyTukey:MathComp:FFT} & $2n$ & {\color{darkgreen} read-only} & {\xno}\\
\cite{Roche:2009:spacetime} & $\bigO{2^{\lceil\log_2 n\rceil}-n}$ & {\color{darkgreen} read-only} & {\xno}\\
\cite{Harvey:2010:issactft} & \textcolor{darkgreen}{$\bigO{1}$} & {\color{darkgreen} read-only} & {\xno}\\
\cref{alg:fftaccu} & \textcolor{darkgreen}{$\bigO{1}$} & mutable & {\xyes}\\
\bottomrule
\end{tabular}
\end{table}
%

\end{document}
