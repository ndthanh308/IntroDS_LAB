We now prove that $18$ additions is the minimal number of additions
required by an in-place algorithm resulting
from any bilinear algorithm for matrix multiplication using only $7$
multiplications.
For this we need to consider  elementary operations on
variables (similar to elementary linear algebra operators):
\emph{variable-switching} (swapping variable $i$ and variable $j$);
\emph{variable-multiplying} (multiplying a variable by a constant);
\emph{variable-addition} (adding one variable, potentially multiplied
by a constant, to another variable).
An \emph{elementary program} is a program making use of only these
three kind of operations.
%
Then we consider the in-place implementation of a linear function on
its input: for $\mat{\alpha}\in\F^{t{\times}m}$ and $\vec{a}\in\F^m$,
we want to compute at least once each of the $t$ coefficients of
$\mat{\alpha}\vec{a}$, but using only elementary operations and using
only the variables of $\vec{a}$ as temporary variables.
%
We start by proving in~\cref{lem:nocannorzero}
that in any bilinear algorithm for matrix
multiplication using only $7$ multiplications, the columns of the
associated matrices $\mat{\alpha},\mat{\beta},\mat{\mu}$
(as in~\cref{eq:alphabetamu})
cannot contain too many zeroes.
%
\begin{lemma}\label{lem:nocannorzero}
If
$(\mat{\alpha},\mat{\beta},\mat{\mu})\in\F^{7{\times}4}\times\F^{7{\times}4}\times\F^{4{\times}7}$
is a bilinear algorithm for matrix multiplication, then none of
$\mat{\alpha},\mat{\beta},\Transpose{\mat{\mu}}$
contain a zero column vector, nor a multiple of a vector of the
canonical basis.
\end{lemma}
\begin{proof}
The dimensions of the matrices indicate that the multiplicative
complexity of the algorithm is $7$.
From~\cite{Groote:1978:optimal} we know that all such bilinear
algorithms can be obtained from one another.
Following~\cite[Lemma~6]{Bshouty:1995:minwinoadd}, then any associated
$\mat{\alpha},\mat{\beta},\Transpose{\mat{\mu}}$ matrix is some row or column
permutation, or the multiplication by some $G\otimes{H}$ (the Kronecker
product of two invertible $2{\times}2$ matrices), of the matrices
of~\cref{eq:alphabetamu}.
By duality~\cite{Hopcroft:1973:duality}, see
also~\cite[Eq. (3)]{Bshouty:1995:minwinoadd},
it is also sufficient to consider any one of the $3$ matrices.
%
We thus let
$G=\begin{smatrix} g_{1,1} & g_{1,2}\\ g_{2,1} & g_{2,2}\end{smatrix}$,
$H=\begin{smatrix} h_{1,1} & h_{1,2}\\ h_{2,1} & h_{2,2}\end{smatrix}$
and
$K=G\otimes{H}$ be their Kronecker product.
%
Then any column of $K$ is of the form
$\Transpose{\begin{bmatrix} ux & uy & vx & vy\end{bmatrix}}$,
for
$u\in\{g_{1,1},g_{1,2}\}$, $v\in\{g_{2,1},g_{2,2}\}$, with $u$ and $v$
both in the same column of $G$
and
$x\in\{h_{1,1},h_{1,2}\}$, $y\in\{h_{2,1},h_{2,2}\}$, with $x$ and $y$
both in the same column of $H$.
Further as $G$ is invertible,
$u$ and $v$
cannot be both zero simultaneously
and, similarly,
$x$ and $y$
cannot be both zero simultaneously.

Now consider for instance
$\mat{\alpha}\cdot{K}$, with
$\mat{\alpha}$ of~\cref{eq:alphabetamu}.
Then any column $\vec{\theta}$ of $\mat{\alpha}\cdot{K}$
is of the form:\\
\(\gTranspose{1.5pt}{\begin{bmatrix}ux &
    uy & -ux-uy+vx+vy & vy & vx+vy & -ux+vx&-ux+vx+vy
  \end{bmatrix}}\).\\
%
%
%
%
%
%
%
%
%
For such a column to be a multiple of a vector of the canonical basis
or the zero vector, at least $6$ of its $7$ coefficients must be zero.
For instance, this means that at least two out of rows $1$, $2$ and $4$
must be zero: or that at least two of $ux$, $uy$ or $vy$ must be
zero. This limits us to three cases: (1) $u=0$, (2) $y=0$ or (3) $x=v=0$.
%
%
If $u=0$, then
$\vec{\theta}=v\gTranspose{1pt}{\begin{bmatrix}0&0&x+y&y&x+y&x&x+y\end{bmatrix}}$;
at least one of rows $4$ or $6$ has to be zero, thus,
w.l.o.g. suppose $x=0$, we obtain that
$\vec{\theta}=vy\gTranspose{1pt}{\begin{bmatrix}0&0&1&1&1&0&1\end{bmatrix}}$
with none of $v$ nor $y$ being zero (otherwise $G$ or $H$ is not
invertible);
such a column cannot be a multiple of a vector of the canonical basis
nor the zero vector.
%
Similarly, if $y=0$, then
$\vec{\theta}=x\gTranspose{1pt}{\begin{bmatrix}u&0&-u+v&0&v&-u+v&-u+v\end{bmatrix}}$;
at least one of rows $1$ or $5$ has to be zero, thus,
w.l.o.g. suppose $v=0$, we obtain that
$\vec{\theta}=ux\gTranspose{1pt}{\begin{bmatrix}1&0&-1&0&0&-1&-1\end{bmatrix}}$;
such a column cannot be a multiple of a vector of the canonical basis
nor the zero vector.
%
Finally, if $x=v=0$, then
$\vec{\theta}=uy\gTranspose{1pt}{\begin{bmatrix}0&1&-1&0&0&0&0\end{bmatrix}}$;
again that column cannot be a multiple of a vector of the canonical basis
nor the zero vector.
%
\end{proof}
%
Now we show that any in-place elementary algorithm requires at least $1$
extra operation to put back the input in its initial state.
%
\begin{lemma}\label{lem:plusone}
Let $\vec{a}\in\F^m$ and $\mat{\alpha}\in\F^{t{\times}m}$ with at
least one row which is neither the zero row, nor a vector of the
canonical basis.
Now suppose that, without any constraints in terms of temporary
registers, $k$ is the minimal number of elementary operations required
to compute $\mat{\alpha}\vec{a}$.
Then any algorithm computing all the $t$ values of
$\mat{\alpha}\vec{a}$, in-place of $\vec{a}$, requires at least $k+1$
elementary operations.
\end{lemma}
\begin{proof}
Consider an in-place algorithm realizing $\mat{\alpha}\vec{a}$ in $f$
operations.
Any zero or canonical basis vector row can
be realized without any operations on $\vec{a}$.
Now take this algorithm at the moment where the last of the other rows of
$\mat{\alpha}$ are realized (at that point all the $t$ values are
realized). Then this last realization (a non trivial linear combination of the
initial values of $\vec{a}$) has to have been stored in one variable
of $\vec{a}$, say $a_i$.
Therefore, at this point, the in-place algorithm
has to perform at least one more operation to put back $a_i$ to its
initial state.
Therefore, by replacing all the in-place computations by operations on
extra registers and omitting the operation(s) that restore this $a_i$,
we obtain an algorithm with less than $f-1$ elementary operations that
realizes $\mat{\alpha}\vec{a}$ and thus: $(f-1)\geq{k}$.
\end{proof}


\begin{proposition}\label{prop:six}
For the in-place realization of each of the two linear operators
$\mat{\alpha}$ and $\mat{\beta}$, of any bilinear matrix
multiplication algorithm using only $7$ multiplications,
and the restoration of the initial states of their input,
at least $6$ operations are needed.
\end{proposition}
\begin{proof}
A bilinear matrix multiplication algorithm has to compute
$\mat{\alpha}\vec{a}$, with $\vec{a}$ the entries of the left input of
the matrix multiplication, while $\mat{\beta}$ deals with the right
input.
These $\mat{\alpha}$ and $\mat{\beta}$ matrices cannot contain a
($4$-dimensional) zero row:
otherwise there would exist an algorithm using less than $6$
multiplications, but $7$ is minimal~\cite{Winograd:1971:minseven}.
If $\mat{\alpha}$ or $\mat{\beta}$ contain at least $5$ rows that are not
vectors of the canonical basis, then they require at
least $5$ non-trivial operations to be computed, and therefore at
least $6$ elementary operations with an in-place algorithm,
by~\cref{lem:plusone}.
The matrices also cannot contain more than $3$ multiple of vectors of the
canonical basis, by~\cite[Lemma~8]{Bshouty:1995:minwinoadd} (and thus
require at least $4$ operations to be computed).

There thus remains now only to consider matrices with exactly $3$ rows
that are multiple of vectors of the canonical basis.
%
Let $\mat{M}$ be the $4{\times}4$ sub-matrix obtained from
$\mat{\alpha}$ (or $\mat{\beta}$) by removing those $3$ vectors of the
canonical basis.
By~\cref{lem:nocannorzero}, no column of $\mat{M}$ can be the zero column:
otherwise a $7$-dimensional column of $\mat{\alpha}$ (or
$\mat{\beta}$) would be either a multiple of a canonical basis vector,
or the zero vector.
This means that every variable of $\vec{a}$ has to be used at least
once to realize the $4$ operations of $\mat{M}\vec{a}$.
Now suppose that there exists an in-place algorithm realizing
$\mat{M}\vec{a}$ in $5$ elementary operations.
Any operations among these $5$ that, as its results, puts back a
variable into its initial state, does not realize any row of
$\mat{M}\vec{a}$ (because putting back a variable to its initial state
is the trivial identity on this initial variable, and this would be
represented by a $4$-dimensional vector of the canonical basis, which
$M$ do not contain, by construction).
Therefore, at most one among these $5$ operations puts back a variable
of $\vec{a}$ into its initial state (otherwise $\mat{M}\vec{a}$, and
therefore  $\mat{\alpha}\vec{a}$ or $\mat{\beta}\vec{a}$, would be
realizable in strictly less than $4$ operations).
This means that at most one variable of $\vec{a}$ can be modified during
the algorithm (otherwise the algorithm would not be able to put back
all its input variables into their initial state).

W.l.o.g suppose this only modified variable is $a_1$.
Finally, as all the other $3$ variables must be used in at least one
of the $5$ elementary operations, at least $3$ operations are
of the form $a_1\pe\lambda_i{a_i}$ for $i=2,3,4$ and some constants $\lambda_i$.
After those, to put back $a_1$ into its initial state, each one
of these $3$ independent variables, $a_2$, $a_3$ and $a_4$, must be ``removed''
from $a_1$ at some point of the elementary program.
But, with a total of $5$ operations, there remains only $2$ other possible
elementary operations, each one of those modifying only~$a_1$.
Therefore not all $3$ variables can be removed and thus no in-place
algorithm can use only $5$ operations.
\end{proof}
%
Finally, there remains to consider the linear combinations of the $7$
multiplications to conclude that~\cref{alg:ipsw} realizes the minimal
number of operations for any in-place algorithm with $7$
multiplications.
%
\begin{theorem}\label{thm:eighteen}
  At least $25$ additions are required to compute in-place
  any bilinear matrix multiplication algorithm using
  only $7$ multiplications and to restore
  its two input matrices to their initial states afterwards.
\end{theorem}
\begin{proof} \Cref{prop:six} shows that at least $6$ operations are
  required to realize $\alpha$ (or $\beta$).
  For $\mu$, we in fact compute $\vec{c}\pe\mu\vec{\rho}$,
  so we need to consider the matrix
  $P=\begin{smatrix}I_4&\mu\end{smatrix}\in\F^{4{\times}11}$
  and
  the vector
  $\vec{\xi}=\begin{smatrix}\vec{c}\\\vec{\rho}\end{smatrix}$.
  Consider now an elementary program that realizes $P\vec{\xi}$,
  in-place of $\vec{c}$ only. This implies for instance that if
  $\vec{\rho}$ is zero, $\vec{c}$ should be put back to its initial
  state at the end of the program.
  Finally, thus consider the transposed program
  $\Transpose{P}\vec{\underline{c}}$: it must be in-place of
  $\vec{\underline{c}}$, while putting back $\vec{\underline{c}}$ to
  its initial state afterwards.
  Now, by \cref{prop:six}, $\Transpose{\mu}$, and thus
  $\Transpose{P}\in\F^{11{\times}4}$, requires at least $6$ elementary
  operations to be performed.
  By Tellegen's transposition principle, see
  also~\cite[Theorem~7]{Kaminski:1988:transpose}, computing the
  transposed program thus requires at least $6+(11-4)=13$ operations.
  This gives a total of at least $6+6+13=25$ additions.
\end{proof}

\Cref{thm:eighteen} thus shows that our~\cref{alg:ipsw} with $18$
elementary additions and $7$ recursive calls (thus $7$ more, and a
total of $18+7=25$ additions) is an optimal in-place
bilinear matrix multiplication algorithm using
only $7$ multiplications.
