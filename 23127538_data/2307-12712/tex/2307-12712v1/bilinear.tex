\RequirePackage{amssymb}
\documentclass{article}
\usepackage{fullpage}
%
\usepackage{amsmath,amsthm}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{arydshln} %
\usepackage{xcolor}
\usepackage{enumitem}

%
%
%
%
%
%
%
%
%
%
%
%
%%  color's stuff (now already in packages xcolor or color
\makeatletter
\def\mathcolor#1#{\@mathcolor{#1}}
\def\@mathcolor#1#2#3{%
\protect\leavevmode
\begingroup
\color#1{#2}#3%
\endgroup
}
\makeatother

%


%
%
%
%
%


%

\usepackage{xspace}
\usepackage{booktabs}
\usepackage[symbol]{footmisc}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\usepackage{algorithm}
\usepackage{algcompatible}
\newcommand{\To}{\textbf{to}\xspace}
\newcommand{\DownTo}{\textbf{down-to}\xspace}
\newcommand{\algorithmicreturn}{\textbf{return}}
\newcommand{\RETURN}{\STATE\algorithmicreturn{}\xspace}
\algnewcommand{\IfThen}[2]{%
  \State \algorithmicif\ #1\ \algorithmicthen\ #2}
\algnewcommand{\IfThenEnd}[2]{%
  \State \algorithmicif\ #1\ \algorithmicthen\ #2\ \algorithmicend\ \algorithmicif}
\algnewcommand{\IfThenElse}[3]{%
  \State \algorithmicif\ #1\ \algorithmicthen\ #2\ \algorithmicelse\ #3}
\renewcommand{\algorithmicrequire}{{\textbf{Input:}}}
\renewcommand{\algorithmicensure}{{\textbf{Output:}}}
\algnewcommand\algorithmicreadonly{\textbf{Read-only:}}
\algnewcommand\READONLY{\item[\algorithmicreadonly]}%
\newcommand\lFOR[2]{\STATE\algorithmicfor~{#1}~\algorithmicdo~{#2}~\algorithmicend}

\newcommand{\D}{\ensuremath{\mathbb{D}}\xspace}
\newcommand{\F}{\ensuremath{\mathbb{F}}\xspace}
\newcommand{\M}{\ensuremath{\mathfrak{M}}}
\newcommand{\bnd}[2]{\ensuremath{#1\mathopen{}(#2)\mathclose{}}}
\newcommand{\bnddisplay}[2]{\ensuremath{#1\mathopen{}\left(#2\right)\mathclose{}}}
\newcommand{\softoh}[1]{\bnd{\tilde{\mathcal{O}}}{#1}}
\newcommand{\bigO}[1]{\bnd{\mathcal{O}}{#1}}
\newcommand{\bigOdisplay}[1]{\bnddisplay{\mathcal{O}}{#1}}


\newcommand{\threshold}{\ensuremath{\text{Threshold}}}
\DeclareMathOperator{\me}{\,\ensuremath{\mathrel{\textrm{--}}=}\,}
\DeclareMathOperator{\pe}{\,\ensuremath{\mathrel{+}=}\,}
\DeclareMathOperator{\fe}{\,\ensuremath{\mathrel{{\star}}=}\,}
\DeclareMathOperator{\de}{\,\ensuremath{\mathrel{/}=}\,}
\newcommand{\rev}[1]{\ensuremath{{\operatorname{rev}}\left(#1\right)}}
\newcommand{\Toeplitz}[1]{\ensuremath{\operatorname{\mathcal{T}_{#1}}}}
\newcommand{\Circulant}[1]{\ensuremath{\operatorname{\mathscr{C}_{#1}}}}
\newcommand{\random}{\stackrel{\$}{\leftarrow}}

%
\newcommand\DFT{\mathsf{DFT}}
\newcommand\brDFT{\mathsf{brDFT}}
\newcommand\brTFT{\mathsf{brTFT}}
\newcommand\partTFT{\mathsf{partTFT}}

%
\makeatletter
\DeclareRobustCommand{\cev}[1]{%
  {\mathpalette\do@cev{#1}}%
}
\newcommand{\do@cev}[2]{%
  \vbox{\offinterlineskip
    \sbox\z@{$\m@th#1 x$}%
    \ialign{##\cr
      \hidewidth\reflectbox{$\m@th#1\vec{}\mkern4mu$}\hidewidth\cr
      \noalign{\kern-\ht\z@}
      $\m@th#1#2$\cr
    }%
  }%
}
\makeatother
\newcommand{\mat}[1]{\textbf{\sc{\ensuremath{#1}}}}

%
\newenvironment{smatrix}{\left[\begin{smallmatrix}}{\end{smallmatrix}\right]}
%
%
\def\matrixsize#1#2{{{#1}\times{#2}}}
\def\MatrixProduct#1#2{{\mat{#1}\cdot\mat{#2}}}
%
\def\triadone{brown}
\def\triadtwo{red}
\def\triadthree{blue}
\def\triadfour{green}
\def\triadfive{purple}
\def\triadsix{dimgray}
\def\triadseven{chocolate}

%
\usepackage{pifont}
\usepackage{savesym}
\savesymbol{checkmark}
\newcommand{\gcheck}{{\color{darkgreen}\checkmark}}
\usepackage{dingbat}
\newcommand{\xyes}{{\gcheck}}
\newcommand{\xno}{{\color{darkred}\ding{55}}}
\newcommand{\xquestion}{\color{darkred}{\fontfamily{cyklop}\selectfont\textit{?}}}


%

\newcommand{\MUL}{\normalfont{\textbf{MUL}}\xspace}
\newcommand{\ADD}{\normalfont{\textbf{ADD}}\xspace}
\newcommand{\SCA}{\normalfont{\textbf{SCA}}\xspace}
%
\title{Fast in-place accumulated bilinear formulae}
\author{Jean-Guillaume Dumas\footnote{
  {Universit\'e Grenoble Alpes}.
  {Laboratoire Jean Kuntzmann, CNRS, UMR 5224}.
  {150 place du Torrent, IMAG - CS 40700},
  {38058 Grenoble, cedex 9}
  {France}.
\href{mailto:Jean-Guillaume.Dumas@univ-grenoble-alpes.fr,Bruno.Grenet@univ-grenoble-alpes.fr}{\{firstname.lastname\}@univ-grenoble-alpes.fr}}
\and{Bruno Grenet}\footnotemark[1]}


%
\usepackage{color,svgcolor}
\usepackage[plainpages=true]{hyperref}
\makeatletter
\hypersetup{%
	pdftitle={\@title},
	pdfauthor={Jean-Guillaume Dumas and Bruno Grenet},
	breaklinks=true,
	colorlinks=true,
	linkcolor=darkred,
	citecolor=blue,
	urlcolor=darkgreen,
    hypertexnames=false,
}
\makeatother
%
\usepackage[capitalize]{cleveref}


%
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{definitions}[theorem]{Definitions}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{facts}[theorem]{Facts}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
%


%
\begin{document}
%
\maketitle
%
%
\tableofcontents
\section{Introduction}
Bilinear operations are ubiquitous in computer science and in
particular in computer algebra and symbolic computation.
One of the most fundamental arithmetic operation is the
multiplication, and when applied to, e.g., polynomials or matrices,
its result is a bilinear function of its inputs.

In terms of arithmetic operations, from the work
of~\cite{MoenckBorodin1972,Kung1974,Strassen:1969:GENO}, many
sub-quadratic (resp. sub-cubic) algorithms
were developed for these tasks.
But these fast algorithms come at the expense
of (potentially large) extra temporary space to perform the
computation. On the contrary, classical, quadratic (resp. cubic)
algorithms, when computed sequentially, quite often require very few
(constant) extra registers.
Further work then proposed simultaneously ``fast'' and ``in-place''
algorithms, for both matrix and polynomial
operations~\cite{jgd:2009:WinoSchedule,Roche:2009:spacetime,Harvey:2010:issactft,Giorgi:2019:issac:reductions,Giorgi:2020:issac:inplace}.

We here propose algorithms to extend the latter line of work for
\emph{accumulated} algorithms arising from a bilinear formula.
Indeed one of the main ingredient of the latter line of work is to use
the (free) space of the output as intermediate storage.
When the result has to be accumulated, i.e., if the output is also
part of the input, this free space does not even exist.

To be able to design accumulated in-place algorithms we thus relax the in-place
model to allow algorithms to also modify their input, therefore to use
them as intermediate storage for instance,
\emph{provided that they are restored to their initial state after
  completion of the procedure}.
This is in fact a natural possibility in many programming environments.
Furthermore, this restoration allows for recursive combinations of
such procedures, as the (non concurrent) recursive calls will not
mess-up the state of their callers.

We propose here a generic technique transforming any bilinear
algorithm into an in-place algorithm under this model.
This then directly applies to polynomial and matrix
multiplication algorithms, including fast ones.

Next we first detail our model for in-place computations in
\cref{ssec:inplace}
and recall some quadratic in-place algorithms
in~\cref{ssec:quad}.
From this, we detail in~\cref{app:bilin} our novel technique for in-place
accumulation.
Then we apply this technique and further optimizations in order to
derive new fast and in-place algorithms for the accumulated
multiplication of matrices, \cref{sec:strassen}, and polynomials,
\cref{sec:inpaccpol}.

\subsection{In-place model}\label{ssec:inplace}
There exist different models for in-place algorithms.
We here choose to call \emph{in-place} an algorithm using only
\textbf{the space of its inputs, its outputs, and at most $\bigO{1}$ extra
  space}. But algorithms are only allowed to modify their inputs,
\textbf{if their inputs are restored to their initial
  state} afterwards.
This is a less powerful model than when the input is purely read-only,
but it turns out to be crucial in our case, especially when we have
accumulation operations.

The algorithms we describe are \emph{in-place with accumulation}. The 
archetypical example is a multiply-accumulate operation $a \pe b\times c$. 
For such an algorithm, the condition is that $b$ and $c$ are restored
to their initial states at the end of the computation, while $a$ (which
is also part of the input) is replaced by $a+bc$.

%
%
%
%
%
%

Also, as a variant, by \emph{over-place}, we mean an algorithm where
the output replaces (parts of) its input (e.g., like
$\vec{a}=b{\cdot}\vec{a}$).
Similarly, we allow all of the input to be modified, provided that the
parts of the input that are not the output are restored afterwards.
In the following we signal by a ``\algorithmicreadonly'' tag the parts
of the input that the algorithm is not allowed to modify (the other
parts are modifiable as long as they are restored). 
Note that in-place algorithms with accumulation are a special case of
over-place algorithms. 

For recurive algorithms, some space may be required to store the recursive
call stack. (This space is bounded by the recursion depth of the
algorithms, and managed in practice by the compiler.) 
In our complexity summary (\Cref{tab:kara,tab:fft}), the space complexity includes
the stack. Nonetheless, we call \emph{in-place} a recursive algorithm whose only
extra space is the call stack.

The main limitations of this model are for black-box inputs, or for
different inputs whose representations share some data. For more details
on these models, we refer to~\cite{Roche:2009:spacetime,Giorgi:2019:issac:reductions}.

\subsection{In place and over-place classical algorithms}\label{ssec:quad}
For the sake of completeness, we recall here,
in~\cref{alg:classicmul}, that classical algorithms for matrix or
polynomial operations can be performed strictly in-place.

\begin{algorithm}[htbp]\caption{Quadratic/cubic in-place accumulated
    polynomial/matrix multiplication}\label{alg:classicmul}
\begin{minipage}{.4\columnwidth}
\begin{algorithmic}[1]
\REQUIRE $A(X)$, $B(X)$, $C(X)$ polynomials of degrees $m$, $n$, $m+n$.
\READONLY $A,B$.
\ENSURE $C(X)\pe{A(X)B(X)}$
\FOR{$i=0$ \To $m$}
\FOR{$j=0$ \To $n$}
\STATE $C[i+j]\pe{A[i]B[j]}$;
\ENDFOR
\ENDFOR
\end{algorithmic}
\,\\
\end{minipage}\hfill
\begin{minipage}{.5\columnwidth}
\begin{algorithmic}[1]
\REQUIRE $A$, $B$, $C$ matrices of dimensions $m{\times}\ell$,
$\ell{\times}n$, $m{\times}n$.
\READONLY $A,B$.
\ENSURE $C\pe{AB}$
\FOR{$i=0$ \To $m$}
\FOR{$j=0$ \To $n$}
\FOR{$k=0$ \To $\ell$}
\STATE $C_{ij}\pe{A_{ik}B_{kj}}$;
\ENDFOR
\ENDFOR
\ENDFOR
\end{algorithmic}
\end{minipage}
\end{algorithm}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%

\section{In-place computation of accumulated bilinear
  formulae}\label{app:bilin}
Karatsuba polynomial multiplication and Strassen matrix multiplication
are famous optimizations of bilinear formulae on their inputs: results
are linear combinations of products of bilinear combinations of the inputs.

To compute recursively such a formula in-place, the idea is to perform
each product one at a time. For each product, both factors are
linearly combined in-place into one of the entry beforehand and
restored afterwards. Then the product of both entries is accumulated in
one part of the output and then distributed to the other parts.
The difficulty is to perform this distribution in-place, without
recomputing the product. For this we presubtract one output to the
other, then accumulate the product to one product, and finally add the
newly accumulated output to the other one: overall both outputs just
have accumulated the product, in-place. Potential constant factors can
also be dealt with pre divisions and post multiplications.
Basically we need two kind of in-place operations, and their
combinations:
\begin{enumerate}
\item In-place accumulation of a quantity multiplied by a
(known in advance) invertible constant; this is shown in~\cref{eq:basemul}.
\begin{equation}\label{eq:basemul}
\mu\left(\left(\mu^{-1}c\right)\pe{m}\right)
\quad\text{computes in-place}\quad
c\gets{c+\mu\cdot{m}}
\end{equation}
\item In-place distribution of the same quantity, without
  recomputation, to several outputs; this is shown
  in~\cref{eq:basedist}.
\begin{equation}\label{eq:basedist}
\left.\begin{aligned}
d&\me{c};\\
c&\pe{m};\\
d&\pe{c};\\
\end{aligned}\right\rbrace
\quad\text{computes in-place}\quad
\begin{cases}
c& \gets{c+m}\\
d& \gets{d+m}\\
\end{cases}
\end{equation}
\end{enumerate}


\Cref{ex:bilin} shows how to combine several of these operations,
also linearly combining parts of the input.
\begin{example}\label{ex:bilin}
  Suppose, that for some inputs/outputs $a,b,c,d,r,s$ one wants to compute an
  intermediate product $p=(a+3b)*(c+d)$ only once and then distribute
  and accumulate that product to two of its outputs (or results),
  such that we have both $r\gets{r+5p}$ and $s\gets{s+2p}$.
  One way to perform this in-place is first to
  accumulate $a\pe{3b}$ and $c\pe{d}$, together with pre-dividing $r$
  by $5$, as in~\cref{eq:basemul}. Then we will compute and
  accumulate $p$ to $r$, knowing that now we directly have $p=ac$.
  But to distribute $p$ to $s$ without recomputing it we first need to
  prepare $s$: divide it by $2$, and pre-subtract $r$. This is
  $s\de{2}$, $s\me{r}$. Now we can compute and accumulate the product
  $r\pe{ac}$. After this, we can reciprocate (or unroll) all our
  precomputations (in order to distribute this product to the other
  result, $s$, and to restore the inputs, other than those that are
  also results, to their initial state).
  For $s$, another possibility is to directly pre-subtract $2r$ and
  to post-add $2r$.
  This is summarized in~\cref{eq:example}.
\begin{equation}\label{eq:example}
%
\left.\begin{aligned}
a\pe{3b}; &\quad c\pe{d}; & r\de{5};\\
s\me{2r}; &\quad r\pe{ac}; & s\pe{2r}; \\
r\fe{5}; &\quad c\me{d}; & a\me{3b};\\
\end{aligned}\right\rbrace
\quad\text{computes in-place}\quad
\begin{cases}
r& \gets{r+5(a+3b)(c+d)}\\
s& \gets{s+2(a+3b)(c+d)}\\
\end{cases}
\end{equation}
\end{example}



\Cref{alg:bilin} shows how to implement this in general, taking into
account the constant (or read-only) multiplicative coefficients of all
the linear combinations. We suppose that inputs are in three distinct
sets: left-hand sides, $\vec{a}$, right-hand sides, $\vec{b}$, and those
accumulated to the results, $\vec{c}$.
We denote by $\otimes$ the point-wise multiplications of
left-hand sides by right-hand sides.
Then~\cref{alg:bilin} computes $\vec{c}\pe\mat{\mu}\vec{m}$, for
$\vec{m}=(\mat{\alpha}\vec{a})\otimes(\mat{\beta}\vec{b})$, with
$\mat{\alpha}$, $\mat{\beta}$ and $\mat{\mu}$ matrices of
constants.

\begin{algorithm}[htbp]
  \caption{In place bilinear formula}\label{alg:bilin}
  \begin{algorithmic}[1]\small
    \REQUIRE $\vec{a}\in\D^m$, $\vec{b}\in\D^n$, $\vec{c}\in\D^s$;
    $\mat{\alpha}\in\D^{t{\times}m}$, $\mat{\beta}\in\D^{t{\times}n}$,
    $\mat{\mu}\in\D^{s{\times}t}$, with no zero-rows in $\alpha$,
    $\beta$, $\mu$.
    \READONLY$\mat{\alpha},\mat{\beta},\mat{\mu}$.
    \ENSURE $\vec{c}\pe\mat{\mu}\vec{m}$, for
    $\vec{m}=(\mat{\alpha}\vec{a})\otimes(\mat{\beta}\vec{b})$.
    \FOR{$\ell=1$ \To $t$}
    \STATE Let $i$ s.t. $\alpha_{\ell,i}\neq{0}$;
    \STATE\label{lin:alpha}$a_i\fe\alpha_{\ell,i}$;
    \algorithmicfor~{$\lambda=1$ \To $m$, $\lambda\neq{i}$, $\alpha_{\ell,\lambda}\neq{0}$ }~
	\algorithmicdo~{$a_i\pe\alpha_{\ell,\lambda}a_\lambda$}~
    \algorithmicend
    \STATE Let $j$ s.t. $\beta_{\ell,j}\neq{0}$;
    \STATE\label{lin:beta}$b_j\fe\beta_{\ell,j}$;
    \algorithmicfor~{$\lambda=1$ \To $n$, $\lambda\neq{j}$, $\beta_{\ell,\lambda}\neq{0}$}~
	\algorithmicdo~{$b_j\pe\beta_{\ell,\lambda}b_\lambda$}~
    \algorithmicend
%
%
%
%
%
%
%
%
    \STATE Let $k$ s.t. $\mu_{k,\ell}\neq{0}$;
    \STATE\label{lin:lambdaaddmul}$c_k\de\mu_{k,\ell}$;
    \algorithmicfor~{$\lambda=1$ \To $s$, $\lambda\neq{k}$, $\mu_{\lambda,\ell}\neq{0}$}~
	\algorithmicdo~{$c_\lambda\me\mu_{\lambda,\ell}{c_k}$}~
    \algorithmicend
    \STATE\label{lin:product}$c_k\pe{a_i\cdot{b_j}}$\hfill\COMMENT{This is the product $m_\ell$}
    \STATE
    \algorithmicfor~{$\lambda=1$ \To $s$, $\lambda\neq{k}$, $\mu_{\lambda,\ell}\neq{0}$}~
	\algorithmicdo~{$c_\lambda\pe\mu_{\lambda,\ell}{c_k}$}~
    \algorithmicend;~$c_k\fe\mu_{k,\ell}$;\hfill\COMMENT{undo~\ref{lin:lambdaaddmul}}
%
%
%
%
%
%
%
%
    \STATE
    \algorithmicfor~{$\lambda=1$ \To $n$, $\lambda\neq{j}$, $\beta_{\ell,\lambda}\neq{0}$}~
	\algorithmicdo~{$b_j\me\beta_{\ell,\lambda}b_\lambda$}~
    \algorithmicend;~$b_j\de\beta_{\ell,j}$;\hfill\COMMENT{undo~\ref{lin:beta}}
    \STATE
    \algorithmicfor~{$\lambda=1$ \To $m$, $\lambda\neq{i}$, $\alpha_{\ell,\lambda}\neq{0}$}~
	\algorithmicdo~{$a_i\me\alpha_{\ell,\lambda}a_\lambda$}~
    \algorithmicend;~$a_i\de\alpha_{\ell,i}$;\hfill\COMMENT{undo~\ref{lin:alpha}}
    \ENDFOR
    \RETURN $\vec{c}$.
  \end{algorithmic}
\end{algorithm}

To simplify the counting of operations,
we denote the addition or subtraction of elements, $\pe$ or $\me$, by \ADD,
the (tensor) product of elements, $\cdot$, by \MUL,
and the scaling by constants, $\fe$ or $\de$, by
\SCA.

We also denote by $\#x$ (resp. $\sharp{x}$) the number of
non-zero (resp. $\not\in\{0,1,-1\}$) elements in a matrix $x$.

\begin{theorem}\label{thm:bilin}
\Cref{alg:bilin} is correct, in-place, and requires
$t$ \MUL,
$2(\#\alpha+\#\beta+\#\mu)-5t$ \ADD and
$2(\sharp\alpha+\sharp\beta+\sharp\mu)$ \SCA operations.
\end{theorem}
\begin{proof}
First, as the only used operations ($\pe$, $\me$, $\fe$, $\de$) are
in-place ones, the algorithm is in-place.
Second, the algorithm is correct both for the input and the
output:
The input is well restored, as
$(\alpha_{\ell,i}a_i+\sum\alpha_{\ell,\lambda}a_\lambda-\sum\alpha_{\ell,\lambda}a_{\lambda})/\alpha_{\ell,i}=a_i$
and
$(\beta_{\ell,j}b_j+\sum\beta_{\ell,\lambda}b_\lambda-\sum\beta_{\ell,\lambda}b_\lambda)/\beta_{\ell,j}=b_j$;
The output is correct as
$c_\lambda-\mu_{\lambda,\ell}c_k/\mu_{k,\ell}+\mu_{\lambda,\ell}(c_k/\mu_{k,\ell}+a_ib_j)=c_\lambda+\mu_{\lambda,\ell}a_ib_j$
and
$(c_k/\mu_{k,\ell}+a_ib_j)\mu_{k,\ell}=c_k+\mu_{k,\ell}a_ib_j$.
Third, for the number of operations,
\cref{lin:alpha} requires one multiplication by a constant for each
non-zero element $a_{\lambda}$ in the row and one less addition.
But multiplications and divisions by $1$ are no-op, and by $-1$ can be
dealt with subtraction. This gives the total of $\#\alpha-t$ additions
and $\sharp\alpha$ constant multiplications.
\cref{lin:beta} is similar for each non-zero element $b_{\lambda}$.
\cref{lin:lambdaaddmul} is for each non-zero element in
$\mu$. Finally \cref{lin:product} performs $t$ multiplications of
elements and $t$ additions. The remaining lines double the number of
\ADD and \SCA.
This is $t+2(\#\alpha+\#\beta+\#\mu-3t)=2(\#\alpha+\#\beta+\#\mu)-5t$ \ADD.
\end{proof}

For instance, to use~\cref{alg:bilin} with matrices or polynomials,
each
product $m_\ell$ is in fact computed recursively (in an actual
implementation of a fixed formula, one could of course combine more
efficiently the pre- and post-computations over the main loop), see
next sections.

\begin{remark} Similarly, slightly more generic operations of the form
  $\vec{c}\gets\vec{\gamma}\otimes\vec{c}+\mat{\mu}\vec{m}$, for
  a vector $\gamma\in\D^{s}$, can also be computed in-place:
  precompute first $\vec{c}\gets\vec{\gamma}\otimes\vec{c}$, then
  call~\cref{alg:bilin}.
\end{remark}

\section{In-place Strassen matrix multiplication with accumulation}\label{sec:strassen}

Considered as~${\matrixsize{2}{2}}$ matrices, the matrix
product with accumulation ~${\mat{C}\pe\MatrixProduct{A}{B}}$ could be computed using
Strassen-Winograd algorithm by performing the following computations:
\begin{equation}
\label{eq:StrassenWinogradMultiplicationAlgorithm}
\begin{array}{ll}
\mathcolor{\triadone}{\rho_{1}}\gets{\mathcolor{\triadone}{a_{11}}\mathcolor{\triadone}{b_{11}}},
&
\mathcolor{\triadtwo}{\rho_{2}}\gets{\mathcolor{\triadtwo}{a_{12}}\mathcolor{\triadtwo}{b_{21}}},
\\
\mathcolor{\triadthree}{\rho_{3}}\gets{(\mathcolor{\triadthree}{-a_{11}-a_{12}+a_{21}+a_{22}})\mathcolor{\triadthree}{b_{22}}},
&
\mathcolor{\triadfour}{\rho_{4}}\gets{\mathcolor{\triadfour}{a_{22}}(\mathcolor{\triadfour}{-b_{11}+b_{12}+b_{21}-b_{22}})},
\\
\mathcolor{\triadfive}{\rho_{5}}\gets{(\mathcolor{\triadfive}{a_{21}+a_{22}})(\mathcolor{\triadfive}{-b_{11}+b_{12}})},
&
\mathcolor{\triadsix}{\rho_{6}}\gets{(\mathcolor{\triadsix}{-a_{11}+a_{21}})(\mathcolor{\triadsix}{b_{12}-b_{22}})},
\\
\mathcolor{\triadseven}{\rho_{7}}\gets{(\mathcolor{\triadseven}{-a_{11}+a_{21}+a_{22}})(\mathcolor{\triadseven}{-b_{11}+b_{12}-b_{22}})},
\\[\medskipamount]
\multicolumn{2}{c}{
\begin{bmatrix} c_{11} &c_{12} \\ c_{21} &c_{22} \end{bmatrix}
\pe
\begin{bmatrix}
\mathcolor{\triadone}{\rho_{1}} + \mathcolor{\triadtwo}{\rho_{2}} &
\mathcolor{\triadone}{\rho_{1}} - \mathcolor{\triadthree}{\rho_{3}} + \mathcolor{\triadfive}{\rho_{5}} - \mathcolor{\triadseven}{\rho_{7}}\\
\mathcolor{\triadone}{\rho_{1}} + \mathcolor{\triadfour}{\rho_{4}} +
\mathcolor{\triadsix}{\rho_{6}} - \mathcolor{\triadseven}{\rho_{7}} &
\mathcolor{\triadone}{\rho_{1}}+\mathcolor{\triadfive}{\rho_{5}} +
\mathcolor{\triadsix}{\rho_{6}} - \mathcolor{\triadseven}{\rho_{7}}
\end{bmatrix}\!.}
\end{array}
\end{equation}
This algorithm uses $7$ multiplications of half-size matrices and $24+4$
additions (that can be factored into only $15+4$,
see~\cite{Winograd:1977:complexite}:
$4$ involving $A$, $4$ involving $B$ and $7$ involving the products,
plus $4$ for the accumulation).
This can be used recursively on matrix blocks, halved at each
iteration, to obtain a sub-cubic algorithm. To save on operations, it
is of course interesting to compute the products only once, that is
store them in extra memory chunks. In order to reduce the overall
memory footprint, it is then desirable to minimize the number (or the
volume) of these extra variables.
To date, the best versions that reduce the extra memory space, also
overwriting the
input matrices (but not putting them back in place) were proposed
in~\cite{jgd:2009:WinoSchedule}.
There, an in-place algorithm for the product without accumulation was
proposed.
But for the accumulated product the best obtained memory footprint,
for a sub-cubic algorithm, was $2$ temporary blocks per recursive
level, thus a total of extra memory required to be~$\frac{2}{3}n^2$.

With~\cref{alg:bilin} we instead obtain a sub-cubic algorithm for
accumulated matrix multiplication with $\bigO{1}$ extra space requirement.

From~\cref{eq:StrassenWinogradMultiplicationAlgorithm} indeed, we can extract
the matrices $\mu$, $\alpha$ and $\beta$ to be used in \cref{alg:bilin} as
follows:
\begin{equation}
\mu=\begin{bmatrix}
\mathcolor{\triadone}{1}&\mathcolor{\triadtwo}{1}&\mathcolor{\triadthree}{0}&\mathcolor{\triadfour}{0}&\mathcolor{\triadfive}{0}&\mathcolor{\triadsix}{0}&\mathcolor{\triadseven}{0}\\
\mathcolor{\triadone}{1}&\mathcolor{\triadtwo}{0}&\mathcolor{\triadthree}{-1}&\mathcolor{\triadfour}{0}&\mathcolor{\triadfive}{1}&\mathcolor{\triadsix}{0}&\mathcolor{\triadseven}{-1}\\
\mathcolor{\triadone}{1}&\mathcolor{\triadtwo}{0}&\mathcolor{\triadthree}{0}&\mathcolor{\triadfour}{1}&\mathcolor{\triadfive}{0}&\mathcolor{\triadsix}{1}&\mathcolor{\triadseven}{-1}\\
\mathcolor{\triadone}{1}&\mathcolor{\triadtwo}{0}&\mathcolor{\triadthree}{0}&\mathcolor{\triadfour}{0}&\mathcolor{\triadfive}{1}&\mathcolor{\triadsix}{1}&\mathcolor{\triadseven}{-1}
\end{bmatrix}\quad\quad
\alpha=\begin{bmatrix}
\mathcolor{\triadone}{1}&\mathcolor{\triadone}{0}&\mathcolor{\triadone}{0}&\mathcolor{\triadone}{0}\\
\mathcolor{\triadtwo}{0}&\mathcolor{\triadtwo}{1}&\mathcolor{\triadtwo}{0}&\mathcolor{\triadtwo}{0}\\
\mathcolor{\triadthree}{-1}&\mathcolor{\triadthree}{-1}&\mathcolor{\triadthree}{1}&\mathcolor{\triadthree}{1}\\
\mathcolor{\triadfour}{0}&\mathcolor{\triadfour}{0}&\mathcolor{\triadfour}{0}&\mathcolor{\triadfour}{1}\\
\mathcolor{\triadfive}{0}&\mathcolor{\triadfive}{0}&\mathcolor{\triadfive}{1}&\mathcolor{\triadfive}{1}\\
\mathcolor{\triadsix}{-1}&\mathcolor{\triadsix}{0}&\mathcolor{\triadsix}{1}&\mathcolor{\triadsix}{0}\\
\mathcolor{\triadseven}{-1}&\mathcolor{\triadseven}{0}&\mathcolor{\triadseven}{1}&\mathcolor{\triadseven}{1}
\end{bmatrix}\quad\quad
\beta=\begin{bmatrix}
\mathcolor{\triadone}{1}&\mathcolor{\triadone}{0}&\mathcolor{\triadone}{0}&\mathcolor{\triadone}{0}\\
\mathcolor{\triadtwo}{0}&\mathcolor{\triadtwo}{0}&\mathcolor{\triadtwo}{1}&\mathcolor{\triadtwo}{0}\\
\mathcolor{\triadthree}{0}&\mathcolor{\triadthree}{0}&\mathcolor{\triadthree}{0}&\mathcolor{\triadthree}{1}\\
\mathcolor{\triadfour}{-1}&\mathcolor{\triadfour}{1}&\mathcolor{\triadfour}{1}&\mathcolor{\triadfour}{-1}\\
\mathcolor{\triadfive}{-1}&\mathcolor{\triadfive}{1}&\mathcolor{\triadfive}{0}&\mathcolor{\triadfive}{0}\\
\mathcolor{\triadsix}{0}&\mathcolor{\triadsix}{1}&\mathcolor{\triadsix}{0}&\mathcolor{\triadsix}{-1}\\
\mathcolor{\triadseven}{-1}&\mathcolor{\triadseven}{1}&\mathcolor{\triadseven}{0}&\mathcolor{\triadseven}{-1}
\end{bmatrix}
\end{equation}

All coefficients being $1$ or $-1$ the resulting in-place algorithm
can of course compute the accumulation $C\pe{AB}$ without constant
multiplications.
It thus requires $7$ recursive calls and, from~\cref{thm:bilin},
at most $2(\#\alpha+\#\beta+\#\mu-3t)=2(14+14+14-3*7)=42$ block
additions.
Just like the $24$ additions
of~\cref{eq:StrassenWinogradMultiplicationAlgorithm} can be factored
into $15$, one can optimize also the in-place algorithm.
For instance, looking at $\alpha$ we see that performing the products
in the order $\rho_{6}$, $\rho_{7}$, $\rho_{3}$, $\rho_{5}$ and
accumulating in $a_{21}$ allows to perform all additions/subtractions
in $A$ with an optimal number of only $6$ operations.
This is similar for $\beta$ if the order
$\rho_{6}$, $\rho_{7}$, $\rho_{4}$, $\rho_{5}$ is used and
accumulation is in $b_{12}$.
Thus ordering for instance $\rho_{6}$, $\rho_{7}$, $\rho_{4}$,
$\rho_{3}$, $\rho_{5}$ will reduce the number of block additions to $26$.
Now looking at $\mu$, $\rho_{6}$ being used only with $\rho_{1}$ one
can save two more additions in $C$ if they are computed one right
before the other. Then $\rho_{6}$ and $\rho_{7}$ together also saves
two more additions in $C$.

So using, e.g. the ordering
$\rho_{1},\rho_{6},\rho_{7},\rho_{4},\rho_{3},\rho_{5},\rho_{2}$
requires only $22$ additions, as shown in~\cref{app:inplsw}.

Thus, without thresholds and for powers of two, the dominant term of
the overall arithmetic cost is $\frac{28}{3}n^{\log_2(7)}$, for the
in-place version,
roughly only half more operations than the $6n^{\log_2(7)}$ cost
for the version using extra temporaries.

Any bilinear algorithm for matrix multiplication (see, e.g.,
\url{https://fmm.univ-lille.fr/}) can be dealt with similarly.

%
\section{In-place polynomial multiplication with
  accumulation}\label{sec:inpaccpol}
\Cref{alg:bilin} can also be used for polynomial multiplication.
One difficulty now is that this does not completely fits the setting,
as multiplication of two size-$n$ inputs will in general span a double
size-$2n$ output.
This is not an issue until one has to distribute separately the two
halves of this $2n$ values (or more generally to different parts of
different outputs).
In the following we show that this can anyway always be done
for polynomial multiplications.

\subsection{In-place Karatsuba polynomial multiplication with
  accumulation}\label{ssec:kara}

For instance, we can immediately obtain an in-place Karatsuba
polynomial multiplication this way.
Karatsuba polynomial multiplication, indeed, writes as:
\begin{equation}\label{eq:kara}
(Ya_1 + a_0)(Yb_1 +
b_0)=a_0b_0+Y((a_1+a_0)(b_1+b_0)-a_0b_0-a_1b_1)+Y^2(a_1b_1).
\end{equation}

From~\cref{eq:kara} we can extract the associated $\mu$, $\alpha$,
$\beta$ matrices, as shown in~\cref{eq:bilinkara}.
%
\begin{equation}\label{eq:bilinkara}
\mu=\begin{bmatrix}
1&0&0\\
-1&1&-1\\
0&0&1
\end{bmatrix}\quad\quad
\alpha=\begin{bmatrix}
1&0\\
1&1\\
0&1
\end{bmatrix}\quad\quad
\beta=\begin{bmatrix}
1&0\\
1&1\\
0&1
\end{bmatrix}
\end{equation}


Then, with $Y=X^t$ and $a_i$, $b_i$, $c_i$ polynomials in $X$ (and $a_0$,
$b_0$, $c_0$ of degree less than $t$), this can be
detailed, with accumulation, in~\cref{eq:karasplit}:
\begin{equation}\label{eq:karasplit}
\fbox{\scalebox{.9}[0.9]{\renewcommand{\baselinestretch}{0.5}
\ensuremath{\begin{aligned}
A(Y)& = Ya_1 + a_0;\quad
B(Y) = Yb_1 + b_0;\quad
C(Y) = Y^3c_{11} + Y^2c_{10} + Yc_{01} + c_{00};\\
m_0& = a_0\cdot{b_0} = m_{01}Y+m_{00};\quad
m_1 = (a_0 + a_1)\cdot(b_0 + b_1)= m_{11}Y+m_{10};\quad
m_2 = a_1\cdot{b_1} = m_{21}Y+m_{20};\\
t_{00} &= c_{00}+m_{00};\quad
t_{01} = c_{01}+m_{01}+m_{10}-m_{00}-m_{20};\\
t_{10} &= c_{10}+m_{11}+m_{20}-m_{01}-m_{21};\quad
t_{11} = c_{11}+m_{21};\\
\text{\textbf{then}}&~C+AB \equiv{Y^3t_{11}+Y^2t_{10}+Yt_{01}+t_{00}}
\end{aligned}}}}
\end{equation}


Thus, in order to deal with the distributions of half of the products of~\cref{eq:karasplit},
each coefficient in $\mu$ in~\cref{eq:bilinkara} can be expanded into
$2{\times}2$ identity blocks, and the middle rows combined two by two,
as each tensor product actually spans
two sub-parts of the result; we obtain~\cref{eq:bilinkarasplit}:
%
\begin{equation}\label{eq:bilinkarasplit}\arraycolsep=2pt
\mu^{(2)}=
{\left[\begin{array}{cccccc}
1&\multicolumn{1}{c|}{0}&0&\multicolumn{1}{c|}{0}&0&0\\
0&\multicolumn{1}{c|}{1}&0&\multicolumn{1}{c|}{0}&0&0\\
\hline
0&0&0&0&0&0\\
0&0&0&0&0&0\\
\end{array}\right]}
+
{\left[\begin{array}{cccccc}
0&0&0&0&0&0\\
\hline
-1&\multicolumn{1}{c|}{0}&1&\multicolumn{1}{c|}{0}&-1&0\\
0&\multicolumn{1}{c|}{-1}&0&\multicolumn{1}{c|}{1}&0&-1\\
\hline
0&0&0&0&0&0
\end{array}\right]}
+
{\left[\begin{array}{cccccc}
0&0&0&0&0&0\\
0&0&0&0&0&0\\
\hline
0&\multicolumn{1}{c|}{0}&0&\multicolumn{1}{c|}{0}&1&0\\
0&\multicolumn{1}{c|}{0}&0&\multicolumn{1}{c|}{0}&0&1
\end{array}\right]}
=
{\left[\begin{array}{cc|cc|cc}
1&0&0&0&0&0\\
-1&1&1&0&-1&0\\
0&-1&0&1&1&-1\\
0&0&0&0&0&1
\end{array}\right]}
\end{equation}



Finally, \cref{eq:karasplit} then translates into an in-place algorithm
thanks to~\cref{alg:bilin,eq:bilinkara,eq:bilinkarasplit}.


The first point is that
products double the degree: this corresponds to a constraint that the
two blocks have to remain together when distributed.

In other words, this means that the $\mu^{(2)}$ matrix needs to be
considered two consecutive columns by two consecutive columns.
This is always possible if the two columns are of full rank $2$.
Indeed, consider a $2\times{2}$ invertible submatrix
$M=\begin{smatrix} v & w\\x & y\end{smatrix}$ of these two columns.
Then computing
$\begin{smatrix}c_i\\c_j\end{smatrix}\pe{}M\begin{smatrix}\rho_0\\\rho_1\end{smatrix}$
is equivalent to computing a $2\times{2}$ version of~\cref{eq:basemul}:
\begin{equation}\label{eq:twobytwo}
M\left(\left(M^{-1}\begin{bmatrix}
      c_i\\c_j\end{bmatrix}\right)\pe\begin{bmatrix}\rho_0\\\rho_1\end{bmatrix}\right)
\end{equation}
The other rows of these two columns can be dealt with as before by
pre- and post- multiplying/dividing by a constant and pre- and post-
adding/subtracting the adequate $c_i$ and $c_j$.
Now to apply a matrix $M=\begin{smatrix}a&b\\c&d\end{smatrix}$ to a
vector of results $\begin{smatrix}\vec{u}\\\vec{v}\end{smatrix}$,
it is sufficient that one of its coefficient is invertible.
W.l.o.g suppose that its upper left element, $a$, is invertible.
Then the in-place evaluation of~\cref{eq:twobytwomul} performs this
application, using the two (known in advance) constants $x=ca^{-1}$ and $y=d-ca^{-1}b$:
%
\begin{equation}\label{eq:twobytwomul}
\left.\begin{aligned}
\vec{u}&\fe{a}\\
\vec{u}&\pe{b\cdot\vec{v}}\\
\vec{v}&\fe{y}\\
\vec{v}&\pe{x\cdot\vec{u}}\\
\end{aligned}\right\rbrace
\quad\text{computes in-place}\quad
\begin{bmatrix}\vec{u}\\\vec{v}\end{bmatrix}
\gets\begin{bmatrix}a&b\\c&d\end{bmatrix}
\otimes
\begin{bmatrix}\vec{u}\\\vec{v}\end{bmatrix}
=\begin{bmatrix}a\vec{u}+b\vec{v}\\c\vec{u}+d\vec{v}\end{bmatrix},~\text{for}~x=ca^{-1}~\text{and}~y=d-xb
%
%
%
%
\end{equation}

\begin{remark}\label{rq:zerotopleft}
  In practice for $2\times{2}$ blocks, if $a$ is not
  invertible, permuting the rows is sufficient since $c$ has to be
  invertible for the matrix to be invertible: for
  $J=\begin{smatrix}0&1\\1&0\end{smatrix}$,
  if $\tilde{M}=\begin{smatrix}c&d\\0&b\end{smatrix}=J\cdot{M}$, then
  $M=J\cdot\tilde{M}$ and $M^{-1}=\tilde{M}^{-1}\cdot{J}$ so
  that~\cref{eq:twobytwo} just becomes
\(J\cdot\tilde{M}\left(\left(\tilde{M}^{-1}\left(J\cdot\begin{smatrix}
      c_i\\c_j\end{smatrix}\right)\right)\pe\begin{smatrix}\rho_0\\\rho_1\end{smatrix}\right)\)
\end{remark}

\input{doubleexpand}


For instance with $m_{00}+Ym_{01}=a_0b_0=\rho_0+Y\rho_1$,
consider the upper left $2\times{2}$ block of $\mu^{(2)}$
in~\cref{eq:bilinkarasplit}, that is
$M=\begin{smatrix} 1&0\\-1&1\end{smatrix}$, whose inverse is
$M^{-1}=\begin{smatrix} 1&0\\1&1\end{smatrix}$.
One has first to precompute
$M^{-1}\begin{smatrix}c_{00}\\c_{01}\end{smatrix}$, that is nothing on
$c_{00}$ and $c_{01}\pe{}c_{00}$ for the second coefficient.
Then, afterwards, the third row, for $c_{10}$, will just be $-m_{01}$:
for this just pre-add $c_{10}\pe{}c_{01}$, and post-subtract
$c_{10}\me{}c_{01}$ after the product actual computation.
This example is exactly lines \ref{lin:m0BEG} to \ref{lin:m0END}
of~\cref{alg:accinplmulkara} thereafter.
One could also consider instead the first and last rows, but in this
particular case without any advantage in term of computations.
To complete~\cref{eq:karasplit}, the computation of $m_2$ is dealt
with in the same manner, while that
of $m_1$ is direct in the results (and requiring pre and post
additions of its inputs). This gives then the whole
of~\cref{alg:accinplmulkara}.



The second point is to deal with unbalanced dimensions and degrees
for $Y=X^t$ and recursive calls. For this, first separate the largest
polynomial in two parts, so that two sub-products are performed: a
large balanced one, and, recursively, a smaller unbalanced one.
Then, for the balanced case, the idea is to ensure that three out of
four parts of the result, $t_{00}$, $t_{01}$ and $t_{10}$, have the
same size and that the last one $t_{11}$ is smaller. This ensures that
all accumulations can be performed in-place.
The obtained procedure is given in~\cref{alg:accinplmulkara}.

\begin{algorithm}[htbp]
\caption{In-place Karatsuba polynomial multiplication with
  accumulation}\label{alg:accinplmulkara}
\begin{algorithmic}[1]
\REQUIRE $A(X)$, $B(X)$, $C(X)$ polynomials of degrees $m$, $n$, $m+n$
with $m\geq{n}$.
\ENSURE $C\pe{AB}$
\IF{$n\leq\threshold$}\hfill\COMMENT{Constant-time if $\threshold\in\bigO{1}$}
\RETURN the quadratic in-place polynomial
multiplication. \hfill\COMMENT{\cref{alg:classicmul}}
\ELSIF{$m>n$}
\STATE Let $A(X)=A_0(X)+X^{n+1}A_1(X)$
\STATE $C_{0..2n}\pe{A_0B}$\hfill\COMMENT{Recursive call}
\IF{$m\geq{2n}$}
\STATE $C_{(n+1)..(n+m)}\pe{A_1B}$\hfill\COMMENT{Recursive call}
\ELSE
\STATE $C_{(n+1)..(n+m)}\pe{BA_1}$\hfill\COMMENT{Recursive call}
\ENDIF
\ELSE
\STATE Let $t=\lceil(2n+1)/4\rceil$; \hfill\COMMENT{$t-1\geq{2n-3t}$ and thus $t>n-t$}
\STATE Let $A=a_0+X^ta_1$; $B=b_0+X^tb_1$; $C=c_{00}+c_{01}X^t+c_{10}X^{2t}+c_{11}X^{3t}$;
\hfill\COMMENT{$d^\circ{c_{11}}=2n-3t$}
\Statex\COMMENT{Iteration for $m_0$}
\STATE\label{lin:m0BEG}$c_{01}\pe{c_{00}}$;
\STATE $c_{10}\pe{c_{01}}$;
\STATE $\begin{bmatrix} c_{00} \\ c_{01}\end{bmatrix} \pe a_0\cdot{b_0} $
\hfill\COMMENT{Recursive call}
\STATE $c_{10}\me{c_{01}}$;\hfill\COMMENT{this is $c_{10}-m_{01}$}
\STATE\label{lin:m0END}$c_{01}\me{c_{00}}$;\hfill\COMMENT{this is $c_{01}+m_{01}-m_{00}$}
\Statex\COMMENT{Iteration for $m_1$}
\STATE $a_{0}\pe{a_{1}}$;\hfill\COMMENT{$d^\circ{a_0}=t\geq{n-t}=d^\circ{a_1}$}
\STATE $b_{0}\pe{b_{1}}$;
\STATE $\begin{bmatrix} c_{01} \\ c_{10}\end{bmatrix} \pe a_0\cdot{b_0} $
\hfill\COMMENT{Recursive call}
\STATE $b_{0}\me{b_{1}}$;
\STATE $a_{0}\me{a_{1}}$;
\Statex\COMMENT{Iteration for $m_2$}
\STATE $c_{10}\pe{c_{11}}$;
\STATE $c_{01}\pe{c_{10}}$;
\STATE $\begin{bmatrix} c_{10} \\ c_{11}\end{bmatrix} \pe a_1\cdot{b_1} $
\hfill\COMMENT{Recursive call}
\STATE $c_{01}\me{c_{10}}$;\hfill\COMMENT{this is $c_{01}+m_{01}-m_{00}+m_{10}-m_{20}$}
\STATE $c_{10}\me{c_{11}}$;\hfill\COMMENT{this is $c_{10}-m_{01}+m_{11}+m_{20}-m_{21}$}
\ENDIF
\RETURN $C$.
\end{algorithmic}
\end{algorithm}

\begin{proposition}
\Cref{alg:accinplmulkara} is correct and requires
$\bigOdisplay{mn^{\log_2(3)-1}}$ operations.
\end{proposition}
\begin{proof}
With the above analysis, correctness comes from that
of~\cref{alg:doublebilin,alg:doubleexpand} applied
to~\cref{eq:bilinkara}.
When $m=n$, with $3$ recursive calls and \bigO{n} extra operations,
the algorithm thus requires overall $\bigO{n^{\log_2(3)}}$ operations.
Otherwise, it requires $\left\lfloor\frac{m}{n}\right\rfloor$ equal
degree calls, then a recursive call with $n$ and $m\bmod{n}$.
Now, let $u_1=m$ and $u_2=n$ and if the Euclidean algorithm on
them requires $k$ steps, let $u_i$ for $i=1..k$ denote the
successive residues within this Euclidean algorithm (and
$u_k\in\{0,1\}$). Let $\kappa = k-1$ if $u_k = 0$ and $\kappa = k$
otherwise.
With these notations, \cref{alg:accinplmulkara} requires
less than
$\bigO{\sum_{i=1}^{\kappa-1}\lfloor\frac{u_i}{u_{i+1}}\rfloor{}u_{i+1}^{\log_2(3)}}
\leq \bigO{\sum_{i=1}^{\kappa-1} u_iu_{i+1}^{\log_2(3)-1}}$
operations.
But, $u_{i+1}\leq{u_2}=n$ and we let $s_i=u_i+u_{i+1}$ so that
$u_i=s_i-u_{i+1}\leq{s_i}$.
Now, from~\cite[Corollary~2.6]{Grenet:2020:euclide}, we have that
$s_i\leq s_1(2/3)^{i-1}$.
Thus the number of operations is bounded by
$\bigO{\sum_{i=1}^{\kappa-1} s_i n^{\log_2(3)-1}}
\leq \bigO{n^{\log_2(3)-1}s_1(\frac{1}{1-(2/3)}-1)}
=\bigO{n^{\log_2(3)-1}(m+n)}=\bigO{mn^{\log_2(3)-1}}$.
\end{proof}

\Cref{alg:accinplmulkara} is compared with previous Karatsuba-like
algorithms for polynomial multiplications designed to reduce their
memory footprint in~\cref{tab:kara} (see also \cite[Table~2.2]{Giorgi:2019:hdr}).


\begin{table}[htbp]\centering
\caption{Reduced-memory algorithms for Karatsuba polynomial
  multiplication}\label{tab:kara}
\begin{tabular}{lccc}
\toprule
Alg. & Memory & inputs & accumulation \\
\midrule
\cite{Thome:2002:karatemp} & $n+5\log{n}$ & read-only & {\xno}\\
\cite{Roche:2009:spacetime,Roche:2011:waterloo} & $5\log{n}$ & read-only & {\xno}\\
\cite{Giorgi:2019:issac:reductions} & $\bigO{1}$ & read-only & {\xno}\\
\cref{alg:accinplmulkara} & $5\log n$ & mutable & {\xyes}\\
\bottomrule
\end{tabular}
\end{table}

For the complexity bound, all coefficients being $1$ or $-1$ the
resulting in-place algorithm can thus compute in fact the accumulation
$C\pe{AB}$ without constant multiplications.
Also, the de-duplication of each recursive output enables some natural
reuse, so in fact there is a cost of
$2(\#\alpha-t+\#\beta-t)=2(4-3+4-3)$ with $t=3$, and
$2(2(\#\mu-t)=4(5-3)=2(\#\mu^{(2)}-2t)$, for a total of at most $12$ block
additions and $3$ recursive accumulated calls.
The simple application of~\cref{eq:karasplit} would require $8$ additions.
Thus, without thresholds and for powers of two, the dominant term of
the overall cost only goes from $10n^{\log_2(3)}$, for the original
version, to $14n^{\log_2(3)}$, for the fully in-place version.

\subsection{Further bilinear polynomial multiplications}\label{ssec:toom}

We have shown that any bilinear algorithm can be transformed into an
in-place version. This approach thus also works for any Toom-$k$
algorithm using $2k-1$ interpolations points instead of the three
points of Karatsuba (Toom-$2$).

For instance Toom-$3$ uses interpolations at
$0,1,-1,2,\infty$. Therefore, $\alpha$ and $\beta$ are the Vandermonde matrices
of these points for the $3$ parts of the input polynomials and
$\mu$ is the inverse of the Vandermonde matrix of these points for the
$5$ parts of the result, as shown in~\cref{eq:toom3} thereafter.


\begin{equation}\label{eq:toom3}\begin{split}
\mu=\begin{bmatrix}
1 & 0 & 0 & 0 & 0 \\
1 & 1 & 1 & 1 & 1 \\
1 & -1 & 1 & -1 & 1 \\
1 & -2 & 4 & -8 & 16 \\
0 & 0 & 0 & 0 & 1
\end{bmatrix}^{-1}=
\begin{bmatrix}
 1 & 0 & 0 & 0 & 0 \\
 \tfrac12 & \tfrac13 & -1 & \tfrac16 & -2 \\
 -1 & \tfrac12 & \tfrac12 & 0 & -1 \\
-\tfrac12 & \tfrac16 & \tfrac12 & -\tfrac16 & 2 \\
 0 & 0 & 0 & 0 & 1
\end{bmatrix}
\\
\alpha=\beta=\begin{bmatrix}
0^0 & 0^1 & 0^2 \\
1^0 & 1^1 & 1^2 \\
(-1)^0 & (-1)^1 & (-1)^2 \\
(-2)^0 & (-2)^1 & (-2)^2 \\
0 & 0 & 1
\end{bmatrix}=\begin{bmatrix}
1 & 0 & 0 \\
1 & 1 & 1 \\
1 & -1 & 1 \\
1 & -2 & 4 \\
0 & 0 & 1
\end{bmatrix}
\end{split}
\end{equation}

With the same kind of duplication as in~\cref{eq:bilinkarasplit}, apart from the
recursive calls, the initially obtained operation count is
$2(11+11-2*5)+2(2(16-5))=68$ additions
and $2(2+2+2(11))=52$ scalar multiplications.
Following the optimization of \cite{Bodrato:2007:WAIFI:toomcook},
we see in $\alpha$ and $\beta$ that the evaluations at $1$ and $-1$
(second and third rows) share one addition. As they are successive in
our main loop, subtracting one at the end of the second iteration,
then followed by re-adding it at the third iteration can be optimized
out. This is $2$ less operations.
Together with shared coefficients in the rows of $\mu$,
some further optimizations of \cite{Bodrato:2007:WAIFI:toomcook} can
probably also be applied, where the same multiplicative
constants appear at successive places.


\subsection{Fast bilinear polynomial multiplication}\label{ssec:fft}


When sufficiently large roots of unity exist, polynomial
multiplications can be computed fast in our in-place model via a
discrete Fourier transform and its inverse, as shown
in~\cref{alg:fft2pow}, for power of two dimensions, and
in~\cref{alg:fftaccu}, for general dimensions.

Let $F\in\D[X]$ of degree $<n$ and $w$ be a principal $n$-th root of unity,
where $n = 2^p$. The discrete Fourier transform of $F$ at $w$ is defined as
$\DFT_n(F,w) = (F(w^0), F(w^1), \dotsc, F(w^{n-1}))$. The map is invertible, of
inverse $\DFT^{-1}_n(\cdot,w) = \frac{1}{n} \DFT_n(\cdot, w^{-1})$. It is known
that the DFT can be computed over-place, replacing the input by the
output~\cite{1965:CooleyTukey:MathComp:FFT}.  Actually, for over-place
algorithms and their extensions to the \emph{truncated Fourier transform}, it is
more natural to work with the \emph{bit-reversed DFT}.  To describe it, let
$[i]_p$ be the length-$p$ bit reversal of $i = \sum_{j=0}^{p-1} d_j 2^j$,
$d_j\in\{0,1\}$, defined by $[i]_p = \sum_{j=0}^{p-1} d_j 2^{p-j}$.  The
bit-reversed DFT is $\brDFT_n(F,w) = (F(w^{[0]_p}), F(w^{[1]_p}), \dotsc,
F(w^{[n-1]_p}))$. If $\pi:\{0,\dotsc,2^p-1\}\to\{0,\dotsc,2^p-1\}$ denotes the
bit-reversal permutation (that is $\pi(i) = [i]_p$), we have $\brDFT_n(\cdot,w)
= \pi\circ\DFT_n(\cdot,w)$. Its inverse is $\brDFT_n^{-1}(\cdot,w) = \frac{1}{n}
\DFT_n(\cdot,w^{-1})\circ\pi=\frac{1}{n}\DFT_n(\pi(\cdot),w^{-1})$ since $\pi$ is an involution.

\begin{remark}
    The Fast Fourier Transform (FFT) algorithm has two main variants:
    \emph{decimation in time} (DIT) and \emph{decimation in frequency} (DIF).
    Both algorithms can be performed over-place, replacing the input by the
    output. Without applying any permutation to the entries of the input/output
    vector, the over-place DIF FFT algorithm naturally computes
    $\brDFT_n(\cdot,w)$, while the over-place DIT FFT algorithm on $w^{-1}$
    computes $\brDFT_n^{-1}(\cdot, w)$.
\end{remark}


\newcommand{\IPpow}{\ensuremath{\text{IP2pow}}\xspace}
\begin{algorithm}[htbp]
\caption{\IPpow: In-place power of two multiplication with accumulation}\label{alg:fft2pow}
\begin{algorithmic}[1]
\REQUIRE $\vec{a}$, $\vec{b}$ and $\vec{c}$ of length $2^L$, $2^L$ and $2^{L+1}$, containing the coefficients of $A$, $B$, $C\in\D[X]$ respectively; $w\in\D$ primitive $2^{L+1}$-th root of unity.
\ENSURE $\vec{c}$ contains the coefficients of $C+A\cdot B$.
\STATE Let $n=2^L$;
\STATE $\vec{c}\gets \brDFT_{2n}(\vec{c},w)$;
    \hfill\COMMENT{over-place}
\STATE\label{lin:DFTab}$\vec{a}\gets \brDFT_n(\vec{a},w^2)$; $\vec{b}\gets \brDFT_n(\vec{b},w^2)$
    \hfill\COMMENT{over-place}
\lFOR{$i=0$ \To $n-1$}{$c_i\pe a_i\times b_i$}
\STATE $\vec{a}\gets \brDFT^{-1}_n(\vec{a},w^2)$; $\vec{b}\gets \brDFT^{-1}_n(\vec{b},w^2)$
    \hfill\COMMENT{Undo \ref{lin:DFTab} over-place}
\lFOR{$i=0$ \To $n-1$}{$a_i\fe w^i$; $b_i\fe w^i$}\label{lin:mulw}
    \hfill\COMMENT{see \cref{rq:geomprog}}
\STATE\label{lin:DFTab2}$\vec{a}\gets \brDFT_n(\vec a,w^2)$; $\vec{b}\gets
\brDFT_n(\vec b,w^2)$
    \hfill\COMMENT{over-place}
\lFOR{$i=0$ \To $n-1$}{$c_{i+n}\pe a_i\times b_i$}
\STATE $\vec{a}\gets \brDFT^{-1}_n(\vec{a},w^2)$; $\vec{b}\gets \brDFT^{-1}_n(\vec{b},w^2)$
    \hfill\COMMENT{Undo \ref{lin:DFTab2} over-place}
\lFOR{$i=0$ \To $n-1$}{$a_i\de w^i$; $b_i\de w^i$}
    \hfill\COMMENT{Undo \ref{lin:mulw}}

\RETURN $\vec{c}\gets \brDFT^{-1}_{2n}(\vec{c},w)$
\end{algorithmic}
\end{algorithm}

%
\begin{remark}\label{rq:geomprog}
To compute a geometric progression, in-place, as
in~\cref{lin:mulw} of~\cref{alg:fft2pow}, one can of course use an
extra variable (say initialized to $x=1$) storing and updating $w^i$
along the loop (using $a_i\fe{x}$; $x\fe{w}$; at each step). This
requires $\bigO{1}$ extra space. One can further reduce this extra
space to nothing, if needed, by using a non-zero input slot instead.
For instance, suppose indeed that $\exists{j},a_j\neq{0}$.
Then pre-divide every other coefficients by $a_j$ and use this
particular variable to update and store $a_jw^i$ (just computing
$a_i\fe{a_j}$; $a_j\fe{w}$; at each step).
Finally re-divide $a_j$ by $w$, $n-j+1$ times.
\end{remark}


\begin{theorem}\label{thm:fft2pow}
  Using an over-place $\brDFT$ algorithm with
  complexity bounded by $\bigO{n\log n}$, \cref{alg:fft2pow} is
  correct, in-place and has complexity bounded by $\bigO{n\log n}$.
\end{theorem}
\begin{proof}
    The algorithm follows the pattern of the standard FFT-based multiplication
    algorithm. Our goal is to compute $\brDFT_{2n}(A,w)$, $\brDFT_{2n}(B,w)$ and
    $\brDFT_{2n}(C,w)$,
    then obtain $\brDFT_{2n}(C+AB,w)$ and finally $C+AB$ using an inverse
    $\brDFT$. Computations on $C$ and then $C+AB$ are performed over-place using
    any standard over-place $\brDFT$ algorithm. The difficulty happens for $A$ and
    $B$ that are stored in length-$n$ arrays. We use the following property of
    the bit reversed order: for $k < n/2$, $[k]_p = 2[k]_{p-1}$, and for
    $k \ge n/2$, $[k]_p = 2[k-n/2]_{p-1}+1$. Therefore, the first $n$
    coefficients of $\brDFT_{2n}(A,w)$ are $(A(w^{2[0]_{p-1}}), \dotsc
    A(w^{2[\frac{n}{2}-1]_{p-1}})) = \brDFT_n(A,w^2)$. Similarly, the next $n$
    coefficients are $\brDFT_n(A(wX), w^2)$. Therefore, one can compute
    $\brDFT(A,w^2)$ and $\brDFT(B,w^2)$ in $\vec a$ and $\vec b$ respectively,
    and update the first $n$ entries of $\vec c$. Next we restore $\vec a$
    and $\vec b$ using $\brDFT^{-1}_n(\cdot,w^2)$. We compute $A(wX)$ and
    $B(wX)$ and again $\brDFT(A(wX),w^2)$ and $\brDFT(B(wX),w^2)$ to update the
    last $n$ entries of $\vec c$. Finally, we restore $\vec a$ and $\vec
    b$ and perform the inverse $\brDFT$ on $\vec c$.
    The cost is dominated by the ten $\brDFT^{\pm1}$ computations.
\end{proof}

The case where $n$ is not a power of two is loosely similar, using as a routine
a truncated Fourier transform (TFT) rather than a
DFT~\cite{2004:vanderHoeven:ISSAC:TFT}.  Let $w$ be an $N$-th root of unity for
some $N = 2^p$. The length-$n$ (bit-reversed) TFT of a polynomial $F\in\D[X]$,
$n < N$, is $\brTFT_n(F,w) = (F(w^{[0]_p}), \dotsc, F(w^{[n-1]}_p))$, that is
the $n$ first coefficients of $\brDFT_{N}(F,w)$.  As for the (bit-reversed) DFT,
the (bit-reversed) TFT and its inverse can be computed
over-place~\cite{Harvey:2010:issactft, Roche:2011:waterloo,
2013:Arnold:ISSAC:TFT, Coxon:2022:JSC:inplaceTFT}.

Given inputs $A$ and $B\in\D[X]$ of respective lengths $m$ and $n$ and an output
$C\in\D[X]$ of length $m+n-1$, we aim to replace $C$ by $C+AB$.  The idea is
first to replace $C$ by $\brTFT_{m+n-1}(C,w)$ where $w$ is a $2^p$-th principal
root of unity, $2^p \ge m+n-1$. That is, the vector $\vec{c}$ now contains as
its $i$-th entry the value $C(w^{[i]_p})$. The goal is then to replace $C(w^{[i]_p})$ by
$C(w^{[i]_p})+A(w^{[i]_p})B(w^{[i]_p})$, for $i=0$ to $m+n-2$. We cannot compute
the length $m+n-1$ $\brTFT$'s of $A$ and $B$ since this takes too much space.
Instead, we will progressively compute some parts of these $\brTFT$'s by means
of (standard) $\brDFT$'s, and update $\vec{c}$ accordingly. The starting point
of this strategy is the following lemma.

\begin{lemma}[\cite{Harvey:2010:issactft,Roche:2011:waterloo}]\label{lem:roche}
    Let $F\in\D[X]$, $\ell$ and $s$ be two integers such that $2^\ell$ divides
    $s$ and $w$ be a $2^p$-th principal root of unity. Define $F_{s,\ell}(X) =
    F(w^{[s]_p}X)\bmod X^{2^\ell-1}$. Then
    \[\brDFT_{2^\ell}(F_{s,\ell},w^{2^{p-\ell}}) =
            (F(w^{[s]_p}),\dotsc,F(w^{[s+2^\ell-1]_p})).\]
\end{lemma}

\begin{proof}
    Let $w_\ell = w^{2^{p-\ell}}$. This is a principal $2^\ell$-th root of unity
    since $w$ is a principal $2^p$-th root of unity. In particular, for any $i <
    2^\ell$, $F_{s,\ell}(w_\ell^{[i]_\ell}) = F(w^{[s]_p}w_\ell^{[i]_\ell})$.
    Now, $w_\ell^{[i]_\ell} = w^{[i]_p}$ since $2^{p-\ell}[i]_\ell = [i]_p$.
    Furthermore, $[s]_p+[i]_p = [s+i]_p$ since $i < 2^\ell$ and $2^\ell$ divides
    $s$.
    Finally, $F_{s,\ell}(w_\ell^{[i]_\ell}) = F(w^{[s+i]_p})$.
\end{proof}

\begin{corollary}\label{cor:partTFT}
    Let $F\in\D[X]$ stored in an array $\vec f$ of length $n$, $\ell$ and $k$ be
    two integers and $w$ be a $2^p$-th principal root of unity, with $2^\ell\le
    n$ and $(k+1)2^\ell \le 2^p$. There exist an algorithm
    $\partTFT_{k,\ell}(\vec f,w)$ that replaces the first $2^\ell$ entries of
    $\vec f$ by $F(w^{[k\cdot 2^\ell]_p})$, \dots,
    $F(w^{[(k+1)\cdot2^\ell-1]_p})$, and an inverse algorithm
    $\partTFT^{-1}_{k,\ell}$ that restores $\vec f$ to its initial state. Both
    algorithms use $O(1)$ extra space and have complexity
    $O(n+\ell\cdot2^\ell)$.
\end{corollary}

\begin{proof}
    Algorithm $\partTFT_{k,\ell}(\vec f, w)$ is the following:
    \begin{algorithmic}[1]
    \lFOR{$i=0$ \To $n-1$}{$f_i\fe w^{i[k\cdot2^\ell]_p}$}
    \lFOR{$i=2^\ell$ \To $n-1$}{$f_{i-2^\ell}\pe f_i$}
    \STATE $\vec{f}_{0..2^\ell-1} \gets \brDFT_{2^\ell}(\vec{f}_{0..2^\ell-1},w^{2^{p-\ell}})$
    \end{algorithmic}
    Its correctness is ensured by~\cref{lem:roche}. Its inverse algorithm
    $\partTFT^{-1}_{k,\ell}(\vec{f},w)$ does the converse:
    \begin{algorithmic}[1]
    \STATE $\vec{f}_{0..2^\ell-1} \gets\brDFT^{-1}_{2^\ell}(\vec{f}_{0..2^\ell-1},w^{2^{p-\ell}})$
    \lFOR{$i=2^\ell$ \To $n-1$}{$f_{i-2^\ell}\me f_i$}
    \lFOR{$i=0$ \To $n-1$}{$f_i\de w^{i[k\cdot2^\ell]_p}$}
    \end{algorithmic}
    In both algorithms, the call to $\brDFT^{\pm1}$ has cost $O(\ell\cdot
    2^\ell)$, and the two other steps have cost $O(n)$.
\end{proof}

To implement the previously sketched strategy, we assume that $m\le n$ for
simplicity. We let $\ell$, $t$ be such that $2^\ell\le m<2^{\ell+1}$ and
$2^{\ell+t}\le n<2^{\ell+t+1}$. Using $\partTFT^{\pm 1}$, we are able to compute
$(A(w^{[k\cdot 2^\ell]_p}), \dotsc, A(w^{[(k+1)\cdot 2^{\ell}-1]_p}))$ for any
$k$, and restore $A$ in its initial state afterwards.  Similarly, we can compute
$(B(w^{[k\cdot 2^{\ell+t}]_p}), \dotsc, B(w^{[(k+1)\cdot 2^{\ell+t}-1]_p}))$ and
restore $B$.

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\begin{algorithm}[htbp]
\caption{In-place fast polynomial multiplication with accumulation}\label{alg:fftaccu}
\begin{algorithmic}[1]
\REQUIRE $\vec{a}$, $\vec{b}$ and $\vec{c}$ of length $m$, $n$ and $m+n-1$,
$m\le n$, containing the coefficients of $A$, $B$, $C\in\D[X]$ respectively;
$w\in\D$ principal $2^p$-th root of unity with $2^{p-1} < m+n-1 < 2^p$
\ENSURE $\vec{c}$ contains the coefficients of $C+A\cdot B$.
\STATE $\vec{c}\gets \brTFT_{m+n-1}(\vec{c},w)$;
    \hfill\COMMENT{over-place}
\STATE $r\gets m+n-1$
\WHILE{$r \ge 0$}
\STATE\label{lin:bounds} $\ell\gets\lfloor\log_2(\min(r, m))\rfloor$; $t\gets\lfloor\log_2\min(r, n)\rfloor-\ell$; $k\gets m+n-1-r$
    \STATE\label{lin:TFTb} $\vec{b}\gets\partTFT_{k,\ell+t}(\vec{b},w)$
    \hfill\COMMENT{over-place: $B(w^{[k\cdot2^{\ell+t}]_p]}), \dotsc, B(w^{[(k+1)\cdot 2^{\ell+t}-1]_p})$}
        \FOR{$s=0$ \To $2^t-1$}
        \STATE\label{lin:TFTa} $\vec{a}\gets\partTFT_{s+k\cdot2^t,\ell}(\vec{a},w)$
        \hfill\COMMENT{over-place: $A(w^{[(k\cdot2^t+s)2^\ell]_p]}), \dotsc, A(w^{[(k\cdot 2^t+s+1)2^\ell-1]_p})$}
        \lFOR{$i=0$ \To $2^\ell-1$}{$c_{i+(k\cdot 2^t+s)2^\ell} \pe  a_i b_{i+s\cdot 2^\ell}$}
        \STATE $\vec{a}\gets\partTFT^{-1}_{s+k\cdot2^t,\ell}(\vec{a},w)$
        \hfill\COMMENT{Undo~\ref{lin:TFTa} over-place}
        \ENDFOR
    \STATE $\vec{b}\gets\partTFT^{-1}_{k,\ell+t}(\vec{b},w)$
    \hfill\COMMENT{Undo~\ref{lin:TFTb} over-place}
    \STATE $r\me 2^{\ell+t}$
\ENDWHILE
\RETURN $\vec{c}\gets \brTFT^{-1}_{m+n-1}(\vec{c},w)$
\end{algorithmic}
\end{algorithm}

\begin{theorem}
    Algorithm~\ref{alg:fftaccu} is correct and in-place.  If the algorithm
    $\brDFT$ used inside $\partTFT$ has complexity $O(n\log n)$, its running
    time is $O((m+n)\log(m+n))$.
\end{theorem}

\begin{proof}
    The fact that the algorithm is in-place comes
    from~\cref{cor:partTFT}.
    The only slight difficulty is to produce, fast
    and in-place, the relevant roots of unity. This is actually dealt with in
    the original over-place TFT algorithm~\cite{Harvey:2010:issactft} and can be
    done the same way here.

    To assess its correctness, first note that the values of
    line~\ref{lin:bounds} are computed so that $2^\ell\le r,m$ and
    $2^{\ell+t}\le r,n$. One iteration of the while loop update the entries
    $c_k$ to $c_{k+2^{\ell+t}-1}$ where $k = m+n-1-r$. To this end,
    we first compute $B(w^{[k\cdot2^{\ell+t}]_p]})$ to $B(w^{[(k+1)\cdot
    2^{\ell+t}-1]_p})$ in $\vec{b}$ using $\partTFT$. Then, since $\vec{a}$ may
    be too small to store $2^{\ell+t}$ values, we compute the corresponding
    evaluations of $A$ by groups of $2^\ell$, using a smaller $\partTFT$. After
    each computation in $\vec{a}$, we update the corresponding entries in
    $\vec{c}$ and restore $\vec{a}$. Finally, at the end of the iteration,
    entries $k$ to $k+2^{\ell+t}-1$ of $\vec{c}$ have been updated and $\vec{b}$
    can be restored. This proves the correctness of the algorithm.

    We now bound the complexity of the algorithm. Since $m\le n$, our aim is to
    bound it by $O(n\log n)$. Let us first bound the number of iterations of the
    while loop. We identify two phases, first iterations where $r \ge n$ and
    then iterations with $r < n$. During the first phase,
    $2^{\ell+t}>\frac{n}{2}$ entries of $\vec{c}$ are updated at each iteration,
    hence the first phase has at most $3$ iterations. In the second phase,
    $2^{\ell+t}>\frac{r}{2}$ entries are updated per iteration. The second phase
    starts with $r < n$ and each iteration decreases $r$ by half, hence the
    second phase has at most $\log n$ iterations.

    In one iteration, the costs come from calls to $\partTFT^{\pm 1}$.  One call
    to $\partTFT^{\pm 1}$ with an input of size $m$ and a transform of length
    $2^\ell$ has cost $O(m+\ell\cdot 2^\ell)$.  To compute the cost of one
    iteration, we separately count the contributions due to the linear term $m$
    and to the non-linear term $\ell\cdot 2^\ell$ in this complexity.  In one
    iteration, there are two calls to $\partTFT^{\pm 1}$ on $\vec{b}$ and
    $2^{t+1}$ calls to $\partTFT^{\pm 1}$ on $\vec{a}$. The contribution of the
    linear terms for these calls is thus $O(n+m\cdot 2^t) = O(n)$ since $m\cdot
    2^t < 2^{\ell+1+t} \le 2n$. Since there are $\log n$ iterations, the global
    cost due to these linear terms is $O(n\log n)$.

    The cost due to the non-linear terms in one iteration is $O((\ell+t)\cdot
    2^{\ell+t})$. In the first phase, $2^{\ell+t} \le n$ and these costs sum to
    $O(n\log n)$. In the second phase, $2^{\ell+t} \le r$. Let $r_i$ be the
    value of $r$ during the $i$-th iteration of the second phase: $r_1 < n$ and
    $r_{i+1} \le r_i/2$ hence $r_i < n/2^{i-1}$. The global cost of the DFT's of
    the second phase is then $O(\sum_i \frac{n}{2^i}\log\frac{n}{2^i}) = O(n\log
    n)$.
\end{proof}


\Cref{alg:fftaccu} is compared with previous FFT-based
algorithms for polynomial multiplications designed to reduce their
memory footprint in~\cref{tab:fft} (see also \cite[Table~2.2]{Giorgi:2019:hdr}).


\begin{table}[htbp]\centering
\caption{Reduced-memory algorithms for FFT polynomial
  multiplication}\label{tab:fft}
\begin{tabular}{lccc}
\toprule
Alg. & Memory & inputs & accumulation \\
\midrule
\cite{1965:CooleyTukey:MathComp:FFT} & $2n$ & read-only & {\xno}\\
\cite{Roche:2009:spacetime} & $\bigO{2^{\lceil\log_2 n\rceil}-n}$ & read-only & {\xno}\\
\cite{Harvey:2010:issactft} & \textcolor{darkgreen}{$\bigO{1}$} & read-only & {\xno}\\
\cref{alg:fftaccu} & \textcolor{darkgreen}{$\bigO{1}$} & mutable & {\xyes}\\
\bottomrule
\end{tabular}
\end{table}

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\section{Conclusion}
We here provide a generic technique mapping any bilinear formula into
an in-place algorithm.
This allows us for instance to provide the first accumulated in-place
Strassen-like matrix multiplication algorithm.
This also allows use to provide fast in-place accumulated polynomial
multiplications algorithms.

Many further accumulated algorithm can then be reduced to these fundamental
building blocks, see for instance Toeplitz, circulant, convolutions or
remaindering operations in~\cite{jgd:2023:inplrem}.

\bibliographystyle{plainurl}
\bibliography{refsbilin}

%
\newpage
%

\appendix
\section{In-place accumulated matrix-multiplication with 7 recursive
  calls and 22 additions}\label{app:inplsw}
\input{2x2}


\end{document}
