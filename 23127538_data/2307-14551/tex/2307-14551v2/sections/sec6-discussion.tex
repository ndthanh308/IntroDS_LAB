%!TEX root = YouTubePolarization.tex

% ====================================================
\section{Discussion}

We performed an algorithm audit of the YouTube recommendation system to test whether one could remove unwanted content from their feed. We paired our audit with a survey to understand whether users actually knew these buttons existed, used them, and believed them to be effective.

In our audit, we found that the stain phase produces a significant increase in stain in the homepage across all topics. We also saw that stain at P2 never reached more than half of recommendations in either the homepage or the videopage. These results confirm our suspicion that watching many videos from a given topic do not completely ``surround" the user topical recommendations like the term ``filter bubble" would suggest, and motivated our usage of the term "stain".

Continuing on results from the stain phase, we broke down stain at P2 into those from channels watched before and those from channels not watched before, finding that their prevalence varied based on topic. 

We saw that both types of stain were present, but to varying degrees depending on topic. On the one hand, \emph{Political Left} received the most stain from new channels for both the homepage and videopage, demonstrating that the platform had a notion of topical similarity by ``inferring" other channels from the political left that the user may like. On the other hand, the \emph{Alt-Right} received the least recommendations from new channels, for both the homepage and videopage. 
%This was true both in absolute terms, and relative to stain from channels watched before. 
This finding is interesting given YouTube's recent public promises to curb misinformation and conspiracy theories \cite{noauthor_continuing_2019}, especially "harmful" ones such as Q-Anon \cite{noauthor_managing_2020}, as well as a shift in company-wide attention towards stopping home-grown, right-wing extremism from spreading on its platform \cite{bergen_youtube_2022}. While the lack of recommendations from new \emph{Alt-Right} channels supplied to agents who watch that content could be evidence of YouTube operationalizing its promises, we cannot formally tell the difference between that and a general lack of \emph{Alt-Right} videos remaining on the platform today.

%Moving onto the scrub phase, we compared different scrubbing strategies and found that \emph{Not interested} was the most effective one on the homepage: It was the only strategy to produce significant decrease in stain across all topics, and using it resulted in the greatest average decrease in stain from P2 to P3 across topics (-97\%). 
\al{
%\marginnote{SPC-1}
Moving onto the scrub phase, we compared different scrubbing strategies and found that \emph{Not interested} was the most effective one on the homepage: It produced significant decrease in stain across all topics, and using it resulted in the greatest average decrease in stain from P2 to P3 across topics (-88\%). 
}
This strategy performed well in removing stain from both channels it had explicitly scrubbed as well as similar ones it didn't interact with. 
Thus, users who would like to remove recommendations from any channels belonging to an unwanted topic should use this strategy.

% [If I have time] Our results differ from those of Mozilla's study, who found no-channel to be the most effective. However, several  

%We then looked at splitting on-topic recommendations at P3 into ones from, and not from, channels that were scrubbed before. Here, we found that while recommendation-based strategies all produced P3 feeds with 0-4\% of on-topic recommendations from scrubbed channels, not-interested consistently produced less on-topic recommendations from channels not scrubbed before. 

In contrast to homepage findings, we found that the videopage never experienced significant effects from the scrub phase. 
%These results stand in contrast to Tomlein and colleagues' finding that agents could significantly reduce conspiratorial recommendations on the videopage by watching many videos debunking the conspiracy theory, for a variety of conspiracy theories. 
%However, here we note a pivotal difference between our experimental setup and theirs: Whereas agents in our study collected videopage recommendations of a video at P3 that was the same as that used in P2, their study used different videos. 
%Specifically, Tomlein et al.'s bots collected them from a video \emph{promoting} agents' assigned conspiracy theory at P2, and then collected them from a video \emph{debunking} it at P3. 
%Put together, our findings both suggest that videopage recommendations may be influenced much more by the video that is playing while they are collected, than any interactions with the system leading up to that collection (including watching any other videos). 
%The implication for users is that they should not expect any scrubbing strategies to save them from further recommendations of an unwanted topic if they are currently watching a video of that topic; Rather, they may want to stop watching content from that topic altogether. 
\al{
%\marginnote{R3-1}
At a cursory glance, it seems that our results stand in contrast to Tomlein and colleagues' finding  that agents could significantly reduce conspiratorial recommendations on the videopage by watching many videos debunking the conspiracy theory. However, upon further inspection it should be noted that in fact we have two separate experiments. Whereas agents in our study collected videopage recommendations from a video at P3 that was the same as that used in P2, their study used a video at P3 that was the semantic \emph{opposite} of that of P2. Specifically, Tomlein et al.'s bots collected them from a video \emph{promoting} agents' assigned conspiracy theory at P2, and then collected them from a video \emph{debunking} it at P3. 
}

Combining our findings with those of \citet{tomlein_audit_2021}  suggests that videopage recommendations may be influenced more by the video that is playing while they are collected, than any interactions with the system leading up to that collection.% (including watching any other videos). 
The implication for users is that they should not expect any scrubbing strategies to save them from further recommendations of an unwanted topic if they plan to keep watching a video of that topic; Rather, they may want to stop watching content from that topic altogether. 

Lastly, we wanted to know how users interacted with these platform features in their daily YouTube usage. We found that US adult YouTube users were most aware of the ``Dislike" button, yet more empirically effective strategies, such as ``Not interested", were less known. Those who knew the ``Not interested" button existed used it at a higher rate and perceived it to be more effective than those who knew about ``Dislike".

%[TODO after survey wrapup]. We confirm Mozilla's study that we should do more stuff to give people more agency by making them cognizant of certain features. 

Put together, our sock puppet and survey findings suggest that if YouTube wanted to allow users to more effectively remove unwanted recommendations, it should make its effective platform-features for doing so more broadly known to the general YouTube population. Doing so would not only benefit users' experience. It would also be in the best interest of the platform because allowing users to have more agency to tailor algorithmic decisions to their preferences can build and maintain their trust in the system \cite{ekstrand_letting_2015}, as well as increase overall satisfaction \cite{shin_how_2020}. 
One implication for platform designers is that they should make buttons such as ``Not interested" more widely known by increasing its discoverability on the website. To that end, Ricks and McCrosky provide a blueprint. In their experiment, they found that when their users were displayed ``Don't recommend this" buttons prominently and clearly on recommendation title cards, instead of being hidden behind a three-dots button or requiring the user to be led away from their current page, they were more than twice as likely to use it \cite{ricks_does_2022}. 
%Future work should look at other platforms, and test further strategies

\al{
%\marginnote{R1-12}
While this study demonstrates the benefits of YouTube's user controls, there still exist challenges to its uptake to remove unwanted recommendations. First, we note that these controls could be used to create digital media environments that run counter to democratic norms of diversity and breadth of perspectives. Thus, policy makers should pay attention to the potential for user agency to further limit their capacity for and consumption of cross-cutting content.

Second, as our survey highlights, knowledge of these buttons is still an issue. Much of the general YouTube-using public was not aware that the ``Not interested" button exists, for example. Even more troubling was that even those who experienced unwanted recommendations recently- thus having ample motivation to discover content removal tactics- still had not become aware of the button. 

Third, user interaction flows from these buttons may violate design principles in a way that limits users' ability to fully understand and anticipate the effects of different user controls \cite{smith_dark_2021}. For instance, they found that users were not fully aware of their effects on recommendations and account settings, and so they shied away from using them at all. Further compounding users' hesitation to take up these features is the perception that some of their effects are irreversible. 

Lastly, the actions that these user control buttons allow are responsive, rather than proactive. Users respond to a poor recommendation by eliminating it, rather than asking YouTube to tailor their recommendations before they see it. Thus, the worrisome effects of misinformation, toxicity, and offensiveness may have already taken its harmful course by the time the user decided to eliminate them. Therefore, these features cannot be seen as a substitute for diligent and thorough content moderation by the platform. 
}