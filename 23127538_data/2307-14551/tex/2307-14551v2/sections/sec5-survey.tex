\section{Survey Study}
\label{sec:survey}

\subsection{Survey Design}

In the sock puppet section of our work, we collected data from simulated users' interactions with YouTube to quantify the effects of platform-features that may help remove unwanted recommendations. In this section, we want to understand better the relationship between real users and these features. We ran a survey to determine this.

\input{tables/table-survey.tex}

\subsubsection{Survey overview.}

We first asked whether respondents had experienced getting unwanted recommendations before. Respondents were specifically asked whether they have experienced this scenario before: ``\emph{You are browsing YouTube, and notice videos recommended to you that you would rather not have recommended (because they are offensive to you, triggering, not safe for work, or some other reason)}''. The buttons we consider are ``Delete'' (delete a video from watch history), ``Dislike'', ``Not interested'', and ``Don't recommend channel''. We asked for respondents' experiences with our disinterest buttons, with respect to three constructs: 

\begin{itemize}
    \item \emph{Awareness}: Before taking this survey, were you aware this button existed?
    \item \emph{Usage}: Have you used this button to remove unwanted recommendations?
    \item \emph{Belief in efficacy}: Recall the times when you used this button to remove unwanted recommendations. How effective do you think it was? Please rate from 1 (not at all effective) to 5 (completely effective).
\end{itemize}

Only those who have experienced unwanted recommendations before and were aware of the buttons were asked to report on their real usage, while others were given a hypothetical question (``\textit{If} you had known this button existed, would you have used it?"). Furthermore, only those who were (1) experienced, (2) aware of button, and (3) have used the button were asked to report their belief in its efficacy in removing recommendations, while others were given a hypothetical question (``\textit{If} you had used this button for this scenario, how effective do you think would you find it?").

\subsubsection{Survey implementation.}
We recruited 300 participants from the survey recruitment platform Prolific, and ran the survey on the survey delivery platform Qualtrics. We selected participants that had used YouTube before, were adults (18+), and resided in the US. Participants were paid \$15 an hour. 
% The authors' institutional IRB team approved the study.
The University of Michigan Health Sciences and Behavioral Sciences Institutional Review Board has determined that this research is exempt from IRB oversight (Study ID: HUM00224551).

Surveys were pretested with colleagues. We emphasized honest rather than ``right'' answers so that respondents would not be tempted to ``please'' us by saying they knew about a button when in reality they did not \cite{paolacci_inside_2014}. We included screenshots of buttons so that they didn't have to know them by name. 
Since attention checks are important to maintaining experimental validity, we also implemented three of them throughout the survey to make sure the respondents were focusing on and comprehending the survey questions.
\al{
%\marginnote{R1-10}
At three separate points, we gave them a question whose format was identical to that of others and instructed them to select a specific choice. For example, \emph{"Please select `Dislike' in the choices below"}.}

We filtered responses by eliminating responses from those who failed any of the three attention checks. Respondents were paid regardless of whether they passed or failed attention checks. Then, we eliminated responses from anybody who answered ``not sure" to our questions.

\subsection{Survey Analysis Methods and Results}
In this subsection, we answer our questions posed in RQ3.

In total, our survey received 274 responses from those who passed all three attention checks. However, our respondent sample did not immediately generalize to a more general population. Thus we used post-stratification, a popular statistical method that adjusts estimates on non-probability samples \cite{salganik_bit_2019}, to generalize our results to the adult YouTube-using population in the US. 

To perform post-stratification, we divided our respondents into binary genders and age buckets (roughly 20 years apart), making a total of eight subgroups. 
We then made estimates of each subgroup's prevalence in the target population by combining Census data on age/gender subgroups \cite{duffin_population_2022} and PEW data on the percentage of each subgroup that use YouTube \cite{auxier_social_2021}.
%For each subgroup, we want the number of people of each subgroup that use YouTube, $n_g$. Dividing by each value the sum yields $r_g$, the ratio within the population that the subgroup $g$ uses YouTube at. The value $n_g$ is used for confidence interval calculation, while the value $r_g$ is used for point estimate.
Comparing our survey sample's distribution among subgroups with that of the target population revealed that our sample skewed young: Among usable responses, we routinely over-sampled the 18-45 subgroups and under-sampled 65+ ones. Fortunately, post-stratification corrects this bias.

We report answers for constructs of interest in Table \ref{table:survey}.
\emph{Awareness} percentages were calculated by aggregating answers from all respondents that passed our attention checks. For \emph{Usage}, we restricted our calculation to those who were both aware that the feature existed, and had experienced having unwanted recommendations. \emph{Belief in efficacy} was the most restricted because we only wanted ratings from those who would be well-informed of its effects from personal usage: Only those who experienced unwanted recommendations, were aware the button existed, and used that button to try to resolve the issue, were considered. These population restrictions are applied to both the table and our discussion of results.

Moving onto results, we find for \emph{Awareness} that survey respondents were most aware of the ``Dislike" button's existence (93.94\%). ``Don't recommend channel" was the least well-known (35.37\%). As for \emph{Usage}, they favored ``Not interested" (82.83\%) and ``No channel" (80.53\%) to remove unwanted recommendations when they experienced it. ``Dislike" was the least used button (37.75\%). Looking at \emph{Belief in efficacy}, users found ``Delete" (3.76), ``Not interested" (3.42), and ``No channel" (4.10) all more effective than the ``Dislike" button (2.52). 

These findings suggest that users do not use the ``Dislike" button to remove unwanted recommendations, despite most knowing about its existence. Respondents' intuition of this button match our empirical findings: We saw in Section \ref{results_stain} that the \emph{Dislike} and \emph{Dislike recommendation} strategies both reduced less stain compared to all other scrubbing strategies (\emph{Delete}, \emph{Not interested}, \emph{No channel}). Meanwhile, the button for our (empirically) most effective scrubbing strategy -- ``Not interested" -- was highly used by respondents who knew about it. However, awareness was a privilege: almost 44\% of survey respondents were unaware of its existence.

%the "\emph{Dislike} button, the least effective strategy empirically, is the most well-known feature (94\%). At the same time, it is the feature that is least likely to be used to remove unwanted recommendations (34\%). Those who used it found it to be the least effective (2.52 out of 5). 

%[TODO] One may be concerned that our awareness measure is misleading because it captures the current state of users, who may or may not have any motivation to become aware of scrubbing strategies, and if they were to become motivated then they could easily and quickly perform an online search and thus become aware of them. 