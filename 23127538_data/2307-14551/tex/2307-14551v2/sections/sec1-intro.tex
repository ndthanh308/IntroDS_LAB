\section{Introduction}
\label{sec:intro}

YouTube is the world's largest long-form video sharing platform, with users watching a billion hours of YouTube's content every day \cite{goodrow_you_2017}. In recent years, the YouTube recommendation algorithm has come under increased scrutiny for its role in promoting conspiracy theories such as anti-vaccination \cite{tomlein_audit_2021} and Alt-Right ideology such as white supremacy, Neo-Nazism content \cite{lewis_alternative_2018}. Studies have found that watching such content can lead to its continual and sometimes increased promotion \cite{hussein_measuring_2020,ribeiro_auditing_2020}. Besides societally-harmful information, YouTube has also been known to make unwanted recommendations to individuals, sometimes in the form of offensive, triggering, or outrageous videos \cite{noauthor_youtube_nodate,stocker_how_2020}. 

In the context of both individual and societal reasons for users to better tailor their personal content on YouTube, the platform provides several buttons such as ``Not interested'' and ``Don't recommend channel'', which allow users to express disinterest in a specific video or channel and alter their recommendation feeds accordingly \cite{burch_youtube_2019,cooper_how_2021}. These buttons exist among a variety of platform features, such as ``Disliking'' videos and deleting videos from one's watch history. All of those buttons may help users eliminate certain content from their feeds.

%The latter is of particular importance due to their messaging that espouses anti-Black and anti-immigrant beliefs that contribute to the further discrimination of already-marginalized communities \cite{noauthor_alt-right_nodate}. 
%In response, YouTube made explicit promises to tackle harmful anti-vaccine and conspiratorial content \cite{el-dardiry_giving_2019}. 

%Despite this, users may be dismayed to find a ``labyrinth'' of circular, deceptive, and unclear user controls that prevent them from taking full agency over their feed.

However, relatively little is known about the efficacy of these buttons in practice, nor about users' awareness of and confidence in them. To address these gaps, this work investigates how well simulated YouTube users (agents) can populate their recommendation feeds with content from a certain topic (the ``stain'' phase), as well as the ability for the recommendations of that topic to be removed by using a strategy to indicate disinterest (the ``scrub'' phase). 
\sw{
%\marginnote{R1-3}
We selected four topics: \emph{Alt-Right}, \emph{Antitheism}, \emph{Political Left}, and \emph{Political Right}. These topics are particularly interesting because, while plenty of literature exists on how one can get recommended more of these topics, little is known about removing them. Also, each topic is a realistic one that some users would no longer want to see. 
}
We examined six strategies (\emph{Watch neutral}, \emph{Dislike}, \emph{Delete}, \emph{Not interested}, \emph{No channel}, and \emph{Dislike recommended}), as well as a \emph{None} control strategy.
Lastly, we conducted a complementary survey study to understand real users' awareness of and experience with those scrubbing strategies on YouTube.


The main findings are as follows:

\begin{itemize}[leftmargin=*]
    \item 
    % We present results from a sock puppet audit experiment that tested how much of a simulated user's YouTube recommendations could be populated with videos of an assigned topic (what we call ``stain'') after watching many videos of that topic. 
    Watching a topic increased its presence on the homepage, though the stain never covers more than half of the recommendations. Watching a topic had less effect on the recommendations shown on videopages.
    \item 
    % We present results from the same experiment, which subsequently tested repeatedly pressing an assigned button to try to remove the stain. 
    For scrubbing a topic from the homepage, the most effective action was clicking the ``Not interested'' button on a recommended video. In contrast, none of the scrubbing significantly reduced the number of recommendations of videos on the topic shown on videopages.
    \item 
% We present results from a survey study that estimated theproportion of adult US YouTube users who were aware of and used each button we tested, as well as the degree to which the participants found them to be effective. Here, we found that many participants 
    Nearly half of survey respondents were not aware of the most effective feature (pressing ``Not interested''). Those who were aware of it used it often, and perceived it to be less effective than the ``Don't recommend channel'' button, contrary to our findings from the audit study.
    % Comparing to other features, those who were aware of this ``Not interested'' button used it at the highest rate, but they only found it to be modestly effective, poorer than the ``Don't recommend channel''.
\end{itemize}

%TODO: Something about accountability

%We additionally find that such recommendations contain both those from channels previously watched and those not previously watched, although the rate of each depends on the topic.

%We take a sock puppet algorithm auditing approach to address the first, mimicking users with software scripts to automatically interact with and gather data from Youtube in a realistic but systematic way. Pretending to be users interested in the Alt-Right, Antitheism, the Political Left, and the Political Right, these ``sock puppets'' first populate their recommendations with such content (the ``staining phase'') before taking on different platform features to remove said content (the ``scrubbing phase''). 

%After we confirm the ability for our sock puppets to generate significant recommendations of certain topics, we then report on the ability for sock puppets to ``scrub'' content of certain topics that they've built up in their feed. This concept is related to ``bursting'' the filter bubble in other studies (Tomlein). While previous studies have focused on watching topically different content as a solution and focus on balance rather than removal, it remains to be seen what strategies can be used to completely remove content from one's feed. Platform-provided features may both take less time and be more effective at removing recommendations, and be easier for a user who is immediately confronted with unwanted recommendations to act on it. Also, while one other study has investigated these features' relative efficacy, it cannot make any claims about the absolute percentage prevalence of content after using these features. 

%Along the way of our sock puppet audit, we pay particular attention to the special case topic of the Alt-Right, a right-wing, conspiratorial, extreme group in the US that has existed prominently on YouTube until recent promises for change \cite{noauthor_continuing_2019}. We are interested in its prevalence and recommendations on the homepage and videopage in response to these promises to curb right-wing extremism.

%We also present a novel survey to understand the degree to which users are aware of, use, and believe in the efficacy of these content removal strategies. While a recent study has elicited the breadth of strategies users can take and their qualitative understandings of how they work \cite{ricks_does_2022}, they cannot make claims to the general population. To the best of our knowledge, quantitative estimates to a generalized population are still unknown. Such information is valuable because it holds YouTube accountable for making its effective content removal strategies well-known, especially in the context of their own commitments to transparency and user agency \cite{el-dardiry_giving_2019}


%In this study we examine the ability for users to populate their recommendations with certain content, and then use various platform features to get rid of them. Some of these features are explicitly designed to tailor one's recommendations. Others were designed without this express purpose, but may still be effective nevertheless. 
%Overall, we seek to contribute knowledge about the role of platform-provided features in aiding personalization algorithms to better align with individual ideals.