\section{Research Questions}
\label{sec:research_questions}

%Our review of literature reveals gaps related to our knowledge of the effects of watching content of a given topic on recommendations of that topic, as well as the effects of platform-provided features that could remove unwanted YouTube recommendations. Lastly, 
%it sheds light on the need to perform experiments that better control for user actions during the experimental phase, 
%we must better understand how well users know about different features. We form our research questions to address these gaps. 

Previous studies of ``filter bubbles'' on YouTube recommendations see that those of a given topic can increase as a result of watching many videos of that topic, but we do not know whether these recommendations are from channels that the user has watched before, or whether they are new channels that YouTube finds similar. Such a breakdown would add to the knowledge of YouTube's role in promoting unwanted content by quantifying how much YouTube is ``inferring'' this content or simply suggesting content from channels it knows the user has seen before. Also, confirming the general result of increasing recommendations of a given topic would allow us to carry the next phase of our study, which attempts to remove them. 

Thus, we first address the question,
\al{
%\marginnote{R1-8}
\textbf{how responsive are YouTube recommendations to watching many videos of the same topic?}}
(RQ1) In particular, do they recommend more videos of the same topic, and if so are they from channels that users watched up to that point or are they new ones? Are they different for different topics? We study four topics whose prevalence on YouTube has been previously studied: \emph{Alt-Right}, \emph{Antitheism}, \emph{Political Left}, and \emph{Political Right} (motivated and described in \Cref{topics}). 
%, because previous studies have investigated their prevalence on the platform. We focus on the homepage and videopage because they have been found to experience personalization effects. 
%These questions put our study in conversation with previous ``filter bubble'' studies, and provides a baseline set of recommendations for our ``scrub phase'' to remove. 

We are also interested in the effects of platform features in removing unwanted recommendations. While a previous study investigated their usage ``in the wild'', the effects of each feature that is uncontaminated by usage of other ones, on topics that are well-defined, is still unknown. Such questions are worth answering because YouTube users in general could benefit from knowing what the most effective strategies for removing unwanted content are, specifically each one's effect on specific topics that they may dislike. 

Thus, we ask,
\al{
%\marginnote{R1-8}
\textbf{how responsive are YouTube recommendations to repeatedly performing a variety of strategies to try to remove unwanted videos of a topic?}}
(RQ2) Are they different between videopage and homepage? How much content is removed from similar channels that are not explicitly interacted with? Do they vary topic to topic? We identified six of these strategies, such as pressing the ``Not interested'' button, and listed them in \Cref{strategies}. 
%In addition to overall percentages of recommendations, we further split them into ones from channels with a video that the simulated user has scrubbed before, and ones from channels whose videos it has never scrubbed before. Doing so allows us to categorize the performance of scrubbing strategies along metrics relevant to the user experience (do users want only specific channels removed, or those as well as similar ones?).

Finally, it is unknown how many YouTube users are aware of each platform feature, how many utilize them, and how effective they find them to be. This information is important because effective strategies may be moot if users do not know their existence, and because users should be both using effective strategies and finding them to be effective.

Therefore, we lastly ask,
\al{
%\marginnote{R1-8} 
\textbf{what are real users' experiences with the platform features that we test in RQ2?}} (RQ3) 
With respect to each platform feature, we designed a survey study to ask how many participants are aware of it, what percentage use it to remove unwanted recommendations (given they are aware), and how effective participants find it to be (given they are aware and have used the feature to try to amend the situation).