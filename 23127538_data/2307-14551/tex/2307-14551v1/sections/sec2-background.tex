\section{Background and Related Work}
\label{sec:background}

% ====================================================
\subsection{Locations of Relevant YouTube Features}

There are three main YouTube pages that are relevant to our study: the homepage, videopage, and watch history page. The \emph{homepage} is the landing page for users upon entering the platform.\footnote{https://www.youtube.com} It presents recommendations in a grid format. If the user is logged in, they will be personalized to their account. Each recommendation contains a dropdown menu, where users are presented with two relevant buttons: the ``Not interested'' and ``Don't recommend channel'' buttons. These allow the users to (ostensibly) indicate disinterest with respect to specific recommendations.

The \emph{videopage} is the page where users see while watching a video. Recommendations are given in a right-hand sidebar. The feature on this page that is relevant to our study is the ``Dislike'' button, indicated by a thumbs-down symbol.

Finally, the \emph{watch history page}\footnote{https://www.youtube.com/feed/history} (or \emph{watch history} for short) is the page that displays a log of the user's previously-watched videos. This page is only available to users who are logged in. What relevant to our study is the option for users to delete specific videos from their watch history. Specifically, the ``Delete'' button is ``X'' located on the upper-right hand corner of each video in the log. 

%We finally note that these pages and features are accurately describe as of data collection (August 2022) and the writing of the paper (January 2023). As the platform often undergoes changes to its user experience and interface design, they may be outdated. 

\subsection{Sock Puppet Algorithm Audits}

Our study takes a sock puppet algorithm audit approach. Several prior work quantifying the YouTube recommender system's role in promoting and removing unwanted content also uses the algorithm auditing approach. So we begin with a review of this method. 
%A well-establish method for quantifying the prevalence, growth, and mitigation of these problematic recommendations is the sock puppet algorithm audit. 

An algorithm audit ``is a method of repeatedly and systematically querying an algorithm with inputs and observing the corresponding outputs in order to draw inferences about its opaque inner workings'' \cite{metaxa_auditing_2021}. Algorithm audit is a tool for researchers to investigate the effects of algorithms whose code and data are shielded from the public.
%under the guise of protecting company intellectual property or preventing fraud and ``gaming'' the system.

One type of algorithm audits is the sock puppet approach \cite{tomlein_audit_2021,haroon_youtube_2022}. Sock puppet audits use code scripts to create simulated users. These fake users -- also called ``agents'' -- interact with the platform or algorithm of interest as if they were the real users. In the meantime, researchers record and compare the recommendations that the agents receive. 
%(on YouTube, for instance, this would involve watching videos or clicking on recommendations). 
% the experimental control that researchers have over their agents' actions allows them to compare the recommendations that they receive. 

\subsection{Recommender System's Role in Promoting Problematic and Unwanted Content}

% Intro sentence
YouTube is one of the most popular video sharing platforms. It allows users across the globe to disseminate information almost instantaneously on topics ranging from fashion to history to politics. In recent years, it has received increasing public scrutiny from journalists and academics alike in assessing its recommendations of problematic content. 

The center of the platform's content dissemination is the recommendation engine, which plays an important role in helping users decide what to watch \cite{solsman_ever_2018,wu2019estimating}. Among a vast, ever-growing pool of videos on the platform, users are suggested what to watch next based on their previous interactions with YouTube. The recommendation engine also incorporates the platform's interest of maintaining user engagement in order to boost advertising revenue \cite{zhao_recommending_2019}.

YouTube's recommendation engine has been theorized to promote problematic recommendations, which can broadly be split into two categories. The first is its propensity to suggest content that violates political, societal, and anti-democratic ideals such as extermism and conspiracy theories \cite{tufekci_opinion_2018,lewis_alternative_2018} as well as political filter bubbles and radicalization \cite{pariser_filter_2011,tufekci_opinion_2018}. The second are those conflicting with individual preferences. Many users have found recommendations on video sharing platforms to be personally offensive, triggering, violent, and outrageous \cite{noauthor_youtube_nodate}, as well as conflicting with their sense of own identity \cite{karizat_algorithmic_2021}, even if the video is completely legal and enjoyable for others \cite{stocker_how_2020}.
%Opaque recommendations may also result in a perceived mismatch between one's own identity and the identity of the user that is portrayed by the recommender system \cite{karizat_algorithmic_2021}.
%Some content, while completely legal and harmless by itself, may be ``contextually-inappropriate'', or harmful with respect to the context in which it is suggested or the specific audience watching it (e.g. children) \cite{stocker_how_2020}. 

Several YouTube algorithm audits have investigated the role of recommender systems in promoting these kinds of content due to personalization, largely focusing on political ideologies and conspiracy theories. While previous studies often refer to this phenomenon as ``filter bubbles'', we instead choose to use the term ``stain''. This is because previous studies (as well as ours) find that topical recommendations rarely take up more than half of one's feed and never reach 100\% after watching many videos of that topic, and we would like to avoid the misleading interpretation of the term ``bubble'' as being completely surrounded by (i.e., having 100\% of) topical recommendations. 

Regardless of the term, studies have agreed that continued consumption of videos of a certain topic will lead to further (and sometimes increased) recommendation of that topic, on both the homepage and the videopage \cite{hussein_measuring_2020,papadamou_it_2021,tomlein_audit_2021,haroon_youtube_2022}. Recommendations in search results, on the other hand, do not experience such personalization effects \cite{tomlein_audit_2021}. Despite these findings, it is still unknown whether the stain is made completely of videos from the channels watched before, or whether YouTube introduces new channels of the topic that have not yet been watched by the user. Such information would demonstrate how much YouTube is recommending content beyond what is obviously related (i.e., that from the same channel), adding clarity to the current debate of the algorithm's role in information personalization.

Researchers have also studied other forms of problematic information personalization such as radicalization, or the process of being recommended content that is progressively more extreme \cite{ribeiro_auditing_2020,hosseinmardi_examining_2021,chen_exposure_2022}. By contrast, our study focuses on the construct of stain on YouTube. 
% \citet{bandy_problematic_2021} have also explored these behaviors in a variety of digital spaces besides YouTube. 

%A closely-related concept is ``radicalization'', which describes the process of being recommended content that is progressively more extreme. Such studies have yet to find evidence that the algorithm ``pulls'' users from mainstream media to right-wing extremism \cite{ribeiro_auditing_2020,hosseinmardi_examining_2021,chen_exposure_2022}, though it has been found to pull users further and further away from trustworthy media sources \cite{spinelli_how_2020}. While an important construct, we do not directly contribute to the literature on radicalization.

%Empirical studies of problematic information personalization is not limited to YouTube. Robertson and colleagues, for example, find little evidence for personalized political filter bubbles on Google search, and instead find a general preference for right-leaning content on higher-ranked results \cite{robertson_auditing_2018}. Bakshy and colleagues compare the propensities for Facebook users to share political news of different ideologies with that of the content ranking system behind the News Feed; They find that a user's Friend network had a greater influence on one's political filter bubble than any informational distortion brought about by the News Feed \cite{bakshy_exposure_2015}. 

% NEW
\subsection{User Controls to Remove Unwanted Content}

Combined calls from academics and journalists alike to mitigate the YouTube recommender system's role in problematic content consumption have contributed to recent platform changes, which include features that purport to grant users more control in tailoring their own recommendations \cite{burch_youtube_2019,cooper_how_2021}. Other social media platforms such as Tik Tok and Instagram have also released and experimented with user controls to tailor their recommendations \cite{ariano_how_2021,noauthor_testing_2022}.

Such features may improve the user satisfaction in online spaces that are mediated by recommender systems. This is especially true on YouTube because much of content viewership comes directly from users clicking on its recommendations \cite{solsman_ever_2018}. However, compared to what is known about the prevalence of unwanted content on YouTube, relatively little is known about the efficacy of features that help to remove it. We review this literature here.

%Features that help users tailor the recommendations are examples of ``contestability'', or the ability to challenge or contest machine decisions \cite{mulligan_shaping_2019}. Such features can help users build trust in the algorithm by giving them a voice in altering decision-making processes that will ultimately affect them. While other social media platforms such as Tik Tok and Instagram have released and experimented with contestability features in recent years \cite{ariano_how_2021,noauthor_testing_2022}, our study focuses exclusively on YouTube.

\subsubsection{Algorithm audits of how to reduce recommendations.}
Two experiments used an intuitive strategy to try to reduce content of a given topic: watching videos of a \emph{different} topic. For example. \citet{tomlein_audit_2021}'s sock puppet audit found that agents were recommended less conspiratorial content after they watched many videos debunking conspiracy theories. \citet{haroon_youtube_2022}'s sock puppet audit found that a politically-biased recommendation feed could be ``debiased'' -- or achieve similar amounts of left and right-leaning videos -- by watching a diet of videos
%(procured by a reinforcement-learning algorithm) 
that heavily featured the ideology that was originally less prevalent. 

These studies suggest that it is possible to remove some unwanted content from one's feed. However, the degree to which it can be done varies and is never 100\%.
%, because it was only shown to diversify and balance one's recommendations among multiple topics rather than completely eliminate them. 
Further, we find it necessary to investigate platform-provided buttons 
in addition to video-watching for a variety of reasons. First, many are designed for the explicit purpose of removing unwanted content (e.g., the ``Not Interested'' button). Second, they may be much faster to perform: studies suggest a minimum of 10 minutes of watching is required to register significant changes to recommendations \cite{papadamou_it_2021}. Meanwhile, pressing buttons can take just seconds. 
Lastly, these buttons may avoid the side effects of infusing too much content from another topic to replace the unwanted topic.

\citet{ricks_does_2022} provide the first quantitative study of such YouTube's platform-provided features, expanding the breadth of recommendation-reduction strategies beyond watching videos of a different topic. 
The researchers supplied YouTube users with a browser extension with a custom ``Stop Recommending'' button displayed on each video recommendation. Then, users were randomly assigned to have their custom button press a native platform button in the background on their behalf. Their results show that clicking on the native ``Don't recommend this channel'' button on videos produced subsequent recommendations that were least similar to them.

%Their results show that, on average, users whose Mozilla-provided button triggered the native '' button received subsequent recommendations to a video they clicked ``Stop Recommending'' on that were 43\% less similar to those received by users in the control group; Reductions caused by all other buttons were less than that. 

\citet{ricks_does_2022}'s study benefits from a large sample. Their field experiment design also presents distinct advantages, particularly an external validity that a sock puppet audit cannot achieve. At the same time, we still find it valuable to perform a sock puppet experiment with a more controlled environment for two reasons. 
First, because users could press ``Stop Recommending'' on any recommendation from any topic, the study was not able to identify the effects of the buttons for well-defined topics.  
%and their similarity measure may not have captured what it was that users wanted more or less of, which could be defined in a simulation study by defining topics with codebooks. 
Second, there are possible confounds from uncontrolled user behavior, such as users watching similar videos to the ones that they pressed ``Stop Recommending'', or cross-contamination between conditions where users clicked on YouTube-provided buttons in addition to Mozilla-provided buttons.

\subsubsection{Users' relationship with user controls.}

A few studies also used qualitative methods to understand users' experiences and perceptions of different strategies to remove unwanted content from their personal feeds.

\citet{ricks_does_2022} survey and interview a subset of their participants from the quantitative arm of their study. They find that users take a variety of strategies to combat unwanted recommendations, generally find platform-provided features to be ineffective, and notice that effective results need to take sustained time and effort.

While these surveys and interviews solicit the breadth of strategies that users have to combat unwanted recommendations,
%(some of which exist outside the platform itself, such as using a private browser), 
the degree to which general YouTube users are aware of each platform-provided feature is still unknown. It is also unknown whether they use these features, even if they are aware of. Such data is important because an effective feature may be moot if not many people know about their existence. 

\citet{smith_dark_2021} also analyzed YouTube user controls to alter recommendation for their adherence to user experience principles. They found that the actions performed by such buttons were reactive (i.e., only useful \emph{after} a user received an unwanted recommendation) and that the feedback provided to the user after clicking them was often unclear and vague. They also found that navigating to some features was difficult, which could limit users' ability to alter their recommendations.

% The desire for more agency in determining one's own content recommendations is not limited to YouTube. For instance, \citet{karizat_algorithmic_2021} investigated how Tik Tok users respond to recommendations that portray an identity that does not fit their own, and found that users engaged in ``algorithmic resistance'' \cite{velkova_algorithmic_2021} by purposely altering an algorithm's outcomes using the platform's buttons and controls. This included liking, disliking, watching or subscribing to certain content strategically.

