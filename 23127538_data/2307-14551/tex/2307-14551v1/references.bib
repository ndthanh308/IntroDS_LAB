@misc{noauthor_alt_2019,
author = {ADL},
title = {From {Alt} {Right} to {Alt} {Lite}: {Naming} the {Hate}},
howpublished = {\url{https://www.adl.org/resources/backgrounders/from-alt-right-to-alt-lite-naming-the-hate}},
journal = {Anti-Defamation League},
year = {2019},
}

@misc{el-dardiry_giving_2019,
title = {Giving you more control over your {Homepage} and {Up} {Next} videos},
howpublished = {\url{https://blog.youtube/news-and-events/giving-you-more-control-over-homepage/}},
journal = {blog.youtube},
author = {El-Dardiry, Essam},
year = {2019},
}

@misc{hoffman_how_2018,
title = {How to {Delete} {Your} {YouTube} {Watch} {History} (and {Search} {History})},
howpublished = {\url{https://www.howtogeek.com/360178/how-to-delete-your-youtube-watch-history-and-search-history/}},
journal = {How-To Geek},
author = {Hoffman, Chris},
year = {2018},
}

@misc{marantz_alt-right_2017,
title = {The {Alt}-{Right} {Branding} {War} {Has} {Torn} the {Movement} in {Two}},
howpublished = {\url{https://www.newyorker.com/news/news-desk/the-alt-right-branding-war-has-torn-the-movement-in-two}},
journal = {The New Yorker},
author = {Marantz, Andrew},
year = {2017},
}

@misc{noauthor_youtube_nodate,
title = {{YouTube} {Regrets}},
author = {Mozilla},
howpublished = {\url{https://foundation.mozilla.org/en/youtube/regrets/}},
journal = {Mozilla Foundation},
year={2019}
}

@misc{weiss_opinion_2018,
title = {Opinion {\textbar} {Meet} the {Renegades} of the {Intellectual} {Dark} {Web}},
howpublished = {\url{https://www.nytimes.com/2018/05/08/opinion/intellectual-dark-web.html}},
journal = {The New York Times},
author = {Weiss, Bari and Winter, Damon},
year = {2018},
}

@misc{bergen_youtube_2022,
title = {{YouTube} {Went} to {War} {Against} {Terrorists}, {Just} {Not} {White} {Nationalists}},
howpublished = {\url{https://www.bloomberg.com/news/features/2022-08-30/youtube-s-video-purge-left-out-right-wing-extremism}},
journal = {Bloomberg.com},
author = {Bergen, Mark},
year = {2022},
}

@misc{burch_youtube_2019,
title = {{YouTube} {Rolls} {Out} {New} ‘{Don}’t {Recommend}’ {Feature}},
howpublished = {\url{https://www.thewrap.com/youtube-rolls-out-new-dont-recommend-feature/}},
author = {Burch, Sean},
year = {2019},
}

@misc{tufekci_opinion_2018,
title = {Opinion {\textbar} {YouTube}, the {Great} {Radicalizer}},
howpublished = {\url{https://www.nytimes.com/2018/03/10/opinion/sunday/youtube-politics-radical.html}},
journal = {The New York Times},
author = {Tufekci, Zeynep},
year = {2018},
}

@misc{cooper_how_2021,
title = {How the {YouTube} {Algorithm} {Works} in 2023: {The} {Complete} {Guide}},
howpublished = {\url{https://blog.hootsuite.com/how-the-youtube-algorithm-works/}},
journal = {Social Media Marketing \& Management Dashboard},
author = {Cooper, Paige},
year = {2021},
}

@misc{ariano_how_2021,
title = {How to `dislike' a {TikTok} video to make the app better understand what kind of content you want to view},
howpublished = {\url{https://www.businessinsider.com/guides/tech/how-to-dislike-a-tiktok}},
journal = {Business Insider},
author = {Ariano, Ryan},
year = {2021},
}

@misc{noauthor_continuing_2019,
title = {Continuing our work to improve recommendations on {YouTube}},
author = {YouTube},
howpublished = {\url{https://blog.youtube/news-and-events/continuing-our-work-to-improve/}},
journal = {blog.youtube},
year = {2019},
}

@misc{noauthor_managing_2020,
title = {Managing harmful conspiracy theories on {YouTube}},
author = {YouTube},
howpublished = {\url{https://blog.youtube/news-and-events/harmful-conspiracy-theories-youtube/}},
journal = {blog.youtube},
year = {2020},
}

@misc{chen_exposure_2022,
title = {Exposure to {Alternative} \& {Extremist} {Content} on {YouTube}},
howpublished = {\url{https://www.adl.org/resources/reports/exposure-to-alternative-extremist-content-on-youtube}},
author = {Chen, Annie Y. and Nyhan, Brendan and Reifler, Jason and Robertson, Ronald E. and Wilson, Christo},
year = {2022},
}

@misc{noauthor_testing_2022,
title = {Testing {More} {Ways} to {Control} {What} {You} {See} on {Instagram}},
author = {Meta},
howpublished = {\url{https://about.fb.com/news/2022/08/testing-ways-to-control-what-you-see-on-instagram/}},
journal = {Meta},
year = {2022},
}

@article{shin_how_2020,
title = {How do users interact with algorithm recommender systems? {The} interaction of users, algorithms, and performance},
journal = {Computers in Human Behavior},
author = {Shin, Donghee},
year = {2020},
}

@inproceedings{ekstrand_letting_2015,
title = {Letting {Users} {Choose} {Recommender} {Algorithms}: {An} {Experimental} {Study}},
booktitle = {ACM RecSys},
author = {Ekstrand, Michael D. and Kluver, Daniel and Harper, F. Maxwell and Konstan, Joseph A.},
year = {2015},
}

@inproceedings{vaccaro_contestability_2019,
title = {Contestability in {Algorithmic} {Systems}},
booktitle = {ACM CSCW},
author = {Vaccaro, Kristen and Karahalios, Karrie and Mulligan, Deirdre K. and Kluttz, Daniel and Hirsch, Tad},
year = {2019},
}

@inproceedings{hirsch_designing_2017,
title = {Designing {Contestability}: {Interaction} {Design}, {Machine} {Learning}, and {Mental} {Health}},
booktitle = {DIS},
author = {Hirsch, Tad and Merced, Kritzia and Narayanan, Shrikanth and Imel, Zac E. and Atkins, David C.},
year = {2017},
}

@techreport{mulligan_shaping_2019,
title = {Shaping {Our} {Tools}: {Contestability} as a {Means} to {Promote} {Responsible} {Algorithmic} {Decision} {Making} in the {Professions}},
institution = {Social Science Research Network},
author = {Mulligan, Deirdre K. and Kluttz, Daniel and Kohli, Nitin},
year = {2019},
}

@article{bakshy_exposure_2015,
title = {Exposure to ideologically diverse news and opinion on {Facebook}},
journal = {Science},
author = {Bakshy, Eytan and Messing, Solomon and Adamic, Lada A.},
year = {2015},
}

@article{robertson_auditing_2018,
title = {Auditing {Partisan} {Audience} {Bias} within {Google} {Search}},
journal = {ACM CSCW},
author = {Robertson, Ronald E. and Jiang, Shan and Joseph, Kenneth and Friedland, Lisa and Lazer, David and Wilson, Christo},
year = {2018},
}

@article{hosseinmardi_examining_2021,
title = {Examining the consumption of radical content on {YouTube}},
journal = {PNAS},
author = {Hosseinmardi, Homa and Ghasemian, Amir and Clauset, Aaron and Mobius, Markus and Rothschild, David M. and Watts, Duncan J.},
year = {2021},
}

@inproceedings{spinelli_how_2020,
title = {How {YouTube} {Leads} {Privacy}-{Seeking} {Users} {Away} from {Reliable} {Information}},
booktitle = {ACM UMAP},
author = {Spinelli, Larissa and Crovella, Mark},
year = {2020},
}

@inproceedings{stocker_riding_2020,
title = {Riding the {Wave} of {Misclassification}: {How} {We} {End} up with {Extreme} {YouTube} {Content}},
booktitle = {Social {Computing} and {Social} {Media}. {Design}, {Ethics}, {User} {Behavior}, and {Social} {Network} {Analysis}},
author = {Stöcker, Christian and Preuss, Mike},
year = {2020},
}

@article{seaver_captivating_2019,
title = {Captivating algorithms: {Recommender} systems as traps},
journal = {Journal of Material Culture},
author = {Seaver, Nick},
year = {2019},
}

@misc{auxier_social_2021,
title = {Social {Media} {Use} in 2021},
howpublished = {\url{https://www.pewresearch.org/internet/2021/04/07/social-media-use-in-2021/}},
journal = {Pew Research Center: Internet, Science \& Tech},
author = {Auxier, Brooke and Anderon, Monica},
year = {2021},
}

@misc{duffin_population_2022,
title = {Population of the {U}.{S}. by sex and age 2021},
howpublished = {\url{https://www.statista.com/statistics/241488/population-of-the-us-by-sex-and-age/}},
journal = {Statista},
author = {Duffin, Erin},
year = {2022},
}

@book{salganik_bit_2019,
title = {Bit by {Bit}},
howpublished = {\url{https://press.princeton.edu/books/paperback/9780691196107/bit-by-bit}},
author = {Salganik, Matthew},
year = {2019},
}

@misc{smith_dark_2021,
title = {Dark {Patterns} in {User} {Controls}: {Exploring} {YouTube}’s {Recommendation} {Settings} – {Simply} {Secure}},
howpublished = {\url{https://simplysecure.org/blog/dark-patterns-in-user-controls-exploring-youtubes-recommendation-settings/}},
author = {Smith, Kelsey and Bullen, Georgia and Huerta, Melissa},
year = {2021},
}

@misc{goodrow_you_2017,
title = {You know what’s cool? {A} billion hours},
howpublished = {\url{https://blog.youtube/news-and-events/you-know-whats-cool-billion-hours/}},
journal = {blog.youtube},
author = {YouTube},
year = {2017},
}

@misc{mclachlan_23_2022,
title = {23 {YouTube} {Stats} {That} {Matter} to {Marketers} in 2023},
howpublished = {\url{https://blog.hootsuite.com/youtube-stats-marketers/}},
journal = {Social Media Marketing \& Management Dashboard},
author = {McLachlan, Stacey},
year = {2022},
}

@misc{noauthor_alt-right_nodate,
	title = {Alt-{Right} {\textbar} {Southern} {Poverty} {Law} {Center}},
	url = {https://www.splcenter.org/fighting-hate/extremist-files/ideology/alt-right},
	urldate = {2022-12-16},
}

@misc{cooper_how_2021-1,
	title = {How {Does} the {YouTube} {Algorithm} {Work} in 2021? {The} {Complete} {Guide}},
	shorttitle = {How {Does} the {YouTube} {Algorithm} {Work} in 2021?},
	url = {https://blog.hootsuite.com/how-the-youtube-algorithm-works/},
	abstract = {Looking to increase your YouTube video views? Step one: find out what’s new with the YouTube algorithm and how it ranks your content.},
	language = {en-US},
	urldate = {2021-12-11},
	journal = {Social Media Marketing \& Management Dashboard},
	author = {Cooper, Paige},
	month = jun,
	year = {2021},
}

@inproceedings{stocker_how_2020,
	address = {Cham},
	title = {How {Facebook} and {Google} {Accidentally} {Created} a {Perfect} {Ecosystem} for {Targeted} {Disinformation}},
	isbn = {978-3-030-39627-5},
	doi = {10.1007/978-3-030-39627-5_11},
	abstract = {Online platforms providing information and media content follow certain goals and optimize for certain metrics when deploying automated decision making systems to recommend pieces of content from the vast amount of media items uploaded to or indexed by their platforms every day. These optimization metrics differ markedly from, for example, the so-called news factors journalists traditionally use to make editorial decisions. Social networks, video platforms and search engines thus create content hierarchies that reflect not only user interest but also their own monetization goals. This sometimes has unintended, societally highly problematic effects: Optimizing for metrics like dwell time, watch time or “engagement” can promote disinformation and propaganda content. This chapter provides examples and discusses relevant mechanisms and interactions.},
	language = {en},
	booktitle = {Disinformation in {Open} {Online} {Media}},
		author = {Stöcker, Christian},
	editor = {Grimme, Christian and Preuss, Mike and Takes, Frank W. and Waldherr, Annie},
	year = {2020},
	keywords = {Disinformation, Media content, Online recommendation systems},
	}

@article{karizat_algorithmic_2021,
title = {Algorithmic {Folk} {Theories} and {Identity}: {How} {TikTok} {Users} {Co}-{Produce} {Knowledge} of {Identity} and {Engage} in {Algorithmic} {Resistance}},
journal = {ACM CSCW},
author = {Karizat, Nadia and Delmonaco, Dan and Eslami, Motahhare and Andalibi, Nazanin},
year = {2021},
}

@inproceedings{zhao_recommending_2019,
title = {Recommending what video to watch next: a multitask ranking system},
booktitle = {ACM RecSys},
author = {Zhao, Zhe and Hong, Lichan and Wei, Li and Chen, Jilin and Nath, Aniruddh and Andrews, Shawn and Kumthekar, Aditee and Sathiamoorthy, Maheswaran and Yi, Xinyang and Chi, Ed},
year = {2019},
}

@inproceedings{wu_cross-partisan_2021,
title = {Cross-{Partisan} {Discussions} on {YouTube}: {Conservatives} {Talk} to {Liberals} but {Liberals} {Don}'t {Talk} to {Conservatives}},
booktitle = {ICWSM},
author = {Wu, Siqi and Resnick, Paul},
year = {2021},
}

@misc{solsman_ever_2018,
	title = {Ever get caught in an unexpected hourlong {YouTube} binge? {Thank} {YouTube} {AI} for that},
	shorttitle = {Ever get caught in an unexpected hourlong {YouTube} binge?},
	url = {https://www.cnet.com/news/youtube-ces-2018-neal-mohan/},
	abstract = {More than 70 percent of the time you watch YouTube, you're riding an addictive chain of AI recommendations.},
	language = {en},
	urldate = {2021-12-27},
	journal = {CNET},
	author = {Solsman, Joan E.},
	month = jan,
	year = {2018},
}

@article{chaslot_toxic_2019,
	title = {The {Toxic} {Potential} of {YouTube}’s {Feedback} {Loop}},
	issn = {1059-1028},
	url = {https://www.wired.com/story/the-toxic-potential-of-youtubes-feedback-loop/},
	abstract = {Opinion: I worked on AI for YouTube’s "recommended for you" feature. We underestimated how the algorithms could go terribly wrong.},
	language = {en-US},
	urldate = {2022-03-21},
	journal = {Wired},
	author = {Chaslot, Guillaume},
	month = jul,
	year = {2019},
	note = {Section: tags},
	keywords = {artificial intelligence, social media, youtube},
}

@article{velkova_algorithmic_2021,
title = {Algorithmic resistance: media practices and the politics of repair},
journal = {Information, Communication \& Society},
author = {Velkova, Julia and Kaun, Anne},
year = {2021},
}

@book{pariser_filter_2011,
	title = {The {Filter} {Bubble}: {How} the {New} {Personalized} {Web} {Is} {Changing} {What} {We} {Read} and {How} {We} {Think}},
	isbn = {978-1-101-51512-9},
	shorttitle = {The {Filter} {Bubble}},
	abstract = {An eye-opening account of how the hidden rise of personalization on the Internet is controlling-and limiting-the information we consume.   In December 2009, Google began customizing its search results for each user. Instead of giving you the most broadly popular result, Google now tries to predict what you are most likely to click on. According to MoveOn.org board president Eli Pariser, Google's change in policy is symptomatic of the most significant shift to take place on the Web in recent years-the rise of personalization. In this groundbreaking investigation of the new hidden Web, Pariser uncovers how this growing trend threatens to control how we consume and share information as a society-and reveals what we can do about it.  Though the phenomenon has gone largely undetected until now, personalized filters are sweeping the Web, creating individual universes of information for each of us. Facebook-the primary news source for an increasing number of Americans-prioritizes the links it believes will appeal to you so that if you are a liberal, you can expect to see only progressive links. Even an old-media bastion like The Washington Post devotes the top of its home page to a news feed with the links your Facebook friends are sharing. Behind the scenes a burgeoning industry of data companies is tracking your personal information to sell to advertisers, from your political leanings to the color you painted your living room to the hiking boots you just browsed on Zappos.  In a personalized world, we will increasingly be typed and fed only news that is pleasant, familiar, and confirms our beliefs-and because these filters are invisible, we won't know what is being hidden from us. Our past interests will determine what we are exposed to in the future, leaving less room for the unexpected encounters that spark creativity, innovation, and the democratic exchange of ideas.  While we all worry that the Internet is eroding privacy or shrinking our attention spans, Pariser uncovers a more pernicious and far- reaching trend on the Internet and shows how we can- and must-change course. With vivid detail and remarkable scope, The Filter Bubble reveals how personalization undermines the Internet's original purpose as an open platform for the spread of ideas and could leave us all in an isolated, echoing world.},
	language = {en},
	author = {Pariser, Eli},
	month = may,
	year = {2011},
	keywords = {Business \& Economics / Consumer Behavior, Business \& Economics / E-Commerce / General, Technology \& Engineering / Telecommunications},
}

@article{paolacci_inside_2014,
	title = {Inside the {Turk}: {Understanding} {Mechanical} {Turk} as a {Participant} {Pool}},
	volume = {23},
	issn = {0963-7214},
	shorttitle = {Inside the {Turk}},
	url = {https://doi.org/10.1177/0963721414531598},
	doi = {10.1177/0963721414531598},
	abstract = {Mechanical Turk (MTurk), an online labor market created by Amazon, has recently become popular among social scientists as a source of survey and experimental data. The workers who populate this market have been assessed on dimensions that are universally relevant to understanding whether, why, and when they should be recruited as research participants. We discuss the characteristics of MTurk as a participant pool for psychology and other social sciences, highlighting the traits of the MTurk samples, why people become MTurk workers and research participants, and how data quality on MTurk compares to that from other pools and depends on controllable and uncontrollable factors.},
	language = {en},
	number = {3},
	urldate = {2022-01-30},
	journal = {Current Directions in Psychological Science},
	author = {Paolacci, Gabriele and Chandler, Jesse},
	month = jun,
	year = {2014},
	keywords = {Mechanical Turk, data collection, online experimentation},
	}

%TODO: Get ICWSM to show up in "book title"

@misc{noauthor_left_2021,
	title = {Left vs. {Right} {Bias}: {How} we rate the bias of media sources},
        author = {{Media Bias Fact Check}},
	shorttitle = {Left vs. {Right} {Bias}},
	url = {https://mediabiasfactcheck.com/left-vs-right-bias-how-we-rate-the-bias-of-media-sources/},
	abstract = {When rating the bias of media sources we use the information below to determine editorial bias. In general, a source rated either right or left favors},
	language = {en-US},
	urldate = {2022-12-13},
	journal = {Media Bias/Fact Check},
	month = may,
	year = {2021},
}

@techreport{horta_ribeiro_youniverse_2020,
	title = {{YouNiverse}: {Large}-{Scale} {Channel} and {Video} {Metadata} from {English}-{Speaking} {YouTube}},
	shorttitle = {{YouNiverse}},
	url = {https://ui.adsabs.harvard.edu/abs/2020arXiv201210378H},
	abstract = {YouTube plays a key role in entertaining and informing people around the globe. However, studying the platform is difficult due to the lack of randomly sampled data and of systematic ways to query the platform's colossal catalog. In this paper, we present YouNiverse, a large collection of channel and video metadata from English-language YouTube. YouNiverse comprises metadata from over 136k channels and 72.9M videos published between May 2005 and October 2019, as well as channel-level time-series data with weekly subscriber and view counts. Leveraging channel ranks from socialblade.com, an online service that provides information about YouTube, we are able to assess and enhance the representativeness of the sample of channels. Additionally, the dataset also contains a table specifying which videos a set of 449M anonymous users commented on. YouNiverse, publicly available at https://doi.org/10.5281/zenodo.4650046, will empower the community to do research with and about YouTube.},
	urldate = {2022-06-01},
	author = {Horta Ribeiro, Manoel and West, Robert},
	month = dec,
	year = {2020},
	note = {Publication Title: arXiv e-prints
ADS Bibcode: 2020arXiv201210378H
Type: article},
	keywords = {Computer Science - Computers and Society, Computer Science - Social and Information Networks},
}

@article{haimson_major_2021,
	title = {The major life events taxonomy: {Social} readjustment, social media information sharing, and online network separation during times of life transition},
	volume = {72},
	issn = {2330-1643},
	shorttitle = {The major life events taxonomy},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.24455},
	doi = {10.1002/asi.24455},
	abstract = {When people experience major life changes, this often impacts their self-presentation, networks, and online behavior in substantial ways. To effectively study major life transitions and events, we surveyed a large U.S. sample (n = 554) to create the Major Life Events Taxonomy, a list of 121 life events in 12 categories. We then applied this taxonomy to a second large U.S. survey sample (n = 775) to understand on average how much social readjustment each event required, how likely each event was to be shared on social media with different types of audiences, and how much online network separation each involved. We found that social readjustment is positively correlated with sharing on social media, with both broad audiences and close ties as well as in online spaces separate from one's network of known ties. Some life transitions involve high levels of sharing with both separate audiences and broad audiences on social media, providing evidence for what previous research has called social media as social transition machinery. Researchers can use the Major Life Events Taxonomy to examine how people's life transition experiences relate to their behaviors, technology use, and health and well-being outcomes.},
	language = {en},
	number = {7},
	urldate = {2022-05-09},
	journal = {Journal of the Association for Information Science and Technology},
	author = {Haimson, Oliver L. and Carter, Albert J. and Corvite, Shanley and Wheeler, Brookelyn and Wang, Lingbo and Liu, Tianxiao and Lige, Alexxus},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.24455},
	}

@article{ledwich_algorithmic_2019,
  title={Algorithmic extremism: Examining YouTube's rabbit hole of radicalization},
  author={Ledwich, Mark and Zaitsev, Anna},
  journal={First Monday},
  year={2020}
}

@inproceedings{ribeiro_auditing_2020,
	address = {New York, NY, USA},
	title = {Auditing radicalization pathways on {YouTube}},
	isbn = {978-1-4503-6936-7},
	url = {https://doi.org/10.1145/3351095.3372879},
	doi = {10.1145/3351095.3372879},
	abstract = {Non-profits, as well as the media, have hypothesized the existence of a radicalization pipeline on YouTube, claiming that users systematically progress towards more extreme content on the platform. Yet, there is to date no substantial quantitative evidence of this alleged pipeline. To close this gap, we conduct a large-scale audit of user radicalization on YouTube. We analyze 330,925 videos posted on 349 channels, which we broadly classified into four types: Media, the Alt-lite, the Intellectual Dark Web (I.D.W.), and the Alt-right. According to the aforementioned radicalization hypothesis, channels in the I.D.W. and the Alt-lite serve as gateways to fringe far-right ideology, here represented by Alt-right channels. Processing 72M+ comments, we show that the three channel types indeed increasingly share the same user base; that users consistently migrate from milder to more extreme content; and that a large percentage of users who consume Alt-right content now consumed Alt-lite and I.D.W. content in the past. We also probe YouTube's recommendation algorithm, looking at more than 2M video and channel recommendations between May/July 2019. We find that Alt-lite content is easily reachable from I.D.W. channels, while Alt-right videos are reachable only through channel recommendations. Overall, we paint a comprehensive picture of user radicalization on YouTube.},
	urldate = {2021-12-11},
	booktitle = {FAccT},
		author = {Ribeiro, Manoel Horta and Ottoni, Raphael and West, Robert and Almeida, Virgílio A. F. and Meira, Wagner},
	month = jan,
	year = {2020},
	keywords = {algorithmic auditing, extremism, hate speech, radicalization},
	}

@article{lewis_alternative_2018,
	title = {Alternative influence: {Broadcasting} the reactionary right on {YouTube}},
	journal = {Data \& Society},
	author = {Lewis, Rebecca},
	year = {2018},
}

@article{ricks_does_2022,
	title = {Does {This} {Button} {Work}? {Investigating} {YouTube}'s ineffective user controls},
	language = {en},
	author = {Ricks, Becca and McCrosky, Jesse},
	month = sep,
	year = {2022},
	}

@techreport{noauthor_notitle_nodate-5,
}

@misc{noauthor_does_nodate,
	title = {Does {This} {Button} {Work}? {Investigating} {YouTube}'s ineffective user controls},
	shorttitle = {Does {This} {Button} {Work}?},
	url = {https://foundation.mozilla.org/en/research/library/user-controls/report/},
	abstract = {Powered by 22,722 volunteers, Mozilla scrutinized YouTube to determine how much control people actually have over the platform’s recommendation algorithm. This is what we learned.},
	language = {en},
	urldate = {2022-10-11},
	journal = {Mozilla Foundation},
}

@article{abul-fottouh_examining_2020,
	title = {Examining algorithmic biases in {YouTube}’s recommendations of vaccine videos},
	volume = {140},
	issn = {1386-5056},
	url = {https://www.sciencedirect.com/science/article/pii/S1386505619308743},
	doi = {10.1016/j.ijmedinf.2020.104175},
	abstract = {Objective
This research examines how YouTube recommends vaccination-related videos. Materials and methods
We used a social network analysis to evaluate how YouTube recommends vaccination related videos to its users.
Results
More pro-vaccine videos (64.75\%) than anti-vaccine (19.98\%) videos are on YouTube, with 15.27\% of videos being neutral in sentiment. YouTube was more likely to recommend neutral and pro-vaccine videos than anti-vaccine videos. There is a homophily effect in which pro-vaccine videos were more likely to recommend other pro-vaccine videos than anti-vaccine ones, and vice versa.
Discussion
Compared to our prior study, the number of recommendations for pro-vaccine videos has significantly increased, suggesting that YouTube’s demonization policy of harmful content and other changes to their recommender algorithm might have been effective in reducing the visibility of anti-vaccine videos. However, there are concerns that anti-vaccine videos are less likely to lead users to pro-vaccine videos due to the homophily effect observed in the recommendation network.
Conclusion
The study demonstrates the influence of YouTube’s recommender systems on the types of vaccine information users discover on YouTube. We conclude with a general discussion of the importance of algorithmic transparency in how social media platforms like YouTube decide what content to feature and recommend to its users.},
	language = {en},
	urldate = {2021-12-28},
	journal = {International Journal of Medical Informatics},
	author = {Abul-Fottouh, Deena and Song, Melodie Yunju and Gruzd, Anatoliy},
	month = aug,
	year = {2020},
	keywords = {Disinformation, Misinformation, Network analysis, Recommender algorithm, Social media, Vaccination, YouTube},
	}

@article{haroon_youtube_2022,
title = {{YouTube}, {The} {Great} {Radicalizer}? {Auditing} and {Mitigating} {Ideological} {Biases} in {YouTube} {Recommendations}},
journal = {arXiv:2203.10666 [cs]},
author = {Haroon, Muhammad and Chhabra, Anshuman and Liu, Xin and Mohapatra, Prasant and Shafiq, Zubair and Wojcieszak, Magdalena},
year = {2022},
}

@inproceedings{spinelli_how_2020-1,
	address = {New York, NY, USA},
	series = {{UMAP} '20 {Adjunct}},
	title = {How {YouTube} {Leads} {Privacy}-{Seeking} {Users} {Away} from {Reliable} {Information}},
	isbn = {978-1-4503-7950-2},
	url = {https://doi.org/10.1145/3386392.3399566},
	doi = {10.1145/3386392.3399566},
	abstract = {Online media is increasingly selected and filtered by recommendation engines. YouTube is one of the most significant sources of socially-generated information, and as such its recommendation policies are important to understand. Because of YouTube's revenue model, the nature of its recommendation policies is fairly opaque. Hence, we present an empirical exploration of the nature of YouTube recommendations, concentrating on socially-impactful dimensions. First, we confirm that YouTube's recommendations generally "lead away" from reliable information sources, with a tendency to direct users over time toward video channels exposing extreme and unscientific viewpoints. Second, we show that there is a fundamental tension between user privacy and extreme recommendations. We show that in general, users who seek privacy by keeping personal information hidden, receive much more extreme and unreliable recommendations from the YouTube engine. This drawback of user privacy in the presence of recommender systems has not been widely appreciated. We quantify this effect along various dimensions, including its dynamics in time, and show that the tradeoff between privacy and unreliability of recommendations is generally pervasive in the YouTube recommendation process.},
	urldate = {2021-12-12},
	booktitle = {Adjunct {Publication} of the 28th {ACM} {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
		author = {Spinelli, Larissa and Crovella, Mark},
	month = jul,
	year = {2020},
	keywords = {YouTube recommendations, social impacts of recommendations},
	}

@incollection{tomlein_audit_2021,
title = {An {Audit} of {Misinformation} {Filter} {Bubbles} on {YouTube}: {Bubble} {Bursting} and {Recent} {Behavior} {Changes}},
booktitle = {ACM RecSys},
author = {Tomlein, Matus and Pecher, Branislav and Simko, Jakub and Srba, Ivan and Moro, Robert and Stefancova, Elena and Kompan, Michal and Hrckova, Andrea and Podrouzek, Juraj and Bielikova, Maria},
year = {2021},
}

@article{papadamou_it_2021,
title = {"{It} is just a flu": {Assessing} the {Effect} of {Watch} {History} on {YouTube}'s {Pseudoscientific} {Video} {Recommendations}},
journal = {arXiv:2010.11638 [cs]},
author = {Papadamou, Kostantinos and Zannettou, Savvas and Blackburn, Jeremy and De Cristofaro, Emiliano and Stringhini, Gianluca and Sirivianos, Michael},
year = {2021},
}

@article{hussein_measuring_2020,
	title = {Measuring {Misinformation} in {Video} {Search} {Platforms}: {An} {Audit} {Study} on {YouTube}},
	volume = {4},
	shorttitle = {Measuring {Misinformation} in {Video} {Search} {Platforms}},
	url = {https://doi.org/10.1145/3392854},
	doi = {10.1145/3392854},
	abstract = {Search engines are the primary gateways of information. Yet, they do not take into account the credibility of search results. There is a growing concern that YouTube, the second largest search engine and the most popular video-sharing platform, has been promoting and recommending misinformative content for certain search topics. In this study, we audit YouTube to verify those claims. Our audit experiments investigate whether personalization (based on age, gender, geolocation, or watch history) contributes to amplifying misinformation. After shortlisting five popular topics known to contain misinformative content and compiling associated search queries representing them, we conduct two sets of audits-Search-and Watch-misinformative audits. Our audits resulted in a dataset of more than 56K videos compiled to link stance (whether promoting misinformation or not) with the personalization attribute audited. Our videos correspond to three major YouTube components: search results, Up-Next, and Top 5 recommendations. We find that demographics, such as, gender, age, and geolocation do not have a significant effect on amplifying misinformation in returned search results for users with brand new accounts. On the other hand, once a user develops a watch history, these attributes do affect the extent of misinformation recommended to them. Further analyses reveal a filter bubble effect, both in the Top 5 and Up-Next recommendations for all topics, except vaccine controversies; for these topics, watching videos that promote misinformation leads to more misinformative video recommendations. In conclusion, YouTube still has a long way to go to mitigate misinformation on its platform.},
	number = {CSCW1},
	urldate = {2021-12-28},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Hussein, Eslam and Juneja, Prerna and Mitra, Tanushree},
	month = may,
	year = {2020},
	keywords = {algorithmic audit, conspiracy theory, group fairness, information retrieval, misinformation, misinformation audit, search engines},
	}

@article{sandvig_auditing_2014,
	title = {Auditing {Algorithms}: {Research} {Methods} for {Detecting} {Discrimination} on {Internet} {Platforms}},
	language = {en},
	author = {Sandvig, Christian and Hamilton, Kevin and Karahalios, Karrie and Langbort, Cedric},
	month = may,
	year = {2014},
	}

@inproceedings{metcalf_algorithmic_2021,
	address = {New York, NY, USA},
	series = {{FAccT} '21},
	title = {Algorithmic {Impact} {Assessments} and {Accountability}: {The} {Co}-construction of {Impacts}},
	isbn = {978-1-4503-8309-7},
	shorttitle = {Algorithmic {Impact} {Assessments} and {Accountability}},
	url = {https://doi.org/10.1145/3442188.3445935},
	doi = {10.1145/3442188.3445935},
	abstract = {Algorithmic impact assessments (AIAs) are an emergent form of accountability for organizations that build and deploy automated decision-support systems. They are modeled after impact assessments in other domains. Our study of the history of impact assessments shows that "impacts" are an evaluative construct that enable actors to identify and ameliorate harms experienced because of a policy decision or system. Every domain has different expectations and norms around what constitutes impacts and harms, how potential harms are rendered as impacts of a particular undertaking, who is responsible for conducting such assessments, and who has the authority to act on them to demand changes to that undertaking. By examining proposals for AIAs in relation to other domains, we find that there is a distinct risk of constructing algorithmic impacts as organizationally understandable metrics that are nonetheless inappropriately distant from the harms experienced by people, and which fall short of building the relationships required for effective accountability. As impact assessments become a commonplace process for evaluating harms, the FAccT community, in its efforts to address this challenge, should A) understand impacts as objects that are co-constructed accountability relationships, B) attempt to construct impacts as close as possible to actual harms, and C) recognize that accountability governance requires the input of various types of expertise and affected communities. We conclude with lessons for assembling cross-expertise consensus for the co-construction of impacts and building robust accountability relationships.},
	urldate = {2021-11-17},
	booktitle = {{Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}}},
		author = {Metcalf, Jacob and Moss, Emanuel and Watkins, Elizabeth Anne and Singh, Ranjit and Elish, Madeleine Clare},
	month = mar,
	year = {2021},
	keywords = {accountability, algorithmic impact assessment, governance, harm, impact},
	}

@article{bandy_problematic_2021,
title = {Problematic {Machine} {Behavior}: {A} {Systematic} {Literature} {Review} of {Algorithm} {Audits}},
journal = {ACM CSCW},
author = {Bandy, Jack},
year = {2021},
}

@article{metaxa_auditing_2021,
title = {Auditing {Algorithms}: {Understanding} {Algorithmic} {Systems} from the {Outside} {In}},
journal={Foundations and Trends{\textregistered} in Human--Computer Interaction},
author = {Metaxa, Danaë and Park, Joon Sung and Robertson, Ronald E. and Karahalios, Karrie and Wilson, Christo and Hancock, Jeff and Sandvig, Christian},
year = {2021},
}

@article{wu2019estimating,
  title={Estimating {Attention} {Flow} in {Online} {Video} {Networks}},
  author={Wu, Siqi and Rizoiu, Marian-Andrei and Xie, Lexing},
  journal={ACM CSCW},
  year={2019}
}