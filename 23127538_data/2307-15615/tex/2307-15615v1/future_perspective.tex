Over the past decade, learning-based registration models have been attracting increasing research interest. As illustrated in the left panel of Fig. \ref{fig:paper_stats}, there has been a growing trend in developing and applying these models since 2013. Unlike other medical image analysis tasks, such as segmentation or classification, which typically necessitate labor-intensive and time-consuming manual annotations to develop high-performing models, registration is inherently a self-supervised task. Traditional registration models have predominantly been unsupervised, requiring only moving and fixed images to execute registration. While traditional registration models are typically unsupervised, learning-based registration models initially began as a supervised process, generating ground truth deformation fields using traditional registration methods. However, these supervised models often could not surpass the performance of traditional methods. Instead, they often served as an intermediate step to expedite conventional approaches like geodetic shooting~\citep{shen2019region}, FLASH~\citep{wang2020deepflash}, etc. Despite traditional methods providing appealing deformation properties such as time-dependent diffeomorphic transformations, researchers have recently begun exploring the unsupervised nature of learning-based registration (as seen in the left panel of Fig. \ref{fig:paper_stats}). By training DNNs using loss functions adapted from traditional methods' objective functions, these methods aim to improve both registration accuracy and speed. Incorporating segmentation and landmark correspondences during training can further enhance registration accuracy, providing capabilities not achievable with traditional methods. Given the rapid progress of deep learning and its growing adoption in medical applications, we anticipate an increasing focus on learning-based medical image registration.

In this section, we provide future perspectives and discuss potential avenues for advancing learning-based medical image registration. Our discussion will focus on the development of registration models, assessment of registration uncertainty, and prospective applications.

% Figure environment removed
\subsection{Deep Learning-based Registration Models}
\subsubsection{Network Architecture}
Network architectures employed in image registration occasionally draw inspiration from other image analysis tasks, such as segmentation. For instance, VoxelMorph~\citep{balakrishnan2019voxelmorph}, CycleMorph~\citep{kim2021cyclemorph}, SYMNet~\citep{mok2020fast}, and DiffuseMorph~\citep{kim2022diffusemorph} all borrow U-Net-like architectures, originally developed for image segmentation. In such cases, they often generate deformation fields at a single resolution. In contrast, traditional registration algorithms have demonstrated the benefits of adopting a multi-resolution registration strategy, which decomposes deformations across multiple scales. This method not only improves registration performance but also imparts beneficial deformation properties, such as the ability to enforce larger deformations. This aspect can be particularly beneficial for lung or abdominal organ registration, where organ displacement between scans can be significant. As discussed in section \ref{sec:multi_res_reg}, there has been a growing interest in integrating multi-resolution strategies into network architecture, and these methods have consistently demonstrated notable performance improvements compared to using a single resolution alone. It is worth noting that this finding has parallels with observations in other image analysis fields, where adopting deep supervision can significantly boost performance~\citep{zhou2019unet++, isensee2021nnu}. 

Moreover, registration task is intrinsically different from other tasks, as it requires the network to capture the correspondences between images rather than comprehending the context contained within the images themselves. This concept is exemplified by a recent work, SynthMorph~\citep{hoffmann2021synthmorph}, where the authors demonstrated that training a viable medical image registration network does not strictly require medical images. Instead, random shapes or synthetic images can also serve as training datasets for registration networks. Consequently, when designing network architectures for image registration, the primary focus should be on their ability to capture spatial correspondences between images. Architectures such as Transformers (particularly cross-attention Transformers), contrastive learning, Siamese networks, and correlation layers, which leverage comparisons between moving and fixed images, are of special interest for image registration. We expect to see an increasing number of studies incorporating these designs in the future, along with other advancements in deep learning applied to image registration.

% L2R 2022 (Universal reg)
%multi-resolution
%registration tailored network

\subsubsection{Loss Function}
In unsupervised models, the image similarity measures predominantly used for mono-modal registration are MSE and NCC, as shown in Table \ref{table:unsup_list}. NCC is generally considered a better choice than MSE, as it is locally adaptive and less sensitive to local intensity variations~\citep{avants2008symmetric}. For multi-modal registration, MI has historically been the preferred choice~\citep{maes1997multimodality, wells1996multi}. However, to auto-differentiate MI using modern deep learning frameworks for end-to-end training, joint and marginal probabilities are often approximated using the Parzen window. A notable drawback of this approximation is the increased computational burden. In actual implementation, each voxel location expands to include a vector, with the elements in the vector representing the probability of the voxel belonging to each intensity bin. Increasing the number of intensity bins effectively results in an increased channel dimension, ultimately leading to a higher computational burden. Conversely, using a small number of bins often limits the registration performance. Recent learning-based methods have explored surrogates to tackle multi-modal registration problems. For instance, given the advantage of learning, anatomical loss functions like Dice can serve as a modality-independent loss function for training the registration network~\citep{hoffmann2021synthmorph}. The trained network can then be applied to images without requiring anatomical segmentation, offering an advantage that traditional methods cannot provide. Multi-modal registration can also be addressed using advanced learning methods, such as contrastive learning and adversarial learning, as discussed in sections \ref{sec:contrast_learn} and \ref{sec:adv_learn}. These methods guide the neural network in understanding similarities and dissimilarities between images across different modalities using paired data without requiring explicit multi-modal similarity measures. We expect future research to continue to develop more efficient and innovative approaches to tackle multi-modal registration.

Regarding the use of regularization in learning-based deformable registration, there is currently an inadequate emphasis on the development and application of spatially-varying regularization. Despite being a significant area of research historically~\citep{schnabel2016advances, schmah2013left,vialard2014spatially,stefanescu2004grid,tang2010reliability,pitiot2008geometrical,gerig2014spatially,simpson2015probabilistic,pace2013locally,myronenko2010intensity,papiez2014implicit,fu2018adaptive,risser2013piecewise,papiez2015liver}, spatially-varying regularization has been largely overshadowed by the rise of learning-based registration, with only a few studies addressing it within a deep learning framework framework~\citep{niethammer2019metric,shen2019region,chen2023spr,chen2021deep2}. As illustrated in Table \ref{table:unsup_list}, most methods opt for a simple spatially-invariant regularization, predominantly employing the diffusion regularizer. However, as outlined in section \ref{sec:def_reg}, spatially-varying regularization provides the advantage of accommodating spatially-varying deformations, preserving discontinuities, and facilitating sliding motion, all of which are essential for a variety of applications. Advancements in modeling spatially-varying regularization within or through deep learning frameworks are eagerly anticipated in the future.

%allow discontinuity
%need for better multi-modal similarity metric <-> Contrastive learning (modal independent)
% \subsection{Evaluation Metrics}
% Yihao
\subsection{Registration Uncertainty}
% Shuwen
Registration uncertainty in medical image analysis is an ongoing challenge and opportunity. On the one hand, advancements in deep learning have the potential to improve registration accuracy and reduce registration uncertainty by extracting features that are robust to noise and other artifacts. On the other hand, uncertainty can be estimated for use in interpreting the registration results and providing valuable information for clinical decision-making.

However, there are several limitations that restrict the further usage of uncertainty estimation in various applications. One significant limitation is the lack of ground truth for evaluating the quality of uncertainty estimation. Without ground truth, it is challenging to validate the accuracy of uncertainty estimation directly. Instead, most existing evaluation methods rely on indirect proofs such as sparsification analysis. This not only affects the reliability of uncertainty estimation, but also limits further developments for better uncertainty estimations. Another limitation is the computational complexity of estimating uncertainty, which can be time-consuming and may limit its usage in real-time clinical applications. Additionally, interpreting uncertainty estimates can be challenging for clinicians, as some statistical measures are not always straightforward. This can limit the adoption of uncertainty estimation in clinical decision-making, where clear and concise information is essential for making informed decisions.

To overcome these limitations, it may be helpful to develop improved evaluation methods that rely on direct validation rather than indirect proofs, such as the creation of synthetic data or the use of simulation frameworks where the ground truth is known. This could enhance the accuracy and reliability of uncertainty estimation. Moreover, new computational techniques and algorithms such as incorporating Markov Chain Monte Carlo into a multilevel framework \citep{schultz2018multilevel} and quantifying image registration uncertainty based on a low dimensional representation of geometric deformations \citep{wang2019registration} can reduce the computational workload. Last, efforts should be made to provide clinicians with more accessible and intuitive ways to interpret uncertainty estimates, such as visual aids or simpler statistic measures.

In addition to the limitations of uncertainty estimation in medical image analysis, there are also many potential applications of registration uncertainty that remain unexplored. One promising area of application is atlas-based segmentation, where registration is often used to align an atlas image to a target image for the purpose of segmenting anatomical structures. In this context, registration uncertainty can be used as a criterion for generating a soft segmentation mask, where the probability of each voxel belonging to a particular anatomical structure is weighted by the uncertainty estimate. Another potential application of registration uncertainty is multi-atlas-based segmentation, where multiple atlases are registered to a target image and combined to produce a final segmentation result. In this context, registration uncertainty can be used to weight different segmentation results, producing a more reliable segmentation. This approach could be particularly useful in cases where some atlases are more appropriate for a particular image than others. 

Overall, these limitations and potential applications of registration uncertainty in medical image analysis offer an exciting range of opportunities for future research, and it is likely that continued progress in this area will have a substantial impact on the field of medical image analysis and beyond.

\subsection{Towards Zero-shot Registration}
Classic registration algorithms, while potentially slower, are usually available for immediate use and provide end-users with the flexibility to choose the similarity measure and weighting of regularization terms that best meet their needs. In contrast, deep learning algorithms are susceptible to the domain shift problem, which arises when a trained network struggles to perform well when presented with input images from a different distribution than the training data. Several sources of domain shift can arise in learning-based registration algorithms, such as changes in the input image modality, different populations of subjects, or variations in the direction of registration.
To address this challenge, researchers have explored several methods to improve the generalizability of registration networks. For instance, SynthMorph~\citep{hoffmann2021synthmorph} uses synthetic images to force the network to learn contrast-invariant features, while HyperMorph~\citep{hoopes2021hypermorph} uses a hypernetwork to enable the adjustment of the regularization term during test time. Although these approaches have shown promising results, they have not yet been widely adopted, and further studies and validations are necessary to establish their effectiveness in real-world scenarios.

Recent developments in zero-shot learning offer a promising avenue for further improving the generalizability of learning-based registration algorithms. In particular, Foundation models that are pretrained on a broad range of data have shown competitive or even superior zero-shot performance compared to prior supervised models in various tasks~\citep{kirillov2023segment, brown2020language}, without requiring specific training data for each new task. Leveraging these techniques can potentially reduce the time and resource requirements for developing deep learning registration algorithms in clinical pipelines, making the existing registration algorithms more accessible and useful to a wider range of users.

\subsection{Metamorphic Image Registration}
As outlined in Section \ref{sec:diff_reg}, diffeomorphic registration is a bijective mapping that preserves topology. In clinical scenarios, however, registration often involves the deformation of a healthy control or an atlas to fit patient images that may contain tumors or other anomalies. For example, longitudinal scans of the same patient with a tumor may need to be mapped to one another to facilitate the study of the tumor progression or response. Such situations violate the one-to-one mapping assumption of diffeomorphisms due to topological changes between scans. To address this challenge, alternative registration methods such as metamorphic registration models~\citep{brett2001spatial, sdika2009nonrigid, niethammer2011geometric, hong2012metamorphic, franccois2022weighted} have been proposed, which can accommodate changes in topology and appearance. For a mathematical definition of metamorphosis, readers can refer to~\citep{trouve2005metamorphoses, younes2010shapes}. However, these methods often require manual segmentation of the anomalies and are optimization-based, which can be time-consuming and computationally expensive, thereby limiting their practical adoption.

Recently proposed learning-based metamorphic registration methods~\citep{wang2023metamorph,han2020deep,bone2020learning, maillard2022deep} have been built upon a metamorphic framework~\citep{trouve2005metamorphoses, younes2010shapes}, which adds time-varying intensity variations on top of the diffeomorphic flow, thereby enabling topological changes over time. The registration networks learn to disentangle geometric and appearance changes and sometimes leverage available segmentation to constrain changes within a desired location (e.g., a tumor). With the success of learning-based image segmentation, the time-consuming manual segmentation previously required by classical methods to guide the metamorphosis can now be addressed using segmentation networks. Several recent approaches take advantage of segmentation networks through joint training~\citep{wang2023metamorph} or by integrating segmentation capabilities directly into the registration network~\citep{han2020deep}. Alternatively, some studies learn to disentangle appearance and shape changes directly from data without requiring to explicitly define a region~\citep{bone2020learning, maillard2022deep}. Despite these recent advancements, learning-based metamorphic registration is still in its infancy. Successfully capturing topological changes still greatly depends on the accuracy of the segmentation network. Meanwhile, the effective modeling of time-varying diffeomorphic flow using DNNs continues to be an area of ongoing research. Considering the practical potential of metamorphic registration, metamorphic registration represents an appealing direction for future investigation in learning-based registration research.

\subsubsection{Spatial-temporal Image Registration}
% Zhangxing
Learning-based image registration methods have primarily focused on aligning a single image pair. However, accurately tracking tissue motion across multiple frames remains a less explored yet crucial task in numerous medical imaging applications, including tagged/cine MRIs, 4D-CT, echocardiography, and motion correction for nuclear medicine imaging. 
Addressing this task involves overcoming several challenges and taking into account key questions that require careful consideration. 
For instance, how can one ensure the preservation of desired properties, such as smoothness, diffeomorphism, and incompressibility, throughout temporally long-range tracking? Achieving accurate 4D tracking while maintaining a reasonable computational burden in terms of both temporal and spatial complexity poses another challenge. Moreover, how can varying input frame lengths be effectively managed? These questions demand further exploration and investigation in order to advance our understanding and capability in motion tracking.

Recent advancements in computer vision, particularly in the context of natural video, have demonstrated promising results. Methods such as correspondence learning~\citep{jabri2020space, bian2022learning, zhang2023boosting, araslanov2021dense} and spatial-temporal representation learning~\citep{kim2019self, yao2020video, wang2019self, qian2021spatiotemporal, dave2022tclr} may offer valuable insights to tackle these challenges we face. By building upon these recent advancements, we may pave the way for more effective and efficient motion-tracking methods for medical imaging applications.
