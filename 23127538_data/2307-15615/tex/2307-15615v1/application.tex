

\subsection{Atlas Construction}
In computational anatomy, atlases have been an essential tool for investigating the variability of human organs across populations and facilitating the segmentation of organs in individual patients. Typically, atlases are constructed through an iterative averaging process (\emph{i.e.}, \textit{procrustean averaging}~\citep{ma2008bayesian}) using a population of patient images~\citep{allassonniere2007towards, davis2004large, guimond2000average, joshi2004unbiased, ma2008bayesian, avants2010optimal}. This procedure commences with the registration of images to a common frame of reference, followed by the computation of an average based on the registered images, which serves as the atlas for the current iteration. The iteration cycle continues until convergence has been achieved, resulting in the final atlas. However, these traditional methods tend to blur regions exhibiting high-frequency deformations~\citep{dey2021generative}. This shortcoming arises from the averaging of intensities when constructing the atlas, which invariably results in the loss of high-frequency information essential for capturing anatomical details.

Recent advancements in learning-based registration have demonstrated significant improvements in the quality of constructed atlases while concurrently expediting the atlas construction process. 
\citet{dalca2019learning} pioneered the development of a brain atlas within a deep learning framework, in which an initial approximation of the atlas is derived from the mean of the brain images under study.
This atlas is then jointly optimized with a registration network, employing the VoxelMorph architecture to align the atlas with individual patient images.
Throughout the training process, both the atlas and the registration network weights are updated.
To promote an unbiased atlas and enhance spatial smoothness in the resulting deformation fields, the authors introduce a Gaussian-inspired prior.
This prior serves to penalize sharp deformation changes while simultaneously encouraging minimal average deformation across the entire dataset. 
Moreover, patient demographic information is conditioned into the network architecture, facilitating the generation of conditional atlases that vary according to the specific attributes of different individuals.
This work has inspired a variety of applications. For instance,~\citet{cheng2020unbiased} establish continuous spatio-temporal cortical surface atlases for neonatal brains.
Similarly, both~\citet{zhao2021learning} and~\citet{bastiaansen2022towards} construct continuous spatio-temporal atlases for fetal and infant brains.
Zhao~\emph{et al.} developed a multi-scale spherical registration network featuring group-wise registration, while Bastiaansen~\emph{et al.} applied group-wise registration to volumetric ultrasound images.
Alternatively,~\citet{yu2020learning} constructed an unconditional and universal atlas while incorporating demographic information into the displacement field generation.
This approach explicitly models morphological changes related to attributes as a diffeomorphic deformation, which captures variations in shape and size.
Recognizing that the necessity for images to be affinely aligned in a preprocessing step as suggested in~\citep{dalca2019learning} could not adequately capture the dynamic size and shape development of fetal brain structures,~\citet{chen2021construction} proposed incorporating an affine network, conditioned on patient demographic data, to register the constructed atlas to individual patient images.
This approach preserves the dynamic size and shape variations of patients at different ages.
\citet{li2021cas} proposed integrating the segmentation produced by a segmentation network into the atlas construction method proposed in~\citep{dalca2019learning}.
This method enables the joint training of segmentation and registration networks while simultaneously constructing both image and segmentation atlases. 
Similarly,~\citet{sinclair2022atlas} embraced the concept of jointly training segmentation and registration networks.
They were motivated by the observation that segmentation networks often yield spurious voxel-wise predictions.
By warping the label map of the constructed atlas to match the segmentation prediction through learning-based diffeomorphic registration, the topology of the original anatomical structure can be preserved, thus avoiding the potential segmentation errors produced by the segmentation network.
Drawing inspiration from \citet{dalca2019learning} and \citet{shu2018deforming},~\citet{siebert2021learning} proposed using a shared encoder to extract features from input images, followed by two decoders.
One decoder generates an unconditional atlas, while the other produces deformation fields that warp the atlas to individual images.
To improve registration performance and enforce unbiased atlas construction, they introduced an inverse consistency and a bias reduction loss, in addition to the commonly seen similarity measure and deformation regularizer.
In a related study,~\citet{wu2022hybrid} proposed a closed-form update for constructing the atlas by leveraging pre-trained registration networks as a priori knowledge of the deformation field.
Their approach involves an alternating update process for both the deformation field, which warps the atlas, and the atlas itself.
This method results in an atlas construction framework that is independent of the registration model choice, offering flexibility in its application.

Researchers have explored various strategies to enhance the quality of the constructed atlases.
\citet{dey2021generative} improved the constructed atlas by incorporating adversarial learning, which improved both the sharpness and centrality of the resulting atlas.
In a similar vein,~\citet{he2021learning} aimed to improve the atlas' sharpness through adversarial learning and by integrating edge information derived from anatomical label maps.
Additionally,~\citet{pei2021learning} leveraged anatomical label maps to improve the quality of the constructed atlas by applying anatomical consistency supervision.
However,~\citet{ding2022aladdin} contended that the importance of atlas sharpness is secondary to the registration model's ability to align corresponding points between images in the atlas space.
Therefore, they focused on the registration model upon which the atlas construction is based and proposed using the constructed atlas as a bridge.
In their method, an image is first warped to align with an atlas and then further warped to match the target image using the registration network. This process facilitates a direct comparison between the warped image and the target image while enabling the construction and evaluation of the atlas without requiring segmentation of the atlas itself.
Inspired by implicit neural shape representations~\citep{mescheder2019occupancy}, ~\citet{yang2022implicitatlas} proposed constructing atlases of anatomical shapes using a continuous occupancy grid instead of representing them in a voxel-based manner.
Given the latent representation of the shape, this alternative approach constructs an atlas based on the linear combination of a learned template matrix.
Their method offers a novel perspective on atlas representation, diverging from traditional voxel-based representations.

Advancements in learning-based atlas construction methods have facilitated the fast construction of high-quality atlases. The following subsection explores the application of the atlases and learning-based image registration in achieving the goal of image segmentation.
%Junyu
\subsection{Multi-atlas Segmentation}
Multi-atlas segmentation is a well-established registration-based segmentation technique in existence for several decades~\citep{rohlfing2003ipmi, iglesias2015multi}.
The typical approach involves registering atlas images or their patches to a target image and fusing the propagated atlas labels.
For deformable registration-based multi-atlas methods, the process of pairwise registration between atlas images and the target image can be computationally expensive and time-consuming. 
However, recent advancements in deep learning-based deformable registration algorithms provide a promising solution to address the speed issue and potentially improve the accuracy of registration, which can subsequently improve the accuracy of multi-atlas segmentation.
While many works have explored the use of deep networks to improve the fusion of multiple registered atlas images~\citep{zhu2020fcn, fang2019automatic, xie2019improving, xie2023deep, yang2018neural}, there are relatively few studies that incorporate deep learning-based deformable registration algorithms into their pipeline.

Ding~\emph{et al.}~\citep{ding2019votenet, ding2020votenet+} proposed VoteNet, which predicts a voxel probability of the agreement between registered atlas images and the segmentation target image.
They adopted Quicksilver~\citep{yang2017quicksilver} as their registration algorithm to speed up the pairwise registration process.
Their follow-up work~\citep{ding2021votenet++} experimented with improving the initial registration results from Quicksilver by incorporating a registration refinement step.
The results showed that registration accuracy is a critical factor in achieving accurate multi-atlas segmentation.
In~\citet{ding2022cross}, the authors addressed the challenging problem of cross-modality multi-atlas segmentation. They proposed a deep network that learns the bi-directional registration between atlas images and the target image, as well as a second network that estimates the weights for label fusion.
To account for the modality differences between the atlas and target images, they used Dice loss as a similarity measure to train their registration network and conditional entropy to train the fusion network.
For registering 3D first-trimester ultrasound images,~\citet{bastiaansen2022multi} proposed a two-stage network for learning an affine transformation.
They then applied the VoxelMorph architecture to perform deformable registration on the affinely aligned images. 
The segmentation of the target images was achieved by propagating the labels of the atlas images and combining them using majority voting.

The good performance of supervised training in image segmentation could be a reason for the relative lack of research on deep learning-based registration in multi-atlas segmentation.
Deep neural networks have demonstrated impressive results in supervised image segmentation tasks, making them a popular choice for many researchers. However, the performance of single atlas segmentation is often used to evaluate the accuracy of a registration algorithm, as discussed in Section~\ref{ss:anatomical_info}.
Due to the close relationship between registration and segmentation, there is an increasing interest in exploring the possibility of integrating the learning of segmentation and registration~\citep{sinclair2022atlas, khor2023anatomically, xu2019deepatlas}.
Overall, the use of deep learning-based registration in multi-atlas segmentation is still in its early stages, and there is a significant opportunity for further research.

\subsection{Uncertainty}
% Shuwen
Accurate registration is critical for many medical image analysis applications, such as image-guided surgery, radiation therapy, and longitudinal studies. 
However, registration uncertainty can arise due to factors such as training data artifacts or predictive model variances.
To address this issue, incorporating registration uncertainty into medical image analysis can help guide the interpretation of the registration results and improve the reliability of various analysis tasks. 

In clinical decision-making, understanding registration uncertainty is critical for image-guided surgery and radiation therapy. The absence of proper registration uncertainty awareness may lead surgeons to presume a substantial registration error throughout the entire region based on a large error in a single location, resulting in the total disregard of registration. Furthermore, the lack of registration uncertainty may also cause surgeons to place unwarranted confidence in regions with inaccurate registration, resulting in potentially severe consequences. 

For image-guided surgery,~\citet{risholm2013bayesian} showed that the registration uncertainty increased at the site of resection using clinical data from neurosurgery for resection of brain tumors, which demonstrated the potential utility of registration uncertainty in recognizing the surgical regions and guiding surgery.
For radiation therapy,~\citet{risholm2011estimation} had previously presented a probabilistic framework to estimate the accumulated radiation dose and corresponding dose uncertainty delivered to significant anatomical structures during radiation therapy, such as the primary tumor and healthy surrounding organs.
The uncertainty in the estimated dose directly results from registration uncertainty in the deformation used to align daily cone-beam CT images with planning CT. 
The accumulated radiation dose is an important metric to monitor during treatment, potentially requiring treatment plan adaptation to conform to the current patient anatomy. 

A study by~\citet{nenoff2020deformable} employed six different deformable registration algorithms to analyze dose uncertainty in proton therapy and investigate their impact on dose accumulation for non-small cell lung cancer patients with inter-fractional anatomy variations.
The results show that dose degradation caused by anatomical changes was more pronounced than the uncertainty arising from using different deformable image registration algorithms for dose accumulation. 
However, accumulated dose variations between these algorithms can still be substantial, leading to additional dose uncertainty.

In longitudinal medical image analysis, registration is an essential step because it enables the comparison of measurements taken at different time points, which is necessary for correcting anatomical variability and tracking changes over time.
Registration uncertainty estimation can be beneficial for longitudinal image processing tasks, such as image smoothing, segmentation prior propagation, joint label fusion, and others. \citet{simpson2011longitudinal}~proposed an approach to calculate the deformable registration uncertainty using a probabilistic registration framework, integrating the uncertainty into spatially normalized statistics for adaptive image smoothing.
This method showed improved classification results in longitudinal MR brain images acquired from Alzheimer's Disease Neuroimaging Initiative compared to not smoothing or using a straightforward Gaussian filter kernel.

In summary, incorporating registration uncertainty into medical image registration can facilitate interpreting registration results and improve the reliability of various medical image analysis tasks. 
It is crucial for clinicians to understand registration uncertainty and its potential applications in clinical decision-making.
Further research is needed to explore other potential applications of registration uncertainty.

\subsection{Motion Estimation}

In the context of medical images, deep learning-based motion estimation has been closely associated with the unsupervised optical flow~\citep{jonschkowski2020matters, stone2021smurf,bian2022learning} and point tracking~\citep{lai2019self, harley2022particle, ranjan2019competitive, bian2022learning} techniques within the computer vision domain. However, the application of motion estimation in medical imaging presents unique challenges, including limited training data, heterogeneous patient data for testing, and special desired properties on the motion field, such as diffeomorphism~(to preserve anatomical relationships) and incompressibility~(to preserve anatomical integrity).
Deep learning-based registration has demonstrated successful outcomes in estimating motion for various organs, such as the human heart, brain, lungs, and tongue.
Registration-based motion estimation plays a significant role in enabling the assessment of changes in the position, shape, and size of organs over time. 
Multiple dynamic imaging modalities are used for motion estimation in medical imaging, including but not limited to cine images,  tagged-MRI~\citep{axel1989heart, axel1989mr}, and echocardiography.

Cine images are a temporal sequence of MR images captured in quick succession, allowing for the monitoring of organ movement and deformation over time.
Recent research~\citep{qin2018joint, morales2019implementation, meng2022mulvimotion, yu2020foal, qin2023generative, lopez2022warppinn, yu2020motion} has successfully applied deep learning-based registration techniques to cine images.
For example, FOAL~\citep{yu2020foal} proposed online optimization to mitigate distribution mismatch between the training and testing datasets for motion estimation, using meta-learning techniques to enable more efficient online optimization with fewer gradient descent steps and smaller data samples, which differs from instance-specific optimization~\citep{balakrishnan2019voxelmorph}. 
\citet{yu2020motion}~applied similarity and smoothness loss to multiple scales of motion fields (pyramid) using a deep supervision strategy. %The study also developed a student model, which runs faster during inference, by training from a teacher model that estimates motion fields more accurately using two-step optimization.
 
The relatively uniform signal within tissues from cine images and the lack of reliable, identifiable landmarks have motivated the exploration of additional regularization methods for estimating motion that is biologically plausible and clinically reliable.
 For example, Qin~\emph{et al.}~\citep{qin2023generative} trained a variational autoencoder-based generative model to capture the prior of biomechanically plausible deformations by reconstructing simulated deformations using finite element models. This prior is then used as regularization during the training of the registration network. 
 Lopez~\emph{et al.}~\citep{lopez2022warppinn} incorporated hyperelastic regularization terms into the framework of physics-informed neural networks~\citep{raissi2019physics} to estimate incompressible motion fields.

Tagged-MRI, on the other hand, employs a spatially modulated periodic pattern to magnetize tissue temporarily, producing transient tags in the image sequence that move with the tissue and capture motion information. It allows for tracking the motion of inner tissue where the region does not have contrast on cine images.
DeepTag~\citep{ye2021deeptag} takes raw 2D tagged images as input and estimates the incremental motion between two consecutive frames using a bi-directional registration network. Then it composes the incremental motion field to estimate motion between any two time frames. Harmonic phase images~\citep{osman1999cardiac} have been found to be more robust to tag fading and imaging artifacts during motion tracking than raw tagged images. DRIMET~\citep{bian2023deep} proposed a simple sinusoidal transformation on the harmonic phase images,  enabling end-to-end training for estimating a 3D dense motion field from tagged-MRI. It also incorporates a Jacobian determinant-based loss that penalizes \textit{symmetrically} for contraction and expansion to estimate a biologically-plaussible incompressible motion field. DRIMET shows promising results in terms of superior registration accuracy, a comparable degree of incompressibility, and faster speed over its traditional iterative-based counterparts~\citep{PVIRA,ilogdemons2011}.

% Echocardiography is a non-invasive and cost-efficient medical imaging procedure that uses ultrasound to create images of the heart. \citep{ta2020semi,ahn2020unsupervised} xxx. 

% 2D vs 3D
Numerous deep learning-based techniques have been devised to estimate 2D motion, and although this may be adequate for certain applications, tracking dense 3D motion is typically necessary or highly desirable when estimating the motion of biological structures.  %The 2D methods require the acquisition of a large number of closely spaced image slices, making them impractical for routine clinical use due to the large amount of time required. 
To address this issue, Meng~\emph{et al.}~\citep{meng2022mulvimotion} integrate features extracted from multi-view 2D cine CMR images captured in both short-axis and long-axis planes to learn a 3D motion field of the heart. The edge map of myocardial wall is used as a shape regularization of the estimated motion field. Alternatively, DRIMET~\citep{bian2023deep} uses sparsely acquired tagged images and interpolates them onto an isotropic grid with a resolution based on the in-plane resolution. This approach is based on the observation that the tag pattern changes slowly in the through-plane direction and therefore will not cause aliasing issues during sampling. By doing so, DRIMET is capable of tracking \textit{dense} 3D motion.

Recent studies have shown that joint learning of segmentation and motion estimation can be mutually beneficial~\citep{qin2018joint,ta2020semi,ahn2020unsupervised}. For instance, Qin~\emph{et al.}~\citep{qin2018joint} employ a dual-branch framework consisting of a segmentation branch and a motion estimation branch to simultaneously estimate motion and segmentation from a sequence of cardiac cine images. During training, a \textit{shared} feature encoder is learned under the premise that joint features can complement both tasks. In contrast, Ta~\emph{et al.}~\citep{ta2020semi} and Ahn~\citep{ahn2020unsupervised} adopt a task-level approach to jointly tackle motion estimation and segmentation in the context of estimating cardiac motion from echocardiography. Specifically, they warp the segmentation (of one time frame) using the estimated motion field and regularize the motion field by incorporating shape information obtained from the segmentation. This approach differs from previous studies which couple motion estimation and segmentation at the feature-level, and may offer a novel perspective on joint learning of these tasks.

In addition to MRIs and echocardiography, numerous deep learning-based algorithms have been developed for motion estimation with 4D-CT~\citep{fu2020lungregnet,ho2023unsupervised,wolterink2022implicit,fechter2020one,hering2021cnn,ji2022one}. 4D-CT imaging captures images at different phases of respiratory or cardiac cycles, providing valuable insights for lung imaging applications, including radiation therapy planning and lung function assessment. DIR-LAB~\citep{castillo2009framework} is a widely-used dataset, containing 4D CT images of ten patients, to evaluate 4D-CT registration techniques, with the aim of registering inspiration images to expiration images. This task is challenging due to the superimposed motion of the heart and lungs, which is larger in scale than the small lung structures being studied.

LungRegNet~\citep{fu2020lungregnet} trains two separate networks to handle large lung motion. One network predicts large motion on a coarse scale, and the other network takes the coarsely warped image and fixed image as input to predict fine motion. In addition to similarity and smoothness losses, an adversarial loss is applied as extra regularization to prevent unrealistic deformed images. Hering \emph{et al.}~\citep{hering2021cnn} employs a coarse-to-fine multi-level optimization strategy. The deformations of coarse levels provide an initial guess for subsequent finer levels. Networks are trained progressively, with each handling one level and initialized with parameters from the previous level. It incorporates a penalty for volume change and utilizes an $l2$ loss function to match corresponding keypoints that are automatically detected. Ho \emph{et al.}~\citep{ho2023unsupervised} applied cycle-consistent training~\citep{kuang2019cycle} to reduce foldings using two networks. After the first network's forward pass, the warped and moving images are sent to the second network to predict inverse deformation, with a similarity loss applied to maximize the similarity between the moving images and inversely-deformed moving images. IDIR~\citep{wolterink2022implicit} use a multi-layer perceptron to represent the transformation function of coordinates and demonstrate the ability to incorporate the Jacobian regularizer, hyperelastic regularizer~\citep{burger2013hyperelastic}, and bending energy~\citep{rueckert1999nonrigid} into the framework. The resulting deformation is void of foldings and achieves a mean target registration error (TRE) of 1.07 mm on DIR-LAB datasets. However, this method requires more time compared to CNN-based approaches, prompting researchers to consider acceleration as a potential future direction. 

In order to accurately register previously unseen images outside of training datasets, the application of one-shot learning has been employed for the estimation of lung motion~\citep{fechter2020one,ji2022one}. Fechter \emph{et al.}~\citep{fechter2020one} concatenated images captured at different phases in the channel dimension in order to leverage temporal information. To minimize memory requirements, they partitioned images into non-overlapping patches and applied a boundary smoothness constraint on the transitions between patches. Additionally, they utilized a coarse-to-fine approach by constructing an image pyramid, where the estimated vector fields of finer scales were added to the upsampled vector fields of coarser scales. The proposed method showed a competitive performance without the need for training in advance. 

\subsection{2D-3D Registration}
Recent progress in the field of interventional procedures for invasive treatment protocols has been associated with high precision in surgeries performed at a reasonable cost~\citep{pfandler2019technical, dlouhy2014surgical}. In these procedures, 2D-3D registration plays a significant role in determining the spatial relationship between the 3D anatomical structures and 2D images, such as X-Ray fluoroscopic images, ultrasound image frames, or endoscopic images. 2D-3D medical image registration primarily involves registering 2D interventional images to 3D pre-operative CT/MR images, \emph{i.e.}, to obtain the 3D geometric transformation that aligns with the 2D view available. 
Conventional 2D-3D registration methods involve iterative optimization methods with similarity metrics \citep{maes1997multimodality}~based on image intensity as the objective function. 
Due to the sparsity of spatial information derived from 2D images, the problem is non-convex, which may lead to convergence at a local minimum if the initial estimate is not sufficiently close to the correct one. 2D-3D registration is a problem with a minimum of six degrees of freedom which may also lead to registration ambiguity as the spatial information along each projection line is compressed to a single point in the 2D plane. This high-dimensional optimization problem increases the difficulty of determining the parameters associated with the depth of anatomical features in the 3D volume. 
%Alternatively, since deep-learning methods do not require explicit functional mappings, it has gained much popularity for this application\citep{unberath2021impact}. 
Alternatively, deep-learning-based methods have gained popularity for this application as they do not require explicit functional mappings~\citep{unberath2021impact}.
In this discussion, we briefly highlight recent advancements in 2D-3D registration, while directing interested readers to \citep{unberath2021impact} for a comprehensive review of the influence of various learning-based methods in this area.

Common 2D-3D registration applications and examples include registration of 2D fluoroscopic/angiography images to 3D CT/MR images of pelvic, lung, or brain regions~\citep{Gu2020ExtendedCR, liao2019multiview, gao2020generalizing, gao2020fiducial, jaganathan2023self, huang2022novel}, registering endoscopy images to CT/MR images~\citep{liu2020extremely, bobrow2022colonoscopy}, and registering 2D Ultrasound (US) frames to 3D MR images to facilitate interventional procedures, such as liver tumor ablation \citep{Wei2021a} or prostate cancer biopsy\citep{guo2022ultrasound}. 

In~\citep{Gu2020ExtendedCR, Wei2021a, huang2022novel}, the 2D-3D registration problem was modeled as a regression learning problem where the network is trained to directly predict the desired geometric parameters. These models are trained by completely relying on the data, \emph{i.e.}, it has little to no tie to the actual image formation physics involved. Specifically, in \citep{Gu2020ExtendedCR} a 2D X-Ray image is registered to a 3D CT volume using a ConvNet, which takes the X-Ray image and a digitally reconstructed radiograph (DRR) from the CT volume at some known pose as input. 
The ConvNet regresses a geodesic loss function over the geometric parameter space to estimate the relative pose between the fixed X-ray image and the DRR from the CT volume without the need for accurate pose initialization.

In~\citep{Wei2021a}, Wei~\emph{et al.} propose a two-step registration process to determine the position and orientation of the ultrasound plane in the 3D MR volume data. In the first step, a ResNet-18 network is employed to determine the US probe orientation. Following this, a U-Net is used to regress a weighted dice loss function, which facilitates the determination of the orientation and position of the corresponding XY plane in the resampled 3D MR volume associated with the US frame.
In \citep{huang2022novel}, Huang~\emph{et al.} also implemented a two-step registration process for aligning 3D MR vessel wall images (VWI) with 2D Digital Subtraction Angiography (DSA) images. This approach encompasses a ConvNet regressor~\citep{miao2016cnn} that estimates the initial pose, followed by an instance-based centroid alignment, which serves to further minimize parameter estimation errors between the images.

As an alternative to formulating registration as a regression problem that necessitates ground truth transformation parameters, several recent studies~\citep{liao2019multiview, gao2020generalizing, gao2020fiducial, jaganathan2023self, guo2022ultrasound} have explored framing it as an unsupervised optimization problem. In such a formulation, the cost function is determined by a similarity metric measured between the transformed and fixed images. Liao~\emph{et al.}~\citep{liao2019multiview} trained a network to track a set of points of interest (POIs) derived from the 3D CT volume in the 2D DRR and in the multi-view fluoroscopic 2D images (used as fixed images), enabling the network to learn the spatial correspondences between the POIs. In this method, a Siamese U-Net architecture is employed to extract features from the DRRs and fixed images, subsequently tracking the POIs within the extracted features. A triangulation layer is incorporated to pinpoint the locations of the tracked POIs within the fixed image in 3D space. Finally, the geometric transformation between the estimated locations of POIs derived from the fixed image and their true positions is determined analytically. In \citep{gao2020generalizing}, Gao~\emph{et al.} proposed a novel differential volume rendering transformer network combined with a feature extraction encoder to approximate the image similarity metric in a manner that renders the geometric parameter estimation as a convex problem with respect to the pose parameters. %In ~\citep{jaganathan2023self} Jaganath~\emph{et al.} propose a novel unsupervised network to perform annotation-free 2D-3D registration between 2D X-Ray projection image and CT volumes. %The network is a combination of Barlow Twins network, adversarial feature discriminator and DL-based registration network. 

The examples and applications discussed thus far have primarily focused on rigid 2D-3D registration. However, non-rigid 2D-3D registration is essential in certain applications, such as cephalometry~\citep{Li2020Nonrigid} and lung tumor tracking in radiation therapy~\citep{foote2019real,dong20232d}. Cephalometry, for instance, involves formulating the problem as deformed 2D-3D registration with the objective of generating a 3D volumetric image from a 2D X-ray image using a 3D skull atlas. Li~\emph{et al.}~\citep{Li2020Nonrigid} developed a convolutional encoder that uniquely codes the cephalogram image into a volumetric image. The network is trained by minimizing the NCC between the synthesized DRR originating from the volumetric image and the 2D cephalogram. 

Numerous deep learning-based models and metrics have been developed to improve the performance of 2D-3D registration in specific applications, although these methods are specialized and not as versatile as traditional optimization methods. Nonetheless, Machine Learning/Deep Learning has been instrumental in tackling the persistent challenges associated with algorithmic approaches. These techniques have tackled a narrow optimal range of parameters, while also decreasing registration ambiguity. CNN-based approaches are also comparably fast. These factors encourage users to further improve learning-based 2D-3D registration pipeline. 
