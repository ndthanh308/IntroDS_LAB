\begin{table*}[!t]
\caption{A compilation of unsupervised deformable image registration models (models are listed in alphabetical order). The table summarizes the models' choices of similarity and auxiliary loss functions, regularization techniques, accuracy measures, and regularity measures.}
\label{table:unsup_list}
\centering
\fontsize{6}{7.5}\selectfont 
 \begin{tabular}{ l ? c | c | c | c | c | c ? c | c ? c | c | c | c | c ? c | c | c | c | c ? c | c | c | c }
 \Xhline{1pt}
 &\multicolumn{6}{c?}{Similarity Loss} &\multicolumn{2}{c?}{Aux. Loss} &\multicolumn{5}{c?}{Regularizer}&\multicolumn{5}{c?}{Accuracy Measure}&\multicolumn{4}{c}{Regularity Measure}\\
 &\rotatebox[origin=c]{90}{MSE} & \rotatebox[origin=c]{90}{NCC} & \rotatebox[origin=c]{90}{Correlation} & \rotatebox[origin=c]{90}{NGF} & \rotatebox[origin=c]{90}{MI} & \rotatebox[origin=c]{90}{MIND-SSC}& \rotatebox[origin=c]{90}{Anatomy}& \rotatebox[origin=c]{90}{Keypoint} & \rotatebox[origin=c]{90}{Diffusion} & \rotatebox[origin=c]{90}{Curvature} & \rotatebox[origin=c]{90}{Bending} & \rotatebox[origin=c]{90}{Jacobian} & \rotatebox[origin=c]{90}{Consistency} & \rotatebox[origin=c]{90}{TRE}& \rotatebox[origin=c]{90}{MSE}& \rotatebox[origin=c]{90}{SSIM} & \rotatebox[origin=c]{90}{Dice} & \rotatebox[origin=c]{90}{HdD} & \rotatebox[origin=c]{90}{$\%\text{ of }|J_\phi\leq0$}& \rotatebox[origin=c]{90}{$\#\text{ of }|J_\phi|\leq0$}& \rotatebox[origin=c]{90}{std.$(|J_\phi|)$}& \rotatebox[origin=c]{90}{$|\nabla J_\phi|$}\\
 
 \Xhline{1pt}%\cline{1-4}\cline{6-9}
 \rowcolor{Gray}
 ADMIR~\citep{tang2020admir}                   & &$\sbullet[1.5]$& & & & & & &$\sbullet[1.5]$& & & & &$\sbullet[1.5]$& & &$\sbullet[1.5]$&$\sbullet[1.5]$& & & &\\
 
 Attention-Reg~\citep{song2022cross}           & & & & & & & $\sbullet[1.5]$& &$\sbullet[1.5]$& & & & &$\sbullet[1.5]$& & &$\sbullet[1.5]$& & & &$\sbullet[1.5]$&\\
 \rowcolor{Gray}
 BIRNet~\citep{fan2019birnet}                  &$\sbullet[1.5]$& & & & & & & & & & & & & & & &$\sbullet[1.5]$& & & & &\\
 
 CondLapIRN~\citep{mok2021conditional}         &  &$\sbullet[1.5]$ & & & & & & & $\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& &$\sbullet[1.5]$& \\
 \rowcolor{Gray}
 CycleMorph~\citep{kim2021cyclemorph}          &$\sbullet[1.5]$&$\sbullet[1.5]$ & & & & & & & $\sbullet[1.5]$& & & &$\sbullet[1.5]$& &$\sbullet[1.5]$&$\sbullet[1.5]$&$\sbullet[1.5]$& &$\sbullet[1.5]$& & &\\
 
 de Vos~\emph{et al.}~\citep{de2020mutual}            & & & & &$\sbullet[1.5]$& & & & & &$\sbullet[1.5]$& & & & & &$\sbullet[1.5]$&$\sbullet[1.5]$&$\sbullet[1.5]$& & &\\ 
 \rowcolor{Gray}
 Deformer~\citep{chen2022deformer}             & &$\sbullet[1.5]$& & & & & & &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$ & &$\sbullet[1.5]$&\\
 
 DiffuseMorph~\citep{kim2022diffusemorph}      & &$\sbullet[1.5]$& & & & & & & $\sbullet[1.5]$& & & & & &$\sbullet[1.5]$&$\sbullet[1.5]$&$\sbullet[1.5]$& &$\sbullet[1.5]$& & &\\
 \rowcolor{Gray}
 DIRNet~\citep{de2017end}                      & & $\sbullet[1.5]$& & & & & & & & & & & & & & &$\sbullet[1.5]$& & & & & \\
 
 DLIR~\citep{de2019deep}                       & &$\sbullet[1.5]$& & & & & & & & &$\sbullet[1.5]$& & & & & &$\sbullet[1.5]$&$\sbullet[1.5]$&$\sbullet[1.5]$& &$\sbullet[1.5]$&\\
 \rowcolor{Gray}
 DNVF~\citep{han2023diffeomorphic}             & &$\sbullet[1.5]$& & & & & & & $\sbullet[1.5]$& & &$\sbullet[1.5]$& & & &$\sbullet[1.5]$&$\sbullet[1.5]$& &$\sbullet[1.5]$& & &\\
 
 DTN~\citep{zhang2021learning}                 &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& & & &$\sbullet[1.5]$&\\
 \rowcolor{Gray}
 Dual-PRNet~\citep{hu2019dual}                 &  &$\sbullet[1.5]$& & & & & & & $\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$ & & & & & \\
 
 Dual-PRNet++~\citep{kang2022dual}              &  &$\sbullet[1.5]$& & & & & & & $\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$&$\sbullet[1.5]$&$\sbullet[1.5]$& & & \\
 \rowcolor{Gray}
 FAIM~\citep{kuang2019faim}                    & &$\sbullet[1.5]$& & & & & & &$\sbullet[1.5]$& & &$\sbullet[1.5]$& & & & &$\sbullet[1.5]$& & &$\sbullet[1.5]$& &\\
 
 Fan~\emph{et al.}~\citep{fan2019adversarial}  & & & & & & & & &$\sbullet[1.5]$& & & & & & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& &\\
 \rowcolor{Gray}
 Fourier-Net~\citep{jia2022fourier}            &$\sbullet[1.5]$&$\sbullet[1.5]$& & & & & & &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$&$\sbullet[1.5]$&$\sbullet[1.5]$& & &\\
 
 GraformerDIR~\citep{yang2022graformerdir}     & &$\sbullet[1.5]$& & & & & & &$\sbullet[1.5]$& & &$\sbullet[1.5]$& & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & &\\
 \rowcolor{Gray}
 Han~\emph{et al.}~\citep{han2022deformable}   & &$\sbullet[1.5]$& & & & & & &$\sbullet[1.5]$& & & & &$\sbullet[1.5]$& & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & & \\
 
 Hering~\emph{et al.}~\citep{hering2021cnn}           &  & & &$\sbullet[1.5]$& & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & & & & & &$\sbullet[1.5]$&$\sbullet[1.5]$&$\sbullet[1.5]$& & & \\
 \rowcolor{Gray}
 HyperMorph~\citep{hoopes2022hyper}       &$\sbullet[1.5]$&$\sbullet[1.5]$& & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & & & &\\ 

 im2grid~\citep{liu2022coordinate}             &$\sbullet[1.5]$& & & & & & & & $\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$&$\sbullet[1.5]$& &\\
 \rowcolor{Gray}
 Krebs~\emph{et al.}~\citep{krebs2019learning}        &  &$\sbullet[1.5]$& & & & & & & $\sbullet[1.5]$& & & & & & $\sbullet[1.5]$& &$\sbullet[1.5]$ &$\sbullet[1.5]$& & & &$\sbullet[1.5]$ \\

 LapIRN~\citep{mok2020large}                   &  &$\sbullet[1.5]$ & & & & & & & $\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& & &$\sbullet[1.5]$&$\sbullet[1.5]$&\\
 \rowcolor{Gray}
 LKU-Net~\citep{jia2022u}                      & &$\sbullet[1.5]$& & & & & $\sbullet[1.5]$& & $\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$&$\sbullet[1.5]$&$\sbullet[1.5]$ & &$\sbullet[1.5]$&\\
 
 
 Li~\emph{et al.}~\citep{li2018non}            & &$\sbullet[1.5]$& & & & & & &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& & & & &\\
 \rowcolor{Gray}
 Liu~\emph{et al.}~\citep{liu2019probabilistic}&$\sbullet[1.5]$& & & & & & & & & & & & & & & &$\sbullet[1.5]$& & & & &\\
 
 MIDIR~\citep{qiu2021learning}                 &  & & & &$\sbullet[1.5]$ & & & & $\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$ & &$\sbullet[1.5]$& & & $\sbullet[1.5]$ \\
 \rowcolor{Gray}
 MS-DIRNet~\citep{lei20204d}                   & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & &$\sbullet[1.5]$&$\sbullet[1.5]$& & & & & & &\\
 
 MS-ODENet~\citep{xu2021multi}                 &$\sbullet[1.5]$& & & & & & & & $\sbullet[1.5]$& & & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & & & &\\
 \rowcolor{Gray}
 NODEO~\citep{wu2022nodeo}                     & &$\sbullet[1.5]$& & & & & & & $\sbullet[1.5]$& & &$\sbullet[1.5]$& & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & &\\
 
 PDD-Net 2.5D~\citep{heinrich2020highly}       & & & & & &$\sbullet[1.5]$&$\sbullet[1.5]$& & $\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& & & &$\sbullet[1.5]$&\\
 \rowcolor{Gray}
 PDD-Net 3D~\citep{heinrich2019closing}        & & & & & &$\sbullet[1.5]$&$\sbullet[1.5]$& & $\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& & & &$\sbullet[1.5]$&\\
 
 PC-SwinMorph~\citep{liu2022pc}                & &$\sbullet[1.5]$& & & & & & & $\sbullet[1.5]$& & & &$\sbullet[1.5]$& & & &$\sbullet[1.5]$& & & & &\\
 \rowcolor{Gray}
 SDHNet~\citep{zhou2023self}                   & &$\sbullet[1.5]$& & & & & & & $\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$&$\sbullet[1.5]$&$\sbullet[1.5]$& & & \\

 Shao~\emph{et al.}~\citep{shao2022multi}      &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& & & & & &$\sbullet[1.5]$& & & & &$\sbullet[1.5]$&$\sbullet[1.5]$&\\
 \rowcolor{Gray}
 SVF-R2Net~\citep{joshi2022diffeomorphic}      &$\sbullet[1.5]$& & & & & & & & & & &$\sbullet[1.5]$& & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & &$\sbullet[1.5]$& &\\

 SYMNet~\citep{mok2020fast}                    &  &$\sbullet[1.5]$ & & & & & & & $\sbullet[1.5]$& & &$\sbullet[1.5]$&$\sbullet[1.5]$& & & &$\sbullet[1.5]$& & &$\sbullet[1.5]$ & &\\
 \rowcolor{Gray}
 SymTrans~\citep{ma2022symmetric}              &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& & &$\sbullet[1.5]$& &\\

 SynthMorph~\citep{hoffmann2021synthmorph}     & & & & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & &\\
 \rowcolor{Gray}
 TM-DCA~\citep{chen2023deform}                 & &$\sbullet[1.5]$& & & & & $\sbullet[1.5]$& & $\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& &$\sbullet[1.5]$&\\

 TM-TVF~\citep{chen2022unsupervised}   &$\sbullet[1.5]$&$\sbullet[1.5]$& & & & & $\sbullet[1.5]$& & $\sbullet[1.5]$& & & & & & &$\sbullet[1.5]$&$\sbullet[1.5]$&$\sbullet[1.5]$ &$\sbullet[1.5]$& &$\sbullet[1.5]$&\\
 \rowcolor{Gray}
 TransMorph~\citep{chen2022transmorph}         &$\sbullet[1.5]$&$\sbullet[1.5]$& & & & & $\sbullet[1.5]$& & $\sbullet[1.5]$& &$\sbullet[1.5]$& & & & &$\sbullet[1.5]$&$\sbullet[1.5]$& &$\sbullet[1.5]$ & & &\\

 ViT-V-Net~\citep{chen2021vitvnet}             &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & &\\
 \rowcolor{Gray}
 VoxelMorph~\citep{balakrishnan2019voxelmorph} &$\sbullet[1.5]$& $\sbullet[1.5]$ & & & & & $\sbullet[1.5]$& &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$&$\sbullet[1.5]$& & \\

 VoxelMorph-diff~\citep{dalca2019unsupervised} &$\sbullet[1.5]$& & & & & &$\sbullet[1.5]$& & $\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$&  &$\sbullet[1.5]$&$\sbullet[1.5]$& & \\
 \rowcolor{Gray}
 VoxelMorph++~\citep{heinrich2022voxelmorph++} & & & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$&$\sbullet[1.5]$& & & & &$\sbullet[1.5]$& & & & & &$\sbullet[1.5]$&$\sbullet[1.5]$&\\
 
 VR-Net~\citep{jia2021learning}                &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$&$\sbullet[1.5]$&$\sbullet[1.5]$& & &$\sbullet[1.5]$\\
 \rowcolor{Gray}
 VTN~\citep{zhao2019unsupervised}              & & &$\sbullet[1.5]$& & & & & & $\sbullet[1.5]$& & &$\sbullet[1.5]$&$\sbullet[1.5]$& & & &$\sbullet[1.5]$ & &$\sbullet[1.5]$& &$\sbullet[1.5]$& \\
 
 XMorpher~\citep{shi2022xmorpher}              & &$\sbullet[1.5]$& & & & & $\sbullet[1.5]$& & $\sbullet[1.5]$& & & &$\sbullet[1.5]$& & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & &\\
 \rowcolor{Gray}
 Zhang~\emph{et al.}~\citep{zhang2020diffeomorphic}   &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& & &$\sbullet[1.5]$& & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & &\\
 
 \Xhline{1pt}
\end{tabular}
\end{table*}

%
% I think we need something like a short intro passage here.
% Not sure if this is sufficient. -AJC
%
Table~\ref{table:unsup_list} provides a compilation of unsupervised DIR models, summarizing the similarity and auxiliary loss functions, as well as other details.
See the text for complete details and discussion.


\subsection{Supervised Learning}
In supervised learning, where the ground truth transformation is used, the loss function is typically easy to define, with the mean square error~(MSE)~\citep{miao2016cnn, krebs2017robust, eppenhof2018deformable, rohe2017svf, cao2017deformable, fan2019birnet}, the equivalent end-point-error (EPE), and mean absolute error (MAE)~\citep{yang2017quicksilver, sokooti2017nonrigid} being the most popular choices.

\subsection{Unsupervised learning}
In unsupervised learning, where there is no ground truth transformation to reference, regularization is usually used to enforce smoothness in the transformation. As a result, the loss function is often similar to the energy function used in traditional methods (\emph{i.e.}, Eqn.~\ref{eqn:energy_func}), which includes an image similarity measure and a transformation regularizer. The following subsections provide a summary of commonly used and recently proposed loss functions for image registration.


\subsection{Similarity Measure}

\noindent\textbf{Mono-modality.} The choice of image similarity measure can vary depending on each specific application. For mono-modal registration, MSE is still a popular choice and has the advantage of having a straightforward probabilistic interpretation of the Gaussian likelihood approximation~\citep{dalca2019unsupervised, chen2022transmorph, kim2021cyclemorph, balakrishnan2019voxelmorph, meng2022enhancing, jia2021learning, liu2022coordinate}.
However, a disadvantage of MSE is that it averages the difference across all voxels in the image, making it sensitive to local intensity variations within the image.
Normalized cross-correlation (NCC) is known to be more robust to local intensity variations and has been found to be superior in brain MR registration applications~\citep{avants2008symmetric}.
NCC has been extended as a loss function for training learning-based models, with the local window computation often being done through convolution operations~\citep{kuang2019faim, chen2022transmorph, kim2021cyclemorph, balakrishnan2019voxelmorph, zhang2018inverse, mok2020fast, mok2020large, mok2021conditional}. One disadvantage of NCC is its higher computing cost in comparison to MSE, which is mainly attributable to the comparatively large convolution kernel size (typically chosen between 5 and 9~\citep{avants2008symmetric, balakrishnan2019voxelmorph, mok2020fast}). The structural similarity index (SSIM)~\citep{wang2004image} has also been demonstrated to be an effective loss function for mono-modal image registration~\citep{chen2020generating, mahapatra2018deformable, sandkuhler2018airlab}. SSIM takes into account luminance, contrast, and structure. It can be thought of as an extension of the NCC, with the structure term in SSIM being the square root of NCC. This allows SSIM to capture more information about the similarity of two images beyond just the degree of correlation between them.

\noindent\textbf{Multi-modality.} For multi-modal applications, traditional methods often use mutual information~(MI)~\citep{viola1997alignment}, correlation ratio~\citep{roche1998correlation}, self-similarity context~(SSC)~\citep{heinrich2013towards}, or normalized gradient fields~(NGF)~\citep{haber2006intensity} as similarity measures. Both MI and correlation ratio evaluate the relationship between the two images by calculating intensity statistics, such as intensity histograms, to measure statistical dependence.
However, the standard method for calculating intensity histograms, which involves counting, is not differentiable, so a Parzen window formulation~\hbox{\citep{thevenaz2000optimization}} is often used to allow the loss to be backpropagated during network training.
Parzen-window-based MI has been employed as a loss function in many multi-modal applications~\citep{qiu2021learning, de2020mutual, nan2020drmime, guo2019multi, hoffmann2021synthmorph}, but it can be relatively difficult to implement and also sensitive to factors such as the number of intensity bins and the smoothness of the Gaussian function.
As far as we are aware, the correlation ratio has not been used in learning-based medical image registration.
It should be noted that these intensity-statistic-based measurements do not take into account local structural information, making them more suitable for rigid/affine registration and less suitable for deformable registration applications~\citep{pluim2000image, heinrich2013towards}.
SSC is another commonly used loss function for multi-modal applications, and it is an improvement on the modality-independent neighborhood descriptor~(MIND)~\citep{heinrich2012mind}.
Both SSC and MIND operate by calculating the descriptor between a voxel and its neighboring voxels within a given image, turning an image of any modality into a feature representation of these descriptors. 
The similarity is determined by summing the absolute differences between the descriptors of the two images.
As SSC and MIND consider local structural information, they are not limited in the same way as MI or correlation ratio, making them more useful for multi-modal deformable registration~\citep{hansen2021graphregnet, mok2021conditional2, yang2020unsupervised, xu2020adversarial, blendowski2021weakly}.
NGF compares images by focusing on the intensity changes, or edges, in the images. The similarity between the two images is determined by the presence of intensity changes at the same locations, regardless of the modalities of the images being compared.
NGF was originally developed for multi-modal applications like brain MR T1-to-T2 and PET-to-CT~\citep{haber2006intensity}.
However, it is now mostly used in learning-based registration models for lung CT registration~\citep{hering2019mlvirnet, hering2021cnn, mok2021conditional2}. 
This is because the complex structure of the lung, including bronchi, fissures, and vessels, can hinder accurate registration~\citep{hering2021cnn}. 
NGF focuses on edges rather than intensity values, making it a more suitable measure for this purpose.

\noindent\textbf{Recent Advancements.} There have been many efforts to improve upon or propose new loss functions due to some limitations of the aforementioned similarity measures.
\citet{terpstra2022loss}~showed that the $\ell^2$ loss (equivalent to MSE) is not optimal for MRI applications, because it does not fully leverage the magnitude and phase information contained in the complex data of MRI. 
The authors introduced $\bot$-loss, a loss function that is based on the polar representation of complex numbers and promotes symmetry in the overall loss landscape.
They demonstrated that a network trained with a combination of $\bot$-loss and $\ell^2$ loss outperforms a network trained with $\ell^2$ loss alone in terms of registration performance.
\citet{czolbe2021semantic}~leveraged a ConvNet feature extractor to obtain image features from the deformed and fixed images, and then computed the NCC between these features as a similarity measure.
The benefit of this approach is that the features produced by the ConvNet feature extractor have less noise, resulting in a more consistent similarity measure in areas with noise which leads to a smoother transformation.
\citet{haskins2019learning}~were the first to propose using a ConvNet to learn a similarity measure for image registration.
However, this method relies on having ground truth target registration error for the training dataset to learn such a similarity measure.
\citet{grzech2022variational}~went one step further and introduced a technique for learning a similarity measure using a variational Bayesian method. The method involves initializing the convolution kernels in the network architecture to model MSE and NCC, and then using variational inference to learn a similarity measure that optimizes the likelihood of the images in the dataset when aligning them to the atlas. Building on the success of adversarial networks in computer vision~\citep{makhzani2015adversarial, goodfellow2020generative}, researchers have developed a number of techniques for image registration that leverage adversarial training~\citep{fan2019adversarial, mahapatra2020training, luo2021deformable}. These methods can be used standalone or in conjunction with a traditional similarity measure.

\subsection{Deformation Regularizer}
% greer2021icon tian2022gradicon
\label{sec:def_reg}
A deformation regularizer, as the terminology implies, is used for DIR, with its usage being not necessary for rigid/affine transformations.
For DIR algorithms, producing smooth deformations is not only a desirable property but a necessary requirement: while diffeomorphic transformations may not be required for certain applications, smoothness remains imperative in almost all cases to avoid trivial solutions such as rearranging voxels~\citep{rohlfing2011image}, with which an almost perfect similarity measure can be achieved but result in unrealistic transformation~(also see Section~\ref{sec:Eval_Metric}).
The regularizer can be considered as a prior in a maximum a posteriori~(MAP) framework, while the similarity measure acts as the data likelihood (\emph{e.g.}, in the case of MSE, the data likelihood becomes a Gaussian likelihood). The diffusion regularizer is a commonly employed deformation regularizer, as demonstrated by its frequent appearance in Table~\ref{table:unsup_list}. This regularization computes the squared $\ell^2$-norm of the gradients of the displacement field, effectively penalizing the disparities between adjacent displacements. Other alternatives for regularization include using the $\ell^1$-norm instead of the $\ell^2$-norm to impart equal penalties on the neighboring disparities, or penalizing the second derivative of the displacements, commonly referred to as bending energy~\citep{rueckert1999nonrigid}. It is important to note that since bending energy and curvature-based regularizers penalize the second derivatives, thereby zeroing out any affine contributions, pre-affine alignment prior to the deformable registration step may not be necessary, as demonstrated in \citep{ding2022aladdin, fischer2003curvature}.
These conventional regularizers enforce an isotropic regularization on the displacement field~\citep{pace2013locally}.
As a result, they discourage discontinuities in the displacements in applications where sliding motion may occur in organs, such as registering exhale and inhale CT scans of the lung.
Historically, various improvements have been made to address this issue, including the isotropic Total Variation~(TV) regularization~\citep{vishnevskiy2016isotropic}, anisotropic diffusion regularization~\citep{pace2013locally}, and adaptive bilateral filtering-based regularization~\citep{papiez2014implicit}.
However, these regularization techniques have not been widely adopted in learning-based image registration.

\noindent\textbf{Recent Advancements.} Enforcing spatial smoothness alone is insufficient to ensure the regularity of the transformations. A different strategy is to penalize the ``folding'' of voxels directly during training, in addition to applying the aforementioned regularizers to enforce smoothness in the deformation.
These foldings can be evaluated using local Jacobian determinants, where the magnitude of the Jacobian determinant indicates if the volume is expanding or shrinking near the voxel location.
A non-positive Jacobian determinant represents a locally non-invertible transformation. Several regularization methods based on local Jacobian determinants have been proposed to penalize such transformations~\citep{kuang2019faim, mok2020fast}.
Meanwhile, with the advent of deep learning, new methods have emerged that leverage the deep learning of deformation regularization from data.
One such method by  \citet{niethammer2019metric},~introduced a method that learns a spatially-varying deformation regularization using training data.
Spatially-varying regularization offers the advantage of accommodating variations in deformation that may be required for different regions within an image, such as the movement of the lungs in relation to other organs (\emph{e.g.}, rib cage) due to respiratory processes.
The technique proposed by Niethammer~\emph{et al.} involves training a registration network to produce not only a deformation field but also a set of weight maps, each of which corresponds to the weight of a Gaussian smoothing kernel in a multi-Gaussian kernel configuration.
The weighted multi-Gaussian kernel is then applied to the deformation field via convolution. To further impose spatial smoothness, an optimal mass transport~(OMT) loss function was introduced to encourage the network to assign larger weights to Gaussian kernels with larger variances.
While this method was developed for a time-stationary velocity field setting, \citet{shen2019region} later expanded upon it by incorporating it into a time-varying velocity field setting.
In this setup, a different set of weight maps are produced for each time point.
More recently, \citet{chen2023spr}~introduced a weighted diffusion regularizer that applies spatially-varying regularization to the deformation field.
The neural network generates a weight volume, assigning a unique regularization weight to each voxel and thus allows for spatially-varying levels of regularization strength.
As the diffusion regularizer is related to Gaussian smoothing, using spatially-varying strengths of diffusion regularization can be considered equivalent to employing a multi-Gaussian kernel, as originally proposed by~\citet{niethammer2019metric}.
This is because the combination of multiple Gaussians still results in a Gaussian. To promote the overall smoothness of the deformation, they further applied a log loss to the weight volume, which encourages the maximum regularization strength when possible. 
In a different approach, \citet{wang2022deep}~employed a regression network to learn the optimal regularization parameter for an optimization-based method, specifically Flash~\citep{zhang2019fast}.
Flash is a geodesic shooting method in the Fourier space that requires only the initial velocity field to compute the time-dependent transformation.
Wang~\emph{et al.} generated ground truth optimal regularization parameters by assuming the prior of the initial velocity field given the regularization parameter as a multivariate Gaussian distribution.
Using gradient descent, they obtained the optimal regularization parameter for each image pair through MAP estimation.
A ConvNet regression encoder then estimates the optimal regularization parameter based on the image pair. 
This approach achieved registration performance comparable to Flash while significantly improving runtime and memory efficiency.
Alternatively, \citet{laves2019deformable}~were inspired by the deep image prior~\citep{ulyanov2018deep}. 
They used a randomly initialized ConvNet as a regularization prior.
They then fed a random image (\emph{i.e.}, a noise image) as input and the network gradually transformed it into a smooth deformation field through iterative optimization.
The deep image prior provided by the ConvNet enables the network to produce a smooth deformation in the early iterations, then gradually adds non-smooth high-frequency deformations.
As a result, early stopping is used for the network to generate a smooth deformation field without the need for explicitly encouraging smoothness in the loss function.

Transformations can also be implicitly regularized by imposing invertibility constraints.
This is achieved by using a symmetric consistency loss or cycle consistency loss.
Symmetric consistency typically uses a single DNN to output both the forward and reverse deformation fields, which transform the moving image to the fixed image and vice versa, respectively.
The similarity between the warped image and the target image is then calculated and backpropagated to update the network~\citep{mok2020fast, liu2022pc}.
Alternatively, a consistency loss can be calculated by composing the network-generated forward and backward deformation fields, and then comparing the outcome with the identity transformation~\citep{greer2021icon,tian2022gradicon}.
The underlying concept is that, theoretically, an invertible mapping should cancel itself when composed with its inverse.
Such an approach by itself imposes invertibility but does not explicitly enforce spatial smoothness over the deformation field.
\citet{greer2021icon}~demonstrated that incorporating such a loss within a DNN framework implicitly imposes spatial regularity on the deformation field without necessitating an additional regularizer to enforce smoothness.
The authors showed that the errors of the DNN in computing the inverse, combined with the implicit bias of DNN favoring more regular outputs, enable such a consistency loss to entail a $H^1-$ or Sobolev-type regularization over the deformation field, thereby implicitly enforcing spatial smoothness.
Later, \citet{tian2022gradicon}~expanded on this regularizer and proposed to regularize deviations of the Jacobian of the composition from the identity matrix.
This improved regularizer led to faster convergence while offering greater flexibility, while maintaining an approximated diffeomorphic transformation.

On the other hand, cycle consistency employs two identical networks, where the first network generates a forward deformation field that deforms the moving image and the second network produces a reverse field that aims to warp the deformed image back to the original moving image~\citep{zhao2019unsupervised, kuang2019cycle, kim2021cyclemorph}.
Both consistency losses have been shown to improve the registration performance and provide regularization to the deformation field. However, since this regularization is not explicitly applied to the deformation fields, a separate deformation regularizer is often required in addition to the consistency loss.

\subsection{Auxiliary Anatomical Information}
\label{ss:anatomical_info}
The overlap of anatomical label maps of the fixed and transformed moving images is a widely used evaluation metric for image registration.
Hence, to improve registration performance on this metric, learning-based methods often incorporate an anatomy loss in their network training.
Various loss functions used in image segmentation tasks, such as Dice loss, cross-entropy, and focal loss (see \citet{ma2021loss}~for a comprehensive review of such loss functions), can be borrowed as the choice of anatomy loss.
Despite the availability of different loss functions, Dice loss remains the most commonly used loss function in learning-based image registration, as evidenced by Table~\ref{table:unsup_list}.
This is likely because Dice loss is confined within the range of $[0,1]$, like NCC, which makes it easier to adjust hyperparameters when used in conjunction with NCC.

When anatomical keypoints are present in both the moving and fixed images, the transformation generated by the DNN can be applied to the keypoints of the moving image.
The resulting transformed keypoints can then be compared with the keypoints of the fixed image to create a loss.
This keypoint supervision has been utilized in optimization-based registration methods to improve performance, as demonstrated in a number of studies~ \citep{ehrhardt2010automatic, polzin2013combining, ruhaak2017estimation, heinrich2015estimating, fischer2003combination}.
\citet{hering2021cnn}~were the first to incorporate keypoint supervision into a DNN framework by comparing the MSE between the transformed and target keypoints, which resulted in a substantial improvement in the target registration error of the keypoint.
Subsequently, \citep{heinrich2022voxelmorph++}~confirmed the superiority of keypoint supervision on multiple benchmark datasets in their work.
It is worth mentioning that the keypoints can be generated automatically before or during the training stage without manual labeling using automatic keypoint detection algorithms~\citep{heinrich2015estimating, ruhaak2017estimation, polzin2013combining}, making it straightforward to integrate into most learning-based registration frameworks.

The combination of anatomy loss and deformation regularization without an intensity-based similarity measure is also common, and in these cases, the anatomy loss serves as a modality-independent similarity measure~\citep{hu2018weakly, song2022cross, blendowski2020multimodal}.
However, the drawback of using anatomy loss without a similarity measure is clear: it does not penalize deformations in areas where anatomical labels are missing or ambiguous.
Thus, to achieve accurate and realistic deformations, the anatomical labels should be as detailed as possible, ideally with a unique label for each organ or structure.
However, obtaining such detailed labels is often challenging as anatomical label maps in medical imaging are usually manually delineated, which is a time-consuming and expensive process.
