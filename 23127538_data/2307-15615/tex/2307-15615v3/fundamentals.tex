Image registration aims to estimate the optimal coordinate transformation that minimizes an energy function of the form:
\begin{equation}
\label{eqn:energy_func}
    \hat{\phi} = \operatorname*{\argmin}_{\phi}E(I_f, I_m\circ\phi) + \lambda R(\phi),
\end{equation}
where $I_f$ and $I_m$ denote the fixed and moving image, respectively, $\phi$ represents the deformation field that maps $I_m$ to $I_f$, and $R$ is a functional of $\phi$.
The first term in the energy function measures the image similarity between the fixed image and the transformed moving image. The second term enforces regularization on the deformation field, with $\lambda$ being a hyperparameter that determines the trade-off between image similarity and deformation field regularity. 
The purpose of the image similarity measure is to quantify the discrepancy between the fixed image and the transformed moving image. The regularization term is typically used in DIR, as it allows for the integration of prior knowledge about the desired characteristics of the deformation field, such as spatial smoothness. Moreover, regularization prevents the deformation field from exhibiting physically implausible behaviors, such as ``folding'' or rearranging of voxels~\citep{rohlfing2011image}.
This is particularly important for medical images because such unrealistic behavior does not accurately reflect the way that organs deform in reality and may lead to a misinterpretation of the registration results. Regularization is often not required for rigid/affine registration because the deformation field is guaranteed to be spatially uniform.
% and does not exhibit the unrealistic behavior seen in DIR.

% Figure environment removed

\subsection{Supervised vs. Unsupervised Learning}
Learning-based registration methods can be broadly categorized as supervised and unsupervised.
In the machine learning paradigm, supervised learning typically refers to the use of extrinsic information during learning (such as labels) whereas unsupervised methods are concerned with discovering properties intrinsic to the data.
Both supervised and unsupervised learning-based registration methods require a training stage that uses pairs of inputs and their corresponding target outputs.
Supervised registration methods use ground truth transformations as target output during the training process.
Unsupervised methods refer to those that do not require ground truth transformations.
Yet, methods that employ landmark correspondences or anatomical label maps during their training phase are still categorized under supervised learning. This is because landmark correspondences are a sparse representation of the ground truth transformations, and matching label maps act as a surrogate for evaluating registration performance. When this extrinsic information is used alongside the image data to aim learning, these methods are referred to as semi-supervised.
In certain contexts, the term "unsupervised" might be misleading. A more precise term could be ``self-supervised'' to underscore the training aspect of deep learning. 
However, for the purposes of clarity and consistency in this discussion, we will use conventional terminology and refer to methods that do not require supervision from extrinsic information as unsupervised.

During the early stages of development, the majority of learning-based registration methods were supervised.
The ground truth transformations required for the training process are typically generated using traditional registration methods, such as \citep{yang2017quicksilver, rohe2017svf, cao2017deformable, hu2018weakly, fan2019birnet}.
However, generating ground-truth transformations this way is a time-consuming process, which is a notable drawback of such methods.
In addition, since these networks are trained to mimic the function of traditional methods, their registration performance may not surpass that of the methods they are based on.
In some cases, post-processing of the deformation fields may be required to further improve registration accuracy~\citep{yang2017quicksilver}.
Alternatively, artificial deformations can also be used as ground truth transformations in certain cases~\citep{miao2016cnn, krebs2017robust, sokooti2017nonrigid, eppenhof2018pulmonary, eppenhof2018deformable}.

More recently, the introduction of spatial transformer networks~\citep{jaderberg2015spatial} has led to a shift towards developing unsupervised methods that do not rely on ground-truth transformation~\citep{vos2017end, li2018non, de2019deep, balakrishnan2019voxelmorph, dalca2019unsupervised, mok2020fast, mok2020large, mok2021conditional, kim2021cyclemorph, chen2022transmorph, chen2021vitvnet}. These methods use the difference between the deformed moving image and the fixed image to update the network, enabling end-to-end training. By removing the reliance on ground truth transformation, these methods offer greater flexibility in modeling different properties of the deformation fields (\emph{e.g.}, smoothness, invertibility).

\subsection{Paradigm for Learning-based Registration}
Recent progress in the field of learning-based medical image registration has been focusing on exploring different ways to improve registration accuracy, such as through modifications to network architectures, loss functions, and training methods, which will be discussed in detail in subsequent sections. Despite these efforts, the fundamental principles of learning-based registration have remained unchanged.
Figure~\ref{fig:img_reg} illustrates the conventional paradigms of learning-based rigid/affine and DIR. Typically, these paradigms consist of the following components:
\begin{enumerate}
    \item Moving and fixed images as input
    \item A deep neural network
    \item The spatial transformer~(for unsupervised methods)
    \item A loss function
\end{enumerate}
The way in which moving and fixed images are inputted into deep neural networks~(DNNs) varies depending on the architecture of the network. They can either be concatenated and sent in as a single input (\emph{e.g.}, VoxelMorph~\citep{balakrishnan2019voxelmorph}) or each image can be processed separately by the DNN, with the feature maps being combined in a deeper stage (\emph{e.g.}, Quicksilver~\citep{yang2017quicksilver}). 

The architecture of DNNs can vary depending on the specific task they are designed to perform and the learning method they will undergo. For affine/rigid registration methods, DNN encoders are used for feature extraction and fully connected layers are used to output the parameters of the predicted transformation. DIR methods use DNNs with both an encoder and decoder, and the result is a deformation field of equal sizes to the input images. In the supervised setting, the network output is compared to ground truth transformations (generated from synthetic transformations or traditional image registration methods) or landmark correspondences using a loss function.
In the unsupervised setting, the predicted transformation is used by the spatial transformer~\citep{jaderberg2015spatial} to warp the moving image, and the transformed image is then evaluated against the fixed image using a loss function that incorporates an image similarity measure. 
When anatomical label maps for the fixed and moving images are available, 
the warped moving label map can also be produced by using the predicted transformation and the spatial transformer. An anatomy loss can be computed using the warped moving label map and the fixed label maps to provide extra
guidance during network training.

There is a diverse range of loss functions to choose from, depending on the learning mode. These are thoroughly discussed in Section \ref{sec:loss}.
The networks are trained by globally optimizing the loss function during the training stage using a training dataset.
The trained networks are then applied to unseen testing images for inference.

Due to the self-supervised nature of image registration, the difference between the transformed moving image and the fixed image can be further reduced at test time.
This is commonly known as \textit{instance-specific optimization}~\citep{balakrishnan2019voxelmorph, siebert2022learn, mok2022robust, heinrich2022voxelmorph++, chen2020generating}.
Specifically, the network weights can be optimized during test time to reduce the dissimilarity of each fixed and moving image pair in the test dataset and further boost the performance.
Registration networks can also be specifically designed to produce diffeomorphic transformations, which are highly desirable in DIR methods and will be discussed further in the next subsection.


\subsection{Diffeomorphic Image Registration}
\label{sec:diff_reg}
Many learning-based DIR methods follow a small deformation model~\citep{balakrishnan2019voxelmorph, kim2021cyclemorph, de2019deep, sokooti2017nonrigid, mok2021conditional, heinrich2019closing, hu2018weakly}.
In this model, $\phi$ in Eqn.~\ref{eqn:energy_func} is approximated by a displacement field, {$d$}, expressed as $\phi = id + {d}$, where the displacement is added to the identity transform, $id$.
Since $\phi$ may not be a one-to-one mapping, this model does not guarantee the invertibility of the deformation.
In some cases, the "inverse" transformation is roughly approximated by subtracting the displacement~\citep{ashburner2007fast}.
In many applications (\emph{e.g.}, \citet{avants2008symmetric, oishi2009atlas, christensen1997volumetric}), diffeomorphic image registration is highly desirable because it provides transformation invertibility and topological preservation. 
% guarantee, where the structures and relationships between parts of the image are preserved after registration.
Diffeomorphic transformations are defined as smooth and continuous one-to-one mappings with a smooth and continuous inverse~(\emph{i.e.}, positive Jacobian determinants).
They are achieved mainly through two approaches: the time-dependent velocity field~\citep{beg2005computing, avants2008symmetric} or the time-stationary velocity field~\citep{arsigny2006log, ashburner2007fast, vercauteren2009diffeomorphic, hernandez2009registration} approach. 

The time-dependent velocity field approach involves integrating sufficiently smooth velocity fields that change over time. The diffeomorphism is established by using a velocity field $v^{(t)}$ at time $t$, and evolving it through~\citep{beg2005computing}:
\begin{equation}
\label{eqn:diff}
    \frac{d\phi^{(t)}}{dt}=v^{(t)}(\phi^{(t)}).
\end{equation}
The diffeomorphic transformation is achieved by starting with an identity transformation, \emph{i.e.} $\phi^{(0)}=id$, and integrating over the unit time period:
\begin{equation}
    \phi^{(1)}=\phi^{(0)}+\int^1_0v^{(t)}(\phi^{(t)})dt.
\end{equation}
{Time-varying models for learning-based registration offer the promise of capturing complex and large deformations while preserving diffeomorphic mappings that ensure biologically realistic transformations. However, such models entail certain challenges. Achieving the time-dependent diffeomorphic mapping can be done by either integrating a sequence of small, sufficiently smooth (first-order differentiable) velocity fields as proposed by the original LDDMM~\citep{beg2005computing}, or by estimating an initial velocity field and subsequently deriving the time-varying velocity fields via geodesic shooting~\citep{miller2006geodesic, zhang2019fast}. Implementing the former within a DNN framework necessitates the network to estimate a series of displacement fields at discrete time points, usually at high resolution, which leads to increased memory requirements. On the other hand, it is numerically difficult to enforce a deformation field to follow a geodesic by geodesic shooting, making end-to-end training challenging without adapting the geodesic shooting method to modern DNN libraries. Such challenges have limited the implementation of time-varying diffeomorphic mapping in current learning-based registration models. Only a handful of studies~\citep{ramon2022lddmm, pathan2018predictive, shen2019region, yang2016fast, yang2017quicksilver, han2021deep, wang2020deepflash, wang2023metamorph, wang2022geo}, have integrated this into a DNN framework. Out of these, only three studies have embedded geodesic shooting into an end-to-end learning framework for medical image registration~\citep{shen2019region, wang2023metamorph, wang2022geo}.}

The time-stationary velocity field approach considers velocity fields that remain constant throughout time.
By using this setting, the evolution of the diffeomorphism in Eqn.~\ref{eqn:diff} can be rewritten as:
\begin{equation}
\frac{d\phi^{(t)}}{dt}=v(\phi^{(t)}),
\end{equation}
where the velocity field, $v$, is now independent of time. 
\citet{dalca2019unsupervised}~were the first to use this setting in a DNN model through the \textit{scaling-and-squaring} method~\citep{arsigny2006log,ashburner2007fast}. 
This method has since become dominant in learning-based diffeomorphic registration models~\citep{mok2020fast, chen2022transmorph, mok2020large, han2023diffeomorphic, zhang2021learning, qiu2021learning, zhao2021s3reg, krebs2019learning}.
The scaling-and-squaring method considers the velocity field as a member of the Lie algebra and the deformation field as a member of the Lie group.
%The velocity field lies in the tangent space at the identity element in the Lie group and its connection to the deformation field is described by an exponential map:
{The relationship between the velocity field and diffeomorphism is governed by the principle that one-parameter subgroups in the Lie group can be equivalently represented via an exponential map~\citep{arsigny2006log,vercauteren2009diffeomorphic,ashburner2007fast}, which is mathematically expressed as:}
\begin{equation}
\phi=\exp{(v)}{=\exp{(2^{-N}v)^{2^N}}},
\end{equation}
which is equivalent to integrating along the velocity field over the unit time period {(in this case, $N$ time steps)}. An alternative perspective is that the Jacobian determinant of a deformation resulting from exponentiating the velocity field is always positive, similar to how the derivative of the exponential of a real number is always positive~\citep{ashburner2007fast}. For further information on the implementation of this method, we direct interested readers to the references cited~\citep{ashburner2007fast, arsigny2006log, dalca2019unsupervised}.
It is important to note that the scaling-and-squaring method cannot guarantee a folding-free transformation in the digital domain when measured by the finite difference approximated Jacobian determinant.
This is because the scaling-and-squaring method involves bilinear or trilinear interpolation that is inconsistent with the piecewise linear transformation assumed by the finite difference based Jacobian determinant computation~\citep{liu2022finite}.