Image registration aims to estimate the optimal coordinate transformation that minimizes an energy function of the form:
\begin{equation}
\label{eqn:energy_func}
    \hat{\phi} = \operatorname*{\argmin}_{\phi}E(I_f, I_m\circ\phi) + \lambda R(\phi),
\end{equation}
where $I_f$ and $I_m$ denote the fixed and moving image, respectively, $\phi$ represents the deformation field that maps $I_m$ to $I_f$, and $R$ is a functional of $\phi$.
The first term in the energy function measures the image similarity between the fixed image and the transformed moving image. The second term enforces regularization on the deformation field, with $\lambda$ being a hyperparameter that determines the trade-off between image similarity and deformation field regularity. 
The purpose of the image similarity measure is to quantify the discrepancy between the fixed image and the transformed moving image. The regularization term is typically used in DIR, as it allows for the integration of prior knowledge about the desired characteristics of the deformation field, such as spatial smoothness. Moreover, regularization prevents the deformation field from exhibiting physically implausible behaviors, such as ``folding'' or rearranging of voxels~\citep{rohlfing2011image}.
This is particularly important for medical images because such unrealistic behavior does not accurately reflect the way that organs deform in reality and may lead to a misinterpretation of the registration results. Regularization is often not required for rigid/affine registration because the deformation field is guaranteed to be spatially uniform.
% and does not exhibit the unrealistic behavior seen in DIR.

% Figure environment removed

\subsection{Supervised vs. Unsupervised Learning}
Learning-based registration methods can be broadly categorized as supervised and unsupervised.
In the machine learning paradigm, supervised learning typically refers to the use of extrinsic information during learning (such as labels) whereas unsupervised methods are concerned with discovering properties intrinsic to the data.
Both supervised and unsupervised learning-based registration methods require a training stage that uses pairs of inputs and their corresponding target outputs.
Supervised registration methods use ground truth transformations as target output during the training process.
Unsupervised methods refer to those that do not require ground truth transformations.
Yet, methods that employ landmark correspondences or anatomical label maps during their training phase are still categorized under supervised learning. This is because landmark correspondences are a sparse representation of the ground truth transformations, and matching label maps act as a surrogate for evaluating registration performance. When this extrinsic information is used alongside the image data to aim learning, these methods are referred to as semi-supervised.
In certain contexts, the term "unsupervised" might be misleading. A more precise term could be ``self-supervised'' to underscore the training aspect of deep learning. 
However, for the purposes of clarity and consistency in this discussion, we will use conventional terminology and refer to methods that do not require supervision from extrinsic information as unsupervised.

During the early stages of development, the majority of learning-based registration methods were supervised.
The ground truth transformations required for the training process are typically generated using traditional registration methods, such as \citep{yang2017quicksilver, rohe2017svf, cao2017deformable, hu2018weakly, fan2019birnet}.
However, generating ground-truth transformations this way is a time-consuming process, which is a notable drawback of such methods.
In addition, since these networks are trained to mimic the function of traditional methods, their registration performance may not surpass that of the methods they are based on.
In some cases, post-processing of the deformation fields may be required to further improve registration accuracy~\citep{yang2017quicksilver}.
Alternatively, artificial deformations can also be used as ground truth transformations in certain cases~\citep{miao2016cnn, krebs2017robust, sokooti2017nonrigid, eppenhof2018pulmonary, eppenhof2018deformable}.

More recently, the introduction of spatial transformer networks~\citep{jaderberg2015spatial} has led to a shift towards developing unsupervised methods that do not rely on ground-truth transformation~\citep{vos2017end, li2018non, de2019deep, balakrishnan2019voxelmorph, dalca2019unsupervised, mok2020fast, mok2020large, mok2021conditional, kim2021cyclemorph, chen2022transmorph, chen2021vitvnet}. These methods use the difference between the deformed moving image and the fixed image to update the network, enabling end-to-end training. By removing the reliance on ground truth transformation, these methods offer greater flexibility in modeling different properties of the deformation fields (\emph{e.g.}, smoothness, invertibility).

\subsection{Paradigm for Learning-based Registration}
Recent progress in the field of learning-based medical image registration has been focusing on exploring different ways to improve registration accuracy, such as through modifications to network architectures, loss functions, and training methods, which will be discussed in detail in subsequent sections. Despite these efforts, the fundamental principles of learning-based registration have remained unchanged.
Figure~\ref{fig:img_reg} illustrates the conventional paradigms of learning-based rigid/affine and DIR. Typically, these paradigms consist of the following components:
\begin{enumerate}
    \item Moving and fixed images as input
    \item A deep neural network
    \item The spatial transformer~(for unsupervised methods)
    \item A loss function
\end{enumerate}
The way in which moving and fixed images are inputted into deep neural networks~(DNNs) varies depending on the architecture of the network. They can either be concatenated and sent in as a single input (\emph{e.g.}, VoxelMorph~\citep{balakrishnan2019voxelmorph}) or each image can be processed separately by the DNN, with the feature maps being combined in a deeper stage (\emph{e.g.}, Quicksilver~\citep{yang2017quicksilver}). 

The architecture of DNNs can vary depending on the specific task they are designed to perform and the learning method they will undergo. For affine/rigid registration methods, DNN encoders are used for feature extraction and fully connected layers are used to output the parameters of the predicted transformation. DIR methods use DNNs with both an encoder and decoder, and the result is a deformation field of equal sizes to the input images. In the supervised setting, the network output is compared to ground truth transformations (generated from synthetic transformations or traditional image registration methods) or landmark correspondences using a loss function.
In the unsupervised setting, the predicted transformation is used by the spatial transformer~\citep{jaderberg2015spatial} to warp the moving image, and the transformed image is then evaluated against the fixed image using a loss function that incorporates an image similarity measure. 
When anatomical label maps for the fixed and moving images are available, 
the warped moving label map can also be produced by using the predicted transformation and the spatial transformer. An anatomy loss can be computed using the warped moving label map and the fixed label maps to provide extra
guidance during network training.

There is a diverse range of loss functions to choose from, depending on the learning mode. These are thoroughly discussed in Section \ref{sec:loss}.
The networks are trained by globally optimizing the loss function during the training stage using a training dataset.
The trained networks are then applied to unseen testing images for inference.

Due to the self-supervised nature of image registration, the difference between the transformed moving image and the fixed image can be further reduced at test time.
This is commonly known as \textit{instance-specific optimization}~\citep{balakrishnan2019voxelmorph, siebert2022learn, mok2022robust, heinrich2022voxelmorph++, chen2020generating}.
Specifically, the network weights can be optimized during test time to reduce the dissimilarity of each fixed and moving image pair in the test dataset and further boost the performance.
Registration networks can also be specifically designed to produce diffeomorphic transformations, which are highly desirable in DIR methods and will be discussed further in the next subsection.


\subsection{Diffeomorphic Image Registration}
\label{sec:diff_reg}
Many learning-based DIR methods follow a small deformation model~\citep{balakrishnan2019voxelmorph, kim2021cyclemorph, de2019deep, sokooti2017nonrigid, mok2021conditional, heinrich2019closing, hu2018weakly}. In this model, $\phi$ in Eqn. \ref{eqn:energy_func} is represented by a displacement field, $v$, expressed as $\phi=id+v$, where the displacement is added to the identity transform, $id$. Since $\phi$ may not be a one-to-one mapping, this model does not guarantee the invertibility of the deformation.
In some cases, the "inverse" transformation is roughly approximated by subtracting the displacement~\citep{ashburner2007fast}.
In many applications (\emph{e.g.}, \citet{avants2008symmetric, oishi2009atlas, christensen1997volumetric}), diffeomorphic image registration is highly desirable because it provides transformation invertibility and topological preservation. 
% guarantee, where the structures and relationships between parts of the image are preserved after registration.
Diffeomorphic transformations are defined as smooth and continuous one-to-one mappings with a smooth and continuous inverse~(\emph{i.e.}, positive Jacobian determinants).
They are achieved mainly through two approaches: the time-dependent velocity field~\citep{beg2005computing, avants2008symmetric} or the time-stationary velocity field~\citep{arsigny2006log, ashburner2007fast, vercauteren2009diffeomorphic, hernandez2009registration} approach. 

The time-dependent velocity field approach involves integrating sufficiently smooth velocity fields that change over time. The diffeomorphism is established by using a velocity field $v^{(t)}$ at time $t$, and evolving it through~\citep{beg2005computing}:
\begin{equation}
\label{eqn:diff}
    \frac{d\phi^{(t)}}{dt}=v^{(t)}(\phi^{(t)}).
\end{equation}
The diffeomorphic transformation is achieved by starting with an identity transformation, \emph{i.e.} $\phi^{(0)}=id$, and integrating over the unit time period:
\begin{equation}
    \phi^{(1)}=\phi^{(0)}+\int^1_0v^{(t)}(\phi^{(t)})dt.
\end{equation}
However, the complexity of the differential equations involved in the time-varying setting has led to limited use of this approach in current learning-based registration models. Only a handful of studies, such as~\cite{ramon2022lddmm, pathan2018predictive, shen2019region, yang2017quicksilver, yang2016fast, han2021deep, wang2020deepflash}, have integrated it into a DNN framework. These studies primarily involve using a DNN to predict an initial momentum field and then updating it through geodesic shooting~\citep{miller2006geodesic, zhang2019fast} to derive the velocity fields. As a result, end-to-end training is not feasible without re-implementing the geodesic shooting framework with modern DNN libraries. To date, only one previous work has achieved this for medical image registration~\citep{shen2019region}.

The time-stationary velocity field approach considers velocity fields that remain constant throughout time.
By using this setting, the evolution of the diffeomorphism in Eqn.~\ref{eqn:diff} can be rewritten as:
\begin{equation}
\frac{d\phi^{(t)}}{dt}=v(\phi^{(t)}),
\end{equation}
where the velocity field, $v$, is now independent of time. 
\citet{dalca2019unsupervised}~were the first to use this setting in a DNN model through the \textit{scaling-and-squaring} method~\citep{arsigny2006log,ashburner2007fast}. 
This method has since become dominant in learning-based diffeomorphic registration models~\citep{mok2020fast, chen2022transmorph, mok2020large, han2023diffeomorphic, zhang2021learning, qiu2021learning, zhao2021s3reg, krebs2019learning}.
The scaling-and-squaring method considers the velocity field as a member of the Lie algebra and the deformation field as a member of the Lie group.
The velocity field lies in the tangent space of the identity element in the Lie group and its connection to the deformation field is described by an exponential map:
\begin{equation}
\phi=\exp{(v)},
\end{equation}
which is equivalent to integrating along the velocity field over the unit time period. An alternative perspective is that the Jacobian determinant of a deformation resulting from exponentiating the velocity field is always positive, similar to how the derivative of the exponential of a real number is always positive~\citep{ashburner2007fast}. For further information on the implementation of this method, we direct interested readers to the references cited~\citep{ashburner2007fast, arsigny2006log, dalca2019unsupervised}.
It is important to note that the scaling-and-squaring method cannot guarantee a folding-free transformation in the digital domain when measured by the finite difference approximated Jacobian determinant.
This is because the scaling-and-squaring method involves bilinear or trilinear interpolation that is inconsistent with the piecewise linear transformation assumed by the finite difference based Jacobian determinant computation~\citep{liu2022finite}.