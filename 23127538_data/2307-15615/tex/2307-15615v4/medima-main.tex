%% This is file `medima-template.tex',
%% 
%% Copyright 2018 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references
%%
%% $Id: medima-template.tex 153 2018-12-01 11:38:32Z rishi $
%% $URL: http://lenova.river-valley.com/svn/elsarticle/trunk/medima-template.tex $
%%
%% Use the option review to obtain double line spacing
%\documentclass[times,review,preprint,authoryear]{elsarticle}

%% Use the options `twocolumn,final' to obtain the final layout
%% Use longtitle option to break abstract to multiple pages if overfull.
%% For Review pdf (With double line spacing)
%\documentclass[times,twocolumn,review]{elsarticle}
%% For abstracts longer than one page.
%\documentclass[times,twocolumn,review,longtitle]{elsarticle}
%% For Review pdf without preprint line
%\documentclass[times,twocolumn,review,nopreprintline]{elsarticle}
%% Final pdf
\documentclass[times,twocolumn,final]{elsarticle}
%%
%\documentclass[times,twocolumn,final,longtitle]{elsarticle}
%%


%% Stylefile to load MEDIMA template
%% Stylefile to load MEDIMA template
\usepackage{medima}
\usepackage{framed,multirow}
\usepackage{booktabs}
%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{array, makecell}
\usepackage{bm}
\usepackage{xspace}
\usepackage{nicematrix}
% \usepackage{boldline}
\usepackage{subfigure}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{rotating}
% Following three lines are needed for this document.
% If you are not loading colors or url, then these are
% not required.
\usepackage{url}
\usepackage{xcolor}
\usepackage[colorlinks = true,
            linkcolor = tealblue,
            urlcolor  = tealblue,
            citecolor = tealblue,
            anchorcolor = tealblue]{hyperref}

\DeclareMathOperator*{\argmax}{arg\,max\xspace}
\DeclareMathOperator*{\argmin}{arg\,min\xspace}

\definecolor{newcolor1}{rgb}{.0, .502, .675}
\definecolor{candyAppleRed}{RGB}{255 8 0}


\newif\ifcb
\usepackage{marginnote}
%
\cbtrue
%
% \cbfalse


\newcommand{\Mnote}[1]{\textcolor{black!25!candyAppleRed}{\bfseries\scriptsize#1}}

\newcommand{\mnote}[1]{%
%
\ifcb%
%
\textcolor{black!25!candyAppleRed}{\marginpar[\begin{flushright}\Mnote{#1}\end{flushright}]{\begin{flushleft}\Mnote{#1}\end{flushleft}}}%
%
\else%
%
\xspace%
%
\fi%
%
}




\makeatletter
\AtBeginDocument{\def\@citecolor{newcolor1}}
\AtBeginDocument{\def\@linkcolor{newcolor1}}
\AtBeginDocument{\def\@anchorcolor{newcolor1}}
\AtBeginDocument{\def\@filecolor{newcolor1}}
\AtBeginDocument{\def\@urlcolor{newcolor1}}
\AtBeginDocument{\def\@menucolor{newcolor1}}
\AtBeginDocument{\def\@pagecolor{newcolor1}}
\makeatother
\hyphenation{Conv-Net}
\hyphenation{Conv-Nets}
\hyphenation{Conv-NeXt}
\newcommand\sbullet[1][.5]{\mathbin{\vcenter{\hbox{\scalebox{#1}{$\bullet$}}}}}
\newcolumntype{?}{!{\vrule width 1pt}}
\usepackage{color, colortbl}
\definecolor{Gray}{gray}{0.9}
\definecolor{newcolor}{rgb}{.8,.349,.1}

\journal{}%Medical Image Analysis}

\begin{document}

\verso{Chen, Liu, Wei \textit{et~al.}}

\begin{frontmatter}

\title{A survey on deep learning in medical image registration: new technologies, uncertainty, evaluation metrics, and beyond}

\author[1]{Junyu \snm{Chen}\corref{cor1}\fnref{fn1}}

\author[2]{Yihao \snm{Liu}\fnref{fn1}}
\author[2]{Shuwen \snm{Wei}\fnref{fn1}}
\fntext[fn1]{Contributed equally to this work.}
\author[2]{Zhangxing \snm{Bian}}
\author[1]{Shalini \snm{Subramanian}}
\author[2]{Aaron \snm{Carass}}
\author[2]{Jerry L. \snm{Prince}}
\author[1]{Yong \snm{Du}}
\address[1]{Department of Radiology and Radiological Science, Johns Hopkins School of Medicine, MD, USA}
\address[2]{Department of Electrical and Computer Engineering, Johns Hopkins University, MD, USA}
\cortext[cor1]{Corresponding author. E-mail address: 
 jchen245@jhmi.edu.}
\received{xxxx}
\finalform{xxxx}
\accepted{xxxx}
\availableonline{xxxx}
\communicated{xxxx}


\begin{abstract}
Deep learning technologies have dramatically reshaped the field of medical image registration over the past decade.
The initial developments, such as {regression-based and U-Net-based networks}, established the foundation for deep learning in image registration.
Subsequent progress has been made in various aspects of deep learning-based registration, including similarity measures, deformation regularizations, {network architectures}, and uncertainty estimation.
These advancements have not only enriched the field of image registration but have also facilitated its application in a wide range of tasks, including atlas construction, multi-atlas segmentation, motion estimation, and 2D-3D registration.
In this paper, we present a comprehensive overview of the most recent advancements in deep learning-based image registration.
We begin with a concise introduction to the core concepts of deep learning-based image registration. Then, we delve into innovative network architectures, loss functions specific to registration, and methods for estimating registration uncertainty. 
Additionally, this paper explores appropriate evaluation metrics for assessing the performance of deep learning models in registration tasks.
Finally, we highlight the practical applications of these novel techniques in medical imaging and discuss the future prospects of deep learning-based image registration.
\end{abstract}

\begin{keyword}
%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
%\MSC 41A05\sep 41A10\sep 65D05\sep 65D17
%% Keywords
\KWD Image Registration\sep Deep Neural Networks\sep Medical Imaging
\end{keyword}

\end{frontmatter}

%\linenumbers

%% main text

\section{Introduction}
\label{sec:introduction}
\input{introduction.tex}

\section{Fundamentals of Learning-based Image Registration}
\label{sec:Fund_Img_Reg}
\input{fundamentals.tex}

\section{Loss Functions}
\label{sec:loss}
\input{loss.tex}
\section{Network Architectures}
\label{sec:net_arch}
\input{network_arch.tex}

\section{Uncertainty in Learning-based Registration}
\label{sec:uncertainty}
\input{uncertainty.tex}

\section{Registration Evaluation Metrics}
\label{sec:Eval_Metric}
\input{evaluation.tex}

\section{Benchmark Datasets for Medical Image Registration}
\label{sec:Datasets}
\input{datasets.tex}

\section{Applications of Medical Image Registration}
\label{sec:Application}
\input{application.tex}

\section{Challenges and Future Perspectives}
\label{sec:future_persp}
\input{future_perspective}

\section{Conclusion}
%
In this survey, we presented a thorough examination of deep learning for medical image registration. In contrast to existing review papers, which might not fully capture the most recent advancements and tend to be systematic in nature with a limited focus on technical aspects, our comprehensive survey analyzed over 250 papers with an emphasis on the most recent technological advancements. Beginning with a review of the fundamentals of learning-based image registration, our investigation incorporated widely-used and novel loss functions, as well as network architectures for image registration. We also thoroughly investigated the estimation methods of registration uncertainty and appropriate metrics of registration accuracy and regularity. Furthermore, we provided insights into potential clinical applications, future perspectives, and challenges, aiming to guide future research in this rapidly evolving field.

\section*{Acknowledgments}
%
Junyu Chen and Yong Du were supported by grants from the National Institutes of Health~(NIH), United States, U01-CA140204~(PI: Y.~Du), R01-EB031023~(PI: Y.~Du), and U01-EB031798~(PI: G.~Sgouros).
Yihao Liu, Shuwen Wei, Zhangxing Bian, Aaron Carass, and Jerry L. Prince were supported by the NIH from National Eye Institute grants R01-EY024655~(PI:~J.L.~Prince) and R01-EY032284~(PI:~J.L.~Prince), as well as the National Science Foundation grant 1819326~(Co-PI: S.~Scott, Co-PI: A.~Carass).
This work was also made possible by a 2023 Johns Hopkins Discovery grant~(Co-PI: J.~Chen, Co-PI: A.~Carass).
{The authors thank Lianrui Zuo for valuable discussions and insights.}

The views expressed in written conference materials or publications and by speakers and moderators do not necessarily reflect the official policies of the NIH; nor does mention by trade names, commercial practices, or organizations imply endorsement by the U.S. Government.

\bibliographystyle{model2-names.bst}
\biboptions{authoryear}
\bibliography{references}

\end{document}

%%
