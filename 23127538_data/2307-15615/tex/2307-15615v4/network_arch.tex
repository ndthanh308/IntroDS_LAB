
\begin{table*}[!h]
        %\setcounter{table}{1}
        \rowcolors{3}{white}{cyan!10}
        \centering
        \caption{{A summary of the anatomy, modality, and network infrastructure of the methods outlined in Table~\ref{table:unsup_list}.}}
        \label{table:infrastructure}
        \fontsize{8.5}{10}\selectfont
        \begin{tabular}{lc lc lc l}
        \\[-1em]\hspace*{-5em}\\[-0.8em]
        \toprule
        \rowcolor{white}\textbf{Method} && \textbf{Anatomy} && \textbf{Modality} && \textbf{Network Infrastructure}\\
        %
        \cmidrule(lr){1-1}
        \cmidrule(lr){3-7}
        %
%
AC-DMiR~\citep{khor2023anatomically} && Brain / Uterus && MRI && Transformer \\
ADMIR~\citep{tang2020admir}  && Brain && MRI && CNN \\
AMNet~\cite{che2023amnet} && Brain && MRI && CNN\\
Attention-Reg~\citep{song2022cross}  && Prostate && US / MRI && CNN (Self Attention) \\
BIRNet~\citep{fan2019birnet}  && Brain && MRI && CNN \\
Baum~\emph{et al.}~\citep{baum2022meta} && Prostate && MRI / Transrectal Ultrasound && CNN \\
CondLapIRN~\citep{mok2021conditional}  && Brain && MRI && CNN \\
CycleMorph~\citep{kim2021cyclemorph}  && Faces / Brain / Liver && Photographs / MRI / CT && CNN \\
de Vos~\emph{et al.}~\citep{de2020mutual}  && Breast && MRI && CNN \\
Deformer~\citep{chen2022deformer}  && Brain && MRI && CNN \\
DiffuseMorph~\citep{kim2022diffusemorph}  && Faces / Brain / Cardiac && Photographs / MRI && DDPM \\
DIRNet~\citep{de2017end}  && Cardiac && MRI && CNN \\ 
DLIR~\citep{de2019deep}  && Cardiac / Chest && MRI / CT && CNN \\ 
DNVF~\citep{han2023diffeomorphic}  && Brain && MRI && DNVF \\
DTN~\citep{zhang2021learning}  && Brain && MRI && CycleGAN-like \\
Dual-PRNet~\citep{hu2019dual}  && Brain && MRI && CNN / Siamese \\
Dual-PRNet++~\citep{kang2022dual}  && Brain && MRI && CNN / Siamese \\
FAIM~\citep{kuang2019faim}  && Brain && MRI && CNN \\
Fan~\emph{et al.}~\citep{fan2019adversarial}  && Brain / Pelvis && MRI / CT && GAN \\
Fourier-Net~\citep{jia2022fourier}  && Brain && MRI && CNN \\
FSDiffReg~\cite{qin2023fsdiffreg} && Cardiac && MRI && DDPM \\ 
GraformerDIR~\citep{yang2022graformerdir}  && Brain / Cardiac && MRI && CNN \\
Han~\emph{et al.}~\citep{han2022deformable}  && Brain && MRI / CT && CycleGAN \\
Hering~\emph{et al.}~\citep{hering2021cnn}  && Lung && CT && CNN \\
HyperMorph~\citep{hoopes2022hyper}  && Brain && MRI && CNN \\
IDIR~\citep{wolterink2022implicit} && Lung && CT && INRs \\
im2grid~\citep{liu2022coordinate}  && Brain && MRI && CNN \\
Krebs~\emph{et al.}~\citep{krebs2019learning}  && Cardiac && MRI && CNN \\
LapIRN~\citep{mok2020large}  && Brain && MRI && CNN \\
LKU-Net~\citep{jia2022u}  && Brain && MRI && CNN \\
Li~\emph{et al.}~\citep{li2018non}  && Brain && MRI && CNN \\
Liu~\emph{et al.}~\citep{liu2019probabilistic} && Brain && MRI && CNN / Siamese \\
MAIRNet~\cite{gao2024mairnet} && Pelvis / Spine && MRI / CT && CNN \\
MIDIR~\citep{qiu2021learning}  && Brain && MRI && CNN \\
ModeT~\citep{wang2023modet} && Brain && MRI && Transformer \\
MS-DIRNet~\citep{lei20204d}  && Abdomen && CT && CNN \\
MS-ODENet~\citep{xu2021multi}  && Brain && MRI && Neural-ODE \\
NICE-Trans~\cite{meng2023non} && Brain && MRI && Transformer \\
NODEO~\citep{wu2022nodeo}  && Brain && MRI && Neural-ODE \\
PC-Reg~\citep{yin2023pc} && Brain / Abdomen && MRI / CT && Transformer \\
PC-SwinMorph~\citep{liu2022pc}  && Brain && MRI && CNN \\
PDD-Net 2.5D~\citep{heinrich2020highly}  && Abdomen && CT && CNN \\
PDD-Net 3D~\citep{heinrich2019closing}  && Abdomen && CT && CNN \\
PIViT~\citep{ma2023pivit}  && Brain && MRI && Transformer \\
R2Net~\citep{joshi2023r2net}  && Brain / Lung / Cardiac && MRI / CT && Neural-ODE \\
SDHNet~\citep{zhou2023self}  && Brain / Liver && MRI / CT && CNN / Student -- Teacher\\
Shao~\emph{et al.}~\citep{shao2022multi}  && Abdomen && Endoscopic && CNN \\
SVF-R2Net~\citep{joshi2022diffeomorphic}  && Brain && MRI && CNN \\
SpineRegNet~\cite{zhao2023spineregnet} && Spine && MRI / CT && CNN \\
SYMNet~\citep{mok2020fast}  && Brain && MRI && CNN \\
SymTrans~\citep{ma2022symmetric}  && Brain && MRI && Transformer \\
SynthMorph~\citep{hoffmann2021synthmorph}  && Brain && MRI && CNN \\
TM-DCA~\citep{chen2023deform}  && Brain && MRI && Transformer \\
TM-TVF~\citep{chen2022unsupervised}  && Faces / Brain && Photographs / MRI && Transformer \\
TransMatch~\citep{chen2023transmatch} && Brain && MRI && Transformer \\
TransMorph~\citep{chen2022transmorph}  && Brain / Abdomen && MRI / CT && Transformer \\
ViT-V-Net~\citep{chen2021vitvnet}  && Brain && MRI && Transformer \\
VoxelMorph~\citep{balakrishnan2019voxelmorph}  && Brain && MRI && CNN \\
VoxelMorph-diff~\citep{dalca2019unsupervised}  && Brain && MRI && CNN \\
VoxelMorph++~\citep{heinrich2022voxelmorph++}  && Chest / Abdomen && CT && CNN \\
VR-Net~\citep{jia2021learning}  && Cardiac && CT && CNN \\
VTN~\citep{zhao2019unsupervised}  && Brain / Liver && MRI / CT && CNN \\
XMorpher~\citep{shi2022xmorpher}  && Brain / Cardiac && MRI / CT && Transformer \\
Zhang~\emph{et al.}~\citep{zhang2020diffeomorphic}  && Brain && MRI && CNN \\
%
\bottomrule
        %
        % Voxelmorph && CNN && MRI && Brain\\
        %
        % Estienne et al., 2019 && CNN / U-Net && MRI && Brain\\
        %
        % Chen et al., 2021 (Infant Cerebellum) && CNN && MRI && Brain\\
        %
        % Huang 2021, TMI && CNN && MRI && Brain\\
        %
        % Hypermorph && CNN && MRI && Brain\\
        %
        % Wolterink, SASHIMI 2017 && CycleGAN && MRI \& CT && Brain\\
        \end{tabular}
        \end{table*}


The application of ConvNets has been the dominant trend in learning-based image registration since its inception.
Among different ConvNets architectures, the U-Net-like architectures~\citep{ronneberger2015u}, which were initially designed for image segmentation tasks, have played an important role.
Many noteworthy ConvNet-based registration models, including RegNet~\citep{sokooti2017nonrigid}, DIRNet~\citep{de2017end}, QuickSilver~\citep{yang2017quicksilver}{,} VoxelMorph~\citep{balakrishnan2019voxelmorph, dalca2019unsupervised}, VTN~\citep{zhao2019unsupervised}, DeepFlash~\citep{wang2020deepflash}, and CycleMorph~\citep{kim2021cyclemorph}, have demonstrated promising performance in various registration applications.
More recently, registration neural networks have witnessed notable advancements beyond the conventional ConvNet designs, owing to the progress of DNN architectures in computer vision and the development of architectures that are specifically tailored for registration tasks. 
Notably, models such as Transformers, diffusion models, and Neural ODEs are gaining increasing attention in the field of image registration.
{Table~\ref{table:infrastructure} summarizes the types of network architectures along with the associated anatomies and modalities of the methods outlined in Table~\ref{table:unsup_list}.
This section provides a comprehensive overview of the \textbf{recent advancements} in network architectures.}

% Figure environment removed

\subsection{Adversarial Learning}
\label{sec:adv_learn}
The majority of adversarial learning applied to image registration relies on the foundational principles of generative adversarial networks~(GANs).
The concept of GANs is derived from a two-player zero-sum game involving a generator and a discriminator~\citep{goodfellow2020generative}.
The objective of the generator is to generate new samples by learning the data distribution, while the discriminator functions as a binary classifier, aiming to accurately distinguish between real and generated samples.
In the context of image registration, the registration network acts as the generator, producing a deformation field and subsequently warping the moving image.
Meanwhile, the discriminator functions as an image similarity measure, distinguishing between the warped image and the fixed image.
This offers the advantage of alleviating the need for an explicit similarity measure, making the approach adaptable to both mono- and multi-modality applications.

{Figure~\ref{f:network_adv} summarizes the designs of adversarial-based DNNs used for medical image registration.} In early applications of adversarial learning to image registration, \citet{fan2018adversarial}~and \citet{yan2018adversarial}~adhered to the aforementioned approach.
The former utilized the generator to produce a deformation field, while the latter employed a ConvNet encoder to generate affine transformation parameters.
Subsequently, a binary discriminator served as a similarity measure between the transformed and fixed images.
In a similar vein, Mahapatra~\emph{et al.}~\citep{mahapatra2018deformable, mahapatra2018joint, mahapatra2020training} applied adversarial learning to multi-modal image registration, with the additional implementation of CycleGAN~\citep{zhu2017unpaired, qin2019unsupervised} to further ensure the inverse consistency of the generated deformation field. 
\citet{elmahdy2019adversarial}~proposed incorporating anatomical label maps into a Wasserstein-GAN~(WGAN) to enhance the segmentation performance of the registration network.
Their generator was a U-Net-based network that generated a deformation field, which warped both the moving image and the associated anatomical label map.
The discriminator's role was to evaluate the alignment between the warped and fixed image, as well as the warped and fixed label maps.
In their approach, image and anatomical similarity measures were still employed, while the discriminator served as an additional measure of the alignment.
Similar approaches can be found in \citet{duan2019adversarial},  \citet{li2019adversarial}, and \citet{luo2021deformable}, where the authors used the discriminator in conjunction with image similarity measures as additional alignment indicators.
In another study, \citet{fan2019adversarial}~proposed a GAN-based registration framework applicable to both mono- and multi-modality registration.
Their generator was also a registration network based on U-Net, with the discriminator serving as the sole measure of image alignment.
However, the definition of positive pairs sent to the discriminator deviated from previous methods.
Ideally, in mono-modality registration, a positive pair would consist of identical images, but this strict requirement is impractical.
Given this observation, the authors proposed that the positive pair comprise the fixed image and an alpha-blended image created from the fixed and moving images.
For multi-modality registration, a positive pair consisted of pre-aligned multi-modal images from the same patient. 
The method was evaluated on mono-modal brain MRI registration and multi-modal pelvic MR and CT registration tasks, demonstrating favorable performance compared to the state-of-the-art at the time.

Given the promising results GANs have demonstrated in image translation, i.e., synthesizing one image modality into another, researchers have made efforts to leverage their capabilities in addressing multi-modal image registration{~\citep{xu2020adversarial, wei2019synthesis, zheng2021symreg, han2022deformable, zhong2023united}}.
This approach involves first synthesizing multi-modal images into the same modality and then applying a registration network to perform the image registration task. In \citep{xu2020adversarial}, the authors tackled the challenge of multi-modal registration of CT and MR images using a CycleGAN-based approach to translate CT images into MR images.
To ensure that the translated images maintained anatomical consistency with the original images, the authors introduced additional loss functions, including MIND and identity loss, alongside the standard CycleGAN loss. 
They then employed a three-stage registration framework to align the original and translated images.
In the first stage, a U-Net-based registration network learned the multi-modal registration between CT and MR images.
In the second stage, a network with the same architecture learned the mono-modal registration between the translated CT and the target MR images.
Finally, the deformation fields created by both registration networks were fused using a convolutional layer to produce the final deformation field.
A similar concept was presented in \citet{wei2019synthesis},~where mutual information was used instead of MIND to enforce structural consistency.
\citet{zheng2021symreg}~integrated an image translation network within a GAN-based image registration framework, where the modality of the moving image was first translated to the modality of the target image before a registration network was applied to register the two images.
The discriminator in this approach acted as an image similarity metric for both the registration and image translation networks.
Additionally, this approach employed a symmetric pipeline that reversed the order of the moving and fixed images, ensuring symmetric consistency in the resulting synthesized and deformation images.
More recently, \citet{han2022deformable}~proposed tackling the multi-modal registration between CT and MR images using a dual-channel framework.
Within each channel, an imaging modality was transformed into a target modality using a probabilistic CycleGAN, which was then followed by a registration network that predicted the deformation in the target modality.
The deformation fields from both channels were then fused, taking advantage of the uncertainty weighting generated by the synthesis networks.
This proposed dual-channel framework can be trained end-to-end, resulting in improved registration accuracy and faster runtime compared to baseline methods.

Adversarial learning has also been employed for knowledge distillation, enabling the transfer of information from a larger teacher network to a smaller student network (\emph{i.e.}, in terms of the number of parameters).
\citet{tran2022light}~aimed to compress the size of a registration network by transferring information from a computationally expensive VTN~\citep{zhao2019unsupervised} to a smaller registration network with only one-tenth of its parameters.
The training process for the student network involved calculating a correlation-based image similarity measure~\citep{zhao2019unsupervised} between the warped image generated by the student network and the fixed image. 
Meanwhile, a discriminator was used to differentiate the deformation field created by the student network and the pre-trained teacher network.
After training, the teacher network was discarded, and only the lightweight student network was used for inference. 
Despite having only one-tenth of the network parameters, the lightweight registration network demonstrated comparable performance to baseline learning-based methods with larger parameter sizes, in terms of both anatomical overlaps and deformation smoothness.

% Figure environment removed

\subsection{Contrastive Learning}
\label{sec:contrast_learn}
The principle of contrastive learning enables DNNs to learn by comparing various examples instead of focusing on single data points independently.
This comparison process typically involves examining positive pairs of similar inputs and negative pairs of dissimilar inputs.
For a comprehensive understanding of this concept and a detailed overview of the evolution of contrastive learning, we recommend interested readers refer to~\citet{le2020contrastive}.
{Figure~\ref{f:network_contrast} provides a broad illustration of the contrastive learning designs used in medical image registration applications.} In the context of image registration, contrastive learning could be particularly beneficial as an alternative to using explicit image similarity metrics, which can be challenging to optimize due to their task-specific nature.
For example, different similarity metrics may be preferred for lung CT registration versus brain MRI registration or multi-modal versus mono-modal registration tasks.
Whereas, contrastive learning empowers the DNN to determine whether two images are registered or not without relying on a specific image similarity metric {(e.g., Fig.~\ref{f:network_contrast} (a))}, making it a more versatile approach for handling different registration tasks. 

\citet{hu2019towards}~were the pioneers in applying contrastive learning to multi-modal affine registration, concentrating on the inter-patient alignment of 2D CT and MR scans for patients with Nasopharyngeal Cancer.
Their method involved using an automatic keypoint detecting algorithm to identify keypoints in the CT and MR scans.
Subsequently, they extracted a patch centered on each keypoint and employed a Siamese network to minimize the contrastive loss, which minimized the distance between corresponding keypoints and maximized the distance between non-corresponding keypoints.
In the testing phase, after establishing correspondences between all keypoints in the CT and MR scans, the optimal affine transformation parameter was determined by means of least-squares fitting.
In another study, \citet{pielawski2020comir}~applied contrastive learning to transform multi-modal images into similar contrastive representations with equivariant properties.
Their method used two independent U-Nets to learn the representations for each modality such that the InfoNCE-based~\citep{oord2018representation} loss between the learned representations is minimized.
This minimization can be understood as maximizing the mutual information between the two learned representations.
Finally, conventional affine registration methods were used to align the learned representations as if they had undergone a mono-modal registration task.
\citet{wetzer2023can}~later investigated the contrastive learning approach proposed in \citet{pielawski2020comir}~to determine whether applying contrastive learning supervisions to the U-Nets' intermediate layers could improve multi-modal image registration performance.
However, they concluded that the best representations for the evaluated registration task were achieved when the contrastive loss was applied only to the features of the final layers.
\citet{casamitjana2021synth}~proposed a contrastive learning-based approach for multi-modal deformable registration.
They introduced a synthesis-by-registration method, where they trained a registration network for mono-modal registration on the target modality domain, and then froze the network's weight for training an image synthesis network using a loss function that leverages the registration network.
The image synthesis network's ability to accurately translate the moving image into the target modality directly influenced the performance of the registration network.
To enhance synthesis performance and ensure geometric consistency, a PatchNCE-based~\citep{park2020contrastive} contrastive loss was used, maximizing the mutual information between pre- and post-synthesis images at the patch level.
This method demonstrated promising results in multi-modal brain MRI registration applications, outperforming both MI-based registration and other image synthesis-based registration methods.
\citet{dey2022contrareg}~also addressed the multi-modal registration task using contrastive loss.
In their method, feature-extracting autoencoders were first pre-trained for each modality to derive modality-specific features.
These autoencoders were then used on the deformed moving image and the fixed image to extract features for a PatchNCE-based~\citep{park2020contrastive} contrastive loss.
In order to optimize contrastive learning, a single positive pair was sampled, corresponding to the multi-scale feature patches of the same spatial location across both modalities, while multiple negative pairs were sampled, corresponding to the feature patches of different spatial locations. 

Until now, the methods based on contrastive learning have been centered on multi-modal image registration. 
However, \citet{liu2020contrastive}~proposed the integration of contrastive learning in the intermediate stages of the network architecture for mono-modal brain MRI registration.
In their method, two identical ConvNet encoders of shared weights were applied to the moving and fixed images, each followed by a fully-connected layer to project ConvNet extracted features onto a latent space where the contrastive loss is applied.
The positive pair for computing the contrastive loss consists of the unregistered moving and fixed image pair, while any other pair apart from the current image pair under registration is considered a negative pair. 
In an extension of their work, \citet{liu2022pc}~proposed to compute the contrastive loss in a similar way, but between patches of the moving and fixed images. 
However, it is important to note that the positive pair used in these two methods contained structural dissimilarities as it was the unregistered image pair, as opposed to the registered images used in the methods mentioned earlier. 
The authors argued that this was because the image contents, including the number of brain structures, were consistent for brain registration.
Nonetheless, further research is needed to fully uncover the potential of these methods.

% Figure environment removed

\subsection{Transformers}
One of the key factors in designing ConvNets is the size of the receptive fields.
While incorporating consecutive convolutional layers and pooling operations can increase the theoretical receptive fields of ConvNets, its effective receptive fields are still limited~\citep{luo2016understanding}.
This makes them less effective at capturing long-range spatial correspondence, which is important to image registration since it aims to identify the correspondence between different parts of the images.
In contrast, Transformers are widely acknowledged for their superior ability to capture long-range dependencies and achieve exceptional performance when trained on large datasets~\citep{li2023transforming}.
Transformers differ from ConvNets in that they employ the self-attention mechanism, in which each local part of an image is compared in relation to the other parts, guiding the network on where to focus. 
Originally developed for natural language processing tasks~\citep{vaswani2017attention}, Transformers have recently become prevalent in various computer vision applications~\citep{dosovitskiy2021an, liu2021swin, liu2022video, chen2021crossvit, yuan2021tokens, dong2022cswin, carion2020end}. 
Inspired by their success, many Transformer-based models have been proposed and have demonstrated promising performance in medical imaging applications. 
{Figure~\ref{f:network_transformer} provides a graphical illustration of the Transformer-based DNNs used in medical image registration.} 
For a comprehensive review of the current Transformer-based models in medical imaging {in general}, interested readers are directed to a review paper by~\citet{li2023transforming}.
Despite their potential, Transformers have certain drawbacks, such as larger computational complexity and a lack of inductive bias when compared to ConvNets, hindering the training process.
To address these shortcomings, Transformers are commonly used in conjunction with ConvNets in medical image registration applications.
{Initially, the field saw the introduction of self-attention-based Transformers for these tasks.
More recently, cross-attention-based Transformers emerged, demonstrating potential advantages.
Given that registration involves identifying correspondences between images, cross-attention, which focuses on the features of one image in relation to another, may enhance this identification process.
We start by examining self-attention-based Transformers and then proceed to discuss the developments in cross-attention-based Transformers specifically designed for medical image registration.}
\paragraph{Self-attention}
\citet{chen2021vitvnet}~were the first to adopt Transformers for registration-based tasks. 
They proposed ViT-V-Net, which employs a ConvNet for extracting high-level features, followed by a Vision Transformer~(ViT)~\citep{dosovitskiy2020image} and a ConvNet decoder to generate a dense displacement field.
Subsequently, they proposed TransMorph~\citep{chen2022transmorph}, which employs a Swin Transformer~\citep{liu2021swin} in the encoder, replacing the ConvNet feature extractor and ViT.
TransMorph is capable of both affine registration and deformable registration.
The study provided empirical evidence that Transformer-based models have larger effective receptive fields than baseline ConvNets.
In inter-subject and atlas-to-subject brain MRI registration, as well as XCAT-to-CT abdomen registration applications, TransMorph achieved significantly improved registration performance when compared to top-performing traditional and ConvNet-based registration models.
\citet{zhang2021learning}~proposed DTN, which consists of two encoder branches with identical architecture.
Each branch contains a ConvNet feature extractor and a ViT.
In DTN, the moving and fixed images are first fed consecutively into one encoder branch, then concatenated and sent to the other branch.
The encoder outputs are then concatenated and sent to a ConvNet decoder to produce a deformation field.
\citet{mok2022affine}~introduced a Transformer encoder, C2FViT, specifically designed to tackle the affine registration problem.
Their Transformer architecture was inspired by ViT, but with augmented patch embedding and feed-forward layers to introduce locality into the model.
C2FViT adopts a coarse-to-fine strategy with an image pyramid for affine registration.
The registration process is carried out in multiple stages of ViTs with identical architectures, each corresponding to a different resolution of the fixed and moving images.
The affine parameters are estimated in each stage, and the moving image is affine-transformed using the parameters from the previous stage to refine the registration progressively. 
C2FViT was evaluated on several benchmark datasets and demonstrated superior performance compared to multiple ConvNet-based and traditional affine registration methods. 
\citet{chen2022deformer}~proposed a Deformer module, which leverages the attention mechanism on feature maps produced by a ConvNet encoder.
The authors argued that the Deformer module facilitated the image-to-spatial transformation mapping process by estimating the displacement vector prediction as a weighted sum of multiple bases.
Employing a coarse-to-fine strategy, the proposed model outperformed both the ConvNet and Transformer models in the comparative analysis. {\citet{yin2023pc} introduced PC-Reg, which uses a Swin Transformer similar to that in TransMorph \citep{chen2022transmorph} but incorporates modifications in patch partitioning through two convolutional residual blocks. The processed features are subsequently directed to a specially designed ConvNet decoder tasked with predicting the a deformation field. The decoder estimates the final deformation field using a coarse-to-fine approach, progressively upsampling the lower-resolution field and employing a separate module at each scale to refine the field by estimating the residuals. This refinement module draws inspiration from the predictor-corrector method commonly used in numerical methods for differential equations. PC-Reg shows promising results, performing competitively in brain MRI and abdominal CT registration tasks.}
\paragraph{Cross-attention}
\citet{song2022cross}~introduced Attention-Reg, a model that adopts cross-attention to correlate features extracted from multi-modal input images by a ConvNet encoder. 
To expedite the training process, they applied a contrastive pre-training strategy to the ConvNet feature extractor, allowing for the extraction of similar features from different modality images. 
The Dice loss was used as the multi-modal similarity measure, and they developed both rigid and deformable variations of the model.
The results showed that Attention-Reg performed favorably against several learning-based rigid and deformable registration models.
Similarly,  \citet{shi2022xmorpher}~introduced XMorpher, a full Transformer architecture featuring dual parallel feature extractors that exchange information via a cross-attention mechanism. 
The cross-attention module developed in their study is based on a Swin Transformer, where attention is computed between base windows of one image and searching windows of another image with differing sizes. 
This cross-attention mechanism exhibited improved performance compared to self-attention-based Transformers and ConvNet models. 
\citet{chen2023deform}~made further improvements to the cross-attention technique used in XMorpher. 
They proposed a novel deformable cross-attention module that enables tokens to be sampled from regions beyond the conventional rectangular window, while also reducing computational complexity. A lightweight ConvNet was introduced to deform the sampling window in a reference.
The attention is then computed between the tokens sampled from the deformed window in the reference and those sampled from a rectangular window in a base. This enables tokens sampled from a larger reference region to guide the network on where to focus within each local window in the base. 
The proposed network includes two encoding paths.
In one path, the moving and fixed images are used as the base and reference, respectively.
In the other path, the roles of the base and reference are switched, with the moving image used as the reference and the fixed image used as the base.
A ConvNet decoder then fuses the features extracted from the two encoders to generate a deformation field. 
Their method was evaluated on brain MRI registration tasks, and it performed favorably against self-attention, cross-attention, and ConvNet-based models. 
{Adopting a similar strategy of incorporating cross-attention for image registration, \citet{chen2023transmatch} developed TransMatch. This model employs a Transformer encoder with dual-stream modules, each alternating between windowed self-attention and cross-attention to enhance the matching of image features through dot-product attention. This process aims to establish more accurate spatial correspondences both between and within images. A ConvNet decoder subsequently processes the concatenated features from both streams to estimate a deformation field. TransMatch was evaluated on mono-modal brain MRI image registration tasks, where it outperformed both traditional ConvNets and self-attention-based Transformers.}
\citet{liu2022coordinate}~proposed im2grid, a model that uses cross-attention to explicitly guide the neural network in comprehending the coordinate system for image registration, which is usually learned implicitly from data.
% Their approach applies cross-attention to the deep features of different resolutions generated independently from the fixed and moving images using a ConvNet encoder.
Their approach uses ConvNet encoders to independently extract hierarchical features from the fixed and moving images.
% To inject the coordinate information into the deep features, a positional encoding layer applies a sinusoidal positional embedding to the feature maps based on an identity grid.
Subsequently, their proposed coordinate translator block computes a softmax score function by comparing the extracted fixed image feature at a voxel location with the features of the moving image within a search window. 
Spatial correspondence between the voxel location in the fixed and moving images is established by linearly combining the coordinates of all voxel locations weighted by the score function.
Their approach is implemented as cross-attention with coordinates as one of the inputs.
This model was evaluated on inter-patient brain MRI registration tasks using publicly available datasets and demonstrated superior performance compared to the comparative ConvNets and Transformer-based models. 
{\citet{wang2023modet},~inspired by the im2grid introduced in~\citet{liu2022coordinate}, aimed to calculate matching scores to weight deformations but also enhanced the the weighting process by noting that feature maps at coarse-level resolutions typically exhibit different motion modes.
Specifically, the authors developed a motion decomposition Transformer, named ModeT.
This Transformer decomposes the feature maps of the moving and fixed images into multi-head feature representations using a linear projection layer followed by LayerNorm.
Cross-attention is then applied between the feature at each spatial location in the fixed image and the features of the neighboring locations in the moving image, employing a technique similar to that used in~\citet{liu2022coordinate}.
This process yields a series of deformation fields, with the count equaling the number of heads used. Subsequently, a competitive weighting module, which consists of a shallow ConvNet ending with a SoftMax activation function, is applied to the sequence of deformation fields. These fields are then aggregated to form a deformation field at each resolution.
Finally, the final deformation field at the full resolution is generated by sequentially composing the upsampled deformation fields from each resolution. This model was tested on brain MRI image registration tasks and showed superior performance compared to several state-of-the-art methods at the time.
In~\citet{khor2023anatomically}, the authors proposed AC-DMiR for joint learning of registration and segmentation. 
Specifically, AC-DMiR adopts two Transformer-based backbone networks, similar to the one used in TransMorph~\citep{chen2022transmorph}, to generate initial predictions: a deformation field for registration and segmentation for the warped moving image.
Subsequently, the feature maps from the registration and segmentation networks are coupled via cross-task attention modules.
These modules employ cross-attention to focus on features from one task to another, followed by self-attention and a decoder to estimate the final deformation field.
This integration allows the correlation of anatomical variations and alignment information from the segmentation network with the registration network, thereby enhancing the estimation of the deformation.
When compared to traditional ConvNet-based and other Transformer-based methods, AC-DMiR shows enhanced registration accuracy in terms of Dice coefficients and comparable deformation regularity on both Brain MRI and Uterus MRI image registration tasks.}

The mechanisms of Transformers have inspired various ConvNet designs in computer vision, leading to a debate on whether Transformers could replace ConvNets for image-related tasks~\citep{li2023transforming}.
ConvNet models such as ConvNeXt~\citep{liu2022convnet} and RepLKNet~\citep{ding2022scaling} have built upon Transformer concepts and demonstrated performance comparable to Transformers.
Inspired by these models,  \citet{jia2022u}~proposed a U-Net with increased kernel sizes to expand the effective receptive field of the U-Net.
Their method compared favorable against several Transformer-based registration methods. 
Currently, ConvNets still possess inherent advantages over Transformers, such as their invariance to input image sizes and the incorporation of inductive bias due to the nature of the convolution operation.
Therefore, there has been a growing interest in advancing ConvNets using Transformer concepts in computer vision.
It is anticipated that further research in this area will lead to improved ConvNet architectures for medical image registration applications.

% Figure environment removed

\subsection{Diffusion Models}
In recent years, diffusion models~\citep{sohl2015deep, ho2020denoising} have garnered significant research interest in computer vision.
Initially designed for generative tasks, such as image synthesis, inpainting, and super-resolution, diffusion models have now been widely explored in various applications in the field of medical image analysis~(see~\citet{kazerouni2022diffusion} for a survey). 
In contrast to other generative models like GANs and VAEs, which are either confined to data with limited variability or generating low-quality samples~\citep{ho2020denoising, kazerouni2022diffusion}, diffusion models have no such restrictions, making them an attractive alternative.
The goal of diffusion models is to use the known forward process of gradual diffusion of information caused by noise to learn the reverse process of recovery of information from noise.
The forward process is similar to the behavior of particles in thermodynamics, where particles spread~(\emph{i.e.}, diffuse) from areas of high concentration to those of low concentration~\citep{kirkwood1960jcp, sohl2015deep}.
The existing diffusion models use iterative steps of diffusion, which can include up to several thousand steps, to carry out the diffusion process. As a result, inference with these models, which requires the reverse diffusion process, is time-consuming.
{To date, only a few studies have incorporated diffusion models into medical image registration~\citep{kim2022diffusemorph,qin2023fsdiffreg}, and Fig.~\ref{f:network_diffusion} provides a graphical representation of these diffusion model-based models.}
In~\citep{kim2022diffusemorph}, the authors proposed DiffuseMorph, which involves a diffusion network and a deformation network.
The diffusion network learns a conditional score function~(\emph{i.e.}, the added noise), while the deformation network uses the latent feature in the reverse diffusion process to estimate the deformation field.
The registration process of DiffuseMorph is a one-step procedure as the fixed image is the target image at the end of the reverse diffusion process~(\emph{i.e.}, $t=0$), and it is already given. 
As a result, there is no need for time-consuming reverse diffusion steps to synthesize a target image from the moving image.
Furthermore, DiffuseMorph offers the added capability of producing continuous deformations through the interpolation of the learned latent space.
The method demonstrated promising results when compared to several ConvNet-based methods on a publicly available Cardiac MRI dataset and a human facial expression dataset.
However, since their forward process adopts the strategy of adding Gaussian noise to the fixed image, their diffusion network learns a conditional score function for the fixed image. {This score function is then used in the registration network, rather than directly inputting the fixed image, allowing the score function to carry semantic information. This results in a method that differs from the conventional diffusion models.} {In \citep{qin2023fsdiffreg}, the authors proposed an alternative diffusion model based on \citep{kim2022diffusemorph} for medical image registration, where the score function is used as a weighting map for the image similarity measure. However, this formulation deviates from conventional diffusion models, where the score function is typically modeled as Gaussian noise.}

\subsection{Neural ODEs}
\label{sec:nodes}
Inspired by Euler's method for discretizing the derivative of ordinary differential equations~(ODEs) into discrete time step updates, \citet{chen2018neural} proposed a new family of DNN models called Neural ODEs. 
In their method, DNN elements that progressively update their input (\emph{e.g.}, residual connections, or recurrent networks) are interpreted as updates of time steps in Euler's method.
Consequently, a chain of these elements in a neural network is essentially a solution of the ODE with Euler's method of the form:
\begin{equation}
    \frac{dh(t)}{dt} = f_\theta(h(t), t),
\end{equation}
and
\begin{equation}
    h(t+1)=h(t)+f_\theta(h(t), t),
\end{equation}
where $h(t)$ represents the $t$-th element, which may be a residual block or a network.
The final output at $t=T$ can be computed by integrating  $f$ over the time interval $[0, T]$, which is evaluated by a numerical solver taking many small time steps, thus approximating a neural network with infinite depth.

The first application of the NeuralODE framework for medical image registration was introduced by~\citet{xu2021multi}.
They proposed MS-ODENet, which parameterizes $h$ at the final time point $T$ (\emph{i.e.}, $h(T)$) as the deformation field that warps the moving image to the fixed image, and $\frac{dh(t)}{dt}$ as the small increment of deformation produced by a network at state $t$ from the preceding state $h(t-1)$.
To alleviate the computational burden of numerical solvers and accelerate the runtime, they proposed solving ODEs at different resolutions in a coarse-to-fine manner.
However, the loss function, consisting of a similarity measure and a deformation regularizer, is applied only to the final deformation field $h(T)$.
Similarly,~\citet{wu2022nodeo} proposed NODEO, which formulated $h(t)$ as the voxel movement at time $t$ and the trajectory of the movement as the solution to the ODE.
Drawing inspiration from dynamical systems, they expressed the ODE as $\frac{dh(t)}{dt} = \mathcal{K}v_\theta(h(t), t)$, where $\mathcal{K}$ is a Gaussian smoothing kernel, $v_\theta$ denotes the velocity of the voxel movement produced by a neural network, and the initial condition $h(0)$ is an identity.
It is noteworthy that this formulation bears similarities to LDDMM~\citep{beg2005computing}, an influential optimization-based method that considers image registration as an energy-minimizing flow of particles over time. 
In contrast to MS-ODENet~\citep{xu2021multi}, which applies loss solely to the deformation at $t=T$, NODEO optimizes image similarity at each $t$ while minimizing the energy of the flow and encouraging spatial smoothness and regularity of the velocity fields through the Gaussian kernel, diffusion regularizer, and Jacobian determinant loss. 
The authors compared NODEO to various widely-used traditional methods and a ConvNet model on brain MRI registration tasks.
It demonstrated superior registration performance measured by Dice while attaining diffeomorphic registration.
{\citet{joshi2023r2net}~introduced R2Net, which integrates both time-varying and time-stationary velocity fields using a neural ODE approach. This method employs Lipschitz continuous residual networks to estimate smooth intermediate velocity fields from an initial momentum, which is predicted by a U-Net-based ConvNet. The underlying theory relies on the Cauchy-Lipschitz theorem, positing that Lipschitz continuous integration ensures a well-defined mapping within the space of diffeomorphisms. The Lipschitz continuous residual networks comprise $N$ blocks, each corresponding to a time step in the integration process. In the time-stationary configuration, these $N$ blocks share weights, contrasting with the time-varying setting where the weights are not shared for each block. R2Net was evaluated against both optimization-based and learning-based diffeomorphic registration methods. It showed competitive performance, producing notably smoother deformations compared to the other methods.}

% Figure environment removed

\subsection{Implicit Neural Representations}
%wolterink2022implicit
Image registration can be formulated as an implicit problem of the form:
\begin{equation}
\label{eqn:imp_func}
    \mathcal{C}(\pmb{x}, \psi)=0,\ \ \psi:\pmb{x}\rightarrow \psi(\pmb{x}),
\end{equation}
where $\pmb{x}\in\mathbb{R}^{2,3}$ is the 2D or 3D spatial coordinate (\emph{i.e.}, from an integer grid), and $\psi$ represents a neural network that maps each coordinate $\pmb{x}$ to a value of interest, subject to the constraint $\mathcal{C}$.
In the context of image registration, $\psi$ typically maps the coordinate $\pmb{x}$ to its deformation $\psi(\pmb{x})$, while $\mathcal{C}$ comprises a similarity measure and a deformation regularizer.
The neural network $\psi$ can be considered as an implicit function of $\pmb{x}$, defined by the relation modeled by $\mathcal{C}$ (Eqn.~\ref{eqn:imp_func}).
This concept is commonly referred to as \textit{implicit neural representations} {(INRs)} in computer vision~\citep{sitzmann2020implicit, mescheder2019occupancy, niemeyer2019occupancy, mildenhall2021nerf}.
Although $\pmb{x}$'s used during training are discrete, the implicit function $\psi(\pmb{x})$ parameterized by a neural network{, often a multi-layer perceptron (MLP),} is a continuous and differentiable function.
As a result, {INRs} provide a more compact representation of a continuous function and facilitate smooth manipulation of that function. {Note that the input to the MLP is an image grid, which remains invariant across different image datasets. Consequently, INRs are commonly used in pairwise optimization-based registration~\citep{han2023diffeomorphic,wolterink2022implicit, byra2023implicit,van2024deformable}, as graphically illustrated in Fig.~\ref{f:network_implicit}.}

\citet{han2023diffeomorphic}~proposed to parameterize a continuous deformation field using a MLP introduced in \citep{sitzmann2020implicit}, given an integer grid representing the spatial coordinates of the voxels.
The MLP thus serves as the implicit function of the integer grid.
Since the MLP is not conditioned on the images and the only input is the coordinates that are deterministic for all images of the same resolution, optimization of the MLP is carried out iteratively and pair-wise for each image pair (similar to how the traditional registration methods are performed).
To further improve the registration performance, the authors proposed a cascade framework that combines the benefits of learning-based registration DNNs with the optimization-based {INRs} provided by the MLP.
Within this framework, the learning-based DNN predicts an initial deformation field, while the MLP produces the residual deformation that refines the initial deformation field, leading to an enhanced overall registration performance.
{Leveraging the capability of INRs to represent continuous image grids, \citet{wolterink2022implicit} introduced an implicit DIR model that facilitates continuous differentiation of the deformation. 
This feature is crucial for calculating deformation regularization terms, such as the first derivative in diffusion regularizer and the second derivative in bending energy. This approach marks a significant shift from conventional DL-based registration models, which typically predict a discrete deformation field and hence rely on finite differences for differentiation. The authors explored various activation functions within the MLP framework and found that periodic activation functions enable the network to represent high-frequency signals necessary for capturing small local deformations. In contrast, ReLU activation functions tend to favor low-frequency functions. The proposed model showed promising results on the DIR-LAB dataset, surpassing DL-based methods. However, it demonstrated slightly inferior performance compared to traditional optimization-based methods on this dataset. 
Building on this foundation, \citet{byra2023implicit} further refined the model by proposing the decomposition of a moving image into a residual image and a support image. The residual image accounts for image artifacts and texture patterns that differ from the fixed image, whereas the support image aids in enhancing registration performance. These images are estimated using two implicit networks designed to minimize an exclusion loss, which encourages the gradient structures of these networks to be decorrelated, as suggested by~\citet{gandelsman2019double}. Image registration is then conducted by jointly minimizing a reconstruction loss, ensuring that the sum of the support and residual images reconstructs the moving image, along with two image dissimilarity measures applied to the deformed moving and support images against the fixed image, a deformation regularizer, and the exclusion loss. This method was evaluated on \textit{in situ} hybridization image registration, where it demonstrated improved performance over both the earlier proposed INRs-based and CNN-based models, as well as traditional optimization-based methods.
In~\citet{van2024deformable}, the author used IRN to parameterize the temporal motion of the intestine.
Instead of using spatial coordinates from an integer grid, they align the coordinate system with the tangent space of the anticipated dominant motion, which conforms to the orientation of the intestines.
The authors contend that this adjustment simplifies the complexity of the IRN and leads to better registration performance.
}
{It is important to note to recognize that these aforementioned INRs-based registration methods, much like traditional approaches, suffer from a common limitation: the optimization is conducted on a pairwise basis without learning from a larger dataset. As a result, these methods are unable to take advantage of the guidance offered by anatomical label maps when such maps are unavailable during the inference process.}
Meanwhile, \citet{sun2022topology}~applied {INRs} to a task of organ shape registration.
Their approach was based on the idea of DeepSDF \citep{park2019deepsdf}, where an auto-decoder maps a latent code representing a unique organ shape and the 3D coordinates of a sampled point to a signed distance function~(SDF).
The value of an SDF determines whether the point lies inside ($<0$), outside ($>0$), or on the surface ($=0$) of the shape, consequently providing an implicit description of the organ shapes.
The resulting SDF is a continuous function, and the auto-decoder serves as the implicit neural representation of the discrete coordinates.
To register points from different organ shapes, the authors modeled the trajectory of the point movement in space as the solution to an ODE, akin to the formulation proposed in NODEO~\citep{wu2022nodeo}.
In this formulation, the time derivative corresponds to the velocity of the point movement at time $t$.
The authors solved this ODE using a NeuralODE solver (as briefly discussed in section \ref{sec:nodes}), resulting in a diffeomorphic mapping between shapes. 

% Figure environment removed

\subsection{Hyperparameter Conditioning}
Inspired by HyperNetworks~\citep{ha2017hypernetworks} and Hyperparameter Optimization~\citep{franceschi2018bilevel}, recent research has introduced methods that integrate hyperparameters directly into the architecture of the registration DNNs{, which is graphically depicted in Fig.~\ref{f:network_hyper}.} This allows for the capturing of a wide range of hyperparameters within a single training process, consequently speeding up the hyperparameter tuning process without requiring multiple networks to be trained from scratch for each hyperparameter value. In the training process of these methods, a distinct hyperparameter value is randomly selected, and the network generates a deformation field associated with that value. Subsequently, the registration loss is calculated using the same hyperparameter value, which is then used to update the network parameters. The hyperparameter being conditioned typically relates to the weight of the deformation regularizer, which affects the smoothness of the deformation produced by the network.

\citet{hoopes2022hyper}~introduced HyperMorph, which is based on the concept of HyperNetworks~\citep{ha2017hypernetworks}. HyperMorph comprises two ConvNets: a hypernetwork and a U-Net-like registration network (\textit{i.e.}VoxelMorph~\citep{balakrishnan2019voxelmorph}).
The hypernetwork estimates the weights of the U-Net based on the provided hyperparameter value for the diffusion regularizer, while the U-Net generates a deformation field to warp the moving image.
In each training step, the hyperparameter value is randomly sampled from a uniform distribution, and the loss is computed using the same sampled value.
After training, the best-performing hyperparameter value is acquired using gradient descent.
In this process, the network weights are fixed, and an optimizer iteratively updates the hyperparameter based on a target objective function (commonly the Dice score) applied to a validation dataset.
In a parallel work, \citet{mok2021conditional}~proposed conditioning the regularization hyperparameter through conditional instance normalization~\citep{dumoulin2017a}. 
In this approach, the feature map statistics within the regularization network are normalized and shifted according to two affine parameters.
These affine parameters are generated by a lightweight mapping network, which takes the sampled hyperparameter value as input. 
Later, \citet{chen2023spr}~expanded the conditional instance normalization to a conditional layer normalization for application in Transformer-based registration models. The training processes in both~\citep{mok2021conditional, chen2023spr} are similar to the one used in HyperMorph, where the hyperparameter value is sampled from a uniform distribution and then employed for loss computation. However, it is worth noting that \citet{mok2021conditional} and \citet{chen2023spr} obtain the best-performing hyperparameter value through a grid search, whereas HyperMorph acquires it via gradient descent.

\subsection{Anatomy-aware Networks}%Discontinuity Permitting Network}
{Given the widespread success of DNN methods in medical image segmentation, anatomical label maps can now be relatively easily obtained using a segmentation network. As a result, some registration models have started to incorporate this prior anatomical information into their design to enhance registration accuracy, moving beyond simply using them as components of a loss function during training.}

{In~\citet{su2023nonuniformly}, the authors proposed using anatomical label maps to extract a set of nonuniformly spaced control points, sampled along the contours of anatomical structures. These control points allow for more precise deformation of anatomical structures, particularly at their boundaries, compared to uniformly distributed control points on image grids. The method involves extracting intensity features, which are generated by a ConvNet encoder, alongside spatial features, both of which are aligned with the positions of the sampled control points. In another approach, }to facilitate a spatially discontinuous deformation, which is important for many registration applications as delineated in Section~\ref{sec:loss}, \citet{chen2021deep2}~proposed an alternative approach. 
Rather than employing a discontinuity-permitted deformation regularization (as briefly mentioned in Section~\ref{sec:loss}), the authors proposed using anatomical label maps to segregate the moving and fixed images into different regions of interest and subsequently generate deformation fields for each region using multiple registration networks.
These deformation fields are then combined to yield a final deformation via addition. However, this method has an immediate drawback in necessitating the anatomical label maps throughout both the training and inference stages.
When label maps are not available, this method becomes infeasible.

\subsection{Correlation Layer}
Optical flow is the name given by the computer vision community to image registration.
In learning-based optical flow, it is common to employ a correlation layer~\citep{dosovitskiy2015flownet} to aid neural networks in pinpointing explicit correspondences between points in images.
This involves computing the correlation between the neighboring features of a spatial location in the moving image and the neighboring features of a range of spatial locations in the fixed image.
The correlation is computed between two feature patches centered at $\pmb{x}_m$ and $\pmb{x}_f$ in the moving and fixed images, respectively, using the following equation:
\begin{equation}
\label{eqn:corr_layer}
c(\pmb{x}_m, \pmb{x}_f)=\sum_{\pmb{o}\in[-k, k]} \langle F_m(\pmb{x}_m + \pmb{o}), F_f(\pmb{x}_f + \pmb{o}) \rangle,
\end{equation}
where $F_m$ and $F_f$ denote the feature patches of the moving and fixed images, respectively, and $k$ defines the patch size.
The selection of locations $\pmb{x}_m$ and $\pmb{x}_f$ is based on a maximum displacement $d$, meaning that for each $\pmb{x}_m$, the range of $\pmb{x}_f$ is limited to the locations that are at most $d$ distance away.
The output of the correlation layer is a set of correlation values that represent the correlation between one feature patch in the moving image and another feature patch in the fixed image.
The output has a size of $H\times W\times D\times d$, where $H\times W\times D$ represents the size of the feature maps. 

Although the concept of directing networks with explicit correspondences between voxels or patches has been employed in computer vision since 2015, it was only recently embraced in medical image registration.
This delay can be attributed to the potential computational challenges introduced by Eqn.~\ref{eqn:corr_layer}.
Since medical images are typically volumetric, the search space for each voxel location would be in a 3D volume, quickly becoming unmanageable as the search distance $d$ grows.
\citep{heinrich2019closing}~was the first to implement a correlation layer in their network design by introducing the PDD-net.
Instead of calculating the scalar product between two features as done in Eqn.~\ref{eqn:corr_layer}, PDD-net computes the correlation as the mean squared error between feature patches centered at each control point in the moving and fixed images:
\begin{equation}
\label{eqn:corr_layer_mse}
c(\pmb{x}_m, \pmb{x}_f) = \sum_{\pmb{o}\in[-k, k]} \Vert F_m(\pmb{x}_m + \pmb{o}) - F_f(\pmb{x}_f + \pmb{o})\Vert^2.
\end{equation}
Moreover, in their correlation layer, the search distance is represented by a 3D matrix, $\pmb{d}^3$, with each element in the vector, $\pmb{d}$, defining a discrete displacement distance from the current center of the feature patch.
This correlation layer produces a 6D matrix, where the first three dimensions outline the shape of the feature maps, and the final three dimensions describe the shape of the search space.
Due to the sparsity of the control points in comparison to the image size, the computational burden of this correlation layer remains relatively low. The correlation layer is applied to features independently extracted from the moving and fixed images using a ConvNet that incorporated deformable convolutional layers as introduced in~\citet{heinrich2019obelisk}.
Subsequently, min-convolutions and mean-field inference are employed to spatially smooth the dissimilarities produced by the correlation layer.
A softmax operation is then applied to the 6D matrix, converting the dissimilarities into pseudo-probabilities. 
The displacement field is subsequently generated by multiplying the probabilities with the displacement distance in $\pmb{d}^3$, resulting in a weighted average of these probabilistic estimates for the 3D displacement field.
The deformation field is then trilinearly interpolated to align with the image resolution. 
\citet{heinrich2020highly}~later extended this approach by proposing a 2.5D approximation of the quantized 3D displacement, significantly reducing the memory burden of the original Pdd-net.
Instead of creating a 6D dissimilarity matrix, they generated three 5D matrices (i.e., the 2.5D dissimilarity matrices), with each matrix representing the dissimilarities computed for two out of the three dimensions.
The 2.5D probabilities produced at the end of the network are interpolated to 3D using B-splines.
To minimize the error during the conversion from 2.5D to 3D, a two-step instance normalization is applied for each pair of test scans using gradient descent.
More recently,~\citet{heinrich2022voxelmorph++} further expanded the concept of probabilistic displacement and incorporated keypoint supervision into VoxelMorph~\citep{balakrishnan2019voxelmorph} through the introduction of VoxelMorph++.
They advanced VoxelMorph in two respects: probabilistic displacement via heatmap prediction and multi-channel instance optimization using one-hot embeddings of the anatomical label maps generated by a segmentation network.
In their model, high-level features are initially extracted from the VoxelMorph decoder, and feature vectors are then sampled at given keypoint locations.
These feature vectors are converted into larger heatmap patches through a convolution block followed by a softmax operation.
Consequently, each heatmap represents the probabilistic displacements of the corresponding keypoint.
The final displacement field is generated as the sum of the displacements weighted by the heatmap.
During the testing phase for each image pair, an instance optimization strategy~\citep{siebert2022fast} refines the displacement field using the supervision provided by the anatomical labels generated from a segmentation network.
The methods discussed in this subsection were evaluated on abdomen and lung CT datasets, where large deformations are necessary for accurate registration.
The architectures proposed in these methods proved to be efficient and demonstrated superior performance compared to traditional methods and learning-based networks that only generate dense displacement fields.

% Figure environment removed

\subsection{Progressive and Pyramid Registration}
\label{sec:multi_res_reg}
Recent research has also demonstrated that employing a network to progressively warp a moving image towards a fixed image, or performing registration through a multi-scale image pyramid employing a coarse-to-fine technique, may significantly improve registration performance.
{Figure~\ref{f:network_prg_ms} graphically summarizes these two types of architectures, in which panel~(a) illustrates the progressive registration framework, while panels~(b) and (c)~depict the multi-scale registration frameworks.}
\citet{zhao2019unsupervised}~introduced the VTN, which leverages cascade registration networks to align moving images with fixed images.
Drawing inspiration from FlowNet2.0~\citep{ilg2017flownet}, each subnetwork is responsible for aligning the current moving image with the fixed image, with the resulting warped image sent into the subsequent subnetwork as the new moving image. The final deformation field is the composition of the intermediate deformation fields produced by the subnetworks.
This approach has been shown effective in handling large displacements.
{Similar trends of adopting a progressive registration framework are evident in the development of image registration models, which have shown enhanced registration performance \citep{jia2021learning, chen2022unsupervised, lara2023deep}.} \citet{chen2022unsupervised}~proposed a method for progressive image registration within a single network.
Their method employs multiple convolution blocks in the decoding stage, each responsible for aligning the current moving image to the fixed image.
{In~\citet{jia2021learning}, the authors introduced VR-Net, an end-to-end learning framework that unfolds the iterative optimization process. First, the nonlinear objective function (i.e., Eqn.~\ref{eqn:energy_func}) is linearized with respect to the deformation by applying a first-order Taylor series expansion. Subsequently, an auxiliary variable is introduced to decouple the objective function into two convex problems. This approach mirrors earlier methods that used quadratic relaxation applied to the variational problem~\citep{chambolle2004algorithm, steinbrucker2009large, heinrich2014non}, where one sub-problem focuses on optimizing similarity and the other concentrates on the regularization of the deformation. VR-Net employs a sequence of network blocks that iteratively updates and refines the deformation field, with each block refining the output from the previous one, effectively mirroring the iterative update process for the decoupled objective function. In~\citet{lara2023deep}, the authors adopt a similar strategy to that of VTN~\citep{zhao2019unsupervised} by employing a sequence of intermediate registration networks that progressively warp the moving image towards the fixed image. The authors augmented the VTN framework for dynamic myocardial perfusion CT registration by introducing a contrast loss specifically designed to focus on the dynamic changes in contrast agent concentration within the heart. The proposed method surpasses several established optimization-based registration methods for this particular task.}
%
% \citep{jia2021learning}
% This citation was just sitting at the end of the above sentence, after the period.
% Not sure where it belongs. -AJC
%

Concurrently, there have been efforts to apply progressive registration using a multi-scale image pyramid approach. 
Given the widespread adoption of hourglass-shaped network architectures in image registration, convolution blocks within the decoder generate deformation fields at multiple resolutions in a coarse-to-fine manner.
These deformation fields at different resolutions are subsequently upsampled and composited to form the final deformation field. Notable methods that adopt this scheme include \citep{jiang2020multi, kang2022dual, liu2022coordinate, lv2022joint}.
In addition to network architecture, the coarse-to-fine training scheme has also been adopted in learning-based image registration.
Taking inspiration from conventional registration methods that often employ multiple stages with varying resolutions, \citet{de2019deep}~pioneered a multi-scale training strategy for deformable image registration.
Their approach involves sequentially training the ConvNet in each stage for a specific image resolution by optimizing the image similarity measure.
Notably, a B-spline framework is adopted thus alleviating the need for a deformation regularizer.
During training, the weights of the preceding ConvNets are held fixed, and after training, the registration is performed through a single pass of input images to the multi-stage ConvNets.
\citet{eppenhof2019progressively}~proposed a novel progressive and multi-scale training scheme for learning-based image registration.
Instead of training a large network on the registration task all at once, they first train smaller versions of the network on lower-resolution images.
The resolution of the training images is then gradually increased, and additional convolutional layers are added to increase the network size.
{A similar approach is employed by~\citet{berg2023employing}.}
\citet{mok2020large}~proposed LapIRN, which adopts a similar pyramid training scheme.
However, unlike the previous training approach, which progressively increases the image resolution and network size of the same network, LapIRN employs three different networks, each producing a deformation field for a specific resolution.
Each network is equipped with a skip connection that propagates feature embeddings from a lower-resolution network to a higher-resolution network.
The networks are trained in a coarse-to-fine manner, with each network producing a deformation field that refines the upsampled deformation field from the previous resolution.
However, using multiple networks to generate a pyramid of deformation fields can be computationally inefficient and increase the network size, which can hinder training.
To address this issue, \citet{hu2020self}~proposed a self-recursive contextual network that employs a single feature extractor to produce features at different resolutions.
Then, a weight-sharing deformation generator and receptive module are then used to recursively generate and refine deformation fields in a coarse-to-fine manner.
Since the network weights are shared between resolutions, this method reduces the computational burden and the size of the network, resulting in more efficient training.
\citet{zhou2023self}~proposed a novel network architecture to leverage progressive registration at both single and multi-scale resolution.
The proposed method iteratively refines the deformation field generated from the previous iteration, with each iteration composing deformation fields of various resolutions to form the new deformation field. 
{\citet{meng2023non}~introduced NICE-Trans, a novel approach to address both affine and deformable registration simultaneously using a coarse-to-fine framework. The model features dual-path ConvNet-based encoders that independently extract features from the moving and fixed images across multiple scales. A Transformer-based decoder is employed to estimate affine parameters at its bottleneck, along with multiple displacement fields at various resolutions of the decoder. The final displacement field is derived by summing the upsampled affine grids and the multi-scale displacements. NICE-Trans has shown superior performance compared to previous DL-based DIR methods, which often rely on optimization-based methods for affine registration as a preprocessing step or require a separate affine network to pre-align the images prior to the DIR network.}

{In~\citet{ma2023pivit}, the authors developed a single Transformer-based network architecture for image registration that integrates both progressive and multi-scale strategies. The architecture features a dual-stream ConvNet decoder that independently extracts multi-scale features from both moving and fixed images. Recognizing that large deformations are typically captured at coarser scales, the network employs a series of Swin Transformer-based blocks~\citep{liu2021swin} at the bottleneck scale. These blocks progressively refine the deformation fields through successive additions, where each field is updated based on the output from the previous block, which takes in the deformed moving features together with the fixed image features. Subsequently, additional blocks that include upsampling and a convolution layer further refine the deformation field from the bottleneck to the full scale. The model was rigorously validated to identify the optimal number of progressive updates, with findings indicating that 2 to 3 updates are adequate to achieve satisfactory registration accuracy for the brain MRI and liver CT registration tasks it was evaluated on. For these tasks, the proposed method surpassed several contemporary registration models, including those based on ConvNets and Transformers.}

The registration methods discussed in this subsection have demonstrated the efficacy of decomposing the registration process into multiple steps, where each step refines the deformation fields from the previous step.
These approaches have consistently shown significant performance gains while enforcing a smoother deformation field for image registration tasks compared to using a single network to generate a deformation field all at once.
% Interestingly, these methods bear a conceptual resemblance to traditional methods that frame deformation as the trajectory of particles in motion (\emph{e.g.}, LDDMM~\citep{beg2005computing, miller2006geodesic}).
% In both cases, the final deformation is modeled as the composition of intermediate velocities of movement, progressively updating the deformation from the identity.
