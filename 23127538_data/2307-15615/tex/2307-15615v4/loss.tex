\begin{table*}[!t]
\caption{A compilation of unsupervised deformable image registration models (models are listed in alphabetical order). The table summarizes the models' choices of similarity and auxiliary loss functions, regularization techniques, accuracy measures, and regularity measures.}
\rowcolors{2}{cyan!10}{white}
\label{table:unsup_list}
\centering
\fontsize{6}{7.5}\selectfont 
 \begin{tabular}{ l ? c | c | c | c | c | c ? c | c ? c | c | c | c | c ? c | c | c | c | c ? c | c | c | c }
 \Xhline{1pt}
 &\multicolumn{6}{c?}{Similarity Loss} &\multicolumn{2}{c?}{Aux. Loss} &\multicolumn{5}{c?}{Regularizer}&\multicolumn{5}{c?}{Accuracy Measure}&\multicolumn{4}{c}{Regularity Measure}\\
 &\rotatebox[origin=c]{90}{MSE} & \rotatebox[origin=c]{90}{{LCC}} & \rotatebox[origin=c]{90}{Correlation} & \rotatebox[origin=c]{90}{NGF} & \rotatebox[origin=c]{90}{MI} & \rotatebox[origin=c]{90}{MIND-SSC}& \rotatebox[origin=c]{90}{Anatomy}& \rotatebox[origin=c]{90}{Landmark} & \rotatebox[origin=c]{90}{Diffusion} & \rotatebox[origin=c]{90}{Curvature} & \rotatebox[origin=c]{90}{Bending} & \rotatebox[origin=c]{90}{Jacobian} & \rotatebox[origin=c]{90}{Consistency} & \rotatebox[origin=c]{90}{TRE}& \rotatebox[origin=c]{90}{MSE}& \rotatebox[origin=c]{90}{SSIM} & \rotatebox[origin=c]{90}{Dice} & \rotatebox[origin=c]{90}{HdD} & \rotatebox[origin=c]{90}{$\%\text{ of }|J_\phi\leq0$}& \rotatebox[origin=c]{90}{$\#\text{ of }|J_\phi|\leq0$}& \rotatebox[origin=c]{90}{std.$(|J_\phi|)$}& \rotatebox[origin=c]{90}{$|\nabla J_\phi|$}\\
 
 \Xhline{1pt}%\cline{1-4}\cline{6-9}
 {AC-DMiR~\citep{khor2023anatomically}}&$\sbullet[1.5]$&$\sbullet[1.5]$& & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& &$\sbullet[1.5]$&$\sbullet[1.5]$& & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & &\\

 ADMIR~\citep{tang2020admir}                   & &$\sbullet[1.5]$& & & & & & &$\sbullet[1.5]$& & & & &$\sbullet[1.5]$& & &$\sbullet[1.5]$&$\sbullet[1.5]$& & & &\\

{AMNet~\citep{che2023amnet}} & &$\sbullet[1.5]$& & & & & & &$\sbullet[1.5]$& & &$\sbullet[1.5]$& & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & &\\
 
 Attention-Reg~\citep{song2022cross}           & & & & & & & $\sbullet[1.5]$& &$\sbullet[1.5]$& & & & &$\sbullet[1.5]$& & &$\sbullet[1.5]$& & & &$\sbullet[1.5]$&\\

 Baum~\emph{et al.}~\cite{baum2022meta}            & & & & & & &$\sbullet[1.5]$& & & &$\sbullet[1.5]$& & &$\sbullet[1.5]$& & &$\sbullet[1.5]$& & & & &\\

 BIRNet~\citep{fan2019birnet}                  &$\sbullet[1.5]$& & & & & & & & & & & & & & & &$\sbullet[1.5]$& & & & &\\
 
 CondLapIRN~\citep{mok2021conditional}         &  &$\sbullet[1.5]$ & & & & & & & $\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& &$\sbullet[1.5]$& \\

 CycleMorph~\citep{kim2021cyclemorph}          &$\sbullet[1.5]$&$\sbullet[1.5]$ & & & & & & & $\sbullet[1.5]$& & & &$\sbullet[1.5]$& &$\sbullet[1.5]$&$\sbullet[1.5]$&$\sbullet[1.5]$& &$\sbullet[1.5]$& & &\\
 
 de Vos~\emph{et al.}~\citep{de2020mutual}            & & & & &$\sbullet[1.5]$& & & & & &$\sbullet[1.5]$& & & & & &$\sbullet[1.5]$&$\sbullet[1.5]$&$\sbullet[1.5]$& & &\\ 

 Deformer~\citep{chen2022deformer}             & &$\sbullet[1.5]$& & & & & & &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$ & &$\sbullet[1.5]$&\\
 
 DiffuseMorph~\citep{kim2022diffusemorph}      & &$\sbullet[1.5]$& & & & & & & $\sbullet[1.5]$& & & & & &$\sbullet[1.5]$&$\sbullet[1.5]$&$\sbullet[1.5]$& &$\sbullet[1.5]$& & &\\

 DIRNet~\citep{de2017end}                      & & $\sbullet[1.5]$& & & & & & & & & & & & & & &$\sbullet[1.5]$& & & & & \\
 
 DLIR~\citep{de2019deep}                       & &$\sbullet[1.5]$& & & & & & & & &$\sbullet[1.5]$& & & & & &$\sbullet[1.5]$&$\sbullet[1.5]$&$\sbullet[1.5]$& &$\sbullet[1.5]$&\\

 DNVF~\citep{han2023diffeomorphic}             & &$\sbullet[1.5]$& & & & & & & $\sbullet[1.5]$& & &$\sbullet[1.5]$& & & &$\sbullet[1.5]$&$\sbullet[1.5]$& &$\sbullet[1.5]$& & &\\
 
 DTN~\citep{zhang2021learning}                 &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& & & &$\sbullet[1.5]$&\\

 Dual-PRNet~\citep{hu2019dual}                 &  &$\sbullet[1.5]$& & & & & & & $\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$ & & & & & \\
 
 Dual-PRNet++~\citep{kang2022dual}              &  &$\sbullet[1.5]$& & & & & & & $\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$&$\sbullet[1.5]$&$\sbullet[1.5]$& & & \\

 FAIM~\citep{kuang2019faim}                    & &$\sbullet[1.5]$& & & & & & &$\sbullet[1.5]$& & &$\sbullet[1.5]$& & & & &$\sbullet[1.5]$& & &$\sbullet[1.5]$& &\\
 
 Fan~\emph{et al.}~\citep{fan2019adversarial}  & & & & & & & & &$\sbullet[1.5]$& & & & & & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& &\\

 Fourier-Net~\citep{jia2022fourier}            &$\sbullet[1.5]$&$\sbullet[1.5]$& & & & & & &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$&$\sbullet[1.5]$&$\sbullet[1.5]$& & &\\

 {FSDiffReg~\citep{qin2023fsdiffreg}} & &$\sbullet[1.5]$& & & & & & &$\sbullet[1.5]$&$\sbullet[1.5]$& & & & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& &$\sbullet[1.5]$&\\
 
 GraformerDIR~\citep{yang2022graformerdir}     & &$\sbullet[1.5]$& & & & & & &$\sbullet[1.5]$& & &$\sbullet[1.5]$& & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & &\\

 Han~\emph{et al.}~\citep{han2022deformable}   & &$\sbullet[1.5]$& & & & & & &$\sbullet[1.5]$& & & & &$\sbullet[1.5]$& & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & & \\
 
 Hering~\emph{et al.}~\citep{hering2021cnn}           &  & & &$\sbullet[1.5]$& & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & & & & & &$\sbullet[1.5]$&$\sbullet[1.5]$&$\sbullet[1.5]$& & & \\

 HyperMorph~\citep{hoopes2022hyper}       &$\sbullet[1.5]$&$\sbullet[1.5]$& & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & & &$\sbullet[1.5]$&\\ 

 {IDIR~\citep{wolterink2022implicit}}       & &$\sbullet[1.5]$& & & & & & & & &$\sbullet[1.5]$&$\sbullet[1.5]$& &$\sbullet[1.5]$& & & & & &$\sbullet[1.5]$& &\\
 
 im2grid~\citep{liu2022coordinate}             &$\sbullet[1.5]$& & & & & & & & $\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$&$\sbullet[1.5]$& &\\

 Krebs~\emph{et al.}~\citep{krebs2019learning}        &  &$\sbullet[1.5]$& & & & & & & $\sbullet[1.5]$& & & & & & $\sbullet[1.5]$& &$\sbullet[1.5]$ &$\sbullet[1.5]$& & & &$\sbullet[1.5]$ \\

 LapIRN~\citep{mok2020large}                   &  &$\sbullet[1.5]$ & & & & & & & $\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& & &$\sbullet[1.5]$&$\sbullet[1.5]$&\\

 LKU-Net~\citep{jia2022u}                      & &$\sbullet[1.5]$& & & & & $\sbullet[1.5]$& & $\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$&$\sbullet[1.5]$&$\sbullet[1.5]$ & &$\sbullet[1.5]$&\\
 
 
 Li~\emph{et al.}~\citep{li2018non}            & &$\sbullet[1.5]$& & & & & & &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& & & & &\\

 Liu~\emph{et al.}~\citep{liu2019probabilistic}&$\sbullet[1.5]$& & & & & & & & & & & & & & & &$\sbullet[1.5]$& & & & &\\

 {MAIRNet~\cite{gao2024mairnet}} & & & & & &$\sbullet[1.5]$& & & &$\sbullet[1.5]$& & & & & & &$\sbullet[1.5]$& & & & &\\
 
 MIDIR~\citep{qiu2021learning}                 &  & & & &$\sbullet[1.5]$ & & & & $\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$ & &$\sbullet[1.5]$& & & $\sbullet[1.5]$ \\

 {ModeT~\citep{wang2023modet}}                 & &$\sbullet[1.5]$& & & & & & & $\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$ & &$\sbullet[1.5]$& & & \\
 
 MS-DIRNet~\citep{lei20204d}                   & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & &$\sbullet[1.5]$&$\sbullet[1.5]$& & & & & & &\\
 
 MS-ODENet~\citep{xu2021multi}                 &$\sbullet[1.5]$& & & & & & & & $\sbullet[1.5]$& & & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & & & &\\

 {NICE-Trans~\citet{meng2023non}}
 & &$\sbullet[1.5]$& & & & & & & $\sbullet[1.5]$& & &$\sbullet[1.5]$& & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & &\\
 
 NODEO~\citep{wu2022nodeo}                     & &$\sbullet[1.5]$& & & & & & & $\sbullet[1.5]$& & &$\sbullet[1.5]$& & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & &\\

 {PC-Reg~\citep{yin2023pc}}     & &$\sbullet[1.5]$& & & & &$\sbullet[1.5]$& & $\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$&$\sbullet[1.5]$&$\sbullet[1.5]$& & &\\

 PC-SwinMorph~\citep{liu2022pc}                & &$\sbullet[1.5]$& & & & & & & $\sbullet[1.5]$& & & &$\sbullet[1.5]$& & & &$\sbullet[1.5]$& & & & &\\
 
 PDD-Net 2.5D~\citep{heinrich2020highly}       & & & & & &$\sbullet[1.5]$&$\sbullet[1.5]$& & $\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& & & &$\sbullet[1.5]$&\\

 PDD-Net 3D~\citep{heinrich2019closing}        & & & & & &$\sbullet[1.5]$&$\sbullet[1.5]$& & $\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& & & &$\sbullet[1.5]$&\\

 {PIViT~\citep{ma2023pivit}}        & &$\sbullet[1.5]$& & & & & & & $\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& & &$\sbullet[1.5]$& &\\

 {R2Net~\citep{joshi2023r2net}}&$\sbullet[1.5]$& & &$\sbullet[1.5]$& & & & & & & & $\sbullet[1.5]$& & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & &$\sbullet[1.5]$& &\\
 

 SDHNet~\citep{zhou2023self}                   & &$\sbullet[1.5]$& & & & & & & $\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$&$\sbullet[1.5]$&$\sbullet[1.5]$& & & \\

 Shao~\emph{et al.}~\citep{shao2022multi}      &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& & & & & &$\sbullet[1.5]$& & & & &$\sbullet[1.5]$&$\sbullet[1.5]$&\\

{SpineRegNet~\cite{zhao2023spineregnet}} & &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& & & & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & &\\

 SVF-R2Net~\citep{joshi2022diffeomorphic}      &$\sbullet[1.5]$& & & & & & & & & & &$\sbullet[1.5]$& & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & &$\sbullet[1.5]$& &\\

 SYMNet~\citep{mok2020fast}                    &  &$\sbullet[1.5]$ & & & & & & & $\sbullet[1.5]$& & &$\sbullet[1.5]$&$\sbullet[1.5]$& & & &$\sbullet[1.5]$& & &$\sbullet[1.5]$ & &\\

 SymTrans~\citep{ma2022symmetric}              &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& & &$\sbullet[1.5]$& &\\

 SynthMorph~\citep{hoffmann2021synthmorph}     & & & & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & &\\

 TM-DCA~\citep{chen2023deform}                 & &$\sbullet[1.5]$& & & & & $\sbullet[1.5]$& & $\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& &$\sbullet[1.5]$&\\

 TM-TVF~\citep{chen2022unsupervised}   &$\sbullet[1.5]$&$\sbullet[1.5]$& & & & & $\sbullet[1.5]$& & $\sbullet[1.5]$& & & & & & &$\sbullet[1.5]$&$\sbullet[1.5]$&$\sbullet[1.5]$ &$\sbullet[1.5]$& &$\sbullet[1.5]$&\\

 {TransMatch~\citep{chen2023transmatch}}         &$\sbullet[1.5]$&$\sbullet[1.5]$& & & & & & & $\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$ & &$\sbullet[1.5]$&\\
 
 TransMorph~\citep{chen2022transmorph}         &$\sbullet[1.5]$&$\sbullet[1.5]$& & & & & $\sbullet[1.5]$& & $\sbullet[1.5]$& &$\sbullet[1.5]$& & & & &$\sbullet[1.5]$&$\sbullet[1.5]$& &$\sbullet[1.5]$ & & &\\

 ViT-V-Net~\citep{chen2021vitvnet}             &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & &\\

 VoxelMorph~\citep{balakrishnan2019voxelmorph} &$\sbullet[1.5]$& $\sbullet[1.5]$ & & & & & $\sbullet[1.5]$& &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$&$\sbullet[1.5]$& & \\

 VoxelMorph-diff~\citep{dalca2019unsupervised} &$\sbullet[1.5]$& & & & & &$\sbullet[1.5]$& & $\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$&  &$\sbullet[1.5]$&$\sbullet[1.5]$& & \\

 VoxelMorph++~\citep{heinrich2022voxelmorph++} & & & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$&$\sbullet[1.5]$& & & & &$\sbullet[1.5]$& & & & & &$\sbullet[1.5]$&$\sbullet[1.5]$&\\
 
 VR-Net~\citep{jia2021learning}                &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$&$\sbullet[1.5]$&$\sbullet[1.5]$& & &$\sbullet[1.5]$\\

 VTN~\citep{zhao2019unsupervised}              & & &$\sbullet[1.5]$& & & & & & $\sbullet[1.5]$& & &$\sbullet[1.5]$&$\sbullet[1.5]$& & & &$\sbullet[1.5]$ & &$\sbullet[1.5]$& &$\sbullet[1.5]$& \\
 
 XMorpher~\citep{shi2022xmorpher}              & &$\sbullet[1.5]$& & & & & $\sbullet[1.5]$& & $\sbullet[1.5]$& & & &$\sbullet[1.5]$& & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & &\\

 Zhang~\emph{et al.}~\citep{zhang2020diffeomorphic}   &$\sbullet[1.5]$& & & & & & & &$\sbullet[1.5]$& & &$\sbullet[1.5]$& & & & &$\sbullet[1.5]$& &$\sbullet[1.5]$& & &\\
 
 \Xhline{1pt}
\end{tabular}
\end{table*}

%
% I think we need something like a short intro passage here.
% Not sure if this is sufficient. -AJC
%
Table~\ref{table:unsup_list} provides a compilation of unsupervised DIR models, summarizing the similarity and auxiliary loss functions, as well as other details.

{The topic of similarity measures is only touched upon briefly in this section and is in itself a wide area of active research, interested readers may find the review articles by~\citet{santini1999tpami} and~\citet{unnikrishnan2005wacv} to be a useful resource.}
See the text for complete details and discussion.


\subsection{Supervised Learning}
In supervised learning, where the ground truth transformation is used, the loss function is typically easy to define, with the mean square error~(MSE)~\citep{miao2016cnn, krebs2017robust, eppenhof2018deformable, rohe2017svf, cao2017deformable, fan2019birnet}, the equivalent end-point-error (EPE), and mean absolute error (MAE)~\citep{yang2017quicksilver, sokooti2017nonrigid} being the most popular choices.
{Recently,~\citet{terpstra2022loss} highlighted that a loss based on $\ell^2$ (equivalent to MSE or EPE) does not distribute errors symmetrically across deformation directions when calculated separately for each direction.
To address this, they introduced $\bot$-loss, which considers the interplay between directions.
In this method, the $x$ and $y$ directions of the deformation field are viewed as the real and imaginary components of a complex image, thereby creating a polar representation of deformations.
$\bot$-loss quantifies the phase error between the predicted and actual complex image representations of the deformation fields.
When combined with Euclidean distance, forming the $\perp$$+\ell^2$ loss, it ensures symmetric error distribution across both deformation directions. The authors demonstrated that a network trained with a combination of $\bot$-loss and $\ell^2$ loss outperforms a network trained with $\ell^2$ loss alone in terms of registration performance. However, the applicability of this method to 3D or 4D registration tasks remains to be further explored, given the challenge of representing additional dimensions within a 2D complex image framework.}

\subsection{Unsupervised \& Semi-supervised Learning}
In unsupervised learning, where there is no ground truth transformation to reference, regularization is usually used to enforce smoothness in the transformation. As a result, the loss function is often similar to the energy function used in traditional methods (\emph{i.e.}, Eqn.~\ref{eqn:energy_func}), which includes an image similarity measure and a transformation regularizer. The following subsections provide a summary of commonly used and recently proposed loss functions for image registration.


\subsection{Similarity Measure}

\noindent\textbf{Mono-modality.} The choice of image similarity measure can vary depending on each specific application. For mono-modal registration, MSE is still a popular choice and has the advantage of having a straightforward probabilistic interpretation of the Gaussian likelihood approximation~\citep{dalca2019unsupervised, chen2022transmorph, kim2021cyclemorph, balakrishnan2019voxelmorph, meng2022enhancing, jia2021learning, liu2022coordinate}.
However, a disadvantage of MSE is that it averages the difference across all voxels in the image, making it sensitive to local intensity variations within the image.
{Local correlation coefficient (LCC)} is known to be more robust to local intensity variations and has been found to be superior in brain MR registration applications~\citep{avants2008symmetric}.
{LCC} has been extended as a loss function for training learning-based models, with the local window computation often being done through convolution operations~\citep{kuang2019faim, chen2022transmorph, kim2021cyclemorph, balakrishnan2019voxelmorph, zhang2018inverse, mok2020fast, mok2020large, mok2021conditional}. One disadvantage of {LCC} is its higher computing cost in comparison to MSE, which is mainly attributable to the comparatively large convolution kernel size (typically chosen between $5\times5\times5$ and $9\times9\times9$ voxels~\citep{avants2008symmetric, balakrishnan2019voxelmorph, mok2020fast}).
{To improve the computational efficiency, Gaussian weighted LCC can be used and implemented as three 1D convolutions.
The locality is controlled by the standard deviation of the Gaussian window.
However, the computation time complexity is independent of the standard deviation, because the 1D Gaussian kernel can be approximated by a fast recursive filter~\citep{cachier2000non}.}
The structural similarity index (SSIM)~\citep{wang2004image} has also been demonstrated to be an effective loss function for mono-modal image registration~\citep{chen2020generating, mahapatra2018deformable, sandkuhler2018airlab}. SSIM takes into account luminance, contrast, and structure. It can be thought of as an extension of the {LCC}, with the structure term in SSIM being the square root of {LCC}. This allows SSIM to capture more information about the similarity of two images beyond just the degree of correlation between them.
{The mono-modality loss function can also be adapted for multi-modality registration by leveraging image synthesis techniques. \citet{liu2023geometry} introduced a network architecture that accepts two images from different modalities to estimate a deformation field. Rather than employing a loss function  designed for multi-modality images, the authors integrate an image-to-image translation network. This network aims to match the modality of the moving image with that of the fixed image, thereby facilitating the use of a mono-modality loss. To ensure that the image-to-image translation network is solely responsible for modifying intensity without compensating for spatial transformations, a specialized training scheme is implemented.}

\noindent\textbf{Multi-modality.} For multi-modal applications, traditional methods often use mutual information~(MI)~\citep{viola1997alignment}, correlation ratio~\citep{roche1998correlation}, self-similarity context~(SSC)~\citep{heinrich2013towards}, or normalized gradient fields~(NGF)~\citep{haber2006intensity} as similarity measures. Both MI and correlation ratio evaluate the relationship between the two images by calculating intensity statistics, such as intensity histograms, to measure statistical dependence.
However, the standard method for calculating intensity histograms, which involves counting, is not differentiable, so a Parzen window formulation~\hbox{\citep{thevenaz2000optimization}} is often used to allow the loss to be backpropagated during network training.
Parzen-window-based MI has been employed as a loss function in many multi-modal applications~\citep{qiu2021learning, de2020mutual, nan2020drmime, guo2019multi, hoffmann2021synthmorph}, but it can be relatively difficult to implement and also sensitive to factors such as the number of intensity bins and the smoothness of the {window} function. {Meanwhile, local MI (LMI) has been adapted for deep learning-based image registration to incorporate detailed spatial information~\cite{guo2019multi}. The calculation of MI in general often requires a vectorized approach to bypass loop operations for speed and automatic differentiation, which significantly increases memory consumption as the spatial dimension must expand to accommodate the number of bins used. Therefore, LMI is typically implemented using non-overlapping square patches to conserve memory, although overlapping patches are also feasible~\cite{guo2019multi}.} 
As far as we are aware, the correlation ratio has not been used in learning-based medical image registration.
It should be noted that these intensity-statistic-based measurements do not take into account local structural information, making them more suitable for rigid/affine registration and less suitable for deformable registration applications~\citep{pluim2000image, heinrich2013towards}.
SSC is another commonly used loss function for multi-modal applications, and it is an improvement on the modality-independent neighborhood descriptor~(MIND)~\citep{heinrich2012mind}.
Both SSC and MIND operate by calculating the descriptor between a voxel and its neighboring voxels within a given image, turning an image of any modality into a feature representation of these descriptors. 
The similarity is determined by summing the absolute differences between the descriptors of the two images.
As SSC and MIND consider local structural information, they are not limited in the same way as MI or correlation ratio, making them more useful for multi-modal deformable registration~\citep{hansen2021graphregnet, mok2021conditional2, yang2020unsupervised, xu2020adversarial, blendowski2021weakly}.
{Note that SSC and MIND generate descriptors by analyzing the spatial intensity distribution within a localized area surrounding each voxel, making them sensitive to local orientations (i.e., they are not rotationally invariant).
Consequently, when large rotations are expected between image pairs, one should calculate SSC and MIND for a warped moving image, instead of applying the warp to the SSC and MIND descriptors of the moving image.}
NGF compares images by focusing on the intensity changes, or edges, in the images. The similarity between the two images is determined by the presence of intensity changes at the same locations, regardless of the modalities of the images being compared.
NGF was originally developed for multi-modal applications like brain MR T1-to-T2 and PET-to-CT~\citep{haber2006intensity}.
However, it is now mostly used in learning-based registration models for lung CT registration~\citep{hering2019mlvirnet, hering2021cnn, mok2021conditional2}. 
This is because the complex structure of the lung, including bronchi, fissures, and vessels, can hinder accurate registration~\citep{hering2021cnn}. 
NGF focuses on edges rather than intensity values, making it a more suitable measure for this purpose.

\noindent\textbf{Recent Advancements.} There have been many efforts to improve upon or propose new loss functions due to some limitations of the aforementioned similarity measures.
{\citet{czolbe2021semantic,czolbe2023semantic}~leveraged a ConvNet feature extractor to obtain image features from the deformed and fixed images, and then computed the {LCC} between these features as a similarity measure.}
The benefit of this approach is that the features produced by the ConvNet feature extractor have less noise, resulting in a more consistent similarity measure in areas with noise which leads to a smoother transformation.
\citet{haskins2019learning}~were the first to propose using a ConvNet to learn a similarity measure for image registration.
However, this method relies on having ground truth target registration error for the training dataset to learn such a similarity measure.
{\citet{ronchetti2023disa} used a ConvNet followed by the inner product operation to approximate LC$^2$ similarity~\citep{fuerst2014automatic}. They showed that the trained network can be used for loss function in multi-modal registration.}
\citet{grzech2022variational}~went one step further and introduced a technique for learning a similarity measure using a variational Bayesian method. The method involves initializing the convolution kernels in the network architecture to model MSE and {LCC}, and then using variational inference to learn a similarity measure that optimizes the likelihood of the images in the dataset when aligning them to the atlas. Building on the success of adversarial networks in computer vision~\citep{makhzani2015adversarial, goodfellow2020generative}, researchers have developed a number of techniques for image registration that leverage adversarial training~\citep{fan2019adversarial, mahapatra2020training, luo2021deformable}. These methods can be used standalone or in conjunction with a traditional similarity measure.


\subsection{Deformation Regularizer}
% greer2021icon tian2022gradicon
\label{sec:def_reg}
A deformation regularizer, as the terminology implies, is used for DIR, with its usage being not necessary for rigid/affine transformations.
For DIR algorithms, producing smooth deformations is not only a desirable property but a necessary requirement: while diffeomorphic transformations may not be required for certain applications, smoothness remains imperative in almost all cases to avoid trivial solutions such as rearranging voxels~\citep{rohlfing2011image}, with which an almost perfect similarity measure can be achieved but result in unrealistic transformation~(also see Section~\ref{sec:Eval_Metric}).
The regularizer can be considered as a prior in a maximum a posteriori~(MAP) framework, while the similarity measure acts as the data likelihood (\emph{e.g.}, in the case of MSE, the data likelihood becomes a Gaussian likelihood). The diffusion regularizer is a commonly employed deformation regularizer, as demonstrated by its frequent appearance in Table~\ref{table:unsup_list}. This regularization computes the squared $\ell^2$-norm of the gradients of the displacement field, effectively penalizing the disparities between adjacent displacements. Other alternatives for regularization include using the $\ell^1$-norm instead of the $\ell^2$-norm to impart equal penalties on the neighboring disparities, or penalizing the second derivative of the displacements, commonly referred to as bending energy~\citep{rueckert1999nonrigid}. It is important to note that since bending energy and curvature-based regularizers penalize the second derivatives, thereby zeroing out any affine contributions, pre-affine alignment prior to the deformable registration step may not be necessary, as demonstrated in \citep{ding2022aladdin, fischer2003curvature}.
These conventional regularizers enforce an isotropic regularization on the displacement field~\citep{pace2013locally}.
As a result, they discourage discontinuities in the displacements in applications where sliding motion may occur in organs, such as registering exhale and inhale CT scans of the lung.
Historically, various improvements have been made to address this issue, including the isotropic Total Variation~(TV) regularization~\citep{vishnevskiy2016isotropic}, anisotropic diffusion regularization~\citep{pace2013locally}, and adaptive bilateral filtering-based regularization~\citep{papiez2014implicit}.
However, these regularization techniques have not been widely adopted in learning-based image registration.

\noindent\textbf{Recent Advancements.}
Enforcing spatial smoothness alone is insufficient to ensure the regularity of the transformations. A different strategy is to penalize the ``folding'' of voxels directly during training, in addition to applying the aforementioned regularizers to enforce smoothness in the deformation.
These foldings can be evaluated using local Jacobian determinants, where the magnitude of the Jacobian determinant indicates if the volume is expanding or shrinking near the voxel location.
A non-positive Jacobian determinant represents a locally non-invertible transformation. Several regularization methods based on local Jacobian determinants have been proposed to penalize such transformations~\citep{kuang2019faim, mok2020fast}.
Meanwhile, with the advent of deep learning, new methods have emerged that leverage the deep learning of deformation regularization from data.
One such method by  \citet{niethammer2019metric},~introduced a method that learns a spatially-varying deformation regularization using training data.
Spatially-varying regularization offers the advantage of accommodating variations in deformation that may be required for different regions within an image, such as the movement of the lungs in relation to other organs (\emph{e.g.}, rib cage) due to respiratory processes.
The technique proposed by Niethammer~\emph{et al.} involves training a registration network to produce not only a deformation field but also a set of weight maps, each of which corresponds to the weight of a Gaussian smoothing kernel in a multi-Gaussian kernel configuration.
The weighted multi-Gaussian kernel is then applied to the deformation field via convolution. To further impose spatial smoothness, an optimal mass transport~(OMT) loss function was introduced to encourage the network to assign larger weights to Gaussian kernels with larger variances.
While this method was developed for a time-stationary velocity field setting, \citet{shen2019region} later expanded upon it by incorporating it into a time-varying velocity field setting.
In this setup, a different set of weight maps are produced for each time point.
More recently, \citet{chen2023spr}~introduced a weighted diffusion regularizer that applies spatially-varying regularization to the deformation field.
The neural network generates a weight volume, assigning a unique regularization weight to each voxel and thus allows for spatially-varying levels of regularization strength.
As the diffusion regularizer is related to Gaussian smoothing, using spatially-varying strengths of diffusion regularization can be considered equivalent to employing a multi-Gaussian kernel, as originally proposed by~\citet{niethammer2019metric}.
This is because the convolution of multiple Gaussian kernels still results in a Gaussian kernel. To promote the overall smoothness of the deformation, they further applied a log loss to the weight volume, which encourages the maximum regularization strength when possible. 
In a different approach, \citet{wang2022deep}~employed a regression network to learn the optimal regularization parameter for an optimization-based method, specifically Flash~\citep{zhang2019fast}.
Flash is a geodesic shooting method in the Fourier space that requires only the initial velocity field to compute the time-dependent transformation.
Wang~\emph{et al.} generated ground truth optimal regularization parameters by assuming the prior of the initial velocity field given the regularization parameter as a multivariate Gaussian distribution.
Using gradient descent, they obtained the optimal regularization parameter for each image pair through MAP estimation.
A ConvNet regression encoder then estimates the optimal regularization parameter based on the image pair. 
This approach achieved registration performance comparable to Flash while significantly improving runtime and memory efficiency.
Alternatively, \citet{laves2019deformable}~were inspired by the deep image prior~\citep{ulyanov2018deep}. 
They used a randomly initialized ConvNet as a regularization prior.
They then fed a random image (\emph{i.e.}, a noise image) as input and the network gradually transformed it into a smooth deformation field through iterative optimization.
The deep image prior provided by the ConvNet enables the network to produce a smooth deformation in the early iterations, then gradually adds non-smooth high-frequency deformations.
As a result, early stopping is used for the network to generate a smooth deformation field without the need for explicitly encouraging smoothness in the loss function.

Transformations can also be implicitly regularized by imposing invertibility constraints.
This is achieved by using a symmetric consistency loss or cycle consistency loss.
Symmetric consistency typically uses a single DNN to output both the forward and reverse deformation fields, which transform the moving image to the fixed image and vice versa, respectively.
The similarity between the warped image and the target image is then calculated and backpropagated to update the network~\citep{mok2020fast, liu2022pc}.
Alternatively, a consistency loss can be calculated by composing the network-generated forward and backward deformation fields, and then comparing the outcome with the identity transformation~\citep{greer2021icon,tian2022gradicon}.
The underlying concept is that, theoretically, an invertible mapping should cancel itself when composed with its inverse.
Such an approach by itself imposes invertibility but does not explicitly enforce spatial smoothness over the deformation field.
\citet{greer2021icon}~demonstrated that incorporating such a loss within a DNN framework implicitly imposes spatial regularity {and inverse consistency} on the deformation field without necessitating an additional regularizer to enforce smoothness.
The authors showed that the errors of the DNN in computing the inverse, combined with the implicit bias of DNN favoring more regular outputs, enable such a consistency loss to entail a $H^1-$ or Sobolev-type regularization over the deformation field, thereby implicitly enforcing spatial smoothness.
Later, \citet{tian2022gradicon}~expanded on this regularizer and proposed to regularize deviations of the Jacobian of the composition from the identity matrix.
This improved regularizer led to faster convergence while offering greater flexibility, while maintaining an approximated diffeomorphic transformation.
{\citet{greer2023inverse}~introduced a simple method to ensure DNN-based registration models are inverse consistent by construction.
They began by defining the output of the registration network as a member of a specified Lie group that inherently supports inverse consistency.
Specifically, suppose a DNN $f$ generates a deformation field, this is mathematically represented as:
\begin{equation}
    f[I_m, I_f]:=\exp{(g(I_m,I_f))}.
\end{equation} The DNN ensures the generated deformation field retains inverse consistency when $g(I_f,I_m)=-g(I_m,I_f)$, as demonstrated by:
\begin{equation}
    f[I_m, I_f]\circ f[I_f, I_m]\equiv\exp{(g(I_m,I_f))}\circ\exp{(-g(I_m,I_f))}=id.
\end{equation} To ensure $g(I_f,I_m)$ meets this condition, the authors propose:
\begin{equation}
    g(I_f,I_m):=M_\theta(I_f,I_m)-M_\theta(I_m,I_f),
\end{equation}
where $M_\theta$ is an arbitrary DNN with parameters $\theta$. This approach structurally guarantees that the deformation field generated by the DNN maintains inverse consistency.
The authors recognize that their previous two-stage approach~\citep{greer2021icon, tian2022gradicon}, which initially estimates the deformation field at a coarse level and subsequently refines it with an additional field, does not guarantee inverse consistency due to the interleaving of inverses. To address this, they introduced a \texttt{TwoStepConsistent} operator that works on the square root of $\exp{(g(I_m,I_f))}$.
This operator can also be adapted for multi-step registration methods, which involve compositions as described later in Section~\ref{sec:multi_res_reg}.
The updated model has been evaluated on both synthetic and real-world brain MRI datasets and has shown significantly more precise inverse consistency compared to models that employ penalty-based approaches for ensuring inverse consistency~\citep{greer2021icon, tian2022gradicon}.}

On the other hand, cycle consistency employs two identical networks, where the first network generates a forward deformation field that deforms the moving image and the second network produces a reverse field that aims to warp the deformed image back to the original moving image~\citep{zhao2019unsupervised, kuang2019cycle, kim2021cyclemorph}.
Both consistency losses have been shown to improve the registration performance and provide regularization to the deformation field. However, since this regularization is not explicitly applied to the deformation fields, a separate deformation regularizer is often required in addition to the consistency loss.

{
Regularization terms in the objective function frequently necessitate tuning additional hyperparameters (e.g., $\lambda$ in Eqn. \ref{eqn:energy_func}) to effectively balance the trade-off between the image similarity measure and the regularity of the deformation field. In a recent study~\citep{dou2023gsmorph}, the authors observed that the similarity and regularity losses often conflict in terms of optimization direction. They proposed a layer-wise gradient surgery process that modifies the gradient of the total loss whenever the inner product of the gradients from these two components is less than zero. This gradient surgery process focuses solely on the direction of the regularization loss gradient, thereby eliminating the need to adjust its weighting.
}

\subsection{Auxiliary Anatomical Information}
\label{ss:anatomical_info}
The overlap of anatomical label maps of the fixed and transformed moving images is a widely used evaluation metric for image registration.
Hence, to improve registration performance on this metric, learning-based methods often apply the estimated deformation to the moving label map during training to compute an extra anatomy loss between the fixed and transformed moving label map.
Various loss functions used in image segmentation tasks, such as Dice loss, cross-entropy, and focal loss (see \citet{ma2021loss}~for a comprehensive review of such loss functions), can be borrowed as the choice of anatomy loss.
Despite the availability of different loss functions, Dice loss remains the most commonly used loss function in learning-based image registration, as evidenced by Table~\ref{table:unsup_list}.
This is likely because Dice loss is confined within the range of $[0,1]$, like {LCC}, which makes it easier to adjust hyperparameters when used in conjunction with {LCC}. 
{Auxiliary anatomical information can also be introduced in the registration network through multi-task learning~\citep{estienne2019u, qin2018joint}.
In particular, \citet{tan2023progressively} showed that such strategy is more effective than the direct application of anatomy loss.}

{\textbf{Recent Advancements.} 
        Building on the concept of integrating segmentation loss while
training the registration network, additional auxiliary data related
to anatomical prior knowledge can also be included during the training
process.} When anatomical landmarks are present in both the moving and fixed images, the transformation generated by the DNN can be applied to the landmarks of the moving image.
The resulting transformed landmarks can then be compared with the landmarks of the fixed image to create a loss.
This landmark supervision has been utilized in optimization-based registration methods to improve performance, as demonstrated in a number of studies~ \citep{ehrhardt2010automatic, polzin2013combining, ruhaak2017estimation, heinrich2015estimating, fischer2003combination}.
\citet{hering2021cnn}~were the first to incorporate landmark supervision into a DNN framework by comparing the MSE between the transformed and target landmarks, which resulted in a substantial improvement in the target registration error of the landmark.
Subsequently, \citep{heinrich2022voxelmorph++}~confirmed the superiority of landmark supervision on multiple benchmark datasets in their work.
It is worth mentioning that the landmarks can be generated automatically before or during the training stage without manual labeling using automatic landmark detection algorithms~\citep{heinrich2015estimating, ruhaak2017estimation, polzin2013combining}, making it straightforward to integrate into most learning-based registration frameworks.

The combination of anatomy loss and deformation regularization without an intensity-based similarity measure is also common, and in these cases, the anatomy loss serves as a modality-independent similarity measure~\citep{hu2018weakly, song2022cross, blendowski2020multimodal}.
However, the drawback of using anatomy loss without a similarity measure is clear: it does not penalize deformations in areas where anatomical labels are missing or ambiguous.
Thus, to achieve accurate and realistic deformations, the anatomical labels should be as detailed as possible, ideally with a unique label for each organ or structure.
However, obtaining such detailed labels is often challenging as anatomical label maps in medical imaging are usually manually delineated, which is a time-consuming and expensive process.
