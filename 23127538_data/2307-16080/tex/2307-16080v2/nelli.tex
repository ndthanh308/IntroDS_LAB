%% LyX 2.3.7 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
% \documentclass[english,format=manuscript,nonacm,screen]{acmart}
\documentclass[english]{acmart}
\makeatletter
\def\@ACM@checkaffil{% Only warnings
    \if@ACM@instpresent\else
    \ClassWarningNoLine{\@classname}{No institution present for an affiliation}%
    \fi
    \if@ACM@citypresent\else
    \ClassWarningNoLine{\@classname}{No city present for an affiliation}%
    \fi
    \if@ACM@countrypresent\else
        \ClassWarningNoLine{\@classname}{No country present for an affiliation}%
    \fi
}
\makeatother
\usepackage[utf8]{inputenc}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\usepackage{graphicx}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers
\setcopyright{none}
\makeatletter
\renewcommand\@formatdoi[1]{\ignorespaces}
\makeatother

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5

% \usepackage{minted}
% \usepackage[finalizecache,cachedir=_minted-nelli]{minted}
\usepackage[frozencache,cachedir=_minted-nelli]{minted}
% \renewcommand{\MintedPygmentize}{/home/mlevental/dev_projects/nelli_paper/pygmentize.py}
\usemintedstyle{colorful}
\definecolor{bg}{rgb}{0.95,0.95,0.95}
\setminted{
    bgcolor=bg,
    escapeinside={||},
    mathescape=true,
	frame=single,
	autogobble=true,
}
\setmintedinline{breaklines,escapeinside={||},mathescape=true}
\AtBeginEnvironment{minted}{%
  \catcode`?\active
  \begingroup\lccode`~=`\?\lowercase{\endgroup\def~{\linebreak}}%
}
\DeclareRobustCommand{\perc}{{%
  \mbox{%
    \fontencoding{\encodingdefault}%
    \fontfamily{pcr}%
    \selectfont
    \symbol{`\%}%
  }%
}}



\usepackage{float} %minted inside figure
\usepackage{url}
\newcommand{\qm}{\text{\usefont{OT1}{iwona}{m}{n}?}}
\newcommand{\us}{\text{\usefont{OT1}{iwona}{m}{n}_}}
\newcommand{\ps}{\text{\usefont{OT1}{iwona}{m}{n}\%}}

\usepackage{tikz}
\usetikzlibrary{calc,tikzmark}
\colorlet{colorMain}{green} 
\colorlet{colorSum}{yellow}

\makeatother

\usepackage{babel}
\usepackage{minted}
\renewcommand{\listingscaption}{Listing}

\begin{document}
\author{Maksim Levental}
\email{mlevental@uchicago.edu}
\affiliation{%
  \institution{University of Chicago}
  \country{USA}
}
\author{Alok Kamatar}
\email{alokvk2@uchicago.edu}
\affiliation{%
  \institution{University of Chicago}
  \country{USA}
}

\author{Ryan Chard}
\email{rchard@anl.gov}
\affiliation{%
  \institution{Argonne National Laboratory}
  \country{USA}
}
% \author{Nicolas Vasilache}
% \email{ntv@google.com}
% \affiliation{%
%   \institution{Google Brain}
%   \country{Switzerland}
% }
% \author{Ingo MÃ¼ller}
% \email{ingomueller@google.com}
% \affiliation{%
%   \institution{Google Brain}
%   \country{Switzerland}
% }
\author{Kyle Chard}
\email{chard@uchicago.edu}
\affiliation{%
  \institution{University of Chicago}
  \country{USA}
}
\author{Ian Foster}
\email{foster@uchicago.edu}
\affiliation{%
  \institution{University of Chicago}
  \country{USA}
}


\title{\texttt{nelli}: A lightweight frontend for MLIR}

\begin{abstract}
Multi-Level Intermediate Representation (MLIR) is a novel compiler infrastructure that aims to provide modular and extensible components to facilitate building domain specific compilers. 
MLIR enables modeling programs across the spectrum of abstraction levels.
However, since all existing frontends are at a very high level of abstraction, the semantics and mechanics of the fundamental transformations available in MLIR are difficult to investigate and employ per se.
To address these challenges, we have developed \texttt{nelli}, a lightweight, Python-embedded, domain-specific, language for generating MLIR representations. 
\texttt{nelli} leverages existing MLIR infrastructure to develop Pythonic syntax and semantics for various MLIR features. 
We describe \texttt{nelli}'s design goals, discuss key details of our implementation, and demonstrate how \texttt{nelli} enables easily defining and lowering compute kernels to diverse hardware platforms.
\end{abstract}
\maketitle
%
\tableofcontents{}

\section{Introduction\label{sec:Introduction}}

MLIR is a modular and extensible compiler infrastructure \citep{https://doi.org/10.48550/arxiv.2002.11054} for progressively transforming (\emph{lowering}) programs from high-level (in terms of abstraction), architecture-independent representations to low-level, architecture-specific representations. 
Such Intermediate Representations (IRs) are termed
\emph{dialects} in the MLIR context in order to emphasize their mutual compatibility and the unified interface that MLIR provides for transforming between them, a process referred to as \emph{running passes}.

MLIR has been applied to various problem domains and in its default distribution (other, so-called ``out-of-tree,'' implementations exist) supports representing programs ranging in abstraction level from dataflow compute graphs, such as can be used to represent Deep Neural Networks (DNNs), to architecture-specific vector instructions. 
Other less quotidian applications of MLIR include modeling gate-level primitives \citep{eldridge2021mlir}, quantum assembly languages \citep{9605269}, and database query languages
\citep{blockhaus2022framework,10.14778/3551793.3551801}. 
By virtue of its close connection to LLVM \citep{lattner2004llvm},
MLIR supports code generation for CPUs, GPUs, and other compute platforms, including abstract runtimes (as opposed to concrete hardware architectures) such as OpenMP.

While the primary value of MLIR is its support for efficient (quick) construction of IRs modeling novel domains, an undeniable secondary value is the ability to use existing dialects, corresponding to established programming
models, in combination with novel transformations tailored to problem-specific hardware configurations. 
For example, while there was been much research on the use of MLIR to lower DNNs to high-performance CPU and GPU platforms \citep{https://doi.org/10.48550/arxiv.2202.03293},
such as data-center class devices and high-powered mobile devices (e.g.,
expensive mobile phones), there is a dearth of work on efficiently
targeting low-power edge devices, such as micro-controllers and single-board computers. 
Yet those latter edge devices, while relatively underpowered, can be an attractive DNN deployment target in instances where power is a scarce commodity, such as IoT,
AgTech, and urban infrastructure monitoring. 
Indeed, it is conceivable that,
given sufficient directed design space exploration (such as can be
realized by using MLIR), these low-power edge devices could effectively
support edge inference of DNNs.

However, while MLIR provides the edge device software architect with a rich existing repository of useful dialects and transformations, the effective use of those capabilities for edge device programming is hindered by the lack of a point of ingress to MLIR capabilities that is not encumbered by assumptions about the roles of the existing dialects and their mutual relationships.
Specifically, almost all extant ingress points take the form of high-level DNN frameworks, such as PyTorch \citep{paszke2017automatic}, TensorFlow
\citep{https://doi.org/10.48550/arxiv.1603.04467}, or ONNX \citep{https://doi.org/10.48550/arxiv.2008.08272}---but 
most optimization actually occurs on lower-level dialects, such as the affine, structured control-flow, and vector dialects. 
Thus, in order to productively investigate possible optimization
opportunities one must distinguish artifacts of the lowering process
from the kernel representations themselves. 
For example, consider investigating the optimization of a ($32\times32$) linear layer (i.e., \mintinline{python}{torch.nn.Linear(32, 32)}).
This ubiquitous DNN operation lowers to the loop nests in Listing
\ref{listing1}; note that the third loop nest is readily identified as
corresponding directly to a matrix-multiplication kernel, but the other
three are somewhat mysterious\footnote{Case in point: the seemingly redundant initialization and subsequent
copy into an intermediate buffer in the lowering of \mintinline{python}{torch.nn.Linear(32, 32)}
is the result of \mintinline{mlir}{torch-mlir} enforcing value semantics
\citep{smith2002introducing} on \mintinline{mlir}{torch.tensor}s,
which, while important, obscures the actual compute kernel.}. Thus, in longer programs (complete DNNs) it becomes difficult to
identify, isolate, and manipulate (e.g., to optimize) IR corresponding
most closely to the compute kernel itself, amongst IR that reflects
certain assumptions/contracts. 
Conversely, there currently exists
no simple and efficient way to emit any of the lower-level dialects
in MLIR (such as \mintinline{mlir}{scf}, \mintinline{mlir}{affine},
\mintinline{mlir}{memref}, or \mintinline{mlir}{vector}) short of
writing the IR ``by hand.''

\medskip{}

% Figure environment removed

% Figure environment removed

In order to address the problem of MLIR's lower-level dialects being inaccessible, we present \texttt{nelli}\footnote{\url{https://github.com/makslevental/nelli}},
a lightweight frontend for MLIR. 
This Python embedded
domain-specific language (eDSL) builds on top of existing MLIR
Python bindings to map Python primitives (such as \mintinline{python}{if}s,
\mintinline{python}{for}s, and \mintinline{python}{class}es) to
various MLIR dialects. 
Our foremost goal in designing \texttt{nelli} was to make
MLIR more ergonomic and thereby more accessible. 
To this end, we make \texttt{nelli} \textquotedbl Pythonic\textquotedbl{} while preserving MLIR semantics vis-a-vis the in-tree Python bindings. 
See Listings \ref{nelli_if_to_scf_if} and \ref{nelli_for_to_affine_for}
for some examples of \texttt{nelli} syntax. 
Notably, \texttt{nelli} captures program control flow and produces fully typed IR with little static analysis on the Python source (hence,
\textit{lightweight}).
Additionally, since \texttt{nelli} is a Python
eDSL, it fully interoperates with existing Python tooling (IDEs, debuggers,etc.) and other elements of the Python ecosystem. 

In the following, we discuss in greater detail \texttt{nelli} design goals, the eDSL implementation approaches that we investigated, and the implementation details of our chosen approach.
We also present three use cases: 1) a kernel tuner that uses a black-box,
gradient-free, optimizer \citep{nevergrad}, demonstrating the power and convenience of Python interoperability;
2) a pipeline for lowering kernels to target GPUs and then evaluating performance on a Raspberry Pi edge device, demonstrating ease
of integration with LLVM, downstream of MLIR; 3) and a pipeline for translating parallelizable kernels to OpenMP programs.
In summary, this work makes the following contributions:
\begin{enumerate}
\item A thorough discussion of several alternative eDSL implementation approaches
(in Python) and their relative merits and deficiencies.
\item A discussion of the design and implementation of an embedded domain-specific
language (\texttt{nelli}) with minimal static (ahead-of-time) analysis
and complexity; 
\item Implements several lowerings that demonstrate capabilities of \texttt{nelli},
with a focus on deploying compute intensive kernels to diverse hardware
platforms.
\end{enumerate}
The remainder of the paper is structured as follows: Section \ref{sec:Background}
reviews the relevant background on eDSLs and MLIR; Section \ref{sec:Implementation}
discusses the implementation of \texttt{nelli}; Section \ref{sec:Demonstration-and-Evaluation}
demonstrates the capabilities of \texttt{nelli}; and, finally, Section
\ref{sec:Related-Work} compares \texttt{nelli} to similar tools.

\section{Background\label{sec:Background}}

We quickly review the necessary background on MLIR, in particular
with respect to DNN deployment to edge devices, and eDSL construction
in general.

\subsection{MLIR\label{subsec:MLIR}}

MLIR is an approach to building reusable and extensible compiler infrastructure.
Practically this means that MLIR constitutes a collection of utilities
for 
\begin{enumerate}
\item Defining mutually compatible IRs, known as \emph{dialects}, that model
programs in particular domains, supporting operations (including attributes)
and types (including traits);
\begin{enumerate}
\item[$\bullet$] Using the Operation Definition Specification (ODS) language implemented
against LLVM's TableGen\footnote{MLIR is an ``in-tree'' LLVM project and thus reuses and extends
many of LLVM's existing facilities.} utility;
\end{enumerate}
\item Defining intra-dialect transformations, such as canonicalization,
inlining, and dead-code elimination;
\begin{enumerate}
\item[$\bullet$] Using a subgraph matching\footnote{Directed, acyclic, graph matching is strictly more powerful than tree
matching \citep{ebner2008generalized}.} and rewriting concept known as a \mintinline{cpp}{RewritePattern};
\end{enumerate}
\item Defining inter-dialect transformations, known as \emph{conversions;}
\begin{enumerate}
\item[$\bullet$] Using \mintinline{cpp}{ConversionPattern}s and \mintinline{cpp}{TypeConverter}s.
\end{enumerate}
\end{enumerate}
In addition to a thriving ecosystem of dialects, tools, and down-stream
projects, MLIR has many ``in-tree'' dialects that model programs
across the abstraction-level spectrum. It also supports target-specific
code generation and runtime execution through the various backends
provided by LLVM; this includes both \texttt{x86\_64} and \texttt{aarch64}/\texttt{arm64}
CPU instruction set architectures (ISAs), NVPTX\footnote{NVIDIA Parallel Thread Execution is a virtual machine instruction
set architecture used by NVIDIA's GPUs as an interface layer between
CUDA and SASS; SASS is the low-level assembly language that compiles
to binary microcode, which executes natively on NVIDIA GPU hardware
\citep{nvidiasass}.} and SPIR-V\footnote{Khronos Group\textquoteright s binary intermediate language SPIR-V
for representing graphics shaders and compute kernels \citep{kessenich2018spir};
both the Vulkan graphics API and the OpenCL compute API support SPIR-V.} GPU pseudo-ISAs, as well as minimal runtimes for each. See Figure
\ref{ladderofdialectabstraction} for the ``ladder of abstraction''
\citep{hayakawa1948art} in terms of MLIR dialects. We briefly describe
a few of the dialects (in-tree and out-of-tree) relevant for DNN deployment
to edge devices.

% Figure environment removed


\subsubsection{High-level dialects}

At the highest level of abstraction, MLIR supports representing DNNs
specified using high-level frameworks such as TensorFlow and PyTorch\footnote{The \mintinline{mlir}{tf} (TensorFlow), \mintinline{mlir}{tfl} (TensorFlowLite),
\mintinline{mlir}{torch} (PyTorch), and \mintinline{mlir}{mhlo}
dialects are all ``out-of-tree'' dialects.}. The role of these dialects (\mintinline{mlir}{tf}, \mintinline{mlir}{tfl},
\mintinline{mlir}{torch}) is to faithfully represent the source model
as specified in the chosen framework and thus they function as points
of ingress into MLIR. As mentioned in the introduction, this effectively
makes TensorFlow and PyTorch the only mature points of ingress into
MLIR (see Section \ref{sec:Related-Work}). In addition,
as evinced by Listings \ref{torchnnconv2dpytorch} and \ref{torchnnconv2dtorchdialect},
the lowering/translation process incurs a high cost with respect to
legibility; naturally, lowering the level of abstraction necessitates
the inclusion of explicit specification of operations that are implicit
(or, at least, taken for granted) in the high-level representation.
With Listings \ref{torchnnconv2dpytorch} and \ref{torchnnconv2dtorchdialect}
in mind, note that in specifying the operation \mintinline{python}{torch.nn.Linear(32, 32)},
one implicitly specifies:
\begin{enumerate}
\item The bias tensor \mintinline{mlir}{|\%|0 = torch.vtensor.literal -> !torch.vtensor<|{[}|32|{]}|,f32>}
needs to be initialized;
\item The weight tensor \mintinline{mlir}{|\%|1 = torch.vtensor.literal -> !torch.vtensor<|{[}|32, 32|{]}|,f32>}
needs to be initialized; 
\item A transpose \mintinline{mlir}{|\%|2 = torch.aten.transpose.int |\%|1, |\%|int0, |\%|int1}
on the weight tensor needs to be performed (since, in PyTorch, weights
are stored in column-order);
\item The bias needs to be added \mintinline{mlir}{|\%|4 = torch.aten.add.Tensor |\%|3, |\%|0, |\%|float1.0e00}
to the result of the matrix multiplication (\mintinline{mlir}{|\%|3 = torch.aten.mm |\%|arg0, |\%|2}).
\end{enumerate}
%
% Figure environment removed

The ultimate effect of this translation process is that targeting
the operation of interest (e.g., \mintinline{python}{torch.nn.Linear(32, 32)})
for investigation and transformation is made more difficult. It's
important to not underestimate the significance of the last point:
subgraph matching, as implemented in MLIR by the \mintinline{cpp}{RewritePattern},
``anchors'' on a target operation. Thus, if an optimizing transformation
(such as loop-unrolling, loop-fusion, loop-tiling) is implemented
targeting the loop nests generated from this high-level representation
(see Listing \ref{listing1}), then running that pass will incur high(er)
runtime cost\footnote{Imagine targeting the third loop-nest in Listing \ref{listing1};
you might develop a \mintinline{cpp}{RewritePattern} that matches
on \mintinline{mlir}{scf.for} but then there are $2+2+3+2=9$ possible
such matches. Thus, one needs to further filter the possible matches
(e.g., by filtering on whether the body contains a sequence of \mintinline{mlir}{arith.mulf}
and \mintinline{mlir}{arith.addf}).} and much higher development time. In MLIR, the partial resolution
to this problem is called \emph{structured code generation} \citep{https://doi.org/10.48550/arxiv.2202.03293},
i.e., high-level operations such as \mintinline{python}{torch.nn.Linear(32, 32)}
are first lowered to a structured representation, such as \mintinline{mlir}{linalg.generic}
(see Listing \ref{linalg-matmul}), which is itself transformed and
lowered to optimized loop-nests (see Listing \ref{listing1}). But,
as can be observed in Listing \ref{linalg-matmul}, these structured
transformations are (currently) limited to kernels implemented in
terms of \texttt{parallel} and \texttt{reduction} iterators.

% Figure environment removed


\subsubsection{Intermediate-level dialects}

An intermediate-level dialect
is one that can be used to represent a kernel explicitly but is abstract
with respect to hardware implementation. Thus, the structured control
flow dialect (\mintinline{mlir}{scf}), which models loops (\mintinline{mlir}{scf.for},
\mintinline{mlir}{scf.while}, \mintinline{mlir}{scf.parallel}),
and the \mintinline{mlir}{memref} dialect, which is intended to model
creation and manipulation of objects with memory reference semantics
(i.e., buffers). See Listing \ref{listing1} for the representation
of \mintinline{python}{torch.nn.Linear(32, 32)} purely in terms of
these dialects.

% Figure environment removed
 The least abstract dialects at this level of abstraction are the
\mintinline{mlir}{arith} dialect, which models basic integer and
floating point mathematical operations, and the \mintinline{mlir}{vector}
dialect, a generic, retargetable, higher-order (i.e., multi-dimensional)
vector that carries semantically useful information for transformations
that enable targeting vector ISAs on concrete targets (e.g., AVX-512,
ARM SVE, etc.).

The dialects at this level of abstraction, especially \mintinline{mlir}{scf}
and \mintinline{mlir}{vector}, are where the real optimization work
occurs; transformations such as loop-unrolling, loop-fusion, loop-tiling
can have enormous impact on the runtime performance of any code \citep{10.1145/3178372.3179509},
but are especially important for numerics intensive code, such as
can be found to constitute the majority of kernels in a DNN. Furthermore,
explicit vectorization (rather than auto-vectorization) is critical
to achieving good performance of various compute-intensive kernels,
deployed to both CPUs and GPUs \citep{dickson2011importance}. Hence,
it's important to be able to efficiently and effectively manipulate
representations of DNNs at this level of abstraction, even more-so
than what MLIR currently enables.

\subsubsection{Low-level dialects (target-specific code generation)}

At the lowest level of abstraction, MLIR contains implementations
of dialects that can interface with hardware specific runtimes and
ISAs, such as \mintinline{mlir}{nvvm}, which models NVPTX instructions,
\mintinline{mlir}{spirv}, and \mintinline{mlir}{llvm}, which faithfully
models LLVM IR and therefore enables targeting all backends supported
by LLVM (including \texttt{x86\_64} and \texttt{aarch64}/\texttt{arm64}
CPU ISAs). The latter dialect includes support for managed runtimes
on top of ISAs (such as OpenMP, in combination with the \mintinline{mlir}{omp}
dialect) and coroutines (in combination with the \mintinline{mlir}{async}
dialect). These target-specific, code-generation focused, dialects
enable end-to-end compilation of MLIR programs (e.g., DNNs) to a variety
of execution environments, including single-core CPU, multi-core (threaded)
CPU, and GPU, including SoTA NVIDIA platforms but also lesser known
vendors that implement the SPIR-V standard (see Section \ref{sec:Demonstration-and-Evaluation}
for demonstrations of \texttt{nelli}'s end-to-end compilation features).

\subsection{eDSL construction in Python}

Given a host language, there are (invariably) several ways to implement
an embedded domain-specific language; the set of avenues available
is only circumscribed by the facilities of the host language and the
goals of the DSL designer. \texttt{nelli} is embedded in Python and
so we discuss two eDSL implementation approaches (including merits
and deficiencies) with Python as the host language. Indeed, each of
these two approaches was validated (i.e., implemented) over the course
of developing \texttt{nelli} and discarded in favor of the chosen
approach (see Section \ref{sec:Implementation}).

\subsubsection{Compiling}

The most straightforward approach to implementing an eDSL in any host
language (conceptually) is to build a compiler using that language
for (a subset of) that language. This involves static (ahead-of-time)
source analysis, including lexing, abstract syntax tree (AST) construction,
control-flow analysis, type inference, and code generation (for the
target language, MLIR or otherwise). Suffice it to say, this is a
monumental undertaking. Nonetheless, the undertaking has been undertaken,
in the context of Python and, specifically, numerics intensive programs,
many times to varying degrees of success \citep{behnel2010cython,nuitka,10.1145/3578360.3580275}.

The scope of such an undertaking is slightly improved by
the fact that Python provides, in its standard library, source lexing
(for Python source code), AST construction, and AST traversal utilities
(in the \mintinline{python}{ast} package). But, comparatively speaking,
these aspects of the undertaking are the least challenging\footnote{Indeed, there exist many lexers and parsers for Python \citep{zimmerman2022langcc,parr1995antlr}
implemented in other, more performant, languages, i.e., preferable
alternatives to the \mintinline{python}{ast} package, if one's goal
were to build a Python compiler.}; the principal challenges are control-flow analysis and type inference.
With respect to the latter, Python's highly permissive runtime and
``duck typing''\footnote{Python is believed to be ``dynamically typed'': this is a widely
held misconception. In fact, every value manipulated by the Python
runtime is a subclass of \mintinline{python}{<class object>}: \mintinline{python}{(1).|\_\_|class|\_\_|.|\_\_|bases|\_\_| == (<class object>)}.
Thus, method resolution (\textbf{which can be patched at runtime})
determines the effective type of a value: \mintinline{python}{(1).|\_\_|class|\_\_|.|\_\_|mro|\_\_| == (<class int>, <class object>)}. } paradigm requires a compiler to reckon with all mutations of an instantiated
object; any object instance can be made to quack like a duck at any
point in the execution of a Python program. More seriously (supposing
property mutations were prevented), Python does not have nested lexical
scopes below the level of a function body: for example, in the following
% Figure environment removed

\noindent the conditional actually ``yields'' two values (\mintinline{python}{b}
in addition to \mintinline{python}{c}) and the same is true for all
such regions (i.e., \mintinline{python}{for}s and \mintinline{python}{with}s),
i.e., they ``leak'' definitions and (possibly) grow the use-def
chains of identifiers in subsequent regions. In addition, irrelevant
of lexical scoping, the conditional actually yields union types (\mintinline{python}{b, c: int |$\mid$ |str |$\mid$ |None})
and hence the target language needs to support such union types. Currently,
MLIR does not support such union types\footnote{Certainly MLIR supports modeling union types but recall that the broader
goal is to translate Python to existing MLIR dialects, rather than
mapping Python to a novel dialect.}. 

\subsubsection{Tracing\label{subsec:Tracing}}

An alternative to ahead-of-time (AOT) compilation of a program is
just-in-time (JIT) compilation, and in particular, compilation of
only the ordered sequence of operations executed during some execution
the program; such a compiler is called a \emph{tracing} JIT, alluding
to the ``tracing'' of the execution path of the program. Several
such tracing JITs have been built for general purpose Python \citep{10.1145/1565824.1565827,lam2015numba,pyston,pyjion}.
A tracing JIT approach obviates the need to perform control-flow analysis
and type-inference, because both are fully reified at runtime. However,
a tracing JIT does not eliminate the need to parse a source representation
of the program, e.g., as in the case of Python, the bytecode representation.
Indeed, Numba \citep{lam2015numba}, Pyston \citep{pyston}, and Pyjion
\citep{pyjion} compile CPython virtual machine bytecode instructions
(as opposed to textual source) directly to (target) assembly language\footnote{All three projects employ a more generic JIT (LLVM for the former
two and the CoreCLR \citep{troelsen2017philosophy} for the latter)
for the ``last mile'' of code generation.}. It's important to emphasize that while, in principle, each of Numba,
Pyston, and Pyjion can be used to compile entire Python programs,
they are frequently used as eDSLs for accelerated implementations
of the numerics intensive portions of Python code, through their partial-compilation
APIs (\mintinline{python}{@njit} and \mintinline{python}{pyjion.enable()},
for Numba and Pyjion respectively). 

An alternative to JIT compiling Python at the bytecode level (i.e.,
handling all opcodes), especially relevant for eDSL construction,
is instrumenting (``hooking'') only a subset of operations in the
host language. For example, function calls and arithmetic operations.
The various Python DNN frameworks (PyTorch \citep{paszke2017automatic},
TensorFlow \citep{https://doi.org/10.48550/arxiv.1603.04467}, JAX
\citep{frostig2018compiling}) take this approach; by restricting
user programs to make calls to functions in their own namespaces and
by overloading various operators on proxy objects (see Listing \ref{operator-overloading}),
the eDSL can wholly own the means of production\footnote{Recall, a production is a rewrite rule specifying a symbol substitution
that can be recursively performed to generate new symbol sequences.
A finite set of productions $P$ is the main component in the specification
of a formal grammar (such as that of a programming language).}, and thereby perform source-to-source translation.
% Figure environment removed
While simple and effective, hooking function calls and operator overloading
suffers from an aesthetically displeasing deficiency: in a host language
(such as Python) where control-flow primitives such as \mintinline{python}{if}s
and \mintinline{python}{for}s cannot be instrumented, they must be
replaced (within the context of the eDSL) with explicit proxies (e.g.,
\mintinline{python}{tf.while|\_|loop} and \mintinline{python}{jax.lax.cond}).
More critically, existing such eDSLs suffer from a fundamental limitation
of the tracing approach: if host-language conditionals are allowed
in any capacity, then the path less traveled by the program will be
not captured by the eDSL. For example, in the following 
% Figure environment removed

\noindent Despite being able to effectively capture all arithmetic
operations on a \mintinline{python}{Tensor}, no tracing eDSL can
capture both arms of the conditional. \texttt{nelli} addresses this
limitation.

\section{Design and implementation of \texttt{nelli}\label{sec:Implementation}}

The primary design goal of \texttt{nelli} is to be \emph{easy to use}
and \emph{simple to understand}, while remaining faithful to the semantics
of MLIR. By semantics of MLIR, we mean that dialects as rendered in
\texttt{nelli} (i.e., names and uses of operations) should reflect
as closely as possible their rendering in MLIR IR. Note, we draw a
subtle distinction between easy and simple: easy to use implies that
it should work (generate MLIR IR) with very little fanfare while simple
to understand means studying the implementation should reward a modicum
of effort (without requiring an inordinate investment). Addressing
the former, much effort on our part has been invested in packaging
\texttt{nelli} for distribution (it can be directly \texttt{pip install}ed
without compiling LLVM/MLIR). Further, in order to reduce the barrier to reuse of existing code, \texttt{nelli} is also extensible (in and of itself) and exposes MLIR in an extensible way.

Addressing the latter precludes various
metaprogramming techniques, such as wholesale source rewriting and
Python \mintinline{python}{metaclass} programming. Additionally,
it precludes the use of dynamic scoping (using \mintinline{python}{contextvars})
to implement patterns such as stacks of monadic interpreters \citep{kiselyov2012typed,10.1145/3158140}.
\texttt{nelli} uses three techniques to accomplish the stated design
goals: operator overloading, trivial source rewriting, and bytecode
rewriting. We discuss each in turn (effectively, in order of increasing
complexity). We also discuss how \texttt{nelli} addresses extensibility.

\subsection{Upstream manicuring and operator overloading}

% Figure environment removed

MLIR, irrelevant of \texttt{nelli}, procedurally generates Python
bindings for functionality related to emitting MLIR IR. This procedural
generation is made possible by virtue of the fact that almost all
operations, in all MLIR dialects, are defined using ODS (see Section
\ref{subsec:MLIR}). Nonetheless, convenient (and robust) as these
existing bindings might be, they are quite verbose, requiring specifying
most attributes of operations explicitly; see Listing \ref{mlirfuncop}
for an example. Thus, some of the work of \texttt{nelli} involves
normalizing the upstream APIs; in particular we implement operator
overloading for various arithmetic operations on values that are results
of \mintinline{mlir}{arith} operations (see Listing \ref{mlirfuncop-1}),
as well indexing and slicing on results of \mintinline{mlir}{memref}
and \mintinline{mlir}{tensor} operations (see Listing \ref{nelli_for_to_affine_for}).
Additionally we overload Python parameter annotations to implement
a minimal form of Hindley-Milner\footnote{In reality, MLIR performs the type inference, \texttt{nelli} simply
requires fully type-annotated function parameters.}, as well as instantiating \mintinline{mlir}{func}s with typed parameters.
Finally, we use Python \mintinline{python}{class} namespaces as models
for \mintinline{mlir}{module}s, including nested \mintinline{mlir}{gpu.module}s
(see Listing \ref{gpumodules}).

% Figure environment removed

% Figure environment removed


\subsection{Trivially rewriting the AST}

It's important to understand how the upstream MLIR Python bindings
function (as a reflection of how MLIR functions). Consider the instantiation
of \mintinline{mlir}{scf.for} in Listing \ref{mlirfuncop}; operations
to be inserted into the body of the \mintinline{mlir}{scf.for} must
have their \mintinline{python}{InsertionPoint}s set to (somewhere
in) the body of the \mintinline{mlir}{scf.for}. Thus, the Python
bindings corresponding to those operations (i.e., \mintinline{python}{arith.MulIOp})
must be executed within the context of \mintinline{python}{with InsertionPoint(loop.body)}.
Eliminating the indentation due to the \mintinline{python}{with}
(which indicates a nested scope where none exists) is worthwhile.
One trivial way to accomplish this is to explicitly \mintinline{python}{|\_\_|enter|\_\_|}
and \mintinline{python}{|\_\_|exit|\_\_|} the \mintinline{python}{InsertionPoint(loop.body)}
context manager; see Listing \ref{mlirfuncop-2}.
% Figure environment removed
But requiring the user to explicitly indicate the end of the \mintinline{python}{for}
loop transforms Pythonic \mintinline{python}{for} loops to Pascal-style
\mintinline{pascal}{for} loops. Thus, \texttt{nelli} rewrites user
functions (at the AST level) and automatically inserts such opening
and closing calls for all \mintinline{python}{for}s and \mintinline{python}{if}s
(see Listing \ref{mlirfuncop-2-1}) 
% Figure environment removed
. Note, since we rewrite the AST (not the source itself), we are able
to patch line numbers for all nodes to reflect original source locations
and thus all Python IDE, error-reporting, and debugging infrastructure
is undeterred i.e., users are able to set breakpoints in functions
and inspect objects just the same as for any Python code\footnote{This is emphatically not the case for eDSLs like Numba and Pyjion
which compile and execute Python using, effectively, their own bytecode
interpreters.}. 

\subsection{Trivially rewriting bytecode}

Rewriting the source AST enables mapping Python control-flow primitives
to various MLIR control-flow operations except for one caveat: as
mentioned in section \ref{subsec:Tracing}, relying on runtime execution
of Python code (and hooks) to capture programs precludes faithful
capture of conditionals. For example, irrespective of arbitrary AST
transformations, only one arm of the conditional in Listing \ref{mlirfuncop-2-1}
can be traced. Although it's debatable whether multi-arm conditionals
are crucial (\mintinline{mlir}{scf.if} does support an \mintinline{mlir}{scf.else}
branch), it would be a strange language that supported only single-arm
conditionals. 

The resolution to the conundrum of the multi-arm conditional lies in rewriting the program
on a deeper level than the AST; recall Python programs are compiled
(by the CPython implementation) to bytecode instructions. 
% Figure environment removed
 The CPython implementation of Python is a stack-based virtual machine
\citep{ike2015inside} that implements conditionals like many other
virtual machines: using jump instructions (see Listing \ref{mlirfuncop-2-1-1}).
Thus, the solution is to simply rewrite the bytecode of the user's
function and remove those jumps\footnote{In fact, the jump instruction is replaced by a \mintinline{python}{NOP}
(no-op) instruction in order to prevent invalidating stack size calculations.}, thereby forcing the CPython interpreter to execute all instructions
in all arms of the conditional. It's important to emphasize that this
transformation is reasonable given the stated goals of \texttt{nelli}:
the eDSL program isn't computing on data and has no intended side-effects
other than to emit MLIR IR. Thus program capture, rather than evaluation,
permits (and encourages) us to fundamentally alter the semantics of
conditionals in this way; certainly, under different circumstances,
such a transformation would be wholly nonsensical.

\subsection{Extensibility}

Currently MLIR is extensible to a limited extent\footnote{The state of affairs is steadily improving; between starting and finishing this manuscript, the authors contributed three substantive improvements.}
\texttt{nelli} addresses extensibility in four ways: the first and second being exercises of existing (but infrequently employed) MLIR APIs, and with the remaining two being novel (relative to MLIR):
\begin{enumerate}
    \item \texttt{nelli} uses the \mintinline{python}{_site_initialize} to register (in-tree) dialects at load-time;
    \item \texttt{nelli} subclasses existing \mintinline{python}{ir.OpView} classes thereby extending their functionality despite being out-of-tree;
    \item \texttt{nelli} is built with exported symbols, thus enabling users to extend the various C++ utility classes that comprise the upstream bindings (without recompiling MLIR);
    \item \texttt{nelli} implements AST walking functionality (akin to Python's \mintinline{python}{ast.NodeTransformer}) which, along with upstream improvements contributed by the authors, enables writing simple IR rewrites wholly in Python.
\end{enumerate}
Extension points (1) and (2) exercise existing MLIR Python bindings APIs in unconventional ways to demonstrate (1) extending the set of registered (available at runtime) dialects and (2) wrapping/polishing existing \mintinline{python}{ir.OpView} APIs (constructors, getters, setters) without repackaging the bindings. Extension points (3) and (4) are more nuanced.

By default, MLIR Python bindings are built with "hidden" symbols (i.e., \mintinline{cpp}{-fvisibility=hidden})\footnote{This is actually a design choice made by \mintinline{cpp}{pybind11} \citep{pybind11} rather than MLIR.}, making those symbols unavailable for symbol resolution of subsequently loaded libraries. 
This has the effect that none of the bindings' utility classes can be extended without recompiling.
\texttt{nelli} exports these symbols (i.e., annotates them with \mintinline{cpp}{__attribute__((visibility("default")))}), thus downstream projects can \mintinline{bash}{pip install nelli} immediately extend bindings for existing MLIR dialects, operations, types, and attributes.
With respect to the final extension point (writing simple IR rewrites wholly in Python), recall that the conventional method for transforming/rewriting IR is by building a \mintinline{cpp}{RewritePattern} using the C++ API.
While the MLIR C++ API is robust and well-designed, like all such APIs, it is rigid and demanding; that is to say, it is not suited for experimentation and quick iteration.
By contrast the Python AST utilities (under the standard library package \mintinline{python}{ast}) present a lightweight API that enables building small (but effective) rewrites of the Python AST.
Taking this user experience as our inspiration, we implement similar functionality by generating AST visitors procedurally from the upstream bindings; one \mintinline{python}{DialectVisitor} for each registered MLIR dialect, including out-of-tree dialects.
Using these \mintinline{python}{DialectVisitor}s, along with the recently contributed \mintinline{python}{replace_all_uses_with} upstream API, we are able to build small but non-trivial IR transformations on MLIR \mintinline{python}{ir.Module}s just as one does using \mintinline{python}{ast.NodeTransformer}.



\section{Demonstration and evaluation}\label{sec:Demonstration-and-Evaluation}

We demonstrate the capabilities of \texttt{nelli} with three small exercises:

\begin{enumerate}
    \item an end-to-end (including execution) GPU example of a batched, multi-channel, 2D convolution that lowers to both NVIDIA devices and Vulkan supporting devices, such as the VideoCore VI 3D found in the Raspberry Pi 4 Model B and the Apple Neural Engine (found in the Apple M1 series of laptops);
    \item an end-to-end that lowers the same kernel to the managed OpenMP runtime;
    \item the previous two examples integrated with a black-box, gradient-free, optimizer \citep{nevergrad} for searching the space of possible optimizing transformations.
\end{enumerate}

\subsection{End-to-end GPU}

% Figure environment removed

We implement a standard 2D-NCHW convolution and parallelize across output elements; see Listing \ref{2dnchwconv}. 
Note, we set the default range to be mapped to \mintinline{mlir}{scf.for} (\mintinline{python}{range_ctor=scf_for}) but explicitly map the outermost four loops to a \mintinline{mlir}{scf.parallel} thereby specifying that each output element (\mintinline{python}{n, co, ho, wo}) should be computed in parallel.
The pass pipeline (cf. Listing \ref{2dnchwconv}) that accompanies the kernel is specialized for NVIDIA devices but only in the final (hardware-specific) passes. 
The higher-level passes effectively perform two functions: \mintinline{python}{gpu_map_parallel_loops} assigns nested \mintinline{mlir}{scf.parallel} loops to the corresponding level of the GPU workgroup hierarchy (grid, block, and thread) and \mintinline{python}{gpu_kernel_outlining} outlines GPU kernel code so that it can be separately compiled and serialized (\mintinline{python}{gpu_to_cubin(chip="sm_75")}).
Note that while this basic example only has one \mintinline{mlir}{scf.parallel} and is thus mapped only to blocks, further transformations (such as tiling) can introduce nested \mintinline{mlir}{scf.parallel}s, thereby inducing a distribution across both blocks and threads.

The presented code lowers fully to both NVIDIA and Vulkan targets almost unaltered\footnote{Vulkan does not support 4D buffers so in practice inputs have to be packed along the batch and channel dimensions.} and therefore this kernel successfully executes on all three of the aforementioned hardware platforms (NVIDIA 3080Ti, Apple Neural Engine, and VideoCore VI). In fact, since MLIR provides utilities for mapping NumPy arrays to the various GPU runtime native buffers (using a buffer descriptor called \mintinline{cpp}{StridedMemRef}), and utilities for interfacing with the hardware runtimes (CUDA runtime and Vulkan runtime), all end-to-end experiments can be executed without ever leaving the comfort of \texttt{nelli}.

\subsection{End-to-end OpenMP}

Since the aforementioned 2D-NCHW convolution kernel is implemented in terms of the \mintinline{mlir}{scf} dialect, which, as the name suggests, models programs in terms of abstract but structured control-flow operations, the same 2D-NCHW convolution kernel can be lowered to the OpenMP runtime with just the flip of a switch; using \texttt{nelli}'s \mintinline{python}{Pipeline}, it's just a matter of substituting \mintinline{python}{convert_scf_to_openmp} for \mintinline{python}{convert_parallel_loops_to_gpu}.
This wraps the loop nest in a \mintinline{mlir}{omp.parallel} context and implements the \mintinline{mlir}{scf.parallel} loop as a \mintinline{mlir}{omp.wsloop}, i.e., worksharing loop.
Just as with the end-to-end GPU implementation, thanks to LLVM's support for OpenMP, all end-to-end experiments can be executed without ever leaving the comfort of \texttt{nelli}.

\subsection{Derivative-free optimization}

% Figure environment removed

Algorithmic correctness is a necessary but not sufficient condition for achieving high performance implementations of numerically intensive kernels; due to the variety of hardware platforms, fine-tuning of the implementation for each platform is critical \citep{li2009note}. 
In many cases, where a formal cost model of the hardware platform isn't available, a directed search of the program transformation space cannot be realized.
In such instances, black-box (gradient-free) optimization techniques can be employed.
To demonstrate the value of having seamless interoperability with the Python ecosystem, we connect \texttt{nelli} to \texttt{Nevergrad} \citep{nevergrad}, a Python package for gradient-free optimization.
We experiment with applying two loop nest transformations: tiling and unrolling \citep{CARDOSO2017137}, using the MLIR pass \mintinline{python}{scf_parallel_loop_tiling} and a loop-unrolling operation (\mintinline{python}{loop_ext.LoopUnrollOp}).
Both transformations are parameterized (by tile sizes and unroll factor, respectively) and it is this parameter space that can be searched over to find the optimal kernel for each hardware platform.

Thus, we set \texttt{Nevergrad} loose on our kernel on three different platforms: a workstation with a NVIDIA 3080Ti GPU, an Apple M1 MacBook Pro with a Neural Engine GPU, and a Raspberry Pi 4 Model B with Broadcom VideoCore VI GPU.
On each platform we apply the 2D-NCHW convolution on an input with shape $\left(N, C, H, W\right) := \left(1, 1, 1280, 1280\right)$, using a $\left(C_i, C_o, K\right) := \left(1, 3, 3\right)$ filter (where $C_i, C_o$ are channels-in, channels-out, respectively), and search the space of possible tile sizes and loop-unroll factors.
In addition, just for fun, we apply loop-unrolling to the OpenMP implementation.
As well, we compare it to the PyTorch implementation of the same kernel.
Figure \ref{nevergrad} shows the results of the experiment.
A few noteworthy observations:
\begin{enumerate}
  \item for this small kernel, OpenMP is fairly performant but loop-unrolling has almost no effect on the implementation because the worksharing loop distribution already distributes work maximally across all available cores;
  \item tiling and loop-unrolling is hugely important for achieving good performance on GPUs;
  \item PyTorch has highly optimized implementations of kernels for common platforms (such as NVIDIA) but otherwise can be outperformed by fine-tuning for a user's specific hardware configuration.
\end{enumerate}
Indeed, each of these conclusions is well-known and understood and thus we observe that enabling users to easily, and transparently, perform this kind of fine-tuning, through a representation like MLIR and an interface like \texttt{nelli}, is a boon to mankind.

\section{Related Work\label{sec:Related-Work}}

There are several projects in the MLIR ecosystem that aim to support a Python frontend for some MLIR dialect (or collection of dialects) but as far as we're aware (i.e., at the time of writing) none that aim to expose \emph{all} builtin dialects through a Python frontend.
Most famous amongst these are JAX~\citep{jax2018github}, which provides a NumPy-conformant interface to the \mintinline{mlir}{hlo}~\citep{stablehlo} family of dialects, and TensorFlow.
Each of these effectively provides a very high-level, Python-embedded, DSL for machine learning (specifically neural network) operations implemented against their respective dialects.
Note, while superficially the Torch-MLIR project~\citep{torchmlir} resembles JAX and TensorFlow, in that it transforms Python to an MLIR dialect (\mintinline{mlir}{torch-mlir}), the Python frontend is in fact provided by PyTorch itself\footnote{Torch-MLIR operates an the intermediate representations exported by PyTorch rather than directly on Python source.}, we do not count it amongst this group of projects.
In this category there is also the interesting \texttt{numba-mlir} project, a MLIR-based Numba backend, where, Numba~\citep{10.1145/2833157.2833162} translates from NumPy-specific Python code to lower-level representations (Numba IR and then LLVM IR) by analyzing the CPython bytecode instructions\footnote{CPython is a stack-based virtual machine with its own assembly/bytecode instructions (see \url{https://docs.python.org/3/library/dis.html}).} the Python compiles to.
\texttt{numba-mlir} lowers to several high-level (higher than LLVM IR) domain-specific dialects (\mintinline{mlir}{ntensor}, \mintinline{mlir}{plier}) by recovering Regionalized Value State Dependence Graphs~\citep{10.1145/3391902} from the Numba IR.
Finally, there is the Pylir project, implemented on top of MLIR, which aims to be an optimizing, ahead-of-time, compiler for all of Python.
Thus, Pylir's goal is not to be a frontend for MLIR in and of itself but to compile Python code to native executables; it accomplishes this impressive feat by parsing Python to its own dialects.

Alternatively, there exist projects that approach the problem orthogonally - they aim to provide a Python interface to a MLIR-like framework but not MLIR itself. 
The projects in this category generally aim to be completely independent of upstream MLIR and thus parse MLIR-native IR into proprietary ASTs and manipulate those ASTs in various ways (transforming, serializing, etc.).
For example, pyMLIR~\citep{pymlir} implements a LALR(1) grammar (extracted from the upstream MLIR documentation) and parser. 
pyMLIR's parser generates an AST representation that further implements Python's \mintinline{python}{ast.NodeTransformer} interface, thus enabling various AST transformations.
The resulting ASTs can be once again serialized to conformant MLIR.
Note, \texttt{nelli}'s AST transformation functionality is inspired by pyMLIR's, but instead of operating on a proprietary AST representation, \texttt{nelli}'s operates on the canonical MLIR AST.
On the other hand, xDSL~\citep{brownxdsl} is a Python-native compiler framework influenced by MLIR but not coupled directly to MLIR; xDSL emitted IR is validated against MLIR but only as part of its continuous integration process.
These projects are all very interesting and impressive accomplishments but they are orthogonal to our goals, i.e., providing a Python frontend to MLIR itself, rather than an arbitrary compiler framework (irrespective of how featureful or MLIR-like that framework might be).

\section{Conclusion\label{sec:Conclusion}}

We described \texttt{nelli}, a lightweight, open source, Python frontend for the Multi-Level Intermediate Representation compiler infrastructure.
\texttt{nelli} aims to make MLIR more accessible by providing a Pythonic syntax for the various MLIR dialects while remaining faithful to MLIR's semantics.
\texttt{nelli} uses operator overloading, AST rewriting, and bytecode rewriting to map Python control flow primitives, like conditionals and loops, to MLIR control flow operations, amongst other design choices.
\texttt{nelli} is designed to be simple to use and understand. It performs minimal static analysis and thus incurs minimal complexity in perform the translation to MLIR, compared to existing frontends. 
As a Python eDSL, it interoperates with existing Python tooling is fully extensible, in terms of dialects, operations, types, and attributes supported.
Further, we demonstrated the utility of \texttt{nelli} by showing end-to-end compilation of an example kernel for different hardware platforms, including integration with a derivative-free optimization library to automatically optimize for those platforms.
In summary, \texttt{nelli} provides an easy way to interface with MLIR and manipulate intermediate representations directly, avoiding the complexities and artifacts of lowering from high-level frameworks. This enables more flexible program analysis and transformation compared to those existing MLIR frontends.

\bibliographystyle{plain}
\bibliography{nelli}

\end{document}
