
\section{Evaluation}\label{sec:evaluation}
% Figure environment removed


As a first step of our study, we study the execution time for CPU and GPU backends when increasing the number of qubits. Fig.~\ref{fig:scaling} shows the scalability of the state vector method on all six benchmarks from 10 to 34 qubits when utilizing either the CPU (orange color) or two GPUs. For the latter, both Qiskit's default Thrust backend (blue) and Nvidia's cuQuantum backend (green) are evaluated. Note that some of the benchmarks could only be run in double precision on the CPU because single precision runs lead to segmentation faults. It can be observed that, with a low number of qubits, the runtime of each benchmark is lower when utilizing the CPU. At a break point between 14 to 18 qubits, the GPU  with the Thrust backend becomes the fastest choice. The cuQuantum backend shows constant behaviour for up to $24-28$ qubits, and only afterwards becomes as fast as, or in some cases faster than the default Thrust backend of Qiskit. Specifically for the QV, RQC and GHZ benchmarks the cuQuantum backend is approximately $1.5-3\times$ faster than the Thrust backend for $28-32$ qubits. By investigating the trace, the nearly constant execution time for the cuQuantum backend for a small number of qubits, i.e., $10-22$, is due to the large overhead of memory allocation and pinning that dominate the execution time. 

Starting from 31 qubits in double precision, and 32 qubits in single precision, both GPUs on the system are utilized for both backends. Up to 34 qubits could be simulated in double precision on two GPUs for the GHZ benchmark. On CPU the same benchmark could be run with 34 qubits in single precision only. Here, the GPUs only reach a 1.1$\times$ speedup and even a 0.8$\times$ slowdown with cuQuantum and Thrust, respectively. For lower qubits a higher speedup is reached, e.g. for 30 qubits in single precision, the Thrust backend reaches a 6$\times$ speedup and the cuQuantum backend a 12$\times$ speedup. Similar behavior was found for the other benchmarks, where the GPUs generally reach a range between 32 and 33 qubits double precision, whereas the CPU could only simulate around 31 to 32 qubits for the same application, either not having sufficient memory available, or running for an exceedingly long time. For example, with this maximum number of qubits the GPU outperforms the CPU with up to a factor of around $13-14\times$ for both the QV and QFT benchmarks in double precision with cuQuantum and Thrust.

As the QW application is characterized by a large depth, overall a lower number of qubits was executable in reasonable time spans. Furthermore, up to the maximum number of 20 qubits still only a single GPU is utilized. With the default Thrust backend the single GPU execution time is more than twice as fast as the CPU runtime after 13 and more qubits. With 20 qubits, the GPU with Thrust backend outperforms the CPU by a factor of nearly four. The cuQuantum backend is not beneficial for this benchmark. While it is close to the default Thrust backend with a larger number of qubits, there is still a difference of around 3-5 seconds between these two.

As we discussed in the background section, the gate fusion performance optimization could lead to a performance boost, especially when GPUs are used. By default, in Qiskit Aer quantum state simulators, the gate fusion is applied to circuits with a number of qubits greater than 14. We study the performance impact of using gate fusion and present it in Fig.~\ref{fig:gatefusion}. We observe a big impact of gate fusion for QV with Thrust backend improving the performance in terms of execution time by a factor of 1.5$\times$ (conversely, the transpiler gate fusion slows down the execution time of the cuQuantum backend). For other applications, we do not observe major impact of gate fusion.
\input{include/breakdown}
% Figure environment removed


After studying the scalability of quantum applications, we investigate the GPU utilization and analyze the breakdown of the different activities. Fig.~\ref{fig:gpu_breakdown} presents compute activity (GPU kernels) in green, initialization in orange, finalization in yellow and GPU idle in brown. We can see that for the QV, QFT, and RQC benchmarks, the compute time dominates the execution time for both Thrust and cuQuantum backends, showing an efficient utilization of the GPU. The QW application is dominated by an initialization and GPU idle time, without an efficient usage of the GPU. Most interestingly, we see cuQuantum can accelerate GPU kernels (green) by $1.2-3.7\times$ compared to the Thrust backend in all applications except the QW benchmark.


To further understand how the quantum applications use the GPU compute units and memory system, we identify with Nvidia Nsight Systems the GPU kernels that take most time when running the two GPU backends and present them on the top of Fig.~\ref{fig:throughput}. We notice that matrix multiply are the dominant compute kernels when using the Thrust backend. Qiskit's Thrust backend provides custom implementation for multiply with matrix 2$\times$2, 4$\times$4 and 8$\times$8 and matrix multiply with generic $N\times N$ size. These large size matrix multiply are the result of the gate fusion optimization. %

We present the percentage of compute and memory throughput in these most used kernels in Fig.~\ref{fig:throughput}. The compute throughput is calculated as the percentage of ALU pipe in active cycles and memory throughput is calculated as the percentage of DRAM in active cycles. The cuQuantum backend shows considerably higher memory and compute throughput than the Thrust backend, except for the Grover and QW applications. We also notice that, compared to Thrust, the cuQuantum backend always tends to stress memory more than compute, i.e., all blue bars are higher than green bars.

%
As last step in our performance characterization of the Qiskit GPU backends for state vector simulation, we present the roofline for the different kernels to determine whether the kernels are memory or compute bound in Fig.~\ref{fig:roofline}). Using the most utilized functions, we show the characteristics of each benchmark setup (Table~\ref{tab:benchmarks_max}, 1 GPU (SP)) in a roofline model of one A100 GPU, with the ridge point at an arithmetic intensity of 6.8, 1448 GiB/s peak memory bandwidth and 10.5 TFLOP/s single precision peak performance. The characteristics are obtained for the default backend (blue) and the cuQuantum backend (green). In Fig.~\ref{fig:roofline}, it can be seen that all applications fall in the memory bound region, except from the QW application that has low bandwidth and compute intensity. However, the performance reached with the latter still states a big gap towards the roofline. When comparing the default backend with the cuQuantum backend, cuQuantum utilizes the compute resources more efficiently and, thus, reaches the roofline for all the other benchmarks, explaining the performance improvement observed earlier in Fig.~\ref{fig:scaling}.

% Figure environment removed

%
For state vector quantum circuit simulations, the memory usage for the state vector scales up exponentially with the number of qubits. Thus, single GPU simulation typically can only support simulations with qubits fewer than 32, as show in the previous experiments. We further scale up the scale of simulations by exploiting additional GPU memory in a multi-GPU setup. 

% Figure environment removed
% Figure environment removed

When performing the simulation on multiple GPUs, the state vector is divided into different chunks and communication is needed between the memory of the two GPUs. The most important factor determining the data movement is the number of Qiskit \texttt{\small{blocking\_qubits}}. To showcase the impact of the \texttt{\small{blocking\_qubits}} on two GPUs, we perform experiments with the QV benchmark with 33 qubits and both GPU backends in single and double precision, varying the number of blocking qubits. The results shown in Fig.~\ref{fig:blocking} suggest that, while for single precision the difference between 26-29 is negligible, for double precision a blocking factor of 28 is most beneficial for both backends. The same was confirmed for other benchmarks and, thus, a factor of 28 was chosen for all other experiments.

When studying the performance on multi-GPU systems, it is critical to understand the memory footprint and data movement across the two GPU memories, as now the state vector is divided into different chunks residing on different memories. Investigating the memory footprint on two GPUs, the total memory usage is 72~GB for all the applications and the two GPU backends. 

Overall, data movement between host and GPUs becomes the top limiting factor of performance, taking more than $90\%$ GPU time in all two-GPU experiments. To understand the data movement, we inspect the tracing of the quantum applications. In Fig. \ref{fig:timeline}, we show the trace of five applications using two GPUs with the Thrust backend. The traces show one experiment running the quantum circuit for the applications with the largest possible number of qubits (see \ref{tab:benchmarks_max}), except for QW as it was only executed on a single GPU. Three colors represent three main phases: memory copies from host to device (green), compute (light blue) and memory copy from the device to the host (red color). By analyzing Fig.~\ref{fig:timeline}, we see that  data movement is similar for all the applications -- there is an extensive data movement from host to device (H2D) and device-to-host (D2H) before and after GPU kernel execution with little to no overlapping. Note that Fig.~\ref{fig:timeline} presents only a part of the whole timeline due to space limit.


%
%


%

%

%

In general, the traces obtained running the cuQuantum backend are similar to the ones with the Thrust backend. However, a main difference can been seen when comparing the traces. In Fig.~\ref{fig:datatransfer}, we show a zoom-in the traces for GHZ application comparing the the Thrust and cuQuantum backends. We can see that, for the data transfer, the cuQuantum backend uses \texttt{cudaMemcpyAsync} to overlap kernel execution (light blue) with device to host data transfer (red). However, because the kernels are much shorter than data transfers, no overall performance improvement is observed.
% Figure environment removed
%

%
As we are using a multi-GPU node, we have the possibility of using MPI for dividing the workload between two GPUs. In an evaluation of distributed processing using MPI, we observe that the runtime increases significantly earlier when using MPI compared to the previous experiments that use an OpenMP based distribution across the two GPUs of our single node setup (Table~\ref{tab:mpi_nompi}). 
\begin{table}[tb]
    \caption{Runtime of the QV benchmark when utilizing two GPUs with the default Thrust backend, with and without MPI communication.}
    \centering
    \label{tab:mpi_nompi}
    \begin{tabular}{c|c|c|c}
        \toprule
        Qubits & Precision & Runtime [s] no MPI & Runtime [s] MPI \\
        \midrule
        \multirow{2}{*}{32} & SP & 10.456 & 15.194 \\
                            & DP & 14.349 & 108.060 \\
        \midrule
        \multirow{2}{*}{33} & SP & 21.041 & 149.562\\
                            & DP & 135.318 & 377.686\\
        \bottomrule
    \end{tabular}
\end{table}
% Figure environment removed

Fig.~\ref{fig:mpi} compares the traces with 33 qubits in single precision where the execution time is deviating significantly. It can be observed that the high \jennifer{H2D} and \jennifer{D2H} transfers are already present in the MPI version of this setup, while the non-MPI version reaches this stage with 33 qubits in double precision as shown earlier in Fig.~\ref{fig:timeline}.



