
@inproceedings{mcmahan_communication-efficient_2017,
	title = {Communication-efficient learning of deep networks from decentralized data},
	abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10–100x as compared to synchronized stochastic gradient descent.},
	booktitle = {AISTATS'17},
	author = {McMahan, H. B. and Moore, Eider and Ramage, Daniel and Hampson, Seth and Ag\"uera y Arcas, Blaise},
	year = {2017},
	annote = {This is THE federated learning paper
},
	file = {McMahan et al_2017_Communication-efficient learning of deep networks from decentralized data.pdf:/Users/chiara/Seafile/ZotLib/McMahan et al_2017_Communication-efficient learning of deep networks from decentralized data.pdf:application/pdf},
}

@software{lorenzo_valerio_2021_5780042,
  author       = {Lorenzo Valerio and
                  Chiara Boldrini and
                  Andrea Passarella},
  title        = {{SAI Simulator for Social AI Gossiping}},
  month        = dec,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {v1.0.0},
  doi          = {10.5281/zenodo.5780042},
  url          = {https://doi.org/10.5281/zenodo.5780042}
}

@article{deng2012mnist,
  title={{The MNIST database of handwritten digit images for machine learning research}},
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
}

@article{Roy2019,
	abstract = {Access to sufficient annotated data is a common challenge in training deep neural networks on medical images. As annotating data is expensive and time-consuming, it is difficult for an individual medical center to reach large enough sample sizes to build their own, personalized models. As an alternative, data from all centers could be pooled to train a centralized model that everyone can use. However, such a strategy is often infeasible due to the privacy-sensitive nature of medical data. Recently, federated learning (FL) has been introduced to collaboratively learn a shared prediction model across centers without the need for sharing data. In FL, clients are locally training models on site-specific datasets for a few epochs and then sharing their model weights with a central server, which orchestrates the overall training process. Importantly, the sharing of models does not compromise patient privacy. A disadvantage of FL is the dependence on a central server, which requires all clients to agree on one trusted central body, and whose failure would disrupt the training process of all clients. In this paper, we introduce BrainTorrent, a new FL framework without a central server, particularly targeted towards medical applications. BrainTorrent presents a highly dynamic peer-to-peer environment, where all centers directly interact with each other without depending on a central body. We demonstrate the overall effectiveness of FL for the challenging task of whole brain segmentation and observe that the proposed server-less BrainTorrent approach does not only outperform the traditional server-based one but reaches a similar performance to a model trained on pooled data.},
	author = {Roy, Abhijit Guha and Siddiqui, Shayan and P{\"o}lsterl, Sebastian and Navab, Nassir and Wachinger, Christian},
	date-added = {2023-04-03 19:28:17 +0200},
	date-modified = {2023-04-03 19:28:17 +0200},
	file = {PDF:/Users/lorenzo/Zotero/storage/2YG78YQN/2019 - Roy et al. - BrainTorrent A Peer-to-Peer Environment for Decentralized Federated Learning.pdf:application/pdf},
	journal = {arXiv},
	pages = {1--9},
	title = {{BrainTorrent}: {A} {Peer}-to-{Peer} {Environment} for {Decentralized} {Federated} {Learning}},
	url = {http://arxiv.org/abs/1905.06731},
	year = {2019},
	bdsk-url-1 = {http://arxiv.org/abs/1905.06731}}

@article{Lian2017,
	abstract = {Most commonly used distributed machine learning systems are either synchronous or centralized asynchronous. Synchronous algorithms like AllReduce-SGD perform poorly in a heterogeneous environment, while asynchronous algorithms using a parameter server suffer from 1) communication bottleneck at parameter servers when workers are many, and 2) significantly worse convergence when the traffic to parameter server is congested. Can we design an algorithm that is robust in a heterogeneous environment, while being communication efficient and maintaining the best-possible convergence rate? In this paper, we propose an asynchronous decentralized stochastic gradient decent algorithm (AD-PSGD) satisfying all above expectations. Our theoretical analysis shows AD-PSGD converges at the optimal O(1/ p K) rate as SGD and has linear speedup w.r.t. number of workers. Empirically, AD-PSGD outperforms the best of decentralized parallel SGD (D-PSGD), asynchronous parallel SGD (A-PSGD), and standard data parallel SGD (AllReduce-SGD), often by orders of magnitude in a heterogeneous environment. When training ResNet-50 on ImageNet with up to 128 GPUs, AD-PSGD converges (w.r.t epochs) similarly to the AllReduce-SGD, but each epoch can be up to 4-8× faster than its synchronous counterparts in a network-sharing HPC environment. To the best of our knowledge, AD-PSGD is the first asynchronous algorithm that achieves a similar epoch-wise convergence rate as AllReduce-SGD, at an over 100-GPU scale.},
	author = {Lian, Xiangru and Zhang, Wei and Zhang, Ce and Liu, Ji},
	date-added = {2023-04-03 19:05:21 +0200},
	date-modified = {2023-04-03 19:05:21 +0200},
	file = {PDF:/Users/lorenzo/Zotero/storage/FBPBAXW2/2017 - Lian et al. - Asynchronous Decentralized Parallel Stochastic Gradient Descent.pdf:application/pdf},
	journal = {arXiv},
	title = {Asynchronous {Decentralized} {Parallel} {Stochastic} {Gradient} {Descent}},
	year = {2017}}

@misc{taya_decentralized_2021,
	abstract = {This paper proposes a fully decentralized federated learning (FL) scheme for Internet of Everything (IoE) devices that are connected via multi-hop networks. Because FL algorithms hardly converge the parameters of machine learning (ML) models, this paper focuses on the convergence of ML models in function spaces. Considering that the representative loss functions of ML tasks e.g, mean squared error (MSE) and Kullback-Leibler (KL) divergence, are convex functionals, algorithms that directly update functions in function spaces could converge to the optimal solution. The key concept of this paper is to tailor a consensusbased optimization algorithm to work in the function space and achieve the global optimum in a distributed manner. This paper first analyzes the convergence of the proposed algorithm in a function space, which is referred to as a meta-algorithm, and shows that the spectral graph theory can be applied to the function space in a manner similar to that of numerical vectors. Then, consensus-based multi-hop federated distillation (CMFD) is developed for a neural network (NN) to implement the meta-algorithm. CMFD leverages knowledge distillation to realize function aggregation among adjacent devices without parameter averaging. An advantage of CMFD is that it works even with different NN models among the distributed learners. Although CMFD does not perfectly reflect the behavior of the metaalgorithm, the discussion of the meta-algorithm's convergence property promotes an intuitive understanding of CMFD, and simulation evaluations show that NN models converge using CMFD for several tasks. The simulation results also show that CMFD achieves higher accuracy than parameter aggregation for weakly connected networks, and CMFD is more stable than parameter aggregation methods.},
	author = {Taya, Akihito and Nishio, Takayuki and Morikura, Masahiro and Yamamoto, Koji},
	file = {Taya et al. - 2021 - Decentralized and Model-Free Federated Learning C.pdf:/Users/lorenzo/Zotero/storage/I2KFDAN9/Taya et al. - 2021 - Decentralized and Model-Free Federated Learning C.pdf:application/pdf},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Computer Science - Networking and Internet Architecture},
	language = {en},
	month = dec,
	note = {arXiv:2104.00352 [cs]},
	publisher = {arXiv},
	shorttitle = {Decentralized and {Model}-{Free} {Federated} {Learning}},
	title = {Decentralized and {Model}-{Free} {Federated} {Learning}: {Consensus}-{Based} {Distillation} in {Function} {Space}},
	url = {http://arxiv.org/abs/2104.00352},
	urldate = {2022-09-29},
	year = {2021},
	bdsk-url-1 = {http://arxiv.org/abs/2104.00352}}

@article{Lalitha2019,
	abstract = {We propose a decentralized learning algorithm over a general social network. The algorithm leaves the training data distributed on the mobile devices while utilizing a peer to peer model aggregation method. The proposed algorithm allows agents with local data to learn a shared model explaining the global training data in a decentralized fashion. The proposed algorithm can be viewed as a Bayesian and peer-to-peer variant of federated learning in which each agent keeps a "posterior probability distribution" over a global model parameters. The agent update its "posterior" based on 1) the local training data and 2) the asynchronous communication and model aggregation with their 1-hop neighbors. This Bayesian formulation allows for a systematic treatment of model aggregation over any arbitrary connected graph. Furthermore, it provides strong analytic guarantees on converge in the realizable case as well as a closed form characterization of the rate of convergence. We also show that our methodology can be combined with efficient Bayesian inference techniques to train Bayesian neural networks in a decentralized manner. By empirical studies we show that our theoretical analysis can guide the design of network/social interactions and data partitioning to achieve convergence.},
	author = {Lalitha, Anusha and Wang, Xinghan and Kilinc, Osman and Lu, Yongxi and Javidi, Tara and Koushanfar, Farinaz},
	file = {PDF:/Users/lorenzo/Zotero/storage/YTE8YZP4/2019 - Lalitha et al. - Decentralized Bayesian Learning over Graphs.pdf:application/pdf},
	journal = {arXiv},
	title = {Decentralized bayesian learning over graphs},
	year = {2019}}

@article{savazzi_federated_2020,
	abstract = {Federated learning (FL) is emerging as a new paradigm to train machine learning (ML) models in distributed systems. Rather than sharing and disclosing the training data set with the server, the model parameters (e.g., neural networks' weights and biases) are optimized collectively by large populations of interconnected devices, acting as local learners. FL can be applied to power-constrained Internet of Things (IoT) devices with slow and sporadic connections. In addition, it does not need data to be exported to third parties, preserving privacy. Despite these benefits, a main limit of existing approaches is the centralized optimization which relies on a server for aggregation and fusion of local parameters; this has the drawback of a single point of failure and scaling issues for increasing network size. This article proposes a fully distributed (or serverless) learning approach: the proposed FL algorithms leverage the cooperation of devices that perform data operations inside the network by iterating local computations and mutual interactions via consensus-based methods. The approach lays the groundwork for integration of FL within 5G and beyond networks characterized by decentralized connectivity and computing, with intelligence distributed over the end devices. The proposed methodology is verified by the experimental data sets collected inside an Industrial IoT (IIoT) environment.},
	author = {Savazzi, Stefano and Nicoli, Monica and Rampa, Vittorio},
	doi = {10.1109/JIOT.2020.2964162},
	file = {Savazzi et al. - 2020 - Federated Learning With Cooperating Devices A Con.pdf:/Users/lorenzo/Zotero/storage/HHVEBQ66/Savazzi et al. - 2020 - Federated Learning With Cooperating Devices A Con.pdf:application/pdf},
	issn = {2327-4662, 2372-2541},
	journal = {IEEE Internet of Things Journal},
	language = {en},
	number = {5},
	pages = {4641--4654},
	shorttitle = {Federated {Learning} {With} {Cooperating} {Devices}},
	title = {Federated {Learning} {With} {Cooperating} {Devices}: {A} {Consensus} {Approach} for {Massive} {IoT} {Networks}},
	url = {https://ieeexplore.ieee.org/document/8950073/},
	urldate = {2023-04-03},
	volume = {7},
	year = {2020},
	bdsk-url-1 = {https://ieeexplore.ieee.org/document/8950073/},
	bdsk-url-2 = {https://doi.org/10.1109/JIOT.2020.2964162}}

@inproceedings{lalitha_fully_nodate,
    title={Fully decentralized federated learning},
  author={Lalitha, Anusha and Shekhar, Shubhanshu and Javidi, Tara and Koushanfar, Farinaz},
  booktitle={{NeurIPS worksh. on Bayesian Deep Learning}},
  year={2018}	
}


@inproceedings{boyd_gossip_2005,
	author = {Boyd, S. and Ghosh, A. and Prabhakar, B. and Shah, D.},
	booktitle = {Proceedings {IEEE} 24th {Annual} {Joint} {Conference} of the {IEEE} {Computer} and {Communications} {Societies}.},
	doi = {10.1109/INFCOM.2005.1498447},
	file = {Full Text:/Users/lorenzo/Zotero/storage/V5GYBGNH/Boyd et al. - 2005 - Gossip algorithms design, analysis and applicatio.pdf:application/pdf},
	pages = {1653--1664 vol. 3},
	title = {Gossip algorithms: design, analysis and applications},
	volume = {3},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/INFCOM.2005.1498447}}

@article{olfati-saber_consensus_2007,
	author = {Olfati-Saber, Reza and Fax, J. Alex and Murray, Richard M.},
	doi = {10.1109/JPROC.2006.887293},
	file = {Full Text:/Users/lorenzo/Zotero/storage/HYZR2LRW/Olfati-Saber et al. - 2007 - Consensus and Cooperation in Networked Multi-Agent.pdf:application/pdf},
	journal = {Proceedings of the IEEE},
	number = {1},
	pages = {215--233},
	title = {Consensus and {Cooperation} in {Networked} {Multi}-{Agent} {Systems}},
	volume = {95},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/JPROC.2006.887293}}

@inproceedings{schenato_distributed_2007,
	author = {Schenato, Luca and Gamba, Giovanni},
	booktitle = {2007 46th {IEEE} {Conference} on {Decision} and {Control}},
	doi = {10.1109/CDC.2007.4434671},
	file = {Full Text:/Users/lorenzo/Zotero/storage/49PSZG87/Schenato and Gamba - 2007 - A distributed consensus protocol for clock synchro.pdf:application/pdf},
	pages = {2289--2294},
	title = {A distributed consensus protocol for clock synchronization in wireless sensor network},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/CDC.2007.4434671}}

@article{aysal_broadcast_2009,
	author = {Aysal, Tuncer Can and Yildiz, Mehmet Ercan and Sarwate, Anand D. and Scaglione, Anna},
	doi = {10.1109/TSP.2009.2016247},
	file = {Full Text:/Users/lorenzo/Zotero/storage/5F5MIUDT/Aysal et al. - 2009 - Broadcast Gossip Algorithms for Consensus.pdf:application/pdf},
	journal = {IEEE Transactions on Signal Processing},
	number = {7},
	pages = {2748--2761},
	title = {Broadcast {Gossip} {Algorithms} for {Consensus}},
	volume = {57},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/TSP.2009.2016247}}

@article{zeng_nonconvex_2018,
	author = {Zeng, Jinshan and Yin, Wotao},
	doi = {10.1109/TSP.2018.2818081},
	file = {Full Text:/Users/lorenzo/Zotero/storage/4ESIAZV3/Zeng and Yin - 2018 - On Nonconvex Decentralized Gradient Descent.pdf:application/pdf},
	journal = {IEEE Transactions on Signal Processing},
	number = {11},
	pages = {2834--2848},
	title = {On {Nonconvex} {Decentralized} {Gradient} {Descent}},
	volume = {66},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TSP.2018.2818081}}

@inproceedings{sirb_consensus_2016,
	author = {Sirb, Benjamin and Ye, Xiaojing},
	booktitle = {2016 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	doi = {10.1109/BigData.2016.7840591},
	file = {Full Text:/Users/lorenzo/Zotero/storage/XXVX3BBQ/Sirb and Ye - 2016 - Consensus optimization with delayed and stochastic.pdf:application/pdf},
	pages = {76--85},
	title = {Consensus optimization with delayed and stochastic gradients on decentralized networks},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/BigData.2016.7840591}}

@inproceedings{xing_decentralized_2020,
	author = {Xing, Hong and Simeone, Osvaldo and Bi, Suzhi},
	booktitle = {2020 {IEEE} 21st {International} {Workshop} on {Signal} {Processing} {Advances} in {Wireless} {Communications} ({SPAWC})},
	doi = {10.1109/SPAWC48557.2020.9154332},
	file = {Full Text:/Users/lorenzo/Zotero/storage/YGFS7FCQ/Xing et al. - 2020 - Decentralized Federated Learning via SGD over Wire.pdf:application/pdf},
	pages = {1--5},
	title = {Decentralized {Federated} {Learning} via {SGD} over {Wireless} {D2D} {Networks}},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/SPAWC48557.2020.9154332}}

@article{lian_can_2017,
	author = {Lian, Xiangru and Zhang, Ce and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Wei and Liu, Ji},
	journal = {Advances in neural information processing systems},
	title = {Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent},
	volume = {30},
	year = {2017}}

@article{lalitha_peer--peer_2019,
	author = {Lalitha, Anusha and Kilinc, Osman Cihan and Javidi, Tara and Koushanfar, Farinaz},
	journal = {arXiv preprint arXiv:1901.11173},
	title = {Peer-to-peer federated learning on graphs},
	year = {2019}}

@article{savazzi_federated_2020-1,
	abstract = {Federated learning (FL) is emerging as a new paradigm to train machine learning (ML) models in distributed systems. Rather than sharing and disclosing the training data set with the server, the model parameters (e.g., neural networks' weights and biases) are optimized collectively by large populations of interconnected devices, acting as local learners. FL can be applied to power-constrained Internet of Things (IoT) devices with slow and sporadic connections. In addition, it does not need data to be exported to third parties, preserving privacy. Despite these benefits, a main limit of existing approaches is the centralized optimization which relies on a server for aggregation and fusion of local parameters; this has the drawback of a single point of failure and scaling issues for increasing network size. This article proposes a fully distributed (or serverless) learning approach: the proposed FL algorithms leverage the cooperation of devices that perform data operations inside the network by iterating local computations and mutual interactions via consensus-based methods. The approach lays the groundwork for integration of FL within 5G and beyond networks characterized by decentralized connectivity and computing, with intelligence distributed over the end devices. The proposed methodology is verified by the experimental data sets collected inside an Industrial IoT (IIoT) environment.},
	author = {Savazzi, Stefano and Nicoli, Monica and Rampa, Vittorio},
	doi = {10.1109/JIOT.2020.2964162},
	file = {Savazzi et al. - 2020 - Federated Learning With Cooperating Devices A Con.pdf:/Users/lorenzo/Zotero/storage/PYME8UXC/Savazzi et al. - 2020 - Federated Learning With Cooperating Devices A Con.pdf:application/pdf},
	issn = {2327-4662, 2372-2541},
	journal = {IEEE Internet of Things Journal},
	language = {en},
	month = may,
	number = {5},
	pages = {4641--4654},
	shorttitle = {Federated {Learning} {With} {Cooperating} {Devices}},
	title = {Federated {Learning} {With} {Cooperating} {Devices}: {A} {Consensus} {Approach} for {Massive} {IoT} {Networks}},
	url = {https://ieeexplore.ieee.org/document/8950073/},
	urldate = {2023-04-03},
	volume = {7},
	year = {2020},
	bdsk-url-1 = {https://ieeexplore.ieee.org/document/8950073/},
	bdsk-url-2 = {https://doi.org/10.1109/JIOT.2020.2964162}}

@article{Nedic:2017aa,
	author = {Nedic, Angelia and Olshevsky, Alex and Shi, Wei},
	journal = {SIAM Journal on Optimization},
	note = {Publisher: SIAM},
	number = {4},
	pages = {2597--2633},
	title = {Achieving geometric convergence for distributed optimization over time-varying graphs},
	volume = {27},
	year = {2017}}

@article{Yuan:2016aa,
	author = {Yuan, Kun and Ling, Qing and Yin, Wotao},
	journal = {SIAM Journal on Optimization},
	note = {Publisher: SIAM},
	number = {3},
	pages = {1835--1854},
	title = {On the convergence of decentralized gradient descent},
	volume = {26},
	year = {2016}}


 %NETWORK CITATIONS
 @article{lee2019review,
  title={A review of stochastic block models and extensions for graph clustering},
  author={Lee, Clement and Wilkinson, Darren J},
  journal={Appl. Netw. Sci.},
  volume={4},
  number={1},
  pages={1--50},
  year={2019},
  publisher={SpringerOpen}
}
@article{erdHos1960evolution,
  title={On the evolution of random graphs},
  author={Erd{\H{o}}s, Paul and R{\'e}nyi, Alfr{\'e}d and others},
  journal={Publ. Math. Inst. Hung. Acad. Sci},
  volume={5},
  number={1},
  pages={17--60},
  year={1960}
}
@article{albert2002statistical,
  title={Statistical mechanics of complex networks},
  author={Albert, R{\'e}ka and Barab{\'a}si, Albert-L{\'a}szl{\'o}},
  journal={{Rev. Mod. Phys.}},
  volume={74},
  number={1},
  pages={47},
  year={2002},
  publisher={APS}
}
@article{yann1998mnist,
  title={{The MNIST database of handwritten digits}},
  author={Yann, Lecun},
  journal={R},
  year={1998}
}


@article{sun_decentralized_2023,
    author = {T. Sun and D. Li and B. Wang},
    journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
    title = {Decentralized Federated Averaging},
    year = {2023},
    volume = {45},
    number = {04},
    issn = {1939-3539},
    pages = {4289-4301},
    abstract = {Federated averaging (FedAvg) is a communication-efficient algorithm for distributed training with an enormous number of clients. In FedAvg, clients keep their data locally for privacy protection; a central parameter server is used to communicate between clients. This central server distributes the parameters to each client and collects the updated parameters from clients. FedAvg is mostly studied in centralized fashions, requiring massive communications between the central server and clients, which leads to possible channel blocking. Moreover, attacking the central server can break the whole system&#x27;s privacy. Indeed, decentralization can significantly reduce the communication of the busiest node (the central one) because all nodes only communicate with their neighbors. To this end, in this paper, we study the decentralized FedAvg with momentum (DFedAvgM), implemented on clients that are connected by an undirected graph. In DFedAvgM, all clients perform stochastic gradient descent with momentum and communicate with their neighbors only. To further reduce the communication cost, we also consider the quantized DFedAvgM. The proposed algorithm involves the mixing matrix, momentum, client training with multiple local iterations, and quantization, introducing extra items in the Lyapunov analysis. Thus, the analysis of this paper is much more challenging than previous decentralized (momentum) SGD or FedAvg. We prove convergence of the (quantized) DFedAvgM under trivial assumptions; the convergence rate can be improved to sublinear when the loss function satisfies the PŁ property. Numerically, we find that the proposed algorithm outperforms FedAvg in both convergence speed and communication cost.},
    keywords = {servers;convergence;costs;training;collaborative work;peer-to-peer computing;privacy},
    doi = {10.1109/TPAMI.2022.3196503},
    publisher = {IEEE Computer Society}
}
