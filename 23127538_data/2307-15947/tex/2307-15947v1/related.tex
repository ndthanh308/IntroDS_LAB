
%!TEX root = main.tex
\vspace{-10pt}
\section{Related work}
\label{sec:rel_work}
% Decentralised learning has been extensively investigated. Initial papers investigated the problem of computing the average over a set of decentralised data physically located on a set of devices connected through a graph-like network topology (e.g., the enabling scenario was related to averaging data from sensor networks) \cite{boyd_gossip_2005,aysal_broadcast_2009}. Further studies investigated Decentralised Gradient Descent (DGD) \cite{Nedic:2017aa, Yuan:2016aa}, analysing its convergence properties under convexity assumptions. In \cite{zeng_nonconvex_2018}, the authors prove that convexity is not a strict requirement for convergence, paving the way for using DGD to address problems of non-convex optimisation. Beyond DGD, also SGD received attention in decentralised settings. In  \cite{sirb_consensus_2016}, the authors proposed a decentralised SGD algorithm and provided convergence guarantees.
% Furthermore, the paper in \cite{lian_can_2017} analyses the communication efficiency of DSGD, considering that the updates' exchange between nodes is synchronous. Their results show that decentralised SGD can be more efficient than centralised SGD, especially when the communication bandwidth is limited. The asynchronous version of DSGD is analysed in \cite{Lian2017}. 

% A different research line investigates 
DFL extends the typical settings of FL, e.g., data heterogeneity and non-convex optimisation, by removing the existence of the central parameter server. %, which typically oversees and coordinates the whole learning process. 
This is a relatively new topic that is gaining attention from the community since it fuses the privacy-related advantages of FL with the potentialities of decentralised and uncoordinated optimisation and learning. 
 % that the information shared with the peers is communicated right after one local update. Others relax this assumption, considering multiple local updates to the local model before sharing the local parameters or the local gradients with the peers.
In \cite{Roy2019}, the authors define a DFL framework for a medical application where a number of hospitals collaborate to train a Neural Network model on local and private data. In \cite{Lalitha2019} the authors propose a Bayesian-like approach where the aggregation phase is done by minimising the Kullback-Leibler divergence between the local model and the ones received from the peers. All these approaches are still considering that the nodes perform just one local update before sharing the parameters (or gradients) with the peers in the network. This aspect is relaxed in \cite{savazzi_federated_2020} and \cite{sun_decentralized_2023}. In \cite{savazzi_federated_2020}, the authors propose a federated consensus algorithm extending FedAvg from~\cite{mcmahan_communication-efficient_2017} in decentralised settings, mainly considering industrial and IoT applications. The authors of \cite{sun_decentralized_2023} propose a Federated Decentralised Average based on SGD where the authors include a momentum term to counterbalance the possible drift introduced by the multiple updates and a quantization scheme to reduce communications. 

Most of these papers assume a decentralised system made of a few nodes connected through controlled network topologies, e.g., rings and full meshes. In this paper, we start exploring how the network topology affects the learning process of different decentralised learning schemes under data heterogeneity. To the best of our knowledge, this is the first paper that considers complex network topologies and relates their key features to the performance of  decentralised learning.


