% \vspace{-13pt}
\section{Introduction}
\label{sec:intro}

We are witnessing a paradigm shift from centralised AI systems to decentralised ones, motivated by the fact that 
% . The de facto standard approach to knowledge extraction from data generated at the edge of the network is typically centralised, i.e., the data collected at the edge is transferred to the cloud for training large-scale AI models, which will be used for providing AI-based services. Despite its effectiveness, such an approach raises several questions regarding data privacy and ownership. People, industries, and, more in general, 
data generators at the edge of the network are increasingly less inclined to share their private data with third parties, even under the promise of gaining an advantage from centralized AI-based services. 
%
Solutions based on decentralised AI systems, such as Federated Learning (FL)~\cite{mcmahan_communication-efficient_2017}, are promising candidates to address these concerns. In FL, the idea is to let the edge devices keep the data locally and have a central role in the knowledge extraction process. Specifically, the devices collaboratively train an AI model without sharing any raw data with each other. The only information shared is the parameters of the AI models that are trained locally. These parameters are subsequently aggregated to form a better and capable \emph{global} model that is iteratively refined through successive rounds of collaboration. 

The standard definition of the FL framework assumes a starred network topology, i.e., a central parameter server oversees the entire process, coordinating the operations of many clients. However, the presence of a centralised controller might also represent a single point of failure, a potential bottleneck when the number of devices collaborating scales up to millions of devices, and an obstacle to spontaneous, direct collaborations among users. 
Decentralised Federated Learning (DFL) represents an alternative to centralised Federated Learning. In DFL the connectivity between devices is represented by a generic graph, and the devices involved in the learning process typically collaborate only with their neighbours. 
The lack of a central controller overcomes the single point of failure problem but introduces other aspects that need to be investigated. Specifically, we claim that, in this scenario, the information locality and the network topology strongly affect the dynamics of the learning process, i.e. how fast and effective the spreading of knowledge about the class labels is.  

While previous work~\cite{sun_decentralized_2023} assumes that the network topology can be controlled by the network operator and optimized to make the learning process more efficient or scalable, we argue that complete decentralization can only be achieved by letting user devices spontaneously organize themselves. This implies that the network topology, in these settings, cannot be controlled by the operator. For example, an edge between two nodes in the graph may represent a trust relationship or willingness to cooperate. If the edges are weighted, the strength of the weights expresses the intensity of trust or cooperation. With this approach, users are free to cooperate with whomever they want, and the operator has no control over the cooperation patterns. Although this scenario poses a challenge from a learning perspective, it also fully exploits the human-centric, impromptu potential of fully decentralized learning systems.

In light of these considerations, the research question we tackle in this paper is how the network topology affects the learning process in a fully decentralized learning system. Specifically, we consider a scenario where a set of devices that are connected through a complex network topology collaborate to train a \emph{common} AI model in a completely decentralised fashion. According to the DFL framework, each device in the system receives a set of models from its neighbours in the graph. These models are first aggregated with the local one (typically through a weighted average) to get a refreshed aggregate model, which is further updated through a number of local training epochs (on local data). Finally, the newly updated models are shared between the neighbours. The devices repeat this until a stopping condition is met.

In our paper, we consider three topologies: Erdos-Renyi, Barabasi-Albert, and Stochastic Block Model, and we analyse through simulations how the learning process is affected by them, considering different non-IID data partitioning settings. The main take-home messages are the following: (i) the initial data distribution on high vs low degree nodes plays a key role in the final accuracy of a decentralized learning process, (ii) when high-degree nodes possess more knowledge, such knowledge spreads  easily in the network, (iii) vice versa, when low-degree nodes have more knowledge, knowledge spreads better when the network is less connected (at first counterintuitive, but connectivity dilutes knowledge in average-based decentralized learning), (iv) when users are grouped in tightly knit communities, it is very difficult for knowledge to circulate outside of the community.
%     - la disposizione iniziale dei dati gioca un ruolo fondamentale sull'accuracy (edge-focus vs hub-focus)
% - quando gli hub hanno piu' conoscenza, questa si diffonde sostanzialmente in modo indipendente dal resto
% - la connettivit√† ha effetti complessi e a volte controintuitivi. In reti debolmente connesse, gli hub non accedono alla conoscenza, ma altri nodi si. In media la conoscenza non circola molto, ma alcuni nodi riescono a scappare dagli hub. Quando la rete diventa piu' connessa c'e' un beneficio generale (in media), ma anche un effetto di trapping dovuto agli hub che non permettono ad altri nodi di accedere alla conoscenza delle foglie.
% - la particolare topologia (con coda pesante o meno) qualitativamente non incide su questi fenomeni.


% Our analysis highlights the different roles that more or less connected nodes (hubs and leaves) have in the learning process, as well as that of the macroscopic network properties (primarily, degree distribution and modularity). 
 % Information flow is a fundamental notion in network research that relates to the movement of data through a network, i.e the transfer of information or signals between nodes in a network.
 % It is relevant in many fields, including communication networks, social networks, and biological networks, where the flow of information plays a crucial role in the behavior of the system.

% There are various ways to study information flow in networks, including random walks, diffusion processes, and percolation theory. The study of information flow in different network topologies has applications in a wide range of fields, including communication networks, social networks, biological networks, and more.
% Our approach consists in using a decentralized architecture, where the nodes communicate directly with each other, without the presence of a centralized coordinator. Here, we analyzed the information flow by studying the ability of the nodes to share their knowledge to the other nodes in the network. Here, we concern ourselves with the information flow in Federated learning. In federated learning, information flow refers to the process of exchanging and aggregating model updates between different participating devices or clients. In this setting, each client has access to some local data and trains a machine learning model on that data. However, instead of sending the raw data to a central server, the client sends only the model updates (i.e., changes in the model's parameters) to the server. The server then aggregates the updates from multiple clients to create an improved global model, which is then sent back to the clients to continue training.
 
%  Of course, different network topologies have different properties that affect the flow of information.
% For example, in a regular network, where each node has the same number of neighbors, the flow of information is constrained by the regular structure of the network.
% In a random network, the flow of information is less constrained, but the lack of structure can result in longer path lengths and less efficient communication. In a scale-free network, where the degree distribution follows a power law, the flow of information is influenced by the presence of highly connected hub nodes.

%The \textit{proceedings} are the records of a conference.
%ACM seeks to give these conference by-products a uniform,
%high-quality appearance.  To do this, ACM has some rigid
%requirements for the format of the proceedings documents: there
%is a specified format (balanced  double columns), a specified
%set of fonts (Arial or Helvetica and Times Roman) in
%certain specified sizes (for instance, 9 point for body copy),
%a specified live area (18 $\times$ 23.5 cm [7" $\times$ 9.25"]) centered on
%the page, specified size of margins (1.9 cm [0.75"]) top, (2.54 cm [1"]) bottom
%and (1.9 cm [.75"]) left and right; specified column width
%(8.45 cm [3.33"]) and gutter size (.83 cm [.33"]).

%The good news is, with only a handful of manual
%settings\footnote{Two of these, the {\texttt{\char'134 numberofauthors}}
%and {\texttt{\char'134 alignauthor}} commands, you have
%already used; another, {\texttt{\char'134 balancecolumns}}, will
%be used in your very last run of \LaTeX\ to ensure
%balanced column heights on the last page.}, the \LaTeX\ document
%class file handles all of this for you.

%The remainder of this document is concerned with showing, in
%the context of an ``actual'' document, the \LaTeX\ commands
%specifically available for denoting the structure of a
%proceedings paper, rather than with giving rigorous descriptions or explanations of such commands.

