% \section{Related work}
% \label{sec:rel_work}

%=================================================================================================================
% \vspace{-8pt}
\section{Decentralized learning}
\label{sec:system_model}
% \vspace{-3pt}
% descriviamo il nostro modello di rete e approccio di learning

We represent the network connecting the nodes as $\mathcal{G}(\mathcal{V},\mathcal{E})$, where $\mathcal{V}$ denotes the set of nodes and $\mathcal{E}$ the set of edges. Without loss of generality, we assume that the graph represents a social network. Exactly the same concepts can be translated to different application domains. We denote with $\omega_{ij}$ the weights on the edge between nodes $i$ and $j$ which, in the case of a social network, would represent the trust/social intimacy between the two nodes. The self-trust $\omega_{ii}$ is a pseudo-parameter with which we capture the importance placed by node $i$ on itself. We assume that only nodes sharing an edge are willing to collaborate with each other: effectively, we use the existence of a social relationship as a proxy of trust.

Each node $i \in \mathcal{V}$ is equipped with a local training dataset~$\mathcal{D}_i$ (containing tuples of features and labels $(x,y) \in \mathcal{X} \times \mathcal{Y}$) and a local model $h_i$ defined by weights $\mathbf{w}_i$, such that $h_i(\mathbf{x}; \mathbf{w}_i)$ yields the prediction of label $y$ for input $\*x$. Let us denote with $\mathcal{D} = \bigcup_i \mathcal{D}_i$ and with $\mathcal{P}$ the label distribution in $\mathcal{D}$. In general, $\mathcal{P}_i$ (i.e., the label distribution of the local dataset on node $i$) may be different from $\mathcal{P}$. This captures a realistic non-IID data distribution. 
%
At time 0, the model $h(\cdot; \*w_i)$ is, as usual, trained on local data, by minimizing a target loss function $\ell$ -- i.e., $\*w_i = \mathrm{argmin}_{\*w} \sum_{k = 1}^{|\mathcal{D}_i |} \ell(y_k, \*w \*x_k)$, with $(y_k, \*x_k) \in \mathcal{D}_i$. 
%
% In the following, we refer to $h(\cdot; \*w_i)$ as the \emph{isolated local model}.
%
% The goal of decentralized learning is to improve the isolated local models $h_i$ by building a local model $f_i$ that takes in information from both the isolated local model $h_i$ and the local models $f_j $from other nodes. 
% 
We assume that nodes entertain a certain number of communication rounds, where they exchange and combine local models. At each communication round, a given node receives the local models from its neighbors in the social graph, and averages it with its local model. 
%As this process resembles a conventional gossiping process in information diffusion, we name this scheme \emph{social AI gossiping}.
Specifically, at each step $t$, the local model of the given node and the local models from the node's neighbors are averaged as follows:
%
\begin{equation}
    \*w_i(t) \leftarrow \frac{\sum_{j \in \mathcal{N}(i)} \omega_{ij} \alpha_{ij} \*w_j(t-1)}{\sum_{j \in \mathcal{N}(i)} \omega_{ij}},
\end{equation}
%
where we have denoted with $\mathcal{N}(i)$ the neighborhood of node~$i$ including itself and $\alpha_{ij}$ is equal to $\frac{|\mathcal{P}_j|}{\sum_{j \in \mathcal{N}_i} | \mathcal{P}_j|}$ (and captures the relative weight of the local dataset of node $j$ in the neighborhood of node $i$).
%
Once the aggregation of models is performed, the local model is trained again on the local data (in this paper, we use learning rate $\eta$ and momentum $\mu$).

This strategy (hereafter, DecAvg) is the natural extension of FedAvg (the most well-known FL approach~\cite{mcmahan_communication-efficient_2017}) to a decentralized setting and is a social-aware generalization of similar strategies proposed in~\cite{sun_decentralized_2023,savazzi_federated_2020}: the aggregation is performed not by the central controller (as in federated settings) but by each node, whereby each node averages the model received from a given neighbor based on the strength of the social link connecting them.


%=================================================================================================================
\vspace{-5pt}
\section{Network topologies}
\label{sec:netowork_topologies}

In this paper, we analyze three different topologies to investigate how their different properties impact knowledge diffusion in a fully decentralized learning process: Erdos-Renyi (ER), Barabasi-Albert (BA), and  Stochastic Block Model (SBM) graphs.

%Erdos-Renyi model
The ER model is a model for generating random graphs with a homogeneous structure, where nodes are connected to each other with a fixed probability. 
ER is defined by two parameters: $N$, the number of nodes in the network, and $p$, the probability of an edge existing between any two nodes in the network (regardless of their degree). The ER model shows a phase transition when the fixed probability $p$ approaches the critical value $p^*=\ln(N)/N $~\cite{erdHos1960evolution}. Specifically, the value $p^*$ is a sharp threshold for the connectedness of the network: for values of $p$ above $p^*$ the network goes from being mostly disconnected to showing a growing clustering coefficient. 

%Barabasi-Albert model
The BA is an algorithm for generating random scale-free networks, i.e., networks with a power-law (or scale-free) degree distribution, using a preferential attachment mechanism~\cite{albert2002statistical}. In the BA model, nodes are connected preferentially based on their degree. Specifically, the probability of an edge forming between two nodes is proportional to the nodes' degree, which leads to the emergence of a scale-free degree distribution. Since the degree distribution follows a power law, few nodes have a very high degree while most nodes have a low degree. This can result in a structure with few well-connected hubs, which is known to facilitate information flow across the network. A BA network is defined by two parameters: $N$, the number of nodes in the network, and $m$, the number of edges added to the network for each new node (hence, the minimum degree of nodes). 

% Stochastic Block Model
The SBM is a probabilistic model for networks that exhibit a modular structure, i.e., the SBM generates a network with a clear community structure where nodes are grouped together based on their connectivity patterns~\cite{lee2019review}.
Nodes belonging to the same group are more closely connected to each other than to nodes in another group.
Formally, the SBM is defined by the following parameters: $N$, the number of nodes in the network;  $B$, the number of communities (called blocks); ${n_1, n_2, ..., n_B}$, the sizes of the blocks where $n_i$ is the number of nodes in block $i$; $p_{ij}$, the probability of an edge existing between a node in block $i$ and a node in block $j$ (with $p_{ii}$ the probability of links inside the block).

These three models each capture important properties of complex networks. ER graphs are homogeneous in terms of degree and with a low clustering coefficient. BA graphs are characterised by a very skewed degree distribution with few high-degree nodes and many low-degree nodes. Finally, SBM graphs feature a well-defined community structure.
%Termine Sezione Network Topologies

% \vspace{-10pt}
\section{Evaluation}
% \vspace{-5pt}

\subsection{Experimental settings}
\label{sub:settings}

% Study cases
In this paper, we consider unweighted graphs with 100 nodes, where edges are generated as follows. 

\textbf{ER.} Three different conditions regarding the parameter $p$ are analyzed: just below the critical value ($p \lessapprox p^*$), at the critical value ($p = p^*$) and just above the critical value ($p \gtrapprox p^*$). 

\textbf{BA.} Three different cases regarding the parameter of preferential attachment are chosen: $m = 2,5,10$, leading to networks with increasingly higher node degree.

\textbf{SBM.} Nodes are grouped into 4 communities of equal size (25 nodes each). The probability of extrinsic connections $(p_{ij}, \; j \neq i)$ is set to~0.01, whereas the probability of intrinsic connections $(p_{ii})$ is set to 0.8 in one case study and 0.5 in the second case study.

%Data distribution
For our experiments, we choose the widely used MNIST image dataset. This dataset contains a set of handwritten digits, thus data are divided into 10 classes. The goal of the analysis is to characterise the effect of the network topology in the knowledge spreading process, i.e., the ability of nodes to learn data patterns they have \emph{not} seen locally, but that other nodes in the network have seen. Therefore, we  split the MNIST classes across nodes as follows. Note that, on the assigned classes, each node gets the same amount of images.


For ER and BA networks, we divide the 10 MNIST classes into two groups: the first group (G1) is composed of classes 0, 1, 2, 3, 4, the second group (G2) of classes 5, 6, 7, 8, 9. All nodes receive an equal share (selected randomly) of data from G1. Data from G2 are allocated only to subsets of nodes. Specifically, we consider two different cases, whereby data in G2 are allocated to the 10\% highest-degree and lowest-degree nodes, respectively. The rationale is thus to allocate ``full knowledge" (i.e., a complete subset of all classes) either to high-degree or to low-degree nodes, and study the effect of the network topology in both cases. In the following, these configurations are referred to as ``hub-focused" and ``edge-focused", respectively. Specifically, starting from the node(s) with the highest (lowest) degree, we pick nodes until we reach 10\% of the network. In case adding all nodes at a given degree results in more than 10\% of the network, we randomly pick, among nodes with that degree, a subset that allows us to fill the 10\% subset.

For SBM networks, we divide the dataset classes into subsets based on the communities the nodes belong to, without overlap. Therefore, since we study  SBM topologies with 4 communities, each community gets two classes: community 1 sees classes 0 and 1; community 2 sees classes 2 and 3; community 3 sees classes 4 and 5; community 4 sees classes 6 and 7. This data distribution is designed to challenge the knowledge spreading process, since maximum learning accuracy can only be achieved if information from all the external communities is brought into the local one. 

% metriche 
For the learning task, we consider a simple classifier as the learned model and focus on two performance figures. On the one hand, we consider the accuracy over time at each node, to assess the effectiveness and speed of knowledge diffusion across the network. On the other hand, for SBM networks, we also investigated the average confusion matrix across nodes of the same community. Specifically, for each node we compute the confusion matrix for the MNIST classes, and then take the average across all nodes in the same community.
% - accuracy [all]
% - confusion matrix [solo sbm]

We implemented the DecAvg scheme within the custom SAISim simulator, available on Zenodo\footnote{\url{https://doi.org/10.5281/zenodo.5780042}}. SAISim is developed in Python and leverages state-of-the-art libraries such as PyTorch and NetworkX for deep learning and complex networks, respectively. On top of that, SAISim implements the primitives for supporting fully decentralized learning. The local models of nodes are Multilayer Perceptrons with three layers (sizes 512, 256, 128) and ReLu activation functions. SGD is used for the optimization, with learning rate~$0.001$ and momentum~$0.5$. 

% \vspace{-14pt}
\subsection{Results}
\label{sec:results}

% In this section, we present the results obtained for the considered network topologies.

% \vspace{-10pt}
\subsubsection{ER}
\label{sec:results_er}
As explained in Section~\ref{sub:settings}, for the ER model we want to evaluate scenarios around the critical value of connectedness. Considering our settings, the critical value $p^*$ is 0.046, hence the chosen values of $p$ are $0.03$, $0.046$, $0.05$. 
%
In Figure~\ref{fig:er_accuracy_tot} we show the evolution of the accuracy per node over time (each curve corresponds to one node). The top-row plots refer to the edge-focused case, where the digits in class G2 are assigned to leaf nodes, while the bottom-row plots correspond to the hub-focused scenario, where the well-connected nodes get all the images in G2. The first interesting result is that, in the edge-focused case, the separation between the curves of the nodes having more data and those that have fewer data is more evident. This means that when leaf nodes possess the missing classes (G2 images), it is harder for knowledge to circulate.
%
This finding holds when we vary $p$ (which increases from left to right). However, in the edge-focused scenario with below-critical $p$ (top left corner), we see that some nodes are eventually able to reach the accuracy of the nodes holding more data. But as we go from left to right, i.e., from less to more connected networks, this ability vanishes. This is quite interesting as well as counterintuitive. When the network is more connected, high-degree nodes (we refrain from calling them hubs, as in ER networks hubs do not exist) get proportionally more connected than leaf nodes. As G2 data are present only in leaf nodes, the additional connectivity plays against knowledge diffusion from leaves, as leaves' models are averaged among larger sets of nodes (due to higher degrees of more connected nodes), and therefore their knowledge ``weighs" less in the average. This is also clear in Figure~\ref{fig:er_accuracy_lessdata_hubshighlight}, where we show the accuracy only for nodes that are only assigned G1 images, and highlight the curves of the highest-degree nodes, which remain at low accuracy. The more the network is connected, the more high-degree nodes ``drag" other nodes toward their performance. In the hub-focused case (lower part of Figure~\ref{fig:er_accuracy_tot}) results are more intuitive, as the highest degree nodes enjoy higher accuracy, and are able to drag all the other nodes closer and closer to their performance.

In Figure~\ref{fig:er_mean_acc_std_all}, we aggregate the accuracy among all nodes in the same experiment and we show the evolution over time of the average and standard deviation of the accuracy. 
Coherently with Figure~\ref{fig:er_accuracy_tot}, the average accuracy in the edge-focus case is lower than in the hub-focus case. This is again due to high-degree nodes (in the former case) blocking the spreading of knowledge from leaves.
An interesting feature can be highlighted by comparing, in both cases, the standard deviation curves for networks below the critical threshold ($p = 0.03$, blue and orange curves in the figure) and the rest, which clearly show a lower rate of decay (after an initial increase). This can be attributed to the effect of longer paths when $p = 0.03$ (because the network is less connected), which hinders the alignment of local models across nodes, which are in this case farther away than for more connected networks (values of $p$ at and beyond the critical threshold). 
%
Also, the standard deviation values are generally higher in the edge-focus case, as hubs hinder other nodes to incorporate knowledge from leaves. This effect, again, is less pronounced for less connected networks, where some nodes are able to ``escape" the dragging effect of high-degree nodes, thus resulting in higher accuracy (but higher standard deviation) at the level of the entire network.

Finally, we can observe another overall effect by jointly analysing the accuracy per node, the average accuracy and the standard deviation for the edge-focus case. Specifically, \emph{on average} nodes in the least connected network achieve lower accuracy (Figure~\ref{fig:er_mean_acc_std_all}, orange and blue curves) even though, as a side effect of the higher standard deviation, some non-leaf nodes are able to increase their accuracy escaping the attraction effect of high-degree nodes. As the network becomes more and more connected, no node in the ``non-leaf and non-hub" class is able to reach the same accuracy, even though, on average, the overall accuracy increases. This means that, as a side effect of additional connectivity, hubs tend to increase their accuracy, even though they block other nodes to efficiently incorporate models of leaf nodes.

%For the mean accuracy plot, we see that the edges focus scenario is fairly below the hubs focus one. Both the scenarios show a visible separation between the accuracy belonging to the network having value of $p$ below the critical value and the ones above. Looking at the standard deviation plot, we can see that the edges focused scenarios show a higher standard deviation. Note that the edges focus scenario lines belonging to the networks of value $p$ above the critical value have standard deviation close, but with a slower decay rate, to the one of the hubs focus scenario with value $p = 0.03$. Thus, we argue that the slower decay rate comes from the average shortest path length. It, in fact, decrements drastically from values of $p$ below the critical value and the ones above. This means that the number of steps required to reach nodes far from each other is less in the second case. Therefore, the variability between nodes is more difficult to eliminate and requires more communication rounds.

% Figure environment removed%
% Figure environment removed%
% Figure environment removed


\subsubsection{BA}
\label{sec:results_ba}

As discussed earlier, for the Barabasi-Albert model we analyze three different settings varying the parameter related to the preferential attachment: $m \, = \,2,5,10$.
%
We show the accuracy over time in Figure~\ref{fig:ba_accuracy_tot}. First, in the hub-focused case (bottom row), the performance for varying values of the minimum degree $m$ is basically indistinguishable (this is confirmed by looking at the average and standard deviation of the accuracy in Figure~\ref{fig:ba_mean_acc_std_all}). This means that hubs (in this case, high-degree nodes are \emph{real} hubs) spread knowledge extremely efficiently, irrespective of the connectivity of the rest of the nodes.
%
% The edge-focus case (top row of Figure~\ref{fig:ba_accuracy_tot} and Figure~\ref{fig:ba_accuracy_lessdata_hubshighlight}) is more interesting. As was the case with ER networks, also in this case higher values of $m$ (hence, higher overall connectivity) result in hubs dragging all the other nodes towards lower accuracy values, not allowing anyone to ``escape" from their attraction. Qualitatively speaking, the patterns are exactly the same as in the case of ER networks\footnote{Note that a direct comparison between cases for the two topologies is not possible, as the number of edges in the networks is different.}. All in all, as the network becomes more connected (higher $m$), hubs benefit, as they tend to increase their accuracy. They also make the non-leaf nodes more compact, dragging them closer to their own performance level. On average, the effect is beneficial as the average accuracy increases. However, no node can reach the accuracy level of the leaf nodes.
%
The edge-focused case (top row of Figure~\ref{fig:ba_accuracy_tot}) is more interesting. As was the case with ER networks, leaf nodes are not able to spread their knowledge efficiently and the accuracy gap between leaf nodes and non-leaf nodes remains strong all throughout. In Figure~\ref{fig:ba_mean_acc_std_all}, we observe that larger values of $m$ (i.e., stronger connectivity) help improve the average accuracy but not significantly, while the variability is reduced. 
%
The hubs in the edge-focused scenario seem to benefit from the existence of lower-degree nodes, corresponding to smaller $m$ (Figure~\ref{fig:ba_accuracy_lessdata_hubshighlight}). Vice versa, when the degree of the other nodes increases, their accuracy is dragged down by them. This results, though, in better average accuracy and reduced variability.

%First, let us analyze the accuracy of each node. Focusing on the differences between the two scenarios, as we can see from Figure~\ref{fig:accuracy_tot}, in the edges focused case the separation from the nodes having more data to those that have less data is more accentuated. We can see that the nodes with less data are unable to reach the accuracy line of those holding more data. This means that the edges are unable to properly share their information, which is shaded in the averaging step of updating the parameters by the hubs. Whereas, in the second case, the hubs are able to carry the other nodes to reach their level of accuracy. This reasoning is more visible in \ref{fig:accuracy_lessdata_hubshighlight}, where we show the accuracy of all the nodes that holds less data and highlighted the nodes that have higher degrees (the hubs). As we can see, increasing the parameter $m$ going from left to right, the influence of the hubs that drag the other nodes to their level of accuracy, thus restraining the width of the curve. Same can be draw from \ref{fig:mean_acc_std_all}. It shows the mean accuracy of all the experiments and the standard deviation of each experiment between all nodes. Regarding the mean accuracy: the edges focused scenario is systematically below the hubs focused scenario. We can see that the hubs focused scenario is basically independent from $m$, whereas for the edges focused one the separation is visible. It goes analogously for the standard deviation: we can see that the edges focused scenarios (blue, green and violet lines) show a higher standard deviation. This confirms that the edges are unable to gather the other nodes together, thus showing a higher variability. 
% Figure environment removed%
% Figure environment removed%
% Figure environment removed


\subsubsection{SBM}
\label{sec:results_sbm}

The SBM topology is different from the previous two as it features four clearly separated communities, with sporadic intercommunity links. For the intracommunity connectivity, we test two scenarios (lower and higher intracommunity connectivity, corresponding to $p_{ii}= 0.5$ and $p_{ii}= 0.8$). Recall that each community holds two non-overlapping MNIST classes (hence, classes 8 and 9 are discarded). Using only intracommunity information, nodes can at most achieve a 0.25 accuracy (perfect classification of the two classes in their training data, zero knowledge on the other six). In order to go beyond 0.25, knowledge must be circulated across communities. The question is whether the occasional intercommunity edges are enough for that, and if the densely connected communities will make it harder for external information to percolate internally. Figure~\ref{fig:sbm_mean_acc_per_community} shows that the latter is happening: with $p_{ii}=0.5$ (less dense communities) the average accuracy grows faster than with $p_{ii}=0.8$. 
%At the same time, the figure shows that the standard deviation of the accuracy is higher for $p_{ii}=0.5$. 
The figure also reveals the presence of stragglers, for whom catching up with the rest of the network takes some time. Interestingly, entire communities appear to be stragglers. Table~\ref{tab:confmat_05} shows that communities are, as expected, very good at classifying classes they have in their local training data. Vice versa, it is very hard for external knowledge to enter the communities. Table~\ref{tab:confmat_05} also shows the number of links pointing towards external communities, which are the conduit for knowledge diffusion. Community 2 enjoys fewer external links, and indeed its learning process is very slow and mediocre (Figure~\ref{fig:sbm_mean_acc_per_community}). However, Community 1 has only slightly fewer links than Community 3, but its learning process is faster and more accurate. We argue that the specific classes assigned to communities might play a role in this case, and we will investigate this aspect in future work.

% If we look at a compact version of the confusion matrix in Table~\ref{tab:confmat_05} (we omit the one for $p_{ii}=0.8$ due to lack of space but the trend is similar), 


% Consider, as an example, Community 1 in Table~\ref{tab:confmat_05}. Despite being connected with Community 0 (which is assigned classes 0 and 1), Community 1 can weakly learn the 0's but not the 1's. The same happens with classes 6 and 7, which Community 1 receives information about from Community 3. Overall, we have shown that the modular structure of the SBM network acts as a barrier to information spreading, with knowledge coming from external communities being watered down in the model averaging. 


% Figure~\ref{fig:sbm_mean_acc_std_all} shows that the latter is happening: with $p_{ii}=0.5$ (less dense communities) the average accuracy grows faster than with $p_{ii}=0.8$. At the same time, the figure shows that the standard deviation of the accuracy is higher for $p_{ii}=0.5$. This reveals the presence of stragglers, for whom catching up with the rest of the network takes some time. Interestingly, entire communities appear to be stragglers (Figure~\ref{fig:sbm_mean_acc_per_community}). Consider, as an example, Community 1 in Table~\ref{tab:confmat_05}. Despite being connected with Community 0 (which is assigned classes 0 and 1), Community 1 can weakly learn the 0's but not the 1's. The same happens with classes 6 and 7, which Community 1 receives information about from Community 3. Overall, we have shown that the modular structure of the SBM network acts as a barrier to information spreading, with knowledge coming from external communities being watered down in the model averaging. 

% The modular structure of the SBM network limits the ability of nodes, connected to other communities they don't belong to, to share their information: in both cases, the maximum accuracy that can be reached is limited to $0.25$. This can be seen in Figure : the curve belonging to the best performing node in the $p_{ii}\,=\,0.5$ case is above the one belonging to the best performing node in the $p_{ii} \, = \, 0.8$ case. 
% This is confirmed in Figure \ref{fig:sbm_mean_acc_std_all} (right) where we plotted the mean accuracy between all nodes for the two cases under study. This also explains the higher standard deviation showed by the SBM with $p_{ii} \,=\, 0.5$ with respect to the standard deviation of the $0.8$ case (Figure \ref{fig:sbm_mean_acc_std_all} left). 
% Focusing on the single communities of the SBM networks: the 0 community shows an overall mean accuracy higher than the others, whereas the other communities show very little improvement on their accuracy (see Figure \ref{fig:mean_acc_per_community}). By looking at Table \ref{tab:confmat_05} and Table \ref{tab:confmat_08} we can see that the 0 community column has higher values than the others belonging to the other communities.

% Overall, we showed that the modular structure of the SBM network acts as a barrier to information spread by dragging down the information coming from other communities.

% % Figure environment removed

% % Figure environment removed

% Figure environment removed
\begin{table}[h!]
   \footnotesize
   \centering
    % \begin{adjustbox}{max width=.5\textwidth}
    % \begin{tabular}{|l|l|l|l|l|l|}
   \begin{tabular}{@{}lllll@{}}
   \toprule
       \textbf{Class} & \textbf{Comm. 1} & \textbf{Comm. 2} & \textbf{Comm. 3} & \textbf{Comm. 4} \\
       & (-, 5,9,7) & (5,-,7,3) & (9,7,-,8) & (7,3,8,-)\\\midrule
       0 & 0.9961 & 0.0002 & 0.0004 & 0.0043 \\ 
       1 & 0.9992 & 4e-05 & 0.0684 & 0.0079 \\ \hline
       2 & 0.0 & 0.9868 & 4e-05 & 0 \\ 
       3 & 0.0146 & 0.9802 & 0 & 0.0002 \\ \hline
       4 & 0.1824 & 0.0076 & 0.9971 & 0.0006 \\ 
       5 & 0.0011 & 4e-05 & 0.9972 & 0.0011 \\ \hline
       6 & 0.0039 & 0 & 0.0003 & 0.9979 \\
       7 & 0.272 & 0.0101 & 0.0225 & 0.9966 \\ \bottomrule \\
   \end{tabular} 
    % \end{adjustbox}
\caption{Accuracy per MNIST class and community for SBM with $p_{ii} \, = \, 0.5$. For each community, we report in square brackets the number of edges pointing toward external community 1, 2, 3, 4, respectively.} 
\label{tab:confmat_05}
\end{table}

%%%% MOST RECENT
% \begin{table}[t]
%     \scriptsize
%     \centering
%     \begin{tabular}{@{}lllll@{}}
%     \toprule
%         \textbf{Class} & \textbf{Comm. 1} & \textbf{Comm. 2} & \textbf{Comm. 3} & \textbf{Comm. 4} \\ 
%         & (-, 5,9,7) & (5,-,7,3) & (9,7,-,8) & (7,3,8,-)\\\midrule
% 0 & 0.99612/0.999 & 0.0002/0.951  & 0.00041/1 & 0.00429/1   \\
% 1 & 0.99922/1   & 4e-05/1     & 0.06844/1 & 0.00793/1   \\ \hline
% 2 & 0.0/0.983     & 0.98682/1   & 4e-05/1   & 0.0/1       \\
% 3 & 0.01461/0.926 & 0.98016/1   & 0.0/1     & 0.0002/1    \\
% \hline
% 4 & 0.18244/0.787 & 0.00762/0.951 & 0.99707/1 & 0.00057/0.967 \\
% 5 & 0.00112/0.867 & 4e-05/0.88    & 0.99717/1 & 0.00112/0.898 \\
% \hline
% 6 & 0.00392/0.941 & 0.0/0.931     & 0.00033/1 & 0.99787/0.999 \\
% 7 & 0.27198/0.698 & 0.01/1      & 0.02249/1 & 0.99658/1   \\ \bottomrule
% \end{tabular}
% \caption{Summary of the confusion matrix for SBM with $p_{ii} \, = \, 0.5$. For each community, we report in square brackets the number of edges pointing toward external community 1,2,3,4, respectively. In each cell, the first number gives the fraction of correctly classified images, the second number gives the fraction of images that are assigned (correctly or not) a label local to the community.}
% \label{tab:confmat_05}
% \end{table}

% \begin{table}[!t]
%     \scriptsize
%     \centering
%     % \begin{adjustbox}{max width=.5\textwidth}
%     % \begin{tabular}{|l|l|l|l|l|l|}
%     \begin{tabular}{@{}lllll@{}}
%     \toprule
%         \textbf{Class} & \textbf{Comm. 1} & \textbf{Comm. 2} & \textbf{Comm. 3} & \textbf{Comm. 4} \\ \midrule
%         0 & 0.9981 & 0.0 & 0.0001 & 0.0003 \\
%         1 & 0.9993 & 0.0 & 0.0109 & 0.0003 \\ \hline
%         2 & 0.0 & 0.9881 & 0.0 & 0.0 \\
%         3 & 0.0004 & 0.991 & 0.0 & 0.0 \\ \hline
%         4 & 0.0809 & 0.0033 & 0.9974 & 0.0002 \\ 
%         5 & 0.0001 & 0.0 & 0.9971 & 0.0002 \\ \hline
%         6 & 0.0 & 0.0 & 0.0 & 0.998 \\
%         7 & 0.1361 & 0.0006 & 0.0056 & 0.9965 \\ \bottomrule
%     \end{tabular}
%     % \end{adjustbox}
%     \caption{Diagonal of the confusion matrix for the SBM in the $p_{ii} \, = \, 0.8$ case.}
%     \label{tab:confmat_08}
% \end{table}




%Complete rules about using these environments and using the
%two different creation commands are in the
%\textit{Author's Guide}; please consult it for more
%detailed instructions.  If you need to use another construct,
%not listed therein, which you want to have the same
%formatting as the Theorem
%or the Definitionshown above,
%use the \texttt{{\char'134}newtheorem} or the
%\texttt{{\char'134}newdef} command,
%respectively, to create it.



