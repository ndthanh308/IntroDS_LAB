% WACV 2024 Paper Template
% based on the CVPR 2023 template (https://media.icml.cc/Conferences/CVPR2023/cvpr2023-author_kit-v1_1-1.zip) with 2-track changes from the WACV 2023 template (https://github.com/wacv-pcs/WACV-2023-Author-Kit)
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review,algorithms]{wacv}      % To produce the REVIEW version for the algorithms track
%\usepackage[review,applications]{wacv}      % To produce the REVIEW version for the applications track
%\usepackage{wacv}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{wacv} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{verbatim}
\usepackage{gensymb}



% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\wacvPaperID{*****} % *** Enter the WACV Paper ID here
\def\confName{WACV}
\def\confYear{2024}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Fake It Without Making It: Conditioned Face Generation for Accurate 3D Face Shape Estimation}

\author{Will Rowan$^1$, Patrik Huber$^1$, Nick Pears$^1$, Andrew Keeling$^2$ \\
\textsuperscript{1}University of York, \textsuperscript{2}University of Leeds}

%\author{Will Rowan Patrik Huber Nick Pears \\
%University of York\\
%York, United Kingdom\\
%{\tt\small \{wjr508,patrik.huber,nick.pears\}@york.ac.uk}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Andrew Keeling\\
%University of Leeds\\
%Leeds, United Kingdom\\
%{\tt\small a.j.keeling@leeds.ac.uk}
%}
\maketitle
% instead of leading with number of images, lead with number of faces - larger number is more impressive

%%%%%%%%% ABSTRACT
\begin{abstract}
   Accurate 3D face shape estimation is an enabling technology with applications in healthcare, security, and creative industries, yet current state-of-the-art methods either rely on self-supervised training with 2D image data or supervised training with very limited 3D data. To bridge this gap, we present a novel approach which uses a conditioned stable diffusion model for face image generation, leveraging the abundance of 2D facial information to inform 3D space. By conditioning stable diffusion on depth maps sampled from a 3D Morphable Model (3DMM) of the human face, we generate diverse and shape-consistent images, forming the basis of SynthFace. We introduce this large-scale synthesised dataset of 250K photorealistic images and corresponding 3DMM parameters. We further propose ControlFace, a deep neural network, trained on SynthFace, which achieves competitive performance on the NoW benchmark, without requiring 3D supervision or manual 3D asset creation. 
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}


Supervised approaches for 3D face shape estimation are limited by a lack of 3D data; 3D capture is costly and time consuming, making large-scale 3D datasets infeasible. This has led to the wide use of self-supervised approaches \cite{tewari2017mofa} \cite{tewari2018self} \cite{wu2019mvf} \cite{shang2020self} \cite{chen2020self}. However, these approaches have been shown to perform poorly in metric reconstruction \cite{sanyal2019learningNoW}.

% add reason for not using this

% also not designed for metrical reconstruction

Another approach is synthesising 3D face datasets using computer graphics. Wood et al. \cite{wood2021faketill} render a large scale dataset using a parametric face model and library of hand-crafted assets to train an accurate 2D landmark regressor. They are then able to fit a 3D face model to the predicted landmarks \cite{wood20223d}. This leads to robust performance but there remains a large domain gap; the images are not photorealistic, the process requires crafted assets, and it is computationally expensive. They propose to `fake it till you make it' with crafted `fake' data enabling them to `make it' with strong performance in the real world. We `fake' it without having to make any assets at all.

  % Figure environment removed


      
   %   The image shows an example of SynthFace along with its depth map.
      
  %    We generate photorealistic faces using conditioned stable diffusion. Depth maps rendered from the FLAME head model are used for conditional control; this enables us to produce shape-consistent images with known 3DMM parameters. Using this approach, we create SynthFace, the largest dataset of its kind. An example image from SynthFace and its corresponding depth map is shown, followed by further images from SynthFace.


 Zielonka et al. \cite{zielonka2022towardsMICA} annotate and unify existing 3D face datasets to enable supervised training of their MICA (MetrIC fAce) network. This is the current state-of-the-art in metric 3D face reconstruction from a single image on the NoW benchmark \cite{sanyal2019learningNoW}. 2D knowledge of the human face is incorporated through their use of ArcFace as a feature extractor. However, this approach does not take advantage of the wealth of 2D face data to increase the size of the dataset itself. We use 2D data in both dataset construction and within our network for accurate 3D face reconstruction from a large-scale 3D dataset.

 % Unlike the limited availability of labelled 3D data, there is a wealth of 2D facial data.

 We propose that image generation has reached a critical point where it can be used to bridge the gap between 2D and 3D, helping us solve 3D face reconstruction in a supervised manner. We use the generative capabilities of a 3D Morphable Model (3DMM) as conditioning for a stable diffusion image generation model. Hence, we combine known 3D shape information about the human face with 2D data on the appearance of human faces. This results in  a photorealistic 2D face image with known 3D shape, as shown in Figure \ref{fig:SynthFace_intro_image}.


The unification of existing 3D datasets through MICA has shown promising results in 3D face shape estimation, but it represents an upper bound on a dataset for supervised 3DMM regression unless more 3D data is collected. We overcome this limitation by devising a dataset generation pipeline which combines 2D and 3D generative models. This is achieved using ControlNet \cite{zhang2023addingcontrol}, which adds conditional control to Stable Diffusion \cite{rombach2022highstable}. We use ControlNet to condition stable diffusion 1.5 on depth maps of our generated 3D faces. In doing so, we produce a large-scale dataset of known 3DMM parameters and generated 2D face images.

%In doing so, we produce a large-scale dataset of known 3DMM parameters and generated 2D face images. We fine-tune MICA on this large-scale synthesized dataset and demonstrate state-of-the-art (SOTA) results, showcasing the effectiveness of our approach. 

The primary contributions of our work are twofold: First, we introduce SynthFace, the first large-scale synthesised dataset of 250K photorealistic faces, depth maps, and corresponding 3DMM parameters, which significantly expands the available data for training and evaluating 3D face shape estimation models. Second, we introduce ControlFace, a network trained on this dataset. With ControlFace, we demonstrate  competitive performance on the NoW benchmark, demonstrating that knowledge from 2D generative  image models can be integrated to improve 3D face shape estimation.

%which indicates that the understanding of faces in stable diffusion methods can be effectively applied from the 2D to the 3D domain. We demonstrate that ControlFace provides accurate, fast 3DMM parameter regression. 

In summary, our work presents a novel approach to bridge the gap between the limited availability of 3D data and the abundance of 2D data for face shape estimation. Our method is simple to implement, easily extensible, and computationally inexpensive. Future improvements to image generation models, conditioning  methods, and 3D face models can all be easily exploited using our method. Through introducing SynthFace and demonstrating the effectiveness of ControlFace, we reveal a promising new direction for improving 3D face shape estimation.


\section{Related Work}




3D face shape estimation from a single image represents a significant challenge in the field of computer vision. It  is an ill-posed problem due to the effects of perspective and scaling, which can result in similar 2D representations from different faces. To tackle this, 3D Morphable Models (3DMMs) have been extensively used since their introduction  by Blanz and Vetter \cite{blanz1999morphable} as they offer prior knowledge of human facial structure and help resolve ambiguities.

3DMMs provide a compact representation of the human face, allow additional constraints to be placed on reconstructions, and facilitate morphing between faces. Furthermore, their generative capabilities enable the sampling of realistic, geometrically consistent faces from within the model's space \cite{egger20203d}.

However, despite the widespread success of supervised learning across computer vision tasks, it has been severely limited in 3D face reconstruction due to a lack of training data. In this context, supervised learning involves the use of paired 2D-to-3D data, whether real or synthetic, which formally comprises a set of face images and their corresponding 3D model representations \cite{sanyal2019learningNoW}.

To navigate the scarcity of 3D supervision, many recent approaches have considered optimisation-based and self-supervised methods, but these have shown poor performance on metric benchmarks \cite{sanyal2019learningNoW}. Consequently, there is a need to explore supervised approaches to reconstruction and the collection of large-scale 3D training data to simplify the task.


In this work, we explore how the analytical and generative applications of 3DMMs can be combined to achieve accurate 3D face reconstruction. To achieve this, we examine current supervised methods for reconstruction, photorealistic face generation in both 2D and 3D, and how these approaches can be integrated to enable accurate 3D face reconstruction.




%3D face shape estimation from a single image is an ill-posed problem; different faces can appear as the same image due to the effects of perspective and scaling. 3D Morphable Models (3DMMs) have been widely used since their introduction by Blanz and Vetter \cite{blanz1999morphable} to represent prior knowledge of the human face. They allow for additional constraints to be applied to a reconstruction, a compact representation, and morphing between faces. These analytical uses are complemented by their generative applications: allowing us to sample realistic, geometrically consistent faces from within the space of the model.

%We will consider both their analytical uses for 

% supervised learning for face shape estimation



\subsection{Supervised Reconstruction}


One of the earliest notable approaches to supervised reconstruction using deep learning is by Tran et al. \cite{tuan2017regressing}. They create surrogate ground truth parameters using pooled multi-image 3DMM estimation. This process involves optimisation-based reconstructions for each image of an individual, with final shape and texture parameters being a weighted average of individual estimations. This is a clever observation: taking advantage of existing 2D multi-image data to improve 3D reconstruction from a single image. This dataset is then used for supervised training with a deep CNN. Despite its novelty in leveraging existing 2D multi-image data for improved 3D reconstruction, this approach is inherently limited by the initial reconstruction method used to generate the training data; at best, it can learn to be as good as this method.

Richardson et al. \cite{richardson20163d} generate face geometries directly from a 3DMM, rendering the face as an image under randomised lighting conditions. This results in a dataset of images with known 3DMM parameters; however these images are far from photorealistic. This points to a wider problem in synthesised approaches: a  domain gap between synthesised and real data that makes generalisation difficult and task performance poor \cite{kar2019meta}.

In contrast, Wood et al. \cite{wood2021faketill} render highly realistic 3D face models for landmark localisation, demonstrating that synthesised data can be used to solve real world problems in the wild. Wood et al. \cite{wood20223d} build upon this work to train a dense landmark regressor for 702 facial points. A morphable model is fitted to these dense landmarks, leading to state-of-the-art results in 3D face reconstruction. 

The success of this approach affirms the potential of network-based methods in advancing 3D shape estimation. However, this approach requires the manual creation of 3D assets with associated time, financial, and computational costs. Furthermore, the rendered images fall short of photorealism which limits their uses for direct 3DMM regression. 

Other approaches have considered using the 3D data we have rather than relying on synthesised datasets. Zielonka et al. \cite{zielonka2022towardsMICA} achieve state-of-the-art performance on the NoW benchmark through unifying existing 3D face datasets. This demonstrates the importance of supervision for reconstruction performance even when supervised with minimal available data. However, this approach already represents the upper bound for supervised learning using 3D data, unless further data is collected. In combining 8 existing datasets, they reach just 2315 individuals; this remains a small dataset for supervised learning techniques. Hence, a generative approach similar to Wood et al. \cite{wood2021faketill} is required for unconstrained dataset generation.

Other significant works in this field include exploring a hybrid loss function for weakly-supervised learning \cite{deng2019accurate}, generating surrogate ground truth data via multi-image 3DMM fitting using joint optimisation \cite{liu2018disentangling}, and learning an image-to-image translation network using known depth and feature maps generated from a 3DMM \cite{sela2017unrestricted}.

In our work, we build upon these existing supervised learning methods, combining 2D generative image models and 3D face models. This approach allows us to develop a dataset larger than that proposed by Wood et al. \cite{wood2021faketill} but without the extensive effort required to create 3D assets. We `fake it' without making it. By leveraging state-of-the-art generative image models, we generate photorealistic images comparable to those used to train MICA \cite{zielonka2022towardsMICA} while being able to scale dataset size to orders of magnitude above theirs. By taking this novel approach, we aim to significantly advance the field of 3D face reconstruction, introducing a new methodology to achieve 3D face reconstruction using supervised learning.


%Without collecting large 3D datasets, errors are introduced 


\subsection{Optimising Identity Vectors}

The loss function used for supervised 3D reconstruction requires careful consideration. Tran et al. \cite{tuan2017regressing} introduce an asymmetric Euclidean loss for minimising errors between predicted and actual parameter vectors; this decouples over-estimation errors from under-estimation errors. A standard Euclidean loss favours estimates close to 0 due to 3DMM parameters following a multivariate Gaussian distribution centred at zero by construction. They report more realistic face reconstructions using their asymmetric Euclidean loss. 

However, these losses minimise distance in the vector space of 3DMM parameters rather than minimising reconstruction error directly. Richardson et al. \cite{richardson20163d} directly calculate the Mean Squared Error (MSE) between generated 3D mesh representations. This ensures the loss takes into account how the parameter values affect the reconstructed geometry. Zielonka et al. \cite{zielonka2022towardsMICA} similarly follow a mesh-based loss but introduce a region dependent weight mask to weigh the facial region much higher than the rest of the head. We seek accurate 3D face shape estimation so we will optimise directly in 3D space using a mesh loss.


%DECA \cite{feng2021learningdeca} uses multiple images of the same person during training to learn to disentangle person-specific details of the face from expression-dependent features such as wrinkles. Sanyal et al. \cite{sanyal2019learningNoW} introduce a novel loss which constrains reconstructions of the same individual to be similar while being dissimilar to others. They further introduce the NoW \cite{sanyal2019learningNoW} benchmark for metrical and non-metrical evaluation of 3D face shape reconstruction.


\subsection{Realistic Parameterised Faces}

Automating the tedious manual work behind photorealistic face generation remains an open challenge and long term goal of 3D face representations \cite{egger20203d}. 3DMMs provide parametric control but generate unrealistic images; Generative Adversarial Networks (GANs) generate photorealistic images but lack explicit control \cite{ghosh2020gif}. Combining the parametric control of a 3DMM with the expressive power of generative models for faces has the potential to create large-scale datasets for supervised 3D face reconstruction.

%Since this stated goal in 2020, photo-realistic representations of 2D faces has been realised. Stable Diffusion \cite{rombach2022highstable}

Recent work has sought to harness the best of both worlds. StyleRig \cite{tewari2020stylerig} was the first approach to offer explicit control over a pretrained StyleGAN through a 3DMM, allowing for parametric editing of generated images. Building on this, Ghosh et al. \cite{ghosh2020gif} condition StyleGAN2 \cite{karras2020analyzing} on rendered FLAME \cite{li2017learningFLAME} geometry and photometric details to add parametric control to GAN-based face generation, facilitating full control over the image generation process. Sun et al. \cite{sun2022cgof++} propose a NeRF-based 3D face synthesis network which enforces similarity with a mesh generated by a 3DMM. However, in all these cases, the resulting images fall short of photorealism.

In the field of image synthesis, probabilistic diffusion models now represent the state-of-the-art, surpassing the capabilities of GANs \cite{dhariwal2021diffusion}. These models, which have developed significantly since their proposal \cite{sohl2015deep}, have been further improved by concurrent advances in transformer-based architectures \cite{vaswani2017attention} and text-image embedding spaces \cite{ramesh2021zero}. Publicly available text-image embedding spaces such as CLIP \cite{radford2021learning} have further diversified and enhanced these models \cite{ramesh2022hierarchical}.

Stable Diffusion is a powerful text-to-image diffusion model, synthesising high resolution images from textual prompts using a Latent Diffusion architecture \cite{rombach2022highstable}. ControlNet \cite{zhang2023addingcontrol}, a HyperNetwork that influences the weights of a larger paired network \cite{ha2016hypernetworks}, enables a variety of input modalities to be used to condition the output of Stable Diffusion. Implementations include depth maps, user sketches, and normal map conditioning networks, among others. We use their depth version of ControlNet that utilises MiDaS \cite{ranftl2020towards} to obtain 3M depth-image-caption pairs for training.

Unlike previous methods, ControlFace enables photorealistic image generation with strong shape control. For our use case, this enables us to create our own large-scale dataset of photorealistic images and known 3DMM parameters, with conditioning depth maps being generated from an existing model of 3D face shape.

%This results in photo-realistic faces that correspond to the constraints of the input. In faces, this means a photorealistic face that can correspond to known shape information provided through a depth map. We use their depth version of ControlNet that utilises MiDaS \cite{ranftl2020towards} to obtain 3,000,000 depth-image-caption pairs for training.

%We will use this to create our own large-scale dataset of photo-realistic images and known 3DMM parameters - with conditioning depth maps being generated from an existing model of 3D face shape.

  % Figure environment removed





\section{SynthFace: Fake It Without Making It}

We present SynthFace, a comprehensive training dataset for 3D face shape estimation, comprising 250K photorealistic faces with 10K distinct 3D facial shapes. These $(512,512)$ resolution images were rendered in 30 hours utilising 12 GTX 1080 GPUs, which demonstrates a significantly lower resource requirement compared to similar work \cite{wood2021faketill}. 

To create SynthFace, we first sample 10K faces from the FLAME head model. For each of these faces, we render five depth maps under different perspective projections; this is achieved by setting a constant 72.4\degree fov and varying the distance between camera and subject. This gives us 50K depth maps. Each depth map captures a different perceived shape due to the effects of perspective projection. This is designed to enable networks trained on SynthFace to disentangle identity and perspective effects from the underlying 3D shape. We then use ControlNet to condition stable diffusion 1.5 to produce photorealistic faces that adhere to the shape of these depth maps. This is performed five times for each depth map, resulting in 250K photorealistic images with corresponding 3DMM parameters which we used to render the conditioning depth maps. Figure \ref{fig:SynthFaceOverview} shows this pipeline.

In contrast to other 3D face datasets, we include a large number of different identities for the same face shape. An identity here is an individual recognisable person in 2D image space; a shape is the 3D mesh as parameterised by the 3DMM. We produce 25 images per distinct 3D shape, each capturing a different visual identity, but with the same underlying shape. Figure \ref{fig:SynthFace_shape_exploration} shows how different identities are included within SynthFace for the same shape. We believe we are the first to incorporate this approach into a dataset for 3D face shape estimation by design. Hence, SynthFace enables disentanglement of shape and identity through supervised learning.


  % Figure environment removed





\begin{comment}
    % Figure environment removed

\end{comment}




\subsection{3D Face Model}

We use the FLAME head model \cite{li2017learningFLAME} as a generative model for face shape. FLAME is a linear 3DMM with both identity and expression parameters. Linear blend skinning (LBS) and pose-determined corrective blendshapes are used to model the neck, jaw, and eyeballs around joints. This results in a head model containing N = 5023 vertices and K = 4 joints. 

FLAME takes coefficients for shape $\vec{\beta} \in \mathbb{R}^{|\beta|},$ pose $ \vec{\theta} \in \mathbb{R}^{|\theta|},$ and expression $ \vec{\psi} \in \mathbb{R}^{|\psi|}$. These are modelled as vertex displacements from a template mesh $\mathbf{\overline{T}}$. A skinning function $W$ rotates the  vertices of $T$ around joints $J \in \mathbb{R}^{3K}$. This is linearly smoothed by blendweights $\mathcal{W} \in \mathbb{R}^{K \times N}$. The model is formally defined as:

\begin{equation}
M(\vec{\beta}, \vec{\theta}, \vec{\psi}) = W(T_P(\vec{\beta}, \vec{\theta}, \vec{\psi}), \mathbf{J}(\vec{\beta}), \vec{\theta}, \mathcal{W}) \tag{1}
\label{FLAME_1}
\end{equation}

where

\begin{equation}
T_P(\vec{\beta}, \vec{\theta}, \vec{\psi}) = \mathbf{\overline{T}} + B_S(\vec{\beta}; S) + B_P(\vec{\theta}; P) + B_E(\vec{\psi}; E) \tag{2}
\label{FLAME_2}
\end{equation}

Due to different face shapes requiring different joint locations, joints are defined as a function of $\vec{\beta}$. Equation \ref{FLAME_2} includes shape, pose, and expression blendshapes. We sample shape coefficients and set pose and expression coefficients to 0. We use equation \ref{FLAME_1} to generate a complete 3D mesh of the head from these coefficients.


% Mathematical details of model

This approach enables us to create an arbitrary number of human head shapes, each compactly represented by a set of 3DMM parameters. Approaches which directly render textured versions of meshes to 2D suffer from low-fidelity, unrealistic outputs. Instead, we extract the depth map of each mesh to pass to ControlNet, generating realistic faces in the 2D domain. 

%We regress expression in addition to identity. This goes further than MICA \cite{zielonka2022towardsMICA} which only regresses identity parameters.

% pose parameters not really needed for regression. Being able to display nice overlay with test images is likely main reason. My model won't be textured anyway though so it seems to matter even less.



\subsection{Depth Map Generation}

%In building SynthFace, we use the first 100 FLAME shape parameters ($\beta$), 50 expression parameters ($\psi$), and 3 pose parameters ($\theta$) specifying global rotation. We also save 68 2D landmark positions and 68 3D landmark positions to enable further downstream tasks. We then render depth maps for each of these faces to later be passed to ControlNet.

In building SynthFace, we use all 300 FLAME shape parameters ($\vec{\beta}$). We later use ArcFace as a feature extractor \cite{deng2019arcface}. This network has been trained to extract discriminative facial features with invariance to rotation and expression of the face. ArcFace uses a novel additive angular margin loss to increase inter-class distance while reducing intra-class distance. Hence, we chose not to model these variations within our dataset. We believe this learning is better performed in the 2D domain with pre-trained networks specialised for these tasks.

%The parameters we choose for the FLAME model are sampled according to defined Gaussian distributions; this enables a wide variation in face shape within our dataset. All parameters are sampled from Gaussian distributions with a mean of 0. The standard deviation (s.d.) values used are as follows:  0.7 for $\beta$, 0.9 for $\vec{\psi}$, 0.1 for pitch, 0.3 for yaw, and 0.15 for roll. Additionally, we vary camera parameters, with a scale of 0.3, x-translation of 0.02, and y-translation of 0.02.

We sample identity parameters, $\vec{\beta}$, individually from a Gaussian distribution with mean 0 and s.d. 0.8. This enables a wide variation of face shape within our dataset. Expression coefficients, $\vec{\psi}$, are set to 0. We further set pose coefficients, $\vec{\theta}$, to 0. This results in a fixed frontal pose, which is suitable as input for identity descriptor networks such as ArcFace \cite{deng2019arcface}. We use a perspective camera with a 72.4\degree field of view. We vary the distance between the camera and subject from 100 to 400 world units using uniform sampling. This leads to perspective projection effects which model changes observed in real life, enabling a network to learn to deal with these effects.



  % Figure environment removed


\begin{comment}
      % Figure environment removed
\end{comment}




\subsection{Conditioned Face Generation}

We use the depth version of ControlNet to modulate the output of Stable Diffusion 1.5. It takes a depth map and textual prompt (positive and negative prompt) as input to produce an image. We produce 5 images per prompt. The inference procedure is set to run for 15 steps. The following prompt was used: Prompt: `studio portrait, profile picture, dslr’, Negative Prompt: `monochrome, illustration, painting, unrealistic, artefacts, low quality, plain background'.

A systematic method was undertaken to iteratively refine our prompt to generate realistic human faces. This process involved starting with a single text prompt, `studio portrait', and iteratively adding single phrases, both to positive and negative prompts, to build an improved prompt. The impact of these additional phrases was qualitatively evaluated in each case with only phrases that produced more visually lifelike outputs kept.



\subsection{Dataset Demographics}

We use FaceLib \cite{FaceLib} to estimate age and gender information from all generated faces within SynthFace. SynthFace is estimated to be 83.1\% male and 16.9\%; this binary is reductive but useful as a diagnostic. Figure \ref{fig:SynthFace_age_distribution} details the estimated distribution of ages in SynthFace. It is important to document the demographic data of a proposed dataset, as performance can be expected to be worse on those outside of the data distribution. Each generated face reflects data distributions within FLAME, Stable Diffusion, and how these are linked through ControlNet.



%In total, we obtain 300,000 photorealistic faces for 60,000 identities. In figure 2, we overlay landmarks from the original heads generated by FLAME over images within our SynthFace dataset. This demonstrates strong positional adherence to the conditioned image. 

%We further run an age and gender estimation network over the faces within SynthFace, reporting the results in figure 3.

%Stable Diffusion (SD) is a technique that facilitates the generation of highly realistic images based on textual prompts. ControlNet \cite{zhang2023addingcontrol} allows for enhanced control over the diffusion process by incorporating a lightweight network that learns to modulate the output.

%In our methodology, we utilise ControlNet+SD1.5 in conjunction with Midas depth estimation to condition the stable diffusion output on depth maps of our rendered heads. This approach results in photorealistic faces that maintain consistency with the shape of the original head. Consequently, we obtain full 3DMM parameters for this set of 300,000 photorealistic faces.

  % Figure environment removed


    % Figure environment removed



\section{ControlFace: Accurate 3D Face Shape Estimation}

We introduce ControlFace, a deep neural network trained on SynthFace. This network aims to disentangle shape from identity and perspective through supervised training on a large dataset which contains multiple identities for the same shape. It accepts an image as input and outputs a shape vector $x \in \mathbb{R}^{300}$ for the FLAME decoder. All architectures, training, and evaluation are implemented using PyTorch \cite{paszke2019pytorch}. Figure \ref{fig:ControlFaceOverview} shows the training process in full. Figure \ref{fig:ControlFaceInference} shows ControlFace at inference time.


\subsection{Training Data}

We use the entirety of SynthFace as our training data. SynthFace contains 250K images of 10K unique shape identities. A unique shape identity is defined as a unique set of 3DMM parameters. For each of these unique shape identities, we render five depth maps under different perspective projections and five images for each of these depth maps.

\subsection{Pre-processing}

First, faces are detected in each image using RetinaFace \cite{deng2020retinaface}. This provides a bounding box used to crop each image and warp it to a frontal pose. The images in SynthFace share a common frontal pose by design. However, this detection and warping step remains crucial. In-the-wild images have various poses which our approach must be able to handle. Next, we use the pretrained ArcFace network as a feature extractor for face description. ArcFace's 512-dimensional output embedding is used as input for a mapping network.

\subsection{Mapping Network}

 We use the mapping network presented by Zielonka et al. \cite{zielonka2022towardsMICA}. This network consists of three fully-connected layers followed by a linear output layer. We train this network to regress a shape vector $x \in \mathbb{R}^{300}$. This vector contains coefficients for all 300 identity bases in the FLAME head model.  Weights are randomly initialised.

 %The fully connected layers contain 512, 300, and 300 neurons respectively.

\subsection{Training Strategy}

We split SynthFace into training and validation sets, following a 80/20 split. We train our mapping network on the training set and select the best performing model based on the validation loss; we use early stopping with a patience of 20 to achieve this and run for 100 epochs. 

We use the AdamW optimizer for optimisation with  learning rate $\eta = 1 \times 10^{-5}$ and weight decay $\lambda = 2 \times 10^{-4}$. We use the same optimisation strategy and loss function as they present originally in \cite{zielonka2022towardsMICA}. We use their masked mesh loss which puts emphasis on inner facial regions in reconstruction. This loss is detailed here:

\begin{equation}
L = \sum_{(I, G)} |\kappa_{\text{mask}}(G_{\text{3DMM}}(M(\text{ArcFace}(I))) - G)|,
\end{equation}


This loss is calculated for all pairs of input images, \(I\), and known meshes, \(G\), within SynthFace. \((G_{\text{3DMM}}(M(\text{ArcFace}(I)))\) is the predicted mesh after the image is passed through ArcFace, the mapping network \(M\), and then the FLAME decoder \(G_{\text{3DMM}}\). \( \kappa_{\text{mask}} \) is a region-dependent weight mask with values: 150 for the face region, 1 for the back of the head, and 0.1 for the eyes and ears.


\section{Experiments and Evaluation}

We test our proposed method against the NoW benchmark \cite{sanyal2019learningNoW}. The NoW benchmark consists of 2054 images for 100 identities. It has become the standard benchmark for evaluating 3D face shape estimation from 2D images. These are split into validation and test sets consisting of 20 and 80 identities respectively. For each individual, the dataset includes images under different poses, occlusions, and expressions. We use the publicly available validation set of NoW for evaluation. First, a rigid alignment of the predicted meshes to the scans is performed using key facial landmarks. Then the scan-to-mesh distance between the predicted mesh and scan is performed for each vertex. The mean, median, and standard deviations of these distances is computed across all images in the given set. Table \ref{tab:table1} shows a comparison of ControlFace with current state-of-the-art methods.

\begin{table}[h!]
\centering
\begin{tabular}{lccc}
\hline
Method & Median& Mean& Std \\ 
\hline
Deep3D \cite{deng2019accurate} & 1.286 & 1.864 & 2.361 \\
DECA (detail) & 1.19 & 1.469 & 1.249 \\
DECA \cite{feng2021learningdeca} & 1.178 & 1.464 & 1.253 \\
AlbedoGAN (detail) & 0.95 & 1.173 & 0.987 \\
MICA \cite{zielonka2022towardsMICA} & 0.913 & 1.130 & 0.948 \\
AlbedoGAN \cite{rai2023towards} & 0.903 & 1.122 & 0.957 \\
ControlFace (ours) & 1.192 & 1.472 & 1.222 \\
\hline
\end{tabular}
\caption{Reconstruction error (mm) on the validation set of the NoW benchmark \cite{sanyal2019learningNoW} in non-metrical reconstruction. Comparison results are presented from \cite{rai2023towards}.}
\label{tab:table1}
\end{table}


Our results are competitive with the current state-of-the-art in 3D face shape estimation without requiring any ground truth data. We achieve this by introducing a novel method for large dataset generation for 3D face shape estimation. Our work with ControlFace demonstrates that supervised training on this dataset leads to accurate 3D face shape estimation.

Crucially, our work is easily extensible. A longer generation time can lead to a larger dataset and improvements in 2D and 3D generative model capabilities can directly feed into future work. We believe this will enable future versions of SynthFace to close the performance gap with methods such as MICA and AlbedoGAN. Datasets for specific use cases, be that large pose variations or expressions, can be created by updating parameters in our generation code. 

In unifying existing 3D face datasets, MICA reaches a natural limit in supervised learning on existing data sources. This is where the opportunity for synthesised approaches such as SynthFace lies. SynthFace can scale beyond this natural limit in real paired data.


\section{Limitations and Future Work}

The current iteration of SynthFace exclusively models variations in shape, leaving out expressive variations. Consequently, ControlFace solely focuses on shape prediction. It may be beneficial for future research to include varying expressions within the dataset or to devise a separate network to model these variations independently.

Our method employs ArcFace to generate a facial identity descriptor, which serves as the input to our mapping network. Importantly, this is an identity embedding and not a shape embedding. We make the assumption that the ArcFace-learned identity encompasses shape and that our mapping network can extract shape from this. Future research should explore retraining ArcFace or similar networks to more specifically extract shape information. Furthermore, the embedding network could be removed entirely, replacing it with a single network that learns to map images to 3DMM parameters in a supervised manner.

We utilise individual depth maps derived from a 3D face model to condition stable diffusion. Our knowledge of the full 3D geometry could be utilised further to improve the conditioned image. This could involve multi-image or even multi-modal conditioning to allow for even greater shape consistency between the 3D model and the generated 2D image.

%Furthermore, we need to contemplate the ethical implications arising from the construction of this dataset and its potential uses. As it stands, our conditioned stable diffusion method generates a dataset predominantly composed of younger men. Previous research has found commercial gender classification systems to perform inconsistently based on an individual’s skin tone, with dark-skinned females being the most frequently misclassified group \cite{buolamwini2018gender}. This was attributed to the systems being trained predominantly on light-skinned subjects.


We must also consider the ethical implications of our work. We show our conditioned stable diffusion approach to generate a dataset of predominantly younger men. This is a clear limitation. We also recognise that we use a deep-learning based age and gender estimator for this analysis which itself will be biased. Commercial gender classification systems were found to exhibit large variations in performance based on an individual’s skin tone; they misclassified dark-skinned females more than any other group \cite{buolamwini2018gender}. Buolamwini et al. found that these systems were trained on datasets including predominantly light-skinned training subjects. 

We agree with their proposals for intersectional error analysis using a benchmark balanced by gender and skin colour \cite{buolamwini2018gender}. This does not currently exist for 3D face shape estimation. Further work should consider the creation of such a benchmark to enable the accuracy of approaches for specific subgroups of individuals to be analysed.

%Full auditing of any deployed model is required to ensure it is not biased based on protected characteristics of individuals. More precisely, these results must be fully documented to achieve trust in the model and its use. 

Generative models like stable diffusion require extensive datasets for training, which typically rely on publicly available data. Consequently, there's a likelihood that individuals' data has been used without their explicit consent. This raises clear ethical and legal concerns, particularly for models deployed in the real world.

Accurate 3D face shape estimation finds  application in areas such as prosthesis design, yet it can also be utilised for malevolent purposes, including deepfake creation and mass surveillance. These potential misuses must be considered during model development and deployment and weighed against potential benefits.


\section{Conclusion}

In this work, we have addressed a key challenge in 3D face shape estimation by proposing a method for generating a large-scale dataset for supervised training. Our method combines existing 2D and 3D generative models to produce photorealistic images with corresponding 3DMM parameters. The resulting dataset, SynthFace, is the largest dataset of its kind and offers unique opportunities to disentangle shape from identity for accurate 3D face shape estimation.

Our results prove competitive with the existing state-of-the-art in the field of 3D face shape estimation. Notably, our technique does not rely on any ground truth data. Unlike previous methods, ours is easily extensible, computationally inexpensive, and produces photorealistic face images. We see this approach to solving 3D problems by using conditioned 2D diffusion models to hold great potential, particularly as existing 3D face datasets reach their limit for supervised learning.

%Additionally, our work demonstrates potential for future expansion. Further dataset enlargement can be achieved simply by extending the generation time. Additionally, advancements in 2D and 3D generative model capabilities can be directly integrated into the method, enhancing its effectiveness.

%We incorporate the effects of perspective projection into our dataset directly and generate multiple images of individuals of different identity that match the same shape. This allows ControlFace, which we train on SynthFace, to disentangle shape from identity.



We expect improvements in image generation, 3D face models, and conditioning networks to all improve the accuracy of this method for 3D face reconstruction; our work provides a clear path for continuous improvement. We believe this work will form the basis of a number of exciting developments in the future of this domain.



%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}

%$\beta$ : 0.7, \vec{\psi}: 0.9, pitch: 0.1, yaw: 0.3, and roll: 0.15. We also vary camera parameters as follows: scale: 0.3, x-translation: 0.02, y-translation: 0.02.

\begin{comment}
    
\begin{equation}
\vec{\beta} \sim \mathcal{N}(0, 0.7^2)
\end{equation}

\begin{equation}
\vec{\psi} \sim \mathcal{N}(0, 0.9^2)
\end{equation}


We also sample for the first three elements of the pose vector $\vec{\theta}$ individually. 




We further sample scale, x-translation, and y-translation for our camera from defined distributions. 


\begin{equation}
\begin{aligned}
\text{scale} &\sim \mathcal{N}(0, 0.3^2) \\
\text{x-translation} &\sim \mathcal{N}(0, 0.02^2) \\
\text{y-translation} &\sim \mathcal{N}(0, 0.02^2)
\end{aligned}
\end{equation}





The expression parameters $\boldsymbol{\theta}_{exp}$ are a 50-dimensional vector that defines the facial expressions. We sampled the $\boldsymbol{\theta}_{exp}$ from a normal distribution:


The pose parameters $\boldsymbol{\theta}_{pose}$ include three angles: pitch, yaw, and roll. We sampled these from normal distributions as follows:

\begin{equation}
\begin{aligned}
\text{pitch} &\sim \mathcal{N}(0, 0.1^2) \\
\text{yaw} &\sim \mathcal{N}(0, 0.3^2) \\
\text{roll} &\sim \mathcal{N}(0, 0.15^2)
\end{aligned}
\end{equation}

%The detail parameters $\boldsymbol{\theta}_{detail}$, a 128-dimensional vector, are used to add fine-grained details to the face. These parameters were sampled from a standard normal distribution:

%\begin{equation}
%\boldsymbol{\theta}_{detail} \sim \mathcal{N}(0, 1)
%\end{equation}

%Lastly, the camera parameters $\boldsymbol{\theta}_{cam}$ includes scale, and x, y translations. We sampled these from normal distributions as follows:


%Each set of parameters sampled in the above manner was fed into the FLAME model to generate a corresponding 3D face mesh, thus resulting in our comprehensive dataset of varied faces.

\end{comment}


\begin{comment}
    Methodology: we want to create a dataset of (images, 3DMM parameters) to enable accurate supervised 3DMM regression from a neural network. We must carefully consider what we want to represent in the dataset: both in diversity of faces we can regress to and features we can train our network to be invariant to.

\textbf{Diversity in faces}
\begin{itemize}
  \item Identity: Need diversity in identity to allow our network to regress to a wide range of possible people.
  \item Expression: Less important but the network must be able to deal with expressions such as smiles.
  \item Third item
\end{itemize}

The pose parameters, particularly the mouth and jaw parameters, will lead to wider variety in 

ArcFace should be invariant to expression, so this factor should largely be removed during regression. This means that using ArcFace while also trying to regress expression from its features makes little sense. 

If I vary the mouth and jaw, would I want to train a network to output these too? Or am I including these to learn invariance, just outputting shape at the end?

I'm going to initially train a model for primarly shape. I will vary the mouth to allow for my model to learn different expressions.

In MICA, they generate the 3D face and minimise the distance between regions. This is rather than directly optimising the 3DMM parameters to be regressed.


The conditional stable diffusion of ControlNet introduces noise such as wrinkles which aren't present in the depth maps. This adds to the photo realism of the images and is advantageous in training a network to be invariant to these features. Instead, the network should learn to extract the overall shape of the individual's face. 
\end{comment}



We estimate age and gender information for all images within our dataset.





Through using conditioned stable diffusion, we significantly reduce the costs associated with large-scale dataset construction and create the largest dataset of its kind. We see this approach to solving 3D problems by using 2D diffusion models to be at an early stage but with great potential.

    \subsection{Societal Impact}
        \begin{itemize}
            \item How the results may influence future research and applications
            \item This method removes the need for costly and time consuming digital asset creation and curation to create large-scale 3D  synthesised face datasets. We fake it without making it. 
            \item We see this approach to solving 3D problems by using 2D diffusion models to be at an early stage with great potential.
        \end{itemize}
    \subsection{Future Research Directions}
        \begin{itemize}
            \item Possible avenues for further investigation and improvement
            \item Adding further constraints to the generated image. multi-image diffusion? We expect improvements in both stable diffusion models and ways of controlling their output will lead to this method producing more competitive results
            \item We will make our dataset and dataset generation code available to allow for reproducibility by other researchers and new models and methods to be plugged into this framework.
        \end{itemize}



Accurate 3D face shape estimation is an enabling technology but one also with harms that need to be clearly identified and mitigated. We use two generative models, FLAME for 3D face shape and Stable Diffusion for photorealistic 2D images. Stable Diffusion. Resulting estimation techniques need to be audited for gender and racial biases before used in production. 


    We encourage future work to consider how these biases may be mitigated. 



        \begin{itemize}
            \item Potential drawbacks and challenges
            \item ethical use of AI considerations
            \item There is a fixed error in how well ControlNet conditions the generated image on the 3D mesh. Future work should consider other conditioning modalities and methods.
            \item Generating synthesised data that is representative of the real world is challenging. There are many factors such as prompting of stable diffusion.
            \item Expression-less meshes. We use ArcFace to extract a facial identity embedding. Crucially, ArcFace produces an identity embedding rather than a shape embedding. We assume the identity encompasses shape and our Mapping Network can extract identity from this. This adds another layer of potential errors. Future work may consider retraining ArcFace and/or regressing identity directly from the generated images. We chose ArcFace to reduce computational costs and take advantage of existing 2D data on the human face, with learned invariance to illumination, rotation etc.
            \item Our diffusion-generated images may be outside the distribution that ArcFace was trained on, leading to poorer identity descriptors.
        \end{itemize}



\begin{itemize}
    \item Run gender recognition and race recognition on the ArcFace input images from my dataset. Report the results - great for limitations and discussion
    \item Show prompt results and iteration in Appendix. Prompting  guide 
    \item Idea of writing a war stories post alongside a paper, detailing decisions you made - which paid off, which didn't. Where you spent time etc.
\end{itemize}






\begin{itemize}
    \item Show within dataset results, quantitively and qualitatively.
    \item Show mean face error in mm on this test set 
    \item I could also run another method or two on this test set and see how they do!
    \item Could compare asymmetric, mse, and mesh error optimisations
    \item Pick best and then evaluate this model against NoW and REALY.
    \item Introduce NoW benchmark
    \item Show evaluation results for both - comparison with the state of the art
    \item Explain why we believe this method achieves the results it does.

    
\end{itemize}


The REALY benchmark offers region-specific accuracy  for the nose, mouth, forehead and cheeks \cite{chai2022realy}.


\subsection{3D Face Shape Estimators as Filters}

We can view 3D face shape estimation as a series of filters applied to an input image to return a 3DMM parameter vector describing an individual's face shape. I will define these filters as operations, often neural networks, which aim to learn invariance to many factors. These are often 

Mingrui work shows that these filters can be leaky but this formulation is helpful in understanding the design choices made when proposing a learning architecture. It is particularly helpful when an architcture includes multiple sequential components, each seeking to filter out some aspects(s) of the input.

RetinaFace: invariance to translation within the frame

Add mathematical notation of some kind for this. Introduce it to the field. 



    \subsection{Experimental Setup}
        \begin{itemize}
            \item model architecture
            \item loss function
            \item Evaluation metrics
        \end{itemize}

\subsection{ContrastFace}

Initial experiments showed slow training in optimising the mesh loss and difficulty in extracting shape from an ArcFace face descriptor. In contrast to other 3D face datasets, we have a large number of different identities for the same shape. An identity here is an individual recognisable person in 2D image space; a shape is the 3D mesh as parameterised by the 3DMM. We believe we are the first to incorporate this approach into a dataset.


RingNet \cite{sanyal2019learningNoW} uses the idea that each person has a unique face shape which enables you to use multiple images of the same individual during training to constrain shape estimation. They formalise this in a novel shape consistency loss for an approach which does not require 3D supervision. They take multiple images of the same person and one image of a different person, enforcing shape similarity between images of the same person and dissimilarity between the shape of the other person. This can be visualised as a ring.

SynthFace allows for further disentanglement of shape from identity by having multiple identities for the same shape. This is combined with 3D supervision.

ContrastFace goes further than RingNet in computing similarity across all 10K identities within SynthFace. We optimise an embedding space that 

This loss should push input ArcFace embeddings to be closer to each other if the shape is the same and further away from different ArcFace embeddings.




%-------------------------------------------------------------------------


    \subsection{FLAME 2017 and Generated Faces}
        \begin{itemize}
            \item FLAME 2017 used for shape. Generate 50,000 faces using normal distributions.
            \item We vary identity, expression, pose, camera position (x, y)
            \item Render 512x512 depth maps of these faces
        \end{itemize}
    \subsection{Conditional Stable Diffusion using ControlNet}
        \begin{itemize}
            \item Then prompted conditional stable diffusion using ControlNet
        \end{itemize}
    \subsection{Visualisation and Demographic Information}
        \begin{itemize}
            \item Visualisation of depth maps, images
            \item Include landmark images of ControlNet output.
            \item Exploration of demographic info - run a detector on all the images and output summary statistics.
        \end{itemize}
\section{Proposed Method}
    \subsection{Overview of the Approach}
        \begin{itemize}
            \item Model architecture: building on existing face recognition networks: could start with MICA or similar - then compare to training e.g. resnet.
        \end{itemize}
    \subsection{Training Procedure}
        \begin{itemize}
            \item Loss function and optimisation
        \end{itemize}



The dataset for this research study was generated by leveraging the FLAME (Face of Large-scale Mesh Encoder) head model \cite{li2017learningFLAME}. The parameters for the FLAME model were randomly varied in accordance with predefined normal distributions. This enables a wide variation in head shape and consequently a large variety of heads that a model trained on this dataset can learn to regress to.

The FLAME model defines a face with several parameters: identity parameters, expression parameters, pose parameters, and camera parameters. We sampled these parameters from their own independently defined normal distributions and used the resulting samples to generate our dataset.



\subsection{Language}

All manuscripts must be in English.

\subsection{Dual submission}

Please refer to the author guidelines on the \confName\ \confYear\ web page for a
discussion of the policy on dual submissions.

\subsection{Paper length}
Papers, excluding the references section, must be no longer than eight pages in length.
The references section will not be included in the page count, and there is no limit on the length of the references section.
For example, a paper of eight pages with two pages of references would have a total length of 10 pages.
{\bf There will be no extra page charges for \confName\ \confYear.}

Overlength papers will simply not be reviewed.
This includes papers where the margins and formatting are deemed to have been significantly altered from those laid down by this style guide.
Note that this \LaTeX\ guide already sets figure captions and references in a smaller font.
The reason such papers will not be reviewed is that there is no provision for supervised revisions of manuscripts.
The reviewing process cannot determine the suitability of the paper for presentation in eight pages if it is reviewed in eleven.

%-------------------------------------------------------------------------
\subsection{The ruler}
The \LaTeX\ style defines a printed ruler which should be present in the version submitted for review.
The ruler is provided in order that reviewers may comment on particular lines in the paper without circumlocution.
If you are preparing a document using a non-\LaTeX\ document preparation system, please arrange for an equivalent ruler to appear on the final output pages.
The presence or absence of the ruler should not change the appearance of any other content on the page.
The camera-ready copy should not contain a ruler.
(\LaTeX\ users may use options of wacv.sty to switch between different versions.)

Reviewers:
note that the ruler measurements do not align well with lines in the paper --- this turns out to be very difficult to do well when the paper contains many figures and equations, and, when done, looks ugly.
Just use fractional references (\eg, this line is $087.5$), although in most cases one would expect that the approximate location will be adequate.


\subsection{Paper ID}
Make sure that the Paper ID from the submission system is visible in the version submitted for review (replacing the ``*****'' you see in this document).
If you are using the \LaTeX\ template, \textbf{make sure to update paper ID in the appropriate place in the tex file}.


\subsection{Mathematics}

Please number all of your sections and displayed equations as in these examples:
\begin{equation}
  E = m\cdot c^2
  \label{eq:important}
\end{equation}
and
\begin{equation}
  v = a\cdot t.
  \label{eq:also-important}
\end{equation}
It is important for readers to be able to refer to any particular equation.
Just because you did not refer to it in the text does not mean some future reader might not need to refer to it.
It is cumbersome to have to use circumlocutions like ``the equation second from the top of page 3 column 1''.
(Note that the ruler will not be present in the final copy, so is not an alternative to equation numbers).
All authors will benefit from reading Mermin's description of how to write mathematics:
\url{http://www.pamitc.org/documents/mermin.pdf}.

\subsection{Blind review}

Many authors misunderstand the concept of anonymizing for blind review.
Blind review does not mean that one must remove citations to one's own work---in fact it is often impossible to review a paper unless the previous citations are known and available.

Blind review means that you do not use the words ``my'' or ``our'' when citing previous work.
That is all.
(But see below for tech reports.)

Saying ``this builds on the work of Lucy Smith [1]'' does not say that you are Lucy Smith;
it says that you are building on her work.
If you are Smith and Jones, do not say ``as we show in [7]'', say ``as Smith and Jones show in [7]'' and at the end of the paper, include reference 7 as you would any other cited work.

An example of a bad paper just asking to be rejected:
\begin{quote}
\begin{center}
    An analysis of the frobnicatable foo filter.
\end{center}

   In this paper we present a performance analysis of our previous paper [1], and show it to be inferior to all previously known methods.
   Why the previous paper was accepted without this analysis is beyond me.

   [1] Removed for blind review
\end{quote}

\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Methods} & \textbf{REALY-F} & \textbf{REALY-S} \\
\hline
Deep3D & 1.657 & 1.691 \\
MCGNet & 1.774 & 1.787 \\
PRNet & 2.013 & 2.032 \\
SADRNet & 1.913 & 1.958 \\
DECA & 2.210 & 2.261 \\
3DDFA-V2 & 1.926 & 1.943 \\
Ours & \textbf{1} & \textbf{1} \\
\hline
\end{tabular}
\caption{Monocular reconstruction quantitative comparison. REALY-F and REALY-S correspond to frontal and side-profile reconstructions.}
\label{tab:comparison}
\end{table}


An example of an acceptable paper:
\begin{quote}
\begin{center}
     An analysis of the frobnicatable foo filter.
\end{center}

   In this paper we present a performance analysis of the  paper of Smith \etal [1], and show it to be inferior to all previously known methods.
   Why the previous paper was accepted without this analysis is beyond me.

   [1] Smith, L and Jones, C. ``The frobnicatable foo filter, a fundamental contribution to human knowledge''. Nature 381(12), 1-213.
\end{quote}

If you are making a submission to another conference at the same time, which covers similar or overlapping material, you may need to refer to that submission in order to explain the differences, just as you would if you had previously published related work.
In such cases, include the anonymized parallel submission~\cite{Authors14} as supplemental material and cite it as
\begin{quote}
[1] Authors. ``The frobnicatable foo filter'', F\&G 2014 Submission ID 324, Supplied as supplemental material {\tt fg324.pdf}.
\end{quote}

Finally, you may feel you need to tell the reader that more details can be found elsewhere, and refer them to a technical report.
For conference submissions, the paper must stand on its own, and not {\em require} the reviewer to go to a tech report for further details.
Thus, you may say in the body of the paper ``further details may be found in~\cite{Authors14b}''.
Then submit the tech report as supplemental material.
Again, you may not assume the reviewers will read this material.

Sometimes your paper is about a problem which you tested using a tool that is widely known to be restricted to a single institution.
For example, let's say it's 1969, you have solved a key problem on the Apollo lander, and you believe that the WACV70 audience would like to hear about your
solution.
The work is a development of your celebrated 1968 paper entitled ``Zero-g frobnication: How being the only people in the world with access to the Apollo lander source code makes us a wow at parties'', by Zeus \etal.

You can handle this paper like any other.
Do not write ``We show how to improve our previous work [Anonymous, 1968].
This time we tested the algorithm on a lunar lander [name of lander removed for blind review]''.
That would be silly, and would immediately identify the authors.
Instead write the following:
\begin{quotation}
\noindent
   We describe a system for zero-g frobnication.
   This system is new because it handles the following cases:
   A, B.  Previous systems [Zeus et al. 1968] did not  handle case B properly.
   Ours handles it by including a foo term in the bar integral.

   ...

   The proposed system was integrated with the Apollo lunar lander, and went all the way to the moon, don't you know.
   It displayed the following behaviours, which show how well we solved cases A and B: ...
\end{quotation}
As you can see, the above text follows standard scientific convention, reads better than the first version, and does not explicitly name you as the authors.
A reviewer might think it likely that the new paper was written by Zeus \etal, but cannot make any decision based on that guess.
He or she would have to be sure that no other authors could have been contracted to solve problem B.
\medskip

\noindent
FAQ\medskip\\
{\bf Q:} Are acknowledgements OK?\\
{\bf A:} No.  Leave them for the final copy.\medskip\\
{\bf Q:} How do I cite my results reported in open challenges?
{\bf A:} To conform with the double-blind review policy, you can report results of other challenge participants together with your results in your paper.
For your results, however, you should not identify yourself and should not mention your participation in the challenge.
Instead present your results referring to the method proposed in your paper and draw conclusions based on the experimental comparison to other results.\medskip\\

% Figure environment removed

\subsection{Miscellaneous}

\noindent
Compare the following:\\
\begin{tabular}{ll}
 \verb'$conf_a$' &  $conf_a$ \\
 \verb'$\mathit{conf}_a$' & $\mathit{conf}_a$
\end{tabular}\\
See The \TeX book, p165.

The space after \eg, meaning ``for example'', should not be a sentence-ending space.
So \eg is correct, {\em e.g.} is not.
The provided \verb'\eg' macro takes care of this.

When citing a multi-author paper, you may save space by using ``et alia'', shortened to ``\etal'' (not ``{\em et.\ al.}'' as ``{\em et}'' is a complete word).
If you use the \verb'\etal' macro provided, then you need not worry about double periods when used at the end of a sentence as in Alpher \etal.
However, use it only when there are three or more authors.
Thus, the following is correct:
   ``Frobnication has been trendy lately.
   It was introduced by Alpher~\cite{Alpher02}, and subsequently developed by
   Alpher and Fotheringham-Smythe~\cite{Alpher03}, and Alpher \etal~\cite{Alpher04}.''

This is incorrect: ``... subsequently developed by Alpher \etal~\cite{Alpher03} ...'' because reference~\cite{Alpher03} has just two authors.


% Update the wacv.cls to do the following automatically.
% For this citation style, keep multiple citations in numerical (not
% chronological) order, so prefer \cite{Alpher03,Alpher02,Authors14} to
% \cite{Alpher02,Alpher03,Authors14}.


% Figure environment removed

%------------------------------------------------------------------------
\section{Formatting your paper}
\label{sec:formatting}

All text must be in a two-column format.
The total allowable size of the text area is $6\frac78$ inches (17.46 cm) wide by $8\frac78$ inches (22.54 cm) high.
Columns are to be $3\frac14$ inches (8.25 cm) wide, with a $\frac{5}{16}$ inch (0.8 cm) space between them.
The main title (on the first page) should begin 1 inch (2.54 cm) from the top edge of the page.
The second and following pages should begin 1 inch (2.54 cm) from the top edge.
On all pages, the bottom margin should be $1\frac{1}{8}$ inches (2.86 cm) from the bottom edge of the page for $8.5 \times 11$-inch paper;
for A4 paper, approximately $1\frac{5}{8}$ inches (4.13 cm) from the bottom edge of the
page.

%-------------------------------------------------------------------------
\subsection{Margins and page numbering}

All printed material, including text, illustrations, and charts, must be kept
within a print area $6\frac{7}{8}$ inches (17.46 cm) wide by $8\frac{7}{8}$ inches (22.54 cm)
high.
%
Page numbers should be in the footer, centered and $\frac{3}{4}$ inches from the bottom of the page.
The review version should have page numbers, yet the final version submitted as camera ready should not show any page numbers.
The \LaTeX\ template takes care of this when used properly.



%-------------------------------------------------------------------------
\subsection{Type style and fonts}

Wherever Times is specified, Times Roman may also be used.
If neither is available on your word processor, please use the font closest in
appearance to Times to which you have access.

MAIN TITLE.
Center the title $1\frac{3}{8}$ inches (3.49 cm) from the top edge of the first page.
The title should be in Times 14-point, boldface type.
Capitalize the first letter of nouns, pronouns, verbs, adjectives, and adverbs;
do not capitalize articles, coordinate conjunctions, or prepositions (unless the title begins with such a word).
Leave two blank lines after the title.

AUTHOR NAME(s) and AFFILIATION(s) are to be centered beneath the title
and printed in Times 12-point, non-boldface type.
This information is to be followed by two blank lines.

The ABSTRACT and MAIN TEXT are to be in a two-column format.

MAIN TEXT.
Type main text in 10-point Times, single-spaced.
Do NOT use double-spacing.
All paragraphs should be indented 1 pica (approx.~$\frac{1}{6}$ inch or 0.422 cm).
Make sure your text is fully justified---that is, flush left and flush right.
Please do not place any additional blank lines between paragraphs.

Figure and table captions should be 9-point Roman type as in \cref{fig:onecol,fig:short}.
Short captions should be centred.

\noindent Callouts should be 9-point Helvetica, non-boldface type.
Initially capitalize only the first word of section titles and first-, second-, and third-order headings.

FIRST-ORDER HEADINGS.
(For example, {\large \bf 1. Introduction}) should be Times 12-point boldface, initially capitalized, flush left, with one blank line before, and one blank line after.

SECOND-ORDER HEADINGS.
(For example, { \bf 1.1. Database elements}) should be Times 11-point boldface, initially capitalized, flush left, with one blank line before, and one after.
If you require a third-order heading (we discourage it), use 10-point Times, boldface, initially capitalized, flush left, preceded by one blank line, followed by a period and your text on the same line.

%-------------------------------------------------------------------------
\subsection{Footnotes}

Please use footnotes\footnote{This is what a footnote looks like.
It often distracts the reader from the main flow of the argument.} sparingly.
Indeed, try to avoid footnotes altogether and include necessary peripheral observations in the text (within parentheses, if you prefer, as in this sentence).
If you wish to use a footnote, place it at the bottom of the column on the page on which it is referenced.
Use Times 8-point type, single-spaced.


%-------------------------------------------------------------------------
\subsection{Cross-references}

For the benefit of author(s) and readers, please use the
{\small\begin{verbatim}
  \cref{...}
\end{verbatim}}  command for cross-referencing to figures, tables, equations, or sections.
This will automatically insert the appropriate label alongside the cross-reference as in this example:
\begin{quotation}
  To see how our method outperforms previous work, please see \cref{fig:onecol} and \cref{tab:example}.
  It is also possible to refer to multiple targets as once, \eg~to \cref{fig:onecol,fig:short-a}.
  You may also return to \cref{sec:formatting} or look at \cref{eq:also-important}.
\end{quotation}
If you do not wish to abbreviate the label, for example at the beginning of the sentence, you can use the
{\small\begin{verbatim}
  \Cref{...}
\end{verbatim}}
command. Here is an example:
\begin{quotation}
  \Cref{fig:onecol} is also quite important.
\end{quotation}

%-------------------------------------------------------------------------
\subsection{References}

List and number all bibliographical references in 9-point Times, single-spaced, at the end of your paper.
When referenced in the text, enclose the citation number in square brackets, for
example~\cite{Authors14}.
Where appropriate, include page numbers and the name(s) of editors of referenced books.
When you cite multiple papers at once, please make sure that you cite them in numerical order like this \cite{Alpher02,Alpher03,Alpher05,Authors14b,Authors14}.
If you use the template as advised, this will be taken care of automatically.

\begin{table}
  \centering
  \begin{tabular}{@{}lc@{}}
    \toprule
    Method & Frobnability \\
    \midrule
    Theirs & Frumpy \\
    Yours & Frobbly \\
    Ours & Makes one's heart Frob\\
    \bottomrule
  \end{tabular}
  \caption{Results.   Ours is better.}
  \label{tab:example}
\end{table}

%-------------------------------------------------------------------------
\subsection{Illustrations, graphs, and photographs}

All graphics should be centered.
In \LaTeX, avoid using the \texttt{center} environment for this purpose, as this adds potentially unwanted whitespace.
Instead use
{\small\begin{verbatim}
  \centering
\end{verbatim}}
at the beginning of your figure.
Please ensure that any point you wish to make is resolvable in a printed copy of the paper.
Resize fonts in figures to match the font in the body text, and choose line widths that render effectively in print.
Readers (and reviewers), even of an electronic copy, may choose to print your paper in order to read it.
You cannot insist that they do otherwise, and therefore must not assume that they can zoom in to see tiny details on a graphic.

When placing figures in \LaTeX, it's almost always best to use \verb+\includegraphics+, and to specify the figure width as a multiple of the line width as in the example below
{\small\begin{verbatim}
   \usepackage{graphicx} ...
   \includegraphics[width=0.8\linewidth]
                   {myfile.pdf}
\end{verbatim}
}


%-------------------------------------------------------------------------
\subsection{Color}

Please refer to the author guidelines on the \confName\ \confYear\ web page for a discussion of the use of color in your document.

If you use color in your plots, please keep in mind that a significant subset of reviewers and readers may have a color vision deficiency; red-green blindness is the most frequent kind.
Hence avoid relying only on color as the discriminative feature in plots (such as red \vs green lines), but add a second discriminative feature to ease disambiguation.

%------------------------------------------------------------------------
\section{Final copy}

You must include your signed IEEE copyright release form when you submit your finished paper.
We MUST have this form before your paper can be published in the proceedings.

Please direct any questions to the production editor in charge of these proceedings at the IEEE Computer Society Press:
\url{https://www.computer.org/about/contact}.

