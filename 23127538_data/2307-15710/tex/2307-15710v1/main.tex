\documentclass[10pt,twocolumn,letterpaper, tikz]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{float}
\usepackage{neuralnetwork}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Semi-Supervised Object Detection in the Open World}

\author{
  Garvita Allabadi \quad 
  Ana Lucic \quad
  Peter Pao-Huang \quad 
  Yu-Xiong Wang \quad 
  Vikram Adve \\
  University of Illinois at Urbana-Champaign \quad \quad\\
  {\tt\small \{{garvita4,alucic2,ytp2,yxw,vadve}\}@illinois.edu}
  }

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
   Existing approaches for semi-supervised object detection assume a fixed set of classes present in training and unlabeled datasets, i.e., in-distribution (ID) data. The performance of these techniques significantly degrades when these techniques are deployed in the open-world, due to the fact that the unlabeled and test data may contain objects that were not seen during training, i.e., out-of-distribution (OOD) data. The two key questions that we explore in this paper are: can we detect these OOD samples and if so, can we learn from them? With these considerations in mind, we propose the Open World Semi-supervised Detection framework (OWSSD) that effectively detects OOD data along with a semi-supervised learning pipeline that learns from both ID and OOD data. We introduce an ensemble based OOD detector consisting of lightweight auto-encoder networks trained only on ID data. Through extensive evaluation, we demonstrate that our method performs competitively against state-of-the-art OOD detection algorithms and also significantly boosts the semi-supervised learning performance in open-world scenarios. 
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

% Figure environment removed

Object detection based on deep learning techniques is highly dependent on two assumptions that often do not hold in a real-world context: (1) availability of large-scale labeled datasets, and (2) a common, fixed set of object classes that appear in both training and unlabeled data, i.e., the so-called ``closed-world'' assumption.  


Semi-supervised learning (SSL) has emerged as a dominant paradigm to address the first limitation. SSL aims to leverage unlabeled data in order to boost model performance without incurring additional labeling costs.
 
Existing SSL approaches for object detection  (\eg,~\cite{sohn2020detection, berthelot2019mixmatch, jeong2019consistency}), however, do not address the second limitation -- they assume the classes in both the training and unlabeled data are sampled from the same distribution, \ie, in-distribution (ID) objects. In practice, the unlabeled data might contain objects that were unknown and unseen during training, which we call out-of-distribution (OOD) objects. 
 
Moreover, recent work \cite{dhamija2020overlooked} shows that existing SSL object detection techniques can actually perform worse \textit{even for ID objects} in the presence of OOD objects.
In particular, despite the systems being trained to reject everything other than the classes of interest, OOD objects end up being incorrectly detected as ID objects, often with high confidence. This issue is highly detrimental in real-world use for state-of-the-art SSL approaches based on pseudo-labeling that use high quality predictions of a model trained on labeled data as pseudo-labels for unlabeled data.

This motivates the development of open-world SSL techniques that correctly classify ID samples and simultaneously discover and group OOD samples, all with a limited amount of labeled ID samples (Figure \ref{fig:ood-problem}). 


In this work, we propose a new framework for \textbf{O}pen-\textbf{W}orld \textbf{S}emi-\textbf{S}upervised Object \textbf{D}etection (OWSSD). We first propose a simple yet effective OOD detector that leverages an ensemble of lightweight auto-encoder networks to distinguish between ID and OOD objects. The ensemble network consists of individual auto-encoders trained and thus {\em specialized} on each of the ID categories. {\em Intriguingly}, we have empirically found that the ensemble of class-specific auto-encoders consistently outperforms a single auto-encoder trained on all labeled ID data, as discussed in Section~\ref{sec:methodology:ood-detection}. The auto-encoders learn to replicate the salient features of their respective ID class data. When faced with an OOD category, these models  commonly result in a high reconstruction error. We threshold the reconstruction error from each of the auto-encoder models to determine if a presented sample belongs to the corresponding class of the trained model. 
To demonstrate the effectiveness of our method, we evaluate our proposed framework for a variety of open-world and semi-supervised settings. OWSSD performs competitively against the state-of-the-art OOD detection algorithms and also boosts the semi-
supervised learning performance in open-world scenarios. 

Our contributions are summarized as follows:

\begin{itemize}
    \item We develop the first OOD-aware semi-supervised object detection algorithm capable of detecting and labeling objects of both ID and OOD classes (Section~\ref{sec:methodology:ssl-pipeline}).
    Our framework aims to discover OOD data on the fly, while at the same time improving the learning of ID classes, all in a semi-supervised fashion. 
    \item We propose a simple yet effective approach for OOD detection using an ensemble of auto-encoder networks for low data regimes (Section~\ref{sec:methodology:ood-detection}).
    \item We propose experimental protocols for both open-world and semi-supervised learning and demonstrate consistent improvements compared to state-of-the-art methods for PASCAL VOC and MS-COCO datasets (Section~\ref{sec:expts}).
\end{itemize}

\section{Related Work}
\label{sec:related}

\noindent\textbf{Semi-Supervised Object Detection.} Semi-supervised learning (SSL)-based approaches have become popular to reduce the need for labeling \cite{sohn2020detection, berthelot2019mixmatch, jeong2019consistency}. The two dominant approaches used for semi-supervised object detection are pseudo-labeling and consistency regularization. Pseudo-labeling methods first train a teacher model on the labeled dataset in a supervised fashion. The teacher model is then used to generate predictions on the unlabeled dataset; these are then filtered based on a confidence threshold as “pseudo-labels”. These pseudo-labels are leveraged with the original labels from the labeled data to further train a child model. FlexMatch \cite{zhang2021flexmatch}, TSSDL \cite{shi2018transductive}, and others \cite{luo2018smooth, yan2019semi, iscen2019label} are popular methods that utilize pseudo-labeling for semi-supervised object detection to achieve state-of-the-art results comparable to fully supervised techniques. 

The consistency regularization approach to SSL aims to minimize a consistency loss between differently augmented versions of an image to extract salient features from unlabeled samples. Some recent work falls under this umbrella including \cite{jeong2019consistency, sajjadi2016regularization, laine2017temporal, tarvainen2017mean, liu2021certainty, luo2018smooth, iscen2019label}.

A limiting factor, however, in semi-supervised learning scenarios is the fixed number of classes in both training and unlabeled data, the assumption being that unlabeled data will mirror the trained data setting. Secondly, as highlighted by \cite{dhamija2020overlooked} and explained in the Introduction, pseudo-labeling in an open-world setting can lead to false positives, thus degrading overall performance.

In this work, we look at a more realistic scenario where novel classes can appear as part of the unlabeled and test data and propose a method for addressing false positive pseudo labels. We adopt the state-of-the-art methods to generate pseudo-labels for ID data and enforce consistency with a strongly augmented image (See section \ref{sec:methodology:ssl-pipeline} for more details).

\noindent\textbf{Open-World Learning.} Open-world learning attempts to capture the type of learning that is intrinsic to humans. In the context of computer vision, the initial dataset used to train a model is often not fully representative of the classes that the model will encounter in future data (i.e. the test set or unlabeled datasets). Open-world learning enables the detection of these novel objects by incrementally adding novel to the set of known classes. Previous work \cite{kim2022learning, kuo2015deepbox, o2015learning, wang2020leads, Maaz2022Multimodal} has studied different methods of object proposals for novel objects by attempting to remove the notion of class (all objects are regarded the same). However, this approach lacks the identification of classes, a prerequisite for object detection; OWSSD handles end-to-end novel object detection from region proposal to unknown object class identification that is then added back to the model for retraining. ORE ~\cite{joseph2021towards} is the first to propose an open-world object detector that identifies novel classes as ‘unknown’ and proceeds to learn the unknown classes once the labels become available. ~\cite{han2022expanding} aims to identify unknown objects by separating high/low-density regions in the latent space. Both these approaches work in a fully-supervised setting. OWSSD, on the other hand, works in the a more challenging semi-supervised setting where unlabeled data can be easily misclassified as one of the ID classes, hurting the overall performance of the detector.  


% Figure environment removed


\noindent\textbf{Open-World Semi-Supervised Classification.} Existing literature \cite{cao2022openworld, saito2021openmatch, rizve2022openldn} has also combined both semi-supervised and open-world learning, validating the motivation behind semi-supervised learning in the open-world context. These methods generally have a unique method of detecting novel classes—for example, OpenLDN \cite{rizve2022openldn} uses pairwise similarity while OpenMatch \cite{saito2021openmatch} uses one-vs-all classifiers for novel class discovery. Methods like ORCA \cite{cao2022openworld} integrate the novel classes into the semi-supervised learning pipeline via pseudo-labels. 

OWSSD follows a similar logic by detecting novel classes through our offline novelty-detection module followed by the process of merging pseudo labels belonging to ID classes with the generated pseudo-labels for OOD classes. The key distinction between earlier mentioned methods and OWSSD is that we tackle the more challenging problem of object detection compared to image classification and the solutions presented in the previously cited work are not directly applicable to the problem we are tackling in this work. Object detection requires not only classification but also localization of multiple objects in an image, in addition to handling background regions. 

In an open-world setting, object detection faces further challenges, primarily because of handling background information. In the fully-supervised setting, a background class is added to existing object categories to distinguish between actual objects (e.g. person, airplane) and background (e.g. water, building). In the open world setting, background could mean actual background or novel objects that were never seen by the model during training.

\noindent\textbf{Open-World Semi-Supervised Object Detection.}
\cite{liuopen} also addresses the open-world semi-supervised object detection setup. Their goal, however, is to improve the performance of ID classes in such a setting, the task they conduct by by filtering out OOD data. In contrast, our approach \textit{both} improves performance for ID classes \textit{as well as} OOD classes, i.e., our proposed framework solves a strictly stronger problem. 

\noindent\textbf{Generalized Zero-Shot Learning (GZSL).} GZSL has been a well-studied area in recent years \cite{chao2016empirical, pourpanah2022review, rahman2018unified, liu2018generalized, felix2018multi} with the ability to detect both ID and OOD classes in test time, contrasting traditional zero-shot learning which assumes only novel classes in the test data. While both GZSL and open-world semi-supervised learning provide the same functionality, their assumptions fundamentally differ. GZSL requires a large initial labeled dataset to predict novel classes based on their relationship to ID classes, which is not only difficult in regular settings but even more untenable in the semi-supervised setting where the labeled dataset is small. OWSSL makes no such assumption and is therefore more practical in realistic situations, especially in the context of semi-supervised learning. 

% Figure environment removed
 
\section{Methodology}
\label{sec:method}

\textbf{Problem Formulation.} For our task of open-world semi-supervised object detection, we are given a small labeled dataset \textit{$D_l = \{(x_1,y_1),(x_2,y_2), \ldots, (x_n,y_n)\}$} and a large unlabeled dataset \textit{\textit{$D_u = \{u_1, u_2, \ldots, u_m\}$}}, where $\{x_i\}$ and $\{u_i\}$ are input images, and $\{y_i\}$ are annotations. \textit{$D_l$} consists of a set of ID categories denoted by \textit{$C_\mathrm{id}$} (\ie, \textit{\textit{$C_{l}$} = \textit{$C_\mathrm{id}$}}), and \textit{$D_u$} consists of both \textit{$C_\mathrm{id}$} and a set of OOD category denoted by \textit{$C_\mathrm{ood}$} (\textit{\textit{i.e., $C_{u}$} = \textit{$C_\mathrm{id}$} $\cup$ \textit{$C_\mathrm{ood}$}}). Each annotation \textit{$y_i$} in \textit{$D_l$} includes a set of object bounding box coordinates and the corresponding class labels from \textit{$C_\mathrm{id}$}.
%associated with $\{x_i\}$

Our objective is to train a detection model on \textit{$D_l$} and \textit{$D_u$} jointly that is able to (1) localize and classify instances belonging to one of the ID categories \textit{$C_\mathrm{id}$}; and (2) localize instances {\em not} belonging to \textit{$C_\mathrm{id}$} while grouping them into a new set called OOD.  
The evaluation is performed on a held-out dataset \textit{$D_v$}that consists of both objects from \textit{$C_\mathrm{id}$} and \textit{$C_\mathrm{ood}$} categories (\ie, \textit{\textit{$C_{v}$} = \textit{$C_\mathrm{id}$} $\cup$ \textit{$C_\mathrm{ood}$}}).

\subsection{Overview}

This section describes our proposed framework, OWSSD, for the novel problem setting of semi-supervised object detection in the open world. Note that this differs from existing work in that OWSSD expands the scope of learning from both ID and OOD data, instead of filtering out OOD samples. 
Our proposed framework consists of three components: (1) a class-agnostic proposal generator for open-world object detection, (2) an ensemble based network for OOD detection, and (3) an OOD-aware semi-supervised learning pipeline to include ID and OOD data in the learning process. 

% Figure environment removed


\subsection{Open-World Object Proposals}
\label{sec:proposal}

The first integral step in OWSSD is to localize objects from both ID and OOD classes. In standard two-stage detectors like Faster R-CNN, object localization is achieved by using a Region Proposal Network (RPN) to generate object proposals, a selection of locations likely to contain objects. This network, however, is trained on a fixed set of ID classes and thus fails to generalize to novel OOD classes -- as shown in Fig.~\ref{fig-proposals}, a trained RPN performs poorly when novel objects appear in an image. 

To overcome this issue, we leverage an Object Localization Network (OLN) \cite{kim2022learning} that produces {\em class-agnostic} proposals that are not limited to ID classes and are thus beneficial to our open-world setting. OLN achieves this by estimating the objectness of a candidate region using geometric cues such as location and shape of an object, regardless of its category. The key insight here is that using a closed-set background vs. foreground classifier hinders generalization in an open-world setting where novel objects are categorized as background. By contrast, using localization cues helps identify novel objects better. Specifically, centerness \cite{tian2019fcos} and IoU scores \cite{jiang2018acquisition} are used for location and shape quality measures, respectively. 



We integrate the OLN into our semi-supervised learning framework and make important modifications as follows. We first train the OLN on the labeled dataset \textit{$D_l$} containing ID classes. We then operate this trained model on the unlabeled dataset \textit{$D_u$} with both ID and OOD classes, and filter the produced proposals based on two criteria -- (1) objectness scores and (2) individual areas of proposals. The objectness score predicted by the model specifies how likely is the proposal to contain an object. Finally, smaller boxes are filtered out to reduce the overall noise in the proposals. 
After filtering, the remaining boxes are used as candidate proposals for the OOD detection step as described in Sec.~\ref{sec:methodology:ood-detection}. 

Additionally, in Sec.~\ref{sec:expts} we conduct experiments where ``perfect'' proposals are generated by an oracle that has access to ground truth localization annotations. The goal is to evaluate different components of our framework when we have precise localization for both ID and OOD categories.



\subsection{Ensemble-Based OOD Detector}
\label{sec:methodology:ood-detection}

We now address one of the main parts of the open-world problem setup -- discovering and localizing OOD objects. We propose an ensemble model that is trained using a small set of labeled ID data \textit{only} and without OOD samples (Figure \ref{fig:ood-detector}).

Our ensemble model consists of multiple auto-encoder networks (each of which consists of two parameterized functions, the Encoder (E) parameterized by $\phi$ and the Decoder (D) parameterized by $\theta$), and is trained individually on the samples of a particular ID class. Each of these models aims to learn a lower-dimensional representation (encoding) for their corresponding category by capturing the most salient characteristics for that category. Given that a single image can contain multiple objects of different categories, we use the box-level features of each of the object present in the image. (See Algorithm \ref{alg:ood-train} for details on the training procedure.)

The auto-encoder learns by minimizing the given loss function that computes the dissimilarity $d$ of the reconstructed output from the original input. The dissimilarity function $d$ gives a measure of how well the output was reconstructed compared to the input called the reconstruction error.

\begin{equation}
    L(\theta ,\phi )= d(x,D_{\theta }(E_{\phi }(x)))
\end{equation}

When an auto-encoder is trained only with a certain category of ID data, the reconstruction error for the samples of the respective category would be low. At test time, when faced with an OOD sample, the model would not accurately reconstruct the input, thus resulting in a high reconstruction error. Since, we only use labeled ID samples for training, each auto-encoder indicates if a given sample belongs to its corresponding category based on the reconstruction error that needs to be less than a specified threshold $\mu$. This threshold $\mu$ is calibrated at the training stage, by observing the reconstruction error of ID categories. Given that every model is responsible only for its own category, all the other ID categories serve as pseudo-OOD data for determining $\mu$. This is done for all the auto-encoder models in the ensemble and final vote determines if a sample is OOD, in case none of the ID models ''claim'' the sample. This simple heuristic represents how the decision boundary between ID and OOD samples is established.  

Specifically, at test time, for an unlabeled image, a class-agnostic proposal generator determines the bounding box locations of potential objects in the image. Each of these bounding boxes passes through a box level feature extractor and then through the ensemble network. Each of the model casts a vote for the box-level feature vector based on the reconstruction error being under the specified threshold $\mu$ (Algorithm \ref{alg:ood-test}).

As seen above, training an auto-encoder network is inherently unsupervised as it does not require any information about the class label. By introducing an ensemble of such networks trained on individual categories, we introduce a specialization for each model. {\em Our key insight} is that this introduced specialization is a useful and necessary requirement for OOD detection with very limited labeled data. When an auto-encoder model is trained using the objects of a single category, the model learns the most salient and informative characteristics for that category. When a single auto-encoder model is trained for all ID categories with very few labeled samples, it overestimates the ID data present and is unable to distinguish between ID and OOD objects effectively. Training such an ensemble of models ensures that a model makes confident predictions when encountered with objects from the same ID category and results in a high reconstruction error when encountered with an object different from the one it was trained on. Section \ref{sec:expts} describes the variety of experiments we conduct which demonstrate that an ensemble model performs better compared to not only a common auto-encoder model trained on all ID classes, but also a variety of state-of-the-art OOD detection algorithms. 

\begin{algorithm}
\caption{OOD Detector Training Algorithm}
\label{alg:ood-train}
\KwData{$D_l$: Images and annotations consisting of class labels and bounding boxes for objects of \textit{$C_\mathrm{id}$} classes}
\KwResult{Ensemble of auto-encoder trained models}
\ForEach {c in \textit{$C_\mathrm{id}$}}{
\ForEach {bb in $D_l$ where class = $c$}
{
$X = extract\_features(bb)$\\
$z = E_{\phi }(x)$\\
$\hat{x}=D_{\theta }(z)$ \\
$ L_(x, \hat{x}) = \frac{1}{N} \sum_{i=1}^{N}||x_i-\hat{x_i}||^2$ 
}
}
\end{algorithm}

\begin{algorithm}
\caption{OOD Detector Testing Algorithm}
\label{alg:ood-test}
\KwData{$D_u$: Images and class-agnostic proposals for objects of \textit{$C_\mathrm{id}$} and \textit{$C_\mathrm{ood}$} classes and 
Ens: Ensemble of auto-encoder trained models}
\KwResult{List of OOD proposals}
\ForEach {bb in $D_u$ where class = $c$}
{
\ForEach {model in Ens}{
$X = extract\_features(bb)$\\
$z = E_{\phi }(x)$\\
$\hat{x}=D_{\theta }(z)$ \\
$ L_(x, \hat{x}) = \frac{1}{N} \sum_{i=1}^{N}||x_i-\hat{x_i}||^2$ \\
\uIf{$ L_(x, \hat{x}) < \mu$}{
     id = \textbf{True} \\
  }
}
\uIf{not id}{
    ood\_list.add(bb)
  }
}
\textbf{return} {ood\_list}
\end{algorithm}

\subsection{OOD-Aware Semi-Supervised Learning Framework}
\label{sec:methodology:ssl-pipeline}


To learn from both labeled and unlabeled datasets in an open world setting, we propose an {\em OOD-aware} semi-supervised learning framework. 

We adopt the Teacher-Student paradigm and use a two-stage training process (Fig.~\ref{fig-owssd-method}). In the first stage, we use weakly-augmented labeled data to train a \textbf{Teacher model}. This model then operates on the unlabeled images to generate bounding boxes and class predictions for the \textit{ID} classes. A subset of confident predictions that are higher than a pre-determined threshold are treated as pseudo-labels. 

In our open-world setting, such pseudo-labels could be highly noisy -- a novel OOD sample might be wrongly classified as an ID category. To overcome this difficulty, the predictions made by the Teacher model that have an Intersection over Union (IoU) score greater than a threshold $\tau$ with the OOD pseudo labels are filtered out from the final list. 

In the second stage, a \textbf{Student model} is trained using using both the labeled data and strongly augmented unlabeled data and the consistency regularization paradigm is used that enforces a model to output the same prediction for an unlabeled sample even after it has been augmented. Therefore, the final loss is computed for the labeled samples, unlabeled ID samples as well as unlabeled OOD samples.   

\begin{equation}
\label{eqn:eq5}
\mathcal{L} = \mathcal{L}_s + \lambda_1 \mathcal{L}_{u_{id}} + \lambda_2 \mathcal{L}_{u_{ood}}.
\end{equation}
The supervised loss $\mathcal{L}_s$ is computed on the model prediction compared to the ground truth labels. In contrast, the unsupervised loss $\mathcal{L}_u$ is computed on the model prediction on a strongly augmented image compared to the pseudo-label. $\lambda$ is a hyper-parameter that controls contribution of the unsupervised loss to the total loss. The details of data augmentation and hyper-parameter settings are described in Sec.~\ref{sec:expts}.


\section{Experiments}
\label{sec:expts}

\noindent\textbf{Datasets.} 
We evaluate OWSSD on two widely-used object detection datasets: PASCAL VOC ~\cite{c3} and MS-COCO ~\cite{c4}. The trainval sets of VOC07 and VOC12 consist of 5,011 and 11,540 images respectively, both from 20 object categories. The VOC07 test set consists of 4,952 images from 20 object categories. The MS-COCO \textit{train2017} set, on the other hand, contains more than 118k labeled images and 850k labeled object instances from 80 object categories. 

Similar to ~\cite{han2022expanding} and ~\cite{joseph2021towards}, we divide 80 COCO classes into four groups (20 classes per group) by their semantics: (1) VOC classes. (2) Outdoor, Accessories, Appliance, Truck. (3) Sports, Food. (4) Electronic, Indoor, Kitchen, Furniture. We construct VOC-COCO-${20, 40, 60}$ with $n=4952$ VOC testing images and {n, 2n, 3n} COCO images containing ${20, 40, 60}$ non-VOC classes with semantic shifts, respectively. Note that the COCO images contain objects from the corresponding COCO classes and also from VOC classes. 

To enable semi-supervised learning, we also construct two additional scenarios. (1) VOC-15 set that samples the first 15 categories from the VOC07 trainval set.  The VOC12 trainval set, with the original 20 categories, is used as the unlabeled dataset. The model performance is evaluated on the VOC07 test set. (2) VOC-COCO-20-Unlabeled set which uses an additional 5000 images containing 20 VOC and 20 COCO classes as an unlabeled set, along with the VOC-COCO-20 set for evaluation. 

\noindent\textbf{Evaluation Metrics.} We report two kinds of metrics: (1) Open-world metrics: To demonstrate the effectiveness of OOD detection, we report the precision, recall, $F_1$ score, and Absolute Open-Set Error (AOSE) ~\cite{miller2018dropout} (which counts the number of misclassified OOD objects) metrics for an OOD detector.
(2) Object-detection metrics for end-to-end evaluation: Following the standard evaluation metrics on COCO, we report the the mean average precision (AP) averaged over different IoU thresholds from 0.5 to 0.95, average precision at IoU=0.50 (AP$^{50}$), average precision at IoU=0.75 (AP$^{75}$), and average recall (AR) for all, ID, and OOD object categories on both benchmarks. 

\noindent\textbf{Implementation Details.} We use Faster R-CNN equipped with a Feature Pyramid Network and a ResNet-50 backbone as our object detector. The backbone is initialized with ImageNet pre-trained weights in both stages of training. The implementation is based on the torchvision library \cite{c1} from PyTorch \cite{c2}. 
For the open-world settings, we use $\mu=0.1$ and $\mu=0.04$ as the threshold for VOC-15 and VOC-COCO-${20, 40, 60}$ data splits. 
The auto-encoder model architecture in this paper utilizes one hidden layer in the encoder and decoder parts of the network. A feature pyramid network vector, 1,024 in length, is used as input to the model that is then compressed 4 times to 256 and eventually 64 (bottleneck layer).
For the semi-supervised settings, we use $\tau=0.9$ as the pseudo-labeling threshold, $\lambda_1=2$ as the unlabeled loss weight for ID objects and $\lambda_2=2$ as the unlabeled loss weight for OOD objects. Our data augmentation strategy consists of RandomHorizontalFlip for the first stage of training, and a combination of ScaleJitter, FixedSizeCrop, and RandomHorizontalFlip for the second stage of training.


\subsection{Main Results}
\label{sec:expts:main}

\noindent\textbf{Comparison with other OOD detection methods.} Given that OOD detection is one of the major components of the problem, we first compare the OOD detection capabilities of OWSSD with other OOD detection methods, namely Local Outlier Factor (LOF) ~\cite{10.1145/342009.335388}, Isolation Forest (IF) \cite{4781136}, One Class Support Vector Machine (OneSVM)~\cite{6790022} and Multiple Objective Generative Adversarial Active Learning (MO\textunderscore{}GAAL) ~\cite{8668550}. 

LOF ~\cite{10.1145/342009.335388} is a density based algorithm that calculates the local density deviation of a data point in relation to its neighbors. Isolation Forest ~\cite{4781136} uses an ensemble of binary decision trees that traverse and sub-sample feature paths to decide whether an image is an outlier. OneSVM ~\cite{6790022} establishes a hypersphere to separate the data that the model was fit on from the data that can be deemed novel. Finally, (MO\textunderscore{}GAAL) ~\cite{8668550} consists of k sub-generators and a discriminator; active learning is achieved through multiple generators that assist in learning the difference between fake and real data to help separate outliers from the training data. Table \ref{tab:ood-detection-other-methods} summarizes our findings. OWSSD's OOD detector outperforms results in a significantly higher recall ans $F_1$ score compared to existing OOD detection methods with the same amount of limited labeled data. The number of misclassified OOD objects represented by the AOSE score is also particularly low demonstrating its OOD detection capabilities. 

\begin{table}[h!]
\begin{center}
\begin{tabular}{l|cccc}
\toprule
Method & Precision & Recall & $F_1$ & AOSE  \\
\midrule
MO-GAAL~\cite{8668550}& 0.49 & 0.23 & 0.31 & 14135  \\
IF~\cite{4781136} & 0.55 & 0.26 & 0.35 & 13615  \\
OneSVM~\cite{6790022} & 0.58 & 0.28 & 0.38 & 13311 \\
LOF~\cite{breunig2000lof} & 0.60 & 0.28 & 0.38 & 13200 \\
\midrule
Ours & 0.54 & 0.69 & 0.61 & 5735 \\
\bottomrule
\end{tabular}
\end{center}
\caption{Comparison to other OOD detection methods}
\label{tab:ood-detection-other-methods}
\end{table}

\noindent\textbf{Comparison with supervised and semi-supervised object detection baselines.}
We now present the end-to-end performance of OWSSD  with a supervised and semi-supervised baseline for the VOC-15 dataset setting.
The semi-supervised baseline consists of state-of-the-art semi-supervised approaches namely pseudo-labeling (PL) and consistency regularization (CR). As seen in table \ref{tab:voc-15-all}, there is a clear progression in the mean average precision and recall, as more model capabilities are added. Semi-supervised learning improves the performance on the already-seen ID classes when unlabeled data is used. OWSSD with OLN generated proposals discovers OOD classes and results in comparable performance. Finally, OWSSD with Oracle generated proposals works best for OOD data as we eliminate the localization error. Although OWSSD with OLN generated proposals discovers OOD classes, the performance of the overall framework could be boosted if the performance of the class-agnostic proposal generator improves, which is evident with Oracle generated proposals.  

\begin{table*}[h]
\begin{center}
\begin{tabular}{l|ccc|ccc|ccc|ccc}
\toprule
& \multicolumn{3}{c|}{mAP} & \multicolumn{3}{c|}{AP$^{50}$} & \multicolumn{3}{c|}{AP$^{75}$} & \multicolumn{3}{c}{AR} \\
\midrule
Method & All & ID & OOD & All & ID & OOD & All & ID & OOD & All & ID & OOD \\
\midrule
FR-CNN ~\cite{ren2015faster} & 30.3 & 32.3 & - & 57.5 & 61.3 & - & 28.3 & 30.2 & - & 42.8 & 45.6 & - \\
FR-CNN + PL ~\cite{lee2013pseudo} + CR & 34.9 & 37.2 & - & 60.3 & 64.3 & - & 35.3 & 37.7 & - & 46.6 & 49.7 & - \\
\midrule
Ours w/ CP & 34.3 & 36.5 & 1.3 & 59.9 & 63.6 & 3.5 & 34.6 & 36.9 & 0.8 & 46.8 & 48.9 & 14.4 \\
Ours w/ OP & \textbf{37.5} & \textbf{38.6} & \textbf{19.8} & \textbf{64.3} & \textbf{66.1} & \textbf{36.8} & \textbf{38.3} & \textbf{39.5} & \textbf{19.8} & \textbf{50.2} & \textbf{50.6} & \textbf{45.1} \\
\bottomrule
\end{tabular}
\end{center}
\caption{Open-world semi-supervised object detection results on the VOC-15 data split. We compare our method with a supervised baseline (FR-CNN), a semi-supervised baseline with pseudo-Labeling (PL) and consistency regularization (CR). Our method has two variants: first with candidate proposals (CP) generated by OLN and second wit oracle generated proposals (OP).}
\label{tab:voc-15-all}
\end{table*}


\noindent\textbf{Comparison with open-world object detection methods.} 
We also compare OWSSD to different open-world object detection methods. Note that these methods are not semi-supervised and work with only labeled data. OWSSD, on the other hand, also includes unlabeled data in its training. However, since the OOD detection is performed with the same amount of labeled ID data, we compare the open-world metrics (AOSE) with OWSSD's performance. 
As seen in table \ref{tab:owod-benchmark}, OWSSD performs better compared to the baselines for OOD detection for two out of three cases with the same amount of labeled ID data. The conclusion we draw from this is that these methods could also leverage OWSSD's OOD-aware semi-supervised learning pipeline for improving the detection performance.    

\begin{table}[H]
\begin{center}
\begin{tabular}{l|ccc}
\toprule
Method & VC-20 & VC-40 & VC-60\\
\midrule
FR-CNN ~\cite{ren2015faster} & 15118 & 23391 & 25472 \\
FR-CNN* ~\cite{ren2015faster} & 11941 & 18257 & 19566 \\
PROSER ~\cite{zhou2021learning}& 13035 & 19831 & 21322 \\
ORE ~\cite{joseph2021towards} & 12811 & 19752 & 21415 \\
DS ~\cite{miller2018dropout} & 12868 & 19775 & 21921\\
OpenDet ~\cite{han2022expanding} & 11286 & 16800 & \textbf{18250} \\
\midrule
Ours & \textbf{5735} & \textbf{11057} & 22102 \\
\bottomrule
\end{tabular}
\end{center}
\caption{Open world object detection benchmark results}
\label{tab:owod-benchmark}
\end{table}

\subsection{Ablation Studies}
\label{sec:expts:ablation}

\noindent\textbf{OOD detector design choice.} OWSSD uses an ensemble of auto-encoder networks for OOD detection. Here we compare the performance of OWSSD's OOD detector to a common auto-encoder model trained for all ID categories with limited labeled data. Additionally, we examine the impact of combination of common and ensemble models and an ensemble model initialized with the weights of common model before fine tuning it for specific ID classes. Table \ref{design} lists the results of our study. The ensemble method results is a better recall and $F_1$ score for OOD detection compared to a common model trained on all labeled ID data, showing that it is able to distinguish OOD classes effectively.  

\begin{table}[H]
\begin{center}
\begin{tabular}{l|cccc}
\toprule
Method & Precision & Recall & F1 Score & AOSE  \\
\midrule
Common & 0.43 & 0.40 & 0.41 & 999 \\
Common+Ens & 0.43 & 0.40 & 0.41 & 999  \\
Ens Finetuned & 0.37 & 0.55 & 0.44 & 756 \\
\midrule
Ours & 0.39 & 0.57 &  0.46 & 715 \\
\bottomrule
\end{tabular}
\end{center}
\caption{OOD Detector design choices. Comparison between our proposed method with a common auto-encoder model trained for all ID categories and its combination with the ensemble model.}
\label{design}
\end{table}

\noindent\textbf{Impact of different error thresholds for OOD detector}

OWSSD's OOD detector consisting of an ensemble of auto-encoder networks categorizes categorizes data into one of the ID classes based on a threshold $\mu$. We study how the choice of value for $\mu$ impacts the OOD detectors performance. As seen in table \ref{threshold}, $\mu=0.1$ results in the best performance for the VOC-15 data split.    

\begin{table}[H]
\begin{center}
\begin{tabular}{l|cccc}
\toprule
Threshold & Precision & Recall & F1 Score & AOSE  \\
\midrule
0.05 & 0.26 & 0.84 & 0.4 & 265 \\
0.08 & 0.34 & 0.67 & 0.45 & 555  \\
0.1 & 0.39 & 0.57 &  0.46 & 715 \\
\bottomrule
\end{tabular}
\end{center}
\caption{Choice of value for reconstruction error threshold for ID vs OOD categorization in the OOD detectors for VOC-15 data split. }
\label{threshold}
\end{table}

\noindent\textbf{Impact of choices in our OOD-aware semi-supervised pipeline}
In our OOD-aware semi-supervised pipeline, if generated pseudo-labels for ID classes (i.e. high quality predictions made by the Teacher model), have an Intersection over Union (IoU) score greater than a threshold $\tau$ with the OOD pseudo labels, they are filtered out from the final list of pseudo labels. In this study, we examine the impact of not filtering out the overlapping pseudo-labels. As seen in table \ref{table-abl-ssl}, when OOD pseudo-labels and ID pseudo-labels are merged with filtering based on their IoU scores, the overall performance for both ID and OOD classes increases, as no false positives are introduced.


\begin{table}[H]
\begin{center}
\begin{tabular}{l|ccc}
\toprule
mAP & All & ID & OOD  \\
\midrule
Without adding OOD data & 60.3 & 64.3 & -  \\
Without filtering  & 63.4 & 65.8 & 27.1  \\
\midrule
Ours & 64.3 & 66.1 & 36.8 \\
\bottomrule
\end{tabular}
\end{center}
\caption{Impact of the OOD-awareness in the semi-supervised object detection pipeline. Our approach results in increased performance by reducing the false positive pseudo labels}
\label{table-abl-ssl}
\end{table}

\section{Conclusion}
We propose an integrated framework for addressing the open-world object detection in a semi-supervised setting that comprises three major components: 1) object proposal generation for unlabeled dataset that includes both ID and OOD instances, 2) an offline module through which OOD instances are identified using an ensemble auto-encoder approach and 3) the integration of pseudo labeled ID and OOD instances into training and the evaluation of the retrained model.  The two major contributions of this work are: 1) a proposed pipeline through which OOD instances can be recognized and added back to the model for retraining rather than discarded and 2) finding that the ensemble of auto-encoder networks that are trained on individual classes of objects can effectively recognize ID and OOD objects. 

\noindent\textbf{Limitations and Future Directions.}
One limitation of this approach is the interdependence of the three mentioned components: the accuracy of the object proposal generation (1) will have a direct impact on the OOD identification (2) which in turn will influence the evaluation metrics of the retrained model (3). While  the use of a class agnostic object proposal generator such as the OLN network is appropriate within the context of an open world setting, getting accurate delineation of objects in images is still a very challenging task that would benefit from improvements. 
We leave the examination of parameters that should guide decisions on the type and nature of proposal generators to use for future work. Similarly, we established that depending on the nature and complexity of the dataset, certain classes of objects stand out rather than blend in and thus become easier to identify as OOD. For example, we noticed that static objects that are more likely to be found in a closed rather than an open environment (for example tv monitor, clock, book) and that are characterized by a more persistent shape are easier to identify as new objects in comparison to, for example, domestic animals that can be found in both open and closed environments such as, dog, cat, sheep, and cow. These types of animals, even when introduced as OOD classes in the experimental set up, are commonly identified as ID classes although they were not previously seen. We leave the exploration of the most distinctive types of object detection features for future work.

\clearpage

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}