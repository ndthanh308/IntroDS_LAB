\section{Generalized Open-World Semi-Supervised Object Detection}
\label{sec:method}

% Figure environment removed

We propose an integrated framework for \textit{Generalized Open-World Semi-Supervised Object Detection} consisting of an Ensemble-Based OOD Explorer (Section \ref{sec:methodology:ood-explorer}) and an OOD aware semi-supervised pipeline (Section \ref{sec:methodology:ood-explorer}). The OOD Explorer plays a crucial role in identifying OOD objects, encompassing both localization and classification into ID or `'unknown' class. The OOD-aware semi-supervised learning pipeline ( Section \ref{sec:methodology:ssl-pipeline}) ensures that the model assimilates knowledge from both ID and OOD data without risking the forgetting of previously learned ID classes. This integrated approach aims to enhance the adaptability and robustness of object detection models in the open-world context. Figure \ref{fig-owssd-method} provides a visual summary of our proposed framework.

\subsection{Problem Formulation}\label{sec:OWSSD:Problem} 

For our task of generalized open-world semi-supervised object detection, we are given a small labeled dataset \textit{$D_l = \{(x_1,y_1),(x_2,y_2), \ldots, (x_n,y_n)\}$} and a large unlabeled dataset \textit{\textit{$D_u = \{u_1, u_2, \ldots, u_m\}$}}, where $\{x_i\}$ and $\{u_i\}$ are input images, and $\{y_i\}$ are annotations. \textit{$D_l$} consists of a set of ID categories denoted by \textit{$C_\mathrm{id}$} (\ie, \textit{\textit{$C_{l}$} = \textit{$C_\mathrm{id}$}}), and \textit{$D_u$} consists of both \textit{$C_\mathrm{id}$} and a set of OOD categories denoted by \textit{$C_\mathrm{ood}$} (\textit{\textit{i.e., $C_{u}$} = \textit{$C_\mathrm{id}$} $\cup$ \textit{$C_\mathrm{ood}$}}). Each annotation \textit{$y_i$} in \textit{$D_l$} includes a set of object bounding box coordinates and the corresponding class labels from \textit{$C_\mathrm{id}$}.

Our objective is to train a detection model on \textit{$D_l$} and \textit{$D_u$} jointly that is able to (1) localize and classify instances belonging to one of the ID categories \textit{$C_\mathrm{id}$}; and (2) identify instances belonging to \textit{$C_\mathrm{ood}$} and localizing them. Note that the model does not have access to any \textit{$C_\mathrm{ood}$} bounding box coordinates or labels. The evaluation is performed on a held-out dataset \textit{$D_v$} that consists of both objects from \textit{$C_\mathrm{id}$} and \textit{$C_\mathrm{ood}$} categories (\ie, \textit{\textit{$C_{v}$} = \textit{$C_\mathrm{id}$} $\cup$ \textit{$C_\mathrm{ood}$}}).

\subsection{Ensemble-Based OOD Explorer}
\label{sec:methodology:ood-explorer}

In the context of object detection, the integration of OOD data involves two primary tasks: OOD Classification, which distinguishes OOD data from ID data, and OOD Localization, which concurrently provides the precise location of OOD objects within the image. 

% Figure environment removed


\textbf{OOD Classification}. We propose an ensemble model that is trained using labeled ID data \textit{only} and without OOD samples. Our ensemble model comprises of multiple auto-encoder networks, each trained on samples from a specific ID class. Each model within the ensemble is dedicated to learning a lower-dimensional representation (encoding) for its respective category. 
An autoencoder network uses an encoder ($z=E_{\phi }(x), E_{\phi }:\mathcal {X}\rightarrow \mathcal {Z}$) to compress its input data and a decoder ($\hat{x}=D_{\theta }(z), D_{\theta }:\mathcal {Z}\rightarrow \mathcal {X}$)to reconstruct the original input. 

The autoencoder learns by minimizing the reconstruction error $R$, which gives a measure of how well the output was reconstructed compared to the input. $R$ is estimated with a dissimilarity function $d$, such that $d(x,\hat{x})$ measures how much $x$ differs from $\hat{x}$ ($R = d(x,D_{\theta }(E_{\phi }(x)))$)

When an auto-encoder model is trained only with a specific category of ID data, the reconstruction error, $R$, for samples of that category is low. At test time, when confronted with an OOD sample, the model is unable to accurately reconstruct the input, resulting in a higher reconstruction error. If this error value is less than a threshold value ($\mu$), then the sample belongs to the corresponding ID class which the model was trained on. This process is repeated for all auto-encoder models in the ensemble. Each model votes if the sample belongs to its class based on the observed $R$. A sample is deemed OOD if none of the models ``claim'' it. This simple heuristic illustrates how the decision boundary between ID and OOD samples is established (Figure \ref{fig:ensemble} shows this process).

This threshold is calibrated during training by observing the reconstruction error of ID categories. Since each model is responsible only for its own category, all other categories function as pseudo-OOD data for determining $\mu$. Finally, given that a single image may contain multiple objects of diverse categories, we leverage the box-level features of each object present in the image (Refer to the appendix for details on the training and test procedure).

As seen above, training an autoencoder network is inherently unsupervised as it does not require any information about the class label. By introducing an ensemble of such networks trained on individual categories, we introduce a specialization for each model. \emph{Our key insight is that this introduced specialization is a useful and necessary requirement for OOD detection with very limited labeled data.} When an autoencoder model is trained using the objects of a single category, the model learns the most salient and informative characteristics for that category. Training such an ensemble of models ensures that a model makes confident predictions when encountered with objects from the same ID category and results in a high reconstruction error when encountered with an object different from the one it was trained on. Section \ref{sec:expts} describes the variety of experiments we conduct which demonstrate that an ensemble model performs better compared to not only a common autoencoder model trained on all ID classes, but also a variety of state-of-the-art OOD detection algorithms. 

\textbf{OOD Localization}. In standard two-stage detectors like Faster R-CNN, object localization is achieved by using a Region Proposal Network (RPN) to generate object proposals, a selection of locations likely to contain objects. This network, however, is trained on a fixed set of ID classes and thus fails to generalize to novel OOD classes.

To address this challenge, we employ CutLER \citep{wang2023cut}, an unsupervised method that generates class-agnostic proposals. This characteristic proves advantageous in our open-world setting, as the proposals are not confined to in-distribution (ID) classes. We utilize the models pre-trained on unsupervised ImageNet, including both Cascade Mask RCNN and Mask RCNN. Subsequently, we filter the proposals based on a confidence score.

Additionally, we also analyzed two other localization methods for OOD localization, namely OLN \citep{kim2022learning} and MOST \citep{rambhatla2023most}. CutLER and MOST use the features extracted from a transformer \citep{vaswani2017attention} network trained with with a self-supervised manner proposed in DINO \citep{caron2021emerging} to localize multiple objects in an image. OLN \citep{kim2022learning}, on the other hand, estimates the objectness of a candidate region by relying on geometric cues such as location and shape of an object, regardless of its category. We show the performance of these methods when incorporated in our OOD Explorer and SSL pipelines in Section \ref{sec:expts:ablation}. 

\subsection{OOD-Aware Semi-Supervised Learning}
\label{sec:methodology:ssl-pipeline}

We now describe how the OOD samples identified by the OOD Explorer are introduced for training along with the ID samples, in our all {\em OOD-aware} semi-supervised learning framework. 

We adopt the Teacher-Student paradigm and use a two-stage training process (Fig.~\ref{fig-owssd-method}). In the first stage, we use the labeled data to train a \textbf{Teacher model}. This model then operates on weakly-augmented unlabeled images to generate bounding boxes and class predictions for the \textit{ID} classes. A subset of confident predictions that are higher than a pre-determined threshold are treated as pseudo-labels. 

In the open-world setting, such pseudo-labels could be highly noisy -- a novel OOD sample might be wrongly classified as an ID category. Therefore, we filter out the predictions made by the  Teacher model that have an Intersection over Union (IoU) score greater than a threshold $\tau$ with the OOD pseudo labels. 

In the second stage, a \textbf{Student model} is trained using both the labeled data and strongly augmented unlabeled data and the consistency regularization paradigm is used which enforces a model to output the same prediction for an unlabeled sample even after augmentation. The final loss is computed for the labeled samples and unlabeled samples (which includes both ID and OOD categories): $\mathcal{L} =~ \mathcal{L}_s + \lambda \mathcal{L}_{u} $

The supervised loss $\mathcal{L}_s$ is the standard loss for Faster RCNN network comprising of the the RPN classification loss (${L}_{cls}^{rpn}$), the RPN regression loss (${L}_{reg}^{rpn}$), the ROI classification loss (${L}_{cls}^{roi}$), and the ROI regression loss (${L}_{reg}^{roi}$). This loss is computed on the model prediction compared to the ground truth labels. In contrast, the unsupervised loss $\mathcal{L}_u$ is computed on the model prediction on a strongly augmented image compared to the pseudo-label, also consists of the four loss terms. $\lambda$ is a hyper-parameter that controls contribution of the unsupervised loss to the total loss. The details of data augmentation and hyper-parameter settings are described in Sec.~\ref{sec:expts}.

Note that since we are introducing a new OOD class to the model, instead of using the weights provided by the Teacher model, the Student model is retrained from scratch to alleviate the \textit{catastrophic forgetting} issue observed in incremental learning scenarios \citep{li2017learning}.
