\clearpage

\title{Appendix}

\maketitle

\section{Implementation Details}

We use Faster R-CNN equipped with a Feature Pyramid Network and a ResNet-50 backbone as our object detector. The backbone is initialized with ImageNet pre-trained weights in both stages of training. The implementation is based on the torchvision library \citep{c1} from PyTorch \citep{c2}.  For the open-world settings, we use $\mu=0.1$ as the threshold for VOC-15 data split. The autoencoder model architecture in this paper uses one hidden layer in the encoder and decoder parts of the network. A Feature Pyramid Network vector, 1,024 in length, is used as input to the model that is then compressed to 256 and eventually to 64 in the bottleneck layer. For the semi-supervised settings, we use $\tau=0.9$ as the pseudo-labeling threshold, $\lambda=2$ as the unlabeled loss weight. Our data augmentation strategy consists of RandomHorizontalFlip for the first stage of training, and a combination of ScaleJitter, FixedSizeCrop, and RandomHorizontalFlip for the second stage of training. For Cutler, we use the ImageNet pre-trained models using Cascade Mask R-CNN and Mask R-CNN as detectors. 


\section{OOD detection algorithms}

\begin{minipage}[t]{0.46\textwidth}
\begin{algorithm}[H]
\caption{OOD Detector Training Algorithm}
\KwData{$D_l$: Images and annotations consisting of class labels and bounding boxes for objects of \textit{$C_\mathrm{id}$} classes}
\KwResult{Ensemble of auto-encoder trained models}
\ForEach {c in \textit{$C_\mathrm{id}$}}{
\ForEach {bb in $D_l$ where class = $c$}
{
$x = extract\_features(bb)$\\
$z = E_{\phi }(x)$\\
$\hat{x}=D_{\theta }(z)$ \\
$ L(x, \hat{x}) = \frac{1}{N} \sum_{i=1}^{N}||x_i-\hat{x_i}||^2$ 
}
}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{0.46\textwidth}
\begin{algorithm}[H]
\caption{OOD Detector Testing Algorithm}
\KwData{$D_u$: Images and class-agnostic proposals for objects of \textit{$C_\mathrm{id}$} and \textit{$C_\mathrm{ood}$} classes;\\
Ens: Ensemble of autoencoder trained models}
\KwResult{List of OOD proposals}
\ForEach {bb in $D_u$ where class = $c$}
{
\ForEach {model in Ens}{
$X = extract\_features(bb)$\\
$z = E_{\phi }(x)$\\
$\hat{x}=D_{\theta }(z)$ \\
$ L_(x, \hat{x}) = \frac{1}{N} \sum_{i=1}^{N}||x_i-\hat{x_i}||^2$ \\
\uIf{$ L_(x, \hat{x}) < \mu$}{
     id = \textbf{True} \\
  }
}
\uIf{not id}{
    ood\_list.add(bb)
  }
}
\textbf{return} {ood\_list}
\end{algorithm}
\end{minipage}

\section{Additional ablation studies}
\subsection{Impact of choices in our OOD-aware semi-supervised pipeline.}
In our OOD-aware semi-supervised pipeline, if generated pseudo-labels for ID classes (i.e. high quality predictions made by the Teacher model) have an Intersection over Union (IoU) score greater than a threshold with the OOD pseudo labels, they are filtered out from the final list of pseudo labels. In this study, we examine the impact of not filtering out the overlapping pseudo-labels. As seen in table \ref{table-abl-ssl}, when OOD pseudo-labels and ID pseudo-labels are merged with filtering based on their IoU scores, the overall performance for both ID and OOD classes increases, as no false positives are introduced.

\begin{table}
\begin{center}
\caption{Impact of the OOD-awareness in the semi-supervised object detection pipeline. Our approach results in increased performance by reducing the false positive pseudo labels.}
\begin{tabular}{l|ccc}
\toprule
mAP & All & ID & OOD  \\
\midrule
Without adding OOD data & 60.3 & 64.3 & -  \\
With OOD data, without filtering  & 63.4 & 65.8 & 27.1  \\
\midrule
With OOD data + filtering & 64.3 & 66.1 & 36.8 \\
\bottomrule
\end{tabular}
\end{center}
\label{table-abl-ssl}
\end{table}


\subsection{Impact of number of layers in the autoencoders in the OOD detector}

We conducted an ablation study on the architecture of the OOD detector. We increased the number of layers from 3 to 5, including the input and output layers. In our Ensemble autoencoder network, we start with 1,024 features that are compressed to 256 and then to 64 in the bottleneck layer. In the architecture with 5 layers, we start with the 1,024 features that are compressed to 512, 256, 128, and 64 in the bottleneck layer. The decoder part is symmetrical to the encoder part. In both architectures, each layer is followed by the Rectified Linear Activation (ReLU) activation function, the number of epochs is held constant at 30, the mean squared error is used as the loss function, and the learning rate is set at 0.001. As Table \ref{tab:design} indicates, we see a slight improvement in the AUROC score with the network with more layers (0.74 versus 0.72). The $F_1$ score, however, has seen a slight decrease (0.43 compared to 0.46) and the FPR has seen an increase (0.23 compared to 0.14). We have not observed a clear benefit to increasing the number of layers in the Ensemble AE architecture. We leave more extensive testing of the autoencoder network hyperparameters, such as non-bottlenecked architecture\citep{Yong2022DoAN} that does not include a bottleneck layer for future work.  

\begin{table}[h]
\begin{center}
\caption{Study on the number of layers in the Ensemble AE architecture}
\begin{tabular}{l|ccc}
\toprule
Method & $F_1$ $\uparrow$ & FPR $\downarrow$ & AUROC $\uparrow$\\
\midrule
Ensemble with 5 layers & 0.43 & 0.23 & 0.74 \\
Ensemble with 3 layers & 0.46 & 0.14 & 0.72 \\
\bottomrule
\end{tabular}
\end{center}
\label{tab:design}
\end{table}


\section{Qualitative Results}

Figures \ref{fig-cascade}, \ref{fig-mrcnn}, \ref{fig-most}, \ref{fig-oln} shows the qualitative results of our approach. We show how different localization methods for OOD classes impact the quality of results. In all cases, the model was trained on 15 ID classes and tested on 20 classes consisting of 15 ID and 5 OOD classes. The OOD classes includes instances of 'potted plant', 'sofa', 'train' and 'tv monitor'. The aim is to detect instances of both ID categories with their class labels and OOD categories with an 'unknown' class label. As seen in the figures, CutLER \citep{wang2023cut} is able to localize OOD objects more effectively compared to OLN \citep{kim2022learning} and MOST \citep{rambhatla2023most}. Both OLN and MOST result in a high number of proposals for OOD objects, some of which are inaccurate or miss relevant objects completely. On the other hand, images with 'sheep' are sometimes misclassified as 'cow', we attribute this to the high degree of similarity between these two classes and similar background that the objects appear in.

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed
