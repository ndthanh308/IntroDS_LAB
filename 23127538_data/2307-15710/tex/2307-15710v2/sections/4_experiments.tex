\begin{table*}[t]
\begin{center}
\begin{tabular}{l|l|ccc|ccc}
\toprule
& & \multicolumn{3}{c|}{AP$^{50}$$\uparrow$} & \multicolumn{3}{c}{AR$\uparrow$} \\
\midrule
Method & Localization & All & ID & OOD & All & ID & OOD \\
\midrule
Labeled Only & RPN  & 57.5 & 61.3 & - & 42.8 & 45.6 & - \\
SSL & RPN & 60.3 & 64.3 & -  & 46.6 & 49.7 & - \\
SSL + OOD \scriptsize{Filtering} & RPN & 60.4 & 64.4 & - & 46.8 & 49.9 & - \\
\midrule
SSL + OOD \scriptsize{Expansion} & RPN + CutLER & 61.0 & 64.4 & 10.2 & 48.2 & 50.0 & 21.2 \\
SSL + OOD \scriptsize{Expansion} & RPN + CutLER* & 61.2 & 64.7 & 8.8 & 48.4 & 50.1 & 21.8 \\
\bottomrule
\end{tabular}
\end{center}
\caption{Generalized Open-world semi-supervised object detection results on the VOC-15 data split. We examine different open-world settings. The first three settings - Labeled only (Supervised), Semi-supervised (SSL) and OOD Filtering are focused on improving the performance for the original ID classes in the presence of OOD objects. The OOD Expansion setting measures model performance when (unlabeled) OOD objects are also included. 
}
\label{tab:voc-15-all}
\end{table*}

\section{Experiments}
\label{sec:expts}

\subsection{Experimental Setting and Datasets}

\noindent\textbf{Datasets.} 
We evaluate OWSSD on the PASCAL VOC ~\citep{c3} object detection datasets. The trainval sets of VOC07 and VOC12 consist of 5,011 and 11,540 images respectively, both from 20 object categories. The VOC07 test set consists of 4,952 images from 20 object categories.  We evaluate the open world semi supervised object detection by creating a VOC-15 set that samples the first 15 categories from the VOC07 trainval set as the ID classes.  The VOC12 trainval set, with the original 20 categories (15 ID classes and 5 OOD classes), is used as the unlabeled dataset. The model performance is evaluated on the VOC07 test set. 

\noindent\textbf{Evaluation Metrics.} We report two kinds of metrics: (1) Object-detection metrics for end-to-end evaluation: We report the the average precision at IoU=0.50 (AP$^{50}$) and average recall (AR) for all, ID, and OOD object categories. (2) Open-world metrics: To demonstrate the effectiveness of OOD detection, we report the $F_1$ score, False Positive Rate (FPR) and Area Under the ROC curve (AUROC) metrics.  

\subsection{Main Results}
\label{sec:expts:main}

\begin{table}
\begin{minipage}[t]{0.45\textwidth}
\begin{center}
\caption{Evaluation of Unsupervised Localization Methods for the OOD Explorer.}
\begin{tabular}{l|cccc}
\toprule
Method & AP$^{50}$$\uparrow$ & AR$\uparrow$\\
\midrule
 OLN & 3.5 & 14.4  \\
 MOST & 5.2  & 16.9 \\ 
 Oracle & 36.8 & 45.1 \\ 
\midrule
CutLER & 10.2 & 21.2 \\
CutLER* & 8.8 & 21.8 \\
\bottomrule
\end{tabular}
\end{center}
\label{tab:localization}
\hfill
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[t]{0.45\textwidth}
\caption{Evaluation of OOD detection methods for object detection.}
\begin{tabular}{l|cccc}
\toprule
Method & $F_1$ $\uparrow$ & FPR $\downarrow$ & AUROC $\uparrow$\\
\midrule
LOF & 0.33 & 0.44 & 0.68 \\
IF & 0.37 & 0.32 & 0.70   \\
OneSVM & 0.33 & 0.49 & 0.68 \\
KNN& 0.35 & 0.44 & 0.70  \\
\midrule
Ours & \textbf{0.46} & \textbf{0.14} & \textbf{0.72} \\
\bottomrule
\end{tabular}
\label{tab:ood-detection-other-methods}
\end{minipage}
\end{table}

\noindent\textbf{Experiments with OOD Expansion:}
The aim for these experiments is to evaluate the efficacy of our proposed technique for open world semi supervised object detection. We examine different open-world settings. The Labeled only, Semi-supervised (SSL) and OOD Filtering settings are focused on improving the performance for the original ID classes in the presence of OOD objects. The OOD Expansion setting measures model performance when (unlabeled) OOD objects are also included in the training process. Our results are presented in Table \ref{tab:voc-15-all}.

We find that introducing OOD filtering through our ensemble OOD detector, combined with SSL, enhances the performance of SSL across ID and All classes, compared to the Label Only and SSL baselines. 
Additionally, when adapting SSL for OOD Expansion using our proposed \textit{OOD Explorer}, we observe substantial improvements, including an increase of +5.3 mean Average Precision (mAP) for all classes and +5.4 mAP for ID classes, compared with using labeled images only, and +0.7 and +0.5 compared with the baseline SSL. Furthermore, our method, in conjunction with CutLER for OOD Localization, demonstrates increased capabilities in detecting OOD objects, achieving an AP$^{50}$ of 10.2 and an Average Recall (AR) of 21.2.
Cutler ~\citep{wang2023cut} uses the MaskRCNN, and using a variant of CutLER with Cascade Mask RCNN (which we label CutLER*) further improves performance slightly for ID objects but hurts slightly for OOD objects.

\noindent\textbf{Comparison of OOD Detection Methods:}
%
We compare the OOD detection capabilities of our proposed ensemble method with other OOD detection methods, namely Local Outlier Factor (LOF) ~\citep{10.1145/342009.335388}, Isolation Forest (IF) \citep{4781136}, One Class Support Vector Machine (OneSVM)~\citep{10.1162/089976601750264965} and K-Nearest Neighbors (KNN). Table \ref{tab:ood-detection-other-methods} summarizes our findings. OWSSD's OOD detector features a significantly higher $F_1$ score compared to existing OOD detection methods with the same amount of limited labeled data. Also, the OWSSD OOD detector results in the highest AUROC score that summarizes the ability of the model to distinguish between the seen and unseen classes. The False Positive Rate reflects the rate of false positive OOD instances: the lower the rate the better performance of the model. The FPR rate for OWSSD OOD is the lowest, which explains why our technique is able to outperform the other methods.

\subsection{Ablation Studies}
\label{sec:expts:ablation}

\noindent\textbf{Evaluation of OOD Localization methods}
%
We evaluated 3 different localization methods for our \textit{OOD Explorer}, namely CutLER \citep{wang2023cut}, MOST \citep{rambhatla2023most} and OLN \citep{kim2022learning}. We also compared the performance against an Oracle that generates perfect proposals for OOD classes, to establish an upper bound. Based on the proposals provided by each of these methods, we use our ensemble based OOD detector to identify the OOD proposals. The performance for OOD classes is reported after training the student model. 

As seen in Table \ref{tab:localization}, CutLER outperforms both OLN and MOST for OOD object detection. Nevertheless, considering the inherent challenges in localizing OOD data without any labels in the context of open-world learning, there remains potential for enhancement, as evidenced by the much better performance of the oracle proposal generator. These results show that accurate localization is a key bottleneck we must address for further performance improvements on open-world semi-supervised object detection.


\noindent\textbf{Impact of different error thresholds for OOD detector:}
%
OWSSD's OOD detectors consisting of an ensemble of autoencoder networks categorizes data into ID or OOD classes based on a threshold value $\mu$. We study how the threshold value impacts the OOD detector's performance. The lower reconstruction error indicates that one of the autoencoder models has been able to reconstruct the object and that object class has been claimed. If, however, none of the classes can effectively reconstruct the object featured in the bounding box, the voting mechanism of separate encoders would result in a higher reconstruction error. We tested a series of threshold ($\mu$) values and show results for the following: 0.05, 0.1, 0.2. As Table 4 indicates, the best balance between precision and recall, as reflected through the $F_1$ score, is achieved with the 0.1 threshold. 

\begin{table}
\begin{minipage}[t]{0.45\textwidth}
\begin{center}
\caption{Choice of reconstruction error threshold for ID vs OOD categorization}
\begin{tabular}{l|cccc}
\toprule
Threshold & $F_1$ $\uparrow$ & FPR $\downarrow$ & AUROC $\uparrow$\\
\midrule
 0.05 & 0.40  & 0.35 & 0.74  \\
 0.2 & 0.29 & 0.003  & 0.59 \\ 
\midrule
Ours 0.1 & 0.46 & 0.14 & 0.72\\
\bottomrule
\end{tabular}
\end{center}
\label{tab:threshold}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[t]{0.45\textwidth}
\begin{center}
\caption{Impact of using an Ensemble compared to a common AE model.}
\begin{tabular}{l|ccc}
\toprule
Method & $F_1$ $\uparrow$ & FPR $\downarrow$ & AUROC $\uparrow$\\
\midrule
Common AE & 0.42 & 0.09 & 0.67\\
Ours & 0.46 & 0.14 & 0.72 \\
\bottomrule
\end{tabular}
\end{center}
\label{design}
\end{minipage}
\end{table}

\noindent\textbf{Evaluating ensemble approach in the OOD detector.} OWSSD uses an ensemble of autoencoder networks for OOD detection. We compared the performance of OWSSD's OOD detector to a common autoencoder model trained for all ID categories with limited labeled data. Table \ref{design} shows the results. The ensemble method results in a better recall and $F_1$ score for OOD detection compared to a common model trained on all labeled ID data, showing that it is able to distinguish OOD classes effectively. 
As noted previously, the ensemble approach enables each autoencoder to have high accuracy in distinguishing the specific ID class it was trained on, which leads to better discrimination of ID vs OOD objects. (The small increase in False Positive Rate did not affect the F1 and AUROC score advantage that the Ensemble AE method has over the Single AE.)

