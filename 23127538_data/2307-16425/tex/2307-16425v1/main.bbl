\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\def\UrlFont{\rmfamily}
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand\BIBentrySTDinterwordspacing{\spaceskip=0pt\relax}
\providecommand\BIBentryALTinterwordstretchfactor{4}
\providecommand\BIBentryALTinterwordspacing{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand\BIBforeignlanguage[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}

\bibitem{bock2016joint}
S.~B{\"o}ck, F.~Krebs, and G.~Widmer, ``Joint beat and downbeat tracking with
  recurrent neural networks.'' in \emph{International Society of Music
  Information Retrieval Conference (ISMIR)}, 2016, pp. 255--261.

\bibitem{bock2019multi}
S.~Böck, M.~E. Davies, and P.~Knees, ``{Multi-Task Learning of Tempo and Beat:
  Learning One to Improve the Other.}'' in \emph{International Society of Music
  Information Retrieval Conference (ISMIR)}, 2019, pp. 486--493.

\bibitem{bock2020deconstruct}
S.~Böck and M.~E. Davies, ``{Deconstruct, Analyse, Reconstruct: How to improve
  Tempo, Beat, and Downbeat Estimation.}'' in \emph{International Society of
  Music Information Retrieval Conference (ISMIR)}, 2020, pp. 574--582.

\bibitem{chen2022toward}
T.-P. Chen and L.~Su, ``Toward postprocessing-free neural networks for joint
  beat and downbeat estimation,'' in \emph{International Society of Music
  Information Retrieval Conference (ISMIR)}, 2022.

\bibitem{zhao2022beat}
J.~Zhao, G.~Xia, and Y.~Wang, ``{Beat Transformer: Demixed Beat and Downbeat
  Tracking with Dilated Self-Attention.}'' in \emph{International Society of
  Music Information Retrieval Conference (ISMIR)}, 2022.

\bibitem{mccallum2019unsupervised}
M.~C. McCallum, ``Unsupervised learning of deep features for music
  segmentation,'' in \emph{International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, 2019, pp. 346--350.

\bibitem{salamon2021deep}
J.~Salamon, O.~Nieto, and N.~J. Bryan, ``Deep embeddings and section fusion
  improve music segmentation.'' in \emph{International Society of Music
  Information Retrieval Conference (ISMIR)}, 2021, pp. 594--601.

\bibitem{nieto2020audio}
O.~Nieto, G.~J. Mysore, C.-i. Wang, J.~B. Smith, J.~Schl{\"u}ter, T.~Grill, and
  B.~McFee, ``Audio-based music structure analysis: Current trends, open
  challenges, and applications,'' \emph{Transactions of the International
  Society for Music Information Retrieval (TISMIR)}, vol.~3, no.~1, 2020.

\bibitem{wang2021supervised}
J.-C. Wang, J.~B. Smith, W.-T. Lu, and X.~Song, ``Supervised metric learning
  for music structure feature,'' in \emph{International Society of Music
  Information Retrieval Conference (ISMIR)}, 2021, pp. 730--737.

\bibitem{wang2022catch}
J.-C. Wang, Y.-N. Hung, and J.~B.~L. Smith, ``{To Catch A Chorus, Verse, Intro,
  or Anything Else: Analyzing a Song with Structural Functions},'' in
  \emph{International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}, 2022, pp. 416--420.

\bibitem{hung2022modeling}
Y.-N. Hung, J.-C. Wang, X.~Song, W.-T. Lu, and M.~Won, ``{Modeling Beats and
  Downbeats with a Time-Frequency Transformer},'' in \emph{International
  Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 2022, pp.
  401--405.

\bibitem{Foote2002}
J.~Foote, ``Automatic audio segmentation using a measure of audio novelty,'' in
  \emph{International Conference on Multimedia and Expo (ICME)}, vol.~1, 2000,
  pp. 452--455.

\bibitem{mcfee2014learning}
B.~McFee and D.~P. Ellis, ``Learning to segment songs with ordinal linear
  discriminant analysis,'' in \emph{International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, 2014, pp. 5197--5201.

\bibitem{Maezawa2019}
A.~Maezawa, ``Music boundary detection based on a hybrid deep model of novelty,
  homogeneity, repetition and duration,'' in \emph{International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)}, 2019, pp. 206--210.

\bibitem{wang2021chorus}
J.-C. Wang, J.~B. Smith, J.~Chen, X.~Song, and Y.~Wang, ``Supervised chorus
  detection for popular music using convolutional neural network and multi-task
  learning,'' in \emph{International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)}, 2021.

\bibitem{hassani2023neighborhood}
A.~Hassani, S.~Walton, J.~Li, S.~Li, and H.~Shi, ``Neighborhood attention
  transformer,'' in \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2023.

\bibitem{nieto2019harmonix}
O.~Nieto, M.~McCallum, M.~E. Davies, A.~Robertson, A.~M. Stark, and E.~Egozy,
  ``The harmonix set: Beats, downbeats, and functional segment annotations of
  western popular music.'' in \emph{International Society of Music Information
  Retrieval Conference (ISMIR)}, 2019, pp. 565--572.

\bibitem{hassani2022dilated}
A.~Hassani and H.~Shi, ``Dilated neighborhood attention transformer,''
  \emph{arXiv:2209.15001}, 2022.

\bibitem{bai2018tcn}
S.~Bai, J.~Z. Kolter, and V.~Koltun, ``An empirical evaluation of generic
  convolutional and recurrent networks for sequence modeling,''
  \emph{arXiv:1803.01271}, 2018.

\bibitem{krebs2015efficient}
F.~Krebs, S.~B{\"o}ck, and G.~Widmer, ``An efficient state-space model for
  joint tempo and meter tracking.'' in \emph{International Society of Music
  Information Retrieval Conference (ISMIR)}, 2015, pp. 72--78.

\bibitem{ullrich2014boundary}
K.~Ullrich, J.~Schl{\"u}ter, and T.~Grill, ``Boundary detection in music
  structure analysis using convolutional neural networks.'' in
  \emph{International Society of Music Information Retrieval Conference
  (ISMIR)}, 2014, pp. 417--422.

\bibitem{matthewdavies2019temporal}
E.~P. MatthewDavies and S.~Böck, ``{Temporal convolutional networks for
  musical audio beat tracking},'' in \emph{European Signal Processing
  Conference (EUSIPCO)}, 2019, pp. 1--5.

\bibitem{davies2009evaluation}
M.~E. Davies, N.~Degara, and M.~D. Plumbley, ``Evaluation methods for musical
  audio beat tracking algorithms,'' \emph{Queen Mary University of London,
  Centre for Digital Music, Tech. Rep. C4DM-TR-09-06}, 2009.

\bibitem{rouard2022hybrid}
S.~Rouard, F.~Massa, and A.~D{\'e}fossez, ``Hybrid transformers for music
  source separation,'' in \emph{International Conference on Acoustics, Speech
  and Signal Processing (ICASSP)}, 2023.

\bibitem{madmom}
S.~B{\"o}ck, F.~Korzeniowski, J.~Schl{\"u}ter, F.~Krebs, and G.~Widmer,
  ``{madmom: a new Python Audio and Music Signal Processing Library},'' in
  \emph{International Conference on Multimedia}, 2016, pp. 1174--1178.

\bibitem{Liu2020On}
L.~Liu, H.~Jiang, P.~He, W.~Chen, X.~Liu, J.~Gao, and J.~Han, ``On the variance
  of the adaptive learning rate and beyond,'' in \emph{International Conference
  on Learning Representations (ICLR)}, 2020.

\bibitem{izmailov2018averaging}
P.~Izmailov, D.~Podoprikhin, T.~Garipov, D.~Vetrov, and A.~G. Wilson,
  ``Averaging weights leads to wider optima and better generalization,'' in
  \emph{Conference on Uncertainty in Artificial Intelligence (UAI)}, 2018, pp.
  876--885.

\end{thebibliography}
