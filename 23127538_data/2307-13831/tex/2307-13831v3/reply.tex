\documentclass[12pt]{amsart}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[dvipdfmx]{color}
\usepackage{tikz}
\usepackage{subcaption}
\usepackage{bm}
\usepackage{url}
\usepackage{graphicx}
\usepackage{float}
\usepackage{listings}
\lstset{language=Matlab,breaklines=true,morekeywords={matlab2tikz},%
keywordstyle=\color{blue},morekeywords=[2]{1},keywordstyle=[2]{\color{black}},%
identifierstyle=\color{black},stringstyle=\color{string},commentstyle=\color{comment},%
showstringspaces=false,numbers=left,numberstyle={\tiny\color{black}},numbersep=9pt,%
emph=[1]{function,for,if,elseif,end},emphstyle=[1]{\bf\color{blue}}}

\setlength{\textwidth}{33truepc}
\setlength{\textheight}{50truepc}
\setlength{\oddsidemargin}{1cm}
\setlength{\evensidemargin}{1cm}
\makeatletter
 \renewcommand{\theequation}{%
 \thesection.\arabic{equation}}
 \@addtoreset{equation}{section}
\makeatother
\newcommand{\Natural }{\mathbf N}
\newcommand{\Integer }{\mathbf Z}
\newcommand{\Rational}{\mathbf Q}
\newcommand{\Real }{\mathbf R}

\newtheorem{dfn}{Definition}[section]
\newtheorem{thm}[dfn]{Theorem}
\newtheorem{prop}[dfn]{Proposition}
\newtheorem{lem}[dfn]{Lemma}
\newtheorem{cor}[dfn]{Corollary}
\newtheorem{rem}[dfn]{Remark}
\newtheorem{expl}[dfn]{Example}
\newtheorem{prob}[dfn]{Problem}
\newtheorem{algo}[dfn]{Algorithm}
\newcommand{\bsquare}{\hbox{\rule{6pt}{6pt}}}
\renewcommand{\theenumi}{\roman{enumi}}
\def\Vec#1{\mbox{\boldmath $#1$}}
\newcommand{\argmax}{\operatornamewithlimits{Argmax}}
\newcommand{\argmin}{\operatornamewithlimits{Argmin}}

\begin{document}
\begin{center}
\textbf{Reply}
\end{center}

\pagestyle{plain}

We would like to express our gratitude to the Action Editor and the reviewers for their valuable comments on our manuscript. We appreciate their detailed assessments and helpful feedback. We have revised the manuscript to incorporate all of the recommendations, which has resulted in an improved presentation of our work. The revised parts of the manuscript are marked in red.\\
 
\textbf{Reviewer sQvN}\\

\underline{Main Comments}:
1. I understand your experiments are trying to demonstrate the correctness of theoretical results. However, I do not find any baselines, which means your approach's effectiveness is unclear. As you mentioned in the first paragraph of Section 1.1, there are many variants of SGD. However, I do not find any of them used as a baseline in the experiment.\\

2. The experiments only considered two very relatively small neural networks. I would like to see the method performance using large networks such as wide-resnet. \\

\underline{Reply}: Thank you for your valuable comments. The revised manuscript gives numerical comparisons of SGD using the Armijo line search (SGD+Armijo) with the existing optimizers, such as SGD using a constant learning rate, the momentum method, Adam, AdamW, and RMSProp for training ResNet-34 on the CIFAR-10 and CIFAR-100 datasets, MLP on the MNIST dataset, and Wide ResNet-50-2 on the CIFAR-10 dataset. The numerical results show that SGD+Armijo using the critical batch size performs better than the other optimizers for any batch size in the sense of minimizing the SFO complexities needed for achieving high training accuracies. Please see Section 4 in the revised manuscript.

\newpage

\textbf{Reviewer FGry}\\ 

\underline{Main Comment 1}:
However, looking through the paper I didn't find the current proofs convincing.\\

\underline{Reply}: Thank you very much for pointing out. We revised the proof of Theorem 3.1. Please see Appendix A.2 for details. During revision of the proof of Theorem 3.1, we found that we could delete $C(\theta_0,f_*)$.\\

\underline{Main Comment 2}:
Other weaknesses\\

\underline{Reply}: Thank you for your valuable comments. We compare the SFO complexity $N_{\mathrm{C}}$ for SGD using a constant learning rate $\alpha$ with $N_{\mathrm{A}}$ for SGD using the Armijo-line-search learning rate $\alpha_k$ $(< \overline{\alpha})$. Let $C_{1,\mathrm{C}}$ (resp. $C_{2,\mathrm{C}}$) be $C_1$ (resp. $C_{2}$) in Table 1 for SGD using a constant learning rate and let $C_{1,\mathrm{A}}$ (resp. $C_{2,\mathrm{A}}$) be $C_1$ (resp. $C_2$) in Table 1 for SGD using the Armijo-line-search learning rate. We have that
\begin{align}\label{comparison_C}
\begin{split}
&C_{1,\mathrm{A}} < C_{1,\mathrm{C}}
\text{ iff } 
\overline{\alpha} < \frac{2}{L_n} - \frac{(t+1)(2-L_n \alpha)\alpha L}{2 \delta (1-c) L_n} \text{ } 
\left(< \frac{2}{L_n} \right), \\
&C_{2,\mathrm{A}} < C_{2,\mathrm{C}}
\text{ iff } 
\overline{\alpha} < \frac{\sigma_{\mathrm{C}}^2 (2 - L_n \overline{\alpha})}{\sigma_{\mathrm{A}}^2 (2 - L_n \alpha)} \alpha
\text{ } 
\bigg( 
\underset{[\text{If } \underline{\alpha} \approx \alpha]}\approx
\frac{\sigma_{\mathrm{C}}^2}{\sigma_{\mathrm{A}}^2} \underbrace{\frac{1}{1-\delta (1-c)}}_{>1} \alpha
\bigg),
\end{split}
\end{align} 
where $\sigma_{\mathrm{C}}^2$ (resp. $\sigma_{\mathrm{A}}^2$) denotes the upper bound of the variance of the stochastic gradient for SGD using a constant learning rate $\alpha$ (resp. the Armijo-line-search learning rate). Hence, if (\ref{comparison_C}) holds, then SGD using the Armijo-line-search learning rate converges faster than SGD using a constant learning rate in the sense that 
\begin{align*}
\frac{C_{1,\mathrm{A}} b^2}{\epsilon^2 b - C_{2,\mathrm{A}}} = N_{\mathrm{A}} < N_{\mathrm{C}}
= \frac{C_{1,\mathrm{C}} b^2}{\epsilon^2 b - C_{2,\mathrm{C}}}.
\end{align*}
It would be difficult to check exactly that (\ref{comparison_C}) holds before implementing SGD, since (\ref{comparison_C}) involves unknown parameters, such as $L_n = \frac{1}{n} \sum_{i\in [n]} L_i$, $\sigma_{\mathrm{C}}^2$, and $\sigma_{\mathrm{A}}^2$. However, it can be expected that (\ref{comparison_C}) holds, since it is known empirically that the relationship between the Armijo-line-search learning rate $\alpha_k$ and a constant learning rate $\alpha$ is $\alpha < \alpha_k$ $(< \overline{\alpha} < \frac{2}{L_n})$ (Section 3.3 provides the derivation of condition (\ref{comparison_C})).

To verify whether SGD using the Armijo-line-search learning rate performs better than SGD using a constant learning rate (see the discussion in condition (\ref{comparison_C})), we numerically compared SGD using the Armijo-line-search learning rate with not only SGD using a constant learning rate but also variants of SGD, such as the momentum method, Adam, AdamW, and RMSProp. We found that SGD using the Armijo-line-search learning rate and the critical batch size performs better than the other optimizers in the sense of minimizing the SFO complexities needed to achieve high training accuracies. Please see Section 4 in the revised manuscript.\\

\underline{Reply to Minor 1}: We think that $\Xi$ may be finite/infinite.\\ 
 
\underline{Reply to Minor 2}: We consider setting small batch sizes, e.g., $b = 1$. From the condition of the domain of $K$, we need to satisfy 
\begin{align}\label{condition_upper}
b = 1 > \frac{C_2}{\epsilon^2} \text{ iff } 
\overline{\alpha} < \frac{2 \epsilon^2}{L_n (\sigma^2 + \epsilon^2)} \text{ } 
\left(< \frac{2}{L_n} \right)
\end{align}
to ensure the results in Theorem 3.2. If the upper bound $\overline{\alpha}$ satisfies the more restricted condition (\ref{condition_upper}) than $\overline{\alpha} < \frac{2}{L_n}$, then SGD using the Armijo line search with $b=1$ and $K(1) = \frac{C_1}{\epsilon^2 - C_2}$ is an $\epsilon$--approximation. Please see also Section 3.2 in the revised manuscript.\\

\underline{Reply to Minor 3}: The list of all considered assumptions in Theorem 3.1 is in Section 3.1. Please check it.\\

\underline{Reply to Minor-Minor}: Thank you for your comment. We added the ``hyperref" package. Please check the revised manuscript.

\end{document}
