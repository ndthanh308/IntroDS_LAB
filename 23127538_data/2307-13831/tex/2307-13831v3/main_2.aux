\relax 
\citation{robb1951,zinkevich2003,nem2009,gha2012,gha2013}
\citation{JMLR:v18:16-595,NEURIPS2019_2557911c,feh2020,chen2020,sca2020,loizou2021}
\citation{polyak1964,nest1983}
\citation{adagrad}
\citation{rmsprop}
\citation{adam}
\citation{reddi2018}
\citation{loshchilov2018decoupled}
\citation{NIPS2017_8a1d6947,pmlr-v206-naganuma23a,pmlr-v202-sato23b}
\citation{shallue2019}
\citation{zhang2019}
\citation{iiduka2022critical,pmlr-v202-sato23b}
\citation{shallue2019}
\citation{zhang2019}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Background}{1}{}\protected@file@percent }
\newlabel{subsec:1.1}{{1.1}{1}}
\citation{NEURIPS2019_2557911c}
\citation{noce}
\citation{NEURIPS2019_2557911c}
\citation{NEURIPS2019_2557911c}
\citation{NEURIPS2019_2557911c}
\citation{NEURIPS2019_2557911c}
\citation{shallue2019,zhang2019,iiduka2022critical,pmlr-v202-sato23b}
\citation{iiduka2022critical,pmlr-v202-sato23b}
\citation{iiduka2022critical,pmlr-v202-sato23b}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Motivation}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Contribution}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Convergence analysis of SGD with Armijo-line-search learning rates}{2}{}\protected@file@percent }
\newlabel{subsec:1.3.1}{{1.3.1}{2}}
\citation{shallue2019,zhang2019,iiduka2022critical,pmlr-v202-sato23b}
\newlabel{main}{{1.1}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Relationship between batch size $b$ and number of steps $K$ to achieve an $\epsilon $--approximation defined by $\qopname  \relax m{min}_{k\in [0:K-1]} \mathbb  {E}[\| \nabla f (\bm  {\theta }_k)\|^2 ] \leq \frac  {C_1}{K} + \frac  {C_2}{b} = \epsilon ^2$ for SGD with a constant learning rate $\alpha \in (0,\frac  {2}{L_n})$ and for SGD with the Armijo-line-search learning rate $\alpha _k \in [\underline  {\alpha }, \overline  {\alpha }]$ ($[0:K-1] := \{0,1,\ldots  ,K-1\}$, $f := \frac  {1}{n} \DOTSB \sum@ \slimits@ _{i\in [n]} f_i$ is bounded below by $f_*$, $L_i$ is the Lipschitz constant of $\nabla f_i$, $L_n := \frac  {1}{n} \DOTSB \sum@ \slimits@ _{i\in [n]} L_i$, and $\sigma ^2$ is the upper bound of the variance of the stochastic gradient)}}{3}{}\protected@file@percent }
\newlabel{table:1}{{1}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}Steps needed for $\epsilon $--approximation of SGD with Armijo line-search-learning rates}{3}{}\protected@file@percent }
\newlabel{subsec:1.3.2}{{1.3.2}{3}}
\newlabel{f_K}{{1.2}{3}}
\citation{NEURIPS2019_2557911c}
\citation{NEURIPS2019_2557911c}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3}Critical batch size minimizing SFO complexity of SGD with Armijo-line-search learning rates}{4}{}\protected@file@percent }
\newlabel{subsec:1.3.3}{{1.3.3}{4}}
\newlabel{f_N}{{1.3}{4}}
\newlabel{CBS}{{1.4}{4}}
\newlabel{comparison_C}{{1.5}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.4}Numerical results supporting our theoretical results}{4}{}\protected@file@percent }
\citation{doi:10.1137/1.9781611974997}
\@writefile{toc}{\contentsline {section}{\numberline {2}Mathematical Preliminaries}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Definitions}{5}{}\protected@file@percent }
\newlabel{subsec:2.1}{{2.1}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Assumptions and problem}{5}{}\protected@file@percent }
\newlabel{prob:1}{{2.1}{5}}
\newlabel{assum:1}{{2.1}{5}}
\newlabel{gradient}{{2.1}{5}}
\newlabel{sigma}{{2.2}{5}}
\citation{noce}
\citation{NEURIPS2019_2557911c}
\citation{NEURIPS2019_2557911c}
\citation{noce}
\citation{NEURIPS2019_2557911c}
\citation{noce}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Stochastic gradient descent using Armijo line search}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Armijo condition}{6}{}\protected@file@percent }
\newlabel{subsec:2.3.1}{{2.3.1}{6}}
\newlabel{iterative}{{2.3}{6}}
\newlabel{optimal_ss}{{2.4}{6}}
\newlabel{armijo_0}{{2.5}{6}}
\newlabel{existence_armijo}{{2.1}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Stochastic gradient descent under Armijo condition}{6}{}\protected@file@percent }
\newlabel{armijo}{{2.6}{6}}
\citation{NEURIPS2019_2557911c}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Stochastic gradient descent using Armijo line search}}{7}{}\protected@file@percent }
\newlabel{algo:1}{{1}{7}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Backtracking Armijo-line-search method \citep  [Algorithm 3.1]{noce}}}{7}{}\protected@file@percent }
\newlabel{algo:2}{{2}{7}}
\newlabel{lem:1}{{2.1}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Analysis of SGD using Armijo Line Search}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Convergence analysis of Algorithm 1\@setref@ {}}{7}{}\protected@file@percent }
\newlabel{subsec:3.1}{{3.1}{7}}
\newlabel{thm:1}{{3.1}{7}}
\citation{NEURIPS2018_d54e99a6}
\citation{NEURIPS2019_2557911c}
\citation{NEURIPS2019_2557911c}
\newlabel{sgd_c}{{3.1}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Steps needed for $\epsilon $--approximation}{8}{}\protected@file@percent }
\newlabel{epsilon}{{3.2}{8}}
\newlabel{thm:2}{{3.2}{9}}
\newlabel{K}{{3.3}{9}}
\newlabel{condition_upper}{{3.4}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Critical batch size minimizing SFO complexity}{9}{}\protected@file@percent }
\newlabel{sec:3.3}{{3.3}{9}}
\newlabel{thm:3}{{3.3}{9}}
\newlabel{SFOC}{{3.5}{9}}
\newlabel{cbs}{{3.6}{9}}
\newlabel{ucbs}{{3.7}{9}}
\citation{NEURIPS2019_2557911c}
\citation{shallue2019,zhang2019,iiduka2022critical}
\newlabel{comp_1}{{3.8}{10}}
\newlabel{comp_2}{{3.9}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Numerical Results}{10}{}\protected@file@percent }
\newlabel{sec:4}{{4}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Training ResNet and MLP on the CIFAR-10, CIFAR-100, and MNIST datasets}{11}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Number of steps for Algorithm 1\@setref@ {} versus batch size needed to train ResNet-34 on CIFAR-10}}{11}{}\protected@file@percent }
\newlabel{fig1}{{1}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces SFO complexity for Algorithm 1\@setref@ {} versus batch size needed to train ResNet-34 on CIFAR-10 (The double-circle symbol denotes the measured critical batch size)}}{11}{}\protected@file@percent }
\newlabel{fig2}{{2}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Number of steps for Algorithm 1\@setref@ {} with $c = 0.20$ and variants of SGD versus batch size needed to train ResNet-34 on CIFAR-10}}{11}{}\protected@file@percent }
\newlabel{fig1_1}{{3}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces SFO complexity for Algorithm 1\@setref@ {} with $c = 0.20$ and variants of SGD versus batch size needed to train ResNet-34 on CIFAR-10}}{11}{}\protected@file@percent }
\newlabel{fig2_1}{{4}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Number of steps for Algorithm 1\@setref@ {} versus batch size needed to train ResNet-34 on CIFAR-100}}{12}{}\protected@file@percent }
\newlabel{fig3}{{5}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces SFO complexity for Algorithm 1\@setref@ {} versus batch size needed to train ResNet-34 on CIFAR-100 (The double-circle symbol denotes the measured critical batch size)}}{12}{}\protected@file@percent }
\newlabel{fig4}{{6}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Number of steps for Algorithm 1\@setref@ {} with $c = 0.25$ and variants of SGD versus batch size needed to train ResNet-34 on CIFAR-100}}{12}{}\protected@file@percent }
\newlabel{fig3_1}{{7}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces SFO complexity for Algorithm 1\@setref@ {} with $c = 0.25$ and variants of SGD versus batch size needed to train ResNet-34 on CIFAR-100}}{12}{}\protected@file@percent }
\newlabel{fig4_1}{{8}{12}}
\citation{zagoruyko2017wide}
\citation{iiduka2022critical}
\citation{pmlr-v202-sato23b}
\citation{NEURIPS2019_2557911c}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Number of steps for Algorithm 1\@setref@ {} versus batch size needed to train MLP on MNIST}}{13}{}\protected@file@percent }
\newlabel{fig5}{{9}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces SFO complexity for Algorithm 1\@setref@ {} versus batch size needed to train MLP on MNIST (The double-circle symbol denotes the measured critical batch size)}}{13}{}\protected@file@percent }
\newlabel{fig6}{{10}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Number of steps for Algorithm 1\@setref@ {} with $c = 0.05$ and variants of SGD versus batch size needed to train MLP on MNIST}}{13}{}\protected@file@percent }
\newlabel{fig5_1}{{11}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces SFO complexity for Algorithm 1\@setref@ {} with $c = 0.05$ and variants of SGD versus batch size needed to train MLP on MNIST}}{13}{}\protected@file@percent }
\newlabel{fig6_1}{{12}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Number of steps for Algorithm 1\@setref@ {} versus batch size needed to train Wide ResNet-50-2 on CIFAR-10}}{14}{}\protected@file@percent }
\newlabel{fig13}{{13}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces SFO complexity for Algorithm 1\@setref@ {} versus batch size needed to train Wide ResNet-50-2 on CIFAR-10 (The double-circle symbol denotes the measured critical batch size)}}{14}{}\protected@file@percent }
\newlabel{fig14}{{14}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Number of steps for Algorithm 1\@setref@ {} with $c = 3 \times 10^{-4}$ and variants of SGD versus batch size needed to train Wide ResNet-50-2 on CIFAR-10}}{14}{}\protected@file@percent }
\newlabel{fig15}{{15}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces SFO complexity for Algorithm 1\@setref@ {} with $c = 3 \times 10^{-4}$ and variants of SGD versus batch size needed to train Wide ResNet-50-2 on CIFAR-10}}{14}{}\protected@file@percent }
\newlabel{fig16}{{16}{14}}
\bibdata{biblio}
\bibcite{doi:10.1137/1.9781611974997}{{1}{2017}{{Beck}}{{}}}
\bibcite{chen2020}{{2}{2020}{{Chen et~al.}}{{Chen, Zheng, AL~Kontar, and Raskutti}}}
\bibcite{adagrad}{{3}{2011}{{Duchi et~al.}}{{Duchi, Hazan, and Singer}}}
\bibcite{feh2020}{{4}{2020}{{Fehrman et~al.}}{{Fehrman, Gess, and Jentzen}}}
\bibcite{galli2023dont}{{5}{2023}{{Galli et~al.}}{{Galli, Rauhut, and Schmidt}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Estimation of critical batch size}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{15}{}\protected@file@percent }
\bibcite{gha2012}{{6}{2012}{{Ghadimi \& Lan}}{{Ghadimi and Lan}}}
\bibcite{gha2013}{{7}{2013}{{Ghadimi \& Lan}}{{Ghadimi and Lan}}}
\bibcite{NIPS2017_8a1d6947}{{8}{2017}{{Heusel et~al.}}{{Heusel, Ramsauer, Unterthiner, Nessler, and Hochreiter}}}
\bibcite{iiduka2022critical}{{9}{2022}{{Iiduka}}{{}}}
\bibcite{JMLR:v18:16-595}{{10}{2018}{{Jain et~al.}}{{Jain, Kakade, Kidambi, Netrapalli, and Sidford}}}
\bibcite{adam}{{11}{2015}{{Kingma \& Ba}}{{Kingma and Ba}}}
\bibcite{loizou2021}{{12}{2021}{{Loizou et~al.}}{{Loizou, Vaswani, Laradji, and Lacoste-Julien}}}
\bibcite{loshchilov2018decoupled}{{13}{2019}{{Loshchilov \& Hutter}}{{Loshchilov and Hutter}}}
\bibcite{pmlr-v206-naganuma23a}{{14}{2023}{{Naganuma \& Iiduka}}{{Naganuma and Iiduka}}}
\bibcite{nem2009}{{15}{2009}{{Nemirovski et~al.}}{{Nemirovski, Juditsky, Lan, and Shapiro}}}
\bibcite{nest1983}{{16}{1983}{{Nesterov}}{{}}}
\bibcite{noce}{{17}{2006}{{Nocedal \& Wright}}{{Nocedal and Wright}}}
\bibcite{polyak1964}{{18}{1964}{{Polyak}}{{}}}
\bibcite{reddi2018}{{19}{2018}{{Reddi et~al.}}{{Reddi, Kale, and Kumar}}}
\bibcite{robb1951}{{20}{1951}{{Robbins \& Monro}}{{Robbins and Monro}}}
\bibcite{pmlr-v202-sato23b}{{21}{2023}{{Sato \& Iiduka}}{{Sato and Iiduka}}}
\bibcite{sca2020}{{22}{2020}{{Scaman \& Malherbe}}{{Scaman and Malherbe}}}
\bibcite{shallue2019}{{23}{2019}{{Shallue et~al.}}{{Shallue, Lee, Antognini, Sohl-Dickstein, Frostig, and Dahl}}}
\bibcite{rmsprop}{{24}{2012}{{Tieleman \& Hinton}}{{Tieleman and Hinton}}}
\bibcite{NEURIPS2019_2557911c}{{25}{2019}{{Vaswani et~al.}}{{Vaswani, Mishkin, Laradji, Schmidt, Gidel, and Lacoste-Julien}}}
\bibcite{NEURIPS2018_d54e99a6}{{26}{2018}{{Virmaux \& Scaman}}{{Virmaux and Scaman}}}
\bibcite{zagoruyko2017wide}{{27}{2017}{{Zagoruyko \& Komodakis}}{{Zagoruyko and Komodakis}}}
\bibcite{zhang2019}{{28}{2019}{{Zhang et~al.}}{{Zhang, Li, Nado, Martens, Sachdeva, Dahl, Shallue, and Grosse}}}
\bibcite{zinkevich2003}{{29}{2003}{{Zinkevich}}{{}}}
\bibstyle{tmlr}
\citation{NEURIPS2019_2557911c}
\citation{galli2023dont}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Proof of lemma 2.1\@setref@ {}}{17}{}\protected@file@percent }
\newlabel{a1}{{A.1}{17}}
\newlabel{vas}{{A.1}{17}}
\newlabel{not_vas}{{A.2}{17}}
\newlabel{ineq:1}{{A.3}{18}}
\newlabel{ineq:2}{{A.4}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Proof of Theorem 3.1\@setref@ {}}{18}{}\protected@file@percent }
\newlabel{a2}{{A.2}{18}}
\newlabel{ineq_main}{{A.5}{18}}
\newlabel{ineq_main_2}{{A.6}{19}}
\newlabel{e_1}{{A.7}{19}}
\newlabel{e_2}{{A.8}{19}}
\newlabel{ineq_main_3}{{A.9}{19}}
\newlabel{total}{{A.10}{19}}
\newlabel{ineq:0}{{A.11}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Proof of Theorem 3.2\@setref@ {}}{20}{}\protected@file@percent }
\newlabel{a3}{{A.3}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Proof of Theorem 3.3\@setref@ {}}{20}{}\protected@file@percent }
\newlabel{a4}{{A.4}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.5}Proof of (3.1\@setref@ {})}{21}{}\protected@file@percent }
\newlabel{a5}{{A.5}{21}}
\gdef \@abspage@last{21}
