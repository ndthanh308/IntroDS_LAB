\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\citation{robb1951,zinkevich2003,nem2009,gha2012,gha2013}
\citation{JMLR:v18:16-595,NEURIPS2019_2557911c,feh2020,chen2020,sca2020,loizou2021}
\citation{polyak1964,nest1983}
\citation{adagrad}
\citation{rmsprop}
\citation{adam}
\citation{reddi2018}
\citation{loshchilov2018decoupled}
\citation{NIPS2017_8a1d6947,pmlr-v206-naganuma23a,pmlr-v202-sato23b}
\citation{shallue2019}
\citation{zhang2019}
\citation{iiduka2022critical,pmlr-v202-sato23b}
\citation{shallue2019}
\citation{zhang2019}
\HyPL@Entry{0<</S/D>>}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Background}{1}{subsection.1.1}\protected@file@percent }
\newlabel{subsec:1.1}{{1.1}{1}{Background}{subsection.1.1}{}}
\citation{NEURIPS2019_2557911c}
\citation{noce}
\citation{NEURIPS2019_2557911c}
\citation{NEURIPS2019_2557911c}
\citation{NEURIPS2019_2557911c}
\citation{NEURIPS2019_2557911c}
\citation{shallue2019,zhang2019,iiduka2022critical,pmlr-v202-sato23b}
\citation{iiduka2022critical,pmlr-v202-sato23b}
\citation{iiduka2022critical,pmlr-v202-sato23b}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Motivation}{2}{subsection.1.2}\protected@file@percent }
\citation{sca2020}
\citation{NEURIPS2018_d54e99a6}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Contribution}{3}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Convergence analysis of SGD with Armijo-line-search learning rates}{3}{subsubsection.1.3.1}\protected@file@percent }
\newlabel{subsec:1.3.1}{{1.3.1}{3}{Convergence analysis of SGD with Armijo-line-search learning rates}{subsubsection.1.3.1}{}}
\newlabel{main}{{1}{3}{Convergence analysis of SGD with Armijo-line-search learning rates}{equation.1.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Relationship between batch size $b$ and number of steps $K$ to achieve an $\epsilon $--approximation defined by $\qopname  \relax m{min}_{k\in [0:K-1]} \mathbb  {E}[\delimiter "026B30D  \nabla f (\bm  {\theta }_k)\delimiter "026B30D ^2 ] \leq \frac  {C_1}{K} + \frac  {C_2}{b} = \epsilon ^2$ for SGD with a constant learning rate $\alpha \in (0,\frac  {2}{L_n})$ and for SGD with the Armijo-line-search learning rate ($[0:K-1] := \{0,1,\ldots  ,K-1\}$. $f := \frac  {1}{n} \DOTSB \sum@ \slimits@ _{i\in [n]} f_i$ is bounded below by $f_*$, $L_i$ is the Lipschitz constant of $\nabla f_i$, $L_n := \frac  {1}{n} \DOTSB \sum@ \slimits@ _{i\in [n]} L_i$, $C(\bm  {\theta }_0, f_*)$ is a positive real number depending on the initial point $\bm  {\theta }_0 \in \mathbb  {R}^d$ and $f_*$, $\sigma ^2$ is the upper bound of the variance of the stochastic gradient, and $\overline  {\alpha }$ is the upper bound of the learning rates)}}{3}{table.1}\protected@file@percent }
\newlabel{table:1}{{1}{3}{Relationship between batch size $b$ and number of steps $K$ to achieve an $\epsilon $--approximation defined by $\min _{k\in [0:K-1]} \mathbb {E}[\| \nabla f (\bm {\theta }_k)\|^2 ] \leq \frac {C_1}{K} + \frac {C_2}{b} = \epsilon ^2$ for SGD with a constant learning rate $\alpha \in (0,\frac {2}{L_n})$ and for SGD with the Armijo-line-search learning rate ($[0:K-1] := \{0,1,\ldots ,K-1\}$. $f := \frac {1}{n} \sum _{i\in [n]} f_i$ is bounded below by $f_*$, $L_i$ is the Lipschitz constant of $\nabla f_i$, $L_n := \frac {1}{n} \sum _{i\in [n]} L_i$, $C(\bm {\theta }_0, f_*)$ is a positive real number depending on the initial point $\bm {\theta }_0 \in \mathbb {R}^d$ and $f_*$, $\sigma ^2$ is the upper bound of the variance of the stochastic gradient, and $\overline {\alpha }$ is the upper bound of the learning rates)}{table.1}{}}
\citation{shallue2019,zhang2019,iiduka2022critical,pmlr-v202-sato23b}
\citation{NEURIPS2019_2557911c}
\citation{doi:10.1137/1.9781611974997}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}Steps needed for $\epsilon $--approximation of SGD with Armijo line-search-learning rates}{4}{subsubsection.1.3.2}\protected@file@percent }
\newlabel{subsec:1.3.2}{{1.3.2}{4}{Steps needed for $\epsilon $--approximation of SGD with Armijo line-search-learning rates}{subsubsection.1.3.2}{}}
\newlabel{f_K}{{2}{4}{Steps needed for $\epsilon $--approximation of SGD with Armijo line-search-learning rates}{equation.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3}Critical batch size minimizing SFO complexity of SGD with Armijo-line-search learning rates}{4}{subsubsection.1.3.3}\protected@file@percent }
\newlabel{subsec:1.3.3}{{1.3.3}{4}{Critical batch size minimizing SFO complexity of SGD with Armijo-line-search learning rates}{subsubsection.1.3.3}{}}
\newlabel{f_N}{{3}{4}{Critical batch size minimizing SFO complexity of SGD with Armijo-line-search learning rates}{equation.1.3}{}}
\newlabel{CBS}{{4}{4}{Critical batch size minimizing SFO complexity of SGD with Armijo-line-search learning rates}{equation.1.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.4}Numerical results supporting our theoretical results}{4}{subsubsection.1.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Mathematical Preliminaries}{4}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Definitions}{4}{subsection.2.1}\protected@file@percent }
\newlabel{subsec:2.1}{{2.1}{4}{Definitions}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Assumptions and problem}{5}{subsection.2.2}\protected@file@percent }
\@writefile{thm}{\contentsline {prob}{{Problem}{2.{1}}{}}{5}{prob.2.1}\protected@file@percent }
\newlabel{prob:1}{{2.{1}}{5}{Assumptions and problem}{prob.2.1}{}}
\@writefile{thm}{\contentsline {assum}{{Assumption}{2.{1}}{}}{5}{assum.2.1}\protected@file@percent }
\newlabel{assum:1}{{2.{1}}{5}{Assumptions and problem}{assum.2.1}{}}
\newlabel{sigma}{{5}{5}{Assumptions and problem}{Item.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Stochastic gradient descent using Armijo line search}{5}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Armijo condition}{5}{subsubsection.2.3.1}\protected@file@percent }
\newlabel{subsec:2.3.1}{{2.3.1}{5}{Armijo condition}{subsubsection.2.3.1}{}}
\newlabel{iterative}{{6}{5}{Armijo condition}{equation.2.6}{}}
\newlabel{optimal_ss}{{7}{5}{Armijo condition}{equation.2.7}{}}
\citation{noce}
\citation{NEURIPS2019_2557911c}
\citation{NEURIPS2019_2557911c}
\citation{noce}
\citation{NEURIPS2019_2557911c}
\citation{noce}
\newlabel{armijo_0}{{8}{6}{Armijo condition}{equation.2.8}{}}
\@writefile{thm}{\contentsline {prop}{{Proposition}{2.{1}}{}}{6}{prop.2.1}\protected@file@percent }
\newlabel{existence_armijo}{{2.{1}}{6}{Armijo condition}{prop.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Stochastic gradient descent under Armijo condition}{6}{subsubsection.2.3.2}\protected@file@percent }
\newlabel{armijo}{{9}{6}{Stochastic gradient descent under Armijo condition}{equation.2.9}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Stochastic gradient descent using Armijo line search}}{6}{algorithm.1}\protected@file@percent }
\newlabel{algo:1}{{1}{6}{Stochastic gradient descent under Armijo condition}{algorithm.1}{}}
\citation{NEURIPS2019_2557911c}
\citation{NEURIPS2018_d54e99a6}
\citation{NEURIPS2019_2557911c}
\citation{NEURIPS2019_2557911c}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Backtracking Armijo-line-search method \citep  [Algorithm 3.1]{noce}}}{7}{algorithm.2}\protected@file@percent }
\newlabel{algo:2}{{2}{7}{Stochastic gradient descent under Armijo condition}{algorithm.2}{}}
\@writefile{thm}{\contentsline {lem}{{Lemma}{2.{1}}{}}{7}{lem.2.1}\protected@file@percent }
\newlabel{lem:1}{{2.{1}}{7}{Stochastic gradient descent under Armijo condition}{lem.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Analysis of SGD using Armijo Line Search}{7}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Convergence analysis of Algorithm \ref  {algo:1}}{7}{subsection.3.1}\protected@file@percent }
\newlabel{subsec:3.1}{{3.1}{7}{Convergence analysis of Algorithm \ref {algo:1}}{subsection.3.1}{}}
\@writefile{thm}{\contentsline {thm}{{Theorem}{3.{1}}{Upper bound of the squared norm of the full gradient}}{7}{thm.3.1}\protected@file@percent }
\newlabel{thm:1}{{3.{1}}{7}{Convergence analysis of Algorithm \ref {algo:1}}{thm.3.1}{}}
\newlabel{sgd_c}{{10}{7}{Convergence analysis of Algorithm \ref {algo:1}}{equation.3.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Steps needed for $\epsilon $--approximation}{8}{subsection.3.2}\protected@file@percent }
\newlabel{epsilon}{{11}{8}{Steps needed for $\epsilon $--approximation}{equation.3.11}{}}
\@writefile{thm}{\contentsline {thm}{{Theorem}{3.{2}}{Steps needed for nonconvex optimization of SGD using Armijo line search}}{8}{thm.3.2}\protected@file@percent }
\newlabel{thm:2}{{3.{2}}{8}{Steps needed for $\epsilon $--approximation}{thm.3.2}{}}
\newlabel{K}{{12}{8}{Steps needed for $\epsilon $--approximation}{equation.3.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Critical batch size minimizing SFO complexity}{8}{subsection.3.3}\protected@file@percent }
\@writefile{thm}{\contentsline {thm}{{Theorem}{3.{3}}{Existence of critical batch size for SGD using Armijo line search}}{8}{thm.3.3}\protected@file@percent }
\newlabel{thm:3}{{3.{3}}{8}{Critical batch size minimizing SFO complexity}{thm.3.3}{}}
\newlabel{SFOC}{{13}{8}{Critical batch size minimizing SFO complexity}{equation.3.13}{}}
\newlabel{cbs}{{14}{8}{Critical batch size minimizing SFO complexity}{Item.2}{}}
\citation{shallue2019,zhang2019,iiduka2022critical}
\citation{NEURIPS2019_2557911c}
\newlabel{ucbs}{{15}{9}{Critical batch size minimizing SFO complexity}{Item.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Numerical Results}{9}{section.4}\protected@file@percent }
\newlabel{sec:4}{{4}{9}{Numerical Results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Training ResNet and MLP on the CIFAR-10, CIFAR-100, and MNIST datasets}{9}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Number of steps for Algorithm \ref  {algo:1} versus batch size needed to train ResNet-34 on CIFAR-10}}{9}{figure.1}\protected@file@percent }
\newlabel{fig1}{{1}{9}{Number of steps for Algorithm \ref {algo:1} versus batch size needed to train ResNet-34 on CIFAR-10}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces SFO complexity for Algorithm \ref  {algo:1} versus batch size needed to train ResNet-34 on CIFAR-10 (The double-circle symbol denotes the measured critical batch size)}}{9}{figure.2}\protected@file@percent }
\newlabel{fig2}{{2}{9}{SFO complexity for Algorithm \ref {algo:1} versus batch size needed to train ResNet-34 on CIFAR-10 (The double-circle symbol denotes the measured critical batch size)}{figure.2}{}}
\citation{iiduka2022critical}
\citation{pmlr-v202-sato23b}
\citation{NEURIPS2019_2557911c}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Number of steps for Algorithm \ref  {algo:1} versus batch size needed to train ResNet-34 on CIFAR-100}}{10}{figure.3}\protected@file@percent }
\newlabel{fig3}{{3}{10}{Number of steps for Algorithm \ref {algo:1} versus batch size needed to train ResNet-34 on CIFAR-100}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces SFO complexity for Algorithm \ref  {algo:1} versus batch size needed to train ResNet-34 on CIFAR-100 (The double-circle symbol denotes the measured critical batch size)}}{10}{figure.4}\protected@file@percent }
\newlabel{fig4}{{4}{10}{SFO complexity for Algorithm \ref {algo:1} versus batch size needed to train ResNet-34 on CIFAR-100 (The double-circle symbol denotes the measured critical batch size)}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Number of steps for Algorithm \ref  {algo:1} versus batch size needed to train MLP on MNIST}}{10}{figure.5}\protected@file@percent }
\newlabel{fig5}{{5}{10}{Number of steps for Algorithm \ref {algo:1} versus batch size needed to train MLP on MNIST}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces SFO complexity for Algorithm \ref  {algo:1} versus batch size needed to train MLP on MNIST (The double-circle symbol denotes the measured critical batch size)}}{10}{figure.6}\protected@file@percent }
\newlabel{fig6}{{6}{10}{SFO complexity for Algorithm \ref {algo:1} versus batch size needed to train MLP on MNIST (The double-circle symbol denotes the measured critical batch size)}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Estimation of critical batch size}{10}{subsection.4.2}\protected@file@percent }
\bibdata{biblio}
\bibcite{doi:10.1137/1.9781611974997}{{1}{2017}{{Beck}}{{}}}
\bibcite{chen2020}{{2}{2020}{{Chen et~al.}}{{Chen, Zheng, AL~Kontar, and Raskutti}}}
\bibcite{adagrad}{{3}{2011}{{Duchi et~al.}}{{Duchi, Hazan, and Singer}}}
\bibcite{feh2020}{{4}{2020}{{Fehrman et~al.}}{{Fehrman, Gess, and Jentzen}}}
\bibcite{gha2012}{{5}{2012}{{Ghadimi \& Lan}}{{Ghadimi and Lan}}}
\bibcite{gha2013}{{6}{2013}{{Ghadimi \& Lan}}{{Ghadimi and Lan}}}
\bibcite{NIPS2017_8a1d6947}{{7}{2017}{{Heusel et~al.}}{{Heusel, Ramsauer, Unterthiner, Nessler, and Hochreiter}}}
\bibcite{iiduka2022critical}{{8}{2022}{{Iiduka}}{{}}}
\bibcite{JMLR:v18:16-595}{{9}{2018}{{Jain et~al.}}{{Jain, Kakade, Kidambi, Netrapalli, and Sidford}}}
\bibcite{adam}{{10}{2015}{{Kingma \& Ba}}{{Kingma and Ba}}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{11}{section.5}\protected@file@percent }
\bibcite{loizou2021}{{11}{2021}{{Loizou et~al.}}{{Loizou, Vaswani, Laradji, and Lacoste-Julien}}}
\bibcite{loshchilov2018decoupled}{{12}{2019}{{Loshchilov \& Hutter}}{{Loshchilov and Hutter}}}
\bibcite{pmlr-v206-naganuma23a}{{13}{2023}{{Naganuma \& Iiduka}}{{Naganuma and Iiduka}}}
\bibcite{nem2009}{{14}{2009}{{Nemirovski et~al.}}{{Nemirovski, Juditsky, Lan, and Shapiro}}}
\bibcite{nest1983}{{15}{1983}{{Nesterov}}{{}}}
\bibcite{noce}{{16}{2006}{{Nocedal \& Wright}}{{Nocedal and Wright}}}
\bibcite{polyak1964}{{17}{1964}{{Polyak}}{{}}}
\bibcite{reddi2018}{{18}{2018}{{Reddi et~al.}}{{Reddi, Kale, and Kumar}}}
\bibcite{robb1951}{{19}{1951}{{Robbins \& Monro}}{{Robbins and Monro}}}
\bibcite{pmlr-v202-sato23b}{{20}{2023}{{Sato \& Iiduka}}{{Sato and Iiduka}}}
\bibcite{sca2020}{{21}{2020}{{Scaman \& Malherbe}}{{Scaman and Malherbe}}}
\bibcite{shallue2019}{{22}{2019}{{Shallue et~al.}}{{Shallue, Lee, Antognini, Sohl-Dickstein, Frostig, and Dahl}}}
\bibcite{rmsprop}{{23}{2012}{{Tieleman \& Hinton}}{{Tieleman and Hinton}}}
\bibcite{NEURIPS2019_2557911c}{{24}{2019}{{Vaswani et~al.}}{{Vaswani, Mishkin, Laradji, Schmidt, Gidel, and Lacoste-Julien}}}
\bibcite{NEURIPS2018_d54e99a6}{{25}{2018}{{Virmaux \& Scaman}}{{Virmaux and Scaman}}}
\bibcite{zhang2019}{{26}{2019}{{Zhang et~al.}}{{Zhang, Li, Nado, Martens, Sachdeva, Dahl, Shallue, and Grosse}}}
\bibcite{zinkevich2003}{{27}{2003}{{Zinkevich}}{{}}}
\bibstyle{tmlr}
\citation{NEURIPS2019_2557911c}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{13}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Proof of lemma \ref  {lem:1}}{13}{subsection.A.1}\protected@file@percent }
\newlabel{a1}{{A.1}{13}{Proof of lemma \ref {lem:1}}{subsection.A.1}{}}
\newlabel{vas}{{16}{13}{Proof of lemma \ref {lem:1}}{equation.A.16}{}}
\newlabel{not_vas}{{17}{13}{Proof of lemma \ref {lem:1}}{equation.A.17}{}}
\newlabel{ineq:1}{{18}{13}{Proof of lemma \ref {lem:1}}{equation.A.18}{}}
\newlabel{ineq:2}{{19}{13}{Proof of lemma \ref {lem:1}}{equation.A.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Proof of Theorem \ref  {thm:1}}{14}{subsection.A.2}\protected@file@percent }
\newlabel{a2}{{A.2}{14}{Proof of Theorem \ref {thm:1}}{subsection.A.2}{}}
\newlabel{lip}{{20}{14}{Proof of Theorem \ref {thm:1}}{equation.A.20}{}}
\newlabel{ineq_main}{{21}{14}{Proof of Theorem \ref {thm:1}}{equation.A.21}{}}
\newlabel{ineq:3}{{22}{14}{Proof of Theorem \ref {thm:1}}{equation.A.22}{}}
\newlabel{e_1}{{23}{14}{Proof of Theorem \ref {thm:1}}{equation.A.23}{}}
\newlabel{e_2}{{24}{14}{Proof of Theorem \ref {thm:1}}{equation.A.24}{}}
\newlabel{total}{{25}{14}{Proof of Theorem \ref {thm:1}}{equation.A.25}{}}
\newlabel{sum}{{26}{15}{Proof of Theorem \ref {thm:1}}{equation.A.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Proof of Theorem \ref  {thm:2}}{16}{subsection.A.3}\protected@file@percent }
\newlabel{a3}{{A.3}{16}{Proof of Theorem \ref {thm:2}}{subsection.A.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Proof of Theorem \ref  {thm:3}}{16}{subsection.A.4}\protected@file@percent }
\newlabel{a4}{{A.4}{16}{Proof of Theorem \ref {thm:3}}{subsection.A.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.5}Proof of (\ref  {sgd_c})}{16}{subsection.A.5}\protected@file@percent }
\newlabel{a5}{{A.5}{16}{Proof of (\ref {sgd_c})}{subsection.A.5}{}}
