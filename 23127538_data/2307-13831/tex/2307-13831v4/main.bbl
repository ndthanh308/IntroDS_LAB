\begin{thebibliography}{27}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Beck(2017)]{doi:10.1137/1.9781611974997}
Amir Beck.
\newblock \emph{First-Order Methods in Optimization}.
\newblock Society for Industrial and Applied Mathematics, Philadelphia, PA,
  2017.

\bibitem[Chen et~al.(2020)Chen, Zheng, AL~Kontar, and Raskutti]{chen2020}
Hao Chen, Lili Zheng, Raed AL~Kontar, and Garvesh Raskutti.
\newblock Stochastic gradient descent in correlated settings: {A} study on
  {G}aussian processes.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, 2020.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{adagrad}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2121--2159,
  2011.

\bibitem[Fehrman et~al.(2020)Fehrman, Gess, and Jentzen]{feh2020}
Benjamin Fehrman, Benjamin Gess, and Arnulf Jentzen.
\newblock Convergence rates for the stochastic gradient descent method for
  non-convex objective functions.
\newblock \emph{Journal of Machine Learning Research}, 21:\penalty0 1--48,
  2020.

\bibitem[Ghadimi \& Lan(2012)Ghadimi and Lan]{gha2012}
Saeed Ghadimi and Guanghui Lan.
\newblock Optimal stochastic approximation algorithms for strongly convex
  stochastic composite optimization {I}: {A} generic algorithmic framework.
\newblock \emph{SIAM Journal on Optimization}, 22:\penalty0 1469--1492, 2012.

\bibitem[Ghadimi \& Lan(2013)Ghadimi and Lan]{gha2013}
Saeed Ghadimi and Guanghui Lan.
\newblock Optimal stochastic approximation algorithms for strongly convex
  stochastic composite optimization {II}: {S}hrinking procedures and optimal
  algorithms.
\newblock \emph{SIAM Journal on Optimization}, 23:\penalty0 2061--2089, 2013.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter]{NIPS2017_8a1d6947}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
  Hochreiter.
\newblock {GAN}s trained by a two time-scale update rule converge to a local
  nash equilibrium.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~30, 2017.

\bibitem[Iiduka(2022)]{iiduka2022critical}
Hideaki Iiduka.
\newblock Critical bach size minimizes stochastic first-order oracle complexity
  of deep learning optimizer using hyperparameters close to one.
\newblock arXiv: 2208.09814, 2022.

\bibitem[Jain et~al.(2018)Jain, Kakade, Kidambi, Netrapalli, and
  Sidford]{JMLR:v18:16-595}
Prateek Jain, Sham~M. Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron
  Sidford.
\newblock Parallelizing stochastic gradient descent for least squares
  regression: Mini-batching, averaging, and model misspecification.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (223):\penalty0 1--42, 2018.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{Proceedings of The International Conference on Learning
  Representations}, 2015.

\bibitem[Loizou et~al.(2021)Loizou, Vaswani, Laradji, and
  Lacoste-Julien]{loizou2021}
Nicolas Loizou, Sharan Vaswani, Issam Laradji, and Simon Lacoste-Julien.
\newblock Stochastic polyak step-size for {SGD}: {A}n adaptive learning rate
  for fast convergence.
\newblock In \emph{Proceedings of the 24th International Conference on
  Artificial Intelligence and Statistics}, volume 130, 2021.

\bibitem[Loshchilov \& Hutter(2019)Loshchilov and
  Hutter]{loshchilov2018decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{Proceedings of The International Conference on Learning
  Representations}, 2019.

\bibitem[Naganuma \& Iiduka(2023)Naganuma and Iiduka]{pmlr-v206-naganuma23a}
Hiroki Naganuma and Hideaki Iiduka.
\newblock Conjugate gradient method for generative adversarial networks.
\newblock In \emph{Proceedings of The 26th International Conference on
  Artificial Intelligence and Statistics}, volume 206 of \emph{Proceedings of
  Machine Learning Research}, pp.\  4381--4408. PMLR, 2023.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and
  Shapiro]{nem2009}
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM Journal on Optimization}, 19:\penalty0 1574--1609, 2009.

\bibitem[Nesterov(1983)]{nest1983}
Yurii Nesterov.
\newblock A method for unconstrained convex minimization problem with the rate
  of convergence ${O}(1/k^2)$.
\newblock \emph{Doklady AN USSR}, 269:\penalty0 543--547, 1983.

\bibitem[Nocedal \& Wright(2006)Nocedal and Wright]{noce}
J.~Nocedal and S.~J. Wright.
\newblock \emph{Numerical Optimization}.
\newblock Springer Series in Operations Research and Financial Engineering.
  Springer, New York, 2nd edition, 2006.

\bibitem[Polyak(1964)]{polyak1964}
Boris~T. Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics},
  4:\penalty0 1--17, 1964.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{reddi2018}
Sashank~J. Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of {A}dam and beyond.
\newblock In \emph{Proceedings of The International Conference on Learning
  Representations}, 2018.

\bibitem[Robbins \& Monro(1951)Robbins and Monro]{robb1951}
Herbert Robbins and Herbert Monro.
\newblock A stochastic approximation method.
\newblock \emph{The Annals of Mathematical Statistics}, 22:\penalty0 400--407,
  1951.

\bibitem[Sato \& Iiduka(2023)Sato and Iiduka]{pmlr-v202-sato23b}
Naoki Sato and Hideaki Iiduka.
\newblock Existence and estimation of critical batch size for training
  generative adversarial networks with two time-scale update rule.
\newblock In \emph{Proceedings of the 40th International Conference on Machine
  Learning}, volume 202 of \emph{Proceedings of Machine Learning Research},
  pp.\  30080--30104. PMLR, 23--29 Jul 2023.

\bibitem[Scaman \& Malherbe(2020)Scaman and Malherbe]{sca2020}
Kevin Scaman and C\'edric Malherbe.
\newblock Robustness analysis of non-convex stochastic gradient descent using
  biased expectations.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, 2020.

\bibitem[Shallue et~al.(2019)Shallue, Lee, Antognini, Sohl-Dickstein, Frostig,
  and Dahl]{shallue2019}
Christopher~J. Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein,
  Roy Frostig, and George~E. Dahl.
\newblock Measuring the effects of data parallelism on neural network training.
\newblock \emph{Journal of Machine Learning Research}, 20:\penalty0 1--49,
  2019.

\bibitem[Tieleman \& Hinton(2012)Tieleman and Hinton]{rmsprop}
Tijmen Tieleman and Geoffrey Hinton.
\newblock {RMSP}rop: {D}ivide the gradient by a running average of its recent
  magnitude.
\newblock \emph{{COURSERA}: {N}eural networks for machine learning},
  4:\penalty0 26--31, 2012.

\bibitem[Vaswani et~al.(2019)Vaswani, Mishkin, Laradji, Schmidt, Gidel, and
  Lacoste-Julien]{NEURIPS2019_2557911c}
Sharan Vaswani, Aaron Mishkin, Issam Laradji, Mark Schmidt, Gauthier Gidel, and
  Simon Lacoste-Julien.
\newblock Painless stochastic gradient: {I}nterpolation, line-search, and
  convergence rates.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, 2019.

\bibitem[Virmaux \& Scaman(2018)Virmaux and Scaman]{NEURIPS2018_d54e99a6}
Aladin Virmaux and Kevin Scaman.
\newblock Lipschitz regularity of deep neural networks: analysis and efficient
  estimation.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~31, 2018.

\bibitem[Zhang et~al.(2019)Zhang, Li, Nado, Martens, Sachdeva, Dahl, Shallue,
  and Grosse]{zhang2019}
Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva,
  George~E. Dahl, Christopher~J. Shallue, and Roger Grosse.
\newblock Which algorithmic choices matter at which batch sizes? {I}nsights
  from a noisy quadratic model.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, 2019.

\bibitem[Zinkevich(2003)]{zinkevich2003}
Martin Zinkevich.
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock In \emph{Proceedings of the 20th International Conference on Machine
  Learning}, pp.\  928--936, 2003.

\end{thebibliography}
