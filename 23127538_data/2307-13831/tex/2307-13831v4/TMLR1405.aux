\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\citation{robb1951,zinkevich2003,nem2009,gha2012,gha2013}
\citation{JMLR:v18:16-595,NEURIPS2019_2557911c,feh2020,chen2020,sca2020,loizou2021}
\citation{polyak1964,nest1983}
\citation{adagrad}
\citation{rmsprop}
\citation{adam}
\citation{reddi2018}
\citation{loshchilov2018decoupled}
\citation{NIPS2017_8a1d6947,pmlr-v206-naganuma23a,pmlr-v202-sato23b}
\citation{shallue2019}
\citation{zhang2019}
\citation{iiduka2022critical,pmlr-v202-sato23b}
\citation{shallue2019}
\citation{zhang2019}
\HyPL@Entry{0<</S/D>>}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Background}{1}{subsection.1.1}\protected@file@percent }
\newlabel{subsec:1.1}{{1.1}{1}{Background}{subsection.1.1}{}}
\citation{NEURIPS2019_2557911c}
\citation{noce}
\citation{NEURIPS2019_2557911c}
\citation{NEURIPS2019_2557911c}
\citation{NEURIPS2019_2557911c}
\citation{NEURIPS2019_2557911c}
\citation{shallue2019,zhang2019,iiduka2022critical,pmlr-v202-sato23b}
\citation{iiduka2022critical,pmlr-v202-sato23b}
\citation{iiduka2022critical,pmlr-v202-sato23b}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Motivation}{2}{subsection.1.2}\protected@file@percent }
\citation{sca2020}
\citation{NEURIPS2018_d54e99a6}
\citation{shallue2019,zhang2019,iiduka2022critical,pmlr-v202-sato23b}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Contribution}{3}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Convergence analysis of SGD with Armijo-line-search learning rates}{3}{subsubsection.1.3.1}\protected@file@percent }
\newlabel{subsec:1.3.1}{{1.3.1}{3}{Convergence analysis of SGD with Armijo-line-search learning rates}{subsubsection.1.3.1}{}}
\newlabel{main}{{1}{3}{Convergence analysis of SGD with Armijo-line-search learning rates}{equation.1.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Relationship between batch size $b$ and number of steps $K$ to achieve an $\epsilon $--approximation defined by $\qopname  \relax m{min}_{k\in [0:K-1]} \mathbb  {E}[\delimiter "026B30D  \nabla f (\bm  {\theta }_k)\delimiter "026B30D ^2 ] \leq \frac  {C_1}{K} + \frac  {C_2}{b} = \epsilon ^2$ for SGD with a constant learning rate $\alpha \in (0,\frac  {2}{L_n})$ and for SGD with the Armijo-line-search learning rate \leavevmode {\color  {red}$\alpha _k \in [\relax \unhbox \voidb@x \hbox {}$\@@underline {\hbox {\alpha }}\mathsurround \z@ $\hbox {}\relax , \overline  {\alpha }]$} ($[0:K-1] := \{0,1,\ldots  ,K-1\}$, $f := \frac  {1}{n} \DOTSB \sum@ \slimits@ _{i\in [n]} f_i$ is bounded below by $f_*$, $L_i$ is the Lipschitz constant of $\nabla f_i$, $L_n := \frac  {1}{n} \DOTSB \sum@ \slimits@ _{i\in [n]} L_i$, and $\sigma ^2$ is the upper bound of the variance of the stochastic gradient)}}{3}{table.1}\protected@file@percent }
\newlabel{table:1}{{1}{3}{Relationship between batch size $b$ and number of steps $K$ to achieve an $\epsilon $--approximation defined by $\min _{k\in [0:K-1]} \mathbb {E}[\| \nabla f (\bm {\theta }_k)\|^2 ] \leq \frac {C_1}{K} + \frac {C_2}{b} = \epsilon ^2$ for SGD with a constant learning rate $\alpha \in (0,\frac {2}{L_n})$ and for SGD with the Armijo-line-search learning rate \textcolor {red}{$\alpha _k \in [\underline {\alpha }, \overline {\alpha }]$} ($[0:K-1] := \{0,1,\ldots ,K-1\}$, $f := \frac {1}{n} \sum _{i\in [n]} f_i$ is bounded below by $f_*$, $L_i$ is the Lipschitz constant of $\nabla f_i$, $L_n := \frac {1}{n} \sum _{i\in [n]} L_i$, and $\sigma ^2$ is the upper bound of the variance of the stochastic gradient)}{table.1}{}}
\citation{NEURIPS2019_2557911c}
\citation{NEURIPS2019_2557911c}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}Steps needed for $\epsilon $--approximation of SGD with Armijo line-search-learning rates}{4}{subsubsection.1.3.2}\protected@file@percent }
\newlabel{subsec:1.3.2}{{1.3.2}{4}{Steps needed for $\epsilon $--approximation of SGD with Armijo line-search-learning rates}{subsubsection.1.3.2}{}}
\newlabel{f_K}{{2}{4}{Steps needed for $\epsilon $--approximation of SGD with Armijo line-search-learning rates}{equation.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3}Critical batch size minimizing SFO complexity of SGD with Armijo-line-search learning rates}{4}{subsubsection.1.3.3}\protected@file@percent }
\newlabel{subsec:1.3.3}{{1.3.3}{4}{Critical batch size minimizing SFO complexity of SGD with Armijo-line-search learning rates}{subsubsection.1.3.3}{}}
\newlabel{f_N}{{3}{4}{Critical batch size minimizing SFO complexity of SGD with Armijo-line-search learning rates}{equation.1.3}{}}
\newlabel{CBS}{{4}{4}{Critical batch size minimizing SFO complexity of SGD with Armijo-line-search learning rates}{equation.1.4}{}}
\newlabel{comparison_C}{{5}{4}{Critical batch size minimizing SFO complexity of SGD with Armijo-line-search learning rates}{equation.1.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.4}Numerical results supporting our theoretical results}{4}{subsubsection.1.3.4}\protected@file@percent }
\citation{doi:10.1137/1.9781611974997}
\@writefile{toc}{\contentsline {section}{\numberline {2}Mathematical Preliminaries}{5}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Definitions}{5}{subsection.2.1}\protected@file@percent }
\newlabel{subsec:2.1}{{2.1}{5}{Definitions}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Assumptions and problem}{5}{subsection.2.2}\protected@file@percent }
\@writefile{thm}{\contentsline {prob}{{Problem}{2.{1}}{}}{5}{prob.2.1}\protected@file@percent }
\newlabel{prob:1}{{2.{1}}{5}{Assumptions and problem}{prob.2.1}{}}
\@writefile{thm}{\contentsline {assum}{{Assumption}{2.{1}}{}}{5}{assum.2.1}\protected@file@percent }
\newlabel{assum:1}{{2.{1}}{5}{Assumptions and problem}{assum.2.1}{}}
\newlabel{gradient}{{6}{5}{Assumptions and problem}{Item.1}{}}
\citation{noce}
\citation{NEURIPS2019_2557911c}
\citation{NEURIPS2019_2557911c}
\newlabel{sigma}{{7}{6}{Assumptions and problem}{Item.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Stochastic gradient descent using Armijo line search}{6}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Armijo condition}{6}{subsubsection.2.3.1}\protected@file@percent }
\newlabel{subsec:2.3.1}{{2.3.1}{6}{Armijo condition}{subsubsection.2.3.1}{}}
\newlabel{iterative}{{8}{6}{Armijo condition}{equation.2.8}{}}
\newlabel{optimal_ss}{{9}{6}{Armijo condition}{equation.2.9}{}}
\newlabel{armijo_0}{{10}{6}{Armijo condition}{equation.2.10}{}}
\@writefile{thm}{\contentsline {prop}{{Proposition}{2.{1}}{}}{6}{prop.2.1}\protected@file@percent }
\newlabel{existence_armijo}{{2.{1}}{6}{Armijo condition}{prop.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Stochastic gradient descent under Armijo condition}{6}{subsubsection.2.3.2}\protected@file@percent }
\newlabel{armijo}{{11}{6}{Stochastic gradient descent under Armijo condition}{equation.2.11}{}}
\citation{noce}
\citation{NEURIPS2019_2557911c}
\citation{noce}
\citation{NEURIPS2019_2557911c}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Stochastic gradient descent using Armijo line search}}{7}{algorithm.1}\protected@file@percent }
\newlabel{algo:1}{{1}{7}{Stochastic gradient descent under Armijo condition}{algorithm.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Backtracking Armijo-line-search method \citep  [Algorithm 3.1]{noce}}}{7}{algorithm.2}\protected@file@percent }
\newlabel{algo:2}{{2}{7}{Stochastic gradient descent under Armijo condition}{algorithm.2}{}}
\@writefile{thm}{\contentsline {lem}{{Lemma}{2.{1}}{}}{7}{lem.2.1}\protected@file@percent }
\newlabel{lem:1}{{2.{1}}{7}{Stochastic gradient descent under Armijo condition}{lem.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Analysis of SGD using Armijo Line Search}{7}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Convergence analysis of Algorithm \ref  {algo:1}}{7}{subsection.3.1}\protected@file@percent }
\newlabel{subsec:3.1}{{3.1}{7}{Convergence analysis of Algorithm \ref {algo:1}}{subsection.3.1}{}}
\@writefile{thm}{\contentsline {thm}{{Theorem}{3.{1}}{Upper bound of the squared norm of the full gradient}}{7}{thm.3.1}\protected@file@percent }
\newlabel{thm:1}{{3.{1}}{7}{Convergence analysis of Algorithm \ref {algo:1}}{thm.3.1}{}}
\citation{NEURIPS2018_d54e99a6}
\citation{NEURIPS2019_2557911c}
\citation{NEURIPS2019_2557911c}
\newlabel{sgd_c}{{12}{8}{Convergence analysis of Algorithm \ref {algo:1}}{equation.3.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Steps needed for $\epsilon $--approximation}{9}{subsection.3.2}\protected@file@percent }
\newlabel{epsilon}{{13}{9}{Steps needed for $\epsilon $--approximation}{equation.3.13}{}}
\@writefile{thm}{\contentsline {thm}{{Theorem}{3.{2}}{Steps needed for nonconvex optimization of SGD using Armijo line search}}{9}{thm.3.2}\protected@file@percent }
\newlabel{thm:2}{{3.{2}}{9}{Steps needed for $\epsilon $--approximation}{thm.3.2}{}}
\newlabel{K}{{14}{9}{Steps needed for $\epsilon $--approximation}{equation.3.14}{}}
\newlabel{condition_upper}{{15}{9}{Steps needed for $\epsilon $--approximation}{equation.3.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Critical batch size minimizing SFO complexity}{9}{subsection.3.3}\protected@file@percent }
\newlabel{sec:3.3}{{3.3}{9}{Critical batch size minimizing SFO complexity}{subsection.3.3}{}}
\@writefile{thm}{\contentsline {thm}{{Theorem}{3.{3}}{Existence of critical batch size for SGD using Armijo line search}}{9}{thm.3.3}\protected@file@percent }
\newlabel{thm:3}{{3.{3}}{9}{Critical batch size minimizing SFO complexity}{thm.3.3}{}}
\newlabel{SFOC}{{16}{9}{Critical batch size minimizing SFO complexity}{equation.3.16}{}}
\newlabel{cbs}{{17}{9}{Critical batch size minimizing SFO complexity}{Item.3}{}}
\citation{NEURIPS2019_2557911c}
\citation{shallue2019,zhang2019,iiduka2022critical}
\newlabel{ucbs}{{18}{10}{Critical batch size minimizing SFO complexity}{Item.4}{}}
\newlabel{comp_1}{{19}{10}{Critical batch size minimizing SFO complexity}{equation.3.19}{}}
\newlabel{comp_2}{{20}{10}{Critical batch size minimizing SFO complexity}{equation.3.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Numerical Results}{10}{section.4}\protected@file@percent }
\newlabel{sec:4}{{4}{10}{Numerical Results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Training ResNet and MLP on the CIFAR-10, CIFAR-100, and MNIST datasets}{11}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Number of steps for Algorithm \ref  {algo:1} versus batch size needed to train ResNet-34 on CIFAR-10}}{11}{figure.1}\protected@file@percent }
\newlabel{fig1}{{1}{11}{Number of steps for Algorithm \ref {algo:1} versus batch size needed to train ResNet-34 on CIFAR-10}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces SFO complexity for Algorithm \ref  {algo:1} versus batch size needed to train ResNet-34 on CIFAR-10 (The double-circle symbol denotes the measured critical batch size)}}{11}{figure.2}\protected@file@percent }
\newlabel{fig2}{{2}{11}{SFO complexity for Algorithm \ref {algo:1} versus batch size needed to train ResNet-34 on CIFAR-10 (The double-circle symbol denotes the measured critical batch size)}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Number of steps for Algorithm \ref  {algo:1} with $c = 0.20$ and variants of SGD versus batch size needed to train ResNet-34 on CIFAR-10}}{11}{figure.3}\protected@file@percent }
\newlabel{fig1_1}{{3}{11}{Number of steps for Algorithm \ref {algo:1} with $c = 0.20$ and variants of SGD versus batch size needed to train ResNet-34 on CIFAR-10}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces SFO complexity for Algorithm \ref  {algo:1} with $c = 0.20$ and variants of SGD versus batch size needed to train ResNet-34 on CIFAR-10}}{11}{figure.4}\protected@file@percent }
\newlabel{fig2_1}{{4}{11}{SFO complexity for Algorithm \ref {algo:1} with $c = 0.20$ and variants of SGD versus batch size needed to train ResNet-34 on CIFAR-10}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Number of steps for Algorithm \ref  {algo:1} versus batch size needed to train ResNet-34 on CIFAR-100}}{12}{figure.5}\protected@file@percent }
\newlabel{fig3}{{5}{12}{Number of steps for Algorithm \ref {algo:1} versus batch size needed to train ResNet-34 on CIFAR-100}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces SFO complexity for Algorithm \ref  {algo:1} versus batch size needed to train ResNet-34 on CIFAR-100 (The double-circle symbol denotes the measured critical batch size)}}{12}{figure.6}\protected@file@percent }
\newlabel{fig4}{{6}{12}{SFO complexity for Algorithm \ref {algo:1} versus batch size needed to train ResNet-34 on CIFAR-100 (The double-circle symbol denotes the measured critical batch size)}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Number of steps for Algorithm \ref  {algo:1} with $c = 0.25$ and variants of SGD versus batch size needed to train ResNet-34 on CIFAR-100}}{12}{figure.7}\protected@file@percent }
\newlabel{fig3_1}{{7}{12}{Number of steps for Algorithm \ref {algo:1} with $c = 0.25$ and variants of SGD versus batch size needed to train ResNet-34 on CIFAR-100}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces SFO complexity for Algorithm \ref  {algo:1} with $c = 0.25$ and variants of SGD versus batch size needed to train ResNet-34 on CIFAR-100}}{12}{figure.8}\protected@file@percent }
\newlabel{fig4_1}{{8}{12}{SFO complexity for Algorithm \ref {algo:1} with $c = 0.25$ and variants of SGD versus batch size needed to train ResNet-34 on CIFAR-100}{figure.8}{}}
\citation{zagoruyko2017wide}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Number of steps for Algorithm \ref  {algo:1} versus batch size needed to train MLP on MNIST}}{13}{figure.9}\protected@file@percent }
\newlabel{fig5}{{9}{13}{Number of steps for Algorithm \ref {algo:1} versus batch size needed to train MLP on MNIST}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces SFO complexity for Algorithm \ref  {algo:1} versus batch size needed to train MLP on MNIST (The double-circle symbol denotes the measured critical batch size)}}{13}{figure.10}\protected@file@percent }
\newlabel{fig6}{{10}{13}{SFO complexity for Algorithm \ref {algo:1} versus batch size needed to train MLP on MNIST (The double-circle symbol denotes the measured critical batch size)}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Number of steps for Algorithm \ref  {algo:1} with $c = 0.05$ and variants of SGD versus batch size needed to train MLP on MNIST}}{13}{figure.11}\protected@file@percent }
\newlabel{fig5_1}{{11}{13}{Number of steps for Algorithm \ref {algo:1} with $c = 0.05$ and variants of SGD versus batch size needed to train MLP on MNIST}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces SFO complexity for Algorithm \ref  {algo:1} with $c = 0.05$ and variants of SGD versus batch size needed to train MLP on MNIST}}{13}{figure.12}\protected@file@percent }
\newlabel{fig6_1}{{12}{13}{SFO complexity for Algorithm \ref {algo:1} with $c = 0.05$ and variants of SGD versus batch size needed to train MLP on MNIST}{figure.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Number of steps for Algorithm \ref  {algo:1} versus batch size needed to train Wide ResNet-50-2 on CIFAR-10}}{13}{figure.13}\protected@file@percent }
\newlabel{fig13}{{13}{13}{Number of steps for Algorithm \ref {algo:1} versus batch size needed to train Wide ResNet-50-2 on CIFAR-10}{figure.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces SFO complexity for Algorithm \ref  {algo:1} versus batch size needed to train Wide ResNet-50-2 on CIFAR-10 (The double-circle symbol denotes the measured critical batch size)}}{13}{figure.14}\protected@file@percent }
\newlabel{fig14}{{14}{13}{SFO complexity for Algorithm \ref {algo:1} versus batch size needed to train Wide ResNet-50-2 on CIFAR-10 (The double-circle symbol denotes the measured critical batch size)}{figure.14}{}}
\citation{iiduka2022critical}
\citation{pmlr-v202-sato23b}
\citation{NEURIPS2019_2557911c}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Number of steps for Algorithm \ref  {algo:1} with $c = 3 \times 10^{-4}$ and variants of SGD versus batch size needed to train Wide ResNet-50-2 on CIFAR-10}}{14}{figure.15}\protected@file@percent }
\newlabel{fig15}{{15}{14}{Number of steps for Algorithm \ref {algo:1} with $c = 3 \times 10^{-4}$ and variants of SGD versus batch size needed to train Wide ResNet-50-2 on CIFAR-10}{figure.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces SFO complexity for Algorithm \ref  {algo:1} with $c = 3 \times 10^{-4}$ and variants of SGD versus batch size needed to train Wide ResNet-50-2 on CIFAR-10}}{14}{figure.16}\protected@file@percent }
\newlabel{fig16}{{16}{14}{SFO complexity for Algorithm \ref {algo:1} with $c = 3 \times 10^{-4}$ and variants of SGD versus batch size needed to train Wide ResNet-50-2 on CIFAR-10}{figure.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Estimation of critical batch size}{14}{subsection.4.2}\protected@file@percent }
\bibdata{biblio}
\bibcite{doi:10.1137/1.9781611974997}{{1}{2017}{{Beck}}{{}}}
\bibcite{chen2020}{{2}{2020}{{Chen et~al.}}{{Chen, Zheng, AL~Kontar, and Raskutti}}}
\bibcite{adagrad}{{3}{2011}{{Duchi et~al.}}{{Duchi, Hazan, and Singer}}}
\bibcite{feh2020}{{4}{2020}{{Fehrman et~al.}}{{Fehrman, Gess, and Jentzen}}}
\bibcite{gha2012}{{5}{2012}{{Ghadimi \& Lan}}{{Ghadimi and Lan}}}
\bibcite{gha2013}{{6}{2013}{{Ghadimi \& Lan}}{{Ghadimi and Lan}}}
\bibcite{NIPS2017_8a1d6947}{{7}{2017}{{Heusel et~al.}}{{Heusel, Ramsauer, Unterthiner, Nessler, and Hochreiter}}}
\bibcite{iiduka2022critical}{{8}{2022}{{Iiduka}}{{}}}
\bibcite{JMLR:v18:16-595}{{9}{2018}{{Jain et~al.}}{{Jain, Kakade, Kidambi, Netrapalli, and Sidford}}}
\bibcite{adam}{{10}{2015}{{Kingma \& Ba}}{{Kingma and Ba}}}
\bibcite{loizou2021}{{11}{2021}{{Loizou et~al.}}{{Loizou, Vaswani, Laradji, and Lacoste-Julien}}}
\bibcite{loshchilov2018decoupled}{{12}{2019}{{Loshchilov \& Hutter}}{{Loshchilov and Hutter}}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{15}{section.5}\protected@file@percent }
\bibcite{pmlr-v206-naganuma23a}{{13}{2023}{{Naganuma \& Iiduka}}{{Naganuma and Iiduka}}}
\bibcite{nem2009}{{14}{2009}{{Nemirovski et~al.}}{{Nemirovski, Juditsky, Lan, and Shapiro}}}
\bibcite{nest1983}{{15}{1983}{{Nesterov}}{{}}}
\bibcite{noce}{{16}{2006}{{Nocedal \& Wright}}{{Nocedal and Wright}}}
\bibcite{polyak1964}{{17}{1964}{{Polyak}}{{}}}
\bibcite{reddi2018}{{18}{2018}{{Reddi et~al.}}{{Reddi, Kale, and Kumar}}}
\bibcite{robb1951}{{19}{1951}{{Robbins \& Monro}}{{Robbins and Monro}}}
\bibcite{pmlr-v202-sato23b}{{20}{2023}{{Sato \& Iiduka}}{{Sato and Iiduka}}}
\bibcite{sca2020}{{21}{2020}{{Scaman \& Malherbe}}{{Scaman and Malherbe}}}
\bibcite{shallue2019}{{22}{2019}{{Shallue et~al.}}{{Shallue, Lee, Antognini, Sohl-Dickstein, Frostig, and Dahl}}}
\bibcite{rmsprop}{{23}{2012}{{Tieleman \& Hinton}}{{Tieleman and Hinton}}}
\bibcite{NEURIPS2019_2557911c}{{24}{2019}{{Vaswani et~al.}}{{Vaswani, Mishkin, Laradji, Schmidt, Gidel, and Lacoste-Julien}}}
\bibcite{NEURIPS2018_d54e99a6}{{25}{2018}{{Virmaux \& Scaman}}{{Virmaux and Scaman}}}
\bibcite{zagoruyko2017wide}{{26}{2017}{{Zagoruyko \& Komodakis}}{{Zagoruyko and Komodakis}}}
\bibcite{zhang2019}{{27}{2019}{{Zhang et~al.}}{{Zhang, Li, Nado, Martens, Sachdeva, Dahl, Shallue, and Grosse}}}
\bibcite{zinkevich2003}{{28}{2003}{{Zinkevich}}{{}}}
\bibstyle{tmlr}
\citation{NEURIPS2019_2557911c}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{17}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Proof of lemma \ref  {lem:1}}{17}{subsection.A.1}\protected@file@percent }
\newlabel{a1}{{A.1}{17}{Proof of lemma \ref {lem:1}}{subsection.A.1}{}}
\newlabel{vas}{{21}{17}{Proof of lemma \ref {lem:1}}{equation.A.21}{}}
\newlabel{not_vas}{{22}{17}{Proof of lemma \ref {lem:1}}{equation.A.22}{}}
\newlabel{ineq:1}{{23}{17}{Proof of lemma \ref {lem:1}}{equation.A.23}{}}
\newlabel{ineq:2}{{24}{17}{Proof of lemma \ref {lem:1}}{equation.A.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Proof of Theorem \ref  {thm:1}}{18}{subsection.A.2}\protected@file@percent }
\newlabel{a2}{{A.2}{18}{Proof of Theorem \ref {thm:1}}{subsection.A.2}{}}
\newlabel{ineq_main}{{25}{18}{Proof of Theorem \ref {thm:1}}{equation.A.25}{}}
\newlabel{ineq_main_2}{{26}{18}{Proof of Theorem \ref {thm:1}}{equation.A.26}{}}
\newlabel{e_1}{{27}{18}{Proof of Theorem \ref {thm:1}}{equation.A.27}{}}
\newlabel{e_2}{{28}{18}{Proof of Theorem \ref {thm:1}}{equation.A.28}{}}
\newlabel{ineq_main_3}{{29}{19}{Proof of Theorem \ref {thm:1}}{equation.A.29}{}}
\newlabel{total}{{30}{19}{Proof of Theorem \ref {thm:1}}{equation.A.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Proof of Theorem \ref  {thm:2}}{19}{subsection.A.3}\protected@file@percent }
\newlabel{a3}{{A.3}{19}{Proof of Theorem \ref {thm:2}}{subsection.A.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Proof of Theorem \ref  {thm:3}}{19}{subsection.A.4}\protected@file@percent }
\newlabel{a4}{{A.4}{19}{Proof of Theorem \ref {thm:3}}{subsection.A.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.5}Proof of (\ref  {sgd_c})}{20}{subsection.A.5}\protected@file@percent }
\newlabel{a5}{{A.5}{20}{Proof of (\ref {sgd_c})}{subsection.A.5}{}}
