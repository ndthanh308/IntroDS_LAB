\begin{CJK*}{UTF8}{gkai}
\begin{table}[h]
\small
\caption{\label{tokenizer-comparison} The outputs of original LLaMA and Chinese Tokenizer. This example is from~\citet{cui2023efficient}.}
\begin{center}
\begin{tabular}{l}
\toprule
\makecell[lt]{\textbf{Inputs:} 人工智能是计算机科学、心理学、哲学等\\学科融合的交叉学科。} \\
\midrule
\makecell[lt]{\textbf{LLaMA:} \_, 人, 工, 智, 能, 是, 计, 算, 机, 科, 学, 、, 心, \\ 理, 学, 、, 0xE5, 0x93, 0xB2, 学, 等, 学, 科, 0xE8, \\ 0x9E, 0x8D, 合, 的, 交, 0xE5, 0x8F, 0x89, 学, 科, 。} \\
\midrule
\makecell[lt]{\textbf{Chinese:} \_, 人工智能, 是, 计算机, 科学, 、, 心理学, 、\\, 哲学, 等, 学科, 融合, 的, 交叉, 学科, 。} \\
\bottomrule
\end{tabular}
\end{center}
\end{table}
\end{CJK*}

\subsection{Training Language-Specific LLMs}
\label{otherlanguageLLMs}
Existing LLMs described above are mostly English-oriented. Thus, it becomes necessary to adapt the superior linguistic ability to other languages.~\citet{DBLP:journals/corr/abs-2304-07854,cui2023efficient} demonstrate existing English-dominated LLaMA has less than 1,000 Chinese characters in its vocabulary and LLaMA has to represent Chinese characters using the byte-based fallback strategy, which significantly increases input length and decreases the inference efficiency. As shown in Table~\ref{tokenizer-comparison}, compared to the default LLaMA tokenizer, the specialized Chinese tokenizer trained using large-scale Chinese corpus can produce more compact and semantically meaningful token representations (e.g., long and complex Chinese phrases). To leverage the linguistic knowledge in orginal LLaMA, ~\citet{cui2023efficient} propose a two-stage Chinese pre-training solution to enable LLaMA to better understand Chinese inputs. Before training they first add 20K Chinese words and phrases into the existing LLaMA vocabulary. In the first stage, they only train the input word embeddings and keep the rest parameters in LLaMA frozen. In the second stage, to save training resources, they add LoRA parameters and jointly train the parameters in the input word embeddings, self-attentive heads and LoRA parameters.~\citet{DBLP:journals/corr/abs-2304-07854} also report the benefits of such strategy under a GPT-4 evaluation framework.