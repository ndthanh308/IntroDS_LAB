\section{Alignment Data Collection}
\label{instructioncollecting}
Aligning LLMs with human expectations necessitates the collection of high-quality training data that authentically reflects human needs and expectations. For the purposes of this survey, we conceptualize an instruction as $I_k = (x_k, y_k)$, where $x_k$ denotes the instruction input and $y_k$ denotes the corresponding response. This data can be derived from an array of sources, encompassing both human-generated instructions and those generated by strong LLMs.
In this section, we summarize these methods of instruction generation and effective strategies for constructing a composite of diverse training instructions.

% previous researchers collect instructions from these sources and discuss how to effectively mix these instructions training.

\subsection{Instructions from Human}
\label{instructionfromhuman}
Human-provided instructions mainly originate from two main sources: pre-existing human-annotated NLP benchmarks and meticulously hand-crafted instructions. 

\subsubsection{NLP Benchmarks}
An intuitive starting point for data collection involves adapting existing NLP benchmarks into natural language instructions. For instance, Figure~\ref{fig:nlpinstruction} offers an example drawn from the Natural Language Inference task. Works such as PromptSource~\cite{bach-etal-2022-promptsource}, FLAN~\cite{wei2022finetuned,longpre2023flan}, and SuperNaturalInstruction~\cite{wang-etal-2022-super,mishra-etal-2022-cross} are at the forefront of this approach. 
% Figure environment removed
These benchmarks represent a substantial array of \emph{diverse and heterogeneous} NLP tasks, such as dialogue, reasoning tasks and coding tasks, unified under the framework of language instructions. In each NLP benchmark, they engage annotators to craft several natural language templates that smoothly integrate all input data into a sequential text. The objective is to enhance LLMs' capability for multi-task learning across training tasks and foster generalization for unseen tasks. OIG~\cite{OIG} also combines instructions from FLAN-like NLP benchmarks with other types of open-ended instructions, such as how-to, maths and coding instructions. Concurrently, \citet{DBLP:journals/corr/abs-2212-09689} put forth the concept of \emph{Unnatural Instructions}, utilizing LLMs to generate new templates or instances bearing resemblance to the original instructions but with notable variances. Interestingly, the authors discovered that \emph{text-davinci-002} outperforms GPT-3 in responding to these generated instructions, given that GPT-3 often devolved into repetitive or tangential outputs after providing the correct answer. This model of instruction creation is highly scalable and can yield millions of instructions effectively. Further, \citet{wang2023far} demonstrated that FLAN-style instructions considerably enhanced the reasoning capabilities of aligned LLMs.



% One straightforward source is to adapt existing NLP benchmarks into natural language instructions. Figure~\ref{fig:nlpinstruction} shows an example of the Natural Language Inference task. PromptSource~\cite{bach-etal-2022-promptsource}, FLAN~\cite{wei2022finetuned,longpre2023flan} and SuperNaturalInstruction~\cite{wang-etal-2022-super,mishra-etal-2022-cross}  are pioneering research efforts in this direction. For each NLP benchmark, they ask annotators to write multiple natural language templates that could incorporate all inputs information into a sequence of coherent text.  These benchmarks 
% represent large number of \emph{diverse and heterogeneous} NLP tasks as unified language instructions and their goal is to train LLMs to perform better multi-task learning on training tasks and generalization on unseen tasks.~\citet{DBLP:journals/corr/abs-2212-09689} propose \emph{Unnatural Instructions} which leverages LLMs to generate new templates or similar but different instances based on those constructed instructions. Interestingly, the authors find that \emph{text-davinci-002} works better than GPT-3 when answering these generated instructions because GPT-3 often degenerates into repetitions or tangents after producing the correct answer. This paradigm is easy to scale up and can effectively produce millions of instructions. \citet{wang2023far} also find that FLAN-style instructions can effectively improve the reasoning ability of aligned LLMs. 
% However, as many NLP datasets focus on a small and specific skill set, these instructions could be relatively narrow and can not be well suited for complicated needs for real-world applications (e.g., chatting).

\subsubsection{Hand-crafted Instructions}
Constructing instructions from NLP benchmarks could be effective and painless. However, as many NLP datasets focus on a small and specific skill set, which means the resultant instructions are also relatively narrow in scope. Consequently, they may fall short in catering to the complex needs of real-world applications, such as engaging in dynamic human conversation.

To combat the above issues, it is possible to construct instructions via intentional manual annotations. How to effectively design a human-in-the-loop annotation framework becomes the key issue. The Databricks company collects a 15k crowd-sourcing instruction dataset \emph{databricks-dolly-15k}~\cite{DatabricksBlog2023DollyV2} from its employees. 
Those people are instructed to create prompt / response pairs in each of eight different instruction categories, including the seven outlined in~\citet{DBLP:conf/nips/Ouyang0JAWMZASR22}, as well as an open-ended free-form category. Importantly, they are \emph{explicitly} instructed not to use external web information, as well as outputs from generative AI systems.
~\citet{Kopf2023OpenAssistantC} construct the \emph{OpenAssistant} corpus with over 10,000 dialogues using more than 13,000 international annotators. The annotation process includes a) writing initial prompts for dialogue; b) replying as an assistant or user; c) ranking dialogue quality to explicitly provide human preferences. As a result, this corpus can be used for SFT and human preference alignment training for LLMs.
~\citet{Zhang2023ChineseOI} construct high-quality Chinese instructions from existing English instruction datasets. They first translate the English instructions into Chinese, then verify whether these translations are usable. Finally, they hire annotators to 
correct and re-organize the instructions into the {task description, input, output} format in the selected corpus. ShareGPT~\footnote{\url{https://sharegpt.com/}}, which is collected by~\citet{vicuna2023}, is an interesting exploration for crowd-sourcing human-written instructions. It is a website that encourages users to upload and share their interesting ChatGPT/GPT4 conversations. Such a mechanism can effectively collect a large number of diverse and human-written instructions that likely trigger high-quality ChatGPT/GPT4 responses. Popular online QA websites, such as Stack Overflow~\footnote{\url{https://stackoverflow.com/}}, Quora~\footnote{\url{https://www.quora.com/}} and Zhihu~\footnote{\url{https://www.zhihu.com/}}, and large user-generated content databases, such as Wikipedia~\footnote{\url{https://en.wikipedia.org/}}, are all reliable sources to provide high-quality human-written prompts for this purpose.Both ~\citet{ding2023enhancing} and~\citet{DBLP:journals/corr/abs-2304-01196} propose to use these resources as the seed instructions to prompt GPT-3.5 to generate high-quality synthetic multi-turn dialogues.

\subsection{Instructions From Strong LLMs}
\label{llminstruction}
With the emergence of strong closed-source LLMs (e.g., ChatGPT/GPT4), 
it is also feasible to automate the collection process to obtain various types of synthetic instructions (e.g., single-turn, multi-turn, and multilingual instructions) by providing appropriate prompts to these LLMs. The main challenge is how to effectively prompt LLMs to generate diverse and high-quality instructions.   

% Figure environment removed


\subsubsection{Self-Instruction}

\emph{Self-Instruct}~\cite{DBLP:journals/corr/abs-2212-10560} were among the pioneers to automate the instruction collection process.
It employed the in-context learning capability of ChatGPT to generate large-scale instructions from a pre-defined set of human-annotated instructions covering diverse topics and task types, as illustrated in Figure~\ref{fig:selfinstruct}. %Operating within the in-context learning framework, \emph{Self-Instruct} prompts ChatGPT to create new task descriptions and corresponding instructions, building upon a pre-defined set of instruction.
%\citet{DBLP:journals/corr/abs-2212-10560} were the pioneers to introduce the concept of \emph{Self-Instruct}, an innovative method for automatically generating large-scale instructions from a modest pool of human-annotated instructions via the application of ChatGPT. \emph{Self-Instruct} prompts ChatGPT to devise new instructions, as illustrated in Figure~\ref{fig:selfinstruct}. Operating within the in-context learning framework, \emph{Self-Instruct} prompts ChatGPT to create new task descriptions and corresponding instructions, building upon pre-existing instruction sets.
The automatically generated instructions are followed by a quality control filtering process, and this iterative process continues until the desired data volume has been achieved. Interestingly, the researchers discovered that GPT-3~\cite{NEURIPS2020_1457c0d6}, fine-tuned with these instructions, performed better than models fine-tuned using instructions derived from NLP benchmarks SuperNI benchmark~\cite{wang-etal-2022-super} and \emph{User-Oriented Instructions}, as discussed in Section~\ref{instructionfromhuman}).
Several follow-up attempts, such as Aplaca~\cite{alpaca} and its variants~\cite{chinese-llama-alpaca} follow this 
\emph{Self-Instruct} framework.
More subsequent research efforts w.r.t. enhancing instruction diversity, quality, and complexity will be elaborated as follows.
%Following this framework, subsequent research efforts primarily concentrate on enhancing instruction diversity, quality, and complexity.

\iffalse
\citet{DBLP:journals/corr/abs-2212-10560} first propose \emph{Self-Instruct} which automatically collects large-scale instructions from a small set of human-annotated instructions via ChatGPT. \emph{Self-Instruct} prompts ChatGPT to generate new instructions. As shown in Figure~\ref{fig:selfinstruct}, \emph{Self-Instruct}  prompts ChatGPT to generate new tasks description and corresponding instructions based on the pre-existing instructions sets under the in-context learning framework. After quality control filtering, the newly generated instructions are added to the instruction set. This process iterates until the target data volume is reached. They find that GPT-3~\cite{NEURIPS2020_1457c0d6} fine-tuned on these instructions successfully outperforms the ones fine-tuned by 
the NLP benchmarks instructions (i.e., discussed in Section~\ref{nlpbenchmarkinstruction}) on the SuperNI benchmark~\cite{wang-etal-2022-super} and \emph{User-Orientated Instructions}. 
Following this framework, afterward research efforts mainly focus on improving the instruction input or output quality.
\fi


\paragraph{Improving Input Quality}
One limitation of the synthetic instructions from strong LLMs often suffer from diversity issues. For example,~\citet{jentzsch2023chatgpt} find that when prompting to generate jokes, ChatGPT only produces 25 unique joke patterns in thousands of samples. 
To improve the instruction input diversity,~\citet{DBLP:journals/corr/abs-2212-10560} propose different input and output generation strategies for different types of instructions. 
They first prompt ChatGPT to classify generated instruction into \emph{classification tasks} or \emph{non-classification tasks}. Then, they deploy output-first and input-first strategies for \emph{classification tasks} and \emph{non-classification tasks}, respectively.
Others propose to add various external information into the input prompts to enhance diversity and factuality, including Wikipedia Category Keywords~\cite{DBLP:journals/corr/abs-2304-1440}, user-generated questions on the Internet (e.g., Quora, StackOverflow)~\cite{DBLP:journals/corr/abs-2304-01196,gpt4all} and instructions from the SuperNaturalInstruction benchmark~\cite{DBLP:journals/corr/abs-2212-09689}.
\citet{yu2023large} also shows that explicitly adding meta-information (e.g., length, topics, style) into the data generation prompts can effectively remove the bias in the generated synthetic data and improve the diversity of those synthetic data.
Furthermore,~\citet{xu2023wizardlm} propose a novel \emph{Evol-Instruct} framework to obtain complex and difficult instructions gradually. 
Instead of using existing instructions to prompt LLMs to produce new instructions via \emph{in-context learning}, in \emph{Evol-Instruct}, there are five different manually-designed prompts to explicitly instruct LLMs to rewrite the existing simple instructions into complex ones using in-depth methods (i.e., including more information on particular topics) or in-Breadth methods (i.e, improving topics/information coverage). The resulting WizardLM model is ranked top in the MT-Bench~\cite{zheng2023judging} and AlpacaEval~\cite{dubois2023alpacafarm}.
\citet{luo2023wizardcoder} further expand this idea to produce complex code and programming instructions from the simple ones and propose the \emph{WizardCoder} model, which outperforms several strong commercial LLMs, e.g., Anthropic's Claude and Google's Bard.~\citet{gunasekar2023textbooks} propose to generate textbook-like instructions prompted with sufficient background knowledge to promote reasoning and basic algorithmic skills of LLMs. They find that the resulting 1.3B LLMs \emph{phi-1} successfully outperform various much larger LLMs, showing the importance of data quality.

\paragraph{Improving Output Quality}
Aside from the provision of high-quality instruction input, a critical requisite is to skillfully prompt LLMs to yield high-quality responses. The conventional method of enhancing response quality entails appending LLM prompts with additional conditions, encompassing the following facets.

\textbf{(1) Reasoning-Provoking Conditions:} ~\citet{wei2022chain} proposed the Chain-of-Thought (CoT) reasoning approach, which includes preconditions in the LLM prompts and  generation the intermediate reasoning processes for complex problems, thereby assisting LLMs in problem-solving. Inspired by CoT, ~\citet{mukherjee2023orca} developed the Orca model, which learns not only the superficial response text from LLMs, but also captures complex reasoning process signals. Specifically, they guided LLMs to respond to reasoning-intensive FLAN instructions with a series of predefined system prompts (e.g., ``think step-by-step and justify your response''), spurring LLMs (e.g., GPT4) to disclose their reasoning process information. Thanks to these advancements, the Orca model significantly outperformed several powerful open-sourced LLMs.

\textbf{(2) Hand-crafted Guiding Principles:} ~\citet{Sun2023PrincipleDrivenSO} introduced  \emph{self-alignment} framework that incorporates 16 manually devised principle rules into input prompts, thereby steering LLMs towards generating useful, ethical, and reliable responses. To augment the impact of these rules, they employed the Chain-of-Thoughts (CoT) technology~\cite{wei2022chain}, elucidating five examples to coach LLMs in discerning which rules to implement prior to generating actual response contents.

\textbf{(3) Role-playing Conditions:} ~\citet{DBLP:journals/corr/abs-2304-10453} devised a method to generate a set of role profiles using a blend of ChatGPT and manual efforts. They created seed instructions for each role profile and applied \emph{self-instruction} to the combination of role profiles and instructions to obtain nuanced responses from LLMs.~\citet{xu2023expertprompting} proposed a two-stage instruction response framework in which an expert profile is initially generated based on the instructions to be answered, followed by using both the expert profile and actual instructions to prompt LLMs for high-quality responses. 

\textbf{(4) Difficulty-monitoring Conditions:} 
~\citet{Jiang2023LionAD} proposed monitoring the quality of instruction response based on external LLM-based evaluations. They first fine-tune foundational LLMs with instruction data to obtain ``student LLMs''. Then, for each of training instruction, they gather responses from both teacher LLMs (e.g., ChatGPT) and student LLMs and prompted LLMs to conduct pairwise evaluation on the quality of both responses. Instructions are retained only when the student LLMs' response falls short of that from the teacher LLMs.
% Apart from generating high-quality instruction input, it is also necessary to appropriately prompt LLMs to produce high-quality responses. The common way to improve response quality is to prompt LLMs with additional conditions in the following aspects. 
% \textbf{(1) Hand-crafted guiding principles:}
% ~\citet{Sun2023PrincipleDrivenSO} propose a novel \emph{self-alignment} framework that adds 16 manually-designed principle rules into the input prompts to guide LLMs to produce helpful, ethical, and reliable responses. 
% To further enhance the implication of these rules, they apply Chain-of-Thoughts (CoT) technology~\cite{wei2022chain}, with 5 demonstrate examples, to instruct LLMs to determine which rules to use before generating the actual response contents.
% \textbf{(2) Reasoning-provoking condition:}
% \citet{wei2022chain} proposes Chain-of-Thought (CoT) reasoning to prompt LLMs with pre-conditions and asks it to generate intermediate reasoning processes for complex problems, and facilitate LLMs in problem-solving.
% Following CoT, ~\citet{mukherjee2023orca} propose the Orca model that not only learns the surface response text from LLMs but also their complex reasoning process signals. 
% Specifically, they prompt LLMs to respond to reasoning-intensive FLAN instructions with a set of pre-defined system prompts (e.g., `` think step-by-step and justify your response'') that encourage the LLMs (e.g., GPT4) to produce their reasoning process information. With these advances, the Orca model successfully outperforms several strong open-sourced LLMs.
% \textbf{(3) Role-playing condition:}
% ~\citet{DBLP:journals/corr/abs-2304-10453} propose to produce a set of role profiles using a combination of ChatGPT and manual efforts. They then manually build seed instructions for each role profile and apply \emph{self-instruction} over the combination of role profiles and instructions to obtain fine-grained responses from LLMs.
% ~\citet{xu2023expertprompting} propose a two-stage instruction response framework where they first generate an expert profile based on the to-be-answer instructions, then both the expert profile and the actual instructions are used to prompt LLMs to produce the high-quality responses.
% ~\citet{Jiang2023LionAD}
% propose to keep track of instruction response quality based on the external LLM-based evaluation. For each instruction, they collect responses from both teacher LLMs (e.g., ChatGPT) and student LLMs (e.g., LLaMA) and prompt LLMs to evaluate the quality of both responses. The instructions are only retained when the response of student LLMs is worse than the one from teacher LLMs.

\subsubsection{Multi-turn Instructions}
In previous sections, we mainly focus on collecting synthetic single-turn instructions. However, LLMs well aligned with human should be capable to interact with users in a dialogue-based setting. To achieve this goal, some research efforts attempt to collect synthetic multi-turn instructions from strong LLMs.
When aligning LLaMA with human, Vicuna~\cite{vicuna2023} leverage instructions from ShareGPT which is website hosting interesting human-LLMs joint conversations. However, ShareGPT requires large volumes of users to upload their conversations.
~\citet{DBLP:journals/corr/abs-2304-01196} propose a novel Self-Chatting framework where questions from popular QA websites are used as the starting topics, then Chat-3.5 is prompted to chat with itself about this question in a four-turn dialogue.~\citet{DBLP:journals/corr/abs-2303-17760} propose \emph{CAMEL}, a ``role-playing'' framework where a human annotators first provide a topic, then LLMs are separately prompted to be ``AI Users'' and ``AI Assistants'' to discuss about this topic.~\citet{DBLP:journals/corr/abs-2304-07854} take a step further and prompt LLMs to first determine the conversation topic and then ask LLMs to chat with themselves to produce dialogue corpus.~\citet{selfee2023} propose a novel revision-based multi-turn dialogue corpus. Specifically, after instructions and initial responses, they further prompt LLMs to generate feedback and the revised version of responses if necessary. They use this dataset to train the \emph{SelFee} model and show that \emph{SelFee} can effectively improve its own answers when prompted to do so without any external guidance. The UltraLLaMA model~\cite{ding2023enhancing} leverages a wide range of real-world information, including (a) real-world knowledge from LLMs and Wikipedia; (b) various text creation tasks; (c) high-quality textual corpus, to produce initial questions and instructions that guide LLMs to generate diverse and high-quality multi-turn dialogues.

\subsubsection{Multilingual Instructions}
The above-generated instructions or dialogues are mostly based on English. To align LLMs with human who speak other languages, it is urgent and essential to expand the existing English resources into Multilingual ones. One straightforward idea is to translate instruction inputs and outputs into the target languages.
~\citet{DBLP:journals/corr/abs-2304-10453} propose two translation strategies: \emph{(a)} Post-answering which first translates the instruction inputs into the target language and then prompts strong LLMs to answer it. This could potentially preserve the specific culture patterns embedded in the target languages, but the output quality may be low as existing strong LLMs are often English-dominated; \emph{(b)} Post-translating which first prompts strong LLMs to respond the instructions in English, then translate both inputs and outputs. This approach could obtain high-quality output text, but lost the specific culture information.~\citet{bactrian} follow the \emph{Post-answering} strategy to construct instruction data for 52 popular languages using Google-Translate, then use these data to fine-tune LLaMA using the LoRA technology. An alternative solution is to mix several langauges in a multi-turn dialogue. BayLing~\cite{Zhang2023BayLingBC} introduces a set of multi-turn \emph{interactive translation} instructions to simultaneously improve multilingual and instruction-following ability for LLMs. Specifically, each multi-turn instruction is essentially a translation task where users first ask LLMs to translate a sentence to another language, then the users gradually add additional requirements (e.g., could you only use 10 words?). This process naturally connects different languages as well as human preferences with LLMs. We also summarize how to effectively adapt English-oriented LLMs to other languages in Appendix~\ref{otherlanguageLLMs}.



\subsection{Instruction Data Management}
\label{datamanagement}
As discussed above, there are extensive approaches focusing on generating high-quality instructions from different sources. 
Naturally, it becomes critical to effectively manage all of these instruction data in the LLMs alignment.

\paragraph{Instruction Implications}
Several studies focus on the implications of instruction data. ~\citet{DBLP:journals/corr/abs-2304-07854} demonstrate that an increment in the total count of training instructions can be advantageous for standard NLP tasks (e.g., information extraction, classification, Closed QA, summarization). Yet, it bears negligible influence on complex reasoning tasks such as Math, Code, CoT, and Brainstorming. Intriguingly, ~\citet{muennighoff2023scaling} discover that adding approximately 50\% of programming instructions not only leaves unaffected the general conversational performance but also enhances the reasoning prowess of LLMs. In parallel,~\citet{ghosal2023flacuna} observe that integrating FLAN-style instructions with synthetic instructions from ChatGPT/GPT-4 effectively enhances LLMs' reasoning and problem-solving capacity.

\citet{wang2023far} conduct a comprehensive analysis of the impacts of various instructions derived from different sources on factual knowledge, reasoning, coding, multilingual, and open-ended scenarios. They also reveal that instructions pertaining to CoT and Coding are vital for augmenting the reasoning capability of LLMs. Additionally, they ascertain that different instructions can affect different LLM capabilities. Therefore, a composite of all instruction types empowers the corresponding LLMs to reach their better overall performance, hinting at the need for more advanced instruction collection techniques and technologies.

% There are some works that attempt to understand the implication of instruction data.~\citet{DBLP:journals/corr/abs-2304-07854} report that increasing the total number of training instructions is beneficial for common NLP tasks (e.g., information extraction, classification, Closed QA, summarizing), but almost has no effect on complex reasoning tasks (i.e., Math, Code, CoT and Brainstorming). Interestingly,~\citet{muennighoff2023scaling} find that adding around 50\% of programming instructions 
% not only has no negative effect on common chatting performance but also improves the reasoning capability of LLMs. Similarly,~\citet{ghosal2023flacuna} find that mixing the FLAN-style instructions with ChatGPT/GPT-4 synthetic instructions could effectively improve LLMs' reasoning and problem-solving ability. ~\citet{wang2023far} conduct a comprehensive investigation on the implication of different instructions from different sources on factual knowledge, reasoning, coding, multilingual and open-ended scenarios. They also discover that instructions with CoT and Coding are critical for improving LLMs reasoning capability. Furturemore, they find different instructions could improve LLMs' different capabilities. Thus, combining all types of instructions enable the corresponding LLMs to achieve best overall performance, suggesting more advanced instruction collection and technologies.




% ~\citet{DBLP:journals/corr/abs-2304-10453} propose to use both single-turn instructions from strong LLMs (e.g., ChatGPT) and dialogue-based data from ShareGPT, which aligns LLMs with human and to improve their conversational skills. The resulting LLM successfully outperforms several Chinese 7B LLMs. However,~\citet{DBLP:journals/corr/abs-2304-07854} demonstrate that this type of strategy cannot improve the quality of generated outputs under the GPT-4 evaluation framework.


\paragraph{Instruction Quantity}
Another critical question in instruction data management is the optimal quantity of instruction data required for effective LLM alignment. ~\citet{alshikh2023becoming} address this question by introducing a novel early-stopping criterion known as \textbf{IFS}. The premise of \textbf{IFS} rests on the observation that, given an input textual prefix, foundational LLMs typically predict ensuing tokens and generate "continuation-like" outputs, while fully instruction-tuned LLMs interpret the input prefix as questions, thereby generating "answer-like" outputs. \textbf{IFS} is quantified as the proportion of "answer-like" outputs within all its outputs given the instructions. The researchers train an external classifier to discriminate between "continuation-like" and "answer-like" outputs, concluding that LLaMA necessitates approximately 8K instructions to achieve a high IFS score. More instructions could potentially induce a semantic shift in the foundational LLMs.~\citet{zhou2023lima} similarly discern that merely 6K high-quality instructions suffice to align with human preferences.
Motivated by these findings, researchers are investigating high-quality instruction selection.~\citet{cao2023instruction} aim to identify predictive features of high-quality instructions. Initially, they extract representative features from the instruction dataset, then utilize these instructions to fine-tune LLMs. The feature importance is based on the model's performance. Their experiments demonstrate the better performance of LLMs trained on the resultant instructions.
Differently,~\citet{chen2023alpagasus} propose using ChatGPT to directly assess the quality of instructions by assigning scores. They report that the LLM trained on the top 9K instructions notably outperforms those trained on the complete set of 52K Alpaca instructions.
% Another important problem in instruction data management is how much instruction data is succfient for LLMs alignment.~\citet{alshikh2023becoming} provide an answer to this question via a novel early-stopping criteria \textbf{IFS}. The intuition behind \textbf{IFS} is that given an input textual prefix, the foundation LLMs tend to predict the next tokens and produce ``continuation-like''
% outputs, while fully instruction-tuned LLMs should treat the input prefix as questions and tend to produce ``answer-like'' outputs. \textbf{IFS} is defined as the ratio of  ``answer-like'' outputs in all of its outputs when prompting
% the instructions. They train an external classifier to distinguish between ``continuation-like'' and ``answer-like''. They find that LLaMA only requires around 8K instructions to reach high-level of IFS score. More instructions could result in a semantic shift to the foundation LLMs.~\citet{zhou2023lima} also find that only 6K high-quality instructions are sufficient to align with human preferences. Motivated by these findings, there are some research focus on the high-qaulity instruction selection.~\citet{cao2023instruction} propose to find out predictive features for high-quality instructions. They first extract several representative features for the instructions dataset and then use these instructions to fine-tune LLMs. The feature importance is then calculated based on these model performance. The experiments show the superior performance of LLMs trained on the resulting instructions.~\citet{chen2023alpagasus}, on the other hand, propose to leverage ChatGPT to directly evaluate the instruction quality by assigning scores. They find that the LLM trained on the top 9K instructions outperform than the ones train on the full 52k Alpaca instructions.







