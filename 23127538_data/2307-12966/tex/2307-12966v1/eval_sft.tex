\section{Alignment Evaluation}
\label{eval}
After collecting instructions and training LLMs on these instructions, we finally consider the evaluation for alignment quality. In this section, we will discuss benchmarks used for evaluation in Section~\ref{evalbenchmark} and the evaluation protocols in Section~\ref{evalparadigm}.

\subsection{Evaluation Benchmarks}
\label{evalbenchmark}
There are various benchmarks to evaluate the aligned LLMs. In general, these benchmarks can be categorized into \emph{Closed-set Benchmarks} and \emph{Open-set Benchmarks}. The former type  focuses on evaluating the skills and knowledge of aligned LLMs, while the latter type often concentrates on the open scenarios where there are no standardized answers. 



\subsubsection{Closed-set Benchmarks}
The closed-set benchmarks mostly include testing instances whose possible answers are predefined and limited to a finite set (e.g., multiple choices). We discuss some of the most commonly used benchmarks below. We refer readers to~\citet{chang2023survey} for more comprehensive introduction of LLMs' evaluation benchmarks. 

\paragraph{General Knowledge}
MMLU~\cite{hendrycks2021measuring} is an English-based benchmark to evaluate LLMs knowledge in zero-shot and few-shot settings. It comprehensively includes questions from the elementary level to an advanced professional level from 57 subjects including STEM, the humanities, the social sciences, etc. The granularity and breadth of the subjects make MMLU ideal for identifying LLMsâ€™ blind spots.
There are also several benchmarks attempting in evaluating the general knowledge in Chinese LLMs.
C-MMLU~\cite{li2023cmmlu}, C-Eval~\cite{huang2023ceval}, M3KE~\cite{liu2023m3ke} and AGIEval~\cite{zhong2023agieval} 
are all Chinese counterparts of MMLU that include diverse sets of questions from multiple subjects with different difficulty levels from various Chinese standardized exams, including Chinese college entrance exams, advanced maths competitions and law exams.
The KoLA benchmark~\cite{yu2023kola} is proposed to evaluate the general real-world knowledge of LLMs.

\paragraph{Reasoning}
Reasoning is a fundamental type of human intelligence that are crucial in solving complicated tasks. Interestingly, research find that LLMs have exhibit emergent behaviors, including the reasoning ability, when they are sufficiently large. Thus, there are several benchmarks in evaluating the ability of arithmetic, commonsense, and symbolic reasoning for LLMs. GSM8K~\cite{cobbe2021training} and Maths~\cite{hendrycks2021measuring} are designed to evaluate the arithmetic reasoning ability for LLMs. CSQA~\cite{talmor-etal-2019-commonsenseqa} and StrategyQA~\cite{geva-etal-2021-aristotle} are proposed to evaluate the commonsense reasoning ability which requires the LLMs to use daily life commonsense to infer in novel situations.~\citet{wei2022chain} propose two novel tasks, Last Letter Concatenation and Coin Flip and measure the Symbolic reasoning ability that involves the manipulation of symbols according to formal rules. BBH~\cite{suzgun2022challenging}, a challenging subset of BIG-Bench~\cite{srivastava2023beyond}, focus on evaluating a wide range of reasoning skills, such as Date Understanding, Word Sorting, and Causal Judgement. 

\paragraph{Coding}
HumanEval~\cite{chen2021evaluating}, HumanEval+~\cite{liu2023your}, and MBPP~\cite{austin2021program} are extensively used benchmarks to evaluate the coding skills of LLMs. They encompass a vast collection of Python programming problems and corresponding test cases to automatically verify the code generated by Code LLMs. The DS-1000 benchmark~\cite{Lai2022DS1000} comprises 1,000 distinct data science workflows spanning seven
libraries. It assesses the performance of code generations against test cases and supports two evaluation modes: completion and insertion. 


\subsubsection{Open-ended Benchmarks}
In contrast to the closed-set benchmarks, the responses to open-set benchmarks can be more flexible and diverse, where aligned LLMs are usually given chatting questions or topics that do not have any fixed reference answers. Early attempts of open-ended benchmarks, such as Vicuna-80~\cite{vicuna2023}, Open-Assistant-953~\cite{Kopf2023OpenAssistantC}, User-Instructions-252~\cite{DBLP:journals/corr/abs-2212-10560}, often leverage a small number of syntactic instructions from LLMs as testing instances. All evaluation candidate LLMs are prompted with the same instructions to provide responses, which are then evaluated against human-based or LLMs-based evaluators. However, these types of benchmarks can only provide comparison several LLMs at a time, making it challenging to reveal a fair comparison among a board range of LLMs, as well as incremental updates when new LLMs become available. AlpacaEval~\cite{dubois2023alpacafarm} tackles this issue by reporting the \emph{Win Rate} of the LLMs candidate to 
the reference LLM \emph{text-davinci-003}. Accordingly, LLMs with higher \emph{Win Rate} are generally better than the ones with lower \emph{Win Rate}. MT-Bench~\cite{zheng2023judging} further increases the evaluation difficulty by proposing 80 multi-turn evaluation instances and wishes LLMs could effectively capture context information in previous turns. FLASK~\cite{Ye2023FLASKFL} proposed to provide fine-grained evaluation towards aligned LLMs. FLASK includes  1,700 instances from 120 datasets. Each testing instance is labelled with a set of 12 foundational and essential ``alignment skills'' (e.g., logical thinking, user alignment, etc.). Accordingly, it is straightforward to evaluate LLMs' capabilities on these skills separately. 



\subsection{Evaluation Paradigm}
\label{evalparadigm}
As open-ended benchmarks often do not have reference answers, it is essential to rely on external human or LLMs evaluators. In this section, we will introduce both human- and LLMs-based evaluation paradigm.

\subsubsection{Human-based Evaluation}

Automatic metrics, such as BLUE~\cite{papineni-etal-2002-bleu} and ROUGE~\cite{lin-2004-rouge}, require ground-truth references and have relatively low correlation with human judgments. Thus, they are not feasible for evaluating responses to open-ended questions.
To bridge this gap, human annotators are used to evaluate the quality of open-ended model responses.
~\citet{DBLP:journals/corr/abs-2212-10560,DBLP:journals/corr/abs-2304-1440} propose to evaluate the response quality in an ordinal classification setting where human annotators are instructed to categorize each response into one of the four levels (i.e., acceptable, minor errors, major errors and unacceptable), separately. However, some other research have found that such classification annotation strategy heavily depend on the subjectivity of annotators, which can result in poor inter-rater reliability~\cite{KALPATHYCRAMER20162345}. Accordingly 
~\citet{alpaca} propose to use a pairwise comparison framework for evaluating the output quality of two LLMs systems. Given the instruction inputs and two model outputs, the human annotators are asked to select a better one. Furthermore, to accurately evaluate multiple LLMs,~\citet{zheng2023judging,dettmers2023qlora} further introduce  the Elo rating system which calculates the relative skill levels of players in zero-sum games such as chess games. Specifically, in Elo system, the player scores are updated based on the result of each pairwise comparison and the current player scores.


\subsubsection{LLMs-based Evaluation}
While human evaluations are often of high quality, it could be inefficient and expensive. In addition, the increasing quality of generated text from LLMs makes it more challenging for human annotators to distinguish between human-written and LLM-generated text in the open-ended NLP tasks~\cite{Clark2021AllT}. Given the strong text capability of LLMs, recent studies propose to incorporate LLMs into the output text evaluation in various NLP tasks without additional expensive references and human efforts.~\citet{tang2023not} propose to improve the traditional automatic metrics by increasing the number of references via LLMs-based paraphrasing systems. However, such method still requires one reference for each evaluation instance. In contrast, ~\citet{liu2023gpteval,fu2023gptscore,chen2023exploring,chiang2023can} propose to 
directly use LLMs to evaluate the generated text quality without a single reference in a wide range of Natural Language Generation (NLG) tasks. Specifically, they construct complicated input instructions with tasks background and evaluation rules and prompt LLMs to follow these evaluation instructions to provide scores for output text. There are also some research efforts that propose LLMs-based evaluation framework for specific NLG tasks, including text summarization~\citet{gao2023human}, code generation~\cite{zhuo2023large}, open-ended QA~\cite{bai2023benchmarking} and conversations~\cite{lin2023llm}. Due to the flexibility of prompts, it is also possible to conduct multi-dimensional evaluation towards the generated text~\cite{lin2023llm,fu2023gptscore}.~\citet{min2023factscore,zha2023alignscore} propose to evaluate factual correctness using both closed-sourced and open-sourced LLMs. Similar to human evaluation, there are also research efforts in explicitly prompting LLMs to conduct pairwise comparisons. To compare the capabilities of two LLMs, instead of assigning scores separately,~\citet{dubois2023alpacafarm,zheng2023judging} explicitly to prompt GPT-4 to select the better response for the same instruction inputs. 


\paragraph{LLMs Evaluation Bias}
Despite LLMs achieve impressive consistency with human judgment, 
~\citet{wang2023large} find that such LLM-based evaluation paradigm suffers from a positional bias and those strong LLMs (i.e., GPT-4) tend to assign higher scores to the first appeared candidates. To calibrate such bias, they propose 
to \textbf{a)} repeat the LLM evaluation process multiple times with different candidate ordering and \textbf{b)} explicitly prompt LLMs to provide chain-of-thoughts for the evaluation before assigning the actual score.~\cite{Wu2023StyleOS} find that LLM-based evaluation prefer candidates with factual errors over shorter candidates and candidates with grammatical errors, despite the former one could impose greater danger than the latter ones. To address this bias, they propose a multi-dimensional Elo rating system which separately evaluates the candidates from the perspective of accuracy, helpfulness and language. Such approach allows a more comprehensive understanding towards the candidates quality than previous one-shot evaluation. Concretely,~\cite{zheng2023judging} systematically show the bias LLMs-based evaluation systems. On top of positional and length bias, they also discover Self-enhancement bias which means LLMs favor their own responses than the ones from other sources. To tackle these biases, their solutions include swapping responses, adding few-shot examples and leveraging CoT and references information. 

\paragraph{Evaluation-Specific LLM}
Despite achieving high-quality automatic evaluation results, the above approaches heavily rely on state-of-the-art closed-source LLMs (e.g., GPT-4) which could result in data privacy issues.~\cite{zheng2023judging} propose to train evaluation-specific LLMs. PandaLM~\cite{wang2023pandalm} is such a specialized evaluation LLMs by fine-tuning LLaMA-7B using around 300K high-quality synthetic evaluation instructions generated from GPT-3.5. Specifically, they first collect large volumes of instructions as well as outputs from a diverse range of open-sourced LLMs, such as LLaMA-7B and Bloom-7B. They then prompt GPT-3.5 to analysis and evaluate the quality of a pair of outputs. Their results on human-annotated meta-evaluation shows that, despite bebing much smaller, PandaLM achieves on-par evaluation performance comparing to GPT-3.5 and GPT-4.

