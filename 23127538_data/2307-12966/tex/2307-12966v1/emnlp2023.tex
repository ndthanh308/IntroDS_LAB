% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{EMNLP2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{url}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{CJKutf8}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{url}
\usepackage{array}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{pifont}
\usepackage{bm}
\usepackage{lipsum}
\usepackage{xcolor}
\usepackage{makecell}
\usepackage{CJKutf8}
\usepackage{tikz}
\usepackage[edges]{forest}
\definecolor{hidden-draw}{RGB}{20,68,106}
\definecolor{hidden-pink}{RGB}{255,245,247}


\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Aligning Large Language Models with Human: A Survey}

\author{\textbf{Yufei Wang}, \textbf{Wanjun Zhong}, \textbf{Liangyou Li}, \textbf{Fei Mi},  \textbf{Xingshan Zeng}, \textbf{Wenyong Huang} \\ \textbf{Lifeng Shang}, \textbf{Xin Jiang}, \textbf{Qun Liu} \\
Huawei Noah's Ark Lab
\\
\{wangyufei44,zhongwanjun1,liliangyou,mifei2,zeng.xingshan,wenyong.huang\}@huawei.com \\
\{Shang.Lifeng,Jiang.Xin,qun.liu\}@huawei.com
}

\begin{document}
\maketitle
\begin{abstract}
    

Large Language Models (LLMs) trained on extensive textual corpora have emerged as leading solutions for a broad array of Natural Language Processing (NLP) tasks. 
Despite their notable performance, these models are prone to certain limitations such as misunderstanding human instructions, generating potentially biased content, or factually incorrect (hallucinated) information. Hence, aligning LLMs with human expectations has become an active area of interest within the research community.
This survey presents a comprehensive overview of these alignment technologies, including the following aspects. 
(1) \textbf{Data collection}:  the methods for effectively collecting high-quality instructions for LLM alignment, including the use of NLP benchmarks, human annotations, and leveraging strong LLMs.
(2) \textbf{Training methodologies}: a detailed review of the prevailing training methods employed for LLM alignment. Our exploration encompasses Supervised Fine-tuning, both Online and Offline human preference training, along with parameter-efficient training mechanisms.
(3) \textbf{Model Evaluation}: the methods for evaluating the effectiveness of these human-aligned LLMs, presenting a multifaceted approach towards their assessment.
In conclusion, we collate and distill our findings, shedding light on several promising future research avenues in the field. This survey, therefore, serves as a valuable resource for anyone invested in understanding and advancing the alignment of LLMs to better suit human-oriented tasks and expectations. An associated GitHub link collecting the latest papers is available at~\url{https://github.com/GaryYufei/AlignLLMHumanSurvey}.
\end{abstract}
% \begin{abstract}
% Large Language Models (LLMs), trained on large textual data, have demonstrated superior performance on various Natural Language Processing (NLP) tasks. However, these LLMs do not necessarily follow given instructions and could generate hallucinated facts and biased contents. Recent NLP research community has witnessed rapid growth in aligning these LLMs with human. 
% This survey provides an overview towards these alignment technologies for LLMs. We first focus on how to effectively collect high-quality instructions for alignment via NLP benchmarks, human annotations and existing strong LLMs. We further review the training technologies for LLMs alignment, including Supervised Fine-tuning, Online and Offline human preference training and parameter-efficient training. Next, we show how to comprehensively evaluate these aligned LLMs. Finally, we summarize our findings as well as several promising research directions.
% \end{abstract}


\input{intro}

\input{instruction_collection}

\input{training}


\input{eval_sft}



\input{challenges_and_future}

\section{Conclusion}
\label{conclusion}
This survey provides an up-to-date review to recent advances of LLMs alignment technologies. We summarize these research efforts into \emph{Alignment Instruction Collection}, \emph{Alignment Training} and \emph{Alignment Evaluation}.  Finally, we pointed out several promising future directions for LLMs alignment. We hope this survey could provide insightful perspectives and inspire further research in improving LLMs alignment.

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix

\section{Appendix}
\label{sec:appendix}
\input{appendix}

\end{document}
