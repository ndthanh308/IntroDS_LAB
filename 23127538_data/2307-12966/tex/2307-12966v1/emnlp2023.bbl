\begin{thebibliography}{109}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{AlShikh et~al.(2023)AlShikh, Daaboul, Goddard, Imel, Kamble,
  Kulkarni, and Russak}]{alshikh2023becoming}
Waseem AlShikh, Manhal Daaboul, Kirk Goddard, Brock Imel, Kiran Kamble,
  Parikshith Kulkarni, and Melisa Russak. 2023.
\newblock Becoming self-instruct: introducing early stopping criteria for
  minimal instruct tuning.
\newblock \emph{arXiv preprint arXiv:2307.03692}.

\bibitem[{Anand et~al.(2023)Anand, Nussbaum, Duderstadt, Schmidt, and
  Mulyar}]{gpt4all}
Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy
  Mulyar. 2023.
\newblock Gpt4all: Training an assistant-style chatbot with large scale data
  distillation from gpt-3.5-turbo.
\newblock \url{https://github.com/nomic-ai/gpt4all}.

\bibitem[{Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan,
  Jiang, Cai, Terry, Le et~al.}]{austin2021program}
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski,
  David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et~al. 2021.
\newblock Program synthesis with large language models.
\newblock \emph{arXiv preprint arXiv:2108.07732}.

\bibitem[{Bach et~al.(2022)Bach, Sanh, Yong, Webson, Raffel, Nayak, Sharma,
  Kim, Bari, Fevry, Alyafeai, Dey, Santilli, Sun, Ben-david, Xu, Chhablani,
  Wang, Fries, Al-shaibani, Sharma, Thakker, Almubarak, Tang, Radev, Jiang, and
  Rush}]{bach-etal-2022-promptsource}
Stephen Bach, Victor Sanh, Zheng~Xin Yong, Albert Webson, Colin Raffel,
  Nihal~V. Nayak, Abheesht Sharma, Taewoon Kim, M~Saiful Bari, Thibault Fevry,
  Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-david,
  Canwen Xu, Gunjan Chhablani, Han Wang, Jason Fries, Maged Al-shaibani, Shanya
  Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Dragomir Radev, Mike
  Tian-jian Jiang, and Alexander Rush. 2022.
\newblock \href {https://doi.org/10.18653/v1/2022.acl-demo.9}
  {{P}rompt{S}ource: An integrated development environment and repository for
  natural language prompts}.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics: System Demonstrations}, pages 93--104, Dublin,
  Ireland. Association for Computational Linguistics.

\bibitem[{Bai et~al.(2023)Bai, Ying, Cao, Lv, He, Wang, Yu, Zeng, Xiao, Lyu
  et~al.}]{bai2023benchmarking}
Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu,
  Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, et~al. 2023.
\newblock Benchmarking foundation models with language-model-as-an-examiner.
\newblock \emph{arXiv preprint arXiv:2306.04181}.

\bibitem[{bench authors(2023)}]{srivastava2023beyond}
BIG bench authors. 2023.
\newblock \href {https://openreview.net/forum?id=uyTL5Bvosj} {Beyond the
  imitation game: Quantifying and extrapolating the capabilities of language
  models}.
\newblock \emph{Transactions on Machine Learning Research}.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei}]{NEURIPS2020_1457c0d6}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
  Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
  Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
  Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
  and Dario Amodei. 2020.
\newblock \href
  {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}
  {Language models are few-shot learners}.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pages 1877--1901. Curran Associates, Inc.

\bibitem[{Cao et~al.(2023)Cao, Kang, and Sun}]{cao2023instruction}
Yihan Cao, Yanbin Kang, and Lichao Sun. 2023.
\newblock Instruction mining: High-quality instruction data selection for large
  language models.
\newblock \emph{arXiv preprint arXiv:2307.06290}.

\bibitem[{Chang et~al.(2023)Chang, Wang, Wang, Wu, Zhu, Chen, Yang, Yi, Wang,
  Wang et~al.}]{chang2023survey}
Yupeng Chang, Xu~Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang,
  Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et~al. 2023.
\newblock A survey on evaluation of large language models.
\newblock \emph{arXiv preprint arXiv:2307.03109}.

\bibitem[{Chen et~al.(2023{\natexlab{a}})Chen, Zhang, Shi, Li, Smola, and
  Yang}]{chen2023parameterefficient}
Jiaao Chen, Aston Zhang, Xingjian Shi, Mu~Li, Alex Smola, and Diyi Yang.
  2023{\natexlab{a}}.
\newblock \href {https://openreview.net/forum?id=XSRSWxyJIC}
  {Parameter-efficient fine-tuning design spaces}.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}.

\bibitem[{Chen et~al.(2023{\natexlab{b}})Chen, Li, Yan, Wang, Gunaratna, Yadav,
  Tang, Srinivasan, Zhou, Huang et~al.}]{chen2023alpagasus}
Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav,
  Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et~al.
  2023{\natexlab{b}}.
\newblock Alpagasus: Training a better alpaca with fewer data.
\newblock \emph{arXiv preprint arXiv:2307.08701}.

\bibitem[{Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards,
  Burda, Joseph, Brockman et~al.}]{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira
  Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
  Brockman, et~al. 2021.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}.

\bibitem[{Chen et~al.(2023{\natexlab{c}})Chen, Lin, Sch{\"a}rli, and
  Zhou}]{chen2023teaching}
Xinyun Chen, Maxwell Lin, Nathanael Sch{\"a}rli, and Denny Zhou.
  2023{\natexlab{c}}.
\newblock Teaching large language models to self-debug.
\newblock \emph{arXiv preprint arXiv:2304.05128}.

\bibitem[{Chen et~al.(2023{\natexlab{d}})Chen, Wang, Jiang, Shi, and
  Xu}]{chen2023exploring}
Yi~Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and Ruifeng Xu.
  2023{\natexlab{d}}.
\newblock Exploring the use of large language models for reference-free text
  quality evaluation: A preliminary empirical study.
\newblock \emph{arXiv preprint arXiv:2304.00723}.

\bibitem[{Chen et~al.(2023{\natexlab{e}})Chen, Jiang, Chen, Wang, Yu, Chen,
  Zhang, Liang, Zhang, Zhang, Li, Wan, Wang, and
  Li}]{DBLP:journals/corr/abs-2304-10453}
Zhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, Guiming Chen,
  Hongbo Zhang, Juhao Liang, Chen Zhang, Zhiyi Zhang, Jianquan Li, Xiang Wan,
  Benyou Wang, and Haizhou Li. 2023{\natexlab{e}}.
\newblock \href {https://doi.org/10.48550/arXiv.2304.10453} {Phoenix:
  Democratizing chatgpt across languages}.
\newblock \emph{CoRR}, abs/2304.10453.

\bibitem[{Chiang and Lee(2023)}]{chiang2023can}
Cheng-Han Chiang and Hung-yi Lee. 2023.
\newblock Can large language models be an alternative to human evaluations?
\newblock \emph{arXiv preprint arXiv:2305.01937}.

\bibitem[{Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang,
  Zhuang, Gonzalez, Stoica, and Xing}]{vicuna2023}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
  Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and
  Eric~P. Xing. 2023.
\newblock \href {https://vicuna.lmsys.org} {Vicuna: An open-source chatbot
  impressing gpt-4 with 90\%* chatgpt quality}.

\bibitem[{Clark et~al.(2021)Clark, August, Serrano, Haduong, Gururangan, and
  Smith}]{Clark2021AllT}
Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan,
  and Noah~A. Smith. 2021.
\newblock All that’s ‘human’ is not gold: Evaluating human evaluation of
  generated text.
\newblock In \emph{Annual Meeting of the Association for Computational
  Linguistics}.

\bibitem[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser,
  Plappert, Tworek, Hilton, Nakano et~al.}]{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
  Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
  et~al. 2021.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}.

\bibitem[{Conover et~al.(2023)Conover, Hayes, Mathur, Xie, Wan, Shah, Ghodsi,
  Wendell, Zaharia, and Xin}]{DatabricksBlog2023DollyV2}
Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali
  Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. 2023.
\newblock \href
  {https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm}
  {Free dolly: Introducing the world's first truly open instruction-tuned llm}.

\bibitem[{Cui et~al.(2023{\natexlab{a}})Cui, Yang, and
  Yao}]{chinese-llama-alpaca}
Yiming Cui, Ziqing Yang, and Xin Yao. 2023{\natexlab{a}}.
\newblock \href {https://arxiv.org/abs/2304.08177} {Efficient and effective
  text encoding for chinese llama and alpaca}.
\newblock \emph{arXiv preprint arXiv:2304.08177}.

\bibitem[{Cui et~al.(2023{\natexlab{b}})Cui, Yang, and Yao}]{cui2023efficient}
Yiming Cui, Ziqing Yang, and Xin Yao. 2023{\natexlab{b}}.
\newblock Efficient and effective text encoding for chinese llama and alpaca.
\newblock \emph{arXiv preprint arXiv:2304.08177}.

\bibitem[{Dettmers et~al.(2023)Dettmers, Pagnoni, Holtzman, and
  Zettlemoyer}]{dettmers2023qlora}
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023.
\newblock Qlora: Efficient finetuning of quantized llms.
\newblock \emph{arXiv preprint arXiv:2305.14314}.

\bibitem[{Ding et~al.(2023)Ding, Chen, Xu, Qin, Zheng, Hu, Liu, Sun, and
  Zhou}]{ding2023enhancing}
Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan
  Liu, Maosong Sun, and Bowen Zhou. 2023.
\newblock Enhancing chat language models by scaling high-quality instructional
  conversations.
\newblock \emph{arXiv preprint arXiv:2305.14233}.

\bibitem[{Dong et~al.(2023)Dong, Xiong, Goyal, Pan, Diao, Zhang, Shum, and
  Zhang}]{dong2023raft}
Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang,
  Kashun Shum, and Tong Zhang. 2023.
\newblock Raft: Reward ranked finetuning for generative foundation model
  alignment.
\newblock \emph{arXiv preprint arXiv:2304.06767}.

\bibitem[{Dubois et~al.(2023)Dubois, Li, Taori, Zhang, Gulrajani, Ba, Guestrin,
  Liang, and Hashimoto}]{dubois2023alpacafarm}
Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba,
  Carlos Guestrin, Percy Liang, and Tatsunori~B Hashimoto. 2023.
\newblock Alpacafarm: A simulation framework for methods that learn from human
  feedback.
\newblock \emph{arXiv preprint arXiv:2305.14387}.

\bibitem[{Edunov et~al.(2018)Edunov, Ott, Auli, Grangier, and
  Ranzato}]{edunov-etal-2018-classical}
Sergey Edunov, Myle Ott, Michael Auli, David Grangier, and Marc{'}Aurelio
  Ranzato. 2018.
\newblock \href {https://doi.org/10.18653/v1/N18-1033} {Classical structured
  prediction losses for sequence to sequence learning}.
\newblock In \emph{Proceedings of the 2018 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long Papers)}, pages 355--364, New Orleans,
  Louisiana. Association for Computational Linguistics.

\bibitem[{Fu et~al.(2023)Fu, Ng, Jiang, and Liu}]{fu2023gptscore}
Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023.
\newblock Gptscore: Evaluate as you desire.
\newblock \emph{arXiv preprint arXiv:2302.04166}.

\bibitem[{Gao et~al.(2023)Gao, Ruan, Sun, Yin, Yang, and Wan}]{gao2023human}
Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang, and Xiaojun Wan.
  2023.
\newblock Human-like summarization evaluation with chatgpt.
\newblock \emph{arXiv preprint arXiv:2304.02554}.

\bibitem[{Geng et~al.(2023)Geng, Gudibande, Liu, Wallace, Abbeel, Levine, and
  Song}]{koala_blogpost_2023}
Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey
  Levine, and Dawn Song. 2023.
\newblock \href {https://bair.berkeley.edu/blog/2023/04/03/koala/} {Koala: A
  dialogue model for academic research}.
\newblock Blog post.

\bibitem[{Geva et~al.(2021)Geva, Khashabi, Segal, Khot, Roth, and
  Berant}]{geva-etal-2021-aristotle}
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan
  Berant. 2021.
\newblock \href {https://doi.org/10.1162/tacl_a_00370} {Did aristotle use a
  laptop? a question answering benchmark with implicit reasoning strategies}.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  9:346--361.

\bibitem[{Ghosal et~al.(2023)Ghosal, Chia, Majumder, and
  Poria}]{ghosal2023flacuna}
Deepanway Ghosal, Yew~Ken Chia, Navonil Majumder, and Soujanya Poria. 2023.
\newblock Flacuna: Unleashing the problem solving power of vicuna using flan
  fine-tuning.
\newblock \emph{arXiv preprint arXiv:2307.02053}.

\bibitem[{Gunasekar et~al.(2023)Gunasekar, Zhang, Aneja, Mendes, Del~Giorno,
  Gopi, Javaheripi, Kauffmann, de~Rosa, Saarikivi
  et~al.}]{gunasekar2023textbooks}
Suriya Gunasekar, Yi~Zhang, Jyoti Aneja, Caio C{\'e}sar~Teodoro Mendes, Allie
  Del~Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo
  de~Rosa, Olli Saarikivi, et~al. 2023.
\newblock Textbooks are all you need.
\newblock \emph{arXiv preprint arXiv:2306.11644}.

\bibitem[{He et~al.(2022)He, Zhou, Ma, Berg-Kirkpatrick, and
  Neubig}]{he2022towards}
Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham
  Neubig. 2022.
\newblock \href {https://openreview.net/forum?id=0RDcd5Axok} {Towards a unified
  view of parameter-efficient transfer learning}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Hendrycks et~al.(2021)Hendrycks, Burns, Basart, Zou, Mazeika, Song,
  and Steinhardt}]{hendrycks2021measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
  Song, and Jacob Steinhardt. 2021.
\newblock \href {https://openreview.net/forum?id=d7KBjmI3GmQ} {Measuring
  massive multitask language understanding}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Honovich et~al.(2022)Honovich, Scialom, Levy, and
  Schick}]{DBLP:journals/corr/abs-2212-09689}
Or~Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2022.
\newblock \href {https://doi.org/10.48550/arXiv.2212.09689} {Unnatural
  instructions: Tuning language models with (almost) no human labor}.
\newblock \emph{CoRR}, abs/2212.09689.

\bibitem[{Hu et~al.(2022)Hu, yelong shen, Wallis, Allen-Zhu, Li, Wang, Wang,
  and Chen}]{hu2022lora}
Edward~J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
  Wang, Lu~Wang, and Weizhu Chen. 2022.
\newblock \href {https://openreview.net/forum?id=nZeVKeeFYf9} {Lo{RA}: Low-rank
  adaptation of large language models}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Huang et~al.(2023)Huang, Bai, Zhu, Zhang, Zhang, Su, Liu, Lv, Zhang,
  Lei, Fu, Sun, and He}]{huang2023ceval}
Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su,
  Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and
  Junxian He. 2023.
\newblock C-eval: A multi-level multi-discipline chinese evaluation suite for
  foundation models.
\newblock \emph{arXiv preprint arXiv:2305.08322}.

\bibitem[{Jentzsch and Kersting(2023)}]{jentzsch2023chatgpt}
Sophie Jentzsch and Kristian Kersting. 2023.
\newblock Chatgpt is fun, but it is not funny! humor is still challenging large
  language models.
\newblock \emph{arXiv preprint arXiv:2306.04563}.

\bibitem[{Ji et~al.(2023)Ji, Gong, Deng, Peng, Niu, Ma, and
  Li}]{DBLP:journals/corr/abs-2304-07854}
Yunjie Ji, Yan Gong, Yong Deng, Yiping Peng, Qiang Niu, Baochang Ma, and
  Xiangang Li. 2023.
\newblock \href {https://doi.org/10.48550/arXiv.2304.07854} {Towards better
  instruction following language models for chinese: Investigating the impact
  of training data and evaluation}.
\newblock \emph{CoRR}, abs/2304.07854.

\bibitem[{Jiang et~al.(2023)Jiang, Chan, Chen, and Wang}]{Jiang2023LionAD}
Yuxin Jiang, Chunkit Chan, Mingyang Chen, and Wei Wang. 2023.
\newblock Lion: Adversarial distillation of closed-source large language model.
\newblock \emph{ArXiv}, abs/2305.12870.

\bibitem[{Kalpathy-Cramer et~al.(2016)Kalpathy-Cramer, Campbell, Erdogmus,
  Tian, Kedarisetti, Moleta, Reynolds, Hutcheson, Shapiro, Repka, Ferrone,
  Drenser, Horowitz, Sonmez, Swan, Ostmo, Jonas, Chan, Chiang, Chiang, Ostmo,
  Sonmez, Campbell, Chan, Jonas, Horowitz, Coki, Eccles, Sarna, Berrocal,
  Negron, Denser, Cumming, Osentoski, Check, Zajechowski, Lee, Kruger,
  McGovern, Simmons, Murthy, Galvis, Rotter, Chen, Li, Taylor, Roll,
  Kalpathy-Cramer, Erdogmus, Martinez-Castellanos, Salinas-Longoria, Romero,
  Arriola, Olguin-Manriquez, Meraz-Gutierrez, Dulanto-Reinoso, and
  Montero-Mendoza}]{KALPATHYCRAMER20162345}
Jayashree Kalpathy-Cramer, J.~Peter Campbell, Deniz Erdogmus, Peng Tian,
  Dharanish Kedarisetti, Chace Moleta, James~D. Reynolds, Kelly Hutcheson,
  Michael~J. Shapiro, Michael~X. Repka, Philip Ferrone, Kimberly Drenser, Jason
  Horowitz, Kemal Sonmez, Ryan Swan, Susan Ostmo, Karyn~E. Jonas, R.V.~Paul
  Chan, Michael~F. Chiang, Michael~F. Chiang, Susan Ostmo, Kemal Sonmez,
  J.~Peter Campbell, R.V.~Paul Chan, Karyn Jonas, Jason Horowitz, Osode Coki,
  Cheryl-Ann Eccles, Leora Sarna, Audina Berrocal, Catherin Negron, Kimberly
  Denser, Kristi Cumming, Tammy Osentoski, Tammy Check, Mary Zajechowski,
  Thomas Lee, Evan Kruger, Kathryn McGovern, Charles Simmons, Raghu Murthy,
  Sharon Galvis, Jerome Rotter, Ida Chen, Xiaohui Li, Kent Taylor, Kaye Roll,
  Jayashree Kalpathy-Cramer, Deniz Erdogmus, Maria~Ana Martinez-Castellanos,
  Samantha Salinas-Longoria, Rafael Romero, Andrea Arriola, Francisco
  Olguin-Manriquez, Miroslava Meraz-Gutierrez, Carlos~M. Dulanto-Reinoso, and
  Cristina Montero-Mendoza. 2016.
\newblock \href {https://doi.org/https://doi.org/10.1016/j.ophtha.2016.07.020}
  {Plus disease in retinopathy of prematurity: Improving diagnosis by ranking
  disease severity and using quantitative image analysis}.
\newblock \emph{Ophthalmology}, 123(11):2345--2351.

\bibitem[{Kopf et~al.(2023)Kopf, Kilcher, von Rutte, Anagnostidis, Tam,
  Stevens, Barhoum, Duc, Stanley, Nagyfi, Shahul, Suri, Glushkov, Dantuluri,
  Maguire, Schuhmann, Nguyen, and Mattick}]{Kopf2023OpenAssistantC}
Andreas Kopf, Yannic Kilcher, Dimitri von Rutte, Sotiris Anagnostidis, Zhi~Rui
  Tam, Keith Stevens, Abdullah Barhoum, Nguyen~Minh Duc, Oliver Stanley,
  Rich'ard Nagyfi, ES~Shahul, Sameer Suri, David Glushkov, Arnav Dantuluri,
  Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick. 2023.
\newblock Openassistant conversations - democratizing large language model
  alignment.
\newblock \emph{ArXiv}, abs/2304.07327.

\bibitem[{Lai et~al.(2022)Lai, Li, Wang, Zhang, Zhong, Zettlemoyer, tau Yih,
  Fried, Wang, and Yu}]{Lai2022DS1000}
Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke
  Zettlemoyer, Scott~Wen tau Yih, Daniel Fried, Sida Wang, and Tao Yu. 2022.
\newblock Ds-1000: A natural and reliable benchmark for data science code
  generation.
\newblock \emph{ArXiv}, abs/2211.11501.

\bibitem[{Lester et~al.(2021)Lester, Al-Rfou, and
  Constant}]{lester-etal-2021-power}
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.emnlp-main.243} {The power of
  scale for parameter-efficient prompt tuning}.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 3045--3059, Online and Punta Cana,
  Dominican Republic. Association for Computational Linguistics.

\bibitem[{Li et~al.(2023{\natexlab{a}})Li, Hammoud, Itani, Khizbullin, and
  Ghanem}]{DBLP:journals/corr/abs-2303-17760}
Guohao Li, Hasan Abed Al~Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and
  Bernard Ghanem. 2023{\natexlab{a}}.
\newblock \href {https://doi.org/10.48550/arXiv.2303.17760} {{CAMEL:}
  communicative agents for "mind" exploration of large scale language model
  society}.
\newblock \emph{CoRR}, abs/2303.17760.

\bibitem[{Li et~al.(2023{\natexlab{b}})Li, Koto, Wu, Aji, and
  Baldwin}]{bactrian}
Haonan Li, Fajri Koto, Minghao Wu, Alham~Fikri Aji, and Timothy Baldwin.
  2023{\natexlab{b}}.
\newblock Bactrian-x: A multilingual replicable instruction-following model
  with low-rank adaptation.
\newblock \emph{arXiv preprint arXiv:2305.15011}.

\bibitem[{Li et~al.(2023{\natexlab{c}})Li, Zhang, Koto, Yang, Zhao, Gong, Duan,
  and Baldwin}]{li2023cmmlu}
Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan
  Duan, and Timothy Baldwin. 2023{\natexlab{c}}.
\newblock Cmmlu: Measuring massive multitask language understanding in chinese.
\newblock \emph{arXiv preprint arXiv:2306.09212}.

\bibitem[{Li and Liang(2021)}]{li-liang-2021-prefix}
Xiang~Lisa Li and Percy Liang. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.acl-long.353} {Prefix-tuning:
  Optimizing continuous prompts for generation}.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pages 4582--4597,
  Online. Association for Computational Linguistics.

\bibitem[{Lin(2004)}]{lin-2004-rouge}
Chin-Yew Lin. 2004.
\newblock \href {https://aclanthology.org/W04-1013} {{ROUGE}: A package for
  automatic evaluation of summaries}.
\newblock In \emph{Text Summarization Branches Out}, pages 74--81, Barcelona,
  Spain. Association for Computational Linguistics.

\bibitem[{Lin and Chen(2023)}]{lin2023llm}
Yen-Ting Lin and Yun-Nung Chen. 2023.
\newblock Llm-eval: Unified multi-dimensional automatic evaluation for
  open-domain conversations with large language models.
\newblock \emph{arXiv preprint arXiv:2305.13711}.

\bibitem[{Liu et~al.(2023{\natexlab{a}})Liu, Jin, Ren, Yu, Dong, Peng, Zhang,
  Peng, Zhang, Lyu et~al.}]{liu2023m3ke}
Chuang Liu, Renren Jin, Yuqi Ren, Linhao Yu, Tianyu Dong, Xiaohan Peng, Shuting
  Zhang, Jianxiang Peng, Peiyi Zhang, Qingqing Lyu, et~al. 2023{\natexlab{a}}.
\newblock M3ke: A massive multi-level multi-subject knowledge evaluation
  benchmark for chinese large language models.
\newblock \emph{arXiv preprint arXiv:2305.10263}.

\bibitem[{Liu et~al.(2022{\natexlab{a}})Liu, Geng, Lee, Mordatch, Levine,
  Narang, and Abbeel}]{Liu2022TowardsBF}
Hao Liu, Xinyang Geng, Lisa Lee, Igor Mordatch, Sergey Levine, Sharan Narang,
  and P.~Abbeel. 2022{\natexlab{a}}.
\newblock Towards better few-shot and finetuning performance with forgetful
  causal language models.

\bibitem[{Liu et~al.(2023{\natexlab{b}})Liu, Sferrazza, and
  Abbeel}]{liu2023languages}
Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. 2023{\natexlab{b}}.
\newblock Languages are rewards: Hindsight finetuning using human feedback.
\newblock \emph{arXiv preprint arXiv:2302.02676}.

\bibitem[{Liu et~al.(2023{\natexlab{c}})Liu, Xia, Wang, and
  Zhang}]{liu2023your}
Jiawei Liu, Chunqiu~Steven Xia, Yuyao Wang, and Lingming Zhang.
  2023{\natexlab{c}}.
\newblock Is your code generated by chatgpt really correct? rigorous evaluation
  of large language models for code generation.
\newblock \emph{arXiv preprint arXiv:2305.01210}.

\bibitem[{Liu et~al.(2022{\natexlab{b}})Liu, Jia, Zhang, Zhuang, Liu, and
  Vosoughi}]{liu2022second}
Ruibo Liu, Chenyan Jia, Ge~Zhang, Ziyu Zhuang, Tony~X Liu, and Soroush
  Vosoughi. 2022{\natexlab{b}}.
\newblock \href {https://openreview.net/forum?id=u6OfmaGIya1} {Second thoughts
  are best: Learning to re-align with human values from text edits}.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Liu et~al.(2023{\natexlab{d}})Liu, Yang, Jia, Zhang, Zhou, Dai, Yang,
  and Vosoughi}]{liu2023training}
Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge~Zhang, Denny Zhou, Andrew~M Dai, Diyi
  Yang, and Soroush Vosoughi. 2023{\natexlab{d}}.
\newblock Training socially aligned language models in simulated human society.
\newblock \emph{arXiv preprint arXiv:2305.16960}.

\bibitem[{Liu et~al.(2023{\natexlab{e}})Liu, Iter, Xu, Wang, Xu, and
  Zhu}]{liu2023gpteval}
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu.
  2023{\natexlab{e}}.
\newblock Gpteval: Nlg evaluation using gpt-4 with better human alignment.
\newblock \emph{arXiv preprint arXiv:2303.16634}.

\bibitem[{Liu et~al.(2022{\natexlab{c}})Liu, Liu, Radev, and
  Neubig}]{liu-etal-2022-brio}
Yixin Liu, Pengfei Liu, Dragomir Radev, and Graham Neubig. 2022{\natexlab{c}}.
\newblock \href {https://doi.org/10.18653/v1/2022.acl-long.207} {{BRIO}:
  Bringing order to abstractive summarization}.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 2890--2903,
  Dublin, Ireland. Association for Computational Linguistics.

\bibitem[{Longpre et~al.(2023)Longpre, Hou, Vu, Webson, Chung, Tay, Zhou, Le,
  Zoph, Wei et~al.}]{longpre2023flan}
Shayne Longpre, Le~Hou, Tu~Vu, Albert Webson, Hyung~Won Chung, Yi~Tay, Denny
  Zhou, Quoc~V Le, Barret Zoph, Jason Wei, et~al. 2023.
\newblock The flan collection: Designing data and methods for effective
  instruction tuning.
\newblock \emph{arXiv preprint arXiv:2301.13688}.

\bibitem[{Luo et~al.(2023)Luo, Xu, Zhao, Sun, Geng, Hu, Tao, Ma, Lin, and
  Jiang}]{luo2023wizardcoder}
Ziyang Luo, Can Xu, Pu~Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang
  Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023.
\newblock Wizardcoder: Empowering code large language models with
  evol-instruct.
\newblock \emph{arXiv preprint arXiv:2306.08568}.

\bibitem[{Mangrulkar et~al.(2022)Mangrulkar, Gugger, Debut, Belkada, and
  Paul}]{peft}
Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, and Sayak
  Paul. 2022.
\newblock Peft: State-of-the-art parameter-efficient fine-tuning methods.
\newblock \url{https://github.com/huggingface/peft}.

\bibitem[{Min et~al.(2023)Min, Krishna, Lyu, Lewis, Yih, Koh, Iyyer,
  Zettlemoyer, and Hajishirzi}]{min2023factscore}
Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang~Wei Koh,
  Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.
\newblock Factscore: Fine-grained atomic evaluation of factual precision in
  long form text generation.
\newblock \emph{arXiv preprint arXiv:2305.14251}.

\bibitem[{Mishra et~al.(2022)Mishra, Khashabi, Baral, and
  Hajishirzi}]{mishra-etal-2022-cross}
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022.
\newblock \href {https://doi.org/10.18653/v1/2022.acl-long.244} {Cross-task
  generalization via natural language crowdsourcing instructions}.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 3470--3487,
  Dublin, Ireland. Association for Computational Linguistics.

\bibitem[{Muennighoff et~al.(2023)Muennighoff, Rush, Barak, Scao, Piktus, Tazi,
  Pyysalo, Wolf, and Raffel}]{muennighoff2023scaling}
Niklas Muennighoff, Alexander~M Rush, Boaz Barak, Teven~Le Scao, Aleksandra
  Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. 2023.
\newblock Scaling data-constrained language models.
\newblock \emph{arXiv preprint arXiv:2305.16264}.

\bibitem[{Mukherjee et~al.(2023)Mukherjee, Mitra, Jawahar, Agarwal, Palangi,
  and Awadallah}]{mukherjee2023orca}
Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid
  Palangi, and Ahmed Awadallah. 2023.
\newblock Orca: Progressive learning from complex explanation traces of gpt-4.
\newblock \emph{arXiv preprint arXiv:2306.02707}.

\bibitem[{Nguyen et~al.(2023)Nguyen, Suri, Tsui, and Schuhmann}]{OIG}
Huu Nguyen, Sameer Suri, Ken Tsui, and Christoph Schuhmann. 2023.
\newblock \href {https://laion.ai/blog/oig-dataset/} {The oig dataset}.

\bibitem[{Nguyen et~al.(2022)Nguyen, Zheng, and
  Grover}]{nguyen2022conserweightive}
Tung Nguyen, Qinqing Zheng, and Aditya Grover. 2022.
\newblock Conserweightive behavioral cloning for reliable offline reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2210.05158}.

\bibitem[{Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Gray, Schulman, Hilton, Kelton, Miller, Simens,
  Askell, Welinder, Christiano, Leike, and
  Lowe}]{DBLP:conf/nips/Ouyang0JAWMZASR22}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John
  Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
  Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
\newblock \href {https://openreview.net/forum?id=TG8KACxEON} {Training language
  models to follow instructions with human feedback}.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Papineni et~al.(2002)Papineni, Roukos, Ward, and
  Zhu}]{papineni-etal-2002-bleu}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.
\newblock \href {https://doi.org/10.3115/1073083.1073135} {{B}leu: a method for
  automatic evaluation of machine translation}.
\newblock In \emph{Proceedings of the 40th Annual Meeting of the Association
  for Computational Linguistics}, pages 311--318, Philadelphia, Pennsylvania,
  USA. Association for Computational Linguistics.

\bibitem[{Peng et~al.(2023)Peng, Li, He, Galley, and Gao}]{peng2023instruction}
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023.
\newblock Instruction tuning with gpt-4.
\newblock \emph{arXiv preprint arXiv:2304.03277}.

\bibitem[{Rafailov et~al.(2023)Rafailov, Sharma, Mitchell, Ermon, Manning, and
  Finn}]{rafailov2023direct}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher~D
  Manning, and Chelsea Finn. 2023.
\newblock Direct preference optimization: Your language model is secretly a
  reward model.
\newblock \emph{arXiv preprint arXiv:2305.18290}.

\bibitem[{Song et~al.(2023)Song, Yu, Li, Yu, Huang, Li, and
  Wang}]{song2023preference}
Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and
  Houfeng Wang. 2023.
\newblock Preference ranking optimization for human alignment.
\newblock \emph{arXiv preprint arXiv:2306.17492}.

\bibitem[{Sun et~al.(2023{\natexlab{a}})Sun, Ji, Ma, and
  Li}]{sun2023comparative}
Xianghui Sun, Yunjie Ji, Baochang Ma, and Xiangang Li. 2023{\natexlab{a}}.
\newblock A comparative study between full-parameter and lora-based fine-tuning
  on chinese instruction data for instruction following large language model.
\newblock \emph{arXiv preprint arXiv:2304.08109}.

\bibitem[{Sun et~al.(2023{\natexlab{b}})Sun, Shen, Zhou, Zhang, Chen, Cox,
  Yang, and Gan}]{Sun2023PrincipleDrivenSO}
Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David~D.
  Cox, Yiming Yang, and Chuang Gan. 2023{\natexlab{b}}.
\newblock Principle-driven self-alignment of language models from scratch with
  minimal human supervision.

\bibitem[{Suzgun et~al.(2022)Suzgun, Scales, Sch{\"a}rli, Gehrmann, Tay, Chung,
  Chowdhery, Le, Chi, Zhou, , and Wei}]{suzgun2022challenging}
Mirac Suzgun, Nathan Scales, Nathanael Sch{\"a}rli, Sebastian Gehrmann, Yi~Tay,
  Hyung~Won Chung, Aakanksha Chowdhery, Quoc~V Le, Ed~H Chi, Denny Zhou, , and
  Jason Wei. 2022.
\newblock Challenging big-bench tasks and whether chain-of-thought can solve
  them.
\newblock \emph{arXiv preprint arXiv:2210.09261}.

\bibitem[{Talmor et~al.(2019)Talmor, Herzig, Lourie, and
  Berant}]{talmor-etal-2019-commonsenseqa}
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019.
\newblock \href {https://doi.org/10.18653/v1/N19-1421} {{C}ommonsense{QA}: A
  question answering challenge targeting commonsense knowledge}.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4149--4158,
  Minneapolis, Minnesota. Association for Computational Linguistics.

\bibitem[{Tang et~al.(2023)Tang, Lu, Jiang, Huang, Zhang, Zhao, and
  Wei}]{tang2023not}
Tianyi Tang, Hongyuan Lu, Yuchen~Eleanor Jiang, Haoyang Huang, Dongdong Zhang,
  Wayne~Xin Zhao, and Furu Wei. 2023.
\newblock Not all metrics are guilty: Improving nlg evaluation with llm
  paraphrasing.
\newblock \emph{arXiv preprint arXiv:2305.15067}.

\bibitem[{Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin,
  Liang, and Hashimoto}]{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
  Guestrin, Percy Liang, and Tatsunori~B. Hashimoto. 2023.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}.

\bibitem[{Team(2023)}]{MosaicML2023Introducing}
MosaicML~NLP Team. 2023.
\newblock \href {www.mosaicml.com/blog/mpt-30b} {Introducing mpt-30b: Raising
  the bar for open-source foundation models}.
\newblock Accessed: 2023-06-22.

\bibitem[{Wang et~al.(2023{\natexlab{a}})Wang, Cheng, Yu, and Liu}]{openchat}
Guan Wang, Sijie Cheng, Qiying Yu, and Changling Liu. 2023{\natexlab{a}}.
\newblock \href {https://doi.org/10.5281/zenodo.8105775} {{OpenChat: Advancing
  Open-source Language Models with Imperfect Data}}.

\bibitem[{Wang et~al.(2023{\natexlab{b}})Wang, Li, Chen, Zhu, Lin, Cao, Liu,
  Liu, and Sui}]{wang2023large}
Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi~Liu,
  Tianyu Liu, and Zhifang Sui. 2023{\natexlab{b}}.
\newblock Large language models are not fair evaluators.
\newblock \emph{arXiv preprint arXiv:2305.17926}.

\bibitem[{Wang et~al.(2023{\natexlab{c}})Wang, Yu, Zeng, Yang, Wang, Chen,
  Jiang, Xie, Wang, Xie et~al.}]{wang2023pandalm}
Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen,
  Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, et~al. 2023{\natexlab{c}}.
\newblock Pandalm: An automatic evaluation benchmark for llm instruction tuning
  optimization.
\newblock \emph{arXiv preprint arXiv:2306.05087}.

\bibitem[{Wang et~al.(2023{\natexlab{d}})Wang, Ivison, Dasigi, Hessel, Khot,
  Chandu, Wadden, MacMillan, Smith, Beltagy et~al.}]{wang2023far}
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot,
  Khyathi~Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah~A Smith,
  Iz~Beltagy, et~al. 2023{\natexlab{d}}.
\newblock How far can camels go? exploring the state of instruction tuning on
  open resources.
\newblock \emph{arXiv preprint arXiv:2306.04751}.

\bibitem[{Wang et~al.(2022{\natexlab{a}})Wang, Kordi, Mishra, Liu, Smith,
  Khashabi, and Hajishirzi}]{DBLP:journals/corr/abs-2212-10560}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A. Smith, Daniel
  Khashabi, and Hannaneh Hajishirzi. 2022{\natexlab{a}}.
\newblock \href {https://doi.org/10.48550/arXiv.2212.10560} {Self-instruct:
  Aligning language model with self generated instructions}.
\newblock \emph{CoRR}, abs/2212.10560.

\bibitem[{Wang et~al.(2022{\natexlab{b}})Wang, Mishra, Alipoormolabashi, Kordi,
  Mirzaei, Naik, Ashok, Dhanasekaran, Arunkumar, Stap, Pathak, Karamanolakis,
  Lai, Purohit, Mondal, Anderson, Kuznia, Doshi, Pal, Patel, Moradshahi,
  Parmar, Purohit, Varshney, Kaza, Verma, Puri, Karia, Doshi, Sampat, Mishra,
  Reddy~A, Patro, Dixit, and Shen}]{wang-etal-2022-super}
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza
  Mirzaei, Atharva Naik, Arjun Ashok, Arut~Selvan Dhanasekaran, Anjana
  Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai,
  Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi,
  Kuntal~Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali
  Purohit, Neeraj Varshney, Phani~Rohitha Kaza, Pulkit Verma, Ravsehaj~Singh
  Puri, Rushang Karia, Savan Doshi, Shailaja~Keyur Sampat, Siddhartha Mishra,
  Sujan Reddy~A, Sumanta Patro, Tanay Dixit, and Xudong Shen.
  2022{\natexlab{b}}.
\newblock \href {https://aclanthology.org/2022.emnlp-main.340}
  {Super-{N}atural{I}nstructions: Generalization via declarative instructions
  on 1600+ {NLP} tasks}.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pages 5085--5109, Abu Dhabi, United Arab
  Emirates. Association for Computational Linguistics.

\bibitem[{Wei et~al.(2022{\natexlab{a}})Wei, Bosma, Zhao, Guu, Yu, Lester, Du,
  Dai, and Le}]{wei2022finetuned}
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester,
  Nan Du, Andrew~M. Dai, and Quoc~V Le. 2022{\natexlab{a}}.
\newblock \href {https://openreview.net/forum?id=gEZrGCozdqR} {Finetuned
  language models are zero-shot learners}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Wei et~al.(2022{\natexlab{b}})Wei, Wang, Schuurmans, Bosma, brian
  ichter, Xia, Chi, Le, and Zhou}]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia,
  Ed~H. Chi, Quoc~V Le, and Denny Zhou. 2022{\natexlab{b}}.
\newblock \href {https://openreview.net/forum?id=_VjQlMeSB_J} {Chain of thought
  prompting elicits reasoning in large language models}.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Wu and Aji(2023)}]{Wu2023StyleOS}
Minghao Wu and Alham~Fikri Aji. 2023.
\newblock Style over substance: Evaluation biases for large language models.
\newblock \emph{ArXiv}, abs/2307.03025.

\bibitem[{Wu et~al.(2023)Wu, Waheed, Zhang, Abdul{-}Mageed, and
  Aji}]{DBLP:journals/corr/abs-2304-1440}
Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul{-}Mageed, and Alham~Fikri
  Aji. 2023.
\newblock \href {https://doi.org/10.48550/arXiv.2304.14402} {Lamini-lm: {A}
  diverse herd of distilled models from large-scale instructions}.
\newblock \emph{CoRR}, abs/2304.14402.

\bibitem[{Xu et~al.(2023{\natexlab{a}})Xu, Yang, Lin, Wang, Zhou, Zhang, and
  Mao}]{xu2023expertprompting}
Benfeng Xu, An~Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, and
  Zhendong Mao. 2023{\natexlab{a}}.
\newblock Expertprompting: Instructing large language models to be
  distinguished experts.
\newblock \emph{arXiv preprint arXiv:2305.14688}.

\bibitem[{Xu et~al.(2023{\natexlab{b}})Xu, Sun, Zheng, Geng, Zhao, Feng, Tao,
  and Jiang}]{xu2023wizardlm}
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu~Zhao, Jiazhan Feng, Chongyang
  Tao, and Daxin Jiang. 2023{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2304.12244} {Wizardlm: Empowering large
  language models to follow complex instructions}.

\bibitem[{Xu et~al.(2023{\natexlab{c}})Xu, Guo, Duan, and
  McAuley}]{DBLP:journals/corr/abs-2304-01196}
Canwen Xu, Daya Guo, Nan Duan, and Julian~J. McAuley. 2023{\natexlab{c}}.
\newblock \href {https://doi.org/10.48550/arXiv.2304.01196} {Baize: An
  open-source chat model with parameter-efficient tuning on self-chat data}.
\newblock \emph{CoRR}, abs/2304.01196.

\bibitem[{Ye et~al.(2023{\natexlab{a}})Ye, Jo, Kim, Kim, Hwang, and
  Seo}]{selfee2023}
Seonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, and
  Minjoon Seo. 2023{\natexlab{a}}.
\newblock \href {https://kaistai.github.io/SelFee/} {Selfee: Iterative
  self-revising llm empowered by self-feedback generation}.
\newblock Blog post.

\bibitem[{Ye et~al.(2023{\natexlab{b}})Ye, Kim, Kim, Hwang, Kim, Jo, Thorne,
  Kim, and Seo}]{Ye2023FLASKFL}
Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae
  Jo, James Thorne, Juho Kim, and Minjoon Seo. 2023{\natexlab{b}}.
\newblock Flask: Fine-grained language model evaluation based on alignment
  skill sets.

\bibitem[{Yu et~al.(2023{\natexlab{a}})Yu, Wang, Tu, Cao, Zhang-Li, Lv, Peng,
  Yao, Zhang, Li et~al.}]{yu2023kola}
Jifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao, Daniel Zhang-Li, Xin Lv, Hao
  Peng, Zijun Yao, Xiaohan Zhang, Hanming Li, et~al. 2023{\natexlab{a}}.
\newblock Kola: Carefully benchmarking world knowledge of large language
  models.
\newblock \emph{arXiv preprint arXiv:2306.09296}.

\bibitem[{Yu et~al.(2023{\natexlab{b}})Yu, Zhuang, Zhang, Meng, Ratner,
  Krishna, Shen, and Zhang}]{yu2023large}
Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu~Meng, Alexander Ratner, Ranjay Krishna,
  Jiaming Shen, and Chao Zhang. 2023{\natexlab{b}}.
\newblock Large language model as attributed training data generator: A tale of
  diversity and bias.
\newblock \emph{arXiv preprint arXiv:2306.15895}.

\bibitem[{Yuan et~al.(2023)Yuan, Yuan, Tan, Wang, Huang, and
  Huang}]{yuan2023rrhf}
Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang.
  2023.
\newblock \href {http://arxiv.org/abs/2304.05302} {Rrhf: Rank responses to
  align language models with human feedback without tears}.

\bibitem[{Zha et~al.(2023)Zha, Yang, Li, and Hu}]{zha2023alignscore}
Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu. 2023.
\newblock Alignscore: Evaluating factual consistency with a unified alignment
  function.
\newblock \emph{arXiv preprint arXiv:2305.16739}.

\bibitem[{Zhang et~al.(2023{\natexlab{a}})Zhang, Shi, Liu, Yuan, Li, Dong, Shu,
  Li, Wang, Lin, Huang, and Fu}]{Zhang2023ChineseOI}
Ge~Zhang, Yemin Shi, Ruibo Liu, Ruibin Yuan, Yizhi Li, Siwei Dong, Yu~Shu,
  Zhaoqun Li, Zekun Wang, Chenghua Lin, Wen-Fen Huang, and Jie Fu.
  2023{\natexlab{a}}.
\newblock Chinese open instruction generalist: A preliminary release.
\newblock \emph{ArXiv}, abs/2304.07987.

\bibitem[{Zhang et~al.(2023{\natexlab{b}})Zhang, Chen, Bukharin, He, Cheng,
  Chen, and Zhao}]{zhang2023adaptive}
Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu~Cheng, Weizhu
  Chen, and Tuo Zhao. 2023{\natexlab{b}}.
\newblock \href {https://openreview.net/forum?id=lq62uWRJjiY} {Adaptive budget
  allocation for parameter-efficient fine-tuning}.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}.

\bibitem[{Zhang et~al.(2023{\natexlab{c}})Zhang, Fang, Zhang, Ma, Zhou, Huang,
  Bu, Gui, Chen, Chen, and Feng}]{Zhang2023BayLingBC}
Shaolei Zhang, Qingkai Fang, Zhuocheng Zhang, Zhengrui Ma, Yan Zhou, Langlin
  Huang, Mengyu Bu, Shangtong Gui, Yunji Chen, Xilin Chen, and Yang Feng.
  2023{\natexlab{c}}.
\newblock Bayling: Bridging cross-lingual alignment and instruction following
  through interactive translation for large language models.
\newblock \emph{ArXiv}, abs/2306.10968.

\bibitem[{Zhang* et~al.(2020)Zhang*, Kishore*, Wu*, Weinberger, and
  Artzi}]{Zhang*2020BERTScore:}
Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian~Q. Weinberger, and Yoav
  Artzi. 2020.
\newblock \href {https://openreview.net/forum?id=SkeHuCVFDr} {Bertscore:
  Evaluating text generation with bert}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Zhao et~al.(2023)Zhao, Khalman, Joshi, Narayan, Saleh, and
  Liu}]{zhao2023calibrating}
Yao Zhao, Mikhail Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, and
  Peter~J Liu. 2023.
\newblock \href {https://openreview.net/forum?id=0qSOodKmJaN} {Calibrating
  sequence likelihood improves conditional language generation}.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}.

\bibitem[{Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li,
  Li, Xing et~al.}]{zheng2023judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao
  Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, et~al. 2023.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock \emph{arXiv preprint arXiv:2306.05685}.

\bibitem[{Zhong et~al.(2023)Zhong, Cui, Guo, Liang, Lu, Wang, Saied, Chen, and
  Duan}]{zhong2023agieval}
Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin
  Saied, Weizhu Chen, and Nan Duan. 2023.
\newblock Agieval: A human-centric benchmark for evaluating foundation models.
\newblock \emph{arXiv preprint arXiv:2304.06364}.

\bibitem[{Zhou et~al.(2023)Zhou, Liu, Xu, Iyer, Sun, Mao, Ma, Efrat, Yu, Yu
  et~al.}]{zhou2023lima}
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe
  Ma, Avia Efrat, Ping Yu, Lili Yu, et~al. 2023.
\newblock Lima: Less is more for alignment.
\newblock \emph{arXiv preprint arXiv:2305.11206}.

\bibitem[{Zhuo(2023)}]{zhuo2023large}
Terry~Yue Zhuo. 2023.
\newblock Large language models are state-of-the-art evaluators of code
  generation.
\newblock \emph{arXiv preprint arXiv:2304.14317}.

\bibitem[{Ziegler et~al.(2019)Ziegler, Stiennon, Wu, Brown, Radford, Amodei,
  Christiano, and Irving}]{ziegler2019finetuning}
Daniel~M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom~B. Brown, Alec Radford,
  Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019.
\newblock \href {https://arxiv.org/abs/1909.08593} {Fine-tuning language models
  from human preferences}.
\newblock \emph{arXiv preprint arXiv:1909.08593}.

\end{thebibliography}
