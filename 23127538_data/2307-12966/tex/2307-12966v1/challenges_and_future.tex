\section{Challenges and Future Directions}
\label{challengesdirection}
The development of LLM alignment is still in a rudimentary stage and thus leaves much room for improvement. In this section, we summarize existing important research efforts of aligning LLMs with human in Table~\ref{llmsummary}. Below, we will discuss some of the challenges as well as the corresponding future research directions.

\input{LLM_table}

\paragraph{Fine-grained Instruction Data Management}
While research on LLMs alignment have been unprecedentedly active, many of these research efforts propose to leverage training instructions from diverse sources, making it challenging to fairly compare among different methods. As discussed in Section~\ref{datamanagement}, there are some interesting findings about the implication of particular instruction dataset. For example, FLAN and programming instructions can improve reasoning capability aligned LLMs~\cite{ghosal2023flacuna} and ShareGPT general performs well across a wide range of benchmarks~\cite{wang2023far}. 
However, there are still many issues in other aspects of instruction data management remaining unclear, including the optimal quality control towards instruction data, optimal instruction training sequence, how to effectively mix-up different instructions. These research efforts could finally enable fine-grained instruction management, allowing researchers and practitioners to construct high-quality instruction data. 





% Thus, it is critical to come up with resource-constrained LLM alignment evaluation framework where certain alignment resources are constrained at a certain level (e.g., maximum 10K instructions, 5 hours training time, etc.), allowing NLP/LLM researchers and practitioners to focus on constructing high-quality alignment components, rather than brute forcing adding up the alignment scale.

\paragraph{LLMs Alignment for non-English Languages} 
Most of existing research in LLMs alignment are English-dominated. While many approaches, such as complex instruction generation~\cite{xu2023wizardlm} and explanation tuning~\cite{mukherjee2023orca}, are language-agnostic, they only explore English-based prompts and it is unclear how well these prompts perform when adapting to other languages, severely hindering the application of LLMs to non-English regions. It is interesting to see \emph{1)} how these alignment technologies perform in various languages, in particular low-resource languages, and \emph{2)} how to effectively transfer the effect of LLMs alignment across different languages.

\paragraph{LLMs Alignment Training Technologies}
As shown in Table~\ref{llmsummary}, most of existing 
aligned LLMs are based on the simple SFT technology. However, SFT does not explicitly incorporate human preference into LLMs. As a result, aligning LLMs solely based on SFT could require a lot more instruction data and training resources. In general, there is a lacking of comprehensive investigation over the effect of various training technologies to incorporate human preference into LLMs. 
Thus, it is critical to come up with resource-constrained LLM alignment training framework where certain alignment resources are given at a certain level (e.g., maximum 10K instructions, 5 hours training time, etc.), allowing researchers and practitioners to verify the effectiveness of various training methods. As increasing number of instruction data have become available, this exploration could further 
promote effective and environmental-friendly LLMs alignment solutions.

\paragraph{Human-in-the-loop LLMs Alignment Data Generation}
Table~\ref{llmsummary} has shown that ShareGPT data has been widely adapted for LLMs alignment. The preliminary analysis in~\citet{wang2023far} also reveal that   ShareGPT  performs consistly well across a wide range of NLP tasks. These results indicate that human is still a key factor in improving LLMs alignment quality. Different from traditional human annotation framework where human provides annotation based on the instructions, ShareGPT is a human-in-the-loop alignment solution where human can freely determine what LLMs should generate. This shows the great potential of human-in-the-loop data generation solution in LLMs alignment. It will be interesting to explore other types of human-in-the-loop solutions to further facilitate LLMs alignment.

\paragraph{Human-LLM Joint Evaluation Framework}
Existing LLM evaluation frameworks either use LLMs for effective evaluation or leverage crowd-sourcing for high-quality evaluation. As shown in~\cite{Wu2023StyleOS,liu2023gpteval}, state-of-the-art LLMs have demonstrated similar or superior evaluation capability in various NLP tasks.
It is feasible to use LLMs as special evaluation annotators and develop LLM-human joint evaluation framework where LLMs and human are assigned with different evaluation tasks based on their own strengths to maintain both efficiency and quality of the evaluation procedure for LLM alignment .


