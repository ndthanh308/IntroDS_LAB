\section{Introduction}
Foundational Large Language Models (LLMs) such as GPT-3 are pre-trained on a vast textual corpus with objectives to predict subsequent tokens. This process equips LLMs with world knowledge, facilitating the generation of coherent and fluent text in response to various inputs. 
Despite these strengths, foundational LLMs are not always adept at interpreting a wide range of instructions and can produce outputs that deviate from human expectations. Additionally, these models may produce biased content or invent (hallucinated) facts, which can limit their practical usefulness.
%usefulness in real-world scenarios.

Therefore, recent NLP research efforts focus on empowering LLMs to understand instructions and to align with human  expectations. 
Early methods for training LLMs to follow instructions primarily use task instruction sets, which are compiled by combining manually crafted task instruction templates with instances from standard NLP tasks.
However, such approaches often fall short of capturing the intricacies of practical user instructions, as these instructions tend to originate from artificial NLP tasks designed to test specific aspects of machine capabilities. Real-world user instructions, on the other hand, are significantly more diverse and complex.
As a result, OpenAI explored Supervised Fine-Tuning (SFT) of LLMs using instructions annotated by a diverse group of human users. Models developed through this process, such as InstructGPT~\cite{DBLP:conf/nips/Ouyang0JAWMZASR22} and ChatGPT~\footnote{\url{https://chat.openai.com/}}, have demonstrated a marked improvement in understanding human instructions and solving complex tasks.
To further enhance alignment, ~\citet{DBLP:conf/nips/Ouyang0JAWMZASR22} incorporate the Reinforcement Learning from Human Feedback (RLHF) approach, which involves learning from human preferences through a reward model trained with human-rated outputs.
% Large language models (LLMs) have dominated numerous Natural Language Processing (NLP) tasks. They are trained using extensive amounts of textual data to predict the next tokens, allowing them to produce coherent and fluent text when given different inputs. However, these LLMs do not always follow the given instructions or goals and could produce unintended and undesirable outputs with hallucinated facts and biased contents, hindering their real-world applications.

% Motivated by this, the NLP research community has recently attempted to align LLMs with human. Early attempts to train instruction-following LLMs mostly construct closed-domain hand-written instruction templates that can be combined with instances in standard NLP tasks. However, these instructions often lack diversity on their surface tokens and only trains LLMs to execute a single NLP task at a time, while real-world user-generated instructions are extremely diverse, requiring skills from multiple NLP tasks. Thus, OpenAI explores training LLMs by using instructions annotated by a diverse set of human users. The resulting LLMs, including InstructionGPT~\cite{DBLP:conf/nips/Ouyang0JAWMZASR22} and ChatGPT~\footnote{\url{https://chat.openai.com/}}, successfully unleash their powerful potential and are capable to perform more
% complex and diverse NLP tasks/instructions. 

% To achieve this goal,~\citet{DBLP:conf/nips/Ouyang0JAWMZASR22} first conduct Supervised Fine-Tuning (SFT) towards LLMs using human-written instructions, then apply Reinforcement learning From Human Feedback (RLHF) approach, based on the PPO framework~\cite{schulman2017proximal}, to incorporate human ratings of the SFT model outputs to further improve the ability of instruction following. 
% Finally, they evaluate the model output quality on various NLP tasks.
% There are much room for improvement in the alignment process: \emph{(a)} Collecting large-scale human-written instructions can be costly and time-consuming; \emph{(b)} PPO-based RLHF requires massive training resources and suffers from the instability issue; \emph{(c)} Only evaluating instruction-following LLMs on a limited number NLP benchmarks may not fully reveal the multifaceted capability of LLMs.

% % Figure environment removed

\input{taxonomy}

There are challenges in alignment processes and the subsequent evaluation:
(a) Collecting high-quality data for both SFT and RLHF stages can be costly and time-consuming.
(b) The training strategies need to be optimized as SFT training is resource-consuming, and reinforcement learning in RLHF often lacks stability.
(c) Evaluating LLMs comprehensively is challenging, as limited NLP benchmarks may not fully reveal the multifaceted capabilities of LLMs.

To address these limitations, extensive research efforts have been devoted. In Figure~\ref{fig:sftframework}, we provide a summary of these multi-aspect approaches.
For aspect (a), the focus is on effectively collecting large-scale, high-quality data for LLM alignment training. Researchers propose leveraging the power of existing NLP benchmarks, human annotators, and state-of-the-art LLMs (e.g., ChatGPT and GPT-4) to generate training instructions.
To tackle aspect (b), solutions involve optimizing the training methods for better efficiency and stability in incorporating human preferences. 
Parameter-efficient training methods have been proposed to reduce computation burden and improve efficiency in LLM alignment. 
Additionally, some researchers consider human preference as ranking-based training signals or replace scalar rewards with language-based feedback to enhance training stability and performance.
Regarding aspect (c), various human-centric LLM evaluation benchmarks and automatic evaluation protocols (e.g., LLMs for evaluation) have been proposed to obtain a comprehensive evaluation of aligned LLMs.

In this survey, we aim to provide a comprehensive overview of alignment technologies for large language models. 
In Section~\ref{instructioncollecting}, we summarize various methods in effective high-quality data collection. Section~\ref{training} focuses on popular training methods to incorporate human preference data into LLMs. The evaluation benchmarks and automatic protocols for instruction-following LLMs are discussed in Section~\ref{eval}. 
By collating and distilling our findings, we shed light on several promising future research avenues in Section~\ref{challengesdirection}.
Through this survey, we aim to provide an overview of the current state of LLM alignment, enabling researchers and practitioners to navigate the complexities of aligning LLMs with human values and expectations.


% Extensive research works have been devoted to tackle these limitations. We have summarize these efforts in Figure~\ref{fig:sftframework}. 
% For \emph{(a)}, the focus is to effectively collect large-scale high-quality data for LLMs alignment training. Researchers propose to leverage the power of existing NLP benchmarks, human annotators and state-of-the-art LLMs (i.e., ChatGPT and GPT-4~\cite{DBLP:journals/corr/abs-2303-08774}) to generate instruction data that are used to fine-tune much smaller but affordable LLMs (e.g., 7B, 13B). For \emph{(b)}, the core issue is to find out 
% alternative training methods, other than RLHF, to  incorporate human preferences into LLMs in a efficient and effective fusion. Some research attempt to inject human preference as ranking signals, while others propose to use language and revision approaches. To reduce training and data budgets, parameter-efficient training methods have also been adapted for LLM alignment. For \emph{(c)}, various specialized LLMs evaluation benchmarks and 
% human-like automatic evaluation protocols (e.g., LLMs for evaluation) have been proposed in order to obtain a comprehensive understanding of the aligned LLMs. 

% In this survey, we aim to summarize these research efforts for improving the alignment process for LLMs. In Section~\ref{instructioncollecting}, we summarize various methods in effectively collecting high-quality instruction data from existing NLP benchmarks, human annotators and state-of-the-art LLMs. Section~\ref{training} focuses on popular training methods to incorporate human preference data into LLMs. The evaluation benchmarks and tools for instruction-following LLMs are discussed in Section~\ref{eval}.  Finally, we discuss several promising research directions for future LLMs alignment technologies and conclude this paper in Section~\ref{challengesdirection} and Section~\ref{conclusion}, respectively.




% As SFT requires much less computational resources than RLHF, recent research efforts focus on developing various SFT technologies to train existing LLMs to follow open instructions. Thus, in this survey, we focus on the SFT technologies for LLMs.

% % Figure environment removed

% When applying SFT to LLMs, one should first construct the instruction data for fine-tuning. Existing works either leverage existing strong instruction-following LLMs to produce synthetic instructions, as well as the corresponding outputs, for training or propose novel human-in-the-loop framework to construct high-quality instruction dataset. Furthermore, given the training instruction dataset, one should prepare the base LLMs for training. Existing research efforts consider various parameter-efficient fine-tuning solutions and adapt the existing LLMs for different needs. Finally, it is also possible to inject human preference signals in the SFT stage. Previous work either propose to add additional input symbols or additional training loss terms to achieve this goal. 

