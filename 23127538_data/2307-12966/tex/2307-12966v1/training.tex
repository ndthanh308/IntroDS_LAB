\section{Alignment Training}
\label{training}
After collecting instructions from various sources, we then consider using these data to fine-tune existing foundational LLMs to align with human. The native solution is Supervised Fine-Tuning (SFT). Specifically, given instruction input $x$, SFT calculates the cross-entropy loss over the ground-truth response $y$ as follows:
\begin{align}
    L_{ft} = -\sum_t \log P_{LLM}(y_{i',t}|x,y_{i',<t})
\end{align}
Essentially, SFT helps LLMs to understand the semantic meaning of prompts and make meaningful responses.
The main limitation of SFT is that it only teaches LLMs about the best responses and cannot provide fine-grained comparisons to sub-optimal ones. However, it is worth noting that SFT objective or SFT model parameters has also been integrated into many human preference training objective to regularize and stabilize the training process of LLMs.
We summarize the research efforts built on top of SFT into: \emph{Online human preference training}, \emph{Offline human preference training} and \emph{Parameter-effective fine-tuning solutions}.





\subsection{Online Human Preference Training}
\label{onlinetraining}
Reinforcement learning from Human Feedback (RLHF)~\cite{DBLP:conf/nips/Ouyang0JAWMZASR22} is designed to learn the human preference signals from external reward models under the PPO framework. Specifically, RLHF consists of three main stages:
\begin{itemize}
  \item \textbf{Step 1:} Collecting a high-quality instruction set and conducting SFT of pre-trained LLMs. 
  \item \textbf{Step 2:} Collecting manually ranked comparison response pairs and train a reward model ${\rm I\!R}$ to justify the quality of generated responses.
  \item \textbf{Step 3:} Optimizing the SFT model (policy) under the PPO reinforcement learning framework with reward calculated by ${\rm I\!R}$.
\end{itemize}
In Step 3, to mitigate over-optimization issues,~\citet{DBLP:conf/nips/Ouyang0JAWMZASR22} add a KL-divergence regularization between the current model weight and the SFT model weight obtained in Step 1. 
However, despite being effective in learning human preferences, PPO training is difficult in implementation and stable training.
Therefore, ~\citet{dong2023raft} try to remove the PPO training in the above process and propose 
a novel Reward rAnked FineTuning (\emph{RAFT}) method, which uses an existing reward model to select the best set of training samples based on the model outputs. Specifically, \emph{RAFT} first samples a large batch of instructions, then uses the current LLMs to respond to these instructions. These data are then ranked by the reward model and only 
top $\frac{1}{k}$ instances are applied for SFT. 
\emph{RAFT} can also be used in offline human preference learning where the global instruction set is continually updated with the top-ranked instructions in each batch. This contiguously updates the global instruction set to improve training data quality at each step.



\subsection{Offline Human Preference Training}
\label{offlinetraining}
Although the above online algorithms have been shown effective in learning human preference, implementing these algorithms could be non-trivial because its training procedure requires interaction between policy, behavior policy, reward, and value model, which requires many hyper-parameters to be tuned to achieve better stability and performance. To avoid this issue, researchers also explore learning human preferences in an offline fashion.

\subsubsection{Ranking-based Approach}
As human preferences are often expressed as a ranking result over a set of responses, some research efforts directly incorporate the ranking information into the LLMs fine-tuning stage. 
~\citet{rafailov2023direct} propose \emph{Direct Preference Optimization} (DPO), which implicitly optimizes the same objective as existing RLHF algorithms (i.e., reward function with a KL-divergence term) discussed above. Specifically, the DPO training objective can be written as:
\begin{equation}
    \mathcal{L}_\text{DPO} = \log \sigma \left[\beta \log (\frac{\pi_{\theta}(y_w\mid x)}{\pi_{\text{SFT}}(y_w\mid x)} \cdot \frac{\pi_{\text{SFT}}(y_l\mid x)}{\pi_{\theta}(y_l\mid x)})\right]
\end{equation}
where $(x, y_w, y_l)$ is one instruction and two of the corresponding outputs with $y_w$ ranked higher than $y_l$. 
Similarly,~\citet{song2023preference} propose \emph{Preference Ranking Optimization} (PRO) method, an extended version of reward model training objective proposed in~\citet{ziegler2019finetuning}, to further fine-tune LLMs to align with human preference. Given instruction $x$ and a set of responses with human preference order $y^1\succ y^2\succ \cdots \succ y^n$, the objective can be defined as follows:
\begin{equation}
\mathcal{L}_{\text{PRO}} = -\sum_{k=1}^{n-1} \log \frac{\exp\left(\pi_{\theta}(y^k\mid x)\right)}{\sum_{i=k}^{n}\exp\left(\pi_{\theta}(y^i\mid x)\right)}
\end{equation}
PRO also adds SFT training objective for the regularization purpose.
Instead of adapting the reward training objective,~\citet{zhao2023calibrating} 
take the first step to calibrate the sequence likelihood using various ranking functions, including rank loss, margin loss, list rank loss~\cite{liu-etal-2022-brio} and expected rank loss~\cite{edunov-etal-2018-classical}. In addition, they also explore to use SFT training objective and KL-divergence as the regularization term. The experiment results on various text generation tasks show that the rank loss with the KL-divergence term performs the best.
However, this paper only uses the BERTScore~\cite{Zhang*2020BERTScore:} between each candidate output and the ground-truth reference to simulate human preferences and they only conduct experiment on small pre-trained language models (i.e., no larger than 2B).~\citet{yuan2023rrhf} propose RRHF, which further optimizes LLaMA-7B to align with human preferences using a similar framework described above. RRHF is based on the list rank loss, but removes the margin terms based on the empirical results. In addition, different from~\citet{liu-etal-2022-brio}, RRHF finds that the SFT training objective is more effective and efficient than KL-divergence in preventing LLMs from over-fitting. These results show that different ranking strategies should be adapted for LLMs with different size. 


% Specifically, given a response $y_i$, RRHF defines its score $p_i$ as length-normalized log probability as follows:
% \begin{equation}
%     p_i = \frac{\sum_t \log P_{LLM}(y_{i,t}|x,y_{i,<t})}{\|y_i\|}
% \end{equation}
% where $P_{LLM}$ donates our base LLM. Motivated by~\citet{liu-etal-2022-brio}, the final ranking loss is then given by:
% \begin{equation}
%     L_{rank} = \sum_{r_i<r_j}\max(0,p_i-p_j)
% \end{equation}
% where $r_i$ and $r_j$ denote human preference over response $y_i$ and $y_j$, respectively. 

% Figure environment removed

\subsubsection{Language-based Approach}
As reinforcement learning algorithms are hard to optimize and LLMs have strong text understanding ability, some works propose to directly use natural language to inject human preference via SFT.~\citet{openchat} introduce the concept of ``conditional behavior cloning'' from offline reinforcement learning literature~\cite{nguyen2022conserweightive} to train LLMs to distinguish high-quality and low-quality instruction responses. Specifically, they design different language-based prefixes for different quality responses (e.g., high-quality response with ``Assistant GPT4:'' and low-quality response with ``Assistant GPT3:''). This approach can effectively leverage both low- and high-quality training data to align LLMs with humans. Chain of Hindsight (CoH)~\cite{liu2023languages}, on the other hand,  directly incorporates human preference as a pair of parallel responses discriminated as low-quality or high-quality using natural language prefixes. As shown in Figure~\ref{fig:coh}, after assigning human feedback to each model output, CoH concatenates the input instructions, LLMs outputs, and the corresponding human feedback together as the input to LLMs. Note that CoH only applies the fine-tuning loss to the actual model outputs, rather than the human feedback sequence and the instructions. During inference, CoH directly puts position feedback (e.g., good) after the input instructions to encourage the LLMs to produce high-quality outputs. It is worthnoting that, similar to~\citet{Liu2022TowardsBF,DBLP:conf/nips/Ouyang0JAWMZASR22}, CoH also incorporates SFT objectives and random words masking to prevent LLMs from over-fitting. 

Alternative approach is to 
%\subsubsection{Revision-based Approach}
explicitly incorporate revision-based instructions into LLMs training. Some preliminary studies have shown that many existing state-of-the-art LLMs have the capability to improve the quality of their responses when explicitly prompting them to do so~\cite{chen2023teaching}. 
Motivated by these findings,~\citet{liu2022second} recommend training LMs to produce edit operations between source (i.e., low-quality responses) and target (i.e., high-quality responses) sequences, which are subsequently integrated into a dynamic programming framework.
~\citet{liu2023training} propose a novel type of instruction called \emph{realignment}, designed to revise responses based on previously generated low-quality feedback and instructions. This compiled data is employed to instruct LLMs to self-correct when they generate bad responses.
Similarly, ~\citet{selfee2023} accumulate a multi-turn dialogue corpus utilizing this self-correction mechanism built with the ChatGPT models. Each dialogue starts with standard instructions, such as those from the Stanford Alpaca dataset. After ChatGPT has responded to the initial instructions, further revisions are explicitly requested until ChatGPT elects to terminate. They found that LLMs trained using these dialogues demonstrated an effective capacity to elevate the quality of their own responses.

% ~\citet{selfee2023} collect a multi-turn dialogue corpus with this self-correction mechanism from the ChatGPT models. Specifically, each dialogue starts from normal instructions (e.g., from the Stanford Alpaca dataset). After ChatGPT responds to the instructions, they further explicitly ask for revision until ChatGPT decides to stop. They find that LLMs trained with such dialogues can effectively learn to improve the quality of their own responses.

\subsection{Parameter-Effective Training}
\label{parametereffective}
Directly fine-tuning all parameters in large language models (LLMs) would theoretically enable these models to adhere to provided instructions. However, this approach demands not only substantial computational resources, such as vast GPU memory but also extensive datasets for instruction training. In an effort to mitigate both computational and data requirements for constructing instruction-following LLMs, one potential route is the implementation of \emph{Parameter-Effective Fine-tuning} strategies. 
Specifically, these methods froze the major part of LLM parameters and only train a limited set of additional parameters. 

\paragraph{Supplementary Parameters}
Building upon this strategy, prefix tuning~\cite{li-liang-2021-prefix} and prompt tuning~\cite{lester-etal-2021-power} are inspired by the successful application of textual prompts in pre-trained language models~\cite{NEURIPS2020_1457c0d6}. 
These methods either prepend trainable tokens to the input layer or each hidden layer, leaving the parameters of LLMs frozen during fine-tuning. Subsequently, ~\citet{he2022towards,chen2023parameterefficient} consolidated these strategies into unified frameworks, fostering more effective solutions for parameter-efficient fine-tuning. 

\paragraph{Shadow Parameters}
While the above methodologies introduce supplementary parameters to LLMs, the following methods focus on training the weight representing model parameter variance without modifying the number of total model parameters during inference.
For instance, Low-Rank Adaptation (LoRA)~\cite{hu2022lora} suggests the addition of pairs of rank-decomposition trainable weight matrices (i.e., update matrices) to the existing weights, which are kept frozen. 
For example, given a neural layer $h=W_0x$, LoRA modifies the forward pass as follows:


\begin{equation}
    h=W_0x+BAx
\end{equation}
where $W_0 \in \mathbb{R}^{d\times k}$, $B \in \mathbb{R}^{d\times r}$, $A \in \mathbb{R}^{r\times k}$, with the rank $r \ll \min(d,k)$. LoRA only updates the parameters of $A$ and $B$ during  training.
Despite being effective, LoRA equally allocates parameter budgets over the whole LLMs, ignoring the varying importance of different weight parameters.~\citet{zhang2023adaptive} propose AdaLoRA to combat this issue. Specifically, AdaLoRA first calculates the parameter importance using the training gradient and then determines the $r$ values for different parameters matrix.~\citet{dettmers2023qlora} propose QLoRA that further improves over LoRA by reducing memory usage, enabling a 65B LLM to be fine-tuned using a single 48G GPU. Specifically, QLoRA quantizes the transformer backbone model to 4-bit precision and uses paged optimizers to handle memory spikes. 

\paragraph{Trade-offs For Parameter-efficient Training}
There are some successful applications of parameter-efficient training technologies, including the \emph{Alpaca-LoRA} project~\footnote{\url{https://github.com/tloen/alpaca-lora}}, which is based on the Hugging Face's PEFT library~\cite{peft} to train Alpaca using a single commercial GPU and~\citet{DBLP:journals/corr/abs-2304-01196}, which apply LoRA to all linear layers in LLaMA to improve its adaption capabilities. However, such an effective training approach could also result in under-fitting issues.~\citet{sun2023comparative} find that given the same set of training instructions, LLMs with LoRA perform worse than the fully fine-tuned ones. Furthermore, they also show that when using LoRA, it is preferable to use larger LLMs than larger training instruction datasets because the former solution uses less training costs and achieves better performance than the later one.
