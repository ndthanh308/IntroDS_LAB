\documentclass[11pt,a4paper]{amsart}
%
\usepackage{amssymb,amsmath,latexsym}
\usepackage{graphicx}
\usepackage{bm,bbm}
\usepackage{color}
\usepackage{tikz}
\usepackage{ifthen}
\usetikzlibrary{snakes}
\usetikzlibrary{patterns}
\usepackage{algorithm}
\usepackage{setspace}
\usepackage[noend]{algpseudocode}
\usepackage{pgfplots}
\usepackage{enumitem}
\usepackage{array,blkarray}
\usepackage{bigdelim}
\usepackage{listings} 
\usepackage[hidelinks]{hyperref}
\usepackage{isotope}
\usepackage{stmaryrd}
%\usepackage{MnSymbol}
\usepackage[]{placeins}
\usepackage[normalem]{ulem}
%\usepackage{mathabx}
\usepackage[
%matha,
mathb,
mathx,
]{mathabx}

%\usepackage{fdsymbol}
%
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\newcommand{\calL}{\mathcal{L}}
\newcommand{\StiefelV}{\mathrm{St}(p,V)}
\newcommand{\Stiefelpn}{\mathrm{St}(p,n)}
\newcommand{\Sskew}{\mathcal{S}_{\rm skew}(p)}
\newcommand{\Ssym}{\mathcal{S}_{\rm sym}(p)}
\newcommand{\equivcl}[1]{\lcorners #1\rcorners} %mathabx
\newcommand{\im}{\mathrm{im}}
\newcommand{\cbf}{\bm{c}}
%
\newcommand{\Dcomment}[1]{{\color{myOrange}{\bf D:} #1}}
\newcommand{\Rcomment}[1]{{\color{myGreen2}{\bf R:} #1}}
\newcommand{\Tcomment}[1]{{\color{cyan}{\bf T:} #1}}
%
%\definecolor{corrRed}{RGB}{18,124,175} 
%\newcommand{\corr}[1]{{\color{corrRed}{#1}}}
%
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{conclusion}[theorem]{Conclusion}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
%\theoremstyle{lemma}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{coroll}[theorem]{Corollary}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{remarks}[theorem]{Remarks}
\newtheorem{assumption}[theorem]{Assumption}
\numberwithin{theorem}{section}
\numberwithin{equation}{section}
\numberwithin{table}{section}
\numberwithin{figure}{section}
%
\newcommand{\quotes}[1]{``#1''}
\newcommand{\ugs}{u_{\mbox{\tiny\rm gs}}}
\newcommand{\ues}{u_{\mbox{\tiny\rm es}}}
\newcommand{\ulin}{u_{\mbox{\tiny\rm lin}}}
\newcommand{\lambdags}{\lambda_{\mbox{\tiny\rm gs}}}
\newcommand{\lambdalin}{\lambda_{\mbox{\tiny\rm lin}}}

\makeatletter
\newsavebox{\@brx}
\newcommand{\llangle}[1][]{\savebox{\@brx}{\(\m@th{#1\langle}\)}%
	\mathopen{\copy\@brx\kern-0.5\wd\@brx\usebox{\@brx}}}
\newcommand{\rrangle}[1][]{\savebox{\@brx}{\(\m@th{#1\rangle}\)}%
	\mathclose{\copy\@brx\kern-0.5\wd\@brx\usebox{\@brx}}}
\makeatother
%
\input{def}
%
\textheight=225mm
\textwidth=150mm
\evensidemargin=30.0mm
\oddsidemargin=30.0mm
\hoffset=-25.4mm
%
\begin{document}
%
% alt: Riemannian Newton methods for Kohn--Sham-like energy minimization problems
\title[Riemannian Newton methods for problems of Kohn--Sham type]{Riemannian Newton methods for energy\\ minimization problems of Kohn--Sham type}
\author[]{R.~Altmann$^{\dagger}$, D.~Peterseim$^{\ddagger}$, T.~Stykel$^{\ddagger}$}
\address{${}^{\dagger}$ Institute of Analysis and Numerics, Otto von Guericke University Magdeburg, Universit\"atsplatz 2, 39106 Magdeburg, Germany}
\address{${}^{\ddagger}$ Department of Mathematics \& Centre for Advanced Analytics and Predictive Sciences (CAAPS), University of Augsburg, Universit\"atsstr.~12a, 86159 Augsburg, Germany}
\email{robert.altmann@ovgu.de, \{daniel.peterseim, tatjana.stykel\}@uni-a.de}
\thanks{The work of Daniel Peterseim is part of a project that has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (Grant agreement No.~865751 --  RandomMultiScales).}
%
\date{\today}
\keywords{}
%
%
%=============================================================================
%=========  Abstract
%=============================================================================
\begin{abstract}
This paper is devoted to the numerical solution of constrained energy mi\-ni\-mization problems arising in computational physics and chemistry such as the Gross--Pitaevskii and Kohn--Sham models. In particular, we introduce the Riemannian Newton methods on the infinite-dimensional Stiefel and Grassmann manifolds. We study the geometry of these two manifolds, its impact on the Newton algorithms, and present expressions of the Riemannian Hessians in the infinite-dimensional setting, which are suitable for variational spatial discretizations. A series of numerical experiments illustrates the performance of the methods and demonstrates its supremacy compared to other well-established schemes such as the self-consistent field iteration and gradient descent schemes.  
\end{abstract}
%
%
%=============================================================================
%=========  Title / Contents
%=============================================================================
\maketitle
%\setcounter{tocdepth}{3}
%\tableofcontents
%
{\tiny {\bf Key words.} Riemannian optimization, Stiefel manifold, Grassmann manifold, Newton method, Kohn--Sham model, Gross--Pitaevskii eigenvalue problem}\\
\indent
{\tiny {\bf AMS subject classifications.} {\bf  65K10}, {\bf 65N25}, {\bf 81Q10}} 
%
%65K10: Numerical optimization and variational techniques
%65N25: Numerical analysis -- PDEs, BVPs -- Eigenvalue probems
%81Q10: Quantum theory -- Mathematical theory -- Selfadjoint op, spectral analysis
%
%
%=============================================================================
%=========  Introduction
%=============================================================================
\section{Introduction}
The Kohn--Sham model~\cite{HohK64,KohS65,LeB05} is a prototypical example of a constrained energy minimization problem stated on the infinite-dimensional Stiefel manifold. This means that the sought-after minimizer is
%contains 
a~$p$-tuple of $L^2$-ortho\-nor\-mal functions. Another well-known example is the Gross--Pitaevskii model for Bose-Einstein condensates of ultracold bosonic gases~\cite{LSY01,PS03}. Here, the special case~$p=1$ is of interest, where we seek a single (minimizing) function on the unit sphere in $L^2$, representing a unit mass constraint. 
% unifying character of the paper 
Since these two applications are relevant for different communities, numerical methods are mostly considered separately. One aim of this paper is to give a~unified approach to solving energy minimization problems. More precisely, we introduce Riemannian Newton methods for minimizing energy functionals of Kohn--Sham type, which also includes the Gross--Pitaevskii model. 

% numerical methods for GP
The numerical solution of the Gross--Pitaevskii model has been studied extensively in recent years. The most common numerical techniques are iterative methods based on {\em Riemannian {\em(}conjugate{\em)} gradient  descent methods} or {\em discretized Riemannian gradient flows} in various metrics~\cite{GaP01,BaD04,BCL06,KaE10,RSS09,DanP17,HenP20,Zha22,CheLLZ23}. A~conceptually different approach is the \mbox{\em $J$-method}~\cite{JarKM14,AltHP21} with its inimitable sensitivity with regard to spectral shifts, allowing remarkable speed-ups in a~Rayleigh quotient iteration manner. 
% 
Reformulating the minimization problem as an~eigenvalue problem with eigenvector nonlinearity -- also known as nonlinear eigenvector problem -- the {\em self-consistent field iteration} (SCF) can be employed; see~\cite{Roo51,Can00,DioC07}. This method involves the solution of a linear eigenvalue problem in each step and is strongly connected to the Newton method~\cite{JarU22,HenJ23}. Considering the extended nonlinear system including the normalization constraint also allows a direct application of Newton or Newton-type methods~\cite{BaoT03,COR09,DuL22}. For an extended review on numerical methods for the Gross--Pitaevskii model, we refer to~\cite{HenJ23}. 

% numerical methods for KS
Most of the above approaches (with appropriate adjustments) have been applied to the Kohn--Sham model as well. This includes the~{\em direct constrained minimization algorithm}~\cite{YanMW06,AloA09,SchRNB09} and the {\em energy-adapted gradient descent method} \cite{AltPS21} -- both based on Riemannian optimization -- as well as {\em gradient flow schemes}~\cite{DaiWZ20,HuWJ23}. Moreover, the SCF algorithm with different types of mixing  is very popular in the computational chemistry community; see, e.g., \cite{CanL00,Can01,LiuWWUY15,CanKL21,BaiLL22}. For a discretized and simplified Kohn--Sham model (without the external potential and the exchange-correlation energy), global convergence and local second-order convergence of an~inexact Riemannian Newton method on the Grassmann manifold has been shown in~\cite{ZhaoBJ15}. 
  
% content of paper
In this paper, the point of origin is an energy functional defined on the infinite-dimen\-sio\-nal Stiefel manifold, which we introduce in Section~\ref{sect:model}. For a better understanding, we recall definitions and properties of the Stiefel manifold and corresponding retractions in Section~\ref{sect:Stiefel}. Moreover, we provide formulae for the Riemannian gradient and the Riemannian Hessian which are needed for the Newton iteration. Since the considered energy functional is invariant under orthogonal matrices, we also discuss the infinite-dimensional Grassmann manifold and examine a~connection of its tangent space to a~certain subspace of the tangent space of the Stiefel manifold. The resulting Newton algorithms are then subject of Section~\ref{sect:Newton}. In particular, we present an inexact Riemannian Newton method on the Grassmann manifold. In Section~\ref{sect:numerics}, we consider the two mentioned examples of the Gross--Pitaevskii and the Kohn--Sham model in more detail. For both applications, we derive the formulae including a spatial discretization and illustrate the supremacy of the inexact Newton approach compared to well-established methods such as the SCF iteration and gradient descent schemes. 
\smallskip

\textbf{Notation} 
The sets of $p\times p$ real symmetric and skew-symmetric matrices are denoted by $\Ssym$ and $\Sskew$, respectively. For $M\in\R^{p\times p}$, we write~$\sym M=\frac{1}{2}(M+M^T)$ for the symmetric part, and~$\trace M$ denotes the trace of $M$. Further, $I_p$ and $0_p$ denote the $p\times p$ identity and zero matrices, respectively. The expression~$\diag(M)$ defines the column vector consisting of the diagonal elements of $M\in\R^{n\times n}$ and~$\Diag(v)$ denotes the diagonal matrix with components of the vector $v\in\R^n$ on the diagonal.
%
%
%=============================================================================
%=========  Model
%=============================================================================
\section{The Energy Functional and Nonlinear Eigenvector Problems}\label{sect:model}
For a~given spatial domain $\Omega\subseteq \R^d$, $d\le3$, we consider the Hilbert spaces $L^2(\Omega)$ and
 \mbox{$\tilde{V}\!\subseteq\! H^1(\Omega)$}. 
%For the latter, we assume $\tilde{V}$ to be dense in $L^2(\Omega)$. 
For $p\geq 1$, we further define the Hilbert spaces $V=\tilde{V}^p$ and $H=[L^2(\Omega)]^p$ of $p$-frames. Throughout this paper, we assume that~$V$ is dense in $H$ and that $V\subseteq H\subseteq V^*$ form a~Gelfand triple, where $V^*$ denotes the dual space of $V$. 

For $\vbf = (v_1, \dots, v_p), \wbf = (w_1, \dots, w_p) \in H$, we define the~dot product 
\[
\vbf\cdot\wbf = \sum_{j=1}^p v_j w_j.
\] 
On the pivot space $H$, we further introduce an~outer product
\begin{equation}\label{eq:outer}
	\out{\vbf}{\wbf} 
	= \begin{bmatrix}
		(v_1, w_1)_{L^2(\Omega)} & \dots & (v_1, w_p)_{L^2(\Omega)} \\
		\vdots & \ddots & \vdots \\ 
		(v_p, w_1)_{L^2(\Omega)} & \dots & (v_p, w_p)_{L^2(\Omega)}
	\end{bmatrix}
	\in \R^{p\times p}
\end{equation}
%
and an~inner product
%
\begin{equation}\label{eq:inner}
	(\vbf, \wbf)_H 
	= \sum_{j=1}^p (v_j, w_j)_{L^2(\Omega)} 
	= \trace\, \out{\vbf}{\wbf}.
\end{equation}
%
%where $\trace$ denotes the trace of a~corresponding matrix. 
The inner product \eqref{eq:inner} induces the norm~$\|\vbf\|_H=\sqrt{(\vbf,\vbf)_H}$ on $H$. The canonical identification $\calI\colon V\to V^*$ is defined by 
%
\[
	\langle\,\calI\vbf,\wbf\rangle=(\vbf,\wbf)_H\qquad 
	\text{for all } \vbf,\wbf\in V,
\]
%
where $\langle\,\cdot\,,\cdot\,\rangle$ denotes the duality pairing on $V^*\times V$. This identification operator can also be written in the form~$\calI = \frakj^*\circ \fraki_H\circ \frakj$ with the trivial embedding~$\frakj\colon V\to H$ (the injective identity operator), the Riesz isomorphism~$\fraki_H\colon H\to H^*$, which reads $\fraki_H(u) = (u,\,\cdot\,)_H$, and the adjoint operator $\frakj^*\colon H^*\to V^*$ satisfying $\frakj^*(f)=f\circ \frakj$ for all $f\in H^*$. Since all these operators act componentwise, we have $\calI(\vbf\Lambda) = \calI(\vbf)\Lambda$ for all $\vbf\in V$ and $\Lambda\in\R^{p\times p}$. 
%
Moreover, since $V$ is a dense subspace of $H$, so is $\frakj(V)$. Hence, $\frakj^*$ is injective as well as~$\calI$ as the composition of the injective operators. As a result, $\calI$ has a~left inverse $\calJ\colon V^*\to V$ such that $\calJ\calI\vbf=\vbf$ for all $\vbf\in V$. 
%
%
%=============================================================================
\subsection{Energy and applications}\label{sect:model:energy}
For a $p$-frame~$\phibf\in V$, we consider the energy functional 
%
\begin{align}
	\calE(\phibf)
	&= \frac 12\, \int_{\Omega} \tr\bigl((\nabla \phibf(x))^T\nabla\phibf(x)\bigr) \dx  
	+ \int_{\Omega} \vartheta(x)\, \rho(\phibf(x)) \dx 
	+ \frac12\, \int_{\Omega} \varGamma(\rho(\phibf(x))) \dx
	\label{eq:energy}
\end{align}
%
with an external potential~$\vartheta$, the density function $\rho(\phibf) = \phibf\cdot \phibf$, 
%$\Gamma(\rho) = \int_0^\rho \gamma(s) \ds$. 
and the smooth nonlinearity $\varGamma(\rho)$. Our aim is to minimize this energy functional on the \emph{infinite-dimensional Stiefel manifold of index $p$} given by 
%
\begin{equation}\label{eq:StiefelV}
	\StiefelV
	= \big\{ \phibf\in V\enskip :\enskip \out{\phibf}{\phibf} = I_p \big\}.
\end{equation}
%
In other words, we are interested in solving the constrained minimization problem 
%
\begin{equation}\label{eq:minSt}
	\min_{\phibf\in \StiefelV} \calE(\phibf). 
\end{equation}
%
A~state of lowest energy is called the {\em ground state}. Such states play an~important role in quantum-mechanical models  as they represent a most stable configuration of atoms and molecules. These models include two famous applications in computational physics and chemistry. 
%
\begin{example}[Gross--Pitaevskii model]
\label{exp:GPEVP}
For $p=1$ and $\varGamma(\rho) = \frac 12 \kappa \rho^2$ with $\kappa\in\mathbb{R}$, the energy functional takes the form
%
\begin{align}
	\calE_{\rm GP}(\phi)
	= \frac 12\, \int_{\Omega} \|\nabla \phi(x)\|^2 \dx
	+ \int_{\Omega} \vartheta(x)\, \phi(x)^2 \dx 
	+ \frac\kappa4\, \int_{\Omega} \phi(x)^4 \dx.
	\label{eq:energyGP}
\end{align}
%
This is the well-known Gross--Pitaevskii energy used in the modeling of Bose--Einstein condensates of ultracold bosonic gases~\cite{LSY01,PS03}. Here,  $\vartheta \in L^\infty(\Omega)$ is the magnetic trapping potential, $\phi\in H_0^1(\Omega)$ is the quantum state of the Bose--Einstein condensate, and~$\kappa$ characterizes the strength and the direction of particle interactions. 
\end{example}
%
\begin{example}[Kohn--Sham model]
\label{exp:KS}
The (non-local) nonlinearity  
%
\[
	\varGamma(\rho) 
    = \rho\, \int_{\Omega} \frac{\rho(\phibf(y))}{\|x-y\|} {\,\rm d}y 
    + 2\,\rho\,\epsilon_\text{xc}(\rho)
\]
% 
yields the Kohn--Sham energy functional 
%
\begin{align}
	\calE_{\rm KS}(\phibf)
	&= \frac 12\, \sum_{j=1}^p \int_{\Omega} \|\nabla \phi_j(x)\|^2 \dx
	+ \int_{\Omega} \vartheta_\text{ion}(x)\, \rho(\phibf(x)) \,{\rm d} x \notag \\
	&\qquad+ \frac 12 \int_{\Omega}\int_{\Omega} \frac{\rho(\phibf(x))\, \rho(\phibf(y))}{\|x-y\|} {\,\rm d}y \dx
	+ \int_{\Omega}\rho(\phibf(x))\, \epsilon_\text{xc}(\rho(\phibf(x))) \dx,
	\label{eq:energyKS}
\end{align}
%
where $\phibf$ denotes a~wave function with $p$ components called single-particle orbitals and $\rho(\phibf)$ is the electronic charge density.  Moreover, $\vartheta_\text{ion}$ is the ionic potential, and $\epsilon_\text{xc}(\rho)$ is the exchange-correlation energy per particle in a~homogeneous electron gas of density~$\rho$. This model is based on the so-called \emph{density functional theory} \cite{HohK64}, which allows a~significant reduction of the degrees of freedom \cite{KohS65,LeB05,CanCM12}. The last integral in~\eqref{eq:energyKS} is a~local density approximation to the exchange-correlation energy obtained by using semi-empirically knowledge of the model~\cite{PerZ81}. In the Kohn--Sham model,  a~ground state corresponds to the low-energy wave function of the considered molecule and the orthogonality condition $\out{\phibf}{\phibf} = I_p$ means that there is no interaction between the electrons in different orbitals. 
\end{example}
%
At this point, it should be emphasized that, since the energy functional $\calE$ in  \eqref{eq:energy} is invariant under orthogonal transformations, i.e. $\calE(\phibf)=\calE(\phibf Q)$ for all orthogonal matrices $Q\in \R^{p\times p}$, the optimal solution to the minimization problem~\eqref{eq:minSt} is not unique. 
To overcome this difficulty, we will transfer this problem to the infinite-dimensional Grassmann manifold defined in Section~\ref{sec:Grassmann}.
%
%
%=============================================================================
\subsection{Connection to nonlinear eigenvector problems}\label{sect:model:NLEVP}
We observe that the directional derivative of $\calE$ from~\eqref{eq:energy} at $\phibf\in V$ along $\wbf\in V$ has the form
\[
	{\rm D}\calE(\phibf)[\wbf] 
	= a_\phibf(\phibf,\wbf),
\]
where 
\begin{align}
	a_\phibf(\vbf,\wbf)
	= \int_\Omega \tr\bigl((\nabla \vbf)^T\nabla\wbf\bigr) \dx 
	+ 2 \int_\Omega  \vartheta\, \vbf\cdot\wbf \dx 
	+ \int_\Omega  \gamma(\rho(\phibf))\, \vbf\cdot\wbf \dx 
	\label{eq:aphi}
\end{align}
%  
with $\gamma(\rho)=\frac{\rm d}{{\rm d}\rho}\varGamma(\rho)$. One can see that for fixed $\phibf\in V$, $a_\phibf$ is a symmetric bilinear form on $V\times V$. Further note that~$a_\phibf$ exhibits a special structure, namely  
%
\begin{equation}\label{eq:aphi}
	a_\phibf(\vbf,\wbf)
	= \sum_{j=1}^p \tilde a_\phibf(v_j, w_j) 
\end{equation}
%
with a symmetric bilinear form~$\tilde a_\phibf\colon \tilde{V}\times \tilde{V}\to \R$ given by
\[
	\tilde a_\phibf(v, w) 
	= \int_\Omega (\nabla v)^T\nabla w \dx 
	+ 2 \int_\Omega \vartheta\, v w \dx 
	+ \int_\Omega \gamma(\rho(\phibf))\, v w \dx.
\]
Within this paper, we assume that~$\tilde a_\phibf$ is bounded and coercive on~$\tilde{V}\times \tilde{V}$. Obviously, the bilinear form $a_\phibf$ inherits these properties such that $a_{\phibf}$ is also bounded and coercive on~$V\times V$. 

Introducing the Lagrangian $\calL(\phibf,\Lambda)=\calE(\phibf)-\frac{1}{2}\trace\big(\Lambda^T(\out{\phibf}{\phibf} - I_p)\big)$ with a~Lagrange multiplier $\Lambda\in\Ssym$, the first-order necessary optimality conditions for the minimization problem \eqref{eq:minSt} yield the~nonlinear eigenvector problem (NLEVP)
%
\begin{subequations}
\label{eq:NLEVP}
\begin{align}
	a_{\phibf_*}(\phibf_*,\wbf)-(\phibf_*\, \Lambda_*, \wbf)_H  
	&= 0 \qquad \text{for all }\wbf\in V,\\%[0.3em]
	\out{\phibf_*}{\phibf_*} - I_p 
	&= 0_p 
\end{align}
\end{subequations}
%
with unknown~$\phibf_*\in V$, which is referred to as  the eigenvector, and $\Lambda_*\in\Ssym$, whose eigenvalues are the lowest $p$ eigenenergies of the system.  
%
Yet another formulation of the NLEVP~\eqref{eq:NLEVP} follows from the special structure of the bilinear form~$a_\phibf$ in~\eqref{eq:aphi}: seek~$\phibf_*=(\phi_{*,1}, \dots, \phi_{*,p})\in \StiefelV$ and $p$~eigenvalues $\lambda_1,\dots,\lambda_p\in\R$ such that 
%
\begin{align}
	\label{eq:NLEVPweakComponents}
	\tilde a_{\phibf_*}(\phi_{*,j}, v) 
	= \lambda_j\, (\phi_{*,j}, v)_{L^2(\Omega)}
	%	\qquad\text{ for all }(v_1, \dots, v_p)\in V.
	\qquad\text{ for all } v\in \tilde V.
\end{align}


For fixed $\phibf\in V$, we introduce the~operator $\calA_\phibf \colon V\to V^*$, defined by   
%
\[
	\langle\calA_\phibf \,\vbf, \wbf\rangle 
	= a_\phibf(\vbf,\wbf) \qquad\text{for all } \vbf,\wbf\in V.
\]
%
Then the NLEVP \eqref{eq:NLEVP} can be written as
%
\begin{subequations}
	\label{eq:NLEVPop}
	\begin{align}
		%\arraycolsep=2pt
		%\begin{array}{rcl}
		\calA_{\phibf_*} \phibf_*-\calI(\phibf_* \Lambda_*) 
		&= \mathbf{0}^{*}, \label{eq:NLEVPop:a}\\
		\out{\phibf_*}{\phibf_*} - I_p 
		&=  0_p, \label{eq:NLEVPop:b}
	\end{align}
\end{subequations}
%
where $\mathbf{0}^{*}\in V^*$ is the zero functional. Using the left inverse $\calJ$ of $\calI$, we find that 
%
\begin{align}\label{eq:Lambda}
	\Lambda_*
	= \out{\phibf_*}{\phibf_*} \Lambda_*
	= \out{\phibf_*}{\phibf_*\, \Lambda_*}
	= \out{\phibf_*}{\calJ\!\calA_{\phibf_*} \,\phibf_*}.
\end{align}
%
\begin{remark}\label{rem:symmetryBracket}
Due to the symmetry of the bilinear form~$\tilde a_\phibf$, we conclude that 
%
\begin{align*}
	\big( \phi_i, (\calJ\!\calA_\phibf \,\phibf)_j \big)_{L^2(\Omega)}
%	= \big( \phi_i, \calJ(\calA_\phibf \,\phibf)_j \big)_{L^2(\Omega)}
%	= \big( \phi_i, \calJ\tilde\calA_\phibf \,\phi_j \big)_{L^2(\Omega)}
%	= \langle \phi_i, \tilde\calA_\phibf \,\phi_j \rangle
	= \tilde a_{\phibf}(\phi_i, \phi_j)
	= \tilde a_{\phibf}(\phi_j, \phi_i)
	= \big( \phi_j, (\calJ\!\calA_\phibf \,\phibf)_i \big)_{L^2(\Omega)}, \quad i,j=1,\ldots,p.
\end{align*}
%
This means that $\out{\phibf}{\calJ\!\calA_\phibf \,\phibf}$ is symmetric for any $\phibf\in V$. 
\end{remark}
%
%
%=============================================================================
%=========  Stiefel and Grassmann
%=============================================================================
\section{The Infinite-dimensional Stiefel and Grassmann Manifolds}\label{sect:Stiefel}
In this section, we summarize definitions and properties of the infinite-dimensional Stiefel and Grassmann manifolds and their tangent spaces, which lay the foundation of the Riemannian optimization schemes in the upcoming section. 
%
%
%=============================================================================
\subsection{The Stiefel manifold}
We consider the infinite-dimensional Stiefel manifold $\StiefelV$ defined in~\eqref{eq:StiefelV}. It is an~embedded submanifold of the Hilbert space~$V$ and has co-dimension $p\,(p+1)/2$; see \cite{AltPS21}.  The {\em tangent space} of $\StiefelV$ at $\phibf\in \StiefelV$ is given by  
%
\begin{align}\label{eq:TangSpaceSt} 
	T_\phibf\,\StiefelV
	= \big\{ \etabf\in V\enskip :\enskip \out{\etabf}{\phibf} + \out{\phibf}{\etabf} = 0_p \big\}. 
\end{align}
%
The Riemannian structure of the Stiefel manifold $\StiefelV$ strongly depends on an underlying metric.
Within this paper, we equip $\StiefelV$ with the Hilbert metric given by
%
\begin{equation}\label{eq:Hmetric}
	g(\etabf,\zetabf)
	=(\etabf,\zetabf)_H
	=\trace\, \out{\etabf}{\zetabf},
	\qquad \etabf,\zetabf\in T_\phibf\,\StiefelV. 
\end{equation}
The {\em normal space} with respect to $g$ is then defined as 
%
\[	
	T_\phibf^{\perp} \,\StiefelV
	= \bigl\{ \xbf \in V\enskip :\enskip g(\xbf,\etabf)=0 \text{ for all } \etabf\in T_\phibf\,\StiefelV \bigr\}.
\]
It can also be represented as 
\begin{equation}\label{eq:normalS}
T_\phibf^{\perp} \,\StiefelV
= \bigl\{ \phibf S \in V\enskip :\enskip S\in \Ssym\bigr\}.
\end{equation}
%
Further, any $\ybf\in V$ can be decomposed as $\ybf = \calP_{\phibf}^{}(\ybf) + \calP_{\phibf}^\perp(\ybf)$, where
\begin{align}
\label{eq:def:HProj}	
	\calP_{\phibf}(\ybf)
	= \ybf - \phibf \sym \out{\phibf}{\ybf} \qquad\text{and}\qquad
	\calP_{\phibf}^{\perp}(\ybf)
	=\phibf \sym \out{\phibf}{\ybf}
	%= \ybf - \phibf\, \out{\phibf}{\ybf} + \phibf \myskew \out{\phibf}{\ybf}
\end{align}
%and $\calP_{\phibf}^{\perp}(\ybf)=\phibf \sym \out{\phibf}{\ybf}$ 
are  the orthogonal projections onto the tangent and normal spaces, respectively. 

% Riemannian grad
The \emph{Riemannian gradient} of a~smooth function $\,\calE\colon\StiefelV\to\R$ with respect to the Hilbert metric~$g$ is the unique element $\grad \calE(\phibf)\in T_\phibf\,\StiefelV$ satisfying the condition 
%
\[
	g(\grad \calE(\phibf), \etabf) = \Drm \overline{\calE}(\phibf)[\etabf] \qquad \text{ for all }\etabf\in T_\phibf\,\StiefelV,
\]
%
where $\overline{\calE}$ denotes a~smooth extension of $\calE$ around $\phibf$ in $V$ and $\Drm \overline{\calE}(\phibf)$ is the Fr\'echet derivative of $\,\overline{\calE}$ in $V$.

For the energy functional $\calE$ in~\eqref{eq:energy}, the Riemannian gradient at $\phibf\in \StiefelV$ with respect to $g$ can be determined by using the $L^2$-Sobolev gradient $\nabla\,\overline{\calE}(\phibf)\in V$ which is defined as a Riesz representation of $\Drm\overline{\calE}(\phibf)$ in the Hilbert space~$V$ with respect to the inner product $g$. Then, for all $\wbf\in V$, we have 
%
\[
	\langle\calA_\phibf \,\phibf, \wbf\rangle 
	= a_\phibf(\phibf,\wbf) 
	= \Drm\overline{\calE}(\phibf)[\wbf]
	= \big(\nabla \,\overline{\calE}(\phibf),\wbf \big)_H 
	= \big\langle\calI \,\nabla \,\overline{\calE}(\phibf), \wbf \big\rangle
\]
%
and, hence, $\nabla\,\overline{\calE}(\phibf) =\calJ\!\calA_\phibf\,\phibf$. Furthermore, for all $\etabf\in T_{\phibf}\,\StiefelV$, we obtain 
\[
\big(\grad \calE(\phibf), \etabf\big)_H 
= \Drm\overline{\calE}(\phibf)[\etabf]
= \big(\nabla \,\overline{\calE}(\phibf),\etabf \big)_H. 
\]
%
This implies that 
%
\begin{equation}\label{eq:gradE_H}
	\grad \calE(\phibf) 
	= \calP_{\phibf}\big(\nabla\, \overline{\calE}(\phibf)\big)
	= \calP_{\phibf}\big(\calJ\!\calA_\phibf\,\phibf\big)
	= \calJ\!\calA_\phibf\,\phibf-\phibf\, \out{\phibf}{\calJ\!\calA_\phibf\,\phibf}.
\end{equation}

% Riemannian Hessian
The \emph{Riemannian Hessian} of $\,\calE$ at  $\phibf\in\StiefelV$ with respect to the metric~$g$, denoted by $\Hess\calE(\phibf)$, is a~linear mapping on the tangent space $T_\phibf\,\StiefelV$ into itself which satisfies 
%
\[
	g(\Hess\calE(\phibf)[\etabf],\xibf) 
	= \Drm^2 \overline{\calE}(\phibf)[\etabf,\,\xibf\,] \qquad 
	\text{ for all }\etabf,\xibf\in T_\phibf\,\StiefelV.
\]
%
An alternative definition of the Riemannian Hessian is possible using 
%the covariant derivative with respect to $\etabf$ for 
the Riemannian connection on the Stiefel manifold $\StiefelV$, cf.~\cite[Sect.~5.3]{AbsiMS08} for the finite-dimensional case. 

The following theorem provides two expressions for the Riemannian Hessian of $\calE$ in terms of the directional derivative of $\grad \calE(\phibf)$ and the $L^2$-Sobolev Hessian $\nabla^2\,\overline{\calE}(\phibf)$ of~$\,\overline{\calE}$, which is a~linear operator mapping $\vbf\in V$ onto the Riesz representation of~$\Drm^2\overline{\calE}(\phibf)[\vbf,\,\cdot\,]$ with respect to the inner product~$(\,\cdot\,,\cdot\,)_H$. 
%
\begin{theorem}\label{th:HessSt}
Let $\phibf\in\StiefelV$ and $\etabf\in T_{\phibf}\,\StiefelV$. Then the Riemannian Hessian of a smooth function $\calE\colon\StiefelV\to \mathbb{R}$ admits the expressions
%
\begin{align}
	\Hess\calE(\phibf)[\etabf] 
	& = \calP_{\phibf} \bigl(\Drm\grad\calE(\phibf)[\etabf]\bigr) \label{eq:HessSt1}\\
	& = \calP_{\phibf}\big( \nabla^2\,\overline{\calE}(\phibf)[\etabf]-\etabf\sym \out{\phibf}{\nabla\,\overline{\calE}(\phibf)}\big),\label{eq:HessSt2}
\end{align}
%
where $\nabla\,\overline{\calE}(\phibf)$ and $\nabla^2\,\overline{\calE}(\phibf)$ denote, respectively, the $L^2$-Sobolev gradient and the $L^2$-Sobolev Hessian of a~smooth extension $\,\overline{\calE}$ of $\,\calE$ around $\phibf$ in $V$.
\end{theorem}
%
\begin{proof} 
% claim 1
Since $\StiefelV$ is an embedded submanifold of the Hilbert space $V$, the expression~\eqref{eq:HessSt1} can be shown similarly to the finite-dimensional case \cite[Prop.~5.3.2]{AbsiMS08}. Indeed, for all $\etabf,\xibf\in T_\phibf\,\StiefelV$, we have
%
\begin{align*}
\big(\Hess\calE(\phibf)[\etabf],\xibf\big)_H 
& =  \Drm^2 \overline{\calE}(\phibf)[\etabf,\,\xibf\,] 
=  \Drm\!\big(\! \Drm\overline{\calE}(\phibf)[\xibf]\big)[\etabf]\\
& = \Drm\!\big( \!\grad \calE(\phibf),\xibf\big)_H[\etabf]
= \big(\!\Drm \grad \calE(\phibf)[\etabf],\xibf\big)_H.
\end{align*}
%
This immediately implies \eqref{eq:HessSt1}.  

% claim 2
In order to prove \eqref{eq:HessSt2}, we first compute the directional derivative 
\begin{equation}\label{eq:DgradE}
	\Drm\grad\calE(\phibf)[\etabf] 
	= \Drm\big(\calP_{\phibf}(\nabla\,\overline{\calE}(\phibf)\big)[\etabf]
	 = \calP_{\phibf}(\nabla^2\,\overline{\calE}(\phibf)[\etabf]) 
	 + \Drm \calP_{\phibf}[\etabf]\nabla\,\overline{\calE}(\phibf).
\end{equation}
%
%\begin{align*}
%	& = \lim_{t\to 0}\frac{1}{t} \Big( \calP_{\phibf+t\etabf}\big(\nabla\,\overline{\calE}(\phibf+t\etabf)\big)-\calP_{\phibf}\big(\nabla\,\overline{\calE}(\phibf)\big)\Big) \\
%	& =  \lim_{t\to 0}\frac{1}{t} \Big(\nabla\,\overline{\calE}(\phibf+t\etabf)-(\phibf+t\etabf)\sym\out{\phibf+t\etabf}{\nabla\,\overline{\calE}(\phibf+t\etabf)}\Big.\\
%	& \qquad\qquad\quad\Big. -\nabla\,\overline{\calE}(\phibf)+\phibf\sym\out{\phibf}{\nabla\,\overline{\calE}(\phibf)}\Big)\\
%	& = \nabla^2\,\overline{\calE}(\phibf)[\etabf] - \phibf \sym\out{\phibf}{\nabla^2\,\overline{\calE}(\phibf)[\etabf]} 
%-\etabf \sym\out{\phibf}{\nabla\,\overline{\calE}(\phibf)} \\
%	& \qquad -\phibf \sym\out{\etabf}{\nabla\,\overline{\calE}(\phibf)}\\
%	& = \calP_{\phibf}( \nabla^2\,\overline{\calE}(\phibf)[\etabf]) 
%	- \etabf \sym\out{\phibf}{\nabla\,\overline{\calE}(\phibf)} 
%	- \phibf \sym\out{\etabf}{\nabla\,\overline{\calE}(\phibf)}.
%\end{align*}
%
Let $\cbf(t)\subset \StiefelV$ be a smooth curve defined on a~neighborhood of $t=0$ such that $\cbf(0)=\phibf$ and $\tfrac{{\rm d}}{{\rm d}t}\cbf(0)=\etabf$. Then for all $\ybf\in V$, we have
\begin{align*}
	\Drm \calP_{\phibf}[\etabf]\, \ybf 
	& = \lim_{t\to 0}\frac{1}{t} \big(\calP_{\cbf(t)}(\ybf)-\calP_{\phibf}(\ybf)\big) \\
	& = \lim_{t\to 0}\frac{1}{t} \big(\ybf-\cbf(t)\sym\out{\cbf(t)}{\ybf}-\ybf+\cbf(0)\sym\out{\cbf(0)}{\ybf}\big)\\
	& = -\lim_{t\to 0}\frac{1}{t} \big(\cbf(t)\sym\out{\cbf(t)-\cbf(0)}{\ybf}+(\cbf(t)-\cbf(0))\sym\out{\cbf(0)}{\ybf}\big)\\
	& = -\phibf \sym\out{\etabf}{\ybf}-\etabf\sym\out{\phibf}{\ybf}.
\end{align*}
%
Inserting \eqref{eq:DgradE} into \eqref{eq:HessSt1} and taking into account that 
%
\begin{align*}
	\calP_{\phibf}\big(\Drm \calP_{\phibf}[\etabf] \nabla\,\overline{\calE}(\phibf)\big)
 	&= -\calP_{\phibf}\big(\phibf \sym\out{\etabf}{\nabla\,\overline{\calE}(\phibf)}+\etabf\sym\out{\phibf}{\nabla\,\overline{\calE}(\phibf)}\big) \\ 
 	&= -\calP_{\phibf}\big(\etabf\sym\out{\phibf}{\nabla\,\overline{\calE}(\phibf)}\big), 
\end{align*}
we obtain \eqref{eq:HessSt2}.
\end{proof}
%
In order to derive a formula for the Riemannian Hessian of the energy functional~$\calE$ in~\eqref{eq:energy}, we first compute the second-order derivative
%
\begin{align*}
	\Drm^2\overline{\calE} (\phibf)[\vbf,\wbf] 
	&= \lim_{t\to 0}\ \frac{1}{t}\, \big\langle \calA_{\phibf+t\vbf} (\phibf+t\vbf)
	-\calA_\phibf\,\phibf,\wbf \big\rangle \\
	&= \lim_{t\to 0}\ \frac{1}{t}\,\bigg(
	\int_\Omega \Big(\tr\bigl((\nabla (\phibf+t\vbf))^T\nabla\wbf\bigr)-\tr\bigl((\nabla \phibf)^T\nabla\wbf\bigr)\Big) \dx  \\
	&\qquad\qquad\quad +2\int_\Omega  \vartheta\, \big((\phibf+t\vbf)\cdot\wbf -\phibf\cdot\wbf\big) \dx \\
	&\qquad\qquad\quad + \int_\Omega \big(\gamma(\rho(\phibf+t\vbf))\, (\phibf+t\vbf)\cdot \wbf-\gamma(\rho(\phibf))\, \phibf\cdot \wbf \big)  \dx \bigg)\\
	&= \int_\Omega \tr\bigl((\nabla \vbf)^T\nabla\wbf\bigr) \dx  
	+ 2\int_\Omega  \vartheta\, \vbf\cdot\wbf \dx \\
  	&\quad + \int_\Omega \gamma(\rho(\phibf))\, \vbf\cdot \wbf \dx  
	+ 2\int_{\Omega} \beta(\rho(\phibf)) (\phibf\cdot\vbf)\, (\phibf\cdot \wbf) \dx \nonumber \\
	&= \langle \calA_\phibf\, \vbf + \calB_\phibf\,\vbf,\wbf\rangle,
\end{align*}
%
where $\beta(\rho)=\frac{\rm d}{{\rm d}\rho}\gamma(\rho)$ and the operator $\calB_\phibf\colon V\to V^*$ has the form
%
\begin{equation}\label{eq:Bphi}
	\langle \calB_\phibf\,\vbf,\wbf\rangle 
	= 2\int_{\Omega} \beta(\rho(\phibf)) (\phibf\cdot\vbf)\, (\phibf\cdot \wbf) \dx.
\end{equation}
%
Hence, the $L^2$-Sobolev Hessian of  $\,\overline{\calE}$ is given by $\nabla^2\,\overline{\calE}(\phibf)[\vbf]=\calJ\!\calA_\phibf\, \vbf+  \calJ\calB_\phibf\,\vbf$ for all $\vbf\in V$.
By the definition of the orthogonal projection onto~$T_\phibf\,\StiefelV$ in~\eqref{eq:def:HProj}, we conclude that for~$\etabf\in T_\phibf\,\StiefelV$ the Riemannian Hessian of~$\calE$ is given by 
%
\begin{align}
	\Hess\calE(\phibf) [\etabf] 
	&= \calP_{\phibf} \big( \calJ\!\calA_\phibf\, \etabf+  \calJ\calB_\phibf\,\etabf-\etabf\,  \out{\phibf}{ \calJ\!\calA_\phibf\,\phibf}\big) \notag \\
	&= \calJ\!\calA_\phibf\, \etabf + \calJ\calB_\phibf\,\etabf 
	-\etabf\,  \out{\phibf}{\calJ\!\calA_\phibf\,\phibf}
	-\phibf\sym \out{\phibf}{\calJ\!\calA_\phibf\, \etabf} \label{eq:defHess} \\
	&\qquad -\phibf\sym \out{\phibf}{\calJ\calB_\phibf\, \etabf}
	+ \phibf\sym \big(\out{\phibf}{\etabf}\out{\phibf}{\calJ\!\calA_\phibf\, \phibf)}\big). \notag
\end{align}

% retractions
Within optimization methods, we need to transfer data from the tangent space to the manifold to keep the iterations on the search space. For this purpose, we can use retractions defined as follows. Let $T\StiefelV$ 
be the tangent bundle to $\StiefelV$. A~smooth mapping $\mathcal{R}\colon T\StiefelV \to \StiefelV$ is called a~\emph{retraction} if for all $\phibf\in\StiefelV$, the restriction of $\calR$ to $T_\phibf\,\StiefelV$, denoted by $\calR_\phibf$, satisfies the following properties:
%
\begin{enumerate}
	\item[1)] $\calR_\phibf(\bm{0}_\phibf)=\phibf$, where $\bm{0}_\phibf$ denotes the origin of $T_\phibf\,\StiefelV$, and 
	\item[2)] $\tfrac{{\rm d}}{{\rm d}t} \calR_\phibf (t\etabf)\big|_{t=0}=\etabf$ for all $\etabf\in T_\phibf\,\StiefelV$.
\end{enumerate}
Retractions provide first-order approximation to an~exponential mapping on a~Riemannian manifold  and are often much easier to compute. 
A retraction $\calR$ on $\StiefelV$ is of \emph{second-order}, if it satisfies $\tfrac{{\rm d}^2}{{\rm d}t^2} \calR_\phibf (t\etabf)\big|_{t=0}\in T_\phibf^{\perp}\,\StiefelV$ for all $(\phibf,\etabf)\in T\StiefelV$.

%
In \cite{AltPS21}, several retractions on the Stiefel manifold $\StiefelV$ have been introduced. They can be considered as an~extension of the corresponding concepts on the matrix Stiefel manifold (see, e.g., \cite{AbsiM12,SatA19}) to the infinite-dimensional case. 

% qr retraction 
For $\vbf\in V$ with linearly independent components, we consider the $qR$~decomposition  $\vbf=\qbf R$, where 
$\qbf\in\StiefelV$ and $R\in\mathbb{R}^{p\times p}$ is upper triangular. Such a~decomposition exists and is unique if we additionally require that $R$ has positive diagonal elements. Then the {\em $qR$~decomposition based retraction} is defined as
$\calR^{qR}(\phibf,\etabf)=\qf(\phibf+\etabf)$, where $\qf(\phibf+\etabf)$ denotes the factor from $\StiefelV$ in the 
$qR$ decomposition of $\phibf+\etabf$. Such a~factor can be computed, e.g., by the modified Gram-Schmidt orthonormalization procedure on~$V$ presented in~\cite{AltPS21}. 
%or by computing the Cholesky factorization $\out{\phibf+\etabf}{\phibf+\etabf}=C^TC$ with an upper triangular matrix $C\in\mathbb{R}^{p\times p}$ and determining $\calR(\phibf,\etabf) = (\phibf+\etabf)\,C^{-1}$.

%
An alternative retraction can be defined by using the polar decomposition $\vbf=\ubf S$, where $\ubf\in\StiefelV$ and $S\in\mathbb{R}^{p\times p}$ is symmetric and positive definite. Assuming that the components of $\vbf$ are linearly independent, $S=\out{\vbf}{\vbf}^{1/2}$ and $\ubf=\vbf\,\out{\vbf}{\vbf}^{-1/2}$ are uniquely defined.
This leads to the {\em polar decomposition based retraction}
\[
	\calR^{\rm pol}(\phibf,\etabf) = (\phibf+\etabf)\out{\phibf+\etabf}{\phibf+\etabf}^{-1/2}, 
\]
which is of second order. Indeed, computing the second-order derivative of $\calR^{\rm pol}_\phibf (t\etabf)$
at $t=0$ and exploiting \eqref{eq:normalS}, we obtain that
\[
	\frac{{\rm d}^2}{{\rm d}t^2} \calR^{\rm pol}_\phibf (t\etabf)\Big|_{t=0} 
	= -\phibf\,\out{\etabf}{\etabf}\in T_{\phibf}^{\perp}\,\StiefelV.
\]
Note that second-order retractions are advantageous for second-order Riemannian optimization methods; see, e.g. \cite[Sect.~6.3]{AbsiMS08}. 
%
%
%=============================================================================
\subsection{The Grassmann manifold}\label{sec:Grassmann}
Let $\OrthGr$ be the orthogonal group of $\R^{p\times p}$. 
%and recall that $\calE(\phibf)=\calE(\phibf Q)$. Because of this, there is no unique minimizer of~\eqref{eq:minSt} and we 
Following \cite{SchRNB09}, we define the infinite-dimensional {\em Grassmann manifold} as the quotient 
%
\[
	\GrassV 
	= \StiefelV/\OrthGr  
\] 
%
of the Stiefel manifold $\StiefelV$ with respect to the equivalence relation 
%
\[
	\phibf\sim\hat{\phibf} \qquad 
	\Longleftrightarrow\qquad \hat{\phibf} = \phibf\, Q \text{ for some } Q\in \OrthGr.
\]
%
The Grassmann manifold $\GrassV$ can be interpreted as the set of the equivalence classes given by 
\[
\equivcl{\phibf} 
= \big\{ \hat{\phibf}\in\StiefelV\enskip :\enskip \hat{\phibf}=\phibf\, Q, \; Q\in\OrthGr \big\}
\] 
for $\phibf\in\StiefelV$. Similarly to the Grassmann matrix manifold \cite[Prop.~3.4.6]{AbsiMS08}, one can show that $\GrassV$ admits a~unique structure of quotient manifold. A {\em canonical projection} from the Stiefel manifold into the Grassmann manifold is defined by
\[
\arraycolsep=2pt
\begin{array}{rcl}
	\pi\colon\StiefelV & \to & \GrassV\\
	\phibf &\mapsto & \equivcl{\phibf}
\end{array}
\] 
and is a smooth submersion. This means that $\Drm\! \pi(\phibf)$ is surjective, and,  hence, the equivalence class $\pi^{-1}(\equivcl{\phibf})$  is an~embedded submanifold of $\StiefelV$; see \cite[Prop.~3.4.4.]{AbsiMS08}. 

In the following, we examine a useful connection of the Stiefel manifold and the Grassmann manifold. More precisely, we show that there is a one-to-one relation between the tangent space of the Grassmann manifold and the so-called horizontal space, a subspace of the tangent space of the Stiefel manifold. The tangent space $T_\phibf \, \StiefelV$ at $\phibf\in \StiefelV$ defined in~\eqref{eq:TangSpaceSt} can be splitted with respect to the projection $\pi$ and the Hilbert metric $g$ as $T_\phibf \, \StiefelV = \calV_{\phibf} \oplus \calH_\phibf$, where 
%
\begin{equation}\label{eq:vertV}
	\calV_{\phibf} 
	%= T_\phibf \,\pi^{-1}\pi (\phibf)
	= T_\phibf \,\pi^{-1}(\equivcl{\phibf}) %\ker\big(\Drm\! \pi(\phibf)\big) 
	= \big\{\phibf\, \varTheta\enskip :\enskip \varTheta\in \Sskew \big\}
\end{equation}
%
is the {\em vertical space} at $\phibf$ and 
%
\begin{align*}
	\calH_{\phibf} 
	= \calV_\phibf^\perp 
	&= \big\{ \xbf \in T_\phibf\, \StiefelV\enskip :\enskip g(\xbf,\vbf)=0 \text{ for all } \vbf\in \calV_\phibf\big\} \\
	&=\big\{ \xbf \in T_\phibf\, \StiefelV\enskip :\enskip \out{\phibf}{\xbf}=0_p\big\}
\end{align*}
%
is the {\em horizontal space} at $\phibf$; see \cite[Lem.~2]{SchRNB09}. The orthogonal projection of a tangent vector $\etabf\in T_\phibf \StiefelV$ onto $\calH_{\phibf}$ is given by 
%
\begin{align}
\label{eq:def:horizontalProj}
	\calP_{\phibf}^{\rm h}(\etabf) 
	= \etabf -\phibf\, \out{\phibf}{\etabf}.
\end{align}
%
One can see that, moving on a~curve in the Stiefel manifold $\StiefelV$ with direction in the vertical space $\calV_{\phibf}$, we stay in the equivalence class $\equivcl{\phibf}$. The tangent space $T_{\equivcl{\phibf}}\GrassV$ of the Grassmann manifold $\GrassV$ can then be identified with the horizontal space $\calH_{\phibf}$ in the sense that for any $\psibf\in T_{\equivcl{\phibf}}\GrassV$ there exists a~unique $\psibf_{\phibf}^{\rm h}\in \calH_{\phibf}$ such that \mbox{$\Drm\!\pi(\phibf)[\psibf_{\phibf}^{\rm h}]=\psibf$}. The unique element  $\psibf_{\phibf}^{\rm h}$ is called the {\em horizontal lift} of $\psibf$ at $\phibf$. This relation allows us to introduce a~metric on the Grassmann manifold $\GrassV$, namely 
%
\[
	g^{\rm Gr}(\psibf,\zetabf) 
	= g(\psibf^{\rm h},\zetabf^{\rm h}), \qquad 
	\psibf,\zetabf\in T_{\equivcl{\phibf}}\GrassV,\; \equivcl{\phibf}\in\GrassV,
\]
%
where $\psibf_{\phibf}^{\rm h}, \zetabf_{\phibf}^{\rm h}\in \calH_{\phibf}$ are the horizontal lifts of $\psibf$ and $\zetabf$ at $\phibf$, respectively. Due to $\psibf_{\phibf Q}^{\rm h}=\psibf_{\phibf}^{\rm h}Q$ for all $Q\in\OrthGr$, one can show that this metric does not depend on the choice of the representative~$\phibf$ of the equivalence class~$\equivcl{\phibf}$. 

The connection of $T_{\equivcl{\phibf}}\GrassV$ and $\calH_{\phibf}$ makes it possible to introduce optimization methods on the Grassmann manifold, while still working on the tangent space of the corresponding Stiefel manifold. 
Using the canonical projection $\pi$, the  minimization problem~\eqref{eq:minSt} on the Stiefel manifold $\StiefelV$ can be written as the minimization problem 
%
\begin{equation} 
	\label{eq:minGrV}
	\min_{\equivcl{\phibf}\in\GrassV} {\calF}(\equivcl{\phibf})
\end{equation}
%
on the Grassmann manifold $\GrassV$, where the cost functional $\calF\colon \GrassV\to \R$ is induced by $\calE$ as $\calE(\phibf)={\calF}(\pi(\phibf))$ and $\calF(\equivcl{\phibf})={\calE}(\pi^{-1}(\equivcl{\phibf}))$. Note that this definition is justified by the fact that $\calE(\phibf)=\calE(\phibf Q)$ for all~$Q\in\OrthGr$. The horizontal lift of the Riemannian gradient $\grad {\calF}(\equivcl{\phibf})\in T_{\equivcl{\phibf}}\GrassV$ with respect to the metric $g^{\rm Gr}$ is given by 
%
\begin{equation}\label{eq:gradFGr}
\grad {\calF}(\equivcl{\phibf})_{\phibf}^{\rm h} 
= \grad{\calE}(\phibf) 
= \calP_{\phibf}^{\rm h} \big(\calJ\!\calA_\phibf\,\phibf \big)
= \calJ\!\calA_\phibf\,\phibf - \phibf\, \out{\phibf}{\calJ\!\calA_\phibf\,\phibf}.
\end{equation}
%
To obtain the horizontal lift of the Riemannian Hessian $\Hess {\calF}(\equivcl{\phibf})[\psibf]$, we proceed as before but replace the projection~$\calP_{\phibf}$ by the orthogonal projection $\calP_{\phibf}^{\rm h}$ onto the horizontal space; see equation~\eqref{eq:def:horizontalProj}. This leads to 
%
\begin{align}
	(\Hess{\calF}(\equivcl{\phibf})[\psibf])_{\phibf}^{\rm h} 
	&= \calP_{\phibf}^{\rm h} \big(\Drm\grad \calE(\phibf)[\psibf_{\phibf}^{\rm h}] \big) \notag \\
	&= \calP_{\phibf}^{\rm h} \big(\calJ\!\calA_\phibf\, \psibf_{\phibf}^{\rm h}+ \calJ\calB_\phibf\,\psibf_{\phibf}^{\rm h}-\psibf_{\phibf}^{\rm h}\out{\phibf}{\calJ\!\calA_\phibf\,\phibf} \big).
%	& = \calJ\!\calA_\phibf\, \psibf^{\rm h}+ \calJ\calB_\phibf\,\psibf^{\rm h}-\psibf^{\rm h}\out{\phibf}{\calJ\!\calA_\phibf\,\phibf}-\phibf \out{\phibf}{\calJ\!\calA_\phibf\, \psibf^{\rm h}} - \phibf\out{\phibf}{\calJ\calB_\phibf\,\psibf^{\rm h}}. 
	\label{eq:HessGrassmann}
\end{align}
%

Retractions on the Grassmann manifold are inherited from that on the Stiefel manifold applied to the horizontal lift; see \cite[Prop.~4.1.3]{AbsiMS08}. For all \mbox{$\equivcl{\phibf}\in\GrassV$} and $\psibf\in T_{\equivcl{\phibf}}\GrassV$, we have
\begin{align*}
	\calR^{{\rm Gr,pol}}(\equivcl{\phibf}, \psibf) & = \pi\big(\calR^{\rm pol}(\phibf+\psibf_{\phibf}^{\rm h})\big), \\
	\calR^{{\rm Gr,qR}}(\equivcl{\phibf}, \psibf) & = \pi\big(\calR^{\rm qR}(\phibf+\psibf_{\phibf}^{\rm h})\big).
\end{align*}
Note that these retractions are independent of the chosen point $\phibf$, providing the same equivalence class on $\GrassV$.

Similar to the matrix case \cite{AbsiMS08,EdeAS98}, we can also derive an explicit expression for the \emph{Grassmann exponential} ${\rm Exp}\colon T\GrassV\to \GrassV$, which maps \mbox{$(\equivcl{\phibf},\psibf)\in  T\GrassV$} to the end point of the unique geodesic starting at $\equivcl{\phibf}$ and going in the direction~$\psibf$. Let $\psibf_{\phibf}^{\rm h}=\ubf\Sigma W^T$ be a singular value decomposition of the horizontal lift $\psibf_{\phibf}^{\rm h}$ of~$\psibf$, where $\ubf\in\StiefelV$, $W\in\OrthGr$, and $\Sigma\in\R^{p\times p}$ is diagonal with nonnegative diagonal elements. Then the Grassmann exponential is given by 
\[
{\rm Exp}(\equivcl{\phibf}, \psibf) = \pi \big(\phibf \,W\cos \Sigma +\ubf \sin\Sigma\big).
\]
Using the property $\psibf_{\phibf}^{\rm h}\in\calH_\phibf$, one can verify that $\phibf \,W\cos \Sigma +\ubf \sin\Sigma \in\StiefelV$. Therefore, it can be considered as a representative of the resulting equivalence class.
%
\begin{remark}
	For $p=1$, the Stiefel manifold coincides with the Grassmann manifold and equals the unit sphere
	\[
	\mathbb{S}= \big\{ \phi\in \tilde{V}\enskip :\enskip \|\phi\|_{L^2(\Omega)} = 1 \big\}.
	\]
	Its tangent space is given by $T_{\phi}\,\mathbb{S}	= \{ \eta\in \tilde{V} : (\eta, \phi)_{L^2(\Omega)} = 0 \}$ and the orthogonal projection onto this space takes the form $\calP_{\phi}(y)=y-(\phi,y)_{L^2(\Omega)}\phi$ for $y\in \tilde{V}$. 
	Furthermore, for $(\phi,\eta)\in T\mathbb{S}$, the second-order retraction and the exponential mapping on $\mathbb{S}$ are given by  
	\begin{align*}
	\calR(\phi,\eta) =\frac{\phi+\eta\qquad}{\|\phi+\eta\|_{L^2(\Omega)}}, \qquad 
	{\rm Exp}(\phi,\eta)=\cos(\|\eta\|_{L^2(\Omega)})\phi+\sin(\|\eta\|_{L^2(\Omega)})\frac{\eta\qquad}{\|\eta\|_{L^2(\Omega)}},
\end{align*}
respectively.
\end{remark}
%
%
%=============================================================================
%=========  Newton Methods
%=============================================================================
\section{Riemannian Newton Methods}\label{sect:Newton}
In this section, we present the Riemannian Newton methods on the Stiefel as well as on the Grassmann manifold and discuss the inexact version of the latter.

Within the Riemannian Newton method on the Stiefel manifold $\StiefelV$, for given $\phibf_k\in \StiefelV$, we first compute the Newton search direction~$\etabf_k\in T_{\phibf_k} \StiefelV$ by solving the Newton equation 
%
\begin{equation}\label{eq:NewtonEqSt}
	\Hess\calE(\phibf_k) [\etabf_k]
	= -\grad \calE(\phibf^{(k)}).
\end{equation}
%
The iterate is then updated by~$\phibf_{k+1}=\calR(\phibf_k,\etabf_k)$ for any retraction on $\StiefelV$. By~\eqref{eq:gradE_H}, $-\grad \calE(\phibf_k)$ belongs to the image of $\Hess \calE(\phibf_k)$ and, hence, the Newton equation \eqref{eq:NewtonEqSt} is indeed solvable. It should, however, be noted that due to the non-uniqueness of the minimizer of \eqref{eq:minSt} caused by the invariance of $\calE$ under orthogonal transformations, we cannot expect that $\Hess \calE(\phibf_k)$ is invertible on $T_{\phibf_k} \StiefelV$ implying that the solution of~\eqref{eq:NewtonEqSt} is non-unique. This is, indeed, vindicated by the following theorem.
%
\begin{theorem}%\label{th:HessStiefel}
Let $\phibf\in\StiefelV$ and let $\calV_{\phibf}$ be the vertical space  given in \eqref{eq:vertV}. Then the Riemannian Hessian of the energy functional~$\calE$ from~\eqref{eq:energy} in~$\phibf$ is non-invertible on~$\calV_{\phibf}$, i.e., $\big(\xibf,\Hess\calE(\phibf) [\etabf]\big)_H=0$ for all $\etabf,\xibf\in\calV_{\phibf}$. 
\end{theorem}
%
\begin{proof}
Let $\etabf,\xibf\in\calV_{\phibf}$ be arbitrary. Then there exist $\varTheta_{\etabf},\varTheta_{\xibf}\in\Sskew$ such that \mbox{$\etabf=\phibf\,\varTheta_{\etabf}$} and $\xibf=\phibf\,\varTheta_{\xibf}$. Using the definition of $\Hess\calE(\phibf)$ in~\eqref{eq:defHess} and the symmetry of the matrix~$\out{\phibf}{\calJ\!\calA_\phibf\, \phibf}$ shown in Remark~\ref{rem:symmetryBracket}, we have
%
\begin{align*}
	\Hess\calE(\phibf) [\etabf] 
	&= \calJ\!\calA_\phibf\, \phibf\,\varTheta_{\etabf} 
	+ \calJ\calB_\phibf\,\phibf\,\varTheta_{\etabf} 
	- \phibf\,\varTheta_{\etabf}\,  \out{\phibf}{\calJ\!\calA_\phibf\,\phibf} 
	- \phibf\sym\big( \out{\phibf}{\calJ\!\calA_\phibf\, \phibf} \varTheta_{\etabf}\big)\\
	&\qquad -\phibf\sym \big(\out{\phibf}{\calJ\calB_\phibf\, \phibf}\varTheta_{\etabf}\big)
	+ \phibf\sym \big(\varTheta_{\etabf}\out{\phibf}{\calJ\!\calA_\phibf\, \phibf}\big)\\
	& = \calJ\!\calA_\phibf\, \phibf\,\varTheta_{\etabf} 
	-\phibf\,  \out{\phibf}{\calJ\!\calA_\phibf\,\phibf} \,\varTheta_{\etabf} +\calJ\calB_\phibf\,\phibf\,\varTheta_{\etabf}
	-\phibf\sym \big(\out{\phibf}{\calJ\calB_\phibf\, \phibf}\varTheta_{\etabf}\big) 
\end{align*}
%
and, hence,
%
\begin{align*}
	\big(\xibf,\Hess\calE(\phibf) [\etabf]\big)_H  
	&=\trace\left(\varTheta_{\xibf}^T\out{\phibf}{\calJ\calB_\phibf\,\phibf} \,\varTheta_{\etabf}^{}-\Theta_{\xibf}^T\sym \big(\out{\phibf}{\calJ\calB_\phibf\, \phibf}\varTheta_{\etabf}^{}\big)\right).
\end{align*}
%
We now show that $\out{\phibf}{\calJ\calB_\phibf\,\phibf} \,\varTheta_{\etabf}= 0$. With the skew-symmetric matrix $\varTheta_{\etabf} = [\theta_{ij}]_{i,j=1}^p$, we first observe that 
%
\[
\phibf\cdot(\phibf\,\varTheta_{\etabf}) 
= \sum_{j=1}^p \phi_j \sum_{i=1}^p \phi_i\theta_{ij}
= \sum_{i=1}^p \phi_i \sum_{j=1}^p \phi_j\theta_{ij}
= -\sum_{i=1}^p \phi_i \sum_{j=1}^p \phi_j\theta_{ji}
= -\phibf\cdot(\phibf\,\varTheta_{\etabf}). 
\]
%
This implies $\phibf\cdot(\phibf\,\varTheta_{\etabf}) = 0$ and, hence, %Then it follows from the definition of $\calB_{\phibf}$ in~\eqref{eq:Bphi} with $\vbf=\phibf\,\varTheta_{\etabf}$ that 
$\out{\phibf}{\calJ\calB_\phibf\,\phibf} \,\varTheta_{\etabf}= 0$. As a result, we conclude that $\big(\xibf,\Hess\calE(\phibf) [\etabf]\big)_H=0$ for all $\etabf,\xibf\in\calV_{\phibf}$. 
\end{proof} 
%
To get rid of the non-uniqueness of the solution of the Newton equation~\eqref{eq:NewtonEqSt}, we now pass
on to the Grassmann manifold $\GrassV$. Given $\equivcl{\phibf_k}\in\GrassV$, the Newton direction
$\psibf_k\in T_{\equivcl{\phibf_k}}\GrassV$ is computed by solving the Newton equation
%
\[
	\Hess\calF(\equivcl{\phibf_k}) [\psibf_k]
= -\grad \calF(\equivcl{\phibf_k}).
\]
By applying the horizontal lift expressions \eqref{eq:gradFGr} and \eqref{eq:HessGrassmann}, this equation leads to 
 \begin{equation}
\label{eq:NewGrV}
	\calP_{\phibf_k}^{\rm h} \big(\Drm\grad \calE(\phibf_k)[(\psibf_k)_{\phibf_k}^{\rm h}] \big) 
	= -\grad \calE(\phibf_k)
\end{equation}
%
with unknown $(\psibf_k)_{\phibf_k}^{\rm h}\in\calH_{\phibf_k}$ being the horizontal lift of~$\psibf_k$ at~$\phibf_k$. Note that this equation is well-defined, since~$-\grad \calE(\phibf_k)$ is an element of the horizontal space $\calH_{\phibf_k}$. 

For solving the Newton equation \eqref{eq:NewGrV}, we can employ any matrix-free iterative linear solver which does not require the storage of the coefficient matrix explicitly but accesses it by computing the matrix-vector product or -- as in our case --  by evaluating the linear operator given in~\eqref{eq:HessGrassmann}. The resulting Riemannian Newton method on the Grassmann manifold is presented
%Assuming an inexact solution of this equation, we arrive 
in Algorithm~\ref{alg:infdim:RiemNewtonGr}.
%
\begin{algorithm}[h]
	\setstretch{1.15}
	\caption{Riemannian Newton method on the Grassmann manifold} 
	\label{alg:infdim:RiemNewtonGr}
	\begin{algorithmic}[1]
		\State {\bf Input}: initial guess $\phibf_0\in\StiefelV$, parameters $\delta,\eta\in(0,1)$, $\sigma\in(0,1/2]$, $\ell_{\max}\in\N$
		\For{$k=0,1,2\ldots$} 
		\State{Solve the Newton equation%~\eqref{eq:NewGrV}, i.e., 
			\[
			\calP_{\phibf_k}^{\rm h}\Bigl(\Drm\grad \calE(\phibf_k)[(\psibf_k)_{\phibf_k}^{\rm h}]\Bigr) 
			= -\grad \calE(\phibf_k)
			\]
			\hspace*{5mm} for $(\psibf_k)_{\phibf_k}^{\rm h} \in \calH_{\phibf_k}$ in an inexact manner by using 
			an~iterative solver. If the condition   
			%
			\begin{align*}				
%				\big\| \calP_{\phibf_k^{\rm h}\big(\Drm\grad \calE(\phibf_k)[(\psibf_k)_{\phibf_k}^{\rm h}]\big) + \grad \calE(\phibf_k)\big\|_H
%				&\leq \eta_k\,\big\|\grad \calE(\phibf_k)\big\|_H,  \label{eq:infdim:cond1} \\
				-\big( \grad \calE(\phibf_k), (\psibf_k)_{\phibf_k}^{\rm h}\big) _H
				&\geq \eta\, \|(\psibf_k)_{\phibf_k}^{\rm h}\|_H^2
			\end{align*} 
			\hspace*{4.4mm} %where $\eta_k=\min\{\eta, c\,\|\grad \calE(\phibf_k)\|_H\}$. 
		cannot be attained within~$\ell_{\max}$	steps, then set $$(\psibf_k)_{\phibf_k}^{\rm h} = -\grad \calE(\phibf_k).$$
		}
		\State{Find the smallest $\ell\in\N_0$ such that the Armijo condition 
			\begin{align*}
				\calE\big(\calR({\phibf_k},\delta^\ell (\psibf_k)_{\phibf_k}^{\rm h})\big) - \calE\big(\phibf_k\big) 
				\le \sigma \,\delta^\ell
				\big( \grad \calE(\phibf_k), (\psibf_k)_{\phibf_k}^{\rm h}\big)_H
				%\label{eq:infdim:condErg}
			\end{align*}
			\hspace*{5mm} is satisfied, where $\calR({\phibf_k},\delta^\ell (\psibf_k)_{\phibf_k}^{\rm h})$
			%= \qf(\phibf^{(k)}+\beta^\ell\, \psibf^{{\rm h},(k)}$ 
			is a retraction on $\StiefelV$.}
		\State{Set $\phibf_{k+1} = \calR\big(\phibf_k,\delta^\ell(\psibf_k)_{\phibf_k}^{\rm h}\big)$.}
		\EndFor
		%				
		\State {\bf Output}: sequence of iterates $\{\phibf_k\}$ with $\phibf_k\in\StiefelV$
	\end{algorithmic}
\end{algorithm} 

% formulation as saddle point problem
The Newton equation~\eqref{eq:NewGrV} can also be formulated as a~saddle point problem. To this end, we introduce the~bilinear form 
%
\[
	\widehat{a}_\phibf(\psibf,\wbf) 
	= \langle \calA_\phibf\,\psibf,\wbf\rangle + \langle \calB_\phibf\, \psibf,\wbf\rangle - (\psibf\, \out{\phibf}{\calJ\!\calA_\phibf\,\phibf},\wbf)_H.  
\]
%
Then, the equivalent problem to~\eqref{eq:NewGrV} reads: find $(\psibf_k)_{\phibf_k}^{\rm h}\in V$ and a~Lagrange multiplier \mbox{$M_k\in \R^{p\times p}$} such that 
%
\begin{subequations}
	\label{eq:Saddle2}
	\begin{align}
		\widehat{a}_{\phibf_k}\big((\psibf_k)_{\phibf_k}^{\rm h},\wbf\big)  + \trace \big( M_k^T\out{\phibf_k}{\wbf}\big) & = -a_{\phibf_k}(\phibf_k,\wbf) & & \text{for all } \wbf\in V, \label{eq:Saddle2_1}\\
		\out{\phibf_k}{(\psibf_k)_{\phibf_k}^{\rm h}} \hspace*{32mm}& = 0_p. && \label{eq:Saddle2_2}
	\end{align}
\end{subequations}
%
The constraint \eqref{eq:Saddle2_2} implies that  
%\mbox{$\psibf^{\rm h}\in T_\phibf\,\StiefelV$} and, in particular, 
$(\psibf_k)_{\phibf_k}^{\rm h}\in\calH_{\phibf_k}$. Further note that any function $\wbf\in \calH_{\phibf_k}$ satisfies~$\out{\phibf_k}{\wbf}= 0_p$. Hence, 
for all $\wbf\in \calH_{\phibf_k}$, equation~\eqref{eq:Saddle2_1} reads  
$$
	\widehat{a}_\phibf\big((\psibf_k)_{\phibf_k}^{\rm h},\wbf\big) 
	= -a_{\phibf_k}(\phibf_k,\wbf)=-(\grad\calE(\phibf_k),\wbf)_H,
$$ 
which is equivalent to the Newton equation \eqref{eq:NewGrV}.  

% \subsection{Positivity of the Hessian}
One important property guaranteeing an~isolated local minimum of the energy is that the Hessian is positive at a~stationary point. For a global minimizer of~\eqref{eq:minSt}, denoted by~$\phibf_*$, we consider the following {\em linear} eigenvalue problem: seek $\phi \in \tilde{V}$ and $\lambda\in \R$ such that 
%
\begin{align}
	\label{eq:NLEVPweakComponents:fixedPhi}	
	\tilde a_{\phibf_*}\!(\phi, v) 
	= \lambda\, (\phi, v)_{L^2(\Omega)}
	\qquad\text{ for all } v\in \tilde V.
\end{align}
%
Then, due to~\eqref{eq:NLEVPweakComponents}, we know that the components of~$\phibf_* = (\phi_{*,1}, \dots, \phi_{*,p})$ satisfy~\eqref{eq:NLEVPweakComponents:fixedPhi} together with the smallest~$p$ eigenvalues denoted by $0 < \lambda_1 \le \dots \le \lambda_p$. In the following, we will assume that these eigenfunctions can be extended to a basis of~$\tilde{V}$. 
%
\begin{assumption}[Basis and spectral gap]
	\label{ass:gap}
	The eigenfunctions~$\phi_{*,1}, \phi_{*,2}, \ldots \in \tilde{V}$ of the eigenvalue problem~\eqref{eq:NLEVPweakComponents:fixedPhi} form an $L^2$-orthonormal basis of $\tilde{V}$. The corresponding eigenvalues~$\lambda_1 \le \lambda_2 \le \dots$ are ordered by size with a spectral gap~$\lambda_p < \lambda_{p+1}$.
\end{assumption}
%
\begin{theorem}[Positive Hessian]
	\label{th:positiveHessian}	
	Let~$\phibf_*$ be a global minimizer of the NLEVP~\eqref{eq:NLEVPop} and let the corresponding eigenvalue problem~\eqref{eq:NLEVPweakComponents:fixedPhi} satisfy Assumption~\textup{\ref{ass:gap}}. Further assume that the nonlinearity fulfills~$(\calJ \calB_{\phibf_*} \psibf_{\phibf_*}^{\rm h}, \psibf_{\phibf_*}^{\rm h})_H \ge 0$ for all $\psibf_{\phibf_*}^{\rm h}\in\calH_{\phibf_*}$. Then the Riemannian Hessian of~$\calF$ at $\equivcl{\phibf_*}$ is positive, i.e., 
	%
	\[
	g^{\rm Gr}\big( \Hess{\calF}(\equivcl{\phibf_*})[\psibf], \psibf \big)
	> 0
	\]
	%
	for all nonzero~$\psibf\in T_{\equivcl{\phibf_*}}\GrassV$. 
\end{theorem}
%
\begin{proof}
	We extend the proof of~\cite[Th.~5.1]{ZhaoBJ15}, which considers the finite-dimensional case for the simplified Kohn--Sham problem, to the infinite-dimensional case in a more general setting. We know from~\eqref{eq:HessGrassmann} that the horizontal lift of the Riemannian Hessian of $\calF$ at $\phibf_*$ takes the form 
	%
	\begin{align*}
		(\Hess{\calF}(\equivcl{\phibf_*})[\psibf])_{\phibf_*}^{\rm h} 
		&= \calP_{\phibf_*}^{\rm h} \big(\calJ\!\calA_{\phibf_*} \psibf_{\phibf_*}^{\rm h}\big) 
		+ \calP_{\phibf_*}^{\rm h}\big( \calJ\calB_{\phibf_*} \psibf_{\phibf_*}^{\rm h}\big) 
		-  \calP_{\phibf_*}^{\rm h}\big(\psibf_{\phibf_*}^{\rm h}\out{\phibf_*}{\calJ\!\calA_{\phibf_*} \phibf_*} \big) \\
		&= T_1(\psibf_{\phibf_*}^{\rm h}) 
		\hspace{1.27cm} + T_2(\psibf_{\phibf_*}^{\rm h}) 
		\hspace{1.31cm}+ T_3(\psibf_{\phibf_*}^{\rm h}). 
	\end{align*}
	%
	For the first term, we make the following considerations. Due to the $H$-orthogonality, each component of $\psibf_{\phibf_*}^{\rm h}$ satisfies $(\phi_{*,l}, (\psibf_{\phibf_*}^{\rm h})_j)_{L^2(\Omega)} = 0$ for $l,j= 1,\dots, p$. Hence, the $j$th component of $\psibf_{\phibf_*}^{\rm h}$ takes the form $\psi_j = (\psibf_{\phibf_*}^{\rm h})_j = \sum_{l>p} \alpha_{lj} \phi_{*,l}$ for some coefficients~$\alpha_{lj}\in\R$ and~$\phi_{*,l}$ denoting the basis from Assumption~\ref{ass:gap}. As a consequence, we get 
	%
	\begin{align*}
		\big( \out{\phibf_*}{\calJ\!\calA_{\phibf_*} \psibf_{\phibf_*}^{\rm h}} \big)_{ij}
		& = \big( \phi_{*,i}, (\calJ\!\calA_{\phibf_*} \psibf_{\phibf_*}^{\rm h})_j \big)_{L^2(\Omega)}
		= \tilde{a}_{\phibf_*}\!(\phi_{*,i}, \psi_j) \\
		& = \sum_{l>p} \alpha_{lj}\, \tilde{a}_{\phibf_*}\!(\phi_{*,i}, \phi_{*,l})
		= \sum_{l>p} \alpha_{lj}\, \lambda_i (\phi_{*,i}, \phi_{*,l})_{L^2(\Omega)}
		= 0
	\end{align*}
	%
	for all $i,j=1,\ldots, p$ and, hence, 
	%
	\[
	T_1(\psibf_{\phibf_*}^{\rm h})
	= \calP_{\phibf_*}^{\rm h} \big(\calJ\!\calA_{\phibf_*} \psibf_{\phibf_*}^{\rm h}  \big)  
	= \calJ\!\calA_{\phibf_*} \psibf_{\phibf_*}^{\rm h} - \phibf_* 
	\out{\phibf_*}{\calJ\!\calA_{\phibf_*} \psibf_{\phibf_*}^{\rm h}	}
	= \calJ\!\calA_{\phibf_*} \psibf_{\phibf_*}^{\rm h}. 
	\]
	%
	For the second term, we get with the assumption on~$\calB_{\phibf_*}$ that 
	%
	\begin{align*}
		g( T_2(\psibf_{\phibf_*}^{\rm h}), \psibf_{\phibf_*}^{\rm h} )
		&= \big(T_2(\psibf_{\phibf_*}^{\rm h}), \psibf_{\phibf_*}^{\rm h}\big)_H \\
		%	= \trace\, \out{\calP_{\phibf_*,H}^{\rm h} ( \calJ \calB_{\phibf_*} \psibf_{\phibf_*}^{\rm h})}{\psibf_{\phibf_*}^{\rm h}}
		&= \big(\calJ \calB_{\phibf_*} \psibf_{\phibf_*}^{\rm h} - \phibf_* \out{\phibf_*}{\calJ \calB_{\phibf_*} \psibf_{\phibf_*}^{\rm h}},\psibf_{\phibf_*}^{\rm h}\big)_H  \\
		%	&= \trace\, \big( \out{\calJ \calB_{\phibf_*} \psibf_{\phibf_*}^{\rm h}}{\psibf_{\phibf_*}^{\rm h}} - \out{\phibf_* \out{\phibf_*}{\calJ \calB_{\phibf_*} \psibf_{\phibf_*}^{\rm h}}}{\psibf_{\phibf_*}^{\rm h}}\big) \\
		&= \big(\calJ \calB_{\phibf_*} \psibf_{\phibf_*}^{\rm h},\psibf_{\phibf_*}^{\rm h}\big)_H  - \out{\phibf_*}{\calJ \calB_{\phibf_*} \psibf_{\phibf_*}^{\rm h}}^T \big(\phibf_*,\psibf_{\phibf_*}^{\rm h}\big)_H\\
		&= \big(\calJ \calB_{\phibf_*} \psibf_{\phibf_*}^{\rm h},\psibf_{\phibf_*}^{\rm h}\big)_H 
		\ge 0.   
	\end{align*}
	%
	Finally, by using \eqref{eq:Lambda}, the third term takes the form  
	%
	\begin{align*}
		T_3(\psibf_{\phibf_*}^{\rm h})
		&= - \calP_{\phibf_*}^{\rm h} \big( \psibf_{\phibf_*}^{\rm h} \out{\phibf_*}{\calJ\!\calA_{\phibf_*}\phibf_*} \big) \\
		&= - \psibf_{\phibf_*}^{\rm h} \out{\phibf_*}{\calJ\!\calA_{\phibf_*}\phibf_*} 
		+ \phibf_* \out{\phibf_*}{\psibf_{\phibf_*}^{\rm h}}\, \out{\phibf_*}{\calJ\!\calA_{\phibf_*}\phibf_*} 
		= - \psibf_{\phibf_*}^{\rm h} \Lambda_*.
	\end{align*}
	%
	Let the columns of $U\in\OrthGr$ form a~basis of eigenvectors corresponding to the eigenvalues $\lambda_1,\ldots,\lambda_p$ of $\Lambda_*$ and let $\psibf_{\phibf_*}^{\rm h}U=(\tilde{\psi}_1,\ldots,\tilde{\psi_p})$. Due to the assumed spectral gap, this yields all together  
	%
	\begin{align*}
		g^{\rm Gr}\big( \Hess{\calF}(\equivcl{\phibf_*})[\psibf], \psibf \big) & = g\big( (\Hess{\calF}(\equivcl{\phibf_*})[\psibf])_{\phibf_*}^{\rm h}, \psibf_{\phibf_*}^{\rm h} \big) \\
		&= g\big( T_1(\psibf_{\phibf_*}^{\rm h}) + T_2(\psibf_{\phibf_*}^{\rm h}) + T_3(\psibf_{\phibf_*}^{\rm h}), \psibf_{\phibf_*}^{\rm h} \big) \\
		%	&\ge - g( \psibf_{\phibf_*}^{\rm h} \Lambda_*, \psibf_{\phibf_*}^{\rm h} )
		%	+ g( \calJ\!\calA_{\phibf_*} \psibf_{\phibf_*}^{\rm h}, \psibf_{\phibf_*}^{\rm h} ) \\
		&\ge \trace\, \out{\calJ\!\calA_{\phibf_*} \psibf_{\phibf_*}^{\rm h}}{\psibf_{\phibf_*}^{\rm h}} 
		- \trace\, \out{\psibf_{\phibf_*}^{\rm h} \Lambda_*}{\psibf_{\phibf_*}^{\rm h}} 		\\
		&= \sum_{j=1}^p \tilde{a}_{\phibf_*}\!(\tilde{\psi}_j, \tilde{\psi}_j) 
		- \sum_{j=1}^p \lambda_j (\tilde{\psi}_j, \tilde{\psi}_j)_{L^2(\Omega)} \\
		&\ge \sum_{j=1}^p ( \lambda_{p+1} - \lambda_j)\, (\tilde{\psi}_j, \tilde{\psi}_j)_{L^2(\Omega)}
		> 0,
	\end{align*}
	%
	which completes the proof.  
\end{proof}
%
\begin{remark}[Connection to the Lagrange--Newton method]
The optimal solution of the constrained minimization problem~\eqref{eq:minSt} can also be determined by the Lagrange--Newton method. Based on the first-order optimality conditions~\eqref{eq:NLEVPop} with a symmetric Lagrange multiplier, we aim to solve the nonlinear system of equations
\[
f(\phibf, \Lambda) = \begin{bmatrix} \calJ\!\calA_{\phibf} \,\phibf-\phibf\, \Lambda \\ 
			\out{\phibf}{\phibf} - I_p \\ \Lambda-\Lambda^T\end{bmatrix} 
			= \begin{bmatrix} \mathbf{0}\, \\ 0_p \\ 0_p\end{bmatrix}.
\]
Computing the Jacobian of $f$, the Lagrange-Newton iteration is given as follows: for given $\phibf_k\in V$ and $\Lambda_k\in\R^{p\times p}$, solve the equations
%
\begin{subequations}
	\label{eq:LagrNewton}
	\begin{align}
	\calJ\!\calA_{\phibf_k} \etabf_k + \calJ\calB_{\phibf_k} \etabf_k 
	-\etabf_k\Lambda_k-\phibf_k\Xi_k 
	&= -\big(\calJ\!\calA_{\phibf_k} \phibf_k -\phibf_k\Lambda_k\big), \label{eq:LagrNewton1} \\
	\out{\phibf_k}{\etabf_k}+\out{\etabf_k}{\phibf_k} 
	&= -\big(\out{\phibf_k}{\phibf_k} - I_p\big), \label{eq:LagrNewton2}\\
	\Xi_k^{}-\Xi_k^T 
	&= -\big(\Lambda_k^{}-\Lambda_k^T\big) \label{eq:LagrNewton3}
	\end{align}
\end{subequations}
%
for $\etabf_k\in V$, $\Xi_k\in\R^{p\times p}$ and update $\phibf_{k+1}=\phibf_k+\etabf_k$, $\Lambda_{k+1}=\Lambda_k+\Xi_k$. Note that $\phibf_{k+1}$ does not necessary belong to $\StiefelV$. Assuming $\phibf_k\in\StiefelV$ and $\Lambda_k\in\Ssym$, however, equations \eqref{eq:LagrNewton2} and \eqref{eq:LagrNewton3} imply that $\etabf_k\in T_{\phibf_k}\StiefelV$ and $\Xi_k\in\Ssym$, respectively. Resolving equation \eqref{eq:LagrNewton1} for symmetric $\Xi_k$, we find that
%
\[
	\Xi_k = \sym\big(\out{\phibf_k}{\calJ\!\calA_{\phibf_k}\,\phibf_k}+\out{\phibf_k}{\calJ\calB_{\phibf_k}\,\phibf_k}-
	\out{\phibf_k}{\etabf_k}\Lambda_k\big)+\out{\phibf_k}{\calJ\!\calA_{\phibf_k}\,\phibf_k}-\Lambda_k.
\]
%
Inserting this matrix into \eqref{eq:LagrNewton1} yields the Newton equation~\eqref{eq:NewtonEqSt}. This shows that the Lagrange--Newton method with the modified update 
\[
	\phibf_{k+1} 
	= \calR(\phibf_k,\etabf_k), \qquad 
	\Lambda_{k+1} 
	= \out{\phibf_{k+1}}{\calJ\!\calA_{\phibf_{k+1}}\,\phibf_{k+1}}
\] 
is equivalent to the Newton method on the Stiefel manifold.
\end{remark}
%
%
%=============================================================================
%=========  Numerics
%=============================================================================
\section{Examples and numerical experiments}\label{sect:numerics}
This section is devoted to the numerical investigation of the Riemannian Newton methods. To this end, we consider the Gross--Pitaevskii eigenvalue problem from Example~\ref{exp:GPEVP} and the Kohn--Sham model from Example~\ref{exp:KS}. 
%
%
%=============================================================================
\subsection{Gross--Pitaevskii eigenvalue problem}\label{sect:numerics:GPEVP}
The minimization of the Gross--Pitaevskii energy functional $\calE_{\rm GP}$ in~\eqref{eq:energyGP} leads to the following nonlinear eigenvector problem: find $\phi\in \tilde{V} = H^1_0(\Omega)$ with $\|\phi\|_{L^2(\Omega)} = 1$ and $\lambda\in\R$ such that 
%
\begin{equation}\label{eq:GPeig}
-\Delta \phi + \vartheta\, \phi + \kappa\, |\phi|^2 \phi 
= \lambda\, \phi 
\end{equation}
%
for some space-dependent external potential~$\vartheta \ge 0$ and an interaction constant $\kappa>0$. The latter means that the particle interactions are repulsive, i.e., we consider the so-called {\em defocussing regime}. In this case, we get the operators
%
\begin{subequations}
	\begin{align}
		\langle \calA_\phi\, v, w\rangle
		&= \int_\Omega (\nabla v)^T \nabla w \dx 
		+ 2 \int_\Omega  \vartheta\, v w \dx 
		+ \kappa\int_\Omega \, \phi^2 v w \dx, \label{eq:Aphi-GP} \\
		\langle \calB_\phi\, v, w\rangle 
		&= 2\kappa \int_{\Omega} \phi^2 v w \dx \label{eq:Bphi-GP} 
	\end{align}
\end{subequations}
%
for $v,w\in \tilde{V}$. One can see that the bilinear form defined through \eqref{eq:Aphi-GP} corresponds to the Laplacian with the $L^2$-shift~$2\vartheta+\kappa\phi^2$. Assuming this shift to be constant and $\Omega=(0,1)^d$ as the spatial domain, Assumption~\ref{ass:gap} is satisfied; see~\cite[Ch.~12]{Wlo87}. 
%	
Moreover, the nonlinear operator from~\eqref{eq:Bphi-GP} fulfills  
%
\[
%\trace\, \out{\calJ \calB_{\phi}\, \psi}{\psi} = 
\big( \calJ \calB_{\phi}\, \psi, \psi \big)_{L^2(\Omega)}
= \big\langle \calB_{\phi}\,\psi, \psi \big\rangle 
= 2\kappa \int_{\Omega} \phi^2\, \psi^2 \dx 
\ge 0,
\]	
such that Theorem~\ref{th:positiveHessian} is applicable.

% FEM
For the spatial discretization of the Gross--Pitaevskii problem \eqref{eq:GPeig}, we use  a~biquadratic finite element method on a~Cartesian mesh of width $h$. Denoting the resulting stiffness matrix by~$A$, the mass matrix by $M$, and the weighted mass matrices by $M_{\vartheta}$ and $M_{\varphi^2}$, respectively, the discrete eigenvalue problem reads 
%
\[
A \varphi + M_{\vartheta} \varphi + \kappa M_{\varphi^2} \varphi 
= \lambda\, M \varphi, \qquad \varphi^TM\varphi =1 
\]
with $\varphi\in \R^n$, where $n$ denotes the number of degrees of freedom.
%
With this, the discrete version of~$\out{\phi}{\calJ\!\calA_\phi\,\phi}=\big(\phi,\calJ\!\calA_\phi\,\phi\big)_{L^2(\Omega)}$ equals~$\lambda_\varphi = \varphi^T (A + 2M_{\vartheta} + \kappa M_{\varphi^2})\, \varphi$ 
and the Newton equation takes the form
%
% Hessian for p=1
%\begin{align*}
%	\Hess_H\calE(\phibf) [\psibf] 
%	&= \calP_{\phibf,H} \big( \calJ\!\calA_\phibf\, \psibf+  \calJ\calB_\phibf\,\psibf-\psibf\,  \out{\phibf}{ \calJ\!\calA_\phibf\,\phibf}\big) \\
%	%
%	&= \calJ\!\calA_\phibf\, \psibf + \calJ\calB_\phibf\,\psibf - \psibf\,  \out{\phibf}{\calJ\!\calA_\phibf\,\phibf} - \phibf \out{\phibf}{\calJ\!\calA_\phibf\, \psibf + \calJ\calB_\phibf\, \psibf}
%\end{align*}
%
% right-hand side: 
%\begin{equation*}
%	-\grad_H \calE(\phibf) 
%	= -\calJ\!\calA_\phibf\,\phibf + \phibf\, \out{\phibf}{\calJ\!\calA_\phibf\,\phibf}.
%\end{equation*}
%
% --> $\calJ\!\calA_\phibf \triangleq A + 2M_V + \kappa M_{\phi^2}$ 
% --> $\calJ\calB_\phibf \triangleq 2\kappa M_{\phi^2}$ 
%
\begin{align*}
	(I - M\varphi \varphi^T)\big((A + 2M_{\vartheta} + 3\kappa M_{\varphi^2})\, \psi - \lambda_\varphi M \psi\big) 
	= - (I - M\varphi \varphi^T)(A + 2M_{\vartheta} + \kappa M_{\varphi^2})\, \varphi
\end{align*}
%
with unknown $\psi\in \{\xi\in \R^n\;:\; \xi^TM\varphi = 0\}=\im (I-\varphi\varphi^T\! M)$.

We demonstrate the performance of the resulting Riemannian Newton method in comparison with the SCF iteration combined with the optimal damping algorithm (ODA) proposed in~\cite{DioC07} and the energy-adapted Riemannian gradient descent method (RGD) of~\cite{HenP20} with a non-monotone step size control as outlined in~\cite{AltPS21}. The numerical experiments are performed on a~sufficiently large bounded domain  $\Omega = (-L,L)^2$, $L=8$, for two types of trapping potentials. In Section~\ref{sect:numerics:GPEVP:exp1}, we consider a simple harmonic trap, whereas in Section~\ref{sect:numerics:GPEVP:exp2}, we add an~additional disorder potential. The interaction parameter~$\kappa$ as well as the spatial resolution~$h$ will be specified below, separately for each case. 
%
%
%results are generated from Matlab script ../../codes/Q24GPE/experiments/groundstate/numexp4NewtonPaperGPE
\subsubsection{Ground state in a harmonic trap}\label{sect:numerics:GPEVP:exp1} 
For the first numerical experiment, we consider the harmonic trapping potential
%
\begin{equation}\label{eq:Vharm}
\vartheta_\text{harm}(x)  
= \tfrac{1}{2}|x|^2
\end{equation}
%
and interaction parameters $\kappa=10,100,1000$. The resulting ground states computed on a Cartesian mesh of width $h/(2L) = 2^{-10}$ are depicted in Figure~\ref{fig:groundstateHarmonic}. 
%
% Figure environment removed
%
To generate a~joint and sufficiently accurate initial value for the three solvers of the discretized nonlinear eigenvector problem, we run the energy-adapted RGD method starting from the biquadratic finite element interpolation of the constant $1$ (respecting the homogeneous Dirichlet boundary condition). We stopped this iteration once the residual fell below the $10^{-2}$ tolerance and used the approximated ground state as the initial state to compare the asymptotic behavior of the three different solvers. The corresponding convergence histories are presented in Figure~\ref{fig:convergenceHarmonic} showing the evolution of the residuals during the iteration processes. It can be observed that the Riemannian Newton method (with sparse direct solution of the Newton equation using the Sherman--Morrison formula~\cite{SheM50}) reaches the tolerance of $10^{-8}$ in only three steps. While the performances of the SCF iteration and the energy-adaptive RGD method abate with increasing $\kappa$, the Riemannian Newton scheme appears to be extremely robust. 

% Figure environment removed
%
The convergence behavior of the Riemannian Newton scheme is also robust to the underlying mesh size $h$ and, hence, independent of the dimension of the discretization space. This is demonstrated in Figure~\ref{fig:meshind} for a fixed choice of $\kappa=1000$. We consider a sequence of meshes with $h/(2L) = 2^{-1},\ldots,2^{-10}$ and use the same procedure as above to generate initial guesses with residuals of order $10^{-2}$. The left graph shows the number of iterations of the Riemannian Newton method to fall below the tolerance of $10^{-10}$ for each of these mesh sizes. An increase in the number of iterations with smaller mesh size is not observed. In our experience, this mesh independence of the Riemannian Newton optimization scheme is representative for many other choices of potentials and interaction parameters. For completeness, Figure~\ref{fig:meshind} also shows the corresponding errors in the minimum energy approximation as a function of the mesh size, demonstrating the optimal fourth-order convergence rate of the biquadratic finite element implementation~\cite{henning2023discrete}. 
%
% Figure environment removed

\subsubsection{Localized ground state in a disorder potential}\label{sect:numerics:GPEVP:exp2}
The second experiment considers the computationally more difficult case where the external potential is the sum of the harmonic potential $\vartheta_\text{harm}$ defined in~\eqref{eq:Vharm} and a~potential $\vartheta_\text{rand}$ reflecting a high degree of disorder. The disorder part $\vartheta_\text{rand}$ is chosen as a~piecewise constant function on the Cartesian mesh of width $2L\varepsilon$, $\varepsilon = 2^{-6}$, taking values $0$ or $\varepsilon^{-2}$ as depicted in Figure~\ref{fig:groundstateDisorder}.
%
For a potential in such a scaling regime, the low-energy eigenstates essentially localize in terms of an~exponential decay of their moduli relative to the small parameter $\varepsilon$. For the linear case, i.e., for~$\kappa=0$, this has been analyzed in~\cite{AltHP20}. For growing~$\kappa$, the ground state consists of a growing number of localized peaks; see~Figure~\ref{fig:groundstateDisorder}. Further details on the phenomenon of localization in the Gross--Pitaevskii equation and the onset of delocalization can be found in~\cite{AltP19,AltHP22}.
%
As in the previous experiments, we use biquadratic finite elements on a Cartesian mesh of width $h/(2L) = 2^{-10}$. To illustrate the localization behavior that occurs with the current parameter scaling for $\kappa\lesssim 1$, we consider the interaction parameters $\kappa=0.1,1,10$. The ground states for $\kappa=1,10$ are shown in Figure~\ref{fig:groundstateDisorder}. The ground state for $\kappa=0.1$ is hardly distinguishable from the one for $\kappa=1$ and, therefore, it is not shown in a~separate figure. 
%
% Figure environment removed

% numerical results
Figure~\ref{fig:convergenceDisorder} displays the convergence history of the residuals for~$\kappa=0.1,1,10$. We employed the same strategy as in Section~\ref{sect:numerics:GPEVP:exp1} to generate suitable initial guesses with the residuals of order~$10^{-2}$ used for all methods. The results clearly indicate that the ground state computations with the disorder potential are already challenging for smaller values of $\kappa$. Particularly, the energy-adapted RGD method needs much larger iteration counts, which according to \cite{henning2023dependency}, may be related to smaller spectral gaps between the first and second eigenvalue. The Riemannian Newton method, on the other hand, still performs well and reaches the prescribed tolerance $10^{-8}$ for the residual in only a few steps in all three examples. For comparison, the SCF iteration converges very fast in the almost linear case but suffers from larger values of~$\kappa$ as the energy-adapted RGD method.  
%
% Figure environment removed
%
%=============================================================================
\subsection{Kohn--Sham model}\label{sect:numerics:KS}
For the Kohn--Sham energy functional $\calE_{\rm KS}$ introduced in~\eqref{eq:energyKS}, we have 
%
\begin{align*}
	\langle \calA_\phibf\,\vbf, \wbf\rangle
	%	= a_\phibf(\psibf,\wbf)
	& = \int_\Omega \tr\bigl((\nabla \vbf)^T\nabla\wbf\bigr) \dx 
	+ 2 \int_\Omega  \vartheta_\text{ion}\, \vbf\cdot\wbf \dx \\
	& \qquad + 2\int_\Omega \!\Big(\!\int_{\Omega} \frac{\rho(\phibf(y))}{\|x-y\|}\, \dy \Big)\, \vbf\cdot\wbf \dx
	+ 2 \int_\Omega \mu_{\rm xc}(\rho(\phibf))\, \vbf\cdot\wbf \dx
\end{align*}
%
with $\mu_{\rm xc}(\rho) = \frac{\rm d}{{\rm d}\rho} \big(\rho\,\epsilon_{\rm xc}(\rho)\big)$. Moreover, the operator $\calB_\phibf$ has the form 
%
\begin{align*}
	\langle\calB_\phibf\,\vbf,\wbf\rangle
	= 4 \int_{\Omega}\!\Big(\!\int_{\Omega} \frac{\phibf\cdot\vbf}{\|x-y\|} \dy \Big) \,\phibf\cdot\wbf \dx 
	+ 4 \int_{\Omega} \zeta_{\rm xc}(\rho(\phibf)) (\phibf\cdot\vbf)\, (\phibf\cdot \wbf) \dx, 
\end{align*} 
%
where $\zeta_{\rm xc}(\rho)=\frac{{\rm d}}{{\rm d}\rho} \mu_{\rm xc}(\rho)$. 
%
The exchange-correlation function $\epsilon_\text{xc}(\rho)$ can additively be decomposed as \mbox{$\epsilon_\text{xc}(\rho) =\epsilon_\text{x}(\rho) + \epsilon_\text{c}(\rho)$}, where the exchange component $\epsilon_\text{x}(\rho)$ has the particular analytical expression  $\epsilon_\text{x}(\rho)=-\frac{3}{4}\big(\frac{3}{\pi}\rho\big)^{1/3}$ and the  correlation component $\epsilon_\text{c}(\rho)$ is usually unknown, but can be fitted by using quantum Monte-Carlo data~\cite{PerW92}. 
For the numerical experiments, we use the MATLAB toolbox KSSOLV~\cite{YanMLW09}, in which the correlation component is implemented as 
%
\begin{equation*}%\label{eq:eps_c}
	\epsilon_\text{c}(\rho) = \left\{\begin{array}{ll} 
		a_1 + a_2\,r(\rho) + \big(a_3 + a_4\,r(\rho)\big) \ln(r(\rho)), &\text{if } r(\rho)<1, \\[0.3em]
		b_1^{-1} \big( 1+b_2\,\sqrt{r(\rho)}+b_3\,r(\rho) \big), 
%		\displaystyle{\frac{b_1}{1+b_2\,\sqrt{r(\rho)}+b_3\,r(\rho)}}, &
	 	&\text{if } r(\rho)\geq 1,\end{array}\right.
\end{equation*}
%
where $r(\rho)=\big(\frac{4\pi}{3}\rho\big)^{-1/3}$ is the Wigner-Seitz radius, and $a_j,b_j\in\R$ are fitted constants; see~\cite[App.~C]{PerZ81}. 

For the spatial discretization, we employ the planewave discretization method as used in KSSOLV. With~$n$ denoting the number of degrees of freedom, the matrix $\Phi\in\C^{n\times p}$ contains the coefficients of the approximation of the wave function $\phibf$. Then the discretized Kohn--Sham energy functional is given by 
%
\[
E(\Phi) 
= \frac{1}{2} \trace\big(\Phi^*(L+2D_{\rm ion})\Phi\big) + 
\frac{1}{2}\,\rho_h(\Phi)^TL^+\rho_h(\Phi) + \rho_h(\Phi)^T\epsilon_{\rm xc}(\rho_h(\Phi)),
\]
%
where $\Phi^*$ denotes the complex conjugate transpose of $\Phi$, $L\in\C^{n\times n}$ is the discrete Laplace matrix, $L^+\in\C^{n\times n}$ its pseudoinverse, \mbox{$D_{\rm ion}\in\R^{n\times n}$} the discretized ionic potential, and~$\rho_h(\Phi) = \diag(\Phi\Phi^*) \in \R^n$ the discretized electronic charge density. Note that the matrix $L$ is Hermitian and $D_{\rm ion}$ is diagonal. In this setting, the minimization problem 
\[
\min\limits_{\Phi\in\Stiefelpn} E(\Phi)
\]
 on the (compact) Stiefel manifold 
%
$
\Stiefelpn 
= \big\{ \Phi\in \C^{n\times p}\; :\; \Phi^* \Phi = I_p \big\} 
$
%
leads to the finite-dimensional nonlinear eigenvector problem 
%
\begin{equation*}%\label{eq:NLEVPdiscr}
	\arraycolsep=2pt
	\begin{array}{rcl}
		A(\Phi)\Phi-\Phi\, \Lambda & = & 0, \\
		\Phi^*\Phi -I_p& = & 0, 
	\end{array}
\end{equation*}
%
where the discrete Kohn--Sham operator is given by
%
\[
A(\Phi) 
= L + 2\, D_{\rm ion} + 2\Diag\big(L^+\rho_h(\Phi) + \mu_{\rm xc}(\rho_h(\Phi))\big).
\]
%
Further, the Riemannian gradient of $E(\Phi)$ becomes
\begin{equation}\label{eq:gradEdiscr}
\grad E(\Phi)
= (I-\Phi\Phi^*)A(\Phi)\Phi
= A(\Phi)\Phi-\Phi\big(\Phi^*A(\Phi)\Phi\big).
\end{equation}
%
In the Riemannian Newton method on the Stiefel manifold~$\Stiefelpn$, we need to solve the equation 
%
\begin{equation}\label{eq:NewtonEqSd}
%	\Hess_e E(\Phi)\Psi
	P_{\Phi}\big(A(\Phi)\Psi+B(\Phi,\Psi)-\Psi\Phi^*A(\Phi)\Phi\big)
%	= (I-\frac{1}{2}\Phi\Phi^*)A(\Phi)\Psi+ (I-\Phi\Phi^*) B(\Phi,\Psi)	- (I-\frac{1}{2}\Phi\Phi^*)\Psi\Phi^*A(\Phi)\Phi -\frac{1}{2}\Phi\big(\Psi^*A(\Phi)-\Phi^*A(\Phi)\Phi\Psi^*\big)\Phi
	= -(I-\Phi\Phi^*) A(\Phi)\Phi 
\end{equation}
%
for $\Psi$ belonging to the tangent space~$T_\Phi\, \Stiefelpn$. Therein, 
%
\[
	P_{\Phi}(Y)
	= Y - \tfrac12\, \Phi \big(\Phi^* Y + Y^* \Phi\big)  
\]
%
is the orthogonal projector onto~$T_\Phi\,\Stiefelpn$ and 
%
\[
	B(\Phi,\Psi) 
	= 2\Diag\big((L^++\Diag(\zeta_{\rm xc}(\rho_h(\Phi))))\diag(\Phi\Psi^*+\Psi\Phi^*)\big)\Phi 
\]
%
is the discretization of the operator $\calB_\phibf$. On the Grassmann manifold $\Grasspn=\Stiefelpn/\OrthGr$, the Newton equation takes the form 
%
\begin{equation}\label{eq:NewtonEqGd}
	(I-\Phi\Phi^*) \big(A(\Phi)\Psi_{\Phi}^{\rm h} + B(\Phi,\Psi_{\Phi}^{\rm h})-\Psi_{\Phi}^{\rm h}\Phi^*A(\Phi)\Phi\big) 
	= -(I-\Phi\Phi^*) A(\Phi)\Phi
\end{equation}
%
for $\Psi_\Phi^{\rm h}\in\calH_{\Phi} = \{\Psi\in T_\Phi\, \Stiefelpn\; :\; \Phi^*\Psi=0\}$. 

In our experiments, we compare the calculation of the ground state by using the SCF iteration with the Anderson charge mixing scheme \cite{YanMLW09}, the energy-adaptive RDG with non-monotone step size control \cite{AltPS21}, and the Riemannian Newton methods on the Stiefel manifold (RNS) and on the Grassmann manifold (RNG). In all these methods, we choose the same initial guess for the wave function by performing one SCF step with a~randomly generated starting point and stop the iterations once the Frobenius norm of the residual 
%\[
$R(\Phi_k) = A(\Phi_k)\Phi_k-\Phi_k\Lambda_k$
%\]
 with $\Lambda_k=\Phi_k^TA(\Phi_k)\Phi_k$ is smaller than the tolerance~$10^{-8}$. Note that due to \eqref{eq:gradEdiscr}, $\|R(\Phi_k)\|_F=\|\grad E(\Phi_k)\|_F$, i.e., the norms of the residuals provide the information on the size of the Riemannian gradients.
In both Newton methods and the energy-adaptive RGD method, we use the qR decomposition based retractions. The reference minimal energy~$E_{\min}$ is computed by the RNG method with the tolerance~$10^{-10}$. 

All algorithms are performed in an {\em inexact manner}, i.e., the occurring linear systems are only solved up to a certain tolerance. In RNG, for instance, we follow Algorithm~\ref{alg:infdim:RiemNewtonGr}  using MINRES as a linear system solver with a maximal number of iterations~$\ell_{\max}=15$ and the adaptive tolerance $\min(1/k, 10^{-3} \|\grad E(\Phi_{k-1})\|_F)$. The remaining parameters are chosen as~\mbox{$\eta=10^{-8}$}, $\delta=0.5$, and~$\sigma=10^{-4}$. In RNS, we proceed similarly, with the only difference that instead of \eqref{eq:NewtonEqGd} we solve the Newton equation of the form~\eqref{eq:NewtonEqSd}. For solving the linear eigenvalue problems in SCF, we employ the KSSOLV built-in LOBPCG algorithm for the pentacene model in Section~\ref{sect:numerics:KS:exp1} and the MATLAB built-in function {\tt eigs} for the graphene model in Section~\ref{sect:numerics:KS:exp2}. Switching to  another eigenvalue solver is necessary due to ill-conditioning in LOBPCG for the latter example. In both cases, the tolerance for the  inner iterations is set to be $\min(10^{-3}, 10^{-3} \|\grad E(\Phi_{k-1})\|_F)$. 
%
%
\subsubsection{Pentacene molecule}\label{sect:numerics:KS:exp1}
In the first numerical experiment, we calculate the ground state for the pentacene molecule ${\rm C}_{22}{\rm H}_{14}$ with $p=51$ electron orbitals. 
A~spatial planewave discretization on a~$80\times\ 55\times 160$ sampling grid gives  the discrete model of dimension $n=44791$. In Figure~\ref{fig:convergencePentacene}, we present the convergence history of the residuals  and the energy reduction during the iterations. One can see that both Newton methods have very similar behavior and converge within $8$ and $9$ iterations, respectively. In comparison, the SCF method requires $18$ iterations to converge, whereas the energy-adapted RGD stagnates at the level slightly above $10^{-8}$ after $73$ iterations; see Table~\ref{eq:KSresults} showing the values of the energy functional, the reached residuals, the number of (outer) iterations, and the CPU time.
%
% Figure environment removed
%
%
\subsubsection{Graphene  lattice}\label{sect:numerics:KS:exp2}
As the second model, we consider a~graphene lattice consisting of carbon atoms arranged in $9$ hexagons with $p=67$ electron orbitals.  We use a~$32\times\ 55\times 160$  sampling grid for the wave function and get a discretized model of dimension  $n=12279$. Figure~\ref{fig:convergenceGraphene} presents the evolution of the residuals and errors in the energy. We observe again that both Newton methods converge very fast compared to the energy-adaptive RGD method which needs $92$ iterations to achieve the tolerance $10^{-8}$ for the residual. In contrary, the SCF iteration has difficulties to converge. Other mixing strategies implemented in KSSOLV do not improve the convergence property of SCF for the graphene model. 
%
% Figure environment removed
%
\begin{table}[th]
\caption{Numerical results for the pentacene and the graphene models.}
\label{eq:KSresults}
\begin{tabular}{c||r|c|c|c|r}		
%	\hline \hline 
	model & method & energy & residual & \#  iterations & CPU time [$s$] \\
	\hline 
	\hline 
	%\multicolumn{5}{l}{Pentacene molecule ${\rm C}_{22}{\rm H}_{14}$, $n=44791$, $p=51$} \\
pentacene &	RNS & $107.28$ & $3.3788\,e\!-\!9$ &$\quad 9$   &  ${2511.33}$\\
&		RNG & $107.28$ & $6.4300\,e\!-\!9$ &$\quad 8$   & $2279.31$\\
&		SCF      & $107.28$ & $7.7982\,e\!-\!9$ & $\enskip 18$ &  $3065.41$\\
&		eaRGD &  $107.28$ & $2.8223\,e\!-\!8$ & $200$ &  $10053.75$\\ %5501.87 for kmax=100	
	\hline \hline
%	\multicolumn{5}{l}{Graphene lattice, $n=12279$, $p=67$} \\
graphene &	RNS & $592.55$ & $5.5035\,e\!-\!9$ &$\quad 8$   &  ${816.77}$\\
&		RNG & $592.55$ & $6.0668\,e\!-\!9$ & $\quad 9$   &  ${938.69}$\\
&		SCF      & $592.66$ & $1.7906\,e\!-\!3$ & $200$ &  $12941.35$\\
&		eaRGD &  $592.55$ & $8.1525\,e\!-\!9$ & $\enskip 92$ &  $2283.77$
\end{tabular}
\end{table}
%
%
%=============================================================================
%=========  Conclusion
%=============================================================================
\section{Conclusion}
In this paper, we have derived Riemannian Newton methods on the infinite-dimensional Stiefel and Grassmann manifolds for Kohn--Sham type energy minimization problems. Starting from an energy functional, we present a unified approach for applications in computational physics (e.g., the Gross--Pitaevskii eigenvalue problem) and computational chemistry (e.g., the Kohn--Sham model). 
%
The remarkable gain in computational efficiency of the Riemannian Newton methods compared to the so far more popular methods such as SCF and gradient descent methods is demonstrated by a series of numerical experiments. 
%
%
%=============================================================================
%=========  Bibliogrphy
%=============================================================================
\bibliographystyle{alpha}
\bibliography{refKS}
%
\end{document}
