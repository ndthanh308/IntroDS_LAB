@inproceedings{QuASoQ,
  title={Using Rule Mining for Automatic Test Oracle Generation},
  author={Duque-Torres, Alejandra and Shalygina, Anastasiia and Pfahl, Dietmar and Ramler, Rudolf},
  booktitle={8th International Workshop on Quantitative Approaches to Software Quality },
  series = {QuASoQ'20},
  year={2020}
}

@inproceedings{hilton_large-scale_2018,
	address = {Montpellier France},
	title = {A large-scale study of test coverage evolution},
	isbn = {978-1-4503-5937-5},
	url = {https://dl.acm.org/doi/10.1145/3238147.3238183},
	doi = {10.1145/3238147.3238183},
	abstract = {Statement coverage is commonly used as a measure of test suite quality. Coverage is often used as a part of a code review process: if a patch decreases overall coverage, or is itself not covered, then the patch is scrutinized more closely. Traditional studies of how coverage changes with code evolution have examined the overall coverage of the entire program, and more recent work directly examines the coverage of patches (changed statements). We present an evaluation much larger than prior studies and moreover consider a new, important kind of change — coverage changes of unchanged statements. We present a large-scale evaluation of code coverage evolution over 7,816 builds of 47 projects written in popular languages including Java, Python, and Scala. We ﬁnd that in large, mature projects, simply measuring the change to statement coverage does not capture the nuances of code evolution. Going beyond considering statement coverage as a simple ratio, we examine how the set of statements covered evolves between project revisions. We present and study new ways to assess the impact of a patch on a project’s test suite quality that both separates coverage of the patch from coverage of the non-patch, and separates changes in coverage from changes in the set of statements covered.},
	language = {en},
	urldate = {2021-04-23},
	booktitle = {Proceedings of the 33rd {ACM}/{IEEE} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {ACM},
	author = {Hilton, Michael and Bell, Jonathan and Marinov, Darko},
	month = sep,
	year = {2018},
	pages = {53--63},
	file = {Hilton et al. - 2018 - A large-scale study of test coverage evolution.pdf:G\:\\My Drive\\work\\papers and articles\\zotero\\storage\\DE8PE4CD\\Hilton et al. - 2018 - A large-scale study of test coverage evolution.pdf:application/pdf},
}

@book{desikan2006software,
  title={Software Testing: Principles and Practice},
  author={Desikan, S. and Ramesh, G.},
  isbn={9788177581218},
  url={https://books.google.ee/books?id=Yt2yRW6du9wC},
  year={2006},
  publisher={Pearson Education Canada}
}

@article{DANGLOT2019110398,
author = {Benjamin Danglot and Oscar Vera-Perez and Zhongxing Yu and Andy Zaidman and Martin Monperrus and Benoit Baudry},
date-modified = {2023-03-30 11:11:09 +0300},
doi = {https://doi.org/10.1016/j.jss.2019.110398},
issn = {0164-1212},
journal = {Journal of Systems and Software},
keywords = {Test amplification, Test augmentation, Test optimization, Test regeneration, Automatic testing},
pages = {110398},
read = {1},
title = {A snowballing literature study on test amplification},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219301736},
volume = {157},
year = {2019},
bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0164121219301736},
bdsk-url-2 = {https://doi.org/10.1016/j.jss.2019.110398}}

@InProceedings{10.1007/978-981-16-7618-5_40,
author="Raza, Ali
and Shah, Babar
and Ashraf, Madnia
and Ilyas, Muhammad",
editor="Ullah, Abrar
and Anwar, Sajid
and Rocha, {\'A}lvaro
and Gill, Steve",
title="Object-Oriented Software Testing: A Review",
booktitle="Proceedings of International Conference on Information Technology and Applications",
year="2022",
publisher="Springer Nature Singapore",
address="Singapore",
pages="461--467",
abstract="Object-oriented (OO) software systems present specific challenges to the testing teams. As the object-oriented software contains the OO methodology and its different components, it is hard for the testing teams to test the software with arbitrary software components and the chance of errors could be increased. So different techniques, models, and methods researchers identified to tackle these challenges. In this paper, we are going to analyze and study the OO software testing. For handling challenges in OO software testing, different techniques and methods are proposed like UML diagrams, evolutionary testing, genetic algorithms, black-box testing, and white-box testing. The methodology used for research is literature review (LR) of the recent decay.",
isbn="978-981-16-7618-5"
}

@article{10.1002/stv.430,
author = {Yoo, S. and Harman, M.},
title = {Regression Testing Minimization, Selection and Prioritization: A Survey},
year = {2012},
issue_date = {March 2012},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {22},
number = {2},
issn = {0960-0833},
url = {https://doi.org/10.1002/stv.430},
doi = {10.1002/stv.430},
abstract = {Regression testing is a testing activity that is performed to provide confidence that changes do not harm the existing behaviour of the software. Test suites tend to grow in size as software evolves, often making it too costly to execute entire test suites. A number of different approaches have been studied to maximize the value of the accrued test suite: minimization, selection and prioritization. Test suite minimization seeks to eliminate redundant test cases in order to reduce the number of tests to run. Test case selection seeks to identify the test cases that are relevant to some set of recent changes. Test case prioritization seeks to order test cases in such a way that early fault detection is maximized. This paper surveys each area of minimization, selection and prioritization technique and discusses open problems and potential directions for future research. Copyright © 2010 John Wiley &amp; Sons, Ltd.},
journal = {Softw. Test. Verif. Reliab.},
month = {mar},
pages = {67–120},
numpages = {54},
keywords = {regression test selection, test suite minimization, test case prioritization, regression testing}
}

@inproceedings{10.1145/3550355.3552451,
author = {Khorram, Faezeh and Bousse, Erwan and Mottu, Jean-Marie and Suny\'{e}, Gerson and G\'{o}mez-Abajo, Pablo and Ca\~{n}izares, Pablo C. and Guerra, Esther and de Lara, Juan},
title = {Automatic Test Amplification for Executable Models},
year = {2022},
isbn = {9781450394666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550355.3552451},
doi = {10.1145/3550355.3552451},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems},
pages = {109–120},
numpages = {12},
keywords = {regression testing, executable DSL, test amplification, executable model},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@article{patel2019partial,
	title={A partial oracle for uniformity statistics},
	author={Patel, Krishna and Hierons, Robert M},
	journal={Softw. Quality Journal},
	volume={27},
	number={4},
	pages={1419--1447},
	year={2019},
	publisher={Springer}
}

@inproceedings{10.1145/3266237.3266273,
	author = {Braga, Rony\'{e}rison and Neto, Pedro Santos and Rab\^{e}lo, Ricardo and Santiago, Jos\'{e} and Souza, Matheus},
	title = {A Machine Learning Approach to Generate Test Oracles},
	year = {2018},
	isbn = {9781450365031},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	booktitle = {Proc. of the XXXII Brazilian Symp. on Softw. Eng.},
	pages = {142–151},
	numpages = {10},
	keywords = {testing automation, machine learning, test oracle},
	location = {Sao Carlos, Brazil},
	series = {SBES '18}
}

@ARTICLE{6963470,  
	author={E. T. {Barr} and M. {Harman} and P. {McMinn} and M. {Shahbaz} and S. {Yoo}},  journal={IEEE Trans. on Softw. Eng.},   
	title={The Oracle Problem in Software Testing: A Survey},   
	year={2015}, 
	volume={41},  
	number={5},  
	pages={507-525},}

@InProceedings{10.1007/11785477_23,
author="Xie, Tao",
editor="Thomas, Dave",
title="Augmenting Automatically Generated Unit-Test Suites with Regression Oracle Checking",
booktitle="ECOOP 2006 -- Object-Oriented Programming",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="380--403",
abstract="A test case consists of two parts: a test input to exercise the program under test and a test oracle to check the correctness of the test execution. A test oracle is often in the form of executable assertions such as in the JUnit testing framework. Manually generated test cases are valuable in exposing program faults in the current program version or regression faults in future program versions. However, manually generated test cases are often insufficient for assuring high software quality. We can then use an existing test-generation tool to generate new test inputs to augment the existing test suite. However, without specifications these automatically generated test inputs often do not have test oracles for exposing faults. In this paper, we have developed an automatic approach and its supporting tool, called Orstra, for augmenting an automatically generated unit-test suite with regression oracle checking. The augmented test suite has an improved capability of guarding against regression faults. In our new approach, Orstra first executes the test suite and collects the class under test's object states exercised by the test suite. On collected object states, Orstra creates assertions for asserting behavior of the object states. On executed observer methods (public methods with non-void returns), Orstra also creates assertions for asserting their return values. Then later when the class is changed, the augmented test suite is executed to check whether assertion violations are reported. We have evaluated Orstra on augmenting automatically generated tests for eleven subjects taken from a variety of sources. The experimental results show that an automatically generated test suite's fault-detection capability can be effectively improved after being augmented by Orstra.",
isbn="978-3-540-35727-8"
}

@article{paper,
	abstract = {In the literature, there is a rather clear segregation between manually written tests by developers and automatically generated ones. In this paper, we explore a third solution: to automatically improve existing test cases written by developers. We present the concept, design and implementation of a system called DSpot, that takes developer-written test cases as input (JUnit tests in Java) and synthesizes improved versions of them as output. Those test improvements are given back to developers as patches or pull requests, that can be directly integrated in the main branch of the test code base. We have evaluated DSpot in a deep, systematic manner over 40 real-world unit test classes from 10 notable and open-source software projects. We have amplified all test methods from those 40 unit test classes. In 26/40 cases, DSpot is able to automatically improve the test under study, by triggering new behaviors and adding new valuable assertions. Next, for ten projects under consideration, we have proposed a test improvement automatically synthesized by DSpot to the lead developers. In total, 13/19 proposed test improvements were accepted by the developers and merged into the main code base. This shows that DSpot is capable of automatically improving unit-tests in real-world, large scale Java software.},
	author = {Danglot, Benjamin and Vera-P{\'e}rez, Oscar Luis and Baudry, Benoit and Monperrus, Martin},
	date = {2019/08/01},
	date-added = {2023-04-13 02:07:57 +0300},
	date-modified = {2023-04-13 02:07:57 +0300},
	doi = {10.1007/s10664-019-09692-y},
	id = {Danglot2019},
	isbn = {1573-7616},
	journal = {Empirical Software Engineering},
	number = {4},
	pages = {2603--2635},
	title = {Automatic test improvement with DSpot: a study with ten mature open-source projects},
	url = {https://doi.org/10.1007/s10664-019-09692-y},
	volume = {24},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1007/s10664-019-09692-y}}

@inproceedings{abdi2019test,
  title={Test amplification in the pharo smalltalk ecosystem},
  author={Abdi, Mehrdad and Rocha, Henrique and Demeyer, Serge},
  booktitle={Proceedings IWST 2019 International Workshop on Smalltalk Technologies. ESUG},
  year={2019}
}

@article{https://doi.org/10.1002/smr.2490,
author = {Schoofs, Ebert and Abdi, Mehrdad and Demeyer, Serge},
title = {AmPyfier: Test amplification in Python},
journal = {Journal of Software: Evolution and Process},
volume = {34},
number = {11},
pages = {e2490},
keywords = {mutation testing, Python, test amplification, unit testing},
doi = {https://doi.org/10.1002/smr.2490},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2490},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/smr.2490},
abstract = {Abstract Test amplification aims to automatically improve a test suite. One technique generates new test methods through transformations of the original tests. These test amplification tools heavily rely on analysis techniques that benefit a lot from type declarations present in the source code of projects written in statically typed languages. In dynamically typed languages, such type declarations are not available, and therefore, research regarding test amplification for those languages is sparse. Recent work has brought test amplification to the dynamically typed language Pharo Smalltalk by introducing the concept of dynamic type profiling. The technique is dependent on Pharo-specific frameworks and has not yet been generalized to other languages. Another significant downside in test amplification tools based on the mutation score of a test suite is their high time cost. In this paper, we present AmPyfier, a tool that brings test amplification and type profiling to the dynamically typed language Python. AmPyfier introduces multi-metric selection in order to increase the time efficiency of test amplification. We evaluated AmPyfier on 11 open-source projects and found that AmPyfier could strengthen 37 out of 54 test classes. Multi-metric selection decreased the time cost ranging from 17\% to 98\% as opposed to selection based on the full mutation score.},
year = {2022}
}
