\section{Introduction}

This documents is divided in three sections:the first one describe how to use the results from the main paper to study the residual noise of low rank approximation filter for non matrix data,  the second one contains the results of the Montecarlo simulation for all the model parameters, the third one contains a qualitative description of the simulated datasets and its analysis with low-rank approximation filter.

All the images, and the caption reported in this document where previously published by Roberto Francischello in the Ph.D. thesis \textit{"Development of new experimental and data processing methods at critical signal-to-noise conditions in nuclear magnetic resonance"}, and here reproduced with his permission.
\section{Residual noise characterization for non matrix datasets}

\subsection{Low-rank approximation filter based on Higher Order Singular Values Decomposition}

In the main article we provided a brief explanation of how HOSVD could be use to build low-rank approximation filter for multi-dimensional datasets. This is the most common approach to the low-rank filter of tensor data with additive noise in MRI.
In the following, we try to make the whole procedure more accessible by using an informal description of the HOSVD procedure.

The algorithm to calculate the HOSVD is:
\begin{itemize}
	\item	Unfold the tensor along the $k$ axis producing a matrix $M_k$
	\item	Calculate the SVD of the matrix $\textbf{M}_k$ and store the left singular vector matrix $\textbf{U}_k$ e the singular values $\textbf{S}_k$
	\item	Repeat from point 1 until all the axis of the tensor are used
	\item	Construct the Tensor core $ \mathcal{C} = \left(\textbf{U}_1, \dots , \textbf{U}_n\right) dot \mathcal{A} $, where $dot$ is the multilinear multiplication
	
\end{itemize}

A low-rank approximation of the tensor $\mathcal{A}$ is built by zeroing some entry of the core tensor, and the singular matrix Sk can be used to decide which term to withhold. Unfortunately, there isnâ€™t an Eckart-Young theorem generalization for the low-rank approximation of tensor and therefore the approximation is not optimal. 

The standard procedure for the calculation of the THOSVD is equivalent to:

\begin{itemize}
	\item Unfold the tensor into a 1-D vector
	\item	Fold the 1-D vector $L_k$ into one of the $\textbf{M}_k$ unfolding matrices
	\item	Apply the linear projection $\textbf{U}_k\textbf{U}_k^{\dagger}$ to the $\textbf{M}_k$ matrix
	\item	Vectorize the matrix $\textbf{M}_k$ 
	\item	Apply a permutation, $\pi_k$, to the 1-D vector such as $\pi_k \textbf{L}_k = vec(\textbf{M}_{k+1})$
	\item	Repeat from point 2 until all the axis of the tensor are used
\end{itemize}

Therefore, the final version of the data is obtained by a consecutive use of linear transform (linear projection and permutation), vectorization, and matricization. 

If $\pi_1, \pi_2, \dots \pi_{n-1}$ are the permutation matrix needed to obtain all the possible matrix configurations of the tensor  $\mathcal{A}$ and $\textbf{Q}_1$ to $\textbf{Q}_n$ are all the projection matrix needed to obtain truncated HOSVD approximation of the tensor A, and the transform matrix $\textbf{T}$ is equal to the product $\textbf{T} = \textbf{Q}_n \pi_{n-1} \textbf{Q}_{n-1} \pi_{n-2} \dots \pi_1 \textbf{Q}_1$, than $\textbf{T} vec(\mathcal{A}) = vec(\hat{\mathcal{A}})$ where $\hat{\mathcal{A}}$ is a low-rank approximation of the tensor $\mathcal{A}$ obtained with the THOSVD. 
Therefore, we can apply the main results of the paper and calculate the statistical distribution of the filtered data using the linear transformation which is equal to the THOSVD procedure. This procedure can be applied both to global low-rank filters and patch-based low-rank filters or any combination of the two. Patch-based local filters require extreme carefulness in the aggregation phase, the final value for the voxel in position $(x1,y1,z1)$ is the weighted mean of all the values of voxel $(x1,y1,z1)$ in the different patch. Therefore, the mean and standard deviation of the denoised signal is just an accounting exercise using the linearity of the mean and the bi-linearity of the variance.  


\subsection{Low-rank approximation filter based on Hankel matrix}
To treat mono-dimensional data using a low-rank approximation filter the original measured signal should be rearranged into a matrix form to exploit a latent low-rank structure. The most common procedure is the form a Hankel matrix, this particular matrix has a low-rank structure if the original signal is autoregressive. The Cadzow filter and the Singular Spectrum Analysis are the most used Hankel-based low-rank approximation filter. A full description of both methods goes far beyond the scope of this appendix, therefore we refer interested readers to the specialistic literature.

Both methods start with the same algorithm:
\begin{itemize}
	\item Forming the Hankel matrix $\textbf{H}$ from the original data
	\item	Calculate the SVD of $\textbf{H}$ 
	\item	Select the reconstruction rank and build the $\hat{\textbf{H}}$  low-rank matrix
	\item	Reconstruct the mono-dimensional signal by the anti-diagonal average of the matrix $\hat{\textbf{H}}$
\end{itemize}
In the Cadzow filter, the procedure is repeated until the Hankel matrix is already of the right rank.

The main result from the article covers the characterization of the residual noise from step 1 to step 3. Therefore, after the third step the signal, after its vectorization, has the mean $\mu$ and the covariance matrix $\textbf{C}$ calculated according to the main result of the paper. Due to the average over the antidiagonal, each element $x_t$ of the final signal is equal to
\begin{align*}
	x_t=\frac{1}{k_t}\sum_{ij}\ h_{ij},\quad i+j=k_t
\end{align*}

Where $h_{ij}$ is an element of the low-rank approximation matrix, $k_t$ is the number of the elements of the $t^{th}$ anti-diagonal. 

The mean of the $x_t$, due to the linearity of the operator of the expected value is:
\begin{align*}
\bar{x_t}&=\frac{1}{k_t}\sum_{ij}{\bar{H}}_{ij},\quad i+j=k_t\\
\bar{x_t}&=\frac{1}{k_t}\sum_{ij}\mu_{ij},\quad i+j=k_t
\end{align*}

The new covariance matrix $\hat{\textbf{C}}$, of the one dimensional signal, due to the bi-linearity of the operator is:
\begin{align*}
\hat{C}_{t_1t_2}&=cov\left[x_{t_1},x_{t_2}\right]\\
\hat{C}_{t_1t_2}&=cov\left[\frac{1}{k_{t1}}\sum_{i+j=k_{t_1}}H_{ij},\frac{1}{k_{t2}}\sum_{l+m=k_{t_2}}H_{lm}\right]\\
\hat{C}_{t_1t_2}&=\frac{1}{k_{t1}}\frac{1}{k_{t2}}\sum_{i+j=k_{t-1}}\sum_{l+m=k_{t_2}}cov \left[H_{ij},H_{lm}\right]
\end{align*}

The covariance of elements $\textbf{H}_{ij}$ and $\textbf{H}_{lm}$ are just the corresponding entry in the covariance matrix (after the low-rank approximation) $\textbf{C}$ depending on the vectorization operation used. If \textbf{H} is an $N \times M$ matrix, with row first vectorization the elements $H_{ij}$ correspond to the element $h_{(i-1)M+j}$ after the vectorization. Therefore, the $cov[\textbf{H}_{ij},\textbf{H}_{lm}]$ is equal to the elements $((i-1)M+j, (l-1)M+m)$ of the covariance matrix $\textbf{C}$ of the low-rank approximation matrix obtained at the third step of the algorithm.

If the starting noise is Gaussian additive noise, the residual noise after the use of the filter is still Gaussian, indeed all the operations applied from steps 1 to 4 are linear. Nevertheless, the final noise is highly correlated even if the original noise is white. In principle, the knowledge of the covariance matrix after the low-rank approximation could be used to optimize the covariance matrix of the data after the anti-diagonal average procedure. If the simple average along the anti-diagonal is substituted with a weighted average, the final correlation matrix became a function of the average coefficients. Therefore, it is possible to minimize the SNR of the final signal or minimize the norm of the covariance matrix.

Some Hankel matrix-based low-rank approximation methods, such as LORA, do not have anti-diagonal average phases, they simply use the first row and the last columns of the matrix to construct the final denoised signal. Therefore, the statistical distribution of the residual noise is directly calculated using the main results from the article. 

\section{Singular values shrinkage filters}

Singular values shrinking, reducing the values of singular values based on on heuristic or theoretical results, aims to reduce the distance between the true signal matrix and the low-rank approximation of the noisy measurements matrix. 
Among those methods the one based on random matrix theory are the most theoretically robust, they start with the SVD of the measurements matrix and define a function f(x) that reduces the intensity of the singular values according to random matrix theory to minimize the distance between the approximation matrix and the noiseless signal matrix. 

The results of the singular values shrinking procedure are equivalent to the linear transform of the measurement matrix with the linear transform $\boldsymbol{\Psi}= \mathbf{UHU}^{H}$, where $\mathbf{H}$ is a diagonal matrix whose diagonal elements are the shrinking coefficients. Then the transformed signal is equal to :

\begin{align}
	\boldsymbol{\Psi}\mathbf{M}&=\boldsymbol{\Psi} \mathbf{U}\boldsymbol{\Sigma}\mathbf{V} \\
	&= \mathbf{UH}\mathbf{U}^{H}\mathbf{U}\boldsymbol{\Sigma}\mathbf{V}\\
	&= \mathbf{UH}\boldsymbol{\Sigma}\mathbf{V}\\
	&= \mathbf{U}\boldsymbol{\Sigma}_\eta \mathbf{V}\\
	&= \mathbf{M}_\eta
\end{align}

Unlike the standard low-rank approximation problem this transform is not a projection matrix.
Nevertheless, almost all the results reported in the article's main body still hold. The mean and the covariance matrix of the filtered data are still obtained from the original mean and covariance matrix as:
\begin{align}
	\bar{\boldsymbol{\mu}} & = \boldsymbol{\Psi} \boldsymbol{\mu} \\
	\bar{\textbf{C}} & = \boldsymbol{\Psi}\textbf{C}\boldsymbol{\Psi}
\end{align}
Also, Gaussian-distributed data are still Gaussian-distributed after the application of the filter.
Finally, the use of singular values shrinking does not change the MLE estimator compared with the standard low-rank approximation filter with the same rank. Indeed, the MLE loss function depends on the products of the inverse of the transform matrix, with the transform matrix itself $\textbf{T}^+\textbf{T}$. When the filter is the standard low-rank approximation $\textbf{T} = \textbf{P}$, where $\textbf{P}$ is the projection matrix when the filter is the singular values shrinking one $\textbf{T}=\boldsymbol{\Psi}$.

\begin{align}
\boldsymbol{\Psi}^{+} \boldsymbol{\Psi} &=  \left( \textbf{UHU}^* \right)^+ \textbf{UHU}^* \\
 & = \textbf{U}^{*+} \textbf{H}^+	\textbf{U}^+ \textbf{UHU}^* \\
 & = \textbf{U}^{*+} \textbf{H}^+ \textbf{HU}^* \\
 & = \textbf{U}^{*+} \textbf{I}_r \textbf{U}^*
\end{align}
$\textbf{H}$ is a diagonal matrix, therefore $\textbf{H}^+\textbf{H}$ is a diagonal matrix, $\textbf{I}_r$ with $r$ ones along the diagonal, corresponding to the non-zero elements of $\textbf{H}$, and zeros otherwise.

This results is equal to that obtained for $\textbf{P}^+\textbf{P}$:

\begin{align}
	\textbf{P}^{+} \textbf{P} &=  \left( \textbf{U} \textbf{I}_r \textbf{U}^* \right)^+ \textbf{U}\textbf{I}_r \textbf{U}^* \\
	& = \textbf{U}^{*+} \textbf{I}_r^+	\textbf{U}^+ \textbf{U}\textbf{I}_r \textbf{U}^* \\
	& = \textbf{U}^{*+} \textbf{I}_r^+ \textbf{I}_r \textbf{U}^* \\
	& = \textbf{U}^{*+} \textbf{I}_r \textbf{U}^*
\end{align}

Since the loss function is equal for both filters the solution to the MLE should be equal. Nevertheless, this result holds only for MLE with additive Gaussian noise, the use of singular values shrinking still changes the mean and the covariance matrix of the residual noise, therefore it can be a useful tool when combined with other kinds of estimators, like peak integration, that have a stronger dependency on the MSE between the noiseless signal and the filtered one.



\section{\label{app}Additional results for the properties of the maximum likelihood estimator on filtered data}
\subsection*{Results form the Montecarlo simulation of model fitting}
This section contains the full results on the estimator performance from the Montecarlo simulation. There are 17 panels, one for each parameter. 
 
The first row of the panels present the comparison of the Mean Square Error (instead of the normalized Root Mean Square Error) on the estimated parameter for the three estimators studied. Left: comparison between the least squares estimation on the raw data\footnote{wich is also the maximum likelihood estimator because the noise is white and Gaussian}, bright green star, and the least squares estimation on the filtered data, solid lines. Right: comparison between the least squares estimation on the raw data, bright green star, and the maximum likelihood estimation on the filtered data, solid lines. All the axes are in logarithmic scale. 

The second row of the panels present the comparison between the p-values obtained using the Wilcoxon signed rank test to test the hypothesis "The difference between the absolute estimation error for the \gls{fls} or the \gls{fml} estimator and the one for the reference estimator the \gls{ols} have zero median". We reported on the abscissa the number of singular values used to define the filter and on the ordinate the \gls{snr} of the  signal. On the left the result for comparison between the \gls{fls} and the reference on the right the comparison between the \gls{fml} and the reference are reported.

Finally, the third row of the panels present the comparison of the Common Language Effect Size for the absolute error in the estimation of the parameters using the three estimators studied. We compare, pair wise, the performance of the estimation on filtered data, both in the least squares and in maximum likelihood framework, with the least squares estimation on the untreated data, as our reference. The CLES is equal to the fraction of estimation that has less absolute error than the reference. We present our data using a heat map, reporting on the abscissa the number of singular values used for defining the filter, and on the ordinate the \gls{snr} level. Bright orange means that in a higher number of pairs the estimator working on the filtered data has less absolute error compared with the reference, bright green means that the reference estimator provides a better estimation.

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

\subsection*{Estimated Cramer Rao Lower Bound }

Here we report the estimated values of the \gls{crlb} for the diagonal elements of the covariance matrix. The \gls{crlb} are reported in 4 panels, each panel reports the \gls{crlb} of the four parameters that describe that model the behavior of each molecule.

The estimation of the \gls{crlb} on the raw data, bright green star, and the \gls{crlb} estimation on the filtered data, solid lines. The number of singular values used to build the filter is color coded, from the single singular value of the yellow line to the 59 singular values of the dark purple line. The discontinuities in the line are caused by improper value for the \gls{crlb}, the \gls{crlb} is always positive while the values were negative, probably due to numerical errors. The origin of these errors is discussed in the main body of the article.



% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed