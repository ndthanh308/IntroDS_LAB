
% % Figure environment removed

% % Figure environment removed

% % Figure environment removed

% % Figure environment removed

% % Figure environment removed

% Figure environment removed

\subsection{Implementation Details}

\paragraph{Training.}
% We use Instant-NGP\fcite{NGP} as our editing framework backbone to achieve real-time editing preview. 
We select Instant-NGP~\cite{mueller2022instant} as the NeRF backbone of our editing framework.
Our implementations are based on the open-source PyTorch implementation torch-ngp~\cite{torch-ngp}. All experiments are run on a single NVIDIA RTX 3090 GPU. Note that we make a slight modification to the original network architecture. Please refer to the supplementary material for details.

During the pretraining stage, we set $\msymbol{weight_pretrain_color}=\msymbol{weight_pretrain_sigma}=1$ and the learning rate is fixed to $0.05$. During the finetuning stage, we set $\msymbol{weight_train_color} = \msymbol{weight_train_depth} = 1$ with an initial learning rate of 0.01. 
% The bit field mask of the editing space is filled so that the editing space can be fully sampled during training. 
Starting from a pretrained NeRF model, we perform 50-100 epochs of local pretraining (for about 0.5-1 seconds) and about 50 epochs of global finetuning (for about 40-60 seconds). The number of epochs and time consumption can be adjusted according to the editing type and the complexity of the scene. Note that we test our performance in the absence of tiny-cuda-nn~\cite{tiny-cuda-nn} which achieves superior speed to our backbone, which indicates that our performance has room for further optimization.
% Note that the training speed is evaluated when tiny-cuda-nn is not enabled.

\paragraph{Datasets.}
We evaluate our editing in the synthetic\Skip{lego, chair, and ship from} NeRF Blender Dataset~\cite{mildenhall2020nerf}, and the real-world captured \Skip{family and truck from}Tanks and Temples~\cite{Knapitsch2017} and \Skip{, and scan83 from} DTU~\cite{jensen2014large} datasets. We follow the official dataset split of the frames for the training and evaluation.

\subsection{Experimental Results}
\label{sec-results}
% \paragraph{Comparisons of rendering quality between teacher and student network.} 

% Figure environment removed

% Figure environment removed


\paragraph{Qualitative NeRF editing results.} 
We provide extensive experimental results in all kinds of editing categories we design, including bounding shape (\cref{fig-bbox,fig-bbox-elf}), brushing (\cref{fig-brush}), anchor (\cref{fig-anchor}), and color (\cref{fig-teaser}). Our method not only achieves a huge performance boost, supporting instant preview at second level, but also produces more visually realistic editing appearances, such as shading effects on the lifted side in \cref{fig-brush} and shadows on the bumped surface in \cref{fig-neumesh}. Besides, results produced by the student network can even outperform the teacher labels, \eg in \cref{fig-bbox-elf} the $F^t$ output contains floating artifacts due to view inconsistency. As analyzed in \cref{sec-train}, the distillation process manages to eliminate this. We also provide an example of object transfer (\cref{fig-bbox-baby}): the bulb in the Lego scene (of Blender dataset) is transferred to the child's head in the family scene of Tanks and Temples dataset.
% \Skip{
% We evaluate our method on all the editing types we design, \ie bounding shape, brushing and anchor, respectively:
% \begin{itemize}
%     \item Bounding shape editing. As shown in \cref{fig-bbox}, we scale the warning light on the top of the Lego model, shorten the chair leg, \zjs{TBD}, and provides plausible results.
%     \item Brushing and color editing. As shown in \cref{fig-brush}, our method edits the scene according to the user's paintings (\ie a cross sign on the chair back, a heart shape on the car logo, and \zjs{TBD}). Note that our brushing method supports simultaneous geometry lifting, as shown in the ``cross'' example. Due to our shading preservation strategy in HSL space, the edited surface can contain realistic visual effects (see the shading effects of the lifted surface).
%     \item Anchor editing. As shown in \cref{fig-anchor}, our method edits the scene according to the anchor points (\ie ship's bow, bulldozer's shovel and \zjs{TBD}) and the stretching direction. The edited geometry has consistent appearance with the anchored area.
% \end{itemize}
% }


% Figure environment removed

% Figure environment removed

% Figure environment removed

\paragraph{Comparisons to baselines.} Existing works have strong restrictions on editing types, which focus on either geometry editing or appearance editing, while ours is capable of doing both simultaneously. Our brushing and anchor tools can create user-guided out-of-proxy geometry structures, which no existing methods support. We make comparisons on color and texture painting supported by NeuMesh~\cite{neumesh} and Liu \etal~\cite{liu2021editing}. 

\cref{fig-neumesh} illustrates two comparisons between our method and NeuMesh~\cite{neumesh} in scribbling and a texture painting task. Our method significantly outperforms NeuMesh, which contains noticeable color bias and artifacts in the results. In contrast, our method even succeeds in rendering the shadow effects caused by geometric bumps.

\cref{fig-neumesh-mic} illustrates the results of the same non-rigid blending applied to the Mic from NeRF Blender\cite{mildenhall2020nerf}. It clearly shows 1) Non-rigid editing can be easily implemented in our framework.
% \vspace{-1em}
\begingroup
% \everymath{\scriptstyle}
% \scriptsize
%your equation
\begin{equation}
\begin{aligned}
        \mathbf{x}^{s}&=R \cdot \mathbf{x}^{t} + t,\\
        \mathbf{d}^{s}&=R \cdot \mathbf{d}^{t}, \\
    F^m \coloneqq (\mathbf{x}^{t}, \mathbf{d}^{t}) &\mapsto (\mathbf{x}^{s}, \mathbf{d}^{s}) 
\end{aligned}
\end{equation}
\endgroup
% \vspace{-1.2em}

\noindent Where $R, t$ are interpolated from the transform matrixes of the three closest coordinates of a pre-defined 3D blending control grid with position and transformation of each control point. 2) Being mesh-free, We have more details than NeuMesh [5], unlimited by mesh resolution.

We also compare our method with Liu \etal~\cite{liu2021editing} in \cref{fig-editnerf}. Liu \etal's method only supports textureless simple objects in their paper, but fails in more complex objects in the experiments of our paper. Their method causes an overall color deterioration within the edited object, which is highly unfavorable. This is because their latent code only models the global color feature of the scene instead of fine-grained local features. On the contrary, our method supports fine-grained local edits due to our local-aware embedding grids.

% \yq{describe the difference}

% \paragraph{Artistic applications (a comic on NeRF).} Based on the four example tools we implemented, we created a comic \textit{Bob the Bulb} (Fig. \ref{fig-comic}) to show the potential applications of our editing method. This might be the first artwork created with NeRF.

% \subsection{Experiments on Bounding Shape Editing}
% \subsection{Experiments on Brush Editing}
% \subsection{Experiments on Anchor Editing}
% \subsection{Experiments on Color Shape Editing}
% \subsection{Real-world Example: NeRF-Rendered Comic}

% Figure environment removed

% Figure environment removed

\subsection{Ablation Studies}
\label{sec-ablation}
\paragraph{Effect of the two-stage training strategy.} To validate the effectiveness of our pretraining and finetuning strategy, we make comparisons between our full strategy (3\textsuperscript{rd} row), finetuning-only (1\textsuperscript{st} row) and pretraining-only (2\textsuperscript{nd} row) in \cref{fig-ab_pre}. Our pretraining can produce a coarse result in only 1 second, while photometric finetuning can hardly change the appearance in such a short period. The pretraining stage also enhances the subsequent finetuning, in 30 seconds our full strategy produces a more complete result. However, pretraining has a side effect of local overfitting and global degradation. Therefore, our two-stage strategy makes a good balance between both and produces optimal results.

\paragraph{MLP fixing in the pretraining stage.} In \cref{fig-ab_fix}, we validate our design of fixing all MLP parameters in the pretraining stage. The result confirms our analysis that MLP mainly contains global information so it leads to global degeneration when MLP decoders are not fixed.
