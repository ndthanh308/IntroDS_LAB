

% % Figure environment removed


% % Figure environment removed

% % Figure environment removed

% % Figure environment removed

% % Figure environment removed

% % Figure environment removed

% Figure environment removed


% Figure environment removed

% Figure environment removed


\subsection{Implementation Details}


\paragraph{Network.} In order to disentangle shape and color latent information within the hashgrids, we split the single hash table in the NeRF network architecture of Instant-NGP~\cite{mueller2022instant} into two: a density grid $\mathcal{G}^{\sigma}$ and a color grid $\mathcal{G}^c$, with the same settings as the original density grid in the open-source PyTorch implementation torch-ngp~\cite{torch-ngp}. We do this to make it possible to make fine-grained edits of one to one of the color or geometry properties without affecting the other. The rest of the network architecture remains the same, including a sigma MLP $f^\sigma$ and a color MLP $f^c$. For a spatial point $\mathbf{x}$ with view direction $\mathbf{d}$, the network predicts volume density $\sigma$ and color $c$ as follows:
\begin{align}
    \sigma, \mathbf{z} &= f^\sigma(\mathcal{G}^{\sigma}(\mathbf{x})) \\
    c &= f^c(\mathcal{G}^c(\mathbf{x}),\mathbf{z},\mathrm{SH}(\mathbf{d}))
\end{align}
where $\mathbf{z}$ is the intermediate geometry feature, and $\mathrm{SH}$ is the spherical harmonics directional encoder~\cite{mueller2022instant}. The same as Instant-NGP's settings, $f^\sigma$ has 2 layers with hidden channel 64, $f^c$ has 3 layers with hidden channel 64, and $\mathbf{z}$ is a 15-channel feature.

We compare our modified NeRF network with the vanilla architecture in the Lego scene of NeRF Blender Synthetic dataset\cite{mildenhall2020nerf}. We train our network and the vanilla network on the scene for 30,000 iterations. The result is as follows:
\begin{itemize}
    \item Ours: training time 441s, PSNR 35.08dB
    \item Vanilla: training time 408s, PSNR 34.44dB
\end{itemize}
We observe slightly slower runtime and higher quality for our modified architecture, indicating that this modification causes negligible changes.

\paragraph{Training.}
% We use Instant-NGP\fcite{NGP} as our editing framework backbone to achieve real-time editing preview. 
We select Instant-NGP~\cite{mueller2022instant} as the NeRF backbone of our editing framework.
Our implementations are based on the open-source PyTorch implementation torch-ngp~\cite{torch-ngp}. All experiments are run on a single NVIDIA RTX 3090 GPU. Note that we make a slight modification to the original network architecture. Please refer to the supplementary material for details.

During the pretraining stage, we set $\msymbol{weight_pretrain_color}=\msymbol{weight_pretrain_sigma}=1$ and the learning rate is fixed to $0.05$. During the finetuning stage, we set $\msymbol{weight_train_color} = \msymbol{weight_train_depth} = 1$ with an initial learning rate of 0.01. 
% The bit field mask of the editing space is filled so that the editing space can be fully sampled during training. 
Starting from a pretrained NeRF model, we perform 50-100 epochs of local pretraining (for about 0.5-1 seconds) and about 50 epochs of global finetuning (for about 40-60 seconds). The number of epochs and time consumption can be adjusted according to the editing type and the complexity of the scene. Note that we test our performance in the absence of tiny-cuda-nn~\cite{tiny-cuda-nn} which achieves superior speed to our backbone, which indicates that our performance has room for further optimization.
% Note that the training speed is evaluated when tiny-cuda-nn is not enabled.

\paragraph{Datasets.}
We evaluate our editing in the synthetic\Skip{lego, chair, and ship from} NeRF Blender Dataset~\cite{mildenhall2020nerf}, and the real-world captured \Skip{family and truck from}Tanks and Temples~\cite{Knapitsch2017} and \Skip{, and scan83 from} DTU~\cite{jensen2014large} datasets. We follow the official dataset split of the frames for the training and evaluation.


% Figure environment removed

% Figure environment removed

% Figure environment removed

\subsection{Experimental Results}
\label{sec-results}
% \paragraph{Comparisons of rendering quality between teacher and student network.} 


\paragraph{Qualitative NeRF editing results.} 
We provide extensive experimental results in all kinds of editing categories we design, including bounding shape (\cref{fig-bbox,fig-bbox-elf}), brushing (\cref{fig-brush}), anchor (\cref{fig-anchor}), and color (\cref{fig-teaser}). Our method not only achieves a huge performance boost, supporting instant preview at the second level but also produces more visually realistic editing appearances, such as shading effects on the lifted side in \cref{fig-brush} and shadows on the bumped surface in \cref{fig-neumesh}. Besides, results produced by the student network can even outperform the teacher labels, \eg in \cref{fig-bbox-elf} the $F^t$ output contains floating artifacts due to view inconsistency. As analyzed in \cref{sec-train}, the distillation process manages to eliminate this. We also provide an example of object transfer (\cref{fig-bbox-baby}): the bulb in the Lego scene (of Blender dataset) is transferred to the child's head in the family scene of Tanks and Temples dataset.
% \Skip{
% We evaluate our method on all the editing types we design, \ie bounding shape, brushing and anchor, respectively:
% \begin{itemize}
%     \item Bounding shape editing. As shown in \cref{fig-bbox}, we scale the warning light on the top of the Lego model, shorten the chair leg, \zjs{TBD}, and provides plausible results.
%     \item Brushing and color editing. As shown in \cref{fig-brush}, our method edits the scene according to the user's paintings (\ie a cross sign on the chair back, a heart shape on the car logo, and \zjs{TBD}). Note that our brushing method supports simultaneous geometry lifting, as shown in the ``cross'' example. Due to our shading preservation strategy in HSL space, the edited surface can contain realistic visual effects (see the shading effects of the lifted surface).
%     \item Anchor editing. As shown in \cref{fig-anchor}, our method edits the scene according to the anchor points (\ie ship's bow, bulldozer's shovel and \zjs{TBD}) and the stretching direction. The edited geometry has consistent appearance with the anchored area.
% \end{itemize}
% }

% Figure environment removed

% Figure environment removed

\paragraph{Comparisons to baselines.} Existing works have strong restrictions on editing types, which focus on either geometry editing or appearance editing, while ours is capable of doing both simultaneously. Our brushing and anchor tools can create user-guided out-of-proxy geometry structures, which no existing methods support. We make comparisons on color and texture painting supported by NeuMesh~\cite{neumesh} and Liu \etal~\cite{liu2021editing}. 

\cref{fig-neumesh} illustrates two comparisons between our method and NeuMesh~\cite{neumesh} in scribbling and a texture painting task. Our method significantly outperforms NeuMesh, which contains noticeable color bias and artifacts in the results. In contrast, our method even succeeds in rendering the shadow effects caused by geometric bumps.

\cref{fig-neumesh-mic} illustrates the results of the same non-rigid blending applied to the Mic from NeRF Blender\cite{mildenhall2020nerf}. It clearly shows that being mesh-free, We have more details than NeuMesh\cite{neumesh}, unlimited by mesh resolution.

\cref{fig-editnerf} shows an overview of the pixel-wise editing ability of existing NeRF editing methods and ours. Liu \etal~\cite{liu2021editing}'s method does not focus on the pixel-wise editing task and only supports textureless simple objects in their paper. Their method causes an overall color deterioration within the edited object, which is highly unfavorable. This is because their latent code only models the global color feature of the scene instead of fine-grained local features. Our method supports fine-grained local edits due to our local-aware embedding grids.

% \yq{describe the difference}

% \paragraph{Artistic applications (a comic on NeRF).} Based on the four example tools we implemented, we created a comic \textit{Bob the Bulb} (Fig. \ref{fig-comic}) to show the potential applications of our editing method. This might be the first artwork created with NeRF.

% \subsection{Experiments on Bounding Shape Editing}
% \subsection{Experiments on Brush Editing}
% \subsection{Experiments on Anchor Editing}
% \subsection{Experiments on Color Shape Editing}
% \subsection{Real-world Example: NeRF-Rendered Comic}


\subsection{Ablation Studies}
\label{sec-ablation}
\paragraph{Effect of the two-stage training strategy.} To validate the effectiveness of our pretraining and finetuning strategy, we make comparisons between our full strategy (3\textsuperscript{rd} row), finetuning-only (1\textsuperscript{st} row) and pretraining-only (2\textsuperscript{nd} row) in \cref{fig-ab_pre}. Our pretraining can produce a coarse result in only 1 second, while photometric finetuning can hardly change the appearance in such a short period. The pretraining stage also enhances the subsequent finetuning, in 30 seconds our full strategy produces a more complete result. However, pretraining has a side effect of local overfitting and global degradation. Therefore, our two-stage strategy makes a good balance between both and produces optimal results.

\paragraph{MLP fixing in the pretraining stage.} In \cref{fig-ab_fix}, we validate our design of fixing all MLP parameters in the pretraining stage. The result confirms our analysis that MLP mainly contains global information so it leads to global degeneration when MLP decoders are not fixed.

