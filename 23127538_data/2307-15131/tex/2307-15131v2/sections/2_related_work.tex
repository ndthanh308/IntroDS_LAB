
% Figure environment removed


\paragraph{Novel view synthesis.} Given a set of posed image captures of a scene, the task of novel view synthesis is to generate photo-realistic images from arbitrary novel views. Recently, neural network have been introduced into the rendering pipeline and leveraged for multiple representations, such as voxels~\cite{Lombardi2019,sitzmann2019deepvoxels}, point clouds~\cite{Aliev2020,dai2020neural}, multi-plane images (MPIs)~\cite{li2020crowdsampling,mildenhall2019llff,zhou2018stereo}, and implicit representations~\cite{sitzmann2019srns,mildenhall2020nerf}. Typically, Neural radiance field (NeRF)~\cite{mildenhall2020nerf} uses a single MLP to implicitly encode a scene into a volumetric field of density and color, and takes advantage of volume rendering to achieve impressive rendering results with view-dependent effects, which inspires a lot of follow-up works on human~\cite{peng2021neural,weng_humannerf_2022_cvpr}, deformable objects~\cite{park2021nerfies,park2021hypernerf}, pose estimations~\cite{lin2021barf}, autonomous system~\cite{ran2023neurar,zeng2023effic}, surface reconstruction~\cite{yariv2021volume,wang2021neus}, indoor scenes~\cite{Yu2022MonoSDF}, city~\cite{tancik2022blocknerf,xiangli2022bungeenerf}, \etc. NeRF's MLP representation can be enhanced and accelerated by hybrid representations, including voxels~\cite{yu_and_fridovichkeil2021plenoxels,SunSC22}, hashgrids~\cite{mueller2022instant} and tensorial decomposition~\cite{Chen2022ECCV,tang2022compressible}. In this paper, our interactive editing framework is developed based on Instant-NGP~\cite{mueller2022instant}, which achieve real-time rendering speed for NeRF inference and state-of-the-art quality of novel view synthesis.


\paragraph{Neural scene editing.} Scene editing has been a widely researched problem in computer vision and graphics. Early method focus on editing a single static view by inserting~\cite{li2020inverse,zhu2022learning}, relighting~\cite{li2022physically}, composition~\cite{perez2003poisson}, object moving~\cite{OM3D2014,shetty_neurips2018}, \etc. With the development of neural rendering, recent works attempt to perform editing at different levels of the 3D scene, which can be categorized as scene-level, object-level, and pixel-level editing. Scene-level editing methods focus on changing in global appearances of a scene, such as lighting~\cite{guo2020object} and global palette~\cite{kuang2022palettenerf}. Intrinsic decomposition~\cite{zhang2021nerfactor,munkberg2022extracting,hasselgren2022nvdiffrecmc,Ye2022IntrinsicNeRF,zhu2023i2,10.1145/3588432.3591493} disentangles material and lighting field and enables texture or lighting editing. However, scene-level methods are only able to modify global attributes and are unable to apply to specified objects. Object-level editing methods use different strategies to manipulate the implicitly represented object. Object-NeRF~\cite{yang2021objectnerf} exploit per-object latent code to decompose neural radiance field into objects, enabling object moving, removal, or duplicating. Liu \etal~\cite{liu2021editing} design a conditional radiance field model which is partially optimized according to the editing instructions to modify semantic-level color or geometry. NeRF-editing~\cite{Yuan22NeRFEditing} and NeuMesh~\cite{neumesh} introduce a deformable mesh reconstructed by NeRF, as an editing proxy to guide object editings. However, these methods are restricted to object-level rigid transformation or are not generalizable to arbitrary out-of-distribution editing categories. In contrast, pixel-level editing aims to provide fine-grained editing guidance precisely selected by pixels, instead of restricted by object entities. To the best of our knowledge, NeuMesh~\cite{neumesh} is the only existing method that achieves editing at this level. However, it depends on the mesh scaffold, which limits the editing categories, \eg cannot create out-of-mesh geometry structures. In contrast, our editing framework does not require any proxy geometry structures, allowing it to be more direct and extensive.

Besides, optimizing the performance of neural editing method remains an open problem. Existing methods require minutes or even hours of optimization and inference. Our method is the first pixel-level neural editing framework to achieve instant interactive (\ie second-level) performance.

\Skip{
Scene-wise solutions... (intrinsic decomposition, palette,...) Object-wise solutions... 

\zjs{Cannot only introduce NeRF-related methods. Need some more classic methods}
{
\color{magenta}
\subsection{Neural Radiance Fields}

Neural radiance fields is a breakthrough in 3D scene reconstruction and have recently become popular in various 3D vision and graphics tasks. Following the framework of NeRF~\cite{mildenhall2020nerf}, different sampling, positional encoding, and volume rendering networks have significantly improved the performance~\cite{kaizhang2020,barron2021mipnerf}\fcite{more} and speed~\cite{mueller2022instant,SunSC22,yu_and_fridovichkeil2021plenoxels,Chen2022ECCV}\fcite{more} of NeRF training and rendering, flourishing its development and applications. NeRFs are proved to be able to reconstruct complex objects and scenes of different scales, from toys\fcite{perfception dataset}, human face\fcite{conerf}, and body\fcite{nerf-people}, to buildings\fcite{nerfinwild, largescale outdoor}, scans ranging of several kilometers\fcite{meganerf}, or even the whole city roadmap\fcite{blocknerf}.

Owing to the Instant-NGP~\cite{mueller2022instant}'s multiple-level hashgrid acceleration, the rendering speed of NeRF can achieve real-time. Based on this, we develop our real-time interactive editing method.

\subsection{NeRF Semantics}

Semantic methods aim to recognize and separate different objects from a NeRF scene so that the objects can be controlled for different manipulating tasks, \eg object removal and object duplication. \fcite{decomposing 3d} trains codes for every single object in a scene to control them one by one. Some works learn extra features in parallel with RGB features as semantics from sparse labels\fcite{inplace}, 2D semantic labels\fcite{dmnerf}, 3D semantic labels\fcite{panoptip}, 2D semantic methods\fcite{panoctic, slotattention}, or the information from the model itself\fcite{unsupervised mul} to generate full-image semantics.\fcite{laterf, spin} use labels of object and non-object pixels as supervision to train NeRF model with segmentation. Semantics is the first step to understanding 3D NeRF objects and can serve object-level editing tasks. However, pixel-wise editing is still an open challenge.

Related to our teacher-student distillation strategy, Tschernezki \etal\fcite{N3F}, Sosuke \etal\fcite{DDF}, and Rahul \etal\fcite{interactive seg} use a similar distillation process to optimize the NeRF model, while they concentrate on object-wise segmenting and our work is under the pixel-wise free editing topic. 

\subsection{NeRF Editing}

NeRF editing is a heated topic and there do exist some previous NeRF editing methods. However, they are mostly scene-wise or object-wise solutions, leaving the pixel-wise editing blank probably. The scene-wise editing methods focus on global appearances, such as lightings\fcite{object centric, nerf2} and global color\fcite{palette}. The object-wise editing methods use different strategies to manipulate the implicitly represented object, like feature code\fcite{learnedinitialization}, deforming existing model or template\fcite{template nerf, codenerf, fig-nerf, neural articulated, nerfies}, combining NeRF with other frameworks, \eg generative model\fcite{giraff, learning dense}, 2D image processing model\fcite{nerf-in}, and language model\fcite{clip-nerf}. 

More related to our proposed method, Yuan \etal\fcite{nerf-editing} and Yang \etal\fcite{neumesh} blend the rendered mesh output of an existing NeRF model and uses the modified mesh to supervise another NeRF model. Liu \etal \fcite{editing conditional} partially updates model parameters with extra losses to efficiently edit the color or remove an object from an existing model. These methods can partly achieve pixel-wise editing, while our method can be more direct and extensive.
}
}
