\section{Theoretical Analysis}\label{sec:theory}

\subsection{Convergence Analysis}\label{sec:convergence}

In this section, we show that by choosing appropriate step sizes $\eta_\theta^t$ and $\eta_q^t$, TRO yields fast convergence rates for both convex and non-convex loss functions. We first state the assumptions of the theorems. Next, we give the convergence rate for convex loss functions in Theorem 1 and the convergence rate for non-convex loss functions in Theorem 2.

\textbf{Definition 1.} (Lipschitz continuity) A mapping $f: \mathcal{X} \rightarrow \mathbb{R}^{m}$ is $G$-Lipschitz continuous if for any $x, y \in \mathcal{X},\|f(x)-f(y)\| \leq G\|x-y\|$. \\
\textbf{Definition 2.} (Smoothness) A function $f: \mathcal{X} \rightarrow \mathbb{R}$ is $L$-smooth if it is differentiable on $\mathcal{X}$ and the gradient $\nabla f$ is L-Lipschitz continuous, \ie, $\|\nabla f(x)-\nabla f(y)\| \leq L\|x-y\|$ for all $x, y \in \mathcal{X}$. 
\textbf{Assumption 1.} We make the following assumptions throughout the paper: Given $\theta$, the loss function $\ell(f_\theta(x),y)$ is G-Lipschitz continuous and L-smooth with respect to $x$.

\textbf{Convex Loss.} 
The expected number of stochastic gradient computations is utilized to estimate the convergence rate.
To reach a duality gap of $\epsilon$~\citep{nemirovski2009robust}, the optimal rate of convergence for solving the stochastic min-max problems is $\mathcal{O}\left(1 / \epsilon^{2}\right)$ if it is convex-concave. 
The duality gap of the pair $(\tilde{\theta}, \tilde{q})$ is defined as $\max _{q \in \Delta_{m}} \mathcal{R}(\tilde{\theta}, q)-\min _{\theta \in \Theta} \mathcal{R}(\theta, \tilde{q})$. In the case of strong duality, $(\tilde{\theta}, \tilde{q})$ is optimal {\it iif} the duality gap is zero. We show  TRO achieves the optimal $\mathcal{O}\left(1 / \epsilon^{2}\right)$ rate. \\
\textbf{Theorem 1.} Consider the dual problem in Eq.~\ref{eq:dual} when the loss function is convex and Assumption 1 holds.
Let $\Theta$ bounded by $R_\Theta$, $\mathbb{E}\left[\left\|\nabla_{\theta} \mathcal{R}(\theta, q)\right\|_{2}^{2}\right] \leq \hat{G}_{\theta}^{2}$, and  $\mathbb{E}\left[\left\|\nabla_{q} \mathcal{R}(\theta, q)\right\|_{2}^{2}\right] \leq \hat{G}_{q}^{2}$.
With step sizes $\eta_{\theta}=2 R_{\Theta} /\left(\hat{G}_{\theta} \sqrt{T}\right)$ and $\eta_{q}=2 /\left(\hat{G}_{q} \sqrt{T}\right)$, the output of TRO satisfies:
\begin{equation}
\mathbb{E}\left[\max _{q \in \Delta_{m}} \mathcal{R}\left(\theta_{T}, q\right)-\min _{\theta \in \Theta} \mathcal{R}\left(\theta, q_{T}\right)\right] \leq \frac{3 R_{\Theta}\hat{G}_{\theta} + 3 \hat{G}_{q}}{\sqrt{T}}.
\end{equation}
Theorem 1 shows that our method requires $T=\mathcal{O}\left(1 / \epsilon^{2}\right)$ iterations to reach a duality gap within $\epsilon$. 

To derive the convergence rate for non-convex functions., we define $\epsilon$-stationary points as follows:\\
\textbf{Definition 3.} ($\epsilon$-stationary point) For a differentiable function $f: \mathcal{X} \rightarrow \mathbb{R}$, a point $x \in \mathcal{X}$ is said to be first-order $\epsilon$-stationary if $\|\nabla f(x)\| \leq \epsilon$.

\textbf{Nonconvex Loss.} The loss function $\ell(f_\theta(x),y)$ can be nonconvex and as a result, $\mathcal{R}(\theta, q)$ is nonconvex in $\theta$. 
Following \cite{collins2020task}, we define $(\tilde{\theta}, \tilde{q})$ is an $(\epsilon, \delta)$-stationary point of $\mathcal{R}$ if:
$\left\|\nabla_{\theta} \mathcal{R}(\tilde{\theta}, \tilde{q})\right\|_{2} \leq \epsilon$ and $\mathcal{R}(\tilde{\theta}, \tilde{q}) \geq \max _{q \in \Delta_{m}} \mathcal{R}(\tilde{\theta}, q)-\delta$, where $\epsilon, \delta>0$.

\textbf{Theorem 2.} If Assumption 1 holds and the loss function is bounded by $B$ and is M-smooth,
the output of Alg.~\ref{alg:overrall} satisfies:
\begin{equation}
\begin{aligned}
&\mathbb{E}\left[\left\|\nabla_{\theta} \mathcal{R}\left(\theta_{T}, q_{T}\right)\right\|_{2}^{2}\right] \leq \frac{\mathcal{R}\left(\theta^{0}, q^{0}\right)+B}{T\eta_{\theta}}+\frac{2 \eta_{q} \sqrt{n} B \hat{G}_{q}}{\eta_\theta}+\frac{\eta_{\theta} M \hat{G}_{\theta}^{2}}{2}, \\
&\mathbb{E}\left[\mathcal{R}\left(\theta_{T}, q_{T}\right)\right] \geq \max _{q \in \Delta_{m}}\left\{\mathbb{E}\left[\mathcal{R}\left(\theta_{T}, q\right)\right]\right\}-\frac{1}{\eta_{q} T}-\frac{\eta_{q} \hat{G}_{q}^{2}}{2}.
\end{aligned}
\end{equation}
Theorem 2 shows that our method converges in expectation to an $(\epsilon, \delta)$-stationary point of $\mathcal{R}$ in $\mathcal{O}(1 / \epsilon^{4})$ stochastic gradient evaluations.

\subsection{Generalization Bounds}\label{sec:bounds}

In this section, we provide learning guarantees for TRO.
Compared to DRO, TRO achieves a lower upper bound on the generalization risks over unseen distributions with the topological constraint.

Let $\mathcal{H}$ denote the family of losses associated with a hypothesis set $\mathcal{F}: \mathcal{H}=\{(x, y) \mapsto \ell(f(x), y): f \in \mathcal{F}\}$, and $\mathbf{n}=\left(n_1, \ldots, n_m\right)$ denote the vector of sample sizes for all training groups.
Following~\cite{mohri2019agnostic}, we define weighted Rademacher complexity for any $\mathcal{F}$ as:
$$
\mathfrak{R}_{\mathbf{n}}(\mathcal{H}, q)=\underset{\substack{S_{e} \sim P_{e}}}{\mathbb{E}}\underset{\boldsymbol{\sigma}}{\mathbb{E}}\left[\sup _{f \in \mathcal{F}} \sum_{e=1}^{m} \frac{q_{e}}{n_{e}} \sum_{i=1}^{n_{e}} \sigma_{e, i} \ell\left(f\left(x_{e, i}\right), y_{e, i}\right)\right],
$$
where $e$ denotes group index, $S_{e}$ a sample of size $n_{e}$, $P_{e}$ the distribution of group $e$, and $\boldsymbol{\sigma}=\left(\sigma_{e, i}\right)_{e \in[m], i \in\left[n_e\right]}$ a collection of Rademacher variables.
The minimax weighted Rademacher complexity for a subset $\Lambda \subseteq \Delta_{m}$ is defined by
$\mathfrak{R}_{\mathbf{n}}(\mathcal{H}, \Lambda) =\max _{q \in \Lambda} \mathfrak{R}_{n}(\mathcal{H}, q)$ where $n=\sum_{e=1}^m n_e$.
Let $P_{\Lambda}$ be the distribution over the mixture of training groups and $\hat{P}_{\Lambda}$ be its empirical counterpart. Let the distribution of some test group be $P$. The learning guarantee for $P$ is shown in Theorem 3.

\textbf{Theorem 3.} Assume the loss function $\ell$ is bounded by $B$. For any $\epsilon \geq 0$ and $\delta>0$, with probability at least $1-\delta$, the following inequality holds for all $f \in \mathcal{F}$ :
$$
\mathcal{R}_{P}(f) \leq \mathcal{R}_{\hat{P}_{\Lambda}}(f, q) + 2 \mathfrak{R}_{\mathbf{n}}(\mathcal{H}, \Lambda)+B \mathcal{D}\left( P \| P_{\Lambda}\right)+ B \sqrt{\frac{1}{2 m} \log \frac{\left|\Lambda\right|}{\delta}}.
$$
% where $d(\cdot)$ is the distance metric.
The upper bound of the generalization risk on $P$ is mainly determined by its distance to $P_{\Lambda}$: $\mathcal{D}\left(P\| P_{\Lambda}\right)$.
With the topological prior, risks on $P$ can be tightly bounded by minimizing $\mathcal{D}\left( P\| P_{\Lambda}\right)$, as long as $P$ falls into the convex hull of training groups.
We empirically verify the effectiveness of the topological prior in minimizing the generalization risks over unseen distributions (see Sec.~\ref{sec:exp}).