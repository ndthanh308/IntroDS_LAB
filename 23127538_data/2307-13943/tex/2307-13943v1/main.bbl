\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arjovsky et~al.(2019)Arjovsky, Bottou, Gulrajani, and
  Lopez-Paz]{arjovsky2019invariant}
Martin Arjovsky, L{\'e}on Bottou, Ishaan Gulrajani, and David Lopez-Paz.
\newblock Invariant risk minimization.
\newblock \emph{arXiv preprint arXiv:1907.02893}, 2019.

\bibitem[Beery et~al.(2018)Beery, Van~Horn, and Perona]{beery2018recognition}
Sara Beery, Grant Van~Horn, and Pietro Perona.
\newblock Recognition in terra incognita.
\newblock In \emph{Proceedings of the European conference on computer vision
  (ECCV)}, pp.\  456--473, 2018.

\bibitem[Bonafilia et~al.(2020)Bonafilia, Tellman, Anderson, and
  Issenberg]{bonafilia2020sen1floods11}
Derrick Bonafilia, Beth Tellman, Tyler Anderson, and Erica Issenberg.
\newblock Sen1floods11: A georeferenced dataset to train and test deep learning
  flood algorithms for sentinel-1.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition Workshops}, pp.\  210--211, 2020.

\bibitem[Boyd et~al.(2004)Boyd, Boyd, and Vandenberghe]{boyd2004convex}
Stephen Boyd, Stephen~P Boyd, and Lieven Vandenberghe.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Coifman \& Lafon(2006)Coifman and Lafon]{coifman2006diffusion}
Ronald~R Coifman and St{\'e}phane Lafon.
\newblock Diffusion maps.
\newblock \emph{Applied and computational harmonic analysis}, 21\penalty0
  (1):\penalty0 5--30, 2006.

\bibitem[Collins et~al.(2020)Collins, Mokhtari, and
  Shakkottai]{collins2020task}
Liam Collins, Aryan Mokhtari, and Sanjay Shakkottai.
\newblock Task-robust model-agnostic meta-learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 18860--18871, 2020.

\bibitem[Creager et~al.(2021)Creager, Jacobsen, and
  Zemel]{creager2021environment}
Elliot Creager, J{\"o}rn-Henrik Jacobsen, and Richard Zemel.
\newblock Environment inference for invariant learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2189--2200. PMLR, 2021.

\bibitem[Delage \& Ye(2010)Delage and Ye]{delage2010distributionally}
Erick Delage and Yinyu Ye.
\newblock Distributionally robust optimization under moment uncertainty with
  application to data-driven problems.
\newblock \emph{Operations research}, 58\penalty0 (3):\penalty0 595--612, 2010.

\bibitem[Duchi \& Namkoong(2021)Duchi and Namkoong]{duchi2021learning}
John~C Duchi and Hongseok Namkoong.
\newblock Learning models with uniform performance via distributionally robust
  optimization.
\newblock \emph{The Annals of Statistics}, 49\penalty0 (3):\penalty0
  1378--1406, 2021.

\bibitem[Fang et~al.(2013)Fang, Xu, and Rockmore]{fang2013unbiased}
Chen Fang, Ye~Xu, and Daniel~N Rockmore.
\newblock Unbiased metric learning: On the utilization of multiple datasets and
  web images for softening bias.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  1657--1664, 2013.

\bibitem[Freeman(1977)]{freeman1977set}
Linton~C Freeman.
\newblock A set of measures of centrality based on betweenness.
\newblock \emph{Sociometry}, pp.\  35--41, 1977.

\bibitem[Frogner et~al.(2021)Frogner, Claici, Chien, and
  Solomon]{frogner2021incorporating}
Charlie Frogner, Sebastian Claici, Edward Chien, and Justin Solomon.
\newblock Incorporating unlabeled data into distributionally robust learning.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (56):\penalty0 1--46, 2021.

\bibitem[Gulrajani \& Lopez-Paz(2021)Gulrajani and
  Lopez-Paz]{gulrajani2020search}
Ishaan Gulrajani and David Lopez-Paz.
\newblock In search of lost domain generalization.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Hu et~al.(2018)Hu, Niu, Sato, and Sugiyama]{hu2018does}
Weihua Hu, Gang Niu, Issei Sato, and Masashi Sugiyama.
\newblock Does distributionally robust supervised learning give robust
  classifiers?
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2029--2037. PMLR, 2018.

\bibitem[Koh et~al.(2021)Koh, Sagawa, Marklund, Xie, Zhang, Balsubramani, Hu,
  Yasunaga, Phillips, Gao, et~al.]{koh2021wilds}
Pang~Wei Koh, Shiori Sagawa, Henrik Marklund, Sang~Michael Xie, Marvin Zhang,
  Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard~Lanas Phillips,
  Irena Gao, et~al.
\newblock Wilds: A benchmark of in-the-wild distribution shifts.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5637--5664. PMLR, 2021.

\bibitem[Koyama \& Yamaguchi(2020)Koyama and Yamaguchi]{koyama2020out}
Masanori Koyama and Shoichiro Yamaguchi.
\newblock Out-of-distribution generalization with maximal invariant predictor.
\newblock 2020.

\bibitem[Krueger et~al.(2021)Krueger, Caballero, Jacobsen, Zhang, Binas, Zhang,
  Le~Priol, and Courville]{krueger2021out}
David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan
  Binas, Dinghuai Zhang, Remi Le~Priol, and Aaron Courville.
\newblock Out-of-distribution generalization via risk extrapolation (rex).
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5815--5826. PMLR, 2021.

\bibitem[Leeb \& Coifman(2016)Leeb and Coifman]{leeb2016holder}
William Leeb and Ronald Coifman.
\newblock H{\"o}lder--lipschitz norms and their duals on spaces with
  semigroups, with applications to earth mover’s distance.
\newblock \emph{Journal of Fourier Analysis and Applications}, 22\penalty0
  (4):\penalty0 910--953, 2016.

\bibitem[Levy et~al.(2020)Levy, Carmon, Duchi, and Sidford]{levy2020large}
Daniel Levy, Yair Carmon, John~C Duchi, and Aaron Sidford.
\newblock Large-scale methods for distributionally robust optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 8847--8860, 2020.

\bibitem[Li et~al.(2017)]{li2017deeper}
Da~Li et~al.
\newblock {Deeper, Broader and Artier Domain Generalization}.
\newblock In \emph{ICCV}, 2017.

\bibitem[Li et~al.(2021)Li, Gao, and Peng]{li2021deep}
Tang Li, Jing Gao, and Xi~Peng.
\newblock Deep learning for spatiotemporal modeling of urbanization.
\newblock \emph{Advances in Neural Information Processing Systems Workshops},
  2021.

\bibitem[Li et~al.(2023)Li, Qiao, Ma, and Peng]{tang2023dre}
Tang Li, Fengchun Qiao, Mengmeng Ma, and Xi~Peng.
\newblock Are data-driven explanations robust against out-of-distribution data?
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, 2023.

\bibitem[Liu et~al.(2021)Liu, Hu, Cui, Li, and Shen]{liu2021heterogeneous}
Jiashuo Liu, Zheyuan Hu, Peng Cui, Bo~Li, and Zheyan Shen.
\newblock Heterogeneous risk minimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6804--6814. PMLR, 2021.

\bibitem[Mohri et~al.(2019)Mohri, Sivek, and Suresh]{mohri2019agnostic}
Mehryar Mohri, Gary Sivek, and Ananda~Theertha Suresh.
\newblock Agnostic federated learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4615--4625. PMLR, 2019.

\bibitem[Namkoong \& Duchi(2016)Namkoong and Duchi]{namkoong2016stochastic}
Hongseok Namkoong and John~C Duchi.
\newblock Stochastic gradient methods for distributionally robust optimization
  with f-divergences.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and
  Shapiro]{nemirovski2009robust}
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM Journal on optimization}, 19\penalty0 (4):\penalty0
  1574--1609, 2009.

\bibitem[Newman(2005)]{newman2005measure}
Mark~EJ Newman.
\newblock A measure of betweenness centrality based on random walks.
\newblock \emph{Social networks}, 27\penalty0 (1):\penalty0 39--54, 2005.

\bibitem[Peng et~al.(2022)Peng, Qiao, and Zhao]{peng2022out}
Xi~Peng, Fengchun Qiao, and Long Zhao.
\newblock Out-of-domain generalization from a single source: An uncertainty
  quantification approach.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2022.

\bibitem[Pezeshki et~al.(2021)Pezeshki, Kaba, Bengio, Courville, Precup, and
  Lajoie]{pezeshki2021gradient}
Mohammad Pezeshki, Oumar Kaba, Yoshua Bengio, Aaron~C Courville, Doina Precup,
  and Guillaume Lajoie.
\newblock Gradient starvation: A learning proclivity in neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Qian et~al.(2019)Qian, Zhu, Tang, Jin, Sun, and Li]{qian2019robust}
Qi~Qian, Shenghuo Zhu, Jiasheng Tang, Rong Jin, Baigui Sun, and Hao Li.
\newblock Robust optimization over multiple domains.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pp.\  4739--4746, 2019.

\bibitem[Qiao \& Peng(2021)Qiao and Peng]{qiao2021uncertainty}
Fengchun Qiao and Xi~Peng.
\newblock Uncertainty-guided model generalization to unseen domains.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pp.\  6790--6800, June 2021.

\bibitem[Qiao et~al.(2020)Qiao, Zhao, and Peng]{qiao2020learning}
Fengchun Qiao, Long Zhao, and Xi~Peng.
\newblock Learning to learn single domain generalization.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  12556--12565, 2020.

\bibitem[Robey et~al.(2021)Robey, Pappas, and Hassani]{robey2021model}
Alexander Robey, George~J Pappas, and Hamed Hassani.
\newblock Model-based domain generalization.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 20210--20229, 2021.

\bibitem[Rosenfeld et~al.(2021)Rosenfeld, Ravikumar, and
  Risteski]{rosenfeld2021risks}
Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski.
\newblock The risks of invariant risk minimization.
\newblock In \emph{International Conference on Learning Representations},
  volume~9, 2021.

\bibitem[Sagawa et~al.(2019)Sagawa, Koh, Hashimoto, and
  Liang]{sagawa2019distributionally}
Shiori Sagawa, Pang~Wei Koh, Tatsunori~B Hashimoto, and Percy Liang.
\newblock Distributionally robust neural networks for group shifts: On the
  importance of regularization for worst-case generalization.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Shafieezadeh~Abadeh et~al.(2018)Shafieezadeh~Abadeh, Nguyen, Kuhn, and
  Mohajerin~Esfahani]{shafieezadeh2018wasserstein}
Soroosh Shafieezadeh~Abadeh, Viet~Anh Nguyen, Daniel Kuhn, and Peyman~M
  Mohajerin~Esfahani.
\newblock Wasserstein distributionally robust kalman filtering.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Shalev-Shwartz \& Wexler(2016)Shalev-Shwartz and
  Wexler]{shalev2016minimizing}
Shai Shalev-Shwartz and Yonatan Wexler.
\newblock Minimizing the maximal loss: How and why.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  793--801. PMLR, 2016.

\bibitem[Staib \& Jegelka(2019)Staib and Jegelka]{staib2019distributionally}
Matthew Staib and Stefanie Jegelka.
\newblock Distributionally robust optimization and generalization in kernel
  methods.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Tian et~al.(2019)Tian, Zhao, Peng, and Metaxas]{tian2019rethinking}
Yu~Tian, Long Zhao, Xi~Peng, and Dimitris Metaxas.
\newblock Rethinking kernel methods for node representation learning on graphs.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Tong et~al.(2021)Tong, Huguet, Natik, MacDonald, Kuchroo, Coifman,
  Wolf, and Krishnaswamy]{tong2021diffusion}
Alexander~Y Tong, Guillaume Huguet, Amine Natik, Kincaid MacDonald, Manik
  Kuchroo, Ronald Coifman, Guy Wolf, and Smita Krishnaswamy.
\newblock Diffusion earth mover’s distance and distribution embeddings.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10336--10346. PMLR, 2021.

\bibitem[Vapnik(1998)]{vapnik1998statistical}
Vladimir Vapnik.
\newblock Statistical learning theory, 1998.

\bibitem[Vose et~al.(2014)Vose, Applequist, Squires, Durre, Menne, Williams~Jr,
  Fenimore, Gleason, and Arndt]{vose2014gridded}
R~Vose, S~Applequist, M~Squires, I~Durre, MJ~Menne, CN~Williams~Jr, C~Fenimore,
  K~Gleason, and D~Arndt.
\newblock Gridded 5km ghcn-daily temperature and precipitation dataset
  (nclimgrid) version 1.
\newblock \emph{Information, NNCfE, editor. Maximum Temperature, Minimum
  Temperature, Average Temperature, and Precipitation}, 2014.

\bibitem[Xu et~al.(2022)Xu, Lee, Wang, Wang, et~al.]{xu2022graph}
Zihao Xu, Guang-He Lee, Yuyang Wang, Hao Wang, et~al.
\newblock Graph-relational domain adaptation.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Zhao et~al.(2020)Zhao, Liu, Peng, and Metaxas]{zhao2020maximum}
Long Zhao, Ting Liu, Xi~Peng, and Dimitris Metaxas.
\newblock Maximum-entropy adversarial data augmentation for improved
  generalization and robustness.
\newblock In \emph{Annual Conference on Neural Information Processing Systems},
  pp.\  14435--14447, 2020.

\end{thebibliography}
