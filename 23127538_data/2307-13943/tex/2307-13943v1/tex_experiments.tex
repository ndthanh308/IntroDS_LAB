\section{Experiments}\label{sec:exp}

We evaluate TRO in a wide range of tasks including classification, regression, and semantic segmentation.
We compare TRO with SOTA baselines on OOD generalization and conduct  ablation study on the key components of TRO.
% Since model selection in OOD generalization has been underscored in recent works~\citep{gulrajani2020search}, we specify the validation set for each dataset.
Following \cite{gulrajani2020search}, we perform model selection based on a validation set constructed from training groups only.
We provide implementation details in Appendix~\ref{sec:details} and results on \textit{DomainBed}~\citep{gulrajani2020search} in Appendix~\ref{sec:addition}.
\begin{table}[t]
	\centering
	\begin{scriptsize}
	\caption{Accuracy (\%) on {\it DG-15} and {\it DG-60}. TRO sets the new SOTA on both {\it DG-15} and {\it DG-60}.}\label{tab:dg_results}
	\resizebox{0.7\linewidth}{!}{
	\begin{tabular}{@{}l|ccccc|cc@{}}
		\toprule
		   & ERM & IRM & REx & SD &  DRO & TRO (physical) & TRO (data)\\
		\hline
		{\it DG-15} & 58.00 &57.87  & 57.22  & 57.56 &43.22 &\underline{67.56} & \textbf{67.89}\\
		{\it DG-60} & 76.02 &76.61 & 86.89 & 81.04 &79.59 &\underline{89.19} & \textbf{90.72}\\
		\bottomrule
	\end{tabular}}
	\end{scriptsize}
\end{table}

% Figure environment removed

% Figure environment removed

{\bf Baselines}. We compare TRO with the following methods: (1) Empirical Risk Minimization (ERM)~\citep{vapnik1998statistical}; (2) Group distributionally robust optimization (DRO)~\citep{sagawa2019distributionally}; (3) Invariant Risk Minimization (IRM)~\citep{arjovsky2019invariant}; (4) Risk Extrapolation (REx)~\citep{krueger2021out};
(5) Spectral Decoupling (SD)~\citep{pezeshki2021gradient}.
% We use the implementation of DomainBed\footnote{\url{https://github.com/facebookresearch/DomainBed}}.
% We show more comparison results in Appendix~\ref{sec:addition}.

\subsection{Classification}\label{sec:classification}

{\bf Datasets}.
{\it DG-15}~\citep{xu2022graph} is a synthetic binary classification dataset with 15 groups. Each group contains 100 data points. In this dataset, adjacent groups have similar decision boundaries. Following \cite{xu2022graph}, we use six connected groups as the training groups, and use others as test groups. Note that, different from \cite{xu2022graph} which focuses on domain adaptation, the data of test groups are unseen in OOD generalization. {\it DG-60}~\citep{xu2022graph} is another synthetic dataset generated using the same procedure as {\it DG-15}, except that it contains 60 groups, with 6,000 data points in total. We randomly select six groups as the training groups and use others as test groups.
Visualization of {\it DG-15} and {\it DG-60} are shown in Fig.~\ref{fig:dg} (a) and (b), respectively.

\begin{comment}
\begin{wrapfigure}{r}{0.48\textwidth}
	  \centering
	  \vspace{5pt}
        \captionof{table}{Group weights $\mathbf{q}$ of DRO and TRO on {\it DG-15}. DRO assign the highest weight to group ``1'' which is the most distant group to test groups, while TRO focuses on the influential group ``5'' which is truly connected to test groups.}\label{tab:dg15_weights}
		\resizebox{1.0\linewidth}{!}{
		\begin{tabular}{@{}lcccccc@{}}
			\toprule
			Group     & 1 & 2 & 3 & 4 & 5 & 6  \\
			\hline
% 			$\mathbf{p}$ (Prior)  & 0.0  &0.26  &0.08  &0.17  &0.21  & \textbf{0.27} \\
		    DRO   & \textbf{0.3}  &0.14  &0.14  &0.15  &0.13  &0.13\\
			TRO   & 0.0  &0.0  &0.0  &0.19  & \textbf{0.81}  &0.0 \\
			\bottomrule
	    \end{tabular}}
\end{wrapfigure}\leavevmode
\end{comment}



\begin{table}[t]
\centering
\begin{scriptsize}
  \caption{Mean Squared Error (MSE) for both tasks E (24) $\rightarrow$ W (24) and N (24) $\rightarrow$ S (24) on \emph{{\it TPT-48}}.
%   We report the average MSE of all test groups as well as more detailed average MSE of Level-1, Level-2, Level-3 test groups, respectively. 
%   TRO outperforms other baselines in all hops of test groups. 
%   Although the data-driven topology can only uncover the topology of training distributions, 
  TRO (data-driven topology) consistently outperforms TRO (physical-based topology) in both tasks, indicating the data-driven topology captures the distributional relation more accurately.}
  \label{tab:tpt}
  \centering
  \resizebox{0.9\linewidth}{!}{
  \begin{tabular}{cl|cccccc|c} \toprule

      Task & Group  & ERM & IRM & REx & SD &  DRO & TRO (physical) & TRO (data) \\  \midrule
     \multirow{4}{*}{E (24)$\rightarrow$W (24)} 
     & Average of Hop-1 groups  & 1.693     & 1.699      & 1.577    & 1.701  & 1.678  & \underline{1.445}  &  \textbf{1.435}  \\
     & Average of Hop-2 groups  & 1.800     & 1.811      & 1.702    & 1.806  & 1.762 & \underline{1.576}   &  \textbf{1.569}  \\
     & Average of Hop-3 groups  & 1.672     & 1.679      & 1.584    & 1.674  & 1.628  & \underline{1.400}  & \textbf{1.392}    \\
     \noalign{\smallskip}\cline{2-9}\noalign{\smallskip}
    % \midrule
     & Average of All test groups  & 1.716  & 1.724    & 1.616  & 1.722 & 1.684  & \underline{1.466} &{\bf 1.458}   \\ 
     \midrule\midrule
    %  \noalign{\smallskip}\hline\hline\noalign{\smallskip}
    
     \multirow{4}{*}{N (24)$\rightarrow$S (24)} 
     & Average of Hop-1 groups  & 1.084      & 1.133      & \textbf{0.487}  & 1.169  & 0.931  & 0.889   &  \underline{0.855} \\
     & Average of Hop-2 groups  & 1.265      & 1.312      & \textbf{0.944} & 1.354  & 1.170  & 0.991  &  \underline{0.950}   \\
     & Average of Hop-3 groups  & 1.975      & 2.021      & 2.266  & 2.091  & 2.027  & \underline{1.678}  &  \textbf{1.604} \\ %\midrule 
     \noalign{\smallskip}\cline{2-9}\noalign{\smallskip}
     & Average of All test groups  & 1.426   &  1.474    & 1.194  & 1.523  & 1.356  & \underline{1.177} &   {\bf 1.129}  \\   
     \bottomrule
  \end{tabular}}
\end{scriptsize}
% \vskip -0.3cm
\end{table}

% Figure environment removed

% Figure environment removed

{\bf Results}. 
The results of {\it DG-15} and {\it DG-60} are summarized in Tab.~\ref{tab:dg_results}.
In both datasets, our method yields the highest accuracy. For {\it DG-15}, we show the detailed results of all groups in Fig.~\ref{fig:dg15_results}. 
% As seen, all methods yield high performance on groups near the training, \eg, group 7-8. Our method achieves better performance on some groups even far away from the training, {\it e.g.}, group 9-12. We notice that all methods achieve 0\% accuracy on groups 14-15, possibly due to the severe concept shift (the shift of $P(Y|X)$). 
% (data with label 0 of group 14-15 are closer to data with label 1 of the training, and vice versa.)
We visualize the decision boundary of {\it DG-15} and {\it DG-60} in Appendix~\ref{sec:addition}.
% For DG-60, we visualize the decision boundary in Fig.~\ref{fig:dg60_results}. As seen, compared to other baselines, the decision boundary of our method is closer to the ground truth, indicating the strong generalization capability.
% Moreover,


\textbf{Ablations study}.
{\it TRO significantly improves the generalization performance by discovering influential groups.}
To investigate the reason why TRO outperforms DRO, we show  group weights $\mathbf{q}$ of DRO and TRO on {\it DG-15} in Fig.~\ref{fig:dg15_weights}. DRO assigns the highest weight to group ``\textit{1}'' which is the furthest group to test groups. 
Instead, TRO prioritizes influential groups ``\textit{2}'', ``\textit{5}'', and ``\textit{6}'' which are truly connected to the test ones, yielding improved performance on unseen distributions.



\subsection{Regression}\label{sec:regression}

{\bf Datasets}. {\it TPT-48}~\citep{vose2014gridded} contains the monthly average temperature for the 48 contiguous states in the US from 2008 to 2019. 
We focus on the regression task to predict the next 6 months' temperature based on the previous first 6 months' temperature. We consider two generalization tasks:
(1) E(24) $\rightarrow$ W(24): we use the 24 eastern states as training groups and the  24 western states as test groups;
(2) N(24) $\rightarrow$ S(24): we use the 24 northern states as training groups and the 24 southern states as test groups. Test groups one hop away from the closest training group are defined as Hop-1 test groups, those two hops away are Hop-2 test groups, and the remaining groups are Hop-3 test groups. 
The visualization of N(24) $\rightarrow$ S(24) on {\it TPT-48} is shown in Fig.~\ref{fig:map_centrality} (left).


{\bf Results}. 
We show the results of \textit{TPT-48} in Tab.~\ref{tab:tpt}. 
% We can see that IRM is even worse than ERM in both tasks. 
% REx achieves consistent improvements on both tasks. 
% indicating it is strong generalization capability on unseen distributions
TRO yields the lowest average MSE on both tasks. We also report the average MSE of Hop-1, Hop-2, and Hop-3 test groups for both tasks.
Although REx yields the lowest error on Hop-1 and Hop-2 groups in N (24) $\rightarrow$ S (24), it yields the highest prediction error on Hop-3 groups. The results indicate that REx may yield compromised performance under large distributional drifts.
%  Explain the limitation of REx
TRO yields the best performance on Hop-3 groups, indicating its strong generalization capability under large distributional drifts.
% These results show TRO consistently achieves the lowest MSE across all the hops of test groups.

\textbf{Ablations study}.
(1) {\it Data-driven topology yields better performance than physical-based topology.}
We show group centrality of both physical and data topology on the task of North $\rightarrow$ South in Fig.~\ref{fig:map_centrality}. 
``\textit{PA}'' is identified by TRO as the \textit{influential} group in physical-based topology; ``\textit{NY}'', ``\textit{PA}'', and ``\textit{MA}'' are identified by TRO as \textit{influential} groups in data-driven topology.
The results prove that the influential groups in data topology are more effective in minimizing the generalization error.\\
% \vspace{-20pt}
\begin{wrapfigure}{r}{0.48\textwidth}
	  \centering
	  \vspace{-10pt}
        \captionof{table}{MSE on {\it TPT-48}. Ignoring either the \textit{worst-case} (IW-ERM) or \textit{influential} (DRO) groups would yield compromised performance.}\label{tab:mse48}
		\resizebox{1.0\linewidth}{!}{
		\begin{tabular}{@{}l|cccc@{}}
			\toprule
			     & Hop-1 Avg.  &Hop-2 Avg.  &Hop-3 Avg.  &Avg.\\
			\hline
			ERM  & 1.084  &1.265  &\underline{1.975}  &1.426 \\
			IW-ERM  & 1.320  & 1.604  &2.635  &1.829 \\
			DRO & \underline{0.931}  & \underline{1.170}  & 2.027  &\underline{1.356} \\
			\midrule
			TRO  & \textbf{0.855} &\textbf{0.950}  &\textbf{1.604}  &\textbf{1.129}  \\
			\bottomrule
	    \end{tabular}}
	     \vspace{-10pt}
\end{wrapfigure}\leavevmode
(2) {\it Strong OOD resilience of TRO comes from the synergy of the worst-case and influential groups.}
To investigate which components contribute to the superior performance of TRO.
We build a simple baseline based on ERM: we directly use the group importance acquired from the topology to weight training groups and the weights are fixed during the training. 
We name this baseline as \textit{importance weighted ERM} (IW-ERM). We show the results of ``N(24)$\rightarrow$S(24)'' on {\it TPT-48} in Tab.~\ref{tab:mse48}. 
The results of IW-ERM are inferior to ERM and DRO, possibly because IW-ERM merely considers influential groups. 
We further show the group importance of DRO and TRO in Fig.~\ref{fig:map_centrality2}.
TRO significantly reduces the generalization risks by not only prioritizing the worst-case groups but also the influential ones.

\subsection{Semantic Segmentation}\label{sec:seg}

{\bf Datasets}. {\it Sen1Floods11}~\citep{bonafilia2020sen1floods11} is a public dataset for global flood mapping.
The dataset provides global coverage of 4,831 chips of 512 x 512 10m satellite images across 11 distinct flood events, covering 120,406 $\text{km}^2$. Each image is associated with its pixel-wise label. Locations of the 11 flood events are shown in Fig.~\ref{fig:flood} (left).
Flood events vary in boundary conditions, terrain, and other latent factors, posing significant OOD challenges to existing models in terms of reliability and explainability~\citep{li2021deep,tang2023dre}.
Following \cite{bonafilia2020sen1floods11},  event ``\textit{BOL}'' is held out for testing, and data of other events are split into training and validation sets with a random 80-20 split.
% We evaluate the performance in terms of Intersection over Union (IoU).

% Figure environment removed


% Figure environment removed

{\bf Results}. 
We show the results of \textit{Sen1Floods11} in Tab.~\ref{tab:flood}. 
ERM achieves the highest IoU on the validation set while TRO achieves the highest IoU on the test set.
The results prove that TRO yields better performance than other baselines on unseen flood events.

\textbf{Ablations study}. (1) {\it Data-driven distributional topology is consistent with domain knowledge.}
We visualize the distributional topology as well as group centrality in Fig.~\ref{fig:flood} (right).
The learned distributional topology is consistent with domain knowledge, enhancing the explainability of TRO.
\begin{comment}
We observe that ``IND'' and ``NGA'' are identified as the most influential events. Both ``IND'' and ``NGA'' are aroused by heavy rainfall, the most prevalent disaster that cause floods.
We empirically found that the least influential events are usually caused by edge cases. For example, ``GHA'' was caused by heavy rains and waters spilling from a dam in neighboring Burkina Faso, and ``KHM'' was caused by a dam that collapsed in southern Laos.
The learned distributional topology is consistent with human knowledge, indicating the strong explainability of TRO.
\end{comment}
(2) {\it Ablation study on $\lambda$}. 
We report IoU under different $\lambda$ on {\it Sen1Floods11} in Fig.~\ref{fig:param}.
IoU remains stable for a wide range of $\lambda$, and $\lambda=0.01$ yields the best performance.




% #####################################################################################################################################
\begin{comment}
% Figure environment removed
\end{comment}

\begin{comment}
% Figure environment removed
\end{comment}

\begin{comment}
% Figure environment removed

% Figure environment removed


\begin{minipage}[t!]{\textwidth}
	%\vspace{0.08in}
	\begin{scriptsize}
	\begin{minipage}[t!]{0.48\textwidth}
        \centering
        \captionof{table}{Mixture weights on {\it DG-15}.}\label{tab:mix15}
		\resizebox{1.0\linewidth}{!}{
		\begin{tabular}{@{}lcccccc@{}}
			\toprule
			     & 1 & 2 & 3 & 4 & 5 & 6  \\
			\hline
			Prior    & 0.0  &0.26  &0.08  &0.17  &0.21  &0.27 \\
			DRO \citep{sagawa2019distributionally}   & 0.3  &0.14  &0.14  &0.15  &0.13  &0.13\\
			Ours   & 0.0  &0.0  &0.0  &0.19  &0.81  &0.0 \\
			\bottomrule
	    \end{tabular}}
	 \end{minipage}
	\hspace{0.1in}
	\begin{minipage}[t!]{0.48\textwidth}
	     \centering
        \captionof{table}{Mixture weights on {\it DG-60}.}\label{tab:mix60}
		\resizebox{1.0\linewidth}{!}{
		\begin{tabular}{@{}lcccccc@{}}
			\toprule
			     & 1  &13  &18  &26  &48  &59 \\
			\hline
			Prior   & 0.23  &0.18  &0.06  &0.16  &0.22  &0.15\\
			DRO \citep{sagawa2019distributionally}   & 0.0  &0.0  &0.02  &0.0  &0.0  &0.98\\
			Ours  & 0.23  &0.01  &0.33  &0.0  &0.35  &0.08 \\
			\bottomrule
	    \end{tabular}}
    \end{minipage}
    \end{scriptsize}
\end{minipage}

\end{comment}

\begin{comment}
% Figure environment removed

% Figure environment removed
\end{comment}

\begin{comment}
\begin{table}[!thp]
\centering
\begin{scriptsize}
  \caption{Segmentation results on {\it Sen1Floods11}. TRO achieves the highest IoU on test set while DRO achieves the highest IoU on validation set.}
  \label{tab:flood}
  \centering
  \resizebox{0.8\linewidth}{!}{
  \begin{tabular}{clcccccc} \toprule
       & Metric  & ERM & IRM & REx & SD &  DRO & TRO \\  \midrule
     \multirow{3}{*}{Validation} 
     & IoU  & 0.418      & 0.462    & 1.832  & 2.423  & 0.47  & 0.443  \\
     & Omission  & 0.073      & 0.057      &  2.177 & 2.470  & 0.037  & 0.063  \\
     & Commission & 0.021     & 0.021      & 3.346  & 3.611  & 0.031  & 0.025  \\ 
     \midrule
     \multirow{3}{*}{Test}
     & IoU  & 0.568           & 0.57       & 1.832  & 2.423  & 0.633  & 0.637  \\
     & Omission  & 0.079      & 0.042      &  2.177 & 2.470  & 0.038  & 0.044  \\
     & Commission & 0.017      & 0.038     & 3.346  & 3.611  & 0.028  & 0.025  \\ 
     \bottomrule
  \end{tabular}}
\end{scriptsize}
% \vskip -0.3cm
\end{table}
\end{comment}

\begin{comment}
\begin{table}[!thp]
\centering
\begin{scriptsize}
  \caption{MSE on {\it TPT-48}. Our method significantly reduce the MSE by striking a good balance between worst-case and influential groups.}\label{tab:mse48}
  \label{tab:tpt}
%   \vskip 0.1cm
  \centering
  \resizebox{0.8\linewidth}{!}{
 	\begin{tabular}{@{}lcccc@{}}
			\toprule
			     & Hop-1 Avg.  &Hop-2 Avg.  &Hop-3 Avg.  &Avg.\\
			\hline
			ERM   & 2.354  &2.393  &3.418  &2.798 \\
			IWERM   & 2.916  & 3.180  &4.032  &3.624 \\
			TRO  & 1.684  &1.773  &2.796  &2.219  \\
			\bottomrule
	    \end{tabular}}
\end{scriptsize}
% \vskip -0.3cm
\end{table}
\end{comment}


\begin{comment}
\subsection{Ablation Study}\label{sec:ablation}

\begin{table}[t!]
	\centering
	\begin{scriptsize}
	\caption{Ablation study on the topological structure of groups. The performance of our method in the presence of partially-known topological structures is only slight worse than that with fully-known topological structures.}\label{tab:partial_results}
	\resizebox{0.55\linewidth}{!}{
	\begin{tabular}{@{}lcccccc@{}}
		\toprule
		   & DG-15 & E(24)$\rightarrow$W(24) & N(24)$\rightarrow$S(24)\\
		\hline
		ERM             &56.89 &1.726  &2.798 \\
		DRO            &52.44 &1.585  &2.514 \\
		Ours (partial)  &64.11      & 1.394 & 2.306 \\
		Ours (full)     &67.57 &1.343  &2.219 \\
		\bottomrule
	\end{tabular}}
	\end{scriptsize}
\end{table}

\begin{minipage}[t!]{\textwidth}
	%\vspace{0.08in}
	\begin{scriptsize}
	\begin{minipage}[t!]{0.48\textwidth}
        \centering
        \captionof{table}{Mixture weights on {\it TPT-48}. Our method focuses the attention on both worst-case and influential groups. }\label{tab:mix48}
		\resizebox{1.0\linewidth}{!}{
		\begin{tabular}{@{}lccccccc@{}}
			\toprule
			     & ND & MN & WI & OR & NE & IL & RI  \\
			\hline
			Prior    & 0.08  &0.02  &0.12  &0.06  &0.16  &0.11  &0.21 \\
			DRO   & 0.74  &0.19  &0.0  &0.0  &0.0  &0.06  &0.0\\
			Ours  & 0.49  &0.02  &0.03  &0.01  &0.03  &0.13  &0.06 \\
			\bottomrule
	    \end{tabular}}
	 \end{minipage}
	\hspace{0.1in}
	\begin{minipage}[t!]{0.48\textwidth}
	     \centering
        \captionof{table}{MSE on {\it TPT-48}. Our method significantly reduce the MSE by striking a good balance between worst-case and influential groups. }\label{tab:mse48}
		\resizebox{1.0\linewidth}{!}{
		\begin{tabular}{@{}lcccc@{}}
			\toprule
			     & Hop-1 Avg.  &Hop-2 Avg.  &Hop-3 Avg.  &Avg.\\
			\hline
			ERM   & 2.354  &2.393  &3.418  &2.798 \\
			IWERM   & 2.916  & 3.180  &4.032  &3.624 \\
			Ours  & 1.684  &1.773  &2.796  &2.219  \\
			\bottomrule
	    \end{tabular}}
    \end{minipage}
    \end{scriptsize}
\end{minipage}


In this section, we consider the scenario when the topological structure is only partially known and evaluate the necessity of distributionally robust optimization. 

% {\bf Partially-known topological structure}. In this scenario, we assume only the topological structure of training groups are available. We use Eq.~\ref{eq:prior_partial} to compute the centrality and construct the prior. We report the results in Tab.~\ref{tab:partial_results}.
% As seen, the result of ours (partial) is only slightly worse than ours (full), indicating the strong flexibility of the proposed method. 

{\bf Necessity of DRO}. We build a simple baseline based on ERM. We directly use the topological prior as the weights for training groups and the weights are fixed during the training. We name this baseline as Importance Weights ERM (IWERM). We show the results of ``N(24)$\rightarrow$S(24)'' on TPT-48. The results are shown in Tab.~\ref{tab:mse48}. We can see that the results of IWERM is even worse than ERM. We further show the mixture weights of some training groups in Tab.~\ref{tab:mix48}. As shown, dominated by the prior, IWERM assign high weights to ``RI'' and ``NE'' which are located close to the north-south boundary. As a result, the model may perform poorly on states near the border or coastal. For DRO, we notice that the model assign high weights to ``ND'' and ``MN'', the state farthest from the north-south boundary and located on the border. Our method strikes a good balance between them: both influential group (``IL'') and worst-case group (``ND'') are assigned with high weight, yielding lower MSE on unseen groups.
\end{comment}

