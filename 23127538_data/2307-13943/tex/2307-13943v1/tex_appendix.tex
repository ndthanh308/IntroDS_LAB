\section{Appendix}

% \subsection{Code instructions}

\subsection{Related Work} 

{\bf Distributionally Robust Optimization}.
In the context of distributionally robust optimization (DRO), \cite{duchi2021learning} and \cite{shalev2016minimizing} argued that minimizing the maximal loss over a set of possible distributions can provide better generalization performance than minimizing the average loss. 
% However, to confer robustness against extensive {\it distributional drifts}, the radius of the divergence ball has to be extremely large, which increases the risks of containing implausible distributions, \textit{e.g.,} outliers, and thus yielding overly pessimistic models with low prediction confidence~\citep{zhai2021doro}.
The robustness guarantee of DRO heavily relies on the quality of the uncertainty set which is typically constructed by moment constraints~\citep{delage2010distributionally}, \textit{f}-divergence~\citep{namkoong2016stochastic} or Wasserstein distance~\citep{shafieezadeh2018wasserstein}.
To avoid yielding overly pessimistic models, group DRO~\citep{hu2018does,sagawa2019distributionally} is proposed to leverage pre-defined data groups to formulate the uncertainty set as the mixture of these groups. Although the uncertainty set of Group DRO is of a wider radius while not being too conservative, our preliminary results show that Group DRO recklessly prioritizes the worst-case groups that incur higher losses than others. Such worst-case groups are not necessarily the influential ones that are truly connected to unseen distributions; optimizing over the worst-case rather than influential groups would yield mediocre OOD generalization performance.


{\bf Out-of-Distribution Generalization}.
The goal of OOD generalization~\citep{qiao2020learning,zhao2020maximum,qiao2021uncertainty} is to generalize the model from source distributions to unseen target distributions.
There are mainly two branches of methods to tackle OOD generalization: group-invariant learning~\citep{arjovsky2019invariant,koyama2020out,liu2021heterogeneous} and distributionally robust optimization. The goal of group-invariant learning is to exploit the causally invariant correlations across multiple distributions. 
Invariant Risk Minimization (IRM) is one of the most representative methods which learns the optimal classifier across source distributions.
However, recent work~\citep{rosenfeld2021risks} shows that IRM methods can fail catastrophically unless the test data are sufficiently similar to the training distribution.



\subsection{Implementation details}\label{sec:details}

In Sec.~\ref{sec:topology}, for all hyperparameters such as the kernel scale $\sigma^2$ and the maximum scale $K$, we use the default values from the official implementation\footnote{\url{https://github.com/KrishnaswamyLab/DiffusionEMD}} of \cite{tong2021diffusion}. 
In Sec.~\ref{sec:optimization}, for learning rate of model parameters $\eta_\theta$, we use default values from \cite{xu2022graph} (\textit{DG-15/-60} and \textit{TPT-48}) and \cite{bonafilia2020sen1floods11} (\textit{Sen1Floods11}).
Therefore, we only tune the learning rate of the mixture distribution $\eta_\mathbf{q}$ and the dual variable $\lambda$.
All results are reported over 3 random seed runs, which is consistent with \cite{koh2021wilds} and \cite{peng2022out}.
% In Figure 7, we did show both mean and standard deviation on the validation set of the Sen1Floods11 dataset across different $\lambda$.
We select $\lambda$ from \{1e-3, 1e-2, 1e-1, 1, 10, 100\} and select $\eta_\mathbf{q}$ from \{1e-4, 1e-3, 1e-2, 1e-1, 1\}.

\subsection{Additional results}\label{sec:addition}


{\it DG-15} and {\it DG-60}. We provide detailed classification results for each group. The results are shown in Fig.~\ref{fig:dg15_results}. We can see that, compared to other baselines, TRO significantly improves the generalization performance on groups that are far from the training groups, such as group ``\textit{13}''.
We further visualize the decision boundary of {\it DG-15} and {\it DG-60} in Fig.~\ref{fig:dg15_db} and Fig.~\ref{fig:dg60_db}, respectively.


% Figure environment removed

% Figure environment removed

% Figure environment removed

\begin{comment}
{\it TPT-48}. Our method does not require learning a very precise graph to calculate the prior $\mathbf{p}$ (group centrality) as different graphs may yield the same prior.
The hyperparameter $K$ (maximal scale in Eq.~\ref{eq:wasserstein}) plays a key role in estimating the diffusion earth moverâ€™s distance.
We visualize the graphs and their corresponding group centrality under different $K$.
Results are shown in Fig~\ref{fig:graphK}.
As observed, although different $K$ yields different graphs, both ``NY'' and ``PA'' are viewed as the most influential groups across all of the graphs.
Therefore, there is no significant difference in mean squared error (MSE) among these graphs.
In conclusion, our method does not require learning a very precise graph as long as the relative centrality of groups remains stable.

% Figure environment removed
\end{comment}

% Figure environment removed



\begin{table}[t]
	\centering
	\begin{scriptsize}
	\caption{Accuracy (\%) on \textit{PACS}. ``\textit{Art}'': ``\textit{Art}'' is the test group while the other three groups are training groups. In average accuracy, TRO outperforms the  SOTA by 0.5\% and outperforms ERM and DRO by 0.8\% and 2.4\%.}\label{tab:pacs}
	\resizebox{0.8\linewidth}{!}{
	\begin{tabular}{@{}l|cccc|c@{}}
\toprule
             & Art                            & Cartoon                        & Photo                          & Sketch                         & Average   \\ \hline
ERM          & \textbf{88.1}(0.1)                      & 77.9(1.3)                      & 97.8(0)                        & 79.1(0.9)                      & 85.7(0.5) \\ 
Group DRO    & 86.4(0.3)                      & 79.9(0.8)                      & \textbf{98.0}(0.3)                      & 72.1(0.7)                      & 84.1(0.4) \\ 
CORAL (SOTA) & {87.7(0.6)} &  {79.2(1.1)} &  {97.6(0)}   &  {\textbf{79.4}(0.7)} & 86.0(0.2) \\ 
TRO (ours)   &  {87.7(0.5)} &  {\textbf{82.1}(0.5)} &  {\textbf{98.0}(0.2)} &  {78.2(1.9)} & \textbf{86.5}(0.4) \\ 
\bottomrule
	\end{tabular}}
	\end{scriptsize}
\end{table}

\begin{table}[htp!]
	\centering
	\begin{scriptsize}
	\caption{Accuracy (\%) on \textit{Terra}. TRO achieves comparable results with the SOTA and outperforms ERM and DRO by 1.8\% and 2.0\%.}\label{tab:terra}
	\resizebox{0.8\linewidth}{!}{
	\begin{tabular}{@{}l|cccc|c@{}}
\toprule
             & L100                            & L38                        & L43                          & L46                         & Average   \\ \hline
ERM          & 50.8(1.8)                   & 42.5(0.7)                      & 57.9(0.6)                        & 37.6(1.2)                      & 47.2(0.4) \\ 
Group DRO    & 47.2(1.6)                      & 40.1(1.6)                      & 57.6(0.9)                    & \textbf{43.0}(0.7)                      & 47.0(0.3) \\ 
MMD (SOTA) & {52.2(5.8)} &  {\textbf{47.0}(0.6)} &  {57.8(1.3)}   &  40.3(0.5) & \textbf{49.3}(1.4) \\ 
TRO (ours)   &  \textbf{53.3}(2.4) &  42.2(1.3) &  \textbf{59.0}(0.8)&  41.3(0.5) & 49.0(0.6) \\ 
\bottomrule
	\end{tabular}}
	\end{scriptsize}
\end{table}

\textit{DomainBed}. Following the instructions of the official implementation of \textit{DomainBed}~\cite{gulrajani2020search}, we have conducted experiments on \textit{PACS}~\citep{li2017deeper}, \textit{Terra}~\citep{beery2018recognition}, and \textit{VLCS}~\citep{fang2013unbiased}. Image samples of the three datasets are shown in Fig.~\ref{fig:DomainBed} (left). 

(1) \textit{PACS} is one of the most popular dataset for out-of-distribution generalization. It consists of images from four groups: ``\textit{Art}'', ``\textit{Cartoon}'', ``\textit{Photo}'' and ``\textit{Sketch}''. Results on \textit{PACS} are shown in Tab.~\ref{tab:pacs}. 
Results of other baselines are from Appendix B.4 of \cite{gulrajani2020search}. 
In average accuracy, TRO outperforms the SOTA by 0.5\%.
To further investigate the results, we visualize the learned topology in Fig.~\ref{fig:DomainBed} (right). 
As observed, when ``\textit{Cartoon}'' is the test group, the topology is a chain graph consisting of three nodes where ``\textit{Art}'' is the most influential group. A possible explanation is that ``\textit{Art}'' may contain more information than  ``\textit{Photo}'' and ``\textit{Sketch}'' as ``\textit{Art}'' can be viewed as the combination of photos and various kinds of styles.
Even though the topology is so simple, it enables our method to significantly outperforms ERM and DRO by 0.8\% and 2.4\% on average. The results empirically demonstrate the strong explainability of our method when the number of training groups is quite limited, \textit{i.e.}, 3.
We would like to point out that when the distributional shift across different groups is small (see explanation on the results of \textit{VLCS}), the influential group may not exist and all groups share the same centrality. In this special case, TRO aims to strike a good balance between the average (ERM) risk and the worst-case (DRO) risk. 

\begin{table}[htp!]
	\centering
	\begin{scriptsize}
	\caption{Accuracy (\%) on \textit{VLCS}. The average accuracy of DRO and TRO is the same. We assume the reason is that the distributional shift across different groups is small~\citep{li2017deeper}, and therefore the influential group may not exist and all groups share the same centrality.}\label{tab:vlcs}
	\resizebox{0.8\linewidth}{!}{
	\begin{tabular}{@{}l|cccc|c@{}}
\toprule
             & Caltech101                            & LabelMe                        & SUN09                          & VOC2007                         & Average   \\ \hline
ERM          & 97.6(1.0)                   & 63.3(0.9)                      & 72.2(0.5)                        & 76.4(1.5)                      & 77.4(0.3) \\ 
Group DRO    & 97.7(0.4)                      & 62.5(1.1)                      & 70.1(0.7)                     & \textbf{78.4}(0.9)                     & 77.2(0.6) \\ 
DANN (SOTA) & {\textbf{98.5}(0.2)} &  {64.9(1.1)} &  \textbf{73.1}(0.7)   &  78.3(0.3) & \textbf{78.7}(0.3) \\ 
TRO (ours)   &  {96.9(0.2)} &  \textbf{65.0}(0.8) &  71.3(0.9) &  75.5(0.9) & 77.2(0.5) \\ 
\bottomrule
	\end{tabular}}
	\end{scriptsize}
\end{table}

(2) \textit{Terra} consists of images of wild animals captured by camera traps under four locations. 
Results on \textit{Terra} are shown in Tab.~\ref{tab:terra}. Results of other baselines are from Appendix B.6 of \cite{gulrajani2020search}.
As observed, in average accuracy, TRO achieves comparable results with the SOTA and outperforms ERM and DRO by 1.8\% and 2.0\%.

(3) Results on \textit{VLCS} are shown in Tab.~\ref{tab:vlcs}. Results of other baselines are from Appendix B.3 of \cite{gulrajani2020search}.
The average accuracy of DRO and TRO is the same.
We assume the reason is that the distributional shift across different groups is small~\citep{li2017deeper}, and therefore the influential group may not exist and all groups share the same centrality.
In this special case, TRO aims to strike a good balance between the average (ERM) risk and the worst-case (DRO) risk.
The images of \textit{VLCS} are all photos and the distributional shift is not as significant as \textit{PACS} (\textit{e.g.}, Photo vs. Sketch).
As stated in Sec.~2.1 of \cite{li2017deeper}, ``despite the famous analysis of dataset bias that motivated the creation of the \textit{VLCS} benchmark, it was later shown that the domain shift is much smaller with recent deep features'', and \textit{PACS}~\citep{li2017deeper} was proposed to address this limitation. 

\subsection{Proof of Theorem 1}\label{sec:theorem1}

{\it Proof}. By using the property of convex loss functions, we can obtain:

\begin{equation*}
\begin{aligned}
\max _{q \in \Delta_m}  \mathcal{R}\left(\theta_T^{ }, q\right)-\min _{\theta \in \Theta}  \mathcal{R}\left(\theta, q_T^{ }\right) \leq \frac{1}{T} \max _{q \in \Delta, \theta \in \Theta}\left\{\sum_{t=1}^T  \mathcal{R}\left(\theta^t, q\right)- \mathcal{R}\left(\theta, q^t\right)\right\},
\end{aligned}
\end{equation*}
where $\forall t \ge 1$:


\begin{equation*}
\begin{aligned}
 \mathcal{R}\left(\theta^t, q\right)- \mathcal{R}\left(\theta, q^t\right)=&  \mathcal{R}\left(\theta^t, q\right)- \mathcal{R}\left(\theta^t, q^t\right)+ \mathcal{R}\left(\theta^t, q^t\right)- \mathcal{R}\left(\theta, q^t\right) \\
\leq &\left\langle\left(q-q^t\right), \nabla_q  \mathcal{R}\left(\theta^t, q^t\right)\right\rangle+\left\langle\left(\theta^t-\theta\right), \nabla_\theta  \mathcal{R}\left(\theta^t, q^t\right)\right\rangle. \\
\end{aligned}
\end{equation*}
By rearranging the terms in the above equation, we obtain:
\begin{equation*}
\begin{aligned}
&\max _{q \in \Delta, \theta \in \Theta}\left\{\sum_{t=1}^T  \mathcal{R}\left(\theta^t, q\right)- \mathcal{R}\left(\theta, q^t\right)\right\} \leq \mathbb{E}\left[\max _{\theta \in \Theta} \sum_{t=1}^T\left\langle\left(\theta^t-\theta\right), \hat{g}_\theta^t\right\rangle\right] + \\
&\mathbb{E}\left[\max _{q \in \Delta_m} \sum_{t=1}^T\left\langle\left(q-q^t\right), \hat{g}_q^t\right\rangle\right]+\mathbb{E}\left[\max _{q \in \Delta_m} \sum_{t=1}^T\left\langle q, \nabla_q  \mathcal{R}\left(\theta^t, q^t\right)-\hat{g}_q^t\right\rangle\right]\\
& + \mathbb{E}\left[\max _{\theta \in \Theta} \sum_{t=1}^T\left\langle \theta, \hat{g}_\theta^t-\nabla_\theta \mathcal{R}\left(\theta^t, q^t\right)\right\rangle\right].
\end{aligned}
\end{equation*}
Following \cite{collins2020task}, we will derive the combined bound by bounding the expectation of each term in the above equation.
For the first term, by utilizing the telescoping sum, we can obtain:
\begin{equation*}
\begin{aligned}
\mathbb{E}\left[\max _{\theta \in \Theta} \sum_{t=1}^T\left\langle\left(\theta^t-\theta\right), \hat{g}_\theta^t\right\rangle\right] &=\frac{1}{2} \sum_{t=1}^T \frac{1}{\eta_\theta}\left\|\theta^t-\theta\right\|_2^2+\eta_\theta\left\|\hat{g}_\theta^t\right\|_2^2-\frac{1}{\eta_\theta}\left\|\theta^t-\eta_\theta \hat{g}_\theta^t-\theta\right\|_2^2\\
& \leq \frac{2 R_{\Theta}^2}{\eta_\theta}+\frac{\eta_\theta}{2} \sum_{t=1}^T\left\|\hat{g}_\theta^t\right\|_2^2 \leq \frac{2 R_{\Theta}^2}{\eta_\theta}+\frac{\eta_\theta T \hat{G}_\theta^2}{2}.\\
\end{aligned}
\end{equation*}
Similarly, for the second term:
\begin{equation*}
\mathbb{E}\left[\max _{q \in \Delta_m} \sum_{t=1}^T\left\langle\left(q-q^t\right), \hat{g}_q^t\right\rangle\right] \leq \frac{2}{\eta_q}+\frac{\eta_q T \hat{G}_q^2}{2}.
\end{equation*}
The third term and the last term are bounded by $\sqrt{T} \tilde{\sigma}_q$ and $R_{\Theta} \sqrt{T} \tilde{\sigma}_\theta$, respectively. To this end, we can derive the overall bound as:
\begin{equation*}
\mathbb{E}\left[\max _{q \in \Delta_m}  \mathcal{R}\left(\theta_T, q\right)-\min _{\theta \in \Theta}  \mathcal{R}\left(\theta, q_T\right)\right] \leq \frac{2 R_{\Theta}^2}{\eta_\theta T}+\frac{\eta_\theta \hat{G}_\theta^2}{2}+\frac{2}{\eta_q T}+\frac{\eta_q \hat{G}_q^2}{2}+\frac{R_{\Theta} \tilde{\sigma}_\theta}{\sqrt{T}}+\frac{\tilde{\sigma}_q}{\sqrt{T}}.
\end{equation*}
The above bound can be minimized by setting the step sizes $\eta_{\theta}=2 R_{\Theta} /\left(\hat{G}_{\theta} \sqrt{T}\right)$ and $\eta_{q}=2 /\left(\hat{G}_{q} \sqrt{T}\right)$.

\subsection{Proof of Theorem 2}\label{sec:theorem2}
{\it Proof}.
Inspired by \cite{qian2019robust} and \cite{collins2020task}, we utilize the property of M-smooth to start the proof:
\begin{equation*}
\mathbb{E}\left[\sum_{i=1}^n q_i^t \ell_i\left(\theta^{t+1}\right) \right] \leq \sum_{i=1}^n q_i^t \ell_i\left(\theta^t\right)-\left(\eta_\theta-\frac{\eta_\theta^2 M}{2}\right)\left\|g_\theta^t\right\|^2+\frac{\eta_\theta^2 M \sigma_\theta^2}{2}.
\end{equation*}
We rearrange these terms by:
\begin{equation*}
\begin{aligned}
\left(\eta_\theta-\frac{\eta_\theta^2 M}{2}\right)\left\|g_\theta^t\right\|^2 \leq & \mathbb{E}\left[\sum_{i=1}^n q_i^t \ell_i\left(\theta^t\right)-\sum_{i=1}^n q_i^t \ell_i\left(\theta^{t+1}\right) \right]+\frac{\eta_\theta^2 M \sigma_\theta^2}{2} \\
=& \mathbb{E}\left[\sum_{i=1}^n q_i^t \ell_i\left(\theta^t\right)-\sum_{i=1}^n q_i^{t+1} \ell_i\left(\theta^{t+1}\right) \right] \\
+& \mathbb{E}\left[\sum_{i=1}^n q_i^{t+1} \ell_i\left(\theta^{t+1}\right)-\sum_{i=1}^n q_i^t \ell_i\left(\theta^{t+1}\right) \right]+\frac{\eta_\theta^2 M \sigma_\theta^2}{2}.
\end{aligned}
\end{equation*}
The second term of the above equation can be bounded by: 
\begin{equation*}
\begin{aligned}
\mathbb{E}\left[\sum_{i=1}^n q_i^{t+1} \ell_i\left(\theta^{t+1}\right)-\sum_{i=1}^n q_i^t \ell_i\left(\theta^{t+1}\right) \right] &=\mathbb{E}\left[\sum_{i=1}^n\left(q_i^{t+1}-q_i^t\right) \ell_i\left(\theta^{t+1}\right) \right] \\
& \leq \mathbb{E}\left[\left\|q^{t+1}-q^t\right\|_2 \sum_{i=1}^n\left(\ell_i\left(\theta^{t+1}\right)\right)^{1 / 2} \right] \\
& \leq \eta_q \sqrt{n} \hat{B} \hat{G}_q.
\end{aligned}
\end{equation*}
By using the Law of Iterated Expectations, we can obtain:
\begin{equation}\label{eq:f}
\begin{aligned}
&\left(\eta_\theta-\frac{\eta_\theta^2 M}{2}\right) \sum_{t=1}^T \mathbb{E}\left[\left\|g_\theta^t\right\|^2\right] \\
&\leq \mathbb{E}\left[\sum_{i=1}^n q_i^1 \ell_i\left(\theta^1\right)\right]-\mathbb{E}\left[\sum_{i=1}^n q_i^{T+1} \ell_i\left(\theta^{T+1}\right)\right]+2 T \eta_q \sqrt{n} \hat{B} \hat{G}_q+\frac{T M \eta_\theta^2 \sigma_\theta^2}{2} \\
&\leq \mathcal{R}\left(\theta^1, q^1\right)+\hat{B}+2 T \eta_q \sqrt{n} \hat{B} \hat{G}_q+\frac{T \eta_\theta^2 M \sigma_\theta^2}{2}.
\end{aligned}
\end{equation}
Next, we investigate the convergence of $q$:
\begin{equation*}
\begin{aligned}
\mathbb{E}\left[\mathcal{R}\left(\theta^t, q\right)-\mathcal{R}\left(\theta^t, q^t\right)  \right] &=\mathbb{E}\left[\frac{1}{2 \eta_q}\left(\left\|q-q^t\right\|_2^2+\left(\eta_q\right)^2\left\|\hat{g}_q^t\right\|_2^2-\left\|q-\left(q^t+\eta_q \hat{g}_q^t\right)\right\|_2^2\right)  \right] \\
& \leq \mathbb{E}\left[\frac{1}{2 \eta_q}\left(\left\|q-q^t\right\|_2^2+\left(\eta_q\right)^2\left\|\hat{g}_q^t\right\|_2^2-\left\|q-q^{t+1}\right\|_2^2\right)  \right] \\
& \leq \mathbb{E}\left[\frac{1}{2 \eta_q}\left(\left\|q-q^t\right\|_2^2+\left(\eta_q\right)^2 \hat{G}_q^2-\left\|q-q^{t+1}\right\|_2^2\right)  \right].
\end{aligned}
\end{equation*}
By aggregating the difference at all time steps, we obtain:
\begin{equation*}
\begin{aligned}
\sum_{t=1}^T \mathbb{E}\left[\mathcal{R}\left(\theta^t, q\right)-\mathcal{R}\left(\theta^t, q^t\right)\right] & \leq \sum_{t=1}^T \frac{1}{2 \eta_q} \mathbb{E}\left[\left\|q-q^t\right\|_2^2\right]-\frac{1}{2 \eta_q} \mathbb{E}\left[\left\|q-q^{t+1}\right\|_2^2\right]+\frac{\eta_q}{2} \hat{G}_q^2 \\
&=\frac{1}{2 \eta_q} \mathbb{E}\left[\left\|q-q^1\right\|_2^2\right]+\frac{\eta_q}{2} T \hat{G}_q^2 \\
& \leq \frac{1}{\eta_q}+\frac{\eta_q T \hat{G}_q^2}{2}.
\end{aligned}
\end{equation*}
Since the above equation holds for all $q \in \Delta_m$, we maximize the right hand side over $q \in \Delta_m$:
\begin{equation}\label{eq:q}
\frac{1}{T} \sum_{t=1}^T \mathbb{E}\left[\mathcal{R}\left(\theta^t, q^t\right)\right] \geq \max _{q \in \Delta_m}\left[\mathcal{R}\left(\theta_T, q\right)\right]-\left(\frac{1}{\eta_q T}+\frac{\eta_q \hat{G}_q^2}{2}\right).
\end{equation}
Eqs.~\ref{eq:f} and \ref{eq:q} show that TRO converges in expectation to an $(\epsilon, \delta)$-stationary point of $\mathcal{R}$ in $\mathcal{O}(1 / \epsilon^{4})$ stochastic gradient evaluations.

% \subsection{qroof of Theorem 3}\label{sec:theorem3}



