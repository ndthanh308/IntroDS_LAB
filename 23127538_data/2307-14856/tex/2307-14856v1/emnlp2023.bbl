\begin{thebibliography}{41}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Bach et~al.(2022)Bach, Sanh, Yong, Webson, Raffel, Nayak, Sharma,
  Kim, Bari, Fevry, Alyafeai, Dey, Santilli, Sun, Ben-david, Xu, Chhablani,
  Wang, Fries, Al-shaibani, Sharma, Thakker, Almubarak, Tang, Radev, Jiang, and
  Rush}]{promptsource}
Stephen Bach, Victor Sanh, Zheng~Xin Yong, Albert Webson, Colin Raffel,
  Nihal~V. Nayak, Abheesht Sharma, Taewoon Kim, M~Saiful Bari, Thibault Fevry,
  Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-david,
  Canwen Xu, Gunjan Chhablani, Han Wang, Jason Fries, Maged Al-shaibani, Shanya
  Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Dragomir Radev, Mike
  Tian-jian Jiang, and Alexander Rush. 2022.
\newblock {P}rompt{S}ource: An integrated development environment and
  repository for natural language prompts.
\newblock In \emph{Proc. the Annual Meeting of the Association for
  Computational Linguistics (ACL)}.

\bibitem[{Black et~al.(2022)Black, Biderman, Hallahan, Anthony, Gao, Golding,
  He, Leahy, McDonell, Phang, Pieler, Prashanth, Purohit, Reynolds, Tow, Wang,
  and Weinbach}]{gpt-neox}
Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence
  Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler,
  USVSN~Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben
  Wang, and Samuel Weinbach. 2022.
\newblock {GPT-NeoX-20B}: An open-source autoregressive language model.
\newblock In \emph{Proc. the Annual Meeting of the Association for
  Computational Linguistics (ACL)}.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei}]{gpt3}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
  Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter,
  Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
  Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
  Ilya Sutskever, and Dario Amodei. 2020.
\newblock Language models are few-shot learners.
\newblock \emph{ArXiv}.

\bibitem[{Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez,
  Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury,
  Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski,
  Garc{\'i}a, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph,
  Spiridonov, Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat,
  Lewkowycz, Moreira, Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat,
  Catasta, Wei, Meier-Hellstern, Eck, Dean, Petrov, and Fiedel}]{palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
  Abhishek Rao, Parker Barnes, Yi~Tay, Noam~M. Shazeer, Vinodkumar Prabhakaran,
  Emily Reif, Nan Du, Benton~C. Hutchinson, Reiner Pope, James Bradbury, Jacob
  Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm
  Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc{\'i}a,
  Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David
  Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David
  Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai,
  Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz,
  Erica~Oliveira Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee,
  Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele
  Catasta, Jason Wei, Kathleen~S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav
  Petrov, and Noah Fiedel. 2022.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{ArXiv}.

\bibitem[{Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova}]{bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{ArXiv}.

\bibitem[{Dong et~al.(2019)Dong, Yang, Wang, Wei, Liu, Wang, Gao, Zhou, and
  Hon}]{unilm}
Li~Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu~Wang, Jianfeng Gao,
  Ming Zhou, and Hsiao-Wuen Hon. 2019.
\newblock Unified language model pre-training for natural language
  understanding and generation.
\newblock In \emph{Proc. the Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang,
  He, Thite, Nabeshima, Presser, and Leahy}]{pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
  Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser,
  and Connor Leahy. 2020.
\newblock The {P}ile: An 800gb dataset of diverse text for language modeling.
\newblock \emph{ArXiv}.

\bibitem[{Gao et~al.(2021)Gao, Tow, Biderman, Black, DiPofi, Foster, Golding,
  Hsu, McDonell, Muennighoff, Phang, Reynolds, Tang, Thite, Wang, Wang, and
  Zou}]{eval-harness}
Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles
  Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
  Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang,
  and Andy Zou. 2021.
\newblock A framework for few-shot language model evaluation.

\bibitem[{Hao et~al.(2022)Hao, Sun, Dong, Han, Gu, and Wei}]{structured}
Yaru Hao, Yutao Sun, Li~Dong, Zhixiong Han, Yuxian Gu, and Furu Wei. 2022.
\newblock Structured prompting: Scaling in-context learning to 1,000 examples.
\newblock \emph{ArXiv}.

\bibitem[{Izacard and Grave(2021)}]{fid}
Gautier Izacard and Edouard Grave. 2021.
\newblock Leveraging passage retrieval with generative models for open domain
  question answering.
\newblock In \emph{Proc. of the Annual Conference of the European Chapter of
  the Association for Computational Linguistics (EACL)}.

\bibitem[{Kim et~al.(2021)Kim, Kim, Lee, Lee, Kwak, Dong~Hyeon, Park, Kim, Kim,
  Seo, Lee, Jeong, Lee, Kim, Ko, Kim, Park, Kim, Kang, Ryu, Yoo, Chang, Suh,
  In, Park, Kim, Kim, Jeong, Yeo, Ham, Park, Lee, Kang, Kang, Ha, Park, and
  Sung}]{hyperclova}
Boseop Kim, HyoungSeok Kim, Sang-Woo Lee, Gichang Lee, Donghyun Kwak, Jeon
  Dong~Hyeon, Sunghyun Park, Sungju Kim, Seonhoon Kim, Dongpil Seo, Heungsub
  Lee, Minyoung Jeong, Sungjae Lee, Minsub Kim, Suk~Hyun Ko, Seokhun Kim,
  Taeyong Park, Jinuk Kim, Soyoung Kang, Na-Hyeon Ryu, Kang~Min Yoo, Minsuk
  Chang, Soobin Suh, Sookyo In, Jinseong Park, Kyungduk Kim, Hiun Kim, Jisu
  Jeong, Yong~Goo Yeo, Donghoon Ham, Dongju Park, Min~Young Lee, Jaewook Kang,
  Inho Kang, Jung-Woo Ha, Woomyoung Park, and Nako Sung. 2021.
\newblock What changes can large-scale language models bring? intensive study
  on {H}yper{CLOVA}: Billions-scale {K}orean generative pretrained
  transformers.
\newblock In \emph{Proc. of the Conference on Empirical Methods in Natural
  Language Processing (EMNLP)}.

\bibitem[{Lester et~al.(2021)Lester, Al-Rfou, and Constant}]{t5-lm}
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In \emph{Proc. of the Conference on Empirical Methods in Natural
  Language Processing (EMNLP)}.

\bibitem[{Lewis et~al.(2019)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy,
  Stoyanov, and Zettlemoyer}]{bart}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
  Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2019.
\newblock {BART:} denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock \emph{ArXiv}.

\bibitem[{Lewis et~al.(2020)Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal,
  K\"{u}ttler, Lewis, Yih, Rockt\"{a}schel, Riedel, and Kiela}]{rag}
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
  Karpukhin, Naman Goyal, Heinrich K\"{u}ttler, Mike Lewis, Wen-tau Yih, Tim
  Rockt\"{a}schel, Sebastian Riedel, and Douwe Kiela. 2020.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks.
\newblock In \emph{Proc. the Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Lin(2004)}]{rouge}
Chin-Yew Lin. 2004.
\newblock Rouge: A package for automatic evaluation of summaries.
\newblock In \emph{Text Summarization Branches Out}.

\bibitem[{Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov}]{roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{ArXiv}.

\bibitem[{Longpre et~al.(2023)Longpre, Hou, Vu, Webson, Chung, Tay, Zhou, Le,
  Zoph, Wei, and Roberts}]{flan2022}
Shayne Longpre, Le~Hou, Tu~Vu, Albert Webson, Hyung~Won Chung, Yi~Tay, Denny
  Zhou, Quoc~V. Le, Barret Zoph, Jason Wei, and Adam Roberts. 2023.
\newblock The flan collection: Designing data and methods for effective
  instruction tuning.
\newblock \emph{ArXiv}.

\bibitem[{Moryossef et~al.(2019)Moryossef, Goldberg, and Dagan}]{webnlg}
Amit Moryossef, Yoav Goldberg, and Ido Dagan. 2019.
\newblock Step-by-step: Separating planning from realization in neural
  data-to-text generation.
\newblock \emph{ArXiv}.

\bibitem[{Mostafazadeh et~al.(2016)Mostafazadeh, Chambers, He, Parikh, Batra,
  Vanderwende, Kohli, and Allen}]{storycloze}
Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra,
  Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016.
\newblock A corpus and cloze evaluation for deeper understanding of commonsense
  stories.
\newblock In \emph{Proc. of The Annual Conference of the North American Chapter
  of the Association for Computational Linguistics (NAACL)}.

\bibitem[{Narayan et~al.(2018)Narayan, Cohen, and Lapata}]{xsum}
Shashi Narayan, Shay~B. Cohen, and Mirella Lapata. 2018.
\newblock Don't give me the details, just the summary! {T}opic-aware
  convolutional neural networks for extreme summarization.
\newblock In \emph{Proc. of the Conference on Empirical Methods in Natural
  Language Processing (EMNLP)}.

\bibitem[{Nie et~al.(2020)Nie, Williams, Dinan, Bansal, Weston, and
  Kiela}]{anli}
Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe
  Kiela. 2020.
\newblock Adversarial {NLI}: A new benchmark for natural language
  understanding.
\newblock In \emph{Proc. the Annual Meeting of the Association for
  Computational Linguistics (ACL)}.

\bibitem[{OpenAI(2023)}]{gpt4}
OpenAI. 2023.
\newblock Gpt-4 technical report.
\newblock \emph{ArXiv}.

\bibitem[{Patel et~al.(2022)Patel, Li, Rasooli, Constant, Raffel, and
  Callison-Burch}]{sap}
Ajay Patel, Bryan Li, Mohammad~Sadegh Rasooli, Noah Constant, Colin Raffel, and
  Chris Callison-Burch. 2022.
\newblock Bidirectional language models are also few-shot learners.
\newblock \emph{ArXiv}.

\bibitem[{Press et~al.(2022)Press, Smith, and Lewis}]{alibi}
Ofir Press, Noah Smith, and Mike Lewis. 2022.
\newblock Train short, test long: Attention with linear biases enables input
  length extrapolation.
\newblock In \emph{Proc. the International Conference on Learning
  Representations (ICLR)}.

\bibitem[{Radford and Narasimhan(2018)}]{gpt1}
Alec Radford and Karthik Narasimhan. 2018.
\newblock Improving language understanding by generative pre-training.
\newblock \emph{ArXiv}.

\bibitem[{Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song,
  Aslanides, Henderson, Ring, Young, Rutherford, Hennigan, Menick, Cassirer,
  Powell, van~den Driessche, Hendricks, Rauh, Huang, Glaese, Welbl, Dathathri,
  Huang, Uesato, Mellor, Higgins, Creswell, McAleese, Wu, Elsen, Jayakumar,
  Buchatskaya, Budden, Sutherland, Simonyan, Paganini, Sifre, Martens, Li,
  Kuncoro, Nematzadeh, Gribovskaya, Donato, Lazaridou, Mensch, Lespiau,
  Tsimpoukelli, Grigorev, Fritz, Sottiaux, Pajarskas, Pohlen, Gong, Toyama,
  de~Masson~d'Autume, Li, Terzi, Mikulik, Babuschkin, Clark, de~Las~Casas, Guy,
  Jones, Bradbury, Johnson, Hechtman, Weidinger, Gabriel, Isaac, Lockhart,
  Osindero, Rimell, Dyer, Vinyals, Ayoub, Stanway, Bennett, Hassabis,
  Kavukcuoglu, and Irving}]{gopher}
Jack~W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,
  Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,
  Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell,
  George van~den Driessche, Lisa~Anne Hendricks, Maribeth Rauh, Po-Sen Huang,
  Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan
  Uesato, John F.~J. Mellor, Irina Higgins, Antonia Creswell, Nathan McAleese,
  Amy Wu, Erich Elsen, Siddhant~M. Jayakumar, Elena Buchatskaya, David Budden,
  Esme Sutherland, Karen Simonyan, Michela Paganini, L.~Sifre, Lena Martens,
  Xiang~Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya,
  Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau,
  Maria Tsimpoukelli, N.~K. Grigorev, Doug Fritz, Thibault Sottiaux, Mantas
  Pajarskas, Tobias Pohlen, Zhitao Gong, Daniel Toyama, Cyprien
  de~Masson~d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor
  Babuschkin, Aidan Clark, Diego de~Las~Casas, Aurelia Guy, Chris Jones, James
  Bradbury, Matthew~G. Johnson, Blake~A. Hechtman, Laura Weidinger, Iason
  Gabriel, William~S. Isaac, Edward Lockhart, Simon Osindero, Laura Rimell,
  Chris Dyer, Oriol Vinyals, Kareem~W. Ayoub, Jeff Stanway, L.~L. Bennett,
  Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. 2021.
\newblock Scaling language models: Methods, analysis \& insights from training
  gopher.
\newblock \emph{ArXiv}.

\bibitem[{Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu}]{t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu. 2020.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock In \emph{Journal of Machine Learning Research (JMLR)}.

\bibitem[{Sakaguchi et~al.(2019)Sakaguchi, Bras, Bhagavatula, and
  Choi}]{winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock \emph{ArXiv}.

\bibitem[{Sanh et~al.(2022)Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai,
  Chaffin, Stiegler, Raja, Dey, Bari, Xu, Thakker, Sharma, Szczechla, Kim,
  Chhablani, Nayak, Datta, Chang, Jiang, Wang, Manica, Shen, Yong, Pandey,
  Bawden, Wang, Neeraj, Rozen, Sharma, Santilli, Fevry, Fries, Teehan, Scao,
  Biderman, Gao, Wolf, and Rush}]{t0}
Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid
  Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M~Saiful
  Bari, Canwen Xu, Urmish Thakker, Shanya~Sharma Sharma, Eliza Szczechla,
  Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang,
  Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng~Xin Yong,
  Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen,
  Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason~Alan Fries, Ryan
  Teehan, Teven~Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander~M
  Rush. 2022.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock In \emph{Proc. the International Conference on Learning
  Representations (ICLR)}.

\bibitem[{Shaw et~al.(2018)Shaw, Uszkoreit, and Vaswani}]{rpe}
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018.
\newblock Self-attention with relative position representations.
\newblock In \emph{Proc. of The Annual Conference of the North American Chapter
  of the Association for Computational Linguistics (NAACL)}.

\bibitem[{Soltan et~al.(2022)Soltan, Ananthakrishnan, FitzGerald, Gupta, Hamza,
  Khan, Peris, Rawls, Rosenbaum, Rumshisky, Prakash, Sridhar, Triefenbach,
  Verma, Tur, and Natarajan}]{alexaTM}
Saleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald, Rahul Gupta, Wael
  Hamza, Haidar Khan, Charith Peris, Stephen Rawls, Andy Rosenbaum, Anna
  Rumshisky, Chandana~Satya Prakash, Mukund Sridhar, Fabian Triefenbach, Apurv
  Verma, Gokhan Tur, and Prem Natarajan. 2022.
\newblock Alexatm 20b: Few-shot learning using a large-scale multilingual
  seq2seq model.
\newblock \emph{ArXiv}.

\bibitem[{Tay et~al.(2022)Tay, Dehghani, Tran, Garc{\'i}a, Bahri, Schuster,
  Zheng, Houlsby, and Metzler}]{ul2}
Yi~Tay, Mostafa Dehghani, Vinh~Quang Tran, Xavier Garc{\'i}a, Dara Bahri, Tal
  Schuster, Huaixiu Zheng, Neil Houlsby, and Donald Metzler. 2022.
\newblock Unifying language learning paradigms.
\newblock \emph{ArXiv}.

\bibitem[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,
  Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and
  Lample}]{llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
  Guillaume Lample. 2023.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{ArXiv}.

\bibitem[{Wang et~al.(2019)Wang, Pruksachatkun, Nangia, Singh, Michael, Hill,
  Levy, and Bowman}]{superglue}
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,
  Felix Hill, Omer Levy, and Samuel Bowman. 2019.
\newblock Superglue: A stickier benchmark for general-purpose language
  understanding systems.
\newblock In \emph{Proc. the Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Wang et~al.(2023)Wang, Le, Gotmare, Bui, Li, and Hoi}]{codet5+}
Yue Wang, Hung Le, Akhilesh~Deepak Gotmare, Nghi D.~Q. Bui, Junnan Li, and
  Steven C.~H. Hoi. 2023.
\newblock Codet5+: Open code large language models for code understanding and
  generation.
\newblock \emph{ArXiv}.

\bibitem[{Wei et~al.(2022{\natexlab{a}})Wei, Bosma, Zhao, Guu, Yu, Lester, Du,
  Dai, and Le}]{flan}
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester,
  Nan Du, Andrew~M. Dai, and Quoc~V Le. 2022{\natexlab{a}}.
\newblock Finetuned language models are zero-shot learners.
\newblock In \emph{Proc. the International Conference on Learning
  Representations (ICLR)}.

\bibitem[{Wei et~al.(2022{\natexlab{b}})Wei, Tay, Bommasani, Raffel, Zoph,
  Borgeaud, Yogatama, Bosma, Zhou, Metzler, Chi, Hashimoto, Vinyals, Liang,
  Dean, and Fedus}]{emergent}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian
  Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed~H.
  Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William
  Fedus. 2022{\natexlab{b}}.
\newblock Emergent abilities of large language models.
\newblock In \emph{Transactions on Machine Learning Research (TMLR)}.

\bibitem[{Workshop(2023)}]{bloom}
BigScience Workshop. 2023.
\newblock Bloom: A 176b-parameter open-access multilingual language model.
\newblock \emph{ArXiv}.

\bibitem[{Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and
  Choi}]{hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.
\newblock {H}ella{S}wag: Can a machine really finish your sentence?
\newblock In \emph{Proc. the Annual Meeting of the Association for
  Computational Linguistics (ACL)}.

\bibitem[{Zeng et~al.(2022)Zeng, Liu, Du, Wang, Lai, Ding, Yang, Xu, Zheng,
  Xia, Tam, Ma, Xue, Zhai, Chen, Zhang, Dong, and Tang}]{glm130b}
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi
  Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng~Lam Tam, Zixuan Ma, Yufei Xue,
  Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, and Jie Tang. 2022.
\newblock Glm-130b: An open bilingual pre-trained model.
\newblock \emph{ArXiv}.

\bibitem[{Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,
  Diab, Li, Lin et~al.}]{opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al. 2022.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{ArXiv}.

\end{thebibliography}
