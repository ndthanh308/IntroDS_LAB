% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{EMNLP2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%custom packages
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{graphicx, subfigure}
\usepackage{changepage}
\usepackage{booktabs}
\usepackage{verbatim}
\usepackage{bbm}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{placeins}

% appendix
\usepackage{enumitem}
\usepackage{titlesec}

\newcommand*{\affaddr}[1]{#1}
\newcommand*{\affmark}[1][*]{\textsuperscript{#1}}
\newcommand*{\email}[1]{\texttt{#1}}
\newcommand\blfootnote[1]{%
  \begingroup \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Exploiting the Potential of Seq2Seq Models as Robust Few-Shot Learners}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Jihyeon Lee\affmark[1]\affmark[*], Dain Kim\affmark[2]\affmark[*]\affmark[\dag], Doohae Jung\affmark[1]\affmark[*], Boseop Kim\affmark[1], Kyoung-Woon On\affmark[1]\\
\affaddr{\affmark[1]Kakao Brain}\\
\affaddr{\affmark[2]Kyunghee University}\\
\email{\affmark[1]\{gina.lee, wavy.ocean, boseop.kim, kyoungwoon.on\}@kakaobrain.com},\\
\email{\affmark[2]dannykm@khu.ac.kr}
}
% \author{Jihyeon Lee\affmark[*]\\
% {\tt\small gina.lee@kakaobrain.com}
% \And
% Dain Kim\affmark[*\dag]\\
% {\tt\small danny980521@gmail.com}
% \And
% Doohae Jung\affmark[*]\\
% {\tt\small wavy.ocean@kakaobrain.com}
% \AND
% Boseop Kim\\
% {\tt\small boseop.kim@kakaobrain.com}
% \And
% Kyoung-Woon On\\
% {\tt\small kyoungwoon.on@kakaobrain.com}
% }

\begin{document}
\maketitle
\begin{abstract}
% In-context learning, which offers excessive advantages compared to fine-tuning, primarily emerges from decoder-only models, while encoder-decoder (i.e., seq2seq) models rather show strength in methods based on weight updates.
% Recently, some studies have demonstrated the possibility of few-shot learning on seq2seq models, particularly for generation tasks such as summarization and translation.
% We argue that seq2seq models can serve as effective few-shot learners for natural language understanding (NLU) tasks, given that the framework is well constructed.
% Firstly, we design the prompt to be similar in structure to the pretraining objective, rather than simply adopting prompts from decoder-only models. 
% Secondly, we disturb the conventional encoder-decoder attention mechanism by leveraging the concept of retrieval augmented generative models, thereby addressing the inherent challenges of seq2seq models.
% Our method outperforms a decoder-only model that is six times larger and demonstrates robust performance improvement compared to traditional seq2seq models across diverse settings.
% Moreover, we present the first-ever validation of the NLU few-shot performance of existing seq2seq models using fair criteria.

In-context learning, which offers substantial advantages over fine-tuning, is predominantly observed in decoder-only models, while encoder-decoder (i.e., seq2seq) models excel in methods that rely on weight updates. Recently, a few studies have demonstrated the feasibility of few-shot learning with seq2seq models; however, this has been limited to tasks that align well with the seq2seq architecture, such as summarization and translation. Inspired by these initial studies, we provide a first-ever extensive experiment comparing the in-context few-shot learning capabilities of decoder-only and encoder-decoder models on a broad range of tasks. Furthermore, we propose two methods to more effectively elicit in-context learning ability in seq2seq models: objective-aligned prompting and a fusion-based approach. Remarkably, our approach outperforms a decoder-only model that is six times larger and exhibits significant performance improvements compared to conventional seq2seq models across a variety of settings. We posit that, with the right configuration and prompt design, seq2seq models can be highly effective few-shot learners for a wide spectrum of applications.~\footnote{We will release the toolkit for the in-context evaluation of seq2seq models.}
% \footnote{Experimental toolkit for seq2seq models would be released.}


\end{abstract}

\section{Introduction}
\label{sec: introduction}

Recent studies have demonstrated that large language models can possess entirely different competencies, referred to as emergent abilities~\citep{gpt3, palm, gopher, emergent}. 
The concept of emergent abilities in Large Language Models (LLMs), initially introduced by \citet{emergent}, posits that increasing the model size and dataset can lead to the sudden emergence of abilities such as in-context learning, complex reasoning, and common-sense reasoning. 
In-context learning, in particular, one of the distinct characteristics of LLMs, serves as a key metric for assessing their effectiveness.
\blfootnote{* Indicates equal contribution.}
\blfootnote{\dag Work done during internship at Kakao Brain.}


In-context learning refers to the ability of the model to perform tasks by leveraging contextual information provided through prompts, without the need for additional training (i.e., without weight updates).
Specifically, in the case of in-context few-shot learning, the model can generate suitable outputs for the desired target input by using a small number of examples. This offers a step beyond traditional frameworks, which typically require extensive training data and fine-tuning of the model. 
Instead, it presents a new paradigm where a single model can effortlessly perform new tasks without necessitating a separate training process.

The capability of in-context learning has been predominantly explored in decoder-only models, as the rapid evolution of pretrained models has mostly focused on these unidirectional architectures. However, the sequence-to-sequence (seq2seq) architecture, despite its significant advantage of encoding contexts without sacrificing bidirectionality, has not been extensively investigated regarding its potential for in-context learning capabilities~\cite{t0, t5-lm, ul2, alexaTM, sap, codet5+}.

\citet{t0} trained an encoder-decoder model using multitask prompts in a supervised manner, but this approach only enabled zero-shot generalization to new tasks. 
\citet{alexaTM} primarily demonstrated the in-context abilities of seq2seq models for generative tasks such as summarization and translation, which are tasks inherently well-suited for seq2seq models. 
They also reported performance on natural language understanding (NLU) benchmarks but only in a zero-shot scenario, making it difficult to confirm their in-context learning proficiency across a wide range of tasks.
% \citet{alexaTM} demonstrate the few-shot ability of the multilingual seq2seq model for generation tasks, summarization and translation, which are well-known to do by seq2seq models; whereas show worse results on SuperGLUE~\cite{superglue} tasks.

% \kloud{Motivated by the current landscape, for the first time, we thoroughly investigate the zero-shot to few-shot performance of two architectures (decoder-only and encoder-decoder) across a wide range of evaluation sets. Our findings demonstrate that seq2seq models are also serve as robust few-shot learners even outperforming their larger decoder-only counterparts~\cite{opt}.}

Motivated by the current landscape, for the first time, we thoroughly investigate the zero-shot to few-shot performance of seq2seq models across a wide range of evaluation sets. 
Our findings demonstrate that seq2seq models also serve as robust few-shot learners not only for generation tasks but also for understanding tasks, even outperforming their larger decoder-only counterparts~\cite{opt}.

% \kloud{Also, we experiment with various ways of incorporating prompts into the seq2seq models. While \citet{t5} and \citet{ul2} use sentinel tokens during the pretraining stage of seq2seq models to optimize the denoising objective, the proper application of these sentinel tokens during the inference stage (i.e., in-context learning) is less well-established, and is often not even mentioned in most studies~\cite{t0, ul2}. We find that aligning prompts with the pretraining objective can yield up to a +32\% performance improvement on SuperGLUE tasks.}

We conduct various experiments on how to structure the prompts in seq2seq models.
While \citet{t5, ul2} popularized the usage of sentinel tokens during the pretraining stage of seq2seq models to optimize the denoising objective, the proper application of the sentinel tokens during the inference stage (i.e., in-context learning) is less well-established, and is often not even mentioned in most studies~\cite{t0, ul2}.
We find that aligning prompts with the pretraining objective yields up to a +20.5\%p performance improvement on the SuperGLUE~\cite{superglue} benchmark.

Furthermore, we propose two fusion-based approaches that enhance the few-shot learning capability of encoder-decoder architecture models. 
The fundamental idea is to independently process each of the few-shot examples through an encoder and then merge these representations for decoding. 
This fusion-based approach offers several advantages. 
Firstly, it addresses the restricted maximum sequence length commonly found in seq2seq models~\cite{t5, ul2, t5-lm}, which is shorter than decoder-only models, by leveraging encoding parallelization. 
Secondly, we achieve more efficient encoding of multiple examples by avoiding unnecessary bidirectional attention calculations between them. 
Our approach demonstrates significant performance improvements in seq2seq settings, surpassing the OPT 66B model~\cite{opt} across various tasks. 
Despite the absence of complete attention across the contexts of the few-shot prompts, our approach consistently outperforms traditional methods. 
Moreover, our methodologies effectively eliminate the permutation bias commonly observed in few-shot learning scenarios.

Overall, our work reveals the few-shot capability of seq2seq models, which has been undervalued compared to their zero-shot and fine-tuned counterparts.
To summarize, our key contributions are:
1) we develop an in-context evaluation toolkit for seq2seq models and conduct extensive experiments to investigate the performance of seq2seq models in zero-shot to few-shot scenarios using fair criteria, 
2) we exploit the potential of few-shot learning in encoder-decoder models by exploring prompting strategies and fusion-based approaches, and
3) we experimentally demonstrate that the seq2seq model can outperform the decoder-only model with 6 times larger parameters across diverse datasets.
In Section~\ref{sec: discussion&conclusion}, we discuss the impact of these unprecedented findings on the evolution of LLMs.

%Section 2

% \kloud{Contribution summary and introduce sections}

% Recent studies have demonstrated that large language models could possess entirely different competence along with their size~\citep{gpt3, palm, gopher, emergent}.
% \citet{emergent} introduce the concept of emergent abilities and put emphasize on a model of a large-enough scale to emerge advanced linguistic abilities such as logical-reasoning, common-sense, and human-like-behavior.
% As more and more the size grows, capability of zero-shot and few-shot learning without additional training, known as prompt-based learning, has become crucial indicator for large language models.

% Capability of prompt-base learning has mainly investigated in decoder-only models since pretrained models have evolved at speed based on unidirectional architectures.
% Sequence-to-sequence (seq2seq) architecture has the great benefit of being able to encode long contexts without losing the bidirectionality, however, only a few studies pursue prompt-base learning on seq2seq models~\cite{t0, t5-lm, ul2, alexaTM, sap, codet5+}.
% \citet{t0} train an encoder-decoder model on a multitask prompts in a supervised fashion but only enables zero-shot generalization to new tasks.
% \citet{alexaTM} demonstrate the few-shot ability of the multilingual seq2seq model for generation tasks, summarization and translation, which are well-known to do by seq2seq models; whereas show worse results on SuperGLUE~\cite{superglue} tasks.
% In short, the ability of natural language understanding (NLU) leveraging in-context learning of seq2seq models is largely uncharted.
% Moreover, ~\citet{eval-harness} provide an evaluation protocol but only applicable for autoregressive language models~\cite{gpt2, gpt3, gpt-neo}.
% ~\citet{alexaTM} also exhibited that few-shot generation performance of their own trained seq2seq model exceeded larger decoder-only models, while hardly explained the reason for the inverse proportional relationship between the number of demonstrations and the evaluation score.}-> rel work들어가면 좋을듯.
% ~\citet{ul2} reported in-context learning ability of UL2 model on SuperGLUE tasks~\cite{superglue}, but strictly confined to zero- and one-shot settings meanwhile large decoder models were demonstrated for the wide range of the number of few-shot demonstrations~\cite{gpt3, gopher}. 
% ~\citet{eval-harness} released a unified baseline, where the in-hands evaluation of few-shot of decoder LLMs is available, and~\cite{why-gpt-learn} even explained the analogy of decoder model's mechanism during in-context learning to the optimizer's mechanism during training time. 
% Motivated by the current state, for the first time, we introduce a unified in-context learning framework for seq2seq models and fully investigate the zero-shot to few-shot NLU performance of recent models with less than 20 billion parameters on extensive evaluation sets.
% Further, we demonstrate that seq2seq models are capable few-shot learner not only for generation tasks but also for understanding tasks, even compare favorably with larger decoder-only counterparts~\cite{opt}.
% We find that optimizing the input-output structure for in-context learning leads to dramatically different results in few-shot scenarios.

% In Section~\ref{sec: closer look at io-structure}, we begin by elucidating that designing the prompts as much to follow the pretraining objective as possible has substantial impact on performance enhancement.
% \citet{t5} and \citet{ul2} popularized the utilization of sentinel tokens in pretraining stage of seq2seq models to optimize the span corruption objective. 
% Meanwhile, the proper usage of the sentinel tokens during the inference stage (i.e., in-context learning) has not been well-established and even unmentioned for most of the studies~\cite{t0, ul2}.
% Through the experiment on several input-output structures using sentinel tokens, we uncover that following to their respective pretraining scheme yields the best performance.
% In the optimal ablative setup for i/o structures, our approach outperforms the original baseline by up to +32\% on SuperGLUE tasks.



% We then propose two different architectures, borrowed from retrieval approaches, in Section~\ref{sec: leveraging retrieval}, to induce the NLU few-shot ability of seq2seq models.
% Especially since few-shot learning is an expandable concept with no limits on the number of examples, the encoder-decoder architecture encounters inherent challenges.
% Firstly, the maximum sequence length of the encoder, mostly limited to 512~\cite{t5, ul2, t5-lm}, is relatively shorter compared to the decoder-only model that supports a sequence length of up to 2048~\cite{opt, gpt3, gpt-neox}.
% The difference in sequence length results in inadequate extrapolation performance~\cite{alibi}.
% To complement the problems, we adopt a strategy of independently processing the few-shot examples in the encoder. This approach results in a notable performance improvement in seq2seq settings and surpasses the OPT 66B model~\cite{opt} on diverse tasks.
% Despite the absence of complete attention across the contexts constituting the few-shot prompts, our method does not bounded to the traditional method, but rather demonstrates better results.
% Moreover, our methodologies eliminate the significant permutation bias commonly observed in decoder-only models.
%뒤에 추가되는 findings가 있으면 요약해서 적기.
%특정 template, subtask 등에 대해 더 자세히 성능 언급해도 좋을듯.
%추후 전체 citation 추가


\section{Exploring prompting strategies for seq2seq models in in-context learning}
\label{sec: closer look at io-structure}

% Figure environment removed

We begin by demonstrating the different prompting strategies suitable for in-context learning of encoder-decoder models.
Since decoder-only models share a unified left-to-right attention architecture~\cite{gpt1}, we can naturally feed the demonstrations and target input in a sequential manner and perform generation to obtain the target output.
For instance, in a 5-shot English-French translation task, the input sequence is formed by concatenating five English-to-French demonstrations, followed by target English input, with the model expected to generate the corresponding French translation as the target output.

However, encoder-decoder models process inputs and outputs independently from the encoder and decoder, respectively, which can result in varying compositions of demonstrations, target input, and target output, as shown in Figure~\ref{fig: io-structure}.
The target input can be utilized either as an encoder input, resulting in the decoder generating only the target output, or as a decoder input, enabling the decoder to directly generate the answer conditioned on the target input.
% We conduct straightforward experiments to find an optimal input-output structure for seq2seq in-context learning.
% We adopt the latter approach and give ablation experiments regarding it.
We evaluate this ablation on the SuperGLUE dataset, with 1, 5, and 10-shot settings using four representative encoder-decoder models.
Table~\ref{tab: io-structure} shows that positioning the target input on the encoder side results in enhanced performance for all model types, with performance gains of up to +20.5\%p.
% The 0-shot experiments are excluded because it is not feasible to input the final context to decoder in the 0-shot setting.
% Further details of experimental settings in this section are described in Section~\ref{sec: experimental setup}.
% We interpret these results as aligning with the pretraining objective of encoder-decoder models.
% We interpret these results to indicate that positioning the target input in the encoder is better aligned with the pretraining objective of encoder-decoder models.
% During the pretraining phase, the context is fed to the encoder to learn connotative bidirectional representations, whereas the decoder generates the target answer solely in an autoregressive manner.
% Since the model has limited experience in generating output conditioned on context, incorporating the context into the encoder is considered more effective.
% We interpret these results as follows: Since the pretraining objective of encoder-decoder models involves generating output conditioned on bidirectionally encoded information from the encoder, rather than generating output conditioned on clues from the former context, placing the target input into the encoder side leads to superior performance.
We interpret these results as follows: During the pretraining stage, the model generates output conditioned on bidirectionally encoded information from the encoder, instead of relying on the preceding textual information from the decoder. As a result, placing the target input on the encoder side shows better performance.
% \begin{table}
% \centering
% \begin{adjustbox}{width=1\columnwidth}
% \begin{tabular}{lccc}
% \toprule
% \multirow{1}[1]{*}{Model}&\multicolumn{1}{c}
% {\textit{encoder}}&\multicolumn{1}{c}{}&
% \multicolumn{1}{c}{\textit{decoder}}\\
% \midrule
% T5~\cite{t5} & \textbf{61.39} && 46.65 \\
% T5-LM~\cite{t5-lm} & \textbf{60.08} && 53.43 \\
% T0~\cite{t0} & \textbf{65.07} && 56.00 \\
% UL2~\cite{ul2} & \textbf{59.64} && 53.27 \\
% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \caption{\textbf{Ablation on the placement of target input.} 
% The \textit{encoder} places the target input on the encoder-side, whereas the \textit{decoder} places it on the decoder-side. Bold denotes the best score. The complete results are reported in Appendix~\ref{app: detail table 1}.}
% \label{tab: io-structure}
% \vspace{-6mm}
% \end{table}

% 샷수 나타낸 버전
% \begin{table}
% \centering
% \begin{adjustbox}{width=1\columnwidth}
% \begin{tabular}{lccccccc}
% \toprule
% \multicolumn{1}{l}{\multirow{2}{*}{Model}} & 
% \multicolumn{3}{c}{\textit{encoder}} & 
% \multicolumn{1}{c}{} & 
% \multicolumn{3}{c}{\textit{decoder}} \\
% \cmidrule{2-4} \cmidrule{6-8}
% & \multicolumn{1}{c}{1-shot} & \multicolumn{1}{c}{5-shot} & \multicolumn{1}{c}{10-shot} &&
% \multicolumn{1}{c}{1-shot} & \multicolumn{1}{c}{5-shot} & \multicolumn{1}{c}{10-shot} \\
% \midrule
% T5 & \textbf{65.53} & \textbf{59.54} & \textbf{59.09} && 44.95 & 47.37 & 47.61 \\
% T5-LM & \textbf{61.72} & \textbf{58.77} & \textbf{59.74} && 51.76 & 53.85 & 54.68 \\
% T0 & \textbf{64.35} & \textbf{67.45} & \textbf{63.42} && 52.47 & 53.97 & 53.36 \\
% UL2 & \textbf{60.05} & \textbf{58.85} & \textbf{60.02} && 51.46 & 57.41 & 59.14 \\
% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \caption{\textbf{Ablation on the placement of target input.} 
% The \textit{encoder} places the target input on the encoder side, whereas the \textit{decoder} places it on the decoder side. Bold denotes the best score. The complete results are reported in Appendix~\ref{app: detail table 1}.}
% \label{tab: io-structure}
% \vspace{-6mm}
% \end{table}

\begin{table}
\setlength\tabcolsep{8pt}
\centering
\begin{adjustbox}{width=1\columnwidth}
\begin{tabular}{lccc}
\toprule
Model & \textit{encoder} & \textit{decoder} \\
 & (1/5/10-shot) & (1/5/10-shot) \\
\midrule
T5 & \textbf{65.53}/\textbf{59.54}/\textbf{59.09} & 44.95/47.37/47.61 \\
T5-LM & \textbf{61.72}/\textbf{58.77}/\textbf{59.74} & 51.76/53.85/54.68 \\
T0 & \textbf{64.35}/\textbf{67.45}/\textbf{63.42} & 52.47/53.97/53.36 \\
UL2 & \textbf{60.05}/\textbf{58.85}/\textbf{60.02} & 51.46/57.41/59.14 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{\textbf{Ablation on the placement of target input.} 
The \textit{encoder} places the target input on the encoder side, whereas the \textit{decoder} places it on the decoder side. Bold denotes the best score within each model and for the specific number of shots. The complete results are reported in Appendix~\ref{app: detail table 1}.}
\label{tab: io-structure}
\vspace{-6mm}
\end{table}

% Another difference arises from the multiple pretraining objectives of encoder-decoder models.
Additionally, recent state-of-the-art models include T5~\citep{t5} and T5 variants~\citep{t5-lm, t0, ul2, sap} suggest different training objectives for language modeling.
% These models particularly employ denoising objectives to enhance the ability of the encoder to learn better bidirectional representations.
\citet{t5, t5-lm, t0} employ denoising objectives to learn better bidirectional representations and utilize sentinel tokens to replace consecutive spans of tokens. 
\citet{ul2} additionally introduce the concept of mode switching with extra paradigm tags (i.e., [NLG],[NLU],[S2S]) that help the model learn suitable representations for a given task.

We argue that, even when performing downstream tasks, it is crucial to design the prompt to resemble the pretraining scheme to achieve optimal performance.
As depicted in Figure~\ref{fig: aligned-prompt}, we conduct experiments by incorporating a sentinel token and a mode tag into the vanilla prompt format.
% We assess the performance on the SuperGLUE dataset, averaging the results across 0, 1, 5, and 10-shot settings.
We assess the performance on the SuperGLUE datasets with various number of few-shot examples.
In accordance with the pretraining methodologies of the models, we position the sentinel token at the end of the input and the model tag at the beginning.
The results are presented in Table~\ref{tab: optimal-prompt}.
Following the pretraining objective yields a maximum performance gain of up to +13\%p.
Since the mode tag is exclusively used in UL2, it is not applied to the remaining models.
As expected, aligning with the pretraining objective produces the most favorable results, with a significant discrepancy in scores.
% \citet{alexaTM} also support to our argument by using two different few-shot learning modes, denoising mode and CLM mode, that correspond to the denoising and CLM tasks in the pretraining stage, respectively.
It is noteworthy that even T5-LM~\cite{t5-lm} and T0~\cite{t0}, which are trained without using the sentinel token during subsequent training, demonstrate the positive impact of adding the sentinel token to the prompt. 
Throughout the following sections, we employ the optimal objective-aligned prompting strategies that we identified in this section, as the default configuration in all experiments.

% 각각의 결과와 이에 대한 추가적인 분석은 어펜딕스에 있다.
% 1.특히 w/o sentinel 방식에서만 t5의 몇몇 태스크에서 샷 수에 관계없이 점수가 일정한 경우가 많았는데 이는 sentinel이 few-shot 상황에서 각 예제를 효과적으로 전달하는 데에 도움을 줌을 시사한다.
% 2. UL2의 경우 mode tag와 sentinel을 모두 추가했을 때 성능이 가장 좋았다. 
% 3. 흥미롭게도, 추가 학습 과정에서 sentinel을 쓰지 않도록 훈련된 t5-lm과 t0 또한 sentienl의 효과를 보았다, 비록 그 gain이 t5나 ul2에 비해 크지 않음에도.
% 4.특히 t0의 경우 zero-shot setting에서 fine-tuning(further training) 됐기 때문에 0-shot에서는 sentinel이 없는 경우에 점수가 더 높은 모습을 보이지만, 이후 예제가 늘어갈수록 그 점수차가 역전되는 모습을 확인할 수 있다.
% 이 모든 결과는 pre-train에 align된 few-shot이 중요함을 뒷받침한다.} 
% Mask tokens used in pre-training play a critical role in terms that they indicate where to be denoised from the given noised context.

% \begin{table}
% \centering
% \begin{adjustbox}{width=1\columnwidth}
% \begin{tabular}{lcccc}
% \toprule
% \multirow{1}[1]{*}{Model}&\multicolumn{1}{c}
% {\textit{vanilla}}&\multicolumn{1}{c}{}&
% \multicolumn{1}{c}{\textit{w/ sentinel}}&
% \multicolumn{1}{c}{\textit{w/ mode tag}}\\
% \midrule
% T5~\cite{t5} & 52.6 && \textbf{59.2} & - \\
% T5-LM~\cite{t5-lm} & 55.0 && \textbf{59.9} & - \\
% T0~\cite{t0} & 64.4 && \textbf{68.9} & - \\
% UL2~\cite{ul2} & 50.0 && 59.4 & \textbf{61.0} \\
% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \caption{\textbf{Ablation on the extra special tokens.}
% Figure~\ref{fig: aligned-prompt} depicts the structures of the \textit{vanilla}, \textit{w/ sentinel}, and \textit{w/ mode tag} types.
% Bold denotes the best score. The complete results are reported in Appendix~\ref{app: detail table 2}.}
% \label{tab: optimal-prompt}
% \vspace{-6mm}
% \end{table}

% 소수점 이하 한 자리
\begin{table}
\centering
\begin{adjustbox}{width=1\columnwidth}
\begin{tabular}{lcccc}
\toprule
Model & \textit{vanilla} & \textit{w/ sentinel} & \textit{w/ mode tag} \\
& (0/1/5/10-shot) & (0/1/5/10-shot) & (0/1/5/10-shot) \\
\midrule
T5 & 51.7/54.6/52.2/52.1 & \textbf{52.9}/\textbf{65.5}/\textbf{59.5}/\textbf{59.1} & - \\
T5-LM & 56.0/50.4/56.6/56.9 & \textbf{59.5}/\textbf{61.7}/\textbf{58.8}/\textbf{59.7} & - \\
T0 & \textbf{73.3}/\textbf{67.3}/60.4/56.8 & 72.2/64.4/\textbf{67.5}/\textbf{63.4} & - \\
UL2 & 52.4/50.1/48.4/49.3 & \textbf{58.8}/60.1/58.9/60.0 & 58.5/\textbf{62.2}/\textbf{61.2}/\textbf{62.3} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{\textbf{Ablation on the usage of sentinel tokens and mode tags.}
Figure~\ref{fig: aligned-prompt} depicts the structures of the \textit{vanilla}, \textit{w/ sentinel}, and \textit{w/ mode tag} types.
For \textit{w/ mode} setting, the sentinel token is also utilized.
Bold denotes the best score within each model and for the specific number of shots. 
Due to space constraints, scores are expressed up to the first decimal place.
The complete results are reported in Appendix~\ref{app: detail table 2}.}
\label{tab: optimal-prompt}
\vspace{-6mm}
\end{table}

% \begin{table}
% \centering
% \begin{adjustbox}{width=1\columnwidth}
% \begin{tabular}{lcccc}
% \toprule
% Model & \textit{vanilla} & \textit{w/ sentinel} & \textit{w/ mode tag} \\
% (\# of shots) & (0/1/5/10) & (0/1/5/10) & (0/1/5/10) \\
% \midrule
% T5 & 51.70/54.60/52.18/52.09 & \textbf{52.89}/\textbf{65.53}/\textbf{59.54}/\textbf{59.09} & - \\
% T5-LM & 56.00/50.36/56.61/56.89 & \textbf{59.49}/\textbf{61.72}/\textbf{58.77}/\textbf{59.74} & - \\
% T0 & \textbf{73.30}/\textbf{67.34}/60.37/56.77 & 72.18/64.35/\textbf{67.46}/\textbf{63.42} & - \\
% UL2 & 52.39/50.10/48.38/49.30 & \textbf{58.84}/60.05/58.85/60.02 & 58.48/\textbf{62.21}/\textbf{61.17}/\textbf{62.29} \\
% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \caption{\textbf{Ablation on the usage of sentinel tokens and mode tags.}
% Figure~\ref{fig: aligned-prompt} depicts the structures of the \textit{vanilla}, \textit{w/ sentinel}, and \textit{w/ mode tag} types.
% For \textit{w/ mode} setting, the sentinel token is also utilized.
% Bold denotes the best score for each model and the number of shots. The complete results are reported in Appendix~\ref{app: detail table 2}.}
% \label{tab: optimal-prompt}
% \vspace{-6mm}
% \end{table}

% \begin{table}
% \centering
% \begin{adjustbox}{width=1\columnwidth}
% \begin{tabular}{lccc}
% \toprule
% Model & \textit{vanilla} & \textit{w/ sentinel} \\
% (\# of shots) & (0/1/5/10) & (0/1/5/10) \\
% \midrule
% T5 & 51.83/53.93/52.53/51.93 & 52.88/65.00/59.10/58.93 \\
% T5-LM & 55.95/52.77/57.03/56.13 & 59.51/61.76/59.51/58.18 \\
% T0 & 73.22/65.95/59.86/56.53 & 72.15/66.89/68.61/64.18 \\
% UL2 & 52.31/50.60/49.30/49.17 & 58.58/60.79/60.15/59.41 \\
% UL2 \textit{w/ mode} & - & 58.20/63.89/62.28/62.08 \\
% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \caption{\textbf{Ablation on the extra special tokens.}
% Figure~\ref{fig: aligned-prompt} depicts the structures of the \textit{vanilla}, \textit{w/ sentinel}, and \textit{w/ mode tag} types.
% Bold denotes the best score. The complete results are reported in Appendix~\ref{app: detail table 2}.}
% \label{tab: optimal-prompt}
% \vspace{-6mm}
% \end{table}

% TODO: One closing sentence.
% Throughout the following sections, we employ the optimal structure that we identified as the default configuration in all experiments.
% 멋있는거 아래 단락..?
% By comprehensive search through the proper input-output data formats for encoder-decoder models discussed in this section, we apply the optimal one in common throughout the following sections.

\section{Fusion-based approaches for few-shot learning}
% \section{Leveraging fusional approaches for few-shot learning}
\label{sec: leveraging retrieval}
% 여기서부터 especially for few-shot setting으로 전개하면 될듯. 
% Multi-encoder models for scalable and robust seq2seq model

% Figure environment removed


% seq2seq model의 few-shot 성능을 저해하는 요소 중 하나는 pretraining objective와 few-shot learning 간의 gap이었으며, section2에서 이를 줄일 수 있었다.
% 이번 섹션에서 우리는 seq2seq model의 few-shot 성능을 저해하는 다른 요인과 함께, 모델 아키텍처와 관계없이 few-shot learning에서 고려되어야할 문제점들도 함께 살펴본다.
% 그리고 이 문제들을 해결하기 위해 fusional approach (influenced by passage retrieval techniques)를 제안한다.
% One of the factors hindering the few-shot performance of seq2seq models is the gap between the pretraining objective and few-shot learning, which we resolve in Section~\ref{sec: closer look at io-structure}.
% In this section, we examine another factor that impairs the few-shot performance of seq2seq models, along with the issues that need to be considered in few-shot learning regardless of the model architecture.
% To tackle these problems, we propose two fusional approaches influenced by passage retrieval techniques.
% In Section~\ref{sec: closer look at io-structure}, we address the gap between the pretraining objective and few-shot learning, which hinders the few-shot performance of seq2seq models.
In this section, we address another factor that impairs the performance of few-shot learning and propose two fusion-based approaches to mitigate this problem.
% In this section, we explore performance-effecting issues that can arise when performing few-shot learning with seq2seq models, and propose potential methodologies influenced by passage retrieval techniques.
% In this section, we examine the factors contributing to the limited performance of seq2seq models in few-shot learning and propose two approaches influenced by passage retrieval techniques to overcome these limitations.


% That is, the performance of the model may be negatively affected if the length of the few-shot context is longer than a certain level.
% That is, encoder-decoder models with shorter training sequence lengths in the encoder exhibit limited performance in handling long-context few-shot learning scenarios.
% That is, encoder-decoder models with shorter training sequence lengths, compared to the decoder-only models, often demonstrate limited performance when dealing with long-context few-shot learning scenarios.
% That is, encoder-decoder models with shorter training sequence lengths (512~\cite{t5, ul2} or 1,024~\cite{t0}), compared to the decoder-only models (2048~\cite{opt, gpt3, gpt-neox}), often demonstrate limited performance when dealing with long-context few-shot learning scenarios.
T5-family models~\cite{t5, t5-lm, t0, ul2} utilize relative position encoding (RPE)~\cite{rpe}, thereby enabling to process long input sequences as long as computing memory allows. 
However, as shown in \citet{alibi}, the extrapolation ability of T5-style RPE diminishes when the input length exceeds twice the maximum pretrained sequence length. 
That is, encoder-decoder models with shorter training sequence lengths of 512~\cite{t5, ul2}, compared to decoder-only models ~\cite{opt, gpt3, gpt-neox}, often exhibit limited performance when dealing with long-context few-shot learning scenarios.
Additionally, there is a challenge in terms of computational cost, which escalates quadratically as the number of shots increases.
Another challenge to address is the presence of permutation bias.
The order of demonstrations affects the positional embedding of tokens, resulting in different predictions even when the examples are identical but the order varies.

% Furthermore, computational cost and permutation bias are additional challenges that should be considered in few-shot learning, regardless of the model architecture.
% As the number of shots increases, the computational cost escalates quadratically.
% Moreover, the order of demonstrations affects the positional embedding of tokens, resulting in different predictions even if the examples are identical but the order varies.
% These limitations diminish the effectiveness of the seq2seq model for few-shot learning, discouraging further investigation in this area.

%\denver{To solve these problems, we propose new few-shot methods inspired by FiD~\cite{fid} and RAG~\cite{rag}. 
%From the retrieval-reader model's point of view, we treat each demonstration as a retrieved document. (e.g. For 5-shot setting, it's like having 5 retrieved documents.) 
%Next, we construct each encoder input by attaching a target question to the demonstration, as depicted in Figure~\ref{fig: main-figure}.}
%알렉사 때문에 뉴 메소드라는 표현이 맞을지 심히 고민된다. -> 이거 일단 회피 가능
%target question이라는 용어가 애매한듯..? 하지만 대체제가 없어 보이기도 한다.

% 이 문제들을 해결하기 위해서 리트리버 리더의 개념을 few-shot에 차용해 적용할 것이다. retrieved documents를 query와 함께 단순히 일렬로 쭉 이어서 reader model의 encoder에 전달할 수 있겠지만, 그렇지않고 멀티 인코더를 도입해 indivisual passage로부터 얻은 정보를 차후에 종합하는 것으로 1) 일렬로 쭉 이어서 길이지는 문제를 방지할 수 있고 2) 일렬로 쭉 잇는 과정에서 발생하는 passage들간의 순서 문제로부터 자유로워질 수 있다. 

% 위 맥락에서, 이제 우리는 각각의 few-shot 예제를 retrieved document로 취급해 생각할 것이다. (퓨샷에서의 예제와 retrieved document 모두 target의 생성을 돕는 additional context라는 공통점) (자연스럽게 original 녹여내기) (e.g. For 5-shot setting, it's like having 5 retrieved documents.) 쿼리는 target question에 해당하고, 원하는 target은 target answer에 해당한다. 보통 퓨샷 예제를 활용하는 방법으로 일렬로 examples를 쭉 잇는 방법을 사용한다. 우리는 이것을 \textit{original} method라고 부르겠다. 이것은 위에서 말한 서로 간에는 관계가 없는 retrieved passage들을 일렬로 잇는 것과 같은 상태이다. 그러면 The decoder parameterized by $\theta$ takes the encoder last hidden state $Enc(z_{1:k}, x)$ and ${i-1}$ tokens $y_{1:i-1}$ and calculates the probability of generating {i}-th token $y_{i}$:

% \denver{To address these problems, we leverage the concept of retrieval and reader for in-context learning.
% few-shot learning에서 사용하는 예시와 retrieved passage는 모두 question에 attach되어 정답을 추론하는데에 도움이 되는 정보를 주는 additional context라는 공통점이 있다.}
%The retrieved documents along with the query can be passed to the reader model's encoder by simply arranging them in a row.
% But otherwise, by integrating the information obtained from individual passages later, we can reduce 1) the extrapolation ability problem caused by long input length and 2) the impact of document order.

% 그냥 깔끔하게 섹션 6 Retrieval augmented generative models 붙여오는게 확실히 깔끔해보입니다..! 이렇게 한다면 AlexaTM 부분은 FiD 뒤쪽으로 빼야할것 같습니다.
% 이 문제들을 해결하기 위해, 우리는 각각의 few-shot 예제를 따로 모델에 주어준 다음 특정 stage에서 fusion한다. 그리고 어떤 stage에서 fusion하는지에 따라 rag-style과 fid-style로 구분한다.
% To address these problems, we handle each few-shot example independently by feeding them into the model separately.
To address these problems, we propose a fusion-based approach, in which each demonstration is independently processed by the encoder.
Afterwards, we merge them together at a specific stage.
We hypothesize that the encoding of relations between demonstrations does not significantly impact in-context learning performance. 
% We categorize this approach into two styles: \textit{RAG-style} and \textit{FiD-style}, depending on the fusion stage, borrowing the names from retrieval-based models.
We categorize this approach into two styles: \textit{early-fusion} and \textit{late-fusion}, differentiated by the stage at which the fusion occurs.
These concepts are inspired by retrieval-based generation models.
% To address these problems, we leverage the passage retrieval technique, an active research area to improve QA systems.
% \citet{rag} presented the general-purpose fine-tuning recipe RAG for retrieval-augmented generative models, which combines parametric (i.e., pretrained seq2seq model as a generator) and non-parametric memory (i.e., retriever consists of query encoder and document index) for language generation.
% \citet{rag} propose retrieval-augmented generation (RAG), which combines parametric (i.e., pretrained seq2seq model as a generator) and non-parametric memory (i.e., retriever consists of query encoder and document index) for language generation.
% Specifically, \citet{rag} introduced RAG, a general-purpose fine-tuning recipe for retrieval-augmented generative models.
% RAG combines parametric (i.e., pretrained seq2seq model as a generator) and non-parametric memory (i.e., retriever consists of query encoder and document index) for language generation.
% More precisely, each retrieved document is combined with the query and passed through the model independently, and a weighted sum of output probabilities is made at the sequence level or token level.
% More precisely, each retrieved document is integrated with the query and passed through the model independently, and the final output probability is calculated as a weighted sum of the probabilities of each output.
\citet{rag} propose Retrieval-Augmented Generation (RAG), 
where each retrieved document is integrated with the query and passed through the model independently, and the final output probability is calculated as a weighted sum of the probabilities of each output.
% Fusion-in-Decoder (Fid), introduced by \citet{fid}, differs from RAG in that the information passed through the encoder is simply concatenated without any weighting and combined at the decoder.
Fusion-in-Decoder (Fid), introduced by \citet{fid}, differs from RAG in that the inputs passed through the encoder are concatenated before being sent to the decoder without any weighting. This allows the model to perform fusion at the decoder level.
% 정리하면, rag-style은 디코더 아웃풋을 합하는 late fusion, fid-style은 인코더의 아웃풋을 합하는 early fusion을 지칭한다.
We apply these ideas to few-shot learning of seq2seq models by treating each few-shot example as a retrieved document.
Therefore, we use the term \textit{late-fusion} to refer to the RAG-style method that combines the decoder outputs, whereas the term \textit{early-fusion} refers to the FiD-style method that combines the encoder outputs.
The formulas are described below.
% Given this perspective, we apply the retrieval augmented generator directly to the few-shot learning scheme of seq2seq models, and valid the idea through ablation experiments in Section~\ref{sec: evaluation results}.

% In this context, we consider each few-shot example as a retrieved document denoted by $z$ (e.g. in the 5-shot setting, five retrieved documents $z_{1:5}$ are available).
%The input question corresponds to query $x$, and our target also corresponds to the target sequence $y$. 
The traditional seq2seq in-context learning, which we refer to as the \textit{original} method, constructs the encoder input by concatenating the target input $x$ with the few-shot examples $z$ to generate the target output $y$ in the decoder~\citep{t5, ul2}. 
% Here, $k$ denotes the number of shots, and the term $z_{1:k}$ represents the summation of each few-shot example $z_1, z_2, ..., z_k$.
% Here, $k$ denotes the number of shots, and the term $z_{1:k}$ represents the summation of each few-shot example $z_1, z_2, ..., z_k$.
% To compute $f_{dec}$, which represents the probability of the $i$-th token of the target answer $y_{i}$, the decoder takes the last hidden state of the encoder $f_{enc}(z_{1:k}, x)$, along with $i-1$ target answer tokens $y_{1:i-1}$. 
The probability of the target output for \textit{original} method can be expressed as 
%\begin{align*} P_{\text{origin}}(y|x)  &\approx P_{\theta}(y|Enc(z_{1:k}, x)) \\&= \prod_i^N P_{\theta}(y_i|Enc(z_{1:k}, x),y_{1:i-1}) \end{align*}
\begin{align*} P_{\text{\textit{origin}}}(y|x,z)  &\approx \prod_i^N f_{\text{dec}}(y_i|f_{\text{enc}}(z_{1:k}, x),y_{1:i-1}) \end{align*}
where $f_{enc}$ and $f_{dec}$ are encoder and decoder, respectively, and $k$ denotes the number of shots. Here, $y_{i}$ is the $i$-th target token and $y_{1:i-1}$ is the partial output.
% This method suffers from extrapolation, computational cost, and permutation bias issues, which limit its effectiveness for few-shot learning tasks.
% This method can easily suffer from extrapolation, computational cost, and permutation bias issues, which limit its effectiveness for few-shot learning tasks.
% This method suffers from both extrapolation and computational cos computational and permutation bias issues, which limit its effectiveness for few-shot learning tasks.
%아래의 original few-shot이라는 용어도 애매한 것 같다. 어떤걸로 바꾸면 좋을까. simple few-shot?
%original few-shot에서 기본적인 수식 설명을 하는만큼, 이보다 더 앞 위치나 아예 앞 섹션에서 소개하고 가는 그림도 생각해볼 수 있을 것 같다.
%original 방법론인만큼 명시적으로 enc-dec에서 few-shot한 예시들을 cite해줘야겠다. (기왕이면 해당 논문에서 구체적으로 소개하고 있으면 좋을 것 같다.)
% original을 따로 paragraph화할 필요는 없을 것 같다. 사실 다른 paragraph도.
% +) 우리는 이것을 \textit{original}이라고 부르기로 했어요.
% \paragraph{Original Few-Shot}
%\denver{In most studies dealing with few-shot in encoder-decoder architecture, encoder input is simply constructed by connecting the input question $x$ after $k$ examples $z_{1:k}$ to generate the target $y$~\citep{t5, ul2}. 
%이것은 위에서 말한 서로 간에는 관계가 없는 retrieved passage들을 일렬로 잇는 것과 같은 상태이다.
%The decoder parameterized by $\theta$ takes the encoder last hidden state $Enc(z_{1:k}, x)$ and ${i-1}$ tokens $y_{1:i-1}$ and calculates the probability of generating {i}-th token $y_{i}$: \begin{align*} P_{Origin}(y|x)  &\approx P_{\theta}(y|Enc(z_{1:k}, x)) \\&= \prod_i^N P_{\theta}(y_i|Enc(z_{1:k}, x),y_{1:i-1}) \end{align*}}
%글을 쓰다보니 original 수식 설명부분이 RAG 논문의 2. Method 부분과 비슷해지는 것 같아 검토가 필요해보입니다.. 수식도 너무 비슷하게 가져온 느낌...
%위 수식에서 $z_{1:k}$를 그냥 FiD처럼 $\overset{k}{\underset{j}{Concat}}(z_j)$으로 표기할까 혹은 간단하게 $z_1, z_2, ... , z_k, x$로 할지 고민된다.

% \paragraph{RAG Style Few-Shot}
%We also leverage RAG~\cite{rag}, which is another approach that addresses the open domain question answering problems through retrieved passages.
% Instead of aggregating the information from the encoder and injecting it into the decoder, we leverage RAG~\cite{rag} so that the information is summed after passing through the entire model.
% The method can be categorized into two types: \textit{RAG-Sequence-style} and \textit{RAG-token-style}, depending on when the output probabilities are aggregated.
% In the \textit{RAG-Sequence-style} few-shot approach, we compute the final probability by aggregating all output probabilities for each one-shot example $P_{\theta}(y|Enc(z_j, x))$, as follows: \begin{align*} P_{Seq}(y|x) &\approx \sum_j^k P_{\theta}(y|Enc(z_j, x)) \\&= \sum_j^k \prod_i^N P_{\theta}(y_i|Enc(z_j, x),y_{1:i-1}) \end{align*}

% For the \textit{RAG-Token-style} type, the process of summing the output probabilities takes place for each token generation, rather than once at the end.
% Instead of aggregating the information from the encoder and injecting it into the decoder, we leverage RAG~\cite{rag}. 
% Unlike the \textit{original} approach, the \textit{RAG-style} approach, as depicted in Figure~\ref{fig: main-figure}-(a), integrates each one-shot demonstration $z_1$ after it passes through the entire model for each token generation.
% This can be formulated as: \begin{align*} p_{RAG}(y|x) \approx \prod_i^N \sum_j^k p_{\theta}(y_i|x,z_j,y_{1:i-1}) \end{align*}
% In contrast to the original RAG methodology in \citet{rag}, few-shot examples are obtained without going through the retrieve process. Therefore, we consider them equally retrieved, and no weighting is applied to individual examples.
% Unlike the \textit{original} approach, we propose a \textit{RAG-style} approach inspired by \cite{rag}.
% As can be seen in Figure~\ref{fig: main-figure}-(a), the information is summed after passing through the entire model for each token generation.
In the \textit{late-fusion} approach, as depicted in Figure~\ref{fig: main-figure}-(b), $j$-th example $z_j$ is concatenated to $x$ and each combination is utilized separately as the model input.
At each token generation step $i$, the information from $k$ instances that have passed through the entire model is aggregated as follows:
%\begin{align*} P_{RAG}(y|x) \approx \prod_i^N \sum_j^k P_{\theta}(y_i|Enc(z_j, x),y_{1:i-1}) \end{align*}
\begin{align*} P_{\text{\textit{late}}}(y|x,z) \approx \prod_i^N \sum_j^k f_{\text{dec}}(y_i|f_{\text{enc}}(z_j, x),y_{1:i-1}) \end{align*}
Unlike the original RAG model, in our few-shot settings, examples are obtained without going through the retrieval process. Therefore, we consider all examples to be equally retrieved, and no weighting is applied to individual examples.

% 앞과 말들이 이어지게 쓰기.
% \paragraph{FiD Style Few-Shot}
%오리지널과 다르게, 우리는 FID에 영감을 받은 접근을 사용했다. FiD는 encoder에서 query와 각각의 retrieved document를 연결한 pairs를 따로 처리하고, 이 결과물을 퓨젼해 디코더에 전달하는 방법론이다. 이것을 적용해보면 
% Unlike the \textit{original} approach, we propose a \textit{FiD-style} approach inspired by \citet{fid}.
% 최종 디코더 아웃풋을 합치는 대신, \textit{FiD-style}은 인코더 아웃풋을 합쳐 디코더에 전달한다.
Instead of combining the decoder outputs, \textit{early-fusion} approach merges the information from the encoder output and passes it to the decoder, as shown in Figure~\ref{fig: main-figure}-(a).
% As depicted in Figure~\ref{fig: main-figure}-(b), $j$-th example $z_j$ is connected to $x$ and each is utilized separately as the encoder input. 
% Its encoder output hidden state vector can then be expressed as:
% \begin{align*} e_j=f_{\text{enc}}(z_j, x) \end{align*} 
% The encoder output hidden state vector can be expressed briefly as $e_j=f_{\text{enc}}(z_j, x)$.
% After all the examples pass through the encoder, all encoder hidden states $e_1, e_2, ..., e_k$ are concatenated sequentially and used for cross-attention, as follows: 
% As depicted in Figure~\ref{fig: main-figure}-(b), the similarity to RAG lies in the fact that each input $(z_j, x)$ is separately fed to the encoder. 
All the last hidden states of the encoder, $h_1, h_2, ..., h_k$, where $h_j=f_{\text{enc}}(z_j, x)$, are concatenated sequentially and used for cross-attention,
% The encoder output hidden state vector can be expressed briefly as $e_j=f_{\text{enc}}(z_j, x)$.
% all encoder hidden states $e_1, e_2, ..., e_k$ are concatenated sequentially and used for cross-attention, as follows: 
%\begin{align*} P_{FiD}(y|x) &\approx  P_{\theta}(y|[e_1,e_2,...,e_k]) \\&=  \prod_i^N P_{\theta}(y_i|[e_1,e_2,...,e_k],y_{1:i-1}) \end{align*}
\begin{align*} P_{\text{\textit{early}}}(y|x,z) &\approx    \prod_i^N f_{\text{dec}}(y_i|[h_1,h_2,...,h_k],y_{1:i-1}) \end{align*}

These fusion-based approaches successfully mitigate the extrapolation problem by maintaining a shorter length of the encoder input. 
Moreover, inference time can be reduced in proportion to the number of examples $k$ through batch processing.
In the following sections, we experimentally demonstrate the capability of the seq2seq model as a robust few-shot learner by integrating the prompting strategies proposed in Section~\ref{sec: closer look at io-structure} with the fusion approaches presented in this section.
%(실제론 오히려 더 걸리나 넘어가도록 하자..! 배치화로 인한 padding+모델 구조상 벡터 차원 2차례에 걸쳐 변경 필요+타겟 퀘스션이 한번만 필요했는데 k개 붙는 상황으로 바뀜+최적화 이슈 등등이 원인일 것 같다.)
% Despite \citet{alexaTM} report the experimental results using the \textit{early-fusion} method in their table, they did not describe the experimental settings in detail and it is not known how much the score improved compared to the original methodology.
%\denver{Despite~\citet{alexaTM} report the experimental results using the \textit{FiD-style} method in their table, they did not describe the experimental settings in detail and it is not known how much the score improved compared to the original methodology.}
% To summarize, we significantly shorten the encoder input length by dispersing the lengthy few-shot demonstrations into the batches of 1-shot demonstrations. 
%section 2에서 제안한 prompting strategies와 본 섹션의 fusion approach를 integrate 해서, seq2seq model도 few-shot learning을 잘 할수 있음을 section5에서 실험으로 증명한다.
% We overcome the drawbacks of the aforementioned \textit{original} approach, including issues of extrapolation, speed, and permutation bias, and integrate the techniques with the concept discussed in Section~\ref{sec: closer look at io-structure}, resulting in a state-of-the-art few-show learning model.
% Particularly, in the case of \textit{RAG-style} method, the order relationship between examples is completely eliminated.
% We also eliminate unnecessary attention between few-shot examples. 
%- 동일한 dataset 및 실험 세팅 (통일된 기준으로 enc-dec 모델들을 비교한다는 점도 언급하기. 여기 혹은 section 2) -> section 4는 어떨까요?


\section{Experimental Setup}
\label{sec: experimental setup}
We provide a detailed description of our systematic experimental setup in the following paragraphs.
To ensure a fair evaluation, we employ identical prompt structures, evaluation tasks, scoring methods, prompt templates, and few-shot demonstrations across all baseline models.

\subsection{Baseline models}
\label{subsec: baseline models}

To test the impacts of our proposed methods on seq2seq models, we employ T5 and its variants, including T5-LM, T0, and UL2, as baseline models.
% T0 is the only the model fine-tuned with plenty of zero-shot multitask prompts
%Among the seq2seq baseline models, UL2 contains 20B parameters while other contain 11B.
Among seq2seq baseline models, T0 is the only model fine-tuned with plenty of zero-shot multitask prompts, and UL2 contains 20B parameters while others contain 11B.
% Throughout this paper, we refer to T5.1.1 as T5 since the released version of T5 checkpoint is a fine-tuned model.
% For the comparison with decoder-only models, we adopt the following as decoder baselines: OPT~\cite{opt}, BLOOM~\cite{bloom} and LLaMA~\cite{llama}, with parameter sizes ranging from 7B to 66B.
For the comparison with decoder-only models, we adopt the following as decoder baselines: OPT~\cite{opt} and BLOOM~\cite{bloom}, with parameter sizes ranging from 7B to 66B.
We provide detailed information about our baseline models in Appendix~\ref{app: training details for baseline models}.

% In our experiments, we employ T5 as a baseline model, along with its variants such as T5-LM, T0, and UL2, to ascertain the applicability of our method across a wide range of high-performing seq2seq models.
% As the original T5 has already undergone fine-tuning on several downstream tasks, we utilize T5 v1.1, which is trained solely on the C4 corpus.
% While \citet{t0} present three types of T5-architecture models (T0, T0+, and T0++) based on variations of fine-tuning datasets, we choose T0 to ensure that the datasets used in our experiments have not been previously encountered during the training process.
% Except for UL2, which has 20B parameters, the sizes of all other models are nearly the same as 11B parameters. 
% To assess the few-shot performance of seq2seq models against decoder-only models of comparable or larger size, we also incorporate the following competent models in our analysis: OPT~\cite{opt}, BLOOM~\cite{bloom} and LLaMA~\cite{llama}, with sizes ranging from 7B to 66B. 
% We provide training details about the baseline models in Appendix~\ref{app: training details for baseline models}.

\FloatBarrier
\begin{table*}[!ht]
% \vspace{-10mm}
\setlength\tabcolsep{2.3pt}
% \setlength\tabcolsep{0.5pt}
\begin{center}
\begin{adjustbox}{width=1\textwidth}{
\begin{tabular}{lccccccccccccc}
\toprule
Model & Shot & RTE & CB & ANLI R1 & ANLI R2 & ANLI R3 & WSC & Winogrande & COPA & StoryCloze & HellaSwag* & WiC & \textbf{average}\\
\midrule
\multicolumn{1}{l}{\multirow{3}{*}{{BLOOM-7B}}} & 
\multicolumn{1}{c}{5} & \multicolumn{1}{c}{59.35} & \multicolumn{1}{c}{51.43} & \multicolumn{1}{c}{33.66} & \multicolumn{1}{c}{33.92} & \multicolumn{1}{c}{34.22} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{65.02} & \multicolumn{1}{c}{75.60} & \multicolumn{1}{c}{70.83} & \multicolumn{1}{c}{46.32} & \multicolumn{1}{c}{50.63} & \multicolumn{1}{c}{50.68} \\
& \multicolumn{1}{c}{10} & \multicolumn{1}{c}{57.11} & \multicolumn{1}{c}{48.21} & \multicolumn{1}{c}{33.84} & \multicolumn{1}{c}{33.44} & \multicolumn{1}{c}{33.95} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{64.61} & \multicolumn{1}{c}{75.00} & \multicolumn{1}{c}{71.47} & \multicolumn{1}{c}{46.04} & \multicolumn{1}{c}{50.03} & \multicolumn{1}{c}{50.02} \\
& \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{57.47} & \multicolumn{1}{c}{57.14} & \multicolumn{1}{c}{33.40} & \multicolumn{1}{c}{33.64} & \multicolumn{1}{c}{34.62} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{64.75} & \multicolumn{1}{c}{78.80} & \multicolumn{1}{c}{71.96} & \multicolumn{1}{c}{46.62} & \multicolumn{1}{c}{50.03} & \multicolumn{1}{c}{51.36} \\
\midrule
\multicolumn{1}{l}{\multirow{3}{*}{{OPT-13B}}} & 
\multicolumn{1}{c}{5} & \multicolumn{1}{c}{50.47} & \multicolumn{1}{c}{33.57} & \multicolumn{1}{c}{34.24} & \multicolumn{1}{c}{33.16} & \multicolumn{1}{c}{34.05} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{68.68} & \multicolumn{1}{c}{86.00} & \multicolumn{1}{c}{79.02} & \multicolumn{1}{c}{52.76} & \multicolumn{1}{c}{52.01} & \multicolumn{1}{c}{50.95} \\
&\multicolumn{1}{c}{10} & \multicolumn{1}{c}{49.17} & \multicolumn{1}{c}{42.14} & \multicolumn{1}{c}{34.04} & \multicolumn{1}{c}{33.52} & \multicolumn{1}{c}{35.00} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{68.29} & \multicolumn{1}{c}{86.40} & \multicolumn{1}{c}{79.70} & \multicolumn{1}{c}{52.54} & \multicolumn{1}{c}{\underline{52.79}} & \multicolumn{1}{c}{51.83} \\
&\multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{49.39} & \multicolumn{1}{c}{40.71} & \multicolumn{1}{c}{35.04} & \multicolumn{1}{c}{32.90} & \multicolumn{1}{c}{35.70} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{68.59} & \multicolumn{1}{c}{87.40} & \multicolumn{1}{c}{80.17} & \multicolumn{1}{c}{52.88} & \multicolumn{1}{c}{\underline{52.45}} & \multicolumn{1}{c}{51.98} \\
\midrule
\multicolumn{1}{l}{\multirow{3}{*}{{OPT-30B}}} & 
\multicolumn{1}{c}{5} & \multicolumn{1}{c}{63.61} & \multicolumn{1}{c}{38.21} & \multicolumn{1}{c}{30.75} & \multicolumn{1}{c}{33.64} & \multicolumn{1}{c}{32.53} & \multicolumn{1}{c}{38.65} & \multicolumn{1}{c}{69.49} & \multicolumn{1}{c}{84.80} & \multicolumn{1}{c}{78.87} & \multicolumn{1}{c}{54.82} & \multicolumn{1}{c}{\underline{52.63}} & \multicolumn{1}{c}{52.55} \\
&\multicolumn{1}{c}{10} & \multicolumn{1}{c}{61.66} & \multicolumn{1}{c}{41.07} & \multicolumn{1}{c}{31.53} & \multicolumn{1}{c}{30.60} & \multicolumn{1}{c}{34.15} & \multicolumn{1}{c}{36.35} & \multicolumn{1}{c}{\underline{70.89}} & \multicolumn{1}{c}{84.60} & \multicolumn{1}{c}{79.58} & \multicolumn{1}{c}{55.26} & \multicolumn{1}{c}{51.22} & \multicolumn{1}{c}{52.45} \\
&\multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{62.67} & \multicolumn{1}{c}{58.57} & \multicolumn{1}{c}{31.78} & \multicolumn{1}{c}{31.56} & \multicolumn{1}{c}{32.72} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{70.75} & \multicolumn{1}{c}{86.60} & \multicolumn{1}{c}{80.57} & \multicolumn{1}{c}{55.70} & \multicolumn{1}{c}{52.23} & \multicolumn{1}{c}{54.52} \\
\midrule
\multicolumn{1}{l}{\multirow{3}{*}{{OPT-66B}}} & 
\multicolumn{1}{c}{5} & \multicolumn{1}{c}{\underline{66.14}} & \multicolumn{1}{c}{49.29} & \multicolumn{1}{c}{32.83} & \multicolumn{1}{c}{33.56} & \multicolumn{1}{c}{34.23} & \multicolumn{1}{c}{36.35} & \multicolumn{1}{c}{\underline{70.13}} & \multicolumn{1}{c}{\underline{88.40}} & \multicolumn{1}{c}{\underline{80.72}} & \multicolumn{1}{c}{\underline{56.72}} & \multicolumn{1}{c}{51.32} & \multicolumn{1}{c}{54.52} \\
&\multicolumn{1}{c}{10} & \multicolumn{1}{c}{\underline{63.68}} & \multicolumn{1}{c}{56.79} & \multicolumn{1}{c}{32.50} & \multicolumn{1}{c}{33.88} & \multicolumn{1}{c}{34.45} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{70.31} & \multicolumn{1}{c}{\underline{88.40}} & \multicolumn{1}{c}{\underline{81.53}} & \multicolumn{1}{c}{\underline{56.62}} & \multicolumn{1}{c}{51.54} & \multicolumn{1}{c}{55.11} \\
&\multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{\underline{68.23}} & \multicolumn{1}{c}{60.36} & \multicolumn{1}{c}{33.90} & \multicolumn{1}{c}{34.52} & \multicolumn{1}{c}{34.83} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{\underline{71.19}} & \multicolumn{1}{c}{\underline{89.80}} & \multicolumn{1}{c}{\underline{82.30}} & \multicolumn{1}{c}{\underline{57.10}} & \multicolumn{1}{c}{52.29} & \multicolumn{1}{c}{56.46} \\
% \midrule
% \multicolumn{1}{l}{\multirow{3}{*}{{LLaMA-7B}}} &
% \multicolumn{1}{c}{5} & \multicolumn{1}{c}{63.54} & \multicolumn{1}{c}{31.07} & \multicolumn{1}{c}{37.28} & \multicolumn{1}{c}{36.30} & \multicolumn{1}{c}{34.43} & \multicolumn{1}{c}{41.92} & \multicolumn{1}{c}{69.98} & \multicolumn{1}{c}{87.60} & \multicolumn{1}{c}{79.79} & \multicolumn{1}{c}{\underline{57.38}} & \multicolumn{1}{c}{50.41} & \multicolumn{1}{c}{53.61} \\
% & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{\underline{66.57}} & \multicolumn{1}{c}{40.00} & \multicolumn{1}{c}{36.86} & \multicolumn{1}{c}{36.74} & \multicolumn{1}{c}{36.05} & \multicolumn{1}{c}{37.88} & \multicolumn{1}{c}{\underline{71.30}} & \multicolumn{1}{c}{87.40} & \multicolumn{1}{c}{80.71} & \multicolumn{1}{c}{\underline{57.36}} & \multicolumn{1}{c}{50.94} & \multicolumn{1}{c}{54.71} \\
% & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{\underline{71.34}} & \multicolumn{1}{c}{38.93} & \multicolumn{1}{c}{36.14} & \multicolumn{1}{c}{37.80} & \multicolumn{1}{c}{37.33} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{\underline{71.87}} & \multicolumn{1}{c}{88.80} & \multicolumn{1}{c}{81.71} & \multicolumn{1}{c}{\underline{57.64}} & \multicolumn{1}{c}{52.54} & \multicolumn{1}{c}{55.51} \\
% \midrule
% \multicolumn{1}{l}{\multirow{3}{*}{{LLaMA-13B}}} & 
% \multicolumn{1}{c}{5} & \multicolumn{1}{c}{\underline{68.59}} & \multicolumn{1}{c}{46.43} & \multicolumn{1}{c}{39.10} & \multicolumn{1}{c}{37.74} & \multicolumn{1}{c}{36.33} & \multicolumn{1}{c}{44.81} & \multicolumn{1}{c}{\underline{75.97}} & \multicolumn{1}{c}{\underline{90.60}} & \multicolumn{1}{c}{\underline{81.81}} & \multicolumn{1}{c}{\underline{60.16}} & \multicolumn{1}{c}{\underline{57.84}} & \multicolumn{1}{c}{\textbf{58.13}} \\
% & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{\underline{68.45}} & \multicolumn{1}{c}{52.14} & \multicolumn{1}{c}{40.60} & \multicolumn{1}{c}{\underline{38.36}} & \multicolumn{1}{c}{37.58} & \multicolumn{1}{c}{40.96} & \multicolumn{1}{c}{\underline{77.03}} & \multicolumn{1}{c}{\underline{92.80}} & \multicolumn{1}{c}{\underline{82.77}} & \multicolumn{1}{c}{\underline{60.48}} & \multicolumn{1}{c}{\underline{61.32}} & \multicolumn{1}{c}{\textbf{59.32}} \\
% & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{68.09} & \multicolumn{1}{c}{52.86} & \multicolumn{1}{c}{40.54} & \multicolumn{1}{c}{\underline{38.28}} & \multicolumn{1}{c}{37.33} & \multicolumn{1}{c}{38.08} & \multicolumn{1}{c}{\underline{78.01}} & \multicolumn{1}{c}{\underline{93.60}} & \multicolumn{1}{c}{\underline{83.18}} & \multicolumn{1}{c}{\underline{60.62}} & \multicolumn{1}{c}{\underline{58.59}} & \multicolumn{1}{c}{\textbf{59.02}} \\
\midrule
\midrule
\multicolumn{1}{l}{\multirow{3}{*}{{T5-11B}}} & 
\multicolumn{1}{c}{5} & \multicolumn{1}{c}{55.02} & \multicolumn{1}{c}{53.93} & \multicolumn{1}{c}{33.70} & \multicolumn{1}{c}{35.00} & \multicolumn{1}{c}{35.62} & \multicolumn{1}{c}{39.04} & \multicolumn{1}{c}{64.64} & \multicolumn{1}{c}{82.80} & \multicolumn{1}{c}{77.71} & \multicolumn{1}{c}{45.32} & \multicolumn{1}{c}{50.50} & \multicolumn{1}{c}{52.12} \\
&\multicolumn{1}{c}{10} & \multicolumn{1}{c}{56.10} & \multicolumn{1}{c}{51.43} & \multicolumn{1}{c}{33.66} & \multicolumn{1}{c}{33.88} & \multicolumn{1}{c}{33.80} & \multicolumn{1}{c}{37.88} & \multicolumn{1}{c}{64.66} & \multicolumn{1}{c}{82.00} & \multicolumn{1}{c}{77.67} & \multicolumn{1}{c}{44.64} & \multicolumn{1}{c}{52.10} & \multicolumn{1}{c}{51.62} \\
&\multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{50.32} & \multicolumn{1}{c}{38.57} & \multicolumn{1}{c}{33.74} & \multicolumn{1}{c}{33.26} & \multicolumn{1}{c}{33.98} & \multicolumn{1}{c}{40.38} & \multicolumn{1}{c}{64.77} & \multicolumn{1}{c}{81.20} & \multicolumn{1}{c}{71.50} & \multicolumn{1}{c}{43.10} & \multicolumn{1}{c}{49.94} & \multicolumn{1}{c}{49.16} \\
\midrule
\multicolumn{1}{l}{\multirow{3}{*}{{T5-11B-\textit{early}}}} & 
\multicolumn{1}{c}{5} & \multicolumn{1}{c}{63.61} & \multicolumn{1}{c}{\underline{73.93}} & \multicolumn{1}{c}{\underline{40.80}} & \multicolumn{1}{c}{\underline{38.18}} & \multicolumn{1}{c}{\underline{39.55}} & \multicolumn{1}{c}{\underline{62.69}} & \multicolumn{1}{c}{62.48} & \multicolumn{1}{c}{84.20} & \multicolumn{1}{c}{77.84} & \multicolumn{1}{c}{45.90} & \multicolumn{1}{c}{50.03} & \multicolumn{1}{c}{\textbf{58.11}} \\
&\multicolumn{1}{c}{10} & \multicolumn{1}{c}{63.39} & \multicolumn{1}{c}{\underline{77.86}} & \multicolumn{1}{c}{\underline{41.18}} & \multicolumn{1}{c}{\underline{38.32}} & \multicolumn{1}{c}{\underline{39.58}} & \multicolumn{1}{c}{\underline{65.19}} & \multicolumn{1}{c}{62.23} & \multicolumn{1}{c}{84.80} & \multicolumn{1}{c}{77.56} & \multicolumn{1}{c}{45.80} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{\textbf{58.72}}\\
&\multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{64.04} & \multicolumn{1}{c}{\underline{75.00}} & \multicolumn{1}{c}{\underline{42.08}} & \multicolumn{1}{c}{37.94} & \multicolumn{1}{c}{\underline{39.98}} & \multicolumn{1}{c}{68.27} & \multicolumn{1}{c}{62.32} & \multicolumn{1}{c}{85.40} & \multicolumn{1}{c}{77.57} & \multicolumn{1}{c}{46.06} & \multicolumn{1}{c}{50.16} & \multicolumn{1}{c}{\textbf{58.98}} \\
\midrule
\multicolumn{1}{l}{\multirow{3}{*}{{T5-11B-\textit{late}}}} & 
\multicolumn{1}{c}{5} & \multicolumn{1}{c}{62.74} & \multicolumn{1}{c}{70.36} & \multicolumn{1}{c}{39.88} & \multicolumn{1}{c}{38.12} & \multicolumn{1}{c}{39.27} & \multicolumn{1}{c}{61.92} & \multicolumn{1}{c}{62.00} & \multicolumn{1}{c}{83.80} & \multicolumn{1}{c}{77.56} & \multicolumn{1}{c}{46.02} & \multicolumn{1}{c}{50.03} & \multicolumn{1}{c}{57.43} \\
& \multicolumn{1}{c}{10} & \multicolumn{1}{c}{63.32} & \multicolumn{1}{c}{73.93} & \multicolumn{1}{c}{40.16} & \multicolumn{1}{c}{\underline{38.32}} & \multicolumn{1}{c}{39.57} & \multicolumn{1}{c}{63.85} & \multicolumn{1}{c}{62.13} & \multicolumn{1}{c}{83.80} & \multicolumn{1}{c}{77.31} & \multicolumn{1}{c}{45.78} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{58.01} \\
& \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{63.90} & \multicolumn{1}{c}{70.36} & \multicolumn{1}{c}{40.64} & \multicolumn{1}{c}{\underline{38.06}} & \multicolumn{1}{c}{39.63} & \multicolumn{1}{c}{\underline{69.42}} & \multicolumn{1}{c}{62.15} & \multicolumn{1}{c}{84.80} & \multicolumn{1}{c}{77.27} & \multicolumn{1}{c}{45.88} & \multicolumn{1}{c}{50.22} & \multicolumn{1}{c}{58.39} \\
\bottomrule
\end{tabular}}
\end{adjustbox}
\caption{
\textbf{Comparison of our approach with various decoder models using minimal templates proposed by \citet{eval-harness}.} 
% The tasks are exactly the same as the evaluation dataset configuration that \citet{t0} used to report their main results. 
Bold denotes the best average score, and underline denotes the best score within each task.
T5 models with \textit{early-} and \textit{late-fusion} demonstrate superiority over decoder models in tasks such as CB and WSC, while exhibiting limitations in tasks such as Winogrande and Hellaswag. 
Tasks denoted with a star (*) indicate that 1K samples are evaluated due to their large size.
The row labeled ``gpt3'' refers to the optimal number of shots for each task suggested by \citet{gpt3}.
Detailed shot configurations are reported in Table~\ref{tab: gpt3 best shot}.
}
\label{tab: compare decoder vs. t5}
\end{center}
\vspace{-5mm}
\end{table*}

\subsection{Evaluation tasks and scoring methods}
\label{subsec: eval tasks and scoring method}

In Table~\ref{tab: io-structure} and~\ref{tab: optimal-prompt}, we evaluate the models on eight subtasks from SuperGLUE benchmarks to evaluate natural language understanding ability.
For comprehensive evaluation across various types of tasks, we additionally assess the baseline models on 11 tasks selected by \citet{t0} including five subtasks from SuperGLUE, Hellaswag~\cite{hellaswag}, ANLI~\cite{anli}, Winogrande~\cite{winogrande}, and StoryCloze~\cite{storycloze}.
We report accuracy as the metric, following the approach by \citet{gpt3}, and the model selects the option with the lowest cross-entropy loss among all the available multiple-choice options.
% By selecting the option representing the lowest cross-entropy loss among the available options, the accuracy is calculated throughout each of all the NLU tasks.
% By default, the loss for the \textit{eos} token, representing the final token in the decoder output, is excluded in the scoring because it is observed that the inclusion of the token generally sacrifices the score.
Any form of normalization with respect to the target output is not applied.
Please refer to Appendix~\ref{sec: normalization grounding} for justification regarding normalization.

We adopt two following generation tasks for the comprehensive evaluation: XSum~\cite{xsum} and WebNLG~\cite{webnlg}, which are summarization and data-to-text tasks, respectively.
All the samples are generated with greedy decoding, and the ROUGE~\cite{rouge} metric is used for the evaluation of generation tasks.

% In Table~\ref{tab: io-structure} and~\ref{tab: optimal-prompt}, we evaluate the models on eight tasks from SuperGLUE, which are widely adopted benchmarks for evaluating natural language understanding ability.
% % % Additionally, for Table~\ref{tab: compare decoder vs. t5} and Figure~\ref{fig: mean and std of models}, we evaluate the models on 11 held-out tasks specified in \cite{t0} to ensure a comprehensive evaluation across various task types.
% Additionally, for Table~\ref{tab: compare decoder vs. t5} and Figure~\ref{fig: mean and std of models}, we evaluate the models on 11 held-out tasks specified from \citet{t0} to ensure a comprehensive evaluation across various task types.
% The 11 datasets consist of the 6 subsets from SuperGLUE, as well as Hellaswag~\cite{hellaswag}, ANLI~\cite{anli}, Winogrande~\cite{winogrande}, and StoryCloze~\cite{storycloze}. 
% The model selects the option with the lowest cross-entropy loss among all the available multiple-choice options.
% By default, the loss for the \textit{eos} token, representing the final token in the decoder output, is excluded in the scoring because it is observed that the inclusion of the token generally sacrifices the score.
% We report accuracy as the metric, following the approach by \citet{gpt3}, and we do not apply length normalization to the target sequence in order to maintain a consistent condition throughout all tasks.
% Please refer to Appendix~\ref{sec: normalization grounding} for justification regarding normalization.

% We also test with XSum~\cite{xsum} and WebNLG~\cite{webnlg} tasks to verify whether the application of our methodology has an impact on the natural language generation task. Here, all the samples are generated with greedy decoding, and the ROUGE~\cite{rouge} metric is employed for evaluation.

\subsection{Prompts and few-shot demonstrations}
\label{subsec: prompts and few-shot demons}

We employ prompt templates offered by Language Model Evaluation Harness~\cite{eval-harness}, a widely adopted few-shot evaluation toolkit for decoder-only models.
Additionally, for each generation task, we adopt a single template offered by PromptSource~\cite{promptsource} since the templates for the generation tasks we utilize are not supported by Language Model Evaluation Harness.
% Since the performance of models can vary depending on the choice of few-shot demonstrations, we report the average scores obtained from five distinct random seeds otherwise not specified.
% To account for the potential performance variation due to different choices of few-shot demonstrations, we conduct five experiments with different random seeds and present the averaged scores, except for Table~\ref{tab: io-structure} and Table~\ref{tab: optimal-prompt}, where only a single seed is used.
% To account for the potential performance variation due to different choices of few-shot demonstrations, 
We conduct five experiments with different random seeds and present the averaged scores for Table~\ref{tab: compare decoder vs. t5} and Figure~\ref{fig: mean and std of models}.
For the other evaluations, we conduct a single experiment.
% We conduct five experiments with different random seeds and present the averaged scores for Table~\ref{tab: compare decoder vs. t5} and Figure~\ref{fig: main-figure} for extensive comparison.
% while conducting a single experiment for other evaluations.
For more detailed information about prompts and few-shot demonstrations, please refer to Appendix~\ref{app: prompt and demons details}.

% To ensure a fair comparison with decoder-only models, we evaluate the models using prompt templates offered by Language Model Evaluation Harness~\cite{eval-harness}.
% This toolkit is widely adopted for assessing the few-shot performance of decoder-only models.
% % Few-shot demonstrations are randomly sampled from the training set for each tasks, and the validation set is used for the target input and target output.
% For the few-shot demonstrations, we distinguish between two settings where the demonstrations used for each predictions are either maintained the same, referred to as the "fixed" setting, or randomly sampled at each time, referred to as "non-fixed" setting.
% Otherwise not specified, we report the average scores obtained from five distinct random seeds for natural language understanding tasks with fixed setting and a score obtained from a single seed for generation tasks with non-fixed setting.
% For more detailed information about prompts and few-shot demonstrations, see Appendix~\ref{app: prompt and demons details}.


\section{Evaluation Results}
\label{sec: evaluation results}
% In this section, we validate the effectiveness of our proposed method across diverse natural language understanding tasks. 
% In Section~\ref{subsec: better than gpt}, we demonstrate that our encoder-decoder model performs better to the larger decoder-only model both in 1-shot and few-shot settings.
% Experiments in Sections~\ref{subsec: encoder-deocder robustness} blah blah.
% Section~\ref{subsec: t0 findings} describes blah blah.
% We also provide blah blah.

\subsection{Seq2seq models perform better than decoder-only models}
\label{subsec: better than gpt}

In Table~\ref{tab: compare decoder vs. t5}, we evaluate both encoder-decoder and decoder-only models on a broad range of NLU benchmarks.
We select T5 as the primary baseline model to apply our approaches because it is the most commonly used seq2seq pretrained model. 
Furthermore, T5-LM and T0 are models that have undergone further training with different objectives, making it unfair to compare them against other pretrained models.
% Nevertheless, in order to ensure an equitable comparison, we compare T5 against larger decoder models, instead of other seq2seq models.
% By employing \textit{FiD-style} and sentinel tokens during inference, T5 achieves a superior performance over OPT-66B model, which is 6 times larger in parameter size, with a maximal margin of 3.6\%p in average as shown in Table~\ref{tab: compare decoder vs. t5}.
As shown in Table~\ref{tab: compare decoder vs. t5}, T5 with both \textit{early-} and \textit{late-fusion} approaches achieve superior performance over OPT-66B model, which is six times larger in parameter size, with a maximal margin of 3.6\%p in average.
%late 모델도 대부분 앞선다?
Even with the best-shot configuration found by GPT-3~\cite{gpt3}, our methods consistently exhibit superior performance compared to the decoder-only baselines in most cases, with a significant margin.
% shows that with the exploitation of \textit{FiD-style} and optimal structure during inference, T5 achieves superior performance over OPT-66B model, which is 6 times larger in parameter size, with a maximal margin of 3.6\%p in average.
Note that all the T5 variants reported in Table~\ref{tab: compare decoder vs. t5} already incorporate the objective-aligned prompt designs suggested in Section~\ref{sec: closer look at io-structure}.
% We also compare seq2seq baselines with other well-known public decoder-only models of similar size.
%rag가 살짝 낮음
%original 모델에 비해 경향성이 확실.
% Another thing to point out is that, while both \textit{early-fusion} and \textit{late-fusion} models demonstrate a tendency of performance improvements as the number of shots increases, the \textit{original} model exhibits a decline in performance. 
% This indicates that independent processing of demonstrations contributes to the enhancement of the few-shot capability of seq2seq models.
Complete results for the remaining seq2seq baselines (i.e., T5-LM, T0, and UL2) can be found in Appendix~\ref{app: detailed results for fig3}.
Additionally, we present the results of our method in comparison to other well-known decoder-only LLMs (i.e., GPT-3, PaLM, and GPT-NeoX) in Appendix~\ref{app: comparison with other LLMs}.

\subsection{Robustness of our approach regardless of the baseline models}
\label{subsec: encoder-deocder robustness}

% \wavy{Concatenated demonstrations with bidirectional attention adversely affect the few-shot performance of seq2seq models, especially as the number of demonstrations increases, as verified in Figure~\ref{fig:mean and std of nets}.
% Along with the results shown by \citet{ul2, alexaTM}, our experiments also show that more than one fully-attended demonstrations inadvertently exert a detrimental impact on the performance of the seq2seq models.}

% \wavy{Independently attended demonstrations as opposed to the fully-attended setting, does not sacrifice the performance with increase in the number of demonstrations, and even show higher gain compared to the large decoder LMs~\cite{opt} in few-shot setting up to ??\%. (수치 \% row 하나 잡아서 비교하기, 0,1-shot 대비 gain을 나타내는 표가 현재 없음)
% Surprisingly, T5~\cite{t5} in FiD-style show consistent increment in scores as the number of demonstrations increases, which are even hardly shown from decoder LLMs and not even from other seq2seq models~\cite{t5-lm, ul2, t0}.
% This tendency hold true for all the independently attended settings through FiD- and RAG-style.}

% \wavy{Upon careful analysis, it becomes evident that the comparative results demonstrate the degree to which bidirectional attention mechanisms concentrate on the provided context.
% It can be hypothesized that the potential of seq2seq models in few-shot tasks may be maximized through independent attentions of demonstrations.
% Consequently, the bidirectional attentions of concatenated sequences might lead to an overwhelming extraction of semantics (significantly surpassing the format of the given task) from the context, thereby introducing biases.}

% 피규어 3에서, 우리는 다양한 T5-family 모델들에 대해 우리의 method를 적용해 샷 수에 따라 달라지는 추세를 관찰한다. 우리는 모델에 관계없이 높은 샷 수에서 original 방법보다 우리의 방법의 점수가 일관되게 더 높은 점을 관찰한다. 이것은 독립적으로 attended setting이 여러개의 example이 있는 상황에서 유리함을 나타낸다. 또한 이 결과는 네 가지 모델이 모두 다르게 학습된 점을 고려했을 때, 우리의 방법이 기존에 학습된 방식에 관계없이 seq2seq model에 일관되게 적용 가능하다는 점을 뒷밤침한다. 
% 흥미롭게도, 태스크별로 그 정도의 차이는 있지만, 평균적으로 FiD가 RAG보다 성능이 높은 모습을 관찰한다. rag저자가 제안한 것과 달리, 우리의 rag-style few-shot은 retrieve 과정에서의 가중치를 활용할 수 없어 가중합을 생략한다. 하지만 fid는 이러한 retrieved score가 필요없는 것이, cross-attention 과정에서 알아서 자신이 주의를 기울일 부분을 선택하도록 설계되었기 때문이다. 따라서 rag는 자신이 상대적으로 더 주의를 기울일 예제를 선택할 수 없지만, fid는 가능하다는 점이 높은 성능을 이끌어낸 주요한 원원인이라고 우리는 판단한다.
% In Figure~\ref{fig: mean and std of models}, we demonstrate the effectiveness of our methods on various T5-family models and observe a trend that varies with the number of shots. 
In Figure~\ref{fig: mean and std of models}, we examine the in-context learning abilities of different seq2seq baseline models and observe a trend that varies with the number of shots.
Our approach demonstrates its effectiveness by consistently improving the performance when applied to all models.
The black lines, which represent the \textit{original} method, predominantly exhibit a decline in score in the few-shot setting, when compared to zero-shot or one-shot inference.
% The T0 model, which was instruct-tuned for the zero-shot tasks, shows the highest zero-shot learning score, while the T5 model exhibits the lowest performance. 
% However, it is observed that the few-shot performance of seq2seq models mostly converges to similarly low scores.
In contrast, the results obtained by applying our approach show a completely different tendency.
The red and green lines, representing the application of \textit{early-fusion} and \textit{late-fusion}, respectively, exhibit an upward curve, indicating that the few-shot learning performance outperforms the 1-shot performance.
The only exception is UL2 where the few-shot performance remains equal to the 1-shot performance.
% The green and red lines, representing the application of \textit{early-fusion} and \textit{late-fusion}, respectively, exhibit an upward curve, indicating that the few-shot learning performance outperforms the 1-shot performance for all four cases.
% Irrespective of the model, our proposed methods outperform the \textit{original} approach consistently when 5 or more examples are given. 
Surprisingly, despite T0 being fine-tuned with zero-shot prompts, both of our approaches result in higher performance with the GPT-3 best-shot setting compared to the zero-shot result of the original T0 model.
Considering that all four models were trained differently, this result reinforces the general applicability of our approach to seq2seq models, irrespective of how they were previously trained.

Additionally, we also observe that \textit{early-fusion} consistently outperforms \textit{late-fusion} by a small margin.
The \textit{late-fusion} approach does not perform a weighted sum for few-shot examples, whereas the \textit{early-fusion} is designed to selectively attend to specific information during the cross-attention process.
Thus, we posit that the capability of the \textit{early-fusion} to selectively prioritize certain examples, an ability absent in the \textit{late-fusion}, is a major factor contributing to its superior performance.
For detailed scores and standard deviations for each setting, please refer to Appendix~\ref{app: detailed results for fig3}.

% \subsection{Robustness regardless of the prompts}
% \label{subsec: t0 findings}
% In this analysis, we evaluate our method on the PromptSource dataset, in order to demonstrate its robustness on a diverse range of datasets. We compare the result based on the pretrained weight of T0, since PromptSource, consists of multiple prompts with diverse wording, is introduced by T0 paper and all the experiments are designed using this dataset.
% Note that T0 is further fine-tuned in supervised manner to facilitate zero-shot task generalization. The results, described in Table~\ref{tab: task taxonomy}, indicate that our model performs well on the PromptSource dataset and is even comparable to zero-shot results.
% Our 5-shot and 10-shot results consistently outperform the one-shot results.
% Moreover, although T0 is adapted to zero-shot performance, our model surprisingly achieves superior performance on half of the task taxonomy, specifically in the domains of coreference resolution and word sense.

% Figure environment removed

\subsection{Validation of our approach in the generation tasks}
So far, we have validated our method across a range of natural language understanding tasks. 
In this subsection, we verify the robustness of our method by evaluating it on several generation tasks.
As mentioned in Section~\ref{sec: introduction}, generation tasks, such as summarization and translation, align well with the objective of the encoder-decoder architecture and have been proven to be successful with seq2seq models.
However, there is still a lack of organized evaluation of the few-shot learning ability on those tasks. 
We thus further evaluate our model on XSum and WebNLG datasets, in both one-shot and few-shot learning settings.
To the best of our knowledge, this is the first work that explores the results of few-shot learning for seq2seq models on the XSum dataset.
% For instance, UL2 and AlexaTM~\cite{alexaTM} report the results of in-context learning for the XSum dataset, but only for one-shot setting.
% \citet{sap} provide in-context learning results ranging from zero-shot to two-shot, but mostly limited to the translation task.
% Thus, we extend the evaluation of our model to the generation tasks including XSum and WebNLG dataset, with both one-shot and few-shot learning settings.
% As verified in Section~\ref{subsec: better than gpt} that the application of \textit{RAG}- or \textit{FiD}-style to encoder-decoder models during few-shot NLU inference has significant advantages, we further test on \textit{XSum} task.

The application of objective-aligned prompting and \textit{early-fusion} method shows a remarkable improvement in the summarization task compared to the original T5 model, as shown in Table~\ref{tab: xsum results}.
A similar trend holds in WebNLG, a data-to-text task, as shown in Table~\ref{tab: webnlg results}.
This implies that our proposed approach not only serves as a robust few-shot learner for understanding tasks but also for generation tasks.

% opt 빼기로
% OPT-13B & 28.37/9.93/22.46 & \textbf{31.32}/11.52/24.72 \\
\begin{table}
\setlength\tabcolsep{8pt}
\begin{center}
\begin{adjustbox}{width=1\columnwidth}{
\begin{tabular}{lcc}
\toprule
\multirow{2}{*}{Model} & 1-shot & 5-shot \\
& (R1/R2/RL) & (R1/R2/RL) \\
\midrule
T5* & 13.72/2.46/11.97 & 7.57/0.66/6.34 \\
T5 & 25.12/8.69/20.72 & 26.39/8.99/21.59 \\
T5-\textit{early} & - & \textbf{30.31}/\textbf{11.55}/\textbf{25.10} \\
\bottomrule
\end{tabular}} 
\end{adjustbox}
\caption{\textbf{Evaluation on XSum dataset.} The asterisk(*) on the right side of the T5 denotes the case where the sentinel tokens are not used during inference time. R1, R2, and RL denotes ROUGE-{1,2,L}, respectively.}
\label{tab: xsum results}
\end{center}
\vspace{-4mm}
\end{table}


\begin{table}
\setlength\tabcolsep{8pt}
\begin{center}
\begin{adjustbox}{width=1\columnwidth}{
\begin{tabular}{lcc}
\toprule
\multirow{2}{*}{Model} & 1-shot & 32-shot \\
& (R1/R2/RL) & (R1/R2/RL) \\
\midrule
T5* & 18.63/7.98/16.28 & 0.25/0.13/0.24 \\
T5 & 39.13/23.44/32.61 & 41.40/22.63/34.09 \\
T5-\textit{early} & - & \textbf{49.47}/\textbf{29.16}/\textbf{40.84} \\
\bottomrule
\end{tabular}} 
\end{adjustbox}
\caption{\textbf{Evaluation on WebNLG dataset.} The asterisk(*) on the right side of the T5 denotes the case where the sentinel tokens are not used during inference time. R1, R2, and RL denotes ROUGE-{1,2,L}, respectively.}
\label{tab: webnlg results}
\end{center}
\vspace{-4.5mm}
\end{table}


\subsection{Analysis of the permutation bias}
\label{subsec: rag robustness}
One challenge of in-context few-shot learning is the potential variation in model predictions due to the order in which demonstrations are fed to the model. 
% The strength of the \textit{late-fusion} approach lies in its example-order agnosticism, as it is not affected by the order of the examples at all.
The \textit{late-fusion} approach eliminates the so-called permutation bias problem, as it operates by fusing the probabilities of decoder outputs regardless of the order in which the inputs are fed.
To verify the effectiveness of our methods in reducing permutation bias, we conduct experiments by reordering the few-shot examples.
We select four tasks, CB, COPA, WSC, and WiC, from each of the four task taxonomies introduced by \cite{t0}: natural language inference, sentence completion, coreference resolution, and word sense, to cover a diverse range of task types.
%4개의 task를 사용했고, t0에 나오는 task taxonomy의 서로 다른 카테고리에서 선별하여 diverse한 task에 대해 실험하고자 했다.
We examine the permutation bias for a 5-shot setting and randomly sample 50 test sets from each task using the same random seed.
In Table~\ref{tab: permutation bias}, we report the average and standard deviation for all 120 possible permutations with 5-shot examples for each task.
%each task에 대해서 설명.
% The strength of the \textit{RAG-style} approach lies in its example-order agnosticism, which is not affected by the order of the examples at all.
% Surprisingly, \textit{FiD-style} is also stable, with lower standard deviations on all tasks than the \textit{original} method, thanks to the elimination of order relationships in the encoder.  
% By implementing the FiD-style method instead of the original method, the standard deviation derived from 120 permuted demonstrations declined by approximately 30\%, while the performance experiences an improvement of 12\%p.
Surprisingly, both the \textit{early-fusion} and \textit{late-fusion} methods demonstrate zero standard deviations for all tasks, thanks to the elimination of order relation on the encoder side. 
In the \textit{early-fusion} method, there is a subtle variation in probability caused by the relative positional bias when the order of the examples is fused.
However, this slight difference does not have any impact on the metric score.
In contrast, the decoder-only baseline and the \textit{original} encoder-decoder baseline show higher standard deviations with respect to the permutation of examples.
% By implementing the \textit{RAG-style} or \textit{FiD-style} method instead of the original method, the standard deviation derived from 120 permuted demonstrations are all removed, while the performance experiences an improvement of up to 7.4\%p.
% The enhancement is even more pronounced, as it not only eradicates the standard deviation of OPT's bias, but also results in a substantial increase of up to 15.8\%p in the score.

\begin{table}
\begin{center}
\begin{adjustbox}{width=1\columnwidth}{
\begin{tabular}{lcccc}
\toprule
 & OPT-13B & T5-\textit{original} & T5-\textit{early} & T5-\textit{late}\\
\midrule
average & 52.16 & 60.57 & \textbf{68.00} & 67.50 \\
std & 2.02 & 4.51 & \textbf{0.00} & \textbf{0.00} \\
\bottomrule
\end{tabular}}
\end{adjustbox}
\caption{\textbf{Evaluate the permutation bias for each method.}
% We select a single task from each of the four task taxonomies introduced by \cite{t0} and examine the permutation bias for 5-shot inference.
% We randomly sample 50 test cases from each task using the same random seed. 
We experiment on an identical set of 5-shot demonstrations, with only a different order.
Bold represents highest score for the average and lowest value for the standard deviation.
}
\label{tab: permutation bias}
\end{center}
\vspace{-6mm}
\end{table}


% \begin{table}
% \setlength\tabcolsep{2.3pt}
% \begin{center}
% \begin{adjustbox}{width=1\columnwidth}{
% \begin{tabular}{lcccc}
% \toprule
% Task Taxonomy & 0-shot & 1-shot & 5-shot & 10-shot\\
% \midrule
% Natural Language Inference & \textbf{51.80} & 47.37 & 49.16 & 49.63\\
% Coreference Resolution & 50.70 & 55.10 & 55.87 & \textbf{56.12}\\
% Sentence Completion & \textbf{85.78} & 84.50 & 84.42 & 84.79\\
% Word Sense & 41.82 & 41.21 & 42.12 & \textbf{42.66}\\
% \bottomrule
% \end{tabular}}
% \end{adjustbox}
% \caption{\textbf{Evaluate on PromptSource dataset with T0 backbone.} The four held-out tasks are categorized by the T0 paper. Bold denotes the best score for each taxonomy. Detailed tasks included are described in appendix.}
% \label{tab: task taxonomy}
% \end{center}
% \vspace{-2mm}
% \end{table}


% \begin{table}
% \setlength\tabcolsep{2.3pt}
% \begin{center}
% \begin{adjustbox}{width=1\columnwidth}{
% \begin{tabular}{lccccc}
% \toprule
% \multirow{2}{*}{Task Taxonomy} & 0-shot  & 0-shot & 1-shot & 5-shot & 10-shot \\
% & (w/o sentinel) & (w/ sentinel) & (w/ sentinel) & (w/ sentinel) & (w/ sentinel) \\
% \midrule
% Natural Language Inference & 54.84 & \textbf{51.80} & 46.67 & 49.16 & 49.63\\
% Coreference Resolution & 60.00 & 50.71 & 55.09 & 55.87 & \textbf{56.12}\\
% Sentence Completion & 71.36 & \textbf{67.37} & 65.74 & 66.28 & 66.85\\
% Word Sense & 56.16 & 53.90 & 52.50 & 54.25 & \textbf{54.37}\\
% \bottomrule
% \end{tabular}} 
% \end{adjustbox}
% \caption{\textbf{Evaluate on PromptSource dataset with T0 backbone.} The four held-out tasks are categorized by the T0 paper. Bold denotes the best score for each taxonomy. Detailed tasks included are described in appendix.}
% \label{tab: task taxonomy}
% \end{center}
% \vspace{-2mm}
% \end{table}


% \begin{table}
% \begin{center}
% \begin{adjustbox}{width=1\columnwidth}{
% \begin{tabular}{lcccc}
% \toprule
%  & T5-Original & T5-FiD & T5-RAG\\
% \midrule
% Permutation bias & 41.82 & 41.21 & 42.12 & \textbf{42.66}\\
% \bottomrule
% \end{tabular}}
% \end{adjustbox}
% \caption{permutation bias 비교}
% \label{tab: task taxonomy}
% \end{center}
% \end{table}

% \begin{table}
% % \vspace{-0.3cm}
% \setlength\tabcolsep{2.3pt}
% \begin{center}
% \scalebox{0.8}{
% \begin{tabular}{lccccc}
% \toprule
% Model & 0-shot & 1-shot & 5-shot & 10-shot & GPT-3 best \\
% \midrule
% UL2 original & 0.5009 & 0.5359 & 0.5183 & 0.5126 & - \\
% UL2 FiD & 0.5009 & 0.5359 & 0.5359 & 0.5357 & - \\
% UL2 Rag-seq & 0.5009 & 0.5359 & 0.5253 & 0.5263 & - \\
% UL2 Rag-tok & 0.5009 & 0.5359 & 0.5267 & 0.5264 & - \\
% \midrule
% T5-LM original & 0.5037 & 0.5272 & 0.5117 & 0.5086 & - \\
% T5-LM FiD & 0.5037 & 0.5272 & 0.5364 & 0.5312 & - \\
% T5-LM Rag-seq & 0.5037 & 0.5272 & 0.5276 & 0.5268 & 0.5301 \\
% T5-LM Rag-tok & 0.5037 & 0.5272 & 0.5249 & 0.5232 & 0.5233 \\
% \midrule
% T0 original & 0.6068 & 0.5686 & 0.5800 & 0.5427 & - \\
% T0 FiD & 0.6068 & 0.5686 & 0.6100 & 0.6015 & - \\
% T0 Rag-seq & 0.6068 & 0.5686 & 0.5970 & 0.5948 & - \\
% T0 Rag-tok & 0.6068 & 0.5686 & 0.6024 & 0.5982 & - \\
% \bottomrule
% \end{tabular}}
% \caption{our best model(3종 중1)이 t5, t0, t5-lm, ul2에 대해 모두 성능 향상을 보임을 나타내는 표 (일단 3가지 다 가져왔습니다)}
% \label{tab: optimal-prompt}
% \end{center}
% \end{table}

% 모델에 따른 각 방법론의 성능과 강건함 비교 as the number of shots increased. table3와 동일한 태스크를 사용해 실험했으며, 그 결과들의 평균값을 보고한다.


%sentinel w/, w/o을 여기선 더이상 언급 필요없을지 추후 check
% \gina{Table 1: opt모델, t5계열 우리 모델 성능 비교 with average minimal template (전체는 어펜딕스 언급)}

% \gina{Table 2: our best model(3종 중1)이 t5, t0, t5-lm, ul2에 대해 모두 성능 향상을 보임을 나타내는 표 or 그래프. 이때 각 enc-dec 모델의 pretrain/finetuning objective에 따라 결과 추이가 어떻게 다른지 발견들 서술하면 좋을듯.}

% \gina{Table 3: t0 paper와의 비교 및 다른 template에서도 된다는걸 보여주기 위해 promptsource template에 대한 t0+fid/rag 결과 report. zero-shot fitted finetuning임에도 shot이 올라갈때 ability가 있다는 발견 설명하기}

% \gina{Table 4: fid와 rag가 table 1~3 세팅에서 성능이 비슷하지만, permutation에 rag는 independent해서 robust하다는 점 보이는 표}
% \section{Seq2seq LMs are also few-shot learners}
% \paragraph{Fully attended demonstrations} 
% \wavy{adversely affect the few-shot performance of seq2seq models, especially as the number of demonstrations increases, as verified in \textbf{Table 0}.
% Along with the results shown in~\citet{ul2, alexaTM}, our experiments also show that more than one fully-attended demonstrations unintendedly have harmful effect on performance of the seq2seq models.} 

% \paragraph{Independently attended demonstrations}
% \wavy{as opposed to the fully attended setting, does not sacrifice the performance with increase in the number of demonstrations, and even show higher gain compared to the large decoder LMs~\cite{opt} in \textbf{N}-shot setting. (수치 \% row 하나 잡아서 비교하기)
% Surprisingly, T5~\cite{t5} in FiD-style showed consistent increment in scores as the number of demonstrations increases, which are even hardly shown from decoder LLMs and not even from other seq2seq models~\cite{t5-lm, ul2, t0}.
% This tendency hold true for all the independently attended settings through FiD and RAG.}
% \wavy{We inspected that these comparative results discover to which bidirectional attentions aggregate from the given context. 
% Assuming that the capability of seq2seq models in few-shot tasks can be maximized by independent attentions of the demonstrations, the bidirectional attentions of concatenated sequences might yield the semantics (far beyond the format of given tasks) from the context, which cause biases.}

% \section{Alignment with Pre-training Objective}
% \paragraph{Mask tokens used in pre-training}
% \wavy{play a critical role in terms that they indicate where to be denoised from the given noised context.
% ~\citet{t5} used a number of sentinel tokens as a mask tokens to apply denoising objective while pre-training.
% In consequence, as we hypothesized with intuition, using sentinel tokens during inference time showed dramatic performance gain as verified in \textbf{Table 0}.}

% \wavy{Notably, the variants of T5~\cite{t5} we experimented~\cite{t5-lm, t0} which are further trained models from T5 without using sentinel tokens, even show performance gain using sentinel tokens.
% This might suggest that initial or long-lasted pre-training objective is the most crucial element in injecting or aggregating capability of seq2seq models.
% In fact, T5-LM~\cite{t5-lm} never surpass T5 in both with and without sentinel token setting.
% Aside from the training objective, this degradation from the additional training might be furthered investigated in future works.}


\section{Related Work}
% \paragraph{Pretrained encoder-decoder models}
% Generative language models that following the Transformer architecture~\cite{transformer} can be broadly classified as either decoder-only or encoder-decoder models.
From UniLM~\cite{unilm} to BART~\cite{bart}, T5, UL2, and AlexaTM~\cite{alexaTM}, encoder-decoder architecture models have continuously advanced and set new records by introducing novel denoising pretraining objectives.
% First introduced by \citet{transformer}, transformer-based encoder-decoder architecture models have continuously advanced by discovering novel pretraining objectives~\cite{t5, bart} or mixing various objectives during pretraining phase~\cite{unilm, ul2, alexaTM} maximally exploiting their own structural aspects.
They excelled in tasks that require a comprehensive understanding of the input context and generate output based on it, such as translation, summarization, and other sequence-to-sequence tasks.
However, this was particularly evident during the fine-tuning process.
The seq2seq models have had limitations in handling sequences long enough, and it was only after the T5 series of papers that they were able to address this to some extent by incorporating relative position encoding.
As a result, in-context few-shot learning, which necessitates a long encoder input length, has received relatively less attention.
As a consequence, despite the recent explosive growth of LLMs, the size of encoder-decoder models has remained stagnant, with less than 20 billion parameters~\cite{ul2, alexaTM}.
This stands in contrast to decoder-only models, which have reached the scale of hundreds of billions~\cite{gpt3, palm, glm130b}.
% \citet{bart} presented various denoising pretraining objectives including infilling, random deletion and sentence deshuffling. 
% \citet{t5} explored the BERT-like objective capable to seq2seq models called denoising method around the same time.
% While the number of parameters of PLM had significant advance for the past few years, dominant pre-training objectives of each architectures still remain similar~\cite{palm, glm, alexaTM}.
% Presuming this phase remain valid for the next few years, our aim was to maximally derive the capability of seq2seq models pretrained on denoising objectives.
% Besides, in general, there were only a few studies of probing model's ability except for decoder LLMs even though there were some superiority of seq2seq LLMs over decoder reported by~\citet{t0, alexaTM, ul2, flan2022}.
% As a consequence, we hope our work be a milestone of journey to figure out the optimal architecture and leveraging approaches of LLMs capabilities.

% \paragraph{In-context learning}
In-context learning has emerged as an alternative to the fine-tuning paradigm~\citep{bert, gpt1}, which incurs expensive training costs, especially in the case of very large language models~\citep{gpt3}.
According to \citet{gpt3}, pretrained decoder-only LLMs achieve proficiency in various tasks simply by prompting a few examples as input, without requiring any parameter updating.
Unlike the many advancements of large decoder-only models~\cite{emergent, gopher, hyperclova, opt, llama, structured}, the mainstream approach for encoder-decoder LLMs to adapt to a specific task remained supervised instruction-tuning~\citep{t0, flan, flan2022}.
Recently, a few studies attempted to explore in-context learning; UL2 and AlexaTM reported zero-shot results on the SuperGLUE dataset, and T0 utilized multitask prompted training to enhance zero-shot performance.
Some of the studies employed techniques that emulate decoder-only models.
\citet{sap} utilized decoder-only-style sequential autoregressive prompting.
\citet{ul2} mixed causal language modeling with denoising objectives.
% and T5-LM further pretrained the model with LM objective.
However, these approaches are restricted to particular setups and there has been a lack of structured results regarding few-shot learning.
% There is a lack of well-established frameworks and limited research on prompt engineering, for in-context learning,.
%framework, prompt tuning등도 아직 알려진 최적값이 없다.
% in-context learning ability has been shown to increase progressively with scale and 
% Ever since the study of \citet{gpt3}, tons of studies to derive decoder LLMs~\cite{emergent, gopher, hyperclova, opt, structured} whereas only scarce number of studies for seq2seq LLMs.
% ul2, alexaTM가 superglue 제로샷을 언급했고, 제로샷과 원샷에 대해서는 ~~~가 얘기했다.
%몇 논문들은 기존의 denoising objective에 causal language odeling을 섞는다던지, lm adapt되게 further pre-training을 한다던지 등의 방법으로 dec-only 모델과 유사하게 학습해 icl성능을 높이고자 했다.
%그러나 여전히 few-shot에 대한 제대로된 실험은 없다.
% Recently, a few studies have attempted to improve the in-context learning ability~\citep{ul2, t0, alexaTM, sap}; T0 perform zero-shot adaptation with multitask prompted tuning, and SAP~\cite{sap} enable decoder-only-style prompting by sequential autoregressive prompting and demonstrate its ability on translation and summarization tasks. However, both approaches are restricted to particular setups.
% However, what both of them reported were mostly confined to zero- or one-shot performance, and failed to explain a reason for the degradation of performance as the number of demonstrations grows.
% We propose unified in-context learning framework for seq2seq models and demonstrate better performance than decoder-only models for general natural language understanding tasks.
% And what we proposed with comprehensive studies for few-shot learning of seq2seq LLMs might pave the way to escalate the upper boundary of seq2seq models.
% Here, we showed that with current methods dominantly used for few-shot learning of seq2seq models, unable to reach to the upper bound of model's capability.

% \paragraph{Retrieval augmented generative models}
% % multi retrieval로 가는 과정 설명 -> few-shot에 대응됨 설명 -> alexatm 언급
% % Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge.
% Passage retrieval technique is an active area of research to improve QA systems.
% \citet{rag} presented general-purpose fine-tuning recipe for retrieval-augmented generation models, which combine parametric (i.e., pre-trained seq2seq model as a generator) and non-parametric memory (i.e., retriever consist of query encoder and document index) for language generation.
% \citet{fid} differs from \citet{sap} by how the generator processes the retrieved passages.
% % \citet{rag, fid} presented open-retrieval question-answering (ORQA) system where 
% % documents relevant to input query are retrieved from dual-encoders, and generating answers as a decoder output after while utilizing documents as encoder inputs. 
% In those systems, encoder and decoder in generative model play different roles in that encoder collates helpful additional information from multiple relevant documents and bind them into vector spaces, while decoder generates answers from input query leveraging encoder-generated keys and values. 
% Given this perspective, we apply the retrieval augmented generator directly to the few-shot learning scheme of seq2seq models, and valid the idea through ablation experiments.
% We find that \citet{alexaTM} endeavored to apply FiD idea to their approach, but hardly explain the strength of proposed method both in empirical and theoretical manner. 
% % and the experiments are strictly constrained to a single generation task and .
% % Despite~\citet{alexaTM} report the experimental results using the \textit{FiD-style} method in their table, they did not describe the experimental settings in detail and it is not known how much the score improved compared to the original methodology.


\section{Discussion and Conclusion}
\label{sec: discussion&conclusion}
In this work, we demonstrate that the seq2seq model with proper adaptation enables few-shot learning across a broad range of tasks.
To the best of our knowledge, no previous research has fully exploited the potential of seq2seq models as robust few-shot learners, outperforming decoder-only models, particularly on language understanding tasks.
Our remarkable results are attributed to two factors: 1) the alignment of prompts with pretraining objectives and 2) the use of fusion-based methods that independently process examples, compensating for the objective incompatibility and structural shortcomings of seq2seq models, respectively.
Through the carefully designed experiments, we verify that our approach exhibits a significant performance advantage under diverse conditions, including varying the number of shots, different baseline models, and a range of tasks, even with the complete removal of permutation bias.
Another major contribution of our work is the provision of a unified in-context learning evaluation toolkit for seq2seq models.
We have conducted controlled experiments to systematically compare and analyze the ability of existing seq2seq models --- a process not previously undertaken.
We plan to release the toolkit.
Importantly, our findings rediscover the strengths of the encoder-decoder architectures and shed new light on their potential as conversational agents (e.g., GPT-4~\cite{gpt4}), which have been underestimated.
We believe that the evolution of seq2seq models is still in progress, and our approach offers a substantial contribution toward maximizing their in-context learning capability.
% Concurrent recent work \cite{flan2022} also empower the findings by unlocking the chain-of-thought capability of seq2seq models, even with 100 times fewer parameters compared to \citet{opt-iml}. 
% \paragraph{Would seq2seq models substitute GPT?}
% \wavy{Under the condition where demonstrations are individually attended, we found that seq2seq LMs are even superior to the decoder LLMs with larger sizes.
% Pre-trained with 512 sequence length~\cite{t5, t5-lm, ul2}, which is 2x or 4x shorter than~\citet{gpt3, opt}, seq2seq models exhibited competitive performance in terms of length extrapolation.
% Also, as~\citet{flan2022} unlocked the chain-of-thought capability of seq2seq models even with 100x fewer parameters compared to~\citet{opt-iml}, we believe that aggregation of seq2seq models' various capabilities is still in progress and that in-context few-shot learning capability can be yielded maximally by the proposed method in this paper.
% }
% \wavy{In the previous sections, we showed that few-shot performance of seq2seq models can be significantly incremented simply by doing inference in an analogous manner to the pre-training objective, along with our intuition. We also found that independently attended attentions of each demonstrations leverage language understanding ability of the models.}

\section{Limitations}

Apart from the reported results in this study, some discrepancies between benchmark performance and qualitative effectiveness still remain.
The benchmark score itself cannot reflect linguistic ability with respect to its naturalness in responses, which is recently represented by GPT-4.
Furthermore, even though we experiment on the baseline models with a similar size, there are still a few mismatches in terms of throughput, the composition of pretraining data, hyper-parameters, and the number of tokens used for pretraining. These differences are further discussed in Section~\ref{app: training details for baseline models}.
In addition, there is a limitation in that our framework has not been applied to the BART-like pretrained encoder-decoder models. 
This is due to the fact that the large publicly available seq2seq models, which exceed a size of 10B, are only from the T5-family.
% xsum 테이블과 같이 생성에서는 NLU 대비 월등하지 않은 점 추가하기?

% \section*{Acknowledgements}

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology, custom}
\bibliographystyle{acl_natbib}

\clearpage

\newlist{appendixenum}{enumerate}{1}
\setlist[appendixenum]{label=\Alph*.}

% \titleformat{\section}
%   {\normalfont\Large\bfseries}
%   {\appendixenum{\thesection}}
%   {1em}
%   {}

\appendix
% \section*{Supplementary Material}
\section*{Appendix}
We complement our paper with additional experimental results and miscellaneous details throughout this material.
Section~\ref{app: detail table 1} provides the complete experimental results for Table~\ref{tab: io-structure} regarding the proper placement of the target input in the encoder-decoder model.
Section~\ref{app: detail table 2} exhibits the impact of objective-aligned prompting for in-context learning, which complements Table~\ref{tab: optimal-prompt}.
% In Section~\ref{app: detail for decoder vs. t5}, we reveal the evaluation scores of various seq2seq models that are not fully presented in Table~\ref{tab: compare decoder vs. t5}.
Moreover, we report the few-shot learning performance of other decoder-only LLMs that are not featured in the main paper, in Section~\ref{app: comparison with other LLMs}.
Section~\ref{app: detailed results for fig3} presents detailed numerical values utilized for reporting Figure~\ref{fig: mean and std of models}.
Section~\ref{app: training details for baseline models} offers detailed information regarding the baseline models used in our experiments.
Section~\ref{app: prompt and demons details}
Lastly, section~\ref{sec: normalization grounding} provides an explanation of how normalization is considered when measuring few-shot scores.


% 0. table 1,2에 대한 전체 테이블. zero, one, few를 포함한 전체 in-context learning에 대해 다 적용되는 것임을 언급 (sap의 table 4,5 형태 참고)
% (std, acc, f1 관련 상세 내용)
% 사용하지 않은 subset들 이유
\section{Detailed results for Table~\ref{tab: io-structure}}
\label{app: detail table 1}
% Table~\ref{tab: io-structure} verify that the placement of few-shot examples has great impact on performance of seq2seq models in high-level. Here, a detailed analysis is provided on whether it is advantageous to have few-shot examples in the encoder or the decoder, based on the specific task and the number of examples used for few-shot inference.
Table~\ref{tab: io-structure} verifies that the placement of a target input greatly impacts the in-context learning performance of seq2seq models.
Table~\ref{tab: full results for structure} provides detailed results of whether it is advantageous to have a target input in the encoder or the decoder, based on each task and each number of examples.
% In most cases, there is a significant difference in scores between the two placement scenarios, which supports the superiority of feeding the target question into the encoder.
In most cases, the target input given to the encoder produces significantly better results than that of the decoder.
Although there are a few instances where the scores are improved when the input is given to the decoder for specific combinations of models and tasks (e.g., UL2 with RTE task, T5-LM with WSC task), these are small exceptions that make it difficult to identify trends.
% , which supports the notion that providing the target question to the encoder is preferable.

\section{Detailed results for Table~\ref{tab: optimal-prompt}}
\label{app: detail table 2}

In Table~\ref{tab: full results for sentinel}, we present the complete results that complement Table~\ref{tab: optimal-prompt}.
% Without using sentinel tokens, the scores for T5 are not constant even when the number of shots varies across five out of eight tasks. 
Without using sentinel tokens, T5 shows consistent scores across five out of eight tasks, regardless of the variation in the number of shots. However, this issue is resolved when sentinel tokens are used.
Since T0 is further trained with the zero-shot prompts, we can observe that without the use of sentinel tokens, the score is initially higher at zero-shot results but significantly decreases as the number of demonstrations increases.
On the other hand, when sentinel tokens are employed during inference, even though the highest scores are still observed in zero-shot scenarios, the performance is almost maintained as the number of demonstrations increases.
% These observations suggest that using sentinel tokens helps the seq2seq model understand the examples better in general few-shot setting.
Based on these observations, we conclude that aligning the prompt structure with the pretraining objective generally helps the seq2seq model better understand few-shot examples.
Note that the scores reported in Table~\ref{tab: full results for sentinel} are not the ones that applied our proposed methodologies like \textit{early-} or \textit{late-fusion}.
%테이블 2의 각 태스크 및 샷 수에 따른 성능의 변화를 관찰하기 위해 우리는 테이블 6를 리포트한다.
%센티넬 토큰을 사용하지 않는 경우, 8개 태스크 중 5개에서 샷 수가 변하더라도 T5의 점수가 일정했다. 그러나 센티넬 토큰을 사용하는 경우에 이 문제는 모두 해결된다. 
%T0의 경우 zero-shot setting에서 fine-tuning(further training) 됐기 때문에 0-shot에서는 sentinel이 없는 경우에 점수가 더 높은 모습을 보이지만, 이후 예제가 늘어갈수록 그 점수차가 역전되는 모습을 확인할 수 있다.
%우리의 above observations는 few-shot 상황에서 sentinel이 각 예제를 효과적으로 전달하는 데에 도움을 줌을 시사한다.
\FloatBarrier
\begin{table*}[!ht]
% \vspace{-10mm}
\setlength\tabcolsep{5pt}
\centering
\begin{adjustbox}{width=1\textwidth}{
\begin{tabular}{lcccccccccccc}
\toprule
Placement & Model & Shot & RTE & COPA & CB & WiC & WSC & BoolQ* & MultiRC* & RECORD* &
\textbf{average}\\
\midrule
\multicolumn{1}{l}{\multirow{13}{*}{{encoder}}} 
& \multicolumn{1}{l}{\multirow{3}{*}{{T5}}} & 
\multicolumn{1}{c}{1} & 60.65 & 84.00 & 82.14 & 50.00 & 39.42 & 86.10 & 43.00 & 78.90 & \textbf{65.53} \\
& & \multicolumn{1}{c}{5} & 52.71 & 84.00 & 67.86 & 50.00 & 38.46 & 61.50 & 42.70 & 79.10 & \textbf{59.54} \\
& & \multicolumn{1}{c}{10} & 52.71 & 84.00 & 51.79 & 54.08 & 36.54 & 61.50 & 56.60 & 76.50 & \textbf{59.09} \\
\cmidrule{2-12}
& \multicolumn{1}{l}{\multirow{3}{*}{{T5-LM}}} &
\multicolumn{1}{c}{1} & 64.02 & 83.00 & 52.57 & 50.00 & 36.54 & 74.30 & 46.20 & 85.50 & \textbf{61.72} \\
& & \multicolumn{1}{c}{5} & 47.29 & 78.00 & 57.14 & 50.00 & 44.23 & 61.50 & 46.90 & 85.10 & \textbf{58.77} \\
& & \multicolumn{1}{c}{10} & 47.29 & 79.00 & 53.57 & 50.00 & 60.58 & 61.50 & 43.60 & 82.40 & \textbf{59.74} \\
\cmidrule{2-12}
& \multicolumn{1}{l}{\multirow{3}{*}{{T0}}} & 
\multicolumn{1}{c}{1} & 74.73 & 82.00 & 58.93 & 50.16 & 35.58 & 74.30 & 58.20 & 80.90 & \textbf{64.35} \\
& & \multicolumn{1}{c}{5} & 71.84 & 80.00 & 69.64 & 56.11 & 52.88 & 70.10 & 57.60 & 81.40 & \textbf{67.45} \\
& & \multicolumn{1}{c}{10} & 47.29 & 82.00 & 51.79 & 56.90 & 64.42 & 70.40 & 57.20 & 77.40 & \textbf{63.42} \\
\cmidrule{2-12}
& \multicolumn{1}{l}{\multirow{3}{*}{{UL2}}} & 
\multicolumn{1}{c}{1} & 54.15 & 86.00 & 42.86 & 51.57 & 36.54 & 78.20 & 42.70 & 88.40 & \textbf{60.05} \\
& & \multicolumn{1}{c}{5} & 49.10 & 81.00 & 50.00 & 50.00 & 36.54 & 63.60 & 54.30 & 86.30 & \textbf{58.85} \\
& & \multicolumn{1}{c}{10} & 47.29 & 84.00 & 50.00 & 50.00 & 36.54 & 68.70 & 57.30 & 86.30 & \textbf{60.02} \\
\midrule
\multicolumn{1}{l}{\multirow{13}{*}{{decoder}}} 
& \multicolumn{1}{l}{\multirow{3}{*}{{T5}}} & 
\multicolumn{1}{c}{1} & 52.71 & 60.00 & 41.07 & 48.12 & 36.54 & 58.50 & 47.60 & 15.10 & 44.95 \\
& & \multicolumn{1}{c}{5} & 52.71 & 77.00 & 41.07 & 46.00 & 38.00 & 58.80 & 51.00 & 14.40 & 47.37 \\
& & \multicolumn{1}{c}{10} & 52.71 & 73.00 & 41.07 & 50.00 & 38.00 & 57.50 & 52.30 & 16.30 & 
 47.61 \\
\cmidrule{2-12}
& \multicolumn{1}{l}{\multirow{3}{*}{{T5-LM}}} &
\multicolumn{1}{c}{1} & 51.99 & 74.00 & 46.43 & 50.00 & 36.54 & 63.00 & 42.60 & 49.50 & 51.76 \\
& & \multicolumn{1}{c}{5} & 47.29 & 72.00 & 48.21 & 47.00 & 62.00 & 61.50 & 42.70 & 50.10 & 53.85 \\
& & \multicolumn{1}{c}{10} & 47.29 & 75.00 & 51.79 & 46.00 & 62.00 & 61.50 & 42.70 & 51.20 & 54.68 \\
\cmidrule{2-12}
& \multicolumn{1}{l}{\multirow{3}{*}{{T0}}} & 
\multicolumn{1}{c}{1} & 53.43 & 77.00 & 48.21 & 48.59 & 36.54 & 61.80 & 48.00 & 46.20 & 52.47 \\
& & \multicolumn{1}{c}{5} & 48.74 & 74.00 & 53.57 & 53.29 & 40.38 & 62.40 & 52.60 & 46.80 & 53.97 \\
& & \multicolumn{1}{c}{10} & 46.93 & 75.00 & 48.21 & 52.82 & 42.31 & 61.30 & 53.90 & 46.40 & 53.36 \\
\cmidrule{2-12}
& \multicolumn{1}{l}{\multirow{3}{*}{{UL2}}} & 
\multicolumn{1}{c}{1} & 62.09 & 61.00 & 12.50 & 50.00 & 41.35 & 63.40 & 43.30 & 78.00 & 51.46 \\
& & \multicolumn{1}{c}{5} & 58.12 & 76.00 & 50.00 & 50.00 & 36.54 & 61.50 & 44.60 & 82.50 & 57.41 \\
& & \multicolumn{1}{c}{10} & 53.07 & 80.00 & 64.29 & 50.00 & 36.54 & 61.40 & 45.90 & 81.90 & 59.14 \\
\bottomrule
\end{tabular}}
\end{adjustbox}
% \caption{opt모델, t5계열 우리 모델 성능 비교 with average minimal template}
\caption{\textbf{Detailed results for Table~\ref{tab: io-structure}.}
The asterisk(*) on the right side of the task indicates that due to the large size of the test dataset, evaluation is performed on a random sample of 1K instances from the test dataset. Bold denotes the best average score among different placements of the target input for each model and number of shots.}
\label{tab: full results for structure}
\end{table*}

% 혹시 다시 쓸 일 있을까 5seed 짜리는 주석 처리해두었습니다.
% \begin{table*}
% \setlength\tabcolsep{5pt}
% \centering
% \begin{adjustbox}{width=1\textwidth}{
% \begin{tabular}{lcccccccccccc}
% \toprule
% Prompt & Model & Shot & RTE & CB & WSC & COPA & WiC & Boolq* & Multirc* & ReCoRD* & 
% \textbf{average}\\
% \midrule
% \multicolumn{1}{l}{\multirow{17}{*}{{Vanilla}}} 
% & \multicolumn{1}{l}{\multirow{4}{*}{{T5}}} & 
% \multicolumn{1}{c}{0} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{41.07} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{59.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.84} & \multicolumn{1}{c}{42.56} & \multicolumn{1}{c}{70.92} & \multicolumn{1}{c}{51.83} \\
% & & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{40.36} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{73.80} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.84} & \multicolumn{1}{c}{42.56} & \multicolumn{1}{c}{73.66} & \multicolumn{1}{c}{53.93} \\
% & & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{40.71} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{66.60} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.84} & \multicolumn{1}{c}{42.56} & \multicolumn{1}{c}{69.30} & \multicolumn{1}{c}{52.53} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{41.07} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{64.40} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.84} & \multicolumn{1}{c}{42.56} & \multicolumn{1}{c}{66.36} & \multicolumn{1}{c}{51.93} \\
% \cmidrule{2-12}
% & \multicolumn{1}{l}{\multirow{4}{*}{{T5-LM}}} &
% \multicolumn{1}{c}{0} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{26.79} & \multicolumn{1}{c}{63.46} & \multicolumn{1}{c}{71.00} & \multicolumn{1}{c}{50.31} & \multicolumn{1}{c}{40.30} & \multicolumn{1}{c}{57.44} & \multicolumn{1}{c}{85.62} & \multicolumn{1}{c}{55.95} \\
% & & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{49.46} & \multicolumn{1}{c}{36.07} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{64.20} & \multicolumn{1}{c}{46.14} & \multicolumn{1}{c}{61.86} & \multicolumn{1}{c}{44.50} & \multicolumn{1}{c}{83.42} & \multicolumn{1}{c}{52.77} \\
% & & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{48.59} & \multicolumn{1}{c}{40.00} & \multicolumn{1}{c}{63.46} & \multicolumn{1}{c}{59.60} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.9} & \multicolumn{1}{c}{54.62} & \multicolumn{1}{c}{78.08} & \multicolumn{1}{c}{57.03} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{47.44} & \multicolumn{1}{c}{34.64} & \multicolumn{1}{c}{63.46} & \multicolumn{1}{c}{58.20} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.84} & \multicolumn{1}{c}{57.44} & \multicolumn{1}{c}{76.02} & \multicolumn{1}{c}{56.13} \\
% \cmidrule{2-12}
% & \multicolumn{1}{l}{\multirow{4}{*}{{T0}}} & 
% \multicolumn{1}{c}{0} & \multicolumn{1}{c}{83.75} & \multicolumn{1}{c}{76.79} & \multicolumn{1}{c}{73.08} & \multicolumn{1}{c}{76.00} & \multicolumn{1}{c}{50.47} & \multicolumn{1}{c}{72.46} & \multicolumn{1}{c}{73.70} & \multicolumn{1}{c}{79.54} & \multicolumn{1}{c}{73.22} \\
% & & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{61.01} & \multicolumn{1}{c}{60.36} & \multicolumn{1}{c}{66.15} & \multicolumn{1}{c}{77.40} & \multicolumn{1}{c}{50.72} & \multicolumn{1}{c}{68.18} & \multicolumn{1}{c}{63.26} & \multicolumn{1}{c}{80.50} & \multicolumn{1}{c}{65.95} \\
% & & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{47.29} & \multicolumn{1}{c}{57.50} & \multicolumn{1}{c}{63.85} & \multicolumn{1}{c}{72.40} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{52.02} & \multicolumn{1}{c}{57.46} & \multicolumn{1}{c}{78.34} & \multicolumn{1}{c}{59.86} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{47.29} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{63.46} & \multicolumn{1}{c}{69.20} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{38.40} & \multicolumn{1}{c}{57.44} & \multicolumn{1}{c}{76.42} & \multicolumn{1}{c}{56.53} \\
% \cmidrule{2-12}
% & \multicolumn{1}{l}{\multirow{4}{*}{{UL2}}} & 
% \multicolumn{1}{c}{0} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{21.43} & \multicolumn{1}{c}{35.58} & \multicolumn{1}{c}{71.00} & \multicolumn{1}{c}{48.75} & \multicolumn{1}{c}{60.76} & \multicolumn{1}{c}{43.50} & \multicolumn{1}{c}{84.80} & \multicolumn{1}{c}{52.31} \\
% & & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{8.93} & \multicolumn{1}{c}{36.92} & \multicolumn{1}{c}{65.8} & \multicolumn{1}{c}{49.94} & \multicolumn{1}{c}{61.02} & \multicolumn{1}{c}{44.28} & \multicolumn{1}{c}{85.18} & \multicolumn{1}{c}{50.60}  \\
% & & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{8.93} & \multicolumn{1}{c}{40.19} & \multicolumn{1}{c}{61.40} & \multicolumn{1}{c}{49.22} & \multicolumn{1}{c}{54.98} & \multicolumn{1}{c}{43.52} & \multicolumn{1}{c}{83.42} & \multicolumn{1}{c}{49.30} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{8.93} & \multicolumn{1}{c}{37.12} & \multicolumn{1}{c}{58.20} & \multicolumn{1}{c}{49.72} & \multicolumn{1}{c}{61.44} & \multicolumn{1}{c}{42.82} & \multicolumn{1}{c}{82.40} & \multicolumn{1}{c}{49.17} \\
% \midrule
% \multicolumn{1}{l}{\multirow{17}{*}{{w/ sentinel }}} 
% & \multicolumn{1}{l}{\multirow{4}{*}{{T5}}} & 
% \multicolumn{1}{c}{0} & \multicolumn{1}{c}{54.87} & \multicolumn{1}{c}{33.93} & \multicolumn{1}{c}{41.35} & \multicolumn{1}{c}{58.00} & \multicolumn{1}{c}{44.67} & \multicolumn{1}{c}{71.48} & \multicolumn{1}{c}{45.60} & \multicolumn{1}{c}{73.14} & \multicolumn{1}{c}{52.88} \\
% & & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{62.38} & \multicolumn{1}{c}{67.50} & \multicolumn{1}{c}{43.46} & \multicolumn{1}{c}{83.60} & \multicolumn{1}{c}{49.62} & \multicolumn{1}{c}{85.08} & \multicolumn{1}{c}{49.62} & \multicolumn{1}{c}{78.74} & \multicolumn{1}{c}{65.00} \\
% & & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{55.02} & \multicolumn{1}{c}{53.93} & \multicolumn{1}{c}{39.04} & \multicolumn{1}{c}{82.80} & \multicolumn{1}{c}{50.50} & \multicolumn{1}{c}{61.86} & \multicolumn{1}{c}{51.72} & \multicolumn{1}{c}{77.96} & \multicolumn{1}{c}{59.10} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{56.10} & \multicolumn{1}{c}{51.43} & \multicolumn{1}{c}{37.88} & \multicolumn{1}{c}{82.00} & \multicolumn{1}{c}{52.10} & \multicolumn{1}{c}{62.14} & \multicolumn{1}{c}{54.10} & \multicolumn{1}{c}{75.70} & \multicolumn{1}{c}{58.93} \\
% \cmidrule{2-12}
% & \multicolumn{1}{l}{\multirow{4}{*}{{T5-LM}}} &
% \multicolumn{1}{c}{0} & \multicolumn{1}{c}{62.45} & \multicolumn{1}{c}{48.21} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{71.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{75.16} & \multicolumn{1}{c}{46.62} & \multicolumn{1}{c}{86.12} & \multicolumn{1}{c}{59.51} \\
% & & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{56.68} & \multicolumn{1}{c}{57.86} & \multicolumn{1}{c}{41.92} & \multicolumn{1}{c}{81.20} & \multicolumn{1}{c}{50.38} & \multicolumn{1}{c}{72.92} & \multicolumn{1}{c}{47.14} & \multicolumn{1}{c}{86.02} & \multicolumn{1}{c}{61.76}  \\
% & & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{47.65} & \multicolumn{1}{c}{56.07} & \multicolumn{1}{c}{43.65} & \multicolumn{1}{c}{77.80} & \multicolumn{1}{c}{49.97} & \multicolumn{1}{c}{68.58} & \multicolumn{1}{c}{47.10} & \multicolumn{1}{c}{85.22} & \multicolumn{1}{c}{59.51} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{47.22} & \multicolumn{1}{c}{47.50} & \multicolumn{1}{c}{52.31} & \multicolumn{1}{c}{79.20} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.86} & \multicolumn{1}{c}{45.00} & \multicolumn{1}{c}{82.32} & \multicolumn{1}{c}{58.18} \\
% \cmidrule{2-12}
% & \multicolumn{1}{l}{\multirow{4}{*}{{T0}}} & 
% \multicolumn{1}{c}{0} & \multicolumn{1}{c}{81.95} & \multicolumn{1}{c}{78.57} & \multicolumn{1}{c}{53.85} & \multicolumn{1}{c}{82.00} & \multicolumn{1}{c}{55.64} & \multicolumn{1}{c}{80.06} & \multicolumn{1}{c}{65.02} & \multicolumn{1}{c}{80.14} & \multicolumn{1}{c}{72.15} \\
% & & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{74.08} & \multicolumn{1}{c}{72.50} & \multicolumn{1}{c}{41.73} & \multicolumn{1}{c}{81.20} & \multicolumn{1}{c}{51.76} & \multicolumn{1}{c}{75.50} & \multicolumn{1}{c}{57.54} & \multicolumn{1}{c}{80.80} & \multicolumn{1}{c}{66.89} \\
% & & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{67.94} & \multicolumn{1}{c}{76.07} & \multicolumn{1}{c}{60.19} & \multicolumn{1}{c}{81.60} & \multicolumn{1}{c}{54.20} & \multicolumn{1}{c}{72.14} & \multicolumn{1}{c}{57.56} & \multicolumn{1}{c}{79.16} & \multicolumn{1}{c}{68.61} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{53.57} & \multicolumn{1}{c}{60.36} & \multicolumn{1}{c}{62.88} & \multicolumn{1}{c}{81.60} & \multicolumn{1}{c}{52.98} & \multicolumn{1}{c}{66.86} & \multicolumn{1}{c}{57.62} & \multicolumn{1}{c}{77.56} & \multicolumn{1}{c}{64.18} \\
% \cmidrule{2-12}
% & \multicolumn{1}{l}{\multirow{4}{*}{{UL2}}} & 
% \multicolumn{1}{c}{0} & \multicolumn{1}{c}{51.62} & \multicolumn{1}{c}{33.93} & \multicolumn{1}{c}{53.85} & \multicolumn{1}{c}{69.00} & \multicolumn{1}{c}{51.41} & \multicolumn{1}{c}{74.96} & \multicolumn{1}{c}{48.90} & \multicolumn{1}{c}{84.96} & \multicolumn{1}{c}{58.58} \\
% & & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{54.87} & \multicolumn{1}{c}{47.50} & \multicolumn{1}{c}{38.65} & \multicolumn{1}{c}{82.80} & \multicolumn{1}{c}{50.34} & \multicolumn{1}{c}{75.84} & \multicolumn{1}{c}{48.94} & \multicolumn{1}{c}{87.38} & \multicolumn{1}{c}{60.79} \\
% & & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{48.01} & \multicolumn{1}{c}{49.64} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{82.80} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{70.98} & \multicolumn{1}{c}{56.74} & \multicolumn{1}{c}{86.48} & \multicolumn{1}{c}{60.15} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{47.44} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{82.20} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{66.18} & \multicolumn{1}{c}{57.48} & \multicolumn{1}{c}{85.42} & \multicolumn{1}{c}{59.41} \\
% \midrule
% \multicolumn{1}{l}{\multirow{4}{*}{{w/ mode}}} & 
% \multicolumn{1}{l}{\multirow{4}{*}{{UL2}}} & 
% \multicolumn{1}{c}{0} & \multicolumn{1}{c}{53.79} & \multicolumn{1}{c}{8.93} & \multicolumn{1}{c}{47.12} & \multicolumn{1}{c}{87.00} & \multicolumn{1}{c}{53.13} & \multicolumn{1}{c}{79.68} & \multicolumn{1}{c}{49.26} & \multicolumn{1}{c}{86.66} & \multicolumn{1}{c}{58.20} \\
% & & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{54.87} & \multicolumn{1}{c}{49.29} & \multicolumn{1}{c}{47.50} & \multicolumn{1}{c}{85.00} & \multicolumn{1}{c}{52.29} & \multicolumn{1}{c}{87.38} & \multicolumn{1}{c}{47.52} & \multicolumn{1}{c}{87.24} & \multicolumn{1}{c}{63.89} \\
% & & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{49.03} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{84.20} & \multicolumn{1}{c}{50.09} & \multicolumn{1}{c}{86.48} & \multicolumn{1}{c}{55.06} & \multicolumn{1}{c}{86.84} & \multicolumn{1}{c}{62.28} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{47.51} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{84.20} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{85.42} & \multicolumn{1}{c}{57.32} & \multicolumn{1}{c}{85.66} & \multicolumn{1}{c}{62.08} \\
% \bottomrule
% \end{tabular}}
% \end{adjustbox}
% % \caption{opt모델, t5계열 우리 모델 성능 비교 with average minimal template}
% \caption{\textbf{Detailed results for seq2seq baselines verified in Table~\ref{tab: optimal-prompt}.}
% Tasks marked with an asterisk(*) on the right-hand side indicate that the total sample size is too large, so that 1K random samples are used for evaluation purposes.}
% \label{tab: full results for sentinel}
% \end{table*}

\begin{table*}
\setlength\tabcolsep{5pt}
\centering
\begin{adjustbox}{width=1\textwidth}{
\begin{tabular}{lcccccccccccc}
\toprule
Prompt & Model & Shot & RTE & CB & WSC & COPA & WiC & Boolq* & Multirc* & ReCoRD* & 
\textbf{average}\\
\midrule
\multicolumn{1}{l}{\multirow{17}{*}{{vanilla}}} 
& \multicolumn{1}{l}{\multirow{4}{*}{{T5}}} & 
\multicolumn{1}{c}{0} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{41.07} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{59.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.50} & \multicolumn{1}{c}{42.70} & \multicolumn{1}{c}{70.10} & \multicolumn{1}{c}{51.70} \\
& & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{39.29} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{79.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.50} & \multicolumn{1}{c}{42.70} & \multicolumn{1}{c}{75.10} & \multicolumn{1}{c}{54.60} \\
& & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{41.07} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{61.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.50} & \multicolumn{1}{c}{42.70} & \multicolumn{1}{c}{71.90} & \multicolumn{1}{c}{52.18} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{41.07} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{63.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.50} & \multicolumn{1}{c}{42.70} & \multicolumn{1}{c}{69.20} & \multicolumn{1}{c}{52.09} \\
\cmidrule{2-12}
& \multicolumn{1}{l}{\multirow{4}{*}{{T5-LM}}} &
\multicolumn{1}{c}{0} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{26.79} & \multicolumn{1}{c}{63.46} & \multicolumn{1}{c}{71.00} & \multicolumn{1}{c}{50.31} & \multicolumn{1}{c}{40.80} & \multicolumn{1}{c}{57.30} & \multicolumn{1}{c}{85.60} & \multicolumn{1}{c}{56.00} \\
& & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{47.29} & \multicolumn{1}{c}{17.86} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{63.00} & \multicolumn{1}{c}{44.83} & \multicolumn{1}{c}{61.60} & \multicolumn{1}{c}{49.60} & \multicolumn{1}{c}{82.20} & \multicolumn{1}{c}{50.36} \\
& & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{46.57} & \multicolumn{1}{c}{35.71} & \multicolumn{1}{c}{63.46} & \multicolumn{1}{c}{60.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.50} & \multicolumn{1}{c}{57.40} & \multicolumn{1}{c}{78.20} & \multicolumn{1}{c}{56.61} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{47.29} & \multicolumn{1}{c}{41.07} & \multicolumn{1}{c}{63.46} & \multicolumn{1}{c}{60.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.50} & \multicolumn{1}{c}{57.30} & \multicolumn{1}{c}{74.50} & \multicolumn{1}{c}{56.89} \\
\cmidrule{2-12}
& \multicolumn{1}{l}{\multirow{4}{*}{{T0}}} & 
\multicolumn{1}{c}{0} & \multicolumn{1}{c}{83.75} & \multicolumn{1}{c}{76.79} & \multicolumn{1}{c}{73.08} & \multicolumn{1}{c}{76.00} & \multicolumn{1}{c}{50.47} & \multicolumn{1}{c}{73.00} & \multicolumn{1}{c}{72.70} & \multicolumn{1}{c}{80.60} & \multicolumn{1}{c}{\textbf{73.30}} \\
& & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{74.73} & \multicolumn{1}{c}{55.36} & \multicolumn{1}{c}{68.27} & \multicolumn{1}{c}{78.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{65.70} & \multicolumn{1}{c}{66.60} & \multicolumn{1}{c}{80.10} & \multicolumn{1}{c}{\textbf{67.34}} \\
& & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{47.29} & \multicolumn{1}{c}{53.57} & \multicolumn{1}{c}{63.46} & \multicolumn{1}{c}{75.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{56.80} & \multicolumn{1}{c}{57.30} & \multicolumn{1}{c}{79.50} & \multicolumn{1}{c}{60.37} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{47.29} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{63.46} & \multicolumn{1}{c}{70.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{38.50} & \multicolumn{1}{c}{57.30} & \multicolumn{1}{c}{77.60} & \multicolumn{1}{c}{56.77} \\
\cmidrule{2-12}
& \multicolumn{1}{l}{\multirow{4}{*}{{UL2}}} & 
\multicolumn{1}{c}{0} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{21.43} & \multicolumn{1}{c}{35.58} & \multicolumn{1}{c}{71.00} & \multicolumn{1}{c}{48.75} & \multicolumn{1}{c}{60.60} & \multicolumn{1}{c}{43.20} & \multicolumn{1}{c}{85.90} & \multicolumn{1}{c}{52.39} \\
& & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{8.93} & \multicolumn{1}{c}{37.50} & \multicolumn{1}{c}{64.00} & \multicolumn{1}{c}{49.84} & \multicolumn{1}{c}{60.10} & \multicolumn{1}{c}{42.70} & \multicolumn{1}{c}{85.00} & \multicolumn{1}{c}{50.10} \\
& & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{8.93} & \multicolumn{1}{c}{38.46} & \multicolumn{1}{c}{59.00} & \multicolumn{1}{c}{50.31} & \multicolumn{1}{c}{50.50} & \multicolumn{1}{c}{43.10} & \multicolumn{1}{c}{84.00} & \multicolumn{1}{c}{48.38} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{8.93} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{59.00} & \multicolumn{1}{c}{49.84} & \multicolumn{1}{c}{61.40} & \multicolumn{1}{c}{42.70} & \multicolumn{1}{c}{83.30} & \multicolumn{1}{c}{49.30} \\
\midrule
\multicolumn{1}{l}{\multirow{17}{*}{{w/ sentinel }}} 
& \multicolumn{1}{l}{\multirow{4}{*}{{T5}}} & 
\multicolumn{1}{c}{0} & \multicolumn{1}{c}{54.87} & \multicolumn{1}{c}{33.93} & \multicolumn{1}{c}{41.35} & \multicolumn{1}{c}{58.00} & \multicolumn{1}{c}{44.67} & \multicolumn{1}{c}{71.60} & \multicolumn{1}{c}{44.90} & \multicolumn{1}{c}{73.80} & \multicolumn{1}{c}{\textbf{52.89}} \\
& & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{60.65} & \multicolumn{1}{c}{82.14} & \multicolumn{1}{c}{39.42} & \multicolumn{1}{c}{84.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{86.10} & \multicolumn{1}{c}{43.00} & \multicolumn{1}{c}{78.90} & \multicolumn{1}{c}{\textbf{65.53}} \\
& & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{67.86} & \multicolumn{1}{c}{38.46} & \multicolumn{1}{c}{84.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.50} & \multicolumn{1}{c}{42.70} & \multicolumn{1}{c}{79.10} & \multicolumn{1}{c}{\textbf{59.54}} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{51.79} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{83.00} & \multicolumn{1}{c}{54.08} & \multicolumn{1}{c}{61.50} & \multicolumn{1}{c}{56.60} & \multicolumn{1}{c}{76.50} & \multicolumn{1}{c}{\textbf{59.09}} \\
\cmidrule{2-12}
& \multicolumn{1}{l}{\multirow{4}{*}{{T5-LM}}} &
\multicolumn{1}{c}{0} & \multicolumn{1}{c}{62.45} & \multicolumn{1}{c}{48.21} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{71.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{75.20} & \multicolumn{1}{c}{46.10} & \multicolumn{1}{c}{86.40} & \multicolumn{1}{c}{\textbf{59.49}} \\
& & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{64.62} & \multicolumn{1}{c}{53.57} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{83.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{74.30} & \multicolumn{1}{c}{46.20} & \multicolumn{1}{c}{85.50} & \multicolumn{1}{c}{\textbf{61.72}} \\
& & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{47.29} & \multicolumn{1}{c}{57.14} & \multicolumn{1}{c}{44.23} & \multicolumn{1}{c}{78.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.50} & \multicolumn{1}{c}{46.90} & \multicolumn{1}{c}{85.10} & \multicolumn{1}{c}{\textbf{58.77}} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{47.29} & \multicolumn{1}{c}{53.57} & \multicolumn{1}{c}{60.58} & \multicolumn{1}{c}{79.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.50} & \multicolumn{1}{c}{43.60} & \multicolumn{1}{c}{82.40} & \multicolumn{1}{c}{\textbf{59.74}} \\
\cmidrule{2-12}
& \multicolumn{1}{l}{\multirow{4}{*}{{T0}}} & 
\multicolumn{1}{c}{0} & \multicolumn{1}{c}{81.95} & \multicolumn{1}{c}{78.57} & \multicolumn{1}{c}{53.85} & \multicolumn{1}{c}{82.00} & \multicolumn{1}{c}{55.64} & \multicolumn{1}{c}{79.40} & \multicolumn{1}{c}{65.20} & \multicolumn{1}{c}{80.80} & \multicolumn{1}{c}{72.18} \\
& & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{74.73} & \multicolumn{1}{c}{58.93} & \multicolumn{1}{c}{35.58} & \multicolumn{1}{c}{82.00} & \multicolumn{1}{c}{50.16} & \multicolumn{1}{c}{74.30} & \multicolumn{1}{c}{58.20} & \multicolumn{1}{c}{80.90} & \multicolumn{1}{c}{64.35} \\
& & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{71.84} & \multicolumn{1}{c}{69.64} & \multicolumn{1}{c}{52.88} & \multicolumn{1}{c}{80.00} & \multicolumn{1}{c}{56.11} & \multicolumn{1}{c}{70.10} & \multicolumn{1}{c}{57.60} & \multicolumn{1}{c}{81.50} & \multicolumn{1}{c}{\textbf{67.46}} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{47.29} & \multicolumn{1}{c}{51.79} & \multicolumn{1}{c}{64.42} & \multicolumn{1}{c}{82.00} & \multicolumn{1}{c}{56.90} & \multicolumn{1}{c}{70.40} & \multicolumn{1}{c}{57.20} & \multicolumn{1}{c}{77.40} & \multicolumn{1}{c}{\textbf{63.42}} \\
\cmidrule{2-12}
& \multicolumn{1}{l}{\multirow{4}{*}{{UL2}}} & 
\multicolumn{1}{c}{0} & \multicolumn{1}{c}{51.62} & \multicolumn{1}{c}{33.93} & \multicolumn{1}{c}{53.85} & \multicolumn{1}{c}{69.00} & \multicolumn{1}{c}{51.41} & \multicolumn{1}{c}{76.60} & \multicolumn{1}{c}{49.70} & \multicolumn{1}{c}{84.60} & \multicolumn{1}{c}{\textbf{58.84}} \\
& & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{54.15} & \multicolumn{1}{c}{42.86} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{86.00} & \multicolumn{1}{c}{51.57} & \multicolumn{1}{c}{78.20} & \multicolumn{1}{c}{42.70} & \multicolumn{1}{c}{88.40} & \multicolumn{1}{c}{60.05} \\
& & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{49.10} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{81.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{63.60} & \multicolumn{1}{c}{54.30} & \multicolumn{1}{c}{86.30} & \multicolumn{1}{c}{58.85} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{47.29} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{84.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{68.70} & \multicolumn{1}{c}{57.30} & \multicolumn{1}{c}{86.30} & \multicolumn{1}{c}{60.02} \\
\midrule
\multicolumn{1}{l}{\multirow{4}{*}{{w/ mode}}} & 
\multicolumn{1}{l}{\multirow{4}{*}{{UL2}}} & 
\multicolumn{1}{c}{0} & \multicolumn{1}{c}{53.79} & \multicolumn{1}{c}{8.93} & \multicolumn{1}{c}{47.12} & \multicolumn{1}{c}{87.00} & \multicolumn{1}{c}{53.13} & \multicolumn{1}{c}{80.30} & \multicolumn{1}{c}{50.40} & \multicolumn{1}{c}{87.20} & \multicolumn{1}{c}{58.48} \\
& & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{48.01} & \multicolumn{1}{c}{44.64} & \multicolumn{1}{c}{46.15} & \multicolumn{1}{c}{86.00} & \multicolumn{1}{c}{54.70} & \multicolumn{1}{c}{88.40} & \multicolumn{1}{c}{42.90} & \multicolumn{1}{c}{86.90} & \multicolumn{1}{c}{\textbf{62.21}} \\
& & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{48.38} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{83.00} & \multicolumn{1}{c}{50.47} & \multicolumn{1}{c}{86.30} & \multicolumn{1}{c}{48.40} & \multicolumn{1}{c}{86.30} & \multicolumn{1}{c}{\textbf{61.17}} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{47.29} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{86.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{86.30} & \multicolumn{1}{c}{57.30} & \multicolumn{1}{c}{84.90} & \multicolumn{1}{c}{\textbf{62.29}} \\
\bottomrule
\end{tabular}}
\end{adjustbox}
% \caption{opt모델, t5계열 우리 모델 성능 비교 with average minimal template}
\caption{\textbf{Detailed results for Table~\ref{tab: optimal-prompt}.}
The asterisk(*) on the right side of the task indicates that due to the large size of the test dataset, evaluation is performed on a random sample of 1K instances from the test dataset. For \textit{w/ mode} setting, the sentinel token is also utilized. Bold denotes the best average score among \textit{vanilla}, \textit{w/ sentinel}, and \textit{w/ mode} approaches, for the same model and the same number of shots.}
\label{tab: full results for sentinel}
\end{table*}


% 1. table3에 대한 전체 baseline, method, shot 테이블
% \section{Additional results for Table~\ref{tab: compare decoder vs. t5}}
% \label{app: detail for decoder vs. t5}

% In order to thoroughly analyze the performance of seq2seq baselines, we report the additive results evaluated on T5, T5-LM, T0, and UL2 for Table~\ref{tab: compare decoder vs. t5} in Table~\ref{tab: full results for seq2seq t0 datasets}.
% Although we compare various decoder-only models with T5 (with our methodologies applied), other encoder-decoder models such as T5-LM and UL2 also achieve significant scores that approximate those of the OPT-30B model, which is much larger.
% % Further, despite it trained additionally with plenty of well-qualified prompts, T0 not only outperforms all the other baselines but also exhibits performance gain with the implementation of \textit{FiD} methodology.
% % Notably, Since T0 is trained with plenty of well-qualified 0-shot prompts, it outperforms other baselines in the 0-shot settings while its performance declines as the number of shots increases.
% % However, T0 also exhibits performance gain with the implementation of \textit{RAG-} and \textit{FiD-style} methodology.
% Surprisingly, for T0, although trained with 0-shot prompts, there were instances when applying our method resulted in higher performance than the 0-shot setting.

% % For the number of demonstrations indicated as gpt3 best-shot, which refers to the number of shots with the highest scores verified in GPT-3, we report each number of shots for tasks in Table~\ref{tab: gpt3 best shot}.
% % Therefore, we conduct experiments on the items labeled as gpt3 best-shot, following the corresponding number of shots for each task in Table~\ref{tab: gpt3 best shot}.
% In Table~\ref{tab: compare decoder vs. t5}, we present the number of demonstrations labeled as ``gpt3''. It refers to the highest-scoring shots verified in GPT-3. The corresponding numbers of shots are reported in Table~\ref{tab: gpt3 best shot}.

\begin{table*}
\setlength\tabcolsep{10pt}
\centering
\begin{adjustbox}{width=1\textwidth}
\begin{tabular}{lcccccccccccc}
\toprule
\multirow{1}[1]{*}{Task} & \multicolumn{1}{c}{RTE} & \multicolumn{1}{c}{CB} & \multicolumn{1}{c}{ANLI1} & \multicolumn{1}{c}{ANLI2} & \multicolumn{1}{c}{ANLI3} & \multicolumn{1}{c}{WSC} & \multicolumn{1}{c}{Winogrande} & \multicolumn{1}{c}{COPA} & \multicolumn{1}{c}{StoryCloze} & \multicolumn{1}{c}{HellaSwag} & \multicolumn{1}{c}{WiC}\\
\midrule
shots & 32 & 32 & 50 & 50 & 50 & 32 & 16 & 32 & 70 & 20 & 32\\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{\textbf{The number of shots with the highest score reported in GPT-3.} 
For each task, the length of concatenated examples approximates the sequence length of the pretrained GPT-3 model. 
Consequently, conducting few-shot learning with such settings is disadvantageous for seq2seq models, which are pretrained on shorter sequence lengths.
}
\label{tab: gpt3 best shot}
\end{table*}

% 웨이비가 볼드 치신 부분 쓸 수도 있으니 기록으로 남기겠습니다..!

% % \caption{opt모델, t5계열 우리 모델 성능 비교 with average minimal template}
% \caption{\textbf{Additional results for Table~\ref{tab: compare decoder vs. t5}.}
% The asterisk(*) on the right side of the task indicates that due to the large size of the test dataset, evaluation is performed on a random sample of 1K instances from the test dataset. 
% The underlined values represent the highest scores within each model for each task, while the bold values represent the settings where the average scores for all the tasks are highest within each model.}
% \label{tab: full results for seq2seq t0 datasets}
% \end{table*}

% \begin{table*}
% \setlength\tabcolsep{6.5pt}
% \centering
% \begin{adjustbox}{width=1\textwidth}{
% \begin{tabular}{lcccccccccccccccc}
% \toprule
% Model & Method & Shot & RTE & CB & ANLI1 & ANLI2 & ANLI3 & WSC & Winogrande & COPA & StoryCloze & HellaSwag* & WiC & 
% \textbf{average} & \textbf{std} \\
% \midrule
% \multicolumn{1}{l}{\multirow{12}{*}{{T5-LM}}} 
% & \multicolumn{1}{l}{\multirow{5}{*}{{Original}}} & 
% \multicolumn{1}{c}{0} & \multicolumn{1}{c}{62.45} & \multicolumn{1}{c}{48.21} & \multicolumn{1}{c}{37.70} & \multicolumn{1}{c}{34.90} & \multicolumn{1}{c}{37.25} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{55.09} & \multicolumn{1}{c}{71.00} & \multicolumn{1}{c}{72.85} & \multicolumn{1}{c}{48.02} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{50.37} & \multicolumn{1}{c}{0.05} \\
% & & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{56.68} & \multicolumn{1}{c}{57.86} & \multicolumn{1}{c}{34.96} & \multicolumn{1}{c}{34.40} & \multicolumn{1}{c}{35.87} & \multicolumn{1}{c}{41.92} & \multicolumn{1}{c}{61.15} & \multicolumn{1}{c}{81.20} & \multicolumn{1}{c}{74.83} & \multicolumn{1}{c}{50.70} & \multicolumn{1}{c}{50.38} & \multicolumn{1}{c}{52.72} & \multicolumn{1}{c}{3.28} \\
% & & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{47.65} & \multicolumn{1}{c}{56.07} & \multicolumn{1}{c}{32.66} & \multicolumn{1}{c}{34.06} & \multicolumn{1}{c}{33.97} & \multicolumn{1}{c}{43.65} & \multicolumn{1}{c}{61.29} & \multicolumn{1}{c}{77.80} & \multicolumn{1}{c}{75.34} & \multicolumn{1}{c}{50.38} & \multicolumn{1}{c}{49.97} & \multicolumn{1}{c}{51.17} & \multicolumn{1}{c}{1.48} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{47.22} & \multicolumn{1}{c}{47.50} & \multicolumn{1}{c}{32.12} & \multicolumn{1}{c}{32.96} & \multicolumn{1}{c}{33.07} & \multicolumn{1}{c}{52.31} & \multicolumn{1}{c}{61.42} & \multicolumn{1}{c}{79.20} & \multicolumn{1}{c}{74.47} & \multicolumn{1}{c}{49.18} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{50.86} & \multicolumn{1}{c}{2.10} \\
% & & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{47.08} & \multicolumn{1}{c}{58.93} & \multicolumn{1}{c}{32.86} & \multicolumn{1}{c}{32.42} & \multicolumn{1}{c}{34.47} & \multicolumn{1}{c}{62.12} & \multicolumn{1}{c}{61.04} & \multicolumn{1}{c}{77.60} & \multicolumn{1}{c}{67.26} & \multicolumn{1}{c}{47.28} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{51.91} & \multicolumn{1}{c}{1.63} \\
% \cmidrule{2-16}
% & \multicolumn{1}{l}{\multirow{3}{*}{{RAG}}} & 
% \multicolumn{1}{c}{5} & \multicolumn{1}{c}{54.22} & \multicolumn{1}{c}{56.79} & \multicolumn{1}{c}{35.60} & \multicolumn{1}{c}{34.48} & \multicolumn{1}{c}{36.93} & \multicolumn{1}{c}{40.38} & \multicolumn{1}{c}{61.18} & \multicolumn{1}{c}{81.60} & \multicolumn{1}{c}{75.51} & \multicolumn{1}{c}{50.30} & \multicolumn{1}{c}{50.34} & \multicolumn{1}{c}{52.49} & \multicolumn{1}{c}{1.61} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{53.14} & \multicolumn{1}{c}{53.93} & \multicolumn{1}{c}{35.74} & \multicolumn{1}{c}{34.26} & \multicolumn{1}{c}{36.78} & \multicolumn{1}{c}{43.46} & \multicolumn{1}{c}{61.17} & \multicolumn{1}{c}{81.20} & \multicolumn{1}{c}{75.29} & \multicolumn{1}{c}{50.28} & \multicolumn{1}{c}{50.22} & \multicolumn{1}{c}{52.32} & \multicolumn{1}{c}{1.91} \\
% & & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{53.29} & \multicolumn{1}{c}{54.29} & \multicolumn{1}{c}{35.48} & \multicolumn{1}{c}{33.96} & \multicolumn{1}{c}{36.67} & \multicolumn{1}{c}{42.88} & \multicolumn{1}{c}{61.37} & \multicolumn{1}{c}{81.60} & \multicolumn{1}{c}{75.02} & \multicolumn{1}{c}{50.20} & \multicolumn{1}{c}{50.91} & \multicolumn{1}{c}{52.33} & \multicolumn{1}{c}{1.34} \\
% \cmidrule{2-16}
% & \multicolumn{1}{l}{\multirow{3}{*}{{FiD}}} & 
% \multicolumn{1}{c}{5} & \multicolumn{1}{c}{53.86} & \multicolumn{1}{c}{56.07} & \multicolumn{1}{c}{35.62} & \multicolumn{1}{c}{34.84} & \multicolumn{1}{c}{36.95} & \multicolumn{1}{c}{52.50} & \multicolumn{1}{c}{61.93} & \multicolumn{1}{c}{82.20} & \multicolumn{1}{c}{75.53} & \multicolumn{1}{c}{50.52} & \multicolumn{1}{c}{49.97} & \multicolumn{1}{c}{53.64} & \multicolumn{1}{c}{2.07} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{53.29} & \multicolumn{1}{c}{53.21} & \multicolumn{1}{c}{35.82} & \multicolumn{1}{c}{34.58} & \multicolumn{1}{c}{36.47} & \multicolumn{1}{c}{50.96} & \multicolumn{1}{c}{61.86} & \multicolumn{1}{c}{81.80} & \multicolumn{1}{c}{75.60} & \multicolumn{1}{c}{50.60} & \multicolumn{1}{c}{50.13} & \multicolumn{1}{c}{53.12} & \multicolumn{1}{c}{1.88} \\
% & & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{52.20} & \multicolumn{1}{c}{53.21} & \multicolumn{1}{c}{35.82} & \multicolumn{1}{c}{34.46} & \multicolumn{1}{c}{36.90} & \multicolumn{1}{c}{60.19} & \multicolumn{1}{c}{61.82} & \multicolumn{1}{c}{82.80} & \multicolumn{1}{c}{75.25} & \multicolumn{1}{c}{50.44} & \multicolumn{1}{c}{50.31} & \multicolumn{1}{c}{53.95} & \multicolumn{1}{c}{1.30} \\
% \midrule
% \multicolumn{1}{l}{\multirow{12}{*}{{T5}}} 
% & \multicolumn{1}{l}{\multirow{5}{*}{{Original}}} & 
% \multicolumn{1}{c}{0} & \multicolumn{1}{c}{54.87} & \multicolumn{1}{c}{33.93} & \multicolumn{1}{c}{35.50} & \multicolumn{1}{c}{33.10} & \multicolumn{1}{c}{33.42} & \multicolumn{1}{c}{41.35} & \multicolumn{1}{c}{52.33} & \multicolumn{1}{c}{58.00} & \multicolumn{1}{c}{65.53} & \multicolumn{1}{c}{38.02} & \multicolumn{1}{c}{44.67} & \multicolumn{1}{c}{44.61} & \multicolumn{1}{c}{0.08} \\
% & & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{62.38} & \multicolumn{1}{c}{67.50} & \multicolumn{1}{c}{37.52} & \multicolumn{1}{c}{\underline{38.20}} & \multicolumn{1}{c}{40.10} & \multicolumn{1}{c}{43.46} & \multicolumn{1}{c}{61.66} & \multicolumn{1}{c}{83.60} & \multicolumn{1}{c}{76.99} & \multicolumn{1}{c}{45.68} & \multicolumn{1}{c}{49.62} & \multicolumn{1}{c}{55.16} & \multicolumn{1}{c}{3.80} \\
% & & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{55.02} & \multicolumn{1}{c}{53.93} & \multicolumn{1}{c}{33.70} & \multicolumn{1}{c}{35.00} & \multicolumn{1}{c}{35.62} & \multicolumn{1}{c}{39.04} & \multicolumn{1}{c}{64.64} & \multicolumn{1}{c}{82.80} & \multicolumn{1}{c}{77.71} & \multicolumn{1}{c}{45.32} & \multicolumn{1}{c}{50.50} & \multicolumn{1}{c}{52.12} & \multicolumn{1}{c}{2.78} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{56.10} & \multicolumn{1}{c}{51.43} & \multicolumn{1}{c}{33.66} & \multicolumn{1}{c}{33.88} & \multicolumn{1}{c}{33.80} & \multicolumn{1}{c}{37.88} & \multicolumn{1}{c}{64.66} & \multicolumn{1}{c}{82.00} & \multicolumn{1}{c}{77.67} & \multicolumn{1}{c}{44.64} & \multicolumn{1}{c}{52.10} & \multicolumn{1}{c}{51.62} & \multicolumn{1}{c}{2.42} \\
% & & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{50.32} & \multicolumn{1}{c}{38.57} & \multicolumn{1}{c}{33.74} & \multicolumn{1}{c}{33.26} & \multicolumn{1}{c}{33.98} & \multicolumn{1}{c}{40.39} & \multicolumn{1}{c}{64.77} & \multicolumn{1}{c}{81.20} & \multicolumn{1}{c}{71.50} & \multicolumn{1}{c}{43.10} & \multicolumn{1}{c}{49.94} & \multicolumn{1}{c}{49.16} & \multicolumn{1}{c}{1.99} \\
% \cmidrule{2-16}
% & \multicolumn{1}{l}{\multirow{3}{*}{{RAG}}} & 
% \multicolumn{1}{c}{5} & \multicolumn{1}{c}{62.74} & \multicolumn{1}{c}{70.36} & \multicolumn{1}{c}{39.88} & \multicolumn{1}{c}{38.12} & \multicolumn{1}{c}{39.27} & \multicolumn{1}{c}{61.92} & \multicolumn{1}{c}{62.00} & \multicolumn{1}{c}{83.80} & \multicolumn{1}{c}{77.56} & \multicolumn{1}{c}{46.02} & \multicolumn{1}{c}{50.03} & \multicolumn{1}{c}{57.43} & \multicolumn{1}{c}{2.20} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{63.32} & \multicolumn{1}{c}{73.93} & \multicolumn{1}{c}{40.16} & \multicolumn{1}{c}{38.32} & \multicolumn{1}{c}{39.57} & \multicolumn{1}{c}{63.85} & \multicolumn{1}{c}{62.13} & \multicolumn{1}{c}{83.80} & \multicolumn{1}{c}{77.31} & \multicolumn{1}{c}{45.78} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{58.01} & \multicolumn{1}{c}{1.93} \\
% & & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{63.90} & \multicolumn{1}{c}{70.36} & \multicolumn{1}{c}{40.64} & \multicolumn{1}{c}{38.06} & \multicolumn{1}{c}{39.63} & \multicolumn{1}{c}{\underline{69.42}} & \multicolumn{1}{c}{62.15} & \multicolumn{1}{c}{84.80} & \multicolumn{1}{c}{77.27} & \multicolumn{1}{c}{45.88} & \multicolumn{1}{c}{50.22} & \multicolumn{1}{c}{58.39} & \multicolumn{1}{c}{1.32} \\
% \cmidrule{2-16}
% & \multicolumn{1}{l}{\multirow{3}{*}{{FiD}}} & 
% \multicolumn{1}{c}{5} & \multicolumn{1}{c}{63.61} & \multicolumn{1}{c}{73.93} & \multicolumn{1}{c}{40.80} & \multicolumn{1}{c}{\underline{38.18}} & \multicolumn{1}{c}{39.55} & \multicolumn{1}{c}{62.69} & \multicolumn{1}{c}{62.48} & \multicolumn{1}{c}{84.20} & \multicolumn{1}{c}{77.84} & \multicolumn{1}{c}{45.90} & \multicolumn{1}{c}{50.03} & \multicolumn{1}{c}{58.11} & \multicolumn{1}{c}{2.00} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{63.39} & \multicolumn{1}{c}{\underline{77.86}} & \multicolumn{1}{c}{41.18} & \multicolumn{1}{c}{38.32} & \multicolumn{1}{c}{39.58} & \multicolumn{1}{c}{\underline{65.19}} & \multicolumn{1}{c}{62.23} & \multicolumn{1}{c}{84.80} & \multicolumn{1}{c}{77.56} & \multicolumn{1}{c}{45.80} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{58.72} & \multicolumn{1}{c}{1.47} \\
% & & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{64.04} & \multicolumn{1}{c}{\underline{75.00}} & \multicolumn{1}{c}{42.08} & \multicolumn{1}{c}{37.94} & \multicolumn{1}{c}{39.98} & \multicolumn{1}{c}{68.27} & \multicolumn{1}{c}{62.32} & \multicolumn{1}{c}{85.40} & \multicolumn{1}{c}{77.57} & \multicolumn{1}{c}{46.06} & \multicolumn{1}{c}{50.16} & \multicolumn{1}{c}{58.98} & \multicolumn{1}{c}{0.97} \\
% \midrule
% \multicolumn{1}{l}{\multirow{12}{*}{{T0}}} 
% & \multicolumn{1}{l}{\multirow{5}{*}{{Original}}} & 
% \multicolumn{1}{c}{0} & \multicolumn{1}{c}{\underline{81.95}} & \multicolumn{1}{c}{\underline{78.57}} & \multicolumn{1}{c}{\underline{45.20}} & \multicolumn{1}{c}{\underline{40.40}} & \multicolumn{1}{c}{\underline{42.42}} & \multicolumn{1}{c}{\underline{53.85}} & \multicolumn{1}{c}{60.93} & \multicolumn{1}{c}{82.00} & \multicolumn{1}{c}{75.95} & \multicolumn{1}{c}{50.56} & \multicolumn{1}{c}{\underline{55.64}} & \multicolumn{1}{c}{\textbf{60.68}} & \multicolumn{1}{c}{0.13} \\
% & & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{\underline{74.08}} & \multicolumn{1}{c}{\underline{72.50}} & \multicolumn{1}{c}{\underline{41.52}} & \multicolumn{1}{c}{37.20} & \multicolumn{1}{c}{\underline{38.73}} & \multicolumn{1}{c}{41.73} & \multicolumn{1}{c}{61.10} & \multicolumn{1}{c}{81.20} & \multicolumn{1}{c}{75.09} & \multicolumn{1}{c}{50.56} & \multicolumn{1}{c}{51.76} & \multicolumn{1}{c}{\textbf{56.86}} & \multicolumn{1}{c}{3.74} \\
% & & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{67.94} & \multicolumn{1}{c}{76.07} & \multicolumn{1}{c}{39.42} & \multicolumn{1}{c}{36.28} & \multicolumn{1}{c}{38.53} & \multicolumn{1}{c}{60.19} & \multicolumn{1}{c}{61.56} & \multicolumn{1}{c}{81.60} & \multicolumn{1}{c}{73.18} & \multicolumn{1}{c}{49.06} & \multicolumn{1}{c}{54.20} & \multicolumn{1}{c}{58.00} & \multicolumn{1}{c}{2.73} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{53.57} & \multicolumn{1}{c}{60.36} & \multicolumn{1}{c}{36.06} & \multicolumn{1}{c}{34.80} & \multicolumn{1}{c}{35.12} & \multicolumn{1}{c}{62.88} & \multicolumn{1}{c}{60.36} & \multicolumn{1}{c}{81.60} & \multicolumn{1}{c}{71.28} & \multicolumn{1}{c}{47.92} & \multicolumn{1}{c}{52.98} & \multicolumn{1}{c}{54.27} & \multicolumn{1}{c}{2.72} \\
% & & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{47.29} & \multicolumn{1}{c}{63.21} & \multicolumn{1}{c}{33.30} & \multicolumn{1}{c}{33.28} & \multicolumn{1}{c}{33.15} & \multicolumn{1}{c}{63.85} & \multicolumn{1}{c}{60.17} & \multicolumn{1}{c}{79.20} & \multicolumn{1}{c}{61.98} & \multicolumn{1}{c}{46.10} & \multicolumn{1}{c}{50.63} & \multicolumn{1}{c}{52.01} & \multicolumn{1}{c}{1.93} \\
% \cmidrule{2-16}
% & \multicolumn{1}{l}{\multirow{3}{*}{{RAG}}} &
% \multicolumn{1}{c}{5} & \multicolumn{1}{c}{81.08} & \multicolumn{1}{c}{\underline{77.50}} & \multicolumn{1}{c}{44.18} & \multicolumn{1}{c}{37.76} & \multicolumn{1}{c}{42.07} & \multicolumn{1}{c}{55.00} & \multicolumn{1}{c}{62.12} & \multicolumn{1}{c}{82.60} & \multicolumn{1}{c}{75.34} & \multicolumn{1}{c}{50.62} & \multicolumn{1}{c}{54.33} & \multicolumn{1}{c}{60.24} & \multicolumn{1}{c}{2.47} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{\underline{80.72}} & \multicolumn{1}{c}{70.71} & \multicolumn{1}{c}{\underline{44.54}} & \multicolumn{1}{c}{38.34} & \multicolumn{1}{c}{\underline{42.42}} & \multicolumn{1}{c}{54.81} & \multicolumn{1}{c}{62.16} & \multicolumn{1}{c}{82.00} & \multicolumn{1}{c}{75.09} & \multicolumn{1}{c}{50.80} & \multicolumn{1}{c}{\underline{56.39}} & \multicolumn{1}{c}{59.82} & \multicolumn{1}{c}{2.62} \\
% & & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{81.37} & \multicolumn{1}{c}{74.29} & \multicolumn{1}{c}{\underline{44.52}} & \multicolumn{1}{c}{\underline{38.12}} & \multicolumn{1}{c}{\underline{41.98}} & \multicolumn{1}{c}{64.42} & \multicolumn{1}{c}{62.23} & \multicolumn{1}{c}{84.40} & \multicolumn{1}{c}{74.89} & \multicolumn{1}{c}{50.38} & \multicolumn{1}{c}{56.02} & \multicolumn{1}{c}{\textbf{61.15}} & \multicolumn{1}{c}{1.92} \\
% \cmidrule{2-16}
% & \multicolumn{1}{l}{\multirow{3}{*}{{FiD}}} &
% \multicolumn{1}{c}{5} & \multicolumn{1}{c}{\underline{81.16}} & \multicolumn{1}{c}{76.43} & \multicolumn{1}{c}{\underline{44.36}} & \multicolumn{1}{c}{37.86} & \multicolumn{1}{c}{\underline{42.60}} & \multicolumn{1}{c}{\underline{63.65}} & \multicolumn{1}{c}{61.86} & \multicolumn{1}{c}{82.20} & \multicolumn{1}{c}{75.25} & \multicolumn{1}{c}{50.40} & \multicolumn{1}{c}{\underline{55.27}} & \multicolumn{1}{c}{\textbf{61.00}} & \multicolumn{1}{c}{2.28} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{79.78} & \multicolumn{1}{c}{68.21} & \multicolumn{1}{c}{44.26} & \multicolumn{1}{c}{\underline{38.58}} & \multicolumn{1}{c}{41.57} & \multicolumn{1}{c}{64.23} & \multicolumn{1}{c}{61.86} & \multicolumn{1}{c}{82.20} & \multicolumn{1}{c}{75.05} & \multicolumn{1}{c}{50.30} & \multicolumn{1}{c}{55.64} & \multicolumn{1}{c}{\textbf{60.15}} & \multicolumn{1}{c}{2.13} \\
% & & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{\underline{81.95}} & \multicolumn{1}{c}{70.71} & \multicolumn{1}{c}{44.38} & \multicolumn{1}{c}{37.90} & \multicolumn{1}{c}{41.90} & \multicolumn{1}{c}{68.08} & \multicolumn{1}{c}{62.18} & \multicolumn{1}{c}{83.80} & \multicolumn{1}{c}{74.86} & \multicolumn{1}{c}{50.10} & \multicolumn{1}{c}{\underline{56.21}} & \multicolumn{1}{c}{61.10} & \multicolumn{1}{c}{1.52} \\
% \midrule
% \multicolumn{1}{l}{\multirow{12}{*}{{UL2}}} 
% & \multicolumn{1}{l}{\multirow{5}{*}{{Original}}} & 
% \multicolumn{1}{c}{0} & \multicolumn{1}{c}{53.79} & \multicolumn{1}{c}{8.93} & \multicolumn{1}{c}{33.10} & \multicolumn{1}{c}{33.00} & \multicolumn{1}{c}{34.92} & \multicolumn{1}{c}{47.12} & \multicolumn{1}{c}{\underline{64.17}} & \multicolumn{1}{c}{\underline{87.00}} & \multicolumn{1}{c}{\underline{78.41}} & \multicolumn{1}{c}{\underline{57.46}} & \multicolumn{1}{c}{53.13} & \multicolumn{1}{c}{50.09} & \multicolumn{1}{c}{0.08} \\
% & & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{54.87} & \multicolumn{1}{c}{49.29} & \multicolumn{1}{c}{34.36} & \multicolumn{1}{c}{33.76} & \multicolumn{1}{c}{33.32} & \multicolumn{1}{c}{\underline{47.50}} & \multicolumn{1}{c}{\underline{63.96}} & \multicolumn{1}{c}{\underline{85.00}} & \multicolumn{1}{c}{\underline{78.31}} & \multicolumn{1}{c}{\underline{56.86}} & \multicolumn{1}{c}{\underline{52.29}} & \multicolumn{1}{c}{53.59} & \multicolumn{1}{c}{2.19} \\
% & & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{49.03} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{33.32} & \multicolumn{1}{c}{33.52} & \multicolumn{1}{c}{33.50} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{\underline{66.35}} & \multicolumn{1}{c}{84.20} & \multicolumn{1}{c}{77.66} & \multicolumn{1}{c}{55.96} & \multicolumn{1}{c}{50.09} & \multicolumn{1}{c}{51.83} & \multicolumn{1}{c}{0.72} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{47.51} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{33.30} & \multicolumn{1}{c}{33.30} & \multicolumn{1}{c}{32.97} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{64.67} & \multicolumn{1}{c}{84.20} & \multicolumn{1}{c}{76.77} & \multicolumn{1}{c}{54.60} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{51.26} & \multicolumn{1}{c}{0.41} \\
% & & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{47.29} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{33.50} & \multicolumn{1}{c}{33.30} & \multicolumn{1}{c}{33.12} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{63.84} & \multicolumn{1}{c}{82.00} & \multicolumn{1}{c}{63.09} & \multicolumn{1}{c}{51.64} & \multicolumn{1}{c}{50.13} & \multicolumn{1}{c}{49.49} & \multicolumn{1}{c}{0.53} \\
% \cmidrule{2-16}
% & \multicolumn{1}{l}{\multirow{3}{*}{{RAG}}} & 
% \multicolumn{1}{c}{5} & \multicolumn{1}{c}{52.78} & \multicolumn{1}{c}{46.07} & \multicolumn{1}{c}{33.24} & \multicolumn{1}{c}{32.62} & \multicolumn{1}{c}{32.90} & \multicolumn{1}{c}{44.81} & \multicolumn{1}{c}{65.89} & \multicolumn{1}{c}{84.20} & \multicolumn{1}{c}{78.97} & \multicolumn{1}{c}{56.80} & \multicolumn{1}{c}{51.03} & \multicolumn{1}{c}{52.67} & \multicolumn{1}{c}{1.44} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{53.36} & \multicolumn{1}{c}{45.71} & \multicolumn{1}{c}{33.46} & \multicolumn{1}{c}{31.94} & \multicolumn{1}{c}{32.62} & \multicolumn{1}{c}{45.58} & \multicolumn{1}{c}{\underline{65.59}} & \multicolumn{1}{c}{84.40} & \multicolumn{1}{c}{78.80} & \multicolumn{1}{c}{56.84} & \multicolumn{1}{c}{50.75} & \multicolumn{1}{c}{52.64} & \multicolumn{1}{c}{0.89} \\
% & & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{53.07} & \multicolumn{1}{c}{46.07} & \multicolumn{1}{c}{33.30} & \multicolumn{1}{c}{31.58} & \multicolumn{1}{c}{32.85} & \multicolumn{1}{c}{46.54} & \multicolumn{1}{c}{\underline{65.73}} & \multicolumn{1}{c}{83.60} & \multicolumn{1}{c}{78.92} & \multicolumn{1}{c}{56.90} & \multicolumn{1}{c}{50.19} & \multicolumn{1}{c}{52.61} & \multicolumn{1}{c}{1.01} \\
% \cmidrule{2-16}
% & \multicolumn{1}{l}{\multirow{3}{*}{{FiD}}} &
% \multicolumn{1}{c}{5} & \multicolumn{1}{c}{55.81} & \multicolumn{1}{c}{47.50} & \multicolumn{1}{c}{33.88} & \multicolumn{1}{c}{32.48} & \multicolumn{1}{c}{32.60} & \multicolumn{1}{c}{47.88} & \multicolumn{1}{c}{65.35} & \multicolumn{1}{c}{\underline{84.80}} & \multicolumn{1}{c}{\underline{79.20}} & \multicolumn{1}{c}{\underline{57.12}} & \multicolumn{1}{c}{52.92} & \multicolumn{1}{c}{53.59} & \multicolumn{1}{c}{1.25} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{54.95} & \multicolumn{1}{c}{47.50} & \multicolumn{1}{c}{34.12} & \multicolumn{1}{c}{31.72} & \multicolumn{1}{c}{32.22} & \multicolumn{1}{c}{49.23} & \multicolumn{1}{c}{65.02} & \multicolumn{1}{c}{\underline{85.60}} & \multicolumn{1}{c}{\underline{79.17}} & \multicolumn{1}{c}{\underline{56.98}} & \multicolumn{1}{c}{52.79} & \multicolumn{1}{c}{53.57} & \multicolumn{1}{c}{1.18} \\
% & & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{54.37} & \multicolumn{1}{c}{47.14} & \multicolumn{1}{c}{33.94} & \multicolumn{1}{c}{31.52} & \multicolumn{1}{c}{31.58} & \multicolumn{1}{c}{49.42} & \multicolumn{1}{c}{65.13} & \multicolumn{1}{c}{\underline{86.00}} & \multicolumn{1}{c}{\underline{79.14}} & \multicolumn{1}{c}{\underline{56.92}} & \multicolumn{1}{c}{52.35} & \multicolumn{1}{c}{53.41} & \multicolumn{1}{c}{1.30} \\
% \bottomrule
% \end{tabular}}
% \end{adjustbox}

\begin{table*}
\setlength\tabcolsep{6.5pt}
\centering
\begin{adjustbox}{width=1\textwidth}{
\begin{tabular}{lcccccccccccccccc}
\toprule
Model & Method & Shot & RTE & CB & ANLI1 & ANLI2 & ANLI3 & WSC & Winogrande & COPA & StoryCloze & HellaSwag* & WiC & 
\textbf{average} & \textbf{std} \\
\midrule
\multicolumn{1}{l}{\multirow{12}{*}{{T5-LM}}} 
& \multicolumn{1}{l}{\multirow{5}{*}{\textit{original}}} & 
\multicolumn{1}{c}{0} & \multicolumn{1}{c}{\underline{62.45}} & \multicolumn{1}{c}{48.21} & \multicolumn{1}{c}{\underline{37.70}} & \multicolumn{1}{c}{34.90} & \multicolumn{1}{c}{37.25} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{55.09} & \multicolumn{1}{c}{71.00} & \multicolumn{1}{c}{72.85} & \multicolumn{1}{c}{48.02} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{50.37} & \multicolumn{1}{c}{0.05} \\
& & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{56.68} & \multicolumn{1}{c}{57.86} & \multicolumn{1}{c}{34.96} & \multicolumn{1}{c}{34.40} & \multicolumn{1}{c}{35.87} & \multicolumn{1}{c}{41.92} & \multicolumn{1}{c}{61.15} & \multicolumn{1}{c}{81.20} & \multicolumn{1}{c}{74.83} & \multicolumn{1}{c}{\underline{50.70}} & \multicolumn{1}{c}{50.38} & \multicolumn{1}{c}{52.72} & \multicolumn{1}{c}{3.28} \\
& & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{47.65} & \multicolumn{1}{c}{56.07} & \multicolumn{1}{c}{32.66} & \multicolumn{1}{c}{34.06} & \multicolumn{1}{c}{33.97} & \multicolumn{1}{c}{43.65} & \multicolumn{1}{c}{61.29} & \multicolumn{1}{c}{77.80} & \multicolumn{1}{c}{75.34} & \multicolumn{1}{c}{50.38} & \multicolumn{1}{c}{49.97} & \multicolumn{1}{c}{51.17} & \multicolumn{1}{c}{1.48} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{47.22} & \multicolumn{1}{c}{47.50} & \multicolumn{1}{c}{32.12} & \multicolumn{1}{c}{32.96} & \multicolumn{1}{c}{33.07} & \multicolumn{1}{c}{52.31} & \multicolumn{1}{c}{61.42} & \multicolumn{1}{c}{79.20} & \multicolumn{1}{c}{74.47} & \multicolumn{1}{c}{49.18} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{50.86} & \multicolumn{1}{c}{2.10} \\
& & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{47.08} & \multicolumn{1}{c}{\underline{58.93}} & \multicolumn{1}{c}{32.86} & \multicolumn{1}{c}{32.42} & \multicolumn{1}{c}{34.47} & \multicolumn{1}{c}{\underline{62.12}} & \multicolumn{1}{c}{61.04} & \multicolumn{1}{c}{77.60} & \multicolumn{1}{c}{67.26} & \multicolumn{1}{c}{47.28} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{51.91} & \multicolumn{1}{c}{1.63} \\
\cmidrule{2-16}
& \multicolumn{1}{l}{\multirow{3}{*}{\textit{early-fusion}}} & 
\multicolumn{1}{c}{5} & \multicolumn{1}{c}{53.86} & \multicolumn{1}{c}{56.07} & \multicolumn{1}{c}{35.62} & \multicolumn{1}{c}{\underline{34.84}} & \multicolumn{1}{c}{\underline{36.95}} & \multicolumn{1}{c}{52.50} & \multicolumn{1}{c}{\underline{61.93}} & \multicolumn{1}{c}{82.20} & \multicolumn{1}{c}{75.53} & \multicolumn{1}{c}{50.52} & \multicolumn{1}{c}{49.97} & \multicolumn{1}{c}{53.64} &
\multicolumn{1}{c}{2.07} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{53.29} & \multicolumn{1}{c}{53.21} & \multicolumn{1}{c}{35.82} & \multicolumn{1}{c}{34.58} & \multicolumn{1}{c}{36.47} & \multicolumn{1}{c}{50.96} & \multicolumn{1}{c}{61.86} & \multicolumn{1}{c}{81.80} & \multicolumn{1}{c}{\underline{75.60}} & \multicolumn{1}{c}{50.60} & \multicolumn{1}{c}{50.13} & \multicolumn{1}{c}{53.12} & \multicolumn{1}{c}{1.88} \\
& & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{52.20} & \multicolumn{1}{c}{53.21} & \multicolumn{1}{c}{35.82} & \multicolumn{1}{c}{34.46} & \multicolumn{1}{c}{36.90} & \multicolumn{1}{c}{60.19} & \multicolumn{1}{c}{61.82} & \multicolumn{1}{c}{\underline{82.80}} & \multicolumn{1}{c}{75.25} & \multicolumn{1}{c}{50.44} & \multicolumn{1}{c}{50.31} & \multicolumn{1}{c}{\textbf{53.95}} & \multicolumn{1}{c}{1.30} \\
\cmidrule{2-16}
& \multicolumn{1}{l}{\multirow{3}{*}{\textit{late-fusion}}} & 
\multicolumn{1}{c}{5} & \multicolumn{1}{c}{54.22} & \multicolumn{1}{c}{56.79} & \multicolumn{1}{c}{35.60} & \multicolumn{1}{c}{34.48} & \multicolumn{1}{c}{36.93} & \multicolumn{1}{c}{40.38} & \multicolumn{1}{c}{61.18} & \multicolumn{1}{c}{81.60} & \multicolumn{1}{c}{75.51} & \multicolumn{1}{c}{50.30} & \multicolumn{1}{c}{50.34} & \multicolumn{1}{c}{52.49} & \multicolumn{1}{c}{1.61} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{53.14} & \multicolumn{1}{c}{53.93} & \multicolumn{1}{c}{35.74} & \multicolumn{1}{c}{34.26} & \multicolumn{1}{c}{36.78} & \multicolumn{1}{c}{43.46} & \multicolumn{1}{c}{61.17} & \multicolumn{1}{c}{81.20} & \multicolumn{1}{c}{75.29} & \multicolumn{1}{c}{50.28} & \multicolumn{1}{c}{50.22} & \multicolumn{1}{c}{52.32} & \multicolumn{1}{c}{1.91} \\
& & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{53.29} & \multicolumn{1}{c}{54.29} & \multicolumn{1}{c}{35.48} & \multicolumn{1}{c}{33.96} & \multicolumn{1}{c}{36.67} & \multicolumn{1}{c}{42.88} & \multicolumn{1}{c}{61.37} & \multicolumn{1}{c}{81.60} & \multicolumn{1}{c}{75.02} & \multicolumn{1}{c}{50.20} & \multicolumn{1}{c}{\underline{50.91}} & \multicolumn{1}{c}{52.33} & \multicolumn{1}{c}{1.34} \\
\midrule
\multicolumn{1}{l}{\multirow{12}{*}{{T5}}} 
& \multicolumn{1}{l}{\multirow{5}{*}{\textit{original}}} & 
\multicolumn{1}{c}{0} & \multicolumn{1}{c}{54.87} & \multicolumn{1}{c}{33.93} & \multicolumn{1}{c}{35.50} & \multicolumn{1}{c}{33.10} & \multicolumn{1}{c}{33.42} & \multicolumn{1}{c}{41.35} & \multicolumn{1}{c}{52.33} & \multicolumn{1}{c}{58.00} & \multicolumn{1}{c}{65.53} & \multicolumn{1}{c}{38.02} & \multicolumn{1}{c}{44.67} & \multicolumn{1}{c}{44.61} & \multicolumn{1}{c}{0.08}\\
& & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{62.38} & \multicolumn{1}{c}{67.50} & \multicolumn{1}{c}{37.52} & \multicolumn{1}{c}{38.20} & \multicolumn{1}{c}{\underline{40.10}} & \multicolumn{1}{c}{43.46} & \multicolumn{1}{c}{61.66} & \multicolumn{1}{c}{83.60} & \multicolumn{1}{c}{77.00} & \multicolumn{1}{c}{45.68} & \multicolumn{1}{c}{49.62} & \multicolumn{1}{c}{55.16} & \multicolumn{1}{c}{3.80} \\
& & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{55.02} & \multicolumn{1}{c}{53.93} & \multicolumn{1}{c}{33.70} & \multicolumn{1}{c}{35.00} & \multicolumn{1}{c}{35.62} & \multicolumn{1}{c}{39.04} & \multicolumn{1}{c}{64.64} & \multicolumn{1}{c}{82.80} & \multicolumn{1}{c}{77.71} & \multicolumn{1}{c}{45.32} & \multicolumn{1}{c}{50.50} & \multicolumn{1}{c}{52.12} & \multicolumn{1}{c}{2.78} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{56.10} & \multicolumn{1}{c}{51.43} & \multicolumn{1}{c}{33.66} & \multicolumn{1}{c}{33.88} & \multicolumn{1}{c}{33.80} & \multicolumn{1}{c}{37.88} & \multicolumn{1}{c}{64.66} & \multicolumn{1}{c}{82.00} & \multicolumn{1}{c}{77.67} & \multicolumn{1}{c}{44.64} & \multicolumn{1}{c}{\underline{52.10}} & \multicolumn{1}{c}{51.62} & \multicolumn{1}{c}{2.42} \\
& & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{50.32} & \multicolumn{1}{c}{38.57} & \multicolumn{1}{c}{33.74} & \multicolumn{1}{c}{33.26} & \multicolumn{1}{c}{33.98} & \multicolumn{1}{c}{40.38} & \multicolumn{1}{c}{\underline{64.77}} & \multicolumn{1}{c}{81.20} & \multicolumn{1}{c}{71.50} & \multicolumn{1}{c}{43.10} & \multicolumn{1}{c}{49.94} & \multicolumn{1}{c}{49.16} & \multicolumn{1}{c}{1.99} \\
\cmidrule{2-16}
& \multicolumn{1}{l}{\multirow{3}{*}{\textit{early-fusion}}} & 
\multicolumn{1}{c}{5} & \multicolumn{1}{c}{63.61} & \multicolumn{1}{c}{73.93} & \multicolumn{1}{c}{40.80} & \multicolumn{1}{c}{38.18} & \multicolumn{1}{c}{39.55} & \multicolumn{1}{c}{62.69} & \multicolumn{1}{c}{62.48} & \multicolumn{1}{c}{84.20} & \multicolumn{1}{c}{\underline{77.84}} & \multicolumn{1}{c}{45.90} & \multicolumn{1}{c}{50.03} & \multicolumn{1}{c}{58.11} & \multicolumn{1}{c}{2.00} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{63.39} & \multicolumn{1}{c}{\underline{77.86}} & \multicolumn{1}{c}{41.18} & \multicolumn{1}{c}{\underline{38.32}} & \multicolumn{1}{c}{39.58} & \multicolumn{1}{c}{65.19} & \multicolumn{1}{c}{62.23} & \multicolumn{1}{c}{84.80} & \multicolumn{1}{c}{77.56} & \multicolumn{1}{c}{45.80} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{58.72} & \multicolumn{1}{c}{1.47} \\
& & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{\underline{64.04}} & \multicolumn{1}{c}{75.00} & \multicolumn{1}{c}{\underline{42.08}} & \multicolumn{1}{c}{37.94} & \multicolumn{1}{c}{39.98} & \multicolumn{1}{c}{68.27} & \multicolumn{1}{c}{62.32} & \multicolumn{1}{c}{\underline{85.40}} & \multicolumn{1}{c}{77.57} & \multicolumn{1}{c}{\underline{46.06}} & \multicolumn{1}{c}{50.16} & \multicolumn{1}{c}{\textbf{58.98}} & \multicolumn{1}{c}{0.97} \\
\cmidrule{2-16}
& \multicolumn{1}{l}{\multirow{3}{*}{\textit{late-fusion}}} & 
\multicolumn{1}{c}{5} & \multicolumn{1}{c}{62.74} & \multicolumn{1}{c}{70.36} & \multicolumn{1}{c}{39.88} & \multicolumn{1}{c}{38.12} & \multicolumn{1}{c}{39.27} & \multicolumn{1}{c}{61.92} & \multicolumn{1}{c}{62.00} & \multicolumn{1}{c}{83.80} & \multicolumn{1}{c}{77.56} & \multicolumn{1}{c}{46.02} & \multicolumn{1}{c}{50.03} & \multicolumn{1}{c}{57.43} & \multicolumn{1}{c}{2.20} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{63.32} & \multicolumn{1}{c}{73.93} & \multicolumn{1}{c}{40.16} & \multicolumn{1}{c}{\underline{38.32}} & \multicolumn{1}{c}{39.57} & \multicolumn{1}{c}{63.85} & \multicolumn{1}{c}{62.13} & \multicolumn{1}{c}{83.80} & \multicolumn{1}{c}{77.31} & \multicolumn{1}{c}{45.78} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{58.01} & \multicolumn{1}{c}{1.93} \\
& & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{63.90} & \multicolumn{1}{c}{70.36} & \multicolumn{1}{c}{40.64} & \multicolumn{1}{c}{38.06} & \multicolumn{1}{c}{39.63} & \multicolumn{1}{c}{\underline{69.42}} & \multicolumn{1}{c}{62.15} & \multicolumn{1}{c}{84.80} & \multicolumn{1}{c}{77.27} & \multicolumn{1}{c}{45.88} & \multicolumn{1}{c}{50.22} & \multicolumn{1}{c}{58.39} & \multicolumn{1}{c}{1.32} \\
\midrule
\multicolumn{1}{l}{\multirow{12}{*}{{T0}}} 
& \multicolumn{1}{l}{\multirow{5}{*}{\textit{original}}} & 
\multicolumn{1}{c}{0} & \multicolumn{1}{c}{\underline{81.95}} & \multicolumn{1}{c}{\underline{78.57}} & \multicolumn{1}{c}{\underline{45.20}} & \multicolumn{1}{c}{\underline{40.40}} & \multicolumn{1}{c}{42.42} & \multicolumn{1}{c}{53.85} & \multicolumn{1}{c}{60.93} & \multicolumn{1}{c}{82.00} & \multicolumn{1}{c}{\underline{75.95}} & \multicolumn{1}{c}{50.56} & \multicolumn{1}{c}{55.64} & \multicolumn{1}{c}{60.68} & \multicolumn{1}{c}{0.13} \\
& & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{74.08} & \multicolumn{1}{c}{72.50} & \multicolumn{1}{c}{41.52} & \multicolumn{1}{c}{37.20} & \multicolumn{1}{c}{38.73} & \multicolumn{1}{c}{41.73} & \multicolumn{1}{c}{61.10} & \multicolumn{1}{c}{81.20} & \multicolumn{1}{c}{75.09} & \multicolumn{1}{c}{50.56} & \multicolumn{1}{c}{51.76} & \multicolumn{1}{c}{56.86} & \multicolumn{1}{c}{3.74} \\
& & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{67.94} & \multicolumn{1}{c}{76.07} & \multicolumn{1}{c}{39.42} & \multicolumn{1}{c}{36.28} & \multicolumn{1}{c}{38.53} & \multicolumn{1}{c}{60.19} & \multicolumn{1}{c}{61.56} & \multicolumn{1}{c}{81.60} & \multicolumn{1}{c}{73.18} & \multicolumn{1}{c}{49.06} & \multicolumn{1}{c}{54.20} & \multicolumn{1}{c}{58.00} & \multicolumn{1}{c}{2.73} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{53.57} & \multicolumn{1}{c}{60.36} & \multicolumn{1}{c}{36.06} & \multicolumn{1}{c}{34.80} & \multicolumn{1}{c}{35.12} & \multicolumn{1}{c}{62.88} & \multicolumn{1}{c}{60.36} & \multicolumn{1}{c}{81.60} & \multicolumn{1}{c}{71.28} & \multicolumn{1}{c}{47.92} & \multicolumn{1}{c}{52.98} & \multicolumn{1}{c}{54.27} & \multicolumn{1}{c}{2.72} \\
& & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{47.29} & \multicolumn{1}{c}{63.21} & \multicolumn{1}{c}{33.30} & \multicolumn{1}{c}{33.28} & \multicolumn{1}{c}{33.15} & \multicolumn{1}{c}{63.85} & \multicolumn{1}{c}{60.17} & \multicolumn{1}{c}{79.20} & \multicolumn{1}{c}{61.98} & \multicolumn{1}{c}{46.10} & \multicolumn{1}{c}{50.63} & \multicolumn{1}{c}{52.01} & \multicolumn{1}{c}{1.93} \\
\cmidrule{2-16}
& \multicolumn{1}{l}{\multirow{3}{*}{\textit{early-fusion}}} &
\multicolumn{1}{c}{5} & \multicolumn{1}{c}{81.16} & \multicolumn{1}{c}{76.43} & \multicolumn{1}{c}{44.36} & \multicolumn{1}{c}{37.86} & \multicolumn{1}{c}{\underline{42.60}} & \multicolumn{1}{c}{63.65} & \multicolumn{1}{c}{61.86} & \multicolumn{1}{c}{82.20} & \multicolumn{1}{c}{75.25} & \multicolumn{1}{c}{50.40} & \multicolumn{1}{c}{55.27} & \multicolumn{1}{c}{61.00} & \multicolumn{1}{c}{2.28} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{79.78} & \multicolumn{1}{c}{68.21} & \multicolumn{1}{c}{44.26} & \multicolumn{1}{c}{38.58} & \multicolumn{1}{c}{41.57} & \multicolumn{1}{c}{64.23} & \multicolumn{1}{c}{61.86} & \multicolumn{1}{c}{82.20} & \multicolumn{1}{c}{75.05} & \multicolumn{1}{c}{50.30} & \multicolumn{1}{c}{55.64} & \multicolumn{1}{c}{60.15} & \multicolumn{1}{c}{2.13} \\
& & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{\underline{81.95}} & \multicolumn{1}{c}{70.71} & \multicolumn{1}{c}{44.38} & \multicolumn{1}{c}{37.90} & \multicolumn{1}{c}{41.90} & \multicolumn{1}{c}{\underline{68.08}} & \multicolumn{1}{c}{62.18} & \multicolumn{1}{c}{83.80} & \multicolumn{1}{c}{74.86} & \multicolumn{1}{c}{50.10} & \multicolumn{1}{c}{56.21} & \multicolumn{1}{c}{61.10} & \multicolumn{1}{c}{1.52} \\
\cmidrule{2-16}
& \multicolumn{1}{l}{\multirow{3}{*}{\textit{late-fusion}}} &
\multicolumn{1}{c}{5} & \multicolumn{1}{c}{81.08} & \multicolumn{1}{c}{77.50} & \multicolumn{1}{c}{44.18} & \multicolumn{1}{c}{37.76} & \multicolumn{1}{c}{42.07} & \multicolumn{1}{c}{55.00} & \multicolumn{1}{c}{62.12} & \multicolumn{1}{c}{82.60} & \multicolumn{1}{c}{75.34} & \multicolumn{1}{c}{50.62} & \multicolumn{1}{c}{54.33} & \multicolumn{1}{c}{60.24} & \multicolumn{1}{c}{2.47} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{80.72} & \multicolumn{1}{c}{70.71} & \multicolumn{1}{c}{44.54} & \multicolumn{1}{c}{38.34} & \multicolumn{1}{c}{42.42} & \multicolumn{1}{c}{54.81} & \multicolumn{1}{c}{62.16} & \multicolumn{1}{c}{82.00} & \multicolumn{1}{c}{75.09} & \multicolumn{1}{c}{\underline{50.80}} & \multicolumn{1}{c}{\underline{56.39}} & \multicolumn{1}{c}{59.82} & \multicolumn{1}{c}{2.62} \\
& & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{81.37} & \multicolumn{1}{c}{74.29} & \multicolumn{1}{c}{44.52} & \multicolumn{1}{c}{38.12} & \multicolumn{1}{c}{41.98} & \multicolumn{1}{c}{64.42} & \multicolumn{1}{c}{\underline{62.23}} & \multicolumn{1}{c}{\underline{84.40}} & \multicolumn{1}{c}{74.89} & \multicolumn{1}{c}{50.38} & \multicolumn{1}{c}{56.02} & \multicolumn{1}{c}{\textbf{61.15}} & \multicolumn{1}{c}{1.92} \\
\midrule
\multicolumn{1}{l}{\multirow{12}{*}{{UL2}}} 
& \multicolumn{1}{l}{\multirow{5}{*}{\textit{original}}} & 
\multicolumn{1}{c}{0} & \multicolumn{1}{c}{53.79} & \multicolumn{1}{c}{8.93} & \multicolumn{1}{c}{33.10} & \multicolumn{1}{c}{33.00} & \multicolumn{1}{c}{\underline{34.92}} & \multicolumn{1}{c}{47.12} & \multicolumn{1}{c}{64.17} & \multicolumn{1}{c}{\underline{87.00}} & \multicolumn{1}{c}{78.41} & \multicolumn{1}{c}{\underline{57.46}} & \multicolumn{1}{c}{\underline{53.13}} & \multicolumn{1}{c}{50.09} & \multicolumn{1}{c}{0.08} \\
& & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{54.87} & \multicolumn{1}{c}{49.29} & \multicolumn{1}{c}{\underline{34.36}} & \multicolumn{1}{c}{\underline{33.76}} & \multicolumn{1}{c}{33.32} & \multicolumn{1}{c}{47.50} & \multicolumn{1}{c}{63.96} & \multicolumn{1}{c}{85.00} & \multicolumn{1}{c}{78.31} & \multicolumn{1}{c}{56.86} & \multicolumn{1}{c}{52.29} & \multicolumn{1}{c}{\textbf{53.59}} & \multicolumn{1}{c}{2.19} \\
& & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{49.03} & \multicolumn{1}{c}{\underline{50.00}} & \multicolumn{1}{c}{33.32} & \multicolumn{1}{c}{33.52} & \multicolumn{1}{c}{33.50} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{\underline{66.35}} & \multicolumn{1}{c}{84.20} & \multicolumn{1}{c}{77.66} & \multicolumn{1}{c}{55.96} & \multicolumn{1}{c}{50.09} & \multicolumn{1}{c}{51.83} & \multicolumn{1}{c}{0.72} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{47.51} & \multicolumn{1}{c}{\underline{50.00}} & \multicolumn{1}{c}{33.30} & \multicolumn{1}{c}{33.30} & \multicolumn{1}{c}{32.97} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{64.67} & \multicolumn{1}{c}{84.20} & \multicolumn{1}{c}{76.77} & \multicolumn{1}{c}{54.60} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{51.26} & \multicolumn{1}{c}{0.41} \\
& & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{47.29} & \multicolumn{1}{c}{\underline{50.00}} & \multicolumn{1}{c}{33.50} & \multicolumn{1}{c}{33.30} & \multicolumn{1}{c}{33.12} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{63.84} & \multicolumn{1}{c}{82.00} & \multicolumn{1}{c}{63.09} & \multicolumn{1}{c}{51.64} & \multicolumn{1}{c}{50.13} & \multicolumn{1}{c}{49.49} & \multicolumn{1}{c}{0.53} \\
\cmidrule{2-16}
& \multicolumn{1}{l}{\multirow{3}{*}{\textit{early-fusion}}} &
\multicolumn{1}{c}{5} & \multicolumn{1}{c}{\underline{55.81}} & \multicolumn{1}{c}{47.50} & \multicolumn{1}{c}{33.88} & \multicolumn{1}{c}{32.48} & \multicolumn{1}{c}{32.60} & \multicolumn{1}{c}{47.88} & \multicolumn{1}{c}{65.35} & \multicolumn{1}{c}{84.80} & \multicolumn{1}{c}{\underline{79.20}} & \multicolumn{1}{c}{57.12} & \multicolumn{1}{c}{52.92} & \multicolumn{1}{c}{\textbf{53.59}} & \multicolumn{1}{c}{1.25} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{54.95} & \multicolumn{1}{c}{47.50} & \multicolumn{1}{c}{34.12} & \multicolumn{1}{c}{31.72} & \multicolumn{1}{c}{32.22} & \multicolumn{1}{c}{49.23} & \multicolumn{1}{c}{65.02} & \multicolumn{1}{c}{85.60} & \multicolumn{1}{c}{79.17} & \multicolumn{1}{c}{56.98} & \multicolumn{1}{c}{52.79} & \multicolumn{1}{c}{53.57} & \multicolumn{1}{c}{1.18} \\
& & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{54.37} & \multicolumn{1}{c}{47.14} & \multicolumn{1}{c}{33.94} & \multicolumn{1}{c}{31.52} & \multicolumn{1}{c}{31.58} & \multicolumn{1}{c}{\underline{49.42}} & \multicolumn{1}{c}{65.13} & \multicolumn{1}{c}{86.00} & \multicolumn{1}{c}{79.14} & \multicolumn{1}{c}{56.92} & \multicolumn{1}{c}{52.35} & \multicolumn{1}{c}{53.41} & \multicolumn{1}{c}{1.30} \\
\cmidrule{2-16}
& \multicolumn{1}{l}{\multirow{3}{*}{\textit{late-fusion}}} & 
\multicolumn{1}{c}{5} & \multicolumn{1}{c}{52.78} & \multicolumn{1}{c}{46.07} & \multicolumn{1}{c}{33.24} & \multicolumn{1}{c}{32.62} & \multicolumn{1}{c}{32.90} & \multicolumn{1}{c}{44.81} & \multicolumn{1}{c}{65.89} & \multicolumn{1}{c}{84.20} & \multicolumn{1}{c}{78.97} & \multicolumn{1}{c}{56.80} & \multicolumn{1}{c}{51.03} & \multicolumn{1}{c}{52.67} & \multicolumn{1}{c}{1.44} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{53.36} & \multicolumn{1}{c}{45.71} & \multicolumn{1}{c}{33.46} & \multicolumn{1}{c}{31.94} & \multicolumn{1}{c}{32.62} & \multicolumn{1}{c}{45.58} & \multicolumn{1}{c}{65.59} & \multicolumn{1}{c}{84.40} & \multicolumn{1}{c}{78.80} & \multicolumn{1}{c}{56.84} & \multicolumn{1}{c}{50.75} & \multicolumn{1}{c}{52.64} & \multicolumn{1}{c}{0.89} \\
& & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{53.07} & \multicolumn{1}{c}{46.07} & \multicolumn{1}{c}{33.30} & \multicolumn{1}{c}{31.58} & \multicolumn{1}{c}{32.85} & \multicolumn{1}{c}{46.54} & \multicolumn{1}{c}{65.73} & \multicolumn{1}{c}{83.60} & \multicolumn{1}{c}{78.92} & \multicolumn{1}{c}{56.90} & \multicolumn{1}{c}{50.19} & \multicolumn{1}{c}{52.61} & \multicolumn{1}{c}{1.01} \\
\bottomrule
\end{tabular}}
\end{adjustbox}
% \caption{opt모델, t5계열 우리 모델 성능 비교 with average minimal template}
\caption{\textbf{Detailed results for Figure~\ref{fig: mean and std of models}. }
The asterisk(*) on the right side of the task indicates that due to the large size of the test dataset, evaluation is performed on a random sample of 1K instances from the test dataset. 
The underlined values represent the highest scores within each model for each task, while the bold values represent the settings where the average scores for all the tasks are highest within each model.}
\label{tab: full results for seq2seq t0 datasets}
\end{table*}


% 3. opt 뿐 아니라 llama, palm, gpt3 등에 대해 비교하는 테이블. 동일 세팅은 아니지만 참고용인것 언급 (eleuther거 확인?)
\section{Comparing with other decoder LLMs}
\label{app: comparison with other LLMs}
% While numerous Large Language Models(LLMs) have their own benchmark evaluation scores reported, it is not guaranteed that these models are evaluated using the same baseline as our Evaluation Harness.
% While numerous Large Language Models have their own benchmark evaluation scores reported, it is not guaranteed that these models are evaluated using the same baseline as our Evaluation Harness.
While numerous LLMs have their own benchmark evaluation scores reported, it is not guaranteed that these models are evaluated using the same baseline.
% Despite this potential discrepancy, for reference purposes, we display the results of our baseline alongside with those of other significant LLMs in Table~\ref{tab: compare to other LLM decoders} including GPT-3, PaLM \cite{palm} and GPT-NeoX \cite{gpt-neox}.
Despite this potential discrepancy, to provide a point of reference, we present the results of our method alongside those of other well-known decoder-only LLMs in Table~\ref{tab: compare to other LLM decoders} including GPT-3, PaLM \cite{palm} and GPT-NeoX \cite{gpt-neox}.
All the records of decoder LLMs in Table~\ref{tab: compare to other LLM decoders} are from the paper.
Compared to PaLM-8B, T5 with \textit{early-fusion} achieves a higher average score.
% Though not fully reported, T5-\textit{FiD} beats GPT-NeoX in average and also shows much more competent performance in ANLI tasks compared to the GPT-3.
Although there is only a small overlap in tasks, T5-\textit{early} beats GPT-NeoX on average and demonstrates much better performance in ANLI tasks compared to GPT-3.
This indicates that our model performs well considering its size, as the throughput of the seq2seq model is comparable to that of a half-sized decoder-only model, as stated by \citet{ul2}.
In this regard, it is unfortunate that the absence of a larger encoder-decoder model hinders us from making an equivalent comparison with decoder models larger than 100B. 

\begin{table*}
\setlength\tabcolsep{8pt}
\centering
\begin{adjustbox}{width=1\textwidth}{
\begin{tabular}{lcccccccccccc}
\toprule
Model & RTE & CB & ALNI1 & ANLI2 & ANLI3 & WSC & Winogrande & COPA & StoryCloze & HellaSwag & WiC & \textbf{average} \\
\midrule
PaLM-8B & 56.7 & 57.1 & 29.8 & 32.5 & 32.7 & 83.2 & 70.1 & 86.0 & 81.5 & 68.6 & 52.4 & 59.1\\
% PaLM-62B & 76.5 & 78.6 & 38.2 & 35.3 & 40.8 & 88.8 & 79.8 & 93.0 & 86.7 & 80.0 & 57.5 \\
PaLM-540B & 81.2 & 89.3 & 56.9 & 56.1 & 51.2 & 89.5 & 85.1 & 95.0 & 89.0 & 83.8 & 64.6 & \textbf{76.5} \\
GPT-NeoX 20B & - & - & 32.2 & 33.1 & 34.6 & 38.5 & 68.3 & - & - & 53.8 & - & (43.4) \\
GPT3 175B & 72.9 & 82.1 & 36.8 & 34.0 & 40.2 & 75.0 & 77.7 & 92.0 & 87.7 & 79.3 & 55.3 & 66.6 \\
\midrule
T5-\textit{early} 11B & 64.0 & 77.9 & 42.1 & 38.3 & 40.0 & 68.3 & 62.5 & 83.8 & 75.3 & 50.4 & 50.2 & 59.3(50.3)\\
\bottomrule
\end{tabular}}
\end{adjustbox}
% \caption{opt모델, t5계열 우리 모델 성능 비교 with average minimal template}
\caption{\textbf{Comparison with other decoder LLMs for various NLU tasks.} The scores for PaLM and GPT-NeoX are 5-shot results. And the scores for GPT3 and T5-\textit{early} are the highest scores among the reported ones for each task. The average scores inside the parentheses indicate the average scores of tasks where the GPT-NeoX report. Note that normalization is applied to the scores for PaLM and GPT3, which might be advantageous compared to when it is not applied.}
\label{tab: compare to other LLM decoders}
\end{table*}


% 4. figure3에 대한 전체 표 

% 원래 figure 3에 대한 글이었던 것
\section{Additional results for Table~\ref{tab: compare decoder vs. t5} and Detailed results for Figure~\ref{fig: mean and std of models}}
\label{app: detailed results for fig3}
% In Figure~\ref{fig: mean and std of models}, we depict the average and standard deviation results for four seq2seq baselines for each methodology. 
% Additionally, we report the exact numeric results for the experiment in Table~\ref{tab: full results for seq2seq t0 datasets}. 
% The ``std'' values we report do not represent the standard deviation of scores across different tasks, but are calculated by averaging the standard deviation values across all tasks, with five seeds evaluated on each task.
% Despite the presence of disparities among individual models, the implementation of our methodologies results in a consistently marked increase in the average score, surpassing that of the original, while simultaneously reducing the standard deviation.

% 원래 테이블 3에 대한 글이었던 것
% In order to thoroughly analyze the performance of seq2seq baselines, we report the additive results evaluated on T5, T5-LM, T0, and UL2 for Table~\ref{tab: compare decoder vs. t5} in Table~\ref{tab: full results for seq2seq t0 datasets}.
% Although we compare various decoder-only models with T5 (with our methodologies applied), other encoder-decoder models such as T5-LM and UL2 also achieve significant scores that approximate those of the OPT-30B model, which is much larger.
% Surprisingly, for T0, although trained with 0-shot prompts, there were instances when applying our method resulted in higher performance than the 0-shot setting.
% In Table~\ref{tab: compare decoder vs. t5}, we present the number of demonstrations labeled as ``gpt3''. It refers to the highest-scoring shots verified in GPT-3. The corresponding numbers of shots are reported in Table~\ref{tab: gpt3 best shot}.
In order to thoroughly analyze the performance of seq2seq baselines, we report the additive results evaluated on T5, T5-LM, T0, and UL2 for Table~\ref{tab: compare decoder vs. t5} in Table~\ref{tab: full results for seq2seq t0 datasets}.
Although we compare various decoder-only models with T5 (with our methodologies applied), other encoder-decoder models such as T5-LM and UL2 also achieve significant scores that approximate those of the OPT-30B model, which is much larger.

% 두가지를 합친 버전
Table~\ref{tab: full results for seq2seq t0 datasets} is also the supplementary table for Figure~\ref{fig: mean and std of models}, which depicts the average and standard deviation results for four seq2seq baselines for each methodology. 
% Additionally, we report the exact numeric results for the experiment in Table~\ref{tab: full results for seq2seq t0 datasets}. 
The ``std'' values we report do not represent the standard deviation of scores across different tasks, but are calculated by averaging the standard deviation values across all tasks, with five seeds evaluated on each task.
% Despite the presence of disparities among individual models, the implementation of our methodologies results in a consistently marked increase in the average score, surpassing that of the \textit{original}, while simultaneously reducing the standard deviation.
By applying our approaches, the average scores consistently increase for all models, while simultaneously reducing the standard deviation.
% Surprisingly, for T0, although trained with 0-shot prompts, there were instances when applying our method resulted in higher performance than the 0-shot setting.

% 7. 각 베이스라인의 상세 parameter, inference IO setting 등 detail
% -all model details. token, size, flops 등
% -baseline들과 세팅 동일하게 한것, 논문에서 따온것 등
\section{Training details for baseline models}
\label{app: training details for baseline models}
Seq2seq models that we select as our baseline are trained in various manners. 
T5 is pretrained only with the denoising objective, utilizing a number of sentinel tokens that indicate where to be denoised. 
However, as the released version of T5 is a fine-tuned model, while T5.1.1 is trained solely on the C4 dataset with 1T tokens, we opt to utilize T5.1.1 as our T5 baseline.
% T5-LM is an additionally trained T5 model (100K steps) with a causal language modeling objective. 
T5-LM is an additionally trained T5 model with a causal language modeling objective.
% For 100K steps, it utilized 100M tokens for further training.
T0 is a variant of T5-LM, which is fine-tuned with plenty of prompts for various tasks.
It trained roughly 12B additional tokens compared to T5-LM.
While \citet{t0} present three types of T5-architecture models (T0, T0+, and T0++) based on variations of fine-tuning datasets, we choose T0 to ensure that the datasets used in our experiments have not been previously encountered during the training process.
Sentinel tokens were not used for further training both for T5-LM and T0.
Finally, UL2 is a variant model of T5, pretrained with three types of denoising objectives: R-, S-, and X-denoising. 
% All three objectives utilized the sentinel token, and are variations of denoising and causal language modeling. 
% All three objectives utilized the sentinel token, and 
All three objectives utilized the sentinel token, with ``R'' and ``S'' respectively corresponding to denoising and causal language modeling objectives.
``X'' represents the extreme version of both objectives.
% R- 과 S- 는 각각 denoising과 causal language modeling obsective에 해당하며, X-는 두 objective의 extreme version이다.
Similar to T5, it used the C4 dataset to train about 1 trillion tokens.
% The UL2 model has 20B parameters, while the rest have the same size of 11B parameters.
% If the same batch size and sequence length as T5 were used, about 100B tokens were additionally learned.
% About 100B tokens were additionally used for further training.
% This implies that training objective for T5-LM and T0 resembles the one with in-context learning.
% {https://huggingface.co/google/ul2}

All four seq2seq baseline models are basically pretrained with sequence lengths of 512~\cite{t5, ul2} or 1,024~\cite{t0}, which account for a quarter or half of the typical decoder LLMs~\cite{gpt3, opt, gopher}.
Therefore, the seq2seq model faces a disadvantage when the number of examples used for few-shot evaluation increases, as it lacks exposure to long-length inputs during training.
% Since \citet{ul2} compared the throughput based solely on the size of the decoder, and from this perspective, we have achieved considerably higher scores despite a significantly lower (less than half of) throughput compared to the decoder model used in the comparison.

According to \citet{opt}, the OPT models are trained on approximately up to 300B tokens, including the Pile \cite{pile} and the datasets used for RoBERTa \cite{roberta} pretraining.
In our experiments, we evaluate three different sizes of OPT models (13B, 30B, and 66B), which are up to 6 times larger than the T5 baseline model.
% At first, we conduct experiments using OPT models to compare our baseline models.
% The primary reason for this choice is that OPT models are the only open-sourced models available with comparable sizes to T5(11B) and UL2(20B), enabling us to experiment with our custom settings.
% During the experiment with OPT models, LLaMA models are released with various sizes so we choose to test with the 7B one and also with the same size of BLOOM model.
% BLOOM and LLaMA are also powerful open-source decoder models. 
% BLOOM is pretrained on about 340B tokens from the Pile dataset, while LLaMA is pretrained on around a trillion tokens with their own curated dataset. 
% For both of the models, we choose the one with the size of 7B parameters from each variant.
Additionally, to compare with decoder models smaller than 13B, we also evaluate the BLOOM-7B model, which is pretrained on approximately 340 billion tokens from the ROOTS corpus.

% 5. prompt selection 관련해서 더 쓸만한 내용
% - promptsource 및 minimal template에서 사용한 prompt 형태 쭉 나열 (alexa, t0 참고) -> 분량 보고 다 할지 일부만 보여줄지 결정(?)
% \section{Prompt selection for natural language generation tasks}
\section{Detailed information about prompts and few-shot demonstrations}
\label{app: prompt and demons details}
% For each predictions of each tasks, the different composition of few-shot demonstrations can have an impact on performance.


For the natural language generation tasks in Table~\ref{tab: xsum results} and Table~\ref{tab: webnlg results}, we adopt one of the prompts from the PromptSource~\cite{promptsource} since Language Model Evaluation Harness does not provide templates for XSum and WebNLG.
We manually select one of the prompts from the PromptSource that best describes the task.
We provide one-shot prompting samples utilizing the selected template in Figure~\ref{fig: xsum & webnlg prompt examples}.

With respect to the few-shot demonstrations, we distinguish between two settings where the demonstrations used for each prediction are maintained the same, referred to as the ``fixed'' setting, or randomly sampled at each time, referred to as the ``non-fixed'' setting.
For the generation tasks, we conduct experiments with a ``non-fixed'' setting, otherwise utilize a ``fixed'' setting.
% Unless otherwise specified, a fixed setting is used for conducting five experiments with different random seeds.
% For experiments that use only a single seed, a non-fixed setting is employed.

% To account for the potential performance variation due to different choices of few-shot demonstrations, we conduct five experiments with different random seeds and present the averaged scores, except for Table~\ref{tab: io-structure} and Table~\ref{tab: optimal-prompt}, where only a single seed is used.



% Figure environment removed

% 6. scoring 관련 bias 쓸 내용? -> normalize 관련
% hellaswag에만 하게 되어있는 점 모호. normalize 적용 X. string 기준 길이 normalize도 애매.
\section{Normalization to the scores}
\label{sec: normalization grounding}
As mentioned in Section~\ref{subsec: eval tasks and scoring method}, we do not apply any normalization to the output losses, with respect to the token or string length of the output sequences.
In the case of HellaSwag, applying normalization often leads to a higher score.
However, it may not hold true for other tasks.
To ensure consistent evaluation across all datasets, we decide to adopt a unified approach without normalization.
% Furthermore, it is typical not to apply length normalization with respect to the output sequences.

Additionally, in \citet{gpt3}, a pre-normalization step is conducted where calibration is applied to each calculation of scores.
With the same context mentioned in the paragraph above, we do not implement any preceding calibration to calculate the output scores.

\end{document}