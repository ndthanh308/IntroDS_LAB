% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@STRING{NIPS="Proc. the Advances in Neural Information Processing Systems (NeurIPS)"}
@STRING{ICML="Proc. the International Conference on Machine Learning (ICML)"}
@STRING{JMLR="Journal of Machine Learning Research (JMLR)"}
@STRING{ICWSM="Proc. the ACM International Conference on Web Search and Data Mining (ICWSM)"}
@STRING{SDM="Proc. the ACM International Conference on Web Search and Data Mining (SDM)"}
@STRING{ICDM="Proc. the IEEE International Conference on Data Mining (ICDM)"}
@STRING{WWW="Proc. the International Conference on World Wide Web (WWW)"}
@STRING{KDD="Proc. the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)"}
@STRING{IJCAI="Proc. the International Joint Conference on Artificial Intelligence (IJCAI)"}
@STRING{UAI="Proc. the Conference on Uncertainty in Artificial Intelligence (UAI)"}
@STRING{RecSys="Proc. the ACM Conference on Recommender systems (RecSys)"}
@STRING{ACL="Proc. the Annual Meeting of the Association for Computational Linguistics (ACL)"}
@STRING(ICDE="Proc. the International Conference on Data Engineering (ICDE)")
@STRING(CIKM="Proc. the ACM Conference on Information and Knowledge Management (CIKM)")
@STRING(TVCG="IEEE Transactions on Visualization and Computer Graphics (TVCG)")
@STRING(AAAI="Proc. the AAAI Conference on Artificial Intelligence (AAAI)")
@STRING(ICLR="Proc. the International Conference on Learning Representations (ICLR)")
@STRING(TOG="Proc. the ACM Transactions on Graphics (ToG)")
@STRING{TPAMI="The IEEE Transactions on Pattern Analysis and Machine Intelligence (TPMAI)"}
@STRING{EMNLP="Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP)"}
@STRING{CVPR="Proc. of the IEEE conference on computer vision and pattern recognition (CVPR)"}
@STRING{NAACL="Proc. of The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)"}
@STRING{ECCV="Proc. of the European Conference on Computer Vision (ECCV)"}
@STRING{ICCV="Proc. of the IEEE international conference on computer vision (ICCV)"}
@STRING{CHI="Proc. of the CHI Conference on Human Factors in Computing Systems (CHI)"}
@STRING{COLING="Proc. of the International Conference on Computational Linguistics (COLING)"}
@STRING{CORR="Computing Research Repository (CoRR)"}
@STRING{PBML="The Prague Bulletin of Mathematical Linguistics (PBML)"}
@STRING{MTJ="Machine Translation"}
@STRING{IJCNLP="Proc. of the International Joint Conference on Natural Language Processing (IJCNLP)"}
@STRING{MTS="Proc. of Machine Translation Summit"}
@STRING{EACL="Proc. of the Annual Conference of the European Chapter of the Association for Computational Linguistics (EACL)"}
@STRING{SMT="Proc. of the Workshop on Statistical Machine Translation (WMT)"}
@STRING{WMT="Proc. of the Conference on Machine Translation (WMT)"}
@STRING{AMTA="Proc. of the association for machine translation in the Americas (AMTA)"}
@STRING{TMLR="Transactions on Machine Learning Research (TMLR)"}
@STRING{ARXIV="ArXiv"}

@inproceedings{t5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  booktitle = JMLR,
  year    = {2020}
}

@inproceedings{t0,
    title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
    author={Victor Sanh and Albert Webson and Colin Raffel and Stephen Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Fevry and Jason Alan Fries and Ryan Teehan and Teven Le Scao and Stella Biderman and Leo Gao and Thomas Wolf and Alexander M Rush},
    booktitle=ICLR,
    year={2022}
}

@article{gpt3,
    title={Language Models are Few-Shot Learners},
    author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
    journal=ARXIV,
    year={2020}
}

@inproceedings{hyperclova,
    title = "What Changes Can Large-scale Language Models Bring? Intensive Study on {H}yper{CLOVA}: Billions-scale {K}orean Generative Pretrained Transformers",
    author = "Kim, Boseop  and
      Kim, HyoungSeok  and
      Lee, Sang-Woo  and
      Lee, Gichang  and
      Kwak, Donghyun  and
      Dong Hyeon, Jeon  and
      Park, Sunghyun  and
      Kim, Sungju  and
      Kim, Seonhoon  and
      Seo, Dongpil  and
      Lee, Heungsub  and
      Jeong, Minyoung  and
      Lee, Sungjae  and
      Kim, Minsub  and
      Ko, Suk Hyun  and
      Kim, Seokhun  and
      Park, Taeyong  and
      Kim, Jinuk  and
      Kang, Soyoung  and
      Ryu, Na-Hyeon  and
      Yoo, Kang Min  and
      Chang, Minsuk  and
      Suh, Soobin  and
      In, Sookyo  and
      Park, Jinseong  and
      Kim, Kyungduk  and
      Kim, Hiun  and
      Jeong, Jisu  and
      Yeo, Yong Goo  and
      Ham, Donghoon  and
      Park, Dongju  and
      Lee, Min Young  and
      Kang, Jaewook  and
      Kang, Inho  and
      Ha, Jung-Woo  and
      Park, Woomyoung  and
      Sung, Nako",
    booktitle = EMNLP,
    year = {2021}
}

@Article{alexaTM,
 author = {Saleh Soltan and Shankar Ananthakrishnan and Jack FitzGerald and Rahul Gupta and Wael Hamza and Haidar Khan and Charith Peris and Stephen Rawls and Andy Rosenbaum and Anna Rumshisky and Chandana Satya Prakash and Mukund Sridhar and Fabian Triefenbach and Apurv Verma and Gokhan Tur and Prem Natarajan},
 title = {AlexaTM 20B: Few-shot learning using a large-scale multilingual seq2seq model},
 year = {2022},
 journal = ARXIV
}

@article{structured,
 author = {Hao, Yaru and Sun, Yutao and Dong, Li and Han, Zhixiong and Gu, Yuxian and Wei, Furu},  
 title = {Structured Prompting: Scaling In-Context Learning to 1,000 Examples},
 year = {2022},
 journal = ARXIV
}

@inproceedings{fid,
    title = "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
    author = "Izacard, Gautier  and
      Grave, Edouard",
    booktitle = EACL,
    year = {2021}
}

@inproceedings{rag,
author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
year = {2020},
booktitle = NIPS
}

@inproceedings{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others}, 
  year={2022},
  booktitle=NIPS
}

@inproceedings{
alibi,
title={Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
author={Ofir Press and Noah Smith and Mike Lewis},
booktitle=ICLR,
year={2022}
}

@article{sap,
  author = {Patel, Ajay and Li, Bryan and Rasooli, Mohammad Sadegh and Constant, Noah and Raffel, Colin and Callison-Burch, Chris},
  title = {Bidirectional Language Models Are Also Few-shot Learners},  
  journal = ARXIV,
  year = {2022}
}

@article{ul2,
  title={Unifying Language Learning Paradigms},
  author={Yi Tay and Mostafa Dehghani and Vinh Quang Tran and Xavier Garc{\'i}a and Dara Bahri and Tal Schuster and Huaixiu Zheng and Neil Houlsby and Donald Metzler},
  journal=ARXIV,
  year={2022}
}

@inproceedings{
flan,
title={Finetuned Language Models are Zero-Shot Learners},
author={Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V Le},
booktitle=ICLR,
year={2022}
}

@inproceedings{promptsource,
    title = "{P}rompt{S}ource: An Integrated Development Environment and Repository for Natural Language Prompts",
    author = "Bach, Stephen  and
      Sanh, Victor  and
      Yong, Zheng Xin  and
      Webson, Albert  and
      Raffel, Colin  and
      Nayak, Nihal V.  and
      Sharma, Abheesht  and
      Kim, Taewoon  and
      Bari, M Saiful  and
      Fevry, Thibault  and
      Alyafeai, Zaid  and
      Dey, Manan  and
      Santilli, Andrea  and
      Sun, Zhiqing  and
      Ben-david, Srulik  and
      Xu, Canwen  and
      Chhablani, Gunjan  and
      Wang, Han  and
      Fries, Jason  and
      Al-shaibani, Maged  and
      Sharma, Shanya  and
      Thakker, Urmish  and
      Almubarak, Khalid  and
      Tang, Xiangru  and
      Radev, Dragomir  and
      Jiang, Mike Tian-jian  and
      Rush, Alexander",
    booktitle = ACL,
    year = "2022"
}

@article{winogrande,
    title={WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
    author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
    journal=ARXIV,
    year={2019}
}

@inproceedings{hellaswag,
    title = "{H}ella{S}wag: Can a Machine Really Finish Your Sentence?",
    author = "Zellers, Rowan  and
      Holtzman, Ari  and
      Bisk, Yonatan  and
      Farhadi, Ali  and
      Choi, Yejin",
    booktitle = ACL,
    year = "2019"
}

@inproceedings{anli,
    title = "Adversarial {NLI}: A New Benchmark for Natural Language Understanding",
    author = "Nie, Yixin  and
      Williams, Adina  and
      Dinan, Emily  and
      Bansal, Mohit  and
      Weston, Jason  and
      Kiela, Douwe",
    booktitle = ACL,
    year = "2020"
}

@inproceedings{storycloze,
    title = "A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories",
    author = "Mostafazadeh, Nasrin  and
      Chambers, Nathanael  and
      He, Xiaodong  and
      Parikh, Devi  and
      Batra, Dhruv  and
      Vanderwende, Lucy  and
      Kohli, Pushmeet  and
      Allen, James",
    booktitle = NAACL,
    year = "2016"
}

@inproceedings{t5-lm,
    title = "The Power of Scale for Parameter-Efficient Prompt Tuning",
    author = "Lester, Brian  and
      Al-Rfou, Rami  and
      Constant, Noah",
    booktitle = EMNLP,
    year = "2021"
}

@software{eval-harness,
  author       = {Gao, Leo and
                  Tow, Jonathan and
                  Biderman, Stella and
                  Black, Sid and
                  DiPofi, Anthony and
                  Foster, Charles and
                  Golding, Laurence and
                  Hsu, Jeffrey and
                  McDonell, Kyle and
                  Muennighoff, Niklas and
                  Phang, Jason and
                  Reynolds, Laria and
                  Tang, Eric and
                  Thite, Anish and
                  Wang, Ben and
                  Wang, Kevin and
                  Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  year         = 2021,
}


@article{scaling-laws,
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  title = {Scaling Laws for Neural Language Models},
  year = {2020},
  journal = ARXIV,
}

@article{why-gpt-learn,
  author = {Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Sui, Zhifang and Wei, Furu},
  title = {Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers},
  year = {2022},
  journal = ARXIV,
}

@inproceedings{superglue,
 author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
 title = {SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems},
 year = {2019},
 booktitle = NIPS,
}

@article{palm,
    title   = {PaLM: Scaling Language Modeling with Pathways},
    author  = {Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam M. Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Benton C. Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garc{\'i}a and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Oliveira Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathleen S. Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
    year    = {2022},
    journal = ARXIV
}

@article{gopher,
  title={Scaling Language Models: Methods, Analysis \& Insights from Training Gopher},
  author={Jack W. Rae and Sebastian Borgeaud and Trevor Cai and Katie Millican and Jordan Hoffmann and Francis Song and John Aslanides and Sarah Henderson and Roman Ring and Susannah Young and Eliza Rutherford and Tom Hennigan and Jacob Menick and Albin Cassirer and Richard Powell and George van den Driessche and Lisa Anne Hendricks and Maribeth Rauh and Po-Sen Huang and Amelia Glaese and Johannes Welbl and Sumanth Dathathri and Saffron Huang and Jonathan Uesato and John F. J. Mellor and Irina Higgins and Antonia Creswell and Nathan McAleese and Amy Wu and Erich Elsen and Siddhant M. Jayakumar and Elena Buchatskaya and David Budden and Esme Sutherland and Karen Simonyan and Michela Paganini and L. Sifre and Lena Martens and Xiang Lorraine Li and Adhiguna Kuncoro and Aida Nematzadeh and Elena Gribovskaya and Domenic Donato and Angeliki Lazaridou and Arthur Mensch and Jean-Baptiste Lespiau and Maria Tsimpoukelli and N. K. Grigorev and Doug Fritz and Thibault Sottiaux and Mantas Pajarskas and Tobias Pohlen and Zhitao Gong and Daniel Toyama and Cyprien de Masson d'Autume and Yujia Li and Tayfun Terzi and Vladimir Mikulik and Igor Babuschkin and Aidan Clark and Diego de Las Casas and Aurelia Guy and Chris Jones and James Bradbury and Matthew G. Johnson and Blake A. Hechtman and Laura Weidinger and Iason Gabriel and William S. Isaac and Edward Lockhart and Simon Osindero and Laura Rimell and Chris Dyer and Oriol Vinyals and Kareem W. Ayoub and Jeff Stanway and L. L. Bennett and Demis Hassabis and Koray Kavukcuoglu and Geoffrey Irving},
  journal=ARXIV,
  year={2021}
}

@inproceedings{
emergent,
title={Emergent Abilities of Large Language Models},
author={Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
booktitle=TMLR,
year={2022},
}


@article{opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  year={2022},
  journal = ARXIV,
}

@article{flan2022,
    author = {Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V. and Zoph, Barret and Wei, Jason and Roberts, Adam},
    title = {The Flan Collection: Designing Data and Methods for Effective Instruction Tuning},
    year = {2023},
    journal = ARXIV,
}


@article{opt-iml,
    author = {Iyer, Srinivasan and Lin, Xi Victoria and Pasunuru, Ramakanth and Mihaylov, Todor and Simig, Daniel and Yu, Ping and Shuster, Kurt and Wang, Tianlu and Liu, Qing and Koura, Punit Singh and Li, Xian and O'Horo, Brian and Pereyra, Gabriel and Wang, Jeff and Dewan, Christopher and Celikyilmaz, Asli and Zettlemoyer, Luke and Stoyanov, Ves},
    title = {OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization},
    year = {2022},
    journal = ARXIV,
}

@inproceedings{transformer,
    author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
    title = {Attention Is All You Need},
    year = {2017},
    booktitle = NIPS,
}

@article{bert,
    author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
    title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
    year = {2018},
    journal = ARXIV,
}

@article{gpt1,
    author={Alec Radford and Karthik Narasimhan},
    title={Improving Language Understanding by Generative Pre-Training},
    year={2018},
    journal = ARXIV,
}

@article{bart,
  author    = {Mike Lewis and
               Yinhan Liu and
               Naman Goyal and
               Marjan Ghazvininejad and
               Abdelrahman Mohamed and
               Omer Levy and
               Veselin Stoyanov and
               Luke Zettlemoyer},
  title     = {{BART:} Denoising Sequence-to-Sequence Pre-training for Natural Language
               Generation, Translation, and Comprehension},
  year      = {2019},
  journal = ARXIV,
}

@inproceedings{glm,
  author    = {Zhengxiao Du and
               Yujie Qian and
               Xiao Liu and
               Ming Ding and
               Jiezhong Qiu and
               Zhilin Yang and
               Jie Tang},
  title     = {{GLM:} General Language Model Pretraining with Autoregressive Blank Infilling},
  booktitle = ACL,
  year      = {2022},
}

@article{gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019},
  journal = ARXIV,
}

@inproceedings{gpt-neox,
  title={{GPT-NeoX-20B}: An Open-Source Autoregressive Language Model},
  author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and Pieler, Michael and Prashanth, USVSN Sai and Purohit, Shivanshu and Reynolds, Laria and Tow, Jonathan and Wang, Ben and Weinbach, Samuel},
  booktitle=ACL,
  year={2022}
}

@inproceedings{unilm,
title = {Unified Language Model Pre-Training for Natural Language Understanding and Generation},
author = {Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
year = {2019},
booktitle = NIPS,
}

@inproceedings{rpe,
    title = "Self-Attention with Relative Position Representations",
    author = "Shaw, Peter and 
    Uszkoreit, Jakob and 
    Vaswani, Ashish",
    booktitle = NAACL,
    year = "2018"
}

@article{llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  journal=ARXIV,
  year={2023}
}

@article{pile,
  title={The {P}ile: An 800GB Dataset of Diverse Text for Language Modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  journal=ARXIV,
  year={2020}
}

@article{roberta,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      journal=ARXIV,
}

@article{bloom,
      title={BLOOM: A 176B-Parameter Open-Access Multilingual Language Model}, 
      author={BigScience Workshop},
      year={2023},
      journal=ARXIV,
}

@article{pythia,
      title={Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling}, 
      author={Stella Biderman and Hailey Schoelkopf and Quentin Anthony and Herbie Bradley and Kyle O'Brien and Eric Hallahan and Mohammad Aflah Khan and Shivanshu Purohit and USVSN Sai Prashanth and Edward Raff and Aviya Skowron and Lintang Sutawika and Oskar van der Wal},
      year={2023},
      journal=ARXIV,
}

@InProceedings{xsum,
  author =      {Shashi Narayan and Shay B. Cohen and Mirella Lapata},
  title =       {Don't Give Me the Details, Just the Summary! {T}opic-Aware Convolutional Neural Networks for Extreme Summarization},
  booktitle =   EMNLP,
  year =        {2018},
}

@article{codet5+,
      title={CodeT5+: Open Code Large Language Models for Code Understanding and Generation}, 
      author={Yue Wang and Hung Le and Akhilesh Deepak Gotmare and Nghi D. Q. Bui and Junnan Li and Steven C. H. Hoi},
      year={2023},
      journal=ARXIV,
}

@article{glm130b,
      title={GLM-130B: An Open Bilingual Pre-trained Model}, 
      author={Aohan Zeng and Xiao Liu and Zhengxiao Du and Zihan Wang and Hanyu Lai and Ming Ding and Zhuoyi Yang and Yifan Xu and Wendi Zheng and Xiao Xia and Weng Lam Tam and Zixuan Ma and Yufei Xue and Jidong Zhai and Wenguang Chen and Peng Zhang and Yuxiao Dong and Jie Tang},
      year={2022},
      journal=ARXIV,
}

@inproceedings{rouge,
    title = {ROUGE: A Package for Automatic Evaluation of Summaries},
    author = {Lin, Chin-Yew},
    booktitle = "Text Summarization Branches Out",
    year = {2004},
}

@article{webnlg,
      title={Step-by-Step: Separating Planning from Realization in Neural Data-to-Text Generation}, 
      author={Amit Moryossef and Yoav Goldberg and Ido Dagan},
      year={2019},
      journal=ARXIV,
}

@article{gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      journal=ARXIV,
}