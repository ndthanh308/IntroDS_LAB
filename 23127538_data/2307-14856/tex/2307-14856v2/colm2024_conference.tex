
\documentclass{article} % For LaTeX2e
\usepackage{colm2024_conference}

\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}

%custom packages
\usepackage{subcaption}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{changepage}
\usepackage{booktabs}
\usepackage{verbatim}
\usepackage{bbm}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{placeins}
\usepackage{anyfontsize}
\newcommand*{\affmark}[1][*]{\textsuperscript{#1}}
\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}%
  \hypersetup{pdfborder={0 0 0}} % 주석 번호 주위의 경계선을 제거
  \footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

% \input{math_commands.tex}

\title{Exploiting the Potential of Seq2Seq Models as Robust Few-Shot Learners}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \colmfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Jihyeon Lee\affmark[*\dag2], Dain Kim\affmark[*\dag3], Doohae Jung\affmark[*1], Boseop Kim\affmark[1], Kyoung-Woon On\affmark[1] \\
Kakao Corp., NVIDIA, Kyunghee University\\
% Pittsburgh, PA 15213, USA \\
\texttt{jihyeonl@nvidia.com}, \texttt{dannykm@khu.ac.kr}, \texttt{\{wavy.jung,mat.mul,kloud.off\}@kakaocorp.com} \\
}
% \author{Jihyeon Lee\affmark[*\dag]\\
% {\tt\small gina.lee@kakaobrain.com}
% \And
% Taehee Kim\affmark[*]\\
% {\tt\small taehee.kim@letsur.ai}\\
% {\tt\small taeheekim@kaist.ac.kr}
% \And
% Yunwon Tae\affmark[*\ddag]\\
% {\tt\small yunwon.tae@vuno.co}
% \AND
% Cheonbok Park\\
% {\tt\small cbok.park@navercorp.com}
% \And
% Jaegul Choo\\
% {\tt\small jchoo@kaist.ac.kr}
% }
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\colmfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
In-context learning, which offers substantial advantages over fine-tuning, is predominantly observed in decoder-only models, while encoder-decoder (i.e., seq2seq) models excel in methods that rely on weight updates. Recently, a few studies have demonstrated the feasibility of few-shot learning with seq2seq models; however, this has been limited to tasks that align well with the seq2seq architecture, such as summarization and translation. Inspired by these initial studies, we provide a first-ever extensive experiment comparing the in-context few-shot learning capabilities of decoder-only and encoder-decoder models on a broad range of tasks. Furthermore, we propose two methods to more effectively elicit in-context learning ability in seq2seq models: objective-aligned prompting and a fusion-based approach. Remarkably, our approach outperforms a decoder-only model that is six times larger and exhibits significant performance improvements compared to conventional seq2seq models across a variety of settings. We posit that, with the right configuration and prompt design, seq2seq models can be highly effective few-shot learners for a wide spectrum of applications.
~\blfootnote{* Indicates equal contribution}
~\blfootnote{\dag Work done at Kakao Corp., correspondence to jihyeonl@nvidia.com and dannykm@khu.ac.kr}
% ~\footnote{We will release the toolkit for the in-context evaluation of seq2seq models.}
\end{abstract}


\section{Introduction}
\label{sec: introduction}

Recent studies have demonstrated that large language models can possess entirely different competencies, referred to as emergent abilities~\citep{gpt3, palm, gopher, emergent}. 
The concept of emergent abilities in Large Language Models (LLMs), initially introduced by \citet{emergent}, posits that increasing the model size and dataset can lead to the sudden emergence of abilities such as in-context learning, complex reasoning, and common-sense reasoning. 
In-context learning, in particular, one of the distinct characteristics of LLMs, serves as a key metric for assessing their effectiveness.


In-context learning refers to the ability of the model to perform tasks by leveraging contextual information provided through prompts, without the need for additional training.
Specifically, in the case of in-context few-shot learning, the model can generate suitable outputs for the desired target input by using a small number of examples. This offers a step beyond traditional frameworks, which typically require extensive training data and fine-tuning of the model. 
Instead, it presents a new paradigm where a single model can effortlessly perform new tasks without necessitating a separate training process.

The capability of in-context learning has been predominantly explored in decoder-only models, as the rapid evolution of pretrained models has focused on these unidirectional architectures. However, the sequence-to-sequence (seq2seq) architecture, despite its significant advantage of encoding contexts without sacrificing bidirectionality, has not been extensively investigated regarding its potential for in-context learning capabilities~\citep{t0, ul2, alexaTM, sap, codet5+}.

\citet{t0} trained an encoder-decoder model using multitask prompts in a supervised manner, but this approach only enabled zero-shot generalization to new tasks. 
\citet{alexaTM} primarily demonstrated the in-context abilities of seq2seq models for generative tasks such as summarization and translation, which are tasks inherently well-suited for seq2seq models. 
They also reported performance on natural language understanding (NLU) benchmarks but only in a zero-shot scenario, making it difficult to confirm their in-context learning proficiency across a wide range of tasks.

% Figure environment removed

Motivated by the current landscape, for the first time, we thoroughly investigate the zero-shot to few-shot performance of seq2seq models across a wide range of evaluation sets. 
Our findings demonstrate that seq2seq models also serve as robust few-shot learners not only for generation tasks but also for understanding tasks, even outperforming their larger decoder-only counterparts~\citep{opt}.


We conduct various experiments on how to structure the prompts in seq2seq models.
While \citet{t5, ul2} popularized the usage of sentinel tokens during the pretraining stage of seq2seq models to optimize the denoising objective, the proper application of the sentinel tokens during the inference stage (i.e., in-context learning) is less well-established, and is often not even mentioned in most studies~\citep{t0, ul2}.
We find that aligning prompts with the pretraining objective yields up to a +20.5\%p performance improvement on the SuperGLUE~\citep{superglue} benchmark.

Furthermore, we propose two fusion-based approaches that enhance the few-shot learning capability of encoder-decoder architecture models. 
The fundamental idea is to independently process each of the few-shot examples through an encoder and then merge these representations for decoding. 
This fusion-based approach offers several advantages. 
Firstly, it addresses the restricted maximum sequence length commonly found in seq2seq models~\citep{t5, ul2, t5-lm}, which is shorter than decoder-only models, by leveraging encoding parallelization. 
Secondly, we achieve more efficient encoding of multiple examples by avoiding unnecessary bidirectional attention calculations between them. 
Our approach demonstrates significant performance improvements in seq2seq settings, surpassing the OPT 66B model~\citep{opt} across various tasks. 
Despite the absence of complete attention across the contexts, our approach consistently outperforms traditional methods. 
Moreover, our methodologies effectively eliminate the permutation bias commonly observed in few-shot learning scenarios.

Overall, our work reveals the few-shot capability of seq2seq models, which has been undervalued compared to their zero-shot and fine-tuned counterparts.
To summarize, our key contributions are:
1) we develop an in-context evaluation toolkit for seq2seq models and conduct extensive experiments to investigate the performance of seq2seq models in zero-shot to few-shot scenarios using fair criteria, 
2) we exploit the potential of few-shot learning in encoder-decoder models by exploring prompting strategies and fusion-based approaches, and
3) we experimentally demonstrate that the seq2seq model can outperform the decoder-only model with 6 times larger parameters across diverse datasets.
In Section~\ref{sec: discussion&conclusion}, we discuss the impact of these unprecedented findings on the evolution of LLMs.

\section{Exploring prompting strategies for seq2seq models in in-context learning}
\label{sec: closer look at io-structure}

We begin by demonstrating the different prompting strategies suitable for in-context learning of encoder-decoder models.
Since decoder-only models share a unified left-to-right attention architecture~\citep{gpt1}, we can naturally feed the demonstrations and target input in a sequential manner and perform generation to obtain the target output.
For instance, in a 5-shot English-French translation task, the input sequence is formed by concatenating five English-to-French demonstrations, followed by target English input, with the model expected to generate the corresponding French translation as the target output.

However, encoder-decoder models process inputs and outputs independently from the encoder and decoder, respectively, which can result in varying compositions of demonstrations, target input, and target output, as shown in Figure~\ref{fig: pretrain-align-is-important}-(a).
The target input can be utilized either as an encoder input, resulting in the decoder generating only the target output, or as a decoder input, enabling the decoder to directly generate the answer conditioned on the target input.
We evaluate this ablation on the SuperGLUE dataset, with 1, 5, and 10-shot settings using four representative encoder-decoder models.
Table~\ref{tab: io-structure} shows that positioning the target input on the encoder side results in enhanced performance for all model types, with performance gains of up to +20.5\%p.
We interpret these results as follows: During the pretraining stage, the model generates output conditioned on bidirectionally encoded information from the encoder, instead of relying on the preceding textual information from the decoder. As a result, placing the target input on the encoder side shows better performance.

\begin{table}[t]
\fontsize{9pt}{9pt}\selectfont
\setlength\tabcolsep{8pt}
\centering
\begin{center}
\begin{tabular}{lccc}
\toprule
Model & \textit{encoder} & \textit{decoder} \\
 & (1/5/10-shot) & (1/5/10-shot) \\
\midrule
T5 & \textbf{65.53}/\textbf{59.54}/\textbf{59.09} & 44.95/47.37/47.61 \\
T5-LM & \textbf{61.72}/\textbf{58.77}/\textbf{59.74} & 51.76/53.85/54.68 \\
T0 & \textbf{64.35}/\textbf{67.45}/\textbf{63.42} & 52.47/53.97/53.36 \\
UL2 & \textbf{60.05}/\textbf{58.85}/\textbf{60.02} & 51.46/57.41/59.14 \\
\bottomrule
\end{tabular}
\caption{\textbf{Ablation on the placement of target input.} 
The \textit{encoder} places the target input on the encoder side, whereas the \textit{decoder} places it on the decoder side. Bold denotes the best score within each model and for the specific number of shots. The complete results are reported in Appendix~\ref{app: detail table 1}.}
\label{tab: io-structure}
\end{center}
\vspace{-3mm}
\end{table}

% 소수점 이하 한 자리
\begin{table}
% \vspace{-3mm}
\fontsize{9pt}{9pt}\selectfont
\centering
\begin{tabular}{lcccc}
\toprule
Model & \textit{vanilla} & \textit{w/ sentinel} & \textit{w/ mode tag} \\
& (0/1/5/10-shot) & (0/1/5/10-shot) & (0/1/5/10-shot) \\
\midrule
T5 & 51.7/54.6/52.2/52.1 & \textbf{52.9}/\textbf{65.5}/\textbf{59.5}/\textbf{59.1} & - \\
T5-LM & 56.0/50.4/56.6/56.9 & \textbf{59.5}/\textbf{61.7}/\textbf{58.8}/\textbf{59.7} & - \\
T0 & \textbf{73.3}/\textbf{67.3}/60.4/56.8 & 72.2/64.4/\textbf{67.5}/\textbf{63.4} & - \\
UL2 & 52.4/50.1/48.4/49.3 & \textbf{58.8}/60.1/58.9/60.0 & 58.5/\textbf{62.2}/\textbf{61.2}/\textbf{62.3} \\
\bottomrule
\end{tabular}
\caption{\textbf{Ablation on the usage of sentinel tokens and mode tags.}
Figure~\ref{fig: pretrain-align-is-important}-(b) depicts the structures of the \textit{vanilla}, \textit{w/ sentinel}, and \textit{w/ mode tag} types.
For \textit{w/ mode} setting, the sentinel token is also utilized.
Bold denotes the best score within each model and for the specific number of shots. 
Due to space constraints, scores are expressed up to the first decimal place.
The complete results are reported in Appendix~\ref{app: detail table 2}.}
\label{tab: optimal-prompt}
\vspace{-3mm}
\end{table}

Additionally, recent models include T5~\citep{t5} and T5 variants~\citep{t5-lm, t0, ul2, sap} suggest different training objectives for language modeling.
\citet{t5, t5-lm, t0} employ denoising objectives to learn better bidirectional representations and utilize sentinel tokens to replace consecutive spans of tokens. 
\citet{ul2} proposed the model UL2, which additionally introduces the concept of mode switching with extra paradigm tags (i.e., [NLG],[NLU],[S2S]) that help the model learn suitable representations for a given task.
% \citet{ul2} proposed the model UL2, where the concept of mode switching with extra paradigm tags (i.e., [NLG],[NLU],[S2S]) is additionally applied in order for the model to learn suitable representations for a given task.

We argue that, even when performing downstream tasks, it is crucial to design the prompt to resemble the pretraining scheme to achieve optimal performance.
For instance, T5 manipulates inputs by masking consecutive tokens of the original text with a single sentinel token and then generates the target sequence by conditioning on that sentinel token. Thus, unlike decoder-only models that simply list examples as inputs, it is a more natural setting to prepend a sentinel token to the target for in-context learning using T5.
As depicted in Figure~\ref{fig: pretrain-align-is-important}-(b), we conduct experiments by incorporating a sentinel token and a mode tag into the vanilla prompt format.
Since the mode tag is exclusively used in UL2, it is not applied to the remaining models.
We assess the performance on the SuperGLUE datasets with various number of few-shot examples.
In accordance with the pretraining methodologies of the models, we position the sentinel token at the end of the input and the model tag at the beginning.
The results are presented in Table~\ref{tab: optimal-prompt}.
Following the pretraining objective yields a maximum performance gain of up to +13\%p.
As expected, aligning with the pretraining objective produces the most favorable results, with a significant discrepancy in scores.
% It is noteworthy that even T5-LM~\citep{t5-lm} and T0~\citep{t0}, which are trained without using the sentinel token during subsequent training, demonstrate the positive impact of adding the sentinel token to the prompt. 
It is noteworthy that even T5-LM~\citep{t5-lm} and T0~\citep{t0}, which have subsequent training phase without using sentinel token, demonstrate the positive impact of adding the sentinel token to the prompt.
Throughout the following sections, we employ the optimal objective-aligned prompting strategies that we identified in this section, as the default configuration in all experiments.

\section{Fusion-based approaches for few-shot learning}
\label{sec: leveraging retrieval}

In this section, we address another factor that impairs the performance of few-shot learning and propose two fusion-based approaches to mitigate this problem.
T5-family models~\citep{t5, t5-lm, t0, ul2} utilize relative position encoding (RPE)~\citep{rpe}, thereby enabling to process long input sequences as long as computing memory allows. 
However, as shown in \citet{alibi}, the extrapolation ability of T5-style RPE diminishes when the input length exceeds twice the maximum pretrained sequence length. 
That is, encoder-decoder models with shorter training sequence lengths of 512~\citep{t5, ul2}, compared to decoder-only models ~\citep{opt, gpt3, gpt-neox}, often exhibit limited performance when dealing with long-context few-shot learning scenarios.
Additionally, there is a challenge in terms of computational cost, which escalates quadratically as the number of shots increases.
Another challenge to address is the presence of permutation bias.
The order of demonstrations affects the positional embedding of tokens, resulting in different predictions even when the examples are identical but the order varies.

To address these problems, we propose a fusion-based approach, in which each demonstration is independently processed by the encoder.
Afterwards, we merge them together at a specific stage.
We hypothesize that the encoding of relations between demonstrations does not significantly impact in-context learning performance. 
We categorize this approach into two styles: \textit{early-fusion} and \textit{late-fusion}, differentiated by the stage at which the fusion occurs.
These concepts are inspired by retrieval-based generation models.
\citet{rag} propose Retrieval-Augmented Generation (RAG), 
where each retrieved document is integrated with the query and passed through the model independently, and the final output probability is calculated as a weighted sum of the probabilities of each output.
Fusion-in-Decoder (Fid), introduced by \citet{fid}, differs from RAG in that the inputs passed through the encoder are concatenated before being sent to the decoder without any weighting. This allows the model to perform fusion at the decoder level.
We apply these ideas to few-shot learning of seq2seq models by treating each few-shot example as a retrieved document.
Therefore, we use the term \textit{late-fusion} to refer to the RAG-style method that combines the decoder outputs, whereas the term \textit{early-fusion} refers to the FiD-style method that combines the encoder outputs.
The formulas are described below.
The traditional seq2seq in-context learning, which we refer to as the \textit{original} method, constructs the encoder input by concatenating the target input $x$ with the few-shot examples $z$ to generate the target output $y$ in the decoder~\citep{t5, ul2}. 
The probability of the target output for \textit{original} method can be expressed as 
\begin{align*} P_{\text{\textit{origin}}}(y|x,z)  &\approx \prod_i^N f_{\text{dec}}(y_i|f_{\text{enc}}(z_{1:k}, x),y_{1:i-1}) \end{align*}
where $f_{enc}$ and $f_{dec}$ are encoder and decoder, respectively, and $k$ denotes the number of shots. Here, $y_{i}$ is the $i$-th target token and $y_{1:i-1}$ is the partial output.
In the \textit{late-fusion} approach, as depicted in Figure~\ref{fig: main-figure}-(b), $j$-th example $z_j$ is concatenated to $x$ and each combination is utilized separately as the model input.
At each token generation step $i$, the information from $k$ instances that have passed through the entire model is aggregated as follows:
\begin{align*} P_{\text{\textit{late}}}(y|x,z) \approx \prod_i^N \sum_j^k f_{\text{dec}}(y_i|f_{\text{enc}}(z_j, x),y_{1:i-1}) \end{align*}
Unlike the original RAG model, in our few-shot settings, examples are obtained without going through the retrieval process. Therefore, we consider all examples to be equally retrieved, and no weighting is applied to individual examples.

% Figure environment removed

Instead of combining the decoder outputs, \textit{early-fusion} approach merges the information from the encoder output and passes it to the decoder, as shown in Figure~\ref{fig: main-figure}-(a).
All the last hidden states of the encoder, $h_1, h_2, ..., h_k$, where $h_j=f_{\text{enc}}(z_j, x)$, are concatenated sequentially and used for cross-attention,
\begin{align*} P_{\text{\textit{early}}}(y|x,z) &\approx    \prod_i^N f_{\text{dec}}(y_i|[h_1,h_2,...,h_k],y_{1:i-1}) \end{align*}

These fusion-based approaches successfully mitigate the extrapolation problem by maintaining a shorter length of the encoder input. 
Moreover, inference time can be reduced in proportion to the number of examples $k$ through batch processing.
In the following sections, we experimentally demonstrate the capability of the seq2seq model as a robust few-shot learner by integrating the prompting strategies proposed in Section~\ref{sec: closer look at io-structure} with the fusion approaches presented in this section.

\section{Experimental setup}
\label{sec: experimental setup}
We provide a detailed description of our experimental setup in the following paragraphs.
To ensure a fair evaluation, we employ identical prompt structures, evaluation tasks, scoring methods, prompt templates, and few-shot demonstrations across all baseline models.

\subsection{Baseline models}
\label{subsec: baseline models}

To test the impacts of our proposed methods on seq2seq models, we employ T5 and its variants, including T5-LM, T0, and UL2, as baseline models.
Among seq2seq baseline models, T0 is the only model fine-tuned with plenty of zero-shot multitask prompts, and UL2 contains 20B parameters while others contain 11B.
For the comparison with decoder-only models, we adopt the following as decoder baselines: OPT~\citep{opt} and BLOOM~\citep{bloom}, with parameter sizes ranging from 7B to 66B.
We provide detailed information about our baseline models in Appendix~\ref{app: training details for baseline models}.

\FloatBarrier
\begin{table*}[!ht]
\setlength\tabcolsep{2.3pt}
% \setlength\tabcolsep{0.5pt}
\begin{center}
\begin{adjustbox}{width=1\textwidth}{
\begin{tabular}{lccccccccccccc}
\toprule
Model & Shot & RTE & CB & ANLI R1 & ANLI R2 & ANLI R3 & WSC & Winogrande & COPA & StoryCloze & HellaSwag* & WiC & \textbf{average}\\
\midrule
\multicolumn{1}{l}{\multirow{3}{*}{{BLOOM-7B}}} & 
\multicolumn{1}{c}{5} & \multicolumn{1}{c}{59.35} & \multicolumn{1}{c}{51.43} & \multicolumn{1}{c}{33.66} & \multicolumn{1}{c}{33.92} & \multicolumn{1}{c}{34.22} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{65.02} & \multicolumn{1}{c}{75.60} & \multicolumn{1}{c}{70.83} & \multicolumn{1}{c}{46.32} & \multicolumn{1}{c}{50.63} & \multicolumn{1}{c}{50.68} \\
& \multicolumn{1}{c}{10} & \multicolumn{1}{c}{57.11} & \multicolumn{1}{c}{48.21} & \multicolumn{1}{c}{33.84} & \multicolumn{1}{c}{33.44} & \multicolumn{1}{c}{33.95} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{64.61} & \multicolumn{1}{c}{75.00} & \multicolumn{1}{c}{71.47} & \multicolumn{1}{c}{46.04} & \multicolumn{1}{c}{50.03} & \multicolumn{1}{c}{50.02} \\
& \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{57.47} & \multicolumn{1}{c}{57.14} & \multicolumn{1}{c}{33.40} & \multicolumn{1}{c}{33.64} & \multicolumn{1}{c}{34.62} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{64.75} & \multicolumn{1}{c}{78.80} & \multicolumn{1}{c}{71.96} & \multicolumn{1}{c}{46.62} & \multicolumn{1}{c}{50.03} & \multicolumn{1}{c}{51.36} \\
\midrule
\multicolumn{1}{l}{\multirow{3}{*}{{OPT-13B}}} & 
\multicolumn{1}{c}{5} & \multicolumn{1}{c}{50.47} & \multicolumn{1}{c}{33.57} & \multicolumn{1}{c}{34.24} & \multicolumn{1}{c}{33.16} & \multicolumn{1}{c}{34.05} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{68.68} & \multicolumn{1}{c}{86.00} & \multicolumn{1}{c}{79.02} & \multicolumn{1}{c}{52.76} & \multicolumn{1}{c}{52.01} & \multicolumn{1}{c}{50.95} \\
&\multicolumn{1}{c}{10} & \multicolumn{1}{c}{49.17} & \multicolumn{1}{c}{42.14} & \multicolumn{1}{c}{34.04} & \multicolumn{1}{c}{33.52} & \multicolumn{1}{c}{35.00} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{68.29} & \multicolumn{1}{c}{86.40} & \multicolumn{1}{c}{79.70} & \multicolumn{1}{c}{52.54} & \multicolumn{1}{c}{\underline{52.79}} & \multicolumn{1}{c}{51.83} \\
&\multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{49.39} & \multicolumn{1}{c}{40.71} & \multicolumn{1}{c}{35.04} & \multicolumn{1}{c}{32.90} & \multicolumn{1}{c}{35.70} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{68.59} & \multicolumn{1}{c}{87.40} & \multicolumn{1}{c}{80.17} & \multicolumn{1}{c}{52.88} & \multicolumn{1}{c}{\underline{52.45}} & \multicolumn{1}{c}{51.98} \\
\midrule
\multicolumn{1}{l}{\multirow{3}{*}{{OPT-30B}}} & 
\multicolumn{1}{c}{5} & \multicolumn{1}{c}{63.61} & \multicolumn{1}{c}{38.21} & \multicolumn{1}{c}{30.75} & \multicolumn{1}{c}{33.64} & \multicolumn{1}{c}{32.53} & \multicolumn{1}{c}{38.65} & \multicolumn{1}{c}{69.49} & \multicolumn{1}{c}{84.80} & \multicolumn{1}{c}{78.87} & \multicolumn{1}{c}{54.82} & \multicolumn{1}{c}{\underline{52.63}} & \multicolumn{1}{c}{52.55} \\
&\multicolumn{1}{c}{10} & \multicolumn{1}{c}{61.66} & \multicolumn{1}{c}{41.07} & \multicolumn{1}{c}{31.53} & \multicolumn{1}{c}{30.60} & \multicolumn{1}{c}{34.15} & \multicolumn{1}{c}{36.35} & \multicolumn{1}{c}{\underline{70.89}} & \multicolumn{1}{c}{84.60} & \multicolumn{1}{c}{79.58} & \multicolumn{1}{c}{55.26} & \multicolumn{1}{c}{51.22} & \multicolumn{1}{c}{52.45} \\
&\multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{62.67} & \multicolumn{1}{c}{58.57} & \multicolumn{1}{c}{31.78} & \multicolumn{1}{c}{31.56} & \multicolumn{1}{c}{32.72} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{70.75} & \multicolumn{1}{c}{86.60} & \multicolumn{1}{c}{80.57} & \multicolumn{1}{c}{55.70} & \multicolumn{1}{c}{52.23} & \multicolumn{1}{c}{54.52} \\
\midrule
\multicolumn{1}{l}{\multirow{3}{*}{{OPT-66B}}} & 
\multicolumn{1}{c}{5} & \multicolumn{1}{c}{\underline{66.14}} & \multicolumn{1}{c}{49.29} & \multicolumn{1}{c}{32.83} & \multicolumn{1}{c}{33.56} & \multicolumn{1}{c}{34.23} & \multicolumn{1}{c}{36.35} & \multicolumn{1}{c}{\underline{70.13}} & \multicolumn{1}{c}{\underline{88.40}} & \multicolumn{1}{c}{\underline{80.72}} & \multicolumn{1}{c}{\underline{56.72}} & \multicolumn{1}{c}{51.32} & \multicolumn{1}{c}{54.52} \\
&\multicolumn{1}{c}{10} & \multicolumn{1}{c}{\underline{63.68}} & \multicolumn{1}{c}{56.79} & \multicolumn{1}{c}{32.50} & \multicolumn{1}{c}{33.88} & \multicolumn{1}{c}{34.45} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{70.31} & \multicolumn{1}{c}{\underline{88.40}} & \multicolumn{1}{c}{\underline{81.53}} & \multicolumn{1}{c}{\underline{56.62}} & \multicolumn{1}{c}{51.54} & \multicolumn{1}{c}{55.11} \\
&\multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{\underline{68.23}} & \multicolumn{1}{c}{60.36} & \multicolumn{1}{c}{33.90} & \multicolumn{1}{c}{34.52} & \multicolumn{1}{c}{34.83} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{\underline{71.19}} & \multicolumn{1}{c}{\underline{89.80}} & \multicolumn{1}{c}{\underline{82.30}} & \multicolumn{1}{c}{\underline{57.10}} & \multicolumn{1}{c}{52.29} & \multicolumn{1}{c}{56.46} \\
\midrule
\multicolumn{1}{l}{\multirow{3}{*}{{T5-11B}}} & 
\multicolumn{1}{c}{5} & \multicolumn{1}{c}{55.02} & \multicolumn{1}{c}{53.93} & \multicolumn{1}{c}{33.70} & \multicolumn{1}{c}{35.00} & \multicolumn{1}{c}{35.62} & \multicolumn{1}{c}{39.04} & \multicolumn{1}{c}{64.64} & \multicolumn{1}{c}{82.80} & \multicolumn{1}{c}{77.71} & \multicolumn{1}{c}{45.32} & \multicolumn{1}{c}{50.50} & \multicolumn{1}{c}{52.12} \\
&\multicolumn{1}{c}{10} & \multicolumn{1}{c}{56.10} & \multicolumn{1}{c}{51.43} & \multicolumn{1}{c}{33.66} & \multicolumn{1}{c}{33.88} & \multicolumn{1}{c}{33.80} & \multicolumn{1}{c}{37.88} & \multicolumn{1}{c}{64.66} & \multicolumn{1}{c}{82.00} & \multicolumn{1}{c}{77.67} & \multicolumn{1}{c}{44.64} & \multicolumn{1}{c}{52.10} & \multicolumn{1}{c}{51.62} \\
&\multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{50.32} & \multicolumn{1}{c}{38.57} & \multicolumn{1}{c}{33.74} & \multicolumn{1}{c}{33.26} & \multicolumn{1}{c}{33.98} & \multicolumn{1}{c}{40.38} & \multicolumn{1}{c}{64.77} & \multicolumn{1}{c}{81.20} & \multicolumn{1}{c}{71.50} & \multicolumn{1}{c}{43.10} & \multicolumn{1}{c}{49.94} & \multicolumn{1}{c}{49.16} \\
\midrule
\multicolumn{1}{l}{\multirow{3}{*}{{T5-11B-\textit{early}}}} & 
\multicolumn{1}{c}{5} & \multicolumn{1}{c}{63.61} & \multicolumn{1}{c}{\underline{73.93}} & \multicolumn{1}{c}{\underline{40.80}} & \multicolumn{1}{c}{\underline{38.18}} & \multicolumn{1}{c}{\underline{39.55}} & \multicolumn{1}{c}{\underline{62.69}} & \multicolumn{1}{c}{62.48} & \multicolumn{1}{c}{84.20} & \multicolumn{1}{c}{77.84} & \multicolumn{1}{c}{45.90} & \multicolumn{1}{c}{50.03} & \multicolumn{1}{c}{\textbf{58.11}} \\
&\multicolumn{1}{c}{10} & \multicolumn{1}{c}{63.39} & \multicolumn{1}{c}{\underline{77.86}} & \multicolumn{1}{c}{\underline{41.18}} & \multicolumn{1}{c}{\underline{38.32}} & \multicolumn{1}{c}{\underline{39.58}} & \multicolumn{1}{c}{\underline{65.19}} & \multicolumn{1}{c}{62.23} & \multicolumn{1}{c}{84.80} & \multicolumn{1}{c}{77.56} & \multicolumn{1}{c}{45.80} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{\textbf{58.72}}\\
&\multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{64.04} & \multicolumn{1}{c}{\underline{75.00}} & \multicolumn{1}{c}{\underline{42.08}} & \multicolumn{1}{c}{37.94} & \multicolumn{1}{c}{\underline{39.98}} & \multicolumn{1}{c}{68.27} & \multicolumn{1}{c}{62.32} & \multicolumn{1}{c}{85.40} & \multicolumn{1}{c}{77.57} & \multicolumn{1}{c}{46.06} & \multicolumn{1}{c}{50.16} & \multicolumn{1}{c}{\textbf{58.98}} \\
\midrule
\multicolumn{1}{l}{\multirow{3}{*}{{T5-11B-\textit{late}}}} & 
\multicolumn{1}{c}{5} & \multicolumn{1}{c}{62.74} & \multicolumn{1}{c}{70.36} & \multicolumn{1}{c}{39.88} & \multicolumn{1}{c}{38.12} & \multicolumn{1}{c}{39.27} & \multicolumn{1}{c}{61.92} & \multicolumn{1}{c}{62.00} & \multicolumn{1}{c}{83.80} & \multicolumn{1}{c}{77.56} & \multicolumn{1}{c}{46.02} & \multicolumn{1}{c}{50.03} & \multicolumn{1}{c}{57.43} \\
& \multicolumn{1}{c}{10} & \multicolumn{1}{c}{63.32} & \multicolumn{1}{c}{73.93} & \multicolumn{1}{c}{40.16} & \multicolumn{1}{c}{\underline{38.32}} & \multicolumn{1}{c}{39.57} & \multicolumn{1}{c}{63.85} & \multicolumn{1}{c}{62.13} & \multicolumn{1}{c}{83.80} & \multicolumn{1}{c}{77.31} & \multicolumn{1}{c}{45.78} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{58.01} \\
& \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{63.90} & \multicolumn{1}{c}{70.36} & \multicolumn{1}{c}{40.64} & \multicolumn{1}{c}{\underline{38.06}} & \multicolumn{1}{c}{39.63} & \multicolumn{1}{c}{\underline{69.42}} & \multicolumn{1}{c}{62.15} & \multicolumn{1}{c}{84.80} & \multicolumn{1}{c}{77.27} & \multicolumn{1}{c}{45.88} & \multicolumn{1}{c}{50.22} & \multicolumn{1}{c}{58.39} \\
\bottomrule
\end{tabular}}
\end{adjustbox}
\caption{
\textbf{Comparison of our approach with various decoder models using minimal templates proposed by \citet{eval-harness}.} 
% The tasks are exactly the same as the evaluation dataset configuration that \citet{t0} used to report their main results. 
Bold denotes the best average score, and underline denotes the best score within each task.
T5~\textit{-early} and \textit{-late} demonstrate superiority over decoder models in tasks such as CB and WSC, while exhibiting limitations in tasks such as Winogrande and Hellaswag. 
Tasks denoted with a star (*) are evaluated with 1K samples due to their size.
The row labeled ``gpt3'' refers to the optimal number of shots for each task suggested by \citet{gpt3}.
Detailed configurations are reported in Table~\ref{tab: gpt3 best shot}.
}
\label{tab: compare decoder vs. t5}
\end{center}
\vspace{-6mm}
\end{table*}

% Figure environment removed

\subsection{Evaluation tasks and scoring methods}
\label{subsec: eval tasks and scoring method}

In Table~\ref{tab: io-structure} and~\ref{tab: optimal-prompt}, we evaluate the models on eight subtasks from SuperGLUE to evaluate language understanding ability.
For comprehensive evaluation across various types of tasks, we additionally assess the baselines on 11 tasks selected by \citet{t0} including five subtasks from SuperGLUE, Hellaswag~\citep{hellaswag}, ANLI~\citep{anli}, Winogrande~\citep{winogrande}, and StoryCloze~\citep{storycloze}.
We report accuracy as the metric and the model selects the option with the lowest cross-entropy loss among all the available multiple-choice options.
Any form of normalization with respect to the target output is not applied.
Please refer to Appendix~\ref{sec: normalization grounding} for justification regarding normalization, and to Appendix~\ref{app: task explanation} for detailed explanations of each benchmark dataset.


We adopt two following generation tasks for the comprehensive evaluation: XSum~\citep{xsum} and WebNLG~\citep{webnlg}, which are summarization and data-to-text tasks, respectively.
All the samples are generated with greedy decoding, and the ROUGE~\citep{rouge} metric is used for the evaluation of generation tasks.


\subsection{Prompts and few-shot demonstrations}
\label{subsec: prompts and few-shot demons}

We employ prompt templates offered by Language Model Evaluation Harness~\citep{eval-harness}, a widely adopted few-shot evaluation toolkit for decoder-only models.
For each generation task, we adopt a single template offered by PromptSource~\citep{promptsource}.
We conduct five experiments with different random seeds and present the averaged scores for Table~\ref{tab: compare decoder vs. t5} and Figure~\ref{fig: mean and std of models}.
For the other evaluations, we conduct a single experiment.
For detailed information about prompts and few-shot demonstrations, please refer to Appendix~\ref{app: prompt and demons details}.


\section{Evaluation results}
\label{sec: evaluation results}

\subsection{Seq2seq models perform better than decoder-only models}
\label{subsec: better than gpt}

In Table~\ref{tab: compare decoder vs. t5}, we evaluate both encoder-decoder and decoder-only models on a broad range of NLU benchmarks.
We select T5 as the primary baseline model to apply our approaches because it is the most commonly used seq2seq pretrained model. 
Furthermore, T5-LM and T0 are models that have undergone further training with different objectives, making it unfair to compare them against other pretrained models.
As shown in Table~\ref{tab: compare decoder vs. t5}, T5 with both \textit{early-} and \textit{late-fusion} approaches achieve superior performance over OPT-66B model, which is six times larger in parameter size, with a maximal margin of 3.6\%p in average.
Even with the best-shot configuration found by GPT-3~\citep{gpt3}, our methods consistently exhibit superior performance compared to the decoder-only baselines in most cases, with a significant margin.
Note that all the T5 variants reported in Table~\ref{tab: compare decoder vs. t5} already incorporate the objective-aligned prompt designs suggested in Section~\ref{sec: closer look at io-structure}.

Complete results for the remaining seq2seq baselines (i.e., T5-LM, T0, and UL2) can be found in Appendix~\ref{app: detailed results for fig3}.
Additionally, we present the results of our method in comparison to other well-known decoder-only LLMs (i.e., GPT-3, PaLM, and GPT-NeoX) in Appendix~\ref{app: comparison with other LLMs}.


\subsection{Robustness of our approach regardless of the baseline models}
\label{subsec: encoder-deocder robustness}

In Figure~\ref{fig: mean and std of models}, we examine the in-context learning abilities of different seq2seq baseline models and observe a trend that varies with the number of shots.
Our approach demonstrates its effectiveness by consistently improving the performance when applied to all models.
The black lines, which represent the \textit{original} method, predominantly exhibit a decline in score in the few-shot setting, when compared to zero-shot or one-shot inference.

In contrast, the results obtained by applying our approach show a completely different tendency.
The red and green lines, representing the application of \textit{early-fusion} and \textit{late-fusion}, respectively, exhibit an upward curve, indicating that the few-shot learning performance outperforms the 1-shot performance.
The only exception is UL2 where the few-shot performance remains equal to the 1-shot performance.

Surprisingly, despite T0 being fine-tuned with zero-shot prompts, both of our approaches result in higher performance with the GPT-3 best-shot setting compared to the zero-shot result of the original T0 model.
Considering that all four models were trained differently, this result reinforces the general applicability of our approach to seq2seq models, irrespective of how they were previously trained.

Additionally, we also observe that \textit{early-fusion} consistently outperforms \textit{late-fusion} by a small margin.
Generally, the pretraining objective of encoder-decoder models is designed such that the decoder aggregates the outputs of the encoder. This process maximizes the joint probability of the input text sequence for the decoder, achieved through the encoder-decoder attention module. In this context, \textit{early-fusion} implicitly selects examples that assist in resolving the test query by fully leveraging the encoder-decoder attention module. On the other hand, \textit{late-fusion} does not differentiate whether individual examples aid in solving the test query; it simply aggregates all responses.
Thus, we posit that the capability of the \textit{early-fusion} to selectively prioritize certain examples, an ability absent in the \textit{late-fusion}, is a major factor contributing to its superior performance.
For detailed scores for each setting, please refer to Appendix~\ref{app: detailed results for fig3}.

\subsection{Validation of our approach in the generation tasks}
In this subsection, we verify the robustness of our method by evaluating it on several generation tasks.
As mentioned in Section~\ref{sec: introduction}, generation tasks, such as summarization and translation, align well with the objective of the encoder-decoder architecture and have been proven to be successful with seq2seq models.
However, there is still a lack of organized evaluation of the few-shot learning ability on those tasks. 
We thus further evaluate our model on XSum and WebNLG datasets, in both one-shot and few-shot learning settings.
Though studies from \citet{ul2} and \citet{alexaTM} report the few-shot results of seq2seq models for XSum dataset, those results only encompass one-shot learning.
Consequently, to the best of our knowledge, this is the first work that explores the results of ``few''-shot learning for seq2seq models on the generation tasks.

% The application of objective-aligned prompting and \textit{early-fusion} method shows a remarkable improvement in the summarization task compared to the original T5 model, as shown in Table~\ref{tab: xsum results}.
The application of objective-aligned prompting and fusion-based approaches shows a remarkable improvement in the summarization task compared to the original T5 model, as shown in Table~\ref{tab: xsum results}.
A similar trend holds in WebNLG, a data-to-text task, as shown in Table~\ref{tab: webnlg results}.
This implies that our proposed approach not only serves as a robust few-shot learner for understanding tasks but also for generation tasks.
The analysis comparing decoder-only models in generative tasks can be found in Appendix~\ref{app: comp w/ dec-only gen tasks}.

\begin{table}
\fontsize{8.5pt}{8.5pt}\selectfont
\begin{minipage}{0.48\textwidth}
\setlength\tabcolsep{6pt}
\centering
\begin{tabular}{lcc}
\toprule
\multirow{2}{*}{Model} & 1-shot & 5-shot \\
& (R1/R2/RL) & (R1/R2/RL) \\
\midrule
T5* & 13.72/2.46/11.97 & 7.57/0.66/6.34 \\
T5 & 25.12/8.69/20.72 & 26.39/8.99/21.59 \\
T5-\textit{early} & - & \textbf{30.31}/\textbf{11.55}/\textbf{25.10} \\
T5-\textit{late} & - & 29.89/11.39/24.70 \\
\bottomrule
\end{tabular}
\caption{\textbf{Evaluation on XSum dataset.} The asterisk(*) on the right side of the T5 denotes the case where the sentinel tokens are not used during inference time. R1, R2, and RL denotes ROUGE-{1,2,L}, respectively.}
\label{tab: xsum results}
\end{minipage}
\hspace{1em}
\begin{minipage}{0.48\textwidth}
\setlength\tabcolsep{6pt}
\centering
\begin{tabular}{lcc}
\toprule
\multirow{2}{*}{Model} & 1-shot & 32-shot \\
& (R1/R2/RL) & (R1/R2/RL) \\
\midrule
T5* & 18.63/7.98/16.28 & 0.25/0.13/0.24 \\
T5 & 39.13/23.44/32.61 & 41.40/22.63/34.09 \\
T5-\textit{early} & - & \textbf{49.47}/\textbf{29.16}/\textbf{40.84} \\
T5-\textit{late} & - & 48.47/28.54/40.60 \\
\bottomrule
\end{tabular}
\caption{\textbf{Evaluation on WebNLG dataset.} The asterisk(*) on the right side of the T5 denotes the case where the sentinel tokens are not used during inference time. R1, R2, and RL denotes ROUGE-{1,2,L}, respectively.}
\label{tab: webnlg results}
\end{minipage}
\end{table}

\begin{table}[t]
\fontsize{9pt}{9pt}\selectfont
% \vspace{-10mm}
\begin{center}
\begin{tabular}{lcccc}
\toprule
 & OPT-13B & T5-\textit{original} & T5-\textit{early} & T5-\textit{late}\\
\midrule
average & 52.16 & 60.57 & \textbf{68.00} & 67.50 \\
std & 2.02 & 4.51 & \textbf{0.00} & \textbf{0.00} \\
\bottomrule
\end{tabular}
% \caption{\textbf{Evaluate the permutation bias for each method.}
\caption{\textbf{Evaluation of permutation bias for each method.}
% We select a single task from each of the four task taxonomies introduced by \cite{t0} and examine the permutation bias for 5-shot inference.
% We randomly sample 50 test cases from each task using the same random seed. 
We experiment on an identical set of 5-shot demonstrations, with only a different order.
Bold represents highest score for the average and lowest value for the standard deviation.
}
\label{tab: permutation bias}
\end{center}
\vspace{-2mm}
\end{table}


\subsection{Analysis of the permutation bias}
\label{subsec: rag robustness}
One challenge of in-context few-shot learning is the potential variation in model predictions due to the order in which demonstrations are fed to the model. 
% The strength of the \textit{late-fusion} approach lies in its example-order agnosticism, as it is not affected by the order of the examples at all.
The \textit{late-fusion} approach eliminates the so-called permutation bias problem, as it operates by fusing the probabilities of decoder outputs regardless of the order in which the inputs are fed.
To verify the effectiveness of our methods in reducing permutation bias, we conduct experiments by reordering the few-shot examples.
We select four tasks, CB, COPA, WSC, and WiC, from each of the four task taxonomies introduced by \citet{t0}: natural language inference, sentence completion, coreference resolution, and word sense, to cover a diverse range of task types.
%4개의 task를 사용했고, t0에 나오는 task taxonomy의 서로 다른 카테고리에서 선별하여 diverse한 task에 대해 실험하고자 했다.
We examine the permutation bias for a 5-shot setting and randomly sample 50 test sets from each task using the same random seed.
In Table~\ref{tab: permutation bias}, we report the average and standard deviation for all 120 possible permutations with 5-shot examples for each task.
Surprisingly, both the \textit{early-fusion} and \textit{late-fusion} methods demonstrate zero standard deviations for all tasks, thanks to the elimination of order relation on the encoder side. 
In the \textit{early-fusion} method, there is a subtle variation in probability caused by the relative positional bias when the order of the examples is fused.
However, this slight difference does not have any impact on the metric score.
In contrast, the decoder-only baseline and the \textit{original} encoder-decoder baseline show higher standard deviations with respect to the permutation of examples.

\section{Related work}
From UniLM~\citep{unilm} to BART~\citep{bart}, T5, UL2, and AlexaTM~\citep{alexaTM}, encoder-decoder architecture models have continuously advanced and set new records by introducing novel denoising pretraining objectives.
They excelled in tasks that require a comprehensive understanding of the input context and generate output based on it, such as translation, summarization, and other sequence-to-sequence tasks.
However, this was particularly evident during the fine-tuning process.
The seq2seq models have had limitations in handling sequences long enough, and it was only after the T5 series of papers that they were able to address this to some extent by incorporating relative position encoding.
As a result, in-context few-shot learning, which necessitates a long encoder input length, has received relatively less attention.
As a consequence, despite the recent explosive growth of LLMs, the size of encoder-decoder models has remained stagnant, with less than 20 billion parameters~\citep{ul2, alexaTM}.
This stands in contrast to decoder-only models, which have reached the scale of hundreds of billions~\citep{gpt3, palm, glm130b}.

In-context learning has emerged as an alternative to the fine-tuning paradigm~\citep{bert, gpt1}, which incurs expensive training costs, especially in the case of very large language models~\citep{gpt3}.
According to \citet{gpt3}, pretrained decoder-only LLMs achieve proficiency in various tasks simply by prompting a few examples as input, without requiring any parameter updating.
Unlike the many advancements of large decoder-only models~\citep{gopher, opt, llama, structured}, the mainstream approach for encoder-decoder LLMs to adapt to a specific task remained supervised instruction-tuning~\citep{t0, flan, flan2022}.
Recently, a few studies attempted to explore in-context learning; UL2 and AlexaTM reported zero-shot results on the SuperGLUE dataset, and T0 utilized multitask prompted training to enhance zero-shot performance.
Some of the studies employed techniques that emulate decoder-only models.
\citet{sap} utilized decoder-only-style sequential autoregressive prompting.
\citet{ul2} mixed causal language modeling with denoising objectives.
However, these approaches are restricted to particular setups and there has been a lack of structured results regarding few-shot learning.

\section{Discussion and conclusion}
\label{sec: discussion&conclusion}
In this work, we demonstrate that the seq2seq model with proper adaptation enables few-shot learning across a broad range of tasks.
To the best of our knowledge, no previous research has fully exploited the potential of seq2seq models as robust few-shot learners, outperforming decoder-only models, particularly on language understanding tasks.
Our remarkable results are attributed to two factors: 1) the alignment of prompts with pretraining objectives and 2) the use of fusion-based methods that independently process examples, compensating for the objective incompatibility and structural shortcomings of seq2seq models, respectively.
Through the carefully designed experiments, we verify that our approach exhibits a significant performance advantage under diverse conditions, including varying the number of shots, different baseline models, and a range of tasks, even with the complete removal of permutation bias.
Another major contribution of our work is the provision of a unified in-context learning evaluation toolkit for seq2seq models.
We have conducted controlled experiments to systematically compare and analyze the ability of existing seq2seq models --- a process not previously undertaken.
We plan to release the toolkit.
Importantly, our findings rediscover the strengths of the encoder-decoder architectures and shed new light on their potential as conversational agents (e.g., GPT-4~\citep{gpt4}), which have been underestimated.
We believe that the evolution of seq2seq models is still in progress, and our approach offers a substantial contribution toward maximizing their in-context learning capability.

\newpage

\bibliography{colm2024_conference}
\bibliographystyle{colm2024_conference}

\newpage
\appendix
\section*{Appendix}
We complement our paper with additional experimental results and miscellaneous details throughout this material.
Section~\ref{app: detail table 1} provides the complete experimental results for Table~\ref{tab: io-structure} regarding the proper placement of the target input in the encoder-decoder model.
Section~\ref{app: detail table 2} exhibits the impact of objective-aligned prompting for in-context learning, which complements Table~\ref{tab: optimal-prompt}.
% In Section~\ref{app: detail for decoder vs. t5}, we reveal the evaluation scores of various seq2seq models that are not fully presented in Table~\ref{tab: compare decoder vs. t5}.
Moreover, we report the few-shot learning performance of other decoder-only LLMs that are not featured in the main paper, in Section~\ref{app: comparison with other LLMs}.
Section~\ref{app: detailed results for fig3} presents detailed numerical values utilized for reporting Figure~\ref{fig: mean and std of models}.
In Section~\ref{app: comp w/ dec-only gen tasks}, we compare our methods with decoder-only models on generative tasks.
Section~\ref{app: training details for baseline models} offers detailed information regarding the baseline models used in our experiments.
Section~\ref{app: prompt and demons details} demonstrates examples of the prompts utilized in the generation tasks.
Section~\ref{sec: normalization grounding} provides an explanation of how normalization is considered when measuring few-shot scores.
Lastly, Section~\ref{app: task explanation} gives explanations for each of the benchmark datasets.


% 0. table 1,2에 대한 전체 테이블. zero, one, few를 포함한 전체 in-context learning에 대해 다 적용되는 것임을 언급 (sap의 table 4,5 형태 참고)
% (std, acc, f1 관련 상세 내용)
% 사용하지 않은 subset들 이유
\section{Detailed results for Table~\ref{tab: io-structure}}
\label{app: detail table 1}
% Table~\ref{tab: io-structure} verify that the placement of few-shot examples has great impact on performance of seq2seq models in high-level. Here, a detailed analysis is provided on whether it is advantageous to have few-shot examples in the encoder or the decoder, based on the specific task and the number of examples used for few-shot inference.
Table~\ref{tab: io-structure} verifies that the placement of a target input greatly impacts the in-context learning performance of seq2seq models.
Table~\ref{tab: full results for structure} provides detailed results of whether it is advantageous to have a target input in the encoder or the decoder, based on each task and each number of examples.
% In most cases, there is a significant difference in scores between the two placement scenarios, which supports the superiority of feeding the target question into the encoder.
In most cases, the target input given to the encoder produces significantly better results than that of the decoder.
Although there are a few instances where the scores are improved when the input is given to the decoder for specific combinations of models and tasks (e.g., UL2 with RTE task, T5-LM with WSC task), these are small exceptions that make it difficult to identify trends.
% , which supports the notion that providing the target question to the encoder is preferable.

\section{Detailed results for Table~\ref{tab: optimal-prompt}}
\label{app: detail table 2}

In Table~\ref{tab: full results for sentinel}, we present the complete results that complement Table~\ref{tab: optimal-prompt}.
% Without using sentinel tokens, the scores for T5 are not constant even when the number of shots varies across five out of eight tasks. 
Without using sentinel tokens, T5 shows consistent scores across five out of eight tasks, regardless of the variation in the number of shots. However, this issue is resolved when sentinel tokens are used.
Since T0 is further trained with the zero-shot prompts, we can observe that without the use of sentinel tokens, the score is initially higher at zero-shot results but significantly decreases as the number of demonstrations increases.
On the other hand, when sentinel tokens are employed during inference, even though the highest scores are still observed in zero-shot scenarios, the performance is almost maintained as the number of demonstrations increases.
% These observations suggest that using sentinel tokens helps the seq2seq model understand the examples better in general few-shot setting.
Based on these observations, we conclude that aligning the prompt structure with the pretraining objective generally helps the seq2seq model better understand few-shot examples.
Note that the scores reported in Table~\ref{tab: full results for sentinel} are not the ones that applied our proposed methodologies like \textit{early-} or \textit{late-fusion}.
%테이블 2의 각 태스크 및 샷 수에 따른 성능의 변화를 관찰하기 위해 우리는 테이블 6를 리포트한다.
%센티넬 토큰을 사용하지 않는 경우, 8개 태스크 중 5개에서 샷 수가 변하더라도 T5의 점수가 일정했다. 그러나 센티넬 토큰을 사용하는 경우에 이 문제는 모두 해결된다. 
%T0의 경우 zero-shot setting에서 fine-tuning(further training) 됐기 때문에 0-shot에서는 sentinel이 없는 경우에 점수가 더 높은 모습을 보이지만, 이후 예제가 늘어갈수록 그 점수차가 역전되는 모습을 확인할 수 있다.
%우리의 above observations는 few-shot 상황에서 sentinel이 각 예제를 효과적으로 전달하는 데에 도움을 줌을 시사한다.

% 3. opt 뿐 아니라 llama, palm, gpt3 등에 대해 비교하는 테이블. 동일 세팅은 아니지만 참고용인것 언급 (eleuther거 확인?)
\section{Comparing with other decoder LLMs}
\label{app: comparison with other LLMs}
% While numerous Large Language Models(LLMs) have their own benchmark evaluation scores reported, it is not guaranteed that these models are evaluated using the same baseline as our Evaluation Harness.
% While numerous Large Language Models have their own benchmark evaluation scores reported, it is not guaranteed that these models are evaluated using the same baseline as our Evaluation Harness.
While numerous LLMs have their own benchmark evaluation scores reported, it is not guaranteed that these models are evaluated using the same baseline.
% Despite this potential discrepancy, for reference purposes, we display the results of our baseline alongside with those of other significant LLMs in Table~\ref{tab: compare to other LLM decoders} including GPT-3, PaLM \cite{palm} and GPT-NeoX \cite{gpt-neox}.
Despite this potential discrepancy, to provide a point of reference, we present the results of our method alongside those of other well-known decoder-only LLMs in Table~\ref{tab: compare to other LLM decoders} including GPT-3, PaLM \citep{palm} and GPT-NeoX \citep{gpt-neox}.
All the records of decoder LLMs in Table~\ref{tab: compare to other LLM decoders} are from the paper.
Compared to PaLM-8B, T5 with \textit{early-fusion} achieves a higher average score.
% Though not fully reported, T5-\textit{FiD} beats GPT-NeoX in average and also shows much more competent performance in ANLI tasks compared to the GPT-3.
Although there is only a small overlap in tasks, T5-\textit{early} beats GPT-NeoX on average and demonstrates much better performance in ANLI tasks compared to GPT-3.
This indicates that our model performs well considering its size, as the throughput of the seq2seq model is comparable to that of a half-sized decoder-only model, as stated by \citet{ul2}.
In this regard, it is unfortunate that the absence of a larger encoder-decoder model hinders us from making an equivalent comparison with decoder models larger than 100B. 

\FloatBarrier
\begin{table*}[!ht]
% \vspace{-10mm}
\setlength\tabcolsep{5pt}
\centering
\begin{adjustbox}{width=1\textwidth}{
\begin{tabular}{lcccccccccccc}
\toprule
Placement & Model & Shot & RTE & COPA & CB & WiC & WSC & BoolQ* & MultiRC* & RECORD* &
\textbf{average}\\
\midrule
\multicolumn{1}{l}{\multirow{13}{*}{{encoder}}} 
& \multicolumn{1}{l}{\multirow{3}{*}{{T5}}} & 
\multicolumn{1}{c}{1} & 60.65 & 84.00 & 82.14 & 50.00 & 39.42 & 86.10 & 43.00 & 78.90 & \textbf{65.53} \\
& & \multicolumn{1}{c}{5} & 52.71 & 84.00 & 67.86 & 50.00 & 38.46 & 61.50 & 42.70 & 79.10 & \textbf{59.54} \\
& & \multicolumn{1}{c}{10} & 52.71 & 84.00 & 51.79 & 54.08 & 36.54 & 61.50 & 56.60 & 76.50 & \textbf{59.09} \\
\cmidrule{2-12}
& \multicolumn{1}{l}{\multirow{3}{*}{{T5-LM}}} &
\multicolumn{1}{c}{1} & 64.02 & 83.00 & 52.57 & 50.00 & 36.54 & 74.30 & 46.20 & 85.50 & \textbf{61.72} \\
& & \multicolumn{1}{c}{5} & 47.29 & 78.00 & 57.14 & 50.00 & 44.23 & 61.50 & 46.90 & 85.10 & \textbf{58.77} \\
& & \multicolumn{1}{c}{10} & 47.29 & 79.00 & 53.57 & 50.00 & 60.58 & 61.50 & 43.60 & 82.40 & \textbf{59.74} \\
\cmidrule{2-12}
& \multicolumn{1}{l}{\multirow{3}{*}{{T0}}} & 
\multicolumn{1}{c}{1} & 74.73 & 82.00 & 58.93 & 50.16 & 35.58 & 74.30 & 58.20 & 80.90 & \textbf{64.35} \\
& & \multicolumn{1}{c}{5} & 71.84 & 80.00 & 69.64 & 56.11 & 52.88 & 70.10 & 57.60 & 81.40 & \textbf{67.45} \\
& & \multicolumn{1}{c}{10} & 47.29 & 82.00 & 51.79 & 56.90 & 64.42 & 70.40 & 57.20 & 77.40 & \textbf{63.42} \\
\cmidrule{2-12}
& \multicolumn{1}{l}{\multirow{3}{*}{{UL2}}} & 
\multicolumn{1}{c}{1} & 54.15 & 86.00 & 42.86 & 51.57 & 36.54 & 78.20 & 42.70 & 88.40 & \textbf{60.05} \\
& & \multicolumn{1}{c}{5} & 49.10 & 81.00 & 50.00 & 50.00 & 36.54 & 63.60 & 54.30 & 86.30 & \textbf{58.85} \\
& & \multicolumn{1}{c}{10} & 47.29 & 84.00 & 50.00 & 50.00 & 36.54 & 68.70 & 57.30 & 86.30 & \textbf{60.02} \\
\midrule
\multicolumn{1}{l}{\multirow{13}{*}{{decoder}}} 
& \multicolumn{1}{l}{\multirow{3}{*}{{T5}}} & 
\multicolumn{1}{c}{1} & 52.71 & 60.00 & 41.07 & 48.12 & 36.54 & 58.50 & 47.60 & 15.10 & 44.95 \\
& & \multicolumn{1}{c}{5} & 52.71 & 77.00 & 41.07 & 46.00 & 38.00 & 58.80 & 51.00 & 14.40 & 47.37 \\
& & \multicolumn{1}{c}{10} & 52.71 & 73.00 & 41.07 & 50.00 & 38.00 & 57.50 & 52.30 & 16.30 & 
 47.61 \\
\cmidrule{2-12}
& \multicolumn{1}{l}{\multirow{3}{*}{{T5-LM}}} &
\multicolumn{1}{c}{1} & 51.99 & 74.00 & 46.43 & 50.00 & 36.54 & 63.00 & 42.60 & 49.50 & 51.76 \\
& & \multicolumn{1}{c}{5} & 47.29 & 72.00 & 48.21 & 47.00 & 62.00 & 61.50 & 42.70 & 50.10 & 53.85 \\
& & \multicolumn{1}{c}{10} & 47.29 & 75.00 & 51.79 & 46.00 & 62.00 & 61.50 & 42.70 & 51.20 & 54.68 \\
\cmidrule{2-12}
& \multicolumn{1}{l}{\multirow{3}{*}{{T0}}} & 
\multicolumn{1}{c}{1} & 53.43 & 77.00 & 48.21 & 48.59 & 36.54 & 61.80 & 48.00 & 46.20 & 52.47 \\
& & \multicolumn{1}{c}{5} & 48.74 & 74.00 & 53.57 & 53.29 & 40.38 & 62.40 & 52.60 & 46.80 & 53.97 \\
& & \multicolumn{1}{c}{10} & 46.93 & 75.00 & 48.21 & 52.82 & 42.31 & 61.30 & 53.90 & 46.40 & 53.36 \\
\cmidrule{2-12}
& \multicolumn{1}{l}{\multirow{3}{*}{{UL2}}} & 
\multicolumn{1}{c}{1} & 62.09 & 61.00 & 12.50 & 50.00 & 41.35 & 63.40 & 43.30 & 78.00 & 51.46 \\
& & \multicolumn{1}{c}{5} & 58.12 & 76.00 & 50.00 & 50.00 & 36.54 & 61.50 & 44.60 & 82.50 & 57.41 \\
& & \multicolumn{1}{c}{10} & 53.07 & 80.00 & 64.29 & 50.00 & 36.54 & 61.40 & 45.90 & 81.90 & 59.14 \\
\bottomrule
\end{tabular}}
\end{adjustbox}
% \caption{opt모델, t5계열 우리 모델 성능 비교 with average minimal template}
\caption{\textbf{Detailed results for Table~\ref{tab: io-structure}.}
The asterisk(*) on the right side of the task indicates that due to the large size of the test dataset, evaluation is performed on a random sample of 1K instances from the test dataset. Bold denotes the best average score among different placements of the target input for each model and number of shots.}
\label{tab: full results for structure}
\end{table*}

% 혹시 다시 쓸 일 있을까 5seed 짜리는 주석 처리해두었습니다.
% \begin{table*}
% \setlength\tabcolsep{5pt}
% \centering
% \begin{adjustbox}{width=1\textwidth}{
% \begin{tabular}{lcccccccccccc}
% \toprule
% Prompt & Model & Shot & RTE & CB & WSC & COPA & WiC & Boolq* & Multirc* & ReCoRD* & 
% \textbf{average}\\
% \midrule
% \multicolumn{1}{l}{\multirow{17}{*}{{Vanilla}}} 
% & \multicolumn{1}{l}{\multirow{4}{*}{{T5}}} & 
% \multicolumn{1}{c}{0} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{41.07} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{59.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.84} & \multicolumn{1}{c}{42.56} & \multicolumn{1}{c}{70.92} & \multicolumn{1}{c}{51.83} \\
% & & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{40.36} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{73.80} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.84} & \multicolumn{1}{c}{42.56} & \multicolumn{1}{c}{73.66} & \multicolumn{1}{c}{53.93} \\
% & & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{40.71} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{66.60} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.84} & \multicolumn{1}{c}{42.56} & \multicolumn{1}{c}{69.30} & \multicolumn{1}{c}{52.53} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{41.07} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{64.40} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.84} & \multicolumn{1}{c}{42.56} & \multicolumn{1}{c}{66.36} & \multicolumn{1}{c}{51.93} \\
% \cmidrule{2-12}
% & \multicolumn{1}{l}{\multirow{4}{*}{{T5-LM}}} &
% \multicolumn{1}{c}{0} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{26.79} & \multicolumn{1}{c}{63.46} & \multicolumn{1}{c}{71.00} & \multicolumn{1}{c}{50.31} & \multicolumn{1}{c}{40.30} & \multicolumn{1}{c}{57.44} & \multicolumn{1}{c}{85.62} & \multicolumn{1}{c}{55.95} \\
% & & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{49.46} & \multicolumn{1}{c}{36.07} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{64.20} & \multicolumn{1}{c}{46.14} & \multicolumn{1}{c}{61.86} & \multicolumn{1}{c}{44.50} & \multicolumn{1}{c}{83.42} & \multicolumn{1}{c}{52.77} \\
% & & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{48.59} & \multicolumn{1}{c}{40.00} & \multicolumn{1}{c}{63.46} & \multicolumn{1}{c}{59.60} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.9} & \multicolumn{1}{c}{54.62} & \multicolumn{1}{c}{78.08} & \multicolumn{1}{c}{57.03} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{47.44} & \multicolumn{1}{c}{34.64} & \multicolumn{1}{c}{63.46} & \multicolumn{1}{c}{58.20} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.84} & \multicolumn{1}{c}{57.44} & \multicolumn{1}{c}{76.02} & \multicolumn{1}{c}{56.13} \\
% \cmidrule{2-12}
% & \multicolumn{1}{l}{\multirow{4}{*}{{T0}}} & 
% \multicolumn{1}{c}{0} & \multicolumn{1}{c}{83.75} & \multicolumn{1}{c}{76.79} & \multicolumn{1}{c}{73.08} & \multicolumn{1}{c}{76.00} & \multicolumn{1}{c}{50.47} & \multicolumn{1}{c}{72.46} & \multicolumn{1}{c}{73.70} & \multicolumn{1}{c}{79.54} & \multicolumn{1}{c}{73.22} \\
% & & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{61.01} & \multicolumn{1}{c}{60.36} & \multicolumn{1}{c}{66.15} & \multicolumn{1}{c}{77.40} & \multicolumn{1}{c}{50.72} & \multicolumn{1}{c}{68.18} & \multicolumn{1}{c}{63.26} & \multicolumn{1}{c}{80.50} & \multicolumn{1}{c}{65.95} \\
% & & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{47.29} & \multicolumn{1}{c}{57.50} & \multicolumn{1}{c}{63.85} & \multicolumn{1}{c}{72.40} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{52.02} & \multicolumn{1}{c}{57.46} & \multicolumn{1}{c}{78.34} & \multicolumn{1}{c}{59.86} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{47.29} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{63.46} & \multicolumn{1}{c}{69.20} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{38.40} & \multicolumn{1}{c}{57.44} & \multicolumn{1}{c}{76.42} & \multicolumn{1}{c}{56.53} \\
% \cmidrule{2-12}
% & \multicolumn{1}{l}{\multirow{4}{*}{{UL2}}} & 
% \multicolumn{1}{c}{0} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{21.43} & \multicolumn{1}{c}{35.58} & \multicolumn{1}{c}{71.00} & \multicolumn{1}{c}{48.75} & \multicolumn{1}{c}{60.76} & \multicolumn{1}{c}{43.50} & \multicolumn{1}{c}{84.80} & \multicolumn{1}{c}{52.31} \\
% & & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{8.93} & \multicolumn{1}{c}{36.92} & \multicolumn{1}{c}{65.8} & \multicolumn{1}{c}{49.94} & \multicolumn{1}{c}{61.02} & \multicolumn{1}{c}{44.28} & \multicolumn{1}{c}{85.18} & \multicolumn{1}{c}{50.60}  \\
% & & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{8.93} & \multicolumn{1}{c}{40.19} & \multicolumn{1}{c}{61.40} & \multicolumn{1}{c}{49.22} & \multicolumn{1}{c}{54.98} & \multicolumn{1}{c}{43.52} & \multicolumn{1}{c}{83.42} & \multicolumn{1}{c}{49.30} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{8.93} & \multicolumn{1}{c}{37.12} & \multicolumn{1}{c}{58.20} & \multicolumn{1}{c}{49.72} & \multicolumn{1}{c}{61.44} & \multicolumn{1}{c}{42.82} & \multicolumn{1}{c}{82.40} & \multicolumn{1}{c}{49.17} \\
% \midrule
% \multicolumn{1}{l}{\multirow{17}{*}{{w/ sentinel }}} 
% & \multicolumn{1}{l}{\multirow{4}{*}{{T5}}} & 
% \multicolumn{1}{c}{0} & \multicolumn{1}{c}{54.87} & \multicolumn{1}{c}{33.93} & \multicolumn{1}{c}{41.35} & \multicolumn{1}{c}{58.00} & \multicolumn{1}{c}{44.67} & \multicolumn{1}{c}{71.48} & \multicolumn{1}{c}{45.60} & \multicolumn{1}{c}{73.14} & \multicolumn{1}{c}{52.88} \\
% & & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{62.38} & \multicolumn{1}{c}{67.50} & \multicolumn{1}{c}{43.46} & \multicolumn{1}{c}{83.60} & \multicolumn{1}{c}{49.62} & \multicolumn{1}{c}{85.08} & \multicolumn{1}{c}{49.62} & \multicolumn{1}{c}{78.74} & \multicolumn{1}{c}{65.00} \\
% & & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{55.02} & \multicolumn{1}{c}{53.93} & \multicolumn{1}{c}{39.04} & \multicolumn{1}{c}{82.80} & \multicolumn{1}{c}{50.50} & \multicolumn{1}{c}{61.86} & \multicolumn{1}{c}{51.72} & \multicolumn{1}{c}{77.96} & \multicolumn{1}{c}{59.10} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{56.10} & \multicolumn{1}{c}{51.43} & \multicolumn{1}{c}{37.88} & \multicolumn{1}{c}{82.00} & \multicolumn{1}{c}{52.10} & \multicolumn{1}{c}{62.14} & \multicolumn{1}{c}{54.10} & \multicolumn{1}{c}{75.70} & \multicolumn{1}{c}{58.93} \\
% \cmidrule{2-12}
% & \multicolumn{1}{l}{\multirow{4}{*}{{T5-LM}}} &
% \multicolumn{1}{c}{0} & \multicolumn{1}{c}{62.45} & \multicolumn{1}{c}{48.21} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{71.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{75.16} & \multicolumn{1}{c}{46.62} & \multicolumn{1}{c}{86.12} & \multicolumn{1}{c}{59.51} \\
% & & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{56.68} & \multicolumn{1}{c}{57.86} & \multicolumn{1}{c}{41.92} & \multicolumn{1}{c}{81.20} & \multicolumn{1}{c}{50.38} & \multicolumn{1}{c}{72.92} & \multicolumn{1}{c}{47.14} & \multicolumn{1}{c}{86.02} & \multicolumn{1}{c}{61.76}  \\
% & & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{47.65} & \multicolumn{1}{c}{56.07} & \multicolumn{1}{c}{43.65} & \multicolumn{1}{c}{77.80} & \multicolumn{1}{c}{49.97} & \multicolumn{1}{c}{68.58} & \multicolumn{1}{c}{47.10} & \multicolumn{1}{c}{85.22} & \multicolumn{1}{c}{59.51} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{47.22} & \multicolumn{1}{c}{47.50} & \multicolumn{1}{c}{52.31} & \multicolumn{1}{c}{79.20} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.86} & \multicolumn{1}{c}{45.00} & \multicolumn{1}{c}{82.32} & \multicolumn{1}{c}{58.18} \\
% \cmidrule{2-12}
% & \multicolumn{1}{l}{\multirow{4}{*}{{T0}}} & 
% \multicolumn{1}{c}{0} & \multicolumn{1}{c}{81.95} & \multicolumn{1}{c}{78.57} & \multicolumn{1}{c}{53.85} & \multicolumn{1}{c}{82.00} & \multicolumn{1}{c}{55.64} & \multicolumn{1}{c}{80.06} & \multicolumn{1}{c}{65.02} & \multicolumn{1}{c}{80.14} & \multicolumn{1}{c}{72.15} \\
% & & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{74.08} & \multicolumn{1}{c}{72.50} & \multicolumn{1}{c}{41.73} & \multicolumn{1}{c}{81.20} & \multicolumn{1}{c}{51.76} & \multicolumn{1}{c}{75.50} & \multicolumn{1}{c}{57.54} & \multicolumn{1}{c}{80.80} & \multicolumn{1}{c}{66.89} \\
% & & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{67.94} & \multicolumn{1}{c}{76.07} & \multicolumn{1}{c}{60.19} & \multicolumn{1}{c}{81.60} & \multicolumn{1}{c}{54.20} & \multicolumn{1}{c}{72.14} & \multicolumn{1}{c}{57.56} & \multicolumn{1}{c}{79.16} & \multicolumn{1}{c}{68.61} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{53.57} & \multicolumn{1}{c}{60.36} & \multicolumn{1}{c}{62.88} & \multicolumn{1}{c}{81.60} & \multicolumn{1}{c}{52.98} & \multicolumn{1}{c}{66.86} & \multicolumn{1}{c}{57.62} & \multicolumn{1}{c}{77.56} & \multicolumn{1}{c}{64.18} \\
% \cmidrule{2-12}
% & \multicolumn{1}{l}{\multirow{4}{*}{{UL2}}} & 
% \multicolumn{1}{c}{0} & \multicolumn{1}{c}{51.62} & \multicolumn{1}{c}{33.93} & \multicolumn{1}{c}{53.85} & \multicolumn{1}{c}{69.00} & \multicolumn{1}{c}{51.41} & \multicolumn{1}{c}{74.96} & \multicolumn{1}{c}{48.90} & \multicolumn{1}{c}{84.96} & \multicolumn{1}{c}{58.58} \\
% & & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{54.87} & \multicolumn{1}{c}{47.50} & \multicolumn{1}{c}{38.65} & \multicolumn{1}{c}{82.80} & \multicolumn{1}{c}{50.34} & \multicolumn{1}{c}{75.84} & \multicolumn{1}{c}{48.94} & \multicolumn{1}{c}{87.38} & \multicolumn{1}{c}{60.79} \\
% & & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{48.01} & \multicolumn{1}{c}{49.64} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{82.80} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{70.98} & \multicolumn{1}{c}{56.74} & \multicolumn{1}{c}{86.48} & \multicolumn{1}{c}{60.15} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{47.44} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{82.20} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{66.18} & \multicolumn{1}{c}{57.48} & \multicolumn{1}{c}{85.42} & \multicolumn{1}{c}{59.41} \\
% \midrule
% \multicolumn{1}{l}{\multirow{4}{*}{{w/ mode}}} & 
% \multicolumn{1}{l}{\multirow{4}{*}{{UL2}}} & 
% \multicolumn{1}{c}{0} & \multicolumn{1}{c}{53.79} & \multicolumn{1}{c}{8.93} & \multicolumn{1}{c}{47.12} & \multicolumn{1}{c}{87.00} & \multicolumn{1}{c}{53.13} & \multicolumn{1}{c}{79.68} & \multicolumn{1}{c}{49.26} & \multicolumn{1}{c}{86.66} & \multicolumn{1}{c}{58.20} \\
% & & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{54.87} & \multicolumn{1}{c}{49.29} & \multicolumn{1}{c}{47.50} & \multicolumn{1}{c}{85.00} & \multicolumn{1}{c}{52.29} & \multicolumn{1}{c}{87.38} & \multicolumn{1}{c}{47.52} & \multicolumn{1}{c}{87.24} & \multicolumn{1}{c}{63.89} \\
% & & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{49.03} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{84.20} & \multicolumn{1}{c}{50.09} & \multicolumn{1}{c}{86.48} & \multicolumn{1}{c}{55.06} & \multicolumn{1}{c}{86.84} & \multicolumn{1}{c}{62.28} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{47.51} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{84.20} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{85.42} & \multicolumn{1}{c}{57.32} & \multicolumn{1}{c}{85.66} & \multicolumn{1}{c}{62.08} \\
% \bottomrule
% \end{tabular}}
% \end{adjustbox}
% % \caption{opt모델, t5계열 우리 모델 성능 비교 with average minimal template}
% \caption{\textbf{Detailed results for seq2seq baselines verified in Table~\ref{tab: optimal-prompt}.}
% Tasks marked with an asterisk(*) on the right-hand side indicate that the total sample size is too large, so that 1K random samples are used for evaluation purposes.}
% \label{tab: full results for sentinel}
% \end{table*}

% 원래 figure 3에 대한 글이었던 것
\section{Additional results for Table~\ref{tab: compare decoder vs. t5} and Detailed results for Figure~\ref{fig: mean and std of models}}
\label{app: detailed results for fig3}
% In Figure~\ref{fig: mean and std of models}, we depict the average and standard deviation results for four seq2seq baselines for each methodology. 
% Additionally, we report the exact numeric results for the experiment in Table~\ref{tab: full results for seq2seq t0 datasets}. 
% The ``std'' values we report do not represent the standard deviation of scores across different tasks, but are calculated by averaging the standard deviation values across all tasks, with five seeds evaluated on each task.
% Despite the presence of disparities among individual models, the implementation of our methodologies results in a consistently marked increase in the average score, surpassing that of the original, while simultaneously reducing the standard deviation.

% 원래 테이블 3에 대한 글이었던 것
% In order to thoroughly analyze the performance of seq2seq baselines, we report the additive results evaluated on T5, T5-LM, T0, and UL2 for Table~\ref{tab: compare decoder vs. t5} in Table~\ref{tab: full results for seq2seq t0 datasets}.
% Although we compare various decoder-only models with T5 (with our methodologies applied), other encoder-decoder models such as T5-LM and UL2 also achieve significant scores that approximate those of the OPT-30B model, which is much larger.
% Surprisingly, for T0, although trained with 0-shot prompts, there were instances when applying our method resulted in higher performance than the 0-shot setting.
% In Table~\ref{tab: compare decoder vs. t5}, we present the number of demonstrations labeled as ``gpt3''. It refers to the highest-scoring shots verified in GPT-3. The corresponding numbers of shots are reported in Table~\ref{tab: gpt3 best shot}.
In order to thoroughly analyze the performance of seq2seq baselines, we report the additive results evaluated on T5, T5-LM, T0, and UL2 for Table~\ref{tab: compare decoder vs. t5} in Table~\ref{tab: full results for seq2seq t0 datasets}.
Although we compare various decoder-only models with T5 (with our methodologies applied), other encoder-decoder models such as T5-LM and UL2 also achieve significant scores that approximate those of the OPT-30B model, which is much larger.

% 두가지를 합친 버전
Table~\ref{tab: full results for seq2seq t0 datasets} is also the supplementary table for Figure~\ref{fig: mean and std of models}, which depicts the average and standard deviation results for four seq2seq baselines for each methodology. 
% Additionally, we report the exact numeric results for the experiment in Table~\ref{tab: full results for seq2seq t0 datasets}. 
The ``std'' values we report do not represent the standard deviation of scores across different tasks, but are calculated by averaging the standard deviation values across all tasks, with five seeds evaluated on each task.
% Despite the presence of disparities among individual models, the implementation of our methodologies results in a consistently marked increase in the average score, surpassing that of the \textit{original}, while simultaneously reducing the standard deviation.
By applying our approaches, the average scores consistently increase for all models, while simultaneously reducing the standard deviation.
% Surprisingly, for T0, although trained with 0-shot prompts, there were instances when applying our method resulted in higher performance than the 0-shot setting.

\section{Comparison with decoder-only models on generative tasks}
\label{app: comp w/ dec-only gen tasks}

The pretraining objectives of existing seq2seq models aren’t ideally configured for free response generation; their focus was more on improving downstream task capabilities through span corruption and local reconstruction objectives, rather than on language modeling objectives. Consequently, for generative tasks, they might be less effective compared to decoder-only models. To illustrate this, we conducted comparisons with decoder-only baselines on representative generation tasks, XSum and WebNLG, with the results detailed in Table~\ref{tab: xsum dec results} and Table~\ref{tab: webnlg dec results}, respectively.
We highlight the scores of both our model and the one exhibiting the best performance. Our best model, T5-early, outperforms OPT-13B in terms of ROUGE-2 and OPT-30B in terms of ROUGE-L on the XSum dataset, despite having only 11B in size. However, it shows slightly lower overall performance compared to OPT-66B. For the WebNLG dataset, our model generally scores lower than decoder-only models. Notably, the significant difference in 5-shot performance between T5* and our proposed T5-early model indicates that our method meaningfully enhances the generation performance of the seq2seq model, irrespective of the pretrained language model’s performance. With the improvements in the seq2seq model’s pretraining objectives, we expect a considerable boost in performance.

% 7. 각 베이스라인의 상세 parameter, inference IO setting 등 detail
% -all model details. token, size, flops 등
% -baseline들과 세팅 동일하게 한것, 논문에서 따온것 등
\section{Training details for baseline models}
\label{app: training details for baseline models}
Seq2seq models that we select as our baseline are trained in various manners. 
T5 is pretrained only with the denoising objective, utilizing a number of sentinel tokens that indicate where to be denoised. 
However, as the released version of T5 is a fine-tuned model, while T5.1.1 is trained solely on the C4 dataset with 1T tokens, we opt to utilize T5.1.1 as our T5 baseline.
% T5-LM is an additionally trained T5 model (100K steps) with a causal language modeling objective. 
T5-LM is an additionally trained T5 model with a causal language modeling objective.
% For 100K steps, it utilized 100M tokens for further training.
T0 is a variant of T5-LM, which is fine-tuned with plenty of prompts for various tasks.
It trained roughly 12B additional tokens compared to T5-LM.
While \citet{t0} present three types of T5-architecture models (T0, T0+, and T0++) based on variations of fine-tuning datasets, we choose T0 to ensure that the datasets used in our experiments have not been previously encountered during the training process.
Sentinel tokens were not used for further training both for T5-LM and T0.
Finally, UL2 is a variant model of T5, pretrained with three types of denoising objectives: R-, S-, and X-denoising. 
% All three objectives utilized the sentinel token, and are variations of denoising and causal language modeling. 
% All three objectives utilized the sentinel token, and 
All three objectives utilized the sentinel token, with ``R'' and ``S'' respectively corresponding to denoising and causal language modeling objectives.
``X'' represents the extreme version of both objectives.
% R- 과 S- 는 각각 denoising과 causal language modeling obsective에 해당하며, X-는 두 objective의 extreme version이다.
Similar to T5, it used the C4 dataset to train about 1 trillion tokens.
% The UL2 model has 20B parameters, while the rest have the same size of 11B parameters.
% If the same batch size and sequence length as T5 were used, about 100B tokens were additionally learned.
% About 100B tokens were additionally used for further training.
% This implies that training objective for T5-LM and T0 resembles the one with in-context learning.
% {https://huggingface.co/google/ul2}

All four seq2seq baseline models are basically pretrained with sequence lengths of 512~\citep{t5, ul2} or 1,024~\citep{t0}, which account for a quarter or half of the typical decoder LLMs~\citep{gpt3, opt, gopher}.
Therefore, the seq2seq model faces a disadvantage when the number of examples used for few-shot evaluation increases, as it lacks exposure to long-length inputs during training.
% Since \citet{ul2} compared the throughput based solely on the size of the decoder, and from this perspective, we have achieved considerably higher scores despite a significantly lower (less than half of) throughput compared to the decoder model used in the comparison.

According to \citet{opt}, the OPT models are trained on approximately up to 300B tokens, including the Pile~\citep{pile} and the datasets used for RoBERTa~\citep{roberta} pretraining.
In our experiments, we evaluate three different sizes of OPT models (13B, 30B, and 66B), which are up to 6 times larger than the T5 baseline model.
% At first, we conduct experiments using OPT models to compare our baseline models.
% The primary reason for this choice is that OPT models are the only open-sourced models available with comparable sizes to T5(11B) and UL2(20B), enabling us to experiment with our custom settings.
% During the experiment with OPT models, LLaMA models are released with various sizes so we choose to test with the 7B one and also with the same size of BLOOM model.
% BLOOM and LLaMA are also powerful open-source decoder models. 
% BLOOM is pretrained on about 340B tokens from the Pile dataset, while LLaMA is pretrained on around a trillion tokens with their own curated dataset. 
% For both of the models, we choose the one with the size of 7B parameters from each variant.
Additionally, to compare with decoder models smaller than 13B, we also evaluate the BLOOM-7B model, which is pretrained on approximately 340 billion tokens from the ROOTS corpus.

\section{Detailed information about prompts and few-shot demonstrations}
\label{app: prompt and demons details}

For the natural language generation tasks in Table~\ref{tab: xsum results} and Table~\ref{tab: webnlg results}, we adopt one of the prompts from the PromptSource~\citep{promptsource} since Language Model Evaluation Harness does not provide templates for XSum and WebNLG.
We manually select one of the prompts from the PromptSource that best describes the task.
We provide one-shot prompting samples utilizing the selected template in Figure~\ref{fig: xsum & webnlg prompt examples}.

With respect to the few-shot demonstrations, we distinguish between two settings where the demonstrations used for each prediction are maintained the same, referred to as the ``fixed'' setting, or randomly sampled at each time, referred to as the ``non-fixed'' setting.
For the generation tasks, we conduct experiments with a ``non-fixed'' setting, otherwise utilize a ``fixed'' setting.

% 6. scoring 관련 bias 쓸 내용? -> normalize 관련
% hellaswag에만 하게 되어있는 점 모호. normalize 적용 X. string 기준 길이 normalize도 애매.
\section{Normalization to the scores}
\label{sec: normalization grounding}
As mentioned in Section~\ref{subsec: eval tasks and scoring method}, we do not apply any normalization to the output losses, with respect to the token or string length of the output sequences.
In the case of HellaSwag, applying normalization often leads to a higher score.
However, it may not hold true for other tasks.
To ensure consistent evaluation across all datasets, we decide to adopt a unified approach without normalization.
% Furthermore, it is typical not to apply length normalization with respect to the output sequences.

Additionally, in \citet{gpt3}, a pre-normalization step is conducted where calibration is applied to each calculation of scores.
With the same context mentioned in the paragraph above, we do not implement any preceding calibration to calculate the output scores.

\begin{table*}
\setlength\tabcolsep{5pt}
\centering
\begin{adjustbox}{width=1\textwidth}{
\begin{tabular}{lcccccccccccc}
\toprule
Prompt & Model & Shot & RTE & CB & WSC & COPA & WiC & Boolq* & Multirc* & ReCoRD* & 
\textbf{average}\\
\midrule
\multicolumn{1}{l}{\multirow{17}{*}{{vanilla}}} 
& \multicolumn{1}{l}{\multirow{4}{*}{{T5}}} & 
\multicolumn{1}{c}{0} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{41.07} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{59.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.50} & \multicolumn{1}{c}{42.70} & \multicolumn{1}{c}{70.10} & \multicolumn{1}{c}{51.70} \\
& & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{39.29} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{79.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.50} & \multicolumn{1}{c}{42.70} & \multicolumn{1}{c}{75.10} & \multicolumn{1}{c}{54.60} \\
& & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{41.07} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{61.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.50} & \multicolumn{1}{c}{42.70} & \multicolumn{1}{c}{71.90} & \multicolumn{1}{c}{52.18} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{41.07} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{63.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.50} & \multicolumn{1}{c}{42.70} & \multicolumn{1}{c}{69.20} & \multicolumn{1}{c}{52.09} \\
\cmidrule{2-12}
& \multicolumn{1}{l}{\multirow{4}{*}{{T5-LM}}} &
\multicolumn{1}{c}{0} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{26.79} & \multicolumn{1}{c}{63.46} & \multicolumn{1}{c}{71.00} & \multicolumn{1}{c}{50.31} & \multicolumn{1}{c}{40.80} & \multicolumn{1}{c}{57.30} & \multicolumn{1}{c}{85.60} & \multicolumn{1}{c}{56.00} \\
& & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{47.29} & \multicolumn{1}{c}{17.86} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{63.00} & \multicolumn{1}{c}{44.83} & \multicolumn{1}{c}{61.60} & \multicolumn{1}{c}{49.60} & \multicolumn{1}{c}{82.20} & \multicolumn{1}{c}{50.36} \\
& & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{46.57} & \multicolumn{1}{c}{35.71} & \multicolumn{1}{c}{63.46} & \multicolumn{1}{c}{60.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.50} & \multicolumn{1}{c}{57.40} & \multicolumn{1}{c}{78.20} & \multicolumn{1}{c}{56.61} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{47.29} & \multicolumn{1}{c}{41.07} & \multicolumn{1}{c}{63.46} & \multicolumn{1}{c}{60.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.50} & \multicolumn{1}{c}{57.30} & \multicolumn{1}{c}{74.50} & \multicolumn{1}{c}{56.89} \\
\cmidrule{2-12}
& \multicolumn{1}{l}{\multirow{4}{*}{{T0}}} & 
\multicolumn{1}{c}{0} & \multicolumn{1}{c}{83.75} & \multicolumn{1}{c}{76.79} & \multicolumn{1}{c}{73.08} & \multicolumn{1}{c}{76.00} & \multicolumn{1}{c}{50.47} & \multicolumn{1}{c}{73.00} & \multicolumn{1}{c}{72.70} & \multicolumn{1}{c}{80.60} & \multicolumn{1}{c}{\textbf{73.30}} \\
& & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{74.73} & \multicolumn{1}{c}{55.36} & \multicolumn{1}{c}{68.27} & \multicolumn{1}{c}{78.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{65.70} & \multicolumn{1}{c}{66.60} & \multicolumn{1}{c}{80.10} & \multicolumn{1}{c}{\textbf{67.34}} \\
& & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{47.29} & \multicolumn{1}{c}{53.57} & \multicolumn{1}{c}{63.46} & \multicolumn{1}{c}{75.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{56.80} & \multicolumn{1}{c}{57.30} & \multicolumn{1}{c}{79.50} & \multicolumn{1}{c}{60.37} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{47.29} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{63.46} & \multicolumn{1}{c}{70.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{38.50} & \multicolumn{1}{c}{57.30} & \multicolumn{1}{c}{77.60} & \multicolumn{1}{c}{56.77} \\
\cmidrule{2-12}
& \multicolumn{1}{l}{\multirow{4}{*}{{UL2}}} & 
\multicolumn{1}{c}{0} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{21.43} & \multicolumn{1}{c}{35.58} & \multicolumn{1}{c}{71.00} & \multicolumn{1}{c}{48.75} & \multicolumn{1}{c}{60.60} & \multicolumn{1}{c}{43.20} & \multicolumn{1}{c}{85.90} & \multicolumn{1}{c}{52.39} \\
& & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{8.93} & \multicolumn{1}{c}{37.50} & \multicolumn{1}{c}{64.00} & \multicolumn{1}{c}{49.84} & \multicolumn{1}{c}{60.10} & \multicolumn{1}{c}{42.70} & \multicolumn{1}{c}{85.00} & \multicolumn{1}{c}{50.10} \\
& & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{8.93} & \multicolumn{1}{c}{38.46} & \multicolumn{1}{c}{59.00} & \multicolumn{1}{c}{50.31} & \multicolumn{1}{c}{50.50} & \multicolumn{1}{c}{43.10} & \multicolumn{1}{c}{84.00} & \multicolumn{1}{c}{48.38} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{8.93} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{59.00} & \multicolumn{1}{c}{49.84} & \multicolumn{1}{c}{61.40} & \multicolumn{1}{c}{42.70} & \multicolumn{1}{c}{83.30} & \multicolumn{1}{c}{49.30} \\
\midrule
\multicolumn{1}{l}{\multirow{17}{*}{{w/ sentinel }}} 
& \multicolumn{1}{l}{\multirow{4}{*}{{T5}}} & 
\multicolumn{1}{c}{0} & \multicolumn{1}{c}{54.87} & \multicolumn{1}{c}{33.93} & \multicolumn{1}{c}{41.35} & \multicolumn{1}{c}{58.00} & \multicolumn{1}{c}{44.67} & \multicolumn{1}{c}{71.60} & \multicolumn{1}{c}{44.90} & \multicolumn{1}{c}{73.80} & \multicolumn{1}{c}{\textbf{52.89}} \\
& & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{60.65} & \multicolumn{1}{c}{82.14} & \multicolumn{1}{c}{39.42} & \multicolumn{1}{c}{84.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{86.10} & \multicolumn{1}{c}{43.00} & \multicolumn{1}{c}{78.90} & \multicolumn{1}{c}{\textbf{65.53}} \\
& & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{67.86} & \multicolumn{1}{c}{38.46} & \multicolumn{1}{c}{84.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.50} & \multicolumn{1}{c}{42.70} & \multicolumn{1}{c}{79.10} & \multicolumn{1}{c}{\textbf{59.54}} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{52.71} & \multicolumn{1}{c}{51.79} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{83.00} & \multicolumn{1}{c}{54.08} & \multicolumn{1}{c}{61.50} & \multicolumn{1}{c}{56.60} & \multicolumn{1}{c}{76.50} & \multicolumn{1}{c}{\textbf{59.09}} \\
\cmidrule{2-12}
& \multicolumn{1}{l}{\multirow{4}{*}{{T5-LM}}} &
\multicolumn{1}{c}{0} & \multicolumn{1}{c}{62.45} & \multicolumn{1}{c}{48.21} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{71.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{75.20} & \multicolumn{1}{c}{46.10} & \multicolumn{1}{c}{86.40} & \multicolumn{1}{c}{\textbf{59.49}} \\
& & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{64.62} & \multicolumn{1}{c}{53.57} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{83.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{74.30} & \multicolumn{1}{c}{46.20} & \multicolumn{1}{c}{85.50} & \multicolumn{1}{c}{\textbf{61.72}} \\
& & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{47.29} & \multicolumn{1}{c}{57.14} & \multicolumn{1}{c}{44.23} & \multicolumn{1}{c}{78.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.50} & \multicolumn{1}{c}{46.90} & \multicolumn{1}{c}{85.10} & \multicolumn{1}{c}{\textbf{58.77}} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{47.29} & \multicolumn{1}{c}{53.57} & \multicolumn{1}{c}{60.58} & \multicolumn{1}{c}{79.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{61.50} & \multicolumn{1}{c}{43.60} & \multicolumn{1}{c}{82.40} & \multicolumn{1}{c}{\textbf{59.74}} \\
\cmidrule{2-12}
& \multicolumn{1}{l}{\multirow{4}{*}{{T0}}} & 
\multicolumn{1}{c}{0} & \multicolumn{1}{c}{81.95} & \multicolumn{1}{c}{78.57} & \multicolumn{1}{c}{53.85} & \multicolumn{1}{c}{82.00} & \multicolumn{1}{c}{55.64} & \multicolumn{1}{c}{79.40} & \multicolumn{1}{c}{65.20} & \multicolumn{1}{c}{80.80} & \multicolumn{1}{c}{72.18} \\
& & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{74.73} & \multicolumn{1}{c}{58.93} & \multicolumn{1}{c}{35.58} & \multicolumn{1}{c}{82.00} & \multicolumn{1}{c}{50.16} & \multicolumn{1}{c}{74.30} & \multicolumn{1}{c}{58.20} & \multicolumn{1}{c}{80.90} & \multicolumn{1}{c}{64.35} \\
& & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{71.84} & \multicolumn{1}{c}{69.64} & \multicolumn{1}{c}{52.88} & \multicolumn{1}{c}{80.00} & \multicolumn{1}{c}{56.11} & \multicolumn{1}{c}{70.10} & \multicolumn{1}{c}{57.60} & \multicolumn{1}{c}{81.50} & \multicolumn{1}{c}{\textbf{67.46}} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{47.29} & \multicolumn{1}{c}{51.79} & \multicolumn{1}{c}{64.42} & \multicolumn{1}{c}{82.00} & \multicolumn{1}{c}{56.90} & \multicolumn{1}{c}{70.40} & \multicolumn{1}{c}{57.20} & \multicolumn{1}{c}{77.40} & \multicolumn{1}{c}{\textbf{63.42}} \\
\cmidrule{2-12}
& \multicolumn{1}{l}{\multirow{4}{*}{{UL2}}} & 
\multicolumn{1}{c}{0} & \multicolumn{1}{c}{51.62} & \multicolumn{1}{c}{33.93} & \multicolumn{1}{c}{53.85} & \multicolumn{1}{c}{69.00} & \multicolumn{1}{c}{51.41} & \multicolumn{1}{c}{76.60} & \multicolumn{1}{c}{49.70} & \multicolumn{1}{c}{84.60} & \multicolumn{1}{c}{\textbf{58.84}} \\
& & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{54.15} & \multicolumn{1}{c}{42.86} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{86.00} & \multicolumn{1}{c}{51.57} & \multicolumn{1}{c}{78.20} & \multicolumn{1}{c}{42.70} & \multicolumn{1}{c}{88.40} & \multicolumn{1}{c}{60.05} \\
& & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{49.10} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{81.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{63.60} & \multicolumn{1}{c}{54.30} & \multicolumn{1}{c}{86.30} & \multicolumn{1}{c}{58.85} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{47.29} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{84.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{68.70} & \multicolumn{1}{c}{57.30} & \multicolumn{1}{c}{86.30} & \multicolumn{1}{c}{60.02} \\
\midrule
\multicolumn{1}{l}{\multirow{4}{*}{{w/ mode}}} & 
\multicolumn{1}{l}{\multirow{4}{*}{{UL2}}} & 
\multicolumn{1}{c}{0} & \multicolumn{1}{c}{53.79} & \multicolumn{1}{c}{8.93} & \multicolumn{1}{c}{47.12} & \multicolumn{1}{c}{87.00} & \multicolumn{1}{c}{53.13} & \multicolumn{1}{c}{80.30} & \multicolumn{1}{c}{50.40} & \multicolumn{1}{c}{87.20} & \multicolumn{1}{c}{58.48} \\
& & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{48.01} & \multicolumn{1}{c}{44.64} & \multicolumn{1}{c}{46.15} & \multicolumn{1}{c}{86.00} & \multicolumn{1}{c}{54.70} & \multicolumn{1}{c}{88.40} & \multicolumn{1}{c}{42.90} & \multicolumn{1}{c}{86.90} & \multicolumn{1}{c}{\textbf{62.21}} \\
& & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{48.38} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{83.00} & \multicolumn{1}{c}{50.47} & \multicolumn{1}{c}{86.30} & \multicolumn{1}{c}{48.40} & \multicolumn{1}{c}{86.30} & \multicolumn{1}{c}{\textbf{61.17}} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{47.29} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{86.00} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{86.30} & \multicolumn{1}{c}{57.30} & \multicolumn{1}{c}{84.90} & \multicolumn{1}{c}{\textbf{62.29}} \\
\bottomrule
\end{tabular}}
\end{adjustbox}
% \caption{opt모델, t5계열 우리 모델 성능 비교 with average minimal template}
\caption{\textbf{Detailed results for Table~\ref{tab: optimal-prompt}.}
The asterisk(*) on the right side of the task indicates that due to the large size of the test dataset, evaluation is performed on a random sample of 1K instances from the test dataset. For \textit{w/ mode} setting, the sentinel token is also utilized. Bold denotes the best average score among \textit{vanilla}, \textit{w/ sentinel}, and \textit{w/ mode} approaches, for the same model and the same number of shots.}
\label{tab: full results for sentinel}
\end{table*}
% \vspace{-1cm}


\begin{table*}
\setlength\tabcolsep{10pt}
\centering
\begin{adjustbox}{width=1\textwidth}
\begin{tabular}{lcccccccccccc}
\toprule
\multirow{1}[1]{*}{Task} & \multicolumn{1}{c}{RTE} & \multicolumn{1}{c}{CB} & \multicolumn{1}{c}{ANLI1} & \multicolumn{1}{c}{ANLI2} & \multicolumn{1}{c}{ANLI3} & \multicolumn{1}{c}{WSC} & \multicolumn{1}{c}{Winogrande} & \multicolumn{1}{c}{COPA} & \multicolumn{1}{c}{StoryCloze} & \multicolumn{1}{c}{HellaSwag} & \multicolumn{1}{c}{WiC}\\
\midrule
shots & 32 & 32 & 50 & 50 & 50 & 32 & 16 & 32 & 70 & 20 & 32\\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{\textbf{The number of shots with the highest score reported in GPT-3.} 
For each task, the length of concatenated examples approximates the sequence length of the pretrained GPT-3 model. 
Consequently, conducting few-shot learning with such settings is disadvantageous for seq2seq models, which are pretrained on shorter sequence lengths.
}
\label{tab: gpt3 best shot}
\end{table*}
\newpage

% 웨이비가 볼드 치신 부분 쓸 수도 있으니 기록으로 남기겠습니다..!

% % \caption{opt모델, t5계열 우리 모델 성능 비교 with average minimal template}
% \caption{\textbf{Additional results for Table~\ref{tab: compare decoder vs. t5}.}
% The asterisk(*) on the right side of the task indicates that due to the large size of the test dataset, evaluation is performed on a random sample of 1K instances from the test dataset. 
% The underlined values represent the highest scores within each model for each task, while the bold values represent the settings where the average scores for all the tasks are highest within each model.}
% \label{tab: full results for seq2seq t0 datasets}
% \end{table*}

% \begin{table*}
% \setlength\tabcolsep{6.5pt}
% \centering
% \begin{adjustbox}{width=1\textwidth}{
% \begin{tabular}{lcccccccccccccccc}
% \toprule
% Model & Method & Shot & RTE & CB & ANLI1 & ANLI2 & ANLI3 & WSC & Winogrande & COPA & StoryCloze & HellaSwag* & WiC & 
% \textbf{average} & \textbf{std} \\
% \midrule
% \multicolumn{1}{l}{\multirow{12}{*}{{T5-LM}}} 
% & \multicolumn{1}{l}{\multirow{5}{*}{{Original}}} & 
% \multicolumn{1}{c}{0} & \multicolumn{1}{c}{62.45} & \multicolumn{1}{c}{48.21} & \multicolumn{1}{c}{37.70} & \multicolumn{1}{c}{34.90} & \multicolumn{1}{c}{37.25} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{55.09} & \multicolumn{1}{c}{71.00} & \multicolumn{1}{c}{72.85} & \multicolumn{1}{c}{48.02} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{50.37} & \multicolumn{1}{c}{0.05} \\
% & & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{56.68} & \multicolumn{1}{c}{57.86} & \multicolumn{1}{c}{34.96} & \multicolumn{1}{c}{34.40} & \multicolumn{1}{c}{35.87} & \multicolumn{1}{c}{41.92} & \multicolumn{1}{c}{61.15} & \multicolumn{1}{c}{81.20} & \multicolumn{1}{c}{74.83} & \multicolumn{1}{c}{50.70} & \multicolumn{1}{c}{50.38} & \multicolumn{1}{c}{52.72} & \multicolumn{1}{c}{3.28} \\
% & & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{47.65} & \multicolumn{1}{c}{56.07} & \multicolumn{1}{c}{32.66} & \multicolumn{1}{c}{34.06} & \multicolumn{1}{c}{33.97} & \multicolumn{1}{c}{43.65} & \multicolumn{1}{c}{61.29} & \multicolumn{1}{c}{77.80} & \multicolumn{1}{c}{75.34} & \multicolumn{1}{c}{50.38} & \multicolumn{1}{c}{49.97} & \multicolumn{1}{c}{51.17} & \multicolumn{1}{c}{1.48} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{47.22} & \multicolumn{1}{c}{47.50} & \multicolumn{1}{c}{32.12} & \multicolumn{1}{c}{32.96} & \multicolumn{1}{c}{33.07} & \multicolumn{1}{c}{52.31} & \multicolumn{1}{c}{61.42} & \multicolumn{1}{c}{79.20} & \multicolumn{1}{c}{74.47} & \multicolumn{1}{c}{49.18} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{50.86} & \multicolumn{1}{c}{2.10} \\
% & & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{47.08} & \multicolumn{1}{c}{58.93} & \multicolumn{1}{c}{32.86} & \multicolumn{1}{c}{32.42} & \multicolumn{1}{c}{34.47} & \multicolumn{1}{c}{62.12} & \multicolumn{1}{c}{61.04} & \multicolumn{1}{c}{77.60} & \multicolumn{1}{c}{67.26} & \multicolumn{1}{c}{47.28} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{51.91} & \multicolumn{1}{c}{1.63} \\
% \cmidrule{2-16}
% & \multicolumn{1}{l}{\multirow{3}{*}{{RAG}}} & 
% \multicolumn{1}{c}{5} & \multicolumn{1}{c}{54.22} & \multicolumn{1}{c}{56.79} & \multicolumn{1}{c}{35.60} & \multicolumn{1}{c}{34.48} & \multicolumn{1}{c}{36.93} & \multicolumn{1}{c}{40.38} & \multicolumn{1}{c}{61.18} & \multicolumn{1}{c}{81.60} & \multicolumn{1}{c}{75.51} & \multicolumn{1}{c}{50.30} & \multicolumn{1}{c}{50.34} & \multicolumn{1}{c}{52.49} & \multicolumn{1}{c}{1.61} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{53.14} & \multicolumn{1}{c}{53.93} & \multicolumn{1}{c}{35.74} & \multicolumn{1}{c}{34.26} & \multicolumn{1}{c}{36.78} & \multicolumn{1}{c}{43.46} & \multicolumn{1}{c}{61.17} & \multicolumn{1}{c}{81.20} & \multicolumn{1}{c}{75.29} & \multicolumn{1}{c}{50.28} & \multicolumn{1}{c}{50.22} & \multicolumn{1}{c}{52.32} & \multicolumn{1}{c}{1.91} \\
% & & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{53.29} & \multicolumn{1}{c}{54.29} & \multicolumn{1}{c}{35.48} & \multicolumn{1}{c}{33.96} & \multicolumn{1}{c}{36.67} & \multicolumn{1}{c}{42.88} & \multicolumn{1}{c}{61.37} & \multicolumn{1}{c}{81.60} & \multicolumn{1}{c}{75.02} & \multicolumn{1}{c}{50.20} & \multicolumn{1}{c}{50.91} & \multicolumn{1}{c}{52.33} & \multicolumn{1}{c}{1.34} \\
% \cmidrule{2-16}
% & \multicolumn{1}{l}{\multirow{3}{*}{{FiD}}} & 
% \multicolumn{1}{c}{5} & \multicolumn{1}{c}{53.86} & \multicolumn{1}{c}{56.07} & \multicolumn{1}{c}{35.62} & \multicolumn{1}{c}{34.84} & \multicolumn{1}{c}{36.95} & \multicolumn{1}{c}{52.50} & \multicolumn{1}{c}{61.93} & \multicolumn{1}{c}{82.20} & \multicolumn{1}{c}{75.53} & \multicolumn{1}{c}{50.52} & \multicolumn{1}{c}{49.97} & \multicolumn{1}{c}{53.64} & \multicolumn{1}{c}{2.07} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{53.29} & \multicolumn{1}{c}{53.21} & \multicolumn{1}{c}{35.82} & \multicolumn{1}{c}{34.58} & \multicolumn{1}{c}{36.47} & \multicolumn{1}{c}{50.96} & \multicolumn{1}{c}{61.86} & \multicolumn{1}{c}{81.80} & \multicolumn{1}{c}{75.60} & \multicolumn{1}{c}{50.60} & \multicolumn{1}{c}{50.13} & \multicolumn{1}{c}{53.12} & \multicolumn{1}{c}{1.88} \\
% & & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{52.20} & \multicolumn{1}{c}{53.21} & \multicolumn{1}{c}{35.82} & \multicolumn{1}{c}{34.46} & \multicolumn{1}{c}{36.90} & \multicolumn{1}{c}{60.19} & \multicolumn{1}{c}{61.82} & \multicolumn{1}{c}{82.80} & \multicolumn{1}{c}{75.25} & \multicolumn{1}{c}{50.44} & \multicolumn{1}{c}{50.31} & \multicolumn{1}{c}{53.95} & \multicolumn{1}{c}{1.30} \\
% \midrule
% \multicolumn{1}{l}{\multirow{12}{*}{{T5}}} 
% & \multicolumn{1}{l}{\multirow{5}{*}{{Original}}} & 
% \multicolumn{1}{c}{0} & \multicolumn{1}{c}{54.87} & \multicolumn{1}{c}{33.93} & \multicolumn{1}{c}{35.50} & \multicolumn{1}{c}{33.10} & \multicolumn{1}{c}{33.42} & \multicolumn{1}{c}{41.35} & \multicolumn{1}{c}{52.33} & \multicolumn{1}{c}{58.00} & \multicolumn{1}{c}{65.53} & \multicolumn{1}{c}{38.02} & \multicolumn{1}{c}{44.67} & \multicolumn{1}{c}{44.61} & \multicolumn{1}{c}{0.08} \\
% & & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{62.38} & \multicolumn{1}{c}{67.50} & \multicolumn{1}{c}{37.52} & \multicolumn{1}{c}{\underline{38.20}} & \multicolumn{1}{c}{40.10} & \multicolumn{1}{c}{43.46} & \multicolumn{1}{c}{61.66} & \multicolumn{1}{c}{83.60} & \multicolumn{1}{c}{76.99} & \multicolumn{1}{c}{45.68} & \multicolumn{1}{c}{49.62} & \multicolumn{1}{c}{55.16} & \multicolumn{1}{c}{3.80} \\
% & & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{55.02} & \multicolumn{1}{c}{53.93} & \multicolumn{1}{c}{33.70} & \multicolumn{1}{c}{35.00} & \multicolumn{1}{c}{35.62} & \multicolumn{1}{c}{39.04} & \multicolumn{1}{c}{64.64} & \multicolumn{1}{c}{82.80} & \multicolumn{1}{c}{77.71} & \multicolumn{1}{c}{45.32} & \multicolumn{1}{c}{50.50} & \multicolumn{1}{c}{52.12} & \multicolumn{1}{c}{2.78} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{56.10} & \multicolumn{1}{c}{51.43} & \multicolumn{1}{c}{33.66} & \multicolumn{1}{c}{33.88} & \multicolumn{1}{c}{33.80} & \multicolumn{1}{c}{37.88} & \multicolumn{1}{c}{64.66} & \multicolumn{1}{c}{82.00} & \multicolumn{1}{c}{77.67} & \multicolumn{1}{c}{44.64} & \multicolumn{1}{c}{52.10} & \multicolumn{1}{c}{51.62} & \multicolumn{1}{c}{2.42} \\
% & & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{50.32} & \multicolumn{1}{c}{38.57} & \multicolumn{1}{c}{33.74} & \multicolumn{1}{c}{33.26} & \multicolumn{1}{c}{33.98} & \multicolumn{1}{c}{40.39} & \multicolumn{1}{c}{64.77} & \multicolumn{1}{c}{81.20} & \multicolumn{1}{c}{71.50} & \multicolumn{1}{c}{43.10} & \multicolumn{1}{c}{49.94} & \multicolumn{1}{c}{49.16} & \multicolumn{1}{c}{1.99} \\
% \cmidrule{2-16}
% & \multicolumn{1}{l}{\multirow{3}{*}{{RAG}}} & 
% \multicolumn{1}{c}{5} & \multicolumn{1}{c}{62.74} & \multicolumn{1}{c}{70.36} & \multicolumn{1}{c}{39.88} & \multicolumn{1}{c}{38.12} & \multicolumn{1}{c}{39.27} & \multicolumn{1}{c}{61.92} & \multicolumn{1}{c}{62.00} & \multicolumn{1}{c}{83.80} & \multicolumn{1}{c}{77.56} & \multicolumn{1}{c}{46.02} & \multicolumn{1}{c}{50.03} & \multicolumn{1}{c}{57.43} & \multicolumn{1}{c}{2.20} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{63.32} & \multicolumn{1}{c}{73.93} & \multicolumn{1}{c}{40.16} & \multicolumn{1}{c}{38.32} & \multicolumn{1}{c}{39.57} & \multicolumn{1}{c}{63.85} & \multicolumn{1}{c}{62.13} & \multicolumn{1}{c}{83.80} & \multicolumn{1}{c}{77.31} & \multicolumn{1}{c}{45.78} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{58.01} & \multicolumn{1}{c}{1.93} \\
% & & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{63.90} & \multicolumn{1}{c}{70.36} & \multicolumn{1}{c}{40.64} & \multicolumn{1}{c}{38.06} & \multicolumn{1}{c}{39.63} & \multicolumn{1}{c}{\underline{69.42}} & \multicolumn{1}{c}{62.15} & \multicolumn{1}{c}{84.80} & \multicolumn{1}{c}{77.27} & \multicolumn{1}{c}{45.88} & \multicolumn{1}{c}{50.22} & \multicolumn{1}{c}{58.39} & \multicolumn{1}{c}{1.32} \\
% \cmidrule{2-16}
% & \multicolumn{1}{l}{\multirow{3}{*}{{FiD}}} & 
% \multicolumn{1}{c}{5} & \multicolumn{1}{c}{63.61} & \multicolumn{1}{c}{73.93} & \multicolumn{1}{c}{40.80} & \multicolumn{1}{c}{\underline{38.18}} & \multicolumn{1}{c}{39.55} & \multicolumn{1}{c}{62.69} & \multicolumn{1}{c}{62.48} & \multicolumn{1}{c}{84.20} & \multicolumn{1}{c}{77.84} & \multicolumn{1}{c}{45.90} & \multicolumn{1}{c}{50.03} & \multicolumn{1}{c}{58.11} & \multicolumn{1}{c}{2.00} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{63.39} & \multicolumn{1}{c}{\underline{77.86}} & \multicolumn{1}{c}{41.18} & \multicolumn{1}{c}{38.32} & \multicolumn{1}{c}{39.58} & \multicolumn{1}{c}{\underline{65.19}} & \multicolumn{1}{c}{62.23} & \multicolumn{1}{c}{84.80} & \multicolumn{1}{c}{77.56} & \multicolumn{1}{c}{45.80} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{58.72} & \multicolumn{1}{c}{1.47} \\
% & & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{64.04} & \multicolumn{1}{c}{\underline{75.00}} & \multicolumn{1}{c}{42.08} & \multicolumn{1}{c}{37.94} & \multicolumn{1}{c}{39.98} & \multicolumn{1}{c}{68.27} & \multicolumn{1}{c}{62.32} & \multicolumn{1}{c}{85.40} & \multicolumn{1}{c}{77.57} & \multicolumn{1}{c}{46.06} & \multicolumn{1}{c}{50.16} & \multicolumn{1}{c}{58.98} & \multicolumn{1}{c}{0.97} \\
% \midrule
% \multicolumn{1}{l}{\multirow{12}{*}{{T0}}} 
% & \multicolumn{1}{l}{\multirow{5}{*}{{Original}}} & 
% \multicolumn{1}{c}{0} & \multicolumn{1}{c}{\underline{81.95}} & \multicolumn{1}{c}{\underline{78.57}} & \multicolumn{1}{c}{\underline{45.20}} & \multicolumn{1}{c}{\underline{40.40}} & \multicolumn{1}{c}{\underline{42.42}} & \multicolumn{1}{c}{\underline{53.85}} & \multicolumn{1}{c}{60.93} & \multicolumn{1}{c}{82.00} & \multicolumn{1}{c}{75.95} & \multicolumn{1}{c}{50.56} & \multicolumn{1}{c}{\underline{55.64}} & \multicolumn{1}{c}{\textbf{60.68}} & \multicolumn{1}{c}{0.13} \\
% & & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{\underline{74.08}} & \multicolumn{1}{c}{\underline{72.50}} & \multicolumn{1}{c}{\underline{41.52}} & \multicolumn{1}{c}{37.20} & \multicolumn{1}{c}{\underline{38.73}} & \multicolumn{1}{c}{41.73} & \multicolumn{1}{c}{61.10} & \multicolumn{1}{c}{81.20} & \multicolumn{1}{c}{75.09} & \multicolumn{1}{c}{50.56} & \multicolumn{1}{c}{51.76} & \multicolumn{1}{c}{\textbf{56.86}} & \multicolumn{1}{c}{3.74} \\
% & & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{67.94} & \multicolumn{1}{c}{76.07} & \multicolumn{1}{c}{39.42} & \multicolumn{1}{c}{36.28} & \multicolumn{1}{c}{38.53} & \multicolumn{1}{c}{60.19} & \multicolumn{1}{c}{61.56} & \multicolumn{1}{c}{81.60} & \multicolumn{1}{c}{73.18} & \multicolumn{1}{c}{49.06} & \multicolumn{1}{c}{54.20} & \multicolumn{1}{c}{58.00} & \multicolumn{1}{c}{2.73} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{53.57} & \multicolumn{1}{c}{60.36} & \multicolumn{1}{c}{36.06} & \multicolumn{1}{c}{34.80} & \multicolumn{1}{c}{35.12} & \multicolumn{1}{c}{62.88} & \multicolumn{1}{c}{60.36} & \multicolumn{1}{c}{81.60} & \multicolumn{1}{c}{71.28} & \multicolumn{1}{c}{47.92} & \multicolumn{1}{c}{52.98} & \multicolumn{1}{c}{54.27} & \multicolumn{1}{c}{2.72} \\
% & & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{47.29} & \multicolumn{1}{c}{63.21} & \multicolumn{1}{c}{33.30} & \multicolumn{1}{c}{33.28} & \multicolumn{1}{c}{33.15} & \multicolumn{1}{c}{63.85} & \multicolumn{1}{c}{60.17} & \multicolumn{1}{c}{79.20} & \multicolumn{1}{c}{61.98} & \multicolumn{1}{c}{46.10} & \multicolumn{1}{c}{50.63} & \multicolumn{1}{c}{52.01} & \multicolumn{1}{c}{1.93} \\
% \cmidrule{2-16}
% & \multicolumn{1}{l}{\multirow{3}{*}{{RAG}}} &
% \multicolumn{1}{c}{5} & \multicolumn{1}{c}{81.08} & \multicolumn{1}{c}{\underline{77.50}} & \multicolumn{1}{c}{44.18} & \multicolumn{1}{c}{37.76} & \multicolumn{1}{c}{42.07} & \multicolumn{1}{c}{55.00} & \multicolumn{1}{c}{62.12} & \multicolumn{1}{c}{82.60} & \multicolumn{1}{c}{75.34} & \multicolumn{1}{c}{50.62} & \multicolumn{1}{c}{54.33} & \multicolumn{1}{c}{60.24} & \multicolumn{1}{c}{2.47} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{\underline{80.72}} & \multicolumn{1}{c}{70.71} & \multicolumn{1}{c}{\underline{44.54}} & \multicolumn{1}{c}{38.34} & \multicolumn{1}{c}{\underline{42.42}} & \multicolumn{1}{c}{54.81} & \multicolumn{1}{c}{62.16} & \multicolumn{1}{c}{82.00} & \multicolumn{1}{c}{75.09} & \multicolumn{1}{c}{50.80} & \multicolumn{1}{c}{\underline{56.39}} & \multicolumn{1}{c}{59.82} & \multicolumn{1}{c}{2.62} \\
% & & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{81.37} & \multicolumn{1}{c}{74.29} & \multicolumn{1}{c}{\underline{44.52}} & \multicolumn{1}{c}{\underline{38.12}} & \multicolumn{1}{c}{\underline{41.98}} & \multicolumn{1}{c}{64.42} & \multicolumn{1}{c}{62.23} & \multicolumn{1}{c}{84.40} & \multicolumn{1}{c}{74.89} & \multicolumn{1}{c}{50.38} & \multicolumn{1}{c}{56.02} & \multicolumn{1}{c}{\textbf{61.15}} & \multicolumn{1}{c}{1.92} \\
% \cmidrule{2-16}
% & \multicolumn{1}{l}{\multirow{3}{*}{{FiD}}} &
% \multicolumn{1}{c}{5} & \multicolumn{1}{c}{\underline{81.16}} & \multicolumn{1}{c}{76.43} & \multicolumn{1}{c}{\underline{44.36}} & \multicolumn{1}{c}{37.86} & \multicolumn{1}{c}{\underline{42.60}} & \multicolumn{1}{c}{\underline{63.65}} & \multicolumn{1}{c}{61.86} & \multicolumn{1}{c}{82.20} & \multicolumn{1}{c}{75.25} & \multicolumn{1}{c}{50.40} & \multicolumn{1}{c}{\underline{55.27}} & \multicolumn{1}{c}{\textbf{61.00}} & \multicolumn{1}{c}{2.28} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{79.78} & \multicolumn{1}{c}{68.21} & \multicolumn{1}{c}{44.26} & \multicolumn{1}{c}{\underline{38.58}} & \multicolumn{1}{c}{41.57} & \multicolumn{1}{c}{64.23} & \multicolumn{1}{c}{61.86} & \multicolumn{1}{c}{82.20} & \multicolumn{1}{c}{75.05} & \multicolumn{1}{c}{50.30} & \multicolumn{1}{c}{55.64} & \multicolumn{1}{c}{\textbf{60.15}} & \multicolumn{1}{c}{2.13} \\
% & & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{\underline{81.95}} & \multicolumn{1}{c}{70.71} & \multicolumn{1}{c}{44.38} & \multicolumn{1}{c}{37.90} & \multicolumn{1}{c}{41.90} & \multicolumn{1}{c}{68.08} & \multicolumn{1}{c}{62.18} & \multicolumn{1}{c}{83.80} & \multicolumn{1}{c}{74.86} & \multicolumn{1}{c}{50.10} & \multicolumn{1}{c}{\underline{56.21}} & \multicolumn{1}{c}{61.10} & \multicolumn{1}{c}{1.52} \\
% \midrule
% \multicolumn{1}{l}{\multirow{12}{*}{{UL2}}} 
% & \multicolumn{1}{l}{\multirow{5}{*}{{Original}}} & 
% \multicolumn{1}{c}{0} & \multicolumn{1}{c}{53.79} & \multicolumn{1}{c}{8.93} & \multicolumn{1}{c}{33.10} & \multicolumn{1}{c}{33.00} & \multicolumn{1}{c}{34.92} & \multicolumn{1}{c}{47.12} & \multicolumn{1}{c}{\underline{64.17}} & \multicolumn{1}{c}{\underline{87.00}} & \multicolumn{1}{c}{\underline{78.41}} & \multicolumn{1}{c}{\underline{57.46}} & \multicolumn{1}{c}{53.13} & \multicolumn{1}{c}{50.09} & \multicolumn{1}{c}{0.08} \\
% & & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{54.87} & \multicolumn{1}{c}{49.29} & \multicolumn{1}{c}{34.36} & \multicolumn{1}{c}{33.76} & \multicolumn{1}{c}{33.32} & \multicolumn{1}{c}{\underline{47.50}} & \multicolumn{1}{c}{\underline{63.96}} & \multicolumn{1}{c}{\underline{85.00}} & \multicolumn{1}{c}{\underline{78.31}} & \multicolumn{1}{c}{\underline{56.86}} & \multicolumn{1}{c}{\underline{52.29}} & \multicolumn{1}{c}{53.59} & \multicolumn{1}{c}{2.19} \\
% & & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{49.03} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{33.32} & \multicolumn{1}{c}{33.52} & \multicolumn{1}{c}{33.50} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{\underline{66.35}} & \multicolumn{1}{c}{84.20} & \multicolumn{1}{c}{77.66} & \multicolumn{1}{c}{55.96} & \multicolumn{1}{c}{50.09} & \multicolumn{1}{c}{51.83} & \multicolumn{1}{c}{0.72} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{47.51} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{33.30} & \multicolumn{1}{c}{33.30} & \multicolumn{1}{c}{32.97} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{64.67} & \multicolumn{1}{c}{84.20} & \multicolumn{1}{c}{76.77} & \multicolumn{1}{c}{54.60} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{51.26} & \multicolumn{1}{c}{0.41} \\
% & & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{47.29} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{33.50} & \multicolumn{1}{c}{33.30} & \multicolumn{1}{c}{33.12} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{63.84} & \multicolumn{1}{c}{82.00} & \multicolumn{1}{c}{63.09} & \multicolumn{1}{c}{51.64} & \multicolumn{1}{c}{50.13} & \multicolumn{1}{c}{49.49} & \multicolumn{1}{c}{0.53} \\
% \cmidrule{2-16}
% & \multicolumn{1}{l}{\multirow{3}{*}{{RAG}}} & 
% \multicolumn{1}{c}{5} & \multicolumn{1}{c}{52.78} & \multicolumn{1}{c}{46.07} & \multicolumn{1}{c}{33.24} & \multicolumn{1}{c}{32.62} & \multicolumn{1}{c}{32.90} & \multicolumn{1}{c}{44.81} & \multicolumn{1}{c}{65.89} & \multicolumn{1}{c}{84.20} & \multicolumn{1}{c}{78.97} & \multicolumn{1}{c}{56.80} & \multicolumn{1}{c}{51.03} & \multicolumn{1}{c}{52.67} & \multicolumn{1}{c}{1.44} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{53.36} & \multicolumn{1}{c}{45.71} & \multicolumn{1}{c}{33.46} & \multicolumn{1}{c}{31.94} & \multicolumn{1}{c}{32.62} & \multicolumn{1}{c}{45.58} & \multicolumn{1}{c}{\underline{65.59}} & \multicolumn{1}{c}{84.40} & \multicolumn{1}{c}{78.80} & \multicolumn{1}{c}{56.84} & \multicolumn{1}{c}{50.75} & \multicolumn{1}{c}{52.64} & \multicolumn{1}{c}{0.89} \\
% & & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{53.07} & \multicolumn{1}{c}{46.07} & \multicolumn{1}{c}{33.30} & \multicolumn{1}{c}{31.58} & \multicolumn{1}{c}{32.85} & \multicolumn{1}{c}{46.54} & \multicolumn{1}{c}{\underline{65.73}} & \multicolumn{1}{c}{83.60} & \multicolumn{1}{c}{78.92} & \multicolumn{1}{c}{56.90} & \multicolumn{1}{c}{50.19} & \multicolumn{1}{c}{52.61} & \multicolumn{1}{c}{1.01} \\
% \cmidrule{2-16}
% & \multicolumn{1}{l}{\multirow{3}{*}{{FiD}}} &
% \multicolumn{1}{c}{5} & \multicolumn{1}{c}{55.81} & \multicolumn{1}{c}{47.50} & \multicolumn{1}{c}{33.88} & \multicolumn{1}{c}{32.48} & \multicolumn{1}{c}{32.60} & \multicolumn{1}{c}{47.88} & \multicolumn{1}{c}{65.35} & \multicolumn{1}{c}{\underline{84.80}} & \multicolumn{1}{c}{\underline{79.20}} & \multicolumn{1}{c}{\underline{57.12}} & \multicolumn{1}{c}{52.92} & \multicolumn{1}{c}{53.59} & \multicolumn{1}{c}{1.25} \\
% & & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{54.95} & \multicolumn{1}{c}{47.50} & \multicolumn{1}{c}{34.12} & \multicolumn{1}{c}{31.72} & \multicolumn{1}{c}{32.22} & \multicolumn{1}{c}{49.23} & \multicolumn{1}{c}{65.02} & \multicolumn{1}{c}{\underline{85.60}} & \multicolumn{1}{c}{\underline{79.17}} & \multicolumn{1}{c}{\underline{56.98}} & \multicolumn{1}{c}{52.79} & \multicolumn{1}{c}{53.57} & \multicolumn{1}{c}{1.18} \\
% & & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{54.37} & \multicolumn{1}{c}{47.14} & \multicolumn{1}{c}{33.94} & \multicolumn{1}{c}{31.52} & \multicolumn{1}{c}{31.58} & \multicolumn{1}{c}{49.42} & \multicolumn{1}{c}{65.13} & \multicolumn{1}{c}{\underline{86.00}} & \multicolumn{1}{c}{\underline{79.14}} & \multicolumn{1}{c}{\underline{56.92}} & \multicolumn{1}{c}{52.35} & \multicolumn{1}{c}{53.41} & \multicolumn{1}{c}{1.30} \\
% \bottomrule
% \end{tabular}}
% \end{adjustbox}

\begin{table*}
\setlength\tabcolsep{8pt}
\centering
\begin{adjustbox}{width=1\textwidth}{
\begin{tabular}{lcccccccccccc}
\toprule
Model & RTE & CB & ALNI1 & ANLI2 & ANLI3 & WSC & Winogrande & COPA & StoryCloze & HellaSwag & WiC & \textbf{average} \\
\midrule
PaLM-8B & 56.7 & 57.1 & 29.8 & 32.5 & 32.7 & 83.2 & 70.1 & 86.0 & 81.5 & 68.6 & 52.4 & 59.1\\
% PaLM-62B & 76.5 & 78.6 & 38.2 & 35.3 & 40.8 & 88.8 & 79.8 & 93.0 & 86.7 & 80.0 & 57.5 \\
PaLM-540B & 81.2 & 89.3 & 56.9 & 56.1 & 51.2 & 89.5 & 85.1 & 95.0 & 89.0 & 83.8 & 64.6 & \textbf{76.5} \\
GPT-NeoX 20B & - & - & 32.2 & 33.1 & 34.6 & 38.5 & 68.3 & - & - & 53.8 & - & (43.4) \\
GPT3 175B & 72.9 & 82.1 & 36.8 & 34.0 & 40.2 & 75.0 & 77.7 & 92.0 & 87.7 & 79.3 & 55.3 & 66.6 \\
\midrule
T5-\textit{early} 11B & 64.0 & 77.9 & 42.1 & 38.3 & 40.0 & 68.3 & 62.5 & 83.8 & 75.3 & 50.4 & 50.2 & 59.3(50.3)\\
\bottomrule
\end{tabular}}
\end{adjustbox}
% \caption{opt모델, t5계열 우리 모델 성능 비교 with average minimal template}
\caption{\textbf{Comparison with other decoder LLMs for various NLU tasks.} The scores for PaLM and GPT-NeoX are 5-shot results. And the scores for GPT3 and T5-\textit{early} are the highest scores among the reported ones for each task. The average scores inside the parentheses indicate the average scores of tasks where the GPT-NeoX report. Note that normalization is applied to the scores for PaLM and GPT3, which might be advantageous compared to when it is not applied.}
\label{tab: compare to other LLM decoders}
\end{table*}

\begin{table*}
\setlength\tabcolsep{6.5pt}
\centering
\begin{adjustbox}{width=1\textwidth}{
\begin{tabular}{lcccccccccccccccc}
\toprule
Model & Method & Shot & RTE & CB & ANLI1 & ANLI2 & ANLI3 & WSC & Winogrande & COPA & StoryCloze & HellaSwag* & WiC & 
\textbf{average} & \textbf{std} \\
\midrule
\multicolumn{1}{l}{\multirow{12}{*}{{T5-LM}}} 
& \multicolumn{1}{l}{\multirow{5}{*}{\textit{original}}} & 
\multicolumn{1}{c}{0} & \multicolumn{1}{c}{\underline{62.45}} & \multicolumn{1}{c}{48.21} & \multicolumn{1}{c}{\underline{37.70}} & \multicolumn{1}{c}{34.90} & \multicolumn{1}{c}{37.25} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{55.09} & \multicolumn{1}{c}{71.00} & \multicolumn{1}{c}{72.85} & \multicolumn{1}{c}{48.02} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{50.37} & \multicolumn{1}{c}{0.05} \\
& & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{56.68} & \multicolumn{1}{c}{57.86} & \multicolumn{1}{c}{34.96} & \multicolumn{1}{c}{34.40} & \multicolumn{1}{c}{35.87} & \multicolumn{1}{c}{41.92} & \multicolumn{1}{c}{61.15} & \multicolumn{1}{c}{81.20} & \multicolumn{1}{c}{74.83} & \multicolumn{1}{c}{\underline{50.70}} & \multicolumn{1}{c}{50.38} & \multicolumn{1}{c}{52.72} & \multicolumn{1}{c}{3.28} \\
& & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{47.65} & \multicolumn{1}{c}{56.07} & \multicolumn{1}{c}{32.66} & \multicolumn{1}{c}{34.06} & \multicolumn{1}{c}{33.97} & \multicolumn{1}{c}{43.65} & \multicolumn{1}{c}{61.29} & \multicolumn{1}{c}{77.80} & \multicolumn{1}{c}{75.34} & \multicolumn{1}{c}{50.38} & \multicolumn{1}{c}{49.97} & \multicolumn{1}{c}{51.17} & \multicolumn{1}{c}{1.48} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{47.22} & \multicolumn{1}{c}{47.50} & \multicolumn{1}{c}{32.12} & \multicolumn{1}{c}{32.96} & \multicolumn{1}{c}{33.07} & \multicolumn{1}{c}{52.31} & \multicolumn{1}{c}{61.42} & \multicolumn{1}{c}{79.20} & \multicolumn{1}{c}{74.47} & \multicolumn{1}{c}{49.18} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{50.86} & \multicolumn{1}{c}{2.10} \\
& & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{47.08} & \multicolumn{1}{c}{\underline{58.93}} & \multicolumn{1}{c}{32.86} & \multicolumn{1}{c}{32.42} & \multicolumn{1}{c}{34.47} & \multicolumn{1}{c}{\underline{62.12}} & \multicolumn{1}{c}{61.04} & \multicolumn{1}{c}{77.60} & \multicolumn{1}{c}{67.26} & \multicolumn{1}{c}{47.28} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{51.91} & \multicolumn{1}{c}{1.63} \\
\cmidrule{2-16}
& \multicolumn{1}{l}{\multirow{3}{*}{\textit{early-fusion}}} & 
\multicolumn{1}{c}{5} & \multicolumn{1}{c}{53.86} & \multicolumn{1}{c}{56.07} & \multicolumn{1}{c}{35.62} & \multicolumn{1}{c}{\underline{34.84}} & \multicolumn{1}{c}{\underline{36.95}} & \multicolumn{1}{c}{52.50} & \multicolumn{1}{c}{\underline{61.93}} & \multicolumn{1}{c}{82.20} & \multicolumn{1}{c}{75.53} & \multicolumn{1}{c}{50.52} & \multicolumn{1}{c}{49.97} & \multicolumn{1}{c}{53.64} &
\multicolumn{1}{c}{2.07} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{53.29} & \multicolumn{1}{c}{53.21} & \multicolumn{1}{c}{35.82} & \multicolumn{1}{c}{34.58} & \multicolumn{1}{c}{36.47} & \multicolumn{1}{c}{50.96} & \multicolumn{1}{c}{61.86} & \multicolumn{1}{c}{81.80} & \multicolumn{1}{c}{\underline{75.60}} & \multicolumn{1}{c}{50.60} & \multicolumn{1}{c}{50.13} & \multicolumn{1}{c}{53.12} & \multicolumn{1}{c}{1.88} \\
& & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{52.20} & \multicolumn{1}{c}{53.21} & \multicolumn{1}{c}{35.82} & \multicolumn{1}{c}{34.46} & \multicolumn{1}{c}{36.90} & \multicolumn{1}{c}{60.19} & \multicolumn{1}{c}{61.82} & \multicolumn{1}{c}{\underline{82.80}} & \multicolumn{1}{c}{75.25} & \multicolumn{1}{c}{50.44} & \multicolumn{1}{c}{50.31} & \multicolumn{1}{c}{\textbf{53.95}} & \multicolumn{1}{c}{1.30} \\
\cmidrule{2-16}
& \multicolumn{1}{l}{\multirow{3}{*}{\textit{late-fusion}}} & 
\multicolumn{1}{c}{5} & \multicolumn{1}{c}{54.22} & \multicolumn{1}{c}{56.79} & \multicolumn{1}{c}{35.60} & \multicolumn{1}{c}{34.48} & \multicolumn{1}{c}{36.93} & \multicolumn{1}{c}{40.38} & \multicolumn{1}{c}{61.18} & \multicolumn{1}{c}{81.60} & \multicolumn{1}{c}{75.51} & \multicolumn{1}{c}{50.30} & \multicolumn{1}{c}{50.34} & \multicolumn{1}{c}{52.49} & \multicolumn{1}{c}{1.61} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{53.14} & \multicolumn{1}{c}{53.93} & \multicolumn{1}{c}{35.74} & \multicolumn{1}{c}{34.26} & \multicolumn{1}{c}{36.78} & \multicolumn{1}{c}{43.46} & \multicolumn{1}{c}{61.17} & \multicolumn{1}{c}{81.20} & \multicolumn{1}{c}{75.29} & \multicolumn{1}{c}{50.28} & \multicolumn{1}{c}{50.22} & \multicolumn{1}{c}{52.32} & \multicolumn{1}{c}{1.91} \\
& & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{53.29} & \multicolumn{1}{c}{54.29} & \multicolumn{1}{c}{35.48} & \multicolumn{1}{c}{33.96} & \multicolumn{1}{c}{36.67} & \multicolumn{1}{c}{42.88} & \multicolumn{1}{c}{61.37} & \multicolumn{1}{c}{81.60} & \multicolumn{1}{c}{75.02} & \multicolumn{1}{c}{50.20} & \multicolumn{1}{c}{\underline{50.91}} & \multicolumn{1}{c}{52.33} & \multicolumn{1}{c}{1.34} \\
\midrule
\multicolumn{1}{l}{\multirow{12}{*}{{T5}}} 
& \multicolumn{1}{l}{\multirow{5}{*}{\textit{original}}} & 
\multicolumn{1}{c}{0} & \multicolumn{1}{c}{54.87} & \multicolumn{1}{c}{33.93} & \multicolumn{1}{c}{35.50} & \multicolumn{1}{c}{33.10} & \multicolumn{1}{c}{33.42} & \multicolumn{1}{c}{41.35} & \multicolumn{1}{c}{52.33} & \multicolumn{1}{c}{58.00} & \multicolumn{1}{c}{65.53} & \multicolumn{1}{c}{38.02} & \multicolumn{1}{c}{44.67} & \multicolumn{1}{c}{44.61} & \multicolumn{1}{c}{0.08}\\
& & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{62.38} & \multicolumn{1}{c}{67.50} & \multicolumn{1}{c}{37.52} & \multicolumn{1}{c}{38.20} & \multicolumn{1}{c}{\underline{40.10}} & \multicolumn{1}{c}{43.46} & \multicolumn{1}{c}{61.66} & \multicolumn{1}{c}{83.60} & \multicolumn{1}{c}{77.00} & \multicolumn{1}{c}{45.68} & \multicolumn{1}{c}{49.62} & \multicolumn{1}{c}{55.16} & \multicolumn{1}{c}{3.80} \\
& & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{55.02} & \multicolumn{1}{c}{53.93} & \multicolumn{1}{c}{33.70} & \multicolumn{1}{c}{35.00} & \multicolumn{1}{c}{35.62} & \multicolumn{1}{c}{39.04} & \multicolumn{1}{c}{64.64} & \multicolumn{1}{c}{82.80} & \multicolumn{1}{c}{77.71} & \multicolumn{1}{c}{45.32} & \multicolumn{1}{c}{50.50} & \multicolumn{1}{c}{52.12} & \multicolumn{1}{c}{2.78} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{56.10} & \multicolumn{1}{c}{51.43} & \multicolumn{1}{c}{33.66} & \multicolumn{1}{c}{33.88} & \multicolumn{1}{c}{33.80} & \multicolumn{1}{c}{37.88} & \multicolumn{1}{c}{64.66} & \multicolumn{1}{c}{82.00} & \multicolumn{1}{c}{77.67} & \multicolumn{1}{c}{44.64} & \multicolumn{1}{c}{\underline{52.10}} & \multicolumn{1}{c}{51.62} & \multicolumn{1}{c}{2.42} \\
& & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{50.32} & \multicolumn{1}{c}{38.57} & \multicolumn{1}{c}{33.74} & \multicolumn{1}{c}{33.26} & \multicolumn{1}{c}{33.98} & \multicolumn{1}{c}{40.38} & \multicolumn{1}{c}{\underline{64.77}} & \multicolumn{1}{c}{81.20} & \multicolumn{1}{c}{71.50} & \multicolumn{1}{c}{43.10} & \multicolumn{1}{c}{49.94} & \multicolumn{1}{c}{49.16} & \multicolumn{1}{c}{1.99} \\
\cmidrule{2-16}
& \multicolumn{1}{l}{\multirow{3}{*}{\textit{early-fusion}}} & 
\multicolumn{1}{c}{5} & \multicolumn{1}{c}{63.61} & \multicolumn{1}{c}{73.93} & \multicolumn{1}{c}{40.80} & \multicolumn{1}{c}{38.18} & \multicolumn{1}{c}{39.55} & \multicolumn{1}{c}{62.69} & \multicolumn{1}{c}{62.48} & \multicolumn{1}{c}{84.20} & \multicolumn{1}{c}{\underline{77.84}} & \multicolumn{1}{c}{45.90} & \multicolumn{1}{c}{50.03} & \multicolumn{1}{c}{58.11} & \multicolumn{1}{c}{2.00} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{63.39} & \multicolumn{1}{c}{\underline{77.86}} & \multicolumn{1}{c}{41.18} & \multicolumn{1}{c}{\underline{38.32}} & \multicolumn{1}{c}{39.58} & \multicolumn{1}{c}{65.19} & \multicolumn{1}{c}{62.23} & \multicolumn{1}{c}{84.80} & \multicolumn{1}{c}{77.56} & \multicolumn{1}{c}{45.80} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{58.72} & \multicolumn{1}{c}{1.47} \\
& & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{\underline{64.04}} & \multicolumn{1}{c}{75.00} & \multicolumn{1}{c}{\underline{42.08}} & \multicolumn{1}{c}{37.94} & \multicolumn{1}{c}{39.98} & \multicolumn{1}{c}{68.27} & \multicolumn{1}{c}{62.32} & \multicolumn{1}{c}{\underline{85.40}} & \multicolumn{1}{c}{77.57} & \multicolumn{1}{c}{\underline{46.06}} & \multicolumn{1}{c}{50.16} & \multicolumn{1}{c}{\textbf{58.98}} & \multicolumn{1}{c}{0.97} \\
\cmidrule{2-16}
& \multicolumn{1}{l}{\multirow{3}{*}{\textit{late-fusion}}} & 
\multicolumn{1}{c}{5} & \multicolumn{1}{c}{62.74} & \multicolumn{1}{c}{70.36} & \multicolumn{1}{c}{39.88} & \multicolumn{1}{c}{38.12} & \multicolumn{1}{c}{39.27} & \multicolumn{1}{c}{61.92} & \multicolumn{1}{c}{62.00} & \multicolumn{1}{c}{83.80} & \multicolumn{1}{c}{77.56} & \multicolumn{1}{c}{46.02} & \multicolumn{1}{c}{50.03} & \multicolumn{1}{c}{57.43} & \multicolumn{1}{c}{2.20} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{63.32} & \multicolumn{1}{c}{73.93} & \multicolumn{1}{c}{40.16} & \multicolumn{1}{c}{\underline{38.32}} & \multicolumn{1}{c}{39.57} & \multicolumn{1}{c}{63.85} & \multicolumn{1}{c}{62.13} & \multicolumn{1}{c}{83.80} & \multicolumn{1}{c}{77.31} & \multicolumn{1}{c}{45.78} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{58.01} & \multicolumn{1}{c}{1.93} \\
& & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{63.90} & \multicolumn{1}{c}{70.36} & \multicolumn{1}{c}{40.64} & \multicolumn{1}{c}{38.06} & \multicolumn{1}{c}{39.63} & \multicolumn{1}{c}{\underline{69.42}} & \multicolumn{1}{c}{62.15} & \multicolumn{1}{c}{84.80} & \multicolumn{1}{c}{77.27} & \multicolumn{1}{c}{45.88} & \multicolumn{1}{c}{50.22} & \multicolumn{1}{c}{58.39} & \multicolumn{1}{c}{1.32} \\
\midrule
\multicolumn{1}{l}{\multirow{12}{*}{{T0}}} 
& \multicolumn{1}{l}{\multirow{5}{*}{\textit{original}}} & 
\multicolumn{1}{c}{0} & \multicolumn{1}{c}{\underline{81.95}} & \multicolumn{1}{c}{\underline{78.57}} & \multicolumn{1}{c}{\underline{45.20}} & \multicolumn{1}{c}{\underline{40.40}} & \multicolumn{1}{c}{42.42} & \multicolumn{1}{c}{53.85} & \multicolumn{1}{c}{60.93} & \multicolumn{1}{c}{82.00} & \multicolumn{1}{c}{\underline{75.95}} & \multicolumn{1}{c}{50.56} & \multicolumn{1}{c}{55.64} & \multicolumn{1}{c}{60.68} & \multicolumn{1}{c}{0.13} \\
& & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{74.08} & \multicolumn{1}{c}{72.50} & \multicolumn{1}{c}{41.52} & \multicolumn{1}{c}{37.20} & \multicolumn{1}{c}{38.73} & \multicolumn{1}{c}{41.73} & \multicolumn{1}{c}{61.10} & \multicolumn{1}{c}{81.20} & \multicolumn{1}{c}{75.09} & \multicolumn{1}{c}{50.56} & \multicolumn{1}{c}{51.76} & \multicolumn{1}{c}{56.86} & \multicolumn{1}{c}{3.74} \\
& & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{67.94} & \multicolumn{1}{c}{76.07} & \multicolumn{1}{c}{39.42} & \multicolumn{1}{c}{36.28} & \multicolumn{1}{c}{38.53} & \multicolumn{1}{c}{60.19} & \multicolumn{1}{c}{61.56} & \multicolumn{1}{c}{81.60} & \multicolumn{1}{c}{73.18} & \multicolumn{1}{c}{49.06} & \multicolumn{1}{c}{54.20} & \multicolumn{1}{c}{58.00} & \multicolumn{1}{c}{2.73} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{53.57} & \multicolumn{1}{c}{60.36} & \multicolumn{1}{c}{36.06} & \multicolumn{1}{c}{34.80} & \multicolumn{1}{c}{35.12} & \multicolumn{1}{c}{62.88} & \multicolumn{1}{c}{60.36} & \multicolumn{1}{c}{81.60} & \multicolumn{1}{c}{71.28} & \multicolumn{1}{c}{47.92} & \multicolumn{1}{c}{52.98} & \multicolumn{1}{c}{54.27} & \multicolumn{1}{c}{2.72} \\
& & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{47.29} & \multicolumn{1}{c}{63.21} & \multicolumn{1}{c}{33.30} & \multicolumn{1}{c}{33.28} & \multicolumn{1}{c}{33.15} & \multicolumn{1}{c}{63.85} & \multicolumn{1}{c}{60.17} & \multicolumn{1}{c}{79.20} & \multicolumn{1}{c}{61.98} & \multicolumn{1}{c}{46.10} & \multicolumn{1}{c}{50.63} & \multicolumn{1}{c}{52.01} & \multicolumn{1}{c}{1.93} \\
\cmidrule{2-16}
& \multicolumn{1}{l}{\multirow{3}{*}{\textit{early-fusion}}} &
\multicolumn{1}{c}{5} & \multicolumn{1}{c}{81.16} & \multicolumn{1}{c}{76.43} & \multicolumn{1}{c}{44.36} & \multicolumn{1}{c}{37.86} & \multicolumn{1}{c}{\underline{42.60}} & \multicolumn{1}{c}{63.65} & \multicolumn{1}{c}{61.86} & \multicolumn{1}{c}{82.20} & \multicolumn{1}{c}{75.25} & \multicolumn{1}{c}{50.40} & \multicolumn{1}{c}{55.27} & \multicolumn{1}{c}{61.00} & \multicolumn{1}{c}{2.28} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{79.78} & \multicolumn{1}{c}{68.21} & \multicolumn{1}{c}{44.26} & \multicolumn{1}{c}{38.58} & \multicolumn{1}{c}{41.57} & \multicolumn{1}{c}{64.23} & \multicolumn{1}{c}{61.86} & \multicolumn{1}{c}{82.20} & \multicolumn{1}{c}{75.05} & \multicolumn{1}{c}{50.30} & \multicolumn{1}{c}{55.64} & \multicolumn{1}{c}{60.15} & \multicolumn{1}{c}{2.13} \\
& & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{\underline{81.95}} & \multicolumn{1}{c}{70.71} & \multicolumn{1}{c}{44.38} & \multicolumn{1}{c}{37.90} & \multicolumn{1}{c}{41.90} & \multicolumn{1}{c}{\underline{68.08}} & \multicolumn{1}{c}{62.18} & \multicolumn{1}{c}{83.80} & \multicolumn{1}{c}{74.86} & \multicolumn{1}{c}{50.10} & \multicolumn{1}{c}{56.21} & \multicolumn{1}{c}{61.10} & \multicolumn{1}{c}{1.52} \\
\cmidrule{2-16}
& \multicolumn{1}{l}{\multirow{3}{*}{\textit{late-fusion}}} &
\multicolumn{1}{c}{5} & \multicolumn{1}{c}{81.08} & \multicolumn{1}{c}{77.50} & \multicolumn{1}{c}{44.18} & \multicolumn{1}{c}{37.76} & \multicolumn{1}{c}{42.07} & \multicolumn{1}{c}{55.00} & \multicolumn{1}{c}{62.12} & \multicolumn{1}{c}{82.60} & \multicolumn{1}{c}{75.34} & \multicolumn{1}{c}{50.62} & \multicolumn{1}{c}{54.33} & \multicolumn{1}{c}{60.24} & \multicolumn{1}{c}{2.47} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{80.72} & \multicolumn{1}{c}{70.71} & \multicolumn{1}{c}{44.54} & \multicolumn{1}{c}{38.34} & \multicolumn{1}{c}{42.42} & \multicolumn{1}{c}{54.81} & \multicolumn{1}{c}{62.16} & \multicolumn{1}{c}{82.00} & \multicolumn{1}{c}{75.09} & \multicolumn{1}{c}{\underline{50.80}} & \multicolumn{1}{c}{\underline{56.39}} & \multicolumn{1}{c}{59.82} & \multicolumn{1}{c}{2.62} \\
& & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{81.37} & \multicolumn{1}{c}{74.29} & \multicolumn{1}{c}{44.52} & \multicolumn{1}{c}{38.12} & \multicolumn{1}{c}{41.98} & \multicolumn{1}{c}{64.42} & \multicolumn{1}{c}{\underline{62.23}} & \multicolumn{1}{c}{\underline{84.40}} & \multicolumn{1}{c}{74.89} & \multicolumn{1}{c}{50.38} & \multicolumn{1}{c}{56.02} & \multicolumn{1}{c}{\textbf{61.15}} & \multicolumn{1}{c}{1.92} \\
\midrule
\multicolumn{1}{l}{\multirow{12}{*}{{UL2}}} 
& \multicolumn{1}{l}{\multirow{5}{*}{\textit{original}}} & 
\multicolumn{1}{c}{0} & \multicolumn{1}{c}{53.79} & \multicolumn{1}{c}{8.93} & \multicolumn{1}{c}{33.10} & \multicolumn{1}{c}{33.00} & \multicolumn{1}{c}{\underline{34.92}} & \multicolumn{1}{c}{47.12} & \multicolumn{1}{c}{64.17} & \multicolumn{1}{c}{\underline{87.00}} & \multicolumn{1}{c}{78.41} & \multicolumn{1}{c}{\underline{57.46}} & \multicolumn{1}{c}{\underline{53.13}} & \multicolumn{1}{c}{50.09} & \multicolumn{1}{c}{0.08} \\
& & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{54.87} & \multicolumn{1}{c}{49.29} & \multicolumn{1}{c}{\underline{34.36}} & \multicolumn{1}{c}{\underline{33.76}} & \multicolumn{1}{c}{33.32} & \multicolumn{1}{c}{47.50} & \multicolumn{1}{c}{63.96} & \multicolumn{1}{c}{85.00} & \multicolumn{1}{c}{78.31} & \multicolumn{1}{c}{56.86} & \multicolumn{1}{c}{52.29} & \multicolumn{1}{c}{\textbf{53.59}} & \multicolumn{1}{c}{2.19} \\
& & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{49.03} & \multicolumn{1}{c}{\underline{50.00}} & \multicolumn{1}{c}{33.32} & \multicolumn{1}{c}{33.52} & \multicolumn{1}{c}{33.50} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{\underline{66.35}} & \multicolumn{1}{c}{84.20} & \multicolumn{1}{c}{77.66} & \multicolumn{1}{c}{55.96} & \multicolumn{1}{c}{50.09} & \multicolumn{1}{c}{51.83} & \multicolumn{1}{c}{0.72} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{47.51} & \multicolumn{1}{c}{\underline{50.00}} & \multicolumn{1}{c}{33.30} & \multicolumn{1}{c}{33.30} & \multicolumn{1}{c}{32.97} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{64.67} & \multicolumn{1}{c}{84.20} & \multicolumn{1}{c}{76.77} & \multicolumn{1}{c}{54.60} & \multicolumn{1}{c}{50.00} & \multicolumn{1}{c}{51.26} & \multicolumn{1}{c}{0.41} \\
& & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{47.29} & \multicolumn{1}{c}{\underline{50.00}} & \multicolumn{1}{c}{33.50} & \multicolumn{1}{c}{33.30} & \multicolumn{1}{c}{33.12} & \multicolumn{1}{c}{36.54} & \multicolumn{1}{c}{63.84} & \multicolumn{1}{c}{82.00} & \multicolumn{1}{c}{63.09} & \multicolumn{1}{c}{51.64} & \multicolumn{1}{c}{50.13} & \multicolumn{1}{c}{49.49} & \multicolumn{1}{c}{0.53} \\
\cmidrule{2-16}
& \multicolumn{1}{l}{\multirow{3}{*}{\textit{early-fusion}}} &
\multicolumn{1}{c}{5} & \multicolumn{1}{c}{\underline{55.81}} & \multicolumn{1}{c}{47.50} & \multicolumn{1}{c}{33.88} & \multicolumn{1}{c}{32.48} & \multicolumn{1}{c}{32.60} & \multicolumn{1}{c}{47.88} & \multicolumn{1}{c}{65.35} & \multicolumn{1}{c}{84.80} & \multicolumn{1}{c}{\underline{79.20}} & \multicolumn{1}{c}{57.12} & \multicolumn{1}{c}{52.92} & \multicolumn{1}{c}{\textbf{53.59}} & \multicolumn{1}{c}{1.25} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{54.95} & \multicolumn{1}{c}{47.50} & \multicolumn{1}{c}{34.12} & \multicolumn{1}{c}{31.72} & \multicolumn{1}{c}{32.22} & \multicolumn{1}{c}{49.23} & \multicolumn{1}{c}{65.02} & \multicolumn{1}{c}{85.60} & \multicolumn{1}{c}{79.17} & \multicolumn{1}{c}{56.98} & \multicolumn{1}{c}{52.79} & \multicolumn{1}{c}{53.57} & \multicolumn{1}{c}{1.18} \\
& & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{54.37} & \multicolumn{1}{c}{47.14} & \multicolumn{1}{c}{33.94} & \multicolumn{1}{c}{31.52} & \multicolumn{1}{c}{31.58} & \multicolumn{1}{c}{\underline{49.42}} & \multicolumn{1}{c}{65.13} & \multicolumn{1}{c}{86.00} & \multicolumn{1}{c}{79.14} & \multicolumn{1}{c}{56.92} & \multicolumn{1}{c}{52.35} & \multicolumn{1}{c}{53.41} & \multicolumn{1}{c}{1.30} \\
\cmidrule{2-16}
& \multicolumn{1}{l}{\multirow{3}{*}{\textit{late-fusion}}} & 
\multicolumn{1}{c}{5} & \multicolumn{1}{c}{52.78} & \multicolumn{1}{c}{46.07} & \multicolumn{1}{c}{33.24} & \multicolumn{1}{c}{32.62} & \multicolumn{1}{c}{32.90} & \multicolumn{1}{c}{44.81} & \multicolumn{1}{c}{65.89} & \multicolumn{1}{c}{84.20} & \multicolumn{1}{c}{78.97} & \multicolumn{1}{c}{56.80} & \multicolumn{1}{c}{51.03} & \multicolumn{1}{c}{52.67} & \multicolumn{1}{c}{1.44} \\
& & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{53.36} & \multicolumn{1}{c}{45.71} & \multicolumn{1}{c}{33.46} & \multicolumn{1}{c}{31.94} & \multicolumn{1}{c}{32.62} & \multicolumn{1}{c}{45.58} & \multicolumn{1}{c}{65.59} & \multicolumn{1}{c}{84.40} & \multicolumn{1}{c}{78.80} & \multicolumn{1}{c}{56.84} & \multicolumn{1}{c}{50.75} & \multicolumn{1}{c}{52.64} & \multicolumn{1}{c}{0.89} \\
& & \multicolumn{1}{c}{gpt3} & \multicolumn{1}{c}{53.07} & \multicolumn{1}{c}{46.07} & \multicolumn{1}{c}{33.30} & \multicolumn{1}{c}{31.58} & \multicolumn{1}{c}{32.85} & \multicolumn{1}{c}{46.54} & \multicolumn{1}{c}{65.73} & \multicolumn{1}{c}{83.60} & \multicolumn{1}{c}{78.92} & \multicolumn{1}{c}{56.90} & \multicolumn{1}{c}{50.19} & \multicolumn{1}{c}{52.61} & \multicolumn{1}{c}{1.01} \\
\bottomrule
\end{tabular}}
\end{adjustbox}
% \caption{opt모델, t5계열 우리 모델 성능 비교 with average minimal template}
\caption{\textbf{Detailed results for Figure~\ref{fig: mean and std of models}. }
The asterisk(*) on the right side of the task indicates that due to the large size of the test dataset, evaluation is performed on a random sample of 1K instances from the test dataset. 
The underlined values represent the highest scores within each model for each task, while the bold values represent the settings where the average scores for all the tasks are highest within each model.}
\label{tab: full results for seq2seq t0 datasets}
\end{table*}

\section{Detailed explanations about each benchmark dataset}
\label{app: task explanation}
As outlined in Section~\ref{subsec: eval tasks and scoring method}, we adopted the SuperGlue benchmark along with four additional datasets to evaluate natural language understanding ability, and XSum and WebNLG for the generation task. The following provides a detailed description of each dataset.
\begin{itemize}
    \item BoolQ (Boolean Questions): BoolQ is a dataset comprising natural yes/no questions derived from Google searches, with each question paired with a relevant Wikipedia passage.
    \item CB (CommitmentBank): CB is a textual entailment task in which models must judge if a given sentence entails, contradicts, or is neutral to a hypothesis. This task tests a model's ability to understand implications and contradictions in texts.
    \item COPA (Choice of Plausible Alternatives): COPA is a causality detection task where models are required to determine either the cause or effect from two possible options for a given statement.
    \item MultiRC (Multi-Sentence Reading Comprehension): MultiRC involves answering questions about a paragraph, with each question having multiple correct answers. It assesses the model's ability to understand and integrate information from multiple sentences.
    \item ReCoRD (Reading Comprehension with Commonsense Reasoning Dataset): ReCoRD is a multiple-choice QA task that challenges models to predict a masked-out entity from a list of possible entities in the provided passage.
    \item RTE (Recognizing Textual Entailment): RTE requires models to determine whether one sentence logically follows from another.
    \item WiC (Word-in-Context): WiC is a binary classification task for word sense disambiguation, requiring models to determine if a polysemous word is used in the same sense across two different sentences.
    \item WSC (Winograd Schema Challenge): WSC is a coreference resolution task that involves selecting the correct referent of the pronoun in a sentence, requiring commonsense reasoning and everyday knowledge.
    \item HellaSwag: HellaSwag is a benchmark dataset for testing commonsense NLI that is particularly challenging for state-of-the-art models, yet its questions are trivial for humans.
    \item ANLI (Adversarial Natural Language Inference): ANLI is a challenging NLI benchmark comprising a series of progressively harder rounds in which models must identify the relationship (entailment, contradiction, or neutral) between pairs of sentences.
    \item WinoGrande: WinoGrande is a large-scale dataset inspired by the original WSC design, but adjusted to improve both the scale and the hardness of the dataset.
    \item StoryCloze: StoryCloze requires models to read a four-sentence story and then select the correct ending from two options.
    \item WebNLG: WebNLG corpus consists of sets of RDF triplets describing facts and the corresponding facts in form of natural language text. It challenges models to generate coherent and fluent text paragraphs from triplets.
    \item XSum (Extreme Summarization): XSum is a dataset designed for evaluating automatic text summarization. It requires models to create a single-sentence summary of a news article.
\end{itemize}

% Figure environment removed


\begin{table}
\fontsize{8.5pt}{8.5pt}\selectfont
\begin{minipage}{0.48\textwidth}
\setlength\tabcolsep{4pt}
\centering
\begin{tabular}{lcc}
\toprule
\multirow{2}{*}{Model} & 1-shot & 5-shot \\
& (R1/R2/RL) & (R1/R2/RL) \\
\midrule
T5* & 13.72/2.46/11.97 & 7.57/0.66/6.34 \\
T5 & 25.12/8.69/20.72 & 26.39/8.99/21.59 \\
T5-\textit{early} & - & \textbf{30.31}/\textbf{11.55}/\textbf{25.10} \\
BLOOM-7B & 21.50/4.75/16.33 & 21.96/5.06/17.04 \\
OPT-13B & 28.37/9.93/22.46 & 31.32/11.52/24.72 \\
OPT-30B & 27.61/10.06/22.20 & 31.54/12.06/25.19 \\
OPT-66B & 29.31/10.64/23.45 & \textbf{32.52}/\textbf{12.86}/\textbf{26.19} \\
\bottomrule
\end{tabular}
\caption{\textbf{Evaluation on XSum dataset.} The asterisk(*) on the right side of the T5 denotes the case where the sentinel tokens are not used during inference time. R1, R2, and RL denotes ROUGE-{1,2,L}, respectively.}
\label{tab: xsum dec results}
\end{minipage}
\hspace{1em}
\begin{minipage}{0.48\textwidth}
\setlength\tabcolsep{4pt}
\centering
\begin{tabular}{lcc}
\toprule
\multirow{2}{*}{Model} & 1-shot & 32-shot \\
& (R1/R2/RL) & (R1/R2/RL) \\
\midrule
T5* & 18.63/7.98/16.28 & 0.25/0.13/0.24 \\
T5 & 39.13/23.44/32.61 & 41.40/22.63/34.09 \\
T5-\textit{early} & - & \textbf{49.47}/\textbf{29.16}/\textbf{40.84} \\
BLOOM-7B & 62.77/38.54/51.19 & 68.07/43.97/56.1 \\
OPT-13B & 55.03/33.04/45.53 & 66.26/43.44/55.85 \\
OPT-30B & 58.00/35.35/47.78 & 66.57/43.07/55.22 \\
OPT-66B & 61.44/37.54/50.53 & \textbf{68.77}/\textbf{45.37}/\textbf{57.75} \\
\bottomrule
\end{tabular}
\caption{\textbf{Evaluation on WebNLG dataset.} The asterisk(*) on the right side of the T5 denotes the case where the sentinel tokens are not used during inference time. R1, R2, and RL denotes ROUGE-{1,2,L}, respectively.}
\label{tab: webnlg dec results}
\end{minipage}
\end{table}

\end{document}
