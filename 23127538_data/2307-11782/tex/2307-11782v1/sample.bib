@article{lei2019stochastic,
	title={Stochastic gradient descent for non-convex learning without bounded gradient assumptions},
	author={Lei, Yunwen and Hu, Ting and Li, Guiying and Tang, Ke},
	journal={IEEE Transactions on Neural Networks and Learning Systems},
	volume={31},
	number={10},
	pages={4394--4400},
	year={2019}
}
@article{zou2018weighted,
	title={Weighted {AdaGrad} with unified momentum},
	author={Zou, Fangyu and Shen, Li and Jie, Zequn and Sun, Ju and Liu, Wei},
	journal={arXiv preprint arXiv:1808.03408},
	year={2018}
}
@article{liu2022hyper,
	title={On hyper-parameter selection for guaranteed convergence of {RMSProp}},
	author={Liu, Jinlan and Xu, Dongpo and Zhang, Huisheng and Mandic, Danilo},
	journal={Cognitive Neurodynamics},
	pages={1--11},
	year={2022}
}
@article{ward2020adagrad,
	title={{AdaGrad} stepsizes: {S}harp convergence over non-convex landscapes},
	author={Ward, Rachel and Wu, Xiaoxia and Bottou, Leon},
	journal={Journal of Machine Learning Research},
	volume={21},
	number={1},
	pages={9047--9076},
	year={2020}
}
@inproceedings{huang2021super,
	title={{Super-Adam}: {F}aster and universal framework of adaptive gradients},
	author={Huang, Feihu and Li, Junyi and Huang, Heng},
	booktitle={Advances in Neural Information Processing Systems (NIPS)},
	volume={34},
	pages={9074--9085},
	year={2021}
}
@inproceedings{he2022revisit,
	title={Revisit last-iterate convergence of {mSGD} under milder requirement on step size},
	author={He, Xingkang and Chen, Lang and Cheng, Difei and Gupta, Vijay and others},
	booktitle={Advances in Neural Information Processing Systems (NIPS)},
	volume={35},
	pages={36559--36570},
	year={2022}
}
@article{guo2020revisiting,
	title={Revisiting {SGD} with increasingly weighted averaging: {O}ptimization and generalization perspectives},
	author={Guo, Zhishuai and Yan, Yan and Yang, Tianbao},
	journal={arXiv preprint arXiv:2003.04339},
	year={2020}
}
@article{grimmer2019convergence,
	title={Convergence rates for deterministic and stochastic subgradient methods without {Lipschitz} continuity},
	author={Grimmer, Benjamin},
	journal={SIAM Journal on Optimization},
	volume={29},
	number={2},
	pages={1350--1365},
	year={2019},
	publisher={SIAM}
}
@article{
	khaled2022better,
	title={Better Theory for {SGD} in the Nonconvex World},
	author={Ahmed Khaled and Peter Richt{\'a}rik},
	journal={Transactions on Machine Learning Research},
	year={2023}
}
@inproceedings{zaheer2018adaptive,
	title={Adaptive methods for non-convex optimization},
	author={Zaheer, Manzil and Reddi, Sashank and Sachan, Devendra and Kale, Satyen and Kumar, Sanjiv},
	booktitle={Advances in Neural Information Processing Systems (NIPS)},
	volume={31},
	year={2018},
	pages={9815–9825}
}
@inproceedings{sebbouh2021almost,
	title={Almost sure convergence rates for stochastic gradient descent and stochastic heavy ball},
	author={Sebbouh, Othmane and Gower, Robert M and Defazio, Aaron},
	booktitle={Conference on Learning Theory (COLT)},
	pages={3935--3971},
	year={2021}
}
@inproceedings{pennington2014glove,
	title={Glove: {Global} vectors for word representation},
	author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
	booktitle={International Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	pages={1532--1543},
	year={2014}
}
@inproceedings{dean2012large,
	title={Large-scale distributed deep networks},
	author={Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc'aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke},
	booktitle={Advances in Neural Information Processing Systems (NIPS)},
	volume={25},
	year={2012}
}
@article{luo2022sgd,
	title={{SGD}-r$\alpha$: {A} real-time $\alpha$-suffix averaging method for {SGD} with biased gradient estimates},
	author={Luo, Jianqi and Liu, Jinlan and Xu, Dongpo and Zhang, Huisheng},
	journal={Neurocomputing},
	volume={487},
	pages={1--8},
	year={2022}
}
@Article{chow:68,
  author = 	 {C. K. Chow and C. N. Liu},
  title = 	 {Approximating discrete probability distributions with dependence trees},
  journal = 	 {IEEE Transactions on Information Theory},
  year = 	 {1968},
  volume = 	 {IT-14},
  number = 	 {3},
  pages = 	 {462--467}}


@Book{pearl:88,
  author = 	 {Judea Pearl},
  title = 	 {Probabilistic {R}easoning in {I}ntelligent {S}ystems: 
		  {N}etworks of {P}lausible {I}nference},
  publisher = 	 {Morgan Kaufman Publishers},
  year = 	 {1988},
  address = 	 {San Mateo, CA}
}

@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The Annals of Mathematical Statistics},
  pages={400--407},
  year={1951}
}
@inproceedings{agarwal2009information,
  title={Information-theoretic lower bounds on the oracle complexity of convex optimization},
  author={Agarwal, Alekh and Wainwright, Martin J and Bartlett, Peter and Ravikumar, Pradeep},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  volume={22},
  pages={1--9},
  year={2009}
}
@article{rakhlin2011making,
  title={Making gradient descent optimal for strongly convex stochastic optimization},
  author={Rakhlin, Alexander and Shamir, Ohad and Sridharan, Karthik},
  journal={arXiv preprint arXiv:1109.5647},
  year={2011}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={7},
   pages={2121--2159},
  year={2011}
}

@inproceedings{li2019convergence,
  title={On the convergence of stochastic gradient descent with adaptive stepsizes},
  author={Li, Xiaoyu and Orabona, Francesco},
  booktitle={The 22nd international conference on artificial intelligence and statistics},
  pages={983--992},
  year={2019},
  organization={PMLR}
}
@article{sun2020novel,
  title={Novel convergence results of adaptive stochastic gradient descents},
  author={Sun, Tao and Qiao, Linbo and Liao, Qing and Li, Dongsheng},
  journal={IEEE Transactions on Image Processing},
  volume={30},
  pages={1044--1056},
  year={2020},
  publisher={IEEE}
}

@article{li2022variance,
	title={Variance reduction on general adaptive stochastic mirror descent},
	author={Li, Wenjie and Wang, Zhanyu and Zhang, Yichen and Cheng, Guang},
	journal={Machine Learning},
	pages={1--39},
	year={2022},
	publisher={Springer}
}
@article{Liu2021ConvergenceAO,
  title={Convergence analysis of {AdaBound} with relaxed bound functions for non-convex optimization},
  author={Jinlan Liu and Jun Kong and Dongpo Xu and Miao Qi and Yinghua Lu},
  journal={Neural Networks},
  year={2021},
  volume={145},
  pages={300-307}
}

@inproceedings{bottou2010large,
  title={Large-scale machine learning with stochastic gradient descent},
  author={Bottou, L{\'e}on},
  booktitle={International Conference on Computational StatisticsParis France (COMPSTAT)},
  pages={177--186},
  year={2010},
}

@article{shen2019deep,
  title={A deep learning method for {Chinese} singer identification},
  author={Shen, Zebang and Yong, Binbin and Zhang, Gaofeng and Zhou, Rui and Zhou, Qingguo},
  journal={Tsinghua Science and Technology},
  volume={24},
  number={4},
  pages={371--378},
  year={2019},
  publisher={TUP}
}

@article{bolte2010characterizations,
  title={Characterizations of {\L}ojasiewicz inequalities: {S}ubgradient flows, talweg, convexity},
  author={Bolte, J{\'e}r{\^o}me and Daniilidis, Aris and Ley, Olivier and Mazet, Laurent},
  journal={Transactions of the American Mathematical Society},
  volume={362},
  number={6},
  pages={3319--3363},
  year={2010}
}

@article{attouch2010proximal,
  title={Proximal alternating minimization and projection methods for non-convex problems: {An} approach based on the Kurdyka-{\L}ojasiewicz inequality},
  author={Attouch, H{\'e}dy and Bolte, J{\'e}r{\^o}me and Redont, Patrick and Soubeyran, Antoine},
  journal={Mathematics of operations research},
  volume={35},
  number={2},
  pages={438--457},
  year={2010},
  publisher={INFORMS}
}
@inproceedings{kingma2014adam,
	author = {Diederik P. Kingma and Jimmy Ba},
	title = {Adam: {A} Method for Stochastic Optimization},
	booktitle = { International Conference on Learning 
	Representations (ICLR)},
	year = {2015}
}

@inproceedings{reddi2019convergence,
  title={On the convergence of {Adam} and beyond},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2018},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
}
@inproceedings{zou2019sufficient,
  title={A sufficient condition for convergences of {Adam} and {RMSProp}},
  author={Zou, Fangyu and Shen, Li and Jie, Zequn and Zhang, Weizhong and Liu, Wei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={11127--11135},
  year={2019}
}
@article{tieleman2012lecture,
  title={Lecture 6.5-{RMSProp}: {Divide} the gradient by a running average of its recent magnitude},
  author={Tieleman, Tijmen and Hinton, Geoffrey},
  journal={COURSERA: Neural Networks for Machine Learning},
  volume={4},
  number={2},
  pages={26--31},
  year={2012}
}
@article{bassily2018exponential,
  title={On exponential convergence of {SGD} in non-convex over-parametrized learning},
  author={Bassily, Raef and Belkin, Mikhail and Ma, Siyuan},
  journal={arXiv preprint arXiv:1811.02564},
  year={2018}
}
@article{guo2021novel,
	title={A novel convergence analysis for algorithms of the {Adam} family},
	author={Guo, Zhishuai and Xu, Yi and Yin, Wotao and Jin, Rong and Yang, Tianbao},
	journal={arXiv preprint arXiv:2112.03459},
	year={2021}
}

@inproceedings{xie2020linear,
  title={Linear convergence of adaptive stochastic gradient descent},
  author={Xie, Yuege and Wu, Xiaoxia and Ward, Rachel},
  booktitle={International Conference on Artificial Intelligence and Statistics (ICAIS)},
  pages={1475--1485},
  year={2020}
}
@article{defossez2020simple,
  title={A simple convergence proof of {Adam} and {AdaGrad}},
  author={D{\'e}fossez, Alexandre and Bottou, L{\'e}on and Bach, Francis and Usunier, Nicolas},
  journal={arXiv preprint arXiv:2003.02395},
  year={2020}
}

@inproceedings{
zhang2022adam,
title={{Adam} Can Converge Without Any Modification On Update Rules},
author={Yushun Zhang and Congliang Chen and Naichen Shi and Ruoyu Sun and Zhiquan Luo},
booktitle={Advances in Neural Information Processing Systems (NIPS)},
 pages = {28386--28399},
year={2022},
}

@article{defossezsimple,
  title={A Simple Convergence Proof of {Adam} and {AdaGrad}},
  author={D{\'e}fossez, Alexandre and Bottou, Leon and Bach, Francis and Usunier, Nicolas},
  journal={Transactions on Machine Learning Research},
  year={2022}
}
@inproceedings{chen2019convergence,
  title={On the convergence of a class of {Adam}-type algorithms for non-convex optimization},
  author={Chen, Xiangyi and Liu, Sijia and Sun, Ruoyu and Hong, Mingyi},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2019},
}

@article{bertsekas2000gradient,
  title={Gradient convergence in gradient methods with errors},
  author={Bertsekas, Dimitri P and Tsitsiklis, John N},
  journal={SIAM Journal on Optimization},
  volume={10},
  number={3},
  pages={627--642},
  year={2000},
  publisher={SIAM}
}
@article{LIU202327,
title = {Last-iterate convergence analysis of stochastic momentum methods for neural networks},
journal = {Neurocomputing},
volume = {527},
pages = {27-35},
year = {2023},
author = {Jinlan Liu and Dongpo Xu and Yinghua Lu and Jun Kong and Danilo P. Mandic}
}
@article{Nemirovski2008RobustSA,
  title={Robust Stochastic Approximation Approach to Stochastic Programming},
  author={Arkadi Nemirovski and Anatoli B. Juditsky and Guanghui Lan and Alexander Shapiro},
  journal={SIAM Journal on Optimization},
  year={2008},
  volume={19},
  pages={1574-1609}
}
@article{dong2022stochastic,
  title={Stochastic Gradient Descent with Dependent Data for Offline Reinforcement Learning},
  author={Dong, Jing and Tong, Xin T},
  journal={arXiv preprint arXiv:2202.02850},
  year={2022}
}
@inproceedings{rakhlin2012making,
  title={Making gradient descent optimal for strongly convex stochastic optimization},
  author={Rakhlin, Alexander and Shamir, Ohad and Sridharan, Karthik},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={1571--1578},
  year={2012}
}
@inproceedings{karimi2016linear,
  title={Linear convergence of gradient and proximal-gradient methods under the {Polyak-{\L}ojasiewicz} condition},
  author={Karimi, Hamed and Nutini, Julie and Schmidt, Mark},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD)},
  pages={795--811},
  year={2016}
}
@inproceedings{
luo2018adaptive,
title={Adaptive Gradient Methods with Dynamic Bound of Learning Rate},
author={Liangchen Luo and Yuanhao Xiong and Yan Liu},
booktitle={International Conference on Learning Representations (ICLR)},
year={2019}
}
@inproceedings{Chen2018ClosingTG,
	title={Closing the generalization gap of adaptive gradient methods in training deep neural networks},
	author={Chen, Jinghui and Zhou, Dongruo and Tang, Yiqi and Yang, Ziyan and Cao, Yuan and Gu, Quanquan},
	booktitle={International Joint Conferences on Artificial Intelligence (IJCAI)},
	pages={3267--3275},
	year={2021}
}

@article{wang2022provable,
  title={Provable Adaptivity in {Adam}},
  author={Wang, Bohan and Zhang, Yushun and Zhang, Huishuai and Meng, Qi and Ma, Zhiming and Liu, Tieyan and Chen, Wei},
  journal={arXiv preprint arXiv:2208.09900},
  year={2022}
}
@article{ghadimi2016accelerated,
  title={Accelerated gradient methods for non-convex nonlinear and stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={Mathematical Programming},
  volume={156},
  number={1-2},
  pages={59--99},
  year={2016},
  publisher={Springer}
}
@book{galambos1995advanced,
  title={Advanced Probability Theory},
  author={Galambos, Janos},
  volume={10},
  year={1995},
  publisher={CRC Press}
}
@article{zorichmathematical,
  title={Mathematical Analysis I},
  author={Zorich, Vladimir A}
}
@article{bottou2018optimization,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={SIAM review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018},
  publisher={SIAM}
}
@inproceedings{liu2022almost,
  title={On almost sure convergence rates of stochastic gradient methods},
  author={Liu, Jun and Yuan, Ye},
  booktitle={Conference on Learning Theory (COLT)},
  pages={2963--2983},
  year={2022},
}
@inproceedings{mukkamala2017variants,
  title={Variants of {RMSProp} and {AdaGrad} with logarithmic regret bounds},
  author={Mukkamala, Mahesh Chandra and Hein, Matthias},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={2545--2553},
  year={2017}
}
@book{rudin1976principles,
  title={Principles of mathematical analysis},
  author={Rudin, Walter},
  volume={3},
  year={1976},
  publisher={McGraw-hill New York}
}
@article{lin2020accelerated,
  title={Accelerated optimization for machine learning},
  author={Lin, Zhouchen and Li, Huan and Fang, Cong},
  journal={Nature Singapore: Springer},
  year={2020},
  publisher={Springer}
}
@inproceedings{sun2019non,
  title={Non-ergodic convergence analysis of {Heavy}-ball algorithms},
  author={Sun, Tao and Yin, Penghang and Li, Dongsheng and Huang, Chun and Guan, Lei and Jiang, Hao},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)},
  pages={5033--5040},
  year={2019}
}
@book{nesterov2003introductory,
	title={Introductory {Lectures} on {Convex} {Optimization}: {A} {Basic} {Course}},
	author={Nesterov, Yurii},
	volume={87},
	year={2003},
	publisher={Springer Science \& Business Media}
}
@article{ruder2016overview,
	title={An overview of gradient descent optimization algorithms},
	author={Ruder, Sebastian},
	journal={arXiv preprint arXiv:1609.04747},
	year={2016}
}
@inproceedings{10.5555/2969033.2969154,
	author = {Dauphin, Yann N. and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
	title = {Identifying and attacking the saddle point problem in high-dimensional non-convex optimization},
	year = {2014},
	booktitle = {Advances in Neural Information Processing Systems (NIPS)},
	pages = {2933–2941}
}
@article{chambolle2016ergodic,
	title={On the ergodic convergence rates of a first-order primal-dual algorithm},
	author={Chambolle, Antonin and Pock, Thomas},
	journal={Mathematical Programming},
	volume={159},
	number={1-2},
	pages={253--287},
	year={2016}
}
@article{defazio2022adaptivity,
	title={Adaptivity without compromise: A momentumized, adaptive, dual averaged gradient method for stochastic optimization},
	author={Defazio, Aaron and Jelassi, Samy},
	journal={Journal of Machine Learning Research},
	volume={23},
	number={1},
	pages={6429--6462},
	year={2022}
}

@article{wang,
	title={On the convergence of stochastic gradient descent with bandwidth-based step size},
	author={Wang, Xiaoyu and Yuan, Yaxiang},
	journal={Journal of Machine Learning Research},
	volume={24},
	number={48},
	pages={1--49},
	year={2023}
}
@article{gadat2022asymptotic,
	title={Asymptotic study of stochastic adaptive algorithms in non-convex landscape},
	author={Gadat, S{\'e}bastien and Gavra, Ioana},
	journal={Journal of Machine Learning Research},
	volume={23},
	number={1},
	pages={10357--10410},
	year={2022}
}
@inproceedings{wang2023convergence,
	title={Convergence of {AdaGrad} for Non-convex Objectives: {S}imple Proofs and Relaxed Assumptions}, 
	author={Bohan Wang and Huishuai Zhang and Zhiming Ma and Wei Chen},
	booktitle ={Conference on Learning Theory (COLT)},
	year={2023}
}




@article{li2022simple,
	title={Simple and optimal stochastic gradient methods for nonsmooth nonconvex optimization},
	author={Li, Zhize and Li, Jian},
	journal={Journal of Machine Learning Research},
	volume={23},
	number={1},
	pages={10891--10951},
	year={2022}
}
@article{new,
	author  = {Lam M. Nguyen and Phuong Ha Nguyen and Peter Richt{{\'a}}rik and Katya Scheinberg and Martin Tak{{\'a}}{\v{c}} and Marten van Dijk},
	title   = {New Convergence Aspects of Stochastic Gradient Algorithms},
	journal = {Journal of Machine Learning Research},
	year    = {2019},
	volume  = {20},
	number  = {176},
	pages   = {1--49}
}
@inproceedings{kavis2022high,
	title={High Probability Bounds for a Class of Nonconvex Algorithms with {AdaGrad} Stepsize},
	author={Kavis, Ali and Levy, Kfir and Cevher, Volkan},
	booktitle={International Conference on Learning Representations (ICLR)},
	year={2022}
}