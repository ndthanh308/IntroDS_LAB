\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2009)Agarwal, Wainwright, Bartlett, and
  Ravikumar]{agarwal2009information}
Alekh Agarwal, Martin~J Wainwright, Peter Bartlett, and Pradeep Ravikumar.
\newblock Information-theoretic lower bounds on the oracle complexity of convex
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  volume~22, pages 1--9, 2009.

\bibitem[Bassily et~al.(2018)Bassily, Belkin, and Ma]{bassily2018exponential}
Raef Bassily, Mikhail Belkin, and Siyuan Ma.
\newblock On exponential convergence of {SGD} in non-convex over-parametrized
  learning.
\newblock \emph{arXiv preprint arXiv:1811.02564}, 2018.

\bibitem[Bertsekas and Tsitsiklis(2000)]{bertsekas2000gradient}
Dimitri~P Bertsekas and John~N Tsitsiklis.
\newblock Gradient convergence in gradient methods with errors.
\newblock \emph{SIAM Journal on Optimization}, 10\penalty0 (3):\penalty0
  627--642, 2000.

\bibitem[Bottou(2010)]{bottou2010large}
L{\'e}on Bottou.
\newblock Large-scale machine learning with stochastic gradient descent.
\newblock In \emph{International Conference on Computational StatisticsParis
  France (COMPSTAT)}, pages 177--186, 2010.

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and
  Nocedal]{bottou2018optimization}
L{\'e}on Bottou, Frank~E Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{SIAM review}, 60\penalty0 (2):\penalty0 223--311, 2018.

\bibitem[Chambolle and Pock(2016)]{chambolle2016ergodic}
Antonin Chambolle and Thomas Pock.
\newblock On the ergodic convergence rates of a first-order primal-dual
  algorithm.
\newblock \emph{Mathematical Programming}, 159\penalty0 (1-2):\penalty0
  253--287, 2016.

\bibitem[Chen et~al.(2021)Chen, Zhou, Tang, Yang, Cao, and
  Gu]{Chen2018ClosingTG}
Jinghui Chen, Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu.
\newblock Closing the generalization gap of adaptive gradient methods in
  training deep neural networks.
\newblock In \emph{International Joint Conferences on Artificial Intelligence
  (IJCAI)}, pages 3267--3275, 2021.

\bibitem[Chen et~al.(2019)Chen, Liu, Sun, and Hong]{chen2019convergence}
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong.
\newblock On the convergence of a class of {Adam}-type algorithms for
  non-convex optimization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Dauphin et~al.(2014)Dauphin, Pascanu, Gulcehre, Cho, Ganguli, and
  Bengio]{10.5555/2969033.2969154}
Yann~N. Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli,
  and Yoshua Bengio.
\newblock Identifying and attacking the saddle point problem in
  high-dimensional non-convex optimization.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  page 2933–2941, 2014.

\bibitem[Dean et~al.(2012)Dean, Corrado, Monga, Chen, Devin, Mao, Ranzato,
  Senior, Tucker, and Yang]{dean2012large}
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
  Marc'aurelio Ranzato, Andrew Senior, Paul Tucker, and Ke~Yang.
\newblock Large-scale distributed deep networks.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  volume~25, 2012.

\bibitem[Defazio and Jelassi(2022)]{defazio2022adaptivity}
Aaron Defazio and Samy Jelassi.
\newblock Adaptivity without compromise: A momentumized, adaptive, dual
  averaged gradient method for stochastic optimization.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0
  (1):\penalty0 6429--6462, 2022.

\bibitem[D{\'e}fossez et~al.(2022)D{\'e}fossez, Bottou, Bach, and
  Usunier]{defossezsimple}
Alexandre D{\'e}fossez, Leon Bottou, Francis Bach, and Nicolas Usunier.
\newblock A simple convergence proof of {Adam} and {AdaGrad}.
\newblock \emph{Transactions on Machine Learning Research}, 2022.

\bibitem[Dong and Tong(2022)]{dong2022stochastic}
Jing Dong and Xin~T Tong.
\newblock Stochastic gradient descent with dependent data for offline
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2202.02850}, 2022.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (7):\penalty0 2121--2159, 2011.

\bibitem[Gadat and Gavra(2022)]{gadat2022asymptotic}
S{\'e}bastien Gadat and Ioana Gavra.
\newblock Asymptotic study of stochastic adaptive algorithms in non-convex
  landscape.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0
  (1):\penalty0 10357--10410, 2022.

\bibitem[Galambos(1995)]{galambos1995advanced}
Janos Galambos.
\newblock \emph{Advanced Probability Theory}, volume~10.
\newblock CRC Press, 1995.

\bibitem[Ghadimi and Lan(2016)]{ghadimi2016accelerated}
Saeed Ghadimi and Guanghui Lan.
\newblock Accelerated gradient methods for non-convex nonlinear and stochastic
  programming.
\newblock \emph{Mathematical Programming}, 156\penalty0 (1-2):\penalty0 59--99,
  2016.

\bibitem[Guo et~al.(2020)Guo, Yan, and Yang]{guo2020revisiting}
Zhishuai Guo, Yan Yan, and Tianbao Yang.
\newblock Revisiting {SGD} with increasingly weighted averaging: {O}ptimization
  and generalization perspectives.
\newblock \emph{arXiv preprint arXiv:2003.04339}, 2020.

\bibitem[Guo et~al.(2021)Guo, Xu, Yin, Jin, and Yang]{guo2021novel}
Zhishuai Guo, Yi~Xu, Wotao Yin, Rong Jin, and Tianbao Yang.
\newblock A novel convergence analysis for algorithms of the {Adam} family.
\newblock \emph{arXiv preprint arXiv:2112.03459}, 2021.

\bibitem[He et~al.(2022)He, Chen, Cheng, Gupta, et~al.]{he2022revisit}
Xingkang He, Lang Chen, Difei Cheng, Vijay Gupta, et~al.
\newblock Revisit last-iterate convergence of {mSGD} under milder requirement
  on step size.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  volume~35, pages 36559--36570, 2022.

\bibitem[Huang et~al.(2021)Huang, Li, and Huang]{huang2021super}
Feihu Huang, Junyi Li, and Heng Huang.
\newblock {Super-Adam}: {F}aster and universal framework of adaptive gradients.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  volume~34, pages 9074--9085, 2021.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and Schmidt]{karimi2016linear}
Hamed Karimi, Julie Nutini, and Mark Schmidt.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the {Polyak-{\L}ojasiewicz} condition.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases (ECML PKDD)}, pages 795--811, 2016.

\bibitem[Kavis et~al.(2022)Kavis, Levy, and Cevher]{kavis2022high}
Ali Kavis, Kfir Levy, and Volkan Cevher.
\newblock High probability bounds for a class of nonconvex algorithms with
  {AdaGrad} stepsize.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2022.

\bibitem[Khaled and Richt{\'a}rik(2023)]{khaled2022better}
Ahmed Khaled and Peter Richt{\'a}rik.
\newblock Better theory for {SGD} in the nonconvex world.
\newblock \emph{Transactions on Machine Learning Research}, 2023.

\bibitem[Kingma and Ba(2015)]{kingma2014adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2015.

\bibitem[Lei et~al.(2019)Lei, Hu, Li, and Tang]{lei2019stochastic}
Yunwen Lei, Ting Hu, Guiying Li, and Ke~Tang.
\newblock Stochastic gradient descent for non-convex learning without bounded
  gradient assumptions.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  31\penalty0 (10):\penalty0 4394--4400, 2019.

\bibitem[Li and Li(2022)]{li2022simple}
Zhize Li and Jian Li.
\newblock Simple and optimal stochastic gradient methods for nonsmooth
  nonconvex optimization.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0
  (1):\penalty0 10891--10951, 2022.

\bibitem[Liu et~al.(2021)Liu, Kong, Xu, Qi, and Lu]{Liu2021ConvergenceAO}
Jinlan Liu, Jun Kong, Dongpo Xu, Miao Qi, and Yinghua Lu.
\newblock Convergence analysis of {AdaBound} with relaxed bound functions for
  non-convex optimization.
\newblock \emph{Neural Networks}, 145:\penalty0 300--307, 2021.

\bibitem[Liu et~al.(2022)Liu, Xu, Zhang, and Mandic]{liu2022hyper}
Jinlan Liu, Dongpo Xu, Huisheng Zhang, and Danilo Mandic.
\newblock On hyper-parameter selection for guaranteed convergence of {RMSProp}.
\newblock \emph{Cognitive Neurodynamics}, pages 1--11, 2022.

\bibitem[Liu et~al.(2023)Liu, Xu, Lu, Kong, and Mandic]{LIU202327}
Jinlan Liu, Dongpo Xu, Yinghua Lu, Jun Kong, and Danilo~P. Mandic.
\newblock Last-iterate convergence analysis of stochastic momentum methods for
  neural networks.
\newblock \emph{Neurocomputing}, 527:\penalty0 27--35, 2023.

\bibitem[Liu and Yuan(2022)]{liu2022almost}
Jun Liu and Ye~Yuan.
\newblock On almost sure convergence rates of stochastic gradient methods.
\newblock In \emph{Conference on Learning Theory (COLT)}, pages 2963--2983,
  2022.

\bibitem[Luo et~al.(2022)Luo, Liu, Xu, and Zhang]{luo2022sgd}
Jianqi Luo, Jinlan Liu, Dongpo Xu, and Huisheng Zhang.
\newblock {SGD}-r$\alpha$: {A} real-time $\alpha$-suffix averaging method for
  {SGD} with biased gradient estimates.
\newblock \emph{Neurocomputing}, 487:\penalty0 1--8, 2022.

\bibitem[Luo et~al.(2019)Luo, Xiong, and Liu]{luo2018adaptive}
Liangchen Luo, Yuanhao Xiong, and Yan Liu.
\newblock Adaptive gradient methods with dynamic bound of learning rate.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Mukkamala and Hein(2017)]{mukkamala2017variants}
Mahesh~Chandra Mukkamala and Matthias Hein.
\newblock Variants of {RMSProp} and {AdaGrad} with logarithmic regret bounds.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  2545--2553, 2017.

\bibitem[Nemirovski et~al.(2008)Nemirovski, Juditsky, Lan, and
  Shapiro]{Nemirovski2008RobustSA}
Arkadi Nemirovski, Anatoli~B. Juditsky, Guanghui Lan, and Alexander Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM Journal on Optimization}, 19:\penalty0 1574--1609, 2008.

\bibitem[Nesterov(2003)]{nesterov2003introductory}
Yurii Nesterov.
\newblock \emph{Introductory {Lectures} on {Convex} {Optimization}: {A} {Basic}
  {Course}}, volume~87.
\newblock Springer Science \& Business Media, 2003.

\bibitem[Nguyen et~al.(2019)Nguyen, Nguyen, Richt{{\'a}}rik, Scheinberg,
  Tak{{\'a}}{\v{c}}, and van Dijk]{new}
Lam~M. Nguyen, Phuong~Ha Nguyen, Peter Richt{{\'a}}rik, Katya Scheinberg,
  Martin Tak{{\'a}}{\v{c}}, and Marten van Dijk.
\newblock New convergence aspects of stochastic gradient algorithms.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (176):\penalty0 1--49, 2019.

\bibitem[Pennington et~al.(2014)Pennington, Socher, and
  Manning]{pennington2014glove}
Jeffrey Pennington, Richard Socher, and Christopher~D Manning.
\newblock Glove: {Global} vectors for word representation.
\newblock In \emph{International Conference on Empirical Methods in Natural
  Language Processing (EMNLP)}, pages 1532--1543, 2014.

\bibitem[Rakhlin et~al.(2012)Rakhlin, Shamir, and Sridharan]{rakhlin2012making}
Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan.
\newblock Making gradient descent optimal for strongly convex stochastic
  optimization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  1571--1578, 2012.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{reddi2019convergence}
Sashank~J Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of {Adam} and beyond.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Robbins and Monro(1951)]{robbins1951stochastic}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock \emph{The Annals of Mathematical Statistics}, pages 400--407, 1951.

\bibitem[Ruder(2016)]{ruder2016overview}
Sebastian Ruder.
\newblock An overview of gradient descent optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1609.04747}, 2016.

\bibitem[Sebbouh et~al.(2021)Sebbouh, Gower, and Defazio]{sebbouh2021almost}
Othmane Sebbouh, Robert~M Gower, and Aaron Defazio.
\newblock Almost sure convergence rates for stochastic gradient descent and
  stochastic heavy ball.
\newblock In \emph{Conference on Learning Theory (COLT)}, pages 3935--3971,
  2021.

\bibitem[Shen et~al.(2019)Shen, Yong, Zhang, Zhou, and Zhou]{shen2019deep}
Zebang Shen, Binbin Yong, Gaofeng Zhang, Rui Zhou, and Qingguo Zhou.
\newblock A deep learning method for {Chinese} singer identification.
\newblock \emph{Tsinghua Science and Technology}, 24\penalty0 (4):\penalty0
  371--378, 2019.

\bibitem[Sun et~al.(2019)Sun, Yin, Li, Huang, Guan, and Jiang]{sun2019non}
Tao Sun, Penghang Yin, Dongsheng Li, Chun Huang, Lei Guan, and Hao Jiang.
\newblock Non-ergodic convergence analysis of {Heavy}-ball algorithms.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence (AAAI)}, pages 5033--5040, 2019.

\bibitem[Sun et~al.(2020)Sun, Qiao, Liao, and Li]{sun2020novel}
Tao Sun, Linbo Qiao, Qing Liao, and Dongsheng Li.
\newblock Novel convergence results of adaptive stochastic gradient descents.
\newblock \emph{IEEE Transactions on Image Processing}, 30:\penalty0
  1044--1056, 2020.

\bibitem[Tieleman and Hinton(2012)]{tieleman2012lecture}
Tijmen Tieleman and Geoffrey Hinton.
\newblock Lecture 6.5-{RMSProp}: {Divide} the gradient by a running average of
  its recent magnitude.
\newblock \emph{COURSERA: Neural Networks for Machine Learning}, 4\penalty0
  (2):\penalty0 26--31, 2012.

\bibitem[Wang et~al.(2022)Wang, Zhang, Zhang, Meng, Ma, Liu, and
  Chen]{wang2022provable}
Bohan Wang, Yushun Zhang, Huishuai Zhang, Qi~Meng, Zhiming Ma, Tieyan Liu, and
  Wei Chen.
\newblock Provable adaptivity in {Adam}.
\newblock \emph{arXiv preprint arXiv:2208.09900}, 2022.

\bibitem[Wang et~al.(2023)Wang, Zhang, Ma, and Chen]{wang2023convergence}
Bohan Wang, Huishuai Zhang, Zhiming Ma, and Wei Chen.
\newblock Convergence of {AdaGrad} for non-convex objectives: {S}imple proofs
  and relaxed assumptions.
\newblock In \emph{Conference on Learning Theory (COLT)}, 2023.

\bibitem[Wang and Yuan(2023)]{wang}
Xiaoyu Wang and Yaxiang Yuan.
\newblock On the convergence of stochastic gradient descent with
  bandwidth-based step size.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0
  (48):\penalty0 1--49, 2023.

\bibitem[Ward et~al.(2020)Ward, Wu, and Bottou]{ward2020adagrad}
Rachel Ward, Xiaoxia Wu, and Leon Bottou.
\newblock {AdaGrad} stepsizes: {S}harp convergence over non-convex landscapes.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (1):\penalty0 9047--9076, 2020.

\bibitem[Xie et~al.(2020)Xie, Wu, and Ward]{xie2020linear}
Yuege Xie, Xiaoxia Wu, and Rachel Ward.
\newblock Linear convergence of adaptive stochastic gradient descent.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (ICAIS)}, pages 1475--1485, 2020.

\bibitem[Zaheer et~al.(2018)Zaheer, Reddi, Sachan, Kale, and
  Kumar]{zaheer2018adaptive}
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar.
\newblock Adaptive methods for non-convex optimization.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  volume~31, page 9815–9825, 2018.

\bibitem[Zhang et~al.(2022)Zhang, Chen, Shi, Sun, and Luo]{zhang2022adam}
Yushun Zhang, Congliang Chen, Naichen Shi, Ruoyu Sun, and Zhiquan Luo.
\newblock {Adam} can converge without any modification on update rules.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pages 28386--28399, 2022.

\bibitem[Zou et~al.(2018)Zou, Shen, Jie, Sun, and Liu]{zou2018weighted}
Fangyu Zou, Li~Shen, Zequn Jie, Ju~Sun, and Wei Liu.
\newblock Weighted {AdaGrad} with unified momentum.
\newblock \emph{arXiv preprint arXiv:1808.03408}, 2018.

\bibitem[Zou et~al.(2019)Zou, Shen, Jie, Zhang, and Liu]{zou2019sufficient}
Fangyu Zou, Li~Shen, Zequn Jie, Weizhong Zhang, and Wei Liu.
\newblock A sufficient condition for convergences of {Adam} and {RMSProp}.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pages 11127--11135, 2019.

\end{thebibliography}
