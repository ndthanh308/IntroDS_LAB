For much of the history of supernova cosmology, parameter estimation was performed via Bayesian methods which maximise the likelihood by minimising a $\chi^{2}$ function between the observed distance modulus of type Ia supernovae (SNe Ia) and the distance modulus predicted by cosmological theory.

This method of parameter estimation is shown in Equation 4 of~\cite{RiessFilippenko1998} and Equation 4 of ~\cite{PerlmutterAldering1999}, which detail the discovery of the accelerated expansion of the Universe using type Ia supernovae (SNe Ia).

Since these early efforts, parameter estimation has expanded in complexity to account for additional systematic uncertainties~\citep{ConleyGuy2011}, and to leverage large simulated data-sets to correct for contamination of core collapse supernovae~\citep{KunzHlozek2012}, and observational biases~\citep{KesslerScolnic2017}. This increased complexity is facilitated by cosmological pipelines to perform accurate parameter estimation. Due to the complex structure of modern cosmological pipelines, it is no longer possible to analytically define a likelihood function which describes the analysis being performed by these pipelines. As such, it is difficult to rigorously test the final cosmological contours produced by these cosmological pipelines, and to validate that the reported uncertainties are accurate.

There have been a number of attempts at developing alternative Bayesian frameworks which do not suffer from a non-analytic likelihood function. One such example is Approximate Bayesian Computation~\citep[ABC;][]{JenningsWolf2016, JenningsMadigan2017}, which uses realistic simulations to perform likelihood free parameter inference, at the cost of dramatically increased computation time due to the large number of simulations required. Another alternative Bayesian framework is Bayesian hierarchical models (BHM), which was implemented for supernova cosmology in~\citep[Steve;][]{HintonDavis2019},~\citep[UNITY;][]{RubinAldering2015}, and~\citep[BayeSN;][]{MandelThorp2021}. BHMs utilise multiple layers of connected parameters, allowing for a more complex analytical likelihood function to be defined.

Though these alternative frameworks have significant advantages over the $\chi^{2}$ minimisation methods, there has not been wide-spread adoption of these techniques, and many modern cosmological analyses, such as the Dark Energy Survey~\citep[DES;][]{DarkEnergySurveyCollaborationAbbott2016}, PANTHEON+~\citep{BroutScolnic2022}, and simulations of the upcoming Legacy Survey of Space and Time~\cite[LSST;][]{LSSTScienceCollaborationAbell2009,SanchezKessler2022,MitraKessler2022} still use the simpler $\chi^{2}$ method.

As modern analyses still use cosmological pipelines which rely on the $\chi^{2}$ methodology, it is important to rigorously test these cosmological pipelines. While each individual component of the SN Ia analysis pipeline is well-tested~\citep{LaskerKessler2019,KesslerNarayan2019,PopovicBrout2021,ToyWiseman2023,TaylorJones2023,KelseySullivan2023,VincenziSullivan2023}, a complete end-to-end consistency check is still necessary to understand the effects and assumptions that propagate between each individual step of the pipeline, and account for any systematic issues that may arise.

In this paper, we present a new methodology to validate the reported cosmological contours of current pipelines. For this effort, we utilize Pippin~\citep{HintonBrout2020}, which automates a number of key components of the SuperNova ANAlysis framework~\citep[SNANA;][]{KesslerBernstein2009} used in DES, LSST's Dark Energy Survey Collaboration, and PANTHEON+. Pippin and SNANA provide substantial functionality, including simulations, light-curve fitting, photometric classification training and evaluation, SNe Ia standardization and bias corrections, and cosmological fitting.

Previous efforts to construct a confidence region include those published in~\cite{BroutScolnic2019}, who used 200 simulated samples to demonstrate that the distribution of best fitting cosmologies produced by their pipeline is consistent with the average of many cosmological contours, which they took as an estimate of the confidence region (CR). This estimate of the CR, while informative, is not statistically rigorous. 

To rigorously estimate the confidence region, we make use of the Neyman construction~\citep{Neyman1937}, a Frequentist methodology that leverages simulations to produce a confidence region. The Neyman construction does not assume the CR is Gaussian or elliptical, and is robust to small sample sizes. We compare the Frequentist confidence region produced from the Neyman construction with the Bayesian contour produced by our cosmological pipeline in order to test for consistency. Producing a CR and comparing it to cosmological contours is a powerful, rigorous, and independent method of evaluating the output of a cosmological pipeline.

In Section~\ref{sec:pippin} we describe the formalism employed by the Pippin cosmological pipeline. Section~\ref{sec:methods} describes our Neyman construction methodology. Finally, the results of applying our methodology to the Pippin cosmological pipeline are presented in Section~\ref{sec:results}.