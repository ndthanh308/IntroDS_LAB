\section{Background and insights}
\label{sec:background}

\new{In the following, we cover the background on wave-based NLOS imaging that forms the basis of our work, as well as related insights to understand our contributions. 
\NEWW{In \sref{sec:nlos-imaging-limitations}} we describe the two key existing challenges that we address in this paper.}
\subsection{Wave-based NLOS image formation}
\new{In a classic NLOS setup, the capture device illuminates and measures indirect light at a diffuse relay surface, lacking direct line of sight to the target scenes being imaged. The phasor-field formulation \cite{Liu2019phasor} brings the NLOS problem into a \textit{virtual} LOS domain, creating computational imaging devices at the relay surface which can directly illuminate and \NEWW{capture} the hidden scene.}
%
To understand our work, it is key to distinguish between the real and computational imaging domains in the phasor-field formulation.
%
First, in the real domain, the acquisition process involves a laser \NEW{device} that emits illumination pulses towards locations $\xl$ on the relay surface (\fref{fig:RSD_image_formation}a). The resulting indirect illumination produced by the hidden scene is then captured by an ultra-fast \NEW{sensing device} at points $\xs\in\lS$ on the relay surface (\fref{fig:RSD_image_formation}b), yielding a time-resolved impulse response function $H(\xl, \xs, t)$, where $t$ represents time.
%
\new{In the second stage, the phasor-field framework operates on $H$ to \emph{computationally} illuminate the hidden scene (\fref{fig:RSD_image_formation}c) and then \newww{compute} images \newww{of the hidden scene under such illumination} (\fref{fig:RSD_image_formation}d). Note that, while the first stage illuminates and senses the scene in the real domain, the second stage (which we describe in the rest of this section) is entirely computational.}



% Figure environment removed

\newww{The time of flight between the laser device and $\xl$ (\fref{fig:RSD_image_formation}a, red) and between $\xs \in \lS$ and the sensing device (\fref{fig:RSD_image_formation}b, blue) introduces temporal delays on the illumination captured in $H(\xl,\xs,t)$. In practice, we shift the temporal dimension of $H$ 
so that $t$ represents the time of flight of light paths that start at $\xl$, scatter in the hidden scene, and end at $\xs$. Placing the origin of the time reference system at the relay surface instead of at the laser and sensing devices is common practice in NLOS imaging and does not affect the algorithms \cite{Liu2019phasor,Marco2021NLOSvLTM}.}


\new{\paragraph{Computationally illuminating the hidden scene}
Points $\xl$ in the relay surface reflect delta illumination pulses emitted by the laser. \neww{In \NEW{the} computational domain, these points $\xl$ are now the \emph{emitters}.}
%
Given the impulse response function $H(\xl, \xs, t)$, captured from a delta illumination pulse $\delta(\xl, t)$, we can compute the response of the scene to any other arbitrary time-resolved illumination function $\Pt(\xl, t)$ emitted from $\xl$ (\fref{fig:RSD_image_formation}c). The resulting time-resolved response $\Pt(\xs, t)$ at points $\xs$ on the relay surface is computed as
%
\begin{align}
    \Pt(\xs, t) = \int\limits_{\mathcal{L}} \Pt(\xl, t) \ast H(\xl, \xs, t) \diff \xl,
    \label{eq:virtual_illumination}
\end{align}
%
where $\ast$ represents a convolution in time. Throughout \neww{our} paper, we only use a single point $\xl$ located at the center of the relay surface.
%
Following the work by \citet{Liu2019phasor}, for the illumination function $\Pt(\xl, t)$  we use a pulse wave with a Gaussian envelope (\fref{fig:RSD_image_formation}c) with wavelength $\lambda_c$ and standard deviation $\sigma$:
%
\begin{align}
    \Pt(\xl, t) = e^{i 2 \pi \frac{t}{\wl_c} - \frac{1}{2} \left( \frac{t}{\sigma} \right)^2 }.
    \label{eq:gaussian-pulse}
\end{align}
%
We discuss the choice of $\wl_c$ and $\sigma$ and their implications in the imaging process in \sref{sec:light_interference}.
}

\paragraph{The \NEW{virtual} camera analogy}
%
The phasor-field framework treats the relay surface as the computational lens in a virtual camera that directly observes the hidden scene, with an aperture $\lS$ defined by points $\xs \in \lS$. 
%
The lens operators used in the phasor-field framework are well-known wave-based lens imaging operators \cite{Liu2019phasor,Liu2020phasor} defined as a function of each frequency component $\fq$ of the signal.
%
Because of this, we transform $\Pt(\xs, t)$ to the frequency domain by applying a Fourier transform over the time domain, obtaining the complex-valued field $\Pf(\xs, \fq) = \mathcal{F}\left\{ \Pt(\xs, t) \right\}$.
%
Each complex value $\Pf(\xs, \fq)$ represents a wave (i.e., the values form a phasor field) resulting from illuminating the scene with the phasor $\Pf(\xl, \fq) = \mathcal{F}\left\{ \Pt(\xl, t) \right\}$, with illumination frequency $\fq$. In the camera analogy, $\Pf(\xs, \fq)$ can be understood as out-of-focus illumination from the hidden scene, and the goal of the computational lens is to focus the phasors $\Pf(\xs, \fq)$ to form an in-focus image of the hidden scene (\fref{fig:RSD_image_formation}d).
%
This focusing operation is specific to the chosen imaging operator $\Phi$, which defines the properties of the \NEW{virtual} camera:
%
\begin{align}
    \Fspace{f}(\xv, \fq) = \Phi(\xv, \Pf(\xs, \fq)),
    \label{eq:imaging_operator}
\end{align}
%
where $\Fspace{f}(\xv, \fq)$ is the resulting image of the hidden scene computed for an imaging frequency $\fq$ at points $\xv$ on the \newww{focal} plane \NEW{of the virtual camera.}
%
\newww{Applying the focusing operation from the phasor field $\Pf(\xs, \fq)$ at a point on a plane behind the relay surface is equivalent to focusing at symmetric locations in front of the relay surface.}
For clarity in the explanations, we use the latter approach in the rest of the paper, 
\NEW{where $\xv$ denotes} a point \NEWW{in} the bounding volume $\lV$ of the hidden scene (\fref{fig:RSD_image_formation}d).
%
\new{
\newww{Due to the large size of the aperture $\lS$ at the relay surface, imaging the hidden scene with a single focal plane (as in conventional photography) results in a very shallow depth of field, which yields out-of-focus illumination from objects outside \NEW{the focal plane} \cite{Marco2021NLOSvLTM}. To mitigate this problem, we use the computational lens 
%
\NEW{to sweep the focal plane across the hidden scene, capturing} a sequence of planar images taken at several focal distances \NEW{(i.e., creating a focal stack). We} arrange these planar images to form a volumetric image of the \NEW{scene contained in} $\lV$.}}


\new{\paragraph{Lens imaging operators.} 
The analogy of a computational lens focusing at any point $\xv$ \NEW{in $\lV$} is defined, in practice, by the propagation of the phasors \neww{$\Pf(\xs, \fq)$} from all points $\xs$ in the aperture $\lS$ to $\xv$.
%
In general, when light travels from any point $\xa$ to another point $\xb$, the phasor $\Pf(\xa, \fq)$ at $\xa$ undergoes a phase shift and attenuation modeled by the Rayleigh-Sommerfeld Diffraction (RSD) operator. The phasor at $\xb$, $\Pf(\xb, \fq)$, is then computed as
%
\begin{align}
    \Pf(\xb, \fq) = \Pf(\xa, \fq) \frac{e^{i k \norm{\xb - \xa}}}{\norm{\xb - \xa}},
    \label{eq:phase_shift}
\end{align}
where $\norm{\xb - \xa}$ is the optical distance between $\xa$ and $\xb$. The wavenumber $k = 2\pi\fq/c$ (where $c$ is the speed of light) is the conversion factor from optical distance to phase. The phase shift (numerator) and attenuation (denominator) form the RSD operator.}
%
\new{ 
The phasor-field formulation uses RSD operators to define its imaging operators $\Phi$, and \NEWW{capture} images $\Fspace{f}(\xv, \fq)$ of the hidden scene with different characteristics. These images can be defined under the following general expression:
%
\begin{align}
    \Fspace{f}(\xv, \fq) = \int \limits_{\lS} \frac{e^{i k t_s c}}{t_s c} \int \limits_{\lL} \frac{e^{i k t_l c}}{t_l c}  \Pf(\xl, \fq) \Fspace{H}(\xl, \xs, \fq) \diff \xl \diff \xs,
    \label{eq:RSD_freq}
\end{align}
%
\neww{where $t_s$ and $t_l$ are parameters of the RSD operators, representing time of flight, and are used to implement the two different camera models \NEW{in our work, as} \NEWW{explained in \sref{sec:time-resolved-models}}. 
%
Note that the imaging frequency $\fq$ is included in the wavenumber $k=2\pi\fq / c$ of these operators.}
%
$\Pf(\xl, \fq)$ and $\Fspace{H}(\xl,\xs,\fq)$ are the frequency-domain counterparts of $\Pt(\xl, t)$ and $H(\xl,\xs,t)$, respectively, obtained via Fourier transform.
%
The innermost integral over $\lL$ is the frequency-domain counterpart of the illumination function (\eref{eq:virtual_illumination}), but including an RSD operator.
%
This allows for more general imaging operators by interpreting $\lL$ as an illumination aperture where another lens \neww{on the relay surface can focus the emitted illumination at specific locations $\xv$ in the scene. In our case, with only a single point $\xl$ in $\lL$, this focus operation simply applies a phase shift and attenuation to the illumination phasor $\Pf(\xl, \fq)$\newww{; note that we can apply a different focus \NEW{ operation} at each location $\xv$ in the scene.} In the time domain, this phase shift effectively shifts the time instant at which each point $\xv$ in the scene is computationally illuminated using $\Pt(\xl, t)$.
%
In the following, we summarize the two camera models used in our work, which result from choosing specific values for the parameters $t_s$ and $t_l$ in \eref{eq:RSD_freq}.}}

\subsection{\new{Time-resolved camera models}} 
\label{sec:time-resolved-models}
%
\new{Throughout our paper we implement two \neww{camera} models introduced by previous work \cite{Liu2019phasor}: the \textit{transient} camera model and the \textit{confocal} camera model.}
These two models allow \NEW{us} to address different challenges in NLOS imaging in this work, such as the missing-cone problem and imaging objects hidden around two corners, by analyzing \newww{captured images of the scene} \neww{at different locations and time instants.}
%
\newww{These models compute time-resolved images of the hidden scene via inverse Fourier transform over the frequency domain.
%
We use $\ftc(\xv,t)=\mathcal{F}^{-1}\{\fftc(\xv, \fq)\}$ and $\fcc(\xv,t)=\mathcal{F}^{-1}\{\ffcc(\xv, \fq)\}$ to refer to time-resolved images computed using the transient and confocal camera models \NEW{$\ftc(\xv,t)$ and $\fcc(\xv,t)$}, respectively. \NEWW{These models result from choosing specific values of $t_s$ and $t_l$ in \eref{eq:RSD_freq}}.}


\newww{
As previously discussed, we mitigate \NEW{depth of field} issues coming from the large camera aperture by computing multiple planar images at different focal distances \NEW{that cover the volume $\lV$}, which form a focal stack\NEWW{; we arrange these planar images} to form a volumetric image $\Fspace{f}(\xv, \Omega)$ \NEW{for all points $\xv$ of the hidden scene}.
%
Any hidden scene element \NEW{located at $\xv$} will therefore be in focus in one planar image \NEW{that forms $\Fspace{f}(\xv, \Omega)$.}
%
Due to the time-frequency correspondence, the resulting time-resolved volumetric image $f(\xv,t) = \mathcal{F}^{-1}\{\Fspace{f}(\xv, \fq)\}$ is composed of a set of time-resolved planar images in the focal stack.
%
\NEWW{In conventional photography, a focal stack contains multiple \NEWW{planar} images of the same scene captured at different focal distances.}
%
The case of $f(\xv,t)$ is analogous, \NEWW{but} also \NEWW{including} the temporal dimension: \NEW{each frame at $t$ of $f(\xv, t)$}
%
combines multiple planar images \NEWW{that capture the hidden scene at a different focal distances and at different time instants}.
%
\NEWW{Due to this effect}, light transport events that occurred at the same time \NEWW{in the scene} are captured in a different order \NEWW{on each planar image that forms the time-resolved volumetric image $f(\xv, t)$}.
%
Consequently, each frame of $f(\xv,t)$ may simultaneously show multiple light transport events of the hidden scene, despite these occurred at different time instants.
%
In our work we compute time-resolved volumetric images $\ftc(\xv,t)$ and $\fcc(\xv,t)$ to address different challenges of NLOS imaging by analyzing light transport events in the hidden scene.
\NEWW{These events have occurred at different instants, but each frame $t$ captures them simultaneously.}}


\paragraph{Transient camera. }
\new{The transient camera model implements a \NEW{computational lens located at} $\lS$, focused at hidden scene points $\xv$ with $t_s = \norm{\xs-\xv}/c$, \newww{and it does not implement any lens for the illumination aperture $\lL$. For this, the RSD operator in $\lL$ of \eref{eq:RSD_freq} is ignored, resulting in}}
%
\begin{align}
    \fftc(\xv, \fq) = \int \limits_{\lS} \frac{e^{i k \norm{\xs - \xv}}}{\norm{\xs - \xv}} \int \limits_{\lL} \Pf(\xl, \fq) \Fspace{H}(\xl, \xs, \fq) \diff \xl  \diff \xs.
    \label{eq:RSD_freq_tc}
\end{align}
%
\neww{The computed images $\ftc(\xv,t) = \invFourier{\fftc(\xv,\fq)}$ of this \neww{camera} model resemble the captures of existing time-resolved cameras \cite{velten2013femto,Heide2013} if the hidden scene were illuminated using $\Pt(\xl, t)$.}
%
\newww{However, as previously discussed, the computed images $\ftc(\xv,t)$ capture the hidden scene at a different time
%
for every point $\xv$. 
%
\NEWW{As a result from the focusing operation in \eref{eq:RSD_freq_tc}}, the hidden scene elements that are in focus at $\xv$ will be captured in $\ftc(\xv, t)$ in the frame with time $t$ corresponding to the time of flight between $\xl$ and the location of the \NEW{\emph{actual}} scene element \NEWW{captured at $\xv$}. Importantly, as we deal with mirror reflections in this work, note that the location of the \NEWW{actual} scene element may not correspond with $\xv$ \NEWW{if $\ftc(\xv,t)$ captures} a mirror reflection of such element at $\xv$. \NEWW{For actual elements of the hidden scene located at $\xv$, the computed images are a good approximation of time-resolved light transport at such elements under computational illumination.}}
%
\neww{In our work, we use this camera model to analyze mirror reflections of known elements (e.g., the known illuminated point $\xl$) in the hidden scene, and show how to leverage them to 
\NEW{address the missing-cone problem.}}

\paragraph{Confocal camera. }\new{We also implement a confocal camera model, which creates two \neww{computational lenses located at} $\lL$ and $\lS$, focused at the same hidden scene point $\xv$\NEW{, respectively} using $t_l = \norm{\xv-\xl}/c$ and $t_s = \norm{\xs-\xv}/c$, resulting in}
%
\begin{align}
    \hspace{-0.08em}\ffcc(\xv, \fq) = \!\! \int \limits_{\lS}\!\! \tfrac{e^{i k \norm{\xs - \xv}}}{\norm{\xs - \xv}}\!\! \int \limits_{\lL} \!\! \tfrac{e^{i k \norm{\xv - \xl}}}{\norm{\xv - \xl}} \Pf(\xl, \fq) \Fspace{H}(\xl, \xs, \fq) \diff \xl  \diff \xs.
    \label{eq:RSD_freq_cc}
\end{align}
%
\new{\neww{Throughout our work, we use a single \NEW{illuminated} point $\xl$, which yields a special case of $\fcc(\xv,t) = \invFourier{\ffcc(\xv, \fq)}$ \NEW{for the frame at} $t=0$.
%
The phase shift defined by $t_l = \norm{\xv-\xl}/c$ in the illumination aperture $\lL$ is equivalent to a temporal shift of the illumination function $\Pt(\xl,t)$
%
\newww{\NEW{corresponding to the} time of flight from $\xl$ to \NEW{each} location $\xv$. 
%
Similar to the transient camera model, \NEWW{the computed images $\fcc(\xv,t)$ capture the hidden scene at a different time for every imaged point $\xv$}}.}
%
\NEW{For this camera model, the frame at \mbox{$t=0$} of} $\fcc(\xv, t)$ represents direct illumination from the emitter \newww{at scene elements that are in focus at} $\xv$.
%
This is equivalent to the imaging models used by the vast majority of existing time-gated NLOS imaging methods \cite{Velten2012nc, OToole2018confocal, Lindell2019wave, Xin2019theory, ahn2019convolutional,Liu2019phasor} to reconstruct single-corner hidden scenes, which consider light with the shortest path $\xl \rarr \xv \rarr \xs$ a good estimator of the hidden geometry at $\xv$. 
%
In our work, we use this camera model as an intermediate step to address the missing-cone problem, and we show how to leverage it to image objects hidden behind \textit{two} corners.}

\section{Current NLOS imaging limitations}
\label{sec:nlos-imaging-limitations}
%
In this paper we address two of the fundamental limitations of NLOS imaging: the missing-cone problem and single-corner imaging, which we summarize in the following.


\subsection{\neww{The missing-cone problem}}
\label{sec:challenges:missing-cone}
%
\new{The missing cone is a fundamental problem of NLOS imaging where, for a relay surface of a given size, certain hidden surfaces cannot be accessed by NLOS measurements (i.e., the impulse response $H$), and thus cannot be reconstructed regardless of the imaging method employed. This problem is inherent to other well-established imaging methods as well e.g., computed tomography \cite{benning2015tgv,delaney1998globally}, which assume \NEW{three}-bounce transport.}
\new{We use the term \emph{null-reconstruction space} to denote the set of such surfaces that cannot be accessed by NLOS measurements. Note that surfaces inside the null-reconstruction space may still reflect some light towards the relay surface. However, their response $H$ only changes its unmodulated components (frequency $\fq = 0$), lacking the required modulated changes (components with frequency $\fq > 0$) to be reconstructed using third-bounce time-of-flight information. }
%
\new{The in-depth analysis by \citet{Liu2019analysis} shows that the missing-cone problem is universal across any NLOS measurement, and therefore affects both the transient camera $\ftc$ and the confocal camera $\fcc$ models.}
%
\new{In Section \ref{sec:contribution-2:goal-2}, we intuitively address the missing-cone problem from our virtual mirror analogy, and use fourth-bounce illumination to image objects that are inside the null-reconstruction space of third-bounce methods.}


\subsection{\neww{Single-corner imaging}}
\label{sec:challanges:single-corner}
%
\new{The existing time-resolved NLOS imaging methods are limited to reconstructing objects hidden behind a single corner (\fref{fig:RSD_image_formation}), based on third-bounce illumination assumptions. However, general scenes typically contain objects hidden behind several occluders, creating higher-order illumination at the relay surface that is equally captured on the impulse response function $H$ by time-resolved sensors\neww{, degrading the imaging results of third-bounce methods}. In our work, we show how to leverage such higher-order illumination \neww{with applications such as imaging objects} hidden behind two corners in \sref{sec:contribution-2:goal-1}.}
