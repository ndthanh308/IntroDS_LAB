\begin{thebibliography}{10}

\bibitem{alkousa2019}
Mohammad Alkousa, Darina Dvinskih, Fedor Stonyakin, Alexander Gasnikov, and
  Dmitry Kovalev.
\newblock Accelerated methods for composite non-bilinear saddle point problem.
\newblock {\em arXiv preprint arXiv: 1906.03620}, 2019.

\bibitem{auslender2006}
Alfred Auslender and Marc Teboulle.
\newblock Interior gradient and proximal methods for convex and conic
  optimization. siam journal on optimization.
\newblock {\em SIAM Journal on Optimization}, 16(3):697--725, 2006.

\bibitem{azizian_2020}
Waïss Azizian, Damien Scieur, Ioannis Mitliagkas, Simon Lacoste-Julien, and
  Gauthier Gidel.
\newblock Accelerating smooth games by manipulating spectral shapes.
\newblock 01 2020.

\bibitem{beznosikov2020decentralized}
Alexander Beznosikov, Valentin Samokhin, and Alexander Gasnikov.
\newblock Distributed sadde-point problems: lower bounds, optimal and robust
  algorithms.
\newblock {\em arXiv preprint arXiv:2010.13112}, 2020.

\bibitem{Brown2020LanguageMA}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, and
  Amanda~Askell et~al.
\newblock Language models are few-shot learners.
\newblock {\em ArXiv}, abs/2005.14165, 2020.

\bibitem{chambolle2011first}
Antonin Chambolle and Thomas Pock.
\newblock A first-order primal-dual algorithm for convex problems with
  applications to imaging.
\newblock {\em Journal of mathematical imaging and vision}, 40:120--145, 2011.

\bibitem{chen2017}
Yunmei Chen, Guanghui Lan, and Yuyuan Ouyang.
\newblock Accelerated schemes for a class of variational inequalities.
\newblock {\em Mathematical programming}, 165:113--149, 2017.

\bibitem{chezhegov2023decentralized}
Savelii Chezhegov, Alexander Rogozin, and Alexander Gasnikov.
\newblock On decentralized nonsmooth optimization.
\newblock {\em arXiv preprint arXiv:2303.08045}, 2023.

\bibitem{cohen2021}
Michael~B. Cohen, Aaren Sidfort, and Kevin Tian.
\newblock Relative lipschitzness in extragradient methods and a direct recipe
  for acceleration.
\newblock {\em arXiv preprint arXiv: 2011.06572}, 2021.

\bibitem{du2017stochastic}
Simon~S Du, Jianshu Chen, Lihong Li, Lin Xiao, and Dengyong Zhou.
\newblock Stochastic variance reduction methods for policy evaluation.
\newblock In {\em International Conference on Machine Learning}, pages
  1049--1058. PMLR, 2017.

\bibitem{du2022}
Simon~S. Du, Gauthier Gidel, Michael~I. Jordan, and Chris~Junchi Li.
\newblock Optimal extragradient-based bilinearly-coupled saddle-point
  optimization.
\newblock {\em arXiv preprint arXiv: 2206.08573}, 2022.

\bibitem{gidel2018variational}
Gauthier Gidel, Hugo Berard, Ga{\"e}tan Vignoud, Pascal Vincent, and Simon
  Lacoste-Julien.
\newblock A variational inequality perspective on generative adversarial
  networks.
\newblock {\em arXiv preprint arXiv:1802.10551}, 2018.

\bibitem{gorbunov2019optimal}
Eduard Gorbunov, Darina Dvinskikh, and Alexander Gasnikov.
\newblock Optimal decentralized distributed algorithms for stochastic convex
  optimization.
\newblock {\em arXiv preprint arXiv:1911.07363}, 2019.

\bibitem{guler1991}
Osman Guler.
\newblock On the convergence of the proximal point algorithm for convex
  minimization.
\newblock {\em SIAM Journal on Optimization}, 29(2):403--419, 1991.

\bibitem{pmlr-v119-ibrahim20a}
Adam Ibrahim, Wa\"{\i}ss Azizian, Gauthier Gidel, and Ioannis Mitliagkas.
\newblock Linear lower bounds and conditioning of differentiable games.
\newblock In Hal~Daumé III and Aarti Singh, editors, {\em Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of {\em
  Proceedings of Machine Learning Research}, pages 4583--4593. PMLR, 13--18 Jul
  2020.

\bibitem{Jin2020:mdp}
Yujia Jin and Aaron Sidford.
\newblock Efficiently solving {MDP}s with stochastic mirror descent.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning (ICML)}, volume 119, pages 4890--4900. PMLR, 2020.

\bibitem{pmlr-v178-jin22b}
Yujia Jin, Aaron Sidford, and Kevin Tian.
\newblock Sharper rates for separable minimax and finite sum optimization via
  primal-dual extragradient methods.
\newblock In Po-Ling Loh and Maxim Raginsky, editors, {\em Proceedings of
  Thirty Fifth Conference on Learning Theory}, volume 178 of {\em Proceedings
  of Machine Learning Research}, pages 4362--4415. PMLR, 02--05 Jul 2022.

\bibitem{Korpelevich1976TheEM}
G.~M. Korpelevich.
\newblock The extragradient method for finding saddle points and other
  problems.
\newblock 1976.

\bibitem{FOAM}
Dmitry Kovalev and Alexander Gasnikov.
\newblock The first optimal algorithm for smooth and
  strongly-convex-strongly-concave minimax optimizatio.
\newblock {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{kovalev2022accelerated}
Dmitry Kovalev, Alexander Gasnikov, and Peter Richt{\'a}rik.
\newblock Accelerated primal-dual gradient method for smooth and convex-concave
  saddle-point problems with bilinear coupling.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{NEURIPS2020_d530d454}
Dmitry Kovalev, Adil Salim, and Peter Richtarik.
\newblock Optimal and practical algorithms for smooth and strongly convex
  decentralized optimization.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 18342--18352. Curran Associates, Inc., 2020.

\bibitem{lan2016gradient}
Guanghui Lan.
\newblock Gradient sliding for composite optimization.
\newblock {\em Mathematical Programming}, 159(1):201--235, 2016.

\bibitem{lan2021}
Guanghui Lan and Yuyuan Ouyang.
\newblock Mirror-prox sliding methods for solving a class of monotone
  variational inequalities.
\newblock {\em arXiv preprint arXiv: 2111.00996}, 2021.

\bibitem{lewis2016}
Adrian~S Lewis and Stephen~J Wright.
\newblock A proximal method for composite minimization.
\newblock {\em Mathematical programming}, 158(1-2):501--546, 2016.

\bibitem{li2022stochastic}
Chris~Junchi LI, Angela Yuan, Gidel Gauthier, Gu~Quanquan, and Michael Jordan.
\newblock Stochastic variance reduction methods for policy evaluationnesterov
  meets optimism: Rate-optimal separable minimax optimization.
\newblock In {\em International Conference on Machine Learning}. PMLR, 2023.

\bibitem{li2020decentralized}
Huan Li, Cong Fang, Wotao Yin, and Zhouchen Lin.
\newblock Decentralized accelerated gradient methods with increasing penalty
  parameters.
\newblock {\em IEEE Transactions on Signal Processing}, 68:4855--4870, 2020.

\bibitem{lin2020}
Tianyi Lin, Chi Jin, and Michael~I. Jordan.
\newblock Near-optimal algorithms for minimax optimization.
\newblock {\em Thirty Third Conference on Learning Theory}, 125:2738--2779,
  2020.

\bibitem{mokhtari2020}
Aryan Mokhtari, Asuman Ozdaglar, and Sarath Pattathil.
\newblock A unified analysis of extra-gradient and optimistic gradient methods
  for saddle point problems: Proximal point approach.
\newblock {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1497--1507, 2019.

\bibitem{monteiro_svaiter2010}
Renato D.~C. Monteiro and B.~F. Svaiter.
\newblock Complexity of variants of tseng’s modified f-b splitting and
  korpelevich’s methods for generalized variational inequalities with
  applications to saddle point and convex optimization problems.
\newblock {\em SIAM Journal on Optimization}, 21(4):1688--1720, 2010.

\bibitem{Nesterov1983AMF}
Yurii Nesterov.
\newblock A method for unconstrained convex minimization problem with the rate
  of convergence $o(1/k^2)$.
\newblock 1983.

\bibitem{nesterov2018lectures}
Yurii Nesterov.
\newblock {\em Lectures on convex optimization}, volume 137.
\newblock Springer, 2018.

\bibitem{nesterov2006scrimali}
Yurii Nesterov and L.~Scrimali.
\newblock Solving strongly monotone variational and quasi-variational
  inequalities.
\newblock 2006.

\bibitem{rokafellar1976}
R~Tyrrell Rockafellar.
\newblock Monotone operators and the proximal point algorithm. siam journal on
  control and optimization.
\newblock {\em SIAM Journal on Optimization}, 14(5):877--898, 1976.

\bibitem{rogozin2023decentralized}
Alexander Rogozin, Demyan Yarmoshik, Ksenia Kopylova, and Alexander Gasnikov.
\newblock Decentralized strongly-convex optimization with affine constraints:
  Primal and dual approaches.
\newblock In {\em Advances in Optimization and Applications: 13th International
  Conference, OPTIMA 2022, Petrovac, Montenegro, September 26--30, 2022,
  Revised Selected Papers}, pages 93--105. Springer, 2023.

\bibitem{pmlr-v151-salim22a}
Adil Salim, Laurent Condat, Dmitry Kovalev, and Peter Richtarik.
\newblock An optimal algorithm for strongly convex minimization under affine
  constraints.
\newblock In Gustau Camps-Valls, Francisco J.~R. Ruiz, and Isabel Valera,
  editors, {\em Proceedings of The 25th International Conference on Artificial
  Intelligence and Statistics}, volume 151 of {\em Proceedings of Machine
  Learning Research}, pages 4482--4498. PMLR, 28--30 Mar 2022.

\bibitem{stefano_2020_fed}
Stefano Savazzi, Monica Nicoli, and Vittorio Rampa.
\newblock Federated learning with cooperating devices: A consensus approach for
  massive iot networks.
\newblock {\em IEEE Internet of Things Journal}, 7:4641--4654, 01 2020.

\bibitem{scaman2017optimal}
Kevin Scaman, Francis Bach, S{\'e}bastien Bubeck, Yin~Tat Lee, and Laurent
  Massouli{\'e}.
\newblock Optimal algorithms for smooth and strongly convex distributed
  optimization in networks.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning (ICML)}, 2017.

\bibitem{ShalevShwartz2014UnderstandingML}
Shai Shalev-Shwartz and Shai Ben-David.
\newblock Understanding machine learning - from theory to algorithms.
\newblock 2014.

\bibitem{smith2017federated}
Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet Talwalkar.
\newblock Federated multi-task learning.
\newblock {\em arXiv preprint arXiv:1705.10467}, 2017.

\bibitem{pmlr-v151-thekumparampil22a}
Kiran~K. Thekumparampil, Niao He, and Sewoong Oh.
\newblock Lifted primal-dual method for bilinearly coupled smooth minimax
  optimization.
\newblock In Gustau Camps-Valls, Francisco J.~R. Ruiz, and Isabel Valera,
  editors, {\em Proceedings of The 25th International Conference on Artificial
  Intelligence and Statistics}, volume 151 of {\em Proceedings of Machine
  Learning Research}, pages 4281--4308. PMLR, 28--30 Mar 2022.

\bibitem{tseng2000}
P.~Tseng.
\newblock A modified forward-backward splitting method for maximal monotone
  mappings.
\newblock {\em Journal on Control and Optimization}, 38 (2):431–446, 2000.

\bibitem{tseng2008}
Paul Tseng.
\newblock On accelerated proximal gradient methods for convex-concave
  optimization.
\newblock {\em submitted to SIAM Journal on Optimization}, 2008.

\bibitem{wang2018distributed}
Weiran Wang, Jialei Wang, Mladen Kolar, and Nathan Srebro.
\newblock Distributed stochastic multi-task learning with graph regularization.
\newblock {\em arXiv preprint arXiv:1802.03830}, 2018.

\bibitem{wang2020}
Yuanhao Wang and Jian Li.
\newblock Improved algorithms for convex-concave minimax optimization.
\newblock {\em Neural Information Processing Systems 33 (NeurIPS 2020)},
  33:4800–4810, 2020.

\bibitem{JMLR:v20:17-608}
Lin Xiao, Adams~Wei Yu, Qihang Lin, and Weizhu Chen.
\newblock Dscovr: Randomized primal-dual block coordinate algorithms for
  asynchronous distributed optimization.
\newblock {\em Journal of Machine Learning Research}, 20(43):1--58, 2019.

\bibitem{xie2021}
Guangzeng Xie, Yuze Han, and Zhihua Zhang.
\newblock Dippa: An improved method for bilinear saddle point problems.
\newblock {\em arXiv preprint arXiv: 2103.08270}, 2021.

\bibitem{zhang2019lower}
Junyu Zhang, Mingyi Hong, and Shuzhong Zhang.
\newblock On lower iteration complexity bounds for the saddle point problems.
\newblock {\em arXiv preprint arXiv:1912.07481}, 2019.

\end{thebibliography}
