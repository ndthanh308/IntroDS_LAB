{
  "title": "Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation",
  "authors": [
    "Yoshiki Masuyama",
    "Xuankai Chang",
    "Wangyou Zhang",
    "Samuele Cornell",
    "Zhong-Qiu Wang",
    "Nobutaka Ono",
    "Yanmin Qian",
    "Shinji Watanabe"
  ],
  "submission_date": "2023-07-23T05:39:39+00:00",
  "revised_dates": [],
  "abstract": "Neural speech separation has made remarkable progress and its integration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we explore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recognition performance from the case with filterbank features. To further improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separation and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%).",
  "categories": [
    "cs.SD",
    "cs.CL",
    "eess.AS"
  ],
  "primary_category": "cs.SD",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.12231",
  "pdf_url": null,
  "comment": "Accepted to IEEE WASPAA 2023",
  "num_versions": null,
  "size_before_bytes": 402868,
  "size_after_bytes": 252180
}