\documentclass[a4paper,onecolumn,unpublished]{quantumarticle}
\pdfoutput=1
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{physics}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{parskip}

\usepackage{amsthm}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}

\theoremstyle{definition}
\newtheorem{defi}{Definition}

\newcommand{\red}[1]{\textcolor{red}{#1}}
\renewcommand{\aa}[2]{\ensuremath{a_{#1}^{#2}}}

% Overline code %%%%%%%%%%%%%%%%%%%%%%%%%%%
\makeatletter
\newsavebox\myboxA
\newsavebox\myboxB
\newlength\mylenA

\newcommand*\xoverline[2][0.75]{%
    \sbox{\myboxA}{$\m@th#2$}%
    \setbox\myboxB\null% Phantom box
    \ht\myboxB=\ht\myboxA%
    \dp\myboxB=\dp\myboxA%
    \wd\myboxB=#1\wd\myboxA% Scale phantom
    \sbox\myboxB{$\m@th\overline{\copy\myboxB}$}%  Overlined phantom
    \setlength\mylenA{\the\wd\myboxA}%   calc width diff
    \addtolength\mylenA{-\the\wd\myboxB}%
    \ifdim\wd\myboxB<\wd\myboxA%
       \rlap{\hskip 0.5\mylenA\usebox\myboxB}{\usebox\myboxA}%
    \else
        \hskip -0.5\mylenA\rlap{\usebox\myboxA}{\hskip 0.5\mylenA\usebox\myboxB}%
    \fi}
\makeatother
%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\title{Gaussian decomposition of magic states for matchgate computations}
\author{Joshua Cudby}
\author{Sergii Strelchuk}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
    Magic states were originally introduced as a resource that enables universal quantum computation using classically simulable Clifford gates. This concept has been extended to matchgate circuits (MGCs) which are made of two-qubit nearest-neighbour quantum gates defined by a set of algebraic constraints. In our work, we study the Gaussian rank of a quantum state --  defined as the minimum number of terms in any decomposition of that state into Gaussian states -- and associated quantities: the Gaussian Fidelity and the Gaussian Extent.
    We investigate the algebraic structure of Gaussian states and find and describe the independent sets of constraints upper-bounding the dimension of the manifold of Gaussian states. Furthermore, we describe the form of linearly dependent triples of Gaussian states and find the dimension of the manifold of solutions. By constructing the corresponding $\epsilon$-net for the Gaussian states, we are able to obtain upper bounds on the Gaussian fidelity. We identify a family of extreme points of the feasible set for the Dual Gaussian extent problem and show that Gaussian extent is multiplicative on systems of 4 qubits; and further that it is multiplicative on primal points whose optimal dual witness is in the above family. These extreme points turn out to be closely related to Extended Hamming Codes.  We show that optimal dual witnesses are unique almost-surely, when the primal point lies in the interior of the normal cone of an extreme point. Furthermore, we show that the Gaussian rank of two copies of our canonical magic state is 4 for symmetry-restricted decompositions. Numerical investigation suggests that no low-rank decompositions exist of either 2 or 3 copies of the magic state. Finally, we consider approximate Gaussian rank and present approximate decompositions for selected magic states. 

\end{abstract}
\maketitle
\tableofcontents
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

The relation between quantum and classical computational power is one of the most intriguing questions in quantum information science. Finding the minimum extra resources that could lift classically efficiently simulable quantum systems to performing universal quantum computation has been a particularly fruitful avenue. It motivates the study of classical simulation algorithms for quantum systems which allows to identify the regimes where quantum computing does not offer an advantage. Consequently, endowing the efficiently classically simulable systems with minimal resources that boost the computational power provides constructive insights into the nature of quantum advantage. The development of classical simulation algorithms has many benefits beyond outlining the limits of classical computations. By simulating quantum systems classically, one can study and probe properties of quantum systems in the absence of a full-scale quantum computer. Furthemore, classical simulation is pivotal for the validation of quantum devices. It provides a way to check the accuracy of noisy quantum computers and other experimental quantum systems. Matching classical simulation results to experimental device outputs helps validate that the devices work correctly. As quantum devices improve, classical simulation provides a valuable (albeit time-dependent) benchmark when quantum computers outperform classical supercomputers to achieve quantum supremacy.

One remarkable class of quantum computations with deep connections to fermionic linear optical systems is realised by matchgate circuits (MGCs)~\cite{Terhal2002, knill2001fermionic, jozsa2008matchgates,valiant2001quantum}. It is known that  circuits composed solely of matchgates can be efficiently simulated classically~\cite{valiant2001quantum, Terhal2002, jozsa2008matchgates}. 
%Gottesman \cite{Gottesman1998} and Knill showed that \textit{stabilizer circuits} are classically efficiently simulable. 
More precisely, MGCs consist of nearest-neighbour 2-qubit gates of the form:
\begin{equation}
    \begin{gathered}
    G(A, \, B) = \begin{pmatrix}
        p & 0 & 0 & q \\
        0 & w & x & 0 \\
        0 & y & z & 0 \\
        r & 0 & 0 & s
    \end{pmatrix}
    \qquad
    A = \begin{pmatrix}
        p & q \\
        r & s
    \end{pmatrix}
    \quad
    B = \begin{pmatrix}
        w & x \\
        y & z
    \end{pmatrix}  
    \end{gathered}
\end{equation}
where $A,\, B \in U(2)$ are such that $\det(A) = \det(B)$, and nearest-neighbour refers to some fixed linear ordering of qubits. Such circuits preserve the parity of input states. We will refer to the output of such circuits acting on a fixed computational-basis input state as even (\textit{resp.} odd) Gaussian states for an input with even (odd) Hamming weight.

Such circuits describe the computational ability of unassisted fermionic linear optics \cite{Terhal2002} and, as such, MGCs and Gaussian states are closely related to the theory of Majorana fermions. Indeed, one can show \cite{Bravyi2004} that a state $\ket{\psi}$ is Gaussian iff:
\begin{equation}
    \Lambda \ket{\psi}\ket{\psi} = \sum_{k = 1}^{2n} c_k \otimes c_k \ket{\psi}\ket{\psi} = 0  \label{eq:gaussian_state_eqn}
\end{equation}
where the $c_i$ are the Majorana fermions, which under the Jordan-Wigner transformation can be represented as:
\begin{equation}
    c_{2k - 1} = \left(\prod_{i = 1}^{k - 1} Z_i\right) X_k \quad
    c_{2k} = \left(\prod_{i = 1}^{k - 1} Z_i\right) Y_k
\end{equation}
It is well known that a description of a gate set is not enough to specify the power of quantum processes, and that the presence of other resources can drastically impact computational power. 
One such resource is the allowed input state, with so-called \textit{magic states} being those inputs which elevate a process to full universal quantum computation \cite{Bravyi2005}. 

It has been shown that every pure fermionic state which is non-Gaussian (that is, cannot be generated by a MGC from a computational basis state), is a magic state for MGCs~\cite{Hebenstreit2019}. The notion of magic states for MGCs turns out to be rather more nuanced when contrasted with another well-known class of quantum computations -- Clifford computations -- and their associated magic states~\cite{Bravyi2019, Labib_2022, lovitz2022new, mehraban2023lower, Peleg_2022}. This is due to the fact that locality of interaction plays a significant role for MGCs~\cite{jozsa2008matchgates, brod2011extending}.

Classically simulating Clifford circuits or MGCs with a large number of the magic state inputs is inefficient in general. However, simulation time can be dramatically reduced if one could find a decomposition of these magic states into a smaller number of terms each of which obtainable by the corresponding gatesets. The quantum process on any term in this sum is classically simulable and by linearity the output of the circuit can be reconstructed at the end. 

A key quantity which determines the run time of these algorithms for the case of Clifford circuits is the stabilizer rank \cite{Bravyi2019}, which is the minimal number of terms needed in a decomposition into stabilizer states. Bravyi et al also define the associated stabilizer extent $\xi$ and fidelity $F_C$ as follows:
\begin{align}
    \xi(\ket{\psi}) &= \min \norm{c}_1^2 \quad \text{s.t. } 
    \ket{\psi} = \sum_{i = 1}^k c_i \ket{s_i} 
    \\
    F_C(\ket{\psi})& = \max_s \abs{\braket{s}{\psi}}^2
\end{align}
where the $\ket{s}$ are stabilizer states. It is known that the stabilizer extent is not multiplicative \cite{Heimendahl2021}, and as part of the proof it was shown that generic Haar-random states have exponentially low fidelity.

Our results are organised as follows. 
In Section \ref{sec:notation}, we define our notation and some preliminaries.
In Section \ref{sec:properties}, we give an explicit description of Gaussian states, and use this characterisation to prove results on the dimension of the Gaussian manifold and on sums of 2 Gaussian states. 
In Section \ref{sec:fidelity}, we construct an $\epsilon$-net for the Gaussian manifold, and use it to upper bound the Gaussian fidelity of generic Haar-random states. 
In Section \ref{sec:extent}, we discuss the multiplicativity of the Gaussian extent~\footnote{While preparing this manuscript we became aware of related results on Gaussian extent by  Oliver Reardon-Smith, Kamil Korzekwa, Michal Oszmaniec (TQC 2023, \cite{korzekwa2023simulation}) and independently by Robert Koenig, Beatriz Cardoso Diaz \cite{koenig2023simulation}}. 
In Section \ref{sec:rank} we discuss our numerical attempt to find low-rank Gaussian decompositions for 2 or 3 copies of a certain Gaussian magic state. 
We also give a proof that no such decomposition exists for 2 copies in a symmetry-reduced case.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Notation and Preliminaries} \label{sec:notation}
We will generally make no distinction between an integer $x$ and its binary representation. 
Addition of binary vectors will be notated as $\oplus$.
The Hamming weight of a binary string is $\abs{\cdot}$ and the Hamming distance is $d(\cdot,\,\cdot)$.
We refer to the first $j$ bits of a binary string as $x_{1:j}$. We use an overbar to denote changing bits of a binary string: $\xoverline{x}^i = x \oplus e_i$ and $\xoverline{x}^{i,\,j} = x \oplus e_i \oplus e_j$.

We denote the set of all binary strings of length $n$ by $\mathcal{B}_n$, and the subset of even-weight strings by $\mathcal{A}_n$. The odd-weight strings are then the complement of this set, denoted $\mathcal{A}_n^\mathsf{c}$.

We denote the set of even-weight Gaussian states on $n$ qubits by $G_n$, and will drop the subscript when it is obvious by context. 
The set of Gaussian operations, i.e. those operations corresponding to matchgate circuits, is $\mathcal{G}_n \subset \mathcal{U}_n$.
The Hilbert space of even-parity states is $\mathcal{H}_n$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Properties of Gaussian states} \label{sec:properties}

Equation \eqref{eq:gaussian_state_eqn} provides a complete characterisation of Gaussian states, however it is not easy to work with in practice.
Starting \textit{a priori} from this equation, it is not immediately clear what kind of objects Gaussian states are, nor how sparse they are in the space of all even-parity pure states.

We derive an explicit representation of Gaussian states by finding an independent set of the constraints imposed by Equation \eqref{eq:gaussian_state_eqn}. 
Immediately, we obtain an upper bound for the dimension of the Gaussian manifold.
This representation is vital for our derivation of the rest of our results.

\subsection{Dimension of the Gaussian manifold}
Let $\ket{\psi} = \sum_{x \in \mathcal{A}_n} a_x \ket{x}$ be an even-parity state.
From Equation \eqref{eq:gaussian_state_eqn}, we can see it is Gaussian if and only if:
\begin{align}
    \Lambda \ket{\psi}\ket{\psi} &= 
    \sum_{k = 1}^{2n} \bigl(c_k \otimes c_k \bigr) \sum_{x, y \in \mathcal{A}_n} a_x a_y \ket{x} \ket{y} \nonumber \\
    &= \sum_{x, y \in \mathcal{A}_n} \left( a_x a_y \sum_{j = 1}^n \biggl( (-1)^{\abs{x_{1:j-1}} + \abs{y_{1:j-1}}} \bigl(1 + (i^2)(-1)^{x_j + y_j} \bigr)  \ket{\xoverline{x}^j}\ket{\xoverline{y}^j} \biggr) \right) \nonumber\\
    &= \sum_{x, y \in \mathcal{A}_n^\mathsf{c}} \left( \sum_{j = 1}^n \biggl( (-1)^{\abs{x_{1:j-1}} + \abs{y_{1:j-1}}} 
    \bigl(1 - (-1)^{x_j + y_j} \bigr) a_{\overline{x}^j} a_{\overline{y}^j} \biggr) \ket{x}\ket{y} \right) \label{eq:expanded_lambda_eq} = 0 
\end{align}
The outer sum of Equation \eqref{eq:expanded_lambda_eq} gives an equation $f(x,y) = 0$ for each pair $(x,\,y) \in {\mathcal{A}_n^\mathsf{c}}^{\otimes 2}$. 
The inner sum dictates that the LHS of this equation comprises one term for each bit where $x \neq y$, and that the terms have alternating signs.
Note that if $d(x,y) = 2$ then the two terms cancel and the equation is trivial, and clearly $f(x,x)$ is trivially zero also.

We therefore have the system of equations
\begin{equation}
    \mathcal{F} = \left\{
    f(x,\, y) = \sum_{i = 1}^{d(x,\,y)} (-1)^{i+1} a_{\xoverline{x}^{k_i}}a_{\xoverline{y}^{k_i}}
    : (x,\,y) \in {\mathcal{A}_n^\mathsf{c}}^{\otimes 2} 
    ;\, d(x,\,y) \geq 4
    ;\, x_{k_i} \neq y_{k_i}
    \right\}
\end{equation}
For example, for $n = 4$, the only non-trivial equation in $\mathcal{F}$ (up to permutation of terms) is
$$
f(0001,\,1110) \equiv f(1, 14) = a_0 a_{15} - a_3 a_{12} + a_5 a_{10} - a_6 a_9 = 0
$$

Ignoring duplication, $\mathcal{F}$ has $2^{n-1}\left( 
2^{n-1} - \tfrac{n(n-1)}{2} -1
\right)$ equations not all of them independent.
To find an independent set of equations, we will use a complexified form of the Implicit Function Theorem:
\begin{thm}[Implicit Function Theorem, see e.g. \cite{Hormander1973}]
    Let $\left\{f_j(w, z) : j = 1, 2, \dots, m \right\}$ be analytic functions of $(w, z) = (w_1, \dots, w_m, z_1, \dots, z_n)$ in a neighbourhood of $(w_0, z_0) \in \mathbb{C}^m \cross \mathbb{C}^n$.
    Suppose $f_j(w_0, z_0) = 0 \ \forall j$ and also that 
    $J = \det \left(\pdv{f_j}{w_k}\right)_{j, k = 1}^{m} \neq 0 \ \text{at} \ (w_0, z_0)$.
    Then $f_j(w, z)$ have a unique analytic solution $w(z)$ in a neighbourhood of $z_0$ such that $w(z_0) = w_0$.
\end{thm}

This leads us to our first result:
\begin{prop}
    The amplitudes of an even-parity Gaussian state on $n$ qubits are constrained to lie on an at most $\frac{n(n-1)}{2}$-dimensional manifold. In particular, there is an independent set of constraint equations of the form $\{f(w) : \abs{w} \geq 4 \}$.
\end{prop}

\begin{proof}
    Assume without loss of generality that $a_0 \neq 0$; (proof generalises easily to any other $a_y \neq 0$).
    
    Consider the following partition of $\mathcal{A}_n$:
    \begin{equation*}
        Z = \left\{ x \in \mathcal{A}_n : \abs{x} \leq 2 \right\} \quad 
        W = \left\{ x \in \mathcal{A}_n : \abs{x} \geq 4 \right\}
    \end{equation*}
    Define a further partition of $W$ based on the first `set' bit:
    \begin{equation*}
        W_k = \left\{
            w \in W : \ w_{1:k-1} = 0 ; \ w_k = 1
        \right\} \quad k = 1,2,\ldots n-3
    \end{equation*}
    For any $w \in W_k$ for some $k$, define $f(w) \coloneqq f(\xoverline{0}^{k},\, \xoverline{w}^{k})$.
    
    We pick the following set of equations.  They fulfil the conditions of the Implicit Function Theorem:
    \begin{equation}
        \mathcal{F}_{\text{indep}} = \bigcup_{k = 1}^{n-3} \left\{
            f(w) : w \in W_k
        \right\}
        \equiv \{ f(w) : \abs{w} \geq 4\}
    \end{equation}
    
    Note that, for $w \in W_k$, we have:
    \begin{equation} \label{eq:jacobian_elements}
        \pdv{f(w)}{v} = \begin{cases}
            a_0 \delta_{wv} &\quad v \in W_k \\
            0 &\quad v \in W_l,\, l < k \\
            \cdot &\quad v \in W_l,\, l > k \\
        \end{cases}
    \end{equation}
    Equation \eqref{eq:jacobian_elements} gives us that the Jacobian is of block-upper-triangular form, with multiples of the identity on the diagonal. Letting $\abs{W_k} = m_k$, we have: 

    \begin{equation}
        J = 
        \left(\begin{array}{c|c|c|c}
          a_0 I_{m_1}  & \cdot & \cdot & \dots \\
        \hline
          0 & a_0 I_{m_2} & \cdot  & \dots \\
        \hline
          0 & 0 & a_0 I_{m_3} & \dots \\
        \hline
          \vdots & \vdots & \vdots & \ddots
        \end{array}\right)
    \end{equation}
    
    The determinant of J is therefore just the product of the determinants of the diagonal blocks and is non-zero as long as $a_0 \neq 0$.
    
    Then the Implicit Function Theorem implies that the dimension of the manifold is reduced by $\abs{W}$. Since $\abs{Z} = \binom{n}{2} + 1$, and imposing normalisation and global phase equivalence we see that the dimension of the manifold is at most $\tfrac{n(n-1)}{2}$ as claimed.
\end{proof}

The proof generalises to any favoured $y \in \mathcal{A}_n$ with $a_y \neq 0$ by replacing the bit string 0 by $y$ and the Hamming weight $\abs{\cdot}$ by $d(y,\,\cdot)$.

The corresponding independent set of constraint equations which define Gaussian states is then:
\begin{equation}
    \mathcal{F}_{\text{indep}}^y = \left\{ f^y(w) \coloneqq  a_y a_w - \sum_{i = 2}^{d(w,\,y)}(-1)^i a_{\overline{y}^{k_1,k_i} } a_{\overline{w}^{k_1,k_i} } = 0 : d(y,\,w) \geq 4 \right\}
\end{equation}

\subsection{Triples of Gaussian states}
The explicit representation above is useful for computing further properties of Gaussian states. For example, we can quantify the conditions for 2 Gaussians to sum to a third:
\begin{prop}
    Suppose $\ket{\psi_0} = \alpha \ket{\psi_1} + \beta \ket{\psi_2}$ where the $\ket{\psi_i}$ are normalised Gaussian states and without loss of generality take $\alpha,\, \beta \in \mathbb{R}$. Then the triple can be expressed as
    \begin{equation}
         \{\ket{\psi_0},\,  \ket{\psi_1}, \ket{\psi_2} \} = \left\{
        U\ket{0}, \, 
        \frac{1}{\alpha} U\left(
        \sum_{\substack{y \in \mathcal{A}_n :\\ \abs{y} \leq 2}} a_y \ket{y}
        \right), \, 
        \frac{1}{\beta}U \left(
            (1-a_0)\ket{0} - \sum_{\substack{y \in \mathcal{A}_n :\\ \abs{y} = 2}} a_y \ket{y}
        \right) 
        \right\}
    \end{equation}
    for some Gaussian operation $U \in \mathcal{G}$.
    Moreover, for any $\ket{\psi}$, the dimension of the manifold of Gaussians for which  $\alpha \ket{\psi} + \beta\ket{\psi'} \in G$ is $2n-3$.
\end{prop}

\begin{proof}
    Let $U \in \mathcal{G}$ be the Gaussian operator s.t. $\ket{\psi_0} = U\ket{0}$.
    Define also 
    $\ket*{\tilde{\psi}_1} = U^\dag\ket{\psi_1}, \, \ket*{\tilde{\psi}_2} = U^\dag\ket{\psi_2}$.
    Then we have $\ket{0} = \alpha \ket*{\tilde{\psi_1}} + \beta \ket*{\tilde{\psi_2}}$.

    Write $\alpha \ket*{\tilde{\psi}_1} = \sum_{x\in \mathcal{A}_n} a_x \ket{x}$ where without loss of generality we can set $0 < a_0 \leq 1$. Then $\beta\ket*{\tilde{\psi}_2} = (1- a_0)\ket{0} - \sum_{x \in \mathcal{A}_n : \abs{x} \geq 2} a_x \ket{x}$.

    We require these states to be Gaussian. 
    For each $x \in \mathcal{A}_n$ with $\abs{x} \geq 4$, we have a constraint of the form:
    \begin{equation} \label{eq:triple_constraint_general}
        f(x) = 
        c_0 c_x - \sum_{i = 2}^{\abs{x}} (-1)^i c_{\overline{0}^{k_1,k_i}} c_{\overline{x}^{k_1,k_i}} = 0
    \end{equation}

    Imposing this constraint for $\alpha \ket*{\tilde{\psi}_1}$ and $\beta\ket*{\tilde{\psi}_2}$ respectively gives:
    \begin{align}
        a_0 a_x - \sum_{i = 2}^{\abs{x}} 
        (-1)^i a_{\overline{0}^{k_1,k_i}} a_{\overline{x}^{k_1,k_i}} &= 0 \label{eq:triples_constraints_1}\\
        (1-a_0)(-a_x) - \sum_{i = 2}^{\abs{x}} (-1)^i 
        (-a_{\overline{0}^{k_1,k_i}} ) (-a_{\overline{x}^{k_1,k_i}} ) &= 0
        \label{eq:triples_constraints_2}
    \end{align}
    Comparing Equations \eqref{eq:triples_constraints_1} \& \eqref{eq:triples_constraints_2} we see that 
    $a_x = 0$ for all $\abs{x} \geq 4$.

    So we have 
    $\alpha \ket*{\tilde{\psi}_1} = a_0 \ket{0} + \sum_{x\in \mathcal{A}_n : \abs{x} = 2} a_x \ket{x}$, and 
    $\beta\ket*{\tilde{\psi}_2} = (1- a_0)\ket{0} - \sum_{x \in \mathcal{A}_n : \abs{x} = 2} a_x\ket{x}$.

    It is still necessary to pick the $a_x$ with $\abs{x} \leq 2$ in such a way that $a_y = 0$ for $\abs{y} \geq 4$. 
    
    Note that $a_y = 0$ for all $\abs{y} = 4$ forces $a_y = 0$ for all $\abs{y} \geq 4$ due to the recursive nature of Equation \eqref{eq:triple_constraint_general}. 
    For $\abs{y} = 6$, since $\abs{\overline{y}^{k_1,\,k_i}} = 4$, Equation \eqref{eq:triple_constraint_general} becomes $f(y) = a_0 a_y = 0$.
    Proceeding iteratively for $\abs{y} = 8,\,10,\,\ldots$, we see that indeed $a_y = 0$ for all $\abs{y} \geq 4$.
    
    The relevant equations, which act as constraints on the $a_x$ with $\abs{x} \leq 2$, are then:
    \begin{equation*}
        \tilde{\mathcal{F}} = \left\{ \tilde{f}(y) = 
        \sum_{i = 2}^{4} (-1)^i a_{\overline{0}^{k_1,k_i}} a_{\overline{y}^{k_1,k_i}} = 0 
        : \abs{y} = 4
        \right\}
    \end{equation*}
    For each $x, \, \exists \, \binom{n-2}{2}$ choices of $y \in \mathcal{A}_n$ with $\abs{y} = 4$ st $d(x, \, y) = 2$.
    In other words, each $a_x$ appears in exactly $\binom{n-2}{2}$ equations. 
    We may assume $a_x \neq 0$ for some $\abs{x} = 2$, else the vectors in the linearly dependent triple are all parallel. 
    In Appendix \ref{sec:app.triple} we show that satisfying the equations explicitly containing $a_x$ is sufficient to satisfy all the equations. 
    This leaves $ \binom{n}{2} - \binom{n-2}{2} = 2n - 3$ of the variables unconstrained.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{An upper bound for Gaussian fidelity} \label{sec:fidelity}
The \textit{Stabilizer Fidelity} was first defined in \cite{Bravyi2019}.
It appeared as a lower bound for the Stabilizer Extent, which we briefly discuss in Section \ref{sec:extent}, and measures the maximum overlap of a state $\ket{\psi}$ with any stabilizer state $\ket{\phi}$.
One can similarly define the \textit{Gaussian Fidelity} similarly.

\begin{defi}
    The \textit{Gaussian Fidelity} is given by $F_G(\ket{\Psi}) \coloneqq \sup_{\ket{\psi} \in G} \abs{\braket{\Psi}{\psi}}^2$.
\end{defi}
We will generally refer to the Gaussian fidelity simply as fidelity for brevity. It provides a convenient measure of closeness of the given state to being Gaussian.
Via the same convex duality arguments as for the stabilizer case, the fidelity naturally gives a lower bound for the Gaussian extent, which is discussed in Section \ref{sec:extent}.

Here, we give an upper bound for the fidelity of generic Haar-random states. 
We begin by constructing an $\epsilon$-net \cite{Hayden2004} for the Gaussian states. 
By a standard union-bound argument, we obtain a bound for the overlap of random states with this discrete set. 
This directly leads to a bound for the overlap with any Gaussian state i.e. a bound for the fidelity.

\subsection{Using a net to bound the fidelity}
\begin{defi}
    An $\epsilon$-net $\mathcal{N}$ is a set of Gaussian states such that for every $\ket{\psi} \in G$ there exists $\ket*{\tilde{\psi}} \in \mathcal{N}$ with $\norm{\ket{\psi}\bra{\psi} - \ket*{\tilde{\psi}}\bra*{\tilde{\psi}}}_1 < \epsilon$ \cite{Hayden2004}. 
\end{defi}

\begin{prop}\label{prop:eps_net}
    Let $\epsilon = 2^{-l}$. Then there is an $\epsilon$-net $\mathcal{N}$ with cardinality $\log_2 \abs{\mathcal{N}} \leq n^4 + 2n^2 + n+ ln^2$.
\end{prop}
We defer the proof to Appendix \ref{sec:app.net}.
\begin{prop}
    For a generic Haar-random state $\ket{\Psi}$, the Gaussian Fidelity is exponentially small with probability exponentially close to unity. 
    In particular, for any $\delta > 0$,
    $F_G(\ket{\Psi}) \leq 2^{-n + 2}(1+ \delta)n^4$ with probability $\approx 1 - e^{-\delta n^4}$.
\end{prop}
\begin{proof}
    Following the proof of Claim 2 in \cite{Bravyi2019}, for any $n$-qubit state $\ket*{\tilde{\psi}}$, we have 
    $$\mathbb{P}\left(\abs{\braket*{\tilde{\psi}}{\Psi}}^2 \geq x\right) = (1-x)^{2^n-1} \leq e^{-x(2^n-1)}$$
    Taking a union bound over $\mathcal{N}$ with $x = 2^{-l}$,
    \begin{align*}
        \mathbb{P}\left(
        \max_{\ket*{\tilde{\psi}} \in \mathcal{N}} \abs{\braket*{\tilde{\psi}}{\Psi}}^2 \geq 2^{-l}
        \right) 
        &\leq \abs{\mathcal{N}} \cdot \exp({-2^{-l}(2^n - 1)}) \\
        & \leq \exp(-2^{n-l} + (n^4 + 2n^2 + n + ln^2)\log(2))
    \end{align*}
    Take $l = n - \log_2((1+\delta)n^4)$ for any $\delta > 0$. Then
    $$
    \mathbb{P}\left(
        \max_{\ket*{\tilde{\psi}} \in \mathcal{N}} \abs{\braket*{\tilde{\psi}}{\Psi}}^2 \geq 2^{-l}
    \right) \leq \exp(-\delta n^4 + \mathcal{O}(n^2))
    $$
    By definition of a $2^{-l}$-net, we can write $\ket{\psi} = \ket*{\tilde{\psi}} + 2^{-l}\ket{\zeta}$, where $\norm{\ket{\zeta}}_1 \leq 1$, $\ket{\psi} \in G$ and $\ket*{\tilde{\psi}} \in \mathcal{N}$. Then
    \begin{align*}
        F_G(\ket{\Psi}) &= \sup_{\ket{\psi} \in G} \abs{\braket{\psi}{\Psi}}^2 \\
        &= \abs{\braket*{\tilde{\psi}}{\Psi} + \epsilon \braket{\zeta}{\Psi}} ^ 2 \\
        &\leq (2^{-\frac{l}{2}} + 2^{-l}\cdot 1)^2 \\
        & \leq 2^{-l + 2}\\
        &= 2^{-n + 2}(1+ \delta)n^4
    \end{align*}
    with probability $\approx 1- e^{-\delta n^4}$.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multiplicativity of Gaussian extent} \label{sec:extent}

The \textit{Stabilizer Extent} was first defined in \cite{Bravyi2019} as a quantity that appears in an upper bound for the \textit{Approximate Stabilizer Rank}. 
It was found to be easier to work with than the rank, enjoying a natural phrasing as a Second Order Cone Program. Various tools from convex optimization then lend themselves to this formulation.

Bravyi et al. showed in \cite{Bravyi2019} that the Stabilizer Extent is multiplicative on systems of at most 3 qubits. 
Conversely, Heimandahl et al. showed in \cite{Heimendahl2021} that it is \textit{not} in general multiplicative; in fact, they showed that random states give a counterexample with probability exponentially close to 1.

%Inspired by the techniques of the aforementioned authors, we report on the successes and challenges of applying their methods to the present problem.

\subsection{Summary of results}

On the one hand, we follow the proof techniques of \cite{Heimendahl2021} almost to their conclusion. 
We show that, generically, optimal dual witnesses are extreme points of the feasible set, and that they are unique. 
The upper bound on the Gaussian Fidelity of Section \ref{sec:fidelity} is also a key ingredient. 
This is almost enough to show non-multiplicativity. 
All that remains is to demonstrate an entangled Gaussian state on $2n$ qubits which has overlap greater than 1 with a two-fold tensor product of extreme points.

On the other hand, we demonstrate a large class of extreme points whose normal cones define regions in (primal) space on which the Gaussian extent \textit{is} multiplicative.
These extreme points are closely related to Extended Hamming Codes, as discussed in Appendix \ref{sec:app.binary_codes}.
We show that on 4 qubits, these extreme points are the only ones possible, thus demonstrating that the extent is multiplicative on tensor products of at most 4 qubits.

Resolving this question thus reduces to answering one of the following questions in the affirmative:
\begin{itemize}
    \item Is there a Gaussian state $\ket{\psi} \in G_{2n}$ which has overlap greater than 1 with a product of extreme points $\ket{y_1}\ket{y_2}$? This would imply the extent is not multiplicative.
    \item Is the given class of extreme points a complete characterisation? This would imply the extent is multiplicative.
\end{itemize}

\subsection{Problem formulation}
For Gaussian states on $n$ qubits, we can formulate the Gaussian extent as a program.
\begin{defi}
    For any $\ket{\Psi} \in \mathcal{H}_n$, the \textit{Gaussian extent} $\xi_G(\ket{\Psi})$ is given by
    \begin{alignat}{3}
        & \sqrt{\xi_G (\ket{\Psi})} = &&\min &&\int_{s \in G} \abs{c(s)} ds \\
        & && \text{s.t.} &&\int_{s \in G} c(s) \ket{s} ds = \ket{\Psi} \nonumber \\
        & &&  &&c(s) \in \mathbb{C} \nonumber
    \end{alignat}
\end{defi}
Through standard methods (see the first Appendix of \cite{Heimendahl2021} for details) we can derive the dual form
\begin{alignat}{3}
    & \sqrt{\xi_G (\ket{\Psi})} = &&\max &&\braket{\Psi}{y}^R \label{eq:dual_problem} \\
    & && \text{s.t.} && \ket{y} \in \mathcal{H}_n \nonumber \\
    & && && \sqrt{F_G(\ket{y})} \leq 1 \nonumber
\end{alignat}
Following standard theory, we define the \textit{feasible set}:
\begin{defi}
    The \textit{feasible set} $M_G$ for the dual problem \eqref{eq:dual_problem} is defined as
    $$
    M_G \coloneqq \{ \ket{y} \in \mathcal{H}_n : \abs{\braket{s}{y}}^2 \leq 1 \ \forall \, \ket{s} \in G \}
    $$
\end{defi}
Since $G$ contains a basis of $\mathcal{H}_n$, both problems are strictly feasible and strong duality holds. Also, since $\ket{\Psi} / \sqrt{F_G(\ket{\Psi})}$ is feasible for the dual, we get the lower bound
$$
\xi_G{(\ket{\Psi})} \geq \frac{1}{F_G(\ket{\Psi})}
$$


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Extreme points of the feasible set} \label{sec:extreme}

For a system on $n$ qubits, define a classical binary code $\mathcal{C} \subseteq \{0,\,1\}^n$ of length $n$ and distance $d = 4$ with the properties:
\begin{itemize}
    \item $\forall x \in \mathcal{C}, \, \abs{x} = 0$ (mod 2)
    \item $\forall y \notin \mathcal{C},\, \abs{y} = 2, \, \exists \, x_1,\, x_2 \in \mathcal{C}$ such that $d(x_i,\, y) = 2$.
\end{itemize}
We have the following proposition:
\begin{prop} \label{prop:extreme_point}
    Define the state $\ket{M_n} = \sum_{x \in \mathcal{C}}\ket{x}$. Then $\ket{M_n}$ is an extreme point of $M_G$.
\end{prop}
Before proceeding with a proof, we give some details about such codes.

\subsubsection{Binary codes and multiple coverings} \label{sec:app.binary_codes}
We introduce several concepts from classical coding theory.

\begin{defi}
    For a binary code $\mathcal{C} \subseteq \{0,\,1\}^n$, the \textit{covering radius} $r_C$ is the least integer $r$ such that every element of $\{0,\,1\}^n$ is contained in at least one ball of radius $r$ centred on a codeword. 
\end{defi}

\begin{defi}
    Suppose the vectors of $\{0,\,1\}^n$ have been listed in some order. The \textit{greedy algorithm} chooses as codewords the first vector on the list, and then all vectors which have distance at least $d$ from all previously picked codewords.
\end{defi}
\begin{defi}
    An ordered basis $\mathcal{B}$ induces an ordering on $\{0,\,1\}^n$ by taking increasingly long sums of $y_i \in \mathcal{B}$, breaking ties by the order of $\mathcal{B}$.
    A $\mathcal{B}$\textit{-greedy code} is obtained by running the greedy algorithm on an ordering induced by $\mathcal{B}$.
\end{defi}
\begin{defi}
    \textit{Hamming Codes} are a family of linear binary codes. 
    Defined by their redundancy $m \geq 2$, they have parameters $[2^m-1, 2^m-m-1, 3]$. \textit{Extended Hamming Codes (EHCs)} are obtained by adding a parity bit to a standard Hamming code, enforcing that codewords have even parity. The resulting code is then a $[2^m, 2^m - m - 1, 4]$ linear code. Denote the EHC with redundancy $m$ by $\mathcal{C}^m$.
\end{defi}
\begin{defi}
    A \textit{multiple covering of furthest points} (MCF) \cite{Cohen1997} with parameters $r, \,\mu$ is a code with covering radius $r_C = r$ and further
    $$
    d(y, \mathcal{C}) = r \implies \exists \, \{x^i\}_{j = 1}^\mu \subset \mathcal{C} \text{ s.t. } d(y,\,x^j) = r
    $$
\end{defi}
\begin{defi}
    We have several methods of modifying codes:
    \begin{itemize}
        \item  \textit{Expurgating} a code is an operation which removes some subset of codewords. This corresponds to deleting rows of the generator matrix.
        \item \textit{Puncturing} a code is an operation which deletes some subset of coordinates from all codewords. This corresponds to deleting columns of the generator matrix.
        \item \textit{Shortening} a code is an operation which, for some coordinate set $T$, expurgates all codewords which are non-zero on $T$, then punctures the resulting code on $T$. This corresponds to deleting columns of the parity-check matrix. Denote a code $\mathcal{C}$ shortened on $T$ by $\mathcal{C}_T$.
    \end{itemize}
\end{defi}

Consider for now codes under less restrictive conditions than mentioned above: even-weight codewords, minimum distance $d = 4$ and covering radius $r_C = 2$.

From \cite{Brualdi1993}, every code with minimum distance $d$ and covering radius at most $d-1$ is a $\mathcal{B}${-greedy code}. Further, for $d = 4$, all $\mathcal{B}$-greedy codes are (possibly shortened) EHCs.

So even under this weaker condition, we can immediately restrict attention to only shortened EHCs.

Let us now return to the full restriction, that we want to \textit{doubly} cover even-weight non-codewords. For the case of $d = 4$, this coincides with the definition of MCF for $r = \mu = 2$.

In \cite{Cohen1997}, it is shown that the EHC is a (perfect) MCF with $r = 2,\, \mu = 2^{m-1}$. 
We consider shortenings of the EHC on coordinate sets $T$ with $|T| = t$.

\begin{prop} \label{prop:shortened_codes}
    For $0 \leq t \leq 2^{m-1} - 2$, $\mathcal{C}^m_T$ is a $[2^m - t, 2^m - m - 1 - t, 4]$ linear code. Moreover, it is an MCF with $r = 2,\,\mu \geq 2$.
\end{prop}

\begin{proof}
    The claimed length is immediate. The claimed dimension follows from the fact that exactly half of the codewords have value $0$ on each coordinate.

    The distance of a code $d$ is such that no set of $d - 1$ columns of the check matrix $H$ are linearly dependent, but some set of $d$ columns is. Since shortening corresponds to deleting columns of $H$, clearly this cannot reduce the distance. Moreover, since the code is linear, the distance is equivalent to the minimum weight of non-zero codewords. For any choice of $T$ with $t \leq 2^{m-1} - 2$, there will be some weight 4 codeword with support on the complement of $T$, and so the puncturing of this codeword will be in $\mathcal{C}^m_T$. Thus $\mathcal{C}^m_T$ is still distance 4.

    For the MCF property, consider some fixed $y$ such that $d(y,\, \mathcal{C}^m) = 2$. Then there are exactly $2^{m-1}$ codewords $x^j$ with $d(y,\, x^j) = 2$.
    We have $d(x^j, \, x^{j'}) \geq 4$ since they are both codewords. But $d(y,\,x^j) = d(y,\,x^{j'}) = 2$ implies $d(x^j,\,x^{j'}) \leq 4$, so we must have equality.
    Since we have $2^{m-1}$ such strings of length $2^m$, we see that each $x^j$ differs from $y$ on 2 bits, and for any bit exactly one $x^j$ differs from $y$ on that bit.

    Now consider any $y_T \in \{0,\,1\}^{2^m - t}$ such that $\abs{y_T} = 0 \text{ (mod } 2)$ and $y_T \notin \mathcal{C}^m_T$. Let $y$ be $y_T$ padded with $0$s at coordinates in $T$, so that the puncturing of $y$ at coordinates $T$ is $y_T$. Then also $y \notin \mathcal{C}^m$.

    Thus there are $2^{m-1}$ codewords $x^j \in \mathcal{C}^m$ with $d(x^j, \, y) = 2$. For each coordinate in $T$, exactly one $x^j$ has a 1 on that bit by the above reasoning. Therefore, at most $t$ of the $x^j$ are expurgated from the code during the shortening. So there are at least $2^{m-1} - t$ codewords $x^j_T \in \mathcal{C}^m_T$ with $d(y_T,\, x^j_T) = 2$. So $\mathcal{C}^m_T$ is an MCF with $r =2$ and $\mu \geq 2^{m-1} - t \geq 2$.
\end{proof}

Proposition \ref{prop:shortened_codes} states that shortened EHCs satisfy our desired properties, and in fact, they are the \text{only} codes that satisfy these properties.

We observe that for any desired length $n$, there is at most one choice of redundancy $m$ for which $2^{m} - 2^{m-1} +2 \leq n \leq 2^m$. Indeed, any $n \neq 2^m + 1$ can be obtained exactly once.

We do however still have some freedom in our choice of code. There are $\binom{2^m}{t}$ choices of coordinate set, which result in different codes. Moreover, we can have codes which are \textit{affine equivalent} to our original code. For our purposes, affine equivalence amounts to arbitrary permutation of the coordinates and translation by any fixed element of $\{0,\,1\}^n$. We will require the translation vector to have even weight.

\subsubsection{Proof of Proposition \ref{prop:extreme_point}}
We first prove that $\ket{M_n} \in M_G$, and then that $\ket{M_n}$ is an extreme point.
\begin{proof}[Feasibility]
    For any $y \in \mathcal{A}_n$, we have $\braket{y}{M_n} \in \{0,\,1\}$, so $\ket{M_n}$ is clearly feasible w.r.t. computational basis states.

    For any Gaussian state of the form $\ket{s} = \alpha \ket{y_1} + \beta \ket{y_2}$ with $d(y_1,\,y_2) = 2$, note that since the distance of the code is 4 only one of the $y_i$ can be a code word. Therefore $\abs{\braket{s}{M_n}}^2 \leq \max(\abs{\alpha}^2,\, \abs{\beta}^2) \leq 1$.
    
    If instead some Gaussian state was to have non-zero amplitudes for 2 code words, then it must also have non-zero amplitudes for at least 2 `balancing' terms which are not code words. 
    The form of such a state which achieves maximum possible overlap with $\ket{M_n}$ is 
    $\ket{s} = \alpha \ket{x_1} + (1- \alpha) \ket{x_2} + \sqrt{\alpha(1-\alpha)}(e^{i\phi}\ket{y_1} + e^{-i\phi}\ket{y_2})$, 
    where $x_i \in \mathcal{C},\, y_i \notin \mathcal{C},\, 0 \leq \alpha \leq 1$. 
    Then $\abs{\braket{s}{M_n}}^2 = 1 - 2\alpha + 2\alpha^2 \leq 1$.
    
    Therefore we have $\abs{\braket{s}{M_n}} \leq 1 \ \forall \, \ket{s} \in G$.
\end{proof}
\begin{proof}[Extremity]
    We show that $A \coloneqq \{ \ket{s} \in G : \braket{s}{M_n} = 1 \}$ spans $\mathcal{H}_n$. By Lemma \ref{lem:extreme_span}, this is sufficient to show that $\ket{M_n}$ is an extreme point.

    Clearly, for $x \in \mathcal{C}$, $\ket{x} \in A$.
    
    For $y \notin \mathcal{C}$, let $x_1,\,x_2 \in \mathcal{C}$ be such that $d(x_i,\,y) = 2$. Then consider the Gaussian states
    \begin{align*}
        \ket{s_1^y} &= \frac{1}{2}(\ket{x_1} + \ket{y} + \ket{y'} + \ket{x_2}) \\
        \ket{s_2^y} &= \frac{1}{2}(\ket{x_1} + i\ket{y} -i \ket{y'} + \ket{x_2})
    \end{align*}
    where $y' \notin \mathcal{C}$ satisfies $d(x_i,\,y') = 2$ and $d(y,\,y') = 4$. This is always possible by choosing $y'$ to differ from $y$ on exactly those bits on which the $x_i$ differ from $y$.
    
    Clearly $\ket{s_i^y} \in A$. Also $\ket{s_1^y} -i \ket{s_2^y} = \ket{y} + \tfrac{1-i}{2}(\ket{x_1} + \ket{x_2})$. 
    
    We then see that $A$ spans $\mathcal{H}_n$: 
    use $\{\ket{s_i^y} : y \notin \mathcal{C}\}$ to get the amplitudes of $y \notin \mathcal{C}$ correct, 
    and then use $\{\ket{x} : x \in \mathcal{C} \}$ to fix up the amplitudes of $x \in \mathcal{C}$.
\end{proof}



\subsection{Extent is multiplicative on products of at most 4 qubits} \label{sec:4_extent}
For $n = 4$, we will use the Krein-Milman Theorem on the feasible set $M_{G_4}$ to characterise the extreme points. 

\begin{thm}[Krein-Milman Theorem]
    Suppose $X$ is a compact, convex subset of a locally convex vector space. Then $X$ is equal to the closed convex hull of its extreme points. Moreover, for $B \subseteq X$, $X$ is equal to the closed convex hull of $B$ if and only if extreme$(X) \subseteq \text{closure}(B)$. 
\end{thm}

In Section \ref{sec:extreme} we defined a family of extreme points $\ket{M_n}$ of $M_G$ on $n$ qubits. 
The canonical representative for $n = 4$ is the state $\ket{M_4} = \ket{0000} + \ket{1111}$. 

It is clear that for any $U \in \mathcal{G}_4$, $U\ket{M_4}$ is also an extreme point of $M_{G_4}$. 

\begin{lem}
    Let $E = \{ U \ket{M_4} : U \in \mathcal{G}_4 \}$.
    Then extreme$(M_{G_4}) = E$.
\end{lem}

\begin{proof}
    Let $\ket{\phi} = \sum_{i \in \mathcal{A}_4} \ket{i} \in M_{G_4}$. Let $\gamma = F_G(\ket{\phi})$ and let $\ket{s} = U\ket{0}$ be any Gaussian which attains the Gaussian Fidelity. Without loss of generality take $\braket{s}{\phi} = \sqrt{\gamma}$.

    We will write $U^\dag \ket{\phi}$ as a convex combination of states in $E$.

    First note $\bra{0}U^\dag \ket{\phi} = \braket{s}{\phi} = \sqrt{\gamma} $.

    For any $\abs{x} = 2$ the states $\ket{\psi_\alpha} = \alpha \ket{0} + \sqrt{1 - \alpha^2}\ket{x} \in G$ for $\alpha \in [0,\,1]$. 
    We require 
    $$
    \abs{\bra{\psi_\alpha}U^\dag\ket{\phi}}^2 = 
    \abs{\alpha \sqrt{\gamma} + \sqrt{1-\alpha^2}\bra{x}U^\dag\ket{\phi}}^2
    \leq \gamma
    $$ 
    Therefore we must have $\bra{x}U^\dag\ket{\phi} = 0$.
    
    Thus $U^\dag \ket{\phi} = \gamma \ket{0} + r e^{i \theta} \ket{15}$ where $r \leq \gamma$. 

    For any $\omega \in [0,\,2\pi)$, let $P_\omega \coloneqq \text{diag}(1,\,e^{i\omega})$ and $G^\omega \coloneqq G_{12}(P_\omega,\,P_\omega) \in \mathcal{G}$.
    Then we can write:
    \begin{equation}
        U^\dag \ket{\phi} = \frac{\gamma + r}{2} G^\omega\ket{M_4}
        + \frac{\gamma - r}{2} G^{\pi + \omega} \ket{M_4}\\
        +\frac{1-\gamma}{2} \ket{M_4}
        +\frac{1-\gamma}{2} G_{12}(-I, -I) \ket{M_4}
    \end{equation}
    which is a convex combination. Applying $U$ to both sides, we see that $\ket{\phi}$ is a convex combination of states in $E$. 
    
    Therefore $M_{G_4} \subseteq \text{convex}(E)$. Since also $E \subseteq M_{G_4}$, we have equality. 
    By Krein-Milman, we infer extreme$(M_{G_4}) \subseteq \text{closure}(E)$. 
    Since $E$ is a closed set and $E \subseteq \text{extreme}(M_{G_4})$, we further have extreme$(M_{G_4}) = E$.
\end{proof}
The final ingredient is to note that when taking tensor products of $\ket{M_4}$, all terms have distance at least 4 from each other, and thus $\ket{M_4}^{\otimes k} \in M_{G_{4k}}$. Applying a Gaussian operator to each subsystem clearly cannot affect feasibility, so any tensor product of states in $E$ is also feasible. 

\begin{lem} \label{lem:4_extent}
    Stabilizer extent is multiplicative on systems of 4 qubits: $$
    \xi_{G_{4k}}(\otimes_i \ket{\phi_i}) = \prod_i \xi_{G_4}(\ket{\phi_i})
    $$
\end{lem}
\begin{proof}
    For each $i$, there is a choice of optimal dual witness $\ket{y_i}$ which is an extreme point, and therefore of the form $U_i \ket{M_4}$. Then $\otimes_i \ket{y_i} \in M_{G_{4k}}$. So we have
    \begin{equation}
        \xi_{G_{4k}}(\otimes_i \ket{\phi_i})
        = \max_{\ket{y} \in M_{G_{4k}}} \abs{\bra{y}(\otimes_i \ket{\phi_i})}^2 
        \geq \abs{(\otimes_i \bra{y_i}) (\otimes_i \ket{\phi_i})}^2 
        = \prod_i \abs{\braket{y_i}{\phi_i}}^2
        = \prod_i\xi_{G_4}(\ket{\phi_i})
    \end{equation}
    So we see that extent is super-multiplicative on these systems. Since extent is by definition sub-multiplicative, the result follows.
\end{proof}

\begin{prop} \label{prop:mult_extent}
    Stabilizer extent is multiplicative on systems of at most 4 qubits.
    Suppose $\ket{\phi} = \otimes_i \ket{\phi_i} \in \mathcal{H}_n$ where $\ket{\phi_i} \in \mathcal{H}_{n_i}$, $1 \leq n_i \leq 4$ and $n = \sum_i n_i$. Then
    $$\xi_{G_n}(\otimes_i \ket{\phi_i}) = \prod_i \xi_{G_{n_i}}(\ket{\phi_i})$$
\end{prop} 

\begin{proof}
    First note that \textit{all} states of even parity on 3 or fewer qubits are Gaussian. So for these systems, the decomposition is trivial and the extent is 1, so we can remove them from consideration.
    In combination with Lemma \ref{lem:4_extent}, the result follows.
\end{proof}

Note that the idea of Lemma \ref{lem:4_extent} applies to any primal state $\ket{\phi} \in \mathcal{H}_n$ which has optimal witness $\ket{y} = U\ket{M_n}$ for some $n \geq 4$ and some $U \in \mathcal{G}$.
Since the terms in $\ket{M_n}$ all have distance at least 4 from each other, $\ket{M_n}\ket{y'} \in G_{n+k}$ for any $\ket{y'} \in G_k$.
Then also $\ket{y}\ket{y'} \in G_{n+k}$.

So for any $\ket{\phi} \in \mathcal{H}_n$ which has optimal witness $\ket{y} = U\ket{M_n}$ and for any $\ket{\phi'} \in \mathcal{H}_k$, 
we have
\begin{equation}
    \xi_{G_{n+k}}(\ket{\phi}\ket{\phi'}) = \xi_{G_n}(\ket{\phi}) \xi_{G_k}(\ket{\phi'})
\end{equation}

\subsection{Optimal dual witnesses are generically unique, extreme points of the feasible set}
\label{subsec:unique_dual_witness}
Following the standard theory for second-order cone problems, we associate a \textit{normal cone} to each feasible point.
\begin{defi}
    The normal cone of a dually-feasible point $\ket{y} \in M_G$ is the set of primal points for which $\ket{y}$ is an optimal witness. In particular:
    $$
        C_y \coloneqq \left\{ 
        \ket{\Psi} \in \mathcal{H}_n : \braket{\Psi}{y}^R = \max_{p \in M_G} \braket{\Psi}{p}^R
        \right\}
    $$
\end{defi}

The union over all $\ket{y} \in M_G$ of these cones must be all of $\mathcal{H}_n$, since every $\ket{\Psi} \in \mathcal{H}_n$ has an optimal dual witness.

It can easily be checked that an equivalent definition is
$$
    C_y = \text{cone}\left\{ \ket{s} \in G : \braket{s}{y} = 1 \right\}
$$
where the cone over a set $X$, denoted cone$\{X\}$, is the set of all linear combinations with non-negative coefficients of a finite subset of elements of $X$.

In Appendix \ref{sec:app.dual}, we show that if $\ket{\Psi} \in \text{relint}(C_y)$ where $\ket{y}$ is an extreme point of $M_G$ then $\ket{y}$ is the unique optimal dual witness. For a Haar-random point, this occurs almost-surely. For points $\ket{\Psi} \notin \text{relint}(C_y)$, optimal dual witnesses are not unique but may still be chosen to be extreme points of $M_G$.

\subsection{Towards a proof that Gaussian extent is not multiplicative in general}
To prove that extent is not multiplicative, we need provide only a single counterexample.
Our attempt to find one focused on using the methods of \cite{Heimendahl2021} in the Gaussian case.

In Section \ref{sec:fidelity}, we showed that  for a generic Haar-random $\ket{\Psi} \in \mathcal{H}_n$, $F_G(\ket{\Psi}) \leq 2^{-n + 3}n^4$.
In Section \ref{subsec:unique_dual_witness}, we showed that almost-surely, $\ket{\Psi}$ has a unique optimal dual witness $\ket{y}$. 
We may lower bound the norm of $\ket{y}$:
\begin{align*}
    \braket{y}{y} = \braket{y}{y} \braket{\Psi}{\Psi}
    \geq \abs{\braket{y}{\Psi}}^2 
    = \xi_G(\ket{\Psi}) 
    \geq \frac{1}{F_G(\ket{\Psi})} 
    \geq 2^{n-3}n^{-4}
\end{align*}

Lemma 5 in \cite{Heimendahl2021} states that the extent is multiplicative on tensor product dictionaries.
The equivalent result here is:

\begin{lem}
    Let $\ket{\Psi_1}$ and $\ket{\Psi_2}$ be any states in $\mathcal{H}_n$ with optimal dual witnesses $\ket{y_1}$ and $\ket{y_2}$.
    Then $\xi_{G \otimes G}(\ket{\Psi_1}\ket{\Psi_2}) = \xi_G(\ket{\Psi_1}) \xi_G(\ket{\Psi_2})$ with optimal dual witness $\ket{y_1}\ket{y_2}$.
\end{lem}
\begin{proof}
    The proof is an immediate extension of the one in \cite{Heimendahl2021}.
\end{proof}
We therefore observe
\begin{equation} \label{eq:extent_ineq}
    \xi_{G_{2n}}(\ket{\Psi_1}\ket{\Psi_2}) \leq \xi_{G \otimes G}(\ket{\Psi_1}\ket{\Psi_2}) = \xi_{G_n}(\ket{\Psi_1})\xi_{G_n}(\ket{\Psi_2})
\end{equation}
The inequality in Equation \eqref{eq:extent_ineq} will become strict if there exists $\ket{s} \in G_{2n}$ with $\abs{\bra{s}(\ket{y_1}\ket{y_2})}^2 > 1$.
Since $\norm{y}$ is exponentially large, it seems plausible that such a state exists.
However, it is not obvious how to find it. 
Indeed, as discussed in Section \ref{sec:4_extent}, for all extreme points known to the authors it was impossible to find such a counterexample.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Gaussian rank} \label{sec:rank}
By the Gottesman-Knill Theorem, inner products of stabilizers with Clifford projectors $\bra{\phi_1}\Pi\ket{\phi_2}$ are classically efficiently computable. 
By decomposing the magic states injected into a Clifford circuit, any quantum process may be brute-force simulated by computing inner products of this form \cite{Bravyi_2016}.

The minimum number of terms in the stabilizer decomposition to be the Stabilizer Rank of a state. 
The number of terms in the above simulation method scales as $\chi_n^2$, where $\chi_n$ is the Stabilizer Rank of $n$ copies of the magic state. The stabilizer rank $\chi_n$ has received significant attention recently (e.g. \cite{Labib_2022}, \cite{Peleg_2022}). 
It is expected that $\chi_n$ must scale super-polynomially, else all quantum process would be simulable in polynomial classical time.
However, it has proven difficult to achieve concrete results, and the state of the art bound is currently $\chi_n = \Omega(n)$.

Since matchgate circuits are also classically simulable, it is natural to consider this simulation technique for matchgate + SWAP-gadget circuits. One therefore may expect that the proof techniques for Stabilizer Rank might carry over to the Gaussian rank.

We initially consider the magic state $\ket{M} = \tfrac{1}{\sqrt{2}}(\ket{0} + \ket{15})$. Clearly, this state has Gaussian rank 2. 
It is also coincides with the normalised state $\ket{M_4}$ defined in Section \ref{sec:extreme}, and has the minimum possible Gaussian fidelity of $1/2$.
In this sense, it is intuitively `maximally magic'.
Also, this state is matchgate-equivalent to the magic state used in the SWAP-gadget of \cite{Hebenstreit2019}. 

We consider tensor properties of $k = 2$ or 3 copies of $\ket{M}$ and seek out decompositions of rank less than $2^k$.

\subsection{No symmetric low-rank decomposition for 2 copies of the magic state}
Based on the numerical results (discussed below in Section \ref{sec:numerical}), we conjecture that the Gaussian rank of 2 copies of $\ket{M}$ is $\chi_2(\ket{M}) = 4$.
We were unable to prove this in full generality, however, we were able to make some progress in a symmetry-reduced case.

Note that $\ket{M}\ket{M}$ is invariant under the (Gaussian) operations 
$O_1 \coloneqq Z_1 Z_2 = G_{12}(I,-I) $ and $O_2 \coloneqq Z_7 Z_8 = G_{78}(I,-I)$. 
We enforce this symmetry on a supposed decomposition of rank 3.
Under this restriction, we were able to analytically prove that no solution exists.

\begin{prop} \label{prop:rank}
    The symmetry-restricted Gaussian rank of $\ket{M}$ is $\tilde{\chi}_2(\ket{M}) = 4$.
    In particular, there is no solution to the equation:
    $$\ket{M}\ket{M} = \sum_{i=1}^3 c_i \ket{\psi_i}$$
    where the $\ket{\psi_i}$ are symmetry-restricted Gaussian states satisfying
    $$O_j \ket{\psi_i} = \ket{\psi_i} \quad j = 1,\,2 \quad i = 1,\,2,\,3$$
\end{prop}

We defer a full proof to Appendix \ref{sec:app.rank}, but give some discussion here.

The symmetry is chosen in order to greatly simplify the constraints on each of the Gaussian states.
Let $c_i \ket{\psi} = \sum_{k} a_k \ket{k}$ be an un-normalized Gaussian state with non-zero amplitudes only for states $\ket{k}$ which are invariant under $O_j$. 
For $a_0 \neq 0$, we are left almost exclusively with constraints which only have 2 terms. 
We can visualise the constraints in a grid, wherein amplitudes in the body of the grid are defined in terms of the leading entries of their row and column.
Table \ref{tab:constraints_0} represents all the constraints present on a restricted Gaussian state.

\begin{table}[t]
    \centering
    \begin{tabular}{c|ccccccc}
         0  & 3  & 65  & 66  & 129 & 130 & 192 & 195 \\\hline
         12 & 15 & 77  & 78  & 141 & 142 & 204 & 207 \\
         20 & 23 & 85  & 86  & 149 & 150 & 212 & 215 \\
         24 & 27 & 89  & 90  & 153 & 154 & 216 & 219 \\
         36 & 39 & 101 & 102 & 165 & 166 & 228 & 231 \\
         40 & 43 & 105 & 106 & 169 & 170 & 232 & 235 \\
         48 & 51 & 113 & 114 & 177 & 178 & 240 & 243 \\
         60 & 63 & 125 & 126 & 189 & 190 & 252 & 255 \\
    \end{tabular}
    \caption{Constraints on a restricted Gaussian state, assuming $a_0 \neq 0$. The constraints are recovered by reading off a component in the body as the product of its row header and column header, divided by $a_0$. E.g. $a_{15} = a_3 a_{12} / a_0$, $a_{90} = a_{66} a_{24} / a_0$ etc.}
    \label{tab:constraints_0}
\end{table}

The only exceptions are a pair of constraints which have 4 terms in.
These can be viewed as fixing the values of the last entry in the first row and column respectively:
\begin{equation}
    \begin{aligned}
        a_0 a_{195} - a_3 a_{192} + a_{65} a_{130} - a_{66} a_{129} &= 0 \\
        a_0 a_{60} - a_{12} a_{48} + a_{20} a_{40} - a_{24} a_{36} &= 0
    \end{aligned}
\end{equation}

If $a_0 = 0$, then we must delete either the first row or the first column.
This is because we have from the second column of the grid:
$$
a_3 a_{12} = a_3 a_{20} = a_3 a_{24}= a_3 a_{36}= a_3 a_{40}= a_3 a_{48}= a_3 a_{60} = 0
$$
Thus either every term in the first column vanishes, or $a_3 = 0$. 
Proceeding similarly along the columns, we see that either the first column or first row must all vanish.

The table is still valid, however, as long as the value in the upper-leftmost entry is non-zero.
For example, we might have $a_{12} = a_{20} = a_{24}= a_{36}= a_{40}= a_{48}= a_{60} = 0$, $a_3 \neq 0$. Then equations are read off like $a_{77} = a_{65}a_{15}/a_3$.

The proof is essentially an exhaustive case-bash. 
We use the fact that for $\ket{\psi_2}$, each term in the body of Table \ref{tab:constraints_0} is  defined twice: 
once by the requirement that $\ket{\psi_2}$ is Gaussian; 
and a second time by the requirement that the $c_i \ket{\psi_i}$ sum to two copies of the magic state.
This gives a system of equations which we prove is insoluble.

Let $c_i \ket{\psi_i} = \sum_k a^i_k \ket{k}$. 
We first consider the case $a_0^i \neq 0$ for $i = 1,\,2,\,3$. 
This case allows for a somewhat neat matrix argument. Cases where some $a_0^i = 0$ become more complicated, spawning sub-cases as we must choose which row or column to delete.
Nonetheless, it is possible to find a contradiction in all cases. 
We note some obvious limitations of this argument.
Firstly, there is little justification for the symmetry reduction beyond some appeal to the symmetry of the problem. 
Without the reduction, the algebraic constraints are hard to work with, with no easy concept of independent vs. dependent components.
It therefore seems unlikely that our proof techniques would straightforwardly extend to the unrestricted case.

Also, the method is very much bespoke to the case of two copies of the magic state. 
With any more copies, the number of terms becomes too unwieldy and it is impractical to unwind all the definitions, even for the reduced case. We therefore expect that further results on lower bounds for the Gaussian rank will require new techniques that we have not considered here.

\subsection{Numerical results} \label{sec:numerical}
We used numerical optimization tools from the \textit{SciPy} \cite{2020SciPy} package to seek out low-rank decompositions.
The algorithm which found the most success was based on simulated annealing with cost function $L(\ket{\Psi}) = \norm{\ket{{M}}\ket{{M}} - \ket{\Psi}}_2^2$.

Our programme failed to find any exact low-rank decompositions for either 2 or 3 copies when using global optimization techniques for hundreds of hours of machine time. We view this as strong evidence that no such decompositions exist, but welcome future attempts with more refined techniques. 

Our code did see some success in finding approximate decompositions, but results were highly dependent on the choice of magic state. 
The code never found an decomposition better than a trivial one for $\ket{M}\ket{M}$: using the 3 Gaussian states to get 3 out of 4 non-zero amplitudes correct.
We conjecture that good decompositions, even approximate ones, do not exist for only few copies of this state due to its low Gaussian fidelity.

Consider as a counterexample the magic state $\ket{M_\alpha} = \alpha \ket{0} + \sqrt{1-\alpha^2}\ket{15}$. 
As $\alpha \xrightarrow{} 1$, $F_G(\ket{M_\alpha}) \xrightarrow{} 1$ also.
For any $k$, It is then trivial to find an approximate Gaussian decomposition of $\ket{M_\alpha}^{\otimes k}$ with rank $1$; namely, $\ket{\Psi} = \alpha^k \ket{0}$. 
This decomposition has loss $L = \sqrt{2}k \alpha^{k-1} \sqrt{1-\alpha}$ to leading order in $(1-\alpha)$.
Such a magic state is not practically useful; we therefore choose to restrict to magic states which are bounded away from Gaussian states.

One alternative which did see success was the magic state $\ket*{\tilde{M}} = \tfrac{1}{\sqrt{8}}(\sqrt{3}\ket{0} + \sqrt{2}\ket{3} + \sqrt{2}\ket{12} + \ket{15})$.
For two copies of this state, decompositions with $L(\ket{\Psi}) \approx 10^{-5}$ were found.

Sample code is available, see Appendix \ref{sec:app.code}. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{gaussian_states_references}
\bibliographystyle{quantum}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Triples of Gaussian states} \label{sec:app.triple}
\begin{prop}
    The manifold of solutions to the system of equations 
    \begin{equation}
        \tilde{\mathcal{F}} = \left\{ \tilde{f}(y) = 
        \sum_{i = 2}^{4} (-1)^i a_{\overline{0}^{k_1,\,k_i}} a_{\overline{y}^{k_1,\,k_i}} = 0 
        : \abs{y} = 4
        \right\} \label{eq:constraint_sys}
    \end{equation}
    has dimension $2n -3$.
\end{prop}
For convenience, we introduce some new notation. For any $x \in \mathcal{A}_n$ with $\abs{x} = w$ and with 1s in locations $i_0,\,i_1,\ldots,\,i_{w-1}$, we will write $x = [i_0,\,i_1,\ldots,\,i_{w-1}]$. 
In expressions such as $[b_0,\,b_1,\,c_0,\,c_1]$ we will always have $b_0 < b_1$, $c_0 < c_1$ and $\{b_0,\,b_1\} \cap \{c_0,\,c_1\} = \emptyset$, but we may not know the relative ordering of the $b_i$ and $c_i$.
\begin{proof}
    Suppose $a_x \neq 0$ for some $x = [b_0,\,b_1]$, which we require for a non-trivial linearly dependent triple.
    
    Define $W_k = \{ y \in \mathcal{A}_n : \abs{y} = 4, \, d(x, y) = k \}$ for $k = 2,\,4,\,6$. 

    First, let $y \in W_2$. Then $y = [b_0,\,b_1,\,c_0,\,c_1]$. Since $a_x \neq 0$, we can write
    \begin{align}
        a_{[c_0,\, c_1]} &= \frac{1}{a_{[b_0,\,b_1]}} \bigl(
        s_0^{b_0,\,b_1,\,c_0,\,c_1} a_{[b_0,\,c_0]}a_{[b_1,\,c_1]} + s_1^{b_0,\,b_1,\,c_0,\,c_1} a_{[b_0,\,c_1]}a_{[b_1,\,c_0]}
        \bigr) \label{eq:fixed_var}\\
        s_0 &= \begin{cases}
            -1 \quad \text{if} \quad b_0<c_0<c_1<b_1 \quad \text{or} \quad c_0<b_0<b_1<c_1\\
            +1 \quad \text{otherwise}
        \end{cases} \nonumber \\
        s_1 &= \begin{cases}
            -1 \quad \text{if} \quad b_0<b_1<c_0<c_1 \quad \text{or} \quad c_0<c_1<b_0<b_1\\
            +1 \quad \text{otherwise}
        \end{cases} \nonumber
    \end{align}

    Equation \eqref{eq:fixed_var} fixes the $\binom{n-2}{2}$ amplitudes $a_{z} = a_{[c_0,\,c_1]}$ with $d(x,\,z) = 4$. We now show that all other equations in $\tilde{\mathcal{F}}$ are identically satisfied. 
    Thus there are $\binom{n}{2} - \binom{n-2}{2} = 2n - 3$ free variables, giving the desired manifold dimension.

    Let $y \in W_4$. Then $y = [b_k,\,d_0,\,d_1,\,d_2]$ for $k = 0$ or 1. The corresponding equation in \eqref{eq:constraint_sys} reads
    \begin{align*}
        &t_0^{b_k, d_0,d_1,d_2} a_{[b_k, \, d_0]}a_{[d_1, \, d_2]} 
        + t_1^{b_k, d_0,d_1,d_2} a_{[b_k, \, d_1]}a_{[d_0, \, d_2]} 
        + t_2^{b_k, d_0,d_1,d_2} a_{[b_k, \, d_2]}a_{[d_0, \, d_1]} = 0\\
        &t_0 = \begin{cases}
            -1 \quad \text{if} \quad d_0<d_1<b_k<d_2\\
            +1 \quad \text{otherwise}
        \end{cases}\\
        &t_1 = \begin{cases}
            -1 \quad \text{if} \quad b_k<d_0<d_1<d_2 \quad \text{or} \quad d_0 < d_1<d_2<b_k\\
            +1 \quad \text{otherwise}
        \end{cases}\\
        &t_2 = \begin{cases}
            -1 \quad \text{if} \quad d_0<b_k<d_1<d_2\\
            +1 \quad \text{otherwise}
        \end{cases}
    \end{align*}
    
    Each of $a_{[d_1, \, d_2]}, a_{[d_0, \, d_2]} \,\&\, a_{[d_0, \, d_1]}$ are fixed by Equation \eqref{eq:fixed_var}. Substituting in, (multiplying out $a_{[b_0,\,b_1]}$), the equation now reads

    \begin{multline}
        t_0^{b_k, d_0,d_1,d_2}a_{[b_k, \, d_0]} \bigl(
        s_0^{b_0,b_1,d_1,d_2} a_{[b_0,\, d_1]}a_{[b_1,\, d_2]} + 
        s_1^{b_0,b_1,d_1,d_2} a_{[b_0,\, d_2]}a_{[b_1,\, d_1]}
        \bigr)\\
        + 
        t_1^{b_k, d_0,d_1,d_2}a_{[b_k, \, d_1]} \bigl(
        s_0^{b_0,b_1,d_0,d_2} a_{[b_0,\, d_0]}a_{[b_1,\, d_2]} + 
        s_1^{b_0,b_1,d_0,d_2} a_{[b_0,\, d_2]}a_{[b_1,\, d_0]}
        \bigr)\\
        +
        t_2^{b_k, d_0,d_1,d_2}a_{[b_k, \, d_0]} \bigl(
        s_0^{b_0,b_1,d_0,d_1} a_{[b_0,\, d_0]}a_{[b_1,\, d_1]} + 
        s_1^{b_0,b_1,d_0,d_1} a_{[b_0,\, d_1]}a_{[b_1,\, d_0]}
        \bigr) = 0 \label{eq:w4_equation}
    \end{multline}
    Labelling the terms of Equation \eqref{eq:w4_equation} in order 1 to 6, we can pair them off as follows. If $k = 0$, pair $(1,3),\,(2,5)\, \& \, (4,6)$. If $k=1$, instead pair $(1,6),\,(2,4)\,\&\,(3,5)$. Then it can be shown that each pair sums to 0.
    For example, for $k = 0$, we have:
    \begin{align*}
        (1) + (3) \quad &\propto \quad  
        t_0^{b_0, d_0,d_1,d_2}s_0^{b_0,b_1,d_1,d_2} 
        + t_1^{b_0, d_0,d_1,d_2}s_0^{b_0,b_1,d_0,d_2} \\
        (2) + (5) \quad &\propto \quad  
        t_0^{b_0, d_0,d_1,d_2}s_1^{b_0,b_1,d_1,d_2} + 
        t_2^{b_0, d_0,d_1,d_2}s_0^{b_0,b_1,d_0,d_1} \\
        (4) + (6) \quad &\propto \quad  
        t_1^{b_0, d_0,d_1,d_2}s_1^{b_0,b_1,d_0,d_2} 
        + t_2^{b_0, d_0,d_1,d_2}s_1^{b_0,b_1,d_0,d_1} 
    \end{align*}
    For example, note that $t_0$ and $t_1$ have the same sign only if $d_0 < b_0 < d_1 < d_2$. If this is the case, then $s_0^{b_0,b_1,d_1,d_2}$ and $s_0^{b_0,b_1,d_0,d_2}$ have different signs, as can be seen from checking the 3 possible relative locations of $b_1$. Thus $(1) + (3) = 0$. Similarly for the other pairs. The remaining cases can all be checked similarly.

    A similar argument holds for $y = [d_0,\,d_1,\,d_2,\,d_3] \in W_6$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\texorpdfstring{An $\epsilon$-net for the Gaussian states}{} \label{sec:app.net}}
For each $j \in \mathcal{A}_n$, define the regions
$$
S_j = \biggl\{ \ket{\psi} = \sum_{j' \in \mathcal{A}_n}c_{j'} \ket{j'} \in G: \abs{c_j} \geq \abs{c_{j'}} \quad \forall \, j' \in \mathcal{A}_n \biggr\}
$$
Clearly, the union over $j$ gives all of $G$. 
In each region $S_j$, Gaussian states may be defined by the set of amplitudes with labels within distance 2 of $j$, given by the set $C = \{ c_k : d(j, k) \leq 2 \}$.
For these amplitudes, we have 
\begin{align*}
    \abs{c_j} &\in [2^{\frac{1-n}{2}}, \, 1] \\
    \abs{c_k} &\in [0, \, \abs{c_j}] \quad \ \forall \, c_k \in C \setminus \{ c_j \}
\end{align*}
Let $\mathcal{N}_j = \{\ket{\psi_i} \} \subset S_j$ be a maximal set of pure states satisfying 
$$
\forall \, \ket{\psi_i}, \ket{\psi_{i'}} \in \mathcal{N}_j, \ \exists \ x \in \mathcal{A}_n \text{ s.t. } d(j, x) \leq 2  \quad \abs{\braket{x}{\psi_i} - \braket{x}{\psi_{i'}}} \geq \eta
$$
Let $\mathcal{N} = \cup_j \mathcal{N}_j$. We wish to find a suitable $\eta$ so that $\mathcal{N}$ is an $\epsilon$-net. We require the 2 following lemmas:

\begin{lem}\label{lem:eta_bound}
    For $\eta = 2^{-n^2} \epsilon $, we have 
    $\sup_{\ket{\psi} \in S_j} \inf_{\ket*{\tilde{\psi}} \in \mathcal{N}_j} \norm{\ket{\psi} - \ket*{\tilde{\psi}}}_1 \leq \epsilon \ \forall \, j \in \mathcal{A}_n$.
\end{lem}

\begin{lem}\label{lem:net_cardinality_bound}
    The cardinality of $\mathcal{N}_j$ is upper bounded by 
    $\abs{\mathcal{N}_j} \leq 2^{2n^2 + 1}\eta^{-n^2}$.
\end{lem}

\begin{proof}[Proof of Proposition \ref{prop:eps_net}]
    By Lemma \ref{lem:eta_bound}, we have
    \begin{align*}
        \sup_{\ket{\psi} \in G} \inf_{\ket*{\tilde{\psi}} \in \mathcal{N}} \norm{\ket{\psi} - \ket*{\tilde{\psi}}}_1 
        &\leq \sup_{j \in \mathcal{A}_n} \sup_{\ket{\psi} \in S_j} \inf_{\ket*{\tilde{\psi}} \in \mathcal{N}_j} \norm{\ket{\psi} - \ket*{\tilde{\psi}}}_1 < \epsilon
    \end{align*}
    So $\mathcal{N}$ is a valid $\epsilon$-net.

    By Lemma \ref{lem:net_cardinality_bound} with $\eta = 2^{-n^2}\epsilon$, we have
    \begin{align*}
        \abs{\mathcal{N}} = \sum_j \abs{\mathcal{N}_j} \leq 2^{n-1} \cdot 2^{2n^2 +1} 2^{n^4}\epsilon^{-n^2}
    \end{align*}

    Let $\epsilon = 2^{-l}$. Then $\log_2 \abs{\mathcal{N}} \leq n^4 + 2n^2 + n+ ln^2$.
\end{proof}


\begin{proof}[Proof of Lemma \ref{lem:net_cardinality_bound}]
    Without loss of generality consider the region $S_0$ and sub-net $\mathcal{N}_0$.
    
    We use a volume argument to estimate the number of states in the sub-net. 
    Note that each state $\ket{\psi_i} \in \mathcal{N}_j$ occupies a disjoint region of $\binom{n}{2}$-dimensional (complex) space with volume $(\pi\eta^2)^{\binom{n}{2}}$.

    The centre of each of these disjoint regions lies in $S_0$. 
    The volume of the total region that they are contained in can be estimated by integrating over each free component $c_x$ with $\abs{x} \leq 2$. Viewed in $\mathbb{R}^2$,
    $c_0$ lies in an annulus with inner radius $2^{\frac{1-n}{2}} - \frac{\eta}{2}$ and outer radius $1+ \frac{\eta}{2}$. 
    Each $c_x$ lies in a disk of radius $\abs{c_0} + \frac{\eta}{2}$.
    Thus we see that the volume is no greater than
    \begin{align*}
        V &= 2\pi \int_{2^{\frac{1-n}{2}} - \frac{\eta}{2}}^{1 + \frac{\eta}{2}} dr_0 \left\{
        \left(\pi(r_0 + \frac{\eta}{2})^2\right)^{\frac{n(n-1)}{2}} 
        \right\}\\
        &= 2\pi^{\frac{n(n-1)}{2}+1} \left[
        \frac{(r_0 + \frac{\eta}{2})^{n(n-1)+1}}{n(n-1)+1} 
        \right]_{2^{\frac{1-n}{2}} - \frac{\eta}{2}}^{1 + \frac{\eta}{2}}\\
        &= 2\pi^{\frac{n(n-1)}{2}+1} \left(
        \frac{(1 + \eta)^{n(n-1)+1}}{n(n-1)+1} - \frac{(2^{\frac{1-n}{2}})^{n(n-1)+1}}{n(n-1)+1}
        \right)\\
        &\leq 2 \cdot \pi^{\frac{n^2}{2}} \cdot 2^{n(n-1)+1} \\
        &\leq 2 \cdot 2^{n^2} \cdot 2^{n^2 - n} = 2^{2n^2- n +1}
    \end{align*}

    The number of states is then no more than
    \begin{align*}
        \abs{\mathcal{N}_j} 
        &\leq \frac{V}{(\pi \eta^2)^{\binom{n}{2}}} \\
        &\leq 2^{2n^2 - n + 1}2^{- n(n-1)/2}\eta^{-n(n-1)} \\
        &\leq 2^{\frac{3}{2}n^2 - \frac{1}{2}n + 1}\eta^{-n^2} \\
        &\leq 2^{2n^2+1}\eta^{-n^2}\qedhere
    \end{align*}
\end{proof}

\begin{proof}[Proof of Lemma \ref{lem:eta_bound}]
    We will frequently use
    $$
    \abs{AB - A'B'} \leq \abs{A- A'}\abs{B} + \abs{B - B'}\abs{A'}
    $$
    and its generalisation to longer strings. Note also that all amplitudes have modulus $\leq 1$, so we can always take further
    $$
    \abs{AB - A'B'} \leq \abs{A- A'} + \abs{B - B'}
    $$
    when all the quantities involved are amplitudes.

    By definition, for any state $\ket{\psi} \in S_0$, $\exists \ket*{\tilde{\psi}} \in \mathcal{N}_j$ such that $\forall \, \abs{x} \leq 2$:
    \begin{gather*}
        c_x \coloneqq \braket{x}{\psi} \quad \tilde{c_x} \coloneqq \braket*{x}{\tilde\psi}\\
        \abs{c_x - \tilde{c_x}} \eqqcolon \abs{\delta_x} \leq \eta \eqqcolon \Delta_2 
    \end{gather*}
    For $\abs{x} \geq 4$, we find a recursion relation, bounding $\abs{\delta_x} \leq \Delta_w$ for $\abs{x} = w, \, w = 4,\, 6,\, 8, \dots, n$ (assume $n$ even for simplicity).
    We proceed by induction. Suppose we have bounded $\abs{\delta_x} \leq \Delta_{w'}$ for all $\abs{x} = w'$, $w' = 2,\, 4,\, \dots, w-2$. 
    
    Then for any $\abs{x} = w$, we have a constraint equation $c_0 c_x = f_x$, where $f_x$ contains $(w-1)$ terms of the form $c_{i}c_{i'}$ where $\abs{i} = 2$ and $\abs{i'} = w - 2$. Again using tildes to denote quantities relating to states on the net, we have
    \begin{align}
        \abs{\delta_x} &= \abs{c_x - \tilde{c}_x} \nonumber \\
        &= \abs{\frac{f_x}{c_0} - \frac{\tilde{f}_x}{\tilde{c}_0}} \nonumber \\
        &\leq \abs{c_0}^{-1} \abs{\tilde{c}_0}^{-1} \left( 
        \abs{\tilde{c}_0 - c_0} \abs{f_x} + \abs{f_x - \tilde{f}_x} \abs{\tilde{c}_0}
        \right)   \label{eq:delta_x}
    \end{align}
    We can bound each term in this expression. Assume that $\eta \leq 2^{\frac{1-n}{2}} - 2^{\frac{-(1+n)}{2}}$.
    Then we have:
    \begin{align*}
        \abs{c_0}^{-1} &\leq 2^{\frac{n-1}{2}} \\
        \abs{\tilde{c}_0}^{-1} &\leq (2^{\frac{1-n}{2}}- \eta)^{-1} \leq 2^{\frac{n+1}{2}} \\
        \abs{\tilde{c}_0 - c_0} &\leq \Delta_2 \\
        \abs{f_x} &= \abs{c_0} \abs{c_x} \leq 1 \\
        \abs{\tilde{c}_0} &\leq 1 \\
        \abs{f_x - \tilde{f}_x} &\leq (w-1)(\Delta_2 + \Delta_{w-2})
    \end{align*}
    Plugging in to Equation \eqref{eq:delta_x}, we obtain:
    \begin{align*}
        \abs{\delta_x} \leq 2^n (\Delta_2 + (w-1)(\Delta_2 + \Delta_{w-2}))
    \end{align*}
    Since this is true for any $x$ with $\abs{x} = w$, and noting that $\Delta_2 \leq \Delta_4 \ldots \leq \Delta_n$, we have
    $\Delta_w = 2^n (2w-1)\Delta_{w-2}$.
    The solution to this recurrence relation with $\Delta_2 = \eta$ is
    $$\Delta_w = \frac{1}{3}2^{\frac{1}{2}(n+2)w - n}\frac{\Gamma(\frac{w}{2} + \frac{3}{4})}{\Gamma(\frac{3}{4})} \eta $$.

    We then have
    \begin{align*}
        \norm{\ket{\psi} - \ket*{\tilde{\psi}}}_1
        &= \sum_{x \in \mathcal{A}_n} \abs{c_x - \tilde{c}_x} \\
        & \leq \sum_{k = 0}^{\frac{n}{2}}\binom{n}{2k} \Delta_{2k} \\
        & \leq 2^{n-1} \Delta_n \\
        & \leq \frac{1}{3} \cdot 2^{\frac{1}{2}n^2 + n - 1} \cdot \frac{\Gamma(\frac{n}{2}+\frac{3}{4})}{\Gamma(\frac{3}{4})} \eta
    \end{align*}
    Using $\Gamma(z+1) = z\Gamma(z)$, we may obtain 
    $$
    \frac{\Gamma(\frac{n}{2}+\frac{3}{4})}{\Gamma(\frac{3}{4})} 
    \leq \left(\frac{n}{2}\right)^{\frac{n}{2}} 
    \leq 2^{\frac{n}{2}(\log_2(n) - 1)}
    $$ 
    So finally we have 
    \begin{equation*}
        \norm{\ket{\psi} - \ket*{\tilde{\psi}}}_1 \leq 2^{\frac{1}{2}(n^2 + \log_2(n) + 1)}\eta \leq 2^{n^2}\eta
        \qedhere
    \end{equation*}
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Properties of optimal dual witnesses for the Gaussian extent \label{sec:app.dual}}
Suppose $\ket{y} \in \text{relint} (M_G)$. Then $\exists \, \alpha > 1$ s.t. $\alpha \ket{y} \in M_G$. 
Then $\forall \, \ket{\Psi} \in \mathcal{H}_n$:
$$
\braket{ \Psi}{ \alpha y }^R 
> \braket{ \Psi}{ y }^R \implies C_y = \emptyset
$$
So we can restrict our attention only to points $y$ on the boundary $\partial M_G$.

Note that since it is a bounded, compact, convex set, $M_G$ is the convex hull of its extreme points by Krein-Millman. Let $E$ denote the set of extreme points. Then for any $\ket{x} \in \partial M_G$, we can write
$$
\ket{x} = \sum_i c_i \ket{y_i}
$$
where $\ket{y_i} \in E$ and $c_i \in \mathbb{R}_+$ with $\sum_i c_i = 1$. Note that by Carath\'eodory's theorem, this sum can be taken to have only $d+1$ terms, and in particular is a finite sum, not an integral.

Let $\ket{\Psi} \in C_x$. Then
\begin{align*}
    \braket{ \Psi}{  x }^R 
    &= \left( \sum_i c_i \braket{ \Psi}{  y_i } \right)^R \\
    &\leq \sum_i c_i \braket{ \Psi}{ x }^R \\
    &= \braket{ \Psi}{  x }^R
\end{align*}
where on the second line we used that the $c_i$ are real and also that $\ket{\Psi} \in C_x$, and in the third line we used that the $c_i$ sum to unity.

We must therefore have equality on the second line, so $\braket{ \Psi}{ x }^R = \braket{ \Psi}{  y_i }^R$. So $\ket{\Psi} \in C_x$ iff $\ket{x} = \sum_i c_i \ket{y_i}$, where $\ket{y_i} \in E$ are all optimal dual witnesses ie $\ket{\Psi} \in C_{y_i}$.

Therefore, $\ket{\Psi} \in \mathcal{H}_n$ has non-unique optimal dual witnesses iff $\ket{\Psi}$ lies in the intersection of some $C_{y_i}$ for $\ket{y_i} \in E$. We now show that the $C_{y_i}$ intersect only on their boundaries.

\begin{prop} \label{prop:unique_cone}
    For distinct $\ket{y}, \, \ket{y'} \in E$,
    $\ket{\Psi} \in \text{relint}(C_y) \implies \ket{\Psi} \notin C_{y'}$.
\end{prop}

Before proceeding with the proof, we will need some technical Lemmas:

\begin{lem}\label{lem:extreme_span}
    Let $\ket{y} \in M_G$. 
    Define the sets:
    $$
    A_y = \left\{ \ket{s} \in G : \braket{s}{y} = 1 \right\} \quad \overline{A}_y = \left\{ \ket{t} = U X_i X_j U^\dag \ket{s} : \ket{s} = U \ket{0} \in A_y ; i \neq j \right\}
    $$
    Then $\ket{y} \in E \implies \text{span}(A_y \cup \overline{A}_y) = \mathcal{H}_n$.
\end{lem}

We first motivate the rather strange definition $\overline{A}_y$. 

Hamming distance plays a key role in the definition of Gaussian states; computational basis states with distance at least 4 are constrained relative to one another.
Thus $\overline{A}_y$ in some sense encodes states $\ket{t}$ which are unconstrained with respect to some $\ket{s} \in A_y$. By this we mean that $\sqrt{\alpha} \ket{s} + \sqrt{1 -\alpha} \ket{t} \in G$ for any $0 \leq \alpha \leq 1$.

Conversely, any $\ket{u} \in \text{span}(A_y \cup \overline{A}_y)^\perp$ has a non-trivial constraint with respect to every $\ket{s} \in A_y$. The best we can do is $\alpha \ket{s} + (1-\alpha)\ket{t} + \{\text{balancing terms}\}$. 

In particular, we have $\abs{a_s}^2 + \abs{a_u}^2 < 1$ where $a_s$ is the part of a Gaussian state in $\text{span}(A_y)$ and $a_u$ the part in $ \text{span}(A_y \cup \overline{A}_y)^\perp$.

\begin{proof}
    Suppose $A_y \cup \overline{A}_y$ does \textit{not} span $\mathcal{H}_n$. 
    
    Let $S = \left\{ \ket{s_i} \right\}$ be a basis for $A_y$, $T = \left\{ \ket{t_i} \right\}$ a basis for $\overline{A}_y$ and $U = \left\{ \ket{u_i} \right\}$ a basis for $\text{span}(A_y \cup \overline{A}_y)^\perp$.

    For any $\ket{t} \in \overline{A}_y$, with corresponding element $\ket{s} \in A_y$, note that 
    \begin{align*}
        \ket{\psi} &= \sqrt{\alpha}\ket{s} + \sqrt{1 - \alpha}e^{-i\phi} \ket{t} \\
        &= U\left(\sqrt{\alpha}\ket{0} + \sqrt{1 - \alpha}e^{-i\phi} X_i X_j\ket{0}\right) \in G
    \end{align*} 
    for any $0 \leq \alpha \leq 1,\, 0 \leq \phi < 2\pi$. 
    
    Then $\ket{y} \in M_G$ requires $\abs{\braket{\psi}{y}}^2 = \abs{\sqrt{\alpha} + \sqrt{1-\alpha}e^{-i\phi}\braket{t}{y}}^2 \leq 1$. 
    Taking $\alpha = 1/(1 + \abs{\braket{t}{y}}^2)$ and $\phi =\text{phase}(\braket{t}{y})$ we see that $\braket{t}{y} = 0$.
    
    Consider $\ket{y_\pm} = \ket{y} \pm \epsilon \ket{u_1}$. We show that these points are feasible for some $\epsilon > 0$, and so $\ket{y}$ is a proper convex combination of feasible points.
    
    First, for any $\ket{\psi} \in G$, first note that $\braket{\psi}{u_1} = 0 \implies \abs{\braket{\psi}{y_\pm}}^2 = \abs{\braket{\psi}{y}}^2 \leq 1$.

    For $\braket{\psi}{u_1} \neq 0$, write $\ket{\psi} = a_s \ket{s} + a_t \ket{t} + a_u \ket{u}$ where $\ket{s} \in \text{span}(A_y)$ etc. Then, noting $\braket{s}{y} = 1,\, \braket{t}{y} = 0$ and $\braket{s}{u} = \braket{t}{u} = 0$, we have:
    \begin{align*}
        \abs{\braket{\psi}{y_\pm}}^2 &= \abs{a_s + a_u \braket{u}{y} \pm \epsilon a_u \braket{u}{u_1}}^2\\
        &\leq \abs{a_s}^2 + \abs{a_u}^2(\abs{\braket{u}{y}}^2 + \epsilon^2 \abs{\braket{u}{u_1}}^2)
    \end{align*}

    If we also write $\ket{y} = b_{s'} \ket{s'} + b_{t'}\ket{t'} + b_{u'} \ket{u'}$, it is clear that $\sup_{\ket{u}}(\abs{\braket{u}{y}}^2) = \abs{b_{u'}}^2$. Moreover, $\abs{b_{u'}} < 1$ else $\braket{u'}{y} = 1$, implying $\ket{u'} \in A_y \cap \text{span}(A_y \cup \overline{A}_y)^\perp$, which is impossible.

    Noting $\abs{\braket{u}{u_1}}^2 \leq 1$, we can therefore take $0 < \epsilon < \sqrt{1 - \abs{b_u}^2}$ to obtain
    $$ \abs{\braket{\psi}{y_\pm}}^2 < \abs{a_s}^2 + \abs{a_u}^2 < 1$$
    So we have $F_G(\ket{y_\pm}) \leq 1$. Thus these are feasible points, and the proper convex combination $\ket{y} = \frac{1}{2}(\ket{y_+} + \ket{y_-})$ implies $\ket{y} \notin E$. 
\end{proof}

\begin{lem} \label{lem:cone_strict_subset}
    For distinct $\ket{y}, \, \ket{y'} \in E$,
    $C_y \not\subset C_{y'}$, where the subset is strict.
\end{lem}

\begin{proof}    
    Define bases for the sets $A_y$ and $A_{y'}$:
    $$
    S_y \coloneqq \left\{ \ket{s_i} : i = 1,\ldots,m \right\} \subset \left\{ \ket{s_i} : i = 1,\ldots,M \right\} \eqqcolon S_y'
    $$
    where $M > m$. Since $\ket{y} \in E$, we must have $\text{span}(A_y)\, \oplus \, \text{span}(A_{y'}) = \text{span}(A_y \cup \overline{A}_y) = \mathcal{H}_n$.

    Consider $\ket{s_M} \in S_{y'} \setminus S_y$. We may write $\ket{s_M} = \ket{s} + \ket{t}$ for $\ket{s} \in \text{span}(A_y),\, \ket{t} \in \text{span}(\overline{A}_y)$. As in the proof of Lemma \ref{lem:extreme_span}, we have that $\braket{t}{y} = 0$. Noting that $\overline{A}_y \subseteq \overline{A}_{y'}$, we also have $\braket{t}{y'} = 0$. Also, since $A_y \subset A_{y'}$, we have $\braket{y}{s} = \braket{y'}{s}$.

    Then since $\ket{s_M} \in A_{y'}$, $1 = \braket{y'}{s_M} = \braket{y'}{s} + \braket{y'}{t} = \braket{y'}{s}$. But then $\braket{y}{s_M} = \braket{y}{s} + \braket{y}{t} = \braket{y}{s} = \braket{y'}{s} = 1$, which implies $\ket{s_M} \in A_y$, which is a contradiction.
\end{proof}

\begin{lem}\label{lem:cone_equal}
    For distinct $\ket{y},\,\ket{y'} \in E$, $C_y \neq C_{y'}$.
\end{lem}

\begin{proof}
    We have $\text{span}(A_y)\oplus \,\text{span}(\overline{A}_y) = \mathcal{H}_n$. We also have $1 = \braket{s}{y} \ \forall \, \ket{s} \in A_y$ and $0 = \braket{t}{y} \ \forall \, \ket{t} \in \overline{A}_y$. This system of equations uniquely determines $\ket{y}$. In particular:
    \begin{equation*}
         C_y = C_{y'} \implies A_y = A_{y'} \implies \overline{A}_y = \overline{A}_{y'} \implies y = y' \qedhere 
    \end{equation*}
\end{proof}

\begin{proof}[Proof of Proposition \ref{prop:unique_cone}]
    Let $\ket{\Psi} \in C_y$.
    By Lemmas \ref{lem:cone_strict_subset} \& \ref{lem:cone_equal}, we have $C_y \nsubseteq C_{y'}$. So $\exists \, \ket{z} \in C_y \setminus C_{y'}$. Then by the definition of relative interior for convex sets, we have
    $$
        \exists \, \ket{w} \in C_y, \, 0 < \lambda < 1 \quad \text{s.t.} \quad \ket{\Psi} = \lambda \ket{w} + (1-\lambda)\ket{z}
    $$
    We then have
    \begin{align*}
        \braket{ \Psi}{  y' }^R
        &= \lambda \braket{ w}{ y' }^R 
        + (1-\lambda) \braket{ z}{  y' }^R \\
        &< \lambda \braket{ w}{  y }^R 
        + (1-\lambda) \braket{ z}{  y }^R \\
        &= \braket{ \Psi}{  y }^R
    \end{align*}
    where the second line uses the facts
    \begin{align*}
        \braket{ z}{ y' }^R 
        &< \braket{ z}{ y }^R \\
        \braket{ w}{  y' }^R
        &\leq \braket{ w}{ y }^R
    \end{align*}
    since $\ket{w} \in C_y,\, \ket{z} \in  C_y \setminus C_{y'}$.
    Therefore, $\ket{\Psi} \notin C_{y'}$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Symmetry-restricted Gaussian rank of 2 copies of a magic state}\label{sec:app.rank}

We give a sketch proof of Proposition \ref{prop:rank} to give a flavour of our proof techniques.
We consider several cases, finding a contradiction in each case.
Recall that the equation we seek to solve is
\begin{equation} \label{eq:gaussian_decomp}
    \ket{M}\ket{M} = \frac{1}{2}(
        \ket{0} + \ket{15} + \ket{240} + \ket{255}
    )
    = \sum_{i = 1}^3 \sum_k a^i_k \ket{k}
\end{equation}
where the $\ket{k}$ are invariant under $Z_1 Z_2$ and $Z_7 Z_8$.

\subsection{\texorpdfstring{$a_0^i \neq 0,\,i=1,\,2,\,3$}{}}
Consider as illustrative examples the two equations each which are enforced on $\aa{15}{3}$ and $\aa{51}{3}$:
\begin{align}
    \aa{15}{3} = (\frac{1}{2}-\aa{15}{1}-\aa{15}{2}) &= \frac{\aa{12}{3} \aa{3}{3}}{\aa{0}{3}} \nonumber\\
    \implies \frac{1}{2}\aa{0}{3} &= 
    \frac{\aa{3}{1}}{\aa{0}{1}}(\frac{1}{2}\aa{12}{1} - \aa{12}{1}\aa{0}{2} + \aa{12}{2}\aa{0}{1}) +
    \frac{\aa{3}{2}}{\aa{0}{2}}(\frac{1}{2}\aa{12}{2} - \aa{12}{2}\aa{0}{1} + \aa{12}{1}\aa{0}{2}) \label{eq:a_15}\\
    \vspace{12pt} \nonumber
    \\
    \aa{51}{3} = -(\aa{51}{1}+\aa{51}{2}) &= \frac{\aa{48}{3} \aa{3}{3}}{\aa{0}{3}} \nonumber\\
    \implies 0 &= 
    \frac{\aa{3}{1}}{\aa{0}{1}}(\frac{1}{2}\aa{48}{1} - \aa{48}{1}\aa{0}{2} + \aa{48}{2}\aa{0}{1}) +
    \frac{\aa{3}{2}}{\aa{0}{2}}(\frac{1}{2}\aa{48}{2} - \aa{48}{2}\aa{0}{1} + \aa{48}{1}\aa{0}{2}) \label{eq:a_51}
\end{align}
Equations \eqref{eq:a_15} \& \eqref{eq:a_51} are structurally very similar. 
Define a function which captures the terms in brackets:
$$
f^{i}_{j} \coloneqq \frac{1}{2}\aa{j}{i} +(-1)^i (\aa{0}{1}\aa{j}{1} - \aa{0}{2}\aa{j}{0})
$$
Then proceeding similarly for each other term in the body of Table \ref{tab:constraints_0}, we obtain the matrix equation:
\begin{gather}
    \begin{pmatrix}
    A \\ A'
    \end{pmatrix}
    \begin{pmatrix}
    X & X'
    \end{pmatrix}
    =
    \begin{pmatrix}
    \frac{1}{2}\aa{0}{3} I_3 & 0 \\
    0 & 0
    \end{pmatrix} \label{eq:matrix_0}
\end{gather}
\begin{alignat*}{2}
    A & = \begin{pmatrix}
        f^0_{12} & f^1_{12} \\
        f^0_{48} & f^1_{48} \\
        f^0_{60} & f^1_{60}
    \end{pmatrix} \quad
    & A' &= \begin{pmatrix}
        f^0_{20} & f^1_{20} \\
        f^0_{24} & f^1_{24} \\
        f^0_{36} & f^1_{36} \\
        f^0_{40} & f^1_{40} 
    \end{pmatrix}
    \\
    X &= \begin{pmatrix}
        \frac{\aa{3}{1}}{\aa{0}{1}} & \frac{\aa{192}{1}}{\aa{0}{1}} & \frac{\aa{195}{1}}{\aa{0}{1}} \\
        \frac{\aa{3}{2}}{\aa{0}{2}} & \frac{\aa{192}{2}}{\aa{0}{2}} & \frac{\aa{195}{2}}{\aa{0}{2}}
    \end{pmatrix} \quad
    & X' &= \begin{pmatrix}
        \frac{\aa{65}{1}}{\aa{0}{1}} & \frac{\aa{66}{1}}{\aa{0}{1}} & \frac{\aa{129}{1}}{\aa{0}{1}} & \frac{\aa{130}{1}}{\aa{0}{1}} \\
        \frac{\aa{65}{2}}{\aa{0}{2}} & \frac{\aa{66}{2}}{\aa{0}{2}} & \frac{\aa{129}{2}}{\aa{0}{2}} & \frac{\aa{130}{1}}{\aa{0}{1}}
    \end{pmatrix}
\end{alignat*}
Note that
$\text{rank}(AX) \leq \min(\text{rank}(A),\text{rank}(X))\leq 2$,
whereas 
$\text{rank}(\aa{0}{3} I_3) = 3$
since by assumption $\aa{0}{3} \neq 0$. Thus there is no solution to \eqref{eq:matrix_0}.

\subsection{\texorpdfstring{$a_0^2 = 0, \ a_0^i \neq 0$, $i = 0, \, 1$}{}}
By symmetry, without loss of generality choose to delete the first column of Table \ref{tab:constraints_0} for $\ket{\psi_3}$, i.e. 
$$\aa{12}{3} = \aa{20}{3} = \aa{24}{3} = \aa{36}{3} = \aa{40}{3} =\aa{48}{3} = \aa{60}{3} = 0$$
Then also note that, by Equation \eqref{eq:gaussian_decomp},
$
    \aa{j}{2} = - \aa{j}{1} \ \ j = 12, \,20, \,24, \,36, \,40, \,48, \,60
$.

Then our special-case constraint gives:
\begin{equation} \label{eq:a_60}
    \aa{60}{1} = \frac{\aa{12}{1}\aa{48}{1} - \aa{20}{1}\aa{40}{1} + \aa{24}{1}\aa{36}{1}}{\aa{0}{1}} =
    - \frac{\aa{12}{2}\aa{48}{2} - \aa{20}{2}\aa{40}{2} + \aa{24}{2}\aa{36}{2}}{\aa{0}{2}} = -\aa{60}{2}
\end{equation}
Then either $\aa{0}{2} = -\aa{0}{1}$ or $\aa{60}{1} = \aa{60}{2} = 0$.

In the first case, we immediately have $\sum_i \aa{0}{i}  =0$, which is not possible by Equation \eqref{eq:gaussian_decomp}.

In the second case, since $\aa{0}{1},\,\aa{0}{2} \neq 0$, we have 
\begin{align*}
    \implies \aa{j}{1} = \aa{j}{2} = 0, \, j &= 63, 125, \, 126, \, 189, \, 190,\, 252, \, 255 \ \  
    (\text{since} \ \aa{j}{1} \propto \aa{60}{1}, \aa{j}{2} \propto \aa{60}{2}) \\
    \implies \aa{j}{3} = 0, \, j &= 63, 125, \, 126, \, 189, \, 190,\, 252 \ \ (\text{since} \ \sum_i\aa{j}{i} = 0)
\end{align*}
Consider $\aa{255}{3}$. Since $\aa{255}{1} = \aa{255}{2} = 0$ we must have $\aa{255}{3} \neq 0$.
Since the rest of the row of 255 is 0 for $\ket{\psi_3}$, it must be the case that only the last column is non-zero for $\ket{\psi_3}$.

Finally, note that 
\begin{align*}
    \sum_i \aa{15}{i} = \aa{12}{0}\left(
        \frac{\aa{3}{0}}{\aa{0}{0}} 
        - \frac{\aa{3}{1}}{\aa{0}{1}}
    \right) \neq 0 
    \\
    \sum_i \aa{51}{i} = \aa{48}{0}\left(
        \frac{\aa{3}{0}}{\aa{0}{0}} 
        - \frac{\aa{3}{1}}{\aa{0}{1}}
    \right) = 0
\end{align*}
Clearly we must have $\aa{48}{0} = \aa{48}{1} = 0 $.
But then $\aa{240}{0} = \aa{240}{1} = 0$, so $\sum_i \aa{240}{i} = 0$, contradicting Equation \eqref{eq:gaussian_decomp}.
\subsection{\texorpdfstring{$a_0^1 = a_0^2 = 0, \ a_0^0 \neq 0$}{}}
This case spawns several sub-cases depending on which column of Table \ref{tab:constraints_0} we delete for each of $\ket{\psi_2}$ \& $\ket{\psi_3}$. 

In each case, the reasoning follows much as the above, and we exclude the details.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Code repository} \label{sec:app.code}
Code is available on GitHub \href{https://github.com/JoshCudby/GaussianDecomposition}{here}.
\end{document}