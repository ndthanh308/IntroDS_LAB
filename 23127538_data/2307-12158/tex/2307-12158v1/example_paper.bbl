\begin{thebibliography}{25}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Akrour et~al.(2011)Akrour, Schoenauer, and
  Sebag]{10.1007/978-3-642-23780-5_11}
Akrour, R., Schoenauer, M., and Sebag, M.
\newblock Preference-based policy learning.
\newblock In Gunopulos, D., Hofmann, T., Malerba, D., and Vazirgiannis, M.
  (eds.), \emph{Machine Learning and Knowledge Discovery in Databases}, pp.\
  12--27, Berlin, Heidelberg, 2011. Springer Berlin Heidelberg.
\newblock ISBN 978-3-642-23780-5.

\bibitem[Argall et~al.(2009)Argall, Chernova, Veloso, and
  Browning]{argall2009survey}
Argall, B.~D., Chernova, S., Veloso, M., and Browning, B.
\newblock A survey of robot learning from demonstration.
\newblock \emph{Robotics and autonomous systems}, 57\penalty0 (5):\penalty0
  469--483, 2009.

\bibitem[Baker et~al.(2022)Baker, Akkaya, Zhokov, Huizinga, Tang, Ecoffet,
  Houghton, Sampedro, and Clune]{baker2022video}
Baker, B., Akkaya, I., Zhokov, P., Huizinga, J., Tang, J., Ecoffet, A.,
  Houghton, B., Sampedro, R., and Clune, J.
\newblock Video pretraining ({VPT}): Learning to act by watching unlabeled
  online videos.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 24639--24654, 2022.

\bibitem[Basu et~al.(2017)Basu, Yang, Hungerman, Singhal, and
  Dragan]{basu2017you}
Basu, C., Yang, Q., Hungerman, D., Singhal, M., and Dragan, A.~D.
\newblock Do you want your autonomous car to drive like you?
\newblock In \emph{Proceedings of the 2017 ACM/IEEE International Conference on
  Human-Robot Interaction}, pp.\  417--425, 2017.

\bibitem[Brown et~al.(2019)Brown, Goo, Nagarajan, and
  Niekum]{brown2019extrapolating}
Brown, D., Goo, W., Nagarajan, P., and Niekum, S.
\newblock Extrapolating beyond suboptimal demonstrations via inverse
  reinforcement learning from observations.
\newblock In \emph{International conference on machine learning}, pp.\
  783--792. PMLR, 2019.

\bibitem[Brown et~al.(2020{\natexlab{a}})Brown, Coleman, Srinivasan, and
  Niekum]{brown2020safe}
Brown, D., Coleman, R., Srinivasan, R., and Niekum, S.
\newblock Safe imitation learning via fast {B}ayesian reward inference from
  preferences.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1165--1177. PMLR, 2020{\natexlab{a}}.

\bibitem[Brown et~al.(2020{\natexlab{b}})Brown, Goo, and
  Niekum]{brown2020better}
Brown, D.~S., Goo, W., and Niekum, S.
\newblock Better-than-demonstrator imitation learning via automatically-ranked
  demonstrations.
\newblock In \emph{Conference on robot learning}, pp.\  330--359. PMLR,
  2020{\natexlab{b}}.

\bibitem[Bıyık et~al.(2020)Bıyık, Palan, Landolfi, Losey, Sadigh,
  et~al.]{erdem2020asking}
Bıyık, E., Palan, M., Landolfi, N.~C., Losey, D.~P., Sadigh, D., et~al.
\newblock Asking easy questions: A user-friendly approach to active reward
  learning.
\newblock In \emph{Conference on Robot Learning}, pp.\  1177--1190. PMLR, 2020.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and
  Amodei]{christiano2017deep}
Christiano, P.~F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D.
\newblock Deep reinforcement learning from human preferences.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Goecks et~al.(2021)Goecks, Waytowich, Watkins, and
  Prakash]{goecks2021combining}
Goecks, V.~G., Waytowich, N., Watkins, D., and Prakash, B.
\newblock Combining learning from human feedback and knowledge engineering to
  solve hierarchical tasks in minecraft.
\newblock \emph{arXiv preprint arXiv:2112.03482}, 2021.

\bibitem[Guss et~al.(2019)Guss, Houghton, Topin, Wang, Codel, Veloso, and
  Salakhutdinov]{guss2019minerl}
Guss, W.~H., Houghton, B., Topin, N., Wang, P., Codel, C., Veloso, M., and
  Salakhutdinov, R.
\newblock {MineRL}: a large-scale dataset of {M}inecraft demonstrations.
\newblock In \emph{Proceedings of the 28th International Joint Conference on
  Artificial Intelligence}, pp.\  2442--2448, 2019.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International conference on machine learning}, pp.\
  1861--1870. PMLR, 2018.

\bibitem[Ibarz et~al.(2018)Ibarz, Leike, Pohlen, Irving, Legg, and
  Amodei]{ibarz2018reward}
Ibarz, B., Leike, J., Pohlen, T., Irving, G., Legg, S., and Amodei, D.
\newblock Reward learning from human preferences and demonstrations in {A}tari.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Joachims et~al.(2017)Joachims, Granka, Pan, Hembrooke, and
  Gay]{joachims2017accurately}
Joachims, T., Granka, L., Pan, B., Hembrooke, H., and Gay, G.
\newblock Accurately interpreting clickthrough data as implicit feedback.
\newblock In \emph{Acm Sigir Forum}, volume~51, pp.\  4--11. Acm New York, NY,
  USA, 2017.

\bibitem[Johnson et~al.(2016)Johnson, Hofmann, Hutton, and
  Bignell]{10.5555/3061053.3061259}
Johnson, M., Hofmann, K., Hutton, T., and Bignell, D.
\newblock The {M}almo platform for artificial intelligence experimentation.
\newblock In \emph{Proceedings of the Twenty-Fifth International Joint
  Conference on Artificial Intelligence}, IJCAI'16, pp.\  4246–4247. AAAI
  Press, 2016.
\newblock ISBN 9781577357704.

\bibitem[Lee et~al.(2021)Lee, Smith, and Abbeel]{lee2021pebble}
Lee, K., Smith, L.~M., and Abbeel, P.
\newblock {PEBBLE}: Feedback-efficient interactive reinforcement learning via
  relabeling experience and unsupervised pre-training.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6152--6163. PMLR, 2021.

\bibitem[Milani et~al.(2023)Milani, Kanervisto, Ramanauskas, Schulhoff,
  Houghton, Mohanty, Galbraith, Chen, Song, Zhou, et~al.]{milani2023towards}
Milani, S., Kanervisto, A., Ramanauskas, K., Schulhoff, S., Houghton, B.,
  Mohanty, S., Galbraith, B., Chen, K., Song, Y., Zhou, T., et~al.
\newblock Towards solving fuzzy tasks with human feedback: A retrospective of
  the {MineRL BASALT} 2022 competition.
\newblock \emph{arXiv preprint arXiv:2303.13512}, 2023.

\bibitem[{MineRL documentation}()]{minerltreechop}
{MineRL documentation}.
\newblock {MineRLTreechop-v0} environment.
\newblock \url{https://minerl.io/docs/environments/#minerltreechop-v0}.
\newblock Accessed: 2023-06-15.

\bibitem[Reddy et~al.(2020)Reddy, Dragan, and Levine]{reddy2020sqil}
Reddy, S., Dragan, A.~D., and Levine, S.
\newblock {SQIL}: Imitation learning via reinforcement learning with sparse
  rewards.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Sadigh et~al.(2017)Sadigh, Dragan, Sastry, and
  Seshia]{sadigh2017active}
Sadigh, D., Dragan, A.~D., Sastry, S., and Seshia, S.~A.
\newblock Active preference-based learning of reward functions.
\newblock In \emph{Robotics: Science and Systems}, 2017.

\bibitem[Shah et~al.(2021)Shah, Wild, Wang, Alex, Houghton, Guss, Mohanty,
  Kanervisto, Milani, Topin, et~al.]{shah2021minerl}
Shah, R., Wild, C., Wang, S.~H., Alex, N., Houghton, B., Guss, W., Mohanty, S.,
  Kanervisto, A., Milani, S., Topin, N., et~al.
\newblock The {MineRL BASALT} competition on learning from human feedback.
\newblock \emph{arXiv preprint arXiv:2107.01969}, 2021.

\bibitem[Shah et~al.(2022)Shah, Wang, Wild, Milani, Kanervisto, Goecks,
  Waytowich, Watkins-Valls, Prakash, Mills, et~al.]{shah2022retrospective}
Shah, R., Wang, S.~H., Wild, C., Milani, S., Kanervisto, A., Goecks, V.~G.,
  Waytowich, N., Watkins-Valls, D., Prakash, B., Mills, E., et~al.
\newblock Retrospective on the 2021 {MineRL BASALT} competition on learning
  from human feedback.
\newblock In \emph{NeurIPS 2021 Competitions and Demonstrations Track}, pp.\
  259--272. PMLR, 2022.

\bibitem[Sui et~al.(2018)Sui, Zoghi, Hofmann, and Yue]{sui2018advancements}
Sui, Y., Zoghi, M., Hofmann, K., and Yue, Y.
\newblock Advancements in dueling bandits.
\newblock In \emph{Proceedings of the 27th International Joint Conference on
  Artificial Intelligence}, pp.\  5502--5510, 2018.

\bibitem[Van~Hasselt et~al.(2016)Van~Hasselt, Guez, and Silver]{van2016deep}
Van~Hasselt, H., Guez, A., and Silver, D.
\newblock Deep reinforcement learning with double q-learning.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~30, 2016.

\bibitem[Yarats et~al.(2021)Yarats, Zhang, Kostrikov, Amos, Pineau, and
  Fergus]{yarats2021improving}
Yarats, D., Zhang, A., Kostrikov, I., Amos, B., Pineau, J., and Fergus, R.
\newblock Improving sample efficiency in model-free reinforcement learning from
  images.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pp.\  10674--10681, 2021.

\end{thebibliography}
