\documentclass[sigconf]{acmart}

\sloppy

\usepackage{multirow}
\usepackage{graphicx,balance}
\usepackage{booktabs}

\usepackage{enumitem}

% for drawing figures
\usepackage{subfigure}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{tikz-qtree}

\usetikzlibrary{matrix}
\usetikzlibrary{patterns}
\usepgfplotslibrary{groupplots}

\definecolor{1}{RGB}{102,102,255}
\definecolor{2}{RGB}{102,206,245}
\definecolor{3}{RGB}{255,247,102}
\definecolor{4}{RGB}{255,178,102}
\definecolor{11}{RGB}{146,209,79}
\definecolor{5}{RGB}{194,209,163}
\definecolor{6}{RGB}{102,163,255}
\definecolor{7}{RGB}{163,223,196}
\definecolor{8}{RGB}{194,163,163}
\definecolor{9}{RGB}{194,102,163}
\definecolor{10}{RGB}{255,102,102}
\definecolor{red1}{RGB}{199,21,133}
\definecolor{red2}{RGB}{255,102,102}
\definecolor{red3}{RGB}{255,182,193}
\definecolor{blue1}{RGB}{25,25,112}
\definecolor{blue2}{RGB}{30,144,255}
\definecolor{blue3}{RGB}{135,206,250}

\usepackage{color}
\newcommand{\red}[1]{\textcolor{red}{#1}}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\copyrightyear{2023}
\acmYear{2023}
\setcopyright{acmlicensed}
\acmConference[RecSys '23]{Seventeenth ACM Conference on Recommender Systems}{September 18--22, 2023}{Singapore, Singapore}
\acmBooktitle{Seventeenth ACM Conference on Recommender Systems (RecSys '23), September 18--22, 2023, Singapore, Singapore}
\acmPrice{15.00}
\acmDOI{10.1145/3604915.3608802}
\acmISBN{979-8-4007-0241-9/23/09}

\begin{document}

\title[Domain Disentanglement with Interpolative Data Augmentation for Dual-Target CDR]{Domain Disentanglement with Interpolative Data Augmentation for Dual-Target Cross-Domain Recommendation}

\author{Jiajie Zhu}
\email{jiajie.zhu1@students.mq.edu.au}
\orcid{0000-0001-8673-1477}
\affiliation{%
  \institution{Macquarie University}
  \country{Australia}
}

\author{Yan Wang}
\authornote{Corresponding author.}
\email{yan.wang@mq.edu.au}
\orcid{0000-0002-5344-1884}
\affiliation{%
  \institution{Macquarie University}
  \country{Australia}
}

\author{Feng Zhu}
\email{zhufeng.zhu@antgroup.com}
\orcid{0000-0003-4200-0423}
\affiliation{%
  \institution{Ant Group}
  \country{China}}


\author{Zhu Sun}
\email{sunzhuntu@gmail.com}
\orcid{0000-0002-3350-7022}
\affiliation{
  \institution{Institute of High Performance Computing and Centre for Frontier AI Research, A*STAR}
  \country{Singapore}}

\begin{abstract}
  The conventional single-target Cross-Domain Recommendation (CDR) aims to improve the recommendation performance on a sparser target domain by transferring the knowledge from a source domain that contains relatively richer information. By contrast, in recent years, dual-target CDR has been proposed to improve the recommendation performance on both domains simultaneously. However, to this end, there are two challenges in dual-target CDR: (1) how to generate both relevant and diverse augmented user representations, and (2) how to effectively decouple domain-independent information from domain-specific information, in addition to domain-shared information, to capture comprehensive user preferences. To address the above two challenges, we propose a \textbf{D}isentanglement-based framework with \textbf{I}nterpolative \textbf{D}ata \textbf{A}ugmentation for dual-target \textbf{C}ross-\textbf{D}omain \textbf{R}ecommendation, called DIDA-CDR. In DIDA-CDR, we first propose an interpolative data augmentation approach to generating both relevant and diverse augmented user representations to augment sparser domain and explore potential user preferences. We then propose a disentanglement module to effectively decouple domain-specific and domain-independent information to capture comprehensive user preferences. Both steps significantly contribute to capturing more comprehensive user preferences, thereby improving the recommendation performance on each domain. Extensive experiments conducted on five real-world datasets show the significant superiority of DIDA-CDR over the state-of-the-art methods.
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002951.10003317.10003347.10003350</concept_id>
<concept_desc>Information systems~Recommender systems</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010257.10010293.10010294</concept_id>
<concept_desc>Computing methodologies~Neural networks</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Recommender systems}
\ccsdesc[500]{Computing methodologies~Neural networks}

\keywords{Cross-Domain Recommendation, Data Augmentation, Disentangled Representation Learning}

% \received{21 April 2023}
% \received[revised]{21 June 2023}
% \received[accepted]{21 June 2023}

\maketitle

\section{Introduction}
To alleviate the data sparsity problem, Cross-Domain Recommendation (CDR) \cite{loni2014cross} aims to employ abundant information from a relatively richer domain to improve recommendation performance on a sparser domain, forming the so-called \emph{single-target CDR} \cite{zhu2019dtcdr}. By contrast, in recent years, \emph{dual-target CDR} has been proposed to improve the recommendation performance on both domains simultaneously by sharing the common knowledge across domains \cite{ijcai2021p639,zang2021survey}.

% Figure environment removed

The existing dual-target CDR methods can be divided into two groups, i.e., (1) conventional methods, (2) disentanglement-based methods. \emph{Conventional methods} (see Fig. 1(a)) mainly utilize various transfer layers \cite{hu2018conet,li2020ddtcdr, zhao2019cross,liu2020cross} to integrate the representations learned by two base encoders in their respective domains. In contrast, \emph{disentanglement-based methods} (see Fig. 1(b)) tend to use the variational autoencoder (VAE) \cite{cao2022disencdr} or other disentangling techniques \cite{guo2023disentangled,zhang2022multi,cao2022cross} to decouple the domain-shared and domain-specific information, and only transfer the domain-shared information to each domain, which enhances the recommendation accuracy on both domains simultaneously. 

However, the existing dual-target CDR models have limitations in terms of effectively capturing comprehensive user preferences for the following reasons. Firstly, even though some of them use data augmentation to mitigate data imbalance between richer and sparser domains, few of them can balance the relevance (being able to represent user preferences in both domains) and diversity (having enough variations in user representations) of the user-item interaction augmentation. As a result, such augmented user representations can hardly provide strong support for subsequently capturing accurate and comprehensive user preferences. Secondly, none of the existing methods decouple all three essential components needed to capture comprehensive user preferences. To be specific, the existing methods only decouple two essential components, i.e.,  domain-shared and domain-specific information, and ignore the existence of domain-independent information. Since domain-independent information has different meanings from the other two types of information, it cannot be ignored when capturing comprehensive user preferences. It is worth mentioning here that some existing works also decouple so called ‘domain-independent information’, but its meaning is different from that in our work (cf. the detailed description in Section 2.1).

Below we introduce domain-shared, domain-specific and domain-independent information respectively with examples, and further differentiate them.

\begin{itemize}[leftmargin=*]
\item[(i)] \textbf{Domain-shared Information:} As shown in Fig. 1(c), there exists some \emph{domain-shared} information in both movie domain and music domain, such as ‘Category’. For example, people who like watching \emph{suspense movies} (i.e., a category in movie domain) tend to like listening to \emph{suspense music} (i.e., a category in music domain), and vice versa. Since the domain-shared information can provide the valuable information for cross-domain recommendations, it needs to be first decoupled and then transferred to both domains.
\item[(ii)] \textbf{Domain-specific Information:} In contrast, there also exists some \emph{domain-specific} information in each domain. For example, the user preference for pictures in a movie (i.e., ‘Frame’ in movie domain) is not applicable in music domain because ‘Frame’ is unique to movie domain. Thus, such domain-specific information should be decoupled too, which helps improve the recommendation performance on its own domain, but it should not be transferred to another domain in order to avoid the negative transfer \cite{zhu2021unified}. 
\item[(iii)] \textbf{Domain-independent Information:} In addition, some \emph{domain-independent} information also exists in each domain, but should not be transferred to other domains. For instance, ‘Rhythm’ exists in each of movie, music and book domains. However, in movie domain, it means the use of sound effects, the speed of camera cuts, and the changes in the pace of movie scenes, etc \cite{shirahama2004video}. In music domain, it means  , etc \cite{hauser2003evolution}. In book domain, it means the fluidity of the writing and the ups and downs of the storyline, etc \cite{gibbs2015writing}. In other words, although ‘Rhythm’ is seemingly common in all three domains, it has different meanings in different domains. Hence, such information is domain-independent and should be extracted from its own domain for capturing comprehensive user preferences, but should not be transferred to other domains.
\item[(iv)] \textbf{Difference:} Different to the domain-independent information, the domain-shared information (e.g., ‘Category’) extracted from two domains expresses the same meaning in each domain. In addition, the domain-specific information, e.g., ‘Frame’ in the movie domain, exists only in its own domain. By contrast, although the domain-independent information, e.g., ‘Rhythm’, exists in each domain, it has different meanings in different domains.

\item[(v)] \textbf{Summary:} Therefore, it is vital to recognize the existence of domain-independent information and to clearly differentiate it from domain-shared and domain-specific information. More importantly, decoupling domain-independent information is crucial for capturing more comprehensive user preferences, otherwise it will cause suboptimal recommendation results. However, existing dual-target CDR methods neglect the above insights. Hence, novel dual-target CDR solutions are needed to incorporate the above insights for capturing more comprehensive user preferences.
\end{itemize}

Following the above discussions, to target superior dual-target CDR, there are two major challenges as follows.

\noindent \textbf{CH1:} \emph{How to generate relevant and diverse augmented user representations to augment sparser domain and explore potential user preferences?} The existing dual-target CDR methods \cite{guo2023disentangled,xiao2023catcl} mainly obtain augmentation views by perturbing the original data. Although these methods can increase the diversity of augmentation by increasing the types of perturbations, they cannot generate augmented user representations that represent user preferences in both domains because they perform data augmentation on the data from each domain separately without considering the correlation between data from both domains, and thus can hardly capture the potential common user preferences in both domains.

\noindent \textbf{CH2:} \emph{How to effectively decouple domain-independent information from domain-specific information, in addition to domain-shared information, to capture comprehensive user preferences on each domain, thereby improving the recommendation performance on both domains?} The existing dual-target CDR methods either ignore decoupling the domain-specific and domain-shared information \cite{hu2018conet} (\emph{Group 1}), or directly transfer the domain-shared information to only fuse with the domain-specific information in each domain, overlooking the domain-independent information \cite{cao2022disencdr} (\emph{Group 2}). The methods in \emph{Group 1} (also see Fig. 1(a)) disregard the discrimination between domain-specific and domain-shared information, which may lead to the negative transfer. The methods in \emph{Group 2} (also see Fig. 1(b)) do not differentiate the domain-independent information from domain-specific information and decouple these two types of information, which results in suboptimal recommendation results.

\noindent \textbf{Our Approach and Contributions:} To address the above challenges, we propose a novel disentanglement-based framework with interpolative data augmentation for dual-target CDR. To the best of our knowledge, this is the first work in the literature that explicitly takes domain-independent information into consideration in addition to domain-shared and domain-specific information, and decouples it to capture more comprehensive user preferences for cross-domain recommendation. The characteristics and contributions of our proposed model are summarized as follows:
\begin{itemize}
    \item We first propose a \textbf{D}isentanglement-based framework with \textbf{I}nterpolative \textbf{D}ata \textbf{A}ugmentation for dual-target \textbf{C}ross-\textbf{D}omain \textbf{R}ecommendation, called DIDA-CDR, which can augment the sparser domain, disentangle three essential components of user preferences and transfer the domain-shared user preferences of common users across domains, thus enhancing the recommendation accuracy on both domains simultaneously;
    \item  To address \textbf{CH1}, we propose an interpolative data augmentation approach to generating both relevant and diverse augmented user representations, which augments the sparser domain and explores the potential common user preferences and therefore improves the recommendation performance on both domains;
    \item  To address \textbf{CH2}, we propose a disentanglement module to effectively decouple the domain-independent and domain-specific user preferences. The disentanglement module also extracts the domain-shared user preferences from augmented user representations, which can be transferred to both domains to provide the valuable information. We then apply the attention mechanism to combine the above three essential components of user preferences to capture more comprehensive user preferences in each domain, which can improve the recommendation performance on each of both domains;
    \item Extensive experiments conducted on five real-world datasets show that our proposed DIDA-CDR outperforms the best-performing state-of-the-art baselines with an average improvement of 8.54\% and 11.10\% with respect to HR@10 and NDCG@10, respectively.
\end{itemize}

\section{Related Work}
In this section, we first review the relevant literature on two major categories of CDR in Section 2.1. Next, since we utilize disentangled representation learning and interpolative data augmentation for our model, we also review the relevant literature on them in Section 2.2 and Section 2.3, respectively \cite{zhu2019dtcdr}.

\subsection{Single-Target and Dual-Target CDR}
\noindent \textbf{Single-Target CDR.} The existing single-target CDR methods can be divided into two categories, i.e., (1) content-based transfer methods and (2) feature-based transfer methods \cite{ijcai2021p639}. Content-based transfer methods \cite{kanagawa2019cross} mainly use the various content information, such as user/item attributes, tags, reviews, etc., to link domains and share their information across domains. By contrast, feature-based transfer methods \cite{hu2019transfer,fu2019deeply} aim to first get embeddings or rating patterns \cite{ijcai2019p587} using various learning techniques, and then transfer them across domains.

\noindent \textbf{Dual-Target CDR.} In contrast to single-target CDR, dual-target CDR aims to achieve better recommendation performance on both domains simultaneously, which can be extended to multiple domains, leading to multi-target CDR \cite{cui2020herograph,zhu2020graphical,guo2023disentangled}. The existing dual-target CDR approaches can be divided into two categories, i.e., (1) conventional approaches, (2) disentanglement-based approaches. For \emph{conventional dual-target CDR}, CoNet \cite{hu2018conet} utilizes cross-connection networks to achieve knowledge transfer between two domains. DDTCDR \cite{li2020ddtcdr} employs a latent orthogonal mapping function to transfer user embeddings across domains. PPGN \cite{zhao2019cross} constructs a cross-domain interaction graph to learn and transfer representations. BiTGCF \cite{liu2020cross} first leverages two base graph encoders to learn user/item embeddings, and then performs the feature propagation and transfer to fuse user embeddings. However, most of them lead to the negative transfer, because they neglect to decouple the domain-shared and domain-specific information.

\begin{table*}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\centering
\caption{Important notations.}
\label{tab:1}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Symbol} & \multicolumn{1}{c|}{\textbf{Definition}}                      & \textbf{Symbol} & \multicolumn{1}{c|}{\textbf{Definition}}                      \\ \hline
$k$           & the dimension of embedding matrix                                                 & $P$             & the predicted domain probability          \\ \hline
$m$         & the number of users                                             &  $O$             & the ground truth domain label                                    \\ \hline
$n$             & the number of items                             &      $\mathbf{E}_u$             & the graph embedding matrix of users                       \\ \hline
$R$            & the rating matrix                           & $\mathbf{E}_v$           & the graph embedding matrix of items               \\ \hline
$\mathcal{U}$           & the set of users                             & ${\mathbf{Z}_{sha}}$             & the domain-shared user preferences                      \\ \hline
$\mathcal{V}$           & the set of items                    & ${\mathbf{Z}_{spe}}$              & the domain-specific user preferences                    \\ \hline
$y$             & the user-item interaction & ${\mathbf{Z}_{ind}}$             & the domain-independent user preferences \\ \hline
${*^{aug}}$              & the notation for data augmentation                             & $\mathbf{E}_u^*$            & the comprehensive user preferences                           \\ \hline
${*^A}$, ${*^B}$           & \tabincell{c}{the notations for domains $A$ and $B$, e.g., ${n^A}$\\represents the number of items on domain $A$}                                 & $\mathbf{E}_u^{aug}$              & \tabincell{c}{the augmented user representations\\of common users}                                        \\ \hline
$\hat *$            & \tabincell{c}{the predicted notations, e.g., ${\hat y_{ij}}$ represents\\the predicted interaction of user ${u}_i$ on item ${v}_j$}                                                & $G$             & \tabincell{c}{the heterogeneous graph, where $Q$\\is the set of user-item relationships}                                \\ \hline
\end{tabular}%
\end{table*}

In addition, we further classify the existing \emph{disentanglement-based dual-target CDR approaches} into two classes, i.e., (1) VAE-based approaches and (2) other approaches, according to disentangling techniques. VAE-based approaches \cite{wang2022disentangled} (\emph{Class 1}) employ the explicit reconstruction loss included in the Evidence Lower Bound (ELBO) and extra regularizers as the disentanglement loss to learn desirable disentangled representations. Other approaches (\emph{Class 2}) often utilize Graph Convolutional Networks (GCNs) \cite{wang2022inter}, adversarial learning \cite{choi2022based,zhao2022multi}, self-supervised learning \cite{li2023one,guo2023disentangled,zhang2023disentangled} and fixed or flexible combination strategies \cite{zhu2019dtcdr,zhu2020graphical, xiao2023catcl} to disentangle the latent knowledge, such as domain-shared and domain-specific information. The domain-shared information is also termed as domain-invariant information in \cite{liu2019jscn,su2022cross}, domain-common information in \cite{choi2022based} and domain-independent information in \cite{liu2020exploiting,sahu2020knowledge} (note that the domain-independent information in other works does not have the same meaning as in our work). However, none of them can effectively capture comprehensive user preferences, because they entangle the domain-independent information with domain-specific information.

\subsection{Disentangled Representation Learning}
Disentangled representation learning is originally introduced in the field of computer vision \cite{chen2018isolating,gonzalez2018image} and is mainly used to learn visual features such as shape, color and location features of objects \cite{zhang2020content}. In addition to computer vision, in recent years, disentangled representation learning has also been used in recommender systems (RSs) \cite{ma2019learning,wang2020disentangled,ma2020disentangled}. The main idea of disentangled representation learning is to focus on decomposing the latent factors behind the observed instances in the low-dimension vector space \cite{bengio2013representation,wang2022disentangledsurvey}. For recommendation, MacridVAE \cite{ma2019learning} performs the macro disentanglement and micro disentanglement based on the user behavior data. DGCF \cite{wang2020disentangled} leverages a graph disentangling module to decouple user embeddings learned from user-item interaction data into fine-grained user intents. The work proposed in \cite{ma2020disentangled} reconstructs the embedding of future sequence by self-supervised learning to decouple the intentions of users.

\subsection{Interpolative Data Augmentation}
Mixup \cite{zhang2017mixup} has been recently proposed as a representative interpolative data augmentation method that enhances the prediction accuracy of networks for image classification. In addition to image classification, interpolative data augmentation has been used in recent RSs \cite{huang2021mixgcf,chen2022graph}. The general idea of interpolative data augmentation is to linearly interpolate inputs and model targets of random samples. For recommendation, existing methods with interpolative data augmentation \cite{bian2022relevant} tend to generate natural augmented user representations to model sequential user behaviors \cite{hou2022towards}.

\section{The Proposed Model}
In this section, first, we formulate the target problem of our proposed model. Next, we present a novel \textbf{D}isentanglement-based framework with \textbf{I}nterpolative \textbf{D}ata \textbf{A}ugmentation for dual-target \textbf{C}ross-\textbf{D}omain \textbf{R}ecommendation, called DIDA-CDR. Finally, we elaborate on the basic components of DIDA-CDR.

\subsection{Problem Formulation}
For the sake of better readability, we list the important notations of this paper in Table 1. This paper considers the dual-target CDR on two domains ${D^A}$ and ${D^B}$ with a shared user set, denoted by $\mathcal{U}$ (of size $m = |\mathcal{U}|$). The sets of items in ${D^A}$ and ${D^B}$ are defined as ${\mathcal{V}^A}$ (of size ${n^A} = |{\mathcal{V}^A}|$) and ${\mathcal{V}^B}$ (of size ${n^B} = |{\mathcal{V}^B}|$), respectively. Let ${\mathbf{R}^A} \in {\{ 0,1\} ^{m \times {n^A}}}$ and ${\mathbf{R}^B} \in {\{ 0,1\} ^{m \times {n^B}}}$ denote the binary user-item interaction matrices in ${D^A}$ and ${D^B}$, respectively. By aggregating the interaction data in each domain, we first construct two heterogeneous graphs ${G^A} = (\mathcal{U},{\mathcal{V}^A},{\mathcal{Q} ^A})$ and ${G^B} = (\mathcal{U},{\mathcal{V}^B},{\mathcal{Q} ^B})$ to learn user embeddings $\mathbf{E}_u^A$, $\mathbf{E}_u^B$ and item embeddings $\mathbf{E}_v^A$, $\mathbf{E}_v^B$ in domains ${D^A}$ and ${D^B}$, respectively, where ${\mathcal{Q} ^A}$ and ${\mathcal{Q} ^B}$ are the edge sets that represent the observed user-item interactions. By linearly interpolating the user embeddings $\mathbf{E}_u^A$ and $\mathbf{E}_u^B$, we then generate augmented user representations $\mathbf{E}_u^{aug}$ to augment the sparser domain ${D^B}$. Given two coarse user embeddings $\mathbf{E}_u^A$, $\mathbf{E}_u^B$ and the augmented user representations $\mathbf{E}_u^{aug}$, our goal is to disentangle domain-shared, domain-specific and domain-independent user preferences, i.e., ${\mathbf{Z}_{sha}}$, ${\mathbf{Z}_{spe}}$, and ${\mathbf{Z}_{ind}}$, and then transfer domain-shared user preferences ${\mathbf{Z}_{sha}}$ to each domain to capture comprehensive user preferences $\mathbf{E}_u^*$, thereby improving the recommendation performance on both domains.

% Figure environment removed

\subsection{Overview of DIDA-CDR}
To enhance the recommendation accuracy on both domains, we propose a novel \underline{d}isentanglement-based framework with \underline{i}nterpolative \underline{d}ata \underline{a}ugmentation for dual-target \underline{c}ross-\underline{d}omain \underline{r}ecommendation, called DIDA-CDR. This framework contains five major components, i.e., (1) \emph{Graph Convolution and Propagation Module}, (2) \emph{Interpolative Data Augmentation Module}, (3) \emph{User Preference Disentanglement Module}, (4) \emph{Information Fusion Module}, and (5) \emph{Prediction Module}. The details of each module of our proposed DIDA-CDR are illustrated in Fig. 2. They are briefly introduced below and described in detail in the following subsections.

\noindent \textbf{(1) Graph Convolution and Propagation.} First, we construct two heterogeneous graphs to extract the high-order user-item interaction relationships using the interaction data in domain $A$ and domain $B$, respectively. Based on the above graphs, we apply the graph convolution and propagation layer in the GCN \cite{KipfW17} to generate user and item embeddings.

\noindent \textbf{(2) Interpolative Data Augmentation.} Next, we propose an interpolative data augmentation approach to augmenting user embeddings at the representation level. The interpolative data augmentation approach linearly interpolates user embeddings in domain $A$ and domain $B$ to generate both relevant and diverse augmented user representations.

\noindent \textbf{(3) User Preference Disentanglement.} Thereafter, we propose a disentanglement module guided by a domain classifier to decouple more accurate domain-specific and domain-independent user preferences from user embeddings generated by GCNs. This module also disentangles the domain-shared user preferences from the augmented user representations, which are then transferred to both domains to provide the valuable information.

\noindent \textbf{(4) Information Fusion.} Next, we use three approaches, i.e., concatenation, element-wise sum, and attention mechanism to incorporate essential components of user preferences, i.e., domain-shared, domain-specific and domain-independent information, which are decoupled by the disentanglement module, to capture comprehensive user preferences.

\noindent \textbf{(5) Prediction.} Finally, we apply the multi-layer perception (MLP) to model the user-item interaction relationships, endowing the non-linearity to our proposed model. Based on the MLP, the predicted user-item interaction matrix can be obtained. The prediction loss between it and the observed user-item interaction matrix, together with two domain classification losses, constitute the final loss for training.

Overall, our model can be easily extended to a multi-target CDR model. Specifically, we can disentangle domain-shared user preferences and transfer them to all domains, decouple the domain-specific and domain-independent user preferences from each domain, and then capture the comprehensive user preferences to improve the recommendation performance on multiple domains simultaneously.

\subsection{Graph Convolution and Propagation}
GCNs excel at capturing the relationships between nodes and learning the representation of graph data, and are well suited for modeling user-item relationships in RSs, because the user-item interaction data can be easily transformed into the graph structure. To distill the high-order user-item interaction relationships, we construct two heterogeneous graphs ${G^A}$ and ${G^B}$ for domains $A$ and $B$, respectively, where nodes refer to entities (i.e., users and items) and edges refer to interactions. In this paper, we apply the graph convolution and propagation layer in the GCN to encode the user and item embeddings according to the user-item interaction matrix ${\mathbf{R}^A}$ (or ${\mathbf{R}^B}$). The node embeddings $\mathbf{E}_0^A$ (or $\mathbf{E}_0^B$) are randomly initialized. Given a graph ${G^A}$, the propagation rule is represented as:
\begin{equation}
\mathbf{E}_l^A = f({\mathbf{\tilde D}^{ - {\textstyle{1 \over 2}}}}{\mathbf{\tilde R}^A}{\mathbf{\tilde D}^{ - {\textstyle{1 \over 2}}}}\mathbf{E}_{l - 1}^A{\mathbf{W}_l} + {\mathbf{b}_l}),
\end{equation}
where ${\mathbf{\tilde R}^A} = {\mathbf{R}^A} + \mathbf{I}$ is the user-item interaction matrix of graph ${G^A}$ after adding a self-loop identity matrix $\mathbf{I}$. $\mathbf{\tilde D}$ is a degree matrix for normalization. ${\mathbf{W}_l}$ and ${\mathbf{b}_l}$ are the trainable weight matrix and bias vector in the ${l^{th}}$ layer respectively, and $\mathbf{E}_l^A$ is the hidden embedding matrix of graph ${G^A}$ in the ${l^{th}}$ layer \cite{meng2021graph}. $f( \cdot )$ denotes the ReLU activation function.

After $l$ times propagation, we can obtain the global hidden representations ${\mathbf{E}^A}$ by concatenating multiple embedding matrices from $\mathbf{E}_0^A$ to $\mathbf{E}_l^A$, which can be rearranged into the user embeddings $\mathbf{E}_u^A$ and item embeddings $\mathbf{E}_v^A$ in domain $A$ \cite{zhao2019cross}. Similarly, we can obtain the user embeddings $\mathbf{E}_u^B$ and item embeddings $\mathbf{E}_v^B$ in domain $B$.

\subsection{Interpolative Data Augmentation}
Although the user embeddings in both domains are obtained by GCNs, the user embeddings learned from the sparser domain are not as accurate as those learned from the richer domain due to the data imbalance between the two domains. To augment the sparser domain and explore the potential user preferences, inspired by the mixup technique \cite{zhang2017mixup}, we design an interpolative data augmentation approach, which generates both relevant and diverse augmented user representations. However, the conventional mixup technique cannot be directly utilized for our task, because the interaction data cannot be directly mixed at the pixel level like images. Therefore, we propose to linearly interpolate the embeddings of common users in both domains. The augmentation of common users ensures that augmented user representations maintain the relevance of user preferences in both domains, i.e., relevant augmented user representations, while corresponding to adding more interaction data for users in the sparser domain. In addition, introducing randomness in linear interpolation instead of using a fixed mixing coefficient can generate diverse augmented user representations, which can provide richer information for subsequent disentanglement. The formula for interpolative data augmentation can be expressed as follows:
\begin{equation}
\mathbf{E}_u^{aug} = \lambda \mathbf{E}_u^A + (1 - \lambda )\mathbf{E}_u^B,
\end{equation}
where $\mathbf{E}_u^{aug}$ denotes the augmented user representations\footnote{The two terms, i.e., embedding and representation, are exchangeable in this paper.}. Since we aim to generate more diverse augmented user representations through data augmentation, instead of using attention-based methods in this module, we propose the interpolative data augmentation approach. $\lambda  \in [0,1]$ is the mixing coefficient sampled from $Beta(\alpha ,\alpha )$, $\alpha  \in (0,\infty )$. The advantages of adopting a mixing coefficient sampled from $Beta(\alpha ,\alpha )$ are as follows. First, since the user embeddings of the common user in each of the two domains should be equivalent, the mixing coefficient should be sampled within the interval $[0,1]$ and be symmetric around 0.5. Beta distribution $Beta(\alpha ,\alpha )$ satisfies this characteristic. Second, it has been proven to effectively improve the generalization ability of the model \cite{wang2021mixup}. Meanwhile, $\lambda $ also introduces randomness into our model, thus weakening the negative transfer that may result from performing a linear interpolation operation with fixed weights (cf. the experimental results in Section 4.3 and the discussion of parameter $\alpha$ in Section 4.6.2).

\subsection{User Preference Disentanglement}
To capture domain comprehensive preferences of users, we utilize disentangled representation learning to extract the essential components of user preferences from previously obtained user embeddings. Inspired by the method introduced in \cite{fu2021meta}, we design a disentanglement module to decouple the domain-specific and domain-independent user preferences. Specifically, our disentanglement module adopts a similar architecture to the encoder of VAE, but it is quite different from VAE. In particular, VAE only encodes one latent feature, while our disentanglement module aims to better learn to decouple the domain-specific and domain-independent user preferences. Although both domain-specific and domain-independent user preferences cannot be transferred to other domains, they have different meanings, thus their importance in capturing comprehensive user preferences is different. If they are not distinguished, they are equally important in capturing comprehensive user preferences, which is inappropriate. By contrast, if they are decoupled, the subsequent information fusion module can use attention mechanism to learn their weights respectively, thereby capturing more accurate and comprehensive user preferences. This module can also disentangle the domain-shared user preferences from the augmented user representations. In this case, our model can not only extract the domain-shared preferences of common users in both domains, but also explore the domain-specific personalized user preferences and domain-independent user preferences, which enhances the comprehensiveness of capturing user preferences, and thus improves the recommendation performance on both domains. To this end, we first feed user embeddings in both domains (see methods introduced in Section 3.3) and augmented user representations (see methods introduced in Section 3.4) into this module and perform the following processing respectively.

Taking domain $A$ as an example, the user embeddings $\mathbf{E}_u^A$ are first entered into the disentanglement module, which consists of several fully connected (FC) layers. More specifically, the first FC layer (see navy blue FC layer in Fig. 2), followed by the ReLU activation function, is utilized to extract general representations. In addition, the subsequent FC layers (also see lake blue FC layers in Fig. 2) outputs two sets of latent vectors, each representing different mean and deviation information of the input user embeddings. Next, following the method introduced in \cite{fu2021meta}, a reparametrization trick is adopted to generate domain-independent $\mathbf{Z}_{ind}^A$ and domain-specific user preferences $\mathbf{Z}_{spe}^A$. Similarly, we can obtain the domain-independent $\mathbf{Z}_{ind}^B$ and domain-specific user preferences $\mathbf{Z}_{spe}^B$ in domain $B$, as well as the domain-shared $\mathbf{Z}_{sha}^{aug}$ and domain-specific user preferences $\mathbf{Z}_{spe}^{aug}$ decoupled from augmented user representations. 

To ensure that the above essential components of user preferences can be accurately disentangled, we introduce a domain classifier ${H_{cls}}( \cdot )$, which includes a single FC layer to predict the domain probability of user preferences, to guide the disentanglement process. In order to supervise the optimization process of disentanglement module, we further set two domain classification tasks to train our DIDA-CDR. First of all, the disentanglement module is guided to decouple the domain-specific information with stronger domain identification ability, i.e., more accurate domain-specific user preferences, by minimizing the domain classification loss ${\mathcal L_{cl{s_1}}}$. In other words, if the decoupled user preferences can be easily recognized by a domain classifier as belonging to a particular domain, then such user preferences are considered as the domain-specific information. By contrast, the domain-independent and domain-shared information are leveraged to confuse the domain classifier, i.e., to make the domain classifier unable to identify the domain to which they belong, to ensure that they can be distinguished from the domain-specific information. In other words, if the decoupled user preferences are no better than random guesses in identifying the domain to which they belong when fed into a domain classifier, then such user preferences are mutually exclusive with the domain-specific information and should be classified as the domain-independent information. When the input to the disentanglement module is the augmented user representations containing the user preferences from both domains, the above user preferences refer to the domain-shared information. Specifically, the domain classification loss ${\mathcal L_{cl{s_1}}}$ can be defined as follows:

\begin{equation}
\begin{split}
{\mathcal L_{cl{s_1}}} &= \frac{1}{3}\sum {[{\ell _1}({H_{cls}}(\mathbf{Z}_{spe}^A),O_{spe}^A)} \\
 &+ {\ell _1}({H_{cls}}(\mathbf{Z}_{spe}^B),O_{spe}^B)\\
 &+ \lambda  \cdot {\ell _1}({H_{cls}}(\mathbf{Z}_{spe}^{aug}),O_{spe}^{augA})\\
 &+ (1 - \lambda ) \cdot {\ell _1}({H_{cls}}(\mathbf{Z}_{spe}^{aug}),O_{spe}^{augB})],
\end{split}
\end{equation}
where ${\ell _1}(P,O)$ denotes the cross-entropy loss function. $P$ is the predicted domain probability of input user preferences and $O$ is the ground truth (GT) domain label. We define the corresponding GT for domain-specific user preferences $\mathbf{Z}_{spe}^A$, $\mathbf{Z}_{spe}^B$ and $\mathbf{Z}_{spe}^{aug}$ as $O_{spe}^A$, $O_{spe}^B$ and $O_{spe}^{aug}$, respectively. All the elements in $O_{spe}^A$ and $O_{spe}^{augA}$ are set to 1, while those in $O_{spe}^B$ and $O_{spe}^{augB}$ are set to 0. Here, $\lambda$ represents the confidence score that an augmented user representation belongs to its initial GT, since augmented user representations are generated by incorporating embeddings of common users in both domains with a mixing coefficient $\lambda $ \cite{fu2021meta}.

Similarly, we define the corresponding GT for domain-independent user preferences $\mathbf{Z}_{ind}^A$, $\mathbf{Z}_{ind}^B$ and domain-shared user preferences $\mathbf{Z}_{sha}^{aug}$ as $O_{ind}^A$, $O_{ind}^B$ and $O_{sha}^{aug}$, respectively. We set all the items in $O_{ind}^A$, $O_{ind}^B$ and $O_{sha}^{aug}$ as $[0.5,0.5]$ \cite{fu2022generalized}. This ensures that the learned domain-shared and domain-independent user preferences cannot be used to identify the domain to which they belong, and thus be distinguished from the domain-specific user preferences. Specifically, the loss function is represented as follows:
\begin{equation}
\begin{split}
{\mathcal L_{cl{s_2}}} &= \frac{1}{3}\sum {[{\ell _2}({H_{cls}}(\mathbf{Z}_{ind}^A),O_{ind}^A)} \\
 &+ {\ell _2}({H_{cls}}(\mathbf{Z}_{ind}^B),O_{ind}^B)\\
 &+ {\ell _2}({H_{cls}}(\mathbf{Z}_{sha}^{aug}),O_{sha}^{aug})],
\end{split}
\end{equation}
where ${\ell _2}(P,O)$ is the Kullback-Leibler divergence loss function.

\begin{table}[t]
\caption{Comparison of information fusion approaches \cite{sun2019multi}.}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c}
\toprule
\hline
                           & Formula \\ \hline
Concatenation              & $\mathbf{E}_u^* = [{\mathbf{Z}_{spe}},{\mathbf{Z}_{ind}},{\mathbf{Z}_{sha}}]$       \\ \hline
Element-wise sum           & $\mathbf{E}_u^* = {\mathbf{Z}_{spe}} + {\mathbf{Z}_{ind}} + {\mathbf{Z}_{sha}}$       \\ \hline
\multirow{2}{*}{Attention} & ${\mathbf{C}_u} = Softmax ({\mathbf{W}_s} \cdot \sigma ({\mathbf{W}_{spe}} \cdot {\mathbf{Z}_{spe}} + {\mathbf{W}_{ind}} \cdot {\mathbf{Z}_{ind}} + {\mathbf{W}_{sha}} \cdot {\mathbf{Z}_{sha}}))$      \\
                           & $\mathbf{E}_u^* = [{\mathbf{Z}_{spe}},{\mathbf{Z}_{ind}},{\mathbf{Z}_{sha}}] \cdot {\mathbf{C}_u}$       \\ \hline
\bottomrule
\end{tabular}
}
\end{table}

\subsection{Information Fusion}
The domain-shared, domain-specific and domain-independent information are three essential components of user preferences, which need to be integrated in a reasonable and efficient way to capture comprehensive user preferences. To this end, in this paper, we leverage three approaches, i.e., concatenation, element-wise sum, and attention mechanism, to aggregate individual representations into comprehensive user preferences \cite{sun2019multi}. The specific operations of these information fusion approaches are expressed in Table 2. (cf. the experimental comparison of them in Section 4.5).

\subsection{Model Prediction and Training}

After the information fusion, we obtain the comprehensive user preferences $\mathbf{E}_u^*$ and we also have the corresponding item embeddings $\mathbf{E}_v$ generated by GCNs. To give our model the non-linearity, we adopt a neural network, i.e., MLP, to represent the user-item interactions. Taking the domain $A$ as an example, the input user embeddings and item embeddings in domain $A$ for the MLP are defined as $\mathbf{S}_{in}^A = \mathbf{E}_u^{A*}$ and $\mathbf{T}_{in}^A = \mathbf{E}_v^A$, respectively. Moreover, the output embeddings of user ${u_i}$ and item ${v_j}$ of MLP is expressed as:
\begin{equation}
\mathbf{S}_i^A = \mathbf{S}_{ou{t_i}}^A = \delta( \ldots \delta(\delta(\mathbf{S}_{i{n_i}}^A \cdot W_{{\mathbf{S}_1}}^A) \cdot W_{{\mathbf{S}_2}}^A)),
\end{equation}
\begin{equation}
\mathbf{T}_i^A = \mathbf{T}_{ou{t_i}}^A = \delta( \ldots \delta(\delta(\mathbf{T}_{i{n_i}}^A \cdot W_{{\mathbf{T}_1}}^A) \cdot W_{{\mathbf{T}_2}}^A)),
\end{equation}
where $\delta( \cdot )$ is the LeakyReLU activation function. $\mathbf{W}_{{\mathbf{S}_1}}^A$, $\mathbf{W}_{{\mathbf{S}_2}}^A \ldots$ and $\mathbf{W}_{{\mathbf{T}_1}}^A$, $\mathbf{W}_{{\mathbf{T}_2}}^A \ldots$ denote the trainable weight matrices of MLP in various layers, respectively. 

Next, the predicted user-item interaction $\hat y_{ij}^A$ between user ${u_i}$ and item ${v_j}$ in domain $A$ can be formulated as follows:
\begin{equation}
\hat y_{ij}^A = cosine(\mathbf{S}_i^A,\mathbf{T}_j^A) = \frac{{\mathbf{S}_i^A \cdot \mathbf{T}_j^A}}{{\| \mathbf{S}_i^A \| \| \mathbf{T}_j^A \|}}.
\end{equation}
Furthermore, the prediction loss in domain $A$ is defined as follows:
\begin{equation}
{\mathcal L_{prd}^A} = \sum\limits_{y \in {\mathcal{Y}^{A+}} \cup {\mathcal{Y}^{A-}}} {\ell _1 (\hat y,y)}  + \gamma (\| {{\mathbf{S}^A}} \|_F^2 + \| {{\mathbf{T}^A}} \|_F^2),
\end{equation}
where ${\ell _1}(\hat y,y)$ is the cross-entropy loss function. $y$ denotes an observed user-item interaction, and $\hat y$ is the corresponding predicted user-item interaction. ${{\mathcal{Y}^{A+}}}$ denotes the set of observed interactions and ${{\mathcal{Y}^{A-}}}$ is a certain number of negative instances randomly sampled from the set of unseen interactions in domain $A$ to avoid over-fitting. $\| {{\mathbf{S}^A}} \|_F^2 + \| {{\mathbf{T}^A}} \|_F^2$ is a regularizer controlled by $\gamma$.

Finally, we utilize a multi-task learning mechanism consisting of a prediction task and two domain classification tasks to optimize our model in domain $A$. Specifically, the final loss function is formulated as follows:
\begin{equation}
{\mathcal L^A} = {\mathcal L_{prd}^A} +  {\mu _1} \cdot {\mathcal L_{cl{s_1}}} +  {\mu _2} \cdot {\mathcal L_{cl{s_2}}}.
\end{equation}
where ${\mu _1}$ and ${\mu _2}$ denote the weights of domain classification losses ${\mathcal L_{cl{s_1}}}$ and ${\mathcal L_{cl{s_2}}}$, respectively (cf. the discussion of parameters ${\mu _1}$ and ${\mu _2}$ in Section 4.6.3). Similarly, we perform the same optimization process for domain $B$.

\section{Experiments and Analysis}
In order to demonstrate the superiority of our proposed DIDA-CDR and explore the effectiveness of its various modules, we conduct extensive experiments on five real-world datasets to answer the following five research questions:

\begin{itemize}[leftmargin=*]
    \item \noindent \textbf{RQ1.} How does our model perform when compared to representative and state-of-the-art baseline models?
    \item \noindent \textbf{RQ2.} How do various modules (i.e., interpolative data augmentation and user preference disentanglement) affect the results of our model?
    \item \noindent \textbf{RQ3.} How do various components of user preferences (i.e., domain-shared, domain-specific and domain-independent information) contribute to the performance improvement of our model?
    \item \noindent \textbf{RQ4.} How do various information fusion approaches influence the performance of our models?
    \item \noindent \textbf{RQ5.} How does the performance of our model change with various values of hyper-parameters?
\end{itemize}

\subsection{Experimental Settings}
\subsubsection{\textbf{Experimental Datasets and Tasks.}}
In order to verify the recommendation performance of our proposed DIDA-CDR, we conduct extensive experiments on five real-world datasets, i.e., Douban subsets (Douban-Movie, Douban-Book and Douban-Music) released in GA-DTCDR \cite{zhu2020graphical} and Amazon subsets (Amazon-Elec and Amazon-Cloth) released in DisenCDR \cite{cao2022disencdr}. For these five datasets, we first convert the explicit ratings into implicit feedback, i.e., we binarize the ratings into 0 and 1 to indicate whether the user has interacted with the item or not. Following the methods introduced in \cite{liu2022exploiting,liu2020cross}, we then filter these datasets to remove users and items with less than 5 interactions. Since the two Amazon subsets are filtered out of the cold-start item entry in the test set, following DisenCDR \cite{cao2022disencdr}, we perform the same preprocessing operation on the three Douban subsets as well for a fair comparison. Finally, we divide the above five subsets into three pairs of datasets, extract the common users in each pair of datasets, and design three dual-target CDR tasks in a scenario where users completely overlap, which can be listed as follows:
\begin{itemize}
    \item \noindent \textbf{Task 1:} Douban-Movie (richer) $\leftrightarrow$ Douban-Book (sparser)
    \item \noindent \textbf{Task 2:} Douban-Movie (richer) $\leftrightarrow$ Douban-Music (sparser)
    \item \noindent \textbf{Task 3:} Amazon-Elec (richer) $\leftrightarrow$ Amazon-Cloth (sparser)
\end{itemize}
The details of these three dual-target CDR tasks and the corresponding datasets are shown in Table 3.

\begin{table}[t]
\caption{Statistics of three dual-target CDR tasks.}
\resizebox{\linewidth}{!}{
\begin{tabular}{cccccc}
\toprule
\hline
Tasks                   & Datasets     & \#Users & \#Items & \#Interactions & Density \\ \hline
\multirow{2}{*}{Task 1} & Douban-Movie & 2106    & 9555    & 907219         & 4.508\% \\
                        & Douban-Book  & 2106    & 6777    & 95974          & 0.672\% \\ \hline
\multirow{2}{*}{Task 2} & Douban-Movie & 1666    & 9555    & 781288         & 4.908\% \\
                        & Douban-Music & 1666    & 5567    & 69681          & 0.751\% \\ \hline
\multirow{2}{*}{Task 3} & Amazon-Elec  & 15761   & 51447   & 224689         & 0.027\% \\
                        & Amazon-Cloth & 15761   & 48781   & 133609         & 0.017\% \\ \hline
\bottomrule
\end{tabular}
}
\end{table}

\subsubsection{\textbf{Parameter Settings.}}
For the graph convolution and propagation module in Fig. 2, the layer structure of GCN is ‘$k$ $\rightarrow$ $k$’ and for the disentanglement module, the layer structure is ‘$2k$ $\rightarrow$ $k$’. In the prediction module, the layer structure of user branch MLP is ‘$k$ $\rightarrow$ $2k$ $\rightarrow$ $k$’, and the layer structure of item branch MLP is ‘$2k$ $\rightarrow$ $2k$ $\rightarrow$ $k$’. $k$ is the embedding dimension. We vary $k$ in the range of $\{64,128\}$, but in order to balance the trade-off of recommendation accuracy and model training time, we finally set $k$ to 64. The parameters of all these layers are initialized as the Gaussian distribution $X \sim \mathcal N(0,0.01)$. For each observed user-item interaction, following GA-DTCDR \cite{zhu2020graphical}, we randomly sample 7 unseen interactions as negative instances. For a fair comparison, we leverage the grid search to tune the choice of parameters of all models. For the baseline models, we tune them based on the best parameter settings listed in their original papers. Specifically, we choose the learning rate from $\{0.01, 0.005, 0.001, 0.0005, 0.0001\}$, and search the regularization coefficient in the range of $\{0.001, 0.0001, 0.00001\}$. In addition, we apply the Adam \cite{kingma2014adam} to optimize all the models, and the batch size is 1024. We train our model and other baseline models with 100 epochs in order to guarantee the convergence. Moreover, we investigate the number of GCN layers $l$ in $\{ 1,2,3,4\}$, $\alpha$ of $Beta(\alpha ,\alpha )$ in $\{0.1, 0.5, 1, 2, 5\}$ and the weights of domain classification losses ${\mu _1}$, ${\mu _2}$ in $\{0.1, 0.3, 0.5, 0.7, 1, 3, 5, 10\}$, and analyze their impact on the recommendation performance of our model in Section 4.6. In the experiments, we set $l = 2$, $\alpha = 1$, and ${\mu _1} = {\mu _2} = 1$ by default and we resample the mixing coefficient $\lambda $ once for training each batch of data.

\subsubsection{\textbf{Evaluation Metrics.}}
Since the leave-one-out method is ubiquitous in baseline models, such as GA-DTCDR \cite{zhu2020graphical} and  BiTGCF \cite{liu2020cross}, we also employ it to evaluate the recommendation performance of our proposed DIDA-CDR and baseline models. In other words, we utilize the last interaction record of each test user to form the test set, while all the other interaction records are used as the training set. Following the methods introduced in \cite{krichene2022sampled,cao2022disencdr}, for each test user-item interaction, we randomly sample 999 items that the test user has not interacted with as negative items, and then predict 1000 candidate scores for ranking. The leave-one-out method contains two main metrics, i.e., Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG) \cite{wang2019kgat}, which are widely-used ranking evaluation metrics \cite{zhu2021unified,liu2022exploiting}. In the experiments, we employ them to evaluate the performance of the top-10 ranking results. For a fair evaluation, we perform all experiments 5 times and present the average results.


\begin{table*}[]%4
\caption{The comparison of the baselines and our approach \cite{zhu2020graphical}.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccc|c|c}
\toprule
\hline
&{\textbf{Model}}        &                                                                                                                                                                     & \textbf{Embedding Strategy}                 & \textbf{Transfer Strategy}           \\ \hline
\multicolumn{1}{c|}{\multirow{9}{*}{Baselines}} & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Single-Domain \\ Recommendation (SDR)\end{tabular}}}                    & \textbf{NGCF} \cite{wang2019neural}        & Non-linear MLP                              & -                                    \\ \cline{3-5} 
\multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                                                                                                                  & \textbf{LightGCN} \cite{he2020lightgcn}    & Non-linear MLP                              & -                                    \\ \cline{2-5} 
\multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Single-Target Cross-Domain \\ Recommendation (CDR)\end{tabular}}}       & \textbf{BPR\_EMCDR} \cite{man2017cross}  & Linear Matrix Factorization (MF)            & MLP                                  \\ \cline{3-5} 
\multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                                                                                                                  & \textbf{BPR\_DCDCSR} \cite{ijcai2018p516} & Linear MF                                   & Combination \& MLP                   \\ \cline{2-5} 
\multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Conventional \\ Dual-Target CDR\end{tabular}}} & \textbf{PPGN} \cite{zhao2019cross}        & Graph Embedding                             & Combination \& MLP                   \\ \cline{3-5} 
\multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                                                                                                                  & \textbf{BiTGCF} \cite{liu2020cross}      & Graph Embedding                             & Transfer Learning                    \\ 
\cline{2-5}
\multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Disentanglement-Based Dual-Target \\ or Multi-Target CDR\end{tabular}}}                          & \textbf{GA-DTCDR} \cite{zhu2020graphical}    & Graph Embedding                             & Combination (Element-wise Attention) \\ \cline{3-5} 
\multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                                                                                                                  & \textbf{DR-MTCDR} \cite{guo2023disentangled}    & Graph Embedding \& Self-supervised Learning & Domain Adaptation                    \\ \cline{3-5} 
\multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{}                                                                                                                  & \textbf{DisenCDR} \cite{cao2022disencdr}    & Graph Embedding \& VAE                      & Transfer Learning                    \\ 
\hline
\multicolumn{1}{c|}{Our Approach}               & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Disentanglement-Based \\ Dual-Target CDR\end{tabular}}                                  & \textbf{DIDA-CDR}     & Graph Embedding \&  Disentanglement         & Transfer Learning \& Combination (Attention)              \\ \hline
\bottomrule
\end{tabular}
}
\end{table*}

\begin{table*}[]
\caption{Performance comparison (\%) of different approaches for three dual-target CDR tasks according to HR@10 and NDCG@10 \cite{zhu2020graphical}. While the results of best-performing baselines are underlined, the best results are marked in bold (* indicates $p < 0.05$, paired t-test of our proposed DIDA-CDR vs. the best-performing baselines) \cite{zhu2021learning}.}
\begin{tabular}{c|cccc|cccc|cccc}
\toprule
\hline
\multirow{3}{*}{Datasets} & \multicolumn{4}{c|}{SDR Baselines}                                 & \multicolumn{4}{c|}{Single-Target CDR Baselines}                                                                   & \multicolumn{4}{c}{\begin{tabular}[c]{@{}c@{}}Conventional Dual-Target \\ CDR Baselines\end{tabular}} \\ \cline{2-13} 
                          & \multicolumn{2}{c|}{NGCF}          & \multicolumn{2}{c|}{LightGCN} & \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}BPR\_EMCDR\\ \_MLP\end{tabular}} & \multicolumn{2}{c|}{BPR\_DCDCSR} & \multicolumn{2}{c|}{PPGN}                                    & \multicolumn{2}{c}{BiTGCF}             \\ \cline{2-13} 
                          & HR    & \multicolumn{1}{c|}{NDCG}  & HR            & NDCG          & HR                           & \multicolumn{1}{c|}{NDCG}                        & HR              & NDCG           & HR                 & \multicolumn{1}{c|}{NDCG}               & HR                 & NDCG              \\ \hline
Douban-Movie              & 10.26 & \multicolumn{1}{c|}{5.37}  & 10.53         & 5.49          & -                            & \multicolumn{1}{c|}{-}                           & -               & -              & 12.03              & \multicolumn{1}{c|}{6.42}               & 12.11              & 6.46              \\
Douban-Book               & 7.31  & \multicolumn{1}{c|}{4.08}  & 7.35          & 4.15          & 6.25                         & \multicolumn{1}{c|}{3.93}                        & 6.74            & 4.02           & 10.52              & \multicolumn{1}{c|}{4.78}               & 10.58              & 4.93              \\ \hline
Douban-Movie              & 9.16  & \multicolumn{1}{c|}{4.23}  & 9.24          & 4.25          & -                            & \multicolumn{1}{c|}{-}                           & -               & -              & 10.09              & \multicolumn{1}{c|}{4.35}               & 10.14              & 4.41              \\
Douban-Music              & 6.11  & \multicolumn{1}{c|}{3.87}  & 6.36          & 3.99          & 5.08                         & \multicolumn{1}{c|}{3.45}                        & 5.97            & 3.79           & 7.24               & \multicolumn{1}{c|}{4.03}               & 7.32               & 4.10              \\ \hline
Amazon-Elec               & 20.22 & \multicolumn{1}{c|}{11.97} & 20.03         & 10.94         & -                            & \multicolumn{1}{c|}{-}                           & -               & -              & 22.06              & \multicolumn{1}{c|}{12.44}              & 21.79              & 12.31             \\
Amazon-Cloth              & 10.95 & \multicolumn{1}{c|}{6.01}  & 11.38         & 6.10          & 9.87                         & \multicolumn{1}{c|}{5.33}                        & 10.90           & 5.86           & 13.04              & \multicolumn{1}{c|}{6.91}               & 13.16              & 6.88              \\ \hline
\bottomrule
\end{tabular}%
\vspace{-0.13in}
\end{table*}

\begin{table*}[]
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|cccccc|cccccccc|cc}
\toprule
\hline
\multirow{3}{*}{\textbf{Datasets}} & \multicolumn{6}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Disentanglement-Based Dual-Target \\ or Multi-Target CDR Baselines\end{tabular}}} & \multicolumn{8}{c|}{\textbf{Disentanglement-Based Dual-Target CDR (our)}}                                                                                                                                                                                                                                             & \multicolumn{2}{c}{\textbf{Improvement}}                                                             \\ \cline{2-17} 
                                   & \multicolumn{2}{c|}{GA-DTCDR}                    & \multicolumn{2}{c|}{DR-MTCDR}                       & \multicolumn{2}{c|}{DisenCDR}   & \multicolumn{2}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}DIDA-CDR\\ \_Fixed\end{tabular}}} & \multicolumn{2}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}DIDA-CDR\\ \_Base\end{tabular}}} & \multicolumn{2}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}DIDA-CDR\\ \_ELBO\end{tabular}}} & \multicolumn{2}{c|}{\textbf{DIDA-CDR}} & \multicolumn{2}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}(DIDA-CDR vs.\\ best baselines)\end{tabular}}} \\ \cline{2-17} 
                                   & HR              & \multicolumn{1}{c|}{NDCG}      & HR             & \multicolumn{1}{c|}{NDCG}          & HR             & NDCG           & HR                               & \multicolumn{1}{c|}{NDCG}                             & HR                               & \multicolumn{1}{c|}{NDCG}                            & HR                               & \multicolumn{1}{c|}{NDCG}                            & HR                 & NDCG              & HR                                                & NDCG                                             \\ \hline
Douban-Movie                       & 12.25           & \multicolumn{1}{c|}{6.51}      & 14.74          & \multicolumn{1}{c|}{7.89}          & {\underline{15.09}}    & {\underline{8.02}}     & 16.12                            & \multicolumn{1}{c|}{8.97}                             & 12.98                            & \multicolumn{1}{c|}{6.51}                            & 15.74                            & \multicolumn{1}{c|}{8.51}                            & \textbf{16.66*}    & \textbf{9.16*}    & 10.40\%                                           & 14.21\%                                          \\
Douban-Book                        & 10.71           & \multicolumn{1}{c|}{5.06}      & {\underline{12.66}}    & \multicolumn{1}{c|}{{\underline{7.45}}}    & 12.40          & 7.27           & 13.08                            & \multicolumn{1}{c|}{7.82}                             & 10.41                            & \multicolumn{1}{c|}{5.52}                            & 12.80                            & \multicolumn{1}{c|}{7.69}                            & \textbf{13.79*}    & \textbf{8.18*}    & 8.93\%                                            & 9.80\%                                           \\ \hline
Douban-Movie                       & 10.35           & \multicolumn{1}{c|}{4.57}      & 11.27          & \multicolumn{1}{c|}{5.74}          & {\underline{12.13}}    & {\underline{5.95}}     & 12.76                            & \multicolumn{1}{c|}{6.23}                             & 10.45                            & \multicolumn{1}{c|}{4.99}                            & 12.55                            & \multicolumn{1}{c|}{6.18}                            & \textbf{13.01*}    & \textbf{6.60*}    & 7.25\%                                            & 10.92\%                                          \\
Douban-Music                       & 7.42            & \multicolumn{1}{c|}{4.19}      & 8.49           & \multicolumn{1}{c|}{4.73}          & {\underline{8.92}}     & {\underline{5.02}}     & 9.54                             & \multicolumn{1}{c|}{5.49}                             & 8.36                             & \multicolumn{1}{c|}{4.24}                            & 9.36                             & \multicolumn{1}{c|}{5.37}                            & \textbf{9.97*}     & \textbf{5.79*}    & 11.77\%                                           & 15.34\%                                          \\ \hline
Amazon-Elec                        & {\underline{23.87}}     & \multicolumn{1}{c|}{13.20}     & 22.34          & \multicolumn{1}{c|}{12.98}         & 23.77          & {\underline{13.61}}    & 24.68                            & \multicolumn{1}{c|}{13.83}                            & 22.03                            & \multicolumn{1}{c|}{12.97}                           & 24.14                            & \multicolumn{1}{c|}{13.72}                           & \textbf{25.05*}    & \textbf{14.36*}   & 4.94\%                                            & 5.51\%                                           \\
Amazon-Cloth                       & 13.94           & \multicolumn{1}{c|}{7.09}      & 14.13          & \multicolumn{1}{c|}{7.59}          & {\underline{15.46}}    & {\underline{8.42}}     & 16.01                            & \multicolumn{1}{c|}{8.94}                             & 12.97                            & \multicolumn{1}{c|}{6.93}                            & 15.76                            & \multicolumn{1}{c|}{8.63}                            & \textbf{16.69*}    & \textbf{9.33}     & 7.96\%                                            & 10.81\%                                          \\ \hline
\bottomrule
\end{tabular}%
}
\end{table*}



\subsubsection{\textbf{Comparison Methods.}}
We select a total of nine representative and state-of-the-art baseline models to compare with our proposed DIDA-CDR. These nine baseline models can be divided into four categories, i.e., (1) Single-Domain Recommendation (SDR), (2) Single-Target Cross-Domain Recommendation (CDR), (3) Conventional Dual-Target CDR, and (4) Disentanglement-Based Dual-Target or Multi-Target CDR. For a clear comparison, in Table 4, we elaborate on the embedding strategies and transfer strategies of the above nine baseline models and our proposed DIDA-CDR.

\subsection{Performance Comparison (for RQ1)}
Table 5 presents the performance comparison\footnote{Due to space limitation, we only show the results when $k=64$ in Table 5. For the results under other values of $k$ that are omitted, similarly, our model also has a significant improvement over other baseline models.} of various approaches for three dual-target CDR tasks according to HR@10 and NDCG@10. Note that since single-target CDR baselines aim to improve the recommendation performance on the sparser domain, we train them on both domains and then only present their experimental results on the sparser domain. From Table 5, we have the following observations: (1) Our proposed DIDA-CDR improves other baseline models on sparser domain by a large margin. Specifically, on sparser domain, it outperforms the best-performing baseline model with an average improvement of 9.55\% in terms of HR@10 and 11.98\% in terms of NDCG@10. This is because we adopt the interpolative data augmentation, which effectively generate both relevant and diverse augmented user representations to augment the sparser domain, and therefore significant improvements in recommendation performance can be obtained on the sparser domain; (2) Disentanglement-based dual-target CDR models improve conventional dual-target CDR models by an average of 14.98\% in terms of HR@10 and 20.94\% in terms of NDCG@10, which shows that decoupling and then transferring the domain-shared information to both domains is an efficient way for dual-target CDR; (3) Compared with other disentanglement-based dual-target CDR baselines, our model can achieve better recommendation performance over them. Specifically, our proposed DIDA-CDR achieves an average increase of 8.54\% in terms of HR@10 and 11.10\% in terms of NDCG@10, compared to the best-performing disentanglement-based dual-target CDR baseline. This is because our model particularly takes domain-independent user preferences into consideration, and the proposed disentanglement module can more effectively disentangle all three essential components of user preferences, thus capturing more comprehensive user preferences. For a more detailed analysis of each module and each component of user preferences  of the proposed DIDA-CDR, please refer to Section 4.3 and Section 4.4, respectively.

\subsection{Ablation Study (for RQ2)}
To show the contribution of each proposed component to the improvement of overall performance, we modify our proposed model to form three variants and conduct an ablation study for three dual-target CDR tasks.
\subsubsection{\textbf{Impact of Interpolative Data Augmentation}}
We construct a variant of DIDA-CDR, namely \textbf{DIDA-CDR\_Fixed}, by replacing the interpolative data augmentation module with a fixed mixing strategy. In fact, we conducted experiments to select the best-performing mixing coefficient, i.e., 0.5, from $\{0.1,0.3,0.5,0.7,0.9\}$ to implement the above variant. From Table 5, we can observe that with the interpolative data augmentation, our proposed DIDA-CDR outperforms \textbf{DIDA-CDR\_Fixed} with an average improvement of 3.7\%. This demonstrates that the interpolative data augmentation can not only augment the sparser domain by introducing randomness to increase the diversity of augmentation, but also generate representative augmented user representations by effectively mixing user embeddings for subsequent disentanglement. Meanwhile, the introduction of randomness also weakens the possible negative transfer caused by the linear interpolation operation with fixed weights, which can also be seen from the experimental results.

\subsubsection{\textbf{Impact of User Preference Disentanglement}}
Furthermore, another variant, namely \textbf{DIDA-CDR\_Base}, directly feeds the generated user embeddings to the information fusion module and does not include the disentanglement module, thus the variant only includes the prediction loss ${\mathcal L_{prd}}$, that is ${\mu _1} = {\mu _2} = 0$. From Table 5, we can observe that without the disentanglement module, the recommendation performance of \textbf{DIDA-CDR\_Base} would degrade to be comparable to that of the conventional dual-target CDR baselines, and weaker than that of disentanglement-based ones. This shows that the disentanglement module can indeed help the model perform more effective cross-domain knowledge transfer without negative transfer by decoupling domain-shared, domain-specific, domain-independent information and transfering only domain-shared information, thus improving the performance of cross-domain recommendations.

\subsubsection{\textbf{Impact of Domain Classifier}}
In addition, following DisenCDR \cite{cao2022disencdr}, we modify the disentanglement module in our model, i.e., replace our domain classification losses with the standard ELBO, to form another variant, namely \textbf{DIDA-CDR\_ELBO}. From Table 5, we can observe that our proposed DIDA-CDR outperforms \textbf{DIDA-CDR\_ELBO} with an average improvement of 5.98\%. This demonstrates that the proposed disentanglement module can indeed collaborate well with the domain classifier to decouple more accurate essential components of user preferences, especially the domain-independent information, to capture comprehensive user preferences, thus enabling the model to achieve better recommendation performance through superior disentanglement.

% Figure environment removed

\subsection{Impact of Various Components of User Preferences (for RQ3)}
To demonstrate that all three components of user preferences, i.e., domain-share, domain-specific and domain-independent information, are essential and effective for recommendation, and do not require transfer of domain-independent user preferences, we compare DIDA-CDR with its four variants, including DIDA-CDR (w/o sha.), DIDA-CDR (w/o spe.), DIDA-CDR (w/o ind.) and DIDA-CDR (transfer ind.). Fig. 3(a)-(b) shows the performance comparison between our model and the above four variants\footnote{Due to space limitation, we only show the results on Task 1 in Fig. 3, i.e., only the results on the pair of datasets consisting of Douban-Movie and Douban-Book are presented, and similar trends can be observed for results on the other omitted tasks. Similarly, Fig. 4 and Fig. 5 only shows the results on Task 1 for the same reason above.}. The differences between various variants and the impact of each component of user preferences are elaborated in the following subsections.

\subsubsection{\textbf{Impact of Domain-Shared User Preferences}}
\textbf{DIDA-CDR (w/o sha.)} extracts the domain-shared user preferences from both domains, but does not transfer them to any domain. From Fig. 3(a)-(b), we can see that our proposed DIDA-CDR outperforms DIDA-CDR (w/o sha.) with an average improvement of 17.21\%. This is because the domain-shared user preferences are valuable information, which plays an important role in cross-domain recommendation and can improve the recommendation performance on both domains simultaneously.

% Figure environment removed

\subsubsection{\textbf{Impact of Domain-Specific User Preferences}}
\textbf{DIDA-CDR (w/o spe.)} does not consider the domain-specific user preferences when making recommendations. From Fig. 3(a)-(b), we can observe that our proposed DIDA-CDR outperforms DIDA-CDR (w/o spe.) with an average improvement of 36.12\%. This is because the domain-specific user preferences are inherent personalized preferences of users in each domain. If it is not considered when making recommendations, the recommendation performance of our model will be significantly reduced.

\subsubsection{\textbf{Impact of Domain-Independent User Preferences}}
\textbf{DIDA-CDR (w/o ind.)} includes the domain-shared and domain-specific user preferences, but does not include the domain-independent user preferences when capturing comprehensive user preferences; \textbf{DIDA-CDR (transfer ind.)} disentangles the domain-independent user preferences and then transfer them to another domain. It can be seen from Fig. 3(a)-(b) that our proposed DIDA-CDR outperforms DIDA-CDR (w/o ind.) and DIDA-CDR (transfer ind.) with an average improvement of 12.40\% and 26.04\%, respectively. In view of this, we make the following qualitative analysis. The domain-independent user preferences seemingly exist in each domain, but actually have different meanings. If they are not included when capturing user preferences, the captured user preferences are incomplete, resulting in the degraded recommendation performance of the model. Moreover, if the domain-independent information is transferred to other domains, it will provide the useless information and cause the performance degradation.

% Figure environment removed

\subsection{Impact of Various Information Fusion Approaches (for RQ4)}
After we obtain three essential user preference components, we compare various information fusion approaches, i.e., concatenation, element-wise sum, and attention, to fuse them into comprehensive user preferences. The performance comparison of three used information fusion approaches is presented in Fig. 3(c)-(d). We find that when our model utilizes the attention mechanism for information fusion, it improves the variants using concatenation and summation by an average of 28.37\% and 20.69\%, respectively. This is because the attention mechanism can not only capture the relationship between various components, but also selectively highlight the key information and weaken the redundant information by learning weights. In this paper, the attention mechanism measures the importance of domain-shared, domain-specific and domain-independent information to comprehensive user preferences through weights, and weakens the redundant information between the domain-shared and domain-independent user preferences, thus enabling our model to achieve better recommendation results.

\subsection{Parameter Sensitivity (for RQ5)}
\subsubsection{\textbf{Impact of $l$.}} Stacking too many layers when training a deep GCN is prone to over-smoothing \cite{li2018deeper}. In order to explore this effect, we investigate the number of GCN layers $l$ in the range of $\{1, 2, 3, 4\}$ in the experiments and report the experimental results in Fig. 4(a)-(b). It can be observed that 2-layer GCN is significantly better than 1-layer GCN, which indicates that stacking a moderate number of layers is beneficial for mining higher-order user-item relationships. However, the recommendation performance of our proposed DIDA-CDR drops on some datasets when $l = 3$ and degrades even more when $l = 4$. The reason may be that when the number of layers is greater than 3, the problem of over-smoothing occurs, resulting in the fact that increasing the number of layers at this time will in turn reduce the recommendation performance of our model.

\subsubsection{\textbf{Impact of $\alpha$.}} $Beta(\alpha ,\alpha )$ is Uniform distribution when $\alpha  = 1$, Bimodal distribution when $\alpha < 1$ and Bell-shaped distribution when $\alpha > 1$ \cite{chen2022graph}. In order to explore from which distribution sampling $\lambda $ can help the recommendation performance of model the most, we search $\alpha$ in the range of $\{0.1, 0.5, 1, 2, 5\}$. The performance comparison is illustrated in Fig. 4(c)-(d). We can see that the best performance of our proposed DIDA-CDR is achieved when $\alpha  = 1$, which presents that $\lambda $ sampled from the Uniform distribution can effectively mix the user embeddings in the richer domain and the sparser domain, thereby effectively alleviating the data imbalance and improving the recommendation performance of model. In contrast, when $\alpha < 1$ or $\alpha > 1$, sampling $\lambda $ from $Beta(\alpha ,\alpha )$ will result in the performance degradation.

\subsubsection{\textbf{Impact of ${\mu _1}$ and ${\mu _2}$.}} To explore the effect of weights of domain classification losses on the overall recommendation performance of our DIDA-CDR, we vary ${\mu _1}$ and ${\mu _2}$ in $\{0.1, 0.3, 0.5, 0.7, 1, 3, 5, 10\}$. The results are reported in Fig. 5. It can be seen from Fig. 5(a)-(b) that the contribution of domain classification loss ${\mathcal L_{cl{s_1}}}$ to decoupling more accurate domain-specific user preferences is small when ${\mu _1} \rightarrow 0$. When ${\mu _1}$ is too large, the domain classification loss ${\mathcal L_{cl{s_1}}}$ receives more attention during the learning process. As a result, the contribution of prediction loss ${\mathcal L_{prd}}$ to the model is weakened, which reduces the recommendation performance of the model. Similarly, from Fig. 5(c)-(d), a similar trend can be observed for the weights ${\mu _2}$ of domain classification loss $L_{cl{s_2}}$. Empirically, we choose ${\mu _1} = {\mu _2} = 1$.

\section{Conclusion and Future Work}
In this paper, we have proposed a novel Disentanglement-based framework with Interpolative Data Augmentation for dual-target Cross-Domain Recommendation, called DIDA-CDR. DIDA-CDR consists of an interpolative data augmentation approach to generating both relevant and diverse augmented user representations to augment the sparser domain and explore the potential user preferences, and a user preference disentanglement module to decouple essential components of user preferences to capture comprehensive user preferences, all of which help improve the recommendation performance on both domains simultaneously. Also, we have conducted extensive experiments on five real-world datasets to show the significant superiority of DIDA-CDR over the state-of-the-art methods. In the future, we plan to extend our model to multi-target CDR scenarios where users partially overlap.

\begin{acks}
This work is partially supported by ARC Discovery Projects DP200101441 and DP230100676.
\end{acks}

\balance

\bibliographystyle{ACM-Reference-Format}
\bibliography{DIDA-CDR}

\end{document}
\endinput
