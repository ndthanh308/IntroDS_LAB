
@incollection{back_multi-objective_2020,
	address = {Cham},
	title = {Multi-{Objective} {Counterfactual} {Explanations}},
	volume = {12269},
	isbn = {978-3-030-58111-4 978-3-030-58112-1},
	url = {http://link.springer.com/10.1007/978-3-030-58112-1_31},
	abstract = {Counterfactual explanations are one of the most popular methods to make predictions of black box machine learning models interpretable by providing explanations in the form of ‘what-if scenarios’. Most current approaches optimize a collapsed, weighted sum of multiple objectives, which are naturally diﬃcult to balance a-priori. We propose the Multi-Objective Counterfactuals (MOC) method, which translates the counterfactual search into a multi-objective optimization problem. Our approach not only returns a diverse set of counterfactuals with different trade-oﬀs between the proposed objectives, but also maintains diversity in feature space. This enables a more detailed post-hoc analysis to facilitate better understanding and also more options for actionable user responses to change the predicted outcome. Our approach is also model-agnostic and works for numerical and categorical input features. We show the usefulness of MOC in concrete cases and compare our approach with state-of-the-art methods for counterfactual explanations.},
	language = {en},
	urldate = {2022-02-16},
	booktitle = {Parallel {Problem} {Solving} from {Nature} – {PPSN} {XVI}},
	publisher = {Springer International Publishing},
	author = {Dandl, Susanne and Molnar, Christoph and Binder, Martin and Bischl, Bernd},
	editor = {Bäck, Thomas and Preuss, Mike and Deutz, André and Wang, Hao and Doerr, Carola and Emmerich, Michael and Trautmann, Heike},
	year = {2020},
	doi = {10.1007/978-3-030-58112-1_31},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {448--469},
	file = {Dandl et al. - 2020 - Multi-Objective Counterfactual Explanations.pdf:/Users/ludwigbothmann/Zotero/storage/VML4W8KJ/Dandl et al. - 2020 - Multi-Objective Counterfactual Explanations.pdf:application/pdf},
}

@inproceedings{kilbertus_avoiding_2017,
	title = {Avoiding {Discrimination} through {Causal} {Reasoning}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/f5f8590cd58a54e94377e6ae2eded4d9-Abstract.html},
	urldate = {2021-11-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kilbertus, Niki and Rojas Carulla, Mateo and Parascandolo, Giambattista and Hardt, Moritz and Janzing, Dominik and Schölkopf, Bernhard},
	year = {2017},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/7HRJJS6Y/Kilbertus et al. - 2017 - Avoiding Discrimination through Causal Reasoning.pdf:application/pdf},
}

@inproceedings{verma_fairness_2018,
	address = {Gothenburg Sweden},
	title = {Fairness definitions explained},
	isbn = {978-1-4503-5746-3},
	doi = {10.1145/3194770.3194776},
	abstract = {Algorithm fairness has started to attract the attention of researchers in AI, Software Engineering and Law communities, with more than twenty different notions of fairness proposed in the last few years. Yet, there is no clear agreement on which definition to apply in each situation. Moreover, the detailed differences between multiple definitions are difficult to grasp. To address this issue, this paper collects the most prominent definitions of fairness for the algorithmic classification problem, explains the rationale behind these definitions, and demonstrates each of them on a single unifying case-study. Our analysis intuitively explains why the same case can be considered fair according to some definitions and unfair according to others.},
	language = {en},
	urldate = {2021-04-27},
	booktitle = {Proceedings of the {International} {Workshop} on {Software} {Fairness}},
	publisher = {ACM},
	author = {Verma, Sahil and Rubin, Julia},
	year = {2018},
	file = {Verma und Rubin - 2018 - Fairness definitions explained.pdf:/Users/ludwigbothmann/Zotero/storage/IYQ5STQ9/Verma und Rubin - 2018 - Fairness definitions explained.pdf:application/pdf},
}

@inproceedings{pfohl_counterfactual_2019,
	title = {Counterfactual {Reasoning} for {Fair} {Clinical} {Risk} {Prediction}},
	url = {https://proceedings.mlr.press/v106/pfohl19a.html},
	abstract = {The use of machine learning systems to support decision making in healthcare raises questions as to what extent these systems may introduce or exacerbate disparities in care for historically underrepresented and mistreated groups, due to biases implicitly embedded in observational data in electronic health records. To address this problem in the context of clinical risk prediction models, we develop an augmented counterfactual fairness criteria that extends the group fairness criteria of equalized odds. We do so by requiring that the same prediction be made for a patient, and a counterfactual patient resulting from changing a sensitive attribute, if the factual and counterfactual outcomes do not differ. We investigate the extent to which the augmented counterfactual fairness criteria may be applied to develop fair models for prolonged inpatient length of stay and mortality with observational electronic health records data. As the fairness criteria is ill-defined without knowledge of the data generating process, we use a variational autoencoder to perform counterfactual inference in the context of an assumed causal graph. While our technique provides a means to trade off maintenance of fairness with reduction in predictive performance in the context of a learned generative model, further work is needed to assess the generality of this approach.},
	language = {en},
	urldate = {2023-02-27},
	booktitle = {Proceedings of the 4th {Machine} {Learning} for {Healthcare} {Conference}},
	publisher = {PMLR},
	author = {Pfohl, Stephen R. and Duan, Tony and Ding, Daisy Yi and Shah, Nigam H.},
	month = oct,
	year = {2019},
	pages = {325--358},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/9I2XTCH4/Pfohl et al. - 2019 - Counterfactual Reasoning for Fair Clinical Risk Pr.pdf:application/pdf},
}

@misc{bothmann_what_2023,
	title = {What {Is} {Fairness}? {Philosophical} {Considerations} and {Implications} {For} {FairML}},
	copyright = {Creative Commons Attribution 4.0 International},
	doi = {10.48550/arXiv.2205.09622},
	publisher = {arXiv},
	author = {Bothmann, Ludwig and Peters, Kristina and Bischl, Bernd},
	year = {2023},
	keywords = {Artificial Intelligence (cs.AI), Computers and Society (cs.CY), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@misc{loftus_causal_2018,
	title = {Causal {Reasoning} for {Algorithmic} {Fairness}},
	url = {http://arxiv.org/abs/1805.05859},
	abstract = {In this work, we argue for the importance of causal reasoning in creating fair algorithms for decision making. We give a review of existing approaches to fairness, describe work in causality necessary for the understanding of causal approaches, argue why causality is necessary for any approach that wishes to be fair, and give a detailed analysis of the many recent approaches to causality-based fairness.},
	language = {en},
	urldate = {2023-02-14},
	publisher = {arXiv},
	author = {Loftus, Joshua R. and Russell, Chris and Kusner, Matt J. and Silva, Ricardo},
	month = may,
	year = {2018},
	note = {arXiv:1805.05859 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Artificial Intelligence},
	file = {Loftus et al. - 2018 - Causal Reasoning for Algorithmic Fairness.pdf:/Users/ludwigbothmann/Zotero/storage/BNSL54VD/Loftus et al. - 2018 - Causal Reasoning for Algorithmic Fairness.pdf:application/pdf},
}

@inproceedings{guyon_design_2008,
	title = {Design and {Analysis} of the {Causation} and {Prediction} {Challenge}},
	url = {http://proceedings.mlr.press/v3/guyon08a.html},
	abstract = {We organized for WCCI 2008 a challenge to evaluate causal modeling techniques,  focusing on predicting the effect of “interventions” performed by an external  agent. Examples of that problem are found in the medical domain to predict  the effect of a drug prior to administering it, or in econometrics to predict  the effect of a new policy prior to issuing it. We concentrate on a given  target variable to be predicted (e.g., health status of a patient) from a  number of candidate predictive variables or “features” (e.g., risk factors  in the medical domain). Under interventions, variable predictive power and  causality are tied together. For instance, both smoking and coughing may be predictive of lung cancer (the target) in the absence of external intervention;  however, prohibiting smoking (a possible cause) may prevent lung cancer, but administering a cough medicine to stop coughing (a possible consequence) would not. We propose four tasks from various application domains, each dataset  including a training set drawn from a “natural” distribution and three test  sets: one from the same distribution as the training set and two corresponding  to data drawn when an external agent is manipulating certain variables. The  goal is to predict a binary target variable, whose values on test data are  withheld. The participants were asked to provide predictions of the target  variable on test data and the list of variables (features) used to make predictions.  The challenge platform remains open for post-challenge submissions and the  organization of other events is under way (see http://clopinet.com/causality).},
	language = {en},
	urldate = {2023-01-31},
	booktitle = {Causation and {Prediction} {Challenge}},
	publisher = {PMLR},
	author = {Guyon, Isabelle and Aliferis, Constantin and Cooper, Greg and Elisseeff, André and Pellet, Jean-Philippe and Spirtes, Peter and Statnikov, Alexander},
	month = dec,
	year = {2008},
	note = {ISSN: 1938-7228},
	pages = {1--33},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/8LJSCHDQ/Guyon et al. - 2008 - Design and Analysis of the Causation and Predictio.pdf:application/pdf},
}

@article{ramanan_causal_2020,
	title = {Causal {Learning} {From} {Predictive} {Modeling} for {Observational} {Data}},
	volume = {3},
	issn = {2624-909X},
	url = {https://www.frontiersin.org/articles/10.3389/fdata.2020.535976},
	abstract = {We consider the problem of learning structured causal models from observational data. In this work, we use causal Bayesian networks to represent causal relationships among model variables. To this effect, we explore the use of two types of independencies—context-specific independence (CSI) and mutual independence (MI). We use CSI to identify the candidate set of causal relationships and then use MI to quantify their strengths and construct a causal model. We validate the learned models on benchmark networks and demonstrate the effectiveness when compared to some of the state-of-the-art Causal Bayesian Network Learning algorithms from observational Data.},
	urldate = {2023-01-31},
	journal = {Frontiers in Big Data},
	author = {Ramanan, Nandini and Natarajan, Sriraam},
	year = {2020},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/32LA94GJ/Ramanan und Natarajan - 2020 - Causal Learning From Predictive Modeling for Obser.pdf:application/pdf},
}

@inproceedings{wang_shapley_2021,
	title = {Shapley {Flow}: {A} {Graph}-based {Approach} to {Interpreting} {Model} {Predictions}},
	shorttitle = {Shapley {Flow}},
	url = {https://proceedings.mlr.press/v130/wang21b.html},
	abstract = {Many existing approaches for estimating feature importance are problematic because they ignore or hide dependencies among features. A causal graph, which encodes the relationships among input variables, can aid in assigning feature importance. However, current approaches that assign credit to nodes in the causal graph fail to explain the entire graph. In light of these limitations, we propose Shapley Flow, a novel approach to interpreting machine learning models. It considers the entire causal graph, and assigns credit to edges instead of treating nodes as the fundamental unit of credit assignment. Shapley Flow is the unique solution to a generalization of the Shapley value axioms for directed acyclic graphs. We demonstrate the benefit of using Shapley Flow to reason about the impact of a model’s input on its output. In addition to maintaining insights from existing approaches, Shapley Flow extends the flat, set-based, view prevalent in game theory based explanation methods to a deeper, graph-based, view. This graph-based view enables users to understand the flow of importance through a system, and reason about potential interventions.},
	language = {en},
	urldate = {2023-01-31},
	booktitle = {Proceedings of {The} 24th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Wang, Jiaxuan and Wiens, Jenna and Lundberg, Scott},
	month = mar,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {721--729},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/75S3HQDU/Wang et al. - 2021 - Shapley Flow A Graph-based Approach to Interpreti.pdf:application/pdf;Supplementary PDF:/Users/ludwigbothmann/Zotero/storage/WPCAGGS6/Wang et al. - 2021 - Shapley Flow A Graph-based Approach to Interpreti.pdf:application/pdf},
}

@inproceedings{nabi_optimal_2022,
	title = {Optimal {Training} of {Fair} {Predictive} {Models}},
	url = {https://proceedings.mlr.press/v177/nabi22a.html},
	abstract = {Recently there has been sustained interest in modifying prediction algorithms to satisfy fairness constraints. These constraints are typically complex nonlinear functionals of the observed data distribution. Focusing on the path-specific causal constraints, we introduce new theoretical results and optimization techniques to make model training easier and more accurate. Specifically, we show how to reparameterize the observed data likelihood such that fairness constraints correspond directly to parameters that appear in the likelihood, transforming a complex constrained optimization objective into a simple optimization problem with box constraints. We also exploit methods from empirical likelihood theory in statistics to improve predictive performance by constraining baseline covariates, without requiring parametric models. We combine the merits of both proposals to optimize a hybrid reparameterized likelihood. The techniques presented here should be applicable more broadly to fair prediction proposals that impose constraints on predictive models.},
	language = {en},
	urldate = {2023-01-31},
	booktitle = {Proceedings of the {First} {Conference} on {Causal} {Learning} and {Reasoning}},
	publisher = {PMLR},
	author = {Nabi, Razieh and Malinsky, Daniel and Shpitser, Ilya},
	month = jun,
	year = {2022},
	pages = {594--617},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/GPJEKCJ8/Nabi et al. - 2022 - Optimal Training of Fair Predictive Models.pdf:application/pdf},
}

@inproceedings{nabi_fair_2018,
	address = {New Orleans, Louisiana, USA},
	series = {{AAAI}'18/{IAAI}'18/{EAAI}'18},
	title = {Fair inference on outcomes},
	isbn = {978-1-57735-800-8},
	doi = {10.5555/3504035.3504270},
	abstract = {In this paper, we consider the problem of fair statistical inference involving outcome variables. Examples include classification and regression problems, and estimating treatment effects in randomized trials or observational data. The issue of fairness arises in such problems where some covariates or treatments are "sensitive," in the sense of having potential of creating discrimination. In this paper, we argue that the presence of discrimination can be formalized in a sensible way as the presence of an effect of a sensitive covariate on the outcome along certain causal pathways, a view which generalizes (Pearl 2009). A fair outcome model can then be learned by solving a constrained optimization problem. We discuss a number of complications that arise in classical statistical inference due to this view and provide workarounds based on recent work in causal and semi-parametric inference.},
	urldate = {2023-01-31},
	booktitle = {Proceedings of the {Thirty}-{Second} {AAAI} {Conference} on {Artificial} {Intelligence} and {Thirtieth} {Innovative} {Applications} of {Artificial} {Intelligence} {Conference} and {Eighth} {AAAI} {Symposium} on {Educational} {Advances} in {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Nabi, Razieh and Shpitser, Ilya},
	month = feb,
	year = {2018},
	pages = {1931--1940},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/US5XDTJA/Nabi und Shpitser - 2018 - Fair inference on outcomes.pdf:application/pdf},
}

@inproceedings{nabi_learning_2019,
	title = {Learning {Optimal} {Fair} {Policies}},
	url = {https://proceedings.mlr.press/v97/nabi19a.html},
	abstract = {Systematic discriminatory biases present in our society influence the way data is collected and stored, the way variables are defined, and the way scientific findings are put into practice as policy. Automated decision procedures and learning algorithms applied to such data may serve to perpetuate existing injustice or unfairness in our society. In this paper, we consider how to make optimal but fair decisions, which “break the cycle of injustice” by correcting for the unfair dependence of both decisions and outcomes on sensitive features (e.g., variables that correspond to gender, race, disability, or other protected attributes). We use methods from causal inference and constrained optimization to learn optimal policies in a way that addresses multiple potential biases which afflict data analysis in sensitive contexts, extending the approach of Nabi \& Shpitser (2018). Our proposal comes equipped with the theoretical guarantee that the chosen fair policy will induce a joint distribution for new instances that satisfies given fairness constraints. We illustrate our approach with both synthetic data and real criminal justice data.},
	language = {en},
	urldate = {2023-01-31},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Nabi, Razieh and Malinsky, Daniel and Shpitser, Ilya},
	month = may,
	year = {2019},
	pages = {4674--4682},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/UCIGD5EN/Nabi et al. - 2019 - Learning Optimal Fair Policies.pdf:application/pdf;Supplementary PDF:/Users/ludwigbothmann/Zotero/storage/MNPH43N4/Nabi et al. - 2019 - Learning Optimal Fair Policies.pdf:application/pdf},
}

@article{yao_survey_2021,
	title = {A {Survey} on {Causal} {Inference}},
	volume = {15},
	issn = {1556-4681},
	url = {https://doi.org/10.1145/3444944},
	doi = {10.1145/3444944},
	abstract = {Causal inference is a critical research topic across many domains, such as statistics, computer science, education, public policy, and economics, for decades. Nowadays, estimating causal effect from observational data has become an appealing research direction owing to the large amount of available data and low budget requirement, compared with randomized controlled trials. Embraced with the rapidly developed machine learning area, various causal effect estimation methods for observational data have sprung up. In this survey, we provide a comprehensive review of causal inference methods under the potential outcome framework, one of the well-known causal inference frameworks. The methods are divided into two categories depending on whether they require all three assumptions of the potential outcome framework or not. For each category, both the traditional statistical methods and the recent machine learning enhanced methods are discussed and compared. The plausible applications of these methods are also presented, including the applications in advertising, recommendation, medicine, and so on. Moreover, the commonly used benchmark datasets as well as the open-source codes are also summarized, which facilitate researchers and practitioners to explore, evaluate and apply the causal inference methods.},
	number = {5},
	urldate = {2023-01-31},
	journal = {ACM Transactions on Knowledge Discovery from Data},
	author = {Yao, Liuyi and Chu, Zhixuan and Li, Sheng and Li, Yaliang and Gao, Jing and Zhang, Aidong},
	month = may,
	year = {2021},
	keywords = {Treatment effect estimation; Representation learning},
	pages = {74:1--74:46},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/EYGYNLM6/Yao et al. - 2021 - A Survey on Causal Inference.pdf:application/pdf},
}

@inproceedings{ding_retiring_2021,
	title = {Retiring {Adult}: {New} {Datasets} for {Fair} {Machine} {Learning}},
	volume = {34},
	shorttitle = {Retiring {Adult}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/32e54441e6382a7fbacbbbaf3c450059-Abstract.html},
	abstract = {Although the fairness community has recognized the importance of data, researchers in the area primarily rely on UCI Adult when it comes to tabular data. Derived from a 1994 US Census survey, this dataset has appeared in hundreds of research papers where it served as the basis for the development and comparison of many algorithmic fairness interventions. We reconstruct a superset of the UCI Adult data from available US Census sources and reveal idiosyncrasies of the UCI Adult dataset that limit its external validity. Our primary contribution is a suite of new datasets derived from US Census surveys that extend the existing data ecosystem for research on fair machine learning. We create prediction tasks relating to income, employment, health, transportation, and housing. The data span multiple years and all states of the United States, allowing researchers to study temporal shift and geographic variation. We highlight a broad initial sweep of new empirical insights relating to trade-offs between fairness criteria, performance of algorithmic interventions, and the role of distribution shift based on our new datasets. Our findings inform ongoing debates, challenge some existing narratives, and point to future research directions.},
	urldate = {2023-01-31},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ding, Frances and Hardt, Moritz and Miller, John and Schmidt, Ludwig},
	year = {2021},
	pages = {6478--6490},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/D3MVS6LK/Ding et al. - 2021 - Retiring Adult New Datasets for Fair Machine Lear.pdf:application/pdf},
}

@article{le_quy_survey_2022,
	title = {A survey on datasets for fairness-aware machine learning},
	volume = {12},
	issn = {1942-4795},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1452},
	doi = {10.1002/widm.1452},
	abstract = {As decision-making increasingly relies on machine learning (ML) and (big) data, the issue of fairness in data-driven artificial intelligence systems is receiving increasing attention from both research and industry. A large variety of fairness-aware ML solutions have been proposed which involve fairness-related interventions in the data, learning algorithms, and/or model outputs. However, a vital part of proposing new approaches is evaluating them empirically on benchmark datasets that represent realistic and diverse settings. Therefore, in this paper, we overview real-world datasets used for fairness-aware ML. We focus on tabular data as the most common data representation for fairness-aware ML. We start our analysis by identifying relationships between the different attributes, particularly with respect to protected attributes and class attribute, using a Bayesian network. For a deeper understanding of bias in the datasets, we investigate interesting relationships using exploratory analysis. This article is categorized under: Commercial, Legal, and Ethical Issues {\textgreater} Fairness in Data Mining Fundamental Concepts of Data and Knowledge {\textgreater} Data Concepts Technologies {\textgreater} Data Preprocessing},
	language = {en},
	number = {3},
	urldate = {2023-01-31},
	journal = {WIREs Data Mining and Knowledge Discovery},
	author = {Le Quy, Tai and Roy, Arjun and Iosifidis, Vasileios and Zhang, Wenbin and Ntoutsi, Eirini},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1452},
	keywords = {benchmark datasets, bias, datasets for fairness, discrimination, fairness-aware machine learning},
	pages = {e1452},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/ZKUMQYBX/Le Quy et al. - 2022 - A survey on datasets for fairness-aware machine le.pdf:application/pdf;Snapshot:/Users/ludwigbothmann/Zotero/storage/8DT6D5QC/widm.html:text/html},
}

@inproceedings{dwork_fairness_2012,
	address = {New York, NY, USA},
	title = {Fairness through awareness},
	doi = {10.1145/2090236.2090255},
	booktitle = {Proceedings of the 3rd {Innovations} in {Theoretical} {Computer} {Science} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
	year = {2012},
	pages = {214--226},
	file = {arXiv Fulltext PDF:/Users/ludwigbothmann/Zotero/storage/ZIMF4AEZ/Dwork et al. - 2011 - Fairness Through Awareness.pdf:application/pdf},
}

@article{angwin_machine_2016,
	title = {Machine bias: {There}’s software used across the country to predict future criminals. and it’s biased against blacks.},
	url = {https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing},
	journal = {ProPublica},
	author = {Angwin, Julia and Larson, Jeff and Mattu, Surya and Kirchner, Lauren},
	year = {2016},
}

@incollection{wu_pc-fairness_2019,
	address = {Red Hook, NY, USA},
	title = {{PC}-fairness: a unified framework for measuring causality-based fairness},
	shorttitle = {{PC}-fairness},
	abstract = {A recent trend of fair machine learning is to define fairness as causality-based notions which concern the causal connection between protected attributes and decisions. However, one common challenge of all causality-based fairness notions is identifiability, i.e., whether they can be uniquely measured from observational data, which is a critical barrier to applying these notions to real-world situations. In this paper, we develop a framework for measuring different causality-based fairness. We propose a unified definition that covers most of previous causality-based fairness notions, namely the path-specific counterfactual fairness (PC fairness). Based on that, we propose a general method in the form of a constrained optimization problem for bounding the path-specific counterfactual fairness under all unidentifiable situations. Experiments on synthetic and real-world datasets show the correctness and effectiveness of our method.},
	number = {306},
	urldate = {2023-01-09},
	booktitle = {Proceedings of the 33rd {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Wu, Yongkai and Zhang, Lu and Wu, Xintao and Tong, Hanghang},
	month = dec,
	year = {2019},
	pages = {3404--3414},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/6G5GBS7R/Wu et al. - 2019 - PC-fairness a unified framework for measuring cau.pdf:application/pdf},
}

@article{vansteelandt_interventional_2017,
	title = {Interventional effects for mediation analysis with multiple mediators},
	volume = {28},
	issn = {1044-3983},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5289540/},
	doi = {10.1097/EDE.0000000000000596},
	abstract = {The mediation formula for the identification of natural (in)direct effects has facilitated mediation analyses that better respect the nature of the data, with greater consideration of the need for confounding control. The default assumptions on which it relies are strong, however. In particular, they are known to be violated when confounders of the mediator–outcome association are affected by the exposure. This complicates extensions of counterfactual-based mediation analysis to settings that involve repeatedly measured mediators, or multiple correlated mediators., VanderWeele, Vansteelandt, and Robins introduced so-called interventional (in)direct effects. These can be identified under much weaker conditions than natural (in)direct effects, but have the drawback of not adding up to the total effect. In this article, we adapt their proposal in order to achieve an exact decomposition of the total effect, and extend it to the multiple mediator setting. Interestingly, the proposed effects capture the path-specific effects of an exposure on an outcome that are mediated by distinct mediators, even when – as often – the structural dependence between the multiple mediators is unknown; for instance, when the direction of the causal effects between the mediators is unknown, or there may be unmeasured common causes of the mediators.},
	number = {2},
	urldate = {2022-12-19},
	journal = {Epidemiology (Cambridge, Mass.)},
	author = {Vansteelandt, Stijn and Daniel, Rhian M.},
	month = mar,
	year = {2017},
	pmid = {27922534},
	pmcid = {PMC5289540},
	pages = {258--265},
	file = {PubMed Central Full Text PDF:/Users/ludwigbothmann/Zotero/storage/L8PILZBQ/Vansteelandt und Daniel - 2017 - Interventional effects for mediation analysis with.pdf:application/pdf},
}

@article{donald_estimation_2014,
	title = {Estimation and inference for distribution functions and quantile functions in treatment effect models},
	volume = {178},
	issn = {03044076},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304407613001826},
	doi = {10.1016/j.jeconom.2013.03.010},
	abstract = {We propose inverse probability weighted estimators for the distribution functions of the potential outcomes under the unconfoundedness assumption and apply the inverse mapping to obtain the quantile functions. We show that these estimators converge weakly to zero mean Gaussian processes. A simulation method is proposed to approximate these limiting processes. Based on these results, we construct tests for stochastic dominance relations between the potential outcomes. Monte-Carlo simulations are conducted to examine the finite sample properties of our tests. We apply our test in an empirical example and find that a job training program had a positive effect on incomes.},
	language = {en},
	urldate = {2022-11-28},
	journal = {Journal of Econometrics},
	author = {Donald, Stephen G. and Hsu, Yu-Chin},
	month = jan,
	year = {2014},
	pages = {383--397},
	file = {Donald und Hsu - 2014 - Estimation and inference for distribution function.pdf:/Users/ludwigbothmann/Zotero/storage/ASHRIF5N/Donald und Hsu - 2014 - Estimation and inference for distribution function.pdf:application/pdf},
}

@incollection{balke_counterfactual_1994,
	title = {Counterfactual {Probabilities}: {Computational} {Methods}, {Bounds} and {Applications}},
	isbn = {978-1-55860-332-5},
	shorttitle = {Counterfactual {Probabilities}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9781558603325500110},
	abstract = {Evaluation of counterfactual queries (e.g., "If A were true, would C have been true?") is important to fault diagnosis, planning, and determination of liability. In this paper we present methods for computing the probabilities of such queries using the formulation proposed in [Balke and Pearl, 1994], where the antecedent of the query is interpreted as an external action that forces the proposition A to be true. When a prior probability is available on the causal mechanisms governing the domain, counterfactual probabilities can be evaluated precisely. However, when causal knowledge is specified as conditional probabilities on the observables, only bounds can computed. This paper develops techniques for evaluating these bounds, and demonstrates their use in two applications: (1) the determination of treatment efficacy from studies in which subjects may choose their own treatment, and (2) the determination of liability in product-safety litigation.},
	language = {en},
	urldate = {2022-11-28},
	booktitle = {Uncertainty {Proceedings} 1994},
	publisher = {Elsevier},
	author = {Balke, Alexander and Pearl, Judea},
	year = {1994},
	doi = {10.1016/B978-1-55860-332-5.50011-0},
	pages = {46--54},
	file = {Balke und Pearl - 1994 - Counterfactual Probabilities Computational Method.pdf:/Users/ludwigbothmann/Zotero/storage/XK2EWD4R/Balke und Pearl - 1994 - Counterfactual Probabilities Computational Method.pdf:application/pdf},
}

@misc{rajabi_through_2022,
	title = {Through a fair looking-glass: mitigating bias in image datasets},
	shorttitle = {Through a fair looking-glass},
	url = {http://arxiv.org/abs/2209.08648},
	abstract = {With the recent growth in computer vision applications, the question of how fair and unbiased they are has yet to be explored. There is abundant evidence that the bias present in training data is reﬂected in the models, or even ampliﬁed. Many previous methods for image dataset de-biasing, including models based on augmenting datasets, are computationally expensive to implement. In this study, we present a fast and effective model to de-bias an image dataset through reconstruction and minimizing the statistical dependence between intended variables. Our architecture includes a U-net to reconstruct images, combined with a pre-trained classiﬁer which penalizes the statistical dependence between target attribute and the protected attribute. We evaluate our proposed model on CelebA dataset, compare the results with a state-of-the-art de-biasing method, and show that the model achieves a promising fairness-accuracy combination.},
	language = {en},
	urldate = {2022-11-28},
	publisher = {arXiv},
	author = {Rajabi, Amirarsalan and Yazdani-Jahromi, Mehdi and Garibay, Ozlem Ozmen and Sukthankar, Gita},
	month = sep,
	year = {2022},
	note = {arXiv:2209.08648 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Rajabi et al. - 2022 - Through a fair looking-glass mitigating bias in i.pdf:/Users/ludwigbothmann/Zotero/storage/F29PCYBB/Rajabi et al. - 2022 - Through a fair looking-glass mitigating bias in i.pdf:application/pdf},
}

@article{rubin_causal_2005,
	title = {Causal {Inference} {Using} {Potential} {Outcomes}},
	volume = {100},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214504000001880},
	doi = {10.1198/016214504000001880},
	abstract = {Causal effects are defined as comparisons of potential outcomes under different treatments on a common set of units. Observed values of the potential outcomes are revealed by the assignment mechanism—a probabilistic model for the treatment each unit receives as a function of covariates and potential outcomes. Fisher made tremendous contributions to causal inference through his work on the design of randomized experiments, but the potential outcomes perspective applies to other complex experiments and nonrandomized studies as well. As noted by Kempthorne in his 1976 discussion of Savage's Fisher lecture, Fisher never bridged his work on experimental design and his work on parametric modeling, a bridge that appears nearly automatic with an appropriate view of the potential outcomes framework, where the potential outcomes and covariates are given a Bayesian distribution to complete the model specification. Also, this framework crisply separates scientific inference for causal effects and decisions based on such inference, a distinction evident in Fisher's discussion of tests of significance versus tests in an accept/reject framework. But Fisher never used the potential outcomes framework, originally proposed by Neyman in the context of randomized experiments, and as a result he provided generally flawed advice concerning the use of the analysis of covariance to adjust for posttreatment concomitants in randomized trials.},
	number = {469},
	urldate = {2022-11-23},
	journal = {Journal of the American Statistical Association},
	author = {Rubin, Donald B},
	month = mar,
	year = {2005},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/016214504000001880},
	keywords = {Analysis of covariance, Assignment mechanism, Assignment-based causal inference, Bayesian inference, Direct causal effects, Fieller–Creasy, Fisher, Neyman, Observational studies, Principal stratification, Randomized experiments, Rubin causal model},
	pages = {322--331},
	file = {Rubin - 2005 - Causal Inference Using Potential Outcomes.pdf:/Users/ludwigbothmann/Zotero/storage/W3MGBWQ7/Rubin - 2005 - Causal Inference Using Potential Outcomes.pdf:application/pdf},
}

@inproceedings{balke_counterfactual_1994-1,
	address = {San Francisco, CA, USA},
	series = {{UAI}'94},
	title = {Counterfactual probabilities: computational methods, bounds and applications},
	isbn = {978-1-55860-332-5},
	shorttitle = {Counterfactual probabilities},
	abstract = {Evaluation of counterfactual queries (e.g., "If A were true, would C have been true?") is important to fault diagnosis, planning, and determination of liability. In this paper we present methods for computing the probabilities of such queries using the formulation proposed in [Balke and Pearl, 1994], where the antecedent of the query is interpreted as an external action that forces the proposition A to be true. When a prior probability is available on the causal mechanisms governing the domain, counterfactual probabilities can be evaluated precisely. However, when causal knowledge is specified as conditional probabilities on the observables, only bounds can computed. This paper develops techniques for evaluating these bounds, and demonstrates their use in two applications: (1) the determination of treatment efficacy from studies in which subjects may choose their own treatment, and (2) the determination of liability in product-safety litigation.},
	urldate = {2022-11-23},
	booktitle = {Proceedings of the {Tenth} international conference on {Uncertainty} in artificial intelligence},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Balke, Alexander and Pearl, Judea},
	month = jul,
	year = {1994},
	pages = {46--54},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/RKS9S7RT/Balke und Pearl - 1994 - Counterfactual probabilities computational method.pdf:application/pdf},
}

@article{noauthor_labor_2022,
	title = {Labor {Market} {Institutions} and the {Distribution} of {Wages}, 1973-1992: {A} {Semiparametric} {Approach}},
	language = {en},
	year = {2022},
	pages = {45},
	file = {2022 - Labor Market Institutions and the Distribution of .pdf:/Users/ludwigbothmann/Zotero/storage/4LV5XCGQ/2022 - Labor Market Institutions and the Distribution of .pdf:application/pdf},
}

@incollection{kosta_causal_2019,
	address = {Cham},
	title = {A {Causal} {Bayesian} {Networks} {Viewpoint} on {Fairness}},
	volume = {547},
	isbn = {978-3-030-16743-1 978-3-030-16744-8},
	url = {http://link.springer.com/10.1007/978-3-030-16744-8_1},
	abstract = {We oﬀer a graphical interpretation of unfairness in a dataset as the presence of an unfair causal eﬀect of the sensitive attribute in the causal Bayesian network representing the data-generation mechanism. We use this viewpoint to revisit the recent debate surrounding the COMPAS pretrial risk assessment tool and, more generally, to point out that fairness evaluation on a model requires careful considerations on the patterns of unfairness underlying the training data. We show that causal Bayesian networks provide us with a powerful tool to measure unfairness in a dataset and to design fair models in complex unfairness scenarios.},
	language = {en},
	urldate = {2022-11-23},
	booktitle = {Privacy and {Identity} {Management}. {Fairness}, {Accountability}, and {Transparency} in the {Age} of {Big} {Data}},
	publisher = {Springer International Publishing},
	author = {Chiappa, Silvia and Isaac, William S.},
	editor = {Kosta, Eleni and Pierson, Jo and Slamanig, Daniel and Fischer-Hübner, Simone and Krenn, Stephan},
	year = {2019},
	doi = {10.1007/978-3-030-16744-8_1},
	note = {Series Title: IFIP Advances in Information and Communication Technology},
	pages = {3--20},
	file = {Chiappa und Isaac - 2019 - A Causal Bayesian Networks Viewpoint on Fairness.pdf:/Users/ludwigbothmann/Zotero/storage/RQNVW5GT/Chiappa und Isaac - 2019 - A Causal Bayesian Networks Viewpoint on Fairness.pdf:application/pdf},
}

@article{chiappa_path-specific_2019,
	title = {Path-{Specific} {Counterfactual} {Fairness}},
	volume = {33},
	issn = {2374-3468, 2159-5399},
	doi = {10.1609/aaai.v33i01.33017801},
	abstract = {We consider the problem of learning fair decision systems from data in which a sensitive attribute might affect the decision along both fair and unfair pathways. We introduce a counterfactual approach to disregard effects along unfair pathways that does not incur in the same loss of individual-speciﬁc information as previous approaches. Our method corrects observations adversely affected by the sensitive attribute, and uses these to form a decision. We leverage recent developments in deep learning and approximate inference to develop a VAE-type method that is widely applicable to complex nonlinear models.},
	language = {en},
	number = {01},
	urldate = {2022-11-23},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chiappa, Silvia},
	month = jul,
	year = {2019},
	pages = {7801--7808},
	file = {Chiappa - 2019 - Path-Specific Counterfactual Fairness.pdf:/Users/ludwigbothmann/Zotero/storage/FPRTI5KB/Chiappa - 2019 - Path-Specific Counterfactual Fairness.pdf:application/pdf},
}

@article{noauthor_inference_2013,
	title = {Inference on {Counterfactual} {Distributions}},
	volume = {81},
	issn = {0012-9682},
	url = {http://doi.wiley.com/10.3982/ECTA10582},
	doi = {10.3982/ECTA10582},
	abstract = {Counterfactual distributions are important ingredients for policy analysis and decomposition analysis in empirical economics. In this article, we develop modeling and inference tools for counterfactual distributions based on regression methods. The counterfactual scenarios that we consider consist of ceteris paribus changes in either the distribution of covariates related to the outcome of interest or the conditional distribution of the outcome given covariates. For either of these scenarios, we derive joint functional central limit theorems and bootstrap validity results for regression-based estimators of the status quo and counterfactual outcome distributions. These results allow us to construct simultaneous conﬁdence sets for function-valued effects of the counterfactual changes, including the effects on the entire distribution and quantile functions of the outcome as well as on related functionals. These conﬁdence sets can be used to test functional hypotheses such as no-effect, positive effect, or stochastic dominance. Our theory applies to general counterfactual changes and covers the main regression methods including classical, quantile, duration, and distribution regressions. We illustrate the results with an empirical application to wage decompositions using data for the United States. As a part of developing the main results, we introduce distribution regression as a comprehensive and ﬂexible tool for modeling and estimating the entire conditional distribution. We show that distribution regression encompasses the Cox duration regression and represents a useful alternative to quantile regression. We establish functional central limit theorems and bootstrap validity results for the empirical distribution regression process and various related functionals.},
	language = {en},
	number = {6},
	urldate = {2022-11-23},
	journal = {Econometrica},
	year = {2013},
	pages = {2205--2268},
	file = {2013 - Inference on Counterfactual Distributions.pdf:/Users/ludwigbothmann/Zotero/storage/829UAM5X/2013 - Inference on Counterfactual Distributions.pdf:application/pdf},
}

@article{jang_constructing_2021,
	title = {Constructing a {Fair} {Classifier} with {Generated} {Fair} {Data}},
	volume = {35},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/16965},
	doi = {10.1609/aaai.v35i9.16965},
	abstract = {Fairness in machine learning is getting rising attention as it is directly related to real-world applications and social problems. Recent methods have been explored to alleviate the discrimination between certain demographic groups that are characterized by sensitive attributes (such as race, age, or gender). Some studies have found that the data itself is biased, so training directly on the data causes unfair decision making. Models directly trained on raw data can replicate or even exacerbate bias in the prediction between demographic groups. This leads to vastly different prediction performance in different demographic groups. In order to address this issue, we propose a new approach to improve machine learning fairness by generating fair data. We introduce a generative model to generate cross-domain samples w.r.t. multiple sensitive attributes. This ensures that we can generate inﬁnite number of samples that are balanced w.r.t. both target label and sensitive attributes to enhance fair prediction. By training the classiﬁer solely with the synthetic data and then transfer the model to real data, we can overcome the under-representation problem which is non-trivial since collecting real data is extremely time and resource consuming. We provide empirical evidence to demonstrate the beneﬁt of our model with respect to both fairness and accuracy.},
	language = {en},
	number = {9},
	urldate = {2022-11-23},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Jang, Taeuk and Zheng, Feng and Wang, Xiaoqian},
	month = may,
	year = {2021},
	pages = {7908--7916},
	file = {Jang et al. - 2021 - Constructing a Fair Classifier with Generated Fair.pdf:/Users/ludwigbothmann/Zotero/storage/RBPQZYZW/Jang et al. - 2021 - Constructing a Fair Classifier with Generated Fair.pdf:application/pdf},
}

@article{johansson_learning_nodate,
	title = {Learning {Representations} for {Counterfactual} {Inference}},
	abstract = {Observational studies are rising in importance due to the widespread accumulation of data in ﬁelds such as healthcare, education, employment and ecology. We consider the task of answering counterfactual questions such as, “Would this patient have lower blood sugar had she received a different medication?”. We propose a new algorithmic framework for counterfactual inference which brings together ideas from domain adaptation and representation learning. In addition to a theoretical justiﬁcation, we perform an empirical comparison with previous approaches to causal inference from observational data. Our deep learning algorithm signiﬁcantly outperforms the previous state-of-the-art.},
	language = {en},
	author = {Johansson, Fredrik D and Shalit, Uri and Sontag, David},
	pages = {10},
	file = {Johansson et al. - Learning Representations for Counterfactual Infere.pdf:/Users/ludwigbothmann/Zotero/storage/4GNF42LD/Johansson et al. - Learning Representations for Counterfactual Infere.pdf:application/pdf},
}

@article{kilbertus_fair_nodate,
	title = {Fair {Decisions} {Despite} {Imperfect} {Predictions}},
	abstract = {Consequential decisions are increasingly informed by sophisticated data-driven predictive models. However, consistently learning accurate predictive models requires access to ground truth labels. Unfortunately, in practice, labels may only exist conditional on certain decisions—if a loan is denied, there is not even an option for the individual to pay back the loan. In this paper, we show that, in this selective labels setting, learning to predict is suboptimal in terms of both fairness and utility. To avoid this undesirable behavior, we propose to directly learn stochastic decision policies that maximize utility under fairness constraints. In the context of fair machine learning, our results suggest the need for a paradigm shift from “learning to predict” to “learning to decide”. Experiments on synthetic and real-world data illustrate the favorable properties of learning to decide, in terms of both utility and fairness.},
	language = {en},
	author = {Kilbertus, Niki and Gomez-Rodriguez, Manuel and Schölkopf, Bernhard and Muandet, Krikamol and Valera, Isabel},
	pages = {10},
	file = {Kilbertus et al. - Fair Decisions Despite Imperfect Predictions.pdf:/Users/ludwigbothmann/Zotero/storage/HSM7CZJ5/Kilbertus et al. - Fair Decisions Despite Imperfect Predictions.pdf:application/pdf},
}

@article{calmon_optimized_nodate,
	title = {Optimized {Pre}-{Processing} for {Discrimination} {Prevention}},
	abstract = {Non-discrimination is a recognized objective in algorithmic decision making. In this paper, we introduce a novel probabilistic formulation of data pre-processing for reducing discrimination. We propose a convex optimization for learning a data transformation with three goals: controlling discrimination, limiting distortion in individual data samples, and preserving utility. We characterize the impact of limited sample size in accomplishing this objective. Two instances of the proposed optimization are applied to datasets, including one on real-world criminal recidivism. Results show that discrimination can be greatly reduced at a small cost in classiﬁcation accuracy.},
	language = {en},
	author = {Calmon, Flavio and Wei, Dennis and Vinzamuri, Bhanukiran and Ramamurthy, Karthikeyan Natesan and Varshney, Kush R},
	pages = {10},
	file = {Calmon et al. - Optimized Pre-Processing for Discrimination Preven.pdf:/Users/ludwigbothmann/Zotero/storage/PVPJD85C/Calmon et al. - Optimized Pre-Processing for Discrimination Preven.pdf:application/pdf},
}

@article{zemel_learning_nodate,
	title = {Learning {Fair} {Representations}},
	abstract = {We propose a learning algorithm for fair classiﬁcation that achieves both group fairness (the proportion of members in a protected group receiving positive classiﬁcation is identical to the proportion in the population as a whole), and individual fairness (similar individuals should be treated similarly). We formulate fairness as an optimization problem of ﬁnding a good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group. We show positive results of our algorithm relative to other known techniques, on three datasets. Moreover, we demonstrate several advantages to our approach. First, our intermediate representation can be used for other classiﬁcation tasks (i.e., transfer learning is possible); secondly, we take a step toward learning a distance metric which can ﬁnd important dimensions of the data for classiﬁcation.},
	language = {en},
	author = {Zemel, Richard},
	pages = {9},
	file = {Zemel - Learning Fair Representations.pdf:/Users/ludwigbothmann/Zotero/storage/XDCHANAU/Zemel - Learning Fair Representations.pdf:application/pdf},
}

@article{wang_repairing_nodate,
	title = {Repairing without {Retraining}: {Avoiding} {Disparate} {Impact} with {Counterfactual} {Distributions}},
	abstract = {When the performance of a machine learning model varies over groups deﬁned by sensitive attributes (e.g., gender or ethnicity), the performance disparity can be expressed in terms of the probability distributions of the input and output variables over each group. In this paper, we exploit this fact to reduce the disparate impact of a ﬁxed classiﬁcation model over a population of interest. Given a black-box classiﬁer, we aim to eliminate the performance gap by perturbing the distribution of input variables for the disadvantaged group. We refer to the perturbed distribution as a counterfactual distribution, and characterize its properties for common fairness criteria. We introduce a descent algorithm to learn a counterfactual distribution from data. We then discuss how the estimated distribution can be used to build a data preprocessor that can reduce disparate impact without training a new model. We validate our approach through experiments on real-world datasets, showing that it can repair different forms of disparity without a signiﬁcant drop in accuracy.},
	language = {en},
	author = {Wang, Hao and Ustun, Berk and Calmon, Flavio P},
	pages = {10},
	file = {Wang et al. - Repairing without Retraining Avoiding Disparate I.pdf:/Users/ludwigbothmann/Zotero/storage/3HZQ8KJ8/Wang et al. - Repairing without Retraining Avoiding Disparate I.pdf:application/pdf},
}

@misc{rajabi_through_2022-1,
	title = {Through a fair looking-glass: mitigating bias in image datasets},
	shorttitle = {Through a fair looking-glass},
	url = {http://arxiv.org/abs/2209.08648},
	doi = {10.48550/arXiv.2209.08648},
	abstract = {With the recent growth in computer vision applications, the question of how fair and unbiased they are has yet to be explored. There is abundant evidence that the bias present in training data is reflected in the models, or even amplified. Many previous methods for image dataset de-biasing, including models based on augmenting datasets, are computationally expensive to implement. In this study, we present a fast and effective model to de-bias an image dataset through reconstruction and minimizing the statistical dependence between intended variables. Our architecture includes a U-net to reconstruct images, combined with a pre-trained classifier which penalizes the statistical dependence between target attribute and the protected attribute. We evaluate our proposed model on CelebA dataset, compare the results with a state-of-the-art de-biasing method, and show that the model achieves a promising fairness-accuracy combination.},
	urldate = {2022-11-22},
	publisher = {arXiv},
	author = {Rajabi, Amirarsalan and Yazdani-Jahromi, Mehdi and Garibay, Ozlem Ozmen and Sukthankar, Gita},
	month = sep,
	year = {2022},
	note = {arXiv:2209.08648 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/ludwigbothmann/Zotero/storage/T2DXNLYE/Rajabi et al. - 2022 - Through a fair looking-glass mitigating bias in i.pdf:application/pdf;arXiv.org Snapshot:/Users/ludwigbothmann/Zotero/storage/WVZDDJL5/2209.html:text/html},
}

@inproceedings{wang_repairing_2019,
	title = {Repairing without {Retraining}: {Avoiding} {Disparate} {Impact} with {Counterfactual} {Distributions}},
	shorttitle = {Repairing without {Retraining}},
	url = {https://proceedings.mlr.press/v97/wang19l.html},
	abstract = {When the performance of a machine learning model varies over groups defined by sensitive attributes (e.g., gender or ethnicity), the performance disparity can be expressed in terms of the probability distributions of the input and output variables over each group. In this paper, we exploit this fact to reduce the disparate impact of a fixed classification model over a population of interest. Given a black-box classifier, we aim to eliminate the performance gap by perturbing the distribution of input variables for the disadvantaged group. We refer to the perturbed distribution as a counterfactual distribution, and characterize its properties for common fairness criteria. We introduce a descent algorithm to learn a counterfactual distribution from data. We then discuss how the estimated distribution can be used to build a data preprocessor that can reduce disparate impact without training a new model. We validate our approach through experiments on real-world datasets, showing that it can repair different forms of disparity without a significant drop in accuracy.},
	language = {en},
	urldate = {2022-11-22},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wang, Hao and Ustun, Berk and Calmon, Flavio},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {6618--6627},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/7MBJWET2/Wang et al. - 2019 - Repairing without Retraining Avoiding Disparate I.pdf:application/pdf;Supplementary PDF:/Users/ludwigbothmann/Zotero/storage/KX4TDYLJ/Wang et al. - 2019 - Repairing without Retraining Avoiding Disparate I.pdf:application/pdf},
}

@inproceedings{calmon_optimized_2017,
	title = {Optimized {Pre}-{Processing} for {Discrimination} {Prevention}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/9a49a25d845a483fae4be7e341368e36-Abstract.html},
	abstract = {Non-discrimination is a recognized objective in algorithmic decision making. In this paper, we introduce a novel probabilistic formulation of data pre-processing for reducing discrimination. We propose a convex optimization for learning a data transformation with three goals: controlling discrimination, limiting distortion in individual data samples, and preserving utility. We characterize the impact of limited sample size in accomplishing this objective. Two instances of the proposed optimization are applied to datasets, including one on real-world criminal recidivism. Results show that discrimination can be greatly reduced at a small cost in classification accuracy.},
	urldate = {2022-11-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Calmon, Flavio and Wei, Dennis and Vinzamuri, Bhanukiran and Natesan Ramamurthy, Karthikeyan and Varshney, Kush R},
	year = {2017},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/QQ3YHIPU/Calmon et al. - 2017 - Optimized Pre-Processing for Discrimination Preven.pdf:application/pdf},
}

@article{wei_optimized_2020,
	title = {Optimized {Score} {Transformation} for {Fair} {Classification}},
	volume = {108},
	url = {https://par.nsf.gov/biblio/10202116-optimized-score-transformation-fair-classification},
	abstract = {This paper considers fair probabilistic classification where the outputs of primary interest are predicted probabilities, commonly referred to as scores. We formulate the problem of transforming scores to satisfy fairness constraints while minimizing the loss in utility. The formulation can be applied either to post-process classifier outputs or to pre-process training data, thus allowing maximum freedom in selecting a classification algorithm. We derive a closed-form expression for the optimal transformed scores and a convex optimization problem for the transformation parameters. In the population limit, the transformed score function is the fairness-constrained minimizer of cross-entropy with respect to the optimal unconstrained scores. In the finite sample setting, we propose to approach this solution using a combination of standard probabilistic classifiers and ADMM. Comprehensive experiments comparing to 10 existing methods show that the proposed FairScoreTransformer has advantages for score-based metrics such as Brier score and AUC while remaining competitive for binary label-based metrics such as accuracy.},
	language = {en},
	urldate = {2022-11-22},
	journal = {Proceedings of Machine Learning Research},
	author = {Wei, D. and Ramamurthy, K. N. and Calmon, F. P.},
	month = aug,
	year = {2020},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/X39L9LI8/Wei et al. - 2020 - Optimized Score Transformation for Fair Classifica.pdf:application/pdf;Snapshot:/Users/ludwigbothmann/Zotero/storage/9I5CGTVH/10202116.html:text/html},
}

@article{petersen_causal_2014,
	title = {Causal {Models} and {Learning} from {Data}},
	volume = {25},
	issn = {1044-3983},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4077670/},
	doi = {10.1097/EDE.0000000000000078},
	abstract = {The practice of epidemiology requires asking causal questions. Formal frameworks for causal inference developed over the past decades have the potential to improve the rigor of this process. However, the appropriate role for formal causal thinking in applied epidemiology remains a matter of debate. We argue that a formal causal framework can help in designing a statistical analysis that comes as close as possible to answering the motivating causal question, while making clear what assumptions are required to endow the resulting estimates with a causal interpretation. A systematic approach for the integration of causal modeling with statistical estimation is presented. We highlight some common points of confusion that occur when causal modeling techniques are applied in practice and provide a broad overview on the types of questions that a causal framework can help to address. Our aims are to argue for the utility of formal causal thinking, to clarify what causal models can and cannot do, and to provide an accessible introduction to the flexible and powerful tools provided by causal models.},
	number = {3},
	urldate = {2022-11-15},
	journal = {Epidemiology (Cambridge, Mass.)},
	author = {Petersen, Maya L. and van der Laan, Mark J.},
	month = may,
	year = {2014},
	pmid = {24713881},
	pmcid = {PMC4077670},
	pages = {418--426},
	file = {PubMed Central Full Text PDF:/Users/ludwigbothmann/Zotero/storage/FHR5WQH6/Petersen und van der Laan - 2014 - Causal Models and Learning from Data.pdf:application/pdf},
}

@book{spirtes_causation_1993,
	address = {New York, NY},
	series = {Lecture {Notes} in {Statistics}},
	title = {Causation, {Prediction}, and {Search}},
	volume = {81},
	isbn = {978-1-4612-7650-0 978-1-4612-2748-9},
	url = {http://link.springer.com/10.1007/978-1-4612-2748-9},
	language = {en},
	urldate = {2022-10-26},
	publisher = {Springer New York},
	author = {Spirtes, Peter and Glymour, Clark and Scheines, Richard},
	editor = {Berger, J. and Fienberg, S. and Gani, J. and Krickeberg, K. and Olkin, I. and Singer, B.},
	year = {1993},
	doi = {10.1007/978-1-4612-2748-9},
	file = {Spirtes et al. - 1993 - Causation, Prediction, and Search.pdf:/Users/ludwigbothmann/Zotero/storage/2EAX8W4Z/Spirtes et al. - 1993 - Causation, Prediction, and Search.pdf:application/pdf},
}

@misc{makhlouf_survey_2022,
	title = {Survey on {Causal}-based {Machine} {Learning} {Fairness} {Notions}},
	doi = {10.48550/arXiv.2010.09553},
	abstract = {Addressing the problem of fairness is crucial to safely use machine learning algorithms to support decisions with a critical impact on people’s lives such as job hiring, child maltreatment, disease diagnosis, loan granting, etc. Several notions of fairness have been deﬁned and examined in the past decade, such as statistical parity and equalized odds. The most recent fairness notions, however, are causal-based and reﬂect the now widely accepted idea that using causality is necessary to appropriately address the problem of fairness. This paper examines an exhaustive list of causal-based fairness notions and study their applicability in real-world scenarios. As the majority of causal-based fairness notions are deﬁned in terms of non-observable quantities (e.g., interventions and counterfactuals), their deployment in practice requires to compute or estimate those quantities using observational data. This paper offers a comprehensive report of the diﬀerent approaches to infer causal quantities from observational data including identiﬁability (Pearl’s SCM framework) and estimation (potential outcome framework). The main contributions of this survey paper are (1) a guideline to help selecting a suitable fairness notion given a speciﬁc realworld scenario, and (2) a ranking of the fairness notions according to Pearl’s causation ladder indicating how diﬃcult it is to deploy each notion in practice.},
	language = {en},
	urldate = {2022-10-19},
	publisher = {arXiv},
	author = {Makhlouf, Karima and Zhioua, Sami and Palamidessi, Catuscia},
	month = jun,
	year = {2022},
	note = {doi: 10.48550/arXiv.2010.09553},
	keywords = {Computer Science - Machine Learning},
	file = {Makhlouf et al. - 2022 - Survey on Causal-based Machine Learning Fairness N.pdf:/Users/ludwigbothmann/Zotero/storage/S9C8B392/Makhlouf et al. - 2022 - Survey on Causal-based Machine Learning Fairness N.pdf:application/pdf},
}

@inproceedings{pan_explaining_2021,
	address = {Virtual Event Singapore},
	title = {Explaining {Algorithmic} {Fairness} {Through} {Fairness}-{Aware} {Causal} {Path} {Decomposition}},
	isbn = {978-1-4503-8332-5},
	url = {https://dl.acm.org/doi/10.1145/3447548.3467258},
	doi = {10.1145/3447548.3467258},
	abstract = {Algorithmic fairness has aroused considerable interests in data mining and machine learning communities recently. So far the existing research has been mostly focusing on the development of quantitative metrics to measure algorithm disparities across different protected groups, and approaches for adjusting the algorithm output to reduce such disparities. In this paper, we propose to study the problem of identification of the source of model disparities. Unlike existing interpretation methods which typically learn feature importance, we consider the causal relationships among feature variables and propose a novel framework to decompose the disparity into the sum of contributions from fairness-aware causal paths, which are paths linking the sensitive attribute and the final predictions, on the graph. We also consider the scenario when the directions on certain edges within those paths cannot be determined. Our framework is also model agnostic and applicable to a variety of quantitative disparity measures. Empirical evaluations on both synthetic and real-world data sets are provided to show that our method can provide precise and comprehensive explanations to the model disparities.},
	language = {en},
	urldate = {2022-10-11},
	booktitle = {Proceedings of the 27th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Pan, Weishen and Cui, Sen and Bian, Jiang and Zhang, Changshui and Wang, Fei},
	month = aug,
	year = {2021},
	pages = {1287--1297},
	file = {Pan et al. - 2021 - Explaining Algorithmic Fairness Through Fairness-A.pdf:/Users/ludwigbothmann/Zotero/storage/2VTP29YC/Pan et al. - 2021 - Explaining Algorithmic Fairness Through Fairness-A.pdf:application/pdf},
}

@article{nogueira_methods_2022,
	title = {Methods and tools for causal discovery and causal inference},
	issn = {1942-4795},
	doi = {10.1002/widm.1449},
	abstract = {Causality is a complex concept, which roots its developments across several fields, such as statistics, economics, epidemiology, computer science, and philosophy. In recent years, the study of causal relationships has become a crucial part of the Artificial Intelligence community, as causality can be a key tool for overcoming some limitations of correlation-based Machine Learning systems. Causality research can generally be divided into two main branches, that is, causal discovery and causal inference. The former focuses on obtaining causal knowledge directly from observational data. The latter aims to estimate the impact deriving from a change of a certain variable over an outcome of interest. This article aims at covering several methodologies that have been developed for both tasks. This survey does not only focus on theoretical aspects. But also provides a practical toolkit for interested researchers and practitioners, including software, datasets, and running examples. This article is categorized under: Algorithmic Development {\textgreater} Causality Discovery Fundamental Concepts of Data and Knowledge {\textgreater} Explainable AI Technologies {\textgreater} Machine Learning},
	language = {en},
	urldate = {2022-10-11},
	journal = {WIREs Data Mining and Knowledge Discovery},
	author = {Nogueira, Ana Rita and Pugnana, Andrea and Ruggieri, Salvatore and Pedreschi, Dino and Gama, João},
	year = {2022},
	keywords = {causal discovery, causal inference, causality},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/F6T6RNQE/Nogueira et al. - 2022 - Methods and tools for causal discovery and causal .pdf:application/pdf;Snapshot:/Users/ludwigbothmann/Zotero/storage/MNFWVUJY/widm.html:text/html},
}

@article{malinsky_causal_2018,
	title = {Causal discovery algorithms: {A} practical guide},
	volume = {13},
	issn = {1747-9991},
	shorttitle = {Causal discovery algorithms},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/phc3.12470},
	doi = {10.1111/phc3.12470},
	abstract = {Many investigations into the world, including philosophical ones, aim to discover causal knowledge, and many experimental methods have been developed to assist in causal discovery. More recently, algorithms have emerged that can also learn causal structure from purely or mostly observational data, as well as experimental data. These methods have started to be applied in various philosophical contexts, such as debates about our concepts of free will and determinism. This paper provides a “user's guide” to these methods, though not in the sense of specifying exact button presses in a software package. Instead, we explain the larger “pipeline” within which these methods are used and discuss key steps in moving from initial research idea to validated causal structure.},
	language = {en},
	number = {1},
	urldate = {2022-10-11},
	journal = {Philosophy Compass},
	author = {Malinsky, Daniel and Danks, David},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/phc3.12470},
	pages = {e12470},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/X3ZLTIA2/Malinsky und Danks - 2018 - Causal discovery algorithms A practical guide.pdf:application/pdf;Snapshot:/Users/ludwigbothmann/Zotero/storage/KL97A5A4/phc3.html:text/html},
}

@article{glymour_review_2019,
	title = {Review of {Causal} {Discovery} {Methods} {Based} on {Graphical} {Models}},
	volume = {10},
	issn = {1664-8021},
	url = {https://www.frontiersin.org/article/10.3389/fgene.2019.00524/full},
	doi = {10.3389/fgene.2019.00524},
	abstract = {A fundamental task in various disciplines of science, including biology, is to ﬁnd underlying causal relations and make use of them. Causal relations can be seen if interventions are properly applied; however, in many cases they are difﬁcult or even impossible to conduct. It is then necessary to discover causal relations by analyzing statistical properties of purely observational data, which is known as causal discovery or causal structure search. This paper aims to give a introduction to and a brief review of the computational methods for causal discovery that were developed in the past three decades, including constraint-based and score-based methods and those based on functional causal models, supplemented by some illustrations and applications.},
	language = {en},
	urldate = {2022-10-11},
	journal = {Frontiers in Genetics},
	author = {Glymour, Clark and Zhang, Kun and Spirtes, Peter},
	month = jun,
	year = {2019},
	pages = {524},
	file = {Glymour et al. - 2019 - Review of Causal Discovery Methods Based on Graphi.pdf:/Users/ludwigbothmann/Zotero/storage/DQQNBXMW/Glymour et al. - 2019 - Review of Causal Discovery Methods Based on Graphi.pdf:application/pdf},
}

@misc{friedler_impossibility_2016,
	title = {On the (im)possibility of fairness},
	doi = {10.48550/arXiv.1609.07236},
	abstract = {What does it mean for an algorithm to be fair? Different papers use different notions of algorithmic fairness, and although these appear internally consistent, they also seem mutually incompatible. We present a mathematical setting in which the distinctions in previous papers can be made formal. In addition to characterizing the spaces of inputs (the "observed" space) and outputs (the "decision" space), we introduce the notion of a construct space: a space that captures unobservable, but meaningful variables for the prediction. We show that in order to prove desirable properties of the entire decision-making process, different mechanisms for fairness require different assumptions about the nature of the mapping from construct space to decision space. The results in this paper imply that future treatments of algorithmic fairness should more explicitly state assumptions about the relationship between constructs and observations.},
	urldate = {2022-03-02},
	author = {Friedler, Sorelle A. and Scheidegger, Carlos and Venkatasubramanian, Suresh},
	month = sep,
	year = {2016},
	keywords = {Computer Science - Computers and Society, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ludwigbothmann/Zotero/storage/XTUL6A7F/Friedler et al. - 2016 - On the (im)possibility of fairness.pdf:application/pdf;arXiv Fulltext PDF:/Users/ludwigbothmann/Zotero/storage/BJDYIC69/Friedler et al. - 2016 - On the (im)possibility of fairness.pdf:application/pdf;arXiv.org Snapshot:/Users/ludwigbothmann/Zotero/storage/96DY7GKV/1609.html:text/html;arXiv.org Snapshot:/Users/ludwigbothmann/Zotero/storage/YAIQ8WP8/1609.html:text/html},
}

@inproceedings{chikahara_learning_2021,
	title = {Learning {Individually} {Fair} {Classifier} with {Path}-{Specific} {Causal}-{Effect} {Constraint}},
	url = {https://proceedings.mlr.press/v130/chikahara21a.html},
	abstract = {Machine learning is used to make decisions for individuals in various fields, which require us to achieve good prediction accuracy while ensuring fairness with respect to sensitive features (e.g., race and gender). This problem, however, remains difficult in complex real-world scenarios. To quantify unfairness under such situations, existing methods utilize path-specific causal effects. However, none of them can ensure fairness for each individual without making impractical functional assumptions about the data. In this paper, we propose a far more practical framework for learning an individually fair classifier. To avoid restrictive functional assumptions, we define the probability of individual unfairness (PIU) and solve an optimization problem where PIU’s upper bound, which can be estimated from data, is controlled to be close to zero. We elucidate why our method can guarantee fairness for each individual. Experimental results show that our method can learn an individually fair classifier at a slight cost of accuracy.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of {The} 24th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Chikahara, Yoichi and Sakaue, Shinsaku and Fujino, Akinori and Kashima, Hisashi},
	month = mar,
	year = {2021},
	pages = {145--153},
	file = {chikahara21a.pdf:/Users/ludwigbothmann/Zotero/storage/WA4QKQP9/chikahara21a.pdf:application/pdf;Full Text PDF:/Users/ludwigbothmann/Zotero/storage/2IC6MEFX/Chikahara et al. - 2021 - Learning Individually Fair Classifier with Path-Sp.pdf:application/pdf;Supplementary PDF:/Users/ludwigbothmann/Zotero/storage/V37AM9XX/Chikahara et al. - 2021 - Learning Individually Fair Classifier with Path-Sp.pdf:application/pdf},
}

@book{pearl_causality_2009,
	edition = {2nd},
	title = {Causality: {Models}, {Reasoning} and {Inference}},
	url = {https://yzhu.io/courses/core/reading/04.causality.pdf},
	publisher = {Cambridge University Press},
	author = {Pearl, Judea},
	year = {2009},
	file = {Pearl - 2000 - Causality Models, Reasoning and Inference.pdf:/Users/ludwigbothmann/Zotero/storage/RP2BCQ8X/Pearl - 2000 - Causality Models, Reasoning and Inference.pdf:application/pdf},
}

@inproceedings{kusner_counterfactual_2017,
	title = {Counterfactual {Fairness}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kusner, Matt J and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
	editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	file = {Kusner et al. - Counterfactual Fairness.pdf:/Users/ludwigbothmann/Zotero/storage/S6WBBP7N/Kusner et al. - Counterfactual Fairness.pdf:application/pdf},
}

@book{aristotle_nicomachean_2009,
	series = {Oxford {World}'s {Classics}},
	title = {The {Nicomachean} ethics (book {V})},
	isbn = {978-0-19-921361-0},
	publisher = {Oxford University Press},
	author = {Aristotle},
	editor = {Ross, D and Brown, L},
	year = {2009},
	doi = {10.1093/actrade/9780199213610.book.1},
}

@book{aristoteles_aristotelis_1831,
	series = {ex rec. {Immanuelis} {Bekkeri} ed. {Acad}. {Regia} {Borussica}},
	title = {Aristotelis {Opera}},
	volume = {2},
	publisher = {de Gruyter},
	author = {Aristoteles},
	editor = {Bekker, Immanuel},
	year = {1831},
	note = {Book V},
}

@misc{chouldechova_frontiers_2018,
	title = {The {Frontiers} of {Fairness} in {Machine} {Learning}},
	doi = {10.48550/arXiv.1810.08810},
	abstract = {The last few years have seen an explosion of academic and popular interest in algorithmic fairness. Despite this interest and the volume and velocity of work that has been produced recently, the fundamental science of fairness in machine learning is still in a nascent state. In March 2018, we convened a group of experts as part of a CCC visioning workshop to assess the state of the field, and distill the most promising research directions going forward. This report summarizes the findings of that workshop. Along the way, it surveys recent theoretical work in the field and points towards promising directions for research.},
	urldate = {2022-04-01},
	author = {Chouldechova, Alexandra and Roth, Aaron},
	month = oct,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Data Structures and Algorithms, Computer Science - Computer Science and Game Theory},
	file = {1810.08810.pdf:/Users/ludwigbothmann/Zotero/storage/86CEWXLQ/1810.08810.pdf:application/pdf;arXiv Fulltext PDF:/Users/ludwigbothmann/Zotero/storage/5RQ68ZDE/Chouldechova and Roth - 2018 - The Frontiers of Fairness in Machine Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/ludwigbothmann/Zotero/storage/RMBENK83/1810.html:text/html},
}

@book{barocas_fairness_2019,
	title = {Fairness and {Machine} {Learning}},
	url = {http://www.fairmlbook.org},
	publisher = {fairmlbook.org},
	author = {Barocas, Solon and Hardt, Moritz and Narayanan, Arvind},
	year = {2019},
}

@inproceedings{bechavod_metric-free_2020,
	title = {Metric-{Free} {Individual} {Fairness} in {Online} {Learning}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/file/80b618ebcac7aa97a6dac2ba65cb7e36-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bechavod, Yahav and Jung, Christopher and Wu, Steven Z.},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {11214--11225},
	file = {Bechavod et al. - Metric-Free Individual Fairness in Online Learning.pdf:/Users/ludwigbothmann/Zotero/storage/VFTTJNJT/Bechavod et al. - Metric-Free Individual Fairness in Online Learning.pdf:application/pdf},
}

@article{weinberger_path-specific_2019,
	title = {Path-{Specific} {Effects}},
	volume = {70},
	issn = {0007-0882, 1464-3537},
	doi = {10.1093/bjps/axx040},
	abstract = {A cause may influence its effect via multiple paths. Paradigmatically (Hesslow, 1974), taking birth control pills both decreases one’s risk of thrombosis by preventing pregnancy and increases it by producing a blood chemical. Building on Pearl (2001), I explicate the notion of a path-specific effect. Roughly, a path-specific effect of C on E via path P is the degree to which a change in C would change E were they to be transmitted only via P. Facts about such effects may be gleaned from the structural equations commonly used to represent the causal relationships among variables. I contrast my analysis of the Hesslow case with those given by theorists of probabilistic causality, who mistakenly link it to issues of causal heterogeneity, token-causation and indeterminism. The reason probabilistic theories misdiagnose this case is that they pay inadequate attention to the structural relationships among variables.},
	language = {en},
	number = {1},
	urldate = {2023-05-30},
	journal = {The British Journal for the Philosophy of Science},
	author = {Weinberger, Naftali},
	month = mar,
	year = {2019},
	pages = {53--76},
	file = {Weinberger - 2019 - Path-Specific Effects.pdf:/Users/ludwigbothmann/Zotero/storage/9NJITM2K/Weinberger - 2019 - Path-Specific Effects.pdf:application/pdf},
}

@inproceedings{zhang_fairness_2018,
	title = {Fairness in {Decision}-{Making} — {The} {Causal} {Explanation} {Formula}},
	copyright = {Copyright (c)},
	doi = {10.1609/aaai.v32i1.11564},
	abstract = {AI plays an increasingly prominent role in society since decisions that were once made by humans are now delegated to automated systems. These systems are currently in charge of deciding bank loans, criminals' incarceration, and the hiring of new employees, and it's not difficult to envision that they will in the future underpin most of the decisions in society. Despite the high complexity entailed by this task, there is still not much understanding of basic properties of such systems. For instance, we currently cannot detect (neither explain nor correct) whether an AI system can be deemed fair (i.e., is abiding by the decision-constraints agreed by society) or it is reinforcing biases and perpetuating a preceding prejudicial practice. Issues of discrimination have been discussed extensively in political and legal circles, but there exists still not much understanding of the formal conditions that a system must meet to be deemed fair. In this paper, we use the language of structural causality (Pearl, 2000) to fill in this gap. We start by introducing three new fine-grained measures of transmission of change from stimulus to effect, which we called counterfactual direct (Ctf-DE), indirect (Ctf-IE), and spurious (Ctf-SE) effects. We then derive what we call the causal explanation formula, which allows the AI designer to quantitatively evaluate fairness and explain the total observed disparity of decisions through different discriminatory mechanisms. We apply these measures to various discrimination analysis tasks and run extensive simulations, including detection, evaluation, and optimization of decision-making under fairness constraints. We conclude studying the trade-off between different types of fairness criteria (outcome and procedural), and provide a quantitative approach to policy implementation and the design of fair AI systems.},
	language = {en},
	urldate = {2023-05-30},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Zhang, Junzhe and Bareinboim, Elias},
	month = apr,
	year = {2018},
	keywords = {and Causality, Change, Knowledge Representation and Reasoning: Action},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/WUIHKIED/Zhang und Bareinboim - 2018 - Fairness in Decision-Making — The Causal Explanati.pdf:application/pdf},
}

@misc{hejazi_efficient_2022,
	title = {Efficient estimation of modified treatment policy effects based on the generalized propensity score},
	doi = {10.48550/arXiv.2205.05777},
	author = {Hejazi, Nima S and Benkeser, David and Díaz, Iván and van der Laan, Mark J},
	year = {2022},
}

@book{hernan_causal_2020,
	title = {Causal {Inference}: {What} {If}},
	language = {en},
	publisher = {Boca Raton: Chapman \& Hall/CRC},
	author = {Hernán, Miguel A and Robins, James M},
	year = {2020},
	file = {Hernan und Robins - Causal Inference What If.pdf:/Users/ludwigbothmann/Zotero/storage/V2NIS8NV/Hernan und Robins - Causal Inference What If.pdf:application/pdf},
}

@inproceedings{hu_whats_2020,
	title = {What's {Sex} {Got} {To} {Do} {With} {Fair} {Machine} {Learning}?},
	url = {http://arxiv.org/abs/2006.01770},
	doi = {10.1145/3351095.3375674},
	abstract = {Debate about fairness in machine learning has largely centered around competing definitions of what fairness or nondiscrimination between groups requires. However, little attention has been paid to what precisely a group is. Many recent approaches to "fairness" require one to specify a causal model of the data generating process. These exercises make an implicit ontological assumption that a racial or sex group is simply a collection of individuals who share a given trait. We show this by exploring the formal assumption of modularity in causal models, which holds that the dependencies captured by one causal pathway are invariant to interventions on any other pathways. Causal models of sex propose two substantive claims: 1) There exists a feature, sex-on-its-own, that is an inherent trait of an individual that causally brings about social phenomena external to it in the world; and 2) the relations between sex and its effects can be modified in whichever ways and the former feature would still retain the meaning that sex has in our world. We argue that this ontological picture is false. Many of the "effects" that sex purportedly "causes" are in fact constitutive features of sex as a social status. They give the social meaning of sex features, meanings that are precisely what make sex discrimination a distinctively morally problematic type of action. Correcting this conceptual error has a number of implications for how models can be used to detect discrimination. Formal diagrams of constitutive relations present an entirely different path toward reasoning about discrimination. Whereas causal diagrams guide the construction of sophisticated modular counterfactuals, constitutive diagrams identify a different kind of counterfactual as central to an inquiry on discrimination: one that asks how the social meaning of a group would be changed if its non-modular features were altered.},
	urldate = {2023-05-30},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	author = {Hu, Lily and Kohler-Hausmann, Issa},
	month = jan,
	year = {2020},
	note = {arXiv:2006.01770 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	pages = {513--513},
	file = {arXiv.org Snapshot:/Users/ludwigbothmann/Zotero/storage/PTZQ4X5B/2006.html:text/html;Full Text PDF:/Users/ludwigbothmann/Zotero/storage/S6SYSF7G/Hu und Kohler-Hausmann - 2020 - What's Sex Got To Do With Fair Machine Learning.pdf:application/pdf},
}

@inproceedings{zhang_equality_2018,
	title = {Equality of {Opportunity} in {Classification}: {A} {Causal} {Approach}},
	volume = {31},
	shorttitle = {Equality of {Opportunity} in {Classification}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/hash/ff1418e8cc993fe8abcfe3ce2003e5c5-Abstract.html},
	abstract = {The Equalized Odds (for short, EO)  is one of the most popular measures of discrimination used in the supervised learning setting. It ascertains fairness through the balance of the misclassification rates (false positive and negative) across the protected groups -- e.g., in the context of law enforcement, an African-American defendant who would not commit a future crime will have an equal opportunity of being released, compared to a non-recidivating Caucasian defendant. Despite this noble goal, it has been acknowledged in the literature that statistical tests based on the EO are oblivious to the underlying causal mechanisms that generated the disparity in the first place (Hardt et al. 2016). This leads to a critical disconnect between statistical measures readable from the data and the meaning of discrimination in the legal system, where compelling evidence that the observed disparity is tied to a specific causal process deemed unfair by society is required to characterize discrimination. The goal of this paper is to develop a principled approach to connect the statistical disparities characterized by the EO  and the underlying, elusive, and frequently unobserved, causal mechanisms that generated such inequality. We start by introducing a new family of counterfactual measures that allows one to explain the misclassification disparities in terms of the underlying mechanisms in an arbitrary, non-parametric structural causal model. This will, in turn, allow legal and data analysts to interpret currently deployed classifiers through causal lens, linking the statistical disparities found in the data to the corresponding causal processes. Leveraging the new family of counterfactual measures, we develop a learning procedure to construct a classifier that is statistically efficient, interpretable, and compatible with the basic human intuition of fairness. We demonstrate our results through experiments in both real (COMPAS) and synthetic datasets.},
	urldate = {2023-05-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Junzhe and Bareinboim, Elias},
	year = {2018},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/UDVAF2JS/Zhang und Bareinboim - 2018 - Equality of Opportunity in Classification A Causa.pdf:application/pdf},
}

@article{plecko_fair_2020,
	title = {Fair {Data} {Adaptation} with {Quantile} {Preservation}},
	volume = {21},
	issn = {1532-4435},
	abstract = {Fairness of classification and regression has received much attention recently and various, partially non-compatible, criteria have been proposed. The fairness criteria can be enforced for a given classifier or, alternatively, the data can be adapted to ensure that every classifier trained on the data will adhere to desired fairness criteria. We present a practical data adaption method based on quantile preservation in causal structural equation models. The data adaptation is based on a presumed counterfactual model for the data. While the counterfactual model itself cannot be verified experimentally, we show that certain population notions of fairness are still guaranteed even if the counterfactual model is misspecified. The nature of the fulfilled observational non-causal fairness notion (such as demographic parity, separation or sufficiency) depends on the structure of the underlying causal model and the choice of resolving variables. We describe an implementation of the proposed data adaptation procedure based on Random Forests (Breiman, 2001) and demonstrate its practical use on simulated and real-world data.},
	number = {1},
	journal = {J. Mach. Learn. Res.},
	author = {Plečko, Drago and Meinshausen, Nicolai},
	month = jan,
	year = {2020},
	note = {Publisher: JMLR.org},
	keywords = {causality, counterfactual fairness, fairness in machine learning, graphical models, supervised learning},
	file = {Pleˇcko und Meinshausen - Fair Data Adaptation with Quantile Preservation.pdf:/Users/ludwigbothmann/Zotero/storage/BULF2QM8/Pleˇcko und Meinshausen - Fair Data Adaptation with Quantile Preservation.pdf:application/pdf},
}
