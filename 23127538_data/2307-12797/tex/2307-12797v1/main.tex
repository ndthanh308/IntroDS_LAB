%% The first command in your LaTeX source must be the \documentclass command.
%%
%% Options:
%% twocolumn : Two column layout.
%% hf: enable header and footer.
\documentclass[
%twocolumn,
%hf,
]{ceurart}

%%
%% One can fix some overfulls
\sloppy

%%
%% Minted listings support
%% Need pygment <http://pygments.org/> <http://pypi.python.org/pypi/Pygments>
\usepackage{listings}
%% auto break lines
\lstset{breaklines=true}

%%%%%%%%%%%%%%%%%%
% Our packages
\usepackage{enumitem}
\usepackage{bm}             % Bold greek letters
\usepackage{mathtools} % For := sign
%\usepackage{amsfonts}
%\usepackage{amsmath}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric}
\usepackage{url}            % simple URL typesetting
\usepackage{caption}
\usepackage{subcaption}
%%%%%%%%%%%%%%%%%%

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

\include{basic_math}
\include{basic_ml}
\include{macros}
\include{ml-svm}

%%
%% Rights management information.
%% CC-BY is default license.
\copyrightyear{2023}
\copyrightclause{Copyright for this paper by its authors.
  Use permitted under Creative Commons License Attribution 4.0
  International (CC BY 4.0).}

%%
%% This command is for the conference information
\conference{AEQUITAS 2023 -- Workshop on Fairness and Bias in AI at 26th European Conference on Artificial Intelligence}% (ECAI) 2023}

%%
%% The "title" command
\title{Causal Fair Machine Learning via Rank-Preserving Interventional Distributions}
%\title{Causal Fair Machine Learning -- Generating and Evaluating Fair Counterfactual Data Sets}

% \tnotemark[1]
% \tnotetext[1]{You can use this document as the template for preparing your
%   publication. We recommend using the latest version of the ceurart style.}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
\author[1,2]{Ludwig Bothmann}[%
orcid=0000-0002-1471-6582,
email=ludwig.bothmann@stat.uni-muenchen.de,
url=https://www.slds.stat.uni-muenchen.de/people/bothmann/,
]
\cormark[1]
%\fnmark[1]
\address[1]{Department of Statistics, LMU Munich, Germany}
\address[2]{Munich Center for Machine Learning (MCML)}
\address[3]{Center for Infectious Disease Epidemiology, School of Public Health, University of Cape Town, South Africa}

\author[1,2]{Susanne Dandl}[%
orcid=0000-0003-4324-4163,
email=susanne.dandl@stat.uni-muenchen.de,
%url=https://kmitd.github.io/ilaria/,
]


\author[1,3]{Michael Schomaker}[%
orcid=0000-0002-8475-0591,
email=michael.schomaker@stat.uni-muenchen.de,
%url=http://conceptbase.sourceforge.net/mjf/,
]

%% Footnotes
\cortext[1]{Corresponding author.}
%\fntext[1]{These authors contributed equally.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  % fairML
  % causality
  % FiND world bzgl, derer die Gleichheit festgetsellt wird
  % modified interventional distributions als estimand
  % warping procedure als estimator
  % evaluation criteria
  % simulation study
  % applied use-case
  A decision can be defined as fair if equal individuals are treated equally and unequals unequally. Adopting this definition, the task of designing machine learning models that mitigate unfairness in automated decision-making systems must include causal thinking when introducing protected attributes. Following a recent proposal, we define individuals as being normatively equal if they are equal in a fictitious, normatively desired (FiND) world, where the protected attribute has no (direct or indirect) causal effect on the target. We propose \myemph{rank-preserving interventional distributions} to define an estimand of this FiND world and a \myemph{warping method} for estimation.
  Evaluation criteria for both the method and resulting model are presented and validated through simulations and empirical data. With this, we show that our warping approach effectively identifies the most discriminated individuals and mitigates unfairness.
\end{abstract}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\begin{keywords}
  fairness in ML \sep
  causal thinking \sep
%  automated decision making \sep
  interventional distributions \sep
  stochastic interventions \sep
  rank-preserving interventions \sep
  quasi-individual fairness
\end{keywords}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle


\section{Introduction}
\label{sec:intro}

Automated decision-making (ADM) systems can support human decision-makers by predicting some variable of interest via a machine learning (ML) model. The data used for learning such ML models can have historical bias, i.e., show normatively undesirable discrimination against certain groups of protected attributes (PAs). When left unaddressed, this historical bias leads to biased ML models, generating fairness problems in ADM systems.
%
The research field of fair machine learning (fairML) has quickly grown around this problem in recent years, giving birth to various ``fairness metrics'' (such as, e.g., demographical parity, see \cite{barocas_fairness_2019} for an overview).
%However, as \citep{bothmann_what_2023} showed recently, these fairness metrics often lack a solid philosophical foundation making it unclear which concept of fairness is measured by the respective metrics.

A critique raised by \cite{bothmann_what_2023} is that the question of \myemph{what fairness is} -- as a philosophical concept -- is rarely discussed. Hence, the proposed fairness metrics lack a philosophical justification, making it unclear which concept of fairness is measured by the respective metrics.
They propose a consistent concept of fairness and outline how this should be integrated into the design of ML models in ADM systems.
Following Aristotle \cite{aristoteles_aristotelis_1831}, they define a treatment as being fair ``if equals are treated equally and if unequals are treated unequally''. Furthermore, they distinguish between descriptively unfair treatment (which can occur without PAs) and normatively unfair treatment (which is a causal notion). For this, they conceive a \myemph{fictitious, normatively desired (FiND) world}, where the PA has no (direct or indirect) causal effect on the target variable. Individuals are normatively considered equal if they are equal in the FiND world. %, and leave concrete algorithms to future research.

We build upon this work by proposing concrete estimands and estimation procedures.
%as well as by proposing evaluation methods and by presenting experimental and real-world results.
As a starting point, we define a directed acyclic graph (DAG) that describes the causal relations in the real world. %The DAG might be designed by pure expert knowledge or supported by methods of causal discovery \citep[see, e.g.,][for an overview]{nogueira_methods_2022}.
The DAG in the FiND world is then created by deleting all arrows that constitute paths from the PA to the target. %, see Figure \ref{fig:DAG} for an example.
We achieve this through specific stochastic interventions leading to \myemph{rank-preserving interventional distributions}.
%on the mediators, such that the mediator distributions do not differ with respect to the groups defined by the PA.
This intervention is rank-preserving in the sense that individuals of the disadvantaged group maintain the group-specific real-world rank (in variables that are intervened on) as population-wide FiND-world rank (see Section \ref{sec:estimand}).
%While there is a growing body of literature for path-specific effects, both in philosophy \citep[e.g.,][]{weinberger_path-specific_2019} and ML \citep[e.g.,][]{chikahara_learning_2021}, we believe that this approach of deleting all the causal effects of the PA resembles more closely the legal necessities of not discriminating based on the PAs. %(however, we would be very interested in a discussion on this question at the workshop).

After identifying the estimand, we propose a \myemph{warping method} for estimation that maps real-world data to a \myemph{warped world} which in turn approximates the FiND world, see Section \ref{sec:training}.
We call this a \myemph{quasi-individual} approach because individual ``merits'' are pulled through to the warped world. % via the quantiles of the residuals.
Finally, an ML model is trained on the warped data that can be used at prediction time after warping the new observation.
%For a new observation at prediction time, first, the warping (as learned during training) is applied, and then the target is predicted with the warped data ML model.
We propose evaluation metrics both for evaluating the warping method in a simulation study and for evaluating an ML model using warped data in an applied use case in Section \ref{sec:evaluation}. %With these metrics, it is possible to (A) identify the individuals that profit or suffer most from unfair discrimination in the real world and (B) identify features that are (globally) most relevant for the discrimination in the real world.
In a simulation study, we show that our warping method is able to approximate the FiND world, identify the most discriminated individuals, and eliminate the effects of the PA in the warped world (Section \ref{sec:results_sim}).
Finally, we apply the proposed methodology to German Credit data, showing how to use our framework in practice (Section \ref{sec:results_german}).

% \bp{Fairnessmetriken messen nicht wirklich Fairness, siehe \citep{bothmann_what_2023}}
% \bp{2 Welten}
% \bp{Hier schlagen wir vor, wie man das konkret umsetzt}
% \bp{Außerdem, wie man das evaluieren kann}
% \bp{auf simulierten Daten und echten Daten}


\section{Related Work}
\label{sec:related-work}

In addition to \myemph{group fairness} concepts (see, e.g., \citep[][]{verma_fairness_2018} for an overview), approaches of (non-causal) \myemph{individual fairness} have been proposed, starting with \cite{dwork_fairness_2012}, who require that similar individuals should be treated similarly (see also \citep{bechavod_metric-free_2020, chouldechova_frontiers_2018, friedler_impossibility_2016}).
An early notion of \myemph{causal fairness} was made by \citep{kusner_counterfactual_2017}, who conceive a fictitious world where an individual belongs to a different subgroup of the PA, defining a decision as fair if it is equal in the real and fictitious world. For a thorough explanation of how this differs from our FiND world, see \cite{bothmann_what_2023}.
Including causality in the fairness debate and conceiving a fictitious world was also proposed by, e.g., \cite{zhang_equality_2018, zhang_fairness_2018, nabi_fair_2018, nabi_learning_2019, nabi_optimal_2022, pfohl_counterfactual_2019}, where different ideas underlie those fictitious worlds.
%While there is a growing body of literature for path-specific effects, both in philosophy \citep[e.g.,][]{weinberger_path-specific_2019} and ML \citep[e.g.,][]{chiappa_path-specific_2019, chikahara_learning_2021}, we believe that our approach of deleting all the causal effects of the PA resembles more closely the legal necessities of not discriminating based on the PAs.
%
With the fairness concept introduced by \citep{bothmann_what_2023}, we distinguish the real world and the FiND world by the idea that in the FiND world, there must be no causal effects from the PA on the target -- neither indirectly, nor directly. This means that we delete all arrows starting in the PA and eventually leading to the target variable (dashed arrows in Figure \ref{fig:DAG}). This idea differs from what the literature on path-specific effects \citep[e.g.,][]{chiappa_path-specific_2019, chikahara_learning_2021, weinberger_path-specific_2019} conceives. However, we believe that this more adequately captures the legal requirements of many laws that demand that individuals must not be discriminated against based on the PA\footnote{E.g., %US Civil Rights Act of 1964: \url{https://www.dol.gov/agencies/oasam/civil-rights-center/statutes/civil-rights-act-of-1964} or 
Charter of Fundamental Rights of the European Union: \url{https://www.citizensinformation.ie/en/government-in-ireland/european-government/eu-law/charter-of-fundamental-rights/}} -- rendering it irrelevant which path in the DAG this effect follows.

% The modification of causal effects through deleting arrows by particular interventions has been criticized by \cite{hu_whats_2020}, claiming that these effects are ``constitutive features'' of the PA that cannot be removed without stripping the PA from the ``meaning [of this variable] in our world''.
% We argue that this graph surgery is a valid method to conceive a FiND world, where the PA has no causal effect on the target. In the FiND world, the PA does not have to have the same meaning as in our real world -- it is more about the idea that people are normatively considered equal if they only differ in the PA, and hence are not distinguishable in the FiND world.


% \bp{Group Fairness; individual fairness; causal fairness - aus what is fairness nehmen}
% \bp{path-specific vs. global}
% \bp{other fictitious worlds: zB Chikahara, Chiappa, Calmon, Kusner, Nabi, Pfohl (?)}

\section{Methods}
\label{sec:methods}

%\ms{Vielleicht könnte man zu Beginn folgendes machen: technisch definieren, was "fair" bedeutet (also mehr als nur Pfeile löschen: wann erkennen wir eine Vorhersage als fair an?). Und dann können wir nachher schauen, ob unser estimand, definiert durch unsere Intervention (stochastisch plus warping?) und danach die Schätzung mit dem Ziel übereinstimmt.}


As derived in \citep{bothmann_what_2023}, in order to derive the decision basis for fair decisions, we must conceive a ``fictitious, normatively desired (FiND) world in which the PA has no causal effect'' on the target variable, ``neither directly nor indirectly''. In the following, we adopt this idea, elaborate it further by concretely specifying causal and statistical estimands, and derive an estimation method, thereby building concrete and actionable algorithms for approximating the FiND world by what we call a ``warped world'' and for using Causal Fair ML (cfML) in applied use-cases.

Our method consists of four fundamental steps: (i) We first define the estimand as the joint distribution in the FiND world, described by stochastic interventions (``rank-preserving interventional distribution'') (see Section \ref{sec:estimand});
%(ii) estimation by learning ``warping models'' to translate real-world data into the warped world and by training an ML model (for predicting the target) in the warped world,
(ii) We then estimate the joint/conditional distributions of interest in the FiND world, based on a specific $g$-formula type-of factorization that follows from specifying the respective identification assumptions and allows us to ``warp'' the real-world data into the warped world (see Section \ref{sec:warping}); With this, we can (iii) train an ML model (for predicting the target) in this warped world,
(see Section \ref{sec:training_warped}) and (iv) predict on a new observation in the warped world using the above warping models and ML model (see Section \ref{sec:prediction}).


\subsection{Estimand}
\label{sec:estimand}

\subsubsection{DAGs in the Real and the FiND World}
\label{sec:dag}

Deriving a DAG falls into the realm of \myemph{Causal Discovery} (see, e.g., \citep{nogueira_methods_2022} for a review of current methods). Since this is a notoriously hard challenge in practice, the alternative is to define the DAG with expert knowledge, as is typically done in epidemiology and medicine, where knowledge from human decision-makers is readily available \citep{hernan_causal_2020}. In the remainder, we are agnostic to the question of how the DAG was constructed and will assume that all DAGs are correct;
%in the sense that they mirror the causal relationships in the real world between the features shown in the DAG;
note that this may be an optimistic (and untestable) assumption and can hamper success in practice.

Two DAGs must be defined: the DAG in the real world and the DAG in the FiND world -- where the PAs have no causal effect on the target. Figure \ref{fig:DAG} shows the two DAGs that we assume for the example of the German Credit data. Note that these DAGs are chosen for illustrative purposes and not because there is empirical evidence or expert knowledge that justifies exactly those DAGs. We reduced the feature set for a clearer presentation: Age (a confounder $C$) is the numerical age of an individual; Gender (the PA $G$) is assumed to be binary (classes \textit{female} and \textit{male}) in the remainder, but note that an extension on multi-categorical gender is methodologically straightforward;
%(in practice, we could run into problems of high estimation errors for small classes, though);
Savings (feature $X_S$) is a binary variable, indicating if the person has small savings (1) or not (0); Amount (feature $X_A$) is the amount of credit applied for; and Risk (target $Y$) is the binary risk category with values good (1) and bad (0).


% Figure environment removed



\subsubsection{Rank-Preserving Interventional Distributions}

There are several possible interventions that can delete the dashed arrows in Figure \ref{fig:DAG} and, hence, lead to the FiND world DAG. We propose the following idea of ``rank-preserving interventional distributions'', which we believe to be the best way of defining those interventions when aiming to mitigate unfairness.
%
We assume that the given DAGs (as shown in Figure \ref{fig:DAG}) correctly mirror the causal relationships in both the real world and the FiND world. 
%(with full acknowledgment that this assumption might be wrong) 
%and use the DAGs shown in Figure \ref{fig:DAG} as an illustrative example. 
Slightly adapting the notation and terminology of \citep{pearl_causality_2009}%\footnote{Definition 7.1.1, p. 203}
, a general structural causal model (SCM) is given by
% XXX
\begin{equation*}
    X_j \coloneqq f_j(pa(X_j), U_j), \quad j \in \pset,
\end{equation*}

\noindent where $U_1, \dots, U_p$ denote exogeneous independent random variables, and $pa(X)$ are parent nodes of $X$. In our example, the SCM in the real world (i.e., pre-intervention) is given by %equations
\begin{align*}
    G & \coloneqq f(U_G) \\
    C & \coloneqq f(U_C) \\
    X_A &\coloneqq f_A(G, C, U_A) \\
    X_S &\coloneqq f_S(G, C, U_S) \\
    Y &\coloneqq f_Y(G, C, X_A, X_S, U_Y),
\end{align*}

\noindent which entails a joint distribution that can be factorized according to our working order:
\begin{equation}\label{formula:pre_int_fact}
P(Y, X_S, X_A, C, G) = P_Y(Y|X_S, X_A, C, G) P_S(X_S|C, G) P_A(X_A|C, G) P_C(C) P_G(G).
\end{equation}
%Generated by this causal model, we assume to have a data set $\D = \Dset$. For finding a mapping from real world to warped world, i.e., $m_y: \Yspace \rightarrow \Yspace, \ y \mapsto \yt$ and $m_\xv: \Xspace \rightarrow \Xspacet, \ \xv \mapsto \xtilde$, we have to make all descendants from the PA neutral w.r.t. the PA. That is, we correct the values of $\xv$ and $y$ by taking out the effect of the PA.
For the FiND world, we must make all descendants from the PA neutral w.r.t.~the PA.
We achieve this by a fictitious intervention rule $d_p$ on the mediators and outcome only, i.e., no ``modification'' of the potentially sensitive PA is required (Eq.~\ref{eq:intervention}).
This intervention leads to a joint post-intervention distribution $P_p(G,C,X_A^{d_p}X_S^{d_p},Y^{d_p})$ in which the dashed arrows have been removed; thus, no effect of Gender on the mediators and the outcome exists -- but the distributions of males and females are comparable and still in line with the data-generating process on which we want to train our ML model. Additionally, our suggested intervention is ``rank-preserving'' in the sense that the quantile of female customers within their strata is transported into the FiND world (see Figure \ref{fig:mod_int_distr}).
%
Thereby, all PA-dependent quantities %$(X_S, X_A, Y)$ 
are transformed into their FiND-world counterparts. 
%$(\tilde{x}_S^{(i)}, \tilde{x}_A^{(i)}, \tilde{y}^{(i)}) \ \forall i \in I_f$, where $I_f$ denotes the index set of female individuals.\footnote{Note that we do not modify male values since we assume that they are equal in both the real and the FiND world.}
Note that we can factorize the joint post-intervention in line with the pre-intervention distribution (Eq.~\ref{formula:pre_int_fact}), where the mediators and outcomes are replaced by their post-intervention counterparts. This leads to a $g$-formula type of factorization, which we can use for plug-in estimation of the relevant counterfactual distributions. A similar, quantile-based approach, can be found earlier in \cite{plecko_fair_2020} which uses quantile regression forests for estimation. 

\begin{flalign}
d_p %= d(G,C,\tilde{q}) 
&=\left\{ \begin{array}{cl}
               X_{A}^{(i)}=\txai  & \text{where} ~ \txai~ \text{is the (} p_A^{(i)}\times100) \text{\% quantile of the conditional} \\& \text{mediator distribution among the reference PA value, i.e.,} \\& P(X_A \leq \txai|C=c^{(i)},G=m) = p_A^{(i)},~ \text{and} ~p_A^{(i)}~ \text{is determined}\\  &\text{by the pre-intervention quantile of unit $i$, i.e.,} \\& p_A^{(i)} = P(X_A \leq x_{A}^{(i)} \mid C=c^{(i)},G=g^{(i)}).\\
               &\\
               X_{S}^{(i)}=\txsi  & \text{where} ~ \txsi~ \text{is the (} p_S^{(i)}\times100) \text{\% quantile of the conditional} \\& \text{mediator distribution among the reference PA value, i.e.,} \\& P(X_S \leq \txsi | C=c^{(i)},G=m) = p_S^{(i)},~ \text{and} ~p_S^{(i)}~ \text{is determined}\\  &\text{by the pre-intervention quantile of unit $i$, i.e.,} \\& p_S^{(i)} = P(X_S \leq x_{S}^{(i)}| C=c^{(i)},G=g^{(i)}).\\
               &\\ 
               Y^{(i)}=\tyi  & \text{where} ~ \tyi~ \text{is the (} p_Y^{(i)}\times100) \text{\% quantile of the counter\-fac\-tual} \\& \text{outcome distribution for the reference PA value, i.e.,} \\& P(Y \leq \tyi \mid \txai,\txsi,C=c^{(i)},G=m) = p_Y^{(i)},~ \text{and} ~p_Y^{(i)}~ \text{is}\\  &\text{based on the pre-intervention quantile of unit $i$, i.e.,} \\& p_Y^{(i)} = P(Y \leq y^{(i)} \mid X_A=x_{A}^{(i)},X_S=x_{S}^{(i)},C=c^{(i)},G=g^{(i)}).\\
               \end{array} 
               \right. & \label{eq:intervention}
\end{flalign}

\iffalse
leading to post-interventional distributions $P_A(X_A|do(G=g), C)$, $P_S(X_S|do(G=g), C)$ and $P_Y(Y|do(G=g), X_A^{G=g},  X_S^{G=g}, C)$, where the value $g$ would be \textit{male}\footnote{We could instead warp males to females, see Section \ref{sec:results_sim} for an investigation of how this changes results.} in the example of Figure \ref{fig:warped_world} and $X^{G=g}$ denotes the counterfactual of $X$ ``had $G$ been $g$''.\footnote{On the question if fictitiously intervening on Gender is thinkable, see, e.g., Pearl in \citep{pearl_causality_2009}, Section 11.4.5, page 361.}

This only affects female observations and variables $X_A$, $X_S$ and $Y$ in the following way, starting on the left-hand side of the DAG.
First, we want to remove the effect that Gender $G$ might have had on the female observations of $X_A$. At the same time, we only want to remove the Gender effect but keep (i) the Age effect and (ii) the remaining individual variability in the post-intervention values $\tilde{X}_A$. In other words, we want to make sure that each female individual maintains the rank that they have w.r.t. $X_A|C, G=f$ in the female real-world population in the FiND world w.r.t. $\tilde{X}_A|C$ and achieve this by replacing it with the respective quantile of $X_A|C, G=m$, see Figure \ref{fig:mod_int_distr}. More formally, we take the individual probability rank
\begin{equation}
p_A^{(i)} = P_A(X_A \leq x_A^{(i)} | G = f, C) \label{eq:prob_rank_amount}
\end{equation}
of individual $i$ (mirroring how high or low $i$'s value $x_A^{(i)}$ is in comparison to what would have been expected by just knowing the Age and Gender) and replace their $x_A^{(i)}$ by
\begin{align}
\tilde{x}_A^{(i)} &\coloneqq Q_A(p_A^{(i)} | do(G=m), C) \\ &= Q_A(p_A^{(i)}|G=m, C) \label{eq:q_male_amount}
\end{align}
where $Q_A$ denotes the quantile function of $X_A$ and (\ref{eq:q_male_amount}) follows ``because the mechanism determining Gender can safely be assumed to be independent of the background factors that influence [\dots] [$X_A$] (thus ensuring no confounding)''.\footnote{Pearl in \citep{pearl_causality_2009}, Section 11.4.5, page 361}

Analogously, we modify $X_S$ via % \lb{für die beiden binären Variablen muss man hier wohl noch etwas arbeiten.. (?)}
\begin{align*}
    p_S^{(i)} &= P_S(X_S \leq x_S^{(i)} | G = f, C) \\
    \tilde{x}_S^{(i)} &\coloneqq Q_S(p_S^{(i)} | do(G=m), C) \\
    &=Q_S(p_S^{(i)}|G=m, C),
\end{align*}

\noindent and $Y$ via
\begin{align}
    p_Y^{(i)} &= P_Y(Y\leq y^{(i)} | G=f, X_A, X_S, C) \\
    \tilde{y}^{(i)} &\coloneqq Q_Y(p_Y^{(i)} | do(G=m), X_A^{G=m}, X_S^{G=m}, C) \\
    &= Q_Y(p_Y^{(i)}|G=m, X_A=\tilde{x}_A^{(i)}, X_S=\tilde{x}_S^{(i)}, C). \label{eq:warp_Y}
\end{align}
\fi 




\iffalse
\begin{align*}
    P(X_S, X_A, Y|do(G=g), C) &= P_A(X_A|do(G=g), C) P_S(X_S|do(G=g), C) \cdot \\
    & \qquad P_Y(Y|do(G=g), X_A^{G=g},  X_S^{G=g}, C) \\
    &= P_A(X_A|G=g, C) P_S(X_S|G=g, C) \cdot\\
    & \qquad P_Y(Y|G=g, \tilde{X}_A,  \tilde{X}_S, C),
\end{align*}

leading to the statistical estimands, which in turn can be estimated by simple plug-in estimators.
\fi

% % Figure environment removed

%\lb{Allgemeiner aufschreiben?}

\subsection{Estimation}
\label{sec:training}

%\textcolor{red}{Michael: needs still improvement}
We base our estimation algorithm on the factorization derived above, i.e.,  we use the empirical distributions of both $G$ and $C$ to implement the intervention to sequentially obtain the respective post-intervention distributions of $X_A$, $X_S$, and $Y$. To determine the distributions and quantiles needed to facilitate the intervention implementation, our proposed algorithm uses the empirical distributions for the PA reference group (i.e., male customers) and a residual-based approach for the non-reference group (i.e., female customers). Alternatively, we could data-adaptively estimate the quantiles from the conditional distributions \cite{hejazi_efficient_2022}, but we do not pursue this more complicated approach further in this manuscript. 
%Since we identified all causal quantities above with a statistical estimand, we can now turn to estimate the respective values, continuing with the same example.

%The basic idea of this approach to mitigating unfairness is as follows: We assume that there is discrimination in the real world w.r.t. the PA (again: we do not distinguish between ``fair'' and ``unfair'' discrimination for the reasons explained above) and conceive a fictitious normatively desired (FiND) world where the discrimination vanishes.

More generally, we approximate the FiND world by ``warping'' the target and the features affected by the PA (see Figure \ref{fig:warped_world}). Once we have a warped data set of ``cleaned'' features and target, we can simply proceed by learning ML models as usual, i.e., focusing on high predictive performance, and without needing to address any ``fairness metrics'' (see again \citep{bothmann_what_2023} for details on the philosophical rationale). The three key steps of our proposed algorithm are:
%and Section \ref{sec:training_warped} for methodological details. We will now make these thoughts more formal; Section \ref{sec:prediction} will elaborate on how to use the results during prediction time.

% % Figure environment removed

% % Figure environment removed

% Figure environment removed


%The training process is divided into two steps: %, which will be described thoroughly in the following. The steps are:

%\begin{enumerate}
%    \item Derive or define the DAGs in the real world and in the FiND world.
    % \item Estimate the causal effects of the PA in the real world.
    (1) Derive a warping from the real world to the warped world (see Section \ref{sec:warping})
    %(approximating the FiND world)
    
    (2) Train and test an ML model using the warped data (see Section \ref{sec:training_warped}).
    
    (3) At the time of prediction, use warping and trained ML model (see Section \ref{sec:prediction})
%\end{enumerate}


\subsubsection{Warping for Approximating the FiND World}
\label{sec:warping}
%\textcolor{red}{Michael: Hier nochmal kurzen informellen Bezug zu den oben definierten Interventionen, also konkret, dass wir über den Residuenansatz die entsprechende Intervention implementieren}
%\lb{Einleitender Satz TBD}
%\lb{Einfügen: For determining the male intervention values and quantiles, we use the respective empirical conditional distributions, i.e. we use the actual measured values}
%\paragraph{Estimation of statistical quantities.}
We propose to implement the interventions defined above (see Eq. \ref{eq:intervention}) by the following residual-based estimation method.
%For determining the male intervention values, we use the respective empirical conditional distributions, i.e., we use the actual measured values.
For determining the female intervention values, we must estimate --
for each variable to be warped -- 
(i) the individual probability rank of female $i$ (e.g., $p_A^{(i)}$ for variable $X_A$) and (ii) the corresponding quantile of the male distribution (e.g., $\txai$).
This means that we must estimate full distributions (not just location parameters) of $X_A|C=c, G=g$ for all values of $c$ and $g$ (analogously for $X_S$ and $Y$), which becomes prohibitively complex in situations with finite data and numeric confounders $C$ or features $X_*$. In our algorithm proposed below, we reduce estimation complexity by only estimating models for the location parameters of these distributions and derive individual probability ranks by using residuals of those models.
% \textcolor{orange}{
% We propose to do this via the following steps (where this applies analogously to $X_S$ and $Y$, respectively):
% \begin{enumerate}
%     \item Estimate a prediction model $\pifem_A(C)$ for $X_A$ in the female population.
%     \item Derive individual probability ranks $p_A^{(i)}$ via residuals w.r.t. model $\pifem_A(C)$.
%     \item Estimate a prediction model $\pim_A(C)$ for $X_A$ in the male population.
%     \item Set $\hat{x}_A^{(i)} = \pim_A(c^{(i)}) + \tilde{q}_A^{(i)}$, where $\tilde{q}_A^{(i)} = \tilde{Q}_A(p_A^{(i)}|C = c^{(i)})$ is the $p_A^{(i)}$-quantile of the residuals of the male model $\pim_A(C)$.
% \end{enumerate}
% }
The five steps of this warping algorithm are explained for feature Amount ($X_A$), warping for other variables works analogously:
%\textcolor{orange}{
%We choose parametric models for both $\pifem_A(C)$ and $\pim_A(C)$, but the method is in general agnostic on this and we could equally well choose any other ML model since we only rely on point predictions and model residuals on training data.
%}

\noindent (1) Estimate prediction models $\pifem_A(C)$ for the female and $\pim_A(C)$ for the male population, where we are agnostic on the model class and can choose any ML model, since we only rely on point predictions and model residuals on training data.
%assume that $X_A|C,G=g$ follows a Gamma-distribution (but the method is in general agnostic on this and we could equally well choose any other ML model since we only rely on point predictions and model residuals on training data), i.e.,
%$$ X_A|C, G=g \sim \text{Ga}(\alpha_g, \beta_g).$$

\noindent (2) Compute residuals as
\begin{align*}
r_f^{(i)} &= \pifem_A(c^{(i)}) - x_A^{(i)} \ \forall i \in I_f, \\
r_m^{(i)} &= \pim_A(c^{(i)}) - x_A^{(i)} \ \forall i \in I_m,
\end{align*}
where $I_f$ and $I_m$ are the female and male index set, respectively.

\noindent (3) Compute the individual probability rank of female $i$ as ranked within the female residuals, i.e., telling us how ``exceptionally high or low'' her value is in comparison to other females, by
$$p_f^{(i)} = \frac{|\{j \in I_f: r_f^{(j)}\leq r_f^{(i)}|\}}{|I_f|}.$$

%(3) Analogously, after estimating parameters $\alpha_m$ and $\beta_m$, we compute the male residuals as
%$$r_m^{(i)} = \pim_A(c^{(i)}) - x_A^{(i)} \ \forall i \in I_m,$$
%where $I_m$ denotes the index set of male individuals.

\noindent (4) Set  $q_m^{(i)}$ to the empirical $p_f^{(i)}$-quantile of the residuals of the male model $\pim_A$, i.e.,
%$$\tilde{q}_A^{(i)} = \min \{r \in R_m: \text{at least} \ p_A^{(i)} \cdot 100 \% \text{ of } R_m \text{ are } \leq r \},$$
$$q_m^{(i)} = \min \{r \in R_m: \frac{|\{j \in R_m: j \leq r\}|}{|R_m|} \geq p_A^{(i)}\},$$
where $R_m = \{r_m^{(i)}: i \in I_m\}$ is the set of male residuals.

\noindent (5) Warp $x_A^{(i)}$ to the sum of male prediction and warped residual, i.e.,
\begin{equation*}
    \hat{x}_A^{(i)} = \pim_A(c^{(i)}) + q_m^{(i)}.
\end{equation*}
Analogously, we can warp $X_S$ and $Y$ (where in the latter case, warped values of Amount and Savings must be plugged into the male prediction in step (5)). However, note that for warping of non-continuous variables (such as Savings and Risk), we define the models to predict the probability scores, not the hard labels. That way, the warped values, e.g., $\hat{x}_S^{(i)}$, are no longer binary, but may be $\in [-1, 2]$. If we need hard labels -- e.g., for learning a binary prediction model, such as for the target variable $Y$ in Section \ref{sec:training_warped} -- we can simply threshold these scores. On the other hand, for use in further warping steps (such as for warping of $\yi$), we can directly use the raw values by plugging them into the prediction function, thereby pulling through finer information than if we would threshold earlier in the process.

Now, we have warped all Gender-dependent quantities  $(x_S^{(i)}, x_A^{(i)}, y^{(i)})$ of female individuals to their warped world counterparts $(\hat{x}_S^{(i)}, \hat{x}_A^{(i)}, \hat{y}^{(i)})$, approximating their FiND world counterparts $(\tilde{x}_S^{(i)}, \tilde{x}_A^{(i)}, \tilde{y}^{(i)}) \ \forall i \in I_f$. To have a complete warped world data set $\D_w = \Dsetw$, we set warped male values and values of non-warped features (e.g., Age) to their real-world value.
Additionally to having warped the training data, we have also estimated warping functions that can be applied for new test data at the time of prediction. %, see Section \ref{sec:prediction}.

\subsubsection{Training ML Models in the Warped World}
\label{sec:training_warped}

We can now use the warped world data $\D_w$ to train a prediction model for the warped target $\hat{Y}$. Assuming that the warping cleaned the data from any PA discrimination, we do not have to account for any fairness metrics in this training step but can just focus on training a model that has high predictive performance. Since we assume that all Gender-related discrimination was eliminated through the warping, we do not use Gender $G$ as a feature in this model (see Section \ref{sec:results_sim} for an investigation of what happens if this assumption is wrong, e.g., due to a misspecified DAG). 
%The usual steps of defining a performance metric (such as accuracy, F1-score, AUC, etc.) and choosing a resampling strategy apply. 
As a result, we obtain a trained model $f(\hat{\xv})$ which can be used for prediction. %in Section \ref{sec:prediction}.


\subsection{Prediction}
\label{sec:prediction}

\paragraph{Warp New Data.} Consider a new observation $\xv^* =(g^*, x_A^*, x_S^*, c^*)$. If this is a male observation, no warping must be done; if this is a female observation, we use the estimated warping functions of Section \ref{sec:warping} as follows for $X_A$ and analogously for $X_S$ (but not for $Y$): %, since we do not have a value here -- which is why we developed a prediction model in the first place):

(1) Compute individual residual $r_f^*$ w.r.t. female model $\pifem_A(c^*)$ as $r_f^* = \pifem_A(c^*) - x_A^*$.

(2) Compute individual probability rank $p_f^*$ w.r.t. female population $I_f$ as above. % of training data

%$$p_A^* = \frac{|\{j \in I_f: r_f^{(j)}\leq r_f^*|\}}{|I_f|}.$$

(3) Set $q_m^*$ to the empirical $p_f^*$-quantile of training data residuals of male model $\pim_A$ as above.

%$$\tilde{q}_A^* = \min \{r \in R_m: \text{at least} \ p_A^* \cdot 100 \% \text{ of } R_m \text{ are } \leq r \}.$$

(4) Warp $x_A^*$ to the sum of male prediction and warped residual, i.e., $\hat{x}_A^* = \pim_A(c^*) + q_m^*$.


\noindent After carrying out the same steps for warping $X_S$, we finally obtain the warped observation $\hat{\xv}^* =(\hat{x}_A^*, \hat{x}_S^*, c^*)$ (recall that we do not use Gender as a feature in the prediction model).


\paragraph{Predict New Data.}
%\label{sec:pred_new}
For predicting the target in the warped world $\hat{y}^*$, we plug the warped observation $\hat{\xv}^*$ into the prediction model trained on the warped world data, i.e., $\hat{y}^* = f(\hat{\xv}^*)$.



\section{Evaluation}
\label{sec:evaluation}


% (A) In a real-world use case for evaluating (i) the concrete warping procedure and (ii) the trained ML model, and (B) in a simulation study for additionally evaluating if the method for mitigating unfairness proposed above works as expected.

% \begin{table*}
%   \caption{Application of evaluation criteria; for metrics W1 -- ML3, comparisons with FiND world are possible.}
%   \label{tab:eval_crit}
%   \begin{tabular}{ccc}
%     \toprule
%     Evaluation criterion & Real-world use case & Simulation study\\
%     \midrule
%     CI1 & -- & x\\
%     CI2 & x & x\\
%     W1 & x & x and comp. with FiND world\\
%     W2 & x & x and comp. with FiND world\\
%     ML1 & x & x and comp. with FiND world\\
%     ML2 & x & x and comp. with FiND world\\
%     ML3 & x & x and comp. with FiND world\\
%   \bottomrule
% \end{tabular}
% \end{table*}

%\subsection{Evaluation of Causal Discovery}
%\label{sec:eval_causal_disc}

%\lb{Delete, since we just assume the DAG is correct?}

% \subsection{Evaluation of Causal Inference}
% \label{sec:eval_causal_inf}

% Assuming that the conceived DAG is correct, we can evaluate the results of the causal inference step, i.e., the estimation of female and male models in the different worlds, w.r.t. two questions, where the first question can only be answered for simulated data:

% \begin{enumerate}[label=(CI\arabic*)]
%     \item Did we correctly estimate the ground truth in real and FiND world, respectively?
%     \item How robust are the estimations wrt small changes in the data?
% \end{enumerate}

% \paragraph{Evaluating (CI1).}
% We use confidence intervals $CI_\theta = [a_\theta,b_\theta]$ for each parameter $\theta \in \thetab$ of the true DGP and say that the estimation process recovered the ground truth if $a_\theta \le \theta \le b_\theta \ \ \forall \theta \in \thetab$. Confidence intervals are straightforward for parametric models and can be derived via, e.g.,  bootstrapping for non-parametric models.

% \paragraph{Evaluating (CI2).}
% We propose to use bootstrap samples of the data to investigate the robustness of the estimates. Let $\D_b, b \in \{1, \dots, B\}$ denote the b-th bootstrap sample and $\thetah_b$ the estimate of $\theta$ using that sample. Large empirical variances $\var_\theta = \frac{1}{B-1}\sum_{b=1}^B (\thetah_b - \frac{1}{B} \sum_{b=1}^B \thetah_b)^2$ denote low robustness.

% \lb{Joa, brauchen wir das wirklich oder können wir uns die Section einfach sparen?}


We propose evaluation criteria that can be used for two purposes: Section \ref{sec:eval-method} describes how to evaluate our proposed warping method for rank-preserving interventional distributions in a simulation study. Section \ref{sec:eval-usecase} describes how the warped data and resulting ML models can be evaluated in an applied use-case. We denote with $\pihi$, $\pithi$, and $\psihi$ the predicted target of individual $i$ in the real, warped, and FiND world, respectively.


\subsection{Evaluation of Warping Method}
\label{sec:eval-method}

We can evaluate our warping method w.r.t. (i) the warped data, asking, e.g., if the FiND world is recovered by the warping and w.r.t (ii) the final ML model -- using the warped data.
%\subsubsection{Evaluation of Warped Data}
%\label{sec:eval_warping}
%For evaluating the warped data, we can compare data sets in real, warped, and FiND world, respectively.
%, from two perspectives: (W1) Comparison on the level of features; (W2) Comparison on the level of observations.
%First, we can evaluate the warping procedure, i.e., compare real-world and warped-world data -- for simulated data, comparisons with FiND world data are also possible:
% \begin{enumerate}[label=(W\arabic*)]
%     \item Comparison on the level of features.
%     \item Comparison on the level of observations.
% \end{enumerate}
%\paragraph{Level of observations (W2).}
\paragraph{(W1) Recovering FiND world.}
In a simulation study %(knowing the distributions in the FiND world) 
we can compare the warped and the FiND world distributions for investigating if the warping procedure recovers the FiND world. For numerical features, we compare warped world and FiND world empirical distributions by Kolmogorov-Smirnov (KS) tests, and for binary features, we use binomial tests. 
Additionally, we use a t-test to test the null hypothesis that there is no discrimination in the warped world between male and female subgroups w.r.t. risk predictions.
If the method works, p-values of these tests should be consistently high, indicating that the null hypotheses cannot be rejected.

\paragraph{(W2) Identifying strongest discriminated individuals.}
In addition to the population-wide perspective of (W1), we are interested in the individual perspective, i.e., if the warping method also recovers the individual ranks of the FiND world w.r.t. the target variable prediction. If this would be the case, we could identify individuals who are most strongly affected by discrimination in the real world by comparing real world and warped world predictions in an applied use case.
For the warped class of the PA, we compute individual risk prediction differences between the real world and the warped world, $d_1^{(i)} = \pihi - \pithi$ and between the real world and the FiND world, $d_2^{(i)} = \pihi - \psihi$, respectively.
%Strongest negatively affected individuals are at the upper end of the distribution. As can be seen, top discriminated individuals (high diff between real and FiND world) are correctly identified (high diff between real and warped world).
We use a t-test to test the null hypothesis that the means of these differences are equal. If the method works, p-values of these tests should be consistently high, and differences $d_1^{(i)} - d_2^{(i)}$ should be small.
Correlation between ranks of $d_1^{(i)}$ and $d_2^{(i)}$ should be high, too.

\subsection{Evaluation in an Applied Use-Case}
\label{sec:eval-usecase}


%\lb{Was mir noch fehlt ist eine richtig coole Metrik, mit der man in einem real-world use case entscheiden kann, ob das alles funktioniert hat oder nicht.}

How can the model be evaluated in an applied use case, i.e., how can we know if the warping method worked and if it removed unfairness? In our opinion, this cannot be answered by evaluating the final ML model w.r.t some ``classical'' fairML metrics.\footnote{As elaborated on thoroughly in \citep{bothmann_what_2023} (main points were summarized above), these kinds of metrics (such as demographical parity, equalized odds, etc.) do not reflect a clearly defined concept of fairness and, hence, are not suitable for deciding if an ML model entails unfairness.} Once we have successfully warped the data from the real to the warped world (approximating the FiND world), we reduced the problem to finding a model with good predictive performance.
%
%Of course, it would be technically possible to train a model in the real world, too, and to compare the models in the real world and the warped world. But it remains unclear, what we can learn from such a comparison.
However, we can train models in the real and the warped world and then compare their behavior:

% \begin{enumerate}[label=(ML\arabic*)]
%     \item Comparison of overall performance. % in both worlds.
%     \item Comparison of individual predictions. % in both worlds.
%     \item Comparison of fairness metrics. % in both worlds.
% \end{enumerate}

\paragraph{Comparing performance (UC1).}
Test performance of the ML models in the real world $\pihf$ and the warped world $\pithf$ can be compared, assuming that both models fit ``their'' world equally well. However, this %is just a qualitative comparison and 
must not be misinterpreted as either of those models being better than the other one, as the models are merely modeling different worlds. % For simulated data, we can additionally evaluate if the performance in the warped and FiND world are similar -- which they should be if the warping worked.

\paragraph{Comparing predictions and identifying strongest discriminated individuals (UC2).}
For each individual $i$, the predictions in the real and the warped worlds can be compared by computing the difference $d_1^{(i)}$. As for (W2), this analysis can reveal individuals that are discriminated most in the real world (either positively or negatively). Additionally, these differences can be aggregated on the subgroup level, and tests can be computed to test the null hypothesis that predictions do not change between the two worlds for the respective subgroup. %For simulated data, we can additionally test if predictions in the warped and FiND world are similar -- which they should be if the warping worked.

\paragraph{Identifying strongest warped individuals (UC3).}
We can also ask which individuals are affected most by the warping. These individuals' feature vectors have the largest distance between the real and the warped world, i.e., %, meaning that the respective individuals are discriminated most in the real world (either positively or negatively).
we can compare $\xi$ and $\xti$ by a suitable distance metric for each individual $i \in \nset$. %in several ways. For example, if there are categorical features, we can utilize the Gower distance. Otherwise, we can use the Euclidean distance, leading to a distance $d_0^{(i)}$ for each individual $i \in \nset$. %For simulated data, we can additionally evaluate if this detection reveals correctly the observations that differ most between real and FiND world.

% \paragraph{Comparing fairness metrics (UC3).}
% Technically, we can compute fairness metrics such as demographical parity, equalized odds, etc., for both the real world and warped world models and compare those values.
% As elaborated on thoroughly in \citep{bothmann_what_2023} (main points were summarized above), these kinds of metrics do not reflect a clearly defined concept of fairness and are hence not suitable for deciding if an ML model entails unfairness.
% Furthermore, as holds for the comparison of performance (ML1), this is not to judge if the warping method worked, because the metrics measure things in different worlds.
% However, it could be illustrative to see which metric changes by how much. %Again, metrics in warped and FiND world should be similar for simulated data.


\paragraph{Identifying important features (UC4).}
For each feature, we can compare the empirical distributions in the real and the warped world, % (and (iii) the FiND world for simulated data),
i.e., of $X_j$ and $\Xtj$ for each $j \in \pset$. We compute distances for each (normalized) feature and, thereby, can identify features that vary most between the two worlds, indicating that these features carry most of the real-world discrimination w.r.t. the PA. % For a real-world use case, we can detect features that are affected most by the mapping and in which sense. %For simulated data, we can additionally evaluate if this detection reveals correctly the features that differ most between real and FiND world.



%\section{Results}
%\label{sec:results}
\section{Simulation Study}
\label{sec:results_sim}

% \subsection{Research Questions}
% \label{sec:results_rqs}

For investigating the behavior of the proposed method, we first conduct a simulation study where we know the true DAG in both the real and the FiND world. % in Section \ref{sec:results_sim}. 
Subsequently, we apply the methods to the German Credit dataset in the Appendix, Section \ref{sec:results_german}.
We seek to answer the following \textbf{research questions}: %} (where (RQ1) -- (RQ5) can only be answered in a simulation study):

%\lb{Das sind ganz schön viele Fragen, vielleicht beschränken wir uns auf einen Teil davon?}

\begin{enumerate}[label=(RQ\arabic*)]
    \item Does our warping method work as expected? In other words, does this method recover the distributions in the FiND world (W1), and is it able to correctly identify the individual ranks of the target in the FiND world (W2)?
    %, while outperforming a simpler baseline (as described below)? % that profit or suffer most from unfair discrimination in the real world?
%    \item Is using warped data for training a model better than the baseline of using the real-world data of just the non-warped subgroup, disregarding data of the other group?
    \item How does misspecification of the DAG affect the results?
    \item What effects does the direction of warping have on performance (e.g., if subgroup A of the PA is warped to subgroup B, versus the other way around)?
    % \item Which differences can be seen if using ML models for warping instead of (generalized) linear (additive) models?
    % \item How do models that are trained with warped data perform w.r.t. ``usual'' fairness metrics such as demographic parity or equalized odds and w.r.t. predictive performance -- also in comparison to their real-world counterparts?
\end{enumerate}


\subsection{Simulation Setup}

\paragraph{Data simulation setup.}
We simulate data from the DAGs depicted in Figure \ref{fig:DAG}. Here, the real-world data simulation contains all arrows, while the FiND world data simulation only contains solid arrows by setting Gender to \textit{male} for all observations, thereby eliminating the Gender effect. The distributions utilized here are (left: real-world, right: FiND world):\footnote{Concrete values can be found in \texttt{simulation\_study.R} in \url{https://github.com/slds-lmu/paper\_2023\_cfml}}
\begin{align*}
    G &\sim \text{B}(\pi_G) & G &\sim \text{B}(\pi_G)\\
    C &\sim \text{Ga}(\alpha_C, \beta_C) & C &\sim \text{Ga}(\alpha_C, \beta_C)\\
    X_A|C, G &\sim \text{Ga}(\alpha_A(C, G), \beta_A(C,G))& \tilde{X}_A|C &\sim \text{Ga}(\alpha_A(C, m), \beta_A(C,m))\\
    X_S|C, G &\sim \text{B}(\pi_S(C,G))& \tilde{X}_S|C &\sim \text{B}(\pi_S(C,m))\\
    Y|X_A, X_S, C, G &\sim \text{B}(\pi_Y(X_A, X_S, C, G))& \tilde{Y}|\tilde{X}_A, \tilde{X}_S, C &\sim \text{B}(\pi_Y(\tilde{X}_A, \tilde{X}_S, C, m)),
\end{align*}
\noindent where we use linear combinations of the features combined with a log- and logit-link for the Gamma and Binomial models, respectively, and mirror the Gender distribution of the German Credit data with $\pi_f=31\%$ females. We perform $M=1,000$ simulations on data sets of size $N_{tr}=10,000$ for training and of size $N_{te}=1,000$ for test, for each world, using the same seed for the two worlds to ensure comparability. Note that Gender and Age are then identical in both worlds, and only the descendants of Gender have differing values. We refer to this setup as (SIM1).
To answer the misspecification behavior question (RQ2), we modify the simulation slightly by sampling Age from $C \sim \text{Ga}(\alpha_C(G), \beta_C(G))$ but ignoring this effect for warping. We refer to this setup as (SIM2).
%allowing a Gender effect on Age without including such an effect in the warping models.

\paragraph{Warping and prediction models.}
For warping models, we estimate models following the same distributional assumptions as in the simulation, i.e., by estimating the parameter vectors of the Gamma and logistic regressions %$\alpha_A, \beta_A, \pi_S, \pi_Y$,
separately for male and female observations of the training data. With these models, we apply the above warping strategy.
As prediction models for the target variable, we train logistic regression models on the training data, warped training data, and FiND world training data, separately. 
%For the baseline, the model for the warped world is trained only on the real-world data of the non-warped group (i.e., the male observations), and for the prediction, the real world test data is used. This corresponds to a modified version of fairness through unawareness \citep[see, e.g.,][]{kusner_counterfactual_2017}, where the modification is that only the data of one subgroup is used for learning the model.
%Is using warped data for training a model better than the baseline of using the real-world data of just the non-warped subgroup, disregarding data of the other group?

\subsection{Results}

%\paragraph{Research questions.}
With these models, we can now answer the above research questions (using a significance threshold of $\alpha=5\%$ for all tests):
%\noindent \lb{Puh, das werden ja viel zu viele Figures und Tables..}

\textit{(RQ1)} Figure \ref{fig:rq1-1} shows the distribution of Amount $X_A$ in the different worlds for male and female observations, aggregated over all iterations of the simulation study. %The visual impression of similar female distributions in warped and FiND world is supported by a p-value of $99.9\%$ of the respective KS test. Figure \ref{fig:rq1-2} shows the distribution of p-values over all iterations for Amount, Saving, and Risk. %, where for the categorical variables, a $\chi^2$-test is used.
%Table XXX shows distributions of Saving and Risk for one iteration.
%We conclude that warping recovers the marginal distributions in the FiND world for each subgroup of the PA since the 
The null hypothesis of equal distributions in the warped and the FiND world is only rejected in $0\%, 0.4\%, 0\%$ of the iterations for Amount, Savings, and Risk, respectively.
The mean difference between male and female risk predictions in the real world is $0.1122 \ (95\%$ CI:  $(0.1117, 0.1127)$). In the warped world, this is reduced to $-0.0016 \ (-0.0021, -0.0012)$, meaning that even if the difference between subgroups is still significantly non-zero, it is smaller by a factor of $70$, i.e., we effectively reduced PA discrimination (and the direction switched from positive to negative). %Values for the baseline are $-0.0456 \ (-0.0458, -0.0454)$ -- only reducing by a factor of $2.5$ and also switching directions.


Investigating individual predictions (W2), we see that correlations between ranks in the warped and the FiND world are high ($0.892$). % and significantly higher than using the baseline ($0.822, \ p<10^{-15}$). 
Figure \ref{fig:rq1-2} shows individual risk prediction differences between the real world and the warped world as well as between the real world and the FiND world for females in one iteration (males are identical in the real world and the FiND world).
The most strongly negatively affected individuals are at the upper end of the distribution. As shown, the most discriminated individuals (large difference between the FiND world and the real world) are correctly identified (large difference between the warped world and the real world).
%Additionally, we carry out a Wilcoxon rank-sum test to test the null hypothesis that ranks of these differences are equal -- for each iteration $m$.
%In $100\%$ of the iterations, the null hypothesis of equal ranks cannot be rejected by the WRS test; correlation between ranks of differences is high as can be seen in Figure \ref{fig:rq2-2}. We conclude that our method is able to identify correctly the individuals that are most discriminated in the real world.
In $81\%$ of iterations, the null hypothesis of equal differences $d_1$ and $d_2$ cannot be rejected. In cases with $p<0.05$, the mean difference is $-0.0023$ -- meaning that the deviation between the warped world and the FiND world is also minimal in these cases. %For the baseline, the null hypothesis is always rejected, with a mean difference of $-0.0456$.
We conclude that warping (i) recovers the marginal distributions in the FiND world, (ii) diminishes discrimination to a very small value, and (iii) correctly identifies the most discriminated individuals. %, and (iv) outperforms the baseline.

% Figure environment removed


% % Figure environment removed

% %\textit{(RQ3)}
% %We carry out the same test as in RQ2 but now use a model trained only on males (instead of males and warped females) for generating warped world predictions.
% Compared with the baseline, we see that in XXX percent of the iterations, the null hypothesis %(that ranks of the differences between real-world and warped-world predictions and between real-world and FiND world predictions, respectively, are equal)
% cannot be rejected. Comparing this to the XXX percent of above, we conclude that it is indeed better to use warped-world data for training the prediction model.

\textit{(RQ2)}
The null hypothesis of equal distribution in the warped world and the FiND world is rejected in $17\%, 4\%, 0\%$ of the iterations for Amount, Savings, and Risk, respectively.
%are comparable to (RQ1), see Figure \ref{fig:rq4-1-2} (B) \lb{Check ratio of WRS-tests where the null is not rejected -- should be smaller than in RQ2} (C) \lb{Check same number for RQ3}
In the real world, the mean difference between risk predictions is $0.1723 \ (0.1718, 0.1728$), which is higher than in (SIM1). In the warped world, this is reduced to $0.0355 \ (0.0350, 0.0360)$ (reducing by a factor of $4.9$) -- far less than above. 
The correlation of ranks ($0.9518$) is higher than above, since discrimination in the FiND world is higher in (SIM2).
In $6.9\%$ of iterations, the null hypothesis of equal differences $d_1$ and $d_2$ cannot be rejected. In cases with $p<0.05$, the mean difference is $0.026$ -- far higher than above.
We conclude that misspecification of the DAG is a relevant factor for degrading the performance of our approach.

% % Figure environment removed

\textit{(RQ3)} By switching the warping direction and warping male to female values, we observe the following:
Recovering of marginal FiND world distributions is equally successful as when warping female to male values. The null hypothesis is rejected  in $0\%, 0.3\%, 0\%$ of the iterations for Amount, Savings, and Risk, respectively (see also Figure \ref{fig:rq5-1-1} in the Appendix).
The mean difference between risk predictions in the warped world is reduced to $0.0065 \ (0.0060, 0.0071)$ -- slightly worse than in the analysis of RQ1, which is due to the imbalance of the data.
The mean correlation of ranks compared with ranks of RQ1 is high ($0.9595$), meaning that individual ranks are comparable for both warping directions.
In $34\%$ of iterations, the null hypothesis of equal differences $d_1$ and $d_2$ cannot be rejected. In cases with $p<0.05$, the mean difference is $0.0073$, meaning that the deviation between the warped world and the FiND world is also minimal in these cases (although a bit higher than in the analysis of RQ1, due to data imbalance).
This means we can also mitigate discrimination and preserve individual ranks with changing the warping direction. Most interestingly, the general level of the risk predictions changes, as shown in Figure \ref{fig:rq5-general-level-shift}. This also makes sense, since we are now warping male to female values. 

% % Figure environment removed


% and answer two questions: (A) Are ranks in warped world predictions equal for both warping directions? Mean correlation of ranks is $99\%$ and the Wilcoxon rank-sum test does not reject the null of equal ranks in a single iteration. %We look at differences of risk predictions in both worlds for each approach separately and test the null hypothesis if these ranks are equal by a Wilcoxon rank-sum test. In XXX percent of the iterations, this null cannot be rejected.
% (B) Are mean predictions similar for both warping directions? The null of equal means is only rejected in $0.2\%$ of the iterations by a two-sample t-test.
% %Are individual predictions in the warped worlds equal? We compute individual risk prediction differences in the two warped worlds and test the null hypothesis that the mean is 0 by a simple t-test. In XXX percent of the iterations, this null cannot be rejected.
% We conclude that warping direction does not matter for (A) the ranking of the individuals and (B) the general level of predictions. % -- in the warped world.

%\textit{(RQ7)} Instead of the above parametric models, we trained support vector machines and random forests for obtaining warping and prediction models. Revisiting the evaluations of the RQs 1--3 we see the following: (A) \lb{Check p-values of KS -- are distributions recovered? Compare with RQ1} (B) \lb{Check ratio of WRS-tests where the null is not rejected -- Compare with RQ3} (C) \lb{Check same number for RQ4 and compare with RQ4}

%\textit{(RQ8)} Table XXX shows averaged DP, EO and XXX for real world, warped world and FiND world models and standard deviations. We see that warped and FiND world are very similar, whereas real-world values differ significantly. As mentioned above, this last finding does not tell us anything about fairness performance of our approach \lb{naja, dann können wir es auch raus tun...}


\subsection{Discussion}
\label{sec:discussion}

We have shown that for the simulation setup above, our proposed method works as expected, recovering the marginal distributions in the FiND world and individual ranks;  %misspecification of the DAG degrades performance and 
direction of warping does not make a relevant difference.
%We have also shown how to use the method in an applied use case. 
However, as this is just an initial study, these investigations should be extended by follow-up work. As subsequent investigations, we would propose to (at least): (i) consider other, diverse DAGs, (ii) compare different ML models for warping and target prediction, and (iii) investigate behavior on other empirical data sets.

A general limitation of our method is that it depends on knowing the true DAG. As shown in RQ2, misspecifying the DAG degrades the performance of the method. Hence, special care should be given to identifying the true DAG in an applied use case by strongly connecting expert knowledge on the subject matter and rigorous application of causal discovery methods.

% \begin{itemize}
%     \item RQ's again (?)
%     \item Limitations (other methods, more data sets, dependence on knowing true DAG, more ML models for warping and not just GLM)
% \end{itemize}

\section{Conclusion and Outlook}
\label{sec:conclusion}

We have presented rank-preserving interventional distributions as a framework to identify a FiND world where no causal effects of a PA exist. Additionally, we have proposed a warping method for estimating FiND world distributions with real-world data. A simulation study showed that the method works for the investigated simulation setup (see Section \ref{sec:discussion} for limitations), and we demonstrated in the Appendix how the method can be applied to empirical data (Section \ref{sec:results_german}). Analyses can be reproduced via a public GitHub repository, which also contains code for applied use cases.\footnote{\url{https://github.com/slds-lmu/paper\_2023\_cfml}}
%
Apart from extending the study as outlined in Section \ref{sec:discussion}, further work should compare our method to other methods that conceive a fictitious world for tackling fairness issues of ML models (see references in Section \ref{sec:related-work}).

% \begin{itemize}
%     \item Our method works
%     \item Our method is usable (see GitHub)
%     \item Next step compare with other methods
%     \item Next step compare on other datasets
%     \item Next step R Package on CRAN
% \end{itemize}


% \section{Appendices}

% If your work needs an appendix, add it before the
% ``\verb|\end{document}|'' command at the conclusion of your source
% document.

% Start the appendix with the ``\verb|\appendix|'' command:
% \begin{lstlisting}
% \appendix
% \end{lstlisting}
% and note that in the appendix, sections are lettered, not
% numbered.

%%
%% The acknowledgments section is defined using the "acknowledgments" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acknowledgments}
  We thank Holger Löwe for helping with visualizations.
\end{acknowledgments}

%%
%% Define the bibliography file to be used
\bibliography{mybib} % Export from zotero

%%
%% If your work has an appendix, this is the place to put it.
\newpage
\appendix

\section{Additional plot for the simulation study}


% Figure environment removed

\section{German Credit Data}
\label{sec:results_german}

% \bp{Describe DAG that we assume}
% \bp{Describe prediction models used for warping and final model --> same as above, should be enough if mentioned}
% \bp{Answer RQ's}

\paragraph{DAG, warping, and prediction models.}
We assume the same DAG as in the simulation study, depicted in Figure \ref{fig:DAG}. For warping and prediction models, we use the same models as in the simulation study.

\paragraph{Evaluation.}
For the evaluation of the behavior of our method for this applied use case, we use the evaluation strategies defined in Section \ref{sec:eval-usecase}. Models are trained on randomly sampled $80\%$ of the training data (i.e., 800 from 1,000 observations) and tested on the remaining $20\%$.

\textit{(UC1)} Test accuracy in the real world is $71\%$ for both the male and the female subgroup. In the warped world, male accuracy is comparable, with test accuracy of $70\%$. However, female accuracy increases to $75\%$, showing increasing performance for the discriminated subgroup.

\textit{(UC2)} Table \ref{tab:pred-diff} shows individuals whose predictions differ most in the two worlds, either positively or negatively. Regressing this difference on features reveals that the risk prediction of young women grows strongly through warping, indicating that this subgroup was discriminated against most strongly in the real world (see Figure \ref{fig:german-1}). Figure \ref{fig:german-4} compares female predictions in both worlds and shows the most strongly affected individuals. Figure \ref{fig:german-2} shows prediction differences for female and male subgroups. While mean differences for females are significantly positive ($p<10^{-12}$), the mean differences for males do not change significantly ($p=0.26$). However, individual predictions and ranks of males do change: Figure \ref{fig:german-3} shows partial effects of Age and Amount on the prediction difference.

\begin{table}[ht]
    \caption{Most discriminated individuals for German Credit data. Last column shows the difference in risk prediction between the two worlds (warped world - real world).}
    \label{tab:pred-diff}
    \centering
    \begin{tabular}{rrrrr}
    \toprule
    Gender & Age & Amount & Saving & Pred-diff\\
    \midrule
    female & 22 &  1567 &     1 &    0.20 \\
    female &20 &  1282 &     1 &    0.20 \\
    \ldots &\ldots &  \ldots &     \ldots &    \ldots \\
    male &57  & 2225   &   1 & -0.03 \\
    male &66 &   766    &  0    &  -0.03\\
    \bottomrule
    \end{tabular}
\end{table}

\textit{(UC3)} Investigating the effect of warping on the individuals reveals similar results as investigating the prediction differences in (UC2) and are omitted for the sake of concise presentation. %Table XXX shows the (female) individuals that are strongest affected by the warping. Regressing the distance between the two worlds on the features reveals that young women undergo the strongest warping. \lb{..or something like that, have to look at the results first.}

\textit{(UC4)} The normalized feature differences between the real world and the warped world for Age, Amount, and Savings are $0.00, 0.01, 0.24$, respectively. This reveals that Savings is affected most by the warping and, hence, carries the strongest discrimination effect in the real world.




% Figure environment removed

%\textit{(ML3)} \lb{Entweder hier eben diese Metriken angucken oder den Punkt einfach streichen - sowohl hier als auch in Section 4}



\end{document}

%%
%% End of file
