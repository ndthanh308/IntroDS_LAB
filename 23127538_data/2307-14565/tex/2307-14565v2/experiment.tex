\section{Experiments}
\label{sec:exp}
We perform extensive evaluation on the performance of different algorithms, using real test data. The results show that our method significantly outperforms the baseline methods in terms of both quality and efficiency. Our labeled benchmark data is available on GitHub\footnote{\url{https://github.com/LiPengCS/Auto-Tables-Benchmark}} for future research.

% \kr{can you summarize the high level takeaways here}

\subsection{Experimental Setup}



%\stitle{Training Data Generation.}
%To generate the training data for \at, we crawl about 60k and 480k tables from PowerBI and Parquet data corpus \peng{add url}, respectively. Since these tables are metadata for relational databases, most of them are already relationalized. However, we observe that there exist a great number of duplicated tables with the same or similar column headers in each data corpus, which may lead to imbalanced training data and negatively impact the model performance~\cite{masko2015impact}. Therefore, we detect the duplicated tables using the column headers (i.e., tables with the same column headers are considered duplicates) and remove the duplicates in the datasets. Also, we observe that there exist a substantial number of tiny tables (i.e., tables with less than 3 columns or rows). Since such small tables are rarely seen in real-world scenarios, they can bias the model training. Therefore, we dropped them in the datasets. After cleaning duplicated and tiny tables, there remain 15,265 tables from PowerBI and 4,574 tables from Parquet, which we call \textit{base relational tables}. Then we generate the training data for each operator using the method introduced in Section~\ref{sec:training_data_generation}. We use data augmentation to enlarge the training data, which can generate up to 20 tables from each base table for each operator. We end up with a total of 1.4 million tables for model training. The details of the distribution of the training data are listed in Table \ref{tab:training_data_distribution}.




\begin{comment}
\begin{table}[!h]
\caption{Training Data Distribution}
\vspace{-2mm}
\label{tab:training_data_distribution}
\scalebox{0.8}{
\begin{tabular}{cccccccc}
\toprule
\textbf{none} & \textbf{transpose} & \textbf{stack} & \textbf{wtl} & \textbf{explode} & \textbf{ffill} & \textbf{pivot} & \textbf{subtitle} \\
\midrule
166k & 213k & 117k & 156k & 196k & 141k & 199k & 215k \\
\bottomrule
\end{tabular}
}
\end{table}
\end{comment}


% To generate training data, we crawl about 60,000 and 480,000 relational tables from PowerBI and Parquet data corpus, respectively. We notice that there exist a great number of tables with the same or similar column headers in each data corpus. To avoid data leakage problem, we use PowerBI to generate training data and use Parquet to generate validation data. To reduce the effect of the duplicated tables, we cleaned each dataset by removing duplicated tables  (i.e., tables with the same column headers are considered duplicates and only one is kept) and we also dropped small tables (i.e., tables with number of rows or columns less than 3). After data cleaning, there remain 15,265 tables from PowerBI and 4,574 tables from Parquet. We then use our proposed data generation method to generate training and validation data for each operator. On training data, we apply the proposed data augmentation strategy. By randomly sampling parameters or randomly cropping tables, we can generate up to 20 tables using one base table for each operator. The numbers of generated training and validation data for each operator are listed in Table~\ref{tab:dataset}. The total number of training/validation data is about 1.1M/22k.




% \begin{table}[!h]
% \caption{Training and Validation Data}
% \vspace{-2mm}
% \label{tab:dataset}
% \scalebox{0.8}{
% \begin{tabular}{l|cc}
% \toprule
%  & PowerBI (Train) & Parquet (Val) \\
%  \midrule
% Base tables & 15265 & 4574 \\
% \midrule
% Transpose & 136986 & 2756 \\
% Stack & 122205 & 2198 \\
% Wide to long & 120981 & 1920 \\
% Explode & 177791 & 4496 \\
% Ffill & 107832 & 1624 \\
% Pivot & 152650 & 4573 \\
% Subtitle & 152317 & 2500 \\
% None & 138708 & 2866 \\
% \midrule
% Total & 1109670 & 22933 \\
% \bottomrule
% \end{tabular}
% }
% \end{table}


%need to evaluate our models on real-world tables.  However, to the best of our knowledge, there is no existing benchmark that provides the real-world unrelational tables with the ground truth operators. Therefore, we construct \textit{\at Benchmark}, an evaluation benchmark that consists real-world tables from various sources for table relationalization tasks. Specifically, we collect real-world tables from three different sources, including forums, jupyter notebooks and excel files. For forum tables, we search forum questions from stackoverflow, PowerBI forum and excel forum, using the operator as the key words and such questions usually provide tables that can be relationalized using the operator. For jupyter notebooks, we crawl the jupyter notebooks that trigger the operators in our DSL. We then extract the input of that operator, which is usually a table that can be relationalized with the operator. For excel files, we sample hundreds of online excel files and manually select files that contain unrelational tables. We then manually label the operator and parameters for each table. The \textit{\at Benchmark} consists of 194 unrelational real-world tables, where 171 tables can be relationalized with one-step transformation and 23 tables need multi-step transformations. The distribution of tables over sources and labels are listed in Table~\ref{tab:autotable_benchmark}. For each unrelational table, we use its relationaized version as the table for None operator. Hence, we end up with 194 unrelational tables and 194 relational tables in our benchmark. \peng{show examples.}

\stitle{Benchmarks.} To study the performance of our method in real-world scenarios, we compile an \atbench benchmark using real cases from three sources: (1) online user forums, (2) Jupyter notebooks, and (3) real spreadsheet-tables and web-tables.


\underline{Forums.} Both technical and non-technical users ask questions on forums, regarding how to restructure their tables. As  Figure~\ref{fig:stackoverflow-ex} shows, users often provide sample input/output tables to demonstrate their needs.  We sample 23 such questions from StackOverflow and Excel user forums as test cases. (We feed \at with user-provided input tables, and evaluate whether the correct transformation can be synthesized to produce the desired output table given by users).

\underline{Notebooks.} Data scientists frequently restructure tables using Python Pandas, often inside Jupyter Notebooks. We sample 79 table-restructuring steps extracted from the Jupyter Notebooks crawled in~\cite{yan2020auto, yang2021auto} as our test cases. We use the transformations programmed by data scientists as the ground truth.


\underline{Excel+Web.} A large fraction of tables ``in the wild'' require transformations before they are fit for querying, as shown in Figure~\ref{fig:combined-ex} and~\ref{fig:combined-web-ex}. We sample 56 real web-tables and 86 spreadsheet-tables (crawled from a search engine) that require such transformations, and manually write the desired transformations as the ground truth.

Combining these sources, we get a total of 244 test cases as our \atbench (of which 26 cases require multi-step transformations). Each test case consists of an input table $T$, ground-truth transformations $M_g$\footnote{It should be noted that for some test cases, there may be more than one transformation sequence that can produce the desired output. We  enumerate all such sequences in our ground-truth, and mark an algorithm as correct as long as it can synthesize one ground-truth sequence.}, and an output table $M_g(T)$ that is relational. 
\iftoggle{fullversion}
{
}
{
Detailed statistics of the benchmark can be found in our technical report~\cite{full}.
}

% save space
%where each test case has a non-relational table and a ground truth relationalized version of the table. Among all test cases, 26 cases need multi-step operators (e.g., \code{ffill} followed by \code{stack}) for relationalization and other 171 cases only need a single-step operator. The distribution of the test cases over different types of operators is summarized in Table~\ref{tab:autotable_benchmark}. 

% From these test cases, we further construct two tabular evaluation datasets \atbenchpos and \atbenchneg, where \atbenchneg contains the 194 non-relational tables and \atbenchneg contains the 194 relationalized tables. 


 

% 
% To ensure that \at can correctly predict tables that are already relational and require no transformations (whose ground-truth transformation is ``\code{none}'' in Table~\ref{tab:dsl}), we  additionally test \at using the relationalized table of all 194 test cases, which results in a total of 388 tests. 


%The benchmark is available at~\cite{full} and will be released on GitHub to facilitate future research.

% and involves 8 table transformation operators.
% The distribution of tables are listed in Table.

% \iftoggle{fullversion}
% {
%     \begin{table}[!h]
%     \caption{Details of \atbench Benchmark}
%     \vspace{-3mm}
%     \label{tab:autotable_benchmark}
%     \scalebox{0.8}{
%     \begin{tabular}{l|ccc|c}
%     \toprule
%     \multicolumn{1}{l}{\textbf{}} & \multicolumn{1}{|l}{\textbf{Forum}} & \multicolumn{1}{l}{\textbf{Notebook}} & \multicolumn{1}{l|}{\textbf{Excel+Web}} & \multicolumn{1}{l}{\textbf{Total}} \\
%     \midrule
%     \multicolumn{1}{l|}{\textbf{Single-Step}} & \textbf{23} & \textbf{75} & \textbf{70} & \textbf{171} \\
%     \quad{} - transpose & 0 & 11 & 11 & 22 \\
%     \quad{} - stack & 10 & 20 & 5 & 35 \\
%     \quad{} - wtl & 6 & 24 & 1 & 31 \\
%     \quad{} - explode & 2 & 17 & 14 & 33 \\
%     \quad{} - ffill & 0 & 0 & 13 & 13 \\
%     \quad{} - pivot & 5 & 3 & 0 & 8 \\
%     \quad{} - subtitle & 0 & 0 & 26 & 26 \\
%     \multicolumn{1}{l|}{\textbf{Multi-Step}} & \textbf{0} & \textbf{4} & \textbf{22} & \textbf{26} \\
%     \midrule
%     \multicolumn{1}{l|}{\textbf{Total}} & \textbf{23} & \textbf{79} & \textbf{92} & \textbf{194} \\
%     \bottomrule
%     \end{tabular}
%     }
%     \end{table}
% }
% {
% }


\iftoggle{fullversion}
    {
    \begin{table}[t]
    %\vspace{-8mm}
    \caption{Details of \atbench Benchmark}
    \vspace{-2mm}
    \label{tab:autotable_benchmark}
    \scalebox{0.8}{
    \begin{tabular}{l|cccc|c}
    \toprule
    \textbf{} & \textbf{Forum} & \textbf{Notebook} & \textbf{Excel} & \textbf{Web} & \textbf{Total} \\
    \midrule
    \textbf{Single-Step} & \textbf{23} & \textbf{75} & \textbf{65} & \textbf{55} & \textbf{218} \\
    \quad{} - transpose & 0 & 11 & 11 & 6 & 28 \\
    \quad{} - stack & 10 & 20 & 2 & 24 & 56 \\
    \quad{} - wtl & 6 & 24 & 1 & 3 & 34 \\
    \quad{} - explode & 2 & 17 & 14 & 15 & 48 \\
    \quad{} - ffill & 0 & 0 & 11 & 7 & 18 \\
    \quad{} - pivot & 5 & 3 & 0 & 0 & 8 \\
    \quad{} - subtitle & 0 & 0 & 26 & 0 & 26 \\
    \textbf{Multi-Step} & \textbf{0} & \textbf{4} & \textbf{21} & \textbf{1} & \textbf{26} \\
    \midrule
    \textbf{Total} & \textbf{23} & \textbf{79} & \textbf{86} & \textbf{56} & \textbf{244} \\
    \bottomrule
    \end{tabular}
    }
    \end{table}
}
{
}


% results table moved up

\begin{table*}[!h]
    \vspace{-15mm}
  \begin{minipage}{\columnwidth}
    \centering
    \caption{Quality comparison using Hit@k, on 244 test cases}
    \vspace{-4mm}
    \label{tab:hit_at_k_pos} 
    \resizebox{\columnwidth}{!}{%
        \begin{tabular}{ccccccccc}
        \toprule
        \multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{No-example methods}} & \multicolumn{4}{c}{\textbf{By-example methods}} \\ \cmidrule(lr){2-5} \cmidrule(lr){6-9}
         & \textbf{Auto-Tables} & \textbf{TaBERT} & \textbf{TURL} & \textbf{GPT-3.5-fs} & \textbf{FF} & \textbf{FR} & \textbf{SQ} & \textbf{SC} \\
         \midrule
        Hit @ 1 & \textbf{0.570} & 0.193 & 0.029  & 0.196  &  0.283 &  0.336 & 0 & 0 \\
        Hit @ 2 & \textbf{0.697} & 0.455 & 0.071 & - & - & - & 0 & 0 \\
        Hit @ 3 & \textbf{0.75} & 0.545  &  0.109 & - &  - & - & 0 & 0 \\
        Upper-bound & - & - & - & - &  0.471 & 0.545  &  0.369 &  0.369 \\
        \bottomrule
        \end{tabular}
    }
    \end{minipage}\hfill % maximize the horizontal separation
    \begin{minipage}{\columnwidth}
    \centering
    \caption{Synthesis latency per test case }
    \vspace{-4mm}
    \label{tab:running_time}
    \resizebox{\columnwidth}{!}{%
        \begin{tabular}{cccc}
        \toprule 
        \textbf{Method} & \textbf{Auto-Tables} & 
        \textbf{\begin{tabular}[c]{@{}c@{}}Foofah \\ (excl. 110 timeout cases)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}FlashRelate\\ (excl. 91 timeout cases)\end{tabular}}  \\
        \midrule
        50 \%tile &\textbf{0.127s} & 0.287s + human effort & 3.4s + human effort  \\
        90 \%tile &\textbf{0.511s} & 22.891s + human effort & 57.16s + human effort  \\
        95 \%tile &\textbf{0.685s} & 39.188s + human effort & 348.6s + human effort  \\
        Average &\textbf{0.224s} & 5.996s + human effort & 59.194s + human effort  \\
        \bottomrule
        \end{tabular}
    }
  \end{minipage}
\end{table*}

\begin{table*}[!h]
  \begin{minipage}[t]{\columnwidth}
    \centering
    \caption{Ablation Studies of \at}
    \label{tab:ablation}
    \vspace{-4mm}
    \resizebox{\columnwidth}{!}{%
        \begin{tabular}{cccccccc}
        \toprule
        \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Full}} & \multirow{2}{*}{\textbf{No Re-rank}} & \multicolumn{5}{c}{\textbf{No Re-rank \&}} \\ \cmidrule(lr){4-8} 
        
         &  &  & \textbf{No Aug} & \textbf{No Bert} & \textbf{No Syn} & \textbf{1x1 Only} & \textbf{5x5} \\
         \midrule
        Hit@1 & \textbf{0.570} & 0.508 & 0.463 & 0.467 & 0.504 & 0.471 & 0.480 \\
        Hit@2 & \textbf{0.697} & 0.652 & 0.582 & 0.627& 0.648 & 0.607 & 0.594 \\
        Hit@3 & \textbf{0.75} & 0.730 & 0.656 & 0.693 & 0.676 & 0.652 & 0.660 \\
        \bottomrule
        \end{tabular}
    }
    \end{minipage}\hfill % maximize the horizontal separation
    \vspace{-4mm}
      \begin{minipage}[t]{\columnwidth}
        \centering
        \caption{Sensitivity to different semantic embeddings. }
        \label{tab:vary_embed}
        \vspace{-4mm}
        \resizebox{\columnwidth}{!}{%
            \begin{tabular}{ccccc}
            \toprule
            \textbf{Embedding methods} & \textbf{sentenceBERT} & \textbf{fastText} & \textbf{GloVe} & \textbf{No Semantic} \\
            \midrule
            Hit@1 & 0.508 & 0.529 & 0.525 &  0.467 \\
            Hit@2 & 0.652 & 0.656 & 0.676 &  0.627\\
            Hit@3 & 0.730 & 0.734 & 0.734 &  0.734 \\  \hline
            \begin{tabular}[c]{@{}c@{}}Avg. latency per-case \\ w/ this embedding\end{tabular}
            & 0.299s & 0.052s & 0.050s & 0.026s \\
            \bottomrule
            \end{tabular}
        }
  \end{minipage}
\end{table*}

\stitle{Evaluation Metrics.} We evaluate the quality and efficiency of different algorithms in synthesizing transformations.

\underline{Quality}. Given an input table $T$, an algorithm $A$ may generate top-$k$ transformations $(\hat{M}_1, \hat{M}_2, ... \hat{M}_k)$, ranked by probabilities, for users to inspect and pick. We evaluate the success rate of synthesis using the standard $Hit@k$ metric~\cite{ir-book}, defined as:
\begin{align*}
    Hit@k(T) = \sum_{i=1}^k \mathbf{1}(\hat{M}_{i}(T) = M_g(T))
\end{align*}
which looks for exact matches between the top-$k$ ranked predictions ($\hat{M}_{i}(T), 1 \leq i \leq k$) and the ground-truth $M_g(T)$.  The overall $Hit@k$ on the entire benchmark, is then simply the average across all test cases $T$. We report $Hit@k$ up to $k=3$.

%defined as the fraction of cases for which the ground truth appears in the top-$k$ predictions. Let $Y_{i}$ denote the ground truth relationalized version of the input table. Then the $Hit@k$ score on a test dataset $D_{test}$ can be computed as: 
%    $$Hit@k = \frac{1}{{|D_{test}|}} \sum_{T_i \in D_{test}}\sum_{j=1}^k \mathbb{1}(\hat{Y}_{ij} = Y_i)$$

%The $Hit@k$ reflects the effectiveness of our method in real-world use cases: we show users $k$ (e.g., $k = 3$) transformed tables induced by the top $k$ synthesized pipelines. If the ground truth relationalized table is included, users can easily identified it and such synthesis can be considered as useful and effective.  
% 
 % Assume that users can look at each table and determine whether a table is relationalized or not. In such case,
%     $$Hit@k = \frac{1}{{|\text{\atbenchpos}|}} \sum_{T_i \in \text{\atbenchpos}}\sum_{j=1}^k \mathbb{1}(\hat{Y}_{ij} = Y_i)$$

%     Note that only non-relational tables (i.e. \atbenchpos) are used for the evaluation as users would simply skip input tables that are already relationalized (i.e. \atbenchneg).

% \begin{itemize}[leftmargin=*]
%     \item Case 1: Assume that users can look at each table and determine whether a table is relationalized or not. In such case, we can show users $k$ (e.g., $k = 3$) transformed tables induced by the top $k$ synthesized pipelines. If the ground truth relationalized table is included, users can easily identified it and such synthesis can be considered as useful and correct. To evaluate the effectiveness of our method in this use case, we use the standard $Hit@k$ score~\cite{ir-book} as our metric, which is defined as the fraction of test cases where the ground truth appears in the top $k$ predictions. Let $Y_{i}$ denote the ground truth relationalized version of the input table. Then the $Hit@k$ score can be computed as follows. 

%     $$Hit@k = \frac{1}{{|\text{\atbenchpos}|}} \sum_{T_i \in \text{\atbenchpos}}\sum_{j=1}^k \mathbb{1}(\hat{Y}_{ij} = Y_i)$$

%     Note that only non-relational tables (i.e. \atbenchpos) are used for the evaluation as users would simply skip input tables that are already relationalized (i.e. \atbenchneg).

%     \item Case 2: Assume that users have no knowledge about the relationalization or do not want to manually look at each table. 
%     In this case, users are most likely to use only the top $1$ synthesized pipeline and they may run the algorithm on both relational and non-relational tables. Therefore, our method not only needs to transform non-relational tables correctly, but also needs to avoid incorrectly transforming tables that are already relational (i.e., predict \code{None} on these tables). To evaluate the performance in this case, we use the standard accuracy score computed as follows, which only takes the top 1 prediction ($\hat{Y}_{i1}$) and is evaluated on both relational and non-relational input tables. 

%     $$Accuracy = \frac{\sum_{T_i \in \text{\atbenchpos} \cup  \text{\atbenchneg}} \mathbb{1}(\hat{Y}_{i1} = Y_i)}{{|\text{\atbenchpos} \cup \text{\atbenchneg}|}} $$

%     Note that in this case, we cannot expect users to provide any ground truth examples for running by-example baseline methods,  such as Foofah and Flashrelate. As as result, we does not consider the by-example baseline methods for this evaluation.
% \end{itemize}
% \yeye{Can discuss: not sure if we need to create two metrics and two settings here, may be a bit over-complicated for reviewers.}

    




% We use the standard Hit@k~\cite{ir-book} as our metric for the effectiveness of synthesis. For each test table, an algorithm can generate $k$ synthesized steps, ranked by probability scores. If the $j$-th prediction is identical to the ground truth\footnote{
% We consider semantic equivalence between operators (e.g., wide-to-long is equivalent to unpivot-followed-by-pivot), and mark synthesized transformations correct as long as output are identical.}, we call it a ``\textit{hit}''. The Hit@k is then defined as the proportion of test cases that have a hit in the top $k$ predictions:
% $$Hit@k = \frac{\sum_{i=1}^n\sum_{j=1}^k \mathbb{1}(y_{ij} == y_i)}{n}$$
% Where $n$ is the total number of test case, $y_i$ the ground truth for the $i$-th test case, and $\hat{y}_{ij}$ the $j$-th prediction for the $i$-th test case. Hit@k is always in [0, 1], where higher is better.


% Unlike our \at method that needs to automatically detect and differentiate between positive examples that require transformations (non-relational tables), and negative examples  that do not require transformations (tables that are already relational), baseline methods with by-example synthesis are designed to operate on positive-examples only. Therefore, we also report a ``Positive-Hit@K (Pos-Hit@K)'' evaluated only on the 194 positive examples, in order to make apple-to-apple comparisons with baselines.

\underline{Efficiency}. We report the latency of synthesis using wall-clock time. All experiments are conducted on a Linux VM with 24 vCPU cores, and 4 Tesla P100 GPUs.


% \begin{table*}[!h]
%   \begin{minipage}{\columnwidth}
%     \centering
%     \caption{Quality comparison using Hit@k, on 194 test cases}
%     \vspace{-4mm}
%     \label{tab:hit_at_k_pos} 
%     \resizebox{\columnwidth}{!}{%
%         \begin{tabular}{ccccccccc}
%         \toprule
%         \multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{No-example methods}} & \multicolumn{4}{c}{\textbf{By-example methods}} \\ \cmidrule(lr){2-5} \cmidrule(lr){6-9}
%          & \textbf{Auto-Tables} & \textbf{TaBERT} & \textbf{TURL} & \revised{\textbf{GPT-3.5 (T-O)}} & \textbf{FF} & \textbf{FR} & \textbf{SQ} & \textbf{SC} \\
%          \midrule
%         Hit @ 1 & \textbf{0.562} & 0.187 & 0.027 & \revised{0.216} & 0.285 & 0.351 & 0 & 0 \\
%         Hit @ 2 & \textbf{0.68} & 0.43 & 0.075 & - & - & - & 0 & 0 \\
%         Hit @ 3 & \textbf{0.722} & 0.539 & 0.124 & - &  - & - & 0 & 0 \\
%         Upper-bound & - & - & - & - & 0.421 & 0.538 & 0.34 & 0.34 \\
%         \bottomrule
%         \end{tabular}
%     }
%     \end{minipage}\hfill % maximize the horizontal separation
%     \begin{minipage}{\columnwidth}
%     \centering
%     \caption{Synthesis latency per test case }
%     \vspace{-4mm}
%     \label{tab:running_time}
%     \resizebox{\columnwidth}{!}{%
%         \begin{tabular}{cccc}
%         \toprule 
%         \textbf{Method} & \textbf{Auto-Tables} & 
%         \textbf{\begin{tabular}[c]{@{}c@{}}Foofah \\ (excl. 89 timeout cases)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}FlashRelate\\ (excl. 91 timeout cases)\end{tabular}}  \\
%         \midrule
%         50 \%tile &\textbf{0.168s} & 0.326s + human effort & 3.4s + human effort  \\
%         90 \%tile &\textbf{0.553s} & 27.146s + human effort & 57.16s + human effort  \\
%         95 \%tile &\textbf{0.690s} & 41.411s + human effort & 348.6s + human effort  \\
%         Average &\textbf{0.299s} & 6.995s + human effort & 59.194s + human effort  \\
%         \bottomrule
%         \end{tabular}
%     }
%   \end{minipage}
% \end{table*}


% \begin{table*}[!h]
%   \begin{minipage}[t]{\columnwidth}
%     \centering
%     \caption{Ablation Studies of \at}
%     \label{tab:ablation}
%     \vspace{-4mm}
%     \resizebox{\columnwidth}{!}{%
%         \begin{tabular}{cccccccc}
%         \toprule
%         \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Full}} & \multirow{2}{*}{\textbf{No Re-rank}} & \multicolumn{5}{c}{\textbf{No Re-rank \&}} \\ \cmidrule(lr){4-8} 
        
%          &  &  & \textbf{No Aug} & \textbf{No Bert} & \textbf{No Syn} & \textbf{1x1 Only} & \textbf{5x5} \\
%          \midrule
%         Hit@1 & \textbf{0.562} & 0.51 & 0.454 & 0.474 & 0.51 & 0.49 & 0.485 \\
%         Hit@2 & \textbf{0.680} & 0.634 & 0.572 & 0.629 & 0.665 & 0.613 & 0.603 \\
%         Hit@3 & \textbf{0.722} & 0.716 & 0.66 & 0.701 & 0.696 & 0.665 & 0.649\\
%         \bottomrule
%         \end{tabular}
%     }
%     \end{minipage}\hfill % maximize the horizontal separation
%       \begin{minipage}[t]{\columnwidth}
%         \centering
%         \caption{Sensitivity to different semantic embeddings. }
%         \label{tab:vary_embed}
%         \vspace{-4mm}
%         \resizebox{\columnwidth}{!}{%
%             \begin{tabular}{ccccc}
%             \toprule
%             \textbf{Embedding methods} & \textbf{sentenceBERT} & \textbf{fastText} & \textbf{GloVe} & \textbf{No Semantic} \\
%             \midrule
%             Hit@1 & 0.51 & 0.521 & 0.536 & 0.474 \\
%             Hit@2 & 0.634 & 0.644 & 0.665 & 0.629 \\
%             Hit@3 & 0.716 & 0.722 & 0.727 & 0.701 \\  \hline
%             \begin{tabular}[c]{@{}c@{}}Avg. latency per-case \\ w/ this embedding\end{tabular}
%             & 0.299s & 0.052s & 0.050s & 0.026s \\
%             \bottomrule
%             \end{tabular}
%         }
%   \end{minipage}
% \end{table*}



\stitle{Methods Compared.} We compare with the following methods.

\begin{itemize}[leftmargin=*]
\item \textit{\at}. This is our approach and is the only method that does not require users to provide input/output examples (unlike other existing methods). In order to train \at, we generate 1.4M (input-table, transformation) pairs evenly distributed across 8 operators, following the self-supervision procedure  (Section~\ref{sec:training_data_generation}), using 15K base relational tables crawled from public sources~\footnote{We use the dataset from~\cite{lin2023auto}, which has thousands of relational Power-BI models crawled from public sources. We sample 15K fact and dimension tables from these models as our ``base'' relational tables. %Details of our data pre-processing can be found in our technical report~\cite{full} in the interest of space. 
Since our training data is collected via Power-BI data models, they are completely separate from our test data (Web and Excel tables).}. We take a fixed size of input with the first 100 rows and 50 columns at the top-left corner of each table and use zero-padding for tables with less rows or columns. We implement our method using 
PyTorch~\cite{paszke2017automatic}, trained using Adam optimizer, with a learning rate of 0.001 for 50 epochs, using a batch size of 256. 

%To ensure that all tables in a batch have the same size, we crop/pad the input table to have $100$ rows and $50$ columns.

% \kr{can you briefly comment on how similar/different is the training tables from the test case tables? e.g., average number of rows, columns etc.}

\item \textit{Foofah (FF)}~\cite{jin2017foofah} synthesizes transformations based on input/output examples. We use 100 cells from the top-right of the ground-truth output table for Foofah to synthesize programs, which simulate the scenario where a user types in 100 output cells (a generous setting as it is unlikely that users are willing to provide so many examples in practice).
We test Foofah using the authors original implementation~\cite{foofah-code}, and we time-out each case after 30 minutes. 

\item \textit{Flash-Relate (FR)}~\cite{barowy2015flashrelate} is another approach to synthesize table-level transformations, which however would require input/output examples. We used an open-source re-implementation of FlashRelate~\cite{flash-relate-code} (since the original system is not publicly available), and we provide it with 100 example output cells from the ground-truth. We use  a similar time-out of 30 minutes for each test case.

\item \textit{SQLSynthesizer (SQ)}~\cite{SQLSynthesizer} is a SQL-by-example algorithm that synthesizes SQL queries based on input/output examples. We use the authors implementation~\cite{Scythe-code}, provide it with 100 example output cells, and also set a time-out of 30 minutes.


\item \textit{Scythe (SC)}~\cite{sql-by-example} is another SQL-by-example method. We used the author's implementation~\cite{PATSQL-code} and provide it with 100 example output cells, like previous methods.
%\item \textit{QBO}~\cite{qbo} \yeye{All 3 Sql-by-example work should technically have 0 coverage of the tasks we have. Though can still try experimentally with Scythe to see.}

\item \textit{TaBERT}~\cite{yin2020tabert} is a table representation approach developed in the NLP literature, and pre-trained using table-content and captions for NL-to-SQL tasks. To test the effectiveness of TaBERT in our transformation task, we replace the table representation in \at (i.e., output of the feature extraction layer in Figure~\ref{fig:input_model_arch}) with TaBERT's representation, and  train the following fully connected layers using the same training data as ours.

% We replace our table representation in \at (Section~\ref{subsec:input-model}) with TaBERT's representation, to test the effectiveness of TaBERT in our transformation task.

\item \textit{TURL}~\cite{deng2022turl} is another table representation approach for data integration tasks. Similar to \textit{TaBERT}, we test the effectiveness of TURL by replacing \at representation with  TURL's.


\item \textit{GPT}~\cite{gpt} is a family of large language models pre-trained on text and code, which can follow instructions to perform a variety of tasks. While we do not expect GPT to perform well on \at tasks, we perform a comparison nevertheless, using GPT-3.5\footnote{We used the ``gpt-3.5-turbo'' API endpoint, accessed in July 2023.} as a baseline. We perform few-shot in-context learning, using a description of the operators, together with pairs of (input-table, desired-operator) in the prompt to demonstrate the task. We provide one example demonstration per operator, for a total of 7 examples (which fit in the context allowed by GPT-3.5). We denote this method as GPT-3.5-fs (few-shot).\footnote{Note that GPT-3.5-fs is still a no-example method, as we use general-purpose examples to demonstrate each operator in our few-shot examples, which are fixed and do not vary based on different input tables.}
\end{itemize}



%\stitle{Training Process.} We implement our method using PyTorch~\cite{paszke2017automatic}. We train our model using a Adam optimizer with learning rate 0.001 for 50 epochs. The batch size is set to be 256. To ensure that all tables in a batch have the same size, we crop/pad the input table to have $100$ rows and $50$ columns.


% Since we are more interested in the performance of our method on non-relational tables, we also compute the Hit@k score only using non-relational tables and we name it as \textit{Positive Hit@k}, defined as follows.

% $$Pos \ Hit@k = \frac{\sum_{i=1}^n\sum_{j=1}^k \mathbb{1}(y_{ij} == y_i \land y_i \neq \text{None})}{\sum_{i=1}^n \mathbb{1}(y_i \neq \text{None})}$$

% We first compare the end-to-end performance of different methods. In this case, we run our model until the predicted operator is None.


%\subsection{Overall Comparisons}


\subsection{Experiment Results}

% % \begin{table}[!h]
% \begin{table}[t]
% \caption{Quality comparison using Hit@k, on 194 test cases}
% \label{tab:hit_at_k_pos} 
% \vspace{-3mm}
% \scalebox{0.75}{
% \begin{tabular}{cccccccc}
% \toprule
% \multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c}{\textbf{No-example methods}} & \multicolumn{4}{c}{\textbf{By-example methods}} \\ \cmidrule(lr){2-4} \cmidrule(lr){5-8}
%  & \textbf{Auto-Tables} & \textbf{TaBERT} & \textbf{TURL} & \textbf{FF} & \textbf{FR} & \textbf{SQ} & \textbf{SC} \\
%  \midrule
% Hit @ 1 & \textbf{0.562} & 0.187 & 0.027 & 0.285 & 0.351 & 0 & 0 \\
% Hit @ 2 & \textbf{0.68} & 0.43 & 0.075 & - & - & 0 & 0 \\
% Hit @ 3 & \textbf{0.722} & 0.539 & 0.124 & - & - & 0 & 0 \\
% Upper-bound & - & - & - & 0.421 & 0.538 & 0.34 & 0.34 \\
% \bottomrule
% \end{tabular}
% }
% \end{table}
% % \vspace{-2em}
% \begin{table}[t]
% \caption{Synthesis latency per test case }
% \label{tab:running_time}
% \vspace{-3mm}
% \scalebox{0.75}{
% \begin{tabular}{cccc}
% \toprule 
% \textbf{Method} & \textbf{Auto-Tables} & 
% \textbf{\begin{tabular}[c]{@{}c@{}}Foofah \\ (excl. 89 timeout cases)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}FlashRelate\\ (excl. 91 timeout cases)\end{tabular}}  \\
% \midrule
% 50 percentile &\textbf{0.168s} & 0.326s + human effort & 3.4s + human effort  \\
% 90 percentile &\textbf{0.553s} & 27.146s + human effort & 57.16s + human effort  \\
% 95 percentile &\textbf{0.690s} & 41.411s + human effort & 348.6s + human effort  \\
% Average &\textbf{0.299s} & 6.995s + human effort & 59.194s + human effort  \\
% \bottomrule
% \end{tabular}
% % \vspace{-3mm}
%  }
% \end{table}


% Figure environment removed



\stitle{Quality Comparison.}  Table~\ref{tab:hit_at_k_pos} shows the comparison between \at and baselines, evaluated on our benchmark with 244 test cases. We group all methods into two classes:  (1) ``No-example methods'' that do not require users to provide any input/output examples, which include our \at, and variants of \at that use TaBERT and TURL for table representations, respectively; and (2) ``By-example methods'' that include Foofah (FF), FlashRelate (FR), SQLSynthesizer (SQ), and Scythe (SC), all of which are provided with 100 ground truth example cells. 

As we can see, \at significantly outperforms all other methods, successfully transforming 75\% of test cases in its top-3, \textit{without needing users to provide any examples},  despite the challenging nature of our tasks. Recall that in our task, even for a single-step transformation, there are thousands of possible operators+parameters to choose from (e.g., a table with 50 columns that requires ``\code{stack}'' will have 50x50 = 2,500 possible parameters of start\_idx and end\_idx) and for two-step transformations, the search space is in the millions (e.g., for ``\code{stack}'' alone it is $2500^2 \approx 6M$), which is clearly non-trivial. 

% This is despite our task being very challenging -- the search space for 2-step transformations alone amounts to $270^2 \approx 72K$ possible choices (recall that our output vector has 270 dimensions), which makes this clearly non-trivial. 

Compared to other no-example methods, \at outperforms TaBERT and TURL respectively by 37.7 and 54.1 percentage point on Hit@1, 20.5 and 64.1 percentage point on Hit@3. This shows the strong benefits for using our proposed table representation and model architecture, which are specifically designed for the table transformation task (Section~\ref{subsec:input-model}). 

% Compared to TaBERT and TURL, we see strong benefits (30-50 percentage points) for using our proposed table representation and model architecture, which are specifically designed for the table transformation task (Section~\ref{subsec:input-model}). 

Compared to by-example methods, the improvement of \at is similarly strong. Considering the fact that these baselines use 100 output example cells (which users need to manually type), whereas our method uses 0 examples, we argue that \at is clearly a better fit for the table-restructuring task at hand. Since some of these methods (FF and FR) only return top-1 programs, we also report in the last row their ``upper-bound'' coverage, based on their DSL (assuming all transformations supported in their DSL can be successfully synthesized). 

%\noindent (1)  \at achieves Hit@1 of 0.562, which is 0.356 better than TaBERT (TA) and 0.535 better than TURL. Comparing with by-example methods that take advantages of users' provided ground truth cells, \at outperforms Foofah (FF) and Flashrelate (FR) by 0.277 and 0.201, respectively. We would like to highlight that although there are only 8 operators, the search space is very large as operators can have parameters and the end-to-end pipeline can contain multiple operators. Therefore, 0.562 is a considerably high scores for Hit@1, which means 56.2\% non-relational test tables can be successfully relationalized with the top 1 synthesized pipeline predicted by \at.

%\noindent (2)  \at achieves Hit@3 of 0.722, which indicates if users examine the top 3 predictions given by \at, the ground truth relationalized tables can be found in 72.2\% of them. In comparison, TaBERT (TA) and TURL is 0.176 and 0.198 worse than our method on Hit@3. Foofah and Flashrelate only output one prediction for each table and thus we cannot compute their Hit@k scores for $k > 1$. Instead, we compute the upper bound Hit@k score (UB) for Foofah (FF) and Flashrelate (FR) by counting the test cases where the ground truth operators are covered by their search space. Note that in practice, the ground truth operators may never be found by Foofah and Flashrelate given the time limit. Even with the best possible scores, Foofah and Flashrelate are still 0.301 and 0.184 worse than \at on Hit@3.

%\noindent (3) TURL performs poorly in general, especially on its Hit@1. This is because it focuses on Web tables, especially tables with known entities. However, most of our test cases are not web tables or web tables with only few known entities.

%\noindent (4) SQL-by-example methods, including SQLSynthesizer (SQ) and Scythe (SC), achieve 0 Hit@k scores, which means they are not able to find SQL queries to transform the input table into the ground truth relationalized table within the given time limit. This is because the SQL queries that perform equivalent transformations as our operators are usually complicated. For example, writing the standard SQL queries that transpose a table can involve multiple case-when subqueries, grouping and aggregations. Therefore, it is difficult to synthesize such a complex SQL query. We nevertheless compute the upper bound Hit@k for them by counting the ground truth operators that can be synthesized by SQL queries. Even compared with this best possible score, our method still outperforms the SQL-by-example methods by a large margin.

%\yeye{Due to space, we will likely need to move ``result breakdown by benchmarks'' and ``quality results with relational tables'' to the full version, which is currently hidden using ``iftoggle\{fullversion\}'', which you can turn on and off in main.tex. Just FYI, we may not need to spend time on these two experiments before the deadline.}


\iftoggle{fullversion}
{

    \underline{Result breakdown by benchmark sources.} We additionally drill down on our quality results, using a break-down by benchmark data sources (forum, notebooks, and Excel+web). We report in Table~\ref{tab:quality-breakdown} the performance of three best-performing methods: \at (AT), TabERT (TA), FlashRelate (FR), in the interest of space.  

    It can be seen from the table that the quality of \at is consistent across the three, confirming its effectiveness across diverse test cases arising from different sources.

    
        
    %     \begin{table}[t]
    %     \caption{Quality comparisons, broken down by data sources}
    %     \label{tab:quality-breakdown}
    %     \scalebox{0.8}{
    %     \begin{tabular}{cccccccccc}
    %     \toprule
    %      & \multicolumn{3}{c}{\textbf{Forum}} & \multicolumn{3}{c}{\textbf{Notebook}} & \multicolumn{3}{c}{\textbf{Excel+Web}} \\ \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}
    %     \textbf{Method} & \textbf{AT} & \textbf{TA} & \textbf{FR} & \textbf{AT} & \textbf{TA} & \textbf{FR} & \textbf{AT} & \textbf{TA} & \textbf{FR} \\ \midrule
    
    % Hit @ 1 & \textbf{0.521} & 0.217 & 0.043 & \textbf{0.582} & 0.215 & 0.278 & \textbf{0.554} & 0.154 & 0.489 \\
    % Hit @ 2 & \textbf{0.696} & 0.522 & - & \textbf{0.722} & 0.531 & - & \textbf{0.641} & 0.319 & - \\
    % Hit @ 3 & \textbf{0.696} & 0.565 & - & \textbf{0.747} & 0.633 & - & \textbf{0.706} & 0.451 & -\\
        
    %     \bottomrule
    %     \end{tabular}
    %     }
    %     \end{table}
    \begin{table}[t]
    \vspace{3mm}
    \caption{Quality comparisons by data sources}
    \vspace{-3mm}
    \label{tab:quality-breakdown}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{ccccccccccccc}
    \toprule
     & \multicolumn{3}{c}{\textbf{Forum}} & \multicolumn{3}{c}{\textbf{Notebook}} & \multicolumn{3}{c}{\textbf{Excel}} & \multicolumn{3}{c}{\textbf{Web}} \\ \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}\cmidrule(lr){11-13}
    \textbf{Method} & \textbf{AT} & \textbf{TA} & \textbf{FR} & \textbf{AT} & \textbf{TA} & \textbf{FR} & \textbf{AT} & \textbf{TA} & \textbf{FR} & \textbf{AT} & \textbf{TA} & \textbf{FR} \\
    \midrule
    Hit @ 1 & \textbf{0.522} & 0.217 & 0.043 & \textbf{0.582} & 0.241 & 0.278 & \textbf{0.558} & 0.174 & 0.5 & \textbf{0.589} & 0.143 & 0.286 \\
    Hit @ 2 & \textbf{0.696} & 0.478 & - & \textbf{0.722} & 0.557 & - & \textbf{0.651} & 0.442 & - & \textbf{0.732} & 0.321 & - \\
    Hit @ 3 & \textbf{0.696} & 0.565 & - & \textbf{0.747} & 0.62 & - & \textbf{0.709} & 0.547 & - & \textbf{0.839} & 0.429 & - \\
    \bottomrule
    \end{tabular}
    }
    \end{table}
}
{
    \underline{Additional quality results.} We report additional results on quality, such as  a breakdown by benchmark sources, and Hit@K in the presence of input tables that are already relational (for which \at should correctly detect and not over-trigger, by performing no transformations), in our technical report~\cite{full}.
}



% removed during revision, add back to full version
%\revised{}
%\begin{comment}
\iftoggle{fullversion}
{
    \underline{Quality comparisons in the presence of relational tables.}  Recall that since \at can detect input tables that are already relational, and predict ``\code{none}'' for such tables, an additional use case of \at is to invoke it on all input tables encountered in spreadsheets or on the web, which include both relational tables (requiring no transformations) and non-relational tables (requiring transformations), such that any tables that \at predicts to require transformations can then be surfaced to users to review and approve. Note that this is a use case that by-example baselines cannot support, as they require users to first manually scan and identify tables requiring transformations.

    For this purpose, we test \at on the 244 test cases that require transformations, as well as the corresponding 244 output tables that are already relational and require no transformations. Using this collection of 488 cases, we not only test whether \at can correctly synthesize transformations on non-relational input, but also whether it can correctly predict ``\code{none}'' on the relational tables not requiring transformations, using the same $Hit@K$. 
 
    Table~\ref{tab:hit_at_k} shows \at achieves high quality, suggesting that it does not ``over-trigger'' on tables that are already relational, and can be effective at this task.
    
    % \begin{table}[t]
    % \vspace{-2mm}
    % \scalebox{0.8}{
    % \begin{tabular}{cccc}
    % \toprule
    % \textbf{Method} & \textbf{Auto-Tables} & \textbf{TaBERT} & \textbf{TURL} \\
    % \midrule
    % Hit @ 1 & \textbf{0.667} & 0.276 & 0.176 \\
    % Hit @ 2 & \textbf{0.781} & 0.584 & 0.392 \\
    % Hit @ 3 & \textbf{0.829} & 0.702 & 0.454 \\
    % \bottomrule
    % \end{tabular}
    % }
    % \caption{Quality comparison using Hit@k, on 194 cases with non-relational input (requiring transformations), and 194 cases with relational input (not requiring transformations).}
    % \label{tab:hit_at_k}
    % \end{table}

    \begin{table}[t]
    \vspace{5mm}
    \caption{Quality comparison using Hit@k, on 244 cases with non-relational input (requiring transformations), and 244 cases with relational input (not requiring transformations).}
    \vspace{-2mm}
    \scalebox{0.8}{
    \begin{tabular}{cccc}
    \toprule
    \textbf{Method} & \textbf{Auto-Tables} & \textbf{TaBERT} & \textbf{TURL} \\
    \midrule
    Hit @ 1 & \textbf{0.695} & 0.258 & 0.175 \\
    Hit @ 2 & \textbf{0.803} & 0.594 & 0.387 \\
    Hit @ 3 & \textbf{0.840} & 0.699 & 0.444 \\
    \bottomrule
    \end{tabular}
    }
    \label{tab:hit_at_k}
    \end{table}
}
{

}
%\end{comment}

% Table~\ref{tab:hit_at_k} shows the overall comparison between our methods and baseline methods. As we can see, \at significantly outperforms all other methods. Specifically, \at achieves Hit@1 of 0.667, which is 0.386 better than TaBERT and 0.491 better than TURL. Comparing with by-example methods, \at achives  Positive Hit@1 of 0.541, which is 0.003 and 0.12 better than the best possible score that Flashrelate and Foofah can achieve, respectively. We would like to highlight that although there are only 8 operators, the search space is very large as operators can have parameters and the end-to-end pipeline can contain multiple operators. Therefore, 0.667 and 0.541 are a considerably high scores for Hit@1, which means \at can generate the end-to-end pipeline correctly with its top-1 prediction in 66.7\% test cases and 54.1\% positive test cases. For Hit@3 and Positive Hit@3, \at achieves 0.829 and 0.722, respectively. This indicates if we examine the top 3 predictions given by \at, the ground truth can be found in 82.9\% of text cases and 72.2\% positive test cases. In comparison, TaBERT is 0.129 worse on Hit@3 and 0.176 worse on Positive Hit@3 than our method. TURL performs poorly on positive cases. This is because it focuses on Web tables, especially tables with entities. However, most our test cases are not web tables with few known entities.






% % Figure environment removed


\stitle{Running Time.} Table~\ref{tab:running_time} compares the average and 50/90/95-th percentile latency, of all methods to synthesize one test case. \at is interactive with sub-second latency on almost all cases, whose average is 0.224.  Foofah and FlashRelate take considerably longer to synthesize, even after we exclude cases that time-out after 30 minutes. This is also not counting the time that users would have to spend typing in output examples for these by-example methods, which we believe make \at substantially more user-friendly for our transformation task.  

%to make predictions on each table and takes less than 0.69s on 95\% tables,  which can be used for real-time prediction. In comparison, Foofah and Flashrelate exceed the time limit (i.e., 30 minutes) on 89 and 91 out of 194 test cases, respectively. Excluding the timeout cases, Foofah and Flashrelate respectively take on average 6.995s and 59.194s on each table. In addition, using by-example methods (e.g., Foofah, Flashrelate), users need to provide ground truth output cells (e.g., up to 100 cells in our experiments), which can take considerable time. 

%We sort the test cases by the number of input cells and group every 20 cases as a bin. 
Figure~\ref{fig:running_time} shows the average latency of \at, on cases with different number of non-empty input cells. As we can see, the latency grows linearly as the number of cells  increases, but since we only need to use at most the top-left 100 rows and 50 columns to correctly synthesize a program, this is always bounded by a couple of seconds at most. Furthermore, we notice that the running time is dominated by SentenceBERT embedding, which accounts for 91.5\% of the latency. 
In comparison, the actual inference time of \at (the green line) is very small and almost constant.

% Figure~\ref{fig:running_time} shows the running time of our method to make prediction on each table. As we can see, the total running time grows as the number of cells in tables grows and is dominated by SentenceBERT embedding. Note that the size of input for our model is fixed by padding, regardless of the actual number of cells in the input table, and hence the time cost on our model (orange line) is almost unchanged in Figure \ref{fig:running_time} and only accounts for a small portion in the total running time. On average, the total running time on each table is 0.25s, which can be used for real-time prediction. In comparison, 
% using by-example methods (e.g., Foofah, Flashrelate), users need to provide 3-5 examples. This corresponds to  typing on average 27 cells or 213 characters for each table in our test cases, which would be much slower than our methods.



% \stitle{Running Time.} Figure~\ref{fig:running_time} shows the running time of our method to make prediction on each table. As we can see, the total running time grows as the number of cells in tables grows and is dominated by SentenceBERT embedding. Note that the size of input for our model is fixed by padding, regardless of the actual number of cells in the input table, and hence the time cost on our model (orange line) is almost unchanged in Figure \ref{fig:running_time} and only accounts for a small portion in the total running time. On average, the total running time on each table is 0.25s, which can be used for real-time prediction. In comparison, 
% using by-example methods (e.g., Foofah, Flashrelate), users need to provide 3-5 examples. This corresponds to  typing on average 27 cells or 213 characters for each table in our test cases, which would be much slower than our methods.




\begin{comment}
% Figure environment removed


% Figure environment removed
\end{comment}



% \begin{table}[t]
% \caption{Ablation Studies of \at}
% \vspace{-3mm}
% \label{tab:ablation}
% \scalebox{0.72}{
% \begin{tabular}{cccccccc}
% \toprule
% \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Full}} & \multirow{2}{*}{\textbf{No Re-rank}} & \multicolumn{5}{c}{\textbf{No Re-rank \&}} \\ \cmidrule(lr){4-8} 
%  &  &  & \textbf{No Aug} & \textbf{No Bert} & \textbf{No Syn} & \textbf{1x1 Only} & \textbf{5x5} \\
%  \midrule
% Hit@1 & \textbf{0.562} & 0.51 & 0.454 & 0.474 & 0.51 & 0.49 & 0.485 \\
% Hit@2 & \textbf{0.680} & 0.634 & 0.572 & 0.629 & 0.665 & 0.613 & 0.603 \\
% Hit@3 & \textbf{0.722} & 0.716 & 0.66 & 0.701 & 0.696 & 0.665 & 0.649\\
% \bottomrule
% \end{tabular}
% }
% % \vspace{-2em}
% \end{table}
% \begin{table}[t]
% \caption{Sensitivity to different semantic embeddings. }
% \vspace{-3mm}
% \label{tab:vary_embed}
% \scalebox{0.75}{
% \begin{tabular}{ccccc}
% \toprule
% \textbf{Embedding methods} & \textbf{sentenceBERT} & \textbf{fastText} & \textbf{GloVe} & \textbf{No Semantic} \\
% \midrule
% Hit@1 & 0.51 & 0.521 & 0.536 & 0.474 \\
% Hit@2 & 0.634 & 0.644 & 0.665 & 0.629 \\
% Hit@3 & 0.716 & 0.722 & 0.727 & 0.701 \\  \hline
% \begin{tabular}[c]{@{}c@{}}Avg. latency per-case \\ w/ this embedding\end{tabular}
% & 0.299s & 0.052s & 0.050s & 0.026s \\
% \bottomrule
% \end{tabular}
% }
% \end{table}
% \vspace{-2mm}
\stitle{Ablation Study}
We perform ablation studies to understand the benefit of \at components, which is shown in Table~\ref{tab:ablation}.


\underline{Contribution of Input/Output Re-Ranking.} To study the contribution of our re-ranking model (Section~\ref{subsec:rerank}), we compare the performance of \at with and without re-ranking. Table~\ref{tab:ablation} shows that our ``Full'' method (with re-ranking) produces substantially better Hit@1 and Hit@2 compared to ``No Re-rank''. %However, there is no substantial difference for Hit@3. This is because the good options are already exhausted at top-3 positions, making the resulting difference small.

% \stitle{Contribution of Input/Output Re-Ranking.} To understand the contribution of our re-ranking (Section~\ref{subsec:rerank}), we evaluate the performance of \at with and without re-ranking. As we can see in Table~\ref{tab:ablation}, re-ranking produces substantial improvement for both Hit and Pos-Hit at the top-2 positions. There is no substantial difference for Hit@3 and Pos-Hit@3, likely because the good options are already exhausted at rank-3 positions, making the resulting difference small.


\underline{Contribution of Data Augmentation.}  To study the benefits of data augmentation in training data generation (Section~\ref{sec:training_data_generation}), we disable augmentation when generating training data (i.e., using only the base relational tables). Table~\ref{tab:ablation} shows this result under ``No Aug'', which suggests that our Hit@k drop substantially, underscoring the importance of data augmentation.

% Data augmentation increases the variation of the training set by producing multiple training examples from each relational base table .

\underline{Contribution of Embeddings.} Recall that we use both syntactic embedding and semantic embedding (sentenceBERT) to represent each cell (Section~\ref{subsec:input-model}). To understand their contributions, we remove each embedding in turn, and the results are shown under ``No Bert'' and ``No Syntactic'' in Table \ref{tab:ablation}. Both results show a substantial drop in performance, confirming their importance (semantic embedding with sentenceBERT is likely more important, as removing it leads to a more significant drop).

%two types of embeddings (No Rerank), the Hit@k generally scores become worse after dropping one of them, except that Hit@2 is slightly increased after dropping syntactic embeddings. This means both types of embeddings help \at to predict the desired transformations. Also, we can see that sentenceBERT is more important, as its removal leads to a more significant drop in overall quality. 

\underline{Contribution of 1D Filters.} Recall that we use convolution filters of size 1x1 and 1x2 to extract features from rows and columns (Section~\ref{subsec:input-model}). To understand the effectiveness of this design, we evaluate our method with alternative filters. First, we replace all the 1x2 filters with 1x1 filters. The result is labeled ``1x1 Only'' and shows a significant drop. %Especially on Hit@1, the score is dropped by 0.088 compared to the original results (No Rerank). This is because using 1x1 filters only may not be easy to capture some information such as the variance of values in a column or row. 
Second, we replace all filters with filters of size 5x5 that is common in computer vision tasks~\cite{alexnet, vgg}, which leads to another substantial drop. Both results confirm the effectiveness of our model design that is tailored to table tasks.



% We remove sentenceBERT and syntactic embedding from \at, to understand their contributions to the overall result. These are shown as ``No Bert'' and ``No Syntactic'' in Table \ref{tab:ablation}. As we can see, while both types of embedding help \at to find the desired transformations, though sentenceBERT is more important, as its removal leads to a more significant drop in overall quality. 

% \stitle{Contribution of Embedding.} We remove sentenceBERT and syntactic embedding from \at, to understand their contributions to the overall result. These are shown as ``No Bert'' and ``No Syntactic'' in Table \ref{tab:ablation}. As we can see, while both types of embedding help \at to find the desired transformations, though sentenceBERT is more important, as its removal leads to a more significant drop in overall quality. 

% \stitle{Contribution of Feature Extraction.} To understand the effectiveness of our model architecture, we evaluate our method with different model architectures. 

%after dropping the sentenceBERT embedding, the performance is dropped significantly. However, after dropping the syntactic embeddings, the performance is only slightly affected. This is because sentenceBERT can also encode some syntactic information. Therefore, even without syntactic embeddings, the model can still perform well. 


% \stitle{Contribution of Model Architecture.} To understand the effectiveness of our model architecture, we evaluate our method with different model architectures. 

% % Figure environment removed


%\subsection{Sensitivity analysis}

\stitle{Sensitivity analysis}
We perform sensitivity analysis to understand the effect of different settings in \at.

\underline{Varying Input Size.} In \at, we feed the top 100 rows and left-most 50 columns from the input table $T$ into the model, which is typically enough to correctly predict the right transformations. To understand its effect on model performance, in Figure ~\ref{fig:vary_input}, we vary the number of rows/columns used here and show the input-only model performance. As we can see, when we increase the number of rows/columns that the model uses, the resulting quality improves until it plateaus at about 30 columns and 50 rows.% input rows and columns. Also, we can that it is more sensitive to change of column size than row size. However, as the the number of rows and columns becomes large enough, the improvement becomes insignificant. 

% \textcolor{red}{add latency}


\underline{Varying Number of Filters.} Figure~\ref{fig:vary_filter} shows the quality of \at input-only model with different numbers of convolution filters (the total number of 1x1 and 1x2 filters for rows/columns before AvgPool in the feature extraction layer in Figure~\ref{fig:input_model_arch}). As we can see, using 32 filters is substantially better than 4 filters, as it can extract more features. However, the improvement beyond 32 filters is not significant, suggesting diminishing returns beyond a certain level of model capacity.

% using 32 filters is substantially better than 4 filters, suggesting more than a few latent ``patterns'' in table data. The quality of our results then plateaus, as using 64 filters does not improve the results.

% removed for revision, add back
%\revised{}
%\begin{comment}
\iftoggle{fullversion}
{
    \underline{Varying Embedding Methods. } We initially choose the powerful (but expensive) sentenceBERT~\cite{reimers-2019-sentence-bert} as our semantic embedding, which is known to excel in NLP tasks. We explore how alternative embeddings, such as GloVe~\cite{pennington2014glove}, and fastText~\cite{bojanowski2017enriching}, would perform in our task. %, considering the fact that sentenceBERT is the most expensive part in \at in terms of latency (Figure~\ref{fig:running_time}).
    We show the performance of input-only model with different embeddings in Table~\ref{tab:vary_embed}. As we can see, \at is interestingly not sensitive to the exact choice of semantic embedding --  using sentenceBERT/GloVe/fastText achieves similar quality, suggesting that \at  can operate at a much lower latency than was shown in Figure~\ref{fig:running_time}, without loss of quality.
    %\end{comment}
}
{

}

% removed for revision, add back
%\revised{}
%\begin{comment}
%\subsection{Error Analysis.} 

\iftoggle{fullversion}
{
    \stitle{Error Analysis.} 
    We analyze mistakes that the \at model makes on 218 tables that need a single-step transformation. We show the errors in both predicting operator-type and parameters.
    
    Table~\ref{tab:confusion_matrix} shows a detailed confusion matrix for single-step top-1 operator-type predictions. We can see that the most common mistakes are between ``\code{transpose}'' and ``\code{stack}'' (9), as well as ``\code{wide-to-long}'' and ``\code{stack}'' (6). Both are not unexpected, as their corresponding input tables share similar characteristics (e.g., the input in Figure~\ref{fig:multi-step-ex} may appear to look like a candidate for ``\code{transpose}'' as well as ``\code{stack}'', due to its homogeneous column groups).
    
    %of different methods, where we only run each method with one step and only evaluate it on tables that need one-step transformation. Note that this task is easier than end-to-end task, which can involve multiple operators and must stop by predicting None correctly when the input table is already relationalized. As we can see, \at performs better than all other methods by a large margin. 
    
    Table~\ref{tab:parameters} shows the accuracy of our parameter predictions for different operators at the top-1 position. % (these parameters are described in Table~\ref{tab:label_vector_encode}). 
    Despite the large space of possible parameters, our predictions are surprisingly accurate, showing the effectiveness of our CNN-inspired model in extracting patterns from tabular data.
    
    
    % \begin{table}[!h]
    % \caption{Confusion matrix for single-step top-1 predictions.}
    % \vspace{-4mm}
    % \scalebox{0.7}{
    % \begin{tabular}{|c|*{8}{c|}}\hline
    % \backslashbox{\textbf{True}}{\textbf{Pred}}
    % &\makebox[2em]{\textbf{trans.}}&\makebox[2em]{\textbf{stack}}
    % &\makebox[2em]{\textbf{wtl}}&\makebox[3em]{\textbf{explode}}&\makebox[2em]{\textbf{ffill}}&\makebox[2em]{\textbf{pivot}}&\makebox[3em]{\textbf{subtitle}}&\makebox[2em]{\textbf{none}}\\\hline
    % \textbf{trans.} & 12 & 9 & 0 & 1 & 0 & 0 & 0 & 0 \\ \hline
    % \textbf{stack} & 2 & 24 & 1 & 0 & 0 & 0 & 0 & 8 \\ \hline
    % \textbf{wtl} & 0 & 6 & 21 & 0 & 0 & 0 & 0 & 4 \\ \hline 
    % \textbf{explode} & 0 & 0 & 0 & 19 & 0 & 0 & 0 & 14 \\ \hline
    % \textbf{ffill} & 0 & 1 & 0 & 1 & 8 & 0 & 0 & 3 \\ \hline
    % \textbf{pivot} & 0 & 0 & 0 & 0 & 0 & 8 & 0 & 0 \\ \hline
    % \textbf{subtitle} & 0 & 1 & 0 & 0 & 0 & 0 & 24 & 1 \\ \hline
    % \end{tabular}
    % }
    % \label{tab:confusion_matrix}
    % \end{table}
    
    \begin{table}[!h]
    \caption{Confusion matrix for single-step top-1 predictions.}
    %\vspace{-4mm}
    \scalebox{0.7}{
    \begin{tabular}{|c|*{8}{c|}}\hline
    \backslashbox{\textbf{True}}{\textbf{Pred}}
    &\makebox[2em]{\textbf{trans.}}&\makebox[2em]{\textbf{stack}}
    &\makebox[2em]{\textbf{wtl}}&\makebox[3em]{\textbf{explode}}&\makebox[2em]{\textbf{ffill}}&\makebox[2em]{\textbf{pivot}}&\makebox[3em]{\textbf{subtitle}}&\makebox[2em]{\textbf{none}}\\\hline
    \textbf{trans.} & 14 & 10 & 1 & 1 & 0 & 0 & 0 & 2 \\ \hline
    \textbf{stack} & 2 & 36 & 3 & 1 & 0 & 0 & 0 & 14 \\ \hline
    \textbf{wtl} & 0 & 6 & 23 & 1 & 0 & 0 & 0 & 4 \\ \hline 
    \textbf{explode} & 0 & 0 & 0 & 32 & 0 & 0 & 0 & 16 \\ \hline
    \textbf{ffill} & 0 & 1 & 0 & 1 & 10 & 0 & 0 & 6 \\ \hline
    \textbf{pivot} & 0 & 0 & 0 & 0 & 0 & 8 & 0 & 0 \\ \hline
    \textbf{subtitle} & 0 & 1 & 0 & 0 & 0 & 0 & 24 & 1 \\ \hline
    \end{tabular}
    }
    \label{tab:confusion_matrix}
    \end{table}
    
    
    % \begin{table}[!h]
    % \caption{Accuracy of  operator parameter predictions}
    % \vspace{-4mm}
    % \scalebox{0.68}{
    % \begin{tabular}{cccccccc}
    % \toprule
    % %\textbf{Parameters} & \textbf{ssi} & \textbf{sei} & \textbf{wsi} & \textbf{wei} & \textbf{eci} & \textbf{fei} & \textbf{prf} \\
    % \textbf{operator} & \textbf{stack} & \textbf{stack} & \textbf{wtl} & \textbf{wtl} & \textbf{explode} & \textbf{ffill} & \textbf{pivot} \\
    % \textbf{parameter} & \textbf{start-idx} & \textbf{end-idx} & \textbf{start-idx} & \textbf{end-idx} & \textbf{col-idx} & \textbf{col-idx} & \textbf{row-freq} \\
    % \midrule
    % Accuracy & 0.958 & 1 & 0.952 & 1 & 0.947 & 1 & 0.875 \\
    % \bottomrule
    % \end{tabular}
    % }
    % \label{tab:parameters}
    % \end{table}
    
    \begin{table}[!h]
    \caption{Accuracy of  operator parameter predictions}
    %\vspace{-4mm}
    \scalebox{0.68}{
    \begin{tabular}{cccccccc}
    \toprule
    %\textbf{Parameters} & \textbf{ssi} & \textbf{sei} & \textbf{wsi} & \textbf{wei} & \textbf{eci} & \textbf{fei} & \textbf{prf} \\
    \textbf{operator} & \textbf{stack} & \textbf{stack} & \textbf{wtl} & \textbf{wtl} & \textbf{explode} & \textbf{ffill} & \textbf{pivot} \\
    \textbf{parameter} & \textbf{start-idx} & \textbf{end-idx} & \textbf{start-idx} & \textbf{end-idx} & \textbf{col-idx} & \textbf{col-idx} & \textbf{row-freq} \\
    \midrule
    Accuracy & 0.889 & 1 & 0.957 & 1 & 0.969 & 1 & 0.875 \\
    \bottomrule
    \end{tabular}
    }
    \label{tab:parameters}
    \end{table}
    %\end{comment}
}
{
\underline{Additional results.} We report  additional results such as sensitivity to different embeddings, error analysis, and accuracy of parameter predictions, in~\cite{full} in the interest of space.
}




%\stitle{\yeye{Varying number of layers?}}
% \yeye{after reading the experiments, I have a feeling that some reviewers may think we don't have enough experiments yet (we have 1 table and 2 figures right now). Maybe we can add some  sensitivity analysis? E.g., (1) sensitivity to trainng corpus (pbi, parquet, pbi+parquet?), (2) sensitivity to hyperparameters, e.g., number of filters, etc. (3) others?}