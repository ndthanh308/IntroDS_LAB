% \documentclass[10pt]{article}
% %\usepackage[top=2.5cm,bottom=2.5cm,left=2.5cm,right=2.5cm]{geometry}
% \usepackage[utf8]{inputenc}
% \usepackage{titlesec}
% \usepackage{palatino}
% \usepackage{soul}
% \usepackage{graphicx}
% \usepackage{wrapfig}
% \usepackage{xcolor}
% \usepackage{xspace}
% \usepackage{comment}
% \usepackage{xr-hyper}
% \usepackage{enumitem}
% \usepackage[hidelinks]{hyperref}

% \usepackage[pdftex,
%             pdfauthor={Cong},
%             pdftitle={Cover Letter},
%             pdfproducer={LaTeX},
%             pdfcreator={pdflatex}]
%             %{hyperref}
% %\usepackage[skip=0pt plus1pt, indent=40pt]{parskip}


% \setlength{\parskip}{8pt}
% \setlength{\parindent}{0pt}
% \titleformat*{\section}{\sffamily\large\bfseries}
% \titlespacing\section{0pt}{4pt}{0pt}
% \titleformat*{\subsection}{\sffamily\bfseries}
% \titlespacing\subsection{0pt}{4pt}{0pt}
% \titleformat*{\paragraph}{\bfseries}
% \titlespacing\paragraph{0pt}{4pt}{1em}


% \newcommand{\abi}{\textsc{Auto-BI}\xspace}
% \newcommand{\code}[1]{\tt\footnotesize{#1}}

% \newcommand{\revised}[1]{{\color{blue}#1}}

% \newcommand{\question}[1]{{\color{red}#1}}

% \makeatletter
% \newcommand*{\addFileDependency}[1]{% argument=file name and extension
%   \typeout{(#1)}
%   \@addtofilelist{#1}
%   \IfFileExists{#1}{}{\typeout{No file #1.}}
% }
% \makeatother

% \newcommand*{\myexternaldocument}[1]{%
%     \externaldocument{#1}%
%     \addFileDependency{#1.tex}%
%     \addFileDependency{#1.aux}%
% }
% \myexternaldocument{main}

% %\title{Cover Letter} 
% %\date{}

% \begin{document}
% %\newcommand{\tool}{\textsc{MagicPush}\xspace}

% %{\sffamily\centering\bfseries\LARGE Cover Letter \par}
% %{\centering\large Donghao Ren \par}
% \vspace{16pt}
% % {\centering\large \url{donghaoren@cs.ucsb.edu} \par}


% We thank the reviewers for their helpful feedback. We have carefully revised our paper as suggested. In our cover letter below, we will respond to each reviewer's comments, with pointers to corresponding changes we made in our revised paper (marked in \revised{blue}). 




% \textbf{Reviewer \#1}

% \textbf{\textgreater R1.W1} Thank you for your comment. It is a good point that for some transformations such as Stack, it may  not be immediately obvious why heuristic rules would often fail, which is something that we should have highlighted more in the paper.

% To better illustrate this, we added a baseline using the heuristic suggested in the review, where we predict Stack based on column data types -- if there is a continuous range of ($\ge 3$) columns in an input table that are of the same type, e.g., all float, we predict such a table to require Stack. For the seemingly simple Stack operator, this heuristic turns out to be ineffective with many false positives (because many input tables that do not require Stack may still have a continuous range of columns that share the same type, and even for cases that require Stack, using this heuristic would often lead to incorrect start/end column index due to the presence of miscellaneous columns that should not be part of the Stack). On our benchmark, we found this heuristic predicts Stack correctly on only 38\% of the cases (accuracy = 0.38, compared to the accuracy of 0.807 from our learning-based Auto-Tables). Furthermore, for other more complex operators such as Wide-to-long or Pivot, it is even more difficult to write heuristics, and the accuracy of using heuristic rules would only be lower (because we would move from a binary classification problem of Stack-vs-None, to a multi-class classification problem with many more choices). We added this discussion in \revised{Section 1} of our revised paper. 

% To further demonstrate the challenging nature of our prediction tasks, we added a new baseline that is powered by GPT-3.5, using few-shot prompting -- specifically, for each transformation operator in our DSL, we give example input tables requiring transformation, and their corresponding ground-truth transformations in the prompt, and then ask the model to predict required transformation-operators on new test cases. As we can see in \revised{Table~\ref{tab:hit_at_k_pos}} of \revised{Section~\ref{sec:exp}}, even if we greatly simplify the task by only asking the model to predict transformation operator types (without requiring the parameters to be correctly predicted), the resulting accuracy is still only around 0.27, 
% despite the promise and the high cost associated with large language models like GPT, again demonstrating the challenging nature of the AutoTable tasks that we aim to address.

% We believe these additional analysis  can serve as good motivations for why a learning-based algorithm like Auto-Tables is needed for our tasks. We added this discussion in \revised{Section 1} of the revised paper, and we thank the reviewer for the great suggestion.
% \\


% \textbf{Reviewer \#4}

% \textbf{\textgreater Order of transformations.}  This is an excellent question -- the order of transformations indeed matters (for example, a sequence with Transpose followed by Stack, is not equivalent to another sequence with Stack followed by Transpose). Although a given sequence of transformations cannot be simply reordered to create semantically equivalent transformations, for some test cases there can exist more than one sequence of transformations to reach a desired outcome, precisely because there are algebraic rules under which two transformation sequences are semantically equivalent, like the reviewer suggested (for example, Pivot is equivalent to Transpose followed by Wide-to-long). 
% In \revised{Section~\ref{sec:preliminary}} of the revised paper, we added more details of these aspects, which we agree would be interesting to readers.

% We should highlight that in our evaluation,  for each test case in our benchmark, we enumerate all possible transformations that can reach the desired relational table as ground-truth, so for some cases there are multiple ground-truth transformation sequences (e.g., can be seen from \url{https://github.com/LiPengCS/Auto-Tables-Benchmark}). If an algorithm correctly predicts any one of the ground-truth sequences, we would mark the algorithm as correct for the test case, which ensures the correctness of the our evaluation in the presence of  multiple semantically equivalent transformation sequences.

% \begin{comment}
% (e.g.,  We observe algebraic rules between certain operators, which make them order independent (analogous to commutativity in binary operators). We added this discussion in experiments. 

% - pivot equivalent to transpose-then-wtl

% - wtl equivalent to stack-split-pivot

% - ffill and stack/wtl are order-independent, if they operate on disjoint subsets of columns

% - explode and wtl/stack/ffill are order-independent, if they operate on disjoint subsets of columns

% (These algebraic rules are not exhaustive, so we eval by results).
% \end{comment}

% \textbf{\textgreater Learning-based method.} We added a discussion on why learning-based methods are necessary, and why rule-based heuristics would fail, in \revised{Section 1} of the revised paper.

% To  illustrate why rule-based methods would often fail, we added a experiment using a heuristic rule for the seemingly simple operator  Stack, by predicting columns to stack based on column data types  --  if there is a continuous range of ($\ge 3$) columns in an input table that are of the same type, e.g., all float, we predict such a table to require Stack  (this was suggested by Reviewer-1 from his comment above). For the simple Stack operator, the heuristic turns out to be ineffective with many false positives (because many input tables that do not require Stack, may still have a continuous range of columns that happen to be of the same type). On our benchmark, we found this heuristic predicts  Stack correctly on only 38\% of the cases (accuracy = 0.38, compared to the accuracy of 0.807 from our learning-based Auto-Tables). For other more complex operators such as Wide-to-long or Pivot, it is even more difficult to write heuristics, and the accuracy of using heuristic rules would be only lower (as we move from a binary classification problem of Stack-vs-None, to a multi-class classification problem with many more operators). We added this discussion in our revised paper.  

% To further demonstrate the challenging nature of our prediction tasks, we added a new baseline that is powered by GPT-3.5, using few-shot prompting -- specifically, we give example input tables requiring transformation, and their corresponding ground-truth transformations in the prompt, and then ask the model to predict required transformation-operators on new test cases. As we can see in \revised{Table~\ref{tab:hit_at_k_pos}} of \revised{Section~\ref{sec:exp}}, even if we greatly simplify the task by only asking the model to predict transformation operator types (without requiring the parameters to be correctly predicted), the resulting accuracy is still only around 0.27, 
% despite the promise and the high cost associated with large language models like GPT, again demonstrating the challenging nature of the AutoTable tasks that we aim to address.

% We added these discussions  in \revised{Section 1} of our revised paper, and we thank the reviewer for the suggestion.

% \textbf{\textgreater Operator distributions.} We added details of the operator distributions in \revised{Table~\ref{tab:autotable_benchmark}} of \revised{Section~\ref{sec:exp}} of our revised paper.
% \\
% \\
% \\
% \\
% \\

% \textbf{Reviewer \#5}

% \textbf{\textgreater Input of variable size.} This is a good question and something that we should have highlighted more in Section~\ref{subsec:input-model} of our paper. 
% Just like deep-learning based computer vision algorithms that can also only accept input pictures of a fixed-size window, we take the first 100 data-rows (plus a header) and the first 50 columns at the top-left corner of each input $T$ (producing a $101 \times 50 \times 423$ tensor), which results in a fixed window size. In cases where we have less than 100 rows or 50 columns, we use the standard practice of ``padding'' the empty cells using vectors of ``0'', which our models can easily learn and ignore such cells. In cases when an input table has more than 100 rows and 50 columns, in most cases our 100x50 window is still sufficient to reveal row/column-wise patterns to predict the correct transformations (because the repeating row/column patterns tend to repeat after 5-10 rows/columns, making the 100x50 window sufficient, as the examples in Figure~\ref{fig:combined-ex} below would all show). 

% We are highlighting these details more, in \revised{Section~\ref{subsec:input-model}} of our revised paper, and we thank the reviewer for the suggestion.

% %We also have a detailed sensitivity analysis in \revised{Figure~\ref{fig:vary_input}} of \revised{Section~\ref{sec:exp}}, which 

% \textbf{\textgreater Challenges associated with web-tables.} 
% Thank you for the comment, there are indeed challenges associated with web tables, many of which are specific in the web table domain. We will give a few common challenges below to ground the discussion. It is important to highlight that from our experience, many of the existing challenges are already solved by mature production-quality systems that extract and normalize web-tables (e.g., in search engines that surface web-tables as answers to user queries), as we will show using a pair of detailed examples in Figure~\ref{fig:web-table-issues} and Figure~\ref{fig:web-table-issues-solved} below. 

% When extracting and normalizing web-tables, here are some of the example challenges that we commonly encounter: %, as illustrated using a real web table shown in Figure~\ref{fig:web-table-issues}:

% \begin{itemize}[noitemsep,nolistsep]
%     \item (1) \underline{Data values spanning multiple rows and columns}. The red box in Figure~\ref{fig:web-table-issues} shows such an issue. Visually it is easy to see that ``Phoenix'' and ``Little Rock'' span two columns. These values should be copied twice  during programatic extraction, to populate as cell-values in both columns when this web-table is extracted and normalized as a relational table. 
%     \item (2) \underline{Hierarchical metadata}. The blue box in Figure~\ref{fig:web-table-issues} shows an issue in this category, where visually we can see that there is a hierarchical column-header with ``Cities'' being at the top-level, and ``Captial''/``Largest'' being at the second level, which need to be parsed correctly and stored as column-headers: ``Cities (Capital)'' and ``Cities (Largest)''. Similarly, the yellow-box here shows another non-standard metadata issue, with the column-header spanning two columns. The challenges in programmatic extraction here include correctly understanding the header-structure, and not parsing the second-level meta-data (``Capital'' and ``Largest'') as data rows, etc.
%     \item (3) \underline{Missing column separators}. There are also large amounts of web tables whose columns boundaries are not clearly delineated in HTML sources. Extracting relational tables from such data is known as the list extraction problem~\cite{webtable-extract-google, webtable-tegra, webtable-extraction}, with mature solutions.
% \end{itemize}

% %\footnote{This table at \url{https://en.wikipedia.org/wiki/William_Holden#Filmography}  shows another example where cells are spanning multiple columns},

% % Figure environment removed


% % Figure environment removed

% % \footnote{This result page can be seen from \url{https://www.bing.com/search?q=List_of_U.S._states_and_territories_by_gdp}.} 

% While challenges like the ones described above are clearly important and non-trivial when extracting and normalizing web-tables into proper relational tables, there are mature solutions that can reliably solve them (e.g., discussed in Section 3 of \cite{webtable-bingdata}, Section 2 of \cite{webtables}, Section 3-5 of~\cite{webtable-extraction}, and~\cite{webtable-tegra, webtable-extract-google}, etc.), with some of these solutions already incorporated into commercial systems such as search engines. 

% To concretely show that such issues are  reliably handled in practice with production-quality web-table extractors, we issue a query (``list of us states and territories''\footnote{\url{https://www.bing.com/search?q=List_of_U.S._states_and_territories_by_gdp}}) in Bing that matches the table in Figure~\ref{fig:web-table-issues}, and show Bing's result in  Figure~\ref{fig:web-table-issues-solved} that surfaces the same web-table as ``table snippets'' in its search result. Note that from Figure~\ref{fig:web-table-issues-solved}, we can see search engines can already programmatically extracte and normalize the web-table in Figure~\ref{fig:web-table-issues} into a relational table, addressing the challenges in table-extraction as highlighted by corresponding boxes in  Figure~\ref{fig:web-table-issues-solved}.


% %\url{https://en.wikipedia.org/wiki/List_of_states_and_territories_of_the_United_States}

% %\url{https://www.bing.com/search?q=List_of_U.S._states_and_territories_by_gdp}


% Conceptually, it is important to note that the type of extraction challenges solved by existing work are highly ``local'' in nature (e.g., local fixes between neighboring rows and columns, for challenge \# 1-3 above), whereas the challenges solved by Auto-Tables are more ``global'' in nature, that require table-level transformations (e.g., transpose and pivot that alters every single cell in a table), which are not studied in prior work and not handled by today's production-quality web-table extractors, thus making Auto-Tables orthogonal to these existing solutions. 

% Also, because we take the processed tables from a mature web table extractor (similar to the one used in the Bing search engine), which already addresses these ``local'' challenges, we  do not consider these aforementioned challenges in our Auto-Table system as we want to leverage existing solutions and not reinventing the wheels.

% In order to better test the effectiveness of Auto-Tables specifically on web-tables, during the revision phase we collected and annotated 50  web-tables as additional test cases, and added these into our benchmark for testing. We re-ran all of our experiments using the new data, and found our results on web-tables to be very similar to what we reported in our original paper (within $5\%$ in most cases). These updated results can be seen from \revised{Table~\ref{tab:hit_at_k_pos} - Table~\ref{tab:quality-breakdown}} of our revised paper, with \revised{Table~\ref{tab:quality-breakdown}} specifically focusing on a breakdown of the quality results based on different data sources (web-tables vs. excel tables vs. notebook tables), which shows that we achieve comparable and slightly better quality on web-tables, when compared to other types of tables.

% Regarding the total number of test cases in our benchmark -- currently we have 244 test cases (after adding the 50 new web-table test cases), which were painstakingly collected and annotated, from real forum questions, user spreadsheets, and web pages, that represent a substantial amount of human efforts (which we open-sourced~\footnote{\url{https://github.com/LiPengCS/Auto-Tables-Benchmark}} to facilitate future research). While 244 may not seem like a large number, it already offers strong  statistical power -- our comparisons with baselines, such as Table~\ref{tab:hit_at_k_pos} and Table~\ref{tab:quality-breakdown}, are all statistical significant at p=0.01 levels (using standard paired t-tests), showing that the current benchmark already allows us to draw reliable conclusions in a statistical sense. 
% As additional reference points, we want to mention that prior work in this area that we cite and compare in the paper, such as Foofah~\cite{jin2017foofah}  from SIGMOD 2017, and Flash-Relate~\cite{barowy2015flashrelate} from PLDI 2015, uses 50 and 43 benchmark cases, respectively, again because of the efforts it requires to collect and annotate test cases, which nevertheless enable good research in Foofah and Flash-Relate, respectively. We hope our effort in compiling this benchmark could similarly inspire others in the community to build on our effort and create novel new research directions. We added acknowledgement of these in our revised paper.


% %However none of the existing web-table understanding approaches retain the current visual structures on web pages, without trying to restructure tables for relational queries, and are thus orthogonal to us. 


% \textbf{\textgreater Figures.} We resized our screenshots to make them more clear, and we thank the reviewer for the suggestion.



% \bibliographystyle{abbrv}
% \bibliography{AutoTables}


% \end{document}