% Generated by IEEEtranN.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtranN.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem[Dayan and Hinton(1992)]{dayan1992feudal}
P.~Dayan and G.~E. Hinton, ``Feudal reinforcement learning,'' \emph{Advances in
  {N}eural {I}nformation {P}rocessing {S}ystems}, vol.~5, 1992.

\bibitem[Sutton et~al.(1999)Sutton, Precup, and Singh]{sutton1999between}
R.~S. Sutton, D.~Precup, and S.~Singh, ``Between mdps and semi-mdps: A
  framework for temporal abstraction in reinforcement learning,''
  \emph{Artificial intelligence}, vol. 112, no. 1-2, pp. 181--211, 1999.

\bibitem[Barto and Mahadevan(2003)]{barto2003recent}
A.~G. Barto and S.~Mahadevan, ``Recent advances in hierarchical reinforcement
  learning,'' \emph{Discrete event dynamic systems}, vol.~13, no.~1, pp.
  41--77, 2003.

\bibitem[Kulkarni et~al.(2016)Kulkarni, Narasimhan, Saeedi,
  et~al.]{kulkarni2016hierarchical}
T.~D. Kulkarni, K.~Narasimhan, A.~Saeedi \emph{et~al.}, ``Hierarchical deep
  reinforcement learning: Integrating temporal abstraction and intrinsic
  motivation,'' \emph{Advances in neural information processing systems},
  vol.~29, 2016.

\bibitem[Nachum et~al.(2018)Nachum, Gu, Lee, et~al.]{nachum2018data}
O.~Nachum, S.~S. Gu, H.~Lee \emph{et~al.}, ``Data-efficient hierarchical
  reinforcement learning,'' \emph{Advances in {N}eural {I}nformation
  {P}rocessing {S}ystems}, vol.~31, 2018.

\bibitem[Vezhnevets et~al.(2017)Vezhnevets, Osindero, Schaul,
  et~al.]{vezhnevets2017feudal}
A.~S. Vezhnevets, S.~Osindero, T.~Schaul \emph{et~al.}, ``Feudal networks for
  hierarchical reinforcement learning,'' in \emph{International {C}onference on
  {M}achine {L}earning}, 2017, pp. 3540--3549.

\bibitem[Dilokthanakul et~al.(2019)Dilokthanakul, Kaplanis, Pawlowski,
  et~al.]{dilokthanakul2019feature}
N.~Dilokthanakul, C.~Kaplanis, N.~Pawlowski \emph{et~al.}, ``Feature control as
  intrinsic motivation for hierarchical reinforcement learning,'' \emph{IEEE
  {T}ransactions on {N}eural {N}etworks and {L}earning {S}ystems}, vol.~30,
  no.~11, pp. 3409--3418, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Guo, Tan, et~al.]{zhang2020generating}
T.~Zhang, S.~Guo, T.~Tan \emph{et~al.}, ``Generating adjacency-constrained
  subgoals in hierarchical reinforcement learning,'' \emph{Advances in Neural
  Information Processing Systems}, vol.~33, pp. 21\,579--21\,590, 2020.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Zheng, Wang, et~al.]{li2020learning}
S.~Li, L.~Zheng, J.~Wang \emph{et~al.}, ``Learning subgoal representations with
  slow dynamics,'' in \emph{International Conference on Learning
  Representations}, 2020.

\bibitem[Li et~al.(2021)Li, Zhang, Wang, et~al.]{li2021active}
S.~Li, J.~Zhang, J.~Wang \emph{et~al.}, ``Active hierarchical exploration with
  stable subgoal representation learning,'' in \emph{International Conference
  on Learning Representations}, 2021.

\bibitem[Eysenbach et~al.(2019)Eysenbach, Salakhutdinov, and
  Levine]{eysenbach2019search}
B.~Eysenbach, R.~R. Salakhutdinov, and S.~Levine, ``Search on the replay
  buffer: Bridging planning and reinforcement learning,'' \emph{Advances in
  Neural Information Processing Systems}, vol.~32, 2019.

\bibitem[Emmons et~al.(2020)Emmons, Jain, Laskin, et~al.]{emmons2020sparse}
S.~Emmons, A.~Jain, M.~Laskin \emph{et~al.}, ``Sparse graphical memory for
  robust planning,'' \emph{Advances in Neural Information Processing Systems},
  vol.~33, pp. 5251--5262, 2020.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
E.~Todorov, T.~Erez, and Y.~Tassa, ``Mujoco: A physics engine for model-based
  control,'' in \emph{IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, 2012, pp. 5026--5033.

\bibitem[Nachum et~al.(2019)Nachum, Tang, Lu, et~al.]{nachum2019does}
O.~Nachum, H.~Tang, X.~Lu \emph{et~al.}, ``Why does hierarchy (sometimes) work
  so well in reinforcement learning?'' \emph{arXiv:1909.10618}, 2019.

\bibitem[Levy et~al.(2019)Levy, Konidaris, Platt, et~al.]{levy2019learning}
A.~Levy, G.~Konidaris, R.~Platt \emph{et~al.}, ``Learning multi-level
  hierarchies with hindsight,'' in \emph{International Conference on Learning
  Representations}, 2019.

\bibitem[P{\'e}r{\'e} et~al.(2018)P{\'e}r{\'e}, Forestier, Sigaud,
  et~al.]{pere2018unsupervised}
A.~P{\'e}r{\'e}, S.~Forestier, O.~Sigaud \emph{et~al.}, ``Unsupervised learning
  of goal spaces for intrinsically motivated goal exploration,'' \emph{arXiv
  preprint arXiv:1803.00781}, 2018.

\bibitem[Nair and Finn(2019)]{nair2019hierarchical}
S.~Nair and C.~Finn, ``Hierarchical foresight: Self-supervised learning of
  long-horizon tasks via visual subgoal generation,'' \emph{arXiv preprint
  arXiv:1909.05829}, 2019.

\bibitem[Ghosh et~al.(2018)Ghosh, Gupta, and Levine]{ghosh2018learning}
D.~Ghosh, A.~Gupta, and S.~Levine, ``Learning actionable representations with
  goal-conditioned policies,'' \emph{arXiv preprint arXiv:1811.07819}, 2018.

\bibitem[Wiskott and Sejnowski(2002)]{wiskott2002slow}
L.~Wiskott and T.~J. Sejnowski, ``Slow feature analysis: Unsupervised learning
  of invariances,'' \emph{Neural computation}, vol.~14, no.~4, pp. 715--770,
  2002.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Yang, and Luo]{li2020focal}
L.~Li, R.~Yang, and D.~Luo, ``Focal: {E}fficient fully-offline
  meta-reinforcement learning via distance metric learning and behavior
  regularization,'' in \emph{International Conference on Learning
  Representations}, 2020.

\bibitem[Zhang et~al.(2019)Zhang, Wetzel, Dorka, et~al.]{zhang2019scheduled}
J.~Zhang, N.~Wetzel, N.~Dorka \emph{et~al.}, ``Scheduled intrinsic drive: A
  hierarchical take on intrinsically motivated exploration,''
  \emph{arXiv:1903.07400}, 2019.

\bibitem[Machado et~al.(2020)Machado, Bellemare, and Bowling]{machado2020count}
M.~C. Machado, M.~G. Bellemare, and M.~Bowling, ``Count-based exploration with
  the successor representation,'' in \emph{Proceedings of the AAAI Conference
  on Artificial Intelligence}, vol.~34, no.~04, 2020, pp. 5125--5133.

\bibitem[R{\"o}der et~al.(2020)R{\"o}der, Eppe, Nguyen,
  et~al.]{roder2020curious}
F.~R{\"o}der, M.~Eppe, P.~D. Nguyen \emph{et~al.}, ``Curious hierarchical
  actor-critic reinforcement learning,'' in \emph{International Conference on
  Artificial Neural Networks}, 2020, pp. 408--419.

\bibitem[Yamamoto et~al.(2018)Yamamoto, Onishi, and
  Tsuruoka]{yamamoto2018hierarchical}
K.~Yamamoto, T.~Onishi, and Y.~Tsuruoka, ``Hierarchical reinforcement learning
  with abductive planning,'' \emph{arXiv preprint arXiv:1806.10792}, 2018.

\bibitem[Li et~al.(2022)Li, Tang, Tomizuka, et~al.]{li2022hierarchical}
J.~Li, C.~Tang, M.~Tomizuka \emph{et~al.}, ``Hierarchical planning through
  goal-conditioned offline reinforcement learning,'' \emph{arXiv:2205.11790},
  2022.

\bibitem[Shang et~al.(2019)Shang, Trott, Zheng, Xiong, and
  Socher]{shang2019learning}
W.~Shang, A.~Trott, S.~Zheng, C.~Xiong, and R.~Socher, ``Learning world graphs
  to accelerate hierarchical reinforcement learning,'' \emph{arXiv:1907.00664},
  2019.

\bibitem[Jin et~al.(2021)Jin, Zhou, Zhang, He, Yu, and Fakoor]{jin2021graph}
J.~Jin, S.~Zhou, W.~Zhang, T.~He, Y.~Yu, and R.~Fakoor, ``Graph-enhanced
  exploration for goal-oriented reinforcement learning,'' 2021.

\bibitem[Zhang et~al.(2021)Zhang, Yang, and Stadie]{zhang2021world}
L.~Zhang, G.~Yang, and B.~C. Stadie, ``World model as a graph: Learning latent
  landmarks for planning,'' in \emph{International Conference on Machine
  Learning}, 2021, pp. 12\,611--12\,620.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, et~al.]{haarnoja2018soft}
T.~Haarnoja, A.~Zhou, P.~Abbeel \emph{et~al.}, ``Soft actor-critic: Off-policy
  maximum entropy deep reinforcement learning with a stochastic actor,'' in
  \emph{International Conference on Machine Learning}, 2018, pp. 1861--1870.

\bibitem[Schaul et~al.(2015)Schaul, Horgan, Gregor,
  et~al.]{schaul2015universal}
T.~Schaul, D.~Horgan, K.~Gregor \emph{et~al.}, ``Universal value function
  approximators,'' in \emph{International Conference on Machine Learning},
  2015, pp. 1312--1320.

\bibitem[Vassilvitskii and Arthur(2006)]{vassilvitskii2006k}
S.~Vassilvitskii and D.~Arthur, ``k-means++: The advantages of careful
  seeding,'' in \emph{Proceedings of the Eighteenth Annual ACM-SIAM Symposium
  on Discrete Algorithms}, 2006, pp. 1027--1035.

\bibitem[Tang et~al.(2017)Tang, Houthooft, Foote, et~al.]{tang2017exploration}
H.~Tang, R.~Houthooft, D.~Foote \emph{et~al.}, ``\# exploration: A study of
  count-based exploration for deep reinforcement learning,'' \emph{Advances in
  {N}eural {I}nformation {P}rocessing {S}ystems}, vol.~30, 2017.

\bibitem[Charikar(2002)]{charikar2002similarity}
M.~S. Charikar, ``Similarity estimation techniques from rounding algorithms,''
  in \emph{Proceedings of the {T}hiry-fourth {A}nnual ACM {S}ymposium on Theory
  of {C}omputing}, 2002, pp. 380--388.

\bibitem[McQuillan and Walden(1977)]{mcquillan1977arpa}
J.~M. McQuillan and D.~C. Walden, ``The arpa network design decisions,''
  \emph{Computer Networks}, vol.~1, no.~5, pp. 243--289, 1977.

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray,
  et~al.]{andrychowicz2017hindsight}
M.~Andrychowicz, F.~Wolski, A.~Ray \emph{et~al.}, ``Hindsight experience
  replay,'' \emph{Advances in neural information processing systems}, vol.~30,
  2017.

\end{thebibliography}
