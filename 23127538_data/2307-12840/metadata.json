{
  "title": "Efficiently Learning One-Hidden-Layer ReLU Networks via Schur Polynomials",
  "authors": [
    "Ilias Diakonikolas",
    "Daniel M. Kane"
  ],
  "submission_date": "2023-07-24T14:37:22+00:00",
  "revised_dates": [
    "2023-07-26T00:43:37+00:00"
  ],
  "abstract": "We study the problem of PAC learning a linear combination of $k$ ReLU activations under the standard Gaussian distribution on $\\mathbb{R}^d$ with respect to the square loss. Our main result is an efficient algorithm for this learning task with sample and computational complexity $(dk/ε)^{O(k)}$, where $ε>0$ is the target accuracy. Prior work had given an algorithm for this problem with complexity $(dk/ε)^{h(k)}$, where the function $h(k)$ scales super-polynomially in $k$. Interestingly, the complexity of our algorithm is near-optimal within the class of Correlational Statistical Query algorithms. At a high-level, our algorithm uses tensor decomposition to identify a subspace such that all the $O(k)$-order moments are small in the orthogonal directions. Its analysis makes essential use of the theory of Schur polynomials to show that the higher-moment error tensors are small given that the lower-order ones are.",
  "categories": [
    "cs.LG",
    "cs.DS",
    "math.ST",
    "stat.ML"
  ],
  "primary_category": "cs.LG",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.12840",
  "pdf_url": null,
  "comment": null,
  "num_versions": null,
  "size_before_bytes": 175188,
  "size_after_bytes": 177252
}