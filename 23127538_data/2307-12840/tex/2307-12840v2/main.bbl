\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{GKLW19}

\bibitem[BJW19]{BakshiJW19}
A.~Bakshi, R.~Jayaram, and D.~P. Woodruff.
\newblock Learning two layer rectified neural networks in polynomial time.
\newblock In {\em Conference on Learning Theory, {COLT} 2019}, pages 195--268,
  2019.

\bibitem[Bon70]{Bon70}
A.~Bonami.
\newblock Etude des coefficients fourier des fonctiones de $l^{p}(g)$.
\newblock {\em Ann. Inst. Fourier (Grenoble)}, 20(2):335--402, 1970.

\bibitem[CDG{\etalchar{+}}23]{CDGKM23}
S.~Chen, Z.~Dou, S.~Goel, A.~R. Klivans, and R.~Meka.
\newblock Learning narrow one-hidden-layer relu networks.
\newblock {\em CoRR}, abs/2304.10524, 2023.
\newblock Conference version in COLT'23.

\bibitem[CGKM22]{ChenGKM22}
S.~Chen, A.~Gollakota, A.~R. Klivans, and Raghu Meka.
\newblock Hardness of noise-free learning for two-hidden-layer neural networks.
\newblock In {\em NeurIPS}, 2022.

\bibitem[CKM21]{ChenKM21}
S.~Chen, A.~R. Klivans, and R.~Meka.
\newblock Learning deep relu networks is fixed-parameter tractable.
\newblock In {\em 62nd {IEEE} Annual Symposium on Foundations of Computer
  Science, {FOCS} 2021}, pages 696--707. {IEEE}, 2021.

\bibitem[DFS16]{DanielyFS16}
A.~Daniely, R.~Frostig, and Y.~Singer.
\newblock Toward deeper understanding of neural networks: The power of
  initialization and a dual view on expressivity.
\newblock In {\em Advances in Neural Information Processing Systems 29: Annual
  Conference on Neural Information Processing Systems 2016}, pages 2253--2261,
  2016.

\bibitem[DGK{\etalchar{+}}20]{DGKK20}
I.~Diakonikolas, S.~Goel, S.~Karmalkar, A.~R. Klivans, and M.~Soltanolkotabi.
\newblock Approximation schemes for relu regression.
\newblock In {\em Conference on Learning Theory, {COLT} 2020}, volume 125 of
  {\em Proceedings of Machine Learning Research}, pages 1452--1485. {PMLR},
  2020.

\bibitem[DK20]{DK20-ag}
I.~Diakonikolas and D.~M. Kane.
\newblock Small covers for near-zero sets of polynomials and learning latent
  variable models.
\newblock In {\em 61st {IEEE} Annual Symposium on Foundations of Computer
  Science, {FOCS} 2020}, pages 184--195, 2020.
\newblock Full version available at https://arxiv.org/abs/2012.07774.

\bibitem[DKKZ20]{DKKZ20}
I.~Diakonikolas, D.~M. Kane, V.~Kontonis, and N.~Zarifis.
\newblock Algorithms and {SQ} lower bounds for {PAC} learning one-hidden-layer
  relu networks.
\newblock In {\em Conference on Learning Theory, {COLT} 2020}, volume 125 of
  {\em Proceedings of Machine Learning Research}, pages 1514--1539. {PMLR},
  2020.

\bibitem[DKMR22]{DKMR22-neuron}
I.~Diakonikolas, D.~Kane, P.~Manurangsi, and L.~Ren.
\newblock Hardness of learning a single neuron with adversarial label noise.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics, {AISTATS} 2022}, volume 151 of {\em Proceedings of Machine
  Learning Research}, pages 8199--8213. {PMLR}, 2022.

\bibitem[DKPZ21]{DiakonikolasKPZ21}
I.~Diakonikolas, D.~M. Kane, T.~Pittas, and N.~Zarifis.
\newblock The optimality of polynomial regression for agnostic learning under
  gaussian marginals in the {SQ} model.
\newblock In {\em Conference on Learning Theory, {COLT} 2021}, volume 134 of
  {\em Proceedings of Machine Learning Research}, pages 1552--1584. {PMLR},
  2021.

\bibitem[DKR23]{DKR23}
I.~Diakonikolas, D.~M. Kane, and L.~Ren.
\newblock Near-optimal cryptographic hardness of agnostically learning
  halfspaces and relu regression under gaussian marginals.
\newblock {\em CoRR}, abs/2302.06512, 2023.
\newblock Conference version in ICML'23.

\bibitem[DKRS22]{DKRS22}
I.~Diakonikolas, D.~Kane, L.~Ren, and Y.~Sun.
\newblock {SQ} lower bounds for learning single neurons with massart noise.
\newblock In {\em NeurIPS}, 2022.

\bibitem[DKTZ22]{DKTZ22}
I.~Diakonikolas, V.~Kontonis, C.~Tzamos, and N.~Zarifis.
\newblock Learning a single neuron with adversarial label noise via gradient
  descent.
\newblock In {\em Conference on Learning Theory}, volume 178 of {\em
  Proceedings of Machine Learning Research}, pages 4313--4361. {PMLR}, 2022.

\bibitem[DKZ20]{DKZ20}
I.~Diakonikolas, D.~M. Kane, and N.~Zarifis.
\newblock Near-optimal {SQ} lower bounds for agnostically learning halfspaces
  and relus under gaussian marginals.
\newblock In {\em Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020},
  2020.

\bibitem[DPT21]{DPT21}
I.~Diakonikolas, J.~H. Park, and C.~Tzamos.
\newblock Relu regression with massart noise.
\newblock In {\em Advances in Neural Information Processing Systems 34: Annual
  Conference on Neural Information Processing Systems 2021, NeurIPS 2021},
  pages 25891--25903, 2021.

\bibitem[GGJ{\etalchar{+}}20]{GoelGJKK20}
S.~Goel, A.~Gollakota, Z.~Jin, S.~Karmalkar, and A.~R. Klivans.
\newblock Superpolynomial lower bounds for learning one-layer neural networks
  using gradient descent.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning, {ICML} 2020}, volume 119 of {\em Proceedings of Machine Learning
  Research}, pages 3587--3596, 2020.

\bibitem[GK19]{GoelK19}
S.~Goel and A.~R. Klivans.
\newblock Learning neural networks with two nonlinear layers in polynomial
  time.
\newblock In {\em Conference on Learning Theory, {COLT} 2019}, pages
  1470--1499, 2019.

\bibitem[GKKT17]{GoelKKT17}
S.~Goel, V.~Kanade, A.~R. Klivans, and J.~Thaler.
\newblock Reliably learning the relu in polynomial time.
\newblock In {\em Proceedings of the 30th Conference on Learning Theory, {COLT}
  2017}, pages 1004--1042, 2017.

\bibitem[GKLW19]{GeKLW19}
R.~Ge, R.~Kuditipudi, Z.~Li, and X.~Wang.
\newblock Learning two-layer neural networks with symmetric inputs.
\newblock In {\em 7th International Conference on Learning Representations,
  {ICLR} 2019}, 2019.

\bibitem[GLM18]{GeLM18}
R.~Ge, J.~D. Lee, and T.~Ma.
\newblock Learning one-hidden-layer neural networks with landscape design.
\newblock In {\em 6th International Conference on Learning Representations,
  {ICLR} 2018}, 2018.

\bibitem[Gro75]{Gross:75}
L.~Gross.
\newblock Logarithmic {Sobolev} inequalities.
\newblock {\em Amer.\ J.\ Math.}, 97(4):1061--1083, 1975.

\bibitem[JSA15]{Janz15}
M.~Janzamin, H.~Sedghi, and A.~Anandkumar.
\newblock Beating the perils of non-convexity: Guaranteed training of neural
  networks using tensor methods, 2015.

\bibitem[Kea98]{Kearns:98}
M.~J. Kearns.
\newblock Efficient noise-tolerant learning from statistical queries.
\newblock {\em Journal of the ACM}, 45(6):983--1006, 1998.

\bibitem[SJA16]{SedghiJA16}
H.~Sedghi, M.~Janzamin, and A.~Anandkumar.
\newblock Provable tensor methods for learning mixtures of generalized linear
  models.
\newblock In {\em Proceedings of the 19th International Conference on
  Artificial Intelligence and Statistics, {AISTATS} 2016}, pages 1223--1231,
  2016.

\bibitem[VW19]{VempalaW19}
S.~Vempala and J.~Wilmes.
\newblock Gradient descent for one-hidden-layer neural networks: Polynomial
  convergence and {SQ} lower bounds.
\newblock In {\em Conference on Learning Theory, {COLT} 2019}, pages
  3115--3117, 2019.

\bibitem[WZDD23]{WZDD23}
P.~Wang, N.~Zarifis, I.~Diakonikolas, and J.~Diakonikolas.
\newblock Robustly learning a single neuron via sharpness.
\newblock {\em CoRR}, abs/2306.07892, 2023.
\newblock Conference version in ICML'23.

\bibitem[ZLJ16]{ZhangLJ16}
Y.~Zhang, J.~D. Lee, and M.~I. Jordan.
\newblock L1-regularized neural networks are improperly learnable in polynomial
  time.
\newblock In {\em Proceedings of the 33nd International Conference on Machine
  Learning, {ICML} 2016}, pages 993--1001, 2016.

\bibitem[ZSJ{\etalchar{+}}17]{ZhongS0BD17}
K.~Zhong, Z.~Song, P.~Jain, P.~L. Bartlett, and I.~S. Dhillon.
\newblock Recovery guarantees for one-hidden-layer neural networks.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning, {ICML} 2017}, pages 4140--4149, 2017.

\end{thebibliography}
