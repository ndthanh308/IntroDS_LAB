%%%%%%%% ICML 2023 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{float}
\usepackage{multirow}
\usepackage{bm}
\usepackage[figuresright]{rotating}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{bbding}
\usepackage{color}
% \usepackage{xcolor}
\usepackage{verbatim}
\usepackage{hyperref}
\hypersetup{
hidelinks,
colorlinks=true,
linkcolor=red,
citecolor=cyan,
urlcolor = magenta
}
\usepackage{orcidlink}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{float}
\usepackage{multirow}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2023}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{MLIC++: Linear Complexity Multi-Reference Entropy Modeling for Learned Image Compression}

\begin{document}

\twocolumn[
\icmltitle{MLIC$^{++}$: Linear Complexity Multi-Reference Entropy Modeling for Learned Image Compression}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Wei Jiang}{pku}
\icmlauthor{Jiayu Yang}{pku}
\icmlauthor{Yongqi Zhai}{pku,pcl}
\icmlauthor{Feng Gao}{pkuart}
\icmlauthor{Ronggang Wang}{pku,pcl}
\\\tt{wei.jiang1999@outlook.com, jiangwei@stu.pku.edu.cn\\rgwang@pkusz.edu.cn}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{pku}{Shenzhen Graduate School, Peking University}
\icmlaffiliation{pkuart}{School of Arts, Peking University}
\icmlaffiliation{pcl}{Pengcheng Laboratory}

\icmlcorrespondingauthor{Ronggang Wang}{rgwang@pkusz.edu.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\textit{\textcolor{magenta}{Compared with version presented at NCW ICML 2023, in this version, we add the details of our prior work presented at ACMMM 2023~\cite{jiang2022mlic}, new comparisons on complexity, and more ablation studies. If you are familiar with MLIC and MLIC$^{+}$, you can directly refer to arxiv version v3.}}} % otherwise use the standard text.

\begin{abstract}
  Recently, learned image compression has achieved impressive performance.
    The entropy model, which estimates the distribution of the latent representation,
    plays a crucial role in enhancing rate-distortion performance.
    The latent representation contains channel-wise, local spatial, and global spatial correlations.
    However, existing global context modules rely on
    computationally intensive quadratic complexity computations
    to capture global correlations.
    The quadratic complexity imposes limitations on the potential of high-resolution image coding.
    Moreover, effectively capturing local, global, and 
    channel-wise contexts with acceptable even linear 
    complexity within a single entropy model remains a challenge.
    To address these limitations, we propose the Linear Complexity Multi-Reference Entropy Model (MEM$^{++}$).
    MEM$^{++}$ effectively captures the diverse range of correlations inherent in the latent representation. Specifically,
    the latent representation is first divided into
    multiple slices. When compressing a particular slice, the
    previously compressed slices serve as its channel-wise contexts.
    % To capture local contexts, we introduce checkerboard attention module that avoids performance degradation.
    To capture local contexts without sacrificing performance,
    we introduce a novel checkerboard attention module with linear complexity.
    Additionally, to capture global contexts, we propose the linear complexity attention-based global correlations
    capturing by leveraging the decomposition of the softmax operation.
    The attention map of the previously decoded slice is implicitly computed and employed to predict global correlations in the current slice.
    Based on MEM$^{++}$, we propose image compression model MLIC$^{++}$.
    Extensive experimental evaluations demonstrate that our MLIC$^{++}$ achieves state-of-the-art performance,
    reducing BD-rate by $13.39\%$ on the Kodak dataset compared to VTM-17.0 in Peak Signal-to-Noise Ratio (PSNR).
    Furthermore, MLIC$^{++}$ exhibits linear GPU memory consumption with resolution, 
    making it highly suitable for high-resolution image coding.
    Code and pre-trained models are available at \tt\url{https://github.com/JiangWeibeta/MLIC}.
  \end{abstract}
  
  \section{Introduction}
  \label{sec:intro}
  Due to the rise of social media, tens of millions of images
are generated and transmitted on the web every second.
% Figure environment removed
In order to conserve bandwidth, service providers are compelled to seek more efficient and effective image compression methods.
Although traditional coding methods like JPEG~\cite{pennebaker1992jpeg},
JPEG2000~\cite{maryline1999jpeg2000}, AVC~\cite{wiegand2003avc}, HEVC~\cite{sullivan2012overview}, and VVC~\cite{bross2021vvc}
have achieved commendable performance, their design relies on manual design for each module.
This lack of joint optimization hampers their ability to fully exploit the potential for further advancements in image compression.\par
Recently, various learned image compression models
~\cite{wu2021learned,guo2021causal,balle2016end,theis2017lossy,balle2018variational,minnen2018joint,hu2020coarse,ma2020iwave}
have emerged, showcasing impressive performance gains.
Notably, certain learned image compression models~\cite{minnen2020channel,cheng2020learned,zou2022the,qian2020learning,xie2021enhanced,he2022elic,jiang2022mlic,chen2021nic,gao2021neural,chen2022two,koyuncu2022contextformer,duan2023lossy,liu2023learned,jiang2023slic,fu2023asymmetric,tang2023joint,chen2023toward} are already comparable to
the advanced traditional method VVC.
These models predominantly rely on auto-encoders or variational auto-encoders~\cite{kingma2014vae},
and follow a process that involves transform, quantization, entropy coding, and inverse transform.
Entropy coding plays an important role
in boosting model performance. An entropy model is utilized to estimate
the entropy of the latent representation.
A powerful and accurate entropy model usually leads to fewer bits.
Expanding contexts of entropy model in learned codecs plays the same role as expanding prediction modes in traditional codecs.\par
State-of-the-art learned image compression models~\cite{cheng2020learned,minnen2020channel,xie2021enhanced,zhu2022transformerbased,he2022elic,jiang2022mlic,jiang2023slic}
commonly enhance the entropy model by incorporating a hyper-prior module~\cite{balle2018variational} or a context module~\cite{minnen2018joint}.
These additional modules enable the estimation of conditional entropy and the utilization of conditional probabilities for entropy coding.
Context modules usually model probabilities and correlations in different dimensions,
including local spatial context module,
global spatial context module, and channel-wise context module.
However, the current global context modules rely on computationally intensive \textit{quadratic} complexity computations,
which consume \textit{huge} GPU memories and have slower encoding and decoding speed,
imposing limitations on the potential for high-resolution image coding.
Furthermore,
effectively capturing local, global, and channel-wise contexts with \textit{acceptable} even \textit{linear} complexity
within a single entropy model remains a challenge.
To overcome the aforementioned limitations,
we propose a novel \textit{linear complexity}  multi-reference entropy model.
This entropy model effectively captures local spatial, global spatial, and channel-wise contexts with \textit{linear} complexity
and can be employed for efficient high-resolution image coding, which is denoted as MEM$^{++}$
to differ from our prior work~\cite{jiang2022mlic} presented at ACMMM 2023.
Based on MEM$^{++}$, we introduce MLIC$^{++}$,
which achieve state-of-the-art performance.\par
In our approach, the latent representations is divided into multiple slices along the channel dimension.
% However, we find there are correlations in all dimensions.
% Modeling correlations in a single dimension leads to
% inaccurate conditional probabilities,
% which inspires us to design multi-reference entropy models MEM and MEM$^+$.
% In our MEM and MEM$^+$, we capture local spatial, global spatial, and channel-wise contexts.
% Based on MEM and MEM$^+$, we propose MLIC and MLIC$^+$, which achieve state-of-art performance.
% To capture channel-wise correlations,
% we divide the latent representations into several slices.
When compressing a particular slice, the previously compressed slices serve as its channel-wise contexts,
which are extracted by a channel-wise context module.
Local and global context modeling are conducted separately for each slice.
The utilization of an auto-regressive local context module~\cite{minnen2018joint,van2016conditional} leads to serial decoding,
while a checkerboard context module~\cite{he2021checkerboard} facilitates
two-pass parallel decoding by dividing the latent representations into anchor and non-anchor parts.
However, it is worth noting that the checkerboard context module may result in a performance degradation of up to $4\%$~\cite{qian2022entroformer}.
To address this issue, we propose a novel overlapped checkerboard window attention with
\textit{linear} complexity, which
further enhances the local context capturing while retaining two-pass decoding.
Some previous methods focus on global context modeling~\cite{qian2020learning,guo2021causal},
which typically involve \textit{quadratic} complexity or the utilization of additional bits to store global similarity as side information.
Additionally, these global context modules often collaborate with serial local spatial context modules, further increasing the computational complexity.
Assuming comparable spatial correlations across different slices,
we initially calculate the attention map of the previously decoded $i-1$-th slice in a \textit{vanilla} approach.
This attention map is utilized to predict the global correlations within the $i$-th slice.
However, in the \textit{vanilla} attention mechanism, the softmax operation dictates the order of computation among tensors,
where the attention map, product of queries and keys are required to be computed first.
To circumvent the \textit{quadratic} complexity,
we employ the decomposition of a softmax operation
into two independent softmax operations such that the product of keys and values can be computed first,
resulting in \textit{linear} complexity.
The proposed linear complexity attention-based global context modules capture 
global correlations in an \textit{implicit} way, as there is no need to directly compute
the attention map during training and testing.
In addition, we also propose the \textit{linear} complexity inter-slice global spatial
context modules to explore the global correlations in all preceding slices.
The \textit{linear complexity} context allows our model to have a \textit{linear} relationship
between consumed GPU memory and resolution \textit{without} additional bits, while
having the performance gain that comes from the global contexts.
Ultimately, we integrate the channel, local, intra-slice, inter-slice global contexts,
along with the side information for multi-reference entropy modeling.
Our contributions are summarized as follows:
\begin{itemize}
%\item We apply the procedure to the multi-dimensional Hawkes process %with exponential kernels
\item To address the degradation associated with checkerboard context modeling
while preserving the benefits of two-pass decoding, we devise a novel
approach called shifted window-based checkerboard attention with \textit{linear} complexity.
This technique enables us to capture local spatial contexts more effectively.
\item In order to exploit the global correlations within the latent representations,
we divide them into slices and utilize the attention map of the
previously decoded slice to predict the global correlations in the current slice.
To overcome the \textit{quadratic} complexity typically associated with the softmax operation
in vanilla attention, we decompose it into two independent
softmax operations, thereby achieving \textit{linear} complexity without
sacrificing performance. Additionally, we explore the global correlations
from previous slices.
\item We design \textit{linear complexity}  multi-reference entropy model MEM$^{++}$ which
captures local spatial, global spatial and channel contexts, as well as
hyper-prior side information. Based on MEM$^{++}$,
we propose MLIC$^{++}$, which achieves state-of-the-art performance.
The complexity and GPU memory consumption of our model is \textit{linear} with the resolution.
Our proposed MLIC$^{++}$ achieved a better trade-off between complexity and performance as depicted in Fig.~\ref{fig:ctx_compare_new}
.
\end{itemize}\par
In comparison to our previous work presented at ACMMM 2023~\cite{jiang2022mlic}, MLIC$^{++}$ introduces several significant advancements.
The primary distinction lies in the utilization of the proposed \textit{linear}
complexity global spatial context modules without sacrificing performance,
as opposed to the \textit{quadratic} complexity observed in our prior work.
This achievement is primarily attributed to the division
of the softmax operation, which eliminates the need for
a specific order of tensor computation. Our proposed modules
incorporate advanced techniques such as learnable position embedding and
depth-wise residual bottlenecks~\cite{jiang2023slic}.
Furthermore, MLIC$^{++}$ captures inter-slice global correlations from all previous slices,
in contrast to our previous work~\cite{jiang2022mlic},
which only considers correlations within the previous one slice.
MLIC$^{++}$
exhibits several advantages,
including reduced GPU memory consumption and faster encoding and decoding speed.
The \textit{linear} complexity makes our MLIC$^{++}$ highly suitable for high-resolution image coding.
These advancements in MLIC$^{++}$ contribute to the field of image compression
by offering improved efficiency and performance,
while maintaining high-quality compression capabilities.
  % In recent years, we have seen much progress in entropy model design~\cite{minnen2018joint,minnen2020channel,he2022elic,jiang2022mlic}.
  % Most entropy models capture correlations in one dimension,
  % however, there are channel-wise, local spatial, and global spatial
  % correlations, leading to sub-optimal performance.
  % To overcome this limitation, Jiang \textit{et al.}
  % introduce the multi-reference entropy model (MEM)~\cite{jiang2022mlic} and propose the learned
  % image compression models MLIC and MLIC$^+$. In MLIC and MLIC$^+$,
  % the latent representation $\hat{\boldsymbol{y}}$ is divided into slices $\{\hat{\boldsymbol{y}}^0, \hat{\boldsymbol{y}}^1, \cdots\}$
  % ~\cite{minnen2020channel} for channel-wise correlation
  % capturing. For the $i$-th slice, they propose the checkerboard attention
  % for local spatial correlations capturing with two-pass decoding, where the slice is divided into
  % anchor part $\hat{\boldsymbol{y}}^i_{ac}$ and non-anchor part $\hat{\boldsymbol{y}}^i_{na}$.
  % In addition, they propose to use the attention map of the previous
  % slice to predict the global correlations in the current slice.
  % The process is
  % $\textrm{softmax}\left(\hat{\boldsymbol{y}}^{i-1}_{na}\left(\hat{\boldsymbol{y}}^{i-1}_{ac}\right)^{\top}\right)\hat{\boldsymbol{y}}^{i}_{ac}$.
  % The main drawback of MLIC and MLIC$^+$ is the quadratic complexity
  % of global correlations capturing, caused by the
  % softmax operation in attention, which specifies the order of matrix calculation.
  % In this paper, we investigate the global correlations capturing
  % with linear complexity. We decompose the softmax operation~\cite{shen2021efficient} into two softmax operations
  % $\textrm{softmax}_2\left(\hat{\boldsymbol{y}}^{i-1}_{na}\right)\textrm{softmax}_1\left(\hat{\boldsymbol{y}}^{i-1}_{ac}\right)^{\top}\hat{\boldsymbol{y}}^{i}_{ac}$.
  % Since $\textrm{softmax}_2\left(\hat{\boldsymbol{y}}^{i-1}_{na}\right)\textrm{softmax}_1\left(\hat{\boldsymbol{y}}^{i-1}_{ac}\right)^{\top}\geq 0$,
  % it can be treated as the global similarity.
  % Such decomposition enables global correlations capturing with linear complexity.
  % Based on such decomposition, we propose linear complexity
  % intra-slice and inter-slice global spatial context modules, and
  % propose learned image compression model MLIC$^{++}$, denoting
  % linear complexity multi-reference entropy modeling for learned image compression.
  \section{Related Works}
  \label{sec:related}
  \subsection{Learned Image Compression}
  Learned image compression~\cite{balle2016end,theis2017lossy} aims to optimize the trade-off
  between distortion $\mathcal{D}$ and entropy, where entropy is typically measured in terms of bit-rate $\mathcal{R}$.
  Large bit-rate usually leads to lower distortion .
  Lagrange multiplier $\lambda$ is employed to adjust the weight of
  distortion to control the target bit-rate. The optimization target is
  \begin{equation}\label{eq:rd}
      \mathcal{L} = \mathcal{R} + \lambda \mathcal{D}.
  \end{equation}\par
  The fundamental learned image compression framework~\cite{balle2016end,theis2017lossy}
  is based on the an auto-encoder with a rate penalty. This framework comprises an
  analysis transform $g_a$, a quantization function $Q$, a synthesis transform $g_s$ and
  an entropy model to estimate rates. The process can be formulated as:
  \begin{equation}
      \boldsymbol{y} = g_a(\boldsymbol{x};\theta), \hat {\boldsymbol{y}} = Q(\boldsymbol{y}), \hat {\boldsymbol{x}} = g_s(\hat {\boldsymbol{y}};\phi),
  \end{equation}
  where $\boldsymbol x$ represents the input image, $g_a$ transform the $\boldsymbol x$ to
  compact latent representation $\boldsymbol y$. $\boldsymbol y$
  is quantized to $\hat {\boldsymbol{y}}$ for entropy coding.
  $\hat {\boldsymbol{x}}$ represents the decompressed image.
  $\theta$ and $\phi$ are parameters of $g_a$ and $g_s$.
  Since quantization is non-differentiable, it can be addressed during training by either adding uniform noise
  $\mathcal{U}(-0.5, 0.5)$~\cite{balle2016end,balle2018variational} or using the straight-through estimator (STE)~\cite{theis2017lossy}.
  In particular, when uniform noise is added, the rate-distortion optimization
  target in Equation~\ref{eq:rd} is equivalent to evidence lower bound (ELBO) optimization in variational
  auto-encoders~\cite{kingma2014vae}.
  To enhance non-linearity, Generalized Divisive Normalization (GDN)~\cite{balle2015gdn} layers or its variants~\cite{qian2020learning} are employed.
  Additionally, self-attention~\cite{vas2017attention,liu2021swin,lu2021tic, zou2022the,liu2023learned,guo2021causal},
  ensemble techniques~\cite{wang2020ensemble}, and block partition~\cite{wu2021learned} are utilized in transform modules for
  more compact latent representations.
  In the basic model, a factorized or
  a non-adaptive density entropy model is adopted.\par
  In subsequent works, a hyper-prior module~\cite{balle2018variational} is introduced to extract side information $\hat {\boldsymbol{z}}$ from $\boldsymbol y$.
  The hyper-prior model estimates the distribution of $\hat {\boldsymbol{y}}$ from
  $\hat {\boldsymbol{z}}$. A univariate Gaussian distribution is commonly employed for the hyper-prior.
  Some works extend it to a mean and scale Gaussian distribution~\cite{minnen2018joint},
  asymmetric Gaussian distribution~\cite{cui2021asym},
  Gaussian mixture model~\cite{cheng2020learned,liu2020unified}, and Gaussian-Laplacian-Logistic mixture model~\cite{fu2023learned}
  for more flexible distribution modeling.
  \subsection{Context-based Entropy Modeling}
  \label{sec:related:context}
  Numerous approaches~\cite{minnen2018joint,minnen2020channel,qian2020learning} have been proposed to improve the accuracy of context modeling in learned image compression.
  These methods encompass various types of context modules, including local spatial, global spatial, and channel-wise context modules.\par
  Local spatial context modules aim to capture correlations between adjacent symbols.
  For instance, Minnen~\textit{et al.}~\cite{minnen2018joint} utilize a pixel-cnn-like~\cite{van2016conditional}
  masked convolutional layer to capture local correlations between
$\hat {\boldsymbol{y}}_i$ and symbols $\hat {\boldsymbol{y}}_{<i}$,
resulting in serial decoding.
He \textit{et al.}~\cite{he2021checkerboard}
divide latent representation $\hat {\boldsymbol{y}}$ into
anchor part $\hat {\boldsymbol{y}}_a$ and non-anchor part $\hat {\boldsymbol{y}}_{na}$,
employing a checkerboard convolution to extract contexts of
$\hat {\boldsymbol{y}}_{na}$ from $\hat {\boldsymbol{y}}_a$, thereby
achieving two-pass parallel decoding.
\par
On the other hand, some approaches focus on modeling correlations between distant symbols.
In~\cite{qian2020learning}, neighboring left and top symbols serve as bases for computing the similarity
between the target symbol and its previous symbols.
Guo~\textit{et al.}~\cite{guo2021causal} employ
the $L2$ distances of symbols to predict global casual dependencies among symbols.
In~\cite{kim2022joint}, the side information
is divided into global side information and local side information, introducing additional bits.
However, these global context modules are typically combined with serial auto-regressive context modules,
which further increase decoding latency.
Moreover, existing global context modules~\cite{qian2020learning,guo2021causal,jiang2022mlic} often exhibit \textit{quadratic} complexity,
making them challenging to apply in high-resolution image coding.
Alternatively, they rely on extra side information~\cite{kim2022joint}, which increases the bit-rate.
\par
Minnen \textit{et al.}~\cite{minnen2020channel} model contexts between channels.
$\hat {\boldsymbol{y}}$ is evenly divided to slices. The current slice
$\hat {\boldsymbol{y}}^i$ is conditioned on previously decoded slices
$\hat {\boldsymbol{y}}^{<i}$. To address the uneven distribution of information among slices,
an unevenly grouped channel-wise context module is introduced in~\cite{he2022elic}.\par
While some local and channel-wise context modules~\cite{ma2021cross,he2022elic,jiang2022mlic}
have demonstrated impressive performance,
effectively capturing local, global, and channel-wise contexts with \textit{acceptable} even \textit{linear} complexity
within a single entropy model remains a challenge.
Addressing these correlations has the potential to further enhance the performance of image compression models.
% Figure environment removed
  % Figure environment removed
\begin{table}[t]
  \scriptsize
  \centering
  \begin{tabular}{c|c}
  \toprule
  Notations                                         & Explanation   \\ \midrule
  $\boldsymbol{x},\hat{\boldsymbol{x}}$                                  & Input and decoded image              \\\midrule
  $\boldsymbol{y},\hat{\boldsymbol{y}}$                                  & Non-quantized and quantized latent representation               \\\midrule
  $\hat{\boldsymbol{y}}^i $                         & The $i$-th slice of $\hat{\boldsymbol{y}}$\\\midrule
  $\hat{\boldsymbol{y}}_{ac},\hat{\boldsymbol{y}}_{na}$                       & Anchor and non-anchor part of $\hat{\boldsymbol{y}}$             \\\midrule
  $\boldsymbol{z},\hat{\boldsymbol{z}}$                                  & Non-quantized and quantized side information            \\\midrule
  $g_{ep}$                                          & Entropy parameter module \\\midrule
  $\mu,\sigma$                                      & Mean and scale of $\hat{\boldsymbol{y}}$           \\\midrule
  $g_a, g_s$                                        & Analysis and synthesis transform         \\\midrule
  $h_a,h_s$                                         & Hyper analysis and synthesis         \\\midrule
  $g_{ch}$                                          & Channel-wise context module           \\\midrule
  $g_{lc,ckbd}$                                     & Vanilla checkerboard context module           \\\midrule
  $g_{lc,attn}$                                     & Shifted Window-based Checkerboard Attention           \\\midrule
  $g_{gc,intra}$                                    & Intra-slice global spatial context module     \\\midrule
  $g_{gc,inter}$                                    & Inter-slice global spatial context module     \\\midrule
  $ {\boldsymbol{\Phi}}_{h},\boldsymbol{\Phi}_{ch},{\boldsymbol{\Phi}}_{lc}$               & Hyper-prior, channel-wise and local spatial context   \\\midrule
  $ {\boldsymbol{\Phi}}_{gc,intra}$                 & Intra-slice global spatial context   \\\midrule
  $ {\boldsymbol{\Phi}}_{gc,inter}$                 & Inter-slice global spatial context   \\\midrule                  
  \multicolumn{1}{c|}{\multirow{2}{*}{MEM $^{++}$}}                            & \multicolumn{1}{c}{Linear complexity }   \\
                                                    &  multi-reference entropy model            \\\midrule
  $M, N, S$                                         & Channel number of $\boldsymbol{y}$, $\boldsymbol{z}$, and $\hat{\boldsymbol{y}}^i$          \\\midrule
  $K$                                               & Kernel size of local spatial context module  \\\midrule
  Q, AE, AD                                         & Quantization, arithmetic encoding and decoding            \\\midrule
  \end{tabular}
  \label{tab:notation}
  \caption{Explanations of notations.}
\end{table}
% Figure environment removed
\section{Method}
\label{sec:method}
\subsection{Motivation}
According to information theory, the conditional entropy is bounded by the entropy:
\begin{equation}
    H(\hat{\boldsymbol{y}}) \geq H(\hat{\boldsymbol{y}}|\boldsymbol{ctx}),
\end{equation}
where $H$ denotes Shannon entropy, $\boldsymbol{ctx}$ is the context of $\hat {\boldsymbol{y}}$.
Exploiting correlations in $\hat {\boldsymbol{y}}$ results in bit savings.\par
In Fig.~\ref{fig:cosine} and Fig.~\ref{heatmap},
channel-wise correlations and spatial correlations in
latent representation of Kodim19 extracted by Cheng'20~\cite{cheng2020learned} are illustrated.\par
Fig.~\ref{fig:cosine} visualizes the features of several channels, revealing their significant similarity.
However, capturing such correlations poses a challenge for spatial context modules,
as they employ the same \textit{mask} for all channels during context extraction.
Consequently, certain correlations may not be fully captured.\par
In Fig.~\ref{heatmap}, cosine similarity between
each symbol and the symbol in the bottom right corner are visualized.
Symbols with the same color exhibit a high degree of correlation.
Neighbouring symbols have a very high degree of similarity.
This observation emphasizes the necessity of a local context module.
Furthermore, a global context module is required to capture the correlations
between symbols in the bottom-left corner and those in the bottom-right corner,
where the grass features share similarities. Additionally, 
the complexity of global context capturing should be carefully considered and minimized for
high-resolution image coding.
The latent representation contains redundancy,
indicating the potential for bit savings by modeling such correlations.\par
However, existing entropy models fail to capture correlations in local spatial, global spatial, and channel domains.
Spatial context modules have limited interactions between channels, while channel-wise context modules
lack interaction within the current slice.
Moreover, extending these models to high-resolution image coding with \textit{acceptable} even \textit{linear} complexity is \textit{non-trivial}.
These challenges, along with the potential to enhance rate-distortion performance,
motivate us to design a \textit{linear} complexity  multi-reference entropy model.
Our proposed \textit{linear} complexity multi-reference entropy model effectively
captures correlations in local spatial, global spatial, and channel domains,
while maintaining a modest complexity for \textit{high-resolution} image coding.
Further details on our model are presented in the subsequent sections.
  \subsection{Overall Architecture}
  \label{sec:method:overview}
  \subsubsection{MLIC$^{++}$}
The overall architecture of proposed model is illustrated in Fig.~\ref{fig:arch}.
This model is named MLIC$^{++}$ to distinguish it from MLIC, and MLIC$^+$, which are introduced in our conference version~\cite{jiang2022mlic}.
The architecture of MLIC$^{++}$, as depicted in Fig.~\ref{fig:arch}, incorporates the analysis transform $g_a$, synthesis transform $g_s$,
hyper analysis $h_a$, and hyper synthesis $h_s$, which are simplified versions of Cheng'20~\cite{cheng2020learned}.
To reduce complexity, attention modules are removed.
\begin{table}[t]
  \scriptsize
  \centering
  \begin{tabular}{lccccccccc}
    \toprule
     $N$    & $M$    & $S$     & $K$    & Entropy Model       \\ \midrule
       $192$  & $320$  &$32$  & $5$   &   MEM$^{++}$($g_{lc,attn}, g_{ch}, g_{gc,intra}, g_{gc,inter}$)   \\\midrule
  \end{tabular}
  \caption{Settings of MLIC$^{++}$ and MEM$^{++}$.}
  \label{tab:settings}
\end{table}
% Figure environment removed
The hyper-parameters and settings of MLIC$^{++}$ are presented in Table~\ref{tab:settings}.
Same to Minnen \textit{et al.}~\cite{minnen2020channel},
we adopt \textit{mixed quantization}, which involves adding uniform noise for entropy estimation
and utilizing STE~\cite{theis2017lossy} to ensure differentiability in the quantization process.
Gaussian mean-scale distribution is adopted for entropy estimation.
For latent representation $\boldsymbol{y}$, the quantization and estimated rate is formulated as:
\begin{equation}
    \hat{\boldsymbol{y}} = \mathrm{STE}(\boldsymbol{y} - \boldsymbol{\mu}) + \boldsymbol{\mu},\\
\end{equation}
\begin{equation}
  \mathcal{R}_{\hat{\boldsymbol{y}}}=\mathbb{E}\left[ -\log \int_{-0.5}^{0.5} p({\boldsymbol{y}+\boldsymbol{u}})d\boldsymbol{u}  \right],
\end{equation}
where $\boldsymbol{\mu}$ is the estimated mean of latent representation $\boldsymbol{y}$, $\boldsymbol{u} \sim \mathcal{U}(-0.5,0.5)$, $\mathcal{R}_{\hat{\boldsymbol{y}}}$ is the estimated rate of
$\hat{\boldsymbol{y}}$.
\subsubsection{MEM$^{++}$}
The proposed linear complexity multi-reference entropy model effectively captures channel-wise, local spatial, and global spatial correlations with linear complexity.
The linear complexity entropy model is denoted as MEM$^{++}$ to distinguish it from MEM, and MEM$^{+}$,
which are proposed in our conference version~\cite{jiang2022mlic}.
To capture multi-correlations, the proposed MEM$^{++}$ consists of four components:
channel-wise context module $g_{ch}$, local spatial context module $g_{lc}$,
intra-slice global spatial context module $g_{gc,intra}$, and inter-slice global spatial context module $g_{gc,inter}$.
In the channel-wise context module, the latent representation $\hat{\boldsymbol{y}}$
is divided into slices $\{\hat {\boldsymbol{y}}^0, \hat {\boldsymbol{y}}^1, \cdots\, \hat {\boldsymbol{y}}^L\}$~\cite{minnen2020channel} along the channel dimension,
$L$ is the number of slices.
For the $i$-th slice $\hat {\boldsymbol{y}}^i$, the channel-wise context module captures
the channel-wise context $\boldsymbol{\Phi}_{ch}^i$ from slices $\hat {\boldsymbol{y}}^{<i}$.
To capture local spatial correlations, checkerboard pattern~\cite{he2021checkerboard} is employed, where
the latent representation $\hat{\boldsymbol{y}}^i$ is divided into
anchor part $\hat{\boldsymbol{y}}_{ac}^i$ and non-anchor part $\hat{\boldsymbol{y}}_{na}^i$.
$\hat{\boldsymbol{y}}_{ac}^i$ is local-context-free.
Local spatial context ${\boldsymbol{\Phi}}_{lc}^i$ of
$\hat{\boldsymbol{y}}_{na}^i$ is captured from $\hat{\boldsymbol{y}}_{ac}^i$.
We propose Overlapped Window-based Checkerboard Attention $g_{lc, attn}$ for better non-linearity and adaptability
to capture local spatial contexts.
The global contexts $\boldsymbol{\Phi}_{gc}$ of $i$-th slice are extracted from two dimensions:
intra-slice contexts $\boldsymbol{\Phi}_{gc, intra}^i$, and inter-slice contexts $\boldsymbol{\Phi}_{gc, inter}^i$.
We propose Intra-Slice Global Context Module $g_{gc, intra}^i$ and
Inter-Slice Global Context Module $g_{gc, inter}$ to capture such correlations.
Since different slices share the similar global similarity~\cite{jiang2022mlic,guo2021causal},
the global similarity of $\hat{\boldsymbol{y}}^{i-1}$ is employed
to predict the global correlations between $\hat{\boldsymbol{y}}^{i}_{ac}$ and $\hat{\boldsymbol{y}}^{i}_{na}$.
The inter-slice global context $\Phi^{i}_{gc,inter}$ are extracted
from slices $\hat{\boldsymbol{y}}^{< i}$ via the
global similarity of slices $\hat{\boldsymbol{y}}^{< i}$.
We introduce these modules in the following sections.
The structure of MEM$^{++}$ is illustrated in Table~\ref{tab:settings}.
We use Equation~\ref{eq:rd} as our loss function
and the estimated rate can be formulated as:
$\mathcal{R} =  \mathcal{R}_{\hat{\boldsymbol{z}}} + \sum^L_{i=0}\left(\mathcal{R}_{\hat{\boldsymbol{y}}^i_{ac}} + \mathcal{R}_{\hat{\boldsymbol{y}}^i_{na}}\right)$, where
\begin{equation}
  \mathcal{R}_{\hat{\boldsymbol{z}}}=\mathbb{E}\left[ -\log \int_{-0.5}^{0.5} p({\boldsymbol{z}+\boldsymbol{u}})d\boldsymbol{u}  \right],
\end{equation}
\begin{equation}
    \mathcal{R}_{\hat{\boldsymbol{y}}^i_{ac}} = \mathbb{E}\left[-\log \int_{-0.5}^{0.5} p\left({\boldsymbol{y}}^i_{ac} + \boldsymbol{u}|\Phi_{h}, \Phi_{ch}^i, \Phi_{gc,inter}^i \right)d\boldsymbol{u} \right],
    \end{equation}
\begin{equation}
  \begin{aligned}
  \begin{split}
  &\mathcal{R}_{\hat{\boldsymbol{y}}^i_{na}} = \\ &\mathbb{E}\left[-\log \int_{-0.5}^{0.5} p\left({\boldsymbol{y}}^i_{na}+\boldsymbol{u}|\Phi_{h}, \Phi_{lc}^i, \Phi_{ch}^i, \Phi_{gc,intra}^i, \Phi_{gc,inter}^i \right) d\boldsymbol{u}\right],
  \end{split}
\end{aligned}
\end{equation}
  $\mathcal{R}_{\hat{\boldsymbol{z}}}$ is the rate of side information,
  $\mathcal{R}_{\hat{\boldsymbol{y}}^i_{ac}}$ denotes the rate of the anchor
  part of $i$-th slice,   $\mathcal{R}_{\hat{\boldsymbol{y}}^i_{na}}$ denotes the rate of the non-anchor
  part of $i$-th slice,
$\boldsymbol{\Phi}_{h}$ is the hyper-priors extracted by hyper analysis $h_a$ and hyper synthesis $h_s$.\par
  % The overall framework of our MLIC$^{++}$ is similar with
  % MLIC$^+$~\cite{jiang2022mlic}. We briefly introduce our framework first.
  % In our framework, we adopt the transform modules of MLIC$^+$,
  % a simplified version of the transform modules of Cheng'20~\cite{cheng2020learned}.
  % We adopt the same transform modules to prove the effectiveness of
  % our proposed linear complexity multi-reference entropy model MEM$^{++}$.
  % In our MEM$^{++}$, we adopt the checkerboard attention~\cite{jiang2022mlic} for local spatial
  % context capturing and use the same settings of MLIC$^{+}$~\cite{jiang2022mlic}
  % for channel-wise context capturing .
  % we use $\Phi_{h}$ to denote the hyper-prior.
  % We first divide the latent representation $\hat{\boldsymbol{y}}$
  % into slices $\{\hat{\boldsymbol{y}}^0,\hat{\boldsymbol{y}}^1,\cdots \}$~\cite{minnen2020channel}.
  % We take the $i$-th slice as an example. We divide the $\hat{\boldsymbol{y}}^i$
  % into anchor part $\hat{\boldsymbol{y}}_{ac}^i$ and non-anchor part
  % $\hat{\boldsymbol{y}}_{na}^i$. We extract channel-wise contexts
  % $\Phi_{ch}^i$ from slices $\hat{\boldsymbol{y}}^{< i}$.
  % $\hat{\boldsymbol{y}}_{ac}^i$ is local-spatial-context-free.
  % We extract local spatial context $\Phi_{lc}^i$ of $\hat{\boldsymbol{y}}_{na}^i$
  % from $\hat{\boldsymbol{y}}_{ac}^i$. Slice different slices
  % share the similar global similarity~\cite{jiang2022mlic,guo2021causal},
  % % we use the global similarity of $\hat{\boldsymbol{y}}^{i-1}$
  % % to predict the global correlations between $\hat{\boldsymbol{y}}^{i}_{ac}$
  % % and $\hat{\boldsymbol{y}}^{i}_{na}$.
  % We extract the intra-slice global context $\Phi^{i}_{gc,intra}$
  % of $\hat{\boldsymbol{y}}_{na}^i$ from $\hat{\boldsymbol{y}}_{ac}^i$
  % via the global similarity of $\hat{\boldsymbol{y}}^{i-1}$.
  % Besides, we extract the inter-slice global context $\Phi^{i}_{gc,inter}$
  % from slices $\hat{\boldsymbol{y}}^{< i}$ via the
  % global similarity of slices $\hat{\boldsymbol{y}}^{< i}$.
  % Therefore, the rate of $\hat{\boldsymbol{y}}^i_{ac}$ and
  % the rate of $\hat{\boldsymbol{y}}^i_{na}$ are:
  % \begin{equation}
  % \begin{aligned}
  %     \mathcal{R}^i_{ac} &= \mathbb{E}\left[-\log_2p_{\hat{\boldsymbol{y}}^i_{ac}}\left(\hat{\boldsymbol{y}}^i_{ac}|\Phi_{h}, \Phi_{ch}^i, \Phi_{gc,inter}^i \right) \right]\\
  %     \mathcal{R}^i_{na} &= \mathbb{E}\left[-\log_2p_{\hat{\boldsymbol{y}}^i_{na}}\left(\hat{\boldsymbol{y}}^i_{na}|\Phi_{h}, \Phi_{ch}^i, \Phi_{gc,intra}^i, \Phi_{gc,inter}^i \right) \right]
  % \end{aligned}
  % \end{equation}
  \subsection{Channel-wise Context Module}\label{sec:method:channel}
To extract channel-wise contexts, the latent representation $\hat {\boldsymbol{y}}$ is first evenly divided
into multiple slices
$\{\hat {\boldsymbol{y}}^0, \hat {\boldsymbol{y}}^1, \cdots, \hat {\boldsymbol{y}}^L\}$ along the channel dimension.
Slice $\hat {\boldsymbol{y}}^i$ is conditioned on
slices $\hat {\boldsymbol{y}}^{<i}$.
A channel context module $g_{ch}$ is employed to squeeze and
extract context information from $\hat {\boldsymbol{y}}^{<i}$ when
encoding and decoding $\hat {\boldsymbol{y}}^i$. $g_{ch}$ consists of
three $3\times 3$ convolutional layers. The channel context becomes
${\boldsymbol{{\boldsymbol{\Phi}}}}_{ch}^i = g_{ch}(\hat {\boldsymbol{y}}^{<i})$.
The channel-wise context module $g_{ch}$ is able to
refer to symbols in the same and close position in the previous
slices and helps select the most relative
channels and extract information beneficial for accurate probability estimation.
The channel number of each slice $S$ is a hyper-parameter.
Following Minnen \textit{et al}~\cite{minnen2020channel},
we set $S$ to $32$ and $L$ to $10$ in our model.
Following existing methods~\cite{minnen2020channel,zou2022the},
latent residual prediction (LRP) modules~\cite{minnen2020channel} are adopted
to predict quantization error according to
decoded slices and hyper-priors $\boldsymbol{\Phi}_{h}$.
Since the channel number of latent representation is freezed during training and
inference and the number of slices is quite small, the encoding speed
and decoding speed is still fast enough in spite of serial process among slices.
% Figure environment removed
\subsection{Checkerboard Attention-based Local Context Module}
One limitation of CNN-based local context modules is their fixed weights,
which restricts their ability to capture content-adaptive contexts.
We argue that context-adaptation is essential due to the vast diversity of images.
In transformers~\cite{vas2017attention,dosovitskiy2020image,liu2021swin},
the attention weight is generated dynamically according to the input,
which inspires us to design a transformer-based content-adaptive local context module.
The local receptive field can be envisioned as a window,
where local spatial contexts are captured by dividing the feature map into windows.
Since each symbol is most relevant to the symbols around it, we propose to
make the divided windows \textit{overlapped}. To achieve this,
we propose the novel checkerboard attention context module $g_{lc, attn}$.
The process of $i$-th slice is taken as an example.
Assuming the resolution of the latent representation $\hat {\boldsymbol{y}}^i$ is $H \times W$,
the stride is set to $1$ to divide $\hat {\boldsymbol{y}}^i$
into $H \times W$  overlapped windows and the window size is $K\times K$.
To extract local correlations, the attention map of each window is computed at first.
Same as the convolutional checkerboard context module, interactions between $\boldsymbol{y}_{ac}^i$ and $\boldsymbol{y}_{na}^i$
and interactions in $\boldsymbol{y}_{na}^i$ are not allowed. An example of the attention mask is illustrated
in Fig.~\ref{ckbd_attn}. Importantly, this attention mechanism does not alter the resolution of each window.
Subsequently, A $K\times K$ convolutional layer is utilized to fuse local context information and
and match the size of the local context with that of $\boldsymbol{y}^i$ before feeding
it to a feed-forward network (FFN)~\cite{vas2017attention}. The overall process is similar to standard transformer~\cite{vas2017attention}.
The process is formulated as:
\begin{equation}
    {{\hat {\boldsymbol{y}}}^i}_{attn} = \textrm{softmax}\left(\frac{{{\hat {\boldsymbol{y}}}^i}_{ac,q} \times ({{\hat {\boldsymbol{y}}}^i}_{ac,k})^\top}{\sqrt{S}} + mask\right) \times {{\hat {\boldsymbol{y}}}^i}_{ac,v},
\end{equation}
\begin{equation}
    {{\hat {\boldsymbol{y}}}^i}_{conv} = \textrm{conv}_{K\times K}({{\hat {\boldsymbol{y}}}^i}_{attn}),
\end{equation}
\begin{equation}
    {\boldsymbol{\Phi}}^i_{lc} = \textrm{FFN}({{\hat {\boldsymbol{y}}}^i}_{conv}) + {{\hat {\boldsymbol{y}}}^i}_{conv},
\end{equation}
where $\hat {\boldsymbol{y}}^i_{ac,q}, \hat {\boldsymbol{y}}^i_{ac,k}, \hat {\boldsymbol{y}}^i_{ac,v} = \textrm{Embed}(\hat {\boldsymbol{y}}^i_{ac})$,
$\hat {\boldsymbol{y}}^i_{ac}$ is anchor part of $i$-th slice, $mask$ the attention mask,
$S$ is the channel number of each slice, FFN is the feed-forward neural network~\cite{vas2017attention}.\par
Note that our overlapped window-partition is with \textit{linear} complexity, since the complexity of each window
is $O(K^4)$.
The complexity of $g_{lc, attn}$ is $\Omega(K^4SL)$,
where $L=HW$, $S$ is the channel number of a slice.
% Channel-wise context modeling is a serial process. Because the the number of slices $L$ is usually less than or equal to $10$, the latency is acceptable.
\subsection{Linear Complexity Intra-Slice Global Context Module}
\label{sec:method:intra}
During the decoding process, it is challenging to determine the global
correlations between the current symbol and other symbols
due to the inherent \textit{encoding-decoding consistency}.
This is because the current symbol is unknown during decoding.
One potential solution is to embed the global correlations into the bit-stream,
but this approach introduces additional bits, thereby increasing the overall bit-rate.
Furthermore, in order to obtain precise global similarity,
it is necessary to calculate the similarity between the current symbol and \textit{all} other symbols,
which consumes a significant number of bits and is impractical to employ in real-world scenarios.
Consequently, representing global similarity with a limited number of bits or \textit{without} the need for additional bits
becomes a \textit{non-trivial} task.
% Figure environment removed
% Figure environment removed
% Figure environment removed
In latent representation $\hat{\boldsymbol{y}} \in \mathbb{R}^{L\times C}$, where $L=H\times W$, $C$ is the channel number,
each channel contains distinct information, but they can be considered as thumbnails, as depicted in Fig.~\ref{fig:cosine}.
Notably, the channels exhibit \textit{similar} global similarities.
This is evident from the visualization of cosine similarities between two slices
of Cheng'20~\cite{cheng2020learned}, as visualized in Fig.~\ref{intra_cosine},
where despite differences in magnitude, the global correlations are similar.
When decoding the current slice $\hat {\boldsymbol{y}}^i \in \mathbb{R}^{L\times S}$, decoded slice
$\hat {\boldsymbol{y}}^{i-1} \in \mathbb{R}^{L\times S}$ assists in estimating the global correlations
in slice $\hat {\boldsymbol{y}}^{i}$.
However, a challenge arises in determining how to estimate these global correlations.
While cosine similarity may be useful,
it is fixed and may not accurately capture the features.
In this regard, attention maps prove to be a suitable choice.
The embedding layer is learnable, which make it flexible
to adjust the method for global correlations estimation by modifying queries, keys, and values.\par
First, the \textit{vanilla} approach is introduced.
The process of $i-1$-th slice and the $i$-th slice are taken as an example.
When compressing or decompressing $\hat {\boldsymbol{y}}^i$,
the correlations between anchor part $\hat {\boldsymbol{y}}^{i-1}_{ac}\in \mathbb{R}^{L\times S}$
and non-anchor part $\hat {\boldsymbol{y}}^{i-1}_{na}\in \mathbb{R}^{L\times S}$ of slice $\hat {\boldsymbol{y}}^{i-1}$ are first computed.
Because the checkerboard local context module makes anchor visible when decoding non-anchor part,
we multiply the anchor part of current slice $\hat {\boldsymbol{y}}^{i}_{a}$ with
the attention map between $\hat {\boldsymbol{y}}^{i-1}_{ac}$ and $\hat {\boldsymbol{y}}^{i-1}_{na}$,
which is employed as the approximation of global similarities between $\hat {\boldsymbol{y}}^{i}_{ac}$ and $\hat {\boldsymbol{y}}^{i}_{na}$.
Due to the local correlations, adjacent symbols have similar global correlations.
A $K\times K$ convolutional layer is employed to refine the attention
map by aggregating global similarities of adjacent symbols.
The process of this Intra-Slice Global Context $g_{gc, inter}$ is parallel and is formulated as:
\begin{equation}\label{eq:vanilla_intra}
    \hat {\boldsymbol{y}}^{i}_{attn} = \underbrace{\textrm{softmax}\left(\frac{{{\hat {\boldsymbol{y}}}^{i-1}}_{na,q} \times \left({{\hat {\boldsymbol{y}}}^{i-1}}_{ac,k}\right)^\top}{\sqrt{S}}\right)}_{\textrm{non-negative}} \times \hat {\boldsymbol{y}}^{i}_{ac,v},
\end{equation}
\begin{equation}
    \hat {\boldsymbol{y}}^{i}_{conv} = \textrm{conv}_{K\times K}(\hat {\boldsymbol{y}}^{i}_{attn}),
\end{equation}
\begin{equation}
    {\boldsymbol{\Phi}}^i_{gc,intra} = \textrm{DepthRB}(\hat {\boldsymbol{y}}^{i}_{conv}),
\end{equation}
where $\hat {\boldsymbol{y}}^{i-1}_{na,q}, \hat {\boldsymbol{y}}^{i-1}_{ac,k} = \textrm{Embedding}\left(\hat {\boldsymbol{y}}^{i-1}\right)$, $\hat {\boldsymbol{y}}^{i}_{ac,v} = \textrm{Embedding}\left(\hat {\boldsymbol{y}}^i_{ac}\right)$,
Embedding is the embedding layer. Embedding layer consists of a $1\times 1$ convolutional layer and
a $3\times 3$ depth-wise convolutional layer. The $3\times 3$ depth-wise convolutional layer is
employed for learnable position embedding.
This is because the self attention is permutation-invariant and lacks inductive bias.
Using a depth-wise convolutional for position embedding does have serval benefits.
First, a $3\times 3$ depth-wise convolution is quite light, which has negligible
influences on overall complexity. Second, a convolution-based position embedding is flexible
for any resolution, due to its translation equivariance. Third, the convolution is
able to embed position information because of the zero-padding and the boundary effects~\cite{kayhan2020translation,islam2019much} of images.
$\textrm{DepthRB}$ is the depth-wise residual bottleneck~\cite{jiang2023slic} and is employed to enhance the non-linearity.\par
One drawback of \textit{vanilla} approach is its \textit{quadratic} complexity.
In Equation~\ref{eq:vanilla_intra}, the softmax operation specifies the order of tensor calculation.
The complexity of ${{\hat {\boldsymbol{y}}}^{i-1}}_{na,q} \times \left({{\hat {\boldsymbol{y}}}^{i-1}}_{a,k}\right)^\top$
is $O(L^2)$. The quadratic complexity leads to huge GPU memory consumption, longer
encoding and decoding time as illustrated in Fig.~\ref{fig:complex}, which makes it hard to employ the vanilla approach
for high-resolution image coding. In Equation~\ref{eq:vanilla_intra}, if $\left(\hat {\boldsymbol{y}}^{i-1}_{ac,k}\right)^\top\times \hat {\boldsymbol{y}}^{i}_{ac,v}$
is computed first, the overall complexity becomes $O(L)$, which is linear with the resolution.
Equation~\ref{eq:vanilla_intra} works because $0 < \textrm{softmax}\left(\frac{{{\hat {\boldsymbol{y}}}^{i-1}}_{na,q} \times \left({{\hat {\boldsymbol{y}}}^{i-1}}_{ac,k}\right)^\top}{\sqrt{S}}\right) < 1$.
The non-negativity makes it can be treated as a learnable similarity metric. If $\textrm{softmax}\left(\frac{{{\hat {\boldsymbol{y}}}^{i-1}}_{na,q} \times \left({{\hat {\boldsymbol{y}}}^{i-1}}_{ac,k}\right)^\top}{\sqrt{S}}\right) \to 0$,
${\hat {\boldsymbol{y}}}^{i-1}_{na,q}$ and ${\hat {\boldsymbol{y}}}^{i-1}_{na,q}$ are near orthogonal.
If $\textrm{softmax}\left(\frac{{{\hat {\boldsymbol{y}}}^{i-1}}_{na,q} \times \left({{\hat {\boldsymbol{y}}}^{i-1}}_{ac,k}\right)^\top}{\sqrt{S}}\right) \to 1$,
${\hat {\boldsymbol{y}}}^{i-1}_{na,q}$ and ${\hat {\boldsymbol{y}}}^{i-1}_{na,q}$ are very similar.
To solve the quadratic complexity, it is necessary to introduce a new operator $\rho$ which avoids the necessity to compute ${{\hat {\boldsymbol{y}}}^{i-1}}_{na,q} \times \left({{\hat {\boldsymbol{y}}}^{i-1}}_{a,k}\right)^\top$
first in practice while retaining the non-negativity. Efficient attention operation~\cite{shen2021efficient}
is introduced for non-negativity and linear complexity, which employ the softmax operation on $\hat{\boldsymbol{y}}^{i-1}_{na}$ in
row and the softmax operation on $\hat{\boldmath{y}}^{i-1}_{ac}$ in
column.
\begin{equation}
    \textrm{Attention} = \underbrace{\textrm{softmax}_2\left(\hat{\boldsymbol{y}}^{i-1}_{na,q}\right)\textrm{softmax}_1\left(\hat{\boldsymbol{y}}^{i-1}_{ac,k}\right)^{\top}}_{\textrm{non-negative}}\hat{\boldsymbol{y}}^i_{ac,v}.
    \label{eq:efficient}
\end{equation}
The process is illustrated in Equation~\ref{eq:efficient}.
In Equation~\ref{eq:efficient}, $\textrm{softmax}_2\left(\hat{\boldsymbol{y}}^{i-1}_{na,q}\right)\textrm{softmax}_1\left(\hat{\boldsymbol{y}}^{i-1}_{ac,k}\right)^{\top}$
is employed as the learnable similarity metric, where
 $0<\textrm{softmax}_2\left(\hat{\boldsymbol{y}}^{i-1}_{na,q}\right)< 1, 0<\textrm{softmax}_1\left(\hat{\boldsymbol{y}}^{i-1}_{ac,k}\right)^{\top}< 1$,
 which makes $0<\textrm{softmax}_2\left(\hat{\boldsymbol{y}}^{i-1}_{na,q}\right)\textrm{softmax}_1\left(\hat{\boldsymbol{y}}^{i-1}_{ac,k}\right)^{\top}< 1$.
 The non-negativity makes
 $\textrm{softmax}_2\left(\hat{\boldsymbol{y}}^{i-1}_{na,q}\right)\textrm{softmax}_1\left(\hat{\boldsymbol{y}}^{i-1}_{ac,k}\right)^{\top}$
 can be employed as a similarity metric.
 If $\textrm{softmax}_2\left(\hat{\boldsymbol{y}}^{i-1}_{na,q}\right)\textrm{softmax}_1\left(\hat{\boldsymbol{y}}^{i-1}_{ac,k}\right)^{\top} \to 0$,
 ${\hat {\boldsymbol{y}}}^{i-1}_{na,q}$ and ${\hat {\boldsymbol{y}}}^{i-1}_{na,q}$ are near orthogonal.
 If $\textrm{softmax}_2\left(\hat{\boldsymbol{y}}^{i-1}_{na,q}\right)\textrm{softmax}_1\left(\hat{\boldsymbol{y}}^{i-1}_{ac,k}\right)^{\top} \to 1$,
 ${\hat {\boldsymbol{y}}}^{i-1}_{na,q}$ and ${\hat {\boldsymbol{y}}}^{i-1}_{na,q}$ are very similar.
 The metric is \textit{implicit}  because there is no need to compute $\textrm{softmax}_2\left(\hat{\boldsymbol{y}}^{i-1}_{na,q}\right)\textrm{softmax}_1\left(\hat{\boldsymbol{y}}^{i-1}_{ac,k}\right)^{\top}$.
  Since we use softmax operation on $\hat{\boldsymbol{y}}^{i-1}_{na,q}$ and
  $\hat{\boldsymbol{y}}^{i-1}_{ac,k}$ separately,
  $\textrm{softmax}_1\left(\hat{\boldsymbol{y}}^{i-1}_{ac,k}\right)^{\top}\hat{\boldsymbol{y}}^{i}_{ac,v}$ can be computed first during training and testing.
  The complexity of it is $O(L)$, which is linear with the resolution.
  The linear complexity makes it easier to employ the global spatial context module
  for high-resolution image coding.
  \subsection{Linear Complexity Inter-Slice Global Context Module}
\label{sec:method:inter}
% Figure environment removed
Because of the global correlations between slices,
intra-slice global context module is extended to the inter-slice global context.
For a symbol at current slice, the symbol in previous slices is employed at the same position as
the approximation of the symbol at current slice,
since there are correlations among slices.
The correlations among slices or channels are illustrated in Fig.~\ref{fig:cosine} and Fig.~\ref{intra_cosine}.
This approximation makes
the anchor part and non-anchor part benefit from more contexts.
The process of $i$-th slice is taken as an example.
Same as linear complexity intra-slice global context module, attention mechanism is
employed to measure the similarity. To make the inter-slice global
context capturing more efficient, the softmax operation in \textit{vanilla} attention
is divided into two independent softmax operations as discussed in Section~\ref{sec:method:intra}.
The overall process is
\begin{equation}\label{eq:vanilla_inter}
  \hat {\boldsymbol{y}}^{i}_{attn} = \underbrace{\textrm{softmax}_2\left(\hat{\boldsymbol{y}}^{< i}_q\right)\textrm{softmax}_1\left(\hat{\boldsymbol{y}}^{< i}_k\right)^{\top}}_{\textrm{non-negative}}\hat{\boldsymbol{y}}^{< i}_v,
\end{equation}
\begin{equation}
  \hat {\boldsymbol{y}}^{i}_{conv} = \textrm{conv}_{K\times K}(\hat {\boldsymbol{y}}^{i}_{attn}),
\end{equation}
\begin{equation}
  {\boldsymbol{\Phi}}^i_{gc,inter} = \textrm{DepthRB}(\hat {\boldsymbol{y}}^{i}_{conv}),
\end{equation}
where $\hat {\boldsymbol{y}}^{< i}_q, \hat {\boldsymbol{y}}^{< i}_{k}, \hat {\boldsymbol{y}}^{< i}_{v} = \textrm{Embedding}(\hat {\boldsymbol{y}}^{< i})$,
Embedding is the embedding layer. Embedding layer consists of a $1\times 1$ convolutional layer and
a $3\times 3$ depth-wise convolutional layer. The $3\times 3$ depth-wise convolutional layer is
employed for learnable position embedding.
$\textrm{DepthRB}$ is the depth-wise residual bottleneck~\cite{jiang2023slic} and is employed to enhance the non-linearity.
Since the $\textrm{softmax}_1\left(\hat{\boldsymbol{y}}^{< i}_k\right)^{\top}\hat{\boldsymbol{y}}^{< i}_v$ is computed
during training and testing, the similarity metric is \textit{implicit} and the overall complexity is $O(L)$, which is linear with the resolution.
\begin{sidewaystable*}
  \centering
  \footnotesize
  \setlength{\tabcolsep}{1.5mm}{
  \begin{tabular}{@{}cccccccccccccc@{}}
  \toprule
  \multicolumn{1}{c|}{\multirow{2}{*}{Methods}}                            & \multicolumn{2}{c}{Kodak~\cite{kodak}}    & \multicolumn{2}{c}{Tecnick~\cite{tecnick2014TESTIMAGES}}  & \multicolumn{2}{c}{CLIC Professional Valid~\cite{CLIC2020}}  \\
  \multicolumn{1}{c|}{}                                                     & \multicolumn{1}{c}{PSNR} & \multicolumn{1}{c}{MS-SSIM}& \multicolumn{1}{c}{PSNR} & \multicolumn{1}{c}{MS-SSIM}& \multicolumn{1}{c}{PSNR} & \multicolumn{1}{c}{MS-SSIM} \\ \midrule
  \multicolumn{1}{c|}{VTM-17.0 Intra~\cite{bross2021vvc}}                                        & $0.00$       & $0.00$ & $0.00$       & $0.00$ & $0.00$       & $0.00$       \\\midrule
  \multicolumn{1}{c|}{Cheng'20 (CVPR'20)~\cite{cheng2020learned}}                                        & $+5.58$       & $-44.21$ & $+7.57$       & $-39.61$ & $+11.71$       & $-41.29$       \\\midrule
  \multicolumn{1}{c|}{Minnen'20 (ICIP'20)~\cite{minnen2020channel}}                                        & $+3.23$       & $-$ & $-0.88$       & $-$ & $-$       & $-$       \\\midrule
  \multicolumn{1}{c|}{Qian'21 (ICLR'21)~\cite{qian2020learning}}                                        & $+10.05$       & $-39.53$ & $+7.52$       & $-$ & $+0.02$       & $-$       \\\midrule
  \multicolumn{1}{c|}{Xie'21 (ACMMM'21)~\cite{xie2021enhanced}}                                        & $+1.55$       & $-43.39$ & $+3.21$       & $-$ & $+0.99$       & $-$       \\\midrule
  \multicolumn{1}{c|}{Guo'22 (TCSVT'22)~\cite{guo2021causal}}                                        & $-4.45$       & $-45.23$ & $-$       & $-$ & $-$       & $-$       \\\midrule
  \multicolumn{1}{c|}{LBHIC (TCSVT'22)~\cite{wu2021learned}}                                        & $-4.56$       & $-50.54$ & $-$       & $-$ & $-$       & $-$       \\\midrule
  \multicolumn{1}{c|}{Entroformer (ICLR'22)~\cite{qian2022entroformer}}                                        & $+4.73$       & $-42.64$ & $+2.31$       & $-$ & $-1.04$       & $-$       \\\midrule
  \multicolumn{1}{c|}{SwinT-Charm (ICLR'22)~\cite{zhu2022transformerbased}}                & $-1.73$      & $-$   & $-$& $-$& $-$& $-$  \\\midrule
  \multicolumn{1}{c|}{NeuralSyntax (CVPR'22)~\cite{wang2022neural}}                & $+8.97$      & $-39.56$   & $-$& $-$& $+5.64$& $-38.92$  \\\midrule
  \multicolumn{1}{c|}{McQuic (CVPR'22)~\cite{zhu2022unified}}                & $-1.57$      & $-47.94$   & $-$& $-$& $+6.82$& $-40.17$  \\\midrule
  \multicolumn{1}{c|}{STF (CVPR'22)~\cite{zou2022the}}         & $-2.48$      & $-47.72$  & $-2.75$      & $-$   & $+0.42$      & $-$      \\\midrule
  \multicolumn{1}{c|}{WACNN (CVPR'22)~\cite{zou2022the}}         & $-2.95$      & $-47.71$  & $-5.09$      & $-$   & $+0.04$      & $-$      \\\midrule
  \multicolumn{1}{c|}{ELIC (CVPR'22)~\cite{he2022elic}}                             & $-5.95$      & $-44.60$   & $-9.14$      & $-$ & $-3.45$      & $-$   \\\midrule
  \multicolumn{1}{c|}{Contextformer (ECCV'22)~\cite{koyuncu2022contextformer}}        & $-5.77$      & $-46.12$ & $-9.05$      & $-42.29$  & $-$      & $-$     \\\midrule
  \multicolumn{1}{c|}{Pan'22 (ECCV'22)~\cite{pan2022content}}        & $+7.56$      & $-36.20$ & $+3.97$      & $-$  & $-$      & $-$     \\\midrule
  \multicolumn{1}{c|}{NVTC (CVPR'23)~\cite{feng2023nvtc}}        & $-1.04$      & $-$ & $-$      & $-$  & $-3.61$      & $-$     \\\midrule
  \multicolumn{1}{c|}{LIC-TCM (CVPR'23)~\cite{liu2023learned}}        & $-10.14$      & $-48.94$ & $-11.47$      & $-$  & $-8.04$      & $-$     \\\midrule
  \multicolumn{1}{c|}{MLIC (ACMMM'23)~\cite{jiang2022mlic}}                                                    & $-8.05$      & $-49.13$ & $-12.73$      & $-47.26$ & $-8.79$      & $-45.79$    \\\midrule
  \multicolumn{1}{c|}{MLIC$^+$ (ACMMM'23)~\cite{jiang2022mlic}}                                                & $-11.39$      & $-52.75$    & $-16.38$      & $ -53.54$    & $-12.56$      & $-48.75$      \\\midrule
  \multicolumn{1}{c|}{MLIC$^{++}$ (Ours)}                                                & \bm{$-13.39$}      & \bm{$-53.63$}  & \bm{$-17.59$} &\bm{$-53.83$}& \bm{$-13.08$}& \bm{$-50.78$}   \\\midrule
  \end{tabular}}
  \caption{BD-Rate $(\%)$ comparison for PSNR (dB) and MS-SSIM. The anchor is VTM-17.0 Intra.}
  \label{tab:rd}
\end{sidewaystable*}
  % Figure environment removed
  \section{Experiments}
  \label{sec:exp}
  \subsection{Implementation Details}
  \label{sec:exp:setup}
  \subsubsection{Training Dataset}
  Our training datasets contains $10^5$ images\footnote[1]{\tt\url{https://github.com/JiangWeibeta/MLIC/blob/main/train_list.txt}}
  with resolutions exceeding $512\times 512$.
  These images are selected from ImageNet~\cite{deng2009imagenet},
  COCO 2017~\cite{lin2014microsoft}, DIV2K~\cite{Agustsson2017NTIRE2C}, and Flickr2K~\cite{lim2017enhanced}.
  To address the existing compression artifacts in JPEG images,
  we follow the approach of Ball{\'e} \textit{et al}~\cite{balle2018variational}
  by further down-sampling the JPEG images using a randomized factor.
  This downsampling process ensures that the minimum height or width of the images falls within the range of 512 to 584 pixels,
  effectively reducing the compression artifacts.
  \subsubsection{Training Strategy}
  MLIC$^{++}$ is built on Pytorch~\cite{paszke2019pytorch} and
  CompressAI~\cite{begaint2020compressai}.
  Following the settings of CompressAI~\cite{begaint2020compressai},
  we set $\lambda \in \{18, 35, 67, 130, 250, 483\} \times 10^{-4}$ for MSE
  and set $\lambda \in \{2.4, 4.58, 8.73, 16.64, 31.73, 60.5\}$ for Multi-Scale Structural Similarity (MS-SSIM)~\cite{wang2003multiscale}.
  The batch size is set to $32$ and models are trained on a single Tesla A100 GPU.
  Each model is trained with an Adam optimizer with $\beta_1=0.9, \beta_2=0.999$.
  We train each model for 2M steps.
The learning rate starts at $10^{-4}$ and drops to $3\times 10^{-5}$ at 1.5M steps,
drops to $10^{-5}$ at 1.8M steps, and drops to $3 \times 10^{-6}$ at 1.9M steps,
drops to $10^{-6}$ at 1.95M steps.
During training, we random crop images to $256\times 256$ patches during the
first $1.2$M steps. To further exploit the effectiveness of global context modules, images are cropped to $512\times 512$ patches
during the rest steps.
Large patches are beneficial for learning global references.
The latent representation can be sparse due to checkerboard partition.
A large latent representation makes it more difficult for the model to
capture the global contexts and improves the generalization ability of the model across different resolutions.
  % We train each model with an Adam optimizer~\cite{kingma2014adam} with
  % $\beta_1=0.9$, $\beta_2=0.999$ and the batch size is $16$.
  % We train each model for $2$M steps.
  % The learning rate starts at $10^{-4}$ and
  % drops to $3\times 10^{-5}$ at $1.6$M steps,
  % drops to $10^{-5}$ at $1.8$M steps, and drops
  % to $3\times 10^{-6}$ at $1.9$M steps, drops to $10^{-6}$ at $1.95$M steps.
  % During training, we randomly crop images to $256\times 256$ patches during the first $1.2$M steps,
  % and crop images to $512\times 512$ patches during the rest steps.
\subsection{Benchmarks and Metrics}
To thoroughly assess the generalization capability of the learned image compression models,
we conduct performance evaluations on three distinct datasets, including Kodak~\cite{kodak}, Tecnick~\cite{tecnick2014TESTIMAGES},
  CLIC Professional Valid~\cite{CLIC2020}.
  \begin{itemize}
  \item Kodak~\cite{kodak} is selected as a test set for almost
  all end-to-end image compression models~\cite{balle2016end,theis2017lossy, balle2018variational,minnen2018joint,cheng2020learned,minnen2020channel,chen2021nic,guo2021causal,wu2021learned,xie2021enhanced,gao2021neural,chen2022two,zou2022the,he2022elic,koyuncu2022contextformer,jiang2022mlic,duan2023lossy,liu2023learned}.
  It contains 24 raw $768\times 512$ images.
  \item Tecnick~\cite{tecnick2014TESTIMAGES} contains 100 $1200\times 1200$ RGB images,
  which is used as a test set in many methods~\cite{balle2018variational,minnen2018joint,xie2021enhanced,kim2022joint,jiang2022mlic,jiang2023slic,liu2023learned,zhu2022transformerbased,duan2023lossy,pan2022content}.
  \item CLIC Professional Valid~\cite{CLIC2020} is the validation set of 3rd Challenge on Learned Image Compression
  which contains $41$ images. Image in this dataset contains around $2048\times 1440$ pixels.
  CLIC Pro Valid is widely used in many recent methods~\cite{cheng2020learned,xie2021enhanced,he2022elic,zou2022the,liu2023learned,jiang2022mlic,wang2022neural,zhu2022unified,duan2023lossy}.
  % \item CLIC Pro Valid, CLIC 2021 Test, CLIC 2022 Test are from Challenge on Learned Image Compression.
  % Images in test datasets contains around $2560\times 1440$ pixels.
  % CLIC Pro Valid is widely used~\cite{cheng2020learned,xie2021enhanced,he2022elic,zou2022the,liu2023learned} and is the validation set of 3rd Challenge on Learned Image Compression
  % which contains $41$ images. CLIC 2021 Test and CLIC 2022 Test dataset are test datasets
  % of 4th and 5th Challenge on Learned Image Compression, which contain 60 and 30 images, respectively.
  % \item JPEGAI Test~\cite{jpegai}  is the test dataset of Learning-based Image Coding Challenge, MMSP 2020, which
  % contains $16$ images in resolution from $1336\times 872$ to $3680\times 2456$.
  \end{itemize}
  \par
  We use the Bjntegaard delta rate (BD-Rate)~\cite{bjontegaard2001calculation}
  to evaluate the performance of learned image compression models.
  \subsection{Rate-Distortion Performance}
  \label{sec:exp:perf}
  \subsubsection{Quantitive Results}
  Rate-distortion curves are presented in Fig.~\ref{fig:rd}.
%   The following commands are employed to obtain the rate-distortion
%   performance of VTM-17.0 Intra:
% \begin{lstlisting}
%   # Convert png image to yuv444 image
%   ffmpeg -i $PNGINPUT
%          -s $WIDTHx$HEIGHT
%          -pix_fmt yuv444p $YUVOUTPUT
%   # Encode
%   ./EncoderApp -c cfg/encoder_intra_vtm.cfg
%   -i $YUVOUTPUT -q $QP,
%   -o /dev/null -b $BINOUTPUT
%   --SourceWidth=$WIDTH
%   --SourceHeight=$HEIGHT
%   --FrameRate=1
%   --FramesToBeEncoded=1
%   --InputBitDepth=8
%   --InputChromaFormat=444
%   --ConformanceWindowMode=1
%   # Decode
%   ./DecoderApp -b $BINOUTPUT
%   -o $RECYUVOUTPUT -d 8
%   # Convert yuv444 image to png image
%   ffmpeg -s $WIDTHx$HEIGHT
%          -pix_fmt yuv444p
%          -i $RECYUVOUTPUT $RECPNGOUTPUT
% \end{lstlisting}
  When compared with Cheng'20~\cite{cheng2020learned},
  our proposed MLIC$^{++}$ can achieve a maximum
  improvement of $0.5\sim0.8$ dB in PSNR and achieve a maximum improvement of $0.6$ dB in MS-SSIM in dB.
  Our MLIC$^{++}$ adopts simplified analysis transform and synthesis transform
  of Cheng'20~\cite{cheng2020learned}, therefore, the improvement of
  model performance is attributed to our linear complexity multi-reference entropy
  modeling. Our linear complexity multi-reference entropy models can capture
  more contexts, which leads to much better rate-distortion performance.
  The improvement also proves correlations exist in multiple dimensions
  since Cheng'20~\cite{cheng2020learned} adopts an spatial auto-regressive context module.
  Compared with ELIC~\cite{he2022elic}, our MLIC$^{++}$ can be up to $0.4$ dB higher at low bit rates in PSNR and
  $1$ dB higher in MS-SSIM~\cite{wang2003multiscale}.\par
  BD-rate reductions are presented in Table~\ref{tab:rd}.
  When computing BD-rate, VTM-17.0 Intra under YUV444 is employed as anchor. Unofficial weights of ELIC\footnote[2]{\tt\url{https://github.com/VincentChandelier/ELiC-ReImplemetation}} are used to
  evaluate the rate-distortion performance of ELIC on Tecnick, CLIC Professional Valid.
  Our MLIC$^{++}$ outperforms our previous MLIC and MLIC$^+$~\cite{jiang2022mlic}.
  Our MLIC$^{++}$ reduces BD-rate by $13.39\%$ over VTM-17.0 Intra on Kodak while
  existing method ELIC only reduces $5.95\%$ BD-rate and LIC-TCM
  only reduces $10.14\%$ BD-rate on Kodak. Moreover, our MLIC$^{++}$ performs better
  on high-resolution datasets, such as Tecnick
  and CLIC Professional Valid. Our MLIC$^{++}$ reduces $17.59\%$ on Tecnick
  and reduces $13.08\%$ BD-rate on CLIC Professional Valid, which are much better than existing methods.
  Our MLIC$^{++}$ achieves state-of-the-art performance on all three datasets. The excellent performance of our MLIC$^{++}$ on these datasets
  also demonstrates the excellent generalization of our MLIC$^{++}$.
  We highlight the BD-rate for MS-SSIM, our proposed
  MLIC$^{++}$ reduces about $50\%$ bits compared to VTM-17.0 Intra, which is a
  large progress in learned image compression.
  \subsubsection{Qualitive Results}
  To further demonstrate superiority of our proposed MLIC$^{++}$, we compare our
  MLIC$^{++}$ with learned image compression models Cheng'20~\cite{cheng2020learned},
  Xie'21~\cite{xie2021enhanced}, STF~\cite{zou2022the}, WACNN~\cite{zou2022the},
  ELIC~\cite{he2022elic} and non-neural codec VTM-17.0 Intra~\cite{bross2021vvc} on
  perceptual quality. Fig.~\ref{fig:vis} presents the reconstructions of Kodim07 from Kodak.
  PSNR value of the image reconstructed by our MLIC$^{++}$ optimized for MSE is $1$dB
    higher than image reconstructed by VTM-17.0 Intra~\cite{bross2021vvc}.
    MS-SSIM of the image reconstructed by our MLIC$^{++}$
    optimized for MS-SSIM is $0.02$ higher than image reconstructed by VTM-17.0 Intra.
    The windowsill of reconstructions are cropped to patches for clearer comparisons.
    The reconstructions of MLIC$^{++}$ have sharper textures retain more details. In terms of visual quality, our MLIC$^{++}$
    have significant improvements on rate-perception performance compared to other models.
  % Figure environment removed
  \begin{table*}[!ht]
    \scriptsize
    \centering
    \setlength{\tabcolsep}{1.5mm}{
    \begin{tabular}{@{}cccccccccccccc@{}}
    \toprule
    \multicolumn{1}{c|}{\multirow{1}{*}{Context Modules}}   & \multicolumn{1}{c}{MLIC$^{++}$} & \multicolumn{1}{c}{Case 1}  & \multicolumn{1}{c}{Case 2}  & \multicolumn{1}{c}{Case 3} & \multicolumn{1}{c}{Case 4} & \multicolumn{1}{c}{Case 5}    & \multicolumn{1}{c}{Case 6}    & \multicolumn{1}{c}{Case 7}    & \multicolumn{1}{c}{Case 8}    \\\midrule
    \multicolumn{1}{c|}{Channel context module $g_{ch}$}                           & \CheckmarkBold                  & \CheckmarkBold    & \CheckmarkBold   & \CheckmarkBold            & \CheckmarkBold                & \CheckmarkBold                & \CheckmarkBold                & \CheckmarkBold                & \XSolidBrush        \\\midrule
    \multicolumn{1}{c|}{Checkerboard context module $g_{lc,ckbd}$}                      & \XSolidBrush                    & \XSolidBrush       & \XSolidBrush   & \XSolidBrush        & \XSolidBrush                  & \XSolidBrush                  & \CheckmarkBold                & \XSolidBrush                  & \XSolidBrush      \\\midrule
    \multicolumn{1}{c|}{Checkerboard attention context module $g_{lc,attn}$}                      & \CheckmarkBold                  & \CheckmarkBold    & \CheckmarkBold      & \CheckmarkBold          & \CheckmarkBold                & \CheckmarkBold                & \XSolidBrush                  & \XSolidBrush                  & \XSolidBrush        \\\midrule
    \multicolumn{1}{c|}{Linear intra-slice global spatial context module $g_{gc,intra}$}                     & \CheckmarkBold                  & \CheckmarkBold     & \XSolidBrush   &  \CheckmarkBold           & \CheckmarkBold                & \XSolidBrush                  & \XSolidBrush                  & \XSolidBrush                  & \XSolidBrush        \\\midrule
    \multicolumn{1}{c|}{Linear inter-slice global spatial context module $g_{gc,inter}$}                     & \CheckmarkBold                  & \XSolidBrush     & \CheckmarkBold    & \XSolidBrush             & \XSolidBrush                  & \XSolidBrush                  & \XSolidBrush                  & \XSolidBrush                  & \XSolidBrush        \\\midrule
    \multicolumn{1}{c|}{Position embedding}                 & \CheckmarkBold                  & \CheckmarkBold       & \CheckmarkBold        & \CheckmarkBold                  & \XSolidBrush                  & \XSolidBrush                  & \XSolidBrush                  & \XSolidBrush & \XSolidBrush        \\\midrule
    \multicolumn{1}{c|}{DepthRB}                 & \CheckmarkBold                  & \CheckmarkBold       & \CheckmarkBold        & \XSolidBrush   & \CheckmarkBold               & \XSolidBrush                  & \XSolidBrush                  & \XSolidBrush                  & \XSolidBrush        \\\midrule
    \multicolumn{1}{c}{}                                    & \bm{$0.00$}                            & \bm{$+3.15$}  & \bm{$+2.79$}   & \bm{$+4.37$}                     & \bm{$+4.10$}        & \bm{$+12.92$}                                    &    \bm{$+15.24$}                           &  \bm{$+17.45$}                    & \bm{$+22.34$}        \\\midrule
    \end{tabular}}
    \caption{Ablation Studies on Kodak. BD-rate (\%) is employed to evaluate their contributions. MLIC$^{++}$ is the anchor.}
    \label{tab:ablation}
  \end{table*}
  \subsection{Computational Complexity}
  The computational complexity of models is measured in four aspects,
  including test GPU memory consumption, encoding time, decoding time, and forward Multiply-Accumulate operations (MACs).
  These metrics provide a comprehensive evaluation of the complexity from
  various perspectives, with particular emphasis on the first three metrics
  due to their direct relevance to real-world scenarios.
  It is worth noting that a model with lower MACs may
  exhibit \textit{slower} encoding and decoding speeds if the context module is \textit{serial} and consume
  a larger amount of GPU memory. Therefore,
  while MACs serves as an important measure of complexity,
  it should be considered in conjunction with the other
  metrics to obtain a more complete understanding of the characteristics of the model.
  % We use the following commands to obtain the peak GPU memory, encoding and decoding time.
% \begin{minipage}{\linewidth}
%     \begin{verbatim}
% torch.cuda.reset_peak_memory_stats()
% torch.cuda.synchronize()
% enc_start_time = time.time()
% bits = codec.compress(input)
% torch.cuda.synchronize()
% enc_end_time = time.time()
% torch.cuda.synchronize()
% dec_start_time = time.time()
% recon = codec.decompress(bits)
% torch.cuda.synchronize()
% dec_end_time = time.time()
% mem = torch.cuda.max_memory_allocated()
% \end{verbatim}
% \end{minipage}
% \begin{lstlisting}
%   # input: input image
%   # bits: bit-stream
%   # recon: decoded image
%   # gpu_mem: peak GPU memory comsuption

%   torch.cuda.reset_peak_memory_stats()
%   torch.cuda.synchronize()
%   enc_start_time = time.time()
%   bits = codec.compress(input)

%   torch.cuda.synchronize()
%   enc_end_time = time.time()
%   torch.cuda.synchronize()
%   dec_start_time = time.time()
%   recon = codec.decompress(bits)
%   torch.cuda.synchronize()
%   dec_end_time = time.time()

%   gpu_mem = torch.cuda.max_memory_allocated()
% \end{lstlisting}
% Figure environment removed
  In order to better fit the real-world scenario,
  we test the model complexities in the case of different resolution images as inputs.
  We select $16$ images with resolution larger than $3584\times 3584$ from LIU4K test dataset~\cite{liu2020comprehensive} and we center crop
  these images to $\{512\times 512, 768\times 768, 1024\times 1024, 1536\times 1536,
  2048\times 2048, 2560\times 2560, 3072\times 3072, 3584\times 3584\}$ patches.
We compared our MLIC$^{++}$ with our prior work MLIC$^{+}$ which employs \textit{vanilla} attention to illustrate the advantages of
  proposed \textit{linear} complexity global context capturing. We also compare our proposed MLIC$^{++}$ with
  recent learned image compression models~\cite{cheng2020learned,qian2020learning,xie2021enhanced,qian2022entroformer,wang2022neural,zou2022the,he2022elic,liu2023learned}.
  The experiments are conducted on a Tesla A100 GPU and a Xeon(R) Platinum 8260C CPU.
  The results are illustrated in Fig.~\ref{fig:complex}.\par
  \subsubsection{On GPU Memory comsuption}
  The quadratic complexity of vanilla attention leads to significantly more memory consumptions
  on high resolution image coding. When compressing $2048\times 2048$ images,
  MLIC$^{+}$ consumes nearly $22.37$ GB GPU memory. Our proposed linear complexity global
  context capturing significantly reduces the consumption of GPU memory as our proposed
  MLIC$^{++}$ only takes about $5.55$ GB GPU memory to compress a $2048\times 2048$ image.
  When compressing $3072\times 3072$ images, MLIC$^{+}$ consumes $46.38$ GB GPU memory while our
  proposed MLIC$^{++}$ only consumes $7.69$ GB GPU memory.
  Compared with recent LIC-TCM~\cite{liu2023learned}, our MLIC$^{++}$ consumes
  $\frac{1}{2}$ of the GPU memory consumed by LIC-TCM when compressing
  $2048\times 2048$ images.
  When compressing $2560\times 2560$ images, our MLIC$^{++}$ only consumes about $\frac{1}{4}$
  of the GPU memory consumed by LIC-TCM.
  When compressing a $3584\times 3584$ image, the GPU memory consumption of our proposed
  MLIC$^{++}$ is still only $12.3$ GB while the LIC-TCM requires $45.95$ GB GPU memory.
  The curve of the GPU memory consumed by our MLIC$^{++}$ as the resolution grows is also much flatter.
  \subsubsection{On Encoding and Decoding Time}
  When counting encoding time and decoding time, entropy coding and
  entropy decoding time are included. Since our MLIC$^{++}$ does not employ
  pixel-cnn-like spatial context capturing, our
  MLIC$^{++}$ encodes and decodes much faster than Cheng'20,
  Xie'21, and Entroformer. The linear complexity global context capturing leads to significant computational overhead reductions
  on high resolution images when compared with quadratic complexity based global context capturing.
  Compared with vanilla-attention based method,
  MLIC$^{++}$ encodes, decodes faster than MLIC$^{+}$ on $\{768\times 768, 1024\times 1024, 1536\times 1536, 2048\times 2048, 2560\times 2560\, 3072\times 3072, 3584\times 3584\}$ images.
  The time of MLIC$^{++}$ to encode $2560\times 2560$ images is about $\frac{1}{2}$ of
  the time of MLIC$^{+}$. Compared with recent LIC-TCM, MLIC$^{++}$
  needs more time to decode low resolution images while needs less time to encode high-resolution
  images, which can be attributed to fewer slices in LIC-TCM.
  At smaller resolutions, the bottleneck of encoding or decoding time
  is the number of slices, since the encoding and decoding of slices is serial, however,
  at larger resolutions, the bottleneck is no longer the number of slices
  but the overall computational complexity due to the high computational
  overhead of each slice.
  \subsubsection{On Forward MACs}
  Compared with MLIC$^{+}$,
  our MLIC$^{++}$, which employs the proposed linear complexity global spatial context
  modules, has lower MACs.
  Compared with the recent LIC-TCM~\cite{liu2023learned}, our MLIC$^{++}$ demonstrates significantly reduced MACs.
  Compared with ELIC~\cite{he2022elic}, STF~\cite{zou2022the}, and
  WACNN~\cite{zou2022the}, our MLIC$^{++}$ has higher MACs when the input is high-resolution image.
  % One reason for the higher MACs is because the transform module of
  % our proposed MLIC$^{++}$ is based on the Cheng'20~\cite{cheng2020learned}.
  One contributing factor to the higher MACs is the utilization of Cheng'20~\cite{cheng2020learned}
  as the basis for the transform module in our proposed MLIC$^{++}$.
  While the context module in Cheng'20 is simpler than that of ELIC,
  STF, and WACNN, the overall MACs of Cheng'20 are
  higher due to its transform modules having higher MACs.
  Since our MLIC$^{++}$ employs a modification of analysis transform and synthesis
  transform of Cheng'20, it results in higher MACs compared to ELIC and WACNN.
  In addition, since our MLIC$^{++}$ employed an more advanced entropy model,
  which leads to higher MACs. However, the entropy model has modest effect on the overall MACs
  since the input image is down-sampled for four times in analysis transform,
  which implies that designing more advanced entropy models is more resource-efficient.\par
  % We argue that MACs are not as effective as GPU memory consumption and codec time
  \subsection{Ablation Studies}
  \label{sec:ablation}
  \subsubsection{Settings}
  % Figure environment removed
  We conduct corresponding ablation studies and evaluate the contribution
  of each module on Kodak~\cite{kodak} dataset.
  Each model is optimized for MSE. We train each model for 2M steps. We use the training strategy in Section~\ref{sec:exp:setup}.
  \subsubsection{Analysis of Channel-wise Context Module}
  The inclusion of a channel-wise context module yields a substantial performance improvement
  when compared to Case 8, which solely incorporates hyper-priors, Case 7,
  incorporating channel-wise context modules, achieves a further reduction of $4.89\%$ in bit-rate.
  The channel-wise context module has the capability to reference symbols in the same and nearby positions
  in the preceding slices. The effectiveness of the channel-wise context
  module provides evidence of redundancy among channels.
  \subsubsection{Analysis of Local Spatial Context Module}
  The vanilla checkerboard context module leads to slight performance degradation~\cite{he2021checkerboard} compared
  with pixel-cnn-like serial context modules~\cite{van2016conditional,minnen2018joint}.
  The vanilla checkerboard context module contains one convolutional layer which is linear.
  The other drawback is the fixed kernel weights during inference.
  In contrast, our proposed checkerboard attention-based local spatial context module
  is non-linear and incorporates dynamic attention map generation with two-pass decoding,
  allowing for improved flexibility and adaptability. Our
  proposed checkerboard attention-based local spatial context module achieves a further reduction of
  $2.32\%$ in bit-rate compared to vanilla checkerboard context module. Compared with Case 7,
  which solely incorporates channel-wise context modules,
  models incorporating both local spatial and channel context modules demonstrate superior performance,
  further validating the presence of redundancy in the local spatial domain.
  \subsubsection{Analysis of Intra-Slice Global Context Module}
  The $i$-th slice is used as an example to illustrate the process.
  In practice, the computation of $\textrm{softmax}_1\left(\hat{\boldsymbol{y}}^{i-1}_{ac,k}\right)^{\top}\hat{\boldsymbol{y}}^i_{ac,v}$ in
  Equation~\ref{eq:efficient} is
  performed first for \textit{linear} complexity. However, we can still compute
  $\textrm{softmax}_2\left(\hat{\boldsymbol{y}}^{i}_{na,q}\right)\left[j\right]\times \textrm{softmax}_1\left(\hat{\boldsymbol{y}}^{i}_{ac,k}\right)^\top$
  as the attention map to validate the ability to capture global dependencies since
  the $\textrm{softmax}_2\left(\hat{\boldsymbol{y}}^{i}_{na,q}\right)\left[j\right]\times \textrm{softmax}_1\left(\hat{\boldsymbol{y}}^{i}_{ac,k}\right)^\top$
  is employed as the \textit{implicit} similarity metric, where $j$ is the index of selected query.
  The attention maps of Kodim01, Kodim11, Kodim 20, Kodim21 captured by proposed intra-slice global spatial context module $g_{gc,intra}$  is illustrated in Fig.~\ref{fig:intra_attn_map}.
  The checkerboard-like pattern in the attention map arises due to the absence of interactions within the anchor and non-anchor parts.
  Our model successfully captures distant correlations between the anchor and non-anchor parts,
  which local context modules are unable to achieve.
  Although our intra-slice global context module may bear some resemblance to cross-attention models,
  we focus solely on the interactions within a single slice.
  We only use the attention map of $\hat {\boldsymbol{y}}^{i-1}$ to predict correlations in $\hat {\boldsymbol{y}}^i$.
  When our proposed global context modules collaborate with local spatial context modules,
  the overall performance is further improved, underscoring the necessity of
  global spatial context modules for capturing global correlations
  and local spatial context modules for capturing local correlations.
  \subsubsection{Analysis of Inter-Slice Global Context Module}
  Taken $i$-th slice as an example,in linear complexity inter-slice global context module $g_{gc,inter}$
  computes $\textrm{softmax}_1\left(\hat{\boldsymbol{y}}^{i}_k\right)^\top\times \hat{\boldsymbol{y}}^{i}_v$ first.
  However, the $\textrm{softmax}_2\left(\hat{\boldsymbol{y}}^{i}_q\right)\left[j\right]\times \textrm{softmax}_1\left(\hat{\boldsymbol{y}}^{i}_k\right)^\top$
  can be computed as the attention map to validate the ability to capture inter-slice global dependencies, where $j$
  is the index of the selected query.
  The attention maps of Kodim01, Kodim11, Kodim20, Kodim21 captured by our inter-slice are illustrated in Fig.~\ref{fig:inter_attn_map}.
  The visualized attention maps clearly demonstrate the effective capture of global dependencies by our $g_{gc,inter}$,
  despite the model being trained in an \textit{implicit} manner.
  When $g_{gc, inter}$ collaborates with the intra-slice global context module $g_{gc,intra}$,
  teh local spatial context module, and the channel-wise context module, the rate-distortion
  performance of the model is further enhanced, which demonstrate the effectiveness
  of inter-slice global context module.
  % Figure environment removed
    \begin{table}[t]
      \centering
      \tiny
      \setlength{\tabcolsep}{4mm}{
      \begin{tabular}{@{}cccccccccccccc@{}}
      \toprule
      \multicolumn{1}{c|}{\multirow{1}{*}{Patch Size}}   & \multicolumn{1}{c}{$256\times 256$} & \multicolumn{1}{c}{$320\times 320$}   & \multicolumn{1}{c}{$512\times 512$}       \\\midrule
      \multicolumn{1}{c|}{}                                    & \bm{$0.00$}                            & \bm{$-3.66$}                               & \bm{$-6.12$}                                    \\\midrule
      \end{tabular}}
      \caption{BD-RATE(\%) of training using different patch size. The anchor is MLIC$^{++}$ trained on $256\times 256$ patches.}
      \label{tab:patch_size}
    \end{table}
    \begin{table}[t]
      \centering
      \tiny
      \setlength{\tabcolsep}{2.6mm}{
      \begin{tabular}{@{}cccccccccccccc@{}}
      \toprule
      \multicolumn{1}{c|}{\multirow{1}{*}{Context Module}}   & \multicolumn{1}{c}{$g_{ch}$} & \multicolumn{1}{c}{$g_{lc,attn}$}   & \multicolumn{1}{c}{$g_{gc,intra}$}   & \multicolumn{1}{c}{$g_{gc,inter}$}    \\\midrule
      \multicolumn{1}{c|}{KParams}                                    & \bm{$5810.11$}                            & \bm{$755.2$}                               & \bm{$732.5$}              &    \bm{$4633.34$}                     \\\midrule
      \multicolumn{1}{c|}{MMACs}                                    & \bm{$8925.18$}                            & \bm{$1148.2$}                               & \bm{$1116.2$}              &         \bm{$7100.50$}                \\\midrule
    \end{tabular}}
    \caption{Parameters and Forward Macs of context modules on Kodak.}
      \label{tab:context_complex}
    \end{table}
  \subsubsection{Analysis on Learnable Position Embedding and DepthRB}
  The position embedding and DepthRB lead to performance gains as illustrated in Table~\ref{tab:ablation}.
  Specifically, when DepthRB is not employed, a FFN is utilized instead. 
  The learnable position embedding is flexible because it is data-driven.
  Other position embedding method, Sinusoidal Position Embedding~\cite{vas2017attention},
  is tried and it leads negligible performance difference compared to model
  without position embedding. Relative Position Embedding~\cite{shaw2018self} is employed on
  attention map, which cannot be employed in our approach because
  the $\textrm{softmax}_1\left(\hat{\boldsymbol{y}}^{i-1}_{ac,k}\right)^{\top}\hat{\boldsymbol{y}}^{i}_{ac,v}$
  and $\textrm{softmax}_1\left(\hat{\boldsymbol{y}}^{< i}_k\right)^{\top}\hat{\boldsymbol{y}}^{< i}_v$ are
  computed first in our approach for linear complexity instead of the attention map
  $\textrm{softmax}_2\left(\hat{\boldsymbol{y}}^{i}_{na,q}\right)\left[j\right]\times \textrm{softmax}_1\left(\hat{\boldsymbol{y}}^{i}_{ac,k}\right)^\top$
  and $\textrm{softmax}_2\left(\hat{\boldsymbol{y}}^{i}_q\right)\left[j\right]\times \textrm{softmax}_1\left(\hat{\boldsymbol{y}}^{i}_k\right)^\top$.
  The learnable position embedding is not employed in our checkerboard attention-based local spatial context module $g_{lc,attn}$,
  because the performance improvement is quite negligible.
  In our $g_{lc,attn}$, the feature is partition into overlapped windows with zero padding.
  The zero padding and boundary effects imply that there is no need to insert a position embedding layer.
  \subsubsection{Comparisons with Model with Vanilla Global Spatial Context Modules}
  The comparison between MLIC$^{++}$ and MLIC$^{++}$ with vanilla quadratic complexity global spatial context modules is
  illustrated in Fig.~\ref{fig:vanilla}. It is evident that modeling global spatial contexts using linear complexity attention
  mechanisms does not result in performance degradation when compared to the vanilla attention mechanism.
  \subsubsection{Analysis of Training with Large Patches}
  In our training strategy, we use $512\times 512$ patches to train MLIC$^{++}$
  during the rest $0.8$M steps.
  We compare the differences in rate-distortion performance between different patch sizes
  $\{256\times 256, 320\times 320, 512\times 512\}$
  during the rest $0.8$M steps in Table~\ref{tab:patch_size}.
  Using large patches further boost the model performance.
  Using $256\times 256$ patches cannot fully exploit the performance of the model.
  The size of latent representation is $16\times 16$ if $256\times 256$ patches
  are adopted. $16\times 16$ latent representation is insufficient for model to learn
  long-range or global dependency as the resolutions of input images of the codec could be 2K or 4K.
  Moreover, checkerboard partition~\cite{he2021checkerboard} is employed, which
  makes the attention map sparse. Therefore, it is required to adopt large patches for better performance.
  Considering the overhead of training and model performance,
  adopting $512\times512$ patches is the best choice.
  \subsubsection{Comparisons among Different Context Modules}
  As illustrated in Table~\ref{tab:ablation}, the proposed global context modules
  $g_{gc,intra}$ and $g_{gc,inter}$ contribute most to performance,
  channel-wise context module $g_{ch}$ has the second highest contribution to performance, and
  local sptail context module $g_{lc,attn }$ has the lowest contribution to performance.
  Since each context module has a different role to perform, it is imperative that they work together
  for performance enhancement. The complexity of each context module is presented
  in Table~\ref{tab:context_complex}. The channel-wise context module
  has the most parameters and highest MACs.
  However, the MACs of the channel context module are only $1.77\%$ of the total MACs.
  The total MACs of all context modules are only $3.63\%$ of the total MACs.
  \section{Conclusion}
  \label{sec:conclusion}
  In this paper, we propose a novel approach for capturing local spatial context using checkerboard attention,
  as well as linear complexity intra-slice and inter-slice global context modules,
  which significantly enhance the performance of the model while maintaining an acceptable \textit{linear} complexity.
  Based on proposed context modules, we propose linear complexity
  multi-reference entropy model MEM$^{++}$.
  Building upon MEM$^{++}$, we obtain state-of-the-art model MLIC$^{++}$.
  MLIC$^{++}$ exhibits \textit{linear} GPU memory consumption with resolution, 
  making it highly suitable for high-resolution image coding.
  % One drawback of MLIC$^{++}$ is observed in its transform modules,
  % which result in a slight degradation in performance when the bit-rate on the Kodak dataset $\geq 1$,
  % as also reported in~\cite{xu2022multi}. This drawback is easy to be fixed by
  % adopting more advanced transform modules~\cite{zou2022the,he2022elic,jiang2023slic,liu2023learned}.
  To make our MLIC$^{++}$ more practical,
  our future work will focus on investigating the asymmetrical design~\cite{yang2023asymmetrically}
  between the analysis and synthesis transforms,
  as well as lighter linear complexity multi-reference entropy model.
\nocite{langley00}

\bibliography{mlicpp}
\bibliographystyle{icml2023}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one, even using the one-column format.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
