
{\section{Introduction}

Graph-structured data has become increasingly prevalent in today's data-driven landscape, particularly in applications involving social networks, biological systems, or recommendation engines. Graph Neural Networks (GNNs) have emerged as powerful tools for analyzing and learning from such data, offering remarkable performance in various tasks. These advantages both in efficiency and utility unfortunately come with a high cost in terms of privacy since the underlying graph structure is usually considered as sensitive information. For example, in a social network, the link between nodes usually represents whether users share common interests, beliefs, political views or even sexual preferences. This unfortunately leaks important information about users and can cause serious damage to their privacy \cite{camb_analytica}.
In this paper, our primary objective is to advance the understanding of edge privacy in GNNs by first developing a novel link-stealing attack, named Node Injection Link Stealing (NILS) attack, and then proposing a tailored Differential Privacy notion to protect against this attack. In this context, the adversary aims to infer the links among a set of target nodes. We focus on a specific scenario: training the GNN model for node classification tasks. In this scenario, the GNN processes input data consisting of the graph structure and node features, subsequently producing predictions that indicate the class membership of each node. This model will be served at inference time through an inference API.

Early works, such as the Linkteller attack \cite{linkteller}, demonstrated that by probing the features of nodes and analyzing the output predictions generated by the GNN, an attacker could successfully infer the links of the graph. Another study in ~\cite{he2021stealing} focused on the correlation of nodes' features in order to infer the links between them.
Unlike those previous works, we propose a stronger adversary who takes advantage of the dynamic nature of GNNs. The adversary adds a new node to the graph, connects it to the target node through a single edge/link} and further queries the model with some malicious input features that are generated following different strategies. Establishing this new connection allows the attacker to infer some of the target node's neighbors and hence steal a subset of the graph's connections. The various adversary strategies mainly differ with respect to the actual values of the input feature vectors.

Similar to previous work \cite{linkteller}, we further study potential defense strategies mainly based on differential privacy (DP). More specifically, we first propose a dedicated privacy notion specifically crafted to counter the proposed adversary, and evaluate existing DP-based defense strategies under this privacy definition, which we call \textbf{one-node-one-edge privacy}.

To summarize, we make the following contributions:
%\begin{enumerate}
{\begin{itemize}
  \item We propose a novel attack NILS for inferring private links in a graph structure by injecting a new node, linking it to a target node, and employing various attack strategies to analyze the changes in the GNN's output;
  \item We provide a comprehensive evaluation of the proposed attack's effectiveness on various datasets, demonstrating its superior performance compared to existing work such as LinkTeller \cite{linkteller} and link-stealing \cite{he2021stealing};
  \item We explore the application of DP mechanisms as a means to mitigate the effectiveness of our proposed attack, evaluating the trade-off between privacy preservation and model utility. To this end, we introduce a new notion of privacy and evaluate defense strategies under this new notion.
%\end{enumerate}
\end{itemize}

