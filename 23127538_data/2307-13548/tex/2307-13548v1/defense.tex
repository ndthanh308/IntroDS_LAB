\section{Defense\label{sec:defense}}

This section introduces the basic notions of DP in the context of GNNs. As a reminder, the goal is to protect the privacy of the graph, in the sense of preventing an adversary from discovering whether, in a given graph, there is a link between two nodes. With this aim, we need to define the neighbouring relation of graphs and further revise the definition of DP.

\subsection{DP for graphs}
Recall from Sec.~\ref{sec:background:DP} that the notion of neighborhood of DP was defined originally for microdata, and, %\ayse{omit}that
accordingly, two databases are said to be neighbors if they differ just in one record.
In the context of graphs at hand, however, this notion must be adapted since two graphs may differ with respect to either one edge or one node. 


In the literature, we find two attempts~\cite{kossinets2006empirical,hay2009accurate} to adapt DP to graphs. %, taking into account these two possibilities. %depending on whether neighboring graphs differ by at most one edge or one node. 
Before examining them, recall from Sec.~\ref{sec:GNNsOverview} that a graph $\mathcal{G} =(V,E)$ is represented with an adjacency matrix $A$, whereby $A_{ij}=1$ if there is a link between node $i$ and node $j$, and $A_{ij} = 0$ otherwise (where $i,j \in \{1,\ldots,|V|\}$). 

\begin{definition}[\textbf{Edge-level adjacent graphs} \cite{kossinets2006empirical}]
$\mathcal{G}$ and $\mathcal{G'}$ are considered \emph{edge-level adjacent graphs} if one can be obtained from the other by removing a single edge. In other words, $\mathcal{G}$ and $\mathcal{G'}$ differ by at most one edge. Hence, their adjacency matrices differ by one element only.
\end{definition}
Accordingly, an edge-level DP mechanism is defined as follows:
\begin{definition}[\textbf{$(\varepsilon, \delta)$-Edge-level differential privacy}]
	\label{def:edgedp}
	A randomized mechanism $\mathcal{M}$ satisfies \textbf{$(\varepsilon,\delta)$-edge-level DP} with $\varepsilon,\delta \geqslant 0$ if, for all pairs of
	edge-level adjacent graphs $\mathcal{G},\mathcal{G}'$ and for all measurable $\mathcal{O}\subseteq \Range(\M)$,
	
	$$\oP\{\mathcal{M}(\mathcal{G})\in \mathcal{O}\} \leqslant e^{\varepsilon} \oP\{\mathcal{M}(\mathcal{G}')\in \mathcal{O}\} + \delta.$$
\end{definition}


%\subsubsection{Node-level differential privacy for graphs}
\begin{definition}[\textbf{Node-level adjacent graphs} \cite{hay2009accurate}]
\label{def:node-level}
$\mathcal{G}$ and $\mathcal{G'}$ are said to be \emph{node-level adjacent graphs} if one can be obtained from the other by removing a single node and all of its incident edges.
\end{definition}
Node-level DP is defined analogously as follows:
\begin{definition}[\textbf{$(\varepsilon, \delta)$-Node-level differential privacy}]
	\label{def:nodedp}
	A randomized mechanism $\mathcal{M}$ satisfies \textbf{$(\varepsilon,\delta)$-node-level DP} with $\varepsilon,\delta \geqslant 0$ if, for all pairs of node-level adjacent graphs $\mathcal{G},\mathcal{G}'$ and for all measurable $\mathcal{O}\subseteq\Range(\mathcal{M})$, the following inequality holds:
	
	$$\oP\{\mathcal{M}(\mathcal{G})\in \mathcal{O}\} \leqslant e^{\varepsilon} \oP\{\mathcal{M}(\mathcal{G}')\in \mathcal{O}\} + \delta$$
\end{definition}


\subsection{One-node-one-edge-level DP}
The adversary defined in Sec.~\ref{sec:node-injection-attack} adds a malicious node to a graph and connects it to a target node through a \emph{single} edge. Countering such an adversary with a node-level DP mechanism (see Definition~\ref{def:node-level}) is clearly not a suitable choice in terms of model accuracy since node-level DP targets a stronger adversary. Trying to hide the presence or absence of one node and \emph{all} of its incident edges intuitively would increase the scale of the noise to be added (for example to the original adjacency matrix), and incur more data inaccuracy than necessary.
Motivated by this, we define a new notion of neighboring graphs (and the corresponding DP mechanism), which is designed to specifically counter the adversary proposed in this work. 
\begin{definition}[\textbf{One-node-one-edge-level adjacent graphs}]
\label{def:1n1e-graphs}
$\mathcal{G}$ and $\mathcal{G'}$ are considered \emph{one-node-one-edge-level adjacent graphs} if one can be obtained from the other by adding a single node with one edge only.
\end{definition}
Note that, as in node-level adjacent graphs (Definition~\ref{def:node-level}), the adjacency matrices of two neighboring graphs (in the sense of one-node-one-edge) differs by one row and one column only, but unlike node-level, the difference in $L_1-$norm between the adjacency matrices is always one.
Based on Definition~\ref{def:1n1e-graphs}, a one-node-one-edge-level DP is defined as follows:
\begin{definition}[\textbf{$(\varepsilon, \delta)$-One-node-one-edge-level differential privacy}]
	\label{def:nodedp}
	A randomized mechanism $\mathcal{M}$ satisfies \textbf{$(\varepsilon,\delta)$-one-node-one-edge-level DP} with $\varepsilon,\delta \geqslant 0$ if, for all pairs of one-node-one-edge-level adjacent graphs $\mathcal{G},\mathcal{G}'$ and for all measurable $\mathcal{O}\subseteq \Range(\mathcal{M})$,
the following holds:
	$$\oP\{\mathcal{M}(\mathcal{G})\in \mathcal{O}\} \leqslant e^{\varepsilon} \oP\{\mathcal{M}(\mathcal{G}')\in \mathcal{O}\} + \delta$$
\end{definition}


\subsection{Countermeasures for our attack}


In this section, we describe one DP-based strategy, namely the LapGraph mechanism, which was introduced in \cite{linkteller}. A much simpler defense approach against any privacy attack to GNNs would of course be output perturbation~\cite{outputperturbation}, whereby the very same output of the GNN prediction is perturbed with some DP mechanism (e.g., the classical Laplace mechanism). While this solution is straightforward to implement and indeed can be used to satisfy the one-node-one-edge-level DP notion, unfortunately, it would significantly deteriorate the accuracy of the GNN output. 
It is easy to see that the $L_1$-global sensitivity of a prediction matrix for the set of nodes $V_{\mathcal{A}}$ is as large as $2\,|V_{\mathcal{A}}|$, which makes us rule out output perturbation. 

To defend against the newly proposed attack, similar to \cite{linkteller}, we propose to apply the LapGraph algorithm, which consists in perturbing the adjacency matrix using the Laplace mechanism and binarizing it by replacing the top-$N$ largest values by 1 and the remaining values by 0. Here, $N$ represents the estimated number of edges in the graph, which is also computed using the Laplace mechanism. % \ayse{omit}in a DP manner. 

By leveraging the post-processing property of DP\footnote{The post-processing of DP allows arbitrary data-independent transformations to DP outputs without affecting their privacy guarantee~\cite{postprocessing}.}, the edge information remains protected even if the adversary observes the predictions generated by the GNN. Furthermore, 
 
each time a user connects a new node, a new adjacency matrix is generated following the same LapGraph mechanism, accumulating this way the privacy budget by the sequential composition property of DP \cite{dwork2014algorithmic}.

Although the LapGraph mechanism was proposed to meet edge-level DP, it is not difficult to show that the mechanism can also be used to satisfy one-node-one-edge-level DP. 
For this, let $f_A$ be the query function returning the adjacency matrix of a graph $G$. Unlike edge-level neighborhood, the corresponding matrices $A, A'$ of two one-node-one-edge neighboring graphs $G,G'$ have different dimensions, namely, %\ayse{this is not a sentence, should be attached to the previous one}
either $A\in \mathbb{R}^{n \times n}$ and $A'\in \mathbb{R}^{(n+1) \times (n+1)}$, or $A\in \mathbb{R}^{(n+1) \times (n+1)}$ and $A'\in \mathbb{R}^{n \times n}$. Without loss of generality, we assume the former case, where $A$ and $A'$ represent the adjacency matrices \emph{before} and \emph{after} the new node is connected to $G$ (resulting in $G'$). We shall also assume that the new node corresponds to the $(n+1)$-th row and, for symmetry, to the $(n+1)$-th column of $A'$. Precisely, since any adjacency matrix is symmetric by definition, the computation of the sensitivity of $f_A$ only requires the upper or lower triangular matrix of $A$.

To enable the subtraction operation $A-A'$ implicit in the definition of the global sensitivity (see Definition~\ref{def:GS}), we append one zero-row and one-zero column to $A$ and denote the resulting matrix by $\bar{A}\in \mathbb{R}^{(n+1) \times (n+1)}$.
As in the case of $A'$, we assume that the appended row and column are in the %\ayse{parantheses}
$(n+1)$-th position of $\bar{A}$.

To compute the sensitivity of $f_A$ for the notion of one-node-one-edge neighboring graphs, we just need to note that the $(n+1)$-th columns (or rows, if we consider the lower triangular of the adjacency matrix) of $\bar{A}$ and $A'$ always differ in one element. The reason is because one-node-one-edge neighboring graphs differ in only one edge. As a result,
$$\|\bar{A}-A'\|_1=1$$
for any pair of neighboring graphs,
which yields an $L_1$-global sensitivity of 1, as in the original LapGraph mechanism intended for edge-level DP. The fact that the two sensitivities coincide implies that the LapGraph version utilized in this work will provide stronger protection for the same level of utility, compared to the original LapGraph.
The reason for that is because, while the scale of the Laplace noise will be the same for a same $\varepsilon$, one-node-one-edge guarantees indistinguishability between any pair of graphs differing not only in one edge and but also in one node. 
% Figure environment removed


\subsection{LapGraph evaluation}
In this section, we evaluate the effectiveness of LapGraph \cite{linkteller} in reducing the success of  NILS attack while ensuring our one-node-one-edge-level DP notion. We also investigate the utility of GCN models trained with LapGraph protection.

\subsubsection{Evaluation setup:} We use the same training hyperparameters and normalization techniques as in the vanilla case, where DP is not applied. Initially, we protect the training graph with LapGraph. Following that, we apply LapGraph each time the graph changes due to node injection by the adversary.
In line with the setup in \cite{linkteller}, we compute the $F_1$ score for our NILS attack as well as the classification task's $F_1$ score for the GCN. This allows us to measure LapGraph protection along with the GCN utility across various privacy budgets $\varepsilon$. We report the results averaged over 5 runs with different random seeds for LapGraph.

\subsubsection{Evaluation results:} Figure \ref{fig:lapgraph_effectiveness} presents the $F_1$ score of the attack for various $\varepsilon$ values. We observe that applying LapGraph reduces the effectiveness of NILS. The $F_1$ score becomes almost zero when the privacy budget $\varepsilon$ is small. However, for large $\varepsilon$, LapGraph provides moderate protection, but the attack's $F_1$ score remains significantly lower than in the non-private case where DP is not applied.

For comparison, in the LinkTeller \cite{linkteller} attack, where LapGraph is applied only once to ensure edge-level DP, LapGraph offers limited protection when $\varepsilon$ is large, allowing LinkTeller to achieve a success rate nearly as high in the non-private case. Conversely, in our scenario, where LapGraph is also applied after the adversary's node injection, LapGraph provides stronger protection. The application of LapGraph during inference makes it more challenging for the adversary to distinguish between the target node's neighbors and non-neighbors, as the prediction scores of all target nodes change after each inference query.
Consequently, the distances between the prediction scores $P$ and $P'$, before and after the node injection, become noisier due to LapGraph's application following the node injection.

To provide insights about the privacy-utility tradeoff of LapGraph, we present in Figure \ref{fig:lapgraph_utility} the utility of the GCNs for different values of the privacy budget. We observe that the utility increases when $\varepsilon$ increases, as expected. Large values of $\varepsilon \geq 7$ give a better utility close to that in the non-private vanilla case. Therefore, carefully choosing an $\varepsilon$ will give fairly good utility and a certain level of protection against NILS attack.



% Figure environment removed

