\section{Node Injection Link Stealing Attack\label{sec:attack}}

GNNs are prone to various privacy attacks that usually aim at learning as much information as possible about their underlying graph structure. GNNs inherit the potential attacks against standard neural networks such as membership inference attacks \cite{MIA_GNN, MIA_Jiayuan}, whereby the goal of the adversary is to ascertain whether a sample is included in the training dataset or not.

As introduced earlier, in this paper, we focus on a particular attack named as \textit{link stealing attack}, where an adversary without access to the adjacency matrix aims to learn whether a particular edge exists or not.

In this section, we first introduce the threat model to characterize the adversary's background knowledge. Then, we propose our node injection link stealing attack that takes advantage of the dynamicity of GNNs.


\subsection{Threat model}
\subsubsection{Environment}
As mentioned in the previous section, we consider a GNN application in which a server has already trained the GNN using a specific dataset and offers access to this GNN through a black-box API. In this context, the black-box API is an interface provided by the server that enables users to interact with the pre-trained GNN model without directly accessing its internal components, such as the model architecture, parameters, or graph structure.
Users can submit prediction queries using node IDs. If a new node needs to be added to the graph, users can employ a \textit{connect} query to attach the node to the graph before querying its prediction based on its ID.
The API processes input data into output predictions, ensuring that the model's underlying computations remain hidden from the user. Users can query this GNN for the purpose of node classification. Hence, the query consists of the queried node's ID and the output of this query is the vector of prediction scores for this particular node. The users do not have the knowledge of edges of this graph. Hence the only information that a user knows is the set of nodes' ids.

\subsubsection{Adversary's goal and knowledge}

We consider an adversary, $\mathcal{A}$, who assumes the role of a GNN user. Her objective is to determine the neighbors of a specific \textit{target node}, $v_t$, selected from a set of \textit{target nodes}, $V_{\mathcal{A}}$, within the graph. This is done based on the GNN's predictions for the node set $V_{\mathcal{A}}$. In simpler terms, $\mathcal{A}$ aims to identify the neighbors of the target node $v_t$ that are included in the target set nodes $V_{\mathcal{A}}$.

We should note that if the adversary aims to identify all the links within the graph, then the set of target nodes $V_{\mathcal{A}}$ becomes the set containing all the nodes of the graph $V$. To achieve this, the adversary may need to perform multiple node injections, targeting different nodes from the graph each time. However, the practicality of such an approach is debatable. The adversary's selection of target nodes reflects her background knowledge about these nodes. For instance, in the context of social networks, the adversary's background knowledge could include information such as users' interests. This information can guide the adversary in selecting target nodes $V_{\mathcal{A}}$ that are more likely to be connected. In our attack scenario, we choose the target nodes uniformly at random.

The adversary $\mathcal{A}$ is able to obtain the predictions of the target nodes $V_{\mathcal{A}}$ by sending the server their corresponding IDs through the provided API.
In addition, the adversary $\mathcal{A}$ is able to use the \textit{connect} query to connect a node $v_m$ to a target node $v_t$. In general, we assume that the adversary does not have access to the features of the nodes in the graph, with the exception of certain attack strategies described in Sec.~\ref{subsection:malicious features strategies}.

\subsection{Node injection link stealing attack}
\label{sec:node-injection-attack}
In this section, we formally define our NILS attack that, unlike existing link-stealing attacks, exploits the dynamic nature of the underlying GNN. Indeed, adversary $\mathcal{A}$ can \textit{connect} new nodes and further query the prediction scores of a set of nodes $V_{\mathcal{A}}$ in the graph. While adding this new node $v_m$, $\mathcal{A}$ can choose which existing node $v_t$ it actually connects to and hence try to discover its neighbors. More formally:

\begin{enumerate}
\item $\mathcal{A}$ first queries the prediction scores of the target nodes $V_{\mathcal{A}}$
and receives the corresponding prediction matrix $P$ of the target nodes $V_{\mathcal{A}}$.
    \item $\mathcal{A}$ generates malicious features of a malicious node $v_m$ based on the obtained prediction matrix $P$ (see Sec.~\ref{sec:strategies} for further details on this step).
    \item Next, $\mathcal{A}$ sends a \emph{connect} query to inject the malicious node $v_m$. The query has the following parameters: the features $x_m$ of the new node, and the ID of the target node $v_t$ the adversary wishes to connect $v_m$ to.
    \item The server adds this \text{malicious} node $v_m$ to the graph and links it to the target node $v_t$.
    \item $\mathcal{A}$ queries back the server for new prediction matrix $P'$ of the target nodes $V_{\mathcal{A}}$ and obtains it.
    \item With access to $P$ and $P'$, $\mathcal{A}$ computes the $L_1$ distance between $P(v)$ and $P'(v)$ of each node $v$ in $V_{\mathcal{A}}$.
    % Next, $\mathcal{A}$ can infer the actual links of $v_t$ by computing the $L_1$ distance between the prediction scores $P$ and $P'$ of each node $v$  in $V_{\mathcal{A}}$ before and after the injection.
    A significant change in the prediction scores of a node $v$ indicates a high probability of being a neighbor to $v_t$. If the difference exceeds a threshold $R$, the adversary infers that node $v$ is a neighbor of $v_t$.
    % Indeed, there is a high chance that the prediction scores of $v_t$'s neighbors have significantly been modified. Hence, for a given node, if this difference is large enough, there is a high chance that this particular node $v$ is a neighbor of $v_t$.
\end{enumerate}

The decision threshold $R$ is determined through an extensive parameter tuning process, aiming for an optimal trade-off between precision and recall in identifying the true neighbors of the target node. This balance is represented by the $F_1$ score. We evaluate various candidate values of $R$, selecting the one that yields the highest $F_1$ score as the optimal threshold. The results reported in our study are based on this optimal value of $R$.

 This attack strategy is depicted in Figure \ref{fig:attack_strategy} and outlined in Algorithm \ref{alg:prob_vecs_attack}.
 
% Figure environment removed

\begin{algorithm}
\caption{Node Injection Link Stealing Attack}
\label{alg:prob_vecs_attack}
\textbf{Input:} set of nodes $V_{\mathcal{A}}$ and target node $v_t$. \\
\textbf{Output:} the identified neighbors of $v_t$ by the adversary.\\

$P$ = GNN($V_{\mathcal{A}}$, $X_{V_{\mathcal{A}}}$) \Comment{Step 1}\\
Generate malicious features $x_m$ of node $v_m$ \Comment{Step 2}\\
Connect node $v_m$ to $v_t$. \Comment{Step 3-4}\\
$P'$ = GNN($V_{\mathcal{A}} \cup v_m$ , $X_{V_{\mathcal{A}}} \cup x_m$) \Comment{Step 5}\\
\For{each node $v$ in $V_{\mathcal{A}}$}{ 
$D(v)$ = $\lVert P(v) - P'(v) \rVert_1$ \Comment{Step 6} \\ 
\If{$D(v) \geq R$}{
      $v$ is a neighbor of $v_t$ \\}
\Else{
      $v$ is not a neighbor of $v_t$ \\}
}
\end{algorithm}

 \subsection{Strategies for malicious node's features\label{subsection:malicious features strategies}}
 \label{sec:strategies}
In order to evaluate how the injection of the malicious node $v_m$ influences the predictions of the GNN, we study five strategies to generate the malicious node's features $x_m$. This helps us to assess the success of our attack. These five strategies are designed with varying degrees of sparsity and stealthiness,
%\javi{[after reading the whole subsection, I'm not sure about what we mean by stealthiness. I think the key point here is whether the adversary knows the features of ]}
enabling us to explore their effectiveness in altering the model's predictions. We define the proposed strategies as follow:

\begin{enumerate}
    \item \textbf{All-ones strategy}: Generates a dense feature vector for the malicious node, containing all ones, as shown in the equation below:
    \begin{equation*}
        x_m = \mathbf{1}.
    \end{equation*}
    This strategy potentially causes significant changes in predictions but may be less stealthy due to its dense feature vector.
    
    \item \textbf{All-zeros strategy}: Creates a sparse feature vector for the malicious node, containing all zeros, as shown in the equation below:
    \begin{equation*}
        x_m = \mathbf{0}.
    \end{equation*}
    This approach may subtly alter the output of the GNN, leading to smaller changes in predictions, while offering increased stealthiness.
    
    \item \textbf{Identity strategy}: Introduces a malicious node with a feature vector identical to the target node's feature vector, as shown below:
    \begin{equation*}
        x_m = x_t.
    \end{equation*}
    This strategy causes confusion in the model's predictions for neighboring nodes and has variable stealthiness based on the similarity between injected and target nodes.
    For this strategy, we assume that $\mathcal{A}$ knows the features of the target node $x_t$.
    
    \item \textbf{Max attributes strategy}: This method creates a malicious node feature vector by computing the element-wise maximum of each attribute in the target nodes' feature matrix.
    Specifically, it considers only nodes from classes different from the target node's class, as shown below:
    \begin{equation*}
    x_{m,k} = \max_{i \in V_{\mathcal{A}}, \text{ with } C_i \neq C_t} X_{i,k}, \quad \text{for} \quad k = 1, \ldots, d.
    \end{equation*}
    Here, $C_i$ represents the class of node $i$, and $C_t$ is the class of the target node.
    This strategy potentially causes significant changes in predictions but may be less stealthy due to exaggerated features. We assume in this strategy that the adversary has access to the features of the set of target nodes $V_{\mathcal{A}}$ and also to their predicted classes by the GNN. The predicted classes are accessible to the adversary after step 1 in Algorithm \ref{alg:prob_vecs_attack}.
    
    \item \textbf{Class representative strategy}: This approach generates a malicious node feature vector by selecting the feature vector of the node with the highest confidence score for a specific class, different from the target node's class, as shown below:
    \begin{equation*}
    x_m = x_{i^*} \text{ with } i^* = \argmax_{\substack{ i \in V_{\mathcal{A}}, \\ C_i \neq C_t}} p_{i,j}.
    \end{equation*}
    In this equation, $x_m$ is the malicious node feature vector, $i^*$ is the node index with the highest confidence score for a specific class different from the target node's class, $V_{\mathcal{A}}$ is the set of target nodes, $C_i$ represents the class of node $i$, and $C_t$ is the class of the target node. This strategy leverages the model's predictions to alter the neighbors of the target node predictions, potentially offering increased stealthiness.
\end{enumerate}
Additionally, we introduce the \textit{so-called} LinkTeller \textbf{Influence} strategy as an alternative to the original method in \cite{linkteller} incorporating their feature perturbation strategy.
This strategy entails perturbing the features of the target node by adding a small real value $\delta$, as shown below:
\begin{equation*}
x_m = x_t + \mathbf{\delta}.
\end{equation*}
We assess the performance of the Influence strategy in comparison to other strategies, aiming to determine whether the attack performance gains are attributable to node injection or the crafting of malicious features. It is worth noting, however, that the Influence strategy may be easily detected if the feature $x_t$ has a discrete nature, given that $x_m$ is real-valued.
