{
  "title": "Learning Multi-modal Representations by Watching Hundreds of Surgical Video Lectures",
  "authors": [
    "Kun Yuan",
    "Vinkle Srivastav",
    "Tong Yu",
    "Joel L. Lavanchy",
    "Jacques Marescaux",
    "Pietro Mascagni",
    "Nassir Navab",
    "Nicolas Padoy"
  ],
  "submission_date": "2023-07-27T22:38:12+00:00",
  "revised_dates": [
    "2024-01-13T13:56:32+00:00",
    "2024-07-22T17:12:10+00:00",
    "2025-03-27T15:37:03+00:00",
    "2025-06-16T19:07:50+00:00"
  ],
  "abstract": "Recent advancements in surgical computer vision applications have been driven by vision-only models, which do not explicitly integrate the rich semantics of language into their design. These methods rely on manually annotated surgical videos to predict a fixed set of object categories, limiting their generalizability to unseen surgical procedures and downstream tasks. In this work, we put forward the idea that the surgical video lectures available through open surgical e-learning platforms can provide effective vision and language supervisory signals for multi-modal representation learning without relying on manual annotations. We address the surgery-specific linguistic challenges present in surgical video lectures by employing multiple complementary automatic speech recognition systems to generate text transcriptions. We then present a novel method, SurgVLP - Surgical Vision Language Pre-training, for multi-modal representation learning. Extensive experiments across diverse surgical procedures and tasks demonstrate that the multi-modal representations learned by SurgVLP exhibit strong transferability and adaptability in surgical video analysis. Furthermore, our zero-shot evaluations highlight SurgVLP's potential as a general-purpose foundation model for surgical workflow analysis, reducing the reliance on extensive manual annotations for downstream tasks, and facilitating adaptation methods such as few-shot learning to build a scalable and data-efficient solution for various downstream surgical applications. The [training code](https://github.com/CAMMA-public/PeskaVLP) and [weights](https://github.com/CAMMA-public/SurgVLP) are public.",
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "primary_category": "cs.CV",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.15220",
  "pdf_url": "https://arxiv.org/pdf/2307.15220v5",
  "comment": "Accepted by Medical Image Analysis (MedIA), 2025",
  "num_versions": null,
  "size_before_bytes": 168989339,
  "size_after_bytes": 1439661
}