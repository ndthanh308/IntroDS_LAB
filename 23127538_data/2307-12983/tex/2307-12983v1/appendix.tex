\begin{appendices}
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{footnote}{0}
\renewcommand{\thefigure}{\Alph{section}.\arabic{figure}}
\renewcommand{\thetable}{\Alph{section}.\arabic{table}}

\section{Pseudo Code}
\label{appsec:code}

\begin{algorithm}
\caption{\actor Process (main process)}
\label{alg:actor}
\begin{algorithmic}
    \FOR{$n=1:W_a$}
    \STATE $\pi \gets$ policy network from \plearner process
    \STATE Initialize an empty buffer $B=\phi$
    \FOR{$t=1:H$}
    \STATE $\bm{a}_t\gets \pi(\bm{s}_t)$ with mixed exploration noise
    \STATE $(\bm{r}_t, \bm{s}_{t+1})\gets$ \textbf{envs}.step($\bm{a}_t$)
    \STATE $B=B\cup{\{\bm{s}_t, \bm{a}_t, \bm{r}_t, \bm{s}_{t+1}\}}$
    \ENDFOR
    \STATE $Q_1, Q_2\gets$ Q functions from \vlearner process
    \STATE send $B, \pi$ to \vlearner, send $\{s_t\}$ in $B$, $Q_1, Q_2$ to \plearner
    \STATE sleep for $t_a$ seconds to satisfy $\beta_{a:v}$ 
    \ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{\plearner Process}
\label{alg:plearner}
\begin{algorithmic}
    \STATE Initialize an empty buffer $B_p=\phi$
    \FOR{$n=1:W_p$}
    \IF {new data received}
    \STATE ${\{s_t\}}\gets$ from \actor process
    \STATE $Q_1, Q_2\gets$ from \actor process
    \STATE $B=B\cup{\{s_t\}}$
    \ENDIF
    \STATE sample a batch of $\{s_t\}$
    \STATE update $\pi$ by maximizing the $\min_{i=1,2}Q_i(s_t, \pi(s_t))$
    \STATE sleep for $t_p$ seconds to satisfy $\beta_{p:v}$ 
    \ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{\vlearner Process}
\label{alg:vlearner}
\begin{algorithmic}
    \STATE Initialize an empty buffer $B_v=\phi$
    \FOR{$n=1:W_v$}
    \IF {new data received}
    \STATE ${\{s_t, a_t, r_t, s_{t+1}\}}\gets$ from \actor process
    \STATE $\pi\gets$ from \actor process
    \STATE $Q_1, Q_2\gets$ from \actor process
    \STATE $B=B\cup{\{s_t\}}$
    \ENDIF
    \STATE sample a batch of $\{s_t, a_t, r_t, s_{t+1}\}$
    \STATE update $Q_1, Q_2$ by minimizing the mean-squared Bellman error (with Double Q-learning)
    \STATE sleep for $t_v$ seconds to satisfy $\beta_{p:v}, \beta_{a:v}$ 
    \ENDFOR
\end{algorithmic}
\end{algorithm}


\section{Training setups}
\subsection{Hyper-parameters}
\label{appsubsec:hyper}

We use the hyper-parameter values shown in \tblref{tbl:hyper} and the reward scaling shown in \tblref{tbl:reward_scale} for all the experiments unless otherwise specified. As for PPO, we use the same hyperparameter setup in ~\citet{makoviychuk2021isaac}.

\begin{table}[!h]
\centering
\caption{Hyper-parameter setup for six Isaac Gym benchmark tasks}
\label{tbl:hyper}
\begin{tabular}{llll} 
\toprule
Hyper-parameter                          & PQL(ours) & DDPG     & SAC       \\ 
\midrule
Num. Environments                        & 4,096      & 4,096     & 4,096      \\
Critic Learning Rate                     & $5 \times 10^{-4}$  & $5 \times 10^{-4}$ & $5 \times 10^{-4}$  \\
Actor Learning Rate                      & $5 \times 10^{-4}$  & $5 \times 10^{-4}$ & $5 \times 10^{-4}$  \\
Learnable Entropy Coefficient            & - & - & True \\
Optimizer                                & Adam      & Adam     & Adam      \\
Target Update Rate ($\tau$) & $5 \times 10^{-2}$  & $5 \times 10^{-2}$ & $5 \times 10^{-2}$  \\
Batch Size                               & 8,192      & 8,192     & 8,192      \\
Num. Epochs ($\beta_{a:v}$)              & 8         & 8        & 8         \\
Discount Factor($\gamma$)                & 0.99      & 0.99     & 0.99      \\
Normalized Observations                  & True      & True     & True      \\
Gradient Clipping                        & 0.5       & 0.5      & 0.5    \\
Exploration Policy                       & Mix       & Mix      & -      \\
$N$-step target                          & 3         & 3        & 3         \\
Warm-up Steps                            & 32        & 32       & 32        \\
Replay Buffer Size                       & $5 \times 10^6$ & $5 \times 10^6$ & $5 \times 10^6$  \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!h]
\centering
\caption{Reward scale}
\label{tbl:reward_scale}
\begin{tabular}{cc} 
\hline
                     & Reward scale  \\ 
\hline
Ant                  & 0.01          \\
Humanoid             & 0.01          \\
ANYmal               & 1.0           \\
Franka Cube Stacking & 0.1           \\
Allegro Hand         & 0.01          \\
Shadow Hand          & 0.01          \\
Ball Balance         & 0.1           \\
DClaw Hand         & 0.01           \\
\hline
\end{tabular}
\end{table}

\subsection{Hardware Configurations}

\tblref{tbl:hardware} lists the hardware configurations of the workstations we used for the experiments. We use the machines with GeForce RTX $3090$ for experiments by default. We also measure how much time it takes for the simulator to generate $1$M interaction data with $4096$ parallel environments on \ant and \shadow. We generate $1$M data via the following command.
\begin{verbatim}
for i in range(244):
    action = torch.randn((4096, 
                          envs.action_space.shape[0]), 
                          device='cuda')
    out = envs.step(action)
\end{verbatim}

\renewcommand{\arraystretch}{1.5}
\begin{table}[!htb]
\centering
\caption{Hareware configurations on different workstations}
\label{tbl:hardware}
\resizebox{\columnwidth}{!}{
\begin{tabular}{c|ccccc} 
\hline
\multicolumn{2}{c}{}                                                                                                        & Workstation 1                & Workstation 2              & Workstation 3      & Workstation 4            \\ 
\hline
\multicolumn{2}{c}{CPU}                                                                                                     & AMD Threadripper 3990X & Intel Xeon Gold 6248 & AMD Rome 7742 & Intel Xeon W-2195  \\
\multicolumn{2}{c}{GPU}                                                                                                     & GeForce RTX 3090             & Tesla V100                 & Tesla A100         & GeForce RTX~2080 Ti      \\
\multicolumn{2}{c}{GPU CUDA Cores}                                                                                          & 10496                        & 5120                       & 6912               & 4352                     \\
\multicolumn{2}{c}{GPU FP32 TFLOPs}                                                                                         & 35.58                        & 16.4                       & 19.5               & 13.45                    \\ 
\hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Time for generating \\ 1M data ($N=4096$) (s)\end{tabular}} & Ant & $1.678\pm 0.006$  &  $2.117\pm0.038$  &   $1.999\pm0.004$   &   $3.397\pm0.014$   \\
   & Shadow Hand &   $6.706\pm0.028$  &$9.051\pm0.035$ & $8.653\pm0.101$   & $10.885\pm0.025$\\
\hline
\end{tabular}
}
\end{table}
\renewcommand{\arraystretch}{1.}

\subsection{Vision experiment setup}
\label{appsubsec:vision}

We render the RGB camera image in a resolution of $48\times48$. The CNN part of our vision network $g(o_t)$ is as follows: 
\begin{verbatim}
Conv(3,32,3,2)-BN(32)-ReLU-3x(Conv(32,32,3,2)-BN(32)-ReLU)
\end{verbatim}
where \verb|Conv(a,b,k,s)| is a Convolutional layer with input channels $a$, output channels $b$, kernel size $k$, stride $s$.

Since our policy input contains a history of observations $(o_{t-2}, o_{t-1}, o_t)$, we use the same CNN to extract the feature of each observation and then concatenate all the embeddings. Then, the concatenated embedding goes through an MLP network $h$:
\begin{verbatim}
    FC(256)-ReLU-FC(63)-ReLU-FC(3)
\end{verbatim}

In summary, at each time step $t$, the policy output is $h[\text{cat}(g(o_{t-2}), g(o_{t-1}), g(o_t))]$. Storing images in a replay buffer can take up a lot of memory. Therefore, we experiment with different placements of the replay buffer: (1) put the replay buffer on a GPU with a big memory, (2) put the replay buffer on CPU RAM. We use the same A100 GPUs for all these image-based experiments. \figref{fig:vision} shows that our method (PQL) works with either the replay buffer on the GPU or CPU, and it achieves much faster learning and better performance than PPO.


% Figure environment removed

\begin{table}[!h]
\centering
\caption{Hyper-parameter setup for the \textit{Ball Balancing} task.}
\label{tbl:hyper-ball}
\begin{tabular}{lll} 
\toprule
Hyper-parameter                          & PQL(ours) & PPO  \\ 
\midrule
Num. Environments                        & 1,024      & 1,024   \\
Critic Learning Rate                     & $5 \times 10^{-4}$  & $5 \times 10^{-4}$ \\
Actor Learning Rate                      & $5 \times 10^{-4}$  & $5 \times 10^{-4}$ \\
Optimizer                                & Adam      & Adam  \\
Target Update Rate ($\tau$) & $5 \times 10^{-2}$  & -  \\
Batch Size                               & 4,096     & 4,096  \\
Horizon length                           & 1         & 16      \\
Num. Epochs                              & 12         & 5      \\
Discount Factor($\gamma$)                & 0.99      & 0.99  \\
Normalized Observations                  & True      & True  \\
Gradient Clipping                        & True      & True  \\
Exploration Policy                       & Mix       & -      \\
$N$-step target                          & 3         & -      \\
Warm-up Steps                            & 32        & -      \\
Replay Buffer Size                       & $10^6$ & -  \\
Clip Ratio                               & -         & 0.2 \\
GAE                                      & -         & True   \\
$\lambda$                                & -         & 0.95   \\
\bottomrule
\end{tabular}
\end{table}

\section{Additional Experiments}
\label{appsec:extra_exps}

\paragraph{$n$-step returns} We investigate how much does $n$-step returns help for PQL. As shown in \figref{fig:nstep_replay}, adding $n$-step return leads to faster learning than not using $n$-step return ($n=1$). However, using a big $n$ value hurt the learning. Empirically we found that $n=3$ gives us the best performance.

\paragraph{Benefit of adding speed control ($\beta_{p:v}, \beta_{a:v})$ on different processes} As we mentioned in \secref{subsec:ratio_balance}, adding speed control using $\beta_{p:v}, \beta{a:v}$ can help reduce the variance of training when the amount of computation resources changes. To provide more insights, we ran experiments without speed control, i.e., each process could run as fast as possible without any waiting. As shown in \figref{fig:beta_benefit}, when there are sufficient compute resources available (with two GPUs), the benefit of having the speed ratio control is not significant. However, when only one GPU is available for running all three processes (\actor, \plearner, \vlearner), we can see that without the ratio control, the learning curves on all six benchmark tasks slow down. We believe this is because all three processes are trying to run as fast as possible, resulting in competition for GPU utilization, which slows overall learning. Adding the ratio control helps balance GPU resource utilization among the three processes. Thus, even with one GPU, the learning performance with ratio control is quite similar to that with two GPUs.

% Figure environment removed



\paragraph{GPU hardware}
The simulation speed and network training speed vary across different GPU models. In \tblref{tbl:hardware}, we list how much time it takes for the simulator to generate $1$M environment interaction data with $4096$ parallel environments on four machines with different GPU models. In our test, the simulation speed on different GPU models is as follows: GeForce 3090 $>$ Tesla A100 $>$ Tesla V100 $>$ GeForce 2080Ti. We test PQL performance on all these four different machine configurations (\tblref{tbl:hardware}). \figref{fig:ant_gpu_type} and \figref{fig:shadow_gpu_type} show that different GPU models affect the policy learning speed, especially on complex tasks like \shadow which takes more simulation time. 


% Figure environment removed

\paragraph{PQL for SAC} As discussed above, PQL framework is flexible and can be combined with different $Q$-learning methods. Here, we show that PQL can be combined with SAC as well. \figref{fig:sac_baselines_time} shows that adding the PQL framework to SAC substantially speeds up the learning speed of SAC.
% Figure environment removed

\paragraph{Sample efficiency compared to baselines}
\figref{fig:baselines_sample} shows the sample efficiency of each algorithm on different environments. Overall, we see that PQL achieves the best sample efficiency. In addition, DDPG(n) also outperforms SAC(n) in terms of sample efficiency on these tasks.

% Figure environment removed

\paragraph{Sweep over different $\beta_{a:v}$ and $\beta_{p:v}$}

\figref{fig:app_actor_critic_ratio} shows the complete learning curves with different $\beta_{p:v}$ values and different number of environments. Similarly, \figref{fig:app_worker_critic_ratio} shows the learning curves for different $\beta_{a:v}$.

% Figure environment removed

% Figure environment removed

\paragraph{Comparison of our implementation with RL-games}
In this work, we implemented all the algorithms (PQL and all the baselines) from scratch, as it gives us the most flexibility in exploring different design choices that can affect learning performance. To show that our codebase provides good performance, we compare it against the most commonly used RL codebase used for Isaac Gym, which is RL-games~\citep{rl-games2022}. However, RL-games only support PPO and SAC. Hence, we compare our implementations of PPO and SAC against the ones in RL-games.


% Figure environment removed



\paragraph{Distributional critic update}
We investigate how a distributional version of the critic update affects the policy learning performance. Here, we utilize categorical parameterization that outputs a discrete-value distribution defined over a fixed set of atoms $z_i$ \citep{bellemare2017distributional}. We use the same hyper-parameters across the six tasks, where the number of atoms $l = 51$ and the bounds on the support from ($-10, 10$). To make sure the values lie on the support defined by the atoms, we scale the reward into a similar range via different scaling factors shown in \tblref{tbl:reward_scale} and apply the categorical projection operator before minimizing the cross-entropy.

\end{appendices}