{
  "title": "Set-Membership Inference Attacks using Data Watermarking",
  "authors": [
    "Mike Laszkiewicz",
    "Denis Lukovnikov",
    "Johannes Lederer",
    "Asja Fischer"
  ],
  "submission_date": "2023-06-22T06:47:56+00:00",
  "revised_dates": [],
  "abstract": "In this work, we propose a set-membership inference attack for generative models using deep image watermarking techniques. In particular, we demonstrate how conditional sampling from a generative model can reveal the watermark that was injected into parts of the training data. Our empirical results demonstrate that the proposed watermarking technique is a principled approach for detecting the non-consensual use of image data in training generative models.",
  "categories": [
    "cs.CV",
    "cs.CR",
    "cs.LG"
  ],
  "primary_category": "cs.CV",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.15067",
  "pdf_url": "https://arxiv.org/pdf/2307.15067v1",
  "comment": "Preliminary work",
  "num_versions": null,
  "size_before_bytes": 115958,
  "size_after_bytes": 116670
}