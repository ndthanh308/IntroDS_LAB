
\def\Figref#1{Fig.~\ref{#1}}
\def\Tabref#1{Table~\ref{#1}}



% Figure environment removed
\section{Experimental Results}
We firstly examine \ours in five toy examples with different artificial failure boundaries (\eg open boundaries, multiple failure regions, and non-centered regions) with their ground-truth log failure probability (LFP) in \Figref{fig:toy}. Onion sampling with 1000 samples and estimated LFP using kernel density estimator (KDE) with a {bandwidth} of 0.75 are shown in the second row. We can see that onion sampling is efficient but also overestimate the LFP. NSF for the estimated LFP is shown in the third row, which shows significant reductions in the overestimation of onion sampling.

We assess \ours on challenging high-dimensional benchmark circuits, namely, three SRAM column circuits with 108, 569, and 1093 variation parameters, respectively. The circuits are synthesized using the Synopsys Design Compiler and the Cadence Virtuoso design tools.
We set the target failure rate of the circuits at approximately $10^{-5}$ to highlight the challenge of the yield estimation problem.
We implement MC as the golden standard to estimate the ground-truth yields.
% The ground truth failure rate is obtained by using MC.  
To show the accuracy and efficiency of \ours, we also implement {the} SOTA IS methods, including Minimized Norm Importance Sampling (MNIS) \cite{MNIS}, Hyperspherical Clustering and Sampling (HSCS) \cite{HSCS}, Adaptive Importance Sampling (AIS) \cite{AIS}, Adaptive Clustering and Sampling (ACS) \cite{ACS}, and surrogate-based methods, including Low-Rank Tensor Approximation (LRTA) \cite{LRTA} and Absolute Shrinkage Deep Kernel learning (ASDK) \cite{ASDK}, as comparison methods.

% and then compare \ours with SOTA IS method, including Mnimized Norm Importance Sampling (MNIS) \cite{MNIS}, Hyperspherical Clustering and Sampling (HSCS) \cite{HSCS}, Adaptive Importance Sampling (AIS) \cite{AIS}, Adaptive Clustering and Sampling (ACS) \cite{ACS}, and surrogate-based methods, including Low-Rank Tensor Approximation (LRTA) \cite{LRTA} and Absolute Shrinkage Deep Kernel learning (ASDK) \cite{ASDK}.
% We assess \ours and other SOTA IS- and surrogate-based yield estimation methods to estimate the failure rate and compare their performance.
% The compared competitors includes Mnimized Norm Importance Sampling (MNIS) \cite{MNIS}, Hyperspherical Clustering and Sampling (HSCS) \cite{HSCS}, Adaptive Importance Sampling (AIS) \cite{AIS}, Adaptive Clustering and Sampling (ACS) \cite{ACS}, Low-Rank Tensor Approximation (LRTA) \cite{LRTA} and Absolute Shrinkage Deep Kernel learning (ASDK) \cite{ASDK}. 
% 
% In this section, we also implement ablation experiments to prove that the proposed pre-sampling method does contribute to the performance.   

% In order to determine when to stop the yield estimation algorithm, we choose
The figure of Merit (FOM) $\rho= \mathrm{std}(P_f) / P_f$ (where $\mathrm{std}(P_f)$ is the stand deviation of estimated yield) is used as the stopping criterion for all methods with $\rho=0.1$ (indicating at least 90\% accurate with 90\% confidence interval) as in many previous works, \eg \cite{MNIS, HSCS, AMSV}. 
For the 108-dimensional case, we use a 4-layer multi-layer perception (MLP), each with 432 hidden units; for the high-dimensional 569 and 1093 cases, we use a 7-layer MLP with 600 hidden units for each layer. ReLu activation is used. The optimization is done with Adam with 500 epochs. The baseline methods use {(default)} setting as suggested in their papers.
All experiments are performed on a Linux system with AMD 5950x and 32GB RAM.
 
 % \help{experimental setting: dense network}

% to indicates a estimation of 
% to terminated the iteratio
% we stop the process of yield estimation and declare that the estimated $P_f$ is 



\begin{table*}
  \centering
      \tabcolsep=5pt
      \caption{Numerical results on three SRAM column circuits}
      \label{AllCaseTable}
      \vspace*{-0.1in}
      \begin{tabular}{l|cccc|cccc|cccc}
        \toprule
        & \multicolumn{4}{|c|}{108-dimensional case} & \multicolumn{4}{|c|}{569-dimensional case}  & \multicolumn{4}{|c}{1093-dimensional case} \\
        \midrule
         Method & Fail. prob. & Rel. error & \# of sim. & Speedup & Fail. prob. & Rel. error & \# of sim. & Speedup & Fail. prob. & Rel. error & \# of sim. & Speedup\\
        \midrule
        MC   & 5.01e-5 &   -   & 699000 & 1x & 2.50e-5 &   -   & 931000 & 1x & 4.80e-5 &   -   & 1189000 & 1x \\
        MNIS & 4.15e-5 & 17.07\% & 47500 & 14.72x & 2.07e-5 & 17.33\% & 59000  & 15.78x & 4.21e-5 & 12.32\% & 81000 & 14.68x\\
        HSCS & 4.84e-5 & 3.36\% & 26500 & 26.38x & 2.86e-5 & 14.27\% & 46500  & 20.02x & 4.30e-5 & 10.47\% & 66000 & 18.02x\\
        AIS  & 4.75e-5 & 5.21\% & 12300 & 56.83x & 2.38e-5 & 4.99\%  & 25700  & 36.23x & 4.43e-5 & 7.75\% & 38000 & 31.29x\\
        ACS  & 5.68e-5 & 13.40\% & 10400 & 67.21x & 2.73e-5 & 9.19\%  & 22500  & 41.38x & 4.42e-5 & 7.83\% & 30400 & 39.11x\\
        LRTA & 4.50e-5 & 10.18\% & 13000 & 53.77x & 2.26e-5 & 9.60\%  & 18500  & 50.32x & 5.25e-5 & 9.38\% & 24000 & 49.54x\\
        ASDK & 4.5e-5 & 10.18\% & 9200 & 75.98x & 2.30e-5 & 8.00\% & 11800 & 78.90x & 6.10e-5 & 27.08\% & 14550 & 81.72x\\
        Proposed & 5.02e-5 & \bf{0.21}\% & \bf{5300} & \bf{131.89x} & 2.49e-5 & \bf{0.25}\% & \bf{3400} & \bf{273.82x} & 4.67e-5 & \bf{2.71}\% & \bf{6400} & \bf{185.78x}\\
      \bottomrule
    \end{tabular}
    \vspace*{-0.1in}
\end{table*}
\subsection{108-Dimensional SRAM Column Circuit} \label{108dim_exp}
An SRAM array is a typical type of random-access memory and uses flip-flop{s} to store data. The SRAM array and cell are shown in \Figref{SRAM_column}, where WL is the word line, and BL is the bit line; two cross-connected inventors composed of four transistors are used for storing data, whereas the other two transistors work as control switches for data transmission.
Each transistor contains three variational parameters.
An 8-bit SRAM array is composed of eight cells, resulting in 108 variation parameters. We choose the delay time of read/write of the SRAM as the output performance metric $\y$ of the circuit. 
%  Our experiment is implemented on a SRAM column circuit with And 
% We compare different methods (MC, MNIS, HSCS, AIS, ACS, LRTA, ASDK, proposed) in accuracy and efficiency.

% In terms of accuracy, 
We show the failure probability estimation and FOM in \Figref{case3_exp}, and the numerical results are concluded in \Tabref{AllCaseTable}.
The advantage of \ours is obvious by giving a 131.89x speedup compared to MC, more than twice faster {than} the second-best method AIS. Except for being the fastest, \ours also achieves the best accuracy among all methods with a relative error of 0.21\%, more than 10x better than the second-best method HSCS. It is also interesting to see that \ours seriously overestimates the failure due to the suboptimal onion sampling. Such a bias is then sequentially reduced as more samples are collected.

% Figure environment removed

% The ground truth of failure rate by the MC method is 5.01e-5, using 699k runs of simulation. And It can be seen that \ours outperforms all other competitors in terms of estimation accuracy and convergence efficiency of achieving a steady state, exhibiting only a 0.21\% relative error and 131.89x speedup to the MC. 
% \ours uses 5300 simulation runs to achieve convergence, while MNIS, HSCS, AIS, ACS, LRTA and ASDK consumes 47.5k, 26.5k, 12.3k, 10.4k, 13k and .... simulation runs respectively.   
% The $P_f$ of IS-based approaches like MNIS, HSCS, AIS and ACS remains very low at the first half of the evolution, because they don't find enough failure regions. When they find a correct failure region, a sudden increase in $p_f$ arises and it will finally converge to the MC ground truth result. AIS and ACS render faster convergence than MNIS and HSCS, because ACS and AIS are able to adaptively discover failure regions while searching areas of MNIS and HSCS are pre-fixed. 
% The $P_f$ evolution of surrogate-based methods like LRTA and ASDK is majorly determined by the model fitting accuracy to the variation parameter space. With enough training data, $P_f$ of surrogate-based methods converges to the ground truth. 
% \ours outperforms all other IS-based competitors because they need a large number of pre-sampling and \ours's novel adaptive scheme speeds up the convergence efficiency.  \ours outperforms all other surrogate-based competitors because surrogate methods need large number of training data to achieve an accurate model due to the ``curse of dimensionality''. 







% \begin{table*}
% \centering
%     \caption{Numerical results on 569-dimensional SRAM column}
%     \begin{tabular}{c|c|cccccccc}
%       \toprule
%        & Results & MC & MNIS & HSCS & AIS & ACS & LRTA & ASDK & Proposed \\
%       \midrule
      
%       \multirow{4}{*}{Successful runs} 
%       & Mean failure prob.   & 2.50e-5 & 2.34e-5 & 2.68e-5 & 2.31e-5 & 2.69e-5 & 2.29e-5 &  & 2.58e-5\\
%       & Mean relative error  & Golden & 6.29\% & 7.13\% & 7.61\% & 7.80\%  & 8.48\% &  & 3.41\% \\
%       & Mean \# of sim.      & 931000.00 & 69816.66 & 49975.00 & 28500.00 &  24812.50  & 19430.00  &  & 3600.00 \\
%       & Mean speedup & 1x & 13.33x & 18.63x & 32.67x & 37.52x & 47.92x & & 258.61x \\
%      \midrule
    
%      \multirow{1}{*}{Failed runs} 
%       & Failed times & N/A & 4/10 & 2/10 & 3/10 & 2/10 & 5/10 &  & 2/10 \\
%     \bottomrule
%   \end{tabular}
% \end{table*}




% \begin{table*}
% \centering
%     \caption{Numerical results on 1093-dimensional SRAM column}
%     \begin{tabular}{c|c|cccccccc}
%       \toprule
%        & Results & MC & MNIS & HSCS & AIS & ACS & LRTA & ASDK & Proposed \\
%       \midrule
      
%       \multirow{4}{*}{Successful runs} 
%       & Mean failure prob.   & 4.80e-5 & 4.51e-5 & 4.50e-5 & 4.59e-5 & 5.08e-5 & 5.30e-5 &  & 4.84e-5\\
%       & Mean relative error  & Golden & 6.09\% & 6.22\% & 4.43\% & 5.91\%  & 10.41\% &  & 0.73\% \\
%       & Mean \# of sim.      & 1189000.00 & 88928.57 & 72500.00 & 40557.14  & 35628.57  & 25125.00   &  &  6600.00\\
%       & Mean speedup & 1x & 13.37x & 16.40x & 29.32x & 33.37x & 47.32x & & 180.15x \\
%      \midrule
    
%      \multirow{1}{*}{Failed runs} 
%       & Failed times & N/A & 3/10 & 4/10 & 3/10 & 3/10 & 6/10 &  & 2/10 \\
%     \bottomrule
%   \end{tabular}
% \end{table*}


% % Figure environment removed



% \end{figure}
% % Figure environment removed



% 合并图表
% \begin{table*}
% \centering
%     \tabcolsep=4pt
%     \caption{Numerical results on three SRAM column circuits}
%     \label{AllCaseTable}
%     \begin{tabular}{c|cccc|cccc|cccc}
%       \toprule
%       & \multicolumn{4}{|c|}{108-dimensional circuit case} & \multicolumn{4}{|c|}{569-dimensional circuit case}  & \multicolumn{4}{|c}{1093-dimensional circuit case} \\
%       \midrule
%        Method & Fail. prob. & Rel. error & \# of sim. & Speedup & Fail. prob. & Rel. error & \# of sim. & Speedup & Fail. prob. & Rel. error & \# of sim. & Speedup\\
%       \midrule
%       MC   & 5.01e-5 &   N/A   & 699000 & 1x & 2.50e-5 &   N/A   & 931000 & 1x & 4.80e-5 &   N/A   & 1189000 & 1x \\
%       MNIS & 4.15e-5 & 17.07\% & 47500 & 14.72x & 2.07e-5 & 17.33\% & 59000  & 15.78x & 4.21e-5 & 12.32\% & 81000 & 14.68x\\
%       HSCS & 4.84e-5 & 3.36\% & 26500 & 26.38x & 2.86e-5 & 14.27\% & 46500  & 20.02x & 4.30e-5 & 10.47\% & 66000 & 18.02x\\
%       AIS  & 4.75e-5 & 5.21\% & 12300 & 56.83x & 2.38e-5 & 4.99\%  & 25700  & 36.23x & 4.43e-5 & 7.75\% & 38000 & 31.29x\\
%       ACS  & 5.68e-5 & 13.40\% & 10400 & 67.21x & 2.73e-5 & 9.19\%  & 22500  & 41.38x & 4.42e-5 & 7.83\% & 30400 & 39.11x\\
%       LRTA & 4.50e-5 & 10.18\% & 13000 & 53.77x & 2.26e-5 & 9.60\%  & 18500  & 50.32x & 5.25e-5 & 9.38\% & 24000 & 49.54x\\
%       ASDK & e-5 & \% &  & x & 2.30e-5 & 8.00\% & 11800 & 78.90x & 6.10e-5 & 27.08\% & 14550 & 81.72x\\
%       Proposed & 5.02e-5 & 0.21\% & 5300 & 131.89x & 2.49e-5 & 0.25\% & 3400 & 273.82x & 4.67e-5 & 2.71\% & 6400 & 185.78x\\
%     \bottomrule
%   \end{tabular}
% \end{table*}



% 独次最好的结果
% \begin{table}
% \centering
%     \caption{Best numerical results on 108-dimensional SRAM column}
%     \label{case3Table}
%     \begin{tabular}{c|cccc}
%       \toprule
%       Method & Failure prob. & Relative error & \# of sim. & Sim. speedup\\
%       \midrule
%       MC   & 5.01e-5 &   N/A   & 699000 & 1x\\
%       MNIS & 4.15e-5 & 17.07\% & 47500 & 14.72x\\
%       HSCS & 4.84e-5 & 3.36\% & 26500 & 26.38x\\
%       AIS  & 4.75e-5 & 5.21\% & 12300 & 56.83x\\
%       ACS  & 5.68e-5 & 13.40\% & 10400 & 67.21x\\
%       LRTA & 4.50e-5 & 10.18\% & 13000 & 53.77x\\
%       ASDK & e-5 & \% &  & x\\
%       Proposed & 5.02e-5 & 0.21\% & 5300 & 131.89x\\
%     \bottomrule
%   \end{tabular}
%   \end{table}

% \begin{table}
% \centering
%     \caption{Best numerical results on 569-dimensional SRAM column}
%     \label{case4Table}
%     \begin{tabular}{c|cccc}
%       \toprule
%       Method & Failure prob. & Relative error & \# of sim. & Sim. speedup\\
%       \midrule
%       MC       & 2.50e-5 &   N/A   & 931000 & 1x\\
%       MNIS     & 2.07e-5 & 17.33\% & 59000  & 15.78x\\
%       HSCS     & 2.86e-5 & 14.27\% & 46500  & 20.02x\\
%       AIS      & 2.38e-5 & 4.99\%  & 25700  & 36.23x\\
%       ACS      & 2.73e-5 & 9.19\%  & 22500  & 41.38x\\
%       LRTA     & 2.26e-5 & 9.60\%  & 18500  & 50.32x\\
%       ASDK     & e-5 & \% &  & x\\
%       Proposed & 2.49e-5 & 0.25\% & 3400 & 273.82x\\
%     \bottomrule
%   \end{tabular}
%   \end{table}

% \begin{table}
% \centering
%     \caption{Best numerical results on 1093-dimensional SRAM column}
%     \label{case5Table}
%     \begin{tabular}{c|cccc}
%       \toprule
%       Method & Failure prob. & Relative error & \# of sim. & Sim. speedup\\
%       \midrule
%       MC       & 4.80e-5 &   N/A   & 1189000 & 1x \\
%       MNIS     & 4.21e-5 & 12.32\% & 81000 & 14.68x\\
%       HSCS     & 4.30e-5 & 10.47\% & 66000 & 18.02x\\
%       AIS      & 4.43e-5 & 7.75\% & 38000 & 31.29x\\
%       ACS      & 4.42e-5 & 7.83\% & 30400 & 39.11x\\
%       LRTA     & 5.25e-5 & 9.38\% & 24000 & 49.54x\\
%       ASDK     & e-5 & \% &  & x\\
%       Proposed & 4.67e-5 & 2.71\% & 6400 & 185.78x\\
%     \bottomrule
%   \end{tabular}
%   \end{table}


% YF的图

% % Figure environment removed

% % Figure environment removed


% \begin{table*}
% \centering
%     \caption{Ablation experiment of pre-sampling method}
%     \begin{tabular}{c|c|c|c|c}
%       \toprule
%          & 1 & 2 & 3 & 4  \\
%        \midrule
%         Target density& % Figure removed&  % Figure removed  & % Figure removed & \\
%       \midrule
%               Pre-sampling  & % Figure removed & % Figure removed & % Figure removed &  \\
%        \midrule
%                Nf-sample & % Figure removed & % Figure removed & % Figure removed &  \\
%         \midrule
%                 Flow density
%     \bottomrule
%   \end{tabular}
% \end{table*}

% Figure environment removed
\subsection{569-Dimensional SRAM Column Circuit}
To validate \ours in practical scenarios, we work on a commercial SRAM array solution with 528 transistors in the design to form bit-cell arrays, sense amplifiers, and power paths. Based on the transistor type (PMOS/NMOS), gate length, and gate width, each transistor will associate with 0-3 variational parameters (\ie mobility, oxide thickness, and saturation velocity) based on the BSIM model.
With a BSIM4 model, this leads to 569 variation parameters. 
% where each transistor instance is instantiated based on a model and instantiation parameters, such as the transistor type (PMOS/NMOS), the gate length, and the gate width, and each transistor has a random number between 0 and 3 associated with it. 
% The random numbers correspond to various parameters that affect the transistor behavior, such as mobility, oxide thickness, saturation velocity, etc.
% we further increase the number of SRAM bit cells in the SRAM column, which leads to an SRAM circuit with 569 variation parameters. 
The delay time of read/write of the SRAM acts as the output metric $\y$. 
% We run the same yield estimation algorithm on the 569-dimensional circuit.
The convergence dynamic is shown in \Figref{case4_exp} with results in \Tabref{AllCaseTable}.
Similarly, the results of \ours are significantly better than the competitors, showing a 273.82x speedup over MC, which is almost 3.5x faster than the second-best method ASDK. Most importantly, the estimation results are very close to the ground truth, with a relative error of 0.25\%. Again, the initial overestimation remains for \ours.


% The numerical results and detailed $P_f$ evolution are shown in  and . 
% The MC method achieves the ground truth of failure rate 2.50e-5 using 931000 simulations. 
% As shown in \Figref{case4_exp}, only after long importance sampling, are other IS-based methods able to discover the correct failure regions and their $P_f$ increases to the peak. In contrast, due to the powerful proposed pre-sampling approach, \ours discovers the correct failure regions and its $P_f$ achieves the peak right after the pre-sampling state, and it converges to the ground truth rapidly. 
% \ours achieves a fail rate of 2.49e-5 using 3400 simulations, exhibiting a relative error of 0.25\%, which outperforms others in terms of accuracy and efficiency. In comparison, MNIS, HSCS, AIS, ACS, LRTA and ASDK uses 59000, 46500, 25700, 22500, 18500 and ... simulations respectively to achieve a fail rate with the larger relative error. Compared with MNIS, \ours can achieve a up to 17.35x more accurate and 69.32x speedup. 


% Figure environment removed
\subsection{1093-Dimensional SRAM Column Circuit}
We further increase the dimensions of the problem by using a detailed BSIM5 model, learning to 1093 variation parameters.
% of the circuit our problem by adding more bit cells, leading to an even more challenging SRAM column circuit with 1093 variation parameters.
As far as we are aware, no previous published work has ever attempted to estimate the yield of such a high-dimensional problem.
% and no former researcher implements yield estimation experiment on such a high-dimensional setting. 
Similarly, the delay time of read/write is used as the output metric. We run 1,189,000 simulations using the MC method and obtain the ground-truth failure rate, 4.80e-5. The results for the competing methods are shown in \Figref{case5_exp} and \Tabref{AllCaseTable}.
% The total experimental numerical results and detailed process are illustrated in \Tabref{AllCaseTable} and \Figref{case5_exp}.
The superior \ours is consistent, although this time, the improvement over the second-best method ASDK is 2.2x, not as significant as in the previous experiments. Nevertheless, the ASDK shows the largest error of 27.08\% among all methods. 

% It can be obviously concluded that \ours is still able to get the failure rate with highest accuracy and efficiency among other 6 methods, which uses merely 6400 runs of simulation and exhibits a estimated result with a 2.71\% relative error. Our method finds the correct failure region after one round of importance sampling, while other IS-based methods takes many rounds. The surrogate methods converge slower than \ours because they need a large number of training data.
% In comparison with MNIS, \ours can reach a up to 12.66x speedup and is 4.55x more accurate.

% \begin{table}
%   \centering
%       \caption{Ablation experiment of pre-sampling method}
%       \label{ablation}
%       \vspace*{-0.1in}
%       \begin{tabular}{c|c|c|c|c}
%          \midrule
%           & AIS&  AIS+  & ACS & ACS+ \\
%         \midrule
%           \# of IS & 13024 & 10854 & 12100 & 9600 \\
%           Rel. error & 7.98\% & 6.79\% & 6.59\% & 4.77\%  \\
%       \bottomrule
%     \end{tabular}
%     \vspace*{-0.15in}
%   \end{table}
\begin{table}
\centering
  \caption{Ablation experiment of pre-sampling method}
  \label{ablation}
  \vspace*{-0.1in}
  \begin{tabular}{c|ccc|ccc}
    \toprule
    & AIS & AIS+ & Impro. & ACS & ACS+ & Impro. \\
    \midrule
    Rel. error & 7.98\% & 6.79\% & 1.18x & 6.59\% & 4.77\% & 1.38x  \\
    \# of IS & 13024  & 10854  & 1.20x & 12100 & 9600 & 1.26x \\
    \bottomrule
  \end{tabular}
  \vspace*{-0.2in}
\end{table}

\subsection{Ablation Study}
% \begin{table}
%   \centering
%       \caption{Ablation experiment of pre-sampling method}
%       \label{ablation}
%       \vspace*{-0.1in}
%       \begin{tabular}{c|c|c|c|c|c|c}
%         \toprule
%            & \multicolumn{2}{|c|}{Origin method} & \multicolumn{2}{|c|}{With ours}  & \multicolumn{2}{|c}{Improvment}\\
%          \midrule
%           & Rel. error&  \# of IS  & Rel. error & \# of IS & Speedup & Acc.\\
%         \midrule
%                 AIS & 7.98\% & 13024 & 6.79\% & 10854 & & \\
%               ACS & 6.59\% & 12100 & 4.77\% & 9600 & & \\
%       \bottomrule
%     \end{tabular}
%     \vspace*{-0.15in}
%   \end{table}
% \subsection{Ablation Study}
% The onion sampling is the key contribution of this work.
We validate the usefulness of the onion sampling by equipping the classic IS methods, namely, AIS and ACS ,as their pre-sampling procedure and compare with their original versions.
% their results with their original LHS sampling.
The experiments are conducted on the same 108-dimensional SRAM experiment for its fast simulation speed.
1700 samples are given for all methods as their initial sampling budget, and the results are shown in \Tabref{ablation}, which shows about 20\% improvement in accuracy speedup with our onion sampling (AIS+ and ACS+).

% We give both method 1000 samples to start with and let them run for 1000 iterations. \help{confirm the number of iterations}.
% 
% We can clearly see that, using hyperspherical pre-sampling AIS and ACS run 11100 and 9300 simulations to achieve the fail rate of 4.74e-5 and 5.68e-5 respectively, and that, using our proposed pre-sampling method AIS and ACS run 10030 and 8000 to achieve the fail rate of 5.31e-5 and 5.45e-5 respectively. It can be concluded that the proposed pre-sampling method does help discover promising failure regions and accelerate the speed of convergence of fail probability.
% 

% To verify the proposed pre-sampling method's contribution to the performance of \ours, we conduct the ablation experiment on the 
% ACS and AIS use the same pre-sampling algorithm, named hyperspherical pre-sampling, which draws samples on hyperspheres with increasing radius in the variation parameter space. We replace AIS and ACS's pre-sampling method with our proposed method, and implements the modified AIS and ACS on the circuit. The circuit setup is as the same as that of the Section \ref{108dim_exp}.
% To evaluate the method fairly, the modified pre-sampling procedure are fixed to run as the same number of simulation as that of the origin pre-sampling procedure.
% The real failure probability of the circuit is set as 5.01e-5. 



\subsection{{Robustness Study}}
To further assess the robustness of our method, we conduct a robustness study on the 108-dimensional SRAM circuit by running experiments 10 times with random initializations. The final estimations with relative errors larger than 50\% are marked fail. The statistical results for successful runs are shown in \Tabref{robustness}. As we expect, the surrogate methods, LRTA and ASDK, suffer from great instability with more than 5 times fail, whereas the IS methods are more stable. In contrast, \ours shows the best performance for all metrics.

% \help{what is the final estimation?}
% \vspace{-0.2in}
\begin{table}
  \vspace{-0.15in}
  \centering
  \caption{Robustness test on 108-dimensional SRAM column}
  \label{robustness}
  \vspace{-0.1in}
    \begin{adjustbox}{width=1\columnwidth,center}
      \begin{tabular}{c|cccccccc}
        \toprule
         Method & MNIS & HSCS & AIS & ACS & LRTA & ASDK & Proposed \\
        \midrule
         Avg. RE  & 16.00\% & 8.90\% & 6.64\% & 6.45\%  & 12.85\% & 10.18\% & \bf{1.91\%} \\
        % & Mean \# of sim.      & 699000.00 & 50883.33 & 31340.00 & 15714.29 & 14071.43   & 13440.00  &  & 5300.00 \\
         Avg. speedup & 13.73x & 22.30x & 44.48x & 49.68x & 52.01x & 75.98x & \bf{131.89x} \\
       \midrule
        \# Fail & 4/10 & 4/10 & 3/10 & 3/10 & 5/10 & 9/10 & \bf{1/10} \\
      \bottomrule
    \end{tabular}
\end{adjustbox}
\vspace*{-0.2in}
\end{table}
  

% \begin{table*}
%   \centering
%       \caption{Numerical results on 108-dimensional SRAM column}
%       \begin{tabular}{c|c|cccccccc}
%         \toprule
%          & Results & MC & MNIS & HSCS & AIS & ACS & LRTA & ASDK & Proposed \\
%         \midrule
        
%         \multirow{2}{*}{Successful runs} 
%         % & Mean failure prob.   & 5.01e-5 & 4.21e-5 & 4.56e-05 & 4.67e-5 & 5.33e-5  & 4.37e-05 &  & 5.10e-5 \\
%         & Mean relative error  & - & 16.00\% & 8.90\% & 6.64\% & 6.45\%  & 12.85\% & 10.18\% & 1.91\% \\
%         % & Mean \# of sim.      & 699000.00 & 50883.33 & 31340.00 & 15714.29 & 14071.43   & 13440.00  &  & 5300.00 \\
%         & Mean speedup & 1x & 13.73x & 22.30x & 44.48x & 49.68x & 52.01x & 75.98x & 131.89x \\
%        \midrule
      
%        \multirow{1}{*}{Failed runs} 
%         & Failed times & N/A & 4/10 & 4/10 & 3/10 & 3/10 & 5/10 & 9/10 & 1/10 \\
%       \bottomrule
%     \end{tabular}
%   \end{table*}
  