\subsection{Problem Statement}
We consider the problem of managing a large-scale EAVs fleet to provide fair and robust E-AMoD service. The proposed method should (i) rebalance idle EAVs for providing fair ride service on the passenger's side; (ii) allocate low-battery EAVs to charging stations for balanced charging service on the EAVs' side; (iii) be robust to mobility demand and EAV supply uncertainties. 
We consider that the city is divided into $N$ regions according to a pre-defined partition method~\cite{ddrobust_Miao, dro_he}, and a day is divided into equal-length time intervals. At each time interval $[t, t+1)$, passengers' ride requests and low-battery EAV's charging needs emerge in each region. After the locations and status of each EAV are observed and updated, a local controller assigns available EAVs to pick up existing passengers in the request queue according to specific trip assignment algorithms, such as methods designed in the literature~\cite{survey_19}, and assigns EAVs that need to be charged to charging stations~\cite{wang2019shared}. Then the predicted passenger demand and available charging spots at each region for time interval $[t, t+1)$ are updated, and a system-level EAV balancing decision is calculated according to the algorithm designed in this work. 

Some assumptions about the model and algorithm are considered as follows. We consider four dynamic statuses for one EV: {\em vacant}, {\em occupied}, {\em low-battery} and {\em still}, similar to the literature~\cite{dro_he}. {\em Vacant} means the EV is not serving any passengers, i.e. idle and its remaining battery exceeds a threshold. The algorithm dispatches vacant EAVs to stay in the current region to pick up passengers or to move to other regions according to the future predicted passenger mobility demand in the following time interval. When a vacant EV picks up passengers, it turns to {\em occupied} status and we have no dispatch command for it until it becomes vacant again. An occupied EV becomes vacant again when it drops off passengers and still has enough battery. Once a vacant EV's battery level is lower than a threshold, it becomes a {\em low-battery} EV. The algorithm will assign low-battery EAVs to some regions for charging according to the availability of charging stations. When low-battery EAVs enter charging stations, they become {\em still} EAVs. Still EAVs become vacant when they finish charging and leave the charging stations.

We focus on a computationally tractable system-level EV balancing algorithm design such that both passenger demand and EV supply uncertainties are considered to maximize the expected total reward of the system or the entire city. The local trip and charging assignment algorithm are out of the scope of this work. For notation convenience, the parameters and variables definition in the following parts of this section omit the time index $t$ when there is no confusion.


 


\subsection{Zero-Sum Stochastic Game}
\label{sec_game}
We formulate the vehicle balancing problem for E-AMoD systems as a zero-sum stochastic game $\mathcal{G}$ between a set of region agents $\mathcal{N}_r$ and a set of adversarial agents $\mathcal{N}_a$. The \textit{region agent} is designed for each region to make dispatch decisions for vacant and low-battery EAVs at every time step. This distributed agent setting is more reasonable for large-scale fleet management than a single agent setting because the action space can be prohibitively large if we use a single agent~\cite{ev_kdd19}.  The \textit{adversarial agent} is designed to model uncertainties that may be caused by delayed information, missing data, inaccurate measurement and prediction errors. The adversarial agent can make dispatching decisions robust to modeling errors and real-world uncertainties by altering the protagonist's observation~\cite{pattanaik2017robust} in the training process. We define $\mathcal{S}_i$ as the state space for region $i$ shared by both the $i$-th region agent $i \in \mathcal{N}_r$ and adversarial agent $i \in \mathcal{N}_a$, and $\mathcal{S}\coloneqq \mathcal{S}_{1} \times \dots \times \mathcal{S}_{N}$ as the joint state space, then $\mathcal{G} \coloneqq  (\mathcal{N}_a, \mathcal{N}_r, \mathcal{S}, \mathcal{A}_a, \mathcal{A}_r, r, T)$. We define $\mathcal{A}_{a,i}$ and $\mathcal{A}_{r,i}$ as the action space of the adversarial agent and region agent of region $i$, and $\mathcal{A}_a \coloneqq \mathcal{A}_{a,1} \times \dots \times \mathcal{A}_{a,N}, \mathcal{A}_r \coloneqq \mathcal{A}_{r,1} \times \dots \times \mathcal{A}_{r,N}$ as the joint action space of adversarial and region agents, respectively. The map $r: \mathcal{S} \times \mathcal{A}_r \times \mathcal{A}_a \rightarrow \mathbf{R}$ is the reward function shared by all agents. The state transition probability function is $T: \mathcal{S}\times \mathcal{A}_r \times \mathcal{A}_a \rightarrow \Delta(\mathcal{S})$, where $\Delta(\mathcal{S})$ represents the set of probability distributions over the joint state space $\mathcal{S}$. $T (s'|s,a_r,a_a)$ is the probability of next state $s' \in \mathcal{S}$ given the current state $s$ and the adversarial joint actions $a_a$ and balancing decisions $a_r$ (region agents' joint actions). We formally define the states and actions in the next section. At each time step $t$, for all $i \in \mathcal{N}_a$, adversarial agent $i$ observes the true state information and chooses its action $a_{a,i}^t$ according to a policy $\pi_{a,i}: \mathcal{S}_i \rightarrow \Delta(\mathcal{A}_{a,i})$ to manipulate $s_i$. Region agent $i$ can only observe the perturbed state information and chooses its action $a_{r,i}^t$ according to a policy $\pi_{r,i}: \mathcal{S}_i \rightarrow \Delta(\mathcal{A}_{r,i})$. We define the adversarial agents' joint policy $\pi_a = \prod_{i \in \mathcal{N}_a}\pi^{a,i}: \mathcal{S} \rightarrow \Delta(\mathcal{A}_a)$ and region agents' joint policy $\pi_r = \prod_{i \in \mathcal{N}_r}\pi^{r,i}: \mathcal{S} \rightarrow \Delta(\mathcal{A}_r)$. After all region agents execute their actions, they get a shared reward $r^t$ and all adversarial agents get a shared opposite reward $-r^t$. The value functions are defined as the discounted return of region agents, i.e., 
action value function $q^{\pi_r, \pi_a}(s,a_r,a_a) = \mathbb{E}[ \sum_{t=1}^{\infty} \gamma^{t-1} r^t | s^1 = s, a_r^1 = a_r,a_a^1 = a_a,a_r^t \sim \pi_r(\cdot | \tilde{s}^t), a_a^t \sim \pi_a(\cdot | s^t)]$, $\tilde{s}^t = f(s^t, a_{a}^t)$, where $f: \mathcal{S}_i \times \mathcal{A}_{a,i} \rightarrow \mathcal{S}_i$ is the state perturbation function which describes the relationship between perturbed state, true state and adversarial agents' actions. We use $\tilde{s}^t = f(s^t,a_a^t)$ to denote $\tilde{s} = (\tilde{s}_1^t, \cdots, \tilde{s}_N^t)$ where $\tilde{s}_i^t = f(s_i^t, a_{a,i}^t)$, $\forall i = 1, \cdots, N$. The power of adversaries can be restricted by adding constraints to the perturbed state such as $\tilde{s} \in B(s, \epsilon)$ where $B(s, \epsilon)$ is a $\epsilon$-ball centered in the true state $s$, or by carefully defining the state perturbation function. Thus, we trade between the robustness and performance of agent policies by adjusting adversaries' perturbation power \cite{he2023robust, han2022solution}. In equation \eqref{def:map}, we give the formal definition of the perturbation function $f$ in our vehicle balancing problem of E-AMoD systems. Our goal is to solve the following min-max problem to get a robust region agents' joint policy.
\begin{align}
\label{minmax}
    \max_{\pi_r}\min_{\pi_a}\mathbb{E}[v^{\pi_r, \pi_a}(s)]
\end{align}
Zero-sum games involve two players/teams with conflicting objectives, where one side's gain is the other side's loss. This adversarial nature makes them suitable for modeling robustness problems since they capture situations where a system needs to perform well under strong perturbations or uncertain conditions. While the zero-sum game model does not explicitly capture road capacity and speed limits, it can still incorporate these aspects indirectly through the reward design and action spaces. 


\subsection{MARL Problem Formulation}
In this section, we formally define the state, action, reward and policy in our robust MARL framework.

\paragraph{State} A state $s_{i}^t \in \mathcal{S}_i$ of a region $i$ contains a vector that indicates its spatiotemporal status from both the local view and global view of the city at  time $t$ (we omit the time subscript $t$ later for convenience), and $s\coloneqq (s_1, \dots,  s_N)$. We define the state $s_i=\{s^{local}_i, s^{global}_i\}$, where $s^{local}_i = (V_i, L_i, d_i, ST_i, ES_i, SP_i)$ as the state from the local view, denoting the predictions about the amount of vacant EAVs, low-battery EAVs, mobility demand, still EAVs, empty charging spots, and total charging spots in region $i$, respectively. We define $s^{global}_i = (t, pos_i)$ as the spatial-temporal information from a global perspective, where $t$ is the time step index, $pos_i$ is region location information (longitudes, latitudes, boundaries, region index).

\paragraph{Region Agents Action} We use $\mathcal{N}(i)$ to denote the set of neighboring regions of region $i$, i.e., the adjacent regions according to the graph structure of the city and $n_i=|\mathcal{N}(i)| + 1$. A region agent $i$'s action $a_{r,i} = (\overline{p}, \overline{q}) \in \Delta^{n_i} \times \Delta^{n_i}$ where $\Delta^{n_i}$ denotes the probability simplex in $n_i$ dimensions. The action consists of balancing decisions for vacant EAVs to potential passenger demand $\overline{p} \in \Delta^{n_i}$, and low-battery EAVs without charging yet to potential available charging stations $\overline{q} \in \Delta^{n_i}$. Here, $p_{j \in \{1,2,...,n_i\}}$ is the $j$-th element of $\overline{p}$ that represents the percentage of vacant EAVs will move to the $j$-th region of $\{ \mathcal{N}(i) \cup \ {\text{region}\ i} \}$. For instance, $p_1= 0.1$ means $10$\% vacant EAVs are arbitrarily chosen and dispatched to the first adjacent region of region $i$. And $\overline{q}$ has a similar definition for low-battery EAVs dispatching decision. After executing region actions, the number of vacant, low-battery and still EAVs in different regions will be changed. Since the sum of the percentages of EAVs dispatched to different directions should be $1$, the dispatching actions have constraints that: $\sum_j \bar p_j = \sum_j \bar q_j \equiv 1$. 

\paragraph{Adversarial Agents Action} Adversarial agent $i \in \mathcal{N}_a$ alters the corresponding region agent's knowledge of the state by adding perturbation $a_{a,i} \in \mathcal{A}_{r,i}$. Here, $a_{a,i} = [\delta_d,\delta_c, \delta_v$] represents the volatility of predicted demand, empty charging spots, and vacant EAVs, respectively. The value of the perturbation simulates the state uncertainties from historical data;
for instance, we use box constraints that $d_l \leq \delta_d \leq d_u; c_l \leq \delta_c \leq c_u; v_l \leq \delta_v \leq v_u$ where the upper and lower bounds are determined by empirical experiments~\cite{dddro_tcps20, ddrobust_Miao}. We will also consider other more complicated formats of action space (such as second-order cone or ellipsoid) in future work. After formally defining the perturbation function $\tilde{s} = (\tilde{s}_1, \cdots, \tilde{s}_N)$, we give the formal definition of the state perturbation function $f: \mathcal{S}_i \times \mathcal{A}_{a,i} \rightarrow \mathcal{S}_i$ in our vehicle balancing problem of the E-AMoD systems. 
The altered local state $\Tilde{s}_i^{local} = (\widetilde{V}_i, \widetilde{L}_i, \widetilde{d}_i, \widetilde{ST}_i, \widetilde{ES}_i, \widetilde{SP}_i)$ 
is related to $s^{local}_i = (V_i, L_i, d_i, ST_i, ES_i, SP_i)$ and adversarial action $a_{a,i}$ by the following equation~\eqref{def:map}:
\begin{equation}
\begin{split}
    \widetilde{V}_i&= V_i + (SP_i - ES_i)\delta_c + V_i\delta_v;\\
    \widetilde{d}_i &= d_i(1+\delta_d);\\
    \widetilde{ST}_i &= ST_i - (SP_i - ES_i)\delta_c; \\
    \widetilde{ES}_i &= (SP_i - ST_i) \times I_{\{\widetilde{ST}_i < SP_i\}}.
\end{split}
\label{def:map}
\end{equation}
We denote $\tilde{s}_i = \{\tilde{s}_i^{local}, s_i^{global}\}$ which means we have $\tilde{s}_i = f(s_i = \{s_i^{local}, s_i^{global}\}, a_{a,i} = [\delta_d,\delta_c, \delta_v])$. Region agents choose their actions according to their policies and the perturbed states. After all region agents execute actions, the  system states change (e.g. number of EAVs).

\paragraph{Reward} Our goal is to optimize the system-level benefit, i.e., balanced charging utilization and fair service, hence, all region agents share common interests and the same reward function. By maximizing the shared reward, region agents are cooperating for the same goal. We let adversarial agents' reward function be the negative one of region agents. Thus, all adversarial agents aim to minimize the region agents' reward. 

We consider both the supply-demand ratio of vacant EAVs to the total mobility demand~\cite{dddro_tcps20, Morari_rideshare}, and the charging utilization rate~\cite{he2023data, p2charge, pricing_20} as service quality metrics for E-AMoD systems. A higher supply-demand ratio means a shorter waiting time for customers in one region. However, with limited EAV supply, achieving high supply-demand ratios in all regions is impossible. Keeping the supply-demand ratio of each region at a similar level allows passengers in the city to receive fair service~\cite{AMoD_queue, mpcmod_icra16}. Similarly, given the limited amounts of charging stations and spots,  to improve charging service quality and charging efficiency with limited infrastructure, balancing the charging utilization rate of all regions across the entire city is usually one objective for EV charging~\cite{EVcharge_19, p2charge}. Therefore, we define the fairness metric of charging utilization rate $u_c(s,a_r,a_a)$ and supply-demand ratio $u_s(s,a_r,a_a)$ as the negative sum of the difference between the local utilization rate (local supply-demand ratio) and the global charging utilization rate (global supply-demand ratio), respectively:
\begin{equation}
\begin{split}
    u_c(s,a_r,a_a) =   \sum^N_{i = 1} -\left\lvert  \frac{ES_i}{ST_i}- \frac{\sum^N_{j = 1}ES_j}{\sum^N_{j = 1}ST_j} \right\rvert,\\
    u_s(s,a_r,a_a)=  \sum^N_{i = 1} -\left\lvert  \frac{d_i}{V_i}- \frac{\sum^N_{j = 1}d_j}{\sum^N_{j = 1}V_j} \right\rvert,
    \label{def:u}
\end{split}
\end{equation}
A larger $u_c(s,a_r,a_a)$ or $u_s(s,a_r,a_a)$ value means a better balanced and fair charging utilization rate or supply-demand ratio among the city. Then we define the reward function $r(s,a_r,a_a)$ as a weighted sum of the city's charging utilization fairness $u_c$ and supply-demand fairness $u_s$, i.e. 
\begin{equation}
    r(s,a_r,a_a) \coloneqq u_c(s, a_r, a_a) + \beta u_s(s, a_r, a_a),
    \label{def:reward}
\end{equation} 
where $\beta$ is a positive weighted coefficient, $s, a_r, a_a$ is the joint state, joint action of all region agents, joint action of all adversarial agents, respectively.  
$r(s,a_r,a_a)$ is calculated after EAVs balance decision execution with a complex dynamic state transition process in the simulator.
One advantage of the proposed MARL method is the form of the reward function we would like to optimize does not need to satisfy the constraints as those in robust or distributionally robust optimization methods~\cite{ddrobust_Miao, dddro_tcps20}. For instance, the objective function does not need to be convex of the decision variable or concave of the uncertain parameters. %


\paragraph{Policy} All region agents share the same action space and deterministic policy, defined as $ \mu_r(\tilde{s}_i,s_{\mathcal{N}(i)}|\theta^r)$, parameterized by $\theta^r$. 
Similarly, all adversarial agents share one policy function, defined as $\mu_a (s_i,s_{\mathcal{N}(i)}|\theta^a)$, 
parameterized by $\theta^a$. We use $o_{a,i}, o_{r,i}$ to denote adversarial agent $i$ and region agent $i$' policy input, respectively. We use $\mathcal{O}$ to denote the set of all $o_{r,i},o_{a,i}$. The policy for each region relates to both its own state and its neighbors' states, since the actions of the region $i$ will affect the number of EAVs at different statuses in the neighbor regions $\mathcal{N}(i)$, and then affect the reward of the neighbor regions. This shared policy design provides a much more efficient learning procedure than training an individual policy function for each individual agent. Since the state of each agent contains spatiotemporal information, the policy of region agents is supposed to make a dispatch decision for each region according to its spatial correlation with other regions for each time step. 
