% Figure environment removed

In this section, we propose a novel MARL algorithm named \textbf{r}obust \textbf{E}-AMoD \textbf{Ba}lancing \textbf{MA}RL (REBAMA, its brief framework structure is in Fig. \ref{fig_alg}) to solve the proposed min-max problem \eqref{minmax} with policy constraints, i.e.,
\begin{align}
    \label{minmax_constraint}
    \max_{\mu_r}\min_{\mu_a}J (\mu_r, \mu_a) := \mathbb{E}[v^{\mu_r, \mu_a}(s)] \\
    \text{s.t. }   \mu_r(o_{r,i}) \in {D}_r,\ \mu_a(o_{a,i}) \in {D}_a, \nonumber\\ \forall o_{r,i}, o_{a,i} \in \mathcal{O}, \forall i = {1, \cdots, N}, \nonumber
\end{align}
where the constraints domains $D_r = \{a_{r,i} = (\overline{p}, \overline{q}) \in \Delta^{n_i} \times \Delta^{n_i} |\sum_j \bar p_j = \sum_j \bar q_j \equiv 1\}$ and ${D}_a = \{ a_{a,i} = [\delta_d,\delta_c, \delta_v] : d_l \leq \delta_d \leq d_u; c_l \leq \delta_c \leq c_u; v_l \leq \delta_v \leq v_u\}$. 
In summary, the produced rebalancing and charging actions should satisfy the normalization constraints, and adversarial agents' actions should meet the box constraints as we have defined in section \ref{sec_game}. 
In this way, ${D}_r, {D}_a$ are convex and closed sets. 

\subsection{Centralized Training Decentralized Execution Framework}
We design an actor-critic algorithm under the centralized training decentralized execution (CTDE) framework. Value-based RL algorithms such as Q-learning \cite{mnih2015human_nature} cannot solve our problem since they cannot be used for continuous spaces. 
Traditional RL algorithms such as policy gradient and actor-critic are poorly suited for multi-agent environments, because the environment becomes non-stationary from any individual agent's perspective as other agents' policies change during training. The CTDE framework is proposed to solve this non-stationary issue and has been utilized in several MARL algorithms~\cite{lowe2017multi}. It extends actor-critic methods that the critic can use extra information about the policies of other agents to ease training, while the actor is only permitted to use local information. In general, in an $N$-agent Markov game with a set of agent policies $\{ \mu_1,...,\mu_N\}$ parameterized by $\{ \theta_1,...,\theta_N\}$ respectively, the critic $q_i(s, a_1,...,a_N)$ is a centralized action-value function that computes the Q-value for agent $i$ based on all agents' action and state information. The gradient for updating policy $\mu_i$ using multi-agent deep deterministic policy gradient algorithm (MADDPG) \cite{lowe2017multi} is:
\begin{align}
\label{maddpg:gradient}
    \nabla_{\theta_i}J(\theta_1, \cdots, \theta_N) = \mathbb{E}_{\mathcal{D}} [\nabla_{\theta^i} \mu_i(o_i) \nabla_{a_i} q_i(s,a_1,...,a_N)],
\end{align}
where $a_i = \mu_i(o_i)$, $o_i$ is agent $i$'s observation, $\mathcal{D}$ is a replay buffer. 

\subsection{Projection Procedure in Policy Training}
We first design a projection procedure in policy training to satisfy the policy constraints. We define a projection operator as
$\Pi_\mathcal{A}(a) = \argmin_{z \in \mathcal{A}} \|a - z\|_2,$ 
where $\mathcal{A}$ is the action space (constrained space), $a$ is an input (potentially infeasible action), $a^p = \Pi_\mathcal{A}(a)$ is the output (projected action). When all constraints on action are linear, the constrained action space $\mathcal{A}$ is a polytope and we can use $\mathcal{H}$-representations to express the constraints as an intersection of $u$ half-spaces: $\mathcal{A} = \{a : \textbf{c}_j^{T}a \leq \textbf{e}_j, j = 1,...,u\}$. Thus, given $\mathcal{A}$ and a potentially infeasible action $a$, we can get a projected action $a^p$ by using Dykstra's projection algorithm~\cite{Gaffke1989}. 
It can generate sequences $a_j^n$ and $I^n_j$, $n \in \mathbb{N},j = 1,...,u$, which is recursively computed as follows:
$$a_j^n = \mathcal{P}_j(a_{j-1}^n - I_j^{n-1}| \mathcal{A}),
        I_j^n = a_j^n - (a^n_{j-1} - I_j^{n-1}),$$
with initial values $a^0_u = a$, $I_j^0 = \textbf{0}$, $j = 1,...,u$ and notations $a_0^n = a^{n-1}_u, n\in \mathbb{N}$, $\mathcal{P}_j$ the projection onto the $j$-th halfspace. The sequence of variables $a_j^n$ and $I_j^n$ is guaranteed to converge to the projection since $\mathcal{A}$ is the intersection of closed convex sets~\cite{Gaffke1989}.


\subsection{Robust E-AMoD Balancing Multi-Agent Reinforcement Learning Algorithm}
We summarize our proposed Robust E-AMoD Balancing Multi-Agent Reinforcement Learning Algorithm (REBAMA) in Algorithm \ref{alg: summary}. REBAMA adopts actor-critic and centralized training decentralized execution framework to deal with continuous state and action spaces, and the non-stationary issues in multi-agent environments. REBAMA also utilizes a policy regression scheme during training to accommodate the policy constraints.

In our algorithm, we use parameterized policies for region and adversarial agents, denoted by $\mu_r(\cdot | \theta^r)$ and $\mu_a(\cdot | \theta^a)$, respectively. Since region and adversarial agents get opposite rewards, their value functions are also opposite. We therefore use one parameterized critic $q(\cdot|\theta^q)$ in our algorithm so that $-q(\cdot|\theta^q)$ becomes the critic of adversarial agents. The superscripts of the parameters are occasionally omitted when there is no confusion. 

We first initialize policies and critic neural networks, target neural networks, and the global state of the environment. For each $i \in \mathcal{N}_a$, adversarial agent $i$ receives the input $o_{a,i} := \{s_i,s_{\mathcal{N}(i)}\}$ of its policy, and get its action $a_{a,i} = \mu_a (o_{a,i}|\theta^a)$. Then according to the perturbation rule defined in \eqref{def:map}, we get the corresponding region agent $i$'s input $o_{r,i} :=\{ \Tilde{s}_i, s_{\mathcal{N}(i)}\}$, which contains perturbed state information. 
The balancing decision $a_{r,i} = \mu_r (o_{r,i}|\theta^r)$ is then calculated. We get the adversarial and region agents' joint actions $a_a, a_r$ by repeating the above steps for all $i$. After executing the region agents' joint actions, we store a transition $(s, {a}_{r}, {a}_{a}, r, s^{\prime})$ in a replay buffer $\mathcal{D}$, where $r$ is a shared reward, and $s^{\prime}$ is the next state. 

At each training iteration, we sample a minibatch of transitions $\mathcal{B}$ from $\mathcal{D}$ and update policies using the following loss function with policy regression:
\begin{align}
\label{policy_reg}
    &\mathcal{L}^p(\theta) = \sum_{\mathcal{B}} \|\mu(o|\theta) - \hat{a}\|^2,\\
\label{def:a}
    &\hat{a} = (1-\delta)\Pi_\mathcal{A}(\mu(o|\bar{\theta})) + \delta x,\\
\label{def:x}
    &x = \argmax_{x \in \mathcal{A}} x^{T}\nabla_a \phi q(s, a_a^\prime, a_r^\prime|\bar\theta^q),
\end{align}
where $\delta$ is a positive step size, $a_a^\prime = \Pi_{\mathcal{A}_a}(\boldsymbol{\mu}_a(\boldsymbol{o}_a | \bar\theta^a))$, and $a_r^\prime = \Pi_{\mathcal{A}_r}(\boldsymbol{\mu}_r(\boldsymbol{o}_r| \bar\theta^r))$. $\phi$ is a coefficient whose value is given later. $\bar\theta^a, \bar\theta^r, \bar\theta^q$ are snapshots of the current actor and critic parameters. We use $\boldsymbol{\mu}_a(\boldsymbol{o}_a) := (\mu_a(o_{a,1}), ... ,\mu_a(o_{a,N}) )$ to denote the joint adversarial policy and $\boldsymbol{\mu}_r(\boldsymbol{o}_r)$ has a similar definition. When updating region agent's policy i.e. $\theta_t = \theta^r_t$, we have $o = o_{r,i}, \phi = 1, a = a_{r,i}^\prime, \mathcal{A} = \mathcal{A}_{r,i}$. Otherwise $o = o_{a,i}, \phi = -1, a = a_{a,i}^\prime, \mathcal{A} = \mathcal{A}_{a,i}$. $i$ is the region index. The centralized action-value function $q$ is updated using the following loss function:
\begin{align}
\begin{split}
    &\mathcal{L}(\theta^q) = \sum_{\mathcal{B}} [q(s, a_a, a_r) - y ]^2,\\
    &y = r + \gamma q^{\prime}(s^{\prime}, a_a^{\prime}, a_r^{\prime})|_{a_a^{\prime}=\boldsymbol{\mu}_a(\boldsymbol{o}_a), a_r^{\prime} = \boldsymbol{\mu}_a(\boldsymbol{o}_r)},
    \end{split}\label{update:critic}
\end{align}
where $q^\prime$ is the target critic neural network.
\begin{algorithm}
 \caption{REBAMA Algorithm}
 \label{alg: summary}
 \begin{algorithmic}
 \renewcommand{\algorithmicensure}{}
 \STATE \textbf{Initialize} critic $q(s,a_a,a_r)$, region policy $\mu_r$ and adversarial policy  $\mu_a$, parameterized by $\theta^q,\theta^r,\theta^a$ respectively.
 \STATE \textbf{Initialize} replay buffer $\mathcal{D}$, target networks $q^{\prime}, \mu_r^{\prime}, \mu_a^{\prime}$ with weights $\theta^{q\prime} \leftarrow \theta^q, \theta^{r \prime} \leftarrow \theta^r, \theta^{a \prime} \leftarrow \theta^a$.
  \FOR {$\text{episode} = 1 \text{ to } M$}
  \STATE \textbf{Save} weights $\bar{\theta}^r \leftarrow \theta^r, \bar{\theta}^a \leftarrow \theta^a, \bar{\theta}^q \leftarrow \theta^q$,
  \FOR{ $t = 1 \text{ to } T$}
  \STATE \textbf{Receive} the initial state $s_1$,
  \STATE \textbf{Adversarial} agents select actions $a_{a,t,i} = {\mu}_a(o_{a,t,i}|\bar{\theta}^a)$ and region agents select actions $a_{r,t,i} = {\mu}_r(o_{r,t,i}|\bar{\theta}^r)$ where $i = 1,...,N$.
  \STATE \textbf{Execute} joint action ${a}_{r,t}$, then get reward $r_t$, next state $s_{t+1}$ and store $(s_t, {a}_{r,t}, {a}_{a,t}, r_t, s_{t+1})$ in $\mathcal{D}$.
  \STATE \textbf{Sample} a random minibatch $\mathcal{B}$ from $\mathcal{D}$.
  \STATE \textbf{Update} policies $\mu_r, \mu_a$ using loss function~\eqref{policy_reg}.
  \STATE \textbf{Update} critic $Q$ according to loss function (\ref{update:critic}).
  \STATE \textbf{Update} target networks $\theta^{q\prime} \leftarrow \tau\theta^q + (1-\tau)\theta^{q\prime}$, $\theta^{r\prime} \leftarrow \tau\theta^r + (1-\tau)\theta^{r\prime}$, $\theta^{a\prime} \leftarrow \tau\theta^a + (1-\tau)\theta^{a\prime}$.
  \ENDFOR
  \ENDFOR
 \end{algorithmic} 
 \end{algorithm}
\begin{Proposition}
When there are no constraints on the action space, the policy gradient of REBAMA can be directly calculated by $\phi \sum_{\mathcal{B}} \nabla_{a} q(s,a_a,a_r | \bar\theta^q)\nabla_{\theta}\mu(o|\theta)$ where $a_a = \boldsymbol\mu_a(\boldsymbol{o}_a|\bar\theta^a), a_r = \boldsymbol\mu_r(\boldsymbol{o}_r|\bar\theta^r)$. 
\end{Proposition}
\begin{proof}
When there are no constraints on the action space, $\hat{a}$ is equivalent to $\mu(o|\bar\theta) + \eta \nabla_{a} \phi q(s,a_a^\prime,a_r^\prime|\bar\theta^q)$ where $\eta$ is a positive step size, $a_a^\prime = \Pi_{\mathcal{A}_a}(\boldsymbol{\mu}_a(\boldsymbol{o}_a | \bar\theta^a))$, $a_r^\prime = \Pi_{\mathcal{A}_r}(\boldsymbol{\mu}_r(\boldsymbol{o}_r| \bar\theta^r))$ Then the gradient of \eqref{policy_reg} equals to:
\begin{align*}
    \nabla_{\theta} \mathcal{L}^p =  2\eta\phi \sum_{\mathcal{B}} \nabla_{a} q(s,a_a^\prime,a_r^\prime|\bar\theta^q)\nabla_{\theta}\mu(o|\theta),
\end{align*}
Comparing to the deterministic policy gradient in~\eqref{maddpg:gradient}, we can see that the policy updating procedure in REBAMA with a learning rate of $\eta_1$ is equivalent to that in deterministic policy gradient with a learning rate of $2|\mathcal{B}|\phi\eta\eta_1$.
\end{proof}











