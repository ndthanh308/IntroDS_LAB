
We use real-world E-taxi data from Shenzhen city to conduct experiments. Three different data sets~\cite{dro_he, wang2019shared} including E-taxi GPS data (vehicle ID, locations, time and speed, etc), transaction data (vehicle ID, pick-up and drop-off time, pick-up and drop-off location, travel distance, etc) and charging station data (locations, the number of charging points, etc) are used to build an E-AMoD system simulator as the training and evaluation environment. 
To test the robustness of our proposed robust method, we inject a Gaussian noise follows $\mathcal{N}(0,1)$ into the state when testing vehicle balancing methods. The simulated map is set as a grid world. The policy networks and critic network are two-layer fully-connected networks, both with 30 nodes. For the policy networks and critic network, the first and second hidden layers' activations are Tanh and ReLu, the output layer is Softmax and Linear,   respectively. We set the batch size $= 600$, the discount factor $= 0.99$, the time interval $= 0.5$ hour, and use the Adam optimizer with a learning rate of $0.001$. For fair comparisons, we use fixed random seeds to initialize simulations in the testing.

\subsection{Comparison of Robust MARL Method and Robust Optimization Method}
We first compare our robust MARL method (REBAMA) with robust optimization method~\cite{dro_he} in table~\ref{tab:model-base}. We test these two methods in the simulator for 5 times with 5 different fixed random seeds when state uncertainty is present,  then compare various average metrics. The proposed REBAMA algorithm performs better on average reward, average fairness of supply-demand ratio and average fairness of charging utilization which is increased by 8.21\%, 9.42\%, 8.29\%, respectively. Though the robust optimization method also considers demand and supply uncertainties, it heavily depends on predetermined parameters such as probability transition function, prediction models, etc. This heavy dependence makes the robust optimization method can be only robust for specific scenarios and not sufficient to capture the complexity of E-AMoD systems. Our REBAMA algorithm is better at capturing E-AMoD system’s demand and supply uncertainties.
\begin{table}[]
\centering
\vspace*{6pt}
\caption{Robust MARL \textbf{VS} Robust Optimization Method}
\vspace*{-6pt}
\begin{tabular}{cccc}
\hline
$\text{Metric}$     & \text{Robust Opt}     & \text{Robust MARL} & \text{Increasing Rate} \\ \hline
$\text{average reward}$ & $-15.83$ & $-14.53$  & $\uparrow8.21\%$  \\\hline
$\text{average } u_s$ &  $-7.01$ & $-6.35$  &  $\uparrow9.42\%$ \\\hline
$\text{average } u_c$ &  $-8.82$ & $-8.18$ &  $\uparrow8.29\%$ \\\hline
\end{tabular}
\label{tab:model-base}
\vspace{-10pt}
\end{table}
% Figure environment removed
% Figure environment removed
% Figure environment removed


\subsection{Comparison of Robust and Non-Robust MARL Methods}
We then compare our proposed robust multi-agent reinforcement learning (MARL) approach to a commonly-used and well-performed non-robust MARL algorithm, i.e. multi-agent deep deterministic policy gradient (MADDPG)~\cite{lowe2017multi} which does not consider supply or demand uncertainties. We train our models until convergence and then evaluate them by averaging various metrics for 5 iterations. To avoid constraint violations, we also apply policy regression to MADDPG.
We report the mean episode rewards as a function of the training episodes in Fig. \ref{fig:reward}. The proposed REBAMA algorithm learns a better policy in terms of mean episode reward which is increased by 19.28\% compared with MADDPG when state uncertainty is present. Higher rewards mean the city has at least a better balanced supply-demand ratio or a fairer charging utilization. 

Since a reward is a weighted sum of fairness of charging utilization $u_c$ and supply-demand $u_s$ defined in \eqref{def:reward} and \eqref{def:u}, we also compare mean episode $u_c$ and $u_s$ of each episodes in Fig.~\ref{fig:u} and Fig.~\ref{fig:sr}. A larger $u_c$ or $u_s$ means a better balanced or fairer service across the city. Our REBAMA algorithm learns a better policy compared to the non-robust algorithm, where the charging utilization fairness and supply-demand fairness are increased by 28.18\%, 3.97\% respectively. Furthermore, we find that REBAMA improves the charging utilization greatly compared to the non-robust algorithm. This can be explained by the fact that charging utilization is very sensitive to EAVs’ supply-side uncertainty.

\subsection{Effectiveness of Policy Regression and Projection}
In Fig.~\ref{fig:projection}, we compare our REBAMA and the MADDPG algorithm without policy regression and projection to study the effectiveness of adding projection and policy regression steps in the training process. Without them, MADDPG stops training in earlier episodes when the policy outputs an infeasible action. REBAMA can handle this interruption problem then the training process will not stop before the training is done.

% Figure environment removed