
The Electric Autonomous Mobility on Demand (E-AMoD) system is an energy-efficient and sustainable alternative to private urban mobility by using Electric Autonomous Vehicles (EAVs) to provide one-way rides to passengers \cite{zardini2021analysis_survey}. With E-AMoD, passengers express their travel needs through mobile applications, phone reservations, or street hails. Vacant EAVs then provide passengers with ride services \cite{shaheen2020mobility}. Besides, the concept of shared use of a vehicle reduces the total travel costs and urban infrastructure needed for parking and increases vehicle utilization \cite{wen2017rebalancing}. In light of it, Pony.ai piloted its first commercial E-AMoD service in 2019~\cite{pony_ai}. E-AMoD system has therefore been recognized as one of the most promising systems to address the challenge of the growing mobility needs and oil dependency. %

However, E-AMoD systems operation presents challenges in multiple aspects. Passenger demands are usually asymmetry distributed because of the spatiotemporal urban mobility nature \cite{gammelli2021graph_cdc}. This problem gets severe in rush hours when passengers travel in similar directions, such as from residential areas to work areas at morning peak. Without re-balancing, many idle EAVs can potentially aggregate in particular areas.

% Figure environment removed


Therefore, we study the vehicle balancing problem \cite{zardini2021analysis_survey, dro_he} for E-AMoD systems in this work. In specific, we consider two main scheduling tasks for operating an EAV fleet, namely (i) vehicle rebalancing, to reposition idle EAVs to other locations, and (ii) charging scheduling, i.e., assigning the charging stations for low-battery EAVs. Recent work regarding vehicle balancing methods can be classified into three typical categories: (1) heuristic methods; (2) optimization-based approaches and (3) Reinforcement Learning (RL)-based approaches. The rule-based heuristics usually lead to sub-optimal solutions \cite{liu2019dynamic_heuristic, vandael2015reinforcement_heuristic}. Optimization-based approaches usually propose an optimization problem based on the system dynamic model \cite{AMoD_queue, xie2019optimal, ddrobust_Miao}. The performance of these methods is therefore heavily affected by modeling knowledge. RL-based methods formulate the problem as a Markov Decision Process (MDP) and apply RL algorithms to find the optimal balancing policy \cite{EVcharge_19, he2020spatio}. We provide more discussions in the Related Work section.

Nevertheless, state uncertainties from different sources may result in the degradation of vehicle balancing decisions. We provide an instance in Fig. \ref{fig_mismatch}, when there is a difference between the true system information and the prediction of mobility demand and vehicle supply, the vehicle balancing methods do not have performance guarantees. It is necessary to consider multiple uncertainties in E-AMoD systems. The EAVs' unique charging patterns (long charging time,  high charging frequency and unpredictable charging behaviors, etc.) increase systemic supply uncertainties~\cite{pricing_20, p2charge, jointcharging}. E-AMod systems' characteristics (unpredictable long daily driving time, uncertain sporadic demands and dispersed mobility patterns, etc.) also increase the uncertainties in demands prediction~\cite{epat, dddro_tcps20}. Existing EAMod balancing algorithms usually do not consider EAV supply uncertainties~\cite{p2charge, pricing_20}, or only consider mobility demand uncertainties~\cite{mpcmod_icra16, dddro_tcps20}.

Hence, in this work, we develop a multi-agent reinforcement learning (MARL) based robust EAV balancing framework for E-AMoD systems, in which region agents make fair vehicle balancing decisions, and adversarial agents  model the state uncertainties including both passenger demand and EV supply uncertainties. Our \textit{main contributions} are as follows:

\begin{itemize}
    \item To the best of our knowledge, we are the first to formulate the E-AMoD system vehicle balancing problem under demand and supply uncertainties as a robust multi-agent reinforcement learning problem under state uncertainties. Via a proper design of the agent, state, action, and reward, we set the goal of the problem as balancing the whole city's charging utilization and mobility service quality.
    \item We design a \textbf{r}obust \textbf{E}-AMoD \textbf{Ba}lancing \textbf{MA}RL algorithm (REBAMA) to train robust policies for providing fair mobility and charging services. It adopts the centralized training and decentralized execution framework with Dykstraâ€™s projection and policy regression to keep actions from violating real-world constraints during policy updates.
    \item We run experiments based on real-world E-taxi system data. Experiments show that our proposed REBAMA algorithm performs better in terms of reward, charging utilization fairness and supply-demand fairness, which are increased by 19.28\%, 28.18\%, and 3.97\%, respectively, compared with a non-robust MARL-based method that does not consider system uncertainties.
\end{itemize}

