\subsection{Performance Evaluation}
\label{subsec:evaluation}
In this section, we conduct a series of experiments to confirm \system's superiority compared to the state-of-the-art.

\subsubsection{Comparing to Conventional SPEs}
\label{subsubsec:conventional}
In the first experiment, we show that TSPEs significantly outperform conventional SPEs when handling TSP applications. We use the default workload configuration shown in Table~\ref{tab:default} in this study. 
We have implemented SL on Flink-1.10.0.
Since the native Flink does not support shared mutable state accesses, we leveraged Redis-6.2.6 with a distributed lock, a common workaround, to store shared mutable states.
We deploy a standalone cluster with a single TaskManager configured with 24 slots and set the parallelism of SL to 24.
To avoid the OOM exception, we set the TaskManager heap size to 100GB.
When locking is disabled (denoted as w/o Locks), execution correctness is not guaranteed in Flink.
The detailed workload configuration is shown in Table~\ref{tab:default}.
As shown in Figure~\ref{fig:overview_comparison}, \system significantly outperforms the two state-of-the-art TSPEs, \tstream (1.6x) and \sstore (3.7x), and Flink (up to 117x).
It is noteworthy that Flink, a popular conventional SPE, achieves orders of magnitude lower throughput in this application. By disabling locks, its throughput increases but is still far lower than any of the TSPEs. In the following, we hence do not further compare \system with Flink.

% Figure environment removed

\subsubsection{Evaluation on Dynamic Workloads}
\label{subsubsec:dynamic}
In this experiment, we show that \system can always select a better-performing scheduling strategy under changing workloads, resulting in lower latency and higher throughput compared to state-of-the-art TSPEs. We use SL as the base application and divide the workloads into four phases. 
Figure \ref{fig:SL_Throughput} and Figure \ref{fig:SL_Latency} compare the throughput and latency of \system against two state-of-the-art TSPEs: \sstore~\cite{S-Store} and \tstream~\cite{tstream}. We mark each phase in the dynamic workload using the dotted grey box.
In the first phase, a large number of events consisting of \emph{Deposit} transactions arrive, and the state accesses distribution is scattered. 
As a result, there are lots of \lds and \tds but few \pds. At the same time, the vertex degree distribution is uniform as the state accesses are scattered. 
As guided by our decision model (Figure~\ref{fig:model}), 
\system selects the \se strategy to resolve a large number of dependencies and selects \csu
for scheduling since there are fewer \pds. 
\system achieves up to 1.27 times higher throughput compared to the second-best. 
In the second phase, we configure the workload with increasing key skewness over time. Hence,
dependencies are gradually contented among a small set of states, which facilitates the resolution of dependencies.
As expected, the performance of all approaches drops. 
\system gradually morphs from \se to \nse strategy to resolve dependencies in a more flexible manner, and constantly outperforms \sstore. 
In the third phase, we configure the workload with an increasing ratio of \emph{Transfer} transactions so that one of the two types of transactions in SL is called intensively in a short period of time. 
As the proportion of \emph{Transfer} transactions increases, there are more and more dependencies between scheduling units. Hence, \system gradually morphs from \csu to \fsu to reduce the dependency resolution overhead and result in a stable throughput.

There is no transaction abort in the first three phases, and the selection of aborting mechanism in \system does not matter. In the fourth phase, we increase the ratio of aborting transactions over time to evaluate the performance of the system under a dynamically changing ratio of aborting transactions. 
In the beginning, \system applies the \ea mechanism to eagerly abort when the operation fails and morphs to \la when aborts are frequent so that transaction aborts can be handled together to reduce context switching overhead.
The results show that \tstream's performance drops when transaction aborts appear. 
This is because of the rapidly increasing overhead of redoing the entire batch of transactions. 
In contrast, \system achieves relatively stable performance and is 2.2x to 3.4x higher than other schemes.
 
A further key takeaway from Figure \ref{fig:SL_Latency} is that \tstream and \sstore have significantly higher tail latency than \system. 
This is mainly because the scheduling strategies in \tstream and \sstore are limited for specific workload characteristics. When workload changes, such as increasing transaction aborts or key skewness, their efficiency drops significantly, resulting in higher processing latency. In contrast, \system dynamically morphs the scheduling strategy according to the change of workload characteristics to deal with different situations, thus achieving a constantly lower processing latency.

% Figure environment removed

% Figure environment removed