\subcompact
\subsection{Evaluation Methodology}
We conduct all experiments on a dual-socket Intel Xeon Gold 6248R server with 384 GB DRAM. 
Each socket contains $24$ cores of 3.00GHz and 35.75MB of L3 cache. To isolate the impact of NUMA, we use one socket of the server in our experiments. We leave it as a future work to address the NUMA effect~\cite{briskstream}. We pin each thread on one core and assign 1 to 24 cores to evaluate the system scalability.
The OS kernel is \emph{Linux 4.15.0-118-generic}. 
We use \emph{JDK 1.8.0\_301}, set \emph{-Xmx} and \emph{-Xms} to be 300 GB. We use G1GC as the garbage collector across all the experiments and configure \system to not clear temporal objects such as the processed TPGs and multi-versions of states. 
We show the impact of clean-up and JVM GC in Section~\ref{subsec:memory-footprint}.
%\tony{The xmx and xms?}
%Based on GC logs, we find that no major GC occurs during the execution and minor GC contributes only xxx $\sim$ xxx\% to the total execution time across all the applications.
% The number of cores assigned to the system and the size of punctuation are system parameters that can be varied by users. 
% We vary both parameters in our experiments. 
\begin{comment}
Repeat with Number of Transactions ($T$)
\end{comment}
%We use the punctuation interval of 10240 as a default execution configuration, and vary it from 5120 to 81920 in our sensitivity studies. \tony{Discussion of the default settings of all other system parameters..}

% \subcompact
% \subsubsection{Benchmark Workloads}
We use three use cases: Streaming Ledger (SL), GrepSum (GS), Toll Processing (TP) from a benchmark proposed by our previous work~\cite{tstream} on evaluating the effectiveness of \system. For all these workloads, we follow the original application logic but tweak the configurations to bring more workload dependencies such that we can better expose the issues of existing TSPEs. In particular, 
% compared to the original~\cite{tstream}, 
we have configured ten times larger sizes of shared mutable states and generated more state transactions accessing overlapped states in our workload settings.
Additionally, we further present two additional use cases that process the real-world datasets: Online Social Event Detection~\cite{sahin2019streaming,olteanu2014crisislex} (OSED) and Stock Exchange Analysis~\cite{sse} (SEA) to illustrate the usefulness of \system in supporting complex real-world data processing scenarios.
%In particular, we have configured 10 times larger sizes of shared mutable states, and generate more state transactions accessing overlapped states in our workload settings compared to the original.


\textbf{\change{Tuning Workload Characteristics.}}
% The benchmark contains .
% We adopt the same implementation of SL in our system, while we make some modification to GS to make it more realistic. 
To better comprehend the system behaviour, we tune the following six workload characteristics. The default workload characteristics and varying ranges are summarized in Table~\ref{tab:default}. 
\emph{1). State Access Distribution ($\theta$):}
% State access distribution are varying for different workloads, where certain states may be more likely to be accessed than others.
Similar to~\cite{tstream}, we modelled the state access distribution as Zipfian skew, and tune the Zipfian factor to vary $\theta$.
\emph{2). Ratio of Aborting Transactions ($a$):} 
% The proportion of the number of transactions to be aborted. 
We tune $a$ by artificially adding transactions that violate the consistency property, such as the account balance can not become negative.
\emph{3). Transaction Length ($l$):}
We tune $l$ by varying the number of atomic state access operations in one transaction.
% Transaction length represents the number of atomic state access operations that must be decomposed for each transaction.
\emph{4). The complexity of a UDF ($C$):}
% \curry{Due to the \pd} (Definition~\ref{def:PD}), 
% Each operation may carry a user-defined function (i.e., the $f$ in Definition~\ref{def:PD}) with various computational complexity. 
We tune $C$ by adding a random delay in each user-defined function (i.e., the $f$ in Definition~\ref{def:PD}).
\emph{5). Number of State Access Per Operation ($r$):}
% an operation may need to read multiple states for resolving \pd. 
We vary the ratio of multiple state access operations to tune $r$.
\emph{6). Number of Transactions ($T$):} 
We tune $T$ by varying the punctuation interval.
% The number of transactions arriving during the punctuation interval.
%The number of state transactions to be processed together in a batch interval. 
%\tony{Double check, there is no mention about punctuation interval.}

% arrived during the punctuation interval.

% It is noteworthy that we have configured 10 times larger sizes of shared mutable states, and generate more state transactions accessing overlapped states in our workload settings compared to the original setting used by Zhang et al.~\cite{tstream}. Both changes are intended to bring more workload dependencies to better expose the issues of existing TSPEs such as TStream.
\textbf{Dynamic Workload Configurations.}
To evaluate the adaptability of \system, we follow the mechanism proposed by Ding et al.~\cite{dynamicWorkload} to generate dynamic workloads.
% We briefly describe how to generate dynamic workload through the following steps: 
% First, to further support diverse workloads, we cover a series of configurations, such as cyclic dependency, ratio of aborting transactions, computation complexity, number of state access per transaction, ratio of different type transactions, etc. 
% Second, 
% Due to a large number of tunable workload configurations and their wide range of adjustments, it is difficult to adjust all parameters in one dynamic workload. 
% Therefore, 
Specifically, we generate various phases of dynamic workloads by different trends, which determines the parameters we want to tune and how they change over time. For example, in a dynamic workload with an increasing tendency to abort transactions, we will increase the ratio of aborting transactions over time. 
% Finally, we can control the rate of workload transfer by setting the number of events per configuration.

%To comprehensively evaluate \system, we will also tweak both workloads with varying workload characteristics. 

%  we .
% to make it more realistic.
% The \emph{Sum} transactions
% are mainly processed by reading on selected states and performing summation among the obtained state values.
% Different from Zhang et al.~\cite{tstream}, 
% We modify transactions to apply multiple summations on different subsets of values to control the transaction length ($l$).
% In addition, we let each \emph{Sum} transaction write the summation result back to a specified state. For simplicity, we let it write to the first state it reads.
% Hence, the number of state access per operation ($r$) can be controlled by tuning the subset of states for summation.
% By default, we set $l=1$ and $r=10$ in GS.

\begin{table}[]
\centering
\caption{Workload default configuration}
\label{tab:default}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{|p{1.5cm}|p{1.8cm}|p{1.8cm}|p{1.8cm}|p{1.8cm}|p{2cm}|}
\hline
\textbf{Workload Char.} & \textbf{SL} & \textbf{GS} & \textbf{TP}  & \textbf{Tweaking ranges} \\ \hline
% $S$ & 2(tables) $\times$ 100k(records) & 1(table) $\times$ 100k(records) & - \\ \hline
$\theta$ & 0.20 & 0.20 & 0.20 & 0.0$\sim$1.0 \\ \hline
$a$ & 1\% & 1\% & 1\%  & 0$\sim$90\% \\ \hline
% $t$ & writeonly 25\% & writeonly 0\% & writeonly (0\%$\sim$100\%) \\ \hline
$l$ & 2 / 4 & 1 & 2  & 1$\sim$10 \\ \hline
$C$ & 10 us & 10 us & 10 us & 0$\sim$100 us \\ \hline
$r$ & 1 / 2  & 2 & 1  & 1$\sim$10 \\ \hline
$T$ & 10240  & 10240 & 40960  & 5120$\sim$81920 \\ \hline
\end{tabular}
}
\end{table} 

% \textbf{Experimental Setup.}
