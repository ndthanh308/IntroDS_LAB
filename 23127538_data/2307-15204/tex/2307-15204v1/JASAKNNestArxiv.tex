\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{amsfonts, amsmath, amssymb, amsthm, mathrsfs}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{1}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
\addtolength{\topmargin}{-1in}%


\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark} %for numbered remarks and if we want unnumbered remarks we can have "*" in \newtheorem*{remark}{Remark}
\newtheorem*{algorithm}{Algorithm} %to get an algorithm (it's unnumbered because we added *)
\setcounter{secnumdepth}{4}%to get numbered subsubsubsections
\setcounter{tocdepth}{4}%to add the subsubsubsections in the table of content
\theoremstyle{plain}
\newtheorem{assumption}{Assumption} %to get assumptions
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}

%For MIKE: command to reduce wriring

\newcommand{\nn}{\nonumber}
%\bibliographystyle{natbib}

\begin{document}


\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if1\blind
{
  \title{\bf Estimation of Causal Effects Under K-Nearest Neighbors Interference}
  \author{
  	Samirah Alzubaidi\\
    Department of Mathematics, Al-Qunfudah University College,\\ Umm Al-Qura University \\
    and \\
    Michael J. Higgins \\
    Department of Statistics, Kansas State University}
  \maketitle
} \fi

\if0\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Estimation of Causal Effects Under K-Nearest Neighbors Interference}
\end{center}
  \medskip
} \fi



% \author[1]{Alzubaidi, Samirah H.}
% \author*[2]{Higgins, Michael J.}

%   \affil[1]{Department of Mathematics, Al-Qunfudah University College, Umm Al-Qura University, Al-Qunfudah, Saudi Arabia, 28821; E-mail: shzubaidi@uqu.edu.sa}

%   \affil[2]{Department of Statistics, Kansas State University, Manhattan, KS, 66506; E-mail: mikehiggins@ksu.edu}
 
\bigskip
\begin{abstract}

Considerable recent work has focused on methods for analyzing experiments which exhibit \textit{treatment interference}---that is, when the treatment status of one unit may affect the response of another unit.
Such settings are common in experiments on social networks. 
We consider a model of treatment interference---the $K$-nearest neighbors interference model (KNNIM)---for which the response of one unit depends not only on the treatment status given to that unit, but also the treatment status of its $K$ ``closest'' neighbors.
We derive causal estimands under KNNIM in a way that allows us to identify how each of the $K$-nearest neighbors contributes to the indirect effect of treatment.
We propose unbiased estimators for these estimands and derive conservative variance estimates for these unbiased estimators.
We then consider extensions of these estimators under an assumption of no weak interaction between direct and indirect effects.
We perform a simulation study to determine the efficacy of these estimators under different treatment interference scenarios.
We apply our methodology to an experiment designed to assess the impact of a conflict-reducing program in middle schools in New Jersey, and we give evidence that the effect of treatment propagates primarily through a unit's closest connection.



% We show how these estimators extend these estimators under


% into a sum of the effects due to each of the $K$ closest neighbors.
% %the indirect effect of treatment into the effects due to the $K$-nearest neighbors.  


% Causal inference in the presence of interference has been a trend in the past decade. 
% In randomized experiments, assessing causal effects requires special care when the treatment condition assigned to one unit is allowed to affect the response of other units.
% In this paper, we extend the potential outcomes approach and the K-nearest neighbors interference framework and define causal effects under the K-neighborhood assumption.
% We provide unbiased estimators of the defined estimands given exact marginal and joint probabilities for possible treatment exposures where we uncover the indirect effects of the K-nearest neighbors.
% We derive conservative standard errors for these estimators.
% We then demonstrate how these estimators may have significantly stronger precision under an assumption of no interaction between direct and indirect effects.
% We verify the efficacy of these methods via simulation and application to a field experiment
% designed to reduce conflict among middle school students in New Jersey.
\end{abstract}

\noindent%
{\it Keywords:}  Causal inference under
interference; Network effects; Randomization inference; Peer effects; Spillover 
\vfill

\newpage
\spacingset{1.9} % DON'T change the spacing!
\section{Introduction}
\label{sec:intro}

%MIKE: We may need to blind the Alzubaidi and Higgins reference for the blinded version that we submit.

In randomized experiments, assessing causal effects requires special care when the treatment condition assigned to one unit is allowed to affect the response of other units. 
Under this setting, a 
%; that is, units are allowed to interact with each other, and so, the \
unit's outcome is not only influenced by its own treatment status---a \textit{direct effect} of treatment---but may also be influenced by other units' treatments---an \textit{indirect effect}~\citep{sobel2006randomized, rosenbaum2007interference, hudgens2008toward}.
In causal inference terminology, these experiments exhibit 
\textit{treatment interference}, \textit{treatment spillover}, \textit{network effects}, or \textit{peer effects}.
 This interference is especially common in settings with a social factor where units are allowed to interact with each other, for example, in studies on social media networks.
% there is a social factor---interaction between units such that the unit's outcome is not only influenced by its own treatment (i.e. direct effect) but also by other units treatments (i.e. indirect effect).

Experiments exhibiting treatment interference violate 
the \textit{stable unit treatment value assumption} (SUTVA)---a foundational assumption of traditional causal inference methods~\citep{splawa1990application, rubin1980randomization, holland1986statistics, imbens2015causal}.
%MIKE: NEYMAN, RUBIN, HOLLAND TRIO FOR SUTVA HERE.(Done).
In particular, SUTVA requires that the treatment assigned to one unit affects only the outcome of that unit and does not affect the outcomes of other units.
%\citep{rubin1980randomization}.
The presence of interference may complicate statistical analysis and lead to inaccurate inference if not carefully taken into account~\citep{sobel2006randomized}.

% ---that treatment assigned to one unit affects only the outcome of that unit and does not affect the outcomes of other units---which is 

Traditionally, the effect of social influence and interaction between units has been viewed as a nuisance prohibiting accurate estimation of the direct effect of treatment.
Considerable work has focused on designing experiments to mitigate the effect of treatment interference, for example, through clustering units that are likely to interact with each other and assigning treatment to these clusters instead of individual units~\citep{ugander2013graph, gui2015network, eckles2016design}. 
However, recent applications---for example, studies conducted on social media platforms and those evaluating the efficacy of vaccination strategies---have giving rise to studies in which quantifying and estimating interference effects is of primary interest~\citep{hudgens2008toward, aronow2017estimating, forastiere2020identification,  sussman2017elements, toulis2013estimation,  alzubaidi2022detecting}.

%Allowing for arbitrary interference between units when analyzing indirect effects is infeasible; such approaches would make direct and indirect effects unidentifiable.  such approaches dramatically and may lead to a lack of power. and/or lacking in power.
Methods for estimating indirect effects often begin by classifying the interaction through defining an exposure mapping on the units under study---a network where nodes represent units under study and edges between vertices indicate that the corresponding units may interact with each other.
There may additionally be an interaction measure computed between each pair of units in the exposure mapping indicating the strength of that interaction.
Common settings include partial interference, in which units are partitioned into groups and treatment interference can persist within groups but not across groups~\citep{sobel2006randomized, rosenbaum2007interference, hudgens2008toward,  tchetgen2012causal, basse2018analyzing}, and neighborhood interference, in which interference is not allowed to persist outside of a small neighborhood of a unit~\citep{sussman2017elements}.  
Once the nature of the treatment interference is specified, estimators can then be defined to estimate both direct and indirect effects~\citep{aronow2017estimating}.
%a units response within a given neighborhood of that unit.  
%only units within a small neighborhood of a given unit can interfere with the response of that ua unit's indirect effect is isolated to  can only occur within a small neighborhood of that unit.
%indirect effects can only for a unit is restricted to a small neighborhood of that unit.
%a unit can only interfere with units within a small neighborhood.
%can only persist within a small neighborhood of a unit.
%---is a prominent special case of this intereference
%Another approach assumes that treatment interference is restricted to a small neighborhood~\citep{sussman2017elements}.
% \citet{aronow2017estimating} develop general estimation methods of treatment effects under arbitrary but known forms of interference.
% % MIKE: Need some examples and cites (Samirah:Done).
% Indirect effects are then estimable through making assumptions on this exposure mapping and interaction measure, for example, by allowing interaction if the interaction measure is sufficiently large,... etc. 

%Consequently, identification of the direct effect of treatment and the social influence on the response has become the primary interest of researchers in causal inference studies
% in the literature has been considered such that highly connected units are included in the same cluster and the randomization is performed on the cluster level.
%Examples of this include studies on the efficacy of vaccines in which vaccinated and non-vaccinated members of a population interact with each other and researchers are interested in overall infection rates.   
% interaction between vaccinated and non-vaccinated members of the population---which is a form of treatment interference---is  result is vaccinating against infectious diseases where potential transmission of the disease results in interference which is of interest in such settings.


We build on this literature by developing estimators for these 
%estimation of 
%direct and indirect 
%these 
treatment effects under the $K$-nearest neighbors interaction model (KNNIM)~\citep{alzubaidi2022detecting}.
In this model, the treatment given to one unit may interfere with response another unit if the first unit is one of the $K$ ``closest'' individuals to the second unit with respect to a given interaction measure.
Quantifying $K$-nearest neighbors effects may help researchers tease out peer effects induced by, for example, interactions between best friends, spouses, siblings, and/or close colleagues.
Additionally, this model has several appealing properties.
First, this model allows for users with stronger interactions to produce larger indirect effects than users with weaker interactions, and will ignore potential indirect effects due to dilapidated, but technically present, connections (e.g.~Facebook friends that no longer interact with each other).  
Second, the marginal and joint probabilities for possible treatment exposures have closed-form expressions under common experimental settings, allowing for unbiased estimation of treatment effects and precise estimation of standard errors.
%MIKE: May need to change the name of mogul to something else if there is a better name in the literature (Samiah: not sure).
%MIKE: THIS SENTENCE COULD BE ON THE CHOPPING BLOCK.
Finally, KNNIM may be effective in estimating indirect effects in the presence of non-transitive \textit{influencer} effects---effects induced by users that have influence over a large number of individuals, but may themselves only directly interact with a handful of individuals.

%MIKE: Just for MIKE: Paragraph about what we do and paragraph with structure of paper.
%MIKE: Moving this up.
%Quantifying the $K$-nearest neighbors effects (treatment effects induced by interactions between best friends, spouses, siblings, coworkers, social media moguls and their followers, etc.) may answer many interesting questions in real-world experiments of how a unit can be affected by its $K$- nearest neighbors.

%MIKE: Taking out the Alzubaidi cite here since it is included in the previous paragraph. 
In this paper, using a potential outcomes approach, we define causal estimands for direct and $K$-nearest-neighbor indirect effects.  
We then derive Horvitz-Thompson estimators \citep{horvitz1952generalization} for these estimands that are unbiased given exact marginal and joint probabilities for possible treatment exposures, and derive conservative standard errors for these estimators. 
We provide a closed-form solution to compute these exposure probabilities under completely-randomized and Bernoulli-randomized experimental designs.
We then demonstrate how these estimators may have significantly stronger precision under an assumption of no interaction between direct and indirect effects.
We conclude by showcasing the efficacy of these methods via simulation and application to a field experiment designed to reduce conflict among middle school students in New Jersey~\citep{paluck2016changing, aronow2017estimating}.

%we develop the theory for estimation of direct and indirect effects under the KNNIM framework for completely-randomized and Bernoulli-randomized experimental designs.
%Under a potential outcomes approach, we define causal estimands for direct 
%approach and the $K$-neighborhood interference to define causal estimands where interference is allowed within $K$-neighborhoods of individuals.

%MIKE: WILL NEED TO DOUBLE CHECK PRIOR TO SUBMISSION
The paper is organized as follows.
Section \ref{makereference3.2} sets up the notation and preliminaries.
The $K$-nearest neighbors interference model is provided in Section \ref{makereference3.3}.
Section \ref{makereference3.4} defines causal effects under KNNIM.
Proposed unbiased estimators with the derived properties are given in Section \ref{makereference3.5}. 
Section \ref{makereference3.6} provides estimators with stronger precision under an assumption of no interaction between direct and indirect effects.
Section \ref{makereference3.7} presents variance estimation.
Simulation studies and real data analysis to demonstrate the proposed methods are provided in Sections \ref{makereference3.8}, \ref{makereference3.9}, and \ref{makereference3.10}. We conclude in Section \ref{makereference3.11}.
% Additionally, using inverse probability weighting, we then propose Horvitz-Thompson unbiased estimators for the defined estimands under $K$-neighborhood interference assumption and then we strengthen this assumption by assuming no interaction between direct and indirect effects and proposing new estimators under this assumption.
% Moreover, we derive properties of the proposed estimators and provide conservative variance estimators of the defined estimands.
% To demonstrate the proposed causal methods, a simulation is conducted to show the estimates of the two assumptions under completely randomized design and Bernoulli randomization.

\subsection{Motivating Example: Reducing Conflict in Schools}
\label{sec:motivexamp}

To motivate our approach, we refer to a recent randomized field experiment to assess the efficacy of an anti-conflict program on middle schools in New Jersey~\citep{paluck2016changing}. 
%in 56 schools in New Jersey \citep{paluck2016changing}. 
The experiment was explicitly designed to determine whether benefits of the program can be propagated through social interactions between students.
In particular, the program was implemented through randomly selecting ``seed students'' from a pre-selected list of eligible seeds within all participating schools.
Seed students were asked to actively participate in and advocate for the anti-conflict program.
School-wide benefits of the program were then propagated through interactions with these seed students.
Analysis was performed only on students that were eligible to be seeds ($N =$ 2,451).
Overall, the program showed a statistically significant improvement in anti-conflict behaviors among students. 
For details of the randomization and analysis, please refer to~\citet{paluck2016changing} and~\citet{aronow2017estimating}.

Notably, to assess potential pathways for treatment interference, students were asked to identify, in order, the 10 other students that they spent the most time with during the previous few weeks.  
This yields a unique dataset in which the strength of the interaction between two individuals under study is explicitly recorded.
Hence, an interference model that allows for direct incorporation of these relative strengths of the interactions, such as KNNIM, can yield additional insights into the structure of the treatment interference.
For example, in Section~\ref{makereference3.10}
, we give evidence that the indirect effect of the treatment was propagated primarily through the student with the ``strongest connection'' to the seed student.   

% that allows for direct incorporation of the relative strengths of the interactions.
% %to be directly incorporated.  
% For this dataset, KNNIM models with $K$ up to 10 may be applicable.

% An analysis performed by \citet{aronow2017estimating} estimated the indirect effect of being a seed student on wearing an orange wristband to be about 0.15.  
% That is,
% %such that having a treated peer increase the probability of wearing a wristband by about 15\%.
% students exposed to treated peers were about 15\% more likely to report wearing an orange wristband in comparison to students in control schools.

% The program was implemented in the following way.  
% First, across 56 schools in New Jersey, a set of potential ``seed'' students were identified.  
% Of these 56 schools, half were randomly chosen to have the program implemented in their school.
% randomly selected to participate in
% implement this program.
% The intervention was administed through seed students---students that are selected to actively participate and advocate for the anti-conflict program., first, a set of potential ``seed'' students were identified.  
% Then, a set, and approximately half of these potential seeds were randomly selected to become seed students. 
% . 
% These seed s---those that are selected to actively participate and advocate for the anti-conflict program.
% %The intervention was administered through ``seed'' students---those that are selected to actively participate and advocate for the anti-conflict program.
% These students attended meetings with the program staff every two weeks to address conflict behaviors in their schools and to talk about strategies to mitigate peer conflict.
% Additionally, seed students were encouraged to publicly reflect their opposition to conflict in their school---for example, identifying a common conflict in their school and creating a hashtag about it---and were also asked to distribute orange wristbands with the intervention logo to students that demonstrate anti-conflict attitudes.

% Seed students were randomly assigned as follows.  
% First, within each of the 56 schools, between 40 and 64 students were identified as being eligible to be seed students.
% Then, from the 56 schools in the study, 28 schools were randomly assigned to receive the anti-conflict program.
% Finally, within each of these assigned schools, half of the eligible students were selected to be seed students.
% Analysis was performed only on students that were eligible to be seeds ($N =$ 2,451).

% Of particular note, to assess potential pathways for treatment interference, students were asked to identify, in order, the 10 other students that they spent the most time with during the previous few weeks.  
% This yields a unique dataset in which the strength of the interaction between two individuals under study is explicitly recorded.
% Hence, statistical analyses may benefit from an interference model, such as KNNIM, that allows for direct incorporation of the relative strengths of the interactions.
% %to be directly incorporated.  
% For this dataset, KNNIM models with $K$ up to 10 may be applicable.

% An analysis performed by \citet{aronow2017estimating} estimated the indirect effect of being a seed student on wearing an orange wristband to be about 0.15.  
% That is,
% %such that having a treated peer increase the probability of wearing a wristband by about 15\%.
% students exposed to treated peers were about 15\% more likely to report wearing an orange wristband in comparison to students in control schools.
% % students exposed to the most influential participants (social referent seeds) were more likely to report wearing an orange wristband in comparison to other students who exposed to control social referent seeds (P-value < 0.05) where the observed value of the estimated effect is 0.073. 


\section{Notation and Preliminaries}
\label{makereference3.2}

%\subsection{Model of response under treatment interference}
Consider an experiment on $N$ units where each unit is 
%randomly 
assigned either a treatment status or a control status. 
%(the randomization mechanism will be elaborated on later).
The Neyman-Rubin Causal Model (NRCM)~\citep{splawa1990application, rubin1974estimating, holland1986statistics} is a commonly-assumed model of response for causal inference.  
Under this model, the observed response of a unit is determined by the treatment status given to that unit and the \textit{potential outcomes} for that unit---the hypothetical responses of that unit under the possible treatment statuses.
A fundamental assumption of this model is the stable-unit treatment value assumption (SUTVA), which requires that there is only a single version of each treatment status and the response of a unit is unaffected by the treatment status of any other unit~\citep{splawa1990application, rubin1980randomization, holland1986statistics, imbens2015causal}. 
%MIKE: For Mike. Watch out here.  is there redundancy with the first paragraph of the introduction? 
Of note, SUTVA is violated for experiments that exhibit treatment interference~\citep{cox1958planning,rubin1980randomization}.  
% in which the outcome of a unit is affected by others treatments---a phenomenon known as \textit{interference}~\citep{cox1958planning,rubin1980randomization}---violate SUTVA.
Failing to account for violations of SUTVA can lead to inaccurate treatment effect estimates~\citep{sobel2006randomized}.

Under interference, the effect of a treatment on a unit may occur through direct application of the treatment to that unit, indirectly through application of treatment to units that affect the response of the original unit, or both~\citep{hudgens2008toward}.
%MIKE: I want to use a sentence like this below.  Can you verify it possibly with a cite (i'm almost certain it's somewhere in aronow and samii)(Samirah: Done).
When interference is allowed to take completely arbitrary forms, treatment effect estimates are often estimated with very low power, or may be unidentifiable~\citep{aronow2017estimating}.
%When interference is allowed to take completely arbitrary forms, estimating causal effects from the results of the experiment is impractical~\citep{aronow2017estimating}.
%MIKE: The above sentence is the one I want you to check.  This is just a double check.
Thus, to make progress on 
%estimating treatment effects under interference
this problem, 
researchers often make assumptions restricting how interference can propagate across units~\citep{toulis2013estimation, aronow2017estimating, ugander2013graph, sussman2017elements}.
% which units are allowed to interfere with each other~\citep{toulis2013estimation, aronow2017estimating, ugander2013graph, sussman2017elements}.
%---which can be thought of as a weakening of SUTVA.
%between those of SUTVA and arbitrary interference models that restrict the extent of interference allowed 
%However, most models in previous work only specify that the units' outcomes are affected by the 
%can be affected by treatment assigned to their neighbors accounting mostly on the 
%number/fraction of treated neighbors, but do not specify which neighbors impact unit response and how they affect the response. 

% potential outcomes approach is a mathematical framework for causal inference \citep{rubin1974estimating} termed as Rubin Causal Model (RCM) by \citet{holland1986statistics}. Let N denote the number of units in the experiment where all units are given either a treatment or a control condition.
% A fundamental assumption in this model is the stable unit treatment value assumption known as SUTVA. One component of this assumption is that the outcome of a unit is not affected by others treatments--- a phenomenon known as interference \citep{cox1958planning,rubin1980randomization}. 

%Causal analysis is often challenging in the presence of interference, and 
% Substantial recent work has been devoted to improving inferences when interference is present.  
% Common approaches of this work include carefully designed experiments that help mitigate treatment interference effects~\citep{ugander2013graph, gui2015network, eckles2016design}
% developing statistical tests to detect the presence of arbitrary  interference~\citep{aronow2012general, saveski2017detecting, athey2018exact,  pouget2019testing, basse2019randomization, alzubaidi2022detecting}, and specifying the structure and models of interference and drawing inferences under these restricted models~\citep{sobel2006randomized, rosenbaum2007interference, hudgens2008toward, tchetgen2012causal, toulis2013estimation, aronow2017estimating, sussman2017elements,  basse2018analyzing, offer2021experimentation}. 
% Our approach aligns with the third of the approaches.
% ---for example, and randomizing treatment across clusters
% (cluster randomized design where units with high interaction are included in the same cluster) 

%work has been devoted towards  and previous work in the literature addresses the presence of interference with the following common approaches: 
% \begin{enumerate}
% \item Mitigate treatment interference effects through the experimental design 
% (cluster randomized design where units with high interaction are included in the same cluster) ~\citep{ugander2013graph, gui2015network, eckles2016design}.
% \item Testing whether interference presents~ \citep{aronow2012general, saveski2017detecting, athey2018exact,  pouget2019testing, basse2019randomization, alzubaidi2022detecting}.
% \item Specify structure and models of interference and draw inference under these models~\citep{sobel2006randomized, rosenbaum2007interference, hudgens2008toward, tchetgen2012causal, toulis2013estimation, aronow2017estimating, sussman2017elements,  basse2018analyzing, offer2021experimentation, alzubaidi2022detecting}. 
% \end{enumerate}

To elucidate these ideas,
we extend the potential outcomes framework to account for both direct and indirect treatment components.  
%MIKE: NEED TO DEFINE W_i
Let $W_i$ be a treatment indicator for unit $i$---that is, $W_i = 1$ if unit $i$ is given treatment and $W_i = 0$ otherwise.
Let $y_{i}(\mathbf W) = y_{i}(W_i, \mathbf W_{-i})$ denote the potential outcome of unit $i$ under treatment allocation $\mathbf W = (W_1, W_2, \ldots, W_N) \in \{0,1\}^N$, where unit $i$ is given treatment $W_i$, and the remaining treatment statuses are allocated according to $\mathbf W_{-i}$.
Responses $Y_i$ satisfy
\begin{equation}
    Y_i = \sum_{\mathbf W \in \{0,1\}^N} y_{i}(\mathbf W)\mathbf{1}(\mathbf W'=\mathbf W),
\end{equation}
where $\mathbf{1}(\mathbf W'=\mathbf W)$ is an indicator variable that is equal to 1 if and only if the observed treatment status $\mathbf W' = \mathbf W$.
That is, the response of unit $i$ only depends on the potential outcomes of unit $i$ and the observed treatment allocation across all $N$ units.

%MIKE: I removed direct and indirect effects to a different section.

%Due to the sheer number of potential treatment assignments possible on $N$ units, it is often not
%when interference is unconstrained, each unit may have up to $2^N$ potential outcomes.
% , leading to unidentifiability of amny making many make common causal quantities of interest .
%Estimation of treatment effects under interference may be problematic without placing constraints on how units are allowed to interfere with each other---each unit may have up to $2^N$ potential outcomes when interference is unconstrained.
% if every possible treatment assignment; in a simple e
Without making assumptions on the amount of interference allowed in a study, it may be impossible to estimate common causal quantities of interest in any practical way---for example, each unit may have up to $2^N$ potential outcomes when interference is unconstrained.
%Thus, to make progress on treatment interference problems, 
%researchers make assumptions between those of SUTVA and arbitrary interference models that 
Thus, researchers often place strong restrictions on the nature of the treatment interference~\citep{toulis2013estimation, aronow2017estimating, ugander2013graph, sussman2017elements, alzubaidi2022detecting}. 
%MIKE: REMEMBER THIS DIRECTION FOR INTERFERENCE
This often begins by constructing an \textit{exposure mapping} $G = (V, E)$---a directed graph where each vertex $i \in V$ represents a unit under study and an edge $\vec{ij} \in E$ indicates that the treatment status of unit $i$ may potentially interfere with the response of unit $j$.  
%MIKE: placing an ordering on d(i,j)... looking for big values of d(i,j).
Each edge $\vec{ij} \in E$ may also have a weight $d(i,j)$ denoting the strength of the the potential interference which may be observed through studying interactions between $i$ and $j$.  
Stronger interference between $i$ and $j$ may either correspond to  a larger or smaller value of $d(i,j)$ depending on the context of the problem---for this paper, stronger interactions are associated with smaller values of $d(i,j)$.  
Under an assumed exposure mapping $G$, for any two treatment allocations $\mathbf W$, $\mathbf W'$, we have that $y_i(\mathbf W) = y_i(\mathbf W')$ if $W_i = W_i'$ and $W_j = W_j'$ for all $j \in V$ such that $\vec{ji} \in E$;
if $\vec{ki} \notin E$, then treatment statuses $W_k$, $W'_k$ may differ without affecting equality of the potential outcomes.

Once the exposure mapping is specified, models may further restrict the nature of interference.
%MKE: CITES.  Also, I really like that gui 15 paper.  is this a potential cite here?
For example, a common assumption is that interference can only occur if a certain number or fraction of neighbors within the exposure mapping are given the treatment condition.
However, few existing models specify exactly which neighbors in the exposure mapping are allowed to interfere with a unit's response, or allow for indirect effects to differ across neighbors. 
As an alternative, we consider a model where interference of treatment on a unit $i$ is restricted to its $K$–nearest neighbors~\citep{ alzubaidi2022detecting}. 
This model allows neighbors with stronger interactions to contribute larger indirect effects, and limits the ability of weakly-interacting units to affect response.




% %MIKE: 
% most models in previous work only specify that the units' outcomes are affected by the 
% %can be affected by treatment assigned to their neighbors accounting mostly on the 
% number/fraction of treated neighbors, but do not specify which neighbors impact unit response and how they affect the response. 
% Suppose we have a finite population $U$ of units indexed by $i = 1, ...,N$.
% We view units under study as a mathematical graph.
% Let $\mathbf{G} = (V, E)$ be a directed graph of $\|V\| = N$ vertices; each vertex $i \in V$ corresponds to a unit under study.
% An edge $ij \in E$ denotes potential interaction between units $i$ and $j$.
% %units that do not interfere with each other




% %MIKE: May use this forastiere cite later.
% % of $\|V\| = N$ vertices; each vertex $i \in V$ corresponds to a unit under study.
% % An edge $ij \in E$ denotes 
% % These interactions between units can be defined by any type of relationships for example, membership to the same group, friendship in social media, geographic proximity, etc.~\citep{forastiere2020identification}.
% % %Edges in $E$ are undirected: $ij = ji$.
% % Throughout the chapter, the terms vertex, unit, and individual will be used interchangeably.


% However, most models in previous work only specify that the units' outcomes are affected by the 
% %can be affected by treatment assigned to their neighbors accounting mostly on the 
% number/fraction of treated neighbors, but do not specify which neighbors impact unit response and how they affect the response. 
% Suppose we have a finite population $U$ of units indexed by $i = 1, ...,N$.
% We view units under study as a mathematical graph.
% Let $\mathbf{G} = (V, E)$ be a directed graph of $\|V\| = N$ vertices; each vertex $i \in V$ corresponds to a unit under study.
% An edge $ij \in E$ denotes potential interaction between units $i$ and $j$.
% These interactions between units can be defined by any type of relationships for example, membership to the same group, friendship in social media, geographic proximity, etc.~\citep{forastiere2020identification}.
% %Edges in $E$ are undirected: $ij = ji$.
% Throughout the chapter, the terms vertex, unit, and individual will be used interchangeably.

% %MIKE: Can't make j denote the jth nearest neighbor of i; nearest neighbors is asymmetric.
% Let $A$ denote the $\mathbb{N}$ $\times$ $\mathbb{N}$ adjacency matrix of $G$.  
% That is, $A_{ij} = 1$ if $ij \in E$ where there is an edge from unit $j$ to $i$ and $A_{ij} = 0$ otherwise. 
% %equal to 1 if there is an edge between units $i$ and $j$ and 0 otherwise where $j = 1,2,\ldots,K$ is the $j^{th}$ nearest neighbor for unit $i$. 
% Since $\mathbf{G}$ has no self-loops, the diagonal elements of the adjacency matrix, $A_{ii} = 0$.

% %MIKE: Need to check notation
% Let $d(i,j)$ denote an interaction or dissimilarity measure between units $i$ and $j$.  
% In the context of treatment interference, smaller values of $d(i,j)$ indicate stronger interactions between units $i$ and $j$.
% We assume, for now, that $d(i,j)$ is only computed for units $i,j$ with $A_{ij} = 1$.
% Let $d(i, (j))$ denote the $j$th smallest value of $\{d(i,j), j \neq i\}$; that is, $d(i, (1)) < d(i, (2)) < \cdots$.
% For ease of exposition, we assume that all values of $d(i,j)$ are unique (in practice, ties may be broken arbitrarily).
% The \textit{$K$-neighborhood} of unit $i$, denoted $\mathcal{N}_{iK}$, is the set of the $K$ ``closest'' units to unit $i$:
% %MIKE: Watch notation
% \begin{equation}
%     \mathcal{N}_{iK} = \{j : d(i,(j) ) \leq d(i, (K)), j = 1,2, \ldots ,K\}.
% \end{equation}
% Define $\mathcal{N}_{-iK} = V \setminus (i \cup N_{iK} )$ as all units in $V$ that are outside of $i$'s $K$-neighborhood.
% Note that the sets
% $\{i, \mathcal{N}_{ik}, \mathcal{N}_{-ik}\}$ form a partition of $V$.


% %denoted as $\mathcal{N}_{iK}$ where $\mathcal{N}_{iK} = \{j :A_{ij} = 1, j = 1,2, \ldots ,K$ is the $j^{th}$ nearest neighbor and $d(i,1)< d(i,2)<  \ldots <d(i,j)< \ldots <d(i,k) \}$. 
% %We can now form a partition of the set V for each unit $i$ as ($i$, $\mathcal{N}_{ik}$, $\mathcal{N}_{-ik}$) where $\mathcal{N}_{ik}$ is $K$-nearest neighbor neighborhood defined above that contains the $K$-nearest neighbors of unit $i$ and $\mathcal{N}_{-ik}$ is the set that excludes unit $i$ and all units in $\mathcal{N}_{ik}$.

% Recall that $W_i$ is a treatment indicator for unit $i$, and let $W = (W_1, W_2, \ldots, W_N)$ = $\{W_{i}, W_{\mathcal{N}_{ik}}, W_{\mathcal{N}_{-ik}}\}$ denote treatment assignment vector for all units $N$.
% %MIKE: We do not use mathbb W again, so i'm removing this.
% %and let $\mathbb{W}$ be a set of all possible randomizations of $W$. 
% %. where each unit $i$ is assigned either to treatment or control, $W_{i}\in\{0,1\}$ and 
% Also recall that $Y_{i}$ is the outcome measured on the unit $i$. 
% Each unit's potential outcome $y_{i}(W)$ is defined as a function of the entire assignment vector of units to the two treatment conditions $W\in\{0,1\}^N = \Omega$.



\section{$K$-Nearest Neighbors Interference Model}
\label{makereference3.3}

% In the context of treatment interference, smaller values of $d(i,j)$ indicate stronger interactions between units $i$ and $j$.
% We assume, for now, that $d(i,j)$ is only computed for units $i,j$ with $A_{ij} = 1$.
% Let $d(i, (j))$ denote the $j$th smallest value of $\{d(i,j), j \neq i\}$; that is, $d(i, (1)) < d(i, (2)) < \cdots$.
% For ease of exposition, we assume that all values of $d(i,j)$ are unique (in practice, ties may be broken arbitrarily).

%MIKE: Cite us (Samirah: Done)
The $K$-nearest neighbors interference model (KNNIM) is a recently-proposed model in which a unit $j$ is only allowed to interfere with the response of unit $i$ if $j$ is within $i$'s $K$-neighborhood~\citep{ alzubaidi2022detecting}.
The \textit{$K$-neighborhood} of unit $i$, denoted $\mathcal{N}_{iK}$, is the set of the $K$ ``closest'' units to unit $i$:
%MIKE: Need slight edit here.  Feel free to use JCI definition.
\begin{align*}
    %\label{defined}
    \mathbf{\mathcal{N}}_{iK} = \{j : d(i,j) \leq d(i, (K)), j = 1,2, \ldots ,K, ~ j \ne i\}.
\end{align*}
where $d(i, (K))$ denotes the $K^\text{th}$ smallest  value of $(d(i,j))^N_{j=1}$.   
Ties between $d(i,j)$ may be broken arbitrarily to ensure that $|\mathbf{\mathcal{N}}_{iK}| = K$.
% \begin{equation}
%     \mathcal{N}_{iK} = \{j : d(i, j) \leq d(i, (K)), j = 1, 
%     \ldots, N\},
% \end{equation}
% where $d(i, (K))$ denotes the $K^\text{th}$ smallest  value of $(d(i,j))^N_{j=1}$; ties between $d(i,j)$ may be broken arbitrarily.
Define $\mathcal{N}_{-iK} = V \setminus (i \cup N_{iK} )$ as all units in $V$ that are outside of $i$'s $K$-neighborhood.
Note that the sets
$\{i\}, \{\mathcal{N}_{ik}\}, \{\mathcal{N}_{-ik}\}$ form a partition of $V$.

Let 
%(W_1, W_2, \ldots, W_N)$ = 
 $\mathbf{W} = (W_{i}, \mathbf{W}_{\mathcal{N}_{iK}}, \mathbf{W}_{\mathcal{N}_{-iK}})$ denote treatment assignment vector for all units $N$, partitioned into the treatment given to unit $i$, the treatments given to $i$'s $K$-nearest neighbors, and the treatments given to all other units.
For clarity, treatment statuses in $\mathbf{W}_{\mathcal{N}_{iK}}$ and  $\mathbf{W}_{\mathcal{N}_{-iK}}$ are given in increasing order with respect to $d(i,j)$ %ordered according to units' proximity to unit $i$ 
(\textit{e.g.}~the first entry of $ \mathbf{W}_{\mathcal{N}_{iK}}$ is the treatment assignment given to the nearest neighbor of unit $i$).
The defining assumption of KNNIM is as follows:  
%$K$-nearest neighbors interference model under the following assumption:
\begin{assumption}
\label{Assumption1}
    ($K$-Neighborhood Interference Assumption (K-NIA)). 
The potential outcomes $y_i(\mathbf W)$ for all units $i$ satisfy
%in a network G and for all treatment assignments $W_{\mathcal{N}_{-iK}}$, $W'_{\mathcal{N}_{-iK}}$ , the potential outcomes satisfy $K$-Neighborhood Interference Assumption if 
\begin{equation} 
y_{i}(W_{i}, \mathbf{W}_{\mathcal{N}_{iK}}, \mathbf{W}_{\mathcal{N}_{-iK}}) = y_{i}(W_{i}, \mathbf{W}_{\mathcal{N}_{iK}}, \mathbf{W}'_{\mathcal{N}_{-iK}}).
\end{equation}
\end{assumption}
In words, K-NIA ensures that the potential outcome of unit $i$ is only affected by its own treatment status and the treatment statuses of its $K$-nearest neighbors.
The treatment statuses of units outside the $K$-neighborhood will not affect the potential outcome of unit $i$.
Thus, this model restricts the number of potential outcomes to be $2^{K+1}$ for each unit. 
For brevity, we will suppress the treatment assignment outside of the $K$-nearest neighbors when denoting potential outcomes under KNNIM:
$y_{i}(W_{i}, \mathbf{W}_{\mathcal{N}_{iK}}) = y_{i}(W_{i}, \mathbf{W}_{\mathcal{N}_{iK}}, \mathbf{W}_{\mathcal{N}_{-iK}})$.

The choice of $K$ is ultimately left to the researcher, though large values of $K$ may not be sufficiently restrictive to allow for reliable estimates and inferences.
A detailed description on how to choose $K$ is given in~\citet{alzubaidi2022detecting}.
As a crude, but useful, rule-of-thumb, we find it practical to choose $K$ such that each exposure used in the estimation of treatment effects is observed at least 30 times. 

% For example, $y_{i}(1, \mathbf{1})$ is the potential outcome of unit $i$ when assigned to treatment where all its K-nearest neighbors are assigned to treatment as well.
% :
% we use $y_{i}(W_{i}, W_{\mathcal{N}_{ik}})$ instead of $y_{i}(W_{i}, W_{\mathcal{N}_{ik}}, W'_{\mathcal{N}_{-ik}})$ where
%  $W_{\mathcal{N}_{ik}}\in\{0,1\}^K$ is the treatment assignment vector of the K-nearest neighbors of unit i respectively (i.e., $W_{\mathcal{N}_{ik}}$ = ($W_{1}W_{2}W_{3}\dots W_{K}$)) such that $W_{\mathcal{N}_{ik}}$ = $\mathbf{1}$ denote that all K-nearest neighbors are treated and $W_{\mathcal{N}_{ik}}$ = $\mathbf{0}$ denote that all K-nearest neighbors are assigned to control. 



%In its most general form, the $K$-nearest neighbor interference model (KNNIM) assumes only that the treatment interference structure satisfies assumption \ref{Assumption1}.



%MIKE: We do not use mathbb W again, so i'm removing this.
%and let $\mathbb{W}$ be a set of all possible randomizations of $W$. 
%. where each unit $i$ is assigned either to treatment or control, $W_{i}\in\{0,1\}$ and 
% Recall that $Y_{i}$ is the outcome measured on the unit $i$. 
% Each unit's potential outcome $y_{i}(W)$ is defined as a function of the entire assignment vector of units to the two treatment conditions $W\in\{0,1\}^N = \Omega$.

%MIKE: is this redundant?
% For the partition of unit $i$ (i, $\mathcal{N}_{ik}$, $\mathcal{N}_{-ik}$), we have the corresponding partitions for the treatments and outcomes  ($W_{i}$, $W_{\mathcal{N}_{ik}}$, $W_{\mathcal{N}_{-ik}}$) and ($Y_{i}$, $Y_{\mathcal{N}_{ik}}$, $Y_{\mathcal{N}_{-ik}}$) respectively.







% To make progress on treatment interference problems, researchers make assumptions between those of SUTVA and arbitrary interference models that restrict the extent of interference allowed~\citep{toulis2013estimation, aronow2017estimating, ugander2013graph, sussman2017elements}. 
% However, most models in previous work only specify that the units' outcomes are affected by the 
% %can be affected by treatment assigned to their neighbors accounting mostly on the 
% number/fraction of treated neighbors, but do not specify which neighbors impact unit response and how they affect the response. 

% We now propose an interference model where we restrict the interference of treatment on a unit $i$ to its $K$--nearest neighbors.
% This allows different neighbors to contribute different effects depending on the proximity of the relationship---neighbors that are close to unit $i$ are more likely to have their treatment status affect the response of unit $i$.

%MIKE: I'm not sure whether I like these remarks in this chapter.  If you include them, move them to 2.2.

% \citet{aronow2017estimating} define exposure mapping as a function that maps an assignment vector and units' specific traits to an exposure value where there is a finite set S of exposure values such that for each unit we have a vector of probabilities of S exposures.
% \citet{toulis2013estimation} introduce k-level interference where a unit is k-exposed if exactly k neighbors are treated for any unit in the set of units that have at least k neighbors.
% \citet{sussman2017elements} consider neighborhood interference assumption (NIA) where the potential outcome of an individual depends only on an individual’s treatment and neighbors’ treatmentS.
% NIA along with other assumptions lead to various models for the potential outcomes where someone can define estimands for the direct and indirect treatment effects in terms of the model parameters. 
% Similarly, \citet{forastiere2020identification} consider neighborhood interference assumption ruling out the dependence of the potential outcome of a unit from treatments assigned outside the neighborhood.
% \citet{forastiere2020identification} assumption differ than \citet{sussman2017elements} assumption in that they assumed that the dependence of the potential outcomes is defined through a specific function.

% \citet{ugander2013graph} develop different exposure models of interference. One exposure model is the full neighborhood exposure to a treatment where  unit $i$ and all its neighbors receive that treatment condition. 
% An absolute k-neighborhood exposure to treatment is that for vertex $i$ with degree d $\geq$ K, vertex $i$ and $\geq$ K neighbors of $i$ receive that treatment condition.
% In addition, fractional q-neighborhood exposure to a treatment which is a vertex $i$ and $\geq$ qd neighbors of $i$ receive that treatment condition. 
% They also introduce stricter versions of the above exposures using core exposures. 



%MIKE: Likely to remove the next section.
%\subsection{Setup and Assumptions}

%MIKE: May want to use lowercase letters for realizations.

%MIKE: Want to talk about exposure maps here
% Under SUTVA, the outcome of unit $i$ depends only on the treatment assigned to unit $i$.  
% That is, for two randomizations $W, W'$, SUTVA implies that $Y_{i}(W) = Y_{i}(W')$ if $W_i = W_i'$. 
% However, when treatment effects interfere across units, units' responses are not only affected by their own treatments but also by the treatments assigned to other units in the network.


% Suppose we have a finite population $U$ of units indexed by $i = 1, ...,N$.
% We view units under study as a mathematical graph.
% Let $\mathbf{G} = (V, E)$ be a directed graph of $\|V\| = N$ vertices; each vertex $i \in V$ corresponds to a unit under study.
% An edge $ij \in E$ denotes potential interaction between units $i$ and $j$.
% These interactions between units can be defined by any type of relationships for example, membership to the same group, friendship in social media, geographic proximity, etc.~\citep{forastiere2020identification}.
% %Edges in $E$ are undirected: $ij = ji$.
% Throughout the chapter, the terms vertex, unit, and individual will be used interchangeably.

% %MIKE: Can't make j denote the jth nearest neighbor of i; nearest neighbors is asymmetric.
% Let $A$ denote the $\mathbb{N}$ $\times$ $\mathbb{N}$ adjacency matrix of $G$.  
% That is, $A_{ij} = 1$ if $ij \in E$ where there is an edge from unit $j$ to $i$ and $A_{ij} = 0$ otherwise. 
% %equal to 1 if there is an edge between units $i$ and $j$ and 0 otherwise where $j = 1,2,\ldots,K$ is the $j^{th}$ nearest neighbor for unit $i$. 
% Since $\mathbf{G}$ has no self-loops, the diagonal elements of the adjacency matrix, $A_{ii} = 0$.

% %MIKE: Need to check notation
% Let $d(i,j)$ denote an interaction or dissimilarity measure between units $i$ and $j$.  
% In the context of treatment interference, smaller values of $d(i,j)$ indicate stronger interactions between units $i$ and $j$.
% We assume, for now, that $d(i,j)$ is only computed for units $i,j$ with $A_{ij} = 1$.
% Let $d(i, (j))$ denote the $j$th smallest value of $\{d(i,j), j \neq i\}$; that is, $d(i, (1)) < d(i, (2)) < \cdots$.
% For ease of exposition, we assume that all values of $d(i,j)$ are unique (in practice, ties may be broken arbitrarily).
% The \textit{$K$-neighborhood} of unit $i$, denoted $\mathcal{N}_{iK}$, is the set of the $K$ ``closest'' units to unit $i$:
% %MIKE: Watch notation
% \begin{equation}
%     \mathcal{N}_{iK} = \{j : d(i,(j) ) \leq d(i, (K)), j = 1,2, \ldots ,K\}.
% \end{equation}
% Define $\mathcal{N}_{-iK} = V \setminus (i \cup N_{iK} )$ as all units in $V$ that are outside of $i$'s $K$-neighborhood.
% Note that the sets
% $\{i, \mathcal{N}_{ik}, \mathcal{N}_{-ik}\}$ form a partition of $V$.


%denoted as $\mathcal{N}_{iK}$ where $\mathcal{N}_{iK} = \{j :A_{ij} = 1, j = 1,2, \ldots ,K$ is the $j^{th}$ nearest neighbor and $d(i,1)< d(i,2)<  \ldots <d(i,j)< \ldots <d(i,k) \}$. 
%We can now form a partition of the set V for each unit $i$ as ($i$, $\mathcal{N}_{ik}$, $\mathcal{N}_{-ik}$) where $\mathcal{N}_{ik}$ is $K$-nearest neighbor neighborhood defined above that contains the $K$-nearest neighbors of unit $i$ and $\mathcal{N}_{-ik}$ is the set that excludes unit $i$ and all units in $\mathcal{N}_{ik}$.




\section{Causal Estimands under KNNIM}
\label{makereference3.4}

Using the potential outcomes framework and following \citet{hudgens2008toward}, we now define causal estimands under KNNIM. 
We start with general definitions of direct and indirect effects and conclude with KNNIM-specific nearest neighbors effects.

\subsection{Direct, Indirect, and Total Effects}
\label{section4.1}
% Since the interference structure is restricted to the KNN, henceforth, for brevity, we will use $y_{i}(W_{i}, W_{\mathcal{N}_{ik}})$ instead of  $y_{i}(W_{i}, W_{\mathcal{N}_{ik}}, W'_{\mathcal{N}_{-ik}})$ where
%  $W_{\mathcal{N}_{ik}}\in\{0,1\}^K$ is the treatment assignment vector of the K-nearest neighbors of unit i respectively (i.e., $W_{\mathcal{N}_{ik}}$ = ($W_{1}W_{2}W_{3}\dots W_{K}$)) such that $W_{\mathcal{N}_{ik}}$ = $\mathbf{1}$ denote that all K-nearest neighbors are treated and $W_{\mathcal{N}_{ik}}$ = $\mathbf{0}$ denote that all K-nearest neighbors are assigned to control. 
%  For eaxmple, $y_{i}(1, \mathbf{1})$ is the potential outcome of unit $i$ when assigned to treatment where all its K-nearest neighbors are assigned to treatment as well.
    % propose estimands for direct, indirect, total and overall causal treatment effects under this setting of interference where they define 
    
%MIKE: Using deltas instead of taus to be consistent with rest of paper.
The \textit{average direct effect} (ADE) $\delta_{dir}$ is the average difference in a unit's potential outcomes when changing that unit's treatment status and holding all other units' treatment status fixed.
It may be defined as
% If we let $y_{i}(W_i, \mathbf W_{-i})$ denote the potential outcome of unit i with $W_{i}$ is the treatment indicator for unit i and $\mathbf W_{-i}$ is the vector of the treatment assignments of other units in unit's i group.
% \citet{hudgens2008toward} define the direct effect as follows:
\begin{equation}
\label{eq:direff}
\delta_{dir} = \frac{1}{N} \sum_{i = 1}^N (y_i(1,\mathbf{1}) - y_i(0,\mathbf{1})),
\end{equation}
where $\mathbf{1}$ denotes a vector of 1's of length $K$.
In contrast to direct effect, the \textit{average indirect effect} (AIE) $\delta_{ind}$ is defined as the average difference in a unit's potential outcome when changing all other treatment statuses from control to treated, holding its own treatment fixed.
It may be defined as
\begin{equation}
\label{eq:indeff}
\delta_{ind} = \frac{1}{N} \sum_{i = 1}^N (y_i(0,\mathbf{1}) - y_i(0,\mathbf{0})),
\end{equation}
where $\mathbf{0}$ denotes a vector of 0's of length $K$.
The \textit{average total effect} (ATOT) $\delta_{tot}$ measures the average difference in potential outcomes between all units receiving treatment and all units receiving control:
%Total effect
\begin{equation}
\delta_{tot} = \frac{1}{N} \sum_{i = 1}^N (y_i(1,\mathbf{1}) - y_i(0,\mathbf{0})). 
\end{equation}
Note that these quantities are defined to satisfy
\begin{equation}
    \label{eq:trtdecomp}
    \delta_{tot} = \delta_{dir} + \delta_{ind}.
\end{equation}
When SUTVA holds, $\delta_{tot} = \delta_{dir}$ and $\delta_{ind} = 0$.

Note, we may define $\delta_{dir} = N^{-1}\sum_{i=1}^N (y_i(1, \mathbf{0}) - y_i(0, \mathbf{0}))$ and $\delta_{ind} = N^{-1}\sum_{i=1}^N (y_i(1, \mathbf{1}) - y_i(1, \mathbf{0}))$ while still ensuring that~\eqref{eq:trtdecomp} holds.  
These quantities may differ from~\eqref{eq:direff} and~\eqref{eq:indeff} if there is interaction between direct effects and indirect effects---that is, if the differences $y_i(1,\mathbf W_{-i}) - y_i(0,\mathbf W_{-i})$ differ depending on the allocation of treatment given to $\mathbf W_{-i}$~\citep{alzubaidi2022detecting}.
%Moreover, direct effects may be defined for each possible $\mathbf W_{-i}$---e.g.~$\tau_{dir}(\mathbf W_{-i}) = N^{-1}\sum_{i=1}^N (y_i(1, \mathbf W_{-i}) - y_i(0, \mathbf W_{-i}))$---however, such definitions may prevent a decomposition of the total effect into direct and indirect effects~\citep{hudgens2008toward}.
%Finally, when SUTVA holds, $\tau_{tot} = \tau_{dir}$ and $\tau_{ind} = 0$.


% The main contributions in this chapter is to define direct, indirect and total effects under the K-neighborhood interference assumption \citep{ alzubaidi2022detecting} and propose unbised estimators for these estimands. 

\subsection{The $\ell^{th}$--Nearest Neighbor Indirect Effect}

We now define nearest-neighbor average treatment effects.
Under KNNIM, these estimands are of primary researcher interest.
Let $\mathbf{W}^{*}_{\ell} = (W^*_{\ell, 1}, W^*_{\ell, 2}, \ldots, W^*_{\ell, K})
\in\{0,1\}^K$ 
denote the treatment assignment vector of length $K$ where the first $\ell$ nearest neighbors are given treatment and the rest are given control:
\begin{equation}
    \mathbf{W}^*_{\ell, j} = \left\{ 
    \begin{array}{ll}
        1, & j \leq \ell,\\
        0, &\text{otherwise}.
    \end{array}\right.
\end{equation}
Note that $\mathbf{W}^{*}_{K} = \mathbf 1$ and $\mathbf{W}^{*}_{0} = \mathbf{0}$. 
%MIKE: Removing th from the acronym (Samirah: Done)
Following our definitions from Section~\ref{section4.1}, we define the \textit{average $\ell^{th}$--nearest neighbor indirect effect} (A$\ell$NNIE) as
\begin{equation}
    \delta_{\ell} = \frac{1}{N} \sum_{i = 1}^N (y_i(0,\mathbf{W}^{*}_{\ell}) - y_i(0,\mathbf{W}^{*}_{\ell-1} )).
\end{equation}
Note that $\mathbf{W}^{*}_{\ell}$ and $\mathbf{W}^{*}_{\ell-1}$ are identical except 
that $W^*_{\ell, \ell} = 1$ and $W^*_{\ell-1, \ell} = 0$.
%the only difference in treatment statuses is the treatment status of the $\ell^{th}$-nearest neighbor.
% \begin{equation} 
% \delta_{\ell} =  \bar{y}(0,W^{*}_{\ell}) - \bar{y}(0, W^{*}_{\ell-1}).
% \end{equation} 
Hence, $\delta_{\ell}$ may be interpreted as the average difference in response due to the treatment status of the $\ell^{th}$--nearest-neighbor.
Additionally, under KNNIM, the AIE is the sum of the A$\ell$NNIEs.
% Therefore, the average of the indirect effect $\delta_{indirect}$ is the sum of all K-nearest neighbors indirect effects $\delta_{1^{st}}$, $\delta_{2^{nd}}$, \dots $\delta_{K}$,i.e.,
\begin{equation} 
\delta_{ind} = \sum_{\ell = 1}^{K} \delta_{\ell}.
%+ \delta_{r}.
\end{equation}
To see this, note that $\sum_{\ell = 1}^{K} \delta_{\ell}$ is a telescoping sum that simplifies to
\begin{equation}
    \sum_{\ell = 1}^{K} \delta_{\ell} =  \sum_{\ell = 1}^{K}\frac{1}{N} \left(\sum_{i = 1}^N y_i(0,\mathbf{W}^{*}_{\ell}) - \sum_{i=1}^Ny_i(0,\mathbf{W}^{*}_{\ell-1} )\right) = \frac{1}{N} \sum_{i = 1}^N (y_i(0,\mathbf{1}) - y_i(0,\mathbf{0} )) = \delta_{ind}.
\end{equation}

%MIKE: Keep the full version in your dissertation but move to an appendix.(Sammirah:Done)
% \begin{proof}
% For $\ell \in \mathcal{N}_{ik}$, if we define $W^{*}_{\ell}$ as previously such that $W^{*}_{1}$ means that the first nearest neighbor is treated where the rest of the nearest neighbors are assigned to control,
% $W^{*}_{2}$ means that the first two nearest neighbors are treated and the rest are assigned to control and so forth.
% $W^{*}_{0}$ denote that all K-nearest neighbors are assigned to control. Then,


% \begin{multline}
% \delta_{1^{st}} + \delta_{2^{nd}}+ \delta_{3^{rd}}+\dots + \delta_{\ell^{th}}+ \dots + \delta_{K} = \\
% \bar{y}(0,W^{*}_{1}) - \bar{y}(0, W^{*}_{0}) 
% + \bar{y}(0,W^{*}_{2}) - \bar{y}(0, W^{*}_{1})
% %+ \bar{y}(0,W^{*}_{3}) - \bar{y}(0, W^{*}_{2}) \\
% + \dots 
% %+ \bar{y}(0,W^{*}_{\ell}) - \bar{y}(0, W^{*}_{\ell - 1})\\
% %+ \dots \\
% + \bar{y}(0,W^{*}_{K}) - \bar{y}(0, W^{*}_{K- 1})\\
% = \bar{y}(0,W^{*}_{K}) - \bar{y}(0, W^{*}_{0}) \\
% =  \bar{y}(0, \mathbf{1}) - \bar{y}(0, \mathbf{0}) \\
% =\delta_{indirect}. \\
% \end{multline}
% \end{proof}


%, otherwise, t
%and let $W^{*}_{\ell-1}\in\{0,1\}^K$ be a vector for the treatment assignment of the K-nearest neighbors where the first $\ell-1$ nearest neighbors are treated and the rest are control. 



% We define the unit-level of a specific $\ell^{th}$ nearest neighbor indirect effect on unit i as the difference between the potential outcome of unit i when assigned to control with the first $\ell$ nearest neighbors are treated and the rest are control and the potential outcome of unit i when assigned to control with the first $\ell-1$ nearest neighbors are treated and the rest are control as follows. 


% \begin{equation} 
% \delta^{i}_{\ell^{th}} =  y_{i}(0, W^{*}_{\ell}) - y_{i}(0, W^{*}_{\ell-1}).
% \end{equation} 
     
 
% \subsection{Unit-Level Total Effect Estimand}
% \label{makereference3.3.1}

% %We define the unit-level total effect as the difference between the potential outcome of unit i when assigned to treatment with all its k-nearest neighbors are also assigned to treatment and the potential outcome of unit i when assigned to control with all its k-nearest neighbors are also assigned to control. 
% %Then, the unit-level total effect is 
% \begin{equation} 
% \delta^{i}_{Total} =  y_{i}(1, \mathbf{1}) - y_{i}(0,\mathbf{0}).
% \end{equation} 


% \subsection{Average Total Effect Estimand}
% \label{makereference3.3.2}
% %We define the average total effect (ATE) as the difference in means between the potential outcomes of units when assigned to treatment with all their k-nearest neighbors are assigned to treatment and the potential outcomes of units when assigned to control with all their k-nearest neighbors are assigned to control.
% %The average total effect estimand is 

% \begin{equation} 
% \delta_{Total} =  \bar{y}(1, \mathbf{1}) - \bar{y}(0,\mathbf{0}).
% \end{equation} 

 
 
 
 
 
 
 
% \subsection{The Unit-Level Direct Effect Estimand}
% \label{makereference3.3.3}
% %We define the unit-level direct effect of the treatment assignment on unit i as the difference between the potential outcome of unit i when assigned to treatment with all its k-nearest neighbors are assigned to treatment and the potential outcome of unit i when assigned to control with all its k-nearest neighbors are assigned to treatment as follow. 
% \begin{equation} 
% \delta^{i}_{direct} =  y_{i}(1,\mathbf{1}) - y_{i}(0,\mathbf{1}).
% \end{equation} 



% \subsection{The Average Direct Effect Estimand}
% \label{makereference3.3.4}
% %We then define the average direct effect (ADE) as the difference in means between the potential outcomes of units when assigned to treatment with all their k-nearest neighbors are assigned to treatment and the potential outcomes of units when assigned to control with all their k-nearest neighbors are assigned to treatment as follow. 
% \begin{equation} 
% \delta_{direct} =  \bar{y}(1,\mathbf{1}) - \bar{y}(0,\mathbf{1}).
% \end{equation} 



% \subsection{The Unit-Level Indirect Effect Estimand}
% \label{makereference3.3.5}
% %We define the unit-level indirect effect of the treatment assignment of the k-nearest neighbors on unit i as the difference between the potential outcome of unit i when assigned to control with all its k-nearest neighbors are assigned to treatment and the potential outcome of unit i when assigned to control with all its k-nearest neighbors are assigned to control as well. 
% \begin{equation} 
% \delta^{i}_{indirect} =  y_{i}(0, \mathbf{1}) - y_{i}(0, \mathbf{0}).
% \end{equation} 




% \subsection{The Average Indirect Effect Estimand}
% \label{makereference3.3.6}
% %We then define the average indirect effect (AIE) of the treatment assignment of the k-nearest neighbors as the difference in means between the potential outcomes of units when assigned to control with all their k-nearest neighbors are assigned to treatment and the potential outcomes of units when assigned to control with all their k-nearest neighbors are assigned to control as well. 
% \begin{equation} 
% \delta_{indirect} =  \bar{y}(0, \mathbf{1}) - \bar{y}(0, \mathbf{0}).
% \end{equation} 






% \subsection{Unit-level $\ell^{th}$ Nearest Neighbor Indirect Effect Estimand}
% \label{makereference3.3.7}
% Let $W^{*}_{\ell}\in\{0,1\}^K$ be a vector for the treatment assignment of the K-nearest neighbors where the first $\ell$ nearest neighbors are treated and the rest are control and let $W^{*}_{\ell-1}\in\{0,1\}^K$ be a vector for the treatment assignment of the K-nearest neighbors where the first $\ell-1$ nearest neighbors are treated and the rest are control. 


% We define the unit-level of a specific $\ell^{th}$ nearest neighbor indirect effect on unit i as the difference between the potential outcome of unit i when assigned to control with the first $\ell$ nearest neighbors are treated and the rest are control and the potential outcome of unit i when assigned to control with the first $\ell-1$ nearest neighbors are treated and the rest are control as follows. 


% \begin{equation} 
% \delta^{i}_{\ell^{th}} =  y_{i}(0, W^{*}_{\ell}) - y_{i}(0, W^{*}_{\ell-1}).
% \end{equation} 



%\subsection{Average of the $\ell^{th}$ Nearest Neighbor Indirect Effect Estimand}
%\label{makereference3.3.8}
%We also define the average of the indirect effect of a specific $\ell^{th}$ nearest neighbor (A$\ell^{th}$NNIE) as the difference in means between the potential outcomes of units when assigned to control with the first $\ell$ nearest neighbors are treated and the rest are control and the potential outcomes of units when assigned to control with the first $\ell-1$ nearest neighbors are treated and the rest are control as follows. 



%To illustrate this assumption, let K = 3 nearest neighbors with treatment vector $W_{\mathcal{N}_{ik}}$ = ($W_{1}W_{2}W_{3})$, then the unit-level first nearest neighbor indirect effect on unit i is
%$\delta_{1^{st}} = Y_{i}(W_{i},W_{\mathcal{N}_{ik}} = (100))$ - %$Y_{i}(W_{i},W_{\mathcal{N}_{ik}} = (000))$ = %$Y_{i}(W_{i},W_{\mathcal{N}_{ik}} = (101))$ - %$Y_{i}(W_{i},W_{\mathcal{N}_{ik}} = (001))$.
%This applies for any $W_{\mathcal{N}_{ik}}$ = ($W_{1}W_{2}W_{3})$ such that the effect of the first nearest neighbor is the difference between the outcome of unit i when the first nearest neighbor is treated and when it is control holding the treatment assignment of the remaining nearest neighbors fixed.


%Under the no-interaction assumption, let $W_{ik}(W_{i},1,W_{*})$ be an indicator variable that is equal to 1 if the first nearest neighbor is treated where $W_{*}$ is the treatment assignment for the rest of the nearest neighbors of unit i and $W_{i}$ is the treatment assignment for unit i.
%Let $N_{W_{*}} =  \sum_{i} W_{ik}(W_{i},1,W_{*}) + \sum_{i} W_{ik}(W_{i},0,W_{*})$ where $N_{W_{*}}$ = 0 if either $\sum_{i} W_{ik}(W_{i},1,W_{*})$ or $\sum_{i} W_{ik}(W_{i},0,W_{*})$ is 0. 
%Let $n^{*}_{k} = \sum_{W_{*}}N_{W_{*}}$.
%Let $\bar{Y_{K}}(W_{ik}(W_{i},1,W_{*})) = \frac{1}{N_{W_{*}}}\sum_{i}Y_{i}(W_{ik}(W_{i},1,W_{*}))$.
%Similarly, let $\bar{Y_{K}}(W_{ik}(W_{i},0,W_{*})) = \frac{1}{N_{W_{*}}}\sum_{i}Y_{i}(W_{ik}(W_{i},0,W_{*}))$.
%Then, the average of the $k^{th}$ Nearest Neighbor Indirect Effect Estimand for units with $W_{i} = 0$ control becomes as follows.

%\begin{equation} 
%\delta_{k^{th}_{c}} =  \sum_{W_{*}}\frac{N_{W_{*}}}{n^{*}_{k}}[\bar{Y_{K}}(W_{ik}(0,1,W_{*})) -  \bar{Y_{K}}(W_{ik}(0,0,W_{*}))] .
%\end{equation} 
%Similarly for treated unit i with $W_{i} = 1$, 
%\begin{equation} 
%\delta_{k^{th}_{t}} =  %\sum_{W_{*}}\frac{N_{W_{*}}}{n^{*}_{k}}[\bar{Y_{K}}(W_{ik}(1,1,W_{*})) -  %\bar{Y_{K}}(W_{ik}(1,0,W_{*}))] .
%\end{equation} 


%This can be generalized for any unit i whether it is $W_{i} = 0$ or $W_{i} = 1$, the estimand becomes as follows.

%\begin{equation} 
%\delta_{k^{th}} =  \frac{N_{t}}{N}\delta_{k^{th}_{t}} +\frac{N_{c}}{N}\delta_{k^{th}_{c}} .
%\end{equation} 
%where $N_{t}$ and $N_{c}$ is the number of treated units and the number of control units respectively. 




% Similarly, we can show that the average total effect $\delta_{Total}$ can be decomposed into the direct and indirect effects ,i.e.,
% \begin{equation} 
% \delta_{Total} = \delta_{direct} + \delta_{indirect}.
% \end{equation}















%\section{Estimation using Difference in Means}
%\label{makereference3.4}



%Next, we estimate the previous quantities of interest using difference in means estimators.


%Now we let $Y_{i}^{obs}(W_{i}, W_{\mathcal{N}_{ik}})$ denotes the observed potential outcome of the focal unit $i$ that depends on its own treatment $W_{i}$ and on the treatments assigned to its K-nearest neighbors $W_{\mathcal{N}_{ik}}$.


%\subsubsection{Average Total Effect Estimator}
%\label{makereference3.4.1}


%\begin{equation} 
%\widehat{\delta}_{_{Total}} =  \bar{Y}^{obs}(1,\mathbf{1}) - \bar{Y}^{obs}(0,\mathbf{0}).
%\end{equation} 





%\subsubsection{Average Direct Effect Estimator}

%\begin{equation} 
%\widehat{\delta}_{direct} =\bar{Y}^{obs}(1,\mathbf{1}) - \bar{Y}^{obs}(0,\mathbf{1}).
%\end{equation} 




%\subsubsection{Average Indirect Effect Estimator}
 
%\begin{equation}\label{kthtest} 
%\widehat{\delta}_{indirect} =  \bar{Y}^{obs}(0,\mathbf{1}) - \bar{Y}^{obs}(0,\mathbf{0}).
%\end{equation} 







%\subsubsection{Average $\ell^{th}$-Nearest Neighbor Indirect Effect Estimator}

%If  we let $W^{*}_{\ell}$ and $W^{*}_{\ell-1}$ as previously defined, 
%the estimator of A$\ell^{th}$NNIE is

%\begin{equation}\label{kthtest} 
%\widehat{\delta}_{\ell^{th}} =  \bar{Y}^{obs}(0,W^{*}_{\ell}) - \bar{Y}^{obs}(0,W^{*}_{\ell-1}).
%\end{equation} 


%Similarly, for treated focal units, the estimator is
%\begin{equation} 
%\widehat{\delta}_{\ell^{th}} =  \bar{Y}^{obs}(1,W^{*}_{\ell}) - \bar{Y}^{obs}(1,W^{*}_{\ell-1}).
%\end{equation} 



\section{Horvitz–Thompson Estimators}
\label{makereference3.5}

%MIKE: This will be moved but I wanted to specify.
% We provide specific formulas for completely-randomized and Bernoulli-randomized experiments.  
% %MIKE: Need Cite. (Samirah: Cite what?)
% However, substantial work has been performed on the design of experiments under interference.
% The estimators will work for any design that allow for marginal and joint treatment probabilities to be computed.
% Even when these probabilities cannot be computed exactly, they may still be estimated, for example, using the approach in~\citep{aronow2017estimating}.
%probabilities may be estimated using
We now derive Horvitz–Thompson (HT) estimators for the estimands described in Section~\ref{makereference3.4}. 
Our approach closely follows that in~\citet{aronow2017estimating}.
% Of particular note, these HT estimators require computing the marginal and joint probabilities of observing various treatment allocations.
Of particular note, these HT estimators require computing, for each unit $i$, various marginal and joint probabilities related to the treatment allocation on $i\cup \mathcal N_{iK}$.
Thankfully, for many common designs, these probabilities can be computed exactly under KNNIM, and we give closed-form solutions for these probabilities under completely-randomized and Bernoulli-randomized designs.
When these probabilities cannot be computed exactly, they may still be estimated, for example, using the approach in~\citet{aronow2017estimating}.
 

%MIKE: Remove Y_{i}^{obs}  notation.  Leave estimators to rely on the potential outcomes.
% Let $Y_{i}^{obs}$ = $Y_{i}^{obs}(W_{i},\mathbf{W}_{\mathcal{N}_{iK}})$ denote the observed potential outcome of unit $i$.
%, which depends on its own treatment status $W_{i}$ and on the treatment statuses of its K-nearest neighbors $\mathbf{W}_{\mathcal{N}_{ik}}$.
%Under KNNIM, there are $2^{K+1}$ potential outcomes for each unit $i$.
%i.e., ${y_{i}(d_{1}), y_{i}(d_{2}), \dots , y_{i}(d_{2^{K+1}})}$.
Let $\pi_i(W,\mathbf{W}_{\mathcal{N}_{K}})$ denote the marginal probability that unit $i$ is given exposure $(W,\mathbf{W}_{\mathcal{N}_{K}})$---that is, the overall treatment allocation assigns treatment $W$ to unit $i$ and assigns treatment conditions $\mathbf{W}_{\mathcal{N}_{K}}$ to $i$'s $K$-neighborhood. 
Define $\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}})$ as the joint probability that units $i$ and $j$ are both given exposure $(W,\mathbf{W}_{\mathcal{N}_{K}})$ and define $\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}), (W',\mathbf{W}'_{\mathcal{N}_{K}}))$ as the joint probability  that unit $i$ receives exposure $(W,\mathbf{W}_{\mathcal{N}_{K}})$ and unit $j$ receives exposure $(W',\mathbf{W}'_{\mathcal{N}_{K}})$.
%the treatment statuses within the $K$--neighborhood unit $i$ has treatment allocation $(W_{i},\mathbf{W}_{\mathcal{N}_{ik}})$ a treatment allocation with unit $i$ given 
% In addition, each unit $i$ has a vector of probabilities ${\pi_{i}(d_{1}), \pi_{i}(d_{2}), \dots ,\pi_{i}(d_{2^{K+1}})}$ where $\pi_{i}(d_{s})$ is the probability of unit $i$ being subject to exposure $d_{s}$.
% For example, under KNNIM, exposure $d_{s}$ can be the case where unit i with all its K-nearest neighbors are treated.
% Let $(W,\mathbf{W}_{\mathcal{N}_{K}})$ denote an exposure that unit $i$ receives under KNNIM where $\pi_{i}(W_{i},W_{\mathcal{N}_{ik}})$ is the probability of unit $i$ being subject to exposure $(W_{i},W_{\mathcal{N}_{ik}})$.
Define indicator variables $I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})$ that are equal to 1 if unit $i$ is given exposure $(W,\mathbf{W}_{\mathcal{N}_{K}})$ and are 0 otherwise.
Finally, let 
\begin{equation}
    \bar y(W,\mathbf{W}_{\mathcal{N}_{K}}) = \frac{1}{N} \sum_{i=1}^N y_i(W,\mathbf{W}_{\mathcal{N}_{K}})
\end{equation}
denote the average potential outcome across all $N$ units for treatment allocation $(W,\mathbf{W}_{\mathcal{N}_{K}})$.
%MIKE: Start here.
The Horvitz-Thompson (HT) estimator~\citep{horvitz1952generalization} for $ \bar y(W,\mathbf{W}_{\mathcal{N}_{K}})$ is
%as follows:
% \begin{align*}\label{Di} 
% I_{i}(W_{i},W_{\mathcal{N}_{ik}}) =  
%     \begin{cases}
%       1, & \text{if unit $i$ is exposed to exposure }  (W_{i},W_{\mathcal{N}_{ik}}). \\
%       0, & \text{otherwise}.
%     \end{cases}
% \end{align*}
%MIKE: Start here.
% \begin{equation} 
% \widehat{y^{T}_{HT}}(W_{i},W_{\mathcal{N}_{ik}}) =  \sum_{i = 1}^{N} I_{i}(W_{i},W_{\mathcal{N}_{ik}}) \frac{Y_{i}^{obs}(W_{i},W_{\mathcal{N}_{ik}})}{\pi_{i}(W_{i},W_{\mathcal{N}_{ik}})}.
% \end{equation}
% The unbiased Horvitz Thompson estimator of the average potential outcomes of units under any exposure $(W_{i},W_{\mathcal{N}_{ik}})$ is
\begin{equation} 
% \bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}) =  \frac{1}{N}\sum_{i = 1}^{N} I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}}) \frac{y_i(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}.
% \bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}) =  \frac{1}{N}\sum_{i = 1}^{N} \frac{Y_i I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}.
% \bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}) =  \frac{1}{N}\sum_{i = 1}^{N}Y_i \frac{ I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}.
 \bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}) =  \frac{1}{N}\sum_{i = 1}^{N}  I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\frac{Y_i}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}.
\end{equation}
This estimator is unbiased for $\bar y(W,\mathbf{W}_{\mathcal{N}_{K}})$,
\begin{equation} 
\label{EXP}
\mathbf{E}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})) 
 =  \bar{y}(W,\mathbf{W}_{\mathcal{N}_{K}}),
\end{equation}
and has variance 
\begin{multline}\label{VAR}
\mathbf{Var}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})) 
 =  \frac{1}{N^2}\sum_{i = 1}^{N}\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})[1 - \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})]\left[ \frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right]^2\\
  + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) - \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})]\frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\frac{y_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})}.
\end{multline}
The covariance of the HT estimators under any two exposures  $(W,\mathbf{W}_{\mathcal{N}_{K}})$ and $(W',\mathbf{W}'_{\mathcal{N}_{K}})$ is 
%MIKE: Is there a way to improve spacing?  Maybe make the ratio term in a second line and add a \times symbol? (Samirah: Done)
\begin{multline}\label{COV}
\mathbf{Cov}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}})) =
 \\\frac{1}{N^2}\left(\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W',\mathbf{W}'_{\mathcal{N}_{K}})) -
 \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})\right] \right.
 \\ \times
\left. \frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}
 \frac{y_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})}{\pi_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})} \right)
 - \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{i}(W',\mathbf{W}'_{\mathcal{N}_{K}})
.
\end{multline}
%MIKE: For dissertation, include the proofs.  But proofs are Probably not necessary for JASA.
%Proofs are provided in Appendix \ref{AppendixA}.


From~\eqref{EXP}, \eqref{VAR}, and \eqref{COV}, the expectation and variance for the difference in HT estimators for the average response under any two unique exposures $(W,\mathbf{W}_{\mathcal{N}_{K}})$, $(W',\mathbf{W}'_{\mathcal{N}_{K}})$ can be computed as follows:
%MIKE: Does it make sense to have the full formula for the variance?
\begin{theorem}\label{Theorem1}
\begin{equation} 
\mathbf{E}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}) - \bar{Y}_{HT}^{obs}(W',\mathbf{W'}_{\mathcal{N}_{K}})) =   \bar y(W,\mathbf{W}_{\mathcal{N}_{K}}) -   \bar y(W',\mathbf{W'}_{\mathcal{N}_{K}}),
\end{equation}
\begin{multline}
\label{VARDIF}
\mathbf{Var}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}) - \bar{Y}_{HT}^{obs}(W',\mathbf{W'}_{\mathcal{N}_{K}}))=\\
\mathbf{Var}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})) + \mathbf{Var}(\bar{Y}_{HT}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}})) 
-2\mathbf{Cov}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}})).
\end{multline}
\end{theorem}

We can then obtain unbiased estimators for the ADE, AIE, ATOT, and the A$\ell$NNIE as follows:
\begin{align} 
\widehat{\delta}_{{HT,dir}} &= \bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1}),\\
\widehat{\delta}_{{HT,ind}} &= \bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0}), \\
\widehat{\delta}_{{HT,tot}}  &= \bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0}),\\
\widehat{\delta}_{HT,\ell} &= \bar{Y}_{HT}^{obs}(0,\mathbf W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,\mathbf W^{*}_{\ell-1}).
\end{align} 
Variances for these estimators are derived as in~\eqref{VARDIF}.
As with the estimands, we have the following relationships between these estimators:
\begin{align}
\label{sumofdirectindirectCH3.1}
\widehat{\delta}_{HT,tot} &= \widehat{\delta}_{HT,dir} + \widehat{\delta}_{HT,ind}, \\
\label{sumofdirectindirectCH3.2}
\widehat{\delta}_{HT,ind} &= \sum_{\ell = 1}^K \widehat{\delta}_{HT,\ell} .
\end{align}
% \begin{align} 
% \widehat{\delta}_{{HT,dir}} &= \bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1}),\\
% \widehat{\delta}_{{HT,ind}} &= \bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0}) \\
% \widehat{\delta}_{{HT,tot}}  &= \bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0}),\\
% \widehat{\delta}_{HT,\ell} &= \bar{Y}_{HT}^{obs}(0,\mathbf W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,\mathbf W^{*}_{\ell-1}).
% \end{align} 

% \begin{theorem}\label{Theorem1}
% Under $K$-NIA assumption,

% \begin{equation} 
% \mathbf{E}(\widehat{\delta}_{HT,Total}) 
%  =  \delta_{Total}.
% \end{equation}

% \begin{multline}
% \mathbf{Var}(\widehat{\delta}_{{HT,Total}}) =  \mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% -2\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0})).
% \end{multline}
% \end{theorem}




% \subsubsection{Average Direct Effect Estimator (HT-ADEE)}

% %Similarly, if we let $d_{s}$ denotes the exposure that the focal unit $i$ and all its K-nearest neighbors are exposed to treatment and $d_{r}$ denotes the exposure that the focal unit $i$ is assigned to control with all its K-nearest neighbors are assigned to treatment, then the average direct effect estimator is


% \begin{equation} 
% \widehat{\delta}_{{HT,direct}} = \bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1}).
% \end{equation} 


% \begin{theorem}\label{Theorem2}
% Under $K$-NIA assumption,

% \begin{equation} 
% \mathbf{E}(\widehat{\delta}_{HT,direct}) 
%  =  \delta_{direct}.
% \end{equation}
% \begin{multline}
% \mathbf{Var}(\widehat{\delta}_{{HT,direct}}) =  \mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% -2\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{1})).
% \end{multline}
% \end{theorem}


% \subsubsection{Average Indirect Effect Estimator (HT-AIEE)}
 
%  %Similarly, if we let $d_{s}$ denotes the exposure that the focal unit $i$ is assigned to control with all its K-nearest neighbors are exposed to treatment and $d_{r}$ denotes the exposure that the focal unit $i$ and all its K-nearest neighbors are assigned to control, then the average indirect effect estimator is

% \begin{equation} 
% \widehat{\delta}_{{HT,indirect}} = \bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0}).
% \end{equation} 


% The average total effect estimator can be decomposed into the average direct and indirect effects estimators as provided in the following lemma, 
% \begin{lemma}\label{lemma2}
% \begin{equation}\label{sumofdirectindirectCH3}
% \widehat{\delta}_{HT,Total} = \widehat{\delta}_{HT,direct} + \widehat{\delta}_{HT,indirect}.
% \end{equation}
% \end{lemma}


% \begin{theorem}\label{Theorem3}
% Under $K$-NIA assumption,

% \begin{equation} 
% \mathbf{E}(\widehat{\delta}_{HT,indirect}) 
%  =  \delta_{indirect}.
% \end{equation}
% \begin{multline}
% \mathbf{Var}(\widehat{\delta}_{HT,indirect}) =  \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) + \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% -2\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0})).
% \end{multline}
% \end{theorem}






% \subsubsection{Average $\ell^{th}$-Nearest Neighbor Indirect Effect Estimator (HT-A$\ell$NNIEE)}
% %Similarly, if we let $d_{s}$ denotes the exposure that the focal unit $i$ is assigned to control with the first $\ell$ nearest neighbors are treated and the rest are control and $d_{r}$ denotes the exposure that the focal unit $i$ is assigned to control with the first $\ell-1$ nearest neighbors are treated and the rest are control, then the average $\ell^{th}$-nearest neighbor effect estimator is as follows.



% \begin{equation} 
% \widehat{\delta}_{{HT,\ell^{th}}} = \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1}).
% \end{equation} 

% MIKE: Keep full discussion in dissertation.
% \subsubsection{Average Total Effect Estimator (HT-ATEE)}
% \label{makereference3.4.1}

% %If we let $d_{s}$ denotes the exposure that the focal unit $i$ and all its K-nearest neighbors are exposed to treatment and $d_{r}$ denotes the exposure that the focal unit $i$ and all its K-nearest neighbors are exposed to control, then the average total effect estimator is


% \begin{equation} 
% \widehat{\delta}_{{HT,Total}}  = \bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0}).
% \end{equation} 


% \begin{theorem}\label{Theorem1}
% Under $K$-NIA assumption,

% \begin{equation} 
% \mathbf{E}(\widehat{\delta}_{HT,Total}) 
%  =  \delta_{Total}.
% \end{equation}

% \begin{multline}
% \mathbf{Var}(\widehat{\delta}_{{HT,Total}}) =  \mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% -2\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0})).
% \end{multline}
% \end{theorem}




% \subsubsection{Average Direct Effect Estimator (HT-ADEE)}

% %Similarly, if we let $d_{s}$ denotes the exposure that the focal unit $i$ and all its K-nearest neighbors are exposed to treatment and $d_{r}$ denotes the exposure that the focal unit $i$ is assigned to control with all its K-nearest neighbors are assigned to treatment, then the average direct effect estimator is


% \begin{equation} 
% \widehat{\delta}_{{HT,direct}} = \bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1}).
% \end{equation} 


% \begin{theorem}\label{Theorem2}
% Under $K$-NIA assumption,

% \begin{equation} 
% \mathbf{E}(\widehat{\delta}_{HT,direct}) 
%  =  \delta_{direct}.
% \end{equation}
% \begin{multline}
% \mathbf{Var}(\widehat{\delta}_{{HT,direct}}) =  \mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% -2\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{1})).
% \end{multline}
% \end{theorem}


% \subsubsection{Average Indirect Effect Estimator (HT-AIEE)}
 
%  %Similarly, if we let $d_{s}$ denotes the exposure that the focal unit $i$ is assigned to control with all its K-nearest neighbors are exposed to treatment and $d_{r}$ denotes the exposure that the focal unit $i$ and all its K-nearest neighbors are assigned to control, then the average indirect effect estimator is

% \begin{equation} 
% \widehat{\delta}_{{HT,indirect}} = \bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0}).
% \end{equation} 


% The average total effect estimator can be decomposed into the average direct and indirect effects estimators as provided in the following lemma, 
% \begin{lemma}\label{lemma2}
% \begin{equation}\label{sumofdirectindirectCH3}
% \widehat{\delta}_{HT,Total} = \widehat{\delta}_{HT,direct} + \widehat{\delta}_{HT,indirect}.
% \end{equation}
% \end{lemma}


% \begin{theorem}\label{Theorem3}
% Under $K$-NIA assumption,

% \begin{equation} 
% \mathbf{E}(\widehat{\delta}_{HT,indirect}) 
%  =  \delta_{indirect}.
% \end{equation}
% \begin{multline}
% \mathbf{Var}(\widehat{\delta}_{HT,indirect}) =  \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) + \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% -2\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0})).
% \end{multline}
% \end{theorem}






% \subsubsection{Average $\ell^{th}$-Nearest Neighbor Indirect Effect Estimator (HT-A$\ell$NNIEE)}
% %Similarly, if we let $d_{s}$ denotes the exposure that the focal unit $i$ is assigned to control with the first $\ell$ nearest neighbors are treated and the rest are control and $d_{r}$ denotes the exposure that the focal unit $i$ is assigned to control with the first $\ell-1$ nearest neighbors are treated and the rest are control, then the average $\ell^{th}$-nearest neighbor effect estimator is as follows.



% \begin{equation} 
% \widehat{\delta}_{{HT,\ell^{th}}} = \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1}).
% \end{equation} 

% %Henceforth, we focus on properties of HT-ADE and HT-A$\ell^{th}$NNIE estimators.

% %Unbiasedness, and the theoretical variance of HT-ADE and HT-A$\ell^{th}$NNIE estimators are provided in the following theorems respectively.

% Note that the indirect effect estimator $\widehat{\delta}_{{HT,indirect}}$ is the sum of all K-nearest neighbors indirect effects estimators ,i.e.,
%  \begin{lemma}\label{lemma3}
% \begin{equation}
% \widehat{\delta}_{{HT,indirect}} = \sum_{\ell = 1}^{K} \widehat{\delta}_{HT,\ell^{th}}.
% %= \widehat{\delta}_{HT,1^{st}} 
% %+ \widehat{\delta}_{HT,2^{nd}}+ \dots + \widehat{\delta}_{HT,\ell^{th}}+ \dots + \widehat{\delta}_{HT,K} 
% %+ \delta_{r}.
% \end{equation}
% \end{lemma}




% \begin{theorem}\label{Theorem4}
% Under $K$-NIA assumption,

% \begin{equation} 
% \mathbf{E}(\widehat{\delta}_{{HT,\ell^{th}}}) 
%  =  \delta_{{\ell^{th}}}.
% \end{equation}
% \end{theorem}



% \begin{multline}
% \mathbf{Var}(\widehat{\delta}_{{HT,\ell^{th}}}) = \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell})) + \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})) 
% \\ -2\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}),\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})).
% \end{multline}
% Proofs of the lemmas and theorems are given in Appendix \ref{AppendixB}.



%\begin{equation}

%\end{equation}


% We aim to estimate the average potential outcomes of units that receive exposure $(W_{i},W_{\mathcal{N}_{ik}})$, i.e., $\bar{y}(W_{i}, W_{\mathcal{N}_{ik}}) = \frac{1}{N}\sum_{i = 1}^{N}y_{i}(W_{i}, W_{\mathcal{N}_{ik}}) = \frac{1}{N}y^{T}(W_{i}, W_{\mathcal{N}_{ik}})$ where $y^{T}(W_{i}, W_{\mathcal{N}_{ik}})$ is the total number of potential outcomes of units exposed to $(W_{i}, W_{\mathcal{N}_{ik}})$.
% By \citet{horvitz1952generalization}, the inverse probability weighted estimator for the total number of potential outcomes of units under any exposure $(W_{i},W_{\mathcal{N}_{ik}})$ is as follows 
% \begin{equation} 
% \widehat{y^{T}_{HT}}(W_{i},W_{\mathcal{N}_{ik}}) =  \sum_{i = 1}^{N} I_{i}(W_{i},W_{\mathcal{N}_{ik}}) \frac{Y_{i}^{obs}(W_{i},W_{\mathcal{N}_{ik}})}{\pi_{i}(W_{i},W_{\mathcal{N}_{ik}})}.
% \end{equation}

% The unbiased Horvitz Thompson estimator of the average potential outcomes of units under any exposure $(W_{i},W_{\mathcal{N}_{ik}})$ is
% \begin{equation} 
% \bar{Y}_{HT}^{obs}(W_{i},W_{\mathcal{N}_{ik}}) =  \frac{1}{N}\sum_{i = 1}^{N} I_{i}(W_{i},W_{\mathcal{N}_{ik}}) \frac{Y_{i}^{obs}(W_{i},W_{\mathcal{N}_{ik}})}{\pi_{i}(W_{i},W_{\mathcal{N}_{ik}})}.
% \end{equation}

%MIKE: Script N when referring to neighborhoods (Done)
%For example
%$\mathbf W_{\mathcal N_{K}}$ instead of $\mathbf W_{N_{K}}$
\subsection{Marginal and Joint Exposure Probabilities \label{probcompute}}

One significant benefit of KNNIM is that this model allows for closed-form expressions 
of the marginal and joint exposure probabilities for many common experimental designs.
We now provide these exposure probabilities under completely-randomized and Bernoulli-randomized designs. 

We first introduce some notation to facilitate this discussion.
For any two units $i$ and $j$, let $b_{ij}$ denote the number of units shared by both $i$ and $j$'s closed $K$-neighborhoods:
$b_{ij} =  |(i \cup \mathcal N_{iK})\cap (j \cup \mathcal N_{jK})|$.
Consider two (not necessarily unique) treatment exposures $(W, \mathbf W_{\mathcal{N}_{K}})$, $(W', \mathbf W'_{\mathcal{N}_{K}})$ given to the closed $K$-neighborhood of units $i$ and $j$ respectively.
These exposures are called \textit{compatible} if they can co-occur within a given treatment assignment.
For example, if $j$ is unit $i$'s nearest neighbor, the exposures $(1,\mathbf 1)$ for unit $i$ and $(0, \mathbf 0)$ for unit $j$ are \textit{incompatible} since unit $j$ is given treatment in the first exposure and control in the second exposure---the probability that $i$ and $j$ can jointly observe two incompatible treatment exposures is 0.
For the exposure $(W, \mathbf W_{\mathcal{N}_{K}})$, let $N_{itK}$ and $N_{icK}$ denote the number of treated and control units respectively within $i \cup \mathcal N_{iK}$. 
For exposure $(W', \mathbf W'_{\mathcal{N}_{K}})$, let $N_{jitK}$ and $N_{jicK}$ denote the number of treated and control units in $(j \cup \mathcal N_{jK}) \setminus (i \cup \mathcal N_{iK}$)---that is, the number of treated and control units for units not already included in $i$'s closed neighborhood.
The specific units given treatment or control do not factor into the probability computations.


% Suppose that unit $i$'s closed $K$-neighborhood $i \cup \mathcal N_{iK}$ is given treatment exposure $(W, \mathbf W_{N_{K}})$.  
% Let $N_{itK}$ and $N_{icK}$ denote the number of treated and control units respectively within $i \cup \mathcal N_{iK}$.
% For another unit $j$, let $b_{ij}$ denote the number of units in the overlap of both closed $K$-neighborhoods for $i$ and $j$: $b_{ij} =  |(i \cup \mathcal N_{iK})\cap (j \cup \mathcal N_{jK})|$.



% , the closed $K$-neighborhood for unit $i$. 
% Suppose that the exposure $(W, \mathbf W_{N_{K}})$ has a total of $N_{itK}$ treatment conditions and $N_{icK}$ control conditions---the specific units given these conditions do not factor into the probability computations.

%MIKE: COMMENT OUT SUBSUBSECTIONS IF NECESSARY
\subsubsection{Exposure Probabilities Under Complete Randomization}

%MIKE: Some cite here woudl be useful.  Any standard experimental design book.
In a completely-randomized design, the number of treated units $N_t$ in the study is selected prior to randomization.  
Each possible treatment has the same $\binom{N}{N_t}^{-1}$ probability of occurring.


% Consider $i \cup \mathcal N_{iK}$, the closed $K$-neighborhood for unit $i$. 
% Suppose that the exposure $(W, \mathbf W_{N_{K}})$ has a total of $N_{itK}$ treatment conditions and $N_{icK}$ control conditions---the specific units given these conditions do not factor into the probability computations.
The marginal probability that  $i \cup \mathcal N_{iK}$ receives exposure $(W, \mathbf W_{\mathcal{N}_{K}})$ is
%there are $N_{tK}$ treated units in $i \cup N_{iK}$ is
%$\pi_{i}(W_{i},W_{\mathcal{N}_{ik}})$ for each unit $i$ can be computed under completely randomized design (CRD) and Bernoulli randomization (BR) respectively as follows,
\begin{equation}\label{CRDpi} 
\pi_{i}(W, \mathbf W_{\mathcal{N}_{K}}) = \frac{\binom {N-K-1} {N_{t}-N_{itK}}}{ \binom N{N_{t}}}.
\end{equation}
The joint probability that unit $i$ receives exposure $(W, \mathbf W_{\mathcal{N}_{K}})$ and unit $j$ receives exposure  $(W', \mathbf W'_{\mathcal{N}_{K}})$ is
\begin{multline}\label{JointCRD}
\pi_{ij}((W, \mathbf W_{\mathcal{N}_{K}}), (W', \mathbf W'_{\mathcal{N}_{K}})) =\\
\begin{cases}
      \frac{\binom {N-K-1} {N_{t}-N_{tK}}}{\binom N{N_{t}}} \frac{\binom {N-2K-2+b_{ij}} {N_{t}-N_{itK}-N_{jitK}}}{\binom {N-K-1} {N_{t}-N_{itK}}}, & (W, \mathbf W_{\mathcal{N}_{K}}),(W', \mathbf W'_{\mathcal{N}_{K}})~\text{are compatible for } i \text{ and } j, \\
       0, & \text{otherwise}.
    \end{cases}
\end{multline}

% Two treatment exposures $(W, \mathbf W_{N_{K}})$, $(W', \mathbf W'_{N_{K}})$ for units $i$ and $j$ respectively are called \textit{compatible} if they can co-occur within a given treatment assignment.
% For example, if $j$ is unit $i$'s nearest neighbor, the exposures $(1,\mathbf 1)$ for unit $i$ and $(0, \mathbf 0)$ for unit $j$ are \textit{incompatible} since unit $j$ is given treatment in the first exposure and control in the second exposure.
% %MIKE: Adding this phrase.
% The joint probability of observing two incompatible treatment assignments is 0.

%MIKE: A figure here may be useful.
% For two closed $K$-neighborhoods $i \cup \mathcal N_{iK}$, $j \cup \mathcal N_{jK}$, let $b_{ij}$ denote the number of units in the overlap of the two neighborhoods: $b_{ij} = |(i \cup \mathcal N_{iK})\cap (j \cup \mathcal N_{jK})|$.
% For two exposures $(W, \mathbf W_{N_{K}})$, $(W', \mathbf W'_{N_{K}})$ for units $i$ and $j$ respectively, let $N_{jitK}$ and $N_{jicK}$ denote the number of treated and control units respectively in $(W', \mathbf W'_{N_{K}})$ not already belonging to $i$'s closed $K$-neighborhood:
% \begin{equation}
%     N_{jitk} = \sum_{\substack{ j' \in \{j \, \cup\, \mathcal N_{jK}\}\\
%                   j' \notin \{i\, \cup \,\mathcal N_{iK}\}}} W_{j'},~~~~N_{jick} = \sum_{\substack{ j' \in \{j \, \cup\, \mathcal N_{jK}\}\\
%                   j' \notin \{i\, \cup \,\mathcal N_{iK}\}}} 1-W_{j'}.
% \end{equation}
% %Let $b$ denote number of the common units in the two $K$-neighborhoods of units $i$ and $j$ i.e., $(i \cup N_{iK} )$ and $(j \cup N_{jK} )$ respectively and  $N_{tf}$ denote number of treated uncommon units in the $K$-neighborhood of unit $j$. 
% Then, the joint probability of units $i$ and $j$ being exposed to  $(W, \mathbf W_{N_{K}})$ and $(W', \mathbf W'_{N_{K}})$ respectively is
% \begin{multline}\label{JointCRD}
% \pi_{ij}((W, \mathbf W_{N_{K}}), (W', \mathbf W'_{N_{K}})) 
% \\ =
% \begin{cases}
%       \frac{\binom {N-K-1} {N_{t}-N_{tK}}}{\binom N{N_{t}}} \frac{\binom {N-2K-2+b_{ij}} {N_{t}-N_{itK}-N_{jitK}}}{\binom {N-K-1} {N_{t}-N_{itK}}}, & (W, \mathbf W_{N_{K}}),(W', \mathbf W'_{N_{K}})~\text{are compatible for } i \text{ and } j, \\
%        0, & \text{otherwise}.
%     \end{cases},
% \end{multline}
\subsubsection{Exposure Probabilities Under Bernoulli Randomization}

%MIKE: Again, need a cite to a textbook.
In a Bernoulli-randomized design, each unit has a pre-specified probability $p$ of being assigned treatment, and treatments are assigned independently across units (\textit{e.g.}~treatment assignment is determined for each unit by flipping a coin that has probability $p$ of landing heads).
Under Bernoulli-randomization, the marginal probability that  $i \cup \mathcal N_{iK}$ receives exposure $(W, \mathbf W_{\mathcal{N}_{K}})$ is
\begin{equation}\label{BRpi}
    \pi_{i}(W, \mathbf W_{\mathcal{N}_{K}}) = p^{N_{itK}}(1-p)^{N_{icK}},
\end{equation}
and joint probability of units $i$ and $j$ being exposed to  $(W, \mathbf W_{\mathcal{N}_{K}})$ and $(W', \mathbf W'_{\mathcal{N}_{K}})$ respectively is
\begin{multline}\label{JointBR}
\pi_{ij}((W_{i},W_{\mathcal{N}_{ik}}),(W'_{j},W'_{\mathcal{N}_{jk}})) 
 = \\
\begin{cases}
      p^{N_{itK}}(1-p)^{N_{icK}}p^{N_{jitK}}(1-p)^{N_{jicK}},  & (W, \mathbf W_{\mathcal{N}_{K}}),(W', \mathbf W'_{\mathcal{N}_{K}})~\text{are compatible for } i \text{ and } j, \\
       0, & \text{otherwise}.
    \end{cases}
\end{multline}

% \begin{equation}\label{JointBR}
% \pi_{ij}((W_{i},W_{\mathcal{N}_{ik}}),(W'_{j},W'_{\mathcal{N}_{jk}})) =
% p^{Ni_{tK}}(1-p^{Ni_{cK}})p^{N_{tf}}(1-p^{N_{cf}}).
% \end{equation}

% where $Ni_{tK}$ and $Ni_{cK}$ is the number of treated and control units in $(i \cup N_{iK})$ and $N_{tf}$ and $N_{cf}$ is the number of treated and control uncommon units in $(j \cup N_{jK})$.

% and 
% \begin{equation}\label{BRpi}
% \pi_{i}(W_{i},W_{\mathcal{N}_{ik}}) = p^{N_{tK}}(1-p^{N_{cK}}),
% \end{equation}

% A completely randomized design uniformly chooses $N_{t}$ units and assigns them to treatment at random while Bernoulli randomization independently assigns each unit to either a treatment or control with probability p where $N_{tK}$ and $N_{cK}$ is the number of treated units and control units in $(i \cup N_{iK} )$ respectively.

% Additionally, let $b$ denote number of the common units in the two $K$-neighborhoods of units $i$ and $j$ i.e., $(i \cup N_{iK} )$ and $(j \cup N_{jK} )$ respectively and  $N_{tf}$ denote number of treated uncommon units in the $K$-neighborhood of unit $j$. Then, the joint probability of units $i$ and $j$ being exposed to two exposures $(W_{i},W_{\mathcal{N}_{ik}})$ and $(W'_{j},W'_{\mathcal{N}_{jk}})$ can be computed under completely randomized design and Bernoulli randomization respectively as follows, 

% and 
% \begin{equation}\label{JointBR}
% \pi_{ij}((W_{i},W_{\mathcal{N}_{ik}}),(W'_{j},W'_{\mathcal{N}_{jk}})) =
% p^{Ni_{tK}}(1 -p^{Ni_{cK}})p^{N_{tf}}(1-p^{N_{cf}}),
% \end{equation}

% where $Ni_{tK}$ and $Ni_{cK}$ is the number of treated and control units in $(i \cup N_{iK})$ and $N_{tf}$ and $N_{cf}$ is the number of treated and control uncommon units in $(j \cup N_{jK})$.
%If we let $\omega$ $\in$ $\Omega$ where $\Omega$ is the set of all possible treatment assignments and $I_{i}(W_{i},W_{\mathcal{N}_{ik}}, \omega)$ is an indicator of whether unit i exposed to exposure $(W_{i},W_{\mathcal{N}_{ik}})$ under the treatment assignment 
%$\omega$, then the joint inclusion probability of units $i$ and $j$ being exposed to the same exposure $(W_{i},W_{\mathcal{N}_{ik}})$; e.i., $(W_{i},W_{\mathcal{N}_{ik}}) = (W_{j},W_{\mathcal{N}_{jk}})$ is 
%\begin{equation}
%\pi_{ij}(W_{i},W_{\mathcal{N}_{ik}}) = \sum_{\omega \in \Omega}I_{i}(W_{i},W_{\mathcal{N}_{ik}}, \omega)I_{j}(W_{j},W_{\mathcal{N}_{jk}}, \omega)Pr(W= \omega),
%\end{equation}
%and the joint probability of units $i$ and $j$ being exposed to two exposures $(W_{i},W_{\mathcal{N}_{ik}})$ and $(W'_{j},W'_{\mathcal{N}_{jk}})$ respectively is
%\begin{equation}
%\pi_{ij}((W_{i},W_{\mathcal{N}_{ik}}),(W'_{j},W'_{\mathcal{N}_{jk}})) = \sum_{\omega \in \Omega}I_{i}(W_{i},W_{\mathcal{N}_{ik}}, \omega)I_{j}(W'_{j},W'_{\mathcal{N}_{jk}}, \omega)Pr(W= \omega).
%\end{equation}

%In practice, $\lvert \Omega \rvert$ can be very large that computing exact probabilities is difficult.
%Instead, joint probabilities can be approximated through simulation by generating R replicates of $\omega$ based on the randomization design as follows.
%\begin{equation}\label{jointprobdk}
%\hat\pi_{ij}(W_{i},W_{\mathcal{N}_{ik}}) = \frac{X_{ij}(W_{i},W_{\mathcal{N}_{ik}})}{R},
%\end{equation}
%where
%\begin{equation}
%X_{ij}(W_{i},W_{\mathcal{N}_{ik}}) = \sum_{r=1}^RI_{i}(W_{i},W_{\mathcal{N}_{ik}}, \omega_{r})I_{j}(W_{j},W_{\mathcal{N}_{jk}}, \omega_{r}),
%\end{equation}
%is the total number that units $i$ and $j$ are exposed to the same exposure $(W_{i},W_{\mathcal{N}_{ik}})$ across all R randomizations.
%Similarly, 
%\begin{equation}\label{jointprobdkds}
%\hat\pi_{ij}((W_{i},W_{\mathcal{N}_{ik}}),(W'_{j},W'_{\mathcal{N}_{jk}})) = \frac{X_{ij}((W_{i},W_{\mathcal{N}_{ik}}),(W'_{j},W'_{\mathcal{N}_{jk}}))}{R},
%\end{equation}
%where

%\begin{equation}
%X_{ij}((W_{i},W_{\mathcal{N}_{ik}}),(W'_{j},W'_{\mathcal{N}_{jk}})) = \sum_{r=1}^RI_{i}(W_{i},W_{\mathcal{N}_{ik}}, \omega_{r})I_{j}(W'_{j},W'_{\mathcal{N}_{jk}}, \omega_{r}).
%\end{equation}





\section{Estimation Under No Weak Interaction Between Direct and Indirect Effects}
\label{makereference3.6}

Next, we consider an additional assumption on the responses of potential outcomes---\textit{no weak interaction between direct and indirect effects}.  
When it holds, this assumption allows for significantly more powerful estimators of direct and indirect effects and makes the two competing definitions of direct and indirect effects in Section~\ref{section4.1} equivalent.
Note, this assumption is similar to the additivity of main effects (AME) assumption considered by~\citet{sussman2017elements}, however, we strictly weaken this assumption by only requiring it to hold for the average of the potential outcomes.
%MIKE: Need to give an example when it may be plausible Additive effects... 


%\setstretch{2}
\begin{assumption}\label{Assumption2}
(No Weak Interaction Between Direct and Indirect Effects) 
There is no weak interaction between direct and indirect effects if, for any two potential treatment exposures $\mathbf{W}_{\mathcal{N}_{K}}, \mathbf{W}'_{\mathcal{N}_{K}}$ on the $K$-neighborhoods
\begin{equation}
\label{noweakint}
\bar y(1,\mathbf{W}_{\mathcal{N}_{K}}) - \bar y (1,\mathbf{W}'_{\mathcal{N}_{K}}) -
(\bar y(0,\mathbf{W}_{\mathcal{N}_{K}}) - \bar y(0,\mathbf{W}'_{\mathcal{N}_{K}})) = 0.
\end{equation}
\end{assumption}

%MIKE: START HERE
While strong, Assumption~\ref{Assumption2} may be plausible under certain settings. 
One such setting could occur if direct and indirect effects of treatment are multiplicative---i.e.~treatment effects may increase response by a certain percentage.  
In this case, taking the log of response may lead to model in which the stronger AME assumption holds.
For example, under KNNIM, we may consider the following model of response:
\begin{equation*}
        Y_i  = Y(0, \mathbf 0)\beta_t^{W_i}\prod_{\ell=1}^K \beta_{\ell}^{W_{i\ell}},
\end{equation*}
where $W_{i\ell}$ indicates whether the $\ell$th nearest neighbor of unit $i$ receives treatment.
After taking logs of both sides, we obtain
\begin{equation*}
        \log Y_i  = Y(0, \mathbf 0) + W_i\log \beta_t + \sum_{\ell=1}^K W_{i\ell} \log \beta_{\ell},
\end{equation*}
thereby satisfying AME---the direct effect is always $\log(\beta_t)$ regardless of the treatment status of any other unit.

Assumption~\ref{Assumption2} may also hold when direct treatment effects are weak relative to the indirect effects. 
For example, consider an experiment on a social network designed to assess the efficacy of an advertisement for an event.  
%whether an advertisement campaign is effective.  
Treatment may involve posting a flyer on a user's ``wall'' if that user had previously expressed interest in attending that event, and response would indicate whether that user attends the event.
% for the event  a flyer on a user's ``wall'' for an event that the user had previously expressed interest in attending, and response would indicate whether that user attends the event.  
Since the user is already interested in the event, the user may not be particularly affected by the flyer on their wall, suggesting a weak direct treatment effect.  
However, if the user notices that their close friends also have flyers on their walls, they all may decide to attend the event together, suggesting a strong indirect effect.

% be effective in their neighborhood interest in the event through flyers po , suggesting a strong notices that many of her 
% . where some users ma treatment is randomly  where treatment and response 
% For example, 

To demonstrate the power of Assumption~\ref{Assumption2}, consider the case of estimating the AIE $\delta_{ind}$.  
When Assumption~\ref{Assumption2} holds,  both  $\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(1,\mathbf{0})$ and $\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})$ unbiasedly estimate $\delta_{ind}$---the latter quantity unbiasedly estimates $\bar{y}(0,\mathbf{1}) - \bar{y}(0,\mathbf{0})$, and under  Assumption~\ref{Assumption2},  $\bar{y}(0,\mathbf{1}) - \bar{y}(0,\mathbf{0}) = \bar{y}(1,\mathbf{1}) - \bar{y}(1,\mathbf{0}) = \delta_{ind}$. 
Thus, these two estimators can be combined to yield a more powerful unbiased estimate of the AIE:
\begin{equation}
    \hat\delta_{ind} = c_1\left(\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(1,\mathbf{0})\right) + c_2 \left(\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})\right)
\end{equation}
where the weights $c_1, c_2$ satisfy $c_1 + c_2 = 1$.  
An analogous approach can be used to derive estimators for the ADE and the A$\ell$NNIEs.

Given strong prior knowledge about the values of the potential outcomes, $c_1$ and $c_2$ may be adjusted to reduce the variance of the estimator (similar to Neyman allocation or optimal allocation in \citet{lohr2019sampling}).
However, when no prior knowledge is available, setting $c_1 = c_2 = 1/2$ may be preferable.
%still yield an effective estimator.  
Additionally, under this choice of weights, 
estimators can be decomposed analogously to~\eqref{sumofdirectindirectCH3.1} and~\eqref{sumofdirectindirectCH3.2}.
%as with the estimators under Assumption~\ref{Assumption1}, 
% for example estimator for ATOT can be decomposed into that for the ADE and the AIE, .
Thus, we proceed assuming $c_1 = c_2 = 1/2$.

\subsection{Estimators Under the no Weak Interaction Assumption}

We begin by proposing unbiased estimators for the average direct, indirect, and $\ell^{th}$ nearest neighbor effects:
\begin{theorem}
Under Assumption~\ref{Assumption2}, the following are unbiased estimators for the ADE, AIE, and the A$\ell$NNIE:
\begin{align} 
    \widehat{\delta^*}_{{HT,dir}} &= 1/2\left(\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1})\right) + 1/2\left(\bar{Y}_{HT}^{obs}(1,\mathbf{0}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})\right) ,\\
    \widehat{\delta^*}_{{HT,ind}} &= 1/2\left(\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(1,\mathbf{0})\right) + 1/2\left(\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})\right) , \\
    % \widehat{\delta}_{{HT,tot}}  &= \bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0}),\\
    \widehat{\delta^*}_{HT,\ell} &= 1/2\left(\bar{Y}_{HT}^{obs}(1,\mathbf W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,\mathbf W^{*}_{\ell-1})\right) +1/2\left(\bar{Y}_{HT}^{obs}(0,\mathbf W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,\mathbf W^{*}_{\ell-1})\right).
\end{align} 
\end{theorem}
\begin{proof}
Earlier in this section, we detailed the proof of unbiasedness for   $\widehat{\delta^*}_{{HT,ind}}$. 
The argument for unbiasedness of $\widehat{\delta^*}_{HT,\ell}$ proceeds identically, as does that for $\widehat{\delta^*}_{HT,dir}$ after rewriting~\eqref{noweakint} as
\begin{equation}
    \bar y(1,\mathbf{W}_{\mathcal{N}_{K}})  -
    \bar y(0,\mathbf{W}_{\mathcal{N}_{K}})   = \bar y (1,\mathbf{W}'_{\mathcal{N}_{K}}) - \bar y(0,\mathbf{W}'_{\mathcal{N}_{K}}).
\end{equation}
\end{proof}
 For brevity, to distinguish between these estimators and the estimators defined in Section~\ref{makereference3.5}, we will refer to the estimators $\widehat{\delta}_{{HT,dir}}$, $\widehat{\delta}_{{HT,ind}}$, and, $\widehat{\delta}_{HT,\ell}$ as estimators ``under Assumption 1'' and $\widehat{\delta}^*_{{HT,dir}}$, $\widehat{\delta}^*_{{HT,ind}}$, and, $\widehat{\delta}^*_{HT,\ell}$ as ``under Assumption 2,'' even though both estimators can be used when Assumption~\ref{Assumption2} holds.



These estimators satisfy a similar decomposition to those derived under Assumption~\ref{Assumption1}:
\begin{align}
\label{sumofdirectindirectCH36.1}
\widehat{\delta}_{HT,tot} &= \widehat{\delta^*}_{HT,dir} + \widehat{\delta^*}_{HT,ind}, \\
\label{sumofdirectindirectCH36.2}
\widehat{\delta^*}_{HT,ind} &= \sum_{\ell = 1}^K \widehat{\delta^*}_{HT,\ell} .
\end{align}
Additionally, the variance of $\widehat{\delta^*}_{{HT,dir}}$, $\widehat{\delta^*}_{HT,ind}$, and $\widehat{\delta^*}_{HT,\ell}$ can be derived from repeated application of~\eqref{VAR} and~\eqref{COV}.  
This result generalizes to similar estimators using any four treatment exposures.
% Additionally, the variance of $\widehat{\delta^*}_{{HT,dir}}$, $\widehat{\delta^*}_{HT,ind}$, $\widehat{\delta^*}_{HT,ind}$, and $\widehat{\delta^*}_{HT,\ell}$ can be derived from repeated application 
% from applying the following theorem, which is obtained by repeated application:
%  for each estimator under Assumption~\ref{Assumption2} can be derived using in the following theorem:
% if we assume that there is no interaction between the direct and indirect effects, the unbiased Horvitz Thompson estimator of HT-ATOT is provided in the following lemma,
%Variances for these estimators are derived as in~\eqref{VARDIF}.

%FOR MIKE AND SAMIRAH
%START HERE WITH A GENERAL FORM FOR VARIANCE OF THESE TYPES OF ESTIMATORS.


% The above estimators under Assumption~\ref{Assumption2} can be represented by a general form with four different exposures $(W, \mathbf W_{\mathcal{N}_{K}}), (W', \mathbf W'_{\mathcal{N}_{K}}), (W^{*}, \mathbf W^{*}_{\mathcal{N}_{K}}), (W^{*\prime}, \mathbf W^{*\prime}_{\mathcal{N}_{K}})$ as follows: 
%  \begin{align} 
%     \widehat{\delta^*} & = 1/2\left(\bar{Y}_{HT}^{obs}(W, \mathbf W_{\mathcal{N}_{K}}) - \bar{Y}_{HT}^{obs}(W', \mathbf W'_{\mathcal{N}_{K}})\right) + 1/2\left(\bar{Y}_{HT}^{obs}(W^{*}, \mathbf W^{*}_{\mathcal{N}_{K}}) - \bar{Y}_{HT}^{obs}(W^{*\prime}, \mathbf W^{*\prime}_{\mathcal{N}_{K}})\right).
% \end{align} 

% for any 
%  applying the following theorem:
%  for each estimator under Assumption~\ref{Assumption2} can be derived using in the following theorem:
%   The variance of estimator under Assumption~\ref{Assumption2} can be derived from applying in the following theorem, :
%MIKE: I DON'T THINK THEY NEED TO BE UNIQUE
\begin{theorem}\label{Theorem6} 
For any four treatment exposures $(W, \mathbf W_{\mathcal{N}_{K}}), (W', \mathbf W'_{\mathcal{N}_{K}}), (W^{*}, \mathbf W^{*}_{\mathcal{N}_{K}}), (W^{*\prime}, \mathbf W^{*\prime}_{\mathcal{N}_{K}})$,
\begin{align}
 \label{varianceass2directCH3}
% \mathbf{E}(\widehat{\delta^*}) & = 1/2\left(\bar y(W,\mathbf{W}_{\mathcal{N}_{K}}) -   \bar y(W',\mathbf{W'}_{\mathcal{N}_{K}})\right)+ 1/2\left(\bar y(W^{*}, \mathbf W^{*}_{\mathcal{N}_{K}}) -   \bar y(W^{*\prime}, \mathbf W^{*\prime}_{\mathcal{N}_{K}})\right), \\  
&\mathbf{Var} \left[1/2\left(\bar{Y}_{HT}^{obs}(W, \mathbf W_{\mathcal{N}_{K}}) - \bar{Y}_{HT}^{obs}(W', \mathbf W'_{\mathcal{N}_{K}})\right) + 1/2\left(\bar{Y}_{HT}^{obs}(W^{*}, \mathbf W^{*}_{\mathcal{N}_{K}}) - \bar{Y}_{HT}^{obs}(W^{*\prime}, \mathbf W^{*\prime}_{\mathcal{N}_{K}})\right)\right] \nn \\  
 = & 1/4\mathbf{Var}(\bar{Y}_{HT}^{obs}(W, \mathbf W_{\mathcal{N}_{K}})) + 1/4\mathbf{Var}(\bar{Y}_{HT}^{obs}(W', \mathbf W'_{\mathcal{N}_{K}})) \nn \\  
 & +  1/4 \mathbf{Var}(\bar{Y}_{HT}^{obs}(W^{*}, \mathbf W^{*}_{\mathcal{N}_{K}})) + 1/4\mathbf{Var}(\bar{Y}_{HT}^{obs}(W^{*'}, \mathbf W^{*'}_{\mathcal{N}_{K}})) \nn \\  
& -  1/2 \mathbf{Cov}(\bar{Y}_{HT}^{obs}(W, \mathbf W_{\mathcal{N}_{K}}), \bar{Y}_{HT}^{obs}(W', \mathbf W'_{\mathcal{N}_{K}})) 
 + 1/2 \mathbf{Cov}(\bar{Y}_{HT}^{obs}(W, \mathbf W_{\mathcal{N}_{K}}), \bar{Y}_{HT}^{obs}(W^{*}, \mathbf W^{*}_{\mathcal{N}_{K}})) \nn \\  
 & -  1/2 \mathbf{Cov}(\bar{Y}_{HT}^{obs}(W, \mathbf W_{\mathcal{N}_{K}}), \bar{Y}_{HT}^{obs}(W^{*'}, \mathbf W^{*'}_{\mathcal{N}_{K}})) 
 - 1/2 \mathbf{Cov}(\bar{Y}_{HT}^{obs}(W', \mathbf W'_{\mathcal{N}_{K}}), \bar{Y}_{HT}^{obs}(W^{*}, \mathbf W^{*}_{\mathcal{N}_{K}}))  \nn \\  
 & +  1/2 \mathbf{Cov}(\bar{Y}_{HT}^{obs}(W', \mathbf W'_{\mathcal{N}_{K}}), \bar{Y}_{HT}^{obs}(W^{*'}, \mathbf W^{*'}_{\mathcal{N}_{K}})) 
 - 1/2 \mathbf{Cov}(\bar{Y}_{HT}^{obs}(W^{*}, \mathbf W^{*}_{\mathcal{N}_{K}}), \bar{Y}_{HT}^{obs}(W^{*'}, \mathbf W^{*'}_{\mathcal{N}_{K}})).
\end{align}
\end{theorem}






% \begin{lemma}\label{lemma4}
% \begin{equation}\label{Ass2TotalApp}
% \widehat{\delta^*}_{HT,Total} =  \widehat{\delta^*}_{HT,direct} +
% \widehat{\delta^*}_{HT,indirect}.
% \end{equation} 
% \end{lemma}

% For $c_{1} = c_{2} = \frac{1}{2}$ as in $\widehat{\delta^*}_{HT,direct}$ and $\widehat{\delta^*}_{HT,indirect}$ in equations \ref{Assumption2directCH3} and \ref{Ass2indirectCH3}, then $\widehat{\delta^*}_{HT,Total} = \widehat{\delta}_{HT,Total}$

% \begin{theorem}\label{Theorem5} 
% Under the no-interaction between direct and indirect effects assumption, and for $c_{1} = c_{2} = \frac{1}{2}$,

% \begin{equation}
% \mathbf{E}(\widehat{\delta^*}_{{HT,Total}}) =\delta_{Total}.
% \end{equation}

% \begin{equation}
%     \mathbf{Var}(\widehat{\delta^*}_{{HT,Total}}) = \mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% -2\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}).
% \end{equation}

% \end{theorem}

% Under assumption \ref{Assumption2}, the unbiased Horvitz Thompson estimator of HT-ADE is as follows,


% \begin{equation}\label{Assumption2directCH3}
% \widehat{\delta^*}_{HT, direct} =  \frac{1}{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1})] +
% \frac{1}{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{0}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})].
% \end{equation} 


% \begin{theorem}\label{Theorem6} 
% Under the no-interaction between direct and indirect effects assumption, 
% \begin{equation} 
% \mathbf{E}(\widehat{\delta^*}_{HT,direct}) = \delta_{direct}.
% \end{equation} 

% \begin{multline} \label{varianceass2directCH3}
% \mathbf{Var}(\widehat{\delta^*}_{HT,direct}) = \frac{1}{4}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + \frac{1}{4}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% \\ + \frac{1}{4}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{0})) + \frac{1}{4}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% \\ - 2(\frac{1}{4}) \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% + 2(\frac{1}{4}) \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(1,\mathbf{0})) 
% \\ -2(\frac{1}{4}) \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% -2(\frac{1}{4}) \mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}), \bar{Y}_{HT}^{obs}(1,\mathbf{0})) 
% \\ + 2(\frac{1}{4}) \mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% -2(\frac{1}{4}) \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})).
% \end{multline}
% \end{theorem}

% Under assumption \ref{Assumption2}, the unbiased Horvitz Thompson estimator of HT-AIE is as follows.

% \begin{equation}\label{Ass2indirectCH3}
% \widehat{\delta^*}_{HT,indirect} =  \frac{1}{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(1,\mathbf{0})] +
% \frac{1}{2}[\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})].
% \end{equation} 

% \begin{theorem}\label{Theorem7} 
% Under the no-interaction between direct and indirect effects assumption, 
% \begin{equation} 
% \mathbf{E}(\widehat{\delta^*}_{HT,indirect}) = \delta_{indirect}.
% \end{equation} 

% \begin{multline} \label{varss2indirectCH3}
% \mathbf{Var}(\widehat{\delta^*}_{HT,indirect}) = \frac{1}{4}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + \frac{1}{4}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{0})) 
% \\ + \frac{1}{4}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) + \frac{1}{4}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% \\ - 2(\frac{1}{4}) \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(1,\mathbf{0})) 
% + 2(\frac{1}{4}) \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% \\ -2(\frac{1}{4}) \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% -2(\frac{1}{4}) \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}), \bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% \\ + 2(\frac{1}{4}) \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% -2(\frac{1}{4}) \mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})).
% \end{multline}
% \end{theorem}


% Under assumption \ref{Assumption2}, if we assume that there is no interaction between the direct and indirect effects, the unbiased Horvitz Thompson estimator of A$\ell^{th}$NNIE is as follows,

% \begin{equation} 
% \widehat{\delta^*}_{{HT,\ell^{th}}} =  \frac{1}{2}[\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})] +
% \frac{1}{2}[\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})].
% \end{equation} 


% \begin{lemma}\label{lemma5}
% \begin{equation}\label{indirectass2}
%     \widehat{\delta^{*}}_{HT,indirect} =  \sum_{\ell =1}^{K}\widehat{\delta^{*}}_{HT,\ell^{th}}.
% \end{equation}
% \end{lemma}


% Unbiasedness, and theoretical variance of HT-A$\ell^{th}$NNIE estimator under assumption \ref{Assumption2} are provided in the following theorem.

% \begin{theorem}\label{Theorem8} 
% Under the no-interaction between direct and indirect effects assumption, 

% \begin{equation} 
% \mathbf{E}(\widehat{\delta^*}_{{HT,\ell^{th}}}) 
%  =  \delta_{{\ell^{th}}}.
% \end{equation}
% \end{theorem}

% \begin{multline} 
% \label{varianceassumption2}
% \mathbf{Var}(\widehat{\delta^*}_{{HT,\ell^{th}}}) = \frac{1}{4}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})) + \frac{1}{4}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})) 
% \\ + \frac{1}{4}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell})) + \frac{1}{4}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1}))
% \\ - 2(\frac{1}{4}) \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}), \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})) 
% + 2(\frac{1}{4}) \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}), \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell})) 
% \\ -2(\frac{1}{4}) \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}), \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})) 
% -2(\frac{1}{4}) \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1}), \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell})) 
% \\ + 2(\frac{1}{4}) \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1}), \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})) 
% -2(\frac{1}{4}) \mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}), \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})) .
% \end{multline}


% Proofs of the lemmas and theorems are given in Appendix \ref{AppendixB}.


% % Note that in $\widehat{\delta^*}_{{HT,\ell^{th}}}$ under assumption \ref{Assumption2}, we chose the weights of treated and control units to be $c_{\ell1} = c_{\ell2} = \frac{1}{2}$.
% % However, let $S^2_{11} = \mathbf{Var}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}))$, $S^2_{12} = \mathbf{Var}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1}))$,
% % $S^2_{21} = \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}))$ and $S^2_{22} = \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1}))$ and assuming that the covariance components in \ref{varianceassumption2} are equal to zero.
% % If the experimenter has prior knowledge from previous studies on $S^2_{11}$, $S^2_{12}$, $S^2_{21}$ and  $S^2_{22}$, then $c_{\ell 1}$ and $c_{\ell 2}$
% % can be chosen such that $c_{\ell 1} = \frac{S^2_{21} + S^2_{22}}{S^2_{11} + S^2_{12} + S^2_{21} + S^2_{22}}$ and $c_{\ell 2} = \frac{S^2_{11} + S^2_{12}}{S^2_{11} + S^2_{12} + S^2_{21} + S^2_{22}}$ which give the minimum variance of HT-A$\ell$NNIEE under assumption \ref{Assumption2}.
% % This applies likewise to other estimators under assumption \ref{Assumption2}.
% % The proof is given in Appendix \ref{AppendixB}.



 





% % %Note that equations \ref{sumoflthNN} and \ref{sumofdirectindirect} hold under assumption \ref{Assumption2} as well as assumption \ref{Assumption3}.




% % %\begin{remark}
% % %In KNNIM, there is no interaction between nearest neighbors treatments.
% % %i.e., for each unit $i$ in a network G and for any K nearest neighbors %$k$,$k'$ $\in {\mathcal{N}_{ik}}$ and if we let $\delta_{kk'}$ be the %indirect effect of the interaction between $k$ and $k'$ treatments, there is %no interaction between K nearest neighbors indirect effects, i.e., 
% % %\begin{equation} 
% % %\delta_{kk'} = 0.
% % %\end{equation}
% % %However, this can be generalized to more general forms that include interaction between the $K$ nearest neighbors treatments.
% % %\end{remark}



% % Restricting the neighborhood interference assumption (NIA) in \citep{sussman2017elements} to K nearest neighbors, assumption \ref{Assumption1} states that the potential outcome of unit $i$ is only affected by its treatment and by the treatments assigned to its K nearest neighbors such that changing treatments for other units outside the $K$-neighborhood will not affect the potential outcome of unit $i$ (i.e., for each unit $i$ and for all $j$ $\in$ $\mathcal{N}_{-ik}$, $\delta_{j^{th}}$ = 0 on unit $i$ where $\delta_{j^{th}}$ is the $j^{th}$ nearest neighbor indirect effect on unit i ).
% % On the other hand, assumption \ref{Assumption2} states that the indirect effect will be the same across all units' treatment groups.
% % Similarly, the direct effect will be the same across all $K$-nearest neighbors treatment groups.
% %while assumption 4 specifies the magnitude of the nearest neighbor effect on unit $i$ where the smaller the distance between unit $i$ and its neighbor, the stronger the relationship and the larger the effect on the outcome of unit $i$.



% %\textbf{corollary 1} (Significance of the Lower Order Indirect Effects).
% %For each unit $i$ in a network G and for any nearest neighbors $\ell$,$\ell'$ $\in {\mathcal{N}_{ik}}$ for which $\delta_{\ell}$ $>$ $\delta_{\ell'}$, if $\delta_{\ell}$ = 0, then $\delta_{\ell'}$ = 0, i.e., if the treatment indirect effect of the lower order nearest neighbors of an individual is not significant, then there is no higher order K nearest neighbor interference.

% %Corollary 1 follows from assumption 4 that insignificance effect of the lower order nearest neighbor implies insignificance of the higher order nearest neighbors indirect effects. 
% %For example, if the first nearest neighbor indirect effect which has the greatest effect is not significant, the effects of the other nearest neighbors will be insignificant as well.  










\section{Variance Estimators}
\label{makereference3.7}

We extend the work provided in \citep{aronow2013conservative, aronow2017estimating} and \citep{lohr2019sampling} to derive conservative estimators for the variance for all considered estimators under KNNIM.  
This requires estimating all variance and covariance terms in~\eqref{VARDIF} and~\eqref{varianceass2directCH3}.

%MIKE: START HERE

% In order to estimate the variance of the provided estimators, we estimate all variance and covariance components in  those variances such that the Horvitz–Thompson estimated variance of the population average of potentail outcomes under exposure $(W_{i},W_{\mathcal{N}_{ik}})$ where units $i$ and $j$ receive the same exposure, i.e. $(W_{i},W_{\mathcal{N}_{ik}})$ = $(W_{j},W_{\mathcal{N}_{jk}})$, is as follows: 

We begin by estimating the $\mathbf{Var}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}))$ terms.
The standard Horvitz-Thompson estimator for these variances is
%MIKE: Need to edit notation
\begin{multline}
\widehat{\mathbf{Var}}_{HT}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})) =  \frac{1}{N^2}\sum_{i = 1}^NI_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})[1 - \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})]\left[ \frac{Y_{i}}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right]^2
\\  + \frac{1}{N^2}\sum_{i = 1}^N\sum_{j \ne i}I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})\frac{[\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) - \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})]}{\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}})}
\\ \times \frac{Y_{i}}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\frac{Y_{j}}{\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})}.
\end{multline}

% \begin{multline}
% \widehat{\mathbf{Var}}_{HT}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})) =  \frac{1}{N^2}\sum_{i = 1}^NI_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})[1 - \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})]\left[ \frac{Y_{i}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right]^2
% \\  + \frac{1}{N^2}\sum_{i = 1}^N\sum_{j \ne i}I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})\frac{[\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) - \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})]}{\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}})}
% \\ \times \frac{Y_{i}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\frac{Y_{j}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})}.
% \end{multline}

If the joint probabilities $\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) > 0$ for all $i$ and $j$, then this estimated variance is unbiased.
However, under KNNIM, there will likely be incompatibility of this exposure for some $i'$ and $j'$ (see Section~\ref{probcompute} for details), and hence $\pi_{i'j'}(W,\mathbf{W}_{\mathcal{N}_{K}}) = 0$ for these units.
%MIKE: Adding in Var to the expression
These probabilities lead to bias in the estimate of $\mathbf{Var}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}))$:
\begin{equation}
\label{eq:exvarhat}
\mathbf{E}(\widehat{\mathbf{Var}}_{HT}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}))) =  \mathbf{Var}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})) + A_{Var}
\end{equation}
where 
\begin{equation}
\label{eq:defavar}
A_{Var} = \frac{1}{N^2}\sum_{i = 1}^N\sum_{\substack{j \ne i,\\ \pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) = 0 }} y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W,\mathbf{W}_{\mathcal{N}_{K}}).
\end{equation}

Young’s inequality as derived in \citet{aronow2013conservative, aronow2017estimating} can be used to obtain a quantity $\widehat A_{Var}$ that satisfies $\mathbf{E}(\widehat A_{Var}) \geq |A_{Var}|$.  
We now give a brief proof of Young's inequality as applied to KNNIM, and demonstrate how it can be used to find $\widehat A_{Var}$.
%For MIKE: Could REWRITE IN TERMS OF ABSOLUTE VALUES to save space.
\begin{lemma}
\label{lem:consest}
    For any two (not necessarily unique) exposures $(W,\mathbf{W}_{\mathcal{N}_{K}}), (W',\mathbf{W'}_{\mathcal{N}_{K}})$,
    \begin{align}
 %        & -\mathbf{E}\left(\frac{I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})Y_{i}^2}{2\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})} +
 % \frac{I_{j}(W',\mathbf{W'}_{\mathcal{N}_{K}})Y_{j}^2}{2\pi_{j}(W',\mathbf{W'}_{\mathcal{N}_{K}})}\right)  \leq 
 |y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W',\mathbf{W'}_{\mathcal{N}_{K}})| \leq 
 \mathbf{E}\left(\frac{I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})Y_{i}^2}{2\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})} +
 \frac{I_{j}(W',\mathbf{W'}_{\mathcal{N}_{K}})Y_{j}^2}{2\pi_{j}(W',\mathbf{W'}_{\mathcal{N}_{K}})}\right).
    \end{align}
\end{lemma}
\begin{proof}
To see this, first note that
\begin{equation}
   % -\sqrt{ y^2_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y^2_{j}(W',\mathbf{W'}_{\mathcal{N}_{K}})}\leq 
   |y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W',\mathbf{W'}_{\mathcal{N}_{K}})| = \sqrt{ y^2_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y^2_{j}(W',\mathbf{W'}_{\mathcal{N}_{K}})}.
\end{equation}
The upper-bound is then proven by applying the arithmetic-mean geometric-mean inequality~\citep{steele2004cauchy}
\begin{equation}
    \sqrt{ y^2_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y^2_{j}(W',\mathbf{W'}_{\mathcal{N}_{K}})} \leq  \frac{y^2_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}{2} +
       \frac{y^2_{j}(W',\mathbf{W'}_{\mathcal{N}_{K}})}{2}
\end{equation}
and by noting that the right-hand side can be unbiasedly estimated by
    \begin{equation}
        \frac{I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})Y_{i}^2}{2\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})} +
 \frac{I_{j}(W',\mathbf{W'}_{\mathcal{N}_{K}})Y_{j}^2}{2\pi_{j}(W',\mathbf{W'}_{\mathcal{N}_{K}})}.
    \end{equation}
%The lower bound is proven analogously.
%     To see the upper bound, by the arithmetic-mean geometric-mean inequality~\citep{steele2004cauchy}, we have that
%     \begin{equation}
%         y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W',\mathbf{W'}_{\mathcal{N}_{K}}) \leq \sqrt{ y^2_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y^2_{j}(W',\mathbf{W'}_{\mathcal{N}_{K}})} \leq  \frac{y^2_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}{2} +
%         \frac{y^2_{j}(W',\mathbf{W'}_{\mathcal{N}_{K}})}{2}.
%     \end{equation}
%     Note that the last term is unbiasedly estimated by
%     \begin{equation}
%         \frac{I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})Y_{i}^2}{2\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})} +
%  \frac{I_{j}(W',\mathbf{W'}_{\mathcal{N}_{K}})Y_{j}^2}{2\pi_{j}(W',\mathbf{W'}_{\mathcal{N}_{K}})}.
%     \end{equation}
%     The inequality is then obtained after applying expectations.
%     The lower bound is proven in the same way after observing that 
%     \begin{equation}
%         y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W',\mathbf{W'}_{\mathcal{N}_{K}}) \geq -\sqrt{ y^2_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y^2_{j}(W',\mathbf{W'}_{\mathcal{N}_{K}})} 
%     \end{equation}
\end{proof}

Thus, we define 
\begin{equation}
\widehat{A}_{Var}(W,\mathbf{W}_{\mathcal{N}_{K}}) = \frac{1}{N^2}\sum_{i = 1}^N\sum_{\substack{j \ne i,\\ \pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) = 0 }}
\left[\frac{I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})Y_{i}^{2}}{2\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})} + \frac{I_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})Y_{j}^{2}}{2\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right].
\end{equation}
From Lemma~\ref{lem:consest}, linearity of expectations, and the triangle inequality, it follows that
\begin{align}
\mathbf E\left(\widehat{A}_{Var}(W,\mathbf{W}_{\mathcal{N}_{K}})\right) =~ & 
\frac{1}{N^2}\sum_{i = 1}^N\sum_{\substack{j \ne i,\\ \pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) = 0 }}
\left[\frac{y^2_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}{2} +
       \frac{y^2_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})}{2}\right] \nn \\ \geq ~ & \frac{1}{N^2}\sum_{i = 1}^N\sum_{\substack{j \ne i,\\ \pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) = 0 }}
|y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})| 
\nn \\ \geq ~ & \left |\frac{1}{N^2}\sum_{i = 1}^N\sum_{\substack{j \ne i,\\ \pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) = 0 }}
y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})\right| = |A_{Var}|. 
\label{eq:consineq}
\end{align}
%MIKE: I wonder if calling ths Var_U rather than Var_A might be better... U for upper-bound, L for lower bound (for covariances)
Therefore, by setting
\begin{equation}
\widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})) = \widehat{\mathbf{Var}}_{HT}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})) + \widehat{A}_{Var}(W,\mathbf{W}_{\mathcal{N}_{K}}),
\end{equation}
we obtain a conservative estimate of the variance:
\begin{equation}
    \mathbf{E}\left(\widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}))\right) \geq \mathbf{Var}_{HT}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})) + |A_{Var}| + A_{Var} \geq \mathbf{Var}_{HT}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}))
\end{equation}
% where
% \begin{multline}
% \widehat{A^{*}}(W,\mathbf{W}_{\mathcal{N}_{K}}) = \frac{1}{N^2}\sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) = 0 }
% \left[\frac{I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})Y_{i}^{2^{obs}}(W,\mathbf{W}_{\mathcal{N}_{K}})}{2\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right.
% \\ \left. + \frac{I_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})Y_{j}^{2^{obs}}(W,\mathbf{W}_{\mathcal{N}_{K}})}{2\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right].
% \end{multline}

% Then, $\widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}))$ is a conservative estimator for the variance of the Horvitz-Thompson estimator of the average potential outcomes under exposure $(W,\mathbf{W}_{\mathcal{N}_{K}})$(the proof in \citet{aronow2013conservative, aronow2017estimating} is reproduced in Appendix A).

We apply a similar approach for estimating covariance components. 
Define
\begin{multline}\label{covhat11}
\widehat{\mathbf{Cov}}_{HT}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}})) = 
\\ \frac{1}{N^2}\sum_{i = 1}^N\sum_{\substack{j \in \{1,\ldots,N\}\\ \pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W', \mathbf{W}'_{\mathcal{N}_{K}})) > 0 } }
\left[\frac{I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}})}{\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W', \mathbf{W}'_{\mathcal{N}_{K}}))}
\frac{Y_{i}}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\frac{Y_{j}}{\pi_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}})}\right]
\\ \times \left[\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W', \mathbf{W}'_{\mathcal{N}_{K}}))-\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}})\right]
\end{multline}




% In particular, the last term in the covariance in \eqref{COV} is unidentified  because each unit receives only one exposure and can only be observed under this exposure. 
% Hence, there is no unbiased estimator for the variance of the proposed estimators.
% However, if the joint probabilities $\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W', \mathbf{W}'_{\mathcal{N}_{K}}))$ for two different exposures $(W,\mathbf{W}_{\mathcal{N}_{K}})$ and $(W', \mathbf{W}'_{\mathcal{N}_{K}}) > 0$  for all $i$ and $j$, an estimator for the covaraince in \ref{COV} can be follows:

% \begin{multline}\label{covhat1}
% \widehat{\mathbf{Cov}}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}})) = 
% \\\frac{1}{N^2}\sum_{i = 1}^N\sum_{j \ne i }
% \left[\frac{I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}})}{\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W', \mathbf{W}'_{\mathcal{N}_{K}}))}
% \frac{Y_{i}}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\frac{Y_{j}}{\pi_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}})}\right.
% \\ \left.\times [\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W', \mathbf{W}'_{\mathcal{N}_{K}}))-\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}})]\right]
% \\  - \frac{1}{N^2}\sum_{i = 1}^N\left[\frac{I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})Y_{i}^{2}}{2\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})} + \frac{I_{i}(W', \mathbf{W}'_{\mathcal{N}_{K}})Y_{i}^{2})}{2\pi_{i}(W', \mathbf{W}'_{\mathcal{N}_{K}})}\right]
% \end{multline}

% \begin{multline}\label{covhat1}
% \widehat{\mathbf{Cov}}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}})))= 
% \\\frac{1}{N^2}\sum_{i \in U}\sum_{j \in U, j \ne i}\left[
% \frac{I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}{\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W', \mathbf{W}'_{\mathcal{N}_{K}})))}
% \frac{Y_{i}}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\frac{Y_{j}}{\pi_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}\right.
% \\ \left.\times [\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W', \mathbf{W}'_{\mathcal{N}_{K}})))-\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}}))]\right]
% \\  - \frac{1}{N^2}\sum_{i \in U}\left[\frac{I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})Y_{i}^{2}}{2\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})} + \frac{I_{i}(W', \mathbf{W}'_{\mathcal{N}_{K}}))Y_{i}^{2})}{2\pi_{i}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}\right]
% \end{multline}


% \begin{multline}\label{covhat1}
% \widehat{\mathbf{Cov}}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}})))= 
% \\\frac{1}{N^2}\sum_{i \in U}\sum_{j \in U, j \ne i}\left[
% \frac{I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}{\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W', \mathbf{W}'_{\mathcal{N}_{K}})))}
% \frac{Y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\frac{Y_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}{\pi_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}\right.
% \\ \left.\times [\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W', \mathbf{W}'_{\mathcal{N}_{K}})))-\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}}))]\right]
% \\  - \frac{1}{N^2}\sum_{i \in U}\left[\frac{I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})Y_{i}^{2^{obs}}(W,\mathbf{W}_{\mathcal{N}_{K}})}{2\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})} + \frac{I_{i}(W', \mathbf{W}'_{\mathcal{N}_{K}}))Y_{i}^{2^{obs}}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}{2\pi_{i}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}\right]
% \end{multline}



% \begin{multline}
% \widehat{\mathbf{Cov}}(\bar{Y}_{HT}^{obs}(W_{i},W_{\mathcal{N}_{ik}}),\bar{Y}_{HT}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}}))= 
% \\\frac{1}{N^2}\sum_{i \in U}\sum_{j \in U, j \ne i}\left[
% \frac{I_{i}(W_{i},W_{\mathcal{N}_{ik}})I_{j}(W'_{j},W'_{\mathcal{N}_{jk}})}{\pi_{ij}((W_{i},W_{\mathcal{N}_{ik}}),(W'_{j},W'_{\mathcal{N}_{jk}}))}
% \frac{Y_{i}(W_{i},W_{\mathcal{N}_{ik}})}{\pi_{i}(W_{i},W_{\mathcal{N}_{ik}})}\frac{Y_{j}(W'_{j},W'_{\mathcal{N}_{jk}})}{\pi_{j}(W'_{j},W'_{\mathcal{N}_{jk}})}\right.
% \\ \left.\times [\pi_{ij}((W_{i},W_{\mathcal{N}_{ik}}),(W'_{j},W'_{\mathcal{N}_{jk}}))-\pi_{i}(W_{i},W_{\mathcal{N}_{ik}})\pi_{j}(W'_{j},W'_{\mathcal{N}_{jk}})]\right]
% \\  - \frac{1}{N^2}\sum_{i \in U}\left[\frac{I_{i}(W_{i},W_{\mathcal{N}_{ik}})Y_{i}^{2^{obs}}(W_{i},W_{\mathcal{N}_{ik}})}{2\pi_{i}(W_{i},W_{\mathcal{N}_{ik}})} + \frac{I_{i}(W'_{i},W'_{\mathcal{N}_{ik}})Y_{i}^{2^{obs}}(W'_{i},W'_{\mathcal{N}_{ik}})}{2\pi_{i}(W'_{i},W'_{\mathcal{N}_{ik}})}\right],
% \end{multline}
Similar to~\eqref{eq:exvarhat} and~\eqref{eq:defavar}, we have:
\begin{align}
\label{eq:excovhat}
    &\mathbf{E}\left(\widehat{\mathbf{Cov}}_{HT}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}})\right) \nn \\=~ & \mathbf{Cov}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}})) - A_{Cov}
\end{align}
where 
\begin{equation}
\label{eq:defacov}
A_{Cov} = \sum_{i = 1}^N\sum_{\substack{j \in \{1,\ldots,N\}\\ \pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W', \mathbf{W}'_{\mathcal{N}_{K}})) = 0 }} y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}}).
\end{equation}

%MIKE: It feels like we could cut even more.
From repeated application of Lemma~\ref{lem:consest} in a way that mimics the argument in~\eqref{eq:consineq}, we can obtain conservative estimators for the covariance.
Specifically, define:
\begin{multline}\label{eq: COVHATA}
\widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}}))= \widehat{\mathbf{Cov}}_{HT}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}}))\\ - \frac{1}{N^2}\sum_{i = 1}^N\sum_{\substack{j \in \{1,\ldots,N\}\\ \pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W', \mathbf{W}'_{\mathcal{N}_{K}})) = 0 }}\left[\frac{I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})Y^2_{i}}{2\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})} + \frac{I_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}})Y^2_{j}}{2\pi_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}})}\right]
\end{multline}
and
\begin{multline}\label{eq: COVHATB}
\widehat{\mathbf{Cov}}_{B}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}}))= 
\widehat{\mathbf{Cov}}_{HT}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}})) \\  + \frac{1}{N^2}\sum_{i = 1}^N\sum_{\substack{j \in \{1,\ldots,N\}\\ \pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W', \mathbf{W}'_{\mathcal{N}_{K}})) = 0 }}\left[\frac{I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})Y^2_{i}}{2\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})} + \frac{I_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}})Y^2_{j}}{2\pi_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}})}\right].
\end{multline}
Note, the only difference in these two quantities is that the last term is subtracted in~\eqref{eq: COVHATA} and added in~\eqref{eq: COVHATB}.
The ensuing Lemma immediately follows.
%It follows immediately that
\begin{lemma}
\begin{multline}\label{EXPECTEDCOVHAT2}
  \mathbf{E}(\widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}}))) \leq \mathbf{Cov}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}})) \\ \leq \mathbf{E}(\widehat{\mathbf{Cov}}_{B}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}}))).
\end{multline}
\end{lemma}



% where the expected value of the estimated covariance is less than or equal to the true covariance \citep{aronow2013conservative, aronow2017estimating}.

% For the case where the joint probabilities $\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W', \mathbf{W}'_{\mathcal{N}_{K}})) = 0$  for some $i$ and $j$, the covariance in \ref{Cov1} can be refined as follows:

% \begin{multline}\label{Covarinaceoftwomeans2}
% \mathbf{Cov}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}})) =
% \\  \frac{1}{N^2}\sum_{i = 1}^N\sum_{\substack{j \ne i,\\ \pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W', \mathbf{W}'_{\mathcal{N}_{K}}))  > 0 }}\left[\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W', \mathbf{W}'_{\mathcal{N}_{K}})) - \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}})\right]
% \\ \times \frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}})} 
% \\ - \frac{1}{N^2}\sum_{i = 1}^N\sum_{\substack{j \ne i,\\ \pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W', \mathbf{W}'_{\mathcal{N}_{K}})) = 0 }} y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}}).
% \end{multline}



% For the case where the joint probabilities $\pi_{ij}((W_{i},W_{\mathcal{N}_{ik}}),(W'_{j},W'_{\mathcal{N}_{jk}})) = 0$  for some $i$ and $j$, the covaraince in \ref{COV} can be refined as follows:

% \begin{multline}\label{Covarinaceoftwomeans}
% \mathbf{Cov}(\bar{Y}_{HT}^{obs}(W_{i},W_{\mathcal{N}_{ik}}),\bar{Y}_{HT}^{obs}(W'_{i},W'_{\mathcal{N}_{ik}})) =
% \\  \frac{1}{N^2}\sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij}((W_{i},W_{\mathcal{N}_{ik}}),(W'_{j},W'_{\mathcal{N}_{jk}})) > 0 }\left[\pi_{ij}((W_{i},W_{\mathcal{N}_{ik}}),(W'_{j},W'_{\mathcal{N}_{jk}})) - \pi_{i}(W_{i},W_{\mathcal{N}_{ik}})\pi_{j}(W'_{j},W'_{\mathcal{N}_{jk}})\right]
% \\ \times \frac{y_{i}(W_{i},W_{\mathcal{N}_{ik}})}{\pi_{i}(W_{i},W_{\mathcal{N}_{ik}})}\frac{y_{j}(W'_{j},W'_{\mathcal{N}_{jk}})}{\pi_{j}(W'_{j},W'_{\mathcal{N}_{jk}})}
% \\ - \frac{1}{N^2}\sum_{i \in U}\sum_{j \in U:\pi_{ij}((W_{i},W_{\mathcal{N}_{ik}}),(W'_{j},W'_{\mathcal{N}_{jk}})) = 0 }  y_{i}(W_{i},W_{\mathcal{N}_{ik}})y_{j}(W'_{j},W'_{\mathcal{N}_{jk}}).
% \end{multline}

%Consequently, the more general covariance estimator is

% \begin{multline}
% \widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(W_{i},W_{\mathcal{N}_{ik}}),\bar{Y}_{HT}^{obs}(W'_{i},W'_{\mathcal{N}_{ik}})= 
% \\\frac{1}{N^2}\sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij}((W_{i},W_{\mathcal{N}_{ik}}),(W'_{j},W'_{\mathcal{N}_{jk}})) > 0 }\left[
% \frac{I_{i}(W_{i},W_{\mathcal{N}_{ik}})I_{j}(W'_{j},W'_{\mathcal{N}_{jk}})}{\pi_{ij}((W_{i},W_{\mathcal{N}_{ik}}),(W'_{j},W'_{\mathcal{N}_{jk}}))}
% \frac{Y_{i}^{obs}(W_{i},W_{\mathcal{N}_{ik}})}{\pi_{i}(W_{i},W_{\mathcal{N}_{ik}})}\frac{Y_{j}^{obs}(W'_{j},W'_{\mathcal{N}_{jk}})}{\pi_{j}(W'_{j},W'_{\mathcal{N}_{jk}})}\right.
% \\ \left.\times [\pi_{ij}((W_{i},W_{\mathcal{N}_{ik}}),(W'_{j},W'_{\mathcal{N}_{jk}}))-\pi_{i}(W_{i},W_{\mathcal{N}_{ik}})\pi_{j}(W'_{j},W'_{\mathcal{N}_{jk}})]\right]
% \\  - \frac{1}{N^2}\sum_{i \in U}\sum_{j \in U:\pi_{ij}((W_{i},W_{\mathcal{N}_{ik}}),(W'_{j},W'_{\mathcal{N}_{jk}})) = 0 }\left[\frac{I_{i}(W_{i},W_{\mathcal{N}_{ik}})Y^2_{i}}{2\pi_{i}(W_{i},W_{\mathcal{N}_{ik}})} + \frac{I_{j}(W'_{j},W'_{\mathcal{N}_{jk}})Y^2_{j}}{2\pi_{j}(W'_{j},W'_{\mathcal{N}_{jk}})}\right].
% \end{multline}

% \begin{multline}\label{COVHATA}
% \widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}}))= 
% \\\frac{1}{N^2}\sum_{i = 1}^N\sum_{\substack{j \ne i,\\ \pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W', \mathbf{W}'_{\mathcal{N}_{K}}))  > 0 }}\left[
% \frac{I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}})}{\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W', \mathbf{W}'_{\mathcal{N}_{K}}))}
% \frac{Y_{i}}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\frac{Y_{j}}{\pi_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}})}\right.
% \\ \left.\times [\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W', \mathbf{W}'_{\mathcal{N}_{K}}))-\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}})]\right]
% \\  - \frac{1}{N^2}\sum_{i = 1}^N\sum_{\substack{j \ne i,\\ \pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W', \mathbf{W}'_{\mathcal{N}_{K}})) = 0 }}\left[\frac{I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})Y^2_{i}}{2\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})} + \frac{I_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}})Y^2_{j}}{2\pi_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}})}\right]
% \end{multline}

% \begin{proposition}\label{propositionA1}
% \begin{equation}\label{EXPECTEDCOVHAT1CH3}
%   \mathbf{E}(\widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}}))) \le \mathbf{Cov}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}})).
% \end{equation}
% \end{proposition}

% \begin{proposition}
% \begin{equation}\label{EXPECTEDCOVHAT1CH3}
%   \mathbf{E}(\widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(W_{i},W_{\mathcal{N}_{ik}}),\bar{Y}_{HT}^{obs}(W'_{i},W'_{\mathcal{N}_{ik}})) \le \mathbf{Cov}(\bar{Y}_{HT}^{obs}(W_{i},W_{\mathcal{N}_{ik}}),\bar{Y}_{HT}^{obs}(W'_{i},W'_{\mathcal{N}_{ik}})).
% \end{equation}
% \end{proposition}


% Since $\widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(W_{i},W_{\mathcal{N}_{ik}})$ is a conservative variance estimator, the covariance estimator in \ref{COVHATA}, provides a conservative variance estimator of any estimator of the the form $\widehat{\delta} = X -Y$ such that $\mathbf{Var}(X-Y)$ = $\mathbf{Var}(X)$ + $\mathbf{Var}(Y)$ -2$\mathbf{Cov}(X,Y)$ which apply to all estimators under assumption 1.
% However, under assumption 2 and assumption 3, we have estimators of the form $\widehat{\delta} = (X - Y) + (W - Z)$ such that $\mathbf{Var}((X - Y) + (W - Z))$ = $\mathbf{Var}(X)$ + $\mathbf{Var}(Y)$ + $\mathbf{Var}(W)$ + $\mathbf{Var}(Z)$ -2$\mathbf{Cov}(X,Y)$ + 2$\mathbf{Cov}(X,W)$ -2$\mathbf{Cov}(X,Z)$ -2$\mathbf{Cov}(Y,W)$ +2$\mathbf{Cov}(Y,Z)$ -2$\mathbf{Cov}(W,Z)$. 
% To get conservative variance estimator of any estimator of the second form, $\widehat{\mathbf{Cov}}_{A}(X,Y)$ can be used to estimate covariance components that are subtracted from the variance components (i.e., the covariance between two averages with negative and positive coefficients) while for added covariance components (i.e., the covariance between two average both with positive coefficients), we need a covariance estimator that is guaranteed to have expectation greater than or equal to the true covariance.

% We provide the following covariance estimator for the second case,

% \begin{multline}\label{COVHATBCH3}
% \widehat{\mathbf{Cov}}_{B}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}}))= 
% \\\frac{1}{N^2}\sum_{i = 1}^N\sum_{\substack{j \ne i,\\ \pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W', \mathbf{W}'_{\mathcal{N}_{K}}))  > 0 }}\left[
% \frac{I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}})}{\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W', \mathbf{W}'_{\mathcal{N}_{K}}))}
% \frac{Y_{i}}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\frac{Y_{j}}{\pi_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}})}\right.
% \\ \left.\times [\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W', \mathbf{W}'_{\mathcal{N}_{K}}))-\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}})]\right]
% \\  + \frac{1}{N^2}\sum_{i = 1}^N\sum_{\substack{j \ne i,\\ \pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W', \mathbf{W}'_{\mathcal{N}_{K}})) = 0 }}\left[\frac{I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})Y^2_{i}}{2\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})} + \frac{I_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}})Y^2_{j}}{2\pi_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}})}\right]
% \end{multline}


% \begin{proposition}\label{EXPECTEDCOVHAT2CH3}
% \begin{align}\label{EXPECTEDCOVHAT2}
%   \mathbf{E}(\widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}}))) \geq \mathbf{Cov}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}}))).
% \end{align}
% \end{proposition}

% \begin{multline}\label{COVHATBCH3}
% \widehat{\mathbf{Cov}}_{B}(\bar{Y}_{HT}^{obs}(W_{i},W_{\mathcal{N}_{ik}}),\bar{Y}_{HT}^{obs}(W'_{i},W'_{\mathcal{N}_{ik}})= 
% \\\frac{1}{N^2}\sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij}((W_{i},W_{\mathcal{N}_{ik}}),(W'_{j},W'_{\mathcal{N}_{jk}})) > 0 }\left[
% \frac{I_{i}(W_{i},W_{\mathcal{N}_{ik}})I_{j}(W'_{j},W'_{\mathcal{N}_{jk}})}{\pi_{ij}((W_{i},W_{\mathcal{N}_{ik}}),(W'_{j},W'_{\mathcal{N}_{jk}}))}
% \frac{Y_{i}^{obs}(W_{i},W_{\mathcal{N}_{ik}})}{\pi_{i}(W_{i},W_{\mathcal{N}_{ik}})}\frac{Y_{j}^{obs}(W'_{j},W'_{\mathcal{N}_{jk}})}{\pi_{j}(W'_{j},W'_{\mathcal{N}_{jk}})}\right.
% \\ \left.\times [\pi_{ij}((W_{i},W_{\mathcal{N}_{ik}}),(W'_{j},W'_{\mathcal{N}_{jk}}))-\pi_{i}(W_{i},W_{\mathcal{N}_{ik}})\pi_{j}(W'_{j},W'_{\mathcal{N}_{jk}})]\right]
% \\  + \frac{1}{N^2}\sum_{i \in U}\sum_{j \in U:\pi_{ij}((W_{i},W_{\mathcal{N}_{ik}}),(W'_{j},W'_{\mathcal{N}_{jk}})) = 0 }\left[\frac{I_{i}(W_{i},W_{\mathcal{N}_{ik}})Y^2_{i}}{2\pi_{i}(W_{i},W_{\mathcal{N}_{ik}})} + \frac{I_{j}(W'_{j},W'_{\mathcal{N}_{jk}})Y^2_{j}}{2\pi_{j}(W'_{j},W'_{\mathcal{N}_{jk}})}\right].
% \end{multline}


% \begin{proposition}
% \begin{equation}\label{EXPECTEDCOVHAT2CH3}
%   \mathbf{E}(\widehat{\mathbf{Cov}}_{B}(\bar{Y}_{HT}^{obs}(W_{i},W_{\mathcal{N}_{ik}}),\bar{Y}_{HT}^{obs}(W'_{i},W'_{\mathcal{N}_{ik}})) \geq \mathbf{Cov}(\bar{Y}_{HT}^{obs}(W_{i},W_{\mathcal{N}_{ik}}),\bar{Y}_{HT}^{obs}(W'_{i},W'_{\mathcal{N}_{ik}})).
% \end{equation}
% \end{proposition}

% The proof follows by Young’s inequality in equations \ref{Young1} and \ref{Young2}.
%Samirah edited this June 14, 2023:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Thereby, the conservative variance estimators for the estimators in the previous section under Assumption \ref{Assumption1} for any two exposures $(W,\mathbf{W}_{\mathcal{N}_{K}})$, $(W',\mathbf{W}'_{\mathcal{N}_{K}})$ 
and under Assumption \ref{Assumption2} for any four treatment exposures $(W, \mathbf W_{\mathcal{N}_{K}}), (W', \mathbf W'_{\mathcal{N}_{K}}), (W^{*}, \mathbf W^{*}_{\mathcal{N}_{K}}), (W^{*\prime}, \mathbf W^{*\prime}_{\mathcal{N}_{K}})$ respectively can be derived as follows,
\begin{multline}
\label{VARDIFEstimate}
\widehat{\mathbf{Var}}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}) - \bar{Y}_{HT}^{obs}(W',\mathbf{W'}_{\mathcal{N}_{K}}))=\\
\widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})) + \widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}})) 
-2\widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}})),
\end{multline}

\begin{align}
 \label{varianceass2directCH3Estimate}
% \mathbf{E}(\widehat{\delta^*}) & = 1/2\left(\bar y(W,\mathbf{W}_{\mathcal{N}_{K}}) -   \bar y(W',\mathbf{W'}_{\mathcal{N}_{K}})\right)+ 1/2\left(\bar y(W^{*}, \mathbf W^{*}_{\mathcal{N}_{K}}) -   \bar y(W^{*\prime}, \mathbf W^{*\prime}_{\mathcal{N}_{K}})\right), \\  
&\widehat{\mathbf{Var}} \left[1/2\left(\bar{Y}_{HT}^{obs}(W, \mathbf W_{\mathcal{N}_{K}}) - \bar{Y}_{HT}^{obs}(W', \mathbf W'_{\mathcal{N}_{K}})\right) + 1/2\left(\bar{Y}_{HT}^{obs}(W^{*}, \mathbf W^{*}_{\mathcal{N}_{K}}) - \bar{Y}_{HT}^{obs}(W^{*\prime}, \mathbf W^{*\prime}_{\mathcal{N}_{K}})\right)\right] \nn \\  
 = & 1/4\widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(W, \mathbf W_{\mathcal{N}_{K}})) + 1/4\widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(W', \mathbf W'_{\mathcal{N}_{K}})) \nn \\  
 & +  1/4 \widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(W^{*}, \mathbf W^{*}_{\mathcal{N}_{K}})) + 1/4\widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(W^{*'}, \mathbf W^{*'}_{\mathcal{N}_{K}})) \nn \\  
& -  1/2 \widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(W, \mathbf W_{\mathcal{N}_{K}}), \bar{Y}_{HT}^{obs}(W', \mathbf W'_{\mathcal{N}_{K}})) 
 + 1/2 \widehat{\mathbf{Cov}}_{B}(\bar{Y}_{HT}^{obs}(W, \mathbf W_{\mathcal{N}_{K}}), \bar{Y}_{HT}^{obs}(W^{*}, \mathbf W^{*}_{\mathcal{N}_{K}})) \nn \\  
 & -  1/2 \widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(W, \mathbf W_{\mathcal{N}_{K}}), \bar{Y}_{HT}^{obs}(W^{*'}, \mathbf W^{*'}_{\mathcal{N}_{K}})) 
 - 1/2 \widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(W', \mathbf W'_{\mathcal{N}_{K}}), \bar{Y}_{HT}^{obs}(W^{*}, \mathbf W^{*}_{\mathcal{N}_{K}}))  \nn \\  
 & +  1/2 \widehat{\mathbf{Cov}}_{B}(\bar{Y}_{HT}^{obs}(W', \mathbf W'_{\mathcal{N}_{K}}), \bar{Y}_{HT}^{obs}(W^{*'}, \mathbf W^{*'}_{\mathcal{N}_{K}})) 
 - 1/2 \widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(W^{*}, \mathbf W^{*}_{\mathcal{N}_{K}}), \bar{Y}_{HT}^{obs}(W^{*'}, \mathbf W^{*'}_{\mathcal{N}_{K}})).
\end{align}

This general form of the variance estimators can be applied for all estimators considered under Assumptions \ref{Assumption1} and \ref{Assumption2}. For example, the conservative variance estimators for the A$\ell$NNIE under Assumptions \ref{Assumption1} and \ref{Assumption2} respectively are as follows,

\begin{multline}
\widehat{\mathbf{Var}}(\widehat{\delta}_{{HT,\ell^{th}}}) = 
\widehat{\mathbf{Var}}(\bar{Y}_{HT}^{obs}(0,\mathbf W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,\mathbf W^{*}_{\ell-1})) = \widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(0, \mathbf W^{*}_{\ell})) + \widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(0, \mathbf W^{*}_{\ell-1})) 
\\ -2\widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(0, \mathbf W^{*}_{\ell}),\bar{Y}_{HT}^{obs}(0, \mathbf W^{*}_{\ell-1})).
\end{multline}

%MIKE: CHANGE THE 2(1/4) to 1/2
%MATCH STYLE ABOVE
\begin{multline} 
\label{varhatassumption2}
\widehat{\mathbf{Var}}(\widehat{\delta^*}_{{HT,\ell^{th}}}) = 
\widehat{\mathbf{Var}}(1/2\left(\bar{Y}_{HT}^{obs}(1,\mathbf W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,\mathbf W^{*}_{\ell-1})\right) +1/2\left(\bar{Y}_{HT}^{obs}(0,\mathbf W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,\mathbf W^{*}_{\ell-1})\right)) \\ 
= \frac{1}{4}\widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(1,\mathbf W^{*}_{\ell})) 
+ \frac{1}{4}\widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(1, \mathbf W^{*}_{\ell-1})) 
\\ + \frac{1}{4}\widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(0, \mathbf W^{*}_{\ell})) + \frac{1}{4}\widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(0, \mathbf W^{*}_{\ell-1}))
\\ -\frac{1}{2} \widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(1, \mathbf W^{*}_{\ell}), \bar{Y}_{HT}^{obs}(1, \mathbf W^{*}_{\ell-1})) 
+\frac{1}{2} \widehat{\mathbf{Cov}}_{B}(\bar{Y}_{HT}^{obs}(1, \mathbf W^{*}_{\ell}), \bar{Y}_{HT}^{obs}(0, \mathbf W^{*}_{\ell})) 
\\ -\frac{1}{2} \widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(1, \mathbf W^{*}_{\ell}), \bar{Y}_{HT}^{obs}(0, \mathbf W^{*}_{\ell-1})) 
-\frac{1}{2}\widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(1, \mathbf W^{*}_{\ell-1}), \bar{Y}_{HT}^{obs}(0, \mathbf W^{*}_{\ell})) 
\\ +\frac{1}{2}\widehat{\mathbf{Cov}}_{B}(\bar{Y}_{HT}^{obs}(1, \mathbf W^{*}_{\ell-1}), \bar{Y}_{HT}^{obs}(0, \mathbf W^{*}_{\ell-1})) 
-\frac{1}{2}\widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(0, \mathbf W^{*}_{\ell}), \bar{Y}_{HT}^{obs}(0, \mathbf W^{*}_{\ell-1})).
\end{multline}







% up to here

% Thereby, the conservative variance estimators of all estimators in the previous section under Assumptions \ref{Assumption1} and \ref{Assumption2} respectively are as follows,

% \begin{multline}
% \widehat{\mathbf{Var}}(\widehat{\delta}_{{HT,Tot}}) = \widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + \widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% \\ -2\widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0})).
% \end{multline}


% \begin{multline}
% \widehat{\mathbf{Var}}(\widehat{\delta}_{{HT,direct}}) = \widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + \widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% \\ -2\widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{1})).
% \end{multline}


% \begin{multline}
% \widehat{\mathbf{Var}}(\widehat{\delta}_{{HT,indirect}}) = \widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) + \widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% \\ -2\widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0})).
% \end{multline}











% \begin{multline}
% \widehat{\mathbf{Var}}(\widehat{\delta}_{{HT,\ell^{th}}}) = \widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell})) + \widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})) 
% \\ -2\widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}),\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1}).
% \end{multline}




% \begin{equation}
%     \widehat{\mathbf{Var}}(\widehat{\delta^*}_{{HT,Total}}) = \widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + \widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% \\ -2\widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0})).
% \end{equation}







% \begin{multline} 
% \widehat{\mathbf{Var}}(\widehat{\delta^*}_{HT,direct}) = \frac{1}{4}\widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + \frac{1}{4}\widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% \\ + \frac{1}{4}\widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(1,\mathbf{0})) + \frac{1}{4}\widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% \\ - 2(\frac{1}{4}) \widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% + 2(\frac{1}{4}) \widehat{\mathbf{Cov}}_{B}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(1,\mathbf{0})) 
% \\ -2(\frac{1}{4}) \widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% -2(\frac{1}{4}) \widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}), \bar{Y}_{HT}^{obs}(1,\mathbf{0})) 
% \\ + 2(\frac{1}{4}) \widehat{\mathbf{Cov}}_{B}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% -2(\frac{1}{4}) \widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})).
% \end{multline}





% \begin{multline} 
% \widehat{\mathbf{Var}}(\widehat{\delta^*}_{HT,indirect}) = \frac{1}{4}\widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + \frac{1}{4}\widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(1,\mathbf{0})) 
% \\ + \frac{1}{4}\widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) + \frac{1}{4}\widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% \\ - 2(\frac{1}{4}) \widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(1,\mathbf{0})) 
% + 2(\frac{1}{4}) \widehat{\mathbf{Cov}}_{B}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% \\ -2(\frac{1}{4}) \widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% -2(\frac{1}{4}) \widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}), \bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% \\ + 2(\frac{1}{4}) \widehat{\mathbf{Cov}}_{B}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% -2(\frac{1}{4}) \widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})).
% \end{multline}



% \begin{multline} 
% \label{varhatassumption2}
% \widehat{\mathbf{Var}}(\widehat{\delta^*}_{{HT,\ell^{th}}}) = \frac{1}{4}\widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})) + \frac{1}{4}\widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})) 
% \\ + \frac{1}{4}\widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell})) + \frac{1}{4}\widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1}))
% \\ -2(\frac{1}{4}) \widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}), \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})) 
% +2(\frac{1}{4}) \widehat{\mathbf{Cov}}_{B}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}), \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell})) 
% \\ -2(\frac{1}{4}) \widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}), \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})) 
% -2(\frac{1}{4})\widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1}), \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell})) 
% \\ +2(\frac{1}{4})\widehat{\mathbf{Cov}}_{B}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1}), \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})) 
% -2(\frac{1}{4})\widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}), \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})).
% \end{multline}


% By linearity of expectation and by equations \ref{VARHATA}, \ref{EXPECTEDCOVHAT2} and \ref{EXPECTEDCOVHAT3} in %Appendix \ref{AppendixA}
% the supplementary material, the covariance of the estimators provided here are conservative. 







%\subsection{Model-Based Tests of K-Nearest Neighbor Interference}
%\label{makereference3}




%MIKE: Some worry they will require us to consider a simulation where
%Assumption 2 is violated.

\section{Simulation}
\label{makereference3.8}

\begin{table}[htbp]

  \centering
  \begin{tabular}[c]{r|ccccccccc}
 \hline
        Model & 1 & 2 & 3 &4 & 5 &  6 &  7 &  8 & 9 \\
\hline
        $\delta_{1}$  & 0 & 0 & 0 & 2 & 2 & 2 & 3 & 3 & 3 \\
        $\delta_{2}$  & 0 & 0 & 0 & 1 & 1 & 1 & 2 & 2 & 2  \\
        $\delta_{3}$  & 0 & 0 & 0 & 0.5 & 0.5 & 0.5 & 1 & 1 & 1  \\
        $\delta_{t}$  & 0 & 1 & 4 & 0 & 1 & 4 & 0 & 1 & 4  \\
\hline
\end{tabular}
  \caption{Interference models.}%
  \label{table3.1}
\end{table}



%MIKE: Tables need edits here.  
%MIKE: Adjust notation.
%MIKE: Shorten column names.
%MIKE: You might consider: Remove Emp. SD.  Remove delta*TOT  
%MIKE: Maybe Add CRD and Bernoulli in Same table.  
%MIKE: Shrink slightly (use \small)?


\begin{table}[t!]
\small
  \caption{Estimates under completely randomized design and Bernoulli randomization model 1.}%
  \label{table3.2}
  \centering
  \begin{tabular}[c]{rc|ccc|ccc}\\
  %\hline
  &&\multicolumn{3}{|c}{Comp. Rand.} & \multicolumn{3}{|c}{Bern. Rand.} \\ \hline
        Estimator & Effects & Emp. EV,  & Emp. Var, & Var Est. & Emp. EV,  & Emp. Var, & Var Est.\\
\hline
       $ \widehat{\delta}_{{HT,tot}}$ & 0 & 0.0779  & 1.1668 &   1.1759 & 0.0059 & 1.2480 & 1.2037 \\
       $\widehat{\delta}_{{HT,dir}}$ & 0 & 0.0337  & 0.6078 &  0.6683  & -0.0150 & 0.6850  & 0.7016 \\
       $ \widehat{\delta^*}_{{HT,dir}}$ & 0 & 0.0303  & 0.2894 &  0.5036 & 0.0103 & 0.3116 & 0.5153 \\
       $\widehat{\delta}_{{HT,ind}}$ & 0 & 0.0442  & 0.9404 &  0.9393 & 0.0210 & 0.8232 & 0.9258 \\
       $ \widehat{\delta^*}_{{HT,ind}}$ & 0 & 0.0476  & 0.5615  & 0.6619 & -0.0044 & 0.5622 & 0.6696\\
       $\widehat{\delta}_{{HT,1^{st}}}$  & 0 & 0.0346  & 0.6273 &  0.6821 & 0.0502 & 0.6502 & 0.6804\\
       $\widehat{\delta^*}_{{HT,1^{st}}}$ & 0 & 0.0191 & 0.2647 &  0.3950 & 0.0173 & 0.2460 & 0.3922\\
        $\widehat{\delta}_{{HT,2^{nd}}}$  & 0 & -0.0111  & 0.4642 &  0.6023 & -0.0181 & 0.4430 & 0.5985 \\
       $\widehat{\delta^*}_{{HT,2^{nd}}}$ & 0 & -0.0021 & 0.2694 &  0.4625 & -0.0085 & 0.2702 & 0.4599\\
        $\widehat{\delta}_{{HT,3^{rd}}}$ & 0 & 0.0207  & 0.4541 &  0.6076 & -0.0110 & 0.4489 & 0.6048 \\
       $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 0 & 0.0306 &   0.3014 &  0.4514 & -0.0132 & 0.3134 & 0.4629 \\
\hline
\end{tabular}
\end{table}





\begin{table}[b!]
\small
  \caption{Estimates under completely randomized design and Bernoulli randomization model 5.}%
  \label{table3.6}
  \centering
  \begin{tabular}[c]{rc|ccc|ccc}\\
  %\hline
  &&\multicolumn{3}{|c}{Comp. Rand.} & \multicolumn{3}{|c}{Bern. Rand.} \\ \hline
        Estimator & Effects & Emp. EV,  & Emp. Var, & Var Est. & Emp. EV,  & Emp. Var, & Var Est.\\
\hline
       $ \widehat{\delta}_{{HT,tot}}$   & 4.5 & 4.5453  & 3.3664 &  3.9196 & 4.6263 & 4.7542 & 5.3978 \\
       $\widehat{\delta}_{{HT,dir}}$  & 1 & 0.9804  & 3.9013 &  4.1313 & 1.1013  & 4.3757 & 4.6502 \\
       $ \widehat{\delta^*}_{{HT,dir}}$ & 1 & 1.0070  & 1.1503 &  1.9781 & 1.0725 & 1.1720 & 2.0765\\
       $\widehat{\delta}_{{HT,ind}}$ & 3.5 & 3.5649  & 1.5750 &  2.3108 & 3.5249  & 1.8276 & 2.5207 \\
       $ \widehat{\delta^*}_{{HT,ind}}$ & 3.5 & 3.5383  & 1.1485 &   1.9574 & 3.5537 & 2.0192 & 2.7832 \\
       $\widehat{\delta}_{{HT,1^{st}}}$  & 2 & 2.0414  & 0.8733 &  1.1102 & 2.0532 & 0.9882 & 1.1739\\
       $\widehat{\delta^*}_{{HT,1^{st}}}$ & 2 & 2.0245  & 0.4873 & 0.9667 & 2.0176 & 0.4648 & 0.9666 \\
        $\widehat{\delta}_{{HT,2^{nd}}}$  & 1 & 1.0003  & 1.4039 &  1.9204 & 0.9878 & 1.4253 & 2.0036 \\
       $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 1 & 0.9930  & 0.9747 &  1.8982 & 0.9582 & 1.1082 & 2.0400 \\
        $\widehat{\delta}_{{HT,3^{rd}}}$ & 0.5 & 0.5230  & 1.8611 &  2.5928 & 0.4838 & 2.2543 & 2.8237 \\
       $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 0.5 & 0.5207 & 1.4523 & 2.3927 & 0.5778 & 2.0343 & 2.7262\\
\hline
\end{tabular}
\end{table}





\begin{table}[tbp]
\small
  \caption{Estimates under completely randomized design and Bernoulli randomization model 9.}%
  \label{table3.10}
   \centering
  \begin{tabular}[c]{rc|ccc|ccc}\\
  %\hline
  &&\multicolumn{3}{|c}{Comp. Rand.} & \multicolumn{3}{|c}{Bern. Rand.} \\ \hline
        Estimator & Effects & Emp. EV,  & Emp. Var, & Var Est. & Emp. EV,  & Emp. Var, & Var Est.\\
\hline
       $ \widehat{\delta}_{{HT,tot}}$   & 10 & 10.0055  & 11.54004 &  14.4106 & 10.2734 & 18.2343 & 21.4468\\
       $\widehat{\delta}_{{HT,dir}}$  & 4 &3.9257  & 14.7035 &  15.0482 & 4.2456 & 17.5899 & 18.4553\\
       $ \widehat{\delta^*}_{{HT,dir}}$ & 4 & 3.9896  & 4.0963 &  7.2216 & 4.1566 & 4.2346 & 7.7429 \\
       $\widehat{\delta}_{{HT,ind}}$ & 6 & 6.0797  & 2.7909 &  4.8503 & 6.0277 & 3.5973 & 5.4461 \\
       $ \widehat{\delta^*}_{{HT,ind}}$ & 6 & 6.0158  & 3.3206 &  6.0066 & 6.1168 & 7.4354 & 9.8815\\
       $\widehat{\delta}_{{HT,1^{st}}}$  & 3 &3.0449  & 1.1582 &  1.6049 & 3.0547 & 1.3709 & 1.7503 \\
       $\widehat{\delta^*}_{{HT,1^{st}}}$ & 3 & 3.0232  & 1.4675 &  3.0152 & 3.0100 & 1.3786 & 3.0134 \\
        $\widehat{\delta}_{{HT,2^{nd}}}$  & 2 &2.0092  & 2.8500 &  3.9616 & 1.9923 & 2.9090 & 4.1353\\
       $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 2 & 1.9838  & 3.2241 &  6.4517 & 1.9150 & 3.8171 & 6.9785 \\
        $\widehat{\delta}_{{HT,3^{rd}}}$ & 1 &1.0256  & 4.5078 &  6.1172 & 0.9806 & 5.3764 & 6.7556\\
       $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 1 & 1.0086  & 5.3193 &  8.5285 & 1.1917 & 7.4434 & 9.8241\\ 
\hline
\end{tabular}
\end{table}


In this section, we assess the performance of our proposed estimators through a simulation study.
In our simulations, we consider a range of indirect effects from no indirect effects to moderate indirect effects and direct effects.
These simulations are designed to verify unbiasedness of our proposed estimators and to assess the tightness of the conservative variance bounds.
Additionally, these simulations will satisfy the assumption of no weak interaction between direct and indirect effects, allowing us to evaluate the gain in precision for estimators exploiting this assumption.
%We consider three different scenarios of the indirect effects where the indirect effects in each scenario represent the degree of interference; the first three models consider no interference, the next three models consider weak interference, and the last three models consider moderate interference.
%starting from no interference in the first three models, followed by weak interference in the second three models scenario and moderate interference in the last three models scenario.
%For each of these scenarios, we also vary the strength of the direct effect of treatment, considering models with no direct effect, a weak direct effect, and a strong direct effect.
%the direct effect takes also three different scenarios such that $\delta_{i}$ takes the values in (0,1,4).

In our simulations, we generate responses under the following KNNIM model with $K = 3$ nearest neighbors:
 \begin{equation} 
Y_{i} = X_{i1} + X_{i2} + X_{i3} + \delta_{1}W_{i1} + \delta_{2}W_{i2} +\delta_{3}W_{i3} + \delta_{t}W_{i} 
\end{equation} 
where $\delta_\ell$ denotes the A$\ell$NNIE, $\ell = 1, 2, 3$, and $\delta_t$ denotes the ADE.  
The $X_{ip}$ are covariates, $X_{ip}\sim N(0,1)$, $p=1,2,3$.  
We use the squared Euclidean distance of the covariates $d(i,j) = \sum_{p=1}^3 (X_{ip} - X_{jp})^2$ as our measure of interaction.
%, and we consider the case where we only have $K = 3$ nearest neighbors. 
In all models considered, closer neighbors provide stronger indirect effects: $|\delta_{1}|\geq  |\delta_{2} \geq |\delta_{3}|$. 
We assess our estimators under two randomization designs: completely randomized designs (CRD) where half of the $N$ units are assigned to treatment and Bernoulli randomized designs (BRD) with probability $p = 0.5$.

We consider 9 models of interference (see Table~\ref{table3.1}).  The first three models consider no interference, the next three models consider weak interference, and the last three models consider moderate interference.  
Within these categories of indirect effects, we also vary the strength of the direct effect, from no direct effect to a strong direct effect.
% given in table 
% The Nine interference models are shown in the second column of Table~\ref{table3.1}.
% The first three elements of the vector ($\delta_{1}, \delta_{2}, \delta_{3}, \delta_{t})$  in Table~\ref{table3.1} represent the first, second, and third nearest neighbor's indirect effect where the last element is the unit's direct effect.

% In all models considered, the closer the distance to unit $i$, the greater the indirect effect: $|\delta_{1}|\geq  |\delta_{2} \geq |\delta_{3}|$. 

%MIKE: For space, we may want to avoid including the EMP SD and use EMP EV for the empirical expected value
For each model, we evaluate the performance of the total, direct, indirect and $\ell_{th}$ nearest neighbor estimators 
%with the estimated variance 
under Assumption \ref{Assumption1} and Assumption \ref{Assumption2}. 
The experiment is replicated 1000 times with sample size N = 256.
The marginal and joint probabilities are computed as in Section~\ref{probcompute}.
For each run of 1000 replications, for each estimator, we compute the empirical expected value of the estimates (Emp. EV, the average value of the estimators), empirical variance (Emp. Var, the variance of the estimators),
%and standard deviation (Emp.S.D.), 
and the average of the estimated variances (Var Est.). 
For brevity, we only include results for the CRD and BRD designs for Model 1 (Table~\ref{table3.2}), Model 5 (Table~\ref{table3.6}) and Model 9 (Table~\ref{table3.10}).
%for model 1 to table  
%MIKE: Is there an appendix?
A complete analysis is included in the supplementary material.
%\ref{table3.10} for model 9 and for BRD design in table \ref{table1Ber} for model 1 to table \ref{table9Ber} for model 9.

%MIKE: May want to see how this table looks if you had it as a 4 row 9 column matrix.  Would take up less space.
% \begin{table}[tbp]
%   \caption{Interference models.}%
%   \label{table3.1}
%   \centering
%   \begin{tabular}[c]{lc}\\
%  %\hline
  
%         Models & ($\delta_{1}$, $\delta_{2$, $\delta_{3$, $\delta_{t}$) \\
% \hline
%         Model 1 & (0,0,0,0)\\
%         Model 2 & (0,0,0,1) \\
%         Model 3 & (0,0,0,4) \\
%         Model 4 & (2,1,0.5,0) \\
%         Model 5 & (2,1,0.5,1) \\
%         Model 6 & (2,1,0.5,4) \\
%         Model 7 & (3,2,1,0) \\
%         Model 8 & (3,2,1,1) \\
%         Model 9 & (3,2,1,4) \\
      
% \hline
% \end{tabular}
% \end{table}
% %%%%%%%%%%%%%%%%%%%%%%%%%

 
% \begin{table}[t!]
%   \caption{Estimates under completely randomized design model 1.}%
%   \label{table3.2}
%   \centering
%   \begin{tabular}[c]{lccccc}\\
%   %\hline
%         Estimator & Effects & Emp.Estimates  & Emp.Var & Emp.S.D. & Var Estimate \\
% \hline
%        $ \widehat{\delta}_{{HT,Total}}$ & 0 & 0.0779  & 1.1668 & 1.0802 &  1.1759 \\
%        $ \widehat{\delta^*}_{{HT,Total}}$ & 0 & 0.0779  & 1.1668 & 1.0802 &  1.1759 \\
%        $\widehat{\delta}_{{HT,direct}}$ & 0 & 0.0337  & 0.6078 & 0.7796 & 0.6683  \\
%        $ \widehat{\delta^*}_{{HT,direct}}$ & 0 & 0.0303  & 0.2894 & 0.5380 & 0.5036\\
%        $\widehat{\delta}_{{HT,indirect}}$ & 0 & 0.0442  & 0.9404 & 0.9697 & 0.9393 \\
%        $ \widehat{\delta^*}_{{HT,indirect}}$ & 0 & 0.0476  & 0.5615 & 0.7493 & 0.6619 \\
%        $\widehat{\delta}_{{HT,1^{st}}}$  & 0 & 0.0346  & 0.6273 & 0.7920 & 0.6821 \\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 0 & 0.0191 & 0.2647 & 0.5145 & 0.3950 \\
%         $\widehat{\delta}_{{HT,2^{nd}}}$  & 0 & -0.0111  & 0.4642 & 0.6813 & 0.6023 \\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$ & 0 & -0.0021 & 0.2694 & 0.5190 & 0.4625 \\
%         $\widehat{\delta}_{{HT,3^{rd}}}$ & 0 & 0.0207  & 0.4541 & 0.6738 & 0.6076 \\
%        $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 0 & 0.0306 &   0.3014 & 0.5490 & 0.4514 \\
% \hline
% \end{tabular}
% \end{table}






% \begin{table}[b!]
%   \caption{Estimates under completely randomized design model 2.}%
%   \label{table3.3}
%   \centering
%   \begin{tabular}[c]{lccccc}\\
%   %\hline
%         Estimator & Effects & Emp.Estimates & Emp.Var & Emp.S.D. & Var Estimate \\
% \hline
%        $ \widehat{\delta}_{{HT,Total}}$ & 1 & 1.0707  & 1.306 & 1.1430 & 1.3314 \\
%        $ \widehat{\delta^*}_{{HT,Total}}$ & 1 & 1.0707  & 1.306 & 1.1430 & 1.3314 \\
%        $\widehat{\delta}_{{HT,direct}}$ & 1 & 1.0265  & 0.7273 & 0.8528 & 0.8050 \\
%        $ \widehat{\delta^*}_{{HT,direct}}$ & 1 & 1.0300  & 0.3377 & 0.5812 &  0.6214\\
%        $\widehat{\delta}_{{HT,indirect}}$ & 0 & 0.0442  & 0.9404 & 0.9697 & 0.9393\\
%        $ \widehat{\delta^*}_{{HT,indirect}}$ & 0 & 0.0406  &  0.6081 & 0.7798 & 0.7381\\
%        $\widehat{\delta}_{{HT,1^{st}}}$  & 0 & 0.0346  & 0.6273 & 0.7920 & 0.6821 \\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 0 & 0.01758  & 0.3009 & 0.5486 & 0.4749  \\
%         $\widehat{\delta}_{{HT,2^{nd}}}$ & 0 & -0.0111   & 0.4642 & 0.6813 & 0.6023 \\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 0 & -0.0052  & 0.3328 & 0.5769 & 0.5601 \\
%         $\widehat{\delta}_{{HT,3^{rd}}}$ & 0 & 0.0207  & 0.4541 & 0.6738 & 0.6076 \\
%        $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 0 & 0.0282 & 0.3525 & 0.5937 & 0.5359\\
% \hline
% \end{tabular}
% \end{table}


% \begin{table}[tbp]
%   \caption{Estimates under completely randomized design model 3.}%
%   \label{table3.4}
%   \centering
%   \begin{tabular}[c]{lccccc}\\
%  % \hline
%         Estimator & Effects & Emp.Estimates & Emp.Var & Emp.S.D. & Var Estimate \\
% \hline
%        $ \widehat{\delta}_{{HT,Total}}$   & 4 &  4.0489  & 2.9225 & 1.7095 & 3.3552\\
%        $ \widehat{\delta^*}_{{HT,Total}}$  & 4 &  4.0489 & 2.9225 & 1.7095 & 3.3552\\
%        $\widehat{\delta}_{{HT,direct}}$  & 4 & 4.0047  & 2.2827 & 1.5108 & 2.5258  \\
%        $ \widehat{\delta^*}_{{HT,direct}}$   & 4 & 4.0292  & 0.9267 & 0.9626 & 2.1155 \\
%        $\widehat{\delta}_{{HT,indirect}}$ & 4 & 0.0442  &  0.9404 & 0.9697 & 0.9393 \\
%        $ \widehat{\delta^*}_{{HT,indirect}}$   & 0 & 0.01978  & 1.1883 & 1.0901 & 1.7146 \\
%        $\widehat{\delta}_{{HT,1^{st}}}$  & 0 &  0.0346  & 0.6273 & 0.7920 & 0.6821 \\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 0 & 0.01286  &  0.8246 &  0.9081 & 1.4569 \\
%         $\widehat{\delta}_{{HT,2^{nd}}}$ & 0 & -0.0111   & 0.4642 & 0.6813 & 0.6023 \\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 0 & -0.01445  & 1.0133 & 1.0066 & 1.7389 \\
%         $\widehat{\delta}_{{HT,3^{rd}}}$ & 0 & 0.0207  & 0.4541 & 0.6738 & 0.6076 \\
%        $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 0 & 0.0213  & 1.0711 & 1.0349 & 1.6299 \\
% \hline
% \end{tabular}
% \end{table}



% \begin{table}[tbp]
%   \caption{Estimates under completely randomized design model 4.}%
%   \label{table3.5}
%   \centering
%   \begin{tabular}[c]{lccccc}\\
%   %\hline
%         Estimator & Effects & Emp.Estimates & Emp.Var & Emp.S.D. & Var Estimate \\
% \hline
%        $ \widehat{\delta}_{{HT,Total}}$ & 3.5 & 3.5526  & 2.5285 & 1.5901 &  2.8557\\
%        $ \widehat{\delta^*}_{{HT,Total}}$ & 3.5 & 3.5526  & 2.5285 & 1.5901 &  2.8557\\
%        $\widehat{\delta}_{{HT,direct}}$ & 0 & -0.0123  & 2.9637 & 1.7215 & 3.2323\\
%        $ \widehat{\delta^*}_{{HT,direct}}$ & 0 & 0.0073  & 0.8913 & 0.9441 & 1.5485\\
%        $\widehat{\delta}_{{HT,indirect}}$ & 3.5 & 3.5649  & 1.5750 & 1.2550 & 2.3108 \\
%        $ \widehat{\delta^*}_{{HT,indirect}}$  & 3.5 & 3.5453  & 0.9523 & 0.9758 & 1.6828 \\
%        $\widehat{\delta}_{{HT,1^{st}}}$  & 2 & 2.0414  & 0.8733 & 0.9345 & 1.1102 \\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 2 & 2.0261  & 0.3901 &  0.6245 & 0.7582\\
%         $\widehat{\delta}_{{HT,2^{nd}}}$  & 1 & 1.0003  & 1.4039 & 1.1848 & 1.9204\\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 1 & 0.9961  & 0.7423 & 0.8615 & 1.4553 \\
%         $\widehat{\delta}_{{HT,3^{rd}}}$ & 0.5 & 0.5230  & 1.8611 & 1.3642 &  2.5928 \\
%        $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 0.5 & 0.5230 & 1.1157 & 1.0563 & 1.8726\\
% \hline
% \end{tabular}
% \end{table}





% \begin{table}[tbp]
%   \caption{Estimates under completely randomized design model 5.}%
%   \label{table3.6}
%   \centering
%   \begin{tabular}[c]{lccccc}\\
%   %\hline
%         Estimator & Effects & Emp.Estimates  & Emp.Var & Emp.S.D. & Var Estimate \\
% \hline
%        $ \widehat{\delta}_{{HT,Total}}$   & 4.5 & 4.5453  & 3.3664 & 1.8347 & 3.9196\\
%        $ \widehat{\delta^*}_{{HT,Total}}$  & 4.5 & 4.5453  & 3.3664 & 1.8347 & 3.9196\\
%        $\widehat{\delta}_{{HT,direct}}$  & 1 & 0.9804  & 3.9013 & 1.9751 & 4.1313\\
%        $ \widehat{\delta^*}_{{HT,direct}}$ & 1 & 1.0070  & 1.1503 & 1.0725 & 1.9781\\
%        $\widehat{\delta}_{{HT,indirect}}$ & 3.5 & 3.5649  & 1.5750 & 1.2550 & 2.3108\\
%        $ \widehat{\delta^*}_{{HT,indirect}}$ & 3.5 & 3.5383  & 1.1485 & 1.0717 & 1.9574 \\
%        $\widehat{\delta}_{{HT,1^{st}}}$  & 2 & 2.0414  & 0.8733 & 0.9345 & 1.1102 \\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 2 & 2.0245  & 0.4873 & 0.6981 & 0.9667 \\
%         $\widehat{\delta}_{{HT,2^{nd}}}$  & 1 & 1.0003  & 1.4039 & 1.1848 & 1.9204\\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 1 & 0.9930  & 0.9747 & 0.9873 & 1.8982\\
%         $\widehat{\delta}_{{HT,3^{rd}}}$ & 0.5 & 0.5230  & 1.8611 & 1.3642 &  2.5928\\
%        $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 0.5 & 0.5207 & 1.4523 & 1.2051 & 2.3927\\
% \hline
% \end{tabular}
% \end{table}





% \begin{table}[tbp]
%   \caption{Estimates under completely randomized design model 6.}%
%   \label{table3.7}
%   \centering
%   \begin{tabular}[c]{lccccc}\\
%   %\hline
%         Estimator & Effects & Emp.Estimates  & Emp.Var & Emp.S.D. & Var Estimate \\
% \hline
%        $ \widehat{\delta}_{{HT,Total}}$   & 7.5 & 7.5236  & 7.0767 & 2.6602 & 8.6687 \\
%        $ \widehat{\delta^*}_{{HT,Total}}$  & 7.5 & 7.5236  & 7.0767 & 2.6602 & 8.6687 \\
%        $\widehat{\delta}_{{HT,direct}}$  & 4 & 3.9586  & 7.9109 & 2.8126 & 8.1389\\
%        $ \widehat{\delta^*}_{{HT,direct}}$ & 4 & 4.0061  & 2.3713 & 1.5399 & 4.4075\\
%        $\widehat{\delta}_{{HT,indirect}}$ & 3.5 & 3.5649  & 1.5750 & 1.2550 & 2.3108\\
%        $ \widehat{\delta^*}_{{HT,indirect}}$ & 3.5 & 3.5174  & 2.1777 & 1.4757 & 3.5289 \\
%        $\widehat{\delta}_{{HT,1^{st}}}$  & 2 & 2.0414  & 0.8733 & 0.9345 & 1.1102 \\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 2 & 2.0198  & 1.1942 & 1.0928 & 2.3348\\
%         $\widehat{\delta}_{{HT,2^{nd}}}$  & 1 & 1.0003  & 1.4039 & 1.1848 & 1.9204\\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 1 & 0.9838  & 2.1622 & 1.4704 & 4.1131\\
%         $\widehat{\delta}_{{HT,3^{rd}}}$ & 0.5 & 0.5230  & 1.8611 & 1.3642 &  2.5928\\
%        $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 0.5 & 0.5138  & 3.0275 & 1.7399 & 4.7936 \\
% \hline
% \end{tabular}
% \end{table}



% \begin{table}[tbp]
%   \caption{Estimates under completely randomized design model 7.}%
%   \label{table3.8}
%   \centering
%   \begin{tabular}[c]{lccccc}\\
%  % \hline
%         Estimator & Effects & Emp.Estimates & Emp.Var & Emp.S.D. & Var Estimate \\
% \hline
%        $ \widehat{\delta}_{{HT,Total}}$ & 6 & 6.0344 & 4.9972 & 2.2354 & 6.0021\\
%        $ \widehat{\delta^*}_{{HT,Total}}$& 6 & 6.0344 & 4.9972 & 2.2354 & 6.0021 \\
%        $\widehat{\delta}_{{HT,direct}}$ & 0 & -0.0452 & 7.4190 & 2.7237 & 7.9636 \\
%        $ \widehat{\delta^*}_{{HT,direct}}$ & 0 & -0.0091 & 2.0143 & 1.4192 & 3.4718\\
%        $\widehat{\delta}_{{HT,indirect}}$ & 6 & 6.0797 & 2.7909 & 1.6705 & 4.850\\
%        $ \widehat{\delta^*}_{{HT,indirect}}$ & 6 & 6.0436 & 1.6675 & 1.2913 & 3.5938\\
%        $\widehat{\delta}_{{HT,1^{st}}}$  & 3 &3.0449  & 1.1582 & 1.0762 & 1.6049\\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 3 & 3.0295  & 0.5412 & 0.7357 & 1.1811\\
%         $\widehat{\delta}_{{HT,2^{nd}}}$  & 2 &2.0092 & 2.8500 & 1.6882 & 3.9616\\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 2 & 1.9961  & 1.4097 & 1.1873 & 2.9671\\
%         $\widehat{\delta}_{{HT,3^{rd}}}$ & 1 &1.0256  & 4.5078 & 2.1231 & 6.1172\\
%        $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 1 & 1.0179  & 2.6139 & 1.6167 & 4.4043\\
% \hline
% \end{tabular}
% \end{table}


% \begin{table}[tbp]
%   \caption{Estimates under completely randomized design model 8.}%
%   \label{table3.9}
%   \centering
%   \begin{tabular}[c]{lccccc}\\
%  % \hline
%         Estimator & Effects & Emp.Estimates & Emp.Var & Emp.S.D. & Var Estimate \\
% \hline
%        $ \widehat{\delta}_{{HT,Total}}$   & 7 & 7.0272  & 6.3337 & 2.5166 & 7.7149 \\
%        $ \widehat{\delta^*}_{{HT,Total}}$  & 7 & 7.0272  & 6.3337 & 2.5166 & 7.7149 \\
%        $\widehat{\delta}_{{HT,direct}}$  & 1 & 0.9474  & 8.9409 & 2.9901 & 9.4071 \\
%        $ \widehat{\delta^*}_{{HT,direct}}$   & 1 & 0.9905 & 2.4239 & 1.5568 & 4.1241 \\
%        $\widehat{\delta}_{{HT,indirect}}$ & 6 & 6.0797 & 2.7909 & 1.6705 & 4.8503\\
%        $ \widehat{\delta^*}_{{HT,indirect}}$ & 6 & 6.0366  & 1.9707 & 1.4038 & 4.0101\\
%        $\widehat{\delta}_{{HT,1^{st}}}$  & 3 & 3.0449  & 1.1582 & 1.0762 & 1.6049\\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 3 & 3.0280 & 0.6690 & 0.8179 & 1.4541\\
%         $\widehat{\delta}_{{HT,2^{nd}}}$  & 2 & 2.0092 & 2.8500 & 1.6882 & 3.9616 \\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 2 & 1.9930 & 1.7408 & 1.3194 & 3.6167\\
%         $\widehat{\delta}_{{HT,3^{rd}}}$ & 1 & 1.0256  & 4.5078 & 2.1231 & 6.1172\\
%        $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 1 & 1.0156 & 3.1489 & 1.7745 & 5.2252 \\
% \hline
% \end{tabular}
% \end{table}




% \begin{table}[tbp]
%   \caption{Estimates under completely randomized design model 9.}%
%   \label{table3.10}
%   \centering
%   \begin{tabular}[c]{lccccc}\\
%  % \hline
%         Estimator & Effects & Emp.Estimates  & Emp.Var & Emp.S.D. & Var Estimate \\
% \hline
%        $ \widehat{\delta}_{{HT,Total}}$   & 10 & 10.0055  & 11.54004 & 3.3970 & 14.4106 \\
%        $ \widehat{\delta^*}_{{HT,Total}}$  & 10 & 10.0055  & 11.54004 & 3.3970 & 14.4106 \\
%        $\widehat{\delta}_{{HT,direct}}$  & 4 &3.9257  & 14.7035 & 3.8345 & 15.0482 \\
%        $ \widehat{\delta^*}_{{HT,direct}}$ & 4 & 3.9896  & 4.0963 & 2.0239 & 7.2216\\
%        $\widehat{\delta}_{{HT,indirect}}$ & 6 & 6.0797  & 2.7909 & 1.6705 & 4.8503  \\
%        $ \widehat{\delta^*}_{{HT,indirect}}$ & 6 & 6.0158  & 3.3206 & 1.8222 & 6.0066\\
%        $\widehat{\delta}_{{HT,1^{st}}}$  & 3 &3.0449  & 1.1582 & 1.0762 & 1.6049\\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 3 & 3.0232  & 1.4675 & 1.2114 & 3.0152\\
%         $\widehat{\delta}_{{HT,2^{nd}}}$  & 2 &2.0092  & 2.8500 & 1.6882 & 3.9616\\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 2 & 1.9838  & 3.2241 & 1.7955 & 6.4517\\
%         $\widehat{\delta}_{{HT,3^{rd}}}$ & 1 &1.0256  & 4.5078 & 2.1231 & 6.1172\\
%        $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 1 & 1.0086  & 5.3193 & 2.3063 & 8.5285\\
% \hline
% \end{tabular}
% \end{table}




% \begin{table}[tbp]
%   \caption{Estimates under Bernoulli randomization model 1.}%
%   \label{table1Ber}
%   \centering
%   \begin{tabular}[c]{lcccccc}\\
%   %\hline
%         Estimator & Effects & Emp.Estimates  & Emp.Var & Emp.S.D. & Var Estimate \\
% \hline
%         $ \widehat{\delta}_{{HT,Total}}$   & 0 & 0.0059  & 1.2480 & 1.1171 & 1.2037  \\
%        $ \widehat{\delta^*}_{{HT,Total}}$  & 0 & 0.0059  & 1.2480 & 1.1171 & 1.2037  \\
%        $\widehat{\delta}_{{HT,direct}}$  & 0 & -0.01509  & 0.6850 & 0.8277 & 0.7016 \\
%        $ \widehat{\delta^*}_{{HT,direct}}$   & 0 & 0.0103  & 0.3116 & 0.5582 & 0.5153 \\
%        $\widehat{\delta}_{{HT,indirect}}$ & 0 & 0.0210 & 0.8232 &  0.9073 & 0.9258 \\
%        $ \widehat{\delta^*}_{{HT,indirect}}$   & 0 & -0.0044 & 0.5622 & 0.7498 & 0.6696 \\
%        $\widehat{\delta}_{{HT,1^{st}}}$  & 0 & 0.0502  & 0.6502 & 0.8063 & 0.6804 \\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 0 & 0.01735  & 0.2460 & 0.4960 & 0.3922 \\
%         $\widehat{\delta}_{{HT,2^{nd}}}$  & 0 & -0.0181  & 0.4430 & 0.6656 & 0.5985 \\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 0 & -0.0085 & 0.2702 & 0.5198 & 0.4599 \\
%         $\widehat{\delta}_{{HT,3^{rd}}}$ & 0 & -0.0110  & 0.4489 & 0.6700 & 0.6048 \\
%        $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 0 & -0.0132 &  0.3134 & 0.5599 & 0.4629 \\
% \hline
% \end{tabular}
% \end{table}





% \begin{table}[tbp]
%   \caption{Estimates under Bernoulli randomization model 2.}%
%   \label{table2Ber}
%   \centering
%   \begin{tabular}[c]{lccccc}\\
%   %\hline
%         Estimator & Effects & Emp.Estimates  & Emp.Var & Emp.S.D. & Var Estimate \\
% \hline
%         $ \widehat{\delta}_{{HT,Total}}$   & 1 & 1.0326  & 1.4420 & 1.2008 & 1.4406 \\
%        $ \widehat{\delta^*}_{{HT,Total}}$   & 1 & 1.0326 & 1.4420 & 1.2008 & 1.4406 \\
%        $\widehat{\delta}_{{HT,direct}}$  & 1 & 1.0116  & 0.8786 & 0.9373 & 0.8960 \\
%        $ \widehat{\delta^*}_{{HT,direct}}$   & 1 & 1.0277 & 0.3573 & 0.5978 & 0.6357  \\
%        $\widehat{\delta}_{{HT,indirect}}$ & 0 & 0.0210  & 0.8232 & 0.9073 & 0.9258 \\
%        $ \widehat{\delta^*}_{{HT,indirect}}$   & 0 & 0.0049  & 0.6462 & 0.8038 & 0.7994  \\
%        $\widehat{\delta}_{{HT,1^{st}}}$  & 0 & 0.0502  & 0.6502 & 0.8063 & 0.6804 \\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 0 & 0.0142 & 0.2858 & 0.5346 & 0.4783  \\
%         $\widehat{\delta}_{{HT,2^{nd}}}$  & 0 & -0.0181  & 0.4430 & 0.6656 & 0.5985 \\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 0 & -0.0177  & 0.3250 & 0.5700 & 0.5625  \\
%         $\widehat{\delta}_{{HT,3^{rd}}}$ & 0 & -0.0110  & 0.4489 & 0.6700 & 0.6048 \\
%        $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 0 & 0.0084  & 0.3846 & 0.6202 & 0.5497  \\
% \hline
% \end{tabular}
% \end{table}





% \begin{table}[tbp]
%   \caption{Estimates under Bernoulli randomization model 3.}%
%   \label{table3Ber}
%   \centering
%   \begin{tabular}[c]{lccccc}\\
%   %\hline
%         Estimator & Effects & Emp.Estimates  & Emp.Var & Emp.S.D. & Var Estimate \\
% \hline
%         $ \widehat{\delta}_{{HT,Total}}$   & 4 & 4.1129  & 4.0302 & 2.0075 & 4.5346  \\
%        $ \widehat{\delta^*}_{{HT,Total}}$  & 4 & 4.1129  & 4.0302 & 2.0075 & 4.5346  \\
%        $\widehat{\delta}_{{HT,direct}}$  & 4 & 4.0919 & 3.4654 & 1.8615 & 3.6069  \\
%        $ \widehat{\delta^*}_{{HT,direct}}$   & 4 & 4.0797 & 0.9582 & 0.9789 & 2.2095  \\
%        $\widehat{\delta}_{{HT,indirect}}$ & 0 & 0.0210  & 0.8232 & 0.9073 & 0.9258 \\
%        $ \widehat{\delta^*}_{{HT,indirect}}$   & 0 & 0.0331 & 1.8081 & 1.3446 & 2.3816  \\
%        $\widehat{\delta}_{{HT,1^{st}}}$  & 0 & 0.0502  & 0.6502 & 0.8063 & 0.6804 \\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 0 & 0.0051 & 0.8466 & 0.9201 & 1.5260  \\
%         $\widehat{\delta}_{{HT,2^{nd}}}$  & 0 & -0.0181 & 0.4430 & 0.6656 & 0.5985 \\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 0 & -0.0456 & 1.0226 & 1.0112 & 1.7932  \\
%         $\widehat{\delta}_{{HT,3^{rd}}}$ & 0 & -0.0110  & 0.4489 & 0.6700 & 0.6048 \\
%        $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 0 & 0.0737  & 1.2557 & 1.1205 & 1.7099  \\
% \hline
% \end{tabular}
% \end{table}



% \begin{table}[tbp]
%   \caption{Estimates under Bernoulli randomization model 4.}%
%   \label{table4Ber}
%   \centering
%   \begin{tabular}[c]{lccccc}\\
%   %\hline
%         Estimator & Effects & Emp.Estimates & Emp.Var & Emp.S.D. & Var Estimate \\
% \hline
%         $ \widehat{\delta}_{{HT,Total}}$  & 3.5 & 3.5995  & 3.3899 & 1.8411 & 3.7707  \\
%        $ \widehat{\delta^*}_{{HT,Total}}$  & 3.5 & 3.5995 & 3.3899 & 1.8411 & 3.7707  \\
%        $\widehat{\delta}_{{HT,direct}}$  & 0 & 0.0745 & 3.1656 & 1.7792 & 3.4406  \\
%        $ \widehat{\delta^*}_{{HT,direct}}$  & 0 & 0.0552 & 0.9018 & 0.9496 & 1.6046  \\
%        $\widehat{\delta}_{{HT,indirect}}$ & 3.5 & 3.5249  & 1.8276 & 1.3519 & 2.5207  \\
%        $ \widehat{\delta^*}_{{HT,indirect}}$   & 3.5 & 3.5443  & 1.5038 & 1.2262 & 2.1947  \\
%        $\widehat{\delta}_{{HT,1^{st}}}$  & 2 & 2.0532  & 0.9882 & 0.9940 & 1.1739  \\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 2 & 2.0206  & 0.3871 & 0.6222 & 0.7689  \\
%         $\widehat{\delta}_{{HT,2^{nd}}}$  & 1 & 0.9878  & 1.4253 & 1.1938 & 2.0036  \\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 1 & 0.9675 & 0.8350 & 0.9137 & 1.5577  \\
%         $\widehat{\delta}_{{HT,3^{rd}}}$ & 0.5 & 0.4838 & 2.2543 & 1.5014 & 2.8237\\
%        $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 0.5 & 0.5561  & 1.5671 & 1.2518 & 2.1355  \\
% \hline
% \end{tabular}
% \end{table}





% \begin{table}[tbp]
%   \caption{Estimates under Bernoulli randomization model 5.}%
%   \label{table5Ber}
%   \centering
%   \begin{tabular}[c]{lccccc}\\
%   %\hline
%         Estimator & Effects & Emp.Estimates & Emp.Var & Emp.S.D. & Var Estimate \\
% \hline
%         $ \widehat{\delta}_{{HT,Total}}$   & 4.5 & 4.6263 & 4.7542 & 2.1804 & 5.3978  \\
%        $ \widehat{\delta^*}_{{HT,Total}}$  & 4.5 & 4.6263 & 4.7542 & 2.1804 & 5.3978  \\
%        $\widehat{\delta}_{{HT,direct}}$  & 1 & 1.1013  & 4.3757 & 2.0918 & 4.6502  \\
%        $ \widehat{\delta^*}_{{HT,direct}}$   & 1 & 1.0725 & 1.1720 & 1.0826 & 2.0765 \\
%        $\widehat{\delta}_{{HT,indirect}}$ & 3.5 & 3.5249 & 1.8276 & 1.3519 & 2.5207  \\
%        $ \widehat{\delta^*}_{{HT,indirect}}$   & 3.5 & 3.5537  & 2.0192 & 1.4210 & 2.7832  \\
%        $\widehat{\delta}_{{HT,1^{st}}}$  & 2 & 2.0532 & 0.9882 & 0.9940 & 1.1739  \\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 2 & 2.0176  & 0.4648 & 0.6818 & 0.9666 \\
%         $\widehat{\delta}_{{HT,2^{nd}}}$ & 1 & 0.9878  & 1.4253 & 1.1938 & 2.0036 \\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 1 & 0.9582  & 1.1082 & 1.0527 & 2.0400  \\
%         $\widehat{\delta}_{{HT,3^{rd}}}$ & 0.5 & 0.4838 & 2.2543 & 1.5014 & 2.8237  \\
%        $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 0.5 & 0.5778 & 2.0343 & 1.4263 & 2.7262  \\
% \hline
% \end{tabular}
% \end{table}




% \begin{table}[tbp]
%   \caption{Estimates under Bernoulli randomization model 6.}%
%   \label{table6Ber}
%   \centering
%   \begin{tabular}[c]{lccccc}\\
%   %\hline
%         Estimator & Effects & Emp.Estimates & Emp.Var & Emp.S.D. & Var Estimate \\
% \hline
%         $ \widehat{\delta}_{{HT,Total}}$   & 7.5 & 7.7065 & 10.8531 & 3.2944 & 12.6623  \\
%        $ \widehat{\delta^*}_{{HT,Total}}$  & 7.5 & 7.7065 & 10.8531 & 3.2944 & 12.6623  \\
%        $\widehat{\delta}_{{HT,direct}}$  & 4 & 4.1815 & 10.0122 & 3.1642 & 10.4070 \\
%        $ \widehat{\delta^*}_{{HT,direct}}$  & 4 & 4.1245 & 2.4463 & 1.5640 & 4.7047  \\
%        $\widehat{\delta}_{{HT,indirect}}$ & 3.5 & 3.5249 & 1.8276 & 1.3519 & 2.5207  \\
%        $ \widehat{\delta^*}_{{HT,indirect}}$   & 3.5 & 3.5819 & 4.4755 & 2.1155 & 5.7415  \\
%        $\widehat{\delta}_{{HT,1^{st}}}$  & 2 & 2.0532 & 0.9882 & 0.9940 & 1.1739  \\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 2 & 2.0084  & 1.1391 & 1.0672 & 2.3490  \\
%         $\widehat{\delta}_{{HT,2^{nd}}}$  & 1 & 0.9878 & 1.4253 & 1.1938 & 2.0036 \\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 1 & 0.9304 & 2.4611 & 1.5687 & 4.4098 \\
%         $\widehat{\delta}_{{HT,3^{rd}}}$ & 0.5 & 0.4838  & 2.2543 & 1.5014 & 2.8237 \\
%        $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 0.5 & 0.6431  & 4.0935 & 2.0232 & 5.3983  \\
% \hline
% \end{tabular}
% \end{table}



% \begin{table}[tbp]
%   \caption{Estimates under Bernoulli randomization model 7.}%
%   \label{table7Ber}
%   \centering
%   \begin{tabular}[c]{lccccc}\\
%   %\hline
%         Estimator & Effects & Emp.Estimates  & Emp.Var & Emp.S.D. & Var Estimate \\
% \hline
%         $ \widehat{\delta}_{{HT,Total}}$   & 6 & 6.1664 & 7.4275 & 2.7253 & 8.5832  \\
%        $ \widehat{\delta^*}_{{HT,Total}}$  & 6 & 6.1664 & 7.4275 & 2.7253 & 8.5832  \\
%        $\widehat{\delta}_{{HT,direct}}$  & 0 & 0.1386  & 7.8388 & 2.7997 & 8.5881  \\
%        $ \widehat{\delta^*}_{{HT,direct}}$   & 0 & 0.0872 & 2.0487 & 1.4313 & 3.6387  \\
%        $\widehat{\delta}_{{HT,indirect}}$ & 6 & 6.0277 & 3.5973 & 1.8966 & 5.4461  \\
%        $ \widehat{\delta^*}_{{HT,indirect}}$   & 6 & 6.0791  & 3.2309 & 1.7974 & 5.0241  \\
%        $\widehat{\delta}_{{HT,1^{st}}}$ & 3 & 3.0547  & 1.3709 & 1.1708 & 1.7503  \\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 3 & 3.0223  & 0.5510 & 0.7423 & 1.2101  \\
%         $\widehat{\delta}_{{HT,2^{nd}}}$  & 2 & 1.9923 & 2.9090 & 1.7056 & 4.1353 \\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 2 & 1.9521 & 1.6716 & 1.2929 & 3.2181  \\
%         $\widehat{\delta}_{{HT,3^{rd}}}$ & 1 & 0.9806  & 5.3764 & 2.3187 & 6.7556  \\
%        $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 1 & 1.1047 & 3.8051 & 1.9506 & 5.1576 \\
% \hline
% \end{tabular}
% \end{table}






% \begin{table}[tbp]
%   \caption{Estimates under Bernoulli randomization model 8.}%
%   \label{table8Ber}
%   \centering
%   \begin{tabular}[c]{lccccc}\\
%   %\hline
%         Estimator & Effects & Emp.Estimates & Emp.Var & Emp.S.D. & Var Estimate \\
% \hline
%         $ \widehat{\delta}_{{HT,Total}}$   & 7 & 7.1931 & 9.6277 & 3.1028 & 11.2033  \\
%        $ \widehat{\delta^*}_{{HT,Total}}$  & 7 & 7.1931 & 9.6277 & 3.1028 & 11.2033  \\
%        $\widehat{\delta}_{{HT,direct}}$ & 1 & 1.1654 & 9.7750 & 3.1265 & 10.5230  \\
%        $ \widehat{\delta^*}_{{HT,direct}}$   & 1 & 1.1045 & 2.4792 & 1.5745 & 4.3616  \\
%        $\widehat{\delta}_{{HT,indirect}}$ & 6 & 6.0277 & 3.5973 & 1.8966 & 5.4461  \\
%        $ \widehat{\delta^*}_{{HT,indirect}}$   & 6 & 6.0885 & 4.0546 & 2.0136 & 5.9402 \\
%        $\widehat{\delta}_{{HT,1^{st}}}$ & 3 & 3.0547 & 1.3709 & 1.1708 & 1.7503  \\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 3 & 3.0192 & 0.6476 & 0.8047 & 1.4636  \\
%         $\widehat{\delta}_{{HT,2^{nd}}}$ & 2 & 1.9923 & 2.9090 & 1.7056 & 4.1353  \\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 2 & 1.9428 & 2.0746 & 1.4403 & 3.9275  \\
%         $\widehat{\delta}_{{HT,3^{rd}}}$ & 1 & 0.9806 & 5.3764 & 2.3187 & 6.7556  \\
%        $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 1 & 1.1264 & 4.5503 & 2.1331 & 6.0993 \\
% \hline
% \end{tabular}
% \end{table}


% \begin{table}[tbp]
%   \caption{Estimates under Bernoulli randomization model 9.}%
%   \label{table9Ber}
%   \centering
%   \begin{tabular}[c]{lccccc}\\
%   %\hline
%         Estimator & Effects & Emp.Estimates & Emp.Var & Emp.S.D. & Var Estimate \\
% \hline
%         $ \widehat{\delta}_{{HT,Total}}$   & 10 & 10.2734  & 18.2343 & 4.2701 & 21.4468 \\
%        $ \widehat{\delta^*}_{{HT,Total}}$  & 10 & 10.2734  & 18.2343 & 4.2701 & 21.4468  \\
%        $\widehat{\delta}_{{HT,direct}}$  & 4 & 4.2456 & 17.5899 & 4.1940 & 18.4553  \\
%        $ \widehat{\delta^*}_{{HT,direct}}$ & 4 & 4.1566 & 4.2346 & 2.0578 & 7.7429 \\
%        $\widehat{\delta}_{{HT,indirect}}$ & 6 & 6.0277 & 3.5973 & 1.8966 & 5.4461  \\
%        $ \widehat{\delta^*}_{{HT,indirect}}$  & 6 & 6.1168 & 7.4354 & 2.7267 & 9.8815  \\
%        $\widehat{\delta}_{{HT,1^{st}}}$  & 3 & 3.0547 & 1.3709 & 1.1708 & 1.7503  \\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 3 & 3.0100 & 1.3786 & 1.1741 & 3.0134  \\
%         $\widehat{\delta}_{{HT,2^{nd}}}$  & 2 & 1.9923  & 2.9090 & 1.7056 & 4.1353  \\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$ & 2 & 1.9150 & 3.8171 & 1.9537 & 6.9785 \\
%         $\widehat{\delta}_{{HT,3^{rd}}}$ & 1 & 0.9806 & 5.3764 & 2.3187 & 6.7556 \\
%        $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 1 & 1.1917  & 7.4434 & 2.7282 & 9.8241 \\
% \hline
% \end{tabular}
% \end{table}






% \begin{table}[tbp]
%   \caption{Estimates of Anti-Conflict Program with K = 2 for only Treated School N = 348.}%
%   \label{tableDataK2T}
%   \centering
%   \begin{tabular}[c]{lcc}\\
%         Estimator & Estimates & S.E. \\
% \hline
%        $ \widehat{\delta}_{{HT,Total}}$  & 0.1899 & 0.0985 \\
%        $ \widehat{\delta^*}_{{HT,Total}}$   & 0.1899 & 0.0985 \\
%        $\widehat{\delta}_{{HT,direct}}$  & -0.0254 & 0.1332 \\
%        $ \widehat{\delta^*}_{{HT,direct}}$  & 0.0559 & 0.0863 \\
%        $\widehat{\delta}_{{HT,indirect}}$  & 0.2154 & 0.0927 \\
%        $ \widehat{\delta^*}_{{HT,indirect}}$  & 0.1340 & 0.0781 \\
%        $\widehat{\delta}_{{HT,1^{st}}}$ & 0.1788 & 0.0822 \\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 0.1019 & 0.0683  \\
%         $\widehat{\delta}_{{HT,2^{nd}}}$ & 0.0365 & 0.1148 \\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$ & 0.0320 & 0.0934 \\
% \hline
% \end{tabular}
% \end{table}



%\begin{table}[tbp]
%  \caption{Estimates of Anti-Conflict Program with K = 2 for only Treated %School N = 47.}%
%  \label{tableData47}
%  \centering
%  \begin{tabular}[c]{lcc}\\
%        Estimator & Estimates & S.E. \\
%\hline
%       $ \widehat{\delta}_{{HT,Total}}$  & 0.5844 & 0.3944 \\
%       $ \widehat{\delta^*}_{{HT,Total}}$   & 0.5844 & 0.3944 \\
%       $\widehat{\delta}_{{HT,direct}}$  & 0.5844 & 0.3783 \\
%       $ \widehat{\delta^*}_{{HT,direct}}$  & 0.4552 & 0.2892 \\
%       $\widehat{\delta}_{{HT,indirect}}$  & 0 & 0 \\
%       $ \widehat{\delta^*}_{{HT,indirect}}$  & 0.1291 & 0.2684  \\
%       $\widehat{\delta}_{{HT,1^{st}}}$ & 0.6521 & 0.4098 \\
%       $\widehat{\delta^*}_{{HT,1^{st}}}$ & 0.4891 & 0.2274 \\
%        $\widehat{\delta}_{{HT,2^{nd}}}$ & -0.6521 & 0.4098 \\
%       $\widehat{\delta^*}_{{HT,2^{nd}}}$ & -0.3599 & 0.3125 \\
%\hline
%\end{tabular}
%\end{table}




%\begin{table}[tbp]
% \caption{CCCCCC.}%
%  \label{tableDataN}
%  \centering
%  \begin{tabular}[c]{lcc}\\
%        Category & Number of Units  \\
%\hline
%       $ \widehat{\delta}_{{HT,Total}}$  &  &  \\
%       $ \widehat{\delta^*}_{{HT,Total}}$   &  &  \\
%       $\widehat{\delta}_{{HT,direct}}$  &  &  \\
%       $ \widehat{\delta^*}_{{HT,direct}}$  &  &   \\
%       $\widehat{\delta}_{{HT,indirect}}$  &  &  \\
%       $ \widehat{\delta^*}_{{HT,indirect}}$  &  &   \\
%       $\widehat{\delta}_{{HT,1^{st}}}$ &  &  \\
%       $\widehat{\delta^*}_{{HT,1^{st}}}$ &  &   \\
%        $\widehat{\delta}_{{HT,2^{nd}}}$ &  &  \\
%       $\widehat{\delta^*}_{{HT,2^{nd}}}$ &  &  \\
%\hline
%\end{tabular}
%\end{table}
%


%\begin{table}[tbp]
%  \caption{Number of units in each exposure of Anti-Conflict Program Experiment with K =2 for N = 348 .}%
%  \label{tableDataTT}
%  \centering
  %\hline
 %\multicolumn{1}{c}{Indirect} \\
 %\hline
 % \begin{tabular}[c]{lcccc}\\
 %       Direct & $(0,0)$&$(0,1)$&
 %       $(1,0)$&$(1,1)$ \\
%\hline
   %    Treated & 38 & 42 & 39 & 34 \\
  %     Control & 40 & 59 & 46 & 50 \\
%\hline
%\end{tabular}
%\end{table}


%\begin{table}[tbp]
%  \caption{Number of units in each exposure of Anti-Conflict Program Experiment with K =2 for N = 47.}%
%  \label{tableDataTT}
%  \centering
  %\hline
 %\multicolumn{1}{c}{Indirect} \\
 %\hline
%  \begin{tabular}[c]{lcccc}\\
%        Direct & $(0,0)$&$(0,1)$&
%        $(1,0)$&$(1,1)$ \\
%\hline
%       Treated & 8 & 4 & 5 & 6 \\
%       Control & 3 & 8 & 8 & 5 \\
%\hline
%\end{tabular}
%\end{table}


%Samirah: which table format do you like Table   \label{tableDataTTT} or   \label{tableDataTT}?




% \begin{table}[tbp]
%   \caption{Number of units in each exposure of Anti-Conflict Program Experiment with K =3.}%
%   \label{tableDataTT}
%   \centering
%   \begin{tabular}[c]{lcccccccc}\\
%         Direct & $(000)$ & $(001)$&$(010)$&
%         $(100)$&$(011)$&$(101)$&$(110)$&$(111)$ \\
% \hline
%        Treated & 5 & 6 & 3 & 6 & 8 & 7 & 11 & 1 \\
%        Control & 6 & 8 & 3 & 4 & 11 & 4 & 10 & 7\\
% \hline
% \end{tabular}
% \end{table}

\section{Discussion}
\label{makereference3.9}
%Tables~\ref{table3.2}, \ref{table3.6} and \ref{table3.10}
  Results indicate, first and foremost, that all estimators under all models accurately estimate all direct and indirect effect parameters---as expected from our theoretical results.
Additionally we also verify the decomposition of estimation of the total effect into direct and indirect components and of the indirect effect estimator into nearest-neighbor components given by~\eqref{sumofdirectindirectCH3.1},~\eqref{sumofdirectindirectCH3.2},~\eqref{sumofdirectindirectCH36.1}, and~\eqref{sumofdirectindirectCH36.2}.
Finally, we confirm that our conservative variance estimators are larger than the true variance, as estimated by the empirical variance.

%MIKE: We need to figure out why this is the case, and possibly compute the individual components of this variance.  I don't they'd reject the paper for this, but it'd be nice to figure out why this is the case.
%MIKE: START HERE. 
%MIKE: REFER TO ESTIMATORS DIFFERENTLY UNDER ASSUMPTION 1 and ASSUMPTION 2?
%MIKE: Rewrite here.  Focus on (Samirah: Done)
%  How do nearest neighbor variances vary for different neighbors?  
%  How good are Assumption 2 estimators at increasing precision
%  Differences between Complete randomization and Bernoulli randomization.
Under CRD, variance estimates tend to be smaller for closer neighbors under both Assumption \ref{Assumption1} and Assumption \ref{Assumption2}.
%for the CRD design, variance estimates tend to be smaller for closer neighbors.
The one exception occurs for models that do not exhibit treatment interference. 
% the closer the neighbor, the smaller the variance estimates except in the first indirect effect scenario when there is no interference (Tables \ref{table3.2}, \ref{table3.6} and \ref{table3.10}).
Additionally, the variance estimates tend to be smaller for estimators under Assumption~\ref{Assumption2} than under Assumption~\ref{Assumption1} (Tables \ref{table3.2}, \ref{table3.6}).
However, this may not be the case when direct effects are large;
the variance estimates for indirect effects estimators under Assumption 1 are about 50\% smaller than under Assumption \ref{Assumption2} (Table \ref{table3.10}).

Similar results hold for BRD except that Assumption~\ref{Assumption2} did not reduce standard errors of the indirect effects in the moderate direct effect scenario (Table \ref{table3.6}).
Moreover, all variance estimates for all estimators in weak and moderate interference scenarios are slightly larger than those under CRD (Tables \ref{table3.6} and \ref{table3.10}), with differences becoming larger as direct and indirect effects increase.
Hence, CRD may be preferable to BRD for this type of data.


% Under completely randomized  design, results of the first and second models in the first scenario where there is no indirect effects show that all estimators under Assumption \ref{Assumption1} and Assumption \ref{Assumption2} are unbiased.
% All variance estimates are greater than the empirical variance for all estimators under both Assumption \ref{Assumption1} and Assumption \ref{Assumption2}. 
% The variance estimates are smaller for all estimators under Assumption \ref{Assumption2} than under Assumption \ref{Assumption1} (Tables \ref{table3.2}, \ref{table3.3}).
% However, when we increase the direct effect, the variance estimates for the indirect effects estimators under Assumption 1 are about 50\% smaller than under Assumption \ref{Assumption2} (Table \ref{table3.4}).

% Similarly, under weak and moderate interference scenarios, all estimators are unbiased and all variance estimates are greater than the empirical variance for all estimators under Assumption 1 and Assumption \ref{Assumption2}.
% In addition, the variance estimates are smaller for all estimators under Assumption \ref{Assumption2} than under Assumption \ref{Assumption1} except model 3, model 6 and model 9 when the direct effect relatively large and close to the overall indirect effects (Tables \ref{table3.5}, \ref{table3.6}, \ref{table3.7}, \ref{table3.8}, \ref{table3.9} and \ref{table3.10}).

% Moreover, Assumption \ref{Assumption2} has improved the variance estimates of the direct effect estimator for all indirect effect scenarios specially when the direct effect increases, the variance estimates get smaller. 

% All previous results of completely randomized apply to Bernoulli randomization except that Assumption \ref{Assumption2} didn't improve standard errors of the indirect effects in models 5 and model8.
% However, under Bernoulli randomization, all variance estimates for all estimators in all scenarios seem to be larger than those of the completely randomized design (Tables \ref{table1Ber} to table \ref{table9Ber}).
% Hence, completely randomized design is preferable over Bernoulli randomization for this type of data.





\section{An Analysis of Social Network Experiment}
\label{makereference3.10}

We now apply our estimators to our motivating example which assessed the efficacy of a program designed to promote anti-conflict behaviors in New Jersey middle schools (see Section~\ref{sec:motivexamp}).
Recall that the experiment was explicitly designed to determine whether benefits of the program were propagated through social interactions between students, and moreover, that the strength of the connection between students was explicitly recorded when surveying students in the experiment.  
For our analysis, we use whether or not a student wears an orange wristband as our response.  
A student may be given an orange wristband by a seed student if the original student is observed to be exhibiting anti-conflict behaviors.
%seed student observes that original s displaying anti-conflict behaviors.


% of, we analyze a field experiment conducted on 56 schools for changing climates of conflict among middle schools students in New Jersey \citep{paluck2016changing}. 
% In this multilevel experiment, 28 out of 56 schools were randomly assigned to receive an anti-conflict program that aims to reduce conflicts among adolescents and to understand how the effects on participants transmit to their social peers.
% In each treated school, 20-32 students were nonrandomly selected as eligible students in which 50\% of those eligible students were randomly assigned to participate in the anti-conflict intervention blocked by gender and grade.
% Following three weeks of the start of school, students in each school were asked to nominate up to 10 students with whom they chose to spent time with in the last few weeks either in school, out of school or online where number 1 is the first person a student spent most time with, number 2 is the second person a student spent most time with and so on. 
% These nominations are used to measure the social connections between students providing the adjacency graph.
% Every two weeks over the course of the year, trained research assistants in each treated school had held meetings with participants to identify conflict types in their school and discuss strategies that encourage participants to reduce conflict behaviour among other students. 
% At the end of the school year, a survey was conducted to measure conflict norms that measures multiple outcomes.
% In this analysis, we focus on one particular response in which students self reported if they had worn an orange wristband issued and distributed to students reflecting their attitudes of anti-conflict norms. 

To better fit the study into our KNNIM framework, 
we restrict our analysis to seed-eligible students 
%in treated school 
who listed two other seed-eligible students among their top 10 closest connections over the prior few weeks. 
%students that they spent the most amount of time with over the past 
%few weeks.
This creates a dataset of $N = 348$ students.
We then analyze the data according to a KNNIM model with $K = 2$, where we assume that the seed-student designations were assigned completely at random to the 348 eligible students.
For more discussion of this choice of $K$, please see~\citet{alzubaidi2022detecting}.
Table~\ref{tableDataTTT} gives the number of people assigned to each of the $2^{K+1}= 8$ possible treatment exposures.

% of the 10 other students they spent two other seed-eligible students in their ``top 10 students'' question item. nominated at least two eligible friends ($N = 348$ with $K = 2$) and we assume that the carried randomization design was completely at random with 153 treated students.


% There are $2^{K+1}$ = 8 exposures and number of students for each of the eight exposures in this dataset are illustrated in table \ref{tableDataTTT}. 
% The marginal and the joint inclusion probabilities are computed using equations  \ref{CRDpi}and \ref{JointCRD}.

We use our proposed estimators to estimate the effect of treatment under Assumption~\ref{Assumption1} and Assumption~\ref{Assumption2}.
Results are presented in Table~\ref{tableDataK2T}. 
Under Assumption \ref{Assumption1},
results suggest that indirect exposure to treatment leads to about a 21\% increase in the probability of wearing a wristband.
The bulk of this effect (about 18\%) is due to the effect from a student's nearest neighbor.  
Our analysis did not find a strong effect of either a direct effect or indirect effects outside of the first nearest neighbor.

% The estimate of the total effect (direct + indirect) of the anti-conflict program on eligible students with at least two eligible nearest neighbors is 18\% increase in the probability of wearing a wrist band while the direct exposure has smaller on the the probability of wearing a wrist band.
Under Assumption \ref{Assumption2}, direct exposure treatment leads to approximately a 5\% increase in the probability of wearing a wrist band while the indirect exposure provides about a 13\% increase in the probability of wearing a wrist band.  
Again, a majority of the indirect effect can be attributed to the treatment status of the nearest neighbor (about 10\%).

Finally, there seems to be differences in the estimates of indirect effect between Assumption~\ref{Assumption1} and Assumption~\ref{Assumption2}.
This could suggest a potential violation of Assumption~\ref{Assumption2} for this dataset; while all estimates of standard errors are considerably smaller under Assumption~\ref{Assumption2}, more care needs to be performed prior to moving forward with such an analysis.
Rigorous methods for testing Assumption~\ref{Assumption2} is an area of future research.


\begin{table}[tbp]
  \caption{Number of units in each exposure of Anti-Conflict Program Experiment with $K =2$, $N= 348$.}%
  \label{tableDataTTT}
  \centering
  \begin{tabular}[c]{lcccc}\\
  %\multirow{2}{c}{Direct}
  \hline
 \multicolumn{5}{c}{Indirect} \\
 \hline
        Direct & $(0,0)$&$(0,1)$&
        $(1,0)$&$(1,1)$ \\
\hline
       Treated & 38 & 42 & 39 & 34 \\
       Control & 40 & 59 & 46 & 50 \\
\hline
\end{tabular}
\end{table}







%MIKE: MAYBE MAKE ESTIMATORS AS COLUMNS TO SAVE SPACE?


\begin{table}[tbp]
  \caption{Estimates of Anti-Conflict Program with K = 2 for only Treated School N = 348.}%
  \footnotesize
  \label{tableDataK2T}
  \centering
  \begin{tabular}[c]{rccccccccc}\\
      Estimator & 
      $ \widehat{\delta}_{{HT,tot}}$ &
      $\widehat{\delta}_{{HT,dir}}$ &
      $ \widehat{\delta^*}_{{HT,dir}}$ &
        $\widehat{\delta}_{{HT,ind}}$ &
        $ \widehat{\delta^*}_{{HT,ind}}$ & $\widehat{\delta}_{{HT,1^{st}}}$ & $\widehat{\delta^*}_{{HT,1^{st}}}$ & $\widehat{\delta}_{{HT,2^{nd}}}$ & $\widehat{\delta^*}_{{HT,2^{nd}}}$ \\
\hline
Estimates & 0.1899 & -0.0254 &  0.0559 & 0.2154 &  0.1340 & 0.1788 &  0.1019 & 0.0365 & 0.0320 \\
S.E. &0.0985 &  0.1332 &  0.0863 & 0.0927 & 0.0781 & 0.0822 & 0.0683 &  0.1148 & 0.0934 \\
\hline
\normalsize
\end{tabular}
\end{table}


% The friend with whom a student chose to spend most time with has an effect about 17\% increase in the probability of wearing a wrist band where it is 3\% for the second friend under both Assumptions \ref{Assumption1} and Assumption \ref{Assumption2}.
% All estimates of standard errors of all estimators are reduced under Assumption \ref{Assumption2}.


%MIKE: Work on conclusion.  More detail about what was done (Samirah: Done).  

\section{Conclusion}
\label{makereference3.11}


Substantial effort has been devoted to developing new techniques and models for causal inference when interference is presence---that is, when the effect of treatment is not limited to the unit that receives treatment, but can also impact other units that interact with the original units. 
One recently-proposed model for treatment interference is the $K$-nearest neighbors interference model (KNNIM)~\citep{alzubaidi2022detecting}, in which the treatment status of one unit can affect the response of the $K$ ``closest'' units to that original unit. 

We define causal estimands related to this model---including the average $\ell$th nearest neighbor treatment effect (A$\ell$NNIE)---and show that the indirect effect of treatment can be expressed as a sum of A$\ell$NNIE effects
That is, we show that the KNNIM model is able to determine how the treatment statuses of each of the individual neighbors of a unit contribute to that unit's response.  
We derive unbiased estimators for these estimands and derive conservative variance estimates for these unbiased estimators.
We consider a new assumption on the potential outcomes under treatment interference---no weak interaction between direct and indirect effects---and extend these estimators under this assumption.
We perform a simulation study to determine the efficacy of these estimators under various settings.

We apply our methodology to a recent experiment testing the efficacy of a program designed to reduce conflict in middle schools in New Jersey~\citep{paluck2016changing}.  
This experiment has two appealing characteristics: it was designed to propagate through peer-to-peer interactions between students and students were explicitly surveyed about which students were their ``closest connections.'' 
Using our approach, we give evidence that that one measure in particular---wearing a wristband given for demonstrating anti-conflict behaviors---is affected primarily by the students' closest connection.






% Causal inference in the presence of interference has been a trend in the past decade. 
% We provided unbiased estimators of the defined estimands given exact marginal and joint probabilities for possible treatment exposures and derived conservative standard errors for these estimators. 
% We also provided a closed-form solution to compute these exposure
% probabilities under completely-randomized and Bernoulli-randomized experimental designs.
% %Estimation of indirect effects under the $K$-neighborhood can answer many interesting questions which has not been studied.
% We uncover indirect effects of the $K$-nearest neighbors which have not been studied and considered in previous work.
% We then demonstrated how these estimators may have significantly stronger precision under an assumption of no interaction between direct and indirect effects.

% We assessed our methods through a simulation that confirmed our theoretical findings.
% Assumption \ref{Assumption2} achieved better results with respect to estimation precision than Assumption \ref{Assumption1} specifically, for smaller direct effects and completely randomized design is preferable over Bernoulli randomization for this type of data.
% %MIKE: Include this paragraph.
% We apply our estimators to a recent study on the efficacy of an anti-conflict program implemented across middle schools in New Jersey.  
% Using our estimators, we suggest that the effect of the program propagates primarily through students' closest connection; further-out connections do not significantly impact students' behavior.
% An extension to this work could be an improvement of the estimation standard errors, specially for large direct effects.
% Another future work is to obtain an optimal design for the $K$-nearest neighbors interference.


%new margin nere


\bibliographystyle{chicago}
\bibliography{References}







\newpage
\begin{center}
{\large\bf SUPPLEMENTARY MATERIAL}
\end{center}



                    %       AppendixA      %
                %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%    Properties of Horvitz-Thompson Estimators     %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


                    %       AppendixB      %
                %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%    Properties of Horvitz-Thompson Estimators in Chapter 3    %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix
% %MIKE: I am just going to remove this.  
% \section{Properties of Horvitz-Thompson Estimators}
% \label{AppendixA}
% \subsection{Explicit formulas for estimators under $K$-NIA Assumption}
% \subsubsection{Estimator for ATOT}

% \begin{equation} 
% \widehat{\delta}_{{HT,tot}} = \bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0}).
% \end{equation} 




% \subsubsection{The Expected Value of ATOT  Estimator}
% \label{makereferenceB.1.1}

% \begin{multline}
% \mathbf{E}(\widehat{\delta}_{{HT,tot}}) = \mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% =\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) - \mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% \\= \bar{y}(1,\mathbf{1}) -  \bar{y}(0,\mathbf{0}) = \delta_{tot}.
% \end{multline}


% \subsubsection{The Variance of ATOT  Estimator}
% \label{makereferenceB.1.2}
% We find the variance by using the properties $\mathbf{Var}(X-Y)$ = $\mathbf{Var}(X)$ + $\mathbf{Var}(Y)$ -2$\mathbf{Cov}(X,Y)$ and $\mathbf{Cov}(X,Y)$ = $\mathbf{E}(XY) - \mathbf{E}(X)\mathbf{E}(Y)$.

% \begin{multline}
% \mathbf{Var}(\widehat{\delta}_{{HT,tot}}) = \mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})) = \mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% \\ -2\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \\ =  \frac{1}{N^2}\sum_{i = 1}^{N}\pi_{i}(1,\mathbf{1})[1 - \pi_{i}(1,\mathbf{1})]\left[ \frac{y_{i}(1,\mathbf{1})}{\pi_{i}(1,\mathbf{1})}\right]^2
% % \\   + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(1,\mathbf{1}) - \pi_{i}(1,\mathbf{1})\pi_{j}(1,\mathbf{1})]\frac{y_{i}(1,\mathbf{1})y_{j}(1,\mathbf{1})}{\pi_{i}(1,\mathbf{1})\pi_{j}(1,\mathbf{1})}
% % \\  +  \frac{1}{N^2}\sum_{i = 1}^{N}\pi_{i}(0,\mathbf{0})[1 - \pi_{i}(0,\mathbf{0})]\left[ \frac{y_{i}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{0})}\right]^2 
% % \\  + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(0,\mathbf{0}) - \pi_{i}(0,\mathbf{0})\pi_{j}(0,\mathbf{0})]\frac{y_{i}(0,\mathbf{0})y_{j}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{0})\pi_{j}(0,\mathbf{0})}
% % \\  - 2 \left( \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,\mathbf{1}),(0,\mathbf{0})) - \pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{0})\right] \frac{y_{i}(1,\mathbf{1})y_{j}(0,\mathbf{0})}{\pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{1})} \right.
% % \\  \left.- \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}(1,\mathbf{1})y_{i}(0,\mathbf{0})\right).
% \end{multline}












% \subsection{Properties of ADE Estimator under $K$-NIA}
% \label{AppendixB.2}

% \begin{equation} 
% \widehat{\delta}_{{HT,dir}} = \bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1}).
% \end{equation} 


% \subsubsection{The Expected Value of ADE Estimator}
% \label{makereferenceB.2.1}

% \begin{multline}
% \mathbf{E}(\widehat{\delta}_{{HT,dir}}) = \mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% =\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) - \mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% \\= \bar{y}(1,\mathbf{1}) -  \bar{y}(0,\mathbf{1}) = \delta_{dir}.
% \end{multline}


% \subsubsection{The Variance of ADE Estimator}
% \label{makereferenceB.2.2}

% We find the variance by using the properties $\mathbf{Var}(X-Y)$ = $\mathbf{Var}(X)$ + $\mathbf{Var}(Y)$ -2$\mathbf{Cov}(X,Y)$ and $\mathbf{Cov}(X,Y)$ = $\mathbf{E}(XY) - \mathbf{E}(X)\mathbf{E}(Y)$.


% \begin{multline}
% \mathbf{Var}(\widehat{\delta}_{{HT,dir}}) = \mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1})) = \mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% \\ -2\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% \\ =  \frac{1}{N^2}\sum_{i = 1}^{N}\pi_{i}(1,\mathbf{1})[1 - \pi_{i}(1,\mathbf{1})]\left[ \frac{y_{i}(1,\mathbf{1})}{\pi_{i}(1,\mathbf{1})}\right]^2
% \\   + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(1,\mathbf{1}) - \pi_{i}(1,\mathbf{1})\pi_{j}(1,\mathbf{1})]\frac{y_{i}(1,\mathbf{1})y_{j}(1,\mathbf{1})}{\pi_{i}(1,\mathbf{1})\pi_{j}(1,\mathbf{1})}
% \\  +  \frac{1}{N^2}\sum_{i = 1}^{N}\pi_{i}(0,\mathbf{1})[1 - \pi_{i}(0,\mathbf{1})]\left[ \frac{y_{i}(0,\mathbf{1})}{\pi_{i}(0,\mathbf{1})}\right]^2 
% \\  + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(0,\mathbf{1}) - \pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{1})]\frac{y_{i}(0,\mathbf{1})y_{j}(0,\mathbf{1})}{\pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{1})}
% \\  - 2 \left( \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,\mathbf{1}),(0,\mathbf{1})) - \pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{1})\right] \frac{y_{i}(1,\mathbf{1})y_{j}(0,\mathbf{1})}{\pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{1})} \right.
% \\  \left.- \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}(1,\mathbf{1})y_{i}(0,\mathbf{1})\right).
% \end{multline}














% \subsection{Properties of AIE Estimator under $K$-NIA}
% \label{AppendixB.3}

% \begin{equation} 
% \widehat{\delta}_{{HT,ind}} = \bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0}).
% \end{equation} 

% Additionally, the average total effect estimator is the sum of the average direct and indirect estimators: 
% \begin{lemma}\label{lemma1B}
% \begin{equation}\label{sumofdirectindirect}
% \widehat{\delta}_{HT,tot} = \widehat{\delta}_{HT,dir} + \widehat{\delta}_{HT,ind}
% \end{equation}
% \end{lemma}
% %where $\delta_{r}$ is the indirect effect of the remaining units that are not included in the $K$-neighborhood $\mathcal{N}_{ik}$.

% \begin{proof}
% \begin{multline}
% \widehat{\delta}_{HT,dir} + \widehat{\delta}_{HT,ind}
% =  \bar{Y}_{HT}^{obs}(1, \mathbf{1}) -\bar{Y}_{HT}^{obs}(0, \mathbf{1})+ \bar{Y}_{HT}^{obs}(0, \mathbf{1}) -\bar{Y}_{HT}^{obs}(0, \mathbf{0}) 
% \\ =  \bar{Y}_{HT}^{obs}(1, \mathbf{1}) - \bar{Y}_{HT}^{obs}(0, \mathbf{0}) 
% \\ = \widehat{\delta}_{HT,tot} 
% \end{multline}
% \end{proof}




% \subsubsection{The Expected Value of AIE Estimator}
% \label{makereferenceB.3.1}

% \begin{multline}
% \mathbf{E}(\widehat{\delta}_{{HT,ind}}) = \mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% =\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) - \mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% \\= \bar{y}(0,\mathbf{1}) -  \bar{y}(0,\mathbf{0}) = \delta_{ind}.
% \end{multline}


% \subsubsection{The Variance of AIE Estimator}
% \label{makereferenceB.3.2}

% We find the variance by using the properties $\mathbf{Var}(X-Y)$ = $\mathbf{Var}(X)$ + $\mathbf{Var}(Y)$ -2$\mathbf{Cov}(X,Y)$ and $\mathbf{Cov}(X,Y)$ = $\mathbf{E}(XY) - \mathbf{E}(X)\mathbf{E}(Y)$.

% \begin{multline}
% \mathbf{Var}(\widehat{\delta}_{{HT,ind}}) = \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})) = \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) + \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% \\ -2\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% \\ =  \frac{1}{N^2}\sum_{i = 1}^{N}\pi_{i}(0,\mathbf{1})[1 - \pi_{i}(0,\mathbf{1})]\left[ \frac{y_{i}(0,\mathbf{1})}{\pi_{i}(0,\mathbf{1})}\right]^2
% \\   + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(0,\mathbf{1}) - \pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{1})]\frac{y_{i}(0,\mathbf{1})y_{j}(0,\mathbf{1})}{\pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{1})}
% \\  +  \frac{1}{N^2}\sum_{i = 1}^{N}\pi_{i}(0,\mathbf{0})[1 - \pi_{i}(0,\mathbf{0})]\left[ \frac{y_{i}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{0})}\right]^2 
% \\  + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(0,\mathbf{0}) - \pi_{i}(0,\mathbf{0})\pi_{j}(0,\mathbf{0})]\frac{y_{i}(0,\mathbf{0})y_{j}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{0})\pi_{j}(0,\mathbf{0})}
% \\  - 2 \left( \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((0,\mathbf{1}),(0,\mathbf{0})) - \pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{0})\right] \frac{y_{i}(0,\mathbf{1})y_{j}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{1})} \right.
% \\  \left.- \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}(0,\mathbf{1})y_{i}(0,\mathbf{0})\right).
% \end{multline}










% \subsection{Properties of A$\ell$NNIE Estimator under $K$-NIA }
% \label{AppendixB.4}

% \begin{equation} 
% \widehat{\delta}_{{HT,\ell}} = \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1}).
% \end{equation} 


% First, note that the indirect effect estimator $\widehat{\delta}_{{HT,ind}}$ is the sum of all K-nearest neighbors indirect effects estimators ,i.e.,
%  \begin{lemma}\label{lemma2B}
% \begin{equation}
% \widehat{\delta}_{{HT,ind}} = \sum_{\ell = 1}^{K} \widehat{\delta}_{HT,\ell}
% %= \widehat{\delta}_{HT,1^{st}} 
% %+ \widehat{\delta}_{HT,2^{nd}}+ \dots + \widehat{\delta}_{HT,\ell}+ \dots + \widehat{\delta}_{HT,K} 
% %+ \delta_{r}.
% \end{equation}
% \end{lemma}
% %where $\delta_{r}$ is the indirect effect of the remaining units that are not included in the $K$-neighborhood $\mathcal{N}_{ik}$.

% \begin{proof}
% For $\ell \in \mathcal{N}_{ik}$, if we define $W^{*}_{\ell}$ as previously such that $W^{*}_{1}$ means that the first nearest neighbor is treated where the rest of the nearest neighbors are assigned to control,
% $W^{*}_{2}$ means that the first two nearest neighbors are treated and the rest are assigned to control and so forth.
% $W^{*}_{0}$ denote that all K-nearest neighbors are assigned to control. Then,


% \begin{multline}
% \widehat{\delta}_{HT,1^{st}} + \widehat{\delta}_{HT,2^{nd}}+ \widehat{\delta}_{HT,3^{rd}}+\dots + \widehat{\delta}_{HT,\ell}+ \dots + \widehat{\delta}_{HT,K} = \\
% \bar{Y}_{HT}^{obs}(0,W^{*}_{1}) - \bar{Y}_{HT}^{obs}(0, W^{*}_{0}) \\
% + \bar{Y}_{HT}^{obs}(0,W^{*}_{2}) - \bar{Y}_{HT}^{obs}(0, W^{*}_{1})\\
% + \bar{Y}_{HT}^{obs}(0,W^{*}_{3}) - \bar{Y}_{HT}^{obs}(0, W^{*}_{2}) \\
% + \dots \\
% + \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0, W^{*}_{\ell - 1})\\
% + \dots \\
% + \bar{Y}_{HT}^{obs}(0,W^{*}_{K}) - \bar{Y}_{HT}^{obs}(0, W^{*}_{K- 1})\\
% = \bar{Y}_{HT}^{obs}(0,W^{*}_{K}) - \bar{Y}_{HT}^{obs}(0, W^{*}_{0}) \\
% =  \bar{Y}_{HT}^{obs}(0, \mathbf{1}) -\bar{Y}_{HT}^{obs}(0, \mathbf{0}) \\
% =\widehat{\delta}_{HT,ind} \\
% \end{multline}
% \end{proof}


% %Since our primary interest is in the indirect effects, we focus on the the K-nearest neighbors and the $\ell^{th}$ nearest neighbor indirect effect and we don't extend the total effects any further. 
% %Similarly, we can show that the average total effect estimator $\widehat{\delta}_{Total}$ is the sum of the direct and indirect effects ,i.e.,
% %\begin{equation}\label{sumofdirectindirect}
% %\widehat{\delta}_{HT,Total} = \widehat{\delta}_{HT,direct} + \widehat{\delta}_{HT,ind}
% %\end{equation}




% \subsubsection{The Expected Value of A$\ell$NNIE Estimator}
% \label{makereferenceB.4.1}

% \begin{multline}
% \mathbf{E}(\widehat{\delta}_{{HT,\ell}}) = \mathbf{E}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})) 
% =\mathbf{E}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell})) - \mathbf{E}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1}))
% \\= \bar{y}(0,W^{*}_{\ell}) -  \bar{y}(0,W^{*}_{\ell-1})= \delta_{\ell}.
% \end{multline}


% \subsubsection{The Variance of A$\ell$NNIE Estimator}
% \label{makereferenceB.4.2}

% We find the variance by using the property $\mathbf{Var}(X-Y)$ = $\mathbf{Var}(X)$ + $\mathbf{Var}(Y)$ -2$\mathbf{Cov}(X,Y)$ and the property $\mathbf{Cov}(X,Y)$ = $\mathbf{E}(XY) - \mathbf{E}(X)\mathbf{E}(Y)$.

% \begin{multline}
% \mathbf{Var}(\widehat{\delta}_{{HT,\ell}}) = \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})) = \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell})) + \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})) 
% \\ -2\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}),\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})
% \\ =  \frac{1}{N^2}\sum_{i = 1}^{N}\pi_{i}(0,W^{*}_{\ell})[1 - \pi_{i}(0,W^{*}_{\ell})]\left[ \frac{y_{i}(0,W^{*}_{\ell})}{\pi_{i}(0,W^{*}_{\ell})}\right]^2
% \\   + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(0,W^{*}_{\ell}) - \pi_{i}(0,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell})]\frac{y_{i}(0,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell})}{\pi_{i}(0,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell})}
% \\  +  \frac{1}{N^2}\sum_{i = 1}^{N}\pi_{i}(0,W^{*}_{\ell-1})[1 - \pi_{i}(0,W^{*}_{\ell-1})]\left[ \frac{y_{i}(0,W^{*}_{\ell-1})}{\pi_{i}(0,W^{*}_{\ell-1})}\right]^2 
% \\  + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(0,W^{*}_{\ell-1}) - \pi_{i}(0,W^{*}_{\ell-1})\pi_{j}(0,W^{*}_{\ell-1})]\frac{y_{i}(0,W^{*}_{\ell-1})y_{j}(0,W^{*}_{\ell-1})}{\pi_{i}(0,W^{*}_{\ell-1})\pi_{j}(0,W^{*}_{\ell-1})}
% \\  - 2 \left( \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((0,W^{*}_{\ell}),(0,W^{*}_{\ell-1})) - \pi_{i}(0,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell-1})\right] \frac{y_{i}(0,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell-1})}{\pi_{i}(0,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell-1})} \right.
% \\  \left.- \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}(0,W^{*}_{\ell})y_{i}(0,W^{*}_{\ell-1})\right).
% \end{multline}










% \subsection{Properties of ATOT  Estimator under the No Weak Interaction between Direct and Indirect Effects Assumption}
% \label{AppendixB.5}


% The unbiased Horvitz Thompson estimator $\widehat{\delta^*}_{HT,tot}$ under the no weak interaction between direct and indirect effects assumption is provided in the following lemma,


% \begin{lemma}\label{lemma3B}
% \begin{equation}\label{Ass2TotalAppB}
% \widehat{\delta^*}_{HT,tot} =  \widehat{\delta^*}_{HT,dir} +
% \widehat{\delta^*}_{HT,ind} % = \widehat{\delta}_{HT,tot}.
% \end{equation} 
% \end{lemma}


% \begin{proof}
% \begin{align*}
% \widehat{\delta^*}_{HT,dir} + \widehat{\delta^*}_{HT,ind}
% =   c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1})] +
% c_{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{0}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})]
% \\+  c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(1,\mathbf{0})] +
% c_{2}[\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})]
% \\= c_{1}\bar{Y}_{HT}^{obs}(1,\mathbf{1}) +  c_{1}\bar{Y}_{HT}^{obs}(1,\mathbf{1}) 
% - c_{2} \bar{Y}_{HT}^{obs}(0,\mathbf{0})
% - c_{2} \bar{Y}_{HT}^{obs}(0,\mathbf{0})
% \\= 2c_{1}\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - 2c_{2}\bar{Y}_{HT}^{obs}(0,\mathbf{0})
% %\\= \widehat{\delta}_{HT,tot} 
% \end{align*}

% and if $c_{1} = c_{2} = \frac{1}{2}$, then 
% $\widehat{\delta^*}_{HT,tot}  = \widehat{\delta}_{HT,tot}$.

% \end{proof}


% \subsubsection{The Expected Value of  ATOT  Estimator}
% \label{makereferenceB.5.1}

% \begin{multline}
% \mathbf{E}(\widehat{\delta^*}_{{HT,tot}}) = \mathbf{E}(\widehat{\delta^*}_{HT,dir}) + \mathbf{E}(\widehat{\delta^*}_{HT,ind}) 
% = \delta_{dir} + \delta_{ind} = \delta_{tot}.
% \end{multline}

% where $\mathbf{E}(\widehat{\delta^*}_{HT,dir})$ and $\mathbf{E}(\widehat{\delta^*}_{HT,ind})$ are provided in the next two subsections.

% \subsubsection{The Variance of ATOT  Estimator}
% \label{makereferenceB.5.2}




% \begin{multline}
% \mathbf{Var}(\widehat{\delta^*}_{{HT,tot}}) = \mathbf{Var}(\widehat{\delta^*}_{HT, dir} + \widehat{\delta^*}_{HT,ind}) \\ = \mathbf{Var}(c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1})] +
% c_{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{0}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})]
% \\+  c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(1,\mathbf{0})] + c_{2}[\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})])
% \\=\mathbf{Var}(2c_{1} \bar{Y}_{HT}^{obs}(1,\mathbf{1}) - 2c_{2}\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% \\= 4c^2_{1}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + 4c^2_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
%  -2(4c_{1}c_{2})\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% \end{multline}

% and for $c_{1} + c_{2} = \frac{1}{2}$,

% \begin{multline}
% \mathbf{Var}(\widehat{\delta^*}_{{HT,tot}}) = \mathbf{Var}(\widehat{\delta}_{{HT,tot}})
% \\ = \mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
%  -2\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% \end{multline}


% % We can also find the variance as follows for $c_{1} + c_{2} = \frac{1}{2}$:
% % \begin{multline}
% % \mathbf{Var}(\widehat{\delta^*}_{{HT,tot}}) = \mathbf{Var}(\widehat{\delta^*}_{HT,direct} + \widehat{\delta^*}_{HT,ind})
% % \\ = \mathbf{Var}(\widehat{\delta^*}_{HT,direct}) + \mathbf{Var}(\widehat{\delta^*}_{HT,ind}) + 2\mathbf{Cov}(\widehat{\delta^*}_{HT,direct} , \widehat{\delta^*}_{HT,ind})
% % \end{multline}

% % where
% % \begin{multline}
% % \mathbf{Var}(\widehat{\delta^*}_{HT,direct}) + \mathbf{Var}(\widehat{\delta^*}_{HT,ind}) =
% % \\ \frac{1}{4}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + \frac{1}{4}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) +
% % \frac{1}{4}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}) + \frac{1}{4}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \\ -\frac{1}{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% % +\frac{1}{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(1,\mathbf{0})) 
% % \\ -\frac{1}{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% % -\frac{1}{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}), \bar{Y}_{HT}^{obs}(1,\mathbf{0})) 
% % \\ +\frac{1}{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% % -\frac{1}{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}), \bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \\ + \frac{1}{4}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + \frac{1}{4}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{0})) +
% % \frac{1}{4}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) + \frac{1}{4}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \\ -\frac{1}{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(1,\mathbf{0})) 
% % + \frac{1}{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% % \\ -\frac{1}{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% % -\frac{1}{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}), \bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% % \\ + \frac{1}{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% % -\frac{1}{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% % \\ = \frac{1}{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + \frac{1}{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{0})) +
% % \frac{1}{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) + \frac{1}{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \\ - \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) - \mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}), \bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % \end{multline}

% % and
% % \begin{multline}
% % \mathbf{Cov}(\widehat{\delta^*}_{HT,direct} , \widehat{\delta^*}_{HT,ind}) = \mathbf{E}(\widehat{\delta^*}_{HT,direct}\widehat{\delta^*}_{HT,ind}) -
% % \mathbf{E}(\widehat{\delta^*}_{HT,direct})\mathbf{E}(\widehat{\delta^*}_{HT,ind})
% % \\=  \mathbf{E}((\frac{1}{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1})] +
% % \frac{1}{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{0}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})])
% % \\ \times(\frac{1}{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(1,\mathbf{0})] +
% % \frac{1}{2}[\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})]))
% % \\ - (\mathbf{E}(\frac{1}{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1})] +
% % \frac{1}{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{0}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})])
% % \\ \times \mathbf{E}(\frac{1}{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(1,\mathbf{0})] +
% % \frac{1}{2}[\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})]))
% % \\ = \mathbf{E}\left(\frac{1}{4}\bar{Y}_{HT}^{obs}(1,\mathbf{1})\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \frac{1}{4}\bar{Y}_{HT}^{obs}(1,\mathbf{1})\bar{Y}_{HT}^{obs}(1,\mathbf{0}) + \frac{1}{4}\bar{Y}_{HT}^{obs}(1,\mathbf{1})\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \frac{1}{4}\bar{Y}_{HT}^{obs}(1,\mathbf{1})\bar{Y}_{HT}^{obs}(0,\mathbf{0})\right.
% % \\ \left. - \frac{1}{4}\bar{Y}_{HT}^{obs}(0,\mathbf{1})\bar{Y}_{HT}^{obs}(1,\mathbf{1}) + \frac{1}{4}\bar{Y}_{HT}^{obs}(0,\mathbf{1})\bar{Y}_{HT}^{obs}(1,\mathbf{0})
% % - \frac{1}{4}\bar{Y}_{HT}^{obs}(0,\mathbf{1})\bar{Y}_{HT}^{obs}(0,\mathbf{1})
% % + \frac{1}{4}\bar{Y}_{HT}^{obs}(0,\mathbf{1})\bar{Y}_{HT}^{obs}(0,\mathbf{0})
% % \right.
% % \\ \left.+ \frac{1}{4}\bar{Y}_{HT}^{obs}(1,\mathbf{0})\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \frac{1}{4}\bar{Y}_{HT}^{obs}(1,\mathbf{0})\bar{Y}_{HT}^{obs}(1,\mathbf{0})
% % + \frac{1}{4}\bar{Y}_{HT}^{obs}(1,\mathbf{0})\bar{Y}_{HT}^{obs}(0,\mathbf{1})
% % - \frac{1}{4}\bar{Y}_{HT}^{obs}(1,\mathbf{0})\bar{Y}_{HT}^{obs}(0,\mathbf{0})\right.
% % \\ \left.- \frac{1}{4}\bar{Y}_{HT}^{obs}(0,\mathbf{0})\bar{Y}_{HT}^{obs}(1,\mathbf{1}) + \frac{1}{4}\bar{Y}_{HT}^{obs}(0,\mathbf{0})\bar{Y}_{HT}^{obs}(1,\mathbf{0}) - \frac{1}{4}\bar{Y}_{HT}^{obs}(0,\mathbf{0})\bar{Y}_{HT}^{obs}(0,\mathbf{1}) 
% % +  \frac{1}{4}\bar{Y}_{HT}^{obs}(0,\mathbf{0})\bar{Y}_{HT}^{obs}(0,\mathbf{0})
% % \right) 
% % \\ - \left(\frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))
% % - \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{0})) 
% % + \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))\right. 
% % \\ \left. - \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % - \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))
% % + \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}))\right.
% % \\ \left.- \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% % + \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % + \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}))\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))\right.
% % \\ \left. - \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}))\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % + \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% % - \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))\right.
% % \\ \left. - \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))
% % + \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % - \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))\right.
% % \\ \left. + \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \right)
% % \\ = \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) 
% % - \frac{1}{2}\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))\bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% % + \frac{1}{2}\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))\bar{Y}_{HT}^{obs}(1,\mathbf{0})) 
% % \\ - \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))\bar{Y}_{HT}^{obs}(0,\mathbf{1})) - \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}))\bar{Y}_{HT}^{obs}(1,\mathbf{0})) 
% % + \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))\bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% % \\ - \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) 
% % +  \frac{1}{2}\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % -  \frac{1}{2}\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % \\ +  \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% % +  \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}))\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{0})) 
% %  - \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% %  \\ = \frac{1}{4}\mathbf{Va}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))
% %  - \frac{1}{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% %  + \frac{1}{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % \\ - \frac{1}{4}\mathbf{Va}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% % - \frac{1}{4}\mathbf{Va}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % + \frac{1}{4}\mathbf{Va}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \end{multline}

% % Therefore, 

% % \begin{multline}
% % \mathbf{Var}(\widehat{\delta^*}_{{HT,tot}}) = \mathbf{Var}(\widehat{\delta^*}_{HT,direct} + \widehat{\delta^*}_{HT,ind})
% % \\ = \mathbf{Var}(\widehat{\delta^*}_{HT,direct}) + \mathbf{Var}(\widehat{\delta^*}_{HT,ind}) + 2\mathbf{Cov}(\widehat{\delta^*}_{HT,direct} , \widehat{\delta^*}_{HT,ind})
% % \\ = \frac{1}{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + \frac{1}{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{0})) +
% % \frac{1}{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) + \frac{1}{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \\ - \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) - \mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}), \bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% %  \\ + 2\left( \frac{1}{4}\mathbf{Va}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))
% %  - \frac{1}{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% %  + \frac{1}{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}))\right.
% % \\ \left.- \frac{1}{4}\mathbf{Va}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% % - \frac{1}{4}\mathbf{Va}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % + \frac{1}{4}\mathbf{Va}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \right)
% % \\ = \mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% %  -2\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \end{multline}









% % We can also find the variance as follows:
% % \begin{multline}
% % \mathbf{Var}(\widehat{\delta^*}_{{HT,tot}}) = \mathbf{Var}(\widehat{\delta^*}_{HT,direct} + \widehat{\delta^*}_{HT,ind}) \\ = \mathbf{Var}(c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1})] +
% % c_{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{0}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})]
% % \\+  c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(1,\mathbf{0})] + c_{2}[\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})])
% % \\ = c^2_{1}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}) + c^2_{1}\mathbf{Var}( \bar{Y}_{HT}^{obs}(0,\mathbf{1}) +
% % c^2_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{0})  + c^2_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})
% % \\+  c^2_{1}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}) + c^2_{1}\mathbf{Var}( \bar{Y}_{HT}^{obs}(1,\mathbf{0}) +
% % c^2_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})  + c^2_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})
% % \\ - 2c^2_{1}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% % + 2c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % \\ - 2c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % + 2c^2_{1}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))
% % \\- 2c^2_{1}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % + 2c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% % \\ - 2c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \\- 2c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}),\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % \\ + 2c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % - 2c^2_{1}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}),\bar{Y}_{HT}^{obs}(1,\mathbf{1}))
% % \\+ 2c^2_{1}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}),\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % - 2c_{1}c_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% % \\ +  2c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \\ - 2c^2_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % + 2c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}),\bar{Y}_{HT}^{obs}(1,\mathbf{1}))
% % \\- 2c_{1}c_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}),\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % + 2c^2_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}),\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% % \\ -  2c^2_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \\ - 2c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}),\bar{Y}_{HT}^{obs}(1,\mathbf{1}))
% % \\+ 2c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}),\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % - 2c^2_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}),\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% % \\ +  2c^2_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \\- 2c^2_{1}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % + 2c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% % \\ - 2c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \\- 2c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}),\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% % \\ + 2c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% %  - 2c^2_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% %  \\
% % \end{multline}

% % This can be simplified as follows
% % \begin{multline}
% % \mathbf{Var}(\widehat{\delta^*}_{{HT,tot}}) = \mathbf{Var}(\widehat{\delta^*}_{HT,direct} + \widehat{\delta^*}_{HT,ind}) \\ = \mathbf{Var}(c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1})] +
% % c_{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{0}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})]
% % \\+  c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(1,\mathbf{0})] + c_{2}[\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})])
% % \\ = 4c^2_{1}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}) + c^2_{1}\mathbf{Var}( \bar{Y}_{HT}^{obs}(0,\mathbf{1}) +
% % c^2_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{0})  + 4c^2_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})
% % \\+ c^2_{1}\mathbf{Var}( \bar{Y}_{HT}^{obs}(1,\mathbf{0}) +
% % c^2_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})  
% % \\ - 4c^2_{1}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% % + 4c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % \\ - 8c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % - 2c^2_{1}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % \\+ 4c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% % - 4c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}),\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % \\ + 4c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % + 2c^2_{1}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}),\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % \\- 2c_{1}c_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% % - 4c^2_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \\- 2c_{1}c_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % + 2c^2_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}),\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% % \\+ 4c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}),\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % - 4c^2_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}),\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% % \\- 2c^2_{1}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% %  \end{multline}

% % and for $c_{1} + c_{2} = \frac{1}{2}$,

% % \begin{multline}
% % \mathbf{Var}(\widehat{\delta^*}_{{HT,tot}}) = \mathbf{Var}(\widehat{\delta}_{{HT,tot}})
% % \\ = \mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% %  -2\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \end{multline}

% \subsection{Properties of ADE Estimator under the No Weak Interaction between Direct and Indirect Effects Assumption}
% \label{AppendixB.6}


% For $c_{1} + c_{2} = 1$,


% \begin{equation}\label{Assumption2directAppB}
% \widehat{\delta^*}_{HT, dir} =  c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1})] +
% c_{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{0}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})]
% \end{equation} 


% % Note that under the no-interaction between direct and indirect effects assumption and for $c_{1} + c_{2} = 1$, $c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1})] =
% % c_{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{0}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})]$ and we have the following.

% \subsubsection{The Expected Value of ADE Estimator}
% \label{makereferenceB.6.1}

% \begin{multline} 
% \mathbf{E}(\widehat{\delta^*}_{HT, dir}) =  \mathbf{E}(c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1})] +
% c_{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{0}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})])
% \\ = \mathbf{E}(c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1})]) +
% \mathbf{E}(c_{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{0}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})])
% \\= c_{1}[\bar{y}(1,\mathbf{1}) -  \bar{y}(0,\mathbf{1})]
% + c_{2}[\bar{y}(1,\mathbf{0}) -  \bar{y}(0,\mathbf{0})]
% = \delta_{dir}
% \end{multline} 

% Hence, $\widehat{\delta^*}_{HT, dir}$ is an unbiased estimator for $\delta_{dir}$.

% \subsubsection{The Variance of ADE Estimator}
% \label{makereferenceB.6.2}




% The variance can simply be computed using the property that $\mathbf{Var(\sum_{i = 1}^{N}a_{i}X_{i})} =\sum_{i=1}^{N}a_{i}\mathbf{Var(X_{i})} + 2\sum_{j\ne i}a_{i}a_{j}\mathbf{Cov}(X_{i}X_{j})$ as follows.




% \begin{multline}
% \mathbf{Var}(\widehat{\delta^{*}}_{HT,dir}) =  \mathbf{Var}(c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1})] +
% c_{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{0}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})])
% %\\ = \mathbf{Var}(c_{\ell1}\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - c_{\ell1}\bar{Y}_{HT}^{obs}(0,\mathbf{1}) +
% %c_{\ell2}\bar{Y}_{HT}^{obs}(1,\mathbf{0}) - c_{\ell2}\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% \\ = c^2_{1}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + c^2_{1}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) +
% c^2_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}) + c^2_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% \\ -2c^2_{1} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% +2c_{1}c_{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(1,\mathbf{0})) 
% \\ -2c_{1}c_{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% -2c_{1}c_{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}), \bar{Y}_{HT}^{obs}(1,\mathbf{0})) 
% \\ +2c_{1}c_{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% -2c^2_{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% \\ =  \frac{c^2_{1}}{N^2}\sum_{i = 1}^{N}\pi_{i}(1,\mathbf{1})[1 - \pi_{i}(1,\mathbf{1})]\left[ \frac{y_{i}(1,\mathbf{1})}{\pi_{i}(1,\mathbf{1})}\right]^2
% \\  + \frac{c^2_{1}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(1,\mathbf{1}) - \pi_{i}(1,\mathbf{1})\pi_{j}(1,\mathbf{1})]\frac{y_{i}(1,\mathbf{1})y_{j}(1,\mathbf{1})}{\pi_{i}(1,\mathbf{1})\pi_{j}(1,\mathbf{1})}.
% \\ +  \frac{c^2_{1}}{N^2}\sum_{i = 1}^{N}\pi_{i}(0,\mathbf{1})[1 - \pi_{i}(0,\mathbf{1})]\left[ \frac{y_{i}(0,\mathbf{1})}{\pi_{i}(0,\mathbf{1})}\right]^2
% \\  + \frac{c^2_{1}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(0,\mathbf{1}) - \pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{1})]\frac{y_{i}(0,\mathbf{1})y_{j}(0,\mathbf{1})}{\pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{1})}
% \\ +  \frac{c^2_{2}}{N^2}\sum_{i = 1}^{N}\pi_{i}(1,\mathbf{0})[1 - \pi_{i}(1,\mathbf{0})]\left[ \frac{y_{i}(1,\mathbf{0})}{\pi_{i}(1,\mathbf{0})}\right]^2
% \\  + \frac{c^2_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(1,\mathbf{0}) - \pi_{i}(1,\mathbf{0})\pi_{j}(1,\mathbf{0})]\frac{y_{i}(1,\mathbf{0})y_{j}(1,\mathbf{0})}{\pi_{i}(1,\mathbf{0})\pi_{j}(1,\mathbf{0})}.
% \\ +  \frac{c^2_{2}}{N^2}\sum_{i = 1}^{N}\pi_{i}(0,\mathbf{0})[1 - \pi_{i}(0,\mathbf{0})]\left[ \frac{y_{i}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{0})}\right]^2
% \\  + \frac{c^2_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(0,\mathbf{0}) - \pi_{i}(0,\mathbf{0})\pi_{j}(0,\mathbf{0})]\frac{y_{i}(0,\mathbf{0})y_{j}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{0})\pi_{j}(0,\mathbf{0})}
% \\ - 2 \left( \frac{c^2_{1}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,\mathbf{1}),(0,\mathbf{1})) - \pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{1})\right] \frac{y_{i}(1,\mathbf{1})y_{j}(0,\mathbf{1})}{\pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{1})}\right.
% \\ \left.- \frac{c^2_{1}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,\mathbf{1})y_{i}(0,\mathbf{1})\right)
% \\ + 2 \left( \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,\mathbf{1}),(1,\mathbf{0})) - \pi_{i}(1,\mathbf{1})\pi_{j}(1,\mathbf{0})\right] \frac{y_{i}(1,\mathbf{1})y_{j}(1,\mathbf{0})}{\pi_{i}(1,\mathbf{1})\pi_{j}(1,\mathbf{0})}\right.
% \\ \left.- \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,\mathbf{1})y_{i}(1,\mathbf{0})\right)
% .
% \end{multline}



% \begin{multline}
% \\ - 2 \left( \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,\mathbf{1}),(0,\mathbf{0})) - \pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{0})\right] \frac{y_{i}(1,\mathbf{1})y_{j}(0,\mathbf{0})}{\pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{0})}\right.
% \\ \left.- \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,\mathbf{1})y_{i}(0,\mathbf{0})\right)
% \\ - 2 \left( \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((0,\mathbf{1}),(1,\mathbf{0})) - \pi_{i}(0,\mathbf{1})\pi_{j}(1,\mathbf{0})\right] \frac{y_{i}(0,\mathbf{1})y_{j}(1,\mathbf{0})}{\pi_{i}(0,\mathbf{1})\pi_{j}(1,\mathbf{0})}\right.
% \\ \left.- \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}  y_{i}(0,\mathbf{1})y_{i}(1,\mathbf{0})\right)
% \\ + 2 \left( \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((0,\mathbf{1}),(0,\mathbf{0})) - \pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{0})\right] \frac{y_{i}(0,\mathbf{1})y_{j}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{0})}\right.
% \\ \left.- \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}  y_{i}(0,\mathbf{1})y_{i}(0,\mathbf{0})\right)
% \\ - 2 \left( \frac{c^2_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,\mathbf{0}),(0,\mathbf{0})) - \pi_{i}(1,\mathbf{0})\pi_{j}(0,\mathbf{0})\right] \frac{y_{i}(1,\mathbf{0})y_{j}(0,\mathbf{0})}{\pi_{i}(1,\mathbf{0})\pi_{j}(0,\mathbf{0})}\right.
% \\ \left.- \frac{c^2_{2}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,\mathbf{0})y_{i}(0,\mathbf{0})\right)
% \end{multline}



% \subsection{Properties of AIE Estimator under the No Weak Interaction between Direct and Indirect Effects Assumption}
% \label{AppendixB.7}

% For $c_{1} + c_{2} = 1$,

% \begin{equation}\label{Assumption2indirect}
% \widehat{\delta^*}_{HT,ind} =  c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(1,\mathbf{0})] +
% c_{2}[\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})]
% \end{equation} 


% % Note that under the no weak interaction between direct and indirect effects assumption and for for $c_{1} + c_{2} = 1$, $c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(1,\mathbf{0})] =
% % c_{2}[\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})]$ and we have the following.




% \subsubsection{The Expected Value of AIE Estiamtor}
% \label{makereferenceB.7.1}

% \begin{multline} 
% \mathbf{E}(\widehat{\delta^*}_{HT,ind}) =  \mathbf{E}(c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(1,\mathbf{0})] +
% c_{2}[\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})])
% \\ = \mathbf{E}(c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(1,\mathbf{0})]) +
% \mathbf{E}(c_{2}[\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})])
% \\= c_{1}[\bar{y}(1,\mathbf{1}) -  \bar{y}(1,\mathbf{0})]
% + c_{2}[\bar{y}(0,\mathbf{1}) -  \bar{y}(0,\mathbf{0})]
% = \delta_{ind}
% \end{multline} 

% Hence, $\widehat{\delta^*}_{HT,ind}$ is an unbiased estimator for $\delta_{ind}$.


% \subsubsection{The Variance of AIE Estiamtor}
% \label{makereferenceB.7.2}
% The variance can simply be computed using the property that $\mathbf{Var(\sum_{i = 1}^{N}a_{i}X_{i})} =\sum_{i=1}^{N}a_{i}\mathbf{Var(X_{i})} + 2\sum_{j\ne i}a_{i}a_{j}\mathbf{Cov}(X_{i}X_{j})$ as follows.




% \begin{multline}
% \mathbf{Var}(\widehat{\delta^{*}}_{HT,ind}) =  \mathbf{Var}(c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(1,\mathbf{0})] +
% c_{2}[\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})])
% %\\ = \mathbf{Var}(c_{\ell1}\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - c_{\ell1}\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1}) +
% %c_{\ell2}\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - %c_{\ell2}\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1}))
% \\ = c^2_{1}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + c^2_{1}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{0})) +
% c^2_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) + c^2_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% \\ -2c^2_{1} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(1,\mathbf{0})) 
% + 2c_{1}c_{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% \\ -2c_{1}c_{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% -2c_{1}c_{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}), \bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% \\ + 2c_{1}c_{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% -2c^2_{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% \\ =  \frac{c^2_{1}}{N^2}\sum_{i = 1}^{N}\pi_{i}(1,\mathbf{1})[1 - \pi_{i}(1,\mathbf{1})]\left[ \frac{y_{i}(1,\mathbf{1})}{\pi_{i}(1,\mathbf{1})}\right]^2
% \\  + \frac{c^2_{1}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(1,\mathbf{1}) - \pi_{i}(1,\mathbf{1})\pi_{j}(1,\mathbf{1})]\frac{y_{i}(1,\mathbf{1})y_{j}(1,\mathbf{1})}{\pi_{i}(1,\mathbf{1})\pi_{j}(1,\mathbf{1})}.
% \\ +  \frac{c^2_{1}}{N^2}\sum_{i = 1}^{N}\pi_{i}(1,\mathbf{0})[1 - \pi_{i}(1,\mathbf{0})]\left[ \frac{y_{i}(1,\mathbf{0})}{\pi_{i}(1,\mathbf{0})}\right]^2
% \\  + \frac{c^2_{1}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(1,\mathbf{0}) - \pi_{i}(1,\mathbf{0})\pi_{j}(1,\mathbf{0})]\frac{y_{i}(1,\mathbf{0})y_{j}(1,\mathbf{0})}{\pi_{i}(1,\mathbf{0})\pi_{j}(1,\mathbf{0})}
% \\ +  \frac{c^2_{2}}{N^2}\sum_{i = 1}^{N}\pi_{i}(0,\mathbf{1})[1 - \pi_{i}((0,\mathbf{1})]\left[ \frac{y_{i}(0,\mathbf{1})}{\pi_{i}(0,\mathbf{1})}\right]^2
% \\  + \frac{c^2_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(0,\mathbf{1}) - \pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{1})]\frac{y_{i}(0,\mathbf{1})y_{j}(0,\mathbf{1})}{\pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{1})}
% \\ +  \frac{c^2_{2}}{N^2}\sum_{i = 1}^{N}\pi_{i}(0,\mathbf{0})[1 - \pi_{i}(0,\mathbf{0})]\left[ \frac{y_{i}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{0})}\right]^2
% \\  + \frac{c^2_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(0,\mathbf{0}) - \pi_{i}(0,\mathbf{0})\pi_{j}(0,\mathbf{0})]\frac{y_{i}(0,\mathbf{0})y_{j}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{0})\pi_{j}(0,\mathbf{0})}
% \\ - 2 \left( \frac{c^2_{1}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,\mathbf{1}),(1,\mathbf{0})) - \pi_{i}(1,\mathbf{1})\pi_{j}(1,\mathbf{0})\right] \frac{y_{i}(1,\mathbf{1})y_{j}(1,\mathbf{0})}{\pi_{i}(1,\mathbf{1})\pi_{j}(1,\mathbf{0})}\right.
% \\ \left.- \frac{c^2_{1}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,\mathbf{1})y_{i}(1,\mathbf{0})\right)
% \\ + 2 \left( \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,\mathbf{1}),(0,\mathbf{1})) - \pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{1})\right] \frac{y_{i}(1,\mathbf{1})y_{j}(0,\mathbf{1})}{\pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{1})}\right.
% \\ \left.- \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,\mathbf{1})y_{i}(0,\mathbf{1})\right)
% .
% \end{multline}



% \begin{multline}
% \\ - 2 \left( \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,\mathbf{1}),(0,\mathbf{0})) - \pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{0})\right] \frac{y_{i}(1,\mathbf{1})y_{j}(0,\mathbf{0})}{\pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{0})}\right.
% \\ \left.- \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,\mathbf{1})y_{i}(0,\mathbf{0})\right)
% \\ - 2 \left( \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,\mathbf{0}),(0,\mathbf{1})) - \pi_{i}(1,\mathbf{0})\pi_{j}(0,\mathbf{1})\right] \frac{y_{i}(1,\mathbf{0})y_{j}(0,\mathbf{1})}{\pi_{i}(1,\mathbf{0})\pi_{j}(0,\mathbf{1})}\right.
% \\ \left.- \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,\mathbf{0})y_{i}(0,\mathbf{1})\right)
% \\ + 2 \left( \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,\mathbf{0}),(0,\mathbf{0})) - \pi_{i}(1,\mathbf{0})\pi_{j}(0,\mathbf{0})\right] \frac{y_{i}(1,\mathbf{0})y_{j}(0,\mathbf{0})}{\pi_{i}(1,\mathbf{0})\pi_{j}(0,\mathbf{0})}\right.
% \\ \left.- \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,\mathbf{0})y_{i}(0,\mathbf{0})\right)
% \\ - 2 \left( \frac{c^2_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((0,\mathbf{1}),(0,\mathbf{0})) - \pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{0})\right] \frac{y_{i}(0,\mathbf{1})y_{j}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{0})}\right.
% \\ \left.- \frac{c^2_{2}}{N^2}\sum_{i = 1}^{N}  y_{i}(0,\mathbf{1})y_{i}(0,\mathbf{0})\right)
% \end{multline}



% \subsection{Properties of A$\ell$NNIE Estimator under the No Weak Interaction between Direct and Indirect Effects Assumption}
% \label{AppendixB.8}



% % Next, we compute the expected value and variance of the unbiased Horvitz Thompson estimator $\widehat{\delta^*}_{{HT,\ell}}$ under the no-interaction between direct and indirect effects assumption.


% For $c_{\ell1} + c_{\ell2} = 1$,
% \begin{equation}\label{Assumption2Estimator}
% \widehat{\delta^*}_{{HT,\ell}} =  c_{\ell1}[\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})] +
% c_{\ell2}[\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})]
% \end{equation} 

% \begin{lemma}\label{lemma4B}
% \begin{equation}\label{indirectass3A}
%     \widehat{\delta^{*}}_{HT,ind} =  \sum_{\ell =1}^{K}\widehat{\delta^{*}}_{HT,\ell}
% \end{equation}
% \end{lemma}

% \begin{proof}

% Under the no-interaction between direct and indirect effects assumption, %and for $c_{\ell1} + c_{\ell2} = 1$,
% %\begin{align*}
% %c_{\ell1}[\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})] =
% %c_{\ell2}[\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})]
% %\end{align*}

 
% \begin{align*}
% \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1}) =
% \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})
% = \widehat{\delta}_{{HT,\ell}}
% \end{align*}
% where $W^{*}_{\ell}\in\{0,1\}^K$ is a vector for the treatment assignment of the K-nearest neighbors with the first $\ell$ nearest neighbors are treated and the rest are control and $W^{*}_{\ell-1}\in\{0,1\}^K$ is the vector for the treatment assignment of the K-nearest neighbors where the first $\ell-1$ nearest neighbors are treated and the rest are control. 
% Since $c_{\ell1} + c_{\ell2} = 1$, the proof follows by lemma \ref{lemma2B}. 

% \end{proof}




% \subsubsection{The Expected Value of A$\ell$NNIE Estimator}
% \label{makereferenceB.8.1}


% \begin{multline} 
% \mathbf{E}(\widehat{\delta^*}_{{HT,\ell}}) =  \mathbf{E}(c_{\ell1}[\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})] +
% c_{\ell2}[\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})])
% \\ = \mathbf{E}(c_{\ell1}[\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})]) +
% \mathbf{E}(c_{\ell2}[\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})])
% \\= c_{\ell1}[\bar{y}(1,W^{*}_{\ell}) -  \bar{y}(1,W^{*}_{\ell-1})]
% + c_{\ell2}[\bar{y}(0,W^{*}_{\ell}) -  \bar{y}(0,W^{*}_{\ell-1})]
% = \delta_{\ell}
% \end{multline} 

% Hence, $\widehat{\delta^*}_{{HT,\ell}}$ is an unbiased estimator for $\delta_{\ell}$.


% \subsubsection{The Variance of A$\ell$NNIE Estimator}
% \label{makereferenceB.8.2}

% The variance can simply be computed using the property that $\mathbf{Var(\sum_{i = 1}^{N}a_{i}X_{i})} =\sum_{i=1}^{N}a_{i}\mathbf{Var(X_{i})} + 2\sum_{j\ne i}a_{i}a_{j}\mathbf{Cov}(X_{i}X_{j})$ as follows.




% \begin{multline}
% \label{varianceassumption2}
% \mathbf{Var}(\widehat{\delta^{*}}_{{HT,\ell}}) =  \mathbf{Var}(c_{\ell1}[\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})] +
% c_{\ell2}[\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})])
% \\ = \mathbf{Var}(c_{\ell1}\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - c_{\ell1}\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1}) +
% c_{\ell2}\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - c_{\ell2}\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1}))
% \\ = c^2_{\ell1}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})) + c^2_{\ell1}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})) +
% c^2_{\ell2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell})) + c^2_{\ell2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1}))
% \\ -2c^2_{\ell1} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}), \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})) 
% + 2c_{\ell1}c_{\ell2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}), \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell})) 
% \\ -2c_{\ell1}c_{\ell2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}), \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})) 
% -2c_{\ell1}c_{\ell2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1}), \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell})) 
% \\ + 2c_{\ell1}c_{\ell2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1}), \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})) 
% -2c^2_{\ell2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}), \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})) 
% \\ =  \frac{c^2_{\ell1}}{N^2}\sum_{i = 1}^{N}\pi_{i}(1,W^{*}_{\ell})[1 - \pi_{i}(1,W^{*}_{\ell})]\left[ \frac{y_{i}(1,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell})}\right]^2
% \\  + \frac{c^2_{\ell1}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(1,W^{*}_{\ell}) - \pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell})]\frac{y_{i}(1,W^{*}_{\ell})y_{j}(1,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell})}.
% \\ +  \frac{c^2_{\ell1}}{N^2}\sum_{i = 1}^{N}\pi_{i}(1,W^{*}_{\ell-1})[1 - \pi_{i}(1,W^{*}_{\ell-1})]\left[ \frac{y_{i}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell-1})}\right]^2
% \\  + \frac{c^2_{\ell1}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(1,W^{*}_{\ell-1}) - \pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(1,W^{*}_{\ell-1})]\frac{y_{i}(1,W^{*}_{\ell-1})y_{j}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(1,W^{*}_{\ell-1})}
% \\ +  \frac{c^2_{\ell2}}{N^2}\sum_{i = 1}^{N}\pi_{i}(0,W^{*}_{\ell})[1 - \pi_{i}(0,W^{*}_{\ell})]\left[ \frac{y_{i}(0,W^{*}_{\ell})}{\pi_{i}(0,W^{*}_{\ell})}\right]^2
% \\  + \frac{c^2_{\ell2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(0,W^{*}_{\ell}) - \pi_{i}(0,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell})]\frac{y_{i}(0,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell})}{\pi_{i}(0,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell})}.
% \\ +  \frac{c^2_{\ell2}}{N^2}\sum_{i = 1}^{N}\pi_{i}(0,W^{*}_{\ell-1})[1 - \pi_{i}(0,W^{*}_{\ell-1})]\left[ \frac{y_{i}(0,W^{*}_{\ell-1})}{\pi_{i}(0,W^{*}_{\ell-1})}\right]^2
% \\  + \frac{c^2_{\ell2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(0,W^{*}_{\ell-1}) - \pi_{i}(0,W^{*}_{\ell-1})\pi_{j}(0,W^{*}_{\ell-1})]\frac{y_{i}(0,W^{*}_{\ell-1})y_{j}(0,W^{*}_{\ell-1})}{\pi_{i}(0,W^{*}_{\ell-1})\pi_{j}(0,W^{*}_{\ell-1})}
% \\ - 2 \left( \frac{c^2_{\ell1}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,W^{*}_{\ell}),(1,W^{*}_{\ell-1})) - \pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell-1})\right] \frac{y_{i}(1,W^{*}_{\ell})y_{j}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell-1})}\right.
% \\ \left.- \frac{c^2_{\ell1}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,W^{*}_{\ell})y_{i}(1,W^{*}_{\ell-1})\right)
% \\ + 2 \left( \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,W^{*}_{\ell}),(0,W^{*}_{\ell})) - \pi_{i}(1,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell})\right] \frac{y_{i}(1,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell})}\right.
% \\ \left.- \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,W^{*}_{\ell})y_{i}(0,W^{*}_{\ell})\right)
% .
% \end{multline}



% \begin{multline}
% \\ - 2 \left( \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,W^{*}_{\ell}),(0,W^{*}_{\ell-1})) - \pi_{i}(1,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell-1})\right] \frac{y_{i}(1,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell-1})}\right.
% \\ \left.- \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,W^{*}_{\ell})y_{i}(0,W^{*}_{\ell-1})\right)
% \\ - 2 \left( \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,W^{*}_{\ell-1}),(0,W^{*}_{\ell})) - \pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(0,W^{*}_{\ell})\right] \frac{y_{i}(1,W^{*}_{\ell-1})y_{j}(0,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(0,W^{*}_{\ell})}\right.
% \\ \left.- \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,W^{*}_{\ell-1})y_{i}(0,W^{*}_{\ell})\right)
% \\ + 2 \left( \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,W^{*}_{\ell-1}),(0,W^{*}_{\ell-1})) - \pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(0,W^{*}_{\ell-1})\right] \frac{y_{i}(1,W^{*}_{\ell-1})y_{j}(0,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(0,W^{*}_{\ell-1})}\right.
% \\ \left.- \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,W^{*}_{\ell-1})y_{i}(0,W^{*}_{\ell-1})\right)
% \\ - 2 \left( \frac{c^2_{\ell2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((0,W^{*}_{\ell}),(0,W^{*}_{\ell-1})) - \pi_{i}(0,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell-1})\right] \frac{y_{i}(0,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell-1})}{\pi_{i}(0,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell-1})}\right.
% \\ \left.- \frac{c^2_{\ell2}}{N^2}\sum_{i = 1}^{N}  y_{i}(0,W^{*}_{\ell})y_{i}(0,W^{*}_{\ell-1})\right)
% \end{multline}






% \subsubsection{Weights in A$\ell$NNIE Estimator under the No Weak Interaction between Direct and Indirect Effects Assumption}
% \label{makereferenceB.8.3}
% Using Lagrange multiplier, we can find the values of $c_{\ell1}$ and $c_{\ell2}$ in \ref{Assumption2Estimator} that gives the minimum variance of A$\ell$NNIE Estimator under the no weak interaction between direct and indirect effects Assumption as follows.

% First, let $S^2_{11} = \mathbf{Var}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}))$, $S^2_{12} = \mathbf{Var}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1}))$,
% $S^2_{21} = \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}))$ and $S^2_{22} = \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1}))$ and assume that the covariance components in \ref{varianceassumption2} are equal to zero. 

% Then, we want to minimize the variance  

% \begin{equation}
% \mathbf{Var}(\widehat{\delta^{*}}_{{HT,\ell}})
% = \sum_{i = 1}^{2}c^2_{\ell i}(S^2_{i1} + S^2_{i2}) 
% \end{equation}

% subject to the constraint $\sum_{i = 1}^{2}c_{\ell i}= 1$

% Using the method of Lagrange multiplier, we minimize the function:

% \begin{multline}
% \phi = \mathbf{Var}(\widehat{\delta^{*}}_{{HT,\ell}})
% - \lambda(\sum_{i = 1}^{2}c^2_{\ell i} -1) 
% = \sum_{i = 1}^{2}c^2_{\ell i}(S^2_{i1} + S^2_{i2})  - \lambda(\sum_{i = 1}^{2}c^2_{\ell i} -1)
% \end{multline}

% Then, $\frac{\partial\phi}{\partial c_{\ell i}} = 0$ gives

% \begin{multline}\label{Ciwithlampda}
% \frac{\partial[\sum_{i = 1}^{2}c^2_{\ell i}(S^2_{i1} + S^2_{i2})  - \lambda(\sum_{i = 1}^{2}c^2_{\ell i} -1)}{\partial c_{\ell i}}] = 0
% \\ 2c_{\ell i}(S^2_{i1} + S^2_{i2}) - \lambda = 0
% \\ c_{\ell i} = \frac{\lambda}{2(S^2_{i1} + S^2_{i2})}
% \end{multline} 

% Now, we find Lagrange multiplier $\lambda$ as follows.

% \begin{multline}
% \sum_{i=1}^{2}c_{\ell i}  = 1 \Rightarrow \frac{\lambda}{2(S^2_{11} + S^2_{12})} + \frac{\lambda}{2(S^2_{21} + S^2_{22})} = 1 
% \\\Rightarrow \frac{\lambda}{2}\left(\frac{S^2_{11} + S^2_{12} + S^2_{21} + S^2_{22}}{(S^2_{11} + S^2_{12})(S^2_{21} + S^2_{22})}\right) = 1 
% \Rightarrow \lambda = \frac{2(S^2_{11} + S^2_{12})(S^2_{21} + S^2_{22})}{S^2_{11} + S^2_{12} + S^2_{21} + S^2_{22}}. 
% \end{multline}

% Substituting in \ref{Ciwithlampda}, we have

% \begin{equation}
% c_{\ell 1} = \frac{S^2_{21} + S^2_{22}}{S^2_{11} + S^2_{12} + S^2_{21} + S^2_{22}}  ,  c_{\ell 2} = \frac{S^2_{11} + S^2_{12}}{S^2_{11} + S^2_{12} + S^2_{21} + S^2_{22}},
% \end{equation}

% which give the minimum variance of A$\ell$NNIE estimator under the no weak interaction between direct and indirect effects assumption.




%%%%%%%%%%%%%%%%%%%%%%


\section{Additional simulation results}
\label{AppendixC}

We provide complete simulation results for all models of response given in Table 1.

\subsection{Estimates under complete randomization}
%%%%%%%  CRD
\begin{table}[h!]
  \caption{Estimates Under Completely Randomized Design Model 2}%
  \label{table3.3}
  \centering
  \begin{tabular}[c]{lccccc}\\
  %\hline
        Estimator & Effects & Emp.Estimates & Emp.Var & Emp.S.D. & Var Estimate \\
\hline
       $ \widehat{\delta}_{{HT,tot}}$ & 1 & 1.0707  & 1.306 & 1.1430 & 1.3314 \\
       $ \widehat{\delta^*}_{{HT,tot}}$ & 1 & 1.0707  & 1.306 & 1.1430 & 1.3314 \\
       $\widehat{\delta}_{{HT,dir}}$ & 1 & 1.0265  & 0.7273 & 0.8528 & 0.8050 \\
       $ \widehat{\delta^*}_{{HT,dir}}$ & 1 & 1.0300  & 0.3377 & 0.5812 &  0.6214\\
       $\widehat{\delta}_{{HT,ind}}$ & 0 & 0.0442  & 0.9404 & 0.9697 & 0.9393\\
       $ \widehat{\delta^*}_{{HT,ind}}$ & 0 & 0.0406  &  0.6081 & 0.7798 & 0.7381\\
       $\widehat{\delta}_{{HT,1^{st}}}$  & 0 & 0.0346  & 0.6273 & 0.7920 & 0.6821 \\
       $\widehat{\delta^*}_{{HT,1^{st}}}$ & 0 & 0.01758  & 0.3009 & 0.5486 & 0.4749  \\
        $\widehat{\delta}_{{HT,2^{nd}}}$ & 0 & -0.0111   & 0.4642 & 0.6813 & 0.6023 \\
       $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 0 & -0.0052  & 0.3328 & 0.5769 & 0.5601 \\
        $\widehat{\delta}_{{HT,3^{rd}}}$ & 0 & 0.0207  & 0.4541 & 0.6738 & 0.6076 \\
       $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 0 & 0.0282 & 0.3525 & 0.5937 & 0.5359\\
\hline
\end{tabular}
\end{table}




\begin{table}%[h!]
  \caption{Estimates Under Completely Randomized Design Model 3}%
  \label{table3.4}
  \centering
  \begin{tabular}[c]{lccccc}\\
 % \hline
        Estimator & Effects & Emp.Estimates & Emp.Var & Emp.S.D. & Var Estimate \\
\hline
       $ \widehat{\delta}_{{HT,tot}}$   & 4 &  4.0489  & 2.9225 & 1.7095 & 3.3552\\
       $ \widehat{\delta^*}_{{HT,tot}}$  & 4 &  4.0489 & 2.9225 & 1.7095 & 3.3552\\
       $\widehat{\delta}_{{HT,dir}}$  & 4 & 4.0047  & 2.2827 & 1.5108 & 2.5258  \\
       $ \widehat{\delta^*}_{{HT,dir}}$   & 4 & 4.0292  & 0.9267 & 0.9626 & 2.1155 \\
       $\widehat{\delta}_{{HT,ind}}$ & 4 & 0.0442  &  0.9404 & 0.9697 & 0.9393 \\
       $ \widehat{\delta^*}_{{HT,ind}}$   & 0 & 0.01978  & 1.1883 & 1.0901 & 1.7146 \\
       $\widehat{\delta}_{{HT,1^{st}}}$  & 0 &  0.0346  & 0.6273 & 0.7920 & 0.6821 \\
       $\widehat{\delta^*}_{{HT,1^{st}}}$ & 0 & 0.01286  &  0.8246 &  0.9081 & 1.4569 \\
        $\widehat{\delta}_{{HT,2^{nd}}}$ & 0 & -0.0111   & 0.4642 & 0.6813 & 0.6023 \\
       $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 0 & -0.01445  & 1.0133 & 1.0066 & 1.7389 \\
        $\widehat{\delta}_{{HT,3^{rd}}}$ & 0 & 0.0207  & 0.4541 & 0.6738 & 0.6076 \\
       $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 0 & 0.0213  & 1.0711 & 1.0349 & 1.6299 \\
\hline
\end{tabular}
\end{table}



\begin{table}%[h!]
  \caption{Estimates Under Completely Randomized Design Model 4}%
  \label{table3.5}
  \centering
  \begin{tabular}[c]{lccccc}\\
  %\hline
        Estimator & Effects & Emp.Estimates & Emp.Var & Emp.S.D. & Var Estimate \\
\hline
       $ \widehat{\delta}_{{HT,tot}}$ & 3.5 & 3.5526  & 2.5285 & 1.5901 &  2.8557\\
       $ \widehat{\delta^*}_{{HT,tot}}$ & 3.5 & 3.5526  & 2.5285 & 1.5901 &  2.8557\\
       $\widehat{\delta}_{{HT,dir}}$ & 0 & -0.0123  & 2.9637 & 1.7215 & 3.2323\\
       $ \widehat{\delta^*}_{{HT,dir}}$ & 0 & 0.0073  & 0.8913 & 0.9441 & 1.5485\\
       $\widehat{\delta}_{{HT,ind}}$ & 3.5 & 3.5649  & 1.5750 & 1.2550 & 2.3108 \\
       $ \widehat{\delta^*}_{{HT,ind}}$  & 3.5 & 3.5453  & 0.9523 & 0.9758 & 1.6828 \\
       $\widehat{\delta}_{{HT,1^{st}}}$  & 2 & 2.0414  & 0.8733 & 0.9345 & 1.1102 \\
       $\widehat{\delta^*}_{{HT,1^{st}}}$ & 2 & 2.0261  & 0.3901 &  0.6245 & 0.7582\\
        $\widehat{\delta}_{{HT,2^{nd}}}$  & 1 & 1.0003  & 1.4039 & 1.1848 & 1.9204\\
       $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 1 & 0.9961  & 0.7423 & 0.8615 & 1.4553 \\
        $\widehat{\delta}_{{HT,3^{rd}}}$ & 0.5 & 0.5230  & 1.8611 & 1.3642 &  2.5928 \\
       $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 0.5 & 0.5230 & 1.1157 & 1.0563 & 1.8726\\
\hline
\end{tabular}
\end{table}



\begin{table}%[H!]
  \caption{Estimates Under Completely Randomized Design Model 6}%
  \label{table3.7}
  \centering
  \begin{tabular}[c]{lccccc}\\
  %\hline
        Estimator & Effects & Emp.Estimates  & Emp.Var & Emp.S.D. & Var Estimate \\
\hline
       $ \widehat{\delta}_{{HT,tot}}$   & 7.5 & 7.5236  & 7.0767 & 2.6602 & 8.6687 \\
       $ \widehat{\delta^*}_{{HT,tot}}$  & 7.5 & 7.5236  & 7.0767 & 2.6602 & 8.6687 \\
       $\widehat{\delta}_{{HT,dir}}$  & 4 & 3.9586  & 7.9109 & 2.8126 & 8.1389\\
       $ \widehat{\delta^*}_{{HT,dir}}$ & 4 & 4.0061  & 2.3713 & 1.5399 & 4.4075\\
       $\widehat{\delta}_{{HT,ind}}$ & 3.5 & 3.5649  & 1.5750 & 1.2550 & 2.3108\\
       $ \widehat{\delta^*}_{{HT,ind}}$ & 3.5 & 3.5174  & 2.1777 & 1.4757 & 3.5289 \\
       $\widehat{\delta}_{{HT,1^{st}}}$  & 2 & 2.0414  & 0.8733 & 0.9345 & 1.1102 \\
       $\widehat{\delta^*}_{{HT,1^{st}}}$ & 2 & 2.0198  & 1.1942 & 1.0928 & 2.3348\\
        $\widehat{\delta}_{{HT,2^{nd}}}$  & 1 & 1.0003  & 1.4039 & 1.1848 & 1.9204\\
       $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 1 & 0.9838  & 2.1622 & 1.4704 & 4.1131\\
        $\widehat{\delta}_{{HT,3^{rd}}}$ & 0.5 & 0.5230  & 1.8611 & 1.3642 &  2.5928\\
       $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 0.5 & 0.5138  & 3.0275 & 1.7399 & 4.7936 \\
\hline
\end{tabular}
\end{table}



\begin{table}%[H!]
  \caption{Estimates Under Completely Randomized Design Model 7}%
  \label{table3.8}
  \centering
  \begin{tabular}[c]{lccccc}\\
 % \hline
        Estimator & Effects & Emp.Estimates & Emp.Var & Emp.S.D. & Var Estimate \\
\hline
       $ \widehat{\delta}_{{HT,tot}}$ & 6 & 6.0344 & 4.9972 & 2.2354 & 6.0021\\
       $ \widehat{\delta^*}_{{HT,tot}}$& 6 & 6.0344 & 4.9972 & 2.2354 & 6.0021 \\
       $\widehat{\delta}_{{HT,dir}}$ & 0 & -0.0452 & 7.4190 & 2.7237 & 7.9636 \\
       $ \widehat{\delta^*}_{{HT,dir}}$ & 0 & -0.0091 & 2.0143 & 1.4192 & 3.4718\\
       $\widehat{\delta}_{{HT,ind}}$ & 6 & 6.0797 & 2.7909 & 1.6705 & 4.850\\
       $ \widehat{\delta^*}_{{HT,ind}}$ & 6 & 6.0436 & 1.6675 & 1.2913 & 3.5938\\
       $\widehat{\delta}_{{HT,1^{st}}}$  & 3 &3.0449  & 1.1582 & 1.0762 & 1.6049\\
       $\widehat{\delta^*}_{{HT,1^{st}}}$ & 3 & 3.0295  & 0.5412 & 0.7357 & 1.1811\\
        $\widehat{\delta}_{{HT,2^{nd}}}$  & 2 &2.0092 & 2.8500 & 1.6882 & 3.9616\\
       $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 2 & 1.9961  & 1.4097 & 1.1873 & 2.9671\\
        $\widehat{\delta}_{{HT,3^{rd}}}$ & 1 &1.0256  & 4.5078 & 2.1231 & 6.1172\\
       $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 1 & 1.0179  & 2.6139 & 1.6167 & 4.4043\\
\hline
\end{tabular}
\end{table}


\begin{table}%[tbp]
  \caption{Estimates Under Completely Randomized Design Model 8}%
  \label{table3.9}
  \centering
  \begin{tabular}[c]{lccccc}\\
 % \hline
        Estimator & Effects & Emp.Estimates & Emp.Var & Emp.S.D. & Var Estimate \\
\hline
       $ \widehat{\delta}_{{HT,tot}}$   & 7 & 7.0272  & 6.3337 & 2.5166 & 7.7149 \\
       $ \widehat{\delta^*}_{{HT,tot}}$  & 7 & 7.0272  & 6.3337 & 2.5166 & 7.7149 \\
       $\widehat{\delta}_{{HT,dir}}$  & 1 & 0.9474  & 8.9409 & 2.9901 & 9.4071 \\
       $ \widehat{\delta^*}_{{HT,dir}}$   & 1 & 0.9905 & 2.4239 & 1.5568 & 4.1241 \\
       $\widehat{\delta}_{{HT,ind}}$ & 6 & 6.0797 & 2.7909 & 1.6705 & 4.8503\\
       $ \widehat{\delta^*}_{{HT,ind}}$ & 6 & 6.0366  & 1.9707 & 1.4038 & 4.0101\\
       $\widehat{\delta}_{{HT,1^{st}}}$  & 3 & 3.0449  & 1.1582 & 1.0762 & 1.6049\\
       $\widehat{\delta^*}_{{HT,1^{st}}}$ & 3 & 3.0280 & 0.6690 & 0.8179 & 1.4541\\
        $\widehat{\delta}_{{HT,2^{nd}}}$  & 2 & 2.0092 & 2.8500 & 1.6882 & 3.9616 \\
       $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 2 & 1.9930 & 1.7408 & 1.3194 & 3.6167\\
        $\widehat{\delta}_{{HT,3^{rd}}}$ & 1 & 1.0256  & 4.5078 & 2.1231 & 6.1172\\
       $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 1 & 1.0156 & 3.1489 & 1.7745 & 5.2252 \\
\hline
\end{tabular}
\end{table}
\newpage

%%%%%%%%%% BR


\subsection{Estimates under Bernoulli randomization}


\begin{table}[h!]
  \caption{Estimates Under Bernoulli Randomization Model 2}%
  \label{table2Ber}
  \centering
  \begin{tabular}[c]{lccccc}\\
  %\hline
        Estimator & Effects & Emp.Estimates  & Emp.Var & Emp.S.D. & Var Estimate \\
\hline
        $ \widehat{\delta}_{{HT,tot}}$   & 1 & 1.0326  & 1.4420 & 1.2008 & 1.4406 \\
       $ \widehat{\delta^*}_{{HT,tot}}$   & 1 & 1.0326 & 1.4420 & 1.2008 & 1.4406 \\
       $\widehat{\delta}_{{HT,dir}}$  & 1 & 1.0116  & 0.8786 & 0.9373 & 0.8960 \\
       $ \widehat{\delta^*}_{{HT,dir}}$   & 1 & 1.0277 & 0.3573 & 0.5978 & 0.6357  \\
       $\widehat{\delta}_{{HT,ind}}$ & 0 & 0.0210  & 0.8232 & 0.9073 & 0.9258 \\
       $ \widehat{\delta^*}_{{HT,ind}}$   & 0 & 0.0049  & 0.6462 & 0.8038 & 0.7994  \\
       $\widehat{\delta}_{{HT,1^{st}}}$  & 0 & 0.0502  & 0.6502 & 0.8063 & 0.6804 \\
       $\widehat{\delta^*}_{{HT,1^{st}}}$ & 0 & 0.0142 & 0.2858 & 0.5346 & 0.4783  \\
        $\widehat{\delta}_{{HT,2^{nd}}}$  & 0 & -0.0181  & 0.4430 & 0.6656 & 0.5985 \\
       $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 0 & -0.0177  & 0.3250 & 0.5700 & 0.5625  \\
        $\widehat{\delta}_{{HT,3^{rd}}}$ & 0 & -0.0110  & 0.4489 & 0.6700 & 0.6048 \\
       $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 0 & 0.0084  & 0.3846 & 0.6202 & 0.5497  \\
\hline
\end{tabular}
\end{table}





\begin{table}[tbp]
  \caption{Estimates Under Bernoulli Randomization Model 3}%
  \label{table3Ber}
  \centering
  \begin{tabular}[c]{lccccc}\\
  %\hline
        Estimator & Effects & Emp.Estimates  & Emp.Var & Emp.S.D. & Var Estimate \\
\hline
        $ \widehat{\delta}_{{HT,tot}}$   & 4 & 4.1129  & 4.0302 & 2.0075 & 4.5346  \\
       $ \widehat{\delta^*}_{{HT,tot}}$  & 4 & 4.1129  & 4.0302 & 2.0075 & 4.5346  \\
       $\widehat{\delta}_{{HT,dir}}$  & 4 & 4.0919 & 3.4654 & 1.8615 & 3.6069  \\
       $ \widehat{\delta^*}_{{HT,dir}}$   & 4 & 4.0797 & 0.9582 & 0.9789 & 2.2095  \\
       $\widehat{\delta}_{{HT,ind}}$ & 0 & 0.0210  & 0.8232 & 0.9073 & 0.9258 \\
       $ \widehat{\delta^*}_{{HT,ind}}$   & 0 & 0.0331 & 1.8081 & 1.3446 & 2.3816  \\
       $\widehat{\delta}_{{HT,1^{st}}}$  & 0 & 0.0502  & 0.6502 & 0.8063 & 0.6804 \\
       $\widehat{\delta^*}_{{HT,1^{st}}}$ & 0 & 0.0051 & 0.8466 & 0.9201 & 1.5260  \\
        $\widehat{\delta}_{{HT,2^{nd}}}$  & 0 & -0.0181 & 0.4430 & 0.6656 & 0.5985 \\
       $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 0 & -0.0456 & 1.0226 & 1.0112 & 1.7932  \\
        $\widehat{\delta}_{{HT,3^{rd}}}$ & 0 & -0.0110  & 0.4489 & 0.6700 & 0.6048 \\
       $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 0 & 0.0737  & 1.2557 & 1.1205 & 1.7099  \\
\hline
\end{tabular}
\end{table}



\begin{table}[tbp]
  \caption{Estimates Under Bernoulli Randomization Model 4}%
  \label{table4Ber}
  \centering
  \begin{tabular}[c]{lccccc}\\
  %\hline
        Estimator & Effects & Emp.Estimates & Emp.Var & Emp.S.D. & Var Estimate \\
\hline
        $ \widehat{\delta}_{{HT,tot}}$  & 3.5 & 3.5995  & 3.3899 & 1.8411 & 3.7707  \\
       $ \widehat{\delta^*}_{{HT,tot}}$  & 3.5 & 3.5995 & 3.3899 & 1.8411 & 3.7707  \\
       $\widehat{\delta}_{{HT,dir}}$  & 0 & 0.0745 & 3.1656 & 1.7792 & 3.4406  \\
       $ \widehat{\delta^*}_{{HT,dir}}$  & 0 & 0.0552 & 0.9018 & 0.9496 & 1.6046  \\
       $\widehat{\delta}_{{HT,ind}}$ & 3.5 & 3.5249  & 1.8276 & 1.3519 & 2.5207  \\
       $ \widehat{\delta^*}_{{HT,ind}}$   & 3.5 & 3.5443  & 1.5038 & 1.2262 & 2.1947  \\
       $\widehat{\delta}_{{HT,1^{st}}}$  & 2 & 2.0532  & 0.9882 & 0.9940 & 1.1739  \\
       $\widehat{\delta^*}_{{HT,1^{st}}}$ & 2 & 2.0206  & 0.3871 & 0.6222 & 0.7689  \\
        $\widehat{\delta}_{{HT,2^{nd}}}$  & 1 & 0.9878  & 1.4253 & 1.1938 & 2.0036  \\
       $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 1 & 0.9675 & 0.8350 & 0.9137 & 1.5577  \\
        $\widehat{\delta}_{{HT,3^{rd}}}$ & 0.5 & 0.4838 & 2.2543 & 1.5014 & 2.8237\\
       $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 0.5 & 0.5561  & 1.5671 & 1.2518 & 2.1355  \\
\hline
\end{tabular}
\end{table}






\begin{table}[tbp]
  \caption{Estimates Under Bernoulli Randomization Model 6}%
  \label{table6Ber}
  \centering
  \begin{tabular}[c]{lccccc}\\
  %\hline
        Estimator & Effects & Emp.Estimates & Emp.Var & Emp.S.D. & Var Estimate \\
\hline
        $ \widehat{\delta}_{{HT,tot}}$   & 7.5 & 7.7065 & 10.8531 & 3.2944 & 12.6623  \\
       $ \widehat{\delta^*}_{{HT,tot}}$  & 7.5 & 7.7065 & 10.8531 & 3.2944 & 12.6623  \\
       $\widehat{\delta}_{{HT,dir}}$  & 4 & 4.1815 & 10.0122 & 3.1642 & 10.4070 \\
       $ \widehat{\delta^*}_{{HT,dir}}$  & 4 & 4.1245 & 2.4463 & 1.5640 & 4.7047  \\
       $\widehat{\delta}_{{HT,ind}}$ & 3.5 & 3.5249 & 1.8276 & 1.3519 & 2.5207  \\
       $ \widehat{\delta^*}_{{HT,ind}}$   & 3.5 & 3.5819 & 4.4755 & 2.1155 & 5.7415  \\
       $\widehat{\delta}_{{HT,1^{st}}}$  & 2 & 2.0532 & 0.9882 & 0.9940 & 1.1739  \\
       $\widehat{\delta^*}_{{HT,1^{st}}}$ & 2 & 2.0084  & 1.1391 & 1.0672 & 2.3490  \\
        $\widehat{\delta}_{{HT,2^{nd}}}$  & 1 & 0.9878 & 1.4253 & 1.1938 & 2.0036 \\
       $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 1 & 0.9304 & 2.4611 & 1.5687 & 4.4098 \\
        $\widehat{\delta}_{{HT,3^{rd}}}$ & 0.5 & 0.4838  & 2.2543 & 1.5014 & 2.8237 \\
       $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 0.5 & 0.6431  & 4.0935 & 2.0232 & 5.3983  \\
\hline
\end{tabular}
\end{table}



\begin{table}[tbp]
  \caption{Estimates Under Bernoulli Randomization Model 7}%
  \label{table7Ber}
  \centering
  \begin{tabular}[c]{lccccc}\\
  %\hline
        Estimator & Effects & Emp.Estimates  & Emp.Var & Emp.S.D. & Var Estimate \\
\hline
        $ \widehat{\delta}_{{HT,tot}}$   & 6 & 6.1664 & 7.4275 & 2.7253 & 8.5832  \\
       $ \widehat{\delta^*}_{{HT,tot}}$  & 6 & 6.1664 & 7.4275 & 2.7253 & 8.5832  \\
       $\widehat{\delta}_{{HT,dir}}$  & 0 & 0.1386  & 7.8388 & 2.7997 & 8.5881  \\
       $ \widehat{\delta^*}_{{HT,dir}}$   & 0 & 0.0872 & 2.0487 & 1.4313 & 3.6387  \\
       $\widehat{\delta}_{{HT,ind}}$ & 6 & 6.0277 & 3.5973 & 1.8966 & 5.4461  \\
       $ \widehat{\delta^*}_{{HT,ind}}$   & 6 & 6.0791  & 3.2309 & 1.7974 & 5.0241  \\
       $\widehat{\delta}_{{HT,1^{st}}}$ & 3 & 3.0547  & 1.3709 & 1.1708 & 1.7503  \\
       $\widehat{\delta^*}_{{HT,1^{st}}}$ & 3 & 3.0223  & 0.5510 & 0.7423 & 1.2101  \\
        $\widehat{\delta}_{{HT,2^{nd}}}$  & 2 & 1.9923 & 2.9090 & 1.7056 & 4.1353 \\
       $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 2 & 1.9521 & 1.6716 & 1.2929 & 3.2181  \\
        $\widehat{\delta}_{{HT,3^{rd}}}$ & 1 & 0.9806  & 5.3764 & 2.3187 & 6.7556  \\
       $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 1 & 1.1047 & 3.8051 & 1.9506 & 5.1576 \\
\hline
\end{tabular}
\end{table}






\begin{table}[tbp]
  \caption{Estimates Under Bernoulli Randomization Model 8}%
  \label{table8Ber}
  \centering
  \begin{tabular}[c]{lccccc}\\
  %\hline
        Estimator & Effects & Emp.Estimates & Emp.Var & Emp.S.D. & Var Estimate \\
\hline
        $ \widehat{\delta}_{{HT,tot}}$   & 7 & 7.1931 & 9.6277 & 3.1028 & 11.2033  \\
       $ \widehat{\delta^*}_{{HT,tot}}$  & 7 & 7.1931 & 9.6277 & 3.1028 & 11.2033  \\
       $\widehat{\delta}_{{HT,dir}}$ & 1 & 1.1654 & 9.7750 & 3.1265 & 10.5230  \\
       $ \widehat{\delta^*}_{{HT,dir}}$   & 1 & 1.1045 & 2.4792 & 1.5745 & 4.3616  \\
       $\widehat{\delta}_{{HT,ind}}$ & 6 & 6.0277 & 3.5973 & 1.8966 & 5.4461  \\
       $ \widehat{\delta^*}_{{HT,ind}}$   & 6 & 6.0885 & 4.0546 & 2.0136 & 5.9402 \\
       $\widehat{\delta}_{{HT,1^{st}}}$ & 3 & 3.0547 & 1.3709 & 1.1708 & 1.7503  \\
       $\widehat{\delta^*}_{{HT,1^{st}}}$ & 3 & 3.0192 & 0.6476 & 0.8047 & 1.4636  \\
        $\widehat{\delta}_{{HT,2^{nd}}}$ & 2 & 1.9923 & 2.9090 & 1.7056 & 4.1353  \\
       $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 2 & 1.9428 & 2.0746 & 1.4403 & 3.9275  \\
        $\widehat{\delta}_{{HT,3^{rd}}}$ & 1 & 0.9806 & 5.3764 & 2.3187 & 6.7556  \\
       $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 1 & 1.1264 & 4.5503 & 2.1331 & 6.0993 \\
\hline
\end{tabular}
\end{table}















% \bigskip
% \begin{center}
% {\large\bf SUPPLEMENTARY MATERIAL}
% \end{center}

% \begin{description}

% \item[Title:] Brief description. (file type)

% \item[R-package for  MYNEW routine:] R-package ÒMYNEWÓ containing code to perform the diagnostic methods described in the article. The package also contains all datasets used as examples in the article. (GNU zipped tar file)

% \item[HIV data set:] Data set used in the illustration of MYNEW method in Section~ 3.2. (.txt file)

% \end{description}

% \section{BibTeX}

% We hope you've chosen to use BibTeX!\ If you have, please feel free to use the package natbib with any bibliography style you're comfortable with. The .bst file agsm has been included here for your convenience. 


% \begin{table}
% \caption{D-optimality values for design $X$ under five different scenarios.  \label{tab:tabone}}
% \begin{center}
% \begin{tabular}{rrrrr}
% one & two & three & four & five\\\hline
% 1.23 & 3.45 & 5.00 & 1.21 & 3.41 \\
% 1.23 & 3.45 & 5.00 & 1.21 & 3.42 \\
% 1.23 & 3.45 & 5.00 & 1.21 & 3.43 \\
% \end{tabular}
% \end{center}
% \end{table}

% \begin{itemize}
% \item Note that figures and tables (such as Figure~\ref{fig:first} and
% Table~\ref{tab:tabone}) should appear in the paper, not at the end or
% in separate files.
% \item In the latex source, near the top of the file the command
% \verb+\newcommand{\blind}{1}+ can be used to hide the authors and
% acknowledgements, producing the required blinded version.
% \item Remember that in the blind version, you should not identify authors
% indirectly in the text.  That is, don't say ``In Smith et. al.  (2009) we
% showed that ...''.  Instead, say ``Smith et. al. (2009) showed that ...''.
% \item These points are only intended to remind you of some requirements.
% Please refer to the instructions for authors
% at \url{http://amstat.tandfonline.com/action/authorSubmission?journalCode=uasa20&page=instructions#.VFkk7fnF_0c}
% \item For more about ASA\ style, please see \url{http://journals.taylorandfrancis.com/amstat/asa-style-guide/}
% \item If you have supplementary material (e.g., software, data, technical
% proofs), identify them in the section below.  In early stages of the
% submission process, you may be unsure what to include as supplementary
% material.  Don't worry---this is something that can be worked out at later stages.
% \end{itemize}


















% %%%%%%%%%%%%%%%%%%%%%    Delete This %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %MIKE: Need to fix this.
% \section{ Appendix A: Properties of Horvitz Thompson Estimator of the Average Potential Outcomes}
% \label{AppendixA}
% %\chapter{Properties of Horvitz Thompson Estimator of the Average Potential Outcomes under any Exposure}



% Here, we derive properties of the Horvitz Thompson estimator of the average potential outcomes under exposure $(W,\mathbf{W}_{\mathcal{N}_{K}})$.

% First, the Horvitz Thompson estimator of the total potential outcomes of units under exposure $(W,\mathbf{W}_{\mathcal{N}_{K}})$ is 
% \begin{equation} 
% \widehat{y^{T}_{HT}}(W,\mathbf{W}_{\mathcal{N}_{K}}) =  \sum_{i = 1}^{N} I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}}) \frac{Y_i}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}.
% \end{equation}

% The unbiased Horvitz Thompson estimator of the average potential outcomes of units under exposure $(W,\mathbf{W}_{\mathcal{N}_{K}})$ is
% \begin{equation} 
% \bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}) =  \frac{1}{N}\sum_{i = 1}^{N} I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}}) \frac{Y_i}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}.
% \end{equation}
% where $I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})$ is the inclusion indicator of unit i which is the only stochastic component of the expression. 
% Hence, $I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})$ is a Bernoulli random variable with $\mathbf{E}[I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})]  = \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})$, $\mathbf{Var}(I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}}))$  = $\mathbf{Cov}(I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}}),I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})) = \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})(1 - \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}}))$
%   and
% $\mathbf{Cov}(I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}}),I_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})) = (\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}})- \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}}))$
% where $\mathbf{E}[I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})]  = \pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}})$ is the inclusion probability of units $i$ and $j$.
% \subsection{The Expected value of Horvitz Thompson Estimator}
% \label{makereferenceA.1}

% The expected value of $\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})$ is

% \begin{multline}
% \mathbf{E}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})) 
% = \mathbf{E}\left[\frac{1}{N}\sum_{i = 1}^{N} I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}}) \frac{Y_i}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right]
% \\= \frac{1}{N}\sum_{i = 1}^{N} \mathbf{E}\left[I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\right] \frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}
% \\= \frac{1}{N}\sum_{i = 1}^{N} \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}}) \frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}
% \\=  \frac{1}{N}\sum_{i = 1}^{N} y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\\ 
% = \bar{y}(W,\mathbf{W}_{\mathcal{N}_{K}}).\\
% \end{multline}





% \subsection{The Variance of Horvitz Thompson Estimator}
% \label{makereferenceA.2}



% Recall the property that $\mathbf{Var}(X)$ = $\mathbf{E}(X^2)$ - $(\mathbf{E}(X))^2$ and $(\sum_{i = 1}^{N}a_{i}X_{i})^2 = \sum_{i = 1}^{N}a^2_{i}X^2_{i}$ + $\sum_{i = 1}^{N}\sum_{j \ne i}a_{i}a_{j}X_{i}X_{j}$ and note that $\mathbf{E}[I^2_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})] = \mathbf{E}[I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})]  = \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})$ and $\pi_{i}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W',\mathbf{W}'_{\mathcal{N}_{K}})) = 0$ since unit i cannot be exposed to two exposures at the same time.



% \begin{multline}
% \left(\mathbf{E}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})\right)^2 =  \left(\mathbf{E}\left(\frac{1}{N}\sum_{i = 1}^{N} I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}}) \frac{Y_i}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right)\right)^2
% \\ = \left(\frac{1}{N}\sum_{i = 1}^{N} \mathbf{E}\left(I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\right) \frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right)^2
% \\ = \frac{1}{N^2}\left(\sum_{i = 1}^{N}  y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\right)^2\\
% \\ = \frac{1}{N^2}\sum_{i = 1}^{N}  y^2_{i}(W,\mathbf{W}_{\mathcal{N}_{K}}) + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})\\
% \end{multline}





% \begin{multline}
% \mathbf{E}((\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}))^2) =  \mathbf{E}\left[\left(\frac{1}{N}\sum_{i = 1}^{N} I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}}) \frac{Y_i}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right)^2\right]
% \\ = \mathbf{E}\left[\left(\frac{1}{N^2}\sum_{i = 1}^{N} I^2_{i}(W,\mathbf{W}_{\mathcal{N}_{K}}) \frac{Y^2_i}{\pi^2_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right)\right.
% \\ \left.+ \left(\frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{j}(W,\mathbf{W}_{\mathcal{N}_{K}}) \frac{Y_iY_j}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right)\right]
% \\ = \mathbf{E}\left[\left(\frac{1}{N^2}\sum_{i = 1}^{N} I^2_{i}(W,\mathbf{W}_{\mathcal{N}_{K}}) \frac{Y^2_i}{\pi^2_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right)\right]
% \\ + \mathbf{E}\left[\left(\frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{j}(W,\mathbf{W}_{\mathcal{N}_{K}}) \frac{Y_iY_j}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right)\right]
% \\ = \left(\frac{1}{N^2}\sum_{i = 1}^{N} \mathbf{E}[I^2_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})] \frac{y^2_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi^2_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right)
% \\ + \left(\frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \mathbf{E}[I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})] \frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right)
% \\ = \frac{1}{N^2}\sum_{i = 1}^{N} \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}}) \frac{y^2_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi^2_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}
% \\ + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) \frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})}
% \\ = \frac{1}{N^2}\sum_{i = 1}^{N} \frac{y^2_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}
% + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) \frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})}
% \end{multline}







% Hence, the variance of $\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})$ is


% \begin{multline}\label{HTmeanvariance}
% \mathbf{Var}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}))
% =\mathbf{E}((\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}))^2) 
% - \left(\mathbf{E}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})\right)^2
% \\ = \frac{1}{N^2}\sum_{i = 1}^{N} \frac{y^2_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}
%  + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) \frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})}
%  \\ - \frac{1}{N^2}\sum_{i = 1}^{N}  y^2_{i}(W,\mathbf{W}_{\mathcal{N}_{K}}) - \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})\\
% \\ =  \frac{1}{N^2}\sum_{i = 1}^{N}\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})[1 - \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})]\left[ \frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right]^2
% \\  + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) - \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})]\frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})}.
% \end{multline}





% \subsection{The Covariance between two Horvitz Thompson Estimators of the Averages}
% \label{makereferenceA.3}


% The covariance between the averages of potential outcomes under any two exposures to treatments $(W,\mathbf{W}_{\mathcal{N}_{K}})$ and $(W',\mathbf{W}'_{\mathcal{N}_{K}})$  using the property that $\mathbf{Cov}(X,Y)$ = $\mathbf{E}(XY) - \mathbf{E}(X)\mathbf{E}(Y)$ is as follows: %$\mathbf{Cov}(\bar{Y}_{HT}^{obs}(W_{i},W^{*}_{\ell}),\bar{Y}_{HT}^{obs}(W'_{i},W^{*'}_{\ell}))$ = $\mathbf{E}(\bar{Y}_{HT}^{obs}(W_{i},W^{*}_{\ell})\bar{Y}_{HT}^{obs}(W'_{i},W^{*'}_{\ell})) - \mathbf{E}(\bar{Y}_{HT}^{obs}(W_{i},W^{*}_{\ell}))\mathbf{E}(\bar{Y}_{HT}^{obs}(W'_{i},W^{*'}_{\ell}))$ as follows:

% Note the following expectations: 


% \begin{multline}
% \mathbf{E}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})\bar{Y}_{HT}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}})))=
% \\ \mathbf{E}\left[\left(\frac{1}{N}\sum_{i = 1}^{N} I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}}) \frac{Y_{i}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right)
% \left(\frac{1}{N}\sum_{i = 1}^{N} I_{i}(W', \mathbf{W}'_{\mathcal{N}_{K}})) \frac{Y_{i}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}{\pi_{i}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}\right)\right]
% \\ = \mathbf{E}\left[\left(\frac{1}{N^2}\sum_{i = 1}^{N} I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{i}(W', \mathbf{W}'_{\mathcal{N}_{K}})) \frac{Y_{i}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})Y_{i}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{i}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}\right)\right.
% \\ \left.+ \left(\frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}})) \frac{Y_{i}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})Y_{j}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}\right)\right]
% \\ = \mathbf{E}\left[\left(\frac{1}{N^2}\sum_{i = 1}^{N} I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{i}(W', \mathbf{W}'_{\mathcal{N}_{K}})) \frac{Y_{i}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})Y_{i}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{i}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}\right)\right]
% \\ + \mathbf{E}\left[\left(\frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}})) \frac{Y_{i}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})Y_{j}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}\right)\right]
% \\ = \left(\frac{1}{N^2}\sum_{i = 1}^{N} \mathbf{E}\left[I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{i}(W', \mathbf{W}'_{\mathcal{N}_{K}}))\right] \frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{i}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{i}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}\right)
% \\ + \left(\frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \mathbf{E}\left[I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}}))\right] \frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{i}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}\right)
% \\ = \frac{1}{N^2}\sum_{i = 1}^{N} \pi_{i}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W', \mathbf{W}'_{\mathcal{N}_{K}}))) \frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{i}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{i}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}
% \\ + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W', \mathbf{W}'_{\mathcal{N}_{K}}))) \frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}
% \\ = \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W', \mathbf{W}'_{\mathcal{N}_{K}}))) \frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}
% \end{multline}

% % \begin{multline}
% % \mathbf{E}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})\bar{Y}_{HT}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}}))=
% % \\ \mathbf{E}\left[\left(\frac{1}{N}\sum_{i = 1}^{N} I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}}) \frac{Y_i(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right)
% % \left(\frac{1}{N}\sum_{i = 1}^{N} I_{i}(W',\mathbf{W}'_{\mathcal{N}_{K}}) \frac{Y_{i}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}})}{\pi_{i}(W',\mathbf{W}'_{\mathcal{N}_{K}})}\right)\right]
% % \\ = \mathbf{E}\left[\left(\frac{1}{N^2}\sum_{i = 1}^{N} I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{i}(W',\mathbf{W}'_{\mathcal{N}_{K}}) \frac{Y_i(W,\mathbf{W}_{\mathcal{N}_{K}})Y_i(W',\mathbf{W}'_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{i}(W',\mathbf{W}'_{\mathcal{N}_{K}})}\right)\right.
% % \\ \left.+ \left(\frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}}) \frac{Y_iY_{j}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})}\right)\right]
% % \\ = \mathbf{E}\left[\left(\frac{1}{N^2}\sum_{i = 1}^{N} I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{i}(W',\mathbf{W}'_{\mathcal{N}_{K}}) \frac{Y_iY_i}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{i}(W',\mathbf{W}'_{\mathcal{N}_{K}})}\right)\right]
% % \\ + \mathbf{E}\left[\left(\frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}}) \frac{Y_iY_{j}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})}\right)\right]
% % \\ = \left(\frac{1}{N^2}\sum_{i = 1}^{N} \mathbf{E}\left[I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{i}(W',\mathbf{W}'_{\mathcal{N}_{K}})\right] \frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{i}(W',\mathbf{W}'_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{i}(W',\mathbf{W}'_{\mathcal{N}_{K}})}\right)
% % \\ + \left(\frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \mathbf{E}\left[I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})\right] \frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{i}(W',\mathbf{W}'_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})}\right)
% % \\ = \frac{1}{N^2}\sum_{i = 1}^{N} \pi_{i}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W',\mathbf{W}'_{\mathcal{N}_{K}})) \frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{i}(W',\mathbf{W}'_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{i}(W',\mathbf{W}'_{\mathcal{N}_{K}})}
% % \\ + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W',\mathbf{W}'_{\mathcal{N}_{K}})) \frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})}
% % \\ = \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W',\mathbf{W}'_{\mathcal{N}_{K}})) \frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})}
% % \end{multline}





% %\begin{multline}
% %\mathbf{E}([\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})]^2) =
% %\\ \mathbf{E}\left(\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\right)^2 + \left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})\right)^2 
% % - 2\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\right)\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})\right)\right)
% %\\= \mathbf{E}\left(\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\right)^2\right) +  \mathbf{E}\left(\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})\right)^2\right) 
% % - 2\mathbf{E}\left(\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\right)\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})\right)\right)
% % \\ = \frac{1}{N^2}\sum_{i = 1}^{N} \frac{y^2_{i}(1,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell})}
% %+ \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}(1,W^{*}_{\ell}) \frac{y_{i}(1,W^{*}_{\ell})y_{j}(1,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell})}
% % \\ + \frac{1}{N^2}\sum_{i = 1}^{N} \frac{y^2_{i}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell-1})}
% %+ \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}(1,W^{*}_{\ell-1}) \frac{y_{i}(1,W^{*}_{\ell-1})y_{j}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(1,W^{*}_{\ell-1})}
% %\\ -2 \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((1,W^{*}_{\ell}),(1,W^{*}_{\ell-1})) \frac{y_{i}(1,W^{*}_{\ell})y_{j}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell-1})}
% %\end{multline}


% % \begin{multline}
% % \mathbf{E}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}))\mathbf{E}(\bar{Y}_{HT}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}})) = 
% % \\ \mathbf{E}\left[\frac{1}{N}\sum_{i = 1}^{N} I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}}) \frac{Y_i}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right]
% % \mathbf{E}\left[\frac{1}{N}\sum_{i = 1}^{N} I_{i}(W',\mathbf{W}'_{\mathcal{N}_{K}}) \frac{Y_{i}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}})}{\pi_{i}(W',\mathbf{W}'_{\mathcal{N}_{K}})}\right]
% % \\= \left[\frac{1}{N}\sum_{i = 1}^{N}  y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\right]
% % \left[\frac{1}{N}\sum_{i = 1}^{N}  y_{i}(W',\mathbf{W}'_{\mathcal{N}_{K}})\right]
% % \\ = \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{i}(W',\mathbf{W}'_{\mathcal{N}_{K}}) + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})
% % \end{multline}



% % \begin{multline}\label{Cov1}
% % \mathbf{Cov}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}})) = \mathbf{E}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})\bar{Y}_{HT}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}})) 
% % \\- \mathbf{E}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}))\mathbf{E}(\bar{Y}_{HT}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}}))
% % \\ = \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W',\mathbf{W}'_{\mathcal{N}_{K}})) \frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})}
% % \\ - \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{i}(W',\mathbf{W}'_{\mathcal{N}_{K}}) - \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})
% % \\ = \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W',\mathbf{W}'_{\mathcal{N}_{K}})) - \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})\right] \frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})} 
% % \\ - \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{i}(W',\mathbf{W}'_{\mathcal{N}_{K}})
% % .
% % \end{multline}


% \begin{multline}
% \mathbf{E}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}))\mathbf{E}(\bar{Y}_{HT}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}}))) = 
% \\ \mathbf{E}\left[\frac{1}{N}\sum_{i = 1}^{N} I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}}) \frac{Y_{i}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right]
% \mathbf{E}\left[\frac{1}{N}\sum_{i = 1}^{N} I_{i}(W', \mathbf{W}'_{\mathcal{N}_{K}})) \frac{Y_{i}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}{\pi_{i}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}\right]
% \\= \left[\frac{1}{N}\sum_{i = 1}^{N}  y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\right]
% \left[\frac{1}{N}\sum_{i = 1}^{N}  y_{i}(W', \mathbf{W}'_{\mathcal{N}_{K}}))\right]
% \\ = \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{i}(W', \mathbf{W}'_{\mathcal{N}_{K}})) + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}}))
% \end{multline}



% \begin{multline}\label{Cov1}
% \mathbf{Cov}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}}))) = \mathbf{E}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})\bar{Y}_{HT}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}}))) 
% \\- \mathbf{E}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}))\mathbf{E}(\bar{Y}_{HT}^{obs}(W', \mathbf{W}'_{\mathcal{N}_{K}})))
% \\ = \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W', \mathbf{W}'_{\mathcal{N}_{K}}))) \frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}
% \\ - \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{i}(W', \mathbf{W}'_{\mathcal{N}_{K}})) - \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}}))
% \\ = \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W', \mathbf{W}'_{\mathcal{N}_{K}}))) - \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}}))\right] \frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}}))}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W', \mathbf{W}'_{\mathcal{N}_{K}}))} 
% \\ - \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{i}(W', \mathbf{W}'_{\mathcal{N}_{K}}))
% .
% \end{multline}


% \subsection{Estimation of the Variance}
% \label{makereferenceA.4}\
% There are two conditions under a measurable design: $\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}}) > 0 $ and $(\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) > 0$. 
% Non-measurable designs are ones that do not meet one of the these two conditions.
% The Horvitz Thompson %and Sen-Yates-Grundy (SYG)% 
% variance estimator of  $\mathbf{Var}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}))$ is provided as follows:

% \begin{multline}
% \widehat{\mathbf{Var}}_{HT}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})) = \frac{1}{N^2}\widehat{\mathbf{Var}}_{HT}{(\widehat{y^{T}_{HT}}(W,\mathbf{W}_{\mathcal{N}_{K}}))}
% \\ =  \frac{1}{N^2}\sum_{i \in U}I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})[1 - \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})]\left[ \frac{Y_i}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right]^2
% \\  + \frac{1}{N^2}\sum_{i \in U}\sum_{j \in U, j \ne i}I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})\frac{[\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) - \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})]}{\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}})}
% \\ \times \frac{Y_iY_j}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})}.
% \end{multline}

% %and and Sen-Yates-Grundy (SYG) variance estimator is 

% %\begin{multline}
% %\widehat{\mathbf{Var}_{SYG}}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})) = \frac{1}{N^2}\widehat{\mathbf{Var}_{{SYG}}}{(\widehat{y^{T}_{HT}}(W,\mathbf{W}_{\mathcal{N}_{K}}))}
% %\\ =  \frac{1}{2}\frac{1}{N^2}\sum_{i \in U}\sum_{j \in U, j \ne i}I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})\frac{ \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}}) - \pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}})}
% %\\ \times \left[\frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})} - \frac{y_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right]^2.
% %\end{multline}

% Under measurable designs,i.e., the joint probabilities $\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) > 0$ for all $i$ and $j$, and  this estimated variance is unbiased.
% However, under non measurable designs when $\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) = 0$ for some $i$ and $j$, then this estimated variance will be biased.

% Let's re-express the variance in equation \ref{HTmeanvariance} as follows:


% \begin{multline}
% \mathbf{Var}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}))
% =  \frac{1}{N^2}\sum_{i = 1}^{N}\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})[1 - \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})]\left[ \frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right]^2
% \\  + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \in U, j \ne i:\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) > 0 }[\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) - \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})]\frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})}
% \\-  \sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) = 0 } y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W,\mathbf{W}_{\mathcal{N}_{K}}).
% \end{multline}

% If $\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) = 0$ for some $i$ and $j$, then

% \begin{multline}\label{ExpectedofVARHAT}
%    \mathbf{E}(\widehat{\mathbf{Var}}_{HT}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}))) =  \mathbf{E}\left(\frac{1}{N^2}\sum_{i \in U}I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})[1 - \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})]\left[ \frac{Y_i}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right]^2\right.
% \\ \left. + \frac{1}{N^2}\sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) > 0 }I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})\frac{[\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) - \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})]}{\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right.
% \\ \left.\times \frac{Y_iY_j}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right) 
% \\=\mathbf{Var}_{HT}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})) + \sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) = 0 } y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W,\mathbf{W}_{\mathcal{N}_{K}}).
% \\=\mathbf{Var}_{HT}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})) + A.
% \end{multline}

% where $A = \sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) = 0 } y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})$ and we can never observe $y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})$ and $y_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})$ together since $\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) = 0$.

% As derived in \citet{aronow2013conservative, aronow2017estimating}, we have the following variance bias correction,


% \begin{multline}
% \widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})) = \widehat{\mathbf{Var}}_{HT}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})) + \widehat{A^{*}}(W,\mathbf{W}_{\mathcal{N}_{K}})
% \\ = \frac{1}{N^2}\sum_{i \in U}I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})[1 - \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})]\left[ \frac{Y_i}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right]^2
% \\  + \frac{1}{N^2}\sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) > 0 }I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})\frac{[\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) - \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})]}{\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}})}
% \\ \times \frac{Y_iY_j}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})} 
% \\+ \frac{1}{N^2}\sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) = 0 }
% \left[\frac{I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})Y^2_i}{2\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right.
%  \left.+ \frac{I_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})Y^2_j}{2\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right]
% \end{multline}

% where
% \begin{multline}
% \widehat{A^{*}}(W,\mathbf{W}_{\mathcal{N}_{K}}) = \frac{1}{N^2}\sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) = 0 }
% \left[\frac{I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})Y^2_i}{2\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right.
% \\ \left.+ \frac{I_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})Y^2_j}{2\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right]
% \end{multline}

% Then, $\widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}))$ is a conservative estimator for the variance of Horvitz Thompson estimator of the average potential outcomes under exposure $(W,\mathbf{W}_{\mathcal{N}_{K}})$.
% \begin{equation}\label{VARHATA}
%   \mathbf{E}(\widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}))  \geq \mathbf{Var}_{HT}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}))
% \end{equation}
 
% \begin{proof}
% First, for simplicity, let $I_{i} = I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})$, $I_{j} = I_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})$, $\pi_{i} = \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})$, $\pi_{j} = \pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})$, $\pi_{ij} = \pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}})$,
% % $Y_{i} = Y_i$ and $Y_{j} = Y_{j}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})$ 
% and instead of the average, consider the variance bias correction of the total as follows,


% \begin{align*}
% \widehat{\mathbf{Var}}_{C}(\widehat{y^{T}_{HT}}(W,\mathbf{W}_{\mathcal{N}_{K}}))  = \sum_{i \in U}I_{i}[1 - \pi_{i}]\left[ \frac{Y_{i}}{\pi_{i}}\right]^2
%  + \sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij} > 0 }I_{i}I_{j}\frac{[\pi_{ij} - \pi_{i}\pi_{j}]}{\pi_{ij}}
% \times \frac{Y_{i}Y_{j}}{\pi_{i}\pi_{j}} 
% \\+ \sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij} = 0 }
% \left[I_{i}\frac{\lvert Y_{i}\rvert^{a_{ij}}}{a_{ij}\pi_{i}}\right.
%  \left.+ I_{j}\frac{\lvert Y_{j}\rvert^{b_{ij}}}{b_{ij}\pi_{j}}\right]
% \end{align*}

% where $a_{ij}$ and $b_{ij}$ are positive real numbers such that $\frac{1}{a_{ij}} + \frac{1}{b_{ij}} = 1$ for all pairs i and $j$ with $\pi_{ij}$ = 0.

% By Young’s inequality, if $\frac{1}{a_{ij}} + \frac{1}{b_{ij}} = 1$,

% \begin{align*}
% \frac{\lvert y_{i}\rvert^{a_{ij}}}{a_{ij}}
%  + \frac{\lvert y_{j}\rvert^{b_{ij}}}{b_{ij}} \geq \lvert y_{i}\rvert \lvert y_{j}\rvert .
%  \end{align*}

%  Define $A^{*}$ such that,
%  \begin{align*}
%  A^{*} = \sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij} = 0 } \frac{\lvert y_{i}\rvert^{a_{ij}}}{a_{ij}}
%  + \frac{\lvert y_{j}\rvert^{b_{ij}}}{b_{ij}} \geq \sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij} = 0 }\lvert y_{i}\rvert \lvert y_{j}\rvert \geq \sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij} = 0 } y_{i}y_{j} = A
%  \end{align*}

% and
% \begin{align*}
% A^{*}\geq \sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij} = 0 }\lvert y_{i}\rvert \lvert y_{j}\rvert \geq \sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij} = 0 } -y_{i}y_{j} = -A.
% \end{align*}

% Therefore, 
% \begin{align*}
% \mathbf{Var}_{HT}(\widehat{y^{T}_{HT}}(W,\mathbf{W}_{\mathcal{N}_{K}})) + A + A^{*} \geq \mathbf{Var}_{HT}(\widehat{y^{T}_{HT}}(W,\mathbf{W}_{\mathcal{N}_{K}}))
% \end{align*}

% and the associated Horvitz Thompson estimator of $A^{*}$ is 
% \begin{align*}
% \widehat{A^{*}} = \sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij} = 0 }
% \left[I_{i}\frac{\lvert Y_{i}\rvert^{a_{ij}}}{a_{ij}\pi_{i}}\right.
%  \left.+ I_{j}\frac{\lvert Y_{j}\rvert^{b_{ij}}}{b_{ij}\pi_{j}}\right]
% \end{align*}
% where unbiasedness of $\widehat{A^{*}}$ follows by  $\mathbf{E}(I_{i})  = \pi_{i}$ and $\mathbf{E}(I_{j})  = \pi_{j}$.
% By equation \ref{ExpectedofVARHAT} and by $\mathbf{E}(\widehat{A^{*}})  = A^{*}$, 

% \begin{align*}
%    \mathbf{E}(\widehat{\mathbf{Var}}_{HT}(\widehat{y^{T}_{HT}}(W,\mathbf{W}_{\mathcal{N}_{K}})) + \widehat{A^{*}}) = \mathbf{Var}_{HT}(\widehat{y^{T}_{HT}}(W,\mathbf{W}_{\mathcal{N}_{K}})) + A + A^{*}.
% \end{align*}

% Hence, 
% \begin{align*}
%    \mathbf{E}(\widehat{\mathbf{Var}}_{C}(\widehat{y^{T}_{HT}}(W,\mathbf{W}_{\mathcal{N}_{K}}))) \geq \mathbf{Var}_{HT}(\widehat{y^{T}_{HT}}(W,\mathbf{W}_{\mathcal{N}_{K}})).
% \end{align*}

% As a special case, assigning all $a_{ij}= b_{ij} = 2$ such that $\frac{1}{2} + \frac{1}{2} = 1$ for all pairs i and $j$ with $\pi_{ij}$ = 0 where

% \begin{align*}
%  A^{*} = \sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij} = 0 } \frac{\lvert y_{i}\rvert^{2}}{2}
%  + \frac{\lvert y_{j}\rvert^{2}}{2}
% \end{align*}
% and 
% \begin{align*}
% \widehat{A^{*}} = \sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij} = 0 }
% \left[I_{i}\frac{\lvert Y_{i}\rvert^{2}}{2\pi_{i}}\right.
%  \left.+ I_{j}\frac{\lvert Y_{j}\rvert^{2}}{2\pi_{j}}\right]
% \end{align*}

% and for $\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}) = \dfrac{1}{N}\widehat{y^{T}_{HT}}(W,\mathbf{W}_{\mathcal{N}_{K}})$,we have 

% \begin{align*}
% \widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})) = \frac{1}{N^2}\sum_{i \in U}I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})[1 - \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})]\left[ \frac{Y_i}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right]^2
% \\  + \frac{1}{N^2}\sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) > 0 }I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})\frac{[\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) - \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})]}{\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}})}
% \\ \times \frac{Y_iY_j}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})} 
% \\+ \frac{1}{N^2}\sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij}(W,\mathbf{W}_{\mathcal{N}_{K}}) = 0 }
% \left[\frac{I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})Y^2_i}{2\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right.
%  \left.+ \frac{I_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})Y^2_j}{2\pi_{j}(W,\mathbf{W}_{\mathcal{N}_{K}})}\right]
% \end{align*}

% Hence, we have proved that,

% \begin{align*}
%   \mathbf{E}(\widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}))  \geq \mathbf{Var}_{HT}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}))
% \end{align*}
% Hence, $\widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})$ is a conservative estimator of $\mathbf{Var}_{HT}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}))$.
% \end{proof}

% Moreover, last term in equation \ref{Cov1} is unidentified  because each unit receives only one exposure and can only be observed under this exposure. 
% Hence, there is no unbiased estimator for the variance of the proposed estimators.
% However, if the joint probabilities $\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W',\mathbf{W}'_{\mathcal{N}_{K}}))$ for two different exposures $(W,\mathbf{W}_{\mathcal{N}_{K}})$ and $(W',\mathbf{W}'_{\mathcal{N}_{K}})> 0$  for all $i$ and $j$, an estimator for the covariance in \ref{Cov1} can be as follows:

% \begin{multline}\label{covhat1}
% \widehat{\mathbf{Cov}}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}}))= 
% \\\frac{1}{N^2}\sum_{i \in U}\sum_{j \in U, j \ne i}\left[
% \frac{I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})}{\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W',\mathbf{W}'_{\mathcal{N}_{K}}))}
% \frac{Y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\frac{Y_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})}{\pi_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})}\right.
% \\ \left.\times [\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W',\mathbf{W}'_{\mathcal{N}_{K}}))-\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})]\right]
% \\  - \frac{1}{N^2}\sum_{i \in U}\left[\frac{I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})Y^2_i}{2\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})} + \frac{I_{i}(W',\mathbf{W}'_{\mathcal{N}_{K}})Y_{i}^{2^{obs}}(W',\mathbf{W}'_{\mathcal{N}_{K}})}{2\pi_{i}(W',\mathbf{W}'_{\mathcal{N}_{K}})}\right]
% \end{multline}

% such that 
% \begin{equation}\label{EXPECTEDCOVHAT1}
%   \mathbf{E}(\widehat{\mathbf{Cov}}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}})))  \leq \mathbf{Cov}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}}))
% \end{equation}
% By Young’s inequality, this can be proved by the fact that the expected value of last term in equation \ref{covhat1} is less than or equal the last term in equation \ref{Cov1}.

% For the case where the joint probabilities $\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W',\mathbf{W}'_{\mathcal{N}_{K}})) = 0$  for some $i$ and $j$, the covaraince in \ref{Cov1} can be refined as follows:

% \begin{multline}\label{Covarinaceoftwomeans2}
% \mathbf{Cov}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}})) =
% \\  \frac{1}{N^2}\sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W',\mathbf{W}'_{\mathcal{N}_{K}})) > 0 }\left[\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W',\mathbf{W}'_{\mathcal{N}_{K}})) - \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})\right]
% \\ \times \frac{y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})} 
% \\ - \frac{1}{N^2}\sum_{i \in U}\sum_{j \in U:\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W',\mathbf{W}'_{\mathcal{N}_{K}})) = 0 }  y_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})y_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}}).
% \end{multline}

% Consequently, the more general covariance estimator is

% \begin{multline}\label{COVHATA}
% \widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}})= 
% \\\frac{1}{N^2}\sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W',\mathbf{W}'_{\mathcal{N}_{K}})) > 0 }\left[
% \frac{I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})}{\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W',\mathbf{W}'_{\mathcal{N}_{K}}))}
% \frac{Y_i}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\frac{Y_{j}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}})}{\pi_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})}\right.
% \\ \left.\times [\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W',\mathbf{W}'_{\mathcal{N}_{K}}))-\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})]\right]
% \\  - \frac{1}{N^2}\sum_{i \in U}\sum_{j \in U:\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W',\mathbf{W}'_{\mathcal{N}_{K}})) = 0 }\left[\frac{I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})Y^2_{i}}{2\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})} + \frac{I_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})Y^2_{j}}{2\pi_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})}\right]
% \end{multline}


% \begin{proposition}
% \begin{equation}\label{EXPECTEDCOVHAT22}
%   \mathbf{E}(\widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}})) \le \mathbf{Cov}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}})).
% \end{equation}
% \end{proposition}

% \begin{proof}
% First, for simplicity, let $I_{i} = I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})$, $I_{j} = I_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})$, $\pi_{i} = \pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})$, $\pi_{j} = \pi_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})$, $\pi_{ij} = \pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W',\mathbf{W}'_{\mathcal{N}_{K}}))$, $Y_{i} = Y_i$ and $Y_{j} = Y_{j}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}})$ and re-express the covariance in \ref{Cov1} as follows,




% \begin{multline}\label{Cov1withA}
% \mathbf{Cov}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}})) = \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij} - \pi_{i}\pi_{j}\right] \frac{y_{i}y_{j}}{\pi_{i}\pi_{j}}  - \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}y_{i}
% \\= \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij} - \pi_{i}\pi_{j}\right] \frac{y_{i}y_{j}}{\pi_{i}\pi_{j}}  - \frac{1}{N^2}A'.
% \end{multline}




% By Young’s inequality, if $\frac{1}{a_{ij}} + \frac{1}{b_{ij}} = 1$,

% \begin{equation}\label{Young1}
% \frac{\lvert y_{i}\rvert^{a_{ij}}}{a_{ij}}
%  + \frac{\lvert y_{j}\rvert^{b_{ij}}}{b_{ij}} \geq \lvert y_{i}\rvert \lvert y_{j}\rvert .
%  \end{equation}


%  Then,
% \begin{equation}\label{Young2}
% A'^{*} =  \sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij} = 0 } \frac{\lvert y_{i}\rvert^{a_{ij}}}{a_{ij}}
%  + \frac{\lvert y_{j}\rvert^{b_{ij}}}{b_{ij}} \geq \sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij} = 0 }\lvert y_{i}\rvert \lvert y_{j}\rvert \geq \sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij} = 0 } y_{i}y_{j} = A'
% \end{equation}

% %and

% %\begin{align*}
%  %- A'^{*} = - \sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij} = 0 } \frac{\lvert  %y_{i}\rvert^{a_{ij}}}{a_{ij}}
%  %+ \frac{\lvert y_{j}\rvert^{b_{ij}}}{b_{ij}} \le - \sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij} = 0 }\lvert y_{i}\rvert \lvert y_{j}\rvert \le - \sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij} = 0 } y_{i}y_{j} = A'
% %\end{align*}

% Therefore, 
% \begin{align*}
% \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij} - \pi_{i}\pi_{j}\right] \frac{y_{i}y_{j}}{\pi_{i}\pi_{j}}  - \frac{1}{N^2}A'^{*}  \le \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij} - \pi_{i}\pi_{j}\right] \frac{y_{i}y_{j}}{\pi_{i}\pi_{j}}  - \frac{1}{N^2}A'.
% \end{align*}

% and the associated Horvitz Thompson estimator of $A'^{*}$ is 
% \begin{align*}
% \widehat{A'^{*}} = \sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij} = 0 }
% \left[I_{i}\frac{\lvert Y_{i}\rvert^{a_{ij}}}{a_{ij}\pi_{i}}\right.
%  \left.+ I_{j}\frac{\lvert Y_{j}\rvert^{b_{ij}}}{b_{ij}\pi_{j}}\right]
% \end{align*}

% and consider the variance bias correction of the average as follows,
% \begin{align*}
% \widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}}) = \frac{1}{N^2}\sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij} > 0 }\left[
% \frac{I_{i}I_{j}}{\pi_{ij}}
% \frac{Y_{i}}{\pi_{i}}\frac{Y_{j}}{\pi_{j}}\times [\pi_{ij}\pi_{j}]\right]
% \\  - \frac{1}{N^2}\sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij} = 0 }
% \left[I_{i}\frac{\lvert Y_{i}\rvert^{a_{ij}}}{a_{ij}\pi_{i}}\right.
%  \left.+ I_{j}\frac{\lvert Y_{j}\rvert^{b_{ij}}}{b_{ij}\pi_{j}}\right]
%  \\ = \frac{1}{N^2}\sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij} > 0 }\left[
% \frac{I_{i}I_{j}}{\pi_{ij}}
% \frac{Y_{i}}{\pi_{i}}\frac{Y_{j}}{\pi_{j}}\times [\pi_{ij}\pi_{j}]\right]
% - \frac{1}{N^2} \widehat{A'^{*}}
%   \end{align*}

% where $a_{ij}$ and $b_{ij}$ are positive real numbers such that $\frac{1}{a_{ij}} + \frac{1}{b_{ij}} = 1$ for all pairs i and $j$ with $\pi_{ij}$ = 0.

% As a special case, assigning all $a_{ij}= b_{ij} = 2$ such that $\frac{1}{2} + \frac{1}{2} = 1$ for all pairs i and $j$ with $\pi_{ij}$ = 0,where
% \begin{align*}
%  A'^{*} = \sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij} = 0 } \frac{\lvert y_{i}\rvert^{2}}{2}
%  + \frac{\lvert y_{j}\rvert^{2}}{2}
% \end{align*}
% and 
% \begin{align*}
% \widehat{A'^{*}} = \sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij} = 0 }
% \left[I_{i}\frac{\lvert Y_{i}\rvert^{2}}{2\pi_{i}}\right.
%  \left.+ I_{j}\frac{\lvert Y_{j}\rvert^{2}}{2\pi_{j}}\right]
% \end{align*}

% we have 

% \begin{align*}
%    \mathbf{E}(\widehat{\mathbf{Cov}}_{A}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}})) \le \mathbf{Cov}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}})).
% \end{align*}
% \end{proof}

% Since $\widehat{\mathbf{Var}}_{A}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}})$ is a conservative variance estimator, the covariance estimator in \ref{COVHATA}, provides a conservative variance estimator of any estimator of the the form $\widehat{\delta} = X -Y$ such that $\mathbf{Var}(X-Y)$ = $\mathbf{Var}(X)$ + $\mathbf{Var}(Y)$ -2$\mathbf{Cov}(X,Y)$ which apply to all estimators under assumption 1.
% However, under assumption 2 and assumption 3, we have estimators of the form $\widehat{\delta} = (X - Y) + (W - Z)$ such that $\mathbf{Var}((X - Y) + (W - Z))$ = $\mathbf{Var}(X)$ + $\mathbf{Var}(Y)$ + $\mathbf{Var}(W)$ + $\mathbf{Var}(Z)$ -2$\mathbf{Cov}(X,Y)$ + 2$\mathbf{Cov}(X,W)$ -2$\mathbf{Cov}(X,Z)$ -2$\mathbf{Cov}(Y,W)$ +2$\mathbf{Cov}(Y,Z)$ -2$\mathbf{Cov}(W,Z)$. 
% To get conservative variance estimator of any estimator of the second form, $\widehat{\mathbf{Cov}}_{A}(X,Y)$ can be used to estimate covariance components that are subtracted from the variance components while for added covariance components we need a covariance estimator that is guaranteed to have expectation greater than or equal to the true covariance.

% We provide the following covariance estimator,

% \begin{multline}\label{COVHATB}
% \widehat{\mathbf{Cov}}_{B}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}})= 
% \\\frac{1}{N^2}\sum_{i \in U}\sum_{j \in U, j \ne i:\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W',\mathbf{W}'_{\mathcal{N}_{K}})) > 0 }\left[
% \frac{I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})I_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})}{\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W',\mathbf{W}'_{\mathcal{N}_{K}}))}
% \frac{Y_i}{\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})}\frac{Y_{j}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}})}{\pi_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})}\right.
% \\ \left.\times [\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W',\mathbf{W}'_{\mathcal{N}_{K}}))-\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})\pi_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})]\right]
% \\  + \frac{1}{N^2}\sum_{i \in U}\sum_{j \in U:\pi_{ij}((W,\mathbf{W}_{\mathcal{N}_{K}}),(W',\mathbf{W}'_{\mathcal{N}_{K}})) = 0 }\left[\frac{I_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})Y^2_{i}}{2\pi_{i}(W,\mathbf{W}_{\mathcal{N}_{K}})} + \frac{I_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})Y^2_{j}}{2\pi_{j}(W',\mathbf{W}'_{\mathcal{N}_{K}})}\right]
% \end{multline}


% \begin{proposition}
% \begin{equation}\label{EXPECTEDCOVHAT3}
%   \mathbf{E}(\widehat{\mathbf{Cov}}_{B}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}})) \geq \mathbf{Cov}(\bar{Y}_{HT}^{obs}(W,\mathbf{W}_{\mathcal{N}_{K}}),\bar{Y}_{HT}^{obs}(W',\mathbf{W}'_{\mathcal{N}_{K}})).
% \end{equation}
% \end{proposition}

% The proof follows by Young’s inequality in equations \ref{Young1} and \ref{Young2}.





%                     %       AppendixB      %
%                 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%    Properties of Horvitz-Thompson Estimators in Chapter 3    %%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{ Appendix B: Properties of Horvitz-Thompson Estimators}
% \label{AppendixB}
% \subsection{Properties of ATOT  Estimator under $K$-NIA }
% \label{AppendixB.1}


% First, we find the expected value and the variance of ATOT  estimator under $K$-NIA:
% \begin{equation} 
% \widehat{\delta}_{{HT,tot}} = \bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0}).
% \end{equation} 




% \subsubsection{The Expected Value of ATOT  Estimator}
% \label{makereferenceB.1.1}

% \begin{multline}
% \mathbf{E}(\widehat{\delta}_{{HT,tot}}) = \mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% =\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) - \mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% \\= \bar{y}(1,\mathbf{1}) -  \bar{y}(0,\mathbf{0}) = \delta_{tot}.
% \end{multline}


% \subsubsection{The Variance of ATOT  Estimator}
% \label{makereferenceB.1.2}

% We find the variance by using the properties $\mathbf{Var}(X-Y)$ = $\mathbf{Var}(X)$ + $\mathbf{Var}(Y)$ -2$\mathbf{Cov}(X,Y)$ and $\mathbf{Cov}(X,Y)$ = $\mathbf{E}(XY) - \mathbf{E}(X)\mathbf{E}(Y)$.


% First, we find $\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})\bar{Y}_{HT}^{obs}(0,\mathbf{0}))$ and $\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))$:

% \begin{multline}
% \mathbf{E}\left[\bar{Y}_{HT}^{obs}(1,\mathbf{1})\bar{Y}_{HT}^{obs}(0,\mathbf{0})\right] 
% =
% \\ \mathbf{E}\left[\left(\frac{1}{N}\sum_{i = 1}^{N} I_{i}(1,\mathbf{1}) \frac{Y_{i}^{obs}(1,\mathbf{1})}{\pi_{i}(1,\mathbf{1})}\right)
% \left(\frac{1}{N}\sum_{i = 1}^{N} I_{i}(0,\mathbf{0}) \frac{Y_{i}^{obs}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{0})}\right)\right]
% \\ = \mathbf{E}\left[\left(\frac{1}{N^2}\sum_{i = 1}^{N} I_{i}(1,\mathbf{1})I_{i}(0,\mathbf{0}) \frac{Y_{i}^{obs}(1,\mathbf{1})Y_{i}^{obs}(0,\mathbf{0})}{\pi_{i}(1,\mathbf{1})\pi_{i}(0,\mathbf{0})}\right)\right.
% \\ \left.+ \left(\frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} I_{i}(1,\mathbf{1})I_{j}(0,\mathbf{0}) \frac{Y_{i}^{obs}(1,\mathbf{1})Y_{j}^{obs}(0,\mathbf{0})}{\pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{0})}\right)\right]
% \\ = \mathbf{E}\left[\left(\frac{1}{N^2}\sum_{i = 1}^{N} I_{i}(1,\mathbf{1})I_{i}(0,\mathbf{0}) \frac{Y_{i}^{obs}(1,\mathbf{1})Y_{i}^{obs}(0,\mathbf{0})}{\pi_{i}(1,\mathbf{1})\pi_{i}(0,\mathbf{0})}\right)\right]
% \\ + \mathbf{E}\left[\left(\frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} I_{i}(1,\mathbf{1})I_{j}(0,\mathbf{0}) \frac{Y_{i}^{obs}(1,\mathbf{1})Y_{j}^{obs}(0,\mathbf{0})}{\pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{0})}\right)\right]
% \\ = \left(\frac{1}{N^2}\sum_{i = 1}^{N} \mathbf{E}\left[I_{i}(1,\mathbf{1})I_{i}(0,\mathbf{0})\right] \frac{y_{i}(1,\mathbf{1})y_{i}(0,\mathbf{0})}{\pi_{i}(1,\mathbf{1})\pi_{i}(0,\mathbf{0})}\right)
% \\ + \left(\frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \mathbf{E}\left[I_{i}(1,\mathbf{1})I_{j}(0,\mathbf{0})\right] \frac{y_{i}(1,\mathbf{1})y_{i}(0,\mathbf{0})}{\pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{0})}\right)
% \\ = \frac{1}{N^2}\sum_{i = 1}^{N} \pi_{i}((1,\mathbf{1}),(0,\mathbf{0})) \frac{y_{i}(1,\mathbf{1})y_{i}(0,\mathbf{0})}{\pi_{i}(1,\mathbf{1})\pi_{i}(0,\mathbf{0})}
% \\ + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((1,\mathbf{1}),(0,\mathbf{0})) \frac{y_{i}(1,\mathbf{1})y_{j}(0,\mathbf{0})}{\pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{0})}
% \\ = \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((1,\mathbf{1}),(0,\mathbf{0})) \frac{y_{i}(1,\mathbf{1})y_{j}(0,\mathbf{0})}{\pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{0})}
% \end{multline}





% %\begin{multline}
% %\mathbf{E}([\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})]^2) =
% %\\ \mathbf{E}\left(\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\right)^2 + \left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})\right)^2 
% % - 2\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\right)\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})\right)\right)
% %\\= \mathbf{E}\left(\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\right)^2\right) +  \mathbf{E}\left(\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})\right)^2\right) 
% % - 2\mathbf{E}\left(\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\right)\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})\right)\right)
% % \\ = \frac{1}{N^2}\sum_{i = 1}^{N} \frac{y^2_{i}(1,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell})}
% %+ \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}(1,W^{*}_{\ell}) \frac{y_{i}(1,W^{*}_{\ell})y_{j}(1,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell})}
% % \\ + \frac{1}{N^2}\sum_{i = 1}^{N} \frac{y^2_{i}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell-1})}
% %+ \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}(1,W^{*}_{\ell-1}) \frac{y_{i}(1,W^{*}_{\ell-1})y_{j}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(1,W^{*}_{\ell-1})}
% %\\ -2 \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((1,W^{*}_{\ell}),(1,W^{*}_{\ell-1})) \frac{y_{i}(1,W^{*}_{\ell})y_{j}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell-1})}
% %\end{multline}


% \begin{multline}
% \mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})) = 
% \\ \mathbf{E}\left[\frac{1}{N}\sum_{i = 1}^{N} I_{i}(1,\mathbf{1}) \frac{Y_{i}^{obs}(1,\mathbf{1})}{\pi_{i}(1,\mathbf{1})}\right]
% \mathbf{E}\left[\frac{1}{N}\sum_{i = 1}^{N} I_{i}(0,\mathbf{0}) \frac{Y_{i}^{obs}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{0})}\right]
% \\= \left[\frac{1}{N}\sum_{i = 1}^{N}  y_{i}(1,\mathbf{1})\right]
% \left[\frac{1}{N}\sum_{i = 1}^{N}  y_{i}(0,\mathbf{0})\right]
% \\ = \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}(1,\mathbf{1})y_{i}(0,\mathbf{0}) + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} y_{i}(1,\mathbf{1})y_{j}(0,\mathbf{0})
% \end{multline}



% \begin{multline}
% \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0})) = \mathbf{E}\left[\bar{Y}_{HT}^{obs}(1,\mathbf{1})\bar{Y}_{HT}^{obs}(0,\mathbf{0})\right] 
%  - \mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% \\ = \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((1,\mathbf{1}),(0,\mathbf{0})) \frac{y_{i}(1,\mathbf{1})y_{j}(0,\mathbf{0})}{\pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{0})}
% \\ -  \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}(1,\mathbf{1})y_{i}(0,\mathbf{0}) + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} y_{i}(1,\mathbf{1})y_{j}(0,\mathbf{0})
% \\ = \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,\mathbf{1}),(0,\mathbf{0})) - \pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{0})\right] \frac{y_{i}(1,\mathbf{1})y_{j}(0,\mathbf{0})}{\pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{0})}
% \\ -  \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}(1,\mathbf{1})y_{i}(0,\mathbf{0}).
% \end{multline}


% \begin{multline}
% \mathbf{Var}(\widehat{\delta}_{{HT,tot}}) = \mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})) = \mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% \\ -2\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% \\ =  \frac{1}{N^2}\sum_{i = 1}^{N}\pi_{i}(1,\mathbf{1})[1 - \pi_{i}(1,\mathbf{1})]\left[ \frac{y_{i}(1,\mathbf{1})}{\pi_{i}(1,\mathbf{1})}\right]^2
% \\   + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(1,\mathbf{1}) - \pi_{i}(1,\mathbf{1})\pi_{j}(1,\mathbf{1})]\frac{y_{i}(1,\mathbf{1})y_{j}(1,\mathbf{1})}{\pi_{i}(1,\mathbf{1})\pi_{j}(1,\mathbf{1})}
% \\  +  \frac{1}{N^2}\sum_{i = 1}^{N}\pi_{i}(0,\mathbf{0})[1 - \pi_{i}(0,\mathbf{0})]\left[ \frac{y_{i}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{0})}\right]^2 
% \\  + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(0,\mathbf{0}) - \pi_{i}(0,\mathbf{0})\pi_{j}(0,\mathbf{0})]\frac{y_{i}(0,\mathbf{0})y_{j}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{0})\pi_{j}(0,\mathbf{0})}
% \\  - 2 \left( \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,\mathbf{1}),(0,\mathbf{0})) - \pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{0})\right] \frac{y_{i}(1,\mathbf{1})y_{j}(0,\mathbf{0})}{\pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{1})} \right.
% \\  \left.- \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}(1,\mathbf{1})y_{i}(0,\mathbf{0})\right).
% \end{multline}












% \subsection{Properties of ADE Estimator under $K$-NIA}
% \label{AppendixB.2}


% First, we find the expected value and the variance of ADE estimator under $K$-NIA:
% \begin{equation} 
% \widehat{\delta}_{{HT,dir}} = \bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1}).
% \end{equation} 




% \subsubsection{The Expected Value of ADE Estimator}
% \label{makereferenceB.2.1}

% \begin{multline}
% \mathbf{E}(\widehat{\delta}_{{HT,dir}}) = \mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% =\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) - \mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% \\= \bar{y}(1,\mathbf{1}) -  \bar{y}(0,\mathbf{1}) = \delta_{dir}.
% \end{multline}


% \subsubsection{The Variance of ADE Estimator}
% \label{makereferenceB.2.2}

% We find the variance by using the properties $\mathbf{Var}(X-Y)$ = $\mathbf{Var}(X)$ + $\mathbf{Var}(Y)$ -2$\mathbf{Cov}(X,Y)$ and $\mathbf{Cov}(X,Y)$ = $\mathbf{E}(XY) - \mathbf{E}(X)\mathbf{E}(Y)$.


% First, we find $\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})\bar{Y}_{HT}^{obs}(0,\mathbf{1}))$ and $\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))$:

% \begin{multline}
% \mathbf{E}\left[\bar{Y}_{HT}^{obs}(1,\mathbf{1})\bar{Y}_{HT}^{obs}(0,\mathbf{1})\right] 
% =
% \\ \mathbf{E}\left[\left(\frac{1}{N}\sum_{i = 1}^{N} I_{i}(1,\mathbf{1}) \frac{Y_{i}^{obs}(1,\mathbf{1})}{\pi_{i}(1,\mathbf{1})}\right)
% \left(\frac{1}{N}\sum_{i = 1}^{N} I_{i}(0,\mathbf{1}) \frac{Y_{i}^{obs}(0,\mathbf{1})}{\pi_{i}(0,\mathbf{1})}\right)\right]
% \\ = \mathbf{E}\left[\left(\frac{1}{N^2}\sum_{i = 1}^{N} I_{i}(1,\mathbf{1})I_{i}(0,\mathbf{1}) \frac{Y_{i}^{obs}(1,\mathbf{1})Y_{i}^{obs}(0,\mathbf{1})}{\pi_{i}(1,\mathbf{1})\pi_{i}(0,\mathbf{1})}\right)\right.
% \\ \left.+ \left(\frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} I_{i}(1,\mathbf{1})I_{j}(0,\mathbf{1}) \frac{Y_{i}^{obs}(1,\mathbf{1})Y_{j}^{obs}(0,\mathbf{1})}{\pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{1})}\right)\right]
% \\ = \mathbf{E}\left[\left(\frac{1}{N^2}\sum_{i = 1}^{N} I_{i}(1,\mathbf{1})I_{i}(0,\mathbf{1}) \frac{Y_{i}^{obs}(1,\mathbf{1})Y_{i}^{obs}(0,\mathbf{1})}{\pi_{i}(1,\mathbf{1})\pi_{i}(0,\mathbf{1})}\right)\right]
% \\ + \mathbf{E}\left[\left(\frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} I_{i}(1,\mathbf{1})I_{j}(0,\mathbf{1}) \frac{Y_{i}^{obs}(1,\mathbf{1})Y_{j}^{obs}(0,\mathbf{1})}{\pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{1})}\right)\right]
% \\ = \left(\frac{1}{N^2}\sum_{i = 1}^{N} \mathbf{E}\left[I_{i}(1,\mathbf{1})I_{i}(0,\mathbf{1})\right] \frac{y_{i}(1,\mathbf{1})y_{i}(0,\mathbf{1})}{\pi_{i}(1,\mathbf{1})\pi_{i}(0,\mathbf{1})}\right)
% \\ + \left(\frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \mathbf{E}\left[I_{i}(1,\mathbf{1})I_{j}(0,\mathbf{1})\right] \frac{y_{i}(1,\mathbf{1})y_{i}(0,\mathbf{1})}{\pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{1})}\right)
% \\ = \frac{1}{N^2}\sum_{i = 1}^{N} \pi_{i}((1,\mathbf{1}),(0,\mathbf{1})) \frac{y_{i}(1,\mathbf{1})y_{i}(0,\mathbf{1})}{\pi_{i}(1,\mathbf{1})\pi_{i}(0,\mathbf{1})}
% \\ + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((1,\mathbf{1}),(0,\mathbf{1})) \frac{y_{i}(1,\mathbf{1})y_{j}(0,\mathbf{1})}{\pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{1})}
% \\ = \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((1,\mathbf{1}),(0,\mathbf{1})) \frac{y_{i}(1,\mathbf{1})y_{j}(0,\mathbf{1})}{\pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{1})}
% \end{multline}





% %\begin{multline}
% %\mathbf{E}([\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})]^2) =
% %\\ \mathbf{E}\left(\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\right)^2 + \left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})\right)^2 
% % - 2\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\right)\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})\right)\right)
% %\\= \mathbf{E}\left(\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\right)^2\right) +  \mathbf{E}\left(\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})\right)^2\right) 
% % - 2\mathbf{E}\left(\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\right)\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})\right)\right)
% % \\ = \frac{1}{N^2}\sum_{i = 1}^{N} \frac{y^2_{i}(1,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell})}
% %+ \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}(1,W^{*}_{\ell}) \frac{y_{i}(1,W^{*}_{\ell})y_{j}(1,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell})}
% % \\ + \frac{1}{N^2}\sum_{i = 1}^{N} \frac{y^2_{i}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell-1})}
% %+ \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}(1,W^{*}_{\ell-1}) \frac{y_{i}(1,W^{*}_{\ell-1})y_{j}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(1,W^{*}_{\ell-1})}
% %\\ -2 \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((1,W^{*}_{\ell}),(1,W^{*}_{\ell-1})) \frac{y_{i}(1,W^{*}_{\ell})y_{j}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell-1})}
% %\end{multline}


% \begin{multline}
% \mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) = 
% \\ \mathbf{E}\left[\frac{1}{N}\sum_{i = 1}^{N} I_{i}(1,\mathbf{1}) \frac{Y_{i}^{obs}(1,\mathbf{1})}{\pi_{i}(1,\mathbf{1})}\right]
% \mathbf{E}\left[\frac{1}{N}\sum_{i = 1}^{N} I_{i}(0,\mathbf{1}) \frac{Y_{i}^{obs}(0,\mathbf{1})}{\pi_{i}(0,\mathbf{1})}\right]
% \\= \left[\frac{1}{N}\sum_{i = 1}^{N}  y_{i}(1,\mathbf{1})\right]
% \left[\frac{1}{N}\sum_{i = 1}^{N}  y_{i}(0,\mathbf{1})\right]
% \\ = \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}(1,\mathbf{1})y_{i}(0,\mathbf{1}) + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} y_{i}(1,\mathbf{1})y_{j}(0,\mathbf{1})
% \end{multline}



% \begin{multline}
% \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{1})) = \mathbf{E}\left[\bar{Y}_{HT}^{obs}(1,\mathbf{1})\bar{Y}_{HT}^{obs}(0,\mathbf{1})\right] 
%  - \mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% \\ = \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((1,\mathbf{1}),(0,\mathbf{1})) \frac{y_{i}(1,\mathbf{1})y_{j}(0,\mathbf{1})}{\pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{1})}
% \\ -  \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}(1,\mathbf{1})y_{i}(0,\mathbf{1}) + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} y_{i}(1,\mathbf{1})y_{j}(0,\mathbf{1})
% \\ = \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,\mathbf{1}),(0,\mathbf{1})) - \pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{1})\right] \frac{y_{i}(1,\mathbf{1})y_{j}(0,\mathbf{1})}{\pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{1})}
% \\ -  \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}(1,\mathbf{1})y_{i}(0,\mathbf{1}).
% \end{multline}


% \begin{multline}
% \mathbf{Var}(\widehat{\delta}_{{HT,dir}}) = \mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1})) = \mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% \\ -2\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% \\ =  \frac{1}{N^2}\sum_{i = 1}^{N}\pi_{i}(1,\mathbf{1})[1 - \pi_{i}(1,\mathbf{1})]\left[ \frac{y_{i}(1,\mathbf{1})}{\pi_{i}(1,\mathbf{1})}\right]^2
% \\   + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(1,\mathbf{1}) - \pi_{i}(1,\mathbf{1})\pi_{j}(1,\mathbf{1})]\frac{y_{i}(1,\mathbf{1})y_{j}(1,\mathbf{1})}{\pi_{i}(1,\mathbf{1})\pi_{j}(1,\mathbf{1})}
% \\  +  \frac{1}{N^2}\sum_{i = 1}^{N}\pi_{i}(0,\mathbf{1})[1 - \pi_{i}(0,\mathbf{1})]\left[ \frac{y_{i}(0,\mathbf{1})}{\pi_{i}(0,\mathbf{1})}\right]^2 
% \\  + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(0,\mathbf{1}) - \pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{1})]\frac{y_{i}(0,\mathbf{1})y_{j}(0,\mathbf{1})}{\pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{1})}
% \\  - 2 \left( \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,\mathbf{1}),(0,\mathbf{1})) - \pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{1})\right] \frac{y_{i}(1,\mathbf{1})y_{j}(0,\mathbf{1})}{\pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{1})} \right.
% \\  \left.- \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}(1,\mathbf{1})y_{i}(0,\mathbf{1})\right).
% \end{multline}














% \subsection{Properties of AIE Estimator under $K$-NIA}
% \label{AppendixB.3}


% First, we find the expected value and the variance of AIE estimator
% \begin{equation} 
% \widehat{\delta}_{{HT,ind}} = \bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0}).
% \end{equation} 

% Additionally, the average total effect estimator is the sum of the average direct and indirect estimators: 
% \begin{lemma}\label{lemma1B}
% \begin{equation}\label{sumofdirectindirect}
% \widehat{\delta}_{HT,tot} = \widehat{\delta}_{HT,dir} + \widehat{\delta}_{HT,ind}
% \end{equation}
% \end{lemma}
% %where $\delta_{r}$ is the indirect effect of the remaining units that are not included in the $K$-neighborhood $\mathcal{N}_{ik}$.

% \begin{proof}
% \begin{multline}
% \widehat{\delta}_{HT,dir} + \widehat{\delta}_{HT,ind}
% =  \bar{Y}_{HT}^{obs}(1, \mathbf{1}) -\bar{Y}_{HT}^{obs}(0, \mathbf{1})+ \bar{Y}_{HT}^{obs}(0, \mathbf{1}) -\bar{Y}_{HT}^{obs}(0, \mathbf{0}) 
% \\ =  \bar{Y}_{HT}^{obs}(1, \mathbf{1}) - \bar{Y}_{HT}^{obs}(0, \mathbf{0}) 
% \\ = \widehat{\delta}_{HT,tot} 
% \end{multline}
% \end{proof}




% \subsubsection{The Expected Value of AIE Estimator}
% \label{makereferenceB.3.1}

% \begin{multline}
% \mathbf{E}(\widehat{\delta}_{{HT,ind}}) = \mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% =\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) - \mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% \\= \bar{y}(0,\mathbf{1}) -  \bar{y}(0,\mathbf{0}) = \delta_{ind}.
% \end{multline}


% \subsubsection{The Variance of AIE Estimator}
% \label{makereferenceB.3.2}

% We find the variance by using the properties $\mathbf{Var}(X-Y)$ = $\mathbf{Var}(X)$ + $\mathbf{Var}(Y)$ -2$\mathbf{Cov}(X,Y)$ and $\mathbf{Cov}(X,Y)$ = $\mathbf{E}(XY) - \mathbf{E}(X)\mathbf{E}(Y)$.


% First, we find $\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})\bar{Y}_{HT}^{obs}(0,\mathbf{0}))$ and $\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))$:

% \begin{multline}
% \mathbf{E}\left[\bar{Y}_{HT}^{obs}(0,\mathbf{1})\bar{Y}_{HT}^{obs}(0,\mathbf{0})\right] 
% =
% \\ \mathbf{E}\left[\left(\frac{1}{N}\sum_{i = 1}^{N} I_{i}(0,\mathbf{1}) \frac{Y_{i}^{obs}(0,\mathbf{1})}{\pi_{i}(0,\mathbf{1})}\right)
% \left(\frac{1}{N}\sum_{i = 1}^{N} I_{i}(0,\mathbf{0}) \frac{Y_{i}^{obs}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{0})}\right)\right]
% \\ = \mathbf{E}\left[\left(\frac{1}{N^2}\sum_{i = 1}^{N} I_{i}(0,\mathbf{1})I_{i}(0,\mathbf{0}) \frac{Y_{i}^{obs}(0,\mathbf{1})Y_{i}^{obs}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{1})\pi_{i}(0,\mathbf{0})}\right)\right.
% \\ \left.+ \left(\frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} I_{i}(0,\mathbf{1})I_{j}(0,\mathbf{0}) \frac{Y_{i}^{obs}(0,\mathbf{1})Y_{j}^{obs}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{0})}\right)\right]
% \\ = \mathbf{E}\left[\left(\frac{1}{N^2}\sum_{i = 1}^{N} I_{i}(0,\mathbf{1})I_{i}(0,\mathbf{0}) \frac{Y_{i}^{obs}(0,\mathbf{1})Y_{i}^{obs}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{1})\pi_{i}(0,\mathbf{0})}\right)\right]
% \\ + \mathbf{E}\left[\left(\frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} I_{i}(0,\mathbf{1})I_{j}(0,\mathbf{0}) \frac{Y_{i}^{obs}(0,\mathbf{1})Y_{j}^{obs}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{0})}\right)\right]
% \\ = \left(\frac{1}{N^2}\sum_{i = 1}^{N} \mathbf{E}\left[I_{i}(0,\mathbf{1})I_{i}(0,\mathbf{0})\right] \frac{y_{i}(0,\mathbf{1})y_{i}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{1})\pi_{i}(0,\mathbf{0})}\right)
% \\ + \left(\frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \mathbf{E}\left[I_{i}(0,\mathbf{1})I_{j}(0,\mathbf{0})\right] \frac{y_{i}(0,\mathbf{1})y_{i}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{0})}\right)
% \\ = \frac{1}{N^2}\sum_{i = 1}^{N} \pi_{i}((0,\mathbf{1}),(0,\mathbf{0})) \frac{y_{i}(0,\mathbf{1})y_{i}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{1})\pi_{i}(0,\mathbf{0})}
% \\ + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((0,\mathbf{1}),(0,\mathbf{0})) \frac{y_{i}(0,\mathbf{1})y_{j}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{0})}
% \\ = \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((0,\mathbf{1}),(0,\mathbf{0})) \frac{y_{i}(0,\mathbf{1})y_{j}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{0})}
% \end{multline}





% %\begin{multline}
% %\mathbf{E}([\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})]^2) =
% %\\ \mathbf{E}\left(\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\right)^2 + \left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})\right)^2 
% % - 2\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\right)\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})\right)\right)
% %\\= \mathbf{E}\left(\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\right)^2\right) +  \mathbf{E}\left(\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})\right)^2\right) 
% % - 2\mathbf{E}\left(\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\right)\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})\right)\right)
% % \\ = \frac{1}{N^2}\sum_{i = 1}^{N} \frac{y^2_{i}(1,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell})}
% %+ \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}(1,W^{*}_{\ell}) \frac{y_{i}(1,W^{*}_{\ell})y_{j}(1,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell})}
% % \\ + \frac{1}{N^2}\sum_{i = 1}^{N} \frac{y^2_{i}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell-1})}
% %+ \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}(1,W^{*}_{\ell-1}) \frac{y_{i}(1,W^{*}_{\ell-1})y_{j}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(1,W^{*}_{\ell-1})}
% %\\ -2 \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((1,W^{*}_{\ell}),(1,W^{*}_{\ell-1})) \frac{y_{i}(1,W^{*}_{\ell})y_{j}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell-1})}
% %\end{multline}


% \begin{multline}
% \mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})) = 
% \\ \mathbf{E}\left[\frac{1}{N}\sum_{i = 1}^{N} I_{i}(0,\mathbf{1}) \frac{Y_{i}^{obs}(0,\mathbf{1})}{\pi_{i}(0,\mathbf{1})}\right]
% \mathbf{E}\left[\frac{1}{N}\sum_{i = 1}^{N} I_{i}(0,\mathbf{0}) \frac{Y_{i}^{obs}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{0})}\right]
% \\= \left[\frac{1}{N}\sum_{i = 1}^{N}  y_{i}(0,\mathbf{1})\right]
% \left[\frac{1}{N}\sum_{i = 1}^{N}  y_{i}(0,\mathbf{0})\right]
% \\ = \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}(0,\mathbf{1})y_{i}(0,\mathbf{0}) + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} y_{i}(0,\mathbf{1})y_{j}(0,\mathbf{0})
% \end{multline}



% \begin{multline}
% \mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0})) = \mathbf{E}\left[\bar{Y}_{HT}^{obs}(0,\mathbf{1})\bar{Y}_{HT}^{obs}(0,\mathbf{0})\right] 
%  - \mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% \\ = \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((0,\mathbf{1}),(0,\mathbf{0})) \frac{y_{i}(0,\mathbf{1})y_{j}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{0})}
% \\ -  \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}(0,\mathbf{1})y_{i}(0,\mathbf{0}) + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} y_{i}(0,\mathbf{1})y_{j}(0,\mathbf{0})
% \\ = \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((0,\mathbf{1}),(0,\mathbf{0})) - \pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{0})\right] \frac{y_{i}(0,\mathbf{1})y_{j}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{0})}
% \\ -  \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}(0,\mathbf{1})y_{i}(0,\mathbf{0}).
% \end{multline}


% \begin{multline}
% \mathbf{Var}(\widehat{\delta}_{{HT,ind}}) = \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})) = \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) + \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% \\ -2\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% \\ =  \frac{1}{N^2}\sum_{i = 1}^{N}\pi_{i}(0,\mathbf{1})[1 - \pi_{i}(0,\mathbf{1})]\left[ \frac{y_{i}(0,\mathbf{1})}{\pi_{i}(0,\mathbf{1})}\right]^2
% \\   + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(0,\mathbf{1}) - \pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{1})]\frac{y_{i}(0,\mathbf{1})y_{j}(0,\mathbf{1})}{\pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{1})}
% \\  +  \frac{1}{N^2}\sum_{i = 1}^{N}\pi_{i}(0,\mathbf{0})[1 - \pi_{i}(0,\mathbf{0})]\left[ \frac{y_{i}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{0})}\right]^2 
% \\  + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(0,\mathbf{0}) - \pi_{i}(0,\mathbf{0})\pi_{j}(0,\mathbf{0})]\frac{y_{i}(0,\mathbf{0})y_{j}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{0})\pi_{j}(0,\mathbf{0})}
% \\  - 2 \left( \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((0,\mathbf{1}),(0,\mathbf{0})) - \pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{0})\right] \frac{y_{i}(0,\mathbf{1})y_{j}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{1})} \right.
% \\  \left.- \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}(0,\mathbf{1})y_{i}(0,\mathbf{0})\right).
% \end{multline}










% \subsection{Properties of A$\ell$NNIE Estimator under $K$-NIA }
% \label{AppendixB.4}


% Now, we find the expected value and the variance of A$\ell$NNIE estimator under $K-NIA$ assumption
% \begin{equation} 
% \widehat{\delta}_{{HT,\ell}} = \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1}).
% \end{equation} 


% First, note that the indirect effect estimator $\widehat{\delta}_{{HT,ind}}$ is the sum of all K-nearest neighbors indirect effects estimators ,i.e.,
%  \begin{lemma}\label{lemma2B}
% \begin{equation}
% \widehat{\delta}_{{HT,ind}} = \sum_{\ell = 1}^{K} \widehat{\delta}_{HT,\ell}
% %= \widehat{\delta}_{HT,1^{st}} 
% %+ \widehat{\delta}_{HT,2^{nd}}+ \dots + \widehat{\delta}_{HT,\ell}+ \dots + \widehat{\delta}_{HT,K} 
% %+ \delta_{r}.
% \end{equation}
% \end{lemma}
% %where $\delta_{r}$ is the indirect effect of the remaining units that are not included in the $K$-neighborhood $\mathcal{N}_{ik}$.

% \begin{proof}
% For $\ell \in \mathcal{N}_{ik}$, if we define $W^{*}_{\ell}$ as previously such that $W^{*}_{1}$ means that the first nearest neighbor is treated where the rest of the nearest neighbors are assigned to control,
% $W^{*}_{2}$ means that the first two nearest neighbors are treated and the rest are assigned to control and so forth.
% $W^{*}_{0}$ denote that all K-nearest neighbors are assigned to control. Then,


% \begin{multline}
% \widehat{\delta}_{HT,1^{st}} + \widehat{\delta}_{HT,2^{nd}}+ \widehat{\delta}_{HT,3^{rd}}+\dots + \widehat{\delta}_{HT,\ell}+ \dots + \widehat{\delta}_{HT,K} = \\
% \bar{Y}_{HT}^{obs}(0,W^{*}_{1}) - \bar{Y}_{HT}^{obs}(0, W^{*}_{0}) \\
% + \bar{Y}_{HT}^{obs}(0,W^{*}_{2}) - \bar{Y}_{HT}^{obs}(0, W^{*}_{1})\\
% + \bar{Y}_{HT}^{obs}(0,W^{*}_{3}) - \bar{Y}_{HT}^{obs}(0, W^{*}_{2}) \\
% + \dots \\
% + \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0, W^{*}_{\ell - 1})\\
% + \dots \\
% + \bar{Y}_{HT}^{obs}(0,W^{*}_{K}) - \bar{Y}_{HT}^{obs}(0, W^{*}_{K- 1})\\
% = \bar{Y}_{HT}^{obs}(0,W^{*}_{K}) - \bar{Y}_{HT}^{obs}(0, W^{*}_{0}) \\
% =  \bar{Y}_{HT}^{obs}(0, \mathbf{1}) -\bar{Y}_{HT}^{obs}(0, \mathbf{0}) \\
% =\widehat{\delta}_{HT,ind} \\
% \end{multline}
% \end{proof}


% %Since our primary interest is in the indirect effects, we focus on the the K-nearest neighbors and the $\ell^{th}$ nearest neighbor indirect effect and we don't extend the total effects any further. 
% %Similarly, we can show that the average total effect estimator $\widehat{\delta}_{Total}$ is the sum of the direct and indirect effects ,i.e.,
% %\begin{equation}\label{sumofdirectindirect}
% %\widehat{\delta}_{HT,Total} = \widehat{\delta}_{HT,direct} + \widehat{\delta}_{HT,ind}
% %\end{equation}




% \subsubsection{The Expected Value of A$\ell$NNIE Estimator}
% \label{makereferenceB.4.1}

% \begin{multline}
% \mathbf{E}(\widehat{\delta}_{{HT,\ell}}) = \mathbf{E}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})) 
% =\mathbf{E}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell})) - \mathbf{E}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1}))
% \\= \bar{y}(0,W^{*}_{\ell}) -  \bar{y}(0,W^{*}_{\ell-1})= \delta_{\ell}.
% \end{multline}


% \subsubsection{The Variance of A$\ell$NNIE Estimator}
% \label{makereferenceB.4.2}

% We find the variance by using the property $\mathbf{Var}(X-Y)$ = $\mathbf{Var}(X)$ + $\mathbf{Var}(Y)$ -2$\mathbf{Cov}(X,Y)$ and the property $\mathbf{Cov}(X,Y)$ = $\mathbf{E}(XY) - \mathbf{E}(X)\mathbf{E}(Y)$.


% First, we find $\mathbf{E}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell})\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1}))$ and $\mathbf{E}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1}))$:

% \begin{multline}
% \mathbf{E}\left[\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell})\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})\right] 
% =
% \\ \mathbf{E}\left[\left(\frac{1}{N}\sum_{i = 1}^{N} I_{i}(0,W^{*}_{\ell}) \frac{Y_{i}^{obs}(0,W^{*}_{\ell})}{\pi_{i}(0,W^{*}_{\ell})}\right)
% \left(\frac{1}{N}\sum_{i = 1}^{N} I_{i}(0,W^{*}_{\ell-1}) \frac{Y_{i}^{obs}(0,W^{*}_{\ell-1})}{\pi_{i}(0,W^{*}_{\ell-1})}\right)\right]
% \\ = \mathbf{E}\left[\left(\frac{1}{N^2}\sum_{i = 1}^{N} I_{i}(0,W^{*}_{\ell})I_{i}(0,W^{*}_{\ell-1}) \frac{Y_{i}^{obs}(0,W^{*}_{\ell})Y_{i}^{obs}(0,W^{*}_{\ell-1})}{\pi_{i}(0,W^{*}_{\ell})\pi_{i}(0,W^{*}_{\ell-1})}\right)\right.
% \\ \left.+ \left(\frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} I_{i}(0,W^{*}_{\ell})I_{j}(0,W^{*}_{\ell-1}) \frac{Y_{i}^{obs}(0,W^{*}_{\ell})Y_{j}^{obs}(0,W^{*}_{\ell-1})}{\pi_{i}(0,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell-1})}\right)\right]
% \\ = \mathbf{E}\left[\left(\frac{1}{N^2}\sum_{i = 1}^{N} I_{i}(0,W^{*}_{\ell})I_{i}(0,W^{*}_{\ell-1}) \frac{Y_{i}^{obs}(0,W^{*}_{\ell})Y_{i}^{obs}(0,W^{*}_{\ell-1})}{\pi_{i}(0,W^{*}_{\ell})\pi_{i}(0,W^{*}_{\ell-1})}\right)\right]
% \\ + \mathbf{E}\left[\left(\frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} I_{i}(0,W^{*}_{\ell})I_{j}(0,W^{*}_{\ell-1}) \frac{Y_{i}^{obs}(0,W^{*}_{\ell})Y_{j}^{obs}(0,W^{*}_{\ell-1})}{\pi_{i}(0,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell-1})}\right)\right]
% \\ = \left(\frac{1}{N^2}\sum_{i = 1}^{N} \mathbf{E}\left[I_{i}(0,W^{*}_{\ell})I_{i}(0,W^{*}_{\ell-1})\right] \frac{y_{i}(0,W^{*}_{\ell})y_{i}(0,W^{*}_{\ell-1})}{\pi_{i}(0,W^{*}_{\ell})\pi_{i}(0,W^{*}_{\ell-1})}\right)
% \\ + \left(\frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \mathbf{E}\left[I_{i}(0,W^{*}_{\ell})I_{j}(0,W^{*}_{\ell-1})\right] \frac{y_{i}(0,W^{*}_{\ell})y_{i}(0,W^{*}_{\ell-1})}{\pi_{i}(0,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell-1})}\right)
% \\ = \frac{1}{N^2}\sum_{i = 1}^{N} \pi_{i}((0,W^{*}_{\ell}),(0,W^{*}_{\ell-1})) \frac{y_{i}(0,W^{*}_{\ell})y_{i}(0,W^{*}_{\ell-1})}{\pi_{i}(0,W^{*}_{\ell})\pi_{i}(0,W^{*}_{\ell-1})}
% \\ + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((0,W^{*}_{\ell}),(0,W^{*}_{\ell-1})) \frac{y_{i}(0,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell-1})}{\pi_{i}(0,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell-1})}
% \\ = \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((0,W^{*}_{\ell}),(0,W^{*}_{\ell-1})) \frac{y_{i}(0,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell-1})}{\pi_{i}(0,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell-1})}
% \end{multline}





% %\begin{multline}
% %\mathbf{E}([\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})]^2) =
% %\\ \mathbf{E}\left(\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\right)^2 + \left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})\right)^2 
% % - 2\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\right)\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})\right)\right)
% %\\= \mathbf{E}\left(\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\right)^2\right) +  \mathbf{E}\left(\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})\right)^2\right) 
% % - 2\mathbf{E}\left(\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\right)\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})\right)\right)
% % \\ = \frac{1}{N^2}\sum_{i = 1}^{N} \frac{y^2_{i}(1,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell})}
% %+ \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}(1,W^{*}_{\ell}) \frac{y_{i}(1,W^{*}_{\ell})y_{j}(1,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell})}
% % \\ + \frac{1}{N^2}\sum_{i = 1}^{N} \frac{y^2_{i}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell-1})}
% %+ \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}(1,W^{*}_{\ell-1}) \frac{y_{i}(1,W^{*}_{\ell-1})y_{j}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(1,W^{*}_{\ell-1})}
% %\\ -2 \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((1,W^{*}_{\ell}),(1,W^{*}_{\ell-1})) \frac{y_{i}(1,W^{*}_{\ell})y_{j}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell-1})}
% %\end{multline}


% \begin{multline}
% \mathbf{E}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})) = 
% \\ \mathbf{E}\left[\frac{1}{N}\sum_{i = 1}^{N} I_{i}(0,W^{*}_{\ell}) \frac{Y_{i}^{obs}(0,W^{*}_{\ell})}{\pi_{i}(0,W^{*}_{\ell})}\right]
% \mathbf{E}\left[\frac{1}{N}\sum_{i = 1}^{N} I_{i}(0,W^{*}_{\ell-1}) \frac{Y_{i}^{obs}(0,W^{*}_{\ell-1})}{\pi_{i}(0,W^{*}_{\ell-1})}\right]
% \\= \left[\frac{1}{N}\sum_{i = 1}^{N}  y_{i}(0,W^{*}_{\ell})\right]
% \left[\frac{1}{N}\sum_{i = 1}^{N}  y_{i}(0,W^{*}_{\ell-1})\right]
% \\ = \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}(0,W^{*}_{\ell})y_{i}(0,W^{*}_{\ell-1}) + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} y_{i}(0,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell-1})
% \end{multline}



% \begin{multline}
% \mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}),\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})) = \mathbf{E}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell})\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})) - \mathbf{E}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1}))
% \\ = \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((0,W^{*}_{\ell}),(0,W^{*}_{\ell-1})) \frac{y_{i}(0,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell-1})}{\pi_{i}(0,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell-1})}
% \\ -  \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}(0,W^{*}_{\ell})y_{i}(0,W^{*}_{\ell-1}) - \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} y_{i}(0,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell-1})
% \\ = \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((0,W^{*}_{\ell}),(0,W^{*}_{\ell-1})) - \pi_{i}(0,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell-1})\right] \frac{y_{i}(0,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell-1})}{\pi_{i}(0,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell-1})} 
% \\ - \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}(0,W^{*}_{\ell})y_{i}(0,W^{*}_{\ell-1}).
% \end{multline}


% \begin{multline}
% \mathbf{Var}(\widehat{\delta}_{{HT,\ell}}) = \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})) = \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell})) + \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})) 
% \\ -2\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}),\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})
% \\ =  \frac{1}{N^2}\sum_{i = 1}^{N}\pi_{i}(0,W^{*}_{\ell})[1 - \pi_{i}(0,W^{*}_{\ell})]\left[ \frac{y_{i}(0,W^{*}_{\ell})}{\pi_{i}(0,W^{*}_{\ell})}\right]^2
% \\   + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(0,W^{*}_{\ell}) - \pi_{i}(0,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell})]\frac{y_{i}(0,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell})}{\pi_{i}(0,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell})}
% \\  +  \frac{1}{N^2}\sum_{i = 1}^{N}\pi_{i}(0,W^{*}_{\ell-1})[1 - \pi_{i}(0,W^{*}_{\ell-1})]\left[ \frac{y_{i}(0,W^{*}_{\ell-1})}{\pi_{i}(0,W^{*}_{\ell-1})}\right]^2 
% \\  + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(0,W^{*}_{\ell-1}) - \pi_{i}(0,W^{*}_{\ell-1})\pi_{j}(0,W^{*}_{\ell-1})]\frac{y_{i}(0,W^{*}_{\ell-1})y_{j}(0,W^{*}_{\ell-1})}{\pi_{i}(0,W^{*}_{\ell-1})\pi_{j}(0,W^{*}_{\ell-1})}
% \\  - 2 \left( \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((0,W^{*}_{\ell}),(0,W^{*}_{\ell-1})) - \pi_{i}(0,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell-1})\right] \frac{y_{i}(0,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell-1})}{\pi_{i}(0,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell-1})} \right.
% \\  \left.- \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}(0,W^{*}_{\ell})y_{i}(0,W^{*}_{\ell-1})\right).
% \end{multline}










% \subsection{Properties of ATOT  Estimator under the No Weak Interaction between Direct and Indirect Effects Assumption}
% \label{AppendixB.5}


% The unbiased Horvitz Thompson estimator of $\widehat{\delta^*}_{HT,tot}$ under the no weak interaction between direct and indirect effects assumption is provided in the following lemma,


% \begin{lemma}\label{lemma3B}
% \begin{equation}\label{Ass2TotalAppB}
% \widehat{\delta^*}_{HT,tot} =  \widehat{\delta^*}_{HT,dir} +
% \widehat{\delta^*}_{HT,ind} % = \widehat{\delta}_{HT,tot}.
% \end{equation} 
% \end{lemma}


% \begin{proof}
% \begin{align*}
% \widehat{\delta^*}_{HT,dir} + \widehat{\delta^*}_{HT,ind}
% =   c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1})] +
% c_{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{0}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})]
% \\+  c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(1,\mathbf{0})] +
% c_{2}[\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})]
% \\= c_{1}\bar{Y}_{HT}^{obs}(1,\mathbf{1}) +  c_{1}\bar{Y}_{HT}^{obs}(1,\mathbf{1}) 
% - c_{2} \bar{Y}_{HT}^{obs}(0,\mathbf{0})
% - c_{2} \bar{Y}_{HT}^{obs}(0,\mathbf{0})
% \\= 2c_{1}\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - 2c_{2}\bar{Y}_{HT}^{obs}(0,\mathbf{0})
% %\\= \widehat{\delta}_{HT,tot} 
% \end{align*}

% and if $c_{1} = c_{2} = \frac{1}{2}$, then 
% $\widehat{\delta^*}_{HT,tot}  = \widehat{\delta}_{HT,tot}$.

% \end{proof}


% \subsubsection{The Expected Value of  ATOT  Estimator}
% \label{makereferenceB.5.1}

% \begin{multline}
% \mathbf{E}(\widehat{\delta^*}_{{HT,tot}}) = \mathbf{E}(\widehat{\delta^*}_{HT,dir}) + \mathbf{E}(\widehat{\delta^*}_{HT,ind}) 
% = \delta_{dir} + \delta_{ind} = \delta_{tot}.
% \end{multline}

% where $\mathbf{E}(\widehat{\delta^*}_{HT,dir})$ and $\mathbf{E}(\widehat{\delta^*}_{HT,ind})$ are provided in the next two sections.

% \subsubsection{The Variance  ATOT  Estimator}
% \label{makereferenceB.5.2}




% \begin{multline}
% \mathbf{Var}(\widehat{\delta^*}_{{HT,tot}}) = \mathbf{Var}(\widehat{\delta^*}_{HT, dir} + \widehat{\delta^*}_{HT,ind}) \\ = \mathbf{Var}(c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1})] +
% c_{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{0}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})]
% \\+  c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(1,\mathbf{0})] + c_{2}[\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})])
% \\=\mathbf{Var}(2c_{1} \bar{Y}_{HT}^{obs}(1,\mathbf{1}) - 2c_{2}\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% \\= 4c^2_{1}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + 4c^2_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
%  -2(4c_{1}c_{2})\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% \end{multline}

% and for $c_{1} + c_{2} = \frac{1}{2}$,

% \begin{multline}
% \mathbf{Var}(\widehat{\delta^*}_{{HT,tot}}) = \mathbf{Var}(\widehat{\delta}_{{HT,tot}})
% \\ = \mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
%  -2\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% \end{multline}


% % We can also find the variance as follows for $c_{1} + c_{2} = \frac{1}{2}$:
% % \begin{multline}
% % \mathbf{Var}(\widehat{\delta^*}_{{HT,tot}}) = \mathbf{Var}(\widehat{\delta^*}_{HT,direct} + \widehat{\delta^*}_{HT,ind})
% % \\ = \mathbf{Var}(\widehat{\delta^*}_{HT,direct}) + \mathbf{Var}(\widehat{\delta^*}_{HT,ind}) + 2\mathbf{Cov}(\widehat{\delta^*}_{HT,direct} , \widehat{\delta^*}_{HT,ind})
% % \end{multline}

% % where
% % \begin{multline}
% % \mathbf{Var}(\widehat{\delta^*}_{HT,direct}) + \mathbf{Var}(\widehat{\delta^*}_{HT,ind}) =
% % \\ \frac{1}{4}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + \frac{1}{4}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) +
% % \frac{1}{4}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}) + \frac{1}{4}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \\ -\frac{1}{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% % +\frac{1}{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(1,\mathbf{0})) 
% % \\ -\frac{1}{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% % -\frac{1}{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}), \bar{Y}_{HT}^{obs}(1,\mathbf{0})) 
% % \\ +\frac{1}{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% % -\frac{1}{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}), \bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \\ + \frac{1}{4}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + \frac{1}{4}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{0})) +
% % \frac{1}{4}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) + \frac{1}{4}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \\ -\frac{1}{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(1,\mathbf{0})) 
% % + \frac{1}{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% % \\ -\frac{1}{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% % -\frac{1}{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}), \bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% % \\ + \frac{1}{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% % -\frac{1}{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% % \\ = \frac{1}{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + \frac{1}{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{0})) +
% % \frac{1}{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) + \frac{1}{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \\ - \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) - \mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}), \bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % \end{multline}

% % and
% % \begin{multline}
% % \mathbf{Cov}(\widehat{\delta^*}_{HT,direct} , \widehat{\delta^*}_{HT,ind}) = \mathbf{E}(\widehat{\delta^*}_{HT,direct}\widehat{\delta^*}_{HT,ind}) -
% % \mathbf{E}(\widehat{\delta^*}_{HT,direct})\mathbf{E}(\widehat{\delta^*}_{HT,ind})
% % \\=  \mathbf{E}((\frac{1}{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1})] +
% % \frac{1}{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{0}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})])
% % \\ \times(\frac{1}{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(1,\mathbf{0})] +
% % \frac{1}{2}[\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})]))
% % \\ - (\mathbf{E}(\frac{1}{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1})] +
% % \frac{1}{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{0}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})])
% % \\ \times \mathbf{E}(\frac{1}{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(1,\mathbf{0})] +
% % \frac{1}{2}[\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})]))
% % \\ = \mathbf{E}\left(\frac{1}{4}\bar{Y}_{HT}^{obs}(1,\mathbf{1})\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \frac{1}{4}\bar{Y}_{HT}^{obs}(1,\mathbf{1})\bar{Y}_{HT}^{obs}(1,\mathbf{0}) + \frac{1}{4}\bar{Y}_{HT}^{obs}(1,\mathbf{1})\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \frac{1}{4}\bar{Y}_{HT}^{obs}(1,\mathbf{1})\bar{Y}_{HT}^{obs}(0,\mathbf{0})\right.
% % \\ \left. - \frac{1}{4}\bar{Y}_{HT}^{obs}(0,\mathbf{1})\bar{Y}_{HT}^{obs}(1,\mathbf{1}) + \frac{1}{4}\bar{Y}_{HT}^{obs}(0,\mathbf{1})\bar{Y}_{HT}^{obs}(1,\mathbf{0})
% % - \frac{1}{4}\bar{Y}_{HT}^{obs}(0,\mathbf{1})\bar{Y}_{HT}^{obs}(0,\mathbf{1})
% % + \frac{1}{4}\bar{Y}_{HT}^{obs}(0,\mathbf{1})\bar{Y}_{HT}^{obs}(0,\mathbf{0})
% % \right.
% % \\ \left.+ \frac{1}{4}\bar{Y}_{HT}^{obs}(1,\mathbf{0})\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \frac{1}{4}\bar{Y}_{HT}^{obs}(1,\mathbf{0})\bar{Y}_{HT}^{obs}(1,\mathbf{0})
% % + \frac{1}{4}\bar{Y}_{HT}^{obs}(1,\mathbf{0})\bar{Y}_{HT}^{obs}(0,\mathbf{1})
% % - \frac{1}{4}\bar{Y}_{HT}^{obs}(1,\mathbf{0})\bar{Y}_{HT}^{obs}(0,\mathbf{0})\right.
% % \\ \left.- \frac{1}{4}\bar{Y}_{HT}^{obs}(0,\mathbf{0})\bar{Y}_{HT}^{obs}(1,\mathbf{1}) + \frac{1}{4}\bar{Y}_{HT}^{obs}(0,\mathbf{0})\bar{Y}_{HT}^{obs}(1,\mathbf{0}) - \frac{1}{4}\bar{Y}_{HT}^{obs}(0,\mathbf{0})\bar{Y}_{HT}^{obs}(0,\mathbf{1}) 
% % +  \frac{1}{4}\bar{Y}_{HT}^{obs}(0,\mathbf{0})\bar{Y}_{HT}^{obs}(0,\mathbf{0})
% % \right) 
% % \\ - \left(\frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))
% % - \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{0})) 
% % + \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))\right. 
% % \\ \left. - \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % - \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))
% % + \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}))\right.
% % \\ \left.- \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% % + \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % + \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}))\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))\right.
% % \\ \left. - \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}))\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % + \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% % - \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))\right.
% % \\ \left. - \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))
% % + \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % - \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))\right.
% % \\ \left. + \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \right)
% % \\ = \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) 
% % - \frac{1}{2}\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))\bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% % + \frac{1}{2}\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))\bar{Y}_{HT}^{obs}(1,\mathbf{0})) 
% % \\ - \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))\bar{Y}_{HT}^{obs}(0,\mathbf{1})) - \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}))\bar{Y}_{HT}^{obs}(1,\mathbf{0})) 
% % + \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))\bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% % \\ - \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) 
% % +  \frac{1}{2}\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % -  \frac{1}{2}\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % \\ +  \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% % +  \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}))\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{0})) 
% %  - \frac{1}{4}\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% %  \\ = \frac{1}{4}\mathbf{Va}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))
% %  - \frac{1}{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% %  + \frac{1}{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % \\ - \frac{1}{4}\mathbf{Va}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% % - \frac{1}{4}\mathbf{Va}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % + \frac{1}{4}\mathbf{Va}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \end{multline}

% % Therefore, 

% % \begin{multline}
% % \mathbf{Var}(\widehat{\delta^*}_{{HT,tot}}) = \mathbf{Var}(\widehat{\delta^*}_{HT,direct} + \widehat{\delta^*}_{HT,ind})
% % \\ = \mathbf{Var}(\widehat{\delta^*}_{HT,direct}) + \mathbf{Var}(\widehat{\delta^*}_{HT,ind}) + 2\mathbf{Cov}(\widehat{\delta^*}_{HT,direct} , \widehat{\delta^*}_{HT,ind})
% % \\ = \frac{1}{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + \frac{1}{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{0})) +
% % \frac{1}{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) + \frac{1}{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \\ - \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) - \mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}), \bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% %  \\ + 2\left( \frac{1}{4}\mathbf{Va}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))
% %  - \frac{1}{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% %  + \frac{1}{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))\mathbf{E}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}))\right.
% % \\ \left.- \frac{1}{4}\mathbf{Va}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% % - \frac{1}{4}\mathbf{Va}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % + \frac{1}{4}\mathbf{Va}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \right)
% % \\ = \mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% %  -2\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \end{multline}









% % We can also find the variance as follows:
% % \begin{multline}
% % \mathbf{Var}(\widehat{\delta^*}_{{HT,tot}}) = \mathbf{Var}(\widehat{\delta^*}_{HT,direct} + \widehat{\delta^*}_{HT,ind}) \\ = \mathbf{Var}(c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1})] +
% % c_{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{0}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})]
% % \\+  c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(1,\mathbf{0})] + c_{2}[\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})])
% % \\ = c^2_{1}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}) + c^2_{1}\mathbf{Var}( \bar{Y}_{HT}^{obs}(0,\mathbf{1}) +
% % c^2_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{0})  + c^2_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})
% % \\+  c^2_{1}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}) + c^2_{1}\mathbf{Var}( \bar{Y}_{HT}^{obs}(1,\mathbf{0}) +
% % c^2_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})  + c^2_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})
% % \\ - 2c^2_{1}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% % + 2c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % \\ - 2c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % + 2c^2_{1}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}))
% % \\- 2c^2_{1}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % + 2c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% % \\ - 2c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \\- 2c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}),\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % \\ + 2c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % - 2c^2_{1}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}),\bar{Y}_{HT}^{obs}(1,\mathbf{1}))
% % \\+ 2c^2_{1}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}),\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % - 2c_{1}c_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% % \\ +  2c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \\ - 2c^2_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % + 2c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}),\bar{Y}_{HT}^{obs}(1,\mathbf{1}))
% % \\- 2c_{1}c_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}),\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % + 2c^2_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}),\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% % \\ -  2c^2_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \\ - 2c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}),\bar{Y}_{HT}^{obs}(1,\mathbf{1}))
% % \\+ 2c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}),\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % - 2c^2_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}),\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% % \\ +  2c^2_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \\- 2c^2_{1}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % + 2c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% % \\ - 2c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \\- 2c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}),\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% % \\ + 2c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% %  - 2c^2_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% %  \\
% % \end{multline}

% % This can be simplified as follows
% % \begin{multline}
% % \mathbf{Var}(\widehat{\delta^*}_{{HT,tot}}) = \mathbf{Var}(\widehat{\delta^*}_{HT,direct} + \widehat{\delta^*}_{HT,ind}) \\ = \mathbf{Var}(c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1})] +
% % c_{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{0}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})]
% % \\+  c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(1,\mathbf{0})] + c_{2}[\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})])
% % \\ = 4c^2_{1}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}) + c^2_{1}\mathbf{Var}( \bar{Y}_{HT}^{obs}(0,\mathbf{1}) +
% % c^2_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{0})  + 4c^2_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})
% % \\+ c^2_{1}\mathbf{Var}( \bar{Y}_{HT}^{obs}(1,\mathbf{0}) +
% % c^2_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})  
% % \\ - 4c^2_{1}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% % + 4c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % \\ - 8c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % - 2c^2_{1}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % \\+ 4c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% % - 4c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}),\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % \\ + 4c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % + 2c^2_{1}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}),\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % \\- 2c_{1}c_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% % - 4c^2_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \\- 2c_{1}c_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % + 2c^2_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}),\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% % \\+ 4c_{1}c_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}),\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% % - 4c^2_{2}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}),\bar{Y}_{HT}^{obs}(0,\mathbf{1}))
% % \\- 2c^2_{1}\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(1,\mathbf{0}))
% %  \end{multline}

% % and for $c_{1} + c_{2} = \frac{1}{2}$,

% % \begin{multline}
% % \mathbf{Var}(\widehat{\delta^*}_{{HT,tot}}) = \mathbf{Var}(\widehat{\delta}_{{HT,tot}})
% % \\ = \mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% %  -2\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}),\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% % \end{multline}

% \subsection{Properties of ADE Estimator under the No Weak Interaction between Direct and Indirect Effects Assumption}
% \label{AppendixB.6}

% First, we compute the expected value and variance of the unbiased Horvitz Thompson estimator $\widehat{\delta^*}_{HT,dir}$ under the no-interaction between direct and indirect effects assumption.

% For $c_{1} + c_{2} = 1$,


% \begin{equation}\label{Assumption2directAppB}
% \widehat{\delta^*}_{HT, dir} =  c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1})] +
% c_{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{0}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})]
% \end{equation} 


% % Note that under the no-interaction between direct and indirect effects assumption and for $c_{1} + c_{2} = 1$, $c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1})] =
% % c_{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{0}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})]$ and we have the following.

% \subsubsection{The Expected Value of ADE Estimator}
% \label{makereferenceB.6.1}

% \begin{multline} 
% \mathbf{E}(\widehat{\delta^*}_{HT, dir}) =  \mathbf{E}(c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1})] +
% c_{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{0}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})])
% \\ = \mathbf{E}(c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1})]) +
% \mathbf{E}(c_{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{0}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})])
% \\= c_{1}[\bar{y}(1,\mathbf{1}) -  \bar{y}(0,\mathbf{1})]
% + c_{2}[\bar{y}(1,\mathbf{0}) -  \bar{y}(0,\mathbf{0})]
% = \delta_{dir}
% \end{multline} 

% Hence, $\widehat{\delta^*}_{HT, dir}$ is an unbiased estimator for $\delta_{dir}$.

% \subsubsection{The Variance of ADE Estimator}
% \label{makereferenceB.6.2}




% The variance can simply be computed using the property that $\mathbf{Var(\sum_{i = 1}^{N}a_{i}X_{i})} =\sum_{i=1}^{N}a_{i}\mathbf{Var(X_{i})} + 2\sum_{j\ne i}a_{i}a_{j}\mathbf{Cov}(X_{i}X_{j})$ as follows.




% \begin{multline}
% \mathbf{Var}(\widehat{\delta^{*}}_{HT,dir}) =  \mathbf{Var}(c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{1})] +
% c_{2}[\bar{Y}_{HT}^{obs}(1,\mathbf{0}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})])
% %\\ = \mathbf{Var}(c_{\ell1}\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - c_{\ell1}\bar{Y}_{HT}^{obs}(0,\mathbf{1}) +
% %c_{\ell2}\bar{Y}_{HT}^{obs}(1,\mathbf{0}) - c_{\ell2}\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% \\ = c^2_{1}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + c^2_{1}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) +
% c^2_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}) + c^2_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% \\ -2c^2_{1} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% +2c_{1}c_{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(1,\mathbf{0})) 
% \\ -2c_{1}c_{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% -2c_{1}c_{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}), \bar{Y}_{HT}^{obs}(1,\mathbf{0})) 
% \\ +2c_{1}c_{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% -2c^2_{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% \\ =  \frac{c^2_{1}}{N^2}\sum_{i = 1}^{N}\pi_{i}(1,\mathbf{1})[1 - \pi_{i}(1,\mathbf{1})]\left[ \frac{y_{i}(1,\mathbf{1})}{\pi_{i}(1,\mathbf{1})}\right]^2
% \\  + \frac{c^2_{1}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(1,\mathbf{1}) - \pi_{i}(1,\mathbf{1})\pi_{j}(1,\mathbf{1})]\frac{y_{i}(1,\mathbf{1})y_{j}(1,\mathbf{1})}{\pi_{i}(1,\mathbf{1})\pi_{j}(1,\mathbf{1})}.
% \\ +  \frac{c^2_{1}}{N^2}\sum_{i = 1}^{N}\pi_{i}(0,\mathbf{1})[1 - \pi_{i}(0,\mathbf{1})]\left[ \frac{y_{i}(0,\mathbf{1})}{\pi_{i}(0,\mathbf{1})}\right]^2
% \\  + \frac{c^2_{1}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(0,\mathbf{1}) - \pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{1})]\frac{y_{i}(0,\mathbf{1})y_{j}(0,\mathbf{1})}{\pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{1})}
% \\ +  \frac{c^2_{2}}{N^2}\sum_{i = 1}^{N}\pi_{i}(1,\mathbf{0})[1 - \pi_{i}(1,\mathbf{0})]\left[ \frac{y_{i}(1,\mathbf{0})}{\pi_{i}(1,\mathbf{0})}\right]^2
% \\  + \frac{c^2_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(1,\mathbf{0}) - \pi_{i}(1,\mathbf{0})\pi_{j}(1,\mathbf{0})]\frac{y_{i}(1,\mathbf{0})y_{j}(1,\mathbf{0})}{\pi_{i}(1,\mathbf{0})\pi_{j}(1,\mathbf{0})}.
% \\ +  \frac{c^2_{2}}{N^2}\sum_{i = 1}^{N}\pi_{i}(0,\mathbf{0})[1 - \pi_{i}(0,\mathbf{0})]\left[ \frac{y_{i}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{0})}\right]^2
% \\  + \frac{c^2_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(0,\mathbf{0}) - \pi_{i}(0,\mathbf{0})\pi_{j}(0,\mathbf{0})]\frac{y_{i}(0,\mathbf{0})y_{j}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{0})\pi_{j}(0,\mathbf{0})}
% \\ - 2 \left( \frac{c^2_{1}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,\mathbf{1}),(0,\mathbf{1})) - \pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{1})\right] \frac{y_{i}(1,\mathbf{1})y_{j}(0,\mathbf{1})}{\pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{1})}\right.
% \\ \left.- \frac{c^2_{1}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,\mathbf{1})y_{i}(0,\mathbf{1})\right)
% \\ + 2 \left( \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,\mathbf{1}),(1,\mathbf{0})) - \pi_{i}(1,\mathbf{1})\pi_{j}(1,\mathbf{0})\right] \frac{y_{i}(1,\mathbf{1})y_{j}(1,\mathbf{0})}{\pi_{i}(1,\mathbf{1})\pi_{j}(1,\mathbf{0})}\right.
% \\ \left.- \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,\mathbf{1})y_{i}(1,\mathbf{0})\right)
% .
% \end{multline}



% \begin{multline}
% \\ - 2 \left( \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,\mathbf{1}),(0,\mathbf{0})) - \pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{0})\right] \frac{y_{i}(1,\mathbf{1})y_{j}(0,\mathbf{0})}{\pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{0})}\right.
% \\ \left.- \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,\mathbf{1})y_{i}(0,\mathbf{0})\right)
% \\ - 2 \left( \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((0,\mathbf{1}),(1,\mathbf{0})) - \pi_{i}(0,\mathbf{1})\pi_{j}(1,\mathbf{0})\right] \frac{y_{i}(0,\mathbf{1})y_{j}(1,\mathbf{0})}{\pi_{i}(0,\mathbf{1})\pi_{j}(1,\mathbf{0})}\right.
% \\ \left.- \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}  y_{i}(0,\mathbf{1})y_{i}(1,\mathbf{0})\right)
% \\ + 2 \left( \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((0,\mathbf{1}),(0,\mathbf{0})) - \pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{0})\right] \frac{y_{i}(0,\mathbf{1})y_{j}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{0})}\right.
% \\ \left.- \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}  y_{i}(0,\mathbf{1})y_{i}(0,\mathbf{0})\right)
% \\ - 2 \left( \frac{c^2_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,\mathbf{0}),(0,\mathbf{0})) - \pi_{i}(1,\mathbf{0})\pi_{j}(0,\mathbf{0})\right] \frac{y_{i}(1,\mathbf{0})y_{j}(0,\mathbf{0})}{\pi_{i}(1,\mathbf{0})\pi_{j}(0,\mathbf{0})}\right.
% \\ \left.- \frac{c^2_{2}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,\mathbf{0})y_{i}(0,\mathbf{0})\right)
% \end{multline}



% \subsection{Properties of AIE Estimator under the No Weak Interaction between Direct and Indirect Effects Assumption}
% \label{AppendixB.7}

% For $c_{1} + c_{2} = 1$,

% \begin{equation}\label{Assumption2indirect}
% \widehat{\delta^*}_{HT,ind} =  c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(1,\mathbf{0})] +
% c_{2}[\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})]
% \end{equation} 


% % Note that under the no weak interaction between direct and indirect effects assumption and for for $c_{1} + c_{2} = 1$, $c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(1,\mathbf{0})] =
% % c_{2}[\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})]$ and we have the following.




% \subsubsection{The Expected Value of AIE Estiamtor}
% \label{makereferenceB.7.1}

% \begin{multline} 
% \mathbf{E}(\widehat{\delta^*}_{HT,ind}) =  \mathbf{E}(c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(1,\mathbf{0})] +
% c_{2}[\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})])
% \\ = \mathbf{E}(c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(1,\mathbf{0})]) +
% \mathbf{E}(c_{2}[\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})])
% \\= c_{1}[\bar{y}(1,\mathbf{1}) -  \bar{y}(1,\mathbf{0})]
% + c_{2}[\bar{y}(0,\mathbf{1}) -  \bar{y}(0,\mathbf{0})]
% = \delta_{ind}
% \end{multline} 

% Hence, $\widehat{\delta^*}_{HT,ind}$ is an unbiased estimator for $\delta_{ind}$.


% \subsubsection{The Variance of AIE Estiamtor}
% \label{makereferenceB.7.2}
% The variance can simply be computed using the property that $\mathbf{Var(\sum_{i = 1}^{N}a_{i}X_{i})} =\sum_{i=1}^{N}a_{i}\mathbf{Var(X_{i})} + 2\sum_{j\ne i}a_{i}a_{j}\mathbf{Cov}(X_{i}X_{j})$ as follows.




% \begin{multline}
% \mathbf{Var}(\widehat{\delta^{*}}_{HT,ind}) =  \mathbf{Var}(c_{1}[\bar{Y}_{HT}^{obs}(1,\mathbf{1}) - \bar{Y}_{HT}^{obs}(1,\mathbf{0})] +
% c_{2}[\bar{Y}_{HT}^{obs}(0,\mathbf{1}) - \bar{Y}_{HT}^{obs}(0,\mathbf{0})])
% %\\ = \mathbf{Var}(c_{\ell1}\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - c_{\ell1}\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1}) +
% %c_{\ell2}\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - %c_{\ell2}\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1}))
% \\ = c^2_{1}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{1})) + c^2_{1}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,\mathbf{0})) +
% c^2_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{1})) + c^2_{2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,\mathbf{0}))
% \\ -2c^2_{1} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(1,\mathbf{0})) 
% + 2c_{1}c_{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% \\ -2c_{1}c_{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% -2c_{1}c_{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}), \bar{Y}_{HT}^{obs}(0,\mathbf{1})) 
% \\ + 2c_{1}c_{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,\mathbf{0}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% -2c^2_{2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,\mathbf{1}), \bar{Y}_{HT}^{obs}(0,\mathbf{0})) 
% \\ =  \frac{c^2_{1}}{N^2}\sum_{i = 1}^{N}\pi_{i}(1,\mathbf{1})[1 - \pi_{i}(1,\mathbf{1})]\left[ \frac{y_{i}(1,\mathbf{1})}{\pi_{i}(1,\mathbf{1})}\right]^2
% \\  + \frac{c^2_{1}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(1,\mathbf{1}) - \pi_{i}(1,\mathbf{1})\pi_{j}(1,\mathbf{1})]\frac{y_{i}(1,\mathbf{1})y_{j}(1,\mathbf{1})}{\pi_{i}(1,\mathbf{1})\pi_{j}(1,\mathbf{1})}.
% \\ +  \frac{c^2_{1}}{N^2}\sum_{i = 1}^{N}\pi_{i}(1,\mathbf{0})[1 - \pi_{i}(1,\mathbf{0})]\left[ \frac{y_{i}(1,\mathbf{0})}{\pi_{i}(1,\mathbf{0})}\right]^2
% \\  + \frac{c^2_{1}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(1,\mathbf{0}) - \pi_{i}(1,\mathbf{0})\pi_{j}(1,\mathbf{0})]\frac{y_{i}(1,\mathbf{0})y_{j}(1,\mathbf{0})}{\pi_{i}(1,\mathbf{0})\pi_{j}(1,\mathbf{0})}
% \\ +  \frac{c^2_{2}}{N^2}\sum_{i = 1}^{N}\pi_{i}(0,\mathbf{1})[1 - \pi_{i}((0,\mathbf{1})]\left[ \frac{y_{i}(0,\mathbf{1})}{\pi_{i}(0,\mathbf{1})}\right]^2
% \\  + \frac{c^2_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(0,\mathbf{1}) - \pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{1})]\frac{y_{i}(0,\mathbf{1})y_{j}(0,\mathbf{1})}{\pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{1})}
% \\ +  \frac{c^2_{2}}{N^2}\sum_{i = 1}^{N}\pi_{i}(0,\mathbf{0})[1 - \pi_{i}(0,\mathbf{0})]\left[ \frac{y_{i}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{0})}\right]^2
% \\  + \frac{c^2_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(0,\mathbf{0}) - \pi_{i}(0,\mathbf{0})\pi_{j}(0,\mathbf{0})]\frac{y_{i}(0,\mathbf{0})y_{j}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{0})\pi_{j}(0,\mathbf{0})}
% \\ - 2 \left( \frac{c^2_{1}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,\mathbf{1}),(1,\mathbf{0})) - \pi_{i}(1,\mathbf{1})\pi_{j}(1,\mathbf{0})\right] \frac{y_{i}(1,\mathbf{1})y_{j}(1,\mathbf{0})}{\pi_{i}(1,\mathbf{1})\pi_{j}(1,\mathbf{0})}\right.
% \\ \left.- \frac{c^2_{1}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,\mathbf{1})y_{i}(1,\mathbf{0})\right)
% \\ + 2 \left( \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,\mathbf{1}),(0,\mathbf{1})) - \pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{1})\right] \frac{y_{i}(1,\mathbf{1})y_{j}(0,\mathbf{1})}{\pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{1})}\right.
% \\ \left.- \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,\mathbf{1})y_{i}(0,\mathbf{1})\right)
% .
% \end{multline}



% \begin{multline}
% \\ - 2 \left( \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,\mathbf{1}),(0,\mathbf{0})) - \pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{0})\right] \frac{y_{i}(1,\mathbf{1})y_{j}(0,\mathbf{0})}{\pi_{i}(1,\mathbf{1})\pi_{j}(0,\mathbf{0})}\right.
% \\ \left.- \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,\mathbf{1})y_{i}(0,\mathbf{0})\right)
% \\ - 2 \left( \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,\mathbf{0}),(0,\mathbf{1})) - \pi_{i}(1,\mathbf{0})\pi_{j}(0,\mathbf{1})\right] \frac{y_{i}(1,\mathbf{0})y_{j}(0,\mathbf{1})}{\pi_{i}(1,\mathbf{0})\pi_{j}(0,\mathbf{1})}\right.
% \\ \left.- \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,\mathbf{0})y_{i}(0,\mathbf{1})\right)
% \\ + 2 \left( \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,\mathbf{0}),(0,\mathbf{0})) - \pi_{i}(1,\mathbf{0})\pi_{j}(0,\mathbf{0})\right] \frac{y_{i}(1,\mathbf{0})y_{j}(0,\mathbf{0})}{\pi_{i}(1,\mathbf{0})\pi_{j}(0,\mathbf{0})}\right.
% \\ \left.- \frac{c_{1}c_{2}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,\mathbf{0})y_{i}(0,\mathbf{0})\right)
% \\ - 2 \left( \frac{c^2_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((0,\mathbf{1}),(0,\mathbf{0})) - \pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{0})\right] \frac{y_{i}(0,\mathbf{1})y_{j}(0,\mathbf{0})}{\pi_{i}(0,\mathbf{1})\pi_{j}(0,\mathbf{0})}\right.
% \\ \left.- \frac{c^2_{2}}{N^2}\sum_{i = 1}^{N}  y_{i}(0,\mathbf{1})y_{i}(0,\mathbf{0})\right)
% \end{multline}



% \subsection{Properties of A$\ell$NNIE Estimator under the No Weak Interaction between Direct and Indirect Effects Assumption}
% \label{AppendixB.8}



% % Next, we compute the expected value and variance of the unbiased Horvitz Thompson estimator $\widehat{\delta^*}_{{HT,\ell}}$ under the no-interaction between direct and indirect effects assumption.


% For $c_{\ell1} + c_{\ell2} = 1$,
% \begin{equation}\label{Assumption2Estimator}
% \widehat{\delta^*}_{{HT,\ell}} =  c_{\ell1}[\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})] +
% c_{\ell2}[\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})]
% \end{equation} 

% \begin{lemma}\label{lemma4B}
% \begin{equation}\label{indirectass3A}
%     \widehat{\delta^{*}}_{HT,ind} =  \sum_{\ell =1}^{K}\widehat{\delta^{*}}_{HT,\ell}
% \end{equation}
% \end{lemma}

% \begin{proof}

% Under the no-interaction between direct and indirect effects assumption, %and for $c_{\ell1} + c_{\ell2} = 1$,
% %\begin{align*}
% %c_{\ell1}[\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})] =
% %c_{\ell2}[\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})]
% %\end{align*}

 
% \begin{align*}
% \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1}) =
% \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})
% = \widehat{\delta}_{{HT,\ell}}
% \end{align*}
% where $W^{*}_{\ell}\in\{0,1\}^K$ is a vector for the treatment assignment of the K-nearest neighbors with the first $\ell$ nearest neighbors are treated and the rest are control and $W^{*}_{\ell-1}\in\{0,1\}^K$ is the vector for the treatment assignment of the K-nearest neighbors where the first $\ell-1$ nearest neighbors are treated and the rest are control. 
% Since $c_{\ell1} + c_{\ell2} = 1$, the proof follows by lemma \ref{lemma2B}. 

% \end{proof}




% \subsubsection{The Expected Value of A$\ell$NNIE Estimator}
% \label{makereferenceB.8.1}


% \begin{multline} 
% \mathbf{E}(\widehat{\delta^*}_{{HT,\ell}}) =  \mathbf{E}(c_{\ell1}[\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})] +
% c_{\ell2}[\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})])
% \\ = \mathbf{E}(c_{\ell1}[\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})]) +
% \mathbf{E}(c_{\ell2}[\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})])
% \\= c_{\ell1}[\bar{y}(1,W^{*}_{\ell}) -  \bar{y}(1,W^{*}_{\ell-1})]
% + c_{\ell2}[\bar{y}(0,W^{*}_{\ell}) -  \bar{y}(0,W^{*}_{\ell-1})]
% = \delta_{\ell}
% \end{multline} 

% Hence, $\widehat{\delta^*}_{{HT,\ell}}$ is an unbiased estimator for $\delta_{\ell}$.


% \subsubsection{The Variance of A$\ell$NNIE Estimator}
% \label{makereferenceB.8.2}


% The variance can be computed where we first find $\mathbf{Var}(c_{\ell1}[\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})])$, $\mathbf{Var}(c_{\ell2}[\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})])$ and $\mathbf{Cov}(c_{\ell1}[\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})],c_{\ell2}[\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})])$ as follows.


% First we find:
% %Now, we find the expected value and the variance of $[\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})]$ using the property $\mathbf{Var}(X-Y)$ = $\mathbf{Var}(X)$ + $\mathbf{Var}(Y)$ -2$\mathbf{Cov}(X,Y)$ and $\mathbf{Cov}(X,Y)$ = $\mathbf{E}(XY) - \mathbf{E}(X)\mathbf{E}(Y)$.

% %Note the following expectations: 

% %\begin{multline}
% %\mathbf{E}([\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})]) 
% %=[\mathbf{E}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})) - %\mathbf{E}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1}))] 
% %= \bar{y}(1,W^{*}_{\ell}) -  \bar{y}(1,W^{*}_{\ell-1}).
% %\end{multline}

% %\begin{multline}
% %\left(\mathbf{E}([\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})])\right)^2 
% %=\left(\bar{y}(1,W^{*}_{\ell}) -  \bar{y}(1,W^{*}_{\ell-1}))\right)^2 
% %\\ = \left(\bar{y}(1,W^{*}_{\ell})\right)^2 + \left(\bar{y}(1,W^{*}_{\ell-1})\right)^2 - 2\left(\bar{y}(1,W^{*}_{\ell})\right)\left(\bar{y}(1,W^{*}_{\ell-1})\right).
% %\end{multline}



% %First, we find $\mathbf{E}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1}))$ and $\mathbf{E}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}))\mathbf{E}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1}))$:

% %\begin{multline}
% %\mathbf{E}\left[\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})\right] 
% %\\ \mathbf{E}\left[\left(\frac{1}{N}\sum_{i = 1}^{N} I_{i}(1,W^{*}_{\ell}) \frac{Y_{i}^{obs}(1,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell})}\right)
% %\left(\frac{1}{N}\sum_{i = 1}^{N} I_{i}(1,W^{*}_{\ell-1}) \frac{Y_{i}^{obs}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell-1})}\right)\right]
% %\\ = \mathbf{E}\left[\left(\frac{1}{N^2}\sum_{i = 1}^{N} I_{i}(1,W^{*}_{\ell})I_{i}(1,W^{*}_{\ell-1}) \frac{Y_{i}^{obs}(1,W^{*}_{\ell})Y_{i}^{obs}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell})\pi_{i}(1,W^{*}_{\ell-1})}\right)\right.
% %\\ \left.+ \left(\frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} I_{i}(1,W^{*}_{\ell})I_{j}(1,W^{*}_{\ell-1}) \frac{Y_{i}^{obs}(1,W^{*}_{\ell})Y_{j}^{obs}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell-1})}\right)\right]
% %\\ = \mathbf{E}\left[\left(\frac{1}{N^2}\sum_{i = 1}^{N} I_{i}(1,W^{*}_{\ell})I_{i}(1,W^{*}_{\ell-1}) \frac{Y_{i}^{obs}(1,W^{*}_{\ell})Y_{i}^{obs}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell})\pi_{i}(1,W^{*}_{\ell-1})}\right)\right]
% %\\ + \mathbf{E}\left[\left(\frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} I_{i}(1,W^{*}_{\ell})I_{j}(1,W^{*}_{\ell-1}) \frac{Y_{i}^{obs}(1,W^{*}_{\ell})Y_{j}^{obs}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell-1})}\right)\right]
% %\\ = \left(\frac{1}{N^2}\sum_{i = 1}^{N} \mathbf{E}\left[I_{i}(1,W^{*}_{\ell})I_{i}(1,W^{*}_{\ell-1})\right] \frac{y_{i}(1,W^{*}_{\ell})y_{i}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell})\pi_{i}(1,W^{*}_{\ell-1})}\right)
% %\\ + \left(\frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \mathbf{E}\left[I_{i}(1,W^{*}_{\ell})I_{j}(1,W^{*}_{\ell-1})\right] \frac{y_{i}(1,W^{*}_{\ell})y_{i}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell-1})}\right)
% %\\ = \frac{1}{N^2}\sum_{i = 1}^{N} \pi_{i}((1,W^{*}_{\ell}),(1,W^{*}_{\ell-1})) \frac{y_{i}(1,W^{*}_{\ell})y_{i}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell})\pi_{i}(1,W^{*}_{\ell-1})}
% %\\ + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((1,W^{*}_{\ell}),(1,W^{*}_{\ell-1})) \frac{y_{i}(1,W^{*}_{\ell})y_{j}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell-1})}
% %\\ = \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((1,W^{*}_{\ell}),(1,W^{*}_{\ell-1})) \frac{y_{i}(1,W^{*}_{\ell})y_{j}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell-1})}
% %\end{multline}





% %\begin{multline}
% %\mathbf{E}([\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})]^2) =
% %\\ \mathbf{E}\left(\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\right)^2 + \left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})\right)^2 
% % - 2\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\right)\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})\right)\right)
% %\\= \mathbf{E}\left(\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\right)^2\right) +  \mathbf{E}\left(\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})\right)^2\right) 
% % - 2\mathbf{E}\left(\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\right)\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})\right)\right)
% % \\ = \frac{1}{N^2}\sum_{i = 1}^{N} \frac{y^2_{i}(1,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell})}
% %+ \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}(1,W^{*}_{\ell}) \frac{y_{i}(1,W^{*}_{\ell})y_{j}(1,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell})}
% % \\ + \frac{1}{N^2}\sum_{i = 1}^{N} \frac{y^2_{i}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell-1})}
% %+ \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}(1,W^{*}_{\ell-1}) \frac{y_{i}(1,W^{*}_{\ell-1})y_{j}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(1,W^{*}_{\ell-1})}
% %\\ -2 \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((1,W^{*}_{\ell}),(1,W^{*}_{\ell-1})) \frac{y_{i}(1,W^{*}_{\ell})y_{j}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell-1})}
% %\end{multline}


% %\begin{multline}
% %\mathbf{E}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}))\mathbf{E}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})) = 
% %\\ \mathbf{E}\left[\frac{1}{N}\sum_{i = 1}^{N} I_{i}(1,W^{*}_{\ell}) \frac{Y_{i}^{obs}(1,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell})}\right]
% %\mathbf{E}\left[\frac{1}{N}\sum_{i = 1}^{N} I_{i}(1,W^{*}_{\ell-1}) \frac{Y_{i}^{obs}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell-1})}\right]
% %\\= \left[\frac{1}{N}\sum_{i = 1}^{N}  y_{i}(1,W^{*}_{\ell})\right]
% %\left[\frac{1}{N}\sum_{i = 1}^{N}  y_{i}(1,W^{*}_{\ell-1})\right]
% %\\ = \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}(1,W^{*}_{\ell})y_{i}(1,W^{*}_{\ell-1}) + \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} y_{i}(1,W^{*}_{\ell})y_{j}(1,W^{*}_{\ell-1})
% %\end{multline}



% %\begin{multline}
% %\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}),\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})) = \mathbf{E}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})) - \mathbf{E}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}))\mathbf{E}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1}))
% %\\ = \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((1,W^{*}_{\ell}),(1,W^{*}_{\ell-1})) \frac{y_{i}(1,W^{*}_{\ell})y_{j}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell-1})}
% %\\ -  \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}(1,W^{*}_{\ell})y_{i}(1,W^{*}_{\ell-1}) - \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i} y_{i}(1,W^{*}_{\ell})y_{j}(1,W^{*}_{\ell-1})
% %\\ = \frac{1}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,W^{*}_{\ell}),(1,W^{*}_{\ell-1})) - \pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell-1})\right] \frac{y_{i}(1,W^{*}_{\ell})y_{j}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell-1})} 
% %\\ - \frac{1}{N^2}\sum_{i = 1}^{N}  y_{i}(1,W^{*}_{\ell})y_{i}(1,W^{*}_{\ell-1}).
% %\end{multline}


% \begin{multline}
% \mathbf{Var}(c_{\ell1}[\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})]) = C^{2}_{1}\left(\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})) + \mathbf{Var}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})) \right.
% \\ \left. -2\mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}),\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})\right)
% \\ =  \frac{C^{2}_{1}}{N^2}\sum_{i = 1}^{N}\pi_{i}(1,W_{\ell})[1 - \pi_{i}(1,W_{\ell})]\left[ \frac{y_{i}(1,W_{\ell})}{\pi_{i}(1,W_{\ell})}\right]^2
% \\   + \frac{C^{2}_{1}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(1,W_{\ell}) - \pi_{i}(1,W_{\ell})\pi_{j}(1,W_{\ell})]\frac{y_{i}(1,W_{\ell})y_{j}(1,W_{\ell})}{\pi_{i}(1,W_{\ell})\pi_{j}(1,W_{\ell})}
% \\  +  \frac{C^{2}_{1}}{N^2}\sum_{i = 1}^{N}\pi_{i}(1,W_{\ell-1})[1 - \pi_{i}(1,W_{\ell-1})]\left[ \frac{y_{i}(1,W_{\ell-1})}{\pi_{i}(1,W_{\ell-1})}\right]^2 
% \\  + \frac{C^{2}_{1}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(1,W_{\ell-1}) - \pi_{i}(1,W_{\ell-1})\pi_{j}(1,W_{\ell-1})]\frac{y_{i}(1,W_{\ell-1})y_{j}(1,W_{\ell-1})}{\pi_{i}(1,W_{\ell-1})\pi_{j}(1,W_{\ell-1})}
% \\  - 2 \left( \frac{C^{2}_{1}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,W^{*}_{\ell}),(1,W^{*}_{\ell-1})) - \pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell-1})\right] \frac{y_{i}(1,W^{*}_{\ell})y_{j}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell-1})} \right.
% \\  \left.- \frac{C^{2}_{1}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,W^{*}_{\ell})y_{i}(1,W^{*}_{\ell-1})\right).
% \end{multline}





% Next, we find $\mathbf{Cov}(c_{\ell1}[\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})],c_{\ell2}[\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})])$. 
% Note that:


% \begin{multline}
% \mathbf{E}\left[c_{\ell1}[\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})]c_{\ell2}[\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})]\right] 
% \\  = c_{\ell1}c_{\ell2}\mathbf{E}\left[\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell})\right)-\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})\right)-\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell})\right) \right.
% \\ \left. + \left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})\right)\right]
% \\ = c_{\ell1}c_{\ell2}\left[\mathbf{E}\left[\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell})\right)\right]-\mathbf{E}\left[\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})\right)\right]\right.
% \\\left.-\mathbf{E}\left[\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell})\right)\right] 
% + \mathbf{E}\left[\left(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})\right)\right]
% \right]
% \\ = \frac{c_{\ell1}c_{\ell2}}{N^2}\left(\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((1,W^{*}_{\ell}),(0,W^{*}_{\ell})) \frac{y_{i}(1,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell})}\right)
% \\ - \frac{c_{\ell1}c_{\ell2}}{N^2}\left(\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((1,W^{*}_{\ell}),(0,W^{*}_{\ell-1})) \frac{y_{i}(1,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell-1})}\right)
% \\ - \frac{c_{\ell1}c_{\ell2}}{N^2}\left(\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((1,W^{*}_{\ell-1}),(0,W^{*}_{\ell})) \frac{y_{i}(1,W^{*}_{\ell-1})y_{j}(0,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(0,W^{*}_{\ell})}\right)
% \\ + \frac{c_{\ell1}c_{\ell2}}{N^2}\left(\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((1,W^{*}_{\ell-1}),(0,W^{*}_{\ell-1})) \frac{y_{i}(1,W^{*}_{\ell-1})y_{j}(0,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(0,W^{*}_{\ell-1})}\right)
% \end{multline}





% \begin{multline}
% \mathbf{E}(c_{\ell1}[\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})])\mathbf{E}(c_{\ell2}[\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})]) 
% \\ =  c_{\ell1}\left[\bar{y}(1,W^{*}_{\ell}) -  \bar{y}(1,W^{*}_{\ell-1})\right]c_{\ell2}\left[\bar{y}(0,W^{*}_{\ell}) -  \bar{y}(0,W^{*}_{\ell-1})\right]
% \\ =  c_{\ell1}c_{\ell2}\left[\bar{y}(1,W^{*}_{\ell})\bar{y}(0,W^{*}_{\ell})
% - \bar{y}(1,W^{*}_{\ell})\bar{y}(0,W^{*}_{\ell-1})
% - \bar{y}(1,W^{*}_{\ell-1})\bar{y}(0,W^{*}_{\ell}) + \bar{y}(1,W^{*}_{\ell-1})\bar{y}(0,W^{*}_{\ell-1})\right]
% \\  =  \left(\frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,W^{*}_{\ell})\sum_{i = 1}^{N}  y_{i}(0,W^{*}_{\ell})\right)
% - \left(\frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,W^{*}_{\ell})\frac{1}{N}\sum_{i = 1}^{N}  y_{i}(0,W^{*}_{\ell-1})\right)
% \\ - \left(\frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,W^{*}_{\ell-1})\frac{1}{N}\sum_{i = 1}^{N}  y_{i}(0,W^{*}_{\ell})\right) 
% + \left(\frac{1}{N}\sum_{i = 1}^{N}  y_{i}(1,W^{*}_{\ell-1})\frac{1}{N}\sum_{i = 1}^{N}  y_{i}(0,W^{*}_{\ell-1})\right)
% \\ =  \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N} y_{i}(1,W^{*}_{\ell})y_{i}(0,W^{*}_{\ell}) 
%  + \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}y_{i}(1,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell})
% \\ - \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N} y_{i}(1,W^{*}_{\ell})y_{i}(0,W^{*}_{\ell-1}) 
% - \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}y_{i}(1,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell-1})
% \\ - \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N} y_{i}(1,W^{*}_{\ell-1})y_{i}(0,W^{*}_{\ell}) 
% - \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}y_{i}(1,W^{*}_{\ell-1})y_{j}(0,W^{*}_{\ell})
% \\ + \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N} y_{i}(1,W^{*}_{\ell-1})y_{i}(0,W^{*}_{\ell-1}) 
% + \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}y_{i}(1,W^{*}_{\ell-1})y_{j}(0,W^{*}_{\ell-1})
% \end{multline}







% \begin{multline}
% \mathbf{Cov}(c_{\ell1}[\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})],c_{\ell2}[\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})]) = \\\mathbf{E}(c_{\ell1}[\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})]c_{\ell2}[\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})]) 
% \\- \mathbf{E}(c_{\ell1}[\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})])\mathbf{E}(c_{\ell2}[\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})])
% \\ = \frac{c_{\ell1}c_{\ell2}}{N^2}\left(\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((1,W^{*}_{\ell}),(0,W^{*}_{\ell})) \frac{y_{i}(1,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell})}\right)
% \\ - \frac{c_{\ell1}c_{\ell2}}{N^2}\left(\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((1,W^{*}_{\ell}),(0,W^{*}_{\ell-1})) \frac{y_{i}(1,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell-1})}\right)
% \\ - \frac{c_{\ell1}c_{\ell2}}{N^2}\left(\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((1,W^{*}_{\ell-1}),(0,W^{*}_{\ell})) \frac{y_{i}(1,W^{*}_{\ell-1})y_{j}(0,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(0,W^{*}_{\ell})}\right)
% \\ + \frac{c_{\ell1}c_{\ell2}}{N^2}\left(\sum_{i = 1}^{N}\sum_{j \ne i} \pi_{ij}((1,W^{*}_{\ell-1}),(0,W^{*}_{\ell-1})) \frac{y_{i}(1,W^{*}_{\ell-1})y_{j}(0,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(0,W^{*}_{\ell-1})}\right)
% \\ -  \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N} y_{i}(1,W^{*}_{\ell})y_{i}(0,W^{*}_{\ell}) 
%  - \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}y_{i}(1,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell})
% \\ + \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N} y_{i}(1,W^{*}_{\ell})y_{i}(0,W^{*}_{\ell-1}) 
% + \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}y_{i}(1,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell-1})
% \\ + \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N} y_{i}(1,W^{*}_{\ell-1})y_{i}(0,W^{*}_{\ell}) 
% + \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}y_{i}(1,W^{*}_{\ell-1})y_{j}(0,W^{*}_{\ell})
% \\ - \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N} y_{i}(1,W^{*}_{\ell-1})y_{i}(0,W^{*}_{\ell-1}) 
% - \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}y_{i}(1,W^{*}_{\ell-1})y_{j}(0,W^{*}_{\ell-1})
% \end{multline}



% We can simplify the covariance further as follows:

% \begin{multline}
% \mathbf{Cov}(c_{\ell1}[\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})],c_{\ell2}[\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})]) 
% \\ = \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}((1,W^{*}_{\ell}),(0,W^{*}_{\ell})) - \pi_{i}(1,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell})]\frac{y_{i}(1,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell})}
% \\  -  \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N} y_{i}(1,W^{*}_{\ell})y_{i}(0,W^{*}_{\ell})
% \\ - \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}((1,W^{*}_{\ell}),(0,W^{*}_{\ell-1})) - \pi_{i}(1,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell-1})]\frac{y_{i}(1,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell-1})}
% \\  +  \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N} y_{i}(1,W^{*}_{\ell})y_{i}(0,W^{*}_{\ell-1}) 
% \\ - \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}((1,W^{*}_{\ell-1}),(0,W^{*}_{\ell})) - \pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(0,W^{*}_{\ell})]\frac{y_{i}(1,W^{*}_{\ell-1})y_{j}(0,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(0,W^{*}_{\ell})}
% \\  +  \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N} y_{i}(1,W^{*}_{\ell-1})y_{i}(0,W^{*}_{\ell}) 
% \\ = \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}((1,W^{*}_{\ell-1}),(0,W^{*}_{\ell-1})) - \pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(0,W^{*}_{\ell-1})]\frac{y_{i}(1,W^{*}_{\ell-1})y_{j}(0,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(0,W^{*}_{\ell-1})}
% \\  -  \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N} y_{i}(1,W^{*}_{\ell-1})y_{i}(0,W^{*}_{\ell-1})
% \end{multline}


% %Note that the variance equation is too long and I divided it into 2 environments.

% %\begin{multline} 
% %\mathbf{Var}(\widehat{\delta^*}_{{HT,\ell^{th}}}) =  \mathbf{Var}(c_{\ell1}[\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})] +
% %c_{\ell2}[\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})])
% %\\ = \mathbf{Var}(c_{\ell1}[\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})]) +
% %\mathbf{Var}(c_{\ell2}[\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})])
% %\\ - 2\mathbf{Cov}(c_{\ell1}[\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})],c_{\ell2}[\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})])
% %\\ =  \frac{C^{2}_{1}}{N^2}\sum_{i = 1}^{N}\pi_{i}(1,W^{*}_{\ell})[1 - \pi_{i}(1,W^{*}_{\ell})]\left[ \frac{y_{i}(1,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell})}\right]^2
% %\\  + \frac{C^{2}_{1}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(1,W^{*}_{\ell}) - \pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell})]\frac{y_{i}(1,W^{*}_{\ell})y_{j}(1,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell})}
% %\\ +  \frac{C^{2}_{1}}{N^2}\sum_{i = 1}^{N}\pi_{i}(1,W^{*}_{\ell-1})[1 - \pi_{i}(1,W^{*}_{\ell-1})]\left[ \frac{y_{i}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell-1})}\right]^2
% %\\  + \frac{{C^{2}_{1}}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(1,W^{*}_{\ell-1}) - \pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(1,W^{*}_{\ell-1})]\frac{y_{i}(1,W^{*}_{\ell-1})y_{j}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(1,W^{*}_{\ell-1})}
% %\\ - 2 \left( \frac{C^{2}_{1}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,W^{*}_{\ell}),(1,W^{*}_{\ell-1})) - \pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell-1})\right] \frac{y_{i}(1,W^{*}_{\ell})y_{j}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell-1})} \right.
% %\\ \left. - \frac{C^{2}_{1}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,W^{*}_{\ell})y_{i}(1,W^{*}_{\ell-1})\right)
% %\\ +  \left( \frac{C^{2}_{2}}{N^2}\sum_{i = 1}^{N}\pi_{i}(0,W^{*}_{\ell})[1 - \pi_{i}(0,W^{*}_{\ell})]\left[ \frac{y_{i}(0,W^{*}_{\ell})}{\pi_{i}(0,W^{*}_{\ell})}\right]^2
% %\\  + \frac{C^{2}_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(0,W^{*}_{\ell}) - \pi_{i}(0,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell})]\frac{y_{i}(0,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell})}{\pi_{i}(0,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell})}
% %\\ +  \frac{C^{2}_{2}}{N^2}\sum_{i = 1}^{N}\pi_{i}(0,W^{*}_{\ell-1})[1 - \pi_{i}(0,W^{*}_{\ell-1})]\left[ \frac{y_{i}(0,W^{*}_{\ell-1})}{\pi_{i}(0,W^{*}_{\ell-1})}\right]^2
% %\\  + \frac{C^{2}_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(0,W^{*}_{\ell-1}) - \pi_{i}(0,W^{*}_{\ell-1})\pi_{j}(0,W^{*}_{\ell-1})]\frac{y_{i}(0,W^{*}_{\ell-1})y_{j}(0,W^{*}_{\ell-1})}{\pi_{i}(0,W^{*}_{\ell-1})\pi_{j}(0,W^{*}_{\ell-1})}
% %\right)
% %\\ - 2 \left( \frac{C^{2}_{2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((0,W^{*}_{\ell}),(0,W^{*}_{\ell-1})) - \pi_{i}(0,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell-1})\right] \frac{y_{i}(0,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell-1})}{\pi_{i}(0,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell-1})} \right.
% %\\ \left.- \frac{C^{2}_{2}}{N^2}\sum_{i = 1}^{N}  y_{i}(0,W^{*}_{\ell})y_{i}(0,W^{*}_{\ell-1})\right)
% %\end{multline} 






% %\begin{multline}
% %\\-2\left( \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}((1,W^{*}_{\ell}),(0,W^{*}_{\ell})) - \pi_{i}(1,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell})]\frac{y_{i}(1,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell})}\right.
% %\\ \left. -  \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N} y_{i}(1,W^{*}_{\ell})y_{i}(0,W^{*}_{\ell}) \right.
% %\\ \left.- \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}((1,W^{*}_{\ell}),(0,W^{*}_{\ell-1})) - \pi_{i}(1,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell-1})]\frac{y_{i}(1,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell-1})}\right.
% %\\ \left. +  \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N} y_{i}(1,W^{*}_{\ell})y_{i}(0,W^{*}_{\ell-1}) \right.
% %\\ \left. - \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}((1,W^{*}_{\ell-1}),(0,W^{*}_{\ell})) - \pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(0,W^{*}_{\ell})]\frac{y_{i}(1,W^{*}_{\ell-1})y_{j}(0,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(0,W^{*}_{\ell})}\right.
% %\\ \left. +  \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N} y_{i}(1,W^{*}_{\ell-1})y_{i}(0,W^{*}_{\ell}) \right.
% %\\ \left.= \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}((1,W^{*}_{\ell-1}),(0,W^{*}_{\ell-1})) - \pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(0,W^{*}_{\ell-1})]\frac{y_{i}(1,W^{*}_{\ell-1})y_{j}(0,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(0,W^{*}_{\ell-1})}\right.
% %\\  \left.-  \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N} y_{i}(1,W^{*}_{\ell-1})y_{i}(0,W^{*}_{\ell-1})
% %\right)
% %.
% %\end{multline}




% %\begin{equation}
% %\mathbf{Var(\sum_{i = 1}^{N}a_{i}X_{i})} 
% %=\sum_{i = 1}^{N}a_{i}\mathbf{Var(X_{i})} + 2\sum_{j \ne %i}a_{i}a_{j}\mathbf{Cov}(X_{i}X_{j}).
% %\end{equation}


% %Note that the variance equation is too long and I divided it into 2 environments.













% The variance can simply be computed using the property that $\mathbf{Var(\sum_{i = 1}^{N}a_{i}X_{i})} =\sum_{i=1}^{N}a_{i}\mathbf{Var(X_{i})} + 2\sum_{j\ne i}a_{i}a_{j}\mathbf{Cov}(X_{i}X_{j})$ as follows.




% \begin{multline}
% \label{varianceassumption2}
% \mathbf{Var}(\widehat{\delta^{*}}_{{HT,\ell}}) =  \mathbf{Var}(c_{\ell1}[\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})] +
% c_{\ell2}[\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})])
% \\ = \mathbf{Var}(c_{\ell1}\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}) - c_{\ell1}\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1}) +
% c_{\ell2}\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}) - c_{\ell2}\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1}))
% \\ = c^2_{\ell1}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell})) + c^2_{\ell1}\mathbf{Var}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})) +
% c^2_{\ell2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell})) + c^2_{\ell2}\mathbf{Var}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1}))
% \\ -2c^2_{\ell1} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}), \bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1})) 
% + 2c_{\ell1}c_{\ell2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}), \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell})) 
% \\ -2c_{\ell1}c_{\ell2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}), \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})) 
% -2c_{\ell1}c_{\ell2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1}), \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell})) 
% \\ + 2c_{\ell1}c_{\ell2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1}), \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})) 
% -2c^2_{\ell2} \mathbf{Cov}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}), \bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1})) 
% \\ =  \frac{c^2_{\ell1}}{N^2}\sum_{i = 1}^{N}\pi_{i}(1,W^{*}_{\ell})[1 - \pi_{i}(1,W^{*}_{\ell})]\left[ \frac{y_{i}(1,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell})}\right]^2
% \\  + \frac{c^2_{\ell1}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(1,W^{*}_{\ell}) - \pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell})]\frac{y_{i}(1,W^{*}_{\ell})y_{j}(1,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell})}.
% \\ +  \frac{c^2_{\ell1}}{N^2}\sum_{i = 1}^{N}\pi_{i}(1,W^{*}_{\ell-1})[1 - \pi_{i}(1,W^{*}_{\ell-1})]\left[ \frac{y_{i}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell-1})}\right]^2
% \\  + \frac{c^2_{\ell1}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(1,W^{*}_{\ell-1}) - \pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(1,W^{*}_{\ell-1})]\frac{y_{i}(1,W^{*}_{\ell-1})y_{j}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(1,W^{*}_{\ell-1})}
% \\ +  \frac{c^2_{\ell2}}{N^2}\sum_{i = 1}^{N}\pi_{i}(0,W^{*}_{\ell})[1 - \pi_{i}(0,W^{*}_{\ell})]\left[ \frac{y_{i}(0,W^{*}_{\ell})}{\pi_{i}(0,W^{*}_{\ell})}\right]^2
% \\  + \frac{c^2_{\ell2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(0,W^{*}_{\ell}) - \pi_{i}(0,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell})]\frac{y_{i}(0,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell})}{\pi_{i}(0,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell})}.
% \\ +  \frac{c^2_{\ell2}}{N^2}\sum_{i = 1}^{N}\pi_{i}(0,W^{*}_{\ell-1})[1 - \pi_{i}(0,W^{*}_{\ell-1})]\left[ \frac{y_{i}(0,W^{*}_{\ell-1})}{\pi_{i}(0,W^{*}_{\ell-1})}\right]^2
% \\  + \frac{c^2_{\ell2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}[\pi_{ij}(0,W^{*}_{\ell-1}) - \pi_{i}(0,W^{*}_{\ell-1})\pi_{j}(0,W^{*}_{\ell-1})]\frac{y_{i}(0,W^{*}_{\ell-1})y_{j}(0,W^{*}_{\ell-1})}{\pi_{i}(0,W^{*}_{\ell-1})\pi_{j}(0,W^{*}_{\ell-1})}
% \\ - 2 \left( \frac{c^2_{\ell1}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,W^{*}_{\ell}),(1,W^{*}_{\ell-1})) - \pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell-1})\right] \frac{y_{i}(1,W^{*}_{\ell})y_{j}(1,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(1,W^{*}_{\ell-1})}\right.
% \\ \left.- \frac{c^2_{\ell1}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,W^{*}_{\ell})y_{i}(1,W^{*}_{\ell-1})\right)
% \\ + 2 \left( \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,W^{*}_{\ell}),(0,W^{*}_{\ell})) - \pi_{i}(1,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell})\right] \frac{y_{i}(1,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell})}\right.
% \\ \left.- \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,W^{*}_{\ell})y_{i}(0,W^{*}_{\ell})\right)
% .
% \end{multline}



% \begin{multline}
% \\ - 2 \left( \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,W^{*}_{\ell}),(0,W^{*}_{\ell-1})) - \pi_{i}(1,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell-1})\right] \frac{y_{i}(1,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell-1})}\right.
% \\ \left.- \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,W^{*}_{\ell})y_{i}(0,W^{*}_{\ell-1})\right)
% \\ - 2 \left( \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,W^{*}_{\ell-1}),(0,W^{*}_{\ell})) - \pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(0,W^{*}_{\ell})\right] \frac{y_{i}(1,W^{*}_{\ell-1})y_{j}(0,W^{*}_{\ell})}{\pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(0,W^{*}_{\ell})}\right.
% \\ \left.- \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,W^{*}_{\ell-1})y_{i}(0,W^{*}_{\ell})\right)
% \\ + 2 \left( \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((1,W^{*}_{\ell-1}),(0,W^{*}_{\ell-1})) - \pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(0,W^{*}_{\ell-1})\right] \frac{y_{i}(1,W^{*}_{\ell-1})y_{j}(0,W^{*}_{\ell-1})}{\pi_{i}(1,W^{*}_{\ell-1})\pi_{j}(0,W^{*}_{\ell-1})}\right.
% \\ \left.- \frac{c_{\ell1}c_{\ell2}}{N^2}\sum_{i = 1}^{N}  y_{i}(1,W^{*}_{\ell-1})y_{i}(0,W^{*}_{\ell-1})\right)
% \\ - 2 \left( \frac{c^2_{\ell2}}{N^2}\sum_{i = 1}^{N}\sum_{j \ne i}\left[\pi_{ij}((0,W^{*}_{\ell}),(0,W^{*}_{\ell-1})) - \pi_{i}(0,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell-1})\right] \frac{y_{i}(0,W^{*}_{\ell})y_{j}(0,W^{*}_{\ell-1})}{\pi_{i}(0,W^{*}_{\ell})\pi_{j}(0,W^{*}_{\ell-1})}\right.
% \\ \left.- \frac{c^2_{\ell2}}{N^2}\sum_{i = 1}^{N}  y_{i}(0,W^{*}_{\ell})y_{i}(0,W^{*}_{\ell-1})\right)
% \end{multline}






% \subsubsection{Weights in A$\ell$NNIE Estimator under the No Weak Interaction between Direct and Indirect Effects Assumption}
% \label{makereferenceB.8.3}
% Using Lagrange multiplier, we can find the values of $c_{\ell1}$ and $c_{\ell2}$ in \ref{Assumption2Estimator} that gives the minimum variance of A$\ell$NNIE Estimator under the no weak interaction between direct and indirect effects Assumption as follows.

% First, let $S^2_{11} = \mathbf{Var}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell}))$, $S^2_{12} = \mathbf{Var}(\bar{Y}_{HT}^{obs}(1,W^{*}_{\ell-1}))$,
% $S^2_{21} = \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell}))$ and $S^2_{22} = \mathbf{Var}(\bar{Y}_{HT}^{obs}(0,W^{*}_{\ell-1}))$ and assume that the covariance components in \ref{varianceassumption2} are equal to zero. 

% Then, we want to minimize the variance  

% \begin{equation}
% \mathbf{Var}(\widehat{\delta^{*}}_{{HT,\ell}})
% = \sum_{i = 1}^{2}c^2_{\ell i}(S^2_{i1} + S^2_{i2}) 
% \end{equation}

% subject to the constraint $\sum_{i = 1}^{2}c_{\ell i}= 1$

% Using the method of Lagrange multiplier, we minimize the function:

% \begin{multline}
% \phi = \mathbf{Var}(\widehat{\delta^{*}}_{{HT,\ell}})
% - \lambda(\sum_{i = 1}^{2}c^2_{\ell i} -1) 
% = \sum_{i = 1}^{2}c^2_{\ell i}(S^2_{i1} + S^2_{i2})  - \lambda(\sum_{i = 1}^{2}c^2_{\ell i} -1)
% \end{multline}

% Then, $\frac{\partial\phi}{\partial c_{\ell i}} = 0$ gives

% \begin{multline}\label{Ciwithlampda}
% \frac{\partial[\sum_{i = 1}^{2}c^2_{\ell i}(S^2_{i1} + S^2_{i2})  - \lambda(\sum_{i = 1}^{2}c^2_{\ell i} -1)}{\partial c_{\ell i}}] = 0
% \\ 2c_{\ell i}(S^2_{i1} + S^2_{i2}) - \lambda = 0
% \\ c_{\ell i} = \frac{\lambda}{2(S^2_{i1} + S^2_{i2})}
% \end{multline} 

% Now, we find Lagrange multiplier $\lambda$ as follows.

% \begin{multline}
% \sum_{i=1}^{2}c_{\ell i}  = 1 \Rightarrow \frac{\lambda}{2(S^2_{11} + S^2_{12})} + \frac{\lambda}{2(S^2_{21} + S^2_{22})} = 1 
% \\\Rightarrow \frac{\lambda}{2}\left(\frac{S^2_{11} + S^2_{12} + S^2_{21} + S^2_{22}}{(S^2_{11} + S^2_{12})(S^2_{21} + S^2_{22})}\right) = 1 
% \Rightarrow \lambda = \frac{2(S^2_{11} + S^2_{12})(S^2_{21} + S^2_{22})}{S^2_{11} + S^2_{12} + S^2_{21} + S^2_{22}}. 
% \end{multline}

% Substituting in \ref{Ciwithlampda}, we have

% \begin{equation}
% c_{\ell 1} = \frac{S^2_{21} + S^2_{22}}{S^2_{11} + S^2_{12} + S^2_{21} + S^2_{22}}  ,  c_{\ell 2} = \frac{S^2_{11} + S^2_{12}}{S^2_{11} + S^2_{12} + S^2_{21} + S^2_{22}},
% \end{equation}

% which give the minimum variance of A$\ell$NNIE estimator under the no weak interaction between direct and indirect effects assumption.




% %%%%%%%%%%%%%%%%%%%%%%


% \section{ Appendix C: More Results of the Simulation}
% \label{AppendixC}


% %%%%%%%  CRD
% \begin{table}%[h!]
%   \caption{Estimates Under Completely Randomized Design Model 2}%
%   \label{table3.3}
%   \centering
%   \begin{tabular}[c]{lccccc}\\
%   %\hline
%         Estimator & Effects & Emp.Estimates & Emp.Var & Emp.S.D. & Var Estimate \\
% \hline
%        $ \widehat{\delta}_{{HT,tot}}$ & 1 & 1.0707  & 1.306 & 1.1430 & 1.3314 \\
%        $ \widehat{\delta^*}_{{HT,tot}}$ & 1 & 1.0707  & 1.306 & 1.1430 & 1.3314 \\
%        $\widehat{\delta}_{{HT,dir}}$ & 1 & 1.0265  & 0.7273 & 0.8528 & 0.8050 \\
%        $ \widehat{\delta^*}_{{HT,dir}}$ & 1 & 1.0300  & 0.3377 & 0.5812 &  0.6214\\
%        $\widehat{\delta}_{{HT,ind}}$ & 0 & 0.0442  & 0.9404 & 0.9697 & 0.9393\\
%        $ \widehat{\delta^*}_{{HT,ind}}$ & 0 & 0.0406  &  0.6081 & 0.7798 & 0.7381\\
%        $\widehat{\delta}_{{HT,1^{st}}}$  & 0 & 0.0346  & 0.6273 & 0.7920 & 0.6821 \\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 0 & 0.01758  & 0.3009 & 0.5486 & 0.4749  \\
%         $\widehat{\delta}_{{HT,2^{nd}}}$ & 0 & -0.0111   & 0.4642 & 0.6813 & 0.6023 \\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 0 & -0.0052  & 0.3328 & 0.5769 & 0.5601 \\
%         $\widehat{\delta}_{{HT,3^{rd}}}$ & 0 & 0.0207  & 0.4541 & 0.6738 & 0.6076 \\
%        $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 0 & 0.0282 & 0.3525 & 0.5937 & 0.5359\\
% \hline
% \end{tabular}
% \end{table}




% \begin{table}%[h!]
%   \caption{Estimates Under Completely Randomized Design Model 3}%
%   \label{table3.4}
%   \centering
%   \begin{tabular}[c]{lccccc}\\
%  % \hline
%         Estimator & Effects & Emp.Estimates & Emp.Var & Emp.S.D. & Var Estimate \\
% \hline
%        $ \widehat{\delta}_{{HT,tot}}$   & 4 &  4.0489  & 2.9225 & 1.7095 & 3.3552\\
%        $ \widehat{\delta^*}_{{HT,tot}}$  & 4 &  4.0489 & 2.9225 & 1.7095 & 3.3552\\
%        $\widehat{\delta}_{{HT,dir}}$  & 4 & 4.0047  & 2.2827 & 1.5108 & 2.5258  \\
%        $ \widehat{\delta^*}_{{HT,dir}}$   & 4 & 4.0292  & 0.9267 & 0.9626 & 2.1155 \\
%        $\widehat{\delta}_{{HT,ind}}$ & 4 & 0.0442  &  0.9404 & 0.9697 & 0.9393 \\
%        $ \widehat{\delta^*}_{{HT,ind}}$   & 0 & 0.01978  & 1.1883 & 1.0901 & 1.7146 \\
%        $\widehat{\delta}_{{HT,1^{st}}}$  & 0 &  0.0346  & 0.6273 & 0.7920 & 0.6821 \\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 0 & 0.01286  &  0.8246 &  0.9081 & 1.4569 \\
%         $\widehat{\delta}_{{HT,2^{nd}}}$ & 0 & -0.0111   & 0.4642 & 0.6813 & 0.6023 \\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 0 & -0.01445  & 1.0133 & 1.0066 & 1.7389 \\
%         $\widehat{\delta}_{{HT,3^{rd}}}$ & 0 & 0.0207  & 0.4541 & 0.6738 & 0.6076 \\
%        $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 0 & 0.0213  & 1.0711 & 1.0349 & 1.6299 \\
% \hline
% \end{tabular}
% \end{table}



% \begin{table}%[h!]
%   \caption{Estimates Under Completely Randomized Design Model 4}%
%   \label{table3.5}
%   \centering
%   \begin{tabular}[c]{lccccc}\\
%   %\hline
%         Estimator & Effects & Emp.Estimates & Emp.Var & Emp.S.D. & Var Estimate \\
% \hline
%        $ \widehat{\delta}_{{HT,tot}}$ & 3.5 & 3.5526  & 2.5285 & 1.5901 &  2.8557\\
%        $ \widehat{\delta^*}_{{HT,tot}}$ & 3.5 & 3.5526  & 2.5285 & 1.5901 &  2.8557\\
%        $\widehat{\delta}_{{HT,dir}}$ & 0 & -0.0123  & 2.9637 & 1.7215 & 3.2323\\
%        $ \widehat{\delta^*}_{{HT,dir}}$ & 0 & 0.0073  & 0.8913 & 0.9441 & 1.5485\\
%        $\widehat{\delta}_{{HT,ind}}$ & 3.5 & 3.5649  & 1.5750 & 1.2550 & 2.3108 \\
%        $ \widehat{\delta^*}_{{HT,ind}}$  & 3.5 & 3.5453  & 0.9523 & 0.9758 & 1.6828 \\
%        $\widehat{\delta}_{{HT,1^{st}}}$  & 2 & 2.0414  & 0.8733 & 0.9345 & 1.1102 \\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 2 & 2.0261  & 0.3901 &  0.6245 & 0.7582\\
%         $\widehat{\delta}_{{HT,2^{nd}}}$  & 1 & 1.0003  & 1.4039 & 1.1848 & 1.9204\\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 1 & 0.9961  & 0.7423 & 0.8615 & 1.4553 \\
%         $\widehat{\delta}_{{HT,3^{rd}}}$ & 0.5 & 0.5230  & 1.8611 & 1.3642 &  2.5928 \\
%        $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 0.5 & 0.5230 & 1.1157 & 1.0563 & 1.8726\\
% \hline
% \end{tabular}
% \end{table}



% \begin{table}%[H!]
%   \caption{Estimates Under Completely Randomized Design Model 6}%
%   \label{table3.7}
%   \centering
%   \begin{tabular}[c]{lccccc}\\
%   %\hline
%         Estimator & Effects & Emp.Estimates  & Emp.Var & Emp.S.D. & Var Estimate \\
% \hline
%        $ \widehat{\delta}_{{HT,tot}}$   & 7.5 & 7.5236  & 7.0767 & 2.6602 & 8.6687 \\
%        $ \widehat{\delta^*}_{{HT,tot}}$  & 7.5 & 7.5236  & 7.0767 & 2.6602 & 8.6687 \\
%        $\widehat{\delta}_{{HT,dir}}$  & 4 & 3.9586  & 7.9109 & 2.8126 & 8.1389\\
%        $ \widehat{\delta^*}_{{HT,dir}}$ & 4 & 4.0061  & 2.3713 & 1.5399 & 4.4075\\
%        $\widehat{\delta}_{{HT,ind}}$ & 3.5 & 3.5649  & 1.5750 & 1.2550 & 2.3108\\
%        $ \widehat{\delta^*}_{{HT,ind}}$ & 3.5 & 3.5174  & 2.1777 & 1.4757 & 3.5289 \\
%        $\widehat{\delta}_{{HT,1^{st}}}$  & 2 & 2.0414  & 0.8733 & 0.9345 & 1.1102 \\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 2 & 2.0198  & 1.1942 & 1.0928 & 2.3348\\
%         $\widehat{\delta}_{{HT,2^{nd}}}$  & 1 & 1.0003  & 1.4039 & 1.1848 & 1.9204\\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 1 & 0.9838  & 2.1622 & 1.4704 & 4.1131\\
%         $\widehat{\delta}_{{HT,3^{rd}}}$ & 0.5 & 0.5230  & 1.8611 & 1.3642 &  2.5928\\
%        $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 0.5 & 0.5138  & 3.0275 & 1.7399 & 4.7936 \\
% \hline
% \end{tabular}
% \end{table}



% \begin{table}%[H!]
%   \caption{Estimates Under Completely Randomized Design Model 7}%
%   \label{table3.8}
%   \centering
%   \begin{tabular}[c]{lccccc}\\
%  % \hline
%         Estimator & Effects & Emp.Estimates & Emp.Var & Emp.S.D. & Var Estimate \\
% \hline
%        $ \widehat{\delta}_{{HT,tot}}$ & 6 & 6.0344 & 4.9972 & 2.2354 & 6.0021\\
%        $ \widehat{\delta^*}_{{HT,tot}}$& 6 & 6.0344 & 4.9972 & 2.2354 & 6.0021 \\
%        $\widehat{\delta}_{{HT,dir}}$ & 0 & -0.0452 & 7.4190 & 2.7237 & 7.9636 \\
%        $ \widehat{\delta^*}_{{HT,dir}}$ & 0 & -0.0091 & 2.0143 & 1.4192 & 3.4718\\
%        $\widehat{\delta}_{{HT,ind}}$ & 6 & 6.0797 & 2.7909 & 1.6705 & 4.850\\
%        $ \widehat{\delta^*}_{{HT,ind}}$ & 6 & 6.0436 & 1.6675 & 1.2913 & 3.5938\\
%        $\widehat{\delta}_{{HT,1^{st}}}$  & 3 &3.0449  & 1.1582 & 1.0762 & 1.6049\\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 3 & 3.0295  & 0.5412 & 0.7357 & 1.1811\\
%         $\widehat{\delta}_{{HT,2^{nd}}}$  & 2 &2.0092 & 2.8500 & 1.6882 & 3.9616\\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 2 & 1.9961  & 1.4097 & 1.1873 & 2.9671\\
%         $\widehat{\delta}_{{HT,3^{rd}}}$ & 1 &1.0256  & 4.5078 & 2.1231 & 6.1172\\
%        $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 1 & 1.0179  & 2.6139 & 1.6167 & 4.4043\\
% \hline
% \end{tabular}
% \end{table}


% \begin{table}%[tbp]
%   \caption{Estimates Under Completely Randomized Design Model 8}%
%   \label{table3.9}
%   \centering
%   \begin{tabular}[c]{lccccc}\\
%  % \hline
%         Estimator & Effects & Emp.Estimates & Emp.Var & Emp.S.D. & Var Estimate \\
% \hline
%        $ \widehat{\delta}_{{HT,tot}}$   & 7 & 7.0272  & 6.3337 & 2.5166 & 7.7149 \\
%        $ \widehat{\delta^*}_{{HT,tot}}$  & 7 & 7.0272  & 6.3337 & 2.5166 & 7.7149 \\
%        $\widehat{\delta}_{{HT,dir}}$  & 1 & 0.9474  & 8.9409 & 2.9901 & 9.4071 \\
%        $ \widehat{\delta^*}_{{HT,dir}}$   & 1 & 0.9905 & 2.4239 & 1.5568 & 4.1241 \\
%        $\widehat{\delta}_{{HT,ind}}$ & 6 & 6.0797 & 2.7909 & 1.6705 & 4.8503\\
%        $ \widehat{\delta^*}_{{HT,ind}}$ & 6 & 6.0366  & 1.9707 & 1.4038 & 4.0101\\
%        $\widehat{\delta}_{{HT,1^{st}}}$  & 3 & 3.0449  & 1.1582 & 1.0762 & 1.6049\\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 3 & 3.0280 & 0.6690 & 0.8179 & 1.4541\\
%         $\widehat{\delta}_{{HT,2^{nd}}}$  & 2 & 2.0092 & 2.8500 & 1.6882 & 3.9616 \\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 2 & 1.9930 & 1.7408 & 1.3194 & 3.6167\\
%         $\widehat{\delta}_{{HT,3^{rd}}}$ & 1 & 1.0256  & 4.5078 & 2.1231 & 6.1172\\
%        $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 1 & 1.0156 & 3.1489 & 1.7745 & 5.2252 \\
% \hline
% \end{tabular}
% \end{table}


% %%%%%%%%%% BR




% \begin{table}[tbp]
%   \caption{Estimates Under Bernoulli Randomization Model 2}%
%   \label{table2Ber}
%   \centering
%   \begin{tabular}[c]{lccccc}\\
%   %\hline
%         Estimator & Effects & Emp.Estimates  & Emp.Var & Emp.S.D. & Var Estimate \\
% \hline
%         $ \widehat{\delta}_{{HT,tot}}$   & 1 & 1.0326  & 1.4420 & 1.2008 & 1.4406 \\
%        $ \widehat{\delta^*}_{{HT,tot}}$   & 1 & 1.0326 & 1.4420 & 1.2008 & 1.4406 \\
%        $\widehat{\delta}_{{HT,dir}}$  & 1 & 1.0116  & 0.8786 & 0.9373 & 0.8960 \\
%        $ \widehat{\delta^*}_{{HT,dir}}$   & 1 & 1.0277 & 0.3573 & 0.5978 & 0.6357  \\
%        $\widehat{\delta}_{{HT,ind}}$ & 0 & 0.0210  & 0.8232 & 0.9073 & 0.9258 \\
%        $ \widehat{\delta^*}_{{HT,ind}}$   & 0 & 0.0049  & 0.6462 & 0.8038 & 0.7994  \\
%        $\widehat{\delta}_{{HT,1^{st}}}$  & 0 & 0.0502  & 0.6502 & 0.8063 & 0.6804 \\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 0 & 0.0142 & 0.2858 & 0.5346 & 0.4783  \\
%         $\widehat{\delta}_{{HT,2^{nd}}}$  & 0 & -0.0181  & 0.4430 & 0.6656 & 0.5985 \\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 0 & -0.0177  & 0.3250 & 0.5700 & 0.5625  \\
%         $\widehat{\delta}_{{HT,3^{rd}}}$ & 0 & -0.0110  & 0.4489 & 0.6700 & 0.6048 \\
%        $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 0 & 0.0084  & 0.3846 & 0.6202 & 0.5497  \\
% \hline
% \end{tabular}
% \end{table}





% \begin{table}[tbp]
%   \caption{Estimates Under Bernoulli Randomization Model 3}%
%   \label{table3Ber}
%   \centering
%   \begin{tabular}[c]{lccccc}\\
%   %\hline
%         Estimator & Effects & Emp.Estimates  & Emp.Var & Emp.S.D. & Var Estimate \\
% \hline
%         $ \widehat{\delta}_{{HT,tot}}$   & 4 & 4.1129  & 4.0302 & 2.0075 & 4.5346  \\
%        $ \widehat{\delta^*}_{{HT,tot}}$  & 4 & 4.1129  & 4.0302 & 2.0075 & 4.5346  \\
%        $\widehat{\delta}_{{HT,dir}}$  & 4 & 4.0919 & 3.4654 & 1.8615 & 3.6069  \\
%        $ \widehat{\delta^*}_{{HT,dir}}$   & 4 & 4.0797 & 0.9582 & 0.9789 & 2.2095  \\
%        $\widehat{\delta}_{{HT,ind}}$ & 0 & 0.0210  & 0.8232 & 0.9073 & 0.9258 \\
%        $ \widehat{\delta^*}_{{HT,ind}}$   & 0 & 0.0331 & 1.8081 & 1.3446 & 2.3816  \\
%        $\widehat{\delta}_{{HT,1^{st}}}$  & 0 & 0.0502  & 0.6502 & 0.8063 & 0.6804 \\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 0 & 0.0051 & 0.8466 & 0.9201 & 1.5260  \\
%         $\widehat{\delta}_{{HT,2^{nd}}}$  & 0 & -0.0181 & 0.4430 & 0.6656 & 0.5985 \\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 0 & -0.0456 & 1.0226 & 1.0112 & 1.7932  \\
%         $\widehat{\delta}_{{HT,3^{rd}}}$ & 0 & -0.0110  & 0.4489 & 0.6700 & 0.6048 \\
%        $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 0 & 0.0737  & 1.2557 & 1.1205 & 1.7099  \\
% \hline
% \end{tabular}
% \end{table}



% \begin{table}[tbp]
%   \caption{Estimates Under Bernoulli Randomization Model 4}%
%   \label{table4Ber}
%   \centering
%   \begin{tabular}[c]{lccccc}\\
%   %\hline
%         Estimator & Effects & Emp.Estimates & Emp.Var & Emp.S.D. & Var Estimate \\
% \hline
%         $ \widehat{\delta}_{{HT,tot}}$  & 3.5 & 3.5995  & 3.3899 & 1.8411 & 3.7707  \\
%        $ \widehat{\delta^*}_{{HT,tot}}$  & 3.5 & 3.5995 & 3.3899 & 1.8411 & 3.7707  \\
%        $\widehat{\delta}_{{HT,dir}}$  & 0 & 0.0745 & 3.1656 & 1.7792 & 3.4406  \\
%        $ \widehat{\delta^*}_{{HT,dir}}$  & 0 & 0.0552 & 0.9018 & 0.9496 & 1.6046  \\
%        $\widehat{\delta}_{{HT,ind}}$ & 3.5 & 3.5249  & 1.8276 & 1.3519 & 2.5207  \\
%        $ \widehat{\delta^*}_{{HT,ind}}$   & 3.5 & 3.5443  & 1.5038 & 1.2262 & 2.1947  \\
%        $\widehat{\delta}_{{HT,1^{st}}}$  & 2 & 2.0532  & 0.9882 & 0.9940 & 1.1739  \\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 2 & 2.0206  & 0.3871 & 0.6222 & 0.7689  \\
%         $\widehat{\delta}_{{HT,2^{nd}}}$  & 1 & 0.9878  & 1.4253 & 1.1938 & 2.0036  \\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 1 & 0.9675 & 0.8350 & 0.9137 & 1.5577  \\
%         $\widehat{\delta}_{{HT,3^{rd}}}$ & 0.5 & 0.4838 & 2.2543 & 1.5014 & 2.8237\\
%        $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 0.5 & 0.5561  & 1.5671 & 1.2518 & 2.1355  \\
% \hline
% \end{tabular}
% \end{table}






% \begin{table}[tbp]
%   \caption{Estimates Under Bernoulli Randomization Model 6}%
%   \label{table6Ber}
%   \centering
%   \begin{tabular}[c]{lccccc}\\
%   %\hline
%         Estimator & Effects & Emp.Estimates & Emp.Var & Emp.S.D. & Var Estimate \\
% \hline
%         $ \widehat{\delta}_{{HT,tot}}$   & 7.5 & 7.7065 & 10.8531 & 3.2944 & 12.6623  \\
%        $ \widehat{\delta^*}_{{HT,tot}}$  & 7.5 & 7.7065 & 10.8531 & 3.2944 & 12.6623  \\
%        $\widehat{\delta}_{{HT,dir}}$  & 4 & 4.1815 & 10.0122 & 3.1642 & 10.4070 \\
%        $ \widehat{\delta^*}_{{HT,dir}}$  & 4 & 4.1245 & 2.4463 & 1.5640 & 4.7047  \\
%        $\widehat{\delta}_{{HT,ind}}$ & 3.5 & 3.5249 & 1.8276 & 1.3519 & 2.5207  \\
%        $ \widehat{\delta^*}_{{HT,ind}}$   & 3.5 & 3.5819 & 4.4755 & 2.1155 & 5.7415  \\
%        $\widehat{\delta}_{{HT,1^{st}}}$  & 2 & 2.0532 & 0.9882 & 0.9940 & 1.1739  \\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 2 & 2.0084  & 1.1391 & 1.0672 & 2.3490  \\
%         $\widehat{\delta}_{{HT,2^{nd}}}$  & 1 & 0.9878 & 1.4253 & 1.1938 & 2.0036 \\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 1 & 0.9304 & 2.4611 & 1.5687 & 4.4098 \\
%         $\widehat{\delta}_{{HT,3^{rd}}}$ & 0.5 & 0.4838  & 2.2543 & 1.5014 & 2.8237 \\
%        $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 0.5 & 0.6431  & 4.0935 & 2.0232 & 5.3983  \\
% \hline
% \end{tabular}
% \end{table}



% \begin{table}[tbp]
%   \caption{Estimates Under Bernoulli Randomization Model 7}%
%   \label{table7Ber}
%   \centering
%   \begin{tabular}[c]{lccccc}\\
%   %\hline
%         Estimator & Effects & Emp.Estimates  & Emp.Var & Emp.S.D. & Var Estimate \\
% \hline
%         $ \widehat{\delta}_{{HT,tot}}$   & 6 & 6.1664 & 7.4275 & 2.7253 & 8.5832  \\
%        $ \widehat{\delta^*}_{{HT,tot}}$  & 6 & 6.1664 & 7.4275 & 2.7253 & 8.5832  \\
%        $\widehat{\delta}_{{HT,dir}}$  & 0 & 0.1386  & 7.8388 & 2.7997 & 8.5881  \\
%        $ \widehat{\delta^*}_{{HT,dir}}$   & 0 & 0.0872 & 2.0487 & 1.4313 & 3.6387  \\
%        $\widehat{\delta}_{{HT,ind}}$ & 6 & 6.0277 & 3.5973 & 1.8966 & 5.4461  \\
%        $ \widehat{\delta^*}_{{HT,ind}}$   & 6 & 6.0791  & 3.2309 & 1.7974 & 5.0241  \\
%        $\widehat{\delta}_{{HT,1^{st}}}$ & 3 & 3.0547  & 1.3709 & 1.1708 & 1.7503  \\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 3 & 3.0223  & 0.5510 & 0.7423 & 1.2101  \\
%         $\widehat{\delta}_{{HT,2^{nd}}}$  & 2 & 1.9923 & 2.9090 & 1.7056 & 4.1353 \\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 2 & 1.9521 & 1.6716 & 1.2929 & 3.2181  \\
%         $\widehat{\delta}_{{HT,3^{rd}}}$ & 1 & 0.9806  & 5.3764 & 2.3187 & 6.7556  \\
%        $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 1 & 1.1047 & 3.8051 & 1.9506 & 5.1576 \\
% \hline
% \end{tabular}
% \end{table}






% \begin{table}[tbp]
%   \caption{Estimates Under Bernoulli Randomization Model 8}%
%   \label{table8Ber}
%   \centering
%   \begin{tabular}[c]{lccccc}\\
%   %\hline
%         Estimator & Effects & Emp.Estimates & Emp.Var & Emp.S.D. & Var Estimate \\
% \hline
%         $ \widehat{\delta}_{{HT,tot}}$   & 7 & 7.1931 & 9.6277 & 3.1028 & 11.2033  \\
%        $ \widehat{\delta^*}_{{HT,tot}}$  & 7 & 7.1931 & 9.6277 & 3.1028 & 11.2033  \\
%        $\widehat{\delta}_{{HT,dir}}$ & 1 & 1.1654 & 9.7750 & 3.1265 & 10.5230  \\
%        $ \widehat{\delta^*}_{{HT,dir}}$   & 1 & 1.1045 & 2.4792 & 1.5745 & 4.3616  \\
%        $\widehat{\delta}_{{HT,ind}}$ & 6 & 6.0277 & 3.5973 & 1.8966 & 5.4461  \\
%        $ \widehat{\delta^*}_{{HT,ind}}$   & 6 & 6.0885 & 4.0546 & 2.0136 & 5.9402 \\
%        $\widehat{\delta}_{{HT,1^{st}}}$ & 3 & 3.0547 & 1.3709 & 1.1708 & 1.7503  \\
%        $\widehat{\delta^*}_{{HT,1^{st}}}$ & 3 & 3.0192 & 0.6476 & 0.8047 & 1.4636  \\
%         $\widehat{\delta}_{{HT,2^{nd}}}$ & 2 & 1.9923 & 2.9090 & 1.7056 & 4.1353  \\
%        $\widehat{\delta^*}_{{HT,2^{nd}}}$  & 2 & 1.9428 & 2.0746 & 1.4403 & 3.9275  \\
%         $\widehat{\delta}_{{HT,3^{rd}}}$ & 1 & 0.9806 & 5.3764 & 2.3187 & 6.7556  \\
%        $\widehat{\delta^*}_{{HT,3^{rd}}}$ & 1 & 1.1264 & 4.5503 & 2.1331 & 6.0993 \\
% \hline
% \end{tabular}
% \end{table}















% % \bigskip
% % \begin{center}
% % {\large\bf SUPPLEMENTARY MATERIAL}
% % \end{center}

% % \begin{description}

% % \item[Title:] Brief description. (file type)

% % \item[R-package for  MYNEW routine:] R-package ÒMYNEWÓ containing code to perform the diagnostic methods described in the article. The package also contains all datasets used as examples in the article. (GNU zipped tar file)

% % \item[HIV data set:] Data set used in the illustration of MYNEW method in Section~ 3.2. (.txt file)

% % \end{description}

% % \section{BibTeX}

% % We hope you've chosen to use BibTeX!\ If you have, please feel free to use the package natbib with any bibliography style you're comfortable with. The .bst file agsm has been included here for your convenience. 


% % \begin{table}
% % \caption{D-optimality values for design $X$ under five different scenarios.  \label{tab:tabone}}
% % \begin{center}
% % \begin{tabular}{rrrrr}
% % one & two & three & four & five\\\hline
% % 1.23 & 3.45 & 5.00 & 1.21 & 3.41 \\
% % 1.23 & 3.45 & 5.00 & 1.21 & 3.42 \\
% % 1.23 & 3.45 & 5.00 & 1.21 & 3.43 \\
% % \end{tabular}
% % \end{center}
% % \end{table}

% % \begin{itemize}
% % \item Note that figures and tables (such as Figure~\ref{fig:first} and
% % Table~\ref{tab:tabone}) should appear in the paper, not at the end or
% % in separate files.
% % \item In the latex source, near the top of the file the command
% % \verb+\newcommand{\blind}{1}+ can be used to hide the authors and
% % acknowledgements, producing the required blinded version.
% % \item Remember that in the blind version, you should not identify authors
% % indirectly in the text.  That is, don't say ``In Smith et. al.  (2009) we
% % showed that ...''.  Instead, say ``Smith et. al. (2009) showed that ...''.
% % \item These points are only intended to remind you of some requirements.
% % Please refer to the instructions for authors
% % at \url{http://amstat.tandfonline.com/action/authorSubmission?journalCode=uasa20&page=instructions#.VFkk7fnF_0c}
% % \item For more about ASA\ style, please see \url{http://journals.taylorandfrancis.com/amstat/asa-style-guide/}
% % \item If you have supplementary material (e.g., software, data, technical
% % proofs), identify them in the section below.  In early stages of the
% % submission process, you may be unsure what to include as supplementary
% % material.  Don't worry---this is something that can be worked out at later stages.
% % \end{itemize}


%\bibliographystyle{agsm}

\end{document}
