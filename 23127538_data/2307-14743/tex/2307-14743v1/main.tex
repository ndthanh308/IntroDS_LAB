%% EDITABLE LINK: see EDIT-LINK.txt

% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
% \usepackage[review]{EMNLP2023}
%\usepackage[review]{ACL2023}
\usepackage[]{ACL2023}


% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

\usepackage{graphicx}
\usepackage{cleveref}
% our "ÚFAL" macros
\usepackage{multirow}
\def\hideXXX#1{}
\def\XXX#1{{\textcolor{red}{XXX #1}}}  % comment/uncomment before submission
\def\XXX#1{\hideXXX{#1}}

\usepackage[normalem]{ulem} % sout, uline
\def\repl#1#2{\textcolor{red}{XXX \sout{#1}}\textcolor{blue}{\uline{#2}}}


\def\XXXifaccepted#1{{\textcolor{green}{Add if accepted: #1}}}
\def\XXXifacceptedWMT#1{}
\def\XXXifaccepted#1{} % hide for now

\def\repllongversion#1#2{\textcolor{orange}{LONG: \sout{#1}|#2}}
%\def\repllongversion#1#2{#2}  % comment here to show what is to be replaced

\def\PV#1{{\textcolor{blue}{PV #1}}}
\def\hidePV#1{}

\def\footurl#1{\footnote{\url{#1}}}

\def\repl#1#2{{\XXX{}\textcolor{red}{\sout{#1}}\textcolor{blue}{#2}}}
\def\DJ#1#2{{\textcolor{red}{\sout{#1}}\textcolor{blue}{#2}}}
\def\parcite#1{\citep{#1}} % (Smith, 2012)
\def\perscite#1{\citet{#1}} % Smith (2012)
\def\inparcite#1{\citealp{#1}} % should be Smith, 2012
\def\furl#1{\footnote{\url{#1}}}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

%\title{How Whisper Streams Real-Time Transcriptions}
%\title{Whisper-Streaming: Turning Whisper into Real-Time Transcription System 
\title{Turning Whisper into Real-Time Transcription System 
%\\
% Whisper-Streaming: Turning Speech LLMs into Real-Time Transcription Systems
}
% \title{Enabling Real-time Speech Transcription and Translation Using Whisper-like Models}
% Whisper-Stream: Real-time Transcription using Whisper
% Achieving Low-Latency Real-Time Transcription with Whisper
\input{authors}


\begin{document}
\maketitle
\begin{abstract}
Whisper is one of the recent state-of-the-art multilingual speech recognition and translation models, however, it is not designed for real time transcription. In this paper, we build on top of Whisper and create \textbf{Whisper-Streaming}, an implementation of real-time speech transcription and translation of Whisper-like models. Whisper-Streaming uses local agreement policy with self-adaptive latency to enable streaming transcription. 
We show that Whisper-Streaming achieves high quality and 3.3 seconds latency on unsegmented long-form speech transcription test set, and we demonstrate its robustness and practical usability as a component in live transcription service at a multilingual conference.

%We show that it performs robustly, \XXX{when compared to Whisper -- not true}, in a variety of latency settings on different types of GPUs \XXX{no, -- in a real-life event}. %Our tests shows that the latency can be 2.5 seconds, in average, 
% \XXX{demonstration, test event}


\end{abstract}

\section{Introduction}

Whisper \cite{Whisper-paper} is a recent state-of-the-art system for automatic speech recognition (ASR) for 97 languages and for translation from 96 languages into English. Whisper models are publicly available under the MIT license. However, the current public implementations of Whisper inference usually allow only offline processing of audio documents that are completely available at the time of processing, without any processing time constraints.

Real-time streaming mode is useful in certain situations, e.g.\ for live
captioning. It means that the source speech audio has to be processed at the
time when it is being recorded. The transcripts or translations have to be
delivered with a short additive latency, e.g.\ in 2 seconds. There are some implementations of Whisper for streaming, but their approach is rather naive, they e.g.\ first record a 30-second audio segment, and then process it. The latency of these methods is large, and the quality on the segment boundaries is low because a simple content unaware segmentation can split a word in the middle.

In this work, we implement, evaluate and demonstrate Whisper in simultaneous
streaming mode using the simple but effective LocalAgreement
\cite{liu20sinterspeech} algorithm. LocalAgreement is one particular
streaming policy that can be used to convert any full-sequence to
full-sequence model to operate in simultaneous streaming mode. It was used
by the winning system CUNI-KIT at IWSLT 2022 simultaneous speech translation
shared task \cite{polak-etal-2022-cuni}. We call our implementation
\textbf{Whisper-Streaming}, although it is applicable to any model with API
similar to Whisper. According to our evaluation, it achieves 3.3 seconds
latency on average for English ASR on the European Parliament speech test
set ESIC \cite{machacek21_interspeech}, when running on NVIDIA A40 GPU, a fast hardware processing unit. We test it also on German and Czech ASR and present the  results and suggestions for the optimal parameters.

The contribution of this work is implementation, evaluation and demonstration of Whisper-Streaming. Given that Whisper-Streaming can be quickly and easily packaged into a product, we want to ensure that the most recent scientific results, such as the algorithm for simultaneous mode, can be accessible to and be used by industrial researchers and engineers. 
Furthermore, we want to reliably evaluate the performance of our implementation and share the results with the research community, to further drive research and development of real-time transcription solutions which have real-life use cases. We expect that our results can be used as strong baselines for future comparison.

We make Whisper-Streaming publicly available\footnote{\url{https://github.com/ufal/whisper_streaming}} along with a demonstration video.\footnote{\url{https://vimeo.com/840442741}}

% Raj: turn Whisper into Real-time
% + establishing practical utility of streaming mode
% + some kind of human eval -- no for ASR only
% De-En translation quality with different latencies
% Cs-En ESIC, Must-C ... let's find a test set

% Figure environment removed


\section{Background}

In this section, we describe the background for the back-end components of our work.

\paragraph{Whisper} \cite{Whisper-paper} is a Transformer model for
speech-to-text transcription and translation trained on a massive amount of
multilingual data. We use
``large-v2''\footnote{\url{https://huggingface.co/openai/whisper-large-v2}}
model because it achieves the highest quality of all Whisper model size options. %is known \XXX{Review Ondřej Plátek: give a reference} to give extremely high quality ASR and speech translation results. 
Since the original release of the whisper backend is rather slow, we use the \texttt{faster-whisper}\furl{https://github.com/guillaumekln/faster-whisper} reimplementation of Whisper inference using CTranslate2, a fast inference engine for Transformer models. It is approximately four times faster than the standard implementation (as reported by the authors). We use it with 16-bit float precision.

Although we primarily use Whisper, the underlying model in our implementation can be easily replaced by any other speech-to-text transcription or translation model (e.g.\ MMS, \inparcite{pratap2023mms}) if it produces word-level timestamps and punctuation.

% Another backend\footnote{\url{https://github.com/sanchit-gandhi/whisper-jax}} which uses JAX is also available, but it does not give time-stamps. Furthermore, it's unclear how fast this implementation works on CPU and how it compares to faster-whisper, which due to its reliance on CTranslate2 works well on CPUs as well as GPUs.

% \XXX{other backend implementation or ASR/ST possible, if it has punctuation and word-level timestamps. (E.g. https://github.com/sanchit-gandhi/whisper-jax has not the timestamps } 
%.% JAX

\paragraph{Streaming} 
% Let us assume we have a model $M$ for processing source sequence $c_1,\cdots,c_n$ into target sequence $t_1,\cdots,t_m$, given previous target output $s$. 
% In case of Whisper, the source sequence is digital audio split to chunks of specific duration. The target sequence is text. 
% The model $M$ infers the target $(t_1,\cdots,t_m) =  M(c_1,\cdots,c_n|s)$.
% In streaming, the source is available consecutively, one chunk at a time. There is a policy $P$ that predicts $t_T$, a target segment at time $T$ as $t_T = P_M(c_{i<T}|s,t_{j<T})$. The policy uses the source chunks $c_{i<T}$ that are available at time $T$, the previous target $s$ and the previous target segments $t_{i<T}$. The policy operates with the model $M$ and with its inputs and outputs. The policy is triggered every time when a new source segment is available. The resulting target segment can be empty, e.g.\ because of too short context.  

% The streaming objective is to minimize latency and to maximize the target quality.

Let us assume a model $M$ that processes a source sequence $c_1,\cdots,c_n$
into a target sequence $t_1,\cdots,t_m$, given a previous target 
$s$ that can be used for for inter-sentence coherence. Streaming involves receiving the source sequence consecutively, one chunk at a time, and producing the target simultaneously. A \textit{streaming policy} $P$ predicts a target segment $t_T$ at time $T$ as $t_T := P_M(c_{i<T}|s,t_{j<T})$. It operates the model $M$ on available source chunks $c_{i<T}$, previous sequence target $s$, and previous target segments $t_{j<T}$. The policy is triggered every time a new source segment is available. An empty target segment can be emitted, e.g.\ when waiting for context. The policy aims to minimize latency and maximize target quality. 

Streaming was originally proposed for simultaneous translation \cite{ma-etal-2019-stacl}, but it is applicable for any sequence-to-sequence task including ASR. \citet{dong-etal-2022-learning} give a summary.

\paragraph{LocalAgreement} 
% LocalAgreement \cite{liu20sinterspeech} is one particular streaming policy. It outputs the longest common prefix of the model on $n$ consecutive source chunks, or an empty segment when less than $n$ chunks are available. We follow the results of CUNI-KIT system from IWSLT 2022 shared task for simultaneous translation from English to German and Japanese \cite{polak-etal-2022-cuni}. The authors compared LocalAgreement to other policies (hold-$n$ and wait-$k$), with various chunk sizes.
% They suggest that the most effective policy is LocalAgreement with $n=2$. Therefore, we use  LocalAgreement-2 policy for detecting the stabilized target segments. 
% Compressed by chatgpt
\cite{liu20sinterspeech} is a streaming policy that outputs the longest common prefix of the model on $n$ consecutive source chunks, or an empty segment when less than $n$ chunks are available. Based on the IWSLT 2022 shared task on simultaneous translation, the CUNI-KIT system compared LocalAgreement to other policies (hold-$n$ and wait-$k$) with different chunk sizes. They found that LocalAgreement with $n=2$ was the most effective policy. Therefore, we use LocalAgreement-2 for identifying stabilized target segments.

\def\param#1{#1}
\def\MinChunkSize{\param{MinChunkSize}}


\section{Whisper-Streaming}
We describe the core components and inner workings of Whisper-Streaming. It consists of the update loop, audio buffer, skipping the confirmed output in audio buffer, trimming the buffer, joining for inter-sentence context, and optional voice activity detection.

% \paragraph{Update Loop} 
% The program consists of a loop that receives the source audio chunk, and then triggers an update of the streaming policy. The parameter \MinChunkSize{} defines the minimal duration that is processed in each iteration. If the computation of the update takes more time than \MinChunkSize{}, the next update is processed right away on the whole audio input that arrived by that time.

% The parameter \MinChunkSize{} controls the latency and quality. 

\paragraph{Update Loop}
The main part of Whisper-Streaming is a program that utilizes a loop to receive source audio chunks and trigger streaming policy updates. The parameter \MinChunkSize{} controls the latency and quality, and determines the minimal duration processed per iteration. If the update computation exceeds \MinChunkSize{}, the next update is performed immediately on the accumulated audio input. This parameter impacts both latency and quality.


% \paragraph{Segmentation}
% We implement the real-time streaming mode for the unsegmented source stream of long-form speech. We expect that the speech consists of multiple sentences in a row without any explicit marking of sentence boundaries, which is a realistic use-case. Alternatively, we could rely on an external segmentation tool that provides the sentence boundaries timing, or we could rely on the gold sentence segmentation.

\paragraph{Audio buffer}
% Whisper is trained to process individual sentences up to 30 seconds long. It also emits punctuation and word-level timestamps.\footnote{When using ``faster-whisper'' or other implementation that supports it.}

% We illustrate the process in \Cref{fig:updates}.
% In each update, we store the incoming audio at the top of the audio buffer (black rectangle in \Cref{fig:updates}), and process Whisper on the whole buffer. 
% We keep an invariant that the buffer always starts with a new sentence.
% We process LocalAgreement-2 on the current and previous Whisper output (see green rectangle in \Cref{fig:updates}). 
% We save the timestamp of the last word of the ``confirmed output'' (blue bar). In the next update, we reprocess Whisper from the beginning of the buffer, skipping the outputs in the part of buffer that precedes the last ``confirmed output'' timestamp (grey background in \Cref{fig:updates}). If the transcription changes in the part that is already confirmed, we ignore the change because it is usually negligible, not meaning altering.

Whisper is trained to handle sequences that are up to 30 seconds long and contain one full sentence. It provides punctuation and word-level timestamps.\footnote{When using ``faster-whisper'' or aannother implementation that supports it.} The process is illustrated in \Cref{fig:updates}. Each update involves storing incoming audio at the top of the audio buffer and processing the entire buffer with Whisper. We keep an invariant that the buffer always starts with a new sentence, to maintain the high quality of Whisper. LocalAgreement-2 is applied to the current and previous Whisper output. The timestamp of the last word in the ``confirmed output'' is saved. In subsequent updates, we always reprocess Whisper from the beginning of the buffer, including the portion preceding the last ``confirmed output'' timestamp (indicated by the gray background in \Cref{fig:updates}). Changes to the transcription in the confirmed portion are disregarded, as they are often insignificant in terms of meaning alteration.

\paragraph{Skipping the confirmed part}
% We detect, which transcribed word is behind or before the end of the last confirmed word from the previous update. The Whisper timestamps are not totally accurate and they are being updated by new audio chunks. Therefore, for each word in the new update if its timestamps is within 1 second interval from the last confirmed word, we compare its preceding $n$-grams for $n=\{1,\dots,5\}$. If they are equal to the suffix in the last confirmed output, we skip them. However, this rule could be improved, e.g.\ by setting and tuning character edit distance threshold, by trimming punctuation and casing from the $n$-grams, etc. We leave this for future work.

When determining the position of transcribed words relative to the last confirmed word from the previous update, we account for the potential inaccuracies and updates in Whisper timestamps due to new audio chunks. If a word's timestamp falls within a 1-second interval from the last confirmed word, we compare its preceding $n$-grams (where $n$ ranges from 1 to 5) with the suffix in the last confirmed output. If they match, we skip those words. However, this rule can be further enhanced in future work by incorporating measures such as setting and fine-tuning a character edit distance threshold, trimming punctuation and casing from the $n$-grams, etc.

\paragraph{Trimming the audio buffer}
% The audio buffer can not grow extensively because the processing of long buffer takes too long to catch up real-time latency. We trim it to approximately 30 seconds of maximum length.

% When the confirmed output contains a sentence final mark (e.g.\ a full stop or other punctuation symbol) that is followed by a word that starts a new sentence, we trim the audio buffer at the timestamp of the final mark. We use Moses sentence segmentation tool \cite{koehn-etal-2007-moses} that implements language specific rules for detecting the sentence boundaries. Therefore, the buffer always contains at most one sentence. 

To avoid inacceptably long spikes in latenc, the audio buffer is limited to around 30 seconds. When the confirmed output includes a sentence-ending punctuation mark followed by a word starting a new sentence, the buffer is trimmed at the punctuation mark's timestamp. A language specific sentence segmentation tool (e.g.\ \inparcite{koehn-etal-2007-moses}) is used for this purpose, ensuring that the buffer always contains a single sentence. Despite this, if the buffer length exceeds 30 seconds, we retain the last confirmed segment marked by Whisper.
% We do not trim on every confirmed word although it would speed up the processing, because the word timestamps are not sufficiently accurate and because Whisper also works better on the full sentences.

% If the buffer length exceeds 30 seconds after sentence boundary detection, we trim the buffer  on the last confirmed segment that is marked by Whisper. %There is a chance that they correspond to coherent phrases that 


\paragraph{Joining for inter-sentence context}
% Whisper transcribe function has a ``prompt'' parameter. It is supposed to contain the transcript of the previous audio buffer that has to be continued. It ensures the consistency within one document, e.g.\ the consistent style, terminology and references.

% We use the last 200 words of the confirmed output from the previous audio buffers as the ``prompt'' parameter. It is illustrated in \Cref{fig:updates} as yellow backgrounded text.

The Whisper transcribe function utilizes a ``prompt'' parameter to maintain consistency within a document (consistent style, terminology, and inter-sentence references). We extract the last 200 words from the confirmed output of previous audio buffers as the ``prompt'' parameter, as shown in \Cref{fig:updates} (yellow backgrounded text).

\paragraph{Voice activity detection}
% We use a parameter to turn Whisper's default voice activity detection (VAD) filter on or off. The parameter affects quality and latency.
There is a parameter to activate or deactivate Whisper's default voice activity detection (VAD) filter, impacting both quality and latency.


\section{Benchmarking Settings}
We describe the dataset for evaluation, metrics, settings and hardware we used to evaluate our model.

\paragraph{Evaluation Data} 
% Since we are studying the parameters for latency and quality, we use the dev set of ESIC corpus \cite{machacek21_interspeech}. It consists of 5 hours of original English speeches from the European Parliament, and its simultaneous interpreting into German and Czech. All the audio tracks with manual transcripts and word-level timestamps. We use ESIC for English, German and Czech ASR evaluation.

For latency and quality analysis, we utilize the dev set of the manually transcribed ESIC corpus \cite{machacek21_interspeech} for English, German, and Czech ASR containing 179 documents. This corpus contains 5 hours of original English speeches from the European Parliament, including simultaneous interpreting into German and Czech. It provides audio tracks with manual transcripts and word-level timestamps. 
% We employ the ESIC corpus for evaluating English, German, and Czech ASR.

% \XXX{why dev set and not test? because we're validating parameters, mostly chunk size, and observe latency-quality to recommend practical setup}

\paragraph{WER}
We use word error rate (WER) after removing punctuation and casing as the standard measure of ASR quality. 
% We report WER after removing punctuation and casing. %\XXX{add CER? unpunct WER? DM 8 June: leaving it for next time}

\paragraph{Latency}
% We report our own implementation of latency. ESIC contains the timestamps of all words in the gold transcripts. We align the gold transcripts to ASR using edit distance,\furl{https://pypi.org/project/edlib/} inferring the edit operation of each gold word. Then we count the ASR latency for all the gold words that are not deleted by the ASR. We define the ASR latency as the difference of the time when the ASR emitted the corresponding word and when the gold word was uttered by the speaker. 
% %We do not count latency for the words that were deleted by ASR.

% We count average latency within a document. When comparing setups on a set of multiple documents, we report the average and standard deviation of the document latency.

In our latency analysis, we implement our own method wherein we use the timestamps provided in the ESIC corpus to align the gold transcripts to the ASR output using edit distance.\furl{https://pypi.org/project/edlib/} This allows us to determine the edit operations for each gold word. We calculate the ASR latency by measuring the time difference between when the ASR emitted a word and when the corresponding gold word was spoken, excluding words deleted by the ASR. We compute the average latency within each document and, when comparing different setups across multiple documents, we report the average latency along with standard deviation.

\paragraph{Hardware}
For benchmarking, we use NVIDIA A40 GPUs. % which are fast and standardized. 
%We also experiment with NVIDIA L40 GPUs, but we observed that A40 GPUs performed better. 
We run Whisper on a computer in a cluster that is used by other processes at the same time, which may allocate the same resources and influence the latency. Since it is not always possible to have a dedicated server for a given service, this makes our evaluation very realistic. Since there will be variations in the latency metrics, we report mean and standard deviations. % on the same. 
% We report
% \XXX{write that it's actually good, realistic use-case raj.dabre: Something like: It is not always possible to have a dedicated server for a given service and thus our evaluation is realistic. However it might be nice to do several runs and report mean/std scores on any time metric raj.dabre: NVM I think you do it in section 4 :)}
% The exact However, we assume it is realistic setup In sectio
% , we study the reproducibility 
% \XXX{faster-whisper float16 precision -- put it somewhere}


\def\AForty{A40}
\def\LForty{L40}
\def\VADno{off}
\def\VADyes{on}
\def\pms{$\pm$}
\begin{table}[t]
    \centering
    \begin{tabular}{ll|rr}
  GPU & VAD &\% WER &  latency [s] \\
  \hline
\bf \AForty{} & \bf \VADno{} & \bf 5.8\pms{}0.9 & \bf 2.85\pms{}0.45 \\
\AForty{} & \VADyes{} & 5.2\pms{}0.9 & 3.12\pms{}0.36 \\
\LForty{} & \VADno{} & 5.1\pms{}1.0 & 3.58\pms{}0.62 \\
\LForty{} & \VADyes{} & 5.0\pms{}0.6 & 3.96\pms{}0.81 \\
    \end{tabular}
    \caption{Average (\pms{}stddev) WER and latency of English ASR of 10 repeated runs of ESIC dev.20080925.013\_007 document, with \MinChunkSize{} 0.1 seconds, using or not using VAD filter, on two GPU types. Bold is the setup that we later use.} 
    % \XXX{switch latency to seconds and 2 decimal digits. WER to 3 dec. digits.}}
    \label{tab:repro}
\end{table}

\paragraph{Ensuring Reproducibility} We simulate real-time processing of long-form transcription and record the  times when Whisper emitted the outputs.
We run the simulation on computers in a cluster that are not entirely under our control. For our simulation process, we block one GPU and a sufficient number of CPUs and RAM capacity. However, it can happen that other processes run at the same time, making a CPU and RAM load that is unpredictably slowing down our simulation. If the \MinChunkSize{} is smaller than time for processing an update, then two runs of the same simulation have different segmentation to chunks, leading to different WER and latency. 

Therefore, we run simulation of the same setup of one document 10 times, to measure the standard deviation of the latency and quality. The setup is English transcription of the ESIC dev.20080925.013\_007 document that is 3 minutes 36 seconds long, on NVIDIA A40 or L40 GPU with 48GB GPU RAM, 8 blocked CPU cores and 200GB of CPU RAM, with or without VAD filter, with \MinChunkSize{} 0.1 seconds.

The results are in %\Cref{fig:repro} and 
\Cref{tab:repro}. We observe small, negligible standard deviation in WER, below or near 1\%. The standard deviation in the average latency is much larger, from 0.36 to 0.81 seconds depending on the setup. 
% We recommend the setup on A40 GPU without VAD, because it has the lowest latency.
We conclude that we must be aware of the standard deviation of latency due to uncontrollable computation conditions. 
%It may be up to 811 miliseconds with L40 GPU and VAD filter.






\section{Results}

We evaluated Whisper-Streaming with various setups for English, German and Czech ASR. We first show the impact of outliers and voice activity detection (VAD) to determine optimal settings, and then present our main results with these settings.

\paragraph{Outliers} 
After processing many setups, we observed extraordinarily high WER on English ASR of a document dev2.20101213.015\_018\_EN\_Gallagher. We realized it is due to noise in the ESIC data set. The first half of the mentioned document is in Irish, and not English as intended. Only the English part is transcribed in gold, but Whisper transcribed both, leading to more precise transcription than the reference.
Except of the Gallagher document, all the reported setups achieved WER between 0 and 52\%, and average latency between 0 and 16.1 seconds. 
%The highest WER was caused by Whisper model hallucination, repeating several sentences. We report the 

\paragraph{VAD} 
We studied the effect of VAD (voice activity detection) filter that is integrated within Whisper backend. The results are in \Cref{tab:vad} and \Cref{fig:vad}. We realized that in ESIC corpus, it is advisable to deactivate the VAD filter for the English original speech, because it is very fluent, not interleaved with silence and has no non-voice sounds. Without VAD, the quality remains nearly the same (difference within 0.2\% WER), and the average latency was substantially lower, between 0.23 to 0.41 seconds.

For the processing of simultaneous interpreting, we recommend activating the VAD filter. The speech of a simultaneous interpreter contains many pauses, especially when waiting for context. With VAD, the latency was only 0.1 seconds larger, because VAD often filters out silence, which reduces the processing load. The quality with VAD was substantially higher, by 2 to 3 \% WER with shorter \MinChunkSize{} on German. With large chunk sizes, the quality is nearly the same (0.3 \% WER difference with 2 seconds \MinChunkSize) because a large chunk size causes the model to have large context and thus a low chance for risking uncertain output. Therefore,  we activated VAD for German and Czech simultaneous interpreting, and we deactivated it for English original speech.

For a real-life setup, we recommend starting Whisper-Streaming shortly before the speech actually starts, so that the first words are not missed, along with turning the VAD filter on so that the silence and non-voice sounds do not cause Whisper to make mistakes. If reducing the latency is important, an adaptive protocol for setting VAD on and off can be implemented. 

% \XXX{sddev in \Cref{sec:vad-stddev} }
% \XXX{in a real-life use-case, we recommend 1) always VAD on, Whisper halucinates when silence or non-voice sounds, 2) adaptive VAD -- turn it on for couple of times, consider turning off if no silence/noise many times in a row, 3) parallel VAD and no-VAD inference processing. VAD is faster, if it finds out there is nothing to filter out, return no-VAD. If VAD detects something, abort no-VAD ASR, cut silence/noise off, run Whisper again}



\def\LanDe{de}
\def\LanEn{en}
\def\LanCs{cs}
\def\VADyes{on}
\def\VADno{off}
\begin{table}[t]
\footnotesize{}
    \centering
    \setlength{\tabcolsep}{4pt}
    \resizebox{1.0\columnwidth}{!}{%
    \begin{tabular}{c@{~}c||rr|r||rr|r}
       %\begin{tabular}{c@{~}c||ll|l||ll|l}
   &        & \multicolumn{3}{c||}{avg.\ \% WER} & \multicolumn{3}{c}{avg. latency [s]} \\
 & m.ch. & \VADno{} & \VADyes{} & diff & \VADno{} & \VADyes{} & diff \\
\hline
% \multirow{4}{*}{\LanEn{}} 
%       & 0.1   & 0.084 &  0.083 & -0.001  &  \bf 3.30 & 3.72 & +0.41 \\
%       & 0.5   & 0.085 &  0.083 & -0.002  &  \bf 3.27 & 3.54 & +0.27 \\
%       & 1.0   & 0.081 &  0.081 & +0.001  &  \bf 3.62 & 3.88 & +0.26 \\
%       & 2.0   & 0.080 &  0.079 & -0.000  &  \bf 5.45 & 5.68 & +0.23 \\
% \hline
% \multirow{4}{*}{\LanDe{}} 
%       & 0.1   & 0.128 & \bf 0.097 & -0.031  &   3.83 & 3.93 & +0.10 \\
%       & 0.5   & 0.123 & \bf 0.095 & -0.028  &   3.97 & 4.11 & +0.14 \\
%       & 1.0   & 0.114 & \bf 0.094 & -0.020  &   4.19 & 4.37 & +0.18 \\
%       & 2.0   & 0.096 & \bf 0.093 & -0.003  &   5.79 & 5.94 & +0.15 \\
\multirow{4}{*}{\LanEn{}} 
      & 0.1s   & 8.4 &  8.3 & -0.1  &  \bf 3.30 & 3.72 & +0.41 \\
      & 0.5s   & 8.5 &  8.3 & -0.2  &  \bf 3.27 & 3.54 & +0.27 \\
      & 1.0s   & 8.1 &  8.1 & +0.1  &  \bf 3.62 & 3.88 & +0.26 \\
      & 2.0s   & 8.0 &  7.9 & -0.0  &  \bf 5.45 & 5.68 & +0.23 \\
\hline
\multirow{4}{*}{\LanDe{}} 
      & 0.1s   & 12.8 & \bf 9.7 & -3.1  &   3.83 & 3.93 & +0.10 \\
      & 0.5s   & 12.3 & \bf 9.5 & -2.8  &   3.97 & 4.11 & +0.14 \\
      & 1.0s   & 11.4 & \bf 9.4 & -2.0  &   4.19 & 4.37 & +0.18 \\
      & 2.0s   & 9.6 & \bf 9.3 & -0.3  &   5.79 & 5.94 & +0.15 \\
    \end{tabular}}
    \caption{Impact of VAD filter on WER and latency on ESIC dev on the streaming ASR with different minimum chunk size (m.ch., in seconds) of the English original speech (\LanEn{}) and German simultaneous interpreting (\LanDe{}). 
    We highlight the remarkable benefit in bold: the original speech without pauses is processed with lower latency (by 0.23 seconds or more) and comparable quality with VAD off. On the contrary, the VAD on achieves higher quality for interpreting with frequent pauses, with small difference in latency.
    }
    %We recommend using VAD for simultaneous interpreting  (\LanDe{}) because of quality gains above 2\% WER (for m.ch. $\leq$ 1s, in bold), and not setting VAD for the speech without pauses), where the latency is above 0.23 seconds lower (in bold) compared to using VAD.}
    \label{tab:vad}
\end{table}

% Figure environment removed



% % Figure environment removed


\paragraph{Performance}

% \XXX{results with comp aware vs unaware and offline}
% \Cref{fig:bounds} \XXX{in \Cref{fig:bounds}, VAD is set for De and Cs, not for En. In offline mode, VAD is set for all 3 langs.}

% \XXX{explain the plot and \Cref{tab:unaware}}

% \XXX{why is unaware latency sometimes less than twice m.ch.? it's local-agreement-2, it must be higher. Dominik's explanations: 1) anticipation -- the model can guess the next word earlier than it hears it for sure. Especially in English, it knows it well. 2) the timestamps in ESIC are automatic, inaccurrate. It could be because of it. 
% 3) in gold, we're considering the middle of the word as its actual timestamp... no, it's not a reason.
% I'm assuming the anticipation.
% Or, something could be misaligned -- negative latency for part of document, in avg. very low.
% }

\Cref{tab:unaware} and \Cref{fig:bounds} summarize the WER and average latency of Whisper-Streaming on ESIC validation set for the three language tracks. Overall, with 1 second \MinChunkSize{}, the average latency is 3.3 seconds for English, 4.4 seconds for German and 4.8 seconds for Czech, while the WER is by 2\% higher than in the offline mode for English and German, and by 6\% higher for Czech. 
Both WER and latency is the lowest on English, followed by German and Czech. This is related to the amount of language specific data used for training Whisper, as well as the morphological complexity of these languages. The latency increases with larger uncertainty because it requires more updates for an agreement. Moreover, the larger \MinChunkSize{}, the larger the latency, but higher the quality because the system has sufficient context. 

\paragraph{Offline mode WER}
We contrast the results with setups that serve as maximum performance estimates. One of them is offline mode in which processing of the whole audio document is done after recording, without any limitations on processing time. It is the default and most optimized setup for Whisper. The WER in offline mode and with VAD is lower than in streaming mode because the context size is not restricted. The model can use even the right (future) context that is unavailable or limited in streaming mode. Moreover, the internal segmentation of the long-form speech into processing chunks is optimized in the offline mode.

\paragraph{Computationally unaware latency}
Another contrastive setup is computationally unaware simulation. It uses an unrealistic assumption that computation for Whisper processing any audio segment is instant, so that the latency caused by computation is not included in the latency measurement. The measurement includes latency caused by uncertainty in the language. The gap between latency in computationally unaware and aware evaluation can be reduced by optimizing the hardware or inference algorithm. Computationally unaware latency can be reduced by improving the model or streaming policy.

We observe that the average computationally unaware latency is approximately twice the chunk size. This is expected because we use local agreement of two consecutive updates. However, the processing of English is actually faster, little less than twice the chunk size. We hypothesize that this could be caused by the anticipation ability of Whisper model. The second possible reason is the inaccuracy of the gold timestamps in ESIC. The timestamps were computed by automatic forced alignment, and thus they may be less accurate in non-standard situations such as overlapping and non-transcribed speech, e.g.\ hesitations and foreign language insertions.

% Figure environment removed

\def\unaware{un.}
\def\aware{aw.}
\begin{table}[h]
% \footnotesize
    \centering
    \setlength{\tabcolsep}{4pt}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{cc|r|rr|rrr}
    %{cc|l|ll|lll}
    &  \% WER       &       & \multicolumn{2}{c|}{\% WER} & \multicolumn{3}{c}{latency [s]} \\
lang. & offline & m.ch. & \unaware{} & \aware{} &  \unaware{} & \aware{} & diff \\
%lang. & \begin{tabular}[c]{@{}l@{}}off.\%\\ WER\end{tabular} & m.ch. & \unaware{} & \aware{} &  \unaware{} & \aware{} & diff \\
\hline
\multirow{3}{*}{\LanEn{}} & \multirow{3}{*}{7.9} 
      & 0.5s   & 9.7 & 8.5   &   1.02 & 3.27 & +2.25 \\
   &   & 1.0s   & 8.5 & 8.1   &   1.91 & 3.62 & +1.71 \\
   &   & 2.0s  & 8.8 & 8.0   &   3.73 & 5.45 & +1.73 \\
   \hline
\multirow{3}{*}{\LanDe{}} & \multirow{3}{*}{9.2} 
      & 0.5s   & 11.1 & 9.5   &   1.11 & 4.11 & +3.00 \\
   &   & 1.0s   & 10.0 & 9.4   &   2.02 & 4.37 & +2.35 \\
   &   & 2.0s   & 10.2 & 9.3   &   3.89 & 5.94 & +2.05 \\
   \hline
\multirow{3}{*}{\LanCs{}} & \multirow{3}{*}{12.3} 
      & 0.5s   & 15.8 & 13.3   &   1.25 & 4.69 & +3.44 \\
  &  & 1.0s   & 13.8 & 12.9   &   2.24 & 4.76 & +2.51 \\
    &  & 2.0s   & 14.0 & 12.8   &   4.29 & 6.29 & +2.00 \\
    \end{tabular}}
    \caption{WER and average latency of Whisper-Streaming on ESIC dev set in three language tracks using different \MinChunkSize{} (``m.ch.''). The realistic setup is computationally aware (``\aware{}''), put into contrast with 
    offline WER (``offline'') and with the computationally unaware simulation (``\unaware{}'').
	The data are the same as in \Cref{fig:bounds}.
	}
    \label{tab:unaware}
\end{table}

% \paragraph{\XXX{}Model size -- maybe?}

% \XXX{maybe report it, but rather not}

% \paragraph{\XXX{}Document latency}

% \XXX{it would be nice to report word latency within one document, as in Subtitler paper. To demonstrate that the latency is variable}

% \section{\XXX{}Features and Issues}

% \XXX{It could be nice to report cherry-picked examples of interesting features -- but it's too much work to find them. And it would be nice to honestly report the issues -- but it's not good to publish them too openly :)}

% - foreign language sentences are handled very well, thanks to multi-lingual Whisper

% - Whisper is able to do zero-shot En->X translation, but low quality

% - in Czech and Slovak, we observed lots of small ASR errors, that are correctable by human without audio, such as "Tady Jelenka Kabrelová -> Tady je Lenka Kabrhelová". Multi-pivoting could be useful, because Cs->En Whisper had the proper name correct.

% - detecting the confirmed output boundary is an issue -- currently, it sometimes repeats 1 to 2-grams, sometimes identical, sometimes changed by e.g. 2 characters edit distance. It's hard to detect

% - VAD could be optimized -- don't process the same audio again, appended by VAD filtered-out chunk. 

% - sometimes Whisper doesn't produce end of sentence full stops, but starts a new sentence with capital letter. Then it's hard for subsequent tasks -- translation etc. 

\section{System Demonstration}

\paragraph{Demonstration video} is available at \url{https://vimeo.com/840442741}. It is a screencast video of Whisper-Streaming real-time outputs that processes live ASR on one ESIC document in three parallel instances for English, German and Czech speech, the original and simultaneous interpreting. The video shows a contrast to gold transcripts with original timing, so that the latency can be observed. The video also contains color highlighting for ASR errors.

\paragraph{Integration with ELITR} To demonstrate practical usability, we integrate Whisper-Streaming with the ELITR (European Live Translator, \inparcite{bojar-etal-2020-elitr}) framework for complex distributed systems for multi-source and multi-target live speech transcription and translation \cite{bojar-etal-2021-elitr}. Within Whisper-Streaming, we implement and release a server that is connected as a worker to Mediator server \cite{franceschini-etal-2020-removing}. Mediator allows a client to request a service of a worker. The client is then allowed to further process the text outputs received by the worker, e.g.\ translate them with another worker and present them at the web view server that delivers real-time captions to event participants during a live multilingual event. %Screenshot of the web view is at \citet{bojar-etal-2021-elitr}. 




\paragraph{Evaluation event}
We evaluated Whisper-Streaming as a component in an experimental live speech translation service at a multilingual conference. For this, we built a pipeline that used five parallel Whisper-Streaming workers, three of them for ASR only (English, Czech and Ukrainian), and two for speech translation (Czech-to-English and Ukrainian-to-English). There were three parallel language streams at the conference, Czech, English and Ukrainian. One of the languages was spoken at the main floor, and the others were provided by human simultaneous interpreting. 

A human operator (as in \inparcite{bojar-etal-2021-operating}) was controlling the technical setup and the outputs using the language knowledge and had an option to redirect the streams, if necessary.
%target English not from the English interpreting, but from Czech floor, from  technical reasons.
The qualitative evaluation at the event showed that Whisper-Streaming is a robust and reliable part of the service, reaching acceptable latency and unexpectedly high quality on English, Czech and Ukrainian long-form speech.

\paragraph{Demonstration at AACL}
Our system demonstration at the IJCNLP-AACL 2023 conference (if accepted) will use the ELITR framework. We will either simulate speech source from a recording, or allow participants to speak into microphone in any of the 97 languages supported by Whisper, and observe the real-time outputs.

\section{Conclusion}

We implemented, evaluated and demonstrated Whisper-Streaming, a tool that effectively operates an offline ASR model Whisper, with 3.3 second average computationally aware latency on English ESIC corpus. 
We described and explained the implementation and its underlying components, including LocalAgreement algorithm for streaming. Lastly, we demonstrated the robustness and practical usability at a real-life multi-lingual conference.

% \XXX{}
% In this paper, we describe Whisper-Streaming, a real time speech transcription model. We use local-agreement to enable the original Whisper model to work in streaming mode, and we find that this helps improve the performance in a real-time speech transcription setting. \XXX{}We benchmark Whisper-Streaming in a variety of latency and performance requirement settings on various GPUs and observe that it works reliably, indicating its practical utility.

\section{Limitations}

% \XXX{bullet points -- make nicer}

% - ESIC is old, it could be leaked in Whisper training data

% - computation cost, performance tests on more affordable hardware is pending

% - latency/quality reported on ESIC may not be generalizable to other langs. The documents are short, there may be some bugs with long documents

% - we do not optimize the algorithm or implementation, but only demonstrate Whisper onlinization

% - the actual latency is fluctuating, we report avg! the latency can be higher, no upper bound 

% - no tests on long-form X-to-English ST, because we don't have a long-form speech test set
% \XXX{I need to correct this}First, the ESIC corpus used for training Whisper is relatively old \XXX{time}, which raises concerns about potential data leakage.
The data collected in ESIC corpus were created relatively long time ago. It raises concerns about potential leakage into Whisper training set, which could compromise our evaluation. 
Additionally, performance tests on more affordable hardware are pending, highlighting the need for further evaluation in terms of computational cost.

It is worth noting that the reported latency and quality metrics obtained from ESIC may not be fully generalizable to other languages or language variants due to the nature of the corpus.  
%There is also a possibility of encountering technical issues when processing longer audio d

Furthermore, our focus is on demonstrating the online capabilities of Whisper rather than optimizing the algorithm or implementation. It is important to recognize that the actual latency experienced may fluctuate, and the reported average latency serves as an indicative measure without providing an upper bound. The streaming policy would need certain modifications to guarantee a maximum latency, at a possible loss in quality.

% Lastly, we have not conducted tests on long-form speech translation from other languages to English due to the absence of a dedicated long-form speech test set for evaluation.

Lastly, we have not conducted comparison tests to other state-of-the-art systems, e.g.\ from IWSLT, because a common evaluation framework is pending, as well as X-to-English long-form speech test set.

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

% \appendix

% \section{\XXX{VAD stddev}}
% \label{sec:vad-stddev}

% \Cref{tab:vad-stddev}

% \begin{table}[ht]
%     \centering
%     \begin{tabular}{l@{~}l@{~}l|rr}
%       lan. & mCh & VAD & WER &  latency [s] \\
%   \hline
% % \LanEn{} & 0.1 & \VADno{} & 0.0840\pms{}0.0346 & 3304\pms{}834 \\
% % \LanDe{} & 0.1 & \VADyes{} & 0.0971\pms{}0.0397 & 3926\pms{}916 \\
% % \LanCs{} & 0.1 & \VADyes{} & 0.1353\pms{}0.0497 & 4584\pms{}991 \\
% % \LanEn{} & 0.5 & \VADno{} & 0.0845\pms{}0.0364 & 3266\pms{}1077 \\
% % \LanDe{} & 0.5 & \VADyes{} & 0.0947\pms{}0.0386 & 4108\pms{}1165 \\
% % \LanCs{} & 0.5 & \VADyes{} & 0.1327\pms{}0.0498 & 4686\pms{}1286 \\
% % \LanEn{} & 1.0 & \VADno{} & 0.0806\pms{}0.0355 & 3619\pms{}695 \\
% % \LanDe{} & 1.0 & \VADyes{} & 0.0939\pms{}0.0385 & 4369\pms{}888 \\
% % \LanCs{} & 1.0 & \VADyes{} & 0.1295\pms{}0.0495 & 4757\pms{}912 \\
% % \LanEn{} & 2.0 & \VADno{} & 0.0799\pms{}0.0349 & 5454\pms{}878 \\
% % \LanDe{} & 2.0 & \VADyes{} & 0.0927\pms{}0.0377 & 5946\pms{}1104 \\
% % \LanCs{} & 2.0 & \VADyes{} & 0.1280\pms{}0.0493 & 6285\pms{}1120 \\
% \LanEn{} & 0.1 & \VADno{} & 0.084\pms{}0.035 & 3.30\pms{}0.83 \\
% \LanEn{} & 0.1 & \VADyes{} & 0.083\pms{}0.036 & 3.72\pms{}1.14 \\
% \LanEn{} & 0.5 & \VADno{} & 0.085\pms{}0.036 & 3.27\pms{}1.08 \\
% \LanEn{} & 0.5 & \VADyes{} & 0.083\pms{}0.036 & 3.54\pms{}0.74 \\
% \LanEn{} & 1.0 & \VADno{} & 0.081\pms{}0.035 & 3.62\pms{}0.70 \\
% \LanEn{} & 1.0 & \VADyes{} & 0.081\pms{}0.035 & 3.88\pms{}0.75 \\
% \LanEn{} & 2.0 & \VADno{} & 0.080\pms{}0.035 & 5.45\pms{}0.88 \\
% \LanEn{} & 2.0 & \VADyes{} & 0.079\pms{}0.034 & 5.68\pms{}0.88 \\
% \hline
% \LanDe{} & 0.1 & \VADno{} & 0.128\pms{}0.048 & 3.83\pms{}0.93 \\
% \LanDe{} & 0.1 & \VADyes{} & 0.097\pms{}0.040 & 3.93\pms{}0.92 \\
% \LanDe{} & 0.5 & \VADno{} & 0.123\pms{}0.046 & 3.97\pms{}1.25 \\
% \LanDe{} & 0.5 & \VADyes{} & 0.095\pms{}0.039 & 4.11\pms{}1.16 \\
% \LanDe{} & 1.0 & \VADno{} & 0.114\pms{}0.044 & 4.19\pms{}1.11 \\
% \LanDe{} & 1.0 & \VADyes{} & 0.094\pms{}0.039 & 4.37\pms{}0.89 \\
% \LanDe{} & 2.0 & \VADno{} & 0.096\pms{}0.041 & 5.79\pms{}1.03 \\
% \LanDe{} & 2.0 & \VADyes{} & 0.093\pms{}0.038 & 5.94\pms{}1.10 \\
%     \end{tabular}
%     \caption{\XXX{contrast VAD on and off for En and De -- it has stddev, unlike \Cref{tab:vad}}}
%     \label{tab:vad-stddev}
% \end{table}

\end{document}
