\section{Stochastic Gradient Descent with \texorpdfstring{$\SW$}{SW} as Loss}\label{sec:SGD}

Training Sliced-Wasserstein generative models consists in training a neural network
\begin{equation}\label{eqn:T}
	T: \app{\R^{\dimp}\times \R^{\dimx}}{\R^{\dimy}}{(\p, x)}{T_\p(x) := T(\p, x)}
\end{equation}
by minimising the SW minibatch loss $\p \longmapsto\mathbb{E}_{X\sim\mxn, Y\sim\myn}\left[\SW_2^2(T_u\#\bbgamma_X, \bbgamma_Y)\right]$ through Stochastic Gradient Descent (as described in \ref{alg:SGD}). The probability distribution $\mx \in \mathcal{P}_2(\R^{\dimx})$ is the law of the input of the generator $T(\p,\cdot)$. The distribution $\my \in \mathcal{P}_2(\R^{\dimy})$ is the data distribution, which $T$ aims to simulate. Finally, $\bbsigma$ will denote the uniform measure on the unit sphere of $\R^{\dimy}$, denoted by $\SS^{\dimy-1}$. Given a list of points $X = (x_1, \cdots, x_\npoints) \in \R^{\npoints \times \dimx},$ denote the associated discrete uniform measure $\bbgamma_X := \frac{1}{\npoints}\sum_i\bbdelta_{x_i}$. By abuse of notation, we write $T_\p(X) := (T_\p(x_1), \cdots, T_\p(x_\npoints)) \in \R^{\npoints \times \dimy}$. \redtwo{The reader may find a summary of this paper's notations in \ref{tab:notations}.}
% Figure environment removed

In the following, we will apply results from \citep{bianchi2022convergence}, and we pave the way to the application of these results by presenting their theoretical framework. Consider a sample loss function $f: \R^{\dimp} \times \Data \longrightarrow \R$ that is locally Lipschitz in the first variable, and $\mdata$ a probability measure on $\Data \subset \R^d$ which is the law of the samples drawn at each SGD iteration. Consider $\varphi: \R^\dimp \times \Data \longrightarrow \R^\dimp$ an \textit{almost-everywhere gradient} of $f$, which is to say that for almost every $(\p, \data) \in \R^{\dimp}\times \Data,\; \varphi(\p, \data) = \partial_\p f(\p, \data)$ (since each $f(\cdot, \data)$ is locally Lipschitz, it is differentiable almost-everywhere by Rademacher's theorem). The complete loss function is the expectation of the sample loss, $F := \p \longrightarrow \int_\Data f(\p, \data)\dd\mdata(\data)$. An SGD trajectory of step $\lr > 0$ for $F$ is a sequence $(\p^{(t)}) \in (\R^\dimp)^\N$ of the form:
$$\p^{(t+1)} = \p^{(t)} - \lr \varphi(\p^{(t)}, \data^{(t+1)}),\quad \left(\p^{(0)}, (\data^{(t)})_{t \in \N}\right) \sim \mpzero \otimes \mdata^{\otimes \N}, $$
where $\mpzero$ is the distribution of the initial position $\p^{(0)}$. Within this framework, we define an SGD scheme described by \ref{alg:SGD}, with $\mdata := \mxn \otimes \myn \otimes \bbsigma$ and the minibatch SW sample loss
\begin{equation}\label{eqn:f}
	f:= \app{\R^{\dimp} \times \R^{\npoints \times \dimx} \times \R^{\npoints \times \dimy} \times \SS^{\dimy-1}}{\R^\dimy}{(\p, X, Y, \theta)}{\W_2^2(P_{\theta}\#T_\p\#\bbgamma_{X}, P_{\theta}\#\bbgamma_{Y})} .
\end{equation}
With this definition for $f$, we have
\begin{equation}\label{eqn:F}
	F(\p) = \mathbb{E}_{(X, Y, \theta) \sim \mdata}\left[f(\p, X, Y, \theta)\right] = \mathbb{E}_{(X, Y) \sim \mxn \otimes \myn}\left[\SW_2^2(T_\p\#\bbgamma_X, \bbgamma_Y)\right],
\end{equation}
thus the population loss compares the "true" data $\my$ with the model's generation $T_\p\#\mx$ using (minibatch) SW. We now wish to define an almost-everywhere gradient of $f$. To this end, notice that one may write $f(\p, X, Y, \theta) = w_\theta(T(\p, X), Y)$, where for $X, Y \in \R^{\npoints \times \dimy}$ and $\theta \in \SS^{\dimy-1},\; w_\theta(X, Y) := \W_2^2(P_\theta\#\bbgamma_X, P_\theta\#\bbgamma_Y)$. The differentiability properties of $w_\theta(\cdot, Y)$ are already known \citep{discrete_sliced_loss, bonneel2015sliced}, in particular one has the following almost-everywhere gradient of $w_\theta(\cdot, Y):$
$$\dr{X}{}{w_\theta}(X, Y) = \left(\cfrac{2}{\npoints}\theta\theta^\top (x_k - y_{\sigma_\theta^{X, Y}(k)})\right)_{k \in \llbracket 1, \npoints \rrbracket} \in \R^{\npoints \times \dimy},$$
where the permutation $\sigma_\theta^{X, Y} \in \mathfrak{S}_\npoints$ is $\sort{Y}{\theta} \circ (\sort{X}{\theta})^{-1}$, with $\sort{Y}{\theta}\in \mathfrak{S}_\npoints$ being a sorting permutation of the list $(\theta^\top  y_1, \cdots, \theta^\top  y_\npoints)$. The sorting permutations are chosen arbitrarily when there is ambiguity. To define an almost-everywhere gradient, we must differentiate $f(\cdot, X, Y, \theta) = \p \longmapsto w_\theta(T(\p, X), Y)$ for which we need regularity assumptions on $T$: this is the goal of \ref{ass:C2_ae}. In the following, $\oll{A}$ denotes the topological closure of a set $A$, $\partial A$ its boundary, and $\bblambda_{\R^\dimp}$ denotes the Lebesgue measure of $\R^\dimp$.

\begin{assumption}\label{ass:C2_ae}
	For every $x \in \R^{\dimx},\;$ there exists a family of disjoint connected open sets $(\U_j(x))_{j \in J(x)}$ such that $\forall j \in J(x),\; T(\cdot, x) \in \mathcal{C}^2(\U_j(x), \R^\dimy)$, $\Reu{j\in J(x)}{}\oll{\U_j(x)} = \R^\dimp$ and $\bblambda_{\R^\dimp}\Big(\Reu{j\in J(x)}{}\partial \U_j(x)\Big) = 0$.
\end{assumption}

Note that for measure-theoretic reasons, the sets $J(x)$ are assumed countable. \redtwo{One may understand this assumption broadly as the neural networks $T$ being piecewise smooth with respect to the parameters $\p$, where the pieces depend on the input data $x$. In practice, \ref{ass:C2_ae} is an assumption on the activation functions of the neural network. For instance, it is of course satisfied in the case of smooth activations, or in the common case of piecewise polynomial activations. We detail suitable neural networks in the Appendix (\ref{sec:suitable_NNs}).}
%

\ref{ass:C2_ae} implies that given $X, Y, \theta$ fixed, $f(\cdot, X, Y, \theta)$ is differentiable almost-everywhere, and that one may define the following almost-everywhere gradient \ref{eqn:ae_grad}.
\begin{equation}\label{eqn:ae_grad}
	\varphi : \app{\R^{\dimp} \times \R^{\npoints \times \dimx} \times \R^{\npoints \times \dimy} \times \SS^{\dimy-1}}{\R^{\dimp}}{(\p, X, Y, \theta)}{\Sum{k=1}{\npoints} \cfrac{2}{\npoints}\left(\dr{\p}{}{T}(\p, x_k)\right)^\top  \theta \theta^\top (T(\p, x_k) - y_{\sigma_\theta^{T(\p, X), Y}(k)})},
\end{equation}
where for $x \in \R^\dimx,\; \dr{\p}{}{T}(\p, x)\in \R^{\dimy \times \dimp}$ denotes the matrix of the differential of $\p \longmapsto T(\p, x)$, which is defined for almost-every $\p$. Given $\p \in \partial \U_j(x)$ (a point of potential non-differentiability), take instead $0$. (Any choice at such points would still define an a.e. gradient, and will make no difference).

Given a step $\lr > 0$, and an initial position $\p^{(0)} \sim \mpzero$, we may now define formally the following fixed-step SGD scheme for $F$:
\begin{equation}\label{eqn:SW_SGD}
	\begin{split}
		\p^{(t+1)} = \p^{(t)} - \lr \varphi(\p^{(t)}, X^{(t+1)}, Y^{(t+1)}, \theta^{(t+1)}), \\ \left(\p^{(0)}, (X^{(t)})_{t \in \N}\ (Y^{(t)})_{t \in \N}\ (\theta^{(t)})_{t \in \N}\right) \sim \mpzero \otimes \mxN \otimes \myN \otimes \bbsigma^{\otimes \N}. 
	\end{split}
\end{equation}
An important technicality that we must verify in order to apply \citet{bianchi2022convergence}'s results is that $\p \longmapsto f(\p, X, Y, \theta)$ and $F$ are locally Lipschitz. Before proving those claims, we reproduce a useful Property from \citep{discrete_sliced_loss}. In the following, $\|X\|_{\infty, 2}$ denotes $\underset{k \in \llbracket 1, \npoints \rrbracket}{\max}\ \|x_k\|_2$ given $X = (x_1, \cdots, x_\npoints) \in \R^{\npoints \times \dimx}$, and $B_{\mathcal{N}}(x, r)$ for $\mathcal{N}$ a norm on $\R^\dimx$, $x \in \R^\dimx$ and $r>0$ shall denote the open ball of $\R^\dimx$ of centre $x$ and radius $r$ for the norm $\mathcal{N}$ (if $\mathcal{N}$ is omitted, then $B$ is an euclidean ball).

\begin{prop}The $(w_\theta(\cdot, Y))_{\theta \in \SS^{\dimy-1}}$ are uniformly locally Lipschitz \citep{discrete_sliced_loss} Prop. 2.1.\label{prop:w_unif_locLip}\
	
	Let $K_w(r, X, Y) := 2\npoints(r + \|X\|_{\infty, 2} + \|Y\|_{\infty, 2})$, for $X, Y \in \R^{\npoints \times \dimy}$ and $r>0$. Then $w_\theta(\cdot, Y)$ is $K_w(r, X, Y)$-Lipschitz in the neighbourhood $B_{\|\cdot\|_{\infty, 2}}(X, r)$:
	$$\forall Y', Y'' \in B_{\|\cdot\|_{\infty, 2}}(X, r),\; \forall \theta \in \SS^{\dimy-1},\; |w_\theta(Y', Y) - w_\theta(Y'', Y)| \leq K_w(r, X, Y) \|Y'-Y''\|_{\infty, 2}.$$
	
\end{prop}

In order to deduce regularity results on $f$ and $F$ from \ref{prop:w_unif_locLip}, \redtwo{we will make the assumption that $T$ is globally Lipschitz in $(\p, x)$. In practice, this is the case when both parameters are enforced to stay within a fixed bounded domain, for instance by multiplying a typical NN with the indicator of such a set. We present this in detail in the Appendix (\ref{sec:suitable_NNs}).}

\begin{assumption}\label{ass:T_loclip}
	There exists $\lipT>0$ such that 
	$$\forall (\p_1, \p_2, x_1, x_2) \in (\R^{\dimp})^2 \times (\R^{\dimx})^2,\; \|T(\p_1, x_1) - T(\p_2, x_2)\|_2 \leq \lipT\left(\|\p_1-\p_2\|_2 + \|x_1 - x_2\|_2\right).$$
\end{assumption}

\begin{prop}\label{prop:f_loclip} Under \ref{ass:T_loclip}, for $\varepsilon > 0,\; \p_0 \in \R^{\dimp},\; X\in \R^{\npoints \times \dimx},\; Y\in \R^{\npoints \times \dimy}$ and $\theta \in \SS^{\dimy-1}$, let $K_f(\varepsilon, \p_0, X, Y) := 2\lipT\npoints(\varepsilon \lipT + \|T(\p_0, X)\|_{\infty, 2} + \|Y\|_{\infty, 2})$. Then $f(\cdot, X, Y, \theta)$ is $K_f(\varepsilon, \p_0, X, Y)$-Lipschitz in $B(\p_0, \varepsilon)$: 
	$$\forall \p, \p' \in B(\p_0, \varepsilon),\; |f(\p, X, Y, \theta) - f(\p', X, Y, \theta)| \leq K_f(\varepsilon, \p_0, X, Y)\|u-u'\|_2.$$
	
\end{prop}

\begin{proof}
	Let $\varepsilon > 0,\; \p_0 \in \R^{\dimp},\; X\in \R^{\npoints \times \dimx},\; Y\in \R^{\npoints \times \dimy}$ and $\theta \in \SS^{\dimy-1}$. Let $\p, \p' \in B(\p_0, \varepsilon)$. Using \ref{ass:T_loclip}, we have $T(\p, X), T(\p', X) \in B_{\|\cdot\|_{\infty, 2}}(T(\p_0, X), r)$, with $r := \varepsilon\lipT$.	
	
	\red{Denoting $\lipT := \lipT_{\oll{B}(u_0, \varepsilon), \oll{B}(0_{\R^\dimx}, \|X\|_{\infty, 2})}$, we apply successively \ref{prop:w_unif_locLip} (first inequality), then \ref{ass:T_loclip} (second inequality):}
	\begin{align*}|f(\p, X, Y, \theta) - f(\p', X, Y, \theta)| &= |w_\theta(T(\p, X), Y) - w_\theta(T(\p', X), Y)|\\
		&\leq K_w(r, T(\p_0, X), Y) \|T(\p, X) - T(\p', X)\|_{\infty, 2} \\
		&\leq 2\npoints(\varepsilon \lipT + \|T(\p_0, X)\|_{\infty, 2} + \|Y\|_{\infty, 2}) \lipT\|u-u'\|_2.
	\end{align*}
	\vspace{-10pt}
\end{proof}

\ref{prop:f_loclip} shows that $f$ is locally Lipschitz in $\p$. We now assume some conditions on the measures $\mx$ and $\my$ in order to prove that $F$ is also locally Lipschitz. \redtwo{Specifically, we require that the data measures $\mx$ and $\my$ be supported on bounded domains, which imposes little restriction in practice.}

\begin{assumption}\label{ass:mx_my}
	$\mx$ and $\my$ are Radon probability measures on $\R^\dimx$ and $\R^\dimy$ respectively, supported by the compacts $\X$ and $\Y$ respectively. Denote $R_x := \underset{x \in \X}{\sup}\ \|x\|_2$ and $R_y := \underset{y \in \Y}{\sup}\ \|y\|_2$.
\end{assumption}

\begin{prop}\label{prop:F_loclip}
	Assume \ref{ass:T_loclip} and \ref{ass:mx_my}. For $\varepsilon > 0,\; \p_0 \in \R^{\dimp},$ let $C_1(\p_0) := \Int{\Xn}{}\|T(\p_0, X)\|_{\infty, 2} \dd\mxn(X)$ and $C_2 := \Int{\Yn}{}\|Y\|_{\infty, 2}\dd \myn(Y)$.
	
	Let $K_F(\varepsilon, \p_0) := 2\lipT\npoints(\varepsilon \lipT + C_1(\p_0) + C_2)$. We have $\forall \p, \p' \in B(\p_0, \varepsilon),\; |F(\p) - F(\p')| \leq K_F(\varepsilon, \p_0)\|\p-\p'\|_2$.
\end{prop}

\begin{proof}
	Let $\varepsilon > 0,\; \p_0 \in \R^{\dimp}$ and $\p, \p' \in B(\p_0, \varepsilon)$. We have	
	\begin{align*} |F(\p) - F(\p')| &\leq \Int{\Xn \times \Yn \times \SS^{\dimy-1}}{}|f(\p, X, Y, \theta) - f(\p', X, Y, \theta)|\dd \mxn(X) \dd \myn(Y) \dd\bbsigma(\theta) \\
		&\leq \Int{\Xn \times \Yn}{}K_f(\varepsilon, \p_0, X, Y)\|\p-\p'\|_2\dd \mxn(X) \dd \myn(Y)\\
		&\leq \Int{\Xn \times \Yn}{}2\lipT\npoints(\varepsilon \lipT + \|T(\p_0, X)\|_{\infty, 2} + \|Y\|_{\infty, 2})\|\p-\p'\|_2\dd \mxn(X) \dd \myn(Y).
	\end{align*}
	\red{Now by \ref{ass:T_loclip}, $X \longmapsto \|T(\p_0, X)\|_{\infty, 2}$ is continuous on the compact $\Xn$, thus upper-bounded by a certain $M(u_0) > 0$. We can define $C_1(\p_0) := \Int{\Xn}{}\|T(\p_0, X)\|_{\infty, 2} \dd\mxn(X)$, which verifies $C_1(\p_0) \leq M(u_0) \mx(\X)^n$. Since $\X$ is compact and $\mx$ is a Radon probability measure by \ref{ass:mx_my}, $\mx(\X)$ is well-defined and finite, thus $C_1(u_0)$ is finite. Likewise, let $C_2 := \Int{\Yn}{}\|Y\|_{\infty, 2}\dd \myn(Y) <+\infty$.}
	
	Finally, $|F(\p) - F(\p')| \leq 2\lipT\npoints(\varepsilon \lipT + C_1(\p_0) + C_2)\|\p-\p'\|_2$.
\end{proof}

Having shown that our losses are locally Lipschitz, we can now turn to convergence results. These conclusions are placed in the context of non-smooth and non-convex optimisation, thus will be tied to the Clarke sub-differential of $F$, which we denote $\partial_C F$. The set of Clarke sub-gradients at a point $\p$ is the convex hull of the limits of gradients of $F$:
\begin{equation}\label{eqn:clarke}
	\partial_C F(\p) := \mathrm{conv}\left\{v \in \R^\dimp:\; \exists (\p^{(t)}) \in (\mathcal{D}_F)^\N: \p^{(t)} \xrightarrow[t \longrightarrow +\infty]{} \p\ \mathrm{and} \; \nabla F(\p^{(t)}) \xrightarrow[t \longrightarrow +\infty]{} v\right\},
\end{equation}
where $\mathcal{D}_F$ is the set of differentiability of $F$. At points $\p$ where $F$ is differentiable, $\partial_CF(\p) = \{\nabla F(\p)\}$, and if $F$ is convex in a neighbourhood of $\p$, then the Clarke differential at $\p$ is the set of its convex sub-gradients. \redtwo{The interested reader may turn to \ref{sec:nonsmooth} for further context on non-smooth and non-convex optimisation.}

\section{Convergence of Interpolated SGD Trajectories on \texorpdfstring{$F$}{F}}\label{sec:interpolated_SGD}

In general, the idea behind SGD is a discretisation of the gradient flow equation $\dot{u}(s) = -\nabla F(u(s))$. In our non-smooth setting, the underlying continuous-time problem is instead the Clarke differential inclusion $\dot{u}(s) \in -\partial_C F(u(s))$. Our objective is to show that in a certain sense, the SGD trajectories approach the set of solutions of this inclusion problem, as the step size decreases. We consider solutions that are absolutely continuous (we will write $\p(\cdot) \in \mathcal{C}_{\mathrm{abs}}(\R_+, \R^\dimp)$) and start within $\mathcal{K} \subset \R^\dimp$, a fixed compact set. We can now define the solution set formally as
\begin{equation}\label{eqn:S}
	S_{-\partial_C F}(\mathcal{K}) := \left\lbrace \p \in \mathcal{C}_{\mathrm{abs}}(\R_+, \R^\dimp)\ |\  \ull{\forall} s\in \R_+,\; \dot{\p}(s) \in -\partial_C F(\p(s));\; \p(0) \in \mathcal{K} \right\rbrace,
\end{equation}
where we write $\ull{\forall}$ for "almost every". In order to compare the discrete SGD trajectories to this set of continuous-time trajectories, we interpolate the discrete points in an affine manner: Equation \ref{eqn:interpolation} defines the \textit{piecewise-affine interpolated SGD trajectory} associated to a discrete SGD trajectory $(\p_\lr^{(t)})_{t \in \N}$ of learning rate $\lr$.
\begin{equation}\label{eqn:interpolation}
	\p_\lr(s) = \p^{(t)}_\lr + \left(\frac{s}{\lr} - t\right)(\p_\lr^{(t+1)} - \p^{(t)}_\lr),\quad \forall s \in [t\lr, (t+1)\lr[, \quad \forall t \in \N.
\end{equation}
In order to compare our interpolated trajectories with the solutions, we consider the metric of uniform convergence on all segments
\begin{equation}\label{eqn:d_c}
	d_c(\p, \p') := \Sum{k \in \N^*}{}\cfrac{1}{2^k}\min\left(1, \underset{s \in [0, k]}{\max}\|\p(s) - \p'(s)\|_{2}\right).
\end{equation}
In order to prove a convergence result on the interpolated trajectories, we will leverage the work of \citet{bianchi2022convergence} which hinges on three conditions on the loss $F$ that we reproduce and verify successively. \redtwo{Firstly, \ref{cond:A1} assumes mild regularity on the sample loss function $f$.}

\begin{condition}\label{cond:A1}\
	\begin{itemize}		
		\item[i)] There exists $\kappa: \R^\dimp \times \Data \longrightarrow \R_+$ measurable such that each $\kappa(\p, \cdot)$ is $\mdata$-integrable, and:		
		$$\exists \varepsilon > 0,\; \forall \p, \p' \in B(\p_0, \varepsilon),\; \forall \data \in \Data,\; |f(\p, \data) - f(\p', \data)|\leq \kappa(\p_0, \data)\|\p-\p'\|_2. $$		
		\item[ii)] There exists $\p \in \R^\dimp$ such that $f(\p, \cdot)$ is $\mdata$-integrable.
	\end{itemize}
\end{condition}

Our regularity result on $f$ \ref{prop:f_loclip} allows us to verify \ref{cond:A1}, by letting $\varepsilon := 1$ and $\kappa(\p_0, \data) := K_f(1, \p_0, X, Y)$. \ref{cond:A1} ii) is immediate since for \textit{all} $\p \in \R^\dimp,\; (X, Y, \theta) \longmapsto w_\theta(T(\p, X), Y)$ is continuous in each variable separately, thanks to the regularity of $T$ provided by \ref{ass:T_loclip}, and to the regularities of $w$. This continuity implies that all $f(\p, \cdot)$ are $\mdata$-integrable, since $\mdata = \mxn \otimes \myn \otimes \bbsigma$ is a compactly supported probability measure under \ref{ass:mx_my}. \redtwo{Secondly, \ref{cond:A2} concerns the local Lipschitz constant $\kappa$ introduced in \ref{cond:A1}: it is assumed to increase slowly with respect to the network parameters $\p$.}

\begin{condition}\label{cond:A2} The function $\kappa$ of \ref{cond:A1} verifies:	
	\begin{itemize}
		\item[i)] There exists $c \geq 0$ such that $\forall \p\in \R^\dimp,\; \Int{\Data}{}\kappa(\p, \data)\dd\mdata(\data) \leq c(1+\|\p\|_2)$.		
		\item[ii)] For every compact $\mathcal{K} \subset \R^\dimp,\; \underset{\p \in \mathcal{K}}{\sup}\ \Int{\Data}{}\kappa(\p, \data)^2\dd\mdata(\data) <+\infty$.
	\end{itemize}	
\end{condition}

\ref{cond:A2}.ii) is verified by $\kappa$ given its regularity. However, \ref{cond:A2}.i) requires that $T(\p, x)$ increase slowly as $\|\p\|_2$ increases, which is more costly.

\begin{assumption}\label{ass:T_slow_increase}
	There exists an $\mx$-integrable function $g: \R^\dimx \longrightarrow\R_+$ such that $\forall \p \in \R^\dimp,\; \forall x \in \R^\dimx,\; \|T(\p, x)\|_2 \leq g(x)(1 + \|\p\|_2)$.
\end{assumption}

\ref{ass:T_slow_increase} is satisfied in particular as soon as $T(\cdot, x)$ is bounded (which is the case for a neural network with bounded activation functions), or if $T$ is of the form $T(\p, x) = \widetilde{T}(\p, x) \mathbbold{1}_{B(0, R)}(\p)$, i.e. limiting the network parameters $\p$ to be bounded. This second case does not yield substantial restrictions in practice \redtwo{(see \ref{sec:suitable_NNs} for a class of NNs that satisfy all of the assumptions)}, yet vastly simplifies theory.  Under \ref{ass:T_slow_increase}, we have for any $\p \in \R^\dimp,$ with $\kappa(\p, \data) = K_f(1, \p, X, Y)$ from \ref{prop:f_loclip} and $C_2$ from \ref{prop:F_loclip},
\begin{align*}\Int{\Xn \times \Yn \times \SS^{\dimy-1}}{}K_f(1, \p, X, Y)\dd\mxn(X) \dd\myn(Y) \dd\bbsigma(\theta) &\leq 4\lipT\npoints\left(\varepsilon \lipT + (1+\|\p\|_2)\Int{\Xn}{}\underset{k \in \llbracket 1, \npoints \rrbracket}{\max}\ g(x_k) \dd\mxn(X) + C_2\right)\\
	&\leq c(1 + \|\p\|_2).
\end{align*}
As a consequence, \ref{cond:A2} holds under our assumptions. We now consider the Markov kernel associated to the SGD schemes:
$$P_\lr : \app{\R^\dimp \times \mathcal{B}(\R^\dimp)}{[0, 1]}{\p, B}{\Int{\Data}{}\mathbbold{1}_B(\p - \lr \varphi(\p, \data))\dd \mdata(\data)}.$$
\redtwo{Given $\p \in \R^{\dimp},\; P_\lr(\p, \cdot)$ is a probability measure on $\R^{\dimp}$ which dictates the law of the positions of the next SGD iteration $\p^{(t+1)}$, conditionally to $\p^{(t)} = \p$.} With $\bblambda_{\R^\dimp}$ denoting the Lebesgue measure on $\R^\dimp$, let $\Gamma := \left\lbrace \lr \in\ ]0, +\infty[\ |\ \forall \mp \ll \bblambda_{\R^\dimp},\ \mp P_\lr \ll \bblambda_{\R^\dimp}\right\rbrace$. $\Gamma$ is the set of learning rates $\alpha$ for which the kernel $P_\alpha$ maps any absolutely continuous probability measure $\mp$ to another such measure. We will verify the following condition, \redtwo{which can be interpreted as the SGD trajectories continuing to explore the entire space for a small enough learning rate $\lr$}:
\begin{condition}\label{cond:A3} The closure of $\Gamma$ contains 0.
\end{condition}

In order to satisfy \ref{cond:A3}, we require an additional regularity condition on the neural network $T$ which we formulate in \ref{ass:d2T2_and_dT_bounded}.

\begin{assumption}\label{ass:d2T2_and_dT_bounded}
	There exists a constant $\MddT>0$, such that  (with the notations of \ref{ass:C2_ae} and \ref{ass:mx_my}) $\forall x \in \X,\; \forall j \in J(x),\; \forall \p \in \U_j(x),\; \forall (i_1, i_2, i_3, i_4) \in \llbracket 1, \dimp\rrbracket^2\times \llbracket 1, \dimy \rrbracket^2,$	
	$$ \left|\cfrac{\partial^2}{\partial \p_{i_1} \partial \p_{i_2}} \Big([T(\p, x)]_{i_3} [T(\p, x)]_{i_4}\Big)\right| \leq \MddT,\; \mathrm{and}\;\left\|\cfrac{\partial^2 T}{\partial \p_{i_1}\partial\p_{i_2}}(\p, x)\right\|_2 \leq \MddT.$$
\end{assumption}

The upper bounds in \ref{ass:d2T2_and_dT_bounded} bear strong consequences on the behaviour of $T$ for $\|u\|_2 \gg 1$, and are only practical for networks of the form $T(\p, x) = \widetilde{T}(\p, x) \mathbbold{1}_{B(0,R)}(\p, x)$, similarly to \ref{ass:T_slow_increase}. \redtwo{We detail the technicalities of verifying this assumption along with the others in the Appendix (\ref{sec:suitable_NNs}).}

\begin{prop}\label{prop:SW_Gamma} Under \ref{ass:C2_ae}, \ref{ass:mx_my} and \ref{ass:d2T2_and_dT_bounded}, for the SGD trajectories \ref{eqn:SW_SGD}, $\Gamma$ contains $]0, \lr_0[$, where $\lr_0 := \left((\dimy^2 + 2R_y )\dimp\MddT\right)^{-1}$.
\end{prop}

We postpone the proof to \ref{sec:proof_SW_gamma}. Now that we have verified \ref{cond:A1}, \ref{cond:A2} and \ref{cond:A3}, we can apply \citep{bianchi2022convergence}, Theorem 2 to $F$, showing a convergence result on interpolated SGD trajectories.

\begin{theorem}\label{thm:SGD_interpolated_cv} Consider a neural network $T$ and measures $\mx$, $\my$ satisfying \ref{ass:C2_ae}, \ref{ass:T_loclip}, \ref{ass:mx_my}, \ref{ass:T_slow_increase} and \ref{ass:d2T2_and_dT_bounded}. Let $\lr_1 < \lr_0$ (see \ref{prop:SW_Gamma}).
	
	Let $(\p_\lr^{(t)}), \lr \in ]0, \lr_1], t \in \N$ a collection of SGD trajectories associated to~\ref{eqn:SW_SGD}. Consider $(\p_\lr)$ their associated interpolations. For any compact $\mathcal{K} \subset \R^\dimp$ and any $\eta > 0$, we have:	
	\begin{equation}
		\underset{\substack{\lr \longrightarrow 0 \\ \lr \in\ ]0, \lr_1]}}{\lim}\ \mpzero \otimes \mxN \otimes \myN \otimes \bbsigma^{\otimes \N} \left( d_c(\p_\lr, S_{-\partial_C F}(\mathcal{K})) > \eta\right) = 0.
	\end{equation}	
\end{theorem}
\vspace{-10pt}
The distance $d_c$ is defined in \ref{eqn:d_c}. As the learning rate decreases, the interpolated trajectories approach the trajectory set $S_{-\partial_C F}$, which is essentially a solution of the \textit{gradient flow equation}  $\dot{u}(s) = -\nabla F(u(s))$ (ignoring the set of non-differentiability, which is $\bblambda_{\R^\dimp}$-null). To get a tangible idea of the concepts at play, if $F$ was $\mathcal{C}^2$ and had a finite amount of critical points, then one would have the convergence of a solution $u(s)$ to a critical point of $F$, as $s \longrightarrow+\infty$. These results have implicit consequences on the value of the parameters  at the "end" of training for low learning rates, which is why we will consider a variant of SGD for which we can say more precise results on the convergence of the parameters.

\section{Convergence of Noised Projected SGD Schemes on \texorpdfstring{$F$}{F}}\label{sec:noised_proj_sgd}

In practice, it is seldom desirable for the parameters of a neural network to reach extremely large values during training. Weight clipping is a common (although contentious) method of enforcing that $T(\p, \cdot)$ stay Lipschitz, which is desirable for theoretical reasons. For instance the 1-Wasserstein duality in Wasserstein GANs \citep{pmlr-v70-arjovsky17a} requires Lipschitz networks, and similarly, Sliced-Wasserstein GANs \citep{deshpande_generative_sw} use weight clipping and enforce their networks to be Lipschitz.

Given a radius $r > 0$, we consider SGD schemes that are restricted to $\p \in \oll{B}(0, r) =: B_r$, by performing \textit{projected} SGD. At each step $t$, we also add a noise $\noise \varepsilon^{(t+1)}$, where $\varepsilon^{(t+1)}$ is an additive noise of law $\mnoise \ll \bblambda_{\R^\p}$, which is often taken as standard Gaussian in practice. These additions yield the following SGD scheme:
\begin{equation}\label{eqn:SW_SGD_projected_noised}
	\begin{split}
		\p^{(t+1)} = \pi_r\left(\p^{(t)} - \lr \varphi(\p^{(t)}, X^{(t+1)}, Y^{(t+1)}, \theta^{(t+1)}) + \lr\noise \varepsilon^{(t+1)}\right), \\ \left(\p^{(0)}, (X^{(t)})_{t \in \N}\ (Y^{(t)})_{t \in \N},\ (\theta^{(t)})_{t \in \N},\ (\varepsilon^{(t)})_{t \in \N}\right) \sim \mpzero \otimes \mxN \otimes \myN \otimes \bbsigma^{\otimes \N} \otimes \mnoise^{\otimes\N},
	\end{split}
\end{equation}
where $\pi_r: \R^\p \longrightarrow B_r$ denotes the orthogonal projection on the ball $B_r := \oll{B}(0, r)$. Thanks to \ref{cond:A1}, \ref{cond:A2} and the additional noise, we can verify the assumptions for \citep{bianchi2022convergence} Theorem 4, yielding the same result as \ref{thm:SGD_interpolated_cv} for the noised projected scheme \ref{eqn:SW_SGD_projected_noised}. In fact, under additional assumptions, we shall prove a stronger mode of convergence for the aforementioned trajectories. The natural context in which to perform gradient descent is on functions that admit a chain rule, which is formalised in the case of almost-everywhere differentiability by the notion of \textit{path differentiability}, as studied thoroughly in  \citep{bolte2021conservative}. \redtwo{We also provide a brief presentation in the Appendix (\ref{sec:conservative_fields}).} %

\begin{condition}\label{cond:A5}$F$ is path differentiable, which is to say that for any $\p \in \mathcal{C}_{\mathrm{abs}}(\R_+, \R^\dimp)$, for almost all $s > 0,\; \forall v \in \partial_CF(\p(s)),\; v^\top  \dot \p(s) = (F \circ \p)'(s)$.
\end{condition} 

\begin{remark} \red{There are alternate equivalent formulations for \ref{cond:A5}. Indeed, as presented in further detail in \ref{sec:conservative_fields}, $F$ is path differentiable if and only if $\partial_C F$ is a conservative field for $F$ if and only if $F$ has a chain rule for $\partial_C$ (the latter is the formulation chosen above in \ref{cond:A5}).}
\end{remark}

In order to satisfy \ref{cond:A5}, we need to make the assumption that the NN input measure $\mx$ and the data measure $\my$ are discrete measures, which is the case for $\my$ in the case of generative neural networks, but is less realistic for $\mx$ in practice. We define $\Sigma_n$ the $n$-simplex: its elements are the $a \in \R^n$ s.t. $\forall i \in \llbracket 1, n \rrbracket,\; a_i \geq 0$ and $\sum_i a_i =1$.

\begin{assumption}\label{ass:mx_my_discrete}
	One may write $\mx = \Sum{k=1}{\npoints_x}a_k \bbdelta_{x_k}$ and $\my = \Sum{k=1}{\npoints_y}b_k \bbdelta_{y_k}$, with the coefficient vectors $a\in \Sigma_{\npoints_x}, b\in \Sigma_{\npoints_y}$, $\X = \lbrace x_1, \cdots, x_{\npoints_x}\rbrace \subset \R^{\dimx}$ and $\Y = \lbrace y_1, \cdots, y_{\npoints_y}\rbrace \subset \R^{\dimy}$.
\end{assumption}  

There is little practical reason to consider non-uniform measures, however the generalisation to any discrete measure makes no theoretical difference. Note that \ref{ass:mx_my} is clearly implied by \ref{ass:mx_my_discrete}. 

In order to show that $F$ is path differentiable, we require the natural assumption that each $T(\cdot, x)$ be path differentiable. Since $T(\cdot, x)$ is a vector-valued function, we need to extend the notion of path-differentiability. Thankfully, \citet{bolte2021conservative} define \textit{conservative mappings} for vector-valued locally Lipschitz functions (Definition 4), which allows us to define naturally path differentiability of a vector-valued function as the path-differentiability of all of its coordinate functions. \red{See \ref{sec:conservative_mappings} for a detailed presentation.}

\begin{assumption}\label{ass:T_path_diff}
	For any $x \in \R^\dimx,\; T(\cdot, x)$ is path differentiable.
\end{assumption}

\red{\ref{ass:T_path_diff} holds as soon as each the neural network has the typical structure of compositions of linear units and typical activations, as was proved by \citet{davis2020stochastic}, Corollary 5.11 and \citet{bolte2021conservative}, Section 6.2.} \redtwo{We provide a more specific class of NNs that are path differentiable and satisfy all our other assumptions in \ref{sec:suitable_NNs}}.

\begin{prop}\label{prop:F_path_diff}
	Under \ref{ass:T_loclip}, \ref{ass:mx_my_discrete} and \ref{ass:T_path_diff}, $F$ is path differentiable.
\end{prop}

\begin{proof}
	We shall use repeatedly the property that the composition of path differentiable functions remains path differentiable, which is proved in \citep{bolte2021conservative}, Lemma 6. 
	
	Let $\SWY: \app{\R^{\npoints \times \dimy} \times \R^{\npoints \times \dimy}}{\R_+}{Y, Y'}{\SW_2^2(\bbgamma_Y, \bbgamma_{Y'})}$. By \citep{discrete_sliced_loss}, Proposition 2.4.3, each $\SWY(\cdot, Y)$ is semi-concave and thus is path differentiable (by \citep{discrete_sliced_loss}, Proposition 4.3.3).
	
	Thanks to \ref{ass:mx_my_discrete}, $\mxn$ and $\myn$ are discrete measures on $\R^{\npoints \times \dimx}$ and $\R^{\npoints \times \dimy}$ respectively, allowing one to write $\mxn = \sum_k a_k\bbdelta_{X_k}$ and $\myn = \sum_l b_l\bbdelta_{Y_l}$. Then $F = \p \longmapsto \sum_{k,l}a_kb_l\SWY(T(\p, X_k), Y_l)$ is path differentiable as a sum (\citep{bolte2021conservative}, Corollary 4) of compositions (\citep{bolte2021conservative}, Lemma 6) of path differentiable functions.
\end{proof}

We have now satisfied all the assumptions to apply \citep{bianchi2022convergence}, Theorem 6, showing that trajectories of \ref{eqn:SW_SGD_projected_noised} converge towards to a set of generalised critical points\footnote{\red{Typically referred to as the set of \textit{Karush-Kahn-Tucker} points of the differential inclusion $\dot u(s) \in -\partial_C F(u(s)) - \mathcal{N}_r(u(s))$.}} $\mathcal{C}_r$ defined as
\begin{equation}\label{eqn:KKT_points}
	\mathcal{C}_r := \left\lbrace \p \in \R^\dimp\ |\ 0 \in -\partial_CF(\p) - \mathcal{N}_r(\p) \right\rbrace, \quad \mathcal{N}_r(\p) = \left\lbrace\begin{array}{c}
		\lbrace 0 \rbrace\ \mathrm{if}\ \|\p\|_2 < r \\
		\lbrace s \p\ |\ s \geq 0 \rbrace\ \mathrm{if}\ \|\p\|_2 = r \\
		\varnothing\ \mathrm{if}\ \|\p\|_2 > r
	\end{array} \right. ,
\end{equation}
where $\mathcal{N}_r(\p)$ refers to the \textit{normal cone} of the ball $\oll{B}(0, r)$ at $x$. The term $\mathcal{N}_r(\p)$ in \ref{eqn:KKT_points} only makes a difference in the pathological case $\|\p\|_2 = r$, which never happens in practice since the idea behind projecting is to do so on a very large ball, in order to avoid gradient explosion, to limit the Lipschitz constant and to satisfy theoretical assumptions. Omitting the $\mathcal{N}_r(\p)$ term, and denoting $\mathcal{D}$ the points where $F$ is differentiable, \ref{eqn:KKT_points} simplifies to $\mathcal{C}_r \cap \mathcal{D} = \lbrace\p \in \mathcal{D}\ |\ \nabla F(\p) = 0\rbrace$, i.e. the critical points of $F$ for the usual differential. Like in \ref{thm:SGD_interpolated_cv}, we let $\lr_1 < \lr_0$, where $\lr_0$ is defined in \ref{prop:SW_Gamma}. We have met the conditions to apply \cite{bianchi2022convergence}, Theorem 6, showing a long-run convergence results on the SGD trajectories \ref{eqn:SW_SGD_projected_noised}.

\begin{theorem}\label{thm:SGD_projected_noised}
	
	Consider a neural network $T$ and measures $\mx$, $\my$ satisfying \ref{ass:C2_ae}, \ref{ass:T_loclip}, \ref{ass:T_slow_increase}, \ref{ass:d2T2_and_dT_bounded}, \ref{ass:mx_my_discrete} and \ref{ass:T_path_diff}. Let $(\p_{\lr}^{(t)})_{t \in \N}$ be SGD trajectories defined by \ref{eqn:SW_SGD_projected_noised} for $r > 0$ and $\lr \in ]0, \lr_1]$. One has	
	$$\forall \eta > 0,\; \underset{t \longrightarrow +\infty}{\oll{\lim}}\ \mpzero \otimes \mxN\otimes \myN \otimes \bbsigma^{\otimes \N} \otimes \mnoise^{\otimes \N}\left(d(\p_{\lr}^{(t)}, \mathcal{C}_r) > \eta\right) \xrightarrow[\substack{\lr \longrightarrow 0\\ \lr \in \left] 0, \lr_1\right]}]{} 0.$$
\end{theorem}
\vspace{-10pt}
The distance $d$ above is the usual euclidean distance. \ref{thm:SGD_projected_noised} shows essentially that as the learning rate approaches 0, the long-run limits of the SGD trajectories approach the set of $\mathcal{C}_r$ in probability. Omitting the points of non-differentiability and the pathological case $\|u\|_2=r$, the general idea is that $u_\alpha^{(\infty)} \xrightarrow[\lr \longrightarrow 0]{} \lbrace u\ :\ \nabla F(u)=0 \rbrace$, which is the convergence that would be achieved by the gradient flow of $F$, in the simpler case of $\mathcal{C}^2$ smoothness.

