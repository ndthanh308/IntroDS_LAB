\section{Introduction}
\subsection{Optimal Transport in Machine Learning}
Optimal Transport (OT) allows the comparison of measures on a metric space by generalising the use of the ground metric. Typical applications use the so-called 2-Wasserstein distance, defined as
\begin{equation}\eqlabel{W2}
	\forall \mx, \my \in \mathcal{P}_2(\R^d),\; \W_2^2(\mx, \my) := \underset{\bbpi \in \Pi(\mx, \my)}{\inf}\Int{\R^d\times\R^d}{}\|x-y\|_2^2\dd \bbpi(x, y),
\end{equation}
where $\mathcal{P}_2(\R^d)$ is the set of probability measures on $\R^d$ admitting a second-order moment and where $\Pi(\mx, \my)$ is the set of measures of $\mathcal{P}_2(\R^d \times \R^d)$ of first marginal $\mx$ and second marginal $\my$. One may find a thorough presentation of its properties in classical monographs such as \citet{computational_ot, santambrogio2015optimal, villani}

The ability to compare probability measures is useful in probability density fitting problems, which are a sub-genre of generation tasks. In this formalism, one considers a probability measure parametrised by a vector $\p$ which is designed to approach a target data distribution $\my$ (typically the real-world dataset). In order to determine suitable parameters, one may choose any probability discrepancy (Kullback-Leibler, Ciszar divergences, f-divergences or Maximum Mean Discrepancy \citep{gretton2006kernel}), or in our case, the Wasserstein distance. In the case of Generative Adversarial Networks, the optimisation problem which trains the "Wasserstein GAN" \citep{pmlr-v70-arjovsky17a} stems from the Kantorovitch-Rubinstein dual expression of the 1-Wasserstein distance.
\subsection{The Sliced Wasserstein Distance as an Alternative}
\redtwo{The Wasserstein distance suffers from the curse of dimensionality, in the sense that the sample complexity for $n$ samples in dimension $d$ is of the order $\mathcal{O}(n^{1/d})$ \citep{dudley1969speed}. Due to this practical limitation and to the computational cost of the Wasserstein distance, the study of cheaper alternatives has become a prominent field of research. A prominent example is Entropic OT introduced by \citet{cuturi2013sinkhorn}, which adds an entropic regularisation term, advantageously making the problem strongly convex. Sample complexity bounds have been derived by \citet{genevay2019sample}, showing a convergence in $\mathcal{O}(\sqrt{n})$ with a constant depending on the regularisation factor.}

Another alternative is the Sliced Wasserstein (SW) Distance introduced by \citet{Rabin_texture_mixing_sw}, which consists in computing the 1D Wasserstein distances between projections of input measures, and averaging over the projections. The aforementioned projection of a measure $\mx$ on $\R^d$ is done by the \textit{push-forward} operation by the map $P_\theta: x \longmapsto \theta^\top  x$. Formally, $P_\theta\#\mx$ is the measure on $\R$ such that for any Borel set $B\subset \R$, $P_\theta\#\mx(B) = \mx(P_\theta^{-1}(B))$. Once the measures are projected onto a line $\R\theta$, the computation of the Wasserstein distance becomes substantially simpler numerically. We illustrate this fact in the discrete case, which arises in practical optimisation settings. Let two discrete measures on $\R^d$: $\bbgamma_X := \frac{1}{\npoints}\sum_k\bbdelta_{x_k},\; \bbgamma_Y := \frac{1}{\npoints}\sum_k\bbdelta_{y_k}$ with supports $X = (x_1, \cdots, x_\npoints)$ and $Y=(y_1, \cdots, y_\npoints) \in \R^{\npoints\times d}$. Their push-forwards by $P_\theta$ are simply computed by the formula $P_\theta \#\bbgamma_X = \frac{1}{n}\sum_k \bbdelta_{P_\theta(x_k)}$, and the 2-Wasserstein distance between their projections can be computed by sorting their supports: let $\sigma$ a permutation sorting $(\theta^\top x_1, \cdots, \theta^\top x_\npoints)$, and $\tau$ a permutation sorting $(\theta^\top y_1, \cdots, \theta^\top y_\npoints)$, one has the simple expression
\begin{equation}
	\W_2^2(P_\theta\#\bbgamma_X, P_\theta\#\bbgamma_Y) = \cfrac{1}{\npoints}\Sum{k=1}{\npoints}(\theta^\top x_{\sigma(k)} - \theta^\top y_{\tau(k)})^2.
\end{equation}
The SW distance is the expectation of this quantity with respect to $\theta\sim \bbsigma$, i.e. uniform on the sphere: $\SW_2^2(\bbgamma_X, \bbgamma_Y) = \mathbb{E}_{\theta \sim \bbsigma}\left[\W_2^2(P_\theta\#\bbgamma_X, P_\theta\#\bbgamma_Y)\right]$. The 2-SW distance is also defined more generally between two measures $\mx, \my \in \mathcal{P}_2(\R^d)$:
\begin{equation}\eqlabel{SW}
	\mathrm{SW}_2^2(\mx, \my) := \Int{\theta \in \SS^{d-1}}{}\W_2^2(P_\theta\#\mx, P_\theta\#\my)\dd \bbsigma(\theta).
\end{equation}
\redtwo{In addition to its computational accessibility, the SW distance enjoys a dimension-free sample complexity \citep{nadjahi_statistical_properties_sliced}. Additional statistical, computational and robustness properties of SW have been explored by \citet{nietert2022statistical}. Moreover, central-limit results have been shown by \citet{xu2022central} for 1-SW and the 1-max-SW distance (a variant of SW introduced by \citet{deshpande2019max}), and related work by \citet{xi2022distributional} shows the convergence of the sliced error process $\theta \longmapsto \sqrt{n}\left(\W_p^p(P_\theta\#\bbgamma_X, P_\theta\#\bbgamma_Y) - \W_p^p(P_\theta\#\mx, P_\theta\#\my)\right)$, where the samples $X\sim\mxn$ and $Y\sim\myn$ are drawn for each $\theta$. Another salient field of research for SW is its metric properties, and while it has been shown to be weaker than the Wasserstein distance in general by \citet{bonnotte}, and metric comparisons with Wasserstein and max-SW have been undergone by \citet{bayraktar2021strong} and \citet{paty2019subspace}.}
%
\redtwo{\subsection{Related Works}}
%
Our subject of interest is the theoretical properties of SW as a loss for implicit generative modelling, which leads to minimising $\mathrm{SW}_2^2(T_\p\#\mx, \my)$ in the parameters $\p$, where $\my$ is the target distribution, and $T_\p\#\mx$ is the image by the NN\footnote{Similarly to the 1D case, $T_\p\#\mx$ is the push-forward measure of $\mx$ by $T_\p$, i.e. the law of $T_\p(x)$ when $x\sim \mx$.} of $\mx$, a low-dimensional input distribution (often chosen as Gaussian or uniform noise). In order to train a NN in this manner, at each iteration one draws $\npoints$ samples from $\mx$ and $\my$ (denoted $\bbgamma_X$ and $\bbgamma_Y$ as discrete measures with $\npoints$ points), as well as a projection $\theta$ (or a batch of projections) and performs an SGD step on the sample loss
\begin{equation}\label{eqn:Loss}
	\mathcal{L}(\p) = \mathrm{SW}_2^2(P_\theta\#T_\p\#\bbgamma_X, P_\theta\#\bbgamma_Y) = \cfrac{1}{\npoints}\Sum{k=1}{\npoints}(\theta^\top T_\p(x_{\sigma(k)}) - \theta^\top y_{\tau(k)})^2.
\end{equation}
Taking the expectation of this loss over the samples yields the minibatch Sliced-Wasserstein discrepancy, a member of the minibatch variants of the OT distances, introduced formally by Fatras et al. \cite{fatras2021minibatch}. The framework \ref{eqn:Loss} fits several Machine Learning applications, for instance, \citet{deshpande_generative_sw} trains GANs and auto-encoders with this method, and \citet{Wu2019_SWAE} consider related dual formulations. Other examples within this formalism include the synthesis of images by minimising the SW distance between features of the optimised image and a target image, as done by \citet{heitz2021sliced} for textures with neural features, and by \citet{Tartavel2016} with wavelet features (amongst other methods).

The general study of convergence of SGD in the context of non-smooth, non-convex functions (as is the case of $\mathcal{L}$ from \ref{eqn:Loss}) is an active field of research: \citet{majewski2018analysis} and \citet{davis2020stochastic} show the convergence of diminishing-step SGD under regularity constraints, while \citet{bolte2021conservative} leverage conservative field theory to show convergence results for training with back-propagation. Finally, the recent work by \citet{bianchi2022convergence} shows the convergence of fixed-step SGD schemes on a general function $F$ under weaker regularity assumptions.

More specifically, the study of convergence for OT-based generative NNs has been tackled by \citet{fatras2021minibatch}, who prove strong convergence results for minibatch variants of classical OT distances, namely the Wasserstein distance, the Entropic OT and the Gromov Wasserstein distance (another OT variant introduced by \citet{memoli2011gromov}). A related study on GANs by \citet{huang2023characterizing} derive optimisation properties for one layer and one dimensional Wasserstein-GANs and generalise to higher dimensions by turning to SW-GANs. Another work by \citet{brechet2023critical} focuses on the theoretical properties of linear NNs trained with the Bures-Wasserstein loss (introduced by \citet{bures1969extension}; see also \citep{Bhatia_Bures_metric} for reference on this metric). Finally, the regularity and optimisation properties of the simpler energy $\SW_2^2(\bbgamma_X, \bbgamma_Y)$ have been studied by \citet{discrete_sliced_loss}.

In practice, it has been observed that SGD in such settings always converges (in the loose numerical sense, see \citep{deshpande_generative_sw}, Section 5, or \citep{heitz2021sliced}, Figure 3), yet this property is not known theoretically. The aim of this work is to bridge the gap between theory and practical observation by proving convergence results for SGD on (minibatch) Sliced Wasserstein generative losses of the form $F(\p) = \mathbb{E}_{X\sim \mxn, Y\sim \myn}\SW_2^2(T_u\#\bbgamma_X, \bbgamma_Y)$.
%
\subsection{Contributions}
\paragraph{Convergence of Interpolated SGD Under Practical Assumptions} Under practically realistic assumptions, we prove in \ref{thm:SGD_interpolated_cv} that piecewise affine interpolations (defined in Equation \ref{eqn:interpolation}) of constant-step SGD schemes on $\p\longmapsto F(\p)$ (formalised in Equation \ref{eqn:SW_SGD}) converge towards the set of sub-gradient flow solutions (see Equation \ref{eqn:S}) as the gradient step decreases. This results signifies that with very small learning rates, SGD trajectories will be close to sub-gradient flows, which themselves converge to critical points of $F$ (omitting serious technicalities). 

The assumptions for this result are practically reasonable: the input measure $\mx$ and the true data measure $\my$ are assumed to be compactly supported. As for the network $(\p, x) \longmapsto T(\p, x)$, we assume that for a fixed datum $x$, $T(\cdot, x)$ is piecewise $\mathcal{C}^2$-smooth and that it is Lipschitz jointly in both variables. 
%
\redtwo{We require additional assumptions on $T$ which are more costly, but are verified as long as $T$ is a NN composed of typical activations and linear units, with the constraint that the parameters $\p$ and data $x$ stay both stay within a fixed bounded domains. We discuss a class of neural networks that satisfy all of the assumptions of the paper in the Appendix (\ref{sec:suitable_NNs}). Furthermore, this result can be extended to other orders $p\neq 2$ of SW: we present the tools for this generalisation in \ref{sec:other_p}.}

\paragraph{Stronger Convergence Under Stricter Assumptions} In order to obtain a stronger convergence result, we consider a variant of SGD where each iteration receives an additive noise (scaled by the learning rate) which allows for better space exploration, and where each iteration is projected on a ball $B(0, r)$ in order to ensure boundedness. This alternative SGD scheme remains within the realm of practical applications, and we show in \ref{thm:SGD_projected_noised} that long-run limits of such trajectories converge towards a set of generalised critical points of $F$, as the gradient step approaches 0. This result is substantially stronger, and can serve as an explanation of the convergence of practical SGD trajectories, specifically towards a set of critical points which amounts to the stationary points of the energy (barring theoretical technicalities).

Unfortunately, we require additional assumptions in order to obtain this stronger convergence result, the most important of which is that the input data measure $\mx$ and the dataset measure $\my$ are discrete. For the latter, this is always the case in practice, however the former assumption is more problematic, since it is common to envision generative NNs as taking an argument from a continuous space (the input is often Gaussian of Uniform noise), thus a discrete setting is a substantial theoretical drawback. For practical concerns, one may argue that the discrete $\mx$ can have an arbitrary fixed amount of points, and leverage strong sample complexity results to ascertain that the discretisation is not costly if the number of samples is large enough.
