\section{Conclusion and Outlook}

Under reasonable assumptions, we have shown that SGD trajectories of parameters of generative NNs with a minibatch SW loss converge towards the desired sub-gradient flow solutions, implying in a weak sense the convergence of said trajectories. Under stronger assumptions, we have shown that trajectories of a mildly modified SGD scheme converge towards a set of generalised critical points of the loss, which provides a missing convergence result for such optimisation problems.

The core limitation of this theoretical work is the assumption that the input data measure $\mx$ is discrete (\ref{ass:mx_my_discrete}), which we required in order to prove that the loss $F$ is path differentiable. In order to generalise to a non-discrete measure, one would need to apply or show a result on the stability of path differentiability through integration: in our case, we want to show that $\int_{\Xn} \SWY(T(\p, X), Y)\dd\mxn(X)$ is path differentiable, knowing that $\p \longmapsto \SWY(T(\p, X), Y)$ is path differentiable by composition (see the proof of \ref{prop:F_path_diff} for the justification). Unfortunately, in general if each $g(\cdot, x)$ is path differentiable, it is not always the case that $\int g(\cdot, x)\dd x$ is path differentiable (at the very least, there is no theorem stating this, even in the simpler case of another sub-class of path differentiable functions, see \citep{bianchi2022convergence}, Section 6.1). However, there is such a theorem (specifically \citep{clarke1990optimization}, Theorem 2.7.2 with Remark 2.3.5) for \textit{Clarke regular} functions \red{(see \ref{sec:Clarke_regular} for a presentation of this regularity class)}, sadly the composition of Clarke regular functions is not always Clarke regular, it is only known to be the case in excessively restrictive cases (see \citep{clarke1990optimization}, Theorems 2.3.9 and 2.3.10). \redtwo{Similarly to the continuous case, the simpler generalisation in which $\mx$ has a countable support adds substantial difficulty, since all of the typical tools (path differentiability itself, Clarke regularity or even definability (see \citep{bolte2021conservative} Section 4.1 for a first introduction) do not have readily applicable results for infinite operations, to our knowledge. As a result, we leave the generalisation to a non-discrete input measure $\mx$ for future work.}

\redtwo{Our studies focus on the 2-SW distance, but our results from \ref{sec:interpolated_SGD} can be extended to $p\in [1, +\infty[$, as presented in the appendix (\ref{sec:other_p}). However, as also discussed in the Appendix, the generalisation of \ref{sec:noised_proj_sgd} is still an open problem, since it has not yet be proven that $X \longmapsto \SW_p^p(\bbgamma_X, \bbgamma_Y)$ is path differentiable for $p\neq 2$.}

\redtwo{This paper studies the use of the \textit{average} SW distance as a loss, and an extension to related distances would be worth considering. The average SW distance aggregates the projected distances through an expectation, while the closely-related \textit{max}-Sliced Wasserstein distance introduced by \citet{deshpande2019max} aggregates the projections via a maximisation on the axis $\theta \in \SS^{d-1}$. The training paradigm presented in \citep{deshpande2019max} differs strongly from our formalism since it applies to GANs, however one could consider an extension of our formalism in which the optimal projection $\theta$ becomes a learned parameter of the neural network. A related extension is the Subspace-Robust Wasserstein distance \citep{paty2019subspace}, which can take the following formulation
%
$$\mathcal{S}^2_k(\mx, \my) =\underset{\substack{0 \preceq \Omega \preceq I_d \\ \mathrm{trace}(\Omega) = k}}{\max}\ \W_2^2(\Omega^{1/2}\#\mx, \Omega^{1/2}\#\my),$$
for which one could consider a similar extension where the positive semi-definite $\Omega$ becomes a learned parameter of $T$.} 

Another avenue for future study would be to tie the flow approximation result from \ref{thm:SGD_interpolated_cv} to Sliced Wasserstein Flows \citep{liutkus19a_SWflow_generation, bonet2022efficient}. The difficulty in seeing the differential inclusion \ref{eqn:S} as a flow of $F$ lies in the non-differentiable nature of the functions at play, as well as the presence of the composition between SW and the neural network $T$, which bodes poorly with Clarke sub-differentials. 