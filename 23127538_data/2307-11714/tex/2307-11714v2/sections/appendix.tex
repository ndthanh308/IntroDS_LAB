\redtwo{\section{Table of Notations}\label{sec:notations}}

\begin{table}[H]
	\centering
	\caption{List of Notations}
	\begin{tabular}{|c|c|}
		\hline
		\textbf{Symbol} & \textbf{Explanation} \\
		\hline
		$\bbgamma_X$ & Given $X = (x_1, \cdots, x_n) \in \R^{n \times d},\; \bbgamma_X = \frac{1}{n}\sum_i\bbdelta_{x_i}$\\
		\hline
		$X$ & $(x_1, \cdots, x_n) \in \R^{\npoints \times \dimx}$ an input data sample of law $\mxn$ \\
		\hline
		$\mx$ & input data probability measure on $\R^\dimx$, supported on $\X$ \\
		\hline
		$Y$ & $(y_1, \cdots, y_n) \in \R^{\npoints \times \dimy}$ a target data sample of law $\myn$ \\
		\hline
		$\my$ & target data probability measure on $\R^\dimy$, supported on $\Y$  \\
		\hline
		$\theta$ & direction in $\SS^{\dimy -1}$\\
		\hline
		$\bbsigma$ & uniform measure on $\SS^{\dimy-1}$\\
		\hline
		$\data := (X, Y, \theta)$ & sample in $X, Y$ and $\theta$\\
		\hline
		$\mdata := \mxn \otimes \myn \otimes \bbsigma$ & probability measure for the samples $\data$, supported on $\Data := \Xn \times \Yn \times \SS^{\dimy - 1}$ \\
		\hline 
		$\p$ & neural network parameters in $\R^\dimp$ \\
		\hline
		$T(\p, X)$ & neural network function defined in \ref{eqn:T} \\
		\hline 
		$f(\p, X, Y, \theta)$ & sample loss function defined in \ref{eqn:f} \\
		\hline
		$F(\p)$ & population loss function defined in \ref{eqn:F} \\
		\hline
		$w_\theta(Y, Y')$ & discrete and projected 2-Wasserstein distance $\W_2^2(P_\theta\#\bbgamma_Y, P_\theta\#\bbgamma_{Y'})$ \\
		\hline
		$\varphi(\p, X, Y, \theta)$ & almost-everywhere gradient of $f(\cdot, X, Y, \theta)$ defined in \ref{eqn:ae_grad}\\
		\hline
		$K_w, K_f, K_F$ & local Lipschitz constants of $w, f, F$ respectively (see Propositions 1, 2, 3)\\
		\hline
		$\lr; \noise$ & SGD learning rate; noise level \\
		\hline
		$\bblambda_{\R^d}; \rho \ll \bblambda_{\R^d}$ & Lebesgue measure on $\R^d$; a measure $\rho$ absolutely continuous w.r.t. $\bblambda_{\R^d}$\\
		\hline
		$\partial_C$ & Clarke differential, defined in \ref{eqn:clarke}\\
		\hline
		$\mpzero$ & probability measure of SGD initialisation $\p^{(0)}$\\
		\hline
		$\varepsilon^{(t)}$ & additive noise in $\R^{\dimp}$ at SGD step $t$\\
		\hline
		$\mnoise$ & additive noise probability measure on $\R^{\dimp}$\\
		\hline
		$B_{\|\cdot\|}(x, R), \oll{B}_{\|\cdot\|}(x, R)$ & open (resp. closed) ball of centre $x$ and radius $R$ for the norm $\|\cdot\|$ \\
		\hline
	\end{tabular}
	\label{tab:notations}
\end{table}

\redtwo{\section{Postponed Proofs}\label{sec:proof_SW_gamma}}

\paragraph{Proof of \ref{prop:SW_Gamma}} 

\begin{proof}
	Let $\mp \ll \bblambda$ and $B \in \mathcal{B}(\R^\dimp)$ such that $\bblambda(B) = 0$. We have, with $\lr' := 2\lr/\npoints,\;\data := (X, Y, \theta),\; \mdata := \mxn \otimes \myn \otimes \bbsigma$ and $\Data := \Xn \times \Yn \times \SS^{\dimy-1},$	
	\begin{align*}\mp P_\lr(B) &= \Int{\R^\dimp\times \Data}{}\mathbbold{1}_B\left[\p -\lr'\Sum{k=1}{\npoints}\left(\dr{\p}{}{T}(\p, x_k)\right)^\top  \theta \theta^\top (T(\p, x_k) - y_{\sigma_\theta^{T(\p, X), Y}(k)})\right]\dd\mp(\p)\dd\mdata(\data) \\
		&\leq \Sum{\tau \in \mathfrak{S}_\npoints}{}\Int{\Data}{}I_\tau(\data)\dd\mdata(\data),
	\end{align*}	
	where $I_\tau(\data) := \Int{\R^\dimp}{}\mathbbold{1}_B\left(\phi_{\tau,\data}(\p)\right)\dd\mp(\p)$, with $\phi_{\tau,\data} := \p - \lr'\underbrace{\Sum{k=1}{\npoints}\left(\dr{\p}{}{T}(\p, x_k)\right)^\top  \theta \theta^\top (T(\p, x_k) - y_{\tau(k)})}_{\psi_{\tau,\data} := }$.
	
	Let $\tau \in \mathfrak{S}_\npoints$ and $(X, Y, \theta) \in \Data$. Using \ref{ass:C2_ae}, separate $I_\tau(\data) = \Sum{j\in J}{}\Int{\U_j(X)}{}\mathbbold{1}_B\left(\p - \psi_{\tau,\data}(\p)\right)\dd\mp(\p),$ where the differentiability structure $(\U_j(X))_{j \in J(X)}$ is obtained using the respective differentiability structures: for each $k \in \llbracket 1, \npoints \rrbracket$, \ref{ass:C2_ae} yields a structure $(\U_{j_k}(x_k))_{j_k \in J_k(x_k)}$ of $\p \longmapsto T(\p, x_k)$, which depends on $x_k$, hence the $k$ indices.
	
	To be precise, define for $j = (j_1, \cdots, j_\npoints) \in J_1(x_1) \times \cdots \times J_\npoints(x_\npoints),\; \U_j(X) := \Inter{k=1}{\npoints}\U_{j_k}(x_k)$, and $J(X) := \left\lbrace (j_1, \cdots, j_\npoints) \in J_1(x_1) \times \cdots \times J_\npoints(x_\npoints)\ |\ \U_j(X) \neq \varnothing \right\rbrace$. In particular, for any $k \in \llbracket 1, \npoints \rrbracket,\; T(\cdot, x_k)$ is $\mathcal{C}^2$ on $\U_j(X)$. Notice that the derivatives are not necessarily defined on the border $\partial \U_j(X)$, which is of Lebesgue measure 0 by \ref{ass:C2_ae}, thus the values of the derivatives on the border do not change the value of the integrals (the integrals may have the value $+\infty$, depending on the behaviour of $\phi_{\tau, s}$, but we shall see that they are all finite when $\lr$ is small enough).
	
	We drop the $\data, \tau$ index in the notation, and focus on the properties of $\phi$ and $\psi$ as functions of $\p$. Our first objective is to determine a constant $K>0$, independent of $\p, \data, \tau$, such that $\psi$ is $K$-Lipschitz on $\U_j(X)$. 
	
	First, let $\chi := \app{\U_j(X)}{\R^\dimp}{\p}{\left(\dr{\p}{}{T}(\p, x_k)\right)^\top  \theta \theta^\top  T(\p, x_k)}$. The function $\chi$ is of class $\mathcal{C}^1$, therefore we determine its Lipschitz constant by upper-bounding the $\|\cdot\|_2$-induced operator norm of its differential, denoted by $\nt{\cfrac{\partial\chi}{\partial\p}(\p)}_2$. Notice that $\chi(\p) = \cfrac{1}{2}\cfrac{\partial}{\partial \p}\left(\theta^\top  T(\p, x_k)\right)^2$.
	
	Now $\nt{\cfrac{\partial^2}{\partial\p^2}\left(\theta^\top  T(\p, x_k)\right)^2}_2 \leq \dimp\underset{(i_1, i_2) \in \llbracket 1, \dimp \rrbracket^2}{\max}\ \left|\cfrac{\partial^2}{\partial\p_{i_1}\partial\p_{i_2}}\left(\theta^\top  T(\p, x_k)\right)^2\right|$, using \ref{ass:d2T2_and_dT_bounded} and $|\theta_i| \leq 1$,
	$$\left|\cfrac{\partial^2}{\partial\p_{i_1}\partial\p_{i_2}}\left(\theta^\top  T(\p, x_k)\right)^2\right| \leq \Sum{(i_3, i_4) \in \llbracket 1, \dimy \rrbracket^2}{}\left|\theta_{i_3}\theta_{i_4}\cfrac{\partial^2 }{\partial\p_{i_1}\partial\p_{i_2}}\Big([T(\p, x_k)]_{i_3}[T(\p, x_k)]_{i_4}\Big)\right| \leq \dimy^2 \MddT.$$	
	We obtain that $\chi$ is $\frac{1}{2}\dimp\dimy^2\MddT$-Lipschitz. 
	
	Second, let $\omega:  \p \in \U_j(X) \longmapsto \left(\dr{\p}{}{T}(\p, x_k)\right)^\top  \theta \theta^\top y_{\tau(k)}$, also of class $\mathcal{C}^1$. We re-write $\left[\cfrac{\partial \omega}{\partial\p}(\p)\right]_{i_1, i_2} = y_{\tau(k)}^\top \theta\theta^\top \cfrac{\partial^2 T}{\partial \p_{i_1}\partial\p_{i_2}}(\p, x_k)$, and conclude similarly by \ref{ass:d2T2_and_dT_bounded} that $\omega$ is $\|y_{\tau(k)}\|_2\dimp \MddT$-Lipschitz.
	
	Finally, $\psi = \Sum{k=1}{\npoints}(\chi_k - \omega_k)$, and is therefore $K := (\frac{1}{2}\dimy^2 + R_y )\dimp\npoints\MddT$-Lipschitz, with $R_y$ from \ref{ass:mx_my}. We have proven that $\nt{\cfrac{\partial \psi}{\partial \p}(\p)}_2 \leq K$ for any $\p \in \U_j(X)$, and that $K$ does not depend on $X, Y, \theta, j$ or $\p$.
	
	We now suppose that $\lr' < \frac{1}{K}$, which is to say $\lr < \frac{\npoints}{2K}$. Under this condition, $\phi: \U_j(X) \longrightarrow \R^\dimp$ is injective. Indeed, if $\phi(\p_1) = \phi(\p_2)$, then $\|\p_1-\p_2\|_2 = \lr'\|\psi(\p_1)-\psi(\p_2)\|_2 \leq \lr'K\|\p_1-\p_2\|_2$, thus $\p_1 = \p_2$. Furthermore, for any $\p \in \U_j(X),\; \cfrac{\partial \phi}{\partial \p}(\p) = \mathrm{Id}_{\R^\dimp} - \lr'\cfrac{\partial \psi}{\partial \p}(\p)$, with $\nt{\lr'\cfrac{\partial \psi}{\partial \p}(\p)}_2 < 1$, thus the matrix $\cfrac{\partial \phi}{\partial \p}(\p)$ is invertible (using the Neumann series method). By the global inverse function theorem, $\phi: \U_j(X) \longrightarrow \phi(\U_j(X))$ is a $\mathcal{C}^1$-diffeomorphism.
	
	\red{Using the change-of-variables formula, we have $\Int{\U_j(X)}{}\mathbbold{1}_B(\phi(\p))\dd\mp(\p) = \Int{\U_j(X)}{}\mathbbold{1}_B(\p')\dd\phi\#\mp(\p') = \phi\#\mp(B)$, we have now shown that $\phi$ is a $\mathcal{C}^1$-diffeomorphism, thus since $\mp \ll \bblambda$, $\phi\#\mp \ll \bblambda$. ($\alpha \ll\beta$ denoting that $\alpha$ is absolutely continuous with respect to $\beta$). Since $\bblambda(B) = 0$, it follows that the integral is 0, then by sum over $j$, $I_\tau(\data) = 0$ and finally $\mp P_\lr(B) = 0$ by integration over $\data$ and sum over $\tau$.}
\end{proof}

\section{Background on Non-Smooth and Non-Convex Analysis}\label{sec:nonsmooth}

This work is placed within the context of non-smooth optimisation, a field of study in part introduced by Clarke with the so-called Clarke differential, which we introduced in Equation \ref{eqn:clarke} (see \citep{clarke1990optimization} for a general reference on this object). The purpose of this appendix is to present several adjacent objects that can be useful to the application of our results, even though we do not need them in order to prove our theorems. 

\subsection{Conservative Fields}\label{sec:conservative_fields}

The Clarke differential $\partial_C$ of a locally Lipschitz function $g: \R^d \longrightarrow \R$ (defined in Equation \ref{eqn:clarke}) is an example of a \textit{set-valued map}. Such a map is a function $D: \R^p \rightrightarrows \R^q$ from the subsets of $\R^p$ to the subsets of $\R^q$, for instance in the case of the Clarke differential, we have the signature $\partial_C g: \R^d \rightrightarrows \R^d$. A set-valued map $D$ is \textit{graph closed} if its graph $\lbrace (\p, v)\ |\ \p \in \R^p,\; v\in D(\p) \rbrace$ is a closed set of $\R^{p+q}$. A set-valued map $D$ is said to be a \textit{conservative field}, when it is graph closed, has non-empty compact values and for any absolutely continuous loop $\bbdelta \in \mathcal{C}_{\mathrm{abs}}([0, 1], \R^p)$ with $\gamma(0) = \gamma(1)$, we have 
$$\Int{0}{1}\underset{v \in D(\gamma(s))}{\max}\ \langle \dot \gamma(s), v \rangle \dd s = 0. $$
Similarly to primitive functions in calculus, one may define a function $g: \R^d \longrightarrow \R$ using a conservative field $D: \R^d\rightrightarrows \R^d$ up to an additive constant through following expression:
\begin{equation}\label{eqn:potential}
	g(\p) = g(0) + \Int{0}{1}\underset{v \in D(\gamma(s))}{\max}\ \langle \dot \gamma(s), v \rangle \dd s, \quad \forall \gamma \in \mathcal{C}_{\mathrm{abs}}([0, 1], \R^p)\ \text{such\ that}\ \gamma(0) = 1\ \text{and}\ \gamma(1)=\p.
\end{equation}
In this case, we say that $g$ is a \textit{potential function} for the field $D$. This notion allows us to define a new regularity class: a function $g: \R^d \longrightarrow \R$ is called \textit{path differentiable} when there exists a conservative field of which it is a potential. A standard result in non-smooth optimisation is the following equivalence between different notions of regularity:

\begin{prop}\label{prop:equiv_path_regular}\citet{bolte2021conservative}, Corollary 2. Let $g: \R^d \longrightarrow \R$ locally Lipschitz. We have the equivalence between the following statements:
	\begin{itemize}
		\item $g$ is path differentiable
		\item $\partial_C g$ is a conservative field
		\item $g$ has a \textit{chain rule} for the Clarke differential $\partial_C$:
		\begin{equation}\label{eqn:chaine_rule}
			\forall \p \in \mathcal{C}_{\mathrm{abs}}(\R_+, \R^d), \; \ull{\forall} s > 0,\; \forall v \in \partial_Cg(\p(s)),\; v^\top \dot \p(s) = (g \circ \p)'(s).
		\end{equation}
	\end{itemize}
\end{prop}
This equivalence justifies the terminology used in \ref{cond:A5}. The reader seeking a complete presentation of conservative field theory may refer to \citep{bolte2021conservative}.

\subsection{Conservative Mappings}\label{sec:conservative_mappings}

The notion of conservative fields for real-valued locally Lipschitz functions $g: \R^d \longrightarrow \R$ can be generalised to \textit{conservative mappings} for vector-valued locally Lipschitz functions $g: \R^p \longrightarrow \R^q$, which one may see as a generalised Jacobian matrix (see \citep{bolte2021conservative}, Section 3.3 for further details). A set-valued map $J: \R^p \rightrightarrows \R^{q\times p}$ is a conservative mapping for such a $g$ if
\begin{equation}\label{eqn:conservative_mappings}
	\forall \p \in \mathcal{C}_{\mathrm{abs}}(\R_+, \R^p),\; \ull{\forall} s > 0,\; (g\circ \p)'(s) = M\dot \p(t),\; \forall M \in  J(\p(s)).
\end{equation}
In this case, we shall say that $g$ is path differentiable. Note that if each coordinate function $g_i$ is the potential of a conservative field $D_i$, then the set-valued map 
$$J(\p) = \left\lbrace \left(\begin{array}{c}
	v_1^\top\\
	\vdots\\
	v_q^\top\\
\end{array}\right)\ :\ \forall i \in \llbracket 1, q \rrbracket,\; v_i \in D_i(\p) \right\rbrace$$ 
is a conservative mapping for $g$ (although not all conservative mappings for $g$ can be written in this manner). As a consequence, one could interpret (simplistically) vector-valued path differentiability as coordinate-wise path differentiability.

\subsection{Clarke Regularity}\label{sec:Clarke_regular}

Another notion of regularity for locally Lipschitz functions is that of \textit{Clarke regularity}. Let $g: \R^p \longrightarrow \R^q$ and $\p \in \R^p$, $g$ is said to be \textit{Clarke regular} at $\p$ if the two quantities
$$g^\circ(\p; v) := \underset{\substack{\p' \rightarrow \p \\ t \searrow 0}}{\operatorname{limsup}} \cfrac{g(\p'+tv) - g(\p')}{t} \quad \text{and} \quad g'(\p;v) := \underset{t \searrow 0}{\operatorname{lim}}\cfrac{g(\p+tv)-g(\p)}{t}$$
exist and are equal for all $v \in \R^p$. Note that this notion implies path differentiability by \citep{bolte2021conservative}, Proposition 2. Clarke regularity is the central concept of Clarke's monograph \citep{clarke1990optimization}.

\redtwo{\subsection{Semi-Algebraic Functions}\label{sec:semi_algebraic}}

In non-smooth analysis, one of the simplest regularity cases is the class of \textit{semi-algebraic} functions, which are essentially piecewise polynomial functions defined on polynomial pieces. To be precise, a set $\mathcal{A} \subset \R^d$ is \textit{semi-algebraic} if it can be written under the form
$$\mathcal{A} = \Reu{i=1}{n}\Inter{j=1}{m}\left\lbrace \p \in \R^d\ |\ P_{i,j}(\p) < 0,\; Q_{i,j}(\p) = 0 \right\rbrace,$$
where the $P_{i,j}$ and $Q_{i,j}$ are real multivariate polynomials. A function $g: \R^p \longrightarrow \R^q$ is \textit{semi-algebraic} if its graph $\mathcal{G} := \lbrace (\p, g(\p))\ |\ \p \in \R^p \rbrace$ is semi-algebraic.

A locally Lipschitz real-valued semi-algebraic function is path differentiable (see for instance \citep{bolte2021conservative}, Proposition 2), and in the light of \citep{bolte2021conservative}, Lemma 3, this is also the case in the vector-valued case. Another useful property of semi-algebraic functions is that their class is stable by composition and product. The interested reader may consult \citep{Wakabayashi_semialgebraic} for additional properties of semi-algebraic objects, or \citep{coste1999introduction, van1996geometric}, for a presentation of o-minimal structures, a generalisation of this concept.

\redtwo{\section{Suitable Neural Networks}\label{sec:suitable_NNs}}

In this section, we detail our claim that typical NN structures satisfy our conditions. To this end, we define a class of practical neural networks whose properties are sufficient (not all NNs that satisfy our assumptions are within this framework). Consider $\classT$ the set of NNs $T$ of the form
$$T: \app{\R^{\dimp}\times \R^{\dimx}}{\R^\dimy}{(\p,x)}{\widetilde{T}(\p, x)\mathbbold{1}^\varepsilon_{B(0, R_\p)}(\p)\mathbbold{1}^\varepsilon_{B(0, R_x)}(x)},$$
with $R_\p, R_x > 0$ and $\varepsilon > 0$. The function $\mathbbold{1}^\varepsilon_{B(0, R)}$ is a smoothed version of the usual indicator function $\mathbbold{1}_{B(0, R)}$: it is any function that has value 1 in $B(0, R - \varepsilon)$, 0 outside $B(0, R + \varepsilon)$ and is $\mathcal{C}^2$-smooth (see \ref{rem:smooth_indicator} for a possible construction). Given that one may take arbitrarily large radii, these indicators are added for theoretical purposes and impose no realistic constraints in practice. Additionally, $\widetilde{T} = h_N$, the $\nlayers$-th layer of a recursive NN structure defined by
$$\layerout_0(\p, x) = x, \quad \forall n \in \llbracket 1, \nlayers \rrbracket,\; \layerout_n = \app{\R^{\dimp}\times \R^{\dimx}}{\R^{d_n}}{(\p,x)}{\activ_n\left(\Sum{i=0}{n-1}\linunit_{n,i}(\p)\layerout_i(\p,x) + \intercept_n \p \right)},$$
where:
\begin{itemize}
	%
	\item \textit{All} functions $\activ_n: \R \longrightarrow \R$ are $\mathcal{C}^2$-smooth, or \textit{all} locally Lipschitz semi-algebraic activation functions (applied entry-wise). The former condition is satisfied by the common sigmoid, hyperbolic tangent or softplus activations. The latter condition applies to the non-differentiable ReLU activation, its "Leaky ReLU" extension, and continuous piecewise polynomial activations. Note that other non-linearities such as softmax can also be considered under the same regularity restrictions, but we limit ourselves to entry-wise non-linearities for notational consistency.
	\item Each dimension $d_n$ is a positive integer, with obviously $d_{\nlayers} = \dimy$, the output dimension.
	\item Each $\linunit_{n, i}$ is a linear map: $\R^\dimp \longrightarrow \R^{d_n \times d_i}$, which maps a parameter vector $\p$ to a $d_n \times d_i$ matrix. Since the entire parameter vector $\p$ is given at each layer, this allows the architecture to only use certain parameters at each layer (as is more typical in practice). One may see this map as a 3-tensor of shape $(d_n, d_i, \dimp)$, as specified in the formulation
	\begin{equation}\label{eqn:A_tensor_form}
		\forall \p \in \R^\dimp,\; \forall \layerout \in \R^{d_i},\; \linunit_{n, i}(u)\layerout = \left(\Sum{j_2=1}{d_i}\Sum{j_3=1}{\dimp}\linunit_{j_1, j_2, j_3}^{(n, i)}\layerout_{j_2}\p_{j_3}\right)_{j_1 \in \llbracket 1, d_n \rrbracket} \in \R^{d_n}.
	\end{equation}
	\item The matrix $\intercept_n \in \R^{d_n \times \dimp}$ determines the intercept from the full parameter vector $\p$.
\end{itemize}

In this model, each layer depends on all the previous layers, allowing for residual inputs for instance. Overall, all typical networks fit this description, once bounded using the indicator functions, with only a technicality on the regularities of the activations which need to be \textit{all} $\mathcal{C}^2$-smooth, or \textit{all} semi-algebraic. One could extend this class of NNs to those with \textit{definable} activations within the same o-minimal structure (similarly to \citet{davis2020stochastic} and \citet{bolte2021conservative}).%

\begin{remark}\label{rem:smooth_indicator} We mention that we may construct a $\mathcal{C}^\infty$-smooth $\mathbbold{1}^\varepsilon_{B(0, R)}$ in $\R^d$ explicitly as follows:
	$$f(s):= \left\lbrace\begin{array}{c}
		e^{-1/s}\text{\ if\ }s>0 \\
		\text{else\ } 0
	\end{array} \right.,\quad g(s):=\cfrac{f(s)}{f(s)+f(1-s)},\quad \mathbbold{1}^\varepsilon_{B(0, R)} := \app{\R^d}{[0, 1]}{\p}{g\left(\cfrac{(R + \varepsilon)^2 - \|\p\|_2^2}{4R\varepsilon}\right)}. $$
\end{remark}

Before proving the properties of NNs from the class $\classT$, we require a technical result on path differentiable functions.

\begin{prop}\label{prop:prod_path_diff} Let $f: \R^d\longrightarrow\R$ path differentiable, and $g:\R^d\longrightarrow\R$ of class $\mathcal{C}^1$. Then their product $fg$ is path differentiable. 
\end{prop}
\begin{proof}
	Our objective is to apply \citep{bolte2021conservative} Corollary 2 (stated in \ref{prop:equiv_path_regular}), which is to say that $h := fg$ admits a chain rule for $\partial_C h$). First, we apply the definition of the Clarke differential and compute
	$$\forall \p \in \R^d,\;\partial_C f(\p) = f(\p)\nabla g(\p) + g(\p)\partial_Cf(\p) := \left\lbrace f(\p)\nabla g(\p) + g(\p)v\ |\ v \in \partial_Cf(\p)\right\rbrace.$$
	Note that we used the smoothness of $g$. We now consider an absolutely continuous curve $\p \in \mathcal{C}_{\mathrm{abs}}(\R_+, \R^d)$. By \cite{bolte2021conservative} Lemma 2, since $f$ is path differentiable, $f\circ \p$ is differentiable almost everywhere. Let $D$ the associated set of differentiability, then let $s\in D$ and $v \in \partial_C h(\p(s))$, writing $v = f(\p(s))\nabla g(\p(s)) + g(\p(s))w$ with $w \in \partial_Cf(\p(s))$. We compute $(h\circ\p)'(s) = (f\circ \p)'(s)g(\p(s)) + f(\p(s))(g\circ \p)'(s)$. Now since $f$ is path differentiable and $w \in \partial_Cf(\p(s))$, by \ref{prop:equiv_path_regular} item 3, we have $(f\circ \p)'(s) = \langle w, \dot \p(s)\rangle$. On the other hand, $(g\circ \p)'(s) = \langle \nabla g(\p(s)), \dot \p(s)\rangle$ since $g$ is $\mathcal{C}^1$. Finally by definition of $v$ and bilinearity of $\langle \cdot, \cdot \rangle$,
	$$(h\circ\p)'(s) = \langle w, \dot \p(s)\rangle g(\p(s)) + f(\p(s))\langle \nabla g(\p(s)), \dot \p(s)\rangle = \langle v, \dot \p(s)\rangle.$$
\end{proof}
We now have all the tools to prove that the class of NNs $\classT$ satisfies all of the assumptions of our paper.

\begin{prop} All networks of the class $\classT$ verify \ref{ass:C2_ae}, \ref{ass:T_loclip}, \ref{ass:T_slow_increase}, \ref{ass:d2T2_and_dT_bounded} and \ref{ass:T_path_diff}.
\end{prop}
\begin{proof}
%
%
%
%
%
%
%
	Let $T \in \classT$, and $\widetilde{T}$ its associated underlying network. We begin with regularity considerations. 
	
	\paragraph{Verifying Assumptions 1 and 7 in the $\mathcal{C}^2$ Case} In the case where the activations are $\mathcal{C}^2$-smooth, then each $\widetilde{T}(\cdot, x)$ is also of class $\mathcal{C}^2$. Furthermore, the smooth indicator $\mathbbold{1}^\varepsilon_{B(0, R_\p)}$ is $\mathcal{C}^\infty$-smooth, thus we can conclude that $T(\cdot, x)$ is $\mathcal{C}^2$-smooth, and thus satisfies \ref{ass:C2_ae} trivially. In particular, $T(\cdot, x)$ is path differentiable for any $x \in \R^\dimx$, thus $T$ also satisfies \ref{ass:T_path_diff}.
	
	\paragraph{Verifying Assumptions 1 and 7 in the Semi-Algebraic Case} In the case where the activations are locally Lipschitz and semi-algebraic, it follows that each $\widetilde{T}(\cdot, x)$ is semi-algebraic, which yields naturally a differentiability structure associated to the polynomial pieces, satisfying \ref{ass:C2_ae}. Furthermore, this regularity yields path differentiability by \citep{bolte2021conservative}, Proposition 2. By product with the smooth indicator function, $T$ is path differentiable by \ref{prop:prod_path_diff}, therefore it satisfies \ref{ass:T_path_diff} .
	
	\paragraph{Verifying Assumption 2 in the $\mathcal{C}^2$ Case} In the case where the activations are $\mathcal{C}^2$-smooth, it is clear that by composition and product $(\p, x) \longmapsto \widetilde{T}(\p, x)$ is \textit{jointly} $\mathcal{C}^2$-smooth. As a consequence, it is Lipschitz jointly in $(\p, x)$ on any compact of $\R^{\dimp}\times\R^{\dimy}$, and by product with the smooth indicators, so is $T$. Since $T$ is zero outside $\oll{B}(0, R_\p+\varepsilon)\times\oll{B}(0, R_x+\varepsilon)$, we conclude that it is globally Lipschitz in $(\p, x)$.
	
	\paragraph{Verifying Assumption 2 in the Semi-Algebraic Case} In the case of locally Lipschitz and semi-algebraic activations, we prove that $\widetilde{T}$ is jointly Lipschitz on any compact $\mathcal{K}$ by strong induction on $n \in \llbracket 1, \nlayers \rrbracket$. Let $\mathcal{K} = \mathcal{K}_1 \times \mathcal{K}_2$ a product compact of $\R^{\dimp}\times\R^{\dimy}$, and $P_n:$ "$\exists \lipT_n >0: \layerout_n$ is $\lipT_n$-Lipschitz on $\mathcal{K}$". The initialisation $P_0$ is trivial, since $z(\p, x) = x$. Let $n \in \llbracket 1, \nlayers \rrbracket$ and assume $P_i$ to hold true for $i \in \llbracket 0, n-1 \rrbracket$. In particular, the $\layerout_i$ are jointly continuous in $(\p, x)$, allowing the definition of 
	$$M := \underset{(\p, x) \in \mathcal{K}}{\max}\ \left|\Sum{i=0}{n-1}\linunit_{n,i}(\p)\layerout_i(\p,x) + \intercept_n \p\right|.$$
	Since $\activ_n$ is locally Lipschitz, a covering argument shows that there exists $\lipT_{\activ_n}>0$ such that $\activ_n$ is $\lipT_{\activ_n}$-Lipschitz on $[-M, M]$. Now let $(\p_1, \p_2) \in \mathcal{K}_1^2$ and $(x_1, x_2) \in \mathcal{K}_2^2$. We have
	\begin{align}\label{eqn:show_T_loclip1}
		\|\layerout_n(\p_1, x_1) - \layerout_n(\p_2, x_2)\|_2 &\leq \lipT_{\activ_n}\left\|\Sum{i=0}{n-1}\linunit_{n,i}(\p_1)\layerout_i(\p_1,x_1) + \intercept_n \p_1 - \Sum{i=0}{n-1}\linunit_{n,i}(\p_2)\layerout_i(\p_2,x_2) - \intercept_n \p_2\right\|_2 \nonumber\\
		&\leq \lipT_{\activ_n} \left(\opn{\intercept_n}\|\p_1 - \p_2\|_2 + \Sum{i=0}{n-1}\left\|\linunit_{n,i}(\p_1)\layerout_i(\p_1,x_1) - \linunit_{n,i}(\p_2)\layerout_i(\p_2,x_2)\right\|_2\right),
	\end{align}
	where $\opn{\cdot}$ denotes the $\|\cdot\|_2$-induced operator norm. Let $i \in \llbracket 0, n-1 \rrbracket$, we separate the norm:
	\begin{align}\label{eqn:show_T_loclip2}
		\left\|\linunit_{n,i}(\p_1)\layerout_i(\p_1,x_1) - \linunit_{n,i}(\p_2)\layerout_i(\p_2,x_2)\right\|_2 & \leq \left\|\linunit_{n,i}(\p_1)\layerout_i(\p_1,x_1) - \linunit_{n,i}(\p_2)\layerout_i(\p_1,x_1)\right\|_2 =: \Delta_1 \nonumber\\
		& + \left\|\linunit_{n,i}(\p_2)\layerout_i(\p_1,x_1) - \linunit_{n,i}(\p_2)\layerout_i(\p_2,x_2)\right\|_2 =: \Delta_2.
	\end{align}
	For $\Delta_1$, use the tensor form \ref{eqn:A_tensor_form} and the inequality $\|x\|_2 \leq \sqrt{d}\|x\|_\infty$ for $x\in \R^d$, then $\|\p\|_\infty \leq \|\p\|_2$:
	\begin{align}\label{eqn:show_T_loclip3}
		\Delta_1 &\leq \sqrt{d_n}\left\|\left(\Sum{j_2=1}{d_i}\Sum{j_3=1}{\dimp}\linunit_{j_1, j_2, j_3}^{(n, i)}\layerout_i(u_1,x_1)_{j_2}(\p_{j_3}^{(1)}-\p_{j_3}^{(2)})\right)_{j_1 \in \llbracket 1, d_n \rrbracket}\right\|_\infty \nonumber\\
		&\leq \sqrt{d_n}\underset{j_1,j_2,j_3}{\max}\ |A_{j_1,j_2,j_3}^{(n,i)}|\ \underset{(\p,x) \in \mathcal{K}_1\times\mathcal{K}_2}{\max}\ \|\layerout_i(\p,x)\|_\infty\ \|\p_1 - \p_2\|_\infty \nonumber\\
		&\leq \lipT_{\Delta_1}\|\p_1 - \p_2\|_2.
	\end{align}
	For $\Delta_2$, we leverage $P_i$ and obtain
	\begin{equation}\label{eqn:show_T_loclip4}
		\Delta_2 \leq \underset{\p \in \mathcal{K}_1}{\max}\ \opn{\linunit_i(\p)}\ \|\layerout_i(\p_1, x_1) - \layerout_i(\p_2, x_2)\|_2 \leq \underset{\p \in \mathcal{K}_1}{\max}\ \opn{\linunit_i(\p)}\ \lipT_i\left(\|\p_1 - \p_2\|_2 + \|x_1 - x_2\|_2\right).
	\end{equation}
	
	Combining \ref{eqn:show_T_loclip1} \ref{eqn:show_T_loclip2} \ref{eqn:show_T_loclip3} and \ref{eqn:show_T_loclip4} shows $P_n$ and concludes the induction, which in turn shows that $\widetilde{T}$ is jointly Lipschitz on any compact. Like in the smooth case, we conclude that $T$ is globally Lipschitz, and thus that \ref{ass:T_loclip} holds.
	
	\paragraph{Verifying Assumption 4} Under both cases of regularity for the activations, 
	$$g := x \longmapsto \underset{\p \in \oll{B}(0, R_\p + \varepsilon)}{\max}\ \|\widetilde{T}(\p, x)\|_2\mathbbold{1}^\varepsilon_{B(0, R_x)}(x)$$
	is measurable and bounded. Furthermore, observe that for $\p, x \in \R^\dimp \times \R^\dimx,\; \|T(\p,x)\|_2 \leq g(x)$. As a consequence, \ref{ass:T_slow_increase} holds. 
	
	\paragraph{Verifying Assumption 5 in the $\mathcal{C}^2$ case} If all activations are $\mathcal{C}^2$-smooth, both $\widetilde{T}$ and its coordinate-wise products $T_i\times T_j$ are $\mathcal{C}^2$-smooth jointly in $(\p, x)$. As a result, one may bound these terms on $(\p, x) \in \oll{B}(0, R_\p + \varepsilon) \times \oll{B}(0, R_x+\varepsilon)$ by a constant $M$, independent of $\p, x$, and the assumption is verified. 
	
	\paragraph{Verifying Assumption 5 in the semi-algebraic case} In the semi-algebraic case, there exists a structure $(\mathcal{U}_j)_{j \in J}$ of open sets of $\R^\dimp \times \R^\dimx$ whose closures cover the entire space, such that $\widetilde{T}$ is polynomial in $(\p, x)$ on each $\mathcal{U}_j$, with $J$ finite (this is possible since $\widetilde{T}$ is jointly semi-algebraic). The NN can be written $T(\p, x) = \widetilde{T}(\p, x)\mathbbold{1}^\varepsilon_{B(0, R_\p)}(\p)\mathbbold{1}^\varepsilon_{B(0, R_x)}(x)$, and is therefore $\mathcal{C}^2$-smooth on each $\mathcal{U}_j$. Furthermore, its restriction to $\mathcal{U}_j$ is extendable $\mathcal{C}^2$-smoothly to $\oll{\mathcal{U}_j}$ (we shall not introduce a different notation to these extensions, for legibility). As a result, one may introduce the following bounds on the derivatives of the coordinate functions on the intersection of the compact $\mathcal{K} := \oll{B}(0, R_\p+\varepsilon)\times\oll{B}(0, R_x+\varepsilon)$ and $\oll{\mathcal{U}_j}$: there exists an $M_j>0$ such that
	$$\forall (\p,x) \in \mathcal{K}\ \cap\ \oll{\mathcal{U}_j},\; \left|\cfrac{\partial^2}{\partial \p_{i_1} \partial \p_{i_2}} \Big([T(\p, x)]_{i_3} [T(\p, x)]_{i_4}\Big)\right| \leq M_j\; \mathrm{and}\;\left\|\cfrac{\partial^2 T}{\partial \p_{i_1}\partial\p_{i_2}}(\p, x)\right\|_2 \leq M_j.$$
	Since $J$ is finite and the $(\mathcal{U}_j)_{j\in J}$ cover $\mathcal{K}$, we deduce that this bound holds for $(\p,x) \in \mathcal{K}$ for a common constant $M>0$. Moreover, since $T$ is the zero function outside of $\mathcal{K}$, this bounds also holds for any $(\p, x) \in \R^\dimp\times\R^\dimx$. Finally, this shows that \ref{ass:d2T2_and_dT_bounded} holds.	
\end{proof}

\redtwo{\section{Generalisation to Other Sliced Wasserstein Orders}\label{sec:other_p}}

In this section, we shall discuss how some of our results can be extended by replacing the 2-SW term $\SW_2^2$ with $\SW_p^p$ for $p\in [1, +\infty[$.

\paragraph{Determining Lipschitz Constants} The first difficulty lies in showing that the functions $w_{\theta}^{(p)}:=(X, Y)\longmapsto\W_p^p(P_\theta\#\bbgamma_X, P_\theta\#\bbgamma_Y)$ still have a locally Lipschitz regularity similar to \ref{prop:w_unif_locLip} (this proposition is only shown for $p=2$ in \citep{discrete_sliced_loss}). We generalise their result in the following proposition.

\begin{prop}\label{prop:wp_unif_loc_lip} Let $K^{(p)}_w(r, X, Y) := 2p\npoints(r + \|X\|_{\infty, 2} + \|Y\|_{\infty, 2})^{p-1}$, for $X, Y \in \R^{\npoints \times \dimy}$ and $r>0$. Then $w^{(p)}_\theta(\cdot, Y)$ is $K^{(p)}_w(r, X, Y)$-Lipschitz in the neighbourhood $B_{\|\cdot\|_{\infty, 2}}(X, r)$:
	$$\forall Y', Y'' \in B_{\|\cdot\|_{\infty, 2}}(X, r),\; \forall \theta \in \SS^{\dimy-1},\; |w_\theta(Y', Y) - w_\theta(Y'', Y)| \leq K^{(p)}_w(r, X, Y) \|Y'-Y''\|_{\infty, 2}.$$
\end{prop}
\begin{proof}
	Let $X, Y \in \R^{\npoints \times \dimy}, r>0$ and $Y', Y'' \in B_{\|\cdot\|_{\infty, 2}}(X, r)$. By \citep{discrete_sliced_loss} Lemma 2.2.1, we have $|w_\theta^{(p)}(Y')-w_\theta^{(p)}(Y'')| \leq 2\|C'-C''\|_F$, where $\|\cdot\|_F$ denotes the Frobenius norm, and $C'$ is a $n\times n$ matrix of entries $C'_{k,l} = |\theta^\top y_k' - \theta^\top y_l|^p$, with similarly $C''_{k,l} = |\theta^\top y_k'' - \theta^\top y_l|^p$. Now consider the function
	$$g_{y_l}:= \app{\R^\dimy}{\R}{y}{|\theta^\top y - \theta^\top y_l|^p},$$
	which satisfies $C_{k,l}' = g_{y_l}(y_k')$, and is differentiable almost-everywhere, with $\nabla g_{y_l}(y) = p|\theta^\top y - \theta^\top y_l|^{p-1}\theta$. For almost every $y \in B(x_k, r)$, we have
	\begin{align*}
		\|\nabla g_{y_l}(y)\|_2 &\leq p\|y-y_l\|_2^{p-1} = p \|y-x_k + x_k -y_l\|_2^{p-1} \\ &\leq p \left(\|y-x_k\|_2 + \|x_k\|_2+ \|y_l\|_2\right)^{p-1}\leq p(r+\|X\|_{\infty, 2}+\|Y\|_{\infty, 2})^{p-1}.
	\end{align*}	
	As a result, $g_{y_l}$ is $p(r+\|X\|_{\infty, 2}+\|Y\|_{\infty, 2})^{p-1}$-Lipschitz in $B(x_k, r)$. Now since $Y', y''\in B_{\|\cdot\|_{\infty, 2}}(X, r)$, we have $y_k', y_k'' \in B(x_k, r)$, thus
	$$|[C']_{k,l} - [C'']_{k,l}| = |g_{y_l}(y_k') - g_{y_l}(y_k'')| \leq p(r+\|X\|_{\infty, 2}+\|Y\|_{\infty, 2})^{p-1} \|y_k' - y_k''\|_2.$$	
	Then $\|C'-C''\|_F  = \sqrt{\sum_{k,l}|[C']_{k,l} - [C'']_{k,l}|^2} \leq np(r+\|X\|_{\infty, 2}+\|Y\|_{\infty, 2})^{p-1} \|Y' - Y''\|_{\infty, 2}.$	
\end{proof}

Our results regarding the local Lipschitz property of $f$ and $F$ adapt immediately using the same method with the different constant $K^{(p)}_w(r, X, Y)$, we obtain the following constant for $f$ (with $\lipT$ from \ref{ass:T_loclip}):
$$K_f^{(p)}(\varepsilon, \p_0, X, Y) = 2pn\lipT\left(\varepsilon \lipT + \|T(\p_0, X)\|_{\infty, 2} + \|Y\|_{\infty, 2}\right)^{p-1},$$
then the following constant for $F$:
$$K_F^{(p)}(\varepsilon, \p_0) = 2pn\lipT\Int{\Xn\times\Yn}{}\left(\varepsilon \lipT + \|T(\p_0, X)\|_{\infty, 2} + \|Y\|_{\infty, 2}\right)^{p-1}\dd\mxn(X)\dd\myn(Y).$$
In order to satisfy \ref{cond:A2} item i) in the case $p\neq 2$, one needs to modify \ref{ass:T_slow_increase} to require $\|T(\p, x)\|_2 \leq g(x)^{1/(p-1)}(1+\|\p\|_2)^{1/(p-1)}$, which in realistic cases is not much more expensive than asking for $T$ to be bounded, which is a property of the class of NNs that we present in \ref{sec:suitable_NNs}. 

\paragraph{Almost-Everywhere Gradient} A second difficulty lies in defining an almost-everywhere gradient $f$, since in our main text we rely on the formulation of an almost-everywhere gradient of $w_\theta^{(2)}(\cdot, Y)$ which was derived only for $p=2$ by \citet{bonneel2015sliced} and \citet{discrete_sliced_loss}. In fact, for $\theta, Y$ fixed $w_\theta^{(p)}(X, Y)$ is piecewise smooth, like $w_\theta^{(2)}(\cdot, Y)$ is piecewise quadratic. As a result, one may show that the following is an almost-everywhere gradient of $w_\theta^{(p)}(\cdot, Y)$:
$$\dr{X}{}{w^{(p)}_\theta}(X, Y) = \left(\cfrac{p}{\npoints}\sign\left(\theta^\top x_k - \theta^\top y_{\sigma_\theta^{X, Y}(k)}\right)\left|\theta^\top x_k - \theta^\top y_{\sigma_\theta^{X, Y}(k)}\right|^{p-1}\theta\right)_{k \in \llbracket 1, \npoints \rrbracket} \in \R^{\npoints \times \dimy}.$$
The chain rule now yields the following almost-everywhere gradient for $f$:
$$\varphi (\p, X, Y, \theta) = \Sum{k=1}{\npoints} \cfrac{p}{\npoints}\sign\left(\theta^\top T(\p, x_k) - \theta^\top y_{\sigma_\theta^{T(\p, X), Y}(k)}\right) \left|\theta^\top T(\p, x_k) - \theta^\top y_{\sigma_\theta^{T(\p, X), Y}(k)}\right|^{p-1} \dr{\p}{}{T}(\p, x_k)\theta.$$
\paragraph{Adapting Proposition 4} Moving on to adapting \ref{prop:SW_Gamma}, the general case $p\neq 2$ makes things substantially more technical, but one may still show that the $\psi$ functions are Lipschitz using restrictions on $T$ its first and second-order derivatives (which can be formulated in a more technical version of \ref{ass:d2T2_and_dT_bounded}). In conclusion, \ref{prop:SW_Gamma} can be adapted to apply to $p\in [1, +\infty[$, and it follows that \ref{thm:SGD_interpolated_cv} also generalises to this case. 

\paragraph{Path Differentiability} Regarding the results from \ref{sec:noised_proj_sgd}, the only substantial difference lies in showing that $T(\cdot, x)$ is path differentiable. The only missing link in the composition chain is the path differentiability of $\SWY^{(p)}:= X \longmapsto \int_{\SS^{d-1}}w_\theta^{(p)}(X, Y)\dd\bbsigma(\theta)$. In the case $p=2$, the difficulty of the integral can be circumvented by noticing that $\SWY$ is semi-concave \citep{discrete_sliced_loss}, Proposition 2.4.3, which implies path differentiability. This argument does not generalise to $p\in [1, +\infty[$ naturally, hence our \ref{thm:SGD_projected_noised} only generalises to $p\in [1, +\infty[$ under the conjecture that $\SWY^{(p)}$ is indeed path differentiable.