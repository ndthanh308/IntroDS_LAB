\section{Appendix}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\subsection{Computing \texorpdfstring{$\SWY$}{E}, \texorpdfstring{$\W_2^2$}{W} and \texorpdfstring{$\SWpY$}{Ep} in a simple case}

\paragraph{Computing \texorpdfstring{$\SWY$}{E}}\label{sec:L2_E} We work in polar coordinates, writing $\theta = \left(\begin{array}{c}
	\cos \phi \\
	\sin \phi
\end{array}\right)$ and $y = \left(\begin{array}{c}
	u \\
	v
\end{array}\right) = r\left(\begin{array}{c}
	\cos \psi \\
	\sin \psi
\end{array}\right)$.

By symmetry of the problem, we can assume $\psi \in [0, \pi/2]$ (i.e. the top-right quadrant $u \geq 0, v \geq 0$).

Now let $\psi \in [0, 2\pi[$, let us compute $\W_2^2(P_\theta\#\gamma_Y, P_\theta\#\gamma_Z)$. Since we project in 1D, computing this slice amounts to sorting $(\theta \cdot y_1, \theta \cdot y_2)$ and $(\theta \cdot z_1, \theta \cdot z_2)$. Let $\sort{Z}{\theta} \in \mathfrak{S}_2$ such that $\theta \cdot y_{\sort{Y}{\theta}(1)} \leq \theta \cdot y_{\sort{Y}{\theta}(2)}$ and similarly $\theta \cdot z_{\sort{Z}{\theta}(1)} \leq \theta \cdot z_{\sort{Z}{\theta}(2)}$.

We always have $\W_2^2(P_\theta\#\gamma_Y, P_\theta\#\gamma_Z) = \cfrac{1}{2}\left(\left(\theta \cdot (y_{\sort{Y}{\theta}(1)} - z_{\sort{Z}{\theta}(1)})\right)^2 + \left(\theta \cdot (y_{\sort{Y}{\theta}(2)} - z_{\sort{Z}{\theta}(2)})\right)^2\right)$.

We split the integral depending on the values of $\sort{Y}{\theta}$ and $\sort{Z}{\theta}$, which vary depending on the angle of the projection $\phi$. We begin with $\sort{Y}{\theta}$:
\begin{equation}\label{eqn:ex_sort_Y}
	\theta \cdot y_1 \geq \theta \cdot y_2 \Longleftrightarrow \cos \phi \cos \psi + \sin\phi \sin \psi \geq 0 \Longleftrightarrow \phi \in [\psi - \pi/2, \psi + \psi/2] + 2\pi \Z.
\end{equation}
The equation for $\sort{Z}{\theta}$ is much simpler:
\begin{equation}\label{eqn:ex_sort_Z}
	\theta \cdot z_1 \geq \theta \cdot z_2 \Longleftrightarrow -\sin \phi \geq 0 \Longleftrightarrow \phi \in [\pi, 2\pi] + 2\pi \Z.
\end{equation}
We divide a period of $2\pi$ in four quadrants corresponding to the four possibilities for $(\sort{Y}{\theta}, \sort{Z}{\theta})$. Since we assume $\psi \in [0, \pi/2]$, we can write this simply as:
\begin{align*}
	\SWY(Y) &= \cfrac{1}{4\pi}\Int{-\pi}{\psi-\pi/2}\left(\left(\theta \cdot (y_1 - z_2)\right)^2 + \left(\theta \cdot (y_2 - z_1)\right)^2\right)\dd\phi
	+ \cfrac{1}{4\pi}\Int{\psi-\pi/2}{0}\left(\left(\theta \cdot (y_2 - z_2)\right)^2 + \left(\theta \cdot (y_1 - z_1)\right)^2\right)\dd\phi \\
	&+ \cfrac{1}{4\pi}\Int{0}{\psi+\pi/2}\left(\left(\theta \cdot (y_2 - z_1)\right)^2 + \left(\theta \cdot (y_1 - z_2)\right)^2\right)\dd\phi
	+ \cfrac{1}{4\pi}\Int{\psi+\pi/2}{\pi}\left(\left(\theta \cdot (y_1 - z_1)\right)^2 + \left(\theta \cdot (y_2 - z_2)\right)^2\right)\dd\phi.
\end{align*}
Elementary trigonometric integration yields
\begin{equation}
	\SWY(Y) = \frac{r^2}{2} + \frac{1}{2} - \frac{2}{\pi}\left(r\cos\psi + r\psi \sin\psi\right) = \frac{u^2+v^2}{2} + \frac{1}{2} - \frac{2}{\pi}\left(u + v\Arctan\frac{v}{u}\right),
\end{equation}
which holds for $\psi \in [0, \pi/2]$. By symmetry, we obtain the following expression for any $(u, v) \in \R^2$ (recall that we stack the vectors in $Y$ line by line):
\begin{equation}
	\SWY\left(\begin{array}{cc}
		u & v \\
		-u & -v
	\end{array}\right) = \frac{u^2+v^2}{2} + \frac{1}{2} - \frac{2}{\pi}\left(|u| + |v|\Arctan\left|\frac{v}{u}\right|\right).
\end{equation}
In the general case, dimension $d$ would require the use of $d$-dimensional spherical coordinates, making the equations~\ref{eqn:ex_sort_Y} and~\ref{eqn:ex_sort_Z} untractable. Furthermore, generalising to $\npoints$ points would separate the integral into $(\npoints!)^2$ parts, losing all hopes of tractability and legibility.

\paragraph{Computing \texorpdfstring{$\W_2^2$}{W}}\label{sec:L2_W} In the case $\npoints=2$, the Kantorovitch LP formulation of the Wasserstein distance can be written as:
$$\underset{a\in [0, 1]}{\min}\ \Sum{k,l \in \llbracket 1, 2 \rrbracket}{}\pi_{k,l}(a)\|y_k-z_l\|_2^2,\quad \text{with}\ \pi(a) := \cfrac{1}{2}\left(\begin{array}{cc}
	1-a & a \\
	a & 1-a
\end{array}\right).$$
Substituting $y_1 = \left(\begin{array}{c}
	u \\
	v
\end{array}\right),\; y_2 = \left(\begin{array}{c}
-u \\
-v
\end{array}\right),\; z_1 = \left(\begin{array}{c}
0 \\
-1
\end{array}\right),\; z_2 = \left(\begin{array}{c}
0 \\
1
\end{array}\right)$ yields:
$$\W_2^2(\gamma_Y, \gamma_Z) = \underset{a \in [0,1]}{\min}\ \left(u^2 + (v+1)^2 - 4av\right) = u^2 + (|v|-1)^2.$$

\paragraph{Computing \texorpdfstring{$\SWpY$}{Ep}}\label{sec:L2_Ep} For simplicity, in the following we will only consider $\theta \in \SS^{d-1}$ such that the $\theta \cdot y_k$ are distinct, and such that the $\theta \cdot z_k$ are also distinct. We will express the cases for the values of the sortings $\sort{Y}{\theta}$ and $\sort{Z}{\theta}$ in a different (yet equivalent) manner.

We have $\sort{Y}{\theta} = I$ if $\theta \cdot y_1 < \theta \cdot y_2$ and $\sort{Y}{\theta} = (2, 1)$ otherwise. Then $\sort{Z}{\theta} \circ (\sort{Y}{\theta})^{-1} = I$ if $\sort{Y}{\theta} = \sort{Z}{\theta}$, and $\sort{Z}{\theta_i} \circ (\sort{Y}{\theta_i})^{-1} = (2, 1)$ otherwise.

The system $\sort{Z}{\theta} \circ (\sort{Y}{\theta})^{-1} = I \Longleftrightarrow \left\lbrace\begin{array}{c}
	\theta \cdot y_1 < \theta \cdot y_2\ \mathrm{and}\ \theta \cdot z_1 < \theta \cdot z_2 \\
	\mathrm{or} \\
	\theta \cdot y_2 < \theta \cdot y_1\ \mathrm{and}\ \theta \cdot z_2 < \theta \cdot z_1 \\
\end{array} \right.$ can be simplified, yielding:
\begin{equation}\label{eqn:2sort_linear}
	\sort{Z}{\theta} \circ (\sort{Y}{\theta})^{-1} = I \Longleftrightarrow \left(\theta \theta^T (z_2 - z_1)\right) \cdot (y_2 - y_1) > 0.
\end{equation}
\ref{eqn:2sort_linear} is a linear equation in $Y$. Additionally, \ref{eqn:2sort_linear} only depends on $y_2 - y_1 = -2y$, which makes our symmetrical simplification inconsequential. Plugging in the specific point values yields a more explicit definition of the cells. We write the condition on $y\in \R^2,$ since $Y = (y, -y)^T$.
\begin{equation}\label{eqn:2cell_polytope}
	\mathcal{C}_\config = \left\lbrace y\in \R^2\ |\ \forall i \in \llbracket 1, p \rrbracket,\; -\sign\left[\theta_i  \cdot \left(\begin{array}{c}
		0 \\
		1
	\end{array}\right)\ \theta_i  \cdot y\right] = +1\  \mathrm{if}\ \config_i = I,\ \mathrm{else} -1 \right\rbrace.
\end{equation}
Equation~\ref{eqn:2cell_polytope} describes $\mathcal{C}_\config$ as an intersection of $p$ half-planes of $\R^2$, thus it is a polytope. Note that we use strict inequalities, which lifts configuration ambiguities, and implies that the $(\mathcal{C}_\config)_{\config \in \mathfrak{S}_2^p}$ are disjoint, and that the union of their closure is $\R^{2}$.

Straightforward computation yields $\underset{X \in \R^{\npoints\times d}}{\argmin}\ q_\config(X) = (A^{-1}(B^{\config}_{1, 1}z_1 + B^{\config}_{1, 2}z_2), A^{-1}(B^{\config}_{2, 1}z_1 + B^{\config}_{2, 2}z_2))$,

where $A := \cfrac{1}{p}\Sum{i=1}{p}\theta_i\theta_i^T$ and $B^{\config}_{k, l} := \cfrac{1}{p}\Sum{\substack{i=1 \\ \config_i(k)=l}}{p}\theta_i\theta_i^T$.

Note that our $\npoints=2$ setting, we have the simplifications $B^{\config}_{1, 2} = A - B^{\config}_{1, 1},\; B^{\config}_{2, 1} = B^{\config}_{1, 2}$ and $B^{\config}_{1, 1} = B^{\config}_{2, 2}$.

Furthermore, $B^{\config}_{k, l}$ is (up to a factor), a Monte-Carlo estimation of $S_{k,l}^{Y,Z}$ (see \ref{cor:crit_points_S}).

\subsection{Discrete Wasserstein stability}\label{sec:WC_stability}

Consider the following generic discrete Kantorovitch problem, given weights on the $\npoints$-simplex $\alpha, \beta \in \Sigma_\npoints$ and a generic cost matrix $C \in \R_+^{\npoints\times \npoints}$.
\begin{equation}\eqlabel{WC}
	\W(\alpha, \beta; C) := \underset{\pi \in \Pi(\alpha, \beta)}{\inf}\ \pi \cdot C
\end{equation}

\begin{lemma}[Stability of the Wasserstein cost]\ Let $\alpha, \oll{\alpha}, \beta, \oll{\beta} \in \Sigma_\npoints$, and $C, \oll{C} \in \R_+^{\npoints\times \npoints}$. Then:

	\begin{equation}
		\left|\W(\alpha, \beta; C) - \W(\oll{\alpha}, \oll{\beta}; \oll{C})\right| \leq 2\|C-\oll{C}\|_F + \sqrt{2}\|C\|_F \left(\|\alpha - \oll{\alpha}\|_2 + \|\beta - \oll{\beta}\|_2\right).
	\end{equation}

\end{lemma}

Note that Step 3 and the general objective of Step 4 are adapted from~\cite{sketchedW}.

\begin{proof}

	\textrm{---} \textit{Step 1}: Re-writing \ref{eqn:WC}

	Consider the Legendre dual problem associated to \ref{eqn:WC}: $\W(\alpha, \beta; C) = \underset{\substack{f, g \in \R^\npoints \\ f \oplus g \leq C}}{\sup}\ f\cdot \alpha + g\cdot \beta$.

	Since the cost and constraint are invariant by a change $(f,g) \mapsto (f+t\mathbf{1}, g-t\mathbf{1})$, we can impose $f_1=0$ and thus restrict to $f \in \R^{\npoints-1}$. Furthermore, we stack the variables by letting $x := (f^T, g^T)^T \in \R^{2\npoints-1}$. The constraint $f\oplus g \leq C$ is equivalent to $Ax \leq b$, where $b \in \R^{\npoints^2}$ is the line-by-line flattening of the matrix $C$, and where:

	$A := \left(\begin{array}{cc}
		A_1 & I_\npoints \\
		\vdots & \vdots \\
		A_\npoints & I_\npoints
	\end{array}\right) \in \R^{\npoints^2 \times (2\npoints -1)}$, where $A_1 := \mathbf{0}_{(\npoints, \npoints-1)} $,

	and for $k \in \llbracket 2, \npoints \rrbracket,\; A_k := \left(\begin{array}{ccc}
		\mathbf{0}_{\npoints, k-2} & \mathbf{1}_{\npoints, 1} & \mathbf{0}_{\npoints, \npoints-k}
	\end{array}\right) \in \R^{\npoints\times (\npoints -1)}$.

	With $a := (\alpha_2, \cdots, \alpha_\npoints, \beta_1, \cdots, \beta_\npoints)^T$, the re-written dual is:
	\begin{equation}\eqlabel{WC'}
		W(\alpha, \beta; C) = \underset{\substack{x \in \R^{2\npoints-1} \\ Ax \leq b}}{\sup}\ x\cdot a
	\end{equation}
	Now notice that $\rank(A) = 2\npoints-1$ and that $A^TA = \npoints I_{2\npoints -1} + J$, where $J := \left(\begin{array}{cc}
		\mathbf{0}_{\npoints-1, \npoints-1} & \mathbf{1}_{\npoints-1, \npoints} \\
		\mathbf{1}_{\npoints, \npoints-1} & \mathbf{0}_{\npoints, \npoints}
	\end{array}\right)$. Since the eigenvalues of $J$ are $(-\sqrt{\npoints(\npoints -1)}, 0, \cdots, 0, \sqrt{\npoints(\npoints-1)})$,

	the smallest eigenvalue of $A^TA$ is $\lambda_{\min}(A^TA) = \npoints - \sqrt{\npoints(\npoints -1)} = \cfrac{\npoints}{\npoints + \sqrt{\npoints(\npoints -1)}}$ by multiplying and dividing by the root conjugate.

	Therefore, the smallest singular value of $A$ is $s_\npoints := \sigma_{\min}(A) = \sqrt{\cfrac{1}{1+ \sqrt{1-\frac{1}{\npoints}}}}$.

	\textrm{---} \textit{Step 2}: Separating the terms

	We consider the re-written dual formulation~\ref{eqn:WC'}, in particular $b, \oll{b} \in \R^{\npoints^2}$ is the line-by-line flattening of $C$ and $\oll{C}$, respectively; and $a := (\alpha_2, \cdots, \alpha_\npoints, \beta_1, \cdots, \beta_\npoints)^T$, with a corresponding definition for $\oll{a}$. Let $\mathcal{P} := \left\lbrace x \in \R^{2\npoints -1}\ |\ Ax \leq b \right\rbrace, \quad \oll{\mathcal{P}} := \left\lbrace x \in \R^{2\npoints-1}\ |\ Ax \leq \oll{b} \right\rbrace$.	We split the difference in two terms by adding and subtracting $\underset{x \in \mathcal{P}}{\sup}\ x \cdot \oll{a}$:
	\vspace{-10pt}
	\begin{align*}\left|\W(\alpha, \beta; C) - \W(\oll{\alpha}, \oll{\beta}; \oll{C})\right|
		&\leq \left|\underset{\oll{x} \in \oll{\mathcal{P}}}{\sup}\ \oll{x} \cdot \oll{a} - \underset{x \in \mathcal{P}}{\sup}\ x \cdot \oll{a}\right| =: \mathrm{I} \\
		&+ \left|\underset{\oll{x} \in \mathcal{P}}{\sup}\ \oll{x} \cdot \oll{a} - \underset{x \in \mathcal{P}}{\sup}\ x \cdot a\right| =: \mathrm{II}
	\end{align*}
	\textrm{---} \textit{Step 3}: Controlling I by a Hausdorff distance

	Let $S := \underset{x \in \mathcal{P}}{\sup}\ x \cdot \oll{a}, \quad \oll{S} := \underset{\oll{x} \in \oll{\mathcal{P}}}{\sup}\ \oll{x} \cdot \oll{a}$ the respective problem values. The suprema are attained (since the primal problem is minimising a continuous function over a compact set, and since we have strong duality in these linear programs, for example). Let $x^* \in \mathcal{P}$ such that $S = x^* \cdot \oll{a}$.

	We consider the $\|\cdot\|_2$-induced Hausdorff distance on $\mathcal{S}(\R^{2\npoints-1})$, the subsets of $\R^{2\npoints-1}$:
	$$\mathrm{for}\  U, V \in \mathcal{S}(\R^{2\npoints-1}), \quad d_H(U, V) = \max\left(\underset{u \in U}{\sup}\ \underset{v \in V}{\inf}\ \|u-v\|_2,\ \underset{v \in V}{\sup}\ \underset{u \in U}{\inf}\ \|u-v\|_2\right)$$
	Notice that $d_H(\mathcal{P}, \oll{\mathcal{P}}) \geq \underset{x \in \mathcal{P}}{\sup}\ \underset{y \in \oll{\mathcal{P}}}{\inf}\ \|x-y\|_2 \geq \underset{y \in \oll{\mathcal{P}}}{\inf}\ \|x^*-y\|_2 = \|x^* - \hat y\|_2$, with $\hat y \in \underset{y \in \oll{\mathcal{P}}}{\argmin}\ \|x^* - y\|_2$.

	Note that $\hat y$ can be defined, since $\oll{\mathcal{P}}$ is closed and $\|\cdot\|_2$ is continuous and coercive.

	Using $\oll{S} \geq \hat y \cdot \oll{a}$, we now have $S - \oll{S} \leq (x^* - \hat y) \cdot \oll{a} \leq \|x^* - \hat y\|_2 \|\oll{a}\|_2$.

	Since $\oll{\alpha}, \oll{\beta} \in \Sigma_\npoints$, we have $\|\oll{a}\|_2 \leq \sqrt{2}$. Furthermore we use $\|x^* - \hat y\|_2 \leq d_H(\mathcal{P}, \oll{\mathcal{P}})$ and deduce $S-\oll{S} \leq \sqrt{2}d_H(\mathcal{P}, \oll{\mathcal{P}})$. By symmetry on $S$ and $\oll{S}$, we conclude:
	$$\mathrm{I} \leq \sqrt{2}d_H(\mathcal{P}, \oll{\mathcal{P}}).$$
	\textrm{---} \textit{Step 4}: Controlling the Hausdorff distance

	For any $x, y \in \R^{2\npoints-1}$, we have $\|Ax - Ay\|_2 \geq s_\npoints\|x-y\|_2$ (see Step 1), with notably $s_\npoints > 0$. In particular, we deduce that $d_H(\mathcal{P}, \oll{\mathcal{P}}) \leq \frac{1}{s_\npoints}d_H(A\mathcal{P}, A\oll{\mathcal{P}})$. Notice that $A\mathcal{P} = \Im A \cap \mathcal{Q}$, where $\mathcal{Q} := \left\lbrace y \in \R^{\npoints^2}\ :\ y \leq b \right\rbrace$, and similarly $A\oll{\mathcal{P}} = \Im A \cap \oll{\mathcal{Q}}$.

	Then $d_H(\Im A \cap \mathcal{Q}, \Im A \cap \oll{\mathcal{Q}}) \leq d_H(\mathcal{Q}, \oll{\mathcal{Q}}) \leq \|\oll{C} - C\|_F$ (there is in fact equality in the second inequality. Indeed, we can compute the Hausdorff distance by solving two successive convex problems, whose values are reached at extremal points of $\mathcal{Q}$ and $\oll{\mathcal{Q}}$).

	Since (by Step 1), the smallest singular value of $A$ is $s_\npoints = \sqrt{\cfrac{1}{1+ \sqrt{1-\frac{1}{\npoints}}}} \geq \cfrac{1}{\sqrt{2}}$ we conclude with:
	$$d_H(\mathcal{P}, \oll{\mathcal{P}}) \leq \sqrt{2}\|\oll{C} - C\|_F.$$
	\textrm{---} \textit{Step 3}: Controlling II

	We have $\mathrm{II} = \left|\underset{\oll{x} \in \mathcal{P}}{\sup}\ \oll{x}\cdot \oll{a} - \underset{x \in \mathcal{P}}{\sup}\ x\cdot a \right|$. Let $\oll{x}^* \in \underset{\oll{x} \in \mathcal{P}}{\argmax}\ \oll{x}\cdot \oll{a}$:
	$$\underset{\oll{x} \in \mathcal{P}}{\sup}\ \oll{x}\cdot \oll{a} - \underset{x \in \mathcal{P}}{\sup}\ x\cdot a \leq \underset{\oll{x} \in \mathcal{P}}{\sup}\ \oll{x}\cdot \oll{a} - \oll{x}^* \cdot a = \overline{x}^* \cdot (\oll{a} - a) \leq \underset{x \in \mathcal{P}}{\sup}\ x\cdot (\oll{a} - a) \leq \underset{x \in \mathcal{P}}{\sup}\ \|x\|_2\cdot \|\oll{a} - a\|_2$$
	By applying the same reasoning to $\underset{x \in \mathcal{P}}{\sup}\ x\cdot a -\underset{\oll{x} \in \mathcal{P}}{\sup}\ \oll{x}\cdot \oll{a}$, we obtain $\mathrm{II} \leq \underset{x \in \mathcal{P}}{\sup}\ \|x\|_2\cdot \|\oll{a} - a\|_2$.

	Now let $x \in \mathcal{P}$. For any $k \in \llbracket 1, 2\npoints-1 \rrbracket, \quad (Ax)_k \leq b_k$, thus $\|Ax\|_2 \leq \|b\|_2$. However, $s_\npoints \|x\|_2 \leq \|Ax\|_2$, thus $\|x\|_2 \leq s_\npoints^{-1}\|b\|_2$. We conclude $\mathrm{II} \leq \sqrt{2}(\|\alpha - \oll{\alpha}\|_2 + \|\beta - \oll{\beta}\|_2)\|C\|_F$.

	\textrm{---} \textit{Step 5}: Wrapping up

	By Step 1 and Step 2 combined we have $\mathrm{I} \leq \sqrt{2}d_H(\mathcal{P}, \oll{\mathcal{P}}) \leq 2\|\oll{C} - C\|_F$.

	Then $\left|\W(\alpha, \beta; C) - \W(\oll{\alpha}, \oll{\beta}; \oll{C})\right| \leq \mathrm{I} + \mathrm{II} \leq 2\|\oll{C} - C\|_F + \sqrt{2}(\|\alpha - \oll{\alpha}\|_2 + \|\beta - \oll{\beta}\|_2)\|C\|_F$.
\end{proof}

\subsection{Proof of \ref{thm:cv_fixed_point_distance} and convergence rate}\label{p:cv_fixed_point_distance}

The proof of \ref{thm:cv_fixed_point_distance} requires matrix concentration technicalities. In the following, $\|\cdot\|_{\mathrm{op}}$ denotes the $\|\cdot\|_2$-induced operator norm on $\R^{d \times d}$, and $S_d(\R)$ denotes the space of symmetric $d\times d$ matrices. We write $\preceq$ for the Loewner order of positive semi-definite symmetric matrices ($A \preceq B$ means that $B-A$ is positive semi-definite). We recall the following Hoeffding inequality.

\begin{theorem}[Matrix Hoeffding Inequality,~\cite{tropp2012concentration}, Theorem 1.3]\label{thm:tropp_matrix_concentration}\
  
	Let $q \in \N^*,\; (X_i)_{i \in \llbracket 1, q \rrbracket}$ independent random variables with values in $S_d(\R)$, such that $\E{X_i} = 0$.
	Suppose that $\forall i \in \llbracket 1, q \rrbracket, \; \exists A_i \in S_d(\R) : X_i^2 \preceq A_i^2$. Let $\sigma^2 := \left\|\Sum{i=1}{q}A_i^2\right\|_{\mathrm{op}}$,
	then for any $t>0$, $\P\left(\left\|\Sum{i=1}{q}X_i\right\|_{\mathrm{op}} \geq t\right) \leq d \exp\left(-\cfrac{t^2}{8\sigma^2}\right).$
\end{theorem}

We deduce from~\ref{thm:tropp_matrix_concentration} the following lemma, where the $X_i$ follow a uniform law on $\Theta \subset \SS^{d-1}$.
\begin{lemma}[Hoeffding applied to $\theta \sim \mathcal{U}(\Theta)$]\label{lemma:hoeffding}\

	Let $(\theta_i)_{i \in \llbracket 1, q \rrbracket}$, independent random vectors following the uniform law on $\Theta \subset\SS^{d-1}$, where $\Theta$ is $\bbsigma$-measurable with $\bbsigma(\Theta) > 0$.
	Let $S_\Theta := \cfrac{1}{s_\Theta}\Int{\Theta}{}\theta \theta^T \dd \bbsigma(\theta)$, where $s_\Theta := \bbsigma(\Theta)$. $S_\Theta$ is the covariance matrix of $\theta \sim \mathcal{U}(\Theta)$.
	Let $\eta \in ]0, 1[$ and $t > 0$. If $q \geq \cfrac{32\log\left(d/\eta\right)}{t^2}$, then with probability exceeding $1 - \eta$ we have $\left\|\cfrac{1}{q}\Sum{i=1}{q}\theta_i\theta_i^T - S_\Theta\right\|_{\mathrm{op}} \leq t$.
	In the case $\Theta = \SS^{d-1}$, the condition $q \geq \cfrac{8\log\left(d/\eta\right)}{t^2}$ is sufficient.
\end{lemma}

\begin{proof}
	The idea is to apply~\ref{thm:tropp_matrix_concentration} to $X_i := \frac{1}{q}\theta_i\theta_i^T - \frac{1}{q}S_\Theta$. First, by definition, $\E{X_i} = 0$.

	We now find $A \in S_d^+(\R)$ such that $X_i^2 \preceq A$. Let $u \in \SS^{d-1}$, we compute:
	$$u^TX_i^2u = \frac{1}{q^2}\left(u^T\theta_i \theta_i^Tu - u^T\theta_i\theta_i^T S_\Theta u - u^T S_\Theta\theta_ i\theta_i^T u + u^T S_\Theta^2 u\right) \leq \left(\frac{1 + \|S_\Theta\|_{\mathrm{op}}}{q}\right)^2.$$
	Moreover, $\|S_\Theta\|_{\mathrm{op}} \leq 1$, since $\forall u \in \SS^{d-1}, \; u^TS_\Theta u = \cfrac{1}{s_\Theta}\Int{\Theta}{}u^T\theta \theta^T u \dd \bbsigma(\theta) \leq \cfrac{1}{s_\Theta}\Int{\Theta}{}1\dd \bbsigma(\theta) = 1$.
	In conclusion $X_i^2 \preceq \frac{4}{q^2}I$. Using the notations of~\ref{thm:tropp_matrix_concentration},  we compute $\sigma^2 = 4/q$, and apply the Matrix Hoeffding inequality with $\Delta := \Sum{i=1}{q}X_i = \cfrac{1}{q}\Sum{i=1}{q}\theta_i \theta_i^T - S_\Theta$. It follows that for any $t > 0,\; \P\left(\|\Delta\|_{\mathrm{op}} \geq t\right) \leq d \exp\left(-\cfrac{qt^2}{32}\right)$.
	In order to have the event $\|\Delta\|_{\mathrm{op}} \leq t$ with probability exceeding $1 - \eta$, it is therefore sufficient that $\eta \geq  d \exp\left(-\cfrac{qt^2}{32}\right)$, which is equivalent to $q \geq \cfrac{32 \log(d/\eta)}{t^2}$.

	In the case $\Theta = \SS^{d-1}$, one has $S_\Theta = I/d$, and a finer Loewner upper-bound can be established, since 
	$u^TX_i^2u =  \cfrac{1}{q^2}\left(u^T\theta_i\theta_i^Tu - \frac{2}{d}u^T\theta_i\theta_i^Tu + \frac{1}{d^2}\right) \leq \left(\cfrac{1 - \frac{1}{d}}{q}\right)^2 \leq \cfrac{1}{q^2}$, and thus $\sigma^2 = 1/q$ here.
	This yields the Hoeffding inequality $\P\left(\|\Delta\|_{\mathrm{op}} \geq t\right) \leq d \exp\left(-\cfrac{qt^2}{8}\right)$, which in turn provides the announced weaker condition on $q$.
\end{proof}

With this tool at hand, we now prove a quantitative concentration result:

\begin{theorem}[Concentration of cell optima]\label{thm:ystar_concentration}\

Let $\mathbf{m} = (\sigma_1, \cdots, \sigma_p)$ be a fixed matching configuration (see~\ref{sec:cells}) and let $(\theta_i)_{i \in \llbracket 1, p \rrbracket} \sim \bbsigma^{\otimes p}$ (uniform on $\SS^{d-1}$).
	For $(k, l) \in \llbracket 1, \npoints \rrbracket^2$, let $q_{k,l} := \#\lbrace i \in \llbracket 1, p \rrbracket\ |\ k = \sigma_i(l) \rbrace$.
	Let $\varepsilon \in ]0, \frac{4}{3}\npoints\oll{c}_Z]$ with $\oll{c}_Z := \underset{l \in \llbracket 1, \npoints \rrbracket}{\max}\ \|z_l\|_2$, let $\eta \in ]0, 1[$, and assume the following:

	\begin{itemize}
		\item[\textbullet] $(H_q): \quad \forall (k, l)\in \llbracket 1, \npoints \rrbracket^2, \; q_{k,l} \geq \oll{q}$ or $q_{k,l} < \ull{q}, $ with $1 \leq \ull{q} \leq  \oll{q} \leq p\ ;$

		\item[\textbullet] $(H_1): \quad p \geq \cfrac{697d^2\npoints^2\oll{c}_Z^2\log\left(3d/\eta\right)}{\varepsilon^2}\; ;$

		\item[\textbullet] $(H_2) : \quad \oll{q} \geq \cfrac{512d^2\oll{c}_Z^2\log(3d\npoints\npoints^+/\eta)}{\varepsilon^2}\; ; \quad \quad  \npoints^+ := \underset{k \in \llbracket 1, \npoints \rrbracket}{\max}\ \#\lbrace l \in \llbracket 1, \npoints \rrbracket\ |\ q_{k,l} \geq \oll{q} \rbrace; $

		\item[\textbullet] $(H_3): \quad \ull{q} \leq \cfrac{\varepsilon}{8d\npoints^-\oll{c}_Z}\ p;\quad \quad  \npoints^- := \underset{k \in \llbracket 1, \npoints \rrbracket}{\max}\ \#\lbrace l \in \llbracket 1, \npoints \rrbracket\ |\ q_{k,l} \leq  \ull{q} \rbrace;$

		\item[\textbullet] $(H_4): \quad p \geq \cfrac{8d^2\npoints^2\oll{c}_Z^2\log(6\npoints^2/\eta)}{\varepsilon^2}.$

	\end{itemize}

	Then with probability exceeding $1-\eta$, writing $Y^* := \underset{Y' \in \R^{\npoints \times d}}{\argmin}\ q_\config(Y')$, we have
	\begin{equation}\label{eqn:ystar_concentration}
          \forall k \in \llbracket 1, \npoints \rrbracket,\; \left\|y_k^* - \Sum{l=1}{\npoints}S_{k,l}z_l\right\|_{2} \leq \varepsilon,
	\end{equation}
        where the normalized conditional covariance matrices $S_{k,l}$ are defined in~\ref{cor:crit_points_S} (we omit the $Y,Z$ exponent here for legibility).
\end{theorem}

%

\begin{proof}
	\textrm{---} \textit{Step 1}: Re-writing~\ref{eqn:next_opt_pos}.

	Remind that the matching configuration $\mathbf{m}$ is fixed here. Let $Y^* := \underset{Y' \in \R^{\npoints\times d}}{\argmin}\ q_\mathbf{m}(Y')$ and $k \in \llbracket 1, \npoints \rrbracket$. By~\ref{eqn:next_opt_pos}, we have $y_k^* = A^{-1}\left(\cfrac{1}{p}\Sum{i=1}{p}\theta_i \theta_i^T z_{\sigma_i(k)}\right)$,
	with $A = \cfrac{1}{p}\Sum{i=1}{p}\theta_i\theta_i^T$. Let $I_{k,l} := \lbrace i \in \llbracket 1, p \rrbracket\ |\ \sigma_i(k) = l \rbrace$. Since the $\sigma_i$ are permutations, we have $\llbracket 1, p \rrbracket = \Reu{l=1}{\npoints}I_{k,l} = \Reu{k=1}{\npoints}I_{k,l}$ and $k \neq k' \Rightarrow I_{k,l} \cap I_{k',l} = \varnothing; \; l \neq l' \Rightarrow I_{k,l} \cap I_{k,l'} = \varnothing$.

	We re-order the sum: $\cfrac{1}{p}\Sum{i=1}{p}\theta_i \theta_i^Tz_{\sigma_i(k)} = \Sum{l=1}{\npoints}\cfrac{1}{p}\Sum{i \in I_{k,l}}{}\theta_i \theta_i^T z_l = \Sum{l=1}{\npoints}\cfrac{q_{k,l}}{p}B_{k,l}z_l,$
	where $q_{k,l} := \#I_{k,l}$ and $B_{k,l} := \cfrac{1}{q_{k,l}}\Sum{i \in I_{k,l}}{}\theta_i \theta_i^T$.
	This invites the definition of the matrix $R = (r_{k,l}),\; r_{k,l} := \cfrac{q_{k,l}}{p}$, which is bi-stochastic by construction.

	\textrm{---} \textit{Step 2}: Separating the terms in $y_k^*$.

	We will see later that the empirical covariance matrix $A$ concentrates towards the covariance matrix of $\theta \sim \bbsigma$, which is $I/d$. In order to quantify the impact of this concentration on $y_k^*$, we introduce the error term: $\delta A^- := A^{-1} - dI$.

	A similar concentration will be observed for $B_{k,l}$, but the $\theta_i$ in the sum are \textit{selected} such that $i \in I_{k,l}$. Recall that since we project in 1D, the permutations $\sigma_i$ arise from a sorting problem, namely $\sigma_i = \sort{Z}{\theta_i} \circ \left(\sort{Y}{\theta_i}\right)^{-1}$, where we recall that $\sort{Y}{\theta}$ is a permutation sorting the numbers $(y_1 \cdot \theta, \cdots, y_\npoints \cdot \theta)$.

        By definition, we have $\sigma_i(k)=l \Longleftrightarrow \theta_i \in \Theta_{k,l} = \left\lbrace \theta \in \SS^{d-1}\ |\ \sort{Z}{\theta} \circ \left(\sort{Y}{\theta}\right)^{-1}(k) = l \right\rbrace$, where we omit again the $Y,Z$ exponent on $\Theta_{k,l}$ for legibility.

	Since the $\theta_i$ in $B_{k,l}$ are drawn under the condition $\theta_i \in \Theta_{k,l}$, we study the concentration $B_{k,l} \approx C_{k,l}$, where $C_{k,l} :=\cfrac{1}{d\bbsigma(\Theta_{k,l})} S_{k,l} $.
        %
	In order to quantify this approximation, we define the error term $\delta B_{k,l} := B_{k,l} - C_{k,l}$.
	Similarly, the $r_{k,l} := \cfrac{q_{k,l}}{p}$ are Monte-Carlo approximations of $\bbsigma(\Theta_{k,l})$, which leads to the definition $\delta r_{k,l} := r_{k,l} - \bbsigma(\Theta_{k,l})$.

	We may now separate the terms in the result from Step 1:
	%
	%
	%
	%
	%
	%
	\begin{align*}
		y_k^* &= (dI + \delta A^-)\left(\Sum{l=1}{\npoints}r_{k,l}(\underbrace{C_{k,l} + \delta B_{k,l}}_{B_{k,l}})z_l\right)\\
                      &=
                        \underbrace{d\Sum{l=1}{\npoints}\bbsigma(\Theta_{k,l})C_{k,l}z_l}_{v} +
                        \underbrace{\delta A^- \left(\Sum{l=1}{\npoints}r_{k,l}B_{k,l}z_l\right)}_{\delta v_1} +
                        \underbrace{d\Sum{\substack{l=1 \\ q_{k,l} \geq \oll{q}}}{\npoints}r_{k,l}\delta B_{k,l}z_l}_{\delta v_2} +
          \underbrace{d\Sum{\substack{l=1 \\ q_{k,l} < \ull{q}}}{\npoints}r_{k,l}\delta B_{k,l}z_l}_{\delta v_3} + \underbrace{d\Sum{l=1}{\npoints}\delta r_{k,l}C_{k,l}z_l}_{\delta v_4}.
		%
	\end{align*}
        
	The separation of the terms in the second equality arises from $(H_q)$, formulated in the theorem. Observe that the first term $v$ is exactly $\Psi(Y^*)$, with $\Psi$ defined in~\ref{sec:E_crit}.
	Our objective is to provide conditions under which $\forall i \in \lbrace 1, 2, 3, 4\rbrace,\;\|\delta v_i\|_2 \leq \varepsilon / 4$ with probability exceeding $1-\eta$. To that end, we let $\varepsilon > 0$ and $\eta \in ]0, 1[$.

	\textrm{---} \textit{Step 3}: Condition for $\|\delta v_2\|_2 \leq \cfrac{\varepsilon}{4}$.

	First of all, note that if the sum defining $\delta v_2$ is empty, the condition holds trivially almost-surely. In the following, we suppose that the sum has at least one nonzero term.
	We have from Step 2, $\|\delta v_2\|_2 = \left\|d\Sum{\substack{l=1 \\ q_{k,l} \geq \oll{q}}}{\npoints}r_{k,l}\delta B_{k,l}z_l\right\|_2 \leq d \oll{c}_Z\Sum{\substack{l=1 \\ q_{k,l} \geq \oll{q}}}{\npoints}r_{k,l}\|\delta B_{k,l}\|_{\mathrm{op}}$.
	Let the shorthands $\npoints_k^+ := \# J_k^+$ and $J_k^+ := \lbrace l \in \llbracket 1, \npoints \rrbracket\ |\ q_{k,l} \geq \oll{q} \rbrace$. We upper-bound the right term by
	$\Sum{\substack{l \in J_k^+}}{}r_{k,l}\|\delta B_{k,l}\|_{\mathrm{op}}  \leq \Sum{l\in J_k^+}{}r_{k,l} \underset{l \in J_k^+}{\max }\|\delta B_{k,l}\|_{\mathrm{op}} \leq \underset{l \in J_k^+}{\max }\|\delta B_{k,l}\|_{\mathrm{op}}$.

	For $l \in J_k^+,$ by \ref{lemma:hoeffding}, we have $\|\delta B_{k,l}\|_{\mathrm{op}} \leq t$ with probability exceeding $1 - \eta/(3\npoints\npoints_k^+)$ provided that $q_{k,l} \geq \cfrac{32\log\left(3d\npoints\npoints_k^+/\eta\right)}{t^2}$.
        Since the probability of $\bigcup_{l \in J_k^+} \{\|\delta B_{k,l}\|_{\mathrm{op}} > t\}$ can be upper bounded by the sum of the probabilities of each of the $n_k^+$ terms, it is upper bounded by $\eta/(3\npoints)$.
        Therefore, writing the event $\{\forall l \in J_k^+,\; \|\delta B_{k,l}\|_{\mathrm{op}} \leq t\}$ as the complementary of this union, we conclude that it holds with probability exceeding $1 - \eta/(3\npoints)$, provided that $ \forall  l \in J_k^+, \; q_{k,l} \geq \cfrac{32\log\left(3d\npoints\npoints_k^+/\eta\right)}{t^2}$.
	A sufficient condition for this last assumption to hold is $(H_2^k): \; \oll{q} \geq \cfrac{32\log\left(3d\npoints\npoints_k^+/\eta\right)}{t^2}$.
        Applying this result to $t := \cfrac{\varepsilon}{4d\oll{c}_Z}$, and by letting $\npoints^+ := \underset{k \in \llbracket 1, \npoints \rrbracket}{\max}\ \npoints_k^+,$ a sufficient condition 
        to have $\|\delta v_2\|_2 \leq \cfrac{\varepsilon}{4}$ with probability exceeding $1 - \eta/(3\npoints)$ is
	$$\quad (H_2) : \quad \oll{q} \geq \cfrac{512d^2\oll{c}_Z^2\log(3d\npoints\npoints^+/\eta)}{\varepsilon^2}.$$
    
	\textrm{---} \textit{Step 4}: Condition for $\|\delta v_3\|_2 \leq \cfrac{\varepsilon}{4}$.

	With a computation analogous to Step 3, we write
	$\|\delta v_3\|_2 = \left\|d\Sum{\substack{l=1 \\ q_{k,l} < \ull{q}}}{\npoints}r_{k,l}\delta B_{k,l}z_l\right\|_2 \leq d\oll{c}_Z \Sum{l \in J_k^-}{}r_{k,l}\|\delta B_{k,l}\|_{\mathrm{op}},$
	where, like in Step 3, we define $\npoints_k^- := \# J_k^-$ and $J_k^- := \lbrace l \in \llbracket 1, \npoints \rrbracket\ |\ q_{k,l} \leq \ull{q} \rbrace$.
	If $\npoints_k^- = 0$ then the objective holds almost-surely, thus we suppose $\npoints_k^- \geq 1$.
	In this setting, the $q_{k,l}$ are small, thus we have little control over $\|\delta B_{k,l}\|_{\mathrm{op}}$, which can be upper bounded by $2$ %

	Leveraging the condition $q_{k,l} \leq \ull{q}$, which holds for $l \in J_k^-$, we have $r_{k,l} = q_{k,l}/p \leq \ull{q} / p$.
	In order to have $\|\delta v_3\|_2 \leq \cfrac{\varepsilon}{4}$ almost-surely, it is sufficient to have $(H^k_3): \quad \ull{q} \leq \cfrac{\varepsilon}{8d\npoints_k^-\oll{c}_Z}\ p.$
	Again, with $\npoints^- := \underset{k \in \llbracket 1, \npoints \rrbracket}{\max}\ \npoints_k^-,$ we obtain the sufficient condition:
	$$(H_3): \quad \ull{q} \leq \cfrac{\varepsilon}{8d\npoints^-\oll{c}_Z}\ p.$$
        
	\textrm{---} \textit{Step 5}: Condition for $\|\delta v_4\|_2 \leq \cfrac{\varepsilon}{4}$.

	By definition, $\delta v_4 = d\Sum{l=1}{\npoints}\delta r_{k,l} C_{k,l}z_l$, and we dominate $\|\delta v_4\|_{2} \leq \oll{c}_Zd\Sum{l=1}{\npoints}|\delta r_{k,l}| \|C_{k,l}\|_{\mathrm{op}}$.
	We use the upper-bound $\|C_{k,l}\|_{\mathrm{op}} \leq 1$ (observe that  $\|C_{k,l}\|_{\mathrm{op}}$ can be made as close to $1$ as desired by choosing $\Theta_{k,l}$ as a very small portion of the sphere).
	In order to have $\|\delta v_4\|_2 \leq \cfrac{\varepsilon}{4}$, it is sufficient to have $\forall l \in \llbracket 1, \npoints \rrbracket,\; |\delta r_{k,l}| \leq \cfrac{\varepsilon}{4d\npoints\oll{c}_Z} =: t$.

	Our objective is to quantify the Monte-Carlo error $\delta r_{k,l} = \cfrac{\#\lbrace i \in \llbracket 1, p \rrbracket\  |\  \theta_i \in \Theta_{k,l}\rbrace}{p} - \bbsigma(\Theta_{k,l})$.
	To that end, we fix $l \in \llbracket 1, \npoints \rrbracket$ and apply the standard Bernoulli Chernoff concentration inequality (additive form) to $X_i := \mathbbold{1}(\theta_i \in \Theta_{k,l})$. By definition, $\E{X_i} = \bbsigma(\Theta_{k,l})$, hence by Chernoff
	$$\P\left(\left|\cfrac{1}{p}\Sum{i=1}{p}X_i - \bbsigma(\Theta_{k,l})\right| > t\right) \leq 2e^{-2pt^2}.$$
	It follows that the inequality $p \geq \cfrac{\log(6\npoints^2/\eta)}{2t^2}$ implies $|\delta r_{k,l}|\leq t$ with probability exceeding $1 - \cfrac{\eta}{3\npoints^2}$.
	Substituting $t = \cfrac{\varepsilon}{4d\npoints\oll{c}_Z}$ yields $(H_4):\; p \geq \cfrac{8d^2\npoints^2\oll{c}_Z^2\log(6\npoints^2/\eta)}{\varepsilon^2}$.
	Using the same reasoning as in previous steps, under $(H_4)$, the event $\left\{ \forall l \in \llbracket 1, \npoints \rrbracket,\; |\delta r_{k,l}| \leq \cfrac{\varepsilon}{4d\npoints\oll{c}_Z} \right\}$ holds with probability exceeding $1 - \cfrac{\eta}{3\npoints}$, which implies that our objective $\|\delta v_4\|_2 \leq \cfrac{\varepsilon}{4}$ also holds with the same probability.

	\textrm{---} \textit{Step 6}: Condition for $\|\delta v_1\|_2 \leq \cfrac{\varepsilon}{4}$.

	We have $\|\delta v_1\|_2 \leq \|\delta A^-\|_{\mathrm{op}} \left\|\Sum{l=1}{\npoints}r_{k,l}B_{k,l}z_l\right\|_2 \leq  \cfrac{\|\delta A^-\|_{\mathrm{op}}}{d} \left(\|v\|_2 + \|\delta v_2\|_2 + \| \delta v_3\|_2 + \|\delta v_4\|_2\right)$.
	In the following, we continue conditionally on the three events $\text{\textquotedblleft}\|\delta v_i\|_2 \leq \cfrac{\varepsilon}{4}\text{\textquotedblright},\; i \in \lbrace 2, 3, 4 \rbrace$, under which:
	$$\|\delta v_1\|_2 \leq \cfrac{\|\delta A^-\|_{\mathrm{op}}}{d} \left(\|v\|_2 + \cfrac{3\varepsilon}{4}\right).$$
	We now dominate $\|v\|_2 = \left\|\Sum{l=1}{\npoints}S_{k,l}z_l\right\|_2$.
      Recall that the $(\Theta_{k,l})_{l \in \llbracket 1, \npoints \rrbracket}$ are disjoint, with $\Reu{l=1}{\npoints}\Theta_{k,l} = \SS^{d-1}$, which implies $\Sum{l=1}{\npoints}S_{k,l} = d\Int{\SS^{d-1}}{}\theta \theta^T \dd \bbsigma(\theta) = I$.
	Since the $S_{k,l}$ are symmetric semi-definite, the previous equation provides $\|S_{k,l}\|_{\mathrm{op}} \leq 1$, which in turn yields $\|v\|_2 \leq \npoints\oll{c}_Z$.
	Assuming $\varepsilon \leq \cfrac{4}{3}\npoints\oll{c}_Z$, we get finally $\|\delta v_1\|_2 \leq \|\delta A^-\|_{\mathrm{op}} \cfrac{2\npoints\oll{c}_Z}{d}$.


	It is sufficient to find a condition under which $\|\delta A^-\|_{\mathrm{op}} \leq \cfrac{d\varepsilon}{8\npoints\oll{c}_Z} =: t$.
	We cannot apply \ref{lemma:hoeffding} directly since $\delta A^-$ has an inverse operation.
	First, $\|\delta A^-\|_{\mathrm{op}} = \|A^{-1} -dI\|_{\mathrm{op}} = \left\|d(I - d\delta A)^{-1} - dI\right\|_{\mathrm{op}}$, with $\delta A := I/d - A$.
	Then, assuming $(H_{\delta A}) : d\|\delta A\|_{\mathrm{op}} < 1$, we use a Neumann series for the inverse:
	$$\|\delta A^{-}\|_{\mathrm{op}} = \left\|\Sum{n=1}{+\infty}(d\delta A)^n\right\|_{\mathrm{op}} \leq \Sum{n=1}{+\infty}(d\|\delta A\|_{\mathrm{op}})^n,$$
	and finally $\|\delta A^-\|_{\mathrm{op}} \leq \cfrac{d^2\|\delta A\|_{\mathrm{op}}}{1 - d\|\delta A\|_{\mathrm{op}}}$. Consider $f := \app{[0, \frac{1}{d}[}{[0, +\infty [}{u}{\frac{d^2u}{1-du}}$.
	The function $f$ is bijective and increasing, with $f^{-1} = \app{[0, +\infty [}{[0, \frac{1}{d}[}{v}{\frac{v}{d(d+v)}}$.
	This analysis yields under $(H_{\delta A}),\; \|\delta A^-\|_{\mathrm{op}} \leq t \Longleftarrow \|\delta A\|_{\mathrm{op}} \leq \cfrac{t}{d(d+t)}$.
	Conveniently, by \ref{lemma:hoeffding}, $\|\delta A\|_{\mathrm{op}} \leq s$ with probability $1 - \eta/3$ if $(H_1):\; p \geq \cfrac{8\log\left(3d/\eta\right)}{s^2}$.
	We can apply this to $\cfrac{t}{d(d+t)} = \cfrac{\varepsilon}{8d\npoints\oll{c}_Z\left(1 + \cfrac{\varepsilon}{8\npoints\oll{c}_Z}\right)}$, but in order to simplify the expression, we apply it to $s := \cfrac{3\varepsilon}{28d\npoints\oll{c}_Z} \leq \cfrac{t}{d(d+t)}$ (the inequality holds thanks to $\varepsilon \leq \frac{4}{3}\npoints\oll{c}_Z$.)

	Now we must quantify the assumption $(H_{\delta A}) : \|\delta A\|_{\mathrm{op}} < 1/d$. Notice that $s \leq 1/d$ and thus the event $\|\delta A\|_{\mathrm{op}} < s$ is contained in the event $\|\delta A\|_{\mathrm{op}} < 1/d$, hence it is sufficient to satisfy $(H_1)$, which we re-write (after upper-bounding $8\times 28^2 / 9 \leq 697$):
	$$(H_1): \quad p \geq \cfrac{697d^2\npoints^2\oll{c}_Z^2\log\left(3d/\eta\right)}{\varepsilon^2}.$$
	To summarise, under $(H_1)$, we have $\|\delta A\|_{\mathrm{op}} \leq s$ with probability exceeding $1 - \eta/3$.
	Conditionnaly to the events $\text{\textquotedblleft}\|\delta A\|_{\mathrm{op}} \leq s\text{\textquotedblright},\;\text{\textquotedblleft}\|\delta v_i\|_2 \leq \cfrac{\varepsilon}{4}\text{\textquotedblright},\; i \in \lbrace 2, 3, 4 \rbrace$, this step shows $\|\delta v_1\|_2 \leq \cfrac{\varepsilon}{4}$.

	\textrm{---} \textit{Step 7}: Wrapping up.

	We now work under the conditions $(H_i),\; i \in \lbrace 1, 2, 3, 4 \rbrace$.
	By Step 1, $\|y_k^* - v_k\|_2 \leq\|\delta v^k_1\|_2 + \|\delta v^k_2\|_2 + \|\delta v^k_3\|_2 + \|\delta v^k_4\|_2$ (we restore the omitted $k$ indices).
	By Step 3, with probability exceeding $1 - \eta/(3\npoints)$, we have $\|\delta v^k_2\|_2 \leq \cfrac{\varepsilon}{4}$,
	thus with probability $1 - \eta/3$ we have $\forall k \in \llbracket 1, \npoints \rrbracket, \; \|\delta v^k_2\|_2 \leq \cfrac{\varepsilon}{4}$.
	By Step 4, we have almost-surely $\forall k \in \llbracket 1, \npoints \rrbracket, \;\|\delta v_3^k\|_2 \leq \cfrac{\varepsilon}{4}$.
	By Step 5, with probability $1 - \eta/3,\; \|\delta A\|_{\mathrm{op}} \leq s$.

	Putting this together yields that with probability $1 - \eta$, we have:
	$$\forall k \in \llbracket 1, \npoints \rrbracket,\; \|\delta v^k_2\|_2 \leq \cfrac{\varepsilon}{4},\; \|\delta v_3^k\|_2 \leq \cfrac{\varepsilon}{4},\; \|\delta v_4^k\|_2 \leq \cfrac{\varepsilon}{4}\ \mathrm{and}\ \|\delta A\|_{\mathrm{op}} \leq s.$$
	Finally, Step 5 shows that conditionally to the events above, $\|\delta v_1^k\|_2 \leq \cfrac{\varepsilon}{4}$ almost-surely.
	Thus with probability exceeding $1 - \eta,\; \forall k \in \llbracket 1, \npoints \rrbracket, \|y_k^* - v_k\|_2 \leq \varepsilon.$
	Since $v_k = \Sum{l=1}{\npoints}S_{k,l}z_l$, with probability over $1 - \eta:\; \forall k \in \llbracket 1, \npoints \rrbracket,\; \left\|y_k^* - \Sum{l=1}{\npoints}S_{k,l}z_l\right\|_{2} \leq \varepsilon. $
\end{proof}

In order to get the summarised result from~\ref{sec:closeness}, we simplify the conditions as follows.
\begin{corollary}[Simplified conditions for~\ref{thm:ystar_concentration}]\label{cor:simplified_condition_concentration}


	With the notations of~\ref{thm:ystar_concentration}, the condition:
	\begin{equation}\label{eqn:simplified_condition_concentration}
		(H_p): \quad p \geq \left(\cfrac{4096 d^3\npoints\oll{c}_Z^3 \log(3d\npoints^2/\eta)}{\varepsilon^3}\right) \vee \left(\cfrac{697d^2\npoints^2\oll{c}_Z^2\log\left(3d/\eta\right)}{\varepsilon^2}\right) \vee \left(\cfrac{8d^2\npoints^2\oll{c}_Z^2\log(6\npoints^2/\eta)}{\varepsilon^2}\right)
	\end{equation}
	implies $(H_q)$ and $(H_i)_{i \in \lbrace 1, 2, 3, 4 \rbrace}$, and thus is sufficient in order to have~\ref{eqn:cv_fixed_point_distance}.
\end{corollary}

\begin{proof}
	The second and third terms of~\ref{eqn:simplified_condition_concentration} correspond to $(H_1)$ and $(H_4)$ respectively.
	Then, using $\npoints^+, \npoints^- \leq \npoints$, we have $(H_2) \Longleftarrow \oll{q} \geq \cfrac{512d^2\oll{c}_Z^2\log(3d\npoints^2\eta)}{\varepsilon^2}$ and $(H_3) \Longleftarrow \ull{q} \leq \cfrac{\varepsilon}{8d\npoints\oll{c}_Z}\ p$.
	Let $q := \cfrac{512d^2\oll{c}_Z^2\log(3d\npoints^2/\eta)}{\varepsilon^2}$; $\oll{q} = \ull{q} = q$. $(H_q)$ and $(H_2)$ are automatically satisfied by this choice.
	For $q$ to satisfy $(H_3)$, it is sufficient to have $\cfrac{512d^2\oll{c}_Z^2\log(3d\npoints^2/\eta)}{\varepsilon^2} \leq \cfrac{\varepsilon}{8d\npoints\oll{c}_Z}\ p$, i.e. $p \geq \cfrac{4096 d^3\npoints\oll{c}_Z^3\log(3d\npoints^2/\eta)}{\varepsilon^3}$
\end{proof}
