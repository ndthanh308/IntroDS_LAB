\section{Stochastic Gradient Descent with \texorpdfstring{$\SW$}{SW} as Loss}\label{sec:SGD}

Training Sliced-Wasserstein generative models consists in training a neural network 
$$T: \app{\R^{\dimp}\times \R^{\dimx}}{\R^{\dimy}}{(\p, x)}{T_\p(x) := T(\p, x)}$$
by minimising $\p \longmapsto\SW_2^2(T_u\#\mx, \my)$ through Stochastic Gradient Descent (as described in \ref{alg:SGD}). The probability distribution $\mx \in \mathcal{P}_2(\R^{\dimx})$ is the law of the input of the generator $T(\p,\cdot)$. The distribution $\my \in \mathcal{P}_2(\R^{\dimy})$ is the data distribution, which $T$ aims to simulate. Finally, $\bbsigma$ will denote the uniform measure on the unit sphere of $\R^{\dimy}$, denoted by $\SS^{\dimy-1}$. Given a list of points $X = (x_1, \cdots, x_\npoints) \in \R^{\npoints \times \dimx},$ denote the associated discrete uniform measure $\gamma_X := \frac{1}{\npoints}\sum_i\delta_{x_i}$. By abuse of notation, we write $T_\p(X) := (T_\p(x_1), \cdots, T_\p(x_\npoints)) \in \R^{\npoints \times \dimy}$.
% Figure environment removed

In the following, we will apply results from \citep{bianchi2022convergence}, and we pave the way to the application of these results by presenting their theoretical framework. Consider a sample loss function $f: \R^{\dimp} \times \Xi \longrightarrow \R$ that is locally Lipschitz in the first variable, and $\zeta$ a probability measure on $\Xi \subset \R^d$ which is the law of the samples drawn at each SGD iteration. Consider $\varphi: \R^\dimp \times \Xi \longrightarrow \R^\dimp$ an \textit{almost-everywhere gradient} of $f$, which is to say that for almost every $(\p, S) \in \R^{\dimp}\times \Xi,\; \varphi(\p, S) = \partial_\p f(\p, S)$ (since each $f(\cdot, S)$ is locally Lipschitz, it is differentiable almost-everywhere by Rademacher's theorem). The complete loss function is $F := \p \longrightarrow \int_\Xi f(\p, S)\dd\zeta(S)$. An SGD trajectory of step $\lr > 0$ for $F$ is a sequence $(\p^{(t)}) \in (\R^\dimp)^\N$ of the form:
$$\p^{(t+1)} = \p^{(t)} - \lr \varphi(\p^{(t)}, S^{(t+1)}),\quad \left(\p^{(0)}, (S^{(t)})_{t \in \N}\right) \sim \mp \otimes \zeta^{\otimes \N}, $$
where $\mp$ is the distribution of the initial position $\p^{(0)}$. Within this framework, we define an SGD scheme described by \ref{alg:SGD}, with $\zeta := \mxn \otimes \myn \otimes \bbsigma$ and
$$f:= \app{\R^{\dimp} \times \R^{\npoints \times \dimx} \times \R^{\npoints \times \dimy} \times \SS^{\dimy-1}}{\R^\dimy}{(\p, X, Y, \theta)}{\W_2^2(P_{\theta}\#T_\p\#\gamma_{X}, P_{\theta}\#\gamma_{Y})} .$$
With this definition for $f$, we have $F(\p) = \mathbb{E}_{(X, Y, \theta) \sim \zeta}\left[\W_2^2(P_\theta\#T_\p\#\gamma_X, P_\theta\#\gamma_Y)\right] = \SW_2^2(T_\p\#\mx, \my)$: the complete loss compares the data $\my$ with the model's generation $T_\p\#\mx$ using SW. We now wish to define an almost-everywhere gradient of $f$. To this end, notice that one may write $f(\p, X, Y, \theta) = w_\theta(T(\p, X), Y)$, where for $Z, Y \in \R^{\npoints \times \dimy}$ and $\theta \in \SS^{\dimy-1},\; w_\theta(Y, Z) := \W_2^2(P_\theta\#\gamma_Z, P_\theta\#\gamma_Y)$. The differentiability properties of $w_\theta(\cdot, Y)$ are already known \citep{discrete_sliced_loss, bonneel2015sliced}, in particular one has the following almost-everywhere gradient of $w_\theta(\cdot, Y):$
$$\dr{Z}{}{w_\theta}(Z, Y) = \left(\cfrac{2}{\npoints}\theta\theta^T(z_k - y_{\sigma_\theta^{Z, Y}(k)})\right)_{k \in \llbracket 1, \npoints \rrbracket} \in \R^{\npoints \times \dimy},$$
where the permutation $\sigma_\theta^{Z, Y} \in \mathfrak{S}_\npoints$ is $\sort{Y}{\theta} \circ (\sort{Z}{\theta})^{-1}$, with $\sort{Y}{\theta}\in \mathfrak{S}_\npoints$ being a sorting permutation of the list $(\theta \cdot y_1, \cdots, \theta \cdot y_\npoints)$. The sorting permutations are chosen arbitrarily when there is ambiguity. To define an almost-everywhere gradient, we must differentiate $f(\cdot, X, Y, \theta) = \p \longmapsto w_\theta(T(\p, X), Y)$ for which we need regularity assumptions on $T$: this is the goal of \ref{ass:C2_ae}. In the following, $\oll{A}$ denotes the topological closure of a set $A$, $\partial A$ its boundary, and $\lambda_{\R^\dimp}$ denotes the Lebesgue measure of $\R^\dimp$.

\begin{assumption}\label{ass:C2_ae}
	For every $x \in \R^{\dimx},\;$ there exists a family of disjoint connected open sets $(\U_j(x))_{j \in J(x)}$ such that $\forall j \in J(x),\; T(\cdot, x) \in \mathcal{C}^2(\U_j(x), \R^\dimy)$, $\Reu{j\in J(x)}{}\oll{\U_j(x)} = \R^\dimp$ and $\lambda_{\R^\dimp}\Big(\Reu{j\in J(x)}{}\partial \U_j(x)\Big) = 0$.
\end{assumption}

Note that for measure-theoretic reasons, the sets $J(x)$ are assumed countable.

%

\ref{ass:C2_ae} implies that given $X, Y, \theta$ fixed, $f(\cdot, X, Y, \theta)$ is differentiable almost-everywhere, and that one may define the following almost-everywhere gradient \ref{eqn:ae_grad}.
\begin{equation}\label{eqn:ae_grad}
	\varphi := \app{\R^{\dimp} \times \R^{\npoints \times \dimx} \times \R^{\npoints \times \dimy} \times \SS^{\dimy-1}}{\R^{\dimp}}{(\p, X, Y, \theta)}{\Sum{k=1}{\npoints} \cfrac{2}{\npoints}\left(\dr{\p}{}{T}(\p, x_k)\right)^T \theta \theta^T(T(\p, x_k) - y_{\sigma_\theta^{T(\p, X), Y}(k)})},
\end{equation}
where for $x \in \R^\dimx,\; \dr{\p}{}{T}(\p, x)\in \R^{\dimy \times \dimp}$ denotes the matrix of the differential of $\p \longmapsto T(\p, x)$, which is defined for almost-every $\p$. Given $\p \in \partial \U_j(x)$ (a point of potential non-differentiability), take instead $0$. (Any choice at such points would still define an a.e. gradient, and will make no difference).

Given a step $\lr > 0$, and an initial position $\p^{(0)} \sim \mp$, we may now define formally the following fixed-step SGD scheme for $F$:
\begin{equation}\label{eqn:SW_SGD}
	\begin{split}
		\p^{(t+1)} = \p^{(t)} - \lr \varphi(\p^{(t)}, X^{(t+1)}, Y^{(t+1)}, \theta^{(t+1)}), \\ \left(\p^{(0)}, (X^{(t)})_{t \in \N}\ (Y^{(t)})_{t \in \N}\ (\theta^{(t)})_{t \in \N}\right) \sim \mp \otimes \mxN \otimes \myN \otimes \bbsigma^{\otimes \N}. 
	\end{split}
\end{equation}
An important technicality that we must verify in order to apply \citet{bianchi2022convergence}'s results is that $\p \longmapsto f(\p, X, Y, \theta)$ and $F$ are locally Lipschitz. Before proving those claims, we reproduce a useful Property from \citep{discrete_sliced_loss}. In the following, $\|X\|_{\infty, 2}$ denotes $\underset{k \in \llbracket 1, \npoints \rrbracket}{\max}\ \|x_k\|_2$ given $X = (x_1, \cdots, x_\npoints) \in \R^{\npoints \times \dimx}$, and $B_{\mathcal{N}}(x, r)$ for $\mathcal{N}$ a norm on $\R^\dimx$, $x \in \R^\dimx$ and $r>0$ shall denote the open ball of $\R^\dimx$ of centre $x$ and radius $r$ for the norm $\mathcal{N}$ (if $\mathcal{N}$ is omitted, then $B$ is an euclidean ball).

\begin{prop}The $(w_\theta(\cdot, Y))_{\theta \in \SS^{\dimy-1}}$ are uniformly locally Lipschitz \citep{discrete_sliced_loss}.\label{prop:w_unif_locLip}\
	
	Let $\kappa_r(Z, Y) := 2\npoints(r + \|Z\|_{\infty, 2} + \|Y\|_{\infty, 2})$, for $Z, Y \in \R^{\npoints \times \dimy}$ and $r>0$. Then $w_\theta(\cdot, Y)$ is $\kappa_r(Z, Y)$-Lipschitz in the neighbourhood $B_{\|\cdot\|_{\infty, 2}}(Z, r)$:
	$$\forall Y', Y'' \in B_{\|\cdot\|_{\infty, 2}}(Z, r),\; \forall \theta \in \SS^{\dimy-1},\; |w_\theta(Y', Y) - w_\theta(Y'', Y)| \leq \kappa_r(Z, Y) \|Y'-Y''\|_{\infty, 2}.$$
	
\end{prop}

In order to deduce regularity results on $f$ and $F$ from \ref{prop:w_unif_locLip}, we will make the following assumption, which under \ref{ass:C2_ae} only requires additional regularity with respect to the data argument.

\begin{assumption}\label{ass:T_loclip}
	For any compacts $\mathcal{K}_1 \subset \R^\dimp$ and $\mathcal{K}_2 \subset \R^\dimx,\;$ there exists $\lipT_{\mathcal{K}_1, \mathcal{K}_2}>0$ such that $\forall (\p_1, \p_2, x_1, x_2) \in \mathcal{K}_1^2 \times \mathcal{K}_2^2,\; \|T(\p_1, x_1) - T(\p_2, x_2)\| \leq \lipT_{\mathcal{K}_1, \mathcal{K}_2}\left(\|\p_1-\p_2\| + \|x_1 - x_2\|\right)$. 
\end{assumption}

\begin{prop}[Regularity of $\p \longmapsto f(\p, X, Y, \theta)$]\label{prop:f_loclip} Under \ref{ass:T_loclip}, for $\varepsilon > 0,\; \p_0 \in \R^{\dimp},\; X\in \R^{\npoints \times \dimx},\; Y\in \R^{\npoints \times \dimy}$ and $\theta \in \SS^{\dimy-1}$, let $\kappa_\varepsilon(\p_0, X, Y) := 2\lipT\npoints(\varepsilon \lipT + \|T(\p_0, X)\|_{\infty, 2} + \|Y\|_{\infty, 2})$, with $\lipT := \lipT_{\oll{B}(u_0, \varepsilon), \oll{B}(0_{\R^\dimx}, \|X\|_{\infty, 2})}$. Then $f(\cdot, X, Y, \theta)$ is $\kappa_\varepsilon(X, Y)$-Lipschitz in $B(\p_0, \varepsilon)$: 
	$$\forall \p, \p' \in B(\p_0, \varepsilon),\; |f(\p, X, Y, \theta) - f(\p', X, Y, \theta)| \leq \kappa_\varepsilon(X, Y)\|u-u'\|_2.$$
	
\end{prop}

\begin{proof}
	Let $\varepsilon > 0,\; \p_0 \in \R^{\dimp},\; X\in \R^{\npoints \times \dimx},\; Y\in \R^{\npoints \times \dimy}$ and $\theta \in \SS^{\dimy-1}$. Let $\p, \p' \in B(\p_0, \varepsilon)$. Using \ref{ass:T_loclip}, we have $T(\p, X), T(\p', X) \in B_{\|\cdot\|_{\infty, 2}}(T(\p_0, X), r)$, with $r := \varepsilon\lipT_{\oll{B}(u_0, \varepsilon), \oll{B}(0_{\R^\dimx}, \|X\|_{\infty, 2})}$.	
	
	By \ref{prop:w_unif_locLip}, we have, with $\lipT := \lipT_{\oll{B}(u_0, \varepsilon), \oll{B}(0_{\R^\dimx}, \|X\|_{\infty, 2})}$
	\begin{align*}|f(\p, X, Y, \theta) - f(\p', X, Y, \theta)| &= |w_\theta(T(\p, X), Y) - w_\theta(T(\p', X), Y)|\\
		&\leq \kappa_r(T(\p_0, X), Y) \|T(\p, X) - T(\p', X)\|_{\infty, 2} \\
		&\leq 2\npoints(\varepsilon \lipT + \|T(\p_0, X)\|_{\infty, 2} + \|Y\|_{\infty, 2}) \lipT\|u-u'\|_2.
	\end{align*}
	\vspace{-20pt}
\end{proof}

\ref{prop:f_loclip} shows that $f$ is locally Lipschitz in $\p$. We now assume some conditions on the measures $\mx$ and $\my$ in order to prove that $F$ is also locally Lipschitz.

\begin{assumption}\label{ass:mx_my}
	$\mx$ and $\my$ are Radon probability measures on $\R^\dimx$ and $\R^\dimy$ respectively, supported by the compacts $\X$ and $\Y$ respectively. Denote $R_x := \underset{x \in \X}{\sup}\ \|x\|_2$ and $R_y := \underset{y \in \Y}{\sup}\ \|y\|_2$.
\end{assumption}

\begin{prop}\label{prop:F_loclip}
	Assume \ref{ass:T_loclip} and \ref{ass:mx_my}. For $\varepsilon > 0,\; \p_0 \in \R^{\dimp},$ let $C_1(\p_0) := \Int{\Xn}{}\|T(\p_0, X)\|_{\infty, 2} \dd\mxn(X),\; C_2 := \Int{\Yn}{}\|Y\|_{\infty, 2}\dd \myn(Y)$ and $\lipT := \lipT_{\oll{B}(u_0, \varepsilon), \oll{B}(0, R_x)}$.
	
	Let $\kappa_\varepsilon(\p_0) := 2\lipT\npoints(\varepsilon \lipT + C_1(\p_0) + C_2)$. We have $\forall \p, \p' \in B(\p_0, \varepsilon),\; |F(\p) - F(\p')| \leq \kappa_\varepsilon(\p_0)\|\p-\p'\|_2$.
\end{prop}

\begin{proof}
	Let $\varepsilon > 0,\; \p_0 \in \R^{\dimp} \p, \p' \in B(\p_0, \varepsilon)$. First, notice that for any $X \in \Xn,\; \|X\|_{\infty, 2} \leq R_x$, thus $\lipT_{\oll{B}(u_0, \varepsilon), \oll{B}(0_{\R^\dimx}, \|X\|_{\infty, 2})} \leq \lipT_{\oll{B}(u_0, \varepsilon), \oll{B}(0, R_x)} =: \lipT$. We have	
	\begin{align*} |F(\p) - F(\p')| &\leq \Int{\Xn \times \Yn \times \SS^{\dimy-1}}{}|f(\p, X, Y, \theta) - f(\p)|\dd \mxn(X) \dd \myn(Y) \dd\bbsigma(\theta) \\
		&\leq \Int{\Xn \times \Yn}{}\kappa_\varepsilon(\p_0, X, Y)\|\p-\p'\|_2\dd \mxn(X) \dd \myn(Y)\\
		&\leq \Int{\Xn \times \Yn}{}2\lipT\npoints(\varepsilon \lipT + \|T(\p_0, X)\|_{\infty, 2} + \|Y\|_{\infty, 2})\|\p-\p'\|_2\dd \mxn(X) \dd \myn(Y).
	\end{align*}
	Now by \ref{ass:T_loclip}, $X \longmapsto \|T(\p_0, X)\|_{\infty, 2}$ is continuous on the compact $\Xn$, allowing the definition of the constant $C_1(\p_0) := \Int{\Xn}{}\|T(\p_0, X)\|_{\infty, 2} \dd\mxn(X)$ ($\mx$ is a Radon probability measure by \ref{ass:mx_my}, thus $C_1$ is finite.). Likewise, let $C_2 := \Int{\Yn}{}\|Y\|_{\infty, 2}\dd \myn(Y) <+\infty$. 
	
	Finally, $|F(\p) - F(\p')| \leq 2\lipT\npoints(\varepsilon \lipT + C_1(\p_0) + C_2)\|\p-\p'\|_2$.
\end{proof}

Having shown that our losses are locally Lipschitz, we can now turn to convergence results. These conclusions are placed in the context of non-smooth and non-convex optimisation, thus will be tied to the Clarke sub-differential of $F$, which we denote $\partial_C F$. The set of Clarke sub-gradients at a points $\p$ is the convex hull of the limits of gradients of $F$:
$$\partial_C F(\p) := \mathrm{conv}\left\{v \in \R^\dimp:\; \exists (\p_i) \in (\mathcal{D}_F)^\N: \p_i \xrightarrow[i \longrightarrow +\infty]{} \p\ \mathrm{and} \; \nabla F(\p_i) \xrightarrow[i \longrightarrow +\infty]{} v\right\},$$
where $\mathcal{D}_F$ is the set of differentiability of $F$. At points $\p$ where $F$ is differentiable, $\partial_CF(\p) = \{\nabla F(\p)\}$, and if $F$ is convex in a neighbourhood of $\p$, then the Clarke differential at $\p$ is the set of its convex sub-gradients.

\section{Convergence of Interpolated SGD Trajectories on \texorpdfstring{$F$}{F}}\label{sec:interpolated_SGD}

In general, the idea behind SGD is a discretisation of the gradient flow equation $\dot{u}(s) = -\nabla F(u(s))$. In our non-smooth setting, the underlying continuous-time problem is is instead the Clarke differential inclusion $\dot{u}(s) \in -\partial_C F(u(s))$. Our objective is to show that in a certain sense, the SGD trajectories approach the set of solutions of this inclusion problem, as the step size decreases. We consider solutions that are absolutely continuous (we will write $\p(\cdot) \in \mathcal{C}_{\mathrm{abs}}(\R_+, \R^\dimp)$) and start within $\mathcal{K} \subset \R^\dimp$, a fixed compact set. We can now define the solution set formally as
\begin{equation}\label{eqn:S}
	S_{-\partial_C F}(\mathcal{K}) := \left\lbrace \p \in \mathcal{C}_{\mathrm{abs}}(\R_+, \R^\dimp)\ |\  \ull{\forall} s\in \R_+,\; \dot{\p}(s) \in -\partial_C F(\p(s));\; \p(0) \in \mathcal{K} \right\rbrace,
\end{equation}
where we write $\ull{\forall}$ for "almost every". In order to compare the discrete SGD trajectories to this set continuous-time trajectories, we interpolate the discrete points in an affine manner: Equation \ref{eqn:interpolation} defines the \textit{piecewise-affine interpolated SGD trajectory} associated to an SGD trajectory $(\p_\lr^{(t)})_{t \in \N}$ of learning rate $\lr$.
\begin{equation}\label{eqn:interpolation}
	\p_\lr(s) = \p^{(t)}_\lr + \left(\frac{s}{\lr} - t\right)(\p_\lr^{(t+1)} - \p^{(t)}_\lr),\quad \forall s \in [t\lr, (t+1)\lr[, \quad \forall t \in \N.
\end{equation}
In order to compare our interpolated trajectories with the solutions, we consider the metric of uniform convergence on all segments
\begin{equation}\label{eqn:d_c}
	d_c(\p, \p') := \Sum{k \in \N^*}{}\cfrac{1}{2^k}\min\left(1, \underset{s \in [0, k]}{\max}\|\p(s) - \p'(s)\|_{\infty, 2}\right).
\end{equation}
In order to prove that the interpolated trajectories, we will leverage the results of \citet{bianchi2022convergence} which hinge on three conditions on the loss $F$ that we reproduce and verify successively.

\begin{condition}\label{cond:A1}\
	\begin{itemize}
		\item[i)] There exists $\kappa: \R^\dimp \times \Xi \longrightarrow \R_+$ measurable such that each $\kappa(\p, \cdot)$ is $\zeta$-integrable, and:		
		$$\exists \varepsilon > 0,\; \forall \p, \p' \in B(\p_0, \varepsilon),\; \forall S \in \Xi,\; |f(\p, S) - f(\p', S)|\leq \kappa(\p_0, S)\|\p-\p'\|_2. $$		
		\item[ii)] There exists $\p \in \R^\dimp$ such that $f(\p, \cdot)$ is $\zeta$-integrable.
	\end{itemize}
\end{condition}

Our regularity result on $f$ \ref{prop:f_loclip} allows us to verify \ref{cond:A1}, by letting $\varepsilon := 1$ and $\kappa(\p_0, S) := \kappa_1(\p_0, X, Y, \theta)$. \ref{cond:A1} ii) is immediate since for \textit{all} $\p \in \R^\dimp,\; (X, Y, \theta) \longmapsto w_\theta(T(\p, X), Y)$ is continuous in each variable separately, thanks to the regularity of $T$ provided by \ref{ass:T_loclip}, and to the regularities of $w$ (as implied by \citep{discrete_sliced_loss}, Lemma 2.2.2, for instance). This continuity implies that all $f(\p, \cdot)$ are $\zeta$-integrable, since $\zeta = \mxn \otimes \myn \otimes \bbsigma$ is a compactly supported Radon measure under \ref{ass:mx_my}.

\begin{condition}\label{cond:A2} The function $\kappa$ of \ref{cond:A1} verifies:	
	\begin{itemize}
		\item[i)] There exists $c \geq 0$ such that $\forall \p\in \R^\dimp,\; \Int{\Xi}{}\kappa(\p, S)\dd\zeta(S) \leq c(1+\|\p\|_2)$.		
		\item[ii)] For every compact $\mathcal{K} \subset \R^\dimp,\; \underset{\p \in \mathcal{K}}{\sup}\ \Int{\Xi}{}\kappa(\p, S)^2\dd\zeta(S) <+\infty$.
	\end{itemize}	
\end{condition}

\ref{cond:A2}.ii) is verified by $\kappa$ given its regularity. However, \ref{cond:A2}.i) requires that $T(\p, x)$ increase slowly as $\|\p\|$ increases, which is more costly.

\begin{assumption}\label{ass:T_slow_increase}
	There exists an $\mx$-integrable function $g: \R^\dimx \longrightarrow\R_+$ such that $\forall \p \in \R^\dimp,\; \forall x \in \R^\dimx,\; \|T(\p, x)\| \leq g(x)(1 + \|\p\|_2)$.
\end{assumption}

\ref{ass:T_slow_increase} is satisfied in particular as soon as $T(\cdot, x)$ is bounded (which is the case for a neural network with bounded activation functions), or if $T$ is of the form $T(\p, x) = \widetilde{T}(\p, x) \mathbbold{1}_{B(0, R)}(\p)$, i.e. limiting the network parameters $\p$ to be bounded. This second case does not yield substantial restrictions in practice, yet vastly simplifies theory. 

Under \ref{ass:T_slow_increase}, we have for any $\p \in \R^\dimp,$ with $\kappa$ from \ref{prop:f_loclip} and $C_2$ from \ref{prop:F_loclip},
\begin{align*}\Int{\Xn \times \Yn \times \SS^{\dimy-1}}{}\kappa_1(\p, X, Y, \theta)\dd\mxn(X) \dd\myn(Y) \dd\bbsigma(\theta) &\leq 2\lipT\npoints\left(\varepsilon \lipT + (1+\|\p\|_2)\Int{\Xn}{}\underset{k \in \llbracket 1, \npoints \rrbracket}{\max}\ g(x_k) \dd\mxn(X) + C_2\right)\\
	&\leq c(1 + \|\p\|_2).
\end{align*}
As a consequence, \ref{cond:A2} holds under our assumptions. We now consider the Markov kernel associated to the SGD schemes:
$$P_\lr : \app{\R^\dimp \times \mathcal{B}(\R^\dimp)}{[0, 1]}{\p, B}{\Int{\Xi}{}\mathbbold{1}_B(\p - \lr \varphi(\p, S))\dd \zeta(S)}.$$
With $\lambda_{\R^\dimp}$ denoting the Lebesgue measure on $\R^\dimp$, let $\Gamma := \left\lbrace \lr \in\ ]0, +\infty[\ |\ \forall \rho \ll \lambda_{\R^\dimp},\ \rho P_\lr \ll \lambda_{\R^\dimp}\right\rbrace$. We will verify the following condition:
\begin{condition}\label{cond:A3} The closure of $\Gamma$ contains 0.
\end{condition}

In order to satisfy \ref{cond:A3}, we require an additional regularity condition on the neural network $T$ which we formulate in \ref{ass:d2T2_and_dT_bounded}.

\begin{assumption}\label{ass:d2T2_and_dT_bounded}
	There exists a constant $\MddT>0$, such that  (with the notations of \ref{ass:C2_ae} and \ref{ass:mx_my}) $\forall x \in \X,\; \forall j \in J(x),\; \forall \p \in \U_j(x),\; \forall (i_1, i_2, i_3, i_4) \in \llbracket 1, \dimp\rrbracket^2\times \llbracket 1, \dimy \rrbracket^2,$	
	$$ \left|\cfrac{\partial^2}{\partial \p_{i_1} \partial \p_{i_2}} \Big([T(\p, x)]_{i_3} [T(\p, x)]_{i_4}\Big)\right| \leq \MddT,\; \mathrm{and}\;\left\|\cfrac{\partial^2 T}{\partial \p_{i_1}\partial\p_{i_2}}(\p, x)\right\|_2 \leq \MddT.$$
\end{assumption}

The upper bounds in assumption bear strong consequences on the behaviour of $T$ for $\|u\|_2 \gg 1$, and are only practical for networks of the form $T(\p, x) = \widetilde{T}(\p, x) \mathbbold{1}_{B(0,R)}(u)$, similarly to \ref{ass:T_slow_increase}.

\begin{prop}\label{prop:SW_Gamma} Under \ref{ass:C2_ae}, \ref{ass:mx_my} and \ref{ass:d2T2_and_dT_bounded}, for the SGD trajectories \ref{eqn:SW_SGD}, $\Gamma\ \supset\ ]0, \lr_0[$, where $\lr_0 := \cfrac{1}{(\dimy^2 + 2R_y )\dimp\MddT}$.
\end{prop}

\begin{proof}
	 Let $\rho \ll \lambda$ and $B \in \mathcal{B}(\R^\dimy)$ such that $\lambda(B) = 0$. We have, with $\lr' := 2\lr/\npoints,\;S := (X, Y, \theta),\; \zeta := \mxn \otimes \myn \otimes \bbsigma$ and $\Xi := \Xn \times \Yn \times \SS^{\dimy-1},$	
	\begin{align*}\rho P_\lr(B) &= \Int{\R^\dimp\times \Xi}{}\mathbbold{1}_B\left[\p -\lr'\Sum{k=1}{\npoints}\left(\dr{\p}{}{T}(\p, x_k)\right)^T \theta \theta^T(T(\p, x_k) - y_{\sigma_\theta^{T(\p, X), Y}(k)})\right]\dd\rho(\p)\dd\zeta(S)
		&\leq \Sum{\tau \in \mathfrak{S}_\npoints}{}\Int{\Xi}{}I_\tau(S)\dd\zeta(S),
	\end{align*}	
	where $I_\tau(S) := \Int{\R^\dimp}{}\mathbbold{1}_B\left(\phi_{\tau,S}(\p)\right)\dd\rho(\p)$, with $\phi_{\tau,S} := \p - \lr'\underbrace{\Sum{k=1}{\npoints}\left(\dr{\p}{}{T}(\p, x_k)\right)^T \theta \theta^T(T(\p, x_k) - y_{\tau(k)})}_{\psi_{\tau,S} := }$.
	
	Let $\tau \in \mathfrak{S}_\npoints$ and $(X, Y, \theta) \in \Xi$. Using \ref{ass:C2_ae}, separate $I_\tau(S) = \Sum{j\in J}{}\Int{\U_j(X)}{}\mathbbold{1}_B\left(\p - \psi_{\tau,S}(\p)\right)\dd\rho(\p),$ where the differentiability structure $(\U_j(X))_{j \in J(X)}$ is obtained using the respective differentiability structures: for each $k \in \llbracket 1, \npoints \rrbracket$, \ref{ass:C2_ae} yields a structure $(\U_{j_k}(x_k))_{j_k \in J_k(x_k)}$ of $\p \longmapsto T(\p, x_k)$, which depends on $x_k$, hence the $k$ indices.
	
	To be precise, define for $j = (j_1, \cdots, j_\npoints) \in J_1(x_1) \times \cdots \times J_\npoints(x_\npoints),\; \U_j(X) := \Inter{k=1}{\npoints}\U_{j_k}(x_k)$, and $J(X) := \left\lbrace (j_1, \cdots, j_\npoints) \in J_1(x_1) \times \cdots \times J_\npoints(x_\npoints)\ |\ \U_j(X) \neq \varnothing \right\rbrace$. In particular, for any $k \in \llbracket 1, \npoints \rrbracket,\; T(\cdot, x_k)$ is $\mathcal{C}^2$ on $\U_j(X)$. Notice that the derivatives are not necessarily defined on the border $\partial \U_j(X)$, which is of Lebesgue measure 0 by \ref{ass:C2_ae}, thus the values of the derivatives on the border do not change the value of the integrals (the integrals may have the value $+\infty$, depending on the behaviour of $\phi_{\tau, s}$, but we shall see that they are all finite when $\lr$ is small enough).
	
	We drop the $S, \tau$ index in the notation, and focus on the properties of $\phi$ and $\psi$ as functions of $\p$. Our first objective is to determine a constant $K>0$, independent of $\p, S, \tau$, such that $\psi$ is $K$-Lipschitz on $\U_j(X)$. 
	
	First, let $\chi := \p \in \U_j(X) \longmapsto \left(\dr{\p}{}{T}(\p, x_k)\right)^T \theta \theta^T T(\p, x_k)$. $\chi$ is of class $\mathcal{C}^1$, therefore we determine its Lipschitz constant by upper-bounding the $\|\cdot\|_2$-induced operator norm of its differential, denoted by $\nt{\cfrac{\partial\chi}{\partial\p}(\p)}_2$. Notice that $\chi(\p) = \cfrac{1}{2}\cfrac{\partial}{\partial \p}\left(\theta \cdot T(\p, x_k)\right)^2$.
	
	Now $\nt{\cfrac{\partial^2}{\partial\p^2}\left(\theta \cdot T(\p, x_k)\right)^2}_2 \leq \dimp\underset{(i_1, i_2) \in \llbracket 1, \dimp \rrbracket^2}{\max}\ \left|\cfrac{\partial^2}{\partial\p_{i_1}\partial\p_{i_2}}\left(\theta \cdot T(\p, x_k)\right)^2\right|$, with by \ref{ass:d2T2_and_dT_bounded},	
	$$\left|\cfrac{\partial^2}{\partial\p_{i_1}\partial\p_{i_2}}\left(\theta \cdot T(\p, x_k)\right)^2\right| \leq \Sum{(i_3, i_4) \in \llbracket 1, \dimy \rrbracket^2}{}\left|\theta_{i_3}\theta_{i_4}\cfrac{\partial^2 }{\partial\p_{i_1}\partial\p_{i_2}}\Big([T(\p, x_k)]_{i_3}[T(\p, x_k)]_{i_4}\Big)\right| \leq \dimy^2 \MddT.$$	
	We obtain that $\chi$ is $\frac{1}{2}\dimp\dimy^2\MddT$-Lipschitz. 
	
	Second, let $\omega:  \p \in \U_j(X) \longmapsto \left(\dr{\p}{}{T}(\p, x_k)\right)^T \theta \theta^Ty_{\tau(k)}$, also of class $\mathcal{C}^1$. We re-write $\left[\cfrac{\partial \omega}{\partial\p}(\p)\right]_{i_1, i_2} = y_{\tau(k)}^T\theta\theta^T\cfrac{\partial^2 T}{\partial \p_{i_1}\partial\p_{i_2}}(\p, x_k)$, and conclude similarly by \ref{ass:d2T2_and_dT_bounded} that $\omega$ is $\|y_{\tau(k)}\|_2\dimp \MddT$-Lipschitz.
	
	Finally, $\psi = \Sum{k=1}{\npoints}(\chi_k - \omega_k)$, and is therefore $K := (\frac{1}{2}\dimy^2 + R_y )\dimp\npoints\MddT$-Lipschitz, with $R_y$ from \ref{ass:mx_my}. We have proven that $\nt{\cfrac{\partial \psi}{\partial \p}(\p)}_2 \leq K$ for any $\p \in \U_j(X)$, and that $K$ does not depend on $X, Y, \theta, j$ or $\p$.
	
	We now suppose that $\lr' < \frac{1}{K}$, which is to say $\lr < \frac{\npoints}{2K}$. Under this condition, $\phi: \U_j(X) \longrightarrow \R^\dimp$ is injective. Indeed, if $\phi(\p_1) = \phi(\p_2)$, then $\|\p_1-\p_2\|_2 = \lr'\|\psi(\p_1)-\psi(\p_2)\|_2 \leq \lr'K\|\p_1-\p_2\|_2$, thus $\p_1 = \p_2$. Furthermore, for any $\p \in \U_j(X),\; \cfrac{\partial \phi}{\partial \p}(\p) = \mathrm{Id}_{\R^\dimp} - \lr'\cfrac{\partial \psi}{\partial \p}(\p)$, with $\nt{\lr'\cfrac{\partial \psi}{\partial \p}(\p)}_2 < 1$, thus the matrix $\cfrac{\partial \phi}{\partial \p}(\p)$ is invertible (using the Neumann series method). By the global inverse function theorem, $\phi: \U_j(X) \longrightarrow \phi(\U_j(X))$ is a $\mathcal{C}^1$-diffeomorphism.
	
	Re-writing $\Int{\U_j(X)}{}\mathbbold{1}_B(\phi(\p))\dd\rho(\p) = \phi\#\rho(B)$, we have now shown that $\phi$ is a $\mathcal{C}^1$-diffeomorphism, thus since $\rho \ll \lambda$, $\phi\#\rho \ll \lambda$. It then follows that the integral is 0, then $I_\tau(S) = 0$ and finally $\rho P_\lr(B) = 0$.
\end{proof}

Now that we have verified \ref{cond:A1}, \ref{cond:A2} and \ref{cond:A3}, we can apply \citep{bianchi2022convergence}, Theorem 2 to $F$. Let $\lr_1 < \lr_0$ (see \ref{prop:SW_Gamma}).

\begin{theorem}[Convergence of the interpolated SGD trajectories]\label{thm:SGD_interpolated_cv} Consider a neural network $T$ and measures $\mx$, $\my$ satisfying \ref{ass:C2_ae}, \ref{ass:T_loclip}, \ref{ass:mx_my}, \ref{ass:T_slow_increase} and \ref{ass:d2T2_and_dT_bounded}.
	
	Let $(\p_\lr^{(t)}), \lr \in ]0, \lr_1], t \in \N$ a collection of SGD trajectories associated to~\ref{eqn:SW_SGD}. Consider $(\p_\lr)$ their associated interpolations. For any compact $\mathcal{K} \subset \R^\dimp$ and any $\varepsilon > 0$, we have:	
	\begin{equation}
		\underset{\substack{\lr \longrightarrow 0 \\ \lr \in\ ]0, \lr_1]}}{\lim}\ \mp \otimes \mxN \otimes \myN \otimes \bbsigma^{\otimes \N} \left( d_c(\p_\lr, S_{-\partial_C F}(\mathcal{K})) > \varepsilon\right) = 0.
	\end{equation}	
\end{theorem}
\vspace{-10pt}
The distance $d_c$ is defined in \ref{eqn:d_c}. As the learning rate decreases, the interpolated trajectories approach the trajectory set $S_{-\partial_C F}$, which is essentially a solution of the \textit{gradient flow equation}  $\dot{u}(s) = -\nabla F(u(s))$ (ignoring the set of non-differentiability, which is $\lambda_{\R^\dimp}$-null). To get a tangible idea of the concepts at play, if $F$ was $\mathcal{C}^2$ and had a finite amount of critical points, then one would have the convergence of a solution $u(s)$ to a critical point of $F$, as $s \longrightarrow+\infty$. These results have implicit consequences on the value of the parameters  at the "end" of training for low learning rates, which is why we will consider a variant of SGD for which we can say more precise results on the convergence of the parameters.

\section{Convergence of Noised Projected SGD Schemes on \texorpdfstring{$F$}{F}}

In practice, it is seldom desirable for the parameters of a neural network to reach extremely large values during training. Weight clipping is a common (although contentious) method of enforcing that $T(\p, \cdot)$ stay Lipschitz, which is desirable for theoretical reasons. For instance the 1-Wasserstein duality in Wasserstein GANs \citep{pmlr-v70-arjovsky17a} requires Lipschitz networks, and similarly, Sliced-Wasserstein GANs \citep{deshpande_generative_sw} use weight clipping and enforce their networks to be Lipschitz.

Given a radius $R_\p > 0$, we consider SGD schemes that are restricted to $\p \in \oll{B}(0, r) =: B_r$, by performing \textit{projected} SGD. At each step $t$, we also add a noise $\noise \varepsilon^{(t+1)}$, where $\varepsilon^{(t+1)}$ is an additive noise of law $\eta \ll \lambda_{\R^\p}$, which is often taken as standard Gaussian in practice. These additions yield the following SGD scheme:
\begin{equation}\label{eqn:SW_SGD_projected_noised}
	\begin{split}
		\p^{(t+1)} = \pi_r\left(\p^{(t)} - \lr \varphi(\p^{(t)}, X^{(t+1)}, Y^{(t+1)}, \theta^{(t+1)}) + \lr\noise \varepsilon^{(t+1)}\right), \\ \left(\p^{(0)}, (X^{(t)})_{t \in \N}\ (Y^{(t)})_{t \in \N},\ (\theta^{(t)})_{t \in \N},\ (\varepsilon^{(t)})_{t \in \N}\right) \sim \mp \otimes \mxN \otimes \myN \otimes \bbsigma^{\otimes \N} \otimes \eta^{\otimes\N},
	\end{split}
\end{equation}
where $\pi_r: \R^\p \longrightarrow B_r$ denotes the orthogonal projection on the ball $B_r := \oll{B}(0, r)$. Thanks to \ref{cond:A1}, \ref{cond:A2} and the additional noise, we can verify the assumptions for \citep{bianchi2022convergence} Theorem 4, yielding the same result as \ref{thm:SGD_interpolated_cv} for the noised projected scheme \ref{eqn:SW_SGD_projected_noised}. In fact, under additional assumptions, we shall prove a stronger mode of convergence for the aforementioned trajectories. The natural context in which to perform gradient descent is on functions that admit a chain rule, which is formalised in the case of almost-everywhere differentiability by the notion of \textit{path differentiability}, as studied thoroughly in  \citep{bolte2021conservative}. We formulate this condition from \citep{bianchi2022convergence} before presenting sufficient conditions on $T$ under which path differentiability shall hold.

\begin{condition}\label{cond:A5}$F$ is path differentiable, which is to say that for any $\p \in \mathcal{C}_{\mathrm{abs}}(\R_+, \R^\dimp)$, for almost all $t > 0,\; \forall v \in \partial_CF(\p(s)),\; v \cdot \dot \p(s) = (F \circ \p)'(s)$.
\end{condition} 

Note that by \citep{bolte2021conservative} Corollary 2, $F$ is path differentiable if and only if $\partial_C F$ is a conservative field for $F$ (in the sense of \citep{bolte2021conservative}, Definition 1) if and only if $F$ has a chain rule for $\partial_C$ (which is the formulation chosen in \ref{cond:A5} by \citep{bianchi2022convergence}). 

In order to satisfy \ref{cond:A5}, we need to make the assumption that the NN input measure $\mx$ and the data measure $\my$ are discrete measures, which is the case for $\my$ in the case of generative neural networks, but is less realistic for $\mx$ in practice. We define $\Sigma_n$ the $n$-simplex: its elements are the $a \in \R^n$ s.t. $\forall i \in \llbracket 1, n \rrbracket,\; a_i \geq 0$ and $\sum_i a_i =1$.

\begin{assumption}\label{ass:mx_my_discrete}
	One may write $\mx = \Sum{k=1}{\npoints_x}a_k \delta_{x_k}$ and $\my = \Sum{k=1}{\npoints_y}b_k \delta_{y_k}$, with the coefficient vectors $a\in \Sigma_{\npoints_x}, b\in \Sigma_{\npoints_y}$, $\X = \lbrace x_1, \cdots, x_{\npoints_x}\rbrace \subset \R^{\dimx}$ and $\Y = \lbrace y_1, \cdots, y_{\npoints_y}\rbrace \subset \R^{\dimy}$.
\end{assumption}  

There is little practical reason to consider non-uniform measures, however the generalisation to any discrete measure makes no theoretical difference. Note that \ref{ass:mx_my} is clearly implied by \ref{ass:mx_my_discrete}. 

In order to show that $F$ is path differentiable, we require the natural assumption that each $T(\cdot, x)$ is path differentiable. Since $T(\cdot, x)$ is a vector-valued function, we need to extend the notion of path-differentiability. Thankfully, \citet{bolte2021conservative} define \textit{conservative mappings} for vector-valued locally Lipschitz functions (Definition 4), which allows us to define naturally path differentiability of a vector-valued function as the path-differentiability of all of its coordinate functions.

\begin{assumption}\label{ass:T_path_diff}
	For any $x \in \R^\dimx,\; T(\cdot, x)$ is path differentiable.
\end{assumption}

\ref{ass:T_path_diff} holds as soon as each $T(\cdot, x)$ is semi-algebraic (i.e. piecewise polynomial, where the pieces are in finite number and can be written through polynomial equations) or more generally definable (see \citep{davis2020stochastic}, Definition 5.10), as proven by \citep{davis2020stochastic}, Theorem 5.8. This is the case for iterated compositions of linear maps and definable activation functions (such as the widespread sigmoid and ReLU), see \citep{davis2020stochastic}, Corollary 5.11, as well as \citep{bolte2021conservative}, \S 6.2 for further explanations on suitable NNs.

\begin{prop}\label{prop:F_path_diff}
	Under \ref{ass:T_loclip}, \ref{ass:mx_my_discrete} and \ref{ass:T_path_diff}, $F$ is path differentiable.
\end{prop}

\begin{proof}
	We shall use repeatedly the property that the composition of path differentiable functions remains path differentiable, which is proved in \citep{bolte2021conservative}, Lemma 6. 
	
	Let $\SWY: \app{\R^{\npoints \times \dimy} \times \R^{\npoints \times \dimy}}{\R_+}{Y, Y'}{\SW_2^2(\gamma_Y, \gamma_{Y'})}$. By \citep{discrete_sliced_loss}, Proposition 2.4.3, each $\SWY(\cdot, Y)$ is semi-concave and thus is path differentiable (by \citep{discrete_sliced_loss}, Proposition 4.3.3).
	
	Thanks to \ref{ass:mx_my_discrete}, $\mxn$ and $\myn$ are discrete measures on $\R^{\npoints \times \dimx}$ and $\R^{\npoints \times \dimy}$ respectively, allowing one to write $\mxn = \sum_k a_k\delta_{X_k}$ and $\myn = \sum_l b_l\delta_{Y_l}$. Then $F = \p \longmapsto \sum_{k,l}a_kb_l\SWY(T(\p, X_k), Y_l)$ is path differentiable as a sum (\citep{bolte2021conservative}, Corollary 4) of compositions (\citep{bolte2021conservative}, Lemma 6) of path differentiable functions.
\end{proof}

We have now satisfied all the assumptions to apply \citep{bianchi2022convergence}, Theorem 6, showing that trajectories of \ref{eqn:SW_SGD_projected_noised} converge towards $\mathcal{Z}_r$, the set of \textit{Karush-Kahn-Tucker} points related to the differential inclusion tied to the discrete scheme \ref{eqn:SW_SGD_projected_noised}:
\begin{equation}\label{eqn:KKT_points}
	\mathcal{Z}_r := \left\lbrace \p \in \R^\dimp\ |\ 0 \in -\partial_CF(\p) - \mathcal{N}_r(\p) \right\rbrace, \quad \mathcal{N}_r(\p) = \left\lbrace\begin{array}{c}
		\lbrace 0 \rbrace\ \mathrm{if}\ \|\p\|_2 < r \\
		\lbrace \lambda \p\ |\ \lambda \geq 0 \rbrace\ \mathrm{if}\ \|\p\|_2 = r \\
		\varnothing\ \mathrm{if}\ \|\p\|_2 > r
	\end{array} \right. ,
\end{equation}
where $\mathcal{N}_r(\p)$ refers to the \textit{normal cone} of the ball $B(0, r)$ at $x$. The term $\mathcal{N}_r(\p)$ in \ref{eqn:KKT_points} only makes a difference in the pathological case $\|\p\|_2 = r$, which never happens in practice since the idea behind projecting is to do so on a very large ball, in order to avoid gradient explosion, to limit the Lipschitz constant and to satisfy theoretical assumptions. Omitting the $\mathcal{N}_r(\p)$ term, and denoting $\mathcal{D}$ the points where $F$ is differentiable, \ref{eqn:KKT_points} simplifies to $\mathcal{Z}_r \cap \mathcal{D} = \lbrace\p \in \mathcal{D}\ |\ \nabla F(\p) = 0\rbrace$, i.e. the critical points of $F$ for the usual differential. Like in \ref{thm:SGD_interpolated_cv}, we let $\lr_1 < \lr_0$, where $\lr_0$ is defined in \ref{prop:SW_Gamma}.

\begin{theorem}[\cite{bianchi2022convergence}, Theorem 6 applied to \ref{eqn:SW_SGD_projected_noised}]\label{thm:SGD_projected_noised}
	
	Consider a neural network $T$ and measures $\mx$, $\my$ satisfying \ref{ass:C2_ae}, \ref{ass:T_loclip}, \ref{ass:T_slow_increase}, \ref{ass:d2T2_and_dT_bounded}, \ref{ass:mx_my_discrete} and \ref{ass:T_path_diff}. Let $(\p_{\lr}^{(t)})_{t \in \N}$ be SGD trajectories defined by \ref{eqn:SW_SGD_projected_noised} for $r > 0$ and $\lr \in ]0, \lr_1]$. One has	
	$$\forall \varepsilon > 0,\; \underset{t \longrightarrow +\infty}{\oll{\lim}}\ \nu \otimes \mxN\otimes \myN \otimes \bbsigma^{\N} \otimes \eta^{\otimes \N}\left(d(\p_{\lr}^{(t)}, \mathcal{Z}_r) > \varepsilon\right) \xrightarrow[\substack{\lr \longrightarrow 0\\ \lr \in \left] 0, \lr_1\right]}]{} 0.$$
\end{theorem}
\vspace{-15pt}
The distance $d$ above is the usual euclidean distance. \ref{thm:SGD_projected_noised} shows essentially that as the learning rate approaches 0, the long-run limits of the SGD trajectories approach the set of $\mathcal{Z}_r$ in probability. Omitting the points of non-differentiability and the pathological case $\|u\|_2=r$, the general idea is that $u_\alpha^{(\infty)} \xrightarrow[\lr \longrightarrow 0]{} \lbrace u\ :\ \nabla F(u)=0 \rbrace$, which is the convergence that would be achieved by the gradient flow of $F$, in the simpler case of $\mathcal{C}^2$ smoothness.

