\section{Introduction}

\subsection{The Sliced Wasserstein Distance in Machine Learning}

Optimal Transport (OT) allows the comparison of measures on a metric space by generalising the use of the ground metric. Typical applications use the so-called 2-Wasserstein distance, defined as
\begin{equation}\eqlabel{W2}
	\forall \nu_1, \nu_2 \in \mathcal{P}_2(\R^d),\; \W_2^2(\nu_1, \nu_2) := \underset{\pi \in \Pi(\nu_1, \nu_2)}{\inf}\Int{\R^d\times\R^d}{}\|x_1-x_2\|^2\dd \pi(x_1, x_2),
\end{equation}
where $\mathcal{P}_2(\R^d)$ is the set of probability measures on $\R^d$ admitting a second-order moment and where $\Pi(\nu_1, \nu_2)$ is the set of measures of $\mathcal{P}_2(\R^d \times \R^d)$ of first marginal $\nu_1$ and second marginal $\nu_2$. One may find a thorough presentation of its properties in classical monographs such as \citet{computational_ot, santambrogio2015optimal, villani}

The ability to compare probability measures is useful in probability density fitting problems, which are a sub-genre of generation tasks. In this formalism, one considers a probability measure $\mu_\p$, parametrised by $\p$ which is designed to approach a target data distribution $\mu$ (typically the real-world dataset). In order to determine suitable parameters, one may choose any probability discrepancy (Kullback-Leibler, Ciszar divergences, f-divergences or Maximum Mean Discrepancy), or in our case, the Wasserstein distance. In the case of Generative Adversarial Networks, the optimisation problem which trains the "Wasserstein GAN" \citep{pmlr-v70-arjovsky17a} stems from the Kantorovitch-Rubinstein dual expression of the 1-Wasserstein distance.

A less cost-intensive alternative to $\W_2^2$ is the Sliced Wasserstein (SW) Distance introduced by \citet{bonneel2015sliced}, which consists in computing the 1D Wasserstein distances between projections of input measures, and averaging over the projections. The aforementioned projection of a measure $\nu$ on $\R^d$ is done by the \textit{push-forward} operation by the map $P_\theta: x \longmapsto \theta \cdot x$. Formally, $P_\theta\#\nu$ is the measure on $\R$ such that for any Borel set $B\subset \R$, $P_\theta\#\nu(B) = \nu(P_\theta^{-1}(B))$. Once the measures are projected onto a line $\R\theta$, the computation of the Wasserstein distance becomes substantially simpler numerically. We shall illustrate this fact in the discrete case, which arises in practical optimisation settings. Let two discrete measures on $\R^d$: $\gamma_X := \frac{1}{\npoints}\sum_k\delta_{x_k},\; \gamma_Y := \frac{1}{\npoints}\sum_k\delta_{y_k}$ with $x_1, \cdots, x_\npoints, y_1, \cdots, y_\npoints \in \R^d$. Their push-forwards by $P_\theta$ are simply computed by the formula $P_\theta \#\gamma_X = \sum_k \delta_{P_\theta(x_k)}$, and the 2-Wasserstein distance between their projections can be computed by sorting their supports: let $\sigma$ a permutation sorting $(\theta^Tx_1, \cdots, \theta^Tx_\npoints)$, and $\tau$ a permutation sorting $(\theta^Ty_1, \cdots, \theta^Ty_\npoints)$, one has the simple expression
\begin{equation}
	\W_2^2(P_\theta\#\gamma_X, P_\theta\#\gamma_Y) = \cfrac{1}{\npoints}\Sum{k=1}{\npoints}(\theta^Tx_{\sigma(k)} - \theta^Ty_{\tau(k)})^2.
\end{equation}
The SW distance is the expectation of this quantity with respect to $\theta\sim \bbsigma$, i.e. uniform on the sphere: $\SW_2^2(\gamma_X, \gamma_Y) = \mathbb{E}_{\theta \sim \bbsigma}\left[\W_2^2(P_\theta\#\gamma_X, P_\theta\#\gamma_Y)\right]$. The 2-SW distance is also defined more generally between two measures $\mu, \nu \in \mathcal{P}_2(\R^d)$:
\begin{equation}\eqlabel{SW}
	\mathrm{SW}_2^2(\mu, \nu) := \Int{\theta \in \SS^{d-1}}{}\W_2^2(P_\theta\#\mu, P_\theta\#\nu)\dd \bbsigma(\theta).
\end{equation}

The implicit generative modelling framework is a formalisation of the training step of generative Neural Networks (NNs), where a network $T$ of parameters $\p$ is learned such as to minimise the discrepancy between $T_\p\#\mx$ \footnote{$T_\p\#\mx$ is the push-forward measure of $\mx$ by $T_\p$, i.e. the law of $T_\p(x)$ when $x\sim \mx$.} and $\my$, where $\mx$ is a low-dimensional input distribution (often chosen as Gaussian or uniform noise), and where $\mu$ is the target distribution. Our case of interest is when the discrepancy is measured with the SW distance, which leads to minimising $\mathrm{SW}_2^2(T_\p\#\mx, \my)$ in $\p$. In order to train a NN in this manner, at each iteration one draws $\npoints$ samples from $\mx$ and $\my$ (denoted $\gamma_X$ and $\gamma_Y$ as discrete measures with $\npoints$ points), as well as a projection $\theta$ (or a batch of $p$ projections) and performs an SGD step on the sample loss
\begin{equation}\label{eqn:Loss}
	\mathcal{L}(\p) = \mathrm{SW}_2^2(P_\theta\#T_\p\#\gamma_X, P_\theta\#\gamma_Y) = \cfrac{1}{\npoints}\Sum{k=1}{\npoints}(\theta^TT_\p(x_{\sigma(k)}) - \theta^Ty_{\tau(k)})^2,
\end{equation}
with respect to the parameters $\p$ (see \ref{alg:SGD} for a precise formalisation). In order to compute this numerically, the main complexity comes from determining the permutations $\sigma$ and $\tau$ by sorting the numbers $(\theta^TT_\p(x_{k}))_k$ and $(y_k)_k$, and summing the results, while the Wasserstein alternative $\W_2^2(T_\p\#\gamma_X, \gamma_Y)$ is done by solving a Linear Program, which is substantially costlier. 

In this paper, we shall study this training method theoretically and prove convergence results. Theoretical guarantees for this optimisation problem are welcome, since this question has not yet been tackled (to our knowledge), even though its use is relatively widespread: for instance, \citet{deshpande_generative_sw} and \citet{Wu2019_SWAE} train GANs and auto-encoders with this method. Other examples within this formalism include the synthesis of images by minimising the SW distance between features of the optimised image and a target image, as done by \citet{heitz2021sliced} for textures with neural features, and by \citet{Tartavel2016} with wavelet features (amongst other methods). 

In practice, it has been observed that SGD in such settings always converges (in the loose numerical sense), yet this property is not known theoretically, since the loss function defined in \ref{eqn:Loss} is not differentiable nor convex in general, because $X \longmapsto \SW_2^2(\gamma_X, \gamma_Y)$ and the neural network do not have such regularities. Several efforts have been made to prove the convergence of SGD trajectories within this theoretically difficult setting: \citet{bianchi2022convergence} show the convergence of fixed-step SGD schemes on a function $F$ with some technical regularity assumptions, \citet{majewski2018analysis} show the convergence of diminishing-step SGD schemes assuming stronger regularity results on $F$. Another notable theoretical work is by \citet{bolte2021conservative}, which leverages conservative field theory to prove convergence for back-propagated SGD on deep NNs with definable activations and loss functions. In the case of Optimal Transport losses, the only work (that we are aware of) that has tackled this problem is by \citet{fatras2021minibatch}, proving strong convergence results for minibatch variants of classical OT distances, namely the Wasserstein, Entropic Wasserstein and Gromov Wasserstein distances. The aim of this work is to bridge the gap between theory and practical observation by proving convergence results for SGD on Sliced Wasserstein generative losses of the form $F(\p) = \SW_2^2(T_u\#\mx, \my)$.

\subsection{Contributions}

\paragraph{Convergence of Interpolated SGD Under Practical Assumptions} Under practically realistic assumptions, we prove in \ref{thm:SGD_interpolated_cv} that piece-wise affine interpolations (defined in Equation \ref{eqn:interpolation}) of constant-step SGD schemes on $\p\longmapsto F(\p)$ (formalised in Equation \ref{eqn:SW_SGD}) converge towards the set of sub-gradient flow solutions (see Equation \ref{eqn:S}) as the gradient step decreases. This results signifies that with very small learning rates, SGD trajectories will be close to sub-gradient flows, which themselves converge to critical points of $F$ (omitting serious technicalities). 

The assumptions needed for this result are practically reasonable: the input measure $\mx$ and the true data measure $\my$ are assumed to be compactly supported. As for the network $(\p, x) \longmapsto T(\p, x)$, we assume that for a fixed datum $x$, $T(\cdot, x)$ is piecewise $\mathcal{C}^2$-smooth and that it is Lipschitz jointly in both variables on any compact. We require additional assumptions on $T$ which are more costly, but are verified as long as $T$ is of the form $T(\p, x) = \widetilde{T}(\p, x)\mathbbold{1}_B(\p)$, where $ \widetilde{T}$ is any typical NN composed of compositions of definable activations (as is the case for all typical activations, see \citep{bolte2021conservative}, \S6.2), and of linear units; and where $\mathbbold{1}_B(\p)$ is the indicator that the parameter $\p$ be in a fixed ball $B$. This form for $T$ is a strong theoretical assumption, but in practice makes little difference, as one may take a fixed ball $B$ to be arbitrarily large.

\paragraph{Stronger Convergence Under Stricter Assumptions} In order to obtain a stronger convergence result, we consider a variant of SGD where each iteration receives an additive noise (scaled by the learning rate) which allows for better space exploration, and where each iteration is projected on a ball $B(0, r)$ in order to ensure boundedness. This alternative SGD scheme remains within the realm of practical applications, and we show in \ref{thm:SGD_projected_noised} that long-run limits of such trajectories converge towards a set of generalised critical points of $F$, as the gradient step approaches 0. This result is substantially stronger, and can serve as an explanation of the convergence of practical SGD trajectories, specifically towards a set of critical points which amounts to the stationary points of the energy (barring theoretical technicalities).

Unfortunately, we require additional assumptions in order to obtain this stronger convergence result, the most important of which is that the input data measure $\mx$ and the dataset measure $\my$ are discrete. For the latter, this is always the case in practice, however the former assumption is more problematic, since it is common to envision generative NNs as taking an argument from a continuous space (the input is often Gaussian of Uniform noise), thus a discrete setting is a substantial theoretical drawback. For practical concerns, one may argue that the discrete $\mx$ can have an arbitrary fixed amount of points, and leverage strong sample complexity results such as those of \cite{nadjahi_statistical_properties_sliced} to ascertain that the discretisation is not costly if the number of samples is large enough.
