\section{Numerical Experiments}\label{sec:xp}

This section illustrates the optimisation properties of $\SWY$ and $\SWpY$
with several numerical experiments. \ref{sec:xp_bcd} studies the optimisation of
$\SWpY$ using the BCD algorithm described in \ref{alg:BCD}, which offers insights on the
cell structure of $\SWpY$ (\ref{sec:cells}). \ref{sec:traj_sgd} focuses on
stochastic gradient descent \ref{alg:SGD} and showcases various SGD trajectories
on $\SWY$ and $\SWpY$ for different learning rates, noise levels or numbers of
projections, as well as the Wasserstein error along iterations. All the
convergence curves shown throughout our experiments also showcase margins of
error, computed by repeating the experiments several times, and corresponding to
the 30\% and 70\% quantiles of the experiment.

In order to assess the quality of a position $Y^{(t)}$, perhaps the most germane
metric is the Wasserstein distance: $\W_2^2(\gamma_{Y^{(t)}}, \gamma_Z)$, which
is why we will study the 2-Wasserstein error of BCD and SGD trajectories in this section.
Unfortunately, this metric is not quite comparable for different dimensions $d$,
notably because $\|(1, \cdots, 1)\|_2^2 = d$. We shall attempt to compensate
this phenomenon by using $\frac{1}{d}\W_2^2(\gamma_{Y^{(t)}}, \gamma_Z)$
instead, which makes the metric more comparable for measures on spaces of
different dimensions.

\subsection{Empirical study of Block Coordinate Descent on \texorpdfstring{$\SWpY$}{Ep}}\label{sec:xp_bcd}

In this section, we shall focus on studying the optimisation properties of the
$\SWpY$ landscape using the BCD algorithm (\ref{alg:BCD}). This method leverages
the cell structure of $\SWpY$ (see \ref{sec:cells}), by moving from cell to cell
by computing the minimum of their associated quadratics (see the discussion in
\ref{sec:Ep_crit_and_bcd}). By \ref{thm:Sp_crit_optloc_stable}, all local optima
of $\SWpY$ are stable cell optima, i.e. fixed points of the BCD, which summarises
briefly the ties between BCD and the optimisation properties of $\SWpY$. As for
the numerical implementation, \ref{alg:BCD} was implemented in Python with
Numpy \cite{harris2020array} using the closed-form formulae for the
updates.

\subsubsection{Illustration in 2D}\label{sec:traj_bcd}

\paragraph{Dataset and implementation details.} 
We start by setting a simple 2D measure $\gamma_Z$ with a support of only
two points represented with stars in \ref{fig:traj_bcd_seed0}. The measure weights are taken as uniform.
%
%
We fix sequences of $p$ projections $(\theta_1,
\cdots, \theta_p)$ for $p\in \lbrace 3, 10, 30, 100\rbrace$ respectively. We
then draw 100 BCD schemes with different initial positions $Y^{(0)} \in
\R^{2\times 2}$, drawn with independent standard Gaussian entries. We take a
stopping criterion threshold of $10^{-5}$ (see \ref{alg:BCD}), and limit to 500
iterations. 

% Figure environment removed

%
%
%
%
%
%

In the case
$p=3$, we observe on \ref{fig:traj_bcd_seed0} 4 points which correspond to strict local optima, and the
schemes appear to have a comparable probability of converging towards each of
them. Note that these points are essentially the same as the ones represented in
\ref{fig:SWpY_sym_p3} for $p=3$, but that they depend on the projection sample. 
Between the two projection realisations, we
observe that these local optima change locations. The cases $p \in \lbrace 10,
30, 100\rbrace$ also exhibit strict local optima, however they appear to be
decreasingly likely to be converged towards. For $p=30$ and $p=100$, notice that
most trajectories end up on the same ellipsoid arcs towards the solution $Z$,
and further remark that these arcs strongly resembles the trajectories of SGD
schemes on $\SWY$ for small learning rates (see \ref{fig:traj_sgd_E_lr_multiple}
in \ref{sec:xp_sgd}). 

%
%
%
%
%

\subsubsection{Wasserstein convergence of BCD schemes on \texorpdfstring{$\SWpY$}{Ep}}

\paragraph{Final Wasserstein error of BCD Schemes.} For a dimension $d \in \lbrace 10, 30, 100\rbrace$ and $\npoints=20$ points, the original measure $\gamma_Z,\; Z \in \R^{\npoints \times d}$ is sampled once for all with independent standard Gaussian entries. Then, for varying numbers of projections $p$, we draw a starting position $Y^{(0)} \in \R^{\npoints \times d}$ with entries that are uniform on $[0, 1]$; and draw $p$ projections as input to the BCD algorithm. We set the stopping criterion threshold as $\varepsilon=10^{-5}$ and the maximum iterations to 1000. In order to produce \ref{fig:BCD_its}, we record the normalised 2-Wasserstein discrepancy $\frac{1}{d}\W_2^2(\gamma_{Y^{(T)}}, \gamma_Z)$ at the final iteration $T$ for 10 realisations for each value of $p$ and $d$.

% Figure environment removed

As a first estimation of the difficulty of optimising $\SWpY$, we consider the evolution - as $p$ increases - of final $\W_2^2$ errors of BCD schemes. The results of the experiments presented in \ref{fig:BCD_its} suggests the existence of a phase transition between an insufficient and a sufficient amount of projections. For instance, in the case $d=10$, there appears to be a cutoff around $p=400$, under which all the BCD realisations converge towards strict local optima, and past which we observe convergence up to numerical precision.

\paragraph{Probability of convergence of BCD schemes.} We can investigate further this empirical cutoff phenomenon by estimating the probability of convergence of a BCD algorithm. This probability is loosely related to the difficulty of optimising the landscape $\SWpY$, since a high probability of BCD convergence indicates either a small number of strict local optima, or that their corresponding cells are extremely small and seldom reached in practice. For varying numbers of projections $p$ and dimensions $d$, we run 100 realisations of BCD schemes. Each sample draws a target measure $\gamma_Z,\; Z \in \R^{\npoints \times d}$ with independent standard Gaussian entries and $\npoints=10$ points, as well as its initialisation $Y^{(0)} \in \R^{\npoints \times d}$ with entries that are uniform on $[0, 1]$ and $p$ projections. Every BCD scheme has a stopping threshold of $\varepsilon=10^{-5}$ and a maximum of 1000 iterations. We consider that a sample scheme has converged (towards the global optimum $\gamma_Z$) if $\frac{1}{d}\W_2^2(\gamma_{Y^{(T)}}, \gamma_{Z}) < 10^{-5}$, which allows us to compute an empirical probability of convergence for each value of $(p, d)$.

% Figure environment removed

The findings in \ref{fig:cv_proba} indicate that the $\W_2^2$ error cutoffs from \ref{fig:BCD_its} have a probabilistic counterpart: the probability of converging to a global optimum transitions from almost 0 to almost 1 relatively suddenly (in the logarithmic scale). We can conjecture that this drop in optimisation difficulty is tied to the number of iterations needed for the convergence of SGD schemes on $\SWY$, especially given the similar behaviour for the $\W_2^2$ error in \ref{fig:SGD_its_E_alpha}.

\subsection{Empirical study of SGD on \texorpdfstring{$\SWY$}{E} and
\texorpdfstring{$\SWpY$}{E}}\label{sec:xp_sgd}


\paragraph{General numerical implementation.} In order to perform gradient
descent on $\SWY$ or $\SWpY$, we compute the gradient \ref{eqn:ae_grad} using
%
{Pytorch}'s 
\cite{pytorch} Stochastic Gradient Descent optimiser, which back-propagates
gradients through the loss $w_\theta:= Y \mapsto\W_2^2(P_{\theta}\#\gamma_{Y},
P_{\theta}\#\gamma_{Z})$, which we compute using the 1D Wasserstein solver from
%
{Python Optimal Transport} \cite{flamary2021pot}.

\subsubsection{Illustration in 2D}\label{sec:traj_sgd}

\paragraph{2D dataset and implementation details.} We define a 2D spiral dataset for the original measure $\gamma_Z,\; Z = (z_1, \cdots, z_{10})^T\in \R^{10\times 2}$ with $z_k = \frac{2k}{10}\left(\cos\left(2k\pi / 10\right),\ \sin\left(2k\pi / 10\right)\right)^T, \quad k \in \llbracket 1, 10 \rrbracket$. The initial position $Y^{(0)}$ is fixed and remains the same across realisations. For schemes on $\SWY$, the projections $\theta^{(t)} \sim \bbsigma$ are fixed beforehand and are the same across experiments. For every realisation of a scheme on $\SWpY,\; p$ unique projections $(\theta_1, \cdots, \theta_p)$ are drawn, then the projections $(\theta^{(t)})$ for the iterations  are drawn from these $p$ fixed projections. For noised schemes, the only variable that is drawn at every sample is the noise $(\varepsilon^{(t)})$. Note that the associated energy landscapes are extremely similar to those illustrated in \ref{sec:L2} and in particular in \ref{fig:SWpY_sym_p3}.

% Figure environment removed

% Figure environment removed

\ref{fig:traj_sgd_E_lr} and \ref{fig:traj_sgd_E_lr_multiple} illustrate the
convergence of SGD schemes on $\SWY$ towards the original measure $\gamma_Z$,
for different learning rate $\alpha$ (provided that $\alpha$ is under a divergence threshold).
\ref{thm:SGD_interpolated_cv} allowed us only to expect a convergence to a
\textit{solution of a Clarke Differential Inclusion} on $\SWY$
\ref{eqn:Clarke_DI}, yet in practice we seem to have convergence to a global
optimum. Furthermore, \ref{thm:SGD_interpolated_cv} shows that the interpolated
SGD trajectories are approximately solutions of the DI $\dot{X}(t) \in
-\partial_C\SWY(X(t))$, which, assuming that the trajectory stays in
$\mathcal{U}$, amounts to $\dot{X}(t) + \nabla \SWY (X(t)) = 0$, which is
exactly the Euclidean Gradient Flow of $\SWY$, as discussed in more detail in
\ref{rem:flows}. This illustration suggests that the SGD schemes approach the
gradient flow \ref{eqn:SWY_flow} as $\alpha \longrightarrow 0$, whereas
\ref{thm:SGD_interpolated_cv} predicts a (weak) convergence towards the set of
solutions of the DI \ref{eqn:Clarke_DI}, which is equal to the gradient flow
provided that the initial position $Y^{(0)}$ belongs to the differentiability set
of $\SWY$ (see \ref{rem:flows} for details). Note that higher learning rates
lead to a "noisier" trajectory, which may impede upon the quality of the
assignment. This shows that there is a trade-off: lower values of $\alpha$ allow
for a better approximation of the (or a) gradient flow of $\SWY$ and potentially
a more precise final position $Y$ and assignment $\tau$, however a larger value
of $\alpha$ yields a substantially faster convergence.

% Figure environment removed

%
%
%
%
%
%

\ref{fig:traj_sgd_E_noise} presents a case where noised SGD schemes on $\SWY$ "converge" whatever the noise level to a global optimum of $\SWY$. Note that the additive noise causes the scheme to oscillate around a solution, with a movement akin to Brownian motion with a scale tied to $\alpha a$. \ref{thm:noised_SGD_cv} shows that such schemes converge (as the step approaches 0) to \textit{Clarke critical points} of $\SWY$, which could theoretically be a saddle point of strict local optimum. In this experiment,  we observe convergence to a global optimum.

% Figure environment removed

% Figure environment removed

\ref{fig:traj_sgd_Ep} illustrates that SGD schemes on $\SWpY$ may converge to strict local optima, which is to be expected, given how numerous they may be (see the discussion in \ref{sec:L2} and \ref{fig:SWpY_sym_p3} therein). For $p=1$, entire lines are local optima, and for $p=3$ and $p=30$, we also observe convergence to strict local optima. Notice that for a large value of $p$ such as $p=100 \gg d=2$, we have similar trajectories in \ref{fig:traj_sgd_Ep_multiple} compared to the $\SWY$ counterpart in \ref{fig:traj_sgd_E_lr_multiple} ($\alpha=0.03$). This observation suggests a stronger property than our results on the approximation of $\SWY$ by $\SWpY$: uniform convergence in \ref{thm:Sp_cvu_S} and a weak link between critical points \ref{thm:cv_fixed_point_distance}. To be precise, this illustration could allow one to hope for a result on the high probability for the proximity of SGD schemes on $\SWpY$ and on $\SWY$ as $p \longrightarrow +\infty$, perhaps with conditions on the sequence of projections $(\theta^{(t)})$.

\subsubsection{Wasserstein convergence of SGD schemes on \texorpdfstring{$\SWY$}{E} and \texorpdfstring{$\SWpY$}{Ep}}

\paragraph{SGD on $\SWY$.}\label{para:numerical_setup_sgd_E} The original measure $\gamma_Z,\; Z \in \R^{\npoints \times d}$ is sampled once for all with independent standard Gaussian entries. For each value of the parameter of interest (the learning rate $\alpha$ or the dimension $d$ respectively), 10 realisations of the SGD schemes are computed with a different initial position $Y^{(0)}$, drawn with independent entries uniform on $[0, 1]$, and different projections $(\theta^{(t)})$. The SGD stopping criterion threshold (see~\ref{alg:SGD}) is set as negative, in order to always end at the maximum number of iterations, $10^6$. For the experiment with varying learning rates $\alpha$, we consider measures with $\npoints=20$ points in dimension $d=10$. For the experiment with varying dimensions $d$, we still take $\npoints=20$ and use the learning rate $\alpha=10$.

% Figure environment removed

In \ref{fig:SGD_its_E_alpha}, we observe that the SGD schemes converge towards the true measure $\gamma_Z$ up to numerical precision, which corresponds to a stronger convergence than the one predicted by~\ref{thm:SGD_interpolated_cv}. The number of iterations needed for convergence obviously depends on the learning rate $\alpha$, which notably can be chosen larger than $\npoints/2$, which is a case that does not fall under the conditions for \ref{thm:SGD_interpolated_cv}. However, in this particular experiment, the SGD schemes diverged as soon as $\alpha \geq 30$, which could suggest that limiting oneself to $\alpha \preceq \npoints$ is reasonable. The dimension $d$ increases significantly the number of iterations required for convergence, furthermore we observe a transition from high $\W_2^2$ error to low error, which is relatively sudden in logarithmic space. These first studies invites an in-depth analysis of the amount of iterations needed to reach convergence, which we propose in \ref{fig:SGD_its_for_cv}. The final $\frac{1}{d}\W_2^2$ error does not seem to depend significantly on the dimension $d$, which provides empirical grounds for the $1/d$ normalisation choice.

\paragraph{Noised SGD on $\SWY$.} \ref{fig:SGD_its_E_noise} shows the Wasserstein error $\frac{1}{d}\W_2^2(\gamma_{Y^{(t)}}, \gamma_Z)$ for the noised SGD iterations on $\SWY$. The numerical setup is the same as above, with the addition of the noise $a\alpha\varepsilon^{(t)}$ at each iteration, where $\varepsilon^{(t)}$ has independent standard Gaussian entries, $a$ is the noise level and $\alpha$ is the learning rate (set to $\alpha=10$). This noise is drawn differently for each SGD scheme. For the experiment with different dimensions, the noise level is taken as $a=10^{-4}$.

% Figure environment removed

The noised SGD scheme errors oscillate around a certain level which depends on the noise level, as the trajectories from \ref{fig:traj_sgd_E_noise} suggest: we observed Brownian-like motion around the target points. Note that the error begins falling drastically past the same iteration threshold, albeit with a higher variance across samples for higher noise levels. At a fixed noise level, the final $\frac{1}{d}\W_2^2$ still depends on the noise level, despite the $1/d$ normalisation. Empirically, the final $\W_2^2$ error seems to be smaller than the noise level $a$, which is reassuring since the noise is entry-wise of law $\mathcal{N}(0, a^2\alpha^2)$, where $\alpha$ is the learning rate. 

\paragraph{SGD on $\SWpY$.} \ref{fig:SGD_its_Ep} also illustrates the Wasserstein error along iterations but this time for  $\SWpY$.
The general SGD setup and initial measure $\gamma_Z$ remain unchanged compared to the schemes on $\SWY$ (with also a learning rate of $\alpha=10$ in particular). In order to handle the projections $(\theta^{(t)})$, for each sample we draw $p$ independent projections $(\theta_1, \cdots, \theta_p)$, then select the $(\theta^{(t)})$ by drawing uniformly amongst these $p$ projections. Given this sequence of projections $(\theta^{(t)})$, the SGD algorithm is then exactly the same as for $\SWY$.

% Figure environment removed

For SGD schemes on $\SWpY$ with small values of projections $p$, we do not have
convergence to $\gamma_{Y^{(t)}} = \gamma_Z$. Intuitively, this could be
understood as the approximation $\SWpY \approx \SWY$ being too rough, allowing
for an excessive amount of numerically attainable strict local optima. This is
illustrated in \ref{fig:SWpY_sym_p3} in a simple case: with $p=3$ in dimension
2, the landscape presents numerous strict local optima that lie within large
basins. However, it is notable that for $p$ large enough ($p \geq 10d = 100$),
we \textit{do} observe convergence to  $\gamma_{Y^{(t)}} = \gamma_Z$ up to
numerical precision. This convergence happens in fewer iterations as $p$
increases, and with a smaller variance with respect to the projection samples.
This suggests a stronger mode of convergence of $\SWpY$ towards $\SWY$, as
hinted at before in \ref{fig:SWpY_sym_p3} and \ref{fig:traj_sgd_Ep_multiple}. 
%

\paragraph{Quantifying the impact of the dimension.} For different values of the number of points $\npoints$ and the dimension $d$, we run 10 samples of SGD on $\SWY$ for an original measure $\gamma_Z$ drawn with standard Gaussian entries (re-drawn for each sample this time). The SGD schemes are done without additive noise, and with a learning rate of $\alpha=10$. In order to save computation time, the SGD stopping threshold is taken as $\beta = 10^{-5}$ (see \ref{alg:SGD}). For each sample, the initial position $Y^{(0)}$ is drawn with entries that are uniform on $[0, 1]$. Our goal is to estimate the number of iterations required for the convergence of the SGD schemes: to this end, we define convergence as the first step $t$ such that $\frac{1}{d}\W_2^2(\gamma_{Y^{(t)}}, \gamma_Z) < 10^{-5}$.

% Figure environment removed

\ref{fig:SGD_its_for_cv} (cautiously) suggests that the number of iterations required for convergence its proportional to $d^{1.25}$ (where convergence means that $\frac{1}{d}\W_2^2$ falls below $\varepsilon$). Note that the exponent on $d$ does not seem to depend on $\npoints$. Obviously, the factor in front of $d^{1.25}$ depends on the number of points $\npoints$, the learning rate $\alpha$ and the convergence threshold $\varepsilon$. This superlinear rule remains fairly prohibitive for large Machine Learning models, which can typically have $d$ and $\npoints$ both in excess of $10^6$.
