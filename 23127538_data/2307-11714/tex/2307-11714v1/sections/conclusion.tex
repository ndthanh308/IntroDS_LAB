\section{Conclusion and Outlook}

Under reasonable assumptions, we have shown that SGD trajectories of parameters of generative NNs with a SW loss converge towards the desired sub-gradient flow solutions, implying in a weak sense the convergence of said trajectories. Under stronger assumptions, we have shown that trajectories of a mildly modified SGD scheme converge towards a set of generalised critical points of the loss, which provides a missing convergence result for such optimisation problems.

The core limitation of this theoretical work is the assumption that the input data measure $\mx$ is discrete (\ref{ass:mx_my_discrete}), which we required in order to prove that the loss $F$ is path differentiable. In order to generalise to a non-discrete measure, one would need to apply or show a result on the stability of path differentiability through integration: in our case, we want to show that $\int_{\Xn} \SWY(T(\p, X), Y)\dd\mxn(X)$ is path differentiable, knowing that $\p \longmapsto \SWY(T(\p, X), Y)$ is path differentiable by composition (see the proof of \ref{prop:F_path_diff} for the justification). Unfortunately, in general if each $g(\cdot, x)$ is path differentiable, it is not always the case that $\int g(\cdot, x)\dd x$ is path differentiable (at the very least, there is no theorem stating this, even in the simpler case of tame functions, see \citep{bianchi2022convergence}, Section 6.1). However, there is such a theorem for \textit{Clarke regular} functions (specifically \citep{clarke1990optimization}, Theorem 2.7.2 with Remark 2.3.5), sadly the composition of Clarke regular functions is not always Clarke regular, it is only known to be the case in excessively restrictive cases (see \citep{clarke1990optimization}, Theorems 2.3.9 and 2.3.10). As a result, we leave the generalisation to a non-discrete input measure $\mx$ for future work.

Another avenue for future study would be to tie the flow approximation result from \ref{thm:SGD_interpolated_cv} to Sliced Wasserstein Flows \citep{liutkus19a_SWflow_generation, bonet2022efficient}. The difficulty in seeing the differential inclusion \ref{eqn:S} as a flow of $F$ lies in the non-differentiable nature of the functions at play, as well as the presence of the composition between SW and the neural network $T$, which bodes poorly with Clarke sub-differentials. 