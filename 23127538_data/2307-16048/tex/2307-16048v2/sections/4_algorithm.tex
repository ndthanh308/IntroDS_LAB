\section{Estimation}
\label{section_algorithm}

We introduce two algorithms for estimating the set of direct causes of a target variable. The first algorithm is based on estimating $S_{\mathcal{F}}(Y)$ via statistical testing, and the second is a score-based algorithm that finds the best-fitting set for $pa_Y$. 

We consider a random sample of size $n\in\mathbb{N}$ from $(Y, \textbf{X})\in\mathbb{R}\times\mathbb{R}^p$, where $Y$ is the target variable, $\textbf{X}$ are the covariates, and $\mathcal{F}\subseteq\mathcal{I}_{m}$. 

\subsection{ISD algorithm for estimating $S_{\mathcal{F}}(Y)$ }
\label{ISD}

For a non-empty set $S\subseteq\{1, \dots, p\}$, define the hypothesis $$H_{0, S} (\mathcal{F}): S \text{   is an }\mathcal{F} \text{-plausible set of parents of Y}. $$ Suppose for the moment that a statistical test for $H_{0, S} (\mathcal{F})$ with size smaller than a significance level $\alpha$ is available. Then, we define $$\hat{S}_{\mathcal{F}}(Y):=\bigcap_{ \emptyset\neq S\subseteq\{1, \dots, p\}:   H_{0, S} (\mathcal{F}) \text{ is not rejected}}S$$ as an intersection of all sets for which  $H_{0, S} (\mathcal{F})$ was not rejected. 

In order to test  $H_{0, S} (\mathcal{F})$,  we propose a procedure called ISD (Independence $+$ Significance $ +$ Distribution). The idea is to decompose  $H_{0, S} (\mathcal{F})$ into three sub-hypothesis. In particular, $H_{0, S} (\mathcal{F})$ is true if and only if there exist a function $\hat{f}_S$ such that $\hat{\varepsilon}_S:=\hat{f}_S^{\leftarrow}(\textbf{X}_S, Y)$  satisfies: 
\begin{enumerate}
    \item   $H_{0, S}^I: \hat{\varepsilon}_S \indep \textbf{X}_S$, \noindent\hfill (\textbf{I}ndependence)
    \item   $H_{0, S}^S$: $\hat{f}_S\in\mathcal{F}$,  \noindent\hfill (\textbf{S}ignificance) \newline (recall that $\hat{f}_S$ must be irreducible almost surely or in other words, all inputs are significant)
    \item    $H_{0, S}^D:$ $\hat{\varepsilon}_S \sim U(0,1)$. \noindent\hfill (\textbf{D}istribution)
\end{enumerate}

We reject  $H_{0, S} (\mathcal{F})$ if and only if one of  $H_{0, S}^I, H_{0, S}^S, H_{0, S}^D$  is rejected. 

\begin{theorem}
\label{theorem_consistency_ISD}
Assume that $(Y, \textbf{X})$ satisfies ( \ref{SCM_for_Y}) with $pa_Y\neq \emptyset$.  Assume that the estimator  $\hat{S}_{\mathcal{F}}(Y)$  is constructed as described above with $\hat{f}_{pa_Y} = f_Y$ and with valid tests $H_{0, S}^I, H_{0, S}^S, H_{0, S}^D$  for all non-empty set $S\subseteq \{1, \dots, p\}$ at level $\alpha$ in a sense that for all $S$, $\sup_{P: H_{0, S}^\cdot \text{is true}}P(H_{0, S}^\cdot \text{ is rejected})\leq \alpha$ for all $\cdot \in \{S, I, D\}$. Then
$$
P(\hat{S}_{\mathcal{F}}(Y) \subseteq pa_Y) \geq 1-3\alpha. 
$$
\end{theorem}
\textit{Proof.} \, The proof follows directly from 
\begin{align*}
    & P(\hat{S}_{\mathcal{F}}(Y) \subseteq pa_Y) \geq  P(H_{0, pa_Y}(\mathcal{F}) \text{ is not rejected}) =  P(H_{0, pa_Y}^I, H_{0, pa_Y}^S, H_{0, pa_Y}^D \text{ are not rejected})\\& \geq \prod_{\cdot \in \{I, S, D\} } P(H_{0, pa_Y}^\cdot\text{ is not rejected}) \geq (1-\alpha)^3 > 1-3\alpha \,\,\,\,\,\,\,\,for\,\,\alpha\in(0,1). \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \qedsymbol
\end{align*} 
In order to find a suitable candidate for the function $\hat{f}_S$, we use classical methods from machine learning. 
If $\mathcal{F} = \mathcal{F}_A$ or $\mathcal{F} = \mathcal{F}_{LS}$, we can apply random forest, neural networks, GAM, or other classical methods \citep{GAM,GAMLSS, Paul2014StatisticallyII}. Using one of these methods, we estimate the conditional mean ${\mu}$ (and variance ${\sigma}$ in $\mathcal{F}_{LS}$ case) and output the residuals $\hat{\eta}_S:=Y -\hat{\mu}(\textbf{X}_S)$ (or $ \hat{\eta}_S:=\frac{Y -\hat{\mu}(\textbf{X}_S)}{\hat{\sigma}(\textbf{X}_S)}$ in $\mathcal{F}_{LS}$ case). Possibly, re-scale the residuals $\hat{\varepsilon}_S := \hat{q}(\hat{\eta}_S)$, where $\hat{q}$ is the empirical distribution function of $\hat{\eta}$ (see the discussion about $H_{0, S}^D$ below). If  $\mathcal{F} = \mathcal{F}_F$ for some distribution function $F$, we can use GAMLSS \citep{GAMLSS} or GAM algorithms for estimating $\theta$. Then, we define $\hat{\varepsilon}_S:=F\big(Y, \hat{\theta}(\textbf{X}_S)\big)$.

Notice that if the chosen method is consistent and ( \ref{SCM_for_Y}) holds, $\hat{f}_{pa_Y}$ converges to $f_{Y}$. Therefore, the choice $\hat{f}_{pa_Y} = f_Y$ in Theorem~\ref{theorem_consistency_ISD} is justified in large sample sizes. The following tests can be used for practical testing of  $H_{0, S}^I, H_{0, S}^S$ and $H_{0, S}^D$:

\begin{enumerate}
    \item $H_{0, S}^I$: We can use a kernel-based HSIC test \citep{Kernel_based_tests} or a copula-based test \citep{copula_based_independence_test}.
    \item $H_{0, S}^S$:  This test ensures that $\hat{f}_S$ is irreducible, meaning that we do not include non-significant (and hence non-causal) covariates into an $\mathcal{F}$-plausible set. In practice, we test the alternative hypothesis \({H}_{0, S}^{S, \text{alt}}: \hat{f}_S \not\in \mathcal{F}\), and we reject \(H_{0, S}^S\) if and only if we do not reject \({H}_{0, S}^{S, \text{alt}}\). The reason is that many methods have been developed for testing \({H}_{0, S}^{S, \text{alt}}\). For example, in the case of linear regression \(Y = \beta \textbf{X}_S + \eta_S\), we test if \(\beta_i \neq 0\) for all \(i \in S\) via classical significance testing. Analogously for GAM or GAMLSS. Alternatively, we can use a permutation test to assess the significance of the covariates \citep{Paul2014StatisticallyII}.
    \item $H_{0, S}^D$: This step is only relevant when a specific noise distribution is assumed. However, the hypothesis \( H_{0, S}^D \) is automatically true in cases such as \(\mathcal{F} = \mathcal{F}_L\), \(\mathcal{F}_A\), or \(\mathcal{F}_{LS}\). In these instances, we omit this test. The reason is that we can use a probability integral transform of the estimated noise to obtain $\hat{\varepsilon}_S\sim U(0,1)$. However in cases such as  $\mathcal{F}=\mathcal{F}_F$, the integral transform breaks the condition $\hat{f}_S\in\mathcal{F}_F$ and testing for $\hat{\varepsilon}_S\sim U(0,1)$ is necessary.  \newline If we opt for testing $H_{0, S}^D:$ $\hat{\varepsilon}_S \sim U(0,1)$, we can use a Kolmogorov-Smirnov or Anderson-Darling test \citep{AD-KStest}.
\end{enumerate}

In our implementation, we opt for HSIC test, GAM estimation, and the Anderson-Darling test. We summarize the algorithm in case of $\mathcal{F} = \mathcal{F}_A$ as follows:

\begin{algorithm}[H]
  \SetAlgoLined
  \KwData{Random sample $(y_1, x_1^1, \dots, x_1^p), \dots, (y_n, x_n^1, \dots, x_n^p)$}
  \KwResult{ REJECT or NOT REJECT  }

1) Estimate \(\hat{f}_S\) in the model \(Y = f_S(\textbf{X}_S) + \eta_S\) (using GAM estimation, for example). Define \(\hat{\eta}_S := Y - \hat{f}_S(\textbf{X}_S)\).

2) Test \(\hat{\eta}_S \indep \textbf{X}_S\) at level \(\alpha\) (using the HSIC test, for example). Set \(H_{0, S}^I = \text{REJECT}\) if this test was rejected, otherwise set \(H_{0, S}^I = \text{NOT REJECT}\).

3) Set \(H_{0, S}^S = \text{NOT REJECT}\) if all covariates are significant at level \(\alpha\) in the model from step 1 (using the permutation test for covariate significance, for example). Otherwise, set \(H_{0, S}^S = \text{REJECT}\).

4) Automatically define \(H_{0, S}^D = \text{NOT REJECT}\) (this step is not relevant in the case \(\mathcal{F} = \mathcal{F}_A\)).

5) Return \(\text{NOT REJECT}\) if all \(H_{0, S}^I\), \(H_{0, S}^S\), and \(H_{0, S}^D\) were not rejected. Otherwise, return \(\text{REJECT}\).


\caption{Testing $H_{0, S} (\mathcal{F})$ in case of $\mathcal{F} = \mathcal{F}_A$ }
  \label{Algorithm}
\end{algorithm}




\subsection{Score-based estimation of $pa_Y$}\label{Section_algorithm2}
We propose a score-based algorithm for estimating the set of direct causes of $Y$. It is a local counterpart of score-based algorithms for estimating the full DAG $\mathcal{G}_0$, following the ideas from \cite{Score-based_causal_learning}, \cite{Peters2014}, and \cite{bodik2023identifiability}. Recall that 
under ( \ref{SCM_for_Y}), the set $S=pa_Y$ should satisfy that $\varepsilon_S\indep \textbf{X}_S$, every ${X}_i, i\in S$ is significant and $\varepsilon_S\sim U(0,1)$. 
Therefore, we use the following score function: 
\begin{equation*}
\begin{split}
\widehat{pa}_Y =  \argmax_{\substack{S\subseteq\{1, \dots, p\}\\
                  S\neq\emptyset}}score(S) &= \argmax_{\substack{S\subseteq\{1, \dots, p\}\\
                  S\neq\emptyset}}\lambda_1 (Independence) + \lambda_2(Significance)+ \lambda_3 (Distribution),
\end{split}
\end{equation*}
where $\lambda_1, \lambda_2, \lambda_3\in [0, \infty)$,  ``\textit{Independence}'' is a measure of independence between $ (\hat{\varepsilon}_S, \textbf{X}_S)$, ``\textit{Significance}'' is a measure of significance of covariates $\textbf{X}_S$, and ``\textit{Distribution}'' is a distance between the distribution of $\hat{\varepsilon}_S$ and $U(0,1)$, where $\hat{\varepsilon}_S$ is the noise estimate defined in Section~\ref{ISD}. 

\textit{The measure of independence} can be chosen as the p-value of the independence test (such as the kernel-based HSIC test or the copula-based test). 
\textit{The measure of significance} corresponds to the estimation method analogously to the ISD case. For linear regression (similarly for GAM or GAMLSS), we compute the corresponding p-values for the hypotheses $\beta_i=0, i\in S$.  Then, \textit{Significance} is the minus of the maximum of the corresponding p-values (worst case option). We can also use a permutation test to assess the covariate's significance in terms of the predictability power and choose the largest p-value. 
\textit{The distance between the distribution of} $\hat{\varepsilon}_S$ \textit{and} $U(0,1)$ can be chosen as the p-value of the Anderson-Darling test. 

The choice of $\lambda_1, \lambda_2, \lambda_3$ describes weights we put on each of the three scores: if $\lambda_1>\lambda_2, \lambda_3$, then our estimate will be very sensitive against the violation of the independence $\varepsilon_S\indep \textbf{X}_S$, but not as sensitive against the violation of the other two properties. 

In our implementation, we opt for the following choices. The \textit{Independence} term is the logarithm of the p-value of the Kernel-based HSIC test, and the \textit{Distribution} term is the logarithm of the p-value of the Anderson-Darling test. We use GAM for the estimation of $\hat{f}_S$ and minus the logarithm of the maximum of the corresponding p-values for the \textit{Significance} term. The logarithmic transformation of the three p-values is used to re-scale the values from $[0,1]$ to $(-\infty, 0]$. The practical choice for the weights is   $\lambda_1 = \lambda_2 = \lambda_3 =1 $ (unless $\mathcal{F}=\mathcal{F}_L,\mathcal{F}_A$, or $\mathcal{F}_{LS}$ when we put $\lambda_3 = 0$). 

\subsubsection{Consistency}

Consistency of the proposed algorithm follows from the results presented in \cite{reviewANMMooij}, who showed consistency of the score-based DAG estimation for additive noise models. In the following, we consider $\mathcal{F} = \mathcal{F}_A$, although it is straightforward to generalize these results for other types of $\mathcal{F}$ (for a discussion about $\mathcal{F} = \mathcal{F}_{LS}$, see \cite{sun2023causeeffect}, and for $\mathcal{F} = \mathcal{F}_F$, see \cite{bodik2023identifiability}). For simplicity, we assume that the measure of independence is the negative value of HSIC test itself (not its p-value as we use in our implementation), and the estimate $\hat{f}_S$ is \textit{suitable} in the sense that noise estimate $\hat{\varepsilon} = \hat{f}_S^\leftarrow(\textbf{X}_S, Y)$ satisfies 
$$
\lim_{n\to\infty}\mathbb{E}_{}\bigg( \frac{1}{n}\sum_{i=1}^n(\varepsilon_i-\hat{\varepsilon}_i)^2   \bigg) = 0,
$$
where the expectation is taken with respect to the distribution of the random sample  \cite[Appendix A.2]{reviewANMMooij}. 

\begin{proposition}\label{Proposition_consistency}
Consider $\mathcal{F} = \mathcal{F}_A$ and 
let $(Y, \textbf{X})\in\mathbb{R}\times \mathbb{R}^p$ follow an SCM with DAG $\mathcal{G}_0$ satisfying ( \ref{SCM_for_Y}). Assume that every $S \neq pa_Y\neq \emptyset$ is not $\mathcal{F}$-plausible. 
Then,   

\begin{equation}
  \lim_{n\to\infty} \mathbb{P}(\widehat{pa}_Y  \neq pa_Y) = 0,
\end{equation}   
where $n$ is the size of the random sample and $\widehat{pa}_Y$ is our score-based estimate from Section~\ref{Section_algorithm2} with $\lambda_1, \lambda_2>0, \lambda_3 = 0$, suitable estimation procedure, and HSIC independence measure. 
\end{proposition}

The proof is in \hyperref[Proof of Proposition_consistency]{Appendix} \ref{Proof of Proposition_consistency}.  If several sets are $\mathcal{F}$-plausible, the score-based algorithm provides no guarantees that the set $S=pa_Y$ will have the best score among them (as opposed to the ISD algorithm that outputs their intersection). 

\subsection{What is a suitable $\mathcal{F}$ in practice?}

The choice of the class $\mathcal{F}$ is a crucial step in the algorithm. The choice of an appropriate model is a common problem in classical statistics; however, it is more subtle in causal discovery. It has been shown \citep{Peters2014} that methods based on restricted structural equation models can outperform traditional methods (these results were shown when estimating the entire DAG). Even if assumptions such as additive noise or Gaussian distribution of the effect given the causes can appear to be strong, such methods have turned out to be rather useful, and small violations of the model still lead to a good estimation procedure. 

The size of  $\mathcal{F}$ is the most important part. If $\mathcal{F}$ contains too many functions, we find that most sets are  $\mathcal{F}$-plausible. On the other hand, overly restrictive  $\mathcal{F}$ can lead to rejecting all sets as potential causes.  If we have knowledge about the data-generation process (such as when $Y$ is a sum of many small events), choosing $\mathcal{F}_F$ for a distribution function $F$ (such as Gaussian) is reasonable. For the choices $\mathcal{F} = \mathcal{F}_A$ or $\mathcal{F}_{LS}$,  there are numerous papers justifying such assumptions in several settings (when estimating the full DAG, \cite{reviewANMMooij}, \cite{Elements_of_Causal_Inference},  \cite{immer2022identifiability}). 

  






