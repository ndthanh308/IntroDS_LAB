\section{Proofs}
\label{Section_proofs}

\input{sections/Proof_child}

 
 
\begin{customlem}{\ref{pairwise_implies_global}}
Pairwise identifiable $\mathcal{F}$-model defined in Appendix~\ref{Appendix_pairwise_identifiability} is identifiable.  
\end{customlem}

 
\begin{proof}\label{Proof of pairwise_implies_global}
For a contradiction, let there be two $\mathcal{F}$-models with causal graphs $\mathcal{G}\neq \mathcal{G}'$ that both generate the same joint distribution $P_{(X_0, \textbf{X})}$.   Using Proposition 29 in \cite{Peters2014}, variables $L,K\in \{X_0, \dots, X_p\}$ exist, such that 
\begin{itemize}
\item $K\to L$ in $\mathcal{G}$ and $L\to K$ in $\mathcal{G}'$,
\item $S:=\underbrace{\big\{pa_L(\mathcal{G})\setminus\{K\}\big\}}_\text{\textbf{Q}}\cup\underbrace{\big\{pa_K(\mathcal{G}')\setminus\{L\}\big\}}_\text{\textbf{R}}\subseteq \big\{nd_L(\mathcal{G}) \cap nd_K(\mathcal{G}')\setminus\{K,L\}\big\} $. 
\end{itemize}
For this $S$, choose $x_S$ according to the condition in the definition of pairwise identifiability. We use the notation $x_S=(x_q, x_r)$, where $q\in \textbf{Q}, r\in \textbf{R}$, and we define $K^\star := K\mid \{X_S=x_S\}$ and $L^\star := L\mid \{X_S=x_S\}$.  Now we use Lemma 36 and Lemma 37 from \cite{Peters2014}.  Since $K\to L$ in $\mathcal{G}$, we get $$K^\star=\tilde{\varepsilon}_{K^\star},\,\,\,\,\,\,\,\,\,\, L^\star = f_{L^\star}(K^\star, \varepsilon_L),$$ 
where $\tilde{\varepsilon}_{K^\star} = K\mid \{X_S=x_S\}$ and $\varepsilon_L\indep K^\star$. We obtained a bivariate $\mathcal{F}$-model with $K^\star\to L^\star$. However, the same holds for the other direction; from $L\to K$ in $\mathcal{G}'$, we get $$L^\star=\tilde{\varepsilon}_{L^\star},\,\,\,\,\,\,\,\,\,\, K^\star = f_{K^\star}(L^\star, \varepsilon_K),$$ 
 where $\tilde{\varepsilon}_{L^\star} =L\mid \{X_S=x_S\}$ and $\varepsilon_K\indep L^\star$. We obtained a bivariate $\mathcal{F}$-model with $L^\star\to K^\star$, which is a contradiction. 
\end{proof}



 
\begin{customprop}{\ref{proposition_for_pairwise_F_model_identifiability}}
Let $(X_0, \textbf{X})$ follow a pairwise identifiable $\mathcal{F}$-model with DAG $\mathcal{G}$, such that all $\textbf{X}$ are neighbors of $X_0$ in $\mathcal{G}$.   Let $S \subseteq \{1, \dots, p\}$ contain a child of $X_0$ in $\mathcal{G}$.  Then, $S$ is not $\mathcal{F}$-plausible. 
\end{customprop}

 
\begin{proof}\label{Proof of proposition_for_pairwise_F_model_identifiability}
    For a contradiction, let $S$ be $\mathcal{F}_A$-plausible. Without loss of generality, let $X_1$ be a childless child of $X_0$ such that $1\in S$ (the set of children of $X_0$ is nonempty by assumption, and one of them must be childless to avoid cycles). The idea of the proof is that we define two bivariate $\mathcal{F}$-models, one with $X_0\to X_1$ and one with $X_1\to X_0$, which will lead to a contradiction with the pairwise identifiability. 

  Since $(X_0, \textbf{X})$ follow an $\mathcal{F}$-model, we can write  $X_i = f_i(\textbf{X}_{pa_i(\mathcal{G})}, \varepsilon_i)$, where $f_i\in\mathcal{F}$ and $\varepsilon_i$ are jointly independent, $i\in \{0, 1, \dots, p\}$. We use the  pairwise identifiability condition. For a specific choice $(X_0, X_1)$ and $\tilde{S} = nd_1(\mathcal{G})\setminus\{0,1\} = \{2, \dots, p\}$ (the second equality holds since $X_1$ is a childless child),  $\textbf{x}_{\tilde{S}}: p_{\tilde{S}}(\textbf{x}_{\tilde{S}})>0$ exists, satisfying the condition that a bivariate $\mathcal{F}$-model defined as
\begin{equation}\label{tyuiop}
\tilde{X}_0=\tilde{\varepsilon}_0, \tilde{X}_1 = \tilde{f}_1(\tilde{X}_0, \tilde{\varepsilon}_1)
\end{equation}
is identifiable, where  $P_{\tilde{\varepsilon}_0} = P_{X_0\mid \textbf{X}_{\tilde{S}} = x_{\tilde{S}}}    $ and $\tilde{f}_1(x, \varepsilon) =f(\textbf{x}_{pa_1\setminus\{0\}}, x, \varepsilon)$, $\tilde{\varepsilon}_1\indep \tilde{\varepsilon}_0$ . 

From the fact that $S$ is $\mathcal{F}$-plausible, $f\in\mathcal{F}$ exists, such that $\varepsilon_S:=f^\leftarrow(\textbf{X}_S, X_0)$ satisfies $\varepsilon_S\indep \textbf{X}_S, \varepsilon_S\sim U(0,1)$. Hence, we can define a model  $$\tilde{\tilde{X}}_1 = \tilde{\tilde{\varepsilon}}_1 , \tilde{\tilde{X}}_0 = \tilde{\tilde{f}}(\tilde{\tilde{X}}_1, \varepsilon_S),$$ where $P_{\tilde{\tilde{\varepsilon}}_1} = P_{X_1\mid \textbf{X}_{\tilde{S}} = x_{\tilde{S}}}    $ and $\tilde{\tilde{f}}(\textbf{x}, \varepsilon) =f(\textbf{x}_{\tilde{S}}, x, \varepsilon)$. In this model, $\varepsilon_S\indep \tilde{\tilde{\varepsilon}}_1$. 

Now, note that $(\tilde{X}_0,\tilde{X}_1)\overset{D}{=}(\tilde{\tilde{X}}_0, \tilde{\tilde{X}}_1)$, since both sides are distributed as $\big[(X_0, X_1)\mid X_{\tilde{S}}\big]$.  This is a contradiction with the identifiability of (\ref{tyuiop}). Therefore,  $S$ is not $\mathcal{F}_F$-plausible.
 

\end{proof}




\begin{customlem}{\ref{LemmaAboutUnidentifiabilityFL}}
Let $(Y, \textbf{X})\in\mathbb{R}\times \mathbb{R}^p$ follow an $\mathcal{F}_L$-model with DAG $\mathcal{G}_0$ and $pa_Y(\mathcal{G}_0)\neq\emptyset$. Then, $|S_{\mathcal{F}_L}(Y)| \leq 1$ ($|S|$ represents the number of elements of the set $S$). Moreover, if  $a,b\in an_Y(\mathcal{G}_0)$ that are d-separated in $\mathcal{G}_0$ exist, then $S_{\mathcal{F}_L}(Y) = \emptyset$. 
\end{customlem}

\begin{proof}\label{Proof of LemmaAboutUnidentifiabilityFL}
First, we show $|S_{\mathcal{F}_L}(Y)| \leq 1$. Let $a\in an_Y(\mathcal{G}_0)\cap Source(\mathcal{G}_0)$. Such $a$ exists since $pa_Y(\mathcal{G}_0)\neq\emptyset$. We show that $S = \{a\}$ is an $\mathcal{F}_L-$plausible set. 

Denote $X_0:= Y$.  Since $\mathcal{G}_0$ is acyclic, it is possible to express recursively each variable $X_j , j= 0, \dots, p, $ as a weighted sum of the noise terms $\varepsilon_0, \dots, \varepsilon_p$ that belong to the ancestors of $X_j$. Let us write the Linear SCM with notation 
\begin{equation*}
X_i = \sum_{j\in pa_i}\beta_{j,i}X_j + \varepsilon_i  = \sum_{j\in an_i}\beta_{j\to i}\varepsilon_j,
\end{equation*}
where $\beta_{j,i}$ are non-zero constants and $\beta_{j\to i}$ is the sum of distinct weighted directed paths from node $j$ to node $i$, with a convention $\beta_{j\to j} := 1$.\footnote{To provide an example of the notation, if $X_1 = \varepsilon_1, X_2 = 2X_1 + \varepsilon_2, X_3 = 3X_1 + 4X_2+\varepsilon_3$, then $X_3 = 11\varepsilon_1 + 4\varepsilon_2 + 1\varepsilon_3 = \beta_{1\to 3}\varepsilon_1 +\beta_{2\to 3}\varepsilon_2 + \beta_{3\to 3}\varepsilon_1 $.}

Using this notation, note that 
$$X_0 = \sum_{j\in an_0}\beta_{j\to i}\varepsilon_j = \beta_{a\to 0}\varepsilon_a + \sum_{j\in an_0\setminus \{a\}}\beta_{j\to i}\varepsilon_j =\beta_{a\to 0} X_a + \sum_{j\in an_0\setminus \{a\}}\beta_{j\to i}\varepsilon_j ,$$ where $X_a\indep \sum_{j\in an_i\setminus \{a\}}\beta_{j\to i}\varepsilon_j$ since $a\in Source(\mathcal{G}_0)$. Hence, $Y - \beta_{a\to 0} X_a\indep X_a$, which is almost the definition of $\mathcal{F}_L$-plausibility of set $S = \{a\}$. More rigorously, for  $S = \{a\}$, we can find $f\in\mathcal{F}_L$ such that  $f_Y^{\leftarrow}({X}_{S}, Y)\indep X_S$ and $f_Y^{\leftarrow}({X}_{S}, Y)\sim U(0,1)$. This function can be defined as 
$$
f(x, \varepsilon) = \beta_{a\to 0}x + g^{-1}(\varepsilon), \,\,\,x\in\mathbb{R}, \varepsilon\in (0,1),
$$
where $g$ is the distribution function of $(Y - \beta_{a\to 0}X_S)$. This function obviously satisfies $f\in\mathcal{F}_L$. Moreover, since $f_Y^{\leftarrow}(X_{S}, Y) = g(Y - \beta_{a\to 0}X_S)$, it holds that $f_Y^{\leftarrow}({X}_{S}, Y)\indep X_S$ and $f_Y^{\leftarrow}({X}_{S}, Y)\sim U(0,1)$, which is what we wanted to show. Hence,  $|S_{\mathcal{F}_L}(Y)|\leq 1$, since $S_{\mathcal{F}_L}(Y)\subseteq S = \{a\}$. 

Now, let $a,b\in an_Y(\mathcal{G}_0)$ that are d-separated in $\mathcal{G}_0$. Let $a', b'\in \mathcal{G}_0$ such that $a'\in \big\{an_a(\mathcal{G}_0)\cup \{a\}\big \}\cap Source(\mathcal{G}_0)$,   $b'\in \big\{an_b(\mathcal{G}_0)\cup \{b\}\big \}\cap Source(\mathcal{G}_0)$. They are well defined since the sets $an_a(\mathcal{G}_0)\cup \{a\}$,  $an_b(\mathcal{G}_0)\cup \{b\}$  must contain some source node. Since $a,b$ are d-separated,   $ \big\{an_a(\mathcal{G}_0)\cup \{a\}\big \}$ and $\big\{an_b(\mathcal{G}_0)\cup \{b\}\big \}$ are disjoint sets, $a'\neq b'$ (they are even d-separated in $\mathcal{G}_0$ \citep{Pearl}). 

Using the same argument as in the first part of the proof, since $a'\in an_Y(\mathcal{G}_0)\cap Source(\mathcal{G}_0)$, it holds that  $S = \{a\}$ is an $\mathcal{F}_L-$plausible set.  $S = \{b\}$ is also an $\mathcal{F}_L-$plausible set since $b'\in an_Y(\mathcal{G}_0)\cap Source(\mathcal{G}_0)$. Together, $S_{\mathcal{F}_L}(Y)\subseteq \{a\}$ and $S_{\mathcal{F}_L}(Y)\subseteq \{b\}$. We showed that  $S_{\mathcal{F}_L}(Y) = \emptyset$. 
\end{proof}


\begin{customlem}{\ref{lemma158}}
 Let $\mathcal{F}\subseteq\mathcal{I}_m$. Let $(X_0, \textbf{X})\in\mathbb{R}\times \mathbb{R}^p$ follow an $\mathcal{F}$-model with DAG $\mathcal{G}_0$ and $pa_{X_0}(\mathcal{G}_0)\neq \emptyset$. Let  $S\subseteq \{1, \dots, p\}$ be a non-empty set. If $(X_0, \textbf{X})$ is marginalizable to $S\cup\{0\}$, then $S_{\mathcal{F}}(X_0)\subseteq S$. 
\end{customlem}
\begin{proof}
 \label{Proof of lemma158}
 Since  $(X_0, \textbf{X})$ is marginalizable to $S\cup\{0\}$,  $(X_0, \textbf{X}_S)$ follows an $\mathcal{F}$-model. Therefore, $f_0\in\mathcal{F}$ exists, such that $X_0 = f_0(X_{\tilde{S}}, \varepsilon_0)$ for some $\tilde{S}\subseteq S$, $\varepsilon_0\indep X_{\tilde{S}}$, $\varepsilon_0\sim U(0,1)$. 
In other words, $ f_0^{\leftarrow}(X_{\tilde{S}}, X_0)\indep X_{\tilde{S}}$,  $f_0^{\leftarrow}(X_{\tilde{S}}, X_0)\sim U(0,1)$, which is exactly the definition of $\mathcal{F}$-plausibility. Hence, $\tilde{S}$ is $\mathcal{F}$-plausible and consequently,  $S_{\mathcal{F}}(X_0)\subseteq \tilde{S}\subseteq S$. 
 \end{proof}






























 
 
 
\begin{customprop}{\ref{Support_proposition}}
Let $(Y, \textbf{X})\in\mathbb{R}\times \mathbb{R}^p$ follow an SCM with DAG $\mathcal{G}_0$. Let $S\subseteq\{1, \dots, p\}$ be a non-empty set. Let  $\underline{\Psi},\overline{\Psi}: \mathbb{R}^{\mid S\mid}\to \mathbb{R}$ be real functions such that
\begin{equation*}
supp(Y\mid \textbf{X}_S=\textbf{x}) = (\underline{\Psi} (\textbf{x}),\overline{\Psi}(\textbf{x})), \,\,\,\,\,\,\, \forall \textbf{x}\in supp(\textbf{X}_S).
\end{equation*}
Moreover, let
\begin{equation} \tag{\ref{eq9987}}
\frac{Y - \underline{\Psi}(\textbf{X}_S)}{\overline{\Psi}(\textbf{X}_S) - \underline{\Psi}(\textbf{X}_S)}\not\indep \textbf{X}_S.
\end{equation}
Then, $S$ is not $\mathcal{F}_{LS}$-plausible. 
\end{customprop}

\begin{proof}\label{Proof of Support_proposition}
For a contradiction, let $S$ be  $\mathcal{F}_{LS}$-plausible. Hence, $f\in\mathcal{F}_{LS}$ exists such that 
\begin{equation}\label{eq74}
f^{\leftarrow}(\textbf{X}_S, Y)\indep\textbf{ X}_S.
\end{equation}
Since $f\in\mathcal{F}_{LS}$, we can write $f^{\leftarrow}(\textbf{x},y) = q\big(\frac{y - \mu(\textbf{x})}{\sigma(\textbf{x})}\big)$  for some functions $\mu(\cdot), \sigma(\cdot)>0$ and for some (continuous) distribution function $q(\cdot)$. Using this notation, (\ref{eq74}) is equivalent to
\begin{equation} \label{eq989}
\frac{Y - {\mu}(\textbf{X}_S)}{{\sigma}(\textbf{X}_S)}\indep \textbf{X}_S.
\end{equation}
Denote $W_{\textbf{x}}:= (Y\mid \textbf{X}_S=\textbf{x})$. From (\ref{eq989}), we get that for all $\textbf{x},\textbf{y}$ in the support of $\textbf{X}_S$, it must hold that
\begin{equation}\label{eq7285}
\frac{W_\textbf{x} - {\mu}(\textbf{x})}{{\sigma}(\textbf{x})}\overset{D}{=}\frac{W_\textbf{y} - {\mu}(\textbf{y})}{{\sigma}(\textbf{y})}.
\end{equation}
Hence, supports must also match, i.e., (\ref{eq7285}) implies
\begin{align*}
\frac{ \underline{\Psi} (\textbf{x}) - {\mu}(\textbf{x})}{{\sigma}(\textbf{x})}&\overset{}{=}\frac{ \underline{\Psi} (\textbf{y}) - {\mu}(\textbf{y})}{{\sigma}(\textbf{y})}, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
\frac{ \overline{\Psi}(\textbf{x}) - {\mu}(\textbf{x})}{{\sigma}(\textbf{x})}\overset{}{=}\frac{ \overline{\Psi}(\textbf{y}) - {\mu}(\textbf{y})}{{\sigma}(\textbf{y})},
\end{align*}
for all $\textbf{x},\textbf{y}$ in the support of $\textbf{X}_S$. Solving for $\mu, \sigma$  gives us 
\begin{align*}
{\mu}(\textbf{x})&\overset{}{=}c_1+ \underline{\Psi} (\textbf{x}) , \,\,\,\,\,\,\,\,\,\,\,
\sigma(\textbf{x})\overset{}{=}c_2\cdot [\overline{\Psi} (\textbf{x})-\underline{\Psi}(\textbf{x})],
\end{align*}
where $c_1 \in  \mathbb{R}, c_2 \in \mathbb{R}_{+}$ are some constants. Plugging this into (\ref{eq989}) gives us a contradiction with (\ref{eq9987}). 
\end{proof}


\begin{customprop}{\ref{LemmaOParetoinseparabilite}}
 Consider $(Y, \textbf{X})\in\mathbb{R}\times \mathbb{R}^p$ satisfying ( \ref{SCM_for_Y}) with  $pa_Y\neq \emptyset$ and $\mathcal{F} = \mathcal{F}_{F}$. where $F$ be a distribution function whose parameter acts multiplicatively. 
 Let $\textbf{X}_{pa_Y}$ be a continuous random vector with full support and independent components. 
\begin{itemize}
\item Consider $f_Y\in\mathcal{F}_F$ in the form $f_Y(\textbf{x}, \varepsilon)=F^{-1}\big(\varepsilon, \theta(\textbf{x})\big)$ with additive function $\theta(x_1, \dots, x_k) = h_1(x_1)+\dots + h_k(x_k)$, where $h_i$ are continuous non-constant real functions. Then, then every $S\subsetneq pa_Y$ is not $\mathcal{F}_{F}$-plausible. 
\item Consider $f_Y\in\mathcal{F}_F$ in the form $f_Y(\textbf{x}, \varepsilon)=F^{-1}\big(\varepsilon, \theta(\textbf{x})\big)$ with multiplicative function   $\theta(x_1, \dots, x_k) = h_1(\textbf{x}_S)\cdot h_2(\textbf{x}_{\{1, \dots, k\}\setminus S})$ for some $S\subsetneq \{1, \dots, k\}$, where $h_1, h_2$ are continuous non-constant non-zero real functions. Then, $S_{\mathcal{F}_F}(Y)=\emptyset$.
\end{itemize}
\end{customprop}
\begin{proof}\label{Proof of LemmaOParetoinseparabilite}
\textbf{The first bullet-point}: For a contradiction, consider that $S=\{1, \dots, s\}\subset \{1, \dots, k\}$ is $\mathcal{F}_{F}$-plausible. Then for almost all $z\in(0,1)$, there exist $g\in\mathcal{F}_F$ such that  $g^{\leftarrow}\big(\textbf{X}_S, f(\textbf{X}, z)\big)\indep \textbf{X}_S$. Since $g\in\mathcal{F}_F$, we can write $g^{\leftarrow}(\textbf{x}_S, \cdot) = F\big(\cdot, \theta_g(\textbf{x}_S)\big)$ for some non-constant function $\theta_g$. Hence, 
$$\textbf{X}_S\indep g^{\leftarrow}\big(\textbf{X}_S, f(\textbf{X}, z)\big)
= F[F^{-1}\big(z, \theta(\textbf{X})\big), \theta_g(\textbf{X}_S)]
= f_1[z, f_2\big(\theta_g(\textbf{X}_S)\big) \cdot\theta(\textbf{X})].$$ 
We use identity (\ref{trivial_identity}). Since $f_1$ is invertible, we obtain 
\begin{equation}\label{dfrge}
\textbf{X}_S\indep f_2\big(\theta_g(\textbf{X}_S)\big) \cdot\theta(\textbf{X}).
\end{equation}
Define $\tilde{\theta}_g(\textbf{X}_S):=f_2\big(\theta_g(\textbf{X}_S)\big)$. Finally, since $\theta(\textbf{X})$ is an additive function from the assumptions, (\ref{dfrge}) is equivalent to
$$
\tilde{\theta}_g(\textbf{X}_S)[h_1(X_1) + \dots + h_k(X_k)]\indep \textbf{X}_S. 
$$
However, that is a contradiction with Lemma \ref{CoolLemma} part 2. 

\textbf{The second bullet-point}:  
We show that $S$ is $\mathcal{F}_F$-plausible set by finding an appropriate function $g\in\mathcal{F}_F$ such that $g^{\leftarrow}\big(\textbf{X}_S, f_Y(\textbf{X}, \varepsilon_Y)\big)\indep \textbf{X}_S$. Since it must hold that  $g\in\mathcal{F}_F$, we write  $g^{\leftarrow}(\textbf{x}_S, \cdot) = F\big(\cdot, \theta_g(\textbf{x}_S)\big)$ for some $\theta_g$. 

Rewrite 
 $$g^{\leftarrow}\big(\textbf{X}_S, f(\textbf{X}, \varepsilon_Y)\big) = F[F^{-1}\big(\varepsilon_Y, \theta(\textbf{X})\big)     ,\theta_g(\textbf{X}_S)]
 = f_1[\varepsilon_Y, f_2\big(\theta_g(\textbf{X}_S)\big) \cdot\theta(\textbf{X})],$$
where $f_1, f_2$ are from (\ref{postMultiplDefinition}). 
We choose $\theta_g$ such that $f_2\big(\theta_g(\textbf{x}_S)\big) = \frac{1}{h_1(\textbf{x}_S)}$. Obviously, $g\in\mathcal{F}_F$. Then, by extending $\theta$ to its multiplicative form, we get 
$$ f_1[\varepsilon_Y, f_2\big(\theta_g(\textbf{X}_S)\big) \cdot\theta(\textbf{X})] =  f_1[\varepsilon_Y,  h_2(\textbf{X}_{\{1, \dots, k\}\setminus S})]\indep \textbf{X}_S.$$
Together, we found $g\in\mathcal{F}_F$ defined by $g^{\leftarrow}(\textbf{x}_S, \cdot) = F\big(\cdot, f_2^{-1}(\frac{1}{h_1(\textbf{x}_S)})\big)$ that satisfy   $g^{\leftarrow}\big(\textbf{X}_S, f(\textbf{X}, \varepsilon_Y)\big)\indep \textbf{X}_S$. Hence, $S$ is $\mathcal{F}_F$-plausible. The analogous argument can be given for the set $\{1, \dots, k\}\setminus S$. Hence,  $S_{\mathcal{F}_F}(Y)\subseteq S\cap( \{1, \dots, k\}\setminus S) = \emptyset$.
\end{proof}


\begin{customprop}{\ref{LemmaOLocationScaleinseparabilite}}
 Consider $(Y, \textbf{X})\in\mathbb{R}\times \mathbb{R}^p$ satisfying ( \ref{SCM_for_Y}) with  $pa_Y\neq \emptyset$ and $\mathcal{F} = \mathcal{F}_{LS}$. 
 Let $\textbf{X}_{pa_Y}$ have full support and independent components. 

 Let $f_Y\in\mathcal{F}_{LS}$  have the form $f_Y(\textbf{x}, \varepsilon)=\mu(\textbf{x}) + \sigma(\textbf{x})\varepsilon$, where $\theta(\textbf{x}) = \big(\mu(\textbf{x}), \sigma(\textbf{x})\big)^\top$ is additive in both components, that is,  
$\mu(\textbf{x}) = h_{1, \mu}(x_1)+\dots + h_{k, \mu}(x_k)$ and 
$\sigma(\textbf{x}) =h_{1, \sigma}(x_1)+\dots + h_{k, \sigma}(x_k)$ for some continuous non-constant non-zero functions $h_{i,\cdot}$, where we also assume $h_{i,\sigma}>0$, $i=1, \dots, k$.  Then, then every $S\subsetneq pa_Y$ is not $\mathcal{F}_{LS}$-plausible. 
\end{customprop}
\begin{proof}
\label{Proof of LemmaOLocationScaleinseparabilite}
For a contradiction, consider that $S\subsetneq \{1, \dots, pa_Y\}$, is $\mathcal{F}_{LS}$-plausible. Without loss of generality, let $S=\{1, \dots, s\}$ for $s<k=|pa_Y|$. Then, $g\in\mathcal{F}_{LS}$ exist such that  $g^{\leftarrow}\big(\textbf{X}_S, Y\big)\indep \textbf{X}_S$. Since $g\in\mathcal{F}_{LS}$, we can write $g(\textbf{x}_S, e) = \mu_g(\textbf{x}_S) + \sigma_g(\textbf{x}_S)q^{-1}(e)$ for some function $\theta_g=(\mu_g, \sigma_g)$ that is irreducible and hence non-constant in neither of the arguments. Inverse of such a function is in the form $g^{\leftarrow}(\textbf{x}_S, e) = q(\frac{e - \mu_g(\textbf{x}_S)}{\sigma_g(\textbf{x}_S)})$. 

Hence, simply rewriting   
$$\textbf{X}_S\indep g^{\leftarrow}\big(\textbf{X}_S, Y\big)
= q\bigg(\frac{Y - \mu_g(\textbf{X}_S)}{\sigma_g(\textbf{X}_S)}\bigg),$$
and using \ref{trivial_identity} and $Y = \mu(\textbf{X}_{pa_Y}) + \sigma(\textbf{X}_{pa_Y})\varepsilon_Y$ we get 
\begin{equation}\label{dfrgeTRI}
\textbf{X}_S\indep \frac{\mu(\textbf{X}_{pa_Y}) + \sigma(\textbf{X}_{pa_Y})\varepsilon_Y - \mu_g(\textbf{X}_S)}{\sigma_g(\textbf{X}_S)}. 
\end{equation}
Equation (\ref{dfrgeTRI}) can be equivalently rewritten into
\begin{equation}\label{erty}
\textbf{X}_S\indep f_1(\textbf{X}_S)  + f_2(\textbf{X}_S)h(\textbf{X}_{S^c}, \varepsilon_Y),
\end{equation}
where $ S^c=\{1, \dots, k\}\setminus S$, and 
%Mozno tam je clen navyse, ale na tom nezalezi
$$f_1(\textbf{x}) =\frac{ h_{1, \mu}(x_1)+\dots + h_{s, \mu}(x_s)- \mu_g(\textbf{x})}{\sigma_g(\textbf{x})},$$ $$f_2(\textbf{x}) =  \frac{ h_{1, \mu}(x_1)+\dots + h_{s, \mu}(x_s)}{\sigma_g(\textbf{x})},$$
$$
h(\textbf{x}, \varepsilon) = h_{s+1, \mu}(x_{s+1})+\dots + h_{k, \mu}(x_k) + [h_{s+1, \sigma}(x_{s+1})+\dots + h_{k, \sigma}(x_k)]\varepsilon.
$$
However, independence (\ref{erty}) is a contradiction with Lemma \ref{CoolLemma} part 4. 
    
\end{proof}


\begin{customthm}{\ref{Theorem_in_section2}}

 Consider $(Y, \textbf{X})\in\mathbb{R}\times \mathbb{R}^p$ satisfying (\ref{SCM_for_Y}) with  $pa_Y\neq \emptyset$ and $\mathcal{F} = \mathcal{F}_A$. 
 Let $\textbf{X}_{pa_Y}$ have independent components. 
 
  \begin{itemize}
     \item If $f_Y$ has a form \begin{equation}
        f_Y(\textbf{x}, e) = h_1(\textbf{x}_S) + h_2(\textbf{x}_{pa_Y\setminus S}) + q^{-1}(e), \,\,\,\,\,\textbf{x}\in\mathbb{R}^{|pa_Y|}, e\in(0,1),\tag{\ref{efaetgfas}}
     \end{equation}
 for some non-empty $S\subset pa_Y$, where $h_1, h_2$ are measurable functions and $q^{-1}$ is a quantile function. Then, $S_{\mathcal{F}_A}(Y) = \emptyset$.
     \item If $f_Y\in\mathcal{F}_A$ is continuous injective\footnote{Remark \ref{remark_lemma_additivity} in Appendix~\ref{Appendix_Auxiliary} shows an example where a periodic function $f$ contradicts this statement. We conjecture that the assumption about injectivity of $f$ can be relaxed to non-periodicity of $f$.    } function that cannot be written as (\ref{efaetgfas}), then every $S\subsetneq pa_Y$ is not $\mathcal{F}_A$-plausible. 
 \end{itemize}
\end{customthm}
 

\begin{proof}
\label{Proof of Theorem_in_section2}

    \textbf{The first bullet-point}:  We show that sets $S$ and $S^c:=pa_Y\setminus S$ are both $\mathcal{F}_A$-plausible. Consider a function $f\in\mathcal{F}_A$ satisfying $f^{\leftarrow}(\textbf{X}_S, Y):= \tilde{q}\big(Y - h_1(\textbf{X}_S)\big)$, where $\tilde{q}$ is a distribution function of $[h_2(\textbf{X}_{pa_Y\setminus S}) + q^{-1}(\varepsilon_Y)]$. Then, $f^{\leftarrow}(\textbf{X}_S, Y)\indep \textbf{X}_S$ since $Y - h_1(\textbf{X}_S) = h_2(\textbf{X}_{pa_Y\setminus S}) + q^{-1}(\varepsilon_Y)\indep \textbf{X}_S$ and we apply identity (\ref{trivial_identity}). Moreover, $f^{\leftarrow}(\textbf{X}_S, Y)\sim U(0,1)$ trivially. We showed that set $S$ satisfies every property for being  $\mathcal{F}_A$-plausible. Set $S = pa_Y\setminus S$ can be analogously shown to be  $\mathcal{F}_A$-plausible as well. Therefore,  $S_{\mathcal{F}_A}(Y)\subseteq S\cap (pa_Y\setminus S)) = \emptyset$.

\textbf{The second bullet-point}: Let us write $Y = f_0(X_1, \dots, X_{|pa_Y|}) + q^{-1}(\varepsilon_Y)$, where $\varepsilon_Y\indep \textbf{X}_{pa_Y}$ for some continuous injective function $f_0$ and continuous quantile function $q^{-1}$.  For a contradiction, consider that an  $\mathcal{F}_A$-plausible non-empty set $S\subset pa_Y$ exists. That means, $f\in\mathcal{F}_A: \mathbb{R}^{|S|+1}\to\mathbb{R}$ exists such that (\ref{Definition_F_plausible}) holds. Since $f\in\mathcal{F}_A$, we can write $f(\textbf{x}, e) = \mu(\textbf{x}) + \tilde{q}^{-1}(e), \,\,\textbf{x}\in\mathbb{R}^{|S|}, e\in (0,1)$ for some measurable function $\mu$ and quantile function $\tilde{q}^{-1}$. Additive functions have an inverse in a form $f^{\leftarrow}(\textbf{x}, y) = \tilde{q}\big(y-\mu(\textbf{x})\big),\,\,\textbf{x}\in\mathbb{R}^{|S|}, y\in\mathbb{R}$ (see discussion in Appendix~\ref{Appendix_A.1.}).   Using (\ref{Definition_F_plausible}) and identity \ref{trivial_identity}, we have $Y- \mu(\textbf{X}_S)\indep \textbf{X}_S$. 

Hence, we have  
\begin{align*}
    Y- \mu(\textbf{X}_S)&\indep \textbf{X}_S 
    \\ f_0(X_1, \dots, X_{|pa_Y|}) + q^{-1}(\varepsilon_Y) - \mu(\textbf{X}_S) &\indep \textbf{X}_S 
    \\f_0(X_1, \dots, X_{|pa_Y|}) - \mu(\textbf{X}_S) &\indep \textbf{X}_S. 
\end{align*}
This is a contradiction with Lemma \ref{CoolLemma} part 1.    
\end{proof}

\begin{customlem}{\ref{lemma_hidden_confounder}}

 Consider $(Y, \textbf{X})\in\mathbb{R}\times \mathbb{R}^p$  satisfy ( \ref{SCM_for_Y}) with  $\mathcal{F} = \mathcal{F}_A$. Consider $\emptyset\neq hid\subset pa_Y$. Let $S\subseteq pa_Y \cap obs$ and $\tilde{S}:=(pa_Y\cap obs)\setminus S$ such that
 $(\textbf{X}^{hid}, \textbf{X}_S)\indep \textbf{X}_{\tilde{S}}$ (one can consider that $\textbf{X}^{hid}$ cause $\textbf{X}_S$ and $Y$, but not $\textbf{X}_{\tilde{S}}$). 
 
 If $f_Y$ has a form \begin{equation*}
        f_Y(\textbf{x}, e) = h_1(\textbf{X}^{hid}, \textbf{X}_S) + h_2(\textbf{X}_{\tilde{S}}) + q^{-1}(e), \,\,\,\,\,\textbf{x}\in\mathbb{R}^{|pa_Y|}, e\in(0,1),
     \end{equation*}
for some continuous non-constant real functions  $h_1, h_2$ and a quantile function $q^{-1}$. Then, $S_{\mathcal{F}_A}(Y) \subseteq \tilde{S}\subset pa_Y$.
\end{customlem}
\begin{proof}
\label{Proof of lemma_hidden_confounder}
The set $\tilde{S}$ is $\mathcal{F}_A$-plausible since  $Y - h_2(\textbf{X}_{\tilde{S}}) =h_1(\textbf{X}^{hid}, \textbf{X}_S) + q^{-1}(\varepsilon_Y)  \indep \textbf{X}_{\tilde{S}}$. Therefore $S_{\mathcal{F}_A}(Y)\subseteq\tilde{S}$. 
\end{proof}


\begin{customprop}{\ref{Proposition_consistency}}
Consider $\mathcal{F} = \mathcal{F}_A$ and 
let $(Y, \textbf{X})\in\mathbb{R}\times \mathbb{R}^p$ follow an SCM with DAG $\mathcal{G}_0$ satisfying ( \ref{SCM_for_Y}). Assume that every $S \neq pa_Y$ is not $\mathcal{F}$-plausible. 
Then,   

\begin{equation}
  \lim_{n\to\infty} \mathbb{P}(\widehat{pa}_Y  \neq pa_Y) = 0,
\end{equation}   
where $n$ is the size of the random sample and $\widehat{pa}_Y$ is our score-based estimate from Section~\ref{Section_algorithm2} with $\lambda_1, \lambda_2>0, \lambda_3 = 0$, suitable estimation procedure, and HSIC independence measure. 
\end{customprop}
\begin{proof}
\label{Proof of Proposition_consistency}
This result is a simple consequence of Theorem 20 in \cite{reviewANMMooij}. We use the same notation. For a rigorous definition of $HSIC$ and $\widehat{HSIC}$, see Appendix A.1 in \cite{reviewANMMooij}. 



We show that $score(S)> score(pa_Y)$ as $n\to\infty$ for any $S\neq pa_Y$. 
The $score(S)$ is defined as the weighted sum of  \textit{Independence} and \textit{Significance} terms. Let us first concentrate on the former. By definition, we write $\textit{Independence} = -\widehat{HSIC}(\textbf{X}_S, \hat{\varepsilon}_S)$. On a population level, it holds (Lemma~12 in \cite{reviewANMMooij}) that  ${HSIC}(\textbf{X}_S, {\varepsilon}_S) > 0 $ and ${HSIC}(\textbf{X}_{pa_Y}, {\varepsilon}_{pa_Y}) = 0$, since $\textbf{X}_S$ and ${\varepsilon}_S$ are not independent (because $S$ is not $\mathcal{F}$-plausible) and  $\textbf{X}_{pa_Y}$ and ${\varepsilon}_{pa_Y}$ are independent (by definition of the SCM). 
By Theorem~20 in \cite{reviewANMMooij}, we obtain $\widehat{HSIC}(\textbf{X}_{pa_Y}, \hat{\varepsilon}_{pa_Y})\to {HSIC}(\textbf{X}_{pa_Y}, {\varepsilon}_{pa_Y})=0$ and $\widehat{HSIC}(\textbf{X}_{S}, \hat{\varepsilon}_{S})\to {HSIC}(\textbf{X}_{S}, {\varepsilon}_{S})>0$, as $n\to\infty$. Therefore, the independence term is strictly smaller (for some large $n$) for $S$ than for $pa_Y$. 

Let us focus on \textit{Significance} term\footnote{We work with the \textit{Significance} term somewhat vaguely in this proof. However, we only need $Significance\to 0$ as $n\to\infty$ for $pa_Y$, which is satisfied for any reasonable method of assessing significance of covariates.}. Since all $\textbf{X}_{pa_Y}$ are significant (otherwise $f_Y\notin\mathcal{I}_m$) we get that $Significance\to 0$ as $n\to\infty$ for $pa_Y$. Moreover, by definition, always $Significance\geq 0$.

Together, we find that $score(pa_Y)>score(S)$ for large $n$, since 
the \textit{Independence} term is strictly smaller (for large $n$) for $S$ than for $pa_Y$ and \textit{Significance} term converges to $0$ for $pa_Y$ and is non-negative.  
We showed that $pa_Y$ has the largest score among all $S\subseteq \{1, \dots, p\}$ (for $n$ large enough). 
\end{proof}

