\section{Introduction}

Causal reasoning holds great significance in numerous fields, including public policy, decision-making and medicine \citep{Holland, TheBookOfWhy}. Randomized control experiments are widely accepted as the gold standard method for determining causal relationships \citep{Pearl_causal_diagrams_biometrika, Rubin}. However, the feasibility of such experiments is often hindered by their high costs and ethical concerns. In such a case, it is important to estimate causal relations from observational data, which are obtained by observing a system without any interventions \citep{Elements_of_Causal_Inference}. 

A classical approach for causal discovery, in the context of unknown causal structure, is to characterise the Markov equivalence class of structures (MEC, \cite{Meek}); however, the full causal structure can be unidentifiable. 
A variety of research papers have proposed various methodologies to deal with unidentifiable structures. These methods are either structural-restriction-based, meaning we add some additional assumptions about the functional relations between the variables, such as assuming nonlinear Gaussian data-generation process \citep{hoyer2008, peters2013identifiability,  Chen_biometrika_causal_discovery, Nonlinear_Causal_Discovery_with_Confounders, bodik2023identifiability}; score-based, meaning we pick a causal structure with the best fit on the data according to some score function \citep{Greedy_search, Score-based_causal_learning}; or information-theory-based, using entropy, mutual information and approximations of Kolmogorov complexity \citep{IGCI, Slope, Natasa_Tagasovska}.  However, these methods were designed either for a bivariate case or to infer the entire causal structure of the system. 

In contrast, we focus on inferring only a local structure rather than a global one. This decision is motivated by the expectation that it may lead to simpler, faster, more robust, accurate, and powerful procedures. The assumptions needed for the identifiability are less strict; to be more specific, our methodology relies on so-called local causal sufficiency, an assumption much weaker than the (global) causal sufficiency. 


Some methodologies have been proposed to infer a local structure around a target variable. These methods are typically divided into three categories: learning a local skeleton (unoriented graph), learning a minimal Markov blanket (a sufficient set), or learning a set of direct causes of the target variable (goal of this paper). Under causal sufficiency and faithfulness \citep{Pearl_book}, the PC algorithm \citep{PCalgorithm} can identify the MEC and consistently learn the skeleton of the full structure. \cite{Local_causal_discovery_2008}, \cite{Local_Causal_discovery2010},  \cite{Wang2014_local_DAG_learning} and \cite{Ling_local_causal_discovery} discuss modifications of the PC algorithm focusing only on the local structure. 
\cite{Local_Causal_discovery_Gao_2021} suggest a methodology for estimating the minimal Markov blanket based on comparing entropies of the variables.  
\cite{Peters_invariance} introduced an invariance causal prediction method (ICP) for estimating the direct causes of the target variable.
\cite{Local_Causal_discovery_Mona_Azadkia} propose a method to learn the direct causes of the target variable under the assumption that the causes are identifiable (specifically, assuming that the underlying structure is a polytree). In contrast, our work focuses on the identifiability of the direct causes and aims to distinguish between different local causal structures (local MEC) using an structural-restrictions-based approach, where we take the ideas from classical approaches and use them locally. 


\subsection{General identifiability of a causal graph}


Consider a DAG (directed acyclic graph) $\mathcal{G}=(V,E)$ with a finite set of vertices (nodes) $V$ and a set of directed edges $E$. We adapt the usual notation of graphical models  (e.g., \citealp{PCalgorithm}); for example, we write $pa_i$, $ch_i$ and $an_i$ for parents, children and ancestors of the node $i$, respectively. Consider a random vector $(X_i)_{i\in V}$ over a probability space $(\Omega, \mathcal{A}, P)$, and we denote $\textbf{X}_S = \{X_s{:}\,\,  s\in S\}$ for $S\subseteq V$. For simplicity, we denote $V = \{0, \dots, p\}$, where $X_0$ (usually denoted by $Y$) is a target variable and $\textbf{X}=(X_1, \dots, X_p)^\top$ are other variables. 

A \textbf{structural causal model} (SCM) with a DAG $\mathcal{G}$ over $(X_0,\textbf{X})$ represents a data-generating process where the variables arise from structural equations 
\begin{equation}\label{definition_general_SCM}
    X_i=f_i(\textbf{X}_{pa_i}, \eta_i),\,\,\,\,\,\,\,\,\, f_i\in\mathcal{F}_i,\,\,\,\,\,\,\,i=0,1,\dots, p, 
\end{equation}
where $f_i\in\mathcal{F}_i$ are the assignments (link functions), $\mathcal{F}_i$ are some subsets of measurable functions, and $\eta_i$ are jointly independent random variables. $X_j$ is called a direct cause of $X_i$ if $j\in pa_i$. A set of variables in a SCM is said to be \textbf{causally sufficient} if there is no hidden common cause that is causing more than one variable. $\textbf{X}$ is \textbf{locally causally sufficient} for $X_0$ if there is no hidden common cause that is causing both $X_0$ and a parent of $X_0$ in $\textbf{X}$. 

We say that $\mathcal{G}$ is \textbf{identifiable} under $(\mathcal{F}_0, \dots, \mathcal{F}_p)$ from the joint distribution of $\textbf{X}$ (we also say that the causal model is identifiable) if there is no DAG $\mathcal{G}'\neq \mathcal{G}$ and functions $f_i'\in\mathcal{F}_i, i=0, \dots, p$ generating the same joint distribution. 
We say that the SCM follows an $\mathcal{F}$\textbf{-model}, if $\mathcal{F}_i = \mathcal{F}$ for all $i=0, 1, \dots, p$ ; in other words, each structural equation in the SCM satisfies $f_i\in\mathcal{F}, i=0, \dots, p$. 

A large number of $\mathcal{F}$-models were proposed in the literature when the full graph $\mathcal{G}$ is identifiable. \cite{Lingam} show that $\mathcal{G}$ is identifiable under the LiNGaM model (Linear Non-Gaussian additive Models where $\mathcal{F}$ consists of all linear functions and the noise variables are non-Gaussian).
\cite{hoyer2008} and \cite{Peters2014}  developed a framework for additive noise models (ANM) where $\mathcal{F}$ consists of functions additive in the last input, that is,  $X_i = g(\textbf{X}_{pa_i}) +  \eta_i$. 
\cite{Zhang2009} consider the post-nonlinear (PNL) causal model where $\mathcal{F}$ consists of post-additive functions, that is, $
X_i = g_1\big(g_2(\textbf{X}_{pa_i}) +  \eta_i\big)$ with an invertible link function $g_1$.  Note that the former two are special cases of the PNL model. 
 \cite{Khemakhem_autoregressive_flows}, \cite{immer2022identifiability}, and \cite{strobl2022identifying} propose several methods and identifiability results for location-scale models, where  $\mathcal{F}$ consists of location-scale functions, that is, $X_i = g_1(\textbf{X}_{pa_i}) +  g_2(\textbf{X}_{pa_i})\eta_i.$
\cite{bodik2023identifiability} proved the identifiability of $\mathcal{G}$ for a class of conditionally parametric causal models ($CPCM(F)$), where the data-generation process is of the form 
\begin{equation}\label{CPCM_def}
X_i=f_i(\textbf{X}_{pa_i}, \varepsilon_i) = F^{-1}\big(\varepsilon_i; \theta_i(\textbf{X}_{pa_i})\big), \,\,\,\,\,\text{ equivalently } X_i\mid \textbf{X}_{pa_i}\sim F\big(\theta_i(\textbf{X}_{pa_i})\big),
\end{equation}
where $\varepsilon_i\sim U(0,1),$ and $F$ is a known distribution function with parameters $\theta_i\in\mathbb{R}^q$ being functions of the direct causes of $X_i$. Note that if $F$ is Gaussian, then we are in the Gaussian Location-scale Models frame-work, in which case (\ref{CPCM_def}) is equivalent to $X_i = \mu_i(\textbf{X}_{pa_i}) + \sigma_i(\textbf{X}_{pa_i})\eta_i,$ where $\eta_i$ is normally distributed and $\theta_i = (\mu_i, \sigma_i)$. 




However, we do not require full identifiability of $\mathcal{G}$ to identify the parents of the target variable. The ICP method \citep{Peters_invariance} does not assume a pre-specified $\mathcal{F}$-model, but rather assumes that the target variable $Y = X_0$ is structurally generated as 
\begin{equation}\label{qwertyuiop}
Y = f_Y(\textbf{X}_{pa_Y}, \eta_Y),\,\,\,\,\,\,\, f_Y \in \mathcal{F}_A, \,\,\,\,\,\,\, \eta_Y \indep \textbf{X}_{pa_Y},
\end{equation}
where $\mathcal{F}_A$ is a space of measurable additive\footnote{The original method introduced in \cite{Peters_invariance} considered only linear functions, while \cite{Christina} generalized the results for non-linear additive functions. Recently, a more general framework related to the ICP was proposed by \cite{Mooij2020}.} functions. The authors additionally assume a multi-environmental setting; that is, we assume that we observe an environmental variable $E$ that is an ancestor of $Y$ but not its parent. This assumption allows for the identification of (a subset of) the parents of $Y$ due to the following invariance property. A set $S \subseteq \{1, \dots, p\}$ is called an $E$-plausible set of causal predictors if $Y \indep E \,| X_{S}$, and 
$$S_E(Y) := \bigcap_{\substack{\tilde{S} \subseteq \{1, \dots, p\}\\{\tilde{S} \text{ is E-plausible}}}} \tilde{S}$$ 
is called the $E$-identifiable set of causal predictors (note that we slightly modified the notation from the original paper). It can be shown that always $S_E(Y) \subseteq pa_Y$.  However, $S_E(Y) = pa_Y$ only if $E$ is ``rich'' enough \citep[Section 4]{Peters_invariance}. 

In this work, we do not require the existence of an environmental variable $E$, but only consider one (observational) dataset. Instead, we restrict the functional space of $f_Y$ to achieve the identifiability of the direct causes of $Y$. For example, we demonstrate that in many scenarios, we can identify (a subset of) the parents of $Y$ assuming \textit{only} (\ref{qwertyuiop}). This contributes to innovative theoretical developments that are relevant for causal identifiability.


\subsection{Main idea of our framework}

We assume the data-generation process of $Y$ in the form 
\begin{equation}\label{SCM_for_Y}
Y=f_Y(\textbf{X}_{pa_Y}, \varepsilon_Y),\,\,\,\,\,\,\, f_Y\in\mathcal{F}, \,\,\,\,\,\,\,\varepsilon_Y\indep \textbf{X}_{pa_Y}, \,\,\,\,\,\,\,\varepsilon_Y\sim U(0,1) \,\,\,\,   \footnotemark. \tag{$\bigstar$ } 
\end{equation}
\footnotetext{Note that we distinguish between the notation of uniformly distributed noise variables (denoted by $\varepsilon_{Y}$) and arbitrarily distributed noise variables (denoted by $\eta_{Y}$). Under the relation $q^{-1}(\varepsilon_Y) = \eta_Y$ where $q^{-1}$ is the quantile function of $\eta_Y$, these cases are equivalent and we use the former notation in the paper without loss of generality.}
Following the structural-restrictions-based ideology, we assume $f_Y\in\mathcal{F}$, where $\mathcal{F}$ is a subset of all measurable functions (for example, a space of all linear functions). It is important to note that we do not assume the existence of a SCM over $(Y, \textbf{X})$. Our goal is to estimate the set $pa_Y$ from a random sample from $(Y, \textbf{X})$. 

Throughout the paper, we assume the following two assumptions: $f_Y$ is \textit{invertible} and \textit{minimal almost surely}, represented by the notation $f_Y\in\mathcal{I}_m$. Invertibility means that the noise variables can be recovered from the observed variables; that is, a function $f_Y^\leftarrow$ exists such that $\varepsilon_Y = f_Y^\leftarrow(\textbf{X}_{pa_Y},Y).$ Minimality represents the property that there does not exist a lower-dimensional function $g$ and $k\leq m\in\mathbb{N}$ such that $f_Y(x_1, \dots, x_m) = g(x_1, \dots, x_{k-1}, x_{k+1}, \dots, x_m)$ almost surely. The assumption of minimality of a function is closely related to causal minimality. For more details and a rigorous definition of the class of functions $\mathcal{I}_m$, see Appendix \ref{Appendix_A.1.}. Overall, we assume that $f_Y\in\mathcal{F}\subseteq\mathcal{I}_m$. 

Our framework is built on the notion of $\mathcal{F}$-identifiability that we now present.  Recall that, without loss of generality, we assume $\varepsilon_Y\sim U(0,1)$.
\begin{definition}\label{Definition1}
A non-empty set $S\subseteq\{1, \dots, p\}$ is called an $\mathcal{F}$-\textbf{plausible} set of parents of $Y$, if 
\begin{equation}\label{Definition_F_plausible}
f\in\mathcal{F} \text{ exists such that for } \varepsilon_S := f^{\leftarrow}(\textbf{X}_{S}, Y)\text{, it holds that }\varepsilon_S\indep \textbf{X}_S, \,\,\,  \varepsilon_S\sim U(0,1).
\end{equation}
We define a set of $\mathcal{F}$-\textbf{identifiable} parents of $Y$ as follows:
\begin{equation*}\label{definition_F_identifiable}
S_\mathcal{F}(Y):= \bigcap_{\substack{S\subseteq \{1, \dots, p\}, S\neq \emptyset\\{ \text{ S is an } \mathcal{F} \text{-plausible}}\\{\text{set of parents of Y}}  }}S.
\end{equation*}     \end{definition}

The constrains on the functional class $\mathcal{F}$ correspond to the data-generation process of $Y$.
If we assume linearity of the covariates, this represents the assumption $f_Y\in\mathcal{F}_L$, where
\begin{equation*}
\mathcal{F}_L = \{f\in\mathcal{I}_m: f(\textbf{x}, \varepsilon) = \beta^T\textbf{x} + q^{-1}(\varepsilon) \text{ for some quantile function }q^{-1} \text{ and } \beta\in\mathbb{R}^{|\textbf{x}|}\}.
\end{equation*}
Note that the restriction $f\in\mathcal{I}_m$ guarantees that the arguments $\beta_i\neq 0$ for all $i$. On the other hand, if we assume that the distribution of $Y\mid \textbf{X}_{pa_Y}$ belongs to a family $F$ (such as Gaussian family), this corresponds to assuming $f_Y\in\mathcal{F}_F$ where 
\begin{equation*}
\mathcal{F}_F := \{f\in\mathcal{I}_m: f(\textbf{x},\varepsilon) =  F^{-1}(\varepsilon;\theta(\textbf{x})) \text{ for some function } \theta\}.
\end{equation*}
We call this restriction conditionally parametric causal model assumption ($CPCM(F)$, see (\ref{CPCM_def})). Table~\ref{tableDefinitions} lists all functional spaces considered in this paper.  



\begin{table}[h]
\begin{tabular}{|l|}
\hline
\multicolumn{1}{|c|}{Summary of different $\mathcal{F}\subset \mathcal{I}_m$ used in the paper}                                                                                                                    \\ \hline
$\mathcal{F}_L = \{f\in\mathcal{I}_m: f(\textbf{x}, \varepsilon) = \beta^T\textbf{x} + q^{-1}(\varepsilon) \text{ for some quantile function }q^{-1} \text{ and }\beta\in\mathbb{R}^{|\textbf{x}|}\}$                                \\ \hline
$\mathcal{F}_A = \{f\in\mathcal{I}_m: f(\textbf{x}, \varepsilon) = \mu(\textbf{x}) + q^{-1}(\varepsilon) \text{ for some }\mu(\cdot)\text{ and quantile function }q^{-1}     \}$        \\ \hline
$\mathcal{F}_{LS} = \{f\in\mathcal{I}_m: f(\textbf{x}, \varepsilon) = \mu(\textbf{x}) + \sigma(\textbf{x}) q^{-1}(\varepsilon) $, \,\,\,\,\,\ for some function $\mu$,\\
\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,$\text{ positive function }\sigma\,\text{ and a quantile function }q^{-1}\}$\\ \hline
$\mathcal{F}_F := \{f\in\mathcal{I}_m: f(\textbf{x},\varepsilon) =  F^{-1}(\varepsilon;\theta(\textbf{x})) \text{ for some function } \theta:\mathbb{R}^{|\textbf{x}|}\to\mathbb{R}^q\}$         \\ \hline
\end{tabular}
\caption{The table summarizes different functional spaces $\mathcal{F}$ used in the paper. $\mathcal{F}_L$, $\mathcal{F}_A$, $\mathcal{F}_{LS}$, and $\mathcal{F}_F$ correspond to the linearity assumption, additivity assumption, location-scale assumption, and $CPCM(F)$ assumption, respectively.   }
\label{tableDefinitions}
\end{table}


The concept of \(\mathcal{F}\)-identifiability provides theoretical limitations for causal estimates under the assumption \(f_Y \in \mathcal{F}\). The set of \(pa_Y\) can be impossible to estimate (or even ill-defined) even with an infinite number of observations. However, we can consistently estimate the set \(S_{\mathcal{F}}(Y)\) (as discussed in Section~\ref{section_algorithm}). The main focus of this paper is to determine which elements belong to \(S_{\mathcal{F}}(Y)\): When does it hold that \(S_{\mathcal{F}}(Y) = pa_Y\)? In the following, we present two examples to provide an initial assessment of the findings presented in Section~\ref{Section_3}.


\begin{example}[3 variable case]\label{example_3_variable_case_introduction}
Consider the following structural causal model: $\mathcal{G}$ is in the form $ X_1 \to Y \to X_2$, where $Y$ is generated as $Y = f_Y(X_1, \varepsilon_Y) =  f_0(X_1) + q^{-1}(\varepsilon_Y)$, with $\varepsilon_Y \indep X_1$, for some non-constant function $f_0$ and a quantile function $q^{-1}$. 

Notice that $f_Y \in \mathcal{F}_A$, and $f_Y^{\leftarrow}(X_1, Y) = q(Y-f_0(X_1)) = \varepsilon_Y\indep X_1$. Therefore,  $S = \{1\} = pa_Y$ is $\mathcal{F}_A$-plausible set and we obtain $S_{\mathcal{F}_A}(Y) \subseteq pa_Y$. In Section~\ref{section_algorithm}, we propose an estimator $\hat{S}_{\mathcal{F}_A}(Y)$ that satisfies  $\hat{S}_{\mathcal{F}_A}(Y) \subseteq pa_Y$ with large probability.  

It is important to note that  we do not impose any assumptions on $X_1$ or $X_2$. In Section~\ref{Section_3}, we demonstrate that typically $S_{\mathcal{F}_A}(Y) = pa_Y$ except in some special cases similar to the special cases when ANM is non-identifiable \citep{Zhang2009}. Hence, we can typically identify and consistently estimate the direct causes of $Y$ from a random sample assuming only $f_Y \in \mathcal{F}_A$ and  $\varepsilon_Y \indep \textbf{X}_{pa_Y}$. To the best of our knowledge, there is no similar result in the literature.
\end{example}

\begin{wrapfigure}{r}{4cm}
% Figure removed
\caption{DAG corresponding to Example~\ref{Teaser example with Gaussian assumptions}.}
\label{three_graphs}
\end{wrapfigure} 
\begin{example}[Gaussian case]\label{Teaser example with Gaussian assumptions}
Consider the following structural causal model: $(X_1, X_2,X_3)$ are parents of $Y$, $X_4$ is a child of $Y$ and $X_5\indep Y$. An example of a DAG satisfying these relations is shown in Figure \ref{three_graphs}, but many other DAGs satisfy these relations. Let
$$Y = \mu_Y(X_1, X_2, X_3) + \eta_Y,\,\,\,  \eta_Y\text{ is Gaussian,}\,\,\,\,\,\,\,\,\eta_Y\indep (X_1, X_2, X_3),$$
where $\mu_Y$ is a measurable function. Then, $S_{\mathcal{F}_{A}}(Y) \subseteq  pa_Y = \{1,2,3\}$ and also $S_{\mathcal{F}_{F}}(Y) \subseteq  pa_Y$, where $F$ is a Gaussian distribution function. 

Additionally, assume that $(X_1, X_2, X_3)$ are normally distributed, $X_5$ is non-degenerate,  
$X_4 = \mu_4(\textbf{X}_{pa_4}) + \eta_4$, where $\eta_4\indep \textbf{X}_{pa_4},$ $\eta_4$ is Gaussian and $\mu_4$ is non-linear in any of the component and $\mu_Y$ satisfies the assumptions of Lemma~\ref{lemma_Gaussian_parents} (for example $\mu_Y(\textbf{X}_{pa_Y}) = \beta_Y\cdot X_1\cdot X_2\cdot X_3$ for $\beta_Y\neq 0$). Then, $S_{\mathcal{F}_F}(Y) = S_{\mathcal{F}_{A}}(Y) = pa_Y = \{1,2,3\}$.
\end{example}






Our methodology is quite general, and the scope of potential applications is broad and encompasses a wide range of fields and domains. Assuming additive or heteroschedastic models is a common practice in domains such as gene expressions, economics or biological networks \citep{yuan2006model}. 


From a practical point of view, we propose two algorithms for estimating the direct causes of a target variable from a random sample. One provides an estimate of $S_\mathcal{F}(Y)$ with a coverage guarantees; that is, with large probability our estimate is a subset of the parents. Such guarantees are rare and highly desirable in causal discovery. However, the output does not have to contain all direct causes. The second is a score-based algorithm estimating $pa_Y$ based on a goodness-of-fit.


In Section~\ref{Section_3}, we dive deeper into mathematical properties of $S_\mathcal{F}(Y)$, where the aim is to find conditions under which $S_\mathcal{F}(Y) = pa_Y$. In Section~\ref{section_algorithm}, we describe our proposed algorithms for estimating $S_\mathcal{F}(Y)$ and $pa_Y$ from a random sample. Section~\ref{section_simulations} contains a short simulation study followed by an application on a real dataset. The paper has four appendices: Appendix~\ref{Speci_appendix} contains some detailed notions and results omitted from the main text for clarity,  Appendix~\ref{section_appendix_simulations}  contains some details about the simulations and the application, Appendix~\ref{Appendix_Auxiliary} provides some auxiliary results needed for the proofs (in particular,  Lemma~\ref{CoolLemma} is the core mathematical result of the paper), and the proofs can be found in Appendix~\ref{Section_proofs}.












