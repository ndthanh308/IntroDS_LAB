\section{Appendix} \label{Speci_appendix}

\subsection{Class of invertible and minimal functions, invertible causal model, and causal minimality}
\label{Appendix_A.1.}

First, let us formally introduce the notions of invertibility and minimality of a real function. Next, we define a class of measurable functions $\mathcal{I}_m$ and a subclass of SCM called invertible causal models. We show that minimality of a link function is equivalent with causal minimality of the causal model. 

\begin{definition}[Invertibility]\label{I}
Let $\mathcal{X}_x\subseteq\mathbb{R}^{p},\mathcal{X}_y\subseteq\mathbb{R}, \mathcal{X}_z\subseteq\mathbb{R}$ be measurable sets. A measurable function $f:\mathcal{X}_x\times\mathcal{X}_y\to \mathcal{X}_z$  is called \textbf{invertible for the last element}, notation $f\in \mathcal{I}$, if there exists a function $f^{\leftarrow}:\mathcal{X}_x\times\mathcal{X}_z\to \mathcal{X}_y$ that fulfills the following: $\forall \textbf{x}\in\mathcal{X}_x, \forall y\in\mathcal{X}_y,z\in\mathcal{X}_z$ such that $y=f(\textbf{x},z)$, then $z=f^{\leftarrow}(\textbf{x},y)$. 
\end{definition}
The previous definition indicates that the element $z$ in a relationship $y=f(\textbf{x},z)$ can be uniquely recovered from $(\textbf{x},y)$. To provide an example, for the function $f(x,z) = x+z$, it holds that $f^{\leftarrow}(x, y) = y-x$, since $f^{\leftarrow}(x, f(x,z)) = f(x,z) - x = z$. More generally, for the additive function defined as $f(\textbf{x},z) = g_1(\textbf{x})+ g_2(z)$, where $g_2$ is invertible, it holds that $f^{\leftarrow}(\textbf{x}, y) = g_2^{-1}(y - g_1(\textbf{x}))$, $\textbf{x}\in\mathbb{R}^d, y, z\in \mathbb{R}$. Overall, if $f$ is differentiable and the partial derivative of $f(\textbf{x},z)$ with respect to $z$ is monotonic, then $f\in\mathcal{I}$ (follows from inverse function theorem). 

\begin{definition}[Minimality]
    We say that a function $f:\mathbb{R}^n\to\mathbb{R}$ \textit{is minimal almost surely}, notation $f\in\mathcal{M}$, if there does not exist a function $g:\mathbb{R}^{n-1}\to \mathbb{R}$ and $k\leq n$, such that $f(x_1, \dots, x_n) = g(x_1, \dots, x_{k-1}, x_{k+1}, \dots, x_n)$ for almost all $ \textbf{x}\in\mathbb{R}^n$ in the support of $f$. Recall that the notion 'almost all' represents the fact that the measure of a set $\{\textbf{x}\in\mathbb{R}^n: f(x_1, \dots, x_n) \neq g(x_1, \dots, x_{k-1}, x_{k+1}, \dots, x_n)\}$ has a Lebesgue measure zero. 
\end{definition}

\begin{definition}[Invertible causal model]
 We denote the set of invertible and almost surely minimal functions by
$$
\mathcal{I}_{m} = \{f\in\mathcal{I}\cap \mathcal{M} \}.
$$
We define the \textbf{ICM} (invertible causal model) as a SCM (\ref{definition_general_SCM}) with structural equations satisfying $f_i\in\mathcal{I}_{m}$ for all $i=0, \dots, p$.  
\end{definition}


Note that $f_i\in\mathcal{I}_{m}$ implies causal minimality of the ICM model, as the following lemma suggests. Recall that a distribution $P_\mathbf{X}$ over $\textbf{X}$ satisfies \textbf{causal minimality} with respect to $\mathcal{G}$ if it is Markov with respect to $\mathcal{G}$, but not to any proper subgraph of $\mathcal{G}$. 

\begin{lemma}\label{Lemma_about_ICM_minimality}
Consider a distribution generated by \hyperref[I]{ICM} with graph $\mathcal{G}_0$ (see Definition  \ref{I}). Let all structural equations $f_j\in\mathcal{I}_{m}$, $\forall j=0, \dots, p$.  Then, the distribution is causally minimal with respect to  $\mathcal{G}_0$. Conversely, if $f_j\not \in\mathcal{M}$ for some $j\in\{0, \dots, p\}$, then the causal minimality is violated. 
\end{lemma}
\begin{proof}
The second claim follows directly from Proposition 4 in \cite{Peters2014}. For the first claim, we use a similar approach as in Proposition 17 from \cite{Peters2014}. 

Let $f_j\in\mathcal{I}_{m}$ for all $j=0, \dots, p$ and let the causal minimality be violated, i.e., let $\tilde{\mathcal{G}}$ be a subgraph of $\mathcal{G}_0$ such that the distribution is Markov wrt $\tilde{\mathcal{G}}$. Find $i,j\in\mathcal{G}_0$ such that $i\to j$ in $\mathcal{G}_0$ but $i\not\to j$ in $\tilde{\mathcal{G}}$. 

In graph $\mathcal{G}_0$ we have a structural equation  $X_j = f_j(\textbf{X}_{pa_j(\mathcal{G}_0)}, \varepsilon_j) = f_j(X_i, \textbf{X}_{pa_j(\mathcal{G}_0)\setminus\{i\}}, \varepsilon_j)$  but in $\tilde{\mathcal{G}}$ we have $X_j = \tilde{f}_j(\textbf{X}_{pa_j(\mathcal{G}_0)\setminus\{i\}}, \varepsilon_j)$. Hence, functions $f_j(X_i, \textbf{X}_{pa_j(\mathcal{G}_0)\setminus\{i\}}, \varepsilon_j)$ and $\tilde{f}_j( \textbf{X}_{pa_j(\mathcal{G}_0)\setminus\{i\}}, \varepsilon_j)$ have to be equal almost surely, which contradicts $f_j\in\mathcal{I}_{m}$. 
\end{proof}




\subsection{ Lemma~\ref{LemmaAboutUnidentifiabilityFL} modified for hidden confounders}
\label{Appendix_A_Lemma}

In the following, we use the notion of m-separability, a generalization of d-separability for mixed-type graphs. For details see \cite{Richardson}. Moreover, we say that a node in a graph is a source node, if all edges associated to the node are directed out-going edges (i.e. only $v\to \cdot$ are allowed). 

\begin{lemma}\label{LemmaAboutUnidentifiabilityFL2}
Let $(Y, \textbf{X})\in\mathbb{R}\times \mathbb{R}^p$ follow an $\mathcal{F}_L$-model with DAG ${\mathcal{G}}_0$. Let $\tilde{\textbf{X}}\subseteq \textbf{X}$ be observed variables (and $\textbf{X}\setminus \tilde{\textbf{X}}$ are unobserved hidden confounders). Let $\tilde{\mathcal{G}}_0$ be a projection of ${\mathcal{G}}_0$ on the observed variables. If there exist a source variable $a\in pa_Y(\tilde{\mathcal{G}}_0)$ then $|S_{\mathcal{F}_L}(Y)| \leq 1$. Moreover, if there exist a pair of source variables  $a,b\in an_Y(\tilde{\mathcal{G}}_0)$ that are m-separated in $\tilde{\mathcal{G}}_0$, then $S_{\mathcal{F}_L}(Y) = \emptyset$. 
\end{lemma}

\begin{proof}\label{Proof of LemmaAboutUnidentifiabilityFL 2}
The proof is fully analogous to the proof of Lemma~\ref{Proof of LemmaAboutUnidentifiabilityFL}. Since we added the assumption that $a,b$ are source variables, the fact that some variables are unobserved does not change any step in the proof. 
\end{proof}



\subsection{Definition of restricted additive noise model from \cite{Peters2014}}
\label{Appendix_restricted_additive_noise_model}

We restate the definition of restricted additive noise model presented in Section 3 in \cite{Peters2014}. 

\begin{definition}
An  $\mathcal{F}_A$-model is called a restricted additive noise model if for all \(j \in V\), \(i \in \text{PA}_j\) and all sets \(S \subseteq V\) with \(\text{PA}_j \setminus \{i\} \subseteq S \subseteq \text{ND}_j \setminus \{i, j\}\), there is an \(x_S\) with \(p_S(x_S) > 0\), such that

\[
\left( f_j ( x_{\text{PA}_j \setminus \{i\}}, \underbrace{\cdot}_{X_i} ), P_{(X_i \mid \textbf{X}_S = \textbf{x}_S)}, P_{\eta_j} \right)
\]
satisfies Condition~\ref{Condition19}. Here, the underbrace indicates the input component of \(f_j\) for variable \(X_i\). In particular, we require the noise variables to have non-vanishing densities and the functions \(f_j\) to be continuous and three times continuously differentiable.
\end{definition}


\begin{condition}\label{Condition19}
    The triple \((f_j, P_{X_i}, P_{\eta_j})\) does not solve the following differential equation for all \(x_i, x_j\) with \(\nu''(x_j - f(x_i)) f'(x_i) \neq 0\):

\[
\xi''' = \xi'' \left( -\frac{\nu''' f'}{\nu''} + \frac{f''}{f'} \right) - 2 \nu'' f'' f' + \nu' f''' + \frac{\nu' \nu''' f'' f'}{\nu''} - \nu' \frac{(f'')^2}{f'},
\]
where \(f := f_j\), \(\xi := \log p_{X_i}\), and \(\nu := \log p_{\eta_j}\) are the logarithms of the strictly positive densities. To improve readability, we have skipped the arguments \(x_j - f(x_i)\), \(x_i\), and \(x_i\) for \(\nu\), \(\xi\), and \(f\) and their derivatives, respectively.
\end{condition}

\cite[Theorem 28]{Peters2014} showed that $\mathcal{G}$ is  identifiable from the joint distribution under a causally minimal restricted additive noise model.

\begin{theorem}[Theorem 20 in \cite{Peters2014}]
\label{theorem20}
     Let \(P_{(X_0, X_1)} \) be generated by a bivariate additive noise model with graph \( \mathcal{G}_0 \) satisfying Condition~\ref{Condition19} and assume causal minimality, i.e., a non-constant function \( f_j \). Then, \( \mathcal{G}_0 \) is identifiable from the joint distribution.
\end{theorem}


\begin{theorem}[Theorem 28 in \cite{Peters2014}]
\label{theorem28}
 Let \( P_{(X_1, \ldots, X_p)} \) be generated by a restricted additive noise model with graph \( \mathcal{G}_0 \) and let \( P_{(X_1, \ldots, X_p) }\) satisfy causal minimality with respect to \( \mathcal{G}_0 \) (which holds for example if the functions \( f_j \) are minimal). Then, \( \mathcal{G}_0 \) is identifiable from the joint distribution.

\end{theorem}




\subsection{General identifiability $\implies$ $\mathcal{F}$-identifiability for non-additive $\mathcal{F}$}
\label{Appendix_pairwise_identifiability}

In the following, we restate the result from Section~\ref{section_general_implies_local_identifiability} for general $\mathcal{F}$. One may expect that $\mathcal{F}$-identifiability of $Y$ follows automatically if we assume an identifiable $\mathcal{F}$-model for all variables in the SCM. This is not the case in general.  
\begin{example}
Consider the following SCM: 
\begin{equation*}
X_1 = \eta_1; \,\,\,\,\,\,\,\,Y = X_1^2 + \eta_Y; \,\,\,\,\,\,\,\,X_2 = \beta_2 Y + \eta_2, 
\end{equation*}
where $\eta_1, \eta_2, \eta_Y$ are independent, $\eta_1\sim U(0,1), \eta_Y, \eta_2\sim N(0, 1)$. This SCM is identifiable (for example, SCM with graph $X_2\to Y\to X_1$ does not allow writing $X_1 = f_1(Y) + \tilde{\eta}_1$ for any $f_1, \tilde{\eta}_1$). However, notice that under conditioning on $X_1 = x\in (0,1)$ we obtain a linear Gaussian case and we can revert the equation between $(Y, X_2)$ and obtain $Y = X_1^2 + \tilde{\beta}_2X_2 + \tilde{\eta}_Y$ for some $\tilde{\beta}_2\in\mathbb{R}, \tilde{\eta}_Y\sim N(0, \sigma^2)$. 
\end{example}

We require a slightly stronger notion of identifiability of $\mathcal{G}$ that we call ``pairwise identifiability''. 

\begin{definition}
Let $(X_0, \textbf{X})\in\mathbb{R}\times \mathbb{R}^p$ follow a SCM (\ref{definition_general_SCM}) with DAG $\mathcal{G}_0$. Let $\mathcal{F}$ be a subset of all measurable functions. We say that the  $\mathcal{F}$-model is \textbf{identifiable}, if there does not exist a graph $\mathcal{G}'\neq \mathcal{G}_0$ and functions $f_i'\in\mathcal{F}, i=0, \dots, p$ generating the same joint distribution. 

We say that the $\mathcal{F}$-model is \textbf{pairwise identifiable}, if for all $i,j\in\mathcal{G}_0, i\in pa_j$ hold the following: $\forall S\subseteq V$ such that  $pa_j\setminus \{i\}\subseteq S \subseteq nd_j\setminus\{i,j\}$ there exist $\textbf{x}_{S}: p_S(\textbf{x}_S)>0$ satisfying that a bivariate model defined as $Z_1=\tilde{\varepsilon}_1, Z_2 = \tilde{f}(Z_1, {\varepsilon}_j)$ is identifiable, where $P_{\tilde{\varepsilon}_1} = P_{X_i\mid \textbf{X}_{S} =\textbf{ x}_S}$, $\tilde{f}(x, \varepsilon) = f(\textbf{x}_{pa_j\setminus\{i\}}, x, \varepsilon)$, $\tilde{\varepsilon}_1\indep {\varepsilon_j}$.  
\end{definition}

In the bivariate case, the notion of identifiability and pairwise identifiability trivially coincides. Note the following observation. 

\begin{lemma}\label{pairwise_implies_global}
Pairwise identifiable $\mathcal{F}$-model is identifiable.  
\end{lemma}
 
The proof can be found in  \hyperref[Proof of pairwise_implies_global]{Appendix} \ref{Proof of pairwise_implies_global}. The following proposition is a counterpart of Proposition~\ref{TheoremFidentifiabilityWithChild}
 from Section~\ref{section_general_implies_local_identifiability} with general $\mathcal{F}$. 


\begin{proposition}
\label{proposition_for_pairwise_F_model_identifiability}
Let $(X_0, \textbf{X})$ follow a pairwise identifiable $\mathcal{F}$-model with DAG $\mathcal{G}$, such that all $\textbf{X}$ are neighbors of $X_0$ in $\mathcal{G}$.   Let $S \subseteq \{1, \dots, p\}$ contain a child of $X_0$ in $\mathcal{G}$.  Then, $S$ is not $\mathcal{F}$-plausible. 
\end{proposition}
 The proof can be found in  \hyperref[Proof of proposition_for_pairwise_F_model_identifiability]{Appendix} \ref{Proof of proposition_for_pairwise_F_model_identifiability}. 

\subsection{ $\mathcal{F}$-plausibility under restricted support        }
\label{Appendix_support}
 
The following proposition discusses a case when $\mathcal{F}$-implausibility results from restricting the support of $Y$ by conditioning on the child of $Y$. This result is specific for a location-scale space of functions $\mathcal{F}_{LS}$, but can be easily modified for other types of $\mathcal{F}$. 

\begin{proposition}[Assuming bounded support]
\label{Support_proposition}
Let $(Y, \textbf{X})\in\mathbb{R}\times \mathbb{R}^p$ follow an SCM with DAG $\mathcal{G}_0$. Let $S\subseteq\{1, \dots, p\}$ be a non-empty set. Let  $\underline{\Psi},\overline{\Psi}: \mathbb{R}^{\mid S\mid}\to \mathbb{R}$ be real functions such that
\begin{equation*} 
supp(Y\mid \textbf{X}_S=\textbf{x}) = \big(\underline{\Psi} (\textbf{x}),\overline{\Psi}(\textbf{x})\big), \,\,\,\,\,\,\, \forall \textbf{x}\in supp(\textbf{X}_S).
\end{equation*}
If
\begin{equation} \label{eq9987}
\frac{Y - \underline{\Psi}(\textbf{X}_S)}{\overline{\Psi}(\textbf{X}_S) - \underline{\Psi}(\textbf{X}_S)}\not\indep \textbf{X}_S,
\end{equation}
then $S$ is not $\mathcal{F}_{LS}$-plausible. 
\end{proposition}
The proof can be found in \hyperref[Proof of Support_proposition]{Appendix} \ref{Proof of Support_proposition}. Proposition \ref{Support_proposition} can be expressed as follows. If the support of $Y$ given $\textbf{X}_S = \textbf{x}_S$ is bounded, then $S$ can be  $\mathcal{F}_{LS}$-plausible only in a very specific case when (\ref{eq9987}) does not hold. 
Typically, (\ref{eq9987}) holds if $S$ contains a child of $Y$. 
\begin{example}\label{Example_o_Supporte}
Consider SCM where $Y$ is a parent of $X_1$ and $X_1 = Y + \eta$, where $Y\indep \eta$. Assume that  $Y, \eta$ are non-negative ($supp(Y) = supp(\eta) = (0, \infty)$). 
Then, $\underline{\Psi}(x)=0$ and  $\overline{\Psi}({x})=x$, since the support of $[Y\mid Y+\eta=x]$ is $(0,x)$. Hence, (\ref{eq9987}) reduces to $\frac{Y}{X_1} \not\indep X_1$. If $\frac{Y}{X_1} \not\indep X_1$, then $S=\{1\}$ is not $\mathcal{F}_{LS}$-plausible. 

How strong is the assumption $\frac{Y}{X_1} \not\indep X_1$? We claim that it holds in typical situations. A notable exception when  $\frac{Y}{X_1} \indep X_1$ holds is when $Y, \eta$ have Gamma distributions with equal scales. 
\end{example}

Proposition \ref{Support_proposition} is applicable only when $S$ contains a child of $Y$. If $S\subseteq pa_Y$, then (\ref{eq9987}) typically does not hold, as the following example illustrates.  
\begin{example}
Consider a bivariate SCM with $X_1\to Y$. Let $Y = X_1 + \eta$, where $X_1\indep \eta$. Assume that $supp(X) = supp(\eta) = (0, 1)$. Then, $\underline{\Psi}(x)=x$ and  $\overline{\Psi}({x})=1+x$. Hence, (\ref{eq9987}) reduces to $Y-X_1 \not\indep X_1$, which is not satisfied, so Proposition \ref{Support_proposition} is not applicable. 
\end{example}
Proposition \ref{Support_proposition} can be also stated for a case when  $\overline{\Psi}({x})=\infty$. In that case, we require stronger assumptions; we replace assumption (\ref{eq9987}) with  $Y - \underline{\Psi}(\textbf{X}_S)\not\indep \textbf{X}_S$ and replace $\mathcal{F}_{LS}$ with $\mathcal{F}_A$ (more restricted set where the scale is fixed). 




\subsection{Theorem~\ref{Theorem_in_section2} restated for location-scale models $\mathcal{F}_{LS}$ and CPCM($F$) models $\mathcal{F}_F$}
\label{Appendix_location_scale_definition}

We restate similar results to Theorem~\ref{Theorem_in_section2} for $\mathcal{F} = \mathcal{F}_{LS}$ and $\mathcal{F}_F$ functional classes. We start with the location-scale result. 

\begin{proposition}[Location-scale]\label{LemmaOLocationScaleinseparabilite}
 Consider $(Y, \textbf{X})\in\mathbb{R}\times \mathbb{R}^p$ satisfying ( \ref{SCM_for_Y}) with  $pa_Y\neq \emptyset$ and $\mathcal{F} = \mathcal{F}_{LS}$. 
 Let $\textbf{X}_{pa_Y}$ have full support and independent components. 

 Let $f_Y\in\mathcal{F}_{LS}$  have the form $f_Y(\textbf{x}, \varepsilon)=\mu(\textbf{x}) + \sigma(\textbf{x})\varepsilon$, where $\theta(\textbf{x}) = \big(\mu(\textbf{x}), \sigma(\textbf{x})\big)^\top$ is additive in both components, that is,  
$\mu(\textbf{x}) = h_{1, \mu}(x_1)+\dots + h_{k, \mu}(x_k)$ and 
$\sigma(\textbf{x}) =h_{1, \sigma}(x_1)+\dots + h_{k, \sigma}(x_k)$ for some continuous non-constant non-zero functions $h_{i,\cdot}$, where we also assume $h_{i,\sigma}>0$, $i=1, \dots, k$.  Then, then every $S\subsetneq pa_Y$ is not $\mathcal{F}_{LS}$-plausible. 
\end{proposition}
Proof can be found in \hyperref[Proof of LemmaOLocationScaleinseparabilite]{Appendix} \ref{Proof of LemmaOLocationScaleinseparabilite}. 

Now we focus on the case $\mathcal{F} = \mathcal{F}_{F}$, where a distribution function $F$ has $q\in\mathbb{N}$ parameters $\theta = (\theta_1, \dots, \theta_q)^\top$. We restrict to such $F$ satisfying the following definition. 


\begin{definition}\label{DefLS}
Let $F$ be a distribution function with one ($q=1$) parameter $\theta$. We say that the \textbf{parameter acts additively} in $F$, if an invertible real function $f_2$ and a function $f_1\in\mathcal{I}_m$ exist such that for all $\theta_1, \theta_2$ holds \footnote{Notation $F_{\theta_1}\big(F^{-1}_{\theta_2}(z)\big)$ is equivalent to $F(F^{-1}(z, \theta_2), \theta_1)$. We believe that this improves the readability.  } 
\begin{equation}\label{postAdditiveDefinition}
F_{\theta_1}\big(F^{-1}_{\theta_2}(z)\big) = f_1\big(z, f_2(\theta_1) + \theta_2\big), \,\,\,\forall z\in(0,1). 
\end{equation}

We say that the \textbf{parameter acts  multiplicatively} in $F$ if an invertible real function $f_2$ and a function $f_1\in\mathcal{I}_m$ exist such that for all $\theta_1, \theta_2$ holds  \begin{equation}\label{postMultiplDefinition}
F_{\theta_1}\big(F^{-1}_{\theta_2}(z)\big) = f_1\big(z, f_2(\theta_1) \cdot\theta_2\big), \,\,\,\forall z\in(0,1).
\end{equation}

Let $F$ be a distribution function with two ($q=2$) parameters $\theta = (\mu, \sigma)^\top\in\mathbb{R}\times \mathbb{R}_+$. We say that $F$ is a \textbf{Location-Scale} distribution, if for all $\theta$ holds  
\begin{equation*}
F_{\theta}\left( \frac{x-\mu}{\sigma}\right) = F_{\theta_0}(x),\,\,\,\,\forall x\in\mathbb{R},
\end{equation*} 
where $F_{\theta_0}$ is called standard distribution and corresponds to a parameter $\theta_0 = (0,1)^\top$.  
\end{definition}
Examples of $F$ whose parameter acts  additively include a Gaussian distribution with fixed variance or a Logistic distribution/Gumbel distribution with fixed scales.  Note that typically, $f_2(x) = -x$,  since $F_{\theta_1}\big(F^{-1}_{\theta_1}(z)\big)=z$ needs to hold.

Examples of $F$ whose parameter acts  multiplicatively include a Gaussian distribution with the fixed expectation or a Pareto distribution (where $F_{\theta_1}\big(F^{-1}_{\theta_2}(z)\big) = z^{\frac{\theta_1}{\theta_2}}= f_1\big(z, f_2(\theta_1) \cdot\theta_2\big)$ for $f_1(z,x) = z^{-1/x}$ and $f_2(x)=-1/x$). Functions $ f_1, f_2$ are not necessarily uniquely defined. 

Examples of Location-Scale types of distributions include Gaussian distribution, logistic distribution, or Cauchy distribution, among many others.







\begin{proposition}\label{LemmaOParetoinseparabilite}
 Consider $(Y, \textbf{X})\in\mathbb{R}\times \mathbb{R}^p$ satisfying ( \ref{SCM_for_Y}) with  $pa_Y\neq \emptyset$ and $\mathcal{F} = \mathcal{F}_{F}$. where $F$ be a distribution function whose parameter acts multiplicatively. 
 Let $\textbf{X}_{pa_Y}$ be a continuous random vector with full support and independent components. 
\begin{itemize}
\item Consider $f_Y\in\mathcal{F}_F$ in the form $f_Y(\textbf{x}, \varepsilon)=F^{-1}\big(\varepsilon, \theta(\textbf{x})\big)$ with additive function $\theta(x_1, \dots, x_k) = h_1(x_1)+\dots + h_k(x_k)$, where $h_i$ are continuous non-constant real functions. Then, then every $S\subsetneq pa_Y$ is not $\mathcal{F}_{F}$-plausible. 
\item Consider $f_Y\in\mathcal{F}_F$ in the form $f_Y(\textbf{x}, \varepsilon)=F^{-1}\big(\varepsilon, \theta(\textbf{x})\big)$ with multiplicative function   $\theta(x_1, \dots, x_k) = h_1(\textbf{x}_S)\cdot h_2(\textbf{x}_{\{1, \dots, k\}\setminus S})$ for some $S\subsetneq \{1, \dots, k\}$, where $h_1, h_2$ are continuous non-constant non-zero real functions. Then, $S_{\mathcal{F}_F}(Y)=\emptyset$.
\end{itemize}
\end{proposition}
Proof can be found in \hyperref[Proof of LemmaOParetoinseparabilite]{Appendix} \ref{Proof of LemmaOParetoinseparabilite}. 
Analogous Proposition~\ref{LemmaOParetoinseparabilite} can be stated for $F$ being additive or location-scale type, where Lemma \ref{CoolLemma} part 3 and 4 would be used instead of part 2. 


Finally, we present the modification of Theorem~\ref{Theorem_in_section2} for (possibly dependent) Gaussian parents. 
\begin{lemma}[Gaussian case]\label{lemma_Gaussian_parents}
 Consider $(Y, \textbf{X})\in\mathbb{R}\times \mathbb{R}^p$ satisfying ( \ref{SCM_for_Y}) with  $pa_Y\neq \emptyset$ and $\mathcal{F} = \mathcal{F}_A$. 
 Let $\textbf{X}_{pa_Y}$ have Gaussian distribution. If $f_Y\in\mathcal{F}_A$ is continuous injective additively transitive \footnote{A function is said to be \textbf{additively transitive}, if $f(x, y+z) = f(x, y) + f(x, z)$. For example, $f(x, y) = xy$ is additively transitive.} function that cannot be written as (\ref{efaetgfas}), then every $S\subsetneq pa_Y$ is not $\mathcal{F}_A$-plausible. In particular, if $pa_Y = \{1, \dots, p\}$ then $S_{\mathcal{F}_A}(Y) = pa_Y$. 
\end{lemma}

\begin{proof}
    The proof is analogous to the proof of Theorem~\ref{Theorem_in_section2}, where we use Lemma~\ref{CoolLemma} part~5 instead of part~1 in the last step. 
\end{proof}


