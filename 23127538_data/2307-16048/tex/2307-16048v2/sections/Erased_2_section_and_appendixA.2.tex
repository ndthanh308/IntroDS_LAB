
\subsection{Results presented in \cite{bodik2023identifiability} and \cite{Zhang2009, Peters2014}}
\label{Appendix_A.2.}
\textcolor{red}{do we really need this section?}
\begin{theorem}
\label{thm2_from_first_paper}
Let $(X_1, X_2)$ follow the bivariate $CPCM(F)$, where $F$ lies in the exponential family of continuous distributions and $T = (T_{1}, \dots, T_{q})^\top$is the corresponding sufficient statistic.

Assume that $\theta$ is not a linear combination of $T_{1}, \dots, T_{q}$. That is, $\theta$ cannot be written as
\begin{equation}\label{eq158}
\theta_{i}(x) \overset{}{=} \sum_{j=1}^{q}a_{i,j}T_{j}(x)+b_i,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \forall x\in supp(X_1),
\end{equation}
for all $i=1, \dots, q,$ and for some constants $a_{i,j},b_i\in\mathbb{R}$, $j=1, \dots, q$. Then, the causal graph $\mathcal{G}$ is identifiable from the joint distribution.
\end{theorem}



 \begin{theorem}\label{thmMultivairateIdentifiability}
Let $P_{\textbf{X}}$ be generated by the $CPCM(F)$ with DAG $\mathcal{G}$ and with density $p_{\textbf{X}}$. Let for all $ i,j\in\mathcal{G}, i\in pa_j$ hold the following: $\forall S\subseteq V$ such that  $pa_j\setminus \{i\}\subseteq S \subseteq nd_j\setminus\{i,j\}$ there exist $\textbf{x}_{S}: p_{\textbf{X}_S}(\textbf{x}_S)>0$ satisfying: a graph $\mathcal{G}$ in a bivariate model defined as $X=\tilde{\varepsilon}_X, Y = F^{-1}\big(\tilde{\varepsilon}_Y, \tilde{\theta}(X)\big)$ is identifiable, where  $P_{\tilde{\varepsilon}_X} = P_{X_i\mid \textbf{X}_{S} =\textbf{ x}_S}    $ and $\tilde{\theta}(x) = \theta_j(\textbf{x}_{pa_j\setminus\{i\}}, x)$, where $x\in supp(X)$.

Then, the causal graph $\mathcal{G}$ is identifiable from the joint distribution.
 \end{theorem}


\begin{theorem}[Theorem 20 in \cite{Peters2014}]
\label{thmANM}
Let $(X_1, X_2)$ follow bivariate additive noise model $X_j = f_j(X_{pa_i})+\eta_j,$ $j=1,2$, where $\eta_1, \eta_2$ are independent with strictly positive density and $f_1, f_2$ are measurable non-constant functions. Assume that $(f_j, P_{\eta_1}, P_{\eta_2})$ does not solve the following differential equation for all $x_i, x_j$ with $\nu''(x_j - f(x_i))f'(x_i)\neq 0:$
\begin{equation}\label{GRC_equality_ANM}
    \xi'''\neq \xi''\bigg(-\frac{\nu'''f'}{\nu''}+\frac{f''}{f'}\bigg) - 2\nu''f''f'+\nu'f''' + \frac{\nu'\nu'''f''f'}{\nu''} - \frac{\nu'(f'')^2}{f'},
\end{equation}
where $f:=f_i, \xi - log( p_{X_i}), \nu = log(p_{\eta_i}),i=1,2,$ where w have skipped the arguments $x_j - f(x_i), x_i, x_i$ for $\nu, \xi, f$ and their derivatives, respectively. 

Then, the causal graph is identifiable from the joint distribution. 
\end{theorem}
For example, if $f_j$ is non-linear and $\eta_j$ is Gaussian, then (\ref{GRC_equality_ANM}) is satisfied \citep[Corollary 22]{Peters2014}. 


\section{$\mathcal{F}$-identifiability of direct causes}
\label{Section_2}
%pa_Y=0
Recall that $(Y, \textbf{X})\in\mathbb{R}\times\mathbb{R}^p$ are random variables, where we assume the data-generating process of the variable of interest $Y$ in the form 
\begin{equation}\label{SCM_for_Y}
Y=f_Y(\textbf{X}_{pa_Y}, \varepsilon_Y),\,\,\,\,\,\,\, f_Y\in\mathcal{F}, \,\,\,\,\,\,\,\varepsilon_Y\indep \textbf{X}_{pa_Y}, \,\,\,\,\,\,\,\varepsilon_Y\sim U(0,1),
\end{equation}
where $\mathcal{F}$ is a subset of all measurable functions and $pa_Y\subseteq\{1, \dots, p\}$. We aim to answer the following question: can we detect the direct causes of $Y$ only from observational data under ( \ref{SCM_for_Y})? In the following section, we develop a framework where we are (typically) able to recover at least a subset of the causal variables from a random sample of $(Y, \textbf{X})$. First, let us formally introduce a class of invertible causal models, a subclass of structural causal models where the noise variables can be recovered from the observed variables. 

\begin{definition}\label{I}
Let $\mathcal{X}_x\subseteq\mathbb{R}^{p},\mathcal{X}_y\subseteq\mathbb{R}, \mathcal{X}_z\subseteq\mathbb{R}$ be measurable sets. A measurable function $f:\mathcal{X}_x\times\mathcal{X}_y\to \mathcal{X}_z$  is called \textbf{invertible for the last element}, notation $f\in \mathcal{I}$, if there exists a function $f^{\leftarrow}:\mathcal{X}_x\times\mathcal{X}_z\to \mathcal{X}_y$ that fulfills the following: $\forall \textbf{x}\in\mathcal{X}_x, \forall y\in\mathcal{X}_y,z\in\mathcal{X}_z$ such that $y=f(\textbf{x},z)$, then $z=f^{\leftarrow}(\textbf{x},y)$. Moreover, denote 
$$
\mathcal{I}_{m} = \{f\in\mathcal{I} : \text{ f is not constant in any of its arguments} \}.
$$
We define the \textbf{ICM} (invertible causal model) as a SCM (\ref{definition_general_SCM}) which structural equations $f_i\in\mathcal{I}_{m}$ for all $i=0, \dots, p$.  
\end{definition}
The previous definition describes that the element $z$ in a relation $y=f(\textbf{x},z)$ can be uniquely recovered from $(\textbf{x},y)$. To provide an example, for the function $f(x,z) = x+z$ holds $f^{\leftarrow}(x, y) = y-x$, since $f^{\leftarrow}(x, f(x,z)) = f(x,z) - x = z$. More generally, for the additive function defined as $f(\textbf{x},z) = g_1(\textbf{x})+ g_2(z), $ holds $f^{\leftarrow}(\textbf{x}, y) = g_2^{-1}(y - g_1(\textbf{x}))$, if the inverse $g^{-1}$ exists, $\textbf{x}\in\mathbb{R}^d, y, z\in \mathbb{R}$. Overall, if $f$ is differentiable and the partial derivative of $f(\textbf{x},z)$ with respect to $z$ is monotonic, then $f\in\mathcal{I}$ (follows from Inverse function theorem \cite{Inverse_function_theorem}). 
Note that $f_i\in\mathcal{I}_{m}$ implies causal minimality of the ICM model (the subscript $m$ in $\mathcal{I}_{m}$ represents the word "minimality"), see Lemma~\ref{Lemma_about_ICM_minimality} in Appendix \ref{Appendix_A.1.}.




The functional space $\mathcal{F}$ in ( \ref{SCM_for_Y}) corresponds to our assumptions that we are willing to make about the data-generating process of $Y$. We will always work with  $\mathcal{F}\subseteq\mathcal{I}_m$. If we assume linearity of the covariates, this represents the assumption $f_Y\in\mathcal{F}_L$, where (recall that without loss of generality, we assume $\varepsilon_Y\sim U(0,1)$)
\begin{equation}\label{mathcalF_L}
\mathcal{F}_L = \{f\in\mathcal{I}_m: f(\textbf{x}, \varepsilon) = \beta^T\textbf{x} + q^{-1}(\varepsilon) \text{ for some quantile function }q^{-1} \text{ and } \beta\in\mathbb{R}^{|\textbf{x}|}\}.
\end{equation}
On the other hand, if we assume $CPCM(F)$ as in (\ref{CPCM_def}), then $f_Y\in\mathcal{F}_F$ where 
\begin{equation}\label{mathcalF_F}
\mathcal{F}_F := \{f\in\mathcal{I}_m: f(\textbf{x},\varepsilon) =  F^{-1}(\varepsilon;\theta(\textbf{x})) \text{ for some function } \theta\}.
\end{equation}
Note that the restriction  $f\in\mathcal{I}_m$ implies $\beta_i\neq 0$ in (\ref{mathcalF_L}), and that $\theta(\textbf{x})$ is non-constant in any of its argument in (\ref{mathcalF_F}).  Table~\ref{tableDefinitions} describes all functional spaces considered in this paper.  

Recall the notion of   $\mathcal{F}$-plausible and   $\mathcal{F}$-identifiable set of parents from Definition~\ref{Definition1}. The principle of independence of the cause and the mechanism directly implies that the set $S=pa_Y$ is always  $\mathcal{F}$-plausible; under ( \ref{SCM_for_Y}) always holds 
\begin{equation}
S_\mathcal{F}(Y) \subseteq pa_Y.
\end{equation}
However, the equality $S_\mathcal{F}(Y)= pa_Y$ does not need to hold. Observe that 
$$\text{if }\mathcal{F}_1\subseteq \mathcal{F}_2\subseteq \mathcal{I}_m, \text{ then }S_{\mathcal{F}_1}(Y)\supseteq S_{\mathcal{F}_2}(Y).$$
This is not surprising; the more restrictions we put on the data-generating process, the larger set of identifiable parents. 

We primarily focus on the case when $\textbf{X}=(X_1, \dots, X_p)$ are neighbors (either parents or children) of $Y$. Using the classical conditional independence approach and d-separation, we can eliminate other variables from being potential parents of $Y$. Nevertheless, the theory can be extended to non-neighbors as well. 

In the next section, we describe properties of $S_\mathcal{F}(Y)$ and conditions under which  $S_\mathcal{F}(Y)=pa_Y$. The following example illustrates our results presented in the next section. 



% Figure environment removed


\begin{example}[Teaser example with Gaussian assumptions]\label{Teaser example with Gaussian assumptions}
Consider an SCM with DAG drawn in Figure \ref{Figure_DAG_for_teaser}. Let $(X_1, X_2, X_3)$ be normally distributed, and $X_5$ is non-degenerate. Such as in Example \ref{Gaussian case}, let 
$$Y = \mu_Y(X_1, X_2, X_3) + \sigma_Y(X_1, X_2, X_3)\varepsilon_Y,\,\,\,  \varepsilon_Y\text{ is Gaussian,}$$
$$X_4 = \mu_4(X_2, Y, X_5) + \sigma_4(X_2, Y, X_5)\varepsilon_4,\,\,\,  \varepsilon_4\text{ is Gaussian,}$$
where $(\mu_4, \sigma_4)^\top$ and  $(\mu_Y, \sigma_Y)^\top$ satisfy some (weak) assumptions presented in Section~\ref{Section_3}. In particular,  $(\mu_4, \sigma_4)^\top$ are not in the form (\ref{norm}) and $\mu_Y$ is not additive (or $\sigma_Y$ is not multiplicative) in the arguments. 

Then, $S_{\mathcal{F}_F}(Y) = S_{\mathcal{F}_{LS}}(Y) = pa_Y = \{1,2,3\}$, where $F$ is a Gaussian distribution function. This result follows from Lemma \ref{GaussianSeparabilita} in combination with Lemma \ref{lemma o unseparability=unplausibility}, Consequence~\ref{PropositionOAdditiveParents} and results in Example \ref{Gaussian case} combined with Theorem \ref{TheoremFidentifiabilityWithChild}, presented in the next section. 
\end{example}




\begin{table}[]
\begin{tabular}{|l|}
\hline
\multicolumn{1}{|c|}{Definitions of different $\mathcal{F}\subset \mathcal{I}_m$ used in the paper}                                                                                                       \\ \hline
$\mathcal{F}_F := \{f\in\mathcal{I}_m: f(\textbf{x},\varepsilon) =  F^{-1}(\varepsilon;\theta(\textbf{x})) \text{ for some function } \theta\}$                       \\ \hline
$\mathcal{F}_L = \{f\in\mathcal{I}_m: f(\textbf{x}, \varepsilon) = \beta^T\textbf{x} + q^{-1}(\varepsilon) \text{ for some quantile function }q^{-1} \text{ and }\beta\neq 0\}$                                \\ \hline
$\mathcal{F}_A = \{f\in\mathcal{I}_m: f(\textbf{x}, \varepsilon) = \mu(\textbf{x}) + g^{-1}(\varepsilon) \text{ for some }\mu(\cdot)\text{ and quantile function }q^{-1}     \}$        \\ \hline
$\mathcal{F}_{LS} = \{f\in\mathcal{I}_m: f(\textbf{x}, \varepsilon) = \mu(\textbf{x}) + \sigma(\textbf{x}) q^{-1}(\varepsilon) $                                                     \\
\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,$\text{ for some functions  }\mu, \sigma>0\,\,\,\,\,\text{ and for some quantile function }q^{-1}\}$ \\ \hline
\end{tabular}
\caption{The table summarizes the definitions of different forms used for the functional space $\mathcal{F}$ representing our assumptions. $\mathcal{F}_F$, $\mathcal{F}_L$, $\mathcal{F}_A$, $\mathcal{F}_{LS}$ correspond to, CPCM assumption, linear assumption, additive assumption and location-scale assumption respectively.   }
\label{tableDefinitions}
\end{table}

