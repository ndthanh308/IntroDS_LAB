%Adress preliminaries and existing identifiability based methods and erase 1.2.
\section{Preliminaries, notation, and existing structural-restrictions-based methods}\label{section_notation}

First, we introduce the graphical causal notation (e.g., \citealp{PCalgorithm}). 
A DAG (directed acyclic graph) $\mathcal{G}=(V,E)$ contains a finite set of vertices (nodes) $V$ and a set of directed edges $E$  between distinct vertices, such that there exists no directed cycle or multiple edges. For a distinct pair $i,j\in V$ with an edge $i\to j$, we say that $i$ is a \textbf{parent} of $j$ and $j$ is a \textbf{child} of $i$, notation  $i\in  pa_{j}(\mathcal{G})$ and $j\in ch_j(\mathcal{G})$, respectively. If there is a directed path from $i$ to $j$, we say that $i$ is an \textbf{ancestor} of $j$ and $j$ is a \textbf{descendant} of $i$, notation $i\in an_j(\mathcal{G})$ and $j\in de_i(\mathcal{G})$, respectively. The skeleton of $\mathcal{G}$ is the undirected graph obtained from $\mathcal{G}$ by replacing directed edges with undirected edges. Nodes $i,j,k\in V$ form a \textbf{v-structure} if  $i$ and $j$ point to $k$, and there is no direct edge between $i$ and $j$. In that case, $k$ is called an unshielded \textbf{collider} and $i$, $j$ are spouses, notation $i\in sp_j(\mathcal{G})$ (and $j\in sp_i(\mathcal{G})$). Even if $i$ and $j$ have an edge connecting them, node $k$ is called a collider. We say that the node $i\in V$ is a \textbf{source} node if $pa_j(\mathcal{G})=\emptyset$, notation $i\in Source(\mathcal{G})$.  We omit the argument $\mathcal{G}$ if evident from the context. 

Consider a random vector $(X_i)_{i\in V}$ over a probability space $(\Omega, \mathcal{A}, P)$. With a slight abuse of notation, we identify the vertices $j \in V$ with the variables $X_j$.  We denote $\textbf{X}_S = \{X_s{:}\,\,  s\in S\}$ for $S\subseteq V$. Typically, we denote $V = \{0, \dots, p\}$, where $X_0$ (usually denoted by $Y$) is a target variable and $\textbf{X}=(X_1, \dots, X_p)^\top$ are other variables (covariates). We assume that $(Y,\textbf{X})$ follow a \textbf{structural causal model} (SCM) with a DAG $\mathcal{G}$  \citep{Pearl_book}, that is, each variable arises from a structural equation 
\begin{equation}\label{definition_general_SCM}
    X_i=f_i(\textbf{X}_{pa_i}, \eta_i),\,\,\,\,\,\,\,i=0,1,\dots, p,
\end{equation}
where $\eta_i$ are jointly independent random variables\footnote{We distinguish between uniformly distributed noise variables (denoted by $\varepsilon_i$) and arbitrarily distributed noise variables (denoted by $\eta_i$). Under the relation $\varepsilon_i = q(\eta_i)$ where $q$ is the distribution of $\eta_i$, these cases are equivalent and we use the former notation in the paper. However in Section~\ref{section_notation}, in order to be consistent with the notation used in the literature, we use the latter.}. The measurable functions $f_i$ are the assignments (or link functions), and we say that $X_j$ is a direct cause of $X_i$ if $j\in pa_i$. 

A joint distribution $P_\mathbf{X}$ over $\textbf{X}$ satisfies a \textbf{Markov property} with respect to $\cal{G}$  if $A\indep_{\cal{G}} B\mid C \implies A\indep B\mid C$ for $A,B,C\subseteq V$ are disjoint subsets of the vertices and  $\indep_{\cal{G}}$ represent a d-separation in $\mathcal{G}$ \citep{Pearl}. On the other hand, if for all $A,B,C\subseteq V$ disjoint subsets of the vertices hold $A\indep_{\cal{G}} B\mid C \impliedby A\indep B\mid C$, we say that the distribution is \textbf{faithful} with respect to $\mathcal{G}$.  $P_\mathbf{X}$ satisfies \textbf{causal minimality} with respect to $\mathcal{G}$ if it is Markov with respect to $\mathcal{G}$, but not to any proper subgraph of $\mathcal{G}$. It can be shown that faithfulness implies causal minimality \citep[Proposition 6.35]{Elements_of_Causal_Inference}. Two graphs $\mathcal{G}_1, \mathcal{G}_2$ are Markov equivalent if the set of distributions that are Markov with respect to $\mathcal{G}_1, \mathcal{G}_2$ is the same. We denote the \textbf{Markov equivalency class} of $\mathcal{G}$ as the set $MEC(\mathcal{G}):=\{\mathcal{G}':  \mathcal{G}\text{ and } \mathcal{G}' \text{ are Markov equivalent}\}$. Two graphs are Markov equivalent if they have the same skeleton and the same v-structures \citep{Pearl}. 

A set of variables $\textbf{X}$ is said to be causally \textbf{sufficient} if there is no hidden common cause that causes more than one variable in $\textbf{X}$ \citep{Sprites2010}. A set  $S\subseteq V$ is called \textbf{Markov blanket} for $X_i$ if $X_i\indep X_{V\setminus (S\cup \{i\})}\mid \textbf{X}_S$. A \textbf{Markov boundary} is a minimal Markov blanket, denoted by $MB_i(\mathcal{G})$. Note that $MB_i(\mathcal{G}) = pa_i(\mathcal{G})\cup ch_i(\mathcal{G}) \cup sp_i(\mathcal{G})$.

For $S\subseteq V$, we define a \textbf{projection} of graph $\mathcal{G}$ on $S$, denoted as $\mathcal{G}[S]$, as a graph with vertices $S$ and the following edges: for distinct $i, j\in S$, there is an edge $i\to j$ in $\mathcal{G}[S]$ if and only if there is a directed path from $i$ to $j$ in $\mathcal{G}$ such that all vertices on this path except $i,j$ do not belong to $S$ ($i\to\dots\to j$).  Moreover, there is a bidirected edge $i-j$ if a path exists between $i,j$ in $\mathcal{G}$  that does not contain a collider and the first edge points towards $i$ and the last edge points towards $j$ ($i\leftarrow\dots\to j$).  

Let $(X_0, \textbf{X})\in\mathbb{R}\times \mathbb{R}^p$ follow an SCM (\ref{definition_general_SCM}) with DAG $\mathcal{G}_0$. Let $\mathcal{F}$ be a subset of all measurable functions. We say that the SCM follows an $\mathcal{F}$\textbf{-model}, if each structural equation in the SCM satisfies $f_i\in\mathcal{F}, i=0, \dots, p$. We say that $\mathcal{G}_0$ is \textbf{identifiable} from the joint distribution under the $\mathcal{F}$-model (we also say that the causal model is identifiable) if there is no graph $\mathcal{G}'\neq \mathcal{G}_0$ and functions $f_i'\in\mathcal{F}, i=0, \dots, p$ generating the same joint distribution. 

In the following, we discuss some well-known results from the literature that address the problem of identifiability and estimation of the causal structure; for a review see \cite{ZhangReview}.

\cite{Lingam} show that $\mathcal{G}_0$ is identifiable under the LiNGaM model (Linear Non-Gaussian additive Models where $\mathcal{F}$ consists of all linear functions and the noise variables are non-Gaussian). \cite{hoyer2008} and \cite{Peters2014}  developed a framework for additive noise models (ANM) where $\mathcal{F}$ consists of functions additive in the last input, that is,  $X_i = g(\textbf{X}_{pa_i}) +  \eta_i$. Under some weak assumptions on $g$,  $\mathcal{G}_0$ is identifiable  \citep[Corollary 31]{Peters2014}, and the authors propose an algorithm for estimating $\mathcal{G}_0$ (for a review on ANM, see \citealp{reviewANMMooij}). 
\cite{Zhang2009} consider the post-nonlinear (PNL) causal model where $\mathcal{F}$ consists of post-additive functions, that is, 
\begin{equation*}
X_i = g_1\big(g_2(\textbf{X}_{pa_i}) +  \eta_i\big)
\end{equation*}
with an invertible link function $g_1$.  Note that the former two are special cases of the PNL model. However, the PNL model is identifiable under some technical assumptions and with several exceptions \citep{Zhang2010}. 
 \cite{ParkVariance} show identifiability in models where $var[X_i\mid \textbf{X}_{pa_i}]$  is a quadratic function of $\mathbb{E}[X_i\mid \textbf{X}_{pa_i}]$. \cite{Galanti} consider the neural SCM with representation $X_i = g_1\big(g_2(\textbf{X}_{pa_i}), \eta_i\big),$ where $g_1, g_2$ are assumed to be neural networks. \cite{Khemakhem_autoregressive_flows}, \cite{immer2022identifiability}, and \cite{strobl2022identifying} propose several methods and identifiability results for location-scale models, where  $\mathcal{F}$ consists of location-scale functions, that is, $X_i = g_1(\textbf{X}_{pa_i}) +  g_2(\textbf{X}_{pa_i})\eta_i.$

\cite{bodik2023identifiability} consider a class of conditionally parametric causal models ($CPCM(F)$), where $X_i\mid \textbf{X}_{pa_i}$ has the conditional distribution $F$ with parameters $\theta_i(\textbf{X}_{pa_i})\in\mathbb{R}^q$ for some $q\in\mathbb{N}$.  That is, the data-generation process is of the form 
\begin{equation}\label{CPCM_def}
X_i=f_i(\textbf{X}_{pa_i}, \varepsilon_i) = F^{-1}\big(\varepsilon_i; \theta_i(\textbf{X}_{pa_i})\big), \,\,\,\,\,\text{ equivalently } X_i\mid \textbf{X}_{pa_i}\sim F\big(\theta_i(\textbf{X}_{pa_i})\big),
\end{equation}
where $\varepsilon_i\sim U(0,1),$ and $F$ is a known distribution function with parameters $\theta_i$ being functions of the direct causes of $X_i$. This model represents a system where the direct causes only affect some characteristics of the distribution but not the class itself. For example, if $F$ is Gaussian, then we are in the Gaussian Location-scale Models framework.


\begin{example}[Gaussian case]\label{Gaussian case}
Suppose that $(X_0,\textbf{X})$ follow $CPCM(F)$ with a Gaussian distribution function $F$. This corresponds to $X_i\mid \textbf{X}_{pa_i}\sim N\big(\mu_i(\textbf{X}_{pa_i}), \sigma_i^2(\textbf{X}_{pa_i})\big)$ for all $i=0, \dots, d$ and for some functions $\theta_i = (\mu_i, \sigma_i)^\top{:}\,\, \mathbb{R}^{|pa_i|}\to\mathbb{R}\times \mathbb{R}^+$. Using linearity of mean and variance in a Gaussian distribution, this is equivalent to assuming that the data-generation process has a form 
$$
X_i = \mu_i(\textbf{X}_{pa_i}) + \sigma_i(\textbf{X}_{pa_i})\eta_i, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \eta_i \text{  is Gaussian.}
$$
Potentially, source nodes can have arbitrary distributions. 

Consequence 3 in \cite{bodik2023identifiability} shows that $\mathcal{G}_0$ is identifiable unless $\mu_i, \sigma_i$ have a specific functional form. More precisely,  $\mathcal{G}_0$ is identifiable if the arguments of $\mu_i, \sigma_i$ (denoted by $\mu_{i,j}, \sigma_{i,j}$, $j=1, \dots, |pa_i|$) do \textit{not} satisfy:  
\begin{equation}\label{norm}
\frac{1}{\sigma_{i,j}^2(x)}=ax^2 + c, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \frac{\mu_{i,j}(x)}{\sigma_{i,j}^2(x)}=d+ex,\,\,\,\,\,\,\,\,\,\,\,x\in\mathbb{R},
\end{equation}
where $a,d,e\in\mathbb{R}, c>0$ are some constants. 
\end{example}
\cite{bodik2023identifiability} show that $\mathcal{G}_0$ is generally identifiable in  $CPCM(F)$ for any $F$ belonging to the exponential family of distributions, with an exception when $\theta$ is equal to the sufficient statistic of $F$. 

 \begin{example}[Pareto case]\label{Pareto case}
Suppose that $(X_0,\textbf{X})$ follow $CPCM(F)$, where $F$ is the Pareto distribution function, that is, the corresponding density function has a form $p(y) = \frac{\theta}{y^{\theta+1}}, \theta>0, y\geq 1$. This model corresponds to a data-generation process where $$X_i\mid \textbf{X}_{pa_i}\sim Pareto\big(\theta_i(\textbf{X}_{pa_i})\big).$$ If $\theta(\textbf{X}_{pa_i})$ is small, then the tail of $X_i$ is large and extremes occur more frequently. Consequence 1 in \cite{bodik2023identifiability} shows that $\mathcal{G}_0$ is identifiable if  $\theta(\textbf{X}_{pa_i})$ does \textit{not} have a logarithmic form in any of its arguments. 
\end{example}



%\subsection{ICM (invertible causal model)}
%\label{Section_ICM}
%
%We introduce a class of invertible causal models, a subclass of structural causal models where the noise %variables $\varepsilon_i$ can be fully recovered from the observed variables. 
%
%\begin{definition}\label{I}
%Let $\mathcal{X}_x\subseteq\mathbb{R}^{p},\mathcal{X}_y\subseteq\mathbb{R}, \mathcal{X}_z\subseteq\mathbb{R}$ be measurable sets. A measurable function $f:\mathcal{X}_x\times\mathcal{X}_y\to \mathcal{X}_z$  is called \textbf{invertible for the last element}, notation $f\in \mathcal{I}$, if there exists a function $f^{\leftarrow}:\mathcal{X}_x\times\mathcal{X}_z\to \mathcal{X}_y$ that fulfills the following: $\forall \textbf{x}\in\mathcal{X}_x, \forall y\in\mathcal{X}_y,z\in\mathcal{X}_z$ such that $y=f(\textbf{x},z)$, then $z=f^{\leftarrow}(\textbf{x},y)$. Moreover, denote 
%$$
%\mathcal{I}_{m} = \{f\in\mathcal{I} : \text{ f is not constant in any of its arguments} \}.
%$$
%We define \textbf{ICM} as an SCM (\ref{definition_general_SCM}) whose structural equations $f_i\in\mathcal{I}_{m}$ for all $i=0, \dots, p$.  
%\end{definition}
%The previous definition describes that the element $z$ in a relation $y=f(\textbf{x},z)$ can be uniquely recovered from $(\textbf{x},y)$. To provide an example, for the function $f(x,z) = x+z$ holds $f^{\leftarrow}(x, y) = y-x$, since $f^{\leftarrow}(x, f(x,z)) = f(x,z) - x = z$. More generally, for the additive function defined as $f(\textbf{x},z) = g_1(\textbf{x})+ g_2(z), $ holds $f^{\leftarrow}(\textbf{x}, y) = g_2^{-1}(y - g_1(\textbf{x}))$, if the inverse $g^{-1}$ exists, $\textbf{x}\in\mathbb{R}^d, y, z\in \mathbb{R}$. Overall, if $f$ is differentiable and the partial derivative of $f(\textbf{x},z)$ with respect to $z$ is monotonic, then $f\in\mathcal{I}$ (follows from Inverse function theorem \cite{Inverse_function_theorem}). 
%Note that $f_i\in\mathcal{I}_{m}$ implies causal minimality of the ICM model (the subscript $m$ in $\mathcal{I}_{m}$ represents the word "minimality"), see Lemma~\ref{Lemma_about_ICM_minimality} in Appendix \ref{Appendix_A.1.}.


