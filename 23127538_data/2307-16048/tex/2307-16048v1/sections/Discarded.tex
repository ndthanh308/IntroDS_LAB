
The following lemma shows that the set $\mathcal{F} = \mathcal{I}_m$ is too large to identify any parents of a target variable. 

\begin{lemma}
Let $(X_0,\textbf{X})$ follow ICM model with DAG $\mathcal{G}$, such that its distribution $P_{(X_0,\textbf{X})}$  has a density with respect to Lebesgue measure. Consider a DAG $\tilde{\mathcal{G}}\neq \mathcal{G}$ such that $P_{(X_0,\textbf{X})}$ is Markov with respect to $\tilde{\mathcal{G}}$. Then, there exist ICM model with DAG $\tilde{\mathcal{G}}$ that entails the distribution $P_{(X_0,\textbf{X})}$.    
\end{lemma}
\begin{proof}
In the proof of Proposition 7.1 in \cite{Elements_of_Causal_Inference}
 is described a method how to define a SCM with DAG $\tilde{\mathcal{G}}$  that entails the distribution $P_{(X_0,\textbf{X})}$. However, such defined SCM also satisfies $f_i\in\mathcal{I}_m$ for all $i=0, \dots, p$. 
\end{proof}


\begin{customthm}{\ref{thmAssymetricMultivariatesufficient}}
Let $(X_1, X_2)$ follow an asymmetrical $(\mathcal{M}_{F_1},\mathcal{M}_{F_2})$-causal model defined in (\ref{asymetrical_F_one_F_two_model}), where $F_1, F_2$ lie in an exponential family of distributions and $T_1, T_2$ are the corresponding sufficient statistics.
Then, the causal graph is identifiable if $\theta_1\not\in \mathfrak{G}_{T_2, T_1}$, or if $\theta_2\not\in \mathfrak{G}_{T_1, T_2}$.

In particular, if the causal graph is not identifiable, then there exist constants $a_i, b_i\in\mathbb{R} $, $a_i\neq 0$, such that 
\begin{equation}\tag{\ref{eqmultivariate}}
\sum_{i=1}^{q_1} a_i\theta_{2,i}'(x) = \sum_{i=1}^{q_2} b_iT_{1,i}'(x), \,\,\,\,\,\,\forall x\in\Omega_T,
\end{equation}
where $\theta_2(x) = (\theta_{2,1}(x), \dots, \theta_{2,q_1}(x))^\top$ is the parameter corresponding to model with $X_1\to X_2$ and $T_1(\cdot) = (T_{1,1}(\cdot), \dots, T_{1,q_2}(\cdot)$ are the sufficient statistics.  
\end{customthm}


\begin{proof}
\label{Proof for thmAssymetricMultivariatesufficient}
If the asymmetrical $(\mathcal{M}_{F_2},\mathcal{M}_{F_1})$-causal model is \textit{not} identifiable, then there exist functions $\theta_1, \theta_2$ such that $X_1 = \varepsilon_1, X_2 = F_2^{-1}(\varepsilon_2, \theta_2(X_1))$ and $X_2 = \varepsilon_2, X_1 = F_1^{-1}(\varepsilon_1, \theta_1(X_2))$ have the same joint density function. 

Write the joint density as
\begin{equation}\label{eq59}
  p(x,y) = p_{X_1}(x_1)p_{X_2\mid {X_1}}(y\mid x) = p_{X_2}(y)p_{{X_1}\mid {X_2}}(x\mid y).
 \end{equation}
Using the notation from (\ref{Exponential family of distributions}), write $p_{{X_2}\mid {X_1}}(y\mid x) = h_{1,1}(y)h_{1,2}[\theta_1(x)]\exp[\sum_{i=1}^{q_2}\theta_{1,i}(x)T_{2,i}(y)]$ and $p_{{X_1}\mid {X_2}}(x\mid y) = h_{2,1}(x)h_{2,2}[{\theta_2}(y)]\exp[\sum_{i=1}^{q_1}{\theta}_{2,i}(y)T_{1,i}(x)]$. 

Now, after a logarithmic transformation of both sides of (\ref{eq59}), we obtain 
\begin{equation}
\begin{split}
\log[p(x,y)] &= \log[p_{X_1}(x)] +  \log[h_{1,1}(y)]+\log\{h_{1,2}[\theta_{1,i}(x)]\} + \sum_{i=1}^{q_2}\theta_{1,i}(x)T_{2,i}(y) \\&
= \log[p_{X_2}(y)] +  \log[h_{2,1}(x)]+\log\{h_{2,2}[\theta_{2,i}(y)]\} + \sum_{i=1}^{q_1}\theta_{2,i}(y)T_{1,i}(x).
\end{split}
\end{equation}
Now, take $\frac{\partial^2 }{\partial x \partial y }$  (differentiate wrt. $x$ and $y$) on both sides. We obtain 
$$
\sum_{i=1}^{q_2}\theta_{1,i}'(x)T_{2,i}'(y) = \sum_{i=1}^{q_1}\tilde{\theta}'_{2,i}(y)T_{1,i}'(x).
$$
That is what we wanted to show. The second part of the claim is trivial by fixing $y$ (note that $T_1'(y) $ is not a zero function unless $X_1\indep {X_2}$). 
 \end{proof}



\section{Generalization of ICP for extremes}

Assume we have $n_e$ observations in each environment $e\in\E$. For each $e\in\mathcal{E}$ there is a target variable $Y^e_i$ and covariates $\textbf{X}^e_i=(X_{1,i}^e, \dots, X_{d,i}^e)^\top$, $i=1, \dots, n_e$. Denote a generic element of this sample as $(\textbf{X}^e, Y^e)$. Denote  $S^\star\subseteq \{1, \dots, d\}$ the true direct causes of the target variable. 

Under Assumption \ref{Assumption1}, we know that the structural equation for the response variable is in form $Y^e=g(\textbf{X}^e_{S^\star}, \varepsilon^e),\varepsilon^e \independent  \textbf{X}^e_{S^\star}$. The distribution of the noise variables and the function $g:\R^{|S^\star|+1}\to \R$  are invariant through environments. 

An usual assumption for the function $g$ \footnote{\cite{Peters} mostly dealt with linear function $g$, and \cite{Christina} mostly with additive form $g(\textbf{X}^e_{S^\star}, \varepsilon^e)=g(\textbf{X}^e_{S^\star}) + \varepsilon^e $ for some smooth function $g$.} is an additivity, i.e. $g(\textbf{X}^e_{S^\star}, \varepsilon^e)=g(\textbf{X}^e_{S^\star}) + \varepsilon^e $. This directly implies that the variables  $\textbf{X}^e_{S^\star}$ affects only the expectation, but the variance and other characteristics of $Y\mid \textbf{X}^e_{S^\star}$ remain constant. In this section, we aim to generalize this approach, allowing the covariates to affect also the other characteristics besides the expectation.

\subsection{ICP for higher moments}
Main idea of our ICP extension lies in the following: for an additive model $Y=g(\textbf{X}_{S^\star}) + \varepsilon$, one can easily define residuals $Y-\hat{g}(\textbf{X}_{S^\star}) = \hat{\varepsilon}$ and work with their invariance. \cite{Christina} explores an approach that first fits a model with data pooled overall environments and then tests for differences between the residual distributions across environments. Here, $\hat{g}$ is an estimation that follow some statistical or machine learning methods.

However, residuals in the model $Y=g(\textbf{X}_{S^\star},\varepsilon)$ are more delicate to define. One possibility is to define "generalized inverse"  $\varepsilon = g^{-1}(Y, \textbf{X}_{S^\star})$, if such $g^{-1}$ exists 
\footnote{Which is trivial in the additive model where $\varepsilon=g^{-1}(y,\textbf{x})=y-g(\textbf{x})$. } and if a consistent estimation of $\hat{g}^{-1}$ is available. This is possible in all parametric models, which enlarge the possible applications of the method. 
We define $\hat{\eta} = \hat{g}^{-1}(Y, \textbf{X}_{S^\star})$, where the link function $\hat{g}^{-1}(Y, \textbf{X}_{S^\star})$ can be estimated by parametric or even non-parametric methods. Here, $\hat{\eta}$ serves as an estimated counter-value of the noise $\varepsilon$.  
Under appropriate assumptions, we should end up with $\hat{\eta}$ being invariant through all environments (Theorem \ref{Theorem 2}). 
Hence, the idea from ICP methodology can be used. 

\subsection{Formal definitions}
\begin{definition}
A function $g:\R^{m}\to \R$  is called \textbf{invertible for the last element}, notation $g\in I$, if there exists a function $g^{-1}:\R^{m}\to \R$ that fulfills the following: $\forall \textbf{x}\in\R^{m-1}, \forall y,z\in\R$ such that $y=g(\textbf{x},z)$, then $z=g^{-1}(\textbf{x},y)$. 
\end{definition}
\begin{observation}
 $g\in I$ if one of the following hold:
\begin{itemize}
\item $g$ is additive,
\item $g$ is differentiable and the partial derivative of $g(\textbf{x},z)$ with respect to $z$ is monotonic (follows from Inverse function theorem (\cite{Inverse_function_theorem})),
\item  $g$ is a quantile function of a parametric family whose parameter is a function of the first $m-1$ elements (see examples below). 
\end{itemize}
\end{observation}

Let us define some important special cases of parametric models for $g$. 

\begin{example}[Linear regression model $Y|\textbf{X}_{S^\star} = \beta^\top \textbf{X}_{S^\star} + \varepsilon$ ]\label{2C}
$\varepsilon^e\sim N(0,\sigma)$, $g(\textbf{x},\varepsilon)=\beta^\top \textbf{x} + \varepsilon$. We assume that $\sigma>0$ and $\beta\in\R^{|S^\star|}$. Moreover, we assume that $\beta$ does not contain any zero element. (An exact setup from Section 2 in \cite{Peters}). 
\end{example}

\begin{example}[Beta Model $Y|\textbf{X}_{S^\star} \sim Beta(A(\textbf{X}_{S^\star}), 1)$ ]\label{2B}
$\varepsilon^e\sim U(0,1)$, $g:\R^{|S^\star|+1}\to \R : g(\textbf{x}, \varepsilon) =  \varepsilon^{\frac{1}{A(\textbf{x})}}$ is a quantile function of a Beta distribution with parameters $A(\textbf{x})$ and $1$, for some function $A(\cdot)>0$ (function with arguments $\textbf{X}_{S^\star}$). Moreover, we assume a structural minimality for $A$  \footnote{Structural minimality implies that the function depend on all of its arguments, page 87 in (\cite{Elements_of_Causal_Inference}).}. 
\end{example}

\begin{example}[GEV Model $Y|\textbf{X}_{S^\star} \sim GEV(\mu(\textbf{X}_{S^\star}), \sigma(\textbf{X}_{S^\star}), \xi(\textbf{X}_{S^\star}))$ ]\label{2A}
$\varepsilon^e\sim U(0,1)$, $g:\R^{|S^\star|+1}\to \R$ is a GEV quantile function with parameters in a functional form  $\mu(\textbf{X}^e_{S^\star}), \sigma(\textbf{X}^e_{S^\star}), \xi(\textbf{X}^e_{S^\star})$. \footnote{More explicitely, we mean a form $g(x, \varepsilon) =  
\mu(x) + \frac{\sigma(\textbf{x})}{\xi(\textbf{x})}( (-\log(\varepsilon)^{-\xi(\textbf{x})}) -1 )$ for $\xi(\textbf{x})\neq 0$ and for the limiting case $\xi(\textbf{x})= 0$ is $g(\textbf{x}, \varepsilon) =  \mu(\textbf{x})-
\sigma(\textbf{x})\log(-\log(\varepsilon))$.}

We assume a joint structural minimality for $\mu, \sigma, \xi$ \footnote{If $i\in S^\star$, then at least one of the functions $\mu, \sigma, \xi$ "depends" on $x_i$ in the sense of (\cite{Elements_of_Causal_Inference})}. 
\end{example}


\begin{remark}
All previous models have $g\in I$. In the GEV model and Beta model, $g^{-1}$ is a GEV distribution function and Beta distribution function respectively (with parameters being functions of covariates). 
\end{remark}

\begin{remark}
In general, we can assume WLOG that $\varepsilon\sim U(0,1)$. Since we did not specifically defined $g$, we can say that all the "unknown parameters" comes from $g$ itself, not from $\varepsilon$. Similar idea follows from \cite{Peters2014}, Proposition 9.
\end{remark}

\begin{remark}
Let us assume that $Y|\textbf{X}_{S^\star}$ has a distribution function in a form $F(\cdot; \theta(\textbf{X}_{S^\star}))$ where $\theta(\textbf{X}_{S^\star})$ is a covariate-dependent parameter (for example, Gaussian family with $\mu, \sigma$ being a functions of $\textbf{X}_{S^\star}$). We can define $g$ as a quantile function $g(\textbf{x},y)=F^{-1}(y, \theta(\textbf{x}))$. Then it indeed holds that  $Y|\textbf{X}_{S^\star}$ has the distribution function in a form $F(\cdot; \theta(\textbf{X}_{S^\star}))$ with $\varepsilon\sim U(0,1)$. 
\end{remark}

\subsection{Non-parametric generalizations  }
\label{Section about non-parametric generalizations}

Denote by $F_{Y\mid \textbf{X}_{S^\star}}$ the conditional distribution of the response variable conditioned on its direct causes (under Assumption 1 it does not depend on $e\in\E$). Define $\eta^e:=F_{Y\mid \textbf{X}_{S^\star}}(Y^e) $.  

Without giving an explicit form of $g$, we can straight away look at the form of  $F_{Y\mid \textbf{X}_{S^\star}}$. 
If we  don't want to put some special form on this relation, we can even handle it fully non-parametrically. However, assuming a parametric form of $F_{Y\mid \textbf{X}_{S^\star}}$ can improve the inference part. 

\cite{KCI} deals with a non-parametric testing of the invariance (\ref{hat(S)}). Since it conditions on $\textbf{X}$, it needs too large sample size for practical purposes. We will only consider parametric families in this paper, since it is justified from asymptotic results. 



\subsection{Invariance of residuals}

Let  $g\in I$  and let us have a consistent estimator (consistent in the sense of Theorem \ref{Theorem 2}) $\hat{g}^{-1}$ of $g^{-1}$. This estimation needs to be based on a pooled dataset \footnote{This is not only for power properties or computational time improvements.  If we estimate $g^{-1}$ in each environment separately, we could loose some identifiability properties (see also argument for linear models in Section 3 in \cite{Peters}).}.
The following theorem shows that $\hat{\eta}^e = \hat{g}^{-1}(\textbf{X}^e_{S^\star}, Y^e)$ are invariant in $\E$ under the correct causal set $S^\star$. 

\begin{theorem}
\label{Theorem 2}
Assume that $(Y, \textbf{X})$ follow a SCM and $S^\star\subseteq \{1, \dots, d\}$ be a non-empty set of true direct causes of $Y$. Let the following hold:

\begin{itemize}
\item Assumption 1 is valid, 
\item $g\in I$ and  $\hat{g}^{-1}$ being a consistent estimator of $g^{-1}$ in the sense that $\hat{g}^{-1}(\textbf{X}_{S^\star}^e, Y^e) - g^{-1}(\textbf{X}_{S^\star}^e, Y^e)\overset{P}{\to} 0$ for all $e\in\E$. Define $\hat{\eta}_{S^\star}^e := \hat{g}^{-1}(\textbf{X}_{S^\star}^e, Y^e)$. 
\end{itemize}

Take $e\neq f\in\mathcal{E}$. Then, as the number of data (in both environments) tends to infinity, holds $\hat{\eta}^e_{S^\star}\overset{D}{=} \hat{\eta}^f_{S^\star}$. 
\end{theorem}
\begin{proof}
Proof is trivial since $g^{-1}(\textbf{X}_{S^\star}^e, Y^e)\overset{D}{=} g^{-1}(\textbf{X}_{S^\star}^f, Y^f)$ from Assumption 1, and $\hat{\eta}_{S^\star}^e \overset{D}{\to}g^{-1}(\textbf{X}_{S^\star}^e, Y^e)$ ; $g^{-1}(\textbf{X}_{S^\star}^f, Y^f)\overset{D}{\leftarrow}\hat{\eta}_{S^\star}^f$ holds from the consistency assumption. 
\end{proof}

We would like to see that $\hat{\eta}^e_{S}\overset{D}{\neq} \hat{\eta}^f_{S}$ for  $S\subseteq S^\star$. We only prove it in the special case when we assume Linear regression model from Example \ref{2C}. 

\begin{theorem}
\label{Theorem 3}
Assume that $(Y, \textbf{X})$ follow a SCM and $S^\star\subseteq \{1, \dots, d\}$ be a non-empty set of true direct causes of $Y$. Suppose that Assumption 1 and Linear assumption from Example \ref{2C} are valid. Let $i\in S^\star$ and set $S= S^\star\setminus \{i\}$. Suppose that $e\neq f\in\mathcal{E}$ fulfill the following condition: $$X_i^f -  d^\top\textbf{ X}_S^{f}\overset{D}{\neq}X_i^e -  c^\top \textbf{X}_S^{e},$$ where $c=\mathbb{E}[(\textbf{X}_S^e \textbf{X}^{e^\top}_S)^{-1} cov(\textbf{X}_S^e, X^e_i) ]$, $d=\mathbb{E}[(\textbf{X}_S^{f} \textbf{X}^{f^\top}_S)^{-1}cov(\textbf{X}_S^f, X^f_i)]$. 

Define $\hat{\eta}^e_S = Y^e - \textbf{x}_S^{\top e}\hat{\beta}_S$ where $\hat{\beta}_S$ is the classical least square estimator based on a pooled dataset.  Then, as the number of data (in both environments) tends to infinity, holds $\hat{\eta}^e_S\overset{D}{\neq} \hat{\eta}^f_S$.

\end{theorem}
\begin{proof}

This is a consequence of a known problem in linear regression about omitted-regressor bias \citep{Omitted_Variable_Bias}. It is well-known that if $\hat{\beta}_S^e$ is an LSE estimate based on regressing $Y^e$ and $X_S^e$ (while the true model contains the entire $S^\star$), then it converges to $\beta_S + bias$ where $bias=\beta_i \mathbb{E}[(X_S^e X^{e^\top}_S)^{-1} cov(X_S^e, X^e_i) ]=\beta_i c $. Therefore, $\hat{\eta}^e_{S}:=Y^e - X_{S}^{e^\top}\hat{\beta}_S = \beta_i X_i^e + \beta_SX_S^e + \varepsilon - X_{S}^{e^\top}\hat{\beta}_S \overset{D}{\to}  \beta_iX_i^e - \beta_i  c^\top X^e_S +\varepsilon =  \varepsilon + \beta_i (X_i^e -  c^\top X_S^{e}) $.
Similarly $\hat{\eta}^f_{S}\overset{D}{\to} \alpha+\varepsilon + \beta_i (X_i^f -  d^\top X_S^{f})$. Therefore, as because $\beta_i\neq 0$ from the assumption that $X_i$ is a direct cause of $Y$, we get $\hat{\eta}^e_S\overset{D}{\neq} \hat{\eta}^f_S\iff (X_i^f -  d^\top X_S^{f})\overset{D}{\neq}(X_i^e -  c^\top X_S^{e}) $ what we wanted to show.
\end{proof}

\begin{remark}
A condition defined in Theorem \ref{Theorem 3} describes that the distribution of $X_i$ changed between environments, and this change can not be "deterministically" captured by other covariates. For example, if $X_i$  and $\textbf{X}_S$ are independent, the condition becomes just about changing the distribution of $X_i$ arbitrarily. 
\end{remark}
It may seem intuitive that a similar condition holds also for more general models, although it becomes too difficult to rigorously formulate and prove it. 


\subsection{Estimating $g^{-1}$ }

Estimation of $g^{-1}$ is straightforward, when we assume a parametric form, whose parameters are modeled as a functions of $X$. Let  $g^{-1}(\textbf{x},y)$, $\textbf{x}\in\R^{|S^\star|}, y\in\R$ be a distribution function $g^{-1}(\textbf{x},y)=F(y, \theta(\textbf{x}))$ where $F$ is a know distribution function and $\theta(\textbf{x})$ are parameters corresponding to a distribution family (Examples \ref{2A} and \ref{2B} are in this parametric form). Then, using appropriate statistical or machine learning methods, we can estimate parameters  $\hat{\theta}(\textbf{x})$ and define $\hat{g}^{-1}(\textbf{x},y) = F(y, \hat{\theta}(\textbf{x}))$.

We will use GAM modelling (\cite{GAM}) or more generally GAMLSS (\cite{GAMLSS}) as this is the classic choice for GEV modelling. Moreover, GAM was also a recommended choice in the related work (\cite{Christina}), where additive link functions were considered. 

\begin{remark}
Another approach is to estimate $\hat{F}_{Y\mid \textbf{X}_{S^\star}}$ fully non-parametrically. This can be done using for example some kernel smoothing (\cite{Hall}). This approach is beyond the scope of this paper. 
\end{remark}


\subsection{Comparing residuals}

Under the conditions from Theorem \ref{Theorem 2}, the residuals corresponding to the true set $S^\star$ should have (asymptotically) the same distribution in all environments. However, these distributions will (ideally) differ if we omit some variable $i\in S^\star$ on which we intervened on. The difference between $\hat{\eta}^e_S, \hat{\eta}^f_S$ can be potentially arbitrary. 

The most general test can correspond to the d-sample Kolmogorov-Smirnov test, or the d-sample Anderson-Darling test (which is believed to behave better than the KS test in most cases). However, these tests are well known for their low power. We can also look at only their first two moments which would increase the power, with the price of losing the generality of the results. 

In the following, we propose testing $H_{0, S}(\mathcal{E})$ as a d-sample AD-test\footnote{D-sample AD-test tests a difference between d-random samples in their distribution (potentially arbitrarily change between any pair). Two sample test uses a distance $AD_{n,m}=\frac{mn}{n+m}\int_{-\infty}^{\infty}\frac{(F_n(x)-G_m(x))^2}{H_{n+m}(x)(1-H_{n+m}(x))} dH_{n+m}(x)$ where $H_{n+m}$ is an empirical distribution function of the pooled datasets. D-sample AD-test is a natural generalization of of two-sample test  \cite{AD-test}.  }
 between residuals with level $\alpha$. Ignoring the sample variability of the estimates (we are not yet in the asymptotic case), we can use Estimator \ref{hat(S)} for an estimation of the causal covariates. Validity of Theorem \ref{Theorem_Level} still holds.   


\begin{remark}[If we make a mistake, we make a similar one in each environment]
In practice, it is hard to say if we are close to the asymptotic properties or not. If $Y(\textbf{x}_{S})= \textbf{x}_{S}^\top\beta+ \varepsilon$, is the distribution of residuals $\hat{\eta}_S = Y(\textbf{x}_S) - \textbf{x}_{S}^\top\hat{\beta}$ close to the distribution of $\varepsilon$? 

If the uncertainity on $\hat{\beta}$ is still large, the 
\end{remark}



\subsection{Final algorithm TODO}

Steps from the previous subsections can be summarized in the following 5-step "algorithm":


\begin{enumerate}
\item We pool the data from all environments,
\item We estimate $\hat{g}^{-1}$ for example using GAM and compute the residuals $\hat{\eta}_i$,
\item Split the residuals corresponding to each environment separately,
\item Using d-sample AD-test, we test whether the distribution of these residuals significantly differ between the environments (on confidence level $\alpha=0.05$),
\item Repeat for every $S\subseteq \{1, \dots, d\}$ and take $\hat{S}^\star$ as the intersection of the non-rejected ones. 
\end{enumerate}








\begin{lemma}
Let  \begin{align*}
    Y&\sim N(\begin{pmatrix}
           \mu_1 \\
            \mu_2 
         \end{pmatrix}, 
         \begin{pmatrix}
           \sigma_1^2 , \rho\sigma_1\sigma_2 \\
           \rho\sigma_1\sigma_2, \sigma_2^2
         \end{pmatrix}).
 \end{align*}
 Then, $
    f(Y)\sim N(\begin{pmatrix}
           0 \\
            0 
         \end{pmatrix}, 
         \begin{pmatrix}
           1 , 0 \\
           0,1
         \end{pmatrix})$ for a linear function $$f(\begin{pmatrix}
          Y_1\\
            Y_2
         \end{pmatrix}) = A\begin{pmatrix}
           \frac{Y_1-\mu_1}{\sigma_1}\\
            \frac{Y_2-\mu_2}{\sigma_2}
         \end{pmatrix}, $$
  where $A=\begin{pmatrix}
           \frac{1}{\sqrt{2(1+\rho)}}, \frac{1}{\sqrt{2(1+\rho)}}\\
            \frac{1}{\sqrt{2(1-\rho)}}, -\frac{1}{\sqrt{2(1-\rho)}}
         \end{pmatrix}       .
         $
\end{lemma}





\begin{theorem}
\label{LinearReg}
Let $(Y_1, X_1^\top)^\top, (Y_2, X_2^\top)^\top,\dots$ are iid with a generic element $(Y, X^\top)^\top,$ where $X$ is $d-$dimensional  vector of covariates. 

Let us have a model where  $
Y=X^\top\beta +\varepsilon
$, where $\varepsilon$ is is a noise variable (not-necessarily normal) with zero mean and finite variance and $\beta$ is a $d-$dimensional vector of real numbers.

Let $\mathbb{E}XX^\top$ exist and positive definite. Let $\hat{\beta}_n$ be a LSE of $\beta$, i.e. $\hat{\beta}_n = (\sum_{i=1}^n X_iX_i^\top)^{-1}(\sum_{i=1}^n X_iY_i)$. Then, $\hat{\beta}_n\overset{a.s.}{\to}\beta$ as $n\to\infty$. 
\end{theorem}

\begin{proof}
Define  $W:=\mathbb{E}XX^\top, \mathbb{X}_n:=(X_1^\top, \dots, X_n^\top)^\top,\mathbb{Y}_n:= (Y_1, \dots, Y_n), \mathbb{W}_n:=\mathbb{X}_n^\top \mathbb{X}_n$.  Trivially from the strong law of large numbers, we have $\frac{1}{n}\mathbb{W}_n\overset{a.s.}{\to}W$. 

Rewrite 
$$
\hat{\beta}_n = (\sum_{i=1}^n X_iX_i^\top)^{-1}(\sum_{i=1}^n X_iY_i)=(\frac{1}{n}\mathbb{X}_n^\top\mathbb{X}_n)^{-1}(\sum_{i=1}^n X_iY_i)=:A_n B_n. 
$$
First term $A_n=(\frac{1}{n}\mathbb{W}_n)^{-1}\overset{a.s.}{\to} W^{-1}$. The second term 
$$
B_n=\sum_{i=1}^n X_i(Y_i-X_i^\top\beta +X_i^\top\beta ) = \sum_{i=1}^n X_i\varepsilon_i + \sum_{i=1}^n X_iX_i^\top\beta=:C_n+D_n.
$$
$C_n\overset{a.s.}{\to} 0_d$ from SLLN (we assumed finite moments of $X$ and zero mean of $\varepsilon$). Moreover, $D_n=\frac{1}{n}\mathbb{W}_n\beta \overset{a.s.}{\to} \mathbb{W}\beta$. 

Finally, $\hat{\beta}_n = A_n(C_n+D_n)$ and $A_n\overset{a.s.}{\to} \mathbb{W}^{-1}, C_n\overset{a.s.}{\to}0, D_n\overset{a.s.}{\to}\mathbb{W}\beta$. Therefore, $\hat{\beta}_n\overset{a.s.}{\to}\beta$, what we wanted to prove. 
\end{proof}




\begin{lemma}
Let $\textbf{X}=(X_1, X_2)$ be a non-binary random vector with independent components. Then, there does not exist a  function $f$  non-constant on the support of $X_1$ such that 
\begin{equation}
f(X_1)(X_1+X_2)\indep X_1.
\end{equation}
\end{lemma}
\begin{proof}
For a contradiction, let such $f$ exist. Define $\xi:=f(X_1)(X_1+X_2)$. Choose distinct $a, b, c$ in the support of $X_1$ such that $f(a), f(b), f(c)$ are not equal. WLOG $f(b)\neq 0$.

Since $\xi\indep X_1$, then $\xi\mid [X_1=a] \overset{D}{=}\xi\mid [X_1=b] \overset{D}{=}\xi\mid [X_1=c]$. Hence, 
\begin{equation}\label{asdf}
f(a)(a+X_2)\overset{D}{=}f(b)(b+X_2)\overset{D}{=}f(c)(c+X_2).
\end{equation}
By dividing by a nonzero constant $f(b)$ and minus $b$ we get
$$
(\frac{f(a)}{f(b)}a-b)+\frac{f(a)}{f(b)}X_2\overset{D}{=}X_2\overset{D}{=}(\frac{f(c)}{f(b)}c-b)+\frac{f(c)}{f(b)}X_2.
$$
Now we use lemma \ref{distributionalequalitylemma}. It holds that $\frac{f(a)}{f(b)}=\pm 1$ and also  $\frac{f(c)}{f(b)}=\pm 1$. Therefore, at least two values of $f(a), f(b), f(c)$ has to be equal and neither of them are zero. WLOG $f(a)= f(b)$. Plugging this into equation \ref{asdf}, we get $a=b$ which is a contradiction since we chose them distinct. 
\end{proof}




