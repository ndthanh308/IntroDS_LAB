\section{Proofs for Section 3}
\label{Section_proofs}
 
\begin{customlem}{\ref{LemmaAboutUnidentifiabilityFL}}
Let $(Y, \textbf{X})\in\mathbb{R}\times \mathbb{R}^p$ follow an $\mathcal{F}_L$-model with DAG $\mathcal{G}_0$ and $pa_Y(\mathcal{G}_0)\neq\emptyset$. Then, $|S_{\mathcal{F}_L}(Y)| \leq 1$ ($|S|$ represents the number of elements of the set $S$). Moreover, if  $a,b\in an_Y(\mathcal{G}_0)$ that are d-separated in $\mathcal{G}_0$ exist, then $S_{\mathcal{F}_L}(Y) = \emptyset$. 
\end{customlem}

\begin{proof}\label{Proof of LemmaAboutUnidentifiabilityFL}
First, we show $|S_{\mathcal{F}_L}(Y)| \leq 1$. Let $a\in an_Y(\mathcal{G}_0)\cap Source(\mathcal{G}_0)$. Such $a$ exists since $pa_Y(\mathcal{G}_0)\neq\emptyset$. We show that $S = \{a\}$ is an $\mathcal{F}_L-$plausible set. 

Denote $X_0:= Y$.  Since $\mathcal{G}_0$ is acyclic, it is possible to express recursively each variable $X_j , j= 0, \dots, p, $ as a weighted sum of the noise terms $\varepsilon_0, \dots, \varepsilon_p$ that belong to the ancestors of $X_j$. Let us write the Linear SCM with notation 
\begin{equation*}
X_i = \sum_{j\in pa_i}\beta_{j,i}X_j + \varepsilon_i  = \sum_{j\in an_i}\beta_{j\to i}\varepsilon_j,
\end{equation*}
where $\beta_{j,i}$ are non-zero constants and $\beta_{j\to i}$ is the sum of distinct weighted directed paths from node $j$ to node $i$, with a convention $\beta_{j\to j} := 1$.\footnote{To provide an example of the notation, if $X_1 = \varepsilon_1, X_2 = 2X_1 + \varepsilon_2, X_3 = 3X_1 + 4X_2+\varepsilon_3$, then $X_3 = 11\varepsilon_1 + 4\varepsilon_2 + 1\varepsilon_3 = \beta_{1\to 3}\varepsilon_1 +\beta_{2\to 3}\varepsilon_2 + \beta_{3\to 3}\varepsilon_1 $.}

Using this notation, note that 
$$X_0 = \sum_{j\in an_0}\beta_{j\to i}\varepsilon_j = \beta_{a\to 0}\varepsilon_a + \sum_{j\in an_0\setminus \{a\}}\beta_{j\to i}\varepsilon_j =\beta_{a\to 0} X_a + \sum_{j\in an_0\setminus \{a\}}\beta_{j\to i}\varepsilon_j ,$$ where $X_a\indep \sum_{j\in an_i\setminus \{a\}}\beta_{j\to i}\varepsilon_j$ since $a\in Source(\mathcal{G}_0)$. Hence, $Y - \beta_{a\to 0} X_a\indep X_a$, which is almost the definition of $\mathcal{F}_L$-plausibility of set $S = \{a\}$. More rigorously, for  $S = \{a\}$, we can find $f\in\mathcal{F}_L$ such that  $f_Y^{\leftarrow}({X}_{S}, Y)\indep X_S$ and $f_Y^{\leftarrow}({X}_{S}, Y)\sim U(0,1)$. This function can be defined as 
$$
f(x, \varepsilon) = \beta_{a\to 0}x + g^{-1}(\varepsilon), \,\,\,x\in\mathbb{R}, \varepsilon\in (0,1),
$$
where $g$ is the distribution function of $(Y - \beta_{a\to 0}X_S)$. This function obviously satisfies $f\in\mathcal{F}_L$. Moreover, since $f_Y^{\leftarrow}(X_{S}, Y) = g(Y - \beta_{a\to 0}X_S)$, it holds that $f_Y^{\leftarrow}({X}_{S}, Y)\indep X_S$ and $f_Y^{\leftarrow}({X}_{S}, Y)\sim U(0,1)$, which is what we wanted to show. Hence,  $|S_{\mathcal{F}_L}(Y)|\leq 1$, since $S_{\mathcal{F}_L}(Y)\subseteq S = \{a\}$. 

Now, let $a,b\in an_Y(\mathcal{G}_0)$ that are d-separated in $\mathcal{G}_0$. Let $a', b'\in \mathcal{G}_0$ such that $a'\in \big\{an_a(\mathcal{G}_0)\cup \{a\}\big \}\cap Source(\mathcal{G}_0)$,   $b'\in \big\{an_b(\mathcal{G}_0)\cup \{b\}\big \}\cap Source(\mathcal{G}_0)$. They are well defined since the sets $an_a(\mathcal{G}_0)\cup \{a\}$,  $an_b(\mathcal{G}_0)\cup \{b\}$  must contain some source node. Since $a,b$ are d-separated,   $ \big\{an_a(\mathcal{G}_0)\cup \{a\}\big \}$ and $\big\{an_b(\mathcal{G}_0)\cup \{b\}\big \}$ are disjoint sets, $a'\neq b'$ (they are even d-separated in $\mathcal{G}_0$ \citep{Pearl}). 

Using the same argument as in the first part of the proof, since $a'\in an_Y(\mathcal{G}_0)\cap Source(\mathcal{G}_0)$, it holds that  $S = \{a\}$ is an $\mathcal{F}_L-$plausible set.  $S = \{b\}$ is also an $\mathcal{F}_L-$plausible set since $b'\in an_Y(\mathcal{G}_0)\cap Source(\mathcal{G}_0)$. Together, $S_{\mathcal{F}_L}(Y)\subseteq \{a\}$ and $S_{\mathcal{F}_L}(Y)\subseteq \{b\}$. We showed that  $S_{\mathcal{F}_L}(Y) = \emptyset$. 
\end{proof}




\begin{customprop}{\ref{TheoremFidentifiabilityWithChild}}
Let $(X_0, \textbf{X})\in\mathbb{R}\times \mathbb{R}^p$ follow an SCM with DAG $\mathcal{G}_0$. Let $S\subseteq\{1, \dots, p\}$ and denote $S_0 = S\cup \{0\}$. Assume that  $\mathcal{G}=\mathcal{G}_0[S_0]$, the projection of $\mathcal{G}_0$ on $S$ defined in Section \ref{section_notation}, is a DAG. Let $S$ contain a childless child of $X_0$, i.e., $\exists j\in  ch_0(\mathcal{G})$, such that $ch_{j}(\mathcal{G})=\emptyset$. 

Let $\mathcal{F}\subseteq \mathcal{I}_{m}$ and let $(X_0, \textbf{X}_S)$ follow an  $\mathcal{F}$-model with graph $\mathcal{G}$, that is pairwise identifiable. Then, $S$ is not $\mathcal{F}$-plausible. 
\end{customprop}
 
 \begin{proof}
 \label{Proof of TheoremFidentifiabilityWithChild}
Recall the definition of pairwise identifiability from Appendix \ref{Appendix_pairwise_identifiability}.  For a contradiction, let $S$ be $\mathcal{F}$-plausible. The idea of the proof is that we define two bivariate $\mathcal{F}$-models, one with $X_0\to X_j$ and one with $X_j\to X_0$, which will lead to a contradiction with the pairwise identifiability. 

  Since $(X_0, \textbf{X}_S)$ follow an $\mathcal{F}$-model, we can write  $X_i = f_i(\textbf{X}_{pa_i(\mathcal{G})}, \varepsilon_i)$, where $f_i\in\mathcal{F}$ and $\varepsilon_i$ are jointly independent, $i\in S_0$. We use the  pairwise identifiability condition. For a specific choice $(X_0, X_j)$ (where $X_j$ is a child of $X_0$), and $\tilde{S} = nd_j(\mathcal{G})\setminus\{0,j\} = S\setminus\{j\}$ (the second equality holds since $j$ is a childless child),  $\textbf{x}_{\tilde{S}}: p_{\tilde{S}}(\textbf{x}_{\tilde{S}})>0$ exists, satisfying the condition that a bivariate $\mathcal{F}$-model defined as
\begin{equation}\label{tyuiop}
\tilde{X}_0=\tilde{\varepsilon}_0, \tilde{X}_j = \tilde{f}_j(\tilde{X}_0, \tilde{\varepsilon}_j)
\end{equation}
is identifiable, where  $P_{\tilde{\varepsilon}_0} = P_{X_0\mid \textbf{X}_{\tilde{S}} = x_{\tilde{S}}}    $ and $\tilde{f}_j(x, \varepsilon) =f(\textbf{x}_{pa_j\setminus\{0\}}, x, \varepsilon)$, $\tilde{\varepsilon}_j\indep \tilde{\varepsilon}_0$ . 

From the fact that $S$ is $\mathcal{F}$-plausible, $f\in\mathcal{F}$ exists, such that $\varepsilon_S:=f^\leftarrow(\textbf{X}_S, X_0)$ satisfies $\varepsilon_S\indep \textbf{X}_S, \varepsilon_S\sim U(0,1)$. Hence, we can define a model  $$\tilde{\tilde{X}}_j = \tilde{\tilde{\varepsilon}}_j , \tilde{\tilde{X}}_0 = \tilde{\tilde{f}}(\tilde{\tilde{X}}_j, \varepsilon_S),$$ where $P_{\tilde{\tilde{\varepsilon}}_j} = P_{X_j\mid \textbf{X}_{\tilde{S}} = x_{\tilde{S}}}    $ and $\tilde{\tilde{f}}(\textbf{x}, \varepsilon) =f(\textbf{x}_{\tilde{S}}, x, \varepsilon)$. In this model, $\varepsilon_S\indep \tilde{\tilde{\varepsilon}}_j$. 

Now, note that $(\tilde{X}_0,\tilde{X}_j)\overset{D}{=}(\tilde{\tilde{X}}_0, \tilde{\tilde{X}}_j)$, since both sides are distributed as $\big[(X_0, X_j)\mid X_{\tilde{S}}\big]$.  This is a contradiction with the identifiability of (\ref{tyuiop}). Therefore,  $S$ is not $\mathcal{F}_F$-plausible.
 \end{proof}
 
 
 
 
\begin{customprop}{\ref{Support_proposition}}
Let $(Y, \textbf{X})\in\mathbb{R}\times \mathbb{R}^p$ follow an SCM with DAG $\mathcal{G}_0$. Let $S\subseteq\{1, \dots, p\}$ be a non-empty set. Let  $\underline{\Psi},\overline{\Psi}: \mathbb{R}^{\mid S\mid}\to \mathbb{R}$ be real functions such that
\begin{equation*}
supp(Y\mid \textbf{X}_S=\textbf{x}) = (\underline{\Psi} (\textbf{x}),\overline{\Psi}(\textbf{x})), \,\,\,\,\,\,\, \forall \textbf{x}\in supp(\textbf{X}_S).
\end{equation*}
Moreover, let
\begin{equation} \tag{\ref{eq9987}}
\frac{Y - \underline{\Psi}(\textbf{X}_S)}{\overline{\Psi}(\textbf{X}_S) - \underline{\Psi}(\textbf{X}_S)}\not\indep \textbf{X}_S.
\end{equation}
Then, $S$ is not $\mathcal{F}_{LS}$-plausible. 
\end{customprop}

\begin{proof}\label{Proof of Support_proposition}
For a contradiction, let $S$ be  $\mathcal{F}_{LS}$-plausible. Hence, $f\in\mathcal{F}_{LS}$ exists such that 
\begin{equation}\label{eq74}
f^{\leftarrow}(\textbf{X}_S, Y)\indep\textbf{ X}_S.
\end{equation}
Since $f\in\mathcal{F}_{LS}$, we can write $f^{\leftarrow}(\textbf{x},y) = q\big(\frac{y - \mu(\textbf{x})}{\sigma(\textbf{x})}\big)$  for some functions $\mu(\cdot), \sigma(\cdot)>0$ and for some (continuous) distribution function $q(\cdot)$. Using this notation, (\ref{eq74}) is equivalent to
\begin{equation} \label{eq989}
\frac{Y - {\mu}(\textbf{X}_S)}{{\sigma}(\textbf{X}_S)}\indep \textbf{X}_S.
\end{equation}
Denote $W_{\textbf{x}}:= (Y\mid \textbf{X}_S=\textbf{x})$. From (\ref{eq989}), we get that for all $\textbf{x},\textbf{y}$ in the support of $\textbf{X}_S$, it must hold that
\begin{equation}\label{eq7285}
\frac{W_\textbf{x} - {\mu}(\textbf{x})}{{\sigma}(\textbf{x})}\overset{D}{=}\frac{W_\textbf{y} - {\mu}(\textbf{y})}{{\sigma}(\textbf{y})}.
\end{equation}
Hence, supports must also match, i.e., (\ref{eq7285}) implies
\begin{align*}
\frac{ \underline{\Psi} (\textbf{x}) - {\mu}(\textbf{x})}{{\sigma}(\textbf{x})}&\overset{}{=}\frac{ \underline{\Psi} (\textbf{y}) - {\mu}(\textbf{y})}{{\sigma}(\textbf{y})}, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
\frac{ \overline{\Psi}(\textbf{x}) - {\mu}(\textbf{x})}{{\sigma}(\textbf{x})}\overset{}{=}\frac{ \overline{\Psi}(\textbf{y}) - {\mu}(\textbf{y})}{{\sigma}(\textbf{y})},
\end{align*}
for all $\textbf{x},\textbf{y}$ in the support of $\textbf{X}_S$. Solving for $\mu, \sigma$  gives us 
\begin{align*}
{\mu}(\textbf{x})&\overset{}{=}c_1+ \underline{\Psi} (\textbf{x}) , \,\,\,\,\,\,\,\,\,\,\,
\sigma(\textbf{x})\overset{}{=}c_2\cdot [\overline{\Psi} (\textbf{x})-\underline{\Psi}(\textbf{x})],
\end{align*}
where $c_1 \in  \mathbb{R}, c_2 \in \mathbb{R}_{+}$ are some constants. Plugging this into (\ref{eq989}) gives us a contradiction with (\ref{eq9987}). 
\end{proof}



\begin{customprop}{\ref{LemmaOParetoinseparabilite}}
Let $F$ be a distribution function whose parameter acts post-multiplicatively. Let  $\textbf{X}=(X_1, \dots, X_k)$ be a continuous random vector with independent components.  
\begin{itemize}
\item Consider $f\in\mathcal{F}_F$ in the form $f(\textbf{x}, \varepsilon)=F^{-1}(\varepsilon, \theta(\textbf{x}))$ with an additive function $\theta(x_1, \dots, x_k) = h_1(x_1)+\dots + h_k(x_k)$, where $h_i$ are continuous non-constant real functions. Then, $f$ is $\mathcal{F}_F-$inseparable wrt $\textbf{X}$. 
\item Consider $f\in\mathcal{F}_F$ in the form $f(\textbf{x}, \varepsilon)=F^{-1}(\varepsilon, \theta(\textbf{x}))$ with a multiplicative function   $\theta(x_1, \dots, x_k) = h_1(\textbf{x}_S)\cdot h_2(\textbf{x}_{\{1, \dots, k\}\setminus S})$ for some $S\subsetneq \{1, \dots, k\}$, where $h_i$ are continuous non-constant non-zero real functions. Then, $f$ is not $\mathcal{F}_F-$inseparable wrt $\textbf{X}$. 
\end{itemize}
\end{customprop}
\begin{proof}\label{Proof of LemmaOParetoinseparabilite}
\textbf{The first bullet-point}: For a contradiction, consider that $S\subset \{1, \dots, k\}$, $z\in(0,1)$, $g\in\mathcal{F}_F$ exist,such that  $g^{\leftarrow}\big(\textbf{X}_S, f(\textbf{X}, z)\big)\indep \textbf{X}_S$. Since $g\in\mathcal{F}_F$, we can write $g^{\leftarrow}(\textbf{x}_S, \cdot) = F\big(\cdot, \theta_g(\textbf{x}_S)\big)$ for some non-constant function $\theta_g$. Hence, 
$$\textbf{X}_S\indep g^{\leftarrow}\big(\textbf{X}_S, f(\textbf{X}, z)\big)
= F[F^{-1}\big(z, \theta(\textbf{X})\big), \theta_g(\textbf{X}_S)]
= f_1[z, f_2\big(\theta_g(\textbf{X}_S)\big) \cdot\theta(\textbf{X})].$$ 
We use identity (\ref{trivial_identity}). Since $f_1$ is invertible, we obtain 
\begin{equation}\label{dfrge}
\textbf{X}_S\indep f_2\big(\theta_g(\textbf{X}_S)\big) \cdot\theta(\textbf{X}).
\end{equation}
Define $\tilde{\theta}_g(\textbf{X}_S):=f_2\big(\theta_g(\textbf{X}_S)\big)$. Finally, since $\theta(\textbf{X})$ is an additive function from the assumptions, (\ref{dfrge}) is equivalent to
$$
\tilde{\theta}_g(\textbf{X}_S)[h_1(X_1) + \dots + h_k(X_k)]\indep \textbf{X}_S. 
$$
However, that is a contradiction with Lemma \ref{CoolLemma} part 1. 

\textbf{The second bullet-point}:  We find an appropriate function $g\in\mathcal{F}_F$ such that for any $z\in (0,1)$, it holds that   $g^{\leftarrow}\big(\textbf{X}_S, f(\textbf{X}, z)\big)\indep \textbf{X}_S$. Since  $g\in\mathcal{F}_F$, we write  $g^{\leftarrow}(\textbf{x}_S, \cdot) = F\big(\cdot, \theta_g(\textbf{x}_S)\big)$ for some $\theta_g$. 

Rewrite 
 $$g^{\leftarrow}\big(\textbf{X}_S, f(\textbf{X}, z)\big) = F[F^{-1}\big(z, \theta(\textbf{X})\big)     ,\theta_g(\textbf{X}_S)]
 = f_1[z, f_2\big(\theta_g(\textbf{X}_S)\big) \cdot\theta(\textbf{X})],$$
where $f_1, f_2$ are from (\ref{postMultiplDefinition}). 
We choose $\theta_g$ such that $f_2\big(\theta_g(\textbf{x}_S)\big) = \frac{1}{h_1(\textbf{x}_S)}$. Obviously, $g\in\mathcal{F}_F$. Then, by extending $\theta$ to its multiplicative form, we get 
$$ f_1[z, f_2\big(\theta_g(\textbf{X}_S)\big) \cdot\theta(\textbf{X})] =  f_1[z,  h_2(\textbf{X}_{\{1, \dots, k\}\setminus S})]\indep \textbf{X}_S.$$
Together, we found $g\in\mathcal{F}_F$ defined by $g^{\leftarrow}(\textbf{x}_S, \cdot) = F\big(\cdot, f_2^{-1}(\frac{1}{h_1(\textbf{x}_S)})\big)$ that satisfy   $g^{\leftarrow}\big(\textbf{X}_S, f(\textbf{X}, z)\big)\indep \textbf{X}_S$. Hence,  $f$ is not $\mathcal{F}_F-$inseparable wrt $\textbf{X}$. 
\end{proof}

\begin{customprop}{\ref{LemmaOAdditiveinseparabilite}}
Let $F$ be a distribution function whose parameter acts post-additively. Let  $\textbf{X}=(X_1, \dots, X_k)$ be a continuous random vector with independent components.  
\begin{itemize}
\item Consider $f\in\mathcal{F}_F$ in the form $f(\textbf{x}, \varepsilon)=F^{-1}(\varepsilon, \theta(\textbf{x}))$ with an additive function $\theta(x_1, \dots, x_k) = h_1(\textbf{x}_S) + h_2(\textbf{x}_{\{1, \dots, k\}\setminus S})$ for some non-empty $S\subset \{1, \dots, k\}$, where $h_i$ are continuous non-constant non-zero real functions. Then, $f$ is not $\mathcal{F}_F-$inseparable wrt $\textbf{X}$. 
\item Consider $f\in\mathcal{F}_F$ in the form $f(\textbf{x}, \varepsilon)=F^{-1}\big(\varepsilon, \theta(\textbf{x})\big)$ with a multiplicative function   $\theta(x_1, \dots, x_k) = h_1(x_1)\cdot h_2(x_2)\dots h_k(x_k)$ where $h_i$ are continuous non-constant non-zero real functions. Then, $f$ is $\mathcal{F}_F-$inseparable wrt $\textbf{X}$. 
\end{itemize}
\end{customprop}

\begin{proof}\label{Proof of LemmaOAdditiveinseparabilite}
\textbf{The first bullet-point}:  We find an appropriate function $g\in\mathcal{F}_F$ such that for any $z\in (0,1)$, it holds that   $g^{\leftarrow}\big(\textbf{X}_S, f(\textbf{X}, z)\big)\indep \textbf{X}_S$. Since  $g\in\mathcal{F}_F$, we write  $g^{\leftarrow}(\textbf{x}_S, \cdot) = F\big(\cdot, \theta_g(\textbf{x}_S)\big)$ for some $\theta_g$. 

Rewrite 
 $$g^{\leftarrow}\big(\textbf{X}_S, f(\textbf{X}, z)\big) = F\bigg( F^{-1}[z, \theta(\textbf{X})]     ,\theta_g(\textbf{X}_S)\bigg)
 = f_1[z, f_2\big(\theta_g(\textbf{X}_S)\big) +\theta(\textbf{X})],$$
where $f_1, f_2$ are from (\ref{postAdditiveDefinition}). 
We choose $\theta_g$ such that $f_2(\theta_g(\textbf{x}_S)) = -h_1(\textbf{x}_S)$. Obviously, $g\in\mathcal{F}_F$. Then, by extending $\theta$ to its additive form, we get 
$$ f_1[z, f_2\big(\theta_g(\textbf{X}_S)\big) +\theta(\textbf{X})] =  f_1[z,  h_2(\textbf{X}_{\{1, \dots, k\}\setminus S})]\indep \textbf{X}_S.$$
Together, we found $g\in\mathcal{F}_F$ defined by $g^{\leftarrow}(\textbf{x}_S, \cdot) = F\big(\cdot, f_2^{-1}(\frac{1}{h_1(\textbf{x}_S)})\big)$ that satisfy   $g^{\leftarrow}\big(\textbf{X}_S, f(\textbf{X}, z)\big)\indep \textbf{X}_S$. Hence,  $f$ is not $\mathcal{F}_F-$inseparable wrt $\textbf{X}$. 

\textbf{The second bullet-point}: 
For a contradiction, consider that  $S\subset \{1, \dots, k\}$, $z\in(0,1)$, $g\in\mathcal{F}_F$ exist, such that  $g^{\leftarrow}\big(\textbf{X}_S, f(\textbf{X}, z)\big)\indep \textbf{X}_S$. Since $g\in\mathcal{F}_F$, we can write $g^{\leftarrow}(\textbf{x}_S, \cdot) = F\big(\cdot, \theta_g(\textbf{x}_S)\big)$ for some non-constant function $\theta_g$. Hence,   
$$\textbf{X}_S\indep g^{\leftarrow}\big(\textbf{X}_S, f(\textbf{X}, z)\big)
= F[ F^{-1}\big(z, \theta(\textbf{X})\big), \theta_g(\textbf{X}_S)]
= f_1[z, f_2\big(\theta_g(\textbf{X}_S)\big) +\theta(\textbf{X})].$$ 
We use identity (\ref{trivial_identity}). Since $f_1$ is invertible, we obtain 
\begin{equation}\label{dfrgeDVA}
\textbf{X}_S\indep f_2\big(\theta_g(\textbf{X}_S)\big)  + \theta(\textbf{X}).
\end{equation}
Define $\tilde{\theta}_g(\textbf{X}_S):=f_2\big(\theta_g(\textbf{X}_S)\big)$. Finally, since $\theta(\textbf{X})$ is an multiplicative function from the assumptions, (\ref{dfrgeDVA}) is equivalent to
\begin{equation*}
\tilde{\theta}_g(\textbf{X}_S) + \big(h_1(X_1)\dots h_k(X_k)\big)\indep \textbf{X}_S. 
\end{equation*}
However, that is a contradiction with Lemma \ref{CoolLemma} part 2. 
\end{proof}





\begin{customprop}{\ref{LemmaOLocationScaleinseparabilite}}
Let $F$ have a location-scale type with $q=2$ parameters. Let $\textbf{X}=(X_1, \dots, X_k)$ be a continuous random vector with independent components. Consider $f\in\mathcal{F}_F$ in the form $f(\textbf{x}, \varepsilon)=F^{-1}\big(\varepsilon, \theta(\textbf{x})\big)$, where $\theta(\textbf{x}) = \big(\mu(\textbf{x}), \sigma(\textbf{x})\big)^\top$ is additive in both components, i.e., 
$\mu(\textbf{x}) = h_{1, \mu}(x_1)+\dots + h_{k, \mu}(x_k)$ and 
$\sigma(\textbf{x}) =h_{1, \sigma}(x_1)+\dots + h_{k, \sigma}(x_k)$ for some continuous non-constant non-zero functions $h_{i,\cdot}$, where we also assume $h_{i,\sigma}>0$, $i=1, \dots, k$.  Then, $f$ is $\mathcal{F}_F-$inseparable wrt $\textbf{X}$. 
\end{customprop}
\begin{proof}
\label{Proof of LemmaOLocationScaleinseparabilite}
For a contradiction, consider that $S\subsetneq \{1, \dots, k\}$, $z\in(0,1)$, $g\in\mathcal{F}_F$ exist, such that  $g^{\leftarrow}\big(\textbf{X}_S, f(\textbf{X}, z)\big)\indep \textbf{X}_S$. Since $g\in\mathcal{F}_F$, we can write $g^{\leftarrow}(\textbf{x}_S, \cdot) = F\big(\cdot, \theta_g(\textbf{x}_S)\big)$ for some function $\theta_g=(\mu_g, \sigma_g)$ that is non-constant in neither of the arguments. Hence, simply rewriting   
$$\textbf{X}_S\indep g^{\leftarrow}\big(\textbf{X}_S, f(\textbf{X}, z)\big)
= F[F^{-1}\big(z, \theta(\textbf{X})\big), \theta_g(\textbf{X}_S)]
= F_{\theta_0} \bigg(\frac{[\mu(\textbf{X}) + \sigma(\textbf{X})F_{\theta_0}^{-1}(z)] - \mu_g(\textbf{X}_S)}{\sigma_g(\textbf{X}_S)} \bigg),$$ 
where $F_{\theta_0}$ is a distribution function of a standardized random variable (e.g., if $F$ is Gaussian, then $F_{\theta_0}(x) = \int_{-\infty}^x\frac{1}{\sqrt{2\pi}}e^{\frac{-y^2}{2}}dy$). 
We use identity (\ref{trivial_identity}) to obtain 
$$
\textbf{X}_S\indep \frac{[\mu(\textbf{X}) + \sigma(\textbf{X})F_{\theta_0}^{-1}(z)] - \mu_g(\textbf{X}_S)}{\sigma_g(\textbf{X}_S)} 
$$
\begin{equation}\label{dfrgeTRI}
\textbf{X}_S\indep \frac{[\mu(\textbf{X}) + \sigma(\textbf{X})F_{\theta_0}^{-1}(z)] - \mu_g(\textbf{X}_S)}{\sigma_g(\textbf{X}_S)}. 
\end{equation}
Equation (\ref{dfrgeTRI}) can be equivalently rewritten into
\begin{equation}\label{erty}
\textbf{X}_S\indep f_1(\textbf{X}_S) + f_2(\textbf{X}_S)h(\textbf{X}_{S^c}),
\end{equation}
where $ S^c=\{1, \dots, k\}\setminus S$, $S = (S[1], \dots, S[s])$,

$$f_1(\textbf{x}) =\frac{ h_{S[1], \mu}(x_1)+\dots + h_{S[s], \mu}(x_s) + [h_{S[1], \sigma}(x_1)+\dots + h_{S[s], \sigma}(x_s)]F_{\theta_0}^{-1}(z) - \mu_g(\textbf{x})}{\sigma_g(\textbf{x})},$$ $$f_2(\textbf{x}) =  \frac{1}{\sigma_g(\textbf{x})},$$
$$
h(\textbf{x}) = [h_{S^c[1], \sigma}(x_1)+\dots + h_{S^c[k-s], \sigma}(x_s)]F_{\theta_0}^{-1}(z).
$$
However, independence (\ref{erty}) is a contradiction with Lemma \ref{CoolLemma} part 3. 
\end{proof}


\begin{customconsequence}{\ref{PropositionOAdditiveParents}}
 Consider $f_Y\in\mathcal{F}_A$ and let $(Y, \textbf{X})\in\mathbb{R}\times \mathbb{R}^p$ follow a SCM with DAG $\mathcal{G}_0$ where $pa_Y$ are $d$-separated.  
 \begin{itemize}
     \item If $f_Y$ has a form $$f_Y(\textbf{x}, e) = h_1(\textbf{x}_S) + h_2(\textbf{x}_{pa_Y\setminus S}) + q^{-1}(e), \,\,\,\,\,\textbf{x}\in\mathbb{R}^{|pa_Y|}, e\in(0,1),$$ for some non-empty $S\subset pa_Y$, where $h_1, h_2$ are continuous non-constant real functions and $q^{-1}$ is a quantile function. Then $S_{\mathcal{F}_A}(Y) = \emptyset$.
     \item If $f_Y$ has a form $$f_Y(\textbf{x}, e) = h_1(x_1) \dots h_{|pa_Y|}(x_{|pa_Y|}) + q^{-1}(e),\,\,\,\,\,\textbf{x}\in\mathbb{R}^{|pa_Y|}, e\in(0,1),$$ where $h_i$ are continuous non-constant non-zero real functions and  $q^{-1}$ is a quantile function. Then, every $S\subset pa_Y$ is not $\mathcal{F}_A$-plausible.
 \end{itemize}
\end{customconsequence}
\begin{proof}
\label{Proof of PropositionOAdditiveParents}
\textbf{The first bullet-point}:  We show that sets $S$ and $S^c:=pa_Y\setminus S$ are both $\mathcal{F}_A$-plausible. Consider a function $f\in\mathcal{F}_A$ satisfying $f^{\leftarrow}(\textbf{X}_S, Y):= \tilde{q}\big(Y - h_1(\textbf{X}_S)\big)$, where $\tilde{q}$ is a distribution function of $[h_2(\textbf{X}_{pa_Y\setminus S}) + q^{-1}(\varepsilon_Y)]$. Then, $f^{\leftarrow}(\textbf{X}_S, Y)\indep \textbf{X}_S$ since $Y - h_1(\textbf{X}_S) = h_2(\textbf{X}_{pa_Y\setminus S}) + q^{-1}(\varepsilon_Y)\indep \textbf{X}_S$ and we apply identity (\ref{trivial_identity}). Moreover, $f^{\leftarrow}(\textbf{X}_S, Y)\sim U(0,1)$ trivially. We showed that set $S$ satisfies every property for being  $\mathcal{F}_A$-plausible. Set $S^c$ can be analogously shown to be  $\mathcal{F}_A$-plausible as well. Therefore,  $S_{\mathcal{F}_A}(Y)\subseteq S\cap S^c = \emptyset$.

\textbf{The second bullet-point}: For a contradiction, consider that an  $\mathcal{F}_A$-plausible non-empty set $S\subset pa_Y$ exists. That means, $f\in\mathcal{F}_A$ exists such that (\ref{Definition_F_plausible}) holds. Since $f\in\mathcal{F}_A$, we can write $f(\textbf{x}, e) = \mu(\textbf{x}) + \tilde{q}^{-1}(e), \,\,\textbf{x}\in\mathbb{R}^{|S|}, e\in (0,1)$ for some function $\mu$ and quantile function $\tilde{q}^{-1}$. Also, we can write $f^{\leftarrow}(\textbf{x}, y) = \tilde{q}\big(y-\mu(\textbf{x})\big),\,\,\textbf{x}\in\mathbb{R}^{|S|}, y\in\mathbb{R}$ (see discussion in Appendix~\ref{Appendix_A.1.}).   Using (\ref{Definition_F_plausible}) and identity \ref{trivial_identity}, we have $Y- \mu(\textbf{X}_S)\indep \textbf{X}_S$. 

From the definition of $f_Y$, we have $Y = h_1(X_1) \dots h_{|pa_Y|}(X_{|pa_Y|}) + q^{-1}(\varepsilon_Y)$, where $\varepsilon_Y\indep \textbf{X}_{pa_Y}$. Hence, we have 
\begin{align*}
    Y- \mu(\textbf{X}_S)&\indep \textbf{X}_S 
    \\ h_1(X_1) \dots h_{|pa_Y|}(X_{|pa_Y|}) + q^{-1}(\varepsilon_Y) - \mu(\textbf{X}_S) &\indep \textbf{X}_S 
    \\h_1(X_1) \dots h_{|pa_Y|}(X_{|pa_Y|}) - \mu(\textbf{X}_S) &\indep \textbf{X}_S. 
\end{align*}
This is a contradiction with Lemma \ref{CoolLemma} part 2.    
\end{proof}

\begin{customprop}{\ref{Proposition_consistency}}
Consider $\mathcal{F} = \mathcal{F}_A$ and 
let $(Y, \textbf{X})\in\mathbb{R}\times \mathbb{R}^p$ follow an SCM with DAG $\mathcal{G}_0$ satisfying (\ref{SCM_for_Y}). Assume that every $S \neq pa_Y$ is not $\mathcal{F}$-plausible. 
Then,   

\begin{equation}
  \lim_{n\to\infty} \mathbb{P}(\widehat{pa}_Y  \neq pa_Y) = 0,
\end{equation}   
where $n$ is the size of the random sample and $\widehat{pa}_Y$ is our score-based estimate from Section~\ref{Section_algorithm2} with $\lambda_1, \lambda_2>0, \lambda_3 = 0$, suitable estimation procedure, and HSIC independence measure. 
\end{customprop}
\begin{proof}
\label{Proof of Proposition_consistency}
This result is a trivial consequence of Theorem 20 in \cite{reviewANMMooij}. We use the same notation. For a rigorous definition of $HSIC$ and $\widehat{HSIC}$, see Appendix A.1 in \cite{reviewANMMooij}. 



We show that $score(S)> score(pa_Y)$ as $n\to\infty$ for any $S\neq pa_Y$. 
The $score(S)$ is defined as the weighted sum of  \textit{Independence} and \textit{Significance} terms. Let us first concentrate on the former. By definition, we write $\textit{Independence} = -\widehat{HSIC}(\textbf{X}_S, \hat{\varepsilon}_S)$. On a population level, it holds (Lemma~12 in \cite{reviewANMMooij}) that  ${HSIC}(\textbf{X}_S, {\varepsilon}_S) > 0 $ and ${HSIC}(\textbf{X}_{pa_Y}, {\varepsilon}_{pa_Y}) = 0$, since $\textbf{X}_S$ and ${\varepsilon}_S$ are not independent (because $S$ is not $\mathcal{F}$-plausible) and  $\textbf{X}_{pa_Y}$ and ${\varepsilon}_{pa_Y}$ are independent (by definition of the SCM). 
By Theorem~20 in \cite{reviewANMMooij}, we obtain $\widehat{HSIC}(\textbf{X}_{pa_Y}, \hat{\varepsilon}_{pa_Y})\to {HSIC}(\textbf{X}_{pa_Y}, {\varepsilon}_{pa_Y})=0$ and $\widehat{HSIC}(\textbf{X}_{S}, \hat{\varepsilon}_{S})\to {HSIC}(\textbf{X}_{S}, {\varepsilon}_{S})>0$, as $n\to\infty$. Therefore, the independence term is strictly smaller (for some large $n$) for $S$ than for $pa_Y$. 

Let us focus on \textit{Significance} term\footnote{We work with the \textit{Significance} term somewhat vaguely in this proof. However, we only need the property that $Significance\to 0$ as $n\to\infty$ for $pa_Y$, which is satisfied for any reasonable method of assessing significance of covariates.}. Since all $\textbf{X}_{pa_Y}$ are significant (otherwise $f_Y\notin\mathcal{I}_m$) we get that $Significance\to 0$ as $n\to\infty$ for $pa_Y$. Moreover, by definition, $Significance\geq 0$ for $S$.

Together, we find that $score(pa_Y)>score(S)$ for large $n$, since 
the \textit{Independence} term is strictly smaller (for large $n$) for $S$ than for $pa_Y$ and \textit{Significance} term converges to $0$ for $pa_Y$ and is non-negative.  
We showed that $pa_Y$ has the largest score among all $S\subseteq \{1, \dots, p\}$ (for $n$ large enough). 
\end{proof}



\begin{lemma}\label{lemmaISDconsistency}
Consider $\mathcal{F} = \mathcal{F}_A$ and let $(Y, \textbf{X})\in\mathbb{R}\times \mathbb{R}^p$ follow an SCM with DAG $\mathcal{G}_0$ satisfying (\ref{SCM_for_Y}), with $S_{\mathcal{F}_A}(Y)\neq\emptyset$. 
Then, the ISD algorithm used with a consistent estimation (\ref{equation_SID}) and an independence-oracle is guaranteed to estimate $\hat{S}_{\mathcal{F}_A}(Y) = S_{\mathcal{F}_A}(Y)$. 
\end{lemma}
\begin{proof}
  A similar statement can be found with regard to the RESIT algorithm \citep{Peters2014}. However here, the proof is trivial.  For every $\mathcal{F}_A$-plausible set $S$, we have ${\varepsilon}_S \indep \textbf{X}_S$. Since our estimation is consistent, we also have (in the limit)
 $\hat{\varepsilon}_S \indep \textbf{X}_S$. Therefore, our independence oracle will output YES on question 1. Trivially, all $\textbf{X}_S$ are significant since  $S$ is an $\mathcal{F}_A$-plausible set, and $\hat{\varepsilon}_S \sim U(0,1)$ is trivially satisfied since for $\mathcal{F} = \mathcal{F}_A$, the third question is redundant. Therefore, the ISD algorithm will mark the set $S$ as  $\mathcal{F}_A$-plausible, and the ISD estimation is correct.  
 \end{proof}

