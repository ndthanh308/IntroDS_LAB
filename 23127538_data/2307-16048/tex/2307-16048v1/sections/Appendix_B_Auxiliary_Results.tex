\section{Auxiliary results}\label{Appendix_Auxiliary}
The median $med(X)$ is defined as $m\in\mathbb{R}$ such that   $P(X\leq m )\geq \frac{1}{2}\leq P(X\geq m )$. It always exists, and is unique for continuous random variables.
\begin{lemma}\label{distributionalequalitylemma}
Let $X$ be a non-degenerate continuous real random variable. Let $a,b\in\mathbb{R}$ such that 
\begin{equation}\label{qwerty}
a+bX\overset{D}{=}X.
\end{equation}
Then, either $(a,b) = (0,1)$ or $(a,b) = (2med(X), -1)$. 
\end{lemma}
\begin{proof}
\textit{Idea of the proof assuming a finite variance of $X$: } If $X$ has finite variance, then (\ref{qwerty}) implies $var(a+bX) = var(X)$, rewriting gives us  $b^2 var(X)=var(X)$, and hence, $b=\pm 1$. Now, (\ref{qwerty}) also implies $\mathbb{E}(a+bX) = \mathbb{E}(X)$, hence $a=(1-b)\mathbb{E}(X)$. Therefore, if $b=1$, then $a= 0$, and if $b=-1$, then $a=2\mathbb{E}(X)$. 

\textit{Proof without the moment assumption:}  (\ref{qwerty}) implies that for any $q\in (0.5,1)$, the difference between the $q$ quantile and $(1-q)$ quantile should be the same on both sides of (\ref{qwerty}). Denote $F^{-1}_X(q)$ a $q-$quantile of $X$ and assume that $F^{-1}_X(q)\neq F^{-1}_X(1-q)$ (since $X$ is non-degenerate, such $q$ exist). We get 
$$
F^{-1}_{a+bX}(q) - F^{-1}_{a+bX}(1-q) = F^{-1}_X(q)- F^{-1}_X(1-q)=:D.
$$
Consider $b\geq 0$. Using linearity of the quantile function, we obtain $a+bF^{-1}_X(q) - \big(a+bF^{-1}_X(1-q)\big) = D$ and hence, $bD=D$, which gives us $b=1$. If $b<0$, then an identity $F^{-1}_{a+bX}(q) = a+\big(1-F^{-1}_{-bX}(1-q)\big) = a+\big(1+bF^{-1}_{X}(1-q)\big)$ hold. Hence, we get  $a+[1+bF^{-1}_X(1-q)] - [a+\big(1+bF^{-1}_X(1-q)\big)] = D$. Rewriting the left side, we get $-bD=D$,  which gives us $b=-1$. 

In the case when $b=1$, trivially $a=0$, since otherwise, $med(a+X)\neq med(X)$. If $b=-1$, then applying median on both sides of (\ref{qwerty}) gives us $med(a-X)= med(X)$ and hence, $a=2med(X)$, as we wanted to show. 
\end{proof}


\begin{lemma}\label{CoolLemma}
Let $\textbf{X}=(X_1, \dots, X_k)$ be a continuous random vector with independent components and $s<k$.  Let $h_1, \dots, h_k$ be continuous non-constant real functions. 
\begin{enumerate}
\item A non-zero function $f$ does not exist such that 
\begin{equation}\label{yui}
  f(X_1, \dots, X_s)\big(h_1(X_1)+\dots+h_k(X_k)\big)\indep (X_1, \dots, X_s).
\end{equation}
\item Moreover, let $h_1, \dots, h_k$ be non-zero. Then, a non-zero function $f$ does not exist such that 
\begin{equation}\label{yuiDVA}
  f(X_1, \dots, X_s) + h_1(X_1)h_2(X_2)\dots h_k(X_k)\indep (X_1, \dots, X_s).
\end{equation}
\item Let $h:\mathbb{R}^{k-s}\to\mathbb{R}$ be measurable function such that $h(X_{s+1}, \dots, X_k)$ is non-degenerate continuous random variable. Functions $f_1, f_2$ does not exist, such that $f_2$ is positive non-constant and
\begin{equation}\label{yuiTRI}
f_1(X_1, \dots, X_s) + f_2(X_1, \dots, X_s)h(X_{s+1}, \dots, X_k)\indep (X_1, \dots, X_s).
\end{equation}
\item Let $\textbf{X}=(X_1, \dots, X_k)$ be non-degenerate Gaussian random vector (possibly with dependent components) and $\beta = (\beta_{s+1}, \dots, \beta_{k})^\top\in \mathbb{R}^{k-s}$ . Functions $f_1, f_2$ does not exist, such that $f_2$ is positive non-constant and
\begin{equation}\label{yuiSTYRI}
f_1(X_1, \dots, X_s) + f_2(X_1, \dots, X_s)(\beta_{s+1}X_{s+1}+ \dots+ \beta_{k}X_k)\indep (X_1, \dots, X_s).
\end{equation}

\end{enumerate}
\end{lemma}
\begin{proof}
Let us introduce functionals (not norms, we only use them to simplify notation) $||\cdot||_{plus}$ and  $||\cdot||_{times}$ , defined by $|| \textbf{a} ||_{plus} = a_1 +\dots + a_d$ , $|| \textbf{a} ||_{times} = a_1 a_2\dots a_d$ , for $\textbf{a}=(a_1, \dots, a_d)^\top\in\mathbb{R}^d$. 
We use notation $\textbf{X}_S=(X_1, \dots, X_s)^\top$ and denote a function $h_S: \mathbb{R}^s\to\mathbb{R}^s: h_S(\textbf{x})= (h_1(x_1), \dots, h_s(x_s))^\top$. 

\textbf{Part 1: }For a contradiction, let such $f$ exist. First, some notation: Let $Y=h_{s+1}(X_{s+1})+\dots+h_{k}(X_k)$ and define $\xi:=f(\textbf{X}_S)(||h_S(\textbf{X}_S)||_{plus}+Y)$, which is the left hand side of (\ref{yui}). 

Choose $\textbf{a}, \textbf{b}, \textbf{c}\in\mathbb{R}^s$ in the support of $\textbf{X}_S$ such that $||h_S(\textbf{a})||_{plus}, ||h_S(\textbf{b})||_{plus}, ||h_S(\textbf{c})||_{plus}$ are distinct and $f(\textbf{b})\neq 0$ (it is possible since $h_i$ are non-constant). 

Since $\xi\indep \textbf{X}_S$, then  $\xi\mid [\textbf{X}_S=\textbf{a}] \overset{D}{=}\xi\mid [\textbf{X}_S=\textbf{b}] \overset{D}{=}\xi\mid [\textbf{X}_S=\textbf{c}]$. Hence, 
\begin{equation}\label{asdfgh}
f(\textbf{a})(||h_S(\textbf{a})||_{plus}+Y)\overset{D}{=}f(\textbf{b})(||h_S(\textbf{b})||_{plus}+Y)\overset{D}{=}f(\textbf{c})(||h_S(\textbf{c})||_{plus}+Y).
\end{equation}
By dividing by a non-zero constant $f(\textbf{b})$ and subtracting a constant $||h_S(\textbf{b})||_{plus}$, we get
$$
\frac{f(\textbf{a})}{f(\textbf{b})}||h_S(\textbf{a})||_{plus}-||h_S(\textbf{b})||_{plus}+\frac{f(\textbf{a})}{f(\textbf{b})}Y\overset{D}{=}Y\overset{D}{=}\frac{f(\textbf{c})}{f(\textbf{b})}||h_S(\textbf{c})||_{plus}-||h_S(\textbf{b})||_{plus}+\frac{f(\textbf{c})}{f(\textbf{b})}Y.
$$
Now we use lemma \ref{distributionalequalitylemma}. It gives us that $\frac{f(\textbf{a})}{f(\textbf{b})}=\pm 1$ and also  $\frac{f(\textbf{c})}{f(\textbf{b})}=\pm 1$. Therefore, at least two values of $f(\textbf{a}), f(\textbf{b}), f(\textbf{c})$ must be equal (and neither of them are zero). WLOG $f(\textbf{a})= f(\textbf{c})$. Plugging this into equation (\ref{asdfgh}), we get $||h_S(\textbf{a})||_{plus}=||h_S(\textbf{c})||_{plus}$, which is a contradiction since we chose them to be distinct. 

\textbf{Part 2: } We proceed in a similar way to the previous part. For a contradiction, let such $f$ exist. First, some notation: let $Y=h_{s+1}(X_{s+1})\dots h_{k}(X_k)$ and define $\xi:=f(\textbf{X}_S) + (||h_S(\textbf{X}_S)||_{times} \cdot Y)$, which is the left hand side of (\ref{yuiDVA}). 

Choose $\textbf{a}, \textbf{b}, \textbf{c}\in\mathbb{R}^s$ in the support of $\textbf{X}_S$ such that $||h_S(\textbf{a})||_{times}, ||h_S(\textbf{b})||_{times}, ||h_S(\textbf{c})||_{times}$ are distinct and $f(\textbf{b})\neq 0$. 

Since $\xi\indep \textbf{X}_S$, then  $\xi\mid [\textbf{X}_S=\textbf{a}] \overset{D}{=}\xi\mid [\textbf{X}_S=\textbf{b}] \overset{D}{=}\xi\mid [\textbf{X}_S=\textbf{c}]$. Hence, 


\begin{equation}\label{asdfghDVA}
f(\textbf{a}) + ||h_S(\textbf{a})||_{times}\cdot Y\overset{D}{=}f(\textbf{b})+||h_S(\textbf{b})||_{times}\cdot Y\overset{D}{=}f(\textbf{c}) + ||h_S(\textbf{c})||_{times}\cdot Y.
\end{equation}
By dividing by a non-zero constant $f(\textbf{b})$ and subtracting a constant $||h_S(\textbf{b})||_{times}$, we get
$$
\frac{f(\textbf{a})}{f(\textbf{b})}||h_S(\textbf{a})||_{times}-||h_S(\textbf{b})||_{times}+\frac{f(\textbf{a})}{f(\textbf{b})}Y\overset{D}{=}Y\overset{D}{=}\frac{f(\textbf{c})}{f(\textbf{b})}||h_S(\textbf{c})||_{times}-||h_S(\textbf{b})||_{times}+\frac{f(\textbf{c})}{f(\textbf{b})}Y.
$$
Now we use lemma \ref{distributionalequalitylemma}. It gives us that $\frac{f(\textbf{a})}{f(\textbf{b})}=\pm 1$ and also  $\frac{f(\textbf{c})}{f(\textbf{b})}=\pm 1$. Therefore, at least two values of $f(\textbf{a}), f(\textbf{b}), f(\textbf{c})$ must be equal (and neither of them are zero). WLOG $f(\textbf{a})= f(\textbf{b})$. Plugging this into equation (\ref{asdfghDVA}), we get $||h_S(\textbf{a})||_{times}=||h_S(\textbf{b})||_{times}$, which is a contradiction since we chose them to be  distinct. 

\textbf{Part 3: } For a contradiction, let $f_1, f_2$ exist. Denote $Y = h(X_{s+1}, \dots, X_k)$. Choose $\textbf{a}, \textbf{b}\in\mathbb{R}^s$ in the support of $\textbf{X}_S$ such that $f_2(\textbf{a})\neq f_2(\textbf{b})\neq 0$. From (\ref{yuiTRI}), we get $f_1(\textbf{a}) + f_2(\textbf{a})Y \overset{D}{=}f_1(\textbf{b}) + f_2(\textbf{b})Y$. By rewriting, we get $\frac{f_1(\textbf{a})-f_1(\textbf{b})}{f_2(\textbf{b})} + \frac{f_2(\textbf{a})}{f_2(\textbf{b})}Y \overset{D}{=}Y$. Applying Lemma  \ref{distributionalequalitylemma}, we obtain $\frac{f_2(\textbf{a})}{f_2(\textbf{b})}=\pm 1$. Since $f_2$ is positive, we get $f_2(\textbf{a}) = f_2(\textbf{b})$. This is a contradiction. 


\textbf{Part 4: } We use the following well-known result. For a multivariate normal vector  \begin{align*}
    \textbf{Z}=(\textbf{Z}_1, \textbf{Z}_2)^\top &\sim N\bigg(\begin{pmatrix}
           \mu_1 \\
            \mu_2 
         \end{pmatrix}, 
         \begin{pmatrix}
           \Sigma_{1,1} ,   \Sigma_{1,2} \\
             \Sigma_{2,1},   \Sigma_{2,2}
         \end{pmatrix}\bigg),
 \end{align*}
where $\textbf{Z}_1, \textbf{Z}_2$ is a partition of $\textbf{Z}$ into smaller sub-vectors, it holds that $(\textbf{Z}_1|\textbf{Z}_2=a)$, the conditional distribution of the first partition given the second, has distribution equal to $N(\mu_a, \tilde{\Sigma})$, where
$\mu_a = \mu_1 + \Sigma_{1,2}\Sigma_{2,2}^{-1}(a-\mu_2)$, $\tilde{\Sigma} = \Sigma_{1,1} - \Sigma_{1,2}\Sigma^{-1}_{2,2}\Sigma_{2,1}$. Specifically, the covariance structure does not depend on $a$. 

\textit{Proof of Part 4: }
 For a contradiction, let $f_1, f_2$ exist. Choose $\textbf{a}, \textbf{b}\in\mathbb{R}^s$ in the support of $\textbf{X}_S$ such that $f_2(\textbf{a})\neq f_2(\textbf{b})\neq 0$. Denote $Y = \beta_{s+1}X_{s+1}+ \dots+ \beta_{k}X_k$ and $Y_a := (Y\mid \textbf{X}_S=a)$ and $Y_b := (Y\mid \textbf{X}_S=b)$ (do not mistake the notation with a do-intervention; $Y_a$ is simply a conditional distribution of $Y$ given $\textbf{X}_S=a$). Notice that $Y$ is also Gaussian. 
 
 The well-known result gives us that $\mu_a + Y_a \overset{D}{=}\mu_b + Y_b$, where $\mu_a, \mu_b$ are constants depending on the mean and covariance structure of $\textbf{X}$, and on $a,b$. 
 
 From (\ref{yuiSTYRI}), we get $$f_1(\textbf{a}) + f_2(\textbf{a})Y_a \overset{D}{=}f_1(\textbf{b}) + f_2(\textbf{b})Y_b.$$
 
 
 By rewriting, we get $\frac{f_1(\textbf{a})-f_1(\textbf{b}) + f_2(a)(\mu_a - \mu_b)}{f_2(\textbf{b})} + \frac{f_2(\textbf{a})}{f_2(\textbf{b})}Y_b \overset{D}{=}Y_b$. Applying Lemma  \ref{distributionalequalitylemma}, we obtain $\frac{f_2(\textbf{a})}{f_2(\textbf{b})}=\pm 1$. Since $f_2$ is positive, we get $f_2(\textbf{a}) = f_2(\textbf{b})$. This is a contradiction. 
 
 
\end{proof}


\begin{lemma}
Let $X,Y$ be continuous random variables and $f$ is a (non-random) strictly increasing function. Then, 
\begin{equation}\label{trivial_identity}
X\indep Y \iff f(X)\indep Y. \tag{\ding{95}} 
\end{equation}
\end{lemma}
\begin{proof}
This statement is trivial. 
\end{proof}




