\section{Introduction}
%TODO
%Note: eta and epsilon incoherence
%Change the plot with c=0.3, 0.6, 0.9

Causal reasoning holds great significance in numerous fields, including public policy, decision-making and medicine \citep{Holland, TheBookOfWhy}. Randomized control experiments are widely accepted as the gold standard method for determining causal relationships \citep{Rubin}. However, the feasibility of such experiments is often hindered by their high costs and ethical concerns. Therefore, it is important to estimate causal relations from observational data, which are obtained by observing a system without any interventions \citep{Elements_of_Causal_Inference}. 

In this paper, we deal with the problem of estimating a set of direct causes of a target variable from a random sample. Typically, research focus lies in estimating the full causal structure, while we are interested only in a local causal structure around one variable of interest. The main issue arises when several causal structures produce the same observed distribution; the set of direct causes can be unidentifiable. We can generally estimate only the Markov equivalence class (MEC). 

A variety of research papers have proposed a methodology to deal with unidentifiable structures. These methods are either structural-restriction-based, meaning we add some additional assumptions about the functional relations between the variables, such as assuming nonlinear Gaussian data-generation process \citep{hoyer2008, Peters2014, reviewANMMooij, Elements_of_Causal_Inference,immer2022identifiability, Nonlinear_Causal_Discovery_with_Confounders, bodik2023identifiability}; score-based, meaning we pick a causal structure with the best fit on the data according to some score function \citep{Greedy_search, Score-based_causal_learning}; or information-theory-based, using mutual information and approximations of Kolmogorov complexity \citep{IGCI, Slope, Natasa_Tagasovska}.  However, these methods were designed either for a bivariate case or to infer the entire causal structure of the system. 

Several methodologies have been proposed to infer a local structure around a target variable. These methods are typically divided into three categories: learning a local skeleton (unoriented graph), learning a minimal Markov blanket (a sufficient set), or learning a set of direct causes of the target variable (goal of this paper). Under causal sufficiency and faithfulness \citep{Pearl_book}, the PC algorithm \citep{PCalgorithm} can identify the MEC and consistently learn the skeleton of the full structure. \cite{Local_causal_discovery_2008}, \cite{Local_Causal_discovery2010} and \cite{Wang2014_local_DAG_learning} discuss modifications of the PC algorithm focusing only on the local structure. 
\cite{Local_Causal_discovery_Gao_2021} suggest a methodology for estimating the minimal Markov blanket based on comparing entropies of the variables.  
\cite{Local_Causal_discovery_Mona_Azadkia} propose a method to learn the direct causes of the target variable under the assumption that the causes are identifiable (specifically, assuming that the underlying structure is a polytree). In contrast, our work aims to distinguish between different local causal structures (local MEC) using an structural-restrictions-based approach, where we take the ideas from classical approaches and use them locally. 

In the following, we present the main ideas of the paper. The theory is based on a structural causal model (SCM; \citealp{Pearl}) where a target variable $Y$ is structurally generated as $Y = f_Y(\textbf{X}_{pa_Y}, \varepsilon_Y)$, where $\textbf{X} = (X_1, \dots, X_p)^\top$ are other variables in the system and $pa_Y\subseteq\{1, \dots, p\}$ are called parents (or direct causes) of $Y$, and $\varepsilon_Y\indep \textbf{X}_{pa_Y}$ is a noise variable. Our goal is to estimate the set $pa_Y$ from a random sample from $(Y, \textbf{X})$. Following the structural-restrictions-based ideology, we assume $f_Y\in\mathcal{F}$, where $\mathcal{F}$ is a subset of all measurable functions (for example, all linear functions). 

Throughout the paper, we restrict the class of functions $\mathcal{F}$ in the following way. We assume that $f_Y$ is invertible (notation $f_Y\in\mathcal{I}$) in the sense that $$\text{a function } f_Y^\leftarrow \text{ exists such that }\varepsilon_Y = f_Y^\leftarrow(\textbf{X}_{pa_Y},Y).$$ In other words, the noise variables can be recovered from the observed variables.  Moreover, we assume that $f_Y\in\mathcal{I}_m$, where 
$$
\mathcal{I}_{m} = \{f\in\mathcal{I} : \,\,f\text{ is not constant in any of its arguments} \}.
$$
This assumption is closely related to causal minimality (the subscript $m$ in $\mathcal{I}_{m}$ represents the word ``minimality''). For more details and a rigorous definition of the class  $\mathcal{I}_m$, see Appendix \ref{Appendix_A.1.}. Overall, we assume that $\mathcal{F}\subseteq\mathcal{I}_m$. 

Our framework is built on a notion of $\mathcal{F}$-identifiability.  Without loss of generality, we assume $\varepsilon_Y\sim U(0,1)$\footnote{To understand why this assumption is without loss of generality, consider the equality $f_Y(\textbf{X}_{pa_Y}, \varepsilon) =f_Y(\textbf{X}_{pa_Y}, q^{-1}(\varepsilon_Y))$, where $\varepsilon_Y\sim U(0,1)$ and $q$ is a distribution function of $\varepsilon$. We define $ \tilde{f}_Y(\textbf{X}_{pa_Y}, \varepsilon_Y)=f_Y(\textbf{X}_{pa_Y}, q^{-1}(\varepsilon_Y))$ and only work with $\tilde{f}_Y$. }. 
\begin{definition}\label{Definition1}
A non-empty set $S\subseteq\{1, \dots, p\}$ is called an $\mathcal{F}$-\textbf{plausible} set of parents of $Y$, if 
\begin{equation}\label{Definition_F_plausible}
f\in\mathcal{F} \text{ exists such that for } \varepsilon_S := f^{\leftarrow}(\textbf{X}_{S}, Y)\text{, it holds that }\varepsilon_S\indep \textbf{X}_S, \,\,\,  \varepsilon_S\sim U(0,1).
\end{equation}
We define a set of $\mathcal{F}$-\textbf{identifiable} parents of $Y$ as follows:
\begin{equation*}\label{definition_F_identifiable}
S_\mathcal{F}(Y):= \bigcap_{S\subseteq \{1, \dots, p\}}\{S:   \text{ S is } \mathcal{F} \text{-plausible set of parents of Y}\}.
\end{equation*}
    
\end{definition}

The functional space $\mathcal{F}$ corresponds to the assumptions that we are willing to make about the data-generation process of $Y$. 
If we assume linearity of the covariates, this represents the assumption $f_Y\in\mathcal{F}_L$, where (recall that without loss of generality, we assume $\varepsilon_Y\sim U(0,1)$)
\begin{equation}\label{mathcalF_L}
\mathcal{F}_L = \{f\in\mathcal{I}_m: f(\textbf{x}, \varepsilon) = \beta^T\textbf{x} + q^{-1}(\varepsilon) \text{ for some quantile function }q^{-1} \text{ and } \beta\in\mathbb{R}^{|\textbf{x}|}\}.
\end{equation}
Note that the restriction  $f\in\mathcal{I}_m$ in (\ref{mathcalF_L}) implies that the arguments $\beta_i\neq 0$. On the other hand, if we assume a Conditionally Parametric Causal Model  ($CPCM(F)$, see (\ref{CPCM_def}) in Section~\ref{section_notation}), this corresponds to assuming $f_Y\in\mathcal{F}_F$ where 
\begin{equation}\label{mathcalF_F}
\mathcal{F}_F := \{f\in\mathcal{I}_m: f(\textbf{x},\varepsilon) =  F^{-1}(\varepsilon;\theta(\textbf{x})) \text{ for some function } \theta\}.
\end{equation}
Table~\ref{tableDefinitions} describes all functional spaces considered in this paper.  

The concept of  $\mathcal{F}$-identifiability provides theoretical limitations for the causal estimates under assumption $f_Y\in\mathcal{F}$. If $S_\mathcal{F}(Y) $ contains one element, we can only identify one cause of $Y$, even with infinite number of observations. The main part of the paper consists of inferring which elements belong to $S_\mathcal{F}(Y) $: When does it hold that  $S_\mathcal{F}(Y) = pa_Y$? 

From a practical point of view, we propose two algorithms for estimating the direct causes of a target variable from a random sample. One provides an estimate of $S_\mathcal{F}(Y)$, that is, it tests the $\mathcal{F}$-plausibility of several sets and outputs their intersection. This shields us against the mistake of including a non-parent in the output. However, the output does not have to contain all direct causes. The second is a score-based algorithm estimating $pa_Y$ based on a goodness-of-fit; even if several sets are $\mathcal{F}$-plausible, the output is the set with the best score. The first algorithm has strong theoretical guarantees for containing only the direct causes of $Y$. In contrast, the score-based algorithm outputs the ``best looking'' set of direct causes without theoretical guarantees for the output. 


We primarily focus on the case when $\textbf{X}=(X_1, \dots, X_p)$ are neighbors (either direct causes or direct effects) of $Y$ in the corresponding SCM. Using the classical conditional independence approach and d-separation (see Section~\ref{section_notation}), we can eliminate other variables from being potential parents of $Y$. Nevertheless, the theory can be extended to non-neighbors as well. 


In the following example, we provide an initial assessment of the findings presented in Section~\ref{Section_3}.
\begin{example}[Teaser example with Gaussian assumptions]\label{Teaser example with Gaussian assumptions}
Consider an SCM defined as follows (see Figure \ref{Figure_DAG_for_teaser}): let $(X_1, X_2, X_3)$ be normally distributed, and $X_5$ be non-degenerate. Let
$$Y = \mu_Y(X_1, X_2, X_3) + \sigma_Y(X_1, X_2, X_3)\varepsilon_Y,\,\,\,  \varepsilon_Y\text{ is Gaussian,}\,\,\,\,\,\,\,\,\varepsilon_Y\indep (X_1, X_2, X_3),$$
$$X_4 = \mu_4(X_2, Y, X_5) + \sigma_4(X_2, Y, X_5)\varepsilon_4,\,\,\,  \varepsilon_4\text{ is Gaussian,}\,\,\,\,\,\,\,\,\,\,\,\,\varepsilon_4\indep (X_1, X_2, X_3, Y),$$
where $(\mu_4, \sigma_4)^\top$ and  $(\mu_Y, \sigma_Y)^\top$ are real functions that satisfy some (weak) assumptions presented in Section~\ref{Section_3}. In particular,  $(\mu_4, \sigma_4)^\top$ are not in the form (\ref{norm}) and $\mu_Y$ is not additive (or $\sigma_Y$ is not multiplicative) in its arguments. 

Then, $S_{\mathcal{F}_F}(Y) = S_{\mathcal{F}_{LS}}(Y) = pa_Y = \{1,2,3\}$, where $F$ is a Gaussian distribution function. This result follows from the theory presented in  Section~\ref{Section_3}; in particular, it is a consequence of Lemma \ref{GaussianSeparabilita} in combination with Lemma \ref{lemma o inseparability=unplausibility}, Consequence~\ref{PropositionOAdditiveParents} and results in Example \ref{Gaussian case} combined with Proposition~\ref{TheoremFidentifiabilityWithChild}.
\end{example}


This paper is structured as follows. Section~\ref{section_notation} provides a standard notation and classical preliminary results from a theory of graphical models.  We also briefly summarize causal discovery methods from the literature that are based on structural restrictions. In Section~\ref{Section_3}, we dive deeper into mathematical properties of $S_\mathcal{F}(Y)$, where the aim is to find conditions under which $S_\mathcal{F}(Y) = pa_Y$. In Section~\ref{section_algorithm}, we describe our proposed algorithms for estimating $S_\mathcal{F}(Y)$ and $pa_Y$ from a random sample. Section~\ref{section_simulations} contains a short simulation study followed by an application on a real dataset. The paper has four appendices: Appendix~\ref{Speci_appendix} contains some detailed notions omitted from the main text for clarity,  Appendix~\ref{Appendix_Auxiliary} provides some auxiliary results needed for the proofs (in particular,  Lemma~\ref{CoolLemma} is the core mathematical result of the paper), the proofs can be found in Appendix~\ref{Section_proofs}, and Appendix~\ref{Appendix_Simulations}  contains some details about the simulations and the application. 


% Figure environment removed


















\begin{table}[]
\begin{tabular}{|l|}
\hline
\multicolumn{1}{|c|}{Summary of different $\mathcal{F}\subset \mathcal{I}_m$ used in the paper}                                                                                                                    \\ \hline
$\mathcal{F}_L = \{f\in\mathcal{I}_m: f(\textbf{x}, \varepsilon) = \beta^T\textbf{x} + q^{-1}(\varepsilon) \text{ for some quantile function }q^{-1} \text{ and }\beta\neq 0\}$                                \\ \hline
$\mathcal{F}_A = \{f\in\mathcal{I}_m: f(\textbf{x}, \varepsilon) = \mu(\textbf{x}) + g^{-1}(\varepsilon) \text{ for some }\mu(\cdot)\text{ and quantile function }q^{-1}     \}$        \\ \hline
$\mathcal{F}_{LS} = \{f\in\mathcal{I}_m: f(\textbf{x}, \varepsilon) = \mu(\textbf{x}) + \sigma(\textbf{x}) q^{-1}(\varepsilon) $                                                     \\
\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,$\text{ for some functions  }\mu, \sigma>0\,\,\,\,\,\text{ and for some quantile function }q^{-1}\}$ \\ \hline
$\mathcal{F}_F := \{f\in\mathcal{I}_m: f(\textbf{x},\varepsilon) =  F^{-1}(\varepsilon;\theta(\textbf{x})) \text{ for some function } \theta:\mathbb{R}^{|\textbf{x}|}\to\mathbb{R}^q\}$         \\ \hline
\end{tabular}
\caption{The table summarizes different functional spaces $\mathcal{F}$ used in the paper. $\mathcal{F}_L$, $\mathcal{F}_A$, $\mathcal{F}_{LS}$, and $\mathcal{F}_F$ correspond to the linearity assumption, additivity assumption, location-scale assumption, and $CPCM(F)$ assumption, respectively.   }
\label{tableDefinitions}
\end{table}










