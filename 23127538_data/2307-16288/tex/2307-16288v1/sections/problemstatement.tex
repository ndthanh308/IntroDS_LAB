\documentclass{article}
\begin{document}	
    To scope our survey paper, we begin first with an overview of the storage systems we are looking at, then define three specific prediction tasks that this paper focuses on.
    
    \subsection{Storage Hierarchy in Datacenters}
    \label{sec:2.1}
    % Figure environment removed
    
    Figure~\ref{fig:hierarchy} shows a programmer's view of storage hierarchy of a datacenter. The server consists of a number of processor sockets, each with a multicore CPU and its internal cache hierarchy, local DRAM, a number of directly attached disk drives or flash based solid state drives. The DRAM and disk/flash drives within the rack are accessible through the first-level rack switches (assuming some sort of remote procedure call API to them exists), and all resources in all racks are accessible via the cluster level switch. The capacity, latency, throughput of each level of storage is shown in the figure.
    
    To increase throughput and robustness at the local drive level, \textit{RAID} technologies are widely deployed, where multiple homogeneous drives are connected to the host server via a RAID controller. To further increase capacity and robustness, and hide the complexity of using multiple remote memories and remote disk/flash devices, \textit{distributed file systems} are proposed, e.g. NFS, GFS, Colossus, etc. Due to their almost unlimited capacity and high level of fault tolerance, distributed file systems are widely used as the storage backend for \textit{big data applications}. For instance, each worker node in Spark\cite{zaharia2012resilient,zaharia2010spark} (similar as the server at the highest level in Figure~\ref{fig:hierarchy}) is connected to a master node for coordination and the HDFS for storage (similar as the lower two levels in Figure~\ref{fig:hierarchy}), and data can be shuffled between multiple worker nodes. When materializing an RDD, Spark will first try to cache it within the distributed memory; if insufficient, some partitions will be stored or evicted to HDFS. Note that different prediction tasks may focus on different part of this hierarchy, which will be elaborated in the following subsection.
    
    \subsection{Task Definitions}
    \label{sec:2.2}
    
    % Figure environment removed
    
    Figure~\ref{fig:task tree} presents an overview of different proactive prediction tasks, each uniquely defined by its \textit{time granularity} and \textit{applicable domain}. By time granularity, we mean how often we need to make such prediction, which further imposes an upper bound on the inference time. More specifically, fine-grained prediction indicates the inference time should be sub-ms or even sub-us scale, while coarse-grained prediction indicates the inference time could be around a few seconds. By applicable domain, we mean which part in the storage hierarchy (as in Section~\ref{sec:2.1}) this task is focusing on. In total, there are six different tasks according to existing literature, as shown in Figure~\ref{fig:task tree}.
    
    Due to space limit, among these six different tasks, we choose four \textit{recently well studied} ones (captioned as Task 1-3 in Figure~\ref{fig:task tree}) for closer inspection. Note that we integrate the two failure prediction tasks (for HDD/SSD respectively) into Task 3 due to their similarity in usage and requirements. Coarse-grained performance prediction for storage device\cite{wang2004storage,yin2006empirical,varki2004issues} was well studied twenty years ago, as a helper of the solver in automated storage management tools like Hippodrome\cite{anderson2002hippodrome}. However, its importance has drastically faded with the emergence of cloud computing and various distributed file systems that are well designed for different workloads. On the other hand, coarse-grained performance prediction for distributed file systems\cite{hsu2016inside} is unluckily receiving little research attention. As a consequence, we didn't include them in our closer inspection.
    
    For each of the chosen tasks, we first describe at a high level the usage of the prediction as well as requirements on the input and output, all of which are imposed by the time granularity and applicable domain of that task. Later, we present a case study for each chosen tasks in Section~\ref{sec:linnos}-\ref{sec:smarter}.
    
    \subsubsection{Task 1: Fine-grained Performance Prediction for Storage Devices}
    \label{sec:task 1}
    In this task, since we make prediction and take corresponding action for \textit{each incoming I/O on the target storage device}, the inference time should be negligible comparing to the original I/O latency: a $<\%3$ overhead per I/O is desirable. Thus, for hard disk drives, the inference time should be sub-1ms, for solid state drives, the inference time should be sub-10us.
    
    \begin{itemize}
        \item \textbf{Usage:} On each device, we want to reject the incoming I/O requests whose SLOs cannot be met, and perform failover to another less busy device within the same RAID array or on another server (in the case of NoSQL systems). The failover has little overhead (only $\sim$15us within the same RAID), comparing to the time spent on waiting otherwise. By performing this quick detection and failover, we cut the "tail" in datacenters.
        \item \textbf{Input:}  In order to meet the requirements on inference time, the input features to the performance model should be minimized to only include those that really matters.
        \item \textbf{Output:} The output can be either a binary label (e.g., whether the SLO can be met), or the precise value of the metric (e.g., I/O latency) which can then be compared with the SLO provided by the application.
    \end{itemize}
    
    Examples of this task include\cite{hao2020linnos,hao2017mittos}. Section~\ref{sec:linnos} is a case study of this task.
    
    \subsubsection{Task 2: Coarse-grained Performance Prediction for Data-intensive Applications}
    \label{sec:task 2}
    In this task, we make prediction for each incoming job, before they are submitted or scheduled. Since a job in big data frameworks (e.g., MapReduce, Spark) always lasts for tens of seconds, we don't have a strict upper bound on the inference time. It would be best if the inference time can be controlled to sub-second scale.
    
    \begin{itemize}
    \item \textbf{Usage:} The prediction should be made at the time of planning and scheduling a job. For datacenter administrators or resource management software in datacenters, the prediction result helps to determine where to launch this new, say Spark, instance so that the SLO won't be violated; for cloud service users, it helps them choose the cheapest VM configuration given a performance goal, or choose the most performant one given a budget. 
    \item \textbf{Input:} Since the prediction is made before the job starts running, static input features are required, i.e. they should be known prior to running. This suggests that no runtime counters or runtime measurements (e.g., CPU utilization, average I/O latency) can serve as the input.
    \item \textbf{Output:} The output should be exact metric such as JCT, query latency, etc.
    \end{itemize}
    
    Although coarse-grained prediction sounds easier, at least similar, to fine-grained prediction in Task 1, more challenges arise for performance prediction at the level of data-intensive applications: 1) The datacenter network is involved in both storage backend (distributed file systems) and the data shuffling between compute nodes, as illustrated in section~\ref{sec:2.1}, which is a common source of contention and thus the source of unpredictability; 2) The application itself may contain randomizations and intricate system optimizations, which are also the source of unpredictability. 
    
    Examples of this task include\cite{fu2021use,ousterhout2017monotasks,venkataraman2016ernest,yadwadkar2014wrangler,verma2011aria}. Section~\ref{sec:mlblackbox} is a case study of this task. 
    
    \subsubsection{Task 3: Coarse-grained Failure Prediction for HDDs and SSDs}
    \label{sec:task 3}
    In this task, we explore the second dimension of proactive prediction: failure. We make failure prediction for every already deployed HDD and SSD on a daily basis. Therefore, there is no strict requirement on the inference time. However, unlike previous tasks, here the prediction horizon is much larger, e.g., 10-20 days should suffice. This means we are predicting whether this drive would fail within the next 10-20 days.
    
    \begin{itemize}
    \item \textbf{Usage:} After predicting whether a certain drive is going to fail in the next few days, the datacenter administrators can further manually examine or replace all suspicious drives. By fixing these suspicious drives, we can prevent correlated failures (i.e., failures within the same spatial domain and the same time period) from happening, for which current redundancy schemes are insufficient. 
    \item \textbf{Input:} SMART attributes together with other informative features should be used as input. Here we are able to use fancier features and summary statistics as input.
    \item \textbf{Output:} The output should be a binary label.
    \end{itemize}
    
    Although failure and performance looks unrelated to each other, they are never strictly decoupled in the systems filed, since failures often adversely affect performance\cite{hao2016tail}, and performance is a strong indicator for failures as we will show later. Examples of this task include \cite{lu2020making,xugeneral,han2020toward,mahdisoltani2017proactive,xiao2018disk,xu2018improving,alter2019ssd}. Section~\ref{sec:smarter} is a case study of this task. 
    
    \ifcsdef{mainfile}{}{\bibliography{../references/main}}
\end{document}