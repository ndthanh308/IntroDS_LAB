\documentclass{article}
\begin{document}	
    Having presented the definitions of proactive prediction, we further present a taxonomy of different prediction \textit{techniques} proposed (shown in Figure~\ref{fig:technique tree}). Note that the techniques surveyed in this section are orthogonal to task definitions in Section~\ref{sec:problemstatement}, i.e., for a certain task, any technique applies as long as it meets the requirements. For instance, either whitebox\cite{hao2017mittos} or blackbox\cite{hao2020linnos} approach can be applied to Task 1.
    
    % Figure environment removed
    
    \subsection{Performance Prediction}
    \label{sec:3.1}
    Performance prediction techniques can be taxonomized as whitebox\cite{hao2017mittos, varki2004issues, ousterhout2017monotasks, verma2011aria} or blackbox\cite{hao2020linnos,wang2004storage,yin2006empirical, fu2021use,venkataraman2016ernest,hsu2016inside,yadwadkar2014wrangler}, according to how they view the underlying device or system, which is further discussed below.
    
    \begin{itemize}
        \item \textbf{Whitebox:} Whitebox approaches assume full knowledge of the underlying system internals. For instance, MittOS\cite{hao2017mittos} built its prediction model based on its understanding of contention and queueing discipline of the underlying resource (e.g., disk spindles vs. SSD channels, FIFO vs. priority). Some whitebox approaches may even re-architect the system in order to build a better prediction model\cite{ousterhout2017monotasks}.
        
        \item \textbf{Blackbox:} Blackbox approaches have no information about the internal components or algorithms of the underlying system. Thus, most blackbox approaches utilize machine learning to learn the systems behavior from examples. 
        
        On the one hand, blackbox approaches are more generalizable and scalable than whitebox ones. Often, the device internals are proprietary and not exposed to researchers. Further, it's almost infeasible to deploy whitebox approaches in production deployment, since numerous vendors - each with various device types - are involved, where each type of device need expertise's inspection in order to build the whitebox model. On the other hand, blackbox approaches are often fundamentally limited, resulting in poor prediction accuracy even under exceptionally simplified settings, as is later shown in Section~\ref{sec:mlblackbox}. 
    \end{itemize}
    
    \subsection{Failure Prediction}
    \label{sec:3.2}
    Most failure prediction techniques are blackbox approaches, and they can be further taxonomized as offline\cite{xugeneral,mahdisoltani2017proactive,alter2019ssd,lu2020making,xu2018improving} or online\cite{han2020toward,xiao2018disk}, according to how the performance model is trained or built, which is further discussed below.
    
    \begin{itemize}
        \item \textbf{Offline:} Offline scheme means that all training data must be available before training the prediction model. In other words, the model won't upgrade once it is deployed in production.
        \item \textbf{Online:} Online scheme means that the prediction model is updated incrementally in real time (i.e., after being deployed), upon receiving each newly generated healthy or failure sample. Some work utilizes online random forest algorithm\cite{xiao2018disk}, while other work formulates a stream mining problem so as to combat concept drift\cite{han2020toward}.

        Online approaches are generally more preferred than offline approaches, since disk logs are continuously generated, in which the statistic patterns may vary over time. That being said, offline schemes may suffer in long-term use, because the testing data is going to come from a different distribution from training data.
    \end{itemize}
    
    \ifcsdef{mainfile}{}{\bibliography{../references/main}}
\end{document}