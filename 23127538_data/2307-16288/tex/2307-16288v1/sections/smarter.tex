\documentclass{article}
\begin{document}	
    In order to explore the second dimension of this proactive prediction, Lu et.al. conducted this field study (referred to as SMARTer) on disk failure prediction, which covers a total of 380,000 hard drives over a period of two months across 64 sites of a large datacenter operator. For the first time, SMARTer demonstrates that disk failure prediction can be made highly accurate by combing disk performance data and disk location data with disk monitoring data.
    
    \subsection{Overview}
    \label{sec:6.1}
    \textbf{Methodology:} In what follows, we first introduce the definition of failure, followed by the three types of data used throughout SMARTer. Given the complexity of disk failures, there is no common, agreed-upon universal definition of a disk failure. SMARTer considers a disk to be failed when there is a failed read/write operation and the disk cannot function properly upon restart. The failure label is tagged by the IT operators of the datacenter we study.
    
    SMART (Self-Monitoring, Analysis and Reporting Technology) attributes is the first source of data. It covers disk health measurements such as correctable errors, temperature, disk spin-up time, etc. The number of available SMART attributes is more than 50, but not all disks log all of the attributes at all times. The SMART attributes are collected and reported at per-day granularity.

    Performance is the second source of data. SMARTer collects two types of performance metrics maintained by the OS kernel: disk-level metrics and server-level metrics. Disk level performance metrics (12 in total) include IOQueue size, throughput, latency, the average waiting time for I/O operations, etc. Server level performance metrics (154 in total) include CPU activity, page in and out activities, etc. Performance metrics are reported at per-hour granularity, since hourly granularity is effective in improving the prediction quality.

    Location is the third source of data. Each disk has four levels of location markers associated with it: site, room, rack, and server.

    To evaluate the effectiveness of different features---SMART attributes (S), Performance metrics (P), and location markers (L), six different feature groups are considered: SPL, SL, SP, PL, S, and P. Precision, recall, F-measure, and Matthews correlation coefficient are used as metrics for evaluation.
    
    \subsubsection{Selection of Attributes}
    \label{sec:6.1.1}
    Since the storage overhead of all performance metrics can become significant at scale and over time, SMARTer leverages J-Index to down-select features. After features are normalized to the scale of 0-1, SMARTer sets a series of threshold candidates for each feature with a step of 0.01, starting from 0 until 1, and calculates the value of corresponding J-Index: since the distribution of failed disk and healthy disk along the feature is known, and the threshold is also known, we can calculate the TP/TN/FP/FN (predict the disk failure if only according to that feature and threshold) and thus J-Index. A higher J-Index means the corresponding threshold candidate is more distinguishable to identify failure disks from healthy disks. Then, the highest J-Index across thresholds is selected for every feature, and features with higher J-Index are selected as input for the ML models. Eventually, 14 SMART attributes, 12 disk level performance metrics, 18 server level performance metrics are selected.
    
    \subsubsection{ML Models}
    \label{sec:6.1.2}
    The input to a ML model is a time sequence of the selected attributes, i.e. multiple and fixed length readings. The output of a ML model is a binary label, indicating whether this disk will fail in the next ten days.
    
    SMARTer implemented and tuned five ML models, including Bayes, RF, GBDT, LSTM, CNN-LSTM. Among them, CNN-LSTM is a newly proposed model (as shown in Figure~\ref{fig:cnnlstm}), where CNN offers advantages in selecting better features, while LSTM is effective at learning sequential data.
    
    % Figure environment removed
    
    \subsubsection{Findings}
    \label{sec:6.1.3}
    Below we summarize the important findings and lessons from SMARTer:\\
    
    \noindent \textbf{Feature Importance:} The SPL feature group performs the best across all ML models, showing that performance and location features are critical for improving the effectiveness of disk failure prediction. More specifically, SMART attributes do not always have the strong predictive capability of making predictions at longer prediction horizon (e.g., 10 days ahead), because the changes in values of SMART attributes are often noticeable only a few hours before the actual failure. On the other hand, the performance may exhibit more variations long before the actual failure. Figure~\ref{fig:error perf} shows the performance difference between raw failed disk and averaged healthy disks. In Figure~\ref{fig:error perf}, some failed disks have a similar value to healthy disks at first, but then their behavior becomes unstable when approaching the impending failure; some other failed disks report a sharp impulse before they fail, as opposed to a linger erratic behavior. Interestingly, the effect of location information is pronounced only in the presence of performance features.
    
    % Figure environment removed
    
    \noindent \textbf{Model Selection:} While there is no single model winner across different feature groups, CNN-LSTM performs close to the best in all the situations. However, when performance features are not available, traditional tree-based models (RF and GBDT) can perform roughly as well as complex neural networks. Considering the large training overhead of neural networks, RF and GBDT are more preferred in this case.\\
    
    \noindent \textbf{Portability:} If we simply try to train the model on one datacenter site and port it to another datacenter site (i.e., test on another unseen site), the MCC score can drop significantly. However, if a model is trained on multiple datacenter sites before testing on a new unseen one, it provides high prediction accuracy, especially when using CNN-LSTM.
    
    \subsection{Evaluation}
    SMARTer is a case study for Task 3. According to the taxonomy presented in Section~\ref{sec:taxonomy}, it is an offline scheme for failure prediction. We summarize strengths and limitations of this field study as follows:\\
    
    \noindent \textbf{Strengths:}
    \begin{itemize}
        \item It is the first paper that demonstrates the strong predictive capability of SMART attributes combined performance metrics and location markers. This encourages researchers to pay more attention to the source of data instead of focusing on ML algorithms alone.
        \item It provides several actionable insights and trade-off lessons learned from a large scale, real world study.
    \end{itemize}
    
    \noindent \textbf{Limitations:}
    \begin{itemize}
        \item It does not consider the heterogeneity of hardware in deployment, i.e., different vendors and different drive types. Every component in the ML pipeline, i.e., feature engineering, model selection, model training and evaluation, is agnostic to the underlying device type.
        \item It directly chooses a simple J-Index based feature selection algorithm without even considering other fancier ones, such as random forest, XGBoost, Pearson correlation, etc. 
    \end{itemize}
    
    \ifcsdef{mainfile}{}{\bibliography{../references/main}}
\end{document}