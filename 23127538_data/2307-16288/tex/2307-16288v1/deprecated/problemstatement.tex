\documentclass{article}
\begin{document}	
    To scope our survey paper, we begin first with an overview of the storage systems we are looking at, then define three specific prediction tasks. We define each prediction task by first explaining how prediction can help (i.e. action to take after the prediction), followed by the requirements it exerts on the time granularity of the prediction, and on the input and output of the ML model.
    
    \subsection{Storage Hierarchy in Datacenters}
    \label{sec:storagehierarchy}
    % Figure environment removed
    
    Figure~\ref{fig:hierarchy} shows a programmer's view of storage hierarchy of a datacenter. The server consists of a number of processor sockets, each with a multicore CPU and its internal cache hierarchy, local DRAM, a number of directly attached disk drives or flash based solid state drives. The DRAM and disk/flash drives within the rack are accessible through the first-level rack switches (assuming some sort of remote procedure call API to them exists), and all resources in all racks are accessible via the cluster level switch. The capacity, latency, throughput of each level of storage is shown in the figure.
    
    To increase throughput and robustness at the local drive level, \textit{RAID} technologies are widely deployed, where multiple homogeneous drives are connected to the host server via a RAID controller. To further increase capacity and robustness, and hide the complexity of using multiple remote memories and remote disk/flash devices, \textit{distributed file systems} are proposed, e.g. NFS, GFS, Colossus, etc. Due to their almost unlimited capacity and high level of fault tolerance, distributed file systems are widely used as the storage backend for big data applications. For instance, each worker node in Spark\cite{zaharia2012resilient, zaharia2010spark} (similar as the server at the highest level in Figure~\ref{fig:hierarchy}) is connected to a master node for coordination and the HDFS for storage (similar as the lower two levels in Figure~\ref{fig:hierarchy}), and data can be shuffled between multiple worker nodes. When materializing an RDD, Spark will first try to cache it within the distributed memory; if insufficient, some partitions will be stored or evicted to HDFS. Note that different prediction tasks may focus on different part of this hierarchy, which will be elaborated in the following subsections.
    
    \subsection{Performance Prediction for Storage System}
    \label{sec:performanceprediction}
    To begin with, we explore the first dimension of proactive prediction: performance. Since disk I/Os contribute a lot to the total response time of data-intensive applications, and in order to start with a relative simple setting, we first focus on the level of each individual disk drive/solid state drive in Figure~\ref{fig:hierarchy}.
    
    \begin{itemize}
        \item \textbf{Action to take:} On each drive, we want to reject the I/O requests whose SLOs cannot be met, and perform failover to another less busy drive within the same RAID array or on another server (in the case of NoSQL systems). The failover has very little overhead ($\sim$15us within the same RAID), comparing to the time spent on waiting otherwise.
        \item \textbf{Time granularity:} Since we make prediction for each incoming I/O, the inference time should be negligible comparing to the original I/O latency. An overhead less than \%3 per I/O is desirable. Thus, for hard disk drives, the inference time should be sub-1ms, for solid state drives, the inference time should be sub-10us.
        \item \textbf{Input:} In order to meet the requirements on inference time, the input features to the model should be minimized to only include those that really matters.
        \item \textbf{Output:} The output can be either a binary label (e.g., whether the SLO can be met), or the precise value of the metric (e.g., I/O latency) which can then be compared with the SLO provided by the application.
    \end{itemize}
    
    In Section~\ref{sec:linnos}, we will show that the ML approach taken satisfies all the requirements above. 
    
    \subsection{End-to-End Performance Prediction for Data-intensive Applications}
    \label{sec:endtoendperformanceprediction}
    Only predicting performance at the level of disk drives/solid state drives is not enough, as both the cloud service users and the datacenter administrators would benefit a lot in terms of optimal configuration searching and resource allocation, if we can generalize the ML-based "blackbox" approach to data-intensive applications. At the level of data-intensive applications, more challenges for end-to-end performance prediction arise: 1) The datacenter network is involved in both storage backend (distributed file systems) and the data shuffling between compute nodes, as illustrated in Section~\ref{sec:storagehierarchy}, which is a common source of contention and thus the source of unpredictability; 2) The application itself may contain randomization and intricate system optimizations, which are also the source of unpredictability. Thus, it is crucial to examine how well ML-based "blackbox" approach works in this scenario.
    
    \begin{itemize}
        \item \textbf{Actions to take:} The prediction should be made at the time of planning and scheduling a job. For datacenter administrators or resource management software in datacenters, the prediction result helps to determine where to launch this new, say Spark, instance so that the SLO won't be violated; For cloud service users, it helps them choose the cheapest VM configuration given a performance goal, or choose the most preferment one given a budget.
        \item \textbf{Time granularity:} Due to the nature of this task, the requirement on inference time is not that strict as in Section~\ref{sec:performanceprediction}. It would be best if the inference time can be controlled at sub-second scale.
        \item \textbf{Input:} Since the prediction is made before the job starts running, static input features are required, i.e. they should also be known prior to running. This suggests that no runtime counters or runtime measurements (e.g., CPU utilization, average I/O latency) can serve as the input.
        \item \textbf{Output:} The output should be exact metric such as JCT, query latency, etc.
    \end{itemize}
    
    In Section~\ref{sec:mlblackbox}, we will show that the ML approach taken satisfies all the requirements above. 
    
    \subsection{Failure Prediction for Storage Systems}
    \label{sec:failureprediction}
    Next, we explore the second dimension of proactive prediction: failure. Again, we first focus on the level of each individual HDD/SSD in Figure~\ref{fig:hierarchy}, as drives are the main reason behind server failures\cite{vishwanath2010characterizing}. Interestingly, failures and performance are never strictly decoupled in the systems filed, since failures often adversely affect performance\cite{hao2016tail}, and performance is a strong indicator for failures as we will show later.
    
    \begin{itemize}
        \item \textbf{Actions to take:} After predicting whether a certain drive is going to fail in the next few days, the datacenter administrators can further manually examine or replace all suspicious drives. By fixing these suspicious drives, we can prevent correlated failures (i.e., failures within the same spatial domain and the same time period) from happening, for which current redundancy schemes are insufficient. 
        \item \textbf{Time granularity:} Since the predictions should be made on a daily basis, there is no strict requirement on the inference time. However, unlike previous tasks, here the prediction horizon is much larger, e.g., 10-20 days should suffice. This means we are predicting whether this drive would fail within the next 10-20 days.
        \item \textbf{Input:} SMART attributes together with other informative features should be used as input. Here we are able to use fancier features and summary statistics as input.
        \item \textbf{Output:} The output should be a binary label.
    \end{itemize}
    
    In Section~\ref{sec:smarter}, we will show that the ML approach taken satisfies all the requirements above. 

    \ifcsdef{mainfile}{}{\bibliography{../references/primary}}
\end{document}