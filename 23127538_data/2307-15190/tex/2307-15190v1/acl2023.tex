\pdfoutput=1

\documentclass[11pt]{article}

\usepackage[]{ACL2023}

\usepackage{times}
\usepackage{latexsym,bm}
\usepackage{amsthm,thmtools, thm-restate}
\newtheorem{theorem}{Theorem}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage{thm-restate}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{amsfonts}
\usepackage{paralist}
\usepackage{subcaption}
\usepackage{pifont}

\usepackage{multirow}
\usepackage{wasysym}
\theoremstyle{plain}
\usepackage{siunitx}
\usepackage{paralist}
\usepackage{fancyhdr}

\declaretheorem[name=Theorem]{thm}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\E}{\operatorname*{\mathbb E}}
\newcommand{\bfY}{{\mathbf Y}}
\newcommand{\rmY}{{\mathrm Y}}

\title{$f$-Divergence Minimization for Sequence-Level Knowledge Distillation}

\author{
Yuqiao Wen$^{1,*}$, Zichao Li$^{2,*}$, Wenyu Du$^3$, Lili Mou$^{1,4}$ \vspace{0.2cm} \\
\normalsize $^1$Dept.~Computing Science \& Alberta Machine Intelligence Institute (Amii), 
University of Alberta\\
\normalsize$^2$Mila, McGill University \quad\quad $^3$The University of Hong Kong \\
\normalsize$^4$Canada CIFAR AI Chair, Amii \quad\quad\ \ \ \quad $^*$Equal contribution \\
\normalsize\texttt{\url{yq.when@gmail.com}, \url{zichao.li@mila.quebec}} \\
\normalsize \texttt{\url{wenyudu@yahoo.com}, \url{doublepower.mou@gmail.com}} \\
}

\begin{document}

\maketitle

\newcommand{\fdistill}{\textsc{$f$-distill}}
\newcommand\scalemath[2]{\scalebox{#1}{\mbox{\ensuremath{\displaystyle #2}}}}

\begin{abstract}

Knowledge distillation (KD) is the process of transferring knowledge from a large model to a small one.
It has gained increasing attention in the natural language processing community, driven by the demands of compressing ever-growing language models.
In this work, we propose an \textsc{$f$-distill} framework, which formulates sequence-level knowledge distillation as minimizing a generalized $f$-divergence function.
We propose four distilling variants under our framework and show that existing SeqKD and ENGINE approaches are approximations of our \fdistill{} methods.
We further derive step-wise decomposition for our \fdistill{}, reducing intractable sequence-level divergence to word-level losses that can be computed in a tractable manner.
Experiments across four datasets show that our methods outperform existing KD approaches, and that our symmetric distilling losses can better force the student to learn from the teacher distribution.\footnote{
Our code is available at \url{https://github.com/MANGA-UOFA/fdistill}
}

\end{abstract}

\section{Introduction} \label{sec:intro}
\allowdisplaybreaks

\renewcommand{\headrulewidth}{0pt}
\fancyhf{}
\cfoot{In \textit{ACL'23}, pages 10817–10834, with additional notes from the conference.}
\thispagestyle{fancy}

Increasingly large language models have continued to achieve state-of-the-art performance across various natural language generation tasks, such as data-to-text generation~\citep{lebret-etal-2016-neural,li-liang-2021-prefix}, summarization~\citep{paulus2018deep,pmlr-v119-zhang20ae}, and dialogue generation~\citep{dialogue-reinforcement,zhang-etal-2020-dialogpt}.
However, super-large language models are inaccessible to most users and researchers due to their prohibitively large model size, emphasizing the importance of high-performing, parameter-efficient small neural models.

A widely used approach to training small models is \textit{knowledge distillation}~\cite[KD,][]{hinton2015distilling}, where the small model (known as the \textit{student}) learns the knowledge from a much larger model (known as the \textit{teacher}). 
KD has shown great success in helping smaller models achieve competitive performance across a wide range of applications~\citep{sun2019pkd,jiao-etal-2020-tinybert,shleifer2020pre}.

Existing KD approaches can be categorized into two main branches: representation matching and distribution matching.
The former aims to imitate the teacher's real-valued intermediate-layer representations, say, with mean squared error~\citep{sun2019pkd,jiao-etal-2020-tinybert}.
Our work focuses on the latter, distribution matching, where the student model learns the teacher's predictive distribution.
\citet{hinton2015distilling} minimize the cross-entropy loss against the teacher-predicted soft labels, which is equivalent to minimizing the Kullback--Leibler (KL) divergence between the teacher and student.
\citet{kim-rush-2016-sequence} propose SeqKD, arguing that KL divergence should be minimized at the sequence level for language models. However, such an approach tends to learn an overly smooth student distribution to cover the entire support  of the teacher distribution due to the asymmetric nature of the KL divergence. This is often known as the \textit{mode-averaging} problem (Figure~\ref{fig:modes}a).


\citet{tu-etal-2020-engine} propose ENGINE, a non-autoregressive translation model that minimizes the energy function defined by the teacher's output distribution. It can be shown that their objective is related to minimizing the reverse KL between the teacher and student (see Section~\ref{subsec:fdistill}). This, on the other hand, results in the \textit{mode-collapsing} problem, where the student model is overly concentrated on certain high-probability regions of the teacher distribution (Figure~\ref{fig:modes}b).

% Figure environment removed


In this paper, we address  knowledge distillation for text generation tasks, and propose \textsc{$f$-distill}, a unified framework that formulates sequence-level knowledge distillation as minimizing $f$-divergence functions.
Existing SeqKD~\citep{kim-rush-2016-sequence} and ENGINE~\citep{tu-etal-2020-engine} methods are  approximations of KL and reverse KL distillations under the \textsc{$f$-distill} framework. Further, our formulation naturally leads to Jensen--Shannon (JS) divergence and total variation distance (TVD) distillations, where the divergence measures are symmetric in teacher and student distributions. This forces the student to learn the teacher's distribution better, alleviating mode averaging and collapsing problems.


We further develop efficient algorithms for our \textsc{$f$-distill} approach. First, we show that sequence-level \textsc{$f$}-divergence can be decomposed step by step either exactly or as an upper bound. Second, we propose to sample from the teacher model in an offline manner, mitigating the additional training cost of symmetric divergence measures (namely, JS and TVD).

We evaluated our approach on four datasets: DART for data-to-text generation~\citep{nan-etal-2021-dart}, XSum for summarization~\citep{narayan-etal-2018-xsum}, WMT16 EN-RO for machine translation~\citep{bojar-etal-2016-wmt16}, and Commonsense Dialogue~\citep{zhou-etal-2021-commonsense}.
Experiments show that our proposed \textsc{$f$-distill} variants consistently outperform existing distribution-matching KD methods, allowing \textsc{$f$-distill} to achieve an add-on performance improvement when combined with representation-matching KD methods.
Further, results show that our symmetric distilling losses outperform asymmetric ones, confirming that extreme mode averaging or collapsing is not ideal.

To sum up, our contributions are three-fold:
\begin{compactenum}
    \item We propose \textsc{$f$-distill}, a novel distilling framework that generalizes KL distillation and balances mode averaging and collapsing;
    \item We derive step-wise decomposition and propose an offline sampling method to efficiently compute sequence-level $f$-divergences; and 
    \item We provide detailed experimental analysis across four text generation datasets to show the effectiveness of our approach.
\end{compactenum}

\section{Approach}

In this section, we first review classic knowledge distilling (KD) algorithms and analyze their drawbacks.
Then, we propose \textsc{$f$-distill}, a generalized distilling framework for sequence-level distillation.

\subsection{Classic KD and Its Drawbacks} \label{subsec:classic-kd}
In classic KD, the KL divergence is often used to train the student model to match the teacher's distribution~\citep{hinton2015distilling}.
For autoregressive text generation, this is decomposed into a step-wise KL divergence:
\begin{align}\label{eq:classicKD}
    J_{\text{KD}} = - \sum_{t=1}^{|\mathbf y|} \sum_{\mathrm Y_t\in V} p(\mathrm Y_t| \mathbf y_{<t}) \log q_{\theta}(\mathrm Y_t| \mathbf y_{<t})
\end{align}
where $\mathbf y = \mathrm y_1\cdots\mathrm y_T$ is the ground-truth sequence and  $V$ is the vocabulary.
$p$ and $q_\theta$ are the predicted distributions of the teacher and student, respectively; they can be additionally conditioned on an input sequence $\mathbf x$, which is omitted here for simplicity. In Eqn.~\eqref{eq:classicKD}, we present the loss by a cross-entropy term, which only differs from the KL divergence $D_\text{KL}(p\|q_\theta)$ by a constant.  

\citet{kim-rush-2016-sequence} propose SeqKD and minimize cross-entropy loss at the sequence level as
\begin{align}
    J_{\text{SeqKD}} &= \mathbb E_{\mathbf Y \sim p} [- \log q_\theta (\mathbf Y)]
\end{align}
In practice, the expectation over the sentence space is intractable, so they approximate it with a hard sequence $\mathbf y$ generated by beam search on the teacher model. Their loss is
\begin{align}
    \hat{J}_{\text{SeqKD}} = - \sum_{t=1}^{|\mathbf y|} \log q_{\theta}(\mathrm y_t|\mathbf y_{<t})
    \label{eq:seqkd-approx}
\end{align}

However, KL-based losses may cause the student model to learn an overly smooth function.
This can be seen in Eqn.~\eqref{eq:seqkd-approx}, where the loss term $-\log q_{\theta}(\mathrm y_t| \mathbf y_{<t})$ goes to infinity when the student assigns a low probability to a teacher-generated token.
As a result, minimizing KL forces the student model to spread its probability mass widely over the vocabulary.
When the student has a limited model capacity, this further leads to the mode-averaging problem, where the learned distribution may not capture any mode of the teacher distribution, as shown in Figure~\ref{fig:modes}a.

\subsection{Our Proposed \textsc{$f$-distill} Framework} \label{subsec:fdistill}

To this end, we propose a generalized \mbox{\textsc{$f$-distill}} framework, a family of distilling methods based on $f$-divergence functions~\citep{classic-fdiv,ieee-fdiv}.

Formally, the $f$-divergence of two distributions is defined as
\begin{align}
    D_f(p(t)\|q(t)) = \sum_{t} q(t)\, f\!\left(\frac{p(t)} {q(t)}\right) &
\end{align}
where $f: (0, \infty) \rightarrow \mathbb R $ is a convex function such that $f(1) = 0$.
Table~\ref{tab:f-div-map} summarizes common divergence functions. 

In the rest of this subsection, we will first present Kullback--Leibler (KL) and reverse KL (RKL) distilling methods, which are closely related to previous work~\cite{kim-rush-2016-sequence,tu-etal-2020-engine}. Then, we will propose Jensen--Shannon (JS) and total variation distance (TVD) distillations; they are based on symmetric $f$-divergence functions, and are able to force the student to better learn from the teacher distribution.

\begin{table}[!t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|l}
\hline
Divergence & $f(t)$ \\ \hline
Kullback--Leibler (KL)                & $t \log t$           \\
Reverse KL (RKL)                 & $- \log t$           \\
Jensen--Shannon (JS)                  & $-(t+1) \log (\frac{t+1}{2}) + t \log t$                    \\
Total variation distance (TVD)       & $\frac12 |t-1|$      \\ \hline
\end{tabular}
}
\caption{Common divergence functions and their corresponding choices of $f$.}
\label{tab:f-div-map}
\end{table}

\textbf{Kullback--Leibler (KL) distillation.} Recall that we denote the teacher distribution by $p$ and the student distribution by $q_\theta$.
Using the common KL divergence leads to the standard distilling objective
\begin{align}\label{eq:KL1}
    & J_{\text{KL}} = D_\text{KL}(p\|q_\theta)=\mathbb E_{\mathbf Y\sim p} \left[ \log \frac{p(\mathbf Y)}{q_\theta (\mathbf Y)} \right]  \\
    &\approx - \sum_{t=1}^{|\mathbf y|} \sum_{\mathrm Y_t \in V} p(\mathrm Y_t| \mathbf{y}_{<t}) \log q_{\theta}(\mathrm Y_t| \mathbf{y}_{<t}) + \text{const}
\end{align}
where $\mathbf y$ is sampled\footnote{In our method, the expectation \eqref{eq:KL1} is approximated by one Monte Carlo-sampled sequence. We denote a sampled sequence by a lower letter $\mathbf y$.} from the teacher distribution $p$.
Here, the constant is the entropy of $p$, which can be ignored as it does not involve the student parameters.

Similar to SeqKD, such KL distillation may also suffer from the mode-averaging problem and learn an overly smooth distribution, because $q_\theta$ is in the denominator in \eqref{eq:KL1}.

However, our KL distillation differs from \mbox{SeqKD} in that we adopt soft labels from the teacher model, i.e., keeping the entire distribution of $p(\mathrm Y_t| {\mathbf{y}}_{<t})$, whereas SeqKD uses a certain decoded sequence~${\mathbf y}$ as shown in Eqn.~\eqref{eq:seqkd-approx}. Experiments will show that our soft labels provide more information than hard SeqKD in sequence-level distilling tasks, which is consistent with early evidence~\citep{bucilua2006model, hinton2015distilling}.

\textbf{Reverse KL (RKL) distillation.}
We propose RKL distillation, which can potentially address the mode-averaging problem:
\begin{align}
    & J_{\text{RKL}} =D_\text{KL}(q_\theta\|p)= \mathbb E_{\mathbf Y'\sim q_\theta} \left[\log\frac{q_\theta (\mathbf Y')}{p(\mathbf Y')}\right] \nonumber \\
    &\approx \sum_{t=1}^{|\mathbf y'|} \sum_{\mathrm Y'_t \in V} \Big[ q_\theta(\mathrm Y'_t | \mathbf y'_{<t}) \log q_\theta(\mathrm Y'_t| \mathbf y'_{<t}) \nonumber \\
    &\quad\quad - q_\theta(\mathrm Y'_t| \mathbf y'_{<t}) \log p(\rmY'_t | \mathbf y_{<t}') \Big]
    \label{eq:rkl}
\end{align}
where $\mathbf y'$ is sampled from the student distribution.
In other words, the loss can be decomposed into the negative log probability of the teacher's predicted probability plus the entropy of the student. 

RKL does not suffer from mode averaging because the student distribution~$q_\theta$ goes to the numerator and does not have to cover the teacher distribution. Also, the entropy term in \eqref{eq:rkl} penalizes the student for learning a wide-spreading distribution, further mitigating the mode-averaging problem.

However, RKL distillation has the opposite problem, known as mode collapsing, where the student only learns one or a few modes of the teacher distribution. This is because the RKL loss would be large, if $q_\theta(\mathbf Y')$ is high but $p(\mathbf Y')$ is low for some $\mathbf Y'$. As a result, the student tends to overly concentrate its probability mass on certain high-probability regions of the teacher model, which may not be ideal either (Figure~\ref{fig:modes}b).

RKL distillation is related to the ENGINE distilling approach~\citep{tu-etal-2020-engine}, which was originally designed to minimize the energy function defined by the teacher model.
In particular, the ENGINE objective approximates RKL less the student entropy: $J_{\text{ENGINE}} = \mathbb E_{\mathbf Y \sim q_\theta}[- \log p(\mathbf Y)]$.
Therefore, ENGINE also suffers from the mode-collapsing problem, resembling RKL distillation.

\bigskip\textbf{Remarks.} KL and RKL have the mode-averaging or mode-collapsing problem, because $D_{\text{KL}}(\cdot\|\cdot)$ is asymmetric in its two arguments, requiring the second distribution to cover the support of the first.
In the following, we will propose two \textsc{$f$-distill} variants based on symmetric divergence functions to seek a balance between these two extremes.


\bigskip\textbf{Jenson--Shannon (JS) distillation.}
Our proposed JS distillation minimizes the JS divergence, which measures the difference between two distributions and their average.
We derive the step-wise decomposition of the sequence-level JS loss:
\begin{align}
    &J_{\text{JS}} = \frac12 \E_{\mathbf Y \sim p} \left[ \log \tfrac{p(\mathbf Y)}{m(\mathbf Y)} \right] \nonumber + \frac12 \E_{\mathbf Y' \sim q_\theta} \left[ \log \tfrac{q_\theta (\mathbf Y')}{m(\mathbf Y')} \right] \nonumber \\
    &\approx \frac12 \sum_{t=1}^{|\mathbf y|} \sum_{\mathrm Y_t \in V} - p(\mathrm Y_t| \mathbf y_{<t}) \log ( m(\mathrm Y_t| \mathbf y_{<t}) ) \nonumber \\
    &+ \frac12 \sum_{t=1}^{|\mathbf y'|} \sum_{\mathrm Y'_t \in V} \left[q_\theta(\mathrm Y'_t| \mathbf y'_{<t}) \log(q_\theta(\mathrm Y'_t| \mathbf y'_{<t}) \right. \nonumber \\
    &\left. -q_\theta(\mathrm Y'_t| \mathbf y'_{<t}) \log ( m(\mathrm Y'_t| \mathbf y'_{<t}) ) \right] + \text{const}\label{eq:JS-main}
\end{align}
where $\mathbf y$ and $\mathbf y'$ are sampled from the teacher's and student's distributions, which are compared with their average $m(\cdot)=\frac12 p(\cdot) + \frac12q_\theta(\cdot)$. Appendix~\ref{apdx:proof} provides the proof of this decomposition, and Subsection~\ref{subsec:imple-consid} presents an efficient approximation by avoiding on-the-fly sampling from the teacher.

\textbf{Total variation distance (TVD) distillation.}
Our \textsc{$f$-distill} gives rise to another novel distilling variant based on the total variation distance
\begin{align}
    J_{\text{TVD}} = \frac12 \sum_{\mathbf Y} |q_\theta(\mathbf Y) - p(\mathbf Y)|
\end{align}
Unlike JS divergence, TVD measures the $\ell^1$ norm between two distributions, and therefore does not have the $\log$ operator, making the gradient more stable than JS distillation.

We would like to decompose the sequence-level TVD step by step due to the intractable summation over the sentence space. However, TVD decomposition is non-trivial, and we show in Appendix~\ref{apdx:proof} that the sequence-level TVD is upper bounded by step-wise terms, being our objective to minimize:

\begin{align} 
    & J_{\text{TVD}} = \frac12 \sum_{\mathbf Y} |q_\theta(\mathbf Y) - p(\mathbf Y)| \nonumber \\ 
    & \scalemath{0.95} {\le \frac14 \E_{\mathbf Y \sim p} \Bigg[ \sum_{t=1}^{|\mathbf Y|} \sum_{\mathrm Y_t \in V} |q_\theta(\mathrm Y_t|\mathbf{Y}_{<t}) - p(\mathrm Y_t|\mathbf{Y}_{<t})| \Bigg] } \nonumber \\
    & \scalemath{0.95} { + \frac14 \E_{\mathbf{Y}' \sim q_\theta} \Bigg[ \sum_{t=1}^{|\mathbf Y'|} \sum_{\mathrm Y_t' \in V} |q_\theta(\mathrm Y_t'|\mathbf{Y}'_{<t}) - p(\mathrm Y_t'|\mathbf{Y}'_{<t})| \Bigg] } \nonumber \\
    & \approx \frac14 \sum_{t=1}^{|\mathbf y|} \sum_{\mathrm Y_t \in V} |q_\theta(\mathrm Y_t|\mathbf y_{<t}) - p(\mathrm Y_t|\mathbf y_{<t})| \nonumber \\
    & + \frac14 \sum_{t=1}^{|\mathbf y'|} \sum_{\mathrm Y'_t \in V} |q_\theta(\mathrm Y'_t|\mathbf{y}'_{<t}) - p(\mathrm Y'_t|\mathbf{y}'_{<t})| \label{eq:TVD}
\end{align}
where ${\mathbf y}$ and ${\mathbf y}'$ are again sampled from the teacher and student models, respectively.

\bigskip
\textbf{Summary.} In this part, we have described our proposed \textsc{$f$-distill} framework with four variants based on different $f$-divergence functions. We have also presented their step-wise decompositions, whose justification is summarized by the following theorem, proved in Appendix~\ref{apdx:proof}.

\begin{restatable}{thm}{decomposition}
(a) The sequence-level KL, RKL, and JS divergences can be decomposed exactly into step-wise terms. (b) The sequence-level TVD can be upper bounded by step-wise terms.
\label{thm:decomposition}
\end{restatable}

\subsection{Implementation Considerations} \label{subsec:imple-consid}
\textbf{Efficient approximation.}
Symmetric distilling losses (i.e., JS and TVD) are slow to compute, because they require sampling from both teacher and student models during training.

We propose to mitigate this by offline sampling for the teacher model to improve training efficiency. Specifically, we obtain teacher samples, i.e., $\mathbf y$ in Eqns.~\eqref{eq:JS-main} and~\eqref{eq:TVD}, beforehand and keep them fixed during training.
This is feasible because the teacher model is unchanged and hence does not require multiple inferences, whereas the student model is continuously updated and thus requires inference in an online fashion.
Experiments show that such a treatment  significantly improves the training efficiency for both JS and TVD distillations.

\textbf{Pre-distillation.}
We warm-start our student model with the techniques developed by~\citet{shleifer2020pre}, who combine MLE training, word-level KL, and hidden state matching. Such a pre-distilling process is crucial to our \textsc{$f$-distill} method, because most variants (namely, RKL, JS, and TVD distillations) require sampling from a student, but a randomly initialized student model generates poor samples, making the distilling process less meaningful.

Notice that, for a fair comparison, all baseline models are built upon the same pre-distilling process. This further confirms that our \textsc{$f$-distill} is compatible with existing techniques and yields add-on performance gain (shown in Section~\ref{subsection:results}).

\section{Experiments}

\begin{table*}[!t]

\begin{subtable}[t]{\textwidth}
\centering
\resizebox{0.768\textwidth}{!}{
\begin{tabular}{|ll|cccccc|}
\hline
\multicolumn{2}{|l|}{\multirow{2}{*}{Model}} & \multicolumn{6}{c|}{DART}                      \\ \cline{3-8} 
\multicolumn{2}{|l|}{} &
  BLEU4$^\uparrow$ &
  METEOR$^\uparrow$ &
  TER$^\downarrow$ &
  BERTScore$^\uparrow$ &
  MoverScore$^\uparrow$ &
  BLEURT$^\uparrow$ \\ \hline
\multicolumn{2}{|l|}{Teacher}                & 48.56 & 39.28 & 45.45 & 83.04 & 68.17 & 40.56 \\ \hline
\multicolumn{1}{|l|}{\multirow{8}{*}{Student}} &
  Non-distill (MLE) &
  43.12 &
  35.71 &
  49.97 &
  79.76 &
  65.65 &
  29.10 \\
\multicolumn{1}{|l|}{}     & Pre-distill     & 45.60 & 36.99 & 47.10 & 81.39 & 66.75 & 34.08 \\ \cline{2-8} 
\multicolumn{1}{|l|}{}     & SeqKD           & 45.54 & 37.17 & 47.49 & 81.15 & 66.65 & 32.88 \\
\multicolumn{1}{|l|}{}     & ENGINE          & 44.40 & 36.51 & 50.63 & 80.18 & 66.20 & 30.94 \\ \cline{2-8} 
\multicolumn{1}{|l|}{}     & KL              & 46.24 & 37.45 & 46.89 & 81.60 & 67.07 & 35.31 \\
\multicolumn{1}{|l|}{}     & RKL             & 45.63 & 37.35 & 47.91 & 81.41 & 67.02 & 35.08 \\
\multicolumn{1}{|l|}{} &
  JS &
  \underline{46.85} &
  \underline{37.75} &
  \underline{46.50} &
  \underline{81.93} &
  \underline{67.30} &
  \underline{36.81} \\
\multicolumn{1}{|l|}{} &
  TVD &
  \textbf{46.95} &
  \textbf{37.88} &
  \textbf{46.35} &
  \textbf{82.08} &
  \textbf{67.36} &
  \textbf{37.17} \\ \hline
\end{tabular}
}
\label{tab:dart-xsum}
\end{subtable}

\vspace{.05cm}
\begin{subtable}[t]{\textwidth}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|ll|ccc|ccc|ScS|}
\hline
\multicolumn{2}{|l|}{\multirow{2}{*}{Model}} &
  \multicolumn{3}{c|}{XSum} &
  \multicolumn{3}{c|}{WMT16 EN-RO} &
  \multicolumn{3}{c|}{Commonsense Dialogue} \\ \cline{3-11} 
\multicolumn{2}{|l|}{} &
  \multicolumn{1}{c}{ROUGE-1$^\uparrow$} &
  \multicolumn{1}{c}{ROUGE-2$^\uparrow$} &
  \multicolumn{1}{c|}{ROUGE-L$^\uparrow$} &
  \multicolumn{1}{c}{BLEU4$^\uparrow$} &
  \multicolumn{1}{c}{chrF$^\uparrow$} &
  \multicolumn{1}{c|}{TER$^\downarrow$} &
  \multicolumn{1}{c}{BLEU1$^\uparrow$} &
  \multicolumn{1}{c}{BLEU2$^\uparrow$} &
  \multicolumn{1}{c|}{BERTScore$^\uparrow$} \\ \hline
\multicolumn{2}{|l|}{Teacher} &
  45.12 &
  22.26 &
  37.18 &
  25.82 &
  55.76 &
  60.57 &
  11.67 &
   5.03 &
  47.69 \\ \hline
\multicolumn{1}{|l|}{\multirow{8}{*}{Student}} &
  Non-distill (MLE) &
  30.00 &
  10.67 &
  24.40 &
  19.90 &
  49.79 &
  69.48 &
  10.23 &
   3.56 &
  45.15 \\
\multicolumn{1}{|l|}{} &
  Pre-distill &
  40.58 &
  17.79 &
  32.55 &
  20.68 &
  50.51 &
  68.38 &
   9.95 &
   3.63 &
  46.22 \\ \cline{2-11} 
\multicolumn{1}{|l|}{} &
  SeqKD &
  39.13 &
  17.53 &
  32.34 &
  21.20 &
  50.81 &
  67.66 &
  10.85 &
  4.17  &
  46.94 \\
\multicolumn{1}{|l|}{} &
  ENGINE &
  39.19 &
  16.18 &
  31.23 &
  17.65 &
  48.37 &
  84.02 &
  10.13 &
  4.26  &
  46.91 \\ \cline{2-11} 
\multicolumn{1}{|l|}{} &
  KL &
  41.28 &
  18.98 &
  33.71 &
  21.45 &
  51.12 &
  \textbf{66.74} &
  9.81 &
  3.52 &
  45.80 \\
\multicolumn{1}{|l|}{} &
  RKL &
  \underline{41.69} &
  19.02 &
  33.92 &
  20.46 &
  50.33 &
  70.78 &
  10.48 &
  4.01  &
  46.68 \\
\multicolumn{1}{|c|}{} &
  JS &
  41.65 &
  \underline{19.22} &
  \underline{34.03} &
  \textbf{21.91} &
  \textbf{51.5} &
  \underline{66.86} &
  \textbf{11.55} &
  \textbf{4.83} &
  \textbf{47.61} \\
\multicolumn{1}{|l|}{} &
  TVD &
  \textbf{41.76} &
  \textbf{19.30} &
  \textbf{34.10} &
  \underline{21.73} &
  \underline{51.13} &
  66.94 &
  \underline{11.39} &
  \underline{4.73} &
  \underline{47.30} \\ \hline
\end{tabular}
}
\label{tab:wmt-cd}
\end{subtable}
\caption{Main results on the DART, XSum, WMT16 EN-RO, and Commonsense Dialogue (CD) datasets.
The best student result is in \textbf{bold} and the second best is \underline{underlined}.
$^{\uparrow/\downarrow}$The higher/lower, the better.
}
\label{tab:exp-rslt}
\end{table*}


\subsection{Settings}

\textbf{Datasets and metrics.} We evaluated \textsc{$f$-distill} on a wide range of text generation tasks.

\underline{$\bullet$ {DART.}} The DART dataset~\citep{nan-etal-2021-dart} is a popular data-to-text generation benchmark, where samples consist of structured data records and their corresponding text descriptions.
We report common string-matching metrics, BLEU~\citep{papineni-etal-2002-bleu}, METEOR~\citep{banerjee-lavie-2005-meteor}, and TER~\citep{snover-etal-2006-study}, as well as popular learned metrics, BERTScore~\citep{zhang2019bertscore}, MoverScore~\citep{zhao-etal-2019-moverscore}, and BLEURT~\citep{sellam-etal-2020-bleurt}.

\underline{$\bullet$ {XSum.}} Extreme Summarization~\cite[XSum,][]{narayan-etal-2018-xsum} is a large-scale dataset consisting of BBC articles and their one-sentence summaries.
We report ROUGE scores, the most widely used metrics for summarization~\citep{lin-2004-rouge}.

\underline{$\bullet$ WMT16 EN-RO.} This dataset contains parallel texts for English and Romanian, and is one of the commonly used machine translation datasets~\citep{bojar-etal-2016-wmt16}. We extracted 100K samples from the original dataset, as the teacher performance is nearly saturated at this size. We report BLEU, chrF~\citep{popovic-2015-chrf}, and TER scores for the translation quality, following existing machine translation literature~\citep{sennrich-etal-2016-neural,barrault-etal-2019-findings}.

\underline{$\bullet$ Commonsense Dialogue.} The Commonsense Dialogue dataset~\citep{zhou-etal-2021-commonsense} consists of dialogue sessions that are grounded on social contexts. We evaluated the output quality by BLEU and BERTScore. We only report BLEU1 and BLEU2, as higher-order BLEU scores are known to be unreliable for dialogue evaluation~\citep{liu-etal-2016-evaluate}.

\textbf{Model architectures.}
We evaluated \textsc{$f$-distill} using state-of-the-art teacher models for different tasks. 
We followed the encoder--decoder architecture and used BART~\citep{lewis-etal-2020-bart} as the teacher for DART and XSum.
We used T5~\citep{t52020}, another encoder--decoder model, for WMT16 EN-RO, as it excels at machine translation.
For Commonsense Dialogue, we followed~\newcite{zhang-etal-2020-dialogpt} and used DialoGPT, a decoder-only model pretrained on massive dialogue data.

Our student models followed the teachers' architectures, but we reduced the number of layers. In our experiments, we generally set the total number of layers to be four; specifically, encoder--decoder models had three encoder layers and one decoder layer, following the suggestion of deep encoders and shallow decoders in \citet{kasai2020deep}. For XSum, we set both the encoder and decoder to be three layers to compensate for the larger dataset. Additional experimental details can be found in Appendix~\ref{apdx:exp-details}.

\subsection{Results and Analyses} \label{subsection:results}

\begin{table*}[!t]
\centering
\resizebox{0.60\textwidth}{!}{
\begin{tabular}{|l|ll|ll|ll|ll|}
\hline
Dataset &
  \multicolumn{2}{c|}{DART} &
  \multicolumn{2}{c|}{XSum} &
  \multicolumn{2}{c|}{MT$_\text{EN-RO}$} &
  \multicolumn{2}{c|}{CD} \\ \hline
TeacherDist &
  \multicolumn{2}{c|}{26.10} &
  \multicolumn{2}{c|}{36.28} &
  \multicolumn{2}{c|}{23.13} &
  \multicolumn{2}{c|}{81.19} \\\hline\hline
Risk &
  $R_\text{llh}$ &
  $R_\text{cvg}$ &
  $R_\text{llh}$ &
  $R_\text{cvg}$ &
  $R_\text{llh}$ &
  $R_\text{cvg}$ &
  $R_\text{llh}$ &
  $R_\text{cvg}$ \\ \hline
KL  & 0.56 & 0.49 & 1.89 & 1.68 & 1.23 & 0.82 & 0.43 & 0.26 \\
RKL & 0.58 & 0.59 & 1.88 & 1.83 & 1.20 & 1.60 & 0.29 & 0.35 \\
TVD & 0.53 & 0.52 & 1.86 & 1.77 & 1.21 & 1.78 & 0.27 & 0.35 \\
JS  & 0.51 & 0.48 & 1.88 & 1.75 & 1.13 & 1.34 & 0.30 & 0.33 \\ \hline
\end{tabular}
}\vspace*{-0.2cm}
\caption{The likelihood risk $R_\text{llh}$ and the coverage risk $R_\text{cvg}$ for different \textsc{$f$-distill} variants. A lower number indicates a higher likelihood or better coverage. We show the teacher diversity for each task by distinct bi-gram percentage~\citep{li-etal-2016-dist} among five teacher-sampled outputs given a test input, which indicates the severity of multi-modality of a task.
}\vspace*{-0.2cm}
\label{tab:mutual-nll}
\end{table*}


\textbf{Main results.}
 Table~\ref{tab:exp-rslt} presents the main results of our \fdistill\ along with a number of competing methods in the four experiments.
 
We first trained a neural network without distillation. The network was identical to our student model in terms of the neural architecture and hyperparameters, but we trained it directly by maximum likelihood estimation (MLE) based on ground-truth target sequences. 
As seen, the non-distilling model performs significantly worse than distilling methods, which agrees with existing literature and justifies the need for knowledge distillation~\citep{hinton2015distilling,tang2019distilling,jiao-etal-2020-tinybert}.

We pre-distilled our student model based on~\citet{shleifer2020pre}, a classic distilling approach that combines ground-truth training, word-level distillation, and intermediate-layer matching. Our \fdistill\ approach requires pre-distillation, because it provides a meaningful initialization of the student model, from which our \fdistill\ would generate samples during training. That being said, all our distilling methods were built on the same pre-distilling model, constituting a fair comparison. The results show that, although the pre-distilling approach outperforms ground-truth MLE training, it is generally worse than other distilling methods. This implies that our contribution is ``orthogonal'' to existing methods, and that our \fdistill\ provides an add-on performance improvement.

We further experimented with SeqKD~\citep{kim-rush-2016-sequence} and ENGINE~\citep{tu-etal-2020-engine}, two established distilling methods in the distribution-matching category (see Section~\ref{sec:intro}). They learn from hard sequences rather than probabilities, and thus are hard approximations of our KL and RKL distillations, respectively (Section~\ref{subsec:classic-kd}). 
As seen, our soft label-based methods consistently outperform SeqKD and ENGINE. 
This suggests that soft labels (i.e., probabilities) provide more informative supervision signals than hard sentences for sequence-level distillation, which is consistent with early literature on classification tasks~\citep{bucilua2006model,hinton2015distilling}. 

Among our \fdistill{} variants, we further observe that symmetric distilling losses (JS and TVD) are consistently better than asymmetric ones (KL and RKL) across all datasets except for WMT16 EN-RO, where KL achieves a slightly better TER performance.
A plausible reason is that the machine translation task is semantically grounded: given a source text, there are limited ways to translate, because the model output has to preserve the meaning of the input sentence. This is analogous to learning a uni-modal distribution, where mode averaging does not occur because there is only one mode. Despite this, JS and TVD perform better in all other scenarios, as their symmetric divergence can force the student to better learn from its teacher distribution. They rank first or second for all tasks in terms of most of the metrics in Table~\ref{tab:exp-rslt}, consistently and largely outperforming previous methods.

\textbf{Likelihood and coverage.}
We further analyze the mode averaging and collapsing behaviors of different distilling methods in Table~\ref{tab:mutual-nll}.
We propose to measure these aspects by a likelihood risk $R_\text{llh}$ and a coverage risk $R_\text{cvg}$.

The \textit{likelihood risk} is computed by
$R_\text{llh} = \frac{1}{|\mathcal D_\text{student}|}\sum\nolimits_{{\mathbf y}'\in \mathcal D_\text{student}} - \log p({\mathbf y}')$. 
Here,  $\mathcal D_\text{student}$ is the set of sentences generated from the student, where we sample a sentence for each input in the test set; $p({\mathbf y}')$ is the teacher's predicted probability of a student-sampled sentence ${\mathbf y}'$.
A large likelihood risk suggests that the student may have averaged the teacher's modes, causing it to generate atypical sentences from the teacher's point of view~(Figure~\ref{fig:modes}a).

On the contrary, the \textit{coverage risk} is computed by
$R_\text{cvg} = \frac{1}{|\mathcal D_\text{teacher}|}\sum\nolimits_{{\mathbf y}\in \mathcal D_\text{teacher}} - \log q_\theta({\mathbf y})
$, 
where we use the student $q_\theta$ to evaluate a teacher-sampled sentence $\mathbf y\in \mathcal D_\text{teacher}$. This measures whether the teacher's samples are typical from the student's point of view, i.e., how well a student covers the support of the teacher's distribution.
A large coverage risk means that the teacher's typical outputs are not captured by the student, which is an indicator of mode collapse (Figure~\ref{fig:modes}b).

In addition, we notice that mode averaging and collapsing are significantly affected by how ``multi-modal'' a task is.
We propose to measure this by the distinct bi-gram percentage~\citep{li-etal-2016-dist} of the teacher model (denoted by TeacherDist): for each test input, we sampled five outputs from the teacher and computed the percentage of distinct bi-grams, which is then averaged across the test set. As seen in Table~\ref{tab:mutual-nll}, the dialogue task exhibits the highest diversity, i.e., it is the most multi-modal, whereas machine translation is the least multi-modal.

Comparing KL and RKL, we find that KL distillation consistently achieves lower $ R_\text{cvg}$ risks (i.e., better coverage) than RKL across all datasets.
This confirms that KL distillation yields a smooth student distribution that covers the teacher's, whereas RKL distillation does not have the covering property due to its mode-collapsing nature.

We further observe that RKL achieves significantly higher likelihood (given by a lower $R_\text{llh}$) on the Commonsense Dialogue dataset.
This shows that the mode-collapsing phenomenon of RKL distillation allows the student to generate plausible responses for the one-to-many dialogue task~(Figure~\ref{fig:modes}b), whereas the mode-averaging KL distillation puts the student in some desolate area in the teacher's distribution (Figure~\ref{fig:modes}a).
On the other hand, RKL does not achieve lower likelihood risks in other tasks, since their one-to-many phenomenon is not as severe as dialogue generation~\citep{wei2019neural, bao-etal-2020-plato, wen2022equal}.

Referring back to Table~\ref{tab:exp-rslt}, we see that mode-averaging KL distillation is preferred over RKL for less multi-modal tasks, such as machine translation (which has a low TeacherDist score), whereas mode-collapsing RKL is preferred for highly multi-modal tasks, such as dialogue generation (which has a higher TeacherDist score).

Last, our symmetric distilling objectives (JS and TVD) generally have moderate likelihood and coverage risks between the two extremes.
This shows that they achieve a compromise between mode collapsing and averaging, allowing them to yield high performance in all tasks (Table~\ref{tab:exp-rslt}).

\textbf{Analysis of the student size.} We analyze our \textsc{$f$-distill} variants with different student sizes in comparison with the SeqKD model. Due to the limited time and resources, we chose the DART dataset as our testbed. We reduced the student model to different sizes by changing the number of encoder layers, as we had already used a single-layer decoder following the suggested architecture in~\citet{kasai2020deep}. Results are shown in Figure~\ref{fig:student-scaling}.

As seen, our \fdistill{} outperforms SeqKD across all model sizes.
The symmetric losses (JS and TVD) also consistently outperform the asymmetric ones (KL and RKL).
This is consistent with our main results and further validates the effectiveness and robustness of our \textsc{$f$-distill} framework.

% Figure environment removed
\begin{table}[!t]
\bigskip
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{|l|ccc|}
\hline
Model         & BLEU4 & BERTScore & Speedup \\ \hline\hline
\multicolumn{4}{|c|}{JS distillation} \\\hline
Online            & 46.85 & 82.02  & 1.00x   \\
Offline (our method)  & 46.85 & 81.93  & 2.25x   \\ \hline\hline
\multicolumn{4}{|c|}{TVD distillation} \\\hline
Online           & 46.57 & 82.03  & 1.00x   \\
Offline (our method) & 46.95 & 82.08  & 2.31x   \\ \hline
\end{tabular}
}
\caption{Training efficiency on the DART dataset. \textbf{Online:} We re-sample sequences from the teacher model in every epoch. \textbf{Offline:} The teacher's samples are obtained beforehand and fixed during training. Note that we always re-sample from the student model because the student is constantly being updated.
}
\label{tab:efficiency}\vspace*{-0.2cm}
\end{table}
\textbf{Analysis of training efficiency.} Our \fdistill\ involves sampling sequences from the teacher. We propose an offline approach that obtains the teacher's samples before training.
We analyze the efficiency of offline sampling for JS and TVD distillations by comparing them with their online counterparts.
We ran this experiment on an NVidia RTX A6000 GPU and an Intel Xeon Gold 5317 CPU.\footnote{To obtain a rigorous time estimate, we ran efficiency analysis on an unshared, consumer-grade server, whereas other experiments were run on clusters (Appendix~\ref{apdx:exp-details}).}

As seen in Table~\ref{tab:efficiency}, the offline variant achieves comparable performance, while the training speed is more than doubled. This is expected, as the offline distilling methods do not require inference from the teacher model during training, which constitutes a significant portion of the training process.
This shows that our symmetric distilling methods can achieve high performance without the need for sampling from both the teacher and student.

\textbf{Human Evaluation.}
We further validated \fdistill\ by human evaluation, where models were rated by fluency, missing information, and hallucination between 1 to 5 on the DART dataset, following previous work~\citep{nan-etal-2021-dart,keymanesh-etal-2022-makes}.
We invited five human annotators to evaluate 50 test samples for four competing models: SeqKD, ENGINE, JS, and TVD.
For each test sample, the annotators were presented with shuffled model outputs, so they could not tell which output was generated by which model.
Results are shown in Table~\ref{tab:he}.

As seen, our \fdistill\ enables students to capture the input data records more faithfully while also retaining a high level of fluency. This is additionally supported by the $p$-values: comparing SeqKD and TVD, there is no statistically significant difference in terms of fluency ($p$-value=32.6\%); however, the improvements for missing information ($p$-value=1.28\%) and hallucination ($p$-value=0.669\%) are statistically significant.
Our human evaluation confirms the effectiveness of \fdistill.

\begin{table}[]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{|l|ccc|}
\hline
Model & Fluency$^\uparrow$ & MissingInfo$^\downarrow$ & Hallucination$^\downarrow$ \\ \hline
SeqKD  & \textbf{4.75} & 1.77        & 1.67 \\
ENGINE & 4.51 & 1.76        & 1.61 \\ \hline
JS     & \underline{4.72} & \underline{1.70}     & \underline{1.48} \\
TVD    & \underline{4.72} & \textbf{1.57}        & \textbf{1.45} \\ \hline
\end{tabular}
}
\caption{Human evaluation on the DART dataset. Comparing SeqKD and TVD, the one-sided Student's $t$-test gives $p$-values of  32.6\%, 1.28\%, and 0.669\% for fluency, missing information, and hallucination, respectively.}
\label{tab:he}
\end{table}

\textbf{Case Study.}
Appendix~\ref{apdx:case-study} shows example outputs for our \fdistill\ variants. Indeed, we observe KL distillation yields short and generic utterances that are believed to be an indicator of mode averaging~\cite{wei2019neural,bao-etal-2020-plato}. Our symmetric losses (JS and TVD) are able to generate more meaningful, fluent, and coherent sentences.

\section{Related Work}

Knowledge distillation (KD) is pioneered by \citet{bucilua2006model}, who use an ensemble model as the teacher to train a single-model student by minimizing the squared difference between their predicted logits.
\citet{hinton2015distilling} propose to directly learn from the output probabilities by minimizing their KL divergence.
\citet{sun2019pkd} propose patient knowledge distillation (PKD), which requires the student to learn from the teacher's intermediate layers.
\citet{jiao-etal-2020-tinybert} propose TinyBERT, extending knowledge distillation for Transformer models by additional treatments on the attention layers.
Other recent distilling methods include finding the optimal layer mapping between two models~\citep{li2020bertEMD, jiao2021improving} and learning from multiple teachers~\citep{multi-teacher-3,wu2021one,li-etal-2022-unsupervised-multiple}.

The success of KD has since sparked significant interest in its applications to text generation.
\citet{kim-rush-2016-sequence} investigate sequence-level knowledge distillation (SeqKD) for neural machine translation, where they use sampled, hard sequences to approximate the KL divergence.  \citet{tu-etal-2020-engine} train a student model by minimizing the energy function defined by a teacher model, which we show is an approximation to reverse KL distillation.
\citet{lin-etal-2020-autoregressive} propose imitation-based KD, where the teacher provides oracle probabilities on student-sampled partial sequences to address the exposure bias problem.
Further, KD has been extensively used to train non-autoregressive text generation models to reduce the complexity of the training data~\citep{gu2018non,shao-etal-2022-one,Huang_Zhou_Zaïane_Mou_Li_2022}.

It is noted that our \textsc{$f$-distill} requires meaningful student sampling and thus is built upon existing KD techniques~\citep{shleifer2020pre}, including word-level and intermediate-layer KD. Nevertheless, it shows that our approach achieves an add-on performance improvement, and that our contributions are orthogonal to previous work. 

Besides KD, common model compression techniques include parameter pruning and sparse modeling.
Parameter pruning first trains a dense network and then removes certain neural weights in hopes of not significantly affecting the model performance~\citep{classic-pruning,liu-etal-2018-efficient,fan-etal-2021-layer}.
Alternatively, one may apply sparse modeling techniques such as regularization during the training process to ensure zero-valued parameters~\citep{frankle2018lottery,louizos2018learning,Tang_Zhao_Wang_Luo_Xie_Zeng_2022}.
Our work does not follow these directions, as we consider the knowledge distilling setting.

Regarding the $f$-divergence function, it has many applications in the machine learning literature.
The standard cross-entropy training is equivalent to minimizing the KL divergence between the ground-truth label distribution (often one-hot) and model distribution~\cite{bishop2006pattern}.
Generative adversarial networks~\citep{goodfellow2014GAN} minimize the Jensen--Shannon divergence by simultaneously training a generator and a discriminator against each other.
\citet{zhao2020rkl} minimize  $\alpha$-divergence for adversarial learning, which generalizes KL and RKL,  and is a special case of $f$-divergence functions.
\citet{pmlr-v139-zhang21n} use total variation distance as a regularizer to encourage the model to predict more distinguishable probabilities.
Further, JSD is used in computer vision KD~\cite{Yin_2020_CVPR,NEURIPS2021_63dc7ed1}, but their tasks do not involve sequential data and the underlying techniques largely differ from our approach.
To the best of our knowledge, we are the first to systematically formulate sequence-level knowledge distillation as $f$-divergence minimization.

\section{Conclusion}

We propose \textsc{$f$-distill}, a family of sequence-level distilling methods beyond minimizing the KL divergence.
Under our framework, we propose and analyze four variants: KL, RKL, JS, and TVD distillations, where existing SeqKD and ENGINE are approximations of KL and RKL  variants; we further derive step-wise decomposition for our \textsc{$f$-distill}.
Results on four text generation tasks show \textsc{$f$-distill} consistently outperforms existing KD methods, and that our symmetric losses (JS and TVD) outperform asymmetric ones by avoiding extreme mode averaging and collapsing.

\section{Limitations}

Our \textsc{$f$-distill} variants are less efficient to train than SeqKD and ENGINE, as we require the teacher's soft probabilities instead of hard, sampled sequences.
However, our methods achieve a significant performance improvement, and more importantly, the additional training time does not affect inference when the model is deployed. This follows the spirit of knowledge distillation in general, i.e., to obtain a small and efficient model for deployment.

Another potential threat to validity is that we have not reported multi-run statistics. In our preliminary experiments, we ran our approach multiple times and found results were generally consistent. Due to our excessive experimentation (estimated at 2000 GPU hours), it is not possible to run each model multiple times. 
 We instead adopted a wide range of established automatic metrics, consistently showing the effectiveness of our approach. We further conducted in-depth analyses to better understand our proposed framework. 
We deem multi-run statistics not crucial to this paper, as this paper does not purely focus on empirical analysis. Rather, our main contributions lie in the novel machine learning framework,  \fdistill, and the theoretical connections between step-wise and sequence-level $f$-divergence functions.

Finally, a limitation of the formally published paper in the proceedings of ACL'23 is that we could not foresee the discussions during the conference, which are highlighted in the next section of this arXiv manuscript.

\section{Notes from ACL'23 Conference}
During the ACL conference, we received a number of questions and feedback, based on which we would like to make two clarifications.

1) Step-wise decomposition for JS distillation requires the conditional of the middle distribution $m(\mathrm Y_t | \mathbf Y_{1:{t-1}})$. Although it is tempting to have 
\begin{align} m(\mathrm Y_t | \mathbf Y_{1:{t-1}}) = \frac12 p(\mathrm Y_t | \mathbf Y_{1:{t-1}}) + \frac12 q_\theta(\mathrm Y_t | \mathbf Y_{1:{t-1}})\label{eq:wrong}
\end{align}
the correct formula should be 
\begin{align}m(\mathrm Y_t | \mathbf Y_{1:{t-1}}) = \frac{m(\mathbf Y_{1:t})}{ m(\mathbf Y_{1:t-1})}\label{eq:correct}
\end{align}
which is not the same as \eqref{eq:wrong}. We realize that we made the above mistake in our implementation, which nevertheless works empirically well and can be thought of as an approximation. It is also noted that calculating \eqref{eq:correct} requires storing the probabilities for each step and would be less efficient.

2) Certain \fdistill\ variants (RKL, JS, and TVD) involve sampling from the student.  Although we have derived step-wise decompositions in Eqns.~\eqref{eq:rkl}, \eqref{eq:JS-main}, and \eqref{eq:TVD}, directly applying backpropogation to them is unable to give the partial gradient with respect to the sampling parameters. In other words, a stop-gradient operation is performed for $q_\theta$ under the expectation~$\mathbb E[\cdot]$. 
A direct optimization requires reinforcement learning, which is known to be unstable for text generation~\citep{Yu_Zhang_Wang_Yu_2017} and is not considered in our approach.
On the other hand, our method optimizes the parameters greedily step by step, which nevertheless pushes the student distribution towards the teacher distribution: a loss of zero is achieved when the student and teacher are the same, no matter whether we propagate the gradient to the sampling distribution.
In this way, we are able to achieve meaningful results, while circumventing the difficulties of RL.

\section*{Acknowledgments}

We thank Wai Hong Ong for discussing the technical details of $m(\cdot)$ in JS distillation, and Lucas Torroba Hennigen for discussing the connection between \fdistill\ and reinforcement learning.

We also thank all reviewers and chairs for their valuable comments. The research is supported in part by the Natural Sciences and Engineering Research Council of Canada (NSERC) under Grant No. RGPIN2020-04465, the Amii Fellow Program, the Canada CIFAR AI Chair Program, a UAHJIC project, a donation from DeepMind, and the Digital Research Alliance of Canada (alliancecan.ca).

\bibliography{custom}
\bibliographystyle{acl_natbib}
\newpage

\appendix

\onecolumn

\pagebreak

\section{Proof of Theorem~\ref{thm:decomposition}} \label{apdx:proof}

\decomposition*

\begin{proof}

\textbf{[Part (a)]} We first consider the JS decomposition. Let $p$ and $q_\theta$ be the predicted distribution for the teacher and student, respectively. Let $m(\mathbf Y) = \frac12 p(\mathbf Y) + \frac12 q_\theta(\mathbf Y)$ be their average. We claim that JS divergence between two length-$T$ sequence\footnote{In practice, $T$ can be thought of as the maximum length. Alternatively, we may consider varying-length sequences by a mixture of different values of $T$.} distributions can be decomposed step by step as
\begin{align}
    D_{\text{JS}}(p(\mathbf Y_{1:T})\|q_\theta(\mathbf Y_{1:T})) :=&\ \frac12 \E_{\mathbf Y_{1:T} \sim p} \left[ \log \frac{p(\mathbf Y_{1:T})}{m(\mathbf Y_{1:T})} \right]
    + \frac12 \E_{\mathbf Y_{1:T}' \sim q_\theta} \left[ \log \frac{q_\theta (\mathbf Y_{1:T}')}{m(\mathbf Y_{1:T}')} \right] \label{eq:JS-apdx-definition} \\ 
    =&\ \frac12  \sum_{t=1}^T \E_{\mathbf Y_{1:t-1} \sim p} \left[\sum_{\mathrm Y_t} p(\mathrm Y_t|\mathbf Y_{1:t-1}) \log \frac{p(\mathrm Y_t | \mathbf Y_{1:t-1})}{m(\mathrm Y_t | \mathbf Y_{1:t-1})}  \right] \nonumber \\
    +&\ \frac12 \sum_{t=1}^T \E_{\mathbf Y'_{1:t-1}\sim q_\theta} \left[ \sum_{\mathrm Y_t'} q_\theta(\mathrm Y_t' | \mathbf Y'_{1:t-1}) \log \frac{q_\theta(\mathrm Y'_t | \mathbf Y'_{1t-1})}{m(\mathrm Y'_t | \mathbf Y'_{1:t-1})} \right] \label{eq:JS-apdx-main}
\end{align}
For implementation, we use Monte Carlo (MC) sampling to approximate $\mathbb E_{\mathbf Y_{1:t-1}\sim p}[\cdot]$ and $\mathbb E_{\mathbf Y'_{1:t-1}\sim q_\theta}[\cdot]$, suggested by Eqn.~\eqref{eq:JS-main}. Then, we explicitly enumerate all $\mathrm Y_t$ and $\mathrm Y'_t$, because a summation over all sequences is not tractable but a step-by-step summation over words is tractable. Compared with a direct MC approximation for~\eqref{eq:JS-apdx-definition}, such step-wise decomposition allows us to propagate gradient into all the words (denoted by $\mathrm Y_t$ for the teacher and $\mathrm Y'_t$ for the student) for every step $t$.

In fact, the partially sampled sequences are reused for the summation over $t=1,\cdots, T$. That is to say, we will first sample the sequences $\mathbf y_{1:T-1}\sim p$ and $\mathbf y_{1:T-1}'\sim q_\theta$ and then compute the summation; thus, the complexity is linear rather than quadratic. 

To prove \eqref{eq:JS-apdx-main}, we first focus on the first term of \eqref{eq:JS-apdx-definition}:
\begin{align}\label{eq:js1}
    & \E_{\mathbf Y_{1:T} \sim p} \left[ \log \frac{p(\mathbf Y_{1:T})}{m(\mathbf Y_{1:T})} \right] \\ \label{eq:js2}
     =& \E_{\mathbf Y_{1:T}\sim p} \left[ \log \prod_{t=1}^T \frac{p(\mathrm Y_{t} | \mathbf Y_{1:t-1})}{m(\mathrm Y_{t} | \mathbf Y_{1:t-1})} \right] \\ \label{eq:js3}
    =& \E_{\mathbf Y_{1:T}\sim p} \Biggl[ \log \prod_{t=1}^{T-1} \frac{ p(\mathrm Y_{t} | \mathbf Y_{1:t-1})}{m(\mathrm Y_{t} | \mathbf Y_{1:t-1})} + \log \frac{p(\mathrm Y_{T} | \mathbf Y_{1:T-1})}{m(\mathrm Y_{T} | \mathbf Y_{1:T-1})} \Biggr]  \\ \label{eq:js4}
    =& \E_{\mathbf Y_{1:T}\sim p} \left[ \log \prod_{t=1}^{T-1} \frac{p(\mathrm Y_{t} | \mathbf Y_{1:t-1})}{m(\mathrm Y_{t} | \mathbf Y_{1:t-1})} \right] + \E_{\mathbf Y_{1:T}\sim p} \left[ \log \frac{p(\mathrm Y_{T} | \mathbf Y_{1:T-1})}{m(\mathrm Y_{T} | \mathbf Y_{1:T-1})} \right] \\ \label{eq:js5}
    =& \E_{\mathbf Y_{1:T-1} \sim p} \left[ \log \frac{p(\mathbf Y_{1:T-1})}{m(\mathbf Y_{1:T-1})} \right] + \E_{\mathbf Y_{1:T-1} \sim p} \left[\sum_{\mathrm Y_{T}} p(\mathrm Y_{T}|\mathbf Y_{1:T-1}) \log \frac{p(\mathrm Y_{T} | \mathbf Y_{1:T-1})}{m(\mathrm Y_{T} | \mathbf Y_{1:T-1})} \right]
\end{align}
where \eqref{eq:js2} decomposes $p(\bfY_{1:T})$ and $m(\bfY_{1:T})$; \eqref{eq:js3} and \eqref{eq:js4} split the $T$th step out. In \eqref{eq:js5}, the first term drops $\rmY_T$ because it does not occur in the expectation, and we rewrite the second term by making the summation over $\mathrm Y_t$ explicit in accordance with our sampling procedure. 

Then, we can unroll the first term of \eqref{eq:js5} recursively, resulting in
\begin{align}\label{eq:js6}
     \E_{\mathbf Y_{1:T} \sim p} \left[ \log \frac{p(\mathbf Y_{1:T})}{m(\mathbf Y_{1:T})} \right] = 
\sum_{t=1}^T \E_{\mathbf Y_{1:t-1} \sim p} \left[\sum_{\mathrm Y_t} p(\mathrm Y_t|\mathbf Y_{1:t-1}) \log \frac{p(\mathrm Y_t | \mathbf Y_{1:t-1})}{m(\mathrm Y_t | \mathbf Y_{1:t-1})}  \right]\end{align}

Likewise, the term $\E_{\mathbf Y_{1:T}' \sim q_\theta} \left[ \log \frac{q_\theta (\mathbf Y_{1:T}')}{m(\mathbf Y_{1:T}')} \right]$ in \eqref{eq:JS-apdx-definition} is treated in a similar fashion, concluding our proof for JS decomposition.

We state KL and RKL decompositions below. Their proofs are similar and thus omitted.

\begin{align}
    D_{\text{KL}}(p(\mathbf Y_{1:T})\|q_\theta(\mathbf Y_{1:T})) =& \sum_{t=1}^T \E_{\mathbf Y_{1:t-1} \sim p} \left[\sum_{\mathrm Y_t} p(\mathrm Y_t|\mathbf Y_{1:t-1}) \log \frac{p(\mathrm Y_t | \mathbf Y_{1:t-1})}{q_\theta(\mathrm Y_t | \mathbf Y_{1:t-1})}  \right] \\
    D_{\text{RKL}}(p(\mathbf Y_{1:T})\|q_\theta(\mathbf Y_{1:T})) =& \sum_{t=1}^T \E_{\mathbf Y'_{1:t-1}\sim q_\theta} \left[ \sum_{\mathrm Y_t'} q_\theta(\mathrm Y_t' | \mathbf Y_{1:t-1}') \log \frac{q_\theta(\mathrm Y'_t | \mathbf Y'_{1:t-1})}{p(\mathrm Y'_t | \mathbf Y'_{1:t-1})} \right] 
\end{align}

\bigskip
\textbf{[Part (b)]} This part shows that the same step-wise decomposition for TVD is an upper bound:
\begin{align}
D_{\text{TVD}}(p(\mathbf Y_{1:T}) \| q_\theta(\mathbf Y_{1:T})) :=& \frac12 \sum_{\mathbf Y_{1:T}} |q_\theta(\mathbf Y_{1:T}) - p(\mathbf Y_{1:T})| \\
\le& \frac12 \left[\rule{0em}{9mm}\right.
 \frac12 \sum_{t=1}^T \E_{\mathbf Y_{1:t-1} \sim p} \left[ \sum_{\mathrm Y_t} \Big| q_\theta (\mathrm Y_t | \mathbf Y_{1:t-1}) - p(\mathrm Y_t | \mathbf Y_{1:t-1}) \Big| \right] \\
 &+\frac12 \sum_{t=1}^T \E_{\mathbf Y'_{1:t-1} \sim q_\theta} \Bigg[ \sum_{\mathrm Y'_t} \Big| q_\theta (\mathrm Y'_t | \mathbf Y'_{1:t-1}) - p(\mathrm Y'_t | \mathbf Y'_{1:t-1}) \Big| \Bigg] \left.\rule{0em}{9mm}\right]
 \label{eq:thm-tvd}
\end{align}

We again start by re-writing the TVD loss in a recursive form

\begin{align}
    & D_{\text{TVD}}(p(\mathbf 
 Y_{1:T}) \| q_\theta(\mathbf Y_{1:T}))  = \frac12 \sum_{\mathbf Y_{1:T}} |q_\theta(\mathbf Y_{1:T}) - p(\mathbf Y_{1:T})| \label{eq:tvd1} \\
    & = \frac12 \sum_{\mathbf Y_{1:T-1}} \sum_{\mathrm Y_T} |q_\theta(\mathbf Y_{1:T-1}) q_\theta (\mathrm Y_T | \mathbf Y_{1:T-1}) - p(\mathbf Y_{1:T-1}) p(\mathrm Y_T | \mathbf Y_{1:T-1})| \label{eq:tvd2} \\
    & = \frac12 \sum_{\mathbf Y_{1:T-1}} \sum_{\mathrm Y_T} \frac{p(\mathbf Y_{1:T-1})}{p(\mathbf Y_{1:T-1})} |q_\theta(\mathbf Y_{1:T-1}) q_\theta (\mathrm Y_T | \mathbf Y_{1:T-1}) - p(\mathbf Y_{1:T-1}) p(\mathrm Y_T | \mathbf Y_{1:T-1})| \label{eq:tvd3} \\
    & = \frac12 \sum_{\mathbf Y_{1:T-1}} p(\mathbf Y_{1:T-1}) \sum_{\mathrm Y_T} \left| \frac{q_\theta(\mathbf Y_{1:T-1}) q_\theta (\mathrm Y_T | \mathbf Y_{1:T-1})}{p (\mathbf Y_{1:T-1})} - p(\mathrm Y_T | \mathbf Y_{1:T-1}) \right| \label{eq:tvd4} \\
    & = \scalemath{0.92} { \frac12 \E_{\mathbf Y_{1:T-1}\sim p} \left[ \sum_{\mathrm Y_T} \Big| \frac{q_\theta(\mathbf Y_{1:T-1}) q_\theta (\mathrm Y_T | \mathbf Y_{1:T-1})}{p (\mathbf Y_{1:T-1})} - q_\theta(\mathrm Y_T | \mathbf Y_{1:T-1}) + q_\theta(\mathrm Y_T | \mathbf Y_{1:T-1}) - p(\mathrm Y_T | \mathbf Y_{1:T-1}) \Big| \right] \label{eq:tvd5} } \\
    & = \scalemath{0.92} { \frac12 \E_{\mathbf Y_{1:T-1}\sim p} \left[ \sum_{\mathrm Y_T} \Big| \frac{q_\theta(\mathrm Y_T | \mathbf Y_{1:T-1})}{p(\mathbf Y_{1:T-1})} \big( q_\theta(\mathbf Y_{1:T-1}) - p(\mathbf Y_{1:T-1}) \big)  + q_\theta(\mathrm Y_T | \mathbf Y_{1:T-1}) - p(\mathrm Y_T | \mathbf Y_{1:T-1}) \Big| \right] \label{eq:tvd6} } \\
    &\leq \scalemath{0.92}{ \frac12 \E_{\mathbf Y_{1:T-1}\sim p} \left[ \sum_{\mathrm Y_T}\left( \Big| \frac{q_\theta(\mathrm Y_T | \mathbf Y_{1:T-1})}{p(\mathbf Y_{1:T-1})} \big( q_\theta(\mathbf Y_{1:T-1}) - p(\mathbf Y_{1:T-1}) \big) \Big| + \Big| q_\theta(\mathrm Y_T | \mathbf Y_{1:T-1}) - p(\mathrm Y_T | \mathbf Y_{1:T-1}) \Big| \right)\right] \label{eq:tvd7} } \\
    &= \frac12 \E_{\mathbf Y_{1:T-1} \sim p} \left[ \sum_{\mathrm Y_T} \left| \frac{q_\theta(\mathrm Y_T | \mathbf Y_{1:T-1})}{p(\mathbf Y_{1:T-1})} \big( q_\theta(\mathbf Y_{1:T-1}) - p(\mathbf Y_{1:T-1}) \big) \right| \right] + \nonumber \\
    & \quad\,\, \frac12 \E_{\mathbf Y_{1:T-1} \sim p} \left[ \sum_{\mathrm Y_T} \left| q_\theta(\mathrm Y_T | \mathbf Y_{1:T-1}) - p(\mathrm Y_T | \mathbf Y_{1:T-1}) \right|  \right] \label{eq:tvd8} \\
    &= \frac12 \E_{\mathbf Y_{1:T-1} \sim p} \left[ \left| \frac{1}{p(\mathbf Y_{1:T-1})} \big( q_\theta(\mathbf Y_{1:T-1}) - p(\mathbf Y_{1:T-1}) \big) \right| \right] + \nonumber \\
    & \quad\,\, \frac12 \E_{\mathbf Y_{1:T-1} \sim p} \left[ \sum_{\mathrm Y_T} \left| q_\theta(\mathrm Y_T | \mathbf Y_{1:T-1}) - p(\mathrm Y_T | \mathbf Y_{1:T-1}) \right|  \right] \label{eq:tvd9} \\
    &= \frac12 \sum_{\mathbf Y_{1:T-1}} \left| q_\theta(\mathbf Y_{1:T-1}) - p(\mathbf Y_{1:T-1}) \right| + \frac12 \E_{\mathbf Y_{1:T-1} \sim p} \left[ \sum_{\mathrm Y_T} \left| q_\theta (\mathrm Y_T | \mathbf Y_{1:T-1}) - p(\mathrm Y_T | \mathbf Y_{1:T-1}) \right| \right] \label{eq:tvd10} \\
    &= \frac12 \sum_{t=1}^T \E_{\mathbf Y_{1:t-1} \sim p} \left[ \sum_{\mathrm Y_t} \left| q_\theta (\mathrm Y_t | \mathbf Y_{1:t-1}) - p(\mathrm Y_t | \mathbf Y_{<1:t}) \right| \right] \label{eq:tvd11}
\end{align}
where \eqref{eq:tvd2} breaks the sequence-level summation into the first $T-1$ steps and the last step,
\eqref{eq:tvd3} multiplies and divides $p({\bfY_{1:T-1}})$, and
\eqref{eq:tvd5} subtracts and adds $q_\theta(\rmY_T | \bfY_{1:T-1})$.
After some regrouping in \eqref{eq:tvd6}, we apply the triangle inequality in \eqref{eq:tvd7}.
In \eqref{eq:tvd8}, we break the expectation into two terms, where the first term is further simplified by summing over $\mathrm Y_T$ in \eqref{eq:tvd9} and expanding the expectation in \eqref{eq:tvd10}.
These manipulations bring the equation to a recursive form. By applying the same technique as in \eqref{eq:js6}, we may further unroll the first term in \eqref{eq:tvd10} and eventually obtain  \eqref{eq:tvd11} as an upper bound.

Likewise, we can obtain the following inequality by multiplying and dividing by $q_\theta(\mathbf y_{1:T-1})$ in \eqref{eq:tvd3}
\begin{align}
     & \mathcal L_{\text{TVD}} \le \frac12 \sum_{t=1}^T \E_{\mathbf Y'_{1:t-1} \sim q_\theta} \left[ \sum_{\mathrm Y'_t} \left| q_\theta (\mathrm Y'_t | \mathbf Y'_{1:t-1}) - p(\mathrm Y'_t | \mathbf Y'_{1:t-1}) \right| \right] \label{eq:tvd12}
\end{align}
These two upper bounds, \eqref{eq:tvd11} and \eqref{eq:tvd12}, are then combined to obtain \eqref{eq:thm-tvd}, concluding the proof.

\end{proof}

Admittedly, both \eqref{eq:tvd11} and \eqref{eq:tvd12} are valid upper bounds for the TVD divergence, but we nevertheless combine these two formulas to obtain a more computationally robust upper bound in the same spirit of JS decomposition.

\begin{table}[!b]
\centering
\resizebox{0.80\columnwidth}{!}{
\begin{tabular}{|l|l|rrr|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Dataset}} &
  \multicolumn{1}{c|}{\multirow{2}{*}{Task}} &
  \multicolumn{3}{c|}{\# of Samples} \\ \cline{3-5} 
\multicolumn{1}{|c|}{} &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c}{Train} &
  \multicolumn{1}{c}{Dev} &
  \multicolumn{1}{c|}{Test} \\ \hline
DART~\citep{nan-etal-2021-dart} &
  Data-to-Text Generation &
  30,526 &
  2,768 &
  4,159 \\
XSum~\citep{narayan-etal-2018-xsum} &
  Summarization &
  204,045 &
  11,332 &
  11,334 \\
WNT16 EN-RO~\citep{bojar-etal-2016-wmt16} &
  Machine Translation &
  100,000 &
  1,999 &
  1,999 \\
Commonsense Dialogue~\citep{zhou-etal-2021-commonsense} &
  Dialogue Generation &
  51,831 &
  6,619 &
  6,610 \\ \hline
\end{tabular}
}
\caption{Statistics of our datasets.}

\label{tab:exp-details}
\end{table}

\section{Experimental Details} \label{apdx:exp-details}

Table~\ref{tab:exp-details} shows the statistics of our datasets.
As seen, we benchmarked our models on a variety of natural language generation tasks with different data sizes. We chose state-of-the-art models as the teachers, with 200M--400M parameters. Accordingly, our students had 50M--150M parameters. 
The high performance of \fdistill\ variants across these datasets highlights the robustness of our approach.

For training, we used the Adam optimizer~\citep{adam-optimizer} with  default hyperparameters $\beta=(0.9, 0.999)$ on DART, XSum, and Commonsense Dialogue.
For WMT16 EN-RO, we followed the T5 teacher model~\citep{t52020} and used the AdaFactor optimizer~\citep{pmlr-v80-adafactor}. 
We chose a small batch size of eight to fit  the student as well as the large teacher in our GPU. 
All student models were trained for 28 epochs for pre-distillation and another 12 epochs for each distilling method, as additional training did not further improve performance.

The main experiments were conducted on AMD Milan 7413 CPUs and NVidia A100 GPUs, and the total training time was estimated at 2000 GPU hours. Note that this is not because our algorithm is slow (efficiency analyzed in Table~\ref{tab:efficiency}), but because we have extensively experimented with a variety of datasets and model variants.

\section{Case Study} \label{apdx:case-study}
Table~\ref{tab:case} shows example outputs for DART and Commonsense Dialogue.
On the DART dataset, the KL and RKL distillations fail to yield coherent responses from the input data records.
By contrast, JS and TVD distillations enable the student to generate sentences of much higher quality: they correctly recognize the name of the train as well as its origin and destination.

We additionally show an example output from the Commonsense Dialogue dataset, because the dialogue task exhibits the most severe multi-modal problem, which in turn requires the student to carefully balance mode averaging and collapsing.
As seen, the KL-distilled student generates a short and generic response, which is consistent with existing literature~\citep{wei2019neural, bao-etal-2020-plato}, explained as  mode averaging in our paper. The RKL-distilled student generates a detailed, but ungrammatical and incoherent, response.
For JS and TVD distillations, the students generate responses that are both coherent and detailed. 
The case studies confirm our main claim that JS and JVD are more effective sequence-level distilling approaches.

\begin{table}[!t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{|l|l|}
\hline
\multicolumn{2}{|c|}{\textbf{DART}} \\ \hline
Input &  (11028/11027, destination, mumbai), (11028/11027, origin, Chennai), (11028/11027, train\_name, mumbai Mail)  \\
Reference & train no . 11028 / 11027 name of the train is mumbai mail has origin and destination of chennai to mumbai \\ \hline
KL & the train name of the train that originates from chennai is mumbai.                                             \\
RKL & the mumbai mumbai mumbai train name is mumbai mumbai and the destination is mumbai.                            \\
JS & the mumbai mail train goes to mumbai and originates from chennai to mumbai.                                     \\
TVD & the mumbai mail train starts from chennai to mumbai.   \\ \hline \hline
\multicolumn{2}{|c|}{\textbf{Commonsense Dialogue}} \\ \hline
Input & Quinn spent many years studying. Finally it became graduation time for him. \\
Reference & I can't describe how happy I am on this day.   \\ \hline
KL & I am done. Now is over. \\
RKL & I am going to miss my mom and dad. I am going to miss my dad and the people I have to do a lot of things.  \\
JS  & I am so excited for my graduation. I can't wait to get back to my home and spend some time with my family. \\
TVD & I am finally done packing my dorm. I can't wait to start my first year of college. \\ \hline
\end{tabular}
}
\caption{Example outputs from \fdistill\ variants.}
\label{tab:case}
\end{table}

\end{document}