% Figure environment removed
To retrieve the rich angular and spatial information from {LF} images, we introduce the SAFM module, which embeds \revised{this} information into angular and spatial features. As shown in Fig.~\ref{fig:SF Module}, the SAFM \revised{comprises} of two steps. \revised{The first step extracts} features from all the {LF} images. All images are firstly divided into patches $P_{(m, n)}^{s_i}$, $P_{(m, n)}^c$, correspondence to SAIs $F_{s_i}$ and $F_c$, and further fed into the convolutional layer to extract features, resulting in $\hat{P}_{(m, n)}^{s_i}$ and $\hat{P}_{(m, n)}^c$, where $m, n$ denotes the \revised{relative patch position} in a single feature map. The patch size follows~\cite {zhang2022cmx}. Especially, this work aims to segment the central view image. Thus, the weights for the center view are \revised{calculated individually}, which ensures that the spatial information of the central view remains independent. The weights \revised{SAIs} are shared among different SAIs. The calculations are presented in Eq.~(\ref{equ:feature_extraction1}) and Eq.~(\ref{equ:feature_extraction2}):
\begin{align}
\hat{P}_{(m, n)}^{s_i} &= \text{Conv}^{sub}(C_{\text{in}}, C_{\text{out}})(P_{(m, n)}^{s_i}), \label{equ:feature_extraction1} \\ 
\hat{P}_{(m, n)}^c &= \text{Conv}^{cen}(C_{\text{in}}, C_{\text{out}})(P_{(m, n)}^c), \label{equ:feature_extraction2}
\end{align}
where $\text{Conv}(C_{in}, C_{out}) $ \revised{denotes} the convolutional layer with input dimension $C_{in}$ and output dimension $C_{out}$. For the first {step}, $C_{in}$ is $3$ and $C_{out}$ is $64$. \revised{Subsequently}, the central image features $F_{spl}$ are obtained by combining different patches $\hat{P}_{(m,n)}^c$ and fed into the Spatial Transformer Block~(STB). \revised{The patches of the central view are also fed into the next step to cooperate with patches from the SAIs to obtain the angular features.} 

Afterward, the second step of \revised{the} SAFM is illustrated in Fig.~\ref{fig:SF Module}, which \revised{is represented by} the yellow gear symbol.

{For each patch $\hat{P}_{(m, n)}^{s_i}$, the pixel score $e{_{{(m,n)}}^{s_i}}$ is calculated by obtaining the \revised{Euclidean distance} between each patch from the sub-aperture feature and the corresponding patch from the center view feature, as \revised{expressed} in Eq.~(\ref{e}):
\begin{align}
e{_{{(m,n)}}^{s_i}} &= {\text{Abs}(\hat{P}^c_{{(m, n)}}} -\hat{P}_{{(m, n)}}^{s_i}), \label{e} 
\end{align}
where $\text{Abs}{(\cdot)}$ denotes \revised{the} absolute value between two patches. Given the $e{_{{(m,n)}}^{s_i}}$, the mask score $t_{(m,n)}^{s_i}$ for $\hat{P}_{(m, n)}^{s_i}$ is obtained by mapping into the range of [1,0] and squaring them to enhance the discrimination, as shown in Eq.~(\ref{mp}).
\begin{align}
t_{{(m,n)}}^{s_i} = {(\Theta\{{e{_{{(m,n)}}^{s_i}}}}\})^2,\label{mp}
\end{align}
where $\Theta\{\cdot\}$ denotes \revised{the mapping} operation. After obtaining the mask scores, \revised{a certain patch} $P_{{(m,n)}}$ of \revised{angular features} can be calculated. \revised{Specially}, the patch $P_{{(m,n)}}$ in \revised{the angular} feature $F_{agl}$ at position $(m,n)$ can be calculated by summarizing the corresponding central view patch $\hat{P}^c_{{(m, n)}}$ with masked sub-aperture patches $\hat{P}_{{(m, n)}}^{s_i}$, as \revised{shown} in Eq.~(\ref{p_final}).
\begin{align}
P_{(m,n)} &= \hat{P}^c_{{(m, n)}} + \sum_{i=1}^{N} {t{_{{(m,n)}}^{s_i}} \cdot \hat{P}_{{(m, n)}}^{s_i}}.  \label{p_final}
\end{align}
Finally, the patch of \revised{the} angular feature $P_{(m,n)}$ is obtained. The angular feature $F_{agl}$ is filled by $P_{(m,n)}$ and further fed into \revised{the} Angular Transformer Block~(AFB).}

