

This section provides a detailed introduction to \revised{the proposed network}, \ie, \emph{Omni-Aperture Fuser (OAFuser)}, which is tailored to {LF} semantic segmentation. The overall OAFuser architecture is presented in Sec.~\ref{sec:3_A_oafuser}. The \emph{Sub-Apeture Fusion Module (SAFM)} for \revised{LF} feature aggregation is introduced in Sec.~\ref{sec:3_B_sf}. The \emph{Center Aperture Rectification Module (CARM)} \revised{is presented} in Sec.~\ref{sec:3_c_ro}.  

\subsection{Proposed OAFuser Architecture}\label{sec:3_A_oafuser}

As shown in Fig.~\ref{fig:Overall}, the proposed OAFuser \revised{comprises} a four-stage encoder and a decoder. The encoder consists of the proposed SAFM and CARM to handle the 
%
feature fusion, feature embedding, and feature rectification, respectively.
For simplicity, the following description is based on stage one, which is \revised{the same as that for the other three stages.}
Especially, the arbitrary number of {LF} images is \revised{denoted as sub-aperture images} $F_{s_i} {\in} \mathbb{R}^{H \times W \times 3}$ and central view image $F_c{\in}\mathbb{R}^{H \times W \times 3}$, where $s_i$ \revised{denotes} the $i$-th sub-aperture image in range $[1, N]$. All \revised{images} are fed into \revised{the} SAFM to embed angular feature $F_{agl}{\in}\mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 64}$ that contains rich angular information and spatial feature $F_{spl}{\in}\mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 64}$ which focus on spatial information for the central view. By applying two different transformer blocks following~\cite{zhang2022cmx}, both \revised{features} are transformed into $F_{agl}^*{\in}\mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 64}$ and $F_{spl}^*{\in}\mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 64}$.~ {One notable aspect of our design is that the sub-aperture features are fed into subsequent stages for further processing, which \revised{differs} from other early-fusion or late-fusion methods.} {\revised{Subsequently, the angular and spatial features are concatenated and fed onto the CARM}, which includes the Horizontal Operation and the Vertical Operation for feature rectification along the horizon and vertical \revised{directions} to eliminate misalignments.} 
\revised{In particular}, the concatenation of $F_{spl}^*$ and $F_{agl}^*$ along the horizon direction is applied to obtain $F_{c1}{\in}\mathbb{R}^{\frac{H}{8} \times \frac{W}{4} \times 64}$. After the Global Rectification and the Local Rectification, the horizontally rectified feature $F^{H}{\in}\mathbb{R}^{2 \times \frac{H}{8} \times \frac{W}{8} \times 64}$ is obtained. Then, given the feature $F^{H}$, the feature $F_{c2}{\in}\mathbb{R}^{\frac{H}{4} \times \frac{W}{8} \times 64}$ is obtained by {splitting} and concatenation \revised{operations along the vertical direction}. Especially, $F^{H}{\in}\mathbb{R}^{2 \times \frac{H}{8} \times \frac{W}{8} \times 64}$ is disentangled into $F^{H}_{agl}{\in}\mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 64}$ and $F^{H}_{spl}{\in}\mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 64}$. $F^{H}_{agl}$ and $F^{H}_{spl}$ are further concatenated along vertical direction. After applying \revised{the Global Rectification and the Local Rectification in the Vertical Operation}, the rectified features $F^{G}{\in}\mathbb{R}^{2 \times \frac{H}{8} \times \frac{W}{8} \times 64}$. $F^{G}_{agl}{\in}\mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 64}$ and $F^{G}_{spl}{\in}\mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 64}$ is obtained by {split operation} of $F^{G}$. (Notably, $F^{G}_{spl}{\in}\mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 64}$ functions \revised{serve as both a spatial feature for subsequent feature fusion and the spatial feature input for stage two.}) Furthermore, \revised{the two rectified features}, \ie,  $F^G_{agl}$ and $F^G_{spl}$ are fused by using the FFM module~\cite{zhang2022cmx}, which integrates two different features, to produce the final feature $f_1$ for this stage. The weights between \revised{the Horizontal Operation and the Vertical Operation} are shared.

Note that one of our crucial designs is \revised{that} {\textbf{all sub-aperture information is fed into each stage}, which allows our network to effectively consider all the SAIs {throughout the entire process}}. The pyramidal features $f_1, f_2, f_3, f_4$ are obtained via the four-stage encoder in a dimension of $\{64,128,320,512\}$. Subsequently, the multi-stage features are further fed into an MLP Decoder~\cite{xie2021segformer} for final prediction.

