Light field cameras \revised{are capable of capturing} intricate angular and spatial details. This allows for acquiring complex light patterns and details from multiple angles, \revised{significantly enhancing the precision of} image semantic segmentation. However, two significant issues arise: (1) The extensive angular information of light field cameras contains a large amount of redundant data, which is overwhelming for the limited hardware resources of intelligent agents. (2) A relative displacement difference exists in the data collected by different micro-lenses. {To address these issues, we propose an \emph{Omni-Aperture Fusion model ({OAFuser})} that leverages dense context from the central view and extracts the angular information from sub-aperture images to generate semantically consistent results.} \revised{{To simultaneously streamline the redundant information} from the light field cameras and avoid feature loss during network propagation, we present a simple yet very effective \emph{Sub-Aperture Fusion Module (SAFM)}. This module efficiently embeds sub-aperture images in angular features, allowing the network to process each sub-aperture image with a minimal computational demand of only (${\sim} 1GFlops$).} {Furthermore, to address the mismatched spatial information across viewpoints, we present a \emph{Center Angular Rectification Module (CARM)} to realize feature resorting and prevent feature occlusion caused by misalignment.} {The proposed OAFuser achieves state-of-the-art performance on four UrbanLF datasets in terms of \emph{all evaluation metrics} and sets a new record of $84.93\%$ in mIoU on the UrbanLF-Real Extended dataset}, with a gain of ${+}3.69\%$. \revised{The source code for OAFuser is available at} \url{https://github.com/FeiBryantkit/OAFuser}.

%
