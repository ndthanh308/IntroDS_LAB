\IEEEPARstart{I}{ntelligent} agents, such as robotics and autonomous driving systems, heavily rely on visual understanding, \eg, image semantic segmentation, which can produce pixel-level prediction results and contribute to determining the category, shape, and position of objects~\cite{lu2022surrogate,mazhar2023rethinking,liu2023testtime_adaptation}. To effectively apply semantic segmentation in real-world scenarios, \emph{accuracy} is one of the most crucial factors.
%
{To enhance the accuracy, advanced models like ConvNets~\cite{vandenhende2020mti,guo2022segnext}, MLP-based methods~\cite{tolstikhin2021mlp}, and attention mechanism~\cite{xie2021segformer,zhang2022vsa}, are introduced. Furthermore, different fusion strategies for segmentation~\cite{zhang2018exfuse,zhang2015sensor,hazirbas2017fusenet,zhang2022cmx} are proposed to improve the perception robustness.}

% Figure environment removed

{In this work, we introduce Light Field (LF) cameras~\cite{georgiev2006light} for scene understanding (Fig.~\ref{fig:depth light}).}
These cameras are equipped with a unique optical system, referred to as an ``omni-aperture'', indicating their ability to utilize all available apertures to capture comprehensive light information. \revised{The volumetric radiance of a scene is recorded}, surpassing the capabilities of \revised{conventional photography} by capturing more complex data, \revised{including} information beyond the standard two-dimensional plane. {Specifically, LF cameras retain details \revised{of the angles} of incoming light rays, effectively yielding four-dimensional LF data.} By harnessing both the \textit{spatial} and \textit{angular} data, \revised{LF cameras} can facilitate \revised{postevent focus shifting} across planes \revised{thereby} enabling refined visual computations \revised{that are} critical for dense vision tasks required in vision perception tasks~\cite{meilland2015dense}.~ For intelligent agents, the ability of LF cameras to simultaneously capture spatial information from multiple directions allows for a more nuanced understanding of the environment. \revised{Furthermore, their equipment and annotation costs are lower than those of the RGB-LiDAR fusion, and LF cameras can provide more angular information than the RGB-D cameras.} 
 
Despite the theoretical benefits,
we observe two under-explored challenges \revised{when applying LF cameras to scenes semantic segmentation task}, as presented in Table~\ref{tab:2 part}, including: 
(1) {Hardware Demand and Extra Effort};
%
(2) The Capacity of the Network.
{To address these challenges, we propose a novel \textbf{Omni-Aperture Fusion (OAFuser)} model including two major insights for LF-based scene understanding.} 
%

First, hardware demand and extra effort \revised{refers} to the computational requirements necessary for processing images, \revised{which involve} the number of floating-point operations (GFlops). Given that LF cameras capture multiple SAIs in a single-shot sample, parallel processing of each image \revised{can lead} to computational overflow. This severely restricts the application of LF cameras on mobile devices.~{For instance, using an LF camera with a spatial resolution of $640{\times}480$ and angular resolution of $9{\times}9$ \revised{produces a macro-pixel image} with $4320{\times}5760$, imposing $81$ times of data stream along the network module.}
%
{However, compressing LF images into a depth map~\cite{chen2020bi,sheng2022urbanlf} only reduces the computational load of the semantic segmentation module. \revised{However, it requires additional auxiliary networks}, leading to a significant increase in \revised{the number of} floating-point operations.}
\revised{In addition}, the quality of the generated depth map~\cite{leistner2022towards,yan2022light} directly \revised{affects} the segmentation performance.
{To overcome these issues and explore the efficiency of using LF cameras without the need for additional computational resources or architectural modifications, we propose a simple yet effective \textbf{Sub-Aperture Fusion Module (SAFM).}~
%
\revised{The proposed module} minimizes computational requirements (${\sim}1~GFlops$ per image) and introduces no \revised{additional} parameters.
%
\revised{When analyzing image features from different viewpoints, rich information from LF images is embedded in angular and spatial feature maps}, which are then fed into transformer blocks. Thanks to the SAFM module, \revised{the proposed} OAFuser network can achieve \revised{improved} performance while avoiding excessive computational burden.}

\begin{table*}[!t]
  \centering
  \renewcommand{\arraystretch}{1.2}
  \begin{adjustbox}{width=1\textwidth}
  \begin{tabular}{c|ccc|ccc}
    \toprule[1mm]
    \multirow{2}{*}{\textbf{Method}}& \multicolumn{3}{c|}{\textbf{Hardware Demand and Extra Effort}} & \multicolumn{3}{c}{\textbf{The Capacity of the Network}} \\ \cline{2-7}
    & \multicolumn{1}{c|}{\textbf{\#Modality}} & \multicolumn{1}{c|}{{\textbf{P-SAI GFlops ($\downarrow$)}}} & \textbf{Independent of {Depth} Map} & \multicolumn{1}{c|}{\textbf{Angular Information}} & \multicolumn{1}{c|}{\textbf{Feature Rectification}} & \textbf{{Robustness} %
    } \\ \midrule[1.5pt]\hline
    RGB Based Network & 1 & N.A. & N.A. & \xmark & \xmark & \xmark \\ \hline
    RGB-D based Network & 2 & N.A. & \xmark & \xmark & \cmark & \xmark \\ \hline
    OCR-LF \cite{yuan2020object} & {>2} & 6.91 & \cmark & \cmark & \xmark & \xmark \\ \hline
    PSPNet-LF \cite{zhao2017pyramid} & {>2} & 13.01 & \cmark & \cmark & \xmark & \xmark \\ \hline
    LF-IENet \cite{cong2023combining} & {>2} & 144.95 & Explicitly & Implicit & \xmark & \xmark \\ \hline
    LF-IENet++~\cite{cong2024end} & {>2} & 159.39 & Explicitly & Implicit & \xmark & \xmark \\ \hline
    \rowcolor[gray]{.9}OAFuser (ours) & {Arbitrary} & 6.56 & \cmark & \cmark & \cmark & \cmark \\ \hline
  \end{tabular}
  \end{adjustbox}
  \caption{Challenges of LF semantic segmentation. \revised{Here}, \textit{\#Modality} indicates the number of sub-aperture images (SAI). The \textit{P-SAI GFlops} illustrates the consumption of GFlops for each SAI. \textit{N.A.} \revised{indicates} that SAIs are not used. \textit{Independent of Depth Map} indicates whether disparity information is required; the \textit{Angular Information} column explores the utilization of angular information from the SAIs; {The \textit{Robustness} indicates the issue of out-of-focused images from LF cameras. \revised{For LF-IENet series}, \textit{Depth Map} is \revised{generated explicitly}, and \textit{Angular Information} is hidden in the implicit branch (the angular feature is utilized through the depth map). Other methods perform multi-stage feature extraction for each image, which restricts the \revised{scalability of the network}. However, OAFuser extracts only spatial and angular features, \revised{which allows} it to handle arbitrary SAIs.}}
  \label{tab:2 part}
  \vskip -3ex
\end{table*}

{Second}, the capacity of the network \revised{concerns} the performance, \ie, the effective {representation} of angular features and encounter misalignment.
%
\revised{Previous methods that stack images into an array~\cite{sheng2022urbanlf,yan2022light} \revised{contain only} one-dimensional angular information}; the implicit angular relationship of LF images is ignored when converting them into a video sequence~\cite{zhuang2020video,wang2021temporal}. {\revised{In addition}, directly merging images or features results in blurred or mixed boundaries, and this phenomenon is particularly severe with \revised{the increasing number of sub-aperture images.}}
%
However, the single SAIs captured by certain micro-lenses contain \revised{a significant amount of} noise~\cite{dansereau2013decoding,bok2016geometric}. Moreover, the initial LF images are not \revised{fully} focused, which can \revised{intensify this problem}. 
In this work, we introduce a \textbf{Center Angular Rectification Module (CARM)} to perform effective rectification between the center view and the aperture-based features.
As shown in Fig.~\ref{fig:fusion strategy}, \revised{previous works} employed early-fusion strategies~\cite{10075555}, late-fusion strategies~\cite{hu2019acnet,zhang2019residual}, or injection methods~\cite{yan2022light}, \revised{which are not suitable for light field images with pixel displacement}. Our network adopts iteration feature rectification to incorporate the misaligned angular feature and spatial features from the SAFM before the fusion stage, as \revised{illustrated in} Fig.~\ref{fig2_4}.
This design allows the proposed network to explore the relationship between different features while realigning the angular and spatial information, which contributes to the performance.

{To demonstrate the effectiveness of the proposed OAFuser architecture, we conduct comprehensive experiments on different datasets. Whether on datasets with a large disparity range (UrbanLF-Syn-Big dataset) or those with a small disparity range (UrbanLF-Syn dataset) or in real-world (UrbanLF-Real dataset) or syn-real mixing (UrbanLF-RealE dataset) scenes, \revised{the proposed} method achieves state-of-the-art performance across various metrics, while using the fewest GFlops.}

At a glance, we deliver the following contributions:
\begin{compactitem}
    \item We propose a novel paradigm, \revised{the Omni-Aperture Fusion model}, \ie, \textbf{OAFuser}, to perform LF semantic segmentation by leveraging the structural characteristics of LF cameras, \revised{which addresses crucial challenges} and enables the handling of an arbitrary number of SAIs. 
    \item {We design an \textbf{Sub-Aperture Fusion Module (SAFM)} that enables our network to \revised{process each SAI using a computational cost} of (${\sim}1GFlops$) without introducing additional parameters,
    and a \textbf{Center Angular Rectification Module (CARM)} to rectify information misalignment caused by variations from different angles.}
    \item {We verify \revised{the proposed method} through extensive experiments on  four datasets, \ie,
    \revised{the} UrbanLF series.}
\end{compactitem}

% Figure environment removed

% Figure environment removed