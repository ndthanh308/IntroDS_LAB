In this section, \revised{an} overview of semantic segmentation is introduced in Sec.~\ref{sec:2_a_ss} while our work is a pixel-level classification task for urban scenes. {\revised{Due to} the \revised{enormous amount} of misaligned image representation from different micro-lenses of \revised{the LF cameras}, various multi-modal semantic segmentation works are presented in Sec.~\ref{sec:2_b_mm}}. {Furthermore, to \revised{enhance the angular information} expression and better utilize the misaligned information of LF cameras, \revised{several applications of LF cameras that have significantly inspired us are also introduced}} in Sec.~\ref{sec:2_c_lf}.

\vspace{-1em}
\subsection{Semantic Segmentation}\label{sec:2_a_ss}

{Semantic scene segmentation, as a fundamental task of computer vision, plays a crucial role in scene understanding tasks, such as autonomous driving and intelligent transportation systems~\cite{zhou2022mtanet,zhang2022trans4trans}.}
Since FCN~\cite{long2015fully} pioneered the use of convolutional neural networks to replace fully connected networks with an end-to-end framework, many segmentation works have emerged based on this approach, and the efficiency and accuracy of segmentation have \revised{significantly} improved.~
For \revised{example}, the works of \cite{chen2018encoder,badrinarayanan2017segnet} adopt an encoder-decoder structure to capture contextual information and local details.
Then, dilated convolution~\cite{chen2018encoder,yang2018denseaspp} is introduced to increase the receptive field.
To enhance the global context representation, Ding \etal~\cite{ding2018context} and He \etal~\cite{he2019adaptive} adopt a pyramidal hierarchy in the encoding path.
Furthermore, the enhancement of prior contextual information~\cite{lin2017refinenet,fu2019dual,wang2021exploring} \revised{contributes to improving segmentation results.}
{Since the introduction of the self-attention mechanism in vision tasks~\cite{dosovitskiy2020image}, many subsequent works~\cite{xie2021segformer,zhang2022vsa,zhang2022trans4trans} propose dense prediction by utilizing attention-based models.
\revised{Other works introduce lightweight backbones to accelerate the inference}~\cite{sandler2018mobilenetv2,tan2019efficientnet,zhang2018shufflenet}.}

Although \revised{these studies} demonstrate excellent results in handling dense prediction tasks, they still suffer from limitations in angular information, leading to performance degradation in handling complete areas.
LF cameras, unlike monocular cameras, are equipped with an omni-aperture feature. {Therefore, LF cameras, which sample the same object from different angles, can contribute to scene understanding tasks by incorporating angular information.}

\subsection{Multi-Modal Semantic Segmentation}\label{sec:2_b_mm}
\revised{Multiple SAIs captured by an LF camera} can be considered as various RGB modalities with inherent relationships. Therefore, the research on multi-modal semantic segmentation is essential \revised{to explore} the potential of LF cameras.
ACNet~\cite{hu2019acnet} and EDCNet~\cite{zhang2021exploring} leverage attention \revised{connections to facilitate} cross-modal interactions in RGB-Depth and RGB-Event semantic segmentation, respectively.
MMFNet~\cite{chen2020mmfnet} enables the fusion of multiple medical images by aggregating different features \revised{in the spatial and channel domains.}
NestedFormer~\cite{xing2022nestedformer} \revised{proposes} a feature aggregation module to \revised{realize multi-modal medical image segmentation}.
\revised{In addition}, ESANet~\cite{seichter2021efficient} and SA-Gate~\cite{chen2020bi} utilize depth maps and RGB images to achieve high-accuracy semantic segmentation by employing fusion modules.
PGSNet~\cite{mei2022glass}, which introduces a dynamic integration module, achieves glass segmentation.
Additionally, other works~\cite{sun2019rtfnet,zhou2021gmnet,ha2017mfnet} adopt RGB-thermal image fusion.
\revised{Although CMX}~\cite{zhang2022cmx,zhang2023delivering} present an arbitrary-modal fusion network, which can handle RGB and any other modality, such as depth, thermal, polarization, event, or LiDAR data, the data sources are aligned with each other before \revised{inputting the data into the network}.~
HRFuser~\cite{broedermann2022hrfuser} realizes the fusion of an arbitrary number of additional modalities by introducing multi-window cross-attention Fig.~\ref{fig2_3}.

However, these methods are unsuitable \revised{for pixel-wise prediction of LF images}. This is because {(1)} they assign a set of computational pipelines for each modality, and \revised{applying them to LF images leads to an explosion in computational and parameter requirements}. {(2)} \revised{Pixel shifting} between different \revised{SAIs caused by micro-lenses} can adversely affect semantic segmentation results. \revised{The proposed OAFuser} focuses on reducing the computational burden of LF information and considers \revised{mismatches} between images captured by all sub-apertures of LF cameras.

\vspace{-1pt}
\subsection{Light Field Scene Understanding}\label{sec:2_c_lf}
Monocular cameras require the focal length to be set in advance during shooting. However, with LF cameras, users can adjust the focus point and refocus \revised{after capture}. \revised{Thus,} users can \revised{determine} which part of the image is sharp and which part is blurred after shooting~\cite{Ruan_2022_CVPR,yang2023joint}. \revised{Because of} their rich visual information, LF cameras have been \revised{wide applied} in various fields, such as saliency detection~\cite{wang2017two,zhang2017saliency}, depth estimation~\cite{honauer2017dataset,peng2020zero}, and super-resolution~\cite{wang2018lfnet,jin2020light}. Research in other related communities is crucial to our work, as light-field cameras are still under-explored in the semantic segmentation domain.

FES~\cite{chen2023fusion} achieves sub-aperture feature fusion via spatial and channel attention.~
NoiseLF~\cite{feng2022learning} utilizes \revised{an} {all-focus} central view image and its corresponding focal stack. It has a unique forgetting matrix and confidence re-weighting strategy to achieve supervised saliency detection \revised{in the presence of} noisy labels.~
\revised{Furthermore, several works~\cite{wang2020spatial,liang2023learning,zhang2019residual} utilize the macro-pixel image, SAIs, epipolar images, or a combination of some of these images to generate high-resolution LF images.}~In addition, AIFLFNet~\cite{zhou2023aif} utilized LF images to estimate depth information. Additionally, SAA-Net~\cite{wu2021spatial} introduces spatial-angular attention modules for LF image reconstruction.~\revised{In particular}, a design from~\cite{wang2022disentangling} proposes a unified block for handling macro-pixel images, which can be used for both super-resolution and disparity estimation.
Furthermore, \cite{9442895} has also implemented LF technology for image reconstruction in the autonomous community.

{Due to the disproportion between the computational cost and the improvements achieved, few works \revised{have focused on LF} semantic segmentation. \cite{jia2021semantic} adopts atrous spatial pyramid pooling to extract multi-scale contextual features and the angular features are acquired from the micro-pixel image. Furthermore, the UrbanLF team~\cite{sheng2022urbanlf} publishes a series of outdoor semantic segmentation datasets and utilizes \revised{stacks of SAIs to segment central-view images}. LFIE-Net~\cite{cong2023combining} introduces an explicit branch to generate disparity maps within the network and cooperates with the central view image to achieve semantic segmentation. \revised{Therefore}, LF-IENet++~\cite{10440124} is used to explore the contribution of SAIs to scene understanding in different disparity ranges.}

{However, compared with multi-modal approaches, LF-based methods, although utilizing depth maps to extract depth information, \revised{require the assistance of auxiliary networks during the depth extraction process and consume a significant amount of computational resources}, {as shown in Table~\ref{tab:Comput Cost}}. When the number of LF images increases from $5$ to $33$, the computational load of the previously best-performing method increases by more than $4000$ GFlops. However, the proposed method only adds $27.9$ Gflops of computational load.} 
