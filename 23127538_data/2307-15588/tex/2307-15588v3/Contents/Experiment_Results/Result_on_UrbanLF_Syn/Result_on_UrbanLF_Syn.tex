
The quantitative results \revised{obtained} on the synthetic dataset are \revised{presented} in Table~\ref{result:syn}.  
Among all the models assessed, OAFuser17 achieves the highest mIoU. Comparatively, OAFuser9 exhibits slightly inferior performance to LF-IENet$^3$. This discrepancy in performance can be primarily attributed to the restricted size of the synthetic dataset, encompassing merely $173$ samples for training.
\begin{table}[t!]
  \centering
  \renewcommand{\arraystretch}{1.3}
  \setlength{\tabcolsep}{2pt}
  \begin{adjustbox}{width=0.48\textwidth}
\begin{tabular}{l|c|ccc}
\toprule[1mm]
\textbf{Method} & \textbf{Type} & \textbf{Acc (\%)} & \textbf{mAcc (\%)} & \textbf{mIoU (\%)} \\ \midrule[1.5pt]\hline
{PSPNet~\cite{zhao2017pyramid}} & RGB & 90.39 & 84.26 & 76.08 \\ 
%
$\text{DeepLabv3}^+$~\cite{chen2018encoder} & RGB & 90.89 & 84.07 & 76.45 \\ 
{OCR~\cite{yuan2020object}} & RGB & 92.37 & 87.60 & 80.13 \\ 
%
{MTINet~\cite{vandenhende2020mti}} & RGB-D & 92.25 & 87.46 & 80.26 \\ 
{ESANet~\cite{seichter2021efficient}} & RGB-D & 92.88 & 87.45 & 80.79 \\ 
{SA-Gate~\cite{chen2020bi}} & RGB-D & {\color[HTML]{0000FF}\text{93.03}} & 88.08 & 81.00 \\ 
%
{TMANet~\cite{wang2021temporal}} & Video & 90.97 & 84.72 & 76.99 \\
{PSPNet-LF~\cite{sheng2022urbanlf}} & LF & 91.16 & 86.12 & 78.48 \\ 
{OCR-LF~\cite{sheng2022urbanlf}} & LF & 92.75 & 87.98 & 80.62 \\ 
{LF-IENet$^4$~\cite{cong2023combining}} & LF & 91.39 & 86.19 & 78.50 \\ 
{LF-IENet$^3$~\cite{cong2023combining}} & LF & {92.91} & {\color[HTML]{0000FF}\text{88.21}} & {\color[HTML]{0000FF}\text{81.11}} \\ \hdashline[1pt/1pt]
\textbf{OAFuser9} & LF & 93.23 (+0.20) & \color[HTML]{FF0000}\textbf{88.26 (+0.05)} & 81.64 (+0.53) \\ 
\textbf{OAFuser17} & LF & {\color[HTML]{FF0000} \textbf{93.42 (+0.39)}} &  88.22 (+0.01) & {\color[HTML]{FF0000} \textbf{81.93 (+0.82)}} \\ \hline
\end{tabular}
\end{adjustbox}
\caption{Quantitative results on the UrbanLF-Syn dataset. Acc (\%), mAcc (\%), and mIoU (\%) are reported. The best results are highlighted in red. The variation term represents the performance difference from the previous best result.}
\label{result:syn}
\end{table}
% Figure environment removed

\revised{OAFuser utilizes the small differences present between different SAIs to capture intricate angular details. These details are then combined with appropriate spatial information obtained from the main viewpoint. This approach differs from previous methods that focus on spatial information limited to the central view and ignore angular information obtained from LF cameras.} {Additionally, a key feature of OAFuser is its ability to eliminate the need for pre-processing of initial {LF} images, \revised{demenstrating its ability} to handle raw {LF} data by effectively utilizing various angular information.} Importantly, \revised{the processing capabilities of the network are not adequately demonstrated when using only focused images obtained from synthetic datasets.} Furthermore, this is demonstrated in Fig.~\ref{fig:Qualitative result} in areas of complex structures, such as the leaves and bicycle pillion seats. \textbf{Compared with LF-IENet, which leads to loss of global context for objects, such as the gaps between objects and their connections with branches being disrupted, the proposed approach demonstrates better segmentation results even with $17$ SAIs.} \revised{This result highlights the superior capability of \revised{the proposed} network in harnessing SAIs}; through sophisticated rectification, the OAFuser consistently synchronizes with the rich tapestry of information gleaned from diverse perspectives. This claim is also supported by experiments conducted on other UrbanLF datasets.



