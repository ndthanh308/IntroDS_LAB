\begin{table}[t!]
  \centering
  \renewcommand{\arraystretch}{1.4}
  \setlength{\tabcolsep}{2pt}
  \begin{adjustbox}{width=0.48\textwidth}
\begin{tabular}{l|c|ccc}
\toprule[1mm]
\textbf{Method} & \textbf{Type} & \textbf{Acc (\%)} & \textbf{mAcc (\%)} & \textbf{mIoU (\%)} \\ \midrule[1.5pt]\hline
{PSPNet~\cite{zhao2017pyramid}} & RGB & 92.78 & 85.08 & 79.20 \\
$\text{DeepabV3}^+$~\cite{chen2018encoder} & RGB & 91.24 & 83.41 & 76.97 \\ 
{SETR~\cite{zheng2021rethinking}} & RGB & 92.69 & 86.45 & 79.87 \\ 
{OCR~\cite{yuan2020object}} & RGB & 92.93 & 86.87 & 80.43 \\ 
{TMANet~\cite{wang2021temporal}} & Video & 92.55 & 84.65 & 78.54 \\
{PSPNet-LF~\cite{zhao2017pyramid}} & LF & 92.61 & 85.22 & 79.48 \\
{OCR-LF~\cite{yuan2020object}} & LF & {\color[HTML]{0000FF}93.35} & {\color[HTML]{0000FF}87.05} & {\color[HTML]{0000FF}81.24} \\ \hdashline[1pt/1pt]
\textbf{OAFuser9} & LF & \color[HTML]{FF0000} \textbf{94.61 (+1.26)} & \color[HTML]{FF0000} \textbf{89.84 (+2.79)} & \color[HTML]{FF0000} \textbf{84.93 (+3.69)} \\ 
\textbf{OAFuser17} & LF & 93.74 (+0.39) & 88.92 (+1.87) & 82.42 (+1.18) \\ \hline
\end{tabular}
\end{adjustbox}
\caption{Quantitative results on the UrbanLF-RealE dataset. Acc (\%), mAcc (\%), and mIoU (\%) are reported. The best results are highlighted in red. The variation term indicates the performance difference from the previous best result.}
\label{result:exten}
\end{table}

UrbanLF-RealE, which involves not only real-world samples but also extends to multiple synthetic samples, poses significant challenges to the performance of the \revised{proposed} model.
This complex combination of data is more aligned with unconstrained scenarios.
As shown in Table~\ref{result:exten}, OAFuser achieves state-of-the-art performance with a mIoU of $84.93\%$.
\revised{This value exceeds those of existing methods by more than $3.69\%$.} The accuracy of OAFuser17 decreases compared \revised{with that of} OAFuser9, which is caused by the challenges posed by a large number of SAIs in complex scenarios.
\revised{With the increasing number of sub-aperture images,} it becomes challenging to accurately distinguish between relevant and irrelevant features in a dataset \revised{containing both} focused and defocused images.
Compared with PSPNet-LF and OCR-LF, which implement the direct fusion of sub-aperture and central-view images, our OAFuser embeds the rich angular feature in an independent branch and focuses on utilizing both spatial and angular features. The qualitative results are presented in Fig.~\ref{fig:RealE-Syn-result} (right).

Above all, the excellent performance \revised{obtained} on the UrbanLF-RealE dataset demonstrates the superiority of the proposed design and sets a new record of {LF} semantic segmentation.
The proposed network exhibits remarkable accuracy without relying on extensive image \revised{pre-processing} and additional devices. This validates the applicability of the proposed network in real-world scenarios.
