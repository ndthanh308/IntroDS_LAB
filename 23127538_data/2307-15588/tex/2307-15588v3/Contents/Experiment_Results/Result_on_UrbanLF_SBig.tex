\begin{table}[t]
  \centering
  \renewcommand{\arraystretch}{1.4}
  \setlength{\tabcolsep}{2pt}
  \begin{adjustbox}{width=0.48\textwidth}
\begin{tabular}{l|c|ccc}
\toprule[1mm]
\textbf{Method}       & \textbf{Type}  & \textbf{Acc (\%)}   & \textbf{mAcc (\%)}  & \textbf{mIoU (\%)}  \\

\midrule[1.5pt]\hline
PSPNet~\cite{zhao2017pyramid}       & RGB   & 88.56 & 82.49 & 74.31 \\
OCR~\cite{yuan2020object}          & RGB   & 90.63 & 84.53 & 77.46 \\
SETR~\cite{zheng2021rethinking}         & RGB   & 89.37 & 84.36 & 76.29 \\
TMANet~\cite{wang2021temporal}       & Video & 88.29 & 81.13 & 73.54 \\
ESANet~\cite{seichter2021efficient}       & RGB-D & \color[HTML]{0000FF}\text{91.83} & 84.64 & 77.21 \\
SA-Gate~\cite{chen2020bi}      & RGB-D & 91.23 & 85.16 & 77.52 \\
PSPNet-LF~\cite{wang2021temporal}    & LF    & 88.78 & 83.38 & 75.15 \\
OCR-LF~\cite{sheng2022urbanlf}       & LF    & 91.09 & 84.99 & 77.90 \\
LF\_IENet$^4$~\cite{cong2023combining}   & LF    & 88.79 & 81.96 & 74.11 \\
LF\_IENet$^3$~\cite{cong2023combining}   & LF    & 91.27 & 84.71 & 78.18 \\
LF\_IENet$^4$++~\cite{cong2024end} & LF    & 89.08 & 82.55 & 74.88 \\
LF\_IENet$^3$++~\cite{cong2024end} & LF    & 91.66 & \color[HTML]{0000FF}\text{85.43} & \color[HTML]{0000FF}\text{79.21} \\ \hdashline[1pt/1pt]
\textbf{OAFuser9} & LF    & 93.39 (+1.56)    & \color[HTML]{FF0000} \textbf{89.66 (+4.23)}   & 81.15 (+1.94) \\
\text{OAFuser17}  & LF    &  \color[HTML]{FF0000} \textbf{94.29 (+2.46)}  &  88.50 (+3.07)   &   \color[HTML]{FF0000} \textbf{81.70 (+2.49)}  \\ \hline
\end{tabular}
\end{adjustbox}
\caption{Quantitative results on the UrbanLF-Syn-Big dataset. Acc ($\%$), mAcc ($\%$), and mIoU ($\%$) are reported. The best results are highlighted in red. The variation term represents the performance difference from the previous result.}%
\label{result:Syn_Big}
\vspace{-0.3em}
\end{table}
% Figure environment removed
\revised{In the UrbanLF-Syn-Big dataset, the LF images exhibit an extended disparity range between adjacent views, which significantly increases the complexity of semantic segmentation tasks because of the pronounced relative pixel displacement across the image pixels.} Methods applied to this dataset have not exceeded the 80\% Acc threshold. In particular, LF-IENet and LF-IENet++ \revised{employ an auxiliary depth} estimation branch \revised{that is tailored to realize large disparity} light field semantic segmentation. However, our implementation, which integrates the SAFM and the CARM, has successfully achieved a precision of $81.70\%$. \revised{This result represents} a $2.49\%$ increment in mIoU, \revised{which is a new record for state-of-the-art performance on this challenging dataset.} By embedding high-dimensional features extracted from LF images, the utilization of angular information is ensured. Additionally, by performing feature correction in both horizontal and vertical directions, the consistency of the feature space is maintained, thereby avoiding pixel position misalignment caused by different viewpoints. As shown in Fig.~\ref{fig:RealE-Syn-result}, \revised{the corner of the table is a \revised{typically} overlooked location, and the area where plants and fences obscure each other poses a significant challenge in pixel-level classification tasks. OAFuser perfectly segmented the corner area of the table.} Although it did not perfectly reconstruct the details of the plants, it maintained the consistency of the geometric meaning between the plants and the fence.
