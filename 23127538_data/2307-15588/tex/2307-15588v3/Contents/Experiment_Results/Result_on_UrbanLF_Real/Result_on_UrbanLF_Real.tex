\begin{table}[ht]
\centering
\renewcommand{\arraystretch}{1.4}
\setlength{\tabcolsep}{2pt}
\begin{adjustbox}{width=0.48\textwidth}
\centering
\begin{tabular}{l|c| cc| c}
\toprule[1mm]
\textbf{Method} & \textbf{Type} & \textbf{Acc (\%)} & \textbf{mAcc (\%)} & \textbf{mIoU (\%)} \\ \midrule[1.5pt]\hline

PSPNet~\cite{zhao2017pyramid} & RGB & 91.75 & 84.00 & 77.03 \\
$\text{DeepLabv3}^+$~\cite{chen2018encoder} & RGB & 91.92 & 83.45 & 76.63  \\
{SETR}~\cite{zheng2021rethinking} & RGB & 91.96 & 84.94 & 78.44\\ 
{OCR}~\cite{yuan2020object} & RGB & 92.02 & 84.99 & 78.75 \\ 
%
{TMANet}~\cite{wang2021temporal} & Video & 91.77 & 83.02 & 76.20 \\ 
{PSPNet-LF}~\cite{zhao2017pyramid} & LF & 91.88 & 84.31 & 77.79 \\
{OCR-LF}~\cite{yuan2020object} & LF & 92.92 & 86.50 & 80.69 \\ 
{LF-IENet$^4$}~\cite{cong2023combining} & LF & 92.40 & 84.94 & 79.03 \\ 
{LF-IENet$^3$}~\cite{cong2023combining} & LF & 93.02 & 86.69 & 80.35 \\ 
{LF-IENet++$^4$~\cite{cong2024end}}     & LF & 92.70 & 85.74 & 79.69 \\
{LF-IENet++$^3$~\cite{cong2024end}}     & LF & {\color[HTML]{0000FF}93.34} & {\color[HTML]{0000FF}87.13} & {\color[HTML]{0000FF}81.09} \\\hdashline[1pt/1pt]
\textbf{OAFuser9} & LF & {\color[HTML]{FF0000}\textbf{94.45 (+1.15)}} & {\color[HTML]{FF0000}\textbf{88.21 (+1.08)}} & {\color[HTML]{FF0000}\textbf{82.69 (+1.60)}} \\ 
\textbf{OAFuser17} & LF & 94.08 (+0.74) & 87.74 (+0.61) & 82.21 (+1.12) \\ 
\hline %\bottomrule[1mm]
\end{tabular}
\end{adjustbox}
\caption{{Quantitative results on the UrbanLF-Real dataset. Acc (\%), mAcc (\%), and mIoU (\%) are reported. The best results are highlighted in red. The variation term \revised{represents} the performance difference from the previous best result. The superscript for ``LF-IENet'' and ``LF-IENet++'' indicates the number of sub-aperture images.}}
\label{result:real}
\end{table}


Table~\ref{result:real} presents the quantitative results \revised{obtained} on the UrbanLF-Real dataset, which is challenging due to issues \revised{such as} out-of-focus from the plenoptic camera and maintaining consistency with {LF} camera implementation without further data pre-processing in real-world scenarios.
\revised{The proposed} OAFuser9 achieves a state-of-the-art mIoU score of $82.69\%$, \revised{demonstrating} an improvement of $1.60\%$ \revised{compared with} the previous methods.
\textbf{Similarly, \revised{the proposed} OAFuser17 achieves an mIoU of $82.21\%$, with an increase of $1.12\%$. \revised{In terms of Acc and mAcc, OAFuser9 achieves an improvement of over $1\%$ compared with the previous works.}}
The small performance gap between our OAFuser9 and OAFuser17 is attributed to the image quality, which will be discussed in Sec.~\ref{sec:5_DCPC}. 

% Figure environment removed

{The qualitative results, as shown in Fig.~\ref{fig:real-result}, also demonstrate the effectiveness of \revised{the proposed} method. For the three sets of images in the left column, we \revised{visually compare different methods} on the test dataset. In complex scene areas, \revised{the proposed} method achieved accurate recognition, maintaining the reasonableness of the geometric shapes of objects, \revised{whereas the previous methods} had some misjudgments. On the validation dataset, we selected challenging shadow areas. Under low-light conditions, \revised{the proposed} method also achieves accurate recognition. Furthermore, \revised{as the number of SAIs escalates, a proportional augmentation emerges in the prevalence of extraneous features within the dataset.}~{Additionally, the prevalence of out-of-focus and blurry images results in imprecise guidance, distinguishing our network from others. In \revised{the proposed method}, we meticulously implement selective embedding of features during the feature integration phase.}}
From another perspective, \revised{this} also demonstrates the necessity of the proposed method when handling semantic segmentation with {LF} cameras.
The results clearly demonstrate the remarkable effectiveness of the suggested model and support \revised{the} proposed approach, which leverages the rich angular information present in SAIs and combines it with the spatial information from the central view. This fusion of data proves to be beneficial for accurately segmenting the central view image.




