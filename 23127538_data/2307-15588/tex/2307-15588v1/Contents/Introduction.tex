\IEEEPARstart{A}{utonomous} driving and intelligent transportation systems heavily rely on computer vision tasks, especially image semantic segmentation, which can produce pixel-level prediction results and contribute to determining the category, shape, and position of objects~\cite{zhou2022mtanet,fan2022mlfnet,zhou2023embedded,yang2023exploring_rgbx}. To effectively apply semantic segmentation and achieve autonomous driving in real-world scenarios, three key factors become crucial: \emph{accuracy}.
%
To enhance the accuracy, advanced models, such as ConvNets~\cite{vandenhende2020mti,guo2022segnext} and MLP-based methods~\cite{tolstikhin2021mlp}, and attention mechanism~\cite{xie2021segformer,zhang2022vsa}, are introduced. Furthermore, different fusion strategies for segmentation~\cite{zhang2018exfuse,zhang2015sensor,hazirbas2017fusenet,zhang2022cmx} are proposed to improve the perception ability.

% Figure environment removed

In this work, we explore the light field (LF) camera, which 
%
can capture both spatial and angular information of the driving scene,
%
making it beneficial for autonomous driving. 
%
Specifically, due to the design of a micro-lens array and image sensor, light field cameras can simultaneously capture spatial information from different directions, yielding visual variations between the central and sub-aperture images.
%
This novel mechanism can be utilized to distinguish those geometrically complex regions.
%
Despite the theoretical benefits,
we observe two under-explored challenges of applying LF cameras in semantic segmentation, as presented in Table~\ref{tab:2 part}: 
(1) high demands on hardware and data processing;
(2) effective use of angular features.
%
In this work, we propose a novel \textbf{Omni-Aperture Fusion (OAFuser)} model, with the aim of addressing the aforementioned two challenges.

%
{First}, high memory cost for redundant sub-aperture and extra efforts related to complex data pre-processing is required to handle LF-based semantic segmentation. For example, assigning a feature extractor to each LF image can over-burden the 
device.
%
The macro-pixel representation in the form of high-resolution images~\cite{wang2022disentangling,jia2021semantic}
%
imposes also significant pressure on device capacity.
%
Especially, using an LF camera with a spatial resolution of $640{\times}480$ and angular resolution of $9{\times}9$ will bring a macro-pixel image with $4320{\times}5760$, imposing $81$ times of data stream along the network module.
%
However, compressing LF images into a depth map~\cite{chen2020bi,sheng2022urbanlf}
%
will additionally increase the complexity of the model.
%
Moreover, the quality of generated depth map~\cite{leistner2022towards,yan2022light} directly impacts the performance of segmentation.
%
To overcome these issues and explore the efficiency of using LF cameras without the need for additional hardware or architectural modifications, we propose a simple yet effective \textbf{Sub-Aperture Fusion Module (SAFM)} that allows our OAFuser network to utilize an arbitrary number of sub-aperture images, so as to achieve efficient semantic segmentation without additional memory cost. Especially, the SAFM can first distinguish the primary and interfering features channel-wise by analyzing the image features from different view directions. Besides, rich information from LF images is embedded into angular feature maps and spatial feature maps and further fed into respective transformer blocks. Thanks to the SAFM module, our OAFuser network can achieve better performance while avoiding excessive memory costs, which is crucial for autonomous driving applications.
%

\begin{table*}[!t]
  \centering
  \renewcommand{\arraystretch}{1.2}
  \begin{adjustbox}{width=1\textwidth}
  \begin{tabular}{c|ccc|ccc}
    \toprule[1mm]
    \multirow{2}{*}{\textbf{Method}}& \multicolumn{3}{c|}{\textbf{Hardware Demand and Extra Efforts}} & \multicolumn{3}{c}{\textbf{The Capacity of the Network}} \\ \cline{2-7}
    & \multicolumn{1}{c|}{\textbf{\#Modality}} & \multicolumn{1}{c|}{\textbf{Memory Cost}} & \textbf{Independent of Depth Map} & \multicolumn{1}{c|}{\textbf{Angular Information}} & \multicolumn{1}{c|}{\textbf{Feature Rectification}} & \textbf{Independent of Image Quality} \\ \midrule[1.5pt]\hline
    RGB Based Network & 1 & n.a. & n.a. & \xmark & \xmark & \xmark \\ \hline
    RGB-D based Network & 2 & n.a. & \xmark & \xmark & \cmark & \xmark \\ \hline
    OCR-LF \cite{sheng2022urbanlf} & More than 2 & \xmark & \cmark & \cmark & \xmark & \xmark \\ \hline
    PSPNet-LF \cite{sheng2022urbanlf} & More than 2 & \xmark & \cmark & \cmark & \xmark & \xmark \\ \hline
    LF-IENet \cite{cong2023combining} & More than 2 & \xmark & Explicit & Implicit & \xmark & \xmark \\ \hline
    OAFuser (ours) & \textbf{Flexible} & \cmark & \cmark & \cmark & \cmark & \cmark \\ \hline
  \end{tabular}
  \end{adjustbox}
  \caption{Challenges of light field semantic segmentation. The \textit{\#Modality} indicates the number of sub-aperture images used. In the \textit{Memory Cost}, \textit{n.a.} denotes that the memory cost also increases with an increase in the number of images. \textit{Independent of Depth Map} indicates whether depth information is required. \textit{Angular Information} column describes the utilization of angular information. Finally, the \textit{Independent of Image Quality} indicates the issue of out-of-focused images from light field cameras. As for LF-IENet, \textit{Disparity Map} is explicitly generated, and \textit{Angular Information} is hidden in the implicit branch.}
  \label{tab:2 part}
\end{table*}

% Figure environment removed

Second, the ability of the network is regarding the performance, \ie, angular feature expression and asymmetric feature rectification. 
%
Previous methods introduce a sub-optimal solution for the light field image representation. While stacking images into an array~\cite{sheng2022urbanlf,yan2022light} contains solely one-dimensional angular information, converting into a video sequence~\cite{zhuang2020video,wang2021temporal} ignores the implicit relationship. Besides, direct merging images or features results in blurred or mixed boundaries,
%
and this phenomenon is particularly severe with the increase of sub-aperture images since the images are captured from different viewpoints. 
Moreover, the images captured by the plenoptic camera~\cite{dansereau2013decoding,bok2016geometric} contain much noise, and the initial light field images are not entirely focused, which can further exacerbate this issue. 
%
In this work, we introduce a \textbf{Center Angular Rectification Module (CARM)} to perform effective rectification between the center view and the aperture-based features. Apart from other strategies in which the features are directly fused with others, our network adopts iteration feature rectification to incorporate the asymmetric angular feature and spatial features from the SAFM before the fusion stage, as shown in Fig.~\ref{fig:fusion strategy}. This design allows our OAFuser network to explore the relationship between different features and has realigned the angular and spatial information, further boosting the segmentation task for road scene understanding.
%

To demonstrate the effectiveness of the proposed OAFuser architecture, we conduct a comprehensive variety of experiments on different light field semantic segmentation datasets.
On the UrbanLF-Real dataset, OAFuser achieves $82.69\%$ in mIoU with an increase of ${+}3.37\%$. On the UrbanLF-Syn dataset, OAFuser surpasses previous works and attains $81.93\%$ in mIoU. Compared to previous state-of-the-art methods, OAFuser reaches the best mIoU of $84.93\%$ with a notable mIoU improvement of ${+}4.53\%$ on the UrbanLF-RealE dataset, while no additional parameters are required.
%

At a glance, we deliver the following contributions:
\begin{compactitem}
    \item We propose a novel Omni-Aperture Fusion model, \ie, \textbf{OAFuser}, to perform light-field semantic segmentation with the arbitrary number of sub-aperture images.  
    %
    \item We design a \textbf{Sub-Aperture Fusion Module (SAFM)} to fuse and embed rich angular information from highly redundant representation, and a \textbf{Center Angular Rectification Module (CARM)} to match and rectify information imbalances caused by variations 
    %
    from different angles.
    \item We verify our method through extensive experiments on  three datasets, \ie, 
    %
    UrbanLF-Real, UrbanLF-Syn, and UrbanLF-RealE, which is a real-world dataset with extended synthetic samples.
\end{compactitem}





