\begin{table}[t!]
  \centering
  \renewcommand{\arraystretch}{1.3}
  \setlength{\tabcolsep}{2pt}
  \begin{adjustbox}{width=0.48\textwidth}
\begin{tabular}{l|c|ccc}
\toprule[1mm]
\textbf{Method} & \textbf{Type} & \textbf{Acc (\%)} & \textbf{mAcc (\%)} & \textbf{mIoU (\%)} \\ \midrule[1.5pt]\hline
{PSPNet~\cite{zhao2017pyramid}} & RGB & 89.39 & 84.48 & 75.78 \\ 
SETR~\cite{zheng2021rethinking} & RGB & 90.97 & 85.26 & 77.69 \\ 
$\text{DeepLabv3}^+$~\cite{chen2018encoder} & RGB & 89.60 & 83.55 & 75.39 \\ 
{OCR~\cite{yuan2020object}} & RGB & 91.50 & 86.96 & 79.36 \\ 
{ACNet~\cite{hu2019acnet}} & RGB-D & 92.53 & 86.62 & 78.56 \\
{MTINet~\cite{vandenhende2020mti}} & RGB-D & 91.24 & 86.94 & 79.10 \\ 
{ESANet~\cite{seichter2021efficient}} & RGB-D & 91.81 & 86.26 & 79.43 \\ 
{SA-Gate~\cite{chen2020bi}} & RGB-D & 92.10 & 87.04 & 79.53 \\ 
{Accel~\cite{jain2019accel}} & Video & 87.56 & 80.52 & 70.48 \\ 
{TDNet~\cite{hu2020temporally}} & Video & 89.06 & 83.43 & 74.71 \\ 
{DAVSS~\cite{zhuang2020video}} & Video & 89.47 & 82.94 & 74.27 \\ 
{TMANet~\cite{wang2021temporal}} & Video & 89.76 & 84.44 & 76.41 \\
{PSPNet-LF~\cite{sheng2022urbanlf}} & LF & 90.55 & 85.91 & 77.88 \\ 
{OCR-LF~\cite{sheng2022urbanlf}} & LF & 92.01 & 87.71 & 80.43 \\ 
{LF-IENet$^4$~\cite{cong2023combining}} & LF & 90.42 & 86.17 & 78.27 \\ 
{LF-IENet$^3$~\cite{cong2023combining}} & LF & {92.41} & {\color[HTML]{FF0000} {\textbf{88.31}}} & {81.78} \\ \hdashline[1pt/1pt]
\textbf{OAFuser9} & LF & 93.23 (+0.82) & 88.26 (-0.05) & 81.64 (-0.14) \\ 
\textbf{OAFuser17} & LF & {\color[HTML]{FF0000} \textbf{93.42 (+1.01)}} & 88.22 (-0.09) & {\color[HTML]{FF0000} \textbf{81.93 (+0.15)}} \\ \hline
\end{tabular}
\end{adjustbox}
\caption{Quantitative results on the UrbanLF-Syn dataset. Acc (\%), mAcc (\%), and mIoU (\%) are reported. The best results are highlighted in red. The variation term indicates the performance difference from the previous best result.}
\label{result:syn}
\end{table}

Table~\ref{result:real} presents the quantitative results on the UrbanLF-Real dataset, which is challenging due to issues like out-of-focus from the plenoptic camera and remaining consistency with light field camera implementation without further data pre-processing in real-world scenarios.
Our OAFuser9 model achieves a state-of-the-art mIoU score of $82.69\%$, showing an improvement of $3.37\%$ compared to previous methods.
Similarly, our OAFuser17 model achieves a mIoU of $82.21\%$, with an increase of $2.89\%$. Regarding Acc and mAcc, both our OAFuser9 and OAFuser17 have improved by over $1\%$ compared to previous works.
The small performance gap between our OAFuser9 and OAFuser17 is attributed to the image quality, which will be discussed in Sec.~\ref{sec:5_DCPC}. 

%
As we increase the number of sub-aperture images, we also observe a corresponding increase in the presence of irrelevant features.
Furthermore, the abundance of out-of-focus and blurry images has led to inaccurate guidance, which surprisingly benefits our network.
From another perspective, it also demonstrates the necessity of our correction module in handling semantic segmentation with light field cameras.
%
The results clearly demonstrate the remarkable effectiveness of the suggested module structure and support our proposed approach, which leverages the rich angular information present in sub-aperture images and combines it with the spatial information from the central view. This fusion of data proves to be beneficial for accurately segmenting the central view image.

