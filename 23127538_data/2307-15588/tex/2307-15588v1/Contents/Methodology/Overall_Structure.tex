% Figure environment removed

This section provides a detailed introduction to our proposed network, \ie, \emph{Omni-Aperture Fuser (OAFuser)}, which is tailored for light field semantic segmentation. The overall OAFuser architecture is presented in Sec.~\ref{sec:3_A_oafuser}. The \emph{Sub-Apeture Fusion Module (SAFM)} for light-field feature aggregation is introduced in Sec.~\ref{sec:3_B_sf}. The \emph{Center Aperture Rectification Module (CARM)} is in Sec.~\ref{sec:3_c_ro}.  
%

% Figure environment removed

\subsection{Proposed OAFuser Architecture}\label{sec:3_A_oafuser}

As shown in Fig.~\ref{fig:Overall}, the proposed OAFuser is constructed with a four-stage encoder and a decoder. The encoder consists of the proposed SAFM and CARM to handle the 
%
feature fusion, feature embedding, and feature rectification, respectively.
For simplicity, the following description is based on stage one, which is the same for the other three stages.
Especially, the arbitrary number of light field images is described as sub-aperture images $F_{s_i} {\in} \mathbb{R}^{H \times W \times 3}$ and central view image $F_c{\in}\mathbb{R}^{H \times W \times 3}$, where $s_i$ is the $i$-th sub-aperture image in range $[1, N]$. All of them are fed into SAFM to embed angular feature $F_{agl}{\in}\mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 64}$ that contains rich angular information and spatial feature $F_{spl}{\in}\mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 64}$ which focus on spatial information for the central view. By applying two different transformer blocks following~\cite{zhang2022cmx}, both of the features transformed into $F_{agl}^*{\in}\mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 64}$ and $F_{spl}^*{\in}\mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 64}$. 
%
After that, the angular and spatial features are concatenated for the next CARM, which includes Horizontal Operation and Vertical Operation for feature rectification along the horizon and vertical direction to eliminate asymmetry. 
%
Specially, the concatenation of $F_{spl}^*$ and $F_{agl}^*$ along the horizon direction is applied to obtain $F_{c1}{\in}\mathbb{R}^{\frac{H}{8} \times \frac{W}{4} \times 64}$. After the Global Rectification and Local Rectification, the horizontally rectified feature $F^{H}{\in}\mathbb{R}^{2 \times \frac{H}{8} \times \frac{W}{8} \times 64}$ is obtained. Then, given the feature $F^{H}$, the feature $F_{c2}{\in}\mathbb{R}^{\frac{H}{4} \times \frac{W}{8} \times 64}$ is obtained by the operation of disentanglement and concatenation along the vertical direction. Especially, $F^{H}{\in}\mathbb{R}^{2 \times \frac{H}{8} \times \frac{W}{8} \times 64}$ is disentangled into $F^{H}_{agl}{\in}\mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 64}$ and $F^{H}_{spl}{\in}\mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 64}$. $F^{H}_{agl}$ and $F^{H}_{spl}$ are further concatenated along vertical direction. After applying Global Rectification and Local Rectification in Vertical Operation, the rectified features $F^{G}{\in}\mathbb{R}^{2 \times \frac{H}{8} \times \frac{W}{8} \times 64}$. $F^{G}_{agl}{\in}\mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 64}$ and $F^{G}_{spl}{\in}\mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times 64}$ is obtained by disentanglement of $F^{G}$. Furthermore, those two rectified features, \ie,  $F^G_{agl}$ and $F^G_{spl}$ are fused by using the FFM module~\cite{zhang2022cmx}, which integrates two different features, to produce the final feature $f_1$ for this stage. The weights between Horizontal Operation and Vertical Operation are shared.

Note that one of our crucial designs is \emph{all sub-aperture information is fed into each stage}, which allows our network to consider all the sub-aperture images throughout the whole process effectively. The pyramidal features $f_1, f_2, f_3, f_4$ are obtained via the four-stage encoder in a dimension of $\{64,128,320,512\}$. Afterward, the multi-stage features are further fed into MLP Decoder~\cite{xie2021segformer} for final prediction.

