In this paper, we explore the potential of light field cameras for scene understanding with semantic segmentation for autonomous driving and intelligent transportation systems.
We propose the Omni-Aperture Fusion (OAFuser) to exploit dense context and angular information from light field apertures. 
We introduce the Sub-Aperture Fusion Module (SAFM), which enables the network to embed angular information from light field cameras without additional memory costs and ensures the feature consistency of angular features.
With the Center Angular Rectification Module (CARM), our network enables the utilization of asymmetric features from different viewpoints. OAFuser sets state-of-art performance compared with existing works on three UrbanLF datasets.

In the future, we strive to establish a new benchmark with more categories and a larger set of training samples to assess the accuracy of using light field cameras for driving scenarios and look into the generalization of OAFuser on more datasets.
