In this section, the overview of semantic segmentation is introduced in Sec.~\ref{sec:2_a_ss}. Since the asymmetric feature
representation from the light field camera, various multi-
modal semantic segmentation works are presented in Sec.~\ref{sec:2_b_mm}.
Furthermore, to boost the angular information expression of
light field cameras, several applications in other areas are also
introduced in Sec.~\ref{sec:2_c_lf}

\subsection{Semantic Segmentation} \label{sec:2_a_ss}
Semantic scene segmentation, as a fundamental task of computer vision, plays a crucial role in scene understanding tasks, such as autonomous driving and intelligent transportation systems~\cite{zhou2022mtanet,zhang2022trans4trans}, by assigning a category to each pixel.
Since FCN~\cite{long2015fully} pioneers the use of convolutional neural networks to replace fully connected networks to propose an end-to-end framework, many segmentation works have emerged based on this approach, and the efficiency and accuracy of segmentation have been largely improved. 
For instance, \cite{chen2018encoder,badrinarayanan2017segnet} adopt an encoder-decoder structure to capture contextual information and local details.
Then, \cite{chen2018encoder,yang2018denseaspp} introduce dilated convolution to increase the receptive field.
To enhance the global context representation, \cite{ding2018context,he2019adaptive} adopt a pyramidal hierarchy in the encoding path.
Furthermore, enhancing prior contextual information~\cite{lin2017refinenet,fu2019dual,wang2021exploring} contributes to improving segmentation results.
Since the introduction of the self-attention mechanism in vision tasks~\cite{dosovitskiy2020image}, many following works~\cite{xie2021segformer,zhang2022vsa,zhang2022trans4trans} propose dense prediction, attention-based models.
Meanwhile, some other works introduce lightweight backbones to speed up the inference~\cite{sandler2018mobilenetv2,tan2019efficientnet,zhang2018shufflenet}.

Although those works demonstrate excellent results in handling dense prediction tasks, they still suffer from the limitation of image quality and lead to performance degeneration in handling complete areas, such as shallow or out-of-fuse areas in real-world self-driving scene understanding scenarios. 

\subsection{Multi-Modal Semantic Segmentation}\label{sec:2_b_mm}
The multiple sub-aperture images captured by the light field camera can be considered as various RGB modalities with inherent relationships. Therefore, the research on multi-modal semantic segmentation is essential for exploring the potential of light field cameras.
ACNet~\cite{hu2019acnet} and EDCNet~\cite{zhang2021exploring} leverage attention connections for facilitating cross-modal interactions in RGB-Depth and RGB-Event semantic segmentation, respectively.
MMFNet~\cite{chen2020mmfnet} enables the fusion of multiple medical images through the aggregation of different features in spatial and channel domains.
NestedFormer~\cite{xing2022nestedformer} proposed a feature aggregation module to fulfill multimodal medical image segmentation.
Furthermore, ESANet~\cite{seichter2021efficient} and SA-Gate~\cite{chen2020bi} utilize depth maps and RGB images to achieve high-accuracy semantic segmentation by employing uniquely designed fusion modules.
PGSNet~\cite{mei2022glass}, which introduces a dynamic integration module, achieves glass segmentation.
Additionally, \cite{sun2019rtfnet,zhou2021gmnet,ha2017mfnet} adopt RGB-thermal image fusion.
Especially, the works of CMX~\cite{zhang2022cmx,zhang2023delivering} present an arbitrary-modal fusion network, which can handle RGB and any other modality, such as depth, thermal, polarization, event, or LiDAR data.
HRFuser~\cite{broedermann2022hrfuser} realizes the fusion of an arbitrary number of additional modalities as supplementary information into RGB images by introducing multi-window cross-attention. 

Different from these methods, which are focused on handling symmetrical modalities, our proposed OAFuser focuses on the utilization of the angular information diversity in light-field images and considers the mismatching from images captured by all sub-apertures of light-field cameras.

\subsection{Light Field Scene Understanding}\label{sec:2_c_lf}
While light-field cameras are still under-explored in semantic segmentation, they have found wide applications in various areas, such as saliency detection~\cite{wang2017two,zhang2017saliency}, depth estimation~\cite{honauer2017dataset,peng2020zero}, and super-resolution~\cite{wang2018lfnet,jin2020light}, due to their rich visual information.
Several works have leveraged the potential of light-field cameras.
FES~\cite{chen2023fusion} achieves sub-aperture feature fusion via spatial and channel attention.
NoiseLF~\cite{feng2022learning} utilizes the all-focus central-view image and its corresponding focal stack, with a unique-designed forgetting matrix and confidence re-weighting strategy, to achieve supervised saliency detection under noisy labels.

Furthermore, several works~\cite{wang2020spatial,liang2023learning,zhang2019residual} employ sub-aperture images, macro-pixel images, epipolar images, or a combination of some of those images to generate high-resolution light field images.
Moreover, AIFLFNet~\cite{zhou2023aif} utilized light field images to estimate depth information. Additionally, SAA-Net~\cite{wu2021spatial} introduces spatial-angular attention modules for light field image reconstruction.
Especially, a design from~\cite{wang2022disentangling} proposes a unified block for handling macro-pixel images, which can be used for both super-resolution and disparity estimation.
For light-field semantic segmentation, the work of~\cite{sheng2022urbanlf} utilizes stacks of sub-aperture images from certain directions achieving the segmentation of central-view images.
LFIE-Net~\cite{cong2023combining} introduces an explicit branch to generate disparity maps within the network and cooperates with the sub-aperture images to achieve dense semantic segmentation.

Unlike existing works which are limited by memory costs when processing light-field images, our proposed OAFuser has the capability to handle an arbitrary number of sub-aperture images without parameter demands for light-field road-scene semantic understanding.
%
