\documentclass[journal]{IEEEtran}
%
    
\usepackage{blindtext}

%-----------------------

\usepackage{amsmath,amssymb,amsfonts,bm}
\usepackage{lipsum}
\usepackage{times}
\usepackage{epsfig}

\usepackage{stfloats}
\usepackage{multicol}
\usepackage[inline]{enumitem}
\usepackage{algorithmic}
\usepackage{graphicx,subcaption}
\usepackage{textcomp}

\usepackage{booktabs}
\usepackage{multirow}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{float}
\usepackage{adjustbox}
\usepackage{array}
\usepackage{dsfont}
%\usepackage{subfig}

\usepackage{pifont}% for marks
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{paralist}
\usepackage[ruled,linesnumbered]{algorithm2e}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  extra package  TF
\usepackage{arydshln}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcommand{\TF}[1]{\textcolor{blue}{#1}}
\usepackage{pifont}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  end extra package  TF


% \usepackage{flushend}
% \usepackage{orcidlink}

%
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.

\usepackage{tabulary}
\usepackage[table]{xcolor}
\newcommand\ver[1]{\rotatebox[origin=c]{90}{#1}}
\newcommand{\fl}[1]{\multicolumn{1}{c}{#1}}
\definecolor{gray}{rgb}{0.3,0.3,0.3}
\definecolor{blue}{rgb}{0,0.5,1}
\definecolor{mask_red}{rgb}{1,0,0.8}
\definecolor{green}{rgb}{0.2,1,0.2}
\definecolor{rblue}{rgb}{0,0,1}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}
\newcommand{\green}[1]{\textcolor[RGB]{96,177,87}{#1}}
\newcommand{\fn}[1]{\footnotesize{#1}}
\newcommand{\gbf}[1]{\green{\bf{\fn{(#1)}}}}
\newcommand{\rbf}[1]{\gray{\bf{\fn{(#1)}}}}

\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=red,citecolor=blue}

\newcommand{\YKL}[1]{\textcolor{orange}{#1}}
\newcommand{\PKY}[1]{\textcolor{purple}{#1}}
\newcommand{\revised}[1]{\textcolor{orange}{#1}}


% Add a period to the end of an abbreviation unless there's one
% already, then \xspace.
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}

\begin{document}

\title{OAFuser: Towards Omni-Aperture Fusion for Light Field Semantic Segmentation of Road Scenes}
\author{Fei Teng\IEEEauthorrefmark{1}, Jiaming Zhang\IEEEauthorrefmark{1}, Kunyu Peng, Kailun Yang\IEEEauthorrefmark{2}, Yaonan Wang, and Rainer Stiefelhagen%
\IEEEcompsocitemizethanks{
\IEEEcompsocthanksitem 
This work was supported in part by the Ministry of Science, Research and the Arts of Baden-WÃ¼rttemberg (MWK) through the Cooperative Graduate School Accessibility through AI-based Assistive Technology (KATE) under Grant BW6-03, in part by the University of Excellence through the ``KIT Future Fields'' project, in part by the Helmholtz Association Initiative and Networking Fund on the HAICORE@KIT partition, and in part by Hangzhou SurImage Technology Company Ltd.
\IEEEcompsocthanksitem F. Teng, J. Zhang, K. Peng, and R. Stiefelhagen are with the Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, 76131 Karlsruhe, Germany.
\IEEEcompsocthanksitem K. Yang and Y. Wang are with the School of Robotics and the National Engineering Research Center of Robot Visual Perception and Control Technology, Hunan University, Changsha 410082, China.
%(E-Mail: kailun.yang@hnu.edu.cn.)
\IEEEcompsocthanksitem \IEEEauthorrefmark{1}Equal contribution.
\IEEEcompsocthanksitem \IEEEauthorrefmark{2}Corresponding author (E-Mail: kailun.yang@hnu.edu.cn.).
}%
}

\maketitle
\bstctlcite{IEEEexample:BSTcontrol}
%
%%%%%%%%% ABSTRACT
\begin{abstract}
\input{Contents/Abstract}

\end{abstract}

%
\begin{IEEEkeywords} 
Semantic segmentation, light field, scene parsing, vision transformers, scene understanding.
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle
%%%%%%%%% BODY TEXT
\section{Introduction}
\input{Contents/Introduction}


\section{Related Work}

\input{Contents/Related_Work}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Methodology %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Methodology}
\input{Contents/Methodology/Overall_Structure}

\subsection{Sub-Aperture Fusion Module}\label{sec:3_B_sf}
\input{Contents/Methodology/SAFM}

\subsection{Central Angular Rectification Module}\label{sec:3_c_ro}
\input{Contents/Methodology/CARM}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Experiment Result %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiment Results}
\subsection{Datasets}\label{sec:4_1_da}
\input{Contents/Experiment_Results/Datasets}

\subsection{Implementation Details}
\input{Contents/Experiment_Results/Implement_Details}

\subsection{Quantitative Results}
To verify our methods, we compare OAFuser with other methods, which include RGB-based methods~\cite{chen2018encoder,zhao2017pyramid,zheng2021rethinking,yuan2020object}, video-based light field semantic segmentation methods~\cite{zhuang2020video,wang2021temporal,jain2019accel,hu2020temporally}, and several specific designs for light field semantic segmentation~\cite{sheng2022urbanlf,cong2023combining} on three datasets, \ie, UrbanLF-Real, UrbanLF-Syn, UrbanLF-RealE. 

\subsubsection{Results on UrbanLF-Real} \
\input{Contents/Experiment_Results/Result_on_UrbanLF_Real/Result_on_UrbanLF-Real}

\subsubsection{Results on UrbanLF-Syn} \
\input{Contents/Experiment_Results/Result_on_UrbanLF_Syn/Result_on_UrbanLF-Syn}

\subsubsection{Results on UrbanLF-RealE} \
\input{Contents/Experiment_Results/Result_on_UrbanLF_Real_E/Result_on_UrbanLF-Real_E}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Ablation Study %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ablation Studies}
%
{In this section, we conduct several ablation studies to validate the influence of various modules within our proposed method. The experiments are carried out in Sec.~\ref{sec:5_a_model} to thoroughly investigate the effects of diverse components incorporated in our method. Additionally, we conduct several experiments in Sec.~\ref{sec:5_a_ma} to determine the optimal combination in the CARM. Furthermore, we compare the impact of different datasets in Sec.~\ref{sec:5_DCPC} and analyze the per-class performance in Sec.~\ref{sec:5_c_per}. Moreover, the exploration of the contributions of sub-aperture images is conducted in Sec.~\ref{sec:5_d_select}.}

\subsection{Ablation Study for the Overall Model}\label{sec:5_a_model}
\input{Contents/Ablation_Study/Model_Ablation}  

\subsection{Ablation Study for CARM} \label{sec:5_a_ma}
\input{Contents/Ablation_Study/Attention_Ablation}

\subsection{Dataset Comparison} \label{sec:5_DCPC}

%

\input{Contents/Ablation_Study/Dataset_Comparison}

\subsection{Per-Class Accuracy Analysis} \label{sec:5_c_per}\
\input{Contents/Ablation_Study/Per_class_accuracy_analysis} 

\subsection{Investigation on the Selection of Sub-Aperture Images} \label{sec:5_d_select}
\input{Contents/Ablation_Study/Selection_of_Sub_aperture_image}

\section{Conclusion}
\input{Contents/Conclusion}

\bibliographystyle{IEEEtran}
\bibliography{bib}

\end{document}
