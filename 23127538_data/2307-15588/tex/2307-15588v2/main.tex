\documentclass[journal]{IEEEtran}
%
    
\usepackage{blindtext}

%-----------------------

\usepackage{amsmath,amssymb,amsfonts,bm}
\usepackage{lipsum}
\usepackage{times}
\usepackage{epsfig}

\usepackage{stfloats}
\usepackage{multicol}
\usepackage[inline]{enumitem}
\usepackage{algorithmic}
\usepackage{graphicx,subcaption}
\usepackage{textcomp}

\usepackage{booktabs}
\usepackage{multirow}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{float}
\usepackage{adjustbox}
\usepackage{array}
\usepackage{dsfont}

%\usepackage{subfig}

\usepackage{pifont}% for marks
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{paralist}
\usepackage[ruled,linesnumbered]{algorithm2e}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  extra package  TF
\usepackage{arydshln}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcommand{\TF}[1]{\textcolor{blue}{#1}}
\usepackage{pifont}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  end extra package  TF


% \usepackage{flushend}
% \usepackage{orcidlink}

%
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.

\usepackage{tabulary}
\usepackage[table]{xcolor}
\newcommand\ver[1]{\rotatebox[origin=c]{90}{#1}}
\newcommand{\fl}[1]{\multicolumn{1}{c}{#1}}
\definecolor{gray}{rgb}{0.3,0.3,0.3}
\definecolor{blue}{rgb}{0,0.5,1}
\definecolor{mask_red}{rgb}{1,0,0.8}
\definecolor{green}{rgb}{0.2,1,0.2}
\definecolor{rblue}{rgb}{0,0,1}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}
\newcommand{\green}[1]{\textcolor[RGB]{96,177,87}{#1}}
\newcommand{\fn}[1]{\footnotesize{#1}}
\newcommand{\gbf}[1]{\green{\bf{\fn{(#1)}}}}
\newcommand{\rbf}[1]{\gray{\bf{\fn{(#1)}}}}

\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=red,citecolor=blue}

\newcommand{\YKL}[1]{\textcolor{orange}{#1}}
\newcommand{\PKY}[1]{\textcolor{purple}{#1}}
\newcommand{\revised}[1]{\textcolor{orange}{#1}}


% Add a period to the end of an abbreviation unless there's one
% already, then \xspace.
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}

\begin{document}

\title{OAFuser: Towards Omni-Aperture Fusion for Light Field Semantic Segmentation}
\author{Fei Teng\IEEEauthorrefmark{1}, Jiaming Zhang\IEEEauthorrefmark{1}, Kunyu Peng, Yaonan Wang, Rainer Stiefelhagen, and Kailun Yang\IEEEauthorrefmark{2}%
\IEEEcompsocitemizethanks{
\IEEEcompsocthanksitem 
This work was supported in part by the Ministry of Science, Research and the Arts of Baden-WÃ¼rttemberg (MWK) through the Cooperative Graduate School Accessibility through AI-based Assistive Technology (KATE) under Grant BW6-03, in part by the University of Excellence through the ``KIT Future Fields'' project, in part by the Helmholtz Association Initiative and Networking Fund on the HAICORE@KIT partition, and in part by Hangzhou SurImage Technology Company Ltd.
\IEEEcompsocthanksitem F. Teng, Y. Wang, and K. Yang are with the School of Robotics and the National Engineering Research Center of Robot Visual Perception and Control Technology, Hunan University, Changsha 410082, China.
\IEEEcompsocthanksitem J. Zhang, K. Peng, and R. Stiefelhagen are with the Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, 76131 Karlsruhe, Germany.
%(E-Mail: kailun.yang@hnu.edu.cn.)
\IEEEcompsocthanksitem \IEEEauthorrefmark{1}Equal contribution.
\IEEEcompsocthanksitem \IEEEauthorrefmark{2}Corresponding author (E-Mail: kailun.yang@hnu.edu.cn.).
}%
}

\maketitle
\bstctlcite{IEEEexample:BSTcontrol}
%
%%%%%%%%% ABSTRACT
\begin{abstract}
\input{Contents/Abstract}



\end{abstract}

\begin{IEEEImpStatement}
\input{Contents/Statement}
\end{IEEEImpStatement}
%
\begin{IEEEkeywords} 
Semantic segmentation, light field, scene parsing, vision transformers, scene understanding.
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle
%%%%%%%%% BODY TEXT
\section{Introduction}
\input{Contents/Introduction}

\section{Related Work}

\input{Contents/Related_Work}

\section{Methodology}
\input{Contents/Methodology/Overall_Structure}

\subsection{Sub-Aperture Fusion Module}\label{sec:3_B_sf}
\input{Contents/Methodology/SAFM}

\subsection{Central Angular Rectification Module}\label{sec:3_c_ro}
\input{Contents/Methodology/CARM}

\section{Experiment Results}
\subsection{Datasets}\label{sec:4_1_da}
\input{Contents/Experiment_Results/Datasets}

\subsection{Implementation Details}
\input{Contents/Experiment_Results/Implement_Details}

\subsection{Quantitative Results}

\begin{table}[t]
\centering
\renewcommand{\arraystretch}{1.4}
\setlength{\tabcolsep}{2pt}
\begin{adjustbox}{width=0.48\textwidth}
\centering
\begin{tabular}{l|c| cc| c}
\toprule[1mm]
\textbf{Method} & \textbf{Type} & \textbf{Acc (\%)} & \textbf{mAcc (\%)} & \textbf{mIoU (\%)} \\ \midrule[1.5pt]\hline

PSPNet~\cite{zhao2017pyramid} & RGB & 91.21 & 83.87 & 76.34 \\
$\text{DeepLabv3}^+$~\cite{chen2018encoder} & RGB & 91.02 & 83.53 & 76.27  \\
{SETR}~\cite{zheng2021rethinking} & RGB & 92.16 & 84.27 & 77.74\\ 
{OCR}~\cite{yuan2020object} & RGB & 92.02 & 85.17 & 78.60 \\ 
{Accel}~\cite{jain2019accel} & Video & 89.15 & 80.69 & 71.64 \\ 
{TDNet}~\cite{hu2020temporally} & Video & 91.05 & 83.38 & 76.48 \\ 
{DAVSS}~\cite{zhuang2020video} & Video & 91.04 & 83.54 & 75.91 \\ 
{TMANet}~\cite{wang2021temporal} & Video & 91.67 & 84.13 & 77.14 \\ 
{PSPNet-LF}~\cite{sheng2022urbanlf} & LF & 92.14 & 84.86 & 78.10 \\
{OCR-LF}~\cite{sheng2022urbanlf} & LF & 92.51 & 86.31 & 79.32 \\ 
{LF-IENet$^4$}~\cite{cong2023combining} & LF & 92.01 & 85.10 & 78.09 \\ 
{LF-IENet$^3$}~\cite{cong2023combining} & LF & 92.09 & 86.03 & 79.19 \\ \hdashline[1pt/1pt]
\textbf{OAFuser9} & LF & {\color[HTML]{FF0000}\textbf{94.45 (+1.94)}} & {\color[HTML]{FF0000}\textbf{88.21 (+1.90)}} & {\color[HTML]{FF0000}\textbf{82.69 (+3.37)}} \\ 
\textbf{OAFuser17} & LF & 94.08 (+1.52) & 87.74 (+1.43) & 82.21 (+2.89) \\ 
\hline %\bottomrule[1mm]
\end{tabular}
\end{adjustbox}
\caption{Quantitative results on the UrbanLF-Real dataset. Acc (\%), mAcc (\%), and mIoU (\%) are reported. The best results are highlighted in red. The variation term indicates the performance difference from the previous best result.}
\label{result:real}
\end{table}

To verify our methods, we compare OAFuser with other methods, which include RGB-based methods~\cite{chen2018encoder,zhao2017pyramid,zheng2021rethinking,yuan2020object}, video-based light field semantic segmentation methods~\cite{zhuang2020video,wang2021temporal,jain2019accel,hu2020temporally}, and several specific designs for light field semantic segmentation~\cite{sheng2022urbanlf,cong2023combining} on three datasets, \ie, UrbanLF-Real, UrbanLF-Syn, and UrbanLF-RealE. 

\subsubsection{Results on UrbanLF-Real} 
\input{Contents/Experiment_Results/Result_on_UrbanLF_Real/Result_on_UrbanLF_Real}

\subsubsection{Results on UrbanLF-Syn} 
\input{Contents/Experiment_Results/Result_on_UrbanLF_Syn/Result_on_UrbanLF_Syn}

\subsubsection{Results on UrbanLF-RealE} 
\input{Contents/Experiment_Results/Result_on_UrbanLF_RealE/Result_on_UrbanLF_RealE}

\section{Ablation Studies}
{{In this section, we conduct several ablation studies to confirm the impact of different modules in our proposed method and to demonstrate the contribution of various sub-aperture images from the light field camera to the understanding of the urban scene.} The experiments are carried out in Sec.~\ref{sec:5_a_model} to thoroughly investigate the effects of diverse components incorporated in our method. Additionally, we conduct several experiments in Sec.~\ref{sec:5_a_ma} to determine the optimal combination in the CARM. Furthermore, we compare the impact of different datasets in Sec.~\ref{sec:5_DCPC} and analyze the per-class performance in Sec.~\ref{sec:5_c_per}. Moreover, the exploration of the contributions of sub-aperture images is conducted in Sec.~\ref{sec:5_d_select}.}

\subsection{Ablation Study for the Overall Model}\label{sec:5_a_model}
\input{Contents/Ablation_Study/Model_Ablation}  

\subsection{Ablation Study for CARM} \label{sec:5_a_ma}
\input{Contents/Ablation_Study/Attention_Ablation}

\subsection{Dataset Comparison} \label{sec:5_DCPC}

\input{Contents/Ablation_Study/Dataset_Comparison}

\subsection{Per-Class Accuracy Analysis} \label{sec:5_c_per}\
\input{Contents/Ablation_Study/Per_class_accuracy_analysis} 

\subsection{Investigation on the Selection of Sub-Aperture Images} \label{sec:5_d_select}
\input{Contents/Ablation_Study/Selection_of_Sub_aperture_image}

\section{Conclusion}
\input{Contents/Conclusion}

\bibliographystyle{IEEEtran}
\bibliography{bib}

\end{document}
