In this paper, we explore the potential of light field cameras for road scene understanding with semantic segmentation.
%
We propose a new, innovative paradigm, the Omni-Aperture Fusion (OAFuser), to exploit dense context and angular information from light field apertures effectively. 
We introduce the Sub-Aperture Fusion Module (SAFM), which enables the network to embed angular information from light field cameras without additional memory costs and ensures the feature consistency of angular features.
With the Center Angular Rectification Module (CARM), our network enables the utilization of asymmetric features from different viewpoints. OAFuser sets state-of-art performance compared with existing works on three UrbanLF datasets. The proposed framework breaks the limitation introduced by redundant data and establishes a new baseline for the further exploration of LF.



In the future, we strive to establish a new benchmark with more categories and a larger set of training samples to assess the accuracy of using light field cameras for multiple scenarios. Furthermore, the plan of visiting other computer vision tasks based on light field cameras is essential, since the application of light field cameras in visual tasks is still an under-explored domain. Summarily, we hope to inspire more interest in light field cameras and advance their development in computer vision community.