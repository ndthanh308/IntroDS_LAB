Recently, to enhance scene understanding tasks, multiple works have introduced depth or thermal information. Inspired by these works, few attempts have implemented semantic segmentation based on light field cameras, which provide both spatial information (RGB image) and angular information (image array along angular coordinates). However, the abundance of information simultaneously restricts the application of light field cameras in dense predictive tasks. In light of this, we propose a pioneering and effective framework that addresses the issue of increasing memory costs as the number of sub-aperture images grows. This has practical value for the exploration and application of light field cameras. The proposed method provides a scalable solution for scene parsing tasks based on light field cameras. This may inspire more members of the community to pay attention to and develop light field cameras.