
The quantitative results on the synthetic dataset are shown in Table~\ref{result:syn}.  
Among all the models assessed, OAFuser17 attains the highest mIoU. Comparatively, OAFuser9 exhibits slightly inferior performance to LF-IENet$^3$. This discrepancy in performance can be primarily attributed to the restricted size of the synthetic dataset, encompassing merely $173$ samples for training purposes.
% Figure environment removed
In concrete terms, OAFuser makes use of the small differences present among different sub-aperture images to capture intricate angular details. These details are then combined with the appropriate spatial information obtained from the main viewpoint. This approach differs from previous methods that mainly focus on spatial information limited to the central view and overlook the angular information available from the {LF} camera.
{Additionally, a key feature of OAFuser is its ability to eliminate the need for pre-processing of initial {LF} images, showcasing its capability to handle raw {LF} data by effectively utilizing various angular information.} {Importantly, the network's processing capabilities are not adequately demonstrated when using entirely focused images from synthetic datasets.} 
Furthermore, it can be clearly seen from Fig.~\ref{fig:Qualitative result} in areas of complex structures, such as the leaves and bicycle pillion seats. Compared with LF-IENet, which leads to loss of global context for objects, such as the gaps between objects and their connection with branches being disrupted, our approach demonstrates better segmentation results even with $17$ sub-aperture images. {This underscores the superior capability of our network in harnessing sub-aperture images; through sophisticated rectification, the OAFuser consistently synchronizes with the rich tapestry of information gleaned from diverse perspectives.} This claim has also been supported by experiments conducted on both the UrbanLF-Real and the UrbanLF-RealE datasets.



