\begin{table}[t!]
  \centering
  \renewcommand{\arraystretch}{1.4}
  \setlength{\tabcolsep}{2pt}
  \begin{adjustbox}{width=0.48\textwidth}
\begin{tabular}{l|c|ccc}
\toprule[1mm]
\textbf{Method} & \textbf{Type} & \textbf{Acc (\%)} & \textbf{mAcc (\%)} & \textbf{mIoU (\%)} \\ \midrule[1.5pt]\hline
{PSPNet~\cite{zhao2017pyramid}} & RGB & 91.73 & 84.47 & 77.71 \\
$\text{DeepabV3}^+$~\cite{chen2018encoder} & RGB & 91.33 & 83.86 & 76.70 \\ 
{SETR~\cite{zheng2021rethinking}} & RGB & 92.72 & 85.55 & 79.06 \\ 
{OCR~\cite{yuan2020object}} & RGB & 92.56 & 86.56 & 79.90 \\ 
{Accel~\cite{jain2019accel}} & Video & 89.40 & 82.30 & 72.85 \\ 
{TDNet~\cite{hu2020temporally}} & Video & 91.48 & 84.25 & 77.52 \\ 
{DAVSS~\cite{zhuang2020video}} & Video & 91.96 & 85.21 & 77.31 \\
{TMANet~\cite{wang2021temporal}} & Video & 91.84 & 84.81 & 78.13 \\
{PSPNet-LF~\cite{sheng2022urbanlf}} & LF & 92.69 & 86.01 & 79.45 \\
{OCR-LF~\cite{sheng2022urbanlf}} & LF & 92.95 & 86.94 & 80.40 \\ \hdashline[1pt/1pt]
\textbf{OAFuser9} & LF & \color[HTML]{FF0000} \textbf{94.61 (+1.66)} & \color[HTML]{FF0000} \textbf{89.84 (+2.90)} & \color[HTML]{FF0000} \textbf{84.93 (+4.53)} \\ 
\textbf{OAFuser17} & LF & 93.74 (+0.79) & 88.92 (+1.98) & 82.42 (+2.02) \\ \hline
\end{tabular}
\end{adjustbox}
\caption{Quantitative results on the UrbanLF-RealE dataset. Acc (\%), mAcc (\%), and mIoU (\%) are reported. The best results are highlighted in red. The variation term indicates the performance difference from the previous best result.}
\label{result:exten}
\end{table}

UrbanLF-RealE, which involves not only real-world samples but also extends to multiple synthetic samples, poses significant challenges to the performance of the model.
This complex combination of data is more aligned with unconstrained scenarios.
As shown in Table~\ref{result:exten}, OAFuser achieves state-of-the-art performance with mIoU of $84.93\%$.
The result exceeds all the existing methods by more than $4.53\%$. The accuracy of OAFuser17 decreases compared to OAFuser9, which is caused by the challenges posed by a large number of sub-aperture images in complex scenarios.
Due to the increasing number of sub-aperture images, it becomes challenging to accurately distinguish between relevant and irrelevant features in a dataset where focused and defocused images are mixed together.
Compared with PSPNet-LF and OCR-LF, which implement the direct fusion of sub-aperture and central-view images, our OAFuser embeds the rich angular feature in an independent branch and focuses on utilizing both spatial and angular features.

Above all, the excellent performance on the UrbanLF-RealE dataset demonstrates the superiority of our design and sets a new record of {LF} semantic segmentation.
The network showcases remarkable accuracy without relying on extensive image preprocessing and additional devices. This validates the applicability of our network in real-world scenarios.
