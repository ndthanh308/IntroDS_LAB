In this section, the overview of semantic segmentation is introduced in Sec.~\ref{sec:2_a_ss}, while our work is a pixel-level classification task for urban scenes. Since the enormous and asymmetric image representation from different microlens of the LF camera, various multi-modal semantic segmentation works are presented in Sec.~\ref{sec:2_b_mm}. Furthermore, to boost the angular information expression and better use the asymmetric information of LF cameras, several applications for LF cameras that have given us a great deal of inspiration are also thoroughly introduced. in Sec.~\ref{sec:2_c_lf}.

\subsection{Semantic Segmentation} \label{sec:2_a_ss}
Semantic scene segmentation, as a fundamental task of computer vision, plays a crucial role in scene understanding tasks, such as autonomous driving and intelligent transportation systems~\cite{zhou2022mtanet,zhang2022trans4trans}, by assigning a category to each pixel.
Since FCN~\cite{long2015fully} pioneers the use of convolutional neural networks to replace fully connected networks to propose an end-to-end framework, many segmentation works have emerged based on this approach, and the efficiency and accuracy of segmentation have been vastly improved. 
For instance, \cite{chen2018encoder,badrinarayanan2017segnet} adopt an encoder-decoder structure to capture contextual information and local details.
Then, \cite{chen2018encoder,yang2018denseaspp} introduce dilated convolution to increase the receptive field.
To enhance the global context representation, \cite{ding2018context,he2019adaptive} adopt a pyramidal hierarchy in the encoding path.
Furthermore, enhancing prior contextual information~\cite{lin2017refinenet,fu2019dual,wang2021exploring} contributes to improving segmentation results.
Since the introduction of the self-attention mechanism in vision tasks~\cite{dosovitskiy2020image}, many following works~\cite{xie2021segformer,zhang2022vsa,zhang2022trans4trans} propose dense prediction, attention-based models.
Meanwhile, some other works introduce lightweight backbones to speed up the inference~\cite{sandler2018mobilenetv2,tan2019efficientnet,zhang2018shufflenet}.

Although those studies demonstrate excellent results in handling dense prediction tasks, they still suffer from limitations in image quality, leading to performance degradation in handling complete areas, such as shallow or out-of-focus areas in real-world driving scene understanding scenarios. LF cameras, unlike monocular cameras, are equipped with an omni-aperture feature. Therefore, focusing on monocular RGB semantic segmentation that emphasizes single-image spatial features fails to utilize the angular information provided by LF cameras. Additionally, the sub-aperture images produced by these cameras contain a significant amount of noise.


\subsection{Multi-Modal Semantic Segmentation}\label{sec:2_b_mm}
The multiple sub-aperture images captured by the LF camera can be considered as various RGB modalities with inherent relationships. Therefore, the research on multi-modal semantic segmentation is essential for exploring the potential of LF cameras.
ACNet~\cite{hu2019acnet} and EDCNet~\cite{zhang2021exploring} leverage attention connections for facilitating cross-modal interactions in RGB-Depth and RGB-Event semantic segmentation, respectively.
MMFNet~\cite{chen2020mmfnet} enables the fusion of multiple medical images by aggregating different features in spatial and channel domains.
NestedFormer~\cite{xing2022nestedformer} proposed a feature aggregation module to fulfill multi-modal medical image segmentation.
Furthermore, ESANet~\cite{seichter2021efficient} and SA-Gate~\cite{chen2020bi} utilize depth maps and RGB images to achieve high-accuracy semantic segmentation by employing uniquely designed fusion modules.
PGSNet~\cite{mei2022glass}, which introduces a dynamic integration module, achieves glass segmentation.
Additionally, \cite{sun2019rtfnet,zhou2021gmnet,ha2017mfnet} adopt RGB-thermal image fusion.
Significantly, the works of CMX~\cite{zhang2022cmx,zhang2023delivering} present an arbitrary-modal fusion network, which can handle RGB and any other modality, such as depth, thermal, polarization, event, or LiDAR data.
HRFuser~\cite{broedermann2022hrfuser} realizes the fusion of an arbitrary number of additional modalities as supplementary information into RGB images by introducing multi-window cross-attention. 

However, these methods, which are focused on handling symmetrical modalities, are not suitable for pixel-wise prediction tasks in LF images. This is because the pixel shifting between different sub-aperture images caused by micro-lenses can adversely affect semantic segmentation results. Our proposed OAFuser concentrates on utilizing the diverse angular information in LF images and considers the mismatching from images captured by all sub-apertures of LF cameras.

\subsection{Light Field Scene Understanding}\label{sec:2_c_lf}
Traditional cameras require the focal length to be set in advance during shooting. However, with LF cameras, users can adjust the focus point and refocus after capturing, meaning users can decide which part of the image is sharp and which part is blurred after the shot~\cite{Ruan_2022_CVPR,yang2023joint}. Due to their rich visual information, LF cameras have found wide applications in various areas, such as saliency detection~\cite{wang2017two,zhang2017saliency}, depth estimation~\cite{honauer2017dataset,peng2020zero}, and super resolution~\cite{wang2018lfnet,jin2020light}. Meanwhile, research in other related communities is crucial to our work, as light-field cameras are still an under-explored area in the vision community.
%While light-field cameras are still under-explored in semantic segmentation, they have found wide applications in various areas, such as saliency detection~\cite{wang2017two,zhang2017saliency}, depth estimation~\cite{honauer2017dataset,peng2020zero}, and super-resolution~\cite{wang2018lfnet,jin2020light}, due to their rich visual information.
%Several works have leveraged the potential of light-field cameras.

FES~\cite{chen2023fusion} achieves sub-aperture feature fusion via spatial and channel attention.~
NoiseLF~\cite{feng2022learning} utilizes the all-focus central-view image and its corresponding focal stack, with a unique-designed forgetting matrix and confidence re-weighting strategy, to achieve supervised saliency detection under noisy labels.~
Furthermore, several works~\cite{wang2020spatial,liang2023learning,zhang2019residual} employ sub-aperture images, macro-pixel images, epipolar images, or a combination of some of those images to generate high-resolution LF images.~Moreover, AIFLFNet~\cite{zhou2023aif} utilized LF images to estimate depth information. Additionally, SAA-Net~\cite{wu2021spatial} introduces spatial-angular attention modules for LF image reconstruction.~Especially, a design from~\cite{wang2022disentangling} proposes a unified block for handling macro-pixel images, which can be used for both super-resolution and disparity estimation.
Furthermore, \cite{9442895} has also implemented LF technology for image reconstruction in the autonomous community.

For light-field semantic segmentation, the work of~\cite{sheng2022urbanlf} utilizes stacks of sub-aperture images from certain directions, achieving the segmentation of central-view images.
LFIE-Net~\cite{cong2023combining} introduces an explicit branch to generate disparity maps within the network and cooperates with the sub-aperture images to achieve dense semantic segmentation. Specifically, multiple applications of LF technology in autonomous vehicles have been introduced, showing promising performance. 

However, the high resolution of LF images presents a significant challenge due to memory costs for these methods. Unlike existing works, our proposed OAFuser has the capability to handle an arbitrary number of sub-aperture images without the demand for increased parameters in the context of light-field road-scene semantic understanding.
