\IEEEPARstart{I}{ntelligent} agents, such as robotics and autonomous driving systems, heavily rely on visual understanding tasks, especially image semantic segmentation, which can produce pixel-level prediction results and contribute to determining the category, shape, and position of objects~\cite{lu2022surrogate,mazhar2023rethinking,liu2023testtime_adaptation}. To effectively apply semantic segmentation in real-world scenarios, \emph{accuracy} is one of the most crucial factors.
%
To enhance the accuracy, advanced models, such as ConvNets~\cite{vandenhende2020mti,guo2022segnext} and MLP-based methods~\cite{tolstikhin2021mlp}, and attention mechanism~\cite{xie2021segformer,zhang2022vsa}, are introduced. Furthermore, different fusion strategies for segmentation~\cite{zhang2018exfuse,zhang2015sensor,hazirbas2017fusenet,zhang2022cmx} are proposed to improve the perception ability.

% Figure environment removed

In this work, we introduced Light Field (LF) cameras~\cite{georgiev2006light}. 
These cameras are equipped with a unique optical system, referred to as an ``omni-aperture'', which indicates their ability to utilize all available apertures to capture comprehensive light information. They record the volumetric radiance of a scene, surpassing the capabilities of traditional photography by capturing more complex data, which includes information beyond the standard two-dimensional plane. Specifically, LF cameras retain details about the angle of incoming light rays, effectively capturing four-dimensional light-field data. For autonomous driving, the ability of LF cameras to simultaneously capture spatial information from multiple directions allows for a more nuanced understanding of the driving environment. The variations captured between the central and sub-aperture images provide rich visual details, especially beneficial for identifying geometrically complex regions. By harnessing both the spatial and angular data, LF technology can facilitate shifting focus across different planes after the fact, enabling refined visual computations critical for dense vision tasks such as those required in robotics vision perception~\cite{meilland2015dense}. 
%

Despite the theoretical benefits,
we observe two under-explored challenges of applying LF cameras in scenes semantic segmentation task, as presented in Table~\ref{tab:2 part}: 
(1) high demands on hardware and data processing;
(2) effective use of angular features.
In this work, we propose a novel \textbf{Omni-Aperture Fusion (OAFuser)} model to address those challenges above, thereby contributing to the community.

{First}, high memory costs for redundant sub-aperture information and extra efforts related to complex data pre-processing are required to handle LF-based semantic segmentation. For example, assigning a feature extractor to each LF image can over-burden the 
device.
Especially, using an LF camera with a spatial resolution of $640{\times}480$ and angular resolution of $9{\times}9$ will bring a macro-pixel image with $4320{\times}5760$, imposing $81$ times of data stream along the network module.
%
However, compressing LF images into a depth map~\cite{chen2020bi,sheng2022urbanlf}
%
will additionally increase the complexity of the model.
Moreover, the quality of the generated depth map~\cite{leistner2022towards,yan2022light} directly impacts the segmentation performance.
%
To overcome these issues and explore the efficiency of using LF cameras without the need for additional hardware or architectural modifications, we propose a simple yet effective \textbf{Sub-Aperture Fusion Module (SAFM)} that allows our OAFuser network to utilize an arbitrary number of sub-aperture images, so as to achieve efficient semantic segmentation without additional memory cost. Especially, the SAFM can distinguish the primary and interfering features channel-wise by analyzing the image features from different viewpoints. Besides, rich information from LF images is embedded into angular feature maps and spatial feature maps and further fed into respective transformer blocks. Thanks to the SAFM module, our OAFuser network can achieve better performance while avoiding excessive memory costs.
%

\begin{table*}[!t]
  \centering
  \renewcommand{\arraystretch}{1.2}
  \begin{adjustbox}{width=1\textwidth}
  \begin{tabular}{c|ccc|ccc}
    \toprule[1mm]
    \multirow{2}{*}{\textbf{Method}}& \multicolumn{3}{c|}{\textbf{Hardware Demand and Extra Efforts}} & \multicolumn{3}{c}{\textbf{The Capacity of the Network}} \\ \cline{2-7}
    & \multicolumn{1}{c|}{\textbf{\#Modality}} & \multicolumn{1}{c|}{\textbf{Memory Cost}} & \textbf{Independent of Depth Map} & \multicolumn{1}{c|}{\textbf{Angular Information}} & \multicolumn{1}{c|}{\textbf{Feature Rectification}} & \textbf{Independent of Image Quality} \\ \midrule[1.5pt]\hline
    RGB Based Network & 1 & n.a. & n.a. & \xmark & \xmark & \xmark \\ \hline
    RGB-D based Network & 2 & n.a. & \xmark & \xmark & \cmark & \xmark \\ \hline
    OCR-LF \cite{sheng2022urbanlf} & More than 2 & \xmark & \cmark & \cmark & \xmark & \xmark \\ \hline
    PSPNet-LF \cite{sheng2022urbanlf} & More than 2 & \xmark & \cmark & \cmark & \xmark & \xmark \\ \hline
    LF-IENet \cite{cong2023combining} & More than 2 & \xmark & Explicit & Implicit & \xmark & \xmark \\ \hline
    OAFuser (ours) & \textbf{Flexible} & \cmark & \cmark & \cmark & \cmark & \cmark \\ \hline
  \end{tabular}
  \end{adjustbox}
  \caption{Challenges of light field semantic segmentation. The \textit{\#Modality} indicates the number of sub-aperture images used. In the \textit{Memory Cost}, \textit{n.a.} denotes that the memory cost also increases with an increase in the number of images. \textit{Independent of Depth Map} indicates whether depth information is required. \textit{Angular Information} column describes the utilization of angular information. Finally, the \textit{Independent of Image Quality} indicates the issue of out-of-focused images from light field cameras. As for LF-IENet, \textit{Disparity Map} is explicitly generated, and \textit{Angular Information} is hidden in the implicit branch.}
  \label{tab:2 part}
\end{table*}


{Second}, the ability of the network regards the performance, \ie, angular feature expression and asymmetric feature rectification.
Previous methods introduce a sub-optimal solution for the LF image representation. While stacking images into an array~\cite{sheng2022urbanlf,yan2022light} contains solely one-dimensional angular information, converting into a video sequence~\cite{zhuang2020video,wang2021temporal} ignores the implicit angular relationship of LF images. Besides, direct merging images or features results in blurred or mixed boundaries, and this phenomenon is particularly severe with the increase of sub-aperture images.
%
However, the single sub-aperture image captured by certain micro-lenses contains much noise~\cite{dansereau2013decoding,bok2016geometric}. Moreover, the initial LF images are not entirely focused, which can exacerbate this issue.~
In this work, we introduce a \textbf{Center Angular Rectification Module (CARM)} to perform effective rectification between the center view and the aperture-based features. Apart from other strategies in which the features are directly fused with others, our network adopts iteration feature rectification to incorporate the asymmetric angular feature and spatial features from the SAFM before the fusion stage, as shown in Fig.~\ref{fig:fusion strategy}. This design allows the OAFuser network to explore the relationship between different features and has realigned the angular and spatial information, further contributing to the segmentation task.

To demonstrate the effectiveness of the proposed OAFuser architecture, we conduct a comprehensive variety of experiments on different LF semantic segmentation datasets.
On the UrbanLF-Real dataset, OAFuser achieves $82.69\%$ in mIoU with an increase of ${+}3.37\%$. On the UrbanLF-Syn dataset, OAFuser surpasses previous works and attains $81.93\%$ in mIoU. Compared to previous state-of-the-art methods, OAFuser reaches the best mIoU of $84.93\%$ with a notable mIoU improvement of ${+}4.53\%$ on the UrbanLF-RealE dataset, while no additional parameters are required.

At a glance, we deliver the following contributions:
\begin{compactitem}
    \item We propose a novel paradigm, Omni-Aperture Fusion model, \ie, \textbf{OAFuser}, to perform LF semantic segmentation leveraging the structural characteristics of LF cameras, which addresses crucial challenges and enables the handling of an arbitrary number of sub-aperture images. 
    \item We design a \textbf{Sub-Aperture Fusion Module (SAFM)} to fuse and embed rich angular information from highly redundant representation, and a \textbf{Center Angular Rectification Module (CARM)} to match and rectify information imbalances caused by variations %
    from different angles.
    \item We verify our method through extensive experiments on  three datasets, \ie, %
    UrbanLF-Real, UrbanLF-Syn, and UrbanLF-RealE, which is a real-world dataset with extended synthetic samples.
\end{compactitem}

% Figure environment removed



