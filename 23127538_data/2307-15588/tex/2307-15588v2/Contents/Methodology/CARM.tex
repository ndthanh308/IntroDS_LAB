To eliminate information asymmetry, {the CARM is a crucial design to rectify both spatial and angular features while simultaneously rearranging the features captured from different viewpoints.} As shown in Fig.~\ref{fig:Rectification}, two features from different transformer blocks are fed into CARM, which includes Horizontal and Vertical Operations. Since the Horizontal and Vertical Operations are symmetrical, only the Horizontal Operation is introduced in detail.
In each operation, Global Rectification and Local Rectification are applied to rectify {feature cues} in different regions. At the end of CARM, the two features $F_{agl}^G$ and $F_{spl}^G$, which correspond to $F_{agl}^*$ and $F_{spl}^*$, are further fed into FFM (Feature Fusion Module). 
% Figure environment removed
Specifically, in the Horizontal Operation, $F_{agl}^*$ and $F_{spl}^*$ is first concatenated along horizontal direction into feature $F_{c1} \in \mathbb{R}^{\frac{H}{8}H \times \frac{W}{4} \times 64}$. {To intuitively represent the dimension change between different steps,} we use $F_{c1} \in \mathbb{R}^{H \times 2W \times C_1}$ as the concatenated feature. $F_{c1}$ is firstly fed into the \textbf{Global Rectification} stage. By applying an embedding process, $F_{c1}$ is projected into $F_{c1}^* \in \mathbb{R}^{H \times 2W \times C_2}$, which contain a set of tokens ${T}^*_{c1} \in \mathbb{R}^{2W \times C_2}$, with the projection matrix $M_{in} \in \mathbb{R}^{C_1 \times C_2}$, where $C_1$ denotes the input dimension of features, and $C_2$ denotes the embedding dimension for each token, 
%
the number of tokens is $2W$. To mitigate the covariate shift, $\dot{F}^*_{c1} \in \mathbb{R}^{H \times 2W \times C_2}$  with tokens $\dot{T}^*_{c1} \in \mathbb{R}^{2W \times C_2}$ is obtained by applying normalization of $F_{c1}^*$, following \cite{dosovitskiy2020image}. Then, the tokens, which represent the feature cues in a row, are utilized to generate query ($Q$), key ($K$), and value ($V$). To be more specific, $Q \in \mathbb{R}^{2W \times C_2}$ and $K \in \mathbb{R}^{2W \times C_2}$ are obtained by multiplication of tokens $\dot{T}^*_{c1}$ with matrices $M_q \in \mathbb{R}^{C_2 \times C_2}$ and $M_k \in \mathbb{R}^{C_2 \times C_2}$, respectively. The matrix $M_v \in \mathbb{R}^{C_2 \times C_2}$ multiplies the $T^*_{c1}$ to generate $V \in \mathbb{R}^{2W \times C_2}$. Furthermore, $Q$, $K$, and $V$ are divided along the channel dimension and fed into eight heads. The similarity of each head can be calculated by dot product and followed by a Softmax function to obtain the similarity scores in each head between those tokens, as in Eq.~(\ref{softmax first}).
\begin{align}
\text{Similarity}_{{head}_1} &=  \text{Softmax}\frac{Q_i\cdot K_i^T}{\sqrt{D_i} },~i. \in [1,8]. \label{softmax first}
\end{align}
Based on the similarity scores, the rectified tokens $T^a_{h_i}$ in each head can be obtained by multiplying with $V_i$, as in Eq.~(\ref{softmax score}).
\begin{align}
T^a_{h_i} &= \text{Similarity}_{{head}_i} \cdot V_i,~ i \in [1,8] .\label{softmax score}
\end{align}
The final tokens $T^a_h \in \mathbb{R}^{2W \times C_2}$ are obtained by concatenation, as in Eq.~(\ref{final score}).
\begin{align}
T^a_{h} &= \text{Concat}\{T^a_{h_1}, T^a_{h_2} ..., T^a_{h_8}\}.  \label{final score}
\end{align}
Given the final tokens $T_h^a$, an MLP layer is adapted, and a linear layer is to project tokens into demotion $C_1$. The final feature ${\overline{F^a_{h}}} \in \mathbb{R}^{H \times 2W \times C_1}$ after Global Rectification can be formulated as in the Eq.~(\ref{MLP}),
\begin{align}
    \overline{F^a_{h}} = \text{LN}(C_2,C_1)(\text{MLP}(F^a_{h}) + F^a_{h}), \label{MLP}
\end{align}
where $LN(C_2,C_1)(\cdot)$ denotes linear projection.

By decoupling the feature ${\overline{F^a_{h}}}$, the rectified feature stack $F_h^l \in \mathbb{R}^{2 \times H \times W \times C_1}$ is obtained. Furthermore, to further rectify the features in surrounding areas, the \textbf{Local Rectification} stage, which contains three convolutional layers, is implemented as in Eq.~(\ref{3d conv}), where $C$ denotes the input channel and output channel, $\{\cdot\}\otimes  2$ denotes repeat those layers two times. 
\begin{align}
    \Hat{F_h^l} &= \{{LReLU}({Conv3D}(C,C)(F^l_h))\}\otimes 2,\\
    {F}^H &= {Conv3D}(C,C)(\Hat{F_h^l}).\label{3d conv}
\end{align}
By disentangling of ${F}^H\in \mathbb{R}^{2 \times H \times W \times C_1}$, $F^H_{agl} \in \mathbb{R}^{ H \times W \times C_1} $ and $F^H_{spl} \in \mathbb{R}^{H \times W \times C_1}$ is obtained and further fed into Vertical Operation after concatenation.

%
