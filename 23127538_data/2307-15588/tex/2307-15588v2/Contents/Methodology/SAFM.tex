
%
% Figure environment removed
To retrieve the rich angular and spatial information from {LF} images, we introduce the SAFM module, which embeds that information into angular and spatial features. As shown in Fig.~\ref{fig:SF Module}, the SAFM consists of two steps. The first step of our SAFM is to extract features from all the {LF} images. All images are firstly divided into patches $P_{(m, n)}^{s_i}$, $P_{(m, n)}^c$, correspondence to sub-aperture images $F_{s_i}$ and $F_c$, and further fed into the convolutional layer to extract features, resulting in $\hat{P}_{(m, n)}^{s_i}$ and $\hat{P}_{(m, n)}^c$, where $m, n$ denotes the relative position of the patch in a single feature map. The patch size follows~\cite {zhang2022cmx}. Especially, our work aims to segment the central view image. Thus, the weights for the center view are performed individually, which ensures the spatial information of the central view remains independent. The weights for sub-aperture images are shared between different sub-aperture images. The calculations are presented in Eq.~(\ref{equ:feature_extraction1}) and Eq.~(\ref{equ:feature_extraction2}):
\begin{align}
\hat{P}_{(m, n)}^{s_i} &= \text{Conv}^{sub}(C_{\text{in}}, C_{\text{out}})(P_{(m, n)}^{s_i}), \label{equ:feature_extraction1} \\ 
\hat{P}_{(m, n)}^c &= \text{Conv}^{cen}(C_{\text{in}}, C_{\text{out}})(P_{(m, n)}^c), \label{equ:feature_extraction2}
\end{align}
where $\text{Conv}(C_{in}, C_{out}) $ means the convolutional layer with input dimension $C_{in}$ and output dimension $C_{out}$. %
For the first stage, $C_{in}$ is $3$ and $C_{out}$ is $64$. After that, the central image features $F_{spl}$ are obtained by combining different patches $\hat{P}_{(m,n)}^c$ and fed into the Spatial Transformer Block~(STB). Meanwhile, the patches of the central view are also fed into the next step to cooperate with patches of the sub-aperture image to obtain angular features. 

Afterward, the second step of SAFM is illustrated in Fig.~\ref{fig:SF Module}, which is presented as the yellow gear symbol.
%
For each patch $\hat{P}_{(m, n)}^{s_i}$ of dimension $k$, the channel score $e{_{{(m,n)}^{k}}^{s_i}}$ is calculated by obtaining the euclidean distance between each patch from the sub-aperture feature in dimension $k$ and the corresponding patch from center view feature in dimension $k$, as in Eq.~(\ref{e}):
\begin{align}
e{_{{(m,n)}^{k}}^{s_i}} &= {\text{Abs}(\hat{P}^c_{{(m, n)}^k}} -\hat{P}_{{(m, n)}^k}^{s_i}), \label{e} 
\end{align}
where $\text{Abs}{(\cdot)}$ denotes absolute value between two patch in channel $k$. Given the $e{_{{(m,n)}^{k}}^{s_i}}$, the mask score $t_{(m,n)^k}^{s_i}$ for $\hat{P}_{(m, n)^k}^{s_i}$ is obtained by mapping into the range of [1,0] and squaring them to enhance the discrimination, as shown in Eq.~(\ref{mp}).
\begin{align}
t_{{(m,n)}^k}^{s_i} = {(\Theta\{{e{_{{(m,n)}^{k}}^{s_i}}}}\})^2,\label{mp}
\end{align}
where $\Theta\{\cdot\}$ denotes mapping operation. After obtaining the mask scores, the certain patch $P_{{(m,n)}^k}$ of angular feature can be calculated. To be more detailed, the patch $P_{{(m,n)}^k}$ in angular feature $F_{agl}$ at position $(m,n)$ in channel $k$ can be calculated by summarizing the corresponding central view patch $\hat{P}^c_{{(m, n)}^k}$ with masked sub-aperture patches $\hat{P}_{{(m, n)}^k}^{s_i}$, as in Eq.~(\ref{p_final}).
\begin{align}
P_{(m,n)^k} &= \hat{P}^c_{{(m, n)}^k} + \sum_{i=1}^{N} {t{_{{(m,n)}^{k}}^{s_i}} \cdot \hat{P}_{{(m, n)}^k}^{s_i}}.  \label{p_final}
\end{align}
Finally, the patch of angular feature $P_{(m,n)}$ is obtained by concatenation of $P_{(m,n)^k}$ as in Eq.~(\ref{feature angular}). The angular feature $F_{agl}$ is filled by $P_{(m,n)}$ and further fed into Angular Transformer Block~(AFB).
\begin{align}
 P_{(m,n)}&=  Concat(P_{(m,n)^k}).\label{feature angular}
\end{align}

