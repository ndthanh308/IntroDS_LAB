
In this section, we introduce the knob estimator in detail. To avoid the influence of explosive knob space, we select the important knobs at first with the adaptive knob ranking (AKR) shown in Section~\ref{sec:ranking} and build the interpretable linear knob estimator (IKE) based on random forest in Section~\ref{sec:estimate}. 


\subsection{Adaptive Knob Ranking}\label{sec:ranking}
Since only a subsets of adjustable knobs have a great impact on the certain estimation task, we design an adaptive knob ranking mechanism to eliminate knobs with less impact on the performance. The knob ranking task is defined as follows:

Given a knob set $K$, a K-P training set for a certain scenario $S$, $D= [(x_1, y_1), ...(x_n,y_n)]$, where $x_i$ is certain knob configuration, such as $[knob1 = a, knob2 = b...,]$, and $y_i$ is the performance on $S$ with $x_i$ as the knob configuration. The goal of knob ranking is to train a model from $D$ to compute the importance of knobs to filter out the important knobs of $K$ for knob estimation.

Specifically, the knob ranking faces one significant trade-off between the accuracy and the time consumption of training data. On the one hand, ranking is a sub-important prior task for database in which we should save resource utilization as possible. On the other hand, it may lead to model underfitting to learn the ranking result of multiple knobs with limited training data. Existing ranking methods~\cite{hutter2014efficient} which depend on fixed model could not efficiently resolve the above trade-off. 

\begin{algorithm}
    \SetAlgoNoLine
    \LinesNumbered
    \SetKwInOut{Input}{input}
    \Input{
        Set of regression models($M$), Dataset($D$), Knob candidates ($K$) \\
    }
    \SetKwInOut{Output}{output}
    \Output{
        Knob importance($W$) \\
    }
    \BlankLine
    $X,y \leftarrow D$ \\
    \For(){ each $m_i \in M$}{
        // \textit{Get the performance of the model}\\ 
        $y' \leftarrow m_i(D)$\\
        $s \leftarrow$ \textit{$R^2$ value between $y''$ and $y$} \\
         \For(){each $k_j \in K$ }{
             $X' \leftarrow$ \textit{Randomly rearrange values of $k_j$ in X}\\
             $y'' \leftarrow m_i(X')$\\
             $s' \leftarrow $ \textit{$R^2$ value between $y''$ and $y$}\\
             $W_{j} \leftarrow W_{j} + (s-s')*s$\\
         }
    }
  return $W$ 
\caption{Knob Importance Rank}\label{alg:knobrank}
\end{algorithm}

To meet the above trade-off, we design a stacking ensemble learning~\cite{stacking} based method for knob importance ranking, which integrates the weighted average ranking of multiple models as the final ranking results. Compared to the single model, our method achieves stronger robustness to the various scenarios due to the integrated models. For ensemble model construction, we utilize the model's performance as the weight to judge whether the model is suitable. Here, we use $R^2$ metric between observed label and its predicted label to measure the performance, i.e. $R^2 =1-\frac{\sum_{D}(y_i - \widehat{y_i})^2}{\sum_{D} (y_i - \overline{y})^2}$, where $\widehat{y_i}$ is the predicted label, $y_i$ is the observed label and $\overline{y}$ is the mean of observed labels. Then, we assign high weights to well-perform models and low weights to poor-perform models in the ensemble. 

The knob ranking algorithm is shown in Algorithm~\ref{alg:knobrank}. In Lines 1-4, we calculate the performance of a specific model using the $R^2$ metric, which measures the goodness-of-fit between predicted and actual values. In Lines 6-8, we randomly shuffle the values of a knob to obtain shuffled data $X'$, and then compute the performance $s'$ of the model. The more important the knob is, the lower the value of $s'$ is, since the model's decision heavily relies on important knobs. In Line 9, we compute the difference between the performance before and after data shuffle and add it to $R_j$, representing the importance of knob $k_j$. Note that in Line 9, we multiply $(s-s')$ by $s$ to assign a higher weight to models that perform better, ensuring the quality of the final knob importance ranking. Because well-performing models can better utilize the important knobs for decision-making, the reliability of the ranking of knob importance is higher.



\begin{algorithm}
  \SetAlgoNoLine
  \LinesNumbered
  \SetKwInOut{Input}{input}
  \Input{
      Dataset($D={X,y}$) 
  }
  \SetKwInOut{Output}{output}
  \Output{
      The parameters of Interpretable Estimator($w$) \\
  }
  \BlankLine
  $R \leftarrow \varnothing$ \\
  // \textit{Get the optimal random forest structure by the Bayesian optimization}\\ 
  $F \leftarrow Bayesian(D)$
  
  \For(){$ t \in F$}{
    \For(){$p \in t$}{
    // \textit{add the path of tree to R}\\
        $R.append(p)$ \\
    }   
  }
   // \textit{init a binary vector according to the number of rules}\\ 
   $V \leftarrow vector( len(D.X), len(R))$\\
   \For(){ $i \in range(0,len(D.X))$}{
     \For(){$j \in range(0, len(R)$} {
         \uIf{$R_j.true(D.X_i$)}{$V_{ij} = 1$}
         \uElse{$V_{ij}=0$}
         }
   }
  $w \leftarrow  arg min(\frac{1}{k}* \sum_{i = 1}^k (D.y_i - D.y'(V_i))^2 + \lambda * ||w||_1)$ // \textit{lasso regression} \\ 
    
return $w$ \\
\caption{The Training Algorithm of IKR}\label{alg:knobestimator}
\end{algorithm}



\subsection{Interpretable Knob Estimator}\label{sec:estimate}

We then propose IKE with the goal of estimatint the performance of certain knob configuration. We first define the knob estimation problem as follows.

\textbf{Input:} A scenario $S$ and k knob configuration $x = [knob1 = a, knob2 = b,...]$ for $S$.

\textbf{Output:} The performance of input knob configuration $y = ATE(x)$ under $S$.

For the above problem, we develop an interpretable estimator based on random forest. We employ the random forest as the basic model for three reasons: (i) we could only gather limited K-P training data due to its huge time consumption of knob revision and query execution. Compared to deep learning methods, the random forest has lower training data demand~\cite{randomf2}; (ii) Random forests are composed of decision trees. naturally, a path from the root to a leaf in a decision tree can be considered as a set of rules. Thus, it is suitable for random forest to use these rules as explanations; and  (iii) The random forest is more robust to the scenario changes~\cite{randomforest} while some black-box model may produce a dramatic result on the new scenarios.
% Figure environment removed
Based on the random forest denoted as $F = \{t_1, t_2, ...\} $, we gather a knob rule set by traversing all the paths of the trees in $F$. As an example in Figure~\ref{fig:tree}, each $t_i$ is a simple-rule tree, and each path of $t_i$ can be used to generate an knob rule, e.g., $r = shared\_buffers < 400MB \& work\_mem <400MB$. These rules represent the relationships between knobs, i.e., the knobs in same path have a co-influence. Since lasso regression has the powerful ability in finding superior variables~\cite{lasso}, we utilizes the lasso regression to further fit the relationships between these rules and the database performance, pruning useless rules and assigning high weight to the high-quality rules. Then, we gather the direct relationships between the knob rules and their influence to the database performance.







The specific algorithm is shown in Algorithm~\ref{alg:knobestimator}, containing two main stages, random forest model training (Lines 1-3) and interpretable rule fitting (Lines 4-20). Line 3 trains the random forest by utilizing the Bayesian optimization~\cite{auto}, which could find the optimal parameters of random forest such as the number of trees and the depth of trees for the current training set. Lines 4-9 iteratively collect all the rules from the trained random forest. Lines 11-19 transform the input from the knob configuration to the rules. We utilize a binary vector $V$ to identify whether the rule is satisfied by the input knob configuration. 





Then, Line 20 fits the relationships between the knob rules and the database performance, where $y_i$ is the performance label of current tuning task, $k$ is the number of rules, $w$ is the weight vector of rules, and $V_{i}$ is a binary vector. If current knob configuration satisfies the $j$th rule, the corresponding $V_{ij} = 1$. Otherwise, $V_{ij} = 0$. In lasso model, each rule $r_j$ corresponds to a weight $w_i$, by which a knob configuration satisfied $r_i$ will influence the database performance by the weight of the rule. 



% Figure environment removed

     % Figure environment removed

  Then, the specific model is shown in Equation~\ref{linear} as a weighted sum of rules. Due to the simple form, the influences of each knob could be easily reflected by the weight of rules. 

  \begin{equation}\label{linear}
    y'(x) = \sum_{i = 1}^{p} w_i * v_i
  \end{equation}
  
  
Furthermore, we can compute the weight of each knob and visually show its impact, or the joint impact of some knobs on the database performance.
In Figure~\ref{tpcc}, we show an example for the weight of a significant knob 'commit\_delay' of Postgresql under the default workload configuration of TPCC.

 