
In this section, we overview the architecture and workflow of our estimator for database knobs. 

% Figure environment removed

\textbf{IWEK Architecture.} Figure~\ref{fig:overview} shows the overall architecture of our system, which consists of four main modules: The \textsf{Collector}, Adaptive Knob Ranking(AKR in Section~\ref{sec:ranking}), Interpretable Knob Estimator(IKE in Section~\ref{sec:estimate}) and \textsf{Experience Repository} (ER). 
\textsf{Collector} gathers user's log to extract the training data for the model. AKR ranks the importance of knobs and obtains a set of important knobs by the ensemble model. Based on the important knobs, the IKE module then discovers the relationship between knobs and performance (K-P for brief) by utilizing the interpretable machine learning methods. The \textsf{ER} serves as a repository for historical experience data to support the transfer of ranking and estimator.% \textsf{AKR}, \textsf{IKE} and \textsf{ER} will be introduced in Section~\ref{sec:ranking},~\ref{sec:estimate} and~\ref{sec:transfer}, respectively.

Then, we define two major concepts of our architecture, the scenario and the experience. Since the knob estimation involves the data, workload and the environment for the database, we define it as a \emph{scenario}, which is a triple ($T$, $W$, $E$), where $T$ is the data in the database, $W$ is the workload for the database and $E$ is the database environment including the database, hardware and operating system. For knob estimation, such a triple may be very large with massive useless information. Thus, we extract the useful features from the triple to construct a fingerprint for the scenario, which will be discussed in Section~\ref{sec:rtran}. 

Experience is the useful historical information for knob estimation. For a scenario, two kinds of experiences are the most important, i.e., the knob ranking and the knob estimators. Thus, with the fingerprint representation for the scenario, we represent a piece of experience as a triple ($f$, $k$, $m$), where $f$ is the fingerprint for the scenario, and $k$ and $m$ are corresponding knob ranking and estimator, respectively. $m(x)$ denotes the predicted performance of knob configuration $x$. All the experiences used in IWEK are stored in \textsf{ER}.

Here, we describe the workflow of our IWEK with an example.  

% Figure environment removed

\textbf{Example.} In the \textsf{ER}, we store two pieces of experiences $e_1$ = $[f_1 , k_1, m_1]$ and $e_2$ = $[f_2, k_2, m_2]$ corresponding to two scenarios, ycsb-1 (table size = 3GB, 100\% select workload, database = Postgresql 15.0) and  tpcc-1(table size = 1GB, 100\% update workload, database = Postgresql 15.0), respectively.   

Next, we introduce the detailed workflow for establishing knob estimator of a new scenario ycsb-2 (90\% select and 10\% delete workload, table size = 2GB, database = Postgresql 15.0) in Figure~\ref{fig:example}, containing the direct learning and the transfer learning (steps 1-10).

(1) \textbf{Direct Learning:} For ycsb-2, the \textsf{Collector} periodically collects performance information under different knob settings and forms the performance data set $D_s$. Then, IWEK invokes AKR to compute the knob ranking to obtain a set of important knob candidates. After collecting more K-P information of important knobs, IKE then fits the data $D_s$ to obtain $m_3$ as an interpretable machine learning model.

(2) \textbf{Transfer Learning:} Firstly, in step 1, $f_3$ matches with the fingerprints $f_1,f_2$ of \textsf{ER} to retrieve similar experiences. Step 2 returns the top-2 experiences, i.e., $[k_1,m_1],[k_2,m_2]$, as the basis of transfer learning. Step 3 apply ranking transfer mechanism based on $k_1$ and $k_2$ to generate $k_3$. According to the important knobs identified by $k_3$, steps 4-5 generate a K-P point of ycsb-2 and form a set $D_3 = \{X, y\} = [(x_1, y_1), ...(x_n,y_n)]$. Steps 6-7 also utilize the $D_3.X$ to generate the K-P points $D_1$ and $D_2$ for $m_1$ and $m_2$, respectively. Then, steps 8-9 assign a proper weight $[w_1,w_2]$ for $m_1$ and $m_2$ according to the similarities of $[D_1,D_2,D_3]$. Finlay, step 10 returns a weighted sum of historical estimator experiences  as the estimator transfer results. For a knob configuration $x$, the predicted performance of $m_3$ is $m_3(x) = w_1\times m_1(x)+ w_2\times m_2(x)$.

