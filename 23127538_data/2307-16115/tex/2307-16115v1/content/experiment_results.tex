In this section, we conduct extensive experiments to test the performance of IWEK. Firstly, in Section~\ref{sec:setting}, we introduce our experimental setup, including the dataset settings and evaluation metrics. We compare the proposed interpretable knob estimator with two typical regression model in Section~\ref{sec:inter}. We experimentally evaluate the performance of transfer learning in Section~\ref{sec:zero}. We evaluate the performance of robustness of IWEK under various $K$ values in Section~\ref{sec:tune}.


\subsection{Experimental Setup}\label{sec:setting}

All experiments were conducted on Postgresql v15.0 and Docker 20.10.19 with a container of 2GB memory, 4 processor cores and 50MB/s hard disk speed. Then we introduce the datasets and metrics of our experiments. 

\noindent \underline{\textbf{Experimental Datasets}} We utilize the open source benchmarks, YCSB and TPCC, implemented by \cite{DifallahPCC13} to evaluate our method, which are widely used in existing works~\cite{2019An}. YCSB and TPCC are designed for testing the performance of OLTP wordload which is more sensitive to the knob changes. As shown in Table~\ref{tab:workload}, we set up 16 scenarios to evaluate the performance of the IWEK. Specifically, we set various configurations in data scale and transaction operation ratios for TPCC (like tpcc-1 [NewOrder=45\%, Payment=40\%, OrderStatus=5\%, Delivery=5\%, StockLevel=5\%]) and YCSB (like ycsb-1 [ReadRecord=50\%, InsertRecord=5\%, ScanRecord=15\%, UpdateRecord=10\%, DeleteRecord=10\%, ReadModifyWriteRecord=10\%]), respectively. 

\begin{table}[h!]	
	\centering 
	\caption{The setting of scenarios}  
	\label{tab:workload}  
\begin{tabular}{|p{0.5cm}|p{1.0cm}|p{0.8cm}|p{3.5cm}|} 
    \hline
	No.&name & scale & transaction ratios \\
    \hline
	1&tpcc-1 & 1GB & 45\%,40\%,5\%,5\%,5\%  \\% 1
    \hline
	2&tpcc-2  & 1GB & 5\%,45\%,5\%,40\%,5\%  \\% 2
    \hline
	3&tpcc-3  & 1GB & 20\%,10\%,50\%,15\%,5\% \\% 18
    \hline
        4&tpcc-4  & 3GB & 60\%,20\%,10\%,5\%,5\% \\% 10
    \hline
        5&tpcc-5  & 3GB & 10\%,20\%,10\%,30\%,30\%  \\% 11
    \hline
        6&tpcc-6  &3GB & 20\%,10\%,50\%,15\%,5\%  \\% 12
    \hline
	7&tpcc-7  & 5GB & 45\%,40\%,5\%,5\%,5\%  \\% 5
    \hline
	8&tpcc-8  & 5GB & 5\%,45\%,5\%,40\%,5\%  \\% 6
    \hline
	9&ycsb-1  & 1GB & 50\%,5\%,15\%,10\%,10\%,10\%  \\% 3
    \hline
	10&ycsb-2  & 1GB & 20\%,5\%,15\%,25\%,10\%,25\%  \\% 4
    \hline
	11&ycsb-3  & 1GB & 20\%,50\%,10\%,10\%,5\%,5\% \\% 19
    \hline
	12&ycsb-4  & 1GB & 20\%,10\%,15\%,20\%,10\%,25\%  \\% 22
    \hline
	13&ycsb-5  & 3GB & 10\%,5\%,15\%,10\%,30\%,30\%  \\% 14
    \hline
	14&ycsb-6  & 3GB & 30\%,10\%,20\%,20\%,10\%,10\% \\% 15
    \hline
	15&yscb-7  & 5GB & 50\%,5\%,15\%,10\%,10\%,10\%  \\% 7
    \hline
	16&ycsb-8  & 5GB &20\%,5\%,15\%,25\%,10\%,25\%  \\% 8



    \hline
    
\end{tabular}
\end{table}


 \noindent \underline{\textbf{Metric:}} In this paper, we utilize three metrics to evaluate the performance of IWEK, the mean prediction error, the Pearson correlation coefficient and the accuracy of knob estimation. 
 The mean prediction error is used to measure the error between the predicted label and real label defined in Formula~\ref{equ:error}.  

\begin{equation}\label{equ:error}
   error = \frac{1}{n} \sum_1^n (y(x_i) - IWEK (x_i) )^2
\end{equation}


The Pearson correlation coefficient is used to measure the correlation between the predicted label and real label defined in Formula~\ref{equ:peason}. $Cov(y_t, IWEK(X))$ is the covariance of the predicted label and real label, and $\sigma_{y_t}$ and $\sigma_{IWEK(X)}$ are the standard deviation of the predicted label and real label, respectively.

\begin{equation}\label{equ:peason}
    error = 1 - \frac{Cov(y_t, IWEK(X))}{\sigma_{y_t} * \sigma_{IWEK(X)}}
\end{equation}

To verify the practicality, we use IWEK for a real binary classification task. Users could use IWEK to compare their old knob configuration and the new knob configuration. This is a classification task to clarify which is the better knob configuration. For evaluating the accuracy of this task, we randomly generate the pairs of knobs, such as $[x_1, x_2]$. The specific formula of the prediction accuracy is shown in the Formula~\ref{equ:acc}, where $TP$ is the number of correctly classified knob pairs, and $T$ is the total number of knob pairs.

\begin{equation}\label{equ:acc}
	accuracy = \frac{TP}{T}
 \end{equation}

\noindent \underline{\textbf{Baselines:}} We compared our knob estimator with two baselines: lasso regression and multilayer perceptron regression (MLP). Lasso regression is a typical regression model with low requirements for the size of the training data, and is insensitive to noise and capable of filtering out irrelevant features\cite{lasso}. MLP can better capture the nonlinear relationship between features and also demonstrate good performance in some non-linear tasks~\cite{MLP}.



\subsection{The Effectiveness for Interpretable Estimator}\label{sec:inter} 

In this section, we evaluate the performance of our interpretable knob estimator under the extensive workloads of Table~\ref{tab:workload}. To test the performance of IWEK with limited training set, we only sample 100 knob-performance points for each workload by utilizing the Latin Hypercube Sampling algorithm. Then we utilize 70 points as the training set and 30 ones as the test set. From the test set, we randomly sample 100 pairs to evaluate the accuracy of our estimator. 




Figure~\ref{a} illustrates the Pearson correlation coefficients of our IKE model compared to two baseline models, MLP and lasso. Overall, our IKE model demonstrates superior predictive performance in the majority of cases. The average Pearson coefficient for IKE reaches 0.93, whereas it is 0.68 for MLP and 0.78 for lasso. Even in the worst-case scenario (tpcc-6), our IKE model still achieves a correlation coefficient of 0.78. In contrast, due to the complex network structure of MLP, it struggles to converge with only 70 training data, resulting in instances where the coefficient becomes p=-0.3, such as tpcc3. Additionally, MLP performs poorly in tpcc-2 (p=0.36) and tpcc-6 (p=0.34). However, if MLP manages to converge, it can achieve a high correlation coefficient, as seen in tpcc-5 (p=0.98). On the other hand, lasso exhibits relatively stable performance with an average coefficient of 0.78 but still lags behind our IKE model significantly. 
% Figure environment removed


Figure~\ref{b} illustrates the classification performance of our IKE, MLP, and lasso models. It is evident that our IKE model demonstrates superior performance compared to MLP and lasso in most scenarios. On average, IKE achieves an accuracy of 80\% (with a minimum accuracy of 69\%), surpassing MLP with an average accuracy of 68\% (minimum accuracy of 48\%) and lasso with an average accuracy of 72\% (minimum accuracy of 57\%). Even in the worst-case scenario, IKE still achieves a respectable accuracy of 69\%. In contrast, the MLP model exhibits notably low accuracy in tpcc-2 (53\%) and ycsb-5 (48\%), approaching random selection results. On the other hand, the lasso model maintains comparatively stable accuracy across different scenarios. Although lasso has limitations in accurately fitting complex knob-performance data (maximum accuracy of 87\%, average accuracy of 72\%), its linear structure enables it to capture the relative relationship among various knob configurations. 

In summary, from the comparison results, IWEK demonstrates good performance across various workloads in the open-source benchmark TPCC and YCSB, as evidenced by correlation coefficient, prediction accuracy.



\subsection{The Performance of Transfer Learning}\label{sec:zero}


% Figure environment removed

In this section, we test the performance of the estimator transfer approach with parameter $N = 10$ and $K = 3$, on metrics of the similarity relationships, the Pearson correlations and the classification accuracy. 

Figure~\ref{fig:heatmap} shows the heat of similarity relationships of K-P distribution. The closer to blue the square is, the less similar the corresponding two scenes are, and while the closer to yellow the square is, the more similar the corresponding two scenes are. It is evident that similar scenario types (e.g. TPCC and TPCC) exhibit higher similarity, while the similarity between different scenarios types (e.g. TPCC and YCSB) is evidently lower.

Figure~\ref{fig:recall} shows the average Top-3 recall of our ranking transfer methods under all the scenarios. We have observed that our ranking transfer achieved a recall rate of 66.6\% in most scenarios. Specifically, for tpcc-1 and tpcc-2, our transfer ranking method successfully identified all the important knobs. However, in the case of tpcc-5 and ycsb-6, only one important knob was recalled. This might be due to the balanced transaction ratios in these scenarios, which make them sensitive to multiple types of knobs, making it challenging to accurately rank them.


% Figure environment removed

Then, we test the performance of the transfer estimation, on the Pearson correlation coefficient and binary classification accuracy. The "origin" represents the relationship between the predicted labels and observed labels obtained by using 70 data points from the current scenario as training samples and 30 data points as testing samples, and "transfer" refers to the prediction results obtained by transfer knob estimator (N = 10, K = 3). For the transfer learning of certain scenario (like tpcc-1), we utilize the remaining scenarios (tpcc-2 to ycsb-8) as the experiences. We can visually observe that in new scenario, our method obtain the effective transferred predicted labels that is close to the "origin". 





% Figure environment removed

Figure~\ref{fig:transfer_zero} shows the Pearson coefficient of the origin estimator and the transferred estimator. As seen in the Figure~\ref{fig:transfer_zero_a}, the transfer estimator can exhibit good performance in most scenarios, achieving an average correlation coefficient of 0.845. This verifies the effectiveness of our transfer algorithm. In addition, as shown in Figure~\ref{fig:transfer_zero_b}, the performance of the transfer estimator in binary classification is still comparable to that of the original estimator with an average accuracy rate of 78.81\%. Even in some scenario like ycsb-5, the transfer estimator outperforms the origin estimator. This result demonstrate that weighted sum of $K$ historical experiences may lead to better classification accuracy.
% Figure environment removed

In addition, the scatterplots in Figure~\ref{fig:transfer} show the points distribution of transfer estimator and the origin estimator. We take the real performance label as the $x$-axis and the predicted performance label as the $y$-axis. Figure~\ref{fig:transfer_a} shows the prediction results on TPCC, with origin error $0.00456$ and transfer error $0.0192$. We observe that the predicted points of transfer model are all concentrated around $y=x$ with only 10 samples. Also, the transfer prediction of YCSB performs well with transfer error $0.0365$.
% Figure environment removed
\subsection{The Evaluation of Robustness}\label{sec:tune}
In this section, we evaluate the performance of IWEK with $K = 1-6$ to test the robustness of IWEK, containing the robustness of ranking transfer and estimator transfer. 

Figure~\ref{fig:robustrecall} illustrates the average ranking transfer results for K ranging from 1 to 6. It is evident that our ranking transfer method exhibits a high level of robustness when faced with changes in K. That is because our ranking transfer assigns weights according to similarity. Then the experiences with the highest similarity receive the greatest weight, while those dissimilar experiences are assigned lower weights.  

Figure~\ref{fig:avg_correlations_trend} shows the average performance of estimator transfer approach under all the scenarios of Table~\ref{tab:workload} as the number of reused historical experiences varies. According to the figure, we observe that the model transfer performs the best when three historical experiences are reused. However, the performance declines when fewer or more than three experiences are reused. This is because when reusing fewer than three experiences, the robustness of model transfer cannot be effectively ensured, while reusing more than three experiences leads to incorporating unrelated models, thereby lowering overall performance. 


As shown in Figure~\ref{fig:avg_accuracy}, the average accuracy of 16 scenarios is more robust to the changes of parameter $K$. The average accuracy only changes from 74.5\% to the 78.5\% with $K$ = 1-6. This is because the accuracy metric focuses on the relative performance between the two knob configurations. Even if there is an error between the predicted label and the true label, the relative relationship of two configuration may still be predicted accurately. 

Overall, our model is robust to changes of $K$, indicating that our transfer model can assign proper weights for experiences.




% Figure environment removed
