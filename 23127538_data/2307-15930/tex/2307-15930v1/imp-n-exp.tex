%% -*- mode: LaTeX; fill-column: 78; -*-

\section{Implementation} \label{sec:impl}
%========================================
\EventDPOR was implemented on top of \Nidhugg.
\Nidhugg~\cite{tacas15:tso} is a state-of-the-art stateless model checker for
C/C++ programs with Pthreads, which works at the level of the LLVM Intermediate
Representation.
% explores all their possible executions, and reports assertion violations and
% crashes, such as segmentation faults.
\Nidhugg comes with a selection of DPOR algorithms. One of them is
\OptimalDPOR, which we have used as a basis for \EventDPOR's implementation.

We have extended the data structures of \Nidhugg with the information
needed by \EventDPOR.
For instance, nodes in wakeup trees contain new information, such as the set
of parked wakeup sequences, and events in executions include the information in
$\tmpaccesses$, used to compute the \done set
as shown in \crefrange{algacsl:initialize-accesses}{algacsl:donesleeptree-add}
of~\cref{alg:eventdpor-access}.
The relation $\happbf{\hb}{E}$ is represented by a
vector clock per event, containing the set of preceding events.
When reversing races (in $\reverserace$) and checking for redundancy
(\cref{algacsl:event-test} of~\cref{alg:eventdpor-access}),
the relation $\happbf{\hb}{E}$ is extended by a saturation operation
(\cref{def:weakall} in \cref{sec:functions-appendix}) that captures ordering constrained induced by serialized message execution.
%% Transitive relations such as $\weaksatrel{\happbf{\hb}{E}}$ are represented by a
%% vector clock per event, containing the set of preceding events.
%% The saturation procedure in \cref{def:weakall} is implemented using
%% fixpoint iteration.

Concerning race reversal, instead of reversing multiple races between messages
executed on the same handler, our implementation detects and reverses only
the race induced by the first conflict, since other races cannot be reversed,
as explained using the example in \cref{fig:example1new}.
Moreover, in cases where $\reverserace$ would return several maximal executions that reverse a race, our implementation instead returns their union, even though it may not form an execution (e.g., since it may contain several incomplete executed messages on a handler). From this union, events will be removed adaptively during wakeup tree insertion to extract only those maximal executions that generate new leaves in a wakeup tree.

\hasbeenremoved{Moreover, we postpone the deletion of incomplete messages from $\exseq''$, described at
\crefrange{algl:converge-begin}{algl:converge-end} in \cref{alg:reverserace},
to the point before adding a new branch in the wakeup tree.
This avoids unnecessary creation of wakeup sequences that will not create any
new leaf in the wakeup tree, since the deletion of incomplete messages
can be guided by the redundancy checks performed during the insertion.
%
Finally, instead of computing $\happbf{sc}{E.w'}$ for each test of form
$p \in \winits{E}{w}$ at \cref{algacsl:event-test}
of \cref{alg:eventdpor-access}, we precompute $\happbf{sc}{E.w}$
before all the tests, which accelerates the redundancy check.}
%% During \textbf{Happens Before Check},
%% we consider additional scheduling constraints between messages that are
%% supposed to be added during \textbf{Witness Construction}.
%% These constraints are computed by a heuristic without doing expensive
%% \textbf{Witness Construction}. 

\section{Evaluation} \label{sec:eval}
%====================================
In this section, we evaluate the performance of our implementation and put it
into context.
%
Since currently there is no other SMC tool for event-driven programs to
compare against,\footnote{All our attempts to use \href{https://github.com/eth-sri/R4}{$R^4$} failed miserably; the tool has not been updated since~2016.}
we have created an API, in the form of a C header file, that
implements event handlers as pthread mutexes (locks) and simulates messages as
threads that wait for their event handler to be free. This API allows us to
use plain C/pthread programs to compare \EventDPOR with the \OptimalDPOR
algorithm implemented in \Nidhugg as baseline, but also with the
\emph{Lock-Aware Partial Order Reduction} (\emph{LAPOR})
algorithm~\citet{LAPOR@OOPSLA-19}, implemented in \GenMC. The \LAPOR algorithm
is often analogous to \EventDPOR w.r.t. the amount of reduction that it can
achieve when event handlers are modeled as global locks.
% Since we are comparing against \LAPOR, we have also included
We also include in our comparison the baseline DPOR algorithm of \GenMC that
tracks the modification order (\genmcmo{\small}) of shared variables.
% , and thus is analogous to the \OptimalDPOR algorithm of \Nidhugg.
%
For \Nidhugg, we used its \texttt{master} branch at the end of~2022;
% (\EventDPOR has been rebased on top of this branch.)
%
for \GenMC, we used version 0.6.1.%
%, because this is its most recent
%version that comes with a working implementation of \LAPOR.
\footnote{\GenMC v0.6.1 (released July 2021) warns that \LAPOR usage with
\genmcmo{\footnotesize} is experimental; in fact, \LAPOR support has been
dropped in more recent \GenMC versions.}
% . On the other hand, there is no \GenMC version without this warning and
% but we did not experience any problems using it.}
%
We have run all benchmarks on a Ryzen 5950X desktop running Arch Linux.
%% and used a timeout of ten hours.

%% These commands are generated in the result columns by the benchmark scripts
\newcommand\SZ{\scriptsize}
\newcommand\error{\textcolor{red}{\SZ \textdagger}\xspace}
\newcommand\timeout{\textcolor{blue}{\SZ \clock}\xspace}%\tiny\StopWatchEnd}}
\newcommand\notavail{\textcolor{gray}{\SZ n/a}\xspace}
\newcommand\bug{\SZ\color{red} bug }

\pgfplotstableset{% global config, for example in the preamble
  % these columns/<colname>/.style={<options>} things define a style
  % which applies to <colname> only.
  columns/benchmark/.style={string type,
    string replace*={_}{\_},
  },
  columns/tool/.style={string type},
  columns/lapormo_traces/.style={column name=\multicolumn{1}{r}{\lapormo{\SZ}}},
  columns/lapormo_time/.style={column name=\multicolumn{1}{r}{\lapormo{\SZ}}},
  columns/genmcmo_traces/.style={column name=\multicolumn{1}{r}{\genmcmo{\SZ}}},
  columns/genmcmo_time/.style={column name=\multicolumn{1}{r}{\genmcmo{\SZ}}},
  columns/optimal_traces/.style={column name=\multicolumn{1}{r}{\opt{\SZ}}},
  columns/optimal_time/.style={column name=\multicolumn{1}{r}{\opt{\SZ}}},
  columns/event_traces/.style={column name=\multicolumn{1}{r}{\evt{\SZ}}},
  columns/event_time/.style={column name=\multicolumn{1}{r}{\evt{\SZ}}},
  every head row/.style={before row={%
       \toprule
      & \multicolumn{4}{c}{Executions (Traces+Blocked)} & \multicolumn{4}{c}{Time (secs)}\\
      \cmidrule(r){2-5}\cmidrule(r){6-9}
      & \multicolumn{2}{c}{\GenMC} & \multicolumn{2}{c}{\Nidhugg} &
      \multicolumn{2}{c}{\GenMC} & \multicolumn{2}{c}{\Nidhugg} \\
      \cmidrule(r){2-3}\cmidrule(r){4-5}\cmidrule(r){6-7}\cmidrule(r){8-9}
    },after row=\midrule},
  every last row/.style={after row=\bottomrule},
  column type={r},
  %% Not the same column as lowercase "benchmark"
  create on use/Benchmark/.style={
    %% Breaks with _ in benchmark names
    create col/assign/.code={%
      \getthisrow{benchmark}\benchmark
      \getthisrow{n}\n
      \edef\entry{\benchmark(\n)}%
      \pgfkeyslet{/pgfplots/table/create col/next content}\entry
    },
  },
  columns/Benchmark/.style={
    column type={l},
  },
  columns={Benchmark,
    genmcmo_traces,lapormo_traces,optimal_traces,event_traces,
    genmcmo_time,lapormo_time,optimal_time,event_time},
  fixed,
  string type, %% Prevents dropping of trailing zeroes
  %% set thousands separator={},
}

We will compare implementations of different DPOR algorithms based on the
number of executions that they explore, as well as the time that this takes.
For some programs, \LAPOR also examines a fair amount
of \emph{blocked} executions (i.e., executions that cannot
be serialized and need to be aborted), which naturally affects its time
performance. In \cref{tab:eval}, we show the number of executions explored by an
entry of the form $T$+$B$, where $T$ is the number of complete traces and $B$
is the number of blocked executions. (We omit the $B$ part when
it is zero.)
% Refer to \cref{tab:eval}.

\begin{table}[t]
  \caption{Performance of different DPOR algorithm implementations.}
  \label{tab:eval}
  %% \smallertabcaptionspace
  \centering\SZ
  \setlength{\tabcolsep}{2pt}
  %\pgfplotstablevertcat{\output}{results/laban/writers.txt}
  \pgfplotstablevertcat{\output}{results/laban/posters.txt}
  \pgfplotstablevertcat{\output}{results/laban/buyers.txt}
  \pgfplotstablevertcat{\output}{results/laban/ping_pong.txt}
  %\pgfplotstablevertcat{\output}{results/laban/2PC.txt}
  \pgfplotstablevertcat{\output}{results/laban/consensus.txt}
  %\pgfplotstablevertcat{\output}{results/laban/db_cache.txt}
  \pgfplotstablevertcat{\output}{results/laban/prolific.txt}
  %\pgfplotstablevertcat{\output}{results/laban/mat_mult.txt}
  \pgfplotstablevertcat{\output}{results/laban/sparse-mat.txt}
  \pgfplotstablevertcat{\output}{results/laban/plb.txt}
  \pgfplotstabletypeset[
    every row no 3/.style={before row=\midrule},
    every row no 6/.style={before row=\midrule},
    every row no 9/.style={before row=\midrule},
    every row no 12/.style={before row=\midrule},
    every row no 15/.style={before row=\midrule},
    every row no 18/.style={before row=\midrule},
  ]{\output}
\end{table}

%% \iffref{app:artifact-link}{See \cref{app:artifact-link} for}{The
%%   supplementary material contains} a link to an artifact with all
%%   benchmarks and pre-compiled tools.

All the benchmark programs we use are parametric, typically on the number of
threads used (and thus messages posted); their parameters are shown inside
parentheses.
%
In the first program (\bench{posters}), each thread posts to a single event
handler two messages containing stores to some atomic global variable, and
then the value of this variable is checked by an assertion. This simple
program allows us to establish the baseline speed of all implementations. We
can see that \GenMC~\genmcmo{\small} is the fastest here. The reason is that 
it does not perform any checks whether the explored executions are sequentially 
consistenct, which allows it to be five times faster than \LAPOR, and seven 
to nine times faster than \Nidhugg's algorithm implementations. We can also
notice that \EventDPOR incurs a small but noticeable overhead over
\OptimalDPOR for the extra machinery that its implementation requires.

The next two benchmarks were taken from a paper by Kragl et al.~\citet{Kragl20}.
%
In \bench{buyers}, $n$ ``buyer'' threads coordinate the purchase of an item
from a ``seller'' as follows: one buyer requests a quote for the item from the
seller, then the buyers coordinate their individual contribution, and finally
if the contributions are enough to buy the item, the order is placed.
%
In \bench{ping-pong}, the ``pong'' handler thread receives messages with
increasing numbers from the ``ping'' thread, which are then acknowledged back
to the ``ping'' event handler.

Looking at~\cref{tab:eval}, we notice that, in both \bench{buyers} and
\bench{ping-pong}, all algorithms explore the same number of traces, but
\LAPOR also explores a significant number of executions that cannot be
serialized and need to be aborted. In fact, for both benchmarks, the aborted
executions significantly outnumber the traces explored.  This affects
negatively the time that \LAPOR takes, and \GenMC \lapormo{\small} becomes the
slowest implementation.  In contrast, \EventDPOR does not suffer from this
problem and shows similar scalability as baseline \GenMC and \OptimalDPOR.

With the four remaining benchmarks, we evaluate all implementations in programs
where algorithms tailored to event-driven programming, either natively
(\EventDPOR) or which are lock-aware (when handlers are implemented as locks),
have an advantage.
%
The first program (\bench{consensus}), again from the paper by Kragl et
al.~\citet{Kragl20}, is a simple \emph{broadcast consensus} protocol for $n$
nodes to agree on a common value. For each node~$i$, two threads are created:
one thread executes a \texttt{broadcast} method that sends the value of
node~$i$ to every other node, and the other thread is an event handler that
executes a \texttt{collect} method which receives~$n$ values and stores the
maximum as its decision. Since every node receives the values of all other
nodes, after the protocol finishes, all nodes have decided on the same value.
%
%% The second benchmark (\bench{db-cache}) is a key-value storage system
%% inspired from Memcached, a well known distributed cache application. There
%% are $n$ clients requesting a fixed sequence of storage accesses to a server
%% via UDP sockets (modeled as threads with event queues). On the server side
%% there is one worker thread per client to fulfill these requests. So
%% multiple worker threads on the server threads may race.
%
The next program (\bench{prolific}) is synthetic: $n$ threads send $n$
messages with an increasing number of stores to and loads from an atomic
global variable to one event handler.
%
The \bench{sparse-mat} program computes the number of non-zero elements of a
sparse matrix of dimension $m \times n$, by dividing the work into $n$ tasks
sent as messages to different handlers, which compute and join their results.
%
The last benchmark (\bench{plb}) is taken from a paper by Jhala and
Majumdar~\citet{popl07:JhalaM}. A fixed sequence of task requests is received
by the main thread. Upon receiving a task, the main thread allocates a space
in memory and posts a message with the pointer to the allocated memory that
will be served by a thread in the future.

Refer again to~\cref{tab:eval}.
%
In \bench{consensus}, all algorithms start with the same number of traces, but
\LAPOR and \EventDPOR need to explore fewer and fewer traces than the other
two algorithms, as the number of nodes (and threads) increases. Here too,
\LAPOR explores a significant number of executions that need to be aborted,
which hurts its time performance. On the other hand, \EventDPOR's handling
of events is optimal here.
%
The \bench{prolific} program shows a case where algorithms not tailored to
events (or locks) explore $(n-1)!$ traces, while \LAPOR and \EventDPOR explore
only $2^n-2$ consistent executions, when running the benchmark with
parameter $n$. It can also be noted that \EventDPOR scales \emph{much} better
than \LAPOR here in terms of time, due to the extra work that \LAPOR needs to
perform in order to check consistency of executions (and abort some of them).
%
The \bench{sparse-mat} program shows another case where algorithms that are
not tailored to events explore a large number of executions unnecessarily
(\timeout denotes timeout). This program also shows that \EventDPOR beats
\LAPOR time-wise even when \LAPOR does not explore executions that need to be
aborted.
%
Finally, \bench{plb} shows a case on which \EventDPOR and \LAPOR really
shine. These algorithms need to explore only one trace, independently of the
size of the matrices and messages exchanged, while DPOR algorithms not
tailored to event-driven programs explore a number of executions which
increases exponentially and fast.

We remark that, in all benchmarks, the inexpensive checks for redundancy were
sufficient, and \EventDPOR explored the optimal number of traces.
Results from an extended set of benchmarks appear in~\cref{app:eval-complete}.
