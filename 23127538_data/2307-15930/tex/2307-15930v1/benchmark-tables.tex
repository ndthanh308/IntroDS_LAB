%% -*- mode: LaTeX; fill-column: 78; -*-

\section{Complete Set of Benchmark Tables} \label{app:eval-complete}
%===================================================================
In this appendix, we include the complete set of benchmark results comparing
the performance of the \EventDPOR with that of the \OptimalDPOR algorithm,
with the LAPOR technique implemented in \GenMC and also with the baseline
algorithm of \GenMC which tracks the modification order (\genmcmo{\small}) of
shared variables. A subset of these results appears in the main body of the
paper.

\paragraph{Baseline Comparison}
%------------------------------
First, we measure the performance of algorithm implementations on three
programs where all algorithms explore the same number of executions.  The
first two of them are simple programs where a number of threads post racing
messages to a \emph{single} event handler. Both programs are parametric on the
number of threads (and messages posted); the value of this parameter is shown
inside parentheses. The messages of the first program (\bench{writers})
consist of a store to the same atomic global variable followed by an assertion
that checks for the value written. The second program (\bench{posters}) is
similar but between the write and the assertion check the messages also post,
to the same handler, another message with an atomic store to the same global
variable; this increases the number of executions to examine.
%
Finally, the third program (\bench{2PC}) is a two-phase commit protocol used
by a coordinator and $n$ participant threads (i.e., $n+1$ handler threads in
total) to decide whether to commit or abort a transaction, by broadcasting and
receiving messages.

\begin{table}[t]
  \caption{Performance on programs where different DPOR algorithms implemented
    in \Nidhugg and \GenMC explore the same number of complete and blocked
    executions. Times (in seconds) show the relative speed of their
    implementations.}
  \label{tab:eval:baseline}
  %% \smallertabcaptionspace
  \centering\SZ
  %\setlength{\tabcolsep}{1pt}
  \pgfplotstablevertcat{\output}{results/laban/writers.txt}
  \pgfplotstablevertcat{\output}{results/laban/posters.txt}
  \pgfplotstablevertcat{\output}{results/laban/2PC.txt}
  \pgfplotstabletypeset[
    every row no 3/.style={before row=\midrule},
    every row no 6/.style={before row=\midrule},
  ]{\output}
\end{table}

Results from running these benchmarks for increasing number of threads are
shown in~\cref{tab:eval:baseline}. As can be seen, all algorithms explore the
same number of executions here. This allows us to establish that:
\begin{enumerate}[(i)]
\item \GenMC \genmcmo{\small} is fastest overall; in particular, it is $3$--$7$
  times faster than \Nidhugg \opt{\small} and about $8$--$9$ times faster
  than \Nidhugg \evt{\small}.
\item The overhead that LAPOR incurs over its baseline implementation in
  \GenMC is significant.
% (four to more than ten times slower).
  Still, for the first two programs, which involve just one event handler and
  no blocked or aborted executions, \GenMC \lapormo{\small} beats \Nidhugg
  \evt{\small}.  However, \Nidhugg \evt{\small} is faster than \GenMC
  \lapormo{\small} on the third program (\bench{2PC}).
\item The overhead that \EventDPOR incurs over \OptimalDPOR for the extra
  machinery that its implementation requires is small but quite noticeable.
\end{enumerate}
%
The results from \bench{2PC} corroborate these conclusions. The blocked
executions in this benchmark are due to \emph{assume-blocking} and affect all
algorithms equally in terms of additional executions examined.  However,
notice that \GenMC \lapormo{\small} is affected more in terms of time overhead
compared to its baseline.

\paragraph{Performance on More Involved Event-Driven Programs}
The next two benchmarks were taken from a recent paper by Kragl et
al.~\citet{Kragl20}.
%
In \bench{buyers}, $n$ ``buyer'' threads coordinate the purchase of an item
from a ``seller'' as follows: one buyer requests a quote for the item from the
seller, then the buyers coordinate their individual contribution, and finally
if the contributions are enough to buy the item, the order is placed.
%
In \bench{ping-pong}, the ``pong'' handler thread receives messages with
increasing numbers from the ``ping'' thread, which are then acknowledged back
to the ``ping'' event handler.

\begin{table}[t]
  \caption{Performance on programs where different DPOR algorithms implemented
    in \Nidhugg and \GenMC examine the same number of traces, but LAPOR also
    explores a significant number of executions that need to be aborted.
    This negatively affects the runtime that SMC using LAPOR takes.}
  \label{tab:eval:whenblocked}
  \setlength{\tabcolsep}{2.5pt}
  \centering\SZ
  %% \pgfplotstablevertcat{\output}{results/laban/n2W.txt}
  \pgfplotstablevertcat{\output}{results/laban/buyers.txt}
  \pgfplotstablevertcat{\output}{results/laban/ping_pong.txt}
  \pgfplotstabletypeset[
    every row no 3/.style={before row=\midrule},
  ]{\output}
\end{table}

Results from running these benchmarks are shown
in~\cref{tab:eval:whenblocked}.
%
In these two programs, all algorithms explore the same number of traces, but
LAPOR also explores a significant number of executions that cannot be
serialized and need to be aborted. This negatively affects the time that SMC
using LAPOR requires; \GenMC \lapormo{\small} becomes the slowest
configuration here.  In contrast, \Nidhugg \evt{\small} shows similar
scalability as baseline \GenMC and \Nidhugg \opt{\small}.

\paragraph{Performance on Event-Driven Programs Showing Complexity
  Differences Between DPOR Algorithms}
Finally, we evaluate all algorithms in programs where algorithms tailored to
event-driven programming, either natively (\EventDPOR) or which are
lock-aware (when handlers are implemented as locks), have an advantage.
%
We use six benchmarks.
%
The first (\bench{consensus}), again from the paper by Kragl et
al.~\citet{Kragl20}, is a simple \emph{broadcast consensus} protocol for $n$
nodes to agree on a common value. For each node~$i$, two threads are created:
one thread executes a \texttt{broadcast} method that sends the value of
node~$i$ to every other node, and the other thread is an event handler that
executes a \texttt{collect} method which receives~$n$ values and stores the
maximum as its decision. Since every node receives the values of all other
nodes, after the protocol finishes, all nodes have decided on the same value.
%
The second benchmark (\bench{db-cache}) is a key-value store system inspired
from Memcached, a well known distributed cache application. There are $n$
clients requesting a fixed sequence of storage accesses to a server via UDP
sockets (modeled as threads with mailboxes). On the server side there is
one worker thread per client to fulfill these requests. So multiple worker
threads on the server threads may race.
%
The third benchmark (\bench{prolific}) is synthetic: $n$ threads send
$n$ messages with an increasing number of stores to and loads from an atomic
global variable to one event handler.
%
The fourth benchmark (\bench{sparse-mat}) computes sparseness (number of
non-zero elements) of a sparse matrix of dimension $m \times n$. The work is
divided among $n$ tasks/messages and sent to different handlers, which then
compute and join these results.
%
The fifth benchmark (\bench{mat-mult}) implements concurrent matrix
multiplication taking two matrices of dimensions $m \times k$ and $k \times n$
as inputs. The work is divided among $n$ tasks/messages and sent to different
handlers, which then compute and join these results.
%
The last benchmark (\bench{plb}) is taken from a paper by Jhala and
Majumdar~\citet{popl07:JhalaM}. The main thread receives a fixed sequence of
task requests. Upon receiving a task, the main thread allocates a space in
memory and posts a message with the pointer to the allocated memory that will
be served by a thread in the future.

%% The sixth benchmark \sdcmt{To be added}, originally in
%% \textsc{LibEel}~\cite{hotos05:CunninghamK} (a library supporting
%% asynchrounous function calls) was transformed by hand to events and event
%% handlers.  The benchmark contains $N$ {\tt listen} messages being posted to
%% a master handler. Each of them spawns one client handler and posts a {\tt
%% new\_client} message to it. The {\tt new\_client} message in each client
%% handler is racing to acquire a shared device {\tt dev} (a global variable)
%% and writes to it by posting a {\tt write} message when successful. Upon
%% failing to acquire the device, it tries again by posting another {\tt
%% new\_client} message to the same client handler. In total $n$ {\tt
%% new\_client} messages are created, which are all conflicting with each
%% other.

\begin{table}[t]
  \caption{Performance on programs that show complexity differences in the
    number of traces that different DPOR algorithms implemented in \Nidhugg
    and \GenMC explore.}
  \label{tab:eval:differences}
  \setlength{\tabcolsep}{1.5pt}
  \centering\SZ
  \pgfplotstablevertcat{\output}{results/laban/consensus.txt}
  \pgfplotstablevertcat{\output}{results/laban/db_cache.txt}
  \pgfplotstablevertcat{\output}{results/laban/prolific.txt}
  \pgfplotstablevertcat{\output}{results/laban/sparse-mat.txt}
  \pgfplotstablevertcat{\output}{results/laban/mat_mult.txt}
  \pgfplotstablevertcat{\output}{results/laban/plb.txt}
  \pgfplotstabletypeset[
    every row no 3/.style={before row=\midrule},
    every row no 6/.style={before row=\midrule},
    every row no 9/.style={before row=\midrule},
    every row no 12/.style={before row=\midrule},
    every row no 15/.style={before row=\midrule},
  ]{\output}
\end{table}

Results from running these six benchmarks are shown in~\cref{tab:eval:differences}.

In \bench{consensus}, all algorithms start with the same number of traces, but
\LAPOR and \EventDPOR need to explore fewer and fewer traces than the other
two algorithms, as the number of nodes (and threads) increases. Here too,
\LAPOR explores a significant number of executions that need to be aborted,
which hurts its time performance. On the other hand, \EventDPOR's handling
of events is optimal in this program, even though it is not non-branching.

The \bench{db-cache} program shows a case where \GenMC, both when running with
\genmcmo{\small} but also with \lapormo{\small}, is non-optimal.  In contrast,
\EventDPOR, even with employing the inexpensive redundancy checks, manages to
explore an optimal number of traces.

The \bench{prolific} program shows a case where algorithms not tailored to
events (or locks) explore $(n-1)!$ traces, while \LAPOR and \EventDPOR explore
only $2^n-2$ consistent executions, when running the benchmark with $n$ nodes.
%
We briefly explain why the number of feasible executions are $2^n-2$. Because
of the access patterns of global variables in this program, each message is
conflicting with the previous and the next messages. In an execution, these
conflicts can be represented by $n$ directed edges. So there are $2^n$
possible reorderings when both directions of each edge are considered. But two
of these reorderings are not possible because they create a cycle, hence the
$2^n-2$.
%
On this program, it can also be noted that \EventDPOR scales \emph{much}
better than \LAPOR here in terms of time, due to the extra work that \LAPOR
needs to perform in order to check consistency of executions (and abort some
of them).

The \bench{sparse-mat} program shows another case where algorithms that are
not tailored to events explore a large number of executions unnecessarily
(\timeout denotes timeout). This program also shows that \EventDPOR beats
\LAPOR time-wise even when \LAPOR does not explore executions that need to be
aborted.

Finally, \bench{plb} shows a case on which \EventDPOR and \LAPOR really
shine. These algorithms need to explore only one trace, independently of the
size of the matrices and messages exchanged, while DPOR algorithms not
tailored to event-driven programs explore a number of executions which
increases exponentially and fast.
