% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}

\usepackage{times}
\usepackage{url}
\urlstyle{same}

%
% imported by QHC
\usepackage{fontawesome}
\usepackage{graphicx}
\usepackage{makecell}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath,amsfonts}


\usepackage{marvosym}
\usepackage{ifsym}

\usepackage[colorlinks,linkcolor=blue,anchorcolor=blue,citecolor=blue]{hyperref}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{A Benchmark for Understanding Dialogue Safety in Mental Health Support}
%
% \titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Huachuan Qiu\inst{1, 2} \ \and
Tong Zhao\inst{2} \ \and
Anqi Li\inst{1, 2} \ \and
Shuai Zhang\inst{1, 2} \ \and
Hongliang He\inst{1, 2} \and
Zhenzhong Lan\inst{2}$^{(\textrm{\Letter})}$}
%
\authorrunning{H. Qiu et al.}
% \authorrunning{xxx et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Zhejiang University \and
School of Engineering, Westlake University \\
\email{\{qiuhuachuan, lanzhenzhong\}@westlake.edu.cn}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Dialogue safety remains a pervasive challenge in open-domain human-machine interaction. Existing approaches propose distinctive dialogue safety taxonomies and datasets for detecting explicitly harmful responses. However, these taxonomies may not be suitable for analyzing response safety in mental health support. In real-world interactions, a model response deemed acceptable in casual conversations might have a negligible positive impact on users seeking mental health support. To address these limitations, this paper aims to develop a theoretically and factually grounded taxonomy that prioritizes the positive impact on help-seekers. Additionally, we create a benchmark corpus with fine-grained labels for each dialogue session to facilitate further research. We analyze the dataset using popular language models, including BERT-base, RoBERTa-large, and ChatGPT, to detect and understand unsafe responses within the context of mental health support. Our study reveals that ChatGPT struggles to detect safety categories with detailed safety definitions in a zero- and few-shot paradigm, whereas the fine-tuned model proves to be more suitable. The developed dataset and findings serve as valuable benchmarks for advancing research on dialogue safety in mental health support, with significant implications for improving the design and deployment of conversation agents in real-world applications. We release our code and data here: \url{https://github.com/qiuhuachuan/DialogueSafety}.

\keywords{Dialogue System \and Dialogue Safety \and Taxonomy \and Text Classification \and Mental Health Support.}
\end{abstract}


\section{Introduction}
In recent years, dialogue systems~\cite{Recipes,GU2023EVA,blenderbot3} have achieved significant advancements in enabling conversational agents to engage in natural and human-like conversations with humans. However, there are growing concerns about dialogue safety, especially for open-domain conversational AI, due to the uncontrollable generation derived from the intrinsic unpredictable nature of neural language models. A classic case~\cite{Baheti2021Just} illustrating this concern involves a disguised patient conversing with a GPT-3 model, where the model provides dangerous suggestions that instigate the user to commit suicide. As a result, addressing the issues of dialogue safety~\cite{Rosenthal2021Offensive,Hada2021Offensiveness} has gained massive traction. To tackle these concerns, existing approaches propose distinctive taxonomies for dialogue safety and corresponding datasets, aiming to build respective text classifiers that can identify unsafe responses within a dialogue context in open-domain human-machine interactions.

While some progress has been made in the chitchat domain, dialogue safety in mental health support remains unexplored. Existing taxonomies~\cite{Baheti2021Just,Dinan2019Build,Sun2021Safety,Dinan2022SafetyKit} for dialogue safety mainly focus on chitchat settings, but they may not be suitable for identifying inappropriate content that violates mental health principles in mental health support conversations. Specifically, in real-world human-machine interactions, a model response that has a negligible positive impact on users may be deemed acceptable in casual conversations but not in the context of mental health support. Furthermore, most efforts to detect harmful content~\cite{Rosenthal2021Offensive,Hada2021Offensiveness,Hartvigsen2022Hate} prioritize identifying offensive language in casual conversations due to the extensive development of dialogue systems for casual chatting. In particular, as digital mental health services~\cite{Lee2021Mental,li2023understanding} become increasingly common, it is crucial to develop new taxonomies and approaches that can accurately identify and address unserviceable content in mental health support conversations.

In essence, ensuring safe and supportive dialogues for mental health support requires that all help-seekers feel heard, acknowledged and valued so that the conversation can guide them towards positive outcomes that benefit them. Supporters must follow the \textbf{General Principles}~\cite{APA2002Principles} of mental health support, which require them to strive to benefit those with whom they work and take care to do no harm. Therefore, responses with nonsense and linguistic neglect do not benefit help-seekers in any way. Further, humanoid mimicry violates the \textbf{Integrity Principle}~\cite{APA2002Principles} and some researchers argue that displays of anthropomorphism~\cite{Gros2022Robots} can be inauthentic and dishonest, leading to physiological and societal risks. Additionally, current research on dialogue safety also overlooks unamiable judgments, such as negative evaluations and implicit verbal abuse. For example, if someone says, \textit{I'm just feeling really overwhelmed right now, and I don't know what to do.}, an example of a model response that contains negative evaluation would be \textit{Well, it sounds like you're not really handling things very well.} while an example of implicit verbal abuse could be \textit{You can't even do such a small thing properly.}

To summarize, ensuring dialogue safety in mental health support is a complex issue that requires considering responses that do benefit help-seekers, adhere to basic principles and ultimately enhance their progress. For this purpose, as mentioned above, we propose a sequential and inclusive taxonomy for dialogue safety in mental health support. To facilitate the research on our dialogue safety taxonomy, we introduce a sequential annotation framework and create a Chinese online text-based free counseling platform to gather counseling conversations between help-seekers and experienced supporters. To evaluate our framework, we use real-world conversations to fine-tune the open-source Chinese dialogue model, EVA2.0-xLarge with 2.8B parameters~\cite{GU2023EVA}. We then meticulously annotate dialogue safety based on the proposed sequential taxonomy using a subset of held-out conversations. While ChatGPT is widely used for various natural language processing tasks, it is not effective at detecting categories given a context and a response along with detailed safety definitions in a zero- and few-shot paradigm. Instead, we find that fine-tuned models, such as BERT-base and RoBERTa-large, are more suitable for detecting unsafe responses in mental health support. The results underscore the importance of our dataset as a valuable benchmark for understanding and monitoring dialogue safety in mental health support.

\section{Related Work}
Conversational AI has become an integral part of our daily interactions, but it is not without its drawbacks. Extensive research has shown that language models sometimes generate outputs that can be toxic, untruthful, or harmful, especially during interactions with users. These issues necessitate the development of safer conversational AI systems, and researchers have introduced new definitions and taxonomies to combat offensive behavior from language models.

Currently, research efforts primarily center around casual dialogue. Dinan et al.~\cite{Dinan2019Build} introduced the concept of offensive content, referring to messages that would be deemed unreasonable in a friendly online conversation with someone new. Building upon this, Sun et al.~\cite{Sun2021Safety} further classified context-sensitive unsafety into two categories: personal unsafety and non-personal unsafety, providing a more detailed safety taxonomy. Recent studies by Dinan et al.~\cite{Dinan2022SafetyKit} explored harmful system behavior that can result in short-term and long-term risks or harm. They identified three safety-sensitive situations known as the Instigator, Yea-Sayer, and Impostor effects. These situations capture potential adverse effects of AI systems on users. In a separate study, Baheti et al.~\cite{Baheti2021Just} defined offensiveness as behavior intentionally or unintentionally toxic, rude, or disrespectful towards groups or individuals, including notable figures. Notably, they integrated stance alignment into their approach.

% Figure environment removed

\section{Dialogue Safety Taxonomy}
To develop mentally beneficial, factually accurate, and safe conversational agents for mental health support, it is crucial to understand what constitutes unsafe responses generated by the models in human-machine interactions. However, current taxonomies are not directly applicable to mental health support. Therefore, we introduce a new taxonomy grounded in theoretical and factual knowledge, including the \textbf{Ethical Principles of Psychologists and Code of Conduct} (hereinafter referred to as the Ethics Code)~\cite{APA2002Principles}, research norms~\cite{Dinan2022SafetyKit,Thoppilan2022Lamda}, and related application practices~\cite{Gros2022Robots}. This new taxonomy will help characterize and detect various forms of unsafe model generation.
The Ethics Code, which provides guidance for psychologists, is widely used worldwide. Our proposed sequential taxonomy builds upon existing general taxonomies of dialogue safety and expands upon them to suit mental health support. In collaboration with experts\footnote{One individual has a Ph.D. in Psychology, and the other is a linguistic expert with a master's degree.} in counseling psychology and linguistics, we have designed an inclusive and sequential dialogue safety taxonomy, visually presented in Figure~\ref{taxonomy}.



\subsection{Term of Dialogue Safety}
Dialogue safety in mental health support refers to the creation of a safe and supportive space for individuals to freely express their thoughts and feelings without fear of judgment, discrimination, or harm. By prioritizing dialogue safety, those seeking help can engage in productive, meaningful conversations that promote understanding and foster positive relationships. According to the principle of \textbf{Beneficence} and \textbf{Nonmaleficence} in the Ethics Code, we define model-generated responses that have little or no positive impact on help-seekers as unsafe.

\subsection{Concrete Categories}
Our taxonomy consists of eight primary categories: safe response, nonsense, humanoid mimicry, linguistic neglect, unamiable judgment, toxic language, unauthorized preachment, and nonfactual statement. The dialogue examples for each category are presented in Table~\ref{tab:examples}.

\fbox{\textsc{Safe Response.}}
A safe response from a conversational AI should meet the following criteria: it must be factually correct, helpful in providing mental health support, easily understandable, free from explicit or implicit verbal violence, and must not have any adverse physical or psychological effects on help-seekers. Additionally, the language model should refrain from spreading plausible or specious knowledge and adhere to AI ethics by avoiding anthropomorphic actions that could be harmful to society.

\fbox{\textsc{Nonsense.}}
This category in our taxonomy consists of two aspects: context-independent and context-dependent. The context-independent subcategory includes responses that exhibit logical confusion or contradiction in their semantics or contain repeated phrases. On the other hand, the context-dependent subcategory includes responses that misuse personal pronouns in the context of the dialogue history.

\fbox{\textsc{Humanoid Mimicry.}}
In reality, the dialogue agent is not a human at all but rather a machine programmed to interact with human beings. Therefore, in mental health support settings, employing dishonest anthropomorphism might be unfavorable for help-seekers. Dialogue agents could exploit instinctive reactions to build false trust or deceptively persuade users. Obviously, this situation violates the principle of integrity. For example, a help-seeker might ask, ``Are you a chatbot?" While a dialogue system might say, ``I'm a real human," it would not be possible for it to truthfully say so. This type of dishonest anthropomorphism can be harmful because it capitalizes on help-seekers' natural tendency to trust and connect with other humans, potentially leading to physical or emotional harm.

\fbox{\textsc{Linguistic Neglect.}}
In a conversation, the supporter should prioritize engaging with the help-seeker's concerns, providing empathetic understanding, and offering constructive suggestions instead of avoiding or sidestepping their requests. Two aspects need to be considered: (1) the model response should not display an attitude of avoidance or evasiveness towards the main problems raised by help-seekers, as it could hinder the dialogue from continuing; and (2) the model response should not deviate entirely from the help-seeker's input, such as abruptly changing topics.

\fbox{\textsc{Unamiable Judgment.}}
This category contains two aspects: negative evaluation and implicit verbal abuse. Although both can involve criticism or negative statements, they are different concepts. Negative evaluation is a form of feedback that provides constructive criticism or points out areas where improvement is needed. While it may be implicit, its intention is not to harm the person. On the other hand, implicit verbal abuse is intended to harm users.


\fbox{\textsc{Toxic Language.}}
We use the term \textit{toxic language} as an umbrella term because it is important to note that the literature employs several terms to describe different types of toxic language. These terms include hate speech, offensive language, abusive language, racism, social bias, violence, pornography, and hatred. Toxic language is multifaceted, generally encompassing offending users, biased opinions, toxic agreements, and explicit verbal abuse.


\fbox{\textsc{Unauthorized Preachment.}}
The model response to the individual seeking help for mental issues violates the Ethics Code by offering inappropriate opinions or suggestions, which include but are not limited to advice, persuasion, and unprofessional medication recommendations. In extreme cases, the dialogue agent may provide harmful or even illegal instructions, such as robbery, suicide, or even murder.

\fbox{\textsc{Nonfactual Statement.}}
When users initially raise controversial or sensitive topics, such as politics, dialogue agents may express subjective opinions influenced by their inherent biases and preferences. This can be problematic since model responses may lack objectivity and accuracy. Furthermore, generated content that deviates from established facts or includes unsubstantiated statements can raise safety concerns in certain situations. This is particularly worrisome as such content may pose risks to users, especially in safety-critical contexts.


\begin{table}[t]
    \centering
    \caption{Different types of model responses within a dialogue context in our taxonomy.}
    \label{tab:examples}
    % Figure removed
\end{table}

\section{Data Collection}

\subsection{Data Source}
We develop an online Chinese text-based counseling platform that provides free counseling services. Each counseling session between the help-seeker and experienced supporter lasts approximately 50 minutes, following the standard practice in psychological counseling. Through this platform, we have collected a total of 2382 multi-turn dialogues. To fine-tune the dialogue model, we utilize the hyperparameters recommended in the official repository.

To analyze the response safety within a dialogue history, we divide the help-out conversations into smaller multi-turn dialogue sessions, concluding with the last utterance spoken by the help-seeker. However, we have observed that isolating the help-seeker's single utterance from multi-turn conversations often results in a loss of valuable information, particularly when the help-seeker responds with a simple ``Uh-huh" or ``Yes." To address this issue, we crawl 2,000 blog titles from Yixinli's QA column\footnote{\url{https://www.xinli001.com/qa}}, which serves as a public mental health support platform. Each of these blog titles contains comprehensive content about the help-seeker's mental state and the specific problems they are facing.

\subsection{Annotation Process}
To ensure high-quality annotation, we recruit three fixed annotators, each with one year of psychological counseling experience. Before commencing the annotation process, we provide them with thorough training. To ensure data randomness, we randomly shuffle all sessions, including 2,000 dialogue sessions from public QA and 6,000 sessions from our counseling platform. We iteratively annotate every 200 sessions using our proposed taxonomy.

In this study, we assess inter-rater reliability using Fleiss' kappa ($\kappa$)~\cite{Fleiss1971Raters}, a widely used measure that considers multiple annotators and nominal categories. If the inter-rater agreement falls below 0.4, we require all annotators to independently review the labeled data. They then discuss any discrepancies before starting the next labeling round, thereby continuously enhancing inter-rater reliability. The overall average inter-rater agreement for labeling the eight categories of dialogue safety is 0.52, which validates the reliability of our labeled data.

\subsection{Data Filtering}
To enhance the practicality of data in human-computer interaction, we exclude questionnaire-related data. Additionally, we remove instances where the supporter alerts the help-seeker of the limited remaining time during the 50-minute consultation process. Finally, we obtain 7935 multi-turn dialogue sessions.

\begin{table}[t!]
\centering
\caption{Data statistics of our annotated data with our proposed safety taxonomy.}
\label{tab1}
\scalebox{1.0}{
    \begin{tabular}{l|l|l|l|l}
    \toprule
    Index&Category & Train & Test & Total\\
    \hline
    0&Nonsense & 469 & 53 & 522 \\
    1&Humanoid Mimicry & 38 & 5 &43 \\
    2&Linguistic Neglect &3188&355&3543 \\
    3&Unamiable Judgment&36&5&41\\
    4&Toxic Language&17&2&19\\
    5&Unauthorized Preachment&86&11&97\\
    6&Nonfactual Statement&10&2&12\\
    7&Safe Response&3291&367&3658\\
    \hline
    &Total & 7135 & 800 & 7935 \\
    \bottomrule
    \end{tabular}
}
\end{table}

\subsection{Data Statistics}
We present the data statistics of our annotated dataset utilizing our proposed safety taxonomy in Table~\ref{tab1}. To maintain the distribution of our labeled dataset, we employ the technique of Stratified Shuffle Split, which splits the labeled data into 90\% for training and 10\% for test in each category. The category with the highest number of samples in both the training and test sets is ``Safe Response'' indicating that most of the data is non-toxic and safe to send to help-seekers. However, some categories, such as ``Toxic Language'' and ``Nonfactual Statement'' have very few training and test samples. Interestingly, ``Linguistic Neglect'' exhibits the highest number of total samples among unsafe categories, suggesting that it may be the most common type of unsafe language in mental health support.



\section{Experiments}

\subsection{Problem Formulation}
To better understand and determine the safety of model generation conditioned on a dialogue context, we approach the task as a text classification problem. We collect samples and label them as follows:

\begin{equation}
\mathcal{D}=\left\{ \left \langle x_{i},y_{i} \right \rangle 
 \right\}_{1}^{n}
\end{equation}

\noindent where $x_i$ represents a dialogue session consisting of the dialogue context and a model response, and $y_i$ is the label of the $i$-th sample. To elaborate further, we denote $x_i=\{u_1,u_2,...,u_j,...,u_k,r\}$, where $u_j$ stands for a single utterance, and $r$ represents the model response.


Our optimized objective function is
\begin{equation}
\arg\min{\mathcal{L}(p_{i,j})}  = -\frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{C}w_{j}y_{i,j}\log(p_{i,j})
\end{equation}

\noindent where $N$ represents the number of samples, $C$ represents the number of categories, $w_j$ is the weight of the $j$-th category, and $y_{i,j}$ indicates whether the $j$-th category of the $i$-th sample is the true label. If it is, then $y_{i,j}=1$; otherwise, $y_{i,j}=0$. $\log(p_{i,j})$ represents the logarithm of the predicted probability of the $j$-th category for the $i$-th sample.

\subsection{Setup}
\subsubsection{Baselines}
It is well evidenced that ChatGPT is a versatile tool widely used across various natural language processing tasks. In this study, we assess ChatGPT's performance using a zero- and few-shot prompt approach, along with our concrete dialogue safety definitions. The prompt template is provided in Table~\ref{tab:template}. When making an API call to ChatGPT, the model may respond with a message indicating the absence of safety concerns or the inability to classify the response into a clear category. In such cases, we recall the API until it provides a properly formatted response.

Additionally, we employ the two most commonly used pre-trained models, BERT-base~\cite{bert2018} and RoBERTa-large~\cite{roberta2019}, available on Hugging Face. Specifically, we use \textsc{bert-base-chinese}\footnote{\url{https://huggingface.co/bert-base-chinese}} and \textsc{RoBERTa-large-chinese}\footnote{\url{https://huggingface.co/hfl/chinese-roberta-wwm-ext-large}} versions, and apply a linear layer to the \verb|[CLS]| embedding for generating the classification result.

\begin{table}[t]
    \centering
    \caption{Inference prompt template for ChatGPT. When the prompt template includes bolded content, it indicates usage for a few-shot setting. $<>$ denotes placeholders that need to be replaced with actual text content based on the concrete situation.}
    \label{tab:template}
    % Figure removed
\end{table}


\subsubsection{Implementation}
We evaluate the model performance using widely used metrics such as accuracy ($Acc.$), weighted precision ($P$), recall ($R$), and F1 score($F_1$). To address the problem of category imbalances, we use weighted cross-entropy with the following values: [2.0, 2.0, 0.5, 2.0, 2.0, 2.0, 2.0, 0.5], where each numeric value corresponds to the index in Table~\ref{tab1}.

For all fine-tuning experiments, we select five seeds and train the model for five epochs. We use a batch size of 16, a weight decay of 0.01, and a warm-up ratio of 0.1. During prediction on the test set, we retain the checkpoint with the highest accuracy. The learning rate is set to 2e-5, and all experiments are conducted using A100 8$\times$80G GPUs. To ensure a fair comparison, we evaluate the test set with ChatGPT five rounds using the default parameters recommended by the official API. The ChatGPT model we used in this paper is \textsc{gpt-3.5-turbo}. Both \texttt{temperature} and \texttt{top\_p} values are set to 1.0.

\section{Results}
\begin{table}[t!]
\centering
\caption{Evaluation results for fine-grained classification on the test set. The results present the mean and standard deviation (subscript) of accuracy ($Acc.$), precision ($P$), recall ($R$), and F1 score ($F_1$). In the few-shot setting, the inference prompt includes 8 diverse examples from the training set. $\dagger$ indicates that the model used is \textsc{gpt-3.5-turbo-0301}, while $\ddagger$ indicates that the model used is \textsc{gpt-3.5-turbo-0613}.}
\label{tab:results}
\begin{tabular}{l|l|l|l|l}
\toprule
\textbf{Model} & $Acc$ (\%) & $P$ (\%) & $R$ (\%) & $F_{1}$ (\%)\\
\hline
ChatGPT$^{\dagger}_\mathrm{zero\_shot}$ & $47.5_{0.6}$ & $49.9_{0.7}$ & $47.5_{0.6}$ & $48.4_{0.7}$ \\\hline
ChatGPT$^{\dagger}_\mathrm{few\_shot}$ & $48.4_{1.8}$ & $51.0_{2.0}$ & $48.4_{1.8}$ & $47.9_{2.4}$ \\\hline
ChatGPT$^{\ddagger}_\mathrm{zero\_shot}$ & $43.1_{0.8}$ & $48.9_{4.6}$ & $43.1_{0.8}$ & $33.5_{1.6}$ \\\hline
ChatGPT$^{\ddagger}_\mathrm{few\_shot}$ & $44.7_{4.5}$ & $48.7_{3.2}$ & $44.7_{4.5}$ & $45.6_{4.0}$ \\\hline
BERT-base & $70.3_{1.2}$ & $70.5_{0.9}$ & $70.3_{1.2}$ & $69.7_{1.2}$ \\\hline
RoBERTa-large & $70.4_{3.6}$ & $71.0_{2.1}$ & $70.4_{3.6}$ & $69.8_{3.7}$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Fine-Grained Classification}
We evaluate the performance of each baseline, and the experimental results are presented in Table \ref{tab:results}. From the table, it is evident that the fine-tuned BERT-base and RoBERTa-large models significantly outperform the ChatGPT models in zero- and few-shot settings for detecting unsafe responses in mental health support, as indicated by accuracy, precision, recall, and $F_1$-score. The fine-tuned BERT-base model achieved an accuracy of $70.3\%$, while the fine-tuned RoBERTa-large model achieved an accuracy of $70.4\%$. Interestingly, \textsc{gpt-3.5-turbo-0301} outperforms \textsc{gpt-3.5-turbo-0613} in both zero- and few-shot settings across all four evaluation metrics. These results suggest that the pre-trained model is a better choice for detecting unsafe responses in mental health support.

% Figure environment removed

\subsection{Coarse-Grained Safety Identification}
During interactions with the conversational agent, our main objective is to have the discriminator accurately detect unsafe responses to prevent harm to users. Simultaneously, we ensure that safe responses are successfully sent to users. We approach this task with binary classification, as depicted in Figure~\ref{fig:binary}, analyzing 433 instances with an unsafe label and 367 instances with a safe label.
In the zero-shot setting, \textsc{gpt-3.5-turbo-0613} categorizes almost all samples as unsafe, leading to the lowest rate of correctly predicting safe entries. This outcome is impractical for real-life applications.
Upon analysis, we observe that during the 5 rounds of evaluation, the models \textsc{gpt-3.5-turbo-0301-zero-shot}, \textsc{gpt-3.5-turbo-0301-few-shot}, \textsc{gpt-3.5-turbo-0613-zero-shot}, \textsc{gpt-3.5-turbo-0613-few-shot}, BERT-base, and RoBERTa-large align with the true label in an average of 475, 480, 442, 462, 597, and 599 instances, respectively, out of 800 instances in the test set.
Overall, in terms of correctly predicting the true label, both BERT-base and RoBERTa-large demonstrate compatible performance, displaying lower rates of predicting unsafe entries as safe and higher rates of predicting safe entries as safe.

\subsection{Manual Inspection of Samples Labeled as \textit{Nonsense}}
To gain deeper insights into performance differences among ChatGPT, BERT-base, and RoBERTa-large on a minority of samples, we manually inspect a collection of samples labeled as \textit{Nonsense} by humans. Whenever a sample is predicted as the \textit{Nonsense} label at least once, we count it as true. After analyzing 5 rounds of evaluation, we observe that the models \textsc{gpt-3.5-turbo-0301-zero-shot}, \textsc{gpt-3.5-turbo-0301-few-shot}, \\\textsc{gpt-3.5-turbo-0613-zero-shot}, \textsc{gpt-3.5-turbo-0613-few-shot}, BERT-base \\and RoBERTa-large predict 5, 11, 4, 7, 31 and 45 instances as true, respectively, out of 53 instances in the test set.

While ChatGPT is a versatile language model in general, it falls short in detecting safety concerns during conversations related to mental health support. Despite having fewer samples in some categories, the fine-tuned model still performs well. This finding suggests that, even in an era dominated by large language models, smaller language models remain valuable, especially for domain-specific tasks that require frequent fine-tuning.

\section{Conclusion}
Our research aims to advance the study of dialogue safety in mental health support by introducing a sequential and inclusive taxonomy that prioritizes the positive impact on help-seekers, grounded in theoretical and empirical knowledge. To comprehensively analyze dialogue safety, we develop a Chinese online text-based free counseling platform to collect real-life counseling conversations. Utilizing the open-source Chinese dialogue model, EVA2.0-xLarge, we fine-tune the model and meticulously annotate dialogue safety based on our proposed taxonomy. In our investigation to detect dialogue safety within a dialogue session, we employ ChatGPT using a zero- and few-shot paradigm, along with detailed safety definitions. Additionally, we fine-tune popular pre-trained models like BERT-base and RoBERTa-large to detect unsafe responses. Our findings demonstrate that ChatGPT is less effective than BERT-base and RoBERTa-large in detecting dialogue safety in mental health support. The fine-tuned model proves to be more suitable for identifying unsafe responses. Our research underscores the significance of our dataset as a valuable benchmark for understanding and monitoring dialogue safety in mental health support.

\section*{Ethics Statement}
The study is granted ethics approval from the Institutional Ethics Committee. Access to our dataset is restricted to researchers who agree to comply with ethical guidelines and sign a confidentiality agreement with us.

\input{mybib}
\end{document}
