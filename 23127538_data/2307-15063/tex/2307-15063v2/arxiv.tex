\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{pdfpages}
\usepackage{multido}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{overpic}
\usepackage{booktabs, multirow} % for borders and merged ranges
\usepackage{changepage,threeparttable} % for wide tables
\usepackage{adjustbox}
\usepackage[table,xcdraw,dvipsnames]{xcolor}

\usepackage{caption}
\usepackage{comment}

\newcommand{\mc}[3]{\multicolumn{#1}{#2}{#3}}

\usepackage[breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}


\newcommand{\notsosmall}{\fontsize{10.5pt}{12pt}\selectfont}


\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\iccvfinalcopy 

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}


\definecolor{somegray}{rgb}{0.5, 0.5, 0.5}
\newcommand{\darkgrayed}[1]{\textcolor{somegray}{#1}}
\makeatletter
\newcommand*\titleheader[1]{\gdef\@titleheader{#1}}
\AtBeginDocument{%
  \let\st@red@title\@title
  \def\@title{%
    \vskip-3em
    \bgroup\normalfont\large\centering\@titleheader\par\egroup
    \vskip1.5em\st@red@title}
}
\makeatother

\titleheader{\darkgrayed{This paper has been accepted for publication at the \\
IEEE/CVF International Conference on Computer Vision (ICCV), Paris, 2023.
\copyright IEEE}}

%%%%%%%%% TITLE
\title{% Figure removed 
To Adapt or Not to Adapt? \\ Real-Time Adaptation for Semantic Segmentation}

\begin{document}

\author{Marc Botet Colomer$^*$ $^{1,2}$  \hspace{1cm} Pier Luigi Dovesi$^*$ $^3$ $^\dagger$ \\  Theodoros Panagiotakopoulos$^4$ \hspace{1cm} Joao Frederico Carvalho$^1$ \hspace{1cm} Linus Härenstam-Nielsen$^{5,6}$ \\ Hossein Azizpour$^2$ \hspace{1cm} Hedvig Kjellström$^{2,3}$ \hspace{1cm} Daniel Cremers$^{5,6,7}$ \hspace{1cm} Matteo Poggi$^8$ \vspace{0.3cm}\\ 
\notsosmall $^1$Univrses \hspace{1cm} $^2$KTH \hspace{1cm} $^3$Silo AI  \hspace{1cm} $^4$King  \hspace{1cm} $^5$Technical University of Munich \\ \notsosmall $^6$Munich Center for Machine Learning \hspace{1cm} 
$^7$University of Oxford \hspace{1cm} $^8$University of Bologna \\
\small{\url{https://marcbotet.github.io/hamlet-web/}}
}

\setcounter{figure}{1}

\twocolumn[{
\renewcommand\twocolumn[1][]{#1}
\maketitle
\begin{center}
    \vspace{-0.5cm}
    % Figure removed 
    \label{fig:teaser}
\end{center}
\vspace{-0.4cm}

\small \hypertarget{fig:teaser}{Figure 1.} \textbf{Real-time adaptation with HAMLET.} Online adaptation to continuous and unforeseeable domain shifts is hard and computationally expensive. HAMLET can deal with it at almost 30FPS outperforming much slower online methods -- \textit{\eg{}} OnDA and CoTTA.
\vspace{0.2cm}
}]

\maketitle


\begin{abstract}
   The goal of Online Domain Adaptation for semantic segmentation is to handle unforeseeable domain changes that occur during deployment, like sudden weather events. However, the high computational costs associated with brute-force adaptation make this paradigm unfeasible for real-world applications.
   In this paper we propose HAMLET, a Hardware-Aware Modular Least Expensive Training framework for real-time domain adaptation. Our approach includes a hardware-aware back-propagation orchestration agent (HAMT) and a dedicated domain-shift detector that enables active control over when and how the model is adapted (LT).
   Thanks to these advancements, our approach is capable of performing semantic segmentation while simultaneously adapting at more than 29FPS on a single consumer-grade GPU. Our framework's encouraging accuracy and speed trade-off is demonstrated on OnDA and SHIFT benchmarks through experimental results.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Semantic segmentation aims at classifying an image at a pixel level, based on the local and global context, to enable a higher level of understanding of the depicted scene.\blfootnote{$^*$ Joint first authorship \hspace{0.3cm} $^\dagger$ Part of the work done while at Univrses}
In recent years, deep learning has become the dominant paradigm to tackle this task effectively employing CNNs~\cite{chen2017deeplab,yuan2021segmentation,chen2020naive} or, more recently, transformers~\cite{xie2021segformer}, at the expense of requiring large quantities of annotated images for training.
Specifically, annotating for this task needs per-pixel labeling, which is an expensive and time-consuming task, severely limiting the availability of training data.

% Figure environment removed

The use of simulations and graphics engines~\cite{gta} to generate annotated frames enabled a marked decrease in the time and cost necessary to gather labeled data thanks to the availability of the ground truth.
However, despite the increasing quality in data realism~\cite{thomas_pandikow_kim_stanley_grieve_2021}, there is a substantial difference between simulated data generated by graphics engines and real-world images, such that leveraging these data for real-world applications requires adapting over a significant domain shift.
The promise of unlocking this cheap and plentiful source of training data has provided a major impulse behind the development of a large body of work on Unsupervised Domain Adaptation (UDA) techniques~\cite{cyclegan,dcan,cycada,ganin,fada}, consisting of training semantic segmentation networks on labelled synthetic frames -- the \textit{source} domain -- and then adapting the network to operate on real images, representing the \textit{target} domain, without requiring human annotation.
However, the synthetic-to-real shift represents only one of many possible domain transitions; specifically, when dealing with real-world deployment, domain shifts can occur from various causes, from different camera placements to different lighting, weather conditions, urban scenario, or any possible combination of the above.
Because of the combinatorial nature of the problem, it is simply impossible to evenly represent all possible deployment domains in a dataset. This \textit{curse of dimensionality} prevents having generalized robust perfomances~\cite{Panagiotakopoulos_ECCV_2022,shift}.
However, the recent advent of \textit{online} domain adaptation~\cite{Panagiotakopoulos_ECCV_2022} potentially allows us to face continuous and unpredictable domain shifts at deployment time, without requiring data associated with such domain shifts beforehand.
Nonetheless, despite its potential, several severe limitations still hamper the online adaptation paradigm.
In particular, continuously performing back-propagation on a frame-by-frame schedule~\cite{Panagiotakopoulos_ECCV_2022} incurs a high computational cost, which negatively affects the performance of the network, dropping its overall framerate to accommodate the need for continuous adaptation.
Various factors are involved in this matter: first, the severity of this overhead is proportional to the complexity of the network itself -- the larger the number of parameters, the heavier the adaptation process becomes; second, we argue that frame-by-frame optimization is an excessive process for the adaptation itself -- not only the network might need much fewer optimization steps to effectively counter domain shifts, but also such an intense adaptation definitely increases the likelihood of catastrophic forgetting over previous domains~\cite{kirkpatrick_overcomming_2016, shift}.
In summary, a practical solution for online domain adaptation in semantic segmentation that can effectively operate in real-world environments and applications still seems to be a distant goal.

In this paper, we propose a novel framework aimed at overcoming these issues and thus allowing for real-time, online domain adaptation: 
\begin{itemize}
    \item We address the problem of online training by designing an automatic lightweight mechanism capable of significantly reducing back-propagation complexity. We exploit the model modularity to automatically choose to train the network subset which yields the highest improvement for the allocated optimisation time. This approach reduces back-propagation FLOPS by 34\% while minimizing the impact on accuracy.
    \item In an orthogonal fashion to the previous contribution, we introduce a lightweight domain detector. This allows us to design principled strategies to activate training only when it really matters as well as setting hyperparameters to maximize adaptation speed. Overall, these strategies increase our speed by over $5\times$ while sacrificing less than 2.6\% in mIoU.
    \item We evaluate our method on multiple online domain adaptation benchmarks both fully synthetic~\cite{shift} and semi-synthetic CityScapes domain sequences~\cite{Panagiotakopoulos_ECCV_2022}, showing superior accuracy and speed compared to other test-time adaptation strategies.
\end{itemize}

Fig. \hyperref[fig:teaser]{1} demonstrates the superior real-time adaptation performance of HAMLET compared to slower methods such as CoTTA~\cite{wang2022continual}, which experience significant drops in performance when forced to maintain a similar framerate by adapting only once every 50 frames. In contrast, HAMLET achieves an impressive 29 FPS while maintaining high accuracy. Additionally, Fig. \ref{fig:radar} offers a glimpse of HAMLET's performance on the Increasing Storm benchmark~\cite{Panagiotakopoulos_ECCV_2022}, further highlighting its favorable accuracy-speed trade-off.

\section{Related Work}

We review the literature relevant to our work, about semantic segmentation and UDA, with particular attention to continuous and online methodologies.

\textbf{Semantic Segmentation.} Very much like classification, deep learning plays a fundamental role in semantic segmentation. Fully Convolutional Network (FCN)~\cite{fcn} represents the pivotal step in this field, adapting common networks by means of learned upsample operators (deconvolutions).
Several works aimed at improving FCN both in terms of speed~\cite{yu2018bisenet,nekrasovlight} and accuracy~\cite{chen2017deeplab,deeplabv2,chen2018encoder}, with a large body of literature focusing on the latter. Major improvements have been achieved by enlarging the receptive field~\cite{zhao2017pspnet,yang2018denseaspp,chen2017deeplab,deeplabv2,chen2018encoder}, introducing refinement modules~\cite{fu2019adaptive,zhou2019context,he2019adaptive}, exploiting boundary cues~\cite{chen2016semantic,ding2019boundary,takikawa2019gated} or using attention mechanisms in different flavors~\cite{fu2019dual,li2019expectation,wang2018non,xie2021segmenting}. 
The recent spread of Transformers in computer vision~\cite{dosovitskiy2020image} reached semantic segmentation as well~\cite{xie2021segmenting,yuan2021segmentation,xie2021segformer}, with SegFormer~\cite{xie2021segformer} representing the state-of-the-art in the field and being the object of studies in the domain adaptation literature as well~\cite{hoyer2021daformer}.

\textbf{Unsupervised Domain Adaptation (UDA).}
This body of research aims at adapting a network trained on a \textit{source}, labeled domain to a \textit{target}, unlabeled one. Early approaches rely on the notion of ``style'' and learn how to transfer it across domains~\cite{cyclegan,dcan,cycada,bdl,stylization,yang_fda_2020}. Common strategies consist of learning domain-invariant features~\cite{ganin,ltir}, often using adversarial learning in the process~\cite{ganin,fada,chen_no_2017,hoffman_fcns_2016,adaptsegnet}.
A popular trend in UDA is \textit{Self-Training}. These methods rely on self-supervision to learn from unlabelled data. In UDA, a successful strategy consists of leveraging target-curated pseudo-labels. Popular approaches for this purpose make use of confidence~\cite{cbst,iast,zou2019confidence}, try to balance the class predictions~\cite{zou2018unsupervised,hoyer2021daformer}, or use prototypes~\cite{chen2019progressive,zhang_category_2019,zhang_prototypical_2021} to improve the quality of the pseudo-labels.
Among many domain shifts, the synthetic-to-real one is the most studied, since the earliest works~\cite{cyclegan,dcan,cycada} to the latest~\cite{wu2022d2ada,lee2022bi,hoyer2022hrda,lai2022decouplenet,gong2022tacs,pan2022ml,jiang2022prototypical}. However, this shift is one of a kind since it occurs only once after training, and without the requirement of avoiding forgetting the source domain.
% Figure environment removed

\textbf{Continuous/Test-Time UDA.} This family of approaches marries UDA with continuous learning, thus dealing with the \textit{catastrophic forgetting} issue ignored in the synthetic-to-real case. 
Most \textit{continuous UDA} approaches deal with it by introducing a Replay Buffer~\cite{bobu_adapting_2018,lao2020continuous,kuznietsov2022towards}, while additional strategies make use of style transfer~\cite{wu_ace_2019}, contrastive~\cite{su_gradient_2020, vs2023towards} or adversarial learning~\cite{wulfmeier_incremental_2018}. Despite the definition, continuous UDA often deals with \textit{offline} adaptation, with well-defined target domains over which to adapt. Conceptually similar to it, is the branch of \textit{test-time adaptation}, or \textit{source-free} UDA, although tackling the problem in deployment rather than offline -- \ie{} with no access to the data from the source domain~\cite{Stan2021UnsupervisedMA}. Popular strategies to deal with it consist of generating pseudo-source data to avoid forgetting~\cite{liu_source-free_2021}, freezing the final layers in the model~\cite{liang_we_2020}, aligning features~\cite{liu2021ttt}, batch norm retraining through entropy minimization~\cite{wang2021tent} or prototypes adaptation~\cite{iwasawa2021testtime}.

\textbf{Online UDA.} Although similar in principle to test-time adaptation, online UDA~\cite{shift,Panagiotakopoulos_ECCV_2022,Volpi_2022_CVPR} aims to tackle multiple domain shifts, occurring unpredictably during deployment in real applications and without clear boundaries between them. On this track, the SHIFT dataset~\cite{shift} provides a synthetic benchmark specifically thought for this scenario, while OASIS~\cite{Volpi_2022_CVPR} proposes a novel protocol to evaluate UDA approaches, considering an online setting and constraining the evaluated methods to deal with frame-by-frame sequences.
As for methods, OnDA~\cite{Panagiotakopoulos_ECCV_2022} implements self-training as the orchestration of a static and a dynamic teacher to achieve effective online adaptation while avoiding forgetting, yet introducing massive overhead.

Real-time performance is an essential aspect of online adaptation, particularly in applications such as autonomous driving where slow models are impractical. A slow adaptation process not only limits the practicality of real-world applications but also fails to provide high accuracy until the adaptation is complete, thereby defeating the original purpose. Therefore, accelerating the adaptation process is crucial for achieving high accuracy in real-time scenarios. 

\section{Methods}
This section introduces HAMLET, a framework for \textbf{H}ardware-\textbf{A}ware \textbf{M}odular \textbf{L}east \textbf{E}xpensive \textbf{T}raining.
The framework aims to solve the problem of online domain adaptation with real-time performance through several synergistic strategies. First, we introduce a Hardware-Aware Modular Training (HAMT) agent able to optimize online a trade-off between model accuracy and adaptation time. HAMT allows us to significantly reduce online training time and GFLOPS. Nevertheless, the cheapest training consists of no training at all. Therefore, as the second strategy, we introduce a formal geometric model for online domain shifts that enable reliable domain shift detection and domain estimator signals (Adaptive Domain Detection, Sec.~\ref{subs:adaptive-domain-detection}). These can be easily integrated to activate the adaptation process only at specific times, \textit{as least as possible}. Moreover, we can further leverage these signals by designing adaptive training policies that dynamically adapt domain-sensitive hyperparameters. We refer to these as Active Training Modulations. We present an overview of HAMLET in Fig.~\ref{fig:architecture}.

\subsection{Model Setup}
Our approach builds on the recent progress in unsupervised domain adaptation and segmentation networks. We start with DAFormer~\cite{hoyer2021daformer}, a state-of-the-art UDA method, and adopt SegFormer~\cite{xie2021segformer} as our segmentation backbone due to its strong generalization capacity. We use three instances of the backbone, all pre-trained on the source domain: a student, a teacher, and a static (\ie{} frozen) teacher. During training, the student receives a mix of target and source images~\cite{dacs} and is supervised with a ``mixed-sample'' cross-entropy loss, $\mathcal{L}_T$ (represented by green, blue and red dashed lines, in Fig. \ref{fig:architecture}). This loss is computed by mixing the teacher's pseudo-labels and source annotations. To improve training stability, the teacher is updated as the exponential moving average (EMA) of the student.
To further regularize the student, we use source samples stored in a replay buffer and apply two additional losses (blue lines in Fig. \ref{fig:architecture}). First, we minimize the feature distance (Euclidean) between the student and the static teacher's encoder, $\mathcal{L}_{FD}$. Then, we employ a supervised cross-entropy task loss $\mathcal{L}_S$. Our complete objective is $\mathcal{L}=\mathcal{L}_S+\mathcal{L}_T+\lambda_{FD} \mathcal{L}_{FD}$, with $\lambda_{FD}$ being a weight factor. During inference on the target domain, only the student is used (red lines in Fig. \ref{fig:architecture}).

\subsection{Hardware-Aware Modular Training (HAMT)}
\label{subs:hardware-aware-modular-training}
Online adaptation requires updating the parameters during deployment time.
However, back-propagation is computationally expensive and hence too slow to be continuously applied on a deployed agent.
Opting for a partial weight update, for example by finetuning the last module of the network, would enable much more efficient training time.
However, domain shifts can manifest as changes in both the data input distribution (such as attributes of the images, \eg{} day/night) and the output distribution (\eg{} class priors).
This information could be encoded in different parts of the network, therefore just updating the very last segment might not suffice.
This motivates the need for orchestrating the training process, to ensure sufficient training while minimizing the computational overhead.
Inspired by reward-punishment~\cite{tonioni2019real} and reinforcement learning~\cite{wang2018haq} policies, we introduce an orchestration agent in charge of deciding how deeply the network shall be fine-tuned through a trade-off between the pseudo-loss minimization rate and the computational time.
In contrast to previous efficient back-propagation approaches~\cite{wei2017minimal, jiang2019accelerating, cheng2022stochastic}, our model is pre-trained on the task and thus requires smaller updates to adapt.
Let us start by modeling the problem. Our model backbone, $f$, is composed of four different modules: $f = m_4 \circ m_3 \circ m_2 \circ m_1$.
This defines our action space $\mathcal{A}=\{\mathrm{T_1}, \mathrm{T_2}, \mathrm{T_3}, \mathrm{T_4}\}$ where $\mathrm{T_4}$ corresponds to training just the last module of the network, $m_4$, while $\mathrm{T_3}$ the last two modules, \ie{} $m_4 \circ m_3$, $\mathrm{T_2}$ the last three, \ie{} $m_4 \circ m_3 \circ m_2$, and $\mathrm{T_1}$ the whole network $f$.
We also define a continuous state space $\mathcal{S}=\{\mathrm{R}, \mathrm{V}\}$ where $\mathrm{R}$ is the second derivative of the EMA teacher pseudo-loss, $l_t$, over time, hence $\mathrm{R_t}=-\frac{\Delta^2 l}{(\Delta t)^2}$, computed in discrete form as $R_t=-(l_t - 2l_{t-1} + l_{t-2})$. $\mathrm{V}$ represents a cumulative vector with the same dimension as the action space $\mathcal{A}$, initialized at zero. Now we have everything in place to employ an expected-improvement based decision model. 
At each time-step $t$, action $T_j$ is selected for $j = \operatorname{argmax} \mathrm{V}_t$. During training step $\mathrm{t}$, $\mathrm{V}[j]$ is updated as:
{
\begin{equation}
\label{eq:naive_update}
\mathrm{V}[j]_{t+1} = \alpha R_t  + (1 - \alpha) \mathrm{V}[j]_{t}  
\end{equation}
}
where $\alpha$ is a smoothing factor, \textit{\eg{}} $0.1$. \ie$\mathrm{V_{t}}$ hold a discrete exponential moving average of $R_t$. Therefore, our policy can be seen as a greedy module selection based on the highest expected loss improvement over its linear approximation. A notable drawback of this policy is that we will inevitably converge towards picking more rewarding, yet expensive, actions \ie{} $\mathrm{T_1}, \mathrm{T_2}$ compared to more efficient but potentially less effective actions \ie{} $\mathrm{T_3}, \mathrm{T_4}$. However, our goal is not to maximize $ -\frac{\Delta^2 l}{(\Delta t)^2}$ where $\Delta t$ is the number of updates, our goal is instead to maximize $ -\frac{\Delta^2 l}{(\Delta \tau)^2}$ where $\Delta \tau$ is a real-time interval. Therefore, we have to introduce in the optimization policy some notion of the actual training cost of each action in $\mathcal{A}$ on the target device.
To start with, we measure the training time associated with each action, obtaining $\omega_T = \{ \omega_{T_1}, \omega_{T_2}, \omega_{T_3}, \omega_{T_4} \}$. With this we can compute the time-conditioning vector $\gamma$ as
{
\small
\begin{equation}
\gamma_j=\frac{e^{\frac{1}{\beta\omega_{T_j}} }}{\sum_{k=1}^K e^{\frac{1}{\beta\omega_{T_k}}}}
\quad
\text{ for } j=1, \ldots, K
\end{equation}
}
where $\beta$ is the softmax temperature, and $K$ the number of actions, \ie{} 4 in our model.
We modify our update policy to favor less computationally expensive modules by scaling the updates with $\gamma$, replacing Eq.~\ref{eq:naive_update} with:

{
\small
\begin{equation}
\mathrm{V}[j]_{t+1} = 
 \begin{cases}
 \gamma_j \alpha R_t + (1 - \alpha) \mathrm{V}[j]_{t}  & \textrm{if } R_t \geq 0\\
 (1 - \gamma_j) \alpha R_t  + (1 - \alpha) \mathrm{V}[j]_{t} & \textrm{if } R_t < 0\\
 \end{cases}
\end{equation}
}

This policy makes it so that more expensive actions receive smaller rewards and larger punishments.
Despite its simplicity, this leads to a significant reduction in FLOPS for an average back-propagation $\beta$, \ie{} $-30\%$ with $\beta=2.75$ or $-43\%$ with $\beta=1$. We finally choose $\beta=1.75$ to obtain a FLOPS reduction of $-34\%$. Exhaustive ablations on HAMT are presented in the {supplementary material}. 

\subsection{Active Training Modulation}
\label{subs:active-training-modulation}
Continuous and test-time adaptation methods tackle online learning as a continuous and constant process carried out on the data stream. Nevertheless, this approach presents several shortcomings when it comes to real-world deployments. Performing adaptation when the deployment domain is unchanged does not lead to further performance improvements on the current domain; instead, it might cause significant forgetting on previous domains, hence hindering model generalization (we present evidence of this in the {supplementary material}). Even if mitigated by HAMT, online training remains a computationally expensive procedure, also due to several teachers' necessary forward passes.
However, knowing when and what kind of adaptation is needed is not a trivial task. We tackle this by introducing an Adaptive Domain Detection mechanism, in Sec.~\ref{subs:adaptive-domain-detection}, and then a set of strategies to reduce the training time while optimizing the learning rate accordingly, in Sec.~\ref{subs:learning-rate-scheduling}.

\subsubsection{Adaptive Domain Detection}
\label{subs:adaptive-domain-detection}
A key element of an online adaptation system consists of acquiring awareness of the trajectory in the data distribution space, \ie{} domains, traveled by the student model during deployment. We can model the problem by setting the trajectory origin in the source domain. With high dimensional data, the data distribution is not tractable, therefore the trajectory cannot be described in closed form. Recent work~\cite{Panagiotakopoulos_ECCV_2022} introduced the notion of distance between the current deployed domain and source by approximating it with the confidence drop of a source pre-trained model. This approach heavily relies on the assumption that the pre-trained model is well-calibrated. While this might hold for domains close to source, the calibration quickly degrades in farther domains~\cite{shift, Panagiotakopoulos_ECCV_2022}. This myopic behavior dampen the simple use of confidence for domain detection. Furthermore, the additional forward pass increases the computational cost during deployment.
We tackle these limitations with an equivalently simple, yet more robust, approach. We modify the backbone of the static teacher $f^\text{st}$ used for the feature distance loss $\mathcal{L}_{FD}$ by connecting a lightweight segmentation head, $d^\text{st}_{1}$, after the first encoder module $m_{1}^\text{st}$: $h^\text{st}_{1} = d^\text{st}_{1} \circ m^\text{st}_{1}$.
This additional decoder, $h^\text{st}_{1}$, is trained offline, on source data, without propagating gradients in the backbone ($m^\text{st}_{1}$ is frozen).
Given a target sample $x_T$, we propose to compute the cross-entropy between the one-hot encoded student prediction $p(x_T) = 1_{\operatorname*{argmax}(f(x_T))}$ and the lightweight decoder prediction $g(x_T) = h^\text{st}_{1}(x_T)$ as

\begin{equation}
H_T^{(i)}=-\sum_{p=1}^{H \times W} \sum_{c=1}^C p\left(x_T^{(i)}\right) \left.\log g\left(x_T^{(i)}\right)\right|_{p,c}
\end{equation}

Thanks to the student model's higher generalization capability (both due to a larger number of parameters and the unsupervised adaptation process), it will always outperform the lightweight decoder head.
Nevertheless, since now the distance is measured in the prediction space, we are not subjected to model miscalibration.
Furthermore, since the student model is in constant adaptation, the domain distance accuracy actually improves over time, leading to better results.
We present evidence of these claims in the {supplementary material}.
We now define a denoised signal by using bin-averaging $A_T^{(i)}=\sum_{j=mi}^{m(i+1)-1}\frac{H_T^{(j)}}{m}$ where $m$ is the bin size.
Domains are modeled as discrete steps of $A_T^{(i)}$

{
\small
\begin{equation}
    B_0 = A_0 \qquad
    B_i =
    \begin{cases}
        A_i & \textrm{if  $|B_{i-1} - A_i|>z$} \\
        B_{i-1} & \textrm{otherwise}
    \end{cases}
\end{equation}
}
where $B$ is the discretized signal and $z$ is the minimum distance used to identify new domains. Finally, we refer to the signed amplitude of domain shifts as $\Delta B_i = B_i - B_{i-1}$, and a domain change is detected whenever $|\Delta B_i| > z$.



\begin{table*}[t]
\scriptsize
\centering
\renewcommand{\tabcolsep}{12pt}
\scalebox{0.8}{
\begin{tabular}{c}
\begin{tabular}{ccccccr@{\tiny$\,\pm\,$}lr@{\tiny$\,\pm\,$}lr@{\tiny$\,\pm\,$}lr@{\tiny$\,\pm\,$}lr@{\tiny$\,\pm\,$}l@{~~}r@{\tiny$\,\pm\,$}l@{~~}r@{\tiny$\,\pm\,$}l@{~~}r@{\tiny$\,\pm\,$}l}
\hline
&      &    &     &     &     & \mc2c{ \cellcolor{orange!40} 200mm}           & \mc2c{\cellcolor{orange!40} All-domains}                 & \mc2c{\cellcolor{green!40}}          & \mc6c{\cellcolor{blue!25}Average GFLOPS} & \mc4c{\cellcolor{blue!25}Adaptation GFLOPS}   \\
& HAMT & LT & ALR & DCM & RCS & \mc2c{\cellcolor{orange!40} (mIoU)} & \mc2c{\cellcolor{orange!40} (mIoU)} & \mc2c{\cellcolor{green!40} FPS} & \mc2c{\cellcolor{blue!25}Total} & \mc2c{\cellcolor{blue!25}Fwd.}   & \mc2c{\cellcolor{blue!25}Bwd.}      & \mc2c{\cellcolor{blue!25}Fwd.} & \mc2c{\cellcolor{blue!25}Bwd.}  \\
\hline 
(A) & --         & --         & --         & --         & --         & 62.2 & {\tiny 0.9} & 69.5 & {\tiny 0.3} & 5.9 & {\tiny 0.0 } & 125.2 & {\tiny 0.0} & 94.4 & {\tiny 0.0} & 30.8 & {\tiny 0.0} & 56.6 & {\tiny 0.0} & 30.8 & {\tiny 0.0} \\
(B) & \checkmark & --         & --         & --         & --         & 60.2 & {\tiny 0.5} & 68.7 & {\tiny 0.3} & 7.0 & {\tiny 0.1 } & 114.7 & {\tiny 0.0} & 94.4 & {\tiny 0.0} & 20.3 & {\tiny 0.0} & 56.6 & {\tiny 0.0} & 20.3 & {\tiny 0.0} \\
(C) & \checkmark & \checkmark & --         & --         & --         & 51.8 & {\tiny 0.5} & 65.7 & {\tiny 0.2} & 29.5 &{\tiny  0.6} &  44.4 & {\tiny 0.5} & 42.6 & {\tiny 0.4} & 1.8  & {\tiny 0.2} & 56.6 & {\tiny 0.0} & 20.2 & {\tiny 0.2}  \\
(D) & \checkmark & \checkmark & \checkmark & --         & --         & 54.1 & {\tiny 1.2} & 65.9 & {\tiny 0.2} & 29.5 &{\tiny  0.5} &  44.4 & {\tiny 0.3} & 42.7 & {\tiny 0.2} &  1.8 & {\tiny 0.1} & 56.6 & {\tiny 0.0} & 20.3 & {\tiny 0.1}  \\
(E) & \checkmark & \checkmark & \checkmark & \checkmark & --         & 56.6 & {\tiny 0.8} & 66.3 & {\tiny 0.1} & 28.9 &{\tiny  0.3} &  44.7 & {\tiny 0.2} & 42.9 & {\tiny 0.2} & 1.8  & {\tiny 0.1} & 56.6 & {\tiny 0.0} & 20.2 & {\tiny 0.0}  \\
(F) & \checkmark & \checkmark & \checkmark & --         & \checkmark & 55.8 & {\tiny 1.0} & 66.3 & {\tiny 0.2} & 29.1 &{\tiny  1.1} &  45.2 & {\tiny 0.1} & 43.2 & {\tiny 0.1} & 2.0  & {\tiny 0.0} & 56.6 & {\tiny 0.0} & 20.3 & {\tiny 0.0}  \\
(G) & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & 58.2 & {\tiny 0.8} & 66.9 & {\tiny 0.3} & 29.7 &{\tiny  0.6} &  45.7 & {\tiny 0.3} & 43.6 & {\tiny 0.2} & 2.1  & {\tiny 0.1} & 56.6 & {\tiny 0.0} & 20.2 & {\tiny 0.1}  \\
\hline
\end{tabular}
\\ (a) 
\\
\begin{tabular}{lrrrrrrrrrrrr}
\toprule
{} &  clear 1 &  200mm &  clear 2 &  100mm &  clear 3 &  75mm &  clear 4 &  \cellcolor{orange!40} clear h-mean & \cellcolor{orange!40} target h-mean & \cellcolor{orange!40} total h-mean &  \cellcolor{green!40}  FPS & \cellcolor{blue!25} GFLOPS\\
\midrule
(A) &   72.9 &   52.2 &     73.6 &   64.2 &     73.0 &  67.6 &     73.4 &         73.2 &          60.6 &       67.2 &  5.6 &   125.2 \\
(B)       &   73.0 &   50.4 &     73.4 &   62.1 &     73.0 &  67.3 &     73.2 &         73.1 &          59.1 &       66.4 &  6.8 &   114.7 \\
(C)        &   73.4 &   46.0 &     73.5 &   61.5 &     73.6 &  66.1 &     73.8 &         73.6 &          56.5 &       65.1 &  7.2 &   100.0 \\
(G)       &   73.4 &   53.6 &     73.1 &   65.2 &     73.5 &  68.2 &     73.2 &         73.3 &          61.6 &       67.8 &  9.1 &    82.2 \\
\bottomrule
\end{tabular} \\ (b) \\
\end{tabular}}


\vspace{-0.3cm}
\caption{\textbf{Ablation studies -- HAMLET components.} Top: Increasing Storm (8925 frames per domain) \cite{Panagiotakopoulos_ECCV_2022}, bottom: Fast Storm C~\cite{Panagiotakopoulos_ECCV_2022} (2975 frames per domain). For each configuration, we report mIoU, framerate, and GFLOPS.}
\vspace{-0.35cm}
\label{tab:ablation}
\end{table*}




\subsubsection{Least Training and Adaptive Learning Rate}
\label{subs:learning-rate-scheduling}
The definitions of $B$ allow us to customize the training process. To this end, we adopt a \textit{Least Training} (LT) strategy and trigger adaptation only when facing a new domain, which occurs when $|\Delta B_i| > z$. Effective online learning performance depends heavily on the choice of hyperparameters such as the learning rate $\eta$ and learning rate decay rate. Therefore, we can adjust these parameters to facilitate adaptation according to the nature and intensity of domain shifts we encounter, we refer to this orchestration as Adaptive Learning Rate (ALR). For example, the larger the domain shift (\ie $|\Delta B_i|$), the more we need to adapt to counteract its effect. This can be achieved by either running more optimization steps or using a higher learning rate.
Whenever a domain shift is detected, we compute the number of adaptation iterations $L = K_l\frac{|\Delta B_i|}{z}$, hence proportionally to the amplitude of the shift $|\Delta B_i|$ relative to the threshold $z$. $K_l$ is a multiplicative factor representing the minimum adaptation iterations. If a new domain shift takes place before the adaptation process completes, we accumulate the required optimization steps.
Then, we can play on two further parameters: $K_l$ and the learning rate schedule. We argue that proper scheduling is crucial for attaining a smoother adaptation. The learning rate, $\eta$, is linearly decayed until the adaptation is concluded -- the smaller the domain shift, the faster the decay. While the initial learning rate, $K_\eta$, should be higher when the domain shift is triggered in domains farther from the source

{
\small
\begin{align}
    K_\eta & = K_{\eta,\textrm{min}}+\frac{(B_\textrm{i}-B_\textrm{source})(K_{\eta,\textrm{max}}-K_{\eta,\textrm{min}})}{B_\textrm{hard}-B_\textrm{source}}
\end{align}
}
where $B_{\text{source}}$ (resp. $B_{\text{hard}}$) is an estimate of $B$ when the network is close to (resp. far from) the source domain; and $K_{\eta,\text{min}}$ (resp. $K_{\eta,\text{max}}$) is the value of $K_\eta$ assigned when the network is close to (resp. far away from) the source. Concerning $K_l$, we posit that moving towards the source requires less adaptation than going towards harder domains: the model shows good recalling of previously explored domains and thanks to the employed regularization strategies

{
\small
\begin{equation}
    K_l =
    \begin{cases}
        K_{l,\text{max}} & \textrm{if $\Delta B_i \geq 0$} \\
        K_\textrm{l,min}+\frac{(B_\textrm{i}-B_\textrm{source})(K_\textrm{l,max}-K_\textrm{l,min})}{B_\textrm{hard}-B_\textrm{source}} & \textrm{otherwise}
    \end{cases}
\end{equation}
}
where $K_{l,\text{min}}$ (resp. $K_{l,\text{max}}$) is the value of $K_l$ assigned when the model is close to (resp. far away from) the source domain. Extensive ablations in the {supplementary material} will highlight how the orchestration of the adaptation hyper-parameters improves the accuracy-speed trade-off.

\subsubsection{Dynamic ClassMix (DCM)}

ClassMix~\cite{olsson2021classmix} provides a simple mechanism for data augmentation by mixing classes from the source dataset into target images.
Usually 50\% of the classes in the source dataset are selected, however we notice that this percentage is a highly sensitive hyperparameter in online domain adaptation.
Injecting a significant portion of source classes has a beneficial impact when adapting to domains closer to the source domain, whereas when adapting to domains further from the source the opposite effect can be observed, as it effectively slows down the adaptation process.
We therefore exploit once more the deployment domain awareness to control the mixing augmentation:


{
\footnotesize
\begin{equation}
    K_\textrm{CM} = K_{\text{CM},\text{min}} + \frac{(B_\textrm{i}-B_\textrm{source})(K_\textrm{CM,max}-K_\textrm{CM,min})}{B_\textrm{hard}-B_\textrm{source}}.
\end{equation}
}
where $K_\textrm{CM}$ is the percentage of source classes used during adaptation; and $K_\textrm{CM, min}$ (resp. $K_\textrm{CM, max}$) is the value of $K_\textrm{CM}$ assigned when the network is close to (resp. far away from) the source domain.

\subsubsection{Buffer Sampling}
Following~\cite{Panagiotakopoulos_ECCV_2022}, to simulate real deployment, we limit our access to the source domain by using a replay buffer. Additionally, instead of initializing at random (with a uniform prior), we apply Rare Class Sampling (RCS) (skewed priors) as in~\cite{hoyer2021daformer}. This incentives a more balanced class distribution over the buffer, ultimately leading to better accuracy.

\section{Experimental Results}
The experiments are carried out on (a) the OnDA benchmarks~\cite{Panagiotakopoulos_ECCV_2022} and (b) the SHIFT dataset~\cite{shift}. (a) is a semi-syntehtic benchmark, as it applies synthetic rain and fog~\cite{tremblay2020rain} over 4 different intensities profiles. The main benchmark, Increasing Storm, presents a storm with a pyramidal intensity profile; see Fig.~\ref{fig:increasing_storm}. In contrast, (b) is a purely synthetic dataset, where both the underlying image and the weather are synthetically generated and thus domain change is fully controllable. 
All models are evaluated using mIoU: following~\cite{Panagiotakopoulos_ECCV_2022}, we report the harmonic mean over domains to present the overall adaptation performance. All experiments were carried out using an Nvidia\texttrademark~RTX~3090 GPU. We refer to {supplementary material} for further details.
\newcommand*{\cdash}{\multicolumn1c{--}}
\begin{table*}[t]
    \centering
    \scalebox{0.65}{
    \begin{tabular}{clrr rr rr rr rr r rrrrr}
    \toprule
    {} & & \mc{2}{c}{clear} & \mc{2}{c}{25mm} & \mc{2}{c}{50mm} & \mc{2}{c}{75mm} & \mc{2}{c}{100mm} &  200mm &  \mc{3}{c}{\cellcolor{orange!40}  h-mean} &      \cellcolor{green!40}  {FPS}   & \cellcolor{blue!25}  {GFLOPS}\\
    & {} &         F &          B &      F &          B &          F &          B &      F &          B &          F &          B &      F &         \cellcolor{orange!40}  F & \cellcolor{orange!40} B & \cellcolor{orange!40}  T & \cellcolor{green!40}  & \cellcolor{blue!25} \\
    \midrule
    (A) & DeepLabV2 (no adaptation) & 64.5 & \cdash & 57.1 & \cdash & 48.7 & \cdash & 41.5 & \cdash & 34.4 & \cdash & 18.5 & 37.3 & \cdash & \cdash & 39.4 &  \cdash\\
    \midrule
    (B) & DeepLabV2 fully supervised (oracle) &  {64.5} &\cdash  & {64.1} &\cdash  & {63.7} &\cdash &  {63.0} &\cdash &  {62.4} &\cdash &   {58.2} & {62.6} &\cdash&\cdash& 39.4 &\cdash\\
    \midrule
    (C) & OnDA & 64.5 & 64.8 & 60.4 & 57.1 & 57.3 & 54.5 & 54.8 & 52.2 & 52.0 & 49.1 & 42.2 & 54.2 & 55.1 &\cdash& 1.3 & \cdash\\
    
    \midrule
    \midrule
    (D) & SegFormer MiT-B1 (no adaptation) &  73.4 & \cdash&  68.8 & \cdash&  64.2 & \cdash&  58.0 & \cdash&  51.8 & \cdash&  31.2 &      57.8 &  \cdash& \cdash&   48.4 & 34.9 \\ 
    (E) & SegFormer MiT-B5 (no adaptation) &  77.6 & \cdash&  73.9 & \cdash&  71.0 & \cdash&  67.2 & \cdash&  62.6 & \cdash&  46.7 & 64.7 &  \cdash& \cdash&   11.5 & 240.4  \\ 
        \midrule
    (F) & SegFormer MiT-B1 fully supervised (oracle)  &  72.9 & \cdash&  72.4 & \cdash&  72.1 & \cdash&  71.5 & \cdash&  70.7 & \cdash&  68.6 & 71.3 &  \cdash& \cdash&   48.4 & 34.9 \\
    \hline 
    (G) & TENT            &  73.0 &  72.8 &  68.5 &  68.6 &  64.5 &  64.8 &  59.7 &  60.2 &  54.5 &  54.8 &  35.9 &   56.2 &  63.6 &  59.9 &  10.0 &\cdash\\
    (H) & TENT + Replay Buffer &  73.0 &  72.8 &  68.5 &  68.6 &  64.5 &  64.8 &  59.7 &  60.2 &  54.4 &  54.7 &  35.8 &   56.1 &  63.6 &      59.9 &   7.8 &\cdash\\
    (I) & CoTTA           &  72.5 & 74.4 & 69.5 & \bfseries 70.9 & 65.9 & \bfseries 68.2 & 66.1 & 64.7 & 64.6 & 63.5 & 57.2 & 65.6 & \bfseries 68.1 & 66.8 & 0.6 & 593.8 \\
    (J) & CoTTA \textit{real-time} & 73.3 & \bfseries 75.4 & \bfseries 70.3 & 70.6 & 66.9 & 66.4 & 62.5 & 61.4 & 57.6 & 56.9 & 39.7 & 59.2 & 65.5 & 62.3 & 27.0 & \bfseries 41.7 \\
    (K) & HAMLET (ours)            &  \bfseries 73.4 &  71.0 & 70.1 &  68.8 &  \bfseries 67.7 &  67.5 &  \bfseries 66.6 &  \bfseries 66.4 &  \bfseries 65.5 &  \bfseries 64.6 &  \bfseries 59.2 &  \bfseries 66.8 &  67.6 &  \bfseries 67.2 &  \bfseries 29.1 & 45.7  \\
    \bottomrule
    \end{tabular}
    }
    \vspace{-0.3cm}
    \caption{\textbf{Comparison against other models -- Increasing storm scenario.} (A-C) methods built over DeepLabv2, (D-E) SegFormer variants trained on source, (F) oracle, (G-K) models adapted online. We report mIoU, framerate, and GFLOPS.}
    %\vspace{-0.5cm}
    \label{tab:comparison}
\end{table*}


\subsection{Ablation Studies}

In Tab.~\ref{tab:ablation} we study the impact of each contribution to adaptation performance, both in terms of accuracy and efficiency. 
For each configuration, we report mIoU over different portions of the sequence, the framerate and the amount of GFLOPS -- respectively averages of: total, forward and backward passes, and dedicated adaptation only, also divided in forward (Fwd) and backward (Bwd). 
Tab.~\ref{tab:ablation} (a) shows results on the Increasing Storm scenario~\cite{Panagiotakopoulos_ECCV_2022}. Here, we show mIoU over the 200mm domain, \ie{} the hardest in the sequence, as well as the mIoU averaged over forward and backward adaptation, \ie, from \textit{clear} to 200mm rain and backward.
Results are averaged over 3 runs with different seeds, with standard deviation being reported. (A) reports the results achieved by na\"ively performing full adaptation of the model. HAMT can increase the framerate by roughly 15\% by reducing the Bwd GFLOPS of 34\%, at the expense of as few as 0.7 mIoU on average, \ie, about 2 points on the 200mm domain. 
The main boost in terms of speed is obviously given by LT (C), which inhibits the training in absence of detected domain shifts. LT increases the framerate by approximately $4\times$ by decimating the total GFLOPS, yet not affecting the adaptation Bwd GFLOPS. This comes with a price in terms of mIoU, dropping by about 4 points on average and more than 10 points on 200mm -- not a moderate drop anymore. LT impact highly depends on the domain sequence experienced during deployment: frequent domain changes could prevent training inhibition, thus neglecting LT gains in terms of efficiency, as we will appreciate later.
The loss in accuracy is progressively regained by adding ALR (D), with further improvements yielded by one between DCM (E) and RCS (F), or both together (G) leading to the full HAMLET configuration. The three together allow for reducing the gap to 2.5 points mIoU -- 4 over the 200mm domain -- without sacrificing any efficiency. 
Tab.~\ref{tab:ablation} (b) shows further results, on a faster version of Storm C~\cite{Panagiotakopoulos_ECCV_2022}. This represents a much more challenging scenario, with harsher and $3\times$ more frequent domain shifts. Here we show the single domains mIoU, as well as harmonic mean on source and target domains, and all frames. As expected, in this benchmark, LT alone (C) results much less effective than before, with a much lower gain in FPS and GFLOPS. Here, the synergy between the HAMT, LT, and the other components (G) allows for the best accuracy and speedup -- even outperforming the full training variant (A) -- highlighting their complementarity.
Further ablations are in the {supplementary material}.
% Figure environment removed

\begin{table*}[t]
\scriptsize
    \centering
    \scalebox{0.78}{
    \renewcommand{\tabcolsep}{10pt}
    \begin{tabular}{lrrrrrrrrrrrrrrr}
    \toprule
    {} & \mc{2}{c}{clear} & \mc{2}{c}{750m} & \mc{2}{c}{375m} & \mc{2}{c}{150m} &  75m & \mc{3}{c}{\cellcolor{orange!40}h-mean} &      \cellcolor{green!40} FPS &  \cellcolor{blue!25}GFLOPS  \\
    {} &          F &          B &      F &          B &          F &          B &      F &          B &          F &          \cellcolor{orange!40}F & \cellcolor{orange!40}B & \cellcolor{orange!40}T & \cellcolor{green!40} & \cellcolor{blue!25}\\
    \midrule  
    OnDA & 64.9 & 65.8 & 63.3 & 62.3 & 60.7 & 58.8 & 51.6 & 49.1 & 42.1 & 55.1 & 54.1 & -- & 1.3 & -- \\
    \hline
    SegFormer MiT-B1 (no adaptation)   &  71.1 &  -- &  70.0 &  -- &  67.5 &  -- &  58.8 &  -- &  46.9 &  61.3 &  -- & -- &   48.4 & 34.9 \\
    Full training &  71.5 &  72.1 &  72.9 &  74.7 &  71.9 &  73.1 &  67.6 &  68.1 &  61.3 &   68.7 &  71.9 &      70.3 &   5.6 & 125.2 \\
    HAMLET (ours) &  71.1 &  71.6 &  70.3 &  70.8 &  68.8 &  69.2 &  64.3 &  64.3 &  57.0 &   65.9 &  68.9 &      67.4 &  24.8 & 50.7 \\
    \bottomrule
    \end{tabular}
    }
    \vspace{-0.3cm}
    \caption{\textbf{Results on foggy domains.} Comparison between OnDA, Source SegFormer, full training adaptation, and HAMLET.}
    \vspace{-0.3cm}
    \label{tab:fog}
\end{table*}
\begin{table*}[t]
\scriptsize
    \centering
    \scalebox{0.78}{
    \renewcommand{\tabcolsep}{8pt}
    \begin{tabular}{lrrrrrrrrrrrrrrrr}
    \toprule
    {} & \mc{2}{c}{Clear} & \mc{2}{c}{Cloudy} & \mc{2}{c}{Overcast} & \mc{2}{c}{Small rain} & \mc{2}{c}{Mid rain} &  Heavy rain & \mc{3}{c}{\cellcolor{orange!40}h-mean} &      \cellcolor{green!40} FPS   & \cellcolor{blue!25}GFLOPS\\
    {} &          F &          B &      F &          B &          F &          B &      F &          B &          F &          B &      F &          \cellcolor{orange!40}F & \cellcolor{orange!40}B & \cellcolor{orange!40}T & \cellcolor{green!40} &\cellcolor{blue!25}\\
    \midrule
    SegFormer MiT-B1 fully supervised (oracle)  &  80.1 &  -- &   79.9 &  -- &     79.8 &  -- &       78.9 &  -- &     78.7 &  -- &       77.1 &   79.1 &  -- &    -- &   48.4 &    34.93 \\
    \hline 
    SegFormer MiT-B1 (no adaptation)  &  79.6 &  -- &   77.1 &  -- &     75.4 &  -- &       73.4 &  -- &     71.4 &  -- &       66.7 &   73.7 &  -- &    -- &   48.4 &    34.93 \\
    Full training &  78.9 &  79.3 &   76.7 &  76.8 &     76.8 &  77.9 &       74.8 &  74.8 &     76.3 &  76.5 &       74.0 &   76.2 &  77.0 &      76.6 &   5.0 &  125.1 \\
    HAMLET (ours)  &  79.6 &  78.9 &   76.9 &  76.6 &     76.1 &  77.4 &       73.3 &  74.3 &     74.2 &  76.0 &       74.2 &   75.7 &  76.6 &      76.1 &  26.8 &   43.9 \\
    \bottomrule
    \end{tabular}}
    \vspace{-0.3cm}
    \caption{\textbf{Results on SHIFT dataset~\cite{shift}.} Comparison between Source SegFormer, full training adaptation, and HAMLET.}
    %\vspace{-0.35cm}
    \label{tab:shift}
\end{table*}

\subsection{Results on Increasing Storm}
Tab.~\ref{tab:comparison} shows a direct comparison between HAMLET and relevant approaches. The presented test-time adaptation strategies namely -- TENT and CoTTA -- were revised to handle the online setting and be fairly compared with HAMLET. All methods start with the same exact initial weights -- with HAMLET requiring the additional lightweight decoder, not needed by TENT and CoTTA -- using SegFormer MiT-B1 as the backbone, since it is $4\times$ faster than SegFormer MiT-B5 and thus better suited to keep real-time performance even during adaptation.
We report results achieved by DeepLabv2 trained on source data only (A), an \textit{oracle} model trained with full supervision (B), as well as OnDA~\cite{Panagiotakopoulos_ECCV_2022} (C) as a reference.
Then, we report SegFormer models trained on the source domain only (D) and (E). In (F) we show the performance achieved by an oracle SegFormer, trained on all domains fully supervised.
Following~\cite{Panagiotakopoulos_ECCV_2022}, columns ``F'' concern forward adaptation from \textit{clear} to 200mm, while columns ``B'' show backward adaptation from 200mm to \textit{clear}, while the h-mean T refers to the overall harmonic mean. 
We can notice how SegFomer results are much more robust to domain changes with respect to DeepLabv2. Indeed, SegFormer MiT-B5 (E), without any adaptation, results more accurate than DeepLabv2 oracle (B), as well as better and faster than OnDA (C). The faster variant (D) outperforms OnDA both in speed and accuracy, reaching 48 FPS.
Nevertheless, domain changes still dampen the full potential of SegFormer. Indeed, the oracle (F) outperforms (D) by about +14 mIoU. However, this is not meaningful for real deployment experiencing unpredictable domain shifts, as it assumes to have data available in advance.
Concerning test-time models, TENT starts adapting properly only beyond 50mm, both with (G) and without (H) frame buffer, while it loses some accuracy on 25mm. This makes its overall forward adaptation performance slightly worse compared to the pre-trained model (D), while being better at backward adaptation. Despite outperforming SegFormer MiT-B1, TENT is both slower and less accurate than SegFormer MiT-B5 running without any adaptation, further suggesting the robustness of the latter and making TENT not suitable for real-world deployment.
On the contrary, CoTTA (I) outperforms both SegFormer models trained on source only, at the expense of dropping the framerate below 1FPS.
It is worth mentioning that these metrics were collected after each domain was completed by each model individually. In an evaluation setup imposing a shared time frame, slower models would present much lower metrics, since their adaptation process would result constantly lagged. In fact, forcing CoTTA to run in real-time, at nearly 30FPS -- \ie{} by training once every 50 frames -- dramatically reduces the effectiveness of the adaptation process (J), with drastic drops in the hardest domains.
Finally, HAMLET (K) succeeds on any fronts, improving the baseline (D) by about 10 points with only a cost of 25\% in terms of speed, while outperforming SegFormer MiT-B5 (E) both on accuracy (+2.5 mIoU) and speed ($3\times$ faster) -- being the only method achieving this, and thus the only suitable choice for real-time applications.
Fig.~\ref{fig:increasing_storm} shows the overall behavior of HAMLET while adapting over the Increasing Storm. In addition to the rain intensity and the mIoU achieved on each domain -- active (bold) or inactive (dashed), \ie{} respectively the mIoU on the domain being currently faced during deployment, and how the current adaptation affects the performance on the other domains to highlight the robustness to forgetting -- we also report how the learning rate is modulated in correspondence of detected domain shifts, with a consequent drop in FPS due to the short training process taking place. For further experiments on harsher and sudden adaptation cycles, we include results of Storms A, B, C~\cite{Panagiotakopoulos_ECCV_2022} in the {supplementary material}.


% Figure environment removed

% Figure environment removed


\subsection{Additional Results: Fog and SHIFT}

\textbf{Fog.} In Tab.~\ref{tab:fog}, we investigate adaptation on the Increasing Fog scenario in the OnDA benchmark~\cite{Panagiotakopoulos_ECCV_2022}. Crucially, for this experiment, we keep the same hyperparameters used for the Increasing Storm, since in both cases the starting SegFormer model is trained on the same source domain. This allows for validating how the proposed setting generalizes at dealing with different kind of domain shifts, beyond those considered in the main experiments. We effectively use Increasing Fog as test set, and compare against SegFormer trained on source (no adaptation) and a model that has been adapted by means of full online training optimization (configuration (A) of Table \ref{tab:ablation}). HAMLET is able to adapt almost as well as the full online training model, with less than a 3 mIoU gap, while enjoying real-time adaptation at nearly $5\times$ the speed using just 40\% of the FLOPS.

\textbf{SHIFT.} We further test HAMLET on the SHIFT dataset~\cite{shift}. Tab.~\ref{tab:shift} collects the results achieved by SegFormer trained on source, full online training and HAMLET respectively, both at forward and backward adaptation across \textit{Clear}, \textit{Cloudy}, \textit{Overcast}, \textit{Small rain}, \textit{Mid rain} and \textit{Heavy rain} domains. Here HAMLET results highly competitive with the full training regime, with only 0.5 drop in average mIoU, while being more than $5\times$ faster.
Fig. \ref{fig:shift} depicts, from top to bottom, the rain intensity characterizing any domain encountered on SHIFT, the mIoU achieved both on current (bold) and inactive (dashed) domains, the learning rate changes based on the domain shift detection, and the framerate achieved at any step. We refer to the supplementary material for a deeper analysis.

\textbf{Qualitative results.} To conclude, Fig. \ref{fig:qual} shows some qualitative examples from CityScapes. We can notice how SegFormer accuracy (second tow) drops with severe rain, whereas HAMLET (third row) is capable of keeping the same segmentation quality across the storm.

\section{Discussion}
\textbf{Orthogonality.} HAMT and LT act independently. Indeed, by strongly constraining the adaptation periods through LT, HAMT has a limited margin of action. The impact of HAMT also depends on the backbone and by carefully crafting modular architectures, one can achieve further optimization. Nevertheless, in a deployment environment where domain shifts occur at high frequencies (\textit{\eg{}}, Storm C), LT is ineffective, while HAMT thrives.

\textbf{Measuring forgetting.} An interesting topic we have not investigated consists of introducing an explicit awareness of which domains have been explored and how well we can recall them, expanding the distance $B$ to multiple dimensions.

\textbf{Safety.} We believe dynamic adaptation has the potential to enhance safety, but we acknowledge the necessity for rigorous testing and verification to safeguard against drift or catastrophic forgetting. This mandates a comprehensive effort from academia, industry, and certification authorities for ensuring the integrity of dynamically adapting models. 

\section{Summary \& Conclusion}
We have presented HAMLET, a framework for real-time adaptation for semantic segmentation that achieves state-of-the-art performance on established benchmarks with continuous domain changes. Our approach combines a hardware-aware backpropagation orchestrator and a specialized domain-shift detector to enable active control over the model's adaptation, resulting in high framerates on a consumer-grade GPU. These advancements enable HAMLET to be a promising solution for in-the-wild deployment, making it a valuable tool for applications that require robust performance in the face of unforeseen domain changes.

\vspace{10pt}
\textbf{Acknowledgement.} The authors thank Gianluca Villani for the insightful discussion on reward-punishment policies, Leonardo Ravaglia for his expertise on hardware-aware training, and Lorenzo Andraghetti for exceptional technical support throughout the project. Their assistance was invaluable in the completion of this work.


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib, morebibs}
}

\newpage\phantom{Supplementary}
\multido{\i=1+1}{12}{
\includepdf[pages={\i}]{hamlet_supplementary.pdf}
}

\end{document}