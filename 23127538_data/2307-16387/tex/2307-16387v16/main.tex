
\documentclass[10pt]{article} % For LaTeX2e

\newif\ifpreprint

% \usepackage{tmlr}
% \preprintfalse

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}

% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:

\usepackage[preprint]{tmlr}
\preprinttrue

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{hyperref}
\usepackage{url}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{adjustbox}
%\usepackage{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{centernot}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{color}
%\usepackage{booktabs}

\usepackage{subfigure}
\usepackage{caption}
\usepackage{dsfont}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{empheq}
%\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{totcount}

\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{makecell}
%\usepackage[table]{xcolor}
\usepackage{xcolor}
\usepackage{changepage}
\usepackage{stfloats}
%\usepackage{cite}
\usepackage{soul}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{algpseudocode}
\usepackage[title]{appendix}
\usepackage{pdfpages}

\usepackage{bm}
\usepackage{amsmath,amssymb,amsfonts}
% \usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{bookmark}
\usepackage{float}
\usepackage{tabularx}
\usepackage{mathtools}
\usepackage{array}
\usepackage{rotating}
\usepackage[skins]{tcolorbox}
\usepackage{tikz}
\usepackage{pifont}

\newcommand{\indep}{\perp \!\!\! \perp}
\newcommand{\nindep}{\not\!\perp\!\!\!\perp}

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{ \node[shape=circle,draw,inner sep=1.5pt] (char) {#1};}}

\title{Relation-First Modeling Paradigm for Causal Representation Learning toward the Development of AGI}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.


\author{
\name Jia Li \email jiaxx213@umn.edu \\
      \addr Department of Computer Science,
      University of Minnesota
      \vspace{-3mm}
      \AND
      \name Xiang Li \email lixx5000@umn.edu \\
      \addr Department of Bioproducts and Biosystems Engineering,       University of Minnesota
      % \name Xiaowei Jia \email xiaowei@pitt.edu\\
      % \addr Department of Computer Science\\ University of Pittsburgh
      }

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{Apr}  % Insert correct month for camera-ready version
\def\year{2023} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version

\definecolor{thegrey}{RGB}{192,192,192}
\definecolor{orchid}{RGB}{204,153,255}
\definecolor{myblue}{RGB}{0,115,207}
\definecolor{mygreen}{RGB}{68,134,25}
%\definecolor{myorange}{RGB}{0,128,0}
\definecolor{mypurple}{RGB}{177,35,200}

\begin{document}


\maketitle
%\vspace{-3mm}

\ifpreprint
\vspace{-4.5mm}
\fi

\begin{abstract}
\vspace{-3mm}

\ifpreprint
\vspace{-1mm}
\fi

The traditional i.i.d.-based learning paradigm faces inherent challenges in addressing causal relationships, which has become increasingly evident with the rise of applications in causal representation learning. 
Our understanding of causality naturally requires a perspective as the creator rather than observer, as the ``what...if'' questions only hold within the possible world we conceive. The traditional perspective limits capturing dynamic causal outcomes and leads to compensatory efforts such as the reliance on hidden confounders.
This paper lays the groundwork for the new perspective, which enables the \emph{relation-first} modeling paradigm for causality. Also, it introduces the Relation-Indexed Representation Learning (RIRL) as a practical implementation, supported by experiments that validate its efficacy.
% The potential surge of causal reasoning in AI models toward AGI is imminent, given the impending saturation of observation-based applications in fields like image and language processing. It is both critical and underrecognized that the essence of causality lies in the \emph{temporal nonlinearity} (i.e., dynamics) of causal effects. Capturing such featured causal representations is key to realizing AGI. This paper advocates for a thorough reevaluation and potential overhaul of existing causal inference theories and the traditional learning paradigm, which predominantly relies on the observational i.i.d assumption.
% Our aim is to align these theories and methodologies with the intrinsic demands of AGI development.
% We introduce a novel \emph{Relation-Oriented} paradigm for relationship modeling, and the \emph{Relation-Indexed Representation Learning} (RIRL) method as its foundational implementation. Extensive experiments confirm RIRL's efficacy in autonomously capturing dynamical effects.



\vspace{-2mm}
\end{abstract}

\ifpreprint
\vspace{-3mm}
\fi

\section{Introduction}

The concept of Artificial General Intelligence (AGI) has prompted extensive discussions over the years yet remains hypothetical, without a practical definition in the context of computer engineering. The pivotal question lies in whether human-like ``understanding'', especially causal reasoning, can be implemented using formalized languages in computer systems \cite{newell2007computer, pavlick2023symbols, marcus2020next}. From an epistemological standpoint, abstract entities (i.e., perceptions, beliefs, desires, etc.) are prevalent and integral to human intelligence. However, in the symbol-grounded modeling processes, variables are typically assigned as observables, representing tangible objects to ensure their values have clear meaning.

Epistemological thinking is often anchored in objective entities, seeking an irreducible ``independent reality'' \cite{eberhardt2022causal}. 
This approach necessitates a metaphysical commitment to constructing knowledge by assuming the unproven prior existence of the ``essence of things'', fundamentally driven by our desire for certainty. Unlike physical science, which is concerned with deciphering natural laws, technology focuses on devising effective methods for problem-solving, aiming for the optimal functional value between the nature of things and human needs. This paper advocates for a shift in perspective when considering technological or engineering issues related to AI or AGI, moving from traditional epistemologies to that of the creator. That is, our fundamental thinking should move from ``truth and reality'' to ``creation and possibility''.

In some respects, both classical statistics and modern machine learnings traditionally rely on epistemology and follow an ``object-first'' modeling paradigm, as illustrated by the practice of assigning pre-specified, unchanging values to variables regardless of the model chosen. In short, individual \emph{objects} (i.e., variables and outcomes) are defined a priori before considering the \emph{relations} (i.e., model functions) between them by assuming that what we observe precisely represents the ``objective truth'' as we understand it.  
This approach, however, poses a fundamental dilemma when dealing with causal relationship models.

Specifically, ``causality'' suggests a range of possible worlds, encompassing all potential futures, whereas ``observations'' identify the single possibility that has actualized into history with 100\% certainty.
Hence, addressing causal questions requires us to adopt the perspective of the ``creator'' (rather than the ``observer''), to expand the objects of our consciousness from given entities (i.e., the observational world) to include possible worlds, where values are assigned ``as supposed to be'', that is, \emph{as dictated by the relationship}.


Admittedly, causal inference and related machine learning methods have made significant contributions to knowledge developments in various fields \cite{wood2015lesson, vukovic2022causal, ombadi2020evaluation}. However, the inherent misalignment between the ``object-first'' modeling principle and our instinctive ``relation-first'' causal understanding has been increasingly accentuated by the application of AI techniques, i.e., the neural network-based methods.
Particularly, integrating causal DAGs (Directed Acyclic Graphs), which represent established knowledge, into network architectures \cite{marwala2015causality, lachapelle2019gradient} is a logical approach to efficiently modeling causations with complex structures. However, surprisingly, this integration has not yet achieved general success \cite{luo2020causal, ma2018using}.

As Scholkopf \cite{scholkopf2021toward} points out, it is commonly presumed that ``the causal variables are given''. In response, they introduce the concept of ``causal representation'' to actively construct variable values as causally dictated, replacing the passively assumed observational values.
However, the practical framework for modeling causality, especially in contrast to mere correlations, remains underexplored. Moreover, this shift in perspective suggests that we are not just dealing with ``a new method'' but rather a new learning paradigm, necessitating in-depth philosophical discussions. Also, the potential transformative implications of this ``relation-first'' paradigm for AI development warrant careful consideration.

This paper will thoroughly explore the ``relation-first'' paradigm in Section 2, and introduce a complete framework for causality modeling by adopting the ``creator's'' perspective in Section 3. In Section 4, we will propose the \emph{Relation-Indexed Representation Learning} (RIRL) method as the initial implementation of this new paradigm, along with extensive experiments to validate RIRL's effectiveness in Section 5. 

\section{Relation-First Paradigm}
\vspace{-2mm}

The ``do-calculus'' format in causal inference \cite{pearl2012calculus, huang2012pearl} is widely used to differentiate the effects from ``observational'' data $X$, and ``interventional'' data $do(X)$ \cite{hoel2013quantifying, eberhardt2022causal}. Specifically, $do(X=x)$ represents an intervention (or action) where the variable $X$ is set to a specific value $x$, distinct from merely observing $X$ taking the value $x$.
However, given the causation represented by $X\rightarrow Y$, why doesn't $do(Y=y)$ appear as the action of another variable $Y$?

Particularly, distinct from the independent state $X$, the notation $do(X)$ incorporates its timing dimension to encompass the process of ``becoming $X$'' as a dynamic.
Such incorporation can be applied to any variable, including $do(Y)$, as we can naturally understand a relationship $do(X)\rightarrow do(Y)$.
For example, consider the statement ``storm lasting for a week causes downstream villages to be drowned by the flood,'' if $do(X)$ is the storm lasting a week, $do(Y)$ could represent the ensuing water-level enhancement, leading to the disaster.

The challenge of accounting for $do(Y)$ arises from the empirical modeling process. In the observational world, $do(X)$ is associated with clearly observed timestamps, like $do(X_t)$, allowing us to focus on modeling its observational states $X_t$ by treating timing $t$ as a solid reference frame. However, when we conceptualize a ``possible world'' to envision $do(Y)$, its potential variations can span across the timing dimension. For instance, a disaster might occur earlier or later, with varying degrees of severity, based on different possible conditions. This variability necessitates treating timing as a computational dimension.

%From the perspective of a ``creator,'' such a $do(Y)$ can naturally occur in a ``possible world'' that we freely create in our consciousness, before it becomes the truth. However, from the ``observer's'' viewpoint, only $do(X)$, which actually exists in our observations, can be described.

%The difficulty of involving $do(Y)$ lies in empirical challenges - we can envision countless possible worlds with varying timing or states. For example, the disaster represented by $do(Y)$ could occur earlier or later with varying degrees of severity, depending on different conditions. This contrasts with the observational world, where only a single ``actual timing'' exists,  

% only a single absolute timing exists, allowing us to safely exclude ``time'' as a dimension in our computations (e.g., concerning whether it occurs earlier or later) and focus solely on the observational values of $X$ and $Y$. But this does not mean that the value distributions along time, i.e., the \emph{dynamics} of $X$ and $Y$, are insignificant for learning. 

However, this does not imply that the timing-dimensional distribution is insignificant for the outcome $Y$. The necessity of incorporating $do(X)$ in modeling highlights the importance of including dynamic features. Specifically, Recurrent Neural Networks (RNNs) are capable of autonomously extracting significant dynamics from sequential observations $x$ to facilitate $do(X)\rightarrow Y$, eliminating the requirement for manual identification of $do(X)$. In contrast, statistical causal inference often demands such identifications \cite{pearl2012calculus}, such as specifying the duration of a disastrous storm on various watersheds under differing hydrological conditions.


In RNNs, $do(X)$ is optimized in latent space as representations related to the outcome $Y$. Initially, they feature the observed sequence $X^t = (X_1, \ldots, X_t)$ with determined timestamps $t$, but as representations rather than observables, they enable the computational flexibility over timing, to assess the significance of the $t$ values or mere the orders.
The capability of RNNs to effectively achieve significant $do(X)$ has led to their growing popularity in relationship modeling \cite{xu2020multivariate}. However, can the same approach be used to autonomously extract $do(Y)$ over a possible timing?

Since the technique has emerged, facilitating $do(Y)$ is no longer considered a significant technical challenge. It is unstrange that inverse learning has become a popular approach \cite{arora2021survey} to compute $do(Y)$ as merely another observed $do(X)$. However, the concept of a ``possible world'' suggests dynamically interacted elements, implying a conceptual space for ``possible timings'' rather than a singular dimension. This requires a shift in perspective from being an ``observer'' to becoming the ``creator''. This section will explore the philosophical foundations and mathematically define the proposed \emph{relation-first} modeling paradigm.
%dopting this ``creator's'' viewpoint, we will explore the proposed relation-first causal modeling paradigm in this section. Accordingly, we will introduce a symbolic definition for the concept of ``causal relationship'' that is directly applicable in engineering contexts.

\subsection{Philosophical Foundation}

Causal Emergence \cite{hoel2013quantifying, hoel2017map} marks a significant philosophical advancement in causal relationship understanding. It posits that while causality is often observed at the micro-level, a macro-level perspective can reveal additional information, denoted as Effect Information (EI), such as $EI(X\rightarrow Y)$. For instance, consider $Y_1$ and $Y_2$ as two complementary components of $Y$, i.e., $Y=Y_1+Y_2$. In this case, the macro-causality $X\rightarrow Y$ can be decomposed into two micro-causal components $X\rightarrow Y_1$ and $X\rightarrow Y_2$. However, $EI(X\rightarrow Y)$ cannot be fully reconstructed by merely combining $EI(X\rightarrow Y_1)$ and $EI(X\rightarrow Y_2)$, since their informative interaction $\phi$ cannot be included by micro-causal view, as illustrated in Figure~\ref{fig:macro}(b).

%observed in micro-causal components only (e.g., in the single relationship $X\rightarrow Y$), the macro-causal system (e.g., a multi-variable causal system comprising $\{X, Y, Z\}$) can exhibit more informative interactions than just the sum of its components (e.g., the system's behavior cannot be fully explained by simply combining $X\rightarrow Y$, $X\rightarrow Z$, and $Y\rightarrow Z$). 
%The macro- and micro-causality originates from economics, while in modeling contexts, their scales can be flexibly determined based on practical questions.

% Figure environment removed

%In this context, the concept of Effective Information (EI), represented as $EI(X\rightarrow Y)$, 
Specifically, the concept of EI is designed to quantify the information generated by the system during the transition from the state of $X$ to the state of $Y$ \cite{tononi2003measuring, hoel2013quantifying}. Furthermore, $\phi$ denotes the minimum EI that can be transferred between $Y_1$ and $Y_2$ \cite{tononi2003measuring}. For clearer interpretation, Figure~\ref{fig:macro}(a) illustrates the uninformative statistical dependence between states $Y_1$ and $Y_2$, represented by the dashed line with $EI(\phi)=0$.
%in Figure~\ref{fig:macro}(a), the causal emergence highlights that ``their informative interaction with $EI(\phi)>0$ may not be fully encapsulated by $EI(X\rightarrow Y)$'' as in Figure~\ref{fig:macro}(b). 
%Similarly, we use $\theta$ to represent the EI of $X\rightarrow Y$ for clarity, such that $E\mathcal{I}(X\rightarrow Y) = E\mathcal{I}(\theta)$.

%The capacity to represent the information between micro components, signified by $\phi$, was initially established in the Integrated Information Theory of Consciousness \cite{tononi2003measuring}.
%We apply its original definition while streamlining the notation to $E\mathcal{I}(X\rightarrow Y) = E\mathcal{I}(\theta)$, where the abstract variable $\theta$ represents the desired effective information. 

However, this phenomenon can be explained by the information loss when reducing a \emph{dynamic} outcome $do(Y)$ to be a \emph{state} $Y$. Let's simply consider the reduction from $do(X)\rightarrow do(Y)$ to $X\rightarrow Y$, likened with: attributing the precipitation on a specific date (i.e., the $X_t$ value) solely as the cause for the disastrous high water-level flooding the village on the $7$th days (i.e., the $Y_{t+7}$ value), regardless of what happened on the other days.
From a computational standpoint, given observables $X\in \mathbb{R}^{n}$ and $Y\in \mathbb{R}^{m}$, this reduction implies the information within $\mathbb{R}^{n+1}\cup\mathbb{R}^{m+1}$ must be compactively represented between $\mathbb{R}^{n}$ and $\mathbb{R}^{m}$.
%do(X)$ can be viewed as a $(n+1)$-dimensional variable, by integrating the $t$-dimension with the observational variable $X\in \mathbb{R}^{n}$, and similarly for $do(Y)\in \mathbb{R}^{m+1}$. So,  
%From an empirical standpoint, this 


%For an observed $X$, we can still manually identify $do(X)$ from data (e.g., specifying the duration days for each qualified storm) while treating $Y$ as purely observational to denote its ``possible state'' at a specific time (e.g., $7$ days later) to achieve $do(X)\rightarrow Y$.

If simplifying the possible timing as the extention of observed timing $t$, identifying a significant $Y_{t+1}$ can still be feasible. However, since $Y_1\rightarrow Y_2$ implies an interaction in a ``possible world'', identifying representative value for outcome $Y$ may prove impractical. Suppose $Y_1$ represents the impact of flood-prevention operations, and $Y_2$ signifies the daily water-level ``without'' these operations. A dynamic outcome $do(Y)_1+do(Y)_2$ can easily represent ``the flood crest expected on the 7th day has been mitigated over following days by our preventions'', but it would be challenging to specify a particular day's water rising for $Y_2$ ``if without'' $Y_1$. 

As \cite{hoel2017map} highlights, leveraging information theory in causality questions allows for formulations of the ``nonexistent'' or ``counterfactual'' statements. Indeed, the concept of ``information'' is inherently tied to \textbf{\emph{relations}}, irrespective of the potential \textbf{\emph{objects}} observed as their outcomes. 
Similar to the employment of the abstract variable $\phi$, we utilize $\theta$ to carry the EI of transitioning from $X_t$ to $Y_{t+7}$. Suppose $\theta=$ ``flooding'', and $EI(\theta)=$ ``what a flooding may imply'', we can then easily conceptualize $do(X)=$ ``continuous storm'' as its cause, and $do(Y)=$ ``disastrous water rise'' as the result in consciousness, without being notified the specific precipitation value  $X_t$ or a measured water-level $Y_{t+7}$.
In other words, our comprehension intrinsically has a ``relation-first'' manner, unlike the ``object-first'' approach we typically apply to modeling.

The so-called ``possible world'' is \textbf{\emph{created}} by our conciousness through innate ``relation-first'' thinking. In this world, the timing dimension is crucial; without a potential \emph{timing distribution}, ``possible observations'' would lose their significance. For instance, we might use a model $Y_{t+7}=f(X^t)$ to predict flooding. However, instead of ``knowing the exact water level on the 7th day'', our true aim is understanding ``how the flood might unfold; if not on the 7th day, then what about the 8th, 9th, and so on?''
With advanced representation learning techniques, particularly the success of RNNs in computing dynamics for the cause, achieving a dynamic outcome should be straightforward. Inversely, it might be time to reassess our conventional learning paradigm, which is based on an ``object-first'' approach, misaligned with our innate understanding.


%Over the years, there have been continual progresses toward mathematically symbolizing our innate ``relation-first'' understanding. However, it cannot be truly achieved under the epistemological framework. %However, truly achieving this remains unattainable within the confines of the framework of epistemology.

% The ``object-first'' thoughts preset humans as observers of the nature world, which are entrenched in the epistemological philosophy, transcending mere computational sciences. Particularly, since causality questions are conceived on our conceptional ``creations'', addressing them naturally needs us to step back into the creator's perspective. 

% Indeed, there has been ongoing progress in mathematically representing the concept of \emph{timing distribution} as aligned with our understanding, but a clear and explicit definition remains elusive. Largely due to being entrenched in epistemology, the timestamp $t$ can only be allowed as a priorly determined ``constant'' in models rather than a ``variable'' with computable values.
% This paper seeks to address this issue fundamentally.

The ``object-first'' mindset positions humans as observers of the natural world, which is deeply embedded in epistemological philosophy, extending beyond mere computational sciences.  Specifically, given that questions of causality originate from our conceptual ``creations'', addressing  these questions necessitates a return to the creator's perspective. This shift allows for the treatment of timing as computable variables rather than fixed observations.
Picard-LindelÃ¶f theorem represents time evolution by using a sequence $X^t = (X_1, \ldots, X_t)$ like captured through a series of snapshots. 
The information-theoretic measurements of causality, such as directed information \cite{massey1990causality} and transfer entropy \cite{schreiber2000measuring}, have linguistically emphasized the distinction between perceiving $X^t$ as ``a sequence of \emph{discrete} states'' versus holistically as ``a \emph{continuous} process''.
The introduction of do-calculus \cite{pearl2012calculus} marks a significant advancement, with the notation $do(X)$ explicitly treating the action of ``becoming $X$'' as a \emph{dynamic unit}. However, its differential nature let it focus on an ``identifiable'' sequence $\{\ldots, do(X_{t-1}), do(X_{t})\}$ rather than the integral $t$-dimension. Also, $do(Y)$ still lacks a foundation for declaration due to the observer's perspective. Even assumed discrete future states with relational constraints defined
\cite{hoel2013quantifying, hoel2017map} still face criticism for an absence of epistemological commitments \cite{eberhardt2022causal}.


% However, as a final endeavor grounded in epistemology, do-calculus possesses two major limitations: 1) $do(X)$ must be ``identifiable'', and 2) confined to specifying ``interventions'' only. 
% The former implies ``manually making sure the \emph{dynamics} over $t$-dimension to be differentiable'' in practice, while the latter restricts its use to the cause $X$, excluding the result $Y$.
% This is understandable since, from the perspective of an ``observer'', the timestamp $t$ can only act as a priorly determined ``constant'' in models, rather than a ``variable'' with computable values.
% Consequently, the notation $do(Y)$ implying possibly valued $t-1$ and $t$ cannot be declared or linguistically referred to, otherwise risking a lack of ``metaphysical commitments'' \cite{eberhardt2022causal}.

Without intending to delve into metaphysical debates, this paper aims to emphasize that for technological inquiries, shifting the perspective from that of an epistemologist, i.e., an observer, to that of a creator can yield models that resonate with our instinctive understanding. This can significantly simplify the questions we encounter, especially vital in the context regarding AGI. For purely philosophical discussions, readers are encouraged to explore the ``creationology'' theory by Mr.Zhao Tingyang.



\subsection{Mathematical Definition of Relation}

% Figure environment removed

A statistical model is typically defined through a function $f(x\mid \theta)$ that represents how a parameter $\theta$ is \emph{functionally related} to potential outcomes $x$ of a random variable $X$ \cite{ly2017tutorial}.
For instance, the coin flip model is also known as the Bernoulli distribution $f(x\mid \theta)=\theta^{x}(1-\theta)^{1-x}$ with $x\in \{0,1\}$, which relates the coin's propensity (i.e. its inherent possibility) $\theta$ to $X=$ ``land heads to the potential outcomes''. Formally, given a known $\theta$, the \emph{functional relationship} $f$ yields a probability density function (pdf) as $p_{\theta}(x)=f(x\mid\theta)$, according to which, $X$ is distributed and denoted as $X\sim f(x;\theta)$.
The Fisher Information $\mathcal{I}_X(\theta)$ of $X$ about $\theta$ is defined as $\mathcal{I}_X(\theta)=\int_{\{0,1\}}(\frac{d}{d\theta}log(f(x\mid\theta))^2 p_{\theta}(x)dx$, with the purpose of building models on the observed $x$ data being to obtain this information. For clarity, we refer to this initial perspective of understanding functional models as the \emph{\textbf{relation-first} principle}.

In practice, we do not limit all functions to pdfs but often shape them for easier understanding. For instance, let $X^n=(X_1,\ldots,X_n)$ represent an $n$-trial coin flip experiment, while to simplify, instead of considering the random vector $X^n$, we may only record the number of heads as $Y=\sum_{i=1}^n X_i$.
If these $n$ random variables are assumed to be independent and identically distributed (i.i.d.), governed by the identical $\theta$, the distribution of $Y$ (known as binomial) that describes how $\theta$ relates to $y$ would be $f(y\mid\theta)=\big(\begin{smallmatrix} n \\ y \end{smallmatrix}\big) \theta^y(1-\theta)^{n-y}$. In this case, the conditional probability of the raw data, $P(X^n\mid Y=y,\theta)=1/\big(\begin{smallmatrix} n \\ y \end{smallmatrix}\big)$ does not depend on $\theta$, implying that once $Y=y$ is given, $X^n$ becomes independent of $\theta$, although $X^n$ and $Y$ each depend on $\theta$ individually. 
It concludes that no information about $\theta$ remains in $X^n$ once $Y=y$ is observed \cite{fisher1920012, stigler1973studies}, denoted as $EI(X^n\rightarrow Y)=0$ in the context of relationship modeling. However, in the absence of the i.i.d. assumption and by using a vector $\vartheta=(\theta_1,\ldots,\theta_n)$  to represent the propensity in the $n$-trial experiment, we find that $EI(X^n\rightarrow Y)>0$ with respect to $\vartheta$.
Here, we revisit the foundational concept of Fisher Information, represented as $\mathcal{I}_{X\rightarrow Y}(\theta)$, to define:


\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{
\begin{minipage}{0.95\textwidth}
\paragraph{Definition 1.} 
A relationship denoted as $X \xrightarrow{\theta} {Y}$ is considered meaningful in the modeling context due to an \emph{informative \textbf{relation}} $\theta$, where $\mathcal{I}_{X\rightarrow Y}(\theta)>0$, simplifying as $\mathcal{I}(\theta) >0$.
\end{minipage}}
%\vspace{-0.8mm}


%its \emph{\textbf{dynaimc} outcome} is defined as $\mathcal{Y} = \operatorname{argmax}_{\mathcal{Y}} \ \mathcal{I}(\theta)$
% between \emph{\textbf{dynamic}} objects $\mathcal{X}$ and $\mathcal{Y}$

Specifically, rather than confining within a function $f(;\theta)$ as its parameter, we treat $\theta$ as an individual variable to encapsulate the effective information (EI) as outlined by Hoel. 
Consequently, the \emph{relation-first principle} asserts that a relationship is characterized and identified by a specific $\theta$, regardless of the appearance of its outcome $Y$, leading to the following inferences:
\begin{enumerate}[itemsep=0em, topsep=-0pt, 
parsep=2pt, partopsep=0pt,
leftmargin=22pt, labelwidth=10pt]
    \item $\mathcal{I}(\theta)$ inherently precedes and is independent of any observations of the outcome, as well as the chosen function $f$ used to describe the outcome distribution $Y\sim f(y;\theta)$.
    \item In a relationship identified by $\mathcal{I}(\theta)$, $Y$ is only used to signify its potential outcomes, without any further ``observational information'' associated with $Y$.
    \item In AI modeling contexts, a relationship is represented by $\mathcal{I}(\theta)$; as a latent space feature, it can be stored and reused to produce outcome observations.
    \item Just like $Y$ serving as the outcome of $\mathcal{I}(\theta)$, variable $X$ is governed by preceding relational information, manifesting as either observable data $x$ or priorly stored representations in modeling contexts.
\end{enumerate}


%Instead of ``defining relation by states'', we propose ``defining dynamics (encompassing all significant states and actions) by relation.'' For observational variables $X\in \mathbb{R}^n$ and $Y\in \mathbb{R}^m$, let's incorporate their \emph{timing dimensions}, $t$ and $\tau$, to formulate \emph{\textbf{dynamic} variables} $\mathcal{X}=\langle X,t \rangle \in \mathbb{R}^{n+1}$ and $\mathcal{Y}=\langle Y,\tau \rangle \in \mathbb{R}^{m+1}$; and introduce the abstract variable $\theta$ to represent the effective information $EI(\mathcal{X} \rightarrow \mathcal{Y})$ defined by Hoel.


\vspace{1mm}
{\vskip 0pt\bf About Relation $\theta$} 
\vspace{-0.5mm}

As emphasized by the Common Cause principle \cite{dawid1979conditional}, ``any nontrivial conditional independence between two observables requires a third, mutual cause'' \cite{scholkopf2021toward}. The crux here, however, is ``nontrivial'' rather than ``cause'' itself. For a system involving $X$ and $Y$, if their connection (i.e., the critical conditions without which they will become independent) deserves a particular description, it must represent unobservable information beyond the observable dependencies present in the system.
We use $\theta$ as an abstract variable to carry this information $\mathcal{I}(\theta)$, unnecessarily referring to tangible entities.
%Section \ref{} will provide practical examples to enhance empirical understanding of $\theta$.
% In our context, this translates to: If $\theta$ represents the critical connection between $X$ and $Y$, its informativeness becomes essential for rendering the system comprising $\{X,Y\}$ as a meaningful relationship for modeling.

Traditionally, descriptions of relationships are constrained by objective notations and focus on ``observable states at specific times''. 
For instance, to represent a particular EI, a state-to-state transition probability matrix $S$ is required \cite{hoel2013quantifying}. 
But $S$ is not solely sufficient to define a $EI(S)$, which also accounts for how the current state $s_0=S$ is related to the probability distributions of past and future states, $S_P$ and $S_F$, respectively. More importantly, manual specification from observed time sequences is necessitated to identify $S_P$, $S$, and $S_F$ irrespective of their observable timestamps. 
However, the advent of representation learning technology facilitates a shift towards ``relational information storage'', eliminating the need to specify observable timestamps. This allows for flexible computations across the timing dimension when the resulting observations are required, laying the groundwork for embodying $\mathcal{I}(\theta)$ in modeling contexts.

For an empirical understanding of $\theta$, let's consider an example: A sociological study explores interpersonal ties using consumption data. Bob and Jim, a father-son duo, consistently spend on craft supplies, indicating the father's influence on the son's hobbies. However, the ``father-son'' relational information, represented by $\mathcal{I}(\theta)$, exists solely in our perception - as knowledge - and cannot be directly inferred from the data alone.
Traditional \emph{object-first} approaches depend on manually labeled data points to signify the targeted $\mathcal{I}(\theta)$ in our consciousness. In contrast, \emph{relation-first} modeling seeks to derive $\mathcal{I}(\theta)$ beyond mere observations, enabling the autonomous identification of data-point pairs characterized as ``father-son''.
%Moreover, $\mathcal{I}(\theta)$ defines the roles of entities within a specific relationship. For instance, Jim's peers with matching video game expenditure patterns could signify their friendships. The roles of ``son'' or ``friend'' are identified through specific $\theta$.
%For example, similar spending patterns on video games among Jim's peers might indicate friendships
%thoroughly collects detailed consumption records of every community member and utilizes AI to study the impact of interpersonal relationships. 
%We can study Jim in the roles of ``son'' and ``friend,'' but it is impossible to discuss ``What is Jim?'' independently of all relationships - this might be a question for biology, physics, or religion, but it certainly is not an engineering question.

Since the representation of $\mathcal{I}(\theta)$ is not limited by observational distributions, it allows outcome computation across the timing dimension. This capability is crucial for enabling ``causality'' in modeling, transcending mere correlational computations.
Specifically, we use the notations $\mathcal{X}$ and $\mathcal{Y}$ to indicate the integration of the timing dimension for $X$ and $Y$, and represent a relationship in the general form $\mathcal{X}\xrightarrow{\theta}\mathcal{Y}$.
We will first introduce $\mathcal{X}$ as a general variable, followed by discussions about the relational outcome $\mathcal{Y}$.


\vspace{2mm}
{\vskip 0pt\bf About Dynamic Variable $\mathcal{X}$} 
\vspace{-0.5mm}


\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{
\begin{minipage}{0.95\textwidth}
\paragraph{Definition 2.} For a variable $X\in \mathbb{R}^n$ observed as a time sequence $x^t=(x_1,\ldots,x_t)$, a \emph{\textbf{dynamic}} variable $\mathcal{X}=\langle X,t \rangle \in \mathbb{R}^{n+1}$ is formulated by integrating the \emph{timing} $t$ as an additional dimension.
\end{minipage}}
%\vspace{-0.8mm}

Time series data analysis is often referred to as being ``spatial-temporal'' \cite{andrienko2003exploratory}.
However, in modeling contexts, ``spatial'' is interpreted broadly and not limited to physical spatial measurements (e.g., geographic coordinates); thus, we prefer the term ``observational''. Furthermore, to avoid the implication of ``short duration'' often associated with ``temporal,'' we use ``timing'' to represent the dimension $t$. 
Unlike the conventional representation in the sequence $X^t=(X_1, \ldots, X_t)$ with static $t$ values (i.e., the timestamps), we consider $\mathcal{X}$ holistically as a \emph{dynamic} variable, similarly for $\mathcal{Y}=\langle Y,\tau \rangle \in \mathbb{R}^{m+1}$. 
The probability distributions of $\mathcal{X}$, as well as $\mathcal{Y}$, span both \emph{observational} and \emph{timing} dimensions simultaneously.

Specifically, $\mathcal{X}$ can be viewed as the integral of discrete $X_t$ or continuous $do(X_t)$ over the timing dimension $t$ within a required range. The necessity for representation by $do(X_t)$, as opposed to $X_t$, underscores the \emph{\textbf{dynamical significance}} of $\mathcal{X}$. Put simply, if $\mathcal{X}$ can be formulated as $\mathcal{X} = \sum_{1}^t X_t$, it equates to $X^t = (X_1, \dots, X_t)$ in modeling. Conversely, $\mathcal{X} = \int_{-\infty}^{\infty} do(X_t) dt$ portrays $\mathcal{X}$ as a \emph{dynamic}, marked by significant dependencies among ${X_{t-1}, X_t}$ for unconstrained $t\in (-\infty,\infty)$. Essentially, $do(X_t)$ represents a differential unit of continuous timing distribution over $t$, highlighting not just the observed state $X_t$ but also the significant dependence $P(X_{t}\mid X_{t-1})$, challenging the i.i.d. assumption.
The ``state-dependent'' and ``state-independent'' concepts refer to Hoel's discussions in causal emergence \cite{hoel2013quantifying}.



\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{
\begin{minipage}{0.95\textwidth}
\paragraph{Theorem 1.} Timing becomes a necessary \emph{computational dimension} if and only if the required variable necessatates \emph{dynamical significance}, characterized by a \emph{\textbf{nonlinear} distribution} across timing. 
\end{minipage}}
\vspace{-1mm}

In simpler terms, if a distribution over timing $t$ cannot be adequately represented by a function of the form $x_{t+1}=f(x^t)$, then its nonlinearity is significant to be considered. Here, the time step $[t,t+1]$ is a predetermined constant timespan value. 
RNN models can effectively extract dynamically significant $\mathcal{X}$
from data sequences $x^t$ to autonomously achieve $\mathcal{X}\xrightarrow{\theta}Y$, due to leveraging the relational constraint by $\mathcal{I}(\theta)$. In other words, RNNs perform indexing through $\theta$ to fulfill dynamical $\mathcal{X}$.
Conversely, if ``predicting'' such an irregularly nonlinear timing-dimensional distribution is crucial, the implication arises that it has been identified as the causal effect of some underlying reason.

\vspace{2mm}
{\vskip 0pt\bf About Dynamic Outcome $\mathcal{Y}$} 
%\vspace{-0.5mm}


%\vspace{2mm}
%\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=red!50!black, colback=yellow!10]{
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{
\begin{minipage}{0.98\textwidth}
\paragraph{Theorem 2.} In modeling contexts, identifying a relationship $\mathcal{X} \xrightarrow{\theta} \mathcal{Y}$ as \emph{Causality}, distinct from mere \emph{Correlation}, depends on the \emph{\textbf{dynamical significance} of the outcome} $\mathcal{Y}$ as required by $\mathcal{I}(\theta)$. 
\end{minipage}}
%\vspace{-1mm}

Figure \ref{fig:symbolic} illustrates the distinction between causality and correlation, where an arrow indicates an informative relation and a dashed line means statistical dependence. 
If conducting the integral operation for both sides of the do-calculus formation $X/do(X)\rightarrow Y$ over timing, we can achieve $\mathcal{X}\rightarrow \sum_{1}^{\tau}Y_{\tau}$ with the variable $\mathcal{X}$ allowing to be dynamically significant but the outcome $\sum_{1}^{\tau}Y_{\tau}$ certainly not.
Essentially, to guarantee $\mathcal{Y}$ presenting in form of $y^\tau=(y_1,\ldots,y_{\tau})$ to match with predetermined timestamps $\{1,\ldots,\tau\}$, do-calculus manually conducts a differentiation operation on the relational information $\mathcal{I}(\theta)$ to discretize the timing outcome. 
This process is to confirm specific $\tau$ values at which $y_{\tau}$ can be identified as the effect of a certain $do(x_t)$ or $x_t$. Accordingly, the state value $y_{\tau}$ will be defined as either the interventional effect $f_V(do(x_t))$ or the observational effect $f_B(x_t)$, with three criteria in place to maintain conditional independence between these two possibilities, given a tangible elemental reason $\Delta\mathcal{I}(\theta)$ (i.e., identifiable $do(x_t)\rightarrow y_{\tau}$ or $x_t\rightarrow y_{\tau}$):

\vspace{-6mm}
\[
\mathcal{Y}=f(\mathcal{X}) = \sum_t f_V(do(x_t)) \cdot f_B(x_t) = \sum_t\left\{
\begin{array}{lll}
    f_B(x_t)=y_{\tau} & \text{ with } f_V(do(x_t))=1 \text{ (Rule 1)}&\multirow{3}{*}{$\left.\begin{matrix}\\ \\ \\ \end{matrix}\right\}=\displaystyle\sum_{\tau} y_{\tau}$} \\
    f_V(do(x_t))=y_{\tau} & \text{ with } f_B(x_t)=1 \text{ (Rule 2)}& \\
    0=y_{\tau} & \text{ with } f_V(do(x_t))=0 \text{ (Rule 3)}& \\
    \text{otherwise} & \text{ not identifiable } &
\end{array}\right.
\]
\vspace{-5mm}


In contrast, the proposed \emph{dynamic} notations $\mathcal{X}=\langle X,t \rangle$ and $\mathcal{Y}=\langle Y,\tau \rangle$ offer advantages in two respects. First, the concept of $do(Y_{\tau})$ can be introduced with $\tau$ indicating its ``possible timing'', which is unfounded under the traditional modeling paradigm; and then, by incorporating $t$ and $\tau$ into computations, the need to distinguish between ``past and future'' has been eliminated.


%No matter whether identified as causality or correlation, both of them are informative relationships just of distinct types, distinguished from mere statistical dependence where $\mathcal{I}(\theta)=0$. 

\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{
\begin{minipage}{0.95\textwidth}
\paragraph{Definition 3.} 
A \textbf{\emph{causality}} characterized by a \emph{dynamically significant outcome} $\mathcal{Y}$ can encompass multiple \textbf{\emph{causal components}}, represented by $\vartheta=(\vartheta_1,\ldots,\vartheta_{T})$. 
Each $\vartheta_{\tau}$ with $\tau\in\{1,\ldots,T\}$ identifies a timing dimension $\tau$ to accommodate the corresponding \textbf{\emph{outcome component}} $\mathcal{Y}_{\tau}$. 

The overall outcome is denoted as $\mathcal{Y} = \sum_{\tau=1}^T \mathcal{Y}_{\tau} = \sum_{\tau=1}^T \int do(Y_{\tau}) d\tau$, simplifying to $ \oint do(Y_{\tau}) d\tau$.
\end{minipage}}
\vspace{1mm}

Definition 3, based on the relation-first principle, uses $\vartheta$ to signify causality. Its distinction from $\theta$ implies that the potential outcome $\mathcal{Y}$ must be dynamically significant.
Specifically, within a general relationship, denoted by $\mathcal{X}\xrightarrow{\theta}\mathcal{Y}$, the dynamic outcome $\mathcal{Y}$ only showcases its capability to encompass nonlinear distribution over timing, whereas $\mathcal{X}\xrightarrow{\vartheta}\mathcal{Y}$ confirms such nature of this relationship, as required by $\mathcal{I}(\vartheta)$.

According to Theorem 1, incorporating the possible timing dimension $\tau$ when computing $\mathcal{Y}$ is necessary for a causality identified by $\mathcal{I}(\vartheta)$.
If a relationship model can be formulated as $f(\mathcal{X})=Y^{\tau}=(Y_1,\ldots, Y_{\tau})$, it is equal to applying the independent state-outcome model $f(\mathcal{X})=Y$ for $\tau$ times in sequence. In other words, $\mathcal{X}\xrightarrow{\theta}Y$ is sufficient to represent this relationship without needing $\tau$.
It often goes unnoticed that a sequence variable $X^t=(X_1,\ldots,X_t)$ in modeling does not imply the $t$-dimension has been incorporated, where $t$ serves as constants, lacking computational flexibility. The same way also applies to $Y^{\tau}$.

However, once including the ``possible timing'' $\tau$ with computable values, it becomes necessary to account for the potential components of $\mathcal{Y}$, which are possible to unfold their dynamics over their own timing separately.
For a simpler understanding, let's revisit the example of ``storm causes flooding.''
Suppose $\mathcal{X}$ represents the storm, and for each watershed, $\vartheta$ encapsulates the effects of $\mathcal{X}$ determined by its unique hydrological conditions. Let $\mathcal{Y}_2$ denote the water levels observed over an extended period, such as the next 30 days, if \emph{without} any flood prevention. Let $\mathcal{Y}_1$ indicate the daily variations in water levels (measured in $\pm$cm to reflect increases or decreases) resulting from flood-prevention efforts. 
In this case, $\vartheta$ can be considered in two components: $\vartheta=(\vartheta_1, \vartheta_2)$, separately identifying $\tau=1$ and $\tau=2$.

Specifically, historical records of disasters without flood prevention could contribute to extracting $\mathcal{I}(\vartheta_2)$, based on which, the $\vartheta_1$ representation can be trained using recent records of flood prevention.
Even if their hydrological conditions are not exactly the same, AI can extract such relational difference $(\vartheta_1-\vartheta_2)$. 
This is because the capability of computing over timing dimensions empowers AI to extract common relational information from different dynamics.
%relational information from dynamically significant $\mathcal{Y}$, by including its timing-dimensional distribution into computations.
From AI's standpoint, regardless of whether the flood crest naturally occurs on the 7th day or is dispersed over the subsequent 30 days, both $\mathcal{Y}_2$ and $(\mathcal{Y}_1+\mathcal{Y}_2)$ are linked to $\mathcal{X}$ through the same volume of water introduced by $\mathcal{X}$. In other words, while AI deals with the computations, discerning what qualifies as a ``disaster'' remains a question for humans.

Conversely, in traditional modeling, $\vartheta$ is often viewed as a common cause of both $\mathcal{X}$ and $\mathcal{Y}$, termed a ``confounder'', and serves as a predetermined functional parameter before computation.
Therefore, if such a parameter is accurately specified to represent $\vartheta_2$, when observations $(\mathcal{Y}_1+\mathcal{Y}_2)$ imply a varied $\vartheta_1$, it becomes critical to identify the potential ``reason'' of such variances.
If the underlying knowledge can be found, manual adjustments are naturally necessitated for $(\mathcal{Y}_1+\mathcal{Y}_2)$ to ensure it performs as being produced by $\vartheta_2$; otherwise, the modeling bias will be attributed to this unknown ``reason'' represented by the difference $(\vartheta_1-\vartheta_2)$, named a hidden confounder.

%to achieve identical $\vartheta_1$ and $\vartheta_2$ ensuring comparable outcome sequences $Y^{\tau}_1$ and $Y^{\tau}_2$. 
%For example, in clinical trials, patient groups receiving different treatments must be ``randomized'' similarly to maintain comparability. This is essentially for eliminating the dynamical significance of the outcomes.


%Under the new paradigm, the goal of modeling is to capture the relations reflected by cause and effect, even under varied conditions, with the interpretation of relational variations being a task for humans. 
%In traditional modeling, $\vartheta$ is often viewed as a ``hidden confounder (i.e., common cause),'' implying unknown factors like ``underlying hydrological conditions''. 
%Therefore, manual adjustments are often required to eliminate potential influences of the unknown $\vartheta$, to ensure the computed outcome components are comparable. 


\vspace{2mm}
{\vskip 0pt\bf About Dependence $\phi$ between Causal Components} 
%\vspace{-0.5mm}

As demonstrated in Figure~\ref{fig:macro},
by introducing the dynamic outcome components in (c), the causal emergence phenomenon in (b) can be explained by ``overflowed'' relational information with $\phi$. 
Here, $do(Y)_1$ and $do(Y)_2$ act as differentiated $\mathcal{Y}_1$ and $\mathcal{Y}_2$, outcome by $\mathcal{I}(\vartheta_1)$ and $\mathcal{I}(\vartheta_2)$. That is, the \emph{relation-first} principle ensures $\vartheta$ to be informatively separable as $\vartheta_1$ and $\vartheta_2$, leaving $\phi$ simply represent their statistical dependence. However, due to their dynamical significance, $\phi$ may impact the conditional timing distribution across $\tau=1$ and $\tau=2$.

\vspace{0.5mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{
\begin{minipage}{0.95\textwidth}
\paragraph{Theorem 3.} Sequential causal modeling is required, if the \textbf{\emph{dependence}} between causal components, represented by $\phi$, has dynamically significant impact on the outcome timing dimension.
\end{minipage}}
\vspace{-1mm}


The sequential modeling procedure was applied in analyzing the ``flooding'' example, where training $\vartheta_1$ is conditioned on the established $\vartheta_2$ to ensure the resulting representation is meaningful. Specifically, the directed dependence $\phi$ from $\vartheta_2$ to $\vartheta_1$ requires that the timing-dimensional computations of $\mathcal{Y}_1$ and $\mathcal{Y}_2$ occur sequentially, with $\vartheta_1$ following $\vartheta_2$. Practically, the sequence is determined by the meaningful interaction $\mathcal{I}(\vartheta_1\mid \vartheta_2)$ or $\mathcal{I}(\vartheta_2\mid \vartheta_1)$, adapted to the requirements of specific applications.
%the dependence between $\vartheta_1$ and $\vartheta_2$ represented by $\phi$ has impacts on the timing dimension of their dynamic outcomes $\mathcal{Y}_1$ and $\mathcal{Y}_2$. As a result, the timing-dimensional distributions of $\mathcal{Y}_1$ and $\mathcal{Y}_2$ must be determined sequentially, 

Suppose the two-step modeling process is $\mathcal{Y}_2=f_2(\mathcal{X};\vartheta_2)$ followed by $\mathcal{Y}_1=f_1(\mathcal{X}\mid \mathcal{Y}_2;\vartheta_1)$. According to the adopted perspective, its information explanation can be notably different. 
From the creator's perspective that enables \emph{relation-first}, $\mathcal{I}(\vartheta)=\mathcal{I}(\vartheta_2)+\mathcal{I}(\vartheta_1) = 2\mathcal{I}(\vartheta_2)+\mathcal{I}(\vartheta_1\mid \vartheta_2)$ encapsulates all information needed to ``create'' the outcome $\mathcal{Y}=\mathcal{Y}_1+\mathcal{Y}_2$, with $\mathcal{I}(\phi)=0$ indicating $\phi$ not an informative relation. When adopting the traditional perspective as an observer, $\vartheta_1$ and $\vartheta_2$ simply denote functional parameters, where the \emph{observational information} manifests as $\mathcal{I}(\phi\mid \mathcal{Y}_2)=\mathcal{I}(\mathcal{Y}_1)-\mathcal{I}(\mathcal{Y}_2)>0$.

For clarity, we use $\vartheta_1 \indep \vartheta_2$ to signify the timing-dimensional independence between $\mathcal{Y}_1$ and $\mathcal{Y}_2$, termed as \textbf{\emph{dynamical independence}}, without altering the conventional understanding within the observational space, like $Y_1\indep Y_2\in \mathbb{R}^m$. On the contrary, $\vartheta_1 \nindep \vartheta_2$ implies a \textbf{\emph{dynamical dependence}}, which is, an \emph{interaction} between $\mathcal{Y}_1$ and $\mathcal{Y}_2$. ``Dynamically dependent or not'' only holds when $\mathcal{Y}_1$ and $\mathcal{Y}_2$ are \emph{dynamically significant}.
%are causal, underscoring the dynamical significance of their outcomes. 

% Figure environment removed

Figure~\ref{fig:interact} is upgraded from the conventional causal Directed Acyclic Graph (DAG) in two aspects: 1) A node represents a state value of the variable, and 2) edge length shows timespans for a data instance (i.e., a data point or realization) to achieve this value.
This allows for the visualization of dynamic interactions through different data instances.
%the dynamical differences among instances, influenced by the same causal effect (or component), as if captured in a snapshot. 
For instance, Figure~\ref{fig:interact}(c) shows that the dependence between $\vartheta_1$ and $\vartheta_2$ inversely impacts their speeds, such that achieving $y_1$ more quickly implies a slower attainment of $y_2$.

%a certain effect magnitude, signified by a static value. %the effects to reach an equivalent magnitude.


\subsection{Potential Development Toward AGI}

As demonstrated, choosing between the observer's or the creator's perspective depends on the questions we are addressing rather than a matter of conflict. In the former, information is gained from observations and represented by observables; while in the latter, relational information preferentially exists as representing the knowledge we aim to construct in modeling, such that once the model is established, we can use it to deduce outcomes as a description of ``possible observations in the future'' without direct observation.

Causality questions inherently require the creator's perspective, since ``informative observations'' cannot emerge out of nowhere. Empirically, it is reflected as the challenge of specifying outcomes in traditional causal modeling, often referred to as ``identification difficulty'' \cite{zhang2012identifiability}.
As mentioned by \cite{scholkopf2021toward}, ``we may need a new learning paradigm'' to depart from the i.i.d.-based modeling assumption, which essentially asserts the objects we are modeling exactly exist as how we expect them to. We term this conventional paradigm as \emph{object-first} and have introduced the \emph{relation-first} principle accordingly.

% Figure environment removed
\vspace{-2mm}

The \emph{relation-first} thinking has been embraced by the definition of Fisher Information, as well as in do-calculus that differentiates the relational information.
Moreover, neural networks with the back-propagation strategy have technologically embodied it.
Therefore, it's unsurprising that the advent of AI-based representation learning signifies a turning point in causality modeling. 
From an engineering standpoint, answering the ``what ... if?'' (i.e., counterfactual) question indicates the capacity of predicting $do(Y)$ as structuralized dynamic outcomes.
% actions, implying a dynamically significant relationship outcome. 
Intriguingly, learning dynamics (i.e., the realization of $do(\cdot)$) and predicting outcomes (i.e., facilitating the role of $Y$) present a paradox under the traditional learning paradigm, as in Figure \ref{fig:overview}.

%As noted by \cite{rumelhart1986learning}, ``a simple procedure in a general purpose is powerful enough.''
%Sometimes, the resolution of problems is not finding an answer, but an approach that makes the problem disappear (from Ludwig Wittgenstein's philosophical works).

\vspace{2mm}
{\vskip 0pt\bf About AI-based Dynamical Learning} 
%\vspace{-0.5mm}

Understanding dynamics is a significant instinctive human ability. Representation learning achieves computational optimizations across the timing dimension, notably embodying such capabilities. Specifically, Large Language Models (LLMs) \cite{gurnee2023language} have sparked discussions about our progress toward AGI \cite{schaeffer2023emergent}. The application of meta-learning \cite{lake2023human}, in particular, has enabled the autonomous identification of semantically meaningful dynamics, demonstrating the potential for human-like intelligence.
Yet, it is also highlighted that LLMs still lack a true comprehension of causality \cite{pavlick2023symbols}.

The complexity of causality lies in potential interactions within a ``possible world'', not just in computing individual possibilities, whether they are dynamically significant or not.
Instead of a single question, ``what ... if?'' stands for a self-extending logic, where the ``if'' condition can be applied to computed results repeatedly, leading to complex structures. 
Thus, causality modeling is to uncover the unobservable knowledge implied by the observable $X/do(X) \rightarrow Y/do(Y)$ phenomenons to enable its outcome beyond direct observations. 

Advanced technologies, such as reinforcement learning \cite{arora2021survey} and causal representation learning, have blurred the boundary between the roles of variable $X/do(X)$ and outcome $Y/do(Y)$, which are manually maintained in traditional causal inference.
They often focus on the advanced efficacy in learning dynamics, yet it is frequently overlooked that the foundational RNN architecture is grounded in $do(X) \rightarrow Y$ without establishing a dynamically interactable $do(Y)$. 
Essentially, any significant dynamics that are autonomously extracted by AI can be attributed to $do(X)$.
Even though within diffusion methods, their computations can be split into multiple rounds of $do(X) \rightarrow Y$, since without an identified meaning as $\mathcal{I}(\vartheta)$, the significance of becoming a $do(Y)$, rather than remaining a sequence of discrete values $Y^{\tau}=(Y_1,\ldots,Y_{\tau})$, is unfounded.

From AI's viewpoint, changes in the values of a sequential variable need not be meaningful, although they may have distinct implications for humans. For instance, a consistent dynamic pattern that varies in unfolding speed might indicate an individual dynamic, $do(X)$, distinct from $X^t$. If this dynamic pattern specifically signifies the effect (like $\mathcal{I}(\vartheta)$) of a certain cause (like $X/do(X)$), it could represent $do(Y)$. However, if the speed change is attributable to another identifiable effect (such as $\mathcal{I}(\omega)$), it showcases a dynamical interaction.

% With no interactions, they simply exist as a cluster of individual relationships, with each directed from an observed dynamic $do(X)$ to a static possibility $Y$. 
%components rather than constructing a ``possible world'' as characterized by causality. In other words, the potential interactions between dynamics dictate the need for us to identify them as dynamics, distinguish their timelines, and discuss the potential structures - In some ways, these necessities impose ``causality'' as a terminology.

%In other words, the potential interactions between dynamics dictate them to be $do(Y)$ components, distinguished from a set of individual $do(X)$ ones.


%It is often overlooked that
%Frequently, it goes unnoticed that
%It is commonly disregarded that
%Many tend to overlook that

\vspace{2mm}
{\vskip 0pt\bf About State Outcomes in Causal Inference} 
%\vspace{-0.5mm}

Causal inference and associated Structural Causal Models (SCMs) focus on causal structures, taking into account potential interactions.
However, the \emph{object-first} paradigm restricts their outcomes to be ``objective observations'', represented by $Y_{\tau}$ with a predetermined timestamp $\tau$. This inherently implies all potential effects conform to a singular ``observed timing''. Thereby, they can be consolidated into a one-time dynamic, leading to ``structuralized observables'' instead of ``structuralized dynamics''.
As in Figure \ref{fig:macro}, the overflowed information $\mathcal{I}(do(Y))-\mathcal{I}(Y)$ (from an observer's perspective) ``emerges'' to form an informative relation $\phi$ in a ``possible world'', rather than a deducible dependence between two dynamics $do(Y)_1$ and $do(Y)_2$.

Such ``causal emergence'' requires significant efforts on theoretical interpretations.
Particularly, the unknown relation $\phi$ is often attributed to the well-known ``hidden confounder'' problem \cite{greenland1999confounding, pearl2000models}, linked to the fundamental assumptions of causal sufficiency and faithfulness \cite{sobel1996introduction}. In practice, converting causal knowledge represented by DAGs into operational causal models demands careful consideration \cite{elwert2013graphical}, where data adjustments and model interpretations often rely on human insight \cite{sanchez2022causal, crown2019real}.
These theoretical accomplishments underpin causal inference's core value in the era dominated by statistical analysis, before the advent of neural networks.
%Currently, leveraging representation learning enables the autonomous extraction of relational information in AI modeling. 
%Given a specific $\mathcal{X}$ as the cause, all potential information required to establish relationships towards a possible $\mathcal{Y}$ is encapsulated by $\mathcal{I}(\vartheta)$.

%Conversely, under the \emph{relation-first} principle,  This is rooted in the observations of $\mathcal{X}$ and aligns with our causal comprehension.

\vspace{2mm}
{\vskip 0pt\bf About Development of Relation-First Paradigm} 
%\vspace{-0.5mm}

As highlighted in Theorem 3, sequential modeling is necessary for causality to achieve structuralized dynamic outcomes. When the prior knowledge of causal structure is given, the relational information $\mathcal{I}(\vartheta)$ has been determined; correspondingly, the sequential input and output data, $x^t=(x_1,\ldots,x_t)$ and $y^{\tau}=(y_1,\ldots,y_{\tau})$, can be chosen to enable AI to extract $\mathcal{I}(\vartheta)$ through them.
While for AI-detected meaningful dynamics, we should purposefully recognize ``if it suggests a $do(Y)$, what $\mathcal{I}(\vartheta)$ have we extracted?'' The gained insights can guide us to make the decision on whether and how to perform the next round of detection based on it.




% indifferent to the modeling process itself.
% The informativeness of achieved models hinges on how well they represent specific knowledge as resonating with our comprehension. For example, to extract ``father-son'' relational features, we can use records of ``fathers'' and ``sons'' as input and output, although from the AI's standpoint, they are indistinguishable from those of unrelated individuals to learn mere dependency.


% AI-based representation learning methods act as tools to extract relational information $\mathcal{I}(\theta)$.
% However, how informative the extracted $\mathcal{I}(\theta)$ is determined by the intended human knowledge.

% Specifically, our decisions are necessary to infuse particular relational information into the representations derived by AI.

% Figure environment removed


In this way, the relational representations in latent space can be accumulated as vital resources, organized and managed through the graphically structured indices, as depicted in Figure \ref{fig:new}. 
This flow mirrors human learning processes \cite{sep-mental-representation}, with these indices serving as causal DAGs in our comprehension. 
If knowledge from various domains could be compiled and made accessible like a library over time, then the representation resource might be continuously optimized across diverse scenarios, thereby enhancing generalizability.

From a human standpoint, deciphering latent space representations becomes unnecessary. With sufficient raw data, we have the opportunity to establish nuanced causal reasoning through the use of graphical indices. Specifically, this involves an indexing process that translates inquiries into specific input-output graphical routines, guiding data streaming through autoencoders to produce human-readable observations. Although convenient, this approach could subject computer ``intelligence'' to more effective control.


\section{Modeling Framework in Creator's Perspective}

Under the traditional i.i.d.-based framework, questions must be addressed individually within their respective modeling processes, even when they share similar underlying knowledge. This necessity arises because each modeling process harbors incorrect premises about the objective reality it faces, which often goes unnoticed because of conventional \emph{object-first} thinking. The advanced modeling flexibility afforded by neural networks further exposes this fundamental issue. Specifically, it is identified as the \emph{model generalizability} challenge by \cite{scholkopf2021toward}. They introduced the concept of \emph{causal representation} learning, underscoring the importance of prioritizing causal relational information before specifying observables.

Rather than merely raising a new method, we aim to emphasize that the \textbf{\emph{shift of perspective}} enables the modeling framework across the ``possible timing space'' beyond solely observational one. As shown in Figure \ref{fig:space}, when adopting the creator's perspective, space $\mathbb{R}^H$ is embraced to accommodate the abstract variables representing the informative relations, where the notion of $\omega$ will be introduced later. 



% Figure environment removed
\vspace{-1mm}

%For a general relationship $\mathcal{X}\xrightarrow{\theta}\mathcal{Y}$, the causal variable $\mathcal{X}\in\mathbb{R}^O$ spans across observational dimensions in $\mathbb{R}^O$, including $t$ that signifies \emph{observed timing}. Conversely, the outcome $\mathcal{Y}$, representing a ``possible world,'' extends across all dimensions in $\mathbb{R}^O$ \emph{except} $t$, and into the \emph{possible timing space} $\mathbb{R}^T$, denoted as $\mathcal{Y}\in\mathbb{R}^{O-1}\cup\mathbb{R}^T$. 


When adopting an observer's perspective, it involves answering a ``what...if'' question just once. However, the genesis of such questions is rooted in the perspective of a ``creator'', aiming to explore all possibilities for the optimal choice, which is precisely what we embrace when seeking technological or engineering solutions.

Every possibility represents an observational outcome (``the what...'') for a specific causal relationship (``the if...'') or a routine of consecutive relationships within a DAG, akin to placing an observer within the creator's conceptual space. Thus, the ``creator's perspective'' acts as a space encompassing all potential ``observer's perspectives'' by treating the latter as a variable.
Within this framework, the once perplexing concept of ``collapse'' in quantum mechanics becomes readily understandable.

From the creator's perspective, a causal relationship $\mathcal{X}\xrightarrow{\vartheta}\mathcal{Y}$ suggests that $\mathcal{Y}$ belongs to $\mathbb{R}^{O-1}\cup \mathbb{R}^{T}$, where $\mathbb{R}^{T}$ represents a $T$-dimensional space with timing $\tau=1,\ldots,T$ sequentially marking the $T$ components of $\mathcal{Y}$. The separation of these components depends on the creator's needs, regardless of which, their aggregate, $\mathcal{Y}=\sum_{\tau=1}^T \mathcal{Y}_{\tau}$, is invariably governed by $\mathcal{I}(\vartheta)$.
However, once the creator places an observer for this relationship, from this ``newborn'' observer's viewpoint, space $\mathbb{R}^{T}$ ceases to exist and is perceived solely as an ``observed timeline'' $\tau$. In other words, $\tau$ has lost its computational flexibility as the ``timing dimension'' but remains merely a sequence of constant timestamps. 

Thus, the term ``collapse'' refers to this singular ``perspective shift''.
Metaphorically, a one-time ``collapse'' is akin to opening SchrÃ¶dinger's box once, and in the modeling context, it signifies that a singular modeling computation has occurred.
Accordingly, Theorem 3 can be reinterpreted: Causality modeling is to facilitate ``structuralized collapses'' within $\mathbb{R}^T$ from the creator's perspective. Importantly, for the creator, $\mathbb{R}^T$ is not limited to representing a single relationship but can also include ``structuralized relationships'' by embracing a broader macro-level perspective. In light of this, we introduce the following definitions.

%\vspace{2mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{
\begin{minipage}{0.95\textwidth}
\paragraph{Definition 4.} 
A causal relation $\vartheta$ can be defined as \textbf{\emph{micro-causal}} if an extraneous relation $\omega$ exists, where $\mathcal{I}(\omega)\not\subseteq\mathcal{I}(\vartheta)$, such that incorporating $\omega$ can form a new, \textbf{\emph{macro-causal}} relation, denoted by $(\vartheta,\omega)$. The process of incorporating $\omega$ is referred to as a \textbf{\emph{generalization}}.
\end{minipage}}
%\vspace{1mm}


\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{
\begin{minipage}{0.95\textwidth}
\paragraph{Definition 5.} 
From the creator's perspective, the \textbf{\emph{macro-level}} possible timing space $\mathbb{R}^T=\sum_{\tau=1}^T \mathbb{R}^{\tau}$ is constructed by aggregating each \textbf{\emph{micro-level}} space $\mathbb{R}^{\tau}$, where $\tau\in\{1,\ldots,T\}$ indicates the \textbf{\emph{timeline}} that houses the sequential timestamps by adopting the observer's perspective for $\mathbb{R}^{\tau}$.
\end{minipage}}
%\vspace{1mm}

To clarify, the $T$-dimensional space $\mathbb{R}^T$ mentioned earlier is considered a micro-level concept, which we formally denote as $\mathbb{R}^{\tau}$. Upon transitioning to the macro-level possible timing space $\mathbb{R}^T$, the creator's perspective is invoked. Within this perspective, both $\mathbb{R}^H$ and $\mathbb{R}^T$ are viewed as conceptual spaces, lacking computationally meaningful notions like ``dimensionality'' or specific ``distributions''. 

In essence, the moment we contemplate a potential ``computation'', the observer's perspective is already established, from which, the micro-level space $\mathbb{R}^{\tau}$ (or a collection of such spaces $\{\mathbb{R}^{\tau}\}$) has been defined and ``primed for collapse'' through the methodologies under contemplation. Philosophically, the notion of a timeline $\tau$ within the ``thought space'' $\mathbb{R}^T$ is characterized as ``relative timing'' \cite{wulf1994reducing, shea2001effects}, in contrast to the ``absolute timing'' represented by $t$ in this paper. 
Moreover, in the modeling context, computations involving $\tau$ can draw upon the established Granger causality approach \cite{granger1993modelling}.

% The implication of $\tau$ depends on our perspective, either as the creator or the observer, and is directly linked to whether including $\mathbb{R}^H$ in view.
% From the creator's perspective, the abstract variable $\theta\in\mathbb{R}^H$ is to accommodate relational information $\mathcal{I}(\theta)$, without specifiable distributions. The number of components of $\mathcal{Y}$, denoted as $T$, is determined by $\mathcal{X}$ and $\mathcal{I}(\theta)$, represented by $\mathcal{Y}=\sum_{\tau=1}^T \mathcal{Y}_{\tau}$, where $\tau=\{1,\ldots,T\}$ sequentially refers to timings of these components, termed $T$ \emph{timelines}. They act as axes to form a $T$-dimensional space $\mathbb{R}^T$ at the micro-level (see Definition 4 below), as required by the given relationship $\mathcal{X}\xrightarrow{\theta}\mathcal{Y}$.

% On the other hand, when taking the observer's perspective to view the causally established outcome $\mathcal{Y}$, the framework is down to $\mathbb{R}^O\cup \mathbb{R}^T$, i.e., the observable union space excluding $\mathbb{R}^H$. In contrast, instead of a multi-dimensional subspace within the macro-level $\mathbb{R}^T$,
% the concept of $\tau$ ``collapses'' into a single timeline, simply becoming another $t$ (close to Granger causality \cite{granger1993modelling}), aligning with the concept of ``objective collapse'' in quantum mechanics. 
% In modeling contexts, a one-time ``computing'' indicates a ``collapse'': Once $\mathcal{Y}$ has been computed to be sequentially observable values, $\tau$ becomes its constant timestamp, losing its computational flexibility as ``timing dimension''. 

% The necessity for a ``perspective shift'' is that the space $\mathbb{R}^T$ can only be perceived and constructed through the \emph{relation-first} principle, adopting a creator's perspective, while established models can be interpreted in either perspective.
% In this section, we will demonstrate the key concepts relative to $\mathbb{R}^H$ and $\mathbb{R}^T$ sequentially, then analyze the fundamental impact of solely adopting the observer's perspective on classic causal inference.

% \%---------------
% Suppose $\theta$ acting as the parameter of a function $f(;\theta)$ that can adequately describe $\mathcal{X} \xrightarrow{\theta} \mathcal{Y}$.
% In that case, we can define
% $\mathcal{Y}^{\theta} = f(\mathcal{X}; \theta)$ as the \emph{\textbf{sufficient} component} of $\mathcal{Y}$ about the relation $\theta$, which is identified by indexing through $\theta$ from $\mathcal{X}$, with its \emph{remaining component}, denoted by $\mathcal{Y}-\mathcal{Y}^{\theta}$, being independent of $\theta$. If this relationship possesses a causal nature in the context of modeling, such defined $\mathcal{Y}^{\vartheta}$ aligns with the concept of \emph{causal representation} raised by \cite{scholkopf2021toward}.

% \%---------------

% More critically, the established latent space representations of $\vartheta_1$ and $\vartheta_2$ are \textbf{\emph{generalizable}} across different applications of AI models, enabling a form of ``knowledge accumulation'' akin to human learning. 

% Subsequently, refining $\vartheta_1$ (and possibly $\vartheta_2$) also entails applying it within various scenarios and data sets that present diverse conditions.
% In this way, causal knowledge across various fields can be progressively compiled into relation-first representations, harnessing the capabilities of AI effectively.




\subsection{Hierarchical Levels by $\omega$}
\label{subsec:unobs_relt}
\vspace{-0.5mm}

As illustrated in Figure \ref{fig:macro}, the ``causal emergence'' phenomenon stems from adopting different perspectives, not truly integrating new relational information. 
We employ the terms ``micro-causal'' and ``macro-causal'' to identify the new information integration, defining the \emph{generalization} process (as per Definition 4), and its inverse is termed \emph{individualization}. 
In modeling, the \textbf{\emph{generalizability}} of an established micro-causal model $f(;\vartheta)$ is its ability to be reused in macro-causality without diminishing $\mathcal{I}(\vartheta)$'s representation.


%As explained, the phenomenon of ``emerged information from micro- to macro-causality'' results from using different perspectives to understand the outcome $\mathcal{Y}$, where $\mathcal{I}(do(Y))$ appears to be more informative than $\mathcal{I}(Y_1) + \mathcal{I}(Y_2) = \mathcal{I}(Y)$. In our context, by explicitly adopting the creator's perspective that enables the \emph{relation-first} principle, relational information $\mathcal{I}(\theta) = \mathcal{I}(\theta_1) + \mathcal{I}(\theta_2)$ features informatively separable components, regardless of $\mathcal{Y}$'s dynamical significance. 

%For example, for causality $\mathcal{X}\xrightarrow{\vartheta}\mathcal{Y}$, the new relation could be $\mathcal{X}\xrightarrow{\omega}\mathcal{Z}$ that indicates an additional component from the same $\mathcal{X}$, or can either be $\mathcal{Z}\xrightarrow{\omega}\mathcal{Y}$, enabling a causal structure with two causes $\mathcal{X}$ and $\mathcal{Z}$. 
The information gained from $\mathcal{I}(\vartheta)$ to $\mathcal{I}(\vartheta,\omega)$ often introduces a new \textbf{\emph{hierarchical level}} of relation, thereby raising generalizability requirements for causal models. This may suggest new observables, potentially as new causes or outcome components, or both.
Let's consider a \emph{logically} causal relationship (without such significance in modeling) as a simple example: Family incomes $X$ affecting grocery shopping frequencies $Y$, represented as $X\xrightarrow{\theta}Y$, where $\theta$ may vary internationally due to cultural differences $\omega$, creating two levels: a global-level $\theta$ and a country-level $(\theta\mid\omega)$. While $\omega$ isn't a direct modeling target, it's an essential condition, necessitating the total information $\mathcal{I}(\theta,\omega) = \mathcal{I}(\theta\mid\omega) + \mathcal{I}(\omega)$.
From the observer's perspective, it equates to incorporating an additional observable, like country $Z$, as a new cause to affect $Y$ with $X$ jointly.

% Figure environment removed


Addressing hierarchies within knowledge is a common issue in relationship modeling, but timing distributional hierarchies present significant challenges to traditional methods, leading to the development of a specialized ``group-specific learning'' \cite{fuller2007developing}, which primarily depends on manual identifications. However, this approach is no longer viable in modern AI-based applications, necessitating the adoption of the \emph{relation-first} modeling paradigm. Below, we present two examples to demonstrate this necessity: one is solely observational, and the other involves a causality with timing hierarchy.
%to handle causal hierarchies effectively.
%One is solely observational learning from computer vision, and the other pertains to clinical studies involving causal hierarchy.

\vspace{2mm}
{\vskip 0pt\bf Observational Hierarchy Example} 
\vspace{-0.5mm}


The AI-created personas on social media can have realistic faces but seldom showcase hands, since AI struggles with the intricate structure of hands, instead treating them as arbitrary assortments of finger-like items.
Figure \ref{fig:hand}(a) shows AI-created hands with faithful color but unrealistic shapes, while humans can effortlessly discern hand gestures from the grayscale sketches in (b).

Human cognition intuitively employs informative relations as the \emph{indices} to visit mental representations \cite{sep-mental-representation}. 
As in (b), this process operates hierarchically, where each higher-level understanding builds upon conclusions drawn at preceding levels. Specifically, Level $\mathbf{I}$ identifies individual fingers; Level $\mathbf{II}$ distinguishes gestures based on the positions of the identified fingers, incorporating additional information from our understanding of how fingers are arranged to constitute a hand, denoted by $\omega_i$; and Level $\mathbf{III}$ grasps the meanings of these gestures from memory, given additional information $\omega_{ii}$ from knowledge.


%To AI, or hypothetical extraterrestrial life unfamiliar with our knowledge, hands in Figure \ref{fig:hand}(a) may appear reasonable.
% AI can successfully differentiate non-overlapping features at various levels. 
%, while similar hand gestures may confuse it. 

Conversely, AI models often do not distinguish the levels of relational information, instead modeling overall as in a relationship $X\xrightarrow{\theta}Y$ with $\theta=(\theta_i, \theta_{ii}, \theta_{iii})$, resulting a lack of informative insights into $\omega$.
However, the hidden information $\mathcal{I}(\omega)$ may not always be essential. For example, AI can generate convincing faces because the appearance of eyes $\theta_i$ strongly indicates the facial angles $\theta_{ii}$, i.e., $\mathcal{I}(\theta_{ii})=\mathcal{I}(\theta_i)$ indicating $\mathcal{I}(\omega_i)=0$, removing the need to distinguish eyes from faces. 

On the other hand, given that $X$ has been fully observed, AI can inversely deduce the relational information using methods such as reinforcement learning \cite{sutton2018reinforcement, arora2021survey}. In this particular case, when AI receives approval for generating hands with five fingers, it may autonomously begin to derive $\mathcal{I}(\theta_i)$. However, when such hierarchies occur on the timing dimension of a dynamically significant $\mathcal{Y}$, they can hardly be autonomously identified, regardless of whether AI techniques are leveraged.

\vspace{2mm}
{\vskip 0pt\bf Timing Hierarchy in Causality Example} 
%\vspace{-0.5mm}


% Figure environment removed

In Figure~\ref{fig:eff}, $\mathcal{B}_{\omega}$ represents the observational sequence $B^t=(B_1,\ldots,B_{30})$ from a group of patients identified by $\omega$. Clinical studies typically aim to estimate the average effect (generalized-level I) on a predetermined day, like $B_{t+30} = f(do(A_t))$. However, our inquiry is indeed the complete level I dynamic $\mathcal{B}_o=\int_{t=1}^{30}do(B_t)dt$, which describes the trend of effect changing over time, without anchored timestamps.
To eliminate the level II dynamic from data, a ``hidden confounder'' is usually introduced to represent their unobserved personal characteristics. 
Let us denote it by $E$, and assume $E$ linearly impact $\mathcal{B}_o$, making the level II dynamic $\mathcal{B}_{\omega}-\mathcal{B}_o$ simply signifying their individualized progress speeds for the same effect $\mathcal{B}_o$.

To accurately represent $\mathcal{B}_o$ with a sequential outcome, traditional methods necessitate an intentional selection or adjustment of training data. This is to ensure the ``influence of $E$'' is eliminated from the data, even unavoidable when adopting RNN models.
%This is essentially equal to manually figuring out the boundary of $\mathcal{B}_o$ by exploring all possible ${\omega}$ values.
%On the other hand, even when adopting an RNN model equipped with a sequential outcome $B^t$, it still presents a sequence of static outcomes without extracting dynamic $\mathcal{B}$, 
In RNNs, the dynamically significant representation is facilitated only on $do(A)$, while the sequential outcome $B^t$ still requires predetermined timestamps. However, once $t$ is specified for all patients without the data selection - for example, let $t=30$ to snapshot $B_{30}$ - bias is inherently introduced, since $B_{30}$ represents the different magnitude of effect $\mathcal{B}_o$ for various patients.


Such hierarchical dynamic outcomes are prevalent in many fields, such as epidemic progression, economic fluctuations, and strategic decision-making. Causal inference typically requires intentional data preprocessing to mitigate inherent biases, including approaches like PSM \cite{benedetto2018statistical} and backdoor adjustment \cite{pearl2009causal}, essentially to identify the targeted levels manually. 
However, they have become impractical due to the modern data volume, and also pose a risk of significant information loss snowballing in structuralized relationship modeling. On the other hand, the significance of timing hierarchies has prompted the development of neural network-based solutions in fields like anomaly detection \cite{wu2018hierarchical} to address specific concerns without the intention of establishing a causal modeling framework.


% Figure environment removed
\vspace{-1mm}

The concept of ``hidden confounder'' is essentially elusive, acting more as an interpretational compensation rather than a constructive effort to enhance the model. 
For example, Figure~\ref{fig:hidden} (a) shows the conventional causal DAG with hidden $E$ depicted. 
Although the ``personal characteristics'' are signified, it is not required to be revealed by collecting additional data. This leads to an illogical implication: ``Our model is biased due to some unknown factors we donât intend to know.'' 
Indeed, this strategy employs a hidden observable to account for the omitted timing-dimensional nonlinearities in statistical models.


As illustrated in Figure~\ref{fig:hidden}(b), the associative causal variable $do(A) * E$ remains unknown, unable to form a modelable relationship.
On the other hand, \emph{relation-first} modeling approaches only require an observed identifier to index the targeted level in representation extractions, like the patient ID denoted by $\omega$. 
%As demonstrated in section (c), this method effectively disentangles effect representations hierarchically in the latent space, thereby achieving model generalizability.

\vspace{-1mm}
\subsection{The Generalizability Challenge across Multiple Timelines in $\mathbb{R}^T$}
\label{subsec:framework_time}
\vspace{-1mm}

% The so-called ``possible world'' represents a conceptual space for storing our structural causal knowledge. It originates from our instinctive ``what if'' thinking, and the construction is supported by possible timings (i.e., the relative timing) in our cognition \cite{wulf1994reducing}. Such timing becomes necessary when we envision a possible future along a specific causal relationship \cite{shea2001effects}; thus, multiple possibilities imply multiple possible timings in causal modeling \cite{coulson2009understanding}. For clarity, we refer to these as timelines within micro-causality, i.e., when they emerge from a single cause. 


% For instance, patients' vital signs are recorded daily in a hospital with chronological timestamps, which indicate their absolute timing. To assess the medical intervention $\mathcal{Y}$, a uniform series of post-medication events must be selected, spanning 30 days from the day after medication. They are considered as distribution along an $t$-axis, i.e., the timeline, marked as $[1,30]$ to represent their relative timing. Suppose the intervention involves two components, such as the primary effect $\mathcal{Y}_1$ and the side effect $\mathcal{Y}_2$, whose mutual influences are of interest. Then, two separate timelines, $t_1$ and $t_2$, must be established to capture their individual dynamics, enabling two-dimensional timing computation, even though they both represent the $[1,30]$ relative timing.

% For a micro-causality $\mathcal{X}\xrightarrow{\vartheta}\mathcal{Y}$, all necessary timelines for the dynamic outcome $\mathcal{Y}$ together formulate the possible timing space $\mathbb{R}^T$. As highlighted in Theorem 3, computations for $\vartheta$ must be sequential within space $\mathbb{R}^T$, taking into account the potential dynamical interactions between these dimensions, to form a multi-round causal structure detection. Otherwise, $\mathbb{R}^T$ ceases to function as a multidimensional space and collapses into a one-dimensional relative timing, implying the absence of dynamical interactions between the outcome components. As a result, the deduced causal models will lose their significance in representing structural causality and lack model generalizability.



% Conventionally, the concept of ``temporal dimension'' is often simplified to be the single absolute timing $\mathbf{t}$, evident from the traditional ``spatial-temporal'' analysis \cite{alkon1988spatial, turner1990spatial, andrienko2003exploratory}, to recent advancements in language models \cite{gurnee2023language}. 
% However, as emphasized in Remark 2, our cognitive perception of ``time'' is more complex, fundamentally enabling our causal reasoning \cite{coulson2009understanding}.

% For an intuitive insight into the implications of neglecting relative timings in $\mathbb{R}^T$, let's consider an analogy:
% Imagine ants dwelling on a floor's two-dimensional plane. To predict risks, the scientists among them create two-dimensional models and instinctively adopt the nearest tree as a height reference. They noticed increased disruptions at the tree's first branch, which indeed correlates to the children's heights, given their curiosity.
% However, without understanding humans as three-dimensional beings, they can only interpret it by adhering to the first branch.
% One day, after relocating to another tree with a lower height, the ants found the risk presenting at the second branch instead, making their model ineffective. They may conclude that human behaviors are too complex, highlighting the model generalizability issue.
% %Similarly, when we specify a single, absolute timeline for all potential events, this timeline becomes our ``tree'', which may introduce inherent modeling biases, affecting the robustness and generalizability of our models.

% As three-dimensional beings, we inherently lack the capacity to fully integrate the fourth dimension - time - into visual perception. Instead, we conceptualize ``space'' in three dimensions to incorporate features of the temporal dimension along a \emph{timeline} within the space, analogous to our ``tree''.
% Yet, ants do not need to fully comprehend the three-dimensional world to build a generalizable model; instead, they need only recognize the ``forest'' out of their vision (i.e., counterfactual), which consists of all ``possible trees'' with \emph{relatively} different branch locations. 
% Similarly, in our modeling, we must include the $\mathbb{R}^T$ space, composed of all potential relative timings within our causal knowledge, although they cannot be directly observed.

% Addressing the counterfactual query ``what effect would be if the cause were changed'' differentiates causality from mere correlations \cite{scholkopf2021toward}. In the proposed framework, counterfactuals are more intuitively interpreted through distributions, potentially offering valuable insights in fields like quantum computing.
% Specifically, the observed prior conditions $\mathcal{X}$ can be considered as features in $\mathbb{R}^O$, whose effects $\mathcal{Y}$ act 
% as a conditional distribution within $\mathbb{R}^{O-1}\cup \mathbb{R}^T$, incorporating $T$ possible observed timings in the future. %This perspective might potentially offer valuable insights in fields like quantum computing.

% %- It might offer an intriguing insight into quantum computing (need further exploration).
% %It might provide an intriguing insight that could potentially inspire advances in quantum computing (further exploration required).

From the creator's perspective, timelines in the macro-level possible timing space $\mathbb{R}^T$ may pertain to different micro-causalities, implying ``structuralized'' causal relationships. This poses a significant generalizability challenge for traditional structural causal models (SCMs).

The example in Figure~\ref{fig:3d} showcases a practical scenario in a clinical study. This 3D causal DAG includes two timelines, $\tau_{\theta}$ and $\tau_{\omega}$, with the $x$-axis categorically arranging observables. The upgrades to causal DAGs, as applied in Figure \ref{fig:interact}, are also adopted here, ensuring that the lengths of the arrows reflect the timespan required to achieve the state values represented by the observable nodes.
Here, the nodes marked in uppercase letters indicate the values representing the mean effects of the current data population, i.e., the group of patients under analysis. Accordingly, the lengths of the arrows indicate their mean timespans.


We use $\Delta \tau_{\theta}$ and $\Delta \tau_{\omega}$ to signify the time steps (i.e., the unit timespans) on $\tau_{\theta}$ and $\tau_{\omega}$, respectively. Considering the triangle $SA'B'$, when each unit of effect is delivered from $S$ to $A'$ (taking $\Delta \tau_{\omega}$), it immediately starts impacting $B'$ through $\overrightarrow{A'B'}$ (with $\Delta \tau_{\theta}$ required); simultaneously, the next unit of effect begins its generation at $S$. 
Under the \emph{relation-first} principle, this dual action requires a two-step modeling process to sequentially extract the dynamic representations on $\tau_{\theta}$ and $\tau_{\omega}$.
However, in traditional SCM, it is represented by the edge $\overrightarrow{SB'}$ with a priorly specified timespan from $S$ to $B'$.
This inherently sets the $\Delta \tau_{\theta}:\Delta \tau_{\omega}$ ratio based on the current population's performance, freezing the state value represented by $B'$ and fixing the geometrical shape of the ${ASB'}$ triangle in this space. 


% Figure environment removed

The lack of model generalizability manifests in various ways, depending on the intended scale of generalization. For instance, when focusing on a finer micro-scale causality, the SCM that describes the mean effects for the current population cannot be tailored to individual patients within this population. Conversely, aiming to generalize this SCM to accommodate other populations, or a broader macro-scale causality, may lead to failure because the preset $\Delta \tau_{\theta}:\Delta \tau_{\omega}$ ratio lacks universal applicability.

%\vspace{2mm}
\subsection{Fundamental Reliance on Assumptions under Object-First}
\label{subsec:appl_assumpt}
\vspace{-1mm}



% Figure environment removed
%\vspace{-2mm}



Figure~\ref{fig:view} categorizes the current causal model applications based on two aspects: 1) if the structure of $\theta/\vartheta$ is known a priori, they are used for structural causation buildup or causal discovery; 2) depending on whether the required outcome is dynamically significant, they can either accurately represent true causality or not.

Under the conventional modeling paradigm, capturing the significant dynamics within causal outcomes autonomously is challenging. When building causal models based on given prior knowledge, the omitted dynamics become readily apparent. If these dynamics can be specifically attributed to certain unobserved observables, like the node $E$ in Figure \ref{fig:hidden}(a), such information loss is attributed to a hidden confounder. Otherwise, they might be overlooked due to the \emph{causal sufficiency} assumption, which presumes that all potential confounders have been observed within the system. Typical examples of approaches susceptible to these issues are structural equation models (SEMs) and functional causal models (FCMs) \cite{glymour2019review, elwert2013graphical}.
Although state-of-the-art deep learning applications have effectively transformed the discrete structural constraint into continuous optimizations \cite{zheng2018dags, zheng2020learning, lachapelle2019gradient}, issues of lack of generalizability still hold \cite{scholkopf2021toward,luo2020causal, ma2018using}.

On the other hand, causal discovery primarily operates within the $\mathbb{R}^O$ space and is incapable of detecting dynamically significant causal outcomes. If the interconnection of observables can be accurately specified as the functional parameter $\theta$, there remains a chance to discover informative correlations. Otherwise, mere conditional dependencies among observables are unreliable for causal reasoning, as seen in Bayesian networks \cite{pearl2000models, peters2014causal}. Typically, undetected dynamics are overlooked due to the \emph{Causal Faithfulness} assumption, which suggests that the observables can fully represent the underlying causal reality.


Furthermore, the causal directions suggested by the results of causal discovery often lack logical causal implications. Consider $X$ and $Y$ in the optional models $Y=f(X;\theta)$ and $X=g(Y;\phi)$, with predetermined parameters, which indicate opposite directions. Typically, the direction $X\rightarrow Y$ would be favored if $\mathcal{L}(\hat{\theta}) > \mathcal{L}(\hat{\phi})$. Let $\mathcal{I}_{X,Y}(\theta)$ denote the information about $\theta$ given $\mathbf{P}(X,Y)$. Using $p(\cdot)$ as the density function, the integral $\int_X p(x;\theta)dx$ remains constant in this context. Then:

\vspace{-6mm}
\begin{align*}
    \mathcal{I}_{X,Y}(\theta) &= \mathbb{E}[(\frac{\partial }{\partial \theta} \log p(X,Y;\theta))^2 \mid \theta] 
    = \int_{Y} \int_X (\frac{\partial }{\partial \theta} \log p(x,y;\theta))^2 p(x,y;\theta) dx dy \\
    &= \alpha \int_Y (\frac{\partial }{\partial \theta} \log p(y;x,\theta))^2 p(y;x,\theta) dy + \beta = \alpha \mathcal{I}_{Y\mid X}(\theta)+\beta, \text{with } \alpha,\beta \text{ being constants.} \\
    \text{Then, } \hat{\theta} &=\argmax\limits_{\theta} \mathbf{P}(Y\mid X,\theta) = \argmin\limits_{\theta} \mathcal{I}_{Y\mid X} (\theta) =\argmin\limits_{\theta} \mathcal{I}_{X,Y}(\theta), \text{ and } \mathcal{L}(\hat{\theta})  \propto 1/\mathcal{I}_{X,Y}(\hat{\theta}).
\end{align*}
\vspace{-5mm}

%The likelihoods of $\hat{\theta}$ and $\hat{\omega}$ rely on the information $\mathcal{I}(\hat{\theta})$ and $\mathcal{I}(\hat{\omega})$. 
The inferred directionality indicates how informatively the observational data distribution can reflect the two predetermined parameters.
Consequently, such directionality is unnecessarily logically meaningful but could be dominated by the data collection process, with the predominant entity deemed the ``cause'', consistent with other existing conclusions \cite{reisach2021beware, kaiser2021unsuitability}.
%rather indicative of distributional dominance as determined by the data collection process. Here, the predominant entity is labeled the ``cause''.
%Even when $\theta$ and $\phi$ are predetermined based on knowledge, they might not provide insights for dynamically significant causal relations.

%they only link to solely observational features of entities, and thus may not accurately reflect true causal relations.
%in the presence of dynamically significant effects.

%their designated distributions appear in the data, %with the predominant one deemed the ``cause'' - It assumes, by default, that observations capture the cause more thoroughly than the effect. While limited data collection techniques made it reasonable in the past, it is no longer safe to assume such observationally inferred directions to hold logical meaning for causality.
    


%provides formulations of factorizations, to achieve of achieving hierarchical disentanglement through relation-indexing

%\vspace{-2mm}
\section{Relation-Indexed Representation Learning (RIRL)}
\label{sec:representation}

This section introduces a method for realizing the proposed \emph{relation-first} paradigm, referred to as RIRL for brevity. Unlike existing causal representation learning, which is primarily confined to the micro-causal scale, RIRL focuses on facilitating \emph{structural causal dynamics exploration} in the latent space. 

Specifically, ``relation-indexed'' refers to its micro-causal realization approach, guided by the \emph{relation-first} principle, where the indexed representations are capable of capturing the dynamic features of causal outcomes across their timing-dimensional distributions. Furthermore, from a macro-causal viewpoint, the extracted representations naturally possess high generalizability, ready to be reused and adapted to various practical conditions. 
This advancement is evident in the structural exploration process within the latent space.

Unlike traditional causal discovery, RIRL exploration spans $\mathbb{R}^{O-1}\cup\mathbb{R}^T$ to detect causally significant dynamics without concerns about ``hidden confounders'', where $\mathbb{R}^T$ encompasses all possibilities of the potential causal structure. The representations obtained in each round of RIRL detection serve as elementary units for reuse, enhancing the flexibility of structural models. This exploration process eventually yields DAG-structured graphical indices, with each input-output pair representing a specific causal routine, readily accessible.

Subsequently, section \ref{subsec:repr_autoencoder} delves into the micro-causal realization to discuss the technical challenges and their resolutions, including the architecture and core layer designs. Section \ref{subsec:RIRL_stacking} introduces the process of ``stacking'' relation-indexed representations in the latent space, to achieve hierarchical disentanglement at an effect node in DAG. Finally, section \ref{subsec:RIRL_discover} demonstrates the exploration algorithm from a macro-causal viewpoint.


\subsection{Micro-Causal Architecture}
\label{subsec:repr_autoencoder}
%\vspace{-1mm}

For a relationship $\mathcal{X}\xrightarrow{\theta}\mathcal{Y}$ given sequential observations $\{x^t\}$ and $\{y^{\tau}\}$, with $|\overrightarrow{x}|= n$ and $|\overrightarrow{y}|= m$, the relation-indexed representation aims to establish $(\mathcal{X},\theta,\mathcal{Y})$ in the latent space $\mathbb{R}^L$.
Firstly, an \emph{initialization} is needed for $\mathcal{X}$ and $\mathcal{Y}$ individually, to construct their latent space representations from observed data sequences.
For clarity, we use 
$\mathcal{H} \in \mathbb{R}^L$ and $\mathcal{V} \in \mathbb{R}^L$ to refer to the latent representations of $\mathcal{X}\in \mathbb{R}^O$ and $\mathcal{Y}\in \mathbb{R}^O$, respectively.
The neural network optimization to derive $\theta$ is a procedure between $\mathcal{H}$ as input and $\mathcal{V}$ as output. 
In each iteration,
$\mathcal{H}$, $\theta$, and $\mathcal{V}$ are sequentially refined in three steps, until the distance between $\mathcal{H}$ and $\mathcal{V}$ is minimized within $\mathbb{R}^L$, without losing their representations for $\mathcal{X}$ and $\mathcal{Y}$.
Consider instances $x$ and $y$ of $\mathcal{X}$ and $\mathcal{Y}$ that are represented by $h$ and $v$ correspondingly in $\mathbb{R}^L$, as in Figure \ref{fig:bridge}. The latent dependency $\mathbf{P}(v| h)$ represents the relational function $f(;\theta)$.
The three optimization steps are as follows:
\begin{enumerate}[topsep=0pt, partopsep=0pt]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{1pt}
    \item {Optimizing the cause-encoder by $\mathbf{P}(h|x)$, the relation model by $\mathbf{P}(v|h)$, and the effect-decoder by $\mathbf{P}(y|v)$ to reconstruct the relationship $x\rightarrow y$, represented as $h\rightarrow v$ in $\mathbb{R}^L$.}
    \item {Fine-tuning the effect-encoder $\mathbf{P}(v|y)$ and effect-decoder $\mathbf{P}(y|v)$ to accurately represent $y$.} 
    \item {Fine-tuning the cause-encoder $\mathbf{P}(h|x)$ and cause-decoder $\mathbf{P}(x|h)$ to accurately represent $x$.}
\end{enumerate}
\vspace{1mm}

In this process, $h$ and $v$ are iteratively adjusted to reduce their distance in $\mathbb{R}^{L}$, with $\theta$ serving as a bridge to span this distance and guiding the output to fulfill the associated representation $( \mathcal{H},\theta,\mathcal{V})$. From the perspective of the effect node $\mathcal{Y}$, this tuple represents its component indexing through $\theta$, denoted as $\mathcal{Y}_{\theta}$.
%Within this system, for each effect, a series of such relation functions $\{f(;\theta)\}$ is maintained, indexing diverse levels of causal inputs for sequentially building the structural model.

However, it introduces a technical challenge: for a micro-causality $\theta$, the dimensionality $L$ of the latent space must satisfy $L \geq rank(\mathcal{X},\theta,\mathcal{Y})$ to provide adequate freedom for computations. To accommodate a structural DAG, this lower boundary can be further enhanced, to be certainly larger than the input vector length $|\overrightarrow{\mathcal{X}}| = t*n$. This necessitates a specialized autoencoder to realize a ``higher-dimensional representation'', where the accuracy of its reconstruction process becomes significant, and essentially requires \emph{invertibility}. 

%Autoencoders are commonly used for dimensionality reduction, especially in structural modeling that involves multiple variables \cite{wang2016auto}.
%, the column-augmented original data matrix often possesses a dimensionality exceeding that of the latent space $\mathbb{R}^L$. 
%In contrast, RIRL aims to model individual causal relationships sequentially within a higher-dimensional latent space $\mathbb{R}^L$, as to hierarchically construct the entire causal structure. 

% Figure environment removed

Figure~\ref{fig:arch} illustrates the designed autoencoder architecture, featured by a pair of symmetrical layers, named \emph{Expander} and \emph{Reducer} (source code is available \footnote{https://github.com/kflijia/bijective\_crossing\_functions/blob/main/code\_bicross\_extracter.py}).
The Expander magnifies the input vector by capturing its higher-order associative features, while the Reducer symmetrically diminishes dimensionality and reverts to its initial formation. 
For example, the Expander showcased in Figure~\ref{fig:arch} implements a \emph{double-wise} expansion. Every duo of digits from $\overrightarrow{\mathcal{X}}$ is encoded into a new digit by associating with a random constant, termed the \emph{Key}. This \emph{Key} is generated by the encoder and replicated by the decoder. Such pairwise processing of $\overrightarrow{\mathcal{X}}$ expands its length from $(t*n)$ to be $(t*n-1)^2$. By concatenating the expanded vectors using multiple \emph{Keys}, $\overrightarrow{\mathcal{X}}$ can be considerably expanded, ready for the subsequent reduction through a regular encoder.

The four blue squares in Figure~\ref{fig:arch} with unique grid patterns signify the resultant vectors of the four distinct \emph{Keys}, with each square symbolizing a $(t*n - 1)^2$ length vector. Similarly, higher-order expansions, such as \emph{triple-wise} across three digits, can be chosen with adapted \emph{Keys} to achieve more precise reconstructions.

% The four differently patterned blue squares represent the vectors expanded by four distinct \emph{Keys}, with the grid patterns indicating their ``signatures''. Each square visualizes a $(d * T_x - 1)^2$ length vector (not signifying a 2-dimensional vector).
% In a similar way, higher-order extensions, such as \emph{triple-wise} ones across every three digits, can also be employed by appropriately adapting \emph{Keys}.

% Figure environment removed

Figure~\ref{fig:extractor} illustrates the encoding and decoding processes within the Expander and Reducer, targeting the digit pair $(x_i, x_j)$ for $i\neq j \in 1,\ldots,n$. The Expander function is defined as $\eta_\kappa(x_i,x_j) = x_j \otimes exp(s(x_i)) + t(x_i)$, which hinges on two elementary functions, $s(\cdot)$ and $t(\cdot)$. The parameter $\kappa$ represents the adopted \emph{Key} comprising of their weights $\kappa=(w_s, w_t)$.
Specifically, the Expander morphs $x_j$ into a new digit $y_j$ utilizing $x_i$ as a chosen attribute. In contrast, the Reducer symmetrically performs the inverse function $\eta_\kappa^{-1}$, defined as $(y_j-t(y_i)) \otimes exp(-s(y_i))$. 
This approach circumvents the need to compute $s^{-1}$ or $t^{-1}$, thereby allowing more flexibility for nonlinear transformations through $s(\cdot)$ and $t(\cdot)$. This is inspired by the groundbreaking work in \cite{dinh2016density} on invertible neural network layers employing bijective functions.



\subsection{Stacking Relation-Indexed Representations}
\label{subsec:RIRL_stacking}
%\vspace{-1mm}
% \noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.94\textwidth}
% \paragraph{Remark 2.} Given a causal graph $G$ with data matrix $\mathbf{X}$ column-augmented by all nodes' attributes, the latent space dimensionality $L$ must satisfy $L\ge rank(\mathbf{X})$ to adequately represent $G$.
% \vspace{-0.5mm}
% \end{minipage}}


In each round of detection during the macro-causal exploration, a micro-causal relationship will be selected for establishment. Nonetheless, the cause node in it may have been the effect node in preceding relations, e.g., the component $\mathcal{Y}_{\theta}$ may already exist at $\mathcal{Y}$ when $\mathcal{Y}\rightarrow\mathcal{Z}$ is going to be established. 
This process of conditional representation buildup is referred to as ``stacking''. 

For a specific node $\mathcal{X}$, the stacking processes, where it serves as the effect, sequentially construct its hierarchical disentanglement according to the DAG. It requires the latent space dimensionality to be larger than $rank(X)+T$, where $T$ represents the in-degree of node $\mathcal{X}$ in this DAG, as well as its number of components as the dynamic effects.
From a macro-causal perspective, $T$ can be viewed as the number of necessary edges in a DAG. While to fit it into $\mathbb{R}^L$, a predetermined $L$ must satisfy $L > rank(\mathbf{X})+T$, where $\mathbf{X}$ represents the data matrix encompassing all observables.
In this study, we bypass further discussions on dimensionality boundaries by assuming $L$ is large enough for exploration, and empirically determine $L$ for the experiments. %We gradually reduce $L$ for the experiments to determine an empirically proper value.

%Since the space $\mathbb{R}^L$ learned by the autoencoder is spanned by the top principal components of $\mathbf{X}$ \cite{baldi1989neural, plaut2018principal, wang2016auto}, possibly reducing $L$ below $rank(\mathbf{X})$ yields a less adequate but more causally significant DAG  latent space through better alignment of dimensions \cite{jain2021mechanism} (Further exploration in this direction is warranted). 
%In this study, we will set aside discussions on the boundaries of dimensionality. Our experiments feature 10 variables with dimensions 1 to 5 (Table~\ref{tab:tower}), and we empirically fine-tune and reduce $L$ from 64 to 16.


%\vspace{-2.7mm}
% Figure environment removed


%Consider a causal structural among $\{\mathcal{X}, \mathcal{Y}, \mathcal{Z}\}$, with their corresponding representations $\{\mathcal{H}, \mathcal{V}, \mathcal{K}\} \in \mathbb{R}^L$ initialized by three autoencoders, respectively. 
Figure~\ref{fig:stack} illustrates the stacking architectures under two different scenarios within a three-node system $\{\mathcal{X}, \mathcal{Y}, \mathcal{Z}\}$. 
In this figure, the established relationship $\mathcal{X}\rightarrow\mathcal{Y}$ is represented by the blue data streams and layers. The scenarios differ in the causal directions between $\mathcal{Y}$ and $\mathcal{Z}$: the left side represents $\mathcal{X}\rightarrow \mathcal{Y}\leftarrow \mathcal{Z}$, while the right side depicts $\mathcal{X}\rightarrow \mathcal{Y}\rightarrow \mathcal{Z}$. 


The hierarchically stacked representations allow for flexible input-output combinations to represent different causal routines as needed. For simple exemplification, we use $\mapsto$ to denote the input and output layers in the stacking architecture. On the left side of Figure~\ref{fig:stack}, $\mathbf{P}(v|h) \mapsto \mathbf{P}(\alpha)$ represents the $\mathcal{X}\rightarrow \mathcal{Y}$ relationship, while $\mathbf{P}(\alpha|k)$ implies $\mathcal{Z}\rightarrow \mathcal{Y}$. Conversely, on the right, $\mathbf{P}(v) \mapsto \mathbf{P}(\beta|k)$ denotes the $\mathcal{Y}\rightarrow \mathcal{Z}$ relationship with $\mathcal{Y}$ as the input. Meanwhile, $\mathbf{P}(v|h) \mapsto \mathbf{P}(\beta|k)$ captures the causal sequence $\mathcal{X} \rightarrow \mathcal{Y}\rightarrow \mathcal{Z}$.


% \subsection{Factorizing the Effect Disentanglement}
% \label{subsec:RIRL_disentangle}

% Consider $\mathcal{Y}=\langle Y, \tau \rangle \in \mathbb{R}^{b+1} \subseteq \mathbb{R}^O$ having a $n$-level hierarchy, with each level built up using a representation function, labeled as $g_i$ for the $i$-th level. For simplicity, here, we use $\omega_i$ to represent the $i$-th level component of $\mathcal{Y}$ in the latent space $\mathbb{R}^L$, while its counterpart in $\mathbb{R}^{b+1}$ is denoted as $\Omega_i$ (i.e., $\mathcal{\hat{Y}}$ at the $i$-th level).
% Let the feature vector $\omega_i$ in $\mathbb{R}^{L}$ primarily spans a sub-dimensional space, $\mathbb{R}^{L_i}$, resulting in the spatial disentanglement sequence $\{\mathbb{R}^{L_1}, \ldots, \mathbb{R}^{L_i},\ldots,\mathbb{R}^{L_n}\}$, which hierarchically represents $\mathcal{Y}$ with $n$ components.
% Function $g_i$ maps from $\mathbb{R}^{b+1}$ to $\mathbb{R}^{L_i}$, taking into account features from all previous levels as attributes.  This gives us:
% \vspace{-4mm}

% \begin{equation}
%     \mathcal{Y} = \sum_{i=1}^{n} \Omega_i, 
%      \text{ where } \Omega_i = g_i \bigl( \omega_i ;\ \Omega_1,\ldots, \Omega_{i-1}\bigr) \text{ with } \Omega_i \in \mathbb{R}^{b+1} \text{ and } \omega_i \in \mathbb{R}^{L_i} \subseteq \mathbb{R}^{L}
% \end{equation}

% \vspace{-3mm}
% %The $t$-th component in the observable data space, denoted as $\Omega_t \in \mathbb{R}^{O}$, is articulated through an observational data sequence with the length of $T_y$, along the absolute timeline $t$.
% %However, in latent space, the objective of $\omega_t$ is to capture dynamics along a relative timeline, $t_i$, which is autonomously determined by the relation at the $i$-th level, not bound by the observational timestamps in $\mathbb{R}^{O}$. 


% % $\{t_1,\ldots,t_i,\ldots,t_n\}$, each uniquely determined by the relationship at their respective levels, apart from the absolute timeline $t$.
% % While in the \emph{observable data space}, the $i$-th level feature, represented as the sum $\Omega_1 +\ldots +\Omega_i$, still maintains its timestamp attribute along $t$.

% In the context of a purely observational hierarchy, with $\mathcal{Y}$ substituted by $Y \in \mathbb{R}^b$, The example depicted in Figure~\ref{fig:hand} (b) can be interpreted as follows: Consider three feature levels represented as $\omega_1\in \mathbb{R}^{L_1}$, $\omega_2\in \mathbb{R}^{L_2}$, and $\omega_3\in \mathbb{R}^{L_3}$. For simplicity, assume the subspaces are mutually exclusive, such that $L=L_1+L_2+L_3$. In the latent space, the triplet $(\omega_1, \omega_2, \omega_3) \in \mathbb{R}^{L}$ comprehensively depicts the image. Their observable counterparts, $\Omega_1$, $\Omega_2$, and $\Omega_3$, are three distinct full-scale images, each showcasing different content. For example, $\Omega_1$ emphasizes finger details, while the combination $\Omega_1+\Omega_2$ reveals the entire hand.


%\vspace{-2mm}
\subsection{Exploration Algorithm in the Latent Space}
\label{subsec:RIRL_discover}
%\vspace{-1mm}

\begin{minipage}{\textwidth}
    \begin{minipage}[b]{0.56\textwidth}
    \raggedright
    \begin{algorithm}[H]
    \footnotesize
    \SetAlgoLined
    \KwResult{ ordered edges set $\mathbf{E}=\{e_1, \ldots,e_n\}$ }
    $\mathbf{E}=\{\}$ ; $N_R = \{ n_0 \mid n_0 \in N, Parent(n_0)=\varnothing\}$ \;
    \While{$N_R \subset N$}{
    $\Delta = \{\}$ \;
    \For{ $n \in N$ }{
        \For{$p \in Parent(n)$}{
            \If{$n \notin N_R$ and $p \in N_R$}{
                $e=(p,n)$; \\ 
                $\beta = \{\}$;\\ 
                \For{$r \in N_R$}{
                    \If{$r \in Parent(n)$ and $r \neq p$}{
                        $\beta = \beta \cup r$}
                    }
                $\delta_e = K(\beta \cup p, n) - K(\beta, n)$;\\
                $\Delta = \Delta \cup \delta_e$;
            }
        }
    }
    $\sigma = argmin_e(\delta_e \mid \delta_e \in \Delta)$;\\
    $\mathbf{E} = \mathbf{E} \cup \sigma$; \ 
    $N_R = N_R \cup n_{\sigma}$;\\
    }
     \caption{RIRL Exploration}
    \end{algorithm}
    \end{minipage}
%\hfill 
    \begin{minipage}[b]{0.35\textwidth}
    \raggedleft
        \label{tab:table1}
        \begin{tabular}{|l|l|}
        \hline
           $G=(N,E)$ & graph $G$ consists of $N$ and $E$ \\
           $N$ & the set of nodes\\
           $E$ & the set of edges\\
           $N_R$ & the set of reachable nodes\\
           $\mathbf{E}$ & the list of discovered edges\\
           $K(\beta, n)$ & KLD metric of effect $\beta\rightarrow n$\\
           $\beta$ & the cause nodes \\
           $n$ & the effect node\\
           $\delta_e$ & KLD Gain of candidate edge $e$\\
           $\Delta=\{\delta_e\}$ & the set $\{\delta_e\}$ for $e$ \\
           $n$,$p$,$r$ & notations of nodes\\
           $e$,$\sigma$ & notations of edges\\
          \hline
        \end{tabular} %
%       \captionof{table}{A table beside a figure}
    \end{minipage}
\end{minipage}
\vspace{1mm}

Algorithm 1 outlines the heuristic exploration procedure among the initialized representations of nodes. We employ the Kullback-Leibler Divergence (KLD) as the optimization criterion to evaluate the similarity between outputs, such as the relational $\mathbf{P}(v|h)$ and the prior $\mathbf{P}(v)$. A lower KLD value indicates a stronger causal strength between the two nodes. Additionally, we adopt the Mean Squared Error (MSE) as another measure of accuracy. Considering its sensitivity to data variances \cite{reisach2021beware}, we do not choose MSE as the primary criterion.

%is a frequently used evaluation metric,  leads us to utilize it as a supplementary measure in this study. %In the graphical representation context, we refer to variables $A$ and $B$ in the edge $A\rightarrow B$ as the \emph{cause node} and \emph{effect node}, respectively.

%\vspace{-1mm}
% Figure environment removed

Figure~\ref{fig:discover} completely illustrates a detection round within the latent space that represents $\mathbb{R}^{O-1}\cup\mathbb{R}^T$. A new representation for the selected edge is stacked upon the previously explored causal structure during this process. It contains four primary steps: In Step 1, two edges, $e_1$ and $e_3$, have been selected in previous detection rounds. In Step 2, $e_1$, having been selected, becomes the preceding effect at node $B$ for the next round. In Step 3, with $e_3$ selected in the new round, the candidate edge $e_2$ from $A$ to $C$ must be deleted and rebuilt since $e_3$ alters the conditions at $C$. Step 4 depicts the resultant structure.




%\vspace{-2mm}
\section{RIRL Exploration Experiments}
%\vspace{-3mm}
\label{sec:experiment}

In the experiments, our objective is to evaluate the proposed RIRL method from three perspectives: 1) the performance of the higher-dimensional representation autoencoder, assessed through its reconstruction accuracy; 2) the effectiveness of hierarchical disentanglement for a specific effect node, as determined by the explored causal DAG; 3) the method's ability to accurately identify the underlying DAG structure through exploration. A comprehensive demonstration of the conducted experiments is available online\footnote{https://github.com/kflijia/bijective\_crossing\_functions.git}. However, it is important to highlight two primary limitations of the experiments, which are detailed as follows:

Firstly, as an initial realization of the \emph{relation-first} paradigm, RIRL struggles with modeling efficiency, since it requires a substantial amount of data points for each micro-causal relationship, making the heuristic exploration process slow. The dataset used is generated synthetically, thus providing adequate instances. However, current general-use simulation systems typically employ a single timeline to generate time sequences - It means that interactions of dynamics across multiple timelines cannot be showcased. Ideally, real-world data like clinical records would be preferable for validating the macro-causal model's generalizability. Due to practical constraints, we are unable to access such data for this study and, therefore, designate it as an area for future work. The issues of generalization inherent in such data have been experimentally confirmed in prior work \cite{li2020teaching}, which readers may find informative.

%the dataset employed in this study may not be the most suitable for evaluating the effectiveness of RIRL. Ideally, real-world data featuring rich structuralized causality across multiple relative timings, like clinical records, would be preferable. 
%However, due to practical constraints, access to such optimal data is limited for this study, leading us to use the current synthetic data and focus solely on feasibility verification. For experimental validation regarding the inherent bias, please refer to prior research \cite{li2020teaching}.

Secondly, the time windows for the cause and effect, denoted by $n$ and $m$, were fixed at 10 and 1, respectively. This arose from an initial oversight in the experimental design stage, wherein the pivotal role of dynamic outcomes was not fully recognized, and our vision was limited by the RNN pattern. While the model can adeptly capture single-hop micro-causality, it struggles with multi-hop routines like $\mathcal{X} \rightarrow \mathcal{Y}\rightarrow \mathcal{Z}$, since the dynamics in $\mathcal{Y}$ have been discredited by $m=1$. However, it does not pose a significant technical challenge to expand the time window in future works.



%\vspace{-1mm}
\subsection{Hydrology Dataset}
%\vspace{-1mm}

\vspace{-5mm}
% Figure environment removed


The employed dataset is from a widely-used synthetic resource in the field of hydrology, aimed at enhancing streamflow predictions based on observed environmental conditions such as temperature and precipitation. %The application of RIRL aims to create a streamflow prediction model that is generalizable across various watersheds. While these watersheds share a fundamental hydrological scheme governed by physical rules, they may exhibit unique features due to unobserved conditions such as economic development and land use. Current models based on physical knowledge, however, often lack the flexibility to fully capture multiple levels of dynamical temporal features across these watersheds.
In hydrology, deep learning, particularly RNN models, has gained favor for extracting observational representations and predicting streamflow \cite{goodwell2020debates, kratzert2018rainfall}. 
We focus on a simulation of the Root River Headwater watershed in Southeast Minnesota, covering 60 consecutive virtual years with daily updates. The simulated data is from the Soil and Water Assessment Tool (SWAT), a comprehensive system grounded in physical modules, to generate dynamically significant hydrological time series. %The performance evaluations predominantly focus on the accuracy of the autoencoder reconstructions.

Figure \ref{fig:stream} displays the causal DAG employed by SWAT, complete with node descriptions. The hydrological routines are color-coded based on their contribution to output streamflow: Surface runoff (the 1st tier) significantly impacts rapid streamflow peaks, followed by lateral flow (the 2nd tier); baseflow dynamics (the 3rd tier) have a subtler influence. Our exploration process aims to reveal these underlying tiers. %relationships from the observed data.


\vspace{-1mm}
\subsection{Higher-Dimensional Reconstruction}
%\vspace{-2mm}

This test is based on ten observable nodes, each requiring an individual autoencoder for initialing its higher-dimensional representation. 
Table \ref{tab:tower} lists the characteristics of these observables after being scaled (i.e., normalized), along with their autoencoders' reconstruction accuracies, assessed in the root mean square error (RMSE), where a lower RMSE indicates higher accuracy for both scaled and unscaled data.


The task is challenged by the limited dimensionalities of the ten observables - maxing out at just 5 and the target node, $J$, having just one attribute. To mitigate this, we duplicate the input vector to a consistent 12-length and add 12 dummy variables for months, resulting in a 24-dimensional input. A double-wise extension amplifies this to 576 dimensions, from which a 16-dimensional representation is extracted via the autoencoder.
Another issue is the presence of meaningful zero-values, such as node $D$ (Snowpack in winter), which contributes numerous zeros in other seasons and is closely linked to node $E$ (Soil Water). We tackle this by adding non-zero indicator variables, called \emph{masks}, evaluated via binary cross-entropy (BCE).

Despite challenges, RMSE values ranging from $0.01$ to $0.09$ indicate success, except for node $F$ (the Aquifer). Given that aquifer research is still emerging (i.e., the 3rd tier baseflow routine), it is likely that node $F$ in this synthetic dataset may better represent noise than meaningful data.


\begin{table*}[t]
\caption{Characteristics of observables, and corresponding reconstruction performances.}
\label{tab:tower}
\vspace{-2mm}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{|
>{\columncolor[HTML]{EFEFEF}}c |c|l|l|l|l|l|l|l|l|}
\hline
\cellcolor[HTML]{C0C0C0}Variable & \cellcolor[HTML]{C0C0C0} Dim & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Mean} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Std} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Min} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Max} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Non-Zero Rate\%} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}RMSE on Scaled} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}RMSE on Unscaled} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}BCE of Mask} \\ \hline
A & 5 & 1.8513 & 1.5496 & -3.3557 & 7.6809 & 87.54 & 0.093 & 0.871 & 0.095 \\ \hline
B & 4 & 0.7687 & 1.1353 & -3.3557 & 5.9710 & 64.52 & 0.076 & 0.678 & 1.132 \\ \hline
C & 2 & 1.0342 & 1.0025 & 0.0 & 6.2145 & 94.42 & 0.037 & 0.089 & 0.428 \\ \hline
D & 3 & 0.0458 & 0.2005 & 0.0 & 5.2434 & 11.40 & 0.015 & 0.679 & 0.445 \\ \hline
E & 2 & 3.1449 & 1.0000 & 0.0285 & 5.0916 & 100 & 0.058 & 3.343 & 0.643 \\ \hline
F & 4 & 0.3922 & 0.8962 & 0.0 & 8.6122 & 59.08 & 0.326 & 7.178 & 2.045 \\ \hline
G & 4 & 0.7180 & 1.1064 & 0.0 & 8.2551 & 47.87 & 0.045 & 0.81 & 1.327 \\ \hline
H & 4 & 0.7344 & 1.0193 & 0.0 & 7.6350 & 49.93 & 0.045 & 0.009 & 1.345 \\ \hline
I & 3 & 0.1432 & 0.6137 & 0.0 & 8.3880 & 21.66 & 0.035 & 0.009 & 1.672 \\ \hline
J & 1 & 0.0410 & 0.2000 & 0.0 & 7.8903 & 21.75 & 0.007 & 0.098 & 1.088 \\ \hline
\end{tabular}}
\vspace{-3mm}
\end{table*}

\begin{table*}[t]
%\vspace{-2.5mm}
\caption{The brief results from the RIRL exploration.}
\label{tab:discv}
\vspace{-2mm}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{|c|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
\cellcolor[HTML]{C0C0C0}Edge & \cellcolor[HTML]{FFCCC9}A$\rightarrow$C & \cellcolor[HTML]{FFCCC9}B$\rightarrow$D & \cellcolor[HTML]{FFCCC9}C$\rightarrow$D & \cellcolor[HTML]{FFCCC9}C$\rightarrow$G & \cellcolor[HTML]{FFCCC9}D$\rightarrow$G & \cellcolor[HTML]{FFCCC9}G$\rightarrow$J & \cellcolor[HTML]{FFCCC9}D$\rightarrow$H & \cellcolor[HTML]{FFCCC9}H$\rightarrow$J & \cellcolor[HTML]{FFCE93}B$\rightarrow$E & \cellcolor[HTML]{FFCE93}E$\rightarrow$G & \cellcolor[HTML]{FFCE93}E$\rightarrow$H & \cellcolor[HTML]{FFCE93}C$\rightarrow$E & \cellcolor[HTML]{ABE9E7}E$\rightarrow$F & \cellcolor[HTML]{ABE9E7}F$\rightarrow$I & \cellcolor[HTML]{ABE9E7}I$\rightarrow$J & \cellcolor[HTML]{ABE9E7}D$\rightarrow$I \\ \hline
\cellcolor[HTML]{C0C0C0}KLD & 7.63 & 8.51 & 10.14 & 11.60 & 27.87 & 5.29 & 25.19 & 15.93 & 37.07 & 39.13 & 39.88 & 46.58 & 53.68 & 45.64 & 17.41 & 75.57 \\ \hline
\rowcolor[HTML]{FFFFFF} 
\cellcolor[HTML]{C0C0C0}Gain & 7.63 & 8.51 & 1.135 & 11.60 & 2.454 & 5.29 & 25.19 & 0.209 & 37.07 & -5.91 & -3.29 & 2.677 & 53.68 & 45.64 & 0.028 & 3.384 \\ \hline
\end{tabular}}
\vspace{-4mm}
\end{table*}


%\vspace{1mm}
\subsection{Hierarchical Disentanglement}
%\vspace{-2mm}

Table \ref{tab:unit} provides the performance of stacking relation-indexed representations. For each effect node, the accuracies of its micro-causal relationship reconstructions are listed, including the ones from each single cause node (e.g., $B\rightarrow D$ or $C\rightarrow D$), and also the one from combined causes (e.g., $BC\rightarrow D$). We call them ``single-cause'' and ``full-cause'' for clarity. 
We also list the performances of their initialized variable representations on the left side, to provide a comparative baseline. 
In micro-causal modeling, the effect node has two outputs with different data stream inputs. One is input from its own encoder (as in optimization step 2), and the other is from the cause-encoder, i.e., indexing through the relation (as in optimization step 1). Their performances are arranged in the middle part, and on the right side of this table, respectively. 

% For example, node $G$ has three possible causes $C$, $D$, and $E$, so we built four causal effect models in total: the individual pairwise causations $C\rightarrow G$, $D\rightarrow G$, $E\rightarrow G$, and the stacked causal effect $CDE\rightarrow G$. For convenience, we refer to them as "\emph{pair-relation}" and "\emph{stacking-relation}" respectively. We also listed the initial reconstruction performance for each variable in the column named "Variable Reconstruction (initial performance)" as a comparison baseline in Table \ref{tab:unit}.


%\vspace{-2mm}
% Figure environment removed

The KLD metrics in Table \ref{tab:unit} indicate the strength of learned causality, with a lower value signifying stronger. 
Due to the data including numerous meaningful zeros, we have an additional reconstruction for the binary outcome as ``whether zero or not'', named ``mask'' and evaluated in Binary Cross Entropy (BCE).

For example, node $J$'s minimal KLD values suggest a significant effect caused by nodes $G$ (Surface Runoff), $H$ (Lateral), and $I$ (Baseflow). In contrast, the high KLD values imply that predicting variable $I$ using $D$ and $F$ is challenging. 
For nodes $D$, $E$, and $J$, the ``full-cause'' are moderate compared to their ``single-cause'' scores, suggesting a lack of informative associations among the cause nodes. In contrast, for nodes $G$ and $H$, lower ``full-cause'' KLD values imply capturing meaningful associative effects through hierarchical stacking. The KLD metric also reveals the most contributive cause node to the effect node. For example, the proximity of the $C\rightarrow G$ strength to $CDE\rightarrow G$ suggests that $C$ is the primary contributor to this causal relationship.

Figure~\ref{fig:G} showcases reconstructed timing distributions for the effect nodes $J$, $G$, and $I$ in the same synthetic year to provide a straightforward overview of the hierarchical disentanglement performances. 
Here, black dots represent the ground truth; the blue line indicates the initialized variable representation and the ``full-cause'' representation generates the red line. 
In addition to RMSE, we also employ the NashâSutcliffe model efficiency coefficient (NSE) as an accuracy metric, commonly used in hydrological predictions. The NSE ranges from -$\infty$ to 1, with values closer to 1 indicating higher accuracy.

The initialized variable representation closely aligns with the ground truth, as shown in Figure~\ref{fig:G}, attesting to the efficacy of our proposed autoencoder architecture. As expected, the ``full-cause'' performs better than the ``single-cause'' for each effect node. Node $J$ exhibits the best prediction, whereas node $I$ presents a challenge. For node $G$, causality from $C$ proves to be significantly stronger than the other two, $D$ and $E$.


% One may observe via the demo that our experiments do not show smooth information flows along successive long causal chains. Since RNNs are designed primarily for capturing the dynamics of causes rather than the effects, relying on them to autonomously construct dynamical representations of the effects might prove unreliable. It underscores a significant opportunity for enhancing effectiveness by improving the architecture.



\vspace{-2mm}
\subsection{DAG Structure Exploration}
\vspace{-2mm}

The first round of detection starts from the source nodes $A$ and $B$ and proceeds to identify their potential edges, until culminating in the target node $J$. Candidate edges are selected based on their contributions to the overall KLD sum (less gain is better). 
Table \ref{tab:discv} shows the detected order of the edges in Figure \ref{fig:stream}, accompanied by corresponding KLD sums in each round, and also the KLD gains after each edge is included. Color-coding in the cells corresponds to Figure \ref{fig:stream}, indicating tiers of causal routines. The arrangement underscores the effectiveness of this latent space exploration approach.

Table \ref{tab:discv_rounds} in Appendix A displays the complete exploration results, with candidate edge evaluations in each round of detection. 
Meanwhile, to provide a clearer context about the dataset qualification with respect to underlying structure identification, we also employ the traditional causal discovery method, Fast Greedy Search (FGES), with a 10-fold cross-validation to perform the same procedure as RIRL exploration. The results in Table \ref{tab:fges} are available in Appendix A, exhibiting the difficulties of using conventional methods.


%\vspace{-3mm}
\begin{table*}[t]
\caption{Performances of micro-causal relationship reconstructions using RIRL, categorized by effect nodes.}
\label{tab:unit}
\vspace{-2mm}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{|l|lll|l|lll|llll|}
\hline
\rowcolor[HTML]{C0C0C0} 
\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} } & \multicolumn{3}{l|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \begin{tabular}[c]{@{}l@{}}\ \ \ \ \ \ Variable Representation\\ \ \ \ \ \ \ \ (Initialized)\end{tabular}}} & \cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} } & \multicolumn{3}{l|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000}  \begin{tabular}[c]{@{}l@{}}\ \ \ \ \ \ Variable Representation\\ \ \ \ \ \ \ \ (in Micro-Causal Models)\end{tabular}}} & \multicolumn{4}{l|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \ \ \ \ \ \ \ \ \ Relation-Indexed Representation}} \\ \cline{2-4} \cline{6-12} 
\rowcolor[HTML]{C0C0C0} 
\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} } & \multicolumn{2}{l|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000}\ \ \ \ \ \  \ \ \ \ RMSE}} & {\color[HTML]{000000} BCE} & \cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} } & \multicolumn{2}{l|}{\cellcolor[HTML]{C0C0C0}\ \ \ \ \ \ \ \ \ \ RMSE} & BCE & \multicolumn{2}{l|}{\cellcolor[HTML]{C0C0C0} \ \ \ \ \ \ \ \ \ RMSE} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}BCE} & KLD \\ \cline{2-4} \cline{6-12} 
\rowcolor[HTML]{C0C0C0} 
\multirow{-3}{*}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \begin{tabular}[c]{@{}l@{}}Efect \\ Node\\ \ \end{tabular}}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Scaled\\ \ \ Values\end{tabular}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Unscaled\\ \ \ Values\end{tabular}} & Mask & \multirow{-3}{*}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \begin{tabular}[c]{@{}l@{}}Cause\\ Node\end{tabular}}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Scaled\\ \ \ Values\end{tabular}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Unscaled\\ \ \ Values\end{tabular}} & Mask & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Scaled\\ \ \ Values\end{tabular}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Unscaled\\ \ \ Values\end{tabular}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Mask} & \begin{tabular}[c]{@{}l@{}}(in latent\\ \ \ space)\end{tabular} \\ \hline
\cellcolor[HTML]{EFEFEF}C & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}0.037} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}0.089} & \cellcolor[HTML]{EFEFEF}0.428 & A & \multicolumn{1}{l|}{0.0295} & \multicolumn{1}{l|}{0.0616} & 0.4278 & \multicolumn{1}{l|}{0.1747} & \multicolumn{1}{l|}{0.3334} & \multicolumn{1}{l|}{0.4278} & 7.6353 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & BC & \multicolumn{1}{l|}{0.0350} & \multicolumn{1}{l|}{1.0179} & 0.1355 & \multicolumn{1}{l|}{0.0509} & \multicolumn{1}{l|}{1.7059} & \multicolumn{1}{l|}{0.1285} & 9.6502 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & B & \multicolumn{1}{l|}{0.0341} & \multicolumn{1}{l|}{1.0361} & 0.1693 & \multicolumn{1}{l|}{0.0516} & \multicolumn{1}{l|}{1.7737} & \multicolumn{1}{l|}{0.1925} & 8.5147 \\ \cline{5-5}
\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}D} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.015}} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.679}} & \multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.445} & C & \multicolumn{1}{l|}{0.0331} & \multicolumn{1}{l|}{0.9818} & 0.3404 & \multicolumn{1}{l|}{0.0512} & \multicolumn{1}{l|}{1.7265} & \multicolumn{1}{l|}{0.3667} & 10.149 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & BC & \multicolumn{1}{l|}{0.4612} & \multicolumn{1}{l|}{26.605} & 0.6427 & \multicolumn{1}{l|}{0.7827} & \multicolumn{1}{l|}{45.149} & \multicolumn{1}{l|}{0.6427} & 39.750 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & B & \multicolumn{1}{l|}{0.6428} & \multicolumn{1}{l|}{37.076} & 0.6427 & \multicolumn{1}{l|}{0.8209} & \multicolumn{1}{l|}{47.353} & \multicolumn{1}{l|}{0.6427} & 37.072 \\ \cline{5-5}
\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}E} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.058}} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}3.343}} & \multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.643} & C & \multicolumn{1}{l|}{0.5212} & \multicolumn{1}{l|}{30.065} & 1.2854 & \multicolumn{1}{l|}{0.7939} & \multicolumn{1}{l|}{45.791} & \multicolumn{1}{l|}{1.2854} & 46.587 \\ \hline
\cellcolor[HTML]{EFEFEF}F & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}0.326} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}7.178} & \cellcolor[HTML]{EFEFEF}2.045 & E & \multicolumn{1}{l|}{0.4334} & \multicolumn{1}{l|}{8.3807} & 3.0895 & \multicolumn{1}{l|}{0.4509} & \multicolumn{1}{l|}{5.9553} & \multicolumn{1}{l|}{3.0895} & 53.680 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & CDE & \multicolumn{1}{l|}{0.0538} & \multicolumn{1}{l|}{0.9598} & 0.0878 & \multicolumn{1}{l|}{0.1719} & \multicolumn{1}{l|}{3.5736} & \multicolumn{1}{l|}{0.1340} & 8.1360 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & C & \multicolumn{1}{l|}{0.1057} & \multicolumn{1}{l|}{1.4219} & 0.1078 & \multicolumn{1}{l|}{0.2996} & \multicolumn{1}{l|}{4.6278} & \multicolumn{1}{l|}{0.1362} & 11.601 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & D & \multicolumn{1}{l|}{0.1773} & \multicolumn{1}{l|}{3.6083} & 0.1842 & \multicolumn{1}{l|}{0.4112} & \multicolumn{1}{l|}{8.0841} & \multicolumn{1}{l|}{0.2228} & 27.879 \\ \cline{5-5}
\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}G} & \multicolumn{1}{l|}{\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}0.045}} & \multicolumn{1}{l|}{\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}0.81}} & \multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}1.327} & E & \multicolumn{1}{l|}{0.1949} & \multicolumn{1}{l|}{4.7124} & 0.1482 & \multicolumn{1}{l|}{0.5564} & \multicolumn{1}{l|}{10.852} & \multicolumn{1}{l|}{0.1877} & 39.133 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & DE & \multicolumn{1}{l|}{0.0889} & \multicolumn{1}{l|}{0.0099} & 2.5980 & \multicolumn{1}{l|}{0.3564} & \multicolumn{1}{l|}{0.0096} & \multicolumn{1}{l|}{2.5980} & 21.905 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & D & \multicolumn{1}{l|}{0.0878} & \multicolumn{1}{l|}{0.0104} & 0.0911 & \multicolumn{1}{l|}{0.4301} & \multicolumn{1}{l|}{0.0095} & \multicolumn{1}{l|}{0.0911} & 25.198 \\ \cline{5-5}
\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}H} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.045}} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.009}} & \multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}1.345} & E & \multicolumn{1}{l|}{0.1162} & \multicolumn{1}{l|}{0.0105} & 0.1482 & \multicolumn{1}{l|}{0.5168} & \multicolumn{1}{l|}{0.0097} & \multicolumn{1}{l|}{3.8514} & 39.886 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & DF & \multicolumn{1}{l|}{0.0600} & \multicolumn{1}{l|}{0.0103} & 3.4493 & \multicolumn{1}{l|}{0.1158} & \multicolumn{1}{l|}{0.0099} & \multicolumn{1}{l|}{3.4493} & 49.033 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & D & \multicolumn{1}{l|}{0.1212} & \multicolumn{1}{l|}{0.0108} & 3.0048 & \multicolumn{1}{l|}{0.2073} & \multicolumn{1}{l|}{0.0108} & \multicolumn{1}{l|}{3.0048} & 75.577 \\ \cline{5-5}
\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}I} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.035}} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.009}} & \multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}1.672} & F & \multicolumn{1}{l|}{0.0540} & \multicolumn{1}{l|}{0.0102} & 3.4493 & \multicolumn{1}{l|}{0.0948} & \multicolumn{1}{l|}{0.0098} & \multicolumn{1}{l|}{3.4493} & 45.648 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & GHI & \multicolumn{1}{l|}{0.0052} & \multicolumn{1}{l|}{0.0742} & 0.2593 & \multicolumn{1}{l|}{0.0090} & \multicolumn{1}{l|}{0.1269} & \multicolumn{1}{l|}{0.2937} & 5.5300 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & G & \multicolumn{1}{l|}{0.0077} & \multicolumn{1}{l|}{0.1085} & 0.4009 & \multicolumn{1}{l|}{0.0099} & \multicolumn{1}{l|}{0.1390} & \multicolumn{1}{l|}{0.4375} & 5.2924 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & H & \multicolumn{1}{l|}{0.0159} & \multicolumn{1}{l|}{0.2239} & 0.4584 & \multicolumn{1}{l|}{0.0393} & \multicolumn{1}{l|}{0.5520} & \multicolumn{1}{l|}{0.4938} & 15.930 \\ \cline{5-5}
\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}J} & \multicolumn{1}{l|}{\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}0.007}} & \multicolumn{1}{l|}{\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}0.098}} & \multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}1.088} & I & \multicolumn{1}{l|}{0.0308} & \multicolumn{1}{l|}{0.4328} & 0.3818 & \multicolumn{1}{l|}{0.0397} & \multicolumn{1}{l|}{0.5564} & \multicolumn{1}{l|}{0.3954} & 17.410 \\ \hline
\end{tabular}
}
\vspace{-3mm}
\end{table*}


\section{Conclusions}\label{sec:conclusion}
\vspace{-3mm}

This paper focuses on the inherent challenges of the traditional i.i.d.-based learning paradigm in addressing causal relationships. Conventionally, we construct statistical models as observers of the world, grounded in epistemology. However, adopting this perspective assumes that our observations accurately reflect the ``reality'' as we understand it, implying that seemingly objective models may actually be based on subjective assumptions. 
This fundamental issue has become increasingly evident in causality modeling, especially with the rise of applications in causal representation learning that aim to automate the specification of causal variables traditionally done manually.

Our understanding of causality is fundamentally based on the creator's perspective, as the ``what...if'' questions are only valid within the possible world we conceive in our consciousness. 
The advocated ``perspective shift''  represents a transformation from an \emph{object-first} to a \emph{relation-first} modeling paradigm, a change that transcends mere methodological or technical advancements. Indeed, this shift has been facilitated by the advent of AI, particularly through neural network-based representation learning, which lays the groundwork for implementing \emph{relation-first} modeling in computer engineering.

The limitation of the observer's perspective in traditional causal inference prevents the capture of dynamic causal outcomes, namely, the nonlinear timing distributions across multiple ``possible timelines''. Accordingly, this oversight has led to compensatory efforts, such as the introduction of hidden confounders and the reliance on the sufficiency assumption. 
These theories have been instrumental in developing knowledge systems across various fields over the past decades. However, with the rapid advancement of AI techniques, the time has come to move beyond the conventional modeling paradigm toward the potential realization of AGI.

In this paper, we present \emph{relation-first} principle and its corresponding modeling framework for structuralized causality representation learning, based on discussions about its philosophical and mathematical underpinnings.
Adopting this new framework allows us to simplify or even bypass complex questions significantly. We also introduce the Relation-Indexed Representation Learning (RIRL) method as an initial application of the \emph{relation-first} paradigm, supported by experiments that validate its efficacy.


% This paper introduces a dimensionality framework from a \emph{Relation-Oriented} perspective to decompose our cognitive space, where relational causal knowledge is stored. 
% Specifically, it conceptualizes the unobservable relations between cause and effect as informative variables in $\mathbb{R}^H$; and the causal structure of dynamics in knowledge, represented by the enhanced DAG, is accommodated by the counterfactual space $\mathbb{R}^T$, 
% across multiple relative timing axes with nonlinear dependence. 
% It highlights the key oversights of the current \emph{Observation-Oriented} paradigm, which relies on the observational i.i.d. assumption and is confined to $\mathbb{R}^O$. 
% %It relies on manual specification to identify dynamical effects from observational static sequences, inherently fraught with difficulty.
% %Specifically, based on the observational i.i.d. assumption, conventional relationship modeling intrinsically overlooks 1) the informative unobservables in $\mathbb{R}^H$, and 2) the structuralized dynamics within multi-dimensional $\mathbb{R}^T$. Instead, due to being confined within $\mathbb{R}^O$, 

% The traditional causal inference, adopting a \emph{Relation-Oriented} viewpoint, identifies the underlying causal structures across relative timings but overlooks the $\mathbb{R}^T$ space due to neglecting temporal nonlinearities, i.e., the dynamics. 
% Under the \emph{Observation-Oriented} paradigm, contemporary causal learning is often challenged by incompletely captured dynamical effects without considering the indexing role of unobservable relations lying in $\mathbb{R}^H$.
% In the case of LLMs, while AI techniques enable the autonomous identification of dynamical effects, they often neglect their interactions, which are emphasized as causal structures in causal inference.

% % When viewed through the lens of the \emph{Relation-Oriented} framework, the multifaceted issues surrounding causality learning become unified, addressing common confusions and concerns from traditional causal inference to modern LLMs. 
% Recalling the queries presented in the Introduction, we systematically summarize these application-related restrictions in our pursuit of AGI, and offer new insights:
% \begin{enumerate}[itemsep=0em, topsep=-1pt,
% parsep=2pt, partopsep=0pt,
% leftmargin=22pt, labelwidth=10pt]
% \item[\ding{118}] \emph{Firstly}, challenges for causal inference models primarily arise from overlooking dynamics, due to their linear modeling constraints. This oversight leads to various compensatory efforts, such as introducing hidden confounders and relying on the causal sufficiency assumption. 
% Causal DAGs inherently provide a \emph{Relation-Oriented} view; with the proposed enhancement incorporating them into the counterfactual $\mathbb{R}^T$ space, they can provide essential support for illustrating structuralized dynamics.

% \item[\ding{118}] \emph{Secondly}, our knowledge inherently contains hierarchical levels due to hidden relations $\omega\in \mathbb{R}^H$, which necessitates generalizability of models. 
% For AI-based causal models, the main challenge lies in incorporating the underlying structure of dynamics to achieve dynamical generalizability. The new paradigm we propose introduces a relation-indexing methodology, enabling the autonomous construction of causal structures by sequentially extracting causal representations.


% \item[\ding{118}] \emph{Thirdly}, while existing language models have made strides in generalizability through meta-learning, they are still limited to absolute timing within $\mathbb{R}^O$, implicitly assuming nonlinear independence among temporal dimensions. Additionally, their neglect of extracting informative $\theta$ prevents them from truly ``understanding'' the captured relationships.
% However, LLMs have demonstrated the effectiveness of meta-learning in addressing temporal dimensional hierarchies, suggesting a promising prospect for \emph{Relation-Oriented} meta-learning in advancing towards AGI.
% \end{enumerate}

% We also introduce a baseline implementation of the \emph{Relation-Oriented} paradigm, primarily to validate the efficacy of the ``relation-indexing'' methodology in implementing causal representations and constructing knowledge-aligned causal structures. Similar thoughts have been attempted in domains with well-established causal knowledge, such as the hierarchical temporal memory in neuroscience \cite{wu2018hierarchical}. The pursuit of AGI is a historically extensive and complex endeavor, requiring a wide array of knowledge-aligned AI model constructions. This study aims to provide foundational insights for future developments in this field.




% Driven by the misalignment issues between causal knowledge and established causal models in widespread AI applications, this study examines fundamental limitations of the dominant \emph{Observation-Oriented} learning paradigm. In response, we advocate for a novel \emph{Relation-Oriented} paradigm, inspired by the relation-centric nature of human knowledge, and complemented by a practical approach of \emph{Relation-Indexed Representation Learning} (RIRL), with demonstrated efficacy.

% The concept of a ``hyper-dimension'' is initially proposed, as an accommodation for unobservable knowledge. We subsequently build a comprehensive framework of dimensionality, to offer more intuitive insights into relationship learning. 
% The discrepancy, between our comprehension of ``time'' and the single timeline used in our causal models, inherently causes misalignment, and results in model generalizability issues.

% \emph{Relation-Oriented} reflects the process of human understanding, 
% aims to mitigate AI misalignment, paving the way toward causally interpretable AGI. Constructing AGI is a long-term, intricate process requiring extensive work within interdisciplinary efforts, and we seek to lay a foundation for its future advancements.



\ifpreprint
\vspace{5mm}
\section*{Acknowledgements}
\vspace{-2mm}

I'd like to extend my heartfelt thanks to the reviewers from TMLR, who have provided invaluable feedback vital for this theory's final completion. Additionally, my gratitude goes to GPT-4 for its assistance in enhancing my English writing.
I also wish to thank my advisor, Prof. Vipin Kumar, for the initial support in the beginning stage of this work.

\hfill Jia Li, Feb 2024

\vspace{10mm}

\else
\vspace{10mm}
%\pagebreak
\fi

\bibliography{main}
\bibliographystyle{tmlr}


\appendix
\section{Appendix: Complete Experimental Results in DAG Structure Exploration Test}
%\includepdf[pages=-]{mainz_append_pdf.pdf}

\pagebreak


%\newpage
\begin{sidewaystable}
%\begin{table*}[t]
\centering
\caption{The Complete Results of RIRL Exploration in the Latent Space.
Each row stands for a round of detection, with `\#'' identifying the round number, and all candidate edges are listed with their KLD gains as below. 1) Green cells: the newly detected edges. 2) Red cells: the selected edge.
3) Blue cells: the trimmed edges accordingly. }
%\vspace{-20mm}
\label{tab:discv_rounds}
\resizebox{1\columnwidth}{!}{%
%\rotatebox{90}{

\begin{tabular}{|c|c|c|cccccccccccccc}
\cline{1-9}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \textbf{A$\rightarrow$ C}} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ D & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{B$\rightarrow$ C}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ D} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 1}} & \cellcolor[HTML]{FFCCC9}{\color[HTML]{000000} \textbf{7.6354}} & 19.7407 & \multicolumn{1}{c|}{60.1876} & \multicolumn{1}{c|}{119.7730} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{8.4753}} & \multicolumn{1}{c|}{8.5147} & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} &  &  &  &  &  &  &  &  \\ \cline{1-13}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ D & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{B$\rightarrow$ D}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ D} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 2}} & 19.7407 & 60.1876 & \multicolumn{1}{c|}{119.7730} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{8.5147}}} & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}10.1490} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}46.5876} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}111.2978} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}11.6012} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}39.2361} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}95.1564} &  &  &  &  \\ \hline
\rowcolor[HTML]{C0C0C0} 
\cellcolor[HTML]{C0C0C0} & \textbf{A$\rightarrow$ D} & A$\rightarrow$ E & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{C$\rightarrow$ D}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 3}} & \cellcolor[HTML]{CBCEFB}\textbf{9.7357} & 60.1876 & \multicolumn{1}{c|}{119.7730} & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{1.1355}}} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{11.6012} & \multicolumn{1}{c|}{39.2361} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}63.7348} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}123.3203} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}27.8798} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}25.1988} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}75.5775} \\ \hline
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \textbf{C$\rightarrow$ G}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 4}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{000000} \textbf{11.6012}}} & \multicolumn{1}{c|}{39.2361} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{27.8798} & \multicolumn{1}{c|}{25.1988} & \multicolumn{1}{c|}{75.5775} &  &  \\ \cline{1-15}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{D$\rightarrow$ G}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}G$\rightarrow$ J} &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 5}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{39.2361} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{2.4540}}} & \multicolumn{1}{c|}{25.1988} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}5.2924} &  &  \\ \cline{1-15}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{G$\rightarrow$ J}}} &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 6}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{39.2361} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{25.1988} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{5.2924}}} &  &  &  \\ \cline{1-14}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{C$\rightarrow$ H}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{D$\rightarrow$ H}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 7}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{39.2361}} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{25.1988}}} & \multicolumn{1}{c|}{75.5775} &  &  &  &  \\ \cline{1-13}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{H$\rightarrow$ J}}} &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 8}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{0.2092}}} &  &  &  &  &  \\ \cline{1-12}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}\textbf{A$\rightarrow$ E} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{C$\rightarrow$ E}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 9}} & \cellcolor[HTML]{CBCEFB}\textbf{60.1876} & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{46.5876}}} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} &  &  &  &  &  &  \\ \cline{1-13}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{B$\rightarrow$ E}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{D$\rightarrow$ E}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ I} &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 10}} & 119.7730 & \cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{-6.8372}} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{17.0407}} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}53.6806} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}-5.9191} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}-3.2931} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}110.2558} &  &  &  &  \\ \cline{1-13}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \cellcolor[HTML]{C0C0C0}B$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{E$\rightarrow$ G}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ I} &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 11}} & 119.7730 & 132.7717 & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{53.6806} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{-5.9191}} & \multicolumn{1}{c|}{-3.2931} & \multicolumn{1}{c|}{110.2558} &  &  &  &  &  &  \\ \cline{1-11}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \cellcolor[HTML]{C0C0C0}B$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{E$\rightarrow$ H}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ I} &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 12}} & 119.7730 & 132.7717 & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{53.6806} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{-3.2931}} & \multicolumn{1}{c|}{110.2558} &  &  &  &  &  &  &  \\ \cline{1-10}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}\textbf{A$\rightarrow$ F} & \cellcolor[HTML]{C0C0C0}\textbf{B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{C$\rightarrow$ F}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{D$\rightarrow$ F}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{E$\rightarrow$ F}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ I} &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 13}} & \cellcolor[HTML]{CBCEFB}\textbf{119.7730} & \cellcolor[HTML]{CBCEFB}\textbf{132.7717} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{111.2978}} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{123.3203}} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{53.6806}} & \multicolumn{1}{c|}{110.2558} &  &  &  &  &  &  &  &  \\ \cline{1-9}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}C$\rightarrow$ I & \cellcolor[HTML]{C0C0C0}D$\rightarrow$ I & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{E$\rightarrow$ I}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{F$\rightarrow$ I}} &  &  &  &  &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 14}} & 95.1564 & 75.5775 & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{110.2558}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{45.6490}} &  &  &  &  &  &  &  &  &  &  &  &  \\ \cline{1-5}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}C$\rightarrow$ I & \cellcolor[HTML]{C0C0C0}D$\rightarrow$ I & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{I$\rightarrow$ J}} &  &  &  &  &  &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 15}} & 15.0222 & 3.3845 & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.0284}} &  &  &  &  &  &  &  &  &  &  &  &  &  \\ \cline{1-4}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}\textbf{C$\rightarrow$ I} & \cellcolor[HTML]{C0C0C0}\textbf{D$\rightarrow$ I} &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 16}} & \cellcolor[HTML]{CBCEFB}\textbf{15.0222} & \cellcolor[HTML]{FFCCC9}\textbf{3.3845} &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\ \cline{1-3}
\end{tabular}

}%}
%\vspace{-3mm}
%\end{table*}

\end{sidewaystable}







\begin{sidewaystable}
%\centering

\caption{Average performance of 10-Fold FGES (Fast Greedy Equivalence Search) causal discovery, with the prior knowledge that each node can only cause the other nodes with the same or greater depth with it. An edge means connecting two attributes from two different nodes, respectively. Thus, the number of possible edges between two nodes is the multiplication of the numbers of their attributes, i.e., the lengths of their data vectors.
\\ (All experiments are performed with 6 different Independent-Test kernels, including chi-square-test, d-sep-test, prob-test, disc-bic-test, fisher-z-test, mvplr-test. But their results turn out to be identical.)}
\label{tab:fges}
\resizebox{1\columnwidth}{!}{%
%\Rotatebox{90}{
%
\begin{tabular}{cccccccccccccccccc}
\hline
\rowcolor[HTML]{C0C0C0} 
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}Cause Node} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A} & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}B} & \multicolumn{3}{c|}{\cellcolor[HTML]{C0C0C0}C} & \multicolumn{3}{c|}{\cellcolor[HTML]{C0C0C0}D} & \multicolumn{3}{c|}{\cellcolor[HTML]{C0C0C0}E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}F} & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}I} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}True\\ Causation\end{tabular}} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}A$\rightarrow$ C} & \multicolumn{1}{c|}{B$\rightarrow$ D} & \multicolumn{1}{c|}{B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}C$\rightarrow$ D} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}C$\rightarrow$ G} & \multicolumn{1}{c|}{D$\rightarrow$ G} & \multicolumn{1}{c|}{D$\rightarrow$ H} & \multicolumn{1}{c|}{D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}E$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}E$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}E$\rightarrow$ H} & \multicolumn{1}{c|}{F$\rightarrow$ I} & \multicolumn{2}{c|}{\cellcolor[HTML]{EFEFEF}G$\rightarrow$ J} & \multicolumn{1}{c|}{H$\rightarrow$ J} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}I$\rightarrow$ J} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Number of\\ Edges\end{tabular}} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}16} & \multicolumn{1}{c|}{24} & \multicolumn{1}{c|}{16} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}6} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}4} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8} & \multicolumn{1}{c|}{12} & \multicolumn{1}{c|}{12} & \multicolumn{1}{c|}{9} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8} & \multicolumn{1}{c|}{12} & \multicolumn{2}{c|}{\cellcolor[HTML]{EFEFEF}4} & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}3} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Probability\\ of Missing\end{tabular}} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.038889} & \multicolumn{1}{c|}{0.125} & \multicolumn{1}{c|}{0.125} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.062} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.06875} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.039286} & \multicolumn{1}{c|}{0.069048} & \multicolumn{1}{c|}{0.2} & \multicolumn{1}{c|}{0.142857} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.3} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.003571} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.2} & \multicolumn{1}{c|}{0.142857} & \multicolumn{2}{c|}{\cellcolor[HTML]{EFEFEF}0.0} & \multicolumn{1}{c|}{0.072727} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.030303} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Wrong \\ Causation\end{tabular}} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}C$\rightarrow$ F} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{D$\rightarrow$ E} & \multicolumn{1}{c|}{D$\rightarrow$ F} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{F$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}G$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}G$\rightarrow$ I} & \multicolumn{1}{c|}{H$\rightarrow$ I} &  \\ \cline{1-1} \cline{5-5} \cline{9-10} \cline{14-17}
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Times\\ of Wrongly\\ Discovered\end{tabular}} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}5.6} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{1.2} & \multicolumn{1}{c|}{0.8} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{5.0} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8.2} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}3.0} & \multicolumn{1}{c|}{2.8} &  \\ \cline{1-1} \cline{5-5} \cline{9-10} \cline{14-17}
\multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{}
\end{tabular}
%}
}


\vspace{0.4in}
%\end{table*}

%\begin{table*}[t]
%\centering
\caption{ Brief Results of the Heuristic Causal Discovery in latent space, identical with Table 3 in the paper body, for better comparison to the traditional FGES methods results on this page.\\
The edges are arranged in detected order (from left to right) and their measured causal strengths in each step are shown below correspondingly.\\
Causal strength is measured by KLD values (less is stronger). Each round of detection is pursuing the least KLD gain globally. All evaluations are in 4-Fold validation average values. Different colors represent the ground truth causality strength tiers (referred to  the Figure 10 in the paper body).
}
\label{tab:discv}
%\resizebox{0.1\columnwidth}{!}{%
%\Rotatebox{90}{
\vspace{2mm}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{|c|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
\cellcolor[HTML]{C0C0C0}Causation & \cellcolor[HTML]{FFCCC9}A$\rightarrow$ C & \cellcolor[HTML]{FFCCC9}B$\rightarrow$ D & \cellcolor[HTML]{FFCCC9}C$\rightarrow$ D & \cellcolor[HTML]{FFCCC9}C$\rightarrow$ G & \cellcolor[HTML]{FFCCC9}D$\rightarrow$ G & \cellcolor[HTML]{FFCCC9}G$\rightarrow$ J & \cellcolor[HTML]{FFCCC9}D$\rightarrow$ H & \cellcolor[HTML]{FFCCC9}H$\rightarrow$ J & \cellcolor[HTML]{FFCE93}C$\rightarrow$ E & \cellcolor[HTML]{FFCE93}B$\rightarrow$ E & \cellcolor[HTML]{FFCE93}E$\rightarrow$ G & \cellcolor[HTML]{FFCE93}E$\rightarrow$ H & \cellcolor[HTML]{ABE9E7}E$\rightarrow$ F & \cellcolor[HTML]{ABE9E7}F$\rightarrow$ I & \cellcolor[HTML]{ABE9E7}I$\rightarrow$ J & \cellcolor[HTML]{ABE9E7}D$\rightarrow$ I \\ \hline
\cellcolor[HTML]{C0C0C0}KLD & 7.63 & 8.51 & 10.14 & 11.60 & 27.87 & 5.29 & 25.19 & 15.93 & 46.58 & 65.93 & 39.13 & 39.88 & 53.68 & 45.64 & 17.41 & 75.57 \\ \hline
\rowcolor[HTML]{FFFFFF} 
\cellcolor[HTML]{C0C0C0}Gain & 7.63 & 8.51 & 1.135 & 11.60 & 2.454 & 5.29 & 25.19 & 0.209 & 46.58 & -6.84 & -5.91 & -3.29 & 53.68 & 45.64 & 0.028 & 3.384 \\ \hline
\end{tabular}
%}}
%\end{table*}
}

\end{sidewaystable}








\end{document}
