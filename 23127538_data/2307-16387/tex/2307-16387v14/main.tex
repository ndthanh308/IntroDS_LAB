
\documentclass[10pt]{article} % For LaTeX2e

\newif\ifpreprint

% \usepackage{tmlr}
% \preprintfalse

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}

% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:

\usepackage[preprint]{tmlr}
\preprinttrue

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{hyperref}
\usepackage{url}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{adjustbox}
%\usepackage{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{centernot}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{color}
%\usepackage{booktabs}

\usepackage{subfigure}
\usepackage{caption}
\usepackage{dsfont}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{empheq}
%\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{totcount}

\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{makecell}
%\usepackage[table]{xcolor}
\usepackage{xcolor}
\usepackage{changepage}
\usepackage{stfloats}
%\usepackage{cite}
\usepackage{soul}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{algpseudocode}
\usepackage[title]{appendix}
\usepackage{pdfpages}

\usepackage{bm}
\usepackage{amsmath,amssymb,amsfonts}
% \usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{bookmark}
\usepackage{float}
\usepackage{tabularx}
\usepackage{mathtools}
\usepackage{array}
\usepackage{rotating}
\usepackage[skins]{tcolorbox}
\usepackage{tikz}
\usepackage{pifont}

\newcommand{\indep}{\perp \!\!\! \perp}
\newcommand{\nindep}{\not\!\perp\!\!\!\perp}

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{ \node[shape=circle,draw,inner sep=1.5pt] (char) {#1};}}

\title{Relation-First Modeling Paradigm for Causal Representation Learning toward the Development of AGI}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.


\author{
\name Jia Li \email jiaxx213@umn.edu \\
      \addr Department of Computer Science,
      University of Minnesota
      \vspace{-3mm}
      \AND
      \name Xiang Li \email lixx5000@umn.edu \\
      \addr Department of Bioproducts and Biosystems Engineering,       University of Minnesota
      % \name Xiaowei Jia \email xiaowei@pitt.edu\\
      % \addr Department of Computer Science\\ University of Pittsburgh
      }

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{Apr}  % Insert correct month for camera-ready version
\def\year{2023} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version

\definecolor{thegrey}{RGB}{192,192,192}
\definecolor{orchid}{RGB}{204,153,255}
\definecolor{myblue}{RGB}{0,115,207}
\definecolor{mygreen}{RGB}{68,134,25}
%\definecolor{myorange}{RGB}{0,128,0}
\definecolor{mypurple}{RGB}{177,35,200}

\begin{document}


\maketitle
%\vspace{-3mm}

\ifpreprint
\vspace{-4.5mm}
\fi

\begin{abstract}
\vspace{-3mm}

\ifpreprint
\vspace{-1mm}
\fi

The traditional i.i.d.-based learning paradigm faces inherent challenges in addressing causal relationships, which has become increasingly evident with the rise of applications in causal representation learning. 
Our understanding of causality naturally requires a perspective as the creator rather than observer, as the ``what...if'' questions only hold within the possible world we conceive. The traditional perspective limits capturing dynamic causal outcomes and leads to compensatory efforts such as the reliance on hidden confounders.
This paper lays the groundwork for the new perspective, which enables the \emph{relation-first} modeling paradigm for causality. Also, it introduces the Relation-Indexed Representation Learning (RIRL) as a practical implementation, supported by experiments that validate its efficacy.
% The potential surge of causal reasoning in AI models toward AGI is imminent, given the impending saturation of observation-based applications in fields like image and language processing. It is both critical and underrecognized that the essence of causality lies in the \emph{temporal nonlinearity} (i.e., dynamics) of causal effects. Capturing such featured causal representations is key to realizing AGI. This paper advocates for a thorough reevaluation and potential overhaul of existing causal inference theories and the traditional learning paradigm, which predominantly relies on the observational i.i.d assumption.
% Our aim is to align these theories and methodologies with the intrinsic demands of AGI development.
% We introduce a novel \emph{Relation-Oriented} paradigm for relationship modeling, and the \emph{Relation-Indexed Representation Learning} (RIRL) method as its foundational implementation. Extensive experiments confirm RIRL's efficacy in autonomously capturing dynamical effects.



\vspace{-2mm}
\end{abstract}

\ifpreprint
\vspace{-3mm}
\fi

\section{Introduction}

The concept of Artificial General Intelligence (AGI) has prompted extensive discussions over the years yet remains hypothetical, without a practical definition in the context of computer engineering. The pivotal question lies in whether human-like ``understanding'', especially causal reasoning, can be implemented using formalized languages in computer systems \cite{newell2007computer, pavlick2023symbols, marcus2020next}. From an epistemological standpoint, abstract entities (i.e., perceptions, beliefs, desires, etc.) are prevalent and integral to human intelligence. However, in the symbol-grounded modeling processes, variables are typically assigned as observables, representing tangible objects to ensure their values have clear meaning.

Epistemological thinking is often anchored in objective entities, seeking an irreducible ``independent reality'' \cite{eberhardt2022causal}. 
This approach necessitates a metaphysical commitment to constructing knowledge by assuming the unproven prior existence of the ``essence of things'', fundamentally driven by our desire for certainty. Unlike physical science, which is concerned with deciphering natural laws, technology focuses on devising effective methods for problem-solving, aiming for the optimal functional value between the nature of things and human needs. This paper advocates for a shift in perspective when considering technological or engineering issues related to AI or AGI, moving from traditional epistemologies to that of the creator. That is, our fundamental thinking should move from ``truth and reality'' to ``creation and possibility''.

In some respects, both classical statistics and modern machine learnings traditionally rely on epistemology and follow an ``object-first'' modeling paradigm, as illustrated by the practice of assigning pre-specified, unchanging values to variables regardless of the model chosen. In short, individual \emph{objects} (i.e., variables and outcomes) are defined a priori before considering the \emph{relations} (i.e., model functions) between them by assuming that what we observe precisely represents the ``objective truth'' as we understand it.  
This approach, however, poses a fundamental dilemma when dealing with causal relationship models.

Specifically, ``causality'' suggests a range of possible worlds, encompassing all potential futures, whereas ``observations'' identify the single possibility that has actualized into history with 100\% certainty.
Hence, addressing causal questions requires us to adopt the perspective of the ``creator'' (rather than the ``observer''), to expand the objects of our consciousness from given entities (i.e., the observational world) to include possible worlds, where values are assigned ``as supposed to be'', that is, \emph{as dictated by the relationship}.


Admittedly, causal inference and related machine learning methods have made significant contributions to knowledge developments in various fields \cite{wood2015lesson, vukovic2022causal, ombadi2020evaluation}. However, the inherent misalignment between the ``object-first'' modeling principle and our instinctive ``relation-first'' causal understanding has been increasingly accentuated by the application of AI techniques, i.e., the neural network-based methods.
Particularly, integrating causal DAGs (Directed Acyclic Graphs), which represent established knowledge, into network architectures \cite{marwala2015causality, lachapelle2019gradient} is a logical approach to efficiently modeling causations with complex structures. However, surprisingly, this integration has not yet achieved general success \cite{luo2020causal, ma2018using}.

As Scholkopf \cite{scholkopf2021toward} points out, it is commonly presumed that ``the causal variables are given''. In response, they introduce the concept of ``causal representation'' to actively construct variable values as causally dictated, replacing the passively assumed observational values.
However, the practical framework for modeling causality, especially in contrast to mere correlations, remains underexplored. Moreover, this shift in perspective suggests that we are not just dealing with ``a new method'' but rather a new learning paradigm, necessitating in-depth philosophical discussions. Also, the potential transformative implications of this ``relation-first'' paradigm for AI development warrant careful consideration.

This paper will thoroughly explore the ``relation-first'' paradigm in Section 2, and introduce a complete framework for causality modeling by adopting the ``creator's'' perspective in Section 3. In Section 4, we will propose the \emph{Relation-Indexed Representation Learning} (RIRL) method as the initial implementation of this new paradigm, along with extensive experiments to validate RIRL's effectiveness in Section 5. 

\section{Relation-First Paradigm}
\vspace{-2mm}

The ``do-calculus'' format in causal inference \cite{pearl2012calculus, huang2012pearl} is widely used to differentiate the effects from ``observational'' data $X$, and ``interventional'' data $do(X)$ \cite{hoel2013quantifying, eberhardt2022causal}. Specifically, $do(X=x)$ represents an intervention (or action) where the variable $X$ is set to a specific value $x$, distinct from merely observing $X$ taking the value $x$.
However, given the causation represented by $X\rightarrow Y$, why doesn't $do(Y=y)$ appear as the action of another variable $Y$?

Particularly, compared to the \textbf{\emph{static}} state $X$, $do(X)$ incorporates its ``temporal aspect'' to represent the process of ``becoming $X$'' as a \textbf{\emph{dynamic}} object. This concept is analogous to $do(Y)$, with the naturally understood relationship $do(X)\rightarrow do(Y)$ being formed.
Take, for instance, the statement ``The storm lasting for a week causes downstream villages to be drowned by the flood'', if $do(X)$ represents the storm lasting a week, then $do(Y)$ could be considered as the ensuing disaster of the villages drowning.

The difficulty of involving $do(Y)$ stems from the empirical difficulties when building models.
We can naturally envision $do(Y)$ before it becomes true, in a ``possible world'' that we freely \emph{create} in our consciousness, with its timing or states varying.
For example, the disaster denoted by $do(Y)$ could happen earlier or later, with different levels of severity, depending on various conditions.
This is in contrast to the observational world of $do(X)$, where only a single ``actual timing'' exists, allowing us to focus on modeling observed states at specific timestamps, like $X_t$, without the need to account for ``varying timing''. 

%From the perspective of a ``creator,'' such a $do(Y)$ can naturally occur in a ``possible world'' that we freely create in our consciousness, before it becomes the truth. However, from the ``observer's'' viewpoint, only $do(X)$, which actually exists in our observations, can be described.

%The difficulty of involving $do(Y)$ lies in empirical challenges - we can envision countless possible worlds with varying timing or states. For example, the disaster represented by $do(Y)$ could occur earlier or later with varying degrees of severity, depending on different conditions. This contrasts with the observational world, where only a single ``actual timing'' exists,  

% only a single absolute timing exists, allowing us to safely exclude ``time'' as a dimension in our computations (e.g., concerning whether it occurs earlier or later) and focus solely on the observational values of $X$ and $Y$. But this does not mean that the value distributions along time, i.e., the \emph{dynamics} of $X$ and $Y$, are insignificant for learning. 

However, it does not mean the distributions over timing are insignificant for modeling. 
On the contrary, the inclusion of $do(X)$ highlights its necessity. 
Particularly, RNNs (Recurrent neural networks) can autonomously identify the significant dynamics to form the ``cause'', effectively achieving $do(X)\rightarrow Y$, without the need to manually pre-specify ``identifiable $do(X)$'' as statistical causal inference often requires \cite{pearl2012calculus} (e.g., determining the duration of a storm necessary to qualify as a significant $do(X)$ for different watersheds).

In RNNs, $do(X)$ is optimized in the form of latent space features, by being related to the outcome $Y$. These features were initialized to represent the sequence $X^t=[X_1,\ldots, X_t]$ with determined timestamp $t$. But, as representations rather than mere observational variables, they allow the $t$-dimension to be incorporated into computations to evaluate the $t$ value's significance.
The capability of RNNs to effectively realize $do(X)\rightarrow Y$ has led to their growing popularity in relationship modeling \cite{xu2020multivariate}. However, the question arises: Can the same approach be used to autonomously extract $do(Y)$ without pre-specifying its timing?

Since the technique has emerged, it may be time for us to see the ``possible world'' in computations. This requires a shift in perspective from being the ``observer'' to becoming a ``creator,'' essential for engaging in causal modeling within a ``relation-first'' paradigm. This section will delve into its philosophical underpinnings, mathematically redefine ``causality'', and explore its broader implications.
%dopting this ``creator's'' viewpoint, we will explore the proposed relation-first causal modeling paradigm in this section. Accordingly, we will introduce a symbolic definition for the concept of ``causal relationship'' that is directly applicable in engineering contexts.

\subsection{Philosophical Foundation}

The Causal Emergence theory \cite{hoel2013quantifying, hoel2017map} represents a significant advancement in the philosophical understanding of causality. It suggests that while causality is often observed in micro-causal components only (e.g., in the single relationship $X\rightarrow Y$), the macro-causal system (e.g., a multi-variable causal system comprising $\{X, Y, Z\}$) can exhibit more informative interactions than just the sum of its components (e.g., the system's behavior cannot be fully explained by simply combining $X\rightarrow Y$, $X\rightarrow Z$, and $Y\rightarrow Z$). 
The macro- and micro-causality originates from economics, while in modeling contexts, their scales can be flexibly determined based on practical questions.
For example, if $X\rightarrow Y$ is viewed as macro-causal, with $Y_1$ and $Y_2$ representing two complementary parts of $Y$ such that $Y=Y_1+Y_2$, it can be divided into two micro-causal components $X\rightarrow Y_1$ and $X\rightarrow Y_2$, as illustrated in Figure~\ref{fig:macro}.  

% Figure environment removed

In this context, the concept of Effective Information (EI), represented as $EI(X\rightarrow Y)$, is employed to measure the information generated by the system during the transition from the state of $X$ to the state of $Y$ \cite{tononi2003measuring, hoel2013quantifying}.
Moreover, the minimum EI that can be exchanged between $Y_1$ and $Y_2$ is represented by an abstract variable $\phi$, signifying the ability to integrate this crucial information \cite{tononi2003measuring}.
Regardless of mere statistical dependence between state components $Y_1$ and $Y_2$, which is depicted by the dashed line with $EI(\phi)=0$ in Figure~\ref{fig:macro}(a), the causal emergence highlights that ``their informative interaction with $EI(\phi)>0$ may not be fully encapsulated by $EI(X\rightarrow Y)$'' as in Figure~\ref{fig:macro}(b). 
%Similarly, we use $\theta$ to represent the EI of $X\rightarrow Y$ for clarity, such that $E\mathcal{I}(X\rightarrow Y) = E\mathcal{I}(\theta)$.

%The capacity to represent the information between micro components, signified by $\phi$, was initially established in the Integrated Information Theory of Consciousness \cite{tononi2003measuring}.
%We apply its original definition while streamlining the notation to $E\mathcal{I}(X\rightarrow Y) = E\mathcal{I}(\theta)$, where the abstract variable $\theta$ represents the desired effective information. 

It can be explained by the information loss when reducing a \emph{dynamic} outcome $do(Y)$ to be a \emph{static} $Y$. For ease of understanding, let's consider the reduction from $do(X)\rightarrow do(Y)$ to $X\rightarrow Y$, which can be likened to attributing the precipitation on a specific date (i.e., the $X_t$ value), as the sole cause for the likelihood of a village being flooded $7$ days later (i.e., the $Y_{t+7}$ value), regardless of what happened on the other days.
From a computational standpoint, given observational variables $X\in \mathbb{R}^{n}$ and $Y\in \mathbb{R}^{m}$, this reduction implies that the interaction between the $\mathbb{R}^{n+1}$ and $\mathbb{R}^{m+1}$ spaces must be simplistically represented between $\mathbb{R}^{n}$ and $\mathbb{R}^{m}$.
%do(X)$ can be viewed as a $(n+1)$-dimensional variable, by integrating the $t$-dimension with the observational variable $X\in \mathbb{R}^{n}$, and similarly for $do(Y)\in \mathbb{R}^{m+1}$. So,  
%From an empirical standpoint, this 


%For an observed $X$, we can still manually identify $do(X)$ from data (e.g., specifying the duration days for each qualified storm) while treating $Y$ as purely observational to denote its ``possible state'' at a specific time (e.g., $7$ days later) to achieve $do(X)\rightarrow Y$.
Still, achieving $do(X)\rightarrow Y$ is feasible by manually identifying $do(X)$ from observed data (e.g., specifying the duration dates for each qualified storm). 
But for $Y_1\rightarrow Y_2$, which implies an interaction in a ``possible world'' with ``possible timings,'' pre-specifying may prove impractical. Suppose $Y_1$ represents the impact of flood-prevention operations, and $Y_2$ signifies the water level on any subsequent day ``without'' these operations. The actual result $do(Y)$ can be dynamically described as ``the flood crest originally expected on the 7th day has been mitigated over the following days, preventing a disaster''. However, it would be challenging to precisely specify a particular day's water rising for $Y_2$ ``if without'' $Y_1$. 

As Hoel emphasizes \cite{hoel2017map}, applying information theory to causality enables us to create ``nonexistent'' or ``counterfactual'' statements. Indeed, the concept of ``information'' is intrinsically linked to \textbf{\emph{relation}} and independent of the \textbf{\emph{objects}} of entities. 
For example, let's use an abstract variable $\theta$ to represent the EI of transitioning from $X_t$ to $Y_{t+7}$ (similar to how $\phi$ has been defined). Suppose $\theta=$ ``flooding'', and $EI(\theta)=$ ``what a flooding may imply'', we can then easily conceptualize $do(X)=$ ``continuous storm'' as its cause, and $do(Y)=$ ``disastrous water rise'' as the result in consciousness, without needing specific observations of precipitation value  $X_t$ and water level $Y_{t+7}$.
In other words, our comprehension is in a ``relation-first'' manner, unlike the ``object-first'' approach typically employed in our modeling processes.

Significantly, the ``possible world'' we seek to predict is inherently \textbf{\emph{created}} by our innate ``relation-first'' thinking, where \emph{timing} is the essential dimension. Without ``possible timing,'' any ``possible observation'' would lose its meaning. For example, one might use the model $Y_{t+7}=f(X^t)$ for disaster prediction. However, the true purpose is not ``knowing the exact water level on the 7th day'' but understanding ``how the disaster might unfold; if not on the 7th day, then what about the 8th, 9th, and so on?''
Today, advanced techniques capable of addressing this question have emerged, especially with the success of RNNs in computing \textbf{\emph{timing distributions}} through latent space representations. Consequently, it is time to reevaluate the conventional definitions and learning paradigm of causality, which were originally established on an ``object-first'' basis.


%Over the years, there have been continual progresses toward mathematically symbolizing our innate ``relation-first'' understanding. However, it cannot be truly achieved under the epistemological framework. %However, truly achieving this remains unattainable within the confines of the framework of epistemology.

Indeed, there has been ongoing progress in mathematically representing the concept of \emph{timing distribution} as aligned with our understanding, but a clear and explicit definition remains elusive. Largely due to being entrenched in epistemology, the timestamp $t$ can only be allowed as a priorly determined ``constant'' in models rather than a ``variable'' with computable values.
This paper seeks to address this issue fundamentally.

The Markov process established the sequence of independent states over time, denoted as $X^t = (X_1, \ldots, X_t)$, like captured through a series of snapshots. 
Information-theoretic measurements of causality (e.g., directed information \cite{massey1990causality}, transfer entropy \cite{schreiber2000measuring}) linguistically emphasized the distinction between perceiving $X^t$ as ``a sequence of \emph{static} states'' versus holistically as ``a \emph{dynamic} transition process''.
The introduction of do-calculus \cite{pearl2012calculus} marks a significant advancement, with the notation $do(X_t)$ explicitly treating the action of ``becoming $X_t$'' as a \emph{dynamic unit}. However, its nature in differential calculus necessitates each ``identifiable'' $do(X)$ be manually specified.
On the other hand, the notation $do(Y_t)$, which implies a ``possibly valued'' $t$, lacks a foundation for declaration. It can only be described in terms of the distribution of future states ``informatively constrained'' by current ones
\cite{hoel2013quantifying, hoel2017map}, but still risks being critiqued for lacking ``metaphysical commitments'' \cite{eberhardt2022causal}.


% However, as a final endeavor grounded in epistemology, do-calculus possesses two major limitations: 1) $do(X)$ must be ``identifiable'', and 2) confined to specifying ``interventions'' only. 
% The former implies ``manually making sure the \emph{dynamics} over $t$-dimension to be differentiable'' in practice, while the latter restricts its use to the cause $X$, excluding the result $Y$.
% This is understandable since, from the perspective of an ``observer'', the timestamp $t$ can only act as a priorly determined ``constant'' in models, rather than a ``variable'' with computable values.
% Consequently, the notation $do(Y)$ implying possibly valued $t-1$ and $t$ cannot be declared or linguistically referred to, otherwise risking a lack of ``metaphysical commitments'' \cite{eberhardt2022causal}.

This paper doesn't intend to engage in metaphysical debates; rather, it aims to highlight that for technological inquiries, shifting from an epistemological to a ``creator's'' perspective can produce models that align with our instinctive understanding, and also dramatically simplify the notations. These aspects are especially crucial in the context of causal-reasoning AI or AGI. For purely philosophical discussions, readers are encouraged to explore the ``creationology'' theory by Mr.Zhao Tingyang.



\subsection{Mathematical Definition of Relation}

% Figure environment removed

A statistical model is typically defined through a function $f(x\mid \theta)$ that represents how a parameter $\theta$ is \emph{functionally related} to potential outcomes $x$ of a random variable $X$ \cite{ly2017tutorial}.
For instance, the coin flip model is also known as the Bernoulli distribution $f(x\mid \theta)=\theta^{x}(1-\theta)^{1-x}$ with $x\in \{0,1\}$, which relates the coin's propensity (i.e. its inherent possibility) $\theta$ to $X=$ ``land heads to the potential outcomes''. Formally, given a known $\theta$, the \emph{functional relationship} $f$ yields a probability density function (pdf) as $p_{\theta}(x)=f(x\mid\theta)$, according to which, $X$ is distributed and denoted as $X\sim f(x;\theta)$.
The Fisher Information $\mathcal{I}_X(\theta)$ of $X$ about $\theta$ is defined as $\mathcal{I}_X(\theta)=\int_{\{0,1\}}(\frac{d}{d\theta}log(f(x\mid\theta))^2 p_{\theta}(x)dx$, with the purpose of building models on the observed $x$ data being to obtain this information. For clarity, we refer to this initial perspective of understanding functional models as the \emph{\textbf{relation-first} principle}.

In practice, we do not limit all functions to pdfs but often shape them for easier understanding. For instance, let $X^n=(X_1,\ldots,X_n)$ represent an $n$-trial coin flip experiment, while to simplify, instead of considering the random vector $X^n$, we may only record the number of heads as $Y=\sum_{i=1}^n X_i$.
If these $n$ random variables are assumed to be independent and identically distributed (i.i.d.), governed by the identical $\theta$, the distribution of $Y$ (known as binomial) that describes how $\theta$ relates to $y$ would be $f(y\mid\theta)=\big(\begin{smallmatrix} n \\ y \end{smallmatrix}\big) \theta^y(1-\theta)^{n-y}$. In this case, the conditional probability of the raw data, $P(X^n\mid Y=y,\theta)=1/\big(\begin{smallmatrix} n \\ y \end{smallmatrix}\big)$ does not depend on $\theta$. This means that once $Y=y$ is given, the conditional probability of $X^n$ becomes independent of $\theta$, although $X^n$ and $Y$ each depend on $\theta$ individually. 
It concludes that no information about $\theta$ remains in $X^n$ once $Y=y$ is observed \cite{fisher1920012, stigler1973studies}, denoted as $EI(X^n\rightarrow Y)=0$ in the context of relationship modeling. However, in the absence of the i.i.d. assumption and by using a vector $\vartheta=(\theta_1,\ldots,\theta_n)$  to represent the propensity in the $n$-trial experiment, we find that $EI(X^n\rightarrow Y)>0$ with respect to $\vartheta$.
Here, we revisit the foundational concept of Fisher Information, represented as $\mathcal{I}_{X\rightarrow Y}(\theta)$, to define:


\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{
\begin{minipage}{0.95\textwidth}
\paragraph{Definition 1.} 
A relationship denoted as $X \xrightarrow{\theta} {Y}$ is considered meaningful in the modeling context due to an \emph{informative \textbf{relation}} $\theta$, where $\mathcal{I}_{X\rightarrow Y}(\theta)>0$, simplifying as $\mathcal{I}(\theta) >0$.
\end{minipage}}
%\vspace{-0.8mm}


%its \emph{\textbf{dynaimc} outcome} is defined as $\mathcal{Y} = \operatorname{argmax}_{\mathcal{Y}} \ \mathcal{I}(\theta)$
% between \emph{\textbf{dynamic}} objects $\mathcal{X}$ and $\mathcal{Y}$

Specifically, rather than confining within a function $f(;\theta)$ as its parameter, we treat $\theta$ as an individual variable to encapsulate the effective information (EI) as outlined by Hoel. 
Consequently, the \emph{relation-first principle} asserts that a relationship is characterized and identified by a specific $\theta$, regardless of the appearance of its outcome $Y$, leading to the following inferences:
\begin{enumerate}[itemsep=0em, topsep=-0pt, 
parsep=2pt, partopsep=0pt,
leftmargin=22pt, labelwidth=10pt]
    \item $\mathcal{I}(\theta)$ inherently precedes and is independent of any observations of the outcome, as well as the chosen function $f$ used to describe the outcome distribution $Y\sim f(y;\theta)$.
    \item In a relationship represented by $\mathcal{I}(\theta)$, $Y$ is only used to signify its potential outcomes, without any further ``information'' defined by $Y$.
    \item In AI modeling contexts, a relationship exists as the representation of $\mathcal{I}(\theta)$, which can be stored, applied, and refined across various scenarios to generate observations accordingly.
    \item As $Y$ is determined by $\mathcal{I}(\theta)$, $X$ is also governed by the preceding relationships, which can manifest as either observed $x$ data or established representations in the current relationship modeling.
\end{enumerate}


%Instead of ``defining relation by states'', we propose ``defining dynamics (encompassing all significant states and actions) by relation.'' For observational variables $X\in \mathbb{R}^n$ and $Y\in \mathbb{R}^m$, let's incorporate their \emph{timing dimensions}, $t$ and $\tau$, to formulate \emph{\textbf{dynamic} variables} $\mathcal{X}=\langle X,t \rangle \in \mathbb{R}^{n+1}$ and $\mathcal{Y}=\langle Y,\tau \rangle \in \mathbb{R}^{m+1}$; and introduce the abstract variable $\theta$ to represent the effective information $EI(\mathcal{X} \rightarrow \mathcal{Y})$ defined by Hoel.


\vspace{1mm}
{\vskip 0pt\bf About Relation $\theta$} 
\vspace{-0.5mm}

As emphasized by the Common Cause principle \cite{dawid1979conditional}, ``any nontrivial conditional independence between two observables requires a third, mutual cause'' \cite{scholkopf2021toward}. The critical factor here, however, is ``nontrivial'' rather than ``cause'' itself. For a system involving $X$ and $Y$, if their connection (i.e., under what conditions they become independent) is significant enough to call for a description, it must include information beyond the statistical dependencies present in the system.
We use an abstract variable $\theta$ to signify such an informative connection between $X$ and $Y$, unnecessarily referring to tangible entities.
%Section \ref{} will provide practical examples to enhance empirical understanding of $\theta$.
% In our context, this translates to: If $\theta$ represents the critical connection between $X$ and $Y$, its informativeness becomes essential for rendering the system comprising $\{X,Y\}$ as a meaningful relationship for modeling.

Traditionally, descriptions of relationships are constrained by objective notations and focus on ``observable states at specific times''. 
For example, quantifying EI requires its attribution to a state-to-state transition probability matrix $S$ \cite{hoel2013quantifying}. 
However, $EI(S)$ cannot be solely defined by $S$; it must also account for how the current state $s_0=S$ or action $do(s_0=S)$ is related to the probability distributions of past and future states, $S_P$ and $S_F$, respectively. In practice, its distinction from mere statistical dependence relies on manual specification.
The rise of representation learning technology offers a shift towards abstracting informative entities beyond mere observational descriptors, irrespective of whether the values representing them possess concrete interpretations. This lays the groundwork for independently conceptualizing $\mathcal{I}(\theta)$.

For a clearer empirical understanding of $\theta$, consider the following example:
A sociological study investigates interpersonal ties through consumption data. Bob and Jim, a father-son duo, have consistent expenditures on craft supplies, suggesting the father's influence on the son's hobbies. However, the ``father-son'' relational information, represented by $\mathcal{I}(\theta)$, exists only in our perception and cannot be deduced directly from the data alone.
Traditional object-first approaches require manual labeling of data points to identify specific outcomes. Conversely, in a relation-first modeling paradigm, abstract knowledge is stored and reused in the form of $\theta$ representation, facilitating autonomous identifications.
%Moreover, $\mathcal{I}(\theta)$ defines the roles of entities within a specific relationship. For instance, Jim's peers with matching video game expenditure patterns could signify their friendships. The roles of ``son'' or ``friend'' are identified through specific $\theta$.
%For example, similar spending patterns on video games among Jim's peers might indicate friendships
%thoroughly collects detailed consumption records of every community member and utilizes AI to study the impact of interpersonal relationships. 
%We can study Jim in the roles of ``son'' and ``friend,'' but it is impossible to discuss ``What is Jim?'' independently of all relationships - this might be a question for biology, physics, or religion, but it certainly is not an engineering question.

More importantly, the existence of $\mathcal{I}(\theta)$, as well as its potential extraction, are not limited between solely \emph{observational} distributions denoted by variables $X$ and $Y$, but can extend to the \emph{timing} dimension - For causal relationships, the significant timing-dimensional distribution of the outcome is essential in differentiating them from mere correlations.
Specifically, we use $\mathcal{X}$ and $\mathcal{Y}$ to indicate their integration with the timing dimension, and represent a relationship in the general form $\mathcal{X}\xrightarrow{\theta}\mathcal{Y}$.
Below, we introduce $\mathcal{X}$ for a general discussion on such variables, followed by incorporating $\mathcal{Y}$ to identify the outcome of the relationship model.


\vspace{2mm}
{\vskip 0pt\bf About Dynamic Variable $\mathcal{X}$} 
\vspace{-0.5mm}


\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{
\begin{minipage}{0.95\textwidth}
\paragraph{Definition 2.} For a variable $X\in \mathbb{R}^n$ observed as a time sequence $x^t=(x_1,\ldots,x_t)$, a \emph{\textbf{dynamic}} variable $\mathcal{X}=\langle X,t \rangle \in \mathbb{R}^{n+1}$ is formulated by integrating $t$ as the \emph{timing dimension}.
\end{minipage}}
%\vspace{-0.8mm}

Time series data analysis is often referred to as being ``spatial-temporal'' \cite{andrienko2003exploratory}.
However, in modeling contexts, ``spatial'' is interpreted broadly and not limited to physical spatial measurements (e.g., geographic coordinates); thus, we prefer the term ``observational''. Furthermore, to avoid the implication of ``short duration'' often associated with ``temporal,'' we use ``timing'' to represent the dimension $t$. 
Unlike the conventional representation in time series $X^t=(X_1, \ldots, X_t)$ with timestamp $t$, we consider $\mathcal{X}$ holistically as a \emph{dynamic} variable, similarly for $\mathcal{Y}=\langle Y,\tau \rangle \in \mathbb{R}^{m+1}$. 
The probability distributions of $\mathcal{X}$, as well as $\mathcal{Y}$, span both \emph{observational} and \emph{timing} dimensions simultaneously.

Specifically, $\mathcal{X}$ can be viewed as the integral of $X_t$ or $do(X_t)$ over the timing dimension $t$ within a required range. The necessity to represent it by $do(X_t)$ rather than $X_t$ is referred to as the \emph{\textbf{dynamical significance}} of $\mathcal{X}$. 
Put simply, if $\mathcal{X}$ can be expressed as $\mathcal{X} = \sum_{1}^t X_t$, then it is equal to $X^t = (X_1, \dots, X_t)$ in modeling. Otherwise, $\mathcal{X} = \int_{-\infty}^{\infty} do(X_t) dt$ indicates $\mathcal{X}$ to be \emph{dynamically significant}, characterized by dependencies among the states $\{X_{t-1}, X_t\}$ with an unconstrained $t\in (-\infty,\infty)$. 
In essence, while including the current state $X_t$, $do(X_t)$ can be seen as additionally integrating the state-dependence from $X_{t-1}$ to $X_{t}$, forming a differential unit within the \emph{timing distribution}.
This concept aligns with the ``state-dependent'' and ``state-independent'' analysis that Hoel discusses as in causal emergence \cite{hoel2013quantifying}.



\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{
\begin{minipage}{0.95\textwidth}
\paragraph{Theorem 1.} Timing becomes a necessary \emph{computational dimension} if and only if the variable in question possesses \emph{dynamical significance}, characterized by a \emph{\textbf{nonlinear} distribution} across the timing. 
\end{minipage}}
\vspace{-1mm}

In simpler terms, if a distribution over timing $t$ cannot be adequately represented by a function of the form $x_{t+1}=f(x^t)$, then its nonlinearity is significant to be considered. Here, $[t,t+1]$ indicates any predetermined time lag, set at a constant value. 
RNN models can effectively extract the nonlinearly significant representation of $\mathcal{X}$
from the sequence $X^t$ and autonomously achieve $\mathcal{X}\xrightarrow{\theta}Y$, by leveraging the relational constraint indicated by $\mathcal{I}(\theta)$, i.e., indexing through $\theta$.
Conversely, if ``predicting'' such an irregular timing distribution is crucial, it implies that it has been identified as the causal effect of some underlying reason.

\vspace{2mm}
{\vskip 0pt\bf About Dynamic Outcome $\mathcal{Y}$} 
%\vspace{-0.5mm}


%\vspace{2mm}
%\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=red!50!black, colback=yellow!10]{
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{
\begin{minipage}{0.98\textwidth}
\paragraph{Theorem 2.} In modeling contexts, identifying a relationship $\mathcal{X} \xrightarrow{\theta} \mathcal{Y}$ as \emph{Causality}, distinct from mere \emph{Correlation}, depends on the \emph{\textbf{dynamical significance} of the outcome} $\mathcal{Y}$ as required by this relationship. 
\end{minipage}}
%\vspace{-1mm}

Figure \ref{fig:symbolic} illustrates the distinction between causality and correlation, where an arrow indicates an informative relation and a dashed line means statistical dependence. 
If conducting the integral operation for both sides of the do-calculus formation $X/do(X)\rightarrow Y$ over timing, we can achieve $\mathcal{X}\rightarrow \sum_{1}^{\tau}Y_{\tau}$ with the variable $\mathcal{X}$ allowing to be dynamically significant but the outcome $\sum_{1}^{\tau}Y_{\tau}$ certainly not.
Essentially, to guarantee $\mathcal{Y}$ presenting in form of sequence $y^\tau=(y_1,\ldots,y_{\tau})$ with constant timestamps $\{1,\ldots,\tau\}$, do-calculus manually conducts a differentiation operation on the relational information $\mathcal{I}(\theta)$ to discretize the outcome timing distribution. 
This process is to confirm specific $\tau$ values at which $y_{\tau}$ can be identified as the effect of a certain $do(x_t)$ or $x_t$. Accordingly, the static $y_{\tau}$ value will be defined as either the interventional effect $f_V(do(x_t))$ or the observational effect $f_B(x_t)$, with three criteria in place to maintain conditional independence between these two possibilities, as identified by tangible $\Delta\mathcal{I}(\theta)$ elements (i.e., identifiable $do(x_t)\rightarrow y_{\tau}$ or $x_t\rightarrow y_{\tau}$):

\vspace{-6mm}
\[
\mathcal{Y}=f(\mathcal{X}) = \sum_t f_V(do(x_t)) \cdot f_B(x_t) = \sum_t\left\{
\begin{array}{lll}
    f_B(x_t)=y_{\tau} & \text{ with } f_V(do(x_t))=1 \text{ (Rule 1)}&\multirow{3}{*}{$\left.\begin{matrix}\\ \\ \\ \end{matrix}\right\}=\displaystyle\sum_{\tau} y_{\tau}$} \\
    f_V(do(x_t))=y_{\tau} & \text{ with } f_B(x_t)=1 \text{ (Rule 2)}& \\
    0=y_{\tau} & \text{ with } f_V(do(x_t))=0 \text{ (Rule 3)}& \\
    \text{otherwise} & \text{ not identifiable } &
\end{array}\right.
\]
\vspace{-5mm}


Particularly, the proposed \emph{dynamic} notations $\mathcal{X}=\langle X,t \rangle$ and $\mathcal{Y}=\langle Y,\tau \rangle$ offer advantages in two respects: 1) the concept of $do(Y_{\tau})$ can be introduced with a computable $\tau$ indicating its possible timing, which cannot stand in the context of causal inference, and 2) by incorporating the timing dimension, $\mathcal{X}$ and $\mathcal{Y}$ transcend the conventional notion of ``timestamps'', eliminating the need to distinguish between ``past and future''.


%No matter whether identified as causality or correlation, both of them are informative relationships just of distinct types, distinguished from mere statistical dependence where $\mathcal{I}(\theta)=0$. 

\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{
\begin{minipage}{0.95\textwidth}
\paragraph{Definition 3.} 
A \textbf{\emph{causality}} characterized by a \emph{dynamically significant outcome} $\mathcal{Y}$ can encompass multiple \textbf{\emph{causal components}}, represented by $\vartheta=(\vartheta_1,\ldots,\vartheta_{T})$. 
Each $\vartheta_{\tau}$ with $\tau\in\{1,\ldots,T\}$ identifies a distinct \textbf{\emph{timeline}} $\tau$ to accommodate its corresponding \textbf{\emph{outcome component}} $\mathcal{Y}_{\tau}$. 

The overall outcome is denoted as $\mathcal{Y} = \sum_{\tau=1}^T \mathcal{Y}_{\tau} = \sum_{\tau=1}^T \int do(Y_{\tau}) d\tau$, simplifying to $ \oint do(Y_{\tau}) d\tau$.
\end{minipage}}
\vspace{1mm}

Definition 3 is grounded in the relation-first principle, emphasizing causality as indicated by $\vartheta$ (different from $\theta$), which determines the appearance of outcome $\mathcal{Y}$ to be dynamically significant. 
Specifically, within a general relationship $\mathcal{X}\xrightarrow{\theta}\mathcal{Y}$, the dynamic outcome $\mathcal{Y}$ showcases its capability as a variable to encompass nonlinear distribution over timing, whereas $\mathcal{X}\xrightarrow{\vartheta}\mathcal{Y}$ confirms such nature of this relationship.

According to Theorem 1, adding timing as a computational dimension is only meaningful for relationships as causality. Simplified, if a relationship model is expressed as $f(\mathcal{X};\theta)=Y^{\tau}=(Y_1,\ldots, Y_{\tau})$, it is equal to a model $f(\mathcal{X};\theta)=Y$ with a static outcome, but applied $\tau$ times in sequence, implying that $\mathcal{X}\xrightarrow{\theta}Y$ can adequately represent the modeled relationship.
Frequently, it goes unnoticed that incorporating a sequence $X^t=(X_1,\ldots,X_t)$ in modeling does not mean the $t$-dimension is computationally active, where $t$ serves as a fixed constant, lacking computational flexibility. The same way also applies to $Y^{\tau}$.

However, once including the ``possible timing'' $\tau$ with computable values, it becomes necessary to account for the potential multiple components of $\mathcal{Y}$ that dynamically develop separately over their own timing, termed as individual \emph{timelines}.
For a simpler understanding, let's revisit the example of ``storm causes flooding.''
Suppose $\mathcal{X}$ represents the storm, and for each watershed, $\vartheta$ encapsulates the effects of $\mathcal{X}$ determined by its unique hydrological conditions. Let $\mathcal{Y}_2$ denote the water levels observed over an extended period, such as the next 30 days, if without any flood prevention. Let $\mathcal{Y}_1$ indicate the daily variations in water levels (measured in $\pm$cm to reflect increases or decreases) resulting from flood-prevention efforts. 
In this case, $\vartheta$ can be considered in two components: $\vartheta=(\vartheta_1, \vartheta_2)$, separately identifying $\tau=1$ and $\tau=2$.

Specifically, historical records of disasters without flood prevention could contribute to training $\vartheta_2$, based on which, $\vartheta_1$ can be trained using recent records of flood prevention.
Even if their hydrological conditions are not exactly the same, AI can represent such relational difference between $\vartheta_1$ and $\vartheta_2$. 
This is because the capability of computing over timing empowers AI to extract the common relational information across different timelines.
%relational information from dynamically significant $\mathcal{Y}$, by including its timing-dimensional distribution into computations.
From AI's standpoint, regardless of whether the flood crest naturally occurs on the 7th day or is dispersed over the subsequent 30 days, both $\mathcal{Y}_2$ and $(\mathcal{Y}_1+\mathcal{Y}_2)$ are linked to $\mathcal{X}$ by the same volume of water introduced by $\mathcal{X}$. In other words, while AI deals with the computations over timing, discerning what qualifies as a ``disaster'' remains humans' determination.

Conversely, in traditional modeling,  $\vartheta$ is often viewed as another causal object, named a ``confounder'' (i.e., common cause) of $\mathcal{X}$ and $\mathcal{Y}$. Therefore, when $\vartheta_1$ and $\vartheta_2$ represent varied conditions, their difference is considered as an unknown object, and naturally requires manual adjustments on the data or even the experiments, to achieve identical $\vartheta_1$ and $\vartheta_2$ ensuring comparable outcome sequences $Y^{\tau}_1$ and $Y^{\tau}_2$. 
For example, in clinical trials, patient groups receiving different treatments must be ``randomized'' similarly to maintain comparability. This is essentially for eliminating the dynamical significance of the outcomes.


%Under the new paradigm, the goal of modeling is to capture the relations reflected by cause and effect, even under varied conditions, with the interpretation of relational variations being a task for humans. 
%In traditional modeling, $\vartheta$ is often viewed as a ``hidden confounder (i.e., common cause),'' implying unknown factors like ``underlying hydrological conditions''. 
%Therefore, manual adjustments are often required to eliminate potential influences of the unknown $\vartheta$, to ensure the computed outcome components are comparable. 


\vspace{2mm}
{\vskip 0pt\bf About Dependence $\phi$ between Causal Components} 
%\vspace{-0.5mm}

Based on Definition 3, the causal emergence phenomenon in Figure~\ref{fig:macro}(b) can be clearly explained as in (c). The two causal components $\vartheta_1$ and $\vartheta_2$ result in dynamic outcome components $do(Y)_1$ and $do(Y)_2$, acting as differentiated $\mathcal{Y}_1$ and $\mathcal{Y}_2$ respectively. As $EI(\phi)=0$, no further relational information exists between $do(Y)_1$ and $do(Y)_2$. However, as dynamics, their statistical dependence can be dynamically significant.

\vspace{0.5mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{
\begin{minipage}{0.95\textwidth}
\paragraph{Theorem 3.} Sequential causal modeling is required, if the \textbf{\emph{dependence}} between causal components, represented by $\phi$, has dynamically significant impact on the outcome components.
\end{minipage}}
%\vspace{-1mm}

The sequential modeling procedure is demonstrated in the analysis of the ``flooding'' example, where the training of $\vartheta_1$ relies on the previously established $\vartheta_2$ to form meaningful representations. That is, the dependence between $\vartheta_1$ and $\vartheta_2$ represented by $\phi$ has impacts on the timing dimension of their dynamic outcomes $\mathcal{Y}_1$ and $\mathcal{Y}_2$. As a result, the timelines of $\mathcal{Y}_1$ and $\mathcal{Y}_2$ must be determined sequentially, depending on the meaningful $\mathcal{I}(\vartheta_1\mid \vartheta_2)$ or $\mathcal{I}(\vartheta_2\mid \vartheta_1)$ tailored to specific applications.


In Figure~\ref{fig:macro}(c), suppose $\mathcal{Y}_2$ depends on $\mathcal{Y}_1$, the modeling process could be in two-step: $\mathcal{Y}_1=f_1(\mathcal{X};\vartheta_1)$ followed by $\mathcal{Y}_2=f_2(\mathcal{X}\mid \mathcal{Y}_1;\vartheta_2)$. 
Remarkably, according to the adopted perspective, its informative explanation can be in different ways. 
Under the relation-first principle (i.e., in the creator's perspective), $\mathcal{I}(\vartheta)=\mathcal{I}(\vartheta_1)+\mathcal{I}(\vartheta_2) = 2\mathcal{I}(\vartheta_1)+\mathcal{I}(\vartheta_2\mid \vartheta_1)$ encapsulates all information needed to ``create'' the outcome $\mathcal{Y}=\mathcal{Y}_1+\mathcal{Y}_2$, with $\mathcal{I}(\phi)=0$ uninformatively indicating a dependence. While adopting the traditional information expression (i.e., from the observer's perspective), $\vartheta_1$ and $\vartheta_2$ simply denote functional parameters without \emph{observational information} associated, and accordingly, we have $\mathcal{I}(\phi\mid \mathcal{Y}_1)=\mathcal{I}(\mathcal{Y}_2)-\mathcal{I}(\mathcal{Y}_1)>0$.

For clarity, we denote $\vartheta_1 \indep \vartheta_2$ to signify the timing-dimensional independence between $\mathcal{Y}_1$ and $\mathcal{Y}_2$, termed as \textbf{\emph{dynamical independence}}, without altering the conventional understanding within the observational space, like $Y_1\indep Y_2\in \mathbb{R}^m$. On the contrary, $\vartheta_1 \nindep \vartheta_2$ implies a \textbf{\emph{dynamical interaction}} between $\mathcal{Y}_1$ and $\mathcal{Y}_2$, while ``whether \emph{dynamically} independent or interacted'' only holds for \emph{dynamically significant} $\mathcal{Y}_1$ and $\mathcal{Y}_2$.
%are causal, underscoring the dynamical significance of their outcomes. 

% Figure environment removed


For better illustration, we draw the examples in Figure~\ref{fig:interact}, by upgrading the conventional Directed Acyclic Graph (DAG) in two aspects: 1) nodes represent static values of data instances (i.e., data points or variable realizations), and 2) edge lengths reflect the timespan to achieve these values. This allows for the visualization of dynamical differences among instances, influenced by the same causal effect (or component), as if captured in a snapshot. For instance, Figure~\ref{fig:interact}(c) indicates $\vartheta_1$ and $\vartheta_2$ are inversely associated about timing, meaning that a quicker achievement of $y_1$ implies a slower one for $y_2$.

%a certain effect magnitude, signified by a static value. %the effects to reach an equivalent magnitude.


\subsection{Potential Development Toward AGI}

As demonstrated, choosing between the observer's or the creator's perspective depends on the questions we are targeting, rather than a matter of conflict. In the former, information is gained from observations and represented by observables; while in the latter, relational information preferentially exists as representing the knowledge we aim to construct in modeling, such that once the model is established, we can use it to deduce outcomes as a description of ``possible observations in the future'' without direct observation.

Causality questions inherently require the creator's perspective, since ``informative observations'' cannot exist because of nothing. Empirically, it is reflected as the challenge of specifying outcomes in traditional causal modeling, often referred to as ``identification difficulty'' \cite{zhang2012identifiability}.
As mentioned by \cite{scholkopf2021toward}, ``we may need a new learning paradigm'' to depart from the i.i.d.-based modeling assumption, which essentially asserts the objects we are modeling exactly exist as how we expect them to. We term this conventional paradigm as \emph{object-first} and have introduced the \emph{relation-first} principle accordingly.

The \emph{relation-first} thinking has emerged in the definition of Fisher Information, as well as in do-calculus that differentiates relational information.
Moreover, neural networks with the back-propagation strategy achieved its technological realization.
Therefore, it's unsurprising that the advent of AI-based representation learning signifies a turning point in causality modeling. As noted by \cite{rumelhart1986learning}, ``a simple procedure in a general purpose is powerful enough.''
%However, instead of directly solving the longstanding questions about causality, this powerful approach needs a fundamental shift away from traditional ``object-first'' thinking, making these perplexing questions disappear by changing our perspective.
Sometimes, the resolution of problems is not finding an answer, but an approach that makes the problem disappear (from Ludwig Wittgenstein's philosophical works).

% Figure environment removed
\vspace{-2mm}

From an engineering standpoint, answering the ``what ... if?'' (i.e., counterfactual) questions indicates the capacity of predicting $do(Y)$ actions, implying a dynamically significant relationship outcome. Intriguingly, learning dynamics (i.e., the realization of $do(\cdot)$) and predicting outcomes (i.e., facilitating the role of $Y$) present a paradox under the traditional \emph{object-first} learning paradigm, as in Figure \ref{fig:overview}.


\vspace{2mm}
{\vskip 0pt\bf About AI-based Dynamical Learning} 
%\vspace{-0.5mm}

Learning from dynamics, in particular, is a significant instinctive ability of humans. Representation learning fulfills optimizing over timing dimension, notably achieving such capabilities, especially in large language models (LLMs) \cite{gurnee2023language}, which evoke discussions about achievements toward AGI \cite{schaeffer2023emergent}. 
Especially, using meta-learning in LLMs \cite{lake2023human} has facilitated clusters of significant dynamic components, showcasing the potential for human-like knowledge generalization capabilities.
However, it has also been highlighted that LLMs lack a true understanding of causality \cite{pavlick2023symbols}.

The complexity of causality modeling lies in the dynamical interactions within a ``possible world'', not just in computing possibilities, whether they are dynamical or not.
Instead of a single question, ``what ... if?'' stands for a self-extending logic, where the ``if'' condition can be repeatedly applied to modeled possibilities. 
Hence, causality modeling aims to uncover the unobservable information or knowledge implied by the observed $X/do(X) \rightarrow Y/do(Y)$ phenomenon, enabling $Y/do(Y)$ as its outcome beyond direct observation. 

Advanced technologies, such as reinforcement learning \cite{arora2021survey} and causal representation learning, have blurred the identification boundaries between causal variables and outcomes, which are manually maintained in traditional causal inference.
The major focus often lies on the improved efficacy in learning dynamics, yet it is frequently overlooked that the foundational RNN architecture aims to accomplish $do(X) \rightarrow Y$ without addressing $do(Y)$. 

Essentially, the significant dynamics (including entities, events, and even deductions) that are autonomously extracted by AI from sequential input data are dedicated to dynamically meaningful $do(X)$. 
Because with no interactions, they simply exist as a cluster of individual components rather than constructing a ``possible world'' as characterized by causality. In other words, the potential interactions between dynamics dictate the need for us to identify them as dynamics, distinguish their timelines, and discuss the potential structures - In some ways, these necessities impose ``causality'' as a terminology.



%It is often overlooked that
%Frequently, it goes unnoticed that
%It is commonly disregarded that
%Many tend to overlook that

\vspace{2mm}
{\vskip 0pt\bf About Static Outcomes in Causal Inference} 
%\vspace{-0.5mm}

The causal inference methodology and structural causal models (SCMs) concentrate on causal structures, emphasizing relations prior to observation. However, the \emph{object-first} paradigm limits our model outcomes to be confined within the observational space, denoted as $Y_{\tau}\in \mathbb{R}^m$, with a predetermined future timestamp $\tau$.
As illustrated in Figure \ref{fig:macro}, the under-represented relational information, indicated by $\mathcal{I}(\vartheta)-\mathcal{I}(\theta)$, ``emerges'' to be with $\phi$, thereby becoming an informative relation in a ``possible world'', implying indeducible.

Such ``causal emergence'' requires significant efforts on theoretical and empirical interpretations.
Particularly, the unknown relation $\phi$ is often attributed to the well-known ``hidden confounder'' problem \cite{greenland1999confounding, pearl2000models}, linked to the fundamental assumptions of causal sufficiency and faithfulness \cite{sobel1996introduction}. In practice, converting causal knowledge represented by DAGs into operational causal models requires meticulous effort \cite{elwert2013graphical}, where data adjustments and model interpretations often rely on human insight \cite{sanchez2022causal, crown2019real}.
These accomplishments form the foundation of causal inference's value in the era dominated by statistical analysis, before the advent of neural networks.
Currently, leveraging representation learning enables the autonomous extraction of relational information in AI modeling. 
%Given a specific $\mathcal{X}$ as the cause, all potential information required to establish relationships towards a possible $\mathcal{Y}$ is encapsulated by $\mathcal{I}(\vartheta)$.

%Conversely, under the \emph{relation-first} principle,  This is rooted in the observations of $\mathcal{X}$ and aligns with our causal comprehension.

\vspace{2mm}
{\vskip 0pt\bf About Development of Relation-First Paradigm} 
%\vspace{-0.5mm}

As highlighted in Theorem 3, causal learning must be intentionally sequential, in alignment with the \emph{relation-first} principle. When modeling with prior knowledge, the targeted relation $\vartheta$ should be determined first, and then, the sequential input and output data, $x^t=(x_1,\ldots,x_t)$ and $y^{\tau}=(y_1,\ldots,y_{\tau})$, can be chosen to enable AI to purposefully extract $\mathcal{I}(\vartheta)$ between them.
While for AI-detected meaningful $do(X)$, we should delve into its significance - questioning ``if it suggests a $do(Y)$, what $\mathcal{I}(\vartheta)$ have we extracted?'' The gained insights guide the decision on whether and how to proceed with the next round of exploration based on it.




% indifferent to the modeling process itself.
% The informativeness of achieved models hinges on how well they represent specific knowledge as resonating with our comprehension. For example, to extract ``father-son'' relational features, we can use records of ``fathers'' and ``sons'' as input and output, although from the AI's standpoint, they are indistinguishable from those of unrelated individuals to learn mere dependency.


% AI-based representation learning methods act as tools to extract relational information $\mathcal{I}(\theta)$.
% However, how informative the extracted $\mathcal{I}(\theta)$ is determined by the intended human knowledge.

% Specifically, our decisions are necessary to infuse particular relational information into the representations derived by AI.

% Figure environment removed


This way, relational representations within the latent space can be accumulated as vital ``resources,'' organized and managed via structured graphical indices, as depicted in Figure \ref{fig:new}. 
This flow mirrors human learning processes \cite{sep-mental-representation}, with these indices corresponding to causal DAGs in our comprehension. 
Over time, knowledge across different domains can be systematically categorized and accessed like a library system, where the established representations can be continuously optimized and refined across various scenarios.

As humans, we do not need to access latent features directly, encapsulated within the ``black box'' of the AGI system. The indexing process translates the inquired relationship into a specific input-output routine of representations, guiding the data reconstruction process to generate observations that humans understand. Despite convenience, this can place the ``intelligence'' of computers under more effective control.

\section{Modeling Framework in Creator's Perspective}

Under the traditional i.i.d.-based modeling framework, questions must be addressed individually, even when they share similar underlying knowledge.
The modeling process usually incorporates unverifiable assumptions about objective reality, including but not limited to the i.i.d., often unnoticed due to the conventional \emph{object-first} thinking. These fundamental issues are further exposed by the advanced flexibility of neural networks, particularly, identified as the challenges to \emph{model generalizability} by Scholkopf \cite{scholkopf2021toward}. They also introduce the concept of \emph{causal representation} learning, underscoring the importance of prioritizing causal relational information before specifying observables.

Rather than raising new methods, we aim to emphasize the \textbf{\emph{shift in perspective}} that enables modeling across the ``possible timing space,'' beyond the observational space limits defined by the traditional paradigm. Furthermore, a hyper-dimensional space is embraced to accommodate the abstract variables representing the informative relations when adopting the creator's perspective. The complete view is depicted in Figure \ref{fig:space}, where the notion of $\omega$ will be introduced in the following. 



% Figure environment removed
\vspace{-2mm}

For a general relationship $\mathcal{X}\xrightarrow{\theta}\mathcal{Y}$, the causal variable $\mathcal{X}\in\mathbb{R}^O$ spans across observational dimensions in $\mathbb{R}^O$, including $t$ that signifies \emph{observed timing}. Conversely, the outcome $\mathcal{Y}$, representing a ``possible world,'' extends across all dimensions in $\mathbb{R}^O$ \emph{except} $t$, and into the \emph{possible timing space} $\mathbb{R}^T$, denoted as $\mathcal{Y}\in\mathbb{R}^{O-1}\cup\mathbb{R}^T$. The concept of ``observed timing vs. possible timing'' is akin to the notion of ``absolute timing vs. relative timing''  \cite{wulf1994reducing, shea2001effects}. However, we focus on distinguishing between the \emph{single-dimensional} $t$ due to be observed, and the potentially \emph{multi-dimensional} $\tau$ leading a space $\mathbb{R}^T$.

The implication of $\tau$ depends on our perspective, either as the creator or the observer, and is directly linked to whether including $\mathbb{R}^H$ in view.
From the creator's perspective, the abstract variable $\theta\in\mathbb{R}^H$ is particularly to accommodate relational information $\mathcal{I}(\theta)$, without specifiable distributions. The number of components of $\mathcal{Y}$, denoted as $T$, is determined by the given $\mathcal{X}$ and $\mathcal{I}(\theta)$, represented by $\mathcal{Y}=\sum_{\tau=1}^T \mathcal{Y}_{\tau}$. The notion of $\tau=\{1,\ldots,T\}$ sequentially refers to timings of these components, termed $T$ \emph{timelines} - they act as axes, forming a $T$-dimensional space, $\mathbb{R}^T$, in accordance with what the given relationship $\mathcal{X}\xrightarrow{\theta}\mathcal{Y}$ requires.

On the other hand, when taking the observer's perspective to view the computed outcomes $\mathcal{Y}$, the framework is down to $\mathbb{R}^O\cup \mathbb{R}^T$, i.e., observed union space excluding $\mathbb{R}^H$.
However, instead of a space $\mathbb{R}^T$,
the concept of $\tau$ ``collapses'' into a single timeline, becoming another $t$ (close to Granger causality \cite{granger1993modelling}), which is align with the concept of ``objective collapse'' in quantum computation. 
Simply put, in modeling contexts, ``computing'' implies ``collapse'': Once $\mathcal{Y}$ is computed to be observable values, $\tau$ has become a constant timestamp that loses its computational flexibility and no longer acts as a ``timing dimension''. Therefore, based on this framework, Theorem 3 can be simply explained - causality modeling is to facilitate ``structuralized collapses'' within $\mathbb{R}^T$.
% However, in this circumstance, both $t$ and $\tau$ no longer function as ``computational dimensions'' but only represent two separately identified timestamps, i.e., acting as predetermined constants in modeling contexts, close to Granger causality model \cite{granger1993modelling}. It remains possible for $\mathcal{Y}$ to comprise multiple components as static sequences with different subscripts, regardless of the significance of timing. 

In this section, we will demonstrate the key concepts relative to $\mathbb{R}^H$ and $\mathbb{R}^T$ sequentially, then analyze the fundamental impact of solely adopting the observer's perspective on classic causal inference.

% \%---------------
% Suppose $\theta$ acting as the parameter of a function $f(;\theta)$ that can adequately describe $\mathcal{X} \xrightarrow{\theta} \mathcal{Y}$.
% In that case, we can define
% $\mathcal{Y}^{\theta} = f(\mathcal{X}; \theta)$ as the \emph{\textbf{sufficient} component} of $\mathcal{Y}$ about the relation $\theta$, which is identified by indexing through $\theta$ from $\mathcal{X}$, with its \emph{remaining component}, denoted by $\mathcal{Y}-\mathcal{Y}^{\theta}$, being independent of $\theta$. If this relationship possesses a causal nature in the context of modeling, such defined $\mathcal{Y}^{\vartheta}$ aligns with the concept of \emph{causal representation} raised by \cite{scholkopf2021toward}.

% \%---------------

% More critically, the established latent space representations of $\vartheta_1$ and $\vartheta_2$ are \textbf{\emph{generalizable}} across different applications of AI models, enabling a form of ``knowledge accumulation'' akin to human learning. 

% Subsequently, refining $\vartheta_1$ (and possibly $\vartheta_2$) also entails applying it within various scenarios and data sets that present diverse conditions.
% In this way, causal knowledge across various fields can be progressively compiled into relation-first representations, harnessing the capabilities of AI effectively.




\subsection{Different Causal Scales by $\omega$}
\label{subsec:unobs_relt}
%\vspace{-0.5mm}


\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{
\begin{minipage}{0.95\textwidth}
\paragraph{Definition 4.} 
A causal relation represented by $\vartheta$ can be defined as \textbf{\emph{micro-causal}} if there exists an extraneous relation $\omega$, where $\mathcal{I}(\omega)\not\subseteq\mathcal{I}(\vartheta)$, such that incorporating $\omega$ can form a new, \textbf{\emph{macro-causal}} relation, denoted by $(\vartheta,\omega)$. The process of incorporating $\omega$ is referred to as a \textbf{\emph{generalization}}.
\end{minipage}}
\vspace{1mm}

Distinct from the usage in causal emergence theory, we employ the terms ``micro-causal'' and ``macro-causal'' to identify a causal \emph{generalization} process, while the inverse reduction process can correspondingly be called \emph{individualization}. 
Thus, in modeling contexts, the \textbf{\emph{generalizability}} of a causal model $f(;\vartheta)$ can be described as its capability to be reusable in a macro-causal modeling process without reducing the representation of $\vartheta$.

As explained, the phenomenon of ``emerged information from micro- to macro-causality'' results from using different perspectives to understand the outcome $\mathcal{Y}$, where $\mathcal{I}(do(Y))$ appears to be more informative than $\mathcal{I}(Y_1) + \mathcal{I}(Y_2) = \mathcal{I}(Y)$. In our context, by explicitly adopting the creator's perspective that enables the \emph{relation-first} principle, relational information $\mathcal{I}(\theta) = \mathcal{I}(\theta_1) + \mathcal{I}(\theta_2)$ features informatively separable components, regardless of $\mathcal{Y}$'s dynamical significance. Hence, the redefined micro- and macro-causality clearly refer to the inclusion of new causal information, denoted by $\mathcal{I}(\omega)$, usually implying new observables.
For example, for causality $\mathcal{X}\xrightarrow{\vartheta}\mathcal{Y}$, the new relation could be $\mathcal{X}\xrightarrow{\omega}\mathcal{Z}$ that indicates an additional component from the same $\mathcal{X}$, or can either be $\mathcal{Z}\xrightarrow{\omega}\mathcal{Y}$, enabling a causal structure with two causes $\mathcal{X}$ and $\mathcal{Z}$. 

In more general cases, $\omega$ may imply a new \textbf{\emph{hierarchical level}}, where it is incorporated as a new condition without becoming a modeling objective. Consider the following example: Suppose family incomes $X$ influence grocery shopping frequencies $Y$ through the relation $\theta$, which could vary across countries due to the influences of cultural differences denoted by $\omega$. The modeling objective at the global level is the relation $\theta$, which becomes conditional $(\theta\mid\omega)$ at the country level. Although $\omega$ does not serve as an additional objective relation, it acts as a critical condition that challenges the model's generalizability across hierarchical levels. Indeed, the overall information $\mathcal{I}(\theta,\omega) = \mathcal{I}(\theta\mid\omega) + \mathcal{I}(\omega)$ is still required, focusing on the conditional part. If considering the countries $Z$ as an additional common cause of $X$ and $Y$, this scenario can also be attributed to the case of ``new observables.''

Put simply, hierarchies within knowledge are often encountered in relationship modeling, which may be associated with particular categories of models, such as group-specific learning \cite{fuller2007developing}, but are essentially subject to the varying causal scale problems that require model generalizability.
Below, we compare two detailed examples: one solely observational and the other causational, to demonstrate the necessity of adopting \emph{relation-first} modeling paradigm to effectively handle causal hierarchies.
%One is solely observational learning from computer vision, and the other pertains to clinical studies involving causal hierarchy.

\vspace{2mm}
{\vskip 0pt\bf Observational Hierarchy Example} 
%\vspace{-0.5mm}


% Figure environment removed


The AI-created personas on social media can have realistic faces but seldom showcase hands, since AI struggles with the intricate structure of hands, instead treating them as arbitrary assortments of finger-like items.
Figure \ref{fig:hand}(a) shows AI-created hands with faithful color but unrealistic shapes, while humans can effortlessly discern hand gestures from the grayscale sketches in (b).

Human cognition intuitively employs informative relations as the \emph{indices} to visit mental representations \cite{sep-mental-representation}. 
As in (b), this process operates hierarchically, where each higher-level understanding builds upon conclusions drawn at preceding levels. Specifically, Level $\mathbf{I}$ identifies individual fingers; Level $\mathbf{II}$ distinguishes gestures based on the positions of the identified fingers, incorporating additional information from our understanding of how fingers are arranged to constitute a hand, denoted by $\omega_i$; and Level $\mathbf{III}$ grasps the meanings of these gestures from memory, given additional information $\omega_{ii}$ from knowledge.


%To AI, or hypothetical extraterrestrial life unfamiliar with our knowledge, hands in Figure \ref{fig:hand}(a) may appear reasonable.
% AI can successfully differentiate non-overlapping features at various levels. 
%, while similar hand gestures may confuse it. 

Conversely, AI models often do not distinguish the levels of relational information, instead modeling overall as in a relationship $X\xrightarrow{\theta}Y$ with $\theta=(\theta_i, \theta_{ii}, \theta_{iii})$, resulting a lack of informative insights into $\omega$.
However, the hidden information $\mathcal{I}(\omega)$ may not always be essential. For example, AI can generate convincing faces because the appearance of eyes $\theta_i$ strongly indicates the facial angles $\theta_{ii}$, i.e., $\mathcal{I}(\theta_{ii})=\mathcal{I}(\theta_i)$ indicating $\mathcal{I}(\omega_i)=0$, removing the need to distinguish eyes from faces. 

On the other hand, given that $X$ has been fully observed, AI can inversely deduce the relational information using methods such as reinforcement learning \cite{sutton2018reinforcement, arora2021survey}. In this particular case, when AI receives approval for generating hands with five fingers, it may autonomously begin to derive $\mathcal{I}(\theta_i)$. However, when such hierarchies occur within $do(Y)$, which represents dynamically significant $\mathcal{Y}$, they can hardly be achieved under the conventional modeling paradigm, regardless of whether AI techniques are leveraged.

\vspace{2mm}
{\vskip 0pt\bf Causal Hierarchy Example} 
%\vspace{-0.5mm}


% Figure environment removed

Figure~\ref{fig:eff} illustrates a dynamically significant causal effect outcome, denoted by $\mathcal{B}_{\omega}$, to indicate that the observations, namely the sequence $B^t$, include individualized information from patients, with $\omega$ representing their IDs. Traditional clinical studies typically aim to estimate the average static effect, $B_{t+30} = f(do(A_t))$, although the essential inquiry concerns the Level I dynamic $\mathcal{B}_o=\int_{t=1}^{30}do(B_t)dt$.

Traditionally, patients' unobserved personal characteristics are often considered a \emph{hidden confounder}. Let us denote it by $E$, and for simplicity, assume $E$ linearly impact $\mathcal{B}_o$, making the Level II dynamic $\mathcal{B}_{\omega}-\mathcal{B}_o$ simply represent personal speeds.
Suppose a Granger causality model is adopted to predict a sequential outcome $B^t$. To eliminate the potential influence of $E$, intentional training data selection becomes necessary, which ensures that the Level I sequence $(B_1,\ldots, B_{30})$ can be derived by linearly averaging all selected records. This is essentially equal to manually figuring out the boundary of $\mathcal{B}_o$ by exploring all possible ${\omega}$ values.

On the other hand, even when adopting an RNN model equipped with a sequential outcome $B^t$, it still presents a sequence of static outcomes without extracting dynamic $\mathcal{B}$, since dynamical learning is facilitated only on $do(A)$. Under the conventional modeling paradigm, the model outcome $B_t$ must be specified for each predetermined timestamp $t$. However, once such a static value is identified as the outcome - for example, $t=30$ to snapshot $B_{30}$ for all patients - bias is inherently determined since $B_{30}$ represents the different magnitude of the effect for various patients.


Such hierarchical dynamic outcomes are prevalent in fields like epidemic progression, economic fluctuations, strategic decision-making, etc. The traditional approaches often use a similar strategy to manually identify the targeted levels, e.g., the group-specific learning methodology \cite{fuller2007developing}. 
These approaches, however, have become impractical due to the modern data volume, and entail a risk of snowballing significant information loss in large-scale structural models. Meanwhile, utilizing neural networks to identify significant events from dynamic hierarchies has emerged in some topics like anomaly detection
\cite{wu2018hierarchical}.

% Figure environment removed
\vspace{-1mm}

The concept of ``hidden confounder'' is essentially elusive, acting more as an interpretational compensation rather than a constructive effort to enhance modeling. 
For example, Figure~\ref{fig:hidden} (a) shows the conventional causal DAG with hidden $E$ in the scenario depicted in Figure~\ref{fig:eff}. 
Although the ``personal characteristics'' are signified by $E$, it is not required to be revealed by collecting additional data, which leads to an illogical implication: ``Our model is biased due to some unknown factors we dont intend to know.'' 

Indeed, this strategy employs a hidden observable to account for the omitted dynamics (i.e., the nonlinearity on the timing dimension) in statistical modeling.
As illustrated in Figure~\ref{fig:hidden}(b), the associative causal variable $do(A) * E$ remains unknown, unable to form a modelable relationship.
In contrast, the \emph{relation-first} modeling approaches only require an observed identifier to index the Level II features in representation extractions, like the patient ID denoted by $\omega$. 
%As demonstrated in section (c), this method effectively disentangles effect representations hierarchically in the latent space, thereby achieving model generalizability.

\vspace{-1mm}
\subsection{Multi-Timeline for Causal Model Generalizability}
\label{subsec:framework_time}
\vspace{-1mm}

The so-called ``possible world'' represents a conceptual space for storing our structural causal knowledge. It originates from our instinctive ``what if'' thinking, and the construction is supported by possible timings (i.e., the relative timing) in our cognition \cite{wulf1994reducing}. Such timing becomes necessary when we envision a possible future along a specific causal relationship \cite{shea2001effects}; thus, multiple possibilities imply multiple possible timings in causal modeling \cite{coulson2009understanding}. For clarity, we refer to these as timelines within micro-causality, i.e., when they emerge from a single cause. 


For instance, patients' vital signs are recorded daily in a hospital with chronological timestamps, which indicate their absolute timing. To assess the medical intervention $\mathcal{Y}$, a uniform series of post-medication events must be selected, spanning 30 days from the day after medication. They are considered as distribution along an $t$-axis, i.e., the timeline, marked as $[1,30]$ to represent their relative timing. Suppose the intervention involves two components, such as the primary effect $\mathcal{Y}_1$ and the side effect $\mathcal{Y}_2$, whose mutual influences are of interest. Then, two separate timelines, $t_1$ and $t_2$, must be established to capture their individual dynamics, enabling two-dimensional timing computation, even though they both represent the $[1,30]$ relative timing.

For a micro-causality $\mathcal{X}\xrightarrow{\vartheta}\mathcal{Y}$, all necessary timelines for the dynamic outcome $\mathcal{Y}$ together formulate the possible timing space $\mathbb{R}^T$. As highlighted in Theorem 3, computations for $\vartheta$ must be sequential within space $\mathbb{R}^T$, taking into account the potential dynamical interactions between these dimensions, to form a multi-round causal structure detection. Otherwise, $\mathbb{R}^T$ ceases to function as a multidimensional space and collapses into a one-dimensional relative timing, implying the absence of dynamical interactions between the outcome components. As a result, the deduced causal models will lose their significance in representing structural causality and lack model generalizability.



% Conventionally, the concept of ``temporal dimension'' is often simplified to be the single absolute timing $\mathbf{t}$, evident from the traditional ``spatial-temporal'' analysis \cite{alkon1988spatial, turner1990spatial, andrienko2003exploratory}, to recent advancements in language models \cite{gurnee2023language}. 
% However, as emphasized in Remark 2, our cognitive perception of ``time'' is more complex, fundamentally enabling our causal reasoning \cite{coulson2009understanding}.

% For an intuitive insight into the implications of neglecting relative timings in $\mathbb{R}^T$, let's consider an analogy:
% Imagine ants dwelling on a floor's two-dimensional plane. To predict risks, the scientists among them create two-dimensional models and instinctively adopt the nearest tree as a height reference. They noticed increased disruptions at the tree's first branch, which indeed correlates to the children's heights, given their curiosity.
% However, without understanding humans as three-dimensional beings, they can only interpret it by adhering to the first branch.
% One day, after relocating to another tree with a lower height, the ants found the risk presenting at the second branch instead, making their model ineffective. They may conclude that human behaviors are too complex, highlighting the model generalizability issue.
% %Similarly, when we specify a single, absolute timeline for all potential events, this timeline becomes our ``tree'', which may introduce inherent modeling biases, affecting the robustness and generalizability of our models.

% As three-dimensional beings, we inherently lack the capacity to fully integrate the fourth dimension - time - into visual perception. Instead, we conceptualize ``space'' in three dimensions to incorporate features of the temporal dimension along a \emph{timeline} within the space, analogous to our ``tree''.
% Yet, ants do not need to fully comprehend the three-dimensional world to build a generalizable model; instead, they need only recognize the ``forest'' out of their vision (i.e., counterfactual), which consists of all ``possible trees'' with \emph{relatively} different branch locations. 
% Similarly, in our modeling, we must include the $\mathbb{R}^T$ space, composed of all potential relative timings within our causal knowledge, although they cannot be directly observed.

% Addressing the counterfactual query ``what effect would be if the cause were changed'' differentiates causality from mere correlations \cite{scholkopf2021toward}. In the proposed framework, counterfactuals are more intuitively interpreted through distributions, potentially offering valuable insights in fields like quantum computing.
% Specifically, the observed prior conditions $\mathcal{X}$ can be considered as features in $\mathbb{R}^O$, whose effects $\mathcal{Y}$ act 
% as a conditional distribution within $\mathbb{R}^{O-1}\cup \mathbb{R}^T$, incorporating $T$ possible observed timings in the future. %This perspective might potentially offer valuable insights in fields like quantum computing.

% %- It might offer an intriguing insight into quantum computing (need further exploration).
% %It might provide an intriguing insight that could potentially inspire advances in quantum computing (further exploration required).



The example in Figure~\ref{fig:3d} showcases a practical scenario in a clinical study. This 3D causal DAG includes two timelines, $T_Y$ and $T_Z$, with the $x$-axis categorically arranging observables. The upgrades to causal DAGs, as applied in Figure \ref{fig:interact}, are also adopted here, ensuring that the lengths of the arrows reflect the timespan required to achieve the static values of the observables.
Here, the static values are denoted by uppercase letters, representing equal magnitudes of causal effects within the current data population, i.e., the group of patients under analysis. Accordingly, the lengths of the arrows indicate their mean timespans.

% Figure environment removed

We use $\Delta t$ and $\Delta \tau$ to signify the actual time steps on $T_Y$ and $T_Z$, respectively. For example, consider the triangle $SA'B'$. As each unit of effect is delivered from $S$ to $A'$ (taking $\Delta \tau$), it immediately starts impacting $B'$ through $\overrightarrow{A'B'}$ (with $\Delta t$ required); simultaneously, the next unit of effect begins its generation at $S$. This dual action occurs concurrently, and in traditional Structural Causal Models (SCM), it is represented by the edge $\overrightarrow{SB'}$ (highlighted in green), which necessitates the actual timespan specification.
However, specifying the timespan of $\overrightarrow{SB'}$ inherently sets the $\Delta t:\Delta \tau$ ratio based on the current population's performance, freezing the static value represented by $B'$ and fixing the valid shape of the ${ASB'}$ triangle in this DAG space. 

The lack of model generalizability in this SCM depends on the targeted causal scales. Focusing solely on the current population (i.e., in micro-causal), the deduced model can describe only the population mean, lacking the capability to be individualized at the patient level.
If we aim to generalize this SCM to fit with other populations (i.e., in macro-causal), it may fail because the preset $\Delta t:\Delta \tau$ ratio is not universally applicable.

%\vspace{2mm}
\subsection{Fundamental Reliance on Causal Assumptions}
\label{subsec:appl_assumpt}
\vspace{-1mm}

Figure~\ref{fig:view} categorizes the applications of causal models based on two aspects: 1) if the structure of $\theta/\vartheta$ is known a priori, they are used for structural causation buildup or causal discovery; 2) depending on whether the required outcome is dynamically significant, they can either accurately represent true causality or not.

Under the conventional modeling paradigm, capturing the significant dynamics within causal outcomes autonomously is challenging. When building causal models based on given prior knowledge, the omitted dynamics become readily apparent. If these dynamics can be specifically attributed to certain unobserved observables, like the node $E$ in Figure \ref{fig:hidden}(a), such information loss is attributed to a hidden confounder. Otherwise, they might be overlooked due to the \emph{causal sufficiency} assumption, which presumes that all potential confounders have been observed within the system. Typical examples of approaches susceptible to these issues are structural equation models (SEMs) and functional causal models (FCMs) \cite{glymour2019review, elwert2013graphical}.
Although state-of-the-art deep learning applications have effectively transformed the discrete structural constraint into continuous optimizations \cite{zheng2018dags, zheng2020learning, lachapelle2019gradient}, issues of lack of generalizability still hold \cite{scholkopf2021toward,luo2020causal, ma2018using}.

On the other hand, causal discovery primarily operates within the $\mathbb{R}^O$ space and is incapable of detecting dynamically significant causal outcomes. If the interconnection of observables can be accurately specified as the functional parameter $\theta$, there remains a chance to discover informative correlations. Otherwise, mere conditional dependencies among observables are unreliable for causal reasoning, as seen in Bayesian networks \cite{pearl2000models, peters2014causal}. Typically, undetected dynamics are overlooked due to the \emph{Causal Faithfulness} assumption, which suggests that the observables can fully represent the underlying causal reality.


Furthermore, the causal directions suggested by the results of causal discovery often lack logical causal implications. Consider $X$ and $Y$ in the optional models $Y=f(X;\theta)$ and $X=g(Y;\phi)$, with predetermined parameters, which indicate opposite directions. Typically, the direction $X\rightarrow Y$ would be favored if $\mathcal{L}(\hat{\theta}) > \mathcal{L}(\hat{\phi})$. Let $\mathcal{I}_{X,Y}(\theta)$ denote the information about $\theta$ given $\mathbf{P}(X,Y)$. Using $p(\cdot)$ as the density function, the integral $\int_X p(x;\theta)dx$ remains constant in this context. Then:

\vspace{-6mm}
\begin{align*}
    \mathcal{I}_{X,Y}(\theta) &= \mathbb{E}[(\frac{\partial }{\partial \theta} \log p(X,Y;\theta))^2 \mid \theta] 
    = \int_{Y} \int_X (\frac{\partial }{\partial \theta} \log p(x,y;\theta))^2 p(x,y;\theta) dx dy \\
    &= \alpha \int_Y (\frac{\partial }{\partial \theta} \log p(y;x,\theta))^2 p(y;x,\theta) dy + \beta = \alpha \mathcal{I}_{Y\mid X}(\theta)+\beta, \text{with } \alpha,\beta \text{ being constants.} \\
    \text{Then, } \hat{\theta} &=\argmax\limits_{\theta} \mathbf{P}(Y\mid X,\theta) = \argmin\limits_{\theta} \mathcal{I}_{Y\mid X} (\theta) =\argmin\limits_{\theta} \mathcal{I}_{X,Y}(\theta), \text{ and } \mathcal{L}(\hat{\theta})  \propto 1/\mathcal{I}_{X,Y}(\hat{\theta}).
\end{align*}
\vspace{-5mm}

%The likelihoods of $\hat{\theta}$ and $\hat{\omega}$ rely on the information $\mathcal{I}(\hat{\theta})$ and $\mathcal{I}(\hat{\omega})$. 
The inferred directionality indicates how informatively the observational data distribution can reflect the two predetermined parameters.
Consequently, such directionality is unnecessarily logically meaningful but could be dominated by the data collection process, with the predominant entity deemed the ``cause'', consistent with other existing conclusions \cite{reisach2021beware, kaiser2021unsuitability}.
%rather indicative of distributional dominance as determined by the data collection process. Here, the predominant entity is labeled the ``cause''.
%Even when $\theta$ and $\phi$ are predetermined based on knowledge, they might not provide insights for dynamically significant causal relations.

%they only link to solely observational features of entities, and thus may not accurately reflect true causal relations.
%in the presence of dynamically significant effects.

%their designated distributions appear in the data, %with the predominant one deemed the ``cause'' - It assumes, by default, that observations capture the cause more thoroughly than the effect. While limited data collection techniques made it reasonable in the past, it is no longer safe to assume such observationally inferred directions to hold logical meaning for causality.
    


% Figure environment removed
%\vspace{-2mm}



%provides formulations of factorizations, to achieve of achieving hierarchical disentanglement through relation-indexing

%\vspace{-2mm}
\section{Relation-Indexed Representation Learning (RIRL)}
\label{sec:representation}

This section introduces a method for realizing the proposed \emph{relation-first} paradigm, referred to as RIRL for brevity. Unlike existing causal representation learning, which is primarily confined to the micro-causal scale, RIRL focuses on facilitating \emph{structural causal dynamics exploration} in the latent space. 

Specifically, ``relation-indexed'' refers to its micro-causal realization approach, guided by the \emph{relation-first} principle, where the indexed representations are capable of capturing the dynamic features of causal outcomes across their timing-dimensional distributions. Furthermore, from a macro-causal perspective, the extracted representations naturally possess high generalizability, ready to be reused and adapted to various practical conditions. 
This advancement is evident in the structural exploration process within the latent space.

Unlike traditional causal discovery, RIRL exploration spans $\mathbb{R}^O\cup\mathbb{R}^T$ to detect causally significant dynamics without concerns about ``hidden confounders''. The causal representations obtained in each round of detection serve as elementary units for reuse, enhancing the flexibility of structural models. This exploration process eventually yields DAG-structured graphical indices, with each input-output pair representing a specific causal routine, readily accessible.

Subsequently, section \ref{subsec:repr_autoencoder} delves into the micro-causal realization to discuss the technical challenges and their resolutions, including the architecture and core layer designs. Section \ref{subsec:RIRL_stacking} introduces the process of ``stacking'' relation-indexed representations in the latent space, to achieve hierarchical disentanglement at an effect node in DAG. Finally, section \ref{subsec:RIRL_discover} demonstrates the exploration algorithm from a macro-causal viewpoint.


\subsection{Micro-Causal Architecture}
\label{subsec:repr_autoencoder}
\vspace{-1mm}

For a relationship $\mathcal{X}\xrightarrow{\theta}\mathcal{Y}$ given sequential observations $\{x^t\}$ and $\{y^{\tau}\}$, with $|\overrightarrow{x}|= n$ and $|\overrightarrow{y}|= m$, the relation-indexed representation aims to establish $(\mathcal{X},\theta,\mathcal{Y})$ in the latent space $\mathbb{R}^L$.
Firstly, an \emph{initialization} is needed for $\mathcal{X}$ and $\mathcal{Y}$ individually, to construct their latent space representations from observed data sequences.
For clarity, we use 
$\mathcal{H} \in \mathbb{R}^L$ and $\mathcal{V} \in \mathbb{R}^L$ to refer to the latent representations of $\mathcal{X}\in \mathbb{R}^O$ and $\mathcal{Y}\in \mathbb{R}^O$, respectively.
The neural network optimization to derive $\theta$ is a procedure between $\mathcal{H}$ as input and $\mathcal{V}$ as output. 
In each iteration,
$\mathcal{H}$, $\theta$, and $\mathcal{V}$ are sequentially refined in three steps, until the distance between $\mathcal{H}$ and $\mathcal{V}$ is minimized within $\mathbb{R}^L$, without losing their representations for $\mathcal{X}$ and $\mathcal{Y}$.
Consider instances $x$ and $y$ of $\mathcal{X}$ and $\mathcal{Y}$ that are represented by $h$ and $v$ correspondingly in $\mathbb{R}^L$, as in Figure \ref{fig:bridge}. The latent dependency $\mathbf{P}(v| h)$ represents the relational function $f(;\theta)$.
The three optimization steps are as follows:
\begin{enumerate}[topsep=0pt, partopsep=0pt]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{1pt}
    \item {Optimizing the cause-encoder by $\mathbf{P}(h|x)$, the relation model by $\mathbf{P}(v|h)$, and the effect-decoder by $\mathbf{P}(y|v)$ to reconstruct the relationship $x\rightarrow y$, represented as $h\rightarrow v$ in $\mathbb{R}^L$.}
    \item {Fine-tuning the effect-encoder $\mathbf{P}(v|y)$ and effect-decoder $\mathbf{P}(y|v)$ to accurately represent $y$.} 
    \item {Fine-tuning the cause-encoder $\mathbf{P}(h|x)$ and cause-decoder $\mathbf{P}(x|h)$ to accurately represent $x$.}
\end{enumerate}
%\vspace{1mm}

In this process, $h$ and $v$ are iteratively adjusted to reduce their distance in $\mathbb{R}^{L}$, with $\theta$ serving as a bridge to span this distance and guiding the output to fulfill the associated representation $( \mathcal{H},\theta,\mathcal{V})$. From the perspective of the effect node $\mathcal{Y}$, this tuple represents its component indexing through $\theta$, denoted as $\mathcal{Y}_{\theta}$.
%Within this system, for each effect, a series of such relation functions $\{f(;\theta)\}$ is maintained, indexing diverse levels of causal inputs for sequentially building the structural model.

However, it introduces a technical challenge: for a micro-causality $\theta$, the dimensionality $L$ of the latent space must satisfy $L \geq rank(\mathcal{X},\theta,\mathcal{Y})$ to provide adequate freedom for computations. To accommodate a structural DAG, this lower boundary can be further enhanced, to be certainly larger than the input vector length $|\overrightarrow{\mathcal{X}}| = t*n$. This necessitates a specialized autoencoder to realize a ``higher-dimensional representation'', where the accuracy of its reconstruction process becomes significant, and essentially requires \emph{invertibility}. 

%Autoencoders are commonly used for dimensionality reduction, especially in structural modeling that involves multiple variables \cite{wang2016auto}.
%, the column-augmented original data matrix often possesses a dimensionality exceeding that of the latent space $\mathbb{R}^L$. 
%In contrast, RIRL aims to model individual causal relationships sequentially within a higher-dimensional latent space $\mathbb{R}^L$, as to hierarchically construct the entire causal structure. 

% Figure environment removed

Figure~\ref{fig:arch} illustrates the designed autoencoder architecture, featured by a pair of symmetrical layers, named \emph{Expander} and \emph{Reducer} (source code is available \footnote{https://github.com/kflijia/bijective\_crossing\_functions/blob/main/code\_bicross\_extracter.py}).
The Expander magnifies the input vector by capturing its higher-order associative features, while the Reducer symmetrically diminishes dimensionality and reverts to its initial formation. 
For example, the Expander showcased in Figure~\ref{fig:arch} implements a \emph{double-wise} expansion. Every duo of digits from $\overrightarrow{\mathcal{X}}$ is encoded into a new digit by associating with a random constant, termed the \emph{Key}. This \emph{Key} is generated by the encoder and replicated by the decoder. Such pairwise processing of $\overrightarrow{\mathcal{X}}$ expands its length from $(t*n)$ to be $(t*n-1)^2$. By concatenating the expanded vectors using multiple \emph{Keys}, $\overrightarrow{\mathcal{X}}$ can be considerably expanded, ready for the subsequent reduction through a regular encoder.

The four blue squares in Figure~\ref{fig:arch} with unique grid patterns signify the resultant vectors of the four distinct \emph{Keys}, with each square symbolizing a $(t*n - 1)^2$ length vector. Similarly, higher-order expansions, such as \emph{triple-wise} across three digits, can be chosen with adapted \emph{Keys} to achieve more precise reconstructions.

% The four differently patterned blue squares represent the vectors expanded by four distinct \emph{Keys}, with the grid patterns indicating their ``signatures''. Each square visualizes a $(d * T_x - 1)^2$ length vector (not signifying a 2-dimensional vector).
% In a similar way, higher-order extensions, such as \emph{triple-wise} ones across every three digits, can also be employed by appropriately adapting \emph{Keys}.

% Figure environment removed

Figure~\ref{fig:extractor} illustrates the encoding and decoding processes within the Expander and Reducer, targeting the digit pair $(x_i, x_j)$ for $i\neq j \in 1,\ldots,n$. The Expander function is defined as $\eta_\kappa(x_i,x_j) = x_j \otimes exp(s(x_i)) + t(x_i)$, which hinges on two elementary functions, $s(\cdot)$ and $t(\cdot)$. The parameter $\kappa$ represents the adopted \emph{Key} comprising of their weights $\kappa=(w_s, w_t)$.
Specifically, the Expander morphs $x_j$ into a new digit $y_j$ utilizing $x_i$ as a chosen attribute. In contrast, the Reducer symmetrically performs the inverse function $\eta_\kappa^{-1}$, defined as $(y_j-t(y_i)) \otimes exp(-s(y_i))$. 
This approach circumvents the need to compute $s^{-1}$ or $t^{-1}$, thereby allowing more flexibility for nonlinear transformations through $s(\cdot)$ and $t(\cdot)$. This is inspired by the groundbreaking work in \cite{dinh2016density} on invertible neural network layers employing bijective functions.



\subsection{Stacking Relation-Indexed Representations}
\label{subsec:RIRL_stacking}
\vspace{-1mm}
% \noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.94\textwidth}
% \paragraph{Remark 2.} Given a causal graph $G$ with data matrix $\mathbf{X}$ column-augmented by all nodes' attributes, the latent space dimensionality $L$ must satisfy $L\ge rank(\mathbf{X})$ to adequately represent $G$.
% \vspace{-0.5mm}
% \end{minipage}}


%\vspace{-2.7mm}
% Figure environment removed

In each round of detection during the macro-causal exploration, a micro-causal relationship will be selected for establishment. Nonetheless, the cause node in it may have been the effect node in preceding relations, e.g., the component $\mathcal{Y}_{\theta}$ may already exist at $\mathcal{Y}$ when $\mathcal{Y}\rightarrow\mathcal{Z}$ is going to be established. 
This process of conditional representation buildup is referred to as ``stacking''. 

For a specific node $\mathcal{X}$, the stacking processes, where it serves as the effect, sequentially construct its hierarchical disentanglement according to the DAG. It requires the latent space dimensionality to be larger than $rank(X)+T$, where $T$ represents the in-degree of node $\mathcal{X}$ in this DAG, as well as its number of components as the dynamic effects.
From a macro-causal perspective, $T$ can be viewed as the number of necessary edges in a DAG. While to fit it into $\mathbb{R}^L$, a predetermined $L$ must satisfy $L > rank(\mathbf{X})+T$, where $\mathbf{X}$ represents the data matrix encompassing all observables.
In this study, we bypass further discussions on dimensionality boundaries by assuming $L$ is large enough for exploration, and empirically determine $L$ for the experiments. %We gradually reduce $L$ for the experiments to determine an empirically proper value.

%Since the space $\mathbb{R}^L$ learned by the autoencoder is spanned by the top principal components of $\mathbf{X}$ \cite{baldi1989neural, plaut2018principal, wang2016auto}, possibly reducing $L$ below $rank(\mathbf{X})$ yields a less adequate but more causally significant DAG  latent space through better alignment of dimensions \cite{jain2021mechanism} (Further exploration in this direction is warranted). 
%In this study, we will set aside discussions on the boundaries of dimensionality. Our experiments feature 10 variables with dimensions 1 to 5 (Table~\ref{tab:tower}), and we empirically fine-tune and reduce $L$ from 64 to 16.

%Consider a causal structural among $\{\mathcal{X}, \mathcal{Y}, \mathcal{Z}\}$, with their corresponding representations $\{\mathcal{H}, \mathcal{V}, \mathcal{K}\} \in \mathbb{R}^L$ initialized by three autoencoders, respectively. 
Figure~\ref{fig:stack} illustrates the stacking architectures under two different scenarios within a three-node system ${\mathcal{X}, \mathcal{Y}, \mathcal{Z}}$. 
In this figure, the established relationship $\mathcal{X}\rightarrow\mathcal{Y}$ is represented by the blue data streams and layers. The scenarios differ in the causal directions between $\mathcal{Y}$ and $\mathcal{Z}$: the left side represents $\mathcal{X}\rightarrow \mathcal{Y}\leftarrow \mathcal{Z}$, while the right side depicts $\mathcal{X}\rightarrow \mathcal{Y}\rightarrow \mathcal{Z}$. 


The hierarchically stacked representations allow for flexible input-output combinations to represent different causal routines as needed. For simple exemplification, we use $\mapsto$ to denote the input and output layers in the stacking architecture. On the left, $\mathbf{P}(v|h) \mapsto \mathbf{P}(\alpha)$ represents the $\mathcal{X}\rightarrow \mathcal{Y}$ relationship, while $\mathbf{P}(\alpha|k)$ implies $\mathcal{Z}\rightarrow \mathcal{Y}$. Conversely, on the right, $\mathbf{P}(v) \mapsto \mathbf{P}(\beta|k)$ denotes the $\mathcal{Y}\rightarrow \mathcal{Z}$ relationship with $\mathcal{Y}$ as the input. Meanwhile, $\mathbf{P}(v|h) \mapsto \mathbf{P}(\beta|k)$ captures the causal sequence $\mathcal{X} \rightarrow \mathcal{Y}\rightarrow \mathcal{Z}$.


% \subsection{Factorizing the Effect Disentanglement}
% \label{subsec:RIRL_disentangle}

% Consider $\mathcal{Y}=\langle Y, \tau \rangle \in \mathbb{R}^{b+1} \subseteq \mathbb{R}^O$ having a $n$-level hierarchy, with each level built up using a representation function, labeled as $g_i$ for the $i$-th level. For simplicity, here, we use $\omega_i$ to represent the $i$-th level component of $\mathcal{Y}$ in the latent space $\mathbb{R}^L$, while its counterpart in $\mathbb{R}^{b+1}$ is denoted as $\Omega_i$ (i.e., $\mathcal{\hat{Y}}$ at the $i$-th level).
% Let the feature vector $\omega_i$ in $\mathbb{R}^{L}$ primarily spans a sub-dimensional space, $\mathbb{R}^{L_i}$, resulting in the spatial disentanglement sequence $\{\mathbb{R}^{L_1}, \ldots, \mathbb{R}^{L_i},\ldots,\mathbb{R}^{L_n}\}$, which hierarchically represents $\mathcal{Y}$ with $n$ components.
% Function $g_i$ maps from $\mathbb{R}^{b+1}$ to $\mathbb{R}^{L_i}$, taking into account features from all previous levels as attributes.  This gives us:
% \vspace{-4mm}

% \begin{equation}
%     \mathcal{Y} = \sum_{i=1}^{n} \Omega_i, 
%      \text{ where } \Omega_i = g_i \bigl( \omega_i ;\ \Omega_1,\ldots, \Omega_{i-1}\bigr) \text{ with } \Omega_i \in \mathbb{R}^{b+1} \text{ and } \omega_i \in \mathbb{R}^{L_i} \subseteq \mathbb{R}^{L}
% \end{equation}

% \vspace{-3mm}
% %The $t$-th component in the observable data space, denoted as $\Omega_t \in \mathbb{R}^{O}$, is articulated through an observational data sequence with the length of $T_y$, along the absolute timeline $t$.
% %However, in latent space, the objective of $\omega_t$ is to capture dynamics along a relative timeline, $t_i$, which is autonomously determined by the relation at the $i$-th level, not bound by the observational timestamps in $\mathbb{R}^{O}$. 


% % $\{t_1,\ldots,t_i,\ldots,t_n\}$, each uniquely determined by the relationship at their respective levels, apart from the absolute timeline $t$.
% % While in the \emph{observable data space}, the $i$-th level feature, represented as the sum $\Omega_1 +\ldots +\Omega_i$, still maintains its timestamp attribute along $t$.

% In the context of a purely observational hierarchy, with $\mathcal{Y}$ substituted by $Y \in \mathbb{R}^b$, The example depicted in Figure~\ref{fig:hand} (b) can be interpreted as follows: Consider three feature levels represented as $\omega_1\in \mathbb{R}^{L_1}$, $\omega_2\in \mathbb{R}^{L_2}$, and $\omega_3\in \mathbb{R}^{L_3}$. For simplicity, assume the subspaces are mutually exclusive, such that $L=L_1+L_2+L_3$. In the latent space, the triplet $(\omega_1, \omega_2, \omega_3) \in \mathbb{R}^{L}$ comprehensively depicts the image. Their observable counterparts, $\Omega_1$, $\Omega_2$, and $\Omega_3$, are three distinct full-scale images, each showcasing different content. For example, $\Omega_1$ emphasizes finger details, while the combination $\Omega_1+\Omega_2$ reveals the entire hand.


\vspace{-2mm}
\subsection{Exploration Algorithm in the Latent Space}
\label{subsec:RIRL_discover}
\vspace{-1mm}

\begin{minipage}{\textwidth}
    \begin{minipage}[b]{0.52\textwidth}
    \raggedright
    \begin{algorithm}[H]
    \footnotesize
    \SetAlgoLined
    \KwResult{ ordered edges set $\mathbf{E}=\{e_1, \ldots,e_n\}$ }
    $\mathbf{E}=\{\}$ ; $N_R = \{ n_0 \mid n_0 \in N, Parent(n_0)=\varnothing\}$ \;
    \While{$N_R \subset N$}{
    $\Delta = \{\}$ \;
    \For{ $n \in N$ }{
        \For{$p \in Parent(n)$}{
            \If{$n \notin N_R$ and $p \in N_R$}{
                $e=(p,n)$; \\ 
                $\beta = \{\}$;\\ 
                \For{$r \in N_R$}{
                    \If{$r \in Parent(n)$ and $r \neq p$}{
                        $\beta = \beta \cup r$}
                    }
                $\delta_e = K(\beta \cup p, n) - K(\beta, n)$;\\
                $\Delta = \Delta \cup \delta_e$;
            }
        }
    }
    $\sigma = argmin_e(\delta_e \mid \delta_e \in \Delta)$;\\
    $\mathbf{E} = \mathbf{E} \cup \sigma$; \ 
    $N_R = N_R \cup n_{\sigma}$;\\
    }
     \caption{RIRL Exploration}
    \end{algorithm}
    \end{minipage}
%\hfill 
    \begin{minipage}[b]{0.4\textwidth}
    \raggedleft
        \label{tab:table1}
        \begin{tabular}{|l|l|}
        \hline
           $G=(N,E)$ & graph $G$ consists of $N$ and $E$ \\
           $N$ & the set of nodes\\
           $E$ & the set of edges\\
           $N_R$ & the set of reachable nodes\\
           $\mathbf{E}$ & the list of discovered edges\\
           $K(\beta, n)$ & KLD metric of effect $\beta\rightarrow n$\\
           $\beta$ & the cause nodes \\
           $n$ & the effect node\\
           $\delta_e$ & KLD Gain of candidate edge $e$\\
           $\Delta=\{\delta_e\}$ & the set $\{\delta_e\}$ for $e$ \\
           $n$,$p$,$r$ & notations of nodes\\
           $e$,$\sigma$ & notations of edges\\
          \hline
        \end{tabular} %
%       \captionof{table}{A table beside a figure}
    \end{minipage}
\end{minipage}
\vspace{1mm}

Algorithm 1 outlines the heuristic exploration procedure among the initialized representations of nodes. We employ the Kullback-Leibler Divergence (KLD) as the optimization criterion to evaluate the similarity between outputs, such as the relational $\mathbf{P}(v|h)$ and the prior $\mathbf{P}(v)$. A lower KLD value indicates a stronger causal strength between the two nodes. Additionally, we adopt the Mean Squared Error (MSE) as another measure of accuracy. Considering its sensitivity to data variances \cite{reisach2021beware}, we do not choose MSE as the primary criterion.

%is a frequently used evaluation metric,  leads us to utilize it as a supplementary measure in this study. %In the graphical representation context, we refer to variables $A$ and $B$ in the edge $A\rightarrow B$ as the \emph{cause node} and \emph{effect node}, respectively.

\vspace{-1mm}
% Figure environment removed

By adopting a macro-causal viewpoint, the example in Figure~\ref{fig:discover} showcases the process of stacking a new representation for the selected edge, encompassing four primary steps: In Step 1, two edges, $e_1$ and $e_3$, have been selected in previous detection rounds. In Step 2, $e_1$, having been selected, becomes the preceding effect at node $B$ for the next round. In Step 3, with $e_3$ selected in the new round, the candidate edge $e_2$ from $A$ to $C$ must be deleted and rebuilt since $e_3$ alters the conditions at $C$. Step 4 depicts the resultant structure.




%\vspace{-2mm}
\section{RIRL Exploration Experiments}
%\vspace{-3mm}
\label{sec:experiment}

In the experiments, our objective is to evaluate the proposed RIRL method from three perspectives: 1) the performance of the higher-dimensional representation autoencoder, assessed through its reconstruction accuracy; 2) the effectiveness of hierarchical disentanglement for a specific effect node, as determined by the explored causal DAG; 3) the method's ability to accurately identify the underlying DAG structure through exploration. A comprehensive demonstration of the conducted experiments is available online\footnote{https://github.com/kflijia/bijective\_crossing\_functions.git}. However, it is important to highlight two primary limitations of the experiments, which are detailed as follows:

Firstly, as an initial realization of the \emph{relation-first} paradigm, RIRL struggles with modeling efficiency, since it requires a substantial amount of data points for each micro-causal relationship, making the heuristic exploration process slow. The dataset used is generated synthetically, thus providing adequate instances. However, current general-use simulation systems typically employ a single timeline to generate time sequences - It means that interactions between dynamic outcome components across multiple timelines cannot be formulated. Ideally, real-world data like clinical records would be preferable for validating the macro-causal model's generalizability. Due to practical constraints, we are unable to access such data for this study and, therefore, designate it as an area for future work. The issues of generalization inherent in such data have been experimentally confirmed in prior work \cite{li2020teaching}, which readers may find informative.

%the dataset employed in this study may not be the most suitable for evaluating the effectiveness of RIRL. Ideally, real-world data featuring rich structuralized causality across multiple relative timings, like clinical records, would be preferable. 
%However, due to practical constraints, access to such optimal data is limited for this study, leading us to use the current synthetic data and focus solely on feasibility verification. For experimental validation regarding the inherent bias, please refer to prior research \cite{li2020teaching}.

Secondly, the time windows for the cause and effect, denoted by $n$ and $m$, were fixed at 10 and 1, respectively. This arose from an initial oversight in the experimental design stage, wherein the pivotal role of dynamic outcomes was not fully recognized, and our vision was limited by the RNN pattern. While the model can adeptly capture single-hop micro-causality, it struggles with multi-hop routines like $\mathcal{X} \rightarrow \mathcal{Y}\rightarrow \mathcal{Z}$, since the dynamics in $\mathcal{Y}$ have been discredited by $m=1$. However, it does not pose a significant technical challenge to expand the time window in future works.



%\vspace{-1mm}
\subsection{Hydrology Dataset}
\vspace{-1mm}

%\vspace{-3mm}
% Figure environment removed


The employed dataset is from a widely-used synthetic resource in the field of hydrology, aimed at enhancing streamflow predictions based on observed environmental conditions such as temperature and precipitation. %The application of RIRL aims to create a streamflow prediction model that is generalizable across various watersheds. While these watersheds share a fundamental hydrological scheme governed by physical rules, they may exhibit unique features due to unobserved conditions such as economic development and land use. Current models based on physical knowledge, however, often lack the flexibility to fully capture multiple levels of dynamical temporal features across these watersheds.
In hydrology, deep learning, particularly RNN models, has gained favor for extracting observational representations and predicting streamflow \cite{goodwell2020debates, kratzert2018rainfall}. 
We focus on a simulation of the Root River Headwater watershed in Southeast Minnesota, covering 60 consecutive virtual years with daily updates. The simulated data is from the Soil and Water Assessment Tool (SWAT), a comprehensive system grounded in physical modules, to generate dynamically significant hydrological time series. %The performance evaluations predominantly focus on the accuracy of the autoencoder reconstructions.

Figure \ref{fig:stream} displays the causal DAG employed by SWAT, complete with node descriptions. The hydrological routines are color-coded based on their contribution to output streamflow: Surface runoff (the 1st tier) significantly impacts rapid streamflow peaks, followed by lateral flow (the 2nd tier); baseflow dynamics (the 3rd tier) have a subtler influence. Our exploration process aims to reveal these underlying tiers. %relationships from the observed data.


\vspace{-1mm}
\subsection{Higher-Dimensional Reconstruction}
%\vspace{-2mm}

This test is based on ten observable nodes, each requiring an individual autoencoder for initialing its higher-dimensional representation. 
Table \ref{tab:tower} lists the characteristics of these observables after being scaled (i.e., normalized), along with their autoencoders' reconstruction accuracies, assessed in the root mean square error (RMSE), where a lower RMSE indicates higher accuracy for both scaled and unscaled data.


The task is challenged by the limited dimensionalities of the ten observables - maxing out at just 5 and the target node, $J$, having just one attribute. To mitigate this, we duplicate the input vector to a consistent 12-length and add 12 dummy variables for months, resulting in a 24-dimensional input. A double-wise extension amplifies this to 576 dimensions, from which a 16-dimensional representation is extracted via the autoencoder.
Another issue is the presence of meaningful zero-values, such as node $D$ (Snowpack in winter), which contributes numerous zeros in other seasons and is closely linked to node $E$ (Soil Water). We tackle this by adding non-zero indicator variables, called \emph{masks}, evaluated via binary cross-entropy (BCE).

Despite challenges, RMSE values ranging from $0.01$ to $0.09$ indicate success, except for node $F$ (the Aquifer). Given that aquifer research is still emerging (i.e., the 3rd tier baseflow routine), it is likely that node $F$ in this synthetic dataset may better represent noise than meaningful data.


\begin{table*}[t]
\caption{Characteristics of observables, and corresponding reconstruction performances.}
\label{tab:tower}
\vspace{-2mm}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{|
>{\columncolor[HTML]{EFEFEF}}c |c|l|l|l|l|l|l|l|l|}
\hline
\cellcolor[HTML]{C0C0C0}Variable & \cellcolor[HTML]{C0C0C0} Dim & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Mean} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Std} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Min} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Max} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Non-Zero Rate\%} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}RMSE on Scaled} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}RMSE on Unscaled} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}BCE of Mask} \\ \hline
A & 5 & 1.8513 & 1.5496 & -3.3557 & 7.6809 & 87.54 & 0.093 & 0.871 & 0.095 \\ \hline
B & 4 & 0.7687 & 1.1353 & -3.3557 & 5.9710 & 64.52 & 0.076 & 0.678 & 1.132 \\ \hline
C & 2 & 1.0342 & 1.0025 & 0.0 & 6.2145 & 94.42 & 0.037 & 0.089 & 0.428 \\ \hline
D & 3 & 0.0458 & 0.2005 & 0.0 & 5.2434 & 11.40 & 0.015 & 0.679 & 0.445 \\ \hline
E & 2 & 3.1449 & 1.0000 & 0.0285 & 5.0916 & 100 & 0.058 & 3.343 & 0.643 \\ \hline
F & 4 & 0.3922 & 0.8962 & 0.0 & 8.6122 & 59.08 & 0.326 & 7.178 & 2.045 \\ \hline
G & 4 & 0.7180 & 1.1064 & 0.0 & 8.2551 & 47.87 & 0.045 & 0.81 & 1.327 \\ \hline
H & 4 & 0.7344 & 1.0193 & 0.0 & 7.6350 & 49.93 & 0.045 & 0.009 & 1.345 \\ \hline
I & 3 & 0.1432 & 0.6137 & 0.0 & 8.3880 & 21.66 & 0.035 & 0.009 & 1.672 \\ \hline
J & 1 & 0.0410 & 0.2000 & 0.0 & 7.8903 & 21.75 & 0.007 & 0.098 & 1.088 \\ \hline
\end{tabular}}
\vspace{-3mm}
\end{table*}

%\vspace{1mm}
\subsection{Hierarchical Disentanglement}
%\vspace{-2mm}

Table \ref{tab:unit} provides the performance of stacking relation-indexed representations. For each effect node, the accuracies of its micro-causal relationship reconstructions are listed, including the ones from each single cause node (e.g., $B\rightarrow D$ or $C\rightarrow D$), and also the one from combined causes (e.g., $BC\rightarrow D$). We call them ``single-cause'' and ``full-cause'' for clarity. 
We also list the performances of their initialized variable representations on the left side, to provide a comparative baseline. 
In micro-causal modeling, the effect node has two outputs with different data stream inputs. One is input from its own encoder (as in optimization step 2), and the other is from the cause-encoder, i.e., indexing through the relation (as in optimization step 1). Their performances are arranged in the middle part, and on the right side of this table, respectively. 

% For example, node $G$ has three possible causes $C$, $D$, and $E$, so we built four causal effect models in total: the individual pairwise causations $C\rightarrow G$, $D\rightarrow G$, $E\rightarrow G$, and the stacked causal effect $CDE\rightarrow G$. For convenience, we refer to them as "\emph{pair-relation}" and "\emph{stacking-relation}" respectively. We also listed the initial reconstruction performance for each variable in the column named "Variable Reconstruction (initial performance)" as a comparison baseline in Table \ref{tab:unit}.


%\vspace{-2mm}
% Figure environment removed

The KLD metrics in Table \ref{tab:unit} indicate the strength of learned causality, with a lower value signifying stronger. 
Due to the data including numerous meaningful zeros, we have an additional reconstruction for the binary outcome as ``whether zero or not'', named ``mask'' and evaluated in Binary Cross Entropy (BCE).

For example, node $J$'s minimal KLD values suggest a significant effect caused by nodes $G$ (Surface Runoff), $H$ (Lateral), and $I$ (Baseflow). In contrast, the high KLD values imply that predicting variable $I$ using $D$ and $F$ is challenging. 
For nodes $D$, $E$, and $J$, the ``full-cause'' are moderate compared to their ``single-cause'' scores, suggesting a lack of informative associations among the cause nodes. In contrast, for nodes $G$ and $H$, lower ``full-cause'' KLD values imply capturing meaningful associative effects through hierarchical stacking. The KLD metric also reveals the most contributive cause node to the effect node. For example, the proximity of the $C\rightarrow G$ strength to $CDE\rightarrow G$ suggests that $C$ is the primary contributor to this causal relationship.

Figure~\ref{fig:G} showcases reconstructed timing distributions for the effect nodes $J$, $G$, and $I$ in the same synthetic year to provide a straightforward overview of the hierarchical disentanglement performances. 
Here, black dots represent the ground truth; the blue line indicates the initialized variable representation and the ``full-cause'' representation generates the red line. 
In addition to RMSE, we also employ the NashSutcliffe model efficiency coefficient (NSE) as an accuracy metric, commonly used in hydrological predictions. The NSE ranges from -$\infty$ to 1, with values closer to 1 indicating higher accuracy.

The initialized variable representation closely aligns with the ground truth, as shown in Figure~\ref{fig:G}, attesting to the efficacy of our proposed autoencoder architecture. As expected, the ``full-cause'' performs better than the ``single-cause'' for each effect node. Node $J$ exhibits the best prediction, whereas node $I$ presents a challenge. For node $G$, causality from $C$ proves to be significantly stronger than the other two, $D$ and $E$.


% One may observe via the demo that our experiments do not show smooth information flows along successive long causal chains. Since RNNs are designed primarily for capturing the dynamics of causes rather than the effects, relying on them to autonomously construct dynamical representations of the effects might prove unreliable. It underscores a significant opportunity for enhancing effectiveness by improving the architecture.



\vspace{-2mm}
\subsection{DAG Structure Exploration}
\vspace{-2mm}

The first round of detection starts from the source nodes $A$ and $B$ and proceeds to identify their potential edges, until culminating in the target node $J$. Candidate edges are selected based on their contributions to the overall KLD sum (less gain is better). 
Table \ref{tab:discv} shows the detected order of the edges in Figure \ref{fig:stream}, accompanied by corresponding KLD sums in each round, and also the KLD gains after each edge is included. Color-coding in the cells corresponds to Figure \ref{fig:stream}, indicating tiers of causal routines. The arrangement underscores the effectiveness of this latent space exploration approach.

Table \ref{tab:discv_rounds} in Appendix A displays the complete exploration results, with candidate edge evaluations in each round of detection. 
Meanwhile, to provide a clearer context about the dataset qualification with respect to underlying structure identification, we also employ the traditional causal discovery method, Fast Greedy Search (FGES), with a 10-fold cross-validation to perform the same procedure as RIRL exploration. The results in Table \ref{tab:fges} are available in Appendix A, exhibiting the difficulties of using conventional methods.


%\vspace{-3mm}
\begin{table*}[t]
\caption{Performances of micro-causal relationship reconstructions using RIRL, categorized by effect nodes.}
\label{tab:unit}
\vspace{-2mm}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{|l|lll|l|lll|llll|}
\hline
\rowcolor[HTML]{C0C0C0} 
\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} } & \multicolumn{3}{l|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \begin{tabular}[c]{@{}l@{}}\ \ \ \ \ \ Variable Representation\\ \ \ \ \ \ \ \ (Initialized)\end{tabular}}} & \cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} } & \multicolumn{3}{l|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000}  \begin{tabular}[c]{@{}l@{}}\ \ \ \ \ \ Variable Representation\\ \ \ \ \ \ \ \ (in Micro-Causal Models)\end{tabular}}} & \multicolumn{4}{l|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \ \ \ \ \ \ \ \ \ Relation-Indexed Representation}} \\ \cline{2-4} \cline{6-12} 
\rowcolor[HTML]{C0C0C0} 
\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} } & \multicolumn{2}{l|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000}\ \ \ \ \ \  \ \ \ \ RMSE}} & {\color[HTML]{000000} BCE} & \cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} } & \multicolumn{2}{l|}{\cellcolor[HTML]{C0C0C0}\ \ \ \ \ \ \ \ \ \ RMSE} & BCE & \multicolumn{2}{l|}{\cellcolor[HTML]{C0C0C0} \ \ \ \ \ \ \ \ \ RMSE} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}BCE} & KLD \\ \cline{2-4} \cline{6-12} 
\rowcolor[HTML]{C0C0C0} 
\multirow{-3}{*}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \begin{tabular}[c]{@{}l@{}}Efect \\ Node\\ \ \end{tabular}}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Scaled\\ \ \ Values\end{tabular}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Unscaled\\ \ \ Values\end{tabular}} & Mask & \multirow{-3}{*}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \begin{tabular}[c]{@{}l@{}}Cause\\ Node\end{tabular}}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Scaled\\ \ \ Values\end{tabular}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Unscaled\\ \ \ Values\end{tabular}} & Mask & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Scaled\\ \ \ Values\end{tabular}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Unscaled\\ \ \ Values\end{tabular}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Mask} & \begin{tabular}[c]{@{}l@{}}(in latent\\ \ \ space)\end{tabular} \\ \hline
\cellcolor[HTML]{EFEFEF}C & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}0.037} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}0.089} & \cellcolor[HTML]{EFEFEF}0.428 & A & \multicolumn{1}{l|}{0.0295} & \multicolumn{1}{l|}{0.0616} & 0.4278 & \multicolumn{1}{l|}{0.1747} & \multicolumn{1}{l|}{0.3334} & \multicolumn{1}{l|}{0.4278} & 7.6353 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & BC & \multicolumn{1}{l|}{0.0350} & \multicolumn{1}{l|}{1.0179} & 0.1355 & \multicolumn{1}{l|}{0.0509} & \multicolumn{1}{l|}{1.7059} & \multicolumn{1}{l|}{0.1285} & 9.6502 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & B & \multicolumn{1}{l|}{0.0341} & \multicolumn{1}{l|}{1.0361} & 0.1693 & \multicolumn{1}{l|}{0.0516} & \multicolumn{1}{l|}{1.7737} & \multicolumn{1}{l|}{0.1925} & 8.5147 \\ \cline{5-5}
\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}D} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.015}} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.679}} & \multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.445} & C & \multicolumn{1}{l|}{0.0331} & \multicolumn{1}{l|}{0.9818} & 0.3404 & \multicolumn{1}{l|}{0.0512} & \multicolumn{1}{l|}{1.7265} & \multicolumn{1}{l|}{0.3667} & 10.149 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & BC & \multicolumn{1}{l|}{0.4612} & \multicolumn{1}{l|}{26.605} & 0.6427 & \multicolumn{1}{l|}{0.7827} & \multicolumn{1}{l|}{45.149} & \multicolumn{1}{l|}{0.6427} & 39.750 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & B & \multicolumn{1}{l|}{0.6428} & \multicolumn{1}{l|}{37.076} & 0.6427 & \multicolumn{1}{l|}{0.8209} & \multicolumn{1}{l|}{47.353} & \multicolumn{1}{l|}{0.6427} & 37.072 \\ \cline{5-5}
\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}E} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.058}} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}3.343}} & \multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.643} & C & \multicolumn{1}{l|}{0.5212} & \multicolumn{1}{l|}{30.065} & 1.2854 & \multicolumn{1}{l|}{0.7939} & \multicolumn{1}{l|}{45.791} & \multicolumn{1}{l|}{1.2854} & 46.587 \\ \hline
\cellcolor[HTML]{EFEFEF}F & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}0.326} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}7.178} & \cellcolor[HTML]{EFEFEF}2.045 & E & \multicolumn{1}{l|}{0.4334} & \multicolumn{1}{l|}{8.3807} & 3.0895 & \multicolumn{1}{l|}{0.4509} & \multicolumn{1}{l|}{5.9553} & \multicolumn{1}{l|}{3.0895} & 53.680 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & CDE & \multicolumn{1}{l|}{0.0538} & \multicolumn{1}{l|}{0.9598} & 0.0878 & \multicolumn{1}{l|}{0.1719} & \multicolumn{1}{l|}{3.5736} & \multicolumn{1}{l|}{0.1340} & 8.1360 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & C & \multicolumn{1}{l|}{0.1057} & \multicolumn{1}{l|}{1.4219} & 0.1078 & \multicolumn{1}{l|}{0.2996} & \multicolumn{1}{l|}{4.6278} & \multicolumn{1}{l|}{0.1362} & 11.601 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & D & \multicolumn{1}{l|}{0.1773} & \multicolumn{1}{l|}{3.6083} & 0.1842 & \multicolumn{1}{l|}{0.4112} & \multicolumn{1}{l|}{8.0841} & \multicolumn{1}{l|}{0.2228} & 27.879 \\ \cline{5-5}
\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}G} & \multicolumn{1}{l|}{\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}0.045}} & \multicolumn{1}{l|}{\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}0.81}} & \multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}1.327} & E & \multicolumn{1}{l|}{0.1949} & \multicolumn{1}{l|}{4.7124} & 0.1482 & \multicolumn{1}{l|}{0.5564} & \multicolumn{1}{l|}{10.852} & \multicolumn{1}{l|}{0.1877} & 39.133 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & DE & \multicolumn{1}{l|}{0.0889} & \multicolumn{1}{l|}{0.0099} & 2.5980 & \multicolumn{1}{l|}{0.3564} & \multicolumn{1}{l|}{0.0096} & \multicolumn{1}{l|}{2.5980} & 21.905 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & D & \multicolumn{1}{l|}{0.0878} & \multicolumn{1}{l|}{0.0104} & 0.0911 & \multicolumn{1}{l|}{0.4301} & \multicolumn{1}{l|}{0.0095} & \multicolumn{1}{l|}{0.0911} & 25.198 \\ \cline{5-5}
\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}H} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.045}} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.009}} & \multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}1.345} & E & \multicolumn{1}{l|}{0.1162} & \multicolumn{1}{l|}{0.0105} & 0.1482 & \multicolumn{1}{l|}{0.5168} & \multicolumn{1}{l|}{0.0097} & \multicolumn{1}{l|}{3.8514} & 39.886 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & DF & \multicolumn{1}{l|}{0.0600} & \multicolumn{1}{l|}{0.0103} & 3.4493 & \multicolumn{1}{l|}{0.1158} & \multicolumn{1}{l|}{0.0099} & \multicolumn{1}{l|}{3.4493} & 49.033 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & D & \multicolumn{1}{l|}{0.1212} & \multicolumn{1}{l|}{0.0108} & 3.0048 & \multicolumn{1}{l|}{0.2073} & \multicolumn{1}{l|}{0.0108} & \multicolumn{1}{l|}{3.0048} & 75.577 \\ \cline{5-5}
\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}I} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.035}} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.009}} & \multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}1.672} & F & \multicolumn{1}{l|}{0.0540} & \multicolumn{1}{l|}{0.0102} & 3.4493 & \multicolumn{1}{l|}{0.0948} & \multicolumn{1}{l|}{0.0098} & \multicolumn{1}{l|}{3.4493} & 45.648 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & GHI & \multicolumn{1}{l|}{0.0052} & \multicolumn{1}{l|}{0.0742} & 0.2593 & \multicolumn{1}{l|}{0.0090} & \multicolumn{1}{l|}{0.1269} & \multicolumn{1}{l|}{0.2937} & 5.5300 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & G & \multicolumn{1}{l|}{0.0077} & \multicolumn{1}{l|}{0.1085} & 0.4009 & \multicolumn{1}{l|}{0.0099} & \multicolumn{1}{l|}{0.1390} & \multicolumn{1}{l|}{0.4375} & 5.2924 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & H & \multicolumn{1}{l|}{0.0159} & \multicolumn{1}{l|}{0.2239} & 0.4584 & \multicolumn{1}{l|}{0.0393} & \multicolumn{1}{l|}{0.5520} & \multicolumn{1}{l|}{0.4938} & 15.930 \\ \cline{5-5}
\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}J} & \multicolumn{1}{l|}{\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}0.007}} & \multicolumn{1}{l|}{\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}0.098}} & \multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}1.088} & I & \multicolumn{1}{l|}{0.0308} & \multicolumn{1}{l|}{0.4328} & 0.3818 & \multicolumn{1}{l|}{0.0397} & \multicolumn{1}{l|}{0.5564} & \multicolumn{1}{l|}{0.3954} & 17.410 \\ \hline
\end{tabular}
}
\vspace{-3mm}
\end{table*}

\begin{table*}[t]
%\vspace{-2.5mm}
\caption{The brief results from the RIRL exploration.}
\label{tab:discv}
\vspace{-2mm}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{|c|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
\cellcolor[HTML]{C0C0C0}Edge & \cellcolor[HTML]{FFCCC9}A$\rightarrow$C & \cellcolor[HTML]{FFCCC9}B$\rightarrow$D & \cellcolor[HTML]{FFCCC9}C$\rightarrow$D & \cellcolor[HTML]{FFCCC9}C$\rightarrow$G & \cellcolor[HTML]{FFCCC9}D$\rightarrow$G & \cellcolor[HTML]{FFCCC9}G$\rightarrow$J & \cellcolor[HTML]{FFCCC9}D$\rightarrow$H & \cellcolor[HTML]{FFCCC9}H$\rightarrow$J & \cellcolor[HTML]{FFCE93}B$\rightarrow$E & \cellcolor[HTML]{FFCE93}E$\rightarrow$G & \cellcolor[HTML]{FFCE93}E$\rightarrow$H & \cellcolor[HTML]{FFCE93}C$\rightarrow$E & \cellcolor[HTML]{ABE9E7}E$\rightarrow$F & \cellcolor[HTML]{ABE9E7}F$\rightarrow$I & \cellcolor[HTML]{ABE9E7}I$\rightarrow$J & \cellcolor[HTML]{ABE9E7}D$\rightarrow$I \\ \hline
\cellcolor[HTML]{C0C0C0}KLD & 7.63 & 8.51 & 10.14 & 11.60 & 27.87 & 5.29 & 25.19 & 15.93 & 37.07 & 39.13 & 39.88 & 46.58 & 53.68 & 45.64 & 17.41 & 75.57 \\ \hline
\rowcolor[HTML]{FFFFFF} 
\cellcolor[HTML]{C0C0C0}Gain & 7.63 & 8.51 & 1.135 & 11.60 & 2.454 & 5.29 & 25.19 & 0.209 & 37.07 & -5.91 & -3.29 & 2.677 & 53.68 & 45.64 & 0.028 & 3.384 \\ \hline
\end{tabular}}
\vspace{-4mm}
\end{table*}



\section{Conclusions}\label{sec:conclusion}
\vspace{-3mm}

This paper focuses on the inherent challenges of the traditional i.i.d.-based learning paradigm in addressing causal relationships. Conventionally, we construct statistical models as observers of the world, grounded in epistemology. However, adopting this perspective assumes that our observations accurately reflect the ``reality'' as we understand it, implying that seemingly objective models may actually be based on subjective assumptions. 
This fundamental issue has become increasingly evident in causality modeling, especially with the rise of applications in causal representation learning that aim to automate the specification of causal variables traditionally done manually.

Our understanding of causality is fundamentally based on the creator's perspective, as the ``what...if'' questions are only valid within the possible world we conceive in our consciousness. 
The advocated ``perspective shift''  represents a transformation from an \emph{object-first} to a \emph{relation-first} modeling paradigm, a change that transcends mere methodological or technical advancements. Indeed, this shift has been facilitated by the advent of AI, particularly through neural network-based representation learning, which lays the groundwork for implementing \emph{relation-first} modeling in computer engineering.

The limitation of the observer's perspective in traditional causal inference prevents the capture of dynamic causal outcomes, namely, the nonlinear timing distributions across multiple ``possible timelines''. Accordingly, this oversight has led to compensatory efforts, such as the introduction of hidden confounders and the reliance on the sufficiency assumption. 
These theories have been instrumental in developing knowledge systems across various fields over the past decades. However, with the rapid advancement of AI techniques, the time has come to move beyond the conventional modeling paradigm toward the potential realization of AGI.

In this paper, we present the \emph{relation-first} framework for causality modeling, based on discussions about its philosophical and mathematical underpinnings.
Adopting this new framework allows us to significantly simplify or even bypass complex questions. We also introduce the Relation-Indexed Representation Learning (RIRL) method as an initial application of the \emph{relation-first} paradigm, supported by experiments that validate its efficacy.

% This paper introduces a dimensionality framework from a \emph{Relation-Oriented} perspective to decompose our cognitive space, where relational causal knowledge is stored. 
% Specifically, it conceptualizes the unobservable relations between cause and effect as informative variables in $\mathbb{R}^H$; and the causal structure of dynamics in knowledge, represented by the enhanced DAG, is accommodated by the counterfactual space $\mathbb{R}^T$, 
% across multiple relative timing axes with nonlinear dependence. 
% It highlights the key oversights of the current \emph{Observation-Oriented} paradigm, which relies on the observational i.i.d. assumption and is confined to $\mathbb{R}^O$. 
% %It relies on manual specification to identify dynamical effects from observational static sequences, inherently fraught with difficulty.
% %Specifically, based on the observational i.i.d. assumption, conventional relationship modeling intrinsically overlooks 1) the informative unobservables in $\mathbb{R}^H$, and 2) the structuralized dynamics within multi-dimensional $\mathbb{R}^T$. Instead, due to being confined within $\mathbb{R}^O$, 

% The traditional causal inference, adopting a \emph{Relation-Oriented} viewpoint, identifies the underlying causal structures across relative timings but overlooks the $\mathbb{R}^T$ space due to neglecting temporal nonlinearities, i.e., the dynamics. 
% Under the \emph{Observation-Oriented} paradigm, contemporary causal learning is often challenged by incompletely captured dynamical effects without considering the indexing role of unobservable relations lying in $\mathbb{R}^H$.
% In the case of LLMs, while AI techniques enable the autonomous identification of dynamical effects, they often neglect their interactions, which are emphasized as causal structures in causal inference.

% % When viewed through the lens of the \emph{Relation-Oriented} framework, the multifaceted issues surrounding causality learning become unified, addressing common confusions and concerns from traditional causal inference to modern LLMs. 
% Recalling the queries presented in the Introduction, we systematically summarize these application-related restrictions in our pursuit of AGI, and offer new insights:
% \begin{enumerate}[itemsep=0em, topsep=-1pt,
% parsep=2pt, partopsep=0pt,
% leftmargin=22pt, labelwidth=10pt]
% \item[\ding{118}] \emph{Firstly}, challenges for causal inference models primarily arise from overlooking dynamics, due to their linear modeling constraints. This oversight leads to various compensatory efforts, such as introducing hidden confounders and relying on the causal sufficiency assumption. 
% Causal DAGs inherently provide a \emph{Relation-Oriented} view; with the proposed enhancement incorporating them into the counterfactual $\mathbb{R}^T$ space, they can provide essential support for illustrating structuralized dynamics.

% \item[\ding{118}] \emph{Secondly}, our knowledge inherently contains hierarchical levels due to hidden relations $\omega\in \mathbb{R}^H$, which necessitates generalizability of models. 
% For AI-based causal models, the main challenge lies in incorporating the underlying structure of dynamics to achieve dynamical generalizability. The new paradigm we propose introduces a relation-indexing methodology, enabling the autonomous construction of causal structures by sequentially extracting causal representations.


% \item[\ding{118}] \emph{Thirdly}, while existing language models have made strides in generalizability through meta-learning, they are still limited to absolute timing within $\mathbb{R}^O$, implicitly assuming nonlinear independence among temporal dimensions. Additionally, their neglect of extracting informative $\theta$ prevents them from truly ``understanding'' the captured relationships.
% However, LLMs have demonstrated the effectiveness of meta-learning in addressing temporal dimensional hierarchies, suggesting a promising prospect for \emph{Relation-Oriented} meta-learning in advancing towards AGI.
% \end{enumerate}

% We also introduce a baseline implementation of the \emph{Relation-Oriented} paradigm, primarily to validate the efficacy of the ``relation-indexing'' methodology in implementing causal representations and constructing knowledge-aligned causal structures. Similar thoughts have been attempted in domains with well-established causal knowledge, such as the hierarchical temporal memory in neuroscience \cite{wu2018hierarchical}. The pursuit of AGI is a historically extensive and complex endeavor, requiring a wide array of knowledge-aligned AI model constructions. This study aims to provide foundational insights for future developments in this field.




% Driven by the misalignment issues between causal knowledge and established causal models in widespread AI applications, this study examines fundamental limitations of the dominant \emph{Observation-Oriented} learning paradigm. In response, we advocate for a novel \emph{Relation-Oriented} paradigm, inspired by the relation-centric nature of human knowledge, and complemented by a practical approach of \emph{Relation-Indexed Representation Learning} (RIRL), with demonstrated efficacy.

% The concept of a ``hyper-dimension'' is initially proposed, as an accommodation for unobservable knowledge. We subsequently build a comprehensive framework of dimensionality, to offer more intuitive insights into relationship learning. 
% The discrepancy, between our comprehension of ``time'' and the single timeline used in our causal models, inherently causes misalignment, and results in model generalizability issues.

% \emph{Relation-Oriented} reflects the process of human understanding, 
% aims to mitigate AI misalignment, paving the way toward causally interpretable AGI. Constructing AGI is a long-term, intricate process requiring extensive work within interdisciplinary efforts, and we seek to lay a foundation for its future advancements.



\ifpreprint
\vspace{5mm}
\section*{Acknowledgements}
\vspace{-2mm}

I'd like to extend my heartfelt thanks to the reviewers from TMLR, who have provided invaluable feedback vital for this theory's final completion. Additionally, my gratitude goes to GPT-4 for its assistance in enhancing my English writing.
I also wish to thank my advisor, Prof. Vipin Kumar, for the initial support in the beginning stage of this work.

\hfill Jia Li, Feb 2024

\vspace{10mm}

\else
\vspace{60mm}
%\pagebreak
\fi

\bibliography{main}
\bibliographystyle{tmlr}


\appendix
\section{Appendix: Complete Experimental Results in DAG Structure Exploration Test}
%\includepdf[pages=-]{mainz_append_pdf.pdf}

\pagebreak


%\newpage
\begin{sidewaystable}
%\begin{table*}[t]
\centering
\caption{The Complete Results of RIRL Exploration in the Latent Space.
Each row stands for a round of detection, with `\#'' identifying the round number, and all candidate edges are listed with their KLD gains as below. 1) Green cells: the newly detected edges. 2) Red cells: the selected edge.
3) Blue cells: the trimmed edges accordingly. }
%\vspace{-20mm}
\label{tab:discv_rounds}
\resizebox{1\columnwidth}{!}{%
%\rotatebox{90}{

\begin{tabular}{|c|c|c|cccccccccccccc}
\cline{1-9}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \textbf{A$\rightarrow$ C}} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ D & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{B$\rightarrow$ C}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ D} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 1}} & \cellcolor[HTML]{FFCCC9}{\color[HTML]{000000} \textbf{7.6354}} & 19.7407 & \multicolumn{1}{c|}{60.1876} & \multicolumn{1}{c|}{119.7730} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{8.4753}} & \multicolumn{1}{c|}{8.5147} & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} &  &  &  &  &  &  &  &  \\ \cline{1-13}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ D & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{B$\rightarrow$ D}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ D} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 2}} & 19.7407 & 60.1876 & \multicolumn{1}{c|}{119.7730} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{8.5147}}} & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}10.1490} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}46.5876} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}111.2978} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}11.6012} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}39.2361} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}95.1564} &  &  &  &  \\ \hline
\rowcolor[HTML]{C0C0C0} 
\cellcolor[HTML]{C0C0C0} & \textbf{A$\rightarrow$ D} & A$\rightarrow$ E & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{C$\rightarrow$ D}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 3}} & \cellcolor[HTML]{CBCEFB}\textbf{9.7357} & 60.1876 & \multicolumn{1}{c|}{119.7730} & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{1.1355}}} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{11.6012} & \multicolumn{1}{c|}{39.2361} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}63.7348} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}123.3203} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}27.8798} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}25.1988} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}75.5775} \\ \hline
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \textbf{C$\rightarrow$ G}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 4}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{000000} \textbf{11.6012}}} & \multicolumn{1}{c|}{39.2361} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{27.8798} & \multicolumn{1}{c|}{25.1988} & \multicolumn{1}{c|}{75.5775} &  &  \\ \cline{1-15}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{D$\rightarrow$ G}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}G$\rightarrow$ J} &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 5}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{39.2361} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{2.4540}}} & \multicolumn{1}{c|}{25.1988} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}5.2924} &  &  \\ \cline{1-15}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{G$\rightarrow$ J}}} &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 6}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{39.2361} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{25.1988} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{5.2924}}} &  &  &  \\ \cline{1-14}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{C$\rightarrow$ H}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{D$\rightarrow$ H}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 7}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{39.2361}} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{25.1988}}} & \multicolumn{1}{c|}{75.5775} &  &  &  &  \\ \cline{1-13}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{H$\rightarrow$ J}}} &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 8}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{0.2092}}} &  &  &  &  &  \\ \cline{1-12}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}\textbf{A$\rightarrow$ E} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{C$\rightarrow$ E}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 9}} & \cellcolor[HTML]{CBCEFB}\textbf{60.1876} & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{46.5876}}} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} &  &  &  &  &  &  \\ \cline{1-13}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{B$\rightarrow$ E}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{D$\rightarrow$ E}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ I} &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 10}} & 119.7730 & \cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{-6.8372}} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{17.0407}} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}53.6806} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}-5.9191} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}-3.2931} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}110.2558} &  &  &  &  \\ \cline{1-13}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \cellcolor[HTML]{C0C0C0}B$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{E$\rightarrow$ G}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ I} &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 11}} & 119.7730 & 132.7717 & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{53.6806} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{-5.9191}} & \multicolumn{1}{c|}{-3.2931} & \multicolumn{1}{c|}{110.2558} &  &  &  &  &  &  \\ \cline{1-11}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \cellcolor[HTML]{C0C0C0}B$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{E$\rightarrow$ H}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ I} &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 12}} & 119.7730 & 132.7717 & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{53.6806} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{-3.2931}} & \multicolumn{1}{c|}{110.2558} &  &  &  &  &  &  &  \\ \cline{1-10}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}\textbf{A$\rightarrow$ F} & \cellcolor[HTML]{C0C0C0}\textbf{B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{C$\rightarrow$ F}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{D$\rightarrow$ F}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{E$\rightarrow$ F}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ I} &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 13}} & \cellcolor[HTML]{CBCEFB}\textbf{119.7730} & \cellcolor[HTML]{CBCEFB}\textbf{132.7717} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{111.2978}} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{123.3203}} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{53.6806}} & \multicolumn{1}{c|}{110.2558} &  &  &  &  &  &  &  &  \\ \cline{1-9}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}C$\rightarrow$ I & \cellcolor[HTML]{C0C0C0}D$\rightarrow$ I & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{E$\rightarrow$ I}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{F$\rightarrow$ I}} &  &  &  &  &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 14}} & 95.1564 & 75.5775 & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{110.2558}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{45.6490}} &  &  &  &  &  &  &  &  &  &  &  &  \\ \cline{1-5}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}C$\rightarrow$ I & \cellcolor[HTML]{C0C0C0}D$\rightarrow$ I & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{I$\rightarrow$ J}} &  &  &  &  &  &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 15}} & 15.0222 & 3.3845 & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.0284}} &  &  &  &  &  &  &  &  &  &  &  &  &  \\ \cline{1-4}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}\textbf{C$\rightarrow$ I} & \cellcolor[HTML]{C0C0C0}\textbf{D$\rightarrow$ I} &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 16}} & \cellcolor[HTML]{CBCEFB}\textbf{15.0222} & \cellcolor[HTML]{FFCCC9}\textbf{3.3845} &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\ \cline{1-3}
\end{tabular}

}%}
%\vspace{-3mm}
%\end{table*}

\end{sidewaystable}







\begin{sidewaystable}
%\centering

\caption{Average performance of 10-Fold FGES (Fast Greedy Equivalence Search) causal discovery, with the prior knowledge that each node can only cause the other nodes with the same or greater depth with it. An edge means connecting two attributes from two different nodes, respectively. Thus, the number of possible edges between two nodes is the multiplication of the numbers of their attributes, i.e., the lengths of their data vectors.
\\ (All experiments are performed with 6 different Independent-Test kernels, including chi-square-test, d-sep-test, prob-test, disc-bic-test, fisher-z-test, mvplr-test. But their results turn out to be identical.)}
\label{tab:fges}
\resizebox{1\columnwidth}{!}{%
%\Rotatebox{90}{
%
\begin{tabular}{cccccccccccccccccc}
\hline
\rowcolor[HTML]{C0C0C0} 
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}Cause Node} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A} & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}B} & \multicolumn{3}{c|}{\cellcolor[HTML]{C0C0C0}C} & \multicolumn{3}{c|}{\cellcolor[HTML]{C0C0C0}D} & \multicolumn{3}{c|}{\cellcolor[HTML]{C0C0C0}E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}F} & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}I} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}True\\ Causation\end{tabular}} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}A$\rightarrow$ C} & \multicolumn{1}{c|}{B$\rightarrow$ D} & \multicolumn{1}{c|}{B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}C$\rightarrow$ D} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}C$\rightarrow$ G} & \multicolumn{1}{c|}{D$\rightarrow$ G} & \multicolumn{1}{c|}{D$\rightarrow$ H} & \multicolumn{1}{c|}{D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}E$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}E$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}E$\rightarrow$ H} & \multicolumn{1}{c|}{F$\rightarrow$ I} & \multicolumn{2}{c|}{\cellcolor[HTML]{EFEFEF}G$\rightarrow$ J} & \multicolumn{1}{c|}{H$\rightarrow$ J} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}I$\rightarrow$ J} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Number of\\ Edges\end{tabular}} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}16} & \multicolumn{1}{c|}{24} & \multicolumn{1}{c|}{16} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}6} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}4} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8} & \multicolumn{1}{c|}{12} & \multicolumn{1}{c|}{12} & \multicolumn{1}{c|}{9} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8} & \multicolumn{1}{c|}{12} & \multicolumn{2}{c|}{\cellcolor[HTML]{EFEFEF}4} & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}3} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Probability\\ of Missing\end{tabular}} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.038889} & \multicolumn{1}{c|}{0.125} & \multicolumn{1}{c|}{0.125} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.062} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.06875} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.039286} & \multicolumn{1}{c|}{0.069048} & \multicolumn{1}{c|}{0.2} & \multicolumn{1}{c|}{0.142857} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.3} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.003571} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.2} & \multicolumn{1}{c|}{0.142857} & \multicolumn{2}{c|}{\cellcolor[HTML]{EFEFEF}0.0} & \multicolumn{1}{c|}{0.072727} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.030303} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Wrong \\ Causation\end{tabular}} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}C$\rightarrow$ F} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{D$\rightarrow$ E} & \multicolumn{1}{c|}{D$\rightarrow$ F} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{F$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}G$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}G$\rightarrow$ I} & \multicolumn{1}{c|}{H$\rightarrow$ I} &  \\ \cline{1-1} \cline{5-5} \cline{9-10} \cline{14-17}
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Times\\ of Wrongly\\ Discovered\end{tabular}} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}5.6} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{1.2} & \multicolumn{1}{c|}{0.8} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{5.0} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8.2} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}3.0} & \multicolumn{1}{c|}{2.8} &  \\ \cline{1-1} \cline{5-5} \cline{9-10} \cline{14-17}
\multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{}
\end{tabular}
%}
}


\vspace{0.4in}
%\end{table*}

%\begin{table*}[t]
%\centering
\caption{ Brief Results of the Heuristic Causal Discovery in latent space, identical with Table 3 in the paper body, for better comparison to the traditional FGES methods results on this page.\\
The edges are arranged in detected order (from left to right) and their measured causal strengths in each step are shown below correspondingly.\\
Causal strength is measured by KLD values (less is stronger). Each round of detection is pursuing the least KLD gain globally. All evaluations are in 4-Fold validation average values. Different colors represent the ground truth causality strength tiers (referred to  the Figure 10 in the paper body).
}
\label{tab:discv}
%\resizebox{0.1\columnwidth}{!}{%
%\Rotatebox{90}{
\vspace{2mm}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{|c|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
\cellcolor[HTML]{C0C0C0}Causation & \cellcolor[HTML]{FFCCC9}A$\rightarrow$ C & \cellcolor[HTML]{FFCCC9}B$\rightarrow$ D & \cellcolor[HTML]{FFCCC9}C$\rightarrow$ D & \cellcolor[HTML]{FFCCC9}C$\rightarrow$ G & \cellcolor[HTML]{FFCCC9}D$\rightarrow$ G & \cellcolor[HTML]{FFCCC9}G$\rightarrow$ J & \cellcolor[HTML]{FFCCC9}D$\rightarrow$ H & \cellcolor[HTML]{FFCCC9}H$\rightarrow$ J & \cellcolor[HTML]{FFCE93}C$\rightarrow$ E & \cellcolor[HTML]{FFCE93}B$\rightarrow$ E & \cellcolor[HTML]{FFCE93}E$\rightarrow$ G & \cellcolor[HTML]{FFCE93}E$\rightarrow$ H & \cellcolor[HTML]{ABE9E7}E$\rightarrow$ F & \cellcolor[HTML]{ABE9E7}F$\rightarrow$ I & \cellcolor[HTML]{ABE9E7}I$\rightarrow$ J & \cellcolor[HTML]{ABE9E7}D$\rightarrow$ I \\ \hline
\cellcolor[HTML]{C0C0C0}KLD & 7.63 & 8.51 & 10.14 & 11.60 & 27.87 & 5.29 & 25.19 & 15.93 & 46.58 & 65.93 & 39.13 & 39.88 & 53.68 & 45.64 & 17.41 & 75.57 \\ \hline
\rowcolor[HTML]{FFFFFF} 
\cellcolor[HTML]{C0C0C0}Gain & 7.63 & 8.51 & 1.135 & 11.60 & 2.454 & 5.29 & 25.19 & 0.209 & 46.58 & -6.84 & -5.91 & -3.29 & 53.68 & 45.64 & 0.028 & 3.384 \\ \hline
\end{tabular}
%}}
%\end{table*}
}

\end{sidewaystable}








\end{document}
