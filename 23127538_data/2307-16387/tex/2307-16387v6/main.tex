
\documentclass[10pt]{article} % For LaTeX2e
%\usepackage{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}

% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage{graphicx}
\usepackage{adjustbox}
%\usepackage{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{centernot}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{color}
%\usepackage{booktabs}

\usepackage{subfigure}
\usepackage{caption}
\usepackage{dsfont}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{empheq}
%\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{totcount}

\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{makecell}
%\usepackage[table]{xcolor}
\usepackage{xcolor}
\usepackage{changepage}
\usepackage{stfloats}
%\usepackage{cite}
\usepackage{soul}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{algpseudocode}
\usepackage[title]{appendix}
\usepackage{pdfpages}

\usepackage{bm}
\usepackage{amsmath,amssymb,amsfonts}
% \usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{bookmark}
\usepackage{float}
\usepackage{tabularx}
\usepackage{mathtools}
\usepackage{array}
\usepackage{rotating}
\usepackage[skins]{tcolorbox}

\title{Relation-Oriented: Toward Causal Knowledge-Aligned AI}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{\name Jia Li \email jiaxx213@umn.edu \\
      \addr Department of Computer Science,
      University of Minnesota
      \AND
      \name Xiang Li \email lixx5000@umn.edu \\
      \addr Department of Bioproducts and Biosystems Engineering,       University of Minnesota
      % \name Xiaowei Jia \email xiaowei@pitt.edu\\
      % \addr Department of Computer Science\\ University of Pittsburgh
      }

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{Apr}  % Insert correct month for camera-ready version
\def\year{2023} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version

\definecolor{thegrey}{RGB}{192,192,192}
\definecolor{orchid}{RGB}{204,153,255}

\begin{document}


\maketitle

\begin{abstract}
\vspace{-2mm}
This study examines the inherent limitations of the prevailing \emph{Observation-Oriented} learning paradigm by understanding relationship modeling from a unique dimensionality perspective.
This paradigm necessitates the identification of modeling objects prior to
defining relations, confining models to observational space, and limiting their access to dynamical temporal features.
By relying on a singular, absolute timeline, it often neglects the multi-dimensional nature of the temporal feature space. This oversight compromises model robustness and generalizability, contributing significantly to the AI misalignment issue.

Drawing from the relation-centric essence of human cognition, this study presents a new \emph{Relation-Oriented} paradigm, complemented by its methodological counterpart, the \emph{relation-defined representation} learning, supported by extensive efficacy experiments. 

\vspace{-2mm}
\end{abstract}

\section{Introduction}
\vspace{-2mm}

\label{sec:intro}
% Causality plays a fundamental role in many fields, such as meteorology, biology, epidemiology, economics, and more. The main goal of causality is to uncover causal relationships from observational data generated by causal processes. Classical statistical methods for causal inference have been successful in constructing domain knowledge over the past few decades \cite{wood2015lesson,vukovic2022causal,ombadi2020evaluation}. However, the technical progress in recent years has dramatically increased the volume and quality of data collected, making building Bayesian networks from observational data challenging due to NP-hardness. On the other hand, machine learning (ML) techniques from computer science have demonstrated advanced effectiveness in handling big data for causal inference \cite{scheines1997introduction, ahmad2018interpretable, sanchez2022causal}. The success of ML-based causal inference is considered to be the bedrock of realizing artificial intelligence \cite{li2020causal}.



%% As widely recognized, AI is primarily designed to emulate human intelligence rather than genuinely comprehending information, unless it is deliberately trained to extract specific features. Achieving explicit interpretability to a certain extent is possible through such targeted training. For instance, the employment of Variational Autoencoders (VAEs) and the subsequent realization of Disentangled Representations based on them serve as a prime example. VAEs facilitate the learning of meaningful and interpretable latent features by separating distinct factors of variation in the data. This approach enables more straightforward manipulation of specific data aspects while keeping others constant, thereby enhancing interpretability. Practical applications of this concept include generating images with controlled attributes such as facial expressions, lighting conditions, or object orientations, as well as disentangling the underlying structure of complex data in various fields like natural language processing, medical imaging, and anomaly detection.

% The fundamental principle of modeling natural phenomena lies in establishing equations based on observables responsible for time evolution, which can be traced back to the Picard-Lindelof theorem introduced in the late 1800s. Upon this foundation, causal inference theories have been extensively developed in the field of statistics, alongside causality-based machine learning approaches in computer science. Intriguingly, this is not the typical manner in which humans process causal information, creating a significant barrier in our pursuit of developing knowledgeable causal AI systems.


The prevailing modeling paradigm rules that observed variables (and outcomes) are the premise of building relationships.
Model variables are often estimated by their observational values with an independent and identical distribution (i.i.d.) setting. Back in the 1890s, Picard-Lindelof theorem introduced a \emph{logical timeline} $t$ to record observational timestamps, establishing the paradigm $x_{t+1}=f(x_t)$ to depict variable $X$'s time evolution. 
Since then, this \textbf{\emph{Observation-Oriented}} principle has become our learning convention, where the temporal dimensionality is equated to the counts of $\{t, t+1\}$ unit, a predetermined constant time lag. 

For a relationship $X\rightarrow Y$, the model can be in form $y_{t+m}=f(x_t)$, or $y_{t+m}=f(\{x_t\})$, where $\{x_t\}=\{x_1, \ldots, x_{t}, x_{t+1}, \ldots, x_T\}$ represents a time sequence of $X$ within a certain length $T$, and a predetermined time progress $m$ from $X$ to $Y$. No matter in which form, the outcome $Y$ is strictly observational only, leaving all potentially significant temporal changes of $Y$ completely managed by $f(\cdot)$. However, although function $f(\cdot)$ can be selected as \emph{linear} or \emph{nonlinear}, the time evolution from $t$ to ${t+m}$ is always left as \emph{linear}.

Such a conventional linearity on the temporal dimension may be sufficient in the past, but not present, given the current technological advancements in data collection and Artificial Intelligence (AI) learning. Exploring nonlinear temporal distributions is gradually becoming essential. From a broader viewpoint, this is calling for a new modeling paradigm \cite{scholkopf2021toward}, which does not rest on the conventional i.i.d. assumed observations, but can treat $t$ as a distinct computational dimension. 

This study aims to fundamentally reveal the inherent deficiency of the current \emph{Observation-Oriented} modeling paradigm (Chapter I: Sections 2-4), and accordingly propose the new \textbf{\emph{Relation-Oriented}} one as desired, along with feasibility validations (Chapter II: Sections 5-7).
Particularly, the single absolute timeline $t$ that we conventionally use, inherently cannot capture the multifaceted nature of temporal dimensionality, leading to widespread biases and resulting in AI models misaligned with our cognitive understanding, contributing significantly to the AI misalignment issue \cite{christian2020alignment}. 

In this paper, we approach the concept of relationships in modeling through a novel \emph{dimensionality framework}, offering a unique perspective. The remainder of this section aims to lay the groundwork. Then, in Chapter I, we will inspect causal learning from the view of temporal dimensionality, highlighting the key role of relations in modeling. Subsequently, Chapter II will concentrate on the proposed \textbf{\emph{relation-defined representation}} learning method, which embodies the advocated \emph{Relation-Oriented} modeling paradigm.

%We propose that shifting toward a \emph{Relation-Oriented} perspective in model construction is vital for accurately integrating ``time'' as a dimension in our computations.
\vspace{-2mm}

\subsection{Manifestation of AI Misalignment}
\vspace{-2mm}

Today, AI has displayed capabilities surpassing humans in solely observational learning tasks, such as generating images, Go gaming, and so on. However, AI may appear ``unintelligent'' in comprehending certain relations that humans find intuitive. For instance, AI-created personas on social media can have realistic faces but barely with the presence of hands, due to AI struggling with the complex structure, instead treating hands as arbitrary assortments of finger-like items.

Moreover, when it comes to time evolution, causal reasoning presents a substantial challenge for AI, although it is innate for humans. Traditional causal learning methods, while having made valuable contributions to various fields of knowledge over the years \cite{wood2015lesson, vukovic2022causal, ombadi2020evaluation}, often suffer from a limitation in their generalizability \cite{scholkopf2021toward}. Unsuccessful neural network applications are particularly evident when addressing large-scale causal questions \cite{luo2020causal}. As a result, these methods are often confined to context-specific applications and encounter difficulties in extending to diverse scenarios. Thus, it is not strange that AI's capability on the temporal dimension remains notably constrained.

The questions ``How to leverage AI's capability in causality'' and ``How to simulate hands with reasonable fingers'' may seemly pertain to specific domains such as causal inference and computer vision. However, they fundamentally converge toward the broader challenge of AI Alignment, encapsulated by the essential question: ``\emph{Why are these relations unseen to AI?}'' 
Reflecting on Dr. Geoffrey Hinton's warning, the misalignment of AI capabilities with human values can result in unintended and potentially harmful consequences. It is becoming increasingly critical to address this essential question.

\vspace{-2mm}
\subsection{Relations in Hyper-Dimension}
\vspace{-1mm}

Consider a pairwise relationship comprised of three elements: two \emph{observable} objects, and a relation connecting them, which comes from our knowledge. The two objects can be solely observational (e.g., images, spatial coordinates of a quadrotor, etc.), or either observational-temporal (e.g., trends of stocks, persistent rain for five hours, etc.).
Interestingly, the ``relation'' has to be \textbf{\emph{unobservable}} to make this relationship meaningful for machine learning, distinguished from mere statistical dependencies. 

This principle was initially introduced in the form of Common Cause \cite{dawid1979conditional, scholkopf2021toward}, suggesting that any nontrivial conditional independence between two observables requires a third, mutual cause (i.e., our \emph{unobservable} ``relation'').
Take the relationship ``Bob has a son named Jim'' as an example. The father-son relation is unobservable information that exists in our knowledge, which can also be seen as the common cause that makes their connection unique rather than any random pairing of ``Bob'' and ``Jim''. Given sufficient observed social activities, AI may deduce this pair of ``Bob'' and ``Jim'' have some special connection, but that does not equate to discerning their genuine father-son relation. 

Put simply, the existence of unobservable element(s) makes the relationship model informative. In other words, the information contained by the model stems from our knowledge, rather than direct observations.
Let's denote the model as $Y=f(X;\theta)$ with $\theta$ indicating the function parameter in demand. Then, in the context of modeling, the term ``relation'' can be represented by $\theta$.

% Figure environment removed
\vspace{-2mm}

From a dimensionality standpoint, a relationship can be viewed as a joint distribution across multiple dimensions: The observable objects feature the distribution on observational-temporal dimensions, while the unobservable relation manifests as some unseen distribution on a hyper-dimension. As illustrated in Figure~\ref{fig:space}, our cognitive space storing the knowledge relationships can be divided into three categories accordingly, where the \textbf{\emph{Hyper-Dimensional}} space symbolizes the collective of all unobservable relations within our knowledge.
Chapter I of this study aims to examine why AI cannot autonomously model certain relations in this space and understand the implications for its learning results.


\vspace{-1mm}
\subsection{Observational and Temporal Spaces}
\vspace{-2mm}

Under the \emph{Observation-Oriented} principle, current models largely operate within the observational space. For example, CNNs (Convolutional Neural Networks) can learn observational associations among two-dimensional pixels; a quadrotor's movement can be estimated in three spatial dimensions; LLMs (Large Language Models) work in a semantic space along a logical timeline representing the order of words.
%s; and medical effects are assessed using observed vital signs in chronological sequence. 
Some applications (e.g., the last two examples) are aligned with the Picard-Lindelof theorem, using a single logical timeline to depict the absolute time evolution, thus often referred to as spatial-temporal analysis \cite{alkon1988spatial, turner1990spatial, andrienko2003exploratory}. However, in a modeling context, an attribute of timestamps is not distinguishable from other observational attributes, unnecessarily to be temporally significant. Thus, we classify this single absolute timeline scenario as within the observational space.

According to our discussions at the beginning, the form of timestamps can only capture \emph{linear} relationships on the temporal dimension, thus fundamentally impeding AI's ability to handle the temporal \emph{nonlinearity}.
This inherent disparity between our knowledge understanding and established models results in misalignment (see Section \ref{sec:causality3_3} for further discussions), accentuated by the rise of highly efficient AI applications.

Moreover, in our cognition (not the modeling context), \emph{\textbf{multiple} logical timelines} may exist to form the temporal feature space in Figure~\ref{fig:space} (see Section \ref{sec:Temporal} for further insights). 
However, the current modeling paradigm has determined they cannot be distinguished as different dimensions in computation but crudely represented by an attribute of timestamps, i.e., consolidated as a single timeline. 

%Within the current \emph{Observation-Oriented} modeling paradigm, this multifaceted nature can create \emph{inherent temporal biases} in AI computation. Such biases fundamentally impede AI's ability to handle nonlinearities in the temporal dimension, often with underlying causal significance.
%In short, they narrow our successful AI applications within \emph{observational} space only, and obstruct causal learning from achieving generalizable success in \emph{observational-temporal} space.

In the observational-temporal joint space, as shown in Figure~\ref{fig:space}, observable distributions can be categorized as either \emph{linear} or \emph{nonlinear}. The temporal-significant ones can manifest as ``static'' or ``dynamical'' temporal features within a modeling context.
For example, in the relationship ``rain leads to wet floors'', the events ``rain'' and ``wet floors'' are snapshots at specific timestamps and are thus viewed as \emph{\textbf{static} temporal} objects. In contrast, events such as ``persistent rain for five hours'' and ``floors becoming progressively wetter'' are considered \emph{\textbf{dynamical} temporal} due to their indispensable sequential patterns on the temporal dimension.

In this paper, we use the term ``feature'' to indicate the potential variable that fully represents the distribution of interest in any dimension. 
Additionally, the observational-temporal joint space may also be referred to as ``observable data space'', in contrast to the ``latent feature space''.

\vspace{-1mm}
\subsection{Hyper-Dimensional Space}
\vspace{-2mm}

Unobservable relations that fall outside the primary modeling objective can profoundly affect relationship models.
This can be traced back to an undetected joint distribution within the hyper-dimensional space.

For example, when evaluating the impact of spicy foods on health, the direct link between spiciness and health is our primary modeling focus. However, there are underlying relations at play - such as how personal traits (individual-level features) are influenced by their cultural context (population-level features). Even if cultural differences are out of our modeling concern, overlooking these hierarchical distributions may introduce biases into our relationship model.
For clarity, we term these hidden hyper-dimensional distributions as \emph{unobservable \textbf{hierarchies}}, sidestepping their relational aspects that fall outside the modeling objective.





% This includes granularity levels we utilize to decode complex phenomena, like how grasping personal traits (a higher individual level) depends on understanding the cultural context (a lower population level). 
% We categorize such \emph{unobservable \textbf{hierarchies}} within hyper dimensions, forming a \emph{hyper-dimensional} space. This space accommodates relationships present in our knowledge but inaccessible to AI's automatic learning from data.
% The problem of AI-generated ``unrealistic hands'' exemplifies this issue (see Section \ref{sec:obs_hier}).

These unobservable hierarchies often signify different granularity levels within the population. Achieving model \emph{\textbf{generalizability}} across these levels is a common concern, dependent on the model's ability to reuse learned lower-level relationships for higher-level learnings \cite{scholkopf2021toward}. We argue that a shift from \emph{Observation-Oriented} to \emph{Relation-Oriented} is essential to realize this goal, in light of the \emph{relation-centric} nature of human intelligence.
In human understanding, relations function as \emph{\textbf{indices}} that point to our mental representations \cite{sep-mental-representation}, crafting interconnected knowledge systems in memory, inclusive of their hierarchical structures.
In line with this perspective, our proposed \emph{relation-defined representation} learning is conceived as an attempt to ``simulate'' the process of human knowledge construction.

%Notably, the leading-edge methodology to teach AI causal reasoning prioritizes the extraction of causal relation-defined representations \cite{scholkopf2021toward}. %providing valuable insight into the pursuit of AI alignment. 

%However, Prevalent VAE (Variational AutoEncoder)-based feature disentanglements fall short in addressing such relation-defined hierarchies. In 

%Therefore, we regard this method as vital for progressing toward AI alignment.

% Our methodology's feasibility rests on autoencoders, enabling a transition from the \emph{observational-temporal} space to the latent feature space, which allows us to represent \emph{dynamical} features in the same manner as \emph{observational} ones.

% In essence, AI requires our guidance to materialize the unobservable hierarchy of knowledge, ensuring it aligns with our instinctive comprehension. Currently, AI's major achievements are within applications where neglecting this hierarchy does not lead to significant errors, such as face simulation without understanding anatomy, compared to the challenge of simulating hands.
% Furthermore, within a level of \emph{causal} knowledge, relationships are likely to involve temporal dimensions. AI's prevalent large-scale-observational modeling strategy, however, complicates the adoption of causal inference theories and leads to uninterpretable outputs.

\begin{center}
   {\vskip 8pt\large\bf Chapter I: Deficiency of Current Observation-Oriented Paradigm} 
\end{center}

This chapter begins by examining the impact of unobservable hierarchies on models in Section \ref{sec:Hierarchy}, to highlight how these hierarchies can result in significant information loss on the temporal dimension, and its challenges for conventional causal inference. In Section \ref{sec:Causality}, we offer a comprehensive critique of the prevailing \emph{Observation-Oriented} causal learning paradigm. Finally, Section \ref{sec:Temporal} delves into the temporal space untouched by the current paradigm, spotlighting its multi-dimensional nature that leads to inherent modeling issues.

%explores the complexity of temporal dimensionality and our successes in capturing dynamical features. The hierarchical relation-defined representations are formally factorized in Section \ref{sec:factorization}, and Section \ref{sec:method} outlines the proposed implementation methodology, followed by Section \ref{sec:experiment} to detail the subsequent validation experiments. Finally, Section \ref{sec:conclusion} concludes the significance and target of this study.

\vspace{-2mm}
\section{Impact of Unobservable Hierarchies}
\vspace{-3mm}
\label{sec:Hierarchy}


% % Figure environment removed




% Typically, machine learning emphasizes observables (i.e., variables) as the principal objects and strives to build robust models around them. This observable-oriented strategy naturally extends to contemporary AI systems. In the area of computer vision, the training images function as diverse observations, with their constituent pixels playing the role of variables. AI systems can learn the associative relationships over these pixels to establish controllable features, like facial expressions, lighting conditions, or object orientations.

% However, certain forms of knowledge humans find intuitive may not be fully captured by observations alone, making AI appear ``unintelligent'' in comprehension.
% An intriguing example lies in a recent trend on social media: AI-created personas with appealing looks draw in millions of followers, while a common trick to discern whether an image of a person is AI-generated or real is to look for the presence of hands.
% Compared to generating diverse faces, it remains challenging for AI models to simulate human hands due to their more flexible exhibitions.
% AI often treats hands as arbitrary assortments of finger-like elements, similar to their processing of less restricted objects such as flowers.
% So, what makes this type of knowledge so elusive for AI systems? And how does human intelligence approach it differently compared to AI?

Unobservable hierarchies in knowledge suggest unknown distributions in the hyper-dimensional space, which are related to but distinct from the modeling objective.
For solely observational learning tasks, such unknowns may lead to troubles, but still have the potential to be uncovered through methods like reinforcement learning.  However, when it comes to observational-temporal causal learning, the \emph{Observation-Oriented} paradigm inherently falls short in capturing dynamical temporal features across all hierarchical levels. This section will illustrate these phenomena via two examples: one from computer vision and another from health informatics. For the latter, we will further dissect the issue from a traditional causal inference perspective.



\subsection{Observational Hierarchy}
\label{sec:obs_hier}
\vspace{-2mm}

% Figure environment removed

Figure \ref{fig:hand}(a) showcases AI-created hands with faithful color but unrealistic shapes, while humans can easily recognize a plausible hand from simple grayscale sketches in (b). Indeed, we can rapidly decompose our observations hierarchically according to different relations in our knowledge, and process sequentially from lower to higher levels: $\mathbf{I}$ identifies fingers through knuckles, nails, and relative lengths; $\mathbf{II}$ denotes hand gestures through positions; $\mathbf{III}$ retrieves the gesture's meaning from memory.
However, such an intuitive hierarchy exists in our cognitions only.
To AI, or similarly, to an extraterrestrial without our knowledge, the hands in Figure \ref{fig:hand}(a) may seem as reasonable as the actual hands. 

Such observational hierarchy may not always create major problems. If features at different levels do not significantly overlap, AI may successfully ``distinguish'' them. 
For instance, AI can generate convincing faces because the appearance of eyes is strongly indicative of the facial angle, eliminating the need for AI to recognize ``eyes'' from ``faces''. 
But various hand gestures may have similar appearances, leading to chaos.

% similar finger appearances may correspond to various hand gestures, causing AI to misinterpret ``hands'' as random associations of fingers.

Even with problems, AI may learn the hidden knowledge via reinforcement learning \cite{sutton2018reinforcement}, under the guidance of human feedback. For example, human approval of five-fingered hands could lead AI to start identifying fingers autonomously. It works because of \emph{completely} captured observational features at each level, while may not function when involving distributions across temporal dimensions.

% As humans, this hierarchical organization allows us to reuse knowledge across various scenarios efficiently.
% For example, we can recognize fingers (Level $\mathbf{I}$ knowledge) from photographs, watercolor paintings, and diverse sketching styles.
% This repeated identification strengthens both the connections within each level of knowledge and the distinctions between different levels. In essence, our knowledge hierarchy is constructed through relationships rather than mere observations.
% As the leading technology for feature disentanglement, VAEs can effectively separate observational distributions \cite{burgess2018understanding}, but have limitations in handling knowledge hierarchy without indexing pre-determined relationships.
% % While VAEs can effectively disentangle features of individual fingers, as shown on the left side of Figure~\ref{fig:hand_relate}, if the disentanglement is trained on data like watercolor paintings, they may struggle to recognize pencil sketches on the right side.


% %The relationship at each level can naturally form graphical models, where the connected features serve as nodes and readily interact with others in our cognition.

% Refer to Figure~\ref{fig:hand_relate}, where the left side shows well-disentangled finger features, with the right side presenting pencil sketches to recognize.
% If VAEs are trained using data like watercolor paintings for the disentanglement on the left, they may find it challenging to adapt to the sketches on the right. Although traditional graphical models could establish the inter-relationship between the two sides, such as the indicator function $f$ depicted in Figure~\ref{fig:hand_relate}, they often necessitate fully observed variables on both sides, which may lead to extensive manual labeling, a solution not always feasible.
% Conversely, the proposed \emph{Relation-Oriented} modeling is to extract relation-defined representations directly from the right-side observations through $f$ as indexing from the left. In this methodology, the function $f$ is not pre-specified; instead, it serves as a filtering mechanism, to exclude color-related aspects from the left side, and keep only relevant information for the right side.



\subsection{Observational-Temporal Hierarchy}
\label{sec:temp_hier}
\vspace{-1.5mm}


% However, spontaneous adjustments on the temporal dimension remain challenging. Before probing its intricacies, let's examine hierarchical dynamics along a single timeline through the lens of traditional causal learning.



% However, when such hierarchies occur on dynamical features in the time dimension, the crux of tackling lies in the interpretable relationships. 
% Let's first investigate this challenge from the perspective of traditional causal learning.

% The prevalent AI systems primarily sidestep the genuine exploration of the temporal dimension. Conversely, traditional causal inference has made decades of effort to gain access to the distributions on the timeline, employing statistical methods based on observational variables. Nonetheless, these methods encounter significant difficulties when confronted with nonlinearity, thereby creating substantial challenges in capturing dynamical features on the temporal dimension - especially when involving unobservable hierarchies.


% Causality, often linked with time-ordered sequences, refers to the phenomenon that a (previous) change in one variable leads to a (subsequent) change in another \cite{pearl2018book, spirtes2000causation}. 
% The logical concept of ``causing'' is typically highlighted to distinguish it from simple correlation in human understanding, but holds no distinct significance for the actual modeling process - Causal modeling often manifests as a correlation with its direction specified according to time.
% Consequently, the chronological ordering and potential time lag between cause and effect events - the two changes - are commonly regarded as signals that emphasize this relationship as causal, apart from mere correlation.
% Interestingly, if we can adequately represent causality as directional correlations traversing time, then the necessity to distinguish between the two concepts may become less critical than it currently is.

% Undoubtedly, the temporal attributes inherent to causality significantly increase its learning complexity when compared with correlation.
% Yet, what essential differences do they introduce to the modeling process? 
% In this subsection, we introduce an innovative perspective by considering \textbf{\emph{time as a standard dimension}} and discuss how the inherent temporal features can shape and influence the process of causal learning, viewed through the lens of feature stratification on this dimension.

% % the trend (long-term direction), the seasonal (systematic, calendar-related movements), and the irregular (unsystematic, short-term fluctuations).  Trend, Seasonality, Cyclical, and Irregularity


% such a sequence from observational data is just a single instance of, rather than $X$'s temporal feature itself, which needs to be learned from a bunch of instances.
% But the timestamp anchor has determined the modeling cannot capture variations 
%The observed sequence may be jointly determined by multiple or even multi-level temporal features.



Figure~\ref{fig:eff}(a) depicts patients' daily effects on $B$ following $do(A)$, with $t$ indicating the elapsed days. 
%This classic causal relationship sees $do(A)$ as the cause, and the time sequence on the $t$-axis reflects the ensuing effect. 
For simplicity, let's assume the patient's (unobserved) personal characteristics linearly influence $M_A$'s release, i.e., uniformly accelerate or decelerate its effective progress.
The individualized causal effects (i.e., the red and blue curves in (a)) are shaped by two levels of \emph{dynamical temporal} features: 1) the population-level effect sequence with a standard length of 30, and 2) the individual-level progress speed. An accurate estimation of the level 1) dynamical feature provides the desired clinical effectiveness evaluation of $M_A$.

% The patients have various speeds for the same medical effect due to their different body features, which are unobserved.
% Without loss of generality, in this case, we suppose the personalized body feature linearly influences the medical efficiency, i.e., uniformly speeds up or slows down the entire progression for each patient with a unique personal rate. Then, 


% Figure environment removed


Figure~\ref{fig:eff}(b) represents patients' effects in a 31-length feature vector, disentangled by two hierarchical levels. Traditional medical effect estimation is often obtained by averaging the patients' after-30-day performances. This essentially builds a correlation model $B_{t+30}=f(do(A_t))$, which only captures a \emph{static temporal} feature $B_{t+30}$, the last step of the level 1) dynamic, disregarding the preceding 29 steps. 
Moreover, even if the estimation method employs a sequence of length 30 (e.g., Granger causality), it can capture the level 1) dynamic at most and is exclusive of further levels.
Causal effects with multiple levels of dynamics are prevalent in various causal learning applications, such as epidemic progression, economic fluctuations, strategic decision-making, etc. The \emph{Observation-Oriented} paradigm necessitates identifying objects before establishing relations, making it often difficult to comprehensively encompass all levels of dynamics.

% Take flood prediction: general physical rules formulate the base-level regulations, applicable universally, while unique hydrological conditions create distinct watershed features. This paradigm applies to diverse phenomena like epidemic progression, economic fluctuations, or strategic decision-making, wherein knowledge extends from broad frameworks to specific idiosyncrasies.

%Although it is known that this process typically spans around 30 days for the overall patient population, the specific timeframe does not alter the individual patient's assessment of the medication's effectiveness. A 10-day deviation, earlier or later, should not impact its clinical efficacy evaluation.

% The general effect can be seen as a standard sequence of length 30, augmented by an additional feature dimension that quantifies the patient's unique rate of progress - a numeric attribute of this variable. 
% This rate could correlate with age or other factors, eliminating the need to consider temporal patterns further, as this length 31 variable does not depend on the timestamp $t$ to anchor the sequence. Put simply, by viewing time - the fourth dimension separate from the three spatial ones - as a regular feature dimension, we can greatly simplify our understanding.
% Although this simplification is intuitively noticeable to researchers, the challenge lies in the fact that temporal features are often difficult to extract from observational data.

% In essence, within the context of Figure~\ref{fig:eff}, the extraction of temporal features is fundamentally the same as executing \emph{\textbf{feature stratification}} on the observable \emph{\textbf{B}}. Here, the rate of progress serves as the individual-level feature, while the standard sequence of change is characterized as the population-level general feature.


% The correlation function $f$ evolves over time and thus may reflect temporal features of effects. However, as a static matching mechanism, it can only represent \textbf{\emph{a single level}} of the temporal feature at best. In this scenario, $f$ is derived from observed $A_t$ to $B_{t+30}$, thus capturing only the final step (i.e., denoted as D30) of the standard time sequence, with the preceding 29 steps missing, as highlighted in Figure~\ref{fig:eff} (b).
% This process only captures a fragment of the population-level features, while completely disregarding the individual-level ones.




\vspace{-2mm}
\subsection{Strange Hidden-Confounder in Causal Inference}
\label{sec:confounder}
\vspace{-2mm}


% Figure environment removed
\vspace{-2mm}


For patients $P_i$ and $P_j$, the estimated last-day effect $B_{t+30}$ is biased, as $P_i$ exceeds 100\% full effect, while $P_j$ only achieves about 75\%. To account for such individual-level biases, causal inference usually introduces a \emph{hidden confounder} into DAG (Directed Acyclic Graph), to represent the unobserved personalized characteristics, depicted as the node $E$ in Figure~\ref{fig:hidden} (a), a strangely involved outer variable. It implies an illogical assertion: ``Our model is biased due to some unknown aspects we have no intention to know.''


It is because, while $E$ is unknown, its effect, the individual-level dynamical feature, is observable, but excluded by the solely observational model $f$. Although hidden, $E$ is observational and thus could be incorporated by $f$ if revealed. Thus, introducing a hidden confounder transforms \emph{observed dynamical} variables into \emph{unobserved observational} ones, which enhances human understanding but unnecessarily benefits the model.

As depicted in Figure~\ref{fig:hidden}(b), traditional causal inference views the individual-level effect as caused by the unobserved composite cause $do(A) * E$, not a directly modelable relationship. Conversely, a \emph{Relation-Oriented} approach just treats relation as an index, to extract representations of \emph{observational-temporal} effects from sequential data, so we can employ any observed identifier, e.g., patient ID. Figure~\ref{fig:hidden}(c) illustrates its implementation architecture, to realize a relation-defined hierarchical disentanglement.









% Traditional causal models regard effects as outcomes in the observational data space $\mathbb{R}^d$, neglecting their temporal attributes on the $(d+1)$-th dimension: time. 
% While \textbf{\emph{cause events}} can have their temporal features captured by sequential models such as RNNs, it remains a crucial fact that \textbf{\emph{effects}} are not inherently categorized as \textbf{\emph{events}} on their own.





% is unobserved, its causal effect has been delivered as the individual-level temporal feature - the personalized speed - which is observed.
% This feature is present in data but overlooked by the model, thereby introducing such unavoidable biases.
% % While some sequential models like RNNs can capture the temporal features of the \textbf{\emph{cause event}}, the fact remains that  \textbf{\emph{effects are not considered events}} in themselves.% within conventional causal learning.


% The existence of $E$ is deduced from a previously identified causal relation, thereby positioning this interpretation as \emph{Relation-Oriented}.
% However, it primarily serves to enhance human comprehension and does not directly benefit the modeling with an \emph{Observable-Oriented} nature.
% Autoencoders provide a solution to this limitation by representing observables as features in latent space, as demonstrated in Figure~\ref{fig:hidden} (c).
% Through the incorporation of extra temporal features, the objective of a causal model can transition from effect outcomes (i.e., the observable $B\in \mathbb{R}^d$) to effect events (i.e., observable time sequences in $\mathbb{R}^{d+1}$ space).
% Additionally, unlike the observational values in data space, features in latent space can be directly disentangled or stratified using the input relations as indexes.
% As demonstrated in Figure~\ref{fig:hidden} (b), this feature stratification process sees $A$ act as the observed population-level cause, with the associated $(A\cdot E)$ symbolizing the unobserved individual-level cause. 
% Thus, by constructing a \emph{Relation-Oriented} model, we can identify the general medical effect across the entire population, without integrating the patients' individualized features.

% \noindent\tcbox[enhanced, drop fuzzy shadow southwest, sharp corners, colframe=yellow!80!black, colback=yellow!10]{\begin{minipage}{0.93\textwidth}
% \paragraph{Lemma 2.} A \emph{Relation-Oriented} approach can stratify or disentangle the temporal features of a causal effect by leveraging autoencoders.
% \end{minipage}}

% In this work, we propose a novel conceptualization of the causal DAG, designed to incorporate potential temporal features inherent in causal effects, as depicted in Figure~\ref{fig:do1} (a).
% This revised DAG is set within a space defined by one or more timeline axes, where the nodes continue to represent observational effect outcomes or their corresponding causal events; However, the directed edges are assigned meaningful lengths that represent the potential time spans of the \emph{effect events}, as required. 
% For example, in Figure~\ref{fig:do1} (a), different edge lengths are used to distinguish the individual-level differences in the efficiency of the same medical effect among patients.
% This reinterpretation serves not merely as a means to visualize the stratification of temporal features, but also provides a foundation for explaining why current AI applications within the sphere of causal knowledge might be leading to misconceptions.







\section{Causality on Temporal Dimension}
\label{sec:Causality}
\vspace{-2mm}

Causality research acts as a gateway into the temporal dimension, going beyond the observational space. 
However, the current causal learning models, formulated as $y_{t+m}=f(x_t)$ %or $y_{t+m}=f(\{x_t\})$ 
for causality $X\rightarrow Y$ with $m$ as a predetermined time progression, do not fully integrate $t$ as a computational dimension. 
%(where the time sequence $\{x_t\}=\{x_1, \ldots, x_{t}, x_{t+1}, \ldots, x_T\}$ has a predefined length $T$).
%Especially, for its effect on $Y$, only the snapshot $y_{t+m}$ taken after a given $m$ timesteps is considered.

Under the prevailing \emph{Observation-Oriented} paradigm, the objects - cause on $X$ and effect on $Y$ - must be pre-identified prior to formulating the relation function $f$. 
While it remains feasible to assign a sequence of $X$ to encompass dynamics for the cause, identifying the exact start and end timestamps for the effect becomes problematic. 
Consequently, traditional causal inference typically treats effects as solely \emph{observational}, with \emph{static} temporal aspects determined by predefined $m$.
When the underlying effects have \emph{dynamical} significance, selecting an appropriate value for $m$ to capture the most relevant snapshot becomes challenging. This \emph{identifiability difficulty} is further magnified when multiple levels of dynamics are present in effects.

Indeed, integrating the concept of \emph{temporal distribution} could greatly streamline causal inference theories, making associated ideas more intuitive. For instance, when we acquire \emph{Counterfactuals} \cite{pearl2009causal}, we are essentially capturing temporal distributions in response to conditional queries. Also, as demonstrated in the prior section, fully capturing the observed dynamics across all hierarchical levels within the model could potentially eliminate the need for hidden confounders.

Next, we begin by redefining the notion of causal models concerning the temporal dimension in Section \ref{sec:causality3_1}, then delve into existing methodologies in Section \ref{sec:causality3_2}, focusing on their capacity to capture temporal distributions, with a particular exploration of the essence of do-calculus.
Section \ref{sec:causality3_3} discusses inherent limitations of the dominant \emph{Observation-Oriented} causal model paradigm. 

%\vspace{-1mm}
\subsection{Redefined Causality Modeling}
\label{sec:causality3_1}
\vspace{-1mm}

% Curiously, one might find it rare to see ``incorporation of time'' defined as the distinctive factor between causality and mere correlation in causal inference theories. While, as in our modeling context, instead of a logical one, what significance does this distinction hold?

% Time series frequently involve a timestamp attribute, logically reflecting the absolute time evolution in reality. 
% For the model, however, this way renders the line between causality and correlation, as it operates within the modeling space we defined, irrespective of the dimensions' temporal significance.
% While, it does not diminish the importance of the temporal dimension, but rather emphasizes that our current causal learning might not completely align with our intuitive understanding of this dimension, revealing a certain discrepancy.
% This section is devoted to discussing the present causal modeling from this particular perspective.

Traditional causal inference heavily emphasizes interpreting models, such as discerning the causal directions, to distinguish them from mere correlations. In essence, the temporal-evolving aspects that set causality apart from correlation are mainly evident in interpretations, rather than directly within the modeling framework.

From a modeling perspective, once the domain is defined, the learning process does not consider the temporal significance behind the dimensions, including the timestamp attribute. Thus, it is understandable that the traditional paradigm leans heavily on interpretation.
With this in mind, we differentiate causality from correlation in the modeling context by integrating distributions along the temporal dimension.


%\hspace{-1mm}
%\vspace{2mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=red!50!black, colback=yellow!10]{\begin{minipage}{0.93\textwidth}
\paragraph{Theorem 1.} Causality vs. Correlation in the modeling context.

$\ \bullet$ Causality is the relationship between \emph{observational-temporal} features, which can be \textbf{\emph{dynamical}}.

$\ \bullet$ Correlation is the relationship between features \textbf{\emph{not dynamical}}.

\end{minipage}}
%\vspace{1mm}

A causality $X\rightarrow Y$ can be divided into two parts: 1) the informative relation connecting $X$ and $Y$, crucial for modeling, and 2) the causal direction, i.e., the roles of cause and effect, mainly significant in interpretation.
Specifically, for model selection, we can employ $Y=f(X;\theta)$ to predict the effect on $Y$, and, conversely, utilize $X=g(Y;\psi)$ to deduce the cause $X$ given $Y$. Both parameters, $\theta$ and $\psi$, are derived from the joint probability $\mathbf{P}(X, Y)$ without imposing modeling restrictions.

In practice, the causal direction is often predetermined for models.
One reason is the importance of aligning with our intuitive understanding of temporal progression. Moreover, the prevailing causal model paradigm displays an \emph{imbalanced} capacity for capturing dynamical features between the cause $X$ and the effect $Y$. For example, in Figure~\ref{fig:hidden}, inverse modeling of $do(A)=f(\{B_t\})$ through RNNs, given a sufficiently long sequence $\{B_t\}=\{B_{t+1},\ldots, B_{t+40}\}$, might fully capture dynamics of $B$ and negate the need for a hidden confounder. 

Within the suggested \emph{Relation-Oriented} approach, we can utilize relations to accurately identify the effect's observational-temporal features and fully extract their representations. As a result, the modeling function $f$ is relieved from encapsulating temporal facets. The differentiation between causality and correlation becomes a matter of connected features, rather than the nature of the relational model.


%Importantly, a relationship with causal significance in a \emph{logical} context does not automatically translate to ``causality'' within a \emph{modeling} context. For this reason, the ``incorporation of time'' alone is not a sufficient condition to validate causal models. 
%often associates the temporal lag between cause and effect as an indicator of causality.



\vspace{-1mm}
\subsection{Learning Temporal Distributions}
\label{sec:causality3_2}
%\vspace{-2mm}
% For observational variable instance $x\in \mathbb{R}^d$, we frequently encode its temporal evolution as a sequence along the timeline, such as $\{x_t\}=\{x_1, \ldots, x_{t}, x_{t+1}, \ldots, x_T\}$, without considering $t$ as the $(d+1)$-th dimension. Accordingly, we seldom view $x$'s changing value over time as a ``distribution'' along the $t$-axis. 

% In prediction $x_{t+m}=f(x_t)$ with integer $m>0$, the potential nonlinearity of function $f$ is only acknowledged within $\mathbb{R}^d$, but leaving the relationship between $x_t$ and $x_{t+m}$ to be \textbf{\emph{linear}}. It  implies that the \textbf{\emph{correlation}} model $x_{t+m}=f(x_t)$ can only capture \textbf{\emph{static}} temporal elements in sequence $\{x_t\}$. If, however, no suitable $(f,m)$ can be found to ensure this model adequately represents $\{x_t\}$, there may exist unrepresented \textbf{\emph{dynamical}} temporal features, which necessitates a \textbf{\emph{causality}} model of the form $\{x_\tau\} = f(\{x_t\})$. Here, cause and effect are as two distinct sequences on logical timelines, $\tau$ and $t$, respectively, such that, the relationship between any pair of $x_t$ and $x_\tau$ could be \textbf{\emph{nonlinear}}.

% Consider this form of causality model: $y_\tau = f(\{x_t\})$ with $\tau = (t+T)+m$, representing a sequence $\{x_t\}$ causing a static outcome $y_\tau$, where timeline $\tau$ is confined as $m$-timestamp later than timeline $t$.
% %($=$ a sequence $\{y_\tau\}$ of length one), $m$ timesteps later on $t$. 

Numerous methods are dedicated to capturing the dynamical features of the \emph{\textbf{cause}} alone, such as autoregressive models \cite{hyvarinen2010estimation} and RNNs \cite{xu2020multivariate}, both employing the modeling formate $y_{t+m}=f(\{x_t\})$ with $\{x_t\}=\{x_1, \ldots, x_{t}, x_{t+1}, \ldots, x_T\}$.
Meanwhile, Granger causality \cite{maziarz2015review}, a method widely recognized in economics, employs a sequence for the \emph{\textbf{effect}} that exhibits significant temporal patterns, in the formate $\{y_\tau\} = f(\{x_t\})$, where $t$ and $\tau$ signifying two separate timelines. 

Yet, using a sequence does not guarantee capturing dynamics. The distinction between ``a sequence of static variables'' and ``a dynamical variable'' hinges on whether the \emph{nonlinear} mutual relationships among these variables can be captured. 
For autoregressive, if the selected model is linear, then $\{x_t\}$ remains a static sequence.
Conversely, RNNs can harness the nonlinearity of $\{x_t\}$, enabling them to encapsulate dynamics even within multiple levels. 
In Granger causality, the effect sequence $\{y_\tau\}$ must be observationally identified before modeling, making it typically a static sequence. At best, it can capture a single-level dynamic with the right parameter setting, e.g., referring to Figure~\ref{fig:eff}(b), a \emph{30-length} average sequence may capture level 1).
%, which can capture dynamics at a single level at most, akin to the level 1) sequence in Figure~\ref{fig:eff}(b)

A more universal approach to represent temporal distributions is do-calculus \cite{pearl2012calculus, huang2012pearl}.
Instead of specifying time sequences, it takes the \emph{identifiable} temporal events as modeling objects to conduct elementary calculus. 
The $do(\cdot)$ format flexibly modulates temporal features for the \emph{\textbf{cause}}. However, such a \emph{differential}-calculus essence also introduces elevated complexity.
Here, we reinterpret its three core rules from an \emph{integral}-calculus perspective, aiming for a more intuitive comprehension. 

For the time sequence $\{x_t\}=\{x_1, \ldots, x_T\}$, let $do(x_t)=\{x_t, x_{t+1}\}$ indicate the occurrence of an instantaneous event $do(x)$ at time $t$. Time lag $\Delta t$ between $\{t,t+1\}$ is sufficiently small to make this event identifiable, such that $do(x_t)$'s \emph{interventional} effect can be depicted as a function of the resultant distribution at $t+1$. Conversely, the effect provoked by static $x_t$ snapshot is called \emph{observational} effect. Then, the observational-temporal distribution of the cause $X\in \mathbb{R}^d$ can be formulated as below:

\vspace{-6mm}
\begin{align*}
    \text{Given } \mathcal{X} & \rightarrow Y \mid Z, \text{ where } \mathcal{X} = \langle X,t \rangle \in \mathbb{R}^{d+1} \text{ encompass the temporal dimension}, \text{ we have} \\
    \mathcal{X} =& \int_0^T do(x_t) \cdot x_t \ dt \text{\ \ with }
    \begin{cases}
      (do(x_t)=1) \mid do(z_t), & \text{ \emph{Observational} only (Rule 1) } \\
      (x_t=1) \mid do(z_t), & \text{ \emph{Interventional} only (Rule 2) }    \\
      (do(x_t)=0) \mid do(z_t), & \text{ No \emph{interventional}  (Rule 3) }   \\
      \text{otherwise} & \text{ Associated \emph{observational} and \emph{interventional} }
    \end{cases} \\
     \text{The effect } & \text{of } \mathcal{X} \text{ can be derived as }
     f(\mathcal{X}) = \int_0^T f_t \big( do(x_t) \cdot x_t \big) \ dt = \sum_{t=0}^{T-1} (y_{t+1}-y_t) =y_T-y_0
\end{align*}
\vspace{-4mm}
% In \emph{do-calculus}, event $do(\cdot)$ acts as the causal variable, required to be ``identifiable'', indicating that its effect can be derived from the subsequently observed distributions. This aligns with the ``elementary'' requirement in our reformulation context, where $\Delta t$ or $dt$ does not intend to be infinitesimal but should facilitate event recognition.
% However, ``identifiability'' implies a preference for linear interpretability. 

Within the graphical system $\{X,Y,Z\}$, the rules of do-calculus tackle three specific scenarios (notably, a specifiable $do(x_t)\cdot x_t$ pertains to Rule 2), where conditional independence is maintained between the \emph{observational} and \emph{interventional} effects. However, these rules bypass more generalized cases.

Utilizing the $do(\cdot)$ format, we can also represent observational-temporal distributions of $Y$ as $\mathcal{Y}=\langle Y,\tau \rangle$, by incorporating an additional timeline $\tau$. However, in the \emph{Observation-Oriented} paradigm, identifiable events for $\mathcal{Y}$ still necessitate our prior specifications.
In contrast, the proposed \emph{Relation-Oriented} approach can autonomously construct $\mathcal{Y}$ via relation-indexing.

% Despite the flexible form of $do(\cdot)$, the specification of ``effect events'' on the timeline is still challenging for traditional methods. Accordingly, \emph{do-calculus} still handles effects as an observational outcome $Y$ instead of dynamical $do(Y)$. 
% Moreover, any applicable $do(X)$ is restricted to identifiable events, intentionally eliminating the timespan to avoid incorporating further temporal dimensional variances. 

% In essence, conventional causal inference turns the causal dynamics into static causal variables, approximating the relationship through correlation models in the form $do(X)\rightarrow Y$, not $do(X)\rightarrow do(Y)$.
 
% However, they remain to be timestamp-specific sequences of fixed lengths, which is, in essence, manually selecting constants to represent target temporal features more effectively, rather than genuinely incorporating them as variables with potential distributions, analogous to specifying 7 as the outcome of rolling two dice.


%construct the cause event in $do(X)\rightarrow Y$, there is no straightforward way to instantiate the ``effect event'' as $do(X)\rightarrow do(Y)$.
%Consequently, traditional causal inference primarily employs correlation models to closely approximate causal relationships, by treating effects as mere static outcomes.



% Subsequently, if the autoencoder technique can adequately represent any dimensional features, including temporal ones, can we eliminate the need to differentiate between causality and correlation?

%The original intention of modeling centers on the relationship itself, rather than its interpretation.
% In summary, the \textbf{\emph{Relation-Oriented}} perspective can be articulated as follows:
% The distinction between causality and correlation hinges on the objects connected by the relationship, rather than the relationship itself. 
% To use an analogy, a double-ended cotton swab is differentiated by the specific characteristics of its cotton ends - such as shape, texture, or color for different uses - rather than the connecting stick itself.


% Additionally, a prevalent misconception is that the cause event must always occur with a noticeable time lag before the resulting effect event. The upcoming example will help debunk this notion.

% Morgan Spurlock conducts a self-experiment in the documentary film ``Super Size Me'' (2004). Over a month, Spurlock restricted his diet solely to McDonald's meals, and consecutively recorded the significant deteriorations in his health and physical appearance, %including rapid weight gain, mood instability, liver damage, etc.,  
% which dramatically demonstrated the potential negative health impacts of a fast-food diet.
% Consider two  time sequences: $\mathbf{x}=\{x_1, \ldots, x_{30}\}$ and $\mathbf{y}=\{y_1, \ldots, y_{30}\}$. Here, each $x_t$ signifies the act of eating McDonald's on the $t^{th}$ day, while each $y_t$ corresponds to the fluctuations in his health indices for that day.
% It is intuitive to recognize that $\mathbf{x}\rightarrow \mathbf{y}$ is a causal relationship, although no time lag exists between $\mathbf{x}$ and $\mathbf{y}$.
% Yet, a time lag is also conceivable in this case. Suppose another person tried this experiment, but since he had a longer responding digestive system, the same effect denoted by $\textbf{y}$ emerged with a 5-day delay, creating a 5-day lag between $x_1$ and $y_1$.
% This instance emphasizes that separation by a time-lapse is not a reliable criterion, even when both the cause and effect include dynamics.




%\vspace{2mm}
\subsection{Limitation of Current Causal Model Paradigm}
\label{sec:causality3_3}
%\vspace{-2mm}

Our innate understanding of causality aligns with Theorem 1. Yet, confining causal models to the observational space can lead to potential misalignments between these models and our intuitive knowledge. We have categorized causal modeling into four scenarios shown in Figure~\ref{fig:view}. Depending on whether the relationship is already in knowledge, the modeling queries can be divided into causal discovery, which seeks new insights, and causal learning, which leverages knowledge to model causality. Further, these applications can be categorized based on the dynamical significance of the effects. For instance, the causality ``raining $\rightarrow$ wet floor'' includes only static temporal features, which is logically a causality but not distinguishable from correlation once modeled.
We explore these scenarios from two perspectives: the \emph{relation} connecting features, critical for modeling, and the \emph{causal direction}, essential for interpretation.


% Figure environment removed
\vspace{-1mm}


%\vspace{4mm}
\subsubsubsection{\emph{(1) Modeled Relation}}
\vspace{-1mm}

Traditional causal inference has made notable advancements in ``downgrading'' dynamical temporal features to be observationally accessible.
For instance, do-calculus explores independence conditions on the temporal dimension.
For overlooked dynamical features of the effect, if existing knowledge can suggest its potential cause, creating a hidden confounder can enhance comprehension; if not, these dynamics may be dismissed based on the \emph{causal Sufficiency assumption}, potentially leading to subsequent challenges.

On the other hand, causal discovery mainly scans the observational space to explore dependencies. As a result, if the underlying causality does not encompass significant dynamics, causal discovery can be effective. However, if such dynamics exist, they largely go undetected. This potential gap may be negated under the \emph{causal Faithfulness assumption} suggesting that observed variables fully represent the causal reality.

\vspace{-1mm}
\subsubsubsection{\emph{(2) Modeled Causal Direction}}
\vspace{-1mm}
% Knowledge-guided causal learning readily complies with established cause-and-effect roles to set the modeling direction. However, the observationally-discovered directions might not necessarily hold causal meaning.

Consider observed variables $X$ and $Y$ in a graphical system, with specified models $Y=f(X;\theta)$ and $X=g(Y;\psi)$. Based on observations, the discovered causal direction between $X$ and $Y$ is determined by the likelihoods of estimated parameters $\hat{\theta}$ and $\hat{\psi}$. Given the joint distribution $\mathbf{P}(X,Y)$, one would prefer $X\rightarrow Y$ if $\mathcal{L}(\hat{\theta}) > \mathcal{L}(\hat{\psi})$. %, then $X\rightarrow Y$ would be preferred.
Now, let $\mathcal{I}(\theta)$ be a simplified form of $\mathcal{I}_{X,Y}(\theta)$, the Fisher information, representing the amount of information contained by $\mathbf{P}(X,Y)$ about unknown $\theta$. Assume $p(\cdot)$ to be the probability density function; then, in this context, $\int_X p(x;\theta) dx$ remains constant. So, we have

\vspace{-6mm}
\begin{align*}
    \mathcal{I}(\theta) &= \mathbb{E}[(\frac{\partial }{\partial \theta} \log p(X,Y;\theta))^2 \mid \theta] 
    = \int_{Y} \int_X (\frac{\partial }{\partial \theta} \log p(x,y;\theta))^2 p(x,y;\theta) dx dy \\
    &= \alpha \int_Y (\frac{\partial }{\partial \theta} \log p(y;x,\theta))^2 p(y;x,\theta) dy + \beta = \alpha \mathcal{I}_{Y\mid X}(\theta)+\beta, \text{with } \alpha,\beta \text{ constants.} \\
    \text{Thus, } \hat{\theta} &=\argmax\limits_{\theta} \mathbf{P}(Y\mid X,\theta) = \argmin\limits_{\theta} \mathcal{I}_{Y\mid X} (\theta) =\argmin\limits_{\theta} \mathcal{I}(\theta), \text{ and } \mathcal{L}(\hat{\theta})  \propto 1/\mathcal{I}(\hat{\theta}).
\end{align*}
\vspace{-4mm}

Subsequently, the likelihoods of the estimated parameters $\hat{\theta}$ and $\hat{\psi}$ depend on the amount of information, $\mathcal{I}(\hat{\theta})$ and $\mathcal{I}(\hat{\psi})$.
That means the learned directionality between $X$ and $Y$ essentially indicates how much their specified distributions are reflected in the data, with the more dominant one deemed the ``cause''. It presumes that the cause is more comprehensively captured in the observations than the effect by default. Due to restricted data collection techniques, such a presumption was justifiable in past decades. But in the present era, assuming such discovered directions to have logical causal meaning is no longer appropriate.

%In summary, a causal direction purely inferred from observations could be causally meaningful in logic.

%, if satisfying: \textbf{1)} the causal relation of interest does not involve significant dynamics, \textbf{2)} the observations are known, a prior, to be more informative about the cause than the effect, and \textbf{3)} the specified distributions are fundamentally accurate.
% Technically speaking, in the traditional modeling context, the term ``causal model'' is not a specific model type. Instead, it designates models that require additional logical interpretations about overlooked dynamics, to bridge comprehension gaps and pave the way for potential improvements.


% %\subsection{A Short Summary}
% To summarize Section \ref{sec:Causality}, the modeling methodology is not inherently related to whether or how we interpret the question as causal. Technically, at the present stage, we primarily employ the ``directed-correlation learning'' and ``informativeness discovery'' to address causal queries.




% \noindent\tcbox[enhanced, drop fuzzy shadow southwest, sharp corners, colframe=yellow!80!black, colback=yellow!10]{
% \begin{minipage}{0.93\textwidth}
% \paragraph{Remark in 3.4} In essence, effective causal learning requires: \textbf{1)} a complete representation of both observational and dynamical features for 
% cause and effect, and \textbf{2)} the capability of incorporating further-level information, to save the intended interpretation (like causal directionality).
% \end{minipage}}
% %\vspace{1mm}

% Specifically, both requirements can be met through \emph{Relation-Oriented Featurization} (refer to Section \ref{sec:factorization}), which is to factorize cause and effect as disentangled features, according to the hierarchically structured prior knowledge.
% This approach uses relations as indices to stratify levels of features across various dimensions; and the disentangled cause or effect representations can simplify their potential feature expansion (to incorporate information about their causal roles), compared to traditional observable-based models. 





\section{The Overlooked Temporal Space}
\label{sec:Temporal}

Data is commonly stored in matrices, with time series data incorporating an extra attribute for the timestamps, which forms a logical timeline to reflect the absolute time evolution in reality. Traditionally, modeling has relied on this timeline to determine the chronological order of all potential events. However, our intuitive understanding of time is far more complex than this singular, simplified absolute timeline.

% Observable-based modeling inherently aligns with our intuitive understanding.
% The tradition of modeling natural phenomena as observable mechanisms responsible for time evolution has consistently been the gold standard.
% Specifically, the absolute timeline serves as a ruler, and the timestamps $\{t\}$ on it act as \emph{anchors} to pinpoint the observational snapshots to form the objective modeling data.

Consider an analogy where ants dwell on a two-dimensional plane of a floor. If these ants were to construct models, they might use the nearest tree as a reference to specify the elevation in their two-dimensional models. %, bypassing the recognition of the third dimension.
By modeling, they observe an increased disruption at the tree's mid-level, which indicates a higher chance of encountering children. However, since they fail to comprehend humans as three-dimensional beings, instead of interpreting this phenomenon in a new dimension ``height'', they solely relate it to the tree's mid-level.
If they migrate to a different tree with a varying height, where mid-level no longer presents a risk, they might conclude that human behavior is too complex to model effectively.
Similarly, when modeling time series, we usually discount the dimension ``time'' as the single absolute timeline, which has become our ``tree''.

Our understanding allows for the simultaneous existence of multiple logical timelines. If one is designated as the absolute timeline, the remaining ones can be viewed as relative timelines, each representing distinctive temporal events, which can be interconnected via specific relationships.
In such \emph{Relation-Oriented} perspective, like, during a causal inference analysis, the temporal dimension contains numerous possible logical timelines that we could choose to construct any necessary scenarios.
However, once we enter a modeling context, like, using AI to model the time series along a single timeline, the temporal significance no longer exists, but only 
a regular dimension containing timestamp values, indistinguishable from other observational attributes.
Metaphorically, if we consider the observational space for AI modeling as Schrdinger's box and our interest is the ``cat'' within, our task is to accurately construct the box, giving adequate consideration to all potential logical timelines,
to ensure the ``cat'' remains \emph{reasonable} upon unveiling.

%\vspace{0.5mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=red!50!black, colback=yellow!10]{\begin{minipage}{0.93\textwidth}
\paragraph{Theorem 2.} The term \emph{Temporal Dimension} encompasses all potential logical timelines, not just a singular one. Consequently, a \emph{Temporal Space} is defined as the space built by chosen timeline axes.
\end{minipage}}
%\vspace{-0.5mm}

Fundamentally, as three-dimensional beings, we are limited from truly understanding temporal dimensionality. As the term ``space'' typically evokes a three-dimensional conception, the notion of ``temporal space'' might seem odd for a four-dimensional creature. 
Like ants can use trees as references without the need to fully comprehend the third dimension, we rely on logical timelines to interpret the fourth. At this juncture, our mission is to recognize the potential ``forest'' beyond the present single ``tree''. 


% Metaphorically, the observational modeling space is akin to Schrdinger's box, and our interest lies in modeling the ``cat'' within. Before opening it, we can envision any plausible attributes for this ``cat''. However, at times, our method of unveiling the box may not match our envisioned scenarios, leading to an unreasonable ``cat'', which is unseen to us but visible to AI. Consequently, while we retain our initial vision, the outcomes delivered by AI can appear unexpected.

This section will demonstrate how the single-timeline-based timestamp specification operation, rooted in the \emph{Observation-Oriented} paradigm, inherently biases modeling and hinders model generalizability.
% initially dissect the mechanism of inherent temporal bias within the current AI systems, due to neglecting the multi-dimensional nature of the temporal space, 
Then we will summarize advancements and challenges on our journey towards realizing causal knowledge-aligned AI.


%This complexity is particularly heightened when multiple timelines exist within the neglected knowledge hierarchy, introducing more substantial inherent temporal biases (refer to Section \ref{sec:CRB}).




\subsection{Inherent Temporal Bias Scheme}
\label{sec:CRB}
\vspace{-2mm}

% The overlooked ``effect events'' can cause more substantial problems, beyond just inadequately representing a single relation. 
% Specifically, in a complex knowledge system using \emph{Structural Causal Model} (SCM), when multiple events occur on separate (relative) timelines, complications arise.
% The effect in $do(X)\rightarrow Y$ can be a cause in another relation like $Y\rightarrow Z$. 
% If the effects along two pathways $do(X)\rightarrow Z$ and $do(X)\rightarrow Y\rightarrow Z$ progress on two different timelines, the SCM $Z=f(Y, do(X))$ may unintentionally become \textbf{\emph{invalid}}.

Modeling event identification typically relies on timestamps derived from a singular timeline in time series data. In structural causal models (SCMs), this can induce \emph{inherent temporal biases}, limiting our capacity to leverage AI's potential in the temporal dimension. This issue becomes more acute in large-scale causal relationships, where more logical timelines may be hidden.

To better ascribe this issue, we improve the causal DAG (directed acyclic graph) \cite{pearl2009causal} as follows: \textbf{1)} incorporating (potentially multiple) logical timelines as axes into the DAG space, and \textbf{2)} defining edges along timeline axes to be vectors with meaningful lengths indicating the timespans of causal effects.
For example, the single-timeline scenario in Figure~\ref{fig:eff} has the new DAG depicted in Figure~\ref{fig:do1}(b), with (a) showing the traditional one as a comparison. 
The edge $do(A)\rightarrow B$ in Figure~\ref{fig:do1}(a) represents the population-level effect only, thus necessities a hidden confounder to explain the individual-level diversities, while in Figure~\ref{fig:do1}(b), they can be explicitly represented by varying lengths of $\overrightarrow{do(A)\  B}$.

Consider an expanded two-timeline scenario in Figure~\ref{fig:do3}(a), where $A$ shorthandly represents $do(A)$. Apart from its primary effect on $B$, $A$ also indirectly influences $B$ through its side effect on another vital sign, $C$, depicted as edges $\overrightarrow{AC}$ and $\overrightarrow{CB}$. For simplicity, assume the timespan for $\overrightarrow{AC}$ is 10 days for all patients, with the individual-level diversity confined to timeline $T_X$ alone.
In conventional single-timeline causal modeling, the SCM function would be $B_{t+30}=f(A_t, C_{t+10})$.
Let's assume $f(A_t, C_{t+10})$ is implemented using RNNs, which can accurately depict the individual-level final effects of $A$ on $B$ for any patient.

The confounding relationship over nodes $\{A, B, C\}$ forms a triangle across timelines $T_X$ and $T_Y$ - such shape geometrically holds for any hierarchical level relationship.
For patients $P_i$ and $P_j$, the \emph{individualization} process is to ``stretch'' this triangle along $T_X$ by different ratios, which is a homographic \emph{linear transformation} in this space.
%The conventional SCM model $B_{t+30}=f(A_t, C_{t+10})$, however, may not realize such a transformation but lead to invalid outputs. 
%For example, let's still consider $f$ as the correlation model that accurately represents the expected population-level effect on $B$, under the causal inference context.
%Assume the causal effect function $f(A_t, C_{t+10})$ can accurately represent any individualized final effects of $A$ on $B$ (no longer a mere correlation). 
However, as illustrated in Figure~\ref{fig:do3} (b) and (c), for either $P_i$ or $P_j$, equating the outcome of $f$ to be $B_{t+30}$ violates the \emph{causal Markov condition} necessary for reasonable SCMs.

% Figure environment removed
\vspace{-3mm}

% Figure environment removed
\vspace{-1mm}

Notably, in this specific case, the violation may not cause significant issues for AI models like RNNs.
%involving the second timeline may not cause further complications compared to Figure~\ref{fig:do1}(b). 
Given the \emph{independence} of dynamics on $T_X$ and $T_Y$, the SCM can be formulated as $B_{t+30}= f_1(A_t)+f_2(C_{t+10})$, suggesting that the cross-timeline confounding can be broken down into two single-timeline issues. %, where capturing hierarchical dynamics might challenge statistical models but not neural networks.
%While in a further special case, if the confounding does not exist, like eliminating the indirect influence $\overrightarrow{CB}$, the SCM predicting $B$ can be formed as $B_{t+30}=\alpha f(A_t)$.
However, making assumptions such as independence or non-confounding is unrealistic. Since each cause-and-effect pair might possess its unique logical timeline, these inherent temporal biases can accumulate exponentially, significantly impacting the robustness of causal models, irrespective of our model selections.

\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=red!50!black, colback=yellow!10]{\begin{minipage}{0.96\textwidth}
\paragraph{Theorem 3.}The \emph{\textbf{inherent} temporal bias} may occur in SCM if it contains: 
\textbf{1)} \emph{Confounding} dynamical temporal features across \emph{Multiple} logical timelines, and \textbf{2)} Unobservable hierarchy.
\end{minipage}}
\vspace{1mm}

It is interesting to notice that most of the successful causal applications instinctively avoid one of the two factors: \emph{confounding} or \emph{multi-timeline}. 
Statistical causal models can facilitate de-confounding as a pre-processing, e.g., the backdoor adjustment \cite{pearl2009causal}.
For AI models, most of the achievements do not potentially involve relative timelines, e.g., the large language model (LLM) in a semantic space, where the phrases are ordered consistently along a single logical timeline.

Unlike AI's black-box nature, causal inference essentially takes a \emph{Relation-Oriented} viewpoint. Nevertheless, in its context, the inherent temporal biases are difficult to recognize, as they often intermingle with the modeling biases resulting from the statistically unsolvable nonlinearity. They have similar manifestations, and both can be addressed by de-confounding.
Consider Figure~\ref{fig:do1}(a), the linearly modeled population-level effect mismatches with individuals $P_i$ and $P_j$, which may not be distinguishable from the mismatching that occurs in Figure~\ref{fig:do3}(b)(c), caused by dynamics across multi-timelines.

% ``confounding biases''.

% In traditional applications, the statistical diversities across levels of subgroups are often summarized as \emph{data heterogeneity} problem \cite{grissom2000heterogeneity}.

% Nevertheless, as we strive towards developing true AI capable of integrating causal knowledge, such inherent biases pose a significant challenge. Therefore, their identification under general circumstances has become a critical task in our current efforts.

% Consider the large language model (LLM), which recently gained a widespread reputation through ChatGPT. The foundation is reinforcement learning based on Markov decision processes (MDPs). 
% In semantic modeling, confounding and heterogeneity are ubiquitous, while fortunately, the multi-timeline problem typically not exists. In short, the phrases' orders represent consistent grammatical meanings.
% The other comparable scenarios include AI in Go gaming and the style-confusing transformation of images.

%Causal inference essentially aims to address the multi-timeline complexity, but traditional statistical models, limited to observational spaces, struggle with direct modeling the temporal distributions. Instead, they often rely on specific conditions to create temporal independence (like in \emph{do-calculus}), and accordingly require manual operations, such as 


% Lastly, AI can undoubtedly do a great job if some applications only need to model one object without concerns about overfitting or generalizability among different objects.




% are often applicable to specific types of problems, we should not limit our aspirations to selective applications. Ultimately, we must confront and address broader questions that embody hybrid complexities, in our pursuit of aligning human knowledge with AI capabilities. 

\subsection{Inherent Impact on SCM Generalizability}

Traditional SCMs usually require specifying timestamps for objective events before modeling relations. While this affects model robustness, the more pressing concern is that neglecting multiple timelines can render established SCMs non-generalizable across different scenarios with analogous core relationships.



% Figure environment removed


Consider the practical scenario depicted in Figure~\ref{fig:3d}. Here, $\Delta t$ and $\Delta \tau$ represent actual time spans. Yet, the crux is not on determining their exact values, but on realizing their intended causal relationship: As each unit of Statin's effect is delivered on LDL via $\overrightarrow{SA'}$, it immediately impacts T2D through $\overrightarrow{A'B'}$. Simultaneously, the next unit effect begins generation. This dual action runs concurrently until $S$ is fully administered.  At $B'$, the ultimate aim of this process is to evaluate the total cumulative influence stemming from $S$.

Given the relationship $\overrightarrow{SB'} = \overrightarrow{SA'}+\overrightarrow{A'B'}$, specifying the $\overrightarrow{SB'}$ time span (i.e., half of the $\overrightarrow{AB'}$ time span) inherently sets the $\Delta t:\Delta \tau$ ratio, defining the ${ASB'}$ triangle's shape in this DAG space. While the estimated mean effect at $B'$ might be precise for the present population, the preset $\Delta t:\Delta \tau$ ratio's universality is questionable, potentially constraining the established SCM's generalizability.

% Contrarily, the proposed relation-defined representation can incorporate its temporal features, focusing on the role of $B'$ as dictated by $\overrightarrow{SB'}$. 
% This approach obviates the need to define time spans for $\overrightarrow{SA'}$ and $\overrightarrow{A'B'}$, thereby relieving us from concerning the $\Delta t:\Delta \tau$ ratio. Within the 3D observational-temporal space, as depicted in Figure~\ref{fig:3d}, we can consider any desired model individualizations for patients or generalizations for other populations as linear transformations of the green-colored subgraph.


\subsection{Toward Causal Knowledge-Aligned AI}
%\vspace{-1.5mm}

Our quest for causal reasoning AI involves broadening our modeling techniques from solely observational to include temporal dimensions, as summarized in Figure~\ref{fig:model}.
The present challenge lies in enabling structural causal models in the joint observational-temporal feature space. Recognizing underlying logical timelines is critical to avoid inherent biases and enhance model generalizability. However, since manual identification is unrealistic, it may have been time for us to consider the new paradigm.


The initial models under i.i.d. assumption only approximate observational associations, proved unreliable for causal reasoning \cite{pearl2000models, peters2017elements}. Correspondingly, the common cause principle highlights the significance of the nontrivial conditional properties, to distinguish structural relationships from statistical dependencies \cite{dawid1979conditional, geiger1993logical}, providing a basis for effectively uncovering the underlying structures in graphical models \cite{peters2014causal}.

Graphical causal models relying on conditional dependencies to construct Bayesian networks (BNs) often operate in observational space and neglect temporal aspects, reducing their causal relevance \cite{scheines1997introduction}. Causally significant models, such as Structural Equation Models (SEMs) and Functional Causal Models (FCMs) \cite{glymour2019review, elwert2013graphical}, can address counterfactual queries \cite{scholkopf2021toward}, with respect to temporal distributions by leveraging prior knowledge, to construct causal DAGs accordingly.
%In practical scenarios, causal learning and discovery often blend, as a system can exhibit a mix of known and unknown, causal and non-causal characteristics.

State-of-the-art deep learning applications on causality, which encode the DAG structural constraint into continuous optimization functions \cite{zheng2018dags, zheng2020learning, lachapelle2019gradient}, undoubtedly enable highly efficient solutions, especially for large-scale problems. However, larger question scales indicate more underlying logical timelines, which may lead to snowballing temporal biases. It can be evident from the limited successful applications of incorporating DAG structure into network architectures \cite{luo2020causal, ma2018using}, e.g., neural architecture search (NAS).

\vspace{-3mm}
% Figure environment removed
%\vspace{-1mm}

Schlkopf \cite{scholkopf2021toward} summarized three key challenges impeding causal AI applications to achieving generalizable success: 1) limited model robustness, 2) insufficient model reusability, and 3) inability to handle data heterogeneity (caused by unobservable hierarchies in knowledge). Notably, all these challenges can be attributed to the timestamp specification required by \emph{Observation-Oriented} structural models.

On the other side, physical models, which explicitly integrate temporal dimensions in computation, and are able to establish abstract concepts through relations, may provide insights into these challenges. 
We believe that the \emph{Relation-Oriented} approach can help bridge the gap between observational and temporal spaces.
%\pagebreak

%designed along these lines, aiming to transcend the observational limitations innate to our prevailing modeling paradigms.
%Figure~\ref{fig:model} provides a succinct summary of these discussed model types. %, arranged from the most knowledge-driven at the top to the most data-driven at the bottom.

%while we leave the temporal dimensional causal discovery as a prospect for future exploration.
%Physical knowledge manifests humans' ability to understand timelines as dimensions and reflects our innate relation-first mentality. The Relation-Oriented approach paves the way for machine learning to shatter its dimensional limitations.



% The notable successes in big data applications emphasize that given sufficient data, AI can vastly outperform human capabilities within observational spaces, even under the strong i.i.d. assumption.
% However, it stumbles on learning ``shifting distributions'', which are trivial for humans to understand \cite{scholkopf2021toward}. 

% In essence, human comprehension can extend beyond the observable reality into hyper-dimensional space, including but not limited to the temporal dimension, i.e., the timeline. For us, ``shifting'' is simply a change along this additional dimension.
% Contrarily, the scope of AI is inherently confined to the input data space, a boundary set by us. Under the prevailing Observable-Oriented modeling paradigm, AI's capacity is restricted to inferring observational distributions
% only, such as the remarkable LLMs (large language models) learning associations within the semantic space.

%in causal learning. First, in domain sciences, integrating \emph{observable} multi-variable and multi-timestep data into correlations can enhance models \cite{guyon2008practical, zhao2013gut, marwala2015causality}, but still far from fully revealing the causal scheme. %Conversely, in AI applications, while 


% Neural networks capably consolidate large observational contexts into a global model, yet more challenging to integrate existing causal knowledge compared to traditional models \cite{luo2020causal}.





% In previous work \cite{li2020teaching}, the existence of CRB in RNN models based on clinical data has been experimentally verified. The main objective of this paper is to analyze the formation mechanism of CRB and summarize the critical factors involved, using the redefined causal DAG (i.e., \emph{do-DAG}) to aid in the visualization of CRB.
% Furthermore, this paper proposes a new \emph{Causal Representation Learning} framework for causal AI, which functions as a generic solution.
% This paper also contributes significantly by implementing the higher-dimensional feature learning autoencoder, which is a crucial technology for achieving CRL.


% Deep Learning (DL) is capable of handling non-linear and higher-order dependencies among variables, while also providing global optimization solutions. Due to its effectiveness, DL is becoming increasingly popular in this area \cite{luo2020causal}.
% One notable contribution is the conversion of the discrete combinatorial constraints on DAG's acyclicity into continuous ones, enabling global optimization on network structures \cite{zheng2018dags, lachapelle2019gradient}. This approach has been further extended to nonparametric modeling \cite{zheng2020learning}.
% Furthermore, to leverage the interpretability from causal inference, some works construct the known causal DAG by properly designing architecture \cite{ma2018using}, while others attempt to infer dependencies directly from neuron weights \cite{lachapelle2019gradient, zheng2020learning}. 
% Such neural architecture search (NAS) methodologies aim to realize accuracy as well as the transparency of neural networks simultaneously but have not reached a general success \cite{luo2020causal}. 

% However, unlike in traditional methodologies, where intentional adjustments can be made to account for the individual-level feature biases, DL's black-box nature can cause such biases to go unnoticed.
% Additionally, due to its high effectiveness, DL is often applied to larger-scale questions with numerous variables, which can further exacerbate the accumulation of unnoticed biases leading to significant global distortions.

% The hope that DL can automatically solve CRBs has been shown to be unrealistic in previous work \cite{li2020teaching}. Instead, DL should be used to model something that we have a prior interpretation of, rather than trying to interpret what DL has learned. Therefore, DL is likely to be applied more often for learning disentangled causal relationships, which may lead to wider adoption of higher-dimensional feature representations in the future for this purpose.



% In a Relation-Oriented context, dynamical features on temporal dimensions can be identifiable and serve as augmented additional features for causes and effects in the observational spaces. This expands the dimensionality of the modeling space by incorporating potential timelines as additional axes. Then, the ability of a model to handle \emph{counterfactual} inquiries corresponds to its capacity to encompass the temporal distributions on queried timeline axes.



%The ``structural'' and ``graphical'' models often blend in practice, with DAG illustrating the structure. 
%, inferring latent structures from observed dependencies, 
%using either constraint-based (like fast causal inference) or score-based (like greedy equivalence search) approaches 





\begin{center}
   {\vskip 8pt\large\bf Chapter II: Realization of Proposed Relation-Oriented Paradigm} 
\end{center}

This chapter begins by formulating the factorizations to achieve hierarchical disentanglement in the latent space. Then, we explore the proposed \emph{relation-defined representation} methodology as an embodiment of the \emph{Relation-Oriented} paradigm. Lastly, we validate its efficacy through comprehensive experiments.

%\vspace{-2mm}

\section{Hierarchical Disentanglement in Latent Space}
\label{sec:factorization}

Given an observational variable $X\in \mathbb{R}^d$, we denote its time sequence of length $T$ as $\{x_t\}=\{x_1, \ldots, x_{t-1}, $ $ x_{t}, x_{t+1}, \ldots, x_T\}$. Our goal is to construct a latent feature space $\mathbb{R}^L$ for two specific purposes:
1) Fully represent the observational-temporal features of $\mathcal{X}=\langle X, t \rangle \in \mathbb{R}^{d+1}$.
2) Hierarchically disentangle $\mathcal{X}$'s representation according to relations in knowledge.
Consequently, the established system realizes the reusability of models at any hierarchical level by indexing through the corresponding relations.

For $\mathcal{Y} = \langle Y, \tau \rangle \in \mathbb{R}^{b+1}$, if the relationship $\mathcal{X}\rightarrow \mathcal{Y}$ identifies certain features of $\mathcal{Y}$'s distribution, 
the proposed \emph{relation-defined representation} learning aims to extract the representation $\mathcal{\hat{Y}}$ as determined by the relation with $\mathcal{X}$. Moreover, the resulting $\mathcal{\hat{Y}}$ should be reusable in developing subsequent levels of $\mathcal{Y}$'s representations, thereby facilitating the generalizability of the relationship model for $\mathcal{X}\rightarrow \mathcal{Y}$.
For instance, in a graphical system $\{\mathcal{X}, \mathcal{Y}, \mathcal{Z}\}$ with relationship $\mathcal{X}\rightarrow \mathcal{Y} \leftarrow \mathcal{Z}$, $\mathcal{Y}$ can be viewed as in a two-level hierarchy. The first level is defined by $\mathcal{X}\rightarrow \mathcal{Y}$ and the second by $\langle\mathcal{X}, \mathcal{Z}\rangle \rightarrow \mathcal{Y}$, where the second level enhances the first by incorporating an additional data stream from $\mathcal{Z}$.


% Next, we will sequentially factorize three transformation processes between the data and latent space $\mathbb{R}^L$: First, observational features from $\mathbb{R}^{d}$ to $\mathbb{R}^L$; Second, observation-temporal features from $\mathbb{R}^{d+1}$ to $\mathbb{R}^L$; Last, the relationship $\mathcal{X}\rightarrow \mathcal{Y}$, from the joint space $\mathbb{R}^{d+1} \circ  \mathbb{R}^{b+1}$ to $\mathbb{R}^L$.

% Depending on the fields of application or study, $\{x_t\}$ may have different explanations, like \emph{time series}, \emph{trajectories}, etc \cite{feng2016survey, anderson1998partitioning,aminikhanghahi2017survey}. For instance, in a spatiotemporal context, the sequential objects may present to be geographical coordinates coupled with timestamps, like $\{<x_1,t_1>, <x_2,t_2>, \ldots, <x_T, t_T>\}$. 
% But, no matter what, one truth remains: $t$ is not considered one of the dimensions constructing the modeling space - It can be seen from the Picard-Lindelof formula, which characterizes our current temporal modeling paradigm:
% If a Lipschitz function $f(X) = dX/dt$ exists in some region of time $t$, then at least locally, the immediate future value of $X$ is predictable:
% \begin{equation}
%     X(t+dt) = X(t) + dt \cdot f(X(t)), \text{ where } f(X) = dX/dt.
% \end{equation}
% Since $f$ has been constrained to the derivative of $X\in \mathbb{R}^d$, the modeling freedom is bound within $d$ dimensions, excluding the temporal one defined by $t$.
% In this section, we will demonstrate the formulated factorization in the latent space $\mathbb{R}^L$ to represent $\langle X, t \rangle \in \mathbb{R}^{d+1}$ in a hierarchical way given knowledge-alined levels - which can also be called \emph{hierarchical disentanglement}.

% This formula characterizes our conventional temporal modeling paradigm, yet interestingly, it does not reflect our intuitive grasp of timelines as an inherent component of the dimensional framework. For instance, in daily life, we can naturally perceive events as objects, predicting future outcomes and discerning prerequisites. Moreover, we can instinctively contemplate ``What if...'' scenarios, using timelines to derive conditional temporal distributions and respond to counterfactual inquiries.


%Intriguingly, human intelligence naturally understands temporal elements without reliance on timestamp anchors, and can systematically construct knowledge by considering timelines as part of the dimensional framework - a prime example is the formulation of Special Relativity in the field of physics.
%Moreover, despite the complex theory about time itself, in our daily lives, we intuitively comprehend timeline as a standard dimension: We can naturally understand the occurrence of events as an entity, and accordingly envision its potential subsequences (as effects or results) and antecedents (as causes or prerequisites). In essence, considering ``What if...'' (as counterfactual) scenarios is a distinctive human ability, setting us apart from other non-intelligent animals.



\subsection{Factorize Observational-Temporal Hierarchy}
\vspace{-1.5mm}
\label{sec:multi-timelines}

Let $X=(X_1,\ldots, X_d) \in \mathbb{R}^d$, and assume $\mathcal{X}=\langle X, t \rangle \in \mathbb{R}^{d+1}$ has an $n$-level hierarchy. Define $\Theta_i$ as the $i$-th level component of $\mathcal{X}$ in the \emph{observable data space}, and its counterpart in the \emph{latent feature space} $\mathbb{R}^L$ as $\theta_i$. 
The representation function $f_i$ facilitates the transformation from $\mathbb{R}^{d+1}$ to $\mathbb{R}^{L_i}$ for the $i$-th level, considering all prior lower-level features as attributes.
$\theta_i$ is a vector in $\mathbb{R}^{L}$, with its significant value residing in a subset of the $L$ dimensions, denoted as $\mathbb{R}^{L_i}$, forming the disentanglement $\{\mathbb{R}^{L_1}, \ldots, \mathbb{R}^{L_i},\ldots,\mathbb{R}^{L_n}\}$.
Then, we obtain:
\begin{equation}
    \mathcal{X} = \sum_{i=1}^{n} \Theta_i, 
     \text{ where } \Theta_i = f_i \bigl(\theta_i ;\ \Theta_1,\ldots, \Theta_{i-1}\bigr) \text{ with } \Theta_i \in \mathbb{R}^{d+1} \text{ and } \theta_i \in \mathbb{R}^{L_i} \subseteq \mathbb{R}^{L}
\end{equation}
To illustrate an observational hierarchy, refer to Figure~\ref{fig:hand} (b). Let $\theta_1\in \mathbb{R}^{L_1}$, $\theta_2\in \mathbb{R}^{L_2}$, and $\theta_3\in \mathbb{R}^{L_3}$ represent the three levels of features, with each subspace being mutually exclusive. That is, $L=L_1+L_2+L_3$. The combined vector $\langle\theta_1, \theta_2, \theta_3\rangle \in \mathbb{R}^{L}$ represent the whole image. 
In correspondence, $\Theta_1$, $\Theta_2$, and $\Theta_3$ are full-scale images, each presenting unique content. For instance, $\Theta_1$ highlights the details of the fingers, whereas $\Theta_1+\Theta_2$ expands to showcase the entire hand.

In the context of an observational-temporal hierarchy, the component $\Theta_i \in \mathbb{R}^{d+1}$ can be expressed as the original time sequence $\{\Theta_{t}\}_i = \{\Theta_{t_i} \in \mathbb{R}^{d} \mid t_i=1,\ldots,T\}$.
Consequently, we obtain a set of relative logical timelines $\{t_1,\ldots,t_i,\ldots,t_n\}$ which, in contrast to the absolute timeline $t$, are each uniquely determined by the relationship at their respective levels.
However, in the \emph{observable data space}, the $i$-th level observational-temporal feature, represented as the sum $\Theta_1 +\ldots +\Theta_i$, still maintains its timestamp attribute along $t$.

% Consider the scenario in Figure~\ref{fig:do3}, with the data matrix comprising four columns: values of $A$, $B$, $C$, and timestamp $t$. We can represent $B$ as a 2-level hierarchy: the first level is defined by its direct effect from $A$, $A\rightarrow \hat{B}$, while the second level involves an additional stream denoting its indirect effect via $C$, $A\rightarrow C\rightarrow \tilde{B}$. 
% The comprehensive effect representation $\hat{B}+\tilde{B}$ presents as two data columns, $B$ and $t$, aligning with its original form. Thus, unlike traditional SCMs, this approach eliminates the need for specifying $B_{30}$.



% $t$-axis is a relative timeline for illustrating medication $A$'s effect on $B$. The data matrix $\mathcal{X}$ has three columns: $A$-value, $B$-value, and $t$-timestamp. Now, consider  Figure~\ref{fig:do3}(a), where an additional effect on $C$ is involved. This scenario contains two relative timelines, $T_X$, and $T_Y$. However, the data matrix $\mathcal{X}$ can still have one column for the $t$-timestamp, incorporating one more column for the observed $C$-value. In short,



%However, irrespective of the complexity of the underlying relationships, they often show up in observations as a multivariate time sequence tied to a single absolute timeline.


\subsection{Factorize Hierarchy of Relationship}
\vspace{-1.5mm}

% Consider $\mathcal{F}(\mathbf{\theta}) = \mathcal{F}(\theta_1,\ldots,\theta_n) = \{ f_i(\theta_i) \mid i=1,\ldots, n\}$  as the collective representation function within this framework, where $\theta=(\theta_1,\ldots,\theta_n)$ denotes the tuple of aggregating all features for $X$. Then, the factorization can be concisely expressed as $X=\mathcal{F}(\mathbf{\theta})$. Practically, $\mathcal{F}$ can be implemented as one or a series of autoencoders, symmetrically for encoding and decoding with the invertibility naturally desired.
 
Given a set of $n$-level hierarchical representation functions for $\mathcal{X}$, denoted by $\mathcal{F}(\vartheta) = \bigl\{ f_i \bigl(\theta_i \bigr) \mid i=1,\ldots, n\bigr\}$, our goal is to define $n$ relationship functions, collectively termed $\mathcal{G}$, such that $\mathcal{Y}=\mathcal{G}(\mathcal{X})$ exhibits an $n$-level hierarchy.
Each $i$-th level relationship function is $ g_i(\mathcal{X}; \varphi_i)$, where $\varphi_i$ is its parameter. Then, we have:
\begin{equation}
    \mathcal{G}(\mathcal{X}) = \sum_{i=1}^{n} g_i(\mathcal{X}; \varphi_i) = \sum_{i=1}^{n} g_i(\Theta_{i}; \varphi_i)  =
    \sum_{i=1}^{n} g_i \bigl(\theta_i;\ \Theta_1,\ldots, \Theta_{i-1}, \varphi_i\bigr) = \mathcal{Y}
\end{equation}
The $i$-th level relation-defined representation for $\mathcal{Y}$ is $g_i (\theta_i;\varphi_i)$ considering the features of the preceding $(i-1)$ levels of $\mathcal{X}$. This relationship can be portrayed as the augmented feature vector $\langle \theta_i, \varphi_i \rangle$ in latent space $\mathbb{R}^L$.
Using $\vartheta_X$ and $\vartheta_Y$ to distinguish the collective hierarchical representations for $\mathcal{X}$ and $\mathcal{Y}$ respectively, the overall relationship from $\mathcal{X}$ to $\mathcal{Y}$ becomes $\vartheta_Y=\langle \vartheta_X, \varphi \rangle$, where $\varphi=\{\varphi_1,\ldots,\varphi_n\}$. %and $\langle \vartheta_X, \varphi \rangle$ 
The term $\langle \vartheta_X, \varphi \rangle$ represents the pairwise augmentations between collections $\vartheta_X$ and $\varphi$.
%The latent model can be interpreted as a causal model, as proposed in \cite{bengio2012unsupervised}



% But, in reality, the overall context is typically far more complex, involving intricate causal relationships among various medications, numerous vital signs, and targets for addressing multifaceted comorbidities.

% The graph shows two distinct timelines, labeled as $\mathcal{T}_Y$ and $\mathcal{T}_Z$, respectively. $\mathcal{T}_Y$ chronicles consecutive calendar dates, for monitoring patients' disease progression through measurements of LDL, blood pressure (BP), and estimated T2D risk. $\mathcal{T}_Z$ represents the direct impact of Statin on LDL, with its use (illustrated as node $S$) determined by physicians based on observed LDL levels.
% An additional axis assists in organizing these observed variables, thus facilitating a comprehensive 3D visualization.

% Despite the Statin, if more medications (like those for BP or blood glucose regulation) are incorporated, more timelines will be needed for their individual temporal influences, which potentially result in higher-dimensional pictures beyond our visualization capacity. Intriguingly, regardless of the inherent complexity, observed data can consistently be structured as a multivariate time sequence anchored within a single \emph{absolute} timeline - in this case, the timestamp dates in chronological order.
% The inclusion of more medications would only increase $d$, with the $(d+1)$-th dimension remaining as the timeline.
% put, the \textbf{\emph{multi-timeline}} relationship from knowledge is unobservable, and thus cannot be fully revealed by data-driven methodologies that solely rely on \textbf{\emph{single-timeline}} observations. 

% Figure~\ref{fig:3d} also depicts the observed relations in the single-timeline view, highlighted in green.
% Typically, the data-driven predictive function forms $B'=f(A, S, C)$, which implicitly sets the timescales for $\mathcal{T}_Y$ and $\mathcal{T}_Z$. For instance, suppose it has been known that reduced LDL requires 30 days (from $t$ to $t+1$) to fully impact the T2D risk. This information intuitively leads to the assumption that a similar 30-day period (from $\tau$ to $\tau+2$) would be sufficient for the $A\rightarrow A'$ effect when modeling $f$.
% In Section \ref{sec:CRB}, we will demonstrate how directly modeling the superficially observed relations can inevitably introduce biases.





\section{Relation-Defined Representation Methodology}
\label{sec:method}

% Figure environment removed


%Essentially, the proposed relation-defined representation is a technique to realize the observational-temporal objects in our cognition as features, allowing AI systems to understand and build models. 
%The inherent temporal biases underscore the observational constraints of traditional methods, while concurrently recognizing their robust ties to our existing knowledge infrastructures.
%our inherent understanding of observational-temporal relationships into a format that AI systems can readily understand, thereby facilitating the building of models.
%The inherent temporal biases do not invalidate traditional methods or diminish our knowledge, but rather, they highlight the challenge of using observational-only techniques to answer questions that inherently involve temporal dimensions.
%The proposed \emph{Relation-Oriented} methodology allows us to leverage AI's strength in modeling temporal distributions, while traditional methods maintain much stronger links with existing knowledge systems.


While the existing \emph{Observation-Oriented} modeling paradigm has its limitations, it still forms the basis of many existing knowledge infrastructures.
As showcased in Figure~\ref{fig:new}, relation-defined representations enable AI to create generalizable models in a latent space rich with human-indecipherable features, and concurrently, AI's potential can be utilized to refine observations, thereby fortifying traditional models through counterfactual effects, de-confounded simulations, and more.

This section presents a specialized autoencoder architecture essential for implementing relation-defined representation; then, outlines the methodology for disentangling hierarchical levels of representations in constructing graphical models. Finally, we introduce a causal discovery algorithm for the latent feature space.

\subsection{Invertible Autoencoder for Higher-Dimensional Representation}
\label{sec:method_1}
\vspace{-1mm}

Autoencoders are generally used for dimensionality reduction, often aligning all observational variables as data attributes for this purpose in structural modeling \cite{wang2016auto}. However, our objective diverges. We aim to model individual relationships to disentangle variables' representations and simultaneously ``stack'' them to form a DAG within the latent space, $\mathbb{R}^L$. This space must be large enough to accommodate potential relationships in the form of $\vartheta_Y= \langle \vartheta_X, \varphi \rangle$. This poses a substantial technical challenge, as we need to achieve higher-dimensional representation extraction for individual variables.



%\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.94\textwidth}
\paragraph{Corollary 1.} For a given graph $G$ and a data matrix $\mathbf{X}$, which is column-augmented by all observational attributes and timestamps of variables in $G$, the dimensionality $L$ of the latent space must be at least as large as $rank(\mathbf{X})$ to adequately represent $G$.
\end{minipage}}
%\vspace{1mm}

Corollary 1 stems from the notion that the autoencoder-learned $\mathbb{R}^L$ is spanned by $\mathbf{X}$'s top principal components, often referred to in Principal Component Analysis (PCA) \cite{baldi1989neural, plaut2018principal, wang2016auto}. Hypothetically, reducing $L$ below $rank(\mathbf{X})$ may yield a less adequate but causally more significant latent space through better alignment \cite{jain2021mechanism} (further exploration is needed).
%This approach has been widely used in natural language processing (NLP) for identifying word analogies \cite{pennington2014glove, rong2014word2vec}, and in bioinformatics for embedding gene expression \cite{belyaeva2021causal, lotfollahi2019scgen}. Further investigation is required to uncover the underlying connections between these applications.
In this study, we will set aside discussions on the boundaries of dimensionality. Our experiments feature 10 variables with dimensions 1 to 5 (Table~\ref{tab:tower}), and we empirically fine-tune and reduce $L$ from 64 to 16.



% Figure environment removed

Figure~\ref{fig:arch} depicts the proposed autoencoder architecture, featured by the symmetrical \emph{Encrypt} and \emph{Decrypt} layers. Encrypt amplifies the input vector $\overrightarrow{x}$ by extracting its higher-order associative features; conversely, Decrypt symmetrically reduces dimensionality and restores $\overrightarrow{x}$ to its original form. To ensure reconstruction accuracy, the \emph{\textbf{invertibility}} of these operations is naturally required.

Figure~\ref{fig:arch} illustrates a \emph{double-wise} associative feature expansion, where each pair of \emph{two} digits from $\overrightarrow{x}$ are encoded to form a new digit, by associating with a randomized constant \emph{Key}, which is created by the encoder and mirrored by the decoder. A double-wise expansion on $\overrightarrow{x}\in \mathbb{R}^d$ generates a $(d-1)(d-1)$ length vector. By using multiple \emph{Keys} and augmenting the derived vectors, $\overrightarrow{x}$ can have a significantly extended dimensionality. 

The four differently patterned blue squares represent the vectors expanded by four distinct \emph{Keys}, with the grid patterns indicating their ``signatures''. Each square visualizes a $(d-1)(d-1)$ length vector (not signifying a 2-dimensional vector).
In a similar way, higher-order extensions, such as \emph{triple-wise} ones across every three digits, can also be employed by appropriately adapting \emph{Keys}.

Figure~\ref{fig:extractor} depicts the encryption and decryption processes used to expand a digit pair $(x_i, x_j)$, where $i\neq j \in 1,\ldots,d$.
The encryption function $f_\theta(x_i,x_j) = x_j \otimes exp(s(x_i)) + t(x_i)$ is defined by two specific elementary functions, $s(\cdot)$ and $t(\cdot)$. The parameter $\theta$, serving as a \emph{Key}, consists of their respective weights, $\theta=(w_s, w_t)$.

Specifically, the encryption of $(x_i,x_j)$ transforms $x_j$ into a new digit $y_j$ using $x_i$ as a selected attribute.
The decryption process symmetrically employs the inverse function $f_\theta^{-1}$, defined as $(y_j-t(y_i)) \otimes exp(-s(y_i))$. 
Notably, this approach sidesteps the need to calculate $s^{-1}$ or $t^{-1}$, allowing $s(\cdot)$ and $t(\cdot)$ to be flexibly specified as needed for nonlinear transformations. This design is inspired by the pioneering work of \cite{dinh2016density} on invertible neural network layers that utilize bijective functions. 

By collectively representing all $f_\theta$ functions as $\mathcal{F}(X; \vartheta)$, where $\vartheta$ encompasses all parameters, the Encrypt and Decrypt layers can be denoted as $Y=\mathcal{F}(X; \vartheta)$ and $X = \mathcal{F}^{-1}(Y; \vartheta)$, respectively.
The source code for both Encrypt and Decrypt is provided \footnote{https://github.com/kflijia/bijective\_crossing\_functions/blob/main/code\_bicross\_extracter.py}, along with a comprehensive experimental demo.


% Figure environment removed


\subsection{Stacking Hierarchical Representations to form SCM}
\label{sec:method_3}
\vspace{-1mm}

Consider a causal system comprising three variables $\{\mathcal{X}, \mathcal{Y}, \mathcal{Z}\}$, each with corresponding representations $\{\mathcal{H}, \mathcal{V}, \mathcal{K}\} \in \mathbb{R}^L$ initially extracted by three separate autoencoders. Figure \ref{fig:bridge} illustrates the process of linking $\mathcal{H}$ and $\mathcal{V}$ to model the relationship $\mathcal{X}\rightarrow \mathcal{Y}$. Additionally, Figure~\ref{fig:stack} depicts how two modeled relationships related to $\mathcal{Y}$ are stacked to form a hierarchically disentangled representation.

% be an instance of variable $\mathcal{X}$, represented as feature $h\in \mathbb{R}^L$ in the latent space. Their joint distribution $P(x,h)$ can be decomposed as $P(x|h)P(h)$, with $P(h)$ as the \emph{prior} and $P(x|h)$ being the \emph{likelihood}.


Consider instances $x$ and $y$  for the relationship $\mathcal{X}\rightarrow \mathcal{Y}$, which are represented as $h$ and $v$ in $\mathbb{R}^L$. To estimate the latent dependency $\mathbf{P}(v| h)$, we use an RNN, as shown in Figure \ref{fig:bridge}, to explicitly include the temporal features of $h$. For the time being, we allow $\mathcal{V}$ to autonomously capture any potential dynamics, with the expectation of future refinements.
Each iteration of the learning process involves three optimizations:
\begin{enumerate}[topsep=0pt, partopsep=0pt]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{1pt}
    \item {Optimizing encoder $\mathbf{P}(h|x)$, RNN model $\mathbf{P}(v|h)$, and decoder $\mathbf{P}(y|v)$ to reconstruct $x\rightarrow y$ relation.}
    \item {Fine-tuning encoder $\mathbf{P}(v|y)$ and decoder $\mathbf{P}(y|v)$ to accurately represent $y$.} 
    \item {Fine-tuning encoder $\mathbf{P}(h|x)$ and decoder $\mathbf{P}(x|h)$ to accurately represent $x$.}
\end{enumerate}
%\vspace{1mm}
Throughout the learning, $h$ and $v$ values are iteratively refined to minimize their distance in $\mathbb{R}^{L}$, and RNN acts as a bridge to traverse this distance, thereby informatively modeling the relation $x\rightarrow y$.


% Initially, the latent variables $\mathcal{H}$ and $\mathcal{V}$ are obtained as two independent feature vectors in the latent space $\mathcal{R}^L$ using the proposed autoencoder architecture. During the learning of $f$, these latent variables are optimized by gradually adjusting their feature values to minimize their distance and come closer to each other. The function $f$ acts as a bridge that connects the two variables by modeling their causal relationship.
% To be specific, optimization of $\mathcal{H}$ is achieved by adjusting the feature values of $\mathcal{H}$ in the latent space $\mathcal{R}^L$ to minimize the distance between the reconstructed $X$ and the original $X$. This optimization step is one of the three in each iteration of the two-autoencoder architecture depicted in Figure \ref{fig:bridge}.

% A complete iteration includes three optimization steps:


% The process of "stacking" two representations for a single variable is adjusting its feature vector values until it simultaneously represents two different roles. In Figure \ref{fig:bridge}, the learning process of $f$ can be seen as two stacking operations: one stacks $X$ with $\overrightarrow{XY}$ by changing the value of initialized $\mathcal{H}$, and the other stacks $Y$ with $\overrightarrow{XY}$ by changing the value of initialized $\mathcal{V}$. The initialization process of $\mathcal{H}$ and $\mathcal{V}$ is to establish them as the representations of $X$ and $Y$, respectively.



Figure~\ref{fig:stack} presents two stacking scenarios for $\mathcal{Y}$ within the $\{\mathcal{X}, \mathcal{Y}, \mathcal{Z}\}$ system, according to different causal directions.
% where two edges' representations are stacked among three variables.
% From the perspective of the autoencoder, all data streams flow through to describe distributions, and no need to identify any do-operations like in RNN modeling. Hence only the single-value notations are used here without denoting any sequences.
Given the established $\mathcal{X}\rightarrow \mathcal{Y}$ relationship in $\mathbb{R}^{L}$, the left-side architecture completes $\mathcal{X}\rightarrow \mathcal{Y}\leftarrow \mathcal{Z}$ structure, while the right-side caters to $\mathcal{X}\rightarrow \mathcal{Y}\rightarrow \mathcal{Z}$. By stacking an additional representation layer, hierarchical disentanglement is formed, allowing for various input-output combinations (denoted as $\mapsto$) based on practical needs.  %rolled back by removing relevant layer(s) from the autoencoder.
%Multiple representation layers in an autoencoder provide a high degree of flexibility for realizing various input and output distributions. 
For instance, in the left-side setup, $\mathbf{P}(v|h) \mapsto \mathbf{P}(\alpha)$ signifies the $\mathcal{X}\rightarrow \mathcal{Y}$ relationship, while $\mathbf{P}(\alpha|k)$ suggests $\mathcal{Z}\rightarrow \mathcal{Y}$. On the right side, $\mathbf{P}(v) \mapsto P(\beta|k)$ indicates the $\mathcal{Y}\rightarrow \mathcal{Z}$ relationship with $\mathcal{Y}$ as input; conversely, $\mathbf{P}(v|h) \mapsto P(\beta|k)$ signifies the causal chain $\mathcal{X} \rightarrow \mathcal{Y}\rightarrow \mathcal{Z}$.

Causal relationships of known edges can be sequentially stacked using existing causal DAGs in domain knowledge.
Additionally, this approach aids in discovering causal structures within the latent space by identifying potential relationships among the initial variable representations.


%\vspace{-2.7mm}
% Figure environment removed



\subsection{Causal Discovery in Latent Space}
\label{sec:method_4}
\vspace{-2mm}

%Suppose we have established a set of feature vectors in the latent space $\mathcal{R}^{L}$ to represent the corresponding data variables. 
Algorithm 1 outlines the heuristic procedure for identifying edges among the initial variable representations. We use Kullback-Leibler Divergence (KLD) as a metric to evaluate the strength of causal relationships. Specifically, as depicted in Figure~\ref{fig:bridge}, KLD evaluates the similarity between the RNN output $\mathbf{P}(v|h)$ and the prior $\mathbf{P}(v)$. Lower KLD values indicate stronger causal relationships due to closer alignment with the ground truth. Although Mean Squared Error (MSE) is a common evaluation metric, its susceptibility to data variances \cite{reisach2021beware, kaiser2021unsuitability} led us to prefer KLD, using MSE as a secondary measure. In the graphical representation context, we refer to variables $A$ and $B$ in the edge $A\rightarrow B$ as the \emph{cause node} and \emph{result node}, respectively.

\begin{minipage}{\textwidth}
    \begin{minipage}[b]{0.52\textwidth}
    \raggedright
    \begin{algorithm}[H]
    \footnotesize
    \SetAlgoLined
    \KwResult{ ordered edges set $\mathbf{E}=\{e_1, \ldots,e_n\}$ }
    $\mathbf{E}=\{\}$ ; $N_R = \{ n_0 \mid n_0 \in N, Parent(n_0)=\varnothing\}$ \;
    \While{$N_R \subset N$}{
    $\Delta = \{\}$ \;
    \For{ $n \in N$ }{
        \For{$p \in Parent(n)$}{
            \If{$n \notin N_R$ and $p \in N_R$}{
                $e=(p,n)$; \ 
                $\beta = \{\}$;\\ 
                \For{$r \in N_R$}{
                    \If{$r \in Parent(n)$ and $r \neq p$}{
                        $\beta = \beta \cup r$}
                    }
                $\delta_e = K(\beta \cup p, n) - K(\beta, n)$;\\
                $\Delta = \Delta \cup \delta_e$;
            }
        }
    }
    $\sigma = argmin_e(\delta_e \mid \delta_e \in \Delta)$;\\
    $\mathbf{E} = \mathbf{E} \cup \sigma$; \ 
    $N_R = N_R \cup n_{\sigma}$;\\
    }
     \caption{Latent Space Causal Discovery}
    \end{algorithm}
    \end{minipage}
%\hfill 
    \begin{minipage}[b]{0.4\textwidth}
    \raggedleft
        \label{tab:table1}
        \begin{tabular}{|l|l|}
        \hline
           $G=(N,E)$ & graph $G$ consists of $N$ and $E$ \\
           $N$ & the set of nodes\\
           $E$ & the set of edges\\
           $N_R$ & the set of reachable nodes\\
           $\mathbf{E}$ & the list of discovered edges\\
           $K(\beta, n)$ & KLD metric of effect $\beta\rightarrow n$\\
           $\beta$ & the cause nodes \\
           $n$ & the result node\\
           $\delta_e$ & KLD Gain of candidate edge $e$\\
           $\Delta=\{\delta_e\}$ & the set $\{\delta_e\}$ for $e$ \\
           $n$,$p$,$r$ & notations of nodes\\
           $e$,$\sigma$ & notations of edges\\
          \hline
        \end{tabular} %
%       \captionof{table}{A table beside a figure}
    \end{minipage}
\end{minipage}



Figure~\ref{fig:discover} illustrates the causal structure discovery process in latent space over four steps. Two edges, ($e_1$ and $e_3$), are sequentially selected, with $e_1$ setting node $B$ as the starting point for $e_3$. In step 3, edge $e_2$ from $A$ to $C$ is deselected and reassessed due to the new edge $e_3$ altering $C$'s existing causal conditions. The final DAG represents the resulting causal structure.

%\vspace{-2mm}
% Figure environment removed




\vspace{-2mm}
\section{Efficacy Validation Experiments}
\vspace{-3mm}
\label{sec:experiment}

The experiments aim to validate the efficacy of the \emph{relation-defined representation} learning method in three areas: 1) extracting higher-dimensional representations with the proposed autoencoder architecture, 2) hierarchically establishing relation-defined representations, and 3) discovering DAG structure in latent space.
A full demonstration of the experiments conducted in this study is available online \footnote{https://github.com/kflijia/bijective\_crossing\_functions.git}.

We use a synthetic hydrology dataset for our experiments, a common resource in the field of hydrology. The task focuses on predicting streamflow based on observed environmental conditions like temperature and precipitation. The application of relation-defined representation learning aims to create a streamflow prediction model that is generalizable across various watersheds. While these watersheds share a fundamental hydrological scheme governed by physical rules, they may exhibit unique features due to unobserved conditions such as economic development and land use. Current models based on physical knowledge, however, often lack the flexibility to fully capture multiple levels of dynamical temporal features across these watersheds.

In fact, to evaluate model robustness and generalizability, health informatics data would be optimal due to their complex confounding dynamics across multiple timelines. However, empirical constraints prevented us from accessing such data for this study. For insights into inherent temporal bias, we refer readers to previous work \cite{li2020teaching}.



% The feasibility experiments do not assess the effectiveness of generalizing established models across different watersheds because the data's long causal chains make it unsuitable for generalizations. However, we plan to explore generalization and individualization in future work. Although EHR data could be a good choice for this purpose, we did not have access to it during this study due to empirical restrictions. The complete experimental demonstration of the feasibility experiments is provided \footnote
% {https://github.com/kflijia/bijective\_crossing\_functions.git}. 



\vspace{-3mm}
% Figure environment removed




%\vspace{-1mm}
\subsection{Hydrology Dataset}
\vspace{-2mm}

In hydrology, deep learning, particularly RNN models, has gained favor for extracting observational representations and predicting streamflow \cite{goodwell2020debates, kratzert2018rainfall}. 
For our experiments, we employ the Soil and Water Assessment Tool (SWAT), a comprehensive system grounded in physical modules, to generate dynamically significant hydrological time series. We focus on a simulation of the Root River Headwater watershed in Southeast Minnesota, covering 60 consecutive virtual years with daily updates. %The performance evaluations predominantly focus on the accuracy of the autoencoder reconstructions.

Figure \ref{fig:stream} displays the causal DAG employed by SWAT, complete with node descriptions. The hierarchy of hydrological routines is color-coded based on their contribution to output streamflow. Surface runoff (1st tier) significantly impacts rapid streamflow peaks, followed by lateral flow (2nd tier). Baseflow dynamics (3rd tier) have a subtler influence. Our causal discovery experiments aim to reveal these underlying relationships from the observed data.


\vspace{-1mm}
\subsection{Higher-Dimensional Variable Representation Test}
\vspace{-2mm}

In this test, we have a total of ten variables (or nodes), each requiring a separate autoencoder for initializing a higher-dimensional representation. Table \ref{tab:tower} lists the statistics of their post-scaled (i.e., normalized) attributes, as well as their autoencoders' reconstruction accuracies. Accuracy is assessed in the root mean square error (RMSE), where a lower RMSE indicates higher accuracy for both scaled and unscaled data.

The task is challenging due to the limited dimensionality of the ten variables - maxing out at just 5 dimensions and the target node, $J$, having just one attribute. To mitigate this, we duplicate their columns to a consistent 12 dimensions and add 12 dummy variables for months, resulting in a 24-dimensional input. A double-wise extension amplifies this to 576 dimensions, from which a 16-dimensional representation is extracted via the autoencoder.
Another issue is the presence of meaningful zero-values, such as node $D$ (Snowpack in winter), which contributes numerous zeros in other seasons and is closely linked to node $E$ (Soil Water). We tackle this by adding non-zero indicator variables, called \emph{masks}, evaluated via binary cross-entropy (BCE).

% Significant challenges also arise from considerable meaningful-zero values. For example, node $D$ (Snowpack in winter) includes numerous zeros in other seasons, closely related to node $E$ (Soil Water) values. We address this by concurrently reconstructing non-zero indicator variables, named masks, within the autoencoder, evaluated using binary cross entropy (BCE). 

Despite challenges, RMSE values ranging from $0.01$ to $0.09$ indicate success, except for node $F$ (the Aquifer). Given that aquifer research is still emerging (i.e., the 3rd tier baseflow routine), it is likely that node $F$ in this synthetic dataset may better represent noise than meaningful data.


\begin{table*}[t]
\caption{Statistics of variable attributes and performances of the variable representation test.}
\label{tab:tower}
\vspace{-3mm}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{|
>{\columncolor[HTML]{EFEFEF}}c |c|l|l|l|l|l|l|l|l|}
\hline
\cellcolor[HTML]{C0C0C0}Variable & \cellcolor[HTML]{C0C0C0} Dim & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Mean} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Std} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Min} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Max} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Non-Zero Rate\%} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}RMSE on Scaled} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}RMSE on Unscaled} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}BCE of Mask} \\ \hline
A & 5 & 1.8513 & 1.5496 & -3.3557 & 7.6809 & 87.54 & 0.093 & 0.871 & 0.095 \\ \hline
B & 4 & 0.7687 & 1.1353 & -3.3557 & 5.9710 & 64.52 & 0.076 & 0.678 & 1.132 \\ \hline
C & 2 & 1.0342 & 1.0025 & 0.0 & 6.2145 & 94.42 & 0.037 & 0.089 & 0.428 \\ \hline
D & 3 & 0.0458 & 0.2005 & 0.0 & 5.2434 & 11.40 & 0.015 & 0.679 & 0.445 \\ \hline
E & 2 & 3.1449 & 1.0000 & 0.0285 & 5.0916 & 100 & 0.058 & 3.343 & 0.643 \\ \hline
F & 4 & 0.3922 & 0.8962 & 0.0 & 8.6122 & 59.08 & 0.326 & 7.178 & 2.045 \\ \hline
G & 4 & 0.7180 & 1.1064 & 0.0 & 8.2551 & 47.87 & 0.045 & 0.81 & 1.327 \\ \hline
H & 4 & 0.7344 & 1.0193 & 0.0 & 7.6350 & 49.93 & 0.045 & 0.009 & 1.345 \\ \hline
I & 3 & 0.1432 & 0.6137 & 0.0 & 8.3880 & 21.66 & 0.035 & 0.009 & 1.672 \\ \hline
J & 1 & 0.0410 & 0.2000 & 0.0 & 7.8903 & 21.75 & 0.007 & 0.098 & 1.088 \\ \hline
\end{tabular}}
\end{table*}




\begin{table*}[t]
\vspace{-2.5mm}
\caption{Brief summary of the latent space causal discovery test.}
\label{tab:discv}
\vspace{-3mm}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{|c|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
\cellcolor[HTML]{C0C0C0}Edge & \cellcolor[HTML]{FFCCC9}A$\rightarrow$C & \cellcolor[HTML]{FFCCC9}B$\rightarrow$D & \cellcolor[HTML]{FFCCC9}C$\rightarrow$D & \cellcolor[HTML]{FFCCC9}C$\rightarrow$G & \cellcolor[HTML]{FFCCC9}D$\rightarrow$G & \cellcolor[HTML]{FFCCC9}G$\rightarrow$J & \cellcolor[HTML]{FFCCC9}D$\rightarrow$H & \cellcolor[HTML]{FFCCC9}H$\rightarrow$J & \cellcolor[HTML]{FFCE93}B$\rightarrow$E & \cellcolor[HTML]{FFCE93}E$\rightarrow$G & \cellcolor[HTML]{FFCE93}E$\rightarrow$H & \cellcolor[HTML]{FFCE93}C$\rightarrow$E & \cellcolor[HTML]{ABE9E7}E$\rightarrow$F & \cellcolor[HTML]{ABE9E7}F$\rightarrow$I & \cellcolor[HTML]{ABE9E7}I$\rightarrow$J & \cellcolor[HTML]{ABE9E7}D$\rightarrow$I \\ \hline
\cellcolor[HTML]{C0C0C0}KLD & 7.63 & 8.51 & 10.14 & 11.60 & 27.87 & 5.29 & 25.19 & 15.93 & 37.07 & 39.13 & 39.88 & 46.58 & 53.68 & 45.64 & 17.41 & 75.57 \\ \hline
\rowcolor[HTML]{FFFFFF} 
\cellcolor[HTML]{C0C0C0}Gain & 7.63 & 8.51 & 1.135 & 11.60 & 2.454 & 5.29 & 25.19 & 0.209 & 37.07 & -5.91 & -3.29 & 2.677 & 53.68 & 45.64 & 0.028 & 3.384 \\ \hline
\end{tabular}}
\vspace{-4mm}
\end{table*}


%\vspace{1mm}
\subsection{Hierarchical Relation-Defined Representations Test}
\vspace{-2mm}

Table \ref{tab:unit} presents the results of the relation-defined representation learning. We use the term ``single-effect'' to describe the accuracy of a specific result node when reconstructed from a single cause node (e.g., $B\rightarrow D$ and $C\rightarrow D$), and ``full-effect'' for the accuracy when all its cause nodes are stacked (e.g., $BC\rightarrow D$). To provide context, we also include baseline performance scores based on the initial variable representations. During the relation learning process, the result node serves two purposes: it maintains its own accurate representation (as per optimization no.2 in \ref{sec:method_3}) and helps reconstruct the relationship (as per optimization no.1). Both aspects are evaluated in Table \ref{tab:unit}.


% For example, node $G$ has three possible causes $C$, $D$, and $E$, so we built four causal effect models in total: the individual pairwise causations $C\rightarrow G$, $D\rightarrow G$, $E\rightarrow G$, and the stacked causal effect $CDE\rightarrow G$. For convenience, we refer to them as "\emph{pair-relation}" and "\emph{stacking-relation}" respectively. We also listed the initial reconstruction performance for each variable in the column named "Variable Reconstruction (initial performance)" as a comparison baseline in Table \ref{tab:unit}.


%\vspace{-3mm}
% Figure environment removed

% To evaluate the latent causal effect learning process, we have three different optimization tasks, according to the three steps, among which both the \emph{second} and \emph{third} ones involve the role of the \emph{result} node. 
% Therefore, in Table \ref{tab:unit}, we provide the performance of each node for both roles. The column named "Variable Reconstruction (as result node)" represents the performance of the \emph{result} node in the \emph{third} optimization task, while the columns named "Latent Causal Effect Reconstruction" represent the causal effect learning performance of RNN models for the \emph{second} optimization task, along with the KLD metrics to display the learned causality strength (the lower value indicates stronger causality). 

The KLD metrics in Table \ref{tab:unit} indicate the strength of learned causality, with a lower value signifying stronger. For instance, node $J$'s minimal KLD values suggest a significant effect caused by nodes $G$ (Surface Runoff), $H$ (Lateral), and $I$ (Baseflow). In contrast, the high KLD values imply that predicting variable $I$ using $D$ and $F$ is challenging. 
For nodes $D$, $E$, and $J$, the ``full-effect'' are moderate compared to their ``single-effect'' scores, suggesting a lack of informative associations among the cause nodes. In contrast, for nodes $G$ and $H$, lower ``full-effect'' KLD values imply capturing meaningful associative effects through hierarchical stacking. The KLD metric also reveals the most contributive cause node to the result node. For example, the proximity of the $C\rightarrow G$ strength to $CDE\rightarrow G$ suggests that $C$ is the primary contributor to this causal relationship.

Figure~\ref{fig:G} showcases reconstructed time series, for the result nodes $J$, $G$, and $I$, in the same synthetic year to provide a straightforward overview of the hierarchical representation performances. %Their initial variable representation reconstructed data are plotted as blue lines, with the ground truth shown in black dots, and red lines displaying simulations from the hierarchically stacked effects representations.
%In addition to RMSE, we also use the NashSutcliffe model efficiency coefficient (NSE), a hydrology-meaningful accuracy evaluation metric, which ranges from -$\infty$ to 1, with 1 indicating the perfect prediction or simulation.
Here, black dots represent the ground truth; the blue line indicates reconstruction via the initial variable representation, and the ``full-effect'' representation generates the red line. 
In addition to RMSE, we also employ the NashSutcliffe model efficiency coefficient (NSE) as an accuracy metric, commonly used in hydrological predictions. The NSE ranges from -$\infty$ to 1, with values closer to 1 indicating higher accuracy.

The initial variable representation closely aligns with the ground truth, as shown in Figure~\ref{fig:G}, attesting to the efficacy of our proposed autoencoder architecture. As expected, the ``full-effect'' performs better than the ``single-effect'' for each result node. Node $J$ exhibits the best prediction, whereas node $I$ presents a challenge. For node $G$, causality from $C$ proves to be significantly stronger than the other two, $D$ and $E$.


One may observe via the demo that our experiments do not show smooth information flows along successive long causal chains. Since RNNs are designed primarily for capturing the dynamics of causes rather than the effects, relying on them to autonomously construct dynamical representations of the effects might prove unreliable. It underscores a significant opportunity for enhancing effectiveness by improving the architecture.



\vspace{-1mm}
\subsection{Latent Space Causal Discovery Test}
\vspace{-2mm}

The discovery test initiates with source nodes $A$ and $B$ and proceeds to identify potential edges, culminating in the target node $J$. Candidate edges are selected based on their contributions to the overall KLD sum (less gain is better). 
Table \ref{tab:discv} shows the order in which existing edges are discovered, along with the corresponding KLD sums and gains after each edge is included. Color-coding in the cells corresponds to Figure \ref{fig:stream}, indicating tiers of causal routines. The arrangement underscores the efficacy of this latent space discovery approach.

A comprehensive list of candidate edges evaluated in each discovery round is provided in Table \ref{tab:discv_rounds} in Appendix A. For comparative purposes, we also performed a 10-fold cross-validation using the conventional FGES discovery method; those results are available in Table \ref{tab:fges} in Appendix A.


%\vspace{-3mm}
\begin{table*}[t]
\caption{Performances of the relation-defined representations, sorted by the result node.}
\label{tab:unit}
\vspace{-2mm}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{|l|lll|l|lll|llll|}
\hline
\rowcolor[HTML]{C0C0C0} 
\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} } & \multicolumn{3}{l|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \begin{tabular}[c]{@{}l@{}}\ \ \ \ \ \ Variable Representation\\ \ \ \ \ \ \ \ (Initial)\end{tabular}}} & \cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} } & \multicolumn{3}{l|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000}  \begin{tabular}[c]{@{}l@{}}\ \ \ \ \ \ Variable Representation\\ \ \ \ \ \ \ \ (in Relation Learning)\end{tabular}}} & \multicolumn{4}{l|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \ \ \ \ \ \ \ \ \ Relationship Reconstruction}} \\ \cline{2-4} \cline{6-12} 
\rowcolor[HTML]{C0C0C0} 
\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} } & \multicolumn{2}{l|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000}\ \ \ \ \ \  \ \ \ \ RMSE}} & {\color[HTML]{000000} BCE} & \cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} } & \multicolumn{2}{l|}{\cellcolor[HTML]{C0C0C0}\ \ \ \ \ \ \ \ \ \ RMSE} & BCE & \multicolumn{2}{l|}{\cellcolor[HTML]{C0C0C0} \ \ \ \ \ \ \ \ \ RMSE} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}BCE} & KLD \\ \cline{2-4} \cline{6-12} 
\rowcolor[HTML]{C0C0C0} 
\multirow{-3}{*}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \begin{tabular}[c]{@{}l@{}}Result\\ Node\\ \ \end{tabular}}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Scaled\\ \ \ Values\end{tabular}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Unscaled\\ \ \ Values\end{tabular}} & Mask & \multirow{-3}{*}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \begin{tabular}[c]{@{}l@{}}Cause\\ Node\end{tabular}}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Scaled\\ \ \ Values\end{tabular}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Unscaled\\ \ \ Values\end{tabular}} & Mask & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Scaled\\ \ \ Values\end{tabular}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Unscaled\\ \ \ Values\end{tabular}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Mask} & \begin{tabular}[c]{@{}l@{}}(in latent\\ \ \ space)\end{tabular} \\ \hline
\cellcolor[HTML]{EFEFEF}C & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}0.037} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}0.089} & \cellcolor[HTML]{EFEFEF}0.428 & A & \multicolumn{1}{l|}{0.0295} & \multicolumn{1}{l|}{0.0616} & 0.4278 & \multicolumn{1}{l|}{0.1747} & \multicolumn{1}{l|}{0.3334} & \multicolumn{1}{l|}{0.4278} & 7.6353 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & BC & \multicolumn{1}{l|}{0.0350} & \multicolumn{1}{l|}{1.0179} & 0.1355 & \multicolumn{1}{l|}{0.0509} & \multicolumn{1}{l|}{1.7059} & \multicolumn{1}{l|}{0.1285} & 9.6502 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & B & \multicolumn{1}{l|}{0.0341} & \multicolumn{1}{l|}{1.0361} & 0.1693 & \multicolumn{1}{l|}{0.0516} & \multicolumn{1}{l|}{1.7737} & \multicolumn{1}{l|}{0.1925} & 8.5147 \\ \cline{5-5}
\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}D} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.015}} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.679}} & \multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.445} & C & \multicolumn{1}{l|}{0.0331} & \multicolumn{1}{l|}{0.9818} & 0.3404 & \multicolumn{1}{l|}{0.0512} & \multicolumn{1}{l|}{1.7265} & \multicolumn{1}{l|}{0.3667} & 10.149 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & BC & \multicolumn{1}{l|}{0.4612} & \multicolumn{1}{l|}{26.605} & 0.6427 & \multicolumn{1}{l|}{0.7827} & \multicolumn{1}{l|}{45.149} & \multicolumn{1}{l|}{0.6427} & 39.750 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & B & \multicolumn{1}{l|}{0.6428} & \multicolumn{1}{l|}{37.076} & 0.6427 & \multicolumn{1}{l|}{0.8209} & \multicolumn{1}{l|}{47.353} & \multicolumn{1}{l|}{0.6427} & 37.072 \\ \cline{5-5}
\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}E} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.058}} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}3.343}} & \multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.643} & C & \multicolumn{1}{l|}{0.5212} & \multicolumn{1}{l|}{30.065} & 1.2854 & \multicolumn{1}{l|}{0.7939} & \multicolumn{1}{l|}{45.791} & \multicolumn{1}{l|}{1.2854} & 46.587 \\ \hline
\cellcolor[HTML]{EFEFEF}F & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}0.326} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}7.178} & \cellcolor[HTML]{EFEFEF}2.045 & E & \multicolumn{1}{l|}{0.4334} & \multicolumn{1}{l|}{8.3807} & 3.0895 & \multicolumn{1}{l|}{0.4509} & \multicolumn{1}{l|}{5.9553} & \multicolumn{1}{l|}{3.0895} & 53.680 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & CDE & \multicolumn{1}{l|}{0.0538} & \multicolumn{1}{l|}{0.9598} & 0.0878 & \multicolumn{1}{l|}{0.1719} & \multicolumn{1}{l|}{3.5736} & \multicolumn{1}{l|}{0.1340} & 8.1360 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & C & \multicolumn{1}{l|}{0.1057} & \multicolumn{1}{l|}{1.4219} & 0.1078 & \multicolumn{1}{l|}{0.2996} & \multicolumn{1}{l|}{4.6278} & \multicolumn{1}{l|}{0.1362} & 11.601 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & D & \multicolumn{1}{l|}{0.1773} & \multicolumn{1}{l|}{3.6083} & 0.1842 & \multicolumn{1}{l|}{0.4112} & \multicolumn{1}{l|}{8.0841} & \multicolumn{1}{l|}{0.2228} & 27.879 \\ \cline{5-5}
\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}G} & \multicolumn{1}{l|}{\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}0.045}} & \multicolumn{1}{l|}{\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}0.81}} & \multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}1.327} & E & \multicolumn{1}{l|}{0.1949} & \multicolumn{1}{l|}{4.7124} & 0.1482 & \multicolumn{1}{l|}{0.5564} & \multicolumn{1}{l|}{10.852} & \multicolumn{1}{l|}{0.1877} & 39.133 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & DE & \multicolumn{1}{l|}{0.0889} & \multicolumn{1}{l|}{0.0099} & 2.5980 & \multicolumn{1}{l|}{0.3564} & \multicolumn{1}{l|}{0.0096} & \multicolumn{1}{l|}{2.5980} & 21.905 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & D & \multicolumn{1}{l|}{0.0878} & \multicolumn{1}{l|}{0.0104} & 0.0911 & \multicolumn{1}{l|}{0.4301} & \multicolumn{1}{l|}{0.0095} & \multicolumn{1}{l|}{0.0911} & 25.198 \\ \cline{5-5}
\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}H} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.045}} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.009}} & \multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}1.345} & E & \multicolumn{1}{l|}{0.1162} & \multicolumn{1}{l|}{0.0105} & 0.1482 & \multicolumn{1}{l|}{0.5168} & \multicolumn{1}{l|}{0.0097} & \multicolumn{1}{l|}{3.8514} & 39.886 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & DF & \multicolumn{1}{l|}{0.0600} & \multicolumn{1}{l|}{0.0103} & 3.4493 & \multicolumn{1}{l|}{0.1158} & \multicolumn{1}{l|}{0.0099} & \multicolumn{1}{l|}{3.4493} & 49.033 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & D & \multicolumn{1}{l|}{0.1212} & \multicolumn{1}{l|}{0.0108} & 3.0048 & \multicolumn{1}{l|}{0.2073} & \multicolumn{1}{l|}{0.0108} & \multicolumn{1}{l|}{3.0048} & 75.577 \\ \cline{5-5}
\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}I} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.035}} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.009}} & \multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}1.672} & F & \multicolumn{1}{l|}{0.0540} & \multicolumn{1}{l|}{0.0102} & 3.4493 & \multicolumn{1}{l|}{0.0948} & \multicolumn{1}{l|}{0.0098} & \multicolumn{1}{l|}{3.4493} & 45.648 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & GHI & \multicolumn{1}{l|}{0.0052} & \multicolumn{1}{l|}{0.0742} & 0.2593 & \multicolumn{1}{l|}{0.0090} & \multicolumn{1}{l|}{0.1269} & \multicolumn{1}{l|}{0.2937} & 5.5300 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & G & \multicolumn{1}{l|}{0.0077} & \multicolumn{1}{l|}{0.1085} & 0.4009 & \multicolumn{1}{l|}{0.0099} & \multicolumn{1}{l|}{0.1390} & \multicolumn{1}{l|}{0.4375} & 5.2924 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & H & \multicolumn{1}{l|}{0.0159} & \multicolumn{1}{l|}{0.2239} & 0.4584 & \multicolumn{1}{l|}{0.0393} & \multicolumn{1}{l|}{0.5520} & \multicolumn{1}{l|}{0.4938} & 15.930 \\ \cline{5-5}
\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}J} & \multicolumn{1}{l|}{\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}0.007}} & \multicolumn{1}{l|}{\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}0.098}} & \multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}1.088} & I & \multicolumn{1}{l|}{0.0308} & \multicolumn{1}{l|}{0.4328} & 0.3818 & \multicolumn{1}{l|}{0.0397} & \multicolumn{1}{l|}{0.5564} & \multicolumn{1}{l|}{0.3954} & 17.410 \\ \hline
\end{tabular}
}
\vspace{-3mm}
\end{table*}


\section{Conclusions}\label{sec:conclusion}
\vspace{-3mm}

Motivated by the issue of AI misalignment, we explore the inherent limitation of the prevalent \emph{Observation-Oriented} paradigm and introduce a new \emph{Relation-Oriented} one, complemented by the practical approach of \emph{relation-defined representation} learning, and validate its efficacy through experimentations.

This research introduces a dimensionality framework for understanding relationship learning, offering new, intuitive insights into causal inference and highlighting the restrictions of the existing \emph{Observation-Oriented} paradigm. This paradigm typically requires pre-identification of modeling objects before defining relations, which confines models to the \emph{observational} space, limiting their access to dynamical temporal features.
Further, by relying on a single, absolute timeline, it neglects the multi-dimensional nature of the \emph{temporal} feature space, compromising both model robustness and generalizability.

At its core, human cognition prioritizes relations, giving rise to our vast relation-centric knowledge systems. We can identify dynamics by navigating the intricate network of relations in unobservable \emph{hyper-dimensional} space. This insight inspired the \emph{Relation-Oriented} paradigm. 

While implementing relation-defined representation learning, we faced significant challenges, including designing an invertible autoencoder for higher-dimensional representation. Nevertheless, thorough experiments have affirmed the feasibility of the proposed methodology.
AI alignment is never a question with a simple answer but calls for interdisciplinary efforts \cite{christian2020alignment}.
Through this work, we aim to contribute to developing more genuine AI and provide a foundation for future advancements.

% Our traditional thinking about AI often assumes that incorporating larger, more diverse datasets can lead to more substantial advances. While that indeed powers AI but also creates more significant misalignments that deviate from our knowledge and cognition. As \cite{christian2020alignment} highlights, AI alignment issues extend beyond technical issues, but point to humans' ``blind spots'' and ``unstated assumptions''.
% We might not differentiate ourselves from AI systems based on our capacity to understand large contexts, but rather on how we perceive the individual relationship.

% This study advocates for a ``\emph{Relation-Oriented}'' modeling principle, which could disrupt our prevalent \emph{Observation-Oriented} modeling convention and align more closely with the relation-centric nature of human comprehension.
% To be specific, the knowledge nodes we construct in our cognition, are motivated by, and indexed through, relationships that confer them specific meanings under our comprehension context, rather than solely reflecting observations.
% Through relations, our understanding can surpass observational limitations, spanning: \textbf{1)} hyper-dimensions that encompass unobservable knowledge hierarchies, and \textbf{2)} temporal spaces where we store dynamic events, governed by multiple logical timelines.

% Additionally, this study presents a feasible \emph{Relation-Oriented} modeling technique, extracting relation-defined representations from observations, to instantiate the knowledge nodes in our understanding.





% In recent years, AI applications like ChatGPT have made impressive strides. However, we have yet to witness AI replace humans in making knowledgeable decisions, such as an AI doctor prescribing medication or an AI lawyer providing legal consultations.
% Indeed, AI regards all connections as associations, while the evolution of human knowledge is rooted in causality.
% Understanding causality requires an awareness of timelines, which is a fundamental difference between humans and AI.
% While humans can intuitively distinguish between causal effects and correlated changes, AI lacks this ability. 


% Causal inference theories develop based on directed acyclic graph (DAG) representing causal structures, without identifying observed relationships as causal or non-causal.
% While DAGs can serve as a model for our awareness of causality, they are not suitable for directing AI's learning. Instead, explicit isolation of timelines as axes is required to construct a multi-dimensional geometric space that enables AI to capture the causal structure.

% This paper introduces the do-DAG as a precise and rigorous specification of the necessary space. Our study thoroughly investigates the geometric implications of this concept, identifies the presence of inherent representation biases within the current causal learning framework, and proposes a general solution to address this issue. We aim for this work to pave the way for future research and advancements in causal AI.

%with the goal of identifying the fundamental obstacles that prevent AI from comprehending human knowledge. Furthermore, we 

%doctors in making prescription decisions or cracking the genetic code automatically, although they have much fewer computational complexities than learning tens of languages, computing pixels of movies, or evaluating billions of possibilities in the Go game.
%So, what sealed AI's capability when learning structural data?
%It seems like AI's capability is ``sealed'' on learning the causally structured data. 

\vspace{4mm}


\section*{Acknowledgements}
\vspace{-3mm}

I want to extend my heartfelt thanks to my friend, Dr. Gao Qiman, the lone companion willing to engage in profound philosophical discussions with me, and who has provided invaluable advice. Furthermore, I express my gratitude to GPT-4 for its crucial aid in improving my English writing.

\hfill Jia Li

%\vspace{40mm}

\bibliography{main}
\bibliographystyle{tmlr}


\appendix
\section{Appendix: Complete Experimental Results of Causal Discovery}
%\includepdf[pages=-]{mainz_append_pdf.pdf}

\pagebreak


%\newpage
\begin{sidewaystable}
%\begin{table*}[t]
\centering
\caption{The Complete Results of Heuristic Causal Discovery in latent space.
Each row stands for a round of detection, with `\#'' identifying the round number, and all candidate edges are listed with their KLD gains as below. 1) Green cells: the newly detected edges. 2) Red cells: the selected edge.
3) Blue cells: the trimmed edges accordingly. }
%\vspace{-20mm}
\label{tab:discv_rounds}
\resizebox{1\columnwidth}{!}{%
%\rotatebox{90}{

\begin{tabular}{|c|c|c|cccccccccccccc}
\cline{1-9}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \textbf{A$\rightarrow$ C}} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ D & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{B$\rightarrow$ C}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ D} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 1}} & \cellcolor[HTML]{FFCCC9}{\color[HTML]{000000} \textbf{7.6354}} & 19.7407 & \multicolumn{1}{c|}{60.1876} & \multicolumn{1}{c|}{119.7730} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{8.4753}} & \multicolumn{1}{c|}{8.5147} & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} &  &  &  &  &  &  &  &  \\ \cline{1-13}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ D & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{B$\rightarrow$ D}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ D} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 2}} & 19.7407 & 60.1876 & \multicolumn{1}{c|}{119.7730} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{8.5147}}} & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}10.1490} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}46.5876} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}111.2978} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}11.6012} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}39.2361} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}95.1564} &  &  &  &  \\ \hline
\rowcolor[HTML]{C0C0C0} 
\cellcolor[HTML]{C0C0C0} & \textbf{A$\rightarrow$ D} & A$\rightarrow$ E & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{C$\rightarrow$ D}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 3}} & \cellcolor[HTML]{CBCEFB}\textbf{9.7357} & 60.1876 & \multicolumn{1}{c|}{119.7730} & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{1.1355}}} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{11.6012} & \multicolumn{1}{c|}{39.2361} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}63.7348} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}123.3203} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}27.8798} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}25.1988} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}75.5775} \\ \hline
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \textbf{C$\rightarrow$ G}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 4}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{000000} \textbf{11.6012}}} & \multicolumn{1}{c|}{39.2361} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{27.8798} & \multicolumn{1}{c|}{25.1988} & \multicolumn{1}{c|}{75.5775} &  &  \\ \cline{1-15}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{D$\rightarrow$ G}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}G$\rightarrow$ J} &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 5}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{39.2361} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{2.4540}}} & \multicolumn{1}{c|}{25.1988} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}5.2924} &  &  \\ \cline{1-15}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{G$\rightarrow$ J}}} &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 6}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{39.2361} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{25.1988} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{5.2924}}} &  &  &  \\ \cline{1-14}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{C$\rightarrow$ H}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{D$\rightarrow$ H}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 7}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{39.2361}} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{25.1988}}} & \multicolumn{1}{c|}{75.5775} &  &  &  &  \\ \cline{1-13}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{H$\rightarrow$ J}}} &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 8}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{0.2092}}} &  &  &  &  &  \\ \cline{1-12}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}\textbf{A$\rightarrow$ E} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{C$\rightarrow$ E}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 9}} & \cellcolor[HTML]{CBCEFB}\textbf{60.1876} & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{46.5876}}} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} &  &  &  &  &  &  \\ \cline{1-13}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{B$\rightarrow$ E}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{D$\rightarrow$ E}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ I} &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 10}} & 119.7730 & \cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{-6.8372}} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{17.0407}} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}53.6806} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}-5.9191} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}-3.2931} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}110.2558} &  &  &  &  \\ \cline{1-13}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \cellcolor[HTML]{C0C0C0}B$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{E$\rightarrow$ G}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ I} &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 11}} & 119.7730 & 132.7717 & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{53.6806} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{-5.9191}} & \multicolumn{1}{c|}{-3.2931} & \multicolumn{1}{c|}{110.2558} &  &  &  &  &  &  \\ \cline{1-11}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \cellcolor[HTML]{C0C0C0}B$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{E$\rightarrow$ H}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ I} &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 12}} & 119.7730 & 132.7717 & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{53.6806} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{-3.2931}} & \multicolumn{1}{c|}{110.2558} &  &  &  &  &  &  &  \\ \cline{1-10}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}\textbf{A$\rightarrow$ F} & \cellcolor[HTML]{C0C0C0}\textbf{B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{C$\rightarrow$ F}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{D$\rightarrow$ F}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{E$\rightarrow$ F}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ I} &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 13}} & \cellcolor[HTML]{CBCEFB}\textbf{119.7730} & \cellcolor[HTML]{CBCEFB}\textbf{132.7717} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{111.2978}} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{123.3203}} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{53.6806}} & \multicolumn{1}{c|}{110.2558} &  &  &  &  &  &  &  &  \\ \cline{1-9}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}C$\rightarrow$ I & \cellcolor[HTML]{C0C0C0}D$\rightarrow$ I & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{E$\rightarrow$ I}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{F$\rightarrow$ I}} &  &  &  &  &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 14}} & 95.1564 & 75.5775 & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{110.2558}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{45.6490}} &  &  &  &  &  &  &  &  &  &  &  &  \\ \cline{1-5}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}C$\rightarrow$ I & \cellcolor[HTML]{C0C0C0}D$\rightarrow$ I & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{I$\rightarrow$ J}} &  &  &  &  &  &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 15}} & 15.0222 & 3.3845 & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.0284}} &  &  &  &  &  &  &  &  &  &  &  &  &  \\ \cline{1-4}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}\textbf{C$\rightarrow$ I} & \cellcolor[HTML]{C0C0C0}\textbf{D$\rightarrow$ I} &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 16}} & \cellcolor[HTML]{CBCEFB}\textbf{15.0222} & \cellcolor[HTML]{FFCCC9}\textbf{3.3845} &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\ \cline{1-3}
\end{tabular}

}%}
%\vspace{-3mm}
%\end{table*}

\end{sidewaystable}







\begin{sidewaystable}
%\centering

\caption{Average performance of 10-Fold FGES (Fast Greedy Equivalence Search) causal discovery, with the prior knowledge that each node can only cause the other nodes with the same or greater depth with it. An edge means connecting two attributes from two different nodes, respectively. Thus, the number of possible edges between two nodes is the multiplication of the numbers of their attributes, i.e., the lengths of their data vectors.
\\ (All experiments are performed with 6 different Independent-Test kernels, including chi-square-test, d-sep-test, prob-test, disc-bic-test, fisher-z-test, mvplr-test. But their results turn out to be identical.)}
\label{tab:fges}
\resizebox{1\columnwidth}{!}{%
%\Rotatebox{90}{
%
\begin{tabular}{cccccccccccccccccc}
\hline
\rowcolor[HTML]{C0C0C0} 
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}Cause Node} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A} & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}B} & \multicolumn{3}{c|}{\cellcolor[HTML]{C0C0C0}C} & \multicolumn{3}{c|}{\cellcolor[HTML]{C0C0C0}D} & \multicolumn{3}{c|}{\cellcolor[HTML]{C0C0C0}E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}F} & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}I} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}True\\ Causation\end{tabular}} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}A$\rightarrow$ C} & \multicolumn{1}{c|}{B$\rightarrow$ D} & \multicolumn{1}{c|}{B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}C$\rightarrow$ D} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}C$\rightarrow$ G} & \multicolumn{1}{c|}{D$\rightarrow$ G} & \multicolumn{1}{c|}{D$\rightarrow$ H} & \multicolumn{1}{c|}{D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}E$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}E$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}E$\rightarrow$ H} & \multicolumn{1}{c|}{F$\rightarrow$ I} & \multicolumn{2}{c|}{\cellcolor[HTML]{EFEFEF}G$\rightarrow$ J} & \multicolumn{1}{c|}{H$\rightarrow$ J} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}I$\rightarrow$ J} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Number of\\ Edges\end{tabular}} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}16} & \multicolumn{1}{c|}{24} & \multicolumn{1}{c|}{16} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}6} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}4} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8} & \multicolumn{1}{c|}{12} & \multicolumn{1}{c|}{12} & \multicolumn{1}{c|}{9} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8} & \multicolumn{1}{c|}{12} & \multicolumn{2}{c|}{\cellcolor[HTML]{EFEFEF}4} & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}3} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Probability\\ of Missing\end{tabular}} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.038889} & \multicolumn{1}{c|}{0.125} & \multicolumn{1}{c|}{0.125} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.062} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.06875} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.039286} & \multicolumn{1}{c|}{0.069048} & \multicolumn{1}{c|}{0.2} & \multicolumn{1}{c|}{0.142857} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.3} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.003571} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.2} & \multicolumn{1}{c|}{0.142857} & \multicolumn{2}{c|}{\cellcolor[HTML]{EFEFEF}0.0} & \multicolumn{1}{c|}{0.072727} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.030303} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Wrong \\ Causation\end{tabular}} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}C$\rightarrow$ F} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{D$\rightarrow$ E} & \multicolumn{1}{c|}{D$\rightarrow$ F} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{F$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}G$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}G$\rightarrow$ I} & \multicolumn{1}{c|}{H$\rightarrow$ I} &  \\ \cline{1-1} \cline{5-5} \cline{9-10} \cline{14-17}
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Times\\ of Wrongly\\ Discovered\end{tabular}} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}5.6} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{1.2} & \multicolumn{1}{c|}{0.8} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{5.0} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8.2} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}3.0} & \multicolumn{1}{c|}{2.8} &  \\ \cline{1-1} \cline{5-5} \cline{9-10} \cline{14-17}
\multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{}
\end{tabular}
%}
}


\vspace{0.4in}
%\end{table*}

%\begin{table*}[t]
%\centering
\caption{ Brief Results of the Heuristic Causal Discovery in latent space, identical with Table 3 in the paper body, for better comparison to the traditional FGES methods results on this page.\\
The edges are arranged in detected order (from left to right) and their measured causal strengths in each step are shown below correspondingly.\\
Causal strength is measured by KLD values (less is stronger). Each round of detection is pursuing the least KLD gain globally. All evaluations are in 4-Fold validation average values. Different colors represent the ground truth causality strength tiers (referred to  the Figure 10 in the paper body).
}
\label{tab:discv}
%\resizebox{0.1\columnwidth}{!}{%
%\Rotatebox{90}{
\vspace{2mm}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{|c|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
\cellcolor[HTML]{C0C0C0}Causation & \cellcolor[HTML]{FFCCC9}A$\rightarrow$ C & \cellcolor[HTML]{FFCCC9}B$\rightarrow$ D & \cellcolor[HTML]{FFCCC9}C$\rightarrow$ D & \cellcolor[HTML]{FFCCC9}C$\rightarrow$ G & \cellcolor[HTML]{FFCCC9}D$\rightarrow$ G & \cellcolor[HTML]{FFCCC9}G$\rightarrow$ J & \cellcolor[HTML]{FFCCC9}D$\rightarrow$ H & \cellcolor[HTML]{FFCCC9}H$\rightarrow$ J & \cellcolor[HTML]{FFCE93}C$\rightarrow$ E & \cellcolor[HTML]{FFCE93}B$\rightarrow$ E & \cellcolor[HTML]{FFCE93}E$\rightarrow$ G & \cellcolor[HTML]{FFCE93}E$\rightarrow$ H & \cellcolor[HTML]{ABE9E7}E$\rightarrow$ F & \cellcolor[HTML]{ABE9E7}F$\rightarrow$ I & \cellcolor[HTML]{ABE9E7}I$\rightarrow$ J & \cellcolor[HTML]{ABE9E7}D$\rightarrow$ I \\ \hline
\cellcolor[HTML]{C0C0C0}KLD & 7.63 & 8.51 & 10.14 & 11.60 & 27.87 & 5.29 & 25.19 & 15.93 & 46.58 & 65.93 & 39.13 & 39.88 & 53.68 & 45.64 & 17.41 & 75.57 \\ \hline
\rowcolor[HTML]{FFFFFF} 
\cellcolor[HTML]{C0C0C0}Gain & 7.63 & 8.51 & 1.135 & 11.60 & 2.454 & 5.29 & 25.19 & 0.209 & 46.58 & -6.84 & -5.91 & -3.29 & 53.68 & 45.64 & 0.028 & 3.384 \\ \hline
\end{tabular}
%}}
%\end{table*}
}

\end{sidewaystable}








\end{document}
