
\documentclass[10pt]{article} % For LaTeX2e

\newif\ifpreprint

% \usepackage{tmlr}
% \preprintfalse

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}

% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:

\usepackage[preprint]{tmlr}
\preprinttrue

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{hyperref}
\usepackage{url}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{adjustbox}
%\usepackage{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{centernot}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{color}
%\usepackage{booktabs}

\usepackage{subfigure}
\usepackage{caption}
\usepackage{dsfont}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{empheq}
%\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{totcount}

\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{makecell}
%\usepackage[table]{xcolor}
\usepackage{xcolor}
\usepackage{changepage}
\usepackage{stfloats}
%\usepackage{cite}
\usepackage{soul}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{algpseudocode}
\usepackage[title]{appendix}
\usepackage{pdfpages}

\usepackage{bm}
\usepackage{amsmath,amssymb,amsfonts}
% \usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{bookmark}
\usepackage{float}
\usepackage{tabularx}
\usepackage{mathtools}
\usepackage{array}
\usepackage{rotating}
\usepackage[skins]{tcolorbox}
\usepackage{tikz}
\usepackage{pifont}

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{ \node[shape=circle,draw,inner sep=1.5pt] (char) {#1};}}

\title{Relation-Oriented: Toward Causal Knowledge-Aligned AGI}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.


\author{
\name Jia Li \email jiaxx213@umn.edu \\
      \addr Department of Computer Science,
      University of Minnesota
      \vspace{-3mm}
      \AND
      \name Xiang Li \email lixx5000@umn.edu \\
      \addr Department of Bioproducts and Biosystems Engineering,       University of Minnesota
      % \name Xiaowei Jia \email xiaowei@pitt.edu\\
      % \addr Department of Computer Science\\ University of Pittsburgh
      }

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{Apr}  % Insert correct month for camera-ready version
\def\year{2023} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version

\definecolor{thegrey}{RGB}{192,192,192}
\definecolor{orchid}{RGB}{204,153,255}
\definecolor{myblue}{RGB}{0,115,207}
\definecolor{mygreen}{RGB}{68,134,25}
%\definecolor{myorange}{RGB}{0,128,0}
\definecolor{mypurple}{RGB}{177,35,200}

\begin{document}


\maketitle
%\vspace{-3mm}

\ifpreprint
\vspace{-4.5mm}
\fi

\begin{abstract}
\vspace{-3mm}

\ifpreprint
\vspace{-1mm}
\fi

The current relationship modeling paradigm, grounded in the observational i.i.d assumption, fundamentally misaligns with our causal knowledge understanding due to two key oversights: 1) the unobservable relations, which lead to undetectable hierarchical levels of knowledge, driving the need for model generalizability; 2) the cognitive relative timings, which crucially support our structural knowledge comprehension, resulting in inherent biases within the present \emph{Observation-Oriented} paradigm. Adopting a novel \emph{Relation-Oriented} perspective, this paper proposes a new framework to unify the various confusions surrounding causality learning, ranging from traditional causal inference to modern language models.

Also, relation-indexed representation learning (RIRL) is raised as a baseline implementation method of the proposed new paradigm, alongside comprehensive experiments demonstrating its efficacy in autonomously identifying dynamical effects in relationship learning.

\vspace{-2mm}
\end{abstract}

\ifpreprint
\vspace{-3mm}
\fi

\section{Introduction}
\vspace{-2.5mm}

\label{sec:intro}




%the capability of representing abstract knowledge and, accordingly, facilitating human-like causal reasoning in symbol-grounded systems \cite{marcus2020next}, like machine learning and Artificial Intelligence (AI). 
%A central question is whether symbols, as well as symbol-grounded systems, such as AI, can represent our empirical understanding and inquiries \cite{newell2007computer, pavlick2023symbols}.

The concept of Artificial General Intelligence (AGI) has prompted extensive discussions over the years \cite{newell2007computer}, with the target toward facilitating human-like causal reasoning and knowledge comprehension in AI systems \cite{marcus2020next}.
In recent years, the large language models (LLMs) have risen as notable achievements in language-understanding tasks and accordingly evoked debates about whether LLMs have edged us closer to realizing AGI  \cite{schaeffer2023emergent}.
Some studies point to their shortcomings in truly comprehending causality \cite{pavlick2023symbols}, %underlying the semantic associations, 
while others argue in favor of LLMs' ability to represent complex spatial and temporal features \cite{gurnee2023language}.
Notably, the use of meta-learning in language models has shown potential in achieving human-like generalization capabilities, at least to a certain extent \cite{lake2023human}.

These debates are anchored in an underlying inquiry: What underpins the distinction between two types of generalization?
One is how humans generalize learned causal knowledge to diverse scenarios, and another is how  AI systems generalize captured associative knowledge among texts and images.


%How do spatial, temporal, and abstract feature representations differentiate and yet interrelate? Accordingly, why does meta-learning seem to partially bridge the human-machine gap?

It appears that classical causal inference has offered a clear delineation among causality, correlations, and mere associations \cite{pearl2000models, peters2017elements}. Moreover, it has provided a robust theoretical groundwork for representing causality in computational models. Based on that, causal learning has been widely utilized and yielded significant contributions to causal knowledge accumulation in various fields \cite{wood2015lesson, vukovic2022causal, ombadi2020evaluation}.
It is thus logical to incorporate well-established causal knowledge, often represented as causal DAGs (Directed Acyclic Graphs), into AI model architectures \cite{marwala2015causality, lachapelle2019gradient}. While this integration has greatly enhanced learning efficiency, it has not yet achieved the level of generalizability that constitutes a success \cite{luo2020causal, ma2018using}.

This likely circles us back to the initial question, as causal inference cannot directly bridge the gap between human-like causal reasoning and current AI systems. However, it does offer a different perspective: How would humans conduct causal reasoning based solely on DAGs? A task that evidently challenges AI.

Even within the realm of causal inference, the process of converting DAGs into operational causal models is rigorous \cite{elwert2013graphical}. Tailored adjustments and interpretations are often required, reliant on human discernment across varied applications \cite{sanchez2022causal, crown2019real}. Key challenges include establishing fundamental causal assumptions \cite{sobel1996introduction}, addressing confounding effects \cite{greenland1999confounding}, ensuring model interpretability \cite{pearl2000models}, etc.
These achievements constitute the cornerstone of the value provided by causal inference methodologies. 
It stands to reason that the answer to this fundamental question may be gleaned from examining the challenges that causal inference has faced and partially overcome.

From an applicational standpoint, \cite{scholkopf2021toward} have synthesized the development of current causal models, underscoring the pivotal role of realizing ``causal representations'' to achieve the generalizability of AI-based causal models across different ``levels of knowledge'' learning.
They propose the potential need for a ``new learning paradigm'' - an idea we find both logical and thought-provoking. 
Our current models, ranging from causal to AI, are chiefly based on the assumption of independent and identically distributed (i.i.d.) observations, a paradigm that may be hindering their ability to autonomously realize generalizable causal reasoning. On the other hand, \cite{zhang2012identifiability} points out the ``identifiability difficulty'' when facing nonlinear effects, an inherent obstacle under the i.i.d observational effect setting. 

For clarity, we designate the prevailing paradigm as \emph{\textbf{Observation-Oriented}} modeling. In this study, we propose a novel paradigm, termed \emph{\textbf{Relation-Oriented}} modeling, inspired by the relation-indexing nature of human cognition processes \cite{sep-mental-representation}. Through this new lens, we seek to pinpoint the intrinsic limitations of the existing paradigm.
Accordingly, to validate the proposed new paradigm, it must shed light on the array of questions that have emerged from the outset.
To encapsulate these queries: 
\begin{enumerate}[itemsep=0em, topsep=-1pt, 
parsep=2pt, partopsep=0pt,
leftmargin=22pt, labelwidth=10pt]
    \item[\ding{118}] \emph{Firstly}, causal inference challenges such as confounding effects, dependency on causal assumptions, and interpretative complexities call for a foundational explanation.
    \item[\ding{118}] \emph{Secondly}, To integrate causal reasoning within AI models, we need a nuanced understanding of ``levels of knowledge,'' the essential role of causal representation, its relevance to the difficulty of identifying nonlinear effects, and potential resolutions to these issues.
    \item[\ding{118}] \emph{Thirdly}, in the context of Large Language Models (LLMs), we must discern the distinction between the ``spatial and temporal'' conceptions in language versus causality comprehension, and critically interpret what meta-learning has accomplished in terms of generalizability.
\end{enumerate}

While these questions might seem disparate, they are intrinsically linked to the fundamental requirement by the \emph{Observation-Oriented} paradigm: it necessitates the prior specification of observable entities (including temporal events).
In solely observational learning tasks (like image recognition), these entities serve as the modeling target. In causal relationship learning, they are priorly identified as causes and effects, with their interrelation acting as the primary learning objective.

This fundamental requirement introduces two \emph{\textbf{primary limitations}} in modeling: 1) the inability to account for unobservable relational knowledge, which leads to undetectable hierarchical levels in modeling, and 2) the obligation to assign timestamps to events, potentially causing the overlook of relative timings underpinning structuralized dynamics in our causal knowledge, and essentially introducing inherent biases.

%the temporal dimension from the nonlinear computational analysis, leading to an oversight of dynamics.

% Figure environment removed


%adopts a \emph{\textbf{Relation-Oriented}} perspective, aiming to re-examine the foundational principles behind learning processes.
%Additionally, we introduce the \emph{Relation-Indexed Representation Learning} (RIRL) methodology as an innovative approach to craft AI systems aligning with our intuitive grasp of causality.

This paper consists of four principal parts: 
\begin{enumerate}[topsep=0pt, partopsep=0pt]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{1pt}
    \item {the Introduction, which sets the foundation for the proposed \emph{Relation-Oriented} perspective in section \ref{subsec:ro_view}, and analyze the roles of unobservable relational knowledge in modeling, using an illustrative example to explain its resulting undetectable hierarchy in section \ref{subsec:unobs_relt} (i.e., the limitation \fbox{L1}).}
    \item {Chapter I, including Sections \ref{sec:framework} through \ref{sec:temporal}, establishes the \emph{Relation-Oriented} framework to decompose relationship modeling from a more precise perspective, and through this framework, examines the fundamental impacts of the outlined limitations, and addresses the queries listed above.} 
    \item {Chapter II, from Sections \ref{sec:representation} to \ref{sec:experiment}, introduces the \emph{Relation-Indexed Representation Learning} (RIRL) methodology as a baseline realization of the \emph{Relation-Oriented} paradigm and evaluates the efficacy of relation-indexed autonomous effect identification.}
    \item  {the Conclusion in Section \ref{sec:conclusion} summarizes the insights and findings of this study.}
\end{enumerate}

% In the subsequent of this Introduction, we first define the \emph{Relation-Oriented} perspective (section \ref{subsec:ro_view}), then analyze the roles of unobservable knowledge in modeling, using an illustrative example (section \ref{subsec:unobs_relt}), and ultimately present an overview of the \emph{Observation-Oriented} limitations and their interrelations (section \ref{subsec:limit_oo}).


\subsection{Relation-Oriented Perspective}
\label{subsec:ro_view}
\vspace{-1mm}

Typically, experiments with $n$ trials produce instances $x^n = x_1, \ldots, x_n$ from sequential random variables $X^n = X_1, \ldots, X_n$, which are usually assumed to be independent and identically distributed (i.i.d). 
When these variables evolve over time, $n$ is often replaced by the timestamp $t$ to get temporal sequence $X^t = X_1, \ldots, X_t$, maintaining the i.i.d assumption, and the relationship function is in shape $Y=f(X^t;\theta)$.

In our research, we abandon the i.i.d assumption over $\{X_i \mid i=1,\ldots,t\}$ in the temporal dimension $\textbf{t}$, instead treat their sequence $X^t$ as a single entity, denoted by variable $\mathcal{X} \in \mathbb{R}^{d+1}$, with $d$ representing the observational dimension of each instance $X_i$.
For clarity, we use $X\in \mathbb{R}^d$ to represent a solely observational variable, and let $\mathcal{X}=\langle X,\mathbf{t} \rangle \in \mathbb{R}^{d+1}$ derived by incorporating the $\mathbf{t}$-dimension to encompass features across both observational and temporal dimensions.
It is worth noting that variables such as $\mathcal{X}$ are conventionally referred to as spatial-temporal \cite{andrienko2003exploratory}. However, in this context, ``spatial'' is broadly interpreted to mean ``observational'' and is not restricted to physical spatial data, such as geographic coordinates.

Consider the functional relationship $\mathcal{Y}=f(\mathcal{X};\theta)$, where $\mathcal{Y}=\langle Y,\mathbf{\tau} \rangle \in \mathbb{R}^{b+1}$ with $\mathbf{\tau}$ representing the temporal evolution of $Y\in \mathbb{R}^b$. We employ the Fisher Information $\mathcal{I}_{\mathcal{X}}(\theta)$ \cite{ly2017tutorial} of $\mathcal{X}$ about $\theta$, to define the component of $\mathcal{Y}$ (signified as $\hat{\mathcal{Y}}$) that is sufficiently identified by indexing through $\theta$:
%\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=red!50!black, colback=yellow!10]{
%\hspace{-2mm}
\begin{minipage}{0.97\textwidth}
\paragraph{Definition 1.} the \emph{\underline{Relation-Indexed Representation}} $\hat{\mathcal{Y}_{\theta}}$ in Relationship Modeling.

Let the \emph{\textbf{relation}} $\theta$ adequately represents the influence of $\mathcal{X}$ on $\mathcal{Y}$, denoted as $\mathcal{X} \xrightarrow{\theta} \mathcal{Y}$, then 
$\hat{\mathcal{Y}_{\theta}} = f(\mathcal{X}; \theta)$ represents the \emph{sufficient} component of $\mathcal{Y}$ about $\theta$, which is, $\mathcal{I}_{\hat{\mathcal{Y}_{\theta}}}(\theta) = \operatorname{max} \ \mathcal{I}_{\hat{\mathcal{Y}}}(\theta) = \mathcal{I}_{\mathcal{X}}(\theta)$.

%defining $\hat{\mathcal{Y}_{\theta}}$ as $\mathcal{Y}$'s sufficient component, $= \operatorname{argmax}_{\theta} \mathcal{I}_{\hat{\mathcal{Y}}}(\theta)$
\end{minipage}}
%\vspace{-0.5mm}

Consequently, $\hat{\mathcal{Y}_{\theta}}$ encapsulates the information within $\mathcal{Y}$ that is entirely derived from $\mathcal{X}$, thus defined as the \emph{relation-indexed representation}.
Accordingly, the remaining component of $\mathcal{Y}$, expressed as $\mathcal{Y}-\hat{\mathcal{Y}_{\theta}}$, does not depend on $\theta$. The \emph{Relation-Oriented} perspective focuses on building models by concentrating on $\theta$.

The notation ``$\rightarrow$'' typically denotes causality, although a directional relationship does not always imply causality in logic.
Nonetheless, for clarity, we will adopt terminology consistent with causal inference: for relationship $\mathcal{X} \xrightarrow{\theta} \mathcal{Y}$, we refer to $\mathcal{X}$ as the \emph{cause} and $\mathcal{Y}$ as the \emph{effect}, with a \emph{relation} $\theta$ connecting them. 
Accordingly, the definition of $\hat{\mathcal{Y}_{\theta}}$ is aligned with the ``causal representation'' concept \cite{scholkopf2021toward}.
Crucially, in this research, both \emph{causality} and \emph{correlation} denote types of relationships with a relation $\theta$ (their difference will be discussed later), while \emph{association} (typically nonlinear) refers to statistical dependency between entities without an informative $\theta$, expressed as $(\mathcal{X},\mathcal{Y})$.

%\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.95\textwidth}
\paragraph{Remark 1.} \hspace{-1em} Given $\mathcal{X} \xrightarrow{\theta} \mathcal{Y}$ with \emph{\textbf{observables}} $\mathcal{X}$ and $\mathcal{Y}$, the relationship model $\mathcal{Y}=f(\mathcal{X};\theta)$ becomes \emph{informative} due to the \emph{\textbf{unobservable}} $\theta$.

%\vspace{-0.5mm}
\end{minipage}}
\vspace{-1mm}

The principle outlined in Remark 1 has its origins in the concept of Common Cause \cite{dawid1979conditional, scholkopf2021toward}, suggesting that any nontrivial (i.e., informative) conditional independence between two observables requires a third, mutual cause (i.e., the unobservable ``relation'' in our context). %It means that an informative $\theta$ differentiates a causal relationship $\mathcal{X} \xrightarrow{\theta} \mathcal{Y}$ from a mere statistical dependency $(\mathcal{Y} \mid \mathcal{X})$.

$\mathcal{X}$ and $\mathcal{Y}$ can be either solely observational entities, equal to $X$ and $Y$ (e.g., images, spatial coordinates of a quadrotor, etc.), or observational-temporal entities (e.g., trends of stocks, a quadrotor's trajectory, etc.). 
Regardless of their characterization, the primary goal of utilizing the function $\mathcal{Y}=f(\mathcal{X};\theta)$ is to encapsulate the unobservable relational knowledge represented by $\theta$, rather than merely associative distribution  $(\mathcal{X},\mathcal{Y})$.


To clarify the concept of informative $\theta$, let's consider a simple example. In the relationship ``Bob (represented as $X$) has a son named Jim (represented as $Y$)'', the father-son relation information $\mathcal{I}(\theta)$ between them is evident to human cognition but unobservable to AI provided sufficiently observed social activities. Also, $\theta$ can be seen as the common cause of $X$ and $Y$ that makes their connection unique, rather than any random pairing of ``Bob'' and ``Jim''. 
Through the observational data, AI might deduce a particular associative pattern over $(X, Y)$, but cannot internalize the unobservable information $\mathcal{I}(\theta)$ between them. 

Drawing on the symbolization provided in Definition 1, a comprehensive \emph{Relation-Oriented} framework is introduced in Section \ref{sec:framework}, offering more complete insights into the modeling of causal relationships.

% In this framework, we will particularly define the \emph{temporal} feature space, distinguishing it from the \emph{observational} feature space. 
% This will enable us to reexamine the limitations in queries from a novel perspective.

\subsection{Unobservable Relational Knowledge}
\label{subsec:unobs_relt}
%\vspace{-1mm}

Unobservable knowledge may not directly serve as the learning objective relation $\theta$, but it can still be relative to and profoundly impact the modeling process. We elucidate this with the following example:
It is notable that on social media, AI-created personas can have realistic faces but seldom showcase hands. This is because AI for visual tasks struggles with the intricate structure of hands, instead treating them as arbitrary assortments of finger-like items.
Figure \ref{fig:hand}(a) provides AI-created hands with faithful color but unrealistic shapes, while humans can effortlessly discern hand gestures from the grayscale sketches in (b).

Humans intuitively employ informative relations as the \emph{\textbf{indices}}, guiding us to specific mental representations \cite{sep-mental-representation}. As illustrated in Figure \ref{fig:hand}(b), our cognition operates hierarchically, progressing through a series of relations, denoted as $\theta=\{\theta_i,\theta_{ii},\theta_{iii}\}$.
Each higher-level understanding builds upon conclusions drawn at preceding levels. Specifically, Level $\mathbf{I}$ identifies individual fingers; Level $\mathbf{II}$ distinguishes gestures based on the positions of the identified fingers, incorporating additional information from our understanding of how fingers are arranged to constitute a hand, denoted by $\omega_i$; and Level $\mathbf{III}$ grasps the meanings of these gestures from memory, given additional information $\omega_{ii}$ from knowledge.


% Figure environment removed


%To AI, or hypothetical extraterrestrial life unfamiliar with our knowledge, hands in Figure \ref{fig:hand}(a) may appear reasonable.
% AI can successfully differentiate non-overlapping features at various levels. 
%, while similar hand gestures may confuse it. 


Typically, these visual learning tasks do not aim to model relations, neither $\theta$ nor $\omega$. Instead, they focus on capturing observational entities (pertaining solely to $X$). 
Without relation-indexing through $\theta$, AI systems may not distinguish entities across different levels but only capture their associative dependence, like $(X_{II}\mid X_I)$ and $(X_{III}\mid X_I, X_{II})$, without deeper, informative insights into $\omega$.

However, for such solely observational learning, the hidden  $\omega$ may not always be essential. 
If entities across levels are observationally distinct and non-overlapping, AI can accurately differentiate them. For instance, AI can generate convincing faces because the appearance of eyes strongly indicates facial angle, removing the need to distinguish ``eyes'' = $X_{II}$ from ``faces'' = $X_I$. Additionally, based on fully-captured levels, AI can inversely uncover the hidden $\omega$ using methods such as reinforcement learning \cite{sutton2018reinforcement, arora2021survey} - In this case, approvals of generated five-fingered hands may lead AI to identify fingers autonomously.

\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=red!50!black, colback=yellow!10]{
%\hspace{-2mm}
\begin{minipage}{0.95\textwidth}
\paragraph{Definition 2.}  \hspace{-1em} \emph{\underline{Hidden Relation} $\omega$} and its resulting \emph{\underline{Undetectable Hierarchy}}.

Different from the \emph{\textbf{indexing} relation} $\theta$, the \emph{\textbf{hidden} relation} $\omega$ can constitute \emph{undetectable hierarchical levels} of knowledge, requiring model \emph{generalizable} to be effective across.
\end{minipage}}

A \emph{generalizable} model enables the learned lower-level relationships to be reusable for higher-level learning tasks \cite{scholkopf2021toward}, which mirrors our inherent capability to generalize knowledge in cognition. For example, our ability to identify fingers can be applied regardless of the types of medium, like images, photos, or videos. Conversely, generalizability also denotes the capacity to \emph{individualize} from higher to lower levels, accommodating different $\omega$ values.

The illustration in Figure \ref{fig:hand} highlights the distinct roles of unobservable relations ($\theta$ and $\omega$) in modeling. Our central concern, however, is modeling relationships with $\theta$ as the primary \emph{\textbf{objective} relation} for learning. In this context, $\omega$ stratifies unobservable $\theta$ into hierarchical levels, culminating in a completely imperceptible joint distribution of $(\theta,\omega)$, which precludes methods like inverse reinforcement learning.


For instance, consider family incomes $X$ influence grocery shopping frequencies $Y$ through relation $\theta$. 
Here, the cultural background $\omega$ emerges as an important factor, such that an effective model $Y=f(X;\theta)$ has to be individualizable, i.e., conditioned on a specific country (represented by a particular $\omega$ value) to ensure practical utility.
On the opposite, a generalization would imply $\omega=\varnothing$.

For the sake of clarity, hereafter in this paper, unless explicitly stated otherwise, the hidden relation $\omega$ represents two hierarchical modeling levels: the generalized level $X_o\xrightarrow{\theta_o} Y_o$ with $\theta_o$ implying $\omega=\varnothing$, and the individualized level $X_{\omega}\xrightarrow{\theta_{\omega}} Y_{\omega}$ given $\theta_o$ with a specific $\omega$ value, collectively notated as $(\theta, \omega) = \begin{pmatrix} \theta_o \\ \theta_{\omega} \end{pmatrix}$.

% \subsection{Limitations of Observation-Oriented Modeling}
% \label{subsec:limit_oo}


% Figure \ref{fig:limit} summarizes the intrinsic limitations of \emph{Observation-Oriented} modeling, with detailed explanations divided into three Sections.
% Labels \fbox{L1} and \fbox{L2} correspond to the two \emph{primary limitations} previously discussed. For \fbox{L1}, as highlighted in section \ref{subsec:unobs_relt}, the hidden relation $\omega$ can lead to undetectable hierarchical levels. For models focusing on observational-only entities, it can lead to visually noticeable AI alignment issues \cite{christian2020alignment} like the case in Figure~\ref{fig:hand}~(a). 
% Furthermore, in relationship modeling, this oversight can potentially reveal more complex intricacies, such as how \fbox{L1} and \fcolorbox{black}{orange!30}{L5} together contribute to \fcolorbox{black}{orange!30}{L6}.

% In our cognitive processes to understand the relationship $\mathcal{X} \xrightarrow{\theta} \mathcal{Y}$, effect $\mathcal{Y}$ can be autonomously indexed and identified through $\theta$ from $\mathcal{X}$. However, in \emph{Observation-Oriented} modeling, due to \fbox{L2}, only the observational $Y$ can be initially recognized without its dynamics (labeled as \fcolorbox{black}{myblue!23}{L\ref{sec:causality}}). Even if using a sequential effect $Y^{\tau}$, the model can only realize dynamics of the cause $\mathcal{X}$ at best, but fail to capture nonlinearities within the $\tau$ dimension.
% This oversight results in \fcolorbox{black}{myblue!23}{L\ref{subsec:caus_imbalance}}, an imbalance in capturing observable dynamics between cause and effect in the model, and \fcolorbox{black}{myblue!23}{L\ref{subsec:caus_identif}}, the challenged causal effect identification. 

% % Figure environment removed

% The overlooked dynamics of effects serve as a key factor preventing AI systems from achieving autonomous causal reasoning (see Section~\ref{sec:causality}).
% The combination of \fcolorbox{black}{myblue!23}{L\ref{sec:causality}} and \fbox{L1} leads to challenges in traditional causal inference. These challenges are labeled as \fcolorbox{black}{mygreen!20}{L\ref{sec:application}} and analyzed in Section \ref{sec:application} through practical examples. 

% AI models are often favored for addressing large-scale questions, such as learning \emph{structural causal relationships} among numerous variables. However, in this context, the overlooked effect dynamics may lie in multi-dimensional temporal feature space (see section \ref{subsec:framework_time} for the concept). This oversight, labeled as \fcolorbox{black}{orange!30}{L5}, can introduce \emph{inherent biases} into structural causal models, labeled as \fcolorbox{black}{orange!30}{L6}. Such biases significantly reduce the models' generalizability and robustness, contributing another critical factor that impedes the realization of causal reasoning in AI (see Section \ref{sec:temporal}).

% delves into the underlying structure, proposing an enhancement to conventional Directed Acyclic Graphs (DAGs) to identify dynamic effects. 
% These dynamics, when interacting across the overlooked multiple logical timelines, combined with \fbox{L1}, can introduce \emph{inherent biases} into structural causal models, labeled as \fcolorbox{black}{orange!30}{L6}. Such biases are significant contributors to the reduction of the models' generalizability and robustness.

%Owing to \fbox{L2}, all potential events within the structural relationship must be specified solely along the observational timeline, denoted by $t$ or $\tau$. 

%For \fcolorbox{black}{myblue!23}{L5}, while incorporating hidden confounders might improve individual model interpretations, it does not bolster structural learning.
%Furthermore, a complex causality structure may introduce ``relative timelines'' (see subsection \ref{subsec:framework_time}), leading to additional temporal dimensions, distinct from the observable $\tau$-timeline.
%Furthermore, when incorporating complex causal relationship structures, ``relative timelines'' may be introduced (see section \ref{subsec:framework_time} for details), but inherently overlooked by \emph{Observation-Oriented} modeling. This oversight, combined with \fbox{L1} and \fbox{L2}, may inherently hinder our progress toward AGI, where the challenges are encapsulated as \fcolorbox{black}{mygreen!23}{L6} and \fcolorbox{black}{mygreen!23}{L7}, and elaborated in Section \ref{sec:temporal}.


\begin{center}
   {\vskip 8pt\large\bf Chapter I: Limitations of Current Observation-Oriented Paradigm} 
\end{center}

The prevalent \emph{Observation-Oriented} modeling paradigm inherently misaligns with the relation-centric human comprehension \cite{sep-mental-representation}. This misalignment may not have been critical in the past. In traditional causal inference, challenges could be addressed through intended adjustments due to the limited scale of questions. Nonetheless, with the advancements in AI-based large models, the consequences of this misalignment have become increasingly significant across various applications.


Section \ref{sec:framework} establishes a \emph{Relation-Oriented} dimensionality framework to symbolize causal relationship models; through which, we recognize the critical role of relative timings (highlighted as limitation \fbox{L2}), and explore the essence of dynamical generalizability for a structuralized relationship model. Subsequently, Section \ref{sec:causality} delves into the critical implications of the frequently overlooked effect dynamics (the secondary impact of \fbox{L2}), and accordingly reevaluates present causal learning challenges based on the new framework. Lastly, Section \ref{sec:temporal} elucidates the inherent biases that \emph{Observation-Oriented} causal models essentially introduced into structural causal relationship learning (the primary impact of \fbox{L2}).


% To complete the proposed \emph{Relation-Oriented} paradigm, a comprehensive framework for symbolizing relationships is first introduced in Section \ref{sec:framework}, to provide a foundational perspective for the subsequent sections. 
% Specifically, we enhance conventional Directed Acyclic Graphs (DAGs) to distinguish the dynamics of effects. Through this enhancement, challenges in causal inference are reexamined from a novel \emph{Relation-Oriented} perspective, as detailed in Section~\ref{sec:application}.

 
% Human understanding inherently indexes through relations \cite{sep-mental-representation}, directing to mental representations 
% about observational and temporal entities. 
% This intrinsic characteristic results in a fundamental misalignment with the \emph{Observation-Oriented} modeling paradigm, evident through various application issues.

% Section \ref{sec:framework} establishes the framework for decomposing relationships into symbolic representations, a foundation of subsequent discussions. In Section \ref{sec:causality}, detailed examples are employed to examine the roles of nonlinear temporal dynamics in causal relationship modeling.
% Subsequently, Section \ref{sec:temporal} underscores the significance of relative timelines in structural causal learning and its influence on model generalizability.

%this discrepancy 

\section{Relation-Oriented Dimensionality Framework}
\label{sec:framework}

In the fervent debates surrounding AGI, a pivotal question persists: Can conceptualized symbols and AI systems grounded in symbolism truly embody human-like understanding in empirical inquiries \cite{newell2007computer, pavlick2023symbols}? We propose that the key lies in representing unobservable elements in knowledge, such as abstractly meaningful relations. These elements are vital for the informativeness of our causal reasoning. The modeling process, by indexing through certain relations, has the potential to mirror logical deductions and symbolize cognitive concepts, ultimately encapsulating their representations. Such representations, stored and used by AI systems, are generalizable as they align with causal knowledge.   

% By directing the learning objective towards extracting these elements from observable data, the symbolized computation within the model remains informative. 
% Such captured relational information has the potential to mirror our cognitive processes of logical deduction.

By Definitions 1 and 2, representing a directional relationship in modeling necessitates two types of variables: the observables $\{\mathcal{X}, \mathcal{Y}\}$, and the unobservables $\theta$ and $\omega$. As specified, $\mathcal{X}$ and $\mathcal{Y}$ include both \emph{observational} and \emph{temporal} features. In response, we adopt the concept of a \emph{hyper-dimension} to integrate these unobservable features. Consequently, we establish a framework, as illustrated in Figure~\ref{fig:space}, to represent relationships as joint distributions across three distinct types of dimensions.
For clarity, ``feature'' refers to the potential variable fully representing a certain distribution of interest. 

Figure~\ref{fig:space} aims to decompose our cognitive space where relational knowledge is stored. The hyper-dimensional space $\mathbb{R}^H$ is constructed by aggregating all \emph{\textbf{unobservable}} relations in our knowledge, such as $(\theta,\omega)\in \mathbb{R}^H$. Conversely, the observational-temporal joint space, $\mathbb{R}^O \cup \mathbb{R}^T$, is considered as the \emph{\textbf{observable}} space. In both $\mathbb{R}^O$ and $\mathbb{R}^T$, a temporal dimension consistently signifies the evolution of timing but represents distinct concepts, as outlined in section \ref{subsec:framework_time}. 
Within such a dimension, linear and nonlinear distributions correspond to \emph{static} and \emph{dynamical} features, respectively, a distinction further explained in section \ref{subsec:framework_obs}.


%For a model to be practically valuable, it must accurately reflect our understanding. Similarly, a successful AGI, to meet our expectations, must be rooted in existing knowledge. Specifically, it should represent relations residing in the unobservable Hyper-dimensional Space, through which reasonable interpretations of observable entities can be generated.


\vspace{0.5mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=red!50!black, colback=yellow!10]{
\hspace{-1mm}\begin{minipage}{0.96\textwidth}
\paragraph{Definition 3.} The \emph{\underline{Relationship Symbolization}} within the proposed Dimensionality Framework.

For the relationship $\mathcal{X}\xrightarrow{\vartheta}\mathcal{Y}$, where $\{\mathcal{X},\mathcal{Y}\}\in \mathbb{R}^O$ and $\vartheta\in \mathbb{R}^T \cup \mathbb{R}^H$, the \emph{\textbf{structuralized} relation} $\vartheta$ can be decomposed as: \vspace{0.5mm} 

\centering
$\vartheta = \overrightarrow{\theta^1\ldots\theta^T}$, where $(\theta^i, \theta^j)\in \mathbb{R}^H$ for any $i \neq j \in \{1,\ldots,T\}$. Accordingly, \vspace{1mm} 

\centering
$(\vartheta, \omega) = \begin{pmatrix} \vartheta_o \\ \vartheta_{\omega} \end{pmatrix} = \begin{pmatrix} \theta^1_o & \ldots & \theta^T_o \\ \theta^1_{\omega} & \ldots & \theta^T_{\omega} \end{pmatrix}$ with any $
 (\theta^i_o,\theta^j_o)\in \mathbb{R}^H$ and $(\theta^i_{\omega}, \theta^j_{\omega})\in \mathbb{R}^H$.

\end{minipage}}
%\vspace{-0.5mm}

% Figure environment removed
%\vspace{-1mm}


\vspace{-1mm}
\subsection{Absolute Timing vs. Relative Timings}
\label{subsec:framework_time}
\vspace{-1mm}

In time series data, the attribute recording observed timestamps, denoted by $t$, typically reflect the \emph{absolute} timing of reality. However, from a modeling standpoint, the $t$ values are indistinguishable from values of other attributes. Therefore, in Figure~\ref{fig:space}, the absolute timing $\mathbf{t}$ serves as a standard dimension within the observational space $\mathbb{R}^O$, along which, both $\mathcal{X}$ and $\mathcal{Y}$ are invariably observed as data sequences $X^t$ and $Y^t$.

In our cognitive space, \emph{\textbf{relative}} timings inherently exist \cite{wulf1994reducing}, supporting structuralized relationships beyond the mere \emph{\textbf{absolute}} timing $\mathbf{t}$. We thereby designate a distinct ``temporal space'' $\mathbb{R}^T$, composed of $T$ relative timings as axes, forming $T$ cognitive \emph{timelines}, to integrate temporal features aligned with our relational knowledge \cite{shea2001effects}.
Instead of treating $\{\mathcal{X}, \mathcal{Y}\} \in \mathbb{R}^O$ as individual variables, under $\vartheta$ (as per Definition 3), they are jointly distributed across $\mathbb{R}^O$ and $\mathbb{R}^T$, represented as $(\mathcal{X}, \mathcal{Y} \mid \vartheta) \in \mathbb{R}^O \cup \mathbb{R}^T$. 

$\vartheta$ can span up to $T$ timing dimensions in $\mathbb{R}^T$, with the effect $\mathcal{Y}=\sum_{i=1}^{T} \hat{\mathcal{Y}^i}$ decomposed into $T$ components, each residing in a distinct timing. Crucially, defining $\vartheta$ as a ``structuralized'' relation not only recognizes its multi-dimensionality but also highlights the potential distributional dependence (typically nonlinear) among these dimensions, represented by $(\theta^i, \theta^j)\in \mathbb{R}^H$ in Definition 3, equating to $(\hat{\mathcal{Y}^i}, \hat{\mathcal{Y}^j})\in \mathbb{R}^O\cup \mathbb{R}^T$. We term these nonlinear temporal dependences as \emph{\textbf{dynamical interactions}} for clarity, which necessitate the establishment of $\mathbb{R}^T$ as a distinct ``space'', rather than mere temporal dimensions within $\mathbb{R}^O$.
%further detailed in section \ref{subsec:framework_info}

%which serve as distinct \emph{timelines} in our cognitive framework to house that structuralized relational knowledge.

%treating $T$ relative timings as $T$ dimensions. 

%Whereas, when $\{\mathcal{X},\mathcal{Y}\}$ are considered alongside $\vartheta$ within the joint space $\mathbb{R}^O \cup \mathbb{R}^T$, 

For instance, patients' vital signs are recorded daily in a hospital with \emph{absolute} chronological timestamps. However, to assess a medical intervention $\mathcal{Y}$, a uniform series of post-medication events must be selected, for example, spanning from the day after medication to the 30th day. This creates a timeline represented by the axis ticked as $[1,30]$ to denote the \emph{relative} timing, regardless of \emph{absolute} timestamps of the selected records.
Yet, if the intervention involves two distinct aspects, such as the primary effect $\hat{\mathcal{Y}^1}$ and the side effect $\hat{\mathcal{Y}^2}$, and their mutual influences are of interest, then two separate relative timings, $\mathbf{t_1}$ and $\mathbf{t_2}$, must be considered for their individual evolutions, even though both may be labeled as $[1,30]$.
%assigning absolute timestamps for their identification will risk overlooking their unique dynamic evolutions.


%\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.95\textwidth}
\paragraph{Remark 2.} \hspace{-1em}
Although $\mathcal{Y}\in \mathbb{R}^O$ is invariably observed as a sequence along the \emph{absolute} timing $\mathbf{t}$, it may represent an \emph{underlying structure} determined by $\mathcal{X}\xrightarrow{\vartheta}\mathcal{Y}$, spinning the multi-dimensional $\mathbb{R}^T$.
%\vspace{-0.5mm}
\end{minipage}}
%\vspace{-1mm}
% The \emph{temporal dimension} in a causal relationship is not limited to a single dimension but can include multiple timing axes (i.e., timelines) to form a multi-dimensional \emph{temporal space}.

Conventionally, the concept of ``temporal dimension'' is often simplified as the single absolute timing $\mathbf{t}$, evident from the traditional ``spatial-temporal'' analysis \cite{alkon1988spatial, turner1990spatial, andrienko2003exploratory}, to recent advancements in language models \cite{gurnee2023language}. 
However, as emphasized in Remark 2, our cognitive perception of ``time'' is more complex, which forms the foundation of our causal knowledge \cite{coulson2009understanding}.

To provide an intuitive insight into the implications of neglecting $\mathbb{R}^T$, let's consider an analogy:
Imagine ants dwelling on a floor's two-dimensional plane. To predict risks, the scientists among them create two-dimensional models and instinctively adopt the nearest tree as a height reference. They noticed increased disruptions at the tree's first branch, which indeed correlates to the children's heights, given their curiosity.
However, without understanding humans as three-dimensional beings, they can only interpret it by adhering to the first branch.
One day, after relocating to another tree with a lower height, the ants found the risk presenting at the second branch instead, making their model ineffective. They may conclude that human behaviors are too complex, highlighting the model generalizability issue.
%Similarly, when we specify a single, absolute timeline for all potential events, this timeline becomes our ``tree'', which may introduce inherent modeling biases, affecting the robustness and generalizability of our models.

As three-dimensional beings, we inherently lack the capacity to fully integrate the fourth dimension - time - into visual perception. Instead, we conceptualize ``space'' in three dimensions to incorporate features of the temporal dimension along a timeline within the space, analogous to our ``tree''.
Yet, ants do not need to fully comprehend the three-dimensional world to build a generalizable model; instead, they need only recognize the existence of the ``forest'', which consists of all ``possible trees'' with relatively different branch locations. 
Similarly, in our modeling, we must include the $\mathbb{R}^T$ space, composed of all potential relative timings within our knowledge, although they cannot be directly observed.

%\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.95\textwidth}
\paragraph{Remark 3.} 
\hspace{-3mm}
\emph{Counterfactuals} can be viewed as posterior distributions in $\mathbb{R}^O\cup \mathbb{R}^T$ given priors in $\mathbb{R}^O$.

\vspace{-0.5mm}
\end{minipage}}
\vspace{-1mm}

The ability to address counterfactual queries, such as ``what effect would be if the cause were changed'', is a crucial aspect of causal models' significance \cite{scholkopf2021toward}. A separate cognitive $\mathbb{R}^T$ space allows a more intuitive interpretation of counterfactuals as conditional distributions, which might offer valuable insights in fields like quantum computing.
In particular, the observed prior conditions can be viewed as features within $\mathbb{R}^O$; then, all subsequent possibilities can be collectively considered 
as a distribution across $\mathbb{R}^T$. %This perspective might potentially offer valuable insights in fields like quantum computing.

%- It might offer an intriguing insight into quantum computing (need further exploration).
%It might provide an intriguing insight that could potentially inspire advances in quantum computing (further exploration required).





% Under the \emph{Observation-Oriented} paradigm, prior identification of effects for dynamics is notably difficult for further discussions). This neglect of temporal nonlinearity and oversight of relative timelines can lead to \emph{\textbf{inherent bias}}, thereby compromising the generalizability of causal models. While such misalignments might have been subtle in the past, the advent of AI enables large-scale models more efficiently, and its black-box nature allows these biases to accumulate exponentially inside, eventually resulting in uninterpretable outputs.
% %indispensable 
% %accentuated
% %disparity 


\subsection{Dynamical vs. Sequential Static}
\label{subsec:framework_obs}
\vspace{-1mm}



The distributions along a dimension can be broadly classified into \emph{linear} and \emph{nonlinear} categories. Within the temporal dimension, these correspond to \emph{\textbf{static}} and \emph{\textbf{dynamical}} temporal features, respectively, and can be represented by corresponding variables. 
Static features are typically linked to specific timestamps. For instance, consider the statement ``rain leads to wet floors''; here ``wet floors'' represents a state that can be identified at a particular point in time. Therefore, it can be denoted as a static variable $X_t$ with a specified timestamp $t$. 
In contrast, the expression ``floors becoming progressively wetter'' necessitates a representation that captures the temporal distribution, to account for changes over time, like $X^t=X_1, \ldots, X_t$. However, this raises the question: Is $X^t$ a dynamical variable or a sequence of static variables?

Within the current machine learning paradigm, the distinction between ``static'' and ``dynamical'' is typically made between ``models'' instead of ``variables'' \cite{static-dynamic}, which refers to whether time is a factor in the model's equations.
However, this essentially requires the function $f(X^t;\theta)$ to represent the \emph{dynamics of effect}, inherently encompassed by $\mathcal{Y}$. As a result, the model selection for $f(;\theta)$, as well as the identification of a \emph{static} outcome $Y_{t+1}$, become crucial in determining how much effect dynamics can be captured \cite{weinberger2022static}, or potentially neglected, which will be discussed in detail in Section~\ref{sec:causality}.

\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=red!50!black, colback=yellow!10]{
%\hspace{-2mm}
\begin{minipage}{0.95\textwidth}
\paragraph{Definition 4.}  \hspace{-1em} A \emph{\underline{Dynamical Outcome}} $\mathcal{Y}$ compared to a sequential static $Y^{\tau}$.

As a \emph{dynamical} variable, $\mathcal{Y}=\langle Y, \tau \rangle\in \mathbb{R}^O$ permits \emph{\textbf{nonlinear} computational freedom} over $\tau$, whereas a \emph{sequential static} variable $Y^{\tau}\in \mathbb{R}^{O-1}$ assumes i.i.d or \emph{\textbf{linear}} changes along $\tau$. They samely appear to be sequential instances $y^{\tau}= y_1,\ldots,y_{\tau}$, while the dynamical significance of $\mathcal{Y}$ is model-dependent.
\end{minipage}}

Definition 4 is based on the proposed \emph{Relation-Oriented} paradigm, wherein the relation $\theta \in \mathbb{R}^H$ and the outcome $\mathcal{Y}\in \mathbb{R}^O\cup \mathbb{R}^T$ are considered individually. Here, $\theta$ represents certain unobservable information within $\mathbb{R}^H$, lacking an explicit distributional representation.
This allows $\mathcal{Y}$ to be an individual variable that encompasses the dynamical effects caused by $\mathcal{X}$.
Similarly, the cause $\mathcal{X}=\langle X,t\rangle\in \mathbb{R}^O$ can also be a dynamical variable, depending on specific models. For example, RNN models typically formulate $Y_{t+1}=f(\mathcal{X};\theta)$ with a dynamical cause represented by latent space features, but remaining the outcome static.

%This is reasonable since the significance of temporal distributions depends on specific modeling demands, while an individual variable can only be characterized as incorporating $\textbf{t}$ as a potential computational dimension.
Accordingly, the statement ``floors becoming progressively wetter'' can be roughly considered as ``linearly increasing from $0\%$ to $100\%$ in 10 minutes'' to be a sequential static feature. It can also be depicted as a continuous nonlinear distribution, a dynamical feature for finer granularity. The latter can cover variances in the former, such as varying progression speeds, which the former cannot. In essence, implementing dynamical variables is crucial for achieving model generalizability across temporal dimensions.


%is static or dynamical hinges on whether the changes from $X_t$ to $Y_{t+1}$ are confined to be linear, as represented by $\theta$. Conventionally, $\theta$ and $Y$ are often considered together, with $\theta$ representing all potential static and dynamical changes in a hybrid \cite{weinberger2022static}, while the observational $Y_{t+1}$ alone displays the resultant static outcome at a specific timestamp.




% %\vspace{1mm}
% \noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.95\textwidth}
% \paragraph{Remark 4.} 
% \hspace{-3mm}
% Regardless of the dynamics represented by $\mathcal{X}$ or the sequential static features by $X^t$, their produced instances consistently manifest as observational sequences $x^t=(x_1,\ldots,x_t)$ in data, where the dynamical significance of $\mathcal{X}$ is model-dependent.

% \vspace{-0.5mm}
% \end{minipage}}
% \vspace{-1mm}


% $\mathcal{X}=\langle X,t \rangle$. 
% The dynamical significance of $\mathcal{X}$, however, can vary depending on the models employed. 
% In the \emph{observable data space}, $\mathcal{X}$ consistently manifests as a data sequence $X^t = (X_1, \ldots, X_t)$.
% If its temporal value changes can be fully described by a linear function, such as $X_{t+1}=2*X_t$, then it simply acts as ``a sequence of static variables''.
% Conversely, if the sequence $(X_1, \ldots, X_t)$ represents a distribution with significant $t$ dimensions, then $\mathcal{X}$ qualifies as a ``dynamical variable''.
% %where each static instance is a snapshot at a distinct point in time.

% For causal relationship $\mathcal{X} \xrightarrow{\theta} \mathcal{Y}$, the $t$ dimension for $\mathcal{X}$ and the $\tau$ dimension for $\mathcal{Y}$ may not be the same timeline in logic.
% Therefore, even if the applied model has computational freedom in the temporal dimension, only one of them, typically $t$, for the cause, can be incorporated as a computational dimension.


% Figure environment removed
\vspace{-2mm}

When considering a structuralized relation $\vartheta$, a valid generalization process requires the model to remain effective over all temporal dimensions at any level, no matter for the absolute timing within $\mathbb{R}^O$, or the relative timings in cognitive $\mathbb{R}^T$. 
In our cognition, the generalized causal knowledge ($\omega=\varnothing$) can be instinctively extracted from individualized varied scenarios (with varying $\omega$ values). However, the undetectability of $(\vartheta, \omega)$ implies our models cannot autonomously fulfill this process, irrespective of whether they are AI-based.

%When constructing knowledge, humans' cognition instinctively extracts general information ($\omega=\varnothing$) from various scenarios and, when applying this knowledge, individualizes it by adapting to specific scenarios (with varying $\omega$ values).
%Accordingly, it is unsurprising that the causal DAGs in our cognitive framework represent generalized knowledge only, and necessitate the generalizability of the causal models, to remain temporally multi-dimensional effective across the hierarchical levels. 

Figure~\ref{fig:logical} showcases models' and humans' perspectives, distinguished as the ``Observed-View'' and ``Logic-View''. (a) and (b) compare a simple dynamical distribution within $\mathbb{R}^O$, while (c) and (d) display a DAG structure across two relative timings in $\mathbb{R}^T$, which exhibits a typical \emph{dynamical confounding} scenario. 
In (c), the static instances $y_A^1$ and $y_B^1$ indicate that the two individualized dynamical effects $\mathcal{Y}_A$ and $\mathcal{Y}_B$ reach the same status value $y^1$ in dimension $\mathbf{t_1}$, signifying that they attain an equivalent magnitude; this is similarly observed in another timing dimension $\mathbf{t_2}$. Notably, the influence from $y^1$ to $y^2$ may highlight a \emph{dynamical interaction} between the effect components $\hat{\mathcal{Y}^1}$ and $\hat{\mathcal{Y}^2}$, rather than a mere linear dependence. 
%static influence.

\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=red!50!black, colback=yellow!10]{
%\hspace{-2mm}
\begin{minipage}{0.95\textwidth}
\paragraph{Definition 5.}  \hspace{-1em} The \emph{\underline{Dynamical Interaction Confounding}} Phenomenon. 

For relationship $\mathcal{X}\xrightarrow{\vartheta} \mathcal{Y}$, when effect $\mathcal{Y}$ encompasses multiple dynamics over distinct relative timings, \emph{dynamical interactions} among them can lead to \emph{dynamical confounding} within the structuralized $\mathcal{Y}$.
\end{minipage}}

% %\vspace{1mm}
% \noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.95\textwidth}
% \paragraph{Remark 4.} 
% \hspace{-3mm}
% The generalization process of a structuralized dynamical effect $\mathcal{Y}$ can be geometrically viewed as a linear transformation of causal DAG within the Observed-View space.

% \vspace{-0.5mm}
% \end{minipage}}
% %\vspace{-1mm}


%\vspace{-1mm}
\subsection{Informative Hyper-Dimensional Space}
\label{subsec:framework_info}
\vspace{-1mm}




%As a result, given the relationship $\mathcal{X}\xrightarrow{\vartheta}\mathcal{Y}$; they may be interacted, forming a structural relationship within $\mathbb{R}^O \cup \mathbb{R}^T$. Although the underlying structure is not directly observable, collectively assuming all featured dynamical events confined within $\mathbb{R}^O$ along the single absolute timing $\textbf{t}$ can be problematic.

% Consequently, when considering an observational-temporal effect variable $\mathcal{Y}$ that includes a timing dimension $\tau$, it is not necessary for $\tau$ to be strictly absolute or relative; it may even represent multiple underlying timing dimensions depending on the specific context.


% Most relationship models function within the observational space, maybe incorporating a timeline to depict the observational evolution over time. Consider the following examples: 1) CNNs grasp pixel associations only in the observational space, 2) A quadrotor's trajectory can be identified as a sequence of spatial coordinates, 3) Large Language Models (LLMs) operate along a semantically meaningful timeline representing the word order, 4) The vital signs of patients are recorded chronologically. 

%  in our cognition across the hyper-dimensional and cognitive temporal spaces. Therefore, the causal reasoning AGI that we envisage can be defined as a system fully encompassing unobservable information denoted by $(\vartheta,\omega)$, , to realize the generalizability of structural models across different levels of knowledge.

In summary, the human-like causal reasoning can be represented as $(\vartheta,\omega)\in \mathbb{R}^T \cup \mathbb{R}^H$. Accordingly, AGI that meets our expectations should adequately encapsulate informative $\vartheta$ and $\omega$. Here, $\vartheta\in \mathbb{R}^T \cup \mathbb{R}^H$ denotes the structuralized causality within our knowledge, while $\omega\in \mathbb{R}^H$ indicates the ability to capture nonlinearities in all dimensions (including temporal dynamics), to achieve model generalizability.

Figure~\ref{fig:overview} provides a fundamental overview of the prevailing relationship modeling methods, highlighting their primary limitations, as briefly summarized in Figure~\ref{fig:limit}. Within this context, $\vartheta_{\omega}$ is used to represent generalizable causal structures in AGI, and we identify the two major obstacles in our pursuit of it.

% The first obstacle arises from the hidden relation $\omega$ (i.e., limitation \fbox{L1}), which limits the ability of the knowledge-driven Logic-View methods (like causal inference-based ones) to generalize models for Observed-View scenarios. 
% The second relates to the cognitive relative timings foundational to our causal knowledge, encompassing structured dynamical effects; these timings are often overlooked by data-driven AI-based methods (i.e., limitation \fbox{L2}), impeding their capacity for causal reasoning.

% Figure environment removed
\vspace{-1mm}


Regular causal models derive the functional parameter $\theta$ from the correlation between cause and effect events that are priorly identified by absolute timestamps. 
Notably, Granger causality \cite{granger1993modelling}, a method well-regarded in economics \cite{maziarz2015review}, introduces separate temporal sequences for cause ($X^t$) and effect ($Y^{\tau}$), suggesting the allowance for multiple timings. 
However, the significance of recognizing temporal dimensions lies in capturing their featured dynamical evolutions. Without nonlinear computations, 
distinguishing between $\textbf{t}$ and $\tau$ for static sequential timestamps offers limited meaning.
%for their \emph{nonlinear independence}, as this enables the representation of coexisting, varied temporal dynamics, particularly in the effects.
%their nonlinear independence, allowing for relatively distinct and diverse temporal dynamics. 

Likewise, causal inference often omits explicit relative-timing axes in causal DAGs due to the typical exclusion of nonlinear dynamics in modeling, as depicted in Figure~\ref{fig:logical} (d).
While inherently adopting a \emph{Relation-Oriented} perspective based on the Logic-View knowledge ($\vartheta_o$), it tends to overlook the Observed-View with varied $\omega$, thus failing to visualize the model generalization needs.
%the failure to address nonlinearities limits model generalizability in any dimension to adapt with varied $\omega$. 
%Therefore, a preprocessing adjustment is often required, to manually transform a causal DAG (denoted by $\vartheta_o$) into a practical model.
To address this, we suggest enhancing conventional DAGs to illustrate dynamical variations across relative timings, as further detailed in Section~\ref{sec:temporal}.

Unsurprisingly, AI-based RNNs are increasingly favored in modern relationship learning \cite{xu2020multivariate}, considering their proficiency in handling nonlinear causes. RNNs transform the observational sequence $X^t$ into a feature representation in latent space, enabling nonlinear computation over $\textbf{t}$ to effectively implement dynamical $\mathcal{X}$. However, potential dynamics of the effect $\mathcal{Y}$ are often overlooked, resulting in \emph{imbalanced} causal function $Y_{t+1}=f(\mathcal{X};\theta)$ with a static outcome $Y_{t+1}$.
This accordingly motivates the emerging trend in \emph{inverse learning} methods \cite{arora2021survey}. Further details will be discussed in Section \ref{sec:causality}.
% Moreover, the overlooked structural $\vartheta$ may result in inherent biases, fundamentally undermining the identifiability of the dynamical entities in AI, forming a vital reason to reconsider the current paradigm, as detailed in Section~\ref{sec:temporal}.

On the other hand, large language models (LLMs) have facilitated the autonomous identification of different dynamics under various conditions ($\omega$ values) in the semantic space 
\cite{gurnee2023language}. However, ``multiple temporal dimensions'' accommodating different dynamics do not necessarily equate to ``multiple relative timings''. 
%Mutually independent dynamics may represent different temporal dimensions, yet they can still be identified simultaneously from the same absolute timing. In contrast, the $\mathbb{R}^T$ space, comprising relative timings, is capable of accommodating dynamical interactions among these dimensions, as represented by the unobservable association $(\theta^i, \theta^j)\in \mathbb{R}^H$ for any distinct $i,j\in \{1,\ldots, T\}$.

\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.96\textwidth}
\paragraph{Remark 4.} 
\hspace{-3mm}
\emph{Parallel Temporal Dimensions} indicate mutual \emph{\textbf{nonlinear independence}}, thus can be identified simultaneously from absolute timing $\mathbf{t}$ within $\mathbb{R}^O$. \\ In contrast, \emph{Relative Timings} suggest potential \emph{\textbf{nonlinear dependence}}, i.e., \emph{dynamical interactions}, forming the multi-dimensional $\mathbb{R}^T$ space, which reflects human counterfactual thinking.

\vspace{-0.5mm}
\end{minipage}}
%\vspace{-1mm}

%the ability to extract the informative parameter $\theta$ distinguishes the anticipated knowledge comprehension by AGI from mere context-associative learning. 
%In language learning tasks, the unobservable $\theta$ represents the semantic relation between phrases $\mathcal{X}$ and $\mathcal{Y}$, 


Essentially, current LLMs primarily focus on semantic associations along an absolute timing $\mathbf{t}$, indicating the order of phrases. Given the consistent sequential semantics in words, the omission of relative timings is reasonably justifiable for the scope of today's context-associative learning tasks. Furthermore, even along $\mathbf{t}$, instead of explicitly extracting $\theta$, most language models implicitly reflect it through the association $(\mathcal{X}, \mathcal{Y})$. This might contribute to AI's ability to generate intelligent responses without truly ``understanding'' in a human sense, due to the absence of an informatively extracted $\theta$. 

Integrating meta-learning with LLMs could potentially enhance the associative model generalizability \cite{lake2023human}, over the $\mathbf{t}$ timing alone. Particularly, given meta-learning's adaptability to diverse conditions in solely observational tasks \cite{hospedales2021meta}, its application could lead to improved hierarchical association $(\mathcal{X}, \mathcal{Y},{\omega})$ to better reflect hierarchical relations $(\theta, {\omega})$.
Given our goal of achieving informative structural knowledge as represented by $\vartheta_{\omega}$ or $(\vartheta, {\omega})$, which encapsulates $(\theta^i, \theta^j)\in \mathbb{R}^H$ for any distinct $i,j\in \{1,\ldots, T\}$, discussing AGI within the current LLM framework might still be premature. We suggest that enabling \emph{Relation-Oriented} meta-learning could potentially bring us closer to this target.


%In solely observational learning tasks, such as image recognition, meta-learning is considered advantageous in deriving $\mathcal{I}(\omega)$ due to its inherent adaptability to different $\omega$ across various contexts \cite{hospedales2021meta}. Consequently, integrating meta-learning with LLMs could potentially enhance the hierarchical association $(\mathcal{X}, \mathcal{Y}, \omega)$, making it more generalizable over the $\mathbf{t}$ timing axis alone \cite{lake2023human}.
%Therefore, discussing structural knowledge $\vartheta$ within Large Language Models (LLMs) may still be premature.


% Considering the generally consistent sequential semantics of words, the existence of potential relative timelines can be basically precluded without concerns of inherent biases.  


% aims to derive an informative parameter $\theta$ from observable $\mathcal{X}$ and $\mathcal{Y}$, to fully represent the knowledge denoted by $\theta \in \mathbb{R}^h$.
% Suppose the association $(\theta,\omega) \in \mathbb{R}^h$ represents $n$-levels $\theta$, denoted as $(\theta,\omega) = (\theta_1,\ldots,\theta_n)$.
% The resulting function $\mathcal{Y}=f(\mathcal{X};\theta)$ could be generalizable across $\omega$ if the hierarchical observables $\{(\mathcal{X}_1,\mathcal{Y}_1), \ldots, (\mathcal{X}_n,\mathcal{Y}_n)\}$ are captured.
% % Suppose the resulting $\theta$ purely encapsulates the knowledge in $\mathbb{R}^h$ without any ancillary information from $\mathcal{X}$ and $\mathcal{Y}$, it stands to reason that the function $f(;\theta)$ would be generalizable across any $\omega \in \mathbb{R}^h$, which mirrors the knowledge generalization processes within our cognition.

% %\vspace{1mm}
% \noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\hspace{-2mm}\begin{minipage}{0.95\textwidth}
% \paragraph{Remark 3.} 
% \hspace{-1mm}
% \emph{Generalizability} of relationship model depends on unobservable association $(\theta, \omega) \in \mathbb{R}^h$.

% \vspace{-0.5mm}
% \end{minipage}}
% \vspace{-1mm}


%In essence, the current meta-learning remains confined to observational $(\mathcal{X},\mathcal{Y})_{\omega}\in \mathbb{R}^O$, while an AGI system incorporating knowledge understanding should involve relational information represented by $\vartheta_{\omega}\in \mathbb{R}^T \cup \mathbb{R}^H$.
%It may stand to reason that the key to enabling AGI lies in implementing \emph{Relation-Oriented} meta-learning. 
% This realization underscores the need to move beyond the current \emph{Observation-Oriented} paradigm, which inherently omits relative timelines as computational dimensions. 
% In contrast, \emph{Relation-Oriented} approaches offer the potential for autonomous identification through relation-indexing, thereby granting computational liberty to the temporal dimension.


\section{Neglected Effect Dynamics in Causality}
\label{sec:causality}
%\vspace{-2mm}
% Contrary to mainstream AI applications that primarily explore observable associations, classical causal inference inherently focuses on deriving an informative parameter $\theta$ between observables $X$ and $Y$, providing a robust theoretical foundation.
% Based on that, the conventional causal learning methods have yielded significant contributions over the past decades, establishing a wealth of causal knowledge across various domains  \cite{wood2015lesson, vukovic2022causal,  ombadi2020evaluation}.



%From a modeling perspective, specified timestamps are associated with observations rather than constituting a separate computational dimension. Consequently, in traditional causal inference, the temporal-evolving aspects that distinguish causality from correlation are not directly built into the current modeling framework. 

%Essentially, causality mandates the incorporation of the timeline as a \emph{computational dimension}, ensuring recognition of significant \emph{distributions} on it, ones that undeniably can exhibit \emph{temporal nonlinearity}.

Traditional causal inference often highlights the interpretability of causal models, notably to be distinguished from mere correlations. In essence, these distinctions are not inherently embedded in the modeling context but are mainly evident in model interpretations, which can potentially guide further causally meaningful improvements for the model.
Given the statistical basis of causal inference, the significance of nonlinear temporal dynamics has not been fully embraced yet.
This section concentrates on these often overlooked dynamics, aiming to provide a more intuitive understanding of causal learning. 
%reassess causal learning inquiries, particularly.

%assuredly
% \emph{Observation-Oriented} modeling fades out the causal significance of these relationships in two aspects: First, manual specifications cannot completely identify dynamics of effects for each level; Second, these dynamics might coexist in various relative timelines, which suggests multiple computational dimensions in the Temporal Feature Space.
% Considering these points, we revisit the definition of causality from a modeling perspective:



%\vspace{2mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=red!50!black, colback=yellow!10]{
\hspace{-2mm}
\begin{minipage}{0.97\textwidth}
\paragraph{Definition 6.} Causality vs. Correlation in the modeling context.

$\ \bullet$ Causality $\mathcal{X}\xrightarrow{\vartheta}\mathcal{Y}$ is the relationship neccessitating dynamical $\mathcal{X}$ and $\mathcal{Y}$, especially the effect $\mathcal{Y}$.

$\ \bullet$ Correlation $X^t\xrightarrow{\theta}Y^t$ only requires static cause and effect, possibly sequential static $X^t$ and $Y^t$.

\end{minipage}}
%\vspace{-1mm}

% exhibits an \emph{imbalance} in capturing dynamics between the cause and the effect (see subsection \ref{sec:causality3_2} for details). For instance, the hierarchical dynamics overlooked in Figure~\ref{fig:hidden} can be fully captured by an inverse model of $do(A)=f(\{B_t,\ldots,B_{t+40}\})$ using RNNs, eliminating the need for a hidden confounder. 


% \vspace{-1mm}
% \subsection{Imbalanced Dynamical Learning}



Timestamp $t$ was first introduced by the Picard-Lindelof theorem in the 1890s, initiating the functional form $Y_{t+1}=f(X_t)$ to represent time evolution.
Subsequently, the time series learning methods, like autoregressive models \cite{hyvarinen2010estimation}, facilitate the form of $Y_{t+1}=f(X^t)$ with a sequential causal variable $X^t$, where the time progress from $t$ to $t+1$ is predetermined. 
For RNNs, the latent space optimization over the representation of $X^t$ is driven by predicting the observed $Y_{t+1}$ through the parameterized relation $\theta$. Consequently, the significant temporal nonlinearity within $X^t$ over $\textbf{t}$ can be captured, enabling the form of $Y_{t+1}=f(\mathcal{X};\theta)$
with a dynamical cause $\mathcal{X}$.
However, the effect $Y_{t+1}$ remains static, leaving its potentially significant dynamics completely managed by the function $f$. While $f$ can be selected as nonlinear to enable $\mathcal{X}$, the time evolution from $X^t$ to $Y_{t+1}$ is always left as \emph{\textbf{linear}}, resulting in \emph{static outcome} sequence $Y^t=Y_1,\ldots, Y_t$.


% Figure environment removed

Figure~\ref{fig:eff}(a) illustrates the often overlooked effect dynamics in traditional causal models. The action $do(A)$ causes dynamical $\mathcal{B}_{\omega}$ (observed as sequence $B^t$), disentangled by two levels in (b): Level $\mathbf{I}$, the generalized standard sequence $\mathcal{B}_o$ of length 30; Level $\mathbf{II}$, the individualized variations $\mathcal{B}_{\omega}-\mathcal{B}_o$. 
Assume the unobserved individualized characteristics linearly impact $\mathcal{B}_o$, making $\omega = P_i, P_j, \ldots$ simply represent speeds.
% The modeling objective is to obtain $\mathcal{B}_o$, as the effectiveness evaluation of $M_A$.

A typical clinical model, like $B_{t+30}=f(do(A_t))$ that averages all patients' D30 static effects as the outcome, turns to neglect D1-D29 within $\mathcal{B}_o$. However, even adopting a sequential outcome $B^t$ (e.g., Granger causality), it remains challenging to accurately estimate $\mathcal{B}_o$ by linear averaging, not to mention further reaching $\mathcal{B}_{\omega}$.
%estimating $B^t$ by averaging all patients' D1-D30 sequences 
Particularly, it requires the selected records to meet certain criteria, essentially equal to manually defining the boundary of $\mathcal{B}_o$ by exploring all possible ${\omega}$ values. %: an exact 30-day span on average, the near-linear variations among patients, the near-normal variational distributions centered on D30, and others.


%involves manually delineating the population-level effect $\theta$ while omitting $\omega$ influences. 

%at best, only the population-level sequence can be accurately estimated by averaging over all patients. This approach, however, excludes further dynamic feature levels, such as individual-level speed.

Such hierarchical dynamical effects are prevalent in fields like epidemic progression, economic fluctuations, and strategic decision-making. They often rely on similar preprocessing to identify specific levels, such as the group-specific learning methodology \cite{fuller2007developing}. 
These approaches have become impractical in AI-based applications and may lead to notable information loss in large-scale structural models. 
%examinations over $\omega$ values
%However, these traditional approaches are impractical in the context of large-scale relational learning within AI applications. 
%Particularly in structural relationships, without manual specifications for each variable, their interchangeable roles of cause and effect can lead to amplified errors notable in the output.



%Dynamic effects, prevalent in applications like epidemic progression, economic fluctuations, and strategic decision-making, often manifest at different levels of granularity. %These levels - as a type of hidden relations - are identifiable by our cognition but not directly observable in the data. 
%Group-specific learning methodologies \cite{fuller2007developing} are typically employed to address these issues, essentially serving as a manual specification of the value of $\omega$ 



% \noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.95\textwidth}
% \paragraph{Remark 2.} RNNs extract dynamic nonlinearity from the cause by indexing via the relation $\theta$.
% \vspace{-1mm}
% \end{minipage}}


\subsection{Identification Difficulty of Dynamical Effect}
\label{subsec:caus_identif}
\vspace{-1mm}
%requires specifying the sequential outcome $Y^t=(Y_1,\ldots,Y_t)$ to represent $\mathcal{Y}$. Then, a functional

In a relationship $\mathcal{X} \xrightarrow{\theta} \mathcal{Y}$, an \emph{Observation-Oriented} model can be ideally formulated as $Y^t=f(\mathcal{X};\theta)$ based on existing knowledge, to derive $\theta$ and generate the static sequential estimations $\hat{Y_{\theta}^t}=\hat{Y_1},\ldots,\hat{Y_t}$ with high accuracy. Yet, two types of errors may present challenges: the discrepancy between the specified outcome sequence and the targeted dynamical effect $\mid\mathcal{Y}-Y^t\mid$; and the modeling error from predetermined function $f(;\theta)$. They contribute to the difficulty of identifying nonlinear effects $\mathcal{Y}$ \cite{zhang2012identifiability}.

%The formulated rough boundary $\bigr[ f(\mathcal{X};\theta), Y^t \bigr]$ can facilitate a more accurate convergence towards $\hat{\mathcal{Y}_{\theta}}$.

%While the use of neural networks, such as RNNs (to extract nonlinear $\theta$ from $\mathcal{X}$ without a predetermined distribution), can effectively bypass the explicit specification of $f(;\theta)$, the need to pinpoint $\mathcal{Y}$ closely to $\hat{\mathcal{Y}_{\theta}}$ persists. Moreover, such \emph{Observation-Oriented} identification predominantly captures the observational features of $\mathcal{Y}$, excluding its (dynamical) temporal ones.

Specifically, due to the static sequence $Y^t$, the task of representing neglected dynamics of $\mathcal{Y}$ shifts either to $f(;\theta)$ or to $\mathcal{X}$. In the former scenario, a factor $\sigma$ representing ``disturbance'' is integrated into the function, resulting in $f(;\theta+\sigma)$ \cite{zhang2012identifiability}. In the latter case, as illustrated in do-calculus \cite{pearl2012calculus, huang2012pearl}, the dynamics of $\mathcal{X}$ need to be manually discretized as identifiable temporal events to ensure their observational effects. 
This enables a fluid transformation from dynamical cause to observational effect,  but the identifiability relies on non-experimental data (controllable $\theta$) and can introduce additional complexities. 


% To avoid specifying time sequences for causes, do-calculus \cite{pearl2012calculus, huang2012pearl} targets \emph{identifiable} events, 

Considering the \emph{differential} essence of do-calculus, we provide a streamlined reinterpretation of its three core rules from an \emph{integral} viewpoint.
Let $do(x_t)=(x_t, x_{t+1})$ indicate the occurrence of an instantaneous event $do(x)$ at time $t$, with the time step $\Delta t$ appropriate to ensure the \emph{interventional} effect of $do(x_t)$ identifiable as a function of the resultant distribution at $t+1$. Meanwhile, a separate \emph{observational} effect is provoked by the static $x_t$. Then, the dynamical cause $\mathcal{X}$ can be discretized as below:

\vspace{-6mm}
\begin{align*}
    \text{Given } \mathcal{X} & \xrightarrow{\theta} Y, \text{ where } \mathcal{X} = \langle X,t \rangle \in \mathbb{R}^{d+1} \text{ with the augmented $\textbf{t}$ dimension residing a $l$-length sequence,} \\
    \mathcal{X} =& \int_0^l do(x_t) \cdot x_t \ dt \text{\ \ with }
    \begin{cases}
      (do(x_t)=1) \mid \theta, & \text{ \emph{Observational} only (Rule 1) } \\
      (x_t=1) \mid \theta, & \text{ \emph{Interventional} only (Rule 2) }    \\
      (do(x_t)=0) \mid \theta, & \text{ No \emph{interventional}  (Rule 3) }   \\
      \text{otherwise} & \text{ Associated \emph{observational} and \emph{interventional} }
    \end{cases} \\
     \text{The effect } & \text{of } \mathcal{X} \text{ can be derived as }
     f(\mathcal{X}) = \int_0^l f_t \big( do(x_t) \cdot x_t \big) \ dt = \sum_{t=0}^{l-1} (y_{t+1}-y_t) =y_l-y_0
\end{align*}
\vspace{-5mm}

Based on a controllable $\theta$, it addresses three criteria that can preserve conditional independence between \emph{observational} and \emph{interventional} effects, completing the chain rule, but sidesteps more generalized cases. If oppositely defining $\mathcal{Y}=\langle Y,\tau \rangle$ as a dynamical effect, discretizing the dynamics in $do(y)$ remains necessary. % under the current paradigm, while the proposed one is designed to construct $\mathcal{Y}$ autonomously.






% Unlike our cognitive processes, where the effect can be identified via knowledge $\theta$ from the cause, under the \emph{Observation-Oriented} paradigm struggles - Even if $Y$ is designated as a sequence, its nonlinear dynamics within the $\tau$-dimension remain unaccounted for by the model, which takes the form $\mathcal{X} \xrightarrow{\theta} Y$ for details).

\subsection{Imbalance between Cause and Effect}
\label{subsec:caus_imbalance}

For the model itself, causal directionality (i.e., the roles of cause and effect)
may not impose restrictions, although it is often emphasized in model interpretations. 
Specifically, when selecting a model for a directional relationship $X\rightarrow Y$,  one could use $Y=f(X;\theta)$ to predict the effect $Y$, or $X=g(Y;\phi)$ to inversely infer the cause $X$. Both parameters, $\theta$ and $\phi$, are obtained from the joint probability $\mathbf{P}(X, Y)$ without imposing modeling constraints. We refer to this as \emph{symmetric directionality} for clarity.

The empirical concerns for modeling directions mainly arise for two reasons: 1) to comply with our intuitive understanding of temporal progression; 2) the current causal modeling exhibits an \emph{\textbf{imbalance}} in capturing dynamics between the cause and the effect, with a typical example as RNNs, represented by $Y=f(\mathcal{X};\theta)$.

Given the symmetric directionality, and to capitalize on the imbalance, inverse learning methodology \cite{arora2021survey} has recently garnered increasing attention, to achieve autonomous dynamical effect identification by inversely assigning the effect as the cause within RNNs. However, this approach is unsuitable for addressing the structuralized relation $\vartheta$. Specifically, the overlooked relative timings within $\vartheta$ may introduce inherent bias; due to stemming from an implicitly assumed nonlinear independence, it cannot be eliminated even by inverting the model. This will be further detailed in  Section~\ref{sec:temporal}.

%\emph{symmetric directionality} to sidestep the challenge of identifying dynamical effects and defining the objective function.

Another factor contributing to the imbalance is the increased difficulty when specifying effect sequence $Y^t$ compared to cause sequence $X^t$. While organizing sequential data around a major causal event (e.g., days of heavy rain) is feasible, pinpointing the precise onset of subsequent effects (e.g., the exact day a flood began due to the rain) remains a more complex task.

%\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.95\textwidth}
\paragraph{Remark 5.} By indexing through $\theta$, simultaneous optimization of $\mathcal{X}$ and $\mathcal{Y}$ can be achieved, mitigating their imbalance and enabling autonomous identification of both dynamical variables.
%With a known relation $\theta$ from observables $\mathcal{X}$ to $\mathcal{Y}$, extracting the representation of $\mathcal{Y}$ by indexing through $\theta$ enables autonomous dynamical effect identification in the $\mathcal{X}\xrightarrow{\theta}\hat{\mathcal{Y}_{\theta}}$ relationship.

\vspace{-1mm}
\end{minipage}}

The \emph{Relation-Oriented} modeling approach seeks to autonomously derive $\theta$ from the feature representations of $\mathcal{X}$ and $\mathcal{Y}$ within the latent space. 
Specifically, the initial sequences $X^t$ and $Y^t$ are transformed into a latent space, $\mathbb{R}^L$, which allows nonlinear computational freedom in their temporal dimensions.
Then, a neural network representing $\theta$ can be trained between them in $\mathbb{R}^L$ without relying on prior assumptions.
%within a unified training process.
%enables the creation of two dynamical representations via a single optimization.

The training process uses $\mathcal{X}$ as the input and $\mathcal{Y}$ as the output, indexed through $\theta$, facilitating the concurrent optimization of both dynamical representations. Consequently, this yields sequentially associated $(\mathcal{X},\theta,\hat{\mathcal{Y}_{\theta}})$, with each individual representation maintaining. The implementation will be outlined in Chapter II.
% while maintaining the individual representations of $\mathcal{X}$ and $\mathcal{Y}$. 
% and achieve relation-indexed optimization for both dynamical $\mathcal{X}$ and $\mathcal{Y}$ simultaneously.



% Contrary to the $\mathcal{X}$ in RNNs that acts as an observational-temporal variable, in traditional methods, sequences $\{x_t\}$ and $\{y_\tau\}$ are purely observational, potentially capturing a limited length of static temporal features and heavily depending on accurate specifications of hidden relations $\omega$.
% Consider Figure \ref{fig:eff}: estimating the level $\textbf{I}$ sequence by averaging patient data from Day1-Day30 necessitates specific criteria, such as a precise 30-day length, near-linear individual diversity, and a normal distribution around Day30. In essence, this involves manually delineating the population-level effect $\theta$ while omitting $\omega$ influences. 



% \section{Causal Inference Challenges}
% \label{sec:application}

% \subsection{Incomplete Causal Effect}
% \label{subsec:appl_incomp}
% \vspace{-1.5mm}




%\vspace{-2mm}
\subsection{Interpretation Complexity}
\label{subsec:appl_elusive}
\vspace{-1mm}


%For patients $P_i$ and $P_j$, the population-level last-day effect $B_{t+30}$ is inaccurate. To counter this individual-level bias and improve model interpretation, statistical causal inference incorporates the ``hidden confounder'' concept into Directed Acyclic Graphs (DAG), %representing the concealed $\omega$ as node $E$ in Figure~\ref{fig:hidden} (a).

Since effect dynamics are often partially overlooked, traditional causal inference introduces the concept of ``hidden confounder'' to enhance model interpretability. 
For example, the node $E$  in Figure~\ref{fig:hidden} (a) symbolizes the unobserved individualized characteristics in the scenario depicted in Figure~\ref{fig:eff}. 

However, this approach does not necessarily require collecting additional data to identify $E$. 
This might lead to an illogical implication: ``Our model is biased due to some unknown factors we dont intend to explore.'' Indeed, this strategy employs a solely observational causal variable, $E$, to account for the overlooked dynamical effect features. While $E$ remains unknown, its inclusion can complete the model interpretation.
Yet, from the modeling perspective, as illustrated in Figure~\ref{fig:hidden}(b), the associative cause $do(A) * E$ remains unknown, failing to provide a modelable relationship for addressing $(\theta, \omega) = \begin{pmatrix} \theta_o \\ \theta_{\omega} \end{pmatrix}$.


% Figure environment removed
\vspace{-1mm}

% compensates for the overlooked level $\mathbf{II}$ dynamic, which essentially transforms an \emph{observable} dynamical feature of the \emph{effect} into a \emph{hidden} observational variable, $E$, associated with the \emph{cause} $do(A)$. 

Fundamentally, incorporating a hidden confounder can improve the model's interpretability but not its generalizability. In contrast, the \emph{Relation-Oriented} approach does not require extra modeling; it leverages $\theta$ as indices to extract $\hat{\mathcal{Y}_{\theta}}$, enabling the use of any observed identifier associated with $\omega$, such as patient IDs. As illustrated in (c), this hierarchical disentanglement of representations in the latent space can effectively encapsulate effect dynamics to achieve generalizability.



%\vspace{2mm}
\subsection{Causal Assumptions Reliance}
\label{subsec:appl_assumpt}
\vspace{-1mm}

Due to the frequently overlooked effect dynamics, traditional causal learning typically relies on foundational causal assumptions to validate practical applications. In Figure~\ref{fig:view}, we categorize causal model applications into four distinct scenarios based on two aspects: Firstly, depending on whether the predetermination for $\theta$ is based on knowledge, they are divided into Causal Discovery and Causation Buildup. Secondly, they are further differentiated by the dynamical significance of their effects.

%For example, the causal relationship ``raining $\rightarrow$ wet floor'' falls into area $\circled{4}$, while ``raining $\rightarrow$ floor becoming wetter'' is in area $\circled{3}$.
%They will be examined from two perspectives in the following: the modeling objective Relation (i.e., $\theta$), and the interpretational Directionality.


% Figure environment removed
%\vspace{-2mm}


% %\vspace{4mm}
% \subsubsubsection{\emph{(1) Modeled Relation}}
% \vspace{-1mm}

As depicted in Figures~\ref{fig:eff} and \ref{fig:hidden}, the individualized dynamical features are easily overlooked in a generalized causation buildup process. Based on existing knowledge, some unobserved entities may be identified as hidden confounders, thereby enriching model interpretations. Nonetheless, if such identification is not easy, the foundational \emph{Causal Sufficiency} assumption may lead to the complete neglect of these dynamics, presuming that all potential ``hidden confounders'' have been observed in the system.

On the other hand, causal discovery typically unearths structural relationships by detecting dependence among observables, but is usually confined to observational attributes, excluding their temporal features. If their dynamical features are not crucial, discovered associations can provide valuable insights into the underlying correlations; if they are essential, significant dynamics might be overlooked due to the \emph{Causal Faithfulness} assumption, which suggests that captured observables can fully represent the causal reality.

% \vspace{-1mm}
% \subsubsubsection{\emph{(2) Modeled Causal Direction}}
% \vspace{-1mm}

Furthermore, although the discovered relationships are directional, these directions frequently lack a logical causal implication.
Consider $X$ and $Y$ with predetermined directional models $Y=f(X;\theta)$ and $X=g(Y;\phi)$. The direction $X\rightarrow Y$ would be favored if $\mathcal{L}(\hat{\theta}) > \mathcal{L}(\hat{\phi})$. 
Let $\mathcal{I}_{X,Y}(\theta)$ denote the Fisher information about $\theta$ given $\mathbf{P}(X,Y)$. Use $p(\cdot)$ as the density function, and $\int_X p(x;\theta) dx$ remains constant in this context. Then:

\vspace{-6mm}
\begin{align*}
    \mathcal{I}_{X,Y}(\theta) &= \mathbb{E}[(\frac{\partial }{\partial \theta} \log p(X,Y;\theta))^2 \mid \theta] 
    = \int_{Y} \int_X (\frac{\partial }{\partial \theta} \log p(x,y;\theta))^2 p(x,y;\theta) dx dy \\
    &= \alpha \int_Y (\frac{\partial }{\partial \theta} \log p(y;x,\theta))^2 p(y;x,\theta) dy + \beta = \alpha \mathcal{I}_{Y\mid X}(\theta)+\beta, \text{with } \alpha,\beta \text{ being constants.} \\
    \text{Then, } \hat{\theta} &=\argmax\limits_{\theta} \mathbf{P}(Y\mid X,\theta) = \argmin\limits_{\theta} \mathcal{I}_{Y\mid X} (\theta) =\argmin\limits_{\theta} \mathcal{I}_{X,Y}(\theta), \text{ and } \mathcal{L}(\hat{\theta})  \propto 1/\mathcal{I}_{X,Y}(\hat{\theta}).
\end{align*}
\vspace{-5mm}

%The likelihoods of $\hat{\theta}$ and $\hat{\omega}$ rely on the information $\mathcal{I}(\hat{\theta})$ and $\mathcal{I}(\hat{\omega})$. 
The inferred directionality indicates how informatively the observational data distribution reflects the two predetermined parameters.
Consequently, such directionality is not logical but could be dominated by the data collection process, with the predominant entity deemed the ``cause'', consistent with existing conclusions \cite{reisach2021beware, kaiser2021unsuitability}.
%rather indicative of distributional dominance as determined by the data collection process. Here, the predominant entity is labeled the ``cause''.
Even when informative $\theta$ and $\phi$ are incorporated based on knowledge, they might not provide insights for dynamically significant causal relations.

%they only link to solely observational features of entities, and thus may not accurately reflect true causal relations.
%in the presence of dynamically significant effects.

%their designated distributions appear in the data, %with the predominant one deemed the ``cause'' - It assumes, by default, that observations capture the cause more thoroughly than the effect. While limited data collection techniques made it reasonable in the past, it is no longer safe to assume such observationally inferred directions to hold logical meaning for causality.



\section{Relative Timings in Structural Causality}
\label{sec:temporal}

Consider a structural relationship $\mathcal{Y} \xleftarrow{\theta_1} do(X) \xrightarrow{\theta_2} \mathcal{Z}$, where two dynamical effects of $do(X)$ progress along distinct relative timings $\mathbf{t_1}$ and $\mathbf{t_2}$. Initially,
$\mathcal{Y}$ and $\mathcal{Z}$ are identified as sequences $Y^t$ and $Z^t$ according to absolute timing $\mathbf{t}$. 
Regarding the interaction between $\mathcal{Y}$ and $\mathcal{Z}$, three distinct scenarios are possible: 1) no interaction, implying $\theta_1 \perp \theta_2 \in\mathbb{R}^H$; 2) $\mathcal{Y}$ and $\mathcal{Z}$ are \emph{\textbf{confounded dynamics}} but with linear dependence only; 3) they form \emph{\textbf{dynamical confounding}} with nonlinear dynamical interactions. 

In scenario 2), AI models like inverse RNNs can accurately capture $\vartheta=\overrightarrow{\theta_1\theta_2}$ by using $do(X)=f\bigl((Y, Z)^t;\vartheta\bigr)$ with associative identification $(Y, Z)^t=\bigl((Y, Z)_1, \ldots,(Y, Z)_t\bigr)$. Yet, if a conventional Structural Causal Model (SCM) lacking dynamical capture capability is used, or if inverse RNNs are employed under the conditions of scenario 3), the associated $(Y, Z)^t$ might introduce \emph{\textbf{inherent bias}}, consequently reducing the model's robustness and generalizability.
Instead, it is necessary to initialize $Y^t$ and $Z^t$ individually, and then engage in a two-step \emph{relation-indexed learning} to sequentially obtain $\mathcal{Y}=f_1(do(X);\theta_1)$, and $\mathcal{Z}=f_2(do(X)\mid \mathcal{Y};\theta_2)$.

This section will first demonstrate the \emph{inherent bias} through an intuitive example (section \ref{subsec:temp_bias}), explore its impact on the generalizability of structural causal models (section \ref{subsec:temp_generalz}), and finally discuss the advancements and challenges on our path toward incorporating structural causal knowledge within AI (section \ref{subsec:temp_toward}).



\subsection{Scheme of the Inherent Bias}
\label{subsec:temp_bias}
\vspace{-1mm}

% Figure environment removed
\vspace{-2mm}

% Consider medical trial data from hospital patients. Vital signs and medication usage are recorded daily, forming the chronological \emph{absolute timeline}. However, to assess the effects of a specific medication, a \emph{relative timeline} is constructed, with time-zero marking a consistent action, such as $do(A)$, for all patients. As a result, events with different chronological timestamps can align on the relative timeline, and vice versa.
% For instance, Figure \ref{fig:eff} illustrates a relative timeline for the effects of $do(A)$, while Figure~\ref{fig:do1}(a) revisits its causal DAG, incorporating the introduced hidden confounder. 


Figure~\ref{fig:do1}(a) revisits the hidden-confounder inclusion depicted in Figure \ref{fig:eff}.
To clearly visualize the dynamical variations across multi-dimensional relative timings, we propose an enhancement to the conventional causal DAGs. This enhancement, as shown in (b), is carried out through two steps:
\begin{enumerate}[itemsep=0em, topsep=-1pt, 
parsep=2pt, partopsep=-1pt,
leftmargin=20pt, labelwidth=10pt]
    \item Consider dynamically significant effects and integrate their relative timings as individual axes.
    \item Use edge lengths to signify timespans needed for reaching a certain effect magnitude in a static value.
    %the effects to reach an equivalent magnitude.
\end{enumerate}


%To more effectively address this issue, the causal DAG (directed acyclic graph) is enhanced in two ways: 1) by incorporating desired logical timelines as axes into the DAG space, and 2) by assuming causal effects are dynamically significant, with varying edge lengths indicating different timespans required to achieve identical effects.
%For instance, Figure~\ref{fig:do1}(a) revisits the hidden-confounder example, which aims to interpret different individualized effects. Alternatively, the enhanced DAG shown in (b) provides a convenient representation of these effects.


Figure~\ref{fig:do3}(a) depicts a structural relationship $\mathcal{B} \xleftarrow{\theta_1} A \xrightarrow{\theta_2} \mathcal{C}$, extending from the scenario in Figure~\ref{fig:do1}(b), with $A$ succinctly replacing $do(A)$. It features two distinct dynamical effects: the primary effect $\mathcal{B}$ via $\theta_1$, represented by the edge $\overrightarrow{AB}$ leading to a static value for vital sign $B$; and a side effect $\mathcal{C}$ via $\theta_2$ on another vital sign $C$, indicated by edge $\overrightarrow{AC}$. Notably, $\mathcal{C}$ can influence $\mathcal{B}$, creating \emph{confounded dynamics} across two timing axes $\mathbf{t_1}$ and $\mathbf{t_2}$.
For simplicity, we assume \emph{dynamical independence}, by fixing the timespan of $\overrightarrow{AC}$ at 10 days for all patients, which is in scenario 2), and focus on modeling the static outcome $B$ to predict the average fully-released medical effect in this population.
%suggesting that $A$ also indirectly affects $B$ through $C$, 

% Figure environment removed
\vspace{-1mm}

%For simplicity, we assume $\theta_1\perp\theta_2 \in \mathbb{R}^H$, with the timespan of $\overrightarrow{AC}$ fixed at 10 days for all patients, and focus on modeling the static outcome $B$ to predict the average fully-released medical effect in this population.

From a geometrical view, the triangle over nodes $\{A, B, C\}$ should remain closed across all populations and individuals to represent the same relationship, as supported by the \emph{Causal Markov} condition. Accordingly, the generalization (and also individualization) process can be geometrically viewed as a \emph{linear transformation} of the causal DAG, depicted as ``stretching'' the triangle along $\mathbf{t_1}$ at various ratios, as in Figure~\ref{fig:do3}(a).

In conventional SCMs, the status of $B$ is typically derived by setting an average timespan for the full release of medicine along $\overrightarrow{AB}$, say 30 days in this case. As illustrated in (b) and (c), the SCM function fails to shape a valid DAG for individual patients, represented by $P_i$ in red and $P_j$ in blue. Consequently, sequential biases would be implied when extending to estimate a sequential outcome like $B^t=(B_1,\ldots, B_{30})$. %the $B^t$ sequences for all patients would be treated as i.i.d. outcomes, implying sequential biases. 


\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=red!50!black, colback=yellow!10]{
%\hspace{-2mm}
\begin{minipage}{0.94\textwidth}
\paragraph{Definition 7.}  
\emph{\underline{Inherent Biases}} within conventional SCMs. 

The \emph{inherent bias} may occur when priorly identifying the causal effect, if it contains: 
1) confounded dynamics across multiple relative timings, and 2) undetectable hierarchy represented by $\omega$.

\end{minipage}}


In this simplified scenario, an inverse RNN model, formulated as $A= f\bigl((B, C)^t\bigr)$, could be effective due to the assumed dynamical independence. However, it is impractical to assume independence or the absence of confounded dynamics for all effects. This is particularly true in large models dealing with complex causal structures, where inherent biases can accumulate, ultimately jeopardizing the model's robustness.

% \vspace{1mm}
% \noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=red!50!black, colback=yellow!10]{\begin{minipage}{0.96\textwidth}
% \paragraph{Theorem 3.}The \emph{\textbf{inherent} bias} may occur in SCM if it contains: 
% 1) \emph{Confounded} dynamical causal effects across \emph{Multiple} logical timelines, and 2) Unobservable Hierarchy (represented by $\omega$).
% \end{minipage}}
% \vspace{1mm}

%Interestingly, most successful causal applications instinctively avoid either \emph{confounding} or \emph{multi-timeline}. Causal inference typically employs de-confounding (like PSM \cite{benedetto2018statistical} and backdoor adjustment \cite{pearl2009causal}), to mitigate inherent bias and other confounding biases resulting from unaddressed nonlinearity.
%Meanwhile, many AI accomplishments, including Large Language Models (LLMs), which operate in a semantic space, do not inherently deal with relative timelines, maintaining words consistently ordered along a singular timeline.



\subsection{Inherently Restricted Generalizability}
\label{subsec:temp_generalz}
\vspace{-1.5mm}

To address the issues of confounded dynamics, traditional causal inference uses various methods to perform ``de-confounding'', such as cutting off interaction through propensity score matching \cite{benedetto2018statistical} and backdoor adjustment \cite{pearl2009causal}. However, these techniques often require intended tailoring for specific applications, necessitating manual identification of dynamical effects. Given the black-box nature and large scale of AI models, such manual adjustments have become increasingly impractical.

Moreover, these methods primarily focus on adapting to statistical linear models, which may not effectively contribute to dynamical generalizability. Subsequently, we will use a practical scenario to clearly illustrate how the specification of timestamps for effects inherently hinders the generalizability of the formulated SCMs.

%Consequently, while still operating under the \emph{Observation-Oriented} paradigm, AI-based causal learning tends to default to the absolute timeline, which is the only directly observable one in the data, to specify timestamps for all events. 
%This method can lead to \emph{inherent biases} accumulating over the structural complexity, ultimately affecting the model's robustness and generalizability.

% Figure environment removed


Figure~\ref{fig:3d} displays an enhanced 3D view DAG, where $\Delta t$ and $\Delta \tau$ signify actual time spans, particularly within the current population, to support the causal reasoning represented by this structure. 
%Yet, the crux is not on determining their exact values, but on realizing their intended causal relationship: 
Consider the triangle $SA'B'$: As each unit of effect from $S$ delivered to $A'$ (spent $\Delta \tau$), it immediately starts to impact $B'$ through $\overrightarrow{A'B'}$ ($\Delta t$ needed); meanwhile, the next unit begins generation at $S$. This dual action runs concurrently until $S$'s effect fully reaches $B'$, representing the single edge $\overrightarrow{SB'}$ within the SCM.
%At $B'$, the ultimate aim of this process is to evaluate the total cumulative influence stemming from $S$.

%($=\frac{1}{2} \overrightarrow{AB'}$)
Due to the equation $\overrightarrow{SB'} = \overrightarrow{SA'} + \overrightarrow{A'B'}$, specifying the time span of $\overrightarrow{SB'}$ inherently determines the $\Delta t:\Delta \tau$ ratio based on the current population's performance, thereby fixing the shape of the ${ASB'}$ triangle in the DAG space. If we focus solely on the accuracy of the estimated mean effect for this population, the SCM function $B'=f(A, C, S)$ can be effective. However, given that the preset $\Delta t:\Delta \tau$ ratio is not universally applicable, the generalizability of the established SCM to other populations becomes questionable.

\subsection{Developments Toward Causal Reasoning AI}
\label{subsec:temp_toward}
\vspace{-1.5mm}

In the pursuit of causal reasoning in machine learning, modeling techniques have evolved from capturing mere associations to learning observational correlations, ultimately advancing to structural causality modeling that incorporates the cognitive temporal space $\mathbb{R}^T$. Figure~\ref{fig:model} summarizes this evolution in an upward trajectory.

\vspace{-2mm}
% Figure environment removed
\vspace{-3mm}

Given AI's capability to learn temporal dynamics, the present challenge involves addressing the dynamical interactions within causal structures.  As shown in sections \ref{subsec:temp_bias} and \ref{subsec:temp_generalz}, conventional SCMs lack the ability to capture dynamics. Even with dynamical independence, where only linear dependence is present, specifying timestamps to identify outcomes can still risk introducing inherent biases. Therefore, it is crucial to develop a new structural knowledge-aligned modeling paradigm, transitioning away from the current \emph{Observation-Oriented} approach. Physical models, explicitly incorporated in temporal dimensional computation, may offer valuable insights into this prospect.

% and are able to establish abstract concepts through relations, may provide insights into these challenges. 
% The relation-indexing approach is designed to bridge the gap between the Observational and Temporal Spaces.
%\pagebreak
%is to ensure the generalizability of structural causal AI models. Recognizing multi-timeline dynamics is essential to avoid biases that obscure AI interpretability. Given the impracticality of manually discerning all potential logical timelines for observable data, it might be time to contemplate a new paradigm.

Under the observational i.i.d. assumption, initial models only approximate associations, proved unreliable for causal reasoning \cite{pearl2000models, peters2017elements}. Subsequently, the common cause principle highlights the significance of the nontrivial condition, to distinguish a relationship from statistical dependencies \cite{dawid1979conditional, geiger1993logical}, providing a basis for constructing graphical models \cite{peters2014causal}.
The initial graphical model relies on conditional dependencies to construct Bayesian networks, with limited causal relevance \cite{scheines1997introduction}. Then, causally significance emphasizes the capability of addressing counterfactual queries \cite{scholkopf2021toward}, like the structural equation models (SEMs) and functional causal models (FCMs) \cite{glymour2019review, elwert2013graphical}, which leverage prior knowledge to establish causal structures.

State-of-the-art deep learning on causality encodes the discrete, DAG-structural constraint into continuous optimization functions \cite{zheng2018dags, zheng2020learning, lachapelle2019gradient}, enabling advanced efficiency, but without noticeable generalizability, evident from the restricted successes in applications like the neural architecture search (NAS) \cite{luo2020causal, ma2018using}. This is reasonable, since the neglected relative timings can lead to inherent biases amplified through complex structures to become significant.

\cite{scholkopf2021toward} summarized our confronting key challenges toward generalizable causal-reasoning AI: 1) limited model robustness, 2) insufficient model reusability, and 3) inability to handle data heterogeneity (i.e., undetectable hierarchies). They are intrinsically linked to the demonstrated inherent biases.

% noting that these challenges can be attributed to the timestamp specification required by \emph{Observation-Oriented} SCMs.





\begin{center}
   {\vskip 8pt\large\bf Chapter II: Realization of Proposed Relation-Oriented Paradigm} 
\end{center}

This chapter introduces the proposed \emph{Relation-Indexed Representation Learning} (RIRL) method, a baseline realization of the raised \emph{Relation-Oriented} modeling paradigm.
RIRL primarily focuses on autonomously identifying dynamical effects, in the form of relation-indexed representations in the latent space. 
In the context of structural modeling, RIRL enables hierarchical disentanglement of effects, according to given DAGs, as a manner of realizing dynamical generalizability across undetectable levels within knowledge.
As a baseline realization, RIRL is suitable for applications with mature structural causal knowledge,
and plenty of data to support neural network training on each known causal relationship.

First, Section \ref{sec:representation} details the technique for extracting relation-indexed representations. Then, building on this, Section \ref{sec:RIRL} presents the RIRL method of establishing structural causal models in the latent space. Lastly, Section \ref{sec:experiment} provides experiments to validate RIRL's efficacy in autonomously identifying effects.


%provides formulations of factorizations, to achieve of achieving hierarchical disentanglement through relation-indexing

%\vspace{-2mm}
\section{Relation-Indexed Representation}
\label{sec:representation}

In the relationship $\mathcal{X}\rightarrow \mathcal{Y}$, we define dynamical $\mathcal{X}=\langle X, t \rangle \in \mathbb{R}^{d+1} \subseteq \mathbb{R}^O$ and $\mathcal{Y} = \langle Y, \tau \rangle \in \mathbb{R}^{b+1} \subseteq \mathbb{R}^O$, given their solely observational variables, $X\in \mathbb{R}^d$ and $Y\in \mathbb{R}^b$.
$\mathcal{X}$ is observed as a data sequence, represented by $X^t = X_1, \ldots, X_t$ with a pre-determined length $l_x$.
For clarity, hereafter in this chapter, its instance $x^t$ will be considered as a $(d*l_x)$-dimensional vector, denoted by $\overrightarrow{x}$ (or $x$ for briefty). 
Similarly, $\mathcal{Y}$ is observed as the data sequence $Y^t$ with a pre-determined length $l_y$, and its instance is referred to as a $(b * l_y)$-dimensional vector $\overrightarrow{y}$ (or $y$ for briefty). 

The relation-indexed representation aims to formulate $(\mathcal{X},\theta,\hat{\mathcal{Y}_{\theta}})$ in the latent space $\mathbb{R}^L$, beginning with an \emph{initialization} to transform $X^t$ and $Y^t$ to be latent space features.
For the sake of clarity, we use 
$\mathcal{H} \in \mathbb{R}^L$ and $\mathcal{V} \in \mathbb{R}^L$ to refer to the latent representations of $\mathcal{X}\in \mathbb{R}^O$ and $\mathcal{Y}\in \mathbb{R}^O$, respectively.

The modeling process is to optimize the neural network function $f(;\theta)$ in $\mathbb{R}^L$, with $\mathcal{H}$ as its input and $\mathcal{V}$ as the output. This process simultaneously refines $\mathcal{H}$, $\theta$, and $\mathcal{V}$, for ultimately achieving $(\mathcal{H}, \theta, \hat{\mathcal{V}_{\theta}}) = (\mathcal{X},\theta,\hat{\mathcal{Y}_{\theta}})$. The refining will present as the distance minimization between $\mathcal{H}$ and $\mathcal{V}$
within $\mathbb{R}^L$. 
Consequently, the dimensionality $L$ of the latent feature space must satisfy $L \ge rank(\mathcal{X},\theta,\mathcal{Y})$, raising a technical challenge that $L$ is larger than the dimensionality of $\overrightarrow{x}$ or $\overrightarrow{y}$. %, where the observable data of $\mathcal{X}$ presents to be $d * T_x$ dimensional.
%in a latent vector with a length $L$ that certainly surpasses its rank $rank(\mathcal{H})$, and possibly its original length of $d * T_x$.

\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.94\textwidth}
\paragraph{Remark 6.} The variable \emph{initialization} necessitates a \emph{higher-dimensional} representation autoencoder.
\end{minipage}}

%The goal of relation-indexing is to obtain $\mathcal{\hat{Y}}$, which is the component of $\mathcal{Y}$ that can be \emph{\textbf{identifiable}} through its relationship with $\mathcal{X}$, accordingly represented as $\mathcal{\hat{V}}$ in the latent feature space.
%Moreover, for the relationship models to be \emph{{generalizable}}, $\mathcal{\hat{V}}$ must serve as a basis, which permits subsequent components of $\mathcal{Y}$ to build upon it, representing its various other relationships, leading to the \emph{{hierarchical disentanglement}} of $\mathcal{Y}$.





% The constructed latent feature space $\mathbb{R}^L$ must ensure: 
% 1) The observational-temporal feature $\mathcal{X}=\langle X, t \rangle \in \mathbb{R}^{d+1}$ is fully represented in $\mathbb{R}^L$ (remark of $T$ is omitted for simplicity);
% 2) The representation $\mathcal{X}$ is hierarchically disentangled within $\mathbb{R}^L$ based on specified relations, facilitating the model's generalizability across various hierarchical levels.

%Consider a relationship $\mathcal{X}\rightarrow \mathcal{Y}$, where $Y\in \mathbb{R}^b$ and $\mathcal{Y} = \langle Y, \tau \rangle \in \mathbb{R}^{b+1}$.
% In the context of the effect $\mathcal{Y}$, relation-indexing entails deriving $\mathcal{\hat{Y}}$, which represents the portion of $\mathcal{Y}$ that is identifiable through its relation with $\mathcal{X}$. This segment can serve as a foundation for additional levels of $\mathcal{Y}$'s representations, which is a generalizing process for the established relationship model $\mathcal{X}\rightarrow \mathcal{Y}$.



\subsection{Higher-Dimensional Autoencoder}
\label{subsec:repr_autoencoder}
\vspace{-1mm}

% Figure environment removed

Autoencoders are commonly used for dimensionality reduction, especially in structural modeling that involves multiple variables \cite{wang2016auto}.
%, the column-augmented original data matrix often possesses a dimensionality exceeding that of the latent space $\mathbb{R}^L$. 
In contrast, RIRL aims to model individual causal relationships sequentially within a higher-dimensional latent space $\mathbb{R}^L$, as to hierarchically construct the entire causal structure. 
As illustrated in Figure~\ref{fig:arch}, the designed autoencoder architecture is featured by the symmetrical \emph{Expander} and \emph{Reducer} layers (source code is available \footnote{https://github.com/kflijia/bijective\_crossing\_functions/blob/main/code\_bicross\_extracter.py}).
The Expander magnifies the input vector $\overrightarrow{x}$ by capturing its higher-order associative features, while the Reducer symmetrically diminishes dimensionality and reverts to its initial state. For precise reconstruction, the \emph{invertibility} of these processes is essential. 

The Expander showcased in Figure~\ref{fig:arch} implements a \emph{double-wise} expansion. Here, every duo of digits from $\overrightarrow{x}$ is encoded into a new digit using an association with a random constant, termed the \emph{Key}. This \emph{Key} is generated by the encoder and replicated by the decoder. Such pairwise processing of $\overrightarrow{x}$ expands its length from $(d*l_x)$ to be $(d * l_x - 1)^2$. By leveraging multiple \emph{Keys} and concatenating their resultant vectors, $\overrightarrow{x}$ can be considerably expanded, ready for the subsequent dimensionality-reduced representation extraction.
The four blue squares with unique grid patterns represent expansions by four distinct \emph{Keys}, with the grid patterns acting as their ``signatures''. Each square symbolizes a $(d * l_x - 1)^2$ length vector. Similarly, higher-order expansions, like \emph{triple-wise} across three digits, can be achieved with adapted \emph{Keys}.

% The four differently patterned blue squares represent the vectors expanded by four distinct \emph{Keys}, with the grid patterns indicating their ``signatures''. Each square visualizes a $(d * T_x - 1)^2$ length vector (not signifying a 2-dimensional vector).
% In a similar way, higher-order extensions, such as \emph{triple-wise} ones across every three digits, can also be employed by appropriately adapting \emph{Keys}.

Figure~\ref{fig:extractor} illustrates the encoding and decoding processes within the Expander and Reducer, targeting the digit pair $(x_i, x_j)$ for $i\neq j \in 1,\ldots,d$. The Expander function is defined as $\eta_\phi(x_i,x_j) = x_j \otimes exp(s(x_i)) + t(x_i)$, which hinges on two elementary functions, $s(\cdot)$ and $t(\cdot)$. The \emph{Key} parameter, $\phi$, embodies their weights, $\phi=(w_s, w_t)$.
Specifically, the Expander morphs $x_j$ into a new digit $y_j$ utilizing $x_i$ as a chosen attribute. In contrast, the Reducer symmetrically uses the inverse function $\eta_\phi^{-1}$, defined as $(y_j-t(y_i)) \otimes exp(-s(y_i))$. 

This approach circumvents the need to compute $s^{-1}$ or $t^{-1}$, thereby allowing more flexibility for nonlinear transformations through $s(\cdot)$ and $t(\cdot)$. This is inspired by the groundbreaking work in \cite{dinh2016density} on invertible neural network layers employing bijective functions.

% Figure environment removed

\subsection{Optimization Steps}
\label{subsec:repr_optimz}
\vspace{-1mm}

% Figure \ref{fig:bridge} illustrates the process of linking $\mathcal{H}$ and $\mathcal{V}$ to model the relationship $\mathcal{X}\rightarrow \mathcal{Y}$. 
%to explicitly include the temporal features of $h$. For now, we suppose $\mathcal{V}$ can capture potential dynamics autonomously, expecting future refinements.

Consider instances $x$ and $y$ of $\mathcal{X}$ and $\mathcal{Y}$, with corresponding representations $h$ and $v$ in $\mathbb{R}^L$. The latent dependency $\mathbf{P}(v| h)$ is used to train the relation function $f(;\theta)$, as illustrated in Figure \ref{fig:bridge}.
In each iteration, the modeling process undergoes three optimization steps:
\begin{enumerate}[topsep=0pt, partopsep=0pt]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{1pt}
    \item {Optimizing the cause-encoder by $\mathbf{P}(h|x)$, the relation model by $\mathbf{P}(v|h)$, and the effect-decoder by $\mathbf{P}(y|v)$ to reconstruct the relationship $x\rightarrow y$, represented as $h\rightarrow v$ in $\mathbb{R}^L$.}
    \item {Fine-tuning the effect-encoder $\mathbf{P}(v|y)$ and effect-decoder $\mathbf{P}(y|v)$ to accurately represent $y$.} 
    \item {Fine-tuning the cause-encoder $\mathbf{P}(h|x)$ and cause-decoder $\mathbf{P}(x|h)$ to accurately represent $x$.}
\end{enumerate}
\vspace{1mm}

During this process, the values of $h$ and $v$ are iteratively adjusted to reduce their distance in $\mathbb{R}^{L}$, with $f(;\theta)$ serving as a bridge to span the distance. Here, the hyper-dimensional variable $\theta \in \mathbb{R}^H$ acts as the index, guiding the output of $f(;\theta)$ to encapsulate associated representations $( \mathcal{{H}},\theta,\hat{\mathcal{V}_{\theta}})$. From $\hat{\mathcal{V}_{\theta}}$, the effect component $\hat{\mathcal{Y}_{\theta}}$ can be reconstructed. 
Within the system, for each effect, a series of such relation functions $\{f(;\theta)\}$ is maintained, indexing diverse levels of causal inputs for sequentially building the structural model.



\section{RIRL: Building Structural Models in Latent Space}
\label{sec:RIRL}


% Figure environment removed


By sequentially constructing relation-indexed representations for each pairwise relationship within the causal DAG, we can achieve the hierarchically disentangled representation for each node, according to its levels defined by the global structure. Simultaneously, the entire structualized causality has also been constructed. Subsequently, section \ref{subsec:RIRL_stacking} details the method for stacking relation-indexed representations, enabling the construction of higher-level representations based on previously established lower-level ones;
section \ref{subsec:RIRL_disentangle} provides the complete factorization process for hierarchical disentanglement; finally, section \ref{subsec:RIRL_discover} discusses a causal discovery algorithm within the latent space among initialized variable representations.


Figure~\ref{fig:new} demonstrates how the RIRL method can encapsulate the black-box nature of AI within the latent space while simultaneously generating interpretable observations. This characteristic can be utilized to enhance conventional \emph{Observation-Oriented} models, for instance, by simulating counterfactual values on demands. Meanwhile, in the latent space, these cryptic representations, although opaque to human interpretation, play a crucial role in achieving model generalization and individualization. These processes are latently managed by AI and remain exclusive to human comprehension.


%Causal relationships of known edges can be sequentially stacked using existing causal DAGs in domain knowledge.
%Additionally, this approach aids in discovering causal structures within the latent space by identifying potential relationships among the initial variable representations.


%latently. Simultaneously, one can enhance traditional causal models by employing desired latent features, to produce observations as needed, such as counterfactual effects.

%This section introduces a specialized autoencoder architecture crucial for implementing this approach, outlines the method for hierarchical representation disentanglement in constructing graphical models, and presents a causal discovery algorithm for the latent feature space.


% \subsection{Disentanglement of Relationship}
% \vspace{-1.5mm}

% Given a set of $n$-level hierarchical representation functions for $\mathcal{X}$, denoted by $\mathcal{F}(\vartheta) = \bigl\{ f_i \bigl(\theta_i \bigr) \mid i=1,\ldots, n\bigr\}$, the goal is to define $n$ relationship functions, collectively termed $\mathcal{G}$, such that $\mathcal{Y}=\mathcal{G}(\mathcal{X})$ exhibits an $n$-level hierarchy.
% Each $i$-th level relationship function is $ g_i(\mathcal{X}; \varphi_i)$, where $\varphi_i$ is its parameter. Then, we have:
% \begin{equation}
%     \mathcal{G}(\mathcal{X}) = \sum_{i=1}^{n} g_i(\mathcal{X}; \varphi_i) = \sum_{i=1}^{n} g_i(\Theta_{i}; \varphi_i)  =
%     \sum_{i=1}^{n} g_i \bigl(\theta_i;\ \Theta_1,\ldots, \Theta_{i-1}, \varphi_i\bigr) = \mathcal{Y}
% \end{equation}
% The $i$-th level relation-indexed representation for $\mathcal{Y}$ is $g_i (\theta_i;\varphi_i)$ considering the features of the preceding $(i-1)$ levels of $\mathcal{X}$. This relationship can be portrayed as the augmented feature vector $\langle \theta_i, \varphi_i \rangle$ in latent space $\mathbb{R}^L$.
% Using $\vartheta_X$ and $\vartheta_Y$ to distinguish the collective hierarchical representations for $\mathcal{X}$ and $\mathcal{Y}$ respectively, the overall relationship from $\mathcal{X}$ to $\mathcal{Y}$ becomes $\vartheta_Y=\langle \vartheta_X, \varphi \rangle$, where $\varphi=\{\varphi_1,\ldots,\varphi_n\}$. %and $\langle \vartheta_X, \varphi \rangle$ 
% The term $\langle \vartheta_X, \varphi \rangle$ represents the pairwise augmentations between collections $\vartheta_X$ and $\varphi$.

\subsection{Stacking Hierarchical Representations}
\label{subsec:RIRL_stacking}
\vspace{-1mm}
% \noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.94\textwidth}
% \paragraph{Remark 2.} Given a causal graph $G$ with data matrix $\mathbf{X}$ column-augmented by all nodes' attributes, the latent space dimensionality $L$ must satisfy $L\ge rank(\mathbf{X})$ to adequately represent $G$.
% \vspace{-0.5mm}
% \end{minipage}}


%\vspace{-2.7mm}
% Figure environment removed


A structural relationship can be represented by a causal graph, denoted as $G$. To construct models in the latent space, the latent dimensionality $L$ must be sufficiently large to adequately represent $G$.
Let's denote a data matrix augmented by all observational attributes in $G$ as $\mathbf{X}$. Given the need to include informative relations $\{\theta\}$ for the edges in $G$, it is essential that $L > rank(\mathbf{X})+T$, where $T$ indicates the number of dynamically significant variables (i.e., nodes) within $G$.

The PCA principle posits that the space $\mathbb{R}^L$ learned by the autoencoder is spanned by the top principal components of $\mathbf{X}$ \cite{baldi1989neural, plaut2018principal, wang2016auto}.
Hypothetically, reducing $L$ below $rank(\mathbf{X})$ may yield a less adequate but causally more significant latent space through better alignment of dimensions \cite{jain2021mechanism} (Further exploration in this direction is warranted). Bypassing a deep dive into dimensionality boundaries, we rely on empirical fine-tuning for the experiments in this study (reducing $L$ from 64 to 16).
%In this study, we will set aside discussions on the boundaries of dimensionality. Our experiments feature 10 variables with dimensions 1 to 5 (Table~\ref{tab:tower}), and we empirically fine-tune and reduce $L$ from 64 to 16.


% Consider a relationship $\mathcal{X}\rightarrow \mathcal{Y}$, where $Y\in \mathbb{R}^b$ and $\mathcal{Y} = \langle Y, \tau \rangle \in \mathbb{R}^{b+1}$.
% In the context of the effect $\mathcal{Y}$, relation-indexing entails deriving $\mathcal{\hat{Y}}$, which represents the portion of $\mathcal{Y}$ that is identifiable through its relation with $\mathcal{X}$. This segment can serve as a foundation for additional levels of $\mathcal{Y}$'s representations, which is a generalizing process for the established relationship model $\mathcal{X}\rightarrow \mathcal{Y}$.
% For example, suppose a graphical system $\{\mathcal{X}, \mathcal{Y}, \mathcal{Z}\}$ has the relationship $\mathcal{X}\rightarrow \mathcal{Y} \leftarrow \mathcal{Z}$, then, $\mathcal{Y}$ can be viewed as in a two-level hierarchy. The first level is defined by $\mathcal{X}\rightarrow \mathcal{Y}$ and the second by $\langle\mathcal{X}, \mathcal{Z}\rangle \rightarrow \mathcal{Y}$, where the second level enhances the first by incorporating an additional data stream from $\mathcal{Z}$.

% Using a collective representation, all $f_\theta$ functions can be expressed as $\mathcal{F}(X; \vartheta)$, where $\vartheta$ includes all parameters. Thus, the Expander and Reducer can be concisely written as $Y=\mathcal{F}(X; \vartheta)$ and $X = \mathcal{F}^{-1}(Y; \vartheta)$. 

Consider a causal structural among $\{\mathcal{X}, \mathcal{Y}, \mathcal{Z}\}$, with their corresponding representations $\{\mathcal{H}, \mathcal{V}, \mathcal{K}\} \in \mathbb{R}^L$ initialized by three autoencoders, respectively. Figure~\ref{fig:stack} illustrates the hierarchical representations buildup.
%the hierarchical assembly of two modeled relationships associated with $\mathcal{Y}$.
Here, two stacking scenarios are displayed based on varying causal directions. With the established $\mathcal{X}\rightarrow \mathcal{Y}$ relationship in $\mathbb{R}^{L}$, the left-side architecture finalizes the $\mathcal{X}\rightarrow \mathcal{Y}\leftarrow \mathcal{Z}$ structure, while the right-side focuses on $\mathcal{X}\rightarrow \mathcal{Y}\rightarrow \mathcal{Z}$. Through the addition of a representation layer, hierarchical disentanglement is formed, allowing for various input-output combinations (denoted as $\mapsto$) according to specific requirements.

For example, on the left, $\mathbf{P}(v|h) \mapsto \mathbf{P}(\alpha)$ represents the $\mathcal{X}\rightarrow \mathcal{Y}$ relationship, whereas $\mathbf{P}(\alpha|k)$ implies $\mathcal{Z}\rightarrow \mathcal{Y}$. Conversely, on the right, $\mathbf{P}(v) \mapsto P(\beta|k)$ denotes the $\mathcal{Y}\rightarrow \mathcal{Z}$ relationship with $\mathcal{Y}$ as input. Meanwhile, $\mathbf{P}(v|h) \mapsto P(\beta|k)$ captures the causal sequence $\mathcal{X} \rightarrow \mathcal{Y}\rightarrow \mathcal{Z}$.


\subsection{Factorizing the Effect Disentanglement}
\label{subsec:RIRL_disentangle}

Consider $\mathcal{Y}=\langle X, \tau \rangle \in \mathbb{R}^{b+1} \subseteq \mathbb{R}^O$ having a $T$-level hierarchy, with each level built up using a representation function, labeled as $g_t$ for the $t$-th level. For simplicity, here, we use $\omega_t$ to represent the $t$-th level component of $\mathcal{Y}$ in the latent space $\mathbb{R}^L$, while its counterpart in $\mathbb{R}^{b+1}$ is denoted as $\Omega_t$. %(i.e., $\mathcal{\hat{Y}}$ at the $t$-th level).
Let the feature vector $\omega_t$ in $\mathbb{R}^{L}$ primarily spans a sub-dimensional space, $\mathbb{R}^{L_t}$, resulting in the spatial disentanglement sequence $\{\mathbb{R}^{L_1}, \ldots, \mathbb{R}^{L_t},\ldots,\mathbb{R}^{L_T}\}$, which hierarchically represents $\mathcal{Y}$ with $T$ relative timings.
Function $g_t$ maps from $\mathbb{R}^{b+1}$ to $\mathbb{R}^{L_t}$, taking into account features from all previous levels as attributes.  This gives us:
\vspace{-4mm}

\begin{equation}
    \mathcal{Y} = \sum_{t=1}^{n} \Omega_t, 
     \text{ where } \Omega_t = g_t \bigl( \omega_t ;\ \Omega_1,\ldots, \Omega_{t-1}\bigr) \text{ with } \Omega_t \in \mathbb{R}^{b+1} \text{ and } \omega_t \in \mathbb{R}^{L_t} \subseteq \mathbb{R}^{L}
\end{equation}

\vspace{-3mm}
%The $t$-th component in the observable data space, denoted as $\Omega_t \in \mathbb{R}^{O}$, is articulated through an observational data sequence with the length of $T_y$, along the absolute timeline $t$.
%However, in latent space, the objective of $\omega_t$ is to capture dynamics along a relative timeline, $t_i$, which is autonomously determined by the relation at the $i$-th level, not bound by the observational timestamps in $\mathbb{R}^{O}$. 


% $\{t_1,\ldots,t_i,\ldots,t_n\}$, each uniquely determined by the relationship at their respective levels, apart from the absolute timeline $t$.
% While in the \emph{observable data space}, the $i$-th level feature, represented as the sum $\Omega_1 +\ldots +\Omega_i$, still maintains its timestamp attribute along $t$.

In the context of a purely observational hierarchy, with $\mathcal{Y}$ substituted by $Y \in \mathbb{R}^b$, The example depicted in Figure~\ref{fig:hand} (b) can be interpreted as follows: Consider three feature levels represented as $\omega_1\in \mathbb{R}^{L_1}$, $\omega_2\in \mathbb{R}^{L_2}$, and $\omega_3\in \mathbb{R}^{L_3}$. For simplicity, assume each subspace is mutually exclusive, such that $L=L_1+L_2+L_3$. In the latent space, the triplet $\langle\omega_1, \omega_2, \omega_3\rangle \in \mathbb{R}^{L}$ comprehensively depicts the image. Their observable counterparts, $\Omega_1$, $\Omega_2$, and $\Omega_3$, are three distinct full-scale images, each showcasing different content. For example, $\Omega_1$ emphasizes finger details, while the combination $\Omega_1+\Omega_2$ reveals the entire hand.



\subsection{Causal Discovery in Latent Space}
\label{subsec:RIRL_discover}
%\vspace{-2mm}


\begin{minipage}{\textwidth}
    \begin{minipage}[b]{0.52\textwidth}
    \raggedright
    \begin{algorithm}[H]
    \footnotesize
    \SetAlgoLined
    \KwResult{ ordered edges set $\mathbf{E}=\{e_1, \ldots,e_n\}$ }
    $\mathbf{E}=\{\}$ ; $N_R = \{ n_0 \mid n_0 \in N, Parent(n_0)=\varnothing\}$ \;
    \While{$N_R \subset N$}{
    $\Delta = \{\}$ \;
    \For{ $n \in N$ }{
        \For{$p \in Parent(n)$}{
            \If{$n \notin N_R$ and $p \in N_R$}{
                $e=(p,n)$; \ 
                $\beta = \{\}$;\\ 
                \For{$r \in N_R$}{
                    \If{$r \in Parent(n)$ and $r \neq p$}{
                        $\beta = \beta \cup r$}
                    }
                $\delta_e = K(\beta \cup p, n) - K(\beta, n)$;\\
                $\Delta = \Delta \cup \delta_e$;
            }
        }
    }
    $\sigma = argmin_e(\delta_e \mid \delta_e \in \Delta)$;\\
    $\mathbf{E} = \mathbf{E} \cup \sigma$; \ 
    $N_R = N_R \cup n_{\sigma}$;\\
    }
     \caption{Latent Space Causal Discovery}
    \end{algorithm}
    \end{minipage}
%\hfill 
    \begin{minipage}[b]{0.4\textwidth}
    \raggedleft
        \label{tab:table1}
        \begin{tabular}{|l|l|}
        \hline
           $G=(N,E)$ & graph $G$ consists of $N$ and $E$ \\
           $N$ & the set of nodes\\
           $E$ & the set of edges\\
           $N_R$ & the set of reachable nodes\\
           $\mathbf{E}$ & the list of discovered edges\\
           $K(\beta, n)$ & KLD metric of effect $\beta\rightarrow n$\\
           $\beta$ & the cause nodes \\
           $n$ & the effect node\\
           $\delta_e$ & KLD Gain of candidate edge $e$\\
           $\Delta=\{\delta_e\}$ & the set $\{\delta_e\}$ for $e$ \\
           $n$,$p$,$r$ & notations of nodes\\
           $e$,$\sigma$ & notations of edges\\
          \hline
        \end{tabular} %
%       \captionof{table}{A table beside a figure}
    \end{minipage}
\end{minipage}


Algorithm 1 outlines the heuristic procedure for investigating edges among the initialized variable representations. We use Kullback-Leibler Divergence (KLD) as a metric to evaluate the strength of causal relationships. Specifically, as depicted in Figure~\ref{fig:bridge}, KLD evaluates the similarity between the relation output $\mathbf{P}(v|h)$ and the prior $\mathbf{P}(v)$. Lower KLD values indicate stronger causal relationships due to closer alignment with the ground truth. Conversely, while Mean Squared Error (MSE) is a frequently used evaluation metric, its sensitivity to data variances \cite{reisach2021beware} leads us to utilize it as a supplementary measure in this study. %In the graphical representation context, we refer to variables $A$ and $B$ in the edge $A\rightarrow B$ as the \emph{cause node} and \emph{effect node}, respectively.

%\vspace{-2mm}
% Figure environment removed


Figure~\ref{fig:discover} illustrates the causal structure discovery process in latent space over four steps. Two edges, ($e_1$ and $e_3$), are sequentially selected, with $e_1$ setting node $B$ as the starting point for $e_3$. In step 3, edge $e_2$ from $A$ to $C$ is deselected and reassessed due to the new edge $e_3$ altering $C$'s existing causal conditions. The final DAG represents the resulting causal structure.


%\vspace{-2mm}
\section{Efficacy Validation Experiments}
%\vspace{-3mm}
\label{sec:experiment}

The experiments aim to validate the efficacy of the RIRL method from three aspects: 1) the performance of the proposed higher-dimensional representations, evaluated by reconstruction accuracy, 2) the construction of a clear effect hierarchy through the stacking of relation-indexed representations, and 3) the identification of DAG structures within the latent space through discovery.
A full demonstration of the conducted experiments in this chapter is available online \footnote{https://github.com/kflijia/bijective\_crossing\_functions.git}, while with two primary limitations detailed as follows:

Firstly, the dataset employed in this study may not be the most suitable for evaluating the effectiveness of RIRL. 
Ideally, real-world data featuring rich structuralized causality across multiple relative timings, like clinical records, would be preferable. However, due to practical constraints, access to such optimal data is limited for this study, leading us to use the current synthetic data and focus solely on feasibility verification. For experimental validation regarding the inherent bias, please refer to prior research \cite{li2020teaching}.

Secondly, the time windows designated for cause and effect, $l_x$ and $l_y$, are fixed at 10 and 1, respectively. This constraint arose from an initial oversight in the experimental design stage, wherein the pivotal role of effect dynamics has not been fully recognized, consequently limited by the RNN pattern. It manifests as restricted successes in building causal chains like $\mathcal{X} \rightarrow \mathcal{Y}\rightarrow \mathcal{Z}$; while the model can adeptly capture single-hop causality, it struggles with multi-hop ones since the dynamics in $\mathcal{Y}$ have been segmented by $l_y=1$. However, extending the length of $l_y$ does not pose a significant technical challenge to future works.



%\vspace{-1mm}
\subsection{Hydrology Dataset}
\vspace{-1mm}

The dataset chosen for our experiments is a widely-used synthetic resource in the field of hydrology, aimed at enhancing streamflow predictions based on observed environmental conditions such as temperature and precipitation. %The application of RIRL aims to create a streamflow prediction model that is generalizable across various watersheds. While these watersheds share a fundamental hydrological scheme governed by physical rules, they may exhibit unique features due to unobserved conditions such as economic development and land use. Current models based on physical knowledge, however, often lack the flexibility to fully capture multiple levels of dynamical temporal features across these watersheds.
In hydrology, deep learning, particularly RNN models, has gained favor for extracting observational representations and predicting streamflow \cite{goodwell2020debates, kratzert2018rainfall}. 
We focus on a simulation of the Root River Headwater watershed in Southeast Minnesota, covering 60 consecutive virtual years with daily updates. The simulated data is from the Soil and Water Assessment Tool (SWAT), a comprehensive system grounded in physical modules, to generate dynamically significant hydrological time series. %The performance evaluations predominantly focus on the accuracy of the autoencoder reconstructions.

Figure \ref{fig:stream} displays the causal DAG employed by SWAT, complete with node descriptions. The hydrological routines are color-coded based on their contribution to output streamflow. Surface runoff (1st tier) significantly impacts rapid streamflow peaks, followed by lateral flow (2nd tier). Baseflow dynamics (3rd tier) have a subtler influence. Our causal discovery experiments aim to reveal these underlying tiers. %relationships from the observed data.


%\vspace{-3mm}
% Figure environment removed



\vspace{-1mm}
\subsection{Higher-Dimensional Variable Representation Test}
%\vspace{-2mm}

In this test, we have a total of ten variables (i.e., nodes), with each requiring an individual autoencoder for initialization. Table \ref{tab:tower} lists the statistical characteristics of their post-scaled (i.e., normalized) attributes, along with their autoencoders' reconstruction accuracies. Accuracy is assessed in the root mean square error (RMSE), where a lower RMSE indicates higher accuracy for both scaled and unscaled data.


The task is challenging due to the limited dimensionalities of the ten variables - maxing out at just 5 and the target node, $J$, having just one attribute. To mitigate this, we duplicate the input vector to a consistent 12-length and add 12 dummy variables for months, resulting in a 24-dimensional input. A double-wise extension amplifies this to 576 dimensions, from which a 16-dimensional representation is extracted via the autoencoder.
Another issue is the presence of meaningful zero-values, such as node $D$ (Snowpack in winter), which contributes numerous zeros in other seasons and is closely linked to node $E$ (Soil Water). We tackle this by adding non-zero indicator variables, called \emph{masks}, evaluated via binary cross-entropy (BCE).

Despite challenges, RMSE values ranging from $0.01$ to $0.09$ indicate success, except for node $F$ (the Aquifer). Given that aquifer research is still emerging (i.e., the 3rd tier baseflow routine), it is likely that node $F$ in this synthetic dataset may better represent noise than meaningful data.


\begin{table*}[t]
\caption{Characteristics of node attributes and their variable representation test results.}
\label{tab:tower}
\vspace{-2mm}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{|
>{\columncolor[HTML]{EFEFEF}}c |c|l|l|l|l|l|l|l|l|}
\hline
\cellcolor[HTML]{C0C0C0}Variable & \cellcolor[HTML]{C0C0C0} Dim & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Mean} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Std} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Min} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Max} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Non-Zero Rate\%} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}RMSE on Scaled} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}RMSE on Unscaled} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}BCE of Mask} \\ \hline
A & 5 & 1.8513 & 1.5496 & -3.3557 & 7.6809 & 87.54 & 0.093 & 0.871 & 0.095 \\ \hline
B & 4 & 0.7687 & 1.1353 & -3.3557 & 5.9710 & 64.52 & 0.076 & 0.678 & 1.132 \\ \hline
C & 2 & 1.0342 & 1.0025 & 0.0 & 6.2145 & 94.42 & 0.037 & 0.089 & 0.428 \\ \hline
D & 3 & 0.0458 & 0.2005 & 0.0 & 5.2434 & 11.40 & 0.015 & 0.679 & 0.445 \\ \hline
E & 2 & 3.1449 & 1.0000 & 0.0285 & 5.0916 & 100 & 0.058 & 3.343 & 0.643 \\ \hline
F & 4 & 0.3922 & 0.8962 & 0.0 & 8.6122 & 59.08 & 0.326 & 7.178 & 2.045 \\ \hline
G & 4 & 0.7180 & 1.1064 & 0.0 & 8.2551 & 47.87 & 0.045 & 0.81 & 1.327 \\ \hline
H & 4 & 0.7344 & 1.0193 & 0.0 & 7.6350 & 49.93 & 0.045 & 0.009 & 1.345 \\ \hline
I & 3 & 0.1432 & 0.6137 & 0.0 & 8.3880 & 21.66 & 0.035 & 0.009 & 1.672 \\ \hline
J & 1 & 0.0410 & 0.2000 & 0.0 & 7.8903 & 21.75 & 0.007 & 0.098 & 1.088 \\ \hline
\end{tabular}}
\end{table*}



%\vspace{1mm}
\subsection{Hierarchical Disentanglement Test}
%\vspace{-2mm}

Table \ref{tab:unit} provides the performance comparison of stacking relation-indexed representations on each node. The term ``single-effect'' is to describe the accuracy of a specific effect node when reconstructed from a single cause node (e.g., $B\rightarrow D$ and $C\rightarrow D$), and ``full-effect'' for the accuracy when all its cause nodes are stacked (e.g., $BC\rightarrow D$). To provide context, we also include baseline performance scores based on the initialized variable representations. During the relation learning process, the effect node serves two purposes: it maintains its own accurate representation (as per optimization no.2 in \ref{subsec:repr_optimz}) and helps reconstruct the relationship (as per optimization no.1 in \ref{subsec:repr_optimz}). Both aspects are evaluated in Table \ref{tab:unit}.


% For example, node $G$ has three possible causes $C$, $D$, and $E$, so we built four causal effect models in total: the individual pairwise causations $C\rightarrow G$, $D\rightarrow G$, $E\rightarrow G$, and the stacked causal effect $CDE\rightarrow G$. For convenience, we refer to them as "\emph{pair-relation}" and "\emph{stacking-relation}" respectively. We also listed the initial reconstruction performance for each variable in the column named "Variable Reconstruction (initial performance)" as a comparison baseline in Table \ref{tab:unit}.


\vspace{-2mm}
% Figure environment removed

The KLD metrics in Table \ref{tab:unit} indicate the strength of learned causality, with a lower value signifying stronger. For instance, node $J$'s minimal KLD values suggest a significant effect caused by nodes $G$ (Surface Runoff), $H$ (Lateral), and $I$ (Baseflow). In contrast, the high KLD values imply that predicting variable $I$ using $D$ and $F$ is challenging. 
For nodes $D$, $E$, and $J$, the ``full-effect'' are moderate compared to their ``single-effect'' scores, suggesting a lack of informative associations among the cause nodes. In contrast, for nodes $G$ and $H$, lower ``full-effect'' KLD values imply capturing meaningful associative effects through hierarchical stacking. The KLD metric also reveals the most contributive cause node to the effect node. For example, the proximity of the $C\rightarrow G$ strength to $CDE\rightarrow G$ suggests that $C$ is the primary contributor to this causal relationship.

Figure~\ref{fig:G} showcases reconstructed time series, for the effect nodes $J$, $G$, and $I$, in the same synthetic year to provide a straightforward overview of the hierarchical representation performances. 
Here, black dots represent the ground truth; the blue line indicates reconstruction via the initial variable representation, and the ``full-effect'' representation generates the red line. 
In addition to RMSE, we also employ the NashSutcliffe model efficiency coefficient (NSE) as an accuracy metric, commonly used in hydrological predictions. The NSE ranges from -$\infty$ to 1, with values closer to 1 indicating higher accuracy.

The initial variable representation closely aligns with the ground truth, as shown in Figure~\ref{fig:G}, attesting to the efficacy of our proposed autoencoder architecture. As expected, the ``full-effect'' performs better than the ``single-effect'' for each effect node. Node $J$ exhibits the best prediction, whereas node $I$ presents a challenge. For node $G$, causality from $C$ proves to be significantly stronger than the other two, $D$ and $E$.


% One may observe via the demo that our experiments do not show smooth information flows along successive long causal chains. Since RNNs are designed primarily for capturing the dynamics of causes rather than the effects, relying on them to autonomously construct dynamical representations of the effects might prove unreliable. It underscores a significant opportunity for enhancing effectiveness by improving the architecture.



\vspace{-1mm}
\subsection{Latent Space Causal Discovery Test}
%\vspace{-2mm}

The discovery test initiates with source nodes $A$ and $B$ and proceeds to identify potential edges, culminating in the target node $J$. Candidate edges are selected based on their contributions to the overall KLD sum (less gain is better). 
Table \ref{tab:discv} shows the order in which existing edges are discovered, along with the corresponding KLD sums and gains after each edge is included. Color-coding in the cells corresponds to Figure \ref{fig:stream}, indicating tiers of causal routines. The arrangement underscores the efficacy of this latent space discovery approach.

A comprehensive list of candidate edges evaluated in each discovery round is provided in Table \ref{tab:discv_rounds} in Appendix A. For comparative purposes, we also performed a 10-fold cross-validation using the conventional FGES discovery method; those results are available in Table \ref{tab:fges} in Appendix A.


%\vspace{-3mm}
\begin{table*}[t]
\caption{Effect Reconstruction Performances of RIRL sorted by effect nodes.}
\label{tab:unit}
\vspace{-2mm}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{|l|lll|l|lll|llll|}
\hline
\rowcolor[HTML]{C0C0C0} 
\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} } & \multicolumn{3}{l|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \begin{tabular}[c]{@{}l@{}}\ \ \ \ \ \ Variable Representation\\ \ \ \ \ \ \ \ (Initial)\end{tabular}}} & \cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} } & \multicolumn{3}{l|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000}  \begin{tabular}[c]{@{}l@{}}\ \ \ \ \ \ Variable Representation\\ \ \ \ \ \ \ \ (in Relation Learning)\end{tabular}}} & \multicolumn{4}{l|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \ \ \ \ \ \ \ \ \ Relationship Reconstruction}} \\ \cline{2-4} \cline{6-12} 
\rowcolor[HTML]{C0C0C0} 
\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} } & \multicolumn{2}{l|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000}\ \ \ \ \ \  \ \ \ \ RMSE}} & {\color[HTML]{000000} BCE} & \cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} } & \multicolumn{2}{l|}{\cellcolor[HTML]{C0C0C0}\ \ \ \ \ \ \ \ \ \ RMSE} & BCE & \multicolumn{2}{l|}{\cellcolor[HTML]{C0C0C0} \ \ \ \ \ \ \ \ \ RMSE} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}BCE} & KLD \\ \cline{2-4} \cline{6-12} 
\rowcolor[HTML]{C0C0C0} 
\multirow{-3}{*}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \begin{tabular}[c]{@{}l@{}}Result\\ Node\\ \ \end{tabular}}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Scaled\\ \ \ Values\end{tabular}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Unscaled\\ \ \ Values\end{tabular}} & Mask & \multirow{-3}{*}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \begin{tabular}[c]{@{}l@{}}Cause\\ Node\end{tabular}}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Scaled\\ \ \ Values\end{tabular}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Unscaled\\ \ \ Values\end{tabular}} & Mask & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Scaled\\ \ \ Values\end{tabular}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Unscaled\\ \ \ Values\end{tabular}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Mask} & \begin{tabular}[c]{@{}l@{}}(in latent\\ \ \ space)\end{tabular} \\ \hline
\cellcolor[HTML]{EFEFEF}C & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}0.037} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}0.089} & \cellcolor[HTML]{EFEFEF}0.428 & A & \multicolumn{1}{l|}{0.0295} & \multicolumn{1}{l|}{0.0616} & 0.4278 & \multicolumn{1}{l|}{0.1747} & \multicolumn{1}{l|}{0.3334} & \multicolumn{1}{l|}{0.4278} & 7.6353 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & BC & \multicolumn{1}{l|}{0.0350} & \multicolumn{1}{l|}{1.0179} & 0.1355 & \multicolumn{1}{l|}{0.0509} & \multicolumn{1}{l|}{1.7059} & \multicolumn{1}{l|}{0.1285} & 9.6502 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & B & \multicolumn{1}{l|}{0.0341} & \multicolumn{1}{l|}{1.0361} & 0.1693 & \multicolumn{1}{l|}{0.0516} & \multicolumn{1}{l|}{1.7737} & \multicolumn{1}{l|}{0.1925} & 8.5147 \\ \cline{5-5}
\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}D} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.015}} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.679}} & \multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.445} & C & \multicolumn{1}{l|}{0.0331} & \multicolumn{1}{l|}{0.9818} & 0.3404 & \multicolumn{1}{l|}{0.0512} & \multicolumn{1}{l|}{1.7265} & \multicolumn{1}{l|}{0.3667} & 10.149 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & BC & \multicolumn{1}{l|}{0.4612} & \multicolumn{1}{l|}{26.605} & 0.6427 & \multicolumn{1}{l|}{0.7827} & \multicolumn{1}{l|}{45.149} & \multicolumn{1}{l|}{0.6427} & 39.750 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & B & \multicolumn{1}{l|}{0.6428} & \multicolumn{1}{l|}{37.076} & 0.6427 & \multicolumn{1}{l|}{0.8209} & \multicolumn{1}{l|}{47.353} & \multicolumn{1}{l|}{0.6427} & 37.072 \\ \cline{5-5}
\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}E} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.058}} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}3.343}} & \multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.643} & C & \multicolumn{1}{l|}{0.5212} & \multicolumn{1}{l|}{30.065} & 1.2854 & \multicolumn{1}{l|}{0.7939} & \multicolumn{1}{l|}{45.791} & \multicolumn{1}{l|}{1.2854} & 46.587 \\ \hline
\cellcolor[HTML]{EFEFEF}F & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}0.326} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}7.178} & \cellcolor[HTML]{EFEFEF}2.045 & E & \multicolumn{1}{l|}{0.4334} & \multicolumn{1}{l|}{8.3807} & 3.0895 & \multicolumn{1}{l|}{0.4509} & \multicolumn{1}{l|}{5.9553} & \multicolumn{1}{l|}{3.0895} & 53.680 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & CDE & \multicolumn{1}{l|}{0.0538} & \multicolumn{1}{l|}{0.9598} & 0.0878 & \multicolumn{1}{l|}{0.1719} & \multicolumn{1}{l|}{3.5736} & \multicolumn{1}{l|}{0.1340} & 8.1360 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & C & \multicolumn{1}{l|}{0.1057} & \multicolumn{1}{l|}{1.4219} & 0.1078 & \multicolumn{1}{l|}{0.2996} & \multicolumn{1}{l|}{4.6278} & \multicolumn{1}{l|}{0.1362} & 11.601 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & D & \multicolumn{1}{l|}{0.1773} & \multicolumn{1}{l|}{3.6083} & 0.1842 & \multicolumn{1}{l|}{0.4112} & \multicolumn{1}{l|}{8.0841} & \multicolumn{1}{l|}{0.2228} & 27.879 \\ \cline{5-5}
\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}G} & \multicolumn{1}{l|}{\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}0.045}} & \multicolumn{1}{l|}{\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}0.81}} & \multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}1.327} & E & \multicolumn{1}{l|}{0.1949} & \multicolumn{1}{l|}{4.7124} & 0.1482 & \multicolumn{1}{l|}{0.5564} & \multicolumn{1}{l|}{10.852} & \multicolumn{1}{l|}{0.1877} & 39.133 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & DE & \multicolumn{1}{l|}{0.0889} & \multicolumn{1}{l|}{0.0099} & 2.5980 & \multicolumn{1}{l|}{0.3564} & \multicolumn{1}{l|}{0.0096} & \multicolumn{1}{l|}{2.5980} & 21.905 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & D & \multicolumn{1}{l|}{0.0878} & \multicolumn{1}{l|}{0.0104} & 0.0911 & \multicolumn{1}{l|}{0.4301} & \multicolumn{1}{l|}{0.0095} & \multicolumn{1}{l|}{0.0911} & 25.198 \\ \cline{5-5}
\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}H} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.045}} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.009}} & \multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}1.345} & E & \multicolumn{1}{l|}{0.1162} & \multicolumn{1}{l|}{0.0105} & 0.1482 & \multicolumn{1}{l|}{0.5168} & \multicolumn{1}{l|}{0.0097} & \multicolumn{1}{l|}{3.8514} & 39.886 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & DF & \multicolumn{1}{l|}{0.0600} & \multicolumn{1}{l|}{0.0103} & 3.4493 & \multicolumn{1}{l|}{0.1158} & \multicolumn{1}{l|}{0.0099} & \multicolumn{1}{l|}{3.4493} & 49.033 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & D & \multicolumn{1}{l|}{0.1212} & \multicolumn{1}{l|}{0.0108} & 3.0048 & \multicolumn{1}{l|}{0.2073} & \multicolumn{1}{l|}{0.0108} & \multicolumn{1}{l|}{3.0048} & 75.577 \\ \cline{5-5}
\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}I} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.035}} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.009}} & \multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}1.672} & F & \multicolumn{1}{l|}{0.0540} & \multicolumn{1}{l|}{0.0102} & 3.4493 & \multicolumn{1}{l|}{0.0948} & \multicolumn{1}{l|}{0.0098} & \multicolumn{1}{l|}{3.4493} & 45.648 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & GHI & \multicolumn{1}{l|}{0.0052} & \multicolumn{1}{l|}{0.0742} & 0.2593 & \multicolumn{1}{l|}{0.0090} & \multicolumn{1}{l|}{0.1269} & \multicolumn{1}{l|}{0.2937} & 5.5300 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & G & \multicolumn{1}{l|}{0.0077} & \multicolumn{1}{l|}{0.1085} & 0.4009 & \multicolumn{1}{l|}{0.0099} & \multicolumn{1}{l|}{0.1390} & \multicolumn{1}{l|}{0.4375} & 5.2924 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & H & \multicolumn{1}{l|}{0.0159} & \multicolumn{1}{l|}{0.2239} & 0.4584 & \multicolumn{1}{l|}{0.0393} & \multicolumn{1}{l|}{0.5520} & \multicolumn{1}{l|}{0.4938} & 15.930 \\ \cline{5-5}
\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}J} & \multicolumn{1}{l|}{\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}0.007}} & \multicolumn{1}{l|}{\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}0.098}} & \multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}1.088} & I & \multicolumn{1}{l|}{0.0308} & \multicolumn{1}{l|}{0.4328} & 0.3818 & \multicolumn{1}{l|}{0.0397} & \multicolumn{1}{l|}{0.5564} & \multicolumn{1}{l|}{0.3954} & 17.410 \\ \hline
\end{tabular}
}
%\vspace{-3mm}
\end{table*}



\begin{table*}[t]
%\vspace{-2.5mm}
\caption{Brief summary of the latent space causal discovery test.}
\label{tab:discv}
\vspace{-2mm}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{|c|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
\cellcolor[HTML]{C0C0C0}Edge & \cellcolor[HTML]{FFCCC9}A$\rightarrow$C & \cellcolor[HTML]{FFCCC9}B$\rightarrow$D & \cellcolor[HTML]{FFCCC9}C$\rightarrow$D & \cellcolor[HTML]{FFCCC9}C$\rightarrow$G & \cellcolor[HTML]{FFCCC9}D$\rightarrow$G & \cellcolor[HTML]{FFCCC9}G$\rightarrow$J & \cellcolor[HTML]{FFCCC9}D$\rightarrow$H & \cellcolor[HTML]{FFCCC9}H$\rightarrow$J & \cellcolor[HTML]{FFCE93}B$\rightarrow$E & \cellcolor[HTML]{FFCE93}E$\rightarrow$G & \cellcolor[HTML]{FFCE93}E$\rightarrow$H & \cellcolor[HTML]{FFCE93}C$\rightarrow$E & \cellcolor[HTML]{ABE9E7}E$\rightarrow$F & \cellcolor[HTML]{ABE9E7}F$\rightarrow$I & \cellcolor[HTML]{ABE9E7}I$\rightarrow$J & \cellcolor[HTML]{ABE9E7}D$\rightarrow$I \\ \hline
\cellcolor[HTML]{C0C0C0}KLD & 7.63 & 8.51 & 10.14 & 11.60 & 27.87 & 5.29 & 25.19 & 15.93 & 37.07 & 39.13 & 39.88 & 46.58 & 53.68 & 45.64 & 17.41 & 75.57 \\ \hline
\rowcolor[HTML]{FFFFFF} 
\cellcolor[HTML]{C0C0C0}Gain & 7.63 & 8.51 & 1.135 & 11.60 & 2.454 & 5.29 & 25.19 & 0.209 & 37.07 & -5.91 & -3.29 & 2.677 & 53.68 & 45.64 & 0.028 & 3.384 \\ \hline
\end{tabular}}
%\vspace{-5mm}
\end{table*}

\section{Conclusions}\label{sec:conclusion}
%\vspace{-2mm}


This paper proposes a dimensionality framework to decompose our intrinsic understanding of relationships, from a novel \emph{Relation-Oriented} perspective. 
Specifically, the unobservable relations in causal knowledge are interpreted as informative distributions within $\mathbb{R}^H$, and the human-featured counterfactual comprehension, i.e., the ``what if'' queries, are symbolized as nonlinear distributions spinning relative timing axes within $\mathbb{R}^T$.
These reveal the fundamental oversights within our current \emph{Observation-Oriented} modeling paradigm, which is based on the observational i.i.d. assumption and confined within $\mathbb{R}^O$. 
It relies on manual specification to identify dynamical effects from observational static sequences, inherently fraught with difficulty.
%Specifically, based on the observational i.i.d. assumption, conventional relationship modeling intrinsically overlooks 1) the informative unobservables in $\mathbb{R}^H$, and 2) the structuralized dynamics within multi-dimensional $\mathbb{R}^T$. Instead, due to being confined within $\mathbb{R}^O$, 

When viewed through the lens of the \emph{Relation-Oriented} framework, the multifaceted issues surrounding causality learning become unified, addressing common confusions and concerns from traditional causal inference to modern LLMs. Recalling the queries presented in the Introduction, we systematically summarize these application-related restrictions in the pursuit of AGI, offering new insights as follows:
\begin{enumerate}[itemsep=0em, topsep=-1pt,
parsep=2pt, partopsep=0pt,
leftmargin=22pt, labelwidth=10pt]
\item[\ding{118}] \emph{Firstly}, challenges in causal inference primarily arise from overlooking effect dynamics, due to the linear modeling constraint. This oversight leads to compensatory efforts in various aspects, such as dealing with hidden confounders and relying on the causal sufficiency assumption. Causal DAGs naturally offer a \emph{Relation-Oriented} view; with the proposed enhancement, they can provide fundamental support.
\item[\ding{118}] \emph{Secondly}, undetectable hierarchical levels, symbolized as the hidden relation $\omega\in \mathbb{R}^H$, are inherent in our knowledge. These levels drive the need for model generalizability. With AI's capability to capture dynamics, the main challenge is incorporating structural causal knowledge to achieve generalizable causal reasoning in AI. The current paradigm struggles with identifying dynamic effects, leading to inherent biases, while transitioning to knowledge-aligned modeling suggests a shift to the new paradigm.
\item[\ding{118}] \emph{Thirdly}, although existing language models, through meta-learning, have achieved more generalizable context associations, they remain limited to observational space bound to absolute timing, and are far from structuralized ``comprehension'' underpinned by relative timings in our cognitive framework. Nevertheless, LLMs have demonstrated the effectiveness of meta-learning across temporal dimensional hierarchies, indicating the potential of \emph{Relation-Oriented} meta-learning in the pursuit of AGI.
\end{enumerate}

We also introduce a baseline implementation of the \emph{Relation-Oriented} paradigm, primarily to validate the efficacy of the ``relation-indexing'' methodology in implementing causal representations on demand and constructing knowledge-aligned hierarchies. In certain domains with well-established structural knowledge, similar approaches have been effectively attempted, such as the introduction of hierarchical temporal memory in neuroscience \cite{wu2018hierarchical}. The pursuit of AGI is a historically extensive and complex endeavor, requiring a wide array of knowledge-aligned AI model constructions. This study aims to provide foundational insights for future developments in this field.




% Driven by the misalignment issues between causal knowledge and established causal models in widespread AI applications, this study examines fundamental limitations of the dominant \emph{Observation-Oriented} learning paradigm. In response, we advocate for a novel \emph{Relation-Oriented} paradigm, inspired by the relation-centric nature of human knowledge, and complemented by a practical approach of \emph{Relation-Indexed Representation Learning} (RIRL), with demonstrated efficacy.

% The concept of a ``hyper-dimension'' is initially proposed, as an accommodation for unobservable knowledge. We subsequently build a comprehensive framework of dimensionality, to offer more intuitive insights into relationship learning. 
% The discrepancy, between our comprehension of ``time'' and the single timeline used in our causal models, inherently causes misalignment, and results in model generalizability issues.

% \emph{Relation-Oriented} reflects the process of human understanding, 
% aims to mitigate AI misalignment, paving the way toward causally interpretable AGI. Constructing AGI is a long-term, intricate process requiring extensive work within interdisciplinary efforts, and we seek to lay a foundation for its future advancements.



\ifpreprint
\vspace{10mm}
\section*{Acknowledgements}
\vspace{-2mm}

I'd like to extend my heartfelt thanks to my friend, Dr. Gao, Qiman, the lone companion willing to engage in profound philosophical discussions with me, and who has provided invaluable advice. Additionally, my gratitude goes to GPT-4 for its assistance in enhancing my English writing.
I also wish to thank my advisor, Prof. Vipin Kumar, for the initial support in the beginning stage of this work.

\hfill Jia Li, Nov 2023

\vspace{80mm}

\else
%\vspace{82mm}
\pagebreak
\fi

\bibliography{main}
\bibliographystyle{tmlr}


\appendix
\section{Appendix: Complete Experimental Results of Causal Discovery}
%\includepdf[pages=-]{mainz_append_pdf.pdf}

\pagebreak


%\newpage
\begin{sidewaystable}
%\begin{table*}[t]
\centering
\caption{The Complete Results of Heuristic Causal Discovery in latent space.
Each row stands for a round of detection, with `\#'' identifying the round number, and all candidate edges are listed with their KLD gains as below. 1) Green cells: the newly detected edges. 2) Red cells: the selected edge.
3) Blue cells: the trimmed edges accordingly. }
%\vspace{-20mm}
\label{tab:discv_rounds}
\resizebox{1\columnwidth}{!}{%
%\rotatebox{90}{

\begin{tabular}{|c|c|c|cccccccccccccc}
\cline{1-9}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \textbf{A$\rightarrow$ C}} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ D & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{B$\rightarrow$ C}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ D} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 1}} & \cellcolor[HTML]{FFCCC9}{\color[HTML]{000000} \textbf{7.6354}} & 19.7407 & \multicolumn{1}{c|}{60.1876} & \multicolumn{1}{c|}{119.7730} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{8.4753}} & \multicolumn{1}{c|}{8.5147} & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} &  &  &  &  &  &  &  &  \\ \cline{1-13}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ D & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{B$\rightarrow$ D}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ D} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 2}} & 19.7407 & 60.1876 & \multicolumn{1}{c|}{119.7730} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{8.5147}}} & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}10.1490} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}46.5876} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}111.2978} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}11.6012} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}39.2361} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}95.1564} &  &  &  &  \\ \hline
\rowcolor[HTML]{C0C0C0} 
\cellcolor[HTML]{C0C0C0} & \textbf{A$\rightarrow$ D} & A$\rightarrow$ E & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{C$\rightarrow$ D}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 3}} & \cellcolor[HTML]{CBCEFB}\textbf{9.7357} & 60.1876 & \multicolumn{1}{c|}{119.7730} & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{1.1355}}} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{11.6012} & \multicolumn{1}{c|}{39.2361} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}63.7348} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}123.3203} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}27.8798} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}25.1988} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}75.5775} \\ \hline
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \textbf{C$\rightarrow$ G}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 4}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{000000} \textbf{11.6012}}} & \multicolumn{1}{c|}{39.2361} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{27.8798} & \multicolumn{1}{c|}{25.1988} & \multicolumn{1}{c|}{75.5775} &  &  \\ \cline{1-15}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{D$\rightarrow$ G}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}G$\rightarrow$ J} &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 5}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{39.2361} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{2.4540}}} & \multicolumn{1}{c|}{25.1988} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}5.2924} &  &  \\ \cline{1-15}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{G$\rightarrow$ J}}} &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 6}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{39.2361} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{25.1988} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{5.2924}}} &  &  &  \\ \cline{1-14}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{C$\rightarrow$ H}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{D$\rightarrow$ H}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 7}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{39.2361}} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{25.1988}}} & \multicolumn{1}{c|}{75.5775} &  &  &  &  \\ \cline{1-13}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{H$\rightarrow$ J}}} &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 8}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{0.2092}}} &  &  &  &  &  \\ \cline{1-12}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}\textbf{A$\rightarrow$ E} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{C$\rightarrow$ E}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 9}} & \cellcolor[HTML]{CBCEFB}\textbf{60.1876} & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{46.5876}}} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} &  &  &  &  &  &  \\ \cline{1-13}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{B$\rightarrow$ E}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{D$\rightarrow$ E}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ I} &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 10}} & 119.7730 & \cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{-6.8372}} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{17.0407}} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}53.6806} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}-5.9191} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}-3.2931} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}110.2558} &  &  &  &  \\ \cline{1-13}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \cellcolor[HTML]{C0C0C0}B$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{E$\rightarrow$ G}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ I} &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 11}} & 119.7730 & 132.7717 & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{53.6806} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{-5.9191}} & \multicolumn{1}{c|}{-3.2931} & \multicolumn{1}{c|}{110.2558} &  &  &  &  &  &  \\ \cline{1-11}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \cellcolor[HTML]{C0C0C0}B$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{E$\rightarrow$ H}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ I} &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 12}} & 119.7730 & 132.7717 & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{53.6806} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{-3.2931}} & \multicolumn{1}{c|}{110.2558} &  &  &  &  &  &  &  \\ \cline{1-10}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}\textbf{A$\rightarrow$ F} & \cellcolor[HTML]{C0C0C0}\textbf{B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{C$\rightarrow$ F}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{D$\rightarrow$ F}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{E$\rightarrow$ F}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ I} &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 13}} & \cellcolor[HTML]{CBCEFB}\textbf{119.7730} & \cellcolor[HTML]{CBCEFB}\textbf{132.7717} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{111.2978}} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{123.3203}} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{53.6806}} & \multicolumn{1}{c|}{110.2558} &  &  &  &  &  &  &  &  \\ \cline{1-9}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}C$\rightarrow$ I & \cellcolor[HTML]{C0C0C0}D$\rightarrow$ I & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{E$\rightarrow$ I}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{F$\rightarrow$ I}} &  &  &  &  &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 14}} & 95.1564 & 75.5775 & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{110.2558}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{45.6490}} &  &  &  &  &  &  &  &  &  &  &  &  \\ \cline{1-5}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}C$\rightarrow$ I & \cellcolor[HTML]{C0C0C0}D$\rightarrow$ I & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{I$\rightarrow$ J}} &  &  &  &  &  &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 15}} & 15.0222 & 3.3845 & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.0284}} &  &  &  &  &  &  &  &  &  &  &  &  &  \\ \cline{1-4}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}\textbf{C$\rightarrow$ I} & \cellcolor[HTML]{C0C0C0}\textbf{D$\rightarrow$ I} &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 16}} & \cellcolor[HTML]{CBCEFB}\textbf{15.0222} & \cellcolor[HTML]{FFCCC9}\textbf{3.3845} &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\ \cline{1-3}
\end{tabular}

}%}
%\vspace{-3mm}
%\end{table*}

\end{sidewaystable}







\begin{sidewaystable}
%\centering

\caption{Average performance of 10-Fold FGES (Fast Greedy Equivalence Search) causal discovery, with the prior knowledge that each node can only cause the other nodes with the same or greater depth with it. An edge means connecting two attributes from two different nodes, respectively. Thus, the number of possible edges between two nodes is the multiplication of the numbers of their attributes, i.e., the lengths of their data vectors.
\\ (All experiments are performed with 6 different Independent-Test kernels, including chi-square-test, d-sep-test, prob-test, disc-bic-test, fisher-z-test, mvplr-test. But their results turn out to be identical.)}
\label{tab:fges}
\resizebox{1\columnwidth}{!}{%
%\Rotatebox{90}{
%
\begin{tabular}{cccccccccccccccccc}
\hline
\rowcolor[HTML]{C0C0C0} 
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}Cause Node} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A} & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}B} & \multicolumn{3}{c|}{\cellcolor[HTML]{C0C0C0}C} & \multicolumn{3}{c|}{\cellcolor[HTML]{C0C0C0}D} & \multicolumn{3}{c|}{\cellcolor[HTML]{C0C0C0}E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}F} & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}I} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}True\\ Causation\end{tabular}} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}A$\rightarrow$ C} & \multicolumn{1}{c|}{B$\rightarrow$ D} & \multicolumn{1}{c|}{B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}C$\rightarrow$ D} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}C$\rightarrow$ G} & \multicolumn{1}{c|}{D$\rightarrow$ G} & \multicolumn{1}{c|}{D$\rightarrow$ H} & \multicolumn{1}{c|}{D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}E$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}E$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}E$\rightarrow$ H} & \multicolumn{1}{c|}{F$\rightarrow$ I} & \multicolumn{2}{c|}{\cellcolor[HTML]{EFEFEF}G$\rightarrow$ J} & \multicolumn{1}{c|}{H$\rightarrow$ J} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}I$\rightarrow$ J} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Number of\\ Edges\end{tabular}} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}16} & \multicolumn{1}{c|}{24} & \multicolumn{1}{c|}{16} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}6} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}4} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8} & \multicolumn{1}{c|}{12} & \multicolumn{1}{c|}{12} & \multicolumn{1}{c|}{9} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8} & \multicolumn{1}{c|}{12} & \multicolumn{2}{c|}{\cellcolor[HTML]{EFEFEF}4} & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}3} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Probability\\ of Missing\end{tabular}} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.038889} & \multicolumn{1}{c|}{0.125} & \multicolumn{1}{c|}{0.125} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.062} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.06875} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.039286} & \multicolumn{1}{c|}{0.069048} & \multicolumn{1}{c|}{0.2} & \multicolumn{1}{c|}{0.142857} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.3} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.003571} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.2} & \multicolumn{1}{c|}{0.142857} & \multicolumn{2}{c|}{\cellcolor[HTML]{EFEFEF}0.0} & \multicolumn{1}{c|}{0.072727} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.030303} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Wrong \\ Causation\end{tabular}} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}C$\rightarrow$ F} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{D$\rightarrow$ E} & \multicolumn{1}{c|}{D$\rightarrow$ F} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{F$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}G$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}G$\rightarrow$ I} & \multicolumn{1}{c|}{H$\rightarrow$ I} &  \\ \cline{1-1} \cline{5-5} \cline{9-10} \cline{14-17}
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Times\\ of Wrongly\\ Discovered\end{tabular}} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}5.6} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{1.2} & \multicolumn{1}{c|}{0.8} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{5.0} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8.2} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}3.0} & \multicolumn{1}{c|}{2.8} &  \\ \cline{1-1} \cline{5-5} \cline{9-10} \cline{14-17}
\multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{}
\end{tabular}
%}
}


\vspace{0.4in}
%\end{table*}

%\begin{table*}[t]
%\centering
\caption{ Brief Results of the Heuristic Causal Discovery in latent space, identical with Table 3 in the paper body, for better comparison to the traditional FGES methods results on this page.\\
The edges are arranged in detected order (from left to right) and their measured causal strengths in each step are shown below correspondingly.\\
Causal strength is measured by KLD values (less is stronger). Each round of detection is pursuing the least KLD gain globally. All evaluations are in 4-Fold validation average values. Different colors represent the ground truth causality strength tiers (referred to  the Figure 10 in the paper body).
}
\label{tab:discv}
%\resizebox{0.1\columnwidth}{!}{%
%\Rotatebox{90}{
\vspace{2mm}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{|c|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
\cellcolor[HTML]{C0C0C0}Causation & \cellcolor[HTML]{FFCCC9}A$\rightarrow$ C & \cellcolor[HTML]{FFCCC9}B$\rightarrow$ D & \cellcolor[HTML]{FFCCC9}C$\rightarrow$ D & \cellcolor[HTML]{FFCCC9}C$\rightarrow$ G & \cellcolor[HTML]{FFCCC9}D$\rightarrow$ G & \cellcolor[HTML]{FFCCC9}G$\rightarrow$ J & \cellcolor[HTML]{FFCCC9}D$\rightarrow$ H & \cellcolor[HTML]{FFCCC9}H$\rightarrow$ J & \cellcolor[HTML]{FFCE93}C$\rightarrow$ E & \cellcolor[HTML]{FFCE93}B$\rightarrow$ E & \cellcolor[HTML]{FFCE93}E$\rightarrow$ G & \cellcolor[HTML]{FFCE93}E$\rightarrow$ H & \cellcolor[HTML]{ABE9E7}E$\rightarrow$ F & \cellcolor[HTML]{ABE9E7}F$\rightarrow$ I & \cellcolor[HTML]{ABE9E7}I$\rightarrow$ J & \cellcolor[HTML]{ABE9E7}D$\rightarrow$ I \\ \hline
\cellcolor[HTML]{C0C0C0}KLD & 7.63 & 8.51 & 10.14 & 11.60 & 27.87 & 5.29 & 25.19 & 15.93 & 46.58 & 65.93 & 39.13 & 39.88 & 53.68 & 45.64 & 17.41 & 75.57 \\ \hline
\rowcolor[HTML]{FFFFFF} 
\cellcolor[HTML]{C0C0C0}Gain & 7.63 & 8.51 & 1.135 & 11.60 & 2.454 & 5.29 & 25.19 & 0.209 & 46.58 & -6.84 & -5.91 & -3.29 & 53.68 & 45.64 & 0.028 & 3.384 \\ \hline
\end{tabular}
%}}
%\end{table*}
}

\end{sidewaystable}








\end{document}
