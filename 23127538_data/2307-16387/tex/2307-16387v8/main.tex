
\documentclass[10pt]{article} % For LaTeX2e
%\usepackage{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}

% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{hyperref}
\usepackage{url}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{adjustbox}
%\usepackage{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{centernot}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{color}
%\usepackage{booktabs}

\usepackage{subfigure}
\usepackage{caption}
\usepackage{dsfont}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{empheq}
%\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{totcount}

\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{makecell}
%\usepackage[table]{xcolor}
\usepackage{xcolor}
\usepackage{changepage}
\usepackage{stfloats}
%\usepackage{cite}
\usepackage{soul}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{algpseudocode}
\usepackage[title]{appendix}
\usepackage{pdfpages}

\usepackage{bm}
\usepackage{amsmath,amssymb,amsfonts}
% \usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{bookmark}
\usepackage{float}
\usepackage{tabularx}
\usepackage{mathtools}
\usepackage{array}
\usepackage{rotating}
\usepackage[skins]{tcolorbox}
\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{ \node[shape=circle,draw,inner sep=1.5pt] (char) {#1};}}

\title{Relation-Oriented: Toward Causal Knowledge-Aligned AGI}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.


\author{
\name Jia Li \email jiaxx213@umn.edu \\
      \addr Department of Computer Science,
      University of Minnesota
      \vspace{-3mm}
      \AND
      \name Xiang Li \email lixx5000@umn.edu \\
      \addr Department of Bioproducts and Biosystems Engineering,       University of Minnesota
      % \name Xiaowei Jia \email xiaowei@pitt.edu\\
      % \addr Department of Computer Science\\ University of Pittsburgh
      }

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{Apr}  % Insert correct month for camera-ready version
\def\year{2023} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version

\definecolor{thegrey}{RGB}{192,192,192}
\definecolor{orchid}{RGB}{204,153,255}

\begin{document}


\maketitle
\vspace{-3mm}

\begin{abstract}
\vspace{-4mm}

\emph{Observation-Oriented} paradigm currently dominates relationship learning models, including AI-based ones, which inherently do not account for relationships with temporally nonlinear effects.
Instead, this paradigm simplifies ``temporal dimension'' to be a \emph{linear observational} timeline, necessitating the prior identification of effects with specific timestamps. Such constraints lead to \emph{identifiability difficulties} for dynamical effects, thereby overlooking the potentially crucial temporal nonlinearity of the modeled relationship.
Moreover, the \emph{multi-dimensional} nature of Temporal Feature Space is largely disregarded, introducing \emph{inherent biases} that seriously compromise the robustness and generalizability of relationship models. This limitation is particularly pronounced in large AI-based causal applications.

%timestamp specification for entities in the relationship (e.g., causes and effects)
%It naturally prioritizes timestamp specification for modeling entities (e.g., causes and effects)
%Additionally, this paradigm neglects the multi-dimensional space that accommodates temporal distributional features, introducing inherent biases that affect the robustness and generalizability of relationship models,

Examining these issues through the lens of a \emph{dimensionality framework}, a fundamental misalignment is identified between our \emph{relation}-indexing comprehension of knowledge and the current modeling paradigm. To address this, a new \emph{Relation-Oriented} paradigm is raised, aimed at facilitating the development of causal knowledge-aligned Artificial General Intelligence (AGI). As its methodological counterpart, the proposed \emph{Relation-Indexed Representation Learning} (RIRL) is validated through efficacy experiments.

\vspace{-2mm}
\end{abstract}

\vspace{-3mm}
\section{Introduction}
\vspace{-2.5mm}

\label{sec:intro}


The current modeling paradigm requires prior identification of variables and outcomes as a prerequisite for constructing the relationship over them, typically based on observational independent and identical distributions (i.i.d.). With respect to the time evolution of these i.i.d.s, the Picard-Lindelof theorem, introduced in the 1890s, established a \emph{logical timeline} $t$ for recording observational timestamps, thereby initiating the $x_{t+1}=f(x_t)$ paradigm to depict the time evolution of variable $X$. 
Since then, this \textbf{\emph{Observation-Oriented}} principle %where the temporal dimensional distribution is represented by counts of predetermined time-lag unit $\{t, t+1\}$, 
has been a conventional approach to relationship learning.

To model a causal relationship $X\rightarrow Y$, the AI-based RNN models act as state-of-the-art \cite{shojaie2022granger}, especially for capturing nonlinear features of the causes. They typically formulate as $y_{t+m}=f(\{x_t\})$, where $\{x_t\}=\{x_1, \ldots, x_{t}, x_{t+1}, \ldots, x_T\}$ denotes a time sequence of $X$ of length $T$, with a predetermined time progress $m$ from $X$ to $Y$. 
In this approach, the temporal distribution of cause $X$ is explicitly included, while outcome $Y$ strictly presents as observational, associated with a specific timestamp. This way leaves all potentially significant dynamics of effects entirely managed by $f(\cdot)$. 
However, whether the selected function $f(\cdot)$ is \emph{linear} or \emph{nonlinear} influences only the dimensionality of $\mathbb{R}^d$, where $X\in \mathbb{R}^d$. Consequently, the time evolution from $t$ to ${t+m}$ for the effect entity $Y$ remains invariably \emph{\textbf{linear}}.

Not due to specific models, such limitation on capturing temporal nonlinearities results from the prevailing \emph{Observation-Oriented} paradigm. Specifically, before modeling the relationship, it requires manual identification of the effect in specific timestamps, thus posing difficulties when the effect presents diverse dynamical features \cite{zhang2012identifiability}.
%It is particularly evident through a common problem in causal learning, the \emph{identifiability difficulty} of causal effects.
While the paradigm may have been adequate in the past, it no longer suffices given the advancements in data collection and Artificial Intelligence (AI) methods. The reliance on i.i.d. observations, coupled with the growing necessity for capturing \emph{\textbf{dynamics}} (i.e., \emph{temporal \textbf{nonlinearities}} \cite{granger1993modelling}), underscores the need for a new modeling paradigm \cite{scholkopf2021toward}.

Drawing inspiration from the relation-centric nature of human comprehension \cite{sep-mental-representation}, this study presents a \emph{dimensionality framework}. This offers a renewed perspective on the concept of ``relationship'' within the modeling context, underscoring the vital \emph{\textbf{indexing}} role of unobservable relations in capturing observable entities, especially temporal linearities.
%This framework reflects the relation-indexing nature of human understanding.
The unique viewpoint uncovers a fundamental misalignment between our intuitive understanding of knowledge and the prevailing relationship learning paradigm, resulting in \emph{inherent biases} within models. This issue plays a significant role in several challenges, such as the difficulty of generalizing causal models \cite{scholkopf2021toward}, the limited effectiveness in leveraging causal knowledge within AI \cite{luo2020causal}, and certain phenomena associated with AI Alignment problems \cite{christian2020alignment}.
%discrepancy

The remainder of this Introduction (subsection \ref{sec:hyper}-\ref{sec:1_4}) lays the groundwork for the \emph{dimensionality framework} used throughout this study.
Chapter I (Sections 2-4) examines the inherent limitations of \emph{Observation-Oriented} relationship learning, particularly its oversight of multi-dimensional dynamics in causal effects. The chapter also introduces the \textbf{\emph{Relation-Oriented}} paradigm, which is crafted to reflect human understanding.
Chapter II (Sections 5-7) concentrates on the \emph{Relation-Indexed Representation Learning} (RIRL) method as a practical realization of the proposed new paradigm, accompanied by efficacy experiments. 

% Particularly, the linear absolute timeline $t$ that we conventionally use inherently fails to capture dynamics of causal effects within the multi-dimensional temporal feature space (see subsection \ref{sec:1_3}). This limitation leads to biases, resulting in AI models misaligned with our cognitive understanding \cite{christian2020alignment} and challenging to generalize \cite{scholkopf2021toward}. 



\vspace{-2mm}
\subsection{Dimensionality Framework}
\label{sec:hyper}
\vspace{-1mm}

Consider a pairwise relationship comprised of three elements: two \emph{\textbf{observable}} entities, and a relation derived from our knowledge to connect them. These two entities can be featured as observational only (e.g., images, spatial coordinates of a quadrotor, etc.), or observational-and-temporal (e.g., trends of stocks, a quadrotor's movement in one hour, etc.).
However, the ``relation'' has to be \emph{\textbf{unobservable}} to make this relationship model \emph{informative}, to be distinguished from mere statistical dependency between two observables. 

This principle was initially introduced in the form of Common Cause \cite{dawid1979conditional, scholkopf2021toward}, suggesting that any nontrivial conditional independence between two observables requires a third, mutual cause (i.e., our unobservable ``relation'').
Take the relationship ``Bob has a son named Jim'' as an example. The father-son relation is unobservable information that exists in our knowledge, which can also be seen as the common cause that makes their connection unique rather than any random pairing of ``Bob'' and ``Jim''. Given sufficiently observed social activities, AI may deduce this pair of ``Bob'' and ``Jim'' are particularly associated, but that does not equate to discerning the father-son relation between them. 

Put simply, the information contained by a relationship model stems from unobservable knowledge (referred to as ``relation'') rather than associated direct observations.
Consider model $Y=f(X;\theta)$ with $\theta$ indicating the function parameter in demand. In the context of modeling, the term ``relation'' can be represented by $\theta$.

% Figure environment removed
\vspace{-2mm}

Therefore, in modeling, a relationship can be interpreted as a joint distribution spanning multiple dimensions. The observational and temporal dimensions include the entities (i.e., $X$ and $Y$), while the unobservable relation (i.e., the modeling objective $\theta$) manifests as an unseen distribution in a \emph{\textbf{hyper-dimension}}. Figure~\ref{fig:space} organizes our cognitive space, which stores knowledge, into three sections accordingly. The hyper-dimensional space represents the aggregate of all unobservable relations in our knowledge. For a model to be practically valuable, it must accurately reflect our understanding. Similarly, a successful AGI, to meet our expectations, must be rooted in existing knowledge. Specifically, it should represent relations residing in the unobservable Hyper-dimensional Space, through which reasonable interpretations of observable entities can be generated.


In this paper, ``feature'' refers to a variable fully representing a distribution of interest in any dimension, while the observational-temporal joint space is sometimes called ``observable data space'', contrasting with ``latent feature space''.



\vspace{-1mm}
\subsection{Observational and Temporal Feature Spaces}
\label{sec:1_3}
\vspace{-2mm}

Most relationship models function within the Observational Space, maybe incorporating a timeline to depict the observational evolution over time. For example, Convolutional Neural Networks (CNNs) recognize pixel associations in purely observational space; a quadrotor's movement is identifiable in a sequence of spatial coordinates; Large Language Models (LLMs) operate along a semantic timeline representing phrase order; and patients' vital signs are recorded chronologically. 
The latter three applications fall under the category of ``spatial-temporal'' analysis \cite{alkon1988spatial, turner1990spatial, andrienko2003exploratory}, where the ``temporal dimension'' is often equated with the observational timeline within the data \cite{gurnee2023language}.

However, our cognitive understanding of ``time'', serving as the foundation to construct knowledge, differs from this approach \cite{coulson2009understanding}.
Observational data timestamps are referred to as the \emph{\textbf{absolute}} timeline \cite{wulf1994reducing}, while in comprehension, \emph{multiple \textbf{relative}} timelines can coexist. Each of these relative timelines represents different causal effects and may exert mutual influences \cite{shea2001effects}.
Additionally, from a modeling perspective, data timestamps are indistinguishable from other observational attributes \cite{shea2001effects}. Consequently, we categorize the \emph{absolute} $t$-timeline as a dimension in the Observational Space (as depicted in Figure \ref{fig:space}); meanwhile, address knowledge-aligned \emph{temporal distributions} separately in a distinct Temporal Space, which naturally possesses \emph{\textbf{multi-dimensions}}, defined by the potential \emph{relative} timelines present.

A \emph{linear} causal relationship implies a \emph{\textbf{static}} effect that can be specified by a particular timestamp, e.g., in the statement “rain leads to wet floors,” the effect of “wet floors” is static, captured at a specific moment. When this effect has significant \emph{\textbf{dynamical}} features - e.g., ``floors becoming progressively wetter'' is dynamic due to its sequential temporal pattern - a temporal distribution must be considered, transforming the relationship into a temporally \emph{nonlinear} one \cite{granger1993modelling}.

Under the \emph{Observation-Oriented} paradigm, prior identification of effects for dynamics is notably difficult (see subsection \ref{sec:causality3_2} for further discussions). This neglect of temporal nonlinearity and oversight of relative timelines can lead to \emph{\textbf{inherent bias}} (as demonstrated in subsection \ref{sec:CRB}), thereby compromising the generalizability of causal models (see subsection \ref{sec:generalize}). While such misalignments might have been subtle in the past, the advent of AI enables large-scale models more efficiently, and its black-box nature allows these biases to accumulate exponentially inside, eventually resulting in uninterpretable outputs.
%indispensable 
%accentuated
%disparity 

\vspace{-1mm}
\subsection{Hyper-Dimensional Feature Space}
\label{sec:1_4}
\vspace{-2mm}

In Hyper-dimensional Space (denoted as $\mathbb{R}^h$), unobservable relations include not only the modeling objective $\theta$, but also other ones that play crucial roles for the model.
Consider $\langle \theta,\omega\rangle$ to be jointly distributed in $\mathbb{R}^h$, connecting observables $X$ and $Y$. While the model $Y=f(X;\theta)$ aims to obtain $\theta$ using given $X$ and $Y$, the unseen $\omega$ can imply various application scenarios that necessitate the model's \emph{\textbf{generalizability}}.

For instance, consider an examination of how family income levels (denoted as $X$) influence grocery shopping frequencies (as $Y$), with influence represented by $\theta$. Underlying cultural factors (denoted as $\omega$) also play a role, such that the established model $Y=f(X;\theta)$ proves to be practically useful only when conditioned on a specific country (represented by a particular $\omega$ value).
In this context, there are two levels of objective relation: a global-level $\theta$ without considering $\omega$, and a local-level $\theta$ conditioned on a specified $\omega$ value.

To be \emph{generalizable} is to traverse these levels effectively, thereby allowing lower-level learned relationships to inform or be reusable for higher-level learnings \cite{scholkopf2021toward}. This also encompasses the capability to \emph{individualize} inversely from higher to lower levels for different $\omega$ values. For simplicity, we refer to $\omega$ as the \emph{\textbf{hidden relation}} and the resulting unseen levels as the \emph{\textbf{unobservable hierarchy}}.

% Such hierarchies are common in learning tasks and hold various meanings in different applications. For instance, they may signify levels of granularity (e.g., population vs. individual), as illustrated in subsection \ref{sec:temp_hier}, or denote decision-making dependencies, as seen in subsection \ref{sec:obs_hier}. 


% These hidden relations, while unobservable to AI, exist within our knowledge. Accordingly, their absence may lead to our current \emph{Observation-Oriented} models \emph{misaligned} with our anticipated understanding. While this absence can often be resolved when the modeling entities are purely observational (refer to subsection \ref{sec:obs_hier}), it becomes a noticeable inherent deficiency under the current paradigm when critical temporal dynamics are involved (refer to subsections \ref{sec:temp_hier} and \ref{sec:confounder}).



% \vspace{-2mm}

% \subsection{Manifestation of AI Misalignment}
% \vspace{-2mm}

% AI has displayed capabilities surpassing humans in observational learning tasks, such as generating images, Go gaming (in a single absolute timeline), etc, but may appear ``unintelligent'' in comprehending some knowledge humans find intuitive. For instance, AI-created personas on social media can have realistic faces but barely with the presence of hands, due to AI treating them as arbitrary assortments of finger-like items. %struggling with the complex structure, instead treating hands 

% Moreover, when it comes to time evolution, causal reasoning presents a substantial challenge for AI. 
% Despite valuable contributions from traditional causal learning methods \cite{wood2015lesson, vukovic2022causal, ombadi2020evaluation}, and the rise of neural network applications tackling large-scale causal questions \cite{luo2020causal}, limitations in model generalizability persist \cite{scholkopf2021toward}.
% Accordingly, our causal model applications are often context-specific, and AI's nonlinear learning capability remains constrained on the temporal dimension.


% The questions ``How to utilize AI in causality'' and ``How to simulate reasonable hands'' may seemly pertain to specific domains such as causal inference and computer vision. However, they fundamentally converge toward the broader challenge of AI Alignment, encapsulated by the essential question: ``Why some relations in knowledge are unseen to AI?'', which is increasingly critical to address for today \cite{christian2020alignment}.
% %Reflecting on Dr. Geoffrey Hinton's warning, the misalignment of AI capabilities with human values can result in unintended and potentially harmful consequences. It is increasingly critical to address this essential question.



\begin{center}
   {\vskip 8pt\large\bf Chapter I: Limitations of Current Observation-Oriented Paradigm} 
\end{center}



%, facilitating understanding of observational and temporal entities. This nature creates a fundamental misalignment with the prevailing modeling paradigm, which prioritizes observational entities as variables and outcomes. 
Human understanding inherently indexes through relations \cite{sep-mental-representation}, directing to mental representations 
about observational and temporal entities. 
This intrinsic characteristic results in a fundamental misalignment with the \emph{Observation-Oriented} modeling paradigm, evident through various application issues.

Section \ref{sec:Hierarchy} explores the impacts of hidden relations on relationship learning and introduces the relation-indexing approach as a solution. Section \ref{sec:Causality} underscores the importance of effect dynamics and the challenges of manual identification in causal learning. Finally, Section \ref{sec:Temporal} highlights the profound implications of overlooking multi-dimensional effect dynamics.

%this discrepancy 

%This chapter explores the influences of hidden relations under the current paradigm (Section \ref{sec:Hierarchy}), re-evaluates causal learning in light of often-overlooked critical temporal features (Section \ref{sec:Causality}), and highlights the multi-dimensionality of the temporal feature space, along with the inherent biases it introduces (Section \ref{sec:Temporal}).


\vspace{-2mm}
\section{Impact of Hidden Relations}
\vspace{-1mm}
\label{sec:Hierarchy}


% % Figure environment removed

Hidden relations imply the existence of unobservable hierarchies. In strictly observational learning tasks, features across various levels can be fully captured, setting off the hidden relations as distinct observable misalignment (subsection \ref{sec:obs_hier}).
%these hidden relations may give rise to intuitive misalignment problems without preventing models from capturing complete observations
However, in relationship learning tasks characterized by temporal dimensions, the complete range of temporal dynamics can hardly be covered (subsection \ref{sec:temp_hier}), 
leading to observable information loss and increased complexity in causal learning (subsection \ref{sec:confounder}).

% Unobservable hierarchies indicate hidden relations, vital but separate from the modeling objective.
% For tasks solely involving observational learning, such information absence might be resolved by leveraging knowledge to enhance modeling (subsection \ref{sec:obs_hier}). 
% However, when it comes to temporally significant causal learning, these hierarchies may lead to a fundamental loss of dynamical features in the temporal dimension (subsection \ref{sec:temp_hier}), presenting a substantial challenge to conventional causal inference methods (subsection \ref{sec:confounder}).


\vspace{-2mm}
\subsection{On Observational Learning}
\label{sec:obs_hier}
\vspace{-2mm}

% Figure environment removed

Figure \ref{fig:hand}(a) displays AI-generated hands with faithful colors but unrealistic shapes, while humans easily recognize plausible hands from grayscale sketches in (b). Indeed, humans hierarchically decide based on knowledge (represented as augmented feature vector $\omega=\langle \omega_1, \omega_2, \omega_3 \rangle$): $\mathbf{I}$ identifies fingers (set $\omega_1$ value); $\mathbf{II}$ discerns gestures by finger positions (set $\omega_2$ value given $\omega_1$); $\mathbf{III}$ retrieves gesture meanings (set $\omega_3$ value given $\omega_1,\omega_2$). However, the hierarchy information $\omega$ in our cognition is unseen to AI. Without guidance from the indexing relations at each level (denoted by $\theta=\{\theta_1, \theta_2, \theta_3\}$), AI discerns only associations, resulting in basic dependencies between levels of entities (e.g., $\mathbf{P}(Y_2\mid Y_1)$) devoid of knowledgable insights (no $\omega,\theta$).

%To AI, or hypothetical extraterrestrial life unfamiliar with our knowledge, hands in Figure \ref{fig:hand}(a) may appear reasonable.
% AI can successfully differentiate non-overlapping features at various levels. 
%, while similar hand gestures may confuse it. 

In associational learning tasks (concerning $Y$ only), the hidden $\omega$ is not always essential. If entities across levels are observationally distinct and non-overlapping, AI can accurately differentiate them. For instance, AI can generate convincing faces because the appearance of eyes strongly indicates facial angle, removing the need to distinguish ``eyes'' ($Y_2$) from ``faces'' ($Y_1$). When all observational levels are fully captured, AI can uncover the hidden $\omega$ using methods such as inverse reinforcement learning \cite{sutton2018reinforcement, arora2021survey}. For example, approvals of generated five-fingered hands may lead AI to identify fingers autonomously.










\subsection{On Temporal Relationship Learning}
\label{sec:temp_hier}
\vspace{-1.5mm}


Figure~\ref{fig:eff}(a) shows an example from health informatics, depicting the causality from action $do(A)$ to sequence $\{B_t\}$, denoted as $\mathcal{B}$. Then, $\mathcal{B}$ can be disentangled as two levels of dynamical features: $\mathbf{I}$ the standard sequence of length 30 ($do(A)\xrightarrow{\theta_1}\mathcal{B}_1$ set $\omega=\varnothing$ ); $\mathbf{II}$ individualized progress variation ($E\xrightarrow{\theta_2}\mathcal{B}_2$ set $\omega=P_i,P_j,\ldots$), where the patient's personal characteristics $E$ is hidden. 
For simplicity, assume influence $\theta_2$ as linear, i.e., $E$ uniformly accelerates or decelerates the effective progress, and $\mathcal{B}_2$ simply interprets the individualized speed for patients ($\omega=P_i,P_j,\ldots$).
The modeling objective is to obtain $\mathcal{B}_1$, as the effectiveness evaluation of $M_A$.


Conventionally, the clinical effectiveness of $M_A$'s is estimated by averaging the performances of all patients after 30 days, resulting in a correlation model $B_{t+30}=f(do(A_t))$. It only captures the \emph{static} feature $B_{t+30}$, the final step of level $\mathbf{I}$ dynamic, neglecting the preceding 29 steps, as represented in Figure~\ref{fig:eff} (b). %complete feature vector, which is disentangled by hierarchical levels.

Significantly, even when adopting a sequence to represent $\mathcal{B}_1$, as in the Granger causality model \cite{granger1993modelling}, capturing the level $\textbf{I}$ dynamic through such a ``sequential static'' variable remains challenging 
(refer to subsection \ref{sec:causality3_2} for more discussions).
To illustrate, obtaining an accurate estimation by averaging sequences from D1 to D30 for all patients necessitates meeting certain criteria: an exact 30-day span; near-linear variations among individuals; and a normal distribution centered on D30; $\ldots$.
In essence, this method involves manually defining the boundary of $\theta_1$ by exploring all possible $\omega$ values.

%involves manually delineating the population-level effect $\theta$ while omitting $\omega$ influences. 

%at best, only the population-level sequence can be accurately estimated by averaging over all patients. This approach, however, excludes further dynamic feature levels, such as individual-level speed.

Hierarchical dynamical effects are commonly observed in various applications, including epidemic progression, economic fluctuations, strategic decision-making, and so on. Traditional approaches to these challenges typically involve a manual specification regarding the potential value of $\omega$, to delimit a particular level of $\theta$. A typical example is the group-specific learning methodologies \cite{fuller2007developing}. 




%Dynamic effects, prevalent in applications like epidemic progression, economic fluctuations, and strategic decision-making, often manifest at different levels of granularity. %These levels - as a type of hidden relations - are identifiable by our cognition but not directly observable in the data. 
%Group-specific learning methodologies \cite{fuller2007developing} are typically employed to address these issues, essentially serving as a manual specification of the value of $\omega$ 

% Figure environment removed




\vspace{-2mm}
\subsection{The Elusive Hidden-Confounder}
\label{sec:confounder}
\vspace{-2mm}


For patients $P_i$ and $P_j$, the population-level last-day effect $B_{t+30}$ is inaccurate. To counter this individual-level bias and improve model interpretation, statistical causal inference incorporates the ``hidden confounder'' concept into Directed Acyclic Graphs (DAG), %representing the concealed $\omega$ 
as node $E$ in Figure~\ref{fig:hidden} (a).
However, this approach does not necessitate collecting additional data for $E$, leading to an illogical implication: ``The model bias stems from unknown factors we don’t intend to explore.''
This strategy indeed compensates for the overlooked level $\mathbf{II}$ dynamic, which essentially transforms an \emph{observable} dynamical feature of the \emph{effect} into a \emph{hidden} observational variable, $E$, associated with the \emph{cause} $do(A)$. 


As shown in Figure~\ref{fig:hidden}(b), the hidden associated cause $do(A) * E$ does not offer a modelable relationship to learn $\{\theta_1,\theta_2\}$.
That is, while introducing $E$ enhances the interpretation, it does not certainly improve the model to encompass further levels.
Contrarily, a \emph{Relation-Oriented} approach only treats relations $\{\theta_1, \theta_2\}$ as indices without additional modeling requirements. It allows AI to autonomously extract dynamical representations for multi-levels, with the indices being any observed identifier for $\omega$, like a patient ID, as shown in Figure~\ref{fig:hidden}(c).
Such hierarchical disentanglement is driven by knowledge, thereby enhancing model generalizability. 



% Figure environment removed
\vspace{-2mm}




\section{Causality on Temporal Dimension}
\label{sec:Causality}
%\vspace{-2mm}

Causal learning serves as a gateway to access the distributions within the temporal dimension, extending beyond the observational space. Under the prevailing \emph{Observation-Oriented} paradigm, timestamps for both causal and effectual events necessitate prior specifications. This approach diverges from our instinctive understanding, where effects are identified by causes indexing through the objective causal relation. %our knowledge.

Furthermore, timestamp specification relies exclusively on the absolute timeline, functioning merely as a regular observational dimension within the modeling context. This approach, to a degree, diminishes the temporal significance of causal relationships, rendering them indistinguishable from correlational ones from a modeling perspective, thus necessitating reliance on interpretations for differentiation.

In response, this section reexamines causality from a frequently overlooked angle - learning dynamical features of effects - with the goal of offering more intuitive insights into relevant theories and concepts. Subsection \ref{sec:causality3_1} revisits the definition of causality within the modeling context. Subsequently, subsection \ref{sec:causality3_2} distinguishes between dynamical and static variables to elucidate the challenges in effect identification. Finally, subsection \ref{sec:causality3_3} explores the limitations present in current applications of causal learning.


% We start by redefining the concept of causal modeling (subsection \ref{sec:causality3_1}), then assess the effectiveness of existing methods in learning dynamics (subsection \ref{sec:causality3_2}), and finally highlight the inherent limitations of the prevailing \emph{Observation-Oriented} causal model paradigm (subsection \ref{sec:causality3_3}).

% However, the current causal learning paradigm fails to grasp temporally nonlinear relationships - 
% It necessitates predefined timestamps for causal effects and focuses solely on their static, snapshotted observational features, thereby limiting their dynamism in the temporal dimension.
% Overlooking key dynamics can misalign the modeled relations with our anticipated knowledge (stored in the hyper-dimensional space).

%The inherent nature of observational learning significantly complicates our modeling methods and the interpretation of inferences on causality. 

%\vspace{-1mm}
\subsection{Causality in Modeling Context}
\label{sec:causality3_1}
%\vspace{-1mm}


% From a modeling perspective, specified timestamps are associated with observations rather than constituting a separate computational dimension. Consequently, in traditional causal inference, the temporal-evolving aspects that distinguish causality from correlation are not directly built into the current modeling framework. Instead, they are mainly evident in model interpretations, guiding potential improvements to the model.
Traditional causal inference often highlights model interpretations, notably distinguishing them from mere correlations, as these distinctions are not inherently embedded within the modeling context. Essentially, causality mandates the incorporation of the timeline as a \emph{computational dimension}, ensuring recognition of significant \emph{distributions} on it, ones that undeniably can exhibit \emph{temporal nonlinearity}.

%assuredly
\emph{Observation-Oriented} modeling fades out the causal significance of these relationships in two aspects: First, manual specifications cannot completely identify dynamics of effects for each level; Second, these dynamics might coexist in various relative timelines, which suggests multiple computational dimensions in the Temporal Feature Space.
Considering these points, we revisit the definition of causality from a modeling perspective:



%\vspace{2mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=red!50!black, colback=yellow!10]{
\hspace{-2mm}
\begin{minipage}{0.97\textwidth}
\paragraph{Definition 1.} Causality vs. Correlation in the modeling context.

$\ \bullet$ Causality = related Observational-Temporal features, including \emph{\textbf{multi-dimensional dynamical} ones}.

$\ \bullet$ Correlation = related Observational features that are \textbf{\emph{not dynamical}}.

\end{minipage}}
%\vspace{-1mm}

In particular, causal modeling is vital because it facilitates the answering of \emph{counterfactual} questions \cite{scholkopf2021toward}, such as, ``What effect would ensue if the cause were altered?'' This capability is akin to fully capturing \emph{temporal dimensional distributions} (i.e., all possible outcomes), thereby providing accurate responses to conditional queries (i.e., ``what if'' scenarios).

%\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.95\textwidth}
\paragraph{Remark 1.}Counterfactuals can be viewed as posterior distributions in the Temporal Feature Space.

\vspace{-0.5mm}
\end{minipage}}
%\vspace{-1mm}

% The current causal modeling framework, which only captures linear features on a specified absolute timeline, may have sufficed in past decades but falls short of current needs. While AI-based models like RNNs facilitate nonlinear modeling, this capability depends on the accurate specification of time sequences (primarily as the cause only) and a correct timeline. Violating the former condition results in a failure to capture dynamics (see subsection \ref{sec:causality3_2}), and violating the latter introduces inherent bias, fundamentally reducing model effectiveness (refer to Section \ref{sec:Temporal}).

In modeling, the directionality of the relationship (i.e., the roles of cause and effect, or the ``causal direction'')
may not impose restrictions, even though it proves important in model interpretations. Specifically, when selecting a model for the relationship $X\rightarrow Y$,  one could use $Y=f(X;\theta)$ to predict the effect $Y$, or $X=g(Y;\phi)$ to inversely infer the cause $X$. Both parameters, $\theta$ and $\phi$, are obtained from the joint probability $\mathbf{P}(X, Y)$ without imposing modeling constraints. We refer to it as \emph{\textbf{symmetric directionality}} for clarity.

In practice, concern for directionality mainly arises for two reasons: First, to maintain alignment with our intuitive understanding of temporal progression; Second, while the current paradigm can facilitate dynamical variables for the \emph{cause}, it does not do so for the \emph{effect} - A typical example is the RNN models.

% exhibits an \emph{imbalance} in capturing dynamics between the cause and the effect (see subsection \ref{sec:causality3_2} for details). For instance, the hierarchical dynamics overlooked in Figure~\ref{fig:hidden} can be fully captured by an inverse model of $do(A)=f(\{B_t,\ldots,B_{t+40}\})$ using RNNs, eliminating the need for a hidden confounder. 


\vspace{-1mm}
\subsection{Difficulty of Identifying Dynamical Effects}
\label{sec:causality3_2}


It is crucial to note that using a sequential variable does not necessarily capture the nonlinearity of the represented entities.
The distinction between ``a \emph{sequence of \textbf{static}} variables'' and ``a \emph{\textbf{dynamical}} variable'' hinges on the model's ability to feature the \emph{nonlinearity} among the sequence's elements.

RNN models technically address the challenges of extracting temporal features from data sequences \cite{xu2020multivariate}. 
Particularly, they transform the observable data sequences into a latent feature space, where the featured distributions can be represented as a feature vector - including the temporal dimensional ones.

However, while this transformation effectively represents temporal dimensional features, the types of the significant features being extracted - whether static linearity or dynamic nonlinearity - depend on the model.
Let's simplify RNNs in the form of $Y=f(\mathcal{X};\theta)$, where the variable $\mathcal{X} = \langle X,t \rangle \in \mathbb{R}^{d+1}$ jointly represents observational-temporal features of the cause $X \in \mathbb{R}^d$. 
The optimization process of $\mathcal{X}$ is driven by the observational $Y$ through the relation $\theta$. Consequently, it can capture dynamical temporal features in the $t$-dimension if they are significant in predicting $Y$.
%fully encapsulates

\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.95\textwidth}
\paragraph{Remark 2.} RNNs extract dynamic nonlinearity from the cause by indexing via the relation $\theta$.
\vspace{-1mm}
\end{minipage}}

Despite their advantages, RNNs are not exempt from the \emph{identifiability difficulty} \cite{zhang2012identifiability}, primarily because of the requirement to specify timestamps for effects. This challenge primarily stems from the dynamical variations in the temporal dimension, brought by hidden $\omega$. Moreover, the difficulty intensifies when characterizing effects in comparison to causes.
While it is feasible to organize sequential data around a major causal event (e.g., days of heavy rain), pinpointing the precise onset of subsequent effects (e.g., the exact day the flood initiated due to that rain) remains a complex task.
%Furthermore, identifying appropriate timelines presents an additional challenge.

Given that the \emph{\textbf{relation-indexing} autonomous identification} primarily targets dynamics of causes rather than effects, the inverse learning methodology \cite{arora2021survey}, which has been gaining increasing attention recently, aspires to achieve the converse. It utilizes \emph{symmetric directionality} to sidestep the challenge of identifying dynamical effects and defining the objective function.


%\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.95\textwidth}
\paragraph{Remark 3.} Autonomous indexing via objective relations $\theta$ can address the challenge of identifiability, but is not embraced by the current \emph{Observation-Oriented} paradigm regarding \emph{effects}.
\vspace{-1mm}
\end{minipage}}

Before the advent of RNNs, traditional methods typically utilized observational sequences to capture a set length of static features. For instance, autoregressive models \cite{hyvarinen2010estimation} are often formulated as $Y=f(\{x_t\};\theta)$. Additionally, Granger causality \cite{granger1993modelling, maziarz2015review}, a method widely recognized in economics, introduces a sequence for the effect: ${y_\tau} = f({x_t};\theta)$, where $t$ and $\tau$ represent separate timelines for cause and effect. As highlighted in the discussions surrounding Figure \ref{fig:eff}, this method relies heavily on the precise specification regarding hidden relations $\omega$, and can hardly achieve generalizability autonomously.

% Contrary to the $\mathcal{X}$ in RNNs that acts as an observational-temporal variable, in traditional methods, sequences $\{x_t\}$ and $\{y_\tau\}$ are purely observational, potentially capturing a limited length of static temporal features and heavily depending on accurate specifications of hidden relations $\omega$.
% Consider Figure \ref{fig:eff}: estimating the level $\textbf{I}$ sequence by averaging patient data from Day1-Day30 necessitates specific criteria, such as a precise 30-day length, near-linear individual diversity, and a normal distribution around Day30. In essence, this involves manually delineating the population-level effect $\theta$ while omitting $\omega$ influences. 

To avoid specifying time sequences for causes, do-calculus \cite{pearl2012calculus, huang2012pearl} targets \emph{identifiable} events, enabling a fluid transformation from dynamical cause to observational effect,  but the identifiability relies on non-experimental data (controllable $\theta$). Given its inherently \emph{differential} nature, which increases its complexity, we provide a streamlined reinterpretation of its three core rules from an \emph{integral} viewpoint.

Let $do(x_t)=\{x_t, x_{t+1}\}$ indicate the occurrence of an instantaneous event $do(x)$ at time $t$, with the time step $\Delta t$ sufficiently small to make this event's \emph{interventional} effect identifiable as a function of the resultant distribution at $t+1$. Meanwhile, a separate \emph{observational} effect is provoked by the static $x_t$. Then, %for variable $X\in \mathbb{R}^d$, its dynamics $\mathcal{X}$ as the cause object can be expressed as follows:

\vspace{-6mm}
\begin{align*}
    \text{Given } \mathcal{X} & \rightarrow Y \mid \theta, \text{ where } \mathcal{X} = \langle X,t \rangle \in \mathbb{R}^{d+1} \text{ with augmented $t$-dimension residing a $T$-length sequence,} \\
    \mathcal{X} =& \int_0^T do(x_t) \cdot x_t \ dt \text{\ \ with }
    \begin{cases}
      (do(x_t)=1) \mid \theta, & \text{ \emph{Observational} only (Rule 1) } \\
      (x_t=1) \mid \theta, & \text{ \emph{Interventional} only (Rule 2) }    \\
      (do(x_t)=0) \mid \theta, & \text{ No \emph{interventional}  (Rule 3) }   \\
      \text{otherwise} & \text{ Associated \emph{observational} and \emph{interventional} }
    \end{cases} \\
     \text{The effect } & \text{of } \mathcal{X} \text{ can be derived as }
     f(\mathcal{X}) = \int_0^T f_t \big( do(x_t) \cdot x_t \big) \ dt = \sum_{t=0}^{T-1} (y_{t+1}-y_t) =y_T-y_0
\end{align*}
\vspace{-5mm}

% graphical criteria
%  if a causal effect is identifiable, there exists a sequence of applications of the rules
% into a formula that only includes observational quantities.

Given a controllable $\theta$, it addresses three criteria that preserve conditional independence between \emph{observational} and \emph{interventional} effects, completing the chain rule, but sidesteps more generalized cases.
%(an identifiable $do(x_t)\cdot x_t$ pertains to Rule 2). 
If one depicts a dynamical effect as $\mathcal{Y}=\langle Y,\tau \rangle$, event specifications for $do(y)$ remain necessary. % under the current paradigm, while the proposed one is designed to construct $\mathcal{Y}$ autonomously.



%\vspace{2mm}
\subsection{Limitations in Application}
\label{sec:causality3_3}
\vspace{-2mm}

Due to effect identification difficulties inherent within the \emph{Observation-Oriented} paradigm, reliance on foundational assumptions is often indispensable. 
For a more detailed exploration of these limitations, Figure~\ref{fig:view} categorizes the applications into four distinct scenarios:
the queries can be divided into Discovery and Buildup, depending on whether the objective relation $\theta$ is known; they can also be further categorized by the dynamical significance of the effect - For example, the causality ``raining $\rightarrow$ wet floor'' falls into area $\circled{4}$, while ``raining $\rightarrow$ floor becoming wetter'' is in area $\circled{3}$.
They will be examined from two perspectives in the following: the modeling objective Relation (i.e., $\theta$), and the interpretational Directionality.


% Figure environment removed
\vspace{-2mm}


%\vspace{4mm}
\subsubsubsection{\emph{(1) Modeled Relation}}
\vspace{-1mm}

Significant progress has been made in transforming specific dynamics into observationally accessible forms, such as the independence utilized in do-calculus.
For the overlooked effect dynamics, if existing knowledge can suggest a potential cause, introducing a hidden confounder can enhance comprehension. If not, they may be dismissed due to the assumed causal \emph{Sufficiency}, potentially leading to subsequent challenges.

Causal discovery mainly examines the Observational Feature Space to uncover dependencies. If the true causality of interest does not have significant dynamics, the discovered associations can be insightful. However, if such dynamics are present, especially in multi-levels, the undetected features may be dismissed by the \emph{Faithfulness} assumption, positing that observables can fully represent causal reality.

\vspace{-1mm}
\subsubsubsection{\emph{(2) Modeled Causal Direction}}
\vspace{-1mm}

Consider causally related variables $X$ and $Y$ with potential directional models $Y=f(X;\theta)$ and $X=g(Y;\phi)$. In the observational space, the discovered direction depends on the likelihoods of estimated $\hat{\theta}$ and $\hat{\phi}$.  The preference is for $X\rightarrow Y$ if $\mathcal{L}(\hat{\theta}) > \mathcal{L}(\hat{\phi})$. %, then $X\rightarrow Y$ would be preferred.
Now, let $\mathcal{I}(\theta)$ be a simplified form of $\mathcal{I}_{X,Y}(\theta)$ (the Fisher information), representing the information in $\mathbf{P}(X,Y)$ about the relevant $\theta$. If $p(\cdot)$ is the density function, then $\int_X p(x;\theta) dx$ is constant in this context. Thus, we have:

\vspace{-6mm}
\begin{align*}
    \mathcal{I}(\theta) &= \mathbb{E}[(\frac{\partial }{\partial \theta} \log p(X,Y;\theta))^2 \mid \theta] 
    = \int_{Y} \int_X (\frac{\partial }{\partial \theta} \log p(x,y;\theta))^2 p(x,y;\theta) dx dy \\
    &= \alpha \int_Y (\frac{\partial }{\partial \theta} \log p(y;x,\theta))^2 p(y;x,\theta) dy + \beta = \alpha \mathcal{I}_{Y\mid X}(\theta)+\beta, \text{with } \alpha,\beta \text{ constants.} \\
    \text{Thus, } \hat{\theta} &=\argmax\limits_{\theta} \mathbf{P}(Y\mid X,\theta) = \argmin\limits_{\theta} \mathcal{I}_{Y\mid X} (\theta) =\argmin\limits_{\theta} \mathcal{I}(\theta), \text{ and } \mathcal{L}(\hat{\theta})  \propto 1/\mathcal{I}(\hat{\theta}).
\end{align*}
\vspace{-5mm}

The likelihoods of the estimated $\hat{\theta}$ and $\hat{\omega}$ rely on the information $\mathcal{I}(\hat{\theta})$ and $\mathcal{I}(\hat{\omega})$. Consequently, the inferred directionality between $X$ and $Y$ reflects the extent to which their designated distributions appear in the data, with the predominant one deemed the ``cause'' - It assumes, by default, that observations capture the cause more thoroughly than the effect. While limited data collection techniques made it reasonable in the past, it is no longer safe to assume such observationally inferred directions to hold logical meaning for causality.



\section{The Overlooked Multi-Dimensional Temporal Space}
\label{sec:Temporal}

As outlined in Definition 1, compared to our innate understanding of causal knowledge, an \emph{Observation-Oriented} viewpoint has two key oversights: 1) the dynamical features of effects, and 2) the multi-dimensional nature of these dynamics. While the former still can be empirically addressed through inverse learning, the latter poses more foundational challenges to structural relationship modeling, underscoring the need for relation-indexing approaches in a new \emph{Relation-Oriented} paradigm.

When understanding structural relationships within knowledge, our logic discerns not just the absolute timeline but also various relative timelines  \cite{coulson2009understanding}, with each capturing distinct effects \cite{shea2001effects}. 
While these effects may originate from a single cause, they possess unique dynamical features and interrelate with one another \cite{wulf1994reducing}. 
Within unobservable hierarchies, such interconnected dynamics (i.e., dynamically significant variables or features) can result in identical timestamps representing different effects across levels, inherently making the manual timestamp specification inaccurate.

Consider a structural causal relationship $\mathcal{Y} \xleftarrow{\theta} \mathcal{X} \xrightarrow{\phi} \mathcal{Z}$ comprising three dynamics $\{\mathcal{X}, \mathcal{Y}, \mathcal{Z}\}$ and two distinct effects on the relative timelines $T_{\theta}$ and $T_{\phi}$. Let's assume a hidden relation, $\omega$, introduces hierarchical levels. While it's feasible to model an individual effect on either $T_{\theta}$ or $T_{\phi}$ by specifying a sequence of timestamps, building a comprehensive structural model that encompasses $\{\mathcal{X}, \mathcal{Y}, \mathcal{Z}\}$ would introduce \emph{\textbf{inherent biases}} if relying solely on any one timeline from either $T_{\theta}$ or $T_{\phi}$.
Moreover, if $\langle\theta, \phi \rangle \in \mathbb{R}^h$ are jointly distributed, meaning $\mathcal{Y}$ and $\mathcal{Z}$ are interrelated, relying on a single timeline becomes unreliable even when considering individual effects. Only \emph{\textbf{autonomous identifications}} specific to each effect can sidestep the complications.

Traditional causal inference, apparently aware of these challenges, has employed various adjustment methods to circumvent \emph{\textbf{confounded dynamics}}, such as propensity score matching \cite{benedetto2018statistical} and backdoor adjustment \cite{pearl2009causal}.
However, these techniques fundamentally depend on manual identification and have become impractical at present, given the black-box nature of AI models and their application to large-scale tasks.
Consequently, while still operating under the \emph{Observation-Oriented} paradigm, AI-based causal learning tends to default to the absolute timeline, which is the only directly observable one in the data, to specify timestamps for all events. 
This method can lead to \emph{inherent biases} accumulating over the structural complexity, ultimately affecting the model's robustness and generalizability.

%To better understand the implications of this limitation, consider the following analogy:
%To shed light on the implications of this oversight, consider the following analogy:

% Imagine ants dwelling on a floor's two-dimensional plane. The ant scientists among them, aiming to predict risks, instinctively use the nearest tree's height as a reference for their two-dimensional models. During their modeling, they noticed an increase in disruptions at the tree's mid-level. This increase correlates to a higher likelihood of encountering children. However, without understanding humans as three-dimensional beings, the ants' interpretations are limited to observations at the tree's mid-level.

% If these ants were to relocate to a tree of different heights, the mid-level would no longer correlate with risk, making their model ineffective. They may conclude that human behavior is too complex to model accurately. 
%Similarly, when we specify a single, absolute timeline for all potential events, this timeline becomes our ``tree'', which may introduce inherent modeling biases, affecting the robustness and generalizability of our models.

%The \emph{Observation-Oriented} modeling paradigm mandates the specification of timestamps for all conceivable events, thereby defaulting to the absolute timeline as the sole observable one. 
%Therefore, when identifying temporal events based on structural causal knowledge, it is crucial to consider all potential relative timelines comprehensively to avoid bias in the temporal dimension.


\vspace{0.5mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=red!50!black, colback=yellow!10]{
\hspace{-1mm}\begin{minipage}{0.97\textwidth}
\paragraph{Definition 2.} The \emph{Temporal Dimension} comprises all potential logical timelines, not a single dimension. A multi-dimensional Temporal Feature Space is defined by the required timelines serving as axes.
\end{minipage}}
%\vspace{-0.5mm}


% Inherently, causal inference adopts a \emph{Relation-Oriented} perspective, acknowledging distinct effects and their respective dynamics. However, given that \emph{Observation-Oriented} models typically rely on a single absolute timeline as a regular observational dimension, various de-confounding methods, including propensity score matching \cite{benedetto2018statistical} and backdoor adjustment \cite{pearl2009causal}, are utilized to eliminate inter-relations within the knowledge structure for more effective modeling.

% In contrast, employing AI methods for causal questions is challenging due to their black-box nature and large scale, which makes manual inspections impractical. Crudely consolidating all dynamics into one timeline exacerbates the inherent biases, yielding uninterpretable results \cite{luo2020causal}.
% Think of traditional causal learning as manually building Schrödinger's box so the ``cat'' appears reasonable upon revelation; the proposed relation-indexing approach aims to have AI autonomously craft this box.

This section will first demonstrate the \emph{inherent bias} through an intuitive example (subsection \ref{sec:CRB}), explore its impact on the generalizability of structural causal models (subsection \ref{sec:generalize}), and finally discuss the advancements and challenges on our path toward causal knowledge-aligned AGI (subsection \ref{sec:toward}).



\subsection{Scheme of the Inherent Bias}
\label{sec:CRB}
\vspace{-1mm}

% Figure environment removed
\vspace{-2mm}


Consider medical trial data from hospital patients. Vital signs and medication usage are recorded daily, forming the chronological \emph{absolute timeline}. However, to assess the effects of a specific medication, a \emph{relative timeline} is constructed, with time-zero marking a consistent action, such as $do(A)$, for all patients. As a result, events with different chronological timestamps can align on the relative timeline, and vice versa.
For instance, Figure \ref{fig:eff} illustrates a relative timeline for the effects of $do(A)$, while Figure~\ref{fig:do1}(a) revisits its causal DAG, incorporating the introduced hidden confounder. For clearly represent hierarchical dynamical effects, the causal DAG is enhanced as depicted in (b) through two steps:
\begin{enumerate}[itemsep=0em, topsep=-4pt, 
parsep=1pt, partopsep=-3pt,
leftmargin=20pt, labelwidth=10pt]
    \item Assume dynamically significant effects and integrate their relative timelines into the DAG space.
    \item Use varying edge lengths to represent timespans required for the effects to reach an equivalent magnitude.
\end{enumerate}


%To more effectively address this issue, the causal DAG (directed acyclic graph) is enhanced in two ways: 1) by incorporating desired logical timelines as axes into the DAG space, and 2) by assuming causal effects are dynamically significant, with varying edge lengths indicating different timespans required to achieve identical effects.
%For instance, Figure~\ref{fig:do1}(a) revisits the hidden-confounder example, which aims to interpret different individualized effects. Alternatively, the enhanced DAG shown in (b) provides a convenient representation of these effects.

% Figure environment removed
\vspace{-1mm}

Figure~\ref{fig:do3}(a) presents an extended scenario where $A$ stands for $do(A)$ for short. It features two distinct effects: the primary effect $\overrightarrow{AB}$ on $B$, and a side effect $\overrightarrow{AC}$ on vital sign $C$, which indirectly affects $B$ through $\overrightarrow{CB}$. 
The confounded nodes $\{A, B, C\}$ form a triangle across timelines $T_X$ and $T_Y$, which should consistently hold for all individuals or populations, based on the causal \emph{Markov} condition requirement.
The processes of \emph{generalization} and \emph{individualization} operate as ``stretching'' this triangle along $T_X$ at different ratios, conducting a homographic \emph{linear transformation} within this DAG space, as depicted in Figure~\ref{fig:do3} (a). 

For simplicity, assume dynamics on $T_X$ and $T_Y$ are independent: fix the $\overrightarrow{AC}$ timespan at 10 days for all patients, focusing on individualized variances only on $T_X$.
Structural Causal Models (SCMs) typically assign a timespan for $\overrightarrow{AB}$, such as 30, to represent the population-level average effect. 
However, as shown in (b) and (c), the SCM function $B_{t+30}=f(A_t, C_{t+10})$ violates the \emph{Markov} condition for either $P_i$ or $P_j$. 

%Importantly, in this simplified example, violations may not pose significant issues for models able to address nonlinearity, due to the independence of dynamics on $T_X$ and $T_Y$. The SCM can become $B_{t+30}= f_1(A_t)+f_2(C_{t+10})$, implying that cross-timeline confounding can be dissected into two single-timeline problems. 
% In extensive causal AI applications, such inherent biases may emerge between any pair of distinct dynamical effects, accruing exponentially and substantially affecting model robustness, regardless of the selected modeling approach.


\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.95\textwidth}
\paragraph{Remark 4.} The \emph{\textbf{inherent bias}} may occur in SCM if it contains: 
1) \emph{Confounded Dynamics} across \emph{Multiple} logical timelines, and 2) Unobservable Hierarchy (represented by hidden $\omega$).
\vspace{-1mm}
\end{minipage}}

In this simplified scenario, SCMs might still function given the independence between  $\overrightarrow{AB}$ and $\overrightarrow{AC}$. 
However, it is impractical always to assume independence or a lack of confounding for all dynamical effects in structural relationship learning.
For broad causal AI applications, neglecting multiple relative timelines can lead to accumulating biases, potentially compromising model robustness irrespective of the chosen model.
Consequently, current AI applications typically focus on tasks not involving relative timelines. For instance, LLMs operate within a semantic space on a single timeline, consistently preserving word order.


% \vspace{1mm}
% \noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=red!50!black, colback=yellow!10]{\begin{minipage}{0.96\textwidth}
% \paragraph{Theorem 3.}The \emph{\textbf{inherent} bias} may occur in SCM if it contains: 
% 1) \emph{Confounded} dynamical causal effects across \emph{Multiple} logical timelines, and 2) Unobservable Hierarchy (represented by $\omega$).
% \end{minipage}}
% \vspace{1mm}

%Interestingly, most successful causal applications instinctively avoid either \emph{confounding} or \emph{multi-timeline}. Causal inference typically employs de-confounding (like PSM \cite{benedetto2018statistical} and backdoor adjustment \cite{pearl2009causal}), to mitigate inherent bias and other confounding biases resulting from unaddressed nonlinearity.
%Meanwhile, many AI accomplishments, including Large Language Models (LLMs), which operate in a semantic space, do not inherently deal with relative timelines, maintaining words consistently ordered along a singular timeline.


% Figure environment removed

\subsection{Inherent Impact on SCM Generalizability}
\label{sec:generalize}


Unobservable hierarchies can imply varied scenarios with the same fundamental relationships. Traditional SCMs, which necessitate timestamp specification along a singular $t$-timeline, compromise not only the robustness but also impede the generalizability of the formulated SCMs across these scenarios.

Consider the practical scenario depicted in Figure~\ref{fig:3d}. Here, $\Delta t$ and $\Delta \tau$ represent actual time spans. Yet, the crux is not on determining their exact values, but on realizing their intended causal relationship: As each unit of Statin's effect is delivered on LDL via $\overrightarrow{SA'}$, it immediately impacts T2D through $\overrightarrow{A'B'}$. Simultaneously, the next unit effect begins generation. This dual action runs concurrently until $S$ is fully administered.  At $B'$, the ultimate aim of this process is to evaluate the total cumulative influence stemming from $S$.

Given the relationship $\overrightarrow{SB'} = \overrightarrow{SA'}+\overrightarrow{A'B'}$, specifying the $\overrightarrow{SB'}$ time span (= half of the $\overrightarrow{AB'}$ time span) inherently sets the $\Delta t:\Delta \tau$ ratio, defining the ${ASB'}$ triangle's shape in the DAG space. While the estimated mean effect at $B'$ might be precise for the present population, the preset $\Delta t:\Delta \tau$ ratio's universality is questionable, potentially constraining the established SCM's generalizability.

\subsection{Toward Causal Knowledge-Aligned AGI}
\label{sec:toward}
%\vspace{-1.5mm}

In pursuit of causally interpretable AI, our modeling techniques expand beyond the purely observational to encompass temporal dimensions, as summarized in Figure~\ref{fig:model}.
At present, the challenge is to ensure the generalizability of structural causal AI models. 
Recognizing multi-timeline dynamics is essential to avoid biases that obscure AI interpretability. Given the impracticality of manually discerning all potential logical timelines for observable data, it might be time to contemplate a new paradigm.


\vspace{-2mm}
% Figure environment removed
\vspace{-1mm}

The initial models under i.i.d. assumption only approximate observational associations, proved unreliable for causal reasoning \cite{pearl2000models, peters2017elements}. Correspondingly, the common cause principle highlights the significance of the nontrivial conditional properties, to distinguish structural relationships from statistical dependencies \cite{dawid1979conditional, geiger1993logical}, providing a basis for effectively uncovering the underlying structures in graphical models \cite{peters2014causal}.

Graphical causal models relying on conditional dependencies to construct Bayesian networks (BNs) often operate in observational space and neglect temporal aspects, reducing their causal relevance \cite{scheines1997introduction}. Causally significant models, such as Structural Equation Models (SEMs) and Functional Causal Models (FCMs) \cite{glymour2019review, elwert2013graphical}, can address counterfactual queries \cite{scholkopf2021toward}, with respect to temporal distributions by leveraging prior knowledge, to construct causal DAGs accordingly.

State-of-the-art deep learning applications on causality, which encode the DAG structural constraint into continuous optimization functions \cite{zheng2018dags, zheng2020learning, lachapelle2019gradient}, undoubtedly enable highly efficient solutions, especially for large-scale problems. However, larger question scales indicate more underlying logical timelines, which may lead to snowballing temporal biases. It can be evident from the limited successful applications of incorporating DAG structure into network architectures \cite{luo2020causal, ma2018using}, e.g., neural architecture search (NAS).

Schölkopf \cite{scholkopf2021toward} summarized three key challenges impeding causal AI applications to achieving generalizable success: 1) limited model robustness, 2) insufficient model reusability, and 3) inability to handle data heterogeneity (caused by unobservable hierarchies in knowledge). There exists an intrinsic connection between these challenges and the inherent bias highlighted in Remark 4.

% noting that these challenges can be attributed to the timestamp specification required by \emph{Observation-Oriented} SCMs.

On the other side, physical models, which explicitly integrate temporal dimensions in computation, and are able to establish abstract concepts through relations, may provide insights into these challenges. 
The relation-indexing approach is designed to bridge the gap between the Observational and Temporal Spaces.
%\pagebreak




\begin{center}
   {\vskip 8pt\large\bf Chapter II: Realization of Proposed Relation-Oriented Paradigm} 
\end{center}

This chapter delves into the realization of autonomously identifying causal effects via relation-indexing, and its role in shaping structural models in the latent feature space.
First, Section \ref{sec:indexed} details the technique for extracting relation-indexed representations, to realize hierarchical disentanglement. Building on this, Section \ref{sec:RIRL} presents the \emph{Relation-Indexed Representation Learning} (RIRL) method, designed to instantiate structural causal models within latent space. Lastly, Section \ref{sec:experiment} provides experimental validation of RIRL's efficacy.


%provides formulations of factorizations, to achieve of achieving hierarchical disentanglement through relation-indexing

%\vspace{-2mm}
\section{Relation-Indexed Hierarchical Disentanglement}
\label{sec:indexed}

For the relationship $\mathcal{X}\rightarrow \mathcal{Y}$, consider $\mathcal{X}=\langle X, t \rangle \in \mathbb{R}^{d+1} = \{x_t\} = \{x_1, \ldots, x_{T_x}\}$ with observational variable $X\in \mathbb{R}^d$ and time sequence $\{x_t\}$ in length $T_x$. Likewise, $\mathcal{Y} = \langle Y, \tau \rangle \in \mathbb{R}^{b+1} = \{y_{\tau}\} = \{y_1, \ldots, y_{T_y}\}$ with $Y\in \mathbb{R}^b$ and $\{y_{\tau}\}$ in length $T_y$. In essence, $\mathcal{X}$ and $\mathcal{Y}$ each incorporate timelines $t$ and $\tau$, represented by data sequences $\{x_t\}$ and $\{y_{\tau}\}$, respectively.
For simplicity, lengths $T_x$ and $T_y$ are omitted in $\mathcal{X}$ and $\mathcal{Y}$ notations.

Relation-indexing begins with the \emph{\textbf{initialization}} of $\mathcal{X}$ and $\mathcal{Y}$ as feature vectors $\mathcal{H}$ and $\mathcal{V}$ in the latent space $\mathbb{R}^L$. A relation model denoted as $f(;\theta)$), then refines these latent vectors to minimize their distance in $\mathbb{R}^L$. Thus, the dimensionality $L$ of the latent space $\mathbb{R}^L$ must be at least the rank of the augmented triplet, given by $L \ge rank(\langle \mathcal{H},\theta,\mathcal{V} \rangle)$.
This presents a technical challenge as it necessitates representing $\mathcal{X}$ in a latent vector with a length that certainly surpasses its rank $rank(\mathcal{H})$, and possibly its original length of $d * T_x$.

\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.94\textwidth}
\paragraph{Remark 5.} The variable \emph{initialization} necessitates a \emph{higher-dimensional} representation autoencoder.
\end{minipage}}

The goal of relation-indexing is to derive $\mathcal{\hat{Y}}$, representing the component of $\mathcal{Y}$, which is \emph{\textbf{identifiable}} through its relation with $\mathcal{X}$, symbolized as $\mathcal{\hat{V}}$ in the latent space.
Moreover,  the \emph{{generalizability}} of the model established for $\mathcal{X}\rightarrow \mathcal{Y}$ requires $\mathcal{\hat{V}}$ to function as a basis for subsequent layered representations of $\mathcal{Y}$, eventually leading to the \emph{{hierarchical disentanglement}} of $\mathcal{Y}$.





% The constructed latent feature space $\mathbb{R}^L$ must ensure: 
% 1) The observational-temporal feature $\mathcal{X}=\langle X, t \rangle \in \mathbb{R}^{d+1}$ is fully represented in $\mathbb{R}^L$ (remark of $T$ is omitted for simplicity);
% 2) The representation $\mathcal{X}$ is hierarchically disentangled within $\mathbb{R}^L$ based on specified relations, facilitating the model's generalizability across various hierarchical levels.

%Consider a relationship $\mathcal{X}\rightarrow \mathcal{Y}$, where $Y\in \mathbb{R}^b$ and $\mathcal{Y} = \langle Y, \tau \rangle \in \mathbb{R}^{b+1}$.
% In the context of the effect $\mathcal{Y}$, relation-indexing entails deriving $\mathcal{\hat{Y}}$, which represents the portion of $\mathcal{Y}$ that is identifiable through its relation with $\mathcal{X}$. This segment can serve as a foundation for additional levels of $\mathcal{Y}$'s representations, which is a generalizing process for the established relationship model $\mathcal{X}\rightarrow \mathcal{Y}$.



\subsection{Higher-Dimensional Autoencoder}
\label{sec:indexed_1}
\vspace{-1mm}


Autoencoders are typically employed for dimensionality \emph{reduction} rather than \emph{expansion}. In prevailing causal structural models, the column-augmented original data matrix often possesses a dimensionality exceeding that of the latent space $\mathbb{R}^L$ \cite{wang2016auto}. 
However, the \emph{Relation-Oriented} approach diverges by modeling individual relations sequentially and integrating them to establish the structure within $\mathbb{R}^L$. 

Figure~\ref{fig:arch} illustrates the autoencoder architecture designed for achieving this higher-dimensional representation.
This architecture is featured by the symmetrical \emph{Expander} and \emph{Reducer} layers (source code is available \footnote{https://github.com/kflijia/bijective\_crossing\_functions/blob/main/code\_bicross\_extracter.py}).
The Expander magnifies the input vector $\overrightarrow{x}$ within length $d * T_x$ by capturing its higher-order associative features, while the Reducer symmetrically diminishes dimensionality and reverts to its initial state. For precise reconstruction, the \emph{\textbf{invertibility}} of these processes is essential. 

The Expander showcased in Figure~\ref{fig:arch} implements a \emph{double-wise} expansion. Here, every duo of digits from $\overrightarrow{x}$ is encoded into a new digit using an association with a random constant, termed the \emph{Key}. This \emph{Key} is generated by the encoder and replicated by the decoder. Such pairwise processing of $\overrightarrow{x}$ results in a vector of length $(d * T_x - 1)^2$. By leveraging multiple \emph{Keys} and concatenating the resultant vectors, the dimensionality of $\overrightarrow{x}$ can be considerably expanded, preparing it for representation extraction.
The four blue squares with unique patterns represent vectors expanded by four distinct \emph{Keys}, their grid patterns acting as ``signatures''. Each square symbolizes a $(d * T_x - 1)^2$ length vector (not implying 2-dimensional). Similarly, higher-order extensions, like \emph{triple-wise} across three digits, can be achieved with adapted \emph{Keys}.

% Figure environment removed

% The four differently patterned blue squares represent the vectors expanded by four distinct \emph{Keys}, with the grid patterns indicating their ``signatures''. Each square visualizes a $(d * T_x - 1)^2$ length vector (not signifying a 2-dimensional vector).
% In a similar way, higher-order extensions, such as \emph{triple-wise} ones across every three digits, can also be employed by appropriately adapting \emph{Keys}.

Figure~\ref{fig:extractor} illustrates the encoding and decoding processes within the Expander and Reducer, targeting the digit pair $(x_i, x_j)$ for $i\neq j \in 1,\ldots,d$. The Expander function is defined as $f_\theta(x_i,x_j) = x_j \otimes exp(s(x_i)) + t(x_i)$, which hinges on two elementary functions, $s(\cdot)$ and $t(\cdot)$. The \emph{Key} parameter, $\theta$, embodies their weights, $\theta=(w_s, w_t)$.
Specifically, the Expander morphs $x_j$ into a new digit $y_j$ utilizing $x_i$ as a chosen attribute. In contrast, the Reducer symmetrically uses the inverse function $f_\theta^{-1}$, defined as $(y_j-t(y_i)) \otimes exp(-s(y_i))$. 
This method avoids calculating $s^{-1}$ or $t^{-1}$, granting flexibility for nonlinear transformations to $s(\cdot)$ and $t(\cdot)$. 
This design is inspired by the pioneering work of \cite{dinh2016density} on invertible neural network layers that utilize bijective functions. 

% Figure environment removed

\subsection{Relation-Indexed Representation}
\label{sec:indexed_2}
\vspace{-1mm}

% Figure \ref{fig:bridge} illustrates the process of linking $\mathcal{H}$ and $\mathcal{V}$ to model the relationship $\mathcal{X}\rightarrow \mathcal{Y}$. 
%to explicitly include the temporal features of $h$. For now, we suppose $\mathcal{V}$ can capture potential dynamics autonomously, expecting future refinements.

Consider $x$ and $y$ as the instances of $\mathcal{X}$ and $\mathcal{Y}$, respectively, with their corresponding vector representations $h$ and $v$ in $\mathbb{R}^L$. The latent dependency $\mathbf{P}(v| h)$ is utilized for training the relation function $f_\theta = f(;\theta)$, as illustrated in Figure \ref{fig:bridge}.
During each iteration, the learning process undergoes three optimization steps:
\begin{enumerate}[topsep=0pt, partopsep=0pt]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{1pt}
    \item {Optimizing the cause-encoder by $\mathbf{P}(h|x)$, the relation model by $\mathbf{P}(v|h)$, and the effect-decoder by $\mathbf{P}(y|v)$ to reconstruct the relationship $x\rightarrow y$, represented as $h\rightarrow v$ in $\mathbb{R}^L$.}
    \item {Fine-tuning the effect-encoder $\mathbf{P}(v|y)$ and effect-decoder $\mathbf{P}(y|v)$ to accurately represent $y$.} 
    \item {Fine-tuning the cause-encoder $\mathbf{P}(h|x)$ and cause-decoder $\mathbf{P}(x|h)$ to accurately represent $x$.}
\end{enumerate}
%\vspace{1mm}
During the learning process, the values of $h$ and $v$ are iteratively adjusted to reduce their distance in $\mathbb{R}^{L}$. The relation function $f_{\theta} = f(;\theta)$ serves as a bridge to span this distance. It effectively represents the hyper-dimensional variable $\theta \in \mathbb{R}^h$ as an index, guiding the output of $f_{\theta}$ to encapsulate the associated representation $\langle \mathcal{\hat{H}},\theta,\mathcal{\hat{V}} \rangle$. From $\mathcal{\hat{V}}$, the effect component $\mathcal{\hat{Y}}$ can be reconstructed. 
Within the system, for each effect, a series of such relation functions $\{f_{\theta}\}$ is maintained, indexing diverse levels of causal inputs for sequentially building the structural model.

\subsection{Hierarchical Disentanglement of Effects}
\label{sec:factorization}

Consider $\mathcal{Y}=\langle Y, t \rangle \in \mathbb{R}^{b+1}$ having an $n$-level hierarchy, with each level built using a representation function, labeled as $g(;\omega_i)$ for the $i$-th level. For clarity, just use $\omega_i$ to represent the $i$-th level feature in the \emph{latent feature space} $\mathbb{R}^L$; its counterpart in the \emph{observable data space} $\mathbb{R}^{b+1}$ is denoted as $\Omega_i$ (i.e., $\mathcal{\hat{Y}}$ at the $i$-th level).

Let the vector $\omega_i$ in $\mathbb{R}^{L}$ primarily spans a sub-dimensional space, $\mathbb{R}^{L_i}$. This results in the hierarchical disentanglement sequence $\{\mathbb{R}^{L_1}, \ldots, \mathbb{R}^{L_i},\ldots,\mathbb{R}^{L_n}\}$ that fully represent $\mathcal{Y}$.
Function $g_i$ maps from $\mathbb{R}^{b+1}$ to $\mathbb{R}^{L_i}$, taking into account features from all previous levels as attributes.  This gives us:
\vspace{-4mm}

\begin{equation}
    \mathcal{Y} = \sum_{i=1}^{n} \Omega_i, 
     \text{ where } \Omega_i = g_i \bigl( \omega_i ;\ \Omega_1,\ldots, \Omega_{i-1}\bigr) \text{ with } \Omega_i \in \mathbb{R}^{d+1} \text{ and } \omega_i \in \mathbb{R}^{L_i} \subseteq \mathbb{R}^{L}
\end{equation}

\vspace{-3mm}
The $i$-th component in the \emph{observable data space}, denoted as $\Omega_i \in \mathbb{R}^{d+1}$, is articulated through an observational data sequence with the length of $T_y$, along the absolute timeline $t$.
However, in latent space, the objective of $\omega_i$ is to capture dynamics along a relative timeline, $t_i$, which is autonomously determined by the relation at the $i$-th level, not bound by the observational timestamps in $\mathbb{R}^{d+1}$. 


% $\{t_1,\ldots,t_i,\ldots,t_n\}$, each uniquely determined by the relationship at their respective levels, apart from the absolute timeline $t$.
% While in the \emph{observable data space}, the $i$-th level feature, represented as the sum $\Omega_1 +\ldots +\Omega_i$, still maintains its timestamp attribute along $t$.

In the context of a purely observational hierarchy, with $\mathcal{Y}$ substituted by $Y \in \mathbb{R}^b$, Figure~\ref{fig:hand} (b) can be interpreted as follows: Consider three feature levels represented as $\omega_1\in \mathbb{R}^{L_1}$, $\omega_2\in \mathbb{R}^{L_2}$, and $\omega_3\in \mathbb{R}^{L_3}$. For simplicity, assume each subspace is mutually exclusive, so that $L=L_1+L_2+L_3$. In the latent space, the triplet $\langle\omega_1, \omega_2, \omega_3\rangle \in \mathbb{R}^{L}$ comprehensively depicts the image. Their observable counterparts, $\Omega_1$, $\Omega_2$, and $\Omega_3$, are three distinct full-scale images, each showcasing different content. For example, $\Omega_1$ emphasizes finger details, while the combination $\Omega_1+\Omega_2$ reveals the entire hand.




\section{RIRL: Building Structural Models in Latent Space}
\label{sec:RIRL}


% Figure environment removed



By sequentially stacking relation-indexed representations, causal structural models can be established as aligned with causal knowledge.
Figure~\ref{fig:new} illustrates how the RIRL method seals the black-box nature of AI within the latent space, while simultaneously generating interpretable observations that enhance existing \emph{Observation-Oriented} models, such as conducting on-demand counterfactual simulations. These cryptic representations, though opaque to humans, can internally promote model generalization and individualization, managed exclusively within the AI's latent space.

This Section first presents the method to construct structural relationship models in the latent space (subsection \ref{sec:RIRL_1}), and describes the technique for discovering structures within the latent space by identifying potential relationships among initialized variable representations (subsection \ref{sec:RIRL_2}).
%Causal relationships of known edges can be sequentially stacked using existing causal DAGs in domain knowledge.
%Additionally, this approach aids in discovering causal structures within the latent space by identifying potential relationships among the initial variable representations.


%latently. Simultaneously, one can enhance traditional causal models by employing desired latent features, to produce observations as needed, such as counterfactual effects.

%This section introduces a specialized autoencoder architecture crucial for implementing this approach, outlines the method for hierarchical representation disentanglement in constructing graphical models, and presents a causal discovery algorithm for the latent feature space.


% \subsection{Disentanglement of Relationship}
% \vspace{-1.5mm}

% Given a set of $n$-level hierarchical representation functions for $\mathcal{X}$, denoted by $\mathcal{F}(\vartheta) = \bigl\{ f_i \bigl(\theta_i \bigr) \mid i=1,\ldots, n\bigr\}$, the goal is to define $n$ relationship functions, collectively termed $\mathcal{G}$, such that $\mathcal{Y}=\mathcal{G}(\mathcal{X})$ exhibits an $n$-level hierarchy.
% Each $i$-th level relationship function is $ g_i(\mathcal{X}; \varphi_i)$, where $\varphi_i$ is its parameter. Then, we have:
% \begin{equation}
%     \mathcal{G}(\mathcal{X}) = \sum_{i=1}^{n} g_i(\mathcal{X}; \varphi_i) = \sum_{i=1}^{n} g_i(\Theta_{i}; \varphi_i)  =
%     \sum_{i=1}^{n} g_i \bigl(\theta_i;\ \Theta_1,\ldots, \Theta_{i-1}, \varphi_i\bigr) = \mathcal{Y}
% \end{equation}
% The $i$-th level relation-indexed representation for $\mathcal{Y}$ is $g_i (\theta_i;\varphi_i)$ considering the features of the preceding $(i-1)$ levels of $\mathcal{X}$. This relationship can be portrayed as the augmented feature vector $\langle \theta_i, \varphi_i \rangle$ in latent space $\mathbb{R}^L$.
% Using $\vartheta_X$ and $\vartheta_Y$ to distinguish the collective hierarchical representations for $\mathcal{X}$ and $\mathcal{Y}$ respectively, the overall relationship from $\mathcal{X}$ to $\mathcal{Y}$ becomes $\vartheta_Y=\langle \vartheta_X, \varphi \rangle$, where $\varphi=\{\varphi_1,\ldots,\varphi_n\}$. %and $\langle \vartheta_X, \varphi \rangle$ 
% The term $\langle \vartheta_X, \varphi \rangle$ represents the pairwise augmentations between collections $\vartheta_X$ and $\varphi$.



\subsection{Stacking Hierarchical Representations}
\label{sec:RIRL_1}
\vspace{-1mm}
% \noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.94\textwidth}
% \paragraph{Remark 2.} Given a causal graph $G$ with data matrix $\mathbf{X}$ column-augmented by all nodes' attributes, the latent space dimensionality $L$ must satisfy $L\ge rank(\mathbf{X})$ to adequately represent $G$.
% \vspace{-0.5mm}
% \end{minipage}}

A structural relationship can be represented by a causal graph, denoted as $G$. To construct models in the latent space, the latent dimensionality $L$ must be sufficiently large to adequately represent $G$.
Let's denote a data matrix augmented by all observational attributes in $G$ as $\mathbf{X}$. Given the need to include informative relations $\{\theta\}$ for the edges in $G$, it is essential that $L > rank(\mathbf{X})+1$, where the $+1$ accounts for the $t$-timeline.

The PCA principle posits that the space $\mathbb{R}^L$ learned by the autoencoder is spanned by the top principal components of $\mathbf{X}$ \cite{baldi1989neural, plaut2018principal, wang2016auto}.
Hypothetically, reducing $L$ below $rank(\mathbf{X})$ may yield a less adequate but causally more significant latent space through better alignment of dimensions \cite{jain2021mechanism} (Further exploration in this direction is warranted). Bypassing a deep dive into dimensionality boundaries, we rely on empirical fine-tuning for the experiments in this study (reducing $L$ from 64 to 16).
%In this study, we will set aside discussions on the boundaries of dimensionality. Our experiments feature 10 variables with dimensions 1 to 5 (Table~\ref{tab:tower}), and we empirically fine-tune and reduce $L$ from 64 to 16.


% Consider a relationship $\mathcal{X}\rightarrow \mathcal{Y}$, where $Y\in \mathbb{R}^b$ and $\mathcal{Y} = \langle Y, \tau \rangle \in \mathbb{R}^{b+1}$.
% In the context of the effect $\mathcal{Y}$, relation-indexing entails deriving $\mathcal{\hat{Y}}$, which represents the portion of $\mathcal{Y}$ that is identifiable through its relation with $\mathcal{X}$. This segment can serve as a foundation for additional levels of $\mathcal{Y}$'s representations, which is a generalizing process for the established relationship model $\mathcal{X}\rightarrow \mathcal{Y}$.
% For example, suppose a graphical system $\{\mathcal{X}, \mathcal{Y}, \mathcal{Z}\}$ has the relationship $\mathcal{X}\rightarrow \mathcal{Y} \leftarrow \mathcal{Z}$, then, $\mathcal{Y}$ can be viewed as in a two-level hierarchy. The first level is defined by $\mathcal{X}\rightarrow \mathcal{Y}$ and the second by $\langle\mathcal{X}, \mathcal{Z}\rangle \rightarrow \mathcal{Y}$, where the second level enhances the first by incorporating an additional data stream from $\mathcal{Z}$.

% Using a collective representation, all $f_\theta$ functions can be expressed as $\mathcal{F}(X; \vartheta)$, where $\vartheta$ includes all parameters. Thus, the Expander and Reducer can be concisely written as $Y=\mathcal{F}(X; \vartheta)$ and $X = \mathcal{F}^{-1}(Y; \vartheta)$. 

%\vspace{-2.7mm}
% Figure environment removed

Consider the structural causal relationship among dynamically significant variables $\{\mathcal{X}, \mathcal{Y}, \mathcal{Z}\}$, each having corresponding representations $\{\mathcal{H}, \mathcal{V}, \mathcal{K}\} \in \mathbb{R}^L$ initially derived from three autoencoders. Figure~\ref{fig:stack} illustrates the hierarchical assembly of two modeled relationships associated with $\mathcal{Y}$.

In Figure~\ref{fig:stack}, two stacking scenarios are displayed based on varying causal directions. With the established $\mathcal{X}\rightarrow \mathcal{Y}$ relationship in $\mathbb{R}^{L}$, the left-side architecture finalizes the $\mathcal{X}\rightarrow \mathcal{Y}\leftarrow \mathcal{Z}$ structure, while the right-side focuses on $\mathcal{X}\rightarrow \mathcal{Y}\rightarrow \mathcal{Z}$. Through the addition of a representation layer, hierarchical disentanglement is formed, allowing for various input-output combinations (denoted as $\mapsto$) according to specific requirements.

For example, on the left, $\mathbf{P}(v|h) \mapsto \mathbf{P}(\alpha)$ represents the $\mathcal{X}\rightarrow \mathcal{Y}$ relationship, whereas $\mathbf{P}(\alpha|k)$ implies $\mathcal{Z}\rightarrow \mathcal{Y}$. Conversely, on the right, $\mathbf{P}(v) \mapsto P(\beta|k)$ denotes the $\mathcal{Y}\rightarrow \mathcal{Z}$ relationship with $\mathcal{Y}$ as input. Meanwhile, $\mathbf{P}(v|h) \mapsto P(\beta|k)$ captures the causal sequence $\mathcal{X} \rightarrow \mathcal{Y}\rightarrow \mathcal{Z}$.


\subsection{Causal Discovery in Latent Space}
\label{sec:RIRL_2}
%\vspace{-2mm}

Algorithm 1 outlines the heuristic procedure for identifying edges among the initial variable representations. We use Kullback-Leibler Divergence (KLD) as a metric to evaluate the strength of causal relationships. Specifically, as depicted in Figure~\ref{fig:bridge}, KLD evaluates the similarity between the RNN output $\mathbf{P}(v|h)$ and the prior $\mathbf{P}(v)$. Lower KLD values indicate stronger causal relationships due to closer alignment with the ground truth. Conversely, while Mean Squared Error (MSE) is a frequently used evaluation metric, its sensitivity to data variances \cite{reisach2021beware, kaiser2021unsuitability} leads us to utilize it as a supplementary measure in this study. %In the graphical representation context, we refer to variables $A$ and $B$ in the edge $A\rightarrow B$ as the \emph{cause node} and \emph{effect node}, respectively.

\begin{minipage}{\textwidth}
    \begin{minipage}[b]{0.52\textwidth}
    \raggedright
    \begin{algorithm}[H]
    \footnotesize
    \SetAlgoLined
    \KwResult{ ordered edges set $\mathbf{E}=\{e_1, \ldots,e_n\}$ }
    $\mathbf{E}=\{\}$ ; $N_R = \{ n_0 \mid n_0 \in N, Parent(n_0)=\varnothing\}$ \;
    \While{$N_R \subset N$}{
    $\Delta = \{\}$ \;
    \For{ $n \in N$ }{
        \For{$p \in Parent(n)$}{
            \If{$n \notin N_R$ and $p \in N_R$}{
                $e=(p,n)$; \ 
                $\beta = \{\}$;\\ 
                \For{$r \in N_R$}{
                    \If{$r \in Parent(n)$ and $r \neq p$}{
                        $\beta = \beta \cup r$}
                    }
                $\delta_e = K(\beta \cup p, n) - K(\beta, n)$;\\
                $\Delta = \Delta \cup \delta_e$;
            }
        }
    }
    $\sigma = argmin_e(\delta_e \mid \delta_e \in \Delta)$;\\
    $\mathbf{E} = \mathbf{E} \cup \sigma$; \ 
    $N_R = N_R \cup n_{\sigma}$;\\
    }
     \caption{Latent Space Causal Discovery}
    \end{algorithm}
    \end{minipage}
%\hfill 
    \begin{minipage}[b]{0.4\textwidth}
    \raggedleft
        \label{tab:table1}
        \begin{tabular}{|l|l|}
        \hline
           $G=(N,E)$ & graph $G$ consists of $N$ and $E$ \\
           $N$ & the set of nodes\\
           $E$ & the set of edges\\
           $N_R$ & the set of reachable nodes\\
           $\mathbf{E}$ & the list of discovered edges\\
           $K(\beta, n)$ & KLD metric of effect $\beta\rightarrow n$\\
           $\beta$ & the cause nodes \\
           $n$ & the effect node\\
           $\delta_e$ & KLD Gain of candidate edge $e$\\
           $\Delta=\{\delta_e\}$ & the set $\{\delta_e\}$ for $e$ \\
           $n$,$p$,$r$ & notations of nodes\\
           $e$,$\sigma$ & notations of edges\\
          \hline
        \end{tabular} %
%       \captionof{table}{A table beside a figure}
    \end{minipage}
\end{minipage}



Figure~\ref{fig:discover} illustrates the causal structure discovery process in latent space over four steps. Two edges, ($e_1$ and $e_3$), are sequentially selected, with $e_1$ setting node $B$ as the starting point for $e_3$. In step 3, edge $e_2$ from $A$ to $C$ is deselected and reassessed due to the new edge $e_3$ altering $C$'s existing causal conditions. The final DAG represents the resulting causal structure.

%\vspace{-2mm}
% Figure environment removed




\vspace{-2mm}
\section{Efficacy Validation Experiments}
\vspace{-3mm}
\label{sec:experiment}

The experiments aim to validate the efficacy of the RIRL method from three aspects: 1) the performance of the proposed higher-dimensional representations, evaluated by reconstruction accuracy, 2) the construction of a clear effect hierarchy through the stacking of relation-indexed representations, and 3) the identification of DAG structures within the latent space through discovery.
A full demonstration of the conducted experiments is available online \footnote{https://github.com/kflijia/bijective\_crossing\_functions.git}.
The experiments in this study present two primary limitations, detailed as follows:

Firstly, the dataset used in the current experiments may not be optimal for assessing the efficacy of RIRL. In particular, real-world causal data, such as clinical records, often contain inherent biases. While empirical constraints limited our access to such data for this study, the synthetic data we utilized may not be ideal for validating the improved model robustness conferred by RIRL. For experiments that validate the presence of such inherent biases, readers are referred to prior research \cite{li2020teaching}.

Secondly, the time windows designated for cause and effect, $T_x$ and $T_y$, are consistently set at 10 and 1, respectively. This constraint arose from an initial oversight in the experimental design, wherein the pivotal role of \emph{dynamics} was not fully recognized, leading to restrictions set by the RNN pattern. This limitation manifests when constructing causal sequences, such as in $\mathcal{X} \rightarrow \mathcal{Y}\rightarrow \mathcal{Z}$. While the model adeptly captures single-hop effects, it struggles with two-hop information due to the dynamics in $\mathcal{Y}$ being segmented into statics by the effect window $T_y=1$, resulting in a loss of dynamic information. However, extending the length of $T_y$ does not pose a significant technical challenge to future works.



%\vspace{-1mm}
\subsection{Hydrology Dataset}
%\vspace{-2mm}


\vspace{-3mm}
% Figure environment removed

The dataset chosen for our experiments is a widely-used synthetic resource in the field of hydrology, aimed at enhancing streamflow predictions based on observed environmental conditions such as temperature and precipitation. %The application of RIRL aims to create a streamflow prediction model that is generalizable across various watersheds. While these watersheds share a fundamental hydrological scheme governed by physical rules, they may exhibit unique features due to unobserved conditions such as economic development and land use. Current models based on physical knowledge, however, often lack the flexibility to fully capture multiple levels of dynamical temporal features across these watersheds.
In hydrology, deep learning, particularly RNN models, has gained favor for extracting observational representations and predicting streamflow \cite{goodwell2020debates, kratzert2018rainfall}. 
We focus on a simulation of the Root River Headwater watershed in Southeast Minnesota, covering 60 consecutive virtual years with daily updates. The simulated data is from the Soil and Water Assessment Tool (SWAT), a comprehensive system grounded in physical modules, to generate dynamically significant hydrological time series. %The performance evaluations predominantly focus on the accuracy of the autoencoder reconstructions.

Figure \ref{fig:stream} displays the causal DAG employed by SWAT, complete with node descriptions. The hydrological routines are color-coded based on their contribution to output streamflow. Surface runoff (1st tier) significantly impacts rapid streamflow peaks, followed by lateral flow (2nd tier). Baseflow dynamics (3rd tier) have a subtler influence. Our causal discovery experiments aim to reveal these underlying tiers. %relationships from the observed data.


\vspace{-1mm}
\subsection{Higher-Dimensional Variable Representation Test}
\vspace{-2mm}

In this test, we have a total of ten variables (or nodes), each requiring a separate autoencoder for initializing a higher-dimensional representation. Table \ref{tab:tower} lists the statistics of their post-scaled (i.e., normalized) attributes, as well as their autoencoders' reconstruction accuracies. Accuracy is assessed in the root mean square error (RMSE), where a lower RMSE indicates higher accuracy for both scaled and unscaled data.


The task is challenging due to the limited dimensionalities of the ten variables - maxing out at just 5 and the target node, $J$, having just one attribute. To mitigate this, we duplicate the input vector to a consistent 12-length and add 12 dummy variables for months, resulting in a 24-dimensional input. A double-wise extension amplifies this to 576 dimensions, from which a 16-dimensional representation is extracted via the autoencoder.
Another issue is the presence of meaningful zero-values, such as node $D$ (Snowpack in winter), which contributes numerous zeros in other seasons and is closely linked to node $E$ (Soil Water). We tackle this by adding non-zero indicator variables, called \emph{masks}, evaluated via binary cross-entropy (BCE).

Despite challenges, RMSE values ranging from $0.01$ to $0.09$ indicate success, except for node $F$ (the Aquifer). Given that aquifer research is still emerging (i.e., the 3rd tier baseflow routine), it is likely that node $F$ in this synthetic dataset may better represent noise than meaningful data.


\begin{table*}[t]
\caption{Statistics of variable attributes and performances of the variable representation test.}
\label{tab:tower}
\vspace{-2mm}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{|
>{\columncolor[HTML]{EFEFEF}}c |c|l|l|l|l|l|l|l|l|}
\hline
\cellcolor[HTML]{C0C0C0}Variable & \cellcolor[HTML]{C0C0C0} Dim & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Mean} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Std} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Min} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Max} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Non-Zero Rate\%} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}RMSE on Scaled} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}RMSE on Unscaled} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}BCE of Mask} \\ \hline
A & 5 & 1.8513 & 1.5496 & -3.3557 & 7.6809 & 87.54 & 0.093 & 0.871 & 0.095 \\ \hline
B & 4 & 0.7687 & 1.1353 & -3.3557 & 5.9710 & 64.52 & 0.076 & 0.678 & 1.132 \\ \hline
C & 2 & 1.0342 & 1.0025 & 0.0 & 6.2145 & 94.42 & 0.037 & 0.089 & 0.428 \\ \hline
D & 3 & 0.0458 & 0.2005 & 0.0 & 5.2434 & 11.40 & 0.015 & 0.679 & 0.445 \\ \hline
E & 2 & 3.1449 & 1.0000 & 0.0285 & 5.0916 & 100 & 0.058 & 3.343 & 0.643 \\ \hline
F & 4 & 0.3922 & 0.8962 & 0.0 & 8.6122 & 59.08 & 0.326 & 7.178 & 2.045 \\ \hline
G & 4 & 0.7180 & 1.1064 & 0.0 & 8.2551 & 47.87 & 0.045 & 0.81 & 1.327 \\ \hline
H & 4 & 0.7344 & 1.0193 & 0.0 & 7.6350 & 49.93 & 0.045 & 0.009 & 1.345 \\ \hline
I & 3 & 0.1432 & 0.6137 & 0.0 & 8.3880 & 21.66 & 0.035 & 0.009 & 1.672 \\ \hline
J & 1 & 0.0410 & 0.2000 & 0.0 & 7.8903 & 21.75 & 0.007 & 0.098 & 1.088 \\ \hline
\end{tabular}}
\end{table*}



\begin{table*}[t]
%\vspace{-2.5mm}
\caption{Brief summary of the latent space causal discovery test.}
\label{tab:discv}
\vspace{-2mm}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{|c|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
\cellcolor[HTML]{C0C0C0}Edge & \cellcolor[HTML]{FFCCC9}A$\rightarrow$C & \cellcolor[HTML]{FFCCC9}B$\rightarrow$D & \cellcolor[HTML]{FFCCC9}C$\rightarrow$D & \cellcolor[HTML]{FFCCC9}C$\rightarrow$G & \cellcolor[HTML]{FFCCC9}D$\rightarrow$G & \cellcolor[HTML]{FFCCC9}G$\rightarrow$J & \cellcolor[HTML]{FFCCC9}D$\rightarrow$H & \cellcolor[HTML]{FFCCC9}H$\rightarrow$J & \cellcolor[HTML]{FFCE93}B$\rightarrow$E & \cellcolor[HTML]{FFCE93}E$\rightarrow$G & \cellcolor[HTML]{FFCE93}E$\rightarrow$H & \cellcolor[HTML]{FFCE93}C$\rightarrow$E & \cellcolor[HTML]{ABE9E7}E$\rightarrow$F & \cellcolor[HTML]{ABE9E7}F$\rightarrow$I & \cellcolor[HTML]{ABE9E7}I$\rightarrow$J & \cellcolor[HTML]{ABE9E7}D$\rightarrow$I \\ \hline
\cellcolor[HTML]{C0C0C0}KLD & 7.63 & 8.51 & 10.14 & 11.60 & 27.87 & 5.29 & 25.19 & 15.93 & 37.07 & 39.13 & 39.88 & 46.58 & 53.68 & 45.64 & 17.41 & 75.57 \\ \hline
\rowcolor[HTML]{FFFFFF} 
\cellcolor[HTML]{C0C0C0}Gain & 7.63 & 8.51 & 1.135 & 11.60 & 2.454 & 5.29 & 25.19 & 0.209 & 37.07 & -5.91 & -3.29 & 2.677 & 53.68 & 45.64 & 0.028 & 3.384 \\ \hline
\end{tabular}}
\vspace{-2mm}
\end{table*}


%\vspace{1mm}
\subsection{Hierarchical Disentanglement Test}
\vspace{-2mm}

Table \ref{tab:unit} provides the performance comparison of stacking relation-indexed representations on each node. The term ``single-effect'' is to describe the accuracy of a specific effect node when reconstructed from a single cause node (e.g., $B\rightarrow D$ and $C\rightarrow D$), and ``full-effect'' for the accuracy when all its cause nodes are stacked (e.g., $BC\rightarrow D$). To provide context, we also include baseline performance scores based on the initial variable representations. During the relation learning process, the effect node serves two purposes: it maintains its own accurate representation (as per optimization no.2 in \ref{sec:indexed_2}) and helps reconstruct the relationship (as per optimization no.1). Both aspects are evaluated in Table \ref{tab:unit}.


% For example, node $G$ has three possible causes $C$, $D$, and $E$, so we built four causal effect models in total: the individual pairwise causations $C\rightarrow G$, $D\rightarrow G$, $E\rightarrow G$, and the stacked causal effect $CDE\rightarrow G$. For convenience, we refer to them as "\emph{pair-relation}" and "\emph{stacking-relation}" respectively. We also listed the initial reconstruction performance for each variable in the column named "Variable Reconstruction (initial performance)" as a comparison baseline in Table \ref{tab:unit}.


%\vspace{-3mm}
% Figure environment removed

The KLD metrics in Table \ref{tab:unit} indicate the strength of learned causality, with a lower value signifying stronger. For instance, node $J$'s minimal KLD values suggest a significant effect caused by nodes $G$ (Surface Runoff), $H$ (Lateral), and $I$ (Baseflow). In contrast, the high KLD values imply that predicting variable $I$ using $D$ and $F$ is challenging. 
For nodes $D$, $E$, and $J$, the ``full-effect'' are moderate compared to their ``single-effect'' scores, suggesting a lack of informative associations among the cause nodes. In contrast, for nodes $G$ and $H$, lower ``full-effect'' KLD values imply capturing meaningful associative effects through hierarchical stacking. The KLD metric also reveals the most contributive cause node to the effect node. For example, the proximity of the $C\rightarrow G$ strength to $CDE\rightarrow G$ suggests that $C$ is the primary contributor to this causal relationship.

Figure~\ref{fig:G} showcases reconstructed time series, for the effect nodes $J$, $G$, and $I$, in the same synthetic year to provide a straightforward overview of the hierarchical representation performances. 
Here, black dots represent the ground truth; the blue line indicates reconstruction via the initial variable representation, and the ``full-effect'' representation generates the red line. 
In addition to RMSE, we also employ the Nash–Sutcliffe model efficiency coefficient (NSE) as an accuracy metric, commonly used in hydrological predictions. The NSE ranges from -$\infty$ to 1, with values closer to 1 indicating higher accuracy.

The initial variable representation closely aligns with the ground truth, as shown in Figure~\ref{fig:G}, attesting to the efficacy of our proposed autoencoder architecture. As expected, the ``full-effect'' performs better than the ``single-effect'' for each effect node. Node $J$ exhibits the best prediction, whereas node $I$ presents a challenge. For node $G$, causality from $C$ proves to be significantly stronger than the other two, $D$ and $E$.


% One may observe via the demo that our experiments do not show smooth information flows along successive long causal chains. Since RNNs are designed primarily for capturing the dynamics of causes rather than the effects, relying on them to autonomously construct dynamical representations of the effects might prove unreliable. It underscores a significant opportunity for enhancing effectiveness by improving the architecture.



\vspace{-1mm}
\subsection{Latent Space Causal Discovery Test}
%\vspace{-2mm}

The discovery test initiates with source nodes $A$ and $B$ and proceeds to identify potential edges, culminating in the target node $J$. Candidate edges are selected based on their contributions to the overall KLD sum (less gain is better). 
Table \ref{tab:discv} shows the order in which existing edges are discovered, along with the corresponding KLD sums and gains after each edge is included. Color-coding in the cells corresponds to Figure \ref{fig:stream}, indicating tiers of causal routines. The arrangement underscores the efficacy of this latent space discovery approach.

A comprehensive list of candidate edges evaluated in each discovery round is provided in Table \ref{tab:discv_rounds} in Appendix A. For comparative purposes, we also performed a 10-fold cross-validation using the conventional FGES discovery method; those results are available in Table \ref{tab:fges} in Appendix A.


%\vspace{-3mm}
\begin{table*}[t]
\caption{Effect Reconstruction Performances of RIRL sorted by effect nodes.}
\label{tab:unit}
\vspace{-2mm}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{|l|lll|l|lll|llll|}
\hline
\rowcolor[HTML]{C0C0C0} 
\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} } & \multicolumn{3}{l|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \begin{tabular}[c]{@{}l@{}}\ \ \ \ \ \ Variable Representation\\ \ \ \ \ \ \ \ (Initial)\end{tabular}}} & \cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} } & \multicolumn{3}{l|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000}  \begin{tabular}[c]{@{}l@{}}\ \ \ \ \ \ Variable Representation\\ \ \ \ \ \ \ \ (in Relation Learning)\end{tabular}}} & \multicolumn{4}{l|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \ \ \ \ \ \ \ \ \ Relationship Reconstruction}} \\ \cline{2-4} \cline{6-12} 
\rowcolor[HTML]{C0C0C0} 
\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} } & \multicolumn{2}{l|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000}\ \ \ \ \ \  \ \ \ \ RMSE}} & {\color[HTML]{000000} BCE} & \cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} } & \multicolumn{2}{l|}{\cellcolor[HTML]{C0C0C0}\ \ \ \ \ \ \ \ \ \ RMSE} & BCE & \multicolumn{2}{l|}{\cellcolor[HTML]{C0C0C0} \ \ \ \ \ \ \ \ \ RMSE} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}BCE} & KLD \\ \cline{2-4} \cline{6-12} 
\rowcolor[HTML]{C0C0C0} 
\multirow{-3}{*}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \begin{tabular}[c]{@{}l@{}}Result\\ Node\\ \ \end{tabular}}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Scaled\\ \ \ Values\end{tabular}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Unscaled\\ \ \ Values\end{tabular}} & Mask & \multirow{-3}{*}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \begin{tabular}[c]{@{}l@{}}Cause\\ Node\end{tabular}}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Scaled\\ \ \ Values\end{tabular}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Unscaled\\ \ \ Values\end{tabular}} & Mask & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Scaled\\ \ \ Values\end{tabular}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Unscaled\\ \ \ Values\end{tabular}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Mask} & \begin{tabular}[c]{@{}l@{}}(in latent\\ \ \ space)\end{tabular} \\ \hline
\cellcolor[HTML]{EFEFEF}C & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}0.037} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}0.089} & \cellcolor[HTML]{EFEFEF}0.428 & A & \multicolumn{1}{l|}{0.0295} & \multicolumn{1}{l|}{0.0616} & 0.4278 & \multicolumn{1}{l|}{0.1747} & \multicolumn{1}{l|}{0.3334} & \multicolumn{1}{l|}{0.4278} & 7.6353 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & BC & \multicolumn{1}{l|}{0.0350} & \multicolumn{1}{l|}{1.0179} & 0.1355 & \multicolumn{1}{l|}{0.0509} & \multicolumn{1}{l|}{1.7059} & \multicolumn{1}{l|}{0.1285} & 9.6502 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & B & \multicolumn{1}{l|}{0.0341} & \multicolumn{1}{l|}{1.0361} & 0.1693 & \multicolumn{1}{l|}{0.0516} & \multicolumn{1}{l|}{1.7737} & \multicolumn{1}{l|}{0.1925} & 8.5147 \\ \cline{5-5}
\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}D} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.015}} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.679}} & \multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.445} & C & \multicolumn{1}{l|}{0.0331} & \multicolumn{1}{l|}{0.9818} & 0.3404 & \multicolumn{1}{l|}{0.0512} & \multicolumn{1}{l|}{1.7265} & \multicolumn{1}{l|}{0.3667} & 10.149 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & BC & \multicolumn{1}{l|}{0.4612} & \multicolumn{1}{l|}{26.605} & 0.6427 & \multicolumn{1}{l|}{0.7827} & \multicolumn{1}{l|}{45.149} & \multicolumn{1}{l|}{0.6427} & 39.750 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & B & \multicolumn{1}{l|}{0.6428} & \multicolumn{1}{l|}{37.076} & 0.6427 & \multicolumn{1}{l|}{0.8209} & \multicolumn{1}{l|}{47.353} & \multicolumn{1}{l|}{0.6427} & 37.072 \\ \cline{5-5}
\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}E} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.058}} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}3.343}} & \multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.643} & C & \multicolumn{1}{l|}{0.5212} & \multicolumn{1}{l|}{30.065} & 1.2854 & \multicolumn{1}{l|}{0.7939} & \multicolumn{1}{l|}{45.791} & \multicolumn{1}{l|}{1.2854} & 46.587 \\ \hline
\cellcolor[HTML]{EFEFEF}F & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}0.326} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}7.178} & \cellcolor[HTML]{EFEFEF}2.045 & E & \multicolumn{1}{l|}{0.4334} & \multicolumn{1}{l|}{8.3807} & 3.0895 & \multicolumn{1}{l|}{0.4509} & \multicolumn{1}{l|}{5.9553} & \multicolumn{1}{l|}{3.0895} & 53.680 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & CDE & \multicolumn{1}{l|}{0.0538} & \multicolumn{1}{l|}{0.9598} & 0.0878 & \multicolumn{1}{l|}{0.1719} & \multicolumn{1}{l|}{3.5736} & \multicolumn{1}{l|}{0.1340} & 8.1360 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & C & \multicolumn{1}{l|}{0.1057} & \multicolumn{1}{l|}{1.4219} & 0.1078 & \multicolumn{1}{l|}{0.2996} & \multicolumn{1}{l|}{4.6278} & \multicolumn{1}{l|}{0.1362} & 11.601 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & D & \multicolumn{1}{l|}{0.1773} & \multicolumn{1}{l|}{3.6083} & 0.1842 & \multicolumn{1}{l|}{0.4112} & \multicolumn{1}{l|}{8.0841} & \multicolumn{1}{l|}{0.2228} & 27.879 \\ \cline{5-5}
\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}G} & \multicolumn{1}{l|}{\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}0.045}} & \multicolumn{1}{l|}{\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}0.81}} & \multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}1.327} & E & \multicolumn{1}{l|}{0.1949} & \multicolumn{1}{l|}{4.7124} & 0.1482 & \multicolumn{1}{l|}{0.5564} & \multicolumn{1}{l|}{10.852} & \multicolumn{1}{l|}{0.1877} & 39.133 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & DE & \multicolumn{1}{l|}{0.0889} & \multicolumn{1}{l|}{0.0099} & 2.5980 & \multicolumn{1}{l|}{0.3564} & \multicolumn{1}{l|}{0.0096} & \multicolumn{1}{l|}{2.5980} & 21.905 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & D & \multicolumn{1}{l|}{0.0878} & \multicolumn{1}{l|}{0.0104} & 0.0911 & \multicolumn{1}{l|}{0.4301} & \multicolumn{1}{l|}{0.0095} & \multicolumn{1}{l|}{0.0911} & 25.198 \\ \cline{5-5}
\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}H} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.045}} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.009}} & \multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}1.345} & E & \multicolumn{1}{l|}{0.1162} & \multicolumn{1}{l|}{0.0105} & 0.1482 & \multicolumn{1}{l|}{0.5168} & \multicolumn{1}{l|}{0.0097} & \multicolumn{1}{l|}{3.8514} & 39.886 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & DF & \multicolumn{1}{l|}{0.0600} & \multicolumn{1}{l|}{0.0103} & 3.4493 & \multicolumn{1}{l|}{0.1158} & \multicolumn{1}{l|}{0.0099} & \multicolumn{1}{l|}{3.4493} & 49.033 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & D & \multicolumn{1}{l|}{0.1212} & \multicolumn{1}{l|}{0.0108} & 3.0048 & \multicolumn{1}{l|}{0.2073} & \multicolumn{1}{l|}{0.0108} & \multicolumn{1}{l|}{3.0048} & 75.577 \\ \cline{5-5}
\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}I} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.035}} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.009}} & \multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}1.672} & F & \multicolumn{1}{l|}{0.0540} & \multicolumn{1}{l|}{0.0102} & 3.4493 & \multicolumn{1}{l|}{0.0948} & \multicolumn{1}{l|}{0.0098} & \multicolumn{1}{l|}{3.4493} & 45.648 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & GHI & \multicolumn{1}{l|}{0.0052} & \multicolumn{1}{l|}{0.0742} & 0.2593 & \multicolumn{1}{l|}{0.0090} & \multicolumn{1}{l|}{0.1269} & \multicolumn{1}{l|}{0.2937} & 5.5300 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & G & \multicolumn{1}{l|}{0.0077} & \multicolumn{1}{l|}{0.1085} & 0.4009 & \multicolumn{1}{l|}{0.0099} & \multicolumn{1}{l|}{0.1390} & \multicolumn{1}{l|}{0.4375} & 5.2924 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & H & \multicolumn{1}{l|}{0.0159} & \multicolumn{1}{l|}{0.2239} & 0.4584 & \multicolumn{1}{l|}{0.0393} & \multicolumn{1}{l|}{0.5520} & \multicolumn{1}{l|}{0.4938} & 15.930 \\ \cline{5-5}
\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}J} & \multicolumn{1}{l|}{\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}0.007}} & \multicolumn{1}{l|}{\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}0.098}} & \multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}1.088} & I & \multicolumn{1}{l|}{0.0308} & \multicolumn{1}{l|}{0.4328} & 0.3818 & \multicolumn{1}{l|}{0.0397} & \multicolumn{1}{l|}{0.5564} & \multicolumn{1}{l|}{0.3954} & 17.410 \\ \hline
\end{tabular}
}
\vspace{-3mm}
\end{table*}


\section{Conclusions}\label{sec:conclusion}
%\vspace{-2mm}

The concept of Artificial General Intelligence (AGI) has sparked extensive discussions over the years \cite{marcus2020next}. Recent debates have particularly focused on whether large language models (LLMs) edge us closer to realizing AGI \cite{schaeffer2023emergent}. A central question is whether symbols, as well as symbol-grounded systems, such as AI, can represent empirical understanding and inquiries \cite{newell2007computer, pavlick2023symbols}.

We posit that the core challenge lies in formulating the human understanding process into symbols, especially those abstract, intangible concepts embedded within our knowledge. Specifically, we require a framework that can symbolize the fundamental inquiries driving our learning while remaining intuitively comprehensible. This study introduces the concept of ``hyper-dimensional'' distributions to represent the unobservable knowledge fragments in our cognitions. And, as distributions, they can interface with familiar ``observable-dimensional'' symbols, such as conventional variables. 

Through this \emph{dimensionality viewpoint}, this study reexamines questions ranging from traditional causal inference to contemporary AI alignment issues, unifying them within a single symbolic framework. Notably, our existing modeling, including AI, appears to overlook two critical aspects: ``dynamics'' and ``relative timelines'', which are integral to human understanding. While efforts have been made to address these, spanning from the classic de-confounding in statistics to the novel introduction of hierarchical temporal memory in neuroscience \cite{wu2018hierarchical}, we emphasize that human comprehension access these abstract spaces indexing via unseen ``relations'', which also infuse our models with richer information.

The journey to achieving AGI will undoubtedly be a historically extensive and complex undertaking, necessitating a vast array of knowledge-aligned models. This study aspires to contribute foundational insights for future developments in the field.




% Driven by the misalignment issues between causal knowledge and established causal models in widespread AI applications, this study examines fundamental limitations of the dominant \emph{Observation-Oriented} learning paradigm. In response, we advocate for a novel \emph{Relation-Oriented} paradigm, inspired by the relation-centric nature of human knowledge, and complemented by a practical approach of \emph{Relation-Indexed Representation Learning} (RIRL), with demonstrated efficacy.

% The concept of a ``hyper-dimension'' is initially proposed, as an accommodation for unobservable knowledge. We subsequently build a comprehensive framework of dimensionality, to offer more intuitive insights into relationship learning. 
% The discrepancy, between our comprehension of ``time'' and the single timeline used in our causal models, inherently causes misalignment, and results in model generalizability issues.

% \emph{Relation-Oriented} reflects the process of human understanding, 
% aims to mitigate AI misalignment, paving the way toward causally interpretable AGI. Constructing AGI is a long-term, intricate process requiring extensive work within interdisciplinary efforts, and we seek to lay a foundation for its future advancements.

\vspace{4mm}


\section*{Acknowledgements}
\vspace{-2mm}

I'd like to extend my heartfelt thanks to my friend, Dr. Gao, Qiman, the lone companion willing to engage in profound philosophical discussions with me, and who has provided invaluable advice. Additionally, my gratitude goes to GPT-4 for its assistance in enhancing my English writing.
I also wish to thank my advisor, Prof. Vipin Kumar, for the initial support in the beginning stage of this work.

\hfill Jia Li, October 2023

\vspace{10mm}

\bibliography{main}
\bibliographystyle{tmlr}


\appendix
\section{Appendix: Complete Experimental Results of Causal Discovery}
%\includepdf[pages=-]{mainz_append_pdf.pdf}

\pagebreak


%\newpage
\begin{sidewaystable}
%\begin{table*}[t]
\centering
\caption{The Complete Results of Heuristic Causal Discovery in latent space.
Each row stands for a round of detection, with `\#'' identifying the round number, and all candidate edges are listed with their KLD gains as below. 1) Green cells: the newly detected edges. 2) Red cells: the selected edge.
3) Blue cells: the trimmed edges accordingly. }
%\vspace{-20mm}
\label{tab:discv_rounds}
\resizebox{1\columnwidth}{!}{%
%\rotatebox{90}{

\begin{tabular}{|c|c|c|cccccccccccccc}
\cline{1-9}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \textbf{A$\rightarrow$ C}} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ D & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{B$\rightarrow$ C}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ D} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 1}} & \cellcolor[HTML]{FFCCC9}{\color[HTML]{000000} \textbf{7.6354}} & 19.7407 & \multicolumn{1}{c|}{60.1876} & \multicolumn{1}{c|}{119.7730} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{8.4753}} & \multicolumn{1}{c|}{8.5147} & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} &  &  &  &  &  &  &  &  \\ \cline{1-13}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ D & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{B$\rightarrow$ D}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ D} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 2}} & 19.7407 & 60.1876 & \multicolumn{1}{c|}{119.7730} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{8.5147}}} & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}10.1490} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}46.5876} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}111.2978} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}11.6012} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}39.2361} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}95.1564} &  &  &  &  \\ \hline
\rowcolor[HTML]{C0C0C0} 
\cellcolor[HTML]{C0C0C0} & \textbf{A$\rightarrow$ D} & A$\rightarrow$ E & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{C$\rightarrow$ D}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 3}} & \cellcolor[HTML]{CBCEFB}\textbf{9.7357} & 60.1876 & \multicolumn{1}{c|}{119.7730} & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{1.1355}}} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{11.6012} & \multicolumn{1}{c|}{39.2361} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}63.7348} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}123.3203} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}27.8798} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}25.1988} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}75.5775} \\ \hline
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \textbf{C$\rightarrow$ G}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 4}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{000000} \textbf{11.6012}}} & \multicolumn{1}{c|}{39.2361} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{27.8798} & \multicolumn{1}{c|}{25.1988} & \multicolumn{1}{c|}{75.5775} &  &  \\ \cline{1-15}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{D$\rightarrow$ G}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}G$\rightarrow$ J} &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 5}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{39.2361} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{2.4540}}} & \multicolumn{1}{c|}{25.1988} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}5.2924} &  &  \\ \cline{1-15}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{G$\rightarrow$ J}}} &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 6}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{39.2361} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{25.1988} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{5.2924}}} &  &  &  \\ \cline{1-14}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{C$\rightarrow$ H}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{D$\rightarrow$ H}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 7}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{39.2361}} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{25.1988}}} & \multicolumn{1}{c|}{75.5775} &  &  &  &  \\ \cline{1-13}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{H$\rightarrow$ J}}} &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 8}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{0.2092}}} &  &  &  &  &  \\ \cline{1-12}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}\textbf{A$\rightarrow$ E} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{C$\rightarrow$ E}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 9}} & \cellcolor[HTML]{CBCEFB}\textbf{60.1876} & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{46.5876}}} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} &  &  &  &  &  &  \\ \cline{1-13}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{B$\rightarrow$ E}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{D$\rightarrow$ E}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ I} &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 10}} & 119.7730 & \cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{-6.8372}} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{17.0407}} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}53.6806} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}-5.9191} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}-3.2931} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}110.2558} &  &  &  &  \\ \cline{1-13}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \cellcolor[HTML]{C0C0C0}B$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{E$\rightarrow$ G}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ I} &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 11}} & 119.7730 & 132.7717 & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{53.6806} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{-5.9191}} & \multicolumn{1}{c|}{-3.2931} & \multicolumn{1}{c|}{110.2558} &  &  &  &  &  &  \\ \cline{1-11}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \cellcolor[HTML]{C0C0C0}B$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{E$\rightarrow$ H}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ I} &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 12}} & 119.7730 & 132.7717 & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{53.6806} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{-3.2931}} & \multicolumn{1}{c|}{110.2558} &  &  &  &  &  &  &  \\ \cline{1-10}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}\textbf{A$\rightarrow$ F} & \cellcolor[HTML]{C0C0C0}\textbf{B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{C$\rightarrow$ F}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{D$\rightarrow$ F}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{E$\rightarrow$ F}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ I} &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 13}} & \cellcolor[HTML]{CBCEFB}\textbf{119.7730} & \cellcolor[HTML]{CBCEFB}\textbf{132.7717} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{111.2978}} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{123.3203}} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{53.6806}} & \multicolumn{1}{c|}{110.2558} &  &  &  &  &  &  &  &  \\ \cline{1-9}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}C$\rightarrow$ I & \cellcolor[HTML]{C0C0C0}D$\rightarrow$ I & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{E$\rightarrow$ I}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{F$\rightarrow$ I}} &  &  &  &  &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 14}} & 95.1564 & 75.5775 & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{110.2558}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{45.6490}} &  &  &  &  &  &  &  &  &  &  &  &  \\ \cline{1-5}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}C$\rightarrow$ I & \cellcolor[HTML]{C0C0C0}D$\rightarrow$ I & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{I$\rightarrow$ J}} &  &  &  &  &  &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 15}} & 15.0222 & 3.3845 & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.0284}} &  &  &  &  &  &  &  &  &  &  &  &  &  \\ \cline{1-4}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}\textbf{C$\rightarrow$ I} & \cellcolor[HTML]{C0C0C0}\textbf{D$\rightarrow$ I} &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 16}} & \cellcolor[HTML]{CBCEFB}\textbf{15.0222} & \cellcolor[HTML]{FFCCC9}\textbf{3.3845} &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\ \cline{1-3}
\end{tabular}

}%}
%\vspace{-3mm}
%\end{table*}

\end{sidewaystable}







\begin{sidewaystable}
%\centering

\caption{Average performance of 10-Fold FGES (Fast Greedy Equivalence Search) causal discovery, with the prior knowledge that each node can only cause the other nodes with the same or greater depth with it. An edge means connecting two attributes from two different nodes, respectively. Thus, the number of possible edges between two nodes is the multiplication of the numbers of their attributes, i.e., the lengths of their data vectors.
\\ (All experiments are performed with 6 different Independent-Test kernels, including chi-square-test, d-sep-test, prob-test, disc-bic-test, fisher-z-test, mvplr-test. But their results turn out to be identical.)}
\label{tab:fges}
\resizebox{1\columnwidth}{!}{%
%\Rotatebox{90}{
%
\begin{tabular}{cccccccccccccccccc}
\hline
\rowcolor[HTML]{C0C0C0} 
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}Cause Node} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A} & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}B} & \multicolumn{3}{c|}{\cellcolor[HTML]{C0C0C0}C} & \multicolumn{3}{c|}{\cellcolor[HTML]{C0C0C0}D} & \multicolumn{3}{c|}{\cellcolor[HTML]{C0C0C0}E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}F} & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}I} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}True\\ Causation\end{tabular}} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}A$\rightarrow$ C} & \multicolumn{1}{c|}{B$\rightarrow$ D} & \multicolumn{1}{c|}{B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}C$\rightarrow$ D} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}C$\rightarrow$ G} & \multicolumn{1}{c|}{D$\rightarrow$ G} & \multicolumn{1}{c|}{D$\rightarrow$ H} & \multicolumn{1}{c|}{D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}E$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}E$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}E$\rightarrow$ H} & \multicolumn{1}{c|}{F$\rightarrow$ I} & \multicolumn{2}{c|}{\cellcolor[HTML]{EFEFEF}G$\rightarrow$ J} & \multicolumn{1}{c|}{H$\rightarrow$ J} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}I$\rightarrow$ J} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Number of\\ Edges\end{tabular}} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}16} & \multicolumn{1}{c|}{24} & \multicolumn{1}{c|}{16} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}6} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}4} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8} & \multicolumn{1}{c|}{12} & \multicolumn{1}{c|}{12} & \multicolumn{1}{c|}{9} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8} & \multicolumn{1}{c|}{12} & \multicolumn{2}{c|}{\cellcolor[HTML]{EFEFEF}4} & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}3} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Probability\\ of Missing\end{tabular}} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.038889} & \multicolumn{1}{c|}{0.125} & \multicolumn{1}{c|}{0.125} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.062} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.06875} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.039286} & \multicolumn{1}{c|}{0.069048} & \multicolumn{1}{c|}{0.2} & \multicolumn{1}{c|}{0.142857} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.3} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.003571} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.2} & \multicolumn{1}{c|}{0.142857} & \multicolumn{2}{c|}{\cellcolor[HTML]{EFEFEF}0.0} & \multicolumn{1}{c|}{0.072727} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.030303} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Wrong \\ Causation\end{tabular}} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}C$\rightarrow$ F} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{D$\rightarrow$ E} & \multicolumn{1}{c|}{D$\rightarrow$ F} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{F$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}G$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}G$\rightarrow$ I} & \multicolumn{1}{c|}{H$\rightarrow$ I} &  \\ \cline{1-1} \cline{5-5} \cline{9-10} \cline{14-17}
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Times\\ of Wrongly\\ Discovered\end{tabular}} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}5.6} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{1.2} & \multicolumn{1}{c|}{0.8} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{5.0} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8.2} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}3.0} & \multicolumn{1}{c|}{2.8} &  \\ \cline{1-1} \cline{5-5} \cline{9-10} \cline{14-17}
\multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{}
\end{tabular}
%}
}


\vspace{0.4in}
%\end{table*}

%\begin{table*}[t]
%\centering
\caption{ Brief Results of the Heuristic Causal Discovery in latent space, identical with Table 3 in the paper body, for better comparison to the traditional FGES methods results on this page.\\
The edges are arranged in detected order (from left to right) and their measured causal strengths in each step are shown below correspondingly.\\
Causal strength is measured by KLD values (less is stronger). Each round of detection is pursuing the least KLD gain globally. All evaluations are in 4-Fold validation average values. Different colors represent the ground truth causality strength tiers (referred to  the Figure 10 in the paper body).
}
\label{tab:discv}
%\resizebox{0.1\columnwidth}{!}{%
%\Rotatebox{90}{
\vspace{2mm}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{|c|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
\cellcolor[HTML]{C0C0C0}Causation & \cellcolor[HTML]{FFCCC9}A$\rightarrow$ C & \cellcolor[HTML]{FFCCC9}B$\rightarrow$ D & \cellcolor[HTML]{FFCCC9}C$\rightarrow$ D & \cellcolor[HTML]{FFCCC9}C$\rightarrow$ G & \cellcolor[HTML]{FFCCC9}D$\rightarrow$ G & \cellcolor[HTML]{FFCCC9}G$\rightarrow$ J & \cellcolor[HTML]{FFCCC9}D$\rightarrow$ H & \cellcolor[HTML]{FFCCC9}H$\rightarrow$ J & \cellcolor[HTML]{FFCE93}C$\rightarrow$ E & \cellcolor[HTML]{FFCE93}B$\rightarrow$ E & \cellcolor[HTML]{FFCE93}E$\rightarrow$ G & \cellcolor[HTML]{FFCE93}E$\rightarrow$ H & \cellcolor[HTML]{ABE9E7}E$\rightarrow$ F & \cellcolor[HTML]{ABE9E7}F$\rightarrow$ I & \cellcolor[HTML]{ABE9E7}I$\rightarrow$ J & \cellcolor[HTML]{ABE9E7}D$\rightarrow$ I \\ \hline
\cellcolor[HTML]{C0C0C0}KLD & 7.63 & 8.51 & 10.14 & 11.60 & 27.87 & 5.29 & 25.19 & 15.93 & 46.58 & 65.93 & 39.13 & 39.88 & 53.68 & 45.64 & 17.41 & 75.57 \\ \hline
\rowcolor[HTML]{FFFFFF} 
\cellcolor[HTML]{C0C0C0}Gain & 7.63 & 8.51 & 1.135 & 11.60 & 2.454 & 5.29 & 25.19 & 0.209 & 46.58 & -6.84 & -5.91 & -3.29 & 53.68 & 45.64 & 0.028 & 3.384 \\ \hline
\end{tabular}
%}}
%\end{table*}
}

\end{sidewaystable}








\end{document}
