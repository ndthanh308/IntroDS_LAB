
\documentclass[10pt]{article} % For LaTeX2e
%\usepackage{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage{graphicx}
\usepackage{adjustbox}
%\usepackage{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{centernot}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{color}
%\usepackage{booktabs}

\usepackage{subfigure}
\usepackage{caption}
\usepackage{dsfont}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{empheq}
%\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{totcount}

\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{makecell}
%\usepackage[table]{xcolor}
\usepackage{xcolor}
\usepackage{changepage}
\usepackage{stfloats}
%\usepackage{cite}
\usepackage{soul}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{algpseudocode}
\usepackage[title]{appendix}
\usepackage{pdfpages}

\usepackage{bm}
\usepackage{amsmath,amssymb,amsfonts}
% \usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{bookmark}
\usepackage{float}
\usepackage{tabularx}
\usepackage{mathtools}
\usepackage{array}
\usepackage{rotating}
\usepackage[skins]{tcolorbox}

\title{Relation-Oriented: Toward Knowledge-Aligned Causal AI}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{\name Jia Li \email jiaxx213@umn.edu \\
      \addr Department of Computer Science,
      University of Minnesota
      \AND
      \name Xiang Li \email lixx5000@umn.edu \\
      \addr Department of Bioproducts and Biosystems Engineering,       University of Minnesota
      \AND
      % \name Xiaowei Jia \email xiaowei@pitt.edu\\
      % \addr Department of Computer Science\\ University of Pittsburgh
      }

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{Apr}  % Insert correct month for camera-ready version
\def\year{2023} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version

\definecolor{thegrey}{RGB}{192,192,192}
\definecolor{orchid}{RGB}{204,153,255}

\begin{document}


\maketitle

\begin{abstract}
In machine learning, we naturally apply an \emph{Observation-Oriented} principle, in which observational variables preexist and set the stage for constructing relationships. While sufficient for traditional models, the integration of AI with big data exposes the \emph{misalignment} between the observational models and our actual comprehension.
Contrarily, humans shape cognitive entities defined by relationships, enabling us to formulate knowledge across temporal and hyper-dimensional spaces, rather than being confined to observational constructs.
From an innovative \emph{Relation-Oriented} perspective, this study examines the roots of this misalignment within our current modeling paradigm, illuminated by intuitive examples from computer vision and health informatics. We also introduce the \emph{relation-defined representation} learning methodology as a practical implementation of \emph{Relation-Oriented} modeling, supported by extensive experimental validation.
\end{abstract}

\section{Introduction}
\label{sec:intro}
% Causality plays a fundamental role in many fields, such as meteorology, biology, epidemiology, economics, and more. The main goal of causality is to uncover causal relationships from observational data generated by causal processes. Classical statistical methods for causal inference have been successful in constructing domain knowledge over the past few decades \cite{wood2015lesson,vukovic2022causal,ombadi2020evaluation}. However, the technical progress in recent years has dramatically increased the volume and quality of data collected, making building Bayesian networks from observational data challenging due to NP-hardness. On the other hand, machine learning (ML) techniques from computer science have demonstrated advanced effectiveness in handling big data for causal inference \cite{scheines1997introduction, ahmad2018interpretable, sanchez2022causal}. The success of ML-based causal inference is considered to be the bedrock of realizing artificial intelligence \cite{li2020causal}.



%% As widely recognized, AI is primarily designed to emulate human intelligence rather than genuinely comprehending information, unless it is deliberately trained to extract specific features. Achieving explicit interpretability to a certain extent is possible through such targeted training. For instance, the employment of Variational Autoencoders (VAEs) and the subsequent realization of Disentangled Representations based on them serve as a prime example. VAEs facilitate the learning of meaningful and interpretable latent features by separating distinct factors of variation in the data. This approach enables more straightforward manipulation of specific data aspects while keeping others constant, thereby enhancing interpretability. Practical applications of this concept include generating images with controlled attributes such as facial expressions, lighting conditions, or object orientations, as well as disentangling the underlying structure of complex data in various fields like natural language processing, medical imaging, and anomaly detection.

% The fundamental principle of modeling natural phenomena lies in establishing equations based on observables responsible for time evolution, which can be traced back to the Picard-Lindelof theorem introduced in the late 1800s. Upon this foundation, causal inference theories have been extensively developed in the field of statistics, alongside causality-based machine learning approaches in computer science. Intriguingly, this is not the typical manner in which humans process causal information, creating a significant barrier in our pursuit of developing knowledgeable causal AI systems.

% In this section, we will examine the distinct characteristics of human intelligence in comprehending causal relationships and investigate methods to train AI systems to develop a similar architecture. Human cognition employs a feature-based approach when discerning causal connections, with the notable aspect of integrating time as a seamlessly incorporated fourth dimension within our mental framework. This understanding could be pivotal in advancing AI capabilities to better mimic human-like causal reasoning.


Existing modeling methods predominantly premise that relationships are established based on observed entities, designated as model variables. This \emph{Observation-Oriented} principle can be traced back to the 1890s Picard-Lindelof theorem, which utilizes timestamps on a \emph{logical timeline} to depict the absolute temporal evolution of observational variables, intending to establish a unique solution within the observational space.
%natural phenomena gives precedence to observations over relations. Specifically, we position \emph{observables} (i.e., observed variables) as primary entities and form equations concerning their relationship, potentially coupled with timestamps to account for their temporal evolution.

Today, Artificial Intelligence (AI) has displayed capabilities surpassing humans in learning observations that meet the i.i.d. assumption, such as image generation in computer vision. Despite these, AI may appear ``unintelligent'' in comprehending certain relations that humans find intuitive. For instance, AI-created personas on social media can have realistic faces but barely with the presence of hands, due to AI struggling with the complex structure, instead treating hands as arbitrary assortments of finger-like items.

Moreover, when inquiries turn to time evolution, the task of causal reasoning makes a significant challenge for AI, although it is innate for humans. While traditional causal learning methods have made valuable contributions to numerous knowledge domains over the years \cite{wood2015lesson, vukovic2022causal, ombadi2020evaluation}, they are often challenged by a lack of generalizability \cite{scholkopf2021toward}.
Specifically, these methods tend to be context-specific and struggle to generalize across diverse scenarios. In contrast to AI's impressive achievements in areas such as Go gaming and natural language processing, its application to causality remains considerably constrained.

The questions ``how to apply deep learning in causality'' and ``how to simulate hands'' may seemly pertain to specific research domains such as causal inference and computer vision. However, they fundamentally converge towards the central issue of \textbf{\emph{AI Alignment}}. Recalling Dr. Geoffrey Hinton's caution, misalignment of AI capabilities with human values can lead to unintended detrimental consequences. Accordingly, the crux of the matter is, indeed ``why causal knowledge and some time-irrelative relationships are unseen to AI? And how should we instruct it?'' - an aspect that is increasingly critical to address.

Our knowledge initially starts from observations, yet our comprehension can surpass the observable reality, and construct knowledge to be \textbf{\emph{hierarchy}} within a hyper-dimension, which is \emph{unobservable} to AI. Such hierarchy denotes granularity levels of phenomena in our comprehension. %, where understanding superior levels relies on the inferior ones. 
For instance, understanding individual traits (a higher individual level) relies on getting their cultural context (a lower population level).
As highlighted by \cite{scholkopf2021toward}, for models to be generalizable, the low-level relations they learn must be reusable in subsequent high-level learnings.

In purely observational learning, the complications due to unobservable hierarchies can be directly illustrated in scenarios like the ``unrealistic hands'' problem (see Section \ref{sec:obs_hier}). 
However, when we engage with causality, entailing a hierarchy of \emph{dynamical} features within the \emph{temporal} dimension, the ensuing complexities are greatly magnified.
Intriguingly, the dynamics along a timeline can be viewed as \emph{nonlinear} relationships (see Section \ref{sec:dynamics}); yet, in spite of neural networks' inherent proficiency in handling nonlinearity, the latent potential of our current \emph{Observation-Oriented} AI systems on the temporal dimension seems largely underutilized. 
% AI, however, falls short in constructing logical timelines in comprehension, a necessary skill for adequately representing the relationships over distinct dynamics indigenous to each unique timeline.

Contrary to AI, human understanding is fundamentally \emph{relation-centric}. Relationships serve as indices that point to our mental representations \cite{sep-mental-representation}, shaping our cognitive understanding of observations and temporal events. Through relationships, we can form interconnected knowledge systems in memory. It's not surprising that the cutting-edge methodology to teach AI causal reasoning focuses on ``causal representations'' \cite{scholkopf2021toward}, which is, letting the modeled objects represent the causal relations, instead of the observations. This concept indeed provides valuable insight into the pursuit of AI alignment. 

Following a similar train of thought, we propose the \textbf{\emph{Relation-Oriented}} principle, and a corresponding implementation method of \emph{relation-defined representation} learning, which we consider an indispensable tool in our pursuit of knowledge-aligned AI. %aim to build a firm theoretical groundwork
Sections \ref{sec:Hierarchy} and \ref{sec:Temporal} thoroughly examine our encountered challenges posed by: \textbf{1)} the unobservable knowledge hierarchy, and \textbf{2)} relationship across \emph{multi-timelines} in such hierarchies, both neglected in our \emph{Observation-Oriented} convention. 
In Section \ref{sec:Causality}, we reassess existing causality methods, which inherently in a \emph{Relation-Oriented} view but come with substantial limitations.
Section \ref{sec:factorization} formally factorizes the process of relation-defined hierarchical representation, followed by Section \ref{sec:method}, which outlines the proposed implementation methodology, subsequently validated by experiments detailed in Section \ref{sec:experiment}.




% Figure environment removed
\vspace{-1mm}

Figure~\ref{fig:space} clarifies the terminologies concerning spaces and dimensions, with the hyper-dimension consisting of all the unobservable hierarchies in our knowledge. Within \emph{Observable} space, relationships that connect distributions broadly fall into two categories: linear and nonlinear. The term ``features'' denotes potential variables that can represent the distributions of interest. 

Typically, AI excels in addressing nonlinearity, while conventional models are adept at linear interpretation. However, in the temporal dimension, related nonlinear dynamics may span across \emph{multiple logical timelines}. Frequently overlooked, this aspect can lead to \emph{inherent temporal biases} in our models - Which narrow our successful AI applications within \emph{observational} space only, and also obstruct causal learning from achieving generalizable success in \emph{observational-temporal} space.

Contrarily, relation-defined representation aligns with knowledge definition, through which, AI can automatically identify the intended dynamics within the corresponding knowledge level.
Our methodology's feasibility rests on autoencoders, enabling a transition from the \emph{observational-temporal} space to the latent feature space, which allows us to represent \emph{dynamical} features in the same manner as \emph{observational} ones.

% In essence, AI requires our guidance to materialize the unobservable hierarchy of knowledge, ensuring it aligns with our instinctive comprehension. Currently, AI's major achievements are within applications where neglecting this hierarchy does not lead to significant errors, such as face simulation without understanding anatomy, compared to the challenge of simulating hands.
% Furthermore, within a level of \emph{causal} knowledge, relationships are likely to involve temporal dimensions. AI's prevalent large-scale-observational modeling strategy, however, complicates the adoption of causal inference theories and leads to uninterpretable outputs.






\section{The Unobservable Hierarchy in Knowledge}
\label{sec:Hierarchy}





% Typically, machine learning emphasizes observables (i.e., variables) as the principal entities and strives to build robust models around them. This observable-oriented strategy naturally extends to contemporary AI systems. In the area of computer vision, the training images function as diverse observations, with their constituent pixels playing the role of variables. AI systems can learn the associative relationships over these pixels to establish controllable features, like facial expressions, lighting conditions, or object orientations.

% However, certain forms of knowledge humans find intuitive may not be fully captured by observations alone, making AI appear ``unintelligent'' in comprehension.
% An intriguing example lies in a recent trend on social media: AI-created personas with appealing looks draw in millions of followers, while a common trick to discern whether an image of a person is AI-generated or real is to look for the presence of hands.
% Compared to generating diverse faces, it remains challenging for AI models to simulate human hands due to their more flexible exhibitions.
% AI often treats hands as arbitrary assortments of finger-like elements, similar to their processing of less restricted objects such as flowers.
% So, what makes this type of knowledge so elusive for AI systems? And how does human intelligence approach it differently compared to AI?

Knowledge hierarchies are inherent across many fields.
Take flood prediction: general physical rules formulate the base-level regulations, applicable universally, while unique hydrological conditions create distinct watershed features. This paradigm applies to diverse phenomena like epidemic progression, economic fluctuations, or strategic decision-making, wherein knowledge extends from broad frameworks to specific idiosyncrasies.

In this section, we explore the knowledge hierarchies over observational and temporal features, via two illustrative examples. The first, grounded in computer vision, underlines how the direct challenges encountered in observational space learning are linked to the unobservable knowledge hierarchy. While the second, from health informatics, demonstrates how traditional causal learning typically only captures static temporal features, overlooking the intrinsic dynamical aspects of causality and their inherent hierarchies.



% Figure environment removed

% Figure environment removed


\subsection{Hierarchy of Observational Features}
\label{sec:obs_hier}
\vspace{-1mm}


Figure \ref{fig:hand}(a) showcases AI-created realistic faces with unusual hands, while humans can easily distinguish a plausible hand and infer the owner's intention, even in colorless pencil sketches, as in (b). We can quickly decompose observations into hierarchical features, progressing from lower to higher levels: $\mathbf{I}$, knuckles, nails, and relative lengths identify fingers; $\mathbf{II}$, their positions indicate a hand gesture; $\mathbf{III}$, the gesture's meaning is retrieved from memory.
Unlike humans, AI systems process all observational features simultaneously, where the intuitive hierarchy in our understanding is unobservable from their perspective.
Similarly, to an extraterrestrial, the unrealistic hands in Figure \ref{fig:hand}(a) may seem as normal as in actual hand photos. 

However, without seeing the hierarchy, the hierarchical features may be revealed as distinguishable, if \emph{no significant overlaps}. For instance, the placement of human eyes informs facial angle, allowing AI to generate plausible faces without recognizing ``eyes'' from ``faces''. On the contrary, some different hand gestures might look similar. Without identification of ``fingers'' (Level $\mathbf{I}$ knowledge), AI may not recognize their varying positions (Level $\mathbf{II}$ knowledge) but misinterpret ``hands'' as arbitrary finger-like items associations. 


%Therefore, sequential instruction to build the hierarchy is crucial for AI. Without the Level $\mathbf{I}$ knowledge of recognizing fingers, it is even challenging to instill in AI that a hand has five fingers.


This hierarchical organization of our comprehension enables us to effectively reuse knowledge across a range of scenarios.
For instance, the finger identification (Level $\mathbf{I}$ knowledge) is transferable across photos, watercolor paintings, and any sketching styles.
As knowledge is accrued over time, the intra-level connections are strengthened, making our understanding of hierarchies more distinct and concrete. The relationship at each level can naturally form graphical models, where the connected features serve as nodes and readily interact with others in our cognition.

The leading technology for disentangling features, Variational AutoEncoders (VAEs) \cite{burgess2018understanding}, are not designed to capture intra-level relations, but primarily to disentangle same-level features under i.i.d setting.
% without accounting for these interconnections. In other words, VAEs are not designed to capture relations, but rather to disassociate observational distributions.
This nature inherently limits VAEs to handling knowledge hierarchy.
For instance, while VAEs can effectively disentangle the finger-level features, as shown on the left side of Figure~\ref{fig:hand_relate}, if the disentanglement is trained on data like watercolor paintings, they may struggle to recognize pencil sketches on the right side.

In traditional \emph{Observation-Oriented} graphical modeling, a prerequisite often involves fully observed variables to establish relation functions, such as the indicator function $f$ presented in Figure~\ref{fig:hand_relate}. This requires extensive manual labeling, which may not always be practical. Conversely, our proposed \emph{Relation-Oriented} modeling emphasizes knowledge-based relations to construct observational representations inversely. Here, $f$ is not a predetermined indicator, but an index of specifying features from the complex observations. Considering the data flow from left to right in Figure~\ref{fig:hand_relate}, a relation-defined $f$ serves as a filter, to efficiently exclude color-related aspects from the VAE-captured representations, keeping only relevant data for the right side.










% Variational AutoEncoders (VAEs) currently lead the field in \emph{feature disentanglement} techniques, but they typically focus on a lateral disentanglement, without incorporating the potential stratifications based on knowledge levels. Here, \emph{a level of knowledge} refers to a previously identified relationship among certain features, designating them as features of the same hierarchical level.
% For example, we can stratify the observational features of the images in Figure \ref{fig:hand} (b) into finger-level features (like knuckles and nails), and hand-level features (like palmprints and gestures) according to the ``Level I'' and ``Level II'' knowledge. 
% VAEs excel at extracting unique and common characteristics among the five fingers from these images. However, they also encompass hand-level features unless these are manually removed in a preprocessing step. Therefore, while VAEs can disentangle features on the same hierarchical level, they lack the capability to autonomously isolate them from features of other levels.



% The question, though, is, ``Why is it necessary to achieve such a feature stratification?''
% The answer lies in realizing the flexible knowledge generalization within the AI systems, to make them ``\textbf{\emph{knowledgable}}'', much like how human intelligence operates.

% The stratified nature of our cognition often allows us to accumulate knowledge across diverse layers, usually without conscious thought. This attribute empowers us to reuse learned knowledge efficiently in various scenarios, much like how well-encapsulated feature nodes can be linked to and invoked from any other nodes in a graphical model.
% To illustrate, when tasked with understanding hand images, your recognition of the pencil drawing's artistic style does not interfere with your decision-making process. 
% Conversely, the disentangled features VAEs learn are narrowly applicable and restricted to the same set of images, not readily transferable to other contexts, such as photos, watercolor paintings, or even sketches in a different style.

% Isolating features from observable data to form \textbf{\emph{reusable elements}} is essential for realizing the knowledgeable AI system. This prompts a fundamental question: ``How can we automate the feature stratification process without the need for manual tagging?''
% Figure~\ref{fig:hand_relate} breaks down the ``Level I'' knowledge depicted in Figure \ref{fig:hand} (b). It consists of three elements: the features we have learned and stored in memory, the features we observe from images, and the matching relationship to connect them, represented as the indicator function $f$.
% The learned finger-level features and their matching relationship are from our knowledge, allowing us to identify the observed features exclusively at the same level. 
% In essence, function $f$ acts as a \emph{Key} to identify the exact level features from observables.



% \noindent\tcbox[enhanced, drop fuzzy shadow southwest, sharp corners, colframe=yellow!80!black, colback=yellow!10]{\begin{minipage}{0.93\textwidth}
% \paragraph{Lemma 1.} A \textbf{\emph{Relation-Oriented}} approach is essential to building a knowledgeable graphical system, as it enables the automatic stratification of features into reusable nodes within this model.
% \end{minipage}}

% Traditional modeling methods often take an \textbf{\emph{Observable-Oriented}} approach, thereby confining the learned knowledge to specific training contexts, and reducing its generalizability.
% %limiting the applicability of learned knowledge to specific training contexts, and constraining its generalization to different scenarios. 
% Using a \textbf{\emph{Relation-Oriented}} strategy might seem excessive for single-relation modeling, but its significance becomes clear from a broader, systematic viewpoint.
% Relation-identified features serve as modular elements, boosting system adaptability across various contexts.
% To visualize this, consider these identified features as nodes of knowledge within our cognitive schema, allowing the system to grow in a way that emulates the process of our knowledge acquisition.
% \emph{Relation-Oriented} modeling serves as the cornerstone in aligning AI with human cognition.
% % We underscore the necessity of adopting a \textbf{\emph{Relation-Oriented}} approach in creating an AI system that is knowledge-generalizable and dynamically scalable. This approach is distinct from the traditional \textbf{\emph{Observable-Oriented}} modeling principles.

 


\subsection{Hierarchy of Dynamical Temporal Features}
\label{sec:temp_hier}
\vspace{-1mm}

A \emph{reinforcement learning}-based AI system \cite{sutton2018reinforcement} can potentially manage knowledge hierarchies in observational space with continuous human feedback. For example, it could learn to identify fingers autonomously, driven by human approval of images featuring typical five-fingered hands. However, spontaneous adjustments on the temporal dimension remain challenging. Before probing its intricacies, let's examine hierarchical dynamics along a single timeline through the lens of traditional causal learning.


% However, when such hierarchies occur on dynamical features in the time dimension, the crux of tackling lies in the interpretable relationships. 
% Let's first investigate this challenge from the perspective of traditional causal learning.

% The prevalent AI systems primarily sidestep the genuine exploration of the temporal dimension. Conversely, traditional causal inference has made decades of effort to gain access to the distributions on the timeline, employing statistical methods based on observational variables. Nonetheless, these methods encounter significant difficulties when confronted with nonlinearity, thereby creating substantial challenges in capturing dynamical features on the temporal dimension - especially when involving unobservable hierarchies.


% Causality, often linked with time-ordered sequences, refers to the phenomenon that a (previous) change in one variable leads to a (subsequent) change in another \cite{pearl2018book, spirtes2000causation}. 
% The logical concept of ``causing'' is typically highlighted to distinguish it from simple correlation in human understanding, but holds no distinct significance for the actual modeling process - Causal modeling often manifests as a correlation with its direction specified according to time.
% Consequently, the chronological ordering and potential time lag between cause and effect events - the two changes - are commonly regarded as signals that emphasize this relationship as causal, apart from mere correlation.
% Interestingly, if we can adequately represent causality as directional correlations traversing time, then the necessity to distinguish between the two concepts may become less critical than it currently is.

% Undoubtedly, the temporal attributes inherent to causality significantly increase its learning complexity when compared with correlation.
% Yet, what essential differences do they introduce to the modeling process? 
% In this subsection, we introduce an innovative perspective by considering \textbf{\emph{time as a standard dimension}} and discuss how the inherent temporal features can shape and influence the process of causal learning, viewed through the lens of feature stratification on this dimension.

% % the trend (long-term direction), the seasonal (systematic, calendar-related movements), and the irregular (unsystematic, short-term fluctuations).  Trend, Seasonality, Cyclical, and Irregularity


% such a sequence from observational data is just a single instance of, rather than $X$'s temporal feature itself, which needs to be learned from a bunch of instances.
% But the timestamp anchor has determined the modeling cannot capture variations 
%The observed sequence may be jointly determined by multiple or even multi-level temporal features.




%The standard population pharmacodynamic model expects to reach its clinical effect in 30 days by general. 
% Figure environment removed

Figure~\ref{fig:eff}(a) depicts the daily effects on patients following the initiation of medication $M_A$ (i.e., $do(A)$), with $t$ indicating the elapsed days. This classic causal relationship sees $do(A)$ as the cause, and the time sequence on the $t$-axis reflects the ensuing effect. Assuming each patient's (unobserved) personal characteristics linearly influence the medication's release - uniformly accelerating or decelerating its progress - the observed time sequence for an individual (like the red and blue curves in (a)) is shaped by two levels of \emph{dynamical} features: \textbf{1)} the population-level effect sequence with a standard length, and \textbf{2)} the individual-level progress speed. $M_A$'s clinical effectiveness should be evaluated primarily based on level \textbf{1)}, regardless of level \textbf{2)}.

% The patients have various speeds for the same medical effect due to their different body features, which are unobserved.
% Without loss of generality, in this case, we suppose the personalized body feature linearly influences the medical efficiency, i.e., uniformly speeds up or slows down the entire progression for each patient with a unique personal rate. Then, 

Figure~\ref{fig:eff}(b) completely represents the dynamical features in a 31-length vector, segmented as two hierarchical levels. Traditional medical effect estimation, often obtained by averaging patients' after-30-days performances, essentially builds a correlation model $B_{t+30}=f(do(A_t))$. It only captures the final step of the standard effect sequence, disregarding the preceding 29 steps, thus only representing a \emph{static} fragment of the population-level dynamic. Moreover, it implicitly assumes a normal distribution of patients' personalized time spans around the mean value of 30 days with a linearly decreasing medical effect.
%general effect at D30 aligns with the mean value of the D30 correlation.
%the patients' personalized time spans adhere to a normal distribution, with the mean value precisely defined by 30 days.



%Although it is known that this process typically spans around 30 days for the overall patient population, the specific timeframe does not alter the individual patient's assessment of the medication's effectiveness. A 10-day deviation, earlier or later, should not impact its clinical efficacy evaluation.

% The general effect can be seen as a standard sequence of length 30, augmented by an additional feature dimension that quantifies the patient's unique rate of progress - a numeric attribute of this variable. 
% This rate could correlate with age or other factors, eliminating the need to consider temporal patterns further, as this length 31 variable does not depend on the timestamp $t$ to anchor the sequence. Put simply, by viewing time - the fourth dimension separate from the three spatial ones - as a regular feature dimension, we can greatly simplify our understanding.
% Although this simplification is intuitively noticeable to researchers, the challenge lies in the fact that temporal features are often difficult to extract from observational data.

% In essence, within the context of Figure~\ref{fig:eff}, the extraction of temporal features is fundamentally the same as executing \emph{\textbf{feature stratification}} on the observable \emph{\textbf{B}}. Here, the rate of progress serves as the individual-level feature, while the standard sequence of change is characterized as the population-level general feature.


% The correlation function $f$ evolves over time and thus may reflect temporal features of effects. However, as a static matching mechanism, it can only represent \textbf{\emph{a single level}} of the temporal feature at best. In this scenario, $f$ is derived from observed $A_t$ to $B_{t+30}$, thus capturing only the final step (i.e., denoted as D30) of the standard time sequence, with the preceding 29 steps missing, as highlighted in Figure~\ref{fig:eff} (b).
% This process only captures a fragment of the population-level features, while completely disregarding the individual-level ones.





% Figure environment removed
\vspace{-4mm}



\subsection{The Elusive Hidden Confounder}
\label{sec:confounder}
\vspace{-2mm}

For individuals like $P_i$ or $P_j$, the average effect estimated by model $f$ introduces a bias: $P_i$ exceeds 100\% of the expected effect after 30 days, while $P_j$ only achieves about 75\%. To account for these individual-level biases, traditional causal DAG (Directed Acyclic Graph) introduces a \emph{hidden confounder} (denoted as $E$), to represent patients' unobserved personal characteristics, as shown in Figure~\ref{fig:hidden} (a).
However, it raises a question - why concern ourselves with a hidden variable outside our model's scope? This inclusion suggests an illogical assertion: ``Our model is biased due to ignorance of some aspects we have no intent to investigate.''


The reason for introducing $E$ is that while this common cause remains unknown, its effect - the individual-level dynamical speeds - is observable. Since such a level is overlooked by the traditional model $f$, it can only be attributed to the potentially existent $E$. Although hidden, $E$ still represents an observational variable and thus could be incorporated by $f$ once revealed. In essence, the introduction of this elusive hidden confounder is to transform \emph{observed temporal} variables into \emph{unobserved observational} ones. This serves to enhance human understanding, though does not necessarily benefit the model.

Figure~\ref{fig:hidden}(b) presents the hierarchical disentanglement of the dynamical features indexed by relations. Traditional causal inference views the individual-level effect as caused by the unobserved composite cause $do(A) * E$, which is an interpretable causal relationship, but not directly modelable.
Conversely, a \emph{Relation-Oriented} approach treats the relation merely as an index without requiring modeling. Thus, we can use any observed identifier, such as patient ID, to pinpoint the individual-level dynamics.
Figure~\ref{fig:hidden}(c) illustrates the implementation architecture of this \emph{Relation-Oriented} approach, showing two separate reconstruction processes of the time sequences data, with and without individual-level dynamical features, respectively.









% Traditional causal models regard effects as outcomes in the observational data space $\mathbb{R}^d$, neglecting their temporal attributes on the $(d+1)$-th dimension: time. 
% While \textbf{\emph{cause events}} can have their temporal features captured by sequential models such as RNNs, it remains a crucial fact that \textbf{\emph{effects}} are not inherently categorized as \textbf{\emph{events}} on their own.





% is unobserved, its causal effect has been delivered as the individual-level temporal feature - the personalized speed - which is observed.
% This feature is present in data but overlooked by the model, thereby introducing such unavoidable biases.
% % While some sequential models like RNNs can capture the temporal features of the \textbf{\emph{cause event}}, the fact remains that  \textbf{\emph{effects are not considered events}} in themselves.% within conventional causal learning.


% The existence of $E$ is deduced from a previously identified causal relation, thereby positioning this interpretation as \emph{Relation-Oriented}.
% However, it primarily serves to enhance human comprehension and does not directly benefit the modeling with an \emph{Observable-Oriented} nature.
% Autoencoders provide a solution to this limitation by representing observables as features in latent space, as demonstrated in Figure~\ref{fig:hidden} (c).
% Through the incorporation of extra temporal features, the objective of a causal model can transition from effect outcomes (i.e., the observable $B\in \mathbb{R}^d$) to effect events (i.e., observable time sequences in $\mathbb{R}^{d+1}$ space).
% Additionally, unlike the observational values in data space, features in latent space can be directly disentangled or stratified using the input relations as indexes.
% As demonstrated in Figure~\ref{fig:hidden} (b), this feature stratification process sees $A$ act as the observed population-level cause, with the associated $(A\cdot E)$ symbolizing the unobserved individual-level cause. 
% Thus, by constructing a \emph{Relation-Oriented} model, we can identify the general medical effect across the entire population, without integrating the patients' individualized features.

% \noindent\tcbox[enhanced, drop fuzzy shadow southwest, sharp corners, colframe=yellow!80!black, colback=yellow!10]{\begin{minipage}{0.93\textwidth}
% \paragraph{Lemma 2.} A \emph{Relation-Oriented} approach can stratify or disentangle the temporal features of a causal effect by leveraging autoencoders.
% \end{minipage}}

% In this work, we propose a novel conceptualization of the causal DAG, designed to incorporate potential temporal features inherent in causal effects, as depicted in Figure~\ref{fig:do1} (a).
% This revised DAG is set within a space defined by one or more timeline axes, where the nodes continue to represent observational effect outcomes or their corresponding causal events; However, the directed edges are assigned meaningful lengths that represent the potential time spans of the \emph{effect events}, as required. 
% For example, in Figure~\ref{fig:do1} (a), different edge lengths are used to distinguish the individual-level differences in the efficiency of the same medical effect among patients.
% This reinterpretation serves not merely as a means to visualize the stratification of temporal features, but also provides a foundation for explaining why current AI applications within the sphere of causal knowledge might be leading to misconceptions.







\section{Causality on the Timeline}
\label{sec:Causality}
\vspace{-2mm}

%The inherent temporal aspects of causality certainly boost its learning complexity beyond mere correlations. 
Causality research can be seen as our probe into the temporal dimension, extending beyond the observational reality. Curiously, one might find it rare to see ``incorporation of time'' defined as the distinctive factor between causality and mere correlation in causal inference theories. While, as in our modeling context, instead of a logical one, what significance does this distinction hold?

To model time series, we frequently involve a timestamp dimension, to logically reflect the absolute time evolution in reality. 
For the model, however, this approach renders the line between causality and correlation indistinct,  as it operates within our defined modeling space, taking timestamps as just another attribute, irrespective of its temporal significance.
This is not to diminish the importance of the temporal dimension, but rather to emphasize that our current causal modeling might not completely align with our intuitive understanding of this dimension, revealing a certain discrepancy.
This section is devoted to discussing our present causal modeling from this particular perspective.

%reliance on the single absolute timeline has obstructed our accurate understanding of the temporal dimension's role within the modeling context.

%Despite the limitations of classical methods in accessing dynamics, inference theories can provide a more nuanced understanding.

% For conventional causal inference, the significant causal dynamics often remain out of reach for the adopted statistical models, necessitating our additional interpretative labor to supplement these overlooked aspects. 
% But, since AI is unburdened by this limitation, is there a compelling requirement to engage with causal interpretations?
% Indeed, modeling distributions on the absolute timeline does not capture the essence of the temporal dimension, while the classical inference theories can offer a more nuanced understanding.


% Yet, what essential differences do they introduce to the modeling process? 

%Preceding the advent of neural networks, causal inference served as a foundational pillar for the progression of numerous scientific fields, notwithstanding the introduction of complex theories that might seem esoteric (refer to Sections \ref{sec:confounder} and \ref{sec:temp_hier}).



\subsection{Causality vs. Correlation}
\vspace{-1mm}
Consider a general model function $Y=f(X;\theta)$, with $\theta$ denoting the model parameter. 
The process of learning $\theta$ is irrespective of how we interpret the $X\rightarrow Y$ relationship, instead, relying solely on the observations of variable $X$ and outcome $Y$. 
A causal relationship comprises two layers: \textbf{1)} the basic connection between $X$ and $Y$ that is modeling-significant, and \textbf{2)} the roles of cause and effect that are not modeling-significant.

To be specific, for causality $X\rightarrow Y$, we can employ the function $Y=f(X;\theta)$ to predict the effect $Y$, and likewise, $X=g(Y,\psi)$ to infer the cause $X$ when given $Y$. Both parameters $\theta$ and $\psi$ are equivalently derived from joint probability $\mathbf{P}(X, Y)$, making directionality more a matter of interpretation than a modeling constraint. So, why is ``modeling direction'' a concern? And why do we even need to specify ``causal models''?

The primary reason lies in our current causal models' limitation when addressing causally significant dynamics, which are often unbalanced between cause and effect. 
For example, in Figure~\ref{fig:eff}, the effect dynamics cannot be fully captured by the correlation model in Figure~\ref{fig:hidden}, necessitating a hidden confounder to inform our prior understanding. However, if such dynamics act as the cause, they could be effectively handled with RNNs.
For more discussions about addressing causally significant dynamics, please refer to Section \ref{sec:dynamics}.

% In other words, if the temporal features, including the hierarchically structured dynamical ones, can be fully represented by variables, a causal relationship can be viewed as a basic connection, like a mere correlation, with extra features added to record its causally significant direction. 


%\hspace{-1mm}
\vspace{2mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=red!50!black, colback=yellow!10]{\begin{minipage}{0.95\textwidth}
\paragraph{Lemma 1.} Causality vs. Correlation in the modeling context.

$\ \bullet$ Causality is connection between features, which can be \emph{observational}, \emph{temporal}, and also \textbf{\emph{dynamical}}.

$\ \bullet$ Correlation is connection between features, which are \textbf{\emph{not dynamical}}.

\end{minipage}}
\vspace{1mm}


% In other words, during modeling,
% causality is distinguishable from correlation, \emph{if and only if} when dynamical features on temporal dimensions are necessary for the relationship (especially for the effects).
In short, the distinction between causality and correlation lies in the connected critical features in this relationship, instead of the connection itself. 
The ``causal model'' concept first emerged within statistical causal inference. Considering the challenges posed by nonlinearity at that time, distinguishing causal relationships and reasonable directions becomes especially crucial for subsequent manual interpretations.


%To use an analogy, a double-ended cotton swab is differentiated by the characteristics of its cotton ends - such as shape, texture, or color for different uses - rather than the connecting stick.

Importantly, a relationship with causal significance in a \emph{logical} context does not automatically translate to ``causality'' within a \emph{modeling} context. For this reason, the ``incorporation of time'' alone is not a sufficient condition to validate causal models. A widespread misconception often associates the temporal lag between cause and effect as an indicator of causality. The following example will help to clarify this misconception.

Morgan Spurlock conducts a self-experiment in the documentary film ``Super Size Me'' (2004). Over a month, Spurlock restricted his diet solely to McDonald's meals, during which he recorded significant deteriorations in his health and physical appearance, including rapid weight gain, mood instability, liver damage, etc.,  which dramatically demonstrated the potential negative health impacts of a fast-food diet.
Consider two  time sequences: $\mathbf{x}=\{x_1, \ldots, x_{30}\}$ and $\mathbf{y}=\{y_1, \ldots, y_{30}\}$. Here, each $x_t$ signifies the act of eating McDonald's on the $t$-th day, while each $y_t$ corresponds to the fluctuations in his health indices for that particular day.
It is intuitive to recognize that $\mathbf{x}\rightarrow \mathbf{y}$ is a causal relationship, although no time lag exists between $\mathbf{x}$ and $\mathbf{y}$.
Yet, a time lag is also conceivable in this case. Suppose another person tried this experiment, but since he had a longer responding digestive system, the same effect denoted by $\textbf{y}$ emerged with a 5-day delay, creating a 5-day lag between $x_1$ and $y_1$.
This instance emphasizes that even when both the cause and effect include dynamics, separation by a time-lapse is not a reliable criterion.

\vspace{2mm}
\subsection{The Current Causal Modeling Context}
%\vspace{-2mm}
Figure~\ref{fig:view} broadly classifies causal queries into four categories, based on whether they incorporate dynamical features, and whether they are already investigated within existing knowledge. We approach the topic from two aspects: the modeling-significant \emph{basic connection}, and the interpretation-significant \emph{causal direction}.


%\vspace{4mm}
\subsubsubsection{\emph{(1) Aspect of the Basic Connection}}
\vspace{-1mm}

The traditional causal inference has made notable advancements in exploring the specific conditions under which dynamical features can be ``downgraded'' to a level accessible by the employed statistical models. Notably, \emph{do-calculus} \cite{pearl2012calculus} probes into identifiable events (i.e., treating dynamics as individual entities) to establish variables in the temporal dimension. Essentially, this involves manually converting dynamics into static temporal elements, but it tends to be greater adaptable in handling causes compared to effects (For a more comprehensive discussion, please refer to section \ref{sec:dynamics}).
%The current causal learning, however, not only models the relation between variables but is also burdened with the relation's causal significance - a task inherently belongs to humans rather than models - as it is potentially required to compensate for the model limitations.
While for the effect dynamics overlooked by models, if existing knowledge can suggest potential causes, the creation of a \emph{hidden confounder} can enhance comprehension; if not, these dynamics may be dismissed based on the \emph{causal sufficiency assumption}, which could lead to subsequent challenges.

On the other hand, causal discovery methods mainly scan the observational space, incorporating static elements lying on the single absolute timeline (i.e., the timestamps attribute). As a result, if the underlying causal mechanism does not encompass crucial dynamics, causal discovery can be effective. However, if such dynamics exist, they largely go undetected. This potential gap may be negated under the \emph{causal faithfulness assumption} that the observed variables fully represent the causal reality.



% Figure environment removed
\vspace{-1mm}


\vspace{-1mm}
\subsubsubsection{\emph{(2) Aspect of the Causal Direction}}
\vspace{-1mm}
% Knowledge-guided causal learning readily complies with established cause-and-effect roles to set the modeling direction. However, the observationally-discovered directions might not necessarily hold causal meaning.

Consider observables $X$ and $Y$ in a graphical system, with specified models $Y=f(X;\theta)$ and $X=g(Y;\psi)$. Based on observations, the discovered causal direction between $X$ and $Y$ is determined by the likelihoods of estimated parameters $\hat{\theta}$ and $\hat{\psi}$. Given the joint distribution $\mathbf{P}(X,Y)$, one would prefer $X\rightarrow Y$ if $\mathcal{L}(\hat{\theta}) > \mathcal{L}(\hat{\psi})$. %, then $X\rightarrow Y$ would be preferred.
Now, let $\mathcal{I}(\theta)$ be a simplified form of $\mathcal{I}_{X,Y}(\theta)$, the Fisher information, representing the amount of information contained by $\mathbf{P}(X,Y)$ about unknown $\theta$. Assume $p(\cdot)$ to be the probability density function; then, in this context, $\int_X p(x;\theta) dx$ remains constant. So, we have

\vspace{-6mm}
\begin{align*}
    \mathcal{I}(\theta) &= \mathbb{E}[(\frac{\partial }{\partial \theta} \log p(X,Y;\theta))^2 \mid \theta] 
    = \int_{Y} \int_X (\frac{\partial }{\partial \theta} \log p(x,y;\theta))^2 p(x,y;\theta) dx dy \\
    &= \alpha \int_Y (\frac{\partial }{\partial \theta} \log p(y;x,\theta))^2 p(y;x,\theta) dy + \beta = \alpha \mathcal{I}_{Y\mid X}(\theta)+\beta, \text{with } \alpha,\beta \text{ constants.} \\
    \text{Thus, } \hat{\theta} &=\argmax\limits_{\theta} \mathbf{P}(Y\mid X,\theta) = \argmin\limits_{\theta} \mathcal{I}_{Y\mid X} (\theta) =\argmin\limits_{\theta} \mathcal{I}(\theta), \text{ and } \mathcal{L}(\hat{\theta})  \propto 1/\mathcal{I}(\hat{\theta}).
\end{align*}
\vspace{-4mm}

Subsequently, the likelihoods of the estimated parameters $\hat{\theta}$ and $\hat{\psi}$ depend on the amount of information, $\mathcal{I}(\hat{\theta})$ and $\mathcal{I}(\hat{\psi})$.
Thus, the directionality learned among variables essentially indicates how much their specified distributions are reflected in the data, with the more dominant one deemed the ``cause''. This interpretation presumes that the cause should be more comprehensively captured in the observations than the effect. While this was a reasonable default setting in previous decades, due to limitations in data collection techniques, it is not necessarily the case in the present era.

In summary, a causal direction purely inferred from observations could be causally meaningful in logic, if satisfying: \textbf{1)} the causal relation of interest does not involve significant dynamics, \textbf{2)} the observations are known, a prior, to be more informative about the cause than the effect, and \textbf{3)} the specified distributions are fundamentally accurate.
Technically speaking, in the traditional modeling context, the term ``causal model'' is not a specific model type. Instead, it designates models that require additional logical interpretations about overlooked dynamics, to bridge comprehension gaps and pave the way for potential improvements.


% %\subsection{A Short Summary}
% To summarize Section \ref{sec:Causality}, the modeling methodology is not inherently related to whether or how we interpret the question as causal. Technically, at the present stage, we primarily employ the ``directed-correlation learning'' and ``informativeness discovery'' to address causal queries.




% \noindent\tcbox[enhanced, drop fuzzy shadow southwest, sharp corners, colframe=yellow!80!black, colback=yellow!10]{
% \begin{minipage}{0.93\textwidth}
% \paragraph{Remark in 3.4} In essence, effective causal learning requires: \textbf{1)} a complete representation of both observational and dynamical features for 
% cause and effect, and \textbf{2)} the capability of incorporating further-level information, to save the intended interpretation (like causal directionality).
% \end{minipage}}
% %\vspace{1mm}

% Specifically, both requirements can be met through \emph{Relation-Oriented Featurization} (refer to Section \ref{sec:factorization}), which is to factorize cause and effect as disentangled features, according to the hierarchically structured prior knowledge.
% This approach uses relations as indices to stratify levels of features across various dimensions; and the disentangled cause or effect representations can simplify their potential feature expansion (to incorporate information about their causal roles), compared to traditional observable-based models. 


\section{The Overlooked Temporal Dimensionality}
\label{sec:Temporal}

Data is commonly stored in matrices, with time series data incorporating an extra attribute for the timestamps, which forms a logical timeline to reflect the absolute time evolution in reality. Traditionally, modeling has relied on this timeline to determine the chronological order of all potential events. However, our intuitive understanding of time is far more complex than this singular, simplified absolute timeline.

% Observable-based modeling inherently aligns with our intuitive understanding.
% The tradition of modeling natural phenomena as observable mechanisms responsible for time evolution has consistently been the gold standard.
% Specifically, the absolute timeline serves as a ruler, and the timestamps $\{t\}$ on it act as \emph{anchors} to pinpoint the observational snapshots to form the objective modeling data.

Consider an analogy where ants dwell on a two-dimensional plane of a floor. If these ants were to construct models, they might use the nearest tree as a reference to specify the elevation in their two-dimensional models. %, bypassing the recognition of the third dimension.
By modeling, they observe an increased disruption at the tree's mid-level, which indicates a higher chance of encountering children. However, since they fail to comprehend humans as three-dimensional beings, instead of interpreting this phenomenon in a new dimension ``height'', they solely relate it to the tree's mid-level.
If they migrate to a different tree with a varying height, where mid-level no longer presents a risk, they might conclude that human behavior is too complex to model effectively.
Similarly, when modeling time series, we usually discount the dimension ``time'' as the single absolute timeline, which has become our ``tree''.

Our understanding allows for the simultaneous existence of multiple logical timelines. If one is designated as the absolute timeline, the remaining ones can be viewed as relative timelines, each representing distinctive temporal events, which can be interconnected via specific relationships.
In such \emph{Relation-Oriented} perspective, like, during a causal inference analysis, the temporal dimension contains numerous possible logical timelines that we could choose to construct any necessary scenarios.
However, once we enter a modeling context, like, using AI to model the time series along a single timeline, the temporal significance no longer exists, but only 
%is relegated to 
a regular dimension containing timestamps, indistinguishable from other observational values.
Metaphorically, if we consider the observational space for AI modeling as Schrdinger's box and our interest is the ``cat'' within, our task is to accurately construct the box, giving adequate consideration to all potential logical timelines,
to ensure the ``cat'' remains \emph{reasonable} upon unveiling.

\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=red!50!black, colback=yellow!10]{\begin{minipage}{0.95\textwidth}
\paragraph{Lemma 2.} \emph{Temporal Dimension} refers to the aggregation of all potential logical timelines, not a single one. A \emph{Temporal Space} in comprehension is the space built by chosen timelines as axes.
\end{minipage}}
%\vspace{1mm}

Fundamentally, as three-dimensional beings, we are limited from truly understanding temporal dimensionality. As the term ``space'' typically evokes a three-dimensional conception, the notion of ``temporal space'' might seem odd for a four-dimensional creature. 
Like ants can use trees as references without the need to fully comprehend the third dimension, we rely on logical timelines to interpret the fourth. At this juncture, our mission is to recognize the potential ``forest'' beyond the present single ``tree''. 


% Metaphorically, the observational modeling space is akin to Schrdinger's box, and our interest lies in modeling the ``cat'' within. Before opening it, we can envision any plausible attributes for this ``cat''. However, at times, our method of unveiling the box may not match our envisioned scenarios, leading to an unreasonable ``cat'', which is unseen to us but visible to AI. Consequently, while we retain our initial vision, the outcomes delivered by AI can appear unexpected.

In this section, we initially dissect the mechanism of inherent temporal bias within the current AI systems, due to neglecting potential multi-timelines; then, reassess existing methods of learning dynamics; and last, summarize advancements and challenges on our journey towards realizing knowledge-aligned causal AI.


%This complexity is particularly heightened when multiple timelines exist within the neglected knowledge hierarchy, introducing more substantial inherent temporal biases (refer to Section \ref{sec:CRB}).




\subsection{Inherent Temporal Bias}
\label{sec:CRB}
\vspace{-2mm}

% The overlooked ``effect events'' can cause more substantial problems, beyond just inadequately representing a single relation. 
% Specifically, in a complex knowledge system using \emph{Structural Causal Model} (SCM), when multiple events occur on separate (relative) timelines, complications arise.
% The effect in $do(X)\rightarrow Y$ can be a cause in another relation like $Y\rightarrow Z$. 
% If the effects along two pathways $do(X)\rightarrow Z$ and $do(X)\rightarrow Y\rightarrow Z$ progress on two different timelines, the SCM $Z=f(Y, do(X))$ may unintentionally become \textbf{\emph{invalid}}.

Overlooking multi-timelines in \emph{structural causal models} (SCMs) can lead to \emph{inherent temporal biases}. These biases substantially limit our ability to fully harness AI's potential in modeling nonlinear dynamics, especially within large-scale causal relationships, which may include more complex multi-timelines.

To better ascribe this issue, we \emph{redefine} the causal Directed Acyclic Graph (DAG) \cite{pearl2009causal} as follows: \textbf{1)} incorporating (potentially multiple) logical timelines as axes into the DAG space, and \textbf{2)} defining edges along timeline axes to be vectors with meaningful lengths indicating the timespans of causal effects.
For example, the single-timeline scenario in Figure~\ref{fig:eff} has the redefined DAG depicted in Figure~\ref{fig:do1}(b), with (a) showing the traditional one as a comparison. 
The edge $do(A)\rightarrow B$ in Figure~\ref{fig:do1}(a) can only (partially) represent population-level effect, thus necessities a hidden confounder to explain the individual-level diversities, while in Figure~\ref{fig:do1}(b), they can be explicitly represented by varying lengths of $\overrightarrow{do(A)\  B}$.

Consider an expanded two-timeline scenario in Figure~\ref{fig:do3}(a), where $A$ shorthandly represents $do(A)$. Apart from its primary effect on $B$, $A$ also indirectly influences $B$ through its side effect on another vital sign, $C$, depicted as edges $\overrightarrow{AC}$ and $\overrightarrow{CB}$. For simplicity, assume the timespan for $\overrightarrow{AC}$ is 10 days for all patients, with the individual-level diversity solely confined to timeline $T_X$.
In conventional single-timeline causal modeling, the SCM function would be $B_{t+30}=f(A_t, C_{t+10})$.
Let's assume $f(A_t, C_{t+10})$ is implemented using RNNs, which could accurately depict the individual-level final effects of $A$ on $B$ for any patient.

The confounding relationship over nodes $\{A, B, C\}$ forms a triangle across timelines $T_X$ and $T_Y$ - such shape geometrically holds for any hierarchical level relationship.
For patients $P_i$ and $P_j$, the \emph{individualization} process is to ``stretch'' this triangle along $T_X$ by different ratios, which is a homographic \emph{linear transformation} in this space.
%The conventional SCM model $B_{t+30}=f(A_t, C_{t+10})$, however, may not realize such a transformation but lead to invalid outputs. 
%For example, let's still consider $f$ as the correlation model that accurately represents the expected population-level effect on $B$, under the causal inference context.
%Assume the causal effect function $f(A_t, C_{t+10})$ can accurately represent any individualized final effects of $A$ on $B$ (no longer a mere correlation). 
However, as illustrated in Figure~\ref{fig:do3} (b) and (c), for either $P_i$ or $P_j$, equating the outcome of $f$ to be $B_{t+30}$ violates the \emph{causal Markov condition} necessary for reasonable SCMs.

% Figure environment removed
\vspace{-3mm}

% Figure environment removed
\vspace{-1mm}

Notably, in this specific case, the violation may not cause significant issues for RNN models.
%involving the second timeline may not cause further complications compared to Figure~\ref{fig:do1}(b). 
Given the independence of dynamical features on $T_X$ and $T_Y$, the SCM can be formulated as $B_{t+30}= f_1(A_t)+f_2(C_{t+10})$. This suggests that the cross-timeline confounding can be broken down into two single-timeline issues, where capturing hierarchical dynamics might challenge statistical models but not RNNs.
%While in a further special case, if the confounding does not exist, like eliminating the indirect influence $\overrightarrow{CB}$, the SCM predicting $B$ can be formed as $B_{t+30}=\alpha f(A_t)$.
However, assuming special conditions like independence or non-confounding is impractical. Given that each pair of cause and effect could potentially inhabit a logical timeline, such temporal biases are inherently prone to exponentially accumulate and impact our SCM applications, regardless of the model implementation.

\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=red!50!black, colback=yellow!10]{\begin{minipage}{0.96\textwidth}
\paragraph{Lemma 3.}The traditional SCM may have the inherent temporal bias if containing: 
\textbf{1)} \emph{Confounding} of dynamics across \emph{Multiple} logical timelines, and \textbf{2)} Unobservable hierarchy of such confounding.
\end{minipage}}
\vspace{1mm}

It is interesting to notice that most of the successful applications instinctively avoid one of the two factors: \emph{confounding} or \emph{multi-timelines}. 
For statistical causal models, the interpretability allows them to be manually adjusted to facilitate de-confounding, e.g., the backdoor adjustment \cite{pearl2009causal}.
For AI models, most of the sweeping achievements do not potentially involve relative timelines, e.g., the large language model (LLM) in a semantic space, where the phrases are ordered consistently along a single logical timeline.

Unlike AI's black-box nature, causal inference inherently takes a \emph{Relation-Oriented} view. But in its context, the inherent temporal biases are difficult to identify, as they often intermingle with biases resulting from the inability to model nonlinearity - They similarly manifest as statistical diversity (or data heterogeneity) among levels, and both can be addressed via de-confounding.
Consider Figure~\ref{fig:do1}(a), a correlation model that only captures the population-level effect can mismatch with individuals $P_i$ and $P_j$, which may not be distinguishable from the model mismatching in Figure~\ref{fig:do3}(b)(c), caused by crossing two timelines.

% ``confounding biases''.

% In traditional applications, the statistical diversities across levels of subgroups are often summarized as \emph{data heterogeneity} problem \cite{grissom2000heterogeneity}.

% Nevertheless, as we strive towards developing true AI capable of integrating causal knowledge, such inherent biases pose a significant challenge. Therefore, their identification under general circumstances has become a critical task in our current efforts.

% Consider the large language model (LLM), which recently gained a widespread reputation through ChatGPT. The foundation is reinforcement learning based on Markov decision processes (MDPs). 
% In semantic modeling, confounding and heterogeneity are ubiquitous, while fortunately, the multi-timeline problem typically not exists. In short, the phrases' orders represent consistent grammatical meanings.
% The other comparable scenarios include AI in Go gaming and the style-confusing transformation of images.

%Causal inference essentially aims to address the multi-timeline complexity, but traditional statistical models, limited to observational spaces, struggle with direct modeling the temporal distributions. Instead, they often rely on specific conditions to create temporal independence (like in \emph{do-calculus}), and accordingly require manual operations, such as 


% Lastly, AI can undoubtedly do a great job if some applications only need to model one object without concerns about overfitting or generalizability among different objects.




% are often applicable to specific types of problems, we should not limit our aspirations to selective applications. Ultimately, we must confront and address broader questions that embody hybrid complexities, in our pursuit of aligning human knowledge with AI capabilities. 



\vspace{-1mm}
\subsection{Learning Dynamics}
\label{sec:dynamics}
\vspace{-2mm}
For observational variable instance $x\in \mathbb{R}^d$, we frequently encode its temporal evolution as a sequence along the timeline, such as $\{x_t\}=\{x_1, \ldots, x_{t}, x_{t+1}, \ldots, x_T\}$, without considering $t$ as the $(d+1)$-th dimension. Accordingly, we seldom view $x$'s changing value over time as a ``distribution'' along the $t$-axis. 

In prediction $x_{t+m}=f(x_t)$ with integer $m>0$, the potential nonlinearity of function $f$ is only acknowledged within $\mathbb{R}^d$, but leaving the relationship between $x_t$ and $x_{t+m}$ to be \textbf{\emph{linear}}. It  implies that the \textbf{\emph{correlation}} model $x_{t+m}=f(x_t)$ can only capture \textbf{\emph{static}} temporal elements in sequence $\{x_t\}$. If, however, no suitable $(f,m)$ can be found to ensure this model adequately represents $\{x_t\}$, there may exist unrepresented \textbf{\emph{dynamical}} temporal features, which necessitates a \textbf{\emph{causality}} model of the form $\{x_\tau\} = f(\{x_t\})$. Here, cause and effect are as two distinct sequences on logical timelines, $\tau$ and $t$, respectively, such that, the relationship between any pair of $x_t$ and $x_\tau$ could be \textbf{\emph{nonlinear}}.

Consider this form of causality model: $y_\tau = f(\{x_t\})$ with $\tau = (t+T)+m$, representing a sequence $\{x_t\}$ causing a static outcome $y_\tau$, where timeline $\tau$ is confined as $m$-timestamp later than timeline $t$.
%($=$ a sequence $\{y_\tau\}$ of length one), $m$ timesteps later on $t$. 
This form is typically adopted in many dynamical learning methods, such as autoregressive models \cite{hyvarinen2010estimation} and RNNs \cite{xu2020multivariate}. 
For autoregressive, if $f$ is defined as a linear function, this model is restricted to only capturing \emph{a \textbf{single-level} dynamic} of the cause.
%with respect to $T$ elements of $\{x_t\} $ 
For instance, in Figure~\ref{fig:eff}(b), let $T=30$, a linear autoregressive model may capture the population-level complete sequence, but still cannot represent individuals' speeds. Contrarily, RNNs embrace nonlinearity for $f$, thus potentially can capture both levels of dynamics (without disentanglement). 

However, RNNs primarily target dynamics only for the cause, while remaining constrained to a static effect. This limitation makes sense, as it is feasible to designate significant time sequences for the cause, but pinpointing precise start and end timestamps for ensuing effects is challenging. 
On the other hand, Granger causality \cite{maziarz2015review}, a method extensively used in economics, acknowledges the existence of potential multi-timelines in causality. It employs the general form $\{y_\tau\} = f(\{x_t\})$, where $t$ and $\tau$ typically represent two distinct timelines, thereby including a sequential effect. However, as primarily a statistical test, it is best suited for handling single-level dynamics and necessitates additional specifications for effect sequences.

A more universally applicable concept of dynamics is the \emph{do-calculus} \cite{pearl2012calculus, huang2012pearl}, which refrains from assuming specific time sequences. Instead, it treats \emph{identifiable} temporal events as objects around which it conducts elementary calculus, adopting a more \emph{Relation-Oriented} viewpoint.
In the subsequent discussion, we will revisit the three rules of \emph{do-calculus} from a differential-and-integral perspective. 

For sequence $\{x_t\}=\{x_1, \ldots, x_T\}$, let $do(x_t)=\{x_t, x_{t+1}\}$ indicate the occurrence of an instantaneous event $do(x)$ at time $t$. Time lag $\Delta t$ between $\{t,t+1\}$ is sufficiently small to make this event elementary, such that $do(x_t)$'s \emph{interventional} effect can be depicted as a function of the resultant distribution at $t+1$. Conversely, the effect provoked by static $x_t$ is \emph{observational} effect. So, dynamics of cause $X\in \mathbb{R}^d$ can be formulated as:

\vspace{-6mm}
\begin{align*}
    \text{Given } \mathcal{X} & \rightarrow Y \mid Z, \text{ where } \mathcal{X} = \langle X,t \rangle \in \mathbb{R}^{d+1} \text{ encompass sequence } \{x_t\}, \text{ we have the cause} \\
    \mathcal{X} =& \int_0^T do(x_t) \cdot x_t \ dt \text{\ \ with }
    \begin{cases}
      (do(x_t)=1) \mid do(z_t), & \text{ \emph{Observational} only (Rule 1) } \\
      (x_t=1) \mid do(z_t), & \text{ \emph{Interventional} only (Rule 2) }    \\
      (do(x_t)=0) \mid do(z_t), & \text{ No \emph{interventional}  (Rule 3) }   \\
      \text{otherwise} & \text{ Associated \emph{observational} and \emph{interventional} }
    \end{cases} \\
     \text{The effect } & \text{of } \mathcal{X} \text{ can be derived as }
     f(\mathcal{X}) = \int_0^T f_t \big( do(x_t) \cdot x_t \big) \ dt = \sum_{t=0}^{T-1} (y_{t+1}-y_t) =y_T-y_0
\end{align*}
\vspace{-4mm}
% In \emph{do-calculus}, event $do(\cdot)$ acts as the causal variable, required to be ``identifiable'', indicating that its effect can be derived from the subsequently observed distributions. This aligns with the ``elementary'' requirement in our reformulation context, where $\Delta t$ or $dt$ does not intend to be infinitesimal but should facilitate event recognition.
% However, ``identifiability'' implies a preference for linear interpretability. 

Fundamentally, the three rules of \emph{do-calculus} address all conceivable conditional independencies between the \emph{observational} and \emph{interventional} effects within the $\{X,Y,Z\}$ graphical system, sidestepping the most general cases (specifiable associated cause $do(x_t)\cdot x_t$ belongs to Rule 2).

Given the substantial flexibility provided by the $do(\cdot)$ format, we can also represent the effect $Y$ dynamically, by introducing timeline $\tau$ as an extra dimension, yielding $\mathcal{Y}=\langle Y,\tau \rangle$. Nonetheless, determining the \emph{identifiable} events within $\mathcal{Y}$ still requires manual specification that may not be practical, in contrast to our proposed relation-defined representation learning that can construct $\mathcal{Y}$ automatically.

% Despite the flexible form of $do(\cdot)$, the specification of ``effect events'' on the timeline is still challenging for traditional methods. Accordingly, \emph{do-calculus} still handles effects as an observational outcome $Y$ instead of dynamical $do(Y)$. 
% Moreover, any applicable $do(X)$ is restricted to identifiable events, intentionally eliminating the timespan to avoid incorporating further temporal dimensional variances. 

% In essence, conventional causal inference turns the causal dynamics into static causal variables, approximating the relationship through correlation models in the form $do(X)\rightarrow Y$, not $do(X)\rightarrow do(Y)$.
 
% However, they remain to be timestamp-specific sequences of fixed lengths, which is, in essence, manually selecting constants to represent target temporal features more effectively, rather than genuinely incorporating them as variables with potential distributions, analogous to specifying 7 as the outcome of rolling two dice.


%construct the cause event in $do(X)\rightarrow Y$, there is no straightforward way to instantiate the ``effect event'' as $do(X)\rightarrow do(Y)$.
%Consequently, traditional causal inference primarily employs correlation models to closely approximate causal relationships, by treating effects as mere static outcomes.



% Subsequently, if the autoencoder technique can adequately represent any dimensional features, including temporal ones, can we eliminate the need to differentiate between causality and correlation?

%The original intention of modeling centers on the relationship itself, rather than its interpretation.
% In summary, the \textbf{\emph{Relation-Oriented}} perspective can be articulated as follows:
% The distinction between causality and correlation hinges on the entities connected by the relationship, rather than the relationship itself. 
% To use an analogy, a double-ended cotton swab is differentiated by the specific characteristics of its cotton ends - such as shape, texture, or color for different uses - rather than the connecting stick itself.




\subsection{Toward Knowledge-Aligned AI}
Our quest for authentic causally reasoning AI that can align with our knowledge, involves the process of broadening our modeling techniques from purely observational to include temporal and hyper-dimensional spaces. Referring to Figure~\ref{fig:model}, our present challenge lies in leveraging AI's potential within structural causal models. The key to this task resides in identifying potential multi-timelines. However, manual identification is not feasible, especially given that both logical timelines and knowledge hierarchies are not directly observable. It is time for us to transition from an \emph{Observation-Oriented} to a \emph{Relation-Oriented} modeling paradigm.


\vspace{-4mm}
% Figure environment removed
\vspace{-1mm}



The initial models under i.i.d. assumption only approximate observational associations, proved unreliable for causal reasoning \cite{pearl2000models, peters2017elements}. Correspondingly, the common cause principle highlights the significance of the nontrivial conditional properties, to distinguish structural relationships from statistical dependencies \cite{dawid1979conditional, geiger1993logical}, providing a basis for effectively uncovering the underlying structures in graphical models \cite{peters2014causal}.

Graphical models, employing conditional dependencies to construct Bayesian networks (BNs), often operate in observational space and neglect temporal aspects, reducing their causal relevance \cite{scheines1997introduction}. Notably, causally significant models, such as Structural Equation Models (SEMs) and Functional Causal Models (FCMs) \cite{glymour2019review, elwert2013graphical}, are able to address \emph{counterfactual} queries \cite{scholkopf2021toward} - We reinterpret it as capturing \emph{temporal distributions} and responding to \emph{conditional} questions.
Typically, these models leverage prior knowledge to construct causal Directed Acyclic Graphs (DAGs).
%In practical scenarios, causal learning and discovery often blend, as a system can exhibit a mix of known and unknown, causal and non-causal characteristics.

State-of-the-art deep learning applications on causality, which encode the DAG structural constraint into continuous optimization functions \cite{zheng2018dags, zheng2020learning, lachapelle2019gradient}, undoubtedly enable efficient solutions for large-scale problems. However, this approach potentially conceals numerous relative timelines, which may result in substantial temporal biases. This is particularly apparent in the limited success of applications that incorporate DAGs into network architectures \cite{luo2020causal, ma2018using}, such as Neural Architecture Search (NAS).
Schlkopf \cite{scholkopf2021toward} succinctly highlights three key challenges impeding the broader success of causal AI: 1) limited model robustness, 2) insufficient model reusability, and 3) inability to handle heterogeneity. The first two challenges can be associated with inherent temporal biases, while the latter originates from the unobservable knowledge hierarchy.

On the flip side, physical models, which explicitly integrate time as a dimension, able to establish abstract concepts through structural knowledge, may provide insights into our upcoming challenges. The proposed \emph{Relation-Oriented} modeling is designed along these lines, aiming to transcend the observational limitations innate to our prevailing modeling paradigms.
%Figure~\ref{fig:model} provides a succinct summary of these discussed model types. %, arranged from the most knowledge-driven at the top to the most data-driven at the bottom.

%while we leave the temporal dimensional causal discovery as a prospect for future exploration.
%Physical knowledge manifests humans' ability to understand timelines as dimensions and reflects our innate relation-first mentality. The Relation-Oriented approach paves the way for machine learning to shatter its dimensional limitations.



% The notable successes in big data applications emphasize that given sufficient data, AI can vastly outperform human capabilities within observational spaces, even under the strong i.i.d. assumption.
% However, it stumbles on learning ``shifting distributions'', which are trivial for humans to understand \cite{scholkopf2021toward}. 

% In essence, human comprehension can extend beyond the observable reality into hyper-dimensional space, including but not limited to the temporal dimension, i.e., the timeline. For us, ``shifting'' is simply a change along this additional dimension.
% Contrarily, the scope of AI is inherently confined to the input data space, a boundary set by us. Under the prevailing Observable-Oriented modeling paradigm, AI's capacity is restricted to inferring observational distributions
% only, such as the remarkable LLMs (large language models) learning associations within the semantic space.

%in causal learning. First, in domain sciences, integrating \emph{observable} multi-variable and multi-timestep data into correlations can enhance models \cite{guyon2008practical, zhao2013gut, marwala2015causality}, but still far from fully revealing the causal scheme. %Conversely, in AI applications, while 


% Neural networks capably consolidate large observational contexts into a global model, yet more challenging to integrate existing causal knowledge compared to traditional models \cite{luo2020causal}.





% In previous work \cite{li2020teaching}, the existence of CRB in RNN models based on clinical data has been experimentally verified. The main objective of this paper is to analyze the formation mechanism of CRB and summarize the critical factors involved, using the redefined causal DAG (i.e., \emph{do-DAG}) to aid in the visualization of CRB.
% Furthermore, this paper proposes a new \emph{Causal Representation Learning} framework for causal AI, which functions as a generic solution.
% This paper also contributes significantly by implementing the higher-dimensional feature learning autoencoder, which is a crucial technology for achieving CRL.


% Deep Learning (DL) is capable of handling non-linear and higher-order dependencies among variables, while also providing global optimization solutions. Due to its effectiveness, DL is becoming increasingly popular in this area \cite{luo2020causal}.
% One notable contribution is the conversion of the discrete combinatorial constraints on DAG's acyclicity into continuous ones, enabling global optimization on network structures \cite{zheng2018dags, lachapelle2019gradient}. This approach has been further extended to nonparametric modeling \cite{zheng2020learning}.
% Furthermore, to leverage the interpretability from causal inference, some works construct the known causal DAG by properly designing architecture \cite{ma2018using}, while others attempt to infer dependencies directly from neuron weights \cite{lachapelle2019gradient, zheng2020learning}. 
% Such neural architecture search (NAS) methodologies aim to realize accuracy as well as the transparency of neural networks simultaneously but have not reached a general success \cite{luo2020causal}. 

% However, unlike in traditional methodologies, where intentional adjustments can be made to account for the individual-level feature biases, DL's black-box nature can cause such biases to go unnoticed.
% Additionally, due to its high effectiveness, DL is often applied to larger-scale questions with numerous variables, which can further exacerbate the accumulation of unnoticed biases leading to significant global distortions.

% The hope that DL can automatically solve CRBs has been shown to be unrealistic in previous work \cite{li2020teaching}. Instead, DL should be used to model something that we have a prior interpretation of, rather than trying to interpret what DL has learned. Therefore, DL is likely to be applied more often for learning disentangled causal relationships, which may lead to wider adoption of higher-dimensional feature representations in the future for this purpose.



% In a Relation-Oriented context, dynamical features on temporal dimensions can be identifiable and serve as augmented additional features for causes and effects in the observational spaces. This expands the dimensionality of the modeling space by incorporating potential timelines as additional axes. Then, the ability of a model to handle \emph{counterfactual} inquiries corresponds to its capacity to encompass the temporal distributions on queried timeline axes.



%The ``structural'' and ``graphical'' models often blend in practice, with DAG illustrating the structure. 
%, inferring latent structures from observed dependencies, 
%using either constraint-based (like fast causal inference) or score-based (like greedy equivalence search) approaches 







\section{Obs-Tmp Representations' Hierarchical Disentanglement}
\label{sec:factorization}

Let $X\in \mathbb{R}^d$ denote an observational variable, with instance sequences in data presenting its dynamics. Suppose a logical timeline $t$ exists, along which, $X$'s sequence can be denoted as 
$\{x_t\}=\{x_1, \ldots, x_{t}, x_{t+1}, \ldots, x_T\}$.
We aim to devise a latent feature space $\mathbb{R}^L$ for two purposes:
\textbf{1)} Establish representation of the observational-temporal (obs-tmp) variable $\mathcal{X}=\langle X, t \rangle \in \mathbb{R}^{d+1}$ in $\mathbb{R}^L$, fully capturing observational and dynamical features of the sequence $\{x_t\}$ in data.
\textbf{2)} Disentangle this representation in $\mathbb{R}^L$ according to desired hierarchy.

Also, we want this hierarchical disentanglement to be indexed via relationships, reflecting different levels of knowledge. The advantage of this form is that the representations we establish, defined and encapsulated by recognized relations, are highly \emph{reusable}, accommodating further structural modeling. Consequently, this method enables the \emph{generalization} of the desired knowledge level across varied contexts.


Consider another observational variable $Y\in \mathbb{R}^b$, and suppose its dynamical sequence is along logical timeline $\tau$, denoted as $\{y_\tau\}$.
Given relationship $\mathcal{X}\rightarrow \mathcal{Y}$, the proposed \emph{\textbf{relation-defined representation}} learning is to establish representation of $\mathcal{\hat{Y}} = \langle \hat{Y}, \tau \rangle \in \mathbb{R}^{b+1}$ in the latent space, which represents a new data sequence $\{\hat{y}_\tau\}$, versioned from $\{y_\tau\}$ by selecting its observational-temporal features that can only be caused by $\mathcal{X}$.

In structural modeling, for example, the causal system $\mathcal{X}\rightarrow \mathcal{Y} \leftarrow \mathcal{Z}$ can be disassembled into $\mathcal{X}\rightarrow \mathcal{\hat{Y}}$ and $\mathcal{Z}\rightarrow \mathcal{\tilde{Y}}$, allowing flexible reuse of both $\mathcal{\hat{Y}}$ and $\mathcal{\tilde{Y}}$ representations.
Furthermore, if consider $\mathcal{Y}$ features a two-level hierarchy, with the first determined by $\mathcal{X}\rightarrow \mathcal{Y}$ and the second by $(\mathcal{X}, \mathcal{Z}) \rightarrow \mathcal{Y}$, we can also establish the representation of $\mathcal{Y}$ as hierarchically disentangled, where the second level builds upon the first, introducing an additional data stream from $\mathcal{Z}$.


Next, we will sequentially factorize three transformation processes between the data and latent space $\mathbb{R}^L$: First, observational features from $\mathbb{R}^{d}$ to $\mathbb{R}^L$; Second, observation-temporal features from $\mathbb{R}^{d+1}$ to $\mathbb{R}^L$; Last, the relationship $\mathcal{X}\rightarrow \mathcal{Y}$, from the joint space $\mathbb{R}^{d+1} \circ  \mathbb{R}^{b+1}$ to $\mathbb{R}^L$.

% Depending on the fields of application or study, $\{x_t\}$ may have different explanations, like \emph{time series}, \emph{trajectories}, etc \cite{feng2016survey, anderson1998partitioning,aminikhanghahi2017survey}. For instance, in a spatiotemporal context, the sequential entities may present to be geographical coordinates coupled with timestamps, like $\{<x_1,t_1>, <x_2,t_2>, \ldots, <x_T, t_T>\}$. 
% But, no matter what, one truth remains: $t$ is not considered one of the dimensions constructing the modeling space - It can be seen from the Picard-Lindelof formula, which characterizes our current temporal modeling paradigm:
% If a Lipschitz function $f(X) = dX/dt$ exists in some region of time $t$, then at least locally, the immediate future value of $X$ is predictable:
% \begin{equation}
%     X(t+dt) = X(t) + dt \cdot f(X(t)), \text{ where } f(X) = dX/dt.
% \end{equation}
% Since $f$ has been constrained to the derivative of $X\in \mathbb{R}^d$, the modeling freedom is bound within $d$ dimensions, excluding the temporal one defined by $t$.
% In this section, we will demonstrate the formulated factorization in the latent space $\mathbb{R}^L$ to represent $\langle X, t \rangle \in \mathbb{R}^{d+1}$ in a hierarchical way given knowledge-alined levels - which can also be called \emph{hierarchical disentanglement}.

% This formula characterizes our conventional temporal modeling paradigm, yet interestingly, it does not reflect our intuitive grasp of timelines as an inherent component of the dimensional framework. For instance, in daily life, we can naturally perceive events as entities, predicting future outcomes and discerning prerequisites. Moreover, we can instinctively contemplate ``What if...'' scenarios, using timelines to derive conditional temporal distributions and respond to counterfactual inquiries.


%Intriguingly, human intelligence naturally understands temporal elements without reliance on timestamp anchors, and can systematically construct knowledge by considering timelines as part of the dimensional framework - a prime example is the formulation of Special Relativity in the field of physics.
%Moreover, despite the complex theory about time itself, in our daily lives, we intuitively comprehend timeline as a standard dimension: We can naturally understand the occurrence of events as an entity, and accordingly envision its potential subsequences (as effects or results) and antecedents (as causes or prerequisites). In essence, considering ``What if...'' (as counterfactual) scenarios is a distinctive human ability, setting us apart from other non-intelligent animals.





% By representing observables as disentangled features, we enable the desired causal interpretation (such as the meaning of direction) to be incorporated as additional features. This refocuses the modeling process on the basic connection, relieving the burden of conveying causal implications.


\subsection{Factorization of Observational Hierarchy}

Assume $X=(X_1,\ldots,X_d) \in \mathbb{R}^d$ exhibits an $n$-level hierarchy. We utilize $\Theta_i$ to signify its $i$-th level observable feature in the data form, while its corresponding latent space feature is denoted as $\theta_i$. Then, we obtain:
\begin{equation}
    X = \sum_{i=1}^{n} \Theta_i, 
     \text{ where } \Theta_i = f_i \bigl(\theta_i ;\ \Theta_1,\ldots, \Theta_{i-1} \bigr) \text{ with } \Theta_i \in \mathbb{R}^{d} \text{ and } \theta_i \in \mathbb{R}^{L_i} \subseteq \mathbb{R}^{L}
\end{equation}
In this context, $\theta_i$ is considered as a vector in $\mathbb{R}^{L}$, in which only a subset of the $L$ dimensions carries significant value, represented as the subspace $\mathbb{R}^{L_i}$. The hierarchical disentanglement is depicted by the arrangement of these subspaces:  $\{\mathbb{R}^{L_1}, \ldots, \mathbb{R}^{L_i},\ldots,\mathbb{R}^{L_n}\}$. Accordingly, the representation function $f_i$ enables the $i$-th level transformation between $\mathbb{R}^{d}$ and $\mathbb{R}^{L_i}$, drawing its potential attributes from all preceding lower-level features. %Notably, a hierarchical level may include multiple features as needed, indicating that $\{\theta_i\}$ might exist as a set.

% In this framework, latent space $\mathbb{R}^{l}$ consists of all necessary feature dimensions, where $\theta_i$ can be defined as spanning any subset of the $l$ dimensions, denoted $\mathbb{R}^{l_i}$. Notably, there are no dimensional constraints between the higher and lower levels.
% In short, the ``hierarchy'' indicates the way of building their representation functions $\{f_i \mid i=1,\ldots,n\}$ (upon each other as attributes), rather than restricting their dimensional overlapping in $\mathbb{R}^{l}$.

To illustrate this factorization, consider Figure~\ref{fig:hand} (b). Let's say $\theta_1$, $\theta_2$, and $\theta_3$ effectively represent the features spanning from Level I to III, exclusively occupying meaningful subspaces $\mathbb{R}^{L_1}$, $\mathbb{R}^{L_2}$, and $\mathbb{R}^{L_3}$. The image can be fully represented as an augmented vector $\langle\theta_1, \theta_2, \theta_3\rangle \in \mathbb{R}^{L}$. Correspondingly, $\Theta_1$, $\Theta_2$, and $\Theta_3$ are full-sized images containing distinct content. Their cumulative representation can progressively render the complete image. E.g., $\Theta_1$ isolates the fingers' details, while $\Theta_1+\Theta_2$ broadens to depict the entire hand.



% In statistics, changes in observations can only be reflected as correlations, but the emergence of DL-based autoencoders has been a game-changer. They can represent observations as features in a latent space where the changes in features can directly reflect the causal effects. 


\subsection{Factorization of Obs-Tmp Hierarchy}
\label{sec:multi-timelines}

For the time sequence of $X$, denoted as $\{X_t\} = \{X_t \in \mathbb{R}^{d} \mid t=1,\ldots,T\}$, it can be viewed as a $d$-dimensional vector extending $T$ times, while we aim to represent it in a new form $\mathcal{X}=\langle X, t \rangle \in \mathbb{R}^{d+1}$ in $\mathbb{R}^L$, such that:
\begin{equation}
    \mathcal{X} = \sum_{i=1}^{n} \Theta_i, 
     \text{ where } \Theta_i = f_i \bigl(\theta_i ;\ \Theta_1,\ldots, \Theta_{i-1}\bigr) \text{ with } \Theta_i \in \mathbb{R}^{d+1} \text{ and } \theta_i \in \mathbb{R}^{L_i} \subseteq \mathbb{R}^{L}
\end{equation}
It is essential to note that each $\theta_i\in \mathbb{R}^{L_i}$ represents an observational-temporal feature, corresponding to the observable component $\Theta_i \in \mathbb{R}^{d+1}$. The latter can also be represented in its original time sequence form $\{\Theta_{t}\}_i = \{\Theta_{t_i} \in \mathbb{R}^{d} \mid t_i=1,\ldots,T\}$.
Hence, we have a collection of logical timelines $\{t_1,\ldots,t_i,\ldots,t_n\}$, \emph{relative} to the \emph{absolute} timeline $t$ in this context. 
These $n$ relative timelines, while may or may not be distinct, are each individually determined by their corresponding relationships.
This allows each $t_i$ to automatically adapt to the specifics of its associated relation, liberating us from recognizing potential timelines. For each level of observation, denoted by the summation $\Theta_1+\ldots+\Theta_i$, the presented form is consistently an observable time sequence along $t$, without concerns about inherent temporal biases. 

Consider the scenario depicted in Figure~\ref{fig:do3}, with data stored in a matrix comprising four columns: values of $A$, $B$, $C$, and timestamps $t$. We can represent $B$ as a 2-level hierarchy: the first level is defined by its direct effect from $A$ ($A\rightarrow \hat{B}$), while the second level incorporates an additional stream denoting its indirect effect via $C$ ($A\rightarrow C\rightarrow \tilde{B}$). 
The comprehensive representation of the effect, $\hat{B}+\tilde{B}$, is naturally depicted as two columns - $B$ and $t$ - aligning with its original form. This approach eliminates the need for specifying $B_{30}$ to build estimation, unlike traditional SCMs.



% $t$-axis is a relative timeline for illustrating medication $A$'s effect on $B$. The data matrix $\mathcal{X}$ has three columns: $A$-value, $B$-value, and $t$-timestamp. Now, consider  Figure~\ref{fig:do3}(a), where an additional effect on $C$ is involved. This scenario contains two relative timelines, $T_X$, and $T_Y$. However, the data matrix $\mathcal{X}$ can still have one column for the $t$-timestamp, incorporating one more column for the observed $C$-value. In short,



%However, irrespective of the complexity of the underlying relationships, they often show up in observations as a multivariate time sequence tied to a single absolute timeline.



\subsection{Factorization of Relation-Defined Hierarchy}

% Consider $\mathcal{F}(\mathbf{\theta}) = \mathcal{F}(\theta_1,\ldots,\theta_n) = \{ f_i(\theta_i) \mid i=1,\ldots, n\}$  as the collective representation function within this framework, where $\theta=(\theta_1,\ldots,\theta_n)$ denotes the tuple of aggregating all features for $X$. Then, the factorization can be concisely expressed as $X=\mathcal{F}(\mathbf{\theta})$. Practically, $\mathcal{F}$ can be implemented as one or a series of autoencoders, symmetrically for encoding and decoding with the invertibility naturally desired.

Consider the relationship $\mathcal{X}\rightarrow \mathcal{Y}$, where $\mathcal{X}=\langle X, t \rangle \in \mathbb{R}^{d+1}$ and $\mathcal{Y}=\langle Y, \tau \rangle \in \mathbb{R}^{b+1}$. 
Given a collection of $n$-level hierarchical representation functions for $\mathcal{X}$, denoted as $\mathcal{F}(\vartheta) = \bigl\{ f_i \bigl(\theta_i \bigr) \mid i=1,\ldots, n\bigr\}$, we aim to define the $n$ relationship functions, collectively referred to as $\mathcal{G}$, to feature $\mathcal{Y}=\mathcal{G}(\mathcal{X})$ as $n$-level hierarchy.
Let the $i$-th level relationship function be $ g_i(\mathcal{X}; \varphi_i)$ with $\varphi_i$ denoting its parameter. Then, we have:
% Accordingly, its representation function $f_i^X \bigl(\theta_i^X \bigr)$ can be derived from observable $\mathcal{Y}$. So, we have:
\begin{equation}
    \mathcal{G}(\mathcal{X}) = \sum_{i=1}^{n} g_i(\mathcal{X}; \varphi_i) = \sum_{i=1}^{n} g_i(\Theta_{i}; \varphi_i)  =
    \sum_{i=1}^{n} g_i \bigl(\theta_i;\ \Theta_1,\ldots, \Theta_{i-1}, \varphi_i\bigr) = \mathcal{Y}
\end{equation}
Therefore, the $i$-th level relation-defined representation of $\mathcal{Y}$ can be expressed as $g_i (\theta_i;\varphi_i)$ given the preceding $(i-1)$ levels observable features of $\mathcal{X}$ as possible attributes. In other words, the $i$-th level relationship can be represented as an augmented feature vector in the latent space $\langle \theta_i, \varphi_i \rangle \in \mathbb{R}^L$.
Now, let's utilize $\vartheta_X$ and $\vartheta_Y$ to differentiate the collective hierarchy representations for $\mathcal{X}$ and $\mathcal{Y}$, respectively. The collective relationship $\mathcal{G}$ from $\mathcal{X}$ to $\mathcal{Y}$ can be represented as the latent space relationship $\vartheta_Y=\langle \vartheta_X, \varphi \rangle$, where $\varphi=\{\varphi_1,\ldots,\varphi_n\}$ and $\langle \vartheta_X, \varphi \rangle$ indicates the pairwise augmentations between collection $\vartheta_X$ and collection $\varphi$.
%The latent model can be interpreted as a causal model, as proposed in \cite{bengio2012unsupervised}



% Figure environment removed



Consider the practical scenario depicted in Figure~\ref{fig:3d}.
From a differential-and-integral perspective, each differentiable unit of medical effect on LDL, delivered through $\overrightarrow{SA'}$, immediately begins influencing T2D via $\overrightarrow{A'B'}$. Concurrently, the subsequent unit effect is being generated. These two processes occur simultaneously until the medication $S$ is fully administered. Ultimately, the $B'$ we seek to evaluate embodies the total integral influence originating from $S$. Put another way, the emphasis in this DAG lies not on the absolute values of $\Delta t$ and $\Delta \tau$, which would represent actual time spans. Rather, it is on the meaning of the nodes, which define what $\Delta t$ and $\Delta \tau$ represent within this specific context.

In traditional causality theories, variables in the SCM function  $B'=f(A,C,S)$ can only signify observational aspects of these nodes. Consequently, the role of $B'$ requires a manual specification for its temporal feature, as to determine the absolute time span for the edge $\overrightarrow{SB'}$. For instance, by empirical experience, the impact of $S$ typically lasts around $30\pm10$ days. Thus, the time span of $\overrightarrow{SB'}$ is set to 30 days for the estimation of the population-level mean effect at $B'$.

While, how long  $\overrightarrow{SB'}$ should be set is not indeed crucial, the critical fact is that assuming a length for $\overrightarrow{SB'}$ essentially fixes the $\Delta t:\Delta \tau$ ratio (consider $\overrightarrow{SB'} = \overrightarrow{SA'}+\overrightarrow{A'B'}$) and consequently sets the shape of triangle ${ASB'}$ for the entire population.
As a result, patients within the $\pm10$ range risk being attributed an invalid ${ASB'}$-triangle and leading to inherent temporal biases, similar to Figure~\ref{fig:do3}(b)(c).
More significantly, even if the mean effect at $B'$ is accurately estimated for this particular population, the derived SCM can hardly be generalizable to all others, because the presumed $\Delta t:\Delta \tau$ ratio may not hold true in general.

Contrarily, the proposed approach of relation-defined representation, applied in learning the edge $\overrightarrow{SB'}$, focuses on the role of $B'$ as dictated by this edge, incorporating its temporal features. 
This obviates the need to define time spans for $\overrightarrow{SA'}$ and $\overrightarrow{A'B'}$, thereby relieving us from concerning the $\Delta t:\Delta \tau$ ratio. Within the 3D observational-temporal space, as depicted in Figure~\ref{fig:3d}, we can consider any desired model individualizations for patients or generalizations for other populations as linear transformations of the green-colored subgraph.


% But, in reality, the overall context is typically far more complex, involving intricate causal relationships among various medications, numerous vital signs, and targets for addressing multifaceted comorbidities.

% The graph shows two distinct timelines, labeled as $\mathcal{T}_Y$ and $\mathcal{T}_Z$, respectively. $\mathcal{T}_Y$ chronicles consecutive calendar dates, for monitoring patients' disease progression through measurements of LDL, blood pressure (BP), and estimated T2D risk. $\mathcal{T}_Z$ represents the direct impact of Statin on LDL, with its use (illustrated as node $S$) determined by physicians based on observed LDL levels.
% An additional axis assists in organizing these observed variables, thus facilitating a comprehensive 3D visualization.

% Despite the Statin, if more medications (like those for BP or blood glucose regulation) are incorporated, more timelines will be needed for their individual temporal influences, which potentially result in higher-dimensional pictures beyond our visualization capacity. Intriguingly, regardless of the inherent complexity, observed data can consistently be structured as a multivariate time sequence anchored within a single \emph{absolute} timeline - in this case, the timestamp dates in chronological order.
% The inclusion of more medications would only increase $d$, with the $(d+1)$-th dimension remaining as the timeline.
% put, the \textbf{\emph{multi-timeline}} relationship from knowledge is unobservable, and thus cannot be fully revealed by data-driven methodologies that solely rely on \textbf{\emph{single-timeline}} observations. 

% Figure~\ref{fig:3d} also depicts the observed relations in the single-timeline view, highlighted in green.
% Typically, the data-driven predictive function forms $B'=f(A, S, C)$, which implicitly sets the timescales for $\mathcal{T}_Y$ and $\mathcal{T}_Z$. For instance, suppose it has been known that reduced LDL requires 30 days (from $t$ to $t+1$) to fully impact the T2D risk. This information intuitively leads to the assumption that a similar 30-day period (from $\tau$ to $\tau+2$) would be sufficient for the $A\rightarrow A'$ effect when modeling $f$.
% In Section \ref{sec:CRB}, we will demonstrate how directly modeling the superficially observed relations can inevitably introduce biases.





\section{Relation-Defined Representation Methodology}
\label{sec:method}
Essentially, the proposed relation-defined representation is a technique for transforming the observational-temporal entities in our cognition into features - a format that AI systems can readily understand and build models on. 
The inherent temporal biases underscore the observational constraints of traditional methods, while concurrently recognizing their robust ties to our existing knowledge infrastructures.
%our inherent understanding of observational-temporal relationships into a format that AI systems can readily understand, thereby facilitating the building of models.
%The inherent temporal biases do not invalidate traditional methods or diminish our knowledge, but rather, they highlight the challenge of using observational-only techniques to answer questions that inherently involve temporal dimensions.
%The proposed \emph{Relation-Oriented} methodology allows us to leverage AI's strength in modeling temporal distributions, while traditional methods maintain much stronger links with existing knowledge systems.
As illustrated in Figure~\ref{fig:new}, we enable AI to develop generalizable models in latent feature space filled with human-unreadable representations, while harnessing its capabilities to amplify observations for bolstering the effectiveness of traditional models.



% Figure environment removed

Consider the practical example in Figure~\ref{fig:3d}. Traditional methods mitigate bias in the model $B'=f(A,C,S)$ by randomizing medication $S$ based on $A$, to disrupt conditional dependence, leading to the use of selective subpopulation while discarding a significant portion. In contrast, the proposed AI approach individually models $\overrightarrow{SB'}$ and $\overrightarrow{AB'}$ without manual adjustments, answering counterfactual questions regarding the medicinal effect. Furthermore, these AI models can simulate de-confounded observations for traditional models, enabling comprehensive knowledge-based studies in larger comorbidity contexts.

In this section, we introduce a proposed autoencoder architecture tailored for implementing relation-defined representation learning; based on it, outline the approach for ``stacking'' hierarchical levels of representations within the latent space for constructing graphical models; and lastly, present a causal discovery algorithm designed to operate within this latent feature space.



% To build representation for edge $A\rightarrow B$, we first create two feature vectors representing the two variables in the same latent space dimension, and then use RNN to model the causal effect $f(do(A))$ between them - RNN can capture the $do(\cdot)$ action from the historical time sequences of $A$ and connect it with the current feature of $B$ as the intervention. 
% To establish a connection between two edges in feature space, we need to combine or "stack" the feature representations of the variables involved. For example, to connect edges $A\rightarrow B$ and $B\rightarrow C$, the feature vector of variable $B$ needs to represent its two roles in the causal chain. Since the feature vector represents the status of the variable rather than its observed value, any changes in the feature vector do not affect the ability to reconstruct the observed value of $B$ in the data space.

% It worth mentioning that 
% Scholkopf et al. \cite{scholkopf2021toward} introduced the term Causal Representation Learning initially in the context of computer vision applications rather than general concepts. However, their "disentangled representation" method follows the same strategy and aligns with the proposed framework.







% Causal Representation Learning (CRL) differs from conventional models as it aims to learn the dependencies among latent space features (instead of data space variables), which represent corresponding observational variables in data space and are first individually extracted by autoencoders.



% Consider a time series $\mathbf{x}=[\ldots, x_{t-1}, x_{t}]$ of successively observed values of $X$, with corresponding latent feature values $\mathbf{h}=[\ldots, h_{t-1}, h_{t}]$. The causation $X\rightarrow Y$ is modeled as $Y=f(do(X))$ in the data space, where $f(\cdot)$ estimates the dependence between the sequence $\mathbf{x}$ and the value $y_{t+1}$. This dependence is denoted as $\mathbf{P}(y_{t+1}|\ \mathbf{x})$.
% Significantly, the causal chain $X\rightarrow Y$ can be decomposed as $\mathbf{x}\rightarrow \mathbf{h}\rightarrow v\rightarrow y$, where $\mathbf{h}$ is the sequence of feature values representing $X$ in the latent space, and $v$ is the feature value representing $Y$ but without the timestamp $t$. By reconstructing the observational dependence $\mathbf{P}(y_{t+1}|\ \mathbf{x})$ as a latent dependence $\mathbf{P}(v|\ \mathbf{h})$, we can model $X\rightarrow Y$ as the tuple $(v,\mathbf{h},f)$ in the latent space. Here, $f(\cdot)$ is the \emph{Latent Causal Effect} model, which models $\mathbf{P}(v|\ \mathbf{h})$ in the $L$-dimensional latent space.


% The ultimate goal of CRL is to build valid Structural Equation Models (SEMs) in order to construct the do-DAG in latent space. One way to achieve this is by "stacking" representations, which involves combining two or more latent dependencies to simulate the posterior distribution. For example, consider the causation $X\rightarrow Y \rightarrow Z$. We may be interested in the indirect causal effect from $X$ to $Z$ via $Y$, decomposed as $\mathbf{x}\rightarrow \mathbf{h}\rightarrow v\rightarrow k\rightarrow z$, where the latent vector $k$ represents the observed value $z_{t+1}$ of $Z$. Our goal is to estimate the posterior $\mathbf{P}(k|\ \mathbf{h})=\mathbf{P}(k|v)\big|_{v=f(\mathbf{h})}$, where $f$ is the latent causal effect model that maps $\mathbf{h}$ to $v$. Given $f$, we only need to estimate the latent model $g(\cdot)$ such that $k=g(v)\big|_{v=f(\mathbf{h})}$, by stacking the target dependence $\mathbf{P}(k|v)$ onto the modeled dependence $\mathbf{P}(v|\ \mathbf{h})$.
% However, it is important to note that training the model $g(\cdot)$ to estimate $k=g(\mathbf{v})$ and $k=g(v)\big|_{v=f(\mathbf{h})}$ are two different processes, as they involve different input data streams. From the perspective of an autoencoder, the former corresponds to $P(v) \mapsto P(k)$, while the latter corresponds to $P(v|h) \mapsto P(k)$.

% In this section, we present our proposed architecture for extracting higher-dimensional representations (\ref{sec:method_1}) and describe the design of its critical layers (\ref{sec:method_2}). We also discuss the methods we use for learning latent causal effects and stacking representations (\ref{sec:method_3}), as well as introduce our causal discovery algorithm based on do-DAG in latent space (\ref{sec:method_4}).


\subsection{Designing Higher-Dimensional Feature Representation Autoencoders}
\label{sec:method_1}
\vspace{-1mm}

Autoencoders, primarily intended for dimensionality reductions \cite{wang2016auto}, commonly treat all variables (i.e., nodes in the Directed Acyclic Graph, DAG) as aligned observations to reduce attribute matrix dimensionality in structural modeling \cite{luo2020causal}. 
However, our focus is on modeling individual relations and ``stacking'' their representations to construct a DAG within the latent space $\mathbb{R}^L$. This demands a large dimensionality for $\mathbb{R}^L$ to accommodate all potential hierarchical features and to sequentially build $L$-dimensional node representations in the DAG, hence presenting a substantial technical challenge in facilitating \emph{high-dimensional feature representations}.



%\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.94\textwidth}
\paragraph{Corollary 1.}Given a graph $G$ and a data matrix $\mathbf{X}$ column-augmented with all observational attributes of variables in $G$, along with a column for timestamps (i.e., the absolute timeline), the latent space dimensionality $L$ must be at least as large as $rank(\mathbf{X})$ to adequately represent $G$.
\end{minipage}}
\vspace{1mm}

Corollary 1 stems from the notion that the autoencoder-learned $\mathbb{R}^L$ is spanned by $\mathbf{X}$'s top principal components, often referred to as Principal Component Analysis (PCA) \cite{baldi1989neural, plaut2018principal, wang2016auto}. Hypothetically, reducing $L$ below $rank(\mathbf{X})$ could yield a less comprehensive but causally more significant latent space through better alignment \cite{jain2021mechanism}, although further exploration is needed.
%This approach has been widely used in natural language processing (NLP) for identifying word analogies \cite{pennington2014glove, rong2014word2vec}, and in bioinformatics for embedding gene expression \cite{belyaeva2021causal, lotfollahi2019scgen}. Further investigation is required to uncover the underlying connections between these applications.
In this study, we will set aside discussions on the boundaries of dimensionality. Our experiments feature 10 variables with dimensions 1 to 5 (Table~\ref{tab:tower}), and we empirically fine-tune and reduce $L$ from 64 to 16.



% Figure environment removed

Figure~\ref{fig:arch} depicts the proposed autoencoder architecture, which employs symmetrical \emph{Encrypt} and \emph{Decrypt} layers at the input and output, respectively. The Encrypt layer acts as an amplifier, expanding the input vector $\overrightarrow{x}$ by extracting its higher-order intrinsic features. Conversely, the Decrypt layer, functioning as a symmetric reducer, restores $\overrightarrow{x}$ to its original form. To ensure reconstruction accuracy, the invertibility of these operations is naturally required.
Figure~\ref{fig:arch} illustrates a \emph{double-wise} feature expansion. In this method, each pair of \emph{two} digits from $\overrightarrow{x}$ is encoded into a new digit, thus capturing their association. This is accomplished using a \emph{Key}, a set of constants created by the encoder and mirrored by the decoder for reverse decryption. The application of a double-wise expansion \emph{Key} on $\overrightarrow{x}\in \mathbb{R}^d$ generates a $(d-1)(d-1)$ length vector. By utilizing multiple \emph{Keys} and augmenting the vectors they produce, $\overrightarrow{x}$ can be significantly extended beyond its original length $d$. The four differently patterned squares in Figure~\ref{fig:arch} represent the results of four distinct \emph{Keys}. Each square visualizes a $(d-1)(d-1)$ length vector (not suggesting 2-dimensionality), with the patterned grids indicating each \emph{Key}'s unique ``signature''.
As an analogy, higher-order extensions such as \emph{triple-wise} ones across every three digits can also be employed, by appropriately adapting the \emph{Key} to encapsulate more intricate associations within the data.

Figure~\ref{fig:extractor} illustrates the Encrypt and Decrypt functions executing a double-wise expansion. These processes transform a digit pair $(x_i, x_j)$, $i\neq j \in 1,\ldots,d$, via encryption $f_\theta(x_i,x_j)$, with $\theta=(w_s,w_t)$ as the \emph{Key} comprising two weights defining elementary functions $s(\cdot)$ and $t(\cdot)$. Specifically, $f_\theta(x_i,x_j) = x_j \otimes exp(s(x_i)) + t(x_i)$ is applied to each digit pair, transforming $x_j$ into a new digit $y_j$ using $x_i$ as a parameter.
The Decrypt layer uses the symmetric inverse function $f_\theta^{-1}$, defined as $(y_j-t(y_i)) \otimes exp(-s(y_i))$. Importantly, this calculation sidesteps the need for $s^{-1}$ or $t^{-1}$, permitting both linear and non-linear transformations. 
With the set of all $f_\theta$ functions denoted as $\mathcal{F}(X; \vartheta)$ - where $X$ is the input variable and $\vartheta$ comprises all parameters - the Encrypt and Decrypt layers can be represented as $Y=\mathcal{F}(X; \vartheta)$ and $X = \mathcal{F}^{-1}(Y; \vartheta)$, respectively.
Drawing from the seminal work of Dinh et al. \cite{dinh2016density} on invertible neural network layers, we employ bijective functions to design our autoencoder. We specifically use the double-wise extension function $f_\theta(x_i,x_j)$, operating on digit pairs, thus preserving reconstruction accuracy. This bijective foundation ensures our architecture's robustness and adaptability, tailoring extension levels to application requirements. The source code for Encrypt and Decrypt is provided\footnote{https://github.com/kflijia/bijective\_crossing\_functions/blob/main/code\_bicross\_extracter.py}, along with a comprehensive experimental demo.


% Figure environment removed



%\vspace{-2.7mm}
% Figure environment removed

\subsection{Structural Model with Hierarchical Representations}
\label{sec:method_3}
\vspace{-1mm}

Consider a causal system comprising three variables: $\{\mathcal{X}, \mathcal{Y}, \mathcal{Z}\}$. For each, a corresponding representation $\{\mathcal{H}, \mathcal{V}, \mathcal{K}\} \in \mathbb{R}^L$ is generated via independent autoencoders with the aforementioned architecture. Figure \ref{fig:bridge} portrays the process of connecting $\mathcal{H}$ and $\mathcal{V}$ to represent the relation $\mathcal{X}\rightarrow \mathcal{Y}$, while Figure~\ref{fig:stack} illustrates stacking these relations to represent the entire causal system, thereby enabling a hierarchical representation.

% be an instance of variable $\mathcal{X}$, represented as feature $h\in \mathbb{R}^L$ in the latent space. Their joint distribution $P(x,h)$ can be decomposed as $P(x|h)P(h)$, with $P(h)$ as the \emph{prior} and $P(x|h)$ being the \emph{likelihood}.


Assume $x$ and $y$ as instances of the relation $\mathcal{X}\rightarrow \mathcal{Y}$, with corresponding latent representations $h$ and $v$. We utilize an RNN model to estimate the latent dependency $\mathbf{P}(v| h)$ as displayed in Figure \ref{fig:bridge}. The training process involves three simultaneous optimizations per iteration:
\begin{enumerate}[topsep=0pt, partopsep=0pt]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{1pt}
    \item {Optimizing encoder $\mathbf{P}(h|x)$, RNN model $\mathbf{P}(v|h)$, and decoder $\mathbf{P}(y|v)$ to reconstruct the effect $x\rightarrow y$.}
    \item {Fine-tuning encoder $\mathbf{P}(v|y)$ and decoder $\mathbf{P}(y|v)$ to accurately represent $y$.} 
    \item {Fine-tuning encoder $\mathbf{P}(h|x)$ and decoder $\mathbf{P}(x|h)$ to accurately represent $x$.}
\end{enumerate}
%\vspace{1mm}
Throughout the learning, $h$ and $v$ values are iteratively refined to minimize their latent space distance, and the RNN functions act as a bridge to traverse this distance, thereby estimating the causal effect $x\rightarrow y$.



% Initially, the latent variables $\mathcal{H}$ and $\mathcal{V}$ are obtained as two independent feature vectors in the latent space $\mathcal{R}^L$ using the proposed autoencoder architecture. During the learning of $f$, these latent variables are optimized by gradually adjusting their feature values to minimize their distance and come closer to each other. The function $f$ acts as a bridge that connects the two variables by modeling their causal relationship.
% To be specific, optimization of $\mathcal{H}$ is achieved by adjusting the feature values of $\mathcal{H}$ in the latent space $\mathcal{R}^L$ to minimize the distance between the reconstructed $X$ and the original $X$. This optimization step is one of the three in each iteration of the two-autoencoder architecture depicted in Figure \ref{fig:bridge}.

% A complete iteration includes three optimization steps:


% The process of "stacking" two representations for a single variable is adjusting its feature vector values until it simultaneously represents two different roles. In Figure \ref{fig:bridge}, the learning process of $f$ can be seen as two stacking operations: one stacks $X$ with $\overrightarrow{XY}$ by changing the value of initialized $\mathcal{H}$, and the other stacks $Y$ with $\overrightarrow{XY}$ by changing the value of initialized $\mathcal{V}$. The initialization process of $\mathcal{H}$ and $\mathcal{V}$ is to establish them as the representations of $X$ and $Y$, respectively.



Figure~\ref{fig:stack} presents two stacking scenarios for $\mathcal{Y}$ in the three-variable causal system comprising $\{\mathcal{X}, \mathcal{Y}, \mathcal{Z}\}$, based on different causal direction settings.
% where two edges' representations are stacked among three variables.
% From the perspective of the autoencoder, all data streams flow through to describe distributions, and no need to identify any do-operations like in RNN modeling. Hence only the single-value notations are used here without denoting any sequences.
For the established latent edge $\overrightarrow{XY}$, the left-side architecture completes the $X\rightarrow Y\leftarrow Z$ relationship, while the right-side caters to $X\rightarrow Y\rightarrow Z$. Stacking is achieved by adding an extra representation layer, thereby forming a hierarchical structure, enabling diverse input-output combinations (denoted as $\mapsto$).  %rolled back by removing relevant layer(s) from the autoencoder.
%Multiple representation layers in an autoencoder provide a high degree of flexibility for realizing various input and output distributions. 
For example, in the left setup, $\mathbf{P}(v|h) \mapsto \mathbf{P}(\alpha)$ signifies the $X\rightarrow Y$ relationship, while $\mathbf{P}(\alpha|k)$ implies $Z\rightarrow Y$. Conversely, the right setup has $\mathbf{P}(v) \mapsto P(\beta|k)$ representing $Y\rightarrow Z$ with $Y$ as input and $\mathbf{P}(v|h) \mapsto P(\beta|k)$ denoting the $X \rightarrow Y\rightarrow Z$ relationship.

Causal effects of edges can be sequentially stacked based on known causal Directed DAGs by leveraging domain knowledge. Additionally, this method can facilitate causal structure discovery in the latent space, identifying potential edges among the initial representations of the variables.



%\vspace{-2mm}
% Figure environment removed


\subsection{Causal Discovery in Latent Space}
\label{sec:method_4}
\vspace{-2mm}

%Suppose we have established a set of feature vectors in the latent space $\mathcal{R}^{L}$ to represent the corresponding data variables. 
Algorithm 1 details the heuristic process of discovering causal edges among the initially established variable representations. It employs the Kullback-Leibler Divergence (KLD) as a metric to assess causal relationship strength. Specifically, KLD measures the similarity between the $RNN$'s output, $\mathbf{P}(v|h)$, and the prior $\mathbf{P}(v)$, as depicted in Figure~\ref{fig:bridge}.
A lower KLD signifies a stronger causal relationship, given its closer alignment with the ground truth. Though Mean Squared Error (MSE) is a conventional evaluation metric, considering it may be influenced by data variances \cite{reisach2021beware, kaiser2021unsuitability}, we primarily employ KLD as the criterion and use MSE as a supplementary metric. For clarity, in the graphical context, for edge $A\rightarrow B$, we refer to variables $A$ and $B$ as the \emph{cause node} and \emph{result node}, respectively.

Figure~\ref{fig:discover} presents an exemplification of the causal structure discovery process within the latent space. Across four steps, two edges ($e_1$ and $e_3$) are successively selected. The selection of $e_1$ establishes node B as the starting point for $e_3$. In step 3, the causal effect of $e_2$ from $A$ to $C$ is deselected from the potential edges and re-evaluated. This is due to the introduction of edge $e_3$ to $C$, modifying $C$'s existing causal conditions. As the procedure unfolds, the ultimately discovered causal structure is represented by the final DAG.


\begin{minipage}{\textwidth}
    \begin{minipage}[b]{0.52\textwidth}
    \raggedright
    \begin{algorithm}[H]
    \footnotesize
    \SetAlgoLined
    \KwResult{ ordered edges set $\mathbf{E}=\{e_1, \ldots,e_n\}$ }
    $\mathbf{E}=\{\}$ ; $N_R = \{ n_0 \mid n_0 \in N, Parent(n_0)=\varnothing\}$ \;
    \While{$N_R \subset N$}{
    $\Delta = \{\}$ \;
    \For{ $n \in N$ }{
        \For{$p \in Parent(n)$}{
            \If{$n \notin N_R$ and $p \in N_R$}{
                $e=(p,n)$; \ 
                $\beta = \{\}$;\\ 
                \For{$r \in N_R$}{
                    \If{$r \in Parent(n)$ and $r \neq p$}{
                        $\beta = \beta \cup r$}
                    }
                $\delta_e = K(\beta \cup p, n) - K(\beta, n)$;\\
                $\Delta = \Delta \cup \delta_e$;
            }
        }
    }
    $\sigma = argmin_e(\delta_e \mid \delta_e \in \Delta)$;\\
    $\mathbf{E} = \mathbf{E} \cup \sigma$; \ 
    $N_R = N_R \cup n_{\sigma}$;\\
    }
     \caption{Latent Space Causal Discovery}
    \end{algorithm}
    \end{minipage}
%\hfill 
    \begin{minipage}[b]{0.4\textwidth}
    \raggedleft
        \label{tab:table1}
        \begin{tabular}{|l|l|}
        \hline
           $G=(N,E)$ & graph $G$ consists of $N$ and $E$ \\
           $N$ & the set of nodes\\
           $E$ & the set of edges\\
           $N_R$ & the set of reachable nodes\\
           $\mathbf{E}$ & the list of discovered edges\\
           $K(\beta, n)$ & KLD metric of effect $\beta\rightarrow n$\\
           $\beta$ & the cause nodes \\
           $n$ & the result node\\
           $\delta_e$ & KLD Gain of candidate edge $e$\\
           $\Delta=\{\delta_e\}$ & the set $\{\delta_e\}$ for $e$ \\
           $n$,$p$,$r$ & notations of nodes\\
           $e$,$\sigma$ & notations of edges\\
          \hline
        \end{tabular} %
%       \captionof{table}{A table beside a figure}
    \end{minipage}
\end{minipage}




% \begin{table}[h!]
% %\caption{Algorithm Notation List }
% \vspace{-6mm}
% %\label{tab:notation}
%   \begin{center}
%     \label{tab:table1}
%     \resizebox{0.7\columnwidth}{!}{%
%     \begin{tabular}{|l|l|}
%     \hline
%        $G=(N,E)$ & graph $G$ consists of nodes set $N$ and edges set $E$\\
%        $N_R$ & the set of reachable nodes\\
%        $\mathbf{E}$ & edges in order of being discovered\\
%        $K(\beta, n)$ & KLD metric between the causes set $\beta$ and effect node $n$\\
%        $\Delta=\{\delta_e\}$ & the set of all KLD Gain $\delta_e$ for each candidate edge $e$\\
%        $n$,$p$,$r$ & notation of node\\
%        $e$,$\sigma$ & notation of edge\\
%       \hline
%     \end{tabular} %
%     }
%   \end{center}
% \vspace{-4mm}
% \end{table}


\vspace{3mm}
\section{Experiments}
\label{sec:experiment}

The experiments aim to validate the efficacy of the proposed \emph{Relation-Oriented} modeling methods in: 1) creating high-dimensional feature representations using our autoencoder architecture, 2) constructing latent effects and stacking them for hierarchical representation, and 3) latent space causal structure discovery.

We employ a synthetic hydrology dataset for the experiments, a prevalent resource in hydrology. The task involves predicting streamflow based on observed environmental conditions like temperature and precipitation. By using relation-defined representation learning on this hydrology data, we aim to construct generalizable causal models across diverse watersheds. %The intent is to calibrate a model established on dataset $X_1$ from watershed No.1 to fit dataset $X_2$ from watershed No.2. 
Despite similarities in hydrological schemes, differences in unmeasurable conditions such as economic developments and land use complicate direct model application. Current physical knowledge-based models, however, are often constrained by limited parameters, which restricts their flexibility in capturing complex relationships within data.

To assess the model's robustness and generalizability, Electronic Health Records (EHR) data would have been an ideal choice, given their rich confounding relationships across multiple timelines. However, due to empirical restrictions, we lost access to the EHR data during this study. To confirm the existence of inherent temporal bias, we direct readers to the previous work \cite{li2020teaching}. A comprehensive demonstration of the experiments in this study can be found in the provided repository\footnote{https://github.com/kflijia/bijective\_crossing\_functions.git}.



% The feasibility experiments do not assess the effectiveness of generalizing established models across different watersheds because the data's long causal chains make it unsuitable for generalizations. However, we plan to explore generalization and individualization in future work. Although EHR data could be a good choice for this purpose, we did not have access to it during this study due to empirical restrictions. The complete experimental demonstration of the feasibility experiments is provided \footnote
% {https://github.com/kflijia/bijective\_crossing\_functions.git}. 



\vspace{-2mm}
% Figure environment removed




\vspace{-1mm}
\subsection{Hydrology Dataset}
\vspace{-2mm}


Our experiments leverage the Soil and Water Assessment Tool (SWAT), a comprehensive hydrology data simulation system rooted in physical modules. We use SWAT's simulation of the Root River Headwater watershed in Southeast Minnesota, selecting 60 consecutive virtual years with daily updates. The performance evaluations predominantly focus on the accuracy of the autoencoder reconstructions.

In hydrology, deep learning methodologies are frequently employed \cite{goodwell2020debates} to distill effective representations from time series data, with RNN models emerging as a favored choice for streamflow prediction \cite{kratzert2018rainfall}. Figure \ref{fig:stream} illustrates the causal DAG used by SWAT, with accompanying node descriptions. The nodes signify different hydrological routines, with the intensity of causality between them determined by their contribution to the output streamflow, denoted by various colors. The surface runoff routine (1st tier causality) plays a significant role in causing swift streamflow peaks, followed by the lateral flow routine (2nd tier causality). The baseflow dynamics (3rd tier causality) exert a more subtle influence. In our causal discovery experiments, we aim to uncover these ground truths from the observed data.


%\vspace{-2mm}
\subsection{Higher-Dimensional Representation Reconstruction Test}
\vspace{-2mm}

As depicted in Figure~\ref{fig:stream}, there are 10 nodes needing initial  representation establishment. Table \ref{tab:tower} displays the statistics of their attributes (post-normalization), and reconstruction performance using the proposed high-dimensional feature representation autoencoders. Accuracy is evaluated via root mean square error (RMSE); lower RMSE equates to higher accuracy, on both scaled (i.e., normalized) and unscaled data.

The task poses challenges due to the exceedingly low dimensionality of the 10 variables, with a maximum of just 5 and the target node, $J$, possessing a single attribute. To counter this, we duplicate their columns to achieve a uniform 12-dimensionality, supplemented by the dummy variables of the 12 months, yielding a 24-dimensional autoencoder input. Through a double-wise feature extension, we generate a 576-dimensional amplified input, from which we extract a 16-dimensional representation via the encoder and decoder.

Significant challenges also arise from considerable meaningful-zero values. For example, node $D$ (Snowpack in winter) includes numerous zeros in other seasons, closely related to node $E$ (Soil Water) values. We address this by concurrently reconstructing non-zero indicator variables, named masks, within the autoencoder, evaluated using binary cross entropy (BCE). %The RMSE performances listed in the table are obtained by multiplying these two results.

Despite these challenges, the shallow RMSE values within $[0.01, 0.09]$ suggest success, barring node $F$ (the Aquifer). Considering that research into the physical schemes under the aquifer system is still in its infancy, it is plausible that in this synthetic dataset, node $F$ is more representative of random noise than other nodes.


\begin{table*}[t]
\caption{Statistics of Attributes and the Reconstruction Performances.}
\label{tab:tower}
\vspace{-3mm}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{|
>{\columncolor[HTML]{EFEFEF}}c |c|l|l|l|l|l|l|l|l|}
\hline
\cellcolor[HTML]{C0C0C0}Variable & \cellcolor[HTML]{C0C0C0} Dim & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Mean} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Std} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Min} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Max} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Non-Zero Rate\%} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}RMSE on Scaled} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}RMSE on Unscaled} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}BCE of Mask} \\ \hline
A & 5 & 1.8513 & 1.5496 & -3.3557 & 7.6809 & 87.54 & 0.093 & 0.871 & 0.095 \\ \hline
B & 4 & 0.7687 & 1.1353 & -3.3557 & 5.9710 & 64.52 & 0.076 & 0.678 & 1.132 \\ \hline
C & 2 & 1.0342 & 1.0025 & 0.0 & 6.2145 & 94.42 & 0.037 & 0.089 & 0.428 \\ \hline
D & 3 & 0.0458 & 0.2005 & 0.0 & 5.2434 & 11.40 & 0.015 & 0.679 & 0.445 \\ \hline
E & 2 & 3.1449 & 1.0000 & 0.0285 & 5.0916 & 100 & 0.058 & 3.343 & 0.643 \\ \hline
F & 4 & 0.3922 & 0.8962 & 0.0 & 8.6122 & 59.08 & 0.326 & 7.178 & 2.045 \\ \hline
G & 4 & 0.7180 & 1.1064 & 0.0 & 8.2551 & 47.87 & 0.045 & 0.81 & 1.327 \\ \hline
H & 4 & 0.7344 & 1.0193 & 0.0 & 7.6350 & 49.93 & 0.045 & 0.009 & 1.345 \\ \hline
I & 3 & 0.1432 & 0.6137 & 0.0 & 8.3880 & 21.66 & 0.035 & 0.009 & 1.672 \\ \hline
J & 1 & 0.0410 & 0.2000 & 0.0 & 7.8903 & 21.75 & 0.007 & 0.098 & 1.088 \\ \hline
\end{tabular}}
\end{table*}



\vspace{-2mm}
\begin{table*}[t]
\caption{Brief Summary of the Latent Causal Discovery Results. }
\label{tab:discv}
\vspace{-3mm}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{|c|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
\cellcolor[HTML]{C0C0C0}Edge & \cellcolor[HTML]{FFCCC9}A$\rightarrow$C & \cellcolor[HTML]{FFCCC9}B$\rightarrow$D & \cellcolor[HTML]{FFCCC9}C$\rightarrow$D & \cellcolor[HTML]{FFCCC9}C$\rightarrow$G & \cellcolor[HTML]{FFCCC9}D$\rightarrow$G & \cellcolor[HTML]{FFCCC9}G$\rightarrow$J & \cellcolor[HTML]{FFCCC9}D$\rightarrow$H & \cellcolor[HTML]{FFCCC9}H$\rightarrow$J & \cellcolor[HTML]{FFCE93}B$\rightarrow$E & \cellcolor[HTML]{FFCE93}E$\rightarrow$G & \cellcolor[HTML]{FFCE93}E$\rightarrow$H & \cellcolor[HTML]{FFCE93}C$\rightarrow$E & \cellcolor[HTML]{ABE9E7}E$\rightarrow$F & \cellcolor[HTML]{ABE9E7}F$\rightarrow$I & \cellcolor[HTML]{ABE9E7}I$\rightarrow$J & \cellcolor[HTML]{ABE9E7}D$\rightarrow$I \\ \hline
\cellcolor[HTML]{C0C0C0}KLD & 7.63 & 8.51 & 10.14 & 11.60 & 27.87 & 5.29 & 25.19 & 15.93 & 37.07 & 39.13 & 39.88 & 46.58 & 53.68 & 45.64 & 17.41 & 75.57 \\ \hline
\rowcolor[HTML]{FFFFFF} 
\cellcolor[HTML]{C0C0C0}Gain & 7.63 & 8.51 & 1.135 & 11.60 & 2.454 & 5.29 & 25.19 & 0.209 & 37.07 & -5.91 & -3.29 & 2.677 & 53.68 & 45.64 & 0.028 & 3.384 \\ \hline
\end{tabular}}
\vspace{-3mm}
\end{table*}



\subsection{Latent Causal Effects Learning Test}
\vspace{-2mm}

Table \ref{tab:unit} shows the results of the latent effect learning, organized by each result node. For convenience, the pairwise relationship performances are referred to as ``pair-effect'', and the hierarchical multi-level performances as ``stacking-effect''.
To facilitate comparison, the baseline performances from the initial variable representation (Table \ref{tab:tower}) are also included. During latent effect estimation, each result node fulfills two roles: preserving an accurate self-representation (optimization 2), and reconstructing the effect (optimization 1). These dual roles are respectively depicted in the middle and right-hand side of Table \ref{tab:tower}.


% For example, node $G$ has three possible causes $C$, $D$, and $E$, so we built four causal effect models in total: the individual pairwise causations $C\rightarrow G$, $D\rightarrow G$, $E\rightarrow G$, and the stacked causal effect $CDE\rightarrow G$. For convenience, we refer to them as "\emph{pair-effect}" and "\emph{stacking-effect}" respectively. We also listed the initial reconstruction performance for each variable in the column named "Variable Reconstruction (initial performance)" as a comparison baseline in Table \ref{tab:unit}.


%\vspace{-3mm}
% Figure environment removed

% To evaluate the latent causal effect learning process, we have three different optimization tasks, according to the three steps, among which both the \emph{second} and \emph{third} ones involve the role of the \emph{result} node. 
% Therefore, in Table \ref{tab:unit}, we provide the performance of each node for both roles. The column named "Variable Reconstruction (as result node)" represents the performance of the \emph{result} node in the \emph{third} optimization task, while the columns named "Latent Causal Effect Reconstruction" represent the causal effect learning performance of RNN models for the \emph{second} optimization task, along with the KLD metrics to display the learned causality strength (the lower value indicates stronger causality). 

The KLD metrics in Table \ref{tab:unit} indicate the strength of learned causality, with a lower value signifying a stronger causal relationship. For instance, node $J$'s minimal KLD values suggest a significant causal effect from nodes $G$ (Surface Runoff), $H$ (Lateral), and $I$ (Baseflow). In contrast, the high KLD values imply that predicting variable $I$ using $D$ and $F$ is challenging. 

For nodes $D$, $E$, and $J$, the stacking-effect causal strengths hover at a middle range compared to their pair-effects, suggesting a potential associative uninformative among their cause nodes. In contrast, for nodes $G$ and $H$, lower stacking-effect KLDs indicate effective capture of associations by hierarchical representations. The KLD metric also unveils the most contributive cause node to the causal effect. For instance, the $C\rightarrow G$ strength being closer to $CDE\rightarrow G$ indicates $C$ as the primary source of this causal effect.

Figure~\ref{fig:G} showcases time series simulations of nodes $J$, $G$, and $I$, in the same synthetic year, to provide a straightforward overview of the hierarchical representation performances. %Their initial variable representation reconstructed data are plotted as blue lines, with the ground truth shown in black dots, and red lines displaying simulations from the hierarchically stacked effects representations.
%In addition to RMSE, we also use the NashSutcliffe model efficiency coefficient (NSE), a hydrology-meaningful accuracy evaluation metric, which ranges from -$\infty$ to 1, with 1 indicating the perfect prediction or simulation.
Here, blue lines represent reconstructed data, black dots the ground truth, and red lines hierarchical representations. We employ not only RMSE but also the NashSutcliffe model efficiency coefficient (NSE) for accuracy evaluation, which ranges from -$\infty$ to 1.
The reconstructions closely mirror the ground truth, and as anticipated, the stacking-effect outperforms the pair-effect in Figure~\ref{fig:G}. Although node $J$ has the best prediction, node $I$ proves challenging. For node $G$, which is predicted from causes $CDE$, $C$ offers the most potent causality.


One might observe via the demo that our experiments do not show smooth information flows along successive long causal chains. Given that RNNs are designed primarily for capturing the dynamics of causes rather than the effects, relying on them to spontaneously organize the effects' dynamical representations might prove unreliable. It underscores a significant opportunity for enhancing effectiveness by improving the architecture.



%\vspace{-1mm}
\subsection{Latent Space Causal Discovery Test}
%\vspace{-1mm}
Table \ref{tab:discv} shows the order of discovered edges, with the KLD values after each edge's inclusion, and respective KLD gains. Cells follow the color-coding scheme from Figure \ref{fig:stream}, representing different tiers of causal routines. For a detailed look at the causal discovery process, see \ref{tab:discv_rounds}, which presents sorted detection rounds. For comparison, we conducted a 10-fold cross-validation using the conventional FGES method; results can be found in Appendix A Table \ref{tab:fges}. The proposed method markedly outperforms the traditional FGES approach.


%\vspace{-3mm}
\begin{table*}[t]
\caption{Performances of Latent Causal Effect Learning via Reconstructions.}
\label{tab:unit}
%\vspace{-3mm}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{|l|lll|l|lll|llll|}
\hline
\rowcolor[HTML]{C0C0C0} 
\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} } & \multicolumn{3}{l|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \begin{tabular}[c]{@{}l@{}}\ \ \ \ \ \ Variable Representation\\ \ \ \ \ \ \ \ (Initial)\end{tabular}}} & \cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} } & \multicolumn{3}{l|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000}  \begin{tabular}[c]{@{}l@{}}\ \ \ \ \ \ Variable Representation\\ \ \ \ \ \ \ \ (in Effect Learning)\end{tabular}}} & \multicolumn{4}{l|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \ \ \ \ \ \ \ \ \ Latent Causal Effect Reconstruction}} \\ \cline{2-4} \cline{6-12} 
\rowcolor[HTML]{C0C0C0} 
\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} } & \multicolumn{2}{l|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000}\ \ \ \ \ \  \ \ \ \ RMSE}} & {\color[HTML]{000000} BCE} & \cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} } & \multicolumn{2}{l|}{\cellcolor[HTML]{C0C0C0}\ \ \ \ \ \ \ \ \ \ RMSE} & BCE & \multicolumn{2}{l|}{\cellcolor[HTML]{C0C0C0} \ \ \ \ \ \ \ \ \ RMSE} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}BCE} & KLD \\ \cline{2-4} \cline{6-12} 
\rowcolor[HTML]{C0C0C0} 
\multirow{-3}{*}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \begin{tabular}[c]{@{}l@{}}Result\\ Node\\ \ \end{tabular}}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Scaled\\ \ \ Values\end{tabular}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Unscaled\\ \ \ Values\end{tabular}} & Mask & \multirow{-3}{*}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \begin{tabular}[c]{@{}l@{}}Cause\\ Node\end{tabular}}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Scaled\\ \ \ Values\end{tabular}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Unscaled\\ \ \ Values\end{tabular}} & Mask & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Scaled\\ \ \ Values\end{tabular}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Unscaled\\ \ \ Values\end{tabular}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Mask} & \begin{tabular}[c]{@{}l@{}}(in latent\\ \ \ space)\end{tabular} \\ \hline
\cellcolor[HTML]{EFEFEF}C & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}0.037} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}0.089} & \cellcolor[HTML]{EFEFEF}0.428 & A & \multicolumn{1}{l|}{0.0295} & \multicolumn{1}{l|}{0.0616} & 0.4278 & \multicolumn{1}{l|}{0.1747} & \multicolumn{1}{l|}{0.3334} & \multicolumn{1}{l|}{0.4278} & 7.6353 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & BC & \multicolumn{1}{l|}{0.0350} & \multicolumn{1}{l|}{1.0179} & 0.1355 & \multicolumn{1}{l|}{0.0509} & \multicolumn{1}{l|}{1.7059} & \multicolumn{1}{l|}{0.1285} & 9.6502 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & B & \multicolumn{1}{l|}{0.0341} & \multicolumn{1}{l|}{1.0361} & 0.1693 & \multicolumn{1}{l|}{0.0516} & \multicolumn{1}{l|}{1.7737} & \multicolumn{1}{l|}{0.1925} & 8.5147 \\ \cline{5-5}
\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}D} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.015}} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.679}} & \multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.445} & C & \multicolumn{1}{l|}{0.0331} & \multicolumn{1}{l|}{0.9818} & 0.3404 & \multicolumn{1}{l|}{0.0512} & \multicolumn{1}{l|}{1.7265} & \multicolumn{1}{l|}{0.3667} & 10.149 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & BC & \multicolumn{1}{l|}{0.4612} & \multicolumn{1}{l|}{26.605} & 0.6427 & \multicolumn{1}{l|}{0.7827} & \multicolumn{1}{l|}{45.149} & \multicolumn{1}{l|}{0.6427} & 39.750 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & B & \multicolumn{1}{l|}{0.6428} & \multicolumn{1}{l|}{37.076} & 0.6427 & \multicolumn{1}{l|}{0.8209} & \multicolumn{1}{l|}{47.353} & \multicolumn{1}{l|}{0.6427} & 37.072 \\ \cline{5-5}
\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}E} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.058}} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}3.343}} & \multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.643} & C & \multicolumn{1}{l|}{0.5212} & \multicolumn{1}{l|}{30.065} & 1.2854 & \multicolumn{1}{l|}{0.7939} & \multicolumn{1}{l|}{45.791} & \multicolumn{1}{l|}{1.2854} & 46.587 \\ \hline
\cellcolor[HTML]{EFEFEF}F & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}0.326} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}7.178} & \cellcolor[HTML]{EFEFEF}2.045 & E & \multicolumn{1}{l|}{0.4334} & \multicolumn{1}{l|}{8.3807} & 3.0895 & \multicolumn{1}{l|}{0.4509} & \multicolumn{1}{l|}{5.9553} & \multicolumn{1}{l|}{3.0895} & 53.680 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & CDE & \multicolumn{1}{l|}{0.0538} & \multicolumn{1}{l|}{0.9598} & 0.0878 & \multicolumn{1}{l|}{0.1719} & \multicolumn{1}{l|}{3.5736} & \multicolumn{1}{l|}{0.1340} & 8.1360 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & C & \multicolumn{1}{l|}{0.1057} & \multicolumn{1}{l|}{1.4219} & 0.1078 & \multicolumn{1}{l|}{0.2996} & \multicolumn{1}{l|}{4.6278} & \multicolumn{1}{l|}{0.1362} & 11.601 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & D & \multicolumn{1}{l|}{0.1773} & \multicolumn{1}{l|}{3.6083} & 0.1842 & \multicolumn{1}{l|}{0.4112} & \multicolumn{1}{l|}{8.0841} & \multicolumn{1}{l|}{0.2228} & 27.879 \\ \cline{5-5}
\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}G} & \multicolumn{1}{l|}{\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}0.045}} & \multicolumn{1}{l|}{\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}0.81}} & \multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}1.327} & E & \multicolumn{1}{l|}{0.1949} & \multicolumn{1}{l|}{4.7124} & 0.1482 & \multicolumn{1}{l|}{0.5564} & \multicolumn{1}{l|}{10.852} & \multicolumn{1}{l|}{0.1877} & 39.133 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & DE & \multicolumn{1}{l|}{0.0889} & \multicolumn{1}{l|}{0.0099} & 2.5980 & \multicolumn{1}{l|}{0.3564} & \multicolumn{1}{l|}{0.0096} & \multicolumn{1}{l|}{2.5980} & 21.905 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & D & \multicolumn{1}{l|}{0.0878} & \multicolumn{1}{l|}{0.0104} & 0.0911 & \multicolumn{1}{l|}{0.4301} & \multicolumn{1}{l|}{0.0095} & \multicolumn{1}{l|}{0.0911} & 25.198 \\ \cline{5-5}
\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}H} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.045}} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.009}} & \multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}1.345} & E & \multicolumn{1}{l|}{0.1162} & \multicolumn{1}{l|}{0.0105} & 0.1482 & \multicolumn{1}{l|}{0.5168} & \multicolumn{1}{l|}{0.0097} & \multicolumn{1}{l|}{3.8514} & 39.886 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & DF & \multicolumn{1}{l|}{0.0600} & \multicolumn{1}{l|}{0.0103} & 3.4493 & \multicolumn{1}{l|}{0.1158} & \multicolumn{1}{l|}{0.0099} & \multicolumn{1}{l|}{3.4493} & 49.033 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & D & \multicolumn{1}{l|}{0.1212} & \multicolumn{1}{l|}{0.0108} & 3.0048 & \multicolumn{1}{l|}{0.2073} & \multicolumn{1}{l|}{0.0108} & \multicolumn{1}{l|}{3.0048} & 75.577 \\ \cline{5-5}
\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}I} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.035}} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.009}} & \multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}1.672} & F & \multicolumn{1}{l|}{0.0540} & \multicolumn{1}{l|}{0.0102} & 3.4493 & \multicolumn{1}{l|}{0.0948} & \multicolumn{1}{l|}{0.0098} & \multicolumn{1}{l|}{3.4493} & 45.648 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & GHI & \multicolumn{1}{l|}{0.0052} & \multicolumn{1}{l|}{0.0742} & 0.2593 & \multicolumn{1}{l|}{0.0090} & \multicolumn{1}{l|}{0.1269} & \multicolumn{1}{l|}{0.2937} & 5.5300 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & G & \multicolumn{1}{l|}{0.0077} & \multicolumn{1}{l|}{0.1085} & 0.4009 & \multicolumn{1}{l|}{0.0099} & \multicolumn{1}{l|}{0.1390} & \multicolumn{1}{l|}{0.4375} & 5.2924 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & H & \multicolumn{1}{l|}{0.0159} & \multicolumn{1}{l|}{0.2239} & 0.4584 & \multicolumn{1}{l|}{0.0393} & \multicolumn{1}{l|}{0.5520} & \multicolumn{1}{l|}{0.4938} & 15.930 \\ \cline{5-5}
\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}J} & \multicolumn{1}{l|}{\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}0.007}} & \multicolumn{1}{l|}{\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}0.098}} & \multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}1.088} & I & \multicolumn{1}{l|}{0.0308} & \multicolumn{1}{l|}{0.4328} & 0.3818 & \multicolumn{1}{l|}{0.0397} & \multicolumn{1}{l|}{0.5564} & \multicolumn{1}{l|}{0.3954} & 17.410 \\ \hline
\end{tabular}
}
\vspace{-3mm}
\end{table*}


\section{Conclusions}\label{sec:conclusion}
%\vspace{-3mm}

Our traditional thinking about AI often assumes that incorporating larger, more diverse datasets can lead to more substantial advances. While that indeed powers AI but also creates more significant misalignments that deviate from our knowledge and cognition. As \cite{christian2020alignment} highlights, AI alignment issues extend beyond technical issues, but point to humans' ``blind spots'' and `` unstated assumptions''.
We might not differentiate ourselves from AI systems based on our capacity to understand large contexts, but rather on how we perceive the individual relationship.

This study advocates for a ``\emph{Relation-Oriented}'' modeling principle, which could disrupt our prevalent \emph{Observation-Oriented} modeling convention and align more closely with the relation-centric nature of human comprehension.
To be specific, the knowledge nodes we construct in our cognition, are motivated by, and indexed through, relationships that confer them specific meanings under our comprehension context, rather than solely reflecting observations.
Through relations, our understanding can surpass observational limitations, spanning: \textbf{1)} hyper-dimensions that encompass unobservable knowledge hierarchies, and \textbf{2)} temporal spaces where we store dynamic events, governed by multiple logical timelines.

Additionally, this study presents a feasible \emph{Relation-Oriented} modeling technique, extracting relation-defined representations from observations, to instantiate the knowledge nodes in our understanding. AI alignment is never a question with a simple answer, but calls for our interdisciplinary efforts \cite{christian2020alignment}.
Through this work, we aim to pave the way toward truly authentic AI and lay the groundwork for future progress.

% In recent years, AI applications like ChatGPT have made impressive strides. However, we have yet to witness AI replace humans in making knowledgeable decisions, such as an AI doctor prescribing medication or an AI lawyer providing legal consultations.
% Indeed, AI regards all connections as associations, while the evolution of human knowledge is rooted in causality.
% Understanding causality requires an awareness of timelines, which is a fundamental difference between humans and AI.
% While humans can intuitively distinguish between causal effects and correlated changes, AI lacks this ability. 


% Causal inference theories develop based on directed acyclic graph (DAG) representing causal structures, without identifying observed relationships as causal or non-causal.
% While DAGs can serve as a model for our awareness of causality, they are not suitable for directing AI's learning. Instead, explicit isolation of timelines as axes is required to construct a multi-dimensional geometric space that enables AI to capture the causal structure.

% This paper introduces the do-DAG as a precise and rigorous specification of the necessary space. Our study thoroughly investigates the geometric implications of this concept, identifies the presence of inherent representation biases within the current causal learning framework, and proposes a general solution to address this issue. We aim for this work to pave the way for future research and advancements in causal AI.

%with the goal of identifying the fundamental obstacles that prevent AI from comprehending human knowledge. Furthermore, we 

%doctors in making prescription decisions or cracking the genetic code automatically, although they have much fewer computational complexities than learning tens of languages, computing pixels of movies, or evaluating billions of possibilities in the Go game.
%So, what sealed AI's capability when learning structural data?
%It seems like AI's capability is ``sealed'' on learning the causally structured data. 

\vspace{8mm}

\section*{Acknowledgements}
I want to extend my heartfelt thanks to my friend, Dr. Gao Qiman, the lone companion willing to engage in profound philosophical discussions with me, and who has provided invaluable advice. Furthermore, I express my gratitude to GPT-4 for its crucial aid in improving my English writing.

\hfill Jia Li


\vspace{30mm}

\bibliography{main}
\bibliographystyle{tmlr}


\appendix
\section{Appendix: Complete Experimental Results of Causal Discovery}
%\includepdf[pages=-]{mainz_append_pdf.pdf}

\pagebreak


%\newpage
\begin{sidewaystable}
%\begin{table*}[t]
\centering
\caption{The Complete Results of Heuristic Causal Discovery in latent space.
Each row stands for a round of detection, with `\#'' identifying the round number, and all candidate edges are listed with their KLD gains as below. 1) Green cells: the newly detected edges. 2) Red cells: the selected edge.
3) Blue cells: the trimmed edges accordingly. }
%\vspace{-20mm}
\label{tab:discv_rounds}
\resizebox{1\columnwidth}{!}{%
%\rotatebox{90}{

\begin{tabular}{|c|c|c|cccccccccccccc}
\cline{1-9}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \textbf{A$\rightarrow$ C}} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ D & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{B$\rightarrow$ C}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ D} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 1}} & \cellcolor[HTML]{FFCCC9}{\color[HTML]{000000} \textbf{7.6354}} & 19.7407 & \multicolumn{1}{c|}{60.1876} & \multicolumn{1}{c|}{119.7730} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{8.4753}} & \multicolumn{1}{c|}{8.5147} & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} &  &  &  &  &  &  &  &  \\ \cline{1-13}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ D & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{B$\rightarrow$ D}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ D} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 2}} & 19.7407 & 60.1876 & \multicolumn{1}{c|}{119.7730} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{8.5147}}} & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}10.1490} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}46.5876} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}111.2978} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}11.6012} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}39.2361} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}95.1564} &  &  &  &  \\ \hline
\rowcolor[HTML]{C0C0C0} 
\cellcolor[HTML]{C0C0C0} & \textbf{A$\rightarrow$ D} & A$\rightarrow$ E & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{C$\rightarrow$ D}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 3}} & \cellcolor[HTML]{CBCEFB}\textbf{9.7357} & 60.1876 & \multicolumn{1}{c|}{119.7730} & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{1.1355}}} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{11.6012} & \multicolumn{1}{c|}{39.2361} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}63.7348} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}123.3203} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}27.8798} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}25.1988} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}75.5775} \\ \hline
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \textbf{C$\rightarrow$ G}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 4}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{000000} \textbf{11.6012}}} & \multicolumn{1}{c|}{39.2361} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{27.8798} & \multicolumn{1}{c|}{25.1988} & \multicolumn{1}{c|}{75.5775} &  &  \\ \cline{1-15}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{D$\rightarrow$ G}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}G$\rightarrow$ J} &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 5}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{39.2361} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{2.4540}}} & \multicolumn{1}{c|}{25.1988} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}5.2924} &  &  \\ \cline{1-15}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{G$\rightarrow$ J}}} &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 6}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{39.2361} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{25.1988} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{5.2924}}} &  &  &  \\ \cline{1-14}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{C$\rightarrow$ H}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{D$\rightarrow$ H}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 7}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{39.2361}} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{25.1988}}} & \multicolumn{1}{c|}{75.5775} &  &  &  &  \\ \cline{1-13}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{H$\rightarrow$ J}}} &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 8}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{0.2092}}} &  &  &  &  &  \\ \cline{1-12}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}\textbf{A$\rightarrow$ E} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{C$\rightarrow$ E}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 9}} & \cellcolor[HTML]{CBCEFB}\textbf{60.1876} & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{46.5876}}} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} &  &  &  &  &  &  \\ \cline{1-13}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{B$\rightarrow$ E}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{D$\rightarrow$ E}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ I} &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 10}} & 119.7730 & \cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{-6.8372}} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{17.0407}} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}53.6806} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}-5.9191} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}-3.2931} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}110.2558} &  &  &  &  \\ \cline{1-13}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \cellcolor[HTML]{C0C0C0}B$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{E$\rightarrow$ G}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ I} &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 11}} & 119.7730 & 132.7717 & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{53.6806} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{-5.9191}} & \multicolumn{1}{c|}{-3.2931} & \multicolumn{1}{c|}{110.2558} &  &  &  &  &  &  \\ \cline{1-11}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \cellcolor[HTML]{C0C0C0}B$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{E$\rightarrow$ H}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ I} &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 12}} & 119.7730 & 132.7717 & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{53.6806} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{-3.2931}} & \multicolumn{1}{c|}{110.2558} &  &  &  &  &  &  &  \\ \cline{1-10}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}\textbf{A$\rightarrow$ F} & \cellcolor[HTML]{C0C0C0}\textbf{B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{C$\rightarrow$ F}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{D$\rightarrow$ F}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{E$\rightarrow$ F}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ I} &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 13}} & \cellcolor[HTML]{CBCEFB}\textbf{119.7730} & \cellcolor[HTML]{CBCEFB}\textbf{132.7717} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{111.2978}} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{123.3203}} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{53.6806}} & \multicolumn{1}{c|}{110.2558} &  &  &  &  &  &  &  &  \\ \cline{1-9}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}C$\rightarrow$ I & \cellcolor[HTML]{C0C0C0}D$\rightarrow$ I & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{E$\rightarrow$ I}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{F$\rightarrow$ I}} &  &  &  &  &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 14}} & 95.1564 & 75.5775 & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{110.2558}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{45.6490}} &  &  &  &  &  &  &  &  &  &  &  &  \\ \cline{1-5}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}C$\rightarrow$ I & \cellcolor[HTML]{C0C0C0}D$\rightarrow$ I & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{I$\rightarrow$ J}} &  &  &  &  &  &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 15}} & 15.0222 & 3.3845 & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.0284}} &  &  &  &  &  &  &  &  &  &  &  &  &  \\ \cline{1-4}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}\textbf{C$\rightarrow$ I} & \cellcolor[HTML]{C0C0C0}\textbf{D$\rightarrow$ I} &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 16}} & \cellcolor[HTML]{CBCEFB}\textbf{15.0222} & \cellcolor[HTML]{FFCCC9}\textbf{3.3845} &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\ \cline{1-3}
\end{tabular}

}%}
%\vspace{-3mm}
%\end{table*}

\end{sidewaystable}







\begin{sidewaystable}
%\centering

\caption{Average performance of 10-Fold FGES (Fast Greedy Equivalence Search) causal discovery, with the prior knowledge that each node can only cause the other nodes with the same or greater depth with it. An edge means connecting two attributes from two different nodes, respectively. Thus, the number of possible edges between two nodes is the multiplication of the numbers of their attributes, i.e., the lengths of their data vectors.
\\ (All experiments are performed with 6 different Independent-Test kernels, including chi-square-test, d-sep-test, prob-test, disc-bic-test, fisher-z-test, mvplr-test. But their results turn out to be identical.)}
\label{tab:fges}
\resizebox{1\columnwidth}{!}{%
%\Rotatebox{90}{
%
\begin{tabular}{cccccccccccccccccc}
\hline
\rowcolor[HTML]{C0C0C0} 
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}Cause Node} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A} & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}B} & \multicolumn{3}{c|}{\cellcolor[HTML]{C0C0C0}C} & \multicolumn{3}{c|}{\cellcolor[HTML]{C0C0C0}D} & \multicolumn{3}{c|}{\cellcolor[HTML]{C0C0C0}E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}F} & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}I} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}True\\ Causation\end{tabular}} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}A$\rightarrow$ C} & \multicolumn{1}{c|}{B$\rightarrow$ D} & \multicolumn{1}{c|}{B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}C$\rightarrow$ D} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}C$\rightarrow$ G} & \multicolumn{1}{c|}{D$\rightarrow$ G} & \multicolumn{1}{c|}{D$\rightarrow$ H} & \multicolumn{1}{c|}{D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}E$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}E$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}E$\rightarrow$ H} & \multicolumn{1}{c|}{F$\rightarrow$ I} & \multicolumn{2}{c|}{\cellcolor[HTML]{EFEFEF}G$\rightarrow$ J} & \multicolumn{1}{c|}{H$\rightarrow$ J} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}I$\rightarrow$ J} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Number of\\ Edges\end{tabular}} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}16} & \multicolumn{1}{c|}{24} & \multicolumn{1}{c|}{16} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}6} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}4} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8} & \multicolumn{1}{c|}{12} & \multicolumn{1}{c|}{12} & \multicolumn{1}{c|}{9} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8} & \multicolumn{1}{c|}{12} & \multicolumn{2}{c|}{\cellcolor[HTML]{EFEFEF}4} & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}3} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Probability\\ of Missing\end{tabular}} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.038889} & \multicolumn{1}{c|}{0.125} & \multicolumn{1}{c|}{0.125} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.062} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.06875} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.039286} & \multicolumn{1}{c|}{0.069048} & \multicolumn{1}{c|}{0.2} & \multicolumn{1}{c|}{0.142857} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.3} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.003571} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.2} & \multicolumn{1}{c|}{0.142857} & \multicolumn{2}{c|}{\cellcolor[HTML]{EFEFEF}0.0} & \multicolumn{1}{c|}{0.072727} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.030303} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Wrong \\ Causation\end{tabular}} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}C$\rightarrow$ F} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{D$\rightarrow$ E} & \multicolumn{1}{c|}{D$\rightarrow$ F} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{F$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}G$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}G$\rightarrow$ I} & \multicolumn{1}{c|}{H$\rightarrow$ I} &  \\ \cline{1-1} \cline{5-5} \cline{9-10} \cline{14-17}
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Times\\ of Wrongly\\ Discovered\end{tabular}} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}5.6} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{1.2} & \multicolumn{1}{c|}{0.8} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{5.0} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8.2} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}3.0} & \multicolumn{1}{c|}{2.8} &  \\ \cline{1-1} \cline{5-5} \cline{9-10} \cline{14-17}
\multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{}
\end{tabular}
%}
}


\vspace{0.4in}
%\end{table*}

%\begin{table*}[t]
%\centering
\caption{ Brief Results of the Heuristic Causal Discovery in latent space, identical with Table 3 in the paper body, for better comparison to the traditional FGES methods results on this page.\\
The edges are arranged in detected order (from left to right) and their measured causal strengths in each step are shown below correspondingly.\\
Causal strength is measured by KLD values (less is stronger). Each round of detection is pursuing the least KLD gain globally. All evaluations are in 4-Fold validation average values. Different colors represent the ground truth causality strength tiers (referred to  the Figure 10 in the paper body).
}
\label{tab:discv}
%\resizebox{0.1\columnwidth}{!}{%
%\Rotatebox{90}{
\vspace{2mm}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{|c|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
\cellcolor[HTML]{C0C0C0}Causation & \cellcolor[HTML]{FFCCC9}A$\rightarrow$ C & \cellcolor[HTML]{FFCCC9}B$\rightarrow$ D & \cellcolor[HTML]{FFCCC9}C$\rightarrow$ D & \cellcolor[HTML]{FFCCC9}C$\rightarrow$ G & \cellcolor[HTML]{FFCCC9}D$\rightarrow$ G & \cellcolor[HTML]{FFCCC9}G$\rightarrow$ J & \cellcolor[HTML]{FFCCC9}D$\rightarrow$ H & \cellcolor[HTML]{FFCCC9}H$\rightarrow$ J & \cellcolor[HTML]{FFCE93}C$\rightarrow$ E & \cellcolor[HTML]{FFCE93}B$\rightarrow$ E & \cellcolor[HTML]{FFCE93}E$\rightarrow$ G & \cellcolor[HTML]{FFCE93}E$\rightarrow$ H & \cellcolor[HTML]{ABE9E7}E$\rightarrow$ F & \cellcolor[HTML]{ABE9E7}F$\rightarrow$ I & \cellcolor[HTML]{ABE9E7}I$\rightarrow$ J & \cellcolor[HTML]{ABE9E7}D$\rightarrow$ I \\ \hline
\cellcolor[HTML]{C0C0C0}KLD & 7.63 & 8.51 & 10.14 & 11.60 & 27.87 & 5.29 & 25.19 & 15.93 & 46.58 & 65.93 & 39.13 & 39.88 & 53.68 & 45.64 & 17.41 & 75.57 \\ \hline
\rowcolor[HTML]{FFFFFF} 
\cellcolor[HTML]{C0C0C0}Gain & 7.63 & 8.51 & 1.135 & 11.60 & 2.454 & 5.29 & 25.19 & 0.209 & 46.58 & -6.84 & -5.91 & -3.29 & 53.68 & 45.64 & 0.028 & 3.384 \\ \hline
\end{tabular}
%}}
%\end{table*}
}

\end{sidewaystable}








\end{document}
