
\documentclass[10pt]{article} % For LaTeX2e

\newif\ifpreprint

% \usepackage{tmlr}
% \preprintfalse

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}

% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:

\usepackage[preprint]{tmlr}
\preprinttrue

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{hyperref}
\usepackage{url}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{adjustbox}
%\usepackage{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{centernot}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{color}
%\usepackage{booktabs}

\usepackage{subfigure}
\usepackage{caption}
\usepackage{dsfont}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{empheq}
%\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{totcount}

\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{makecell}
%\usepackage[table]{xcolor}
\usepackage{xcolor}
\usepackage{changepage}
\usepackage{stfloats}
%\usepackage{cite}
\usepackage{soul}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{algpseudocode}
\usepackage[title]{appendix}
\usepackage{pdfpages}

\usepackage{bm}
\usepackage{amsmath,amssymb,amsfonts}
% \usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{bookmark}
\usepackage{float}
\usepackage{tabularx}
\usepackage{mathtools}
\usepackage{array}
\usepackage{rotating}
\usepackage[skins]{tcolorbox}
\usepackage{tikz}
\usepackage{pifont}

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{ \node[shape=circle,draw,inner sep=1.5pt] (char) {#1};}}

\title{Relation-Oriented: Toward Causal Knowledge-Aligned AGI}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.


\author{
\name Jia Li \email jiaxx213@umn.edu \\
      \addr Department of Computer Science,
      University of Minnesota
      \vspace{-3mm}
      \AND
      \name Xiang Li \email lixx5000@umn.edu \\
      \addr Department of Bioproducts and Biosystems Engineering,       University of Minnesota
      % \name Xiaowei Jia \email xiaowei@pitt.edu\\
      % \addr Department of Computer Science\\ University of Pittsburgh
      }

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{Apr}  % Insert correct month for camera-ready version
\def\year{2023} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version

\definecolor{thegrey}{RGB}{192,192,192}
\definecolor{orchid}{RGB}{204,153,255}
\definecolor{myblue}{RGB}{0,115,207}
\definecolor{mygreen}{RGB}{68,134,25}
%\definecolor{myorange}{RGB}{0,128,0}
\definecolor{mypurple}{RGB}{177,35,200}

\begin{document}


\maketitle
%\vspace{-3mm}

\ifpreprint
\vspace{-4.5mm}
\fi

\begin{abstract}
\vspace{-3mm}

\ifpreprint
\vspace{-1mm}
\fi

The potential surge of causal reasoning in AI models toward AGI is imminent, given the impending saturation of observation-based applications in fields like image and language processing. It is both critical and underrecognized that the essence of causality lies in the \emph{temporal nonlinearity} (i.e., dynamics) of causal effects. Capturing such featured causal representations is key to realizing AGI. This paper advocates for a thorough reevaluation and potential overhaul of existing causal inference theories and the traditional learning paradigm, which predominantly relies on the observational i.i.d assumption.
Our aim is to align these theories and methodologies with the intrinsic demands of AGI development.
We introduce a novel \emph{Relation-Oriented} paradigm for relationship modeling, and the \emph{Relation-Indexed Representation Learning} (RIRL) method as its foundational implementation. Extensive experiments confirm RIRL's efficacy in autonomously capturing dynamical effects.


% The current relationship modeling paradigm, grounded in the observational i.i.d assumption, inherently misaligns with our causal knowledge comprehension due to two vital oversights: 1) the unobservable relations, which lead to undetectable hierarchical levels of knowledge, driving the need for model generalizability; 2) the counterfactual relative timings to support our structural causal reasoning, which lead to inherent biases in models under the current \emph{Observation-Oriented} paradigm. This paper proposes a novel \emph{Relation-Oriented} framework, to reconsider these fundamental questions and unify various confusions surrounding AI-based causal learning, ranging from traditional causal inference to modern language models.

% Also, \emph{relation-indexed representation learning} (RIRL) is raised as a baseline implementation method of the proposed new paradigm, alongside comprehensive experiments demonstrating its efficacy in autonomously identifying dynamical effects in relationship modeling.

\vspace{-2mm}
\end{abstract}

\ifpreprint
\vspace{-3mm}
\fi

\section{Introduction}
\vspace{-2.5mm}

\label{sec:intro}




%the capability of representing abstract knowledge and, accordingly, facilitating human-like causal reasoning in symbol-grounded systems \cite{marcus2020next}, like machine learning and Artificial Intelligence (AI). 
%A central question is whether symbols, as well as symbol-grounded systems, such as AI, can represent our empirical understanding and inquiries \cite{newell2007computer, pavlick2023symbols}.

The concept of Artificial General Intelligence (AGI) has prompted extensive discussions over the years \cite{newell2007computer}, with the target toward facilitating human-like causal reasoning and knowledge comprehension in AI systems \cite{marcus2020next}.
In recent years, the large language models (LLMs) have risen as notable achievements in semantic understanding tasks and accordingly evoked debates about whether LLMs have edged us closer to realizing AGI  \cite{schaeffer2023emergent}.
Some studies point to their shortcomings in truly comprehending causality \cite{pavlick2023symbols}, %underlying the semantic associations, 
while others argue in favor of LLMs' ability to represent complex spatial and temporal features \cite{gurnee2023language}.
Notably, the use of meta-learning in language models has shown potential in achieving human-like generalization capabilities, at least to a certain extent \cite{lake2023human}.

These debates are anchored in a fundamental question: What underpins the distinction between two types of generalization?
One is how humans generalize learned causal knowledge to diverse scenarios, and another is how  AI systems generalize captured associative knowledge among texts and images.


%How do spatial, temporal, and abstract feature representations differentiate and yet interrelate? Accordingly, why does meta-learning seem to partially bridge the human-machine gap?

It appears that classical causal inference has offered a clear delineation among causality, correlation, and mere association \cite{pearl2000models, peters2017elements}. Moreover, it has provided a robust theoretical groundwork for representing causality in computational models. Based on that, causal learning has been widely utilized and yielded significant contributions to knowledge accumulation in various fields \cite{wood2015lesson, vukovic2022causal, ombadi2020evaluation}.
Thus, it is logical to incorporate well-established causal knowledge, often represented as causal DAGs (Directed Acyclic Graphs), into AI model architectures \cite{marwala2015causality, lachapelle2019gradient}. While this integration has greatly enhanced learning efficiency, it has not yet achieved the level of generalizability that constitutes a success \cite{luo2020causal, ma2018using}.

This finding likely circles us back to the beginning, as causal inference does not directly bridge the gap between AI models and causal reasoning. However, it does offer a valuable perspective: How would humans conduct causal reasoning based solely on a DAG? While AI is evidently challenged.

Indeed, even within the realm of causal inference, converting DAGs into operational causal models requires rigorous effort \cite{elwert2013graphical}. In various applications, data adjustments and model interpretations are often tailored, relying heavily on human discernment \cite{sanchez2022causal, crown2019real}. Challenges include verifying basic causal assumptions \cite{sobel1996introduction}, addressing confounding effects \cite{greenland1999confounding}, and ensuring model interpretability \cite{pearl2000models}, among others.
These achieved methodologies constitute the cornerstone of the value provided by causal inference. 
It stands to reason that the answer to our question may be gleaned by examining the challenges causal inference has faced and resolutions adopted.

From an applicational standpoint,
\cite{scholkopf2021toward} synthesize the development of causal models, emphasizing the crucial role of ``causal representations'' in achieving AI-based causal models' generalizability across various ``levels of knowledge'' learning.
They propose the potential need for a ``new learning paradigm'' - an idea we find both logical and thought-provoking. 
Our current models, ranging from causal to AI, are chiefly based on assumed independent and identically distributed (i.i.d) observations, possibly hindering their ability to achieve generalizable causal learning. Moreover, \cite{zhang2012identifiability} points out the ``identification difficulty'' when facing nonlinear (i.e., dynamical) effects, an inherent obstacle under the observational i.i.d setting. 

For clarity, we designate the prevailing paradigm as \emph{\textbf{Observation-Oriented}} modeling. In this study, we propose a novel framework, termed \emph{\textbf{Relation-Oriented}} modeling, inspired by the relation-indexing nature of human cognition processes \cite{sep-mental-representation}. Through this new lens, we seek to pinpoint the intrinsic limitations underlying existing modeling approaches.
Accordingly, to validate the proposed new paradigm, it must shed light on the array of questions that have emerged from the outset.
To encapsulate these queries: 
\begin{enumerate}[itemsep=0em, topsep=-0pt, 
parsep=2pt, partopsep=0pt,
leftmargin=22pt, labelwidth=10pt]
    \item[\ding{118}] \emph{Firstly}, causal inference challenges such as confounding effects, dependency on causal assumptions, and interpretative complexities call for a foundational explanation.
    \item[\ding{118}] \emph{Secondly}, To integrate causal reasoning within AI models, we need a nuanced understanding of ``levels of knowledge,'' the essential role of causal representation, its relevance to the difficulty of identifying temporally nonlinear effects, and potential resolutions to these issues.
    \item[\ding{118}] \emph{Thirdly}, in the context of Large Language Models (LLMs), it is crucial to discern the distinction between the ``spatial and temporal'' concepts in language understanding versus those in causality comprehension, and critically interpret what meta-learning has accomplished in terms of generalizability.
\end{enumerate}

While these questions may appear disparate, they are intrinsically linked by a fundamental requirement under the observational i.i.d assumption: the prior specification of observables (including their temporal events) in modeling.
The specified observational entities serve as the modeling target in solely observational learning tasks (like image recognition). In causal relationship learning, they are priorly identified as causes and effects, with their interrelation acting as the learning objective.

This requirement leads to two \emph{\textbf{primary limitations}} in modeling: 1) the inability to account for unobservable relational knowledge, which leads to undetectable hierarchical levels to challenge the model's generalizability, and 2) the prior obligation to identify relational effects along the absolute timing, potentially overlooking the underlying relative timings, which underpin our causal knowledge structure, and leading to inherent biases.

%the temporal dimension from the nonlinear computational analysis, leading to an oversight of dynamics.

% Figure environment removed


%adopts a \emph{\textbf{Relation-Oriented}} perspective, aiming to re-examine the foundational principles behind learning processes.
%Additionally, we introduce the \emph{Relation-Indexed Representation Learning} (RIRL) methodology as an innovative approach to craft AI systems aligning with our intuitive grasp of causality.

This paper consists of four principal parts: 
\begin{enumerate}[topsep=0pt, partopsep=0pt]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{1pt}
    \item {the Introduction: sets the foundation for the proposed \emph{Relation-Oriented} perspective in section \ref{subsec:ro_view}; also analyzes the roles of unobservable relational knowledge in modeling and uses an illustrative example to explain the resulting undetectable hierarchy (i.e., the limitation \fbox{L1}) in section \ref{subsec:unobs_relt}.}
    \item {Chapter I, including Sections \ref{sec:framework} though \ref{sec:temporal}: establishes the \emph{Relation-Oriented} framework, to precisely decompose relationship modeling from a distributional perspective; and through which, to examine the fundamental impacts of the outlined limitations, and addresses the queries listed above.} 
    \item {Chapter II, from Sections \ref{sec:representation} to \ref{sec:experiment}: introduces the \emph{Relation-Indexed Representation Learning} (RIRL) methodology as a baseline realization of the \emph{Relation-Oriented} paradigm and evaluates the efficacy of using relation-indexed (i.e., causal) representations to identify dynamical effects.}
    \item  {the Conclusion in Section \ref{sec:conclusion}: summarizes the insights and findings of this study.}
\end{enumerate}

% In the subsequent of this Introduction, we first define the \emph{Relation-Oriented} perspective (section \ref{subsec:ro_view}), then analyze the roles of unobservable knowledge in modeling, using an illustrative example (section \ref{subsec:unobs_relt}), and ultimately present an overview of the \emph{Observation-Oriented} limitations and their interrelations (section \ref{subsec:limit_oo}).


\subsection{Relation-Oriented Perspective}
\label{subsec:ro_view}
\vspace{-1mm}

Typically, experiments with $n$ trials produce instances $x^n = x_1, \ldots, x_n$ from sequential random variables $X^n = X_1, \ldots, X_n$, which are usually assumed to be independent and identically distributed (i.i.d). 
When they evolve over time, $n$ is often replaced by the timestamp $t$ to get a temporal sequence $X^t = X_1, \ldots, X_t$, maintaining the i.i.d assumption, and the relationship function is usually in shape $Y=f(X^t;\theta)$.

In this study, we abandon the assumed independence over $\{X_i \mid i=1,\ldots,t\}$ on the temporal dimension $\textbf{t}$, instead treat their sequence $X^t$ as a single entity, denoted by variable $\mathcal{X} \in \mathbb{R}^{d+1}$, with $d$ representing the observational dimension of each instance $X_i$.
For clarity, we use $X\in \mathbb{R}^d$ to represent a solely observational variable, and let $\mathcal{X}=\langle X,\mathbf{t} \rangle \in \mathbb{R}^{d+1}$ derived by incorporating the $\mathbf{t}$-dimension to encompass features across both observational and temporal dimensions.
It is worth noting that variables such as $\mathcal{X}$ are conventionally referred to as spatial-temporal \cite{andrienko2003exploratory}, while in this context, ``spatial'' is broadly interpreted to mean ``observational'', not restricted to be physically spatial like the geographic coordinates.

Consider the functional relationship model $\mathcal{Y}=f(\mathcal{X};\theta)$, where $\mathcal{Y}=\langle Y,\mathbf{\tau} \rangle \in \mathbb{R}^{b+1}$ with $\mathbf{\tau}$ representing the temporal evolution of $Y\in \mathbb{R}^b$. We employ the Fisher Information $\mathcal{I}_{\mathcal{X}}(\theta)$ \cite{ly2017tutorial} of $\mathcal{X}$ about $\theta$, to define the component of $\mathcal{Y}$ (signified as $\hat{\mathcal{Y}}$) that is sufficiently identified by indexing through $\theta$:

\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=red!50!black, colback=yellow!10]{
%\hspace{-2mm}
\begin{minipage}{0.97\textwidth}
\paragraph{Definition 1.} the \emph{\underline{Relation-Indexed Representation}} $\hat{\mathcal{Y}_{\theta}}$ in Relationship Modeling.

Let the \emph{\textbf{relation}} $\theta$ adequately represents the influence of $\mathcal{X}$ on $\mathcal{Y}$, denoted as $\mathcal{X} \xrightarrow{\theta} \mathcal{Y}$, then 
$\hat{\mathcal{Y}_{\theta}} = f(\mathcal{X}; \theta)$ represents the \emph{sufficient} component of $\mathcal{Y}$ about $\theta$, which is, $\mathcal{I}_{\hat{\mathcal{Y}_{\theta}}}(\theta) = \operatorname{max} \ \mathcal{I}_{\hat{\mathcal{Y}}}(\theta) = \mathcal{I}_{\mathcal{X}}(\theta)$.

%defining $\hat{\mathcal{Y}_{\theta}}$ as $\mathcal{Y}$'s sufficient component, $= \operatorname{argmax}_{\theta} \mathcal{I}_{\hat{\mathcal{Y}}}(\theta)$
\end{minipage}}
\vspace{-0.8mm}

Consequently, $\hat{\mathcal{Y}_{\theta}}$ encapsulates the information within $\mathcal{Y}$ that is entirely derived from $\mathcal{X}$, thus defined as the \emph{relation-indexed representation}.
Accordingly, the remaining component of $\mathcal{Y}$, expressed as $\mathcal{Y}-\hat{\mathcal{Y}_{\theta}}$, does not depend on $\theta$. The proposed \emph{Relation-Oriented} modeling focuses on realizing the indexing role of $\theta$.

The notation ``$\rightarrow$'' typically denotes causality, although a directional relationship does not necessarily imply causation in logic.
Nonetheless, for clarity, we will adopt terminology consistent with causal inference: for relationship $\mathcal{X} \xrightarrow{\theta} \mathcal{Y}$, we refer to $\mathcal{X}$ as the \emph{cause} and $\mathcal{Y}$ as the \emph{effect}, with a \emph{relation} $\theta$ connecting them. 
Accordingly, the defined $\hat{\mathcal{Y}_{\theta}}$ aligns with the ``causal representation'' concept \cite{scholkopf2021toward}.
Crucially, through this paper, both \emph{causality} and \emph{correlation} denote types of relationships with a relation $\theta$ (their difference will be discussed later), while \emph{association} refers to statistical dependency (typically nonlinear) between entities without an informative $\theta$, expressed as $(\mathcal{X},\mathcal{Y})$.

\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.95\textwidth}
\paragraph{Remark 1.} \hspace{-1em} Given $\mathcal{X} \xrightarrow{\theta} \mathcal{Y}$ with \emph{\textbf{observables}} $\mathcal{X}$ and $\mathcal{Y}$, the relationship model $\mathcal{Y}=f(\mathcal{X};\theta)$ becomes \emph{informative} due to the \emph{\textbf{unobservable}} $\theta$.

%\vspace{-0.5mm}
\end{minipage}}
\vspace{-0.5mm}

The outlined Remark 1 has its origins in the principle of Common Cause \cite{dawid1979conditional, scholkopf2021toward}, suggesting that any nontrivial (i.e., informative) conditional independence between two observables requires a third, mutual cause (i.e., the unobservable ``relation'' in our context). %It means that an informative $\theta$ differentiates a causal relationship $\mathcal{X} \xrightarrow{\theta} \mathcal{Y}$ from a mere statistical dependency $(\mathcal{Y} \mid \mathcal{X})$.

$\mathcal{X}$ and $\mathcal{Y}$ can be either solely observational entities, equal to $X$ and $Y$ (e.g., images, spatial coordinates of a quadrotor, etc.), or observational-temporal entities (e.g., trends of stocks, a quadrotor's trajectory, etc.). 
Regardless of their characterization, the primary goal of adopting $\mathcal{Y}=f(\mathcal{X};\theta)$ is to encapsulate the unobservable relational knowledge represented by $\theta$, rather than merely distributional association  $(\mathcal{X},\mathcal{Y})$.


To clarify the concept of informative $\theta$, consider a simple example: In the relationship ``Bob (represented as $X$) has a son named Jim (represented as $Y$)'', the father-son relational information $\mathcal{I}_{\mathcal{X}\mathcal{Y}}(\theta)$ between them is evident to humans, but unobservable to AI systems provided sufficiently observed social activities. Also, $\theta$ can be seen as the common cause of $X$ and $Y$ that makes their connection unique, rather than any random pairing of ``Bob'' and ``Jim''. 
Through the provided observations, AI may deduce a particular associative pattern over $(X, Y)$, but cannot internalize the unobservable information $\mathcal{I}_{\mathcal{X}\mathcal{Y}}(\theta)$. 

Based on the symbolization in Definition 1 and the principle of Remark 1, a \emph{Relation-Oriented} framework will be established in Section \ref{sec:framework} to offer more complete insides into relationship modeling. %, offering more complete insights into the modeling of causal relationships.

% In this framework, we will particularly define the \emph{temporal} feature space, distinguishing it from the \emph{observational} feature space. 
% This will enable us to reexamine the limitations in queries from a novel perspective.

\subsection{Unobservable Relational Knowledge}
\label{subsec:unobs_relt}
%\vspace{-0.5mm}

The unobservable relations in knowledge may not directly serve as the learning objective $\theta$, but still be relative to and profoundly impact the modeling process. We elucidate this phenomenon through an example:
Notably, on social media, AI-created personas can have realistic faces but seldom showcase hands. This is because AI for visual tasks struggles with the intricate structure of hands, instead treating them as arbitrary assortments of finger-like items.
Figure \ref{fig:hand}(a) provides AI-created hands with faithful color but unrealistic shapes, while humans can effortlessly discern hand gestures from the grayscale sketches in (b).

Human cognition intuitively employs informative relations as the \emph{\textbf{indices}}, guiding us to visit specific mental representations \cite{sep-mental-representation}. As illustrated in (b), our cognitive process operates hierarchically, through a series of relations, denoted by $\theta=\{\theta_i,\theta_{ii},\theta_{iii}\}$.
Each higher-level understanding builds upon conclusions drawn at preceding levels. Specifically, Level $\mathbf{I}$ identifies individual fingers; Level $\mathbf{II}$ distinguishes gestures based on the positions of the identified fingers, incorporating additional information from our understanding of how fingers are arranged to constitute a hand, denoted by $\omega_i$; and Level $\mathbf{III}$ grasps the meanings of these gestures from memory, given additional information $\omega_{ii}$ from knowledge.


% Figure environment removed


%To AI, or hypothetical extraterrestrial life unfamiliar with our knowledge, hands in Figure \ref{fig:hand}(a) may appear reasonable.
% AI can successfully differentiate non-overlapping features at various levels. 
%, while similar hand gestures may confuse it. 


Typically, these visual learning tasks do not directly capture relational information, neither $\mathcal{I}_X(\theta)$ nor $\mathcal{I}_X(\omega)$, focusing instead on modeling the entity $Y$ solely based on observations $X$. 
Without indexing through $\theta$, AI systems may struggle to distinguish different levels in $Y$. They tend to encapsulate the observational dependences, such as $(X_{II}\mid X_I)$ and $(X_{III}\mid X_I, X_{II})$, entirely within the association $(X_I, X_{II}, X_{III})$, resulting a lack of informative insights into $\omega$.

However, the hidden  $\omega$ may not always be essential. 
%If relations $\theta$ across levels are non-overlapping, AI can accurately differentiate them. 
For instance, AI can generate convincing faces because the appearance of eyes $\theta_i$ strongly indicates the facial angles $\theta_{ii}$, i.e., $\mathcal{I}_X(\theta_{ii})\not>\mathcal{I}_X(\theta_i)$, removing the need to distinguish eyes $Y_{I}$ from faces $Y_{II}$ to reveal $\mathcal{I}_X(\omega_i)=0$. Furthermore, with observational levels in $X$ fully captured, AI can inversely detect indexing relations using methods like reinforcement learning \cite{sutton2018reinforcement, arora2021survey}. For example, in Figure~\ref{fig:hand}, when AI systems receive approval for generated five-fingered hands $(Y_{II} \mid Y_I)$, reflecting $\omega_i$ through $(X_{II} \mid X_I)$, they may autonomously begin to derive $X_I \xrightarrow{\theta_i} Y_I$.

\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=red!50!black, colback=yellow!10]{
%\hspace{-2mm}
\begin{minipage}{0.95\textwidth}
\paragraph{Definition 2.}  \hspace{-1em} \emph{\underline{Hidden Relation} $\omega$} and its resulting \emph{\underline{Knowledge Hierarchy}}.

Unlike the \emph{\textbf{indexing relation}} $\theta$ as a learning objective, the \emph{\textbf{hidden relation}} $\omega$ forms hierarchical knowledge levels, necessitating model \emph{generalizability} to maintain effectiveness across.
\end{minipage}}
\vspace{1mm}

The illustration in Figure \ref{fig:hand} shows different roles of unobservable relations, $\theta$ and $\omega$, in solely observational learning tasks. Our main focus, however, is on relationship models that explicitly incorporate $\theta$ as a functional parameter. 
A \emph{generalizable} model allows lower-level knowledge like $\theta_i$ to be reusable for higher-level learning tasks like $\theta_{ii}$ \cite{scholkopf2021toward},  reflecting our innate ability to generalize knowledge cognitively. %For example, our ability to identify fingers can be applied regardless of the types of medium, like images, photos, or videos. 
Generalizable also denotes the capacity to \emph{individualize} from higher to lower levels, to accommodate different $\omega_i$ values.


Consider this example: Family incomes $X$ influence grocery shopping frequencies $Y$ through relation $\theta$. 
Here, the cultural background $\omega$ emerges as an important factor, such that an effective model $Y=f(X;\theta)$ has to be individualizable, i.e., conditioned on a specific country (represented by a particular $\omega$ value) to ensure practical utility.
On the opposite, a generalization would imply $\omega=\varnothing$.

For the sake of clarity, hereafter in this paper, unless explicitly stated otherwise, the hidden relation $\omega$ represents two hierarchical levels: The generalized level $X_o\xrightarrow{\theta_o} Y_o$ with $\theta_o$ implying $\omega=\varnothing$, and the individualized level $X_{\omega}\xrightarrow{\theta_{\omega}} Y_{\omega}$ given $\theta_o$ with a specific $\omega$ value, collectively notated as $(\theta, \omega) = \begin{pmatrix} \theta_o \\ \theta_{\omega} \end{pmatrix}$.

In the context of \emph{causal} relationship learning, the temporal events for $\mathcal{X}$ and $\mathcal{Y}$ are usually pre-identified. However, this may not guarantee their \emph{temporal features} to be completely captured by the model, making  $(\theta,\omega)$ \emph{\textbf{undetectable}} for AI models, and precluding methods like inverse reinforcement learning.
%$\omega$ stratifies unobservable $\theta$ into hierarchical levels, culminating in a completely imperceptible 

% \subsection{Limitations of Observation-Oriented Modeling}
% \label{subsec:limit_oo}


% Figure \ref{fig:limit} summarizes the intrinsic limitations of \emph{Observation-Oriented} modeling, with detailed explanations divided into three Sections.
% Labels \fbox{L1} and \fbox{L2} correspond to the two \emph{primary limitations} previously discussed. For \fbox{L1}, as highlighted in section \ref{subsec:unobs_relt}, the hidden relation $\omega$ can lead to undetectable hierarchical levels. For models focusing on observational-only entities, it can lead to visually noticeable AI alignment issues \cite{christian2020alignment} like the case in Figure~\ref{fig:hand}~(a). 
% Furthermore, in relationship modeling, this oversight can potentially reveal more complex intricacies, such as how \fbox{L1} and \fcolorbox{black}{orange!30}{L5} together contribute to \fcolorbox{black}{orange!30}{L6}.

% In our cognitive processes to understand the relationship $\mathcal{X} \xrightarrow{\theta} \mathcal{Y}$, effect $\mathcal{Y}$ can be autonomously indexed and identified through $\theta$ from $\mathcal{X}$. However, in \emph{Observation-Oriented} modeling, due to \fbox{L2}, only the observational $Y$ can be initially recognized without its dynamics (labeled as \fcolorbox{black}{myblue!23}{L\ref{sec:causality}}). Even if using a sequential effect $Y^{\tau}$, the model can only realize dynamics of the cause $\mathcal{X}$ at best, but fail to capture nonlinearities within the $\tau$ dimension.
% This oversight results in \fcolorbox{black}{myblue!23}{L\ref{subsec:caus_imbalance}}, an imbalance in capturing observable dynamics between cause and effect in the model, and \fcolorbox{black}{myblue!23}{L\ref{subsec:caus_identif}}, the challenged causal effect identification. 

% % Figure environment removed

% The overlooked dynamics of effects serve as a key factor preventing AI systems from achieving autonomous causal reasoning (see Section~\ref{sec:causality}).
% The combination of \fcolorbox{black}{myblue!23}{L\ref{sec:causality}} and \fbox{L1} leads to challenges in traditional causal inference. These challenges are labeled as \fcolorbox{black}{mygreen!20}{L\ref{sec:application}} and analyzed in Section \ref{sec:application} through practical examples. 

% AI models are often favored for addressing large-scale questions, such as learning \emph{structural causal relationships} among numerous variables. However, in this context, the overlooked effect dynamics may lie in multi-dimensional temporal feature space (see section \ref{subsec:framework_time} for the concept). This oversight, labeled as \fcolorbox{black}{orange!30}{L5}, can introduce \emph{inherent biases} into structural causal models, labeled as \fcolorbox{black}{orange!30}{L6}. Such biases significantly reduce the models' generalizability and robustness, contributing another critical factor that impedes the realization of causal reasoning in AI (see Section \ref{sec:temporal}).

% delves into the underlying structure, proposing an enhancement to conventional Directed Acyclic Graphs (DAGs) to identify dynamic effects. 
% These dynamics, when interacting across the overlooked multiple logical timelines, combined with \fbox{L1}, can introduce \emph{inherent biases} into structural causal models, labeled as \fcolorbox{black}{orange!30}{L6}. Such biases are significant contributors to the reduction of the models' generalizability and robustness.

%Owing to \fbox{L2}, all potential events within the structural relationship must be specified solely along the observational timeline, denoted by $t$ or $\tau$. 

%For \fcolorbox{black}{myblue!23}{L5}, while incorporating hidden confounders might improve individual model interpretations, it does not bolster structural learning.
%Furthermore, a complex causality structure may introduce ``relative timelines'' (see subsection \ref{subsec:framework_time}), leading to additional temporal dimensions, distinct from the observable $\tau$-timeline.
%Furthermore, when incorporating complex causal relationship structures, ``relative timelines'' may be introduced (see section \ref{subsec:framework_time} for details), but inherently overlooked by \emph{Observation-Oriented} modeling. This oversight, combined with \fbox{L1} and \fbox{L2}, may inherently hinder our progress toward AGI, where the challenges are encapsulated as \fcolorbox{black}{mygreen!23}{L6} and \fcolorbox{black}{mygreen!23}{L7}, and elaborated in Section \ref{sec:temporal}.


\begin{center}
   {\vskip 8pt\large\bf Chapter I: Limitations of Current Observation-Oriented Paradigm} 
\end{center}

The prevailing \emph{Observation-Oriented} machine learning paradigm misaligns with the relation-centric essence of human comprehension \cite{sep-mental-representation}, which may not have been critical in the past. In traditional causal inference, challenges could be addressed through intended adjustments due to the limited scale of questions. Nonetheless, with the advancements in AI-based large models, the consequences of this misalignment have become increasingly significant across various applications.


Section \ref{sec:framework} introduces a \emph{Relation-Oriented} dimensionality framework, representing relationships as decomposed distributions. This framework underscores the critical role of relative timings in structural causal reasoning (highlighted as limitation \fbox{L2}), and the importance of dynamic capturing for generalizable causal models. Subsequently, Section \ref{sec:causality} explores the implications of often-overlooked dynamical effects (the secondary impact of \fbox{L2}), mainly in response to the \ding{118} outlined challenges. Lastly, Section \ref{sec:temporal} thoroughly examines the scheme and impact of inherent biases due to overlooking the underlying relative timings in structural causal models (the primary impact of \fbox{L2}).


% To complete the proposed \emph{Relation-Oriented} paradigm, a comprehensive framework for symbolizing relationships is first introduced in Section \ref{sec:framework}, to provide a foundational perspective for the subsequent sections. 
% Specifically, we enhance conventional Directed Acyclic Graphs (DAGs) to distinguish the dynamics of effects. Through this enhancement, challenges in causal inference are reexamined from a novel \emph{Relation-Oriented} perspective, as detailed in Section~\ref{sec:application}.

 
% Human understanding inherently indexes through relations \cite{sep-mental-representation}, directing to mental representations 
% about observational and temporal entities. 
% This intrinsic characteristic results in a fundamental misalignment with the \emph{Observation-Oriented} modeling paradigm, evident through various application issues.

% Section \ref{sec:framework} establishes the framework for decomposing relationships into symbolic representations, a foundation of subsequent discussions. In Section \ref{sec:causality}, detailed examples are employed to examine the roles of nonlinear temporal dynamics in causal relationship modeling.
% Subsequently, Section \ref{sec:temporal} underscores the significance of relative timelines in structural causal learning and its influence on model generalizability.

%this discrepancy 

\section{Relation-Oriented Dimensionality Framework}
\label{sec:framework}

A central question in the debates surrounding AGI persists: Can AI systems, which rely on mathematical symbolizations, achieve a human-like understanding sufficient to handle empirical inquiries \cite{newell2007computer, pavlick2023symbols}? We propose to focus on representing unobservable elements within our knowledge, such as abstractly meaningful relations, which are vital for the informativeness of causal reasoning. By indexing through these relations, AI models have the potential to reflect our logical deductions, embody the cognitive concepts they lead to, and ultimately construct their representations. 
As aligning with our causal knowledge, these representations can yield generalizable models, critical for actualizing causal reasoning in AGI.

% By directing the learning objective towards extracting these elements from observable data, the symbolized computation within the model remains informative. 
% Such captured relational information has the potential to mirror our cognitive processes of logical deduction.

By Definitions 1 and 2, representing a relationship necessitates two types of variables: the observables $\{\mathcal{X}, \mathcal{Y}\}$, and the unobservables $(\theta,\omega)$. As specified, $\mathcal{X}$ and $\mathcal{Y}$ include both \emph{observational} and \emph{temporal} features. In response, we adopt the concept of a \emph{hyper-dimension} to integrate these unobservable features. Consequently, we establish a framework, as illustrated in Figure~\ref{fig:space}, to represent relationships as joint distributions across three distinct types of dimensions.
For clarity, ``feature'' refers to the potential variable fully representing a certain distribution of interest. 

Figure~\ref{fig:space} aims to decompose our cognitive space where relational knowledge is stored. The hyper-dimensional space $\mathbb{R}^H$ is constructed by aggregating all \emph{\textbf{unobservable}} relations in our knowledge, such as $(\theta,\omega)\in \mathbb{R}^H$. Conversely, the observational-temporal joint space, $\mathbb{R}^O \cup \mathbb{R}^T$, is considered as the \emph{\textbf{observable}} space. In both $\mathbb{R}^O$ and $\mathbb{R}^T$, a temporal dimension consistently signifies the evolution of timing but represents distinct concepts, as outlined in section \ref{subsec:framework_time}. 
Within such a dimension, linear and nonlinear distributions correspond to \emph{static} and \emph{dynamical} features, respectively, a distinction further explained in section \ref{subsec:framework_obs}.


%For a model to be practically valuable, it must accurately reflect our understanding. Similarly, a successful AGI, to meet our expectations, must be rooted in existing knowledge. Specifically, it should represent relations residing in the unobservable Hyper-dimensional Space, through which reasonable interpretations of observable entities can be generated.


\vspace{0.5mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=red!50!black, colback=yellow!10]{
\hspace{-1mm}\begin{minipage}{0.96\textwidth}
\paragraph{Definition 3.} The \emph{\underline{Relationship Representation}} within the proposed Dimensionality Framework.

For the relationship $\mathcal{X}\xrightarrow{\vartheta}\mathcal{Y}$, where $\{\mathcal{X},\mathcal{Y}\}\in \mathbb{R}^O$, the \emph{\textbf{structuralized relation}} $\vartheta\in \mathbb{R}^T \cup \mathbb{R}^H$ can be decomposed as: \vspace{0.5mm} 

\centering
$\vartheta = \overrightarrow{\theta^1\ldots\theta^T}$, where $(\theta^i, \theta^j)\in \mathbb{R}^H$ for any $i \neq j \in \{1,\ldots,T\}$. Accordingly, \vspace{1mm} 

\centering
$(\vartheta, \omega) = \begin{pmatrix} \vartheta_o \\ \vartheta_{\omega} \end{pmatrix} = \begin{pmatrix} \theta^1_o & \ldots & \theta^T_o \\ \theta^1_{\omega} & \ldots & \theta^T_{\omega} \end{pmatrix}$ with any $
 (\theta^i_o,\theta^j_o)\in \mathbb{R}^H$ and $(\theta^i_{\omega}, \theta^j_{\omega})\in \mathbb{R}^H$.

\end{minipage}}
%\vspace{-0.5mm}

% Figure environment removed
%\vspace{-1mm}


\vspace{-1mm}
\subsection{Absolute Timing vs. Relative Timings}
\label{subsec:framework_time}
\vspace{-1mm}

In spatial-temporal data, the attribute recording observed timestamp $t$ typically reflects the \emph{\textbf{absolute}} timing of reality. However,  from a modeling view, the temporally meaningful $t$ values are indistinguishable from other attributes. As shown in Figure~\ref{fig:space}, the absolute timing $\mathbf{t}$ serves as a standard dimension within the observational space $\mathbb{R}^O$, along which, $\mathcal{X}$ and $\mathcal{Y}$ are invariably observed as data sequences $X^t$ and $Y^t$.

Contrarily, in our cognition, \emph{\textbf{relative}} timings inherently exist \cite{wulf1994reducing} to support the ``what if'' thinking and form structualized causal knowledge. We thereby designate a distinct ``temporal space'' $\mathbb{R}^T$, composed of $T$ relative timings as axes (i.e., $T$ cognitive \emph{timelines} \cite{shea2001effects}), to accommodate the knowledge-aligned (i.e., under $\vartheta$, as per Definition 3) temporal distributions.
Instead of as $\mathcal{Y} \in \mathbb{R}^O$, it can distribute across $\mathbb{R}^T$, represented as $(\mathcal{Y} \mid \mathcal{X}, \vartheta) \in \mathbb{R}^{O-1} \cup \mathbb{R}^T$, or jointly represented as $(\mathcal{X}, \mathcal{Y} \mid \vartheta) \in \mathbb{R}^{O} \cup \mathbb{R}^T$. 

$\vartheta$ can span up to $T$ timing dimensions in $\mathbb{R}^T$, with the effect $\mathcal{Y}=\sum_{i=1}^{T} \hat{\mathcal{Y}^i}$ decomposed into $T$ components, each residing in a distinct timing. Crucially, defining $\vartheta$ as a ``structuralized'' relation not only recognizes its multi-dimensionality but also highlights the potential \emph{nonlinear dependence} among these timings, manifested as $(\hat{\mathcal{Y}^i}, \hat{\mathcal{Y}^j})\in \mathbb{R}^{O-1}\cup \mathbb{R}^T$, while more precisely represented by $(\theta^i, \theta^j)\in \mathbb{R}^H$ in Definition 3. We term these nonlinear temporal dependences as \emph{\textbf{dynamical interactions}} for clarity, which necessitate the establishment of a distinct $\mathbb{R}^T$ space, rather than additional temporal dimensions within $\mathbb{R}^O$ (detailed in section \ref{subsec:framework_info}).
%further detailed in section \ref{subsec:framework_info}

%which serve as distinct \emph{timelines} in our cognitive framework to house that structuralized relational knowledge.

%treating $T$ relative timings as $T$ dimensions. 

%Whereas, when $\{\mathcal{X},\mathcal{Y}\}$ are considered alongside $\vartheta$ within the joint space $\mathbb{R}^O \cup \mathbb{R}^T$, 

For instance, patients' vital signs are recorded daily in a hospital with \emph{absolute} chronological timestamps. However, to assess a medical intervention $\mathcal{Y}$, a uniform series of post-medication events must be selected, for example, spanning from the day after medication to the 30th day. This creates a timeline represented by the axis ticked as $[1,30]$ to denote the \emph{relative} timing, regardless of \emph{absolute} timestamps of the selected records.
Yet, if the intervention involves two distinct aspects, such as the primary effect $\hat{\mathcal{Y}^1}$ and the side effect $\hat{\mathcal{Y}^2}$, and their mutual influences are of interest, then two separate relative timings, $\mathbf{t_1}$ and $\mathbf{t_2}$, must be considered for their individual evolutions, even though both may be labeled as $[1,30]$.
%assigning absolute timestamps for their identification will risk overlooking their unique dynamic evolutions.


%\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.95\textwidth}
\paragraph{Remark 2.} \hspace{-1em}
Although $\mathcal{Y}\in \mathbb{R}^O$ is \emph{observed} as a sequence along the absolute timing $\mathbf{t}$, it may represent an \emph{underlying} structure determined by $\mathcal{X}\xrightarrow{\vartheta}\mathcal{Y}$, spinning multiple relative timing axes in $\mathbb{R}^T$ space.
%\vspace{-0.5mm}
\end{minipage}}
%\vspace{-1mm}
% The \emph{temporal dimension} in a causal relationship is not limited to a single dimension but can include multiple timing axes (i.e., timelines) to form a multi-dimensional \emph{temporal space}.

Conventionally, the concept of ``temporal dimension'' is often simplified to be the single absolute timing $\mathbf{t}$, evident from the traditional ``spatial-temporal'' analysis \cite{alkon1988spatial, turner1990spatial, andrienko2003exploratory}, to recent advancements in language models \cite{gurnee2023language}. 
However, as emphasized in Remark 2, our cognitive perception of ``time'' is more complex, fundamentally enabling our causal reasoning \cite{coulson2009understanding}.

For an intuitive insight into the implications of neglecting relative timings in $\mathbb{R}^T$, let's consider an analogy:
Imagine ants dwelling on a floor's two-dimensional plane. To predict risks, the scientists among them create two-dimensional models and instinctively adopt the nearest tree as a height reference. They noticed increased disruptions at the tree's first branch, which indeed correlates to the children's heights, given their curiosity.
However, without understanding humans as three-dimensional beings, they can only interpret it by adhering to the first branch.
One day, after relocating to another tree with a lower height, the ants found the risk presenting at the second branch instead, making their model ineffective. They may conclude that human behaviors are too complex, highlighting the model generalizability issue.
%Similarly, when we specify a single, absolute timeline for all potential events, this timeline becomes our ``tree'', which may introduce inherent modeling biases, affecting the robustness and generalizability of our models.

As three-dimensional beings, we inherently lack the capacity to fully integrate the fourth dimension - time - into visual perception. Instead, we conceptualize ``space'' in three dimensions to incorporate features of the temporal dimension along a \emph{timeline} within the space, analogous to our ``tree''.
Yet, ants do not need to fully comprehend the three-dimensional world to build a generalizable model; instead, they need only recognize the ``forest'' out of their vision (i.e., counterfactual), which consists of all ``possible trees'' with \emph{relatively} different branch locations. 
Similarly, in our modeling, we must include the $\mathbb{R}^T$ space, composed of all potential relative timings within our causal knowledge, although they cannot be directly observed.

%\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.95\textwidth}
\paragraph{Remark 3.} 
\hspace{-3mm}
\emph{Counterfactuals} can be considered as posterior distributions within $\mathbb{R}^O\cup \mathbb{R}^T$.

\vspace{-0.5mm}
\end{minipage}}
%\vspace{-1mm}

Addressing the counterfactual query ``what effect would be if the cause were changed'' differentiates causality from mere correlations \cite{scholkopf2021toward}. In the proposed framework, counterfactuals are more intuitively interpreted through distributions, potentially offering valuable insights in fields like quantum computing.
Specifically, the observed prior conditions $\mathcal{X}$ can be considered as features in $\mathbb{R}^O$, whose effects $\mathcal{Y}$ act 
as a conditional distribution within $\mathbb{R}^{O-1}\cup \mathbb{R}^T$, incorporating $T$ possible observed timings in the future. %This perspective might potentially offer valuable insights in fields like quantum computing.

%- It might offer an intriguing insight into quantum computing (need further exploration).
%It might provide an intriguing insight that could potentially inspire advances in quantum computing (further exploration required).





% Under the \emph{Observation-Oriented} paradigm, prior identification of effects for dynamics is notably difficult for further discussions). This neglect of temporal nonlinearity and oversight of relative timelines can lead to \emph{\textbf{inherent bias}}, thereby compromising the generalizability of causal models. While such misalignments might have been subtle in the past, the advent of AI enables large-scale models more efficiently, and its black-box nature allows these biases to accumulate exponentially inside, eventually resulting in uninterpretable outputs.
% %indispensable 
% %accentuated
% %disparity 


\subsection{Dynamical vs. Sequential Static}
\label{subsec:framework_obs}
\vspace{-1mm}



The distributions along a dimension can be broadly classified into \emph{linear} and \emph{nonlinear} categories. Within the temporal dimension, these correspond to \emph{\textbf{static}} and \emph{\textbf{dynamical}} temporal features, respectively, and can be represented by corresponding variables. 
Static features are typically linked to specific timestamps. For instance, consider the statement ``rain leads to wet floors''; here ``wet floors'' represents a state that can be identified at a particular point in time. Therefore, it can be denoted as a static variable $X_t$ with a specified timestamp $t$. 
In contrast, the expression ``floors becoming progressively wetter'' necessitates a representation that captures the temporal distribution, to account for changes over time, like $X^t=X_1, \ldots, X_t$. However, this raises a question: Is $X^t$ a dynamical variable or a sequence of static variables?

Within the current machine learning paradigm, the distinction between ``static'' and ``dynamical'' is typically made between ``models'' instead of  
\ifpreprint
``variables'',
\else
``variables'' \cite{static-dynamic},
\fi
which refers to whether time is a factor in the model's equations.
However, this essentially requires the function $f(X^t;\theta)$ to represent the \emph{dynamics of effect} inherently encompassed by $\mathcal{Y}$. As a result, the model assumption for $f(;\theta)$, as well as the identification of a \emph{static} outcome $Y_{t+1}$, become crucial in determining how much effect dynamics can be captured \cite{weinberger2022static}, or potentially neglected, which will be discussed further in Section~\ref{sec:causality}.

\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=red!50!black, colback=yellow!10]{
%\hspace{-2mm}
\begin{minipage}{0.95\textwidth}
\paragraph{Definition 4.}  \hspace{-1em} The \emph{\underline{Dynamically Significant}} $\mathcal{Y}$ vs. Sequential Static $Y^{\tau}$.

As a \emph{dynamical} variable, $\mathcal{Y}=\langle Y, \tau \rangle\in \mathbb{R}^O$ permits \emph{\textbf{nonlinear} computational freedom} over $\tau$, whereas a \emph{sequential static} variable $Y^{\tau}\in \mathbb{R}^{O-1}$ assumes i.i.d or \emph{\textbf{linear}} changes along $\tau$. They samely manifest as sequential instances $y^{\tau}= y_1,\ldots,y_{\tau}$, while the dynamical significance of $\mathcal{Y}$ is \emph{\textbf{model-dependent}}.
\end{minipage}}

Definition 4 is based on the proposed \emph{Relation-Oriented} paradigm: The relation $\theta \in \mathbb{R}^H$ and the outcome $\mathcal{Y}\in \mathbb{R}^O$ are considered individually, where $\theta$ represents certain unobservable information within $\mathbb{R}^H$, lacking an explicit distributional representation.
This allows $\mathcal{Y}$ to be considered a variable that encompasses the dynamical effects caused by $\mathcal{X}$.
Similarly, the cause $\mathcal{X}=\langle X,t\rangle\in \mathbb{R}^O$ can also be a dynamical variable whose fulfillment depends on specific models. For example, RNN models typically formulate $Y_{t+1}=f(\mathcal{X};\theta)$ with a dynamical cause represented by latent space features, but remaining the outcome static.

%This is reasonable since the significance of temporal distributions depends on specific modeling demands, while an individual variable can only be characterized as incorporating $\textbf{t}$ as a potential computational dimension.
Accordingly, the statement ``floors becoming progressively wetter'' can be roughly considered as ``linearly increasing from $0\%$ to $100\%$ in 10 minutes'' to be a sequential static feature. It can also be depicted as a continuous nonlinear distribution, a dynamical feature for finer granularity. The latter can cover variances in the former, such as varying progression speeds, which the former cannot. In essence, the fulfillment of dynamical variables is crucial for achieving model generalizability over temporal dimensions.


%is static or dynamical hinges on whether the changes from $X_t$ to $Y_{t+1}$ are confined to be linear, as represented by $\theta$. Conventionally, $\theta$ and $Y$ are often considered together, with $\theta$ representing all potential static and dynamical changes in a hybrid \cite{weinberger2022static}, while the observational $Y_{t+1}$ alone displays the resultant static outcome at a specific timestamp.




% %\vspace{1mm}
% \noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.95\textwidth}
% \paragraph{Remark 4.} 
% \hspace{-3mm}
% Regardless of the dynamics represented by $\mathcal{X}$ or the sequential static features by $X^t$, their produced instances consistently manifest as observational sequences $x^t=(x_1,\ldots,x_t)$ in data, where the dynamical significance of $\mathcal{X}$ is model-dependent.

% \vspace{-0.5mm}
% \end{minipage}}
% \vspace{-1mm}


% $\mathcal{X}=\langle X,t \rangle$. 
% The dynamical significance of $\mathcal{X}$, however, can vary depending on the models employed. 
% In the \emph{observable data space}, $\mathcal{X}$ consistently manifests as a data sequence $X^t = (X_1, \ldots, X_t)$.
% If its temporal value changes can be fully described by a linear function, such as $X_{t+1}=2*X_t$, then it simply acts as ``a sequence of static variables''.
% Conversely, if the sequence $(X_1, \ldots, X_t)$ represents a distribution with significant $t$ dimensions, then $\mathcal{X}$ qualifies as a ``dynamical variable''.
% %where each static instance is a snapshot at a distinct point in time.

% For causal relationship $\mathcal{X} \xrightarrow{\theta} \mathcal{Y}$, the $t$ dimension for $\mathcal{X}$ and the $\tau$ dimension for $\mathcal{Y}$ may not be the same timeline in logic.
% Therefore, even if the applied model has computational freedom in the temporal dimension, only one of them, typically $t$, for the cause, can be incorporated as a computational dimension.


% Figure environment removed
\vspace{-2mm}

Considering a structuralized relation $\vartheta$, a valid model-generalizing process requires the model to remain effective over temporal dimensions at any level, including both the absolute timing within $\mathbb{R}^O$ and the relative timings in counterfactual $\mathbb{R}^T$. 
In our cognition, the generalized causal knowledge ($\omega=\varnothing$) can be instinctively extracted from individualized varied scenarios (with varying $\omega$ values). However, the undetectability of $(\vartheta, \omega)$ implies our models cannot autonomously fulfill this process, irrespective of whether they are AI-based.

%When constructing knowledge, humans' cognition instinctively extracts general information ($\omega=\varnothing$) from various scenarios and, when applying this knowledge, individualizes it by adapting to specific scenarios (with varying $\omega$ values).
%Accordingly, it is unsurprising that the causal DAGs in our cognitive framework represent generalized knowledge only, and necessitate the generalizability of the causal models, to remain temporally multi-dimensional effective across the hierarchical levels. 

Figure~\ref{fig:logical} showcases models' and humans' perspectives, distinguished as the ``Observed-View'' and ``Logical-View''. (a) and (b) compare a dynamical distribution along absolute timing in $\mathbb{R}^O$, while (c) and (d) display a DAG structure across two relative timings in $\mathbb{R}^T$, which exhibits a typical \emph{dynamical confounding} scenario. 

In (c), the static instances $y_A^1$ and $y_B^1$ signify that the two individualized dynamical effects $\mathcal{Y}_A$ and $\mathcal{Y}_B$ reach the same status value $y^1$ in dimension $\mathbf{t_1}$, i.e., attaining an equivalent magnitude; this situation is similarly observed in another timing dimension $\mathbf{t_2}$. Notably, the edge from $y^1$ to $y^2$ indicates an interaction between the two dynamical effect components $\hat{\mathcal{Y}^1}$ and $\hat{\mathcal{Y}^2}$, which can be either static or dynamical, suggesting their linear or nonlinear dependence (detailed definitions are provided in Section~\ref{sec:temporal}). 
%static influence.

\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=red!50!black, colback=yellow!10]{
%\hspace{-2mm}
\begin{minipage}{0.95\textwidth}
\paragraph{Definition 5.}  \hspace{-1em} The \emph{\underline{Dynamical Confounding}} Phenomenon. 

For relationship $\mathcal{X}\xrightarrow{\vartheta} \mathcal{Y}$, when dynamical effect $\mathcal{Y}$ comprises multiple components over distinct relative timings, the \emph{interaction} among them can lead to \emph{dynamical confounding} within $\mathbb{R}^{O-1}\cup \mathbb{R}^T$.
\end{minipage}}

% %\vspace{1mm}
% \noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.95\textwidth}
% \paragraph{Remark 4.} 
% \hspace{-3mm}
% The generalization process of a structuralized dynamical effect $\mathcal{Y}$ can be geometrically viewed as a linear transformation of causal DAG within the Observed-View space.

% \vspace{-0.5mm}
% \end{minipage}}
% %\vspace{-1mm}


%\vspace{-1mm}
\subsection{Informative Hyper-Dimensional Space}
\label{subsec:framework_info}
\vspace{-1mm}




%As a result, given the relationship $\mathcal{X}\xrightarrow{\vartheta}\mathcal{Y}$; they may be interacted, forming a structural relationship within $\mathbb{R}^O \cup \mathbb{R}^T$. Although the underlying structure is not directly observable, collectively assuming all featured dynamical events confined within $\mathbb{R}^O$ along the single absolute timing $\textbf{t}$ can be problematic.

% Consequently, when considering an observational-temporal effect variable $\mathcal{Y}$ that includes a timing dimension $\tau$, it is not necessary for $\tau$ to be strictly absolute or relative; it may even represent multiple underlying timing dimensions depending on the specific context.


% Most relationship models function within the observational space, maybe incorporating a timeline to depict the observational evolution over time. Consider the following examples: 1) CNNs grasp pixel associations only in the observational space, 2) A quadrotor's trajectory can be identified as a sequence of spatial coordinates, 3) Large Language Models (LLMs) operate along a semantically meaningful timeline representing the word order, 4) The vital signs of patients are recorded chronologically. 

%  in our cognition across the hyper-dimensional and cognitive temporal spaces. Therefore, the causal reasoning AGI that we envisage can be defined as a system fully encompassing unobservable information denoted by $(\vartheta,\omega)$, , to realize the generalizability of structural models across different levels of knowledge.

In summary, our structural causal reasoning can be represented as $(\vartheta,\omega)\in \mathbb{R}^T \cup \mathbb{R}^H$. Accordingly, AGI that meets our expectations should adequately encapsulate informative $\vartheta$ and $\omega$. Here, $\vartheta\in \mathbb{R}^T \cup \mathbb{R}^H$ denotes the structuralized causality within our knowledge, while $\omega\in \mathbb{R}^H$ indicates the ability to capture nonlinearities in all dimensions (including temporal dynamics), to achieve model generalizability.

Figure~\ref{fig:overview} provides a fundamental overview of prevailing relationship models, highlighting their intrinsic limitations, as outlined in Figure~\ref{fig:limit}. In this context, $\vartheta_{\omega}$ is used to represent generalizable causal structures within AGI, and we accordingly summarize the two major obstacles in our pursuit of it.

% The first obstacle arises from the hidden relation $\omega$ (i.e., limitation \fbox{L1}), which limits the ability of the knowledge-driven Logic-View methods (like causal inference-based ones) to generalize models for Observed-View scenarios. 
% The second relates to the cognitive relative timings foundational to our causal knowledge, encompassing structured dynamical effects; these timings are often overlooked by data-driven AI-based methods (i.e., limitation \fbox{L2}), impeding their capacity for causal reasoning.

% Figure environment removed
\vspace{-1mm}


Regular relationship models derive the functional parameter $\theta$ from the correlation between static cause and effect events, $X^t$ and $Y^t$, priorly identified using absolute timestamps. 
Notably, Granger causality \cite{granger1993modelling}, a method well-regarded in economics \cite{maziarz2015review}, introduces separate temporal sequences for cause $X^t$ and effect $Y^{\tau}$, suggesting multiple timings. 
However, without nonlinear computational capabilities over them, 
distinguishing between $\textbf{t}$ and $\tau$ for static timestamps offers limited meaning.
%for their \emph{nonlinear independence}, as this enables the representation of coexisting, varied temporal dynamics, particularly in the effects.
%their nonlinear independence, allowing for relatively distinct and diverse temporal dynamics. 


%\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.95\textwidth}
\paragraph{Remark 4.} 
\hspace{-3mm}
The significance of \emph{Temporal Dimensions} lies in allowing distinct dynamical evolutions.

\vspace{-0.5mm}
\end{minipage}}
%\vspace{-1mm}


As depicted in Figure~\ref{fig:logical} (d), causal inference often omits explicit relative timing axes in causal DAGs, because of the typical exclusion of nonlinear dynamics.
While inherently adopting a \emph{Relation-Oriented} perspective based on the Logical-View structural knowledge $\vartheta_o$, it tends to overlook the Observed-View scenarios with varied $\omega$, thus failing to exhibit causal models' \emph{dynamical generalization} needs.
%the failure to address nonlinearities limits model generalizability in any dimension to adapt with varied $\omega$. 
%Therefore, a preprocessing adjustment is often required, to manually transform a causal DAG (denoted by $\vartheta_o$) into a practical model.
To address this, we suggest enhancing DAGs to visualize dynamical variations across multiple timings, as introduced in Section~\ref{sec:temporal}.

AI-based RNNs are increasingly favored in modern relationship learning \cite{xu2020multivariate}, a trend that reflects their proficiency in handling temporally nonlinear causes. RNNs transform the sequence $X^t$ into a feature representation in the latent space, enabling nonlinear computation over $\textbf{t}$ to effectively fulfill dynamical~$\mathcal{X}$. However, potential dynamics of the effect $\mathcal{Y}$ are often overlooked, resulting in an \emph{imbalanced} causal model function $Y_{t+1}=f(\mathcal{X};\theta)$ with a static outcome $Y_{t+1}$.
This accordingly motivates the emerging trend in \emph{inverse learning} methods \cite{arora2021survey}. Further details will be discussed in Section \ref{sec:causality}.
% Moreover, the overlooked structural $\vartheta$ may result in inherent biases, fundamentally undermining the identifiability of the dynamical entities in AI, forming a vital reason to reconsider the current paradigm, as detailed in Section~\ref{sec:temporal}.

Notably, large language models (LLMs) are able to identify different temporal changes in the semantic space, including nonlinear ones 
\cite{gurnee2023language}. However, ``multiple temporal dimensions'' to accommodate distinct dynamics do not necessarily equate to ``multiple relative timings''. 
%Mutually independent dynamics may represent different temporal dimensions, yet they can still be identified simultaneously from the same absolute timing. In contrast, the $\mathbb{R}^T$ space, comprising relative timings, is capable of accommodating dynamical interactions among these dimensions, as represented by the unobservable association $(\theta^i, \theta^j)\in \mathbb{R}^H$ for any distinct $i,j\in \{1,\ldots, T\}$.

\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.96\textwidth}
\paragraph{Remark 5.} 
\hspace{-3mm}
\emph{Temporal Dimensions} with \emph{\textbf{nonlinear independence}} can be simultaneously identified from absolute timing $\mathbf{t}$ within $\mathbb{R}^O$; while \emph{Relative Timings} indicate potential \emph{\textbf{nonlinear dependence}}, i.e., dynamical interactions, requiring a counterfactual space $\mathbb{R}^T$ to house the underlying structure.

\vspace{-0.5mm}
\end{minipage}}
%\vspace{-1mm}

%the ability to extract the informative parameter $\theta$ distinguishes the anticipated knowledge comprehension by AGI from mere context-associative learning. 
%In language learning tasks, the unobservable $\theta$ represents the semantic relation between phrases $\mathcal{X}$ and $\mathcal{Y}$, 

Current LLMs primarily focus on capturing semantic associations based on the absolute timing $\mathbf{t}$, which indicates the order of phrases. 
This approach categorizes them within the scope of solely observational learning, lacking a \emph{Relation-Oriented} perspective. However, considering the consistent sequential semantics in words, overlooking relative timings is reasonably justifiable for basic context-associative learning needs.

Nevertheless, the association $(\mathcal{X}, \mathcal{Y})$ captured along $\mathbf{t}$ only reflects $\theta$ rather than explicitly representing it, meaning it does not extract $\mathcal{I}(\theta)$.
This may contribute to AI's ability to generate intelligent responses without truly ``understanding'' in the human sense, due to the lack of an informatively represented $\theta$. 

Given the adaptability of meta-learning to diverse observational learning tasks \cite{hospedales2021meta}, its integration with LLMs could significantly improve the generalizability of associative models \cite{lake2023human}. 
This application may enable capturing $(\mathcal{X}, \mathcal{Y},{\omega})$ to reflect hierarchical relations $(\theta, {\omega})$ over the timing $\mathbf{t}$.
However, compared to our goal of explicitly representing $\vartheta_{\omega}$, with encapsulated $(\theta^i, \theta^j)\in \mathbb{R}^H$ for all $i\neq j\in \{1,\ldots, T\}$ to achieve causal reasoning, discussing AGI within the current LLM framework may still be premature. We suggest enabling \emph{Relation-Oriented} meta-learning could be a significant step towards this goal.


%In solely observational learning tasks, such as image recognition, meta-learning is considered advantageous in deriving $\mathcal{I}(\omega)$ due to its inherent adaptability to different $\omega$ across various contexts \cite{hospedales2021meta}. Consequently, integrating meta-learning with LLMs could potentially enhance the hierarchical association $(\mathcal{X}, \mathcal{Y}, \omega)$, making it more generalizable over the $\mathbf{t}$ timing axis alone \cite{lake2023human}.
%Therefore, discussing structural knowledge $\vartheta$ within Large Language Models (LLMs) may still be premature.


% Considering the generally consistent sequential semantics of words, the existence of potential relative timelines can be basically precluded without concerns of inherent biases.  


% aims to derive an informative parameter $\theta$ from observable $\mathcal{X}$ and $\mathcal{Y}$, to fully represent the knowledge denoted by $\theta \in \mathbb{R}^h$.
% Suppose the association $(\theta,\omega) \in \mathbb{R}^h$ represents $n$-levels $\theta$, denoted as $(\theta,\omega) = (\theta_1,\ldots,\theta_n)$.
% The resulting function $\mathcal{Y}=f(\mathcal{X};\theta)$ could be generalizable across $\omega$ if the hierarchical observables $\{(\mathcal{X}_1,\mathcal{Y}_1), \ldots, (\mathcal{X}_n,\mathcal{Y}_n)\}$ are captured.
% % Suppose the resulting $\theta$ purely encapsulates the knowledge in $\mathbb{R}^h$ without any ancillary information from $\mathcal{X}$ and $\mathcal{Y}$, it stands to reason that the function $f(;\theta)$ would be generalizable across any $\omega \in \mathbb{R}^h$, which mirrors the knowledge generalization processes within our cognition.

% %\vspace{1mm}
% \noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\hspace{-2mm}\begin{minipage}{0.95\textwidth}
% \paragraph{Remark 3.} 
% \hspace{-1mm}
% \emph{Generalizability} of relationship model depends on unobservable association $(\theta, \omega) \in \mathbb{R}^h$.

% \vspace{-0.5mm}
% \end{minipage}}
% \vspace{-1mm}


%In essence, the current meta-learning remains confined to observational $(\mathcal{X},\mathcal{Y})_{\omega}\in \mathbb{R}^O$, while an AGI system incorporating knowledge understanding should involve relational information represented by $\vartheta_{\omega}\in \mathbb{R}^T \cup \mathbb{R}^H$.
%It may stand to reason that the key to enabling AGI lies in implementing \emph{Relation-Oriented} meta-learning. 
% This realization underscores the need to move beyond the current \emph{Observation-Oriented} paradigm, which inherently omits relative timelines as computational dimensions. 
% In contrast, \emph{Relation-Oriented} approaches offer the potential for autonomous identification through relation-indexing, thereby granting computational liberty to the temporal dimension.


\section{Neglected Dynamical Effects in Causal Learning}
\label{sec:causality}
%\vspace{-2mm}
% Contrary to mainstream AI applications that primarily explore observable associations, classical causal inference inherently focuses on deriving an informative parameter $\theta$ between observables $X$ and $Y$, providing a robust theoretical foundation.
% Based on that, the conventional causal learning methods have yielded significant contributions over the past decades, establishing a wealth of causal knowledge across various domains  \cite{wood2015lesson, vukovic2022causal,  ombadi2020evaluation}.



%From a modeling perspective, specified timestamps are associated with observations rather than constituting a separate computational dimension. Consequently, in traditional causal inference, the temporal-evolving aspects that distinguish causality from correlation are not directly built into the current modeling framework. 

%Essentially, causality mandates the incorporation of the timeline as a \emph{computational dimension}, ensuring recognition of significant \emph{distributions} on it, ones that undeniably can exhibit \emph{temporal nonlinearity}.

Traditional causal inference emphasizes the interpretability of causal models, particularly in differentiating them from mere correlations. These distinctions, while not inherently integrated into the modeling context, are mainly evident in model interpretations. %Such clarity can guide causally meaningful enhancements of the model.
Despite the statistical basis of causal inference, the importance of nonlinear temporal dynamics is yet to be fully acknowledged. This section focuses on these frequently overlooked dynamics, striving to provide a more intuitive understanding of causal learning.  
%reassess causal learning inquiries, particularly.

%assuredly
% \emph{Observation-Oriented} modeling fades out the causal significance of these relationships in two aspects: First, manual specifications cannot completely identify dynamics of effects for each level; Second, these dynamics might coexist in various relative timelines, which suggests multiple computational dimensions in the Temporal Feature Space.
% Considering these points, we revisit the definition of causality from a modeling perspective:



%\vspace{2mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=red!50!black, colback=yellow!10]{
%\hspace{-2mm}
\begin{minipage}{0.93\textwidth}
\paragraph{Definition 6.} \underline{Causality vs. Correlation} in the modeling context.

$\ \bullet$ Causality $\mathcal{X}\xrightarrow{\vartheta}\mathcal{Y}$ is the relationship neccessitating dynamical effect $\mathcal{Y}\in \mathbb{R}^{O-1}\cup \mathbb{R}^T$.

$\ \bullet$ Correlation $X^t\xrightarrow{\theta}Y^t$ only requires static cause and effect, possibly sequential $X^t$ and $Y^t$.

\end{minipage}}
%\vspace{-1mm}

% exhibits an \emph{imbalance} in capturing dynamics between the cause and the effect (see subsection \ref{sec:causality3_2} for details). For instance, the hierarchical dynamics overlooked in Figure~\ref{fig:hidden} can be fully captured by an inverse model of $do(A)=f(\{B_t,\ldots,B_{t+40}\})$ using RNNs, eliminating the need for a hidden confounder. 


% \vspace{-1mm}
% \subsection{Imbalanced Dynamical Learning}



The timestamp $t$, first introduced by the Picard-Lindelof theorem in the 1890s, initiates the functional form $Y_{t+1}=f(X_t)$ to represent time evolution.
Then, time series learning methods, like autoregressive models \cite{hyvarinen2010estimation}, facilitate the form of $Y_{t+1}=f(X^t)$ using a sequential causal variable $X^t$ with a predetermined time progress from $t$ to $t+1$. 

For RNNs, the latent space optimization over $X^t$ is driven by predicting observed $Y_{t+1}$ value through the parameterized relation $\theta$, enabling the form of $Y_{t+1}=f(\mathcal{X};\theta)$
with a dynamical cause $\mathcal{X}$.
However, the effect $Y_{t+1}$ remains static, with its potential dynamics governed by the function $f$. While $f$ can be selected as linear or nonliner, it influences $\mathcal{X}$ only but still leave the time evolution from $t$ to ${t+1}$ as \emph{\textbf{linear}}. %resulting in static outcome $Y^t=Y_1,\ldots, Y_t$.


% Figure environment removed

The example in Figure~\ref{fig:eff} illustrates the often overlooked effect dynamics in traditional causal models. The action $do(A)$ causes dynamical $\mathcal{B}_{\omega}$ (observed as sequence $B^t$), disentangled by two levels in (b): Level $\mathbf{I}$, the generalized standard sequence $\mathcal{B}_o$ of length 30; Level $\mathbf{II}$, the individualized variations $\mathcal{B}_{\omega}-\mathcal{B}_o$. 
Assume the unobserved individualized characteristics linearly impact $\mathcal{B}_o$, making $\omega = P_i, P_j, \ldots$ simply represent speeds.
% The modeling objective is to obtain $\mathcal{B}_o$, as the effectiveness evaluation of $M_A$.

A typical clinical model, like $B_{t+30}=f(do(A_t))$ that averages all patients' D30 static effects as the outcome, turns to neglect D1-D29 within $\mathcal{B}_o$. However, even adopting a sequential outcome $B^t$ (e.g., Granger causality), it remains challenging to accurately estimate $\mathcal{B}_o$ by linear averaging, not to mention further reaching $\mathcal{B}_{\omega}$.
%estimating $B^t$ by averaging all patients' D1-D30 sequences 
Particularly, it requires the selected records to meet certain criteria, essentially equal to manually defining the boundary of $\mathcal{B}_o$ by exploring all possible ${\omega}$ values. %: an exact 30-day span on average, the near-linear variations among patients, the near-normal variational distributions centered on D30, and others.


%involves manually delineating the population-level effect $\theta$ while omitting $\omega$ influences. 

%at best, only the population-level sequence can be accurately estimated by averaging over all patients. This approach, however, excludes further dynamic feature levels, such as individual-level speed.

Such hierarchical dynamical effects are prevalent in fields like epidemic progression, economic fluctuations, strategic decision-making, etc. They often rely on a similar strategy to manually identify specific levels, e.g., the group-specific learning methodology \cite{fuller2007developing}. 
These approaches have become impractical in AI-based applications and may lead to notable information loss in large-scale structural models. 
%examinations over $\omega$ values
%However, these traditional approaches are impractical in the context of large-scale relational learning within AI applications. 
%Particularly in structural relationships, without manual specifications for each variable, their interchangeable roles of cause and effect can lead to amplified errors notable in the output.



%Dynamic effects, prevalent in applications like epidemic progression, economic fluctuations, and strategic decision-making, often manifest at different levels of granularity. %These levels - as a type of hidden relations - are identifiable by our cognition but not directly observable in the data. 
%Group-specific learning methodologies \cite{fuller2007developing} are typically employed to address these issues, essentially serving as a manual specification of the value of $\omega$ 



% \noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.95\textwidth}
% \paragraph{Remark 2.} RNNs extract dynamic nonlinearity from the cause by indexing via the relation $\theta$.
% \vspace{-1mm}
% \end{minipage}}


\subsection{Identification Difficulty of Nonlinear Effect}
\label{subsec:caus_identif}
\vspace{-1mm}
%requires specifying the sequential outcome $Y^t=(Y_1,\ldots,Y_t)$ to represent $\mathcal{Y}$. Then, a functional

%In a relationship $\mathcal{X} \xrightarrow{\theta} \mathcal{Y}$, 

The \emph{Observation-Oriented} modeling usually pre-identifies static effects based on existing knowledge, such as $Y_t=f(\mathcal{X};\theta)$, to derive $\theta$ and accordingly output static sequential estimations $\hat{Y_{\theta}^t}=\hat{Y_1},\ldots,\hat{Y_t}$. 
Yet, two types of errors may challenge the accuracy: the discrepancy between the targeted nonlinear (i.e., dynamical) effect and the specified outcome sequence $\mid\mathcal{Y}-Y^t\mid$; and the approximation error due to the predetermined model function $f(;\theta)$. They contribute to the identification difficulty in causal learning \cite{zhang2012identifiability}.

%The formulated rough boundary $\bigr[ f(\mathcal{X};\theta), Y^t \bigr]$ can facilitate a more accurate convergence towards $\hat{\mathcal{Y}_{\theta}}$.

%While the use of neural networks, such as RNNs (to extract nonlinear $\theta$ from $\mathcal{X}$ without a predetermined distribution), can effectively bypass the explicit specification of $f(;\theta)$, the need to pinpoint $\mathcal{Y}$ closely to $\hat{\mathcal{Y}_{\theta}}$ persists. Moreover, such \emph{Observation-Oriented} identification predominantly captures the observational features of $\mathcal{Y}$, excluding its (dynamical) temporal ones.

Specifically, due to static outcome, the burden of representing the dynamical aspects of $\mathcal{Y}$ shifts either to $f(;\theta)$ or to $\mathcal{X}$. In the former scenario, a factor $\sigma$ signifying ``disturbance'' is integrated into the function, resulting in $f(;\theta+\sigma)$ \cite{zhang2012identifiability}. In the latter case, as treated in do-calculus \cite{pearl2012calculus, huang2012pearl}, the dynamical $\mathcal{X}$ needs to be manually discretized as temporal events to ensure their identifiable effects. 
This enables a fluid transformation from dynamical cause to observational effect,  but the identifiability relies on non-experimental data (controllable $\theta$) and can introduce additional complexities. 


% To avoid specifying time sequences for causes, do-calculus \cite{pearl2012calculus, huang2012pearl} targets \emph{identifiable} events, 

Considering the \emph{differential} essence of do-calculus, we provide a streamlined reinterpretation of its three core rules from an \emph{integral} viewpoint.
Let $do(x_t)=(x_t, x_{t+1})$ indicate the occurrence of an instantaneous event $do(x)$ at time $t$, with the time step $\Delta t$ appropriate to ensure the \emph{interventional} effect of $do(x_t)$ identifiable as a function of the resultant distribution at $t+1$. Meanwhile, a separate \emph{observational} effect is provoked by the static $x_t$ value. Then, the dynamical cause $\mathcal{X}$ can be discretized as below:

\vspace{-6mm}
\begin{align*}
    \text{Given } \mathcal{X} & \xrightarrow{\theta} Y, \text{ where } \mathcal{X} = \langle X,t \rangle \in \mathbb{R}^{d+1} \text{ with the augmented $\textbf{t}$ dimension residing a $l$-length sequence,} \\
    \mathcal{X} =& \int_0^l do(x_t) \cdot x_t \ dt \text{\ \ with }
    \begin{cases}
      (do(x_t)=1) \mid \theta, & \text{ \emph{Observational} only (Rule 1) } \\
      (x_t=1) \mid \theta, & \text{ \emph{Interventional} only (Rule 2) }    \\
      (do(x_t)=0) \mid \theta, & \text{ No \emph{interventional}  (Rule 3) }   \\
      \text{otherwise} & \text{ Associated \emph{observational} and \emph{interventional} }
    \end{cases} \\
     \text{The effect } & \text{of } \mathcal{X} \text{ can be derived as }
     f(\mathcal{X}) = \int_0^l f_t \big( do(x_t) \cdot x_t \big) \ dt = \sum_{t=0}^{l-1} (y_{t+1}-y_t) =y_l-y_0
\end{align*}
\vspace{-5mm}

Based on a controllable $\theta$, it addresses three criteria that can preserve conditional independence between \emph{observational} and \emph{interventional} effects, completing the chain rule, but sidesteps more generalized cases. If oppositely defining $\mathcal{Y}=\langle Y,\tau \rangle$ as a dynamical effect, discretizing it in $do(y)$ remains necessary. % under the current paradigm, while the proposed one is designed to construct $\mathcal{Y}$ autonomously.






% Unlike our cognitive processes, where the effect can be identified via knowledge $\theta$ from the cause, under the \emph{Observation-Oriented} paradigm struggles - Even if $Y$ is designated as a sequence, its nonlinear dynamics within the $\tau$-dimension remain unaccounted for by the model, which takes the form $\mathcal{X} \xrightarrow{\theta} Y$ for details).

\subsection{Imbalance between Cause and Effect}
\label{subsec:caus_imbalance}

For the modeling computation, causal directionality (i.e., the roles of cause and effect)
may not impose restrictions, although it is often emphasized in model interpretations. 
Specifically, when selecting a model function for $X\rightarrow Y$,  one could use $Y=f(X;\theta)$ to predict the effect $Y$, or $X=g(Y;\phi)$ to inversely infer the cause $X$. Both parameters, $\theta$ and $\phi$, are obtained from the joint probability $\mathbf{P}(X, Y)$ without imposing modeling constraints. We refer to this as \emph{symmetric directionality} for clarity.

Concern for causal model direction mainly arises for two reasons: 1) it aligns with our intuitive understanding of temporal progression, and 2) there is an inherent \emph{\textbf{imbalance}} between how causal models capture dynamics of the cause and the effect, with RNNs as a typical example, formulated as $Y=f(\mathcal{X};\theta)$.

Given the symmetric directionality, inverse learning methods \cite{arora2021survey} capitalizing on this imbalance have recently garnered increasing attention, to achieve autonomous effect identification by inversely assigning the effect as the cause within RNNs. 
It is suitable for relationships along a single absolute timing, but not for addressing causal structures represented by $\vartheta$. Specifically, the neglected relative timings implicitly assume nonlinear independence between distinct dynamical effects, which could lead to inherent bias regardless of the modeling direction.
This will be further detailed in  Section~\ref{sec:temporal}.

%\emph{symmetric directionality} to sidestep the challenge of identifying dynamical effects and defining the objective function.

Another factor contributing to this imbalance is the increased empirical difficulty when specifying effect sequence $Y^t$ compared to cause sequence $X^t$. While organizing time series data around a major causal event (e.g., days of heavy rain) is feasible, pinpointing the precise onset of subsequent effects (e.g., the exact day a flood began due to the rain) remains a more complex task.

%\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.95\textwidth}
\paragraph{Remark 6.} By indexing through $\theta$, the optimization of $\mathcal{X}$ and $\mathcal{Y}$ can be achieved simultaneously, mitigating their imbalance and enabling autonomous effect identification.
%With a known relation $\theta$ from observables $\mathcal{X}$ to $\mathcal{Y}$, extracting the representation of $\mathcal{Y}$ by indexing through $\theta$ enables autonomous dynamical effect identification in the $\mathcal{X}\xrightarrow{\theta}\hat{\mathcal{Y}_{\theta}}$ relationship.

\vspace{-1mm}
\end{minipage}}

The proposed \emph{Relation-Oriented} modeling aims to derive $\theta$ between feature representations of $\mathcal{X}$ and $\mathcal{Y}$ within a latent space $\mathbb{R}^L$. 
Specifically, initially specified sequences $X^t$ and $Y^t$ are transformed into $\mathbb{R}^L$, enabling nonlinear computational freedom on their temporal dimensions. Then, a neural network without functional model assumption can derive $\theta$ by forming the optimization stream $\mathcal{X} \xrightarrow{\theta} \mathcal{Y}$ 
in $\mathbb{R}^L$.
%within a unified training process.
%enables the creation of two dynamical representations via a single optimization.

The training process uses $\mathcal{X}$ as the input and $\mathcal{Y}$ as the output, indexed through $\theta$, facilitating the concurrent optimization of both dynamical representations. It consequently yields $(\mathcal{X},\theta,\hat{\mathcal{Y}_{\theta}})$ in a sequential association, with each individual representation maintained. The implementation will be introduced in Chapter II.
% while maintaining the individual representations of $\mathcal{X}$ and $\mathcal{Y}$. 
% and achieve relation-indexed optimization for both dynamical $\mathcal{X}$ and $\mathcal{Y}$ simultaneously.



% Contrary to the $\mathcal{X}$ in RNNs that acts as an observational-temporal variable, in traditional methods, sequences $\{x_t\}$ and $\{y_\tau\}$ are purely observational, potentially capturing a limited length of static temporal features and heavily depending on accurate specifications of hidden relations $\omega$.
% Consider Figure \ref{fig:eff}: estimating the level $\textbf{I}$ sequence by averaging patient data from Day1-Day30 necessitates specific criteria, such as a precise 30-day length, near-linear individual diversity, and a normal distribution around Day30. In essence, this involves manually delineating the population-level effect $\theta$ while omitting $\omega$ influences. 



% \section{Causal Inference Challenges}
% \label{sec:application}

% \subsection{Incomplete Causal Effect}
% \label{subsec:appl_incomp}
% \vspace{-1.5mm}




%\vspace{-2mm}
\subsection{Interpretation Complexity}
\label{subsec:appl_elusive}
\vspace{-1mm}


%For patients $P_i$ and $P_j$, the population-level last-day effect $B_{t+30}$ is inaccurate. To counter this individual-level bias and improve model interpretation, statistical causal inference incorporates the ``hidden confounder'' concept into Directed Acyclic Graphs (DAG), %representing the concealed $\omega$ as node $E$ in Figure~\ref{fig:hidden} (a).

To deal with the often-overlooked dynamical effects, traditional causal inference introduces the concept of ``hidden confounder'' to enhance model interpretability. 
For example, the node $E$ in Figure~\ref{fig:hidden} (a) signifies the unobserved individualized characteristics in the scenario depicted in Figure~\ref{fig:eff}. 

However, this approach does not necessarily require collecting additional data to identify $E$. 
This might lead to an illogical implication: ``Our model is biased due to some unknown factors we dont intend to know.'' Indeed, this strategy employs a solely observational causal variable $E$ to account for the overlooked dynamical aspects of the effect. While $E$ remains unknown, its inclusion can complete the model interpretation.
Yet, from the modeling perspective, as illustrated in Figure~\ref{fig:hidden}(b), the associative cause $do(A) * E$ remains unknown, failing to provide a modelable relationship for addressing $(\theta, \omega) = \begin{pmatrix} \theta_o \\ \theta_{\omega} \end{pmatrix}$.


% Figure environment removed
\vspace{-1mm}

% compensates for the overlooked level $\mathbf{II}$ dynamic, which essentially transforms an \emph{observable} dynamical feature of the \emph{effect} into a \emph{hidden} observational variable, $E$, associated with the \emph{cause} $do(A)$. 

Incorporating hidden confounders aims to enhance the model's interpretability, though it does not necessarily improve generalizability. In contrast, our \emph{Relation-Oriented} approach bypasses the need to identify cause and effect but simply leverages $\theta$ as the index to extract $\hat{\mathcal{Y}_{\theta}}$, allowing the use of any observed identifier associated with $\omega$ (e.g., patient IDs). As demonstrated in section (c), this method effectively disentangles effect representations hierarchically in the latent space, thereby achieving model generalizability.


%\vspace{2mm}
\subsection{Causal Assumptions Reliance}
\label{subsec:appl_assumpt}
\vspace{-1mm}
Another consequence of the often-overlooked dynamical effects is the reliance of causal models on foundational assumptions to validate their practical applications. In Figure~\ref{fig:view}, we categorize causal model applications into four scenarios based on two aspects: 1) depending on whether the predetermined function $f(;\theta)$ is supported by knowledge, they are divided into Causal Discovery and Causation Buildup; 2) the dynamical significance of effects further differentiates them as causality and correlation from the modeling perspective.

%For example, the causal relationship ``raining $\rightarrow$ wet floor'' falls into area $\circled{4}$, while ``raining $\rightarrow$ floor becoming wetter'' is in area $\circled{3}$.
%They will be examined from two perspectives in the following: the modeling objective Relation (i.e., $\theta$), and the interpretational Directionality.


% Figure environment removed
%\vspace{-2mm}


% %\vspace{4mm}
% \subsubsubsection{\emph{(1) Modeled Relation}}
% \vspace{-1mm}

Within a generalized-level causation buildup (e.g., the scenario in Figures~\ref{fig:eff}), the dynamical features of the individualized level can be easily overlooked. Based on knowledge, some unobserved entities may be identified as hidden confounders, to enhance model interpretations. Nonetheless, if such identification is not easy, the foundational \emph{Causal Sufficiency} assumption may lead to neglect of these features, presuming that all potential ``hidden confounders'' have been observed in the system.

On the other hand, causal discovery typically detects relation structures based on observational dependences but excludes the dynamical features of the observables. If these features are not crucial, the captured dependencies can provide valuable insights into the underlying correlations. Otherwise, significant dynamics may be neglected due to the \emph{Causal Faithfulness} assumption, which suggests that the captured observables can fully represent the underlying causal reality.

% \vspace{-1mm}
% \subsubsubsection{\emph{(2) Modeled Causal Direction}}
% \vspace{-1mm}

Furthermore, although the discovered relationships are directional, these directions frequently lack a logical causal implication.
Consider $X$ and $Y$ with predetermined directional models $Y=f(X;\theta)$ and $X=g(Y;\phi)$. The direction $X\rightarrow Y$ would be favored if $\mathcal{L}(\hat{\theta}) > \mathcal{L}(\hat{\phi})$. 
Let $\mathcal{I}_{X,Y}(\theta)$ denote the information about $\theta$ given $\mathbf{P}(X,Y)$. Using $p(\cdot)$ as the density function, $\int_X p(x;\theta) dx$ remains constant in this context. Then:

\vspace{-6mm}
\begin{align*}
    \mathcal{I}_{X,Y}(\theta) &= \mathbb{E}[(\frac{\partial }{\partial \theta} \log p(X,Y;\theta))^2 \mid \theta] 
    = \int_{Y} \int_X (\frac{\partial }{\partial \theta} \log p(x,y;\theta))^2 p(x,y;\theta) dx dy \\
    &= \alpha \int_Y (\frac{\partial }{\partial \theta} \log p(y;x,\theta))^2 p(y;x,\theta) dy + \beta = \alpha \mathcal{I}_{Y\mid X}(\theta)+\beta, \text{with } \alpha,\beta \text{ being constants.} \\
    \text{Then, } \hat{\theta} &=\argmax\limits_{\theta} \mathbf{P}(Y\mid X,\theta) = \argmin\limits_{\theta} \mathcal{I}_{Y\mid X} (\theta) =\argmin\limits_{\theta} \mathcal{I}_{X,Y}(\theta), \text{ and } \mathcal{L}(\hat{\theta})  \propto 1/\mathcal{I}_{X,Y}(\hat{\theta}).
\end{align*}
\vspace{-5mm}

%The likelihoods of $\hat{\theta}$ and $\hat{\omega}$ rely on the information $\mathcal{I}(\hat{\theta})$ and $\mathcal{I}(\hat{\omega})$. 
The inferred directionality indicates how informatively the observational data distribution can reflect the two predetermined parameters.
Consequently, such directionality is unnecessarily logical but could be dominated by the data collection process, with the predominant entity deemed the ``cause'', consistent with other existing conclusions \cite{reisach2021beware, kaiser2021unsuitability}.
%rather indicative of distributional dominance as determined by the data collection process. Here, the predominant entity is labeled the ``cause''.
Even when $\theta$ and $\phi$ are predetermined based on knowledge, they might not provide insights for dynamically significant causal relations.

%they only link to solely observational features of entities, and thus may not accurately reflect true causal relations.
%in the presence of dynamically significant effects.

%their designated distributions appear in the data, %with the predominant one deemed the ``cause'' - It assumes, by default, that observations capture the cause more thoroughly than the effect. While limited data collection techniques made it reasonable in the past, it is no longer safe to assume such observationally inferred directions to hold logical meaning for causality.



\section{Relative Timings in Structural Causality}
\label{sec:temporal}



% Consider medical trial data from hospital patients. Vital signs and medication usage are recorded daily, forming the chronological \emph{absolute timeline}. However, to assess the effects of a specific medication, a \emph{relative timeline} is constructed, with time-zero marking a consistent action, such as $do(A)$, for all patients. As a result, events with different chronological timestamps can align on the relative timeline, and vice versa.
% For instance, Figure \ref{fig:eff} illustrates a relative timeline for the effects of $do(A)$, while Figure~\ref{fig:do1}(a) revisits its causal DAG, incorporating the introduced hidden confounder. 




To visualize the dynamical variations across multiple relative timings, we propose an enhancement to the conventional causal DAGs, which will be utilized in the following sections. 
Figure~\ref{fig:do1}(a) revisits the example in Figure \ref{fig:hidden} with hidden-confounder, while the enhancement shown in (b) is carried out through two steps:
\begin{enumerate}[itemsep=0em, topsep=-1pt, 
parsep=2pt, partopsep=-1pt,
leftmargin=20pt, labelwidth=10pt]
    \item Consider dynamical effects to integrate necessary relative timings as explicit axes.
    \item Use edge lengths to denote timespans for reaching a certain effect magnitude, signified by a static value.
    %the effects to reach an equivalent magnitude.
\end{enumerate}

% Figure environment removed
\vspace{-2mm}

Section \ref{subsec:temp_bias} presents the concept of \emph{inherent bias} via an intuitive example alongside definitive discussions; section \ref{subsec:temp_generalz} explores its essential impact on the generalizability of structural models; finally, section \ref{subsec:temp_toward} delves into the advancements and challenges we face in achieving structuralized causal reasoning within AI.



%To more effectively address this issue, the causal DAG (directed acyclic graph) is enhanced in two ways: 1) by incorporating desired logical timelines as axes into the DAG space, and 2) by assuming causal effects are dynamically significant, with varying edge lengths indicating different timespans required to achieve identical effects.
%For instance, Figure~\ref{fig:do1}(a) revisits the hidden-confounder example, which aims to interpret different individualized effects. Alternatively, the enhanced DAG shown in (b) provides a convenient representation of these effects.


\subsection{Scheme of the Inherent Bias}
\label{subsec:temp_bias}
\vspace{-1mm}

Figure~\ref{fig:do3}(a) shows an example causal structure $\mathcal{B} \xleftarrow{} do(A) \xrightarrow{} \mathcal{C}$ extended from the Figure~\ref{fig:do1}(b) scenario, featuring two medical effects $\mathcal{B}$ and $\mathcal{C}$, on two distinct vital signs $B$ and $C$, respectively.
The primary effect $\mathcal{B}$ is represented as edge $\overrightarrow{AB}$ along $\mathbf{t_1}$, and similar for the side effect $\mathcal{C}$.
For simplicity, we assume \emph{nonlinear independence} between $\mathcal{B}$ and $\mathcal{C}$ by fixing the timespan of $\overrightarrow{AC}$ at $10$ days for all patients (i.e., $\mathcal{C}$ is dynamically insignificant, denoted as $\mathcal{C}=C$), and focus on predicting a static outcome $B$, as the average primary effect for the present population.
Notably, $C$ can influence $B$ through the \emph{static interaction} edge $\overrightarrow{CB}$.

%the primary effect $\mathcal{B}$ via $\theta_1$, represented by the edge $\overrightarrow{AB}$ leading to a static value for vital sign $B$; and a side effect $\mathcal{C}$ via $\theta_2$ on another vital sign $C$, indicated by edge $\overrightarrow{AC}$. Notably, $\mathcal{C}$ can influence $\mathcal{B}$, creating \emph{confounded dynamics} across two timing axes $\mathbf{t_1}$ and $\mathbf{t_2}$.
%which is in scenario 2), 
%suggesting that $A$ also indirectly affects $B$ through $C$, 

% Figure environment removed
\vspace{-1mm}

%For simplicity, we assume $\theta_1\perp\theta_2 \in \mathbb{R}^H$, with the timespan of $\overrightarrow{AC}$ fixed at 10 days for all patients, and focus on modeling the static outcome $B$ to predict the average fully-released medical effect in this population.

From a geometrical view, the triangle over nodes $\{A, B, C\}$ should remain closed for all populations and individuals to represent the same relationship, as supported by the \emph{Causal Markov} condition. Accordingly, the generalization (and also individualization) process can be geometrically viewed as a \emph{linear transformation} of this DAG, depicted as ``stretching'' the triangle along $\mathbf{t_1}$ at various ratios, as in Figure~\ref{fig:do3}(a).

In conventional SCMs, the status of $B$ is typically identified by setting an average timespan in absolute timing $\mathbf{t}$, for full medicine release along $\overrightarrow{AB}$, say 30 days in this case. As shown in Figure~\ref{fig:do3}(b) and (c), the SCM function fails to shape a valid DAG for individual patients, $P_i$ in red and $P_j$ in blue. Sequential biases would be implied when extending to estimate a sequential outcome like $B^t=B_1,\ldots, B_{30}$. %the $B^t$ sequences for all patients would be treated as i.i.d. outcomes, implying sequential biases. 


\vspace{1mm}
\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=red!50!black, colback=yellow!10]{
%\hspace{-2mm}
\begin{minipage}{0.94\textwidth}
\paragraph{Definition 7.}  
The \emph{\underline{Inherent Bias}} in SCM assuming \emph{no interactions}. 

The \emph{inherent bias} may occur within pre-identified effects if existing: 
1) dynamical significance of effects, 2) confounding with interactions across multiple relative timings, and 3) undetectable hierarchy.

\end{minipage}}


Given a structure $\mathcal{Y} \xleftarrow{\theta_1} do(X) \xrightarrow{\theta_2} \mathcal{Z}$,
%Initially, $\mathcal{Y}$ and $\mathcal{Z}$ are identified as sequences $Y^t$ and $Z^t$ according to absolute timing $\mathbf{t}$. 
there exist three scenarios regarding interaction between dynamical effects $\mathcal{Y}$ and $\mathcal{Z}$: 1) \emph{no interaction}; 2) only a \emph{static interaction} between them, implying their \emph{linear dependence} and forming a confounding; 3) A \emph{dynamical interaction} between them, implying their \emph{nonlinear dependence} and forming a dynamical confounding. 
Figure~\ref{fig:interact} illustrates these definitions.

% Figure environment removed
%\vspace{-1mm}


\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.94\textwidth}
\paragraph{Remark 7.} The \emph{\textbf{inherent bias}} remains to exist within AI-based SCMs, which assume \emph{nonlinear independences} (i.e., scenarios 1 and 2) among the captured dynamical effects.
\end{minipage}}

The second scenario, defined as $\theta_1 \propto \theta_2 \in \mathbb{R}^H$, serves as a special case of the third, $(\theta_1, \theta_2) \in \mathbb{R}^H$. This presupposes the dynamically significant effects do not exhibit nonlinear temporal dependence, among their temporal dimensions.
Indeed, it is practically challenging to distinguish the three scenarios from data alone without prior knowledge. Thus, making the assumption $(\theta_1, \theta_2) \in \mathbb{R}^H$ becomes a cautious default setting.

To construct an SCM, the effects $\mathcal{Y}$ and $\mathcal{Z}$ are initially identified as $Y^t$ and $Z^t$ in absolute timing~$\mathbf{t}$.
If satisfying $\theta_1 \propto \theta_2 \in \mathbb{R}^H$, AI models like inverse RNNs can accurately capture $\vartheta=\overrightarrow{\theta_1\theta_2}$ by constructing $do(X)=f\bigl((Y, Z)^t;\vartheta\bigr)$ using their associative identification $(Y, Z)^t=\bigl((Y, Z)_1, \ldots,(Y, Z)_t\bigr)$. However, under a more general condition $(\theta_1, \theta_2) \in \mathbb{R}^H$, it may introduce \emph{\textbf{inherent bias}}, as neglecting dynamical interactions among dynamical effects, as highlighted in Remark 7. Its scheme is similar to Definition 7, where the conventional SCM assumes linearly independent static effects, i.e., $\mathcal{Y}=Y$, $\mathcal{Z}=Z$, and $\theta_1 \perp \theta_2$.

%consequently reducing the model's robustness and generalizability.
It is essential to adopt a two-step \emph{relation-indexed learning} to sequentially obtain models $\mathcal{Y}=f_1(do(X);\theta_1)$ and $\mathcal{Z}=f_2(do(X)\mid \mathcal{Y};\theta_2)$.
Without this approach, when modeling large-scale causal structures, the inherent biases can accumulate within AI models, compromising robustness and leading to irrational outputs.

% \vspace{1mm}
% \noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=red!50!black, colback=yellow!10]{\begin{minipage}{0.96\textwidth}
% \paragraph{Theorem 3.}The \emph{\textbf{inherent} bias} may occur in SCM if it contains: 
% 1) \emph{Confounded} dynamical causal effects across \emph{Multiple} logical timelines, and 2) Unobservable Hierarchy (represented by $\omega$).
% \end{minipage}}
% \vspace{1mm}

%Interestingly, most successful causal applications instinctively avoid either \emph{confounding} or \emph{multi-timeline}. Causal inference typically employs de-confounding (like PSM \cite{benedetto2018statistical} and backdoor adjustment \cite{pearl2009causal}), to mitigate inherent bias and other confounding biases resulting from unaddressed nonlinearity.
%Meanwhile, many AI accomplishments, including Large Language Models (LLMs), which operate in a semantic space, do not inherently deal with relative timelines, maintaining words consistently ordered along a singular timeline.



\subsection{Inherently Restricted Generalizability}
\label{subsec:temp_generalz}
\vspace{-1.5mm}

To address the inherent biases within conventional SCMs, traditional causal inference uses various methods to perform ``de-confounding'', such as propensity score matching \cite{benedetto2018statistical},  backdoor adjustment \cite{pearl2009causal}, etc. 
They aim to cut off the static interactions between dynamical effects, although without explicitly recognizing these dynamics.
These techniques typically rely on intended tailoring for each specific application. 
Given the black-box nature and the large scale of AI models, such manual identification and adjustment approaches become increasingly impractical for modern causal learning inquiries.

Moreover, they primarily deal with linear dependence only, to adapt to statistical linear models, which may not contribute to dynamical generalizability.
Subsequently, we employ a practical example to illustrate how effect identifications inherently hinder SCM's generalizability, under the general condition $(\theta_i, \theta_j) \in \mathbb{R}^H$.

%Consequently, while still operating under the \emph{Observation-Oriented} paradigm, AI-based causal learning tends to default to the absolute timeline, which is the only directly observable one in the data, to specify timestamps for all events. 
%This method can lead to \emph{inherent biases} accumulating over the structural complexity, ultimately affecting the model's robustness and generalizability.


Figure~\ref{fig:3d} displays a 3D view enhanced DAG, where $\Delta t$ and $\Delta \tau$ signify actual time spans for the present population, to support their causal reasoning represented by this structure. 
%Yet, the crux is not on determining their exact values, but on realizing their intended causal relationship: 
For the triangle $SA'B'$, as each unit of effect from $S$ delivered to $A'$ (spent $\Delta \tau$), it immediately starts to impact $B'$ through $\overrightarrow{A'B'}$ ($\Delta t$ needed); meanwhile, the next unit begins generation at $S$. This dual action runs concurrently until $S$'s effect fully reaches $B'$, represented as the single edge $\overrightarrow{SB'}$ within this SCM.
%At $B'$, the ultimate aim of this process is to evaluate the total cumulative influence stemming from $S$.

%($=\frac{1}{2} \overrightarrow{AB'}$)
Due to the equation $\overrightarrow{SB'} = \overrightarrow{SA'} + \overrightarrow{A'B'}$, specifying the time span of $\overrightarrow{SB'}$ inherently determines the $\Delta t:\Delta \tau$ ratio based on the current population's performance, thereby fixing the shape of the ${ASB'}$ triangle in the DAG space. If we focus solely on the accuracy of the estimated mean effect for this population, the SCM function $B'=f(A, C, S)$ may be effective. However, given that the preset $\Delta t:\Delta \tau$ ratio is not universally applicable, the generalizability of the established SCM to other populations becomes questionable.

% Figure environment removed

\subsection{Developments Toward Causal Reasoning AI}
\label{subsec:temp_toward}
\vspace{-1.5mm}

To pursue causal reasoning in machine learning, our modeling techniques have evolved from capturing mere associations to observational correlations, ultimately advancing to build structural causal models spinning the counterfactual temporal space $\mathbb{R}^T$. Figure~\ref{fig:model} summarizes this evolution in an upward trajectory.

\vspace{-2mm}
% Figure environment removed
\vspace{-3mm}

Given AI's capability to learn nonlinear dynamics, the present challenge is incorporating the underlying dynamical interactions within the causal knowledge structure. 
%As shown in sections \ref{subsec:temp_bias} and \ref{subsec:temp_generalz}, conventional SCMs lack the ability to capture dynamics. Even with dynamical independence, where only linear dependence is present, specifying timestamps to identify outcomes can still risk introducing inherent biases. Therefore, 
Considering the risk of introducing inherent biases, finding a new modeling paradigm is crucial to realizing causal knowledge-aligned AI.
%transitioning away from the current \emph{Observation-Oriented} approach. 
Physical models, explicitly incorporated in temporal dimensional computation, may offer valuable insights into this prospect.

% and are able to establish abstract concepts through relations, may provide insights into these challenges. 
% The relation-indexing approach is designed to bridge the gap between the Observational and Temporal Spaces.
%\pagebreak
%is to ensure the generalizability of structural causal AI models. Recognizing multi-timeline dynamics is essential to avoid biases that obscure AI interpretability. Given the impracticality of manually discerning all potential logical timelines for observable data, it might be time to contemplate a new paradigm.

Under the observational i.i.d. assumption, initial models only approximate associations, proved unreliable for causal reasoning \cite{pearl2000models, peters2017elements}. Subsequently, the common cause principle highlights the significance of the nontrivial condition, to distinguish a relationship from statistical dependencies \cite{dawid1979conditional, geiger1993logical}, providing a basis for constructing graphical models \cite{peters2014causal}.
The initial graphical model relies on conditional dependencies to construct Bayesian networks, with limited causal relevance \cite{scheines1997introduction}. Then, causally significance emphasizes the capability of addressing counterfactual queries \cite{scholkopf2021toward}, like the structural equation models (SEMs) and functional causal models (FCMs) \cite{glymour2019review, elwert2013graphical}, which leverage prior knowledge to establish causal structures.

State-of-the-art deep learning on causality encodes the discrete, DAG-structural constraint into continuous optimization functions \cite{zheng2018dags, zheng2020learning, lachapelle2019gradient}, enabling advanced efficiency, but without noticeable generalizability, evident from the restricted successes in applications like the neural architecture search (NAS) \cite{luo2020causal, ma2018using}. This is reasonable since the neglected interactions among relative timings can lead to inherent biases amplified through complex structures to become significant.
\cite{scholkopf2021toward} summarized our confronting key challenges toward generalizable causal-reasoning AI: 1) limited model robustness, 2) insufficient model reusability, and 3) inability to handle data heterogeneity (i.e., undetectable hierarchies). They are intrinsically linked to the demonstrated inherent biases.

% noting that these challenges can be attributed to the timestamp specification required by \emph{Observation-Oriented} SCMs.





\begin{center}
   {\vskip 8pt\large\bf Chapter II: Realization of Proposed Relation-Oriented Paradigm} 
\end{center}

This chapter introduces the proposed \emph{Relation-Indexed Representation Learning} (RIRL) method, a baseline realization of the raised \emph{Relation-Oriented} modeling paradigm.
RIRL primarily focuses on autonomously identifying dynamical effects, in the form of relation-indexed representations in the latent space. 
In the context of structural modeling, RIRL enables hierarchical disentanglement of effects, according to given DAGs, as a manner of realizing dynamical generalizability across undetectable levels within knowledge.
As a baseline realization, RIRL is suitable for applications with mature structural causal knowledge,
and plenty of data to support neural network training on each known causal relationship.

First, Section \ref{sec:representation} details the technique for extracting relation-indexed representations. Then, building on this, Section \ref{sec:RIRL} presents the RIRL method of establishing structural causal models in the latent space. Lastly, Section \ref{sec:experiment} provides experiments to validate RIRL's efficacy in autonomously identifying effects.


%provides formulations of factorizations, to achieve of achieving hierarchical disentanglement through relation-indexing

%\vspace{-2mm}
\section{Relation-Indexed Representation}
\label{sec:representation}

In the relationship $\mathcal{X}\rightarrow \mathcal{Y}$, we define dynamical $\mathcal{X}=\langle X, t \rangle \in \mathbb{R}^{d+1} \subseteq \mathbb{R}^O$ and $\mathcal{Y} = \langle Y, \tau \rangle \in \mathbb{R}^{b+1} \subseteq \mathbb{R}^O$, given their solely observational variables, $X\in \mathbb{R}^d$ and $Y\in \mathbb{R}^b$.
$\mathcal{X}$ is observed as a data sequence, represented by $X^t = X_1, \ldots, X_t$ with a pre-determined length $l_x$.
For clarity, hereafter in this chapter, its instance $x^t$ will be considered as a $(d*l_x)$-dimensional vector, denoted by $\overrightarrow{x}$ (or $x$ for briefty). 
Similarly, $\mathcal{Y}$ is observed as the data sequence $Y^t$ with a pre-determined length $l_y$, and its instance is referred to as a $(b * l_y)$-dimensional vector $\overrightarrow{y}$ (or $y$ for briefty). 

The relation-indexed representation aims to formulate $(\mathcal{X},\theta,\hat{\mathcal{Y}_{\theta}})$ in the latent space $\mathbb{R}^L$, beginning with an \emph{initialization} to transform $X^t$ and $Y^t$ to be latent space features.
For the sake of clarity, we use 
$\mathcal{H} \in \mathbb{R}^L$ and $\mathcal{V} \in \mathbb{R}^L$ to refer to the latent representations of $\mathcal{X}\in \mathbb{R}^O$ and $\mathcal{Y}\in \mathbb{R}^O$, respectively.

The modeling process is to optimize the neural network function $f(;\theta)$ in $\mathbb{R}^L$, with $\mathcal{H}$ as its input and $\mathcal{V}$ as the output. This process simultaneously refines $\mathcal{H}$, $\theta$, and $\mathcal{V}$, for ultimately achieving $(\mathcal{H}, \theta, \hat{\mathcal{V}_{\theta}}) = (\mathcal{X},\theta,\hat{\mathcal{Y}_{\theta}})$. The refining will present as the distance minimization between $\mathcal{H}$ and $\mathcal{V}$
within $\mathbb{R}^L$. 
Consequently, the dimensionality $L$ of the latent feature space must satisfy $L \ge rank(\mathcal{X},\theta,\mathcal{Y})$, raising a technical challenge that $L$ is larger than the dimensionality of $\overrightarrow{x}$ or $\overrightarrow{y}$. %, where the observable data of $\mathcal{X}$ presents to be $d * T_x$ dimensional.
%in a latent vector with a length $L$ that certainly surpasses its rank $rank(\mathcal{H})$, and possibly its original length of $d * T_x$.

\noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.94\textwidth}
\paragraph{Remark 8.} The variable \emph{initialization} necessitates a \emph{higher-dimensional} representation autoencoder.
\end{minipage}}

%The goal of relation-indexing is to obtain $\mathcal{\hat{Y}}$, which is the component of $\mathcal{Y}$ that can be \emph{\textbf{identifiable}} through its relationship with $\mathcal{X}$, accordingly represented as $\mathcal{\hat{V}}$ in the latent feature space.
%Moreover, for the relationship models to be \emph{{generalizable}}, $\mathcal{\hat{V}}$ must serve as a basis, which permits subsequent components of $\mathcal{Y}$ to build upon it, representing its various other relationships, leading to the \emph{{hierarchical disentanglement}} of $\mathcal{Y}$.





% The constructed latent feature space $\mathbb{R}^L$ must ensure: 
% 1) The observational-temporal feature $\mathcal{X}=\langle X, t \rangle \in \mathbb{R}^{d+1}$ is fully represented in $\mathbb{R}^L$ (remark of $T$ is omitted for simplicity);
% 2) The representation $\mathcal{X}$ is hierarchically disentangled within $\mathbb{R}^L$ based on specified relations, facilitating the model's generalizability across various hierarchical levels.

%Consider a relationship $\mathcal{X}\rightarrow \mathcal{Y}$, where $Y\in \mathbb{R}^b$ and $\mathcal{Y} = \langle Y, \tau \rangle \in \mathbb{R}^{b+1}$.
% In the context of the effect $\mathcal{Y}$, relation-indexing entails deriving $\mathcal{\hat{Y}}$, which represents the portion of $\mathcal{Y}$ that is identifiable through its relation with $\mathcal{X}$. This segment can serve as a foundation for additional levels of $\mathcal{Y}$'s representations, which is a generalizing process for the established relationship model $\mathcal{X}\rightarrow \mathcal{Y}$.



\subsection{Higher-Dimensional Autoencoder}
\label{subsec:repr_autoencoder}
\vspace{-1mm}

Autoencoders are commonly used for dimensionality reduction, especially in structural modeling that involves multiple variables \cite{wang2016auto}.
%, the column-augmented original data matrix often possesses a dimensionality exceeding that of the latent space $\mathbb{R}^L$. 
In contrast, RIRL aims to model individual causal relationships sequentially within a higher-dimensional latent space $\mathbb{R}^L$, as to hierarchically construct the entire causal structure. 
As illustrated in Figure~\ref{fig:arch}, the designed autoencoder architecture is featured by the symmetrical \emph{Expander} and \emph{Reducer} layers (source code is available \footnote{https://github.com/kflijia/bijective\_crossing\_functions/blob/main/code\_bicross\_extracter.py}).
The Expander magnifies the input vector $\overrightarrow{x}$ by capturing its higher-order associative features, while the Reducer symmetrically diminishes dimensionality and reverts to its initial state. For precise reconstruction, the \emph{invertibility} of these processes is essential. 

% Figure environment removed

The Expander showcased in Figure~\ref{fig:arch} implements a \emph{double-wise} expansion. Here, every duo of digits from $\overrightarrow{x}$ is encoded into a new digit using an association with a random constant, termed the \emph{Key}. This \emph{Key} is generated by the encoder and replicated by the decoder. Such pairwise processing of $\overrightarrow{x}$ expands its length from $(d*l_x)$ to be $(d * l_x - 1)^2$. By leveraging multiple \emph{Keys} and concatenating their resultant vectors, $\overrightarrow{x}$ can be considerably expanded, ready for the subsequent dimensionality-reduced representation extraction.

The four blue squares with unique grid patterns represent expansions by four distinct \emph{Keys}, with the grid patterns acting as their ``signatures''. Each square symbolizes a $(d * l_x - 1)^2$ length vector. Similarly, higher-order expansions, like \emph{triple-wise} across three digits, can be achieved with adapted \emph{Keys}.

% The four differently patterned blue squares represent the vectors expanded by four distinct \emph{Keys}, with the grid patterns indicating their ``signatures''. Each square visualizes a $(d * T_x - 1)^2$ length vector (not signifying a 2-dimensional vector).
% In a similar way, higher-order extensions, such as \emph{triple-wise} ones across every three digits, can also be employed by appropriately adapting \emph{Keys}.

% Figure environment removed

Figure~\ref{fig:extractor} illustrates the encoding and decoding processes within the Expander and Reducer, targeting the digit pair $(x_i, x_j)$ for $i\neq j \in 1,\ldots,d$. The Expander function is defined as $\eta_\phi(x_i,x_j) = x_j \otimes exp(s(x_i)) + t(x_i)$, which hinges on two elementary functions, $s(\cdot)$ and $t(\cdot)$. The \emph{Key} parameter, $\phi$, embodies their weights, $\phi=(w_s, w_t)$.
Specifically, the Expander morphs $x_j$ into a new digit $y_j$ utilizing $x_i$ as a chosen attribute. In contrast, the Reducer symmetrically uses the inverse function $\eta_\phi^{-1}$, defined as $(y_j-t(y_i)) \otimes exp(-s(y_i))$. 


This approach circumvents the need to compute $s^{-1}$ or $t^{-1}$, thereby allowing more flexibility for nonlinear transformations through $s(\cdot)$ and $t(\cdot)$. This is inspired by the groundbreaking work in \cite{dinh2016density} on invertible neural network layers employing bijective functions.


\subsection{Optimization Steps}
\label{subsec:repr_optimz}
\vspace{-1mm}

% Figure \ref{fig:bridge} illustrates the process of linking $\mathcal{H}$ and $\mathcal{V}$ to model the relationship $\mathcal{X}\rightarrow \mathcal{Y}$. 
%to explicitly include the temporal features of $h$. For now, we suppose $\mathcal{V}$ can capture potential dynamics autonomously, expecting future refinements.

Consider instances $x$ and $y$ of $\mathcal{X}$ and $\mathcal{Y}$, with corresponding representations $h$ and $v$ in $\mathbb{R}^L$. The latent dependency $\mathbf{P}(v| h)$ is used to train the relation function $f(;\theta)$, as illustrated in Figure \ref{fig:bridge}.
In each iteration, the modeling process undergoes three optimization steps:
\begin{enumerate}[topsep=0pt, partopsep=0pt]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{1pt}
    \item {Optimizing the cause-encoder by $\mathbf{P}(h|x)$, the relation model by $\mathbf{P}(v|h)$, and the effect-decoder by $\mathbf{P}(y|v)$ to reconstruct the relationship $x\rightarrow y$, represented as $h\rightarrow v$ in $\mathbb{R}^L$.}
    \item {Fine-tuning the effect-encoder $\mathbf{P}(v|y)$ and effect-decoder $\mathbf{P}(y|v)$ to accurately represent $y$.} 
    \item {Fine-tuning the cause-encoder $\mathbf{P}(h|x)$ and cause-decoder $\mathbf{P}(x|h)$ to accurately represent $x$.}
\end{enumerate}
\vspace{1mm}

During this process, the values of $h$ and $v$ are iteratively adjusted to reduce their distance in $\mathbb{R}^{L}$, with $f(;\theta)$ serving as a bridge to span the distance. Here, the hyper-dimensional variable $\theta \in \mathbb{R}^H$ acts as the index, guiding the output of $f(;\theta)$ to fulfill associated representations $( \mathcal{{H}},\theta,\hat{\mathcal{V}_{\theta}})$. From $\hat{\mathcal{V}_{\theta}}$, the effect component $\hat{\mathcal{Y}_{\theta}}$, also the causal representation, can be reconstructed. 
Within this system, for each effect, a series of such relation functions $\{f(;\theta)\}$ is maintained, indexing diverse levels of causal inputs for sequentially building the structural model.



\section{RIRL: Building Structural Models in Latent Space}
\label{sec:RIRL}


% Figure environment removed


By sequentially constructing relation-indexed representations for each pairwise relationship within the causal DAG, we can achieve the hierarchically disentangled representation for each node, according to its levels defined by the global structure. Simultaneously, the entire structualized causality has also been constructed. Subsequently, section \ref{subsec:RIRL_stacking} details the method for stacking relation-indexed representations, enabling the construction of higher-level representations based on previously established lower-level ones;
section \ref{subsec:RIRL_disentangle} provides the complete factorization process for hierarchical disentanglement; finally, section \ref{subsec:RIRL_discover} discusses a causal discovery algorithm within the latent space among initialized variable representations.


Figure~\ref{fig:new} demonstrates how the RIRL method can encapsulate the black-box nature of AI within the latent space while simultaneously generating interpretable observations. This characteristic can be utilized to enhance conventional \emph{Observation-Oriented} models, for instance, by simulating counterfactual values on demands. Meanwhile, in the latent space, these cryptic representations, although opaque to human interpretation, play a crucial role in achieving model generalization and individualization. These processes are latently managed by AI and remain exclusive to human comprehension.


%Causal relationships of known edges can be sequentially stacked using existing causal DAGs in domain knowledge.
%Additionally, this approach aids in discovering causal structures within the latent space by identifying potential relationships among the initial variable representations.


%latently. Simultaneously, one can enhance traditional causal models by employing desired latent features, to produce observations as needed, such as counterfactual effects.

%This section introduces a specialized autoencoder architecture crucial for implementing this approach, outlines the method for hierarchical representation disentanglement in constructing graphical models, and presents a causal discovery algorithm for the latent feature space.


% \subsection{Disentanglement of Relationship}
% \vspace{-1.5mm}

% Given a set of $n$-level hierarchical representation functions for $\mathcal{X}$, denoted by $\mathcal{F}(\vartheta) = \bigl\{ f_i \bigl(\theta_i \bigr) \mid i=1,\ldots, n\bigr\}$, the goal is to define $n$ relationship functions, collectively termed $\mathcal{G}$, such that $\mathcal{Y}=\mathcal{G}(\mathcal{X})$ exhibits an $n$-level hierarchy.
% Each $i$-th level relationship function is $ g_i(\mathcal{X}; \varphi_i)$, where $\varphi_i$ is its parameter. Then, we have:
% \begin{equation}
%     \mathcal{G}(\mathcal{X}) = \sum_{i=1}^{n} g_i(\mathcal{X}; \varphi_i) = \sum_{i=1}^{n} g_i(\Theta_{i}; \varphi_i)  =
%     \sum_{i=1}^{n} g_i \bigl(\theta_i;\ \Theta_1,\ldots, \Theta_{i-1}, \varphi_i\bigr) = \mathcal{Y}
% \end{equation}
% The $i$-th level relation-indexed representation for $\mathcal{Y}$ is $g_i (\theta_i;\varphi_i)$ considering the features of the preceding $(i-1)$ levels of $\mathcal{X}$. This relationship can be portrayed as the augmented feature vector $\langle \theta_i, \varphi_i \rangle$ in latent space $\mathbb{R}^L$.
% Using $\vartheta_X$ and $\vartheta_Y$ to distinguish the collective hierarchical representations for $\mathcal{X}$ and $\mathcal{Y}$ respectively, the overall relationship from $\mathcal{X}$ to $\mathcal{Y}$ becomes $\vartheta_Y=\langle \vartheta_X, \varphi \rangle$, where $\varphi=\{\varphi_1,\ldots,\varphi_n\}$. %and $\langle \vartheta_X, \varphi \rangle$ 
% The term $\langle \vartheta_X, \varphi \rangle$ represents the pairwise augmentations between collections $\vartheta_X$ and $\varphi$.

\subsection{Stacking Hierarchical Representations}
\label{subsec:RIRL_stacking}
\vspace{-1mm}
% \noindent\tcbox[enhanced, drop fuzzy shadow southwest, colframe=yellow!10, colback=yellow!10]{\begin{minipage}{0.94\textwidth}
% \paragraph{Remark 2.} Given a causal graph $G$ with data matrix $\mathbf{X}$ column-augmented by all nodes' attributes, the latent space dimensionality $L$ must satisfy $L\ge rank(\mathbf{X})$ to adequately represent $G$.
% \vspace{-0.5mm}
% \end{minipage}}


%\vspace{-2.7mm}
% Figure environment removed


A structural relationship can be represented by a causal graph, denoted as $G$. To construct models in the latent space, the latent dimensionality $L$ must be sufficiently large to adequately represent $G$.
Let's denote a data matrix augmented by all observational attributes in $G$ as $\mathbf{X}$. Given the need to include informative relations $\{\theta\}$ for the edges in $G$, it is essential that $L > rank(\mathbf{X})+T$, where $T$ indicates the number of dynamically significant variables (i.e., nodes) within $G$.

The PCA principle posits that the space $\mathbb{R}^L$ learned by the autoencoder is spanned by the top principal components of $\mathbf{X}$ \cite{baldi1989neural, plaut2018principal, wang2016auto}.
Hypothetically, reducing $L$ below $rank(\mathbf{X})$ may yield a less adequate but causally more significant latent space through better alignment of dimensions \cite{jain2021mechanism} (Further exploration in this direction is warranted). Bypassing a deep dive into dimensionality boundaries, we rely on empirical fine-tuning for the experiments in this study (reducing $L$ from 64 to 16).
%In this study, we will set aside discussions on the boundaries of dimensionality. Our experiments feature 10 variables with dimensions 1 to 5 (Table~\ref{tab:tower}), and we empirically fine-tune and reduce $L$ from 64 to 16.


% Consider a relationship $\mathcal{X}\rightarrow \mathcal{Y}$, where $Y\in \mathbb{R}^b$ and $\mathcal{Y} = \langle Y, \tau \rangle \in \mathbb{R}^{b+1}$.
% In the context of the effect $\mathcal{Y}$, relation-indexing entails deriving $\mathcal{\hat{Y}}$, which represents the portion of $\mathcal{Y}$ that is identifiable through its relation with $\mathcal{X}$. This segment can serve as a foundation for additional levels of $\mathcal{Y}$'s representations, which is a generalizing process for the established relationship model $\mathcal{X}\rightarrow \mathcal{Y}$.
% For example, suppose a graphical system $\{\mathcal{X}, \mathcal{Y}, \mathcal{Z}\}$ has the relationship $\mathcal{X}\rightarrow \mathcal{Y} \leftarrow \mathcal{Z}$, then, $\mathcal{Y}$ can be viewed as in a two-level hierarchy. The first level is defined by $\mathcal{X}\rightarrow \mathcal{Y}$ and the second by $\langle\mathcal{X}, \mathcal{Z}\rangle \rightarrow \mathcal{Y}$, where the second level enhances the first by incorporating an additional data stream from $\mathcal{Z}$.

% Using a collective representation, all $f_\theta$ functions can be expressed as $\mathcal{F}(X; \vartheta)$, where $\vartheta$ includes all parameters. Thus, the Expander and Reducer can be concisely written as $Y=\mathcal{F}(X; \vartheta)$ and $X = \mathcal{F}^{-1}(Y; \vartheta)$. 

Consider a causal structural among $\{\mathcal{X}, \mathcal{Y}, \mathcal{Z}\}$, with their corresponding representations $\{\mathcal{H}, \mathcal{V}, \mathcal{K}\} \in \mathbb{R}^L$ initialized by three autoencoders, respectively. Figure~\ref{fig:stack} illustrates the hierarchical representations buildup.
%the hierarchical assembly of two modeled relationships associated with $\mathcal{Y}$.
Here, two stacking scenarios are displayed based on varying causal directions. With the established $\mathcal{X}\rightarrow \mathcal{Y}$ relationship in $\mathbb{R}^{L}$, the left-side architecture finalizes the $\mathcal{X}\rightarrow \mathcal{Y}\leftarrow \mathcal{Z}$ structure, while the right-side focuses on $\mathcal{X}\rightarrow \mathcal{Y}\rightarrow \mathcal{Z}$. Through the addition of a representation layer, hierarchical disentanglement is formed, allowing for various input-output combinations (denoted as $\mapsto$) according to specific requirements.

For example, on the left, $\mathbf{P}(v|h) \mapsto \mathbf{P}(\alpha)$ represents the $\mathcal{X}\rightarrow \mathcal{Y}$ relationship, whereas $\mathbf{P}(\alpha|k)$ implies $\mathcal{Z}\rightarrow \mathcal{Y}$. Conversely, on the right, $\mathbf{P}(v) \mapsto P(\beta|k)$ denotes the $\mathcal{Y}\rightarrow \mathcal{Z}$ relationship with $\mathcal{Y}$ as input. Meanwhile, $\mathbf{P}(v|h) \mapsto P(\beta|k)$ captures the causal sequence $\mathcal{X} \rightarrow \mathcal{Y}\rightarrow \mathcal{Z}$.


\subsection{Factorizing the Effect Disentanglement}
\label{subsec:RIRL_disentangle}

Consider $\mathcal{Y}=\langle Y, \tau \rangle \in \mathbb{R}^{b+1} \subseteq \mathbb{R}^O$ having a $n$-level hierarchy, with each level built up using a representation function, labeled as $g_i$ for the $i$-th level. For simplicity, here, we use $\omega_i$ to represent the $i$-th level component of $\mathcal{Y}$ in the latent space $\mathbb{R}^L$, while its counterpart in $\mathbb{R}^{b+1}$ is denoted as $\Omega_i$ (i.e., $\mathcal{\hat{Y}}$ at the $i$-th level).
Let the feature vector $\omega_i$ in $\mathbb{R}^{L}$ primarily spans a sub-dimensional space, $\mathbb{R}^{L_i}$, resulting in the spatial disentanglement sequence $\{\mathbb{R}^{L_1}, \ldots, \mathbb{R}^{L_i},\ldots,\mathbb{R}^{L_n}\}$, which hierarchically represents $\mathcal{Y}$ with $n$ components.
Function $g_i$ maps from $\mathbb{R}^{b+1}$ to $\mathbb{R}^{L_i}$, taking into account features from all previous levels as attributes.  This gives us:
\vspace{-4mm}

\begin{equation}
    \mathcal{Y} = \sum_{i=1}^{n} \Omega_i, 
     \text{ where } \Omega_i = g_i \bigl( \omega_i ;\ \Omega_1,\ldots, \Omega_{i-1}\bigr) \text{ with } \Omega_i \in \mathbb{R}^{b+1} \text{ and } \omega_i \in \mathbb{R}^{L_i} \subseteq \mathbb{R}^{L}
\end{equation}

\vspace{-3mm}
%The $t$-th component in the observable data space, denoted as $\Omega_t \in \mathbb{R}^{O}$, is articulated through an observational data sequence with the length of $T_y$, along the absolute timeline $t$.
%However, in latent space, the objective of $\omega_t$ is to capture dynamics along a relative timeline, $t_i$, which is autonomously determined by the relation at the $i$-th level, not bound by the observational timestamps in $\mathbb{R}^{O}$. 


% $\{t_1,\ldots,t_i,\ldots,t_n\}$, each uniquely determined by the relationship at their respective levels, apart from the absolute timeline $t$.
% While in the \emph{observable data space}, the $i$-th level feature, represented as the sum $\Omega_1 +\ldots +\Omega_i$, still maintains its timestamp attribute along $t$.

In the context of a purely observational hierarchy, with $\mathcal{Y}$ substituted by $Y \in \mathbb{R}^b$, The example depicted in Figure~\ref{fig:hand} (b) can be interpreted as follows: Consider three feature levels represented as $\omega_1\in \mathbb{R}^{L_1}$, $\omega_2\in \mathbb{R}^{L_2}$, and $\omega_3\in \mathbb{R}^{L_3}$. For simplicity, assume the subspaces are mutually exclusive, such that $L=L_1+L_2+L_3$. In the latent space, the triplet $(\omega_1, \omega_2, \omega_3) \in \mathbb{R}^{L}$ comprehensively depicts the image. Their observable counterparts, $\Omega_1$, $\Omega_2$, and $\Omega_3$, are three distinct full-scale images, each showcasing different content. For example, $\Omega_1$ emphasizes finger details, while the combination $\Omega_1+\Omega_2$ reveals the entire hand.



\subsection{Causal Discovery in Latent Space}
\label{subsec:RIRL_discover}
%\vspace{-2mm}

Algorithm 1 outlines the heuristic procedure for investigating edges among the initialized variable representations. We use Kullback-Leibler Divergence (KLD) as a metric to evaluate the strength of causal relationships. Specifically, as depicted in Figure~\ref{fig:bridge}, KLD evaluates the similarity between the relation output $\mathbf{P}(v|h)$ and the prior $\mathbf{P}(v)$. Lower KLD values indicate stronger causal relationships due to closer alignment with the ground truth. Conversely, while Mean Squared Error (MSE) is a frequently used evaluation metric, its sensitivity to data variances \cite{reisach2021beware} leads us to utilize it as a supplementary measure in this study. %In the graphical representation context, we refer to variables $A$ and $B$ in the edge $A\rightarrow B$ as the \emph{cause node} and \emph{effect node}, respectively.


\begin{minipage}{\textwidth}
    \begin{minipage}[b]{0.52\textwidth}
    \raggedright
    \begin{algorithm}[H]
    \footnotesize
    \SetAlgoLined
    \KwResult{ ordered edges set $\mathbf{E}=\{e_1, \ldots,e_n\}$ }
    $\mathbf{E}=\{\}$ ; $N_R = \{ n_0 \mid n_0 \in N, Parent(n_0)=\varnothing\}$ \;
    \While{$N_R \subset N$}{
    $\Delta = \{\}$ \;
    \For{ $n \in N$ }{
        \For{$p \in Parent(n)$}{
            \If{$n \notin N_R$ and $p \in N_R$}{
                $e=(p,n)$; \ 
                $\beta = \{\}$;\\ 
                \For{$r \in N_R$}{
                    \If{$r \in Parent(n)$ and $r \neq p$}{
                        $\beta = \beta \cup r$}
                    }
                $\delta_e = K(\beta \cup p, n) - K(\beta, n)$;\\
                $\Delta = \Delta \cup \delta_e$;
            }
        }
    }
    $\sigma = argmin_e(\delta_e \mid \delta_e \in \Delta)$;\\
    $\mathbf{E} = \mathbf{E} \cup \sigma$; \ 
    $N_R = N_R \cup n_{\sigma}$;\\
    }
     \caption{Latent Space Causal Discovery}
    \end{algorithm}
    \end{minipage}
%\hfill 
    \begin{minipage}[b]{0.4\textwidth}
    \raggedleft
        \label{tab:table1}
        \begin{tabular}{|l|l|}
        \hline
           $G=(N,E)$ & graph $G$ consists of $N$ and $E$ \\
           $N$ & the set of nodes\\
           $E$ & the set of edges\\
           $N_R$ & the set of reachable nodes\\
           $\mathbf{E}$ & the list of discovered edges\\
           $K(\beta, n)$ & KLD metric of effect $\beta\rightarrow n$\\
           $\beta$ & the cause nodes \\
           $n$ & the effect node\\
           $\delta_e$ & KLD Gain of candidate edge $e$\\
           $\Delta=\{\delta_e\}$ & the set $\{\delta_e\}$ for $e$ \\
           $n$,$p$,$r$ & notations of nodes\\
           $e$,$\sigma$ & notations of edges\\
          \hline
        \end{tabular} %
%       \captionof{table}{A table beside a figure}
    \end{minipage}
\end{minipage}

Figure~\ref{fig:discover} illustrates the causal structure discovery process in latent space over four steps. Two edges, ($e_1$ and $e_3$), are sequentially selected, with $e_1$ setting node $B$ as the starting point for $e_3$. In step 3, edge $e_2$ from $A$ to $C$ is deselected and reassessed due to the new edge $e_3$ altering $C$'s present causal conditions. The final DAG represents the resulting causal structure.


%\vspace{-2mm}
% Figure environment removed



%\vspace{-2mm}
\section{Efficacy Validation Experiments}
%\vspace{-3mm}
\label{sec:experiment}

The experiments aim to validate the efficacy of the RIRL method from three aspects: 1) the performance of the proposed higher-dimensional representations, evaluated by reconstruction accuracy, 2) the construction of a clear effect hierarchy through the stacking of relation-indexed representations, and 3) the identification of DAG structures within the latent space through discovery.
A full demonstration of the conducted experiments in this chapter is available online \footnote{https://github.com/kflijia/bijective\_crossing\_functions.git}, while with two primary limitations detailed as follows:

Firstly, the dataset employed in this study may not be the most suitable for evaluating the effectiveness of RIRL. 
Ideally, real-world data featuring rich structuralized causality across multiple relative timings, like clinical records, would be preferable. However, due to practical constraints, access to such optimal data is limited for this study, leading us to use the current synthetic data and focus solely on feasibility verification. For experimental validation regarding the inherent bias, please refer to prior research \cite{li2020teaching}.

Secondly, the time windows designated for cause and effect, $l_x$ and $l_y$, are fixed at 10 and 1, respectively. This constraint arose from an initial oversight in the experimental design stage, wherein the pivotal role of effect dynamics has not been fully recognized, consequently limited by the RNN pattern. It manifests as restricted successes in building causal chains like $\mathcal{X} \rightarrow \mathcal{Y}\rightarrow \mathcal{Z}$; while the model can adeptly capture single-hop causality, it struggles with multi-hop ones since the dynamics in $\mathcal{Y}$ have been segmented by $l_y=1$. However, extending the length of $l_y$ does not pose a significant technical challenge to future works.



%\vspace{-1mm}
\subsection{Hydrology Dataset}
\vspace{-1mm}

%\vspace{-3mm}
% Figure environment removed


The dataset chosen for our experiments is a widely-used synthetic resource in the field of hydrology, aimed at enhancing streamflow predictions based on observed environmental conditions such as temperature and precipitation. %The application of RIRL aims to create a streamflow prediction model that is generalizable across various watersheds. While these watersheds share a fundamental hydrological scheme governed by physical rules, they may exhibit unique features due to unobserved conditions such as economic development and land use. Current models based on physical knowledge, however, often lack the flexibility to fully capture multiple levels of dynamical temporal features across these watersheds.
In hydrology, deep learning, particularly RNN models, has gained favor for extracting observational representations and predicting streamflow \cite{goodwell2020debates, kratzert2018rainfall}. 
We focus on a simulation of the Root River Headwater watershed in Southeast Minnesota, covering 60 consecutive virtual years with daily updates. The simulated data is from the Soil and Water Assessment Tool (SWAT), a comprehensive system grounded in physical modules, to generate dynamically significant hydrological time series. %The performance evaluations predominantly focus on the accuracy of the autoencoder reconstructions.

Figure \ref{fig:stream} displays the causal DAG employed by SWAT, complete with node descriptions. The hydrological routines are color-coded based on their contribution to output streamflow. Surface runoff (1st tier) significantly impacts rapid streamflow peaks, followed by lateral flow (2nd tier). Baseflow dynamics (3rd tier) have a subtler influence. Our causal discovery experiments aim to reveal these underlying tiers. %relationships from the observed data.


\vspace{-1mm}
\subsection{Higher-Dimensional Variable Representation Test}
%\vspace{-2mm}

In this test, we have a total of ten variables (i.e., nodes), with each requiring an individual autoencoder for initialization. Table \ref{tab:tower} lists the statistical characteristics of their post-scaled (i.e., normalized) attributes, along with their autoencoders' reconstruction accuracies. Accuracy is assessed in the root mean square error (RMSE), where a lower RMSE indicates higher accuracy for both scaled and unscaled data.


The task is challenging due to the limited dimensionalities of the ten variables - maxing out at just 5 and the target node, $J$, having just one attribute. To mitigate this, we duplicate the input vector to a consistent 12-length and add 12 dummy variables for months, resulting in a 24-dimensional input. A double-wise extension amplifies this to 576 dimensions, from which a 16-dimensional representation is extracted via the autoencoder.
Another issue is the presence of meaningful zero-values, such as node $D$ (Snowpack in winter), which contributes numerous zeros in other seasons and is closely linked to node $E$ (Soil Water). We tackle this by adding non-zero indicator variables, called \emph{masks}, evaluated via binary cross-entropy (BCE).

Despite challenges, RMSE values ranging from $0.01$ to $0.09$ indicate success, except for node $F$ (the Aquifer). Given that aquifer research is still emerging (i.e., the 3rd tier baseflow routine), it is likely that node $F$ in this synthetic dataset may better represent noise than meaningful data.


\begin{table*}[t]
\caption{Characteristics of node attributes and their variable representation test results.}
\label{tab:tower}
\vspace{-2mm}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{|
>{\columncolor[HTML]{EFEFEF}}c |c|l|l|l|l|l|l|l|l|}
\hline
\cellcolor[HTML]{C0C0C0}Variable & \cellcolor[HTML]{C0C0C0} Dim & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Mean} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Std} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Min} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Max} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Non-Zero Rate\%} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}RMSE on Scaled} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}RMSE on Unscaled} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}BCE of Mask} \\ \hline
A & 5 & 1.8513 & 1.5496 & -3.3557 & 7.6809 & 87.54 & 0.093 & 0.871 & 0.095 \\ \hline
B & 4 & 0.7687 & 1.1353 & -3.3557 & 5.9710 & 64.52 & 0.076 & 0.678 & 1.132 \\ \hline
C & 2 & 1.0342 & 1.0025 & 0.0 & 6.2145 & 94.42 & 0.037 & 0.089 & 0.428 \\ \hline
D & 3 & 0.0458 & 0.2005 & 0.0 & 5.2434 & 11.40 & 0.015 & 0.679 & 0.445 \\ \hline
E & 2 & 3.1449 & 1.0000 & 0.0285 & 5.0916 & 100 & 0.058 & 3.343 & 0.643 \\ \hline
F & 4 & 0.3922 & 0.8962 & 0.0 & 8.6122 & 59.08 & 0.326 & 7.178 & 2.045 \\ \hline
G & 4 & 0.7180 & 1.1064 & 0.0 & 8.2551 & 47.87 & 0.045 & 0.81 & 1.327 \\ \hline
H & 4 & 0.7344 & 1.0193 & 0.0 & 7.6350 & 49.93 & 0.045 & 0.009 & 1.345 \\ \hline
I & 3 & 0.1432 & 0.6137 & 0.0 & 8.3880 & 21.66 & 0.035 & 0.009 & 1.672 \\ \hline
J & 1 & 0.0410 & 0.2000 & 0.0 & 7.8903 & 21.75 & 0.007 & 0.098 & 1.088 \\ \hline
\end{tabular}}
\vspace{-3mm}
\end{table*}

%\vspace{1mm}
\subsection{Hierarchical Disentanglement Test}
%\vspace{-2mm}

Table \ref{tab:unit} provides the performance comparison of stacking relation-indexed representations on each node. The term ``single-effect'' is to describe the accuracy of a specific effect node when reconstructed from a single cause node (e.g., $B\rightarrow D$ and $C\rightarrow D$), and ``full-effect'' for the accuracy when all its cause nodes are stacked (e.g., $BC\rightarrow D$). To provide context, we also include baseline performance scores based on the initialized variable representations. During the relation learning process, the effect node serves two purposes: it maintains its own accurate representation (as per optimization no.2 in \ref{subsec:repr_optimz}) and helps reconstruct the relationship (as per optimization no.1 in \ref{subsec:repr_optimz}). Both aspects are evaluated in Table \ref{tab:unit}.


% For example, node $G$ has three possible causes $C$, $D$, and $E$, so we built four causal effect models in total: the individual pairwise causations $C\rightarrow G$, $D\rightarrow G$, $E\rightarrow G$, and the stacked causal effect $CDE\rightarrow G$. For convenience, we refer to them as "\emph{pair-relation}" and "\emph{stacking-relation}" respectively. We also listed the initial reconstruction performance for each variable in the column named "Variable Reconstruction (initial performance)" as a comparison baseline in Table \ref{tab:unit}.


%\vspace{-2mm}
% Figure environment removed

The KLD metrics in Table \ref{tab:unit} indicate the strength of learned causality, with a lower value signifying stronger. For instance, node $J$'s minimal KLD values suggest a significant effect caused by nodes $G$ (Surface Runoff), $H$ (Lateral), and $I$ (Baseflow). In contrast, the high KLD values imply that predicting variable $I$ using $D$ and $F$ is challenging. 
For nodes $D$, $E$, and $J$, the ``full-effect'' are moderate compared to their ``single-effect'' scores, suggesting a lack of informative associations among the cause nodes. In contrast, for nodes $G$ and $H$, lower ``full-effect'' KLD values imply capturing meaningful associative effects through hierarchical stacking. The KLD metric also reveals the most contributive cause node to the effect node. For example, the proximity of the $C\rightarrow G$ strength to $CDE\rightarrow G$ suggests that $C$ is the primary contributor to this causal relationship.

Figure~\ref{fig:G} showcases reconstructed time series, for the effect nodes $J$, $G$, and $I$, in the same synthetic year to provide a straightforward overview of the hierarchical representation performances. 
Here, black dots represent the ground truth; the blue line indicates reconstruction via the initial variable representation, and the ``full-effect'' representation generates the red line. 
In addition to RMSE, we also employ the NashSutcliffe model efficiency coefficient (NSE) as an accuracy metric, commonly used in hydrological predictions. The NSE ranges from -$\infty$ to 1, with values closer to 1 indicating higher accuracy.

The initial variable representation closely aligns with the ground truth, as shown in Figure~\ref{fig:G}, attesting to the efficacy of our proposed autoencoder architecture. As expected, the ``full-effect'' performs better than the ``single-effect'' for each effect node. Node $J$ exhibits the best prediction, whereas node $I$ presents a challenge. For node $G$, causality from $C$ proves to be significantly stronger than the other two, $D$ and $E$.


% One may observe via the demo that our experiments do not show smooth information flows along successive long causal chains. Since RNNs are designed primarily for capturing the dynamics of causes rather than the effects, relying on them to autonomously construct dynamical representations of the effects might prove unreliable. It underscores a significant opportunity for enhancing effectiveness by improving the architecture.



\vspace{-1mm}
\subsection{Latent Space Causal Discovery Test}
%\vspace{-2mm}

The discovery test initiates with source nodes $A$ and $B$ and proceeds to identify potential edges, culminating in the target node $J$. Candidate edges are selected based on their contributions to the overall KLD sum (less gain is better). 
Table \ref{tab:discv} shows the order in which existing edges are discovered, along with the corresponding KLD sums and gains after each edge is included. Color-coding in the cells corresponds to Figure \ref{fig:stream}, indicating tiers of causal routines. The arrangement underscores the efficacy of this latent space discovery approach.

A comprehensive list of candidate edges evaluated in each discovery round is provided in Table \ref{tab:discv_rounds} in Appendix A. For comparative purposes, we also performed a 10-fold cross-validation using the conventional FGES discovery method; those results are available in Table \ref{tab:fges} in Appendix A.


%\vspace{-3mm}
\begin{table*}[t]
\caption{Effect Reconstruction Performances of RIRL sorted by effect nodes.}
\label{tab:unit}
\vspace{-2mm}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{|l|lll|l|lll|llll|}
\hline
\rowcolor[HTML]{C0C0C0} 
\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} } & \multicolumn{3}{l|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \begin{tabular}[c]{@{}l@{}}\ \ \ \ \ \ Variable Representation\\ \ \ \ \ \ \ \ (Initial)\end{tabular}}} & \cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} } & \multicolumn{3}{l|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000}  \begin{tabular}[c]{@{}l@{}}\ \ \ \ \ \ Variable Representation\\ \ \ \ \ \ \ \ (in Relation Learning)\end{tabular}}} & \multicolumn{4}{l|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \ \ \ \ \ \ \ \ \ Relationship Reconstruction}} \\ \cline{2-4} \cline{6-12} 
\rowcolor[HTML]{C0C0C0} 
\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} } & \multicolumn{2}{l|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000}\ \ \ \ \ \  \ \ \ \ RMSE}} & {\color[HTML]{000000} BCE} & \cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} } & \multicolumn{2}{l|}{\cellcolor[HTML]{C0C0C0}\ \ \ \ \ \ \ \ \ \ RMSE} & BCE & \multicolumn{2}{l|}{\cellcolor[HTML]{C0C0C0} \ \ \ \ \ \ \ \ \ RMSE} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}BCE} & KLD \\ \cline{2-4} \cline{6-12} 
\rowcolor[HTML]{C0C0C0} 
\multirow{-3}{*}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \begin{tabular}[c]{@{}l@{}}Result\\ Node\\ \ \end{tabular}}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Scaled\\ \ \ Values\end{tabular}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Unscaled\\ \ \ Values\end{tabular}} & Mask & \multirow{-3}{*}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \begin{tabular}[c]{@{}l@{}}Cause\\ Node\end{tabular}}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Scaled\\ \ \ Values\end{tabular}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Unscaled\\ \ \ Values\end{tabular}} & Mask & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Scaled\\ \ \ Values\end{tabular}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}l@{}}on Unscaled\\ \ \ Values\end{tabular}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Mask} & \begin{tabular}[c]{@{}l@{}}(in latent\\ \ \ space)\end{tabular} \\ \hline
\cellcolor[HTML]{EFEFEF}C & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}0.037} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}0.089} & \cellcolor[HTML]{EFEFEF}0.428 & A & \multicolumn{1}{l|}{0.0295} & \multicolumn{1}{l|}{0.0616} & 0.4278 & \multicolumn{1}{l|}{0.1747} & \multicolumn{1}{l|}{0.3334} & \multicolumn{1}{l|}{0.4278} & 7.6353 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & BC & \multicolumn{1}{l|}{0.0350} & \multicolumn{1}{l|}{1.0179} & 0.1355 & \multicolumn{1}{l|}{0.0509} & \multicolumn{1}{l|}{1.7059} & \multicolumn{1}{l|}{0.1285} & 9.6502 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & B & \multicolumn{1}{l|}{0.0341} & \multicolumn{1}{l|}{1.0361} & 0.1693 & \multicolumn{1}{l|}{0.0516} & \multicolumn{1}{l|}{1.7737} & \multicolumn{1}{l|}{0.1925} & 8.5147 \\ \cline{5-5}
\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}D} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.015}} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.679}} & \multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.445} & C & \multicolumn{1}{l|}{0.0331} & \multicolumn{1}{l|}{0.9818} & 0.3404 & \multicolumn{1}{l|}{0.0512} & \multicolumn{1}{l|}{1.7265} & \multicolumn{1}{l|}{0.3667} & 10.149 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & BC & \multicolumn{1}{l|}{0.4612} & \multicolumn{1}{l|}{26.605} & 0.6427 & \multicolumn{1}{l|}{0.7827} & \multicolumn{1}{l|}{45.149} & \multicolumn{1}{l|}{0.6427} & 39.750 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & B & \multicolumn{1}{l|}{0.6428} & \multicolumn{1}{l|}{37.076} & 0.6427 & \multicolumn{1}{l|}{0.8209} & \multicolumn{1}{l|}{47.353} & \multicolumn{1}{l|}{0.6427} & 37.072 \\ \cline{5-5}
\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}E} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.058}} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}3.343}} & \multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.643} & C & \multicolumn{1}{l|}{0.5212} & \multicolumn{1}{l|}{30.065} & 1.2854 & \multicolumn{1}{l|}{0.7939} & \multicolumn{1}{l|}{45.791} & \multicolumn{1}{l|}{1.2854} & 46.587 \\ \hline
\cellcolor[HTML]{EFEFEF}F & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}0.326} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}7.178} & \cellcolor[HTML]{EFEFEF}2.045 & E & \multicolumn{1}{l|}{0.4334} & \multicolumn{1}{l|}{8.3807} & 3.0895 & \multicolumn{1}{l|}{0.4509} & \multicolumn{1}{l|}{5.9553} & \multicolumn{1}{l|}{3.0895} & 53.680 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & CDE & \multicolumn{1}{l|}{0.0538} & \multicolumn{1}{l|}{0.9598} & 0.0878 & \multicolumn{1}{l|}{0.1719} & \multicolumn{1}{l|}{3.5736} & \multicolumn{1}{l|}{0.1340} & 8.1360 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & C & \multicolumn{1}{l|}{0.1057} & \multicolumn{1}{l|}{1.4219} & 0.1078 & \multicolumn{1}{l|}{0.2996} & \multicolumn{1}{l|}{4.6278} & \multicolumn{1}{l|}{0.1362} & 11.601 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & D & \multicolumn{1}{l|}{0.1773} & \multicolumn{1}{l|}{3.6083} & 0.1842 & \multicolumn{1}{l|}{0.4112} & \multicolumn{1}{l|}{8.0841} & \multicolumn{1}{l|}{0.2228} & 27.879 \\ \cline{5-5}
\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}G} & \multicolumn{1}{l|}{\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}0.045}} & \multicolumn{1}{l|}{\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}0.81}} & \multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}1.327} & E & \multicolumn{1}{l|}{0.1949} & \multicolumn{1}{l|}{4.7124} & 0.1482 & \multicolumn{1}{l|}{0.5564} & \multicolumn{1}{l|}{10.852} & \multicolumn{1}{l|}{0.1877} & 39.133 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & DE & \multicolumn{1}{l|}{0.0889} & \multicolumn{1}{l|}{0.0099} & 2.5980 & \multicolumn{1}{l|}{0.3564} & \multicolumn{1}{l|}{0.0096} & \multicolumn{1}{l|}{2.5980} & 21.905 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & D & \multicolumn{1}{l|}{0.0878} & \multicolumn{1}{l|}{0.0104} & 0.0911 & \multicolumn{1}{l|}{0.4301} & \multicolumn{1}{l|}{0.0095} & \multicolumn{1}{l|}{0.0911} & 25.198 \\ \cline{5-5}
\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}H} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.045}} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.009}} & \multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}1.345} & E & \multicolumn{1}{l|}{0.1162} & \multicolumn{1}{l|}{0.0105} & 0.1482 & \multicolumn{1}{l|}{0.5168} & \multicolumn{1}{l|}{0.0097} & \multicolumn{1}{l|}{3.8514} & 39.886 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & DF & \multicolumn{1}{l|}{0.0600} & \multicolumn{1}{l|}{0.0103} & 3.4493 & \multicolumn{1}{l|}{0.1158} & \multicolumn{1}{l|}{0.0099} & \multicolumn{1}{l|}{3.4493} & 49.033 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & D & \multicolumn{1}{l|}{0.1212} & \multicolumn{1}{l|}{0.0108} & 3.0048 & \multicolumn{1}{l|}{0.2073} & \multicolumn{1}{l|}{0.0108} & \multicolumn{1}{l|}{3.0048} & 75.577 \\ \cline{5-5}
\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}I} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.035}} & \multicolumn{1}{l|}{\multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}0.009}} & \multirow{-3}{*}{\cellcolor[HTML]{EFEFEF}1.672} & F & \multicolumn{1}{l|}{0.0540} & \multicolumn{1}{l|}{0.0102} & 3.4493 & \multicolumn{1}{l|}{0.0948} & \multicolumn{1}{l|}{0.0098} & \multicolumn{1}{l|}{3.4493} & 45.648 \\ \hline
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & GHI & \multicolumn{1}{l|}{0.0052} & \multicolumn{1}{l|}{0.0742} & 0.2593 & \multicolumn{1}{l|}{0.0090} & \multicolumn{1}{l|}{0.1269} & \multicolumn{1}{l|}{0.2937} & 5.5300 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & G & \multicolumn{1}{l|}{0.0077} & \multicolumn{1}{l|}{0.1085} & 0.4009 & \multicolumn{1}{l|}{0.0099} & \multicolumn{1}{l|}{0.1390} & \multicolumn{1}{l|}{0.4375} & 5.2924 \\ \cline{5-5}
\cellcolor[HTML]{EFEFEF} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}} & \cellcolor[HTML]{EFEFEF} & H & \multicolumn{1}{l|}{0.0159} & \multicolumn{1}{l|}{0.2239} & 0.4584 & \multicolumn{1}{l|}{0.0393} & \multicolumn{1}{l|}{0.5520} & \multicolumn{1}{l|}{0.4938} & 15.930 \\ \cline{5-5}
\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}J} & \multicolumn{1}{l|}{\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}0.007}} & \multicolumn{1}{l|}{\multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}0.098}} & \multirow{-4}{*}{\cellcolor[HTML]{EFEFEF}1.088} & I & \multicolumn{1}{l|}{0.0308} & \multicolumn{1}{l|}{0.4328} & 0.3818 & \multicolumn{1}{l|}{0.0397} & \multicolumn{1}{l|}{0.5564} & \multicolumn{1}{l|}{0.3954} & 17.410 \\ \hline
\end{tabular}
}
\vspace{-3mm}
\end{table*}

\begin{table*}[t]
%\vspace{-2.5mm}
\caption{Brief summary of the latent space causal discovery test.}
\label{tab:discv}
\vspace{-2mm}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{|c|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
\cellcolor[HTML]{C0C0C0}Edge & \cellcolor[HTML]{FFCCC9}A$\rightarrow$C & \cellcolor[HTML]{FFCCC9}B$\rightarrow$D & \cellcolor[HTML]{FFCCC9}C$\rightarrow$D & \cellcolor[HTML]{FFCCC9}C$\rightarrow$G & \cellcolor[HTML]{FFCCC9}D$\rightarrow$G & \cellcolor[HTML]{FFCCC9}G$\rightarrow$J & \cellcolor[HTML]{FFCCC9}D$\rightarrow$H & \cellcolor[HTML]{FFCCC9}H$\rightarrow$J & \cellcolor[HTML]{FFCE93}B$\rightarrow$E & \cellcolor[HTML]{FFCE93}E$\rightarrow$G & \cellcolor[HTML]{FFCE93}E$\rightarrow$H & \cellcolor[HTML]{FFCE93}C$\rightarrow$E & \cellcolor[HTML]{ABE9E7}E$\rightarrow$F & \cellcolor[HTML]{ABE9E7}F$\rightarrow$I & \cellcolor[HTML]{ABE9E7}I$\rightarrow$J & \cellcolor[HTML]{ABE9E7}D$\rightarrow$I \\ \hline
\cellcolor[HTML]{C0C0C0}KLD & 7.63 & 8.51 & 10.14 & 11.60 & 27.87 & 5.29 & 25.19 & 15.93 & 37.07 & 39.13 & 39.88 & 46.58 & 53.68 & 45.64 & 17.41 & 75.57 \\ \hline
\rowcolor[HTML]{FFFFFF} 
\cellcolor[HTML]{C0C0C0}Gain & 7.63 & 8.51 & 1.135 & 11.60 & 2.454 & 5.29 & 25.19 & 0.209 & 37.07 & -5.91 & -3.29 & 2.677 & 53.68 & 45.64 & 0.028 & 3.384 \\ \hline
\end{tabular}}
\vspace{-5mm}
\end{table*}



\section{Conclusions}\label{sec:conclusion}
\vspace{-2mm}


This paper introduces a dimensionality framework from a \emph{Relation-Oriented} perspective to decompose our cognitive space, where relational causal knowledge is stored. 
Specifically, it conceptualizes the unobservable relations between cause and effect as informative variables in $\mathbb{R}^H$; and the causal structure of dynamics in knowledge, represented by the enhanced DAG, is accommodated by the counterfactual space $\mathbb{R}^T$, 
across multiple relative timing axes with nonlinear dependence. 
It highlights the key oversights of the current \emph{Observation-Oriented} paradigm, which relies on the observational i.i.d. assumption and is confined to $\mathbb{R}^O$. 
%It relies on manual specification to identify dynamical effects from observational static sequences, inherently fraught with difficulty.
%Specifically, based on the observational i.i.d. assumption, conventional relationship modeling intrinsically overlooks 1) the informative unobservables in $\mathbb{R}^H$, and 2) the structuralized dynamics within multi-dimensional $\mathbb{R}^T$. Instead, due to being confined within $\mathbb{R}^O$, 

The traditional causal inference, adopting a \emph{Relation-Oriented} viewpoint, identifies the underlying causal structures across relative timings but overlooks the $\mathbb{R}^T$ space due to neglecting temporal nonlinearities, i.e., the dynamics. 
Under the \emph{Observation-Oriented} paradigm, contemporary causal learning is often challenged by incompletely captured dynamical effects without considering the indexing role of unobservable relations lying in $\mathbb{R}^H$.
In the case of LLMs, while AI techniques enable the autonomous identification of dynamical effects, they often neglect their interactions, which are emphasized as causal structures in causal inference.

% When viewed through the lens of the \emph{Relation-Oriented} framework, the multifaceted issues surrounding causality learning become unified, addressing common confusions and concerns from traditional causal inference to modern LLMs. 
Recalling the queries presented in the Introduction, we systematically summarize these application-related restrictions in our pursuit of AGI, and offer new insights:
\begin{enumerate}[itemsep=0em, topsep=-1pt,
parsep=2pt, partopsep=0pt,
leftmargin=22pt, labelwidth=10pt]
\item[\ding{118}] \emph{Firstly}, challenges for causal inference models primarily arise from overlooking dynamics, due to their linear modeling constraints. This oversight leads to various compensatory efforts, such as introducing hidden confounders and relying on the causal sufficiency assumption. 
Causal DAGs inherently provide a \emph{Relation-Oriented} view; with the proposed enhancement incorporating them into the counterfactual $\mathbb{R}^T$ space, they can provide essential support for illustrating structuralized dynamics.

\item[\ding{118}] \emph{Secondly}, our knowledge inherently contains hierarchical levels due to hidden relations $\omega\in \mathbb{R}^H$, which necessitates generalizability of models. 
For AI-based causal models, the main challenge lies in incorporating the underlying structure of dynamics to achieve dynamical generalizability. The new paradigm we propose introduces a relation-indexing methodology, enabling the autonomous construction of causal structures by sequentially extracting causal representations.


\item[\ding{118}] \emph{Thirdly}, while existing language models have made strides in generalizability through meta-learning, they are still limited to absolute timing within $\mathbb{R}^O$, implicitly assuming nonlinear independence among temporal dimensions. Additionally, their neglect of extracting informative $\theta$ prevents them from truly ``understanding'' the captured relationships.
However, LLMs have demonstrated the effectiveness of meta-learning in addressing temporal dimensional hierarchies, suggesting a promising prospect for \emph{Relation-Oriented} meta-learning in advancing towards AGI.
\end{enumerate}

We also introduce a baseline implementation of the \emph{Relation-Oriented} paradigm, primarily to validate the efficacy of the ``relation-indexing'' methodology in implementing causal representations and constructing knowledge-aligned causal structures. Similar thoughts have been attempted in domains with well-established causal knowledge, such as the hierarchical temporal memory in neuroscience \cite{wu2018hierarchical}. The pursuit of AGI is a historically extensive and complex endeavor, requiring a wide array of knowledge-aligned AI model constructions. This study aims to provide foundational insights for future developments in this field.




% Driven by the misalignment issues between causal knowledge and established causal models in widespread AI applications, this study examines fundamental limitations of the dominant \emph{Observation-Oriented} learning paradigm. In response, we advocate for a novel \emph{Relation-Oriented} paradigm, inspired by the relation-centric nature of human knowledge, and complemented by a practical approach of \emph{Relation-Indexed Representation Learning} (RIRL), with demonstrated efficacy.

% The concept of a ``hyper-dimension'' is initially proposed, as an accommodation for unobservable knowledge. We subsequently build a comprehensive framework of dimensionality, to offer more intuitive insights into relationship learning. 
% The discrepancy, between our comprehension of ``time'' and the single timeline used in our causal models, inherently causes misalignment, and results in model generalizability issues.

% \emph{Relation-Oriented} reflects the process of human understanding, 
% aims to mitigate AI misalignment, paving the way toward causally interpretable AGI. Constructing AGI is a long-term, intricate process requiring extensive work within interdisciplinary efforts, and we seek to lay a foundation for its future advancements.



\ifpreprint
\vspace{5mm}
\section*{Acknowledgements}
\vspace{-2mm}

I'd like to extend my heartfelt thanks to my friend, Dr. Gao, Qiman, the lone companion willing to engage in profound philosophical discussions with me, and who has provided invaluable advice. Additionally, my gratitude goes to GPT-4 for its assistance in enhancing my English writing.
I also wish to thank my advisor, Prof. Vipin Kumar, for the initial support in the beginning stage of this work.

\hfill Jia Li, Nov 2023

\vspace{10mm}

\else
\vspace{60mm}
%\pagebreak
\fi

\bibliography{main}
\bibliographystyle{tmlr}


\appendix
\section{Appendix: Complete Experimental Results of Causal Discovery}
%\includepdf[pages=-]{mainz_append_pdf.pdf}

\pagebreak


%\newpage
\begin{sidewaystable}
%\begin{table*}[t]
\centering
\caption{The Complete Results of Heuristic Causal Discovery in latent space.
Each row stands for a round of detection, with `\#'' identifying the round number, and all candidate edges are listed with their KLD gains as below. 1) Green cells: the newly detected edges. 2) Red cells: the selected edge.
3) Blue cells: the trimmed edges accordingly. }
%\vspace{-20mm}
\label{tab:discv_rounds}
\resizebox{1\columnwidth}{!}{%
%\rotatebox{90}{

\begin{tabular}{|c|c|c|cccccccccccccc}
\cline{1-9}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \textbf{A$\rightarrow$ C}} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ D & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{B$\rightarrow$ C}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ D} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 1}} & \cellcolor[HTML]{FFCCC9}{\color[HTML]{000000} \textbf{7.6354}} & 19.7407 & \multicolumn{1}{c|}{60.1876} & \multicolumn{1}{c|}{119.7730} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{8.4753}} & \multicolumn{1}{c|}{8.5147} & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} &  &  &  &  &  &  &  &  \\ \cline{1-13}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ D & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{B$\rightarrow$ D}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ D} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 2}} & 19.7407 & 60.1876 & \multicolumn{1}{c|}{119.7730} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{8.5147}}} & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}10.1490} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}46.5876} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}111.2978} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}11.6012} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}39.2361} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}95.1564} &  &  &  &  \\ \hline
\rowcolor[HTML]{C0C0C0} 
\cellcolor[HTML]{C0C0C0} & \textbf{A$\rightarrow$ D} & A$\rightarrow$ E & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{C$\rightarrow$ D}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 3}} & \cellcolor[HTML]{CBCEFB}\textbf{9.7357} & 60.1876 & \multicolumn{1}{c|}{119.7730} & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{1.1355}}} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{11.6012} & \multicolumn{1}{c|}{39.2361} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}63.7348} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}123.3203} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}27.8798} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}25.1988} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}75.5775} \\ \hline
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{000000} \textbf{C$\rightarrow$ G}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 4}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{000000} \textbf{11.6012}}} & \multicolumn{1}{c|}{39.2361} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{27.8798} & \multicolumn{1}{c|}{25.1988} & \multicolumn{1}{c|}{75.5775} &  &  \\ \cline{1-15}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{D$\rightarrow$ G}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}G$\rightarrow$ J} &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 5}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{39.2361} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{2.4540}}} & \multicolumn{1}{c|}{25.1988} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}5.2924} &  &  \\ \cline{1-15}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{G$\rightarrow$ J}}} &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 6}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{39.2361} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{25.1988} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{5.2924}}} &  &  &  \\ \cline{1-14}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{C$\rightarrow$ H}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{D$\rightarrow$ H}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 7}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{39.2361}} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{25.1988}}} & \multicolumn{1}{c|}{75.5775} &  &  &  &  \\ \cline{1-13}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ E & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{H$\rightarrow$ J}}} &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 8}} & 60.1876 & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{46.5876} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{0.2092}}} &  &  &  &  &  \\ \cline{1-12}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}\textbf{A$\rightarrow$ E} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{C$\rightarrow$ E}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 9}} & \cellcolor[HTML]{CBCEFB}\textbf{60.1876} & 119.7730 & \multicolumn{1}{c|}{65.9335} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{46.5876}}} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{63.7348} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} &  &  &  &  &  &  \\ \cline{1-13}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \cellcolor[HTML]{C0C0C0}{\color[HTML]{333333} \textbf{B$\rightarrow$ E}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{D$\rightarrow$ E}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ I} &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 10}} & 119.7730 & \cellcolor[HTML]{FFCCC9}{\color[HTML]{333333} \textbf{-6.8372}} & \multicolumn{1}{c|}{132.7717} & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{17.0407}} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}53.6806} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}-5.9191} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}-3.2931} & \multicolumn{1}{c|}{\cellcolor[HTML]{B8F5B7}110.2558} &  &  &  &  \\ \cline{1-13}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \cellcolor[HTML]{C0C0C0}B$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{E$\rightarrow$ G}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ I} &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 11}} & 119.7730 & 132.7717 & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{53.6806} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{-5.9191}} & \multicolumn{1}{c|}{-3.2931} & \multicolumn{1}{c|}{110.2558} &  &  &  &  &  &  \\ \cline{1-11}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}A$\rightarrow$ F & \cellcolor[HTML]{C0C0C0}B$\rightarrow$ F & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{E$\rightarrow$ H}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ I} &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 12}} & 119.7730 & 132.7717 & \multicolumn{1}{c|}{111.2978} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{123.3203} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{53.6806} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{-3.2931}} & \multicolumn{1}{c|}{110.2558} &  &  &  &  &  &  &  \\ \cline{1-10}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}\textbf{A$\rightarrow$ F} & \cellcolor[HTML]{C0C0C0}\textbf{B$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{C$\rightarrow$ F}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}C$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{D$\rightarrow$ F}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{E$\rightarrow$ F}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}E$\rightarrow$ I} &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 13}} & \cellcolor[HTML]{CBCEFB}\textbf{119.7730} & \cellcolor[HTML]{CBCEFB}\textbf{132.7717} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{111.2978}} & \multicolumn{1}{c|}{95.1564} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{123.3203}} & \multicolumn{1}{c|}{75.5775} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{53.6806}} & \multicolumn{1}{c|}{110.2558} &  &  &  &  &  &  &  &  \\ \cline{1-9}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}C$\rightarrow$ I & \cellcolor[HTML]{C0C0C0}D$\rightarrow$ I & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{E$\rightarrow$ I}} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{F$\rightarrow$ I}} &  &  &  &  &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 14}} & 95.1564 & 75.5775 & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{110.2558}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{45.6490}} &  &  &  &  &  &  &  &  &  &  &  &  \\ \cline{1-5}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}C$\rightarrow$ I & \cellcolor[HTML]{C0C0C0}D$\rightarrow$ I & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}\textbf{I$\rightarrow$ J}} &  &  &  &  &  &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 15}} & 15.0222 & 3.3845 & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.0284}} &  &  &  &  &  &  &  &  &  &  &  &  &  \\ \cline{1-4}
\cellcolor[HTML]{C0C0C0} & \cellcolor[HTML]{C0C0C0}\textbf{C$\rightarrow$ I} & \cellcolor[HTML]{C0C0C0}\textbf{D$\rightarrow$ I} &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\textbf{\# 16}} & \cellcolor[HTML]{CBCEFB}\textbf{15.0222} & \cellcolor[HTML]{FFCCC9}\textbf{3.3845} &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\ \cline{1-3}
\end{tabular}

}%}
%\vspace{-3mm}
%\end{table*}

\end{sidewaystable}







\begin{sidewaystable}
%\centering

\caption{Average performance of 10-Fold FGES (Fast Greedy Equivalence Search) causal discovery, with the prior knowledge that each node can only cause the other nodes with the same or greater depth with it. An edge means connecting two attributes from two different nodes, respectively. Thus, the number of possible edges between two nodes is the multiplication of the numbers of their attributes, i.e., the lengths of their data vectors.
\\ (All experiments are performed with 6 different Independent-Test kernels, including chi-square-test, d-sep-test, prob-test, disc-bic-test, fisher-z-test, mvplr-test. But their results turn out to be identical.)}
\label{tab:fges}
\resizebox{1\columnwidth}{!}{%
%\Rotatebox{90}{
%
\begin{tabular}{cccccccccccccccccc}
\hline
\rowcolor[HTML]{C0C0C0} 
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}Cause Node} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}A} & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}B} & \multicolumn{3}{c|}{\cellcolor[HTML]{C0C0C0}C} & \multicolumn{3}{c|}{\cellcolor[HTML]{C0C0C0}D} & \multicolumn{3}{c|}{\cellcolor[HTML]{C0C0C0}E} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}F} & \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}G} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}H} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}I} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}True\\ Causation\end{tabular}} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}A$\rightarrow$ C} & \multicolumn{1}{c|}{B$\rightarrow$ D} & \multicolumn{1}{c|}{B$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}C$\rightarrow$ D} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}C$\rightarrow$ E} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}C$\rightarrow$ G} & \multicolumn{1}{c|}{D$\rightarrow$ G} & \multicolumn{1}{c|}{D$\rightarrow$ H} & \multicolumn{1}{c|}{D$\rightarrow$ I} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}E$\rightarrow$ F} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}E$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}E$\rightarrow$ H} & \multicolumn{1}{c|}{F$\rightarrow$ I} & \multicolumn{2}{c|}{\cellcolor[HTML]{EFEFEF}G$\rightarrow$ J} & \multicolumn{1}{c|}{H$\rightarrow$ J} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}I$\rightarrow$ J} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Number of\\ Edges\end{tabular}} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}16} & \multicolumn{1}{c|}{24} & \multicolumn{1}{c|}{16} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}6} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}4} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8} & \multicolumn{1}{c|}{12} & \multicolumn{1}{c|}{12} & \multicolumn{1}{c|}{9} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8} & \multicolumn{1}{c|}{12} & \multicolumn{2}{c|}{\cellcolor[HTML]{EFEFEF}4} & \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}3} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Probability\\ of Missing\end{tabular}} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.038889} & \multicolumn{1}{c|}{0.125} & \multicolumn{1}{c|}{0.125} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.062} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.06875} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.039286} & \multicolumn{1}{c|}{0.069048} & \multicolumn{1}{c|}{0.2} & \multicolumn{1}{c|}{0.142857} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.3} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.003571} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.2} & \multicolumn{1}{c|}{0.142857} & \multicolumn{2}{c|}{\cellcolor[HTML]{EFEFEF}0.0} & \multicolumn{1}{c|}{0.072727} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}0.030303} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Wrong \\ Causation\end{tabular}} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}C$\rightarrow$ F} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{D$\rightarrow$ E} & \multicolumn{1}{c|}{D$\rightarrow$ F} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{F$\rightarrow$ G} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}G$\rightarrow$ H} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}G$\rightarrow$ I} & \multicolumn{1}{c|}{H$\rightarrow$ I} &  \\ \cline{1-1} \cline{5-5} \cline{9-10} \cline{14-17}
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Times\\ of Wrongly\\ Discovered\end{tabular}} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}5.6} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{1.2} & \multicolumn{1}{c|}{0.8} &  &  & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{5.0} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}8.2} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}3.0} & \multicolumn{1}{c|}{2.8} &  \\ \cline{1-1} \cline{5-5} \cline{9-10} \cline{14-17}
\multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{}
\end{tabular}
%}
}


\vspace{0.4in}
%\end{table*}

%\begin{table*}[t]
%\centering
\caption{ Brief Results of the Heuristic Causal Discovery in latent space, identical with Table 3 in the paper body, for better comparison to the traditional FGES methods results on this page.\\
The edges are arranged in detected order (from left to right) and their measured causal strengths in each step are shown below correspondingly.\\
Causal strength is measured by KLD values (less is stronger). Each round of detection is pursuing the least KLD gain globally. All evaluations are in 4-Fold validation average values. Different colors represent the ground truth causality strength tiers (referred to  the Figure 10 in the paper body).
}
\label{tab:discv}
%\resizebox{0.1\columnwidth}{!}{%
%\Rotatebox{90}{
\vspace{2mm}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{|c|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
\cellcolor[HTML]{C0C0C0}Causation & \cellcolor[HTML]{FFCCC9}A$\rightarrow$ C & \cellcolor[HTML]{FFCCC9}B$\rightarrow$ D & \cellcolor[HTML]{FFCCC9}C$\rightarrow$ D & \cellcolor[HTML]{FFCCC9}C$\rightarrow$ G & \cellcolor[HTML]{FFCCC9}D$\rightarrow$ G & \cellcolor[HTML]{FFCCC9}G$\rightarrow$ J & \cellcolor[HTML]{FFCCC9}D$\rightarrow$ H & \cellcolor[HTML]{FFCCC9}H$\rightarrow$ J & \cellcolor[HTML]{FFCE93}C$\rightarrow$ E & \cellcolor[HTML]{FFCE93}B$\rightarrow$ E & \cellcolor[HTML]{FFCE93}E$\rightarrow$ G & \cellcolor[HTML]{FFCE93}E$\rightarrow$ H & \cellcolor[HTML]{ABE9E7}E$\rightarrow$ F & \cellcolor[HTML]{ABE9E7}F$\rightarrow$ I & \cellcolor[HTML]{ABE9E7}I$\rightarrow$ J & \cellcolor[HTML]{ABE9E7}D$\rightarrow$ I \\ \hline
\cellcolor[HTML]{C0C0C0}KLD & 7.63 & 8.51 & 10.14 & 11.60 & 27.87 & 5.29 & 25.19 & 15.93 & 46.58 & 65.93 & 39.13 & 39.88 & 53.68 & 45.64 & 17.41 & 75.57 \\ \hline
\rowcolor[HTML]{FFFFFF} 
\cellcolor[HTML]{C0C0C0}Gain & 7.63 & 8.51 & 1.135 & 11.60 & 2.454 & 5.29 & 25.19 & 0.209 & 46.58 & -6.84 & -5.91 & -3.29 & 53.68 & 45.64 & 0.028 & 3.384 \\ \hline
\end{tabular}
%}}
%\end{table*}
}

\end{sidewaystable}








\end{document}
