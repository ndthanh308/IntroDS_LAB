 
 \pdfoutput=1

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in
\usepackage{arxiv}
\usepackage[export]{adjustbox}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{advdate}
\usepackage{mathtools}
\usepackage{subcaption}
\newcommand{\comment}[1]{}
\usepackage{float}
\usepackage{placeins}
%  \usepackage[utf8x]{inputenc} 
\usepackage{natbib}   

\usepackage{blindtext}
\input{math_commands.tex}
\usepackage{amsthm}
\usepackage[scientific-notation=true]{siunitx}
\usepackage{amsthm}
\usepackage{float}
\usepackage{placeins}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{remark}{Remark}[section]

\newtheorem{lemma}{Lemma}
\newtheorem{sublemma}{Lemma}[lemma]

% \theoremstyle{proposition}
\newtheorem{proposition}{Proposition}[section]
\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage{xcolor}
\newcommand{\todo}[1]{\textcolor{red}{[TODO: #1]}}
% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage[hyphens]{url}
%\usepackage[hidelinks]{hyperref}
\usepackage[hidelinks,bookmarks=false]{hyperref}
\usepackage{nccmath}    % align & gather doesn't use shortskip: https://tex.stackexchange.com/a/386266/206774

\usepackage[utf8]{inputenc}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\urlstyle{same}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{booktabs}
\usepackage{natbib} 


% Algorithmic modifications (defining the \BREAK command)
\makeatletter
\newcommand{\ALOOP}[1]{\ALC@it\algorithmicloop\ #1%
  \begin{ALC@loop}}
\newcommand{\ENDALOOP}{\end{ALC@loop}\ALC@it\algorithmicendloop}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand{\algorithmicbreak}{\textbf{break}}
\newcommand{\BREAK}{\STATE \algorithmicbreak}
\makeatother




\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\eqnref}[1]{(\ref{#1})}

\newcommand{\neighborhood}[1]{\gN({#1})}
\newcommand{\closedneighborhood}[1]{\overline{\gN}({#1})}
\newcommand{\cnsize}{P}
\newcommand{\layersuperscript}{l}
\newcommand{\gso}{\mS}
\newcommand{\recfn}{\rho}
\newcommand{\feedfn}{\phi}


\DeclareMathOperator*{\concat}{\scalebox{1}[1.5]{$\parallel$}}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.

\title{Prot2Text: Multimodal Protein's Function Generation with GNNs and Transformers}

\author{Hadi Abdine$^1$ \and Michail Chatzianastasis$^1$ \and Costas Bouyioukos$^{2,3}$ \and Michalis Vazirgiannis$^{1}$ \\
\affiliations
$^1$ DaSciM, LIX, École Polytechnique, Institut Polytechnique de Paris, France.\\
$^2$ Epigenetics and Cell Fate, CNRS UMR7216, Université Paris Cité, F-75013 Paris, France.\\
$^3$ Bioinformatics Research Laboratory, Department of Biological Sciences, University of Cyprus, Nicosia, CY 1678, Cyprus\\ 
\emails
\{hadi.abdine, ~michail.chatzianastasis\}@polytechnique.edu, \\
costas.bouyioukos@univ-paris-diderot.fr, \quad
mvazirg@lix.polytechnique.fr
}

% \author{
% Michail Chatzianastasis$^1$\footnote{Contact Author}\and
% Johannes F. Lutzeyer$^1$\and
% George Dasoulas$^1$\And
% Michalis Vazirgiannis$^1$\\
% \affiliations
% $^1$ DaSciM, LIX, École Polytechnique, Institut Polytechnique de Paris, France.\\
% \emails
% \{michail.chatzianastasis, ~johannes.lutzeyer\}@polytechnique.edu, \\
% george.dasoulas1@gmail.com, \quad
% mvazirg@lix.polytechnique.fr
% }

\begin{document}

\maketitle

 
\begin{abstract}
The complex nature of big biological systems pushed some scientists to classify its understanding under the inconceivable missions.
%
Different leveled challenges complicated this task, one of is the prediction of a protein's function.
% Protein function's prediction plays a crucial role in understanding the intricate workings of biological systems.
%
In recent years, significant progress has been made in this field through the development of various machine learning approaches.
%
However, most existing methods formulate the task as a multi-classification problem, i.e assigning predefined labels to proteins.
%
In this work, we propose a novel approach, \textbf{Prot2Text}, which predicts a protein function's in a free text style, moving beyond the conventional binary or categorical classifications.
%
By combining Graph Neural Networks(GNNs) and Large Language Models(LLMs), in an encoder-decoder framework, our model effectively integrates diverse data types including proteins' sequences, structures, and textual annotations.
%
This multimodal approach allows for a holistic representation of proteins' functions, enabling the generation of detailed and accurate descriptions.
%
To evaluate our model, we extracted a multimodal protein dataset from SwissProt, and demonstrate empirically the effectiveness of Prot2Text.
%
These results highlight the transformative impact of multimodal models, specifically the fusion of GNNs and LLMs, empowering researchers with powerful tools for more accurate prediction of proteins' functions. 
%
The code, the models and a demo will be publicly released.
\end{abstract}

\section{Introduction}
Understanding proteins' function is a central challenge in the field of biological sciences, as proteins play a crucial role in various biological processes.
%
Accurate prediction of proteins' function is essential for various applications, such as drug discovery, enabling researchers to identify and target specific proteins that play critical roles in disease pathways~\citep{HA2021394}.
%
Traditionally, proteins' functions prediction has been approached through classification methods, assigning predefined labels to proteins based on their characteristics~\citep{10.1093/bioinformatics/btz595}.
%
However, this approach often oversimplifies the complexity of proteins' functionality, limiting the depth of our understanding.
%
To address this limitation, we propose a novel view on proteins' functions prediction based on reformulating the task using free-text proteins' descriptions instead of relying on predefined labels.
% To address this limitation, we propose a novel view on proteins' functions prediction, 
% and instead of relying on predefined labels, we reformulate the task using free-text protein function descriptions. 
%
The rapid progress in transformer-based models has brought a massive revolution to the field of Natural Language Processing (NLP).
%
These models have demonstrated impressive language generation capabilities, allowing them to perform a wide range of NLP tasks with remarkable performance, including text completion, translation, sentiment analysis and question-answering~\citep{transformers,radford2019language-gpt2,brown2020language}.
%
On the other hand, Graph Neural Networks(GNNs) have emerged as a powerful tool for modeling graph-structured data, capturing the intricate relationships between different elements in a graph~\citep{kipf2017semi,reiser2022graph}.
%
However, the integration of GNNs and transformers faces various challenges, such as effectively handling the heterogeneity of data representations, and the field is still in its early stages.
%
Despite this, the potential benefits of leveraging both GNNs and transformers for graph-to-text applications, such as predicting the functional properties of proteins are substantial.
%
%The rapid progress in transformer-based multimodal models has revolutionized the field of natural language processing (NLP) by enabling the unified processing of diverse input data types, including text, images, and audio~\citep{liu2023visual,driess2023palm}. These models have the remarkable ability to associate and synthesize information from multiple modalities, leading to significant advancements in various NLP tasks, including multimodal sentiment analysis, visual question answering, and image captioning.
%
To that end, we develop a novel multimodal framework, \textbf{Prot2Text}, that can generate detailed and accurate descriptions of proteins' functions in free text.
%
We effectively integrate GNNs and Large Language Models (LLMs), to encompass both structural and sequential information of the protein's 3D structure and amino acid's sequence respectively.
%
The encoder-decoder architecture forms the backbone of our model, with the encoder component employing a Relational Graph Convolution Network~(RGCN)~\citep{schlichtkrull2018modeling} to process the proteins' graphs and the ESM protein language model~\citep{lin2023evolutionary-esm2}) to encode the protein's sequence.
%
The decoder component utilizes a pre-trained GPT2 model to generate detailed proteins' descriptions.
%
To train our multimodal model, we compile a dataset of proteins extracted from SwissProt, a comprehensive collection of protein annotations obtained from the UniProt database~\citep{uniprot2015uniprot}.
%
This dataset encompasses a vast number of proteins, each annotated with its corresponding function or description.
%
In addition to the textual information, we obtain the 3D structure representation of the proteins from AlphaFold~\citep{varadi2022alphafold}.
%
We further release this curated dataset to the public, allowing other researchers to use it for benchmarking and further advancements in the field.
%
Our main contributions can be summarized as follows:
\begin{itemize}
    \item We introduce the \textbf{Prot2Text} framework, a novel multimodal approach for generating proteins' functions in free text. Our model combines both GNNs and ESM to encode the protein in a fused representation while a pre-trained GPT2 decodes the protein's text description. 
    \item We propose various baselines for protein text generation and demonstrate that the integration of both graph and sequence protein information leads to better generation capabilities. 
    \item We further release a comprehensive multimodal protein dataset, which includes $256,690$ protein structures, sequences, and textual function descriptions.
    % 
    Researchers can leverage this dataset to benchmark and compare their models, thereby driving advancements in the field and enabling for a more robust and standardized evaluation of proteins' functions prediction methods in free text format.
\end{itemize}


\section{Related Work}
\paragraph{Transformers.}
The transformer-based encoder-decoder model was first introduced by \citet{transformers} in their paper “Attention is all you need”.
%
Since then, this model architecture has become the de-facto standard encoder-decoder architecture in Natural Language Processing (NLP).
%
Despite going through significant research on different pre-training objectives for transformer-based encoder-decoder models such as T5 \citep{raffel2019exploring-T5} and Bart \citep{lewis-etal-2020-bart}, the model architecture has remained largely the same.
%
\citet{radford2018improving-gpt} took advantage of the transformer architecture \citep{transformers}, which is superior and conceptually simpler than Recurrent Neural Networks to introduce the OpenAI GPT model.
%
Specifically, they pre-trained a left-to-right transformer decoder as a general language model using the GPT architecture.
%
Following, they fine-tuned the model on 12 different language understanding tasks by applying various transformations to the input.
%
Later on, GPT-2 \citep{radford2019language-gpt2} was introduced, a more advanced version of GPT having more trainable parameters.
%
The authors showed that as long as general language models have very high capacities, they can reach reasonable performance on many specific natural language processing tasks. 
%
The use of the transformer's architecture is expanded later to include modalities other than natural language such as images \citep{vit}, protein amino-acid sequences \citep{rives2021biological-esm1, lin2023evolutionary-esm2}, and molecules SMILES string \citep{fabian2020molecular-molbert, chithrananda2020chemberta}.
%
They are all pretrained with the Masked Language Modeling task (MLM) introduced in BERT \citep{devlin-etal-2019-bert} and are able mostly to perform discriminative tasks.
%
\paragraph{Multimodal models.}
The success of the transformer's architecture uni-modality tasks made this architecture broadly studied for multimodal representation learning.
%
One example is The CLIP (Contrastive Language-Image Pre-training) model \citep{clip} which is a transformer model that facilitates cross-modal understanding between images and text.
%
It combines a ViT vision encoder, with a transformer-based language encoder to learn joint representations of images and their associated textual descriptions.
%
By leveraging transformers in both the vision and text encoders, the CLIP model benefits from their ability to capture long-range dependencies.
%
Another example is the MolT5 \citep{edwards2022translation-MolT5} which is a self-supervised learning framework based on the T5 model \citep{raffel2019exploring-T5} for pretraining models on a vast amount of unlabeled natural language text and molecule SMILES strings.
%
MolT5 is able to perform bidirectional translation between molecule representations and natural language allowing molecule captioning and generation providing text prompts.

\paragraph{Graph Neural Networks.}
Graph neural networks (GNNs) have emerged as a powerful framework for modeling and analyzing graph-structured data~\citep{scarselli2009graph,kipf2017semi}.
%
By iteratively exchanging and integrating information among nodes, GNNs can propagate and refine features throughout the graph, ultimately encoding a comprehensive understanding of the graph's structure and semantics.
%
This ability to capture complex relationships within graphs has contributed to the success of GNNs in various domains, including social network analysis, recommendation systems, and bioinformatics~\citep{zitnik2018modeling,zhang2021graph,chatzianastasis2023explainable}.
%
Numerous studies have suggested various enhancements and expansions to the GNNs' models.
%
Some notable contributions include the introduction of more expressive and adaptable aggregation functions, such as those proposed by \citet{murphy2019relational}, \citet{seo2019discriminative} and \citet{chatzianastasis2023graph}.
%
Moreover, several schemes have been developed to incorporate different local structures or high-order neighborhoods, as explored by \citet{morris2020weisfeiler} and \citet{nikolentzos2020k}.
%
Furthermore, the domain of GNNs has expanded to encompass heterogeneous graphs, where nodes and edges can have different types and semantics, leading to the development of Heterogeneous Graph Neural Networks effectively handling such complex graph structures~\citep{schlichtkrull2018modeling,zhang2019heterogeneous}.

\paragraph{Protein Representation Learning.}
In the field of protein representation learning, various approaches have emerged over the years, aiming to capture meaningful information from proteins using different data modalities and computational techniques.
%
One prominent avenue of research is focused on sequence-based representations, that extract features solely from the amino-acid sequences of proteins.
%
To achieve this, deep learning techniques such as Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) have been applied, enabling the direct learning of representations from protein sequences~\citep{liu2017deep,bileschi2019using}.
%
Drawing inspiration from the remarkable achievements of language models in Natural Language Processing (NLP), researchers have also developed pre-trained language models tailored specifically for proteins \citep{brandes2022proteinbert,lin2023evolutionary}.
%
These models leverage large-scale protein datasets to learn powerful representations that can subsequently be utilized for various prediction tasks.
%
In addition to sequence-based approaches, graph-based representations leverage the three-dimensional (3D) structure of proteins to capture their functional properties.
%
\citet{zhang2022protein} proposed a graph neural network model with a contrastive pertaining strategy for function prediction and fold classification tasks.
%
\citet{chen20233d} proposed a 3D-equivariant graph neural network, specifically designed to estimate the accuracy of protein structural models.
%
\citet{wang2022learning} used a hierarchical graph network, which captures the hierarchical relations present in proteins and learns representations at different levels of granularity.
%
Hybrid approaches integrate multiple data modalities, such as protein sequences, structures, and functional annotations, to create comprehensive representations.
%
These methods combine the strengths of sequence-based and graph-based approaches to capture diverse aspects of protein function.
%
\citet{gligorijevic2021structure} proposed DeepFRI which combines sequence features extracted from a pre-trained protein language model with protein structures.
%
Our work aims to leverage protein sequence and structure models to generate free text annotations of proteins.

\section{Methodology}
In this section, we present our proposed multimodal framework, \textbf{Prot2Text}, for generating protein function descriptions in free text.
%
%We begin by describing the protein graph construction. Then, we analyze the multimodal encoder-decoder model, which integrates Graph Neural Networks (GNNs) and Large Language Models to achieve a comprehensive text generation of protein function.

% Figure environment removed

%Let a heterogeneous graph denoted as \(G = (V, E, R)\), where \(V = [N] := \{1, ..., N\}\) is the set of vertices, \(E \subseteq V \times V\) is the set of edges and $R$ is a set of edge types. Each node $u$ is associated with a feature vector $\vx_u \in \R^d$. 
%Each edge $i=(v,u)$ is associated with an edge type $\ve_i \in R$.
%We denote the neighborhood of type $r$ of a vertex $u$ by $\mathcal{N}_r(u)$ such that
%$\mathcal{N}_r(u)$ = $\{v : (v, u) \in E_r\}$, where $E_r$ is the set of edges with $r$ edge type. 


\paragraph{Graph Construction.}
%We begin our approach by employing AlphaFold~\citep{}, a state-of-the-art deep learning model developed for the prediction of protein structures. AlphaFold utilizes a novel two-step process: a co-evolutionary neural network and a refinement network. The co-evolutionary neural network analyzes multiple sequence alignments (MSAs) of homologous proteins to capture evolutionary information, while the refinement network refines the 3D model generated by the co-evolutionary network through an attention-based architecture. The combination of these steps results in highly accurate 3D protein structures.
%
Upon obtaining the 3D proteins' structures using AlphaFold, we proceed to represent the proteins as a heterogeneous graph \(G = (V, E, R)\), where \(V = [N] := \{1, ..., N\}\) is the set of vertices representing the amino-acids of the proteins, \(E \subseteq V \times V\) is the set of edges representing various interactions between the nodes and $R$ is a set of different edge interactions.
%
Each node $u$ is associated with a feature vector $\vx_u \in \R^d$, encompassing relevant information such as local structural features, and physicochemical properties of the associated amino-acids.
%
This enables the graph to retain fine-grained information critical to the protein's structure and function. 
%

To model the diverse interactions and relationships between amino acids, we introduce different types of edges connecting the nodes.
%
Therefore, each edge $i=(v,u)$ is associated with an edge type $\ve_i \in R$.
%
Sequential edges are employed to connect adjacent nodes in the protein sequence, effectively representing the sequential order of amino acids and capturing the linear arrangement of the protein's primary structure.
%
This sequential information is crucial for understanding the folding patterns and functional motifs within the protein.
%
Additionally, we utilize spatial edges to establish connections between nodes that are in close spatial proximity within the 3D structure of the protein.
%
These edges play a pivotal role in encoding the protein's tertiary structure and folding patterns, enabling us to capture the intricate spatial arrangements of amino acids within the protein's core.
%
We further extend the graph construction to include hydrogen bond interactions as an additional edge type.
%
Hydrogen bonds are fundamental non-covalent interactions that are of paramount importance in stabilizing protein structures and enabling specific molecular recognition events. Through the integration of the different edge types, our comprehensive protein graph provides a more holistic and detailed depiction of the protein's structure while capturing both short and long-range interactions. 

\paragraph{Graph Encoding.}
To encode the protein graph $G$ into a vector $\vh_G \in \R^{d_{out}}$, we employ a Relational Graph Convolutional Neural Network(RGCN)~\citep{schlichtkrull2018modeling}, which effectively considers the various edge types present in the graph in the message-passing mechanism.
%
We denote the neighborhood of type $r$ of a vertex $u$ by $\mathcal{N}_r(u)$ such that
$\mathcal{N}_r(u)$ = $\{v : (v, u) \in E_r\}$, where $E_r$ is the set of edges with $r$ edge type.
%
In layer $k$ of the GNN, we update the node representations as follows:  
\begin{equation}
\label{eq:rgcn}
\vx^{k}_i = \sigma\left(\mW_{\textrm{root}}^k \cdot
\vx_i^{k-1} + \sum_{r \in \mathcal{R}} \sum_{j \in \mathcal{N}_r(i)}
\frac{1}{|\mathcal{N}_r(i)|} \mW_r^k \cdot \vx_j^{k-1}\right),
\end{equation}
where $\mW_{\textrm{root}}^k$ represents the learnable weight matrix for the root transformation in layer $k$, $\mW_r^k$ denotes the learnable weight matrix of layer $k$ for relation $r$ and $\sigma(\cdot)$ is an element-wise activation function such as ReLU.
%
This formulation allows nodes to update their representations by incorporating information from neighboring nodes based on the specific edge types, capturing the structural and relational dependencies within the protein graph.
%
To obtain the graph representation from the node representations of the last layer $K$ of the GNN, we apply a mean-pooling layer as follows:
\begin{equation}
    \vh_G = \frac{1}{N} \sum_{i=1}^N \vx_i^K  
\end{equation}
The resulting vector $\vh_G$ serves as an informative encoded representation of the protein graph, capturing the essential structural and relational characteristics.
%
This representation plays a crucial role in the subsequent text generation process, where it will be utilized to generate detailed and accurate protein functions.

\paragraph{Sequence Encoding.}
%\subsection{Multimodal Protein Encoding}
%Protein Language Models (PLMs) are a groundbreaking class of artificial intelligence models designed to understand and generate proteins' structural and functional properties. PLMs are typically pre-trained on massive datasets that include vast amounts of protein sequences using the masked language modeling objective (MLM).
To encode the protein sequence \(P_S\), we used ESM2-35M \citep{lin2023evolutionary-esm2} as our base model.
%
ESM2 is a protein language model that uses a transformer-based architecture and an attention mechanism to learn the interaction patterns between pairs of amino acids in the input sequence.
%
This allows the ESM model to capture evolutionary information about proteins and their properties.
%
In order to achieve uniform representation dimensions for all modalities within the spatial domain, a projection layer is applied after the last hidden layer of the ESM model.
%
This layer functions as a projection layer that transforms the individual amino-acid representations, derived from the ESM embedding dimension, into the graph embedding dimension $d_{out}$.
%
As a result, a matrix denoted as $\mH^0_{S} \in \R^{N,d_{out}}$ is formed, containing the amino-acid representations:
\begin{equation}
    \mH^0_{S} = ESM(P_S)\mW_{p}
\end{equation}
where $\mW_{p}$ is a trainable matrix.
\paragraph{Multimodal Fusion.}
% To obtain the final protein encoding, the sequence representation $\mH_S$ is enriched by the graph representation $\vh_G$ of the protein using cross attention. Cross-attention is a pivotal mechanism utilized in multimodal deep learning architectures to effectively obtain the ultimate encoding for a given input instance by fusing information from distinct modalities. In this process, each modality's representation is attentively weighted and combined with the other modalities' representations, thus enabling contextually enriched encoding.
% We have implemented four cross-attention layers, each followed by projection layers and normalization layers. The final multimodal protein representation $\mH_P$ is given by the following equation: 
%$$\mH_P = \left(\mH_S+softmax\left(\frac{(\mH_S\mW_Q)(\vh_G\mW_K)^T}{\sqrt{d_{out}}}\right)\vh_G\mW_V\right)W_{O}$$
To obtain the final protein encoding, we utilize a fusion block that combines the representation of each amino acid inside the matrix $\mH^0_S$ with the graph representation vector $\vh_G$.
%
The fusion process involves a simple element-wise addition of the two representations, followed by a projection layer.
%
This fusion block enables the integration of information from both the sequence and the graph representations in a straightforward manner.
%
Thus, allowing each amino acid to be contextually enriched with information from the graph representation.
%
Additionally, a normalization layer is applied after each fusion block to maintain stable training and further enhance the learning process.
%
Specifically, for each amino acid representation in $\mH^k_S$, and the graph representation $\vh_G$, the fusion block computes the combined representation $\mH^{k+1}_S$ as follows:
\begin{equation}
    \mH^{k+1}_{S} = \left(\mH^k_{S}+ \mathbf{1}_n\vh_{G}\mW^k_{V}\right)\mW^k_{O},
\end{equation}
where $\mW_{V} and \mW_{O}$ are trainable matrices.\\
By using this fusion block multiple times in the architecture (four times in this case), the model can capture complex interactions and dependencies between the sequence and graph representations, leading to an effective and contextually enriched encoding of the protein data.
%
The fusion block could be seen as a special case of the transformers cross-attention block when the the input from the encoder represents only one token.

\paragraph{Text Generation}
We employed the transformer decoder architecture for generating protein descriptions.
%
We initialized the main components of the decoder, namely the text embedding matrix, self-attention, and language modeling head, with the pre-trained weights of GPT-2.
%
By doing so, we leveraged the GPT-2 model's capacity to grasp the underlying textual semantics.
%
We forward the protein representation obtained from the protein encoder as input to the multi-head cross-attention module within the transformer decoder.
%
This interaction enabled the model to effectively incorporate context from the protein representation, contributing to the generation of coherent and meaningful protein descriptions.
%
We adopted the identical vocabulary and tokenizer from the GPT-2 model, with the introduction of two unique special tokens.
%
These additional tokens serve as essential markers, enabling the model to discern the precise boundaries of each protein description within the input text.
%
In the training phase, we employed Causal Language Modeling (CLM) as the training objective to optimize our model.
%
Causal Language Modeling involves training the model to predict the next token in a sequence given the preceding tokens.
%
This unidirectional prediction process ensures that the model generates text in a causal manner, without access to future tokens.
%
The maximum length of each description is 256 tokens.

\section{Experimental Results}
\paragraph{Dataset.}

% \todo{1: Include section about the data? (UniProt Release 2022\_04, UniProtKB/Swiss-Prot , includes data for 568 363 proteins, xxx xxx proteins have a description, processing: remove all proteins that contains \textit{By Similarity} description and removing the PubMed info from the description, splitting the dataset based on 40\% similarity using CD-Hit, dividing the resulting test set into validation and test sets [50-50], train\_set size: , valid\_set size: , test\_set size:}
To train the Prot2Text framework using proteins' structures, sequences and textual descriptions, we build a multimodal dataset with $256,690$ proteins.
%
For each protein, we have three crucial information: the corresponding sequence, the AlphaFold accession ID and the textual description.
%
To build this dataset, we used the SwissProt dababase \citep{swissprot} including the UniProtKB \citep{uniprot} Release 2022\_04.
%
Initially, The SwissProt database in this release has $568,363$ proteins on which we perform the following: (1) Select the following properties: \texttt{name} that gives the full name of the protein, \texttt{sequence} that gives the amino-acid sequence of the protein, \texttt{AlphaFoldDB} that gives the accession ID of the protein in AlphaFold database, \texttt{taxon} and \texttt{text} that gives the protein textual description.
%
(2) Eliminate all samples that do not have all three crucial information.
%
(3) Remove all samples with a duplicate amino-acid sequence.
%
(4) Remove all the samples where the textual description contains \textit{"(By Similarity)"}.
%
% (5) Apply the CD-HIT clustering algorithm \citep{cdhit} to form two datasets of $248,215$ proteins used as the training dataset and, $8,375$ proteins that are splitted randomly into a validation dataset with $4,172$ proteins and a test dataset with $4,023$ proteins.
(5) Apply the CD-HIT clustering algorithm \citep{cdhit} to create a train/validation/test scheme with $248,215$, $4,172$ and $4,023$ proteins respectively.
%
The maximum similarity threshold between the (train, validation
test) sets used in the CD-HIT algorithm is 40\%.
%
(6) Preprocess the textual description to remove the \textit{"PubMed"} information.
%
The AlphaFoldDB accession is then used to download the protein structure in a ".PDB" file format using version 4 from AlphaFoldDB.

\paragraph{Baselines.}
In our experimental evaluation, we employed a comprehensive set of baselines to rigorously assess the performance of the Prot2Text framework.
%
Specifically, we compared our approach against unimodal encoders, namely RGCN, ESM, and a vanilla-Transformer trained from scratch.
%
These encoders exclusively focus on either the protein graph or the protein sequence representation.
%
Furthermore, we compared it with a multimodal baseline, RGCN$+$ESM, that concatenates the graph and sequence representations without fusing the representation of each amino-acid and the structure representation.
%
Finally, we compare with RGCN $\times$ vanilla-Transformer baseline, which has similar architecture as Prot2Text but instead uses a vanilla-Transformer model from scratch instead of the pre-trained ESM2.
%
In all ESM models, we use the last hidden state.
%
The vanilla-Transformer baseline follows the same configuration and number of parameters as the pre-trained ESM2-35M.

\paragraph{Training Details.} 
We implemented all the models using PyTorch and utilized $64$ NVIDIA V100 GPUs for training.
%
We used the AdamW optimizer~\citep{loshchilov2018decoupled} with $\epsilon=10^{-6}$, $\beta_1=0.9$, $\beta_2=0.999$, with a learning rate starting from $2.10^{-4}$ and decreasing to zero using a cosine scheduler.
%
We used a warm-up of 6\% of the total training steps.
%
We fixed the batch size to four per GPU and we trained the models for 25 epochs.
%
For the GNN encoder, we used 6 layers with a hidden size equal to GPT-2's hidden size (768 for GPT-2 and $1,024$ for GPT-2\textsubscript{MEDIUM}) in each layer.
%
We used the same tokenizer and configuration of ESM2 including the hidden layer and hidden dimension.
%
The training for each Base model lasted for approximately $12$ hours.
%
All experiments were carried out using the \texttt{transformers} library~\citep{wolf-etal-2020-transformers}.

\paragraph{Metrics.}
In the experiments, we used several metrics to evaluate the performance of the model in the text generation task.
%
Specifically, we used \textit{BLEU Score}~\citep{papineni2002bleu} which is a widely used metric for evaluating the quality of machine-generated text.
%
It measures the similarity between the generated text and the reference text based on n-grams.
%
A higher BLEU score indicates better similarity between the generated and reference text.
%
We further used \textit{Rouge-1, Rouge-2} and \textit{Rouge-L} scores ~\citep{lin2004rouge}, which measure the overlap of unigrams, bigrams, and longest common subsequence between the generated text and the reference text, respectively.
%
Finally, we used \textit{BERT Score}~\citep{zhang2019bertscore}, which measures the similarity between the generated text and the reference text using contextualized word embeddings from a transformer-based model.
%
In our experiments we choose to use BioBERT\textsubscript{LARGE}-cased v1.1 \citep{lee2020biobert} to compute the \textit{BertScore}.

\begin{center}
\begin{table*}[t]
\centering
  \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    \textbf{Model}  & \textbf{\# Params}  & \textbf{BLEU Score} & \textbf{Rouge-1} & \textbf{Rouge-2} & \textbf{Rouge-L} & \textbf{BERT Score} \\
    \hline
    vanilla-Transformer  & 225M & 14.12 &27.13&18.53&25.34& 75.47 \\
    %\hline
    ESM2-35M  & 225M & 27.99 &45.74&36.84&43.47& 82.69 \\
    % \hline
    RGCN  & 220M & 19.50 &35.12&26.50&33.36& 78.58 \\
    % \hline
    RGCN $+$ ESM2-35  & 255M & 26.91 & 44.13 & 35.14& 41.90 & 82.03 \\
    RGCN $\times$ vanilla-Transformer  & 283M & 25.64 &41.08&32.93&39.21& 80.80 \\
    \textbf{Prot2Text}\textsubscript{BASE}  & 283M & \textbf{31.21} &\textbf{48.86}&\textbf{40.23}&\textbf{46.59}& \textbf{83.74} \\
    % \hline
    % RGCN X ESM2-150 & GPT2-Medium & 1B &  \textbf{31.74}&\textbf{48.67}&\textbf{40.58}&\textbf{46.70}&\textbf{83.46} \\
    \hline
  \end{tabular}
  \caption{Test set results for different encoder models, including unimodal encoders such as vanilla-Transformer, ESM2-35M, and RGCN, as well as multimodal encoders such as RGCN $\times$ vanilla-Transformer and RGCN $+$ ESM2-35. All models share the same GPT-2 decoder. 
  Prot2Text\textsubscript{BASE} achieves the highest performance across all evaluation metrics, including BLEU score, Rouge scores, and BERT Score.}
  \label{tab:encoder-decoder}
\end{table*}
\end{center}

% Figure environment removed

\begin{center}
\begin{table*}[t]
\centering
  \begin{tabular}{|c|c|c|c|c|c|c|c|}
    \hline
    \textbf{Model}  & \textbf{\# Params} & \textbf{BLEU Score} & \textbf{Rouge-1} & \textbf{Rouge-2} & \textbf{Rouge-L} & \textbf{BERT Score} \\
    \hline
    \textbf{Prot2Text}\textsubscript{SMALL}  & 256M & 26.30 &44.12&35.74&42.16& 82.06 \\
    % \hline
    \textbf{Prot2Text}\textsubscript{BASE} & 283M & 31.21 &\textbf{48.86}&40.23&46.59& \textbf{83.74} \\
    % \hline
    \textbf{Prot2Text}\textsubscript{MEDIUM}  & 760M &  \textbf{31.74}&48.67&\textbf{40.66}&\textbf{46.75}&83.46\\
    
    % \hline
    
    \hline
  \end{tabular}
  \caption{Test set results for different size variations of Prot2Text. Larger models outperform their smaller counterparts across most evaluation metrics, indicating the benefits of employing larger language models in the Prot2Text framework. The Prot2Text{BASE} model, strikes an optimal balance between performance and computational efficiency. This configuration demonstrates improved performance compared to the smaller model while still maintaining reasonable computational costs.}
  \label{tab:ablation_study}
\end{table*}
\end{center}


\paragraph{Results.}
We report the results in Table \ref{tab:encoder-decoder}, for different encoder models, including unimodal encoders like vanilla-Transformer, ESM2-35M, and RGCN, and multimodal encoders like RGCN $\times$ vanilla-Transformer and RGCN $+$ ESM2-35.
%
All models use a GPT-2 decoder. %that has, without the cross-attention blocks, 124M trainable parameters.
% By comparing Prot2Text's performance against unimodal encoders and the concatenation-based multimodal approach, we gained critical insights into the significance of cross-attention and the benefits of an integrated and contextualized multimodal representation. These comparisons yielded valuable findings that substantiated the merits of the Prot2Text model, highlighting its capacity to leverage the complementary strengths of GNNs and ESM in a coherent and synergistic manner for enhanced protein function prediction and accurate text generation.
The unimodal vanilla-Transformer baseline, relying solely on the amino-acid sequence of the protein, exhibits the lowest performance across all evaluation metrics.
%
However, we observe a significant improvement in performance when using the unimodal graph encoder RGCN.
%
The RGCN outperforms the vanilla-Transformer by over five absolute points in terms of BLEU score and three points in terms of BERT score.
%
This performance disparity highlights the importance of incorporating structural information through the RGCN encoder for protein's function prediction.
%

On the other hand, leveraging the pre-trained protein language model ESM2-35M instead of initializing the vanilla-Transformer randomly, results in a remarkable improvement in performance.
%
The ESM2-35M encoder leads to a substantial increase of over 13.5 BLEU score points and 18 Rouge-L points compared to the standard vanilla-Transformer configuration.
%
This notable enhancement can be attributed to the pretraining of ESM2-35M using masked protein modeling, which enables the encoder to capture intricate relationships and patterns within protein sequences.
%

In the context of multimodal protein representation, the evaluation results demonstrate that Prot2Text$_{BASE}$ exhibits superior performance across all assessment metrics.
%
Notably, it achieves the highest BLEU Score of $31.21$, the highest Rouge-1 score of $48.86$, the highest Rouge-2 score of $40.23$, the highest Rouge-L score of $46.59$, and the highest BERT Score of $83.74$.
%
These outcomes highlight the effectiveness of fusing protein structure and amino-acid information in a multimodal manner.
%
The incorporation of protein structure, facilitated by the Relational Graph Convolutional Network (RGCN) with the sequential representations of amino-acids from ESM2-35, significantly enhances the overall performance across all evaluation metrics.
%
This improvement is attributed to the enriched understanding of proteins achieved through the synergy of these two modalities.
%
Furthermore, the efficacy of the multimodal fusion approach is corroborated by the results obtained from RGCN $\times$ vanilla-Transformer.
%
Introducing structural information using RGCN to the randomly initialized vanilla-Transformer yields a substantial improvement of over $10$ BLEU score points compared to using the vanilla-Transformer alone, and more than $4.5$ BLEU score points improvement over using RGCN in isolation.
%

Finally, to show the importance of the fusion block in the Prot2Text framework, we compare it against 
RGCN $+$ ESM2-25, which concatenates the protein structure representation to the amino-acids representation.
%
In this case, the graph representation will simply be passed to the decoder alongside the ESM output.
%
We notice that using this strategy leads to slightly worse results than using the ESM alone.


\paragraph{Ablation Study: Scaling to Larger Models.}
We conducted an ablation study to assess the performance of our Prot2Text framework as we varied the number of parameters.
%
The primary objective of this experiment was to evaluate the benefits of employing larger models in terms of generating more accurate and detailed textual representations of protein's function.
%
To conduct the ablation study, we systematically varied the size of the underlying language models, ESM and GPT.
%
We evaluated each configuration on the same test set of proteins and used the same evaluation metrics as described earlier.
%
The results of the ablation study, presented in Table \ref{tab:ablation_study}, show a trend of performance improvement as we scale up the model's architecture.
%
Larger versions of both ESM and GPT outperformed their smaller counterparts in most evaluation metrics.
%
The increase in model size led to more accurate and relevant descriptions, indicating the benefit of leveraging larger language models in the Prot2Text framework.
%
However, the computational cost of larger models is higher, so we choose as our base model the one with the $283$M parameters, instead of the $760$M, to strike a balance between performance and computational efficiency.

\paragraph{Visualization of Generated Descriptions.}
To gain deeper insights into the quality of the generated proteins' functions by our \textit{Prot2Text} framework, we provide in Figure~\ref{fig:generated_examples} a textual comparison of the pre-defined labels and generated text outputs for a selected set of proteins from the test set.
%
It illustrates a comparison between the ground truth and the corresponding descriptions generated by \textit{Prot2Text} for three different proteins (\textit{P36108}, \textit{Q8NG08} and \textit{P35713}) along with each protein's name, amino acid sequence and 3D structural representation.
%
The results indicate a successful detailed reconstruction of the different proteins' functions including richer information than the known description.
%
Following, Fig.~\ref{fig:generated_examples} showcases the model's ability to generate coherent and informative free-text descriptions that align closely with the ground truth annotations.
%


\section{Conclusion}
In conclusion, our paper introduces Prot2Text, a pioneering multimodal framework, for the accurate prediction of a protein's function in free text format, from graph and sequential input.
%
By reformulating the task as free-text prediction, we address the limitations of traditional classification-based methods, allowing for a more nuanced and in-depth understanding of a protein's functionality.
%
Leveraging the power of GNNs and LLMs, we integrate structural and textual protein information, resulting in highly detailed and coherent generated protein descriptions.
%
The release of a comprehensive multimodal protein dataset further empowers the scientific community to benchmark and advance the field of protein function prediction in free text format.
%
This innovative approach opens new horizons for research and applications in drug discovery, protein engineering, and various biological sciences, with the potential to revolutionize our understanding of proteins' functions.

% \section{Limitation and Future Work}
% One limitation of our proposed Prot2Text model is that the RGCN encoder is not pretrained. Unlike the ESM encoder, which benefits from pretraining on a large corpus, the RGCN encoder lacks this initial knowledge. While using two encoders (ESM and RGCN) in our model aims to integrate diverse information sources, we must acknowledge that the ESM encoder benefits from pretrained weights, whereas the RGCN encoder does not. This imbalance in information utilization might hinder the model's ability to effectively leverage the full potential of both encoders during training. As a result, the RGCN encoder might struggle to capture complex patterns and may not fully leverage the underlying protein data, potentially leading to suboptimal performance.
% %
% To overcome this issue, we plan to introduce \todo{future work, pretrain RGCN? Michail?}

\bibliographystyle{apalike}
\bibliography{arxiv}

\end{document}
