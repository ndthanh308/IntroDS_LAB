\begin{thebibliography}{}

\bibitem[Bairoch and Apweiler, 1996]{swissprot}
Bairoch, A. and Apweiler, R. (1996).
\newblock {The SWISS-PROT Protein Sequence Data Bank and Its New Supplement
  TREMBL}.
\newblock {\em Nucleic Acids Research}, 24(1):21--25.

\bibitem[Bileschi et~al., 2019]{bileschi2019using}
Bileschi, M.~L., Belanger, D., Bryant, D., Sanderson, T., Carter, B., Sculley,
  D., DePristo, M.~A., and Colwell, L.~J. (2019).
\newblock Using deep learning to annotate the protein universe.
\newblock {\em BioRxiv}, page 626507.

\bibitem[Brandes et~al., 2022]{brandes2022proteinbert}
Brandes, N., Ofer, D., Peleg, Y., Rappoport, N., and Linial, M. (2022).
\newblock Proteinbert: a universal deep-learning model of protein sequence and
  function.
\newblock {\em Bioinformatics}, 38(8):2102--2110.

\bibitem[Brown et~al., 2020]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al. (2020).
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901.

\bibitem[Chatzianastasis et~al., 2023a]{chatzianastasis2023graph}
Chatzianastasis, M., Lutzeyer, J., Dasoulas, G., and Vazirgiannis, M. (2023a).
\newblock Graph ordering attention networks.
\newblock In {\em Proceedings of the 37th AAAI Conference on Artificial
  Intelligence}, pages 7006--7014.

\bibitem[Chatzianastasis et~al., 2023b]{chatzianastasis2023explainable}
Chatzianastasis, M., Vazirgiannis, M., and Zhang, Z. (2023b).
\newblock Explainable multilayer graph neural network for cancer gene
  prediction.
\newblock {\em arXiv preprint arXiv:2301.08831}.

\bibitem[Chen et~al., 2023]{chen20233d}
Chen, C., Chen, X., Morehead, A., Wu, T., and Cheng, J. (2023).
\newblock 3d-equivariant graph neural networks for protein model quality
  assessment.
\newblock {\em Bioinformatics}, 39(1):btad030.

\bibitem[Chithrananda et~al., 2020]{chithrananda2020chemberta}
Chithrananda, S., Grand, G., and Ramsundar, B. (2020).
\newblock Chemberta: large-scale self-supervised pretraining for molecular
  property prediction.
\newblock {\em arXiv preprint arXiv:2010.09885}.

\bibitem[Consortium, 2016]{uniprot}
Consortium, T.~U. (2016).
\newblock {UniProt: the universal protein knowledgebase}.
\newblock {\em Nucleic Acids Research}, 45(D1):D158--D169.

\bibitem[Consortium, 2015]{uniprot2015uniprot}
Consortium, U. (2015).
\newblock Uniprot: a hub for protein information.
\newblock {\em Nucleic acids research}, 43(D1):D204--D212.

\bibitem[Devlin et~al., 2019]{devlin-etal-2019-bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019).
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186,
  Minneapolis, Minnesota. Association for Computational Linguistics.

\bibitem[Dosovitskiy et~al., 2021]{vit}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,
  Uszkoreit, J., and Houlsby, N. (2021).
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Edwards et~al., 2022]{edwards2022translation-MolT5}
Edwards, C., Lai, T., Ros, K., Honke, G., Cho, K., and Ji, H. (2022).
\newblock Translation between molecules and natural language.
\newblock In {\em Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pages 292--305.

\bibitem[Fabian et~al., 2020]{fabian2020molecular-molbert}
Fabian, B., Edlich, T., Gaspar, H., Segler, M., Meyers, J., Fiscato, M., and
  Ahmed, M. (2020).
\newblock Molecular representation learning with language models and
  domain-relevant auxiliary tasks.

\bibitem[Gligorijevi{\'c} et~al., 2021]{gligorijevic2021structure}
Gligorijevi{\'c}, V., Renfrew, P.~D., Kosciolek, T., Leman, J.~K., Berenberg,
  D., Vatanen, T., Chandler, C., Taylor, B.~C., Fisk, I.~M., Vlamakis, H.,
  et~al. (2021).
\newblock Structure-based protein function prediction using graph convolutional
  networks.
\newblock {\em Nature communications}, 12(1):3168.

\bibitem[Ha et~al., 2021]{HA2021394}
Ha, J., Park, H., Park, J., and Park, S.~B. (2021).
\newblock Recent advances in identifying protein targets in drug discovery.
\newblock {\em Cell Chemical Biology}, 28(3):394--423.

\bibitem[Kipf and Welling, 2017]{kipf2017semi}
Kipf, T.~N. and Welling, M. (2017).
\newblock {Semi-Supervised Classification with Graph Convolutional Networks}.
\newblock In {\em 5th International Conference on Learning Representations}.

\bibitem[Kulmanov and Hoehndorf, 2019]{10.1093/bioinformatics/btz595}
Kulmanov, M. and Hoehndorf, R. (2019).
\newblock {DeepGOPlus: improved protein function prediction from sequence}.
\newblock {\em Bioinformatics}, 36(2):422--429.

\bibitem[Lee et~al., 2020]{lee2020biobert}
Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C.~H., and Kang, J. (2020).
\newblock Biobert: a pre-trained biomedical language representation model for
  biomedical text mining.
\newblock {\em Bioinformatics}, 36(4):1234--1240.

\bibitem[Lewis et~al., 2020]{lewis-etal-2020-bart}
Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O.,
  Stoyanov, V., and Zettlemoyer, L. (2020).
\newblock {BART}: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock In {\em Proceedings of the 58th Annual Meeting of the Association for
  Computational Linguistics}, pages 7871--7880, Online. Association for
  Computational Linguistics.

\bibitem[Li and Godzik, 2006]{cdhit}
Li, W. and Godzik, A. (2006).
\newblock {Cd-hit: a fast program for clustering and comparing large sets of
  protein or nucleotide sequences}.
\newblock {\em Bioinformatics}, 22(13):1658--1659.

\bibitem[Lin, 2004]{lin2004rouge}
Lin, C.-Y. (2004).
\newblock Rouge: A package for automatic evaluation of summaries.
\newblock In {\em Text summarization branches out}, pages 74--81.

\bibitem[Lin et~al., 2023a]{lin2023evolutionary-esm2}
Lin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., Smetanin, N., Verkuil,
  R., Kabeli, O., Shmueli, Y., dos Santos~Costa, A., Fazel-Zarandi, M., Sercu,
  T., Candido, S., and Rives, A. (2023a).
\newblock Evolutionary-scale prediction of atomic-level protein structure with
  a language model.
\newblock {\em Science}, 379(6637):1123--1130.
\newblock Earlier versions as preprint: bioRxiv 2022.07.20.500902.

\bibitem[Lin et~al., 2023b]{lin2023evolutionary}
Lin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., Smetanin, N., Verkuil,
  R., Kabeli, O., Shmueli, Y., et~al. (2023b).
\newblock Evolutionary-scale prediction of atomic-level protein structure with
  a language model.
\newblock {\em Science}, 379(6637):1123--1130.

\bibitem[Liu, 2017]{liu2017deep}
Liu, X. (2017).
\newblock Deep recurrent neural network for protein function prediction from
  sequence.
\newblock {\em arXiv preprint arXiv:1701.08318}.

\bibitem[Loshchilov and Hutter, 2019]{loshchilov2018decoupled}
Loshchilov, I. and Hutter, F. (2019).
\newblock Decoupled weight decay regularization.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Morris et~al., 2020]{morris2020weisfeiler}
Morris, C., Rattan, G., and Mutzel, P. (2020).
\newblock {Weisfeiler and Leman go sparse: Towards scalable higher-order graph
  embeddings}.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~34.

\bibitem[Murphy et~al., 2019]{murphy2019relational}
Murphy, R., Srinivasan, B., Rao, V., and Ribeiro, B. (2019).
\newblock {Relational Pooling for Graph Representations}.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning}, pages 4663--4673.

\bibitem[Nikolentzos et~al., 2020]{nikolentzos2020k}
Nikolentzos, G., Dasoulas, G., and Vazirgiannis, M. (2020).
\newblock {k-hop graph neural networks}.
\newblock {\em Neural Networks}, 130:195--205.

\bibitem[Papineni et~al., 2002]{papineni2002bleu}
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002).
\newblock Bleu: a method for automatic evaluation of machine translation.
\newblock In {\em Proceedings of the 40th annual meeting of the Association for
  Computational Linguistics}, pages 311--318.

\bibitem[Radford et~al., 2021]{clip}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
  G., Askell, A., Mishkin, P., Clark, J., and Krueger, G. (2021).
\newblock Clip: Learning transferable visual models from natural language
  supervision.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Radford et~al., 2018]{radford2018improving-gpt}
Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. (2018).
\newblock Improving language understanding by generative pre-training.

\bibitem[Radford et~al., 2019]{radford2019language-gpt2}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019).
\newblock Language models are unsupervised multitask learners.

\bibitem[Raffel et~al., 2019]{raffel2019exploring-T5}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J. (2019).
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em arXiv preprint arXiv:1910.10683}.

\bibitem[Reiser et~al., 2022]{reiser2022graph}
Reiser, P., Neubert, M., Eberhard, A., Torresi, L., Zhou, C., Shao, C., Metni,
  H., van Hoesel, C., Schopmans, H., Sommer, T., et~al. (2022).
\newblock Graph neural networks for materials science and chemistry.
\newblock {\em Communications Materials}, 3(1):93.

\bibitem[Rives et~al., 2021]{rives2021biological-esm1}
Rives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J., Guo, D., Ott, M.,
  Zitnick, C.~L., Ma, J., et~al. (2021).
\newblock Biological structure and function emerge from scaling unsupervised
  learning to 250 million protein sequences.
\newblock {\em Proceedings of the National Academy of Sciences},
  118(15):e2016239118.
\newblock bioRxiv 10.1101/622803.

\bibitem[Scarselli et~al., 2009]{scarselli2009graph}
Scarselli, F., Gori, M., Tsoi, A.~C., Hagenbuchner, M., and Monfardini, G.
  (2009).
\newblock {The Graph Neural Network Model}.
\newblock {\em IEEE Transactions on Neural Networks}, 20(1):61--80.

\bibitem[Schlichtkrull et~al., 2018]{schlichtkrull2018modeling}
Schlichtkrull, M., Kipf, T.~N., Bloem, P., Van Den~Berg, R., Titov, I., and
  Welling, M. (2018).
\newblock Modeling relational data with graph convolutional networks.
\newblock In {\em The Semantic Web: 15th International Conference, ESWC 2018,
  Heraklion, Crete, Greece, June 3--7, 2018, Proceedings 15}, pages 593--607.
  Springer.

\bibitem[Seo et~al., 2019]{seo2019discriminative}
Seo, Y., Loukas, A., and Perraudin, N. (2019).
\newblock Discriminative structural graph classification.
\newblock {\em arXiv preprint arXiv:1905.13422}.

\bibitem[Varadi et~al., 2022]{varadi2022alphafold}
Varadi, M., Anyango, S., Deshpande, M., Nair, S., Natassia, C., Yordanova, G.,
  Yuan, D., Stroe, O., Wood, G., Laydon, A., et~al. (2022).
\newblock Alphafold protein structure database: massively expanding the
  structural coverage of protein-sequence space with high-accuracy models.
\newblock {\em Nucleic acids research}, 50(D1):D439--D444.

\bibitem[Vaswani et~al., 2017]{transformers}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L.~u., and Polosukhin, I. (2017).
\newblock Attention is all you need.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R., editors, {\em Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc.

\bibitem[Wang et~al., 2022]{wang2022learning}
Wang, L., Liu, H., Liu, Y., Kurtin, J., and Ji, S. (2022).
\newblock Learning protein representations via complete 3d graph networks.
\newblock {\em arXiv preprint arXiv:2207.12600}.

\bibitem[Wolf et~al., 2020]{wolf-etal-2020-transformers}
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P.,
  Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen,
  P., Ma, C., Jernite, Y., Plu, J., Xu, C., Le~Scao, T., Gugger, S., Drame, M.,
  Lhoest, Q., and Rush, A. (2020).
\newblock Transformers: State-of-the-art natural language processing.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pages 38--45, Online.
  Association for Computational Linguistics.

\bibitem[Zhang et~al., 2019]{zhang2019heterogeneous}
Zhang, C., Song, D., Huang, C., Swami, A., and Chawla, N.~V. (2019).
\newblock Heterogeneous graph neural network.
\newblock In {\em Proceedings of the 25th ACM SIGKDD international conference
  on knowledge discovery \& data mining}, pages 793--803.

\bibitem[Zhang et~al., 2020]{zhang2019bertscore}
Zhang, T., Kishore, V., Wu, F., Weinberger, K.~Q., and Artzi, Y. (2020).
\newblock Bertscore: Evaluating text generation with bert.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Zhang et~al., 2021]{zhang2021graph}
Zhang, X.-M., Liang, L., Liu, L., and Tang, M.-J. (2021).
\newblock Graph neural networks and their current applications in
  bioinformatics.
\newblock {\em Frontiers in genetics}, 12:690049.

\bibitem[Zhang et~al., 2022]{zhang2022protein}
Zhang, Z., Xu, M., Jamasb, A., Chenthamarakshan, V., Lozano, A., Das, P., and
  Tang, J. (2022).
\newblock Protein representation learning by geometric structure pretraining.
\newblock {\em arXiv preprint arXiv:2203.06125}.

\bibitem[Zitnik et~al., 2018]{zitnik2018modeling}
Zitnik, M., Agrawal, M., and Leskovec, J. (2018).
\newblock Modeling polypharmacy side effects with graph convolutional networks.
\newblock {\em Bioinformatics}, 34(13):i457--i466.

\end{thebibliography}
