

\section{Experiments}

We use the NuScenes \cite{nuscenes2019} and Argoverse \cite{DBLP:conf/cvpr/ChangLSSBHW0LRH19} datasets. The data processing and the train/test splits are the same as~\cite{Can_2021_ICCV}. 

\noindent\textbf{Implementation.}
The BEV area that our method outputs the lane graph is from -25 to 25m in x-direction and 1 to 50m in z direction with a 25cm resolution. We opt for using three Bezier control points. Our implementation is in Pytorch and runs with 7FPS. We set the latent space optimization loss trade-off parameter to $\alpha=0.02$. We run 600 iterations for the latent space optimization with a learning rate of 0.1. We use 32 helper vectors and 100 query vectors. The latent space of the WAE, and the hidden dimension of the transformer is 512. For the training of the WAE, $\beta=2$, $\gamma=1$ and $\theta=1$.


\noindent\textbf{Baselines.}
We compare against state-of-the-art~\textbf{transformer} and~\textbf{Polygon-RNN} based methods proposed in~\cite{Can_2021_ICCV} as well as another baseline which uses the method~\textbf{PINET}~\cite{DBLP:journals/corr/abs-2002-06604} to extract lane boundaries. We only train our method using the transformer based estimates. However, our method can be used in conjunction with any method.

% Figure environment removed

\noindent\textbf{Ablations.} We trained an autoencoder with the true lane graphs, refereed as \textbf{OursTrue}, instead of the proposed method where the autoencoder is trained with the estimated lane graphs (\textbf{Ours}), see Table~\ref{tab:nusc}. For both networks, we report the performance when no latent space optimization is carried out (\textbf{w/o Opt}) and when we apply latent space optimization. Moreover, we report the convergence behaviour of the latent space optimization for both the true lane graph trained network and ours (Fig.~\ref{fig:graphs}). 


