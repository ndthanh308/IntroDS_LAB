% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.2 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{nyt/global//global/global}
    \entry{ba_layer_2016}{misc}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=750bb29ebaf655e36626e464041edb6d}{%
           family={Ba},
           familyi={B\bibinitperiod},
           given={Jimmy\bibnamedelima Lei},
           giveni={J\bibinitperiod\bibinitdelim L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9ce51427604561924b91aa544c9c1b2a}{%
           family={Kiros},
           familyi={K\bibinitperiod},
           given={Jamie\bibnamedelima Ryan},
           giveni={J\bibinitperiod\bibinitdelim R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=813bd95fe553e6079cd53a567b238287}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey\bibnamedelima E.},
           giveni={G\bibinitperiod\bibinitdelim E\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{059348b2d0b3fe9a67209502c14057d1}
      \strng{fullhash}{059348b2d0b3fe9a67209502c14057d1}
      \strng{bibnamehash}{059348b2d0b3fe9a67209502c14057d1}
      \strng{authorbibnamehash}{059348b2d0b3fe9a67209502c14057d1}
      \strng{authornamehash}{059348b2d0b3fe9a67209502c14057d1}
      \strng{authorfullhash}{059348b2d0b3fe9a67209502c14057d1}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.}
      \field{month}{7}
      \field{note}{arXiv:1607.06450 [cs, stat]}
      \field{title}{Layer {Normalization}}
      \field{urlday}{5}
      \field{urlmonth}{4}
      \field{urlyear}{2023}
      \field{year}{2016}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://arxiv.org/abs/1607.06450
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1607.06450
      \endverb
      \keyw{Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{bradbury_jax_2018}{misc}{}
      \name{author}{11}{}{%
        {{un=0,uniquepart=base,hash=b75383e6b48c8360c7a60031424c85cf}{%
           family={Bradbury},
           familyi={B\bibinitperiod},
           given={James},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=29563c986154ca2b45d286b4dd5ef92a}{%
           family={Frostig},
           familyi={F\bibinitperiod},
           given={Roy},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7ab2d7e7f2bfceec36e42ad1962fde11}{%
           family={Hawkins},
           familyi={H\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e4d515e534ba184f553d5a2a926e0ea7}{%
           family={Johnson},
           familyi={J\bibinitperiod},
           given={Matthew\bibnamedelima James},
           giveni={M\bibinitperiod\bibinitdelim J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0101ae5b219809cf901e7645ca40b9ae}{%
           family={Leary},
           familyi={L\bibinitperiod},
           given={Chris},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=07e607fb522f06610feb023ecfa712e2}{%
           family={Maclaurin},
           familyi={M\bibinitperiod},
           given={Dougal},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=bca7d753be3217d63404d860acb6e5f5}{%
           family={Necula},
           familyi={N\bibinitperiod},
           given={George},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=56bf0b340039cf8594436a624ff548a9}{%
           family={Paszke},
           familyi={P\bibinitperiod},
           given={Adam},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0fd9a0e34f1b2adda41357c948d14986}{%
           family={VanderPlas},
           familyi={V\bibinitperiod},
           given={Jake},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=02f809ed2547f721c304772c06378c2d}{%
           family={Wanderman-Milne},
           familyi={W\bibinithyphendelim M\bibinitperiod},
           given={Skye},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=fe167f78433aabd363ac3c06710a1945}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Qiao},
           giveni={Q\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{be9c2f36b976223518e841214253df92}
      \strng{fullhash}{12f83c04441cfb40a3c9b086929c12bf}
      \strng{bibnamehash}{be9c2f36b976223518e841214253df92}
      \strng{authorbibnamehash}{be9c2f36b976223518e841214253df92}
      \strng{authornamehash}{be9c2f36b976223518e841214253df92}
      \strng{authorfullhash}{12f83c04441cfb40a3c9b086929c12bf}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{JAX}: composable transformations of {Python}+{NumPy} programs}
      \field{year}{2018}
      \verb{urlraw}
      \verb http://github.com/google/jax
      \endverb
      \verb{url}
      \verb http://github.com/google/jax
      \endverb
    \endentry
    \entry{brandstetter_message_2022}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=4954f897f0e2228c69f8903b2e1e25d1}{%
           family={Brandstetter},
           familyi={B\bibinitperiod},
           given={Johannes},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3992d9b130c5891a53b5f0bb2a9b1992}{%
           family={Worrall},
           familyi={W\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=53d2880ad8047b61cdae2c6b2803e763}{%
           family={Welling},
           familyi={W\bibinitperiod},
           given={Max},
           giveni={M\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{3fb29a184b650596283fb5c7fe1c7ec7}
      \strng{fullhash}{3fb29a184b650596283fb5c7fe1c7ec7}
      \strng{bibnamehash}{3fb29a184b650596283fb5c7fe1c7ec7}
      \strng{authorbibnamehash}{3fb29a184b650596283fb5c7fe1c7ec7}
      \strng{authornamehash}{3fb29a184b650596283fb5c7fe1c7ec7}
      \strng{authorfullhash}{3fb29a184b650596283fb5c7fe1c7ec7}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The numerical solution of partial differential equations (PDEs) is difficult, having led to a century of research so far. Recently, there have been pushes to build neural--numerical hybrid solvers, which piggy-backs the modern trend towards fully end-to-end learned systems. Most works so far can only generalize over a subset of properties to which a generic solver would be faced, including: resolution, topology, geometry, boundary conditions, domain discretization regularity, dimensionality, etc. In this work, we build a solver, satisfying these properties, where all the components are based on neural message passing, replacing all heuristically designed components in the computation graph with backprop-optimized neural function approximators. We show that neural message passing solvers representationally contain some classical methods, such as finite differences, finite volumes, and WENO schemes. In order to encourage stability in training autoregressive models, we put forward a method that is based on the principle of zero-stability, posing stability as a domain adaptation problem. We validate our method on various fluid-like flow problems, demonstrating fast, stable, and accurate performance across different domain topologies, discretization, etc. in 1D and 2D. Our model outperforms state-of-the-art numerical solvers in the low resolution regime in terms of speed and accuracy.}
      \field{journaltitle}{arXiv:2202.03376 [cs, math]}
      \field{month}{2}
      \field{note}{arXiv: 2202.03376}
      \field{title}{Message {Passing} {Neural} {PDE} {Solvers}}
      \field{urlday}{9}
      \field{urlmonth}{2}
      \field{urlyear}{2022}
      \field{year}{2022}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://arxiv.org/abs/2202.03376
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2202.03376
      \endverb
      \keyw{Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Mathematics - Numerical Analysis}
    \endentry
    \entry{brunton_machine_2020}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=572af75cb67759c9684ffbb4d11cddf8}{%
           family={Brunton},
           familyi={B\bibinitperiod},
           given={Steven\bibnamedelima L.},
           giveni={S\bibinitperiod\bibinitdelim L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6011c3915ad8a4919693d991e4c15b4f}{%
           family={Noack},
           familyi={N\bibinitperiod},
           given={Bernd\bibnamedelima R.},
           giveni={B\bibinitperiod\bibinitdelim R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d0a18518b57cef85c32665e92ec3dea1}{%
           family={Koumoutsakos},
           familyi={K\bibinitperiod},
           given={Petros},
           giveni={P\bibinitperiod},
           givenun=0}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{badd0a36684ee812601268e86af05eb6}
      \strng{fullhash}{badd0a36684ee812601268e86af05eb6}
      \strng{bibnamehash}{badd0a36684ee812601268e86af05eb6}
      \strng{authorbibnamehash}{badd0a36684ee812601268e86af05eb6}
      \strng{authornamehash}{badd0a36684ee812601268e86af05eb6}
      \strng{authorfullhash}{badd0a36684ee812601268e86af05eb6}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The field of fluid mechanics is rapidly advancing, driven by unprecedented volumes of data from experiments, field measurements, and large-scale simulations at multiple spatiotemporal scales. Machine learning (ML) offers a wealth of techniques to extract information from data that can be translated into knowledge about the underlying fluid mechanics. Moreover, ML algorithms can augment domain knowledge and automate tasks related to flow control and optimization. This article presents an overview of past history, current developments, and emerging opportunities of ML for fluid mechanics. We outline fundamental ML methodologies and discuss their uses for understanding, modeling, optimizing, and controlling fluid flows. The strengths and limitations of these methods are addressed from the perspective of scientific inquiry that considers data as an inherent part of modeling, experiments, and simulations. ML provides a powerful informationprocessing framework that can augment, and possibly even transform, current lines of fluid mechanics research and industrial applications.}
      \field{issn}{0066-4189, 1545-4479}
      \field{journaltitle}{Annual Review of Fluid Mechanics}
      \field{month}{1}
      \field{number}{1}
      \field{title}{Machine {Learning} for {Fluid} {Mechanics}}
      \field{urlday}{28}
      \field{urlmonth}{10}
      \field{urlyear}{2020}
      \field{volume}{52}
      \field{year}{2020}
      \field{urldateera}{ce}
      \field{pages}{477\bibrangedash 508}
      \range{pages}{32}
      \verb{doi}
      \verb 10.1146/annurev-fluid-010719-060214
      \endverb
      \verb{urlraw}
      \verb https://www.annualreviews.org/doi/10.1146/annurev-fluid-010719-060214
      \endverb
      \verb{url}
      \verb https://www.annualreviews.org/doi/10.1146/annurev-fluid-010719-060214
      \endverb
    \endentry
    \entry{chandler_invariant_2013}{article}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=a7242ba6163db226bc3b43cc734814a3}{%
           family={Chandler},
           familyi={C\bibinitperiod},
           given={Gary\bibnamedelima J.},
           giveni={G\bibinitperiod\bibinitdelim J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5af863c57808ba4e0ad7cd606dc5b270}{%
           family={Kerswell},
           familyi={K\bibinitperiod},
           given={Rich\bibnamedelima R.},
           giveni={R\bibinitperiod\bibinitdelim R\bibinitperiod},
           givenun=0}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{dd533f0214432bba782edb117a7cd629}
      \strng{fullhash}{dd533f0214432bba782edb117a7cd629}
      \strng{bibnamehash}{dd533f0214432bba782edb117a7cd629}
      \strng{authorbibnamehash}{dd533f0214432bba782edb117a7cd629}
      \strng{authornamehash}{dd533f0214432bba782edb117a7cd629}
      \strng{authorfullhash}{dd533f0214432bba782edb117a7cd629}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We consider long-time simulations of two-dimensional turbulence body forced by sin4yx{\textasciicircum}sin⁡4yx{\textasciicircum}{\textbackslash}sin 4y{\textbackslash}hat \{{\textbackslash}boldsymbol\{x\}\} on the torus (x,y)∈[0,2π]2(x,y)∈[0,2π]2(x, y){\textbackslash}in {\textbackslash}mathop\{[0, 2{\textbackslash}mathrm\{{\textbackslash}pi\} ] \}{\textbackslash}nolimits {\textasciicircum}\{2\} with the purpose of extracting simple invariant sets or ‘exact recurrent flows’ embedded in this turbulence. Each recurrent flow represents a sustained closed cycle of dynamical processes which underpins the turbulence. These are used to reconstruct the turbulence statistics using periodic orbit theory. The approach is found to be reasonably successful at a low value of the forcing where the flow is close to but not fully in its asymptotic (strongly) turbulent regime. Here, a total of 50 recurrent flows are found with the majority buried in the part of phase space most populated by the turbulence giving rise to a good reproduction of the energy and dissipation p.d.f. However, at higher forcing amplitudes now in the asymptotic turbulent regime, the generated turbulence data set proves insufficiently long to yield enough recurrent flows to make viable predictions. Despite this, the general approach seems promising providing enough simulation data is available since it is open to extensive automation and naturally generates dynamically important exact solutions for the flow.}
      \field{issn}{0022-1120, 1469-7645}
      \field{journaltitle}{Journal of Fluid Mechanics}
      \field{month}{5}
      \field{note}{Publisher: Cambridge University Press}
      \field{title}{Invariant recurrent solutions embedded in a turbulent two-dimensional {Kolmogorov} flow}
      \field{urlday}{10}
      \field{urlmonth}{9}
      \field{urlyear}{2021}
      \field{volume}{722}
      \field{year}{2013}
      \field{urldateera}{ce}
      \field{pages}{554\bibrangedash 595}
      \range{pages}{42}
      \verb{doi}
      \verb 10.1017/jfm.2013.122
      \endverb
      \verb{urlraw}
      \verb https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/invariant-recurrent-solutions-embedded-in-a-turbulent-twodimensional-kolmogorov-flow/78CC6B29A670F84CBC79D29408DC2674
      \endverb
      \verb{url}
      \verb https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/invariant-recurrent-solutions-embedded-in-a-turbulent-twodimensional-kolmogorov-flow/78CC6B29A670F84CBC79D29408DC2674
      \endverb
      \keyw{chaos,nonlinear dynamical systems,turbulence theory}
    \endentry
    \entry{czarnecki_sobolev_2017}{report}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=2e540193e28e5b3aa63fef2e515c637b}{%
           family={Czarnecki},
           familyi={C\bibinitperiod},
           given={Wojciech\bibnamedelima Marian},
           giveni={W\bibinitperiod\bibinitdelim M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=643f92e8f89f2746a4c1aa077d225755}{%
           family={Osindero},
           familyi={O\bibinitperiod},
           given={Simon},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7dc1446dea7ff50b2b02fb83780cc9c6}{%
           family={Jaderberg},
           familyi={J\bibinitperiod},
           given={Max},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=16a4ef3acfbb2ee8e5ebfdf4fbf37e3d}{%
           family={Świrszcz},
           familyi={Ś\bibinitperiod},
           given={Grzegorz},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7045b009b04d57bd2e19b5dfa0864d4f}{%
           family={Pascanu},
           familyi={P\bibinitperiod},
           given={Razvan},
           giveni={R\bibinitperiod},
           givenun=0}}%
      }
      \list{institution}{1}{%
        {arXiv}%
      }
      \strng{namehash}{05c32f811c41aaf099d50dcc02afc1f9}
      \strng{fullhash}{059f41b8560aefcb7aab2c3e7ae16cf2}
      \strng{bibnamehash}{05c32f811c41aaf099d50dcc02afc1f9}
      \strng{authorbibnamehash}{05c32f811c41aaf099d50dcc02afc1f9}
      \strng{authornamehash}{05c32f811c41aaf099d50dcc02afc1f9}
      \strng{authorfullhash}{059f41b8560aefcb7aab2c3e7ae16cf2}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{At the heart of deep learning we aim to use neural networks as function approximators - training them to produce outputs from inputs in emulation of a ground truth function or data creation process. In many cases we only have access to input-output pairs from the ground truth, however it is becoming more common to have access to derivatives of the target output with respect to the input - for example when the ground truth function is itself a neural network such as in network compression or distillation. Generally these target derivatives are not computed, or are ignored. This paper introduces Sobolev Training for neural networks, which is a method for incorporating these target derivatives in addition the to target values while training. By optimising neural networks to not only approximate the function's outputs but also the function's derivatives we encode additional information about the target function within the parameters of the neural network. Thereby we can improve the quality of our predictors, as well as the data-efficiency and generalization capabilities of our learned function approximation. We provide theoretical justifications for such an approach as well as examples of empirical evidence on three distinct domains: regression on classical optimisation datasets, distilling policies of an agent playing Atari, and on large-scale applications of synthetic gradients. In all three domains the use of Sobolev Training, employing target derivatives in addition to target values, results in models with higher accuracy and stronger generalisation.}
      \field{month}{7}
      \field{note}{arXiv:1706.04859 [cs] type: article}
      \field{number}{arXiv:1706.04859}
      \field{title}{Sobolev {Training} for {Neural} {Networks}}
      \field{type}{techreport}
      \field{urlday}{24}
      \field{urlmonth}{5}
      \field{urlyear}{2022}
      \field{year}{2017}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1706.04859
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1706.04859
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1706.04859
      \endverb
      \keyw{Computer Science - Machine Learning}
    \endentry
    \entry{dresdner_learning_2022}{misc}{}
      \name{author}{7}{}{%
        {{un=0,uniquepart=base,hash=53fd45bb5f3e069219ef00fe211b9c35}{%
           family={Dresdner},
           familyi={D\bibinitperiod},
           given={Gideon},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e7dda8c99b1c7acd0431fbb13074ed4f}{%
           family={Kochkov},
           familyi={K\bibinitperiod},
           given={Dmitrii},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ffd21a6d214c71fe0fc0dc4836182088}{%
           family={Norgaard},
           familyi={N\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7680a6e363ae439655e38cbdecdae24f}{%
           family={Zepeda-Núñez},
           familyi={Z\bibinithyphendelim N\bibinitperiod},
           given={Leonardo},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c6c990e4c2936de2d6693b152156de1a}{%
           family={Smith},
           familyi={S\bibinitperiod},
           given={Jamie\bibnamedelima A.},
           giveni={J\bibinitperiod\bibinitdelim A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7b08b1e717ce032c2c0945b3183ef038}{%
           family={Brenner},
           familyi={B\bibinitperiod},
           given={Michael\bibnamedelima P.},
           giveni={M\bibinitperiod\bibinitdelim P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=4a22828b76006f42d747b699d2ef5167}{%
           family={Hoyer},
           familyi={H\bibinitperiod},
           given={Stephan},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{7583b56e12c0418104de667fcf89f65e}
      \strng{fullhash}{53ebfd2aa1316fb43f854256b0801908}
      \strng{bibnamehash}{7583b56e12c0418104de667fcf89f65e}
      \strng{authorbibnamehash}{7583b56e12c0418104de667fcf89f65e}
      \strng{authornamehash}{7583b56e12c0418104de667fcf89f65e}
      \strng{authorfullhash}{53ebfd2aa1316fb43f854256b0801908}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Despite their ubiquity throughout science and engineering, only a handful of partial differential equations (PDEs) have analytical, or closed-form solutions. This motivates a vast amount of classical work on numerical simulation of PDEs and more recently, a whirlwind of research into data-driven techniques leveraging machine learning (ML). A recent line of work indicates that a hybrid of classical numerical techniques with machine learning can offer significant improvements over either approach alone. In this work, we show that the choice of the numerical scheme is crucial when incorporating physics-based priors. We build upon Fourier-based spectral methods, which are considerably more efficient than other numerical schemes for simulating PDEs with smooth and periodic solutions. Specifically, we develop ML-augmented spectral solvers for three model PDEs of fluid dynamics, which improve upon the accuracy of standard spectral solvers at the same resolution. We also demonstrate a handful of key design principles for combining machine learning and numerical methods for solving PDEs.}
      \field{month}{7}
      \field{note}{arXiv:2207.00556 [physics]}
      \field{title}{Learning to correct spectral methods for simulating turbulent flows}
      \field{urlday}{30}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{year}{2022}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2207.00556
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2207.00556
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2207.00556
      \endverb
      \keyw{Computer Science - Machine Learning,Physics - Fluid Dynamics}
    \endentry
    \entry{duraisamy_turbulence_2019}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=652768fc415935cb81439f4c2d4ec6b6}{%
           family={Duraisamy},
           familyi={D\bibinitperiod},
           given={Karthik},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f4fc3ea06f405e9a60f3950ebbdc575e}{%
           family={Iaccarino},
           familyi={I\bibinitperiod},
           given={Gianluca},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e50d2f7cfc7f2aea3adfbf6ed8952990}{%
           family={Xiao},
           familyi={X\bibinitperiod},
           given={Heng},
           giveni={H\bibinitperiod},
           givenun=0}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{47449c5993a31cff981bb6edfeb801c6}
      \strng{fullhash}{47449c5993a31cff981bb6edfeb801c6}
      \strng{bibnamehash}{47449c5993a31cff981bb6edfeb801c6}
      \strng{authorbibnamehash}{47449c5993a31cff981bb6edfeb801c6}
      \strng{authornamehash}{47449c5993a31cff981bb6edfeb801c6}
      \strng{authorfullhash}{47449c5993a31cff981bb6edfeb801c6}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Data from experiments and direct simulations of turbulence have historically been used to calibrate simple engineering models such as those based on the Reynolds-averaged Navier–Stokes (RANS) equations. In the past few years, with the availability of large and diverse datasets, researchers have begun to explore methods to systematically inform turbulence models with data, with the goal of quantifying and reducing model uncertainties. This review surveys recent developments in bounding uncertainties in RANS models via physical constraints, in adopting statistical inference to characterize model coeﬃcients and estimate discrepancy, and in using machine learning to improve turbulence models. Key principles, achievements and challenges are discussed. A central perspective advocated in this review is that by exploiting foundational knowledge in turbulence modeling and physical constraints, data-driven approaches can yield useful predictive models.}
      \field{issn}{0066-4189, 1545-4479}
      \field{journaltitle}{Annual Review of Fluid Mechanics}
      \field{month}{1}
      \field{note}{arXiv: 1804.00183}
      \field{number}{1}
      \field{title}{Turbulence {Modeling} in the {Age} of {Data}}
      \field{urlday}{19}
      \field{urlmonth}{11}
      \field{urlyear}{2020}
      \field{volume}{51}
      \field{year}{2019}
      \field{urldateera}{ce}
      \field{pages}{357\bibrangedash 377}
      \range{pages}{21}
      \verb{doi}
      \verb 10.1146/annurev-fluid-010518-040547
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1804.00183
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1804.00183
      \endverb
      \keyw{Physics - Computational Physics,Physics - Fluid Dynamics}
    \endentry
    \entry{gelbrecht_neural_2021}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=43070af1c2bc7c406e21dc52bc92ceae}{%
           family={Gelbrecht},
           familyi={G\bibinitperiod},
           given={Maximilian},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1f20151d06e4b3734bf8cb6038c0495f}{%
           family={Boers},
           familyi={B\bibinitperiod},
           given={Niklas},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=386a83bdf018c3a1b34a18d28dd4a054}{%
           family={Kurths},
           familyi={K\bibinitperiod},
           given={Jürgen},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{abedea3f65214c91caec01adce6cf9c7}
      \strng{fullhash}{abedea3f65214c91caec01adce6cf9c7}
      \strng{bibnamehash}{abedea3f65214c91caec01adce6cf9c7}
      \strng{authorbibnamehash}{abedea3f65214c91caec01adce6cf9c7}
      \strng{authornamehash}{abedea3f65214c91caec01adce6cf9c7}
      \strng{authorfullhash}{abedea3f65214c91caec01adce6cf9c7}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{When predicting complex systems one typically relies on differential equation which can often be incomplete, missing unknown inﬂuences or higher order effects. By augmenting the equations with artiﬁcial neural networks we can compensate these deﬁciencies. We show that this can be used to predict paradigmatic, high-dimensional chaotic partial differential equations even when only short and incomplete datasets are available. The forecast horizon for these high dimensional systems is about an order of magnitude larger than the length of the training data.}
      \field{issn}{1367-2630}
      \field{journaltitle}{New Journal of Physics}
      \field{month}{4}
      \field{number}{4}
      \field{title}{Neural partial differential equations for chaotic systems}
      \field{urlday}{17}
      \field{urlmonth}{1}
      \field{urlyear}{2022}
      \field{volume}{23}
      \field{year}{2021}
      \field{urldateera}{ce}
      \field{pages}{043005}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1088/1367-2630/abeb90
      \endverb
      \verb{urlraw}
      \verb https://iopscience.iop.org/article/10.1088/1367-2630/abeb90
      \endverb
      \verb{url}
      \verb https://iopscience.iop.org/article/10.1088/1367-2630/abeb90
      \endverb
    \endentry
    \entry{gilpin_chaos_2021}{article}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=36c336cc2e6f5c192226456775d1ba2b}{%
           family={Gilpin},
           familyi={G\bibinitperiod},
           given={William},
           giveni={W\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{36c336cc2e6f5c192226456775d1ba2b}
      \strng{fullhash}{36c336cc2e6f5c192226456775d1ba2b}
      \strng{bibnamehash}{36c336cc2e6f5c192226456775d1ba2b}
      \strng{authorbibnamehash}{36c336cc2e6f5c192226456775d1ba2b}
      \strng{authornamehash}{36c336cc2e6f5c192226456775d1ba2b}
      \strng{authorfullhash}{36c336cc2e6f5c192226456775d1ba2b}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The striking fractal geometry of strange attractors underscores the generative nature of chaos: like probability distributions, chaotic systems can be repeatedly measured to produce arbitrarily-detailed information about the underlying attractor. Chaotic systems thus pose a unique challenge to modern statistical learning techniques, while retaining quantifiable mathematical properties that make them controllable and interpretable as benchmarks. Here, we present a growing database currently comprising 131 known chaotic dynamical systems spanning fields such as astrophysics, climatology, and biochemistry. Each system is paired with precomputed multivariate and univariate time series. Our dataset has comparable scale to existing static time series databases; however, our systems can be re-integrated to produce additional datasets of arbitrary length and granularity. Our dataset is annotated with known mathematical properties of each system, and we perform feature analysis to broadly categorize the diverse dynamics present across the collection. Chaotic systems inherently challenge forecasting models, and across extensive benchmarks we correlate forecasting performance with the degree of chaos present. We also exploit the unique generative properties of our dataset in several proof-of-concept experiments: surrogate transfer learning to improve time series classification, importance sampling to accelerate model training, and benchmarking symbolic regression algorithms.}
      \field{journaltitle}{arXiv:2110.05266 [nlin]}
      \field{month}{10}
      \field{note}{arXiv: 2110.05266}
      \field{title}{Chaos as an interpretable benchmark for forecasting and data-driven modelling}
      \field{urlday}{13}
      \field{urlmonth}{10}
      \field{urlyear}{2021}
      \field{year}{2021}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://arxiv.org/abs/2110.05266
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2110.05266
      \endverb
      \keyw{Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Nonlinear Sciences - Chaotic Dynamics}
    \endentry
    \entry{goodfellow_deep_2016}{book}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=5d2585c11210cf1d4512e6e0a03ec315}{%
           family={Goodfellow},
           familyi={G\bibinitperiod},
           given={Ian},
           giveni={I\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ccec1ccd2e1aa86960eb2e872c6b7020}{%
           family={Courville},
           familyi={C\bibinitperiod},
           given={Aaron},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {MIT Press}%
      }
      \strng{namehash}{3ae53fe582e8a815b118d26947eaa326}
      \strng{fullhash}{3ae53fe582e8a815b118d26947eaa326}
      \strng{bibnamehash}{3ae53fe582e8a815b118d26947eaa326}
      \strng{authorbibnamehash}{3ae53fe582e8a815b118d26947eaa326}
      \strng{authornamehash}{3ae53fe582e8a815b118d26947eaa326}
      \strng{authorfullhash}{3ae53fe582e8a815b118d26947eaa326}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{Deep {Learning}}
      \field{year}{2016}
      \verb{urlraw}
      \verb http://www.deeplearningbook.org
      \endverb
      \verb{url}
      \verb http://www.deeplearningbook.org
      \endverb
    \endentry
    \entry{hendrycks_gaussian_2020}{misc}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=86d0b4ecd6b6066d49e7aecde6e5e630}{%
           family={Hendrycks},
           familyi={H\bibinitperiod},
           given={Dan},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6bc70be3492f0800ecee6926d290244f}{%
           family={Gimpel},
           familyi={G\bibinitperiod},
           given={Kevin},
           giveni={K\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{7d47b8f6df5cfe597280f14a9ffb6143}
      \strng{fullhash}{7d47b8f6df5cfe597280f14a9ffb6143}
      \strng{bibnamehash}{7d47b8f6df5cfe597280f14a9ffb6143}
      \strng{authorbibnamehash}{7d47b8f6df5cfe597280f14a9ffb6143}
      \strng{authornamehash}{7d47b8f6df5cfe597280f14a9ffb6143}
      \strng{authorfullhash}{7d47b8f6df5cfe597280f14a9ffb6143}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is \$x{\textbackslash}Phi(x)\$, where \${\textbackslash}Phi(x)\$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs (\$x{\textbackslash}mathbf\{1\}\_\{x{>}0\}\$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.}
      \field{month}{7}
      \field{note}{arXiv:1606.08415 [cs]}
      \field{title}{Gaussian {Error} {Linear} {Units} ({GELUs})}
      \field{urlday}{27}
      \field{urlmonth}{3}
      \field{urlyear}{2023}
      \field{year}{2020}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://arxiv.org/abs/1606.08415
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1606.08415
      \endverb
      \keyw{Computer Science - Machine Learning}
    \endentry
    \entry{krishnapriyan_characterizing_2021}{article}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=2c28a9dbe8b0ff3fbe8419f91c494380}{%
           family={Krishnapriyan},
           familyi={K\bibinitperiod},
           given={Aditi\bibnamedelima S.},
           giveni={A\bibinitperiod\bibinitdelim S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ff99b6be6f7ad0c9f311e5ddf91c01f7}{%
           family={Gholami},
           familyi={G\bibinitperiod},
           given={Amir},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2b2d4c95737d2d23f0073f459c476a5b}{%
           family={Zhe},
           familyi={Z\bibinitperiod},
           given={Shandian},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7c6885eab8c65e8a9bd2bec343195cf8}{%
           family={Kirby},
           familyi={K\bibinitperiod},
           given={Robert\bibnamedelima M.},
           giveni={R\bibinitperiod\bibinitdelim M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6428c5852da0dfbdd50e39418c9571bf}{%
           family={Mahoney},
           familyi={M\bibinitperiod},
           given={Michael\bibnamedelima W.},
           giveni={M\bibinitperiod\bibinitdelim W\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{9f843b65003ef4dc2175a0a5230a48cc}
      \strng{fullhash}{81a14c02559ac81c2699a8082143d491}
      \strng{bibnamehash}{9f843b65003ef4dc2175a0a5230a48cc}
      \strng{authorbibnamehash}{9f843b65003ef4dc2175a0a5230a48cc}
      \strng{authornamehash}{9f843b65003ef4dc2175a0a5230a48cc}
      \strng{authorfullhash}{81a14c02559ac81c2699a8082143d491}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recent work in scientific machine learning has developed so-called physics-informed neural network (PINN) models. The typical approach is to incorporate physical domain knowledge as soft constraints on an empirical loss function and use existing machine learning methodologies to train the model. We demonstrate that, while existing PINN methodologies can learn good models for relatively trivial problems, they can easily fail to learn relevant physical phenomena for even slightly more complex problems. In particular, we analyze several distinct situations of widespread physical interest, including learning differential equations with convection, reaction, and diffusion operators. We provide evidence that the soft regularization in PINNs, which involves PDE-based differential operators, can introduce a number of subtle problems, including making the problem more ill-conditioned. Importantly, we show that these possible failure modes are not due to the lack of expressivity in the NN architecture, but that the PINN's setup makes the loss landscape very hard to optimize. We then describe two promising solutions to address these failure modes. The first approach is to use curriculum regularization, where the PINN's loss term starts from a simple PDE regularization, and becomes progressively more complex as the NN gets trained. The second approach is to pose the problem as a sequence-to-sequence learning task, rather than learning to predict the entire space-time at once. Extensive testing shows that we can achieve up to 1-2 orders of magnitude lower error with these methods as compared to regular PINN training.}
      \field{journaltitle}{arXiv:2109.01050 [physics]}
      \field{month}{11}
      \field{note}{arXiv: 2109.01050}
      \field{title}{Characterizing possible failure modes in physics-informed neural networks}
      \field{urlday}{29}
      \field{urlmonth}{3}
      \field{urlyear}{2022}
      \field{year}{2021}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://arxiv.org/abs/2109.01050
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2109.01050
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Numerical Analysis,Physics - Computational Physics}
    \endentry
    \entry{lesjak_chaotic_2021}{article}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=13836f2784f3063dc7c637a76e8e9c49}{%
           family={Lesjak},
           familyi={L\bibinitperiod},
           given={Mathias},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=326372873765c3d7ca8ae67846c1e98b}{%
           family={Doan},
           familyi={D\bibinitperiod},
           given={Nguyen\bibnamedelimb Anh\bibnamedelima Khoa},
           giveni={N\bibinitperiod\bibinitdelim A\bibinitperiod\bibinitdelim K\bibinitperiod},
           givenun=0}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{9e2b7d7c28564ff3bf8db434ca66be5d}
      \strng{fullhash}{9e2b7d7c28564ff3bf8db434ca66be5d}
      \strng{bibnamehash}{9e2b7d7c28564ff3bf8db434ca66be5d}
      \strng{authorbibnamehash}{9e2b7d7c28564ff3bf8db434ca66be5d}
      \strng{authornamehash}{9e2b7d7c28564ff3bf8db434ca66be5d}
      \strng{authorfullhash}{9e2b7d7c28564ff3bf8db434ca66be5d}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We explore the possibility of combining a knowledge-based reduced order model (ROM) with a reservoir computing approach to learn and predict the dynamics of chaotic systems. The ROM is based on proper orthogonal decomposition (POD) with Galerkin projection to capture the essential dynamics of the chaotic system while the reservoir computing approach used is based on echo state networks (ESNs). Two different hybrid approaches are explored: one where the ESN corrects the modal coefficients of the ROM (hybrid-ESN-A) and one where the ESN uses and corrects the ROM prediction in full state space (hybrid-ESN-B). These approaches are applied on two chaotic systems: the Charney–DeVore system and the Kuramoto–Sivashinsky equation and are compared to the ROM obtained using POD/Galerkin projection and to the data-only approach based uniquely on the ESN. The hybrid-ESN-B approach is seen to provide the best prediction accuracy, outperforming the other hybrid approach, the POD/Galerkin projection ROM, and the data-only ESN, especially when using ESNs with a small number of neurons. In addition, the influence of the accuracy of the ROM on the overall prediction accuracy of the hybrid-ESN-B is assessed rigorously by considering ROMs composed of different numbers of POD modes. Further analysis on how hybrid-ESN-B blends the prediction from the ROM and the ESN to predict the evolution of the system is also provided.}
      \field{issn}{2632-6736}
      \field{journaltitle}{Data-Centric Engineering}
      \field{note}{Publisher: Cambridge University Press}
      \field{title}{Chaotic systems learning with hybrid echo state network/proper orthogonal decomposition based model}
      \field{urlday}{27}
      \field{urlmonth}{10}
      \field{urlyear}{2021}
      \field{volume}{2}
      \field{year}{2021}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.1017/dce.2021.17
      \endverb
      \verb{urlraw}
      \verb https://www.cambridge.org/core/journals/data-centric-engineering/article/chaotic-systems-learning-with-hybrid-echo-state-networkproper-orthogonal-decomposition-based-model/636D2CB1BA6EC278427271CE624F29B2
      \endverb
      \verb{url}
      \verb https://www.cambridge.org/core/journals/data-centric-engineering/article/chaotic-systems-learning-with-hybrid-echo-state-networkproper-orthogonal-decomposition-based-model/636D2CB1BA6EC278427271CE624F29B2
      \endverb
      \keyw{Chaotic systems,echo state network,proper orthogonal decomposition,reservoir computing,surrogate modeling}
    \endentry
    \entry{li_fourier_2020}{article}{}
      \name{author}{7}{ul=2}{%
        {{un=0,uniquepart=base,hash=009036050bf23abe561429f6eaa0b018}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Zongyi},
           giveni={Z\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d3715d69863d59245589f244c5866bd4}{%
           family={Kovachki},
           familyi={K\bibinitperiod},
           given={Nikola},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3648b51d63e1ae389e46f87296d4c89a}{%
           family={Azizzadenesheli},
           familyi={A\bibinitperiod},
           given={Kamyar},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9caea1a7b561311e5146f4c7f740496c}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Burigede},
           giveni={B\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1e523aae18c868f77439b35add494ff9}{%
           family={Bhattacharya},
           familyi={B\bibinitperiod},
           given={Kaushik},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9fdd1b883dec513483f9ee5c99b7e251}{%
           family={Stuart},
           familyi={S\bibinitperiod},
           given={Andrew},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=bd4a2be6ede169a3c794a1cd7e3fb9a2}{%
           family={Anandkumar},
           familyi={A\bibinitperiod},
           given={Anima},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{502de4b3473ec7b0ec1eb60736d5b721}
      \strng{fullhash}{3e58006f2c869702116637d04136c3b9}
      \strng{bibnamehash}{502de4b3473ec7b0ec1eb60736d5b721}
      \strng{authorbibnamehash}{502de4b3473ec7b0ec1eb60736d5b721}
      \strng{authornamehash}{502de4b3473ec7b0ec1eb60736d5b721}
      \strng{authorfullhash}{3e58006f2c869702116637d04136c3b9}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and the Navier-Stokes equation (including the turbulent regime). Our Fourier neural operator shows state-of-the-art performance compared to existing neural network methodologies and it is up to three orders of magnitude faster compared to traditional PDE solvers.}
      \field{journaltitle}{arXiv:2010.08895 [cs, math]}
      \field{month}{10}
      \field{note}{arXiv: 2010.08895}
      \field{title}{Fourier {Neural} {Operator} for {Parametric} {Partial} {Differential} {Equations}}
      \field{urlday}{31}
      \field{urlmonth}{10}
      \field{urlyear}{2020}
      \field{year}{2020}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://arxiv.org/abs/2010.08895
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2010.08895
      \endverb
      \keyw{Computer Science - Machine Learning,Mathematics - Numerical Analysis}
    \endentry
    \entry{li_learning_2022}{misc}{}
      \name{author}{8}{ul=2}{%
        {{un=0,uniquepart=base,hash=009036050bf23abe561429f6eaa0b018}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Zongyi},
           giveni={Z\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a60247ed916f17dde7bd3f592bbac2ae}{%
           family={Liu-Schiaffini},
           familyi={L\bibinithyphendelim S\bibinitperiod},
           given={Miguel},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d3715d69863d59245589f244c5866bd4}{%
           family={Kovachki},
           familyi={K\bibinitperiod},
           given={Nikola},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9caea1a7b561311e5146f4c7f740496c}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Burigede},
           giveni={B\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3648b51d63e1ae389e46f87296d4c89a}{%
           family={Azizzadenesheli},
           familyi={A\bibinitperiod},
           given={Kamyar},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1e523aae18c868f77439b35add494ff9}{%
           family={Bhattacharya},
           familyi={B\bibinitperiod},
           given={Kaushik},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9fdd1b883dec513483f9ee5c99b7e251}{%
           family={Stuart},
           familyi={S\bibinitperiod},
           given={Andrew},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=bd4a2be6ede169a3c794a1cd7e3fb9a2}{%
           family={Anandkumar},
           familyi={A\bibinitperiod},
           given={Anima},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{8163ef0e34b85ac7f04b70adb7ed53ce}
      \strng{fullhash}{c1d8500d01638bd2b39a3e7e3af5a5cd}
      \strng{bibnamehash}{8163ef0e34b85ac7f04b70adb7ed53ce}
      \strng{authorbibnamehash}{8163ef0e34b85ac7f04b70adb7ed53ce}
      \strng{authornamehash}{8163ef0e34b85ac7f04b70adb7ed53ce}
      \strng{authorfullhash}{c1d8500d01638bd2b39a3e7e3af5a5cd}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Chaotic systems are notoriously challenging to predict because of their sensitivity to perturbations and errors due to time stepping. Despite this unpredictable behavior, for many dissipative systems the statistics of the long term trajectories are governed by an invariant measure supported on a set, known as the global attractor; for many problems this set is finite dimensional, even if the state space is infinite dimensional. For Markovian systems, the statistical properties of long-term trajectories are uniquely determined by the solution operator that maps the evolution of the system over arbitrary positive time increments. In this work, we propose a machine learning framework to learn the underlying solution operator for dissipative chaotic systems, showing that the resulting learned operator accurately captures short-time trajectories and long-time statistical behavior. Using this framework, we are able to predict various statistics of the invariant measure for the turbulent Kolmogorov Flow dynamics with Reynolds numbers up to 5000.}
      \field{month}{9}
      \field{note}{arXiv:2106.06898 [cs, math]}
      \field{title}{Learning {Dissipative} {Dynamics} in {Chaotic} {Systems}}
      \field{urlday}{13}
      \field{urlmonth}{4}
      \field{urlyear}{2023}
      \field{year}{2022}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://arxiv.org/abs/2106.06898
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2106.06898
      \endverb
      \keyw{Computer Science - Machine Learning,Mathematics - Dynamical Systems}
    \endentry
    \entry{rahaman_spectral_2019}{article}{}
      \name{author}{8}{}{%
        {{un=0,uniquepart=base,hash=d4c778f9c80602fb7b4cd44117a0d8a0}{%
           family={Rahaman},
           familyi={R\bibinitperiod},
           given={Nasim},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2d7e0f228a4066de5e2bbaf703a16d30}{%
           family={Baratin},
           familyi={B\bibinitperiod},
           given={Aristide},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0f0c297dd56ddda3c4590124b80d0472}{%
           family={Arpit},
           familyi={A\bibinitperiod},
           given={Devansh},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=4b0006f485d3207563598a41053661bd}{%
           family={Draxler},
           familyi={D\bibinitperiod},
           given={Felix},
           giveni={F\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d2827b813d1d73df44360201e3296bea}{%
           family={Lin},
           familyi={L\bibinitperiod},
           given={Min},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7871c4e1ed2fbdab6da3b739b1a73478}{%
           family={Hamprecht},
           familyi={H\bibinitperiod},
           given={Fred\bibnamedelima A.},
           giveni={F\bibinitperiod\bibinitdelim A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ccec1ccd2e1aa86960eb2e872c6b7020}{%
           family={Courville},
           familyi={C\bibinitperiod},
           given={Aaron},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{9da4ff5c16c96a7276e20d967deb609d}
      \strng{fullhash}{d67237c9b1e83f582d6882a52ea4e648}
      \strng{bibnamehash}{9da4ff5c16c96a7276e20d967deb609d}
      \strng{authorbibnamehash}{9da4ff5c16c96a7276e20d967deb609d}
      \strng{authornamehash}{9da4ff5c16c96a7276e20d967deb609d}
      \strng{authorfullhash}{d67237c9b1e83f582d6882a52ea4e648}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with \$100{\textbackslash}\%\$ accuracy. In this work, we present properties of neural networks that complement this aspect of expressivity. By using tools from Fourier analysis, we show that deep ReLU networks are biased towards low frequency functions, meaning that they cannot have local fluctuations without affecting their global behavior. Intuitively, this property is in line with the observation that over-parameterized networks find simple patterns that generalize across data samples. We also investigate how the shape of the data manifold affects expressivity by showing evidence that learning high frequencies gets {\textbackslash}emph\{easier\} with increasing manifold complexity, and present a theoretical understanding of this behavior. Finally, we study the robustness of the frequency components with respect to parameter perturbation, to develop the intuition that the parameters must be finely tuned to express high frequency functions.}
      \field{journaltitle}{arXiv:1806.08734 [cs, stat]}
      \field{month}{5}
      \field{note}{arXiv: 1806.08734}
      \field{title}{On the {Spectral} {Bias} of {Neural} {Networks}}
      \field{urlday}{12}
      \field{urlmonth}{2}
      \field{urlyear}{2021}
      \field{year}{2019}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://arxiv.org/abs/1806.08734
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1806.08734
      \endverb
      \keyw{Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{rahman_u-no_2022}{misc}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=4e0a4a2fff9e4c918ad4503ff42f4dcd}{%
           family={Rahman},
           familyi={R\bibinitperiod},
           given={Md\bibnamedelima Ashiqur},
           giveni={M\bibinitperiod\bibinitdelim A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=223dfe7e2e0a24e2acdeaa8c9d742122}{%
           family={Ross},
           familyi={R\bibinitperiod},
           given={Zachary\bibnamedelima E.},
           giveni={Z\bibinitperiod\bibinitdelim E\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3648b51d63e1ae389e46f87296d4c89a}{%
           family={Azizzadenesheli},
           familyi={A\bibinitperiod},
           given={Kamyar},
           giveni={K\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{5a970a392dbce81ef0dda6adab5920e5}
      \strng{fullhash}{5a970a392dbce81ef0dda6adab5920e5}
      \strng{bibnamehash}{5a970a392dbce81ef0dda6adab5920e5}
      \strng{authorbibnamehash}{5a970a392dbce81ef0dda6adab5920e5}
      \strng{authornamehash}{5a970a392dbce81ef0dda6adab5920e5}
      \strng{authorfullhash}{5a970a392dbce81ef0dda6adab5920e5}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Neural operators generalize classical neural networks to maps between infinite-dimensional spaces, e.g. function spaces. Prior works on neural operators proposed a series of novel architectures to learn such maps and demonstrated unprecedented success in learning solution operators of partial differential equations. Due to their close proximity to fully connected architectures, these models mainly suffer from high memory usage and are generally limited to shallow deep learning models. In this paper, we propose U-shaped Neural Operator (U-NO), a U-shaped memory enhanced architecture that allows for deeper neural operators. U-NOs exploit the problem structures in function predictions and demonstrate fast training, data efficiency, and robustness with respect to hyperparameters choices. We study the performance of U-NO on PDE benchmarks, namely, Darcy's flow law and the Navier-Stokes equations. We show that U-NO results in an average of 14\% and 34\% prediction improvement on Darcy's flow and turbulent Navier-Stokes equations, respectively, over the state of art. On Navier-Stokes 3D spatio-temporal operator learning task, we show U-NO provides 40\% improvement over the state of art methods.}
      \field{month}{5}
      \field{note}{Number: arXiv:2204.11127 arXiv:2204.11127 [cs]}
      \field{shorttitle}{U-{NO}}
      \field{title}{U-{NO}: {U}-shaped {Neural} {Operators}}
      \field{urlday}{23}
      \field{urlmonth}{6}
      \field{urlyear}{2022}
      \field{year}{2022}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2204.11127
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2204.11127
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2204.11127
      \endverb
      \keyw{Computer Science - Machine Learning}
    \endentry
    \entry{raissi_physics-informed_2019}{article}{}
      \name{author}{3}{}{%
        {{un=2,uniquepart=given,hash=a3194cb4f8e3959e569236521aa2fd86}{%
           family={Raissi},
           familyi={R\bibinitperiod},
           given={M.},
           giveni={M\bibinitperiod},
           givenun=2}}%
        {{un=2,uniquepart=given,hash=c2e8423b7b49343a85abf9af92ee2a82}{%
           family={Perdikaris},
           familyi={P\bibinitperiod},
           given={P.},
           giveni={P\bibinitperiod},
           givenun=2}}%
        {{un=2,uniquepart=given,hash=1c2f5930e36485e8d38a87746769fdcc}{%
           family={Karniadakis},
           familyi={K\bibinitperiod},
           given={G.\bibnamedelimi E.},
           giveni={G\bibinitperiod\bibinitdelim E\bibinitperiod},
           givenun=2}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{a17a0649dc8bad70026488d4ea37c508}
      \strng{fullhash}{a17a0649dc8bad70026488d4ea37c508}
      \strng{bibnamehash}{a17a0649dc8bad70026488d4ea37c508}
      \strng{authorbibnamehash}{a17a0649dc8bad70026488d4ea37c508}
      \strng{authornamehash}{a17a0649dc8bad70026488d4ea37c508}
      \strng{authorfullhash}{a17a0649dc8bad70026488d4ea37c508}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.}
      \field{issn}{0021-9991}
      \field{journaltitle}{Journal of Computational Physics}
      \field{month}{2}
      \field{shorttitle}{Physics-informed neural networks}
      \field{title}{Physics-informed neural networks: {A} deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations}
      \field{urlday}{20}
      \field{urlmonth}{10}
      \field{urlyear}{2020}
      \field{volume}{378}
      \field{year}{2019}
      \field{urldateera}{ce}
      \field{pages}{686\bibrangedash 707}
      \range{pages}{22}
      \verb{doi}
      \verb 10.1016/j.jcp.2018.10.045
      \endverb
      \verb{urlraw}
      \verb http://www.sciencedirect.com/science/article/pii/S0021999118307125
      \endverb
      \verb{url}
      \verb http://www.sciencedirect.com/science/article/pii/S0021999118307125
      \endverb
      \keyw{Data-driven scientific computing,Machine learning,Nonlinear dynamics,Predictive modeling,Runge–Kutta methods}
    \endentry
    \entry{raissi_hidden_2018}{article}{}
      \name{author}{3}{}{%
        {{un=2,uniquepart=given,hash=95a17c39168b9e6b8812f207b4e036c1}{%
           family={Raissi},
           familyi={R\bibinitperiod},
           given={Maziar},
           giveni={M\bibinitperiod},
           givenun=2}}%
        {{un=0,uniquepart=base,hash=ce3864f324c7cfd1f7ae472c18c609c4}{%
           family={Yazdani},
           familyi={Y\bibinitperiod},
           given={Alireza},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=2,uniquepart=given,hash=bfaf154b1f300f885a75adce4c6b773e}{%
           family={Karniadakis},
           familyi={K\bibinitperiod},
           given={George\bibnamedelima Em},
           giveni={G\bibinitperiod\bibinitdelim E\bibinitperiod},
           givenun=2}}%
      }
      \strng{namehash}{5844c9626bd0e4afec299ee910600642}
      \strng{fullhash}{5844c9626bd0e4afec299ee910600642}
      \strng{bibnamehash}{5844c9626bd0e4afec299ee910600642}
      \strng{authorbibnamehash}{5844c9626bd0e4afec299ee910600642}
      \strng{authornamehash}{5844c9626bd0e4afec299ee910600642}
      \strng{authorfullhash}{5844c9626bd0e4afec299ee910600642}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We present hidden fluid mechanics (HFM), a physics informed deep learning framework capable of encoding an important class of physical laws governing fluid motions, namely the Navier-Stokes equations. In particular, we seek to leverage the underlying conservation laws (i.e., for mass, momentum, and energy) to infer hidden quantities of interest such as velocity and pressure fields merely from spatio-temporal visualizations of a passive scaler (e.g., dye or smoke), transported in arbitrarily complex domains (e.g., in human arteries or brain aneurysms). Our approach towards solving the aforementioned data assimilation problem is unique as we design an algorithm that is agnostic to the geometry or the initial and boundary conditions. This makes HFM highly flexible in choosing the spatio-temporal domain of interest for data acquisition as well as subsequent training and predictions. Consequently, the predictions made by HFM are among those cases where a pure machine learning strategy or a mere scientific computing approach simply cannot reproduce. The proposed algorithm achieves accurate predictions of the pressure and velocity fields in both two and three dimensional flows for several benchmark problems motivated by real-world applications. Our results demonstrate that this relatively simple methodology can be used in physical and biomedical problems to extract valuable quantitative information (e.g., lift and drag forces or wall shear stresses in arteries) for which direct measurements may not be possible.}
      \field{journaltitle}{arXiv:1808.04327 [physics, stat]}
      \field{month}{8}
      \field{note}{arXiv: 1808.04327}
      \field{shorttitle}{Hidden {Fluid} {Mechanics}}
      \field{title}{Hidden {Fluid} {Mechanics}: {A} {Navier}-{Stokes} {Informed} {Deep} {Learning} {Framework} for {Assimilating} {Flow} {Visualization} {Data}}
      \field{urlday}{28}
      \field{urlmonth}{10}
      \field{urlyear}{2020}
      \field{year}{2018}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://arxiv.org/abs/1808.04327
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1808.04327
      \endverb
      \keyw{Computer Science - Computational Engineering,Finance,and Science,Computer Science - Machine Learning,Physics - Fluid Dynamics,Statistics - Machine Learning}
    \endentry
    \entry{ramachandran_searching_2017}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=e67314b379d3959e35d4acce17b95a6d}{%
           family={Ramachandran},
           familyi={R\bibinitperiod},
           given={Prajit},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=91aafc93e68e8c6a55e8cd804c2acaf2}{%
           family={Zoph},
           familyi={Z\bibinitperiod},
           given={Barret},
           giveni={B\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c636f146591d51579a8119b777394878}{%
           family={Le},
           familyi={L\bibinitperiod},
           given={Quoc\bibnamedelima V.},
           giveni={Q\bibinitperiod\bibinitdelim V\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{057007a647f95fd49b81c4ff27c3c59e}
      \strng{fullhash}{057007a647f95fd49b81c4ff27c3c59e}
      \strng{bibnamehash}{057007a647f95fd49b81c4ff27c3c59e}
      \strng{authorbibnamehash}{057007a647f95fd49b81c4ff27c3c59e}
      \strng{authornamehash}{057007a647f95fd49b81c4ff27c3c59e}
      \strng{authorfullhash}{057007a647f95fd49b81c4ff27c3c59e}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, \$f(x) = x {\textbackslash}cdot {\textbackslash}text\{sigmoid\}({\textbackslash}beta x)\$, which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9{\textbackslash}\% for Mobile NASNet-A and 0.6{\textbackslash}\% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.}
      \field{journaltitle}{arXiv:1710.05941 [cs]}
      \field{month}{10}
      \field{note}{arXiv: 1710.05941}
      \field{title}{Searching for {Activation} {Functions}}
      \field{urlday}{27}
      \field{urlmonth}{11}
      \field{urlyear}{2020}
      \field{year}{2017}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://arxiv.org/abs/1710.05941
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1710.05941
      \endverb
      \keyw{Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
    \endentry
    \entry{sanchez-gonzalez_learning_2020}{misc}{}
      \name{author}{6}{ul=2}{%
        {{un=0,uniquepart=base,hash=cfe62c9365c523ba7d4f7fd1147cff41}{%
           family={Sanchez-Gonzalez},
           familyi={S\bibinithyphendelim G\bibinitperiod},
           given={Alvaro},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3f8b59cd2dfceed714d405a08c389acb}{%
           family={Godwin},
           familyi={G\bibinitperiod},
           given={Jonathan},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a726408819b6f955652c3ffaaedef966}{%
           family={Pfaff},
           familyi={P\bibinitperiod},
           given={Tobias},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6b37c8aefa150635663b2a6525952a7a}{%
           family={Ying},
           familyi={Y\bibinitperiod},
           given={Rex},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=900d107125ff0ca84698cb909e4f6c51}{%
           family={Leskovec},
           familyi={L\bibinitperiod},
           given={Jure},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2e5b4b0fd5bc4903da838ce86f63998b}{%
           family={Battaglia},
           familyi={B\bibinitperiod},
           given={Peter\bibnamedelima W.},
           giveni={P\bibinitperiod\bibinitdelim W\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{fb6f360e8bbf8558bbda6c469bdc117f}
      \strng{fullhash}{3d5b64021935f881dbba3091a29904aa}
      \strng{bibnamehash}{fb6f360e8bbf8558bbda6c469bdc117f}
      \strng{authorbibnamehash}{fb6f360e8bbf8558bbda6c469bdc117f}
      \strng{authornamehash}{fb6f360e8bbf8558bbda6c469bdc117f}
      \strng{authorfullhash}{3d5b64021935f881dbba3091a29904aa}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Here we present a machine learning framework and model implementation that can learn to simulate a wide variety of challenging physical domains, involving fluids, rigid solids, and deformable materials interacting with one another. Our framework---which we term "Graph Network-based Simulators" (GNS)---represents the state of a physical system with particles, expressed as nodes in a graph, and computes dynamics via learned message-passing. Our results show that our model can generalize from single-timestep predictions with thousands of particles during training, to different initial conditions, thousands of timesteps, and at least an order of magnitude more particles at test time. Our model was robust to hyperparameter choices across various evaluation metrics: the main determinants of long-term performance were the number of message-passing steps, and mitigating the accumulation of error by corrupting the training data with noise. Our GNS framework advances the state-of-the-art in learned physical simulation, and holds promise for solving a wide range of complex forward and inverse problems.}
      \field{month}{9}
      \field{note}{arXiv:2002.09405 [physics, stat]}
      \field{title}{Learning to {Simulate} {Complex} {Physics} with {Graph} {Networks}}
      \field{urlday}{11}
      \field{urlmonth}{1}
      \field{urlyear}{2023}
      \field{year}{2020}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://arxiv.org/abs/2002.09405
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2002.09405
      \endverb
      \keyw{Computer Science - Machine Learning,Physics - Computational Physics,Statistics - Machine Learning}
    \endentry
    \entry{sanchez-gonzalez_learning_2021}{article}{}
      \name{author}{10}{ul=2}{%
        {{un=0,uniquepart=base,hash=cfe62c9365c523ba7d4f7fd1147cff41}{%
           family={Sanchez-Gonzalez},
           familyi={S\bibinithyphendelim G\bibinitperiod},
           given={Alvaro},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2270062cd55c887966fece35c0101aff}{%
           family={Stachenfeld},
           familyi={S\bibinitperiod},
           given={Kimberly},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b7bdc3ca33f156f0c7d50edea100f8a4}{%
           family={Fielding},
           familyi={F\bibinitperiod},
           given={Drummond\bibnamedelima B},
           giveni={D\bibinitperiod\bibinitdelim B\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e7dda8c99b1c7acd0431fbb13074ed4f}{%
           family={Kochkov},
           familyi={K\bibinitperiod},
           given={Dmitrii},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=987d3dced51a437a02e32a98445c2893}{%
           family={Cranmer},
           familyi={C\bibinitperiod},
           given={Miles},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a726408819b6f955652c3ffaaedef966}{%
           family={Pfaff},
           familyi={P\bibinitperiod},
           given={Tobias},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3f8b59cd2dfceed714d405a08c389acb}{%
           family={Godwin},
           familyi={G\bibinitperiod},
           given={Jonathan},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9e24ba919b5b64b92edbcd99491413f0}{%
           family={Cui},
           familyi={C\bibinitperiod},
           given={Can},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3b755f0738ae9e96bfddd2247581347f}{%
           family={Ho},
           familyi={H\bibinitperiod},
           given={Shirley},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5a63429a6e1733e8f3f4dc71cbb6eec9}{%
           family={Battaglia},
           familyi={B\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod},
           givenun=0}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{e0fcdcb4ff4464414459c6934b63633c}
      \strng{fullhash}{e89ba88b2881b0e8494deb059ca89eb8}
      \strng{bibnamehash}{e0fcdcb4ff4464414459c6934b63633c}
      \strng{authorbibnamehash}{e0fcdcb4ff4464414459c6934b63633c}
      \strng{authornamehash}{e0fcdcb4ff4464414459c6934b63633c}
      \strng{authorfullhash}{e89ba88b2881b0e8494deb059ca89eb8}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Given the rise of machine learning (ML) for simulation, an important question is: to what extent can learned models supplement or replace traditional simulators? Here we develop a fully learned simulator model based on domain-general Convolutional Neural Network (CNN) methods, and study its performance on a range of turbulence problems in astrophysics. We compare the learned model to specialized PDE solvers in terms of spatial and temporal resolution, numerical stability, and generalization performance. We ﬁnd that the learned models outperform coarsened solvers on certain metrics, particularly in their ability to preserve high-frequency information at low resolution, and describe ways to improve generalization beyond the training distribution. To our knowledge, our model is the ﬁrst to be trained on Athena++ (a state-of-the-art simulator widely used in computational ﬂuid dynamics and magneto-hydrodynamics), and more generally, the ﬁrst fully-learned astrophysical turbulence simulator.}
      \field{title}{{LEARNING} {GENERAL}-{PURPOSE} {CNN}-{BASED} {SIMULA}- {TORS} {FOR} {ASTROPHYSICAL} {TURBULENCE}}
      \field{year}{2021}
      \field{pages}{12}
      \range{pages}{1}
    \endentry
    \entry{stachenfeld_learned_2022}{misc}{}
      \name{author}{10}{}{%
        {{un=0,uniquepart=base,hash=2270062cd55c887966fece35c0101aff}{%
           family={Stachenfeld},
           familyi={S\bibinitperiod},
           given={Kimberly},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b7bdc3ca33f156f0c7d50edea100f8a4}{%
           family={Fielding},
           familyi={F\bibinitperiod},
           given={Drummond\bibnamedelima B.},
           giveni={D\bibinitperiod\bibinitdelim B\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e7dda8c99b1c7acd0431fbb13074ed4f}{%
           family={Kochkov},
           familyi={K\bibinitperiod},
           given={Dmitrii},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=987d3dced51a437a02e32a98445c2893}{%
           family={Cranmer},
           familyi={C\bibinitperiod},
           given={Miles},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a726408819b6f955652c3ffaaedef966}{%
           family={Pfaff},
           familyi={P\bibinitperiod},
           given={Tobias},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3f8b59cd2dfceed714d405a08c389acb}{%
           family={Godwin},
           familyi={G\bibinitperiod},
           given={Jonathan},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9e24ba919b5b64b92edbcd99491413f0}{%
           family={Cui},
           familyi={C\bibinitperiod},
           given={Can},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3b755f0738ae9e96bfddd2247581347f}{%
           family={Ho},
           familyi={H\bibinitperiod},
           given={Shirley},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5a63429a6e1733e8f3f4dc71cbb6eec9}{%
           family={Battaglia},
           familyi={B\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=cfe62c9365c523ba7d4f7fd1147cff41}{%
           family={Sanchez-Gonzalez},
           familyi={S\bibinithyphendelim G\bibinitperiod},
           given={Alvaro},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{e29238bd181f562de0eff49f08d880b8}
      \strng{fullhash}{1a974f45c3a4bb24706c395fc0767c5e}
      \strng{bibnamehash}{e29238bd181f562de0eff49f08d880b8}
      \strng{authorbibnamehash}{e29238bd181f562de0eff49f08d880b8}
      \strng{authornamehash}{e29238bd181f562de0eff49f08d880b8}
      \strng{authorfullhash}{1a974f45c3a4bb24706c395fc0767c5e}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Turbulence simulation with classical numerical solvers requires high-resolution grids to accurately resolve dynamics. Here we train learned simulators at low spatial and temporal resolutions to capture turbulent dynamics generated at high resolution. We show that our proposed model can simulate turbulent dynamics more accurately than classical numerical solvers at the comparably low resolutions across various scientifically relevant metrics. Our model is trained end-to-end from data and is capable of learning a range of challenging chaotic and turbulent dynamics at low resolution, including trajectories generated by the state-of-the-art Athena++ engine. We show that our simpler, general-purpose architecture outperforms various more specialized, turbulence-specific architectures from the learned turbulence simulation literature. In general, we see that learned simulators yield unstable trajectories; however, we show that tuning training noise and temporal downsampling solves this problem. We also find that while generalization beyond the training distribution is a challenge for learned models, training noise, added loss constraints, and dataset augmentation can help. Broadly, we conclude that our learned simulator outperforms traditional solvers run on coarser grids, and emphasize that simple design choices can offer stability and robust generalization.}
      \field{month}{4}
      \field{note}{arXiv:2112.15275 [physics]}
      \field{title}{Learned {Coarse} {Models} for {Efficient} {Turbulence} {Simulation}}
      \field{urlday}{19}
      \field{urlmonth}{9}
      \field{urlyear}{2022}
      \field{year}{2022}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2112.15275
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2112.15275
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2112.15275
      \endverb
      \keyw{Computer Science - Machine Learning,Physics - Computational Physics,Physics - Fluid Dynamics}
    \endentry
    \entry{vinuesa_potential_2021}{report}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=127780749e189f5f4d204959b9a0d714}{%
           family={Vinuesa},
           familyi={V\bibinitperiod},
           given={Ricardo},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=572af75cb67759c9684ffbb4d11cddf8}{%
           family={Brunton},
           familyi={B\bibinitperiod},
           given={Steven\bibnamedelima L.},
           giveni={S\bibinitperiod\bibinitdelim L\bibinitperiod},
           givenun=0}}%
      }
      \list{institution}{1}{%
        {arXiv}%
      }
      \strng{namehash}{50aedcbbc3a610840aac7b42fff0306d}
      \strng{fullhash}{50aedcbbc3a610840aac7b42fff0306d}
      \strng{bibnamehash}{50aedcbbc3a610840aac7b42fff0306d}
      \strng{authorbibnamehash}{50aedcbbc3a610840aac7b42fff0306d}
      \strng{authornamehash}{50aedcbbc3a610840aac7b42fff0306d}
      \strng{authorfullhash}{50aedcbbc3a610840aac7b42fff0306d}
      \field{sortinit}{V}
      \field{sortinithash}{afb52128e5b4dc4b843768c0113d673b}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Machine learning is rapidly becoming a core technology for scientific computing, with numerous opportunities to advance the field of computational fluid dynamics. This paper highlights some of the areas of highest potential impact, including to accelerate direct numerical simulations, to improve turbulence closure modelling, and to develop enhanced reduced-order models. In each of these areas, it is possible to improve machine learning capabilities by incorporating physics into the process, and in turn, to improve the simulation of fluids to uncover new physical understanding. Despite the promise of machine learning described here, we also note that classical methods are often more efficient for many tasks. We also emphasize that in order to harness the full potential of machine learning to improve computational fluid dynamics, it is essential for the community to continue to establish benchmark systems and best practices for open-source software, data sharing, and reproducible research.}
      \field{month}{10}
      \field{note}{arXiv:2110.02085 [physics] type: article}
      \field{number}{arXiv:2110.02085}
      \field{title}{The {Potential} of {Machine} {Learning} to {Enhance} {Computational} {Fluid} {Dynamics}}
      \field{type}{techreport}
      \field{urlday}{20}
      \field{urlmonth}{5}
      \field{urlyear}{2022}
      \field{year}{2021}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2110.02085
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2110.02085
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2110.02085
      \endverb
      \keyw{Computer Science - Machine Learning,Physics - Computational Physics,Physics - Fluid Dynamics}
    \endentry
    \entry{wang_understanding_2020}{article}{}
      \name{author}{3}{}{%
        {{un=1,uniquepart=given,hash=15db674b3efc6848f40514aa0034717a}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Sifan},
           giveni={S\bibinitperiod},
           givenun=1}}%
        {{un=0,uniquepart=base,hash=00a1efab84cff4f68ff48983d8397f76}{%
           family={Teng},
           familyi={T\bibinitperiod},
           given={Yujun},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=2,uniquepart=given,hash=197d71332633e07aa578718628fdc405}{%
           family={Perdikaris},
           familyi={P\bibinitperiod},
           given={Paris},
           giveni={P\bibinitperiod},
           givenun=2}}%
      }
      \strng{namehash}{cd1083c1f65115a089ff8588b82928cf}
      \strng{fullhash}{cd1083c1f65115a089ff8588b82928cf}
      \strng{bibnamehash}{cd1083c1f65115a089ff8588b82928cf}
      \strng{authorbibnamehash}{cd1083c1f65115a089ff8588b82928cf}
      \strng{authornamehash}{cd1083c1f65115a089ff8588b82928cf}
      \strng{authorfullhash}{cd1083c1f65115a089ff8588b82928cf}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The widespread use of neural networks across different scientific domains often involves constraining them to satisfy certain symmetries, conservation laws, or other domain knowledge. Such constraints are often imposed as soft penalties during model training and effectively act as domain-specific regularizers of the empirical risk loss. Physics-informed neural networks is an example of this philosophy in which the outputs of deep neural networks are constrained to approximately satisfy a given set of partial differential equations. In this work we review recent advances in scientific machine learning with a specific focus on the effectiveness of physics-informed neural networks in predicting outcomes of physical systems and discovering hidden physics from noisy data. We will also identify and analyze a fundamental mode of failure of such approaches that is related to numerical stiffness leading to unbalanced back-propagated gradients during model training. To address this limitation we present a learning rate annealing algorithm that utilizes gradient statistics during model training to balance the interplay between different terms in composite loss functions. We also propose a novel neural network architecture that is more resilient to such gradient pathologies. Taken together, our developments provide new insights into the training of constrained neural networks and consistently improve the predictive accuracy of physics-informed neural networks by a factor of 50-100x across a range of problems in computational physics. All code and data accompanying this manuscript are publicly available at {\textbackslash}url\{https://github.com/PredictiveIntelligenceLab/GradientPathologiesPINNs\}.}
      \field{journaltitle}{arXiv:2001.04536 [cs, math, stat]}
      \field{month}{1}
      \field{note}{arXiv: 2001.04536}
      \field{title}{Understanding and mitigating gradient pathologies in physics-informed neural networks}
      \field{urlday}{1}
      \field{urlmonth}{12}
      \field{urlyear}{2020}
      \field{year}{2020}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://arxiv.org/abs/2001.04536
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2001.04536
      \endverb
      \keyw{Computer Science - Machine Learning,Mathematics - Numerical Analysis,Statistics - Machine Learning}
    \endentry
    \entry{wang_eigenvector_2020}{article}{}
      \name{author}{3}{}{%
        {{un=1,uniquepart=given,hash=15db674b3efc6848f40514aa0034717a}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Sifan},
           giveni={S\bibinitperiod},
           givenun=1}}%
        {{un=1,uniquepart=given,hash=750f950ac7e7c6eb92db4793d9c16a0d}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Hanwen},
           giveni={H\bibinitperiod},
           givenun=1}}%
        {{un=2,uniquepart=given,hash=197d71332633e07aa578718628fdc405}{%
           family={Perdikaris},
           familyi={P\bibinitperiod},
           given={Paris},
           giveni={P\bibinitperiod},
           givenun=2}}%
      }
      \strng{namehash}{5877c4af378c998dc46786641aefe024}
      \strng{fullhash}{5877c4af378c998dc46786641aefe024}
      \strng{bibnamehash}{5877c4af378c998dc46786641aefe024}
      \strng{authorbibnamehash}{5877c4af378c998dc46786641aefe024}
      \strng{authornamehash}{5877c4af378c998dc46786641aefe024}
      \strng{authorfullhash}{5877c4af378c998dc46786641aefe024}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Physics-informed neural networks (PINNs) are demonstrating remarkable promise in integrating physical models with gappy and noisy observational data, but they still struggle in cases where the target functions to be approximated exhibit high-frequency or multi-scale features. In this work we investigate this limitation through the lens of Neural Tangent Kernel (NTK) theory and elucidate how PINNs are biased towards learning functions along the dominant eigen-directions of their limiting NTK. Using this observation, we construct novel architectures that employ spatio-temporal and multi-scale random Fourier features, and justify how such coordinate embedding layers can lead to robust and accurate PINN models. Numerical examples are presented for several challenging cases where conventional PINN models fail, including wave propagation and reaction-diffusion dynamics, illustrating how the proposed methods can be used to effectively tackle both forward and inverse problems involving partial differential equations with multi-scale behavior. All code an data accompanying this manuscript will be made publicly available at {\textbackslash}url\{https://github.com/PredictiveIntelligenceLab/MultiscalePINNs\}.}
      \field{journaltitle}{arXiv:2012.10047 [cs, stat]}
      \field{month}{12}
      \field{note}{arXiv: 2012.10047}
      \field{shorttitle}{On the eigenvector bias of {Fourier} feature networks}
      \field{title}{On the eigenvector bias of {Fourier} feature networks: {From} regression to solving multi-scale {PDEs} with physics-informed neural networks}
      \field{urlday}{23}
      \field{urlmonth}{4}
      \field{urlyear}{2021}
      \field{year}{2020}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://arxiv.org/abs/2012.10047
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2012.10047
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{wu_group_2018}{misc}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=b626dcc418c3a5c74cfaa4c5e643a71f}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Yuxin},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6b4b60e909e78633945f3f9c9dc83e01}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Kaiming},
           giveni={K\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{cdf9f82b747ba630cd44730e2b9982af}
      \strng{fullhash}{cdf9f82b747ba630cd44730e2b9982af}
      \strng{bibnamehash}{cdf9f82b747ba630cd44730e2b9982af}
      \strng{authorbibnamehash}{cdf9f82b747ba630cd44730e2b9982af}
      \strng{authornamehash}{cdf9f82b747ba630cd44730e2b9982af}
      \strng{authorfullhash}{cdf9f82b747ba630cd44730e2b9982af}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems --- BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6\% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.}
      \field{month}{6}
      \field{note}{arXiv:1803.08494 [cs]}
      \field{title}{Group {Normalization}}
      \field{urlday}{5}
      \field{urlmonth}{4}
      \field{urlyear}{2023}
      \field{year}{2018}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://arxiv.org/abs/1803.08494
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1803.08494
      \endverb
      \keyw{Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
    \endentry
    \entry{yousif_physics-guided_2022}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=1eff01d0b478064e7bd65f00e061c826}{%
           family={Yousif},
           familyi={Y\bibinitperiod},
           given={Mustafa\bibnamedelima Z.},
           giveni={M\bibinitperiod\bibinitdelim Z\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=911a1d901396a399717bab61070c297c}{%
           family={Yu},
           familyi={Y\bibinitperiod},
           given={Linqi},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=64ece4155b889c15f3eaafe44251f1d9}{%
           family={Lim},
           familyi={L\bibinitperiod},
           given={HeeChang},
           giveni={H\bibinitperiod},
           givenun=0}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{e328630047226b98b27339167ec14cef}
      \strng{fullhash}{e328630047226b98b27339167ec14cef}
      \strng{bibnamehash}{e328630047226b98b27339167ec14cef}
      \strng{authorbibnamehash}{e328630047226b98b27339167ec14cef}
      \strng{authornamehash}{e328630047226b98b27339167ec14cef}
      \strng{authorfullhash}{e328630047226b98b27339167ec14cef}
      \field{sortinit}{Y}
      \field{sortinithash}{fd67ad5a9ef0f7456bdd9aab10fe1495}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{year}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{, In this paper, we propose an efficient method for generating turbulent inflow conditions based on deep neural networks. We utilise the combination of a multiscale convolutional auto-encoder with a subpixel convolution layer (MSCSPMSCSP\{{\textbackslash}rm MSC\}\_\{{\textbackslash}rm \{SP\}\}-AE) and a long short-term memory (LSTM) model. Physical constraints represented by the flow gradient, Reynolds stress tensor and spectral content of the flow are embedded in the loss function of the MSCSPMSCSP\{{\textbackslash}rm MSC\}\_\{{\textbackslash}rm \{SP\}\}-AE to enable the model to generate realistic turbulent inflow conditions with accurate statistics and spectra, as compared with the ground truth data. Direct numerical simulation (DNS) data of turbulent channel flow at two friction Reynolds numbers Reτ=180Reτ=180Re\_\{{\textbackslash}tau \} = 180 and 550 are used to assess the performance of the model obtained from the combination of the MSCSPMSCSP\{{\textbackslash}rm MSC\}\_\{{\textbackslash}rm \{SP\}\}-AE and the LSTM model. The model exhibits a commendable ability to predict instantaneous flow fields with detailed fluctuations and produces turbulence statistics and spectral content similar to those obtained from the DNS. The effects of changing various salient components in the model are thoroughly investigated. Furthermore, the impact of performing transfer learning (TL) using different amounts of training data on the training process and the model performance is examined by using the weights of the model trained on data of the flow at Reτ=180Reτ=180Re\_\{{\textbackslash}tau \} = 180 to initialise the weights for training the model with data of the flow at Reτ=550Reτ=550Re\_\{{\textbackslash}tau \} = 550. The results show that by using only 25\% of the full training data, the time that is required for successful training can be reduced by a factor of approximately 80\% without affecting the performance of the model for the spanwise velocity, wall-normal velocity and pressure, and with an improvement of the model performance for the streamwise velocity. The results also indicate that using physics-guided deep-learning-based models can be efficient in terms of predicting the dynamics of turbulent flows with relatively low computational cost.}
      \field{issn}{0022-1120, 1469-7645}
      \field{journaltitle}{Journal of Fluid Mechanics}
      \field{month}{4}
      \field{note}{Publisher: Cambridge University Press}
      \field{title}{Physics-guided deep learning for generating turbulent inflow conditions}
      \field{urlday}{13}
      \field{urlmonth}{10}
      \field{urlyear}{2022}
      \field{volume}{936}
      \field{year}{2022}
      \field{urldateera}{ce}
      \field{pages}{A21}
      \range{pages}{-1}
      \verb{doi}
      \verb 10.1017/jfm.2022.61
      \endverb
      \verb{urlraw}
      \verb https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/physicsguided-deep-learning-for-generating-turbulent-inflow-conditions/5AC04D36A013FD8661CB8381860ADC8C
      \endverb
      \verb{url}
      \verb https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/physicsguided-deep-learning-for-generating-turbulent-inflow-conditions/5AC04D36A013FD8661CB8381860ADC8C
      \endverb
      \keyw{machine learning,turbulence simulation,turbulent boundary layers}
    \endentry
  \enddatalist
\endrefsection
\endinput

