\begin{thebibliography}{27}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{gpt2}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI Blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang,
  He, Thite, Nabeshima, et~al.]{gao2020pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
  Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al.
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock \emph{arXiv preprint arXiv:2101.00027}, 2020.

\bibitem[Kashyap et~al.(2022)Kashyap, Kashyap, et~al.]{kashyap2022gpt}
Rohan Kashyap, Vivek Kashyap, et~al.
\newblock Gpt-neo for commonsense reasoning-a theoretical and practical lens.
\newblock \emph{arXiv preprint arXiv:2211.15593}, 2022.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,
  Diab, Li, Lin, Mihaylov, Ott, Shleifer, Shuster, Simig, Koura, Sridhar, Wang,
  and Zettlemoyer]{Zhang2022OPTOP}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, Todor Mihaylov,
  Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit~Singh Koura, Anjali
  Sridhar, Tianlu Wang, and Luke Zettlemoyer.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{ArXiv}, abs/2205.01068, 2022.

\bibitem[Everett(2017)]{everett2017language}
Daniel Everett.
\newblock \emph{How language began: The story of humanity’s greatest
  invention}.
\newblock Profile Books, 2017.

\bibitem[Reiter(2007)]{Reiterdata2text}
Ehud Reiter.
\newblock An architecture for data-to-text systems.
\newblock In \emph{Proceedings of the Eleventh European Workshop on Natural
  Language Generation}, ENLG '07, pages 97--104, Stroudsburg, PA, USA, 2007.
  Association for Computational Linguistics.
\newblock URL \url{http://dl.acm.org/citation.cfm?id=1610163.1610180}.

\bibitem[Sripada et~al.(2003)Sripada, Reiter, and Davy]{sumtime}
Somayajulu Sripada, Ehud Reiter, and Ian Davy.
\newblock Sumtime-mousam: Configurable marine weather forecast generator.
\newblock \emph{Expert Update}, 6\penalty0 (3):\penalty0 4--10, 2003.

\bibitem[H{\"a}rm{\"a} and Helaoui(2016)]{harma2016probabilistic}
Aki H{\"a}rm{\"a} and Rim Helaoui.
\newblock Probabilistic scoring of validated insights for personal health
  services.
\newblock In \emph{2016 IEEE Symposium Series on Computational Intelligence
  (SSCI)}, pages 1--6. IEEE, 2016.

\bibitem[Susaiyah et~al.(2020)Susaiyah, H{\"a}rm{\"a}, Reiter, Helaoui,
  Petkovi{\'c}, et~al.]{susaiyah2020towards}
Allmin Susaiyah, Aki H{\"a}rm{\"a}, Ehud Reiter, Rim Helaoui, Milan
  Petkovi{\'c}, et~al.
\newblock Towards a generalised framework for behaviour insight mining.
\newblock In \emph{SmartPHIL: 1st Workshop on Smart Personal Health
  Interfaces}. ACM, 2020.

\bibitem[Chintagunta et~al.(2021)Chintagunta, Katariya, Amatriain, and
  Kannan]{chintagunta2021medically}
Bharath Chintagunta, Namit Katariya, Xavier Amatriain, and Anitha Kannan.
\newblock Medically aware gpt-3 as a data generator for medical dialogue
  summarization.
\newblock In \emph{Machine Learning for Healthcare Conference}, pages 354--372.
  PMLR, 2021.

\bibitem[Bakker et~al.(2022)Bakker, Chadwick, Sheahan, Tessler,
  Campbell-Gillingham, Balaguer, McAleese, Glaese, Aslanides, Botvinick,
  et~al.]{bakker2022fine}
Michiel~A Bakker, Martin~J Chadwick, Hannah~R Sheahan, Michael~Henry Tessler,
  Lucy Campbell-Gillingham, Jan Balaguer, Nat McAleese, Amelia Glaese, John
  Aslanides, Matthew~M Botvinick, et~al.
\newblock Fine-tuning language models to find agreement among humans with
  diverse preferences.
\newblock \emph{arXiv preprint arXiv:2211.15006}, 2022.

\bibitem[van Stegeren and My{\'s}liwiec(2021)]{van2021fine}
Judith van Stegeren and Jakub My{\'s}liwiec.
\newblock Fine-tuning gpt-2 on annotated rpg quests for npc dialogue
  generation.
\newblock In \emph{The 16th International Conference on the Foundations of
  Digital Games (FDG) 2021}, pages 1--8, 2021.

\bibitem[Lee and Hsiang(2020)]{lee2020patent}
Jieh-Sheng Lee and Jieh Hsiang.
\newblock Patent claim generation by fine-tuning openai gpt-2.
\newblock \emph{World Patent Information}, 62:\penalty0 101983, 2020.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards,
  Burda, Joseph, Brockman, et~al.]{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira
  Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
  Brockman, et~al.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}, 2021.

\bibitem[Bender et~al.(2021)Bender, Gebru, McMillan-Major, and
  Shmitchell]{bender2021dangers}
Emily~M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret
  Shmitchell.
\newblock On the dangers of stochastic parrots: Can language models be too big?
\newblock \emph{Proceedings of FAccT}, 2021.

\bibitem[Liang et~al.(2021)Liang, Wu, Morency, and
  Salakhutdinov]{DBLP:journals/corr/abs-2106-13219}
Paul~Pu Liang, Chiyu Wu, Louis{-}Philippe Morency, and Ruslan Salakhutdinov.
\newblock Towards understanding and mitigating social biases in language
  models.
\newblock \emph{CoRR}, abs/2106.13219, 2021.
\newblock URL \url{https://arxiv.org/abs/2106.13219}.

\bibitem[Dutta et~al.(2022)Dutta, Juneja, Das, and Chakraborty]{dutta2022can}
Subhabrata Dutta, Jeevesh Juneja, Dipankar Das, and Tanmoy Chakraborty.
\newblock Can unsupervised knowledge transfer from social discussions help
  argument mining?
\newblock \emph{arXiv preprint arXiv:2203.12881}, 2022.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
  Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes
  Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[Sheng et~al.(2020)Sheng, Chang, Natarajan, and Peng]{sheng2020towards}
Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng.
\newblock Towards controllable biases in language generation.
\newblock \emph{arXiv preprint arXiv:2005.00268}, 2020.

\bibitem[Sheng et~al.(2019)Sheng, Chang, Natarajan, and Peng]{sheng2019woman}
Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng.
\newblock The woman worked as a babysitter: On biases in language generation.
\newblock \emph{arXiv preprint arXiv:1909.01326}, 2019.

\bibitem[Loria(2020)]{loria2020textblob}
Steven Loria.
\newblock Textblob documentation.
\newblock \emph{Release 0.16}, \url{https://textblob.readthedocs.io}, 2020.

\bibitem[Oraby et~al.(2019)Oraby, Harrison, Ebrahimi, and
  Walker]{oraby2019curate}
Shereen Oraby, Vrindavan Harrison, Abteen Ebrahimi, and Marilyn Walker.
\newblock Curate and generate: A corpus and method for joint control of
  semantics and style in neural nlg.
\newblock \emph{arXiv preprint arXiv:1906.01334}, 2019.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness,
  Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska,
  et~al.]{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the national academy of sciences}, 114\penalty0
  (13):\penalty0 3521--3526, 2017.

\bibitem[Grootendorst(2020)]{grootendorst2020keybert}
Maarten Grootendorst.
\newblock Keybert: Minimal keyword extraction with bert., 2020.
\newblock URL \url{https://doi.org/10.5281/zenodo.4461265}.

\bibitem[Susaiyah et~al.(2021)Susaiyah, H{\"a}rm{\"a}, Reiter, and
  Petkovi{\'c}]{susaiyah2021neural}
Allmin Susaiyah, Aki H{\"a}rm{\"a}, Ehud Reiter, and Milan Petkovi{\'c}.
\newblock Neural scoring of logical inferences from data using feedback.
\newblock \emph{International Journal of Interactive Multimedia \& Artificial
  Intelligence}, 6\penalty0 (5), 2021.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu,
  Scao, Gugger, Drame, Lhoest, and Rush]{wolf-etal-2020-transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe
  Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
  Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
  and Alexander~M. Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pages 38--45, Online,
  October 2020. Association for Computational Linguistics.
\newblock URL \url{https://www.aclweb.org/anthology/2020.emnlp-demos.6}.

\end{thebibliography}
