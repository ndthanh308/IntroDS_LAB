\section{Related Work}
Algorithmic social media feed ranking has consequences not just for individual users, but also for society. Social media AIs shape people's beliefs \cite{brady2023overperception}, affect their mental well-being \cite{hancock2022psychological, kreski2021social}, and change their behaviors \cite{vannucci2020social}. These consequences accrue to the individual, of course, but also aggregate to the societal level, for example through their impact on democratic discourse. 

\subsection{Encoding Societal Values into Social Media AIs }

Trading off societal outcomes is a complex issue. Today, most recommender systems, including those that power social media feeds, center around individual user experiences. Recommendation algorithms rely on key metrics, or objectives, that determine how to score candidate items to filter, rank, and display to users~\cite{eckles_2022}. 
Most commonly, feed ranking systems focus on metrics of user engagement (e.g., clicks, views, comments, likes), which serve as proxies for user satisfaction and, ultimately, platform revenue~\cite{milli2021optimizing,ciampaglia2018algorithmic}. These algorithms can end up maximizing individual experience at the cost of societal values such as pro-democratic attitudes or partisan animosity. For instance, maximizing engagement can amplify anti-social behaviors such as online harassment \cite{are2020instagram, munn2020angry} or focus our attention on pro-attitudinal political content \cite{sunstein2001http,rowland2011filter}. 

Though engagement metrics can be misleading as proxies for user benefit~\cite{kleinberg2022challenge}, these metrics are amenable to measurement and modeling since behavioral traces from organic platform use are the most abundant form of data (in contrast to explicit signals like user surveys or human annotation)~\cite{stray2022building}.Some platforms periodically survey their users to gather high-level feedback, such as Facebook's survey that asked users to gauge whether posts were good or bad for the world~\cite{fb_goodfortheworld}, but survey feedback is much more limited in scale, so it is challenging to formulate algorithmic objectives around this feedback. 

To address these issues, social media platforms have explored efforts that incorporate some notions of societal values into ranking models. Content moderation models are a common strategy to ensure that content does not violate platform policies or community guidelines~\cite{gillespie2018custodians}, and such models are often developed with the help of crowdsourced data annotations~\cite{youtube_four_rs}. Beyond content moderation, platforms have also taken measures to combat the societal harms posed by content such as terrorism or extremism, misinformation, and violence with automated detection and removal~\cite{fb_enforcement, twitter_enforcement, youtube_enforcement}. On the modeling side, recent developments in AI value alignment, such as Constitutional AI~\cite{bai2022constitutional} and reinforcement learning from human feedback (RLHF)~\cite{ouyang2022training} present technical methods for holistically steering AI models to better align with human descriptions and demonstrations of desired behavior.
These steps are critical in handling acute, negative societal values and preserving general norms of communication. Our work seeks to extend these approaches to not only combat stark policy violations, but also to capture a wider range of societal values that goes beyond policy enforcement.

Meanwhile, researchers have also investigated interventions on social media feeds to counter various user harms that may relate to societal values. For example, Gobo addressed the lack of control over social media feeds by introducing a system that allowed users to aggregate and filter content across platforms~\cite{bhargava2019gobo}. In a related vein, the HabitLab system sought to grant users agency over social media usage by enacting productivity interventions~\cite{kovacs2018habitlab}. Other work has explored interventions that expose users to alternative feeds that may productively differ from those that they typically see: ``Blue Feed, Red Feed'' presented contrasting liberal and conservative Facebook feeds~\cite{bluefeed_redfeed}, and researchers have explored feed re-ranking interventions aimed at achieving ideological balance~\cite{celis2019controlling} or bridging-based ranking to build trust across divides~\cite{ovadya2023bridging}. We build on this prior work in presenting concrete implementation strategies to intervene on social media feeds. However, while prior approaches may not entirely capture or align with societal values, we address the challenge of bringing implementation and societal values in line.

Given the difficulty of quantifying societal values and the resulting scarcity of social media ranking models that optimize for such values, we need new research agendas that might address these issues. 
Recent work has started to map and articulate the space of human values that could be instantiated in social media.
For example,~\citet{stray2022building} provide a broad set of over 30 human values compiled from multidisciplinary experts, such as well-being, freedom of expression, and civic engagement. 
Prior work has introduced valuable process frameworks that outline how to bridge from human values to algorithmic systems~\cite{stray2022building, zhu2018valuesensitivealgorithm}, but it remains challenging to connect those values to implementation. 

Our work thus sets out to connect from societal values to social science constructs to concrete algorithmic implementations, and we demonstrate the promise of this approach with a societal value of democratic attitudes.

\subsection{Social Media Algorithms and Partisan Animosity}

In light of a growing body of literature suggesting harmful connections between social media algorithms and democracy, we focus on democratic attitudes in our work. Partisan animosity refers to tendency of partisans to hold negative views of opposing partisans, but positive views of co-partisans~\cite{iyengar2015polarization,iyengar2019origins}. 
Social scientists, practitioners, and activists have long been interested in reducing partisan animosity among Americans \cite{wojcieszak2020can, ahler2018parties}.
In the United States, this divide appears to be growing more extreme~\cite{boxell2021trends}, driving worry about undemocratic practices and existential risks to democracy \cite{kingzette2021affective}. 
Partisan animosity is often associated with affective polarization \cite{voelkel2023megastudy}; in this study, we focus on affective polarization instead of issue polarization, which reflects partisans' disagreement about a certain political issues (e.g., abortion, gun control) \cite{hartman2022interventions}. 
Prior work has operationalized partisan animosity as hostility and aversion to a political party (often the opposing party) and has measured this with ratings of warmth on a feeling thermometer (e.g., \cite{iyengar2019origins, voelkel2023megastudy}).

Since digital media introduces heavy personalization and amplifies messages at a vastly different speed and scale than prior forms of media, researchers have investigated potential links between algorithmic behavior, media consumption, and user polarization.  For example, there is evidence that the Twitter algorithm amplifies content from the political right~\cite{huszar2022algorithmic} and that the ranking algorithms amplify partisanship and out-group animosity, especially for political tweets~\cite{milli2023twitter}. Studies have similarly indicated that Facebook usage promotes political polarization~\cite{alcott2020welfare}.
However, there is disagreement about the mechanisms by which social media influences polarization. Earlier research hypothesized about the role of echo chambers (or selective exposure) in isolating communities and driving them towards more extreme and divergent positions \cite{sunstein2001http}. More recent evidence supports alternative mechanisms like partisan sorting, whereby polarization is not driven by isolation, but by repeated \textit{exposure} to individuals outside of local networks, which might cause local conflicts to align on global partisan lines~\cite{tornberg2022digital}.  

Thus, there are a variety of possible mechanisms by which algorithmic ranking on social media ultimately influences the political beliefs of users. While much of this prior work studies the impact of existing social media, our work aims to build on these prior findings to \textit{redesign} social media ranking algorithms and experimentally tie these design decisions to users’ political beliefs. Most prior work either uses observational data to investigate whether current social media AIs exacerbate partisan animosity \cite{milli2023twitter, tornberg2022digital} or examines the effect of bottom-up interventions on participants' anti-democratic attitudes such as partisan animosity \cite{voelkel2023megastudy}. To our knowledge, prior work has not taken a top-down approach to implement high-level democratic values into feed algorithms. Adding to prior literature, our work translates social science measures of anti-democratic attitudes directly into objective functions and examines effects on partisan animosity.

\subsection{Democratic Values As A Lever: A Sociotechnical Approach}
Given the potential harms that current engagement-based social media feeds may pose to democracy, our work uses democratic values as a lever to examine whether a feed that embeds democratic values could reduce partisan animosity. The key question then becomes: how do we operationalize democratic values? 
To address this question, we first adopt a measure of anti-democratic attitudes from political science research \cite{voelkel2023megastudy, hartman2022interventions}, where the construct has been previously vetted and tested. We utilize its eight sub-scales to label each social media post, producing a continuous rating of the extent to which each post potentially impacts anti-democratic attitudes, and then replicate the manual rating using an algorithmic approach. 

We do not claim that the construct of anti-democratic attitudes is the only construct that might matter---far from it---nor that its current operationalization is perfect. However, we find it far more productive to draw on social science expertise rather than re-invent the wheel. We chose to operationalize anti-democratic attitudes as an example due to its recent large-scale vetting in a large study by ~\citet{voelkel2023megastudy} that includes 25 interventions ($N$ = 32,059). In this study, anti-democratic attitudes are measured for US partisans using eight variables, namely \textit{partisan animosity}, \textit{support for undemocratic practices}, \textit{support for partisan violence}, \textit{support for undemocratic candidates}, \textit{opposition to bipartisanship}, \textit{social distrust}, \textit{social distance}, \textit{biased evaluation of politicized facts}. Related work by \citet{hartman2022interventions} defined \textit{partisan animosity} as negative thoughts, feelings or behaviors towards outgroup; they argue that partisan animosity is an umbrella term that synthesizes a variety of concepts such as affective polarization, interpersonal polarization, and political sectarianism \cite{hartman2022interventions}. The Voelkel et al. megastudy found that almost all of their interventions successfully reduced partisan animosity, and several interventions reduced support for undemocratic practices and partisan violence \cite{voelkel2023megastudy}. Other work in the field of political science also measured a subset of these eight outcome variables. For instance, \citet{hartman2022interventions} measured partisan animosity because they argue that the rising partisan animosity is associated with the decrease in support for democracy. \citet{druckman2023correcting} measured partisan animosity, support for undemocratic practices, and support for partisan violence through a survey experiment, and found that correcting misperceptions of out-partisans can decrease American legislators’ support for undemocratic practices and marginally significantly reduce their partisan animosity.  

We envision that future work can develop a larger set of these constructs to integrate into social media AIs and trade off amongst one another. One reason we choose anti-democratic attitudes as our construct of interest here is that it is relatively broad, measuring not just partisan animosity but also several other subscales, which aligns with realistic social media settings where there are multiple competing values to consider and trade off. The Voelkel et al. study was also conducted by a nationally-representative pool of social science researchers and studied a large, diverse sample of participants, so it represents a significant contribution to the literature on anti-democratic attitudes. However, our approach is not restricted to this particular paper and choice of construct, and the same approach could be used to translate other constructs to algorithmic objectives.