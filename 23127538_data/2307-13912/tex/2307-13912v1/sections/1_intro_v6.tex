\section{Introduction}

Can social media support a healthy democracy? Social media AIs such as feed ranking algorithms bear nontrivial responsibility in how people perceive contentious political issues \cite{huszar2022algorithmic}. Researchers have long been interested in the effects of social media AIs on partisan information consumption (e.g.,~\cite{bail2022breaking, guess2019accurate, robertson2023users, bakshy2015exposure}) because these AIs shape people's beliefs \cite{brady2023overperception}, affect their well-being \cite{hancock2022psychological, kreski2021social}, and change their behaviors \cite{vannucci2020social}.

A key outcome of interest in a healthy democracy is \textit{partisan animosity}~\cite{milli2023twitter, tornberg2022digital}: negative thoughts, feelings and behaviours towards a political out-group. In the United States, for example, over 80\% of members of both political parties express concern that the country is growing increasingly divided~\cite{pewanimosity}. High levels of partisan animosity undermine our collective will to address public issues~\cite{hartman2022interventions}. Unfortunately, mounting evidence suggests that social media is associated with increases in partisan animosity in established democracies~\cite{lorenz2023systematic, milli2023twitter, tornberg2022digital}. While this effect can be mitigated by users' own behavior and choices~\cite{robertson2023users,bakshy2015exposure}, many studies demonstrate that feed algorithms can amplify political content~\cite{huszar2022algorithmic} and decrease people's trust in democracy~\cite{lorenz2023systematic}.

Could we design social media AIs to more directly consider their impact on partisan animosity? Social media AIs, and specifically feed ranking algorithms, are centered around accurately predicting engagement signals such as likes and clicks~\cite{narayanan2023,huszar2022algorithmic,liu2010personalized}.
In contrast, partisan animosity yields no observable engagement behavior to train on, and it can even be anti-correlated with traditional engagement signals: maximizing engagement can amplify anti-social behaviors~\cite{are2020instagram, munn2020angry} and focus attention on pro-attitudinal political content~\cite{sunstein2001http,rowland2011filter}. Lacking a method for modeling partisan animosity directly in the algorithm, researchers and practitioners instead engage in mitigation strategies such as ideologically balanced feeds~\cite{celis2019controlling} and content moderation policies~\cite{gillespie2018custodians}. However, these mitigations are indirect: they are not \textit{designed} to treat a societal value (here, reducing partisan animosity) as a first-class algorithmic objective.

To address this gap, we demonstrate a method for integrating established and vetted social scientific constructs into objective functions, which we term \textit{societal objective functions}. Specifically, we observe that survey measurements and qualitative codebooks in the social and behavioral sciences have long needed to be precise to facilitate reliability, and that this precision enables their translation into prompts interpretable by large language models such as GPT-4. In this case, we draw on the political science literature and specifically its measurement of \textit{anti-democratic attitudes}. This measure, which was recently tested in a large study that received widespread attention~\cite{voelkel2023megastudy}, spans eight variables that describe willingness to engage in good faith in the democratic process: partisan animosity, support for undemocratic practices, support for partisan violence, support for undemocratic candidates, opposition to bipartisanship, social distrust, social distance, and biased evaluation of politicized facts. We adapt each of these eight variables into prompts for a large language model (LLM) and then combine them into a joint \textit{democratic attitude model} that agrees with manual human annotation on political social media posts. This democratic attitude model can be applied at scale to estimate the impact of every post in a social media feed on anti-democratic attitudes, enabling democratic attitudes to be integrated into a social media feed ranking algorithm. We summarize the steps of our societal objective function method in Figure~\ref{fig:method} and several of the key terms used throughout the paper in Table~\ref{tab:term_bank}.

% Figure environment removed

\input{figures/term_bank}

We integrated this democratic attitude model into a feed ranking algorithm and tested it across three studies to investigate its impact on partisan animosity as well as traditional platform engagement measures. In Study 1, we began with \textit{manual, human labels} for the construct to establish its behavioral effectiveness before translating it into an AI: we manually annotated a prepared feed of political social media content using the existing anti-democratic attitude scale (Krippendorff's $\alpha = .895$). We conduct a preregistered, between-subjects online experiment among U.S. partisans (Democrats or Republicans, $N$ = 1,380) to examine the effect of \textit{manual democratic attitude feeds} that use these manual labels for re-ranking, removal, and warning against a traditional \textit{engagement-based feed} or a \textit{chronological feed} on participants' partisan animosity, support for undemocratic practices, feed-level satisfaction metrics, and engagement, as shown in Figure~\ref{fig:condition}.

% Figure environment removed

We found that our democratic attitude feeds---specifically the \textit{downranking feed} and \textit{remove-and-replace feed}---significantly reduced partisan animosity compared to a traditional engagement-based feed. These results hold for both conservatives and liberals, and hold without compromising participants' engagement level or their ratings of their experience on the platform. Though freedom of speech is cited as a frequent concern for algorithmic feed ranking methods, we found that the democratic attitude feeds that re-rank or remove do not prompt perceived threats to freedom of speech. 

Next, we translated from manual annotation to an automated societal objective function. In Study 2, we used zero-shot prompting with a large language model (LLM) to translate the original anti-democratic attitude construct into a democratic attitude model via GPT-4. Testing the accuracy of the democratic attitude model, we observed that the democratic attitude model ratings correlated highly with those from human coders (Spearman's $\rho = .75$), indicating that the manual approach can be replicated at scale using LLMs. Then, in Study 3, we replicated Study 1, this time using the algorithmic democratic attitude model rather than the manual labels. The original results replicated; the algorithmic democratic attitude feed significantly reduced partisan animosity with an effect size similar to that of the manual democratic attitude feed. 

Our work introduces a novel method of translating social science theory to embed societal values in feeds via algorithmic objectives, which we term \textit{societal objective functions}. We accomplish this by operationalizing social science constructs into a manual codebook, using the codebook to manually re-rank feeds, and validating the effect that re-ranked feeds have on societal outcomes with online experiments (Study 1). Then, we scale up the codebook using zero-shot prompting with LLMs and show that algorithmic ranking can replicate both manual feed ranking (Study 2) and online experiment results (Study 3). We demonstrate the viability of our method with the specific construct of democratic attitudes, but our approach carries implications for how we might incorporate a much broader set of societal values into social media feeds. Given a new societal value of interest (e.g., wellbeing, cultural diversity, environmental sustainability), future work might leverage this method to identify relevant social science constructs and instantiate them into additional algorithmic objectives to embed societal values in feeds. Today's social media feeds lack an understanding of the impact they have on societal values like mitigating anti-democratic attitudes---our work validates a promising pathway to directly embed societal values into the objectives that drive social media AIs.