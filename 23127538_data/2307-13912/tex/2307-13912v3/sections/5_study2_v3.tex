\section{Study 2: Replicating Manual Annotations with a Large Language Model}

Next, we sought to explore whether we could use algorithmic methods to replicate the effects observed in Study 1. While we found that manual expert annotations served as an effective basis for downranking to mitigate partisan animosity in social media feeds, it is not feasible for manual annotations to serve real-world social media ranking systems. Thus, we turned to large language models (in particular, the GPT-4 model) to investigate how faithfully we could mirror expert annotations with automated approaches that could scale to re-rank social media feeds in production.

\subsection{Democratic Attitude Rating Using GPT-4}

Recent advances in large language models (LLMs) present an opportunity to carry out a broad range of classification tasks purely through natural language instructions. These ``zero-shot'' capabilities---the ability of models to perform tasks without requiring any additional training examples---grant a great deal of flexibility to communicate nuanced constructs~\cite{kojima2022zeroShot, ouyang2022rlhf}. While more traditional deep learning models require substantial technical expertise and effort to gather data and train a performant task-specific model, zero-shot prompting substantially lowers the barrier by only requiring a verbal descriptions of the task and its requirements.

We thus focus on understanding the performance of LLMs for democratic values ranking: if zero-shot modeling with LLMs could sufficiently replicate manual annotations, it would present a path forward to scale up the effect of manual downranking while maintaining accessibility for stakeholders to adjust the values embedded in a social media platform.

\subsubsection{Method}
We leverage GPT-4---at the time of writing, the state-of-the-art large language model developed by OpenAI~\cite{openai2023gpt4}---to explore the feasibility of automating our democratic attitude model. GPT-4 is a transformer-based model that is pretrained to predict the next token in a document and further fine-tuned to generate outputs aligned with human preferences. Owing to the massive scale of data and parameters with which LLMs have been trained, researchers have found that these models display ``in-context learning'' capabilities, whereby a model can carry out a task given only an input of natural language instructions and a few demonstrations of the task~\cite{brown2020fewShot}. Subsequently, instruction tuning methods like those used in GPT-4 have improved LLM capabilities to follow human instructions with methods like reinforcement learning from human feedback (RLHF)~\cite{ouyang2022rlhf}. These advancements have improved LLM performance on zero-shot learning, which omits demonstrations and relies purely on the model's understanding of natural language instructions. Earlier models like GPT-3 and GPT-3.5 have displayed promising performance in applying predefined qualitative codebooks or labeling policies to classify text data. For example, prior work has achieved moderate to substantial agreement with human labels in social science tasks such as political ideology detection, misinformation classification, question analysis, and implicit hate speech detection~\cite{xiao2023qualitative, ziems2023large, huang2023chatgptAnnotation, do2022augmentedSocialScientist}.

To develop our zero-shot prompts, we draw from the definitions provided by~\citet{voelkel2023megastudy} as well as the codebook that our own research team developed in Study 1 to perform manual annotation of the social media post inventory. We split this inventory into a development set (n=205) and a test set (n=200); we used only the development set to iterate on and refine our prompts and reserved the test set for evaluation after we arrived at our finalized set of prompts.


For each anti-democratic variable, we use a prompt of the following format to annotate a social media post:
\begin{lstlisting}[language=Markdown]
    [System Message]
    Please rate the following message's {variable name} from 1 to 3. 
    {variable name} is defined as {variable definition}. 
    Your rating should consider whether the following factors exist in the following message:
        
    {Variable factors: a list of factors relevant to the variable and how they map to the 3-point scale}
        
    After your rating, please provide reasoning in the following format: 
    Rating:__ ### Reason: __ (### is the separator)

    [User Message]
    {social media post content}
\end{lstlisting}
The model then returns an ``Assistant Message'' that we then parse. The variable factors differ for each anti-democratic variable and are listed in full in Appendix~\ref{appendix:prompts}. For a sample of the general format, the variable factors for the ``support for undemocratic practices'' variable are as follows:
\begin{lstlisting}[language=Markdown]
    A: Show support for undemocratic practices
    B1: Partisan name-calling
    B2: Emotion or exaggeration

    Rate 1 if doesn't satisfy any of the factors
    Rate 2 if doesn't satisfy A, but satisfies B1 or B2
    Rate 3 if satisfies A, B1 and B2
\end{lstlisting}


\subsubsection{Implementation details}
We use the OpenAI Python API for GPT-4 to perform our zero-shot modeling task. We use the \texttt{gpt-4-0314} chat completion model, which was the state-of-the-art version at the time of development, and we use a temperature setting of 0.7, consistent with other work that aims to emulate human annotations using LLMs~\cite{pangakis2023automated, hamalainen2023synthetic}.

\subsection{Results}
First, we compared the \textit{overall} ranking results (the total score ranging from 8 to 24) produced by manual ratings versus those produced by automated ratings. We evaluated on our held-out test set ($n = 200$) to understand the holistic impact of the two rating methods on social media feed ranking. 
To capture the overall ranking similarity, we considered two metrics. First, Spearman's rank correlation coefficient, Spearman's $\rho$, is a non-parametric measure of rank correlation that is used to assess the strength of and direction of a monotonic relationship between two variables. Meanwhile, Krippendorff's $\alpha$ is a measure of the agreement between pairs of ratings and is often used in content analysis to assess inter-coder agreement; while other agreement measures such as Cohen's $\kappa$ are designed for categorical variables, Krippendorff's $\alpha$ can be applied to ordinal variables like our anti-democratic attitude scores. We used both metrics to capture the similarity in the final feed ranking order as well as the agreement of the cumulative scores between GPT-4 and manual rating methods.

We observed that the automated ranking results produced by GPT-4 highly correlated with the manual ranking provided by human annotators (Spearman's $\rho$= .75, $p$ <.001, Krippendorff's $\alpha$ = .78). This result suggests that, at the omnibus level of the overall anti-democratic attitude construct, the prompts succeed in closely mirroring the manual labels. These results are consistent with that of prior work using LLMs, which have reported Spearman's $\rho$ values ranging from $.54$~\cite{yang2023large} to $.68$~\cite{kim2023aiaugmented}. We also repeated our procedure with GPT-3.5, the model version that preceded GPT-4, and find comparable performance results, as documented in Appendix~\ref{appendix:gpt-3.5}.

% Figure environment removed

\input{figures/table_gpt_allVars}

Then, diving into the \textit{individual} anti-democratic attitude variables, we observed substantial agreement between GPT-4 ratings and manual ratings, with a median Krippendorff's $\alpha$ of $.647$, as shown in Table~\ref{tab:gpt_allVars}. 
Treating the 3-point scale options as a multiclass classification task, we observed a mean classification accuracy of $.815$. 
Since the manual ratings for all variables were skewed towards the negative class (across variables, a mean of 79.2\% examples received a manual rating of 1), we also calculated the F1 score, which is a more reliable indicator of model performance for imbalanced datasets. We observed a mean F1 score of $.577$ across variables. 
The only two individual variables that displayed Krippendorff's $\alpha$ and F1 scores below $.5$ were \textit{support for undemocratic practices} and \textit{support for undemocratic candidates}. Based on manual ratings, both of these variables displayed extremely large skew towards the negative class such that 99\% of examples in the test set were manually annotated with a score of 1. This skew made it challenging to calibrate the model and resulted in lower performance results. However, the large skew on those variables meant that even in the manually-ranked feeds, these two variables most often had a value of 1 and contributed less to the overall democratic attitude score than the other variables that displayed greater variation. Thus, misalignments in the GPT-4 ratings for the highly skewed variables did not appear to substantially affect alignment with manual ratings for the overall feed ranking.
Thus, we concluded that our automated democratic values ranking method using GPT-4 appears to effectively replicate our manual ranking outcomes.

\subsubsection{Summary}
Adapting qualitative codebooks as zero-shot prompts, algorithmic feed ranking using GPT-4 achieved a strong correlation with the manual democratic attitude feed ranking. For individual anti-democratic attitudes, GPT-4 ratings generally aligned well with manual ratings except for two variables, \textit{support for undemocratic practices} and \textit{support for undemocratic candidates}, which were especially rare in the dataset.