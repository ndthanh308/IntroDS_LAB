\section{Discussion}
Our work seeks to directly embed democratic values into social media feed algorithms. We focused on political discourse on social media, and our results showed that encoding democratic values into social media feeds could reduce partisan animosity without compromising engagement and satisfaction with the feed. We identified a social science construct and its associated measures, used LLMs to replicate human annotations on those constructs, and redesigned social media feed ranking objectives to account for these LLM annotations. Our results shed light on the impact of feeds that encode societal values, which provides important theoretical and design implications for the field.

\subsection{Theoretical Implications}
The first theoretical contribution of our work is the societal objective function, which seeks to translate social science theory into algorithmic objectives that drive user-facing systems. This sociotechnical approach allows us to embed societal values in social media AIs by translating social science measures of anti-democratic attitudes directly into objective functions. Our work is one of the first studies that both 1)~identifies the success of translating measures from social science theory into prompts for LLMs such as GPT-4, and 2)~proves that introducing societal objective functions into user-facing systems can result in significant attitudinal changes (i.e., reducing partisan animosity) by conducting empirical experimental studies with users. We envision that this approach can apply to many other domains to translate measures developed by the social scientists in areas such as mental health, well-being, and social equity into different algorithmic objectives. This approach opens up a new area for social media AIs to test and understand how a feed embedding various values might affect users' attitudes and behaviors, along with other outcome variables that social media platforms may care about (engagement, user experience, etc).

Second, we took a top-down, value-centered approach to intervene on reducing partisan animosity, which broadens the scope of polarization literature. Prior work tends to take two forms: one body of literature investigates whether today's social media algorithms amplify partisan animosity~\cite{milli2023twitter, tornberg2022digital} or harm democracy~\cite{lorenz2023systematic}. Another line of work investigates the efficacy of bottom-up, ``light-touch'' interventions via websites or social media platforms, such as correcting out-partisan misperceptions through a chatbot~\cite{hartman2022interventions} or deploying interventions like readings, videos, or activities~\cite{voelkel2023megastudy}. However, prior work has not taken a top-down approach to investigate the impact of embedding high-level democratic values into social media AIs. 

Our third theoretical contribution is that our democratic attitude feed could significantly reduce partisan animosity among US partisans without compromising their feed engagement level. This outcome is, of course, dependent on further field and longitudinal study. However, such results are consistent with past work that indicates that time spent on feed was not in conflict with reducing polarization \cite{saveski2022perspective}. Prior studies have focused on comparisons between feeds ranked in chronological order and feeds ranked by engagement metrics. For instance, one of the recent studies on Facebook during the 2020 US presidential election has examined the effect of substituting the algorithmic feed with a reverse-chronologically-sorted feed \cite{guess2023social}. Extending prior work, our experiments examined seven different feeds that moved beyond chronological and engagement-based ranking to consider alternative feed designs oriented around democratic attitudes, such as a content warning feed that adds friction to viewing anti-democratic information. Furthermore, we see evidence that democratic attitude feeds are more effective in reducing partisan animosity among weak partisans than strong partisans, which suggests that democratic attitude feeds are more likely to depolarize people who are less extreme on the ideological spectrum.
 
In addition to the engagement metric, our work also examines people's perceived threat to freedom of speech and found that partisans exposed to the content warning feed perceived a significant higher level of threat to freedom of speech compared to partisans exposed to other feeds, which is consistent with prior work \cite{dillard2005nature, moyer2010explaining}. We found a backfire effect in the content warning feed: conservatives are more likely to increase partisan animosity in the content warning feed compared to the engagement feed, which suggests that while the content warning condition may nudge people on the decision made by the platform, it may also accidentally create the backfire effect which over-correct peopleâ€™s estimation of the percentage of anti-democratic information online.

\subsection{Design Implications}
In ``The Two Cultures'' \cite{snow1959two}, C\@.P\@. Snow famously worried that the sciences were developing a culture wholly separate from the human-centered endeavors. Toward bridging this divide, our work suggests that it is possible to transfer some social science constructs and replicate their effects using a technical system based solely on natural language instruction inputs. These findings carry implications for future technical approaches that might embed societal values in social media and sociotechnical systems more broadly.

\subsubsection{Incorporating societal values in social media}
In this work, we focus on encoding democratic attitudes in social media feeds because these values carry important ramifications for a healthy democracy. Today's social media \textit{already} embeds values. Though implicit, social media AIs hold a de facto position on how democratic attitudes are rendered in the feed, and they similarly hold a de facto position for any other societal value we may choose to study~\cite{bernstein2023embeddingJOTS}. Thus, we believe our societal objective functions approach should be used to experiment with a wide range of other societal values such as mental health, self-expression, diversity, and environmental sustainability~\cite{stray2022building}. Work in this vein would allow us to make important values both explicit and tuneable.
Furthermore, once a broader range of values have been explored and tested with users, we might start to explore feeds that combine different sets of values and better understand how the values trade off against each other and against status quo metrics like engagement and revenue that tend to be more aligned with corporate interests. While we found that encoding democratic attitudes did not have a negative impact on engagement metrics, as we expand the types of values and the combinations of values that we attempt to encode, we likely will encounter tougher tradeoffs between societal values and financial feasibility for platforms. Field experiments will be critical to better understand and navigate these tradeoffs. 
There are also opportunities to intervene in different parts of the social media experience. While we intervened on feed ranking algorithms, societal objective functions might be valuable within post authoring interactions to encourage posters to self-reflect on the values expressed in their posts, or in notification systems to selectively notify users about content that upholds important values.
Finally, it would be valuable to explore the impact of feeds that encode societal values both over longer stretches of time with longitudinal studies and across a diverse set of communities and platforms with expanded field deployments. A benefit of our LLM-based method is that it can scale up to large amounts of content and users, and it can be flexibly adapted to perform ranking for a range of potential platforms.
 
Building on prior work on Value-Sensitive Algorithm Design~\cite{zhu2018valuesensitivealgorithm} which outlined methods for incorporating stakeholder values in a bottom-up manner, our work may also apply to non-social media contexts. Any task that requires content to be classified or scored, for example news recommendation, ad targeting, or toxicity detection, may benefit from integrating societal values. Thus, our approach could potentially help to streamline the workflow of transferring social science findings from more focused, in-lab studies to large-scale deployments in real-world systems. Additionally, since our approach does not require substantial technical expertise to implement and train a custom model, it may help to expand the set of researchers who can test alternative algorithms and interfaces. 

\subsubsection{Technical method: extensions and limitations}
The algorithmic approach taken in our work leverages a large language model to directly generate ratings. Past work has explored the use of LLMs to perform deductive coding that applies existing qualitative codebooks or labeling policies~\cite{xiao2023qualitative, ziems2023large, huang2023chatgptAnnotation, wang2021reducelabeling}. We extend this work to design a codebook for an LLM to provide ratings for a complex concept of anti-democratic attitudes. Furthermore, a key contribution of our work is our societal objective functions method, which not only evaluates the LLM ratings themselves (with comparisons to human ratings), but also investigates the \textit{impact} of the LLM ratings when used to power a user-facing system in an online experiment.
However, future approaches could utilize LLMs in a variety of other ways to achieve more efficient or fine-tuned results. For example, the LLM output could be used to perform knowledge distillation to train a smaller model that may be far less costly or time-intensive to use for inference to serve production systems~\cite{hsieh2023distilling, tang2019distilling}. Alternatively, the LLM-based method could be used to first validate the social science constructs, and experimenters could proceed to build custom models for each construct using more traditional data-driven approaches. 

While the LLM-based method substantially lowers the barrier to entry, there are risks of using such a model for production social computing systems. First, LLM ratings may not always align with desired manual annotations and human judgment, especially if the model is applied to subjective concepts where the human population may disagree~\cite{santurkar2023opinions}. Thus, it is important that model-generated ratings are closely monitored for any real-world deployments or field studies. Large language models may not achieve as high performance and annotator alignment for tasks that require domain-specific knowledge that falls outside of the training data of internet text, and their alignment with current societal values may shift over time. Furthermore, these models encode known biases~\cite{li2021gender, nadeem2021stereoset, sheng2019nlg_bias}, and they may have a variety of biases that are not yet understood that may impact performance. For example, if an LLM displays poor understanding of the sentiment of language used by a particular community, it may struggle to recognize anti-democratic speech from that community. Another risk of directly applying LLM-based ranking models in production systems is that such models could be attacked with adversarial inputs that take advantage of known limitations such as prompt injection~\cite{ignore_previous_prompt} and vulnerabilities similar to those of traditional computer programs~\cite{kang2023exploiting}, so system developers must include safeguards against such practices. 

\subsubsection{Implications for industry practitioners}

Why should social media companies redesign their algorithms? If status quo ranking algorithms are serving these companies well, there may be little incentive to alter their algorithms to incorporate societal values~\cite{ciampaglia2018algorithmic, milli2021optimizing}. While engagement-oriented objectives speak to shorter-term user satisfaction and retention, societal objective functions speak to longer-term impacts on users and society at large. Thus, our method may enable platforms to better evaluate how the design decisions of today impact the viability of their platform far into the future. Furthermore, our work presents initial evidence that societal objective functions may \textit{not} require compromises on the engagement metrics that social media platforms already care about.

If industry practitioners take on our approach, we recommend that they test the impact of manually-annotated social media feeds on real users before implementing any large-scale, algorithmic approaches, as we demonstrated in our work. This gradual, phased approach can help to reduce risks of harm to a platform and its users. Since our study was conducted on a much smaller group of participants than would be exposed on a real social media platform, unforeseen effects may result from large-scale deployments without such precautions.

Finally, given that societal objective functions carry tremendous ethical implications on users and society, we recommend that industry practitioners continuously seek feedback from ethicists, researchers, policymakers, and community stakeholders to better understand the risks and benefits they pose. 

\subsection{Limitations}
We note several limitations of our work that may serve as important directions for future work. First, the participant pool used for our online experiments is limited to partisans based in the United States, so results may vary for U.S. non-partisans and participants based in other countries. In addition, these participants were sourced from a crowdsourcing platform (CloudResearch Connect), so they may not comprise a nationally representative sample of the pool of U.S. partisans, as workers CloudResearch may have relatively higher digital literacy than the general population \cite{yaqub2020effects, jia2022understanding}.

Next, our data source was restricted to only political content sourced from CrowdTangle, which draws from public posts on Facebook. Since political content comprises a relatively small portion of social media feeds (\citet{scharkow2017measurement} and \citet{guess2019accurate} suggest 6\% political tweets and 2\% political Facebook posts), our democratic attitude model may have a subtler effect on real-world feeds that hold a proportionally smaller inventory of political content. Results may also differ for feed posts sourced from different platforms---such as Twitter, Reddit, or Instagram---or for posts authored for a private rather than public audience. In addition, our Study 3 results indicate an overall decrease in partisan animosity across parties and conditions when we replicated Study 1 three months after it had been conducted. We suggest that future work use timely social media posts as stimuli for studies on social media AIs, which are generally time-sensitive.

While we observed that downranking and remove-and-replace democratic attitude feeds reduced partisan animosity, we note that these reductions are comparable to the reductions achieved by a non-algorithmically ranked, reverse-chronological feed. These results suggest that status quo engagement-based ranking may pose the greatest risks to democratic attitudes and that platforms may consider chronological ordering if they turn to non-algorithmic ranking. However, given that platforms may prefer curated ranking approaches in contrast to idiosyncratic chronological ordering~\cite{twitter_reverse_chron}, democratic attitude feeds present a means to mitigate partisan animosity while maintaining curatorial control.

As noted earlier, while our online experiments demonstrated promising results by which democratic attitude feeds could reduce partisan animosity, further experiments are necessary to determine whether such feed interventions could bring about longitudinal effects. We recommend that future work carry out large-scale, longitudinal field experiments that intervene on users' own social media feeds to understand the impact of democratic attitude feeds in the real world.

Finally, our work focused on the societal value of mitigating partisan animosity, but we have not explored other potential societal values. There may be differing challenges in translating other social scientific constructs into societal objective functions, which may require different technical methods and may result in different effects on users in online experiments. An exciting direction of future work will be to explore a range of other societal values to investigate whether and how they can be translated into societal objective functions.


\subsection{Ethics and Societal Impact}
Given the substantial influence that social media platforms hold on the diffusion of information, societal attitudes, and the functioning of democratic processes, we must consider the ethical implications of our work.
First, any form of algorithmic feed ranking inherently poses risks to freedom of expression.
In a democracy, all citizens are allowed the right to express their perspectives and opinions equally \cite{bernholz2021democratictheory}. A social media feed that algorithmically curates social media posts may not capture the full spectrum of perspectives on the platform and, by design, must limit the visibility of certain posts. While our work aims to explicitly foreground societal values that might enhance the diversity of information on social media feeds, our ranking method shares limitations with existing algorithmic ranking methods in that no social media platform can show all posts \cite{sep-freedom-speech}. 

Another risk of our approach is that it may lack nuance on how to handle posts that may appear misaligned with societal values. For example, while partisan animosity in large doses may exacerbate political divides, anger and other negative emotions may be appropriate in the face of injustice; protest and debate is a critical part of political discourse. To avoid this failure mode, societal objective functions must take care to achieve balance among multiple values rather than solely focusing on any one individual value.

Next, our work provides a method to encode societal values in algorithmic objectives, but our method alone does not determine \textit{what values} are encoded or \textit{who decides} what values to encode. Thus, there are risks that the same methods that allow us to encode pro-social societal values could be exploited by bad actors to proliferate harmful content or manipulate ranking algorithms~\cite{torres2022manipulating}. 
We note that today's social media platforms already implicitly or explicitly encode certain societal values that reflect the values of their developers~\cite{seaver2017algorithms}, even if they were not intentionally specified. By making societal values an explicit, measurable goal, our approach provides to good-faith actors both a more realistic understanding of the values encoded in their status quo systems and a means to monitor improvements or adversarial attacks to those values. While we cannot prevent malicious actors from attempting to generate societally harmful content with similar methods, approaches like ours can help platforms to enhance their ``defense'' in detecting and downranking harmful content. Another potential concern is that societal objective functions may have uneven impacts across diverse communities, which may complicate decisions about what values to implement.
Extensive research has demonstrated that social media platforms often contribute to and amplify social inequities~\cite{robinson2015digitalinequities, hargittai2013oxford, lutz2022inequalites}, and even attempts to promote algorithmic fairness may not in fact address the needs of marginalized communities~\cite{gorwa2020algomoderation, hoffmann2019fairness, binns2017fairness}. Continued research on societal objective functions should incorporate the input of marginalized communities and carefully investigate the impacts on these communities to avoid perpetuating harmful disparities.


