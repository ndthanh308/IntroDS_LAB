\section{Performance Evaluation}
\label{sec:experiment}
We implemented a prototype of \sys~on top of Cheetah~\cite{huang2022cheetah} and evaluated its performance with different datasets. 
In this section, we present our experimental results.
% We start with a description of the experimental setup and evaluation datasets.

\noindent \textbf{Experimental Setup.} 
The experiment runs on a laptop running Centos 7.9 equipped with Xeon(R) Gold 6240 2.6GHZ CPU with 32~GB RAM. 
%The experiment is simulated with the CS and the verifier running on the same machine.  
The network setting is LAN with RTT 0.1~ms and bandwidth 1~Gbps.
We run all the experiments in a single-threaded environment.
We set the BFV parameter $N$ as 4096, $t$ as 20 bits, and $q$ as $60+49$ bits. The security level $\lambda$ is set as 128 bits. 
We also evaluated the performance of the existing works~\cite{boddeti2018secure} and \cite{engelsma2022hers} in the same environment with the same values for parameters. We compared their results with \sys. The time we report is averaged over ten trials. 

% To see the performance of our protocol under different network settings, we use \textsf{tc} tool to simulate two different bandwidth and round-trip latency including local-area network~(LAN, RTT: 0.1~ms, 1~Gbps) and wide-area network~(WAN, RTT: 80~ms, 40~Mbps).

\noindent \textbf{Datasets.} 
Similar to~\cite{boddeti2018secure} and~\cite{engelsma2022hers}, we evaluate the performance of CryptoMask with datasets that have different numbers of face images and dimensions. %To have a fair comparison, we also re-run these two existing works~\cite{boddeti2018secure} and~\cite{engelsma2022hers}. 
To show how the accuracy is influenced by precision scaling, as done in~\cite{boddeti2018secure}, we use a real dataset LFW~\cite{huang2008labeled} for the evaluation, which can be obtained from~\cite{LFWTech}. 
Specifically, LFW consists of 13,233 face images of 5,749 subjects. 
As done in~\cite{boddeti2018secure} and~\cite{engelsma2022hers}, We utilize the state-of-the-art face representation FaceNet~\cite{schroff2015facenet} to extract face vectors.

% We also evaluated the performance of the existing works~\cite{boddeti2018secure} and \cite{engelsma2022hers} in the same environment with the same dataset and compared their results with \sys. 
% We use the same values for parameters $N$ and $t$ for the two works, 
% and \refscui{$q$ is set to $36+36+36$, the value used in their paper.}{Does the value of $q$ affect the security and performance? Explain Why we cannot set the same value for $q$ for the 3 works.}. 

% The time we report is averaged over ten trials. 

% Figure environment removed


% Figure environment removed
%put into appendix
% \begin{table}[]
% \scriptsize
% \centering
% \caption{F{\upshape ace recognition accuracy for LWF dataset} (TAR @ FAR in $\%$)}\label{Table::Accuracy}
% \begin{tabular}{c|ccc}
% \hline
% \multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c|}{128-D FaceNet (Accuracy)}                    \\ \cline{2-4} 
%                         & \multicolumn{1}{c|}{0.01\%} & \multicolumn{1}{c|}{0.1\%} & 1\%   \\ \hline
% No FHE                  & \multicolumn{1}{c|}{98.70}  & \multicolumn{1}{c|}{98.70} & 98.70 \\ \hline
% FHE($1.0\times10^{-4}$)          & \multicolumn{1}{c|}{98.70}  & \multicolumn{1}{c|}{98.70} & 98.70 \\ \hline
% FHE($2.5\times10^{-3}$)         & \multicolumn{1}{c|}{98.70}  & \multicolumn{1}{c|}{98.70} & 98.70 \\ \hline
% FHE($1.0\times10^{-2}$)          & \multicolumn{1}{c|}{98.76}  & \multicolumn{1}{c|}{98.76} & 98.76 \\ \hline
% FHE($1.0\times10^{-1}$)          & \multicolumn{1}{c|}{98.50}  & \multicolumn{1}{c|}{98.50} & 98.50 \\ \hline
% \end{tabular}
% \end{table}




% \begin{table*}[tb]
% \centering
% \begin{tabular}{|c|c|ccc|ccc|}
% \hline
% \multirow{2}{*}{Dataset} & \multirow{2}{*}{Method} & \multicolumn{3}{c|}{128-D FaceNet (Validation Rate)}             & \multicolumn{3}{c|}{128-D FaceNet (Accuracy)}                    \\ \cline{3-8} 
%                          &                         & \multicolumn{1}{c|}{0.01\%} & \multicolumn{1}{c|}{0.1\%} & 1\%   & \multicolumn{1}{c|}{0.01\%} & \multicolumn{1}{c|}{0.1\%} & 1\%   \\ \hline
% \multirow{5}{*}{LWF}     & No FHE                  & \multicolumn{1}{c|}{67.57}  & \multicolumn{1}{c|}{93.50} & 98.57 & \multicolumn{1}{c|}{98.70}  & \multicolumn{1}{c|}{98.70} & 98.70 \\ \cline{2-8} 
%                          & FHE ($1.0\times10^{-4}$)          & \multicolumn{1}{c|}{67.57}  & \multicolumn{1}{c|}{93.50} & 98.57 & \multicolumn{1}{c|}{98.70}  & \multicolumn{1}{c|}{98.70} & 98.70 \\ \cline{2-8} 
%                          & FHE ($2.5\times10^{-3}$)         & \multicolumn{1}{c|}{67.23}  & \multicolumn{1}{c|}{93.52} & 98.53 & \multicolumn{1}{c|}{98.70}  & \multicolumn{1}{c|}{98.70} & 98.70 \\ \cline{2-8} 
%                          & FHE ($1.0\times10^{-2}$)          & \multicolumn{1}{c|}{67.73}  & \multicolumn{1}{c|}{93.50} & 98.53 & \multicolumn{1}{c|}{98.76}  & \multicolumn{1}{c|}{98.76} & 98.76 \\ \cline{2-8} 
%                          & FHE ($1.0\times10^{-1}$)          & \multicolumn{1}{c|}{77.10}  & \multicolumn{1}{c|}{90.57} & 97.90 & \multicolumn{1}{c|}{98.50}  & \multicolumn{1}{c|}{98.50} & 98.50 \\ \hline
% \end{tabular}
% \caption{F{\upshape ace Recognition Accuracy for LWF Dataset} (TAR @ FAR in $\%$)}\label{Table::Accuracy}
% \end{table*}

% %stacked and grouped bar 
% % Figure environment removed




\subsection{Efficiency}
Following the same dataset construction from~\cite{engelsma2022hers}, we evaluate \sys~on four representations at different dimensions (32-D, 64-D, 128-D, and 512-D).
Fig.~\ref{Fig::Total Computation} and Fig.~\ref{Fig::Total Communication} separately report the concrete computation and communication overhead with dataset sizes varying from 1 to 100 million. In the following, for simplicity, we use SFM to name the work in~\cite{boddeti2018secure} and use HERS to name the work in~\cite{engelsma2022hers}.

\noindent \textbf{Computation overhead.}
We report two computation overhead lines of CryptoMask in Fig.~\ref{Fig::Total Computation} where CryptoMask-W denotes we fully implement CryptoMask while CryptoMask-WO represents the version without the secure result-revealing protocol. 
In particular, CryptoMask-WO, SFM, and HERS have comparable information leakage, where they all leak the computed similarity to the verifier. 
% The reason is these two existing works are set on top of a relaxed security which allows the verifier to learn all evaluated distances. 
% Instead, the secure result-revealing protocol ensures the verifier \textbf{cannot} learn anything beyond whether her face image exists in the database with a slight increase of computation cost.

From Fig.~\ref{Fig::Total Computation} we can see both CryptoMask-W and CryptoMask-WO outperform SFM in the four dimensions settings. 
% This is because SFM requires more computation resources for homomorphic multiplication than CryptoMask.
The reason is the primary computation overhead in secure face recognition is caused by the homomorphic multiplication, which is $m$ times in~\cite{boddeti2018secure} while it is $\lceil \frac{m}{N-d}\rceil d $ times in CryptoMask. 
Compared with HERS, CryptoMask-WO shows the same tendency but enjoys less computation overhead. The main reason is we provide optimizations for computation. 
As for CryptoMask-W, the required computation overhead is near to HERS but achieves better security by concealing the similarity between face vectors from the verifier. 
CryptoMask is sensitive to the feature dimension, and the running time gap between SFM and CryptoMask-W drops with the increase of the dimension. For example, when working on 32-D, CryptoMask-W outperforms SFM by $283 \times$ against a gallery of 100 million. When working on 512-D, CryptoMask-W only saves around $132 \times$ computation than SFM, yet CryptoMask still shows its high efficiency for the large-scale dataset. Even when compared with similar work HERS, CryptoMask-W lies between CryptoMask-WO and HERS, indicating that it enjoys a better computation overhead while ensuring database security. 

\noindent \textbf{Communication.}
Fig.~\ref{Fig::Total Communication} details the communication consumption of CryptoMask-W, SFM and HERS. It shows that CryptoMask-W requires the least communication resource than the other two. The main reason comes from the given communication optimizations mentioned in Section~\ref{subsec::optimization}. 
% The reason for this is that we employ three optimizations which highly reduces the size of ciphertext in transfer. One optimization is from SEAL library~\footnote{\url{https://github.com/microsoft/SEAL}} which compresses the original ciphertext into its half size.~\notejbai{to check, give a brief technique about how to achieve it} It is notable that this ciphertext compression can only be used for data to be decrypted. This is because it will cause the decryption error if the data is computed over compressed ciphertext. Therefore, we can benefit from the compression technique while~\cite{engelsma2022hers} cannot because it requires to do rotation over encrypted data. It is clear that protocol~\cite{boddeti2018secure} suffers from linear communication complexity thus although it can benefit from above optimization but the improvement is very limited. Another ciphertext size reduction of our protocol is gained from Cheetah~\footnote{\url{https://github.com/Alibaba-Gemini-Lab/OpenCheetah}}. The observation is that the CS only needs to send high-end bits of two parts of ciphertext to the verifier. In this way, we saves around $16\%-25\%$ communication with a negligible decryption failing chance~(i.e., $< 2^{-38}$). We refer to Cheetah~\cite{huang2022cheetah} for more detailed analysis. Different from Cheetah~\cite{huang2022cheetah}, we do not need to perform RLWE to LWE extraction function which is a key design in their protocol. However, in our protocol, the verifier can extract the coefficients by herself after decrypting RLWE ciphertext. By doing this, $\lceil \frac{N}{d}-1\rceil(N+1)q$ required ciphertext size for $\lceil \frac{N}{d}-1\rceil$ using extraction function is reduced to $2Nq$. 


% \subsection{Accuracy}
% We report the results of face recognition on dataset LFW for state-of-the-art face representation FaceNet in Table~\ref{Table::Accuracy}. We only test face templates of 128-D. For more results on different representations, we refer to~\cite{boddeti2018secure}, which is also constructed on BFV. Same as~\cite{boddeti2018secure}, we report true acceptance rate~(TAR) at three different operating points of $0.01\%, 0.1\%$ and $1.0\%$ false accept rates~(FARs). 
% % Different from previous work only provides accuracy rate, we additionally set validation rate as another metric for a more accurate result. 
% We first report the performance of the unencrypted face images. 
% We treat these outputs as a baseline to compare. To evaluate encrypted face images, we consider four different quantization for each element in facial features. Specifically, we employ precision of 0.1, 0.01, 0.0025 and 0.0001. It shows that the performance of most given precision is competitive with the performance conducted from the raw data. We conclude that CryptoMask working over HE and MPC can perform as well as the one working over raw data.

% In contrast, CryptoMask provides protection for the database, the queried face image and the queried result.


% One may notice that the one with precision of 0.0025 offers a bit higher accuracy at $0.1\%$ FAR than original face features. It is reasonable since 



