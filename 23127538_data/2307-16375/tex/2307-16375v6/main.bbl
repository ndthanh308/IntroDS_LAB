\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown_language_2020}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language {Models} are {Few}-{Shot} {Learners}.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems} 33}, pages 1877--1901, 2020.

\bibitem[Cai et~al.(2022)Cai, Yan, Ma, Wu, Huang, Cheng, Su, and Yu]{cai_tensoropt_2022}
Zhenkun Cai, Xiao Yan, Kaihao Ma, Yidi Wu, Yuzhen Huang, James Cheng, Teng Su, and Fan Yu.
\newblock {TensorOpt}: {Exploring} the {Tradeoffs} in {Distributed} {DNN} {Training} {With} {Auto}-{Parallelism}.
\newblock \emph{IEEE Transactions on Parallel and Distributed Systems}, 33\penalty0 (8):\penalty0 1967--1981, 2022.

\bibitem[Chen et~al.(2016)Chen, Xu, Zhang, and Guestrin]{chen_training_2016}
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.
\newblock Training {Deep} {Nets} with {Sublinear} {Memory} {Cost}.
\newblock \emph{CoRR}, abs/1604.06174, 2016.

\bibitem[Chowdhery et~al.(2023)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez, Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury, Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski, Garcia, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov, Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira, Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat, Catasta, Wei, Meier-Hellstern, Eck, Dean, Petrov, and Fiedel]{chowdhery_palm_2023}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai, Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.
\newblock {PaLM}: {Scaling} {Language} {Modeling} with {Pathways}.
\newblock \emph{Journal of Machine Learning Research}, 24:\penalty0 240:1--240:113, 2023.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin_bert_2019}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}.
\newblock In \emph{The {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}}, pages 4171--4186, 2019.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby]{dosovitskiy_image_2021}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}.
\newblock In \emph{{International} {Conference} on {Learning} {Representations}}, 2021.

\bibitem[Du et~al.(2022)Du, Huang, Dai, Tong, Lepikhin, Xu, Krikun, Zhou, Yu, Firat, Zoph, Fedus, Bosma, Zhou, Wang, Wang, Webster, Pellat, Robinson, Meier-Hellstern, Duke, Dixon, Zhang, Le, Wu, Chen, and Cui]{du_glam_2022}
Nan Du, Yanping Huang, Andrew~M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams~Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten~P. Bosma, Zongwei Zhou, Tao Wang, Yu~Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen~S. Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc~V. Le, Yonghui Wu, Zhifeng Chen, and Claire Cui.
\newblock {GLaM}: {Efficient} {Scaling} of {Language} {Models} with {Mixture}-of-{Experts}.
\newblock In \emph{International {Conference} on {Machine} {Learning}}, pages 5547--5569, 2022.

\bibitem[{FairScale authors}(2021)]{fairscale_authors_fairscale_2021}
{FairScale authors}.
\newblock {FairScale}: {A} general purpose modular {PyTorch} library for high performance and large scale training.
\newblock \href{https://github.com/facebookresearch/fairscale}{https://github.com/facebookresearch/fairscale}, 2021.

\bibitem[Fan et~al.(2021)Fan, Rong, Meng, Cao, Wang, Zheng, Wu, Long, Yang, Xia, Diao, Liu, and Lin]{fan_dapple_2021}
Shiqing Fan, Yi Rong, Chen Meng, Zongyan Cao, Siyu Wang, Zhen Zheng, Chuan Wu, Guoping Long, Jun Yang, Lixue Xia, Lansong Diao, Xiaoyong Liu, and Wei Lin.
\newblock {DAPPLE}: {A} {Pipelined} {Data} {Parallel} {Approach} for {Training} {Large} {Models}.
\newblock In \emph{{Symposium} on {Principles} and {Practice} of {Parallel} {Programming}}, pages 431--445, 2021.

\bibitem[Fedus et~al.(2022)Fedus, Zoph, and Shazeer]{fedus_switch_2022}
William Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch {Transformers}: {Scaling} to {Trillion} {Parameter} {Models} with {Simple} and {Efficient} {Sparsity}.
\newblock \emph{The Journal of Machine Learning Research}, 23\penalty0 (120):\penalty0 5232–5270, 2022.

\bibitem[Flynn(1966)]{flynn_very_1966}
Michael~J. Flynn.
\newblock Very {High-speed} {Computing} {Systems}.
\newblock \emph{Proceedings of the IEEE}, 54\penalty0 (12):\penalty0 1901--1909, 1966.

\bibitem[Flynn(1972)]{flynn_computer_1972}
Michael~J. Flynn.
\newblock Some {Computer} {Organizations} and {Their} {Effectiveness}.
\newblock \emph{IEEE Transactions on Computers}, C-21\penalty0 (9):\penalty0 948--960, 1972.

\bibitem[{Gurobi Optimization, LLC}(2023)]{gurobi_optimization_llc_gurobi_2023}
{Gurobi Optimization, LLC}.
\newblock Gurobi {Optimizer} {Reference} {Manual}.
\newblock \href{https://www.gurobi.com}{https://www.gurobi.com}, 2023.

\bibitem[He et~al.(2021)He, Li, Soltanolkotabi, and Avestimehr]{he_pipetransformer_2021}
Chaoyang He, Shen Li, Mahdi Soltanolkotabi, and Salman Avestimehr.
\newblock {PipeTransformer}: {Automated} {Elastic} {Pipelining} for {Distributed} {Training} of {Large}-scale {Models}.
\newblock In \emph{{International} {Conference} on {Machine} {Learning}}, pages 4150--4159, 2021.

\bibitem[Huang et~al.(2019)Huang, Cheng, Bapna, Firat, Chen, Chen, Lee, Ngiam, Le, Wu, and Chen]{huang_gpipe_2019}
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia~Xu Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc~V. Le, Yonghui Wu, and Zhifeng Chen.
\newblock {GPipe}: {Efficient} {Training} of {Giant} {Neural} {Networks} using {Pipeline} {Parallelism}.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems} 32}, pages 103--112, 2019.

\bibitem[Jia et~al.(2018)Jia, Lin, Qi, and Aiken]{jia_exploring_2018}
Zhihao Jia, Sina Lin, Charles~R. Qi, and Alex Aiken.
\newblock Exploring {Hidden} {Dimensions} in {Parallelizing} {Convolutional} {Neural} {Networks}.
\newblock In \emph{{International} {Conference} on {Machine} {Learning}}, pages 2279--2288, 2018.

\bibitem[Jia et~al.(2019)Jia, Zaharia, and Aiken]{jia_beyond_2019}
Zhihao Jia, Matei Zaharia, and Alex Aiken.
\newblock Beyond {Data} and {Model} {Parallelism} for {Deep} {Neural} {Networks}.
\newblock In \emph{{Machine} {Learning} and {Systems}}, pages 1--13, 2019.

\bibitem[Kingma and Ba(2015)]{kingma_adam_2015}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} {Method} for {Stochastic} {Optimization}.
\newblock In \emph{{International} {Conference} on {Learning} {Representations}}, 2015.

\bibitem[Lazimy(1982)]{lazimy_mixed_1982}
Rafael Lazimy.
\newblock Mixed-integer {Quadratic} {Programming}.
\newblock \emph{Math. Program.}, 22\penalty0 (1):\penalty0 332–349, 1982.

\bibitem[Lepikhin et~al.(2021)Lepikhin, Lee, Xu, Chen, Firat, Huang, Krikun, Shazeer, and Chen]{lepikhin_gshard_2021}
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen.
\newblock {GShard}: {Scaling} {Giant} {Models} with {Conditional} {Computation} and {Automatic} {Sharding}.
\newblock In \emph{{International} {Conference} on {Learning} {Representations}}, 2021.

\bibitem[Li and Hoefler(2021)]{li_chimera_2021}
Shigang Li and Torsten Hoefler.
\newblock Chimera: {Efficiently} {Training} {Large}-{Scale} {Neural} {Networks} with {Bidirectional} {Pipelines}.
\newblock In \emph{International {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}}, pages 27:1--27:14, 2021.

\bibitem[Li et~al.(2020)Li, Zhao, Varma, Salpekar, Noordhuis, Li, Paszke, Smith, Vaughan, Damania, and Chintala]{li_pytorch_2020}
Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, and Soumith Chintala.
\newblock {PyTorch} {Distributed}: {Experiences} on {Accelerating} {Data} {Parallel} {Training}.
\newblock \emph{Proceedings of the VLDB Endowment}, 13\penalty0 (12):\penalty0 3005--3018, 2020.

\bibitem[Liu et~al.(2023)Liu, Li, Fang, Shao, Yao, and You]{liu_colossal-auto_2023}
Yuliang Liu, Shenggui Li, Jiarui Fang, Yanjun Shao, Boyuan Yao, and Yang You.
\newblock Colossal-{Auto}: {Unified} {Automation} of {Parallelization} and {Activation} {Checkpoint} for {Large}-scale {Models}.
\newblock \emph{CoRR}, abs/2302.02599, 2023.

\bibitem[Liu et~al.(2021)Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and Guo]{liu_swin_2021}
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
\newblock Swin {Transformer}: {Hierarchical} {Vision} {Transformer} using {Shifted} {Windows}.
\newblock In \emph{{International} {Conference} on {Computer} {Vision}}, pages 9992--10002, 2021.

\bibitem[Miao et~al.(2022)Miao, Wang, Jiang, Shi, Nie, Zhang, and Cui]{miao_galvatron_2022}
Xupeng Miao, Yujie Wang, Youhe Jiang, Chunan Shi, Xiaonan Nie, Hailin Zhang, and Bin Cui.
\newblock Galvatron: {Efficient} {Transformer} {Training} over {Multiple} {GPUs} {Using} {Automatic} {Parallelism}.
\newblock \emph{Proceedings of the VLDB Endowment}, 16\penalty0 (3):\penalty0 470--479, 2022.

\bibitem[Micikevicius et~al.(2018)Micikevicius, Narang, Alben, Diamos, Elsen, García, Ginsburg, Houston, Kuchaiev, Venkatesh, and Wu]{micikevicius_mixed_2018}
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory~F. Diamos, Erich Elsen, David García, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu.
\newblock Mixed {Precision} {Training}.
\newblock In \emph{{International} {Conference} on {Learning} {Representations}}, 2018.

\bibitem[Microsoft(2021)]{deepspeed-3d}
Microsoft.
\newblock {Megatron}-{DeepSpeed}.
\newblock \href{https://github.com/microsoft/Megatron-DeepSpeed/tree/main}{\url{https://github.com/microsoft/Megatron-DeepSpeed/tree/main}}, 2021.

\bibitem[Narayanan et~al.(2019)Narayanan, Harlap, Phanishayee, Seshadri, Devanur, Ganger, Gibbons, and Zaharia]{narayanan_pipedream_2019}
Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil~R. Devanur, Gregory~R. Ganger, Phillip~B. Gibbons, and Matei Zaharia.
\newblock {PipeDream}: {Generalized} {Pipeline} {Parallelism} for {DNN} {Training}.
\newblock In \emph{{Symposium} on {Operating} {Systems} {Principles}}, pages 1--15, 2019.

\bibitem[Narayanan et~al.(2021{\natexlab{a}})Narayanan, Phanishayee, Shi, Chen, and Zaharia]{narayanan_memory-efficient_2021}
Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia.
\newblock Memory-{Efficient} {Pipeline}-{Parallel} {DNN} {Training}.
\newblock In \emph{{International} {Conference} on {Machine} {Learning}}, pages 7937--7947, 2021{\natexlab{a}}.

\bibitem[Narayanan et~al.(2021{\natexlab{b}})Narayanan, Shoeybi, Casper, LeGresley, Patwary, Korthikanti, Vainbrand, Kashinkunti, Bernauer, Catanzaro, Phanishayee, and Zaharia]{narayanan_efficient_2021}
Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, and Matei Zaharia.
\newblock Efficient {Large-scale} {Language} {Model} {Training} on {GPU} {Clusters} {Using} {Megatron}-{LM}.
\newblock In \emph{International {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}}, pages 58:1--58:15, 2021{\natexlab{b}}.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel_exploring_2020}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0 (1):\penalty0 5485--5551, 2020.

\bibitem[Rajbhandari et~al.(2020)Rajbhandari, Rasley, Ruwase, and He]{rajbhandari_zero_2020}
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.
\newblock {ZeRO}: {Memory} {Optimizations} {Toward} {Training} {Trillion} {Parameter} {Models}.
\newblock In \emph{{International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}}, pages 20:1--20:16, 2020.

\bibitem[Rashidi et~al.(2021)Rashidi, Denton, Sridharan, Srinivasan, Suresh, Nie, and Krishna]{rashidi_enabling_2021}
Saeed Rashidi, Matthew Denton, Srinivas Sridharan, Sudarshan Srinivasan, Amoghavarsha Suresh, Jade Nie, and Tushar Krishna.
\newblock Enabling {Compute}-{Communication} {Overlap} in {Distributed} {Deep} {Learning} {Training} {Platforms}.
\newblock In \emph{{International} {Symposium} on {Computer} {Architecture}}, pages 540--553, 2021.

\bibitem[Rasley et~al.(2020)Rasley, Rajbhandari, Ruwase, and He]{rasley_deepspeed_2020}
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He.
\newblock {DeepSpeed}: {System} {Optimizations} {Enable} {Training} {Deep} {Learning} {Models} with {Over} 100 {Billion} {Parameters}.
\newblock In \emph{{The} 26th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}}, pages 3505--3506, 2020.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma, Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]{imagenet15russakovsky}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander~C. Berg, and Li Fei-Fei.
\newblock {ImageNet Large Scale Visual Recognition Challenge}.
\newblock \emph{International Journal of Computer Vision}, 115\penalty0 (3):\penalty0 211--252, 2015.

\bibitem[Schaarschmidt et~al.(2021)Schaarschmidt, Grewe, Vytiniotis, Paszke, Schmid, Norman, Molloy, Godwin, Rink, Nair, and Belov]{schaarschmidt_automap_2021}
Michael Schaarschmidt, Dominik Grewe, Dimitrios Vytiniotis, Adam Paszke, Georg~Stefan Schmid, Tamara Norman, James Molloy, Jonathan Godwin, Norman~Alexander Rink, Vinod Nair, and Dan Belov.
\newblock Automap: {Towards} {Ergonomic} {Automated} {Parallelism} for {ML} {Models}.
\newblock \emph{CoRR}, abs/2112.02958, 2021.

\bibitem[Shazeer et~al.(2018)Shazeer, Cheng, Parmar, Tran, Vaswani, Koanantakool, Hawkins, Lee, Hong, Young, Sepassi, and Hechtman]{shazeer_mesh-tensorflow_2018}
Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, and Blake~A. Hechtman.
\newblock Mesh-{TensorFlow}: {Deep} {Learning} for {Supercomputers}.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems} 31}, pages 10435--10444, 2018.

\bibitem[Tarnawski et~al.(2020)Tarnawski, Phanishayee, Devanur, Mahajan, and Paravecino]{tarnawski_efficient_2020}
Jakub Tarnawski, Amar Phanishayee, Nikhil~R. Devanur, Divya Mahajan, and Fanny~Nina Paravecino.
\newblock Efficient {Algorithms} for {Device} {Placement} of {DNN} {Graph} {Operators}.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems} 33}, pages 15451--15463, 2020.

\bibitem[Tarnawski et~al.(2021)Tarnawski, Narayanan, and Phanishayee]{tarnawski_piper_2021}
Jakub Tarnawski, Deepak Narayanan, and Amar Phanishayee.
\newblock Piper: {Multidimensional} {Planner} for {DNN} {Parallelization}.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems} 34}, pages 24829--24840, 2021.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozière, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{touvron_llama_2023}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
\newblock {LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}.
\newblock \emph{CoRR}, abs/2302.13971, 2023{\natexlab{a}}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Canton-Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom]{touvron_llama_2023-1}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
  Scialom.
\newblock Llama 2: {Open} {Foundation} and {Fine}-{Tuned} {Chat} {Models}.
\newblock \emph{CoRR}, abs/2307.09288, 2023{\natexlab{b}}.

\bibitem[Wang et~al.(2019)Wang, Huang, and Li]{wang_supporting_2019}
Minjie Wang, Chien-Chin Huang, and Jinyang Li.
\newblock Supporting {Very} {Large} {Models} using {Automatic} {Dataflow} {Graph} {Partitioning}.
\newblock In \emph{Proceedings of the {Fourteenth} {EuroSys} {Conference}}, pages 26:1--26:17, 2019.

\bibitem[{Wikimedia Foundation}(2023)]{wikidump}
{Wikimedia Foundation}.
\newblock Wikimedia {Downloads}.
\newblock \href{https://dumps.wikimedia.org}{\url{https://dumps.wikimedia.org}}, 2023.

\bibitem[Xu et~al.(2021)Xu, Lee, Chen, Hechtman, Huang, Joshi, Krikun, Lepikhin, Ly, Maggioni, Pang, Shazeer, Wang, Wang, Wu, and Chen]{xu_gspmd_2021}
Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake~A. Hechtman, Yanping Huang, Rahul Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, Ruoming Pang, Noam Shazeer, Shibo Wang, Tao Wang, Yonghui Wu, and Zhifeng Chen.
\newblock {GSPMD}: {General} and {Scalable} {Parallelization} for {ML} {Computation} {Graphs}.
\newblock \emph{CoRR}, abs/2105.04663, 2021.

\bibitem[Yuan et~al.(2022)Yuan, He, Davis, Zhang, Dao, Chen, Liang, Ré, and Zhang]{yuan_decentralized_2022}
Binhang Yuan, Yongjun He, Jared Davis, Tianyi Zhang, Tri Dao, Beidi Chen, Percy Liang, Christopher Ré, and Ce Zhang.
\newblock Decentralized {Training} of {Foundation} {Models} in {Heterogeneous} {Environments}.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems} 35}, pages 25464--25477, 2022.

\bibitem[Zhao et~al.(2022)Zhao, Li, Chen, Guan, Jiang, Huang, Qing, Wang, Wang, Zhang, Li, Luo, and Cui]{zhao_vpipe_2022}
Shixiong Zhao, Fanxin Li, Xusheng Chen, Xiuxian Guan, Jianyu Jiang, Dong Huang, Yuhao Qing, Sen Wang, Peng Wang, Gong Zhang, Cheng Li, Ping Luo, and Heming Cui.
\newblock {vPipe}: {A} {Virtualized} {Acceleration} {System} for {Achieving} {Efficient} and {Scalable} {Pipeline} {Parallel} {DNN} {Training}.
\newblock \emph{IEEE Transactions on Parallel and Distributed Systems}, 33\penalty0 (3):\penalty0 489--506, 2022.

\bibitem[Zheng et~al.(2022)Zheng, Li, Zhang, Zhuang, Chen, Huang, Wang, Xu, Zhuo, Xing, Gonzalez, and Stoica]{zheng_alpa_2022}
Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric~P. Xing, Joseph~E. Gonzalez, and Ion Stoica.
\newblock Alpa: {Automating} {Inter}- and {Intra}-{Operator} {Parallelism} for {Distributed} {Deep} {Learning}.
\newblock In \emph{{USENIX} {Symposium} on {Operating} {Systems} {Design} and {Implementation}}, pages 559--578, 2022.

\end{thebibliography}
