\appendix

\section{Proof of the linear form for the contiguous set}\label{appendix:linear-form-of-contigous-constraint}
To facilitate our discussion, we adopt the linear form of the contiguous constraint as presented in the main paper. We denote $P_{ui}$ as a 0-1 variable indicating whether layer $u$ is to be placed on the $i$-th computation stage, $ps$ as the number of computation stages in the pipeline. Besides, $G(V, E)$ represents the computation graph for the model. Then, we formalize the theorem as follows:

\begin{myTheo}
    A subgraph $V_i=\{\forall u\in V: P_{ui}=1\}$ is contiguous if and only if there exists $Z_{vi}$ such that constraints \eqref{eqn:contiguous-expand-1}, \eqref{eqn:contiguous-expand-2}, and \eqref{eqn:contiguous-expand-3} are satisfied.
\end{myTheo}

Previous work \citep{tarnawski_efficient_2020} has proven this theorem. Our proof draws on the process of this work. The details of the proof are as follows:

\begin{myProof}
    "If": Assume that there exists nodes $u, w\in V_i$ and $v \notin V_i$ such that $v$ and $w$ are reachable from $u$ and $v$, respectively. Hence, $P_{ui} = 1$, $P_{wi} = 1$, and $P_{vi} = 0$. Without losing generality, we assume $\langle u, v\rangle\in E$. Thus, according to constraint \eqref{eqn:contiguous-expand-3}, we have $Z_{vi}\leqslant P_{vi}-P_{ui}+1=0$. By applying constraint \eqref{eqn:contiguous-expand-2} repeatedly following the path from $v$ to $w$, we have $Z_{wi}\leqslant Z_{vi}$. Thus, $Z_{wi}\leqslant 0$. However, we also have $Z_{wi}\geqslant P_{wi}=1$ according to constraint \eqref{eqn:contiguous-expand-1}. A contradiction.

    "Only if": First, we define $Z_{vi}=1$ if a node $w\in S$ is reachable from $v$. Otherwise, $Z_{vi}=0$. Thus, constraints \eqref{eqn:contiguous-expand-1} and \eqref{eqn:contiguous-expand-2} are satisfied according to this kind of definition. For constraint \eqref{eqn:contiguous-expand-3}, if $P_{vi}=1$, the constraint will hold true regardless of whether $P_{ui}$ is $1$ or $0$. If $P_{vi}=0$ and $P_{ui}=0$, $Z_{vi}\leqslant P_{vi}-P_{ui}+1=1$ will also hold true because $Z_{vi}$ could be either $0$ or $1$. Finally, if $P_{vi}=0$ and $P_{ui}=1$, $Z_{vi}=0$ will hold true because $V_i$ is a contiguous set and we couldn't find any $w\in V_i$, such that $w$ is reachable from $v$.\qed
\end{myProof}

\section{QIP formulation for intra-layer-only parallelism}\label{appendix:miqp-for-intra-layer-parallelism}
Here we present the QIP formulation for intra-layer-only parallelism with explanations.
\begin{alignat}{2}
\min\quad& tpi=p_1, &{}&\tag{QIP}\label{eqn:appendix:lp-no-pp}\\
\mbox{s.t.}\quad
&\sum_{u\in V}S_{u}^\mathsf{T}A_{u}+\sum_{\langle u,v\rangle\in E}S_{u}^\mathsf{T}R_{uv}S_{v}=p_1,&\quad&\label{eqn:appendix:computation-for-no-pp}\\
&\sum_{u\in V}S_u^\mathsf{T} M_u\leqslant m,&\quad&\label{eqn:appendix:memory-constraint-for-no-pp}\\
&\sum_{k=1}^{\lvert g_u\rvert}S_{uk}=1,&\quad&\forall u\in V,\label{eqn:appendix:strategy-to-choose-for-no-pp}\\
&S_{uk}\in\{0,1\},&\quad&\forall u\in V, k\in\{1,\dots,\lvert g_u\rvert\}.\label{eqn:appendix:suk}
\end{alignat}

The objective function \eqref{eqn:appendix:lp-no-pp} tends to minimize the TPI, thereby maximizing training throughput. This function solely takes the value of $p_1$ into account, as there is only one computation stage involved in the intra-layer-only parallelism. Subsequently, we proceed to explain the constraints of this formulation:
\begin{itemize}
    \item Constraint \eqref{eqn:appendix:computation-for-no-pp} encodes the intra-layer-only computation and communication costs as $p_1$. The first summation term for any $u\in V$ represents the cost of choosing intra-layer strategies for all layers. The second term represents the summation of resharding costs on all edges.
    \item Constraint \eqref{eqn:appendix:memory-constraint-for-no-pp} encodes that the memory consumption on a single device should not exceed its device memory bound $m$. It is worth noting that $m$ should be an identical constant across multiple devices because these devices are homogeneous.
    \item Constraints \eqref{eqn:appendix:strategy-to-choose-for-no-pp} and \eqref{eqn:appendix:suk} indicate that each layer should select exactly one strategy.
\end{itemize}


\section{Experiment detail}\label{appendix:experiment-settings}
\textbf{Gurobi configuration}\quad When tackling the MIQP problem, UniAP employs several configurations for the Gurobi Optimizer~\citep{gurobi_optimization_llc_gurobi_2023}. In particular, we set \textit{TimeLimit} to 60 seconds, \textit{MIPFocus} to 1, \textit{NumericFocus} to 1, and remain other configurations to default. Furthermore, we have implemented an early stopping mechanism to terminate the optimization process as early as possible. There are two conditions that can activate the mechanism. Firstly, if the current runtime exceeds 15 seconds and the relative MIP optimality gap is less than 4\%, we will terminate the optimization. Secondly, if the current runtime exceeds 5 seconds and the best objective bound is worse than the optimal solution obtained in the previous optimization process, we will terminate the optimization.


\textbf{Model detail}\quad Table~\ref{tab:appendix-details-for-four-transformer-like-models} presents the details of four Transformer-like models selected for our evaluations. Two of these models, namely BERT-Huge~\citep{devlin_bert_2019} and T5-Large~\citep{raffel_exploring_2020}, belong to the domain of natural language processing (NLP). At the same time, the remaining two, ViT-Huge~\citep{dosovitskiy_image_2021} and Swin-Huge~\citep{liu_swin_2021}, are associated with computer vision (CV). It is noteworthy that BERT-Huge and ViT-Huge share the same hidden layer type, whereas T5-Large and Swin-Huge have multiple layer types. Numbers separated by slashes represent the statistical information for different layer types. For instance, Swin-Huge comprises four types of layers, each with 2, 2, 42, and 2 layers, respectively.
\begin{table}
  \centering
  \caption{Details for four Transformer-like models.}
    \begin{tabular}{ccccc}
    \toprule
    Model & Layers & Hidden size & Sequence length & Parameter size \\
    \midrule
    BERT-Huge & 32    & 1280  & 512   & 672M \\
    T5-Large & 16/16 & 1024  & 512   & 502M \\
    ViT-Huge & 32    & 1280  & 196 & 632M \\
    Swin-Huge & 2/2/42/2 & 320/640/1280/2560 & 49*64/49*16/49*4/49*1 & 1016M \\
    \bottomrule
    \end{tabular}
  \label{tab:appendix-details-for-four-transformer-like-models}
\end{table}


\textbf{Training detail}\quad UniAP is based on the PyTorch framework and integrates models from HuggingFace Transformers. It employs various types of parallelism, including Pipeline Parallelism~(PP), Data Parallelism~(DP), Tensor Parallelism~(TP), and Fully Sharded Data Parallelism~(FSDP), utilizing GPipe~\citep{huang_gpipe_2019}, PyTorch DDP~\citep{li_pytorch_2020}, Megatron-LM~\citep{narayanan_efficient_2021}, and FairScale~\citep{fairscale_authors_fairscale_2021}, respectively. For NLP models, we use the English Wikipedia dataset~\citep{wikidump}, while the ImageNet-1K dataset~\citep{imagenet15russakovsky} is used for CV models. We train these models using the Adam optimizer~\citep{kingma_adam_2015} and precision of FP32. We omit hyperparameters here such as learning rate and weight decay as these have minimal impact on training throughput. 
The model parameters in the HuggingFace Transformer are configured to align with the specifications of each individual model. For instance, we set \textit{hidden\_size} to 1280, \textit{num\_hidden\_layers} to 32, \textit{num\_attention\_heads} to 16, and \textit{seq\_length} to 512 for BERT-Huge. Regarding other hyperparameters in the HuggingFace configurations, we set \textit{hidden\_dropout\_prob} and \textit{attention\_probs\_dropout\_prob} to 0.0 for ViT-Huge. For Swin-Huge, we set \textit{drop\_path\_rate} to 0.2. We remain other configurations to default. It should be noted that the training batch sizes for each experiment are outlined in the main paper.


\section{Case study: BERT-Huge}\label{appendix:case-study}
In this section, we present a visualization of the optimal parallelism strategy discovered by UniAP. As represented in Figure \ref{fig:visualization-model}, the strategy pertains to training BERT-Huge with 32 hidden layers in a 2-node environment \textit{EnvB} with a mini-batch size of 16. Each node was equipped with 2 Xeon E5-2620 v4 CPUs, 4 TITAN Xp 12GB GPUs, and 125GB of primary memory. These nodes are interconnected via a 10Gbps network. It should be noted that we only showcase the parallelism strategy for the hidden layers here for simplicity but without losing generality.

% Figure environment removed

Here, we provide further topological information for a node within \textit{EnvB}. As illustrated in Figure~\ref{fig:topo}, we categorize the GPUs numbered 0 and 1 in each node and refer to them collectively as \textit{GPUGroup0}. Similarly, we label the GPUs numbered 2 and 3 as \textit{GPUGroup1}. In \textit{EnvB}, the interconnects within each GPU group (i.e., PCIe) have superior bandwidth than that between different groups (i.e., QPI). We collectively designate these two connection bandwidths as intra-node bandwidth, which is higher than inter-node bandwidth.

% Figure environment removed

In this example, UniAP has identified a parallelism strategy for inter-layer parallelism that involves a two-stage pipeline. This strategy utilizes parallelism in a manner that is both efficient and effective. Specifically, the communication cost of point-to-point~(P2P) between two nodes is less than that of all-reduce. Additionally, the inter-node bandwidth is lower than that of the intra-node. These factors make the two-stage PP approach a reasonable choice. Moreover, the pipeline has been designed such that each stage comprises an equal number of layers. This design leverages the homogeneity of the nodes and ensures load balancing across the cluster.

UniAP employs an intra-layer parallelism strategy within each PP stage. It utilizes a 2-way DP for the initial 12 hidden layers in each stage between \textit{GPUGroup0} and \textit{GPUGroup1}. For the remaining four hidden layers, a 2-way FSDP is utilized between \textit{GPUGroup0} and \textit{GPUGroup1} to reduce memory footprint and meet memory constraints. Within each GPU group, UniAP employs a 2-way TP for each layer. In general, TP incurs more significant communication volumes than DP and FSDP. In order to achieve maximum training throughput on \textit{EnvB}, it is necessary to implement parallelism strategies that prioritize higher communication volumes within each group and lower volumes between groups. Therefore, the strategy for BERT-Huge with 32 hidden layers combines the best elements of PP, DP, TP, and FSDP to maximize training throughput.