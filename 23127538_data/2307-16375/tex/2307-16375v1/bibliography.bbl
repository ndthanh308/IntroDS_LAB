
\begin{thebibliography}{10}

\bibitem{devlin_bert_2019}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for
  {Language} {Understanding}.
\newblock In {\em The {North} {American} {Chapter} of the {Association} for
  {Computational} {Linguistics}: {Human} {Language} {Technologies}}, pages
  4171--4186, 2019.

\bibitem{brown_language_2020}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
  Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter,
  Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
  Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
  Ilya Sutskever, and Dario Amodei.
\newblock Language {Models} are {Few}-{Shot} {Learners}.
\newblock In {\em Advances in {Neural} {Information} {Processing} {Systems}
  33}, pages 1877--1901, 2020.

\bibitem{raffel_exploring_2020}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em The Journal of Machine Learning Research}, 21(1):5485--5551,
  2020.

\bibitem{dosovitskiy_image_2021}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image}
  {Recognition} at {Scale}.
\newblock In {\em {International} {Conference} on {Learning} {Representations}
  9}, 2021.

\bibitem{liu_swin_2021}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin {Transformer}: {Hierarchical} {Vision} {Transformer} using
  {Shifted} {Windows}.
\newblock In {\em {International} {Conference} on {Computer} {Vision}}, pages
  9992--10002, 2021.

\bibitem{narayanan_efficient_2021}
Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa
  Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie
  Bernauer, Bryan Catanzaro, Amar Phanishayee, and Matei Zaharia.
\newblock Efficient {Large-scale} {Language} {Model} {Training} on {GPU}
  {Clusters} {Using} {Megatron}-{LM}.
\newblock In {\em International {Conference} for {High} {Performance}
  {Computing}, {Networking}, {Storage} and {Analysis}}, page~58, 2021.

\bibitem{shazeer_mesh-tensorflow_2018}
Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn
  Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young,
  Ryan Sepassi, and Blake~A. Hechtman.
\newblock Mesh-{TensorFlow}: {Deep} {Learning} for {Supercomputers}.
\newblock In {\em Advances in {Neural} {Information} {Processing} {Systems}
  31}, pages 10435--10444, 2018.

\bibitem{xu_gspmd_2021}
Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake~A. Hechtman, Yanping Huang,
  Rahul Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni,
  Ruoming Pang, Noam Shazeer, Shibo Wang, Tao Wang, Yonghui Wu, and Zhifeng
  Chen.
\newblock {GSPMD}: {General} and {Scalable} {Parallelization} for {ML}
  {Computation} {Graphs}.
\newblock {\em CoRR}, abs/2105.04663, 2021.

\bibitem{flynn_very_1966}
Michael~J. Flynn.
\newblock Very {High-speed} {Computing} {Systems}.
\newblock {\em Proceedings of the IEEE}, 54(12):1901--1909, 1966.

\bibitem{flynn_computer_1972}
Michael~J. Flynn.
\newblock Some {Computer} {Organizations} and {Their} {Effectiveness}.
\newblock {\em IEEE Transactions on Computers}, C-21(9):948--960, 1972.

\bibitem{huang_gpipe_2019}
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia~Xu
  Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc~V. Le, Yonghui Wu, and Zhifeng Chen.
\newblock {GPipe}: {Efficient} {Training} of {Giant} {Neural} {Networks} using
  {Pipeline} {Parallelism}.
\newblock In {\em Advances in {Neural} {Information} {Processing} {Systems}
  32}, pages 103--112, 2019.

\bibitem{narayanan_pipedream_2019}
Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil~R.
  Devanur, Gregory~R. Ganger, Phillip~B. Gibbons, and Matei Zaharia.
\newblock {PipeDream}: {Generalized} {Pipeline} {Parallelism} for {DNN}
  {Training}.
\newblock In {\em {Symposium} on {Operating} {Systems} {Principles}}, pages
  1--15, 2019.

\bibitem{narayanan_memory-efficient_2021}
Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia.
\newblock Memory-{Efficient} {Pipeline}-{Parallel} {DNN} {Training}.
\newblock In {\em {International} {Conference} on {Machine} {Learning}}, pages
  7937--7947, 2021.

\bibitem{fan_dapple_2021}
Shiqing Fan, Yi~Rong, Chen Meng, Zongyan Cao, Siyu Wang, Zhen Zheng, Chuan Wu,
  Guoping Long, Jun Yang, Lixue Xia, Lansong Diao, Xiaoyong Liu, and Wei Lin.
\newblock {DAPPLE}: {A} {Pipelined} {Data} {Parallel} {Approach} for {Training}
  {Large} {Models}.
\newblock In {\em {Symposium} on {Principles} and {Practice} of {Parallel}
  {Programming}}, pages 431--445, 2021.

\bibitem{li_chimera_2021}
Shigang Li and Torsten Hoefler.
\newblock Chimera: {Efficiently} {Training} {Large}-{Scale} {Neural} {Networks}
  with {Bidirectional} {Pipelines}.
\newblock In {\em International {Conference} for {High} {Performance}
  {Computing}, {Networking}, {Storage} and {Analysis}}, page~27, 2021.

\bibitem{lepikhin_gshard_2021}
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping
  Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen.
\newblock {GShard}: {Scaling} {Giant} {Models} with {Conditional} {Computation}
  and {Automatic} {Sharding}.
\newblock In {\em {International} {Conference} on {Learning}
  {Representations}}, 2021.

\bibitem{du_glam_2022}
Nan Du, Yanping Huang, Andrew~M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong
  Xu, Maxim Krikun, Yanqi Zhou, Adams~Wei Yu, Orhan Firat, Barret Zoph, Liam
  Fedus, Maarten~P. Bosma, Zongwei Zhou, Tao Wang, Yu~Emma Wang, Kellie
  Webster, Marie Pellat, Kevin Robinson, Kathleen~S. Meier-Hellstern, Toju
  Duke, Lucas Dixon, Kun Zhang, Quoc~V. Le, Yonghui Wu, Zhifeng Chen, and
  Claire Cui.
\newblock {GLaM}: {Efficient} {Scaling} of {Language} {Models} with
  {Mixture}-of-{Experts}.
\newblock In {\em International {Conference} on {Machine} {Learning}}, pages
  5547--5569, 2022.

\bibitem{fedus_switch_2022}
William Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch {Transformers}: {Scaling} to {Trillion} {Parameter} {Models}
  with {Simple} and {Efficient} {Sparsity}.
\newblock {\em The Journal of Machine Learning Research}, 23(120):1--39, 2022.

\bibitem{li_pytorch_2020}
Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li,
  Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, and Soumith Chintala.
\newblock {PyTorch} {Distributed}: {Experiences} on {Accelerating} {Data}
  {Parallel} {Training}.
\newblock {\em Proceedings of the VLDB Endowment}, 13(12):3005--3018, 2020.

\bibitem{rasley_deepspeed_2020}
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He.
\newblock {DeepSpeed}: {System} {Optimizations} {Enable} {Training} {Deep}
  {Learning} {Models} with {Over} 100 {Billion} {Parameters}.
\newblock In {\em {The} 26th {ACM} {SIGKDD} {Conference} on {Knowledge}
  {Discovery} and {Data} {Mining}}, pages 3505--3506, 2020.

\bibitem{fairscale_authors_fairscale_2021}
{FairScale authors}.
\newblock {FairScale}: {A} general purpose modular {PyTorch} library for high
  performance and large scale training, 2021.

\bibitem{zhao_vpipe_2022}
Shixiong Zhao, Fanxin Li, Xusheng Chen, Xiuxian Guan, Jianyu Jiang, Dong Huang,
  Yuhao Qing, Sen Wang, Peng Wang, Gong Zhang, Cheng Li, Ping Luo, and Heming
  Cui.
\newblock {vPipe}: {A} {Virtualized} {Acceleration} {System} for {Achieving}
  {Efficient} and {Scalable} {Pipeline} {Parallel} {DNN} {Training}.
\newblock {\em IEEE Transactions on Parallel and Distributed Systems},
  33(3):489--506, 2022.

\bibitem{jia_exploring_2018}
Zhihao Jia, Sina Lin, Charles~R. Qi, and Alex Aiken.
\newblock Exploring {Hidden} {Dimensions} in {Parallelizing} {Convolutional}
  {Neural} {Networks}.
\newblock In {\em {International} {Conference} on {Machine} {Learning}}, pages
  2279--2288, 2018.

\bibitem{cai_tensoropt_2022}
Zhenkun Cai, Xiao Yan, Kaihao Ma, Yidi Wu, Yuzhen Huang, James Cheng, Teng Su,
  and Fan Yu.
\newblock {TensorOpt}: {Exploring} the {Tradeoffs} in {Distributed} {DNN}
  {Training} {With} {Auto}-{Parallelism}.
\newblock {\em IEEE Transactions on Parallel and Distributed Systems},
  33(8):1967--1981, 2022.

\bibitem{wang_supporting_2019}
Minjie Wang, Chien-Chin Huang, and Jinyang Li.
\newblock Supporting {Very} {Large} {Models} using {Automatic} {Dataflow}
  {Graph} {Partitioning}.
\newblock In {\em Proceedings of the {Fourteenth} {EuroSys} {Conference}},
  pages 26:1--26:17, 2019.

\bibitem{jia_beyond_2019}
Zhihao Jia, Matei Zaharia, and Alex Aiken.
\newblock Beyond {Data} and {Model} {Parallelism} for {Deep} {Neural}
  {Networks}.
\newblock In {\em {Machine} {Learning} and {Systems}}, pages 1--13, 2019.

\bibitem{schaarschmidt_automap_2021}
Michael Schaarschmidt, Dominik Grewe, Dimitrios Vytiniotis, Adam Paszke,
  Georg~Stefan Schmid, Tamara Norman, James Molloy, Jonathan Godwin,
  Norman~Alexander Rink, Vinod Nair, and Dan Belov.
\newblock Automap: {Towards} {Ergonomic} {Automated} {Parallelism} for {ML}
  {Models}.
\newblock {\em CoRR}, abs/2112.02958, 2021.

\bibitem{liu_colossal-auto_2023}
Yuliang Liu, Shenggui Li, Jiarui Fang, Yanjun Shao, Boyuan Yao, and Yang You.
\newblock Colossal-{Auto}: {Unified} {Automation} of {Parallelization} and
  {Activation} {Checkpoint} for {Large}-scale {Models}.
\newblock {\em CoRR}, abs/2302.02599, 2023.

\bibitem{he_pipetransformer_2021}
Chaoyang He, Shen Li, Mahdi Soltanolkotabi, and Salman Avestimehr.
\newblock {PipeTransformer}: {Automated} {Elastic} {Pipelining} for
  {Distributed} {Training} of {Large}-scale {Models}.
\newblock In {\em {International} {Conference} on {Machine} {Learning}}, pages
  4150--4159, 2021.

\bibitem{tarnawski_efficient_2020}
Jakub Tarnawski, Amar Phanishayee, Nikhil~R. Devanur, Divya Mahajan, and
  Fanny~Nina Paravecino.
\newblock Efficient {Algorithms} for {Device} {Placement} of {DNN} {Graph}
  {Operators}.
\newblock In {\em Advances in {Neural} {Information} {Processing} {Systems}
  33}, pages 15451--15463, 2020.

\bibitem{tarnawski_piper_2021}
Jakub Tarnawski, Deepak Narayanan, and Amar Phanishayee.
\newblock Piper: {Multidimensional} {Planner} for {DNN} {Parallelization}.
\newblock In {\em Advances in {Neural} {Information} {Processing} {Systems}
  34}, pages 24829--24840, 2021.

\bibitem{zheng_alpa_2022}
Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping
  Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric~P. Xing, Joseph~E.
  Gonzalez, and Ion Stoica.
\newblock Alpa: {Automating} {Inter}- and {Intra}-{Operator} {Parallelism} for
  {Distributed} {Deep} {Learning}.
\newblock In {\em {USENIX} {Symposium} on {Operating} {Systems} {Design} and
  {Implementation}}, pages 559--578, 2022.

\bibitem{lazimy_mixed_1982}
Rafael Lazimy.
\newblock Mixed-integer {Quadratic} {Programming}.
\newblock {\em Math. Program.}, 22(1):332â€“349, December 1982.

\bibitem{wang_auto-map_2020}
Siyu Wang, Yi~Rong, Shiqing Fan, Zhen Zheng, Lansong Diao, Guoping Long, Jun
  Yang, Xiaoyong Liu, and Wei Lin.
\newblock Auto-{MAP}: {A} {DQN} {Framework} for {Exploring} {Distributed}
  {Execution} {Plans} for {DNN} {Workloads}.
\newblock {\em CoRR}, abs/2007.04069, 2020.

\bibitem{miao_galvatron_2022}
Xupeng Miao, Yujie Wang, Youhe Jiang, Chunan Shi, Xiaonan Nie, Hailin Zhang,
  and Bin Cui.
\newblock Galvatron: {Efficient} {Transformer} {Training} over {Multiple}
  {GPUs} {Using} {Automatic} {Parallelism}.
\newblock {\em Proceedings of the VLDB Endowment}, 16(3):470--479, 2022.

\bibitem{rashidi_enabling_2021}
Saeed Rashidi, Matthew Denton, Srinivas Sridharan, Sudarshan Srinivasan,
  Amoghavarsha Suresh, Jade Nie, and Tushar Krishna.
\newblock Enabling {Compute}-{Communication} {Overlap} in {Distributed} {Deep}
  {Learning} {Training} {Platforms}.
\newblock In {\em {International} {Symposium} on {Computer} {Architecture}},
  pages 540--553, 2021.

\bibitem{gurobi_optimization_llc_gurobi_2023}
{Gurobi Optimization, LLC}.
\newblock Gurobi {Optimizer} {Reference} {Manual}, 2023.

\bibitem{rajbhandari_zero_2020}
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.
\newblock {ZeRO}: {Memory} {Optimizations} {Toward} {Training} {Trillion}
  {Parameter} {Models}.
\newblock In {\em {International} {Conference} for {High} {Performance}
  {Computing}, {Networking}, {Storage} and {Analysis}}, page~20, 2020.

\bibitem{deepspeed-3d}
Microsoft.
\newblock Deepspeed {3D}.
\newblock
  \url{https://github.com/microsoft/Megatron-DeepSpeed/blob/main/examples/pretrain_bert_distributed_with_mp.sh},
  2021.

\bibitem{wikidump}
Wikimedia Foundation.
\newblock Wikimedia {Downloads}.
\newblock \url{https://dumps.wikimedia.org}, 2023.

\bibitem{imagenet15russakovsky}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein,
  Alexander~C. Berg, and Li~Fei-Fei.
\newblock {ImageNet Large Scale Visual Recognition Challenge}.
\newblock {\em International Journal of Computer Vision},
  115(3):211--252, 2015.

\bibitem{kingma_adam_2015}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} {Method} for {Stochastic} {Optimization}.
\newblock In {\em 3rd {International} {Conference} on {Learning}
  {Representations}}, 2015.

\end{thebibliography}
