
\section{Introduction}
Deep learning models have been widely used in many applications.
For example, BERT~\citep{devlin_bert_2019}, GPT-3~\citep{brown_language_2020}, and T5~\citep{raffel_exploring_2020} achieved state-of-the-art~(SOTA) results on different natural language processing~(NLP) tasks. 
For computer vision~(CV), Transformer-like models such as ViT~\citep{dosovitskiy_image_2021} and Swin Transformer~\citep{liu_swin_2021} deliver excellent accuracy performance upon multiple tasks. 


At the same time, training deep learning models has been a critical problem troubling the community due to the long training time, especially for those large models with billions of parameters~\citep{brown_language_2020}. 
In order to enhance the training efficiency, researchers propose some manually designed parallel training strategies~\citep{narayanan_efficient_2021,shazeer_mesh-tensorflow_2018,xu_gspmd_2021}. 
However, selecting, tuning, and combining these strategies require extensive domain knowledge in deep learning models and hardware environments. With the increasing diversity of modern hardware architectures~\cite{flynn_very_1966,flynn_computer_1972} and the rapid development of deep learning models, these manually designed approaches are bringing heavier burdens to developers. 
Hence, \emph{automatic parallelism} is introduced to automate the parallel strategy searching for training models.


There are two main categories of parallelism in deep learning models: inter-layer parallelism~\citep{huang_gpipe_2019,narayanan_pipedream_2019,narayanan_memory-efficient_2021,fan_dapple_2021,li_chimera_2021,lepikhin_gshard_2021,du_glam_2022,fedus_switch_2022} and intra-layer parallelism~\citep{li_pytorch_2020,narayanan_efficient_2021,rasley_deepspeed_2020,fairscale_authors_fairscale_2021}. 
Inter-layer parallelism partitions the model into disjoint sets on different devices without slicing tensors. 
Alternatively, intra-layer parallelism partitions tensors in a layer along one or more axes and distributes them across different devices.


Current automatic parallelism techniques focus on optimizing strategies within these two categories. However, they treat these two categories separately. 
Some methods~\citep{zhao_vpipe_2022,jia_exploring_2018,cai_tensoropt_2022,wang_supporting_2019,jia_beyond_2019,schaarschmidt_automap_2021,liu_colossal-auto_2023} overlook potential opportunities for inter- or intra-layer parallelism, the others optimize inter- and intra-layer parallelism hierarchically and sequentially~\citep{narayanan_pipedream_2019,fan_dapple_2021,he_pipetransformer_2021,tarnawski_efficient_2020,tarnawski_piper_2021,zheng_alpa_2022}. 
As a result, current automatic parallelism techniques often fail to achieve the global optima and instead become trapped in local optima. 
Therefore, a unified inter- and intra-layer approach is needed to enhance the effectiveness of automatic parallelism.


This paper aims to find the optimal parallelism strategy while simultaneously considering inter- and intra-layer parallelism. 
It enables us to search in a more extensive strategy space where the globally optimal solution lurk. 
However, unifying inter- and intra-layer parallelism in automatic parallelism brings us two challenges. 
Firstly, to adopt a unified perspective on the inter- and intra-layer automatic parallelism, we should not formalize them with separate formulations as prior works. Therefore, how can we express these parallelism strategies in a unified formulation? 
Secondly, previous methods take a long time to obtain the solution with a limited strategy space. Therefore, how can we ensure that the best solution can be obtained in a reasonable time while expanding the strategy space?


To solve the above challenges, we propose UniAP. For the first challenge, UniAP adopts the mixed integer quadratic programming~(MIQP)~\citep{lazimy_mixed_1982} to search for the globally optimal parallel strategy automatically. 
It unifies the inter- and intra-layer automatic parallelism in a single MIQP formulation. 
For the second challenge, our complexity analysis and experimental results show that UniAP can obtain the globally optimal solution in a significantly shorter time.


The contributions of this paper are summarized as follows: 
\begin{itemize}
    \item We propose UniAP, the first framework to unify inter- and intra-layer automatic parallelism in model training.
    \item The optimal parallel strategies discovered by UniAP exhibit scalability on training throughput and strategy searching time.
    \item The experimental results show that UniAP speeds up model training on four Transformer-like models by up to 1.70$\times$ and reduces the strategy searching time by up to 16$\times$, compared with the SOTA method.
\end{itemize}
