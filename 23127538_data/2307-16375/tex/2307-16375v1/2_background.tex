\section{Background}
\subsection{Inter- and intra-layer parallelism}
In general, there exist two main categories of parallelism strategies for deep learning models: inter- and intra-layer parallelism. If we want to divide them further, inter-layer parallelism mainly includes pipeline parallelism~(PP) in our context. Meanwhile, intra-layer parallelism mainly includes data parallelism~(DP), tensor parallelism~(TP), and fully sharded data parallelism~(FSDP). Most manual and automatic parallelism approaches search for the optimal strategy within these dimensions.


\subsection{Manual parallelism}
Manual parallelism refers to parallel computing strategies designed and optimized by human experts. Representative methods include Megatron-LM~\citep{narayanan_efficient_2021}, Mesh-TensorFlow~\citep{shazeer_mesh-tensorflow_2018}, and GSPMD~\citep{xu_gspmd_2021}. Megatron-LM is a high-performance computing library for parallel Transformer training. It exhibits superior efficiency in both computing and scaling on clusters. Mesh-TensorFlow and GSPMD require users to annotate the desired intra-layer parallel computing mode. Such methods rely on expert design and manual tuning, challenging their automatic application to other models.

\subsection{Automatic parallelism}
\paragraph{Inter- or intra-layer-only automatic parallelism} For inter-layer-only automatic parallelism, GPipe~\citep{huang_gpipe_2019} and vPipe~\citep{zhao_vpipe_2022} employ a balanced partition algorithm and a dynamic layer partitioning middleware to partition pipelines, respectively. The parallel strategies they generate could be more optimal because both algorithms are greedy. For intra-layer-only automatic parallelism, OptCNN~\citep{jia_exploring_2018}, TensorOpt~\citep{cai_tensoropt_2022}, and Tofu~\citep{wang_supporting_2019} employ dynamic programming methods to solve DP and TP together. Meanwhile, FlexFlow~\citep{jia_beyond_2019} and Automap~\citep{schaarschmidt_automap_2021} use a Monte Carlo approach to find the parallel execution plan. Colossal-Auto~\citep{liu_colossal-auto_2023} utilizes integer programming techniques to generate strategies for intra-layer parallelism. These methods explore a more limited strategy space for automatic parallelism and do not produce a globally optimal solution.
\paragraph{Inter- and intra-layer automatic parallelism} Auto-MAP~\citep{wang_auto-map_2020} presents a Deep Q-Network~(DQN) for DP-only, TP-only, and PP-only strategy searching, which requires relatively high model training time. PipeDream~\citep{narayanan_pipedream_2019}, DAPPLE~\citep{fan_dapple_2021}, and PipeTransformer~\citep{he_pipetransformer_2021} use pure dynamic programming to determine optimal strategies for both DP and PP. DNN-partitioning~\citep{tarnawski_efficient_2020} adopts integer and dynamic programming to explore DP and PP strategies. All of these approaches neglect potential optimization opportunities in TP. Piper~\citep{tarnawski_piper_2021} and Alpa~\citep{zheng_alpa_2022} adopt a hierarchical approach to automatic parallelism, considering DP, TP, and PP. The main difference is that Piper searches for strategies in layer granularity, while Alpa searches for operator granularity. This perspective produces locally near-optimal solutions rather than globally optimal ones. Galvatron~\citep{miao_galvatron_2022} uses pure dynamic programming to determine DP, TP, and FSDP strategies on a single pipeline stage. As for PP, it partitions stages and determines micro-batch size using naive greedy algorithms. Compared with them, UniAP holds the most extensive search space for PP, DP, TP, and FSDP.
