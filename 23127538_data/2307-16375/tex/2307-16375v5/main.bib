


@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	year = {2017},
	pages = {5998--6008},
}

@inproceedings{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	booktitle = {The {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2019},
	pages = {4171--4186},
}

@article{raffel_exploring_2020,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}


@inproceedings{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 33},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year = {2020},
     pages = {1877--1901},
}

@inproceedings{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	booktitle = {{International} {Conference} on {Learning} {Representations}},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	year = {2021},
}

@inproceedings{liu_swin_2021,
	title = {Swin {Transformer}: {Hierarchical} {Vision} {Transformer} using {Shifted} {Windows}},
	booktitle = {{International} {Conference} on {Computer} {Vision}},
	author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	year = {2021},
	pages = {9992--10002},
}

@article{miao_galvatron_2022,
	title = {Galvatron: {Efficient} {Transformer} {Training} over {Multiple} {GPUs} {Using} {Automatic} {Parallelism}},
 volume = {16},
	number = {3},
	journal = {Proceedings of the VLDB Endowment},
	author = {Miao, Xupeng and Wang, Yujie and Jiang, Youhe and Shi, Chunan and Nie, Xiaonan and Zhang, Hailin and Cui, Bin},
	year = {2022},
	pages = {470--479},
}

@inproceedings{tarnawski_piper_2021,
	title = {Piper: {Multidimensional} {Planner} for {DNN} {Parallelization}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 34},
	author = {Tarnawski, Jakub and Narayanan, Deepak and Phanishayee, Amar},
	year = {2021},
	pages = {24829--24840},
}

@inproceedings{zheng_alpa_2022,
	title = {Alpa: {Automating} {Inter}- and {Intra}-{Operator} {Parallelism} for {Distributed} {Deep} {Learning}},
	booktitle = {{USENIX} {Symposium} on {Operating} {Systems} {Design} and {Implementation}},
	author = {Zheng, Lianmin and Li, Zhuohan and Zhang, Hao and Zhuang, Yonghao and Chen, Zhifeng and Huang, Yanping and Wang, Yida and Xu, Yuanzhong and Zhuo, Danyang and Xing, Eric P. and Gonzalez, Joseph E. and Stoica, Ion},
	year = {2022},
	pages = {559--578},
}

@article{li_pytorch_2020,
	title = {{PyTorch} {Distributed}: {Experiences} on {Accelerating} {Data} {Parallel} {Training}},
	volume = {13},
	shorttitle = {{PyTorch} {Distributed}},
	number = {12},
	journal = {Proceedings of the VLDB Endowment},
	author = {Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and Chintala, Soumith},
	year = {2020},
	pages = {3005--3018},
}

@inproceedings{narayanan_efficient_2021,
	title = {Efficient {Large-scale} {Language} {Model} {Training} on {GPU} {Clusters} {Using} {Megatron}-{LM}},
	booktitle = {International {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	author = {Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and Phanishayee, Amar and Zaharia, Matei},
	year = {2021},
    pages = {58:1-58:15}
}

@inproceedings{rajbhandari_zero_2020,
	title = {{ZeRO}: {Memory} {Optimizations} {Toward} {Training} {Trillion} {Parameter} {Models}},
	shorttitle = {{ZeRO}},
	booktitle = {{International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	author = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
	year = {2020},
	pages = {20:1-20:16},
}

@inproceedings{rasley_deepspeed_2020,
	title = {{DeepSpeed}: {System} {Optimizations} {Enable} {Training} {Deep} {Learning} {Models} with {Over} 100 {Billion} {Parameters}},
	shorttitle = {{DeepSpeed}},
	booktitle = {{The} 26th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	author = {Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
	year = {2020},
	pages = {3505--3506},
}

@inproceedings{ren_zero-offload_2021,
	title = {{ZeRO}-{Offload}: {Democratizing} {Billion}-{Scale} {Model} {Training}},
	shorttitle = {{ZeRO}-{Offload}},
	booktitle = {{USENIX} {Annual} {Technical} {Conference}},
	author = {Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong},
	year = {2021},
	pages = {551--564},
}

@misc{fairscale_authors_fairscale_2021,
	title = {{FairScale}: {A} general purpose modular {PyTorch} library for high performance and large scale training},
	howpublished = {\href{https://github.com/facebookresearch/fairscale}{https://github.com/facebookresearch/fairscale}},
	author = {{FairScale authors}},
	year = {2021},
}

@inproceedings{huang_gpipe_2019,
	title = {{GPipe}: {Efficient} {Training} of {Giant} {Neural} {Networks} using {Pipeline} {Parallelism}},
	shorttitle = {{GPipe}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	author = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia Xu and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V. and Wu, Yonghui and Chen, Zhifeng},
	year = {2019},
	pages = {103--112},
}

@inproceedings{fan_dapple_2021,
	title = {{DAPPLE}: {A} {Pipelined} {Data} {Parallel} {Approach} for {Training} {Large} {Models}},
	shorttitle = {{DAPPLE}},
	booktitle = {{Symposium} on {Principles} and {Practice} of {Parallel} {Programming}},
	author = {Fan, Shiqing and Rong, Yi and Meng, Chen and Cao, Zongyan and Wang, Siyu and Zheng, Zhen and Wu, Chuan and Long, Guoping and Yang, Jun and Xia, Lixue and Diao, Lansong and Liu, Xiaoyong and Lin, Wei},
	year = {2021},
	pages = {431--445},
}

@inproceedings{narayanan_memory-efficient_2021,
	title = {Memory-{Efficient} {Pipeline}-{Parallel} {DNN} {Training}},
	booktitle = {{International} {Conference} on {Machine} {Learning}},
	author = {Narayanan, Deepak and Phanishayee, Amar and Shi, Kaiyu and Chen, Xie and Zaharia, Matei},
	year = {2021},
	pages = {7937--7947},
}

@article{jia_highly_2018,
	title = {Highly {Scalable} {Deep} {Learning} {Training} {System} with {Mixed}-{Precision}: {Training} {ImageNet} in {Four} {Minutes}},
	volume = {abs/1807.11205},
	shorttitle = {Highly {Scalable} {Deep} {Learning} {Training} {System} with {Mixed}-{Precision}},
	journal = {CoRR},
	author = {Jia, Xianyan and Song, Shutao and He, Wei and Wang, Yangzihao and Rong, Haidong and Zhou, Feihu and Xie, Liqiang and Guo, Zhenyu and Yang, Yuanzhou and Yu, Liwei and Chen, Tiegang and Hu, Guangxiao and Shi, Shaohuai and Chu, Xiaowen},
	year = {2018},
}

@inproceedings{tarnawski_efficient_2020,
	title = {Efficient {Algorithms} for {Device} {Placement} of {DNN} {Graph} {Operators}},
     pages = {15451--15463},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 33},
	author = {Tarnawski, Jakub and Phanishayee, Amar and Devanur, Nikhil R. and Mahajan, Divya and Paravecino, Fanny Nina},
	year = {2020},
}

@misc{gurobi_optimization_llc_gurobi_2023,
	title = {Gurobi {Optimizer} {Reference} {Manual}},
	howpublished = {\href{https://www.gurobi.com}{https://www.gurobi.com}},
	author = {{Gurobi Optimization, LLC}},
	year = {2023},
}

@inproceedings{shazeer_mesh-tensorflow_2018,
	title = {Mesh-{TensorFlow}: {Deep} {Learning} for {Supercomputers}},
	shorttitle = {Mesh-{TensorFlow}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	author = {Shazeer, Noam and Cheng, Youlong and Parmar, Niki and Tran, Dustin and Vaswani, Ashish and Koanantakool, Penporn and Hawkins, Peter and Lee, HyoukJoong and Hong, Mingsheng and Young, Cliff and Sepassi, Ryan and Hechtman, Blake A.},
	year = {2018},
	pages = {10435--10444},
}

@article{xu_gspmd_2021,
	title = {{GSPMD}: {General} and {Scalable} {Parallelization} for {ML} {Computation} {Graphs}},
	volume = {abs/2105.04663},
	shorttitle = {{GSPMD}},
	journal = {CoRR},
	author = {Xu, Yuanzhong and Lee, HyoukJoong and Chen, Dehao and Hechtman, Blake A. and Huang, Yanping and Joshi, Rahul and Krikun, Maxim and Lepikhin, Dmitry and Ly, Andy and Maggioni, Marcello and Pang, Ruoming and Shazeer, Noam and Wang, Shibo and Wang, Tao and Wu, Yonghui and Chen, Zhifeng},
	year = {2021},
}

@article{zhao_vpipe_2022,
	title = {{vPipe}: {A} {Virtualized} {Acceleration} {System} for {Achieving} {Efficient} and {Scalable} {Pipeline} {Parallel} {DNN} {Training}},
	volume = {33},
	shorttitle = {{vPipe}},
	number = {3},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Zhao, Shixiong and Li, Fanxin and Chen, Xusheng and Guan, Xiuxian and Jiang, Jianyu and Huang, Dong and Qing, Yuhao and Wang, Sen and Wang, Peng and Zhang, Gong and Li, Cheng and Luo, Ping and Cui, Heming},
	year = {2022},
	pages = {489--506},
}

@inproceedings{jia_exploring_2018,
	title = {Exploring {Hidden} {Dimensions} in {Parallelizing} {Convolutional} {Neural} {Networks}},
	booktitle = {{International} {Conference} on {Machine} {Learning}},
	author = {Jia, Zhihao and Lin, Sina and Qi, Charles R. and Aiken, Alex},
	year = {2018},
	pages = {2279--2288},
}

@article{cai_tensoropt_2022,
	title = {{TensorOpt}: {Exploring} the {Tradeoffs} in {Distributed} {DNN} {Training} {With} {Auto}-{Parallelism}},
	volume = {33},
	shorttitle = {{TensorOpt}},
	number = {8},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Cai, Zhenkun and Yan, Xiao and Ma, Kaihao and Wu, Yidi and Huang, Yuzhen and Cheng, James and Su, Teng and Yu, Fan},
	year = {2022},
	pages = {1967--1981},
	file = {已提交版本:C\:\\Users\\linh\\Zotero\\storage\\RXURA8S7\\Cai 等 - 2022 - TensorOpt Exploring the Tradeoffs in Distributed .pdf:application/pdf},
}

@inproceedings{wang_supporting_2019,
	title = {Supporting {Very} {Large} {Models} using {Automatic} {Dataflow} {Graph} {Partitioning}},
	booktitle = {Proceedings of the {Fourteenth} {EuroSys} {Conference}},
	author = {Wang, Minjie and Huang, Chien-Chin and Li, Jinyang},
	year = {2019},
	pages = {26:1--26:17},
}

@inproceedings{jia_beyond_2019,
	title = {Beyond {Data} and {Model} {Parallelism} for {Deep} {Neural} {Networks}},
 pages = {1--13},
	booktitle = {{Machine} {Learning} and {Systems}},
	author = {Jia, Zhihao and Zaharia, Matei and Aiken, Alex},
	year = {2019},
}

@article{schaarschmidt_automap_2021,
	title = {Automap: {Towards} {Ergonomic} {Automated} {Parallelism} for {ML} {Models}},
	volume = {abs/2112.02958},
	shorttitle = {Automap},
	journal = {CoRR},
	author = {Schaarschmidt, Michael and Grewe, Dominik and Vytiniotis, Dimitrios and Paszke, Adam and Schmid, Georg Stefan and Norman, Tamara and Molloy, James and Godwin, Jonathan and Rink, Norman Alexander and Nair, Vinod and Belov, Dan},
	year = {2021},
}

@article{wang_auto-map_2020,
	title = {Auto-{MAP}: {A} {DQN} {Framework} for {Exploring} {Distributed} {Execution} {Plans} for {DNN} {Workloads}},
	volume = {abs/2007.04069},
	shorttitle = {Auto-{MAP}},
	journal = {CoRR},
	author = {Wang, Siyu and Rong, Yi and Fan, Shiqing and Zheng, Zhen and Diao, Lansong and Long, Guoping and Yang, Jun and Liu, Xiaoyong and Lin, Wei},
	year = {2020},
}

@inproceedings{narayanan_pipedream_2019,
	title = {{PipeDream}: {Generalized} {Pipeline} {Parallelism} for {DNN} {Training}},
	shorttitle = {{PipeDream}},
	booktitle = {{Symposium} on {Operating} {Systems} {Principles}},
	author = {Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil R. and Ganger, Gregory R. and Gibbons, Phillip B. and Zaharia, Matei},
	year = {2019},
	pages = {1--15},
}

@inproceedings{he_pipetransformer_2021,
	title = {{PipeTransformer}: {Automated} {Elastic} {Pipelining} for {Distributed} {Training} of {Large}-scale {Models}},
	shorttitle = {{PipeTransformer}},
	booktitle = {{International} {Conference} on {Machine} {Learning}},
	author = {He, Chaoyang and Li, Shen and Soltanolkotabi, Mahdi and Avestimehr, Salman},
	year = {2021},
	pages = {4150--4159},
}

@inproceedings{micikevicius_mixed_2018,
	title = {Mixed {Precision} {Training}},
	booktitle = {{International} {Conference} on {Learning} {Representations}},
	author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory F. and Elsen, Erich and García, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
	year = {2018},
}

@article{chen_training_2016,
	title = {Training {Deep} {Nets} with {Sublinear} {Memory} {Cost}},
	volume = {abs/1604.06174},
	journal = {CoRR},
	author = {Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
	year = {2016},
}

@inproceedings{li_chimera_2021,
	title = {Chimera: {Efficiently} {Training} {Large}-{Scale} {Neural} {Networks} with {Bidirectional} {Pipelines}},
	shorttitle = {Chimera},
	booktitle = {International {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	author = {Li, Shigang and Hoefler, Torsten},
	year = {2021},
	pages = {27:1-27:14},
	file = {已提交版本:C\:\\Users\\linh\\Zotero\\storage\\6BJ28MQI\\Li 和 Hoefler - 2021 - Chimera efficiently training large-scale neural n.pdf:application/pdf},
}

@article{pia_mixed-integer_2017,
	title = {Mixed-{Integer} {Quadratic} {Programming} is in {NP}},
	volume = {162},
	number = {1-2},
	journal = {Math. Program.},
	author = {Pia, Alberto Del and Dey, Santanu S. and Molinaro, Marco},
	year = {2017},
	pages = {225--240},
	file = {已提交版本:C\:\\Users\\linh\\Zotero\\storage\\ZAX6KZNL\\Pia 等 - 2017 - Mixed-integer quadratic programming is in NP.pdf:application/pdf},
}

@article{lazimy_mixed_1982,
	author = {Lazimy, Rafael},
	title = {Mixed-Integer {Quadratic} {Programming}},
	year = {1982},
	issue_date = {December  1982},
	address = {Berlin, Heidelberg},
	volume = {22},
	number = {1},
	abstract = {This paper considers mixed-integer quadratic programs in which the objective function is quadratic in the integer and in the continuous variables, and the constraints are linear in the variables of both types. The generalized Benders' decomposition is a suitable approach for solving such programs. However, the program does not become more tractable if this method is used, since Benders' cuts are quadratic in the integer variables. A new equivalent formulation that renders the program tractable is developed, under which the dual objective function is linear in the integer variables and the dual constraint set is independent of these variables. Benders' cuts that are derived from the new formulation are linear in the integer variables, and the original problem is decomposed into a series of integer linear master problems and standard quadratic subproblems. The new formulation does not introduce new primary variables or new constraints into the computational steps of the decomposition algorithm.},
	journal = {Math. Program.},
	month = dec,
	pages = {332–349},
	numpages = {18},
	keywords = {Quadratic Duality Theory, Quadratic Programming, Mixed-Integer Quadratic Programming, Integer Linear Programs, Generalized Benders Decomposition, Generalized Inverses}
}


@article{achterberg_scip_2009,
	title = {{SCIP}: {Solving} {Constraint} {Integer} {Programs}},
	volume = {1},
	abstract = {Constraint integer programming (CIP) is a novel paradigm which integrates constraint programming (CP), mixed integer programming (MIP), and satisfiability (SAT) modeling and solving techniques. In this paper we discuss the software framework and solver SCIP (Solving Constraint Integer Programs), which is free for academic and non-commercial use and can be downloaded in source code. This paper gives an overview of the main design concepts of SCIP and how it can be used to solve constraint integer programs. To illustrate the performance and flexibility of SCIP, we apply it to two different problem classes. First, we consider mixed integer programming and show by computational experiments that SCIP is almost competitive to specialized commercial MIP solvers, even though SCIP supports the more general constraint integer programming paradigm. We develop new ingredients that improve current MIP solving technology. As a second application, we employ SCIP to solve chip design verification problems as they arise in the logic design of integrated circuits. This application goes far beyond traditional MIP solving, as it includes several highly non-linear constraints, which can be handled nicely within the constraint integer programming framework. We show anecdotally how the different solving techniques from MIP, CP, and SAT work together inside SCIP to deal with such constraint classes. Finally, experimental results show that our approach outperforms current state-of-the-art techniques for proving the validity of properties on circuits containing arithmetic.},
	number = {1},
	journal = {Mathematical Programming Computation},
	author = {Achterberg, Tobias},
	year = {2009},
	pages = {1--41},
}


@article{liu_colossal-auto_2023,
	title = {Colossal-{Auto}: {Unified} {Automation} of {Parallelization} and {Activation} {Checkpoint} for {Large}-scale {Models}},
	volume = {abs/2302.02599},
	shorttitle = {Colossal-{Auto}},
	journal = {CoRR},
	author = {Liu, Yuliang and Li, Shenggui and Fang, Jiarui and Shao, Yanjun and Yao, Boyuan and You, Yang},
	year = {2023},
}


@article{pudipeddi_training_2020,
	title = {Training {Large} {Neural} {Networks} with {Constant} {Memory} using a {New} {Execution} {Algorithm}},
	volume = {abs/2002.05645},
	journal = {CoRR},
	author = {Pudipeddi, Bharadwaj and Mesmakhosroshahi, Maral and Xi, Jinwen and Bharadwaj, Sujeeth},
	year = {2020},
}


@inproceedings{sun_stronghold_2022,
	title = {{STRONGHOLD}: {Fast} and {Affordable} {Billion}-{Scale} {Deep} {Learning} {Model} {Training}},
	shorttitle = {{STRONGHOLD}},
	booktitle = {{International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	author = {Sun, Xiaoyang and Wang, Wei and Qiu, Shenghao and Yang, Renyu and Huang, Songfang and Xu, Jie and Wang, Zheng},
	year = {2022},
	pages = {1--17},
}

@article{flynn_very_1966,
	title = {Very {High-speed} {Computing} {Systems}},
	volume = {54},
	number = {12},
	journal = {Proceedings of the IEEE},
	author = {Flynn, Michael J.},
	year = {1966},
	keywords = {Arithmetic, Art, Computer aided instruction, Hardware, Impedance matching, Large-scale systems, Pervasive computing, Scientific computing, Turing machines},
	pages = {1901--1909},
}


@article{flynn_computer_1972,
	title = {Some {Computer} {Organizations} and {Their} {Effectiveness}},
	volume = {C-21},
	number = {9},
	journal = {IEEE Transactions on Computers},
	author = {Flynn, Michael J.},
	year = {1972},
	keywords = {Bandwidth, Computational modeling, Computer organization, Computers, Data mining, Entropy, instruction stream, Organizations, overlapped, parallel processors, Probability density function, resource hierarchy},
	pages = {948--960},
}

@article{ben-nun_demystifying_2019,
	title = {Demystifying {Parallel} and {Distributed} {Deep} {Learning}: {An} {In}-depth {Concurrency} {Analysis}},
	volume = {52},
	shorttitle = {Demystifying {Parallel} and {Distributed} {Deep} {Learning}},
	number = {4},
	journal = {ACM Comput. Surv.},
	author = {Ben-Nun, Tal and Hoefler, Torsten},
	year = {2019},
	pages = {65:1--65:43},
}


@inproceedings{rashidi_enabling_2021,
	title = {Enabling {Compute}-{Communication} {Overlap} in {Distributed} {Deep} {Learning} {Training} {Platforms}},
	booktitle = {{International} {Symposium} on {Computer} {Architecture}},
	author = {Rashidi, Saeed and Denton, Matthew and Sridharan, Srinivas and Srinivasan, Sudarshan and Suresh, Amoghavarsha and Nie, Jade and Krishna, Tushar},
	year = {2021},
	pages = {540--553},
}

@misc{deepspeed-3d,
   author = {Microsoft},
   title = {{Megatron}-{DeepSpeed}},
   howpublished = {\href{https://github.com/microsoft/Megatron-DeepSpeed/tree/main}{\url{https://github.com/microsoft/Megatron-DeepSpeed/tree/main}}},
   year={2021}
}

@inproceedings{du_glam_2022,
	title = {{GLaM}: {Efficient} {Scaling} of {Language} {Models} with {Mixture}-of-{Experts}},
	shorttitle = {{GLaM}},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Du, Nan and Huang, Yanping and Dai, Andrew M. and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and Zoph, Barret and Fedus, Liam and Bosma, Maarten P. and Zhou, Zongwei and Wang, Tao and Wang, Yu Emma and Webster, Kellie and Pellat, Marie and Robinson, Kevin and Meier-Hellstern, Kathleen S. and Duke, Toju and Dixon, Lucas and Zhang, Kun and Le, Quoc V. and Wu, Yonghui and Chen, Zhifeng and Cui, Claire},
	pages = {5547--5569},
     year = {2022}
}

@inproceedings{lepikhin_gshard_2021,
	title = {{GShard}: {Scaling} {Giant} {Models} with {Conditional} {Computation} and {Automatic} {Sharding}},
	shorttitle = {{GShard}},
	booktitle = {{International} {Conference} on {Learning} {Representations}},
	author = {Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
	year = {2021},
}


@article{fedus_switch_2022,
	title = {Switch {Transformers}: {Scaling} to {Trillion} {Parameter} {Models} with {Simple} and {Efficient} {Sparsity}},
	volume = {23},
	number = {120},
	journal = {The Journal of Machine Learning Research},
	author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
	year = {2022},
	pages = {5232–5270},
}

@misc{wikidump,
   author = {{Wikimedia Foundation}},
   title = {Wikimedia {Downloads}},
   howpublished = {\href{https://dumps.wikimedia.org}{\url{https://dumps.wikimedia.org}}},
   year={2023}
}

@article{imagenet15russakovsky,
    Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
    Title = { {ImageNet Large Scale Visual Recognition Challenge} },
    Year = {2015},
    journal   = {International Journal of Computer Vision},
    volume={115},
    number={3},
    pages={211-252}
}

@inproceedings{kingma_adam_2015,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	booktitle = {{International} {Conference} on {Learning} {Representations}},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	year = {2015},
}

@article{chowdhery_palm_2023,
	title = {{PaLM}: {Scaling} {Language} {Modeling} with {Pathways}},
	volume = {24},
	journal = {Journal of Machine Learning Research},
	author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
	year = {2023},
	pages = {240:1--240:113},
}


@article{touvron_llama_2023,
	title = {{LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}},
	volume = {abs/2302.13971},
	journal = {CoRR},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurélien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
	year = {2023},
}

@article{touvron_llama_2023-1,
	title = {Llama 2: {Open} {Foundation} and {Fine}-{Tuned} {Chat} {Models}},
	volume = {abs/2307.09288},
	journal = {CoRR},
	author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Canton-Ferrer, Cristian and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurélien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
	year = {2023},
}


@inproceedings{dao_flashattention_2022,
	title = {{FlashAttention}: {Fast} and {Memory}-{Efficient} {Exact} {Attention} with {IO}-{Awareness}},
	booktitle = {{Advances in {Neural} {Information} {Processing} {Systems} 35}},
	author = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and Ré, Christopher},
	year = {2022},
    pages = {16344--16359}
}

@article{dao_flashattention-2_2023,
	title = {{FlashAttention}-2: {Faster} {Attention} with {Better} {Parallelism} and {Work} {Partitioning}},
	volume = {abs/2307.08691},
	journal = {CoRR},
	author = {Dao, Tri},
	year = {2023},
}


@inproceedings{yuan_decentralized_2022,
	title = {Decentralized {Training} of {Foundation} {Models} in {Heterogeneous} {Environments}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 35},
	author = {Yuan, Binhang and He, Yongjun and Davis, Jared and Zhang, Tianyi and Dao, Tri and Chen, Beidi and Liang, Percy and Ré, Christopher and Zhang, Ce},
	year = {2022},
    pages = {25464--25477}
}


@inproceedings{wolf_transformers_2020,
	title = {Transformers: {State}-of-the-{Art} {Natural} {Language} {Processing}},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Rémi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and Platen, Patrick von and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
	year = {2020},
	pages = {38--45},
}

@inproceedings{hu_lora_2022,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	booktitle = {{International} {Conference} on {Learning} {Representations}},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	year = {2022},
}

@inproceedings{li_sequence_2023,
	title = {Sequence {Parallelism}: {Long} {Sequence} {Training} from {System} {Perspective}},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	author = {Li, Shenggui and Xue, Fuzhao and Baranwal, Chaitanya and Li, Yongbin and You, Yang},
	year = {2023},
	pages = {2391--2404},
}

@article{jacobs_deepspeed_2023,
	title = {{DeepSpeed} {Ulysses}: {System} {Optimizations} for {Enabling} {Training} of {Extreme} {Long} {Sequence} {Transformer} {Models}},
	volume = {abs/2309.14509},
	journal = {CoRR},
	author = {Jacobs, Sam Ade and Tanaka, Masahiro and Zhang, Chengming and Zhang, Minjia and Song, Shuaiwen Leon and Rajbhandari, Samyam and He, Yuxiong},
	year = {2023},
}


@inproceedings{shazeer_outrageously_2017,
	title = {Outrageously {Large} {Neural} {Networks}: {The} {Sparsely}-{Gated} {Mixture}-of-{Experts} {Layer}},
	booktitle = {{International} {Conference} on {Learning} {Representations}},
	author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc V. and Hinton, Geoffrey E. and Dean, Jeff},
	year = {2017},
}

@inproceedings{he_fastermoe_2022,
	title = {{FasterMoE}: Modeling and Optimizing Training of Large-scale Dynamic Pre-trained Models},
	booktitle = {Proceedings of the 27th {ACM} {SIGPLAN} {Symposium} on {Principles} and {Practice} of {Parallel} {Programming}},
	author = {He, Jiaao and Zhai, Jidong and Antunes, Tiago and Wang, Haojie and Luo, Fuwen and Shi, Shangfeng and Li, Qin},
	year = {2022},
	pages = {120--134},
}