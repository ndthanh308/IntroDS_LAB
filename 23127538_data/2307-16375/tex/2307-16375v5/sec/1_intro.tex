\section{Introduction}
\label{sec:introduction}
Deep learning models have demonstrated promising performance across many domains. 
For example, deep learning models such as BERT~\citep{devlin_bert_2019}, GPT-3~\citep{brown_language_2020} and T5~\citep{raffel_exploring_2020} achieve state-of-the-art~(SOTA) performance on many natural language processing~(NLP) tasks. 
For computer vision~(CV), deep learning models such as ViT~\citep{dosovitskiy_image_2021} and Swin Transformer~\citep{liu_swin_2021} achieve good accuracy on multiple tasks. 

Distributed learning~(also called parallel learning) on clusters with several machines or GPUs is commonly used for training deep learning models, especially for some large models with billions of parameters~\citep{brown_language_2020,touvron_llama_2023,touvron_llama_2023-1}. Several parallel strategies, including pipeline parallelism~(PP), data parallelism~(DP), tensor parallelism~(TP), and fully sharded data parallelism~(FSDP), have been proposed for distributed learning. These parallel strategies can be divided into two main categories: inter-layer parallelism and intra-layer parallelism. Inter-layer parallelism~\citep{huang_gpipe_2019,narayanan_pipedream_2019,narayanan_memory-efficient_2021,fan_dapple_2021,li_chimera_2021,lepikhin_gshard_2021,du_glam_2022,fedus_switch_2022}, which includes PP, partitions the model into disjoint sets without partitioning tensors in each layer. 
Intra-layer parallelism~\citep{li_pytorch_2020,rasley_deepspeed_2020,narayanan_efficient_2021,fairscale_authors_fairscale_2021}, which includes DP, TP, and FSDP, partitions tensors in a layer along one or more axes.

 
The parallel method\footnote{To avoid confusion, we treat `parallel method' and `parallel strategy' as two different terminologies in this paper.} in one specific distributed learning method or system typically adopts one parallel strategy or a combination of several parallel strategies. 
Existing parallel methods can be divided into two categories: manual parallelism~(MP) methods and automatic parallelism~(AP) methods. %Different combinations of parallel strategies will significantly affect the training efficiency of the    distributed learning process. To enhance the training efficiency, researchers have proposed some manually designed parallel training strategies~\citep{narayanan_efficient_2021,shazeer_mesh-tensorflow_2018,xu_gspmd_2021}. 
In MP methods ~\citep{shazeer_mesh-tensorflow_2018,narayanan_efficient_2021,xu_gspmd_2021}, one or several parallel strategies are manually optimized by researchers or developers. MP methods require extensive domain knowledge in deep learning models and hardware architectures. With the rapid development of deep learning models and the increasing diversity of modern hardware architectures~\citep{flynn_very_1966,flynn_computer_1972}, MP methods demand considerable human effort and have limited flexibility.


To address the two limitations of MP methods, AP methods~\citep{narayanan_pipedream_2019,he_pipetransformer_2021,zheng_alpa_2022} have recently been
proposed for automating the parallel strategy optimization process. Although existing AP methods have achieved promising progress, they optimize the two categories of parallel strategies separately rather than jointly. 
More specifically, some methods optimize only one category of parallel strategies~\citep{jia_exploring_2018,wang_supporting_2019,jia_beyond_2019,schaarschmidt_automap_2021,zhao_vpipe_2022,cai_tensoropt_2022,liu_colossal-auto_2023}, and the others optimize inter- and intra-layer parallelism hierarchically~\citep{narayanan_pipedream_2019,tarnawski_efficient_2020,tarnawski_piper_2021,fan_dapple_2021,he_pipetransformer_2021,zheng_alpa_2022}. Hence, existing AP methods suffer from sub-optimal solutions.


%We hereby summarize our contributions as follows:
In this paper, we propose a novel AP method called UniAP for distributed learning. The contributions of UniAP are outlined as follows: 
\begin{itemize}
\item 
 UniAP unifies inter- and intra-layer automatic parallelism by mixed integer quadratic programming~(MIQP)~\citep{lazimy_mixed_1982}. 
 \item To the best of our knowledge, UniAP is the first parallel method that can optimize the two categories of parallel strategies jointly rather than separately to find an optimal solution.
 \item Experimental results show that UniAP outperforms state-of-the-art methods by up to 3.80$\times$ in throughput and reduces strategy optimization time by up to 107$\times$ across five Transformer-based models.
\end{itemize}