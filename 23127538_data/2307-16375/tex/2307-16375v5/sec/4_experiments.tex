
\section{Experiment}
\label{sec:experiment}
\subsection{Experiment setup}
\label{sec:experiment-setup}
We conduct experiments on four different kinds of environments to validate the effectiveness and universality of our method. \textsc{EnvA} refers to a node~(machine) with 1 Xeon 6248 CPU, 8 V100-SXM2 32GB GPUs, and 472GB memory. \textsc{EnvB} refers to 4 nodes interconnected with 10Gbps networks and each node has 2 Xeon E5-2620 v4 CPUs, 4 TITAN Xp 12GB GPUs, and 125GB memory. \textsc{EnvC} refers to a node with 8 A100 40GB PCIe GPUs.  \textsc{EnvD} is a cloud cluster with 16 nodes and each node has 4 DCUs~(a type of non-NVIDIA GPU). In the following text, we will specify the number of GPUs used after the environment name (e.g., \textsc{EnvB-8}) only when partial nodes of the environment are used. 

We evaluate UniAP with five Transformer-based models, BERT-Huge~\citep{devlin_bert_2019}, T5-Large~\citep{raffel_exploring_2020}, ViT-Huge~\citep{dosovitskiy_image_2021}, Swin-Huge~\citep{liu_swin_2021}, and Llama~\citep{touvron_llama_2023,touvron_llama_2023-1}. We follow the common practice of training these transformer-based models. To eliminate factors that affect training throughput, we turn off techniques orthogonal to parallel strategies, such as activation checkpointing~\citep{chen_training_2016}. However, we integrate FP16 mixed precision training~\citep{micikevicius_mixed_2018} for the largest model, Llama, in some cases to successfully orchestrate the process. More details on these models are provided in Appendix~\ref{appendix:experiment-settings}.

The experimental evaluation concentrates on two primary metrics: training throughput and strategy optimization time. The former is calculated by averaging throughput from the 10th to the 60th iteration of training, while the latter is determined by measuring the time of the UOP. More details are provided in Appendix \ref{appendix:experiment-settings}.  Besides, we provide the accuracy of our cost model in Appendix \ref{appendix:experiment:performance_modeling}.

\begin{table*}[t]
    \begin{center}
    \begin{small}
    \begin{threeparttable}
    \begin{tabular}{cccccccc}
    \toprule
     \multirow{2}[0]{*}{Env.} & \multirow{2}[0]{*}{Model} & \multicolumn{3}{c}{Training throughput (samples/s)} & \multirow{2}[0]{*}{\makecell{Minimum\\speedup}} & \multirow{2}[0]{*}{\makecell{Maximum\\speedup}}\\
     \cmidrule(r){3-5}
    & & Galvatron & Alpa & UniAP &  \\
    \midrule
   \multirow{4}[0]{*}{\textsc{EnvA}} & BERT-Huge &  \textbf{33.46~$\pm$~0.28 } &  31.56~$\pm$~0.04 &  \textbf{33.46~$\pm$~0.28 } & 1.00 &  1.06 \\
    & T5-Large & \textbf{23.29~$\pm$~0.04} &  \texttt{MEM}$\times$\tnote{2)} &  \textbf{23.29~$\pm$~0.04} & 1.00 &  1.00 \\
    & ViT-Huge &   \textbf{109.51~$\pm$~0.07 } &  97.66~$\pm$~1.42 &  \textbf{109.51~$\pm$~0.07 } & 1.00 &  1.12 \\
    & Swin-Huge & \verb+CUDA+$\times$\tnote{3)} & \verb+N/A+\tnote{4)}  &  \textbf{67.96~$\pm$~0.12 } & \verb+N/A+\tnote{4)} & \verb+N/A+\tnote{4)}\\
    \midrule
    \multirow{4}[0]{*}{\textsc{EnvB-8}}
    & BERT-Huge &  6.27~$\pm$~0.17 &  8.95~$\pm$~0.06 &  \textbf{10.77~$\pm$~0.13} &  1.20 &  1.71 \\
    & T5-Large\tnote{1)} &  \textbf{8.06~$\pm$~0.06} & \verb+MEM+$\times$\tnote{2)}  &  7.98~$\pm$~0.05 &  0.99  &  0.99\\
    & ViT-Huge  &  32.20~$\pm$~0.17 &  38.74~$\pm$~0.20 &  \textbf{45.58~$\pm$~0.54} &  1.18 &  1.41 \\
    & Swin-Huge &  13.90~$\pm$~0.17 & \verb+N/A+\tnote{4)} &  \textbf{19.08~$\pm$~0.10} &  1.37 &  1.37\\
    \midrule
    \textsc{EnvC} & Llama-7B & 1.22~$\pm$~0.01 & \verb+N/A+\tnote{4)} & \textbf{4.63~$\pm$~0.007} & 3.80 & 3.80 \\
    \midrule
    \midrule
    \multirow{2}[0]{*}{Env.} & \multirow{2}[0]{*}{Model} & \multicolumn{3}{c}{Strategy optimization time (min.)} & \multirow{2}[0]{*}{\makecell{Minimum\\speedup}} & \multirow{2}[0]{*}{\makecell{Maximum\\ speedup}}\\
     \cmidrule(r){3-5}
    & & Galvatron & Alpa & UniAP &  \\
    \midrule
    \multirow{4}[0]{*}{\textsc{EnvA}}
    & BERT-Huge &  6.44~$\pm$~0.588 &  $>$~40 &  \textbf{0.37~$\pm$~0.002} &  17.29 &  $>$~107.41\\
    & T5-Large &  12.41~$\pm$~0.122 &  \texttt{MEM}$\times$\tnote{2)} &  \textbf{0.89~$\pm$~0.007} &  13.98 &  13.98\\
    & ViT-Huge &  6.29~$\pm$~0.464 &  $>$~40 &  \textbf{0.57~$\pm$~0.009} &  10.95 &  $>$~69.60 \\
    & Swin-Huge &  11.88~$\pm$~0.666 & \verb+N/A+\tnote{4)} &  \textbf{2.16~$\pm$~0.004} &  5.49 &  5.49 \\
    \midrule
    \multirow{4}[0]{*}{\textsc{EnvB-8}}
    & BERT-Huge &  2.04~$\pm$~0.010 &  $>$~40 &  \textbf{1.51~$\pm$~0.005} &  1.34 &  $>$~26.32\\
    & T5-Large\tnote{1)} &  2.64~$\pm$~0.110 & \verb+MEM+$\times$\tnote{2)} &  \textbf{0.91~$\pm$~0.005} &  2.90 &  2.90\\
    & ViT-Huge &  2.37~$\pm$~0.180 &  $>$~40 &  \textbf{1.11~$\pm$~0.011} &  2.14 &  $>$~36.01\\
    & Swin-Huge &  4.29~$\pm$~0.320 & \verb+N/A+\tnote{4)} &  \textbf{2.29~$\pm$~0.010} &  1.87 &  1.87\\
    \midrule
    \textsc{EnvC} & Llama-7B & 6.84~$\pm$~0.055 & \verb+N/A+\tnote{4)} & \textbf{0.58~$\pm$~0.006} & 11.83 & 11.83 \\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
    \footnotesize
    \item[1)] T5-Large tested on \textsc{EnvB} is restricted to 16/16 layers to avoid out-of-memory~(OOM) exceptions.
    \item[2)] \texttt{MEM}$\times$: OOM exceptions during strategy optimization.
    \item[3)] \texttt{CUDA}$\times$: CUDA OOM exceptions during model training.
    \item[4)] The official implementation for Swin-Huge and Llama in Alpa is absent. We have endeavored to adopt the code, yet encountered compilation errors. Consequently, experiments related to these models on Alpa are marked as \texttt{N/A}.
    \end{tablenotes}
    \end{threeparttable}
\end{small}
\end{center}
\vskip -0.2in
\caption{Training throughput and strategy optimization time on \textsc{EnvA}, \textsc{EnvB-8}, and \textsc{EnvC}.}
\label{tab:throughput_and_optimization_time}
\end{table*}

\begin{table*}[t]
\begin{center}
\begin{small}
% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{threeparttable}
\begin{tabular}{ccccccc}
    \toprule
    \multirow{2}[0]{*}{Model}  & \multicolumn{3}{c}{\makecell{Training throughput (samples/s)}} & \multicolumn{3}{c}{\makecell{Strategy optimization time (min.)}} \\
    \cmidrule(lr){2-4}\cmidrule(lr){5-7}
           & Megatron & DeepSpeed & UniAP & Megatron & DeepSpeed & UniAP \\
    \midrule
    Llama-7B  & \textbf{2.01~$\pm$~0.005}  & \texttt{SOL}$\times$\tnote{1)}  & \textbf{2.01~$\pm$~0.005}  & > 8.0 hours & \texttt{SOL}$\times$\tnote{1)}  & \textbf{3.07~$\pm$~0.121}  \\
    Llama-13B &  \textbf{0.82~$\pm$~0.001}  &   \texttt{SOL}$\times$\tnote{1)}    &  \textbf{0.82~$\pm$~0.001}     &    > 2.5 hours   &    \texttt{SOL}$\times$\tnote{1)}   & \textbf{1.95~$\pm$~0.076} \\
\bottomrule
\end{tabular}

\begin{tablenotes}
    \footnotesize
    \item[1)] \texttt{SOL}$\times$: No solution after strategy optimization.

\end{tablenotes}

\end{threeparttable}
\end{small}
\end{center}
\vskip -0.2in
\caption{Training throughput and strategy optimization time on \textsc{EnvD-32}.}
\label{tab:dcu}%
\vskip -0.2in
\end{table*}

% % Figure environment removed

\begin{table*}[t]
\begin{center}
\begin{small}
% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{threeparttable}
\begin{tabular}{p{2.5em}cp{2.5em}ccccccc}
    \toprule
    &\multirow{2}[0]{*}{Model}& & \multirow{2}[0]{*}{Batch size} & \multicolumn{4}{c}{Training throughput (samples/s)} & \multirow{2}[0]{*}{\#infeasible\tnote{5)}} & \multirow{2}[0]{*}{\#candidate\tnote{6)}} \\
     \cmidrule(lr){5-8}
    & && & 1 st.\tnote{1)} & 2 nd.\tnote{2)} & Slowest\tnote{3)} & Median\tnote{4)} & & \\
    \midrule
    &Llama-7B& & 8 & 2.01 & 1.92 & 0.22  & 0.82  & 41 & 64 \\
    &Llama-13B& & 4 & 0.82 & 0.58  & 0.27  & 0.42  &  42  & 48 \\
\bottomrule
\end{tabular}
\begin{tablenotes}
    \footnotesize
    \item[1)] 1 st.: The training throughput achieved by the fastest parallel strategy.
    \item[2)] 2 nd.: The training throughput achieved by the second fastest parallel strategy.
    \item[3)] Slowest: The training throughput achieved by the slowest parallel strategy.
    \item[4)] Median: The median value of training throughputs across parallel strategies that will successfully train the model.
    \item[5)] \#infeasible: The number of parallel strategies that will encounter exceptions such as CUDA OOM during model training.
    \item[6)] \#candidate: The total number of candidate parallel strategies.
\end{tablenotes}
\end{threeparttable}
\end{small}
\end{center}
\vskip -0.2in
\caption{Statistics on the candidate parallel strategies for Megatron.}
\label{tab:statistics-on-megatron}%
\vskip -1.5em
\end{table*}





\subsection{Training throughput and strategy optimization time}\label{subsec:experiment:optimization-time-and-throughput}
We compare the training throughput and strategy optimization time of UniAP with those of the baselines on \textsc{EnvA}, \textsc{EnvB-8}, \textsc{EnvC}, and \textsc{EnvD-32}. For experiments on \textsc{EnvA}, we set the mini-batch size to be 32, 16, 128, and 128 for BERT, T5, ViT, and Swin, respectively. For experiments on \textsc{EnvB-8}, we set the mini-batch size to be 16, 8, 64, and 32 for these four models, respectively. For the Llama model~\citep{touvron_llama_2023,touvron_llama_2023-1} run on \textsc{EnvC}, we set the mini-batch size to be 8. For experiments on \textsc{EnvA}, \textsc{EnvB-8}, and \textsc{EnvC}, we choose Galvatron~\citep{miao_galvatron_2022} and Alpa~\citep{zheng_alpa_2022} as baselines because they have achieved SOTA performance. Specifically, Galvatron has surpassed other methods, including PyTorch DDP~\citep{li_pytorch_2020}, Megatron-LM~\citep{narayanan_efficient_2021}, FSDP~\citep{rajbhandari_zero_2020,fairscale_authors_fairscale_2021}, GPipe~\citep{huang_gpipe_2019}, and DeepSpeed 3D~\citep{deepspeed-3d} in terms of training throughput, as reported in the original paper~\citep{miao_galvatron_2022}. Furthermore, Alpa utilizes the Just-In-Time~(JIT) compilation feature in JAX and outperforms Megatron-LM and DeepSpeed.

For benchmarks on  \textsc{EnvD-32}, pytorch libraries and frameworks need specialized adaptations due to the heterogeneity of DCUs against NVIDIA GPUs. Thus, this part of experiments is based on adapted pytorch libraries and Megatron-DeepSpeed\citep{deepspeed-3d} framework provided by the platform developers, and we select MP methods including Megatron~\citep{narayanan_efficient_2021} and DeepSpeed~(ZeRO-3)~\citep{rasley_deepspeed_2020} as our baseline methods. We set the mini-batch size as 8 for Llama-7B and 4 for Llama-13B. The remaining configurations align with those for Llama-7B in Section~\ref{sec:experiment-setup}, such as activating FP16 mixed precision training.
 
Table~\ref{tab:throughput_and_optimization_time} shows the training throughput and strategy optimization time on \textsc{EnvA}, \textsc{EnvB-8}, and \textsc{EnvC}. On \textsc{EnvA}, UniAP and Galvatron get the same optimal strategy for BERT-Huge, T5-Large, and ViT-Huge, outperforming Alpa in terms of training throughput and strategy optimization time. In addition, UniAP finds a solution for Swin-Huge, while Galvatron encounters CUDA OOM issues. In particular, UniAP achieves a maximum optimization speedup that is 17$\times$ faster than Galvatron and hundreds of times faster than Alpa on BERT-Huge. This is mainly due to the ability of the MIQP solver to search for an optimal strategy on multiple threads, while the dynamic programming based methods like Galvatron and Alpa run on a single thread due to their strong data dependency.

On \textsc{EnvB-8}, UniAP consistently demonstrates competitive or larger training throughput compared to Galvatron and Alpa. We attribute the performance improvement to the larger strategy space in UniAP. A detailed study is provided in Section~\ref{subsec:experiment:further study}. Furthermore, the strategy optimization time of UniAP is also significantly shorter than the two baseline methods.

On \textsc{EnvC}, UniAP shows an optimization speedup of 11.83$\times$ and a training speedup of 3.80$\times$ compared to Galvatron on Llama. We examine the parallel strategy they adopted. UniAP employs an 8-stage PP with a micro-batch size of 1, whereas Galvatron employs a 4-stage PP with a micro-batch size of 3 (with the final micro-batch containing 2 samples). Within each PP stage, Galvatron utilizes a 2-way TP. For \textsc{EnvC}, which is equipped with 8 A100 40GB PCIe GPUs, minimizing communication volume is critical. Since PP has significantly less inter-device communication volume than TP, the parallel strategy discovered by UniAP is more reasonable than Galvatron in this case.

Table~\ref{tab:dcu} shows the results on \textsc{EnvD-32}. On this environment, UniAP consistently identifies the fastest parallel strategies while expending considerably less time on strategy optimization compared to Megatron~(about 157$\times$ faster for Llama-7B and 77$\times$ faster for Llama-13B). Please note that the strategy optimization time Megatron needs for Llama-7B surpasses that for Llama-13B. We attribute this phenomenon to the variations in the mini-batch size. Specifically, the escalation from a mini-batch size of 4 (employed in Llama-13B) to 8 (employed in Llama-7B) leads to a rise in the number of candidate parallel strategies, as well as the parallel strategies that will successfully train the model. As a result, the strategy optimization time will become increasingly long when the mini-batch size exceeds 8 in the pretraining scenario~\citep{brown_language_2020,touvron_llama_2023,touvron_llama_2023-1}. In this situation, UniAP will identify the optimal parallel strategy much faster.

Furthermore, our experiments highlight a limitation encountered with DeepSpeed~(ZeRO-3). It requires the mini-batch size to be divisible evenly by the total number of computation devices. This specific prerequisite prevents DeepSpeed from successfully launching the training process with 32 DCUs.

To facilitate further discussions, we provide a case study of the optimal parallel strategy in Appendix~\ref{appendix:case-study}.


% Figure environment removed

\begin{table}[t]
\begin{center}
\begin{small}
% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{threeparttable}
\begin{tabular}{ccc}
    \toprule
    Number of & Training throughput  & Strategy optimization\\
    DCUs &(samples/s) & time(min.) \\
    \midrule
    16 &0.1726~$\pm$~0.0005&0.75~$\pm$~0.0100 \\
    32 &0.3393~$\pm$~0.0004&1.64~$\pm$~0.0321 \\
    64 &0.6656~$\pm$~0.0023&1.55~$\pm$~0.0058 \\
\bottomrule
\end{tabular}
\end{threeparttable}
\end{small}
\end{center}
\vskip -0.15in
\caption{Scalability on training throughput among DCU clusters.}
\label{tab:scalability-on-dcu}%
\vskip -1em
\end{table}

\subsection{Scalability}\label{subsec:experiment:scalability}

We study the scalability of UniAP using BERT, T5, ViT, and Swin on \textsc{EnvB}. We set the mini-batch sizes for each data point as 8, 4, 32, and 16 times the number of nodes~(denoted as `\#nodes'). The experimental results are shown in Figure~\ref{fig:scalability}. In Figure~\ref{fig:scalability:throughput}, the training throughput of the optimal strategy demonstrate near-linearity as the number of nodes and mini-batch size increase. In Figure~\ref{fig:scalability:optimization-time}, the strategy optimization time matches the complexity analysis in Section~\ref{subsec:method:complexity-analysis}.

Additionally, we use DCU nodes in \textsc{EnvD} to scale UniAP to larger clusters on Llama-7B, with precision of FP32. We conducted experiments on clusters with 16, 32, and 64 DCUs, where we set the mini-batch size to 1/8 of the number of DCUs (i.e., for a cluster with 16 DCUs, we set the mini-batch size to 2). The experimental results are shown in Table \ref{tab:scalability-on-dcu}, further demonstrating the near-linear scalability of UniAP.


% and the corresponding strategy optimization time demonstrate near-linearity This phenomenon reveals that UniAP is scalable and verifies the computational complexity analysis in Section~\ref{subsec:method:unified-optimization}.


\subsection{Further study on the significance of UniAP}
\label{subsec:experiment:further study}
Finally, we investigate the significance of UniAP by answering two questions.
The first question is why we need AP rather than MP. We examine the statistics on the candidate parallel strategies for Megatron, as shown in Table~\ref{tab:statistics-on-megatron}. With MP, most users are unfamiliar with the complex hyper-parameter detail of the candidate parallel strategies and typically choose a parallel strategy at random, which will result in a 64.1\% (41/64) chance of failing to train Llama-7B and an 87.5\% (42/48) chance of failing to train Llama-13B even when the hardware resources are sufficient. Even if they successfully eliminate all infeasible parallel strategies, they still only have a 50\% chance of identifying a parallel strategy with training throughput better than 0.82 samples/s for Llama-7B and 0.42 samples/s for Llama-13B. When the users can't identify the fastest parallel strategy out of hundreds of candidates, they may sacrifice at least 4.4\% of the training throughput for Llama-7B and 29.2\% for Llama-13B. In such circumstances, UniAP is able to identify the fastest strategy, as shown in Table~\ref{tab:dcu}. 

% Figure environment removed

%\begin{table}
%\vskip -0.2in
%\caption{Ablation study on the importance of unifying strategy space.}
%\label{tab:ablation}%
%\begin{center}
%\begin{small}
%\begin{threeparttable}
%    \begin{tabular}{cccc}
%    \toprule
%    \multirow{3}[0]{*}{Model} & \multicolumn{3}{c}{Training throughput (samples/s)} \\
%    \cmidrule(r){2-4}
%          & UniAP (Inter-only) & UniAP (Intra-only) & UniAP \\
%    \midrule
%    BERT-Huge &  \texttt{SOL}$\times$\tnote{1)} &  2.48~$\pm$~0.02 &  \textbf{10.77~$\pm$~0.13} \\
%    T5-Large &  \texttt{SOL}$\times$\tnote{1}   &  2.92~$\pm$~0.01 &  \textbf{9.01~$\pm$~0.06} \\
%    ViT-Huge &  \textbf{45.58~$\pm$~0.54} & \verb+CUDA+$\times$\tnote{2}   &  \textbf{45.58~$\pm$~0.54} \\
%    Swin-Huge &  \textbf{19.08~$\pm$~0.10} &  4.66~$\pm$~0.02 &  \textbf{19.08~$\pm$~0.10} \\
%    \bottomrule
%\end{tabular}%
%\begin{tablenotes}
    %\footnotesize
    %\item[1)] \texttt{SOL}$\times$: No solution after strategy optimization.
    %\item[2)] \texttt{CUDA}$\times$: CUDA OOM exceptions during model training.
%\end{tablenotes}
%\end{threeparttable}
%\end{small}
%\end{center}
%\vskip -0.2in
%\end{table}%

The second question is why UniAP achieves better performance than other AP methods.
We investigate the importance of the strategy space for the optimality of parallel strategies with an ablation study. Specifically, we constrain the strategy space to inter-layer-only and intra-layer-only strategies and evaluate the training throughput of the resulting optimal strategy on \textsc{EnvB}. We set the mini-batch sizes to be 16, 12, 64, and 32, respectively. 
Results are shown in Figure~\ref{fig:ablation}. We can find that constraining the strategy space compromises the optimality of parallel strategies or gets strategies that encounter OOM across different models. 
%For example, constraining the strategy space to inter-layer-only parallelism yield infeasible solution for BERT-Huge on \textsc{EnvD}. Meanwhile, constraining the strategy space to intra-layer-only parallelism may reduce the training throughput by 4.34$\times$ for BERT-Huge on \textsc{EnvD}.
Hence, unifying inter- and intra-layer AP for joint optimization makes UniAP outperform other AP methods. 

% \begin{table}[t]
% \caption{Scalability on training throughput among DCU clusters.}
% \vskip -0.2in
% \label{tab:scalability-on-dcu}%
% \begin{center}
% \begin{small}
% % Table generated by Excel2LaTeX from sheet 'Sheet1'
% \begin{threeparttable}
% \begin{tabular}{ccc}
%     \toprule
%     \multirow{2}[0]{*}{Number of DCUs} & \multicolumn{2}{c}{\makecell{Training throughput (samples/s)}} \\
%     \cmidrule(lr){2-3} &
%     UniAP & Galvatron \\
%     \midrule
%     16 & \textbf{0.1726~$\pm$~0.0005}&\texttt{SOL}$\times$\tnote{1)} \\
%     32 & \textbf{0.3393~$\pm$~0.0004}&0.2809~$\pm$~0.0002 \\
%     64 & \textbf{0.6656~$\pm$~0.0023}&0.4589~$\pm$~0.0014 \\
% \bottomrule
% \end{tabular}
% \begin{tablenotes}
%     \footnotesize
%     \item[1)] \texttt{SOL}$\times$: No solution after strategy optimization.
% \end{tablenotes}
% \end{threeparttable}
% \end{small}
% \end{center}
% \vskip -0.35in
% \end{table}

% Furthermore, we conducted experiments on DCU clusters with 16, 32, and 64 DCUs, where we set the mini-batch size to 1/8 of the number of DCUs (i.e., for a cluster with 16 DCUs, we set the mini-batch size to 2). The experimental results are shown in Table \ref{tab:scalability-on-dcu}, demonstrating the near-linear scalability of UniAP and showing that UniAP still outperforms Galvatron in these environments.
 
 
% We choose widely used distributed training framework Magatron,  DeepSpeed and Megatron-DeepSpeed as baselines. Due to the design of DCU, pytorch libraries and frameworks need specialized adaptations. So this part of experiments is based on adapted pytorch libraries and Megatron-DeepSpeed\citep{deepspeed-3d} framework provided by the platform developers. Specifically, we only choose the cases where the parallel strategies we get using UniAP are supported by Megatron-DeepSpeed for comparison.Note that all strategies in baselines are naturally supported by Megatron-DeepSpeed. In more details, the strategy space for Megatron is any combination of TP and PP, the strategy space for DeepSpeed is pure FSDP, the strategy space for Megatron-DeepSpeed is any combination of DP, TP, PP and FSDP except strategies contain PP and FSDP at the same time. The results are shown in Table~\ref{tab:dcu}.

%\section{Limitation}
%\label{appendix:limitation}
%UniAP is currently designed and tested on homogeneous clusters, but incorporating automatic parallelism for training deep models on heterogeneous clusters~(e.g., a cluster equipped with both NVIDIA GPUs and DCUs) is another important research topic. Given that current parallel techniques primarily target homogeneous clusters with limited emphasis on heterogeneous clusters, we have chosen to leave this topic for future exploration.
% Despite its advantages, UniAP has limitations at present. 
% \begin{enumerate}
%     % \item UniAP does not validate other inter-layer parallelism mechanisms except for the GPipe-style PP. Some alternative inter-layer parallelism approaches\cite{fan_dapple_2021,narayanan_efficient_2021} may result in less memory footprint or higher training throughput than GPipe. 
%     \item UniAP is designed  We left it as our future work.
%     \item UniAP mainly focuses on optimizing common parallel strategies like pipeline parallelism, data parallelism, tensor parallelism, and fully-sharded data parallelism. However, there are emerging parallel strategies, such as sequence parallelism~\citep{li_sequence_2023,jacobs_deepspeed_2023} and expert parallelism~\citep{shazeer_outrageously_2017,he_fastermoe_2022}. We plan to incorporate them into our framework in the future.
%     \item UniAP does not automatically optimize other training techniques, such as mixed precision training~\citep{micikevicius_mixed_2018}, activation recomputation~\citep{chen_training_2016}, and low-rank methods~\citep{hu_lora_2022}. Due to the orthogonal nature of these techniques to our approach, we decided to left them as our future work.
%     % \item Due to the limited hardware resources, we have yet to conduct experiments for UniAP on Transformer models with larger parameter sizes. Therefore, the effectiveness of UniAP on larger models and larger clusters remains to be validated in the future.
% \end{enumerate}



%\section{Broader impact}
%\label{appendix:broader-impact}
%We cannot foresee any direct negative social impacts for UniAP currently. However, training a Transformer-like model requires significant energy, imposing a substantial burden on the planet. UniAP can significantly accelerate the training process of deep learning models, thereby minimizing energy consumption as much as possible.