\newpage
\appendix
\onecolumn

{\fontsize{20pt}{30pt}\selectfont
Appendix
}

\section{Visualization for parallel methods}
\label{appendix:different-parallel-methods}


In order to provide an intuitive analysis, we present visualizations in Figure~\ref{fig:parallel_methods} on different parallel methods for optimizing parallel strategies for a three-layer model. The numbered boxes represent the corresponding layers of the model. The layers with gray backgrounds represent the final parallel strategy for each method. The layers without dotted lines indicate that DP was applied to them, while TP was applied to the others. The direction of the dotted lines indicates the dimension of the tensor that TP is applied to. The solid arrows between layers represent connections within a single pipeline stage, while the dashed arrows represent connections between consecutive pipeline stages. For example, the parallel strategy optimized by MP (Figure~\ref{fig:parallel-method:MP}) represents that PP and TP are utilized to train the model. Specifically, layer 0 of the model is assigned to the first pipeline stage, while layer 1 and 2 are assigned to the second pipeline stage. Additionally, TP is activated across all model layers.


As shown in Figure~\ref{fig:parallel-method:MP}, MP methods are manually designed and rely on expert knowledge, resulting in limited flexibility. Inter-layer-only and intra-layer-only AP methods (Figure~\ref{fig:parallel-method:inter} and \ref{fig:parallel-method:intra}) optimize~(search) from a set of candidate inter-layer-only and intra-layer-only parallel strategies, respectively. 
To the best of our knowledge, existing inter- and intra-layer parallelism AP methods optimize parallel strategies hierarchically. Thus, we refer to them as Hierarchical AP in Figure~\ref{fig:parallel-method:hierarchical}. They initially adopt algorithms, such as greedy or dynamic programming, to propose candidate inter-layer parallel strategies. Subsequently, they optimize the intra-layer parallel strategy for every fixed inter-layer parallel strategy. Compared to them, UniAP jointly optimize all possible parallel strategies from intra-layer-only parallelism to inter- and intra-layer parallelism. Consequently, UniAP has the largest strategy space for exploration.


\section{Proof of the linear form for the contiguous set}\label{appendix:linear-form-of-contigous-constraint}
To facilitate our discussion, we adopt the linear form of the order-preserving constraint as presented in the main paper. We denote $\P_{ui}$ as a 0-1 variable indicating whether layer $u$ is to be placed on the $i$-th computation stage, $pp\_size$ as the number of computation stages in the pipeline. Besides, $\mathcal{G}(\VB, \EB)$ represents the computation graph for the model. Then, we formalize the theorem as follows:

\begin{theorem}
    A subgraph $\VB_i=\{\forall u\in \VB: \P_{ui}=1\}$ is contiguous if and only if there exists $\Z_{vi}$ such that Equation~\eqref{eqn:method:order-preserving:1}, \eqref{eqn:method:order-preserving:2}, and \eqref{eqn:method:order-preserving:3} are satisfied.
\end{theorem}

Previous work~\citep{tarnawski_efficient_2020} has proven this theorem. Our proof draws on the process of this work. The details of the proof are as follows:

\begin{proof}
\vskip -0.1in
    "If": Assume that there exists nodes $u, w\in \VB_i$ and $v \notin \VB_i$ such that $v$ and $w$ are reachable from $u$ and $v$, respectively. Hence, $\P_{ui} = 1$, $\P_{wi} = 1$, and $\P_{vi} = 0$. Without losing generality, we assume $\langle u, v\rangle\in \EB$. Thus, according to Equation \eqref{eqn:method:order-preserving:3}, we have $\Z_{vi}\leqslant \P_{vi}-\P_{ui}+1=0$. By applying Equation~\eqref{eqn:method:order-preserving:2} repeatedly following the path from $v$ to $w$, we have $\Z_{wi}\leqslant \Z_{vi}$. Thus, $\Z_{wi}\leqslant 0$. However, we also have $\Z_{wi}\geqslant \P_{wi}=1$ according to Equation~\eqref{eqn:method:order-preserving:1}. A contradiction.

    "Only if": First, we define $\Z_{vi}=1$ if a node $w\in \VB_i$ is reachable from $v~(v\in\VB)$. Otherwise, $\Z_{vi}=0$. Thus, Equation~\eqref{eqn:method:order-preserving:1} and \eqref{eqn:method:order-preserving:2} are satisfied according to this kind of definition. For Equation~\eqref{eqn:method:order-preserving:3}, if $\P_{vi}=1$, the constraint will hold true regardless of whether $\P_{ui}$ is $1$ or $0$. If $\P_{vi}=0$ and $\P_{ui}=0$, $\Z_{vi}\leqslant \P_{vi}-\P_{ui}+1=1$ will also hold true because $\Z_{vi}$ could be either $0$ or $1$. Finally, if $\P_{vi}=0$ and $\P_{ui}=1$, $\Z_{vi}=0$ will hold true because $\VB_i$ is a contiguous set and we cannot find any $w\in \VB_i$, such that $w$ is reachable from $v$.
\end{proof}

% Figure environment removed

\section{QIP formulation for intra-layer-only parallelism}\label{appendix:miqp-for-intra-layer-parallelism}
Here we present the QIP formulation for intra-layer-only parallelism with explanations.

\textbf{Objective function}\quad In terms of intra-layer-only parallelism, there is only one computation stage involved. As a result, the objective function takes into account only the value of $p_1$. We hereby formalize the equation as
\begin{equation}
    \min\quad tpi_{gpipe}=p_1.\label{eqn:appendix:lp-no-pp}
\end{equation}

\textbf{Computation-stage constraint}\quad With only one computation stage in intra-layer-only parallelism, the communication-stage constraint can be omitted, and the computation and communication cost can be modeled for $p_1$. Thus, we could formalize the constraint as
\begin{equation}
\sum_{u\in \VB}\S_{u}^\mathsf{T}\A_{u}+\sum_{\langle u,v\rangle\in \EB}\S_{u}^\mathsf{T}\R_{uv}\S_{v}=p_1.~\label{eqn:appendix:computation-for-no-pp}
\end{equation}
In the equation, the first summation term for any $u\in \VB$ represents the cost of choosing intra-layer strategies for all layers, while the second term represents the summation of resharding costs on all edges.


\textbf{Memory constraint}\quad Similar to the memory constraint in MIQP, it is necessary to ensure that the memory usage on a single device does not exceed its device memory bound $m$ in QIP. This restriction gives 
\begin{equation}
    \sum_{u\in V}\S_u^\mathsf{T} \M_u\leqslant m.~\label{eqn:appendix:memory-constraint-for-no-pp}
\end{equation}
It is worth noting that $m$ should be an identical constant across multiple devices if these devices are homogeneous. Otherwise, the value of $m$ varies.

\textbf{Strategy-selection constraint}\quad For intra-layer-only parallelism, the layer-placement constraint can be safely omitted because it is designed for PP. However, the strategy-selection constraint is necessary because each layer can only select one intra-layer strategy. Therefore, the strategy-selection constraint for QIP is identical to Equation~\eqref{eqn:method:strategy-selection:1} and \eqref{eqn:method:strategy-selection:2} for MIQP.

By combining objective function~\eqref{eqn:appendix:lp-no-pp} and constraints~\eqref{eqn:method:strategy-selection:1}, \eqref{eqn:method:strategy-selection:2}, \eqref{eqn:appendix:computation-for-no-pp}, and \eqref{eqn:appendix:memory-constraint-for-no-pp}, we have the QIP expression for optimizing the intra-layer-only AP. Like MIQP expression for optimizing the inter- and intra-layer AP, UniAP will eventually get the minimum TPI and corresponding parallel strategies by invoking the off-the-shelf solver.

\section{Visualization for the candidate solution}\label{appendix:visualization_for_p_and_s}
% Figure environment removed

In this section, we proceed to visually represent a potential solution for UOP. Given a deep learning model $\mathcal{G}$, pipeline parallel size $pp\_size$, and number of micro-batches $c$, UniAP will determine layer placement $\P$ for inter-layer parallelism and parallel strategy $\S$ for intra-layer parallelism using an off-the-shelf solver. As Figure \ref{fig:ps_explanation} shows, the solver is optimizing a three-layer model with two pipeline stages, each assigned four GPUs. At this time, a candidate solution could be
\begin{equation}
    \P=\begin{bmatrix}
        1 & 0 \\
        1 & 0 \\
        0 & 1
    \end{bmatrix},~
    \S=
    \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 1 \\
        0 & 0 & 0 \\ 
        \vdots & \vdots & \vdots \\
        0 & 0 & 0
    \end{bmatrix}.
\end{equation}

Here, the $u$-th row of matrix $\P$ denotes the placement strategy for layer $u$, where $\P_{ui}=1$ signifies the placement of layer $u$ on stage $i$, while $0$ indicates otherwise. For example, $\P_{l_{0}}=\left[1,~0\right]$ denotes the placement of layer $l_0$ on pipeline stage 1. Additionally, the $u$-th column of matrix $\S$ denotes the selected intra-layer parallel strategy for layer $u$, where $\S_{uj}=1$ denotes the selection of the $j$-th strategy from the intra-layer parallel strategy set. For example, $\S_{l_{0}}=\left[1,~0,~0,~\cdots,~0\right]^{\mathsf{T}}$ indicates that layer $l_0$ will adopt only the DP strategy, while $\S_{l_{1}}=\left[0,~1,~0,~\cdots,~0\right]^{\mathsf{T}}$ indicates that layer $l_1$ will employ a strategy where DP is performed on GPU 0, 1 and GPU 2, 3, and TP is performed across these two GPU groups.

There exist numerous combinations of $\P$ and $\S$. The off-the-shelf solver will automatically search for the optimal solution given pipeline parallel size $pp\_size$ and the number of micro-batches $c$. By solving the MIQP expression and enumerating every possible $pp\_size$ and $c$ in the UOP process, UniAP will ultimately derive an optimal parallel strategy for the deep learning model within the current hardware environment.

\section{Experiment detail}\label{appendix:experiment-settings}
\textbf{Gurobi configuration}\quad When tackling the MIQP problem, UniAP employs several configurations for the Gurobi Optimizer 10.1~\citep{gurobi_optimization_llc_gurobi_2023}. In particular, we set \textit{TimeLimit} to 60 seconds, \textit{MIPFocus} to 1, \textit{NumericFocus} to 1, and remain other configurations to default. For instance, we establish the \textit{MIPGap} parameter as the default value of 1e-4 to serve as a strict termination criterion. Furthermore, we have implemented an early stopping mechanism to terminate the optimization process as early as possible. There are two conditions that can activate the mechanism. Firstly, if the current runtime exceeds 15 seconds and the relative MIP optimality gap is less than 4\%, we will terminate the optimization. Secondly, if the current runtime exceeds 5 seconds and the best objective bound is worse than the optimal solution obtained in the previous optimization process, we will terminate the optimization.

% \begin{table}[t]
% \caption{Details for five Transformer-based models. L: Number of hidden layers; H: Hidden size; S: Sequence length.}
% \label{tab:appendix-details-for-five-transformer-based-models}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
%     \begin{tabular}{ccccc}
%     \toprule
%     Model & L & H & S & \#params \\
%     \midrule
%     BERT-Huge & 32    & 1280  & 512   & 672M \\
%     T5-Large & 24/24 & 1024  & 512   & 737M \\
%     ViT-Huge & 32    & 1280  & 196 & 632M \\
%     Swin-Huge & 2/2/42/2 & 320/640/1280/2560 & 49*64/49*16/49*4/49*1 & 1.02B \\
%     Llama-7B & 32 & 4096 & 2048 & 7B \\
%     \bottomrule
%     \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}
\begin{table}[t]
     \caption{Summary of the evaluated models.}
	\label{tab:summary-models}
    \begin{center}
    \begin{small}
	\begin{threeparttable}
\begin{tabular}{ccccccc}
    \toprule
    \multirow{2}{*}{Model} & \multirow{2}{*}{Task\tnote{1)}} & \multicolumn{4}{c}{Statistics} & \multirow{2}{*}{Precision} \\
    \cmidrule(r){3-6}
    && \#hidden layers  &  Hidden size   & Sequence length   & \#params &\\
    \midrule
    BERT-Huge & PT   & 32 & 1280 & 512 & 672M     & FP32      \\
    T5-Large  & CG   & 24/24 & 1024 & 512 & 737M    & FP32      \\
    ViT-Huge  & IC   & 32 & 1280 & 196 & 632M    & FP32      \\
    Swin-Huge & IC   & 2/2/42/2 & 320 & 49 $\times$ 64 & 1.02B   & FP32      \\
    Llama-7B  & CLM  & 32& 4096 & 2048 & 7B      & FP16   \\
    Llama-13B & CLM & 40 & 5120 & 2048 & 13B & FP16 \\
    \bottomrule
\end{tabular}
\begin{tablenotes}
    \footnotesize
    \item[1)] PT: Pretraining; CG: Conditional Generation; IC: Image Classification; CLM: Causal Language Modeling.
\end{tablenotes}
\end{threeparttable}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\textbf{Model detail}\quad Table~\ref{tab:summary-models} summarizes six Transformer-based models selected for our evaluations. Four of these models, namely BERT-Huge~\citep{devlin_bert_2019}, T5-Large~\citep{raffel_exploring_2020}, Llama-7B, and Llama-13B~\citep{touvron_llama_2023,touvron_llama_2023-1}, belong to the domain of natural language processing (NLP). At the same time, the remaining two, ViT-Huge~\citep{dosovitskiy_image_2021} and Swin-Huge~\citep{liu_swin_2021}, are associated with computer vision (CV). It is noteworthy that BERT, ViT, and Llama maintain consistent types of hidden layers respectively, whereas T5 and Swin have different types of hidden layers. Numbers separated by slashes represent the statistical information for different layer types. For instance, Swin-Huge comprises four types of layers, each with 2, 2, 42, and 2 layers, respectively.

\textbf{Training detail}\quad UniAP is based on the PyTorch framework and integrates models from HuggingFace Transformers. It employs various types of parallelism, including Pipeline Parallelism~(PP), Data Parallelism~(DP), Tensor Parallelism~(TP), and Fully Sharded Data Parallelism~(FSDP), utilizing GPipe~\citep{huang_gpipe_2019}, PyTorch DDP~\citep{li_pytorch_2020}, Megatron-LM~\citep{narayanan_efficient_2021}, and FairScale~\citep{fairscale_authors_fairscale_2021}, respectively. For NLP models, we use the English Wikipedia dataset~\citep{wikidump}, while the ImageNet-1K dataset~\citep{imagenet15russakovsky} is used for CV models. We train these models using the Adam optimizer~\citep{kingma_adam_2015}. We omit hyperparameters here such as learning rate and weight decay as these have minimal impact on training throughput. 
The model parameters in the HuggingFace Transformers are configured to align with the specifications of each individual model. For instance, we set \textit{hidden\_size} to 1280, \textit{num\_hidden\_layers} to 32, \textit{num\_attention\_heads} to 16, and \textit{seq\_length} to 512 for BERT-Huge. Regarding other hyperparameters in the HuggingFace configurations, we set \textit{hidden\_dropout\_prob} and \textit{attention\_probs\_dropout\_prob} to 0.0 for ViT-Huge. For Swin-Huge, we set \textit{drop\_path\_rate} to 0.2. We remain other configurations to default. It should be noted that the training batch sizes for each experiment are outlined in the main paper.

% \section{Scalability}\label{appendix:experiment:scalability}
% % Figure environment removed
% We study the scalability of UniAP on \textsc{EnvD}, the result of which is shown in Figure~\ref{fig:scalability}. We can find that the training throughput of the optimal strategy and the corresponding strategy optimization time demonstrate near-linearity as the number of nodes and mini-batch size increase. This phenomenon reveals that UniAP is scalable and verifies the computational complexity analysis in Section~\ref{subsec:method:unified-optimization}.

% \section{Estimation accuracy}\label{appendix:experiment:performance_modeling}
% Some variables in UniAP and other AP methods are estimated values rather than actual running values. The TPI~(inverse of training throughput) returned by UniAP and other AP methods is one of them. Accurate estimation for TPI or training throughput is crucial for evaluating candidate parallel strategies and ensuring the optimality of the solution. To quantify the accuracy of the estimated training throughput, we introduce a metric called \emph{relative estimation error~(REE)} $e$ for training throughput:
% \begin{equation}
%     e(T, \hat{T}) = \frac{|T - \hat{T}|}{T} \times 100\%,
% \end{equation}
% where $T$ is the actual training throughput and $\hat{T}$ is the estimated training throughput.

% % Figure environment removed

% We evaluate the optimal parallel strategies obtained from \textsc{EnvA} and \textsc{EnvB} and visualize the REE of UniAP in Figure~\ref{fig:performance-modeling}. The results show that UniAP achieves an average REE of 3.59\%, which is relatively small. In contrast, the average REE for Galvatron~\citep{miao_galvatron_2022} in our experiments is 11.17\%, which is larger than that of UniAP.

\section{Case study: BERT-Huge}\label{appendix:case-study}
% Figure environment removed
In this section, we present a visualization of the optimal parallel strategy discovered by UniAP. As represented in Figure \ref{fig:visualization-model}, the strategy pertains to training BERT-Huge with 32 hidden layers in a 2-node environment \textit{EnvB} with a mini-batch size of 16. Each node was equipped with 2 Xeon E5-2620 v4 CPUs, 4 TITAN Xp 12GB GPUs, and 125GB memory. These nodes are interconnected via a 10Gbps network. It should be noted that we only showcase the parallel strategy for the hidden layers here for simplicity but without losing generality.

Here, we provide further topological information for a node within \textit{EnvB}. As illustrated in Figure~\ref{fig:topo}, we categorize the GPUs numbered 0 and 1 in each node and refer to them collectively as \textit{GPUGroup0}. Similarly, we label the GPUs numbered 2 and 3 as \textit{GPUGroup1}. In \textit{EnvB}, the interconnects within each GPU group (i.e., PCIe) have superior bandwidth than that between different groups (i.e., QPI). We collectively designate these two connection bandwidths as intra-node bandwidth, which is higher than inter-node bandwidth.

% Figure environment removed

In this example, UniAP has identified a parallel strategy for inter-layer parallelism that involves a two-stage pipeline. This strategy utilizes parallelism in a manner that is both efficient and effective. Specifically, the communication cost of point-to-point~(P2P) between two nodes is less than that of all-reduce. Given that the inter-node bandwidth is lower than the intra-node bandwidth, the two-stage PP becomes a reasonable choice. Moreover, the pipeline has been designed such that each stage comprises an equal number of layers. This design leverages the homogeneity of the nodes and ensures load balancing across the cluster.

Within each PP stage, UniAP employs an intra-layer parallel strategy. It utilizes a 2-way DP for the initial 12 hidden layers in each stage between \textit{GPUGroup0} and \textit{GPUGroup1}. For the remaining four hidden layers, a 2-way FSDP is utilized between \textit{GPUGroup0} and \textit{GPUGroup1} to reduce memory footprint and meet memory constraints. Within each GPU group, UniAP employs a 2-way TP for each layer. In general, TP incurs more significant communication volumes than DP and FSDP. In order to achieve maximum training throughput on \textit{EnvB}, it is necessary to implement parallel strategies that prioritize higher communication volumes within each group and lower volumes between groups. Therefore, the strategy for BERT-Huge with 32 hidden layers combines the best elements of PP, DP, TP, and FSDP to maximize training throughput.

In addition, we have conducted calculations for the model FLOPs utilizatio~(MFU)\citep{chowdhery_palm_2023} for Galvatron, Alpa, and UniAP in this scenario to validate our analysis. MFU is independent of hardware, frameworks, or implementations. Therefore, it allows us to examine the performance of different parallel strategies solely from a strategic perspective. For BERT-Huge, the resulting MFUs for UniAP, Galvatron, and Alpa are 58.44\%, 58.44\%, and 55.10\% on \textsc{EnvA}, while 23.6\%, 13.7\%, and 19.6\% on \textsc{EnvB}, respectively. These results validate that UniAP's optimization of inter- and intra-layer AP results in superior performance compared to Galvatron and Alpa.


\section{Scaling to larger clusters and larger models}
\label{appendix:dcu}
We adapt UniAP to a cloud cluster to further scale UniAP to larger clusters and larger models. The experiments are conducted on 8 nodes. Each of them has 1 Intel Xeon Phi Processor 7285, 4 16GB PCIe DCU, 128GB DDR4 DRAM, and 200Gb Infiniband interconnection network. We name this environment as \textsc{EnvE}. It is worth noting that DCU is a kind of GPU, which is different from NVIDIA GPUs. We select MP methods including Megatron~\citep{narayanan_efficient_2021} and DeepSpeed~(ZeRO-3)~\citep{rasley_deepspeed_2020} as our baseline methods. %Comparisons on their supported parallel strategies against UniAP are outlined in Table~\ref{tab:supported_parallel_strategies}.
% \begin{table}[t]
%     \caption{Supported parallel strategies.}
%     \label{tab:supported_parallel_strategies}
%     \begin{center}
%     \begin{small}
%     \begin{threeparttable}
%     \begin{tabular}{cccccc}
%         \toprule
%         Method & PP & DP & TP & FSDP & HP\tnote{1)} \\
%         \midrule
%         Megatron & \checkmark & \checkmark & \checkmark & $\times$ & $\times$ \\
%         DeepSpeed & $\times$ & \checkmark & $\times$ & \checkmark & $\times$ \\
%         % Megatron-DeepSpeed & \checkmark & \checkmark & \checkmark & \checkmark & $\times$ \\
%         UniAP & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\
%         \bottomrule
%     \end{tabular}
%     \begin{tablenotes}
%         \footnotesize
%         \item[1)] HP: Hybrid parallel strategies for different layers.
%     \end{tablenotes}
%     \end{threeparttable}
%     \end{small}
%     \end{center}
% \end{table}

In the experiment, we evaluate these baselines and UniAP with Llama-7B and Llama-13B~\citep{touvron_llama_2023}. We set the mini-batch size as 8 for Llama-7B and 4 for Llama-13B. The remaining configurations align with those for Llama-7B in Section~\ref{sec:experiment}, such as activating FP16 mixed precision training.

We focus on two metrics: training throughput and strategy optimization time. For UniAP, the former metric is calculated by averaging throughput from the 10th to the 60th iteration of training, while the later metric is determined by measuring the time of the UOP~(Section~\ref{subsec:method:unified-optimization}). Please note that Megatron~\citep{narayanan_efficient_2021} and DeepSpeed~\citep{rasley_deepspeed_2020} do not automatically optimize parallel strategies. Consequently, we undertake an exhaustive approach, examining every combination of parallel strategies and the corresponding parallel configurations they accommodate. For each combination, we assess the average throughput over the 10th to 60th training iteration, ultimately adopting the fastest strategy as their optimization result. The total time spent in the process is recorded as their strategy optimization time.

\begin{table}[t]
\vskip -0.2in
\caption{Training throughput and strategy optimization time on \textsc{EnvE}.}
\label{tab:dcu}%
\begin{center}
\begin{small}
% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{threeparttable}
\begin{tabular}{ccccccc}
    \toprule
    \multirow{2}[0]{*}{Model}  & \multicolumn{3}{c}{\makecell{Training throughput (samples/s)}} & \multicolumn{3}{c}{\makecell{Strategy optimization time (min.)}} \\
    \cmidrule(lr){2-4}\cmidrule(lr){5-7}
           & Megatron & DeepSpeed & UniAP & Megatron & DeepSpeed & UniAP \\
    \midrule
    Llama-7B  & \textbf{2.01~$\pm$~0.005}  & \texttt{SOL}$\times$\tnote{1)}  & \textbf{2.01~$\pm$~0.005}  & > 8.0 hours & \texttt{SOL}$\times$\tnote{1)}  & \textbf{3.07~$\pm$~0.121}  \\
    Llama-13B &  \textbf{0.82~$\pm$~0.001}  &   \texttt{SOL}$\times$\tnote{1)}    &  \textbf{0.82~$\pm$~0.001}     &    > 2.5 hours   &    \texttt{SOL}$\times$\tnote{1)}   & \textbf{1.95~$\pm$~0.076} \\
\bottomrule
\end{tabular}
\begin{tablenotes}
    \footnotesize
    \item[1)] \texttt{SOL}$\times$: No solution after strategy optimization.
\end{tablenotes}
\end{threeparttable}
\end{small}
\end{center}
\vskip -0.2in
\end{table}

As shown in Table~\ref{tab:dcu}, UniAP consistently identifies the fastest parallel strategies while expending considerably less time on strategy optimization compared to Megatron~(about 157$\times$ faster for Llama-7B and 77$\times$ faster for Llama-13B). Please note that the strategy optimization time Megatron needs for Llama-7B surpasses that for Llama-13B. We attribute this phenomenon to the variations in the mini-batch size. Specifically, the escalation from a mini-batch size of 4 (employed in Llama-13B) to 8 (employed in Llama-7B) leads to a rise in the number of candidate parallel strategies, as well as the parallel 
 strategies that will successfully train the model. As a result, the strategy optimization time will become increasingly long when the mini-batch size exceeds 8 in the pretraining scenario~\citep{brown_language_2020,touvron_llama_2023,touvron_llama_2023-1}. In this situation, UniAP will identify the optimal parallel strategy much faster.

Furthermore, our experiments highlight a limitation encountered with DeepSpeed~(ZeRO-3). It requires the mini-batch size to be divisible evenly by the total number of computation devices. This specific prerequisite prevents DeepSpeed from successfully launching the training process with 32 DCUs.

\begin{table}[t]
\caption{Statistics on the candidate parallel strategies for Megatron.}
\label{tab:statistics-on-megatron}%
\begin{center}
\begin{small}
% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{threeparttable}
\begin{tabular}{cccccccc}
    \toprule
    \multirow{2}[0]{*}{Model} & \multirow{2}[0]{*}{Batch size} & \multicolumn{4}{c}{Training throughput (samples/s)} & \multirow{2}[0]{*}{\#infeasible\tnote{5)}} & \multirow{2}[0]{*}{\#candidate\tnote{6)}} \\
     \cmidrule(lr){3-6}
    & & Top-1\tnote{1)} & Top-2\tnote{2)} & Slowest\tnote{3)} & Median\tnote{4)} & & \\
    \midrule
    Llama-7B & 8 & 2.01 & 1.92 & 0.22  & 0.82  & 41 & 64 \\
    Llama-13B & 4 & 0.82 & 0.58  & 0.27  & 0.42  &  42  & 48 \\
\bottomrule
\end{tabular}
\begin{tablenotes}
    \footnotesize
    \item[1)] Top-1: The training throughput achieved by the fastest parallel strategy.
    \item[2)] Top-2: The training throughput achieved by the second fastest parallel strategy.
    \item[3)] Slowest: The training throughput achieved by the slowest parallel strategy.
    \item[4)] Median: The median value of training throughputs across parallel strategies that will successfully train the model.
    \item[5)] \#infeasible: The number of parallel strategies that will encounter exceptions such as CUDA OOM during model training.
    \item[6)] \#candidate: The total number of candidate parallel strategies.
\end{tablenotes}
\end{threeparttable}
\end{small}
\end{center}
\vskip -0.2in
\end{table}

We further examine the statistics on the candidate parallel strategies for Megatron, as shown in Table~\ref{tab:statistics-on-megatron}. When the users are unable to identify the fastest parallel strategy out of hundreds of candidates, they may be required to sacrifice at least 4.4\% of the training throughput for Llama-7B and 29.2\% for Llama-13B. Even if they successfully eliminate all infeasible parallel strategies, which constitute the majority of candidates~(Table~\ref{tab:statistics-on-megatron}), they still only have a 50\% chance of identifying a parallel strategy with training throughput better than 0.82 samples/s for Llama-7B and 0.42 samples/s for Llama-13B. Consequently, it is challenging for users to identify the fastest parallel strategy for larger clusters and larger models. In such circumstances, UniAP is able to demonstrate its capacity to scale and accommodate both larger clusters and larger models.


 
 
% We choose widely used distributed training framework Magatron,  DeepSpeed and Megatron-DeepSpeed as baselines. Due to the design of DCU, pytorch libraries and frameworks need specialized adaptations. So this part of experiments is based on adapted pytorch libraries and Megatron-DeepSpeed\citep{deepspeed-3d} framework provided by the platform developers. Specifically, we only choose the cases where the parallel strategies we get using UniAP are supported by Megatron-DeepSpeed for comparison.Note that all strategies in baselines are naturally supported by Megatron-DeepSpeed. In more details, the strategy space for Megatron is any combination of TP and PP, the strategy space for DeepSpeed is pure FSDP, the strategy space for Megatron-DeepSpeed is any combination of DP, TP, PP and FSDP except strategies contain PP and FSDP at the same time. The results are shown in Table~\ref{tab:dcu}.

%\section{Limitation}
%\label{appendix:limitation}
%UniAP is currently designed and tested on homogeneous clusters, but incorporating automatic parallelism for training deep models on heterogeneous clusters~(e.g., a cluster equipped with both NVIDIA GPUs and DCUs) is another important research topic. Given that current parallel techniques primarily target homogeneous clusters with limited emphasis on heterogeneous clusters, we have chosen to leave this topic for future exploration.
% Despite its advantages, UniAP has limitations at present. 
% \begin{enumerate}
%     % \item UniAP does not validate other inter-layer parallelism mechanisms except for the GPipe-style PP. Some alternative inter-layer parallelism approaches\cite{fan_dapple_2021,narayanan_efficient_2021} may result in less memory footprint or higher training throughput than GPipe. 
%     \item UniAP is designed  We left it as our future work.
%     \item UniAP mainly focuses on optimizing common parallel strategies like pipeline parallelism, data parallelism, tensor parallelism, and fully-sharded data parallelism. However, there are emerging parallel strategies, such as sequence parallelism~\citep{li_sequence_2023,jacobs_deepspeed_2023} and expert parallelism~\citep{shazeer_outrageously_2017,he_fastermoe_2022}. We plan to incorporate them into our framework in the future.
%     \item UniAP does not automatically optimize other training techniques, such as mixed precision training~\citep{micikevicius_mixed_2018}, activation recomputation~\citep{chen_training_2016}, and low-rank methods~\citep{hu_lora_2022}. Due to the orthogonal nature of these techniques to our approach, we decided to left them as our future work.
%     % \item Due to the limited hardware resources, we have yet to conduct experiments for UniAP on Transformer models with larger parameter sizes. Therefore, the effectiveness of UniAP on larger models and larger clusters remains to be validated in the future.
% \end{enumerate}



%\section{Broader impact}
%\label{appendix:broader-impact}
%We cannot foresee any direct negative social impacts for UniAP currently. However, training a Transformer-like model requires significant energy, imposing a substantial burden on the planet. UniAP can significantly accelerate the training process of deep learning models, thereby minimizing energy consumption as much as possible.