\begin{thebibliography}{10}

\bibitem{devlin_bert_2019}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}.
\newblock In {\em The {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}}, pages 4171--4186, 2019.

\bibitem{brown_language_2020}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language {Models} are {Few}-{Shot} {Learners}.
\newblock In {\em Advances in {Neural} {Information} {Processing} {Systems} 33}, pages 1877--1901, 2020.

\bibitem{raffel_exploring_2020}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock {\em The Journal of Machine Learning Research}, 21(1):5485--5551, 2020.

\bibitem{dosovitskiy_image_2021}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}.
\newblock In {\em {International} {Conference} on {Learning} {Representations}}, 2021.

\bibitem{liu_swin_2021}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
\newblock Swin {Transformer}: {Hierarchical} {Vision} {Transformer} using {Shifted} {Windows}.
\newblock In {\em {International} {Conference} on {Computer} {Vision}}, pages 9992--10002, 2021.

\bibitem{touvron_llama_2023}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
\newblock {LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}.
\newblock {\em CoRR}, abs/2302.13971, 2023.

\bibitem{touvron_llama_2023-1}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
  Scialom.
\newblock Llama 2: {Open} {Foundation} and {Fine}-{Tuned} {Chat} {Models}.
\newblock {\em CoRR}, abs/2307.09288, 2023.

\bibitem{huang_gpipe_2019}
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia~Xu Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc~V. Le, Yonghui Wu, and Zhifeng Chen.
\newblock {GPipe}: {Efficient} {Training} of {Giant} {Neural} {Networks} using {Pipeline} {Parallelism}.
\newblock In {\em Advances in {Neural} {Information} {Processing} {Systems} 32}, pages 103--112, 2019.

\bibitem{narayanan_pipedream_2019}
Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil~R. Devanur, Gregory~R. Ganger, Phillip~B. Gibbons, and Matei Zaharia.
\newblock {PipeDream}: {Generalized} {Pipeline} {Parallelism} for {DNN} {Training}.
\newblock In {\em {Symposium} on {Operating} {Systems} {Principles}}, pages 1--15, 2019.

\bibitem{narayanan_memory-efficient_2021}
Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia.
\newblock Memory-{Efficient} {Pipeline}-{Parallel} {DNN} {Training}.
\newblock In {\em {International} {Conference} on {Machine} {Learning}}, pages 7937--7947, 2021.

\bibitem{fan_dapple_2021}
Shiqing Fan, Yi~Rong, Chen Meng, Zongyan Cao, Siyu Wang, Zhen Zheng, Chuan Wu, Guoping Long, Jun Yang, Lixue Xia, Lansong Diao, Xiaoyong Liu, and Wei Lin.
\newblock {DAPPLE}: {A} {Pipelined} {Data} {Parallel} {Approach} for {Training} {Large} {Models}.
\newblock In {\em {Symposium} on {Principles} and {Practice} of {Parallel} {Programming}}, pages 431--445, 2021.

\bibitem{li_chimera_2021}
Shigang Li and Torsten Hoefler.
\newblock Chimera: {Efficiently} {Training} {Large}-{Scale} {Neural} {Networks} with {Bidirectional} {Pipelines}.
\newblock In {\em International {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}}, pages 27:1--27:14, 2021.

\bibitem{lepikhin_gshard_2021}
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen.
\newblock {GShard}: {Scaling} {Giant} {Models} with {Conditional} {Computation} and {Automatic} {Sharding}.
\newblock In {\em {International} {Conference} on {Learning} {Representations}}, 2021.

\bibitem{du_glam_2022}
Nan Du, Yanping Huang, Andrew~M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams~Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten~P. Bosma, Zongwei Zhou, Tao Wang, Yu~Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen~S. Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc~V. Le, Yonghui Wu, Zhifeng Chen, and Claire Cui.
\newblock {GLaM}: {Efficient} {Scaling} of {Language} {Models} with {Mixture}-of-{Experts}.
\newblock In {\em International {Conference} on {Machine} {Learning}}, pages 5547--5569, 2022.

\bibitem{fedus_switch_2022}
William Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch {Transformers}: {Scaling} to {Trillion} {Parameter} {Models} with {Simple} and {Efficient} {Sparsity}.
\newblock {\em The Journal of Machine Learning Research}, 23(120):5232–5270, 2022.

\bibitem{li_pytorch_2020}
Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, and Soumith Chintala.
\newblock {PyTorch} {Distributed}: {Experiences} on {Accelerating} {Data} {Parallel} {Training}.
\newblock {\em Proceedings of the VLDB Endowment}, 13(12):3005--3018, 2020.

\bibitem{rasley_deepspeed_2020}
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He.
\newblock {DeepSpeed}: {System} {Optimizations} {Enable} {Training} {Deep} {Learning} {Models} with {Over} 100 {Billion} {Parameters}.
\newblock In {\em {The} 26th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}}, pages 3505--3506, 2020.

\bibitem{narayanan_efficient_2021}
Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, and Matei Zaharia.
\newblock Efficient {Large-scale} {Language} {Model} {Training} on {GPU} {Clusters} {Using} {Megatron}-{LM}.
\newblock In {\em International {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}}, pages 58:1--58:15, 2021.

\bibitem{fairscale_authors_fairscale_2021}
{FairScale authors}.
\newblock {FairScale}: {A} general purpose modular {PyTorch} library for high performance and large scale training.
\newblock \href{https://github.com/facebookresearch/fairscale}{https://github.com/facebookresearch/fairscale}, 2021.

\bibitem{shazeer_mesh-tensorflow_2018}
Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, and Blake~A. Hechtman.
\newblock Mesh-{TensorFlow}: {Deep} {Learning} for {Supercomputers}.
\newblock In {\em Advances in {Neural} {Information} {Processing} {Systems} 31}, pages 10435--10444, 2018.

\bibitem{xu_gspmd_2021}
Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake~A. Hechtman, Yanping Huang, Rahul Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, Ruoming Pang, Noam Shazeer, Shibo Wang, Tao Wang, Yonghui Wu, and Zhifeng Chen.
\newblock {GSPMD}: {General} and {Scalable} {Parallelization} for {ML} {Computation} {Graphs}.
\newblock {\em CoRR}, abs/2105.04663, 2021.

\bibitem{flynn_very_1966}
Michael~J. Flynn.
\newblock Very {High-speed} {Computing} {Systems}.
\newblock {\em Proceedings of the IEEE}, 54(12):1901--1909, 1966.

\bibitem{flynn_computer_1972}
Michael~J. Flynn.
\newblock Some {Computer} {Organizations} and {Their} {Effectiveness}.
\newblock {\em IEEE Transactions on Computers}, C-21(9):948--960, 1972.

\bibitem{he_pipetransformer_2021}
Chaoyang He, Shen Li, Mahdi Soltanolkotabi, and Salman Avestimehr.
\newblock {PipeTransformer}: {Automated} {Elastic} {Pipelining} for {Distributed} {Training} of {Large}-scale {Models}.
\newblock In {\em {International} {Conference} on {Machine} {Learning}}, pages 4150--4159, 2021.

\bibitem{zheng_alpa_2022}
Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric~P. Xing, Joseph~E. Gonzalez, and Ion Stoica.
\newblock Alpa: {Automating} {Inter}- and {Intra}-{Operator} {Parallelism} for {Distributed} {Deep} {Learning}.
\newblock In {\em {USENIX} {Symposium} on {Operating} {Systems} {Design} and {Implementation}}, pages 559--578, 2022.

\bibitem{jia_exploring_2018}
Zhihao Jia, Sina Lin, Charles~R. Qi, and Alex Aiken.
\newblock Exploring {Hidden} {Dimensions} in {Parallelizing} {Convolutional} {Neural} {Networks}.
\newblock In {\em {International} {Conference} on {Machine} {Learning}}, pages 2279--2288, 2018.

\bibitem{wang_supporting_2019}
Minjie Wang, Chien-Chin Huang, and Jinyang Li.
\newblock Supporting {Very} {Large} {Models} using {Automatic} {Dataflow} {Graph} {Partitioning}.
\newblock In {\em Proceedings of the {Fourteenth} {EuroSys} {Conference}}, pages 26:1--26:17, 2019.

\bibitem{jia_beyond_2019}
Zhihao Jia, Matei Zaharia, and Alex Aiken.
\newblock Beyond {Data} and {Model} {Parallelism} for {Deep} {Neural} {Networks}.
\newblock In {\em {Machine} {Learning} and {Systems}}, pages 1--13, 2019.

\bibitem{schaarschmidt_automap_2021}
Michael Schaarschmidt, Dominik Grewe, Dimitrios Vytiniotis, Adam Paszke, Georg~Stefan Schmid, Tamara Norman, James Molloy, Jonathan Godwin, Norman~Alexander Rink, Vinod Nair, and Dan Belov.
\newblock Automap: {Towards} {Ergonomic} {Automated} {Parallelism} for {ML} {Models}.
\newblock {\em CoRR}, abs/2112.02958, 2021.

\bibitem{zhao_vpipe_2022}
Shixiong Zhao, Fanxin Li, Xusheng Chen, Xiuxian Guan, Jianyu Jiang, Dong Huang, Yuhao Qing, Sen Wang, Peng Wang, Gong Zhang, Cheng Li, Ping Luo, and Heming Cui.
\newblock {vPipe}: {A} {Virtualized} {Acceleration} {System} for {Achieving} {Efficient} and {Scalable} {Pipeline} {Parallel} {DNN} {Training}.
\newblock {\em IEEE Transactions on Parallel and Distributed Systems}, 33(3):489--506, 2022.

\bibitem{cai_tensoropt_2022}
Zhenkun Cai, Xiao Yan, Kaihao Ma, Yidi Wu, Yuzhen Huang, James Cheng, Teng Su, and Fan Yu.
\newblock {TensorOpt}: {Exploring} the {Tradeoffs} in {Distributed} {DNN} {Training} {With} {Auto}-{Parallelism}.
\newblock {\em IEEE Transactions on Parallel and Distributed Systems}, 33(8):1967--1981, 2022.

\bibitem{liu_colossal-auto_2023}
Yuliang Liu, Shenggui Li, Jiarui Fang, Yanjun Shao, Boyuan Yao, and Yang You.
\newblock Colossal-{Auto}: {Unified} {Automation} of {Parallelization} and {Activation} {Checkpoint} for {Large}-scale {Models}.
\newblock {\em CoRR}, abs/2302.02599, 2023.

\bibitem{tarnawski_efficient_2020}
Jakub Tarnawski, Amar Phanishayee, Nikhil~R. Devanur, Divya Mahajan, and Fanny~Nina Paravecino.
\newblock Efficient {Algorithms} for {Device} {Placement} of {DNN} {Graph} {Operators}.
\newblock In {\em Advances in {Neural} {Information} {Processing} {Systems} 33}, pages 15451--15463, 2020.

\bibitem{tarnawski_piper_2021}
Jakub Tarnawski, Deepak Narayanan, and Amar Phanishayee.
\newblock Piper: {Multidimensional} {Planner} for {DNN} {Parallelization}.
\newblock In {\em Advances in {Neural} {Information} {Processing} {Systems} 34}, pages 24829--24840, 2021.

\bibitem{lazimy_mixed_1982}
Rafael Lazimy.
\newblock Mixed-integer {Quadratic} {Programming}.
\newblock {\em Math. Program.}, 22(1):332–349, December 1982.

\bibitem{yuan_decentralized_2022}
Binhang Yuan, Yongjun He, Jared Davis, Tianyi Zhang, Tri Dao, Beidi Chen, Percy Liang, Christopher Ré, and Ce~Zhang.
\newblock Decentralized {Training} of {Foundation} {Models} in {Heterogeneous} {Environments}.
\newblock In {\em Advances in {Neural} {Information} {Processing} {Systems} 35}, pages 25464--25477, 2022.

\bibitem{miao_galvatron_2022}
Xupeng Miao, Yujie Wang, Youhe Jiang, Chunan Shi, Xiaonan Nie, Hailin Zhang, and Bin Cui.
\newblock Galvatron: {Efficient} {Transformer} {Training} over {Multiple} {GPUs} {Using} {Automatic} {Parallelism}.
\newblock {\em Proceedings of the VLDB Endowment}, 16(3):470--479, 2022.

\bibitem{rashidi_enabling_2021}
Saeed Rashidi, Matthew Denton, Srinivas Sridharan, Sudarshan Srinivasan, Amoghavarsha Suresh, Jade Nie, and Tushar Krishna.
\newblock Enabling {Compute}-{Communication} {Overlap} in {Distributed} {Deep} {Learning} {Training} {Platforms}.
\newblock In {\em {International} {Symposium} on {Computer} {Architecture}}, pages 540--553, 2021.

\bibitem{gurobi_optimization_llc_gurobi_2023}
{Gurobi Optimization, LLC}.
\newblock Gurobi {Optimizer} {Reference} {Manual}.
\newblock \href{https://www.gurobi.com}{https://www.gurobi.com}, 2023.

\bibitem{chen_training_2016}
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.
\newblock Training {Deep} {Nets} with {Sublinear} {Memory} {Cost}.
\newblock {\em CoRR}, abs/1604.06174, 2016.

\bibitem{micikevicius_mixed_2018}
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory~F. Diamos, Erich Elsen, David García, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu.
\newblock Mixed {Precision} {Training}.
\newblock In {\em {International} {Conference} on {Learning} {Representations}}, 2018.

\bibitem{rajbhandari_zero_2020}
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.
\newblock {ZeRO}: {Memory} {Optimizations} {Toward} {Training} {Trillion} {Parameter} {Models}.
\newblock In {\em {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}}, pages 20:1--20:16, 2020.

\bibitem{deepspeed-3d}
Microsoft.
\newblock {Megatron}-{DeepSpeed}.
\newblock \href{https://github.com/microsoft/Megatron-DeepSpeed/tree/main}{https://github.com/microsoft/Megatron-DeepSpeed/tree/main}, 2021.

\bibitem{wikidump}
{Wikimedia Foundation}.
\newblock Wikimedia {Downloads}.
\newblock \href{https://dumps.wikimedia.org}{https://dumps.wikimedia.org}, 2023.

\bibitem{imagenet15russakovsky}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander~C. Berg, and Li~Fei-Fei.
\newblock {ImageNet Large Scale Visual Recognition Challenge}.
\newblock {\em International Journal of Computer Vision}, 115(3):211--252, 2015.

\bibitem{kingma_adam_2015}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} {Method} for {Stochastic} {Optimization}.
\newblock In {\em {International} {Conference} on {Learning} {Representations}}, 2015.

\bibitem{chowdhery_palm_2023}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi~Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai, Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.
\newblock {PaLM}: {Scaling} {Language} {Modeling} with {Pathways}.
\newblock {\em Journal of Machine Learning Research}, 24:240:1--240:113, 2023.

\end{thebibliography}
