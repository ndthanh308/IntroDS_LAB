
\section{Experiment}
\label{sec:experiment}
We conduct experiments on five different kinds of environments. \textsc{EnvA} refers to a node~(machine) with 1 Xeon 6248 CPU, 8 V100-SXM2 32GB GPUs, and 472GB memory. \textsc{EnvB} refers to 2 nodes interconnected with 10Gbps networks, where each node has 2 Xeon E5-2620 v4 CPUs, 4 TITAN Xp 12GB GPUs, and 125GB memory. \textsc{EnvC} refers to a node with 8 A100 40GB PCIe GPUs. \textsc{EnvD} has 4 nodes, each with the same configuration as that in \textsc{EnvB}. \textsc{EnvE} is a cloud cluster with 8 nodes and each node has 4 DCUs~(a type of non-NVIDIA GPU).

We evaluate UniAP with five Transformer-based models, BERT-Huge~\citep{devlin_bert_2019}, T5-Large~\citep{raffel_exploring_2020}, ViT-Huge~\citep{dosovitskiy_image_2021}, Swin-Huge~\citep{liu_swin_2021}, and Llama~\citep{touvron_llama_2023,touvron_llama_2023-1}. We follow the common practice of training these transformer-based models. To eliminate factors that affect training throughput, we turn off techniques orthogonal to parallel strategies, such as activation checkpointing~\citep{chen_training_2016}. However, we integrate FP16 mixed precision training~\citep{micikevicius_mixed_2018} for the largest model, Llama, to successfully orchestrate the process. More details on these models are provided in Appendix~\ref{appendix:experiment-settings}.

The experimental evaluation concentrates on two primary metrics: training throughput and strategy optimization time. The former is calculated by averaging throughput from the 10th to the 60th iteration of training, while the latter is determined by measuring the time of the UOP. More details are provided in Appendix \ref{appendix:experiment-settings}.  

\begin{table*}[t]
    \caption{Training throughput and strategy optimization time on five Transformer-based models.}
    \label{tab:throughput_and_optimization_time}
    \begin{center}
    \begin{small}
    \begin{threeparttable}
    \begin{tabular}{cccccccc}
    \toprule
     \multirow{2}[0]{*}{Env.} & \multirow{2}[0]{*}{Model} & \multicolumn{3}{c}{Training throughput (samples/s)} & \multirow{2}[0]{*}{\makecell{Minimum\\speedup}} & \multirow{2}[0]{*}{\makecell{Maximum\\speedup}}\\
     \cmidrule(r){3-5}
    & & Galvatron & Alpa & UniAP &  \\
    \midrule
   \multirow{4}[0]{*}{\textsc{EnvA}} & BERT-Huge &  \textbf{33.46~$\pm$~0.28 } &  31.56~$\pm$~0.04 &  \textbf{33.46~$\pm$~0.28 } & 1.00 &  1.06 \\
    & T5-Large & \textbf{23.29~$\pm$~0.04} &  \texttt{MEM}$\times$\tnote{2)} &  \textbf{23.29~$\pm$~0.04} & 1.00 &  1.00 \\
    & ViT-Huge &   \textbf{109.51~$\pm$~0.07 } &  97.66~$\pm$~1.42 &  \textbf{109.51~$\pm$~0.07 } & 1.00 &  1.12 \\
    & Swin-Huge & \verb+CUDA+$\times$\tnote{3)} & \verb+N/A+\tnote{4)}  &  \textbf{67.96~$\pm$~0.12 } & \verb+N/A+\tnote{4)} & \verb+N/A+\tnote{4)}\\
    \midrule
    \multirow{4}[0]{*}{\textsc{EnvB}}
    & BERT-Huge &  6.27~$\pm$~0.17 &  8.95~$\pm$~0.06 &  \textbf{10.77~$\pm$~0.13} &  1.20 &  1.71 \\
    & T5-Large\tnote{1)} &  \textbf{8.06~$\pm$~0.06} & \verb+MEM+$\times$\tnote{2)}  &  7.98~$\pm$~0.05 &  0.99  &  0.99\\
    & ViT-Huge  &  32.20~$\pm$~0.17 &  38.74~$\pm$~0.20 &  \textbf{45.58~$\pm$~0.54} &  1.18 &  1.41 \\
    & Swin-Huge &  13.90~$\pm$~0.17 & \verb+N/A+\tnote{4)} &  \textbf{19.08~$\pm$~0.10} &  1.37 &  1.37\\
    \midrule
    \textsc{EnvC} & Llama-7B & 1.22~$\pm$~0.01 & \verb+N/A+\tnote{4)} & \textbf{4.63~$\pm$~0.007} & 3.80 & 3.80 \\
    \midrule
    \midrule
    \multirow{2}[0]{*}{Env.} & \multirow{2}[0]{*}{Model} & \multicolumn{3}{c}{Strategy optimization time (min.)} & \multirow{2}[0]{*}{\makecell{Minimum\\speedup}} & \multirow{2}[0]{*}{\makecell{Maximum\\ speedup}}\\
     \cmidrule(r){3-5}
    & & Galvatron & Alpa & UniAP &  \\
    \midrule
    \multirow{4}[0]{*}{\textsc{EnvA}}
    & BERT-Huge &  6.44~$\pm$~0.588 &  $>$~40 &  \textbf{0.37~$\pm$~0.002} &  17.29 &  $>$~107.41\\
    & T5-Large &  12.41~$\pm$~0.122 &  \texttt{MEM}$\times$\tnote{2)} &  \textbf{0.89~$\pm$~0.007} &  13.98 &  13.98\\
    & ViT-Huge &  6.29~$\pm$~0.464 &  $>$~40 &  \textbf{0.57~$\pm$~0.009} &  10.95 &  $>$~69.60 \\
    & Swin-Huge &  11.88~$\pm$~0.666 & \verb+N/A+\tnote{4)} &  \textbf{2.16~$\pm$~0.004} &  5.49 &  5.49 \\
    \midrule
    \multirow{4}[0]{*}{\textsc{EnvB}}
    & BERT-Huge &  2.04~$\pm$~0.010 &  $>$~40 &  \textbf{1.51~$\pm$~0.005} &  1.34 &  $>$~26.32\\
    & T5-Large\tnote{1)} &  2.64~$\pm$~0.110 & \verb+MEM+$\times$\tnote{2)} &  \textbf{0.91~$\pm$~0.005} &  2.90 &  2.90\\
    & ViT-Huge &  2.37~$\pm$~0.180 &  $>$~40 &  \textbf{1.11~$\pm$~0.011} &  2.14 &  $>$~36.01\\
    & Swin-Huge &  4.29~$\pm$~0.320 & \verb+N/A+\tnote{4)} &  \textbf{2.29~$\pm$~0.010} &  1.87 &  1.87\\
    \midrule
    \textsc{EnvC} & Llama-7B & 6.84~$\pm$~0.055 & \verb+N/A+\tnote{4)} & \textbf{0.58~$\pm$~0.006} & 11.83 & 11.83 \\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
    \footnotesize
    \item[1)] T5-Large tested on \textsc{EnvB} is restricted to 16/16 layers to avoid out-of-memory~(OOM) exceptions.
    \item[2)] \texttt{MEM}$\times$: OOM exceptions during strategy optimization.
    \item[3)] \texttt{CUDA}$\times$: CUDA OOM exceptions during model training.
    \item[4)] The official implementation for Swin-Huge and Llama in Alpa is absent. We have endeavored to adopt the code, yet encountered compilation errors. Consequently, experiments related to these models on Alpa are marked as \texttt{N/A}.
    \end{tablenotes}
    \end{threeparttable}
\end{small}
\end{center}
\vskip -0.2in
\end{table*}

% % Figure environment removed


\subsection{Training throughput and strategy optimization time}\label{subsec:experiment:optimization-time-and-throughput}
We compare the training throughput and strategy optimization time of UniAP with those of the baselines on \textsc{EnvA}, \textsc{EnvB}, and \textsc{EnvC}. For experiments conducted on \textsc{EnvA}, we set the mini-batch size to be 32, 16, 128, and 128 for BERT, T5, ViT, and Swin, respectively. For experiments conducted on \textsc{EnvB}, we set the mini-batch size to be 16, 8, 64, and 32 for these four models, respectively. For the Llama model~\citep{touvron_llama_2023,touvron_llama_2023-1} run on \textsc{EnvC}, we set the mini-batch size to be 8.

We choose Galvatron~\citep{miao_galvatron_2022} and Alpa~\citep{zheng_alpa_2022} as baselines because they have achieved SOTA performance.
Specifically, Galvatron has surpassed other methods, including PyTorch DDP~\citep{li_pytorch_2020}, Megatron-LM~\citep{narayanan_efficient_2021}, FSDP~\citep{rajbhandari_zero_2020,fairscale_authors_fairscale_2021}, GPipe~\citep{huang_gpipe_2019}, and DeepSpeed 3D~\citep{deepspeed-3d} in terms of training throughput, as reported in the original paper~\citep{miao_galvatron_2022}.
Furthermore, Alpa utilizes the Just-In-Time~(JIT) compilation feature in JAX and outperforms Megatron-LM and DeepSpeed.
 
Table~\ref{tab:throughput_and_optimization_time} shows the training throughput and strategy optimization time on \textsc{EnvA}, \textsc{EnvB}, and \textsc{EnvC}. On \textsc{EnvA}, UniAP and Galvatron get the same optimal strategy for BERT-Huge, T5-Large, and ViT-Huge, outperforming Alpa in terms of training throughput and strategy optimization time. In addition, UniAP finds a solution for Swin-Huge, while Galvatron encounters CUDA OOM problems. In particular, UniAP achieves a maximum optimization speedup that is 17$\times$ faster than Galvatron and hundreds of times faster than Alpa on BERT-Huge. This is mainly due to the ability of the MIQP solver to search for an optimal strategy on multiple threads, while the dynamic programming based methods like Galvatron and Alpa run on a single thread due to their strong data dependency.

On \textsc{EnvB}, UniAP consistently demonstrates competitive or larger training throughput compared to Galvatron and Alpa. We attribute the performance improvement to the larger strategy space in UniAP. A detailed study of the importance of strategy space is provided in Section~\ref{subsec:experiment:ablation-study}. Furthermore, the strategy optimization time of UniAP is also significantly shorter than the two baseline methods.

On \textsc{EnvC}, UniAP shows an optimization speedup of 11.83$\times$ and a training speedup of 3.80$\times$ compared to Galvatron on Llama. We examine the parallel strategy they adopted. The results show that UniAP employs an 8-stage PP with a micro-batch size of 1, whereas Galvatron employs a 4-stage PP with a micro-batch size of 3 (with the final micro-batch containing 2 samples). Within each PP stage, Galvatron utilizes a 2-way TP. For \textsc{EnvC}, which is equipped with 8 A100 40GB PCIe GPUs, minimizing communication volume is critical. Since PP has significantly less inter-device communication volume than TP, the parallel strategy discovered by UniAP is more reasonable than Galvatron in this case.

To facilitate further discussions, we provide another case study of the optimal parallel strategy for BERT-Huge in Appendix~\ref{appendix:case-study}.

% By the way, we evaluate UniAP on a large cluster of 32 accelerators. We train Llama-7B and compare UniAP with the widely used framework DeepSpeed and Megatron. Find more details in Appendix~\ref{appendix:dcu}
% 
% Moreover, we find that UniAP identifies superior solutions with higher model FLOPs utilization~(MFU)~\citep{chowdhery_palm_2023}, compared to Galvatron and Alpa. For example, the resulting MFUs on BERT-Huge for UniAP, Galvatron, and Alpa are 58.44\%, 58.44\%, and 55.10\% on \textsc{EnvA}, while 23.6\%, 13.7\%, and 19.6\% on \textsc{EnvB}. These results are due to the larger strategy space of UniAP, which is achieved by jointly optimizing inter- and intra-layer AP using an MIQP formulation.

% % Figure environment removed

% % Figure environment removed

\subsection{Estimation accuracy}\label{subsec:experiment:performance_modeling}
Some variables in UniAP and other AP methods are estimated values rather than actual running values. The TPI~(inverse of training throughput) is the most important one. Accurate estimation for TPI or training throughput is crucial for evaluating candidate parallel strategies and ensuring the optimality of the solution. We hereby introduce a metric called \emph{relative estimation error~(REE)} $e$ to quantify the accuracy of the estimated training throughput $\hat{T}$ to the actual training throughput $T$:
\begin{equation}
    e(T, \hat{T}) = \frac{|T - \hat{T}|}{T} \times 100\%.
\end{equation}
We evaluate the optimal parallel strategies obtained from \textsc{EnvA} and \textsc{EnvB}. Results show that UniAP achieves an average REE of 3.59\%, which is lower than 11.17\% for Galvatron.

\subsection{Scalability}\label{subsec:experiment:scalability}
% Figure environment removed

We study the scalability of UniAP using BERT, T5, ViT, and Swin on \textsc{EnvD}. We set the mini-batch sizes for each data point as 8, 4, 32, and 16 times the number of nodes~(denoted as `\#nodes'). The experimental results are shown in Figure~\ref{fig:scalability}. In Figure~\ref{fig:scalability:throughput}, the training throughput of the optimal strategy demonstrate near-linearity as the number of nodes and mini-batch size increase. In Figure~\ref{fig:scalability:optimization-time}, the strategy optimization time matches the complexity analysis in Section~\ref{subsec:method:complexity-analysis}.

Additionally, we use \textsc{EnvE} to scale UniAP to larger clusters and larger models including Llama-13B. UniAP also outperforms other baselines in this experiment. Details are available in Appendix~\ref{appendix:dcu}.


% and the corresponding strategy optimization time demonstrate near-linearity This phenomenon reveals that UniAP is scalable and verifies the computational complexity analysis in Section~\ref{subsec:method:unified-optimization}.

\subsection{Ablation study}\label{subsec:experiment:ablation-study}
\begin{table}
\vskip -0.2in
\caption{Ablation study on the importance of unifying strategy space.}
\label{tab:ablation}%
\begin{center}
\begin{small}
\begin{threeparttable}
    \begin{tabular}{cccc}
    \toprule
    \multirow{3}[0]{*}{Model} & \multicolumn{3}{c}{Training throughput (samples/s)} \\
    \cmidrule(r){2-4}
          & UniAP (Inter-only) & UniAP (Intra-only) & UniAP \\
    \midrule
    BERT-Huge &  \texttt{SOL}$\times$\tnote{1)} &  2.48~$\pm$~0.02 &  \textbf{10.77~$\pm$~0.13} \\
    T5-Large &  \texttt{SOL}$\times$\tnote{1}   &  2.92~$\pm$~0.01 &  \textbf{9.01~$\pm$~0.06} \\
    ViT-Huge &  \textbf{45.58~$\pm$~0.54} & \verb+CUDA+$\times$\tnote{2}   &  \textbf{45.58~$\pm$~0.54} \\
    Swin-Huge &  \textbf{19.08~$\pm$~0.10} &  4.66~$\pm$~0.02 &  \textbf{19.08~$\pm$~0.10} \\
    \bottomrule
\end{tabular}%
\begin{tablenotes}
    \footnotesize
    \item[1)] \texttt{SOL}$\times$: No solution after strategy optimization.
    \item[2)] \texttt{CUDA}$\times$: CUDA OOM exceptions during model training.
\end{tablenotes}
\end{threeparttable}
\end{small}
\end{center}
\vskip -0.2in
\end{table}%

We investigate the importance of the strategy space for the optimality of parallel strategies with an ablation study. Specifically, we constrain the strategy space to inter-layer-only and intra-layer-only strategies and evaluate the training throughput of the resulting optimal strategy on \textsc{EnvB}. We set the mini-batch sizes to be 16, 12, 64, and 32, respectively. 

Results are shown in Table~\ref{tab:ablation}. We can find that constraining the strategy space compromises the optimality of parallel strategies or gets strategies that encounter OOM across different models. 
%For example, constraining the strategy space to inter-layer-only parallelism yield infeasible solution for BERT-Huge on \textsc{EnvD}. Meanwhile, constraining the strategy space to intra-layer-only parallelism may reduce the training throughput by 4.34$\times$ for BERT-Huge on \textsc{EnvD}.
Hence, unifying inter- and intra-layer AP for joint optimization is essential and necessary. 

% \subsection{Training Large Language Model}\label{subsec:experiment:case-study-llama}
% Finally, given the widespread adoption of LLMs, we extend UniAP to one of the open-source LLMs to validate the soundness of our algorithm. Specifically, we conduct assessments on strategy optimization time and training throughput for Llama~\citep{touvron_llama_2023,touvron_llama_2023-1} on \textsc{EnvC}. We have effectively integrated FP16 mixed precision~\citep{micikevicius_mixed_2018} into profiling, strategy optimization, and training, highlighting the independence of UniAP from common training techniques. Apart from mixed precision training, we established the mini-batch size as 8, sequence length as 2048, and deactivated activation checkpointing~\citep{chen_training_2016}.
% \begin{table}
% \caption{Strategy optimization time (min.) and training throughput (samples/s) of Llama-7B.}
%     \label{tab:llama}
%     \vskip 0.15in
%     \begin{center}
%     \begin{small}
%     \begin{sc}
%     \begin{tabular}{ccc}
%     \toprule
%         Method & \makecell{Strategy Sear-\\ching Time} &  \makecell{Training\\Throughput} \\
%     \midrule
%         Megatron &  &\\
%         Galvatron & & \\
%         UniAP & \textbf{0.49~$\pm$~0.007} & \textbf{1.96~$\pm$~0.03}\\
%     \bottomrule
%     \end{tabular}
%     \end{sc}
%     \end{small}
%     \end{center}
%     \vskip -0.1in
% \end{table}

% As depicted in Table~\ref{tab:llama}, UniAP consistently shows shorter strategy optimization time and higher training throughput compared to two baseline systems. Therefore, we conclude that UniAP is also well-suited for LLMs.