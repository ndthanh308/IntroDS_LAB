\section{Background}
\subsection{Parallel Strategy}
\label{subsec:background:parallel-strtegy}
\paragraph{Pipeline parallelism~(PP)} In PP, each worker~(machine or GPU) holds a subset of model layers. Adjacent layers on different workers need to transfer activations in the forward propagation~(FP) step and gradients in the backward propagation~(BP) step. 
\paragraph{Data parallelism~(DP)} In DP, each worker holds a replica of the whole model and partitions training samples. In each iteration, each worker computes gradients and synchronizes them with the other workers using all-reduce collective communication~(CC). All workers will have the same model parameters after the synchronization step.
\paragraph{Tensor parallelism~(TP)} In TP, each worker holds a replica of training samples and partitions within model layers. In each iteration, each worker computes its local outputs in FP and its local gradients in BP. To synchronize outputs and gradients, all workers will perform all-reduce CC in FP and BP steps according to the partition scheme.
\paragraph{Fully sharded data parallelism~(FSDP)} FSDP partitions optimizer states, parameters and gradients of the model into separate workers. During the FP and BP step of each iteration, FSDP performs an all-gather CC to obtain the complete parameters for the relevant layer, respectively. After computing the gradients, FSDP conducts a reduce-scatter CC to distribute the global gradients among the workers.

\subsection{Manual Parallelism}
MP refers to the parallel methods in which human experts design and optimize the parallel strategies. Representative MP methods include Megatron-LM~\citep{narayanan_efficient_2021}, Mesh-TensorFlow~\citep{shazeer_mesh-tensorflow_2018}, and GSPMD~\citep{xu_gspmd_2021}. Megatron-LM manually designs TP and PP strategies for training Transformer-based models and exhibits superior efficiency. Mesh-TensorFlow and GSPMD require human effort to designate and tune the intra-layer parallel strategy. These methods rely on expert design and have little flexibility, challenging their automatic application to other models.

\subsection{Automatic Parallelism}
\paragraph{Inter-layer-only AP or intra-layer-only AP} For inter-layer-only AP, GPipe~\citep{huang_gpipe_2019} and vPipe~\citep{zhao_vpipe_2022} employ a balanced partition algorithm and a dynamic layer partitioning middleware to partition pipelines, respectively. For intra-layer-only AP, OptCNN~\citep{jia_exploring_2018}, TensorOpt~\citep{cai_tensoropt_2022}, and Tofu~\citep{wang_supporting_2019} employ dynamic programming methods to optimize DP and TP strategies together. FlexFlow~\citep{jia_beyond_2019} and Automap~\citep{schaarschmidt_automap_2021} use the Monte Carlo method to find the optimal DP and TP strategy. Colossal-Auto~\citep{liu_colossal-auto_2023} utilizes integer programming techniques to generate intra-layer parallelism and activation checkpointing strategies without optimizing inter-layer parallelism. All these methods optimize only one category of parallel strategies.


\paragraph{Inter- and intra-layer AP} PipeDream~\citep{narayanan_pipedream_2019}, DAPPLE~\citep{fan_dapple_2021}, and PipeTransformer~\citep{he_pipetransformer_2021} use dynamic programming to determine optimal strategies for both DP and PP. DNN-partitioning~\citep{tarnawski_efficient_2020} adopts integer and dynamic programming to explore DP and PP strategies. Piper~\citep{tarnawski_piper_2021} and Alpa~\citep{zheng_alpa_2022} adopt a parallel method considering DP, TP, and PP.
Galvatron~\citep{miao_galvatron_2022} uses dynamic programming to determine DP, TP, and FSDP strategies in a single pipeline stage. As for PP, it partitions stages and determines micro-batch size using naive greedy algorithms. All these methods are hierarchical, which will result in sub-optimal solutions.

