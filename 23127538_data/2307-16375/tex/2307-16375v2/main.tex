\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{booktabs}

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[arxiv]{conference2024}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\allowdisplaybreaks[4]

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}


\conferencetitlerunning{UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming}

\begin{document}

\twocolumn[
\conferencetitle{\texorpdfstring{UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism\\by Mixed Integer Quadratic Programming}{UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming}}

\conferencesetsymbol{equal}{*}

\begin{conferenceauthorlist}
\conferenceauthor{Hao Lin}{equal,nju}
\conferenceauthor{Ke Wu}{equal,nju}
\conferenceauthor{Jie Li}{nju}
\conferenceauthor{Jun Li}{nju}
\conferenceauthor{Wu-Jun Li}{nju}
\end{conferenceauthorlist}

\conferenceaffiliation{nju}{National Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing University, Nanjing 210023, China}

\conferencecorrespondingauthor{Wu-Jun Li}{liwujun@nju.edu.cn}

\conferencekeywords{Automatic Parallelism, Mixed Integer Quadratic Programming}

\vskip 0.3in
]


\printAffiliationsAndNotice{\conferenceEqualContribution}

\begin{abstract}
Distributed learning is commonly used for training deep learning models, especially large models. In distributed learning, manual parallelism~(MP) methods demand considerable human effort and have limited flexibility. Hence, automatic parallelism~(AP) methods have recently been proposed for automating the parallel strategy optimization process. Existing AP methods suffer from sub-optimal solutions because they do not jointly optimize the two categories of parallel strategies~(i.e., inter-layer parallelism and intra-layer parallelism). In this paper, we propose a novel AP method called UniAP, which unifies inter- and intra-layer automatic parallelism by mixed integer quadratic programming. To the best of our knowledge, UniAP is the first parallel method that can jointly optimize the two categories of parallel strategies to find an optimal solution. Experimental results show that UniAP outperforms state-of-the-art methods by up to 1.71$\times$ in throughput and reduces strategy optimization time by up to 107$\times$ across five Transformer-based models.
\end{abstract}

\input{1_introduction}


\input{2_background}


\input{3_method}


\input{4_experiments}


\input{5_conclusion}

\newpage

\input{6_impact_statement}

\input{bibiliography.bbl}


\input{7_supplementary}


\end{document}
