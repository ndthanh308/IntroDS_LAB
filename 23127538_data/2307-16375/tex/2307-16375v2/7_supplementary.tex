\newpage
\appendix
\onecolumn

\section{Proof of the Linear Form for the Contiguous Set}\label{appendix:linear-form-of-contigous-constraint}
To facilitate our discussion, we adopt the linear form of the order-preserving constraint as presented in the main paper. We denote $P_{ui}$ as a 0-1 variable indicating whether layer $u$ is to be placed on the $i$-th computation stage, $deg$ as the number of computation stages in the pipeline. Besides, $\mathcal{G}(V, E)$ represents the computation graph for the model. Then, we formalize the theorem as follows:

\begin{theorem}
    A subgraph $V_i=\{\forall u\in V: P_{ui}=1\}$ is contiguous if and only if there exists $Z_{vi}$ such that \eqref{eqn:method:order-preserving:1}, \eqref{eqn:method:order-preserving:2}, and \eqref{eqn:method:order-preserving:3} are satisfied.
\end{theorem}

Previous work \citep{tarnawski_efficient_2020} has proven this theorem. Our proof draws on the process of this work. The details of the proof are as follows:

\begin{proof}
\vskip -0.1in
    "If": Assume that there exists nodes $u, w\in V_i$ and $v \notin V_i$ such that $v$ and $w$ are reachable from $u$ and $v$, respectively. Hence, $P_{ui} = 1$, $P_{wi} = 1$, and $P_{vi} = 0$. Without losing generality, we assume $\langle u, v\rangle\in E$. Thus, according to \eqref{eqn:method:order-preserving:3}, we have $Z_{vi}\leqslant P_{vi}-P_{ui}+1=0$. By applying \eqref{eqn:method:order-preserving:2} repeatedly following the path from $v$ to $w$, we have $Z_{wi}\leqslant Z_{vi}$. Thus, $Z_{wi}\leqslant 0$. However, we also have $Z_{wi}\geqslant P_{wi}=1$ according to \eqref{eqn:method:order-preserving:1}. A contradiction.

    "Only if": First, we define $Z_{vi}=1$ if a node $w\in S$ is reachable from $v$. Otherwise, $Z_{vi}=0$. Thus, \eqref{eqn:method:order-preserving:1} and \eqref{eqn:method:order-preserving:2} are satisfied according to this kind of definition. For \eqref{eqn:method:order-preserving:3}, if $P_{vi}=1$, the constraint will hold true regardless of whether $P_{ui}$ is $1$ or $0$. If $P_{vi}=0$ and $P_{ui}=0$, $Z_{vi}\leqslant P_{vi}-P_{ui}+1=1$ will also hold true because $Z_{vi}$ could be either $0$ or $1$. Finally, if $P_{vi}=0$ and $P_{ui}=1$, $Z_{vi}=0$ will hold true because $V_i$ is a contiguous set and we cannot find any $w\in V_i$, such that $w$ is reachable from $v$.
\end{proof}

\section{QIP Formulation for Intra-layer-only Parallelism}\label{appendix:miqp-for-intra-layer-parallelism}
Here we present the QIP formulation for intra-layer-only parallelism with explanations.

\paragraph{Objective function} In terms of intra-layer-only parallelism, there is only one computation stage involved. As a result, the objective function takes into account only the value of $p_1$. We hereby formalize the equation as
\begin{equation}
    \min\quad tpi_{gpipe}=p_1.\label{eqn:appendix:lp-no-pp}
\end{equation}

\paragraph{Computation-stage constraint} With only one computation stage in intra-layer-only parallelism, the communication-stage constraint can be omitted, and the computation and communication cost can be modeled for $p_1$. Thus, we could formalize the constraint as
\begin{equation}
\sum_{u\in V}S_{u}^\mathsf{T}A_{u}+\sum_{\langle u,v\rangle\in E}S_{u}^\mathsf{T}R_{uv}S_{v}=p_1.~\label{eqn:appendix:computation-for-no-pp}
\end{equation}
In the equation, the first summation term for any $u\in V$ represents the cost of choosing intra-layer strategies for all layers, while the second term represents the summation of resharding costs on all edges.


\paragraph{Memory constraint} Similar to the memory constraint in MIQP, it is necessary to ensure that the memory usage on a single device does not exceed its device memory bound $m$ in QIP. This restriction gives 
\begin{equation}
    \sum_{u\in V}S_u^\mathsf{T} M_u\leqslant m.~\label{eqn:appendix:memory-constraint-for-no-pp}
\end{equation}
It is worth noting that $m$ should be an identical constant across multiple devices if these devices are homogeneous. Otherwise, the value of $m$ varies.

\paragraph{Strategy-selection constraint} For intra-layer-only parallelism, the layer-placement constraint can be safely omitted because it is designed for PP. However, the strategy-selection constraint is necessary because each layer can only select one intra-layer strategy. Therefore, the strategy-selection constraint for QIP is identical to \eqref{eqn:method:strategy-selection:1} and \eqref{eqn:method:strategy-selection:2} for MIQP.

By combining objective function \eqref{eqn:appendix:lp-no-pp} and constraints \eqref{eqn:method:strategy-selection:1}, \eqref{eqn:method:strategy-selection:2}, \eqref{eqn:appendix:computation-for-no-pp}, and \eqref{eqn:appendix:memory-constraint-for-no-pp}, we have the QIP expression for optimizing the intra-layer-only AP. Like MIQP expression for optimizing the inter- and intra-layer AP, UniAP will eventually get the minimum TPI and corresponding parallel strategies by invoking the off-the-shelf solver.

\section{Visualization for the Candidate Solution}\label{appendix:visualization_for_p_and_s}
% Figure environment removed

In this section, we proceed to visually represent a potential solution for UOP. Given a deep learning model $\mathcal{G}$, pipeline degree $deg$, and number of micro-batches $c$, UniAP will determine layer placement $P$ for inter-layer parallelism and parallel strategy $S$ for intra-layer parallelism using an off-the-shelf solver. As Figure \ref{fig:ps_explanation} shows, the solver is optimizing a three-layer model with two pipeline stages, each assigned four GPUs. At this time, a candidate solution could be
\begin{equation}
    P=\begin{bmatrix}
        1 & 0 \\
        1 & 0 \\
        0 & 1
    \end{bmatrix},~
    S=
    \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 1 \\
        0 & 0 & 0 \\ 
        \vdots & \vdots & \vdots \\
        0 & 0 & 0
    \end{bmatrix}.
\end{equation}

Here, the $u$-th row of matrix $P$ denotes the placement strategy for layer $u$, where $P_{ui}=1$ signifies the placement of layer $u$ on stage $i$, while $0$ indicates otherwise. For example, $P_{l_{0}}=\left[1,~0\right]$ denotes the placement of layer $l_0$ on pipeline stage 1. Additionally, the $u$-th column of matrix $S$ denotes the selected intra-layer parallel strategy for layer $u$, where $S_{uj}=1$ denotes the selection of the $j$-th strategy from the intra-layer parallel strategy set. For example, $S_{l_{0}}=\left[1,~0,~0,~\cdots,~0\right]^{\mathsf{T}}$ indicates that layer $l_0$ will adopt only the DP strategy, while $S_{l_{1}}=\left[0,~1,~0,~\cdots,~0\right]^{\mathsf{T}}$ indicates that layer $l_1$ will employ a strategy where DP is performed on GPU 0, 1 and GPU 2, 3, and TP is performed across these two GPU groups.

There exist numerous combinations of $P$ and $S$. The off-the-shelf solver will automatically search for the optimal solution given pipeline degree $deg$ and the number of micro-batches $c$. By solving the MIQP expression and enumerating every possible $deg$ and $c$ in the UOP process, UniAP will ultimately derive an optimal parallel strategy for the deep learning model within the current hardware environment.

\section{Experiment Detail}\label{appendix:experiment-settings}
\paragraph{Gurobi configuration} When tackling the MIQP problem, UniAP employs several configurations for the Gurobi Optimizer 10.1~\citep{gurobi_optimization_llc_gurobi_2023}. In particular, we set \textit{TimeLimit} to 60 seconds, \textit{MIPFocus} to 1, \textit{NumericFocus} to 1, and remain other configurations to default. For instance, we establish the \textit{MIPGap} parameter as the default value of 1e-4 to serve as a strict termination criterion. Furthermore, we have implemented an early stopping mechanism to terminate the optimization process as early as possible. There are two conditions that can activate the mechanism. Firstly, if the current runtime exceeds 15 seconds and the relative MIP optimality gap is less than 4\%, we will terminate the optimization. Secondly, if the current runtime exceeds 5 seconds and the best objective bound is worse than the optimal solution obtained in the previous optimization process, we will terminate the optimization.

\begin{table}[t]
\caption{Details for five Transformer-based models. L: Number of hidden layers; H: Hidden size; S: Sequence length.}
\label{tab:appendix-details-for-five-transformer-based-models}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
    \begin{tabular}{ccccc}
    \toprule
    Model & L & H & S & \#params \\
    \midrule
    BERT-Huge & 32    & 1280  & 512   & 672M \\
    T5-Large & 16/16 & 1024  & 512   & 502M \\
    ViT-Huge & 32    & 1280  & 196 & 632M \\
    Swin-Huge & 2/2/42/2 & 320/640/1280/2560 & 49*64/49*16/49*4/49*1 & 1.02B \\
    LLaMA-7B & 32 & 4096 & 2048 & 7B \\
    \bottomrule
    \end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\paragraph{Model detail} Table~\ref{tab:appendix-details-for-five-transformer-based-models} presents the details of five Transformer-based models selected for our evaluations. Three of these models, namely BERT-Huge~\citep{devlin_bert_2019}, T5-Large~\citep{raffel_exploring_2020}, and LLaMA-7B~\citep{touvron_llama_2023,touvron_llama_2023-1}, belong to the domain of natural language processing (NLP). At the same time, the remaining two, ViT-Huge~\citep{dosovitskiy_image_2021} and Swin-Huge~\citep{liu_swin_2021}, are associated with computer vision (CV). It is noteworthy that BERT, ViT, and LLaMA maintain consistent types of hidden layers respectively, whereas T5 and Swin have different types of hidden layers. Numbers separated by slashes represent the statistical information for different layer types. For instance, Swin-Huge comprises four types of layers, each with 2, 2, 42, and 2 layers, respectively.

\paragraph{Training detail} UniAP is based on the PyTorch framework and integrates models from HuggingFace Transformers. It employs various types of parallelism, including Pipeline Parallelism~(PP), Data Parallelism~(DP), Tensor Parallelism~(TP), and Fully Sharded Data Parallelism~(FSDP), utilizing GPipe~\citep{huang_gpipe_2019}, PyTorch DDP~\citep{li_pytorch_2020}, Megatron-LM~\citep{narayanan_efficient_2021}, and FairScale~\citep{fairscale_authors_fairscale_2021}, respectively. For NLP models, we use the English Wikipedia dataset~\citep{wikidump}, while the ImageNet-1K dataset~\citep{imagenet15russakovsky} is used for CV models. We train these models using the Adam optimizer~\citep{kingma_adam_2015} and precision of FP32. We omit hyperparameters here such as learning rate and weight decay as these have minimal impact on training throughput. 
The model parameters in the HuggingFace Transformer are configured to align with the specifications of each individual model. For instance, we set \textit{hidden\_size} to 1280, \textit{num\_hidden\_layers} to 32, \textit{num\_attention\_heads} to 16, and \textit{seq\_length} to 512 for BERT-Huge. Regarding other hyperparameters in the HuggingFace configurations, we set \textit{hidden\_dropout\_prob} and \textit{attention\_probs\_dropout\_prob} to 0.0 for ViT-Huge. For Swin-Huge, we set \textit{drop\_path\_rate} to 0.2. We remain other configurations to default. It should be noted that the training batch sizes for each experiment are outlined in the main paper.

\section{Case study: BERT-Huge}\label{appendix:case-study}
% Figure environment removed
In this section, we present a visualization of the optimal parallel strategy discovered by UniAP. As represented in Figure \ref{fig:visualization-model}, the strategy pertains to training BERT-Huge with 32 hidden layers in a 2-node environment \textit{EnvB} with a mini-batch size of 16. Each node was equipped with 2 Xeon E5-2620 v4 CPUs, 4 TITAN Xp 12GB GPUs, and 125GB memory. These nodes are interconnected via a 10Gbps network. It should be noted that we only showcase the parallel strategy for the hidden layers here for simplicity but without losing generality.

Here, we provide further topological information for a node within \textit{EnvB}. As illustrated in Figure~\ref{fig:topo}, we categorize the GPUs numbered 0 and 1 in each node and refer to them collectively as \textit{GPUGroup0}. Similarly, we label the GPUs numbered 2 and 3 as \textit{GPUGroup1}. In \textit{EnvB}, the interconnects within each GPU group (i.e., PCIe) have superior bandwidth than that between different groups (i.e., QPI). We collectively designate these two connection bandwidths as intra-node bandwidth, which is higher than inter-node bandwidth.

% Figure environment removed

In this example, UniAP has identified a parallel strategy for inter-layer parallelism that involves a two-stage pipeline. This strategy utilizes parallelism in a manner that is both efficient and effective. Specifically, the communication cost of point-to-point~(P2P) between two nodes is less than that of all-reduce. Additionally, the inter-node bandwidth is lower than that of the intra-node. These factors make the two-stage PP a reasonable choice. Moreover, the pipeline has been designed such that each stage comprises an equal number of layers. This design leverages the homogeneity of the nodes and ensures load balancing across the cluster.

UniAP employs an intra-layer parallel strategy within each PP stage. It utilizes a 2-way DP for the initial 12 hidden layers in each stage between \textit{GPUGroup0} and \textit{GPUGroup1}. For the remaining four hidden layers, a 2-way FSDP is utilized between \textit{GPUGroup0} and \textit{GPUGroup1} to reduce memory footprint and meet memory constraints. Within each GPU group, UniAP employs a 2-way TP for each layer. In general, TP incurs more significant communication volumes than DP and FSDP. In order to achieve maximum training throughput on \textit{EnvB}, it is necessary to implement parallel strategies that prioritize higher communication volumes within each group and lower volumes between groups. Therefore, the strategy for BERT-Huge with 32 hidden layers combines the best elements of PP, DP, TP, and FSDP to maximize training throughput.

In addition, we have conducted calculations for the model FLOPs utilizatio~(MFU) for Galvatron, Alpa, and UniAP in this scenario to validate our analysis. MFU is a metric introduced by \citet{chowdhery_palm_2023}, which is independent of hardware, frameworks, or implementations. Therefore, it allows us to examine the performance of different parallel strategies solely from a strategic perspective. For BERT-Huge-32, the resulting MFUs for UniAP, Galvatron, and Alpa on \textit{EnvB} are 23.6\%, 13.7\% and 19.6\%
, respectively. Therefore, we conclude that UniAP does utilize its larger strategy space to find an optimal solution, rather than a sub-optimal one.
