\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei]{brown_language_2020}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
  Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,
  D.~M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,
  S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever,
  I., and Amodei, D.
\newblock Language {Models} are {Few}-{Shot} {Learners}.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}
  33}, pp.\  1877--1901, 2020.

\bibitem[Cai et~al.(2022)Cai, Yan, Ma, Wu, Huang, Cheng, Su, and
  Yu]{cai_tensoropt_2022}
Cai, Z., Yan, X., Ma, K., Wu, Y., Huang, Y., Cheng, J., Su, T., and Yu, F.
\newblock {TensorOpt}: {Exploring} the {Tradeoffs} in {Distributed} {DNN}
  {Training} {With} {Auto}-{Parallelism}.
\newblock \emph{IEEE Transactions on Parallel and Distributed Systems},
  33\penalty0 (8):\penalty0 1967--1981, 2022.

\bibitem[Chen et~al.(2016)Chen, Xu, Zhang, and Guestrin]{chen_training_2016}
Chen, T., Xu, B., Zhang, C., and Guestrin, C.
\newblock Training {Deep} {Nets} with {Sublinear} {Memory} {Cost}.
\newblock \emph{CoRR}, abs/1604.06174, 2016.

\bibitem[Chowdhery et~al.(2023)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez,
  Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury,
  Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski,
  Garcia, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov,
  Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira,
  Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat, Catasta, Wei,
  Meier-Hellstern, Eck, Dean, Petrov, and Fiedel]{chowdhery_palm_2023}
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,
  Barham, P., Chung, H.~W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K.,
  Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N.,
  Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J.,
  Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A.,
  Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K.,
  Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov,
  A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A.~M., Pillai,
  T.~S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee,
  K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J.,
  Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N.
\newblock {PaLM}: {Scaling} {Language} {Modeling} with {Pathways}.
\newblock \emph{Journal of Machine Learning Research}, 24:\penalty0
  240:1--240:113, 2023.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova]{devlin_bert_2019}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock {BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for
  {Language} {Understanding}.
\newblock In \emph{The {North} {American} {Chapter} of the {Association} for
  {Computational} {Linguistics}: {Human} {Language} {Technologies}}, pp.\
  4171--4186, 2019.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy_image_2021}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,
  Uszkoreit, J., and Houlsby, N.
\newblock An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image}
  {Recognition} at {Scale}.
\newblock In \emph{{International} {Conference} on {Learning} {Representations}
  9}, 2021.

\bibitem[Du et~al.(2022)Du, Huang, Dai, Tong, Lepikhin, Xu, Krikun, Zhou, Yu,
  Firat, Zoph, Fedus, Bosma, Zhou, Wang, Wang, Webster, Pellat, Robinson,
  Meier-Hellstern, Duke, Dixon, Zhang, Le, Wu, Chen, and Cui]{du_glam_2022}
Du, N., Huang, Y., Dai, A.~M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M.,
  Zhou, Y., Yu, A.~W., Firat, O., Zoph, B., Fedus, L., Bosma, M.~P., Zhou, Z.,
  Wang, T., Wang, Y.~E., Webster, K., Pellat, M., Robinson, K.,
  Meier-Hellstern, K.~S., Duke, T., Dixon, L., Zhang, K., Le, Q.~V., Wu, Y.,
  Chen, Z., and Cui, C.
\newblock {GLaM}: {Efficient} {Scaling} of {Language} {Models} with
  {Mixture}-of-{Experts}.
\newblock In \emph{International {Conference} on {Machine} {Learning}}, pp.\
  5547--5569, 2022.

\bibitem[{FairScale authors}(2021)]{fairscale_authors_fairscale_2021}
{FairScale authors}.
\newblock {FairScale}: {A} general purpose modular {PyTorch} library for high
  performance and large scale training.
\newblock \url{https://github.com/facebookresearch/fairscale}, 2021.

\bibitem[Fan et~al.(2021)Fan, Rong, Meng, Cao, Wang, Zheng, Wu, Long, Yang,
  Xia, Diao, Liu, and Lin]{fan_dapple_2021}
Fan, S., Rong, Y., Meng, C., Cao, Z., Wang, S., Zheng, Z., Wu, C., Long, G.,
  Yang, J., Xia, L., Diao, L., Liu, X., and Lin, W.
\newblock {DAPPLE}: {A} {Pipelined} {Data} {Parallel} {Approach} for {Training}
  {Large} {Models}.
\newblock In \emph{{Symposium} on {Principles} and {Practice} of {Parallel}
  {Programming}}, pp.\  431--445, 2021.

\bibitem[Fedus et~al.(2022)Fedus, Zoph, and Shazeer]{fedus_switch_2022}
Fedus, W., Zoph, B., and Shazeer, N.
\newblock Switch {Transformers}: {Scaling} to {Trillion} {Parameter} {Models}
  with {Simple} and {Efficient} {Sparsity}.
\newblock \emph{The Journal of Machine Learning Research}, 23\penalty0
  (120):\penalty0 5232–5270, 2022.

\bibitem[Flynn(1966)]{flynn_very_1966}
Flynn, M.~J.
\newblock Very {High-speed} {Computing} {Systems}.
\newblock \emph{Proceedings of the IEEE}, 54\penalty0 (12):\penalty0
  1901--1909, 1966.

\bibitem[Flynn(1972)]{flynn_computer_1972}
Flynn, M.~J.
\newblock Some {Computer} {Organizations} and {Their} {Effectiveness}.
\newblock \emph{IEEE Transactions on Computers}, C-21\penalty0 (9):\penalty0
  948--960, 1972.

\bibitem[{Gurobi Optimization, LLC}(2023)]{gurobi_optimization_llc_gurobi_2023}
{Gurobi Optimization, LLC}.
\newblock Gurobi {Optimizer} {Reference} {Manual}.
\newblock \url{https://www.gurobi.com}, 2023.

\bibitem[He et~al.(2021)He, Li, Soltanolkotabi, and
  Avestimehr]{he_pipetransformer_2021}
He, C., Li, S., Soltanolkotabi, M., and Avestimehr, S.
\newblock {PipeTransformer}: {Automated} {Elastic} {Pipelining} for
  {Distributed} {Training} of {Large}-scale {Models}.
\newblock In \emph{{International} {Conference} on {Machine} {Learning}}, pp.\
  4150--4159, 2021.

\bibitem[Huang et~al.(2019)Huang, Cheng, Bapna, Firat, Chen, Chen, Lee, Ngiam,
  Le, Wu, and Chen]{huang_gpipe_2019}
Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen, M.~X., Lee, H.,
  Ngiam, J., Le, Q.~V., Wu, Y., and Chen, Z.
\newblock {GPipe}: {Efficient} {Training} of {Giant} {Neural} {Networks} using
  {Pipeline} {Parallelism}.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}
  32}, pp.\  103--112, 2019.

\bibitem[Jia et~al.(2018)Jia, Lin, Qi, and Aiken]{jia_exploring_2018}
Jia, Z., Lin, S., Qi, C.~R., and Aiken, A.
\newblock Exploring {Hidden} {Dimensions} in {Parallelizing} {Convolutional}
  {Neural} {Networks}.
\newblock In \emph{{International} {Conference} on {Machine} {Learning}}, pp.\
  2279--2288, 2018.

\bibitem[Jia et~al.(2019)Jia, Zaharia, and Aiken]{jia_beyond_2019}
Jia, Z., Zaharia, M., and Aiken, A.
\newblock Beyond {Data} and {Model} {Parallelism} for {Deep} {Neural}
  {Networks}.
\newblock In \emph{{Machine} {Learning} and {Systems}}, pp.\  1--13, 2019.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma_adam_2015}
Kingma, D.~P. and Ba, J.
\newblock Adam: {A} {Method} for {Stochastic} {Optimization}.
\newblock In \emph{3rd {International} {Conference} on {Learning}
  {Representations}}, 2015.

\bibitem[Lazimy(1982)]{lazimy_mixed_1982}
Lazimy, R.
\newblock Mixed-integer {Quadratic} {Programming}.
\newblock \emph{Math. Program.}, 22\penalty0 (1):\penalty0 332–349, December
  1982.

\bibitem[Lepikhin et~al.(2021)Lepikhin, Lee, Xu, Chen, Firat, Huang, Krikun,
  Shazeer, and Chen]{lepikhin_gshard_2021}
Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M.,
  Shazeer, N., and Chen, Z.
\newblock {GShard}: {Scaling} {Giant} {Models} with {Conditional} {Computation}
  and {Automatic} {Sharding}.
\newblock In \emph{{International} {Conference} on {Learning}
  {Representations}}, 2021.

\bibitem[Li \& Hoefler(2021)Li and Hoefler]{li_chimera_2021}
Li, S. and Hoefler, T.
\newblock Chimera: {Efficiently} {Training} {Large}-{Scale} {Neural} {Networks}
  with {Bidirectional} {Pipelines}.
\newblock In \emph{International {Conference} for {High} {Performance}
  {Computing}, {Networking}, {Storage} and {Analysis}}, pp.\  27:1--27:14,
  2021.

\bibitem[Li et~al.(2020)Li, Zhao, Varma, Salpekar, Noordhuis, Li, Paszke,
  Smith, Vaughan, Damania, and Chintala]{li_pytorch_2020}
Li, S., Zhao, Y., Varma, R., Salpekar, O., Noordhuis, P., Li, T., Paszke, A.,
  Smith, J., Vaughan, B., Damania, P., and Chintala, S.
\newblock {PyTorch} {Distributed}: {Experiences} on {Accelerating} {Data}
  {Parallel} {Training}.
\newblock \emph{Proceedings of the VLDB Endowment}, 13\penalty0 (12):\penalty0
  3005--3018, 2020.

\bibitem[Liu et~al.(2023)Liu, Li, Fang, Shao, Yao, and
  You]{liu_colossal-auto_2023}
Liu, Y., Li, S., Fang, J., Shao, Y., Yao, B., and You, Y.
\newblock Colossal-{Auto}: {Unified} {Automation} of {Parallelization} and
  {Activation} {Checkpoint} for {Large}-scale {Models}.
\newblock \emph{CoRR}, abs/2302.02599, 2023.

\bibitem[Liu et~al.(2021)Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and
  Guo]{liu_swin_2021}
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B.
\newblock Swin {Transformer}: {Hierarchical} {Vision} {Transformer} using
  {Shifted} {Windows}.
\newblock In \emph{{International} {Conference} on {Computer} {Vision}}, pp.\
  9992--10002, 2021.

\bibitem[Miao et~al.(2022)Miao, Wang, Jiang, Shi, Nie, Zhang, and
  Cui]{miao_galvatron_2022}
Miao, X., Wang, Y., Jiang, Y., Shi, C., Nie, X., Zhang, H., and Cui, B.
\newblock Galvatron: {Efficient} {Transformer} {Training} over {Multiple}
  {GPUs} {Using} {Automatic} {Parallelism}.
\newblock \emph{Proceedings of the VLDB Endowment}, 16\penalty0 (3):\penalty0
  470--479, 2022.

\bibitem[Micikevicius et~al.(2018)Micikevicius, Narang, Alben, Diamos, Elsen,
  García, Ginsburg, Houston, Kuchaiev, Venkatesh, and
  Wu]{micikevicius_mixed_2018}
Micikevicius, P., Narang, S., Alben, J., Diamos, G.~F., Elsen, E., García, D.,
  Ginsburg, B., Houston, M., Kuchaiev, O., Venkatesh, G., and Wu, H.
\newblock Mixed {Precision} {Training}.
\newblock In \emph{{International} {Conference} on {Learning}
  {Representations}}, 2018.

\bibitem[Microsoft(2021)]{deepspeed-3d}
Microsoft.
\newblock Deepspeed {3D}.
\newblock
  \url{https://github.com/microsoft/Megatron-DeepSpeed/blob/main/examples/pretrain_bert_distributed_with_mp.sh},
  2021.

\bibitem[Narayanan et~al.(2019)Narayanan, Harlap, Phanishayee, Seshadri,
  Devanur, Ganger, Gibbons, and Zaharia]{narayanan_pipedream_2019}
Narayanan, D., Harlap, A., Phanishayee, A., Seshadri, V., Devanur, N.~R.,
  Ganger, G.~R., Gibbons, P.~B., and Zaharia, M.
\newblock {PipeDream}: {Generalized} {Pipeline} {Parallelism} for {DNN}
  {Training}.
\newblock In \emph{{Symposium} on {Operating} {Systems} {Principles}}, pp.\
  1--15, 2019.

\bibitem[Narayanan et~al.(2021{\natexlab{a}})Narayanan, Phanishayee, Shi, Chen,
  and Zaharia]{narayanan_memory-efficient_2021}
Narayanan, D., Phanishayee, A., Shi, K., Chen, X., and Zaharia, M.
\newblock Memory-{Efficient} {Pipeline}-{Parallel} {DNN} {Training}.
\newblock In \emph{{International} {Conference} on {Machine} {Learning}}, pp.\
  7937--7947, 2021{\natexlab{a}}.

\bibitem[Narayanan et~al.(2021{\natexlab{b}})Narayanan, Shoeybi, Casper,
  LeGresley, Patwary, Korthikanti, Vainbrand, Kashinkunti, Bernauer, Catanzaro,
  Phanishayee, and Zaharia]{narayanan_efficient_2021}
Narayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Patwary, M.,
  Korthikanti, V., Vainbrand, D., Kashinkunti, P., Bernauer, J., Catanzaro, B.,
  Phanishayee, A., and Zaharia, M.
\newblock Efficient {Large-scale} {Language} {Model} {Training} on {GPU}
  {Clusters} {Using} {Megatron}-{LM}.
\newblock In \emph{International {Conference} for {High} {Performance}
  {Computing}, {Networking}, {Storage} and {Analysis}}, pp.\  58:1--58:15,
  2021{\natexlab{b}}.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel_exploring_2020}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0
  (1):\penalty0 5485--5551, 2020.

\bibitem[Rajbhandari et~al.(2020)Rajbhandari, Rasley, Ruwase, and
  He]{rajbhandari_zero_2020}
Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y.
\newblock {ZeRO}: {Memory} {Optimizations} {Toward} {Training} {Trillion}
  {Parameter} {Models}.
\newblock In \emph{{International} {Conference} for {High} {Performance}
  {Computing}, {Networking}, {Storage} and {Analysis}}, pp.\  20:1--20:16,
  2020.

\bibitem[Rashidi et~al.(2021)Rashidi, Denton, Sridharan, Srinivasan, Suresh,
  Nie, and Krishna]{rashidi_enabling_2021}
Rashidi, S., Denton, M., Sridharan, S., Srinivasan, S., Suresh, A., Nie, J.,
  and Krishna, T.
\newblock Enabling {Compute}-{Communication} {Overlap} in {Distributed} {Deep}
  {Learning} {Training} {Platforms}.
\newblock In \emph{{International} {Symposium} on {Computer} {Architecture}},
  pp.\  540--553, 2021.

\bibitem[Rasley et~al.(2020)Rasley, Rajbhandari, Ruwase, and
  He]{rasley_deepspeed_2020}
Rasley, J., Rajbhandari, S., Ruwase, O., and He, Y.
\newblock {DeepSpeed}: {System} {Optimizations} {Enable} {Training} {Deep}
  {Learning} {Models} with {Over} 100 {Billion} {Parameters}.
\newblock In \emph{{The} 26th {ACM} {SIGKDD} {Conference} on {Knowledge}
  {Discovery} and {Data} {Mining}}, pp.\  3505--3506, 2020.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]{imagenet15russakovsky}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., Berg, A.~C., and Fei-Fei, L.
\newblock {ImageNet Large Scale Visual Recognition Challenge}.
\newblock \emph{International Journal of Computer Vision}, 115\penalty0
  (3):\penalty0 211--252, 2015.

\bibitem[Schaarschmidt et~al.(2021)Schaarschmidt, Grewe, Vytiniotis, Paszke,
  Schmid, Norman, Molloy, Godwin, Rink, Nair, and
  Belov]{schaarschmidt_automap_2021}
Schaarschmidt, M., Grewe, D., Vytiniotis, D., Paszke, A., Schmid, G.~S.,
  Norman, T., Molloy, J., Godwin, J., Rink, N.~A., Nair, V., and Belov, D.
\newblock Automap: {Towards} {Ergonomic} {Automated} {Parallelism} for {ML}
  {Models}.
\newblock \emph{CoRR}, abs/2112.02958, 2021.

\bibitem[Shazeer et~al.(2018)Shazeer, Cheng, Parmar, Tran, Vaswani,
  Koanantakool, Hawkins, Lee, Hong, Young, Sepassi, and
  Hechtman]{shazeer_mesh-tensorflow_2018}
Shazeer, N., Cheng, Y., Parmar, N., Tran, D., Vaswani, A., Koanantakool, P.,
  Hawkins, P., Lee, H., Hong, M., Young, C., Sepassi, R., and Hechtman, B.~A.
\newblock Mesh-{TensorFlow}: {Deep} {Learning} for {Supercomputers}.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}
  31}, pp.\  10435--10444, 2018.

\bibitem[Tarnawski et~al.(2020)Tarnawski, Phanishayee, Devanur, Mahajan, and
  Paravecino]{tarnawski_efficient_2020}
Tarnawski, J., Phanishayee, A., Devanur, N.~R., Mahajan, D., and Paravecino,
  F.~N.
\newblock Efficient {Algorithms} for {Device} {Placement} of {DNN} {Graph}
  {Operators}.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}
  33}, pp.\  15451--15463, 2020.

\bibitem[Tarnawski et~al.(2021)Tarnawski, Narayanan, and
  Phanishayee]{tarnawski_piper_2021}
Tarnawski, J., Narayanan, D., and Phanishayee, A.
\newblock Piper: {Multidimensional} {Planner} for {DNN} {Parallelization}.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}
  34}, pp.\  24829--24840, 2021.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet,
  Lachaux, Lacroix, Rozière, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave,
  and Lample]{touvron_llama_2023}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix,
  T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin,
  A., Grave, E., and Lample, G.
\newblock {LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}.
\newblock \emph{CoRR}, abs/2302.13971, 2023{\natexlab{a}}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert,
  Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher,
  Canton-Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao,
  Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa,
  Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet,
  Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi,
  Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu,
  Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and
  Scialom]{touvron_llama_2023-1}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y.,
  Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L.,
  Canton-Ferrer, C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J.,
  Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini,
  S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I.,
  Korenev, A., Koura, P.~S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich,
  D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I.,
  Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A.,
  Silva, R., Smith, E.~M., Subramanian, R., Tan, X.~E., Tang, B., Taylor, R.,
  Williams, A., Kuan, J.~X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A.,
  Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and
  Scialom, T.
\newblock Llama 2: {Open} {Foundation} and {Fine}-{Tuned} {Chat} {Models}.
\newblock \emph{CoRR}, abs/2307.09288, 2023{\natexlab{b}}.

\bibitem[Wang et~al.(2019)Wang, Huang, and Li]{wang_supporting_2019}
Wang, M., Huang, C.-C., and Li, J.
\newblock Supporting {Very} {Large} {Models} using {Automatic} {Dataflow}
  {Graph} {Partitioning}.
\newblock In \emph{Proceedings of the {Fourteenth} {EuroSys} {Conference}},
  pp.\  26:1--26:17, 2019.

\bibitem[{Wikimedia Foundation}(2023)]{wikidump}
{Wikimedia Foundation}.
\newblock Wikimedia {Downloads}.
\newblock \url{https://dumps.wikimedia.org}, 2023.

\bibitem[Xu et~al.(2021)Xu, Lee, Chen, Hechtman, Huang, Joshi, Krikun,
  Lepikhin, Ly, Maggioni, Pang, Shazeer, Wang, Wang, Wu, and
  Chen]{xu_gspmd_2021}
Xu, Y., Lee, H., Chen, D., Hechtman, B.~A., Huang, Y., Joshi, R., Krikun, M.,
  Lepikhin, D., Ly, A., Maggioni, M., Pang, R., Shazeer, N., Wang, S., Wang,
  T., Wu, Y., and Chen, Z.
\newblock {GSPMD}: {General} and {Scalable} {Parallelization} for {ML}
  {Computation} {Graphs}.
\newblock \emph{CoRR}, abs/2105.04663, 2021.

\bibitem[Zhao et~al.(2022)Zhao, Li, Chen, Guan, Jiang, Huang, Qing, Wang, Wang,
  Zhang, Li, Luo, and Cui]{zhao_vpipe_2022}
Zhao, S., Li, F., Chen, X., Guan, X., Jiang, J., Huang, D., Qing, Y., Wang, S.,
  Wang, P., Zhang, G., Li, C., Luo, P., and Cui, H.
\newblock {vPipe}: {A} {Virtualized} {Acceleration} {System} for {Achieving}
  {Efficient} and {Scalable} {Pipeline} {Parallel} {DNN} {Training}.
\newblock \emph{IEEE Transactions on Parallel and Distributed Systems},
  33\penalty0 (3):\penalty0 489--506, 2022.

\bibitem[Zheng et~al.(2022)Zheng, Li, Zhang, Zhuang, Chen, Huang, Wang, Xu,
  Zhuo, Xing, Gonzalez, and Stoica]{zheng_alpa_2022}
Zheng, L., Li, Z., Zhang, H., Zhuang, Y., Chen, Z., Huang, Y., Wang, Y., Xu,
  Y., Zhuo, D., Xing, E.~P., Gonzalez, J.~E., and Stoica, I.
\newblock Alpa: {Automating} {Inter}- and {Intra}-{Operator} {Parallelism} for
  {Distributed} {Deep} {Learning}.
\newblock In \emph{{USENIX} {Symposium} on {Operating} {Systems} {Design} and
  {Implementation}}, pp.\  559--578, 2022.

\end{thebibliography}
