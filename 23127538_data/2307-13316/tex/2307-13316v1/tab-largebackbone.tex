\begin{table}[t]
\centering
\renewcommand{\arraystretch}{1.15}
\resizebox{1\linewidth}{!}{\begin{tabular}{c|c|ccc|cc}
\multirow{2}{*}{Method} &\multirow{2}{*}{Backbone} &\multirow{2}{*}{AuPRC$\uparrow$} &\multirow{2}{*}{FPR$_{95}\downarrow$}  & \multirow{2}{*}{FLOPs$\downarrow$} & Training $\downarrow$\\\
& & & & & Parameters \\
\Xhline{3\arrayrulewidth}
Mask2Former~\cite{cheng2022masked}& ResNet-50 &10.60 &89.35 &\textbf{226G} &44M\\

& ResNet-101 &9.11 &45.83 &293G &63M\\

& Swin-T &24.54 &37.98 &232G &42M\\
& Swin-S &30.96 &36.78 &313G &69M\\
\hline
% Mask2Anomaly$^\dag$ & ResNet-50 &28.14 &87.30 &258G &47M\\
Mask2Anomaly$^\ddag$ & ResNet-50 &\textbf{32.35} &\textbf{25.95} &258G &\textbf{23M}\\
\end{tabular}}
\caption{\textbf{Architectural Efficiency of Mask2Anomaly:} Mask2Anomaly outperforms the best Mask2Former architecture having Swin-S backbone with only 30\% trainable parameters. Mask2Anomaly$^\ddag$ only uses global mask attention.}
\label{tab:largebackbone} \vspace{-1em}
\end{table}




