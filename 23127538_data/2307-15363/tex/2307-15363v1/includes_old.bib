
@article{rosenberger_object-independent_2020,
	title = {Object-{Independent} {Human}-to-{Robot} {Handovers} {Using} {Real} {Time} {Robotic} {Vision}},
	volume = {6},
	issn = {2377-3766},
	doi = {10.1109/LRA.2020.3026970},
	abstract = {We present an approach for safe, and object-independent human-to-robot handovers using real time robotic vision, and manipulation. We aim for general applicability with a generic object detector, a fast grasp selection algorithm, and by using a single gripper-mounted RGB-D camera, hence not relying on external sensors. The robot is controlled via visual servoing towards the object of interest. Putting a high emphasis on safety, we use two perception modules: human body part segmentation, and hand/finger segmentation. Pixels that are deemed to belong to the human are filtered out from candidate grasp poses, hence ensuring that the robot safely picks the object without colliding with the human partner. The grasp selection, and perception modules run concurrently in real-time, which allows monitoring of the progress. In experiments with 13 objects, the robot was able to successfully take the object from the human in 81.9\% of the trials.},
	number = {1},
	journal = {IEEE Robotics and Automation Letters},
	author = {Rosenberger, P. and Cosgun, A. and Newbury, R. and Kwan, J. and Ortenzi, V. and Corke, P. and Grafinger, M.},
	year = {2020},
	note = {4},
	pages = {17--23},
	file = {IEEE Xplore Full Text PDF:/home/brendan/Zotero/storage/AC54VQFP/Rosenberger et al. - 2021 - Object-Independent Human-to-Robot Handovers Using .pdf:application/pdf;IEEE Xplore Abstract Record:/home/brendan/Zotero/storage/RMDY5GWN/9206048.html:text/html}
}

@inproceedings{ali_human_2013,
	title = {Human tracking by a mobile robot using {3D} features},
	doi = {10.1109/ROBIO.2013.6739841},
	abstract = {Detection and Tracking of human being is a very important problem in Computer Vision. Human robot interaction is a very essential need for service robots where robots are required to detect and track human beings in order to provide the required service. In this paper we present an improved novel approach for tracking a target person in crowded environment. We used multi-sensor data fusion approach by combining the data of stereo camera and laser rangefinder (LRF) to perform human tracking. The system gathers the features of human upper body, face and legs in the target person selection phase and then the robot will start following the target person. Camera is used for upper body and face detection while laser rangefinder is used for gathering legs data. Template matching and triangulation is done in order to get the dimensions of upper body and face. Target person tracking is done using Cam shift tracker. Thus our method presents a novel approach that uses all these techniques to track a target person in a crowded environment.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Robotics} and {Biomimetics} ({ROBIO})},
	author = {Ali, B. and Qureshi, A. H. and Iqbal, K. F. and Ayaz, Y. and Gilani, S. O. and Jamil, M. and Muhammad, N. and Ahmed, F. and Muhammad, M. S. and Kim, W. and Ra, M.},
	month = dec,
	year = {2013},
	note = {00009},
	keywords = {1bt\_include, 3D features, body detection, Cam shift, cam shift tracker, camera, Cameras, computer vision, Face, face detection, face recognition, Haar classifiers, human detection, human robot interaction, human tracking, human-robot interaction, image matching, laser rangefinder, Lasers, leg detection, Legged locomotion, LRF, mobile robot, mobile robots, multisensor data fusion, object detection, object tracking, robot vision, Robot vision systems, sensor fusion, service robot, service robots, Service robots, stereo camera, stereo image processing, target person tracking, target tracking, Target tracking, template matching, triangulation, upper body detection},
	pages = {2464--2469},
	file = {Full Text:/home/brendan/Zotero/storage/LCSDPF2H/Ali et al. - 2013 - Human tracking by a mobile robot using 3D features.pdf:application/pdf}
}

@article{almonfrey_flexible_2018,
	title = {A flexible human detection service suitable for {Intelligent} {Spaces} based on a multi-camera network},
	volume = {14},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044780931&doi=10.1177%2f1550147718763550&partnerID=40&md5=0126fd712e881063a40f7099e7d82e1a},
	doi = {10.1177/1550147718763550},
	abstract = {The research field of the Intelligent Spaces has experienced increasing attention in the last decade. As an instance of the ubiquitous computing paradigm, the general idea is to extract information from the ambient and use it to interact and provide services to the actors present in the environment. The sensory analysis is mandatory in this area and humans are usually the principal actors involved. In this sense, we propose a human detector to be used in an Intelligent Space based on a multi-camera network. Our human detector is implemented in the same paradigm of our Intelligent Space. As a contribution of the present work, the human detector is designed to be a service that is scalable, reliable and parallelizable. It is also a concern of our service to be flexible, less structured as possible, attending different Intelligent Space applications and services, as well as their requirements. As it can be found in different everyday environments, a multi-camera system is used to overcome some difficulties traditionally faced by existing human detection approaches. To validate our approach, we implement three different applications that are proof of concept of many day-to-day real tasks. Two of these applications involve human–robot interaction. With respect to time and detection performance requirements, our human detection service has proved to be suitable for interacting with the other services of our Intelligent Space, in order to successfully complete the tasks of each application. © 2018, © The Author(s) 2018.},
	number = {3},
	journal = {International Journal of Distributed Sensor Networks},
	author = {Almonfrey, D. and do Carmo, A.P. and de Queiroz, F.M. and Picoreti, R. and Vassallo, R.F. and Salles, E.O.T.},
	year = {2018},
	note = {00007},
	keywords = {1bt\_include},
	annote = {cited By 3},
	file = {Full Text:/home/brendan/Zotero/storage/637GXTYD/Almonfrey et al. - 2018 - A flexible human detection service suitable for In.pdf:application/pdf}
}

@inproceedings{bdiwi_handing-over_2013,
	title = {Handing-over model-free objects to human hand with the help of vision/force robot control},
	doi = {10.1109/SSD.2013.6564138},
	abstract = {Handing-over objects from robot to humans is essential step to perform different tasks especially those requiring physical interaction between the robot and the human, e.g. service robots can help elderly, blind and disabled people or human-robot teamwork could work together in factories. This work will propose a new robot system which combines visual servoing and force control in order to hand over model-free objects from undefined place to human hand. This work will present: 1. vision algorithms which help the robot system to detect and to segment the objects without any information about their model. 2. The possibility for visual tracking of human hand with the help of Kinect camera. 3. The importance of fusion vision and force control in order to ensure the safety during the human-robot interaction. 4. How the robot will deliver the objects to the human hand with the help of vision and force control. This work will be supported with experimental results.},
	booktitle = {10th {International} {Multi}-{Conferences} on {Systems}, {Signals} {Devices} 2013 ({SSD13})},
	author = {Bdiwi, M. and Suchý, J. and Winkler, A.},
	month = mar,
	year = {2013},
	note = {00005},
	keywords = {1bt\_include, 1vision, cameras, Cameras, dexterous manipulators, Feature extraction, Force, force control, Force control, force robot control, fusion vision, human hand, human-robot interaction, Human-robot interaction, human-robot physical interaction, human-robot teamwork, image processing, Image segmentation, Kinect camera, model-free objects, object segmentation, object tracking, Robot kinematics, robot system, robot vision, service robots, vision algorithms, visual servoing, visual tracking},
	pages = {1--6},
	file = {Full Text:/home/brendan/Zotero/storage/4T35CJGJ/Bdiwi et al. - 2013 - Handing-over model-free objects to human hand with.pdf:application/pdf}
}

@inproceedings{droeschel_towards_2011,
	title = {Towards joint attention for a domestic service robot - person awareness and gesture recognition using {Time}-of-{Flight} cameras},
	doi = {10.1109/ICRA.2011.5980067},
	abstract = {Joint attention between a human user and a robot is essential for effective human-robot interaction. In this work, we propose an approach to person awareness and to the perception of showing and pointing gestures for a domestic service robot. In contrast to previous work, we do not require the person to be at a predefined position, but instead actively approach and orient towards the communication partner. For perceiving showing and pointing gestures and for estimating the pointing direction a Time-of-Flight camera is used. Estimated pointing directions and shown objects are matched to objects in the robot's environment. Both the perception of showing and pointing gestures as well as the accurary of estimated pointing directions have been evaluated in a set of different experiments. The results show that both gestures are adequatly perceived by the robot. Furthermore, our system achieves a higher accuracy in estimating the pointing direction than is reported in the literature for a stereo-based system. In addition, the overall system has been successfully tested in two international RoboCup@Home competitions and the 2010 ICRA Mobile Manipulation Challenge.},
	booktitle = {2011 {IEEE} {International} {Conference} on {Robotics} and {Automation}},
	author = {Droeschel, D. and Stückler, J. and Holz, D. and Behnke, S.},
	month = may,
	year = {2011},
	note = {00050},
	keywords = {1bt\_include, Cameras, domestic service robot person awareness, Face, gesture recognition, human-robot interaction, Humans, Image segmentation, Robot vision systems, service robots, stereo-based system, time-of-flight cameras},
	pages = {1205--1210},
	file = {Full Text:/home/brendan/Zotero/storage/QK7SFIKG/Droeschel et al. - 2011 - Towards joint attention for a domestic service rob.pdf:application/pdf}
}

@article{gao_user_2020,
	title = {User {Modelling} {Using} {Multimodal} {Information} for {Personalised} {Dressing} {Assistance}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.2978207},
	abstract = {Assistive robots in home environments are steadily increasing in popularity. Due to significant variabilities in human behaviour, as well as physical characteristics and individual preferences, personalising assistance poses a challenging problem. In this paper, we focus on an assistive dressing task that involves physical contact with a human's upper body, in which the goal is to improve the comfort level of the individual. Two aspects are considered to be significant in improving a user's comfort level: having more natural postures and exerting less effort. However, a dressing path that fulfils these two criteria may not be found at one time. Therefore, we propose a user modelling method that combines vision and force data to enable the robot to search for an optimised dressing path for each user and improve as the human-robot interaction progresses. We compare the proposed method against two single-modality state-of-the-art user modelling methods designed for personalised assistive dressing by user studies (31 subjects). Experimental results show that the proposed method provides personalised assistance that results in more natural postures and less effort for human users.},
	journal = {IEEE Access},
	author = {Gao, Y. and Chang, H. J. and Demiris, Y.},
	year = {2020},
	note = {00000},
	keywords = {1bt\_include, Adaptation models, assisted living, assistive dressing, assistive robots, Data models, dressing path optimisation, feature extraction, Force, Hidden Markov models, home environments, human behaviour, human users, human-robot interaction, Human-robot interaction, human-robot interaction progresses, humans upper body, interactive systems, multimodal information, Multimodal user modelling, natural postures, optimisation, personalised assistive dressing, personalised dressing assistance, physical characteristics, physical contact, Robots, service robots, Task analysis, user modelling, user modelling method, users comfort level, vision and force fusion},
	pages = {45700--45714},
	file = {Full Text:/home/brendan/Zotero/storage/CLREHM5K/Gao et al. - 2020 - User Modelling Using Multimodal Information for Pe.pdf:application/pdf}
}

@article{giuliani_design_2010,
	title = {Design principles for safety in human-robot interaction},
	volume = {2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857625100&doi=10.1007%2fs12369-010-0052-0&partnerID=40&md5=1e0b3124361f249d526b1cf8d87be6c8},
	doi = {10.1007/s12369-010-0052-0},
	abstract = {The interaction of humans and robots has the potential to set new grounds in industrial applications as well as in service robotics because it combines the strengths of humans, such as flexibility and adaptability, and the strengths of robots, such as power and precision. However, for a successful interaction the safety of the human has to be guaranteed at all times. This goal can be reached by the use of specialised robot hardware but we argue that safety in human-robot interaction can also be done with regular industrial robots, if they are equipped with additional sensors to track the human's position and to analyse the human's verbal and non-verbal utterances, and if the software that is controlling the robot is especially designed towards safety in the interaction. For this reason, we propose three design principles for an increased safety in robot architectures and any other software component that controls a robot for human-robot interaction: robustness, fast reaction time, and context awareness. We present a robot architecture that is based on these principles and show approaches for speech processing, vision processing, and robot control that also follow these guidelines. © Springer Science \& Business Media BV 2010.},
	number = {3},
	journal = {International Journal of Social Robotics},
	author = {Giuliani, M. and Lenz, C. and Müller, T. and Rickert, M. and Knoll, A.},
	year = {2010},
	note = {00044},
	keywords = {1bt\_include},
	pages = {253--274},
	annote = {cited By 26},
	file = {Full Text:/home/brendan/Zotero/storage/ALXYD9FG/Giuliani et al. - 2010 - Design principles for safety in human-robot intera.pdf:application/pdf}
}

@article{kahlouche_human_2019,
	title = {Human {Activity} {Recognition} {Based} on {Ensemble} {Classifier} {Model}},
	volume = {682},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092719860&doi=10.1007%2f978-981-15-6403-1_78&partnerID=40&md5=20976e09fe5f4eb202a7e9a741ea1ebb},
	doi = {10.1007/978-981-15-6403-1_78},
	abstract = {In this work, we address the problem of Human Activity Recognition (HAR), applied to service robot. In addition, a real time HRI system able to understand some common interactive human activities is designed. To classify activities into different classes, a combination of three supervised machine-learning algorithms: Support Vector Machine (SVM), Decision Tree (DT) and Artificial Neural Network (ANN) is used, based on the idea that a set of classifiers improve machine learning results. Our approach uses as input a view invariant 3D data of skeleton joints, which are rich body movement information recorded from a single Microsoft Kinect camera to create specific dataset of six interactive activities. The algorithm was able to successfully classify and recognize activities being performed in front of the camera. The system framework is realized on the Robot Operating System (ROS), and real-life activity interaction between our service robot and the user was conducted to demonstrate the effectiveness of the developed HRI system. © 2021, Springer Nature Singapore Pte Ltd.},
	journal = {Lecture Notes in Electrical Engineering},
	author = {Kahlouche, S. and Belhocine, M.},
	year = {2019},
	note = {00000},
	keywords = {1bt\_include},
	pages = {1121--1132},
	annote = {cited By 0},
	file = {Full Text:/home/brendan/Zotero/storage/5ZIYS2ZS/Kahlouche and Belhocine - 2021 - Human Activity Recognition Based on Ensemble Class.pdf:application/pdf}
}

@article{kollmitz_deep_2019,
	title = {Deep {3D} perception of people and their mobility aids},
	volume = {114},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060737586&doi=10.1016%2fj.robot.2019.01.011&partnerID=40&md5=efd73acbfab5d00a1b6f1f30efc2817c},
	doi = {10.1016/j.robot.2019.01.011},
	abstract = {Robots operating in populated environments, such as hospitals, office environments or airports, encounter a large variety of people with some of them having an advanced need for cautious interaction because of their advanced age or motion impairments. To provide appropriate assistance and support robot helpers require the ability to recognize people and their potential requirements. In this article, we present a people detection framework that distinguishes people according to the mobility aids they use. Our framework uses a deep convolutional neural network for detecting people in image data. For human-aware robots it is necessary to know where people are in a 3D world reference frame instead of only locating them in a 2D image, therefore we add a 3D centroid regression output to the network to predict the Cartesian position of people. We further use a probabilistic class, position and velocity tracker to account for false detections and occlusions. Our framework comes in two variants: The depth only variant targets high privacy demands, while the RGB only framework provides improved detection performance for non-critical applications. Both variants do not require additional geometric information about the environment. We demonstrate our approach using a dedicated dataset acquired with the support of a mobile robotic platform. The dataset contains five classes: pedestrian, person in wheelchair, pedestrian pushing a person in a wheelchair, person using crutches and person using a walking frame. Our framework achieves an mAP of 0.87 for RGB and 0.79 for depth images at a detection distance threshold of 0.5m on our dataset, with a runtime of 53ms per image. The annotated dataset is publicly available and our framework is made open source as a ROS people detector. © 2019 Elsevier B.V.},
	journal = {Robotics and Autonomous Systems},
	author = {Kollmitz, M. and Eitel, A. and Vasquez, A. and Burgard, W.},
	year = {2019},
	note = {00009},
	keywords = {1bt\_include},
	pages = {29--40},
	annote = {cited By 6},
	file = {Kollmitz et al. - 2019 - Deep 3D perception of people and their mobility ai.pdf:/home/brendan/Zotero/storage/R68RV6BX/Kollmitz et al. - 2019 - Deep 3D perception of people and their mobility ai.pdf:application/pdf}
}

@inproceedings{luo_human_2010,
	title = {Human tracking and following using sound source localization for multisensor based mobile assistive companion robot},
	doi = {10.1109/IECON.2010.5675451},
	abstract = {We have developed a mobile assistive companion robot by combining a vision sensor and a laser range sensor to track and follow a target person. Although it works well in most cases, robot might lose target occasionally due to external factors such as bad view conditions or unconstructed environments. To solve this problem, we develop a speech system and sound source detection system to achieve sound source localization and speech interaction between users and robot. When robot gets lost during the tracking and following process, it will inform the user, and wait for a clapping sound from the user to re-localize user's location. The proposed method integrates human robot interaction based on speech system and sound source detection to retrieve target person's location when robot get lost, which is significantly different from other solutions which use motion model or Bayesian filters such as Kalman filters or particle filters to estimate user's location when robot is losing target. In this paper, we have demonstrated the success of the proposed method experimentally.},
	booktitle = {{IECON} 2010 - 36th {Annual} {Conference} on {IEEE} {Industrial} {Electronics} {Society}},
	author = {Luo, R. C. and Huang, C. H. and Lin, T. T.},
	month = nov,
	year = {2010},
	note = {00022},
	keywords = {1bt\_include, Feature extraction, filtering theory, human robot interaction, human-robot interaction, Humans, image sensors, laser range sensor, laser ranging, Microphones, mobile assistive companion robot, mobile robots, path planning, Robot kinematics, Robot sensing systems, robot vision, sensor fusion, sound source detection system, sound source localization, Speech, speech recognition, speech system, target tracking, vision sensor},
	pages = {1552--1557},
	annote = {cited By 10},
	file = {Full Text:/home/brendan/Zotero/storage/NY9FIUQB/Luo et al. - 2010 - Human tracking and following using sound source lo.pdf:application/pdf}
}

@article{masmoudi_expressive_2011,
	title = {Expressive robot to support elderly},
	volume = {29},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865460136&doi=10.3233%2f978-1-60750-814-4-557&partnerID=40&md5=510ee58a412a14c3e5bf659d38199518},
	doi = {10.3233/978-1-60750-814-4-557},
	abstract = {Objective. Handibio, in their partnership with Robopec, aims to create a robot equipped with emotional intelligence to support the elderly suffering from slight cognitive disabilities. The originality of this work lies in the development of hardware and software technologies while taking into account the benefit / risk / cost. Main content. The elderly affected by light cognitive disabilities wish to carry on living at home, but usually suffer from loneliness and isolation, both causes of depression. These people need services, particularly in the form of cognitive training and enhancement of socialization that the information and communication technologies (ICT) can provide. However, these ICT are not always accepted as they are not adapted well enough to the needs of these people. Our idea is that a robot serving a cognitive disable person, adapted to the difficulties of this same person and controlled by them, could contribute to their support at home providing with several types of services, from the most simple such as politely reminding this person to take his/her medication, to cognitive and psychological support. To make the relationship more human and intuitive, the robot should have emotion expressions and could make use of speech so that people could interact directly with the robot. This study presents an experimental protocol in order to assess the interaction between the robot and the elderly. It consists in carrying out both abilities test to recognize facial expression to define the difficulties caused by the specificities and variabilities of the environment, and ability evaluates of the robot's to transmit emotion to these persons. Results. Reeti is a "PCBot", that is to say the combination of a PC and an expressive robot. Its flexible "skin" and its 15-degree range give an infinity of facial expression possibilities to it. Its two mobile eyes equipped with two HD video cameras enable it to have an acute vision and a 3D perception of its surroundings. If connected to the Internet, it can send email alerts or simply display a monitoring interface with a remote control. Doing so, it shows a huge potential as an experimental platform for researchers in several fields such as vision, human-robot interaction, therapy assisted with robotics, behavior analysis, sociology and psychology in order to support the elderly and cognitive disable people. The robot has the ability to recognize people and their emotions through the automatic interpretation of their facial expressions. Moreover, it is able to reproduce some basic facial expressions in order to exchange with people. Conclusion. This work highlights the importance of human computer interaction in order to assist the elderly. Facial emotion recognition is necessary in the human-robot interaction, hence the need to overcome technical obstacles in the field of facial expression recognition, especially on elderly subjects. © 2011 The authors and IOS Press. All rights reserved.},
	journal = {Assistive Technology Research Series},
	author = {Masmoudi, R. and Bouchouicha, M. and Gorce, P.},
	year = {2011},
	note = {00000},
	keywords = {1bt\_include},
	pages = {557--564},
	annote = {cited By 1},
	file = {Masmoudi et al. - 2011 - Expressive robot to support elderly.pdf:/home/brendan/Zotero/storage/KYV8Z2NJ/Masmoudi et al. - 2011 - Expressive robot to support elderly.pdf:application/pdf}
}

@inproceedings{monajjemi_hri_2013,
	title = {{HRI} in the sky: {Creating} and commanding teams of {UAVs} with a vision-mediated gestural interface},
	doi = {10.1109/IROS.2013.6696415},
	abstract = {Extending our previous work in real-time vision-based Human Robot Interaction (HRI) with multi-robot systems, we present the first example of creating, modifying and commanding teams of UAVs by an uninstrumented human. To create a team the user focuses attention on an individual robot by simply looking at it, then adds or removes it from the current team with a motion-based hand gesture. Another gesture commands the entire team to begin task execution. Robots communicate among themselves by wireless network to ensure that no more than one robot is focused, and so that the whole team agrees that it has been commanded. Since robots can be added and removed from the team, the system is robust to incorrect additions. A series of trials with two and three very low-cost UAVs and off-board processing demonstrates the practicality of our approach.},
	booktitle = {2013 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	author = {Monajjemi, V. M. and Wawerla, J. and Vaughan, R. and Mori, G.},
	month = nov,
	year = {2013},
	note = {00104},
	keywords = {1bt\_include, 1vision, Aircraft, autonomous aerial vehicles, Cameras, Face, gesture recognition, HRI, human robot interaction, human-robot interaction, motion-based hand gesture, multi-robot systems, multirobot systems, Optical imaging, Robot kinematics, Robot sensing systems, UAV, uninstrumented human, vision-mediated gestural interface, wireless network},
	pages = {617--623},
	file = {Full Text:/home/brendan/Zotero/storage/AWHAEIE3/Monajjemi et al. - 2013 - HRI in the sky Creating and commanding teams of U.pdf:application/pdf}
}

@article{tan_safety_2010,
	title = {Safety strategy for human-robot collaboration: {Design} and development in cellular manufacturing},
	volume = {24},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951841739&doi=10.1163%2f016918610X493633&partnerID=40&md5=dd2a40fe6091c2799e897aabe09a04e7},
	doi = {10.1163/016918610X493633},
	abstract = {Our research aims to design and develop a safety strategy for a human-robot collaboration system. Although robotic assistance in a cellular manufacturing system is promising, safety is the uppermost consideration before it can be materialized. Five main safety designs are developed in this work. (i) Safe working areas for humans and robots. (ii) To control the behavior of the robot based on the collaboration requirements, light curtains defined safe collaborative working zones. (iii) Additionally, the robot system was developed using safe mechanical design and Dual Check Safety control strategies in terms of robot speed and travel area to minimize collaboration risks. (iv) A vision system using IP cameras was developed to monitor operator safety conditions by measuring the body posture and position of the operator. (v) The operation control system coordinated the collaborative flow between the operator and robot system. Apart from these developments, risk assessments were conducted to evaluate the safety design of the system, and a mental safety study was performed to investigate robot motion speed and working distance on the operator's physiological effects. Our findings demonstrate the feasibility of the prototype system to safely perform assembly operations. © 2010 Koninklijke Brill NV, Leiden and The Robotics Society of Japan.},
	number = {5-6},
	journal = {Advanced Robotics},
	author = {Tan, J.T.C. and Duan, F. and Kato, R. and Arai, T.},
	year = {2010},
	note = {00032},
	keywords = {1bt\_include},
	pages = {839--860},
	annote = {cited By 20},
	file = {Full Text:/home/brendan/Zotero/storage/JTZJUUSA/Tan et al. - 2010 - Safety strategy for human-robot collaboration Des.pdf:application/pdf}
}

@article{scheggi_human-robot_2014,
	title = {Human-robot formation control via visual and vibrotactile haptic feedback},
	volume = {7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919883510&doi=10.1109%2fTOH.2014.2332173&partnerID=40&md5=9208b435af3aa46bf7824d51cdbf609a},
	doi = {10.1109/TOH.2014.2332173},
	abstract = {In this paper we present a new visuo-haptic interaction mechanism for human-robot formation control. The formation setup consists of a human leader and multiple follower robots. The mobile robots are equipped only with RGB-D cameras, and they should maintain a desired distance and orientation to the leader at all times. Mechanical limitations common to all the robots limit the possible trajectories that the human can take. In this regard, vibrotactile feedback provided by a haptic bracelet guides the human along trajectories that are feasible for the team by warning her/him when the formation constraints are being violated. Psychophysical tests on the bracelet together with real-world experiments conducted with a team of Pioneer robots show the effectiveness of the proposed visuo-haptic paradigm for the coordination of mixed human-robot teams. © 2014 IEEE.},
	number = {4},
	journal = {IEEE Transactions on Haptics},
	author = {Scheggi, S. and Morbidi, F. and Prattichizzo, D.},
	year = {2014},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {499--511},
	annote = {cited By 28},
	file = {Full Text:/Users/tid010/Zotero/storage/S95CQUWZ/Scheggi et al. - 2014 - Human-robot formation control via visual and vibro.pdf:application/pdf},
}

@inproceedings{scimmi_experimental_2019,
	title = {Experimental {Real}-{Time} {Setup} for {Vision} {Driven} {Hand}-{Over} with a {Collaborative} {Robot}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083104719&doi=10.1109%2fICCAD46983.2019.9037961&partnerID=40&md5=747d34c4381c2879d8b8e400e0267d92},
	doi = {10.1109/ICCAD46983.2019.9037961},
	abstract = {Collaborative robotics aims to make possible a close collaboration between human operators and robots in the industry scenario. To achieve this goal, the robot must be able to adapt its behavior to the movements of human co-worker. This work presents a control strategy that permits to drive a UR3 collaborative robot in order to perform a hand-over task with a human operator. The movements of the operator are acquired by a 3D vision system made of multiple Microsoft Kinect sensors, so to prevent possible occlusions related to the presence of the robot or dynamic obstacles in the field of view of the sensors. Thus, the human skeletons extracted from depth sensors are handled by an algorithm that generates an optimized skeleton. A path-planning algorithm uses the skeleton information to calculates the robot joints velocities to accomplish the task. Experimental tests have been conducted to verify the effectiveness of the hand-over strategy here proposed. The hardware setup prepared for the experimental phase of the work and the results obtained from the tests are presented and discussed. © 2019 IEEE.},
	booktitle = {2019 {International} {Conference} on {Control}, {Automation} and {Diagnosis}, {ICCAD} 2019 - {Proceedings}},
	author = {Scimmi, L.S. and Melchiorre, M. and Mauro, S. and Pastorelli, S.},
	year = {2019},
	keywords = {1scopus},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/FB6ZAVEC/Scimmi et al. - 2019 - Experimental Real-Time Setup for Vision Driven Han.pdf:application/pdf},
}

@inproceedings{pasinetti_development_2018,
	title = {Development and {Characterization} of a {Safety} {System} for {Robotic} {Cells} {Based} on {Multiple} {Time} of {Flight} ({TOF}) {Cameras} and {Point} {Cloud} {Analysis}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052518474&doi=10.1109%2fMETROI4.2018.8439037&partnerID=40&md5=8d60a3b270d13153e325bf69e33b8ea0},
	doi = {10.1109/METROI4.2018.8439037},
	abstract = {In this paper, a vision system for safety applications in human-robot collaboration is presented. The system is based on two Time-Of-Flight (TOF) cameras for 3D acquisition. The point clouds are registered in a common reference system, and human and robot recognition are then implemented. Human recognition is performed using a customized version of the Histogram of Oriented Gradient (HOG) algorithm. Robot recognition is achieved using a procedure based on the Kanade-Lucas-Tomasi (KLT) algorithm. Two safety strategies have been developed. The first one is based on the definition of suitable comfort zones of both the operator and the robot; the second implements virtual barriers between the operator and the robot. The vision system has been characterized in terms of (i) human and robot recognition performance, (ii) correctness of the detection of safety situations and (iii) evaluation of the time delays in the detection. The results show that the human operator is robustly recognized provided that he moves frontally with respect to the TOF cameras and the robot is always recognized. The safety situations are always identified correctly with an average time delay of 0.86 0.63 s (k=1). © 2018 IEEE.},
	booktitle = {2018 {Workshop} on {Metrology} for {Industry} 4.0 and {IoT}, {MetroInd} 4.0 and {IoT} 2018 - {Proceedings}},
	author = {Pasinetti, S. and Nuzzi, C. and Lancini, M. and Sansoni, G. and Docchio, F. and Fornaser, A.},
	year = {2018},
	keywords = {1scopus},
	pages = {34--39},
	annote = {cited By 2},
	file = {Full Text:/Users/tid010/Zotero/storage/YGH3J2WB/Pasinetti et al. - 2018 - Development and Characterization of a Safety Syste.pdf:application/pdf},
}

@inproceedings{fang_vehicle-mounted_2019,
	title = {Vehicle-mounted with tracked robotic system based on the kinect},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081590957&doi=10.1109%2fWCMEIM48965.2019.00110&partnerID=40&md5=bb98d58389044f20135d58f7478c212f},
	doi = {10.1109/WCMEIM48965.2019.00110},
	abstract = {In the research of robot controlling, existing methods are mainly traditional human-computer interactive control methods such as mobile APP or handle. These traditional methods, however, rely on external devices to control the robot, which is inefficient to perform more complex functions. Although the Kinect has more benefits than other sensors in motion recognition, there is still no proper research on motion and gesture recognition of a tracked vehicle robot. Therefore, this paper proposes a research method of vehicle robot system based on Kinect dynamic gesture recognition of depth information. By using the Kinect sensor, we can recognize and obtain information such as human bones and gestures, and finally, realize accurate control of vehiclemounted robots with multiple gestures. ©2019 IEEE.},
	booktitle = {Proceedings - 2019 2nd {World} {Conference} on {Mechanical} {Engineering} and {Intelligent} {Manufacturing}, {WCMEIM} 2019},
	author = {Fang, J. and Qiao, M. and Pei, Y.},
	year = {2019},
	keywords = {1scopus},
	pages = {521--524},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/BMZL7A9G/Fang et al. - 2019 - Vehicle-mounted with tracked robotic system based .pdf:application/pdf},
}

@inproceedings{canal_gesture_2015,
	title = {Gesture based human multi-robot interaction},
	volume = {2015-September},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950981593&doi=10.1109%2fIJCNN.2015.7280540&partnerID=40&md5=80a33faff31d3190faff415c1fa1cd87},
	doi = {10.1109/IJCNN.2015.7280540},
	abstract = {The emergence of robot applications for non-technical users implies designing new ways of interaction between robotic platforms and users. The main goal of this work is the development of a gestural interface to interact with robots in a similar way as humans do, allowing the user to provide information of the task with non-verbal communication. The gesture recognition application has been implemented using the Microsoft's Kinect™ v2 sensor. Hence, a real-time algorithm based on skeletal features is described to deal with both, static gestures and dynamic ones, being the latter recognized using a weighted Dynamic Time Warping method. The gesture recognition application has been implemented in a multi-robot case. A NAO humanoid robot is in charge of interacting with the users and respond to the visual signals they produce. Moreover, a wheeled Wifibot robot carries both the sensor and the NAO robot, easing navigation when necessary. A broad set of user tests have been carried out demonstrating that the system is, indeed, a natural approach to human robot interaction, with a fast response and easy to use, showing high gesture recognition rates. © 2015 IEEE.},
	booktitle = {Proceedings of the {International} {Joint} {Conference} on {Neural} {Networks}},
	author = {Canal, G. and Angulo, C. and Escalera, S.},
	year = {2015},
	keywords = {1scopus},
	annote = {cited By 4},
	file = {Full Text:/Users/tid010/Zotero/storage/T4TUGGLI/Canal et al. - 2015 - Gesture based human multi-robot interaction.pdf:application/pdf},
}

@article{burger_two-handed_2012,
	title = {Two-handed gesture recognition and fusion with speech to command a robot},
	volume = {32},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862114714&doi=10.1007%2fs10514-011-9263-y&partnerID=40&md5=af76768e476b2086073af14be163518c},
	doi = {10.1007/s10514-011-9263-y},
	abstract = {Assistance is currently a pivotal research area in robotics, with huge societal potential. Since assistant robots directly interact with people, finding natural and easy-to-use user interfaces is of fundamental importance. This paper describes a flexible multimodal interface based on speech and gesture modalities in order to control our mobile robot named Jido. The vision system uses a stereo head mounted on a pan-tilt unit and a bank of collaborative particle filters devoted to the upper human body extremities to track and recognize pointing/symbolic mono but also bi-manual gestures. Such framework constitutes our first contribution, as it is shown, to give proper handling of natural artifacts (self-occlusion, camera out of view field, hand deformation) when performing 3D gestures using one or the other hand even both. A speech recognition and understanding system based on the Julius engine is also developed and embedded in order to process deictic and anaphoric utterances. The second contribution deals with a probabilistic and multi-hypothesis interpreter framework to fuse results from speech and gesture components. Such interpreter is shown to improve the classification rates of multimodal commands compared to using either modality alone. Finally, we report on successful live experiments in human-centered settings. Results are reported in the context of an interactive manipulation task, where users specify local motion commands to Jido and perform safe object exchanges. © 2011 Springer-Verlag.},
	number = {2},
	journal = {Autonomous Robots},
	author = {Burger, B. and Ferrané, I. and Lerasle, F. and Infantes, G.},
	year = {2012},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {129--147},
	annote = {cited By 54},
	file = {Full Text:/Users/tid010/Zotero/storage/P3D5UXG6/Burger et al. - 2012 - Two-handed gesture recognition and fusion with spe.pdf:application/pdf},
}

@inproceedings{fujii_gesture_2014,
	title = {Gesture recognition system for {Human}-{Robot} {Interaction} and its application to robotic service task},
	volume = {2209},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938237053&partnerID=40&md5=65a01757df8b0aa92518a19dc3e25b06},
	abstract = {This paper presents a gesture recognition system for Human-Robot Interaction. And it has been employed to the development of a service robot that can be operated by the human gesture given as the user's command. Human motion is detected by the Kinect sensor on the robot and recognized as one of the predefined commands by using an algorithm based on Hidden Markov Model (HMM). Its recognition rates about the predefined gestures were verified through several experiments and compared to the region-based recognition method of the previous research. Finally, the developed system has been applied to Human-Robot Interaction for service tasks in office environment.},
	booktitle = {Lecture {Notes} in {Engineering} and {Computer} {Science}},
	author = {Fujii, T. and Lee, J.H. and Okamoto, S.},
	year = {2014},
	note = {Issue: January},
	keywords = {1scopus},
	pages = {63--68},
	annote = {cited By 16},
	file = {Fujii et al. - 2014 - Gesture recognition system for Human-Robot Interac.pdf:/Users/tid010/Zotero/storage/6TDCLXQX/Fujii et al. - 2014 - Gesture recognition system for Human-Robot Interac.pdf:application/pdf},
}

@inproceedings{augello_towards_2020,
	title = {Towards an intelligent system for supporting gesture acquisition and reproduction in humanoid robots},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091661890&doi=10.18293%2fDMSVIVA2020-017&partnerID=40&md5=951b4f6eba6af53383f183d3e01bd3f6},
	doi = {10.18293/DMSVIVA2020-017},
	abstract = {In this paper, an intelligent system for supporting gesture acquisition and reproduction in humanoid robots, which is based on the well-known Microsoft Kinect framework, is introduced and discussed in this paper. The idea that has inspired the paper is represented by endowing an humanoid robot with the capability to mimic the motion of a human user in real time. As a further extension, the latter amenity may serve as a basis for further gesture based human-robot interactions. © 2020 DMSVIVA 2020 - Proceedings of the 26th International DMS Conference on Visualization and Visual Languages. All rights reserved.},
	booktitle = {{DMSVIVA} 2020 - {Proceedings} of the 26th {International} {DMS} {Conference} on {Visualization} and {Visual} {Languages}},
	author = {Augello, A. and Ciulla, A. and Cuzzocrea, A. and Gaglio, S. and Pilato, G. and Vella, F.},
	year = {2020},
	keywords = {1scopus},
	pages = {82--86},
	annote = {cited By 0},
	file = {Augello et al. - 2020 - Towards an intelligent system for supporting gestu.pdf:/Users/tid010/Zotero/storage/MAPQKV6U/Augello et al. - 2020 - Towards an intelligent system for supporting gestu.pdf:application/pdf},
}

@article{werner_evaluation_2013,
	title = {Evaluation of the acceptance of a social assistive robot for physical training support together with older users and domain experts},
	volume = {33},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888988664&doi=10.3233%2f978-1-61499-304-9-137&partnerID=40&md5=99e0d9f8f8dda44d7d7be13ef315494d},
	doi = {10.3233/978-1-61499-304-9-137},
	abstract = {According to recent studies a strength of socially assistive robots (SARs) is the ability to motivate users to perform tasks in a multimodal manner. Within this paper the evaluation of a SAR based prototype for support of physical therapy of older users at home is described. By performing a user study with a training system consisting of a social assistive robot (NAO) in combination with the Microsoft Kinect, the acceptance of human robot interaction (HRI) within the field of physical training as well as the impact on user motivation was evaluated. Results regarding motivational abilities of a SAR and the user acceptance towards the system are given. © 2013 The authors and IOS Press. All rights reserved.},
	journal = {Assistive Technology Research Series},
	author = {Werner, F. and Krainer, D. and Oberzaucher, J. and Werner, K.},
	year = {2013},
	keywords = {1scopus},
	pages = {137--142},
	annote = {cited By 4},
	file = {Werner et al. - 2013 - Evaluation of the acceptance of a social assistive.pdf:/Users/tid010/Zotero/storage/USXHINHJ/Werner et al. - 2013 - Evaluation of the acceptance of a social assistive.pdf:application/pdf},
}

@inproceedings{gui_teaching_2018,
	title = {Teaching {Robots} to {Predict} {Human} {Motion}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055437273&doi=10.1109%2fIROS.2018.8594452&partnerID=40&md5=7c89b51e3192bcfab42365bd165471c3},
	doi = {10.1109/IROS.2018.8594452},
	abstract = {Teaching a robot to predict and mimic how a human moves or acts in the near future by observing a series of historical human movements is a crucial first step in human-robot interaction and collaboration. In this paper, we instrument a robot with such a prediction ability by leveraging recent deep learning and computer vision techniques. First, our system takes images from the robot camera as input to produce the corresponding human skeleton based on real-time human pose estimation obtained with the OpenPose library. Then, conditioning on this historical sequence, the robot forecasts plausible motion through a motion predictor, generating a corresponding demonstration. Because of a lack of high-level fidelity validation, existing forecasting algorithms suffer from error accumulation and inaccurate prediction. Inspired by generative adversarial networks (GANs), we introduce a global discriminator that examines whether the predicted sequence is smooth and realistic. Our resulting motion GAN model achieves superior prediction performance to state-of-the-art approaches when evaluated on the standard H3.6M dataset. Based on this motion GAN model, the robot demonstrates its ability to replay the predicted motion in a human-like manner when interacting with a person. © 2018 IEEE.},
	booktitle = {{IEEE} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	author = {Gui, L.-Y. and Zhang, K. and Wang, Y.-X. and Liang, X. and Moura, J.M.F. and Veloso, M.},
	year = {2018},
	keywords = {1scopus},
	pages = {562--567},
	annote = {cited By 18},
	file = {Full Text:/Users/tid010/Zotero/storage/FK7S5JX5/Gui et al. - 2018 - Teaching Robots to Predict Human Motion.pdf:application/pdf},
}

@inproceedings{bellarbi_social_2017,
	title = {A social planning and navigation for tour-guide robot in human environment},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011292183&doi=10.1109%2fICMIC.2016.7804186&partnerID=40&md5=3b94c603b7f5ce3d0edb59d1fb858d63},
	doi = {10.1109/ICMIC.2016.7804186},
	abstract = {The biggest challenges in modern robotics is service robots, which are able to execute many tasks for humans in their presence. This perspective naturally causes the problem of social navigation of mobile robots as well as human-robot interaction. In this article, we present the implementation of a navigation method on a mobile robot in indoor environment; we detail the algorithm and the implementation of human-robot interaction social rules. The principle is to define a goal for the robot, which plans a path that drives it to its goal, choosing the shortest, the smoothest, and the safety way, avoiding socially the dynamic obstacles. For this purpose, the robot uses a laser sensor for building environments maps, localization, and detection of new obstacles, and an RGB-D camera (Kinect sensor) for social avoidance. © 2016 University of MEDEA, Algeria.},
	booktitle = {Proceedings of 2016 8th {International} {Conference} on {Modelling}, {Identification} and {Control}, {ICMIC} 2016},
	author = {Bellarbi, A. and Kahlouche, S. and Achour, N. and Ouadah, N.},
	year = {2017},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {622--627},
	annote = {cited By 5},
	file = {Full Text:/Users/tid010/Zotero/storage/5FKJGSTG/Bellarbi et al. - 2017 - A social planning and navigation for tour-guide ro.pdf:application/pdf},
}

@inproceedings{wang_wheeled_2013,
	title = {Wheeled robot control based on gesture recognition using the {Kinect} sensor},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898810745&doi=10.1109%2fROBIO.2013.6739488&partnerID=40&md5=bc8cb06191f70f163495ee4273e75eca},
	doi = {10.1109/ROBIO.2013.6739488},
	abstract = {Human Machine Interaction (HMI) plays a very important role in intelligent service robot research. Traditional HMI methods such as keyboard and mouse cannot satisfy the high demands in some environments. To solve this problem, many researchers pay attention to vision-based gesture recognition research recently. This paper proposes a simple method to control the movement of a robot based on Kinect which provides skeleton data with low computation, acceptable performance and financial cost. The method can recognize eleven defined gestures by using the coordinates of joints, which are obtained from the skeleton model provided by the Kinect SDK. A Khepera III robot is used as a prototype control object, to verify the effectiveness of the proposed method. The experimental results show that the success rate of gesture recognition is over 96\%. The proposed method is robust to work in real-time. © 2013 IEEE.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Robotics} and {Biomimetics}, {ROBIO} 2013},
	author = {Wang, Y. and Song, G. and Qiao, G. and Zhang, Y. and Zhang, J. and Wang, W.},
	year = {2013},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {378--383},
	annote = {cited By 8},
	file = {Full Text:/Users/tid010/Zotero/storage/DYXPUTBX/Wang et al. - 2013 - Wheeled robot control based on gesture recognition.pdf:application/pdf},
}

@inproceedings{mao_medical_2018,
	title = {The medical service robot interaction based on kinect},
	volume = {2018-February},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050651983&doi=10.1109%2fITCOSP.2017.8303077&partnerID=40&md5=b6978a928c4c39be985546e118244719},
	doi = {10.1109/ITCOSP.2017.8303077},
	abstract = {In view of human body motion recognition technology, a gesture recognition method based on LabVIEW is proposed, which is applied to the control of medical service robot. The 3D somatosensory camera of Kinect is used to track the human skeleton points, and then capture human actions in real time on the LabVIEW platform, identify different actions of the body. Finally, the motion commands are sent to the robot through the Bluetooth communication to complete the robot forward, turning and wheelchair mode conversion, etc. After testing, the method of hand gesture recognition based on the LabVIEW, can be very good for tracking the human body and identifying the body's actions. The human computer interaction of the medical service robot is realized, which provides convenience for the assistant doctor to complete the rehabilitation and nursing of the patients. © 2017 IEEE.},
	booktitle = {Proceedings of the 2017 {IEEE} {International} {Conference} on {Intelligent} {Techniques} in {Control}, {Optimization} and {Signal} {Processing}, {INCOS} 2017},
	author = {Mao, L. and Zhu, P.},
	year = {2018},
	keywords = {1include\_search1, 1scopus, 1search1},
	pages = {1--7},
	annote = {cited By 2},
	file = {Full Text:/Users/tid010/Zotero/storage/QYK77FFY/Mao and Zhu - 2018 - The medical service robot interaction based on kin.pdf:application/pdf},
}

@article{chan_collision-free_2020,
	title = {Collision-free path planning based on new navigation function for an industrial robotic manipulator in human-robot coexistence environments},
	volume = {43},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087379188&doi=10.1080%2f02533839.2020.1771210&partnerID=40&md5=5b083679ae3053b54fa4b460635a0e0e},
	doi = {10.1080/02533839.2020.1771210},
	abstract = {This paper presents a method for real-time collision-free path planning that uses a new navigation function for environments in which humans and robots cooperate or coexist, which guarantees the safety of a human operator who works with a collaborative robot. This method follows 10 skeletal joints in a human operator, which are detected using an RGB-D sensor, in order to detect humans. A new navigation function ensures collision-free planning for a robotic manipulator that avoids local minima and generates a shorter path. Simulation results show the effectiveness of the proposed method in comparison to two existing methods. The applicability and practicability of the proposed method are demonstrated by experiments using a 6-DoF industrial manipulator HIWIN RA605 with an RGB-D vision sensor (Kinect V1), a real-time operation system extension (RTX64) and an EtherCAT network protocol. © 2020 The Chinese Institute of Engineers.},
	number = {6},
	journal = {Journal of the Chinese Institute of Engineers, Transactions of the Chinese Institute of Engineers,Series A},
	author = {Chan, C.-C. and Tsai, C.-C.},
	year = {2020},
	keywords = {1scopus},
	pages = {508--518},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/QJVSAAZ7/Chan and Tsai - 2020 - Collision-free path planning based on new navigati.pdf:application/pdf},
}

@inproceedings{talebpour-board_2016,
	title = {On-board human-aware navigation for indoor resource-constrained robots: {A} case-study with the ranger},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963725673&doi=10.1109%2fSII.2015.7404955&partnerID=40&md5=23a1ac525d90e4b471abf66c2989ec2b},
	doi = {10.1109/SII.2015.7404955},
	abstract = {Introducing simple robotic platforms into domestic environments is faced with the challenge of social acceptability. Therefore human-aware navigation is a must for robots operating in environments shared with human users. In this work, we focus on the human-aware navigation problem in a structured environment for a robot with limited sensing and constrained maneuvering called Ranger. The Ranger is a simple domestic robotic platform designed for interacting with children. The system combines person detection and tracking - which is the result of fusing laser-scan and depth-image based detectors provided by an RGB-D camera -, basic autonomous navigation and the concept of personal space. We rely only on the on-board sensors for mapping, localization, human tracking, and navigation. Systematic experiments are carried out with a real robot in the presence of a human in order to compare our human-aware navigation with a non human-aware simple approach. The results show that human-aware navigation is able to achieve trajectories which are respecting the personal spaces of the human and are thus more acceptable for the users. © 2015 IEEE.},
	booktitle = {2015 {IEEE}/{SICE} {International} {Symposium} on {System} {Integration}, {SII} 2015},
	author = {Talebpour, Z. and Navarro, I. and Martinoli, A.},
	year = {2016},
	keywords = {1scopus},
	pages = {63--68},
	annote = {cited By 4},
	file = {Submitted Version:/Users/tid010/Zotero/storage/GECS5S5S/Talebpour et al. - 2016 - On-board human-aware navigation for indoor resourc.pdf:application/pdf},
}

@inproceedings{paulo_vision-based_2012,
	title = {Vision-based hand segmentation techniques for human-robot interaction for real-time applications},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856694609&partnerID=40&md5=a6a3bbed4537f2d964f66ea6f3d5bf9c},
	abstract = {One of the most important tasks in hand recognition applications for human-robot interaction is hand segmentation. This work presents a method that uses a Microsoft Kinect camera, for hand localization and segmentation. One important aspect for robot control besides hand localization is hand orientation, which is used in this work to control robot heading direction (left or right) and linear velocity. The system first calculates hand position, and then a kalman filter is used to estimate displacement and linear velocity in a smoother way. Experimental results show that the system is easy to use, and can be applied on several different human-computer interface applications. © 2012 Taylor \& Francis Group.},
	booktitle = {Computational {Vision} and {Medical} {Image} {Processing}, {Proceedings} of {VipIMAGE} 2011 - 3rd {ECCOMAS} {Thematic} {Conference} on {Computational} {Vision} and {Medical} {Image} {Processing}},
	author = {Paulo, T. and Fernando, R. and Gil, L.},
	year = {2012},
	keywords = {1scopus},
	pages = {31--35},
	annote = {cited By 2},
	file = {Paulo et al. - 2012 - Vision-based hand segmentation techniques for huma.pdf:/Users/tid010/Zotero/storage/JFA5AJAT/Paulo et al. - 2012 - Vision-based hand segmentation techniques for huma.pdf:application/pdf},
}

@inproceedings{long_kinect-based_2018,
	title = {Kinect-based {Human} {Body} {Tracking} {System} {Control} of {Medical} {Care} {Service} {Robot}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060824729&doi=10.1109%2fWRC-SARA.2018.8584246&partnerID=40&md5=853b8bdd77f376ffc8f83c072f7a2e64},
	doi = {10.1109/WRC-SARA.2018.8584246},
	abstract = {In order to realize the human body following of medical care service robot, Kinect v2 is used to detect the target person in real time, and the improved Gaussian filter algorithm is used to denoise the depth image. In this paper, Kalman filtering algorithm is used to predict human movement information, thus ensuring safer and more reliable real-Time tracking. When the target character is blocked, the robot can be stopped safely and effectively. This system uses the gesture of the target person to start or stop following, and it can realize good human-computer interaction. Finally, a series of human body following test experiments are designed to verify the effectiveness of the proposed method and the feasibility of the robot system to achieve human body following in hospital environment. © 2018 IEEE.},
	booktitle = {2018 {WRC} {Symposium} on {Advanced} {Robotics} and {Automation}, {WRC} {SARA} 2018 - {Proceeding}},
	author = {Long, Y. and Xu, Y. and Xiao, Z. and Shen, Z.},
	year = {2018},
	keywords = {1scopus},
	pages = {65--69},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/BKA7RR6J/Long et al. - 2018 - Kinect-based Human Body Tracking System Control of.pdf:application/pdf},
}

@inproceedings{kahily_real-time_2016,
	title = {Real-time human detection and tracking from a mobile armed robot using {RGB}-{D} sensor},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994130871&doi=10.1109%2fSTARTUP.2016.7583953&partnerID=40&md5=a3d15c328ca25a9dc3c9fb960d406200},
	doi = {10.1109/STARTUP.2016.7583953},
	abstract = {This paper presents a prototype of a military robot which implements an application of real time detection and tracking of human beings. A depth sensing camera system by Microsoft Kinect is utilized on a physical robot for human detection and aiming in both static and dynamic modes of operation. Human detection in the static mode is obtained by using APIs of Open Natural Interaction (OpenNI) framework and the aiming routine is carried out using Regression Analysis between image pixels and the gun control motors. In the dynamic mode, human detection is done by implementing several algorithms such as voxelization of point clouds, RANSAC, and Histogram of Oriented Gradients in Point Cloud Library and the aiming routine is carried out using inverse kinematics. © 2016 IEEE.},
	booktitle = {{IEEE} {WCTFTR} 2016 - {Proceedings} of 2016 {World} {Conference} on {Futuristic} {Trends} in {Research} and {Innovation} for {Social} {Welfare}},
	author = {Kahily, H.M. and Sudheer, A.P.},
	year = {2016},
	keywords = {1scopus},
	annote = {cited By 3},
	file = {Full Text:/Users/tid010/Zotero/storage/7654RK4L/Kahily and Sudheer - 2016 - Real-time human detection and tracking from a mobi.pdf:application/pdf},
}

@inproceedings{fareed_gesture_2015,
	title = {Gesture based wireless single-armed robot in cartesian {3D} space using kinect},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946944543&doi=10.1109%2fCSNT.2015.86&partnerID=40&md5=1d4a6fa62e19128da93c48f178076843},
	doi = {10.1109/CSNT.2015.86},
	abstract = {Human Machine Interaction (HMI) has always played an important role in everybody's life motivating research in the area of intelligent service robots. Conventional methods such as remote controllers or wearables cannot cater the high demands in some scenarios. To overcome this situation, the challenge is to develop vision-based gesture recognition techniques. This paper describes our work of controlling an Arduino based wheeled, one armed robot, used as a prototype, controlled through various gestures of the arms and legs. For gesture recognition, we make use of skeletal tracing ability of Kinect - a product of Microsoft. Bluetooth is used to make the controls wireless. Since it is not line of sight operation, the robot also captures the environment video and transmits it over radio frequency in real-time and displayed on the screen. On the user end, according to the received video, the operator guides the robot and uses the arm to pick and place objects with the help of predetermined gestures. © 2015 IEEE.},
	booktitle = {Proceedings - 2015 5th {International} {Conference} on {Communication} {Systems} and {Network} {Technologies}, {CSNT} 2015},
	author = {Fareed, M.M.F.M. and Akram, Q.I. and Anees, S.B.A. and Fakih, A.H.},
	year = {2015},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {1210--1215},
	annote = {cited By 3},
	file = {Full Text:/Users/tid010/Zotero/storage/SES26BT2/Fareed et al. - 2015 - Gesture based wireless single-armed robot in carte.pdf:application/pdf},
}

@inproceedings{bolano_towards_2018,
	title = {Towards a {Vision}-{Based} {Concept} for {Gesture} {Control} of a {Robot} {Providing} {Visual} {Feedback}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064140315&doi=10.1109%2fROBIO.2018.8665314&partnerID=40&md5=8ee1080ec496af95c7e28d64d3e50efb},
	doi = {10.1109/ROBIO.2018.8665314},
	abstract = {Human-Robot Interaction (HRI) plays an important and growing role, both in industrial applications and in game development. Over recent years, robots can be controlled by gestures via special devices, but these methods are not intuitive and require usually a learning phase. This paper proposes an intuitive method for controlling a robot end-effector using human gestures. Vision based techniques were used to track the position of the user's hand, which is directly translated in control signals. The use of a 3D camera sensor allows to easily control the robot tool position in all dimensions. Our approach includes a Graphical User Interface (GUI), to ease the control through interactive, visual feedback. This interface, including 3D markers, text messages and the visualization of the user's point cloud and the robot model, enables a control mechanism which does not require a teaching phase. Our approach was tested and evaluated using realistic experiments to prove that our approach works reliably and is extremely intuitive. © 2018 IEEE.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Robotics} and {Biomimetics}, {ROBIO} 2018},
	author = {Bolano, G. and Tanev, A. and Steffen, L. and Roennau, A. and Dillmann, R.},
	year = {2018},
	keywords = {1scopus},
	pages = {386--392},
	annote = {cited By 2},
	file = {Full Text:/Users/tid010/Zotero/storage/PHIETDTT/Bolano et al. - 2018 - Towards a Vision-Based Concept for Gesture Control.pdf:application/pdf},
}

@article{pereira_human-robot_2013,
	title = {Human-robot interaction and cooperation through people detection and gesture recognition},
	volume = {24},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879385199&doi=10.1007%2fs40313-013-0040-3&partnerID=40&md5=96fe03c3d975fc7e41d890ca7ecac8c2},
	doi = {10.1007/s40313-013-0040-3},
	abstract = {In this paper, we propose a system that performs human-robot interaction in order to carry out a cooperation between a robot and a person. The system is based on people detection and gesture recognition. Human beings are detected using a technique that combines face and legs detection from data provided by a camera and a laser range finder. Besides that the gesture recognition method allows the person to choose the kind of help he/she wants. In this work, the cooperative tasks the robot can perform are to guide the user up to a desired place in the work environment, to carry a load together with or for a person, to follow a person or navigate to a place to get something the user wants. Many experiments were performed and two of them (person guidance and navigation up to a predefined place carrying an object) will be shown in this paper with the purpose of validating the proposed system. © 2013 Brazilian Society for Automatics - SBA.},
	number = {3},
	journal = {Journal of Control, Automation and Electrical Systems},
	author = {Pereira, F.G. and Vassallo, R.F. and Salles, E.O.T.},
	year = {2013},
	keywords = {1scopus},
	pages = {187--198},
	annote = {cited By 12},
	file = {Full Text:/Users/tid010/Zotero/storage/VF6JFWLD/Pereira et al. - 2013 - Human-robot interaction and cooperation through pe.pdf:application/pdf},
}

@article{lu_research_2020,
	title = {Research and {Implementation} of {Real}-{Time} {Motion} {Control} of {Robot} {Based} on {Kinect}},
	volume = {166},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091304774&doi=10.1007%2f978-3-030-57745-2_65&partnerID=40&md5=10621b1c2579a5ea67a50362403884bd},
	doi = {10.1007/978-3-030-57745-2_65},
	abstract = {With the development of human-computer interaction technology, the interactive experience between human and machine becomes more and more simple and natural. The ideal state of human-computer interaction can be operated by natural devices such as voice, gestures and expressions, which have been tried and applied in the Kinect field. Kinect, as a new generation of motion sensing device, can achieve the input function by capturing the user’s motion and speech recognition in real time, so as to control the movement of the robot and effectively improve its intelligence. The research of this paper mainly includes skeleton tracking and robot control. Kinect1 generation was used to obtain the human body depth and skeleton images. VS software was used for programming. OpenNI and Kinect SDK libraries were called to identify the human arm joints and obtain the spatial coordinates. The robot controller USES the STMF series 32-bit processor, accepts serial port data, controls the robot arm steering gear, and completes the human body movement imitation. The motion trajectory is smooth and stable, and the motion effect is good. © 2020, Springer Nature Switzerland AG.},
	journal = {Smart Innovation, Systems and Technologies},
	author = {Lu, G. and Tang, W. and Zheng, J. and Chen, T. and Zou, X.},
	year = {2020},
	keywords = {1scopus},
	pages = {779--792},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/YWNQ476G/Lu et al. - 2020 - Research and Implementation of Real-Time Motion Co.pdf:application/pdf},
}

@inproceedings{broccia_gestural_2011,
	title = {Gestural interaction for robot motion control},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883250884&doi=10.2312%2fLocalChapterEvents%2fItalChap%2fItalianChapConf2011%2f061-066&partnerID=40&md5=cb25345d629567fda8073a7f7fd53bea},
	doi = {10.2312/LocalChapterEvents/ItalChap/ItalianChapConf2011/061-066},
	abstract = {Recent advances in gesture recognition made the problem of controlling a humanoid robot in the most natural possible way an interesting challenge. Learning from Demonstration field takes strong advantage from this kind of interaction since users who have no robotics knowledge are allowed to teach new tasks to robots easier than ever before. In this work we present a cheap and easy way to implement humanoid robot along with a visual interaction interface allowing users to control it. The visual system is based on the Microsoft Kinect's RGB-D camera. Users can deal with the robot just by standing in front of the depth camera and mimicking a particular task they want to be performed by the robot. Our framework is cheap, easy to reproduce, and does not strictly depend on the particular underlying sensor or gesture recognition system. © The Eurographics Association 2011.},
	booktitle = {Eurographics {Italian} {Chapter} {Conference} 2011},
	author = {Broccia, G. and Livesu, M. and Scateni, R.},
	year = {2011},
	keywords = {1scopus},
	pages = {61--66},
	annote = {cited By 3},
	file = {Full Text:/Users/tid010/Zotero/storage/K6ZRU3CA/Broccia et al. - 2011 - Gestural interaction for robot motion control.pdf:application/pdf},
}

@article{lee_real-time_2020,
	title = {Real-time human action recognition with a low-cost {RGB} camera and mobile robot platform},
	volume = {20},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085157361&doi=10.3390%2fs20102886&partnerID=40&md5=551a77bc6c90bed36b1154115773fed9},
	doi = {10.3390/s20102886},
	abstract = {Human action recognition is an important research area in the field of computer vision that can be applied in surveillance, assisted living, and robotic systems interacting with people. Although various approaches have been widely used, recent studies have mainly focused on deeplearning networks using Kinect camera that can easily generate data on skeleton joints using depth data, and have achieved satisfactory performances. However, their models are deep and complex to achieve a higher recognition score; therefore, they cannot be applied to a mobile robot platform using a Kinect camera. To overcome these limitations, we suggest a method to classify human actions in real-time using a single RGB camera, which can be applied to the mobile robot platform as well. We integrated two open-source libraries, i.e., Open Pose and 3D-baseline, to extract skeleton joints on RGB images, and classified the actions using convolutional neural networks. Finally, we set up the mobile robot platform including an NVIDIA JETSON XAVIER embedded board and tracking algorithm to monitor a person continuously. We achieved an accuracy of 70\% on the NTURGBD training dataset, and the whole process was performed on an average of 15 frames per second (FPS) on an embedded board system. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.},
	number = {10},
	journal = {Sensors (Switzerland)},
	author = {Lee, J. and Ahn, B.},
	year = {2020},
	keywords = {1scopus},
	annote = {cited By 1},
	file = {Full Text:/Users/tid010/Zotero/storage/ZJCJ4WG3/Lee and Ahn - 2020 - Real-time human action recognition with a low-cost.pdf:application/pdf},
}

@article{cicirelli_kinect-based_2015,
	title = {A kinect-based gesture recognition approach for a natural human robot interface},
	volume = {12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930227223&doi=10.5772%2f59974&partnerID=40&md5=78296717e15dbc3c01b4c31f7b7b56f5},
	doi = {10.5772/59974},
	abstract = {In this paper, we present a gesture recognition system for the development of a human-robot interaction (HRI) interface. Kinect cameras and the OpenNI framework are used to obtain real-time tracking of a human skeleton. Ten different gestures, performed by different persons, are defined. Quaternions of joint angles are first used as robust and significant features. Next, neural network (NN) classifiers are trained to recognize the different gestures. This work deals with different challenging tasks, such as the real-time implementation of a gesture recognition system and the temporal resolution of gestures. The HRI interface developed in this work includes three Kinect cameras placed at different locations in an indoor environment and an autonomous mobile robot that can be remotely controlled by one operator standing in front of one of the Kinects. Moreover, the system is supplied with a people re-identification module which guarantees that only one person at a time has control of the robot. The system's performance is first validated offline, and then online experiments are carried out, proving the real-time operation of the system as required by a HRI interface. © 2015 The Author(s). Licensee InTech.},
	journal = {International Journal of Advanced Robotic Systems},
	author = {Cicirelli, G. and Attolico, C. and Guaragnella, C. and D'Orazio, T.},
	year = {2015},
	keywords = {1scopus},
	annote = {cited By 34},
	file = {Full Text:/Users/tid010/Zotero/storage/NP6U878H/Cicirelli et al. - 2015 - A kinect-based gesture recognition approach for a .pdf:application/pdf},
}

@article{anuradha_human_2020,
	title = {Human detection and following robot},
	volume = {9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082858300&partnerID=40&md5=76978175cda39acb033058f4d669c257},
	abstract = {Human following robots has been mostly developed and researched by many countries due to its prized applications in daily life and in industrial applications. Mainly the human following robot requires several techniques to generate an interaction between human and robot. The human following robot, can be implemented using various approaches such as the stereo camera, a Laser Range Finder (LFR) and RFID system. The Kinect Xbox 360 sensor is used for this research to track the human and it is developed for identifying and tracking the targeted human using skeleton view. Compared to other existing tracking methods, the human skeleton tracking method has the capability to distinguish a person and other objects in an efficient manner. Furthermore, the Kinect Xbox 360 sensor can detect person's position and distance as well. Human skeleton law is used here for the purpose of tracking the human. This method is more accurate because it detects only human beings in the field of view and keeps the reference point of a person longer than the method of the center of mass. For this research, the laptop is used to process the data from the Kinect sensor and transfer to the Arduino Mega 2560 through serial communication. Kinect sensor is the main part of the system which is used for identifying the human. All the data in the Kinect sensor processes by the software in the personal computer located on the robot. Moreover, Arduino instructs to control the speed and direction of the mobile robot via the motor controller. In existing systems, depth image of the human has been taken to follow the human. But in this system one reference point of the human Skelton is taken to do that. This method saves the processing power and the time in image processing and reduce the errors. Hence the accuracy of this system is high. The results for the whole hardware and software the mobile robot able to follow human according to the coordinate positions of the person detect by Kinect sensor. The interaction of human and robot assist human in various situations such as carry loads that are required by people working in airports, hospitals, and other moving activities. Human following robot can bring many benefits to mankind. Through this implementation a robust method has been developed to address to robots and make them follow the master on move. Significant of results are checked by using Chi-Squared test at 0.05 level of significance. © IJSTR 2020.},
	number = {3},
	journal = {International Journal of Scientific and Technology Research},
	author = {Anuradha, U.A.D.N. and Kumari, K.W.S.N. and Chathuranga, K.W.S.},
	year = {2020},
	keywords = {1scopus},
	pages = {6359--6363},
	annote = {cited By 0},
	file = {Anuradha et al. - 2020 - Human detection and following robot.pdf:/Users/tid010/Zotero/storage/W7ZM6D2W/Anuradha et al. - 2020 - Human detection and following robot.pdf:application/pdf},
}

@inproceedings{foster_two_2012,
	title = {Two people walk into a bar: {Dynamic} multi-party social interaction with a robot agent},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870204994&doi=10.1145%2f2388676.2388680&partnerID=40&md5=1978d0318d6f613845a0471f32bf0f44},
	doi = {10.1145/2388676.2388680},
	abstract = {We introduce a humanoid robot bartender that is capable of dealing with multiple customers in a dynamic, multi-party social setting. The robot system incorporates state-of-the-art components for computer vision, linguistic processing, state management, high level reasoning, and robot control. In a user evaluation, 31 participants interacted with the bartender in a range of social situations. Most customers successfully obtained a drink from the bartender in all scenarios, and the factors that had the greatest impact on subjective satisfaction were task success and dialogue efficiency. Copyright 2012 ACM.},
	booktitle = {{ICMI}'12 - {Proceedings} of the {ACM} {International} {Conference} on {Multimodal} {Interaction}},
	author = {Foster, M.E. and Gaschler, A. and Giuliani, M. and Isard, A. and Pateraki, M. and Petrick, R.P.A.},
	year = {2012},
	keywords = {1scopus},
	pages = {3--10},
	annote = {cited By 52},
	file = {Full Text:/Users/tid010/Zotero/storage/BJWEMJIT/Foster et al. - 2012 - Two people walk into a bar Dynamic multi-party so.pdf:application/pdf},
}

@inproceedings{hassan_computationally_2016,
	title = {A computationally low cost vision based tracking algorithm for human following robot},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978512195&doi=10.1109%2fICCAR.2016.7486699&partnerID=40&md5=b4c002b2b04e573600919aecbaaf6200},
	doi = {10.1109/ICCAR.2016.7486699},
	abstract = {Recently, there has been an increasing interest in the field of human interactive robotics. Contrary to otherwise complex and resource hungry algorithms, we in this work have presented a computationally low cost algorithm for a human following robotic application. Instead of detecting the human, the algorithm makes use of a specific colour tag placed on the human subject which is detected by a camera mounted on the robot. Sensors including range sensor, magnetometer and optical encoders are utilized in tandem to assist the human following process. The method is tested on a custom built robotic platform running Raspberry pi minicomputer. We have performed and presented the results of several experiments for the evaluation of our method. © 2016 IEEE.},
	booktitle = {Proceedings - 2016 the 2nd {International} {Conference} on {Control}, {Automation} and {Robotics}, {ICCAR} 2016},
	author = {Hassan, M.S. and Khan, A.F. and Khan, M.W. and Uzair, M. and Khurshid, K.},
	year = {2016},
	keywords = {1scopus},
	pages = {62--65},
	annote = {cited By 3},
	file = {Full Text:/Users/tid010/Zotero/storage/UDK3KWYU/Hassan et al. - 2016 - A computationally low cost vision based tracking a.pdf:application/pdf},
}

@inproceedings{jia_autonomous_2011,
	title = {Autonomous robot human detecting and tracking based on stereo vision},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-81055137573&doi=10.1109%2fICMA.2011.5985736&partnerID=40&md5=4771d4dd68b63dd44e46fc0bc0825145},
	doi = {10.1109/ICMA.2011.5985736},
	abstract = {Detecting and tracking people are challenging problems, because the human body is non-rigid and the detected human are easily occluded by other objects. In this paper, we present a robust human detecting and tracking system which can be used in indoor environments. The proposed method is to get the human's disparity image from stereo camera, and then we extract the identify model human of by image processing. Hu moment is chosen to detect human because it has the invariant character of translation, rotation, proportion. Robust human tracking is performed with Extend Kalman Filter (EKF) as it's flexible and easy to apply in practical environments. In proposed system, the operator can monitor the process; modify the parameters and observe the experiment results in developed interactive GUI. Our experiments have demonstrated that the method improves human detecting and tracking robustness. © 2011 IEEE.},
	booktitle = {2011 {IEEE} {International} {Conference} on {Mechatronics} and {Automation}, {ICMA} 2011},
	author = {Jia, S. and Zhao, L. and Li, X. and Cui, W. and Sheng, J.},
	year = {2011},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {640--645},
	annote = {cited By 4},
	file = {Full Text:/Users/tid010/Zotero/storage/ZGANIGP6/Jia et al. - 2011 - Autonomous robot human detecting and tracking base.pdf:application/pdf},
}

@inproceedings{yang_socially-aware_2019,
	title = {Socially-aware navigation of omnidirectional mobile robot with extended social force model in multi-human environment},
	volume = {2019-October},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076762480&doi=10.1109%2fSMC.2019.8913844&partnerID=40&md5=773e2383e9c04a04b878473bdc9a9ffd},
	doi = {10.1109/SMC.2019.8913844},
	abstract = {We propose a new navigation method for an omnidirectional mobile robot to maneuver in a complex and populated environment. In an indoor populated environment, for example, robot has to react to such circumstances and achieve both physical and psychological navigational safety. From social researches, human motion is a hybrid system composed of holonomic and non-holonomic movements. Based on this concept, we develop a socially-aware navigation method for an omnidirectional mobile robot to achieve natural, humanlike movement. By using laser range finder and camera as sensors, robot detects geometric features and human behavior information. From the geometric features robot can construct an environment model and extract the information about distances and directions of the obstacles. Whereas with the heading and orientation information from camera, robot can model individual humans successfully. While acquiring these models as context, a suitable navigation behavior would be executed. To interact with surroundings, we develop the extended Social Force Model(ESFM) to describe the interactive force, referred to as social force, with human and environment. As a whole, the contributions of our work are twofold one being that we propose a dynamic grouping model based on human behavior using learning-based method and another being that we develop an extended Social Force Model for the system based on which a successful navigation strategy can be realized. © 2019 IEEE.},
	booktitle = {Conference {Proceedings} - {IEEE} {International} {Conference} on {Systems}, {Man} and {Cybernetics}},
	author = {Yang, C.-T. and Zhang, T. and Chen, L.-P. and Fu, L.-C.},
	year = {2019},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {1963--1968},
	annote = {cited By 4},
	file = {Full Text:/Users/tid010/Zotero/storage/KHNGW2IS/Yang et al. - 2019 - Socially-aware navigation of omnidirectional mobil.pdf:application/pdf},
}

@inproceedings{miller_self-driving_2019,
	title = {Self-{Driving} {Mobile} {Robots} {Using} {Human}-{Robot} {Interactions}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062228856&doi=10.1109%2fSMC.2018.00219&partnerID=40&md5=1f86f0b5146699d465e905f25e9f9f9d},
	doi = {10.1109/SMC.2018.00219},
	abstract = {This paper presents a robotic system implementation featuring an autonomy solution that incorporates human intents through effective human-robot interactions. More specifically, vision-based human pose estimation is used to identify gestures that correspond to human intents. These intents are mapped to predefined commands that schedule autonomous operations of the robotic system. The paper presents an overview of an autonomy solution for mobile robots, an approach to human-following operation, and an approach to automated decision-making for parking operations. Experimental results of the physical robot implementation are presented. © 2018 IEEE.},
	booktitle = {Proceedings - 2018 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics}, {SMC} 2018},
	author = {Miller, J. and Hong, S. and Lu, J.},
	year = {2019},
	keywords = {1scopus},
	pages = {1251--1256},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/5SW3M55J/Miller et al. - 2019 - Self-Driving Mobile Robots Using Human-Robot Inter.pdf:application/pdf},
}

@inproceedings{taheri_social_2014,
	title = {Social robots as assistants for autism therapy in {Iran}: {Research} in progress},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922664025&doi=10.1109%2fICRoM.2014.6990995&partnerID=40&md5=a3855058d962c3582aa7a70c65005832},
	doi = {10.1109/ICRoM.2014.6990995},
	abstract = {Autistic children are often impaired in initiating and responding to Joint Attention. In recent years, there has been an increase in the application of robots in diagnosis and treatment of autism. The purpose of the current research has been primarily to originate the proper therapeutic scenarios and to implement two interactive humanoid robots as therapy assistants in autism treatment in Iran. To this end, the humanoid robots were programmed and teleoperated via Microsoft Kinect Sensor and PhantomOmni Haptic Robot to elicit reactions consisting of imitation of humans by the humanoid robots and vice versa. In this paper, we elaborate on the therapeutic items that we have designed to improve joint attention and imitation in autistic children through using humanoid robots. Moreover, the fairly promising results of some interventions conducted in a pilot study on four autistic cases will be addressed and discussed. Our research target is to increase social interaction and involve autistic children in dyadic/triadic interactions which seems quite possible due to the findings of the pilot study conducted. © 2014 IEEE.},
	booktitle = {2014 2nd {RSI}/{ISM} {International} {Conference} on {Robotics} and {Mechatronics}, {ICRoM} 2014},
	author = {Taheri, A.R. and Alemi, M. and Meghdari, A. and Pouretemad, H.R. and Basiri, N.M.},
	year = {2014},
	keywords = {1scopus},
	pages = {760--766},
	annote = {cited By 22},
	file = {Full Text:/Users/tid010/Zotero/storage/X8SWPXD3/Taheri et al. - 2014 - Social robots as assistants for autism therapy in .pdf:application/pdf},
}

@inproceedings{araiza-lllan_dynamic_2018,
	title = {Dynamic {Regions} to {Enhance} {Safety} in {Human}-{Robot} {Interactions}},
	volume = {2018-September},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057227674&doi=10.1109%2fETFA.2018.8502453&partnerID=40&md5=0550d4bd773b53fdb8dc9f58938ec944},
	doi = {10.1109/ETFA.2018.8502453},
	abstract = {The adoption of robots for collaborative tasks strongly depends on ensuring the safety of the operators, through both internal sensors for collision detection and protective stop, and external sensors to monitor human presence. External industrial safety sensors (e.g. laser scanners) are expensive, do not distinguish between a trained operator and a bystander, and the cycle time may be increased considerably as the robot stops or reduces its speed without considering the behaviour of the people around it in a more effective manner. We present a dynamic safety solution for human-robot collaboration that tracks human behaviour, based on on RGB-D cameras. Our solution updates in real-time the stopping and speed reducing areas of a robot, according to the robot's speed and the spatial relation between the robot and operators or bystanders. In our solution, the data from a commercial RGB-D camera provides more information about the people in the space, compared to industrial safety sensors. The presented solution is first evaluated through a case study of a collaborative robot in a pick-and-place task in a manufacturing workshop. Then, the solution is compared with off-the-shelf industrial safety sensors. Finally, we characterize the system's capabilities experimentally. The results indicate that using the proposed system significantly reduces the total average cycle time of the task, compared to traditional industrial safety setups. © 2018 IEEE.},
	booktitle = {{IEEE} {International} {Conference} on {Emerging} {Technologies} and {Factory} {Automation}, {ETFA}},
	author = {Araiza-Lllan, D. and De San Bernabe Clemente, A.},
	year = {2018},
	keywords = {1scopus},
	pages = {693--698},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/LB6N7A9K/Araiza-Lllan and De San Bernabe Clemente - 2018 - Dynamic Regions to Enhance Safety in Human-Robot I.pdf:application/pdf},
}

@article{bingol_practical_2020,
	title = {Practical application of a safe human-robot interaction software},
	volume = {47},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078963315&doi=10.1108%2fIR-09-2019-0180&partnerID=40&md5=043a205a0b8cc2d81b0a423f58e43b20},
	doi = {10.1108/IR-09-2019-0180},
	abstract = {Purpose: Because of the increased use of robots in the industry, it has become inevitable for humans and robots to be able to work together. Therefore, human security has become the primary noncompromising factor of joint human and robot operations. For this reason, the purpose of this study was to develop a safe human-robot interaction software based on vision and touch. Design/methodology/approach: The software consists of three modules. Firstly, the vision module has two tasks: to determine whether there is a human presence and to measure the distance between the robot and the human within the robot’s working space using convolutional neural networks (CNNs) and depth sensors. Secondly, the touch detection module perceives whether or not a human physically touches the robot within the same work environment using robot axis torques, wavelet packet decomposition algorithm and CNN. Lastly, the robot’s operating speed is adjusted according to hazard levels came from vision and touch module using the robot’s control module. Findings: The developed software was tested with an industrial robot manipulator and successful results were obtained with minimal error. Practical implications: The success of the developed algorithm was demonstrated in the current study and the algorithm can be used in other industrial robots for safety. Originality/value: In this study, a new and practical safety algorithm is proposed and the health of people working with industrial robots is guaranteed. © 2020, Emerald Publishing Limited.},
	number = {3},
	journal = {Industrial Robot},
	author = {Bingol, M.C. and Aydogmus, O.},
	year = {2020},
	keywords = {1scopus},
	pages = {359--368},
	annote = {cited By 2},
	file = {Bingol and Aydogmus - 2020 - Practical application of a safe human-robot intera.pdf:/Users/tid010/Zotero/storage/7ETSJA7G/Bingol and Aydogmus - 2020 - Practical application of a safe human-robot intera.pdf:application/pdf},
}

@inproceedings{angonese_multiple_2017,
	title = {Multiple people detection and identification system integrated with a dynamic simultaneous localization and mapping system for an autonomous mobile robotic platform},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029368093&doi=10.1109%2fMILTECHS.2017.7988861&partnerID=40&md5=19636d79c444d78b4f59444aa7b09760},
	doi = {10.1109/MILTECHS.2017.7988861},
	abstract = {This paper presents the integration of a multiple people detection and identification system with a dynamic simultaneous localization and mapping system for an autonomous robotic platform. This integration allows the exploration and navigation of the robot considering people identification. The robotic platform consists of a Pioneer 3DX robot equipped with an RGBD camera, a Sick Lms200 sensor laser and a computer using the robot operating system (ROS). The idea is to integrate the people detection and identification system to the simultaneous localization and mapping (SLAM) system of the robot using ROS. The people detection and identification system is performed in two steps. The first one is for detecting multiple people on scene and the other one is for an individual person identification. Both steps are implemented as ROS nodes that works integrated with the SLAM ROS node. The multiple people detection's node uses a manual feature extraction technique based on HOG (Histogram of Oriented Gradients) detectors, implemented using the PCL library (Point Cloud Library) in C ++. The person's identification node is based on a Deep Convolutional Neural Network (CNN) that are implemented using the MatLab MatConvNet library. This step receives the detected people centroid from the previous step and performs the classification of a specific person. After that, the desired person centroid is send to the SLAM node, that consider it during the mapping process. Tests were made objecting the evaluation of accurateness in the people's detection and identification process. It allowed us to evaluate the people detection system during the navigation and exploration of the robot, considering the real time interaction of people recognition in a semi-structured environment. © 2017 IEEE.},
	booktitle = {{ICMT} 2017 - 6th {International} {Conference} on {Military} {Technologies}},
	author = {Angonese, A.T. and Ferreira Rosa, P.F.},
	year = {2017},
	keywords = {1scopus},
	pages = {779--786},
	annote = {cited By 2},
	file = {Full Text:/Users/tid010/Zotero/storage/TQPZ4YII/Angonese and Ferreira Rosa - 2017 - Multiple people detection and identification syste.pdf:application/pdf},
}

@inproceedings{landi_prediction_2019,
	title = {Prediction of {Human} {Arm} {Target} for {Robot} {Reaching} {Movements}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081160661&doi=10.1109%2fIROS40897.2019.8968559&partnerID=40&md5=4bbfa51b6964980f6a4970e00890ba49},
	doi = {10.1109/IROS40897.2019.8968559},
	abstract = {The raise of collaborative robotics has allowed to create new spaces where robots and humans work in proximity. Consequently, to predict human movements and his/her final intention becomes crucial to anticipate robot next move, preserving safety and increasing efficiency. In this paper we propose a human-arm prediction algorithm that allows to infer if the human operator is moving towards the robot to intentionally interact with it. The human hand position is tracked by an RGB-D camera online. By combining the Minimum Jerk model with Semi-Adaptable Neural Networks we obtain a reliable prediction of the human hand trajectory and final target in a short amount of time. The proposed algorithm was tested in a multi-movements scenario with FANUC LR Mate 200iD/7L industrial robot. © 2019 IEEE.},
	booktitle = {{IEEE} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	author = {Landi, C.T. and Cheng, Y. and Ferraguti, F. and Bonfe, M. and Secchi, C. and Tomizuka, M.},
	year = {2019},
	keywords = {1scopus},
	pages = {5950--5957},
	annote = {cited By 2},
	file = {Full Text:/Users/tid010/Zotero/storage/KICACKZ7/Landi et al. - 2019 - Prediction of Human Arm Target for Robot Reaching .pdf:application/pdf},
}

@inproceedings{ehlers_human-robot_2016,
	title = {A human-robot interaction interface for mobile and stationary robots based on real-time {3D} human body and hand-finger pose estimation},
	volume = {2016-November},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996538586&doi=10.1109%2fETFA.2016.7733719&partnerID=40&md5=3d8ce06d52bf31227aef0b08da1254c0},
	doi = {10.1109/ETFA.2016.7733719},
	abstract = {This paper presents a real-time gesture-based human-robot interaction (HRI) interface for mobile and stationary robots. A human detection approach is used to estimate the entire 3D point cloud of a human being inside the field of view of a moving camera. Afterwards, the pose of the human body is estimated using an efficient self-organizing map approach. Furthermore, a hand-finger pose estimation approach based on a self-scaling kinematic hand skeleton is presented and evaluated. A trained support vector machine is used to classify 29 hand-finger gestures based on the angles of the finger joints. The HRI interface is integrated into the ROS framework and qualitatively evaluated in a first test scenario on a mobile robot equipped with an RGB-D camera for gesture interaction. Since the hand-finger pose, the hand-finger gesture, as well as the whole body pose are estimated, the interface allows a flexible implementation of various applications. © 2016 IEEE.},
	booktitle = {{IEEE} {International} {Conference} on {Emerging} {Technologies} and {Factory} {Automation}, {ETFA}},
	author = {Ehlers, K. and Brama, K.},
	year = {2016},
	keywords = {1scopus},
	annote = {cited By 14},
	file = {Full Text:/Users/tid010/Zotero/storage/B9ZKUFH3/Ehlers and Brama - 2016 - A human-robot interaction interface for mobile and.pdf:application/pdf},
}

@inproceedings{sosnowski_mirror_2010,
	title = {Mirror my emotions! {Combining} facial expression analysis and synthesis on a robot},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863926598&partnerID=40&md5=109248e6985b1ff75829cc333a0d272e},
	abstract = {Everyday human communication relies on a large number of different communication mechanisms like spoken language, facial expressions, body pose or gestures. Facial expressions are one of the main communication mechanisms and pass large amounts of information between human dialogue partners [22]. Therefore, the analysis and the synthesis of facial expressions are important steps towards an intuitive human-machine interaction and form valuable research targets. We present a system that tackles both challenges. It relies on a fully automated, model-based, real-time capable approach to distinguish universal facial expressions and their intensities from camera images. Facial expression synthesis is conducted via the robot head EDDIE, a flexible low-cost emotion-display with 23 degrees of freedom. Static facial expressions at continuous intensities are included, as well as smooth transitions based on the circumplex model of affect. Miniature off-the-shelf mechatronic components are used to provide high functionality at low cost. Evaluations conducted in a user-study show that emotions displayed by EDDIE are recognizable by humans very well. By combining facial expression recognition and display on the robot, a first demonstration is presented in which the robot mirrors the human's emotions, as a basis for further research in the field of emotional closed loop systems.},
	booktitle = {Proceedings of the 2nd {International} {Symposium} on {New} {Frontiers} in {Human}-{Robot} {Interaction} - {A} {Symposium} at the {AISB} 2010 {Convention}},
	author = {Sosnowski, S. and Mayer, C. and Kühnlenz, K. and Radig, B.},
	year = {2010},
	keywords = {1scopus},
	pages = {108--112},
	annote = {cited By 4},
	file = {Sosnowski et al. - 2010 - Mirror my emotions! Combining facial expression an.pdf:/Users/tid010/Zotero/storage/R75MWSUM/Sosnowski et al. - 2010 - Mirror my emotions! Combining facial expression an.pdf:application/pdf},
}

@article{martin_real-time_2019,
	title = {Real-{Time} {Gestural} {Control} of {Robot} {Manipulator} {Through} {Deep} {Learning} {Human}-{Pose} {Inference}},
	volume = {11754 LNCS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077007991&doi=10.1007%2f978-3-030-34995-0_51&partnerID=40&md5=22e01cd596eb920237aafe5f607a24af},
	doi = {10.1007/978-3-030-34995-0_51},
	abstract = {With the raise of collaborative robots, human-robot interaction needs to be as natural as possible. In this work, we present a framework for real-time continuous motion control of a real collaborative robot (cobot) from gestures captured by an RGB camera. Through deep learning existing techniques, we obtain human skeletal pose information both in 2D and 3D. We use it to design a controller that makes the robot mirror in real-time the movements of a human arm or hand. © 2019, Springer Nature Switzerland AG.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Martin, J.B. and Moutarde, F.},
	year = {2019},
	keywords = {1scopus},
	pages = {565--572},
	annote = {cited By 0},
	file = {Submitted Version:/Users/tid010/Zotero/storage/ERKKB7BX/Martin and Moutarde - 2019 - Real-Time Gestural Control of Robot Manipulator Th.pdf:application/pdf},
}

@article{arenas_deep_2017,
	title = {Deep convolutional neural network for hand gesture recognition used for human-robot interaction},
	volume = {12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047496304&doi=10.3923%2fjeasci.2017.9278.9285&partnerID=40&md5=db7621ab2479dc80229278f1aacc3693},
	doi = {10.3923/jeasci.2017.9278.9285},
	abstract = {This study presents the training and validation of a deep convolutional neural network architecture used for a human-robot interaction. Two different datasets of images were employed with the aim of recognizing 2 kinds of hand gestures which are "closed" and "open" and control a robotic arm with these gestures. To choose the best training in the network, different behavioral parameters such as training accuracy and loss were evaluated to obtain the best training epoch and validation parameters such as validation accuracy and internal behavior of the network through the activations of the convolution layers. Once the trained network is chosen, camera tests and interaction with a robotic arm are performed, evaluating the interaction between the user and the actions of the robot through the network. © Medwell Journals, 2017.},
	number = {Specialissue11},
	journal = {Journal of Engineering and Applied Sciences},
	author = {Arenas, J.O.P. and Beleno, R.D.H. and Moreno, R.J.},
	year = {2017},
	keywords = {1scopus},
	pages = {9278--9285},
	annote = {cited By 1},
	file = {Arenas et al. - 2017 - Deep convolutional neural network for hand gesture.pdf:/Users/tid010/Zotero/storage/5KRSAN9Y/Arenas et al. - 2017 - Deep convolutional neural network for hand gesture.pdf:application/pdf},
}

@inproceedings{saleh_nonverbal_2015,
	title = {Nonverbal communication with a humanoid robot via head gestures},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957035238&doi=10.1145%2f2769493.2769543&partnerID=40&md5=30e3ff1ad43b88cb8ea2d32c90693069},
	doi = {10.1145/2769493.2769543},
	abstract = {Social interactive robots require sophisticated perception and cognition abilities to behave and interact in a natural humanlike way. The proper perception of behavior of interaction partner plays a crucial role in social robotics. The interpretation of these behaviors and mapping them to their exact meanings is also an important aspect that interactive robots should have. This paper proposes an interaction model for communicating verbally and nonverbally with human. Human behavior, during the interaction with the robot, is perceived and then interpreted depending on the situation in which the behavior has been detected. In this model, head gestures are used as a back channel (feedback) for the robot to adapt the interaction scenario. The back channel signals can be consciously or unconsciously generated by human. Simultaneously, the eye gazes are also detected to ensure right interpretation of head gestures. In order to recognize the human head gestures, head poses have been tracked over time. A stream of images with their corresponding depth information, acquired from a Kinect sensor, are used to find, track, and estimate the head poses of human. The proposed model has been tested in various experiments with different scenarios in interaction with human. © 2015 ACM.},
	booktitle = {8th {ACM} {International} {Conference} on {PErvasive} {Technologies} {Related} to {Assistive} {Environments}, {PETRA} 2015 - {Proceedings}},
	author = {Saleh, S. and Berns, K.},
	year = {2015},
	keywords = {1scopus},
	annote = {cited By 4},
	file = {Full Text:/Users/tid010/Zotero/storage/LAM5NM3U/Saleh and Berns - 2015 - Nonverbal communication with a humanoid robot via .pdf:application/pdf},
}

@inproceedings{luo_tracking_2011,
	title = {Tracking with pointing gesture recognition for human-robot interaction},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863155475&doi=10.1109%2fSII.2011.6147623&partnerID=40&md5=ebc4ae9543782a64eb8a4ba850c839fd},
	doi = {10.1109/SII.2011.6147623},
	abstract = {In the field of human-robot interaction, it is the key problem for the robot to know the information of the interactive partner. Without obtaining these information the robot couldn't realize what is the partner means. Currently most of the interactive robots have vision sensors to catch human face, and then perform some reactions to the interactive partner. But it could only access under strict conditions because when the partner get out of the sight, the robot couldn't know where to find him. So it is important for the robot to find other information indicated by partner. Some of the interactive robot has audition sensors to localize the sound source. The robot could recognize the location of the sound source produced by partner and try to face him. But the audition system need more computation and hardware resources, so it's hard to be embed in a existing systems. Therefore, we try to find another solution. In this paper, we presented a tracking system with pointing gesture recognition for human-robot interaction. By showing fingers we can provide information for the robot to know that there is another person who wants to interact with it. Finally the whole system will implement at our humanoid robot head. © 2011 IEEE.},
	booktitle = {2011 {IEEE}/{SICE} {International} {Symposium} on {System} {Integration}, {SII} 2011},
	author = {Luo, R.C. and Chang, S.-R. and Yang, Y.-P.},
	year = {2011},
	keywords = {1scopus},
	pages = {1220--1225},
	annote = {cited By 5},
	file = {Full Text:/Users/tid010/Zotero/storage/AXL8F7IC/Luo et al. - 2011 - Tracking with pointing gesture recognition for hum.pdf:application/pdf},
}

@inproceedings{gori_all_2012,
	title = {All gestures you can: {A} memory game against a humanoid robot},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885624862&doi=10.1109%2fHUMANOIDS.2012.6651540&partnerID=40&md5=433c90866159367a1fdf567269c01cea},
	doi = {10.1109/HUMANOIDS.2012.6651540},
	abstract = {We address the problem of real-time gesture recognition, and we prove that our system can be used in real scenarios presenting an original memory game; the object of the game is to perform the longest sequence of gestures that it is possible to remember. We explore the human-robot interaction field, letting the player confront a humanoid robot, iCub. Our main contribution is two-fold: on one hand, we present a robust and real-time gesture recognition system; on the other hand, we place the presented system in a real scenario, where its reliability and its effectiveness are remarkably stressed. This game has ranked 2nd at ChaLearn Kinect Demonstration Competition1. © 2012 IEEE.},
	booktitle = {{IEEE}-{RAS} {International} {Conference} on {Humanoid} {Robots}},
	author = {Gori, I. and Fanello, S.R. and Metta, G. and Odone, F.},
	year = {2012},
	keywords = {1scopus},
	pages = {330--336},
	annote = {cited By 5},
	file = {Full Text:/Users/tid010/Zotero/storage/G26WHXX2/Gori et al. - 2012 - All gestures you can A memory game against a huma.pdf:application/pdf},
}

@inproceedings{li_learning_2018,
	title = {Learning complex assembly skills from kinect based human robot interaction},
	volume = {2018-January},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049909176&doi=10.1109%2fROBIO.2017.8324818&partnerID=40&md5=4a45f4afa2a7d2dbb72e1cacae11883a},
	doi = {10.1109/ROBIO.2017.8324818},
	abstract = {Acquiring complex assembly skills is still a challenging task for robot programming. Because of the sensory and body structure differences, the human knowledge has to be demonstrated, recorded, converted and finally learned by the robot, in an inexplicit and indirect way. During this process, 'how to demonstrate', 'how to convert' and 'how to learn' are the key problems. In this paper, Kinect sensor is utilized to provide the behavior information of the human demonstrator. Through natural human robot interaction, body skeleton and joint 3D coordinates are provided in real-Time, which can fully describe the human intension and task related skills. To overcome the structural and individual differences, a Cartesian level unified mapping method is proposed to convert the human motion and match the specified robot. The recorded data set are modeled using Gaussian mixture model(GMM) and Gaussian mixture regression(GMR), which can extract redundancies across multiple demonstrations and build robust models to regenerate the dynamics of the recorded movements. The proposed methodologies are implemented in the imNEU humanoid robot platform. Experimental results verify the effectiveness. © 2017 IEEE.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Robotics} and {Biomimetics}, {ROBIO} 2017},
	author = {Li, X. and Cheng, H. and Ji, G. and Chen, J.},
	year = {2018},
	keywords = {1scopus},
	pages = {2646--2651},
	annote = {cited By 2},
	file = {Full Text:/Users/tid010/Zotero/storage/RCJRWUNU/Li et al. - 2018 - Learning complex assembly skills from kinect based.pdf:application/pdf},
}

@article{li_cnn_2019,
	title = {{CNN} and {LSTM} {Based} {Facial} {Expression} {Analysis} {Model} for a {Humanoid} {Robot}},
	volume = {7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073888886&doi=10.1109%2fACCESS.2019.2928364&partnerID=40&md5=acace8c3a1e17f09cd0cb56d6f4e53f8},
	doi = {10.1109/ACCESS.2019.2928364},
	abstract = {Robots must be able to recognize human emotions to improve the human-robot interaction (HRI). This study proposes an emotion recognition system for a humanoid robot. The robot is equipped with a camera to capture users' facial images, and it uses this system to recognize users' emotions and responds appropriately. The emotion recognition system, based on a deep neural network, learns six basic emotions: happiness, anger, disgust, fear, sadness, and surprise. First, a convolutional neural network (CNN) is used to extract visual features by learning on a large number of static images. Second, a long short-term memory (LSTM) recurrent neural network is used to determine the relationship between the transformation of facial expressions in image sequences and the six basic emotions. Third, CNN and LSTM are combined to exploit their advantages in the proposed model. Finally, the performance of the emotion recognition system is improved by using transfer learning, that is, by transferring knowledge of related but different problems. The performance of the proposed system is verified through leave-one-out cross-validation and compared with that of other models. The system is applied to a humanoid robot to demonstrate its practicability for improving the HRI. © 2013 IEEE.},
	journal = {IEEE Access},
	author = {Li, T.-H.S. and Kuo, P.-H. and Tsai, T.-N. and Luan, P.-C.},
	year = {2019},
	keywords = {1scopus},
	pages = {93998--94011},
	annote = {cited By 7},
	file = {Full Text:/Users/tid010/Zotero/storage/YW34K9ZN/Li et al. - 2019 - CNN and LSTM Based Facial Expression Analysis Mode.pdf:application/pdf},
}

@inproceedings{indrajit_development_2013,
	title = {Development of whole body motion imitation in humanoid robot},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890281300&doi=10.1109%2fQiR.2013.6632552&partnerID=40&md5=4b57014d1e71a5d720d9b58c75b5bc90},
	doi = {10.1109/QiR.2013.6632552},
	abstract = {To be able to interact with humans, robot are made to perform similar motion to human movement. Imitation Learning or often called Motion Capture is one of the humanoid robot control techniques with human as an actor and the robot as an agent who will imitate the movement of the actor. This method offers flexibility and ease to modify robot system. This paper explores the method of designing whole body imitation in Humanoid Robot. Robot motion is controlled by Joint Space Control with reference motion captured by natural human motion through Microsoft Kinect. The motions are also preserved in database for later used on robot motion generation and teaching as well. Finally, the effectiveness of the proposed method is illustrated by the experiment of imitating Indonesia Traditional Dances motion using humanoid robot with 18 DOF. © 2013 IEEE.},
	booktitle = {2013 {International} {Conference} on {Quality} in {Research}, {QiR} 2013 - {In} {Conjunction} with {ICCS} 2013: {The} 2nd {International} {Conference} on {Civic} {Space}},
	author = {Indrajit, W. and Muis, A.},
	year = {2013},
	keywords = {1scopus},
	pages = {138--141},
	annote = {cited By 6},
	file = {Full Text:/Users/tid010/Zotero/storage/FZ68JLJK/Indrajit and Muis - 2013 - Development of whole body motion imitation in huma.pdf:application/pdf},
}

@article{jarosz_detecting_2019,
	title = {Detecting gaze direction using robot-mounted and mobile-device cameras},
	volume = {20},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076427641&doi=10.7494%2fcsci.2019.20.4.3435&partnerID=40&md5=8914fe279cf10738ae0b2ca0865a484c},
	doi = {10.7494/csci.2019.20.4.3435},
	abstract = {Two common channels through which humans communicate are speech and gaze. Eye gaze is an important mode of communication: it allows people to better understand each others' intentions, desires, interests, and so on. The goal of this research is to develop a framework for gaze-triggered events that can be executed on a robot and mobile devices and allows experiments to be performed. We experimentally evaluate the framework and techniques for extracting gaze direction based on a robot-mounted camera or a mobile-device camera that are implemented in the framework. We investigate the impact of light on the accuracy of gaze estimation and also how the overall accuracy depends on user eye and head movements. Our research shows that light intensity is important and the placement of a light source is crucial. All of the robot-mounted gaze-detection modules we tested were found to be similar with regard to their accuracy. The framework we developed was tested in a human-robot interaction experiment involving a job-interview scenario. The exible structure of this scenario allowed us to test different components of the framework in varied real-world scenarios, which was very useful for progressing towards our long-term research goal of designing intuitive gaze-based interfaces for human-robot communication. © 2019 AGH University of Science and Technology Press.},
	number = {4},
	journal = {Computer Science},
	author = {Jarosz, M. and Nawrocki, P. and Placzkiewicz, L. and Sniezynski, B. and Zielinski, M. and Indurkhya, B.},
	year = {2019},
	keywords = {1scopus},
	pages = {455--476},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/JZ5K66MX/Jarosz et al. - 2019 - Detecting gaze direction using robot-mounted and m.pdf:application/pdf},
}

@inproceedings{csapo_multimodal_2012,
	title = {Multimodal conversational interaction with a humanoid robot},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874432668&doi=10.1109%2fCogInfoCom.2012.6421935&partnerID=40&md5=ac9b94a5349240439be4b830ccd2c63c},
	doi = {10.1109/CogInfoCom.2012.6421935},
	abstract = {The paper presents a multimodal conversational interaction system for the Nao humanoid robot. The system was developed at the 8th International Summer Workshop on Multimodal Interfaces, Metz, 2012. We implemented WikiTalk, an existing spoken dialogue system for open-domain conversations, on Nao. This greatly extended the robot's interaction capabilities by enabling Nao to talk about an unlimited range of topics. In addition to speech interaction, we developed a wide range of multimodal interactive behaviours by the robot, including face-tracking, nodding, communicative gesturing, proximity detection and tactile interrupts. We made video recordings of user interactions and used questionnaires to evaluate the system. We further extended the robot's capabilities by linking Nao with Kinect. © 2012 IEEE.},
	booktitle = {3rd {IEEE} {International} {Conference} on {Cognitive} {Infocommunications}, {CogInfoCom} 2012 - {Proceedings}},
	author = {Csapó, A. and Gilmartin, E. and Grizou, J. and Han, J. and Meena, R. and Anastasiou, D. and Jokinen, K. and Wilcock, G.},
	year = {2012},
	keywords = {1scopus},
	pages = {667--672},
	annote = {cited By 35},
	file = {Submitted Version:/Users/tid010/Zotero/storage/8R3SEQ34/Csapó et al. - 2012 - Multimodal conversational interaction with a human.pdf:application/pdf},
}

@article{wu_toward_2020,
	title = {Toward {Design} of a {Drip}-{Stand} {Patient} {Follower} {Robot}},
	volume = {2020},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082745126&doi=10.1155%2f2020%2f9080642&partnerID=40&md5=1b1465430bbf1cd3892f558727ea0171},
	doi = {10.1155/2020/9080642},
	abstract = {A person following robot is an application of service robotics that primarily focuses on human-robot interaction, for example, in security and health care. This paper explores some of the design and development challenges of a patient follower robot. Our motivation stemmed from common mobility challenges associated with patients holding on and pulling the medical drip stand. Unlike other designs for person following robots, the proposed design objectives need to preserve as much as patient privacy and operational challenges in the hospital environment. We placed a single camera closer to the ground, which can result in a narrower field of view to preserve patient privacy. Through a unique design of artificial markers placed on various hospital clothing, we have shown how the visual tracking algorithm can determine the spatial location of the patient with respect to the robot. The robot control algorithm is implemented in three parts: (a) patient detection; (b) distance estimation; and (c) trajectory controller. For patient detection, the proposed algorithm utilizes two complementary tools for target detection, namely, template matching and colour histogram comparison. We applied a pinhole camera model for the estimation of distance from the robot to the patient. We proposed a novel movement trajectory planner to maintain the dynamic tipping stability of the robot by adjusting the peak acceleration. The paper further demonstrates the practicality of the proposed design through several experimental case studies. © 2020 Zewen Wu and Shahram Payandeh.},
	journal = {Journal of Robotics},
	author = {Wu, Z. and Payandeh, S.},
	year = {2020},
	keywords = {1scopus, 1search1, 1include\_search1},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/CJAN9ATR/Wu and Payandeh - 2020 - Toward Design of a Drip-Stand Patient Follower Rob.pdf:application/pdf},
}

@article{vircikova_teach_2015,
	title = {Teach your robot how you want it to express emotions: {On} the personalized affective human-humanoid interaction},
	volume = {316},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032118207&doi=10.1007%2f978-3-319-10783-7_9&partnerID=40&md5=22af8b608d901d438a4c1910a06c47ef},
	doi = {10.1007/978-3-319-10783-7_9},
	abstract = {We believe that in order for robots to interact naturally with humans, they should be able to express affective behavior. This paper deals with the development of an affective model for social robotics in which the resulting robotic expressions adapt according to the human subjective preferences. We have developed a method which can be used by non-technical individuals to design the affective models of humanoid robots. Our vision of the future research is that the proposed personalization will be treated, from user’s perspective, as an empathic response of the machine. We see the major contribution of this unique approach especially in long-term human-robot relationships and it could ultimately lead to robots being accepted in a wider domain. © Springer International Publishing Switzerland 2015.},
	journal = {Advances in Intelligent Systems and Computing},
	author = {Virčíkova, M. and Sinčák, P.},
	year = {2015},
	keywords = {1scopus},
	pages = {81--92},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/JD8UQ25D/Virčíkova and Sinčák - 2015 - Teach your robot how you want it to express emotio.pdf:application/pdf},
}

@article{lam_real-time_2011,
	title = {A real-time vision-based framework for human-robot interaction},
	volume = {7066 LNCS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-81255193898&doi=10.1007%2f978-3-642-25191-7_25&partnerID=40&md5=8ff574abf0a0492a4fb685e677459d61},
	doi = {10.1007/978-3-642-25191-7_25},
	abstract = {Building human-friendly robots which are able to interact and cooperate with humans has been an active research field in recent years. A major challenge in this field is to develop robots that can interact and cooperate with humans by understanding human communication modalities. Nonetheless, human face is a dynamic object and has a high degree of variability in its appearance, which makes face detection a difficult problem. In this paper, we present a real-time vision-based framework to detect human face and analysis of the human face direction in window area to interact with robot. A cascade of feature detectors trained with boosting technique has been employed. Experimental results using servo motors connect to SD21 and PIC16F887A microcontroller; and the MIABOT Pro have validated our approach. Our future work is to build an intelligent wheelchair whose motion can be controlled by the user's face direction. © 2011 Springer-Verlag.},
	number = {PART 1},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Lam, M.C. and Prabuwono, A.S. and Arshad, H. and Chan, C.S.},
	year = {2011},
	keywords = {1scopus},
	pages = {257--267},
	annote = {cited By 3},
	file = {Full Text:/Users/tid010/Zotero/storage/P3KC4QNG/Lam et al. - 2011 - A real-time vision-based framework for human-robot.pdf:application/pdf},
}

@inproceedings{bilac_gaze_2017,
	title = {Gaze and filled pause detection for smooth human-robot conversations},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044476245&doi=10.1109%2fHUMANOIDS.2017.8246889&partnerID=40&md5=d3699cec63430dbe27481c10dc501004},
	doi = {10.1109/HUMANOIDS.2017.8246889},
	abstract = {Let the human speak! Interactive robots and voice interfaces such as Pepper, Amazon Alexa, and OK Google are becoming more and more popular, allowing for more natural interaction compared to screens or keyboards. One issue with voice interfaces is that they tend to require a 'robotic' flow of human speech. Humans must be careful to not produce disfluencies, such as hesitations or extended pauses between words. If they do, the agent may assume that the human has finished their speech turn, and interrupts them mid-Thought. Interactive robots often rely on the same limited dialogue technology built for speech interfaces. Yet humanoid robots have the potential to also use their vision systems to determine when the human has finished their speaking turn. In this paper, we introduce HOMAGE (Human-rObot Multimodal Audio and Gaze End-of-Turn), a multimodal turntaking system for conversational humanoid robots. We created a dataset of humans spontaneously hesitating when responding to a robot's open-ended questions such as, 'What was your favorite moment this year?'. Our analyses found that users produced both auditory filled pauses such as 'uhhh', as well as gaze away from the robot to keep their speaking turn. We then trained a machine learning system to detect the auditory filled pauses and integrated it along with gaze into the Pepper humanoid robot's real-Time dialog system. Experiments with 28 naive users revealed that adding auditory filled pause detection and gaze tracking significantly reduced robot interruptions. Furthermore, user turns were 2.1 times longer (without repetitions), suggesting that this strategy allows humans to express themselves more, toward less time pressure and better robot listeners. © 2017 IEEE.},
	booktitle = {{IEEE}-{RAS} {International} {Conference} on {Humanoid} {Robots}},
	author = {Bilac, M. and Chamoux, M. and Lim, A.},
	year = {2017},
	keywords = {1scopus},
	pages = {297--304},
	annote = {cited By 1},
	file = {Full Text:/Users/tid010/Zotero/storage/M3295KEW/Bilac et al. - 2017 - Gaze and filled pause detection for smooth human-r.pdf:application/pdf},
}

@article{aranda_friendly_2010,
	title = {Friendly human-machine interaction in an adapted robotized kitchen},
	volume = {6179 LNCS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954902636&doi=10.1007%2f978-3-642-14097-6_50&partnerID=40&md5=3010231c1bb97cfa8546e2eb024056aa},
	doi = {10.1007/978-3-642-14097-6_50},
	abstract = {The concept and design of a friendly human-machine interaction system for an adapted robotized kitchen is presented. The kitchen is conceived in a modular way in order to be adaptable to a great diversity in level and type of assistance needs. An interaction manager has been developed which assist the user to control the system actions dynamically according to the given orders and the present state of the environment. Real time enhanced perception of the scenario is achieved by means of a 3D computer vision system. The main goal of the present project is to provide this kitchen with the necessary intelligent behavior to be able to actuate efficiently by interpreting the users' will. © 2010 Springer-Verlag Berlin Heidelberg.},
	number = {PART 1},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Aranda, J. and Vinagre, M. and Martín, E.X. and Casamitjana, M. and Casals, A.},
	year = {2010},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {312--319},
	annote = {cited By 3},
	file = {Full Text:/Users/tid010/Zotero/storage/6PJRQKF7/Aranda et al. - 2010 - Friendly human-machine interaction in an adapted r.pdf:application/pdf},
}

@inproceedings{sriram_mobile_2019,
	title = {Mobile robot assistance for disabled and senior citizens using hand gestures},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081327211&doi=10.1109%2fPETPES47060.2019.9003821&partnerID=40&md5=9aeab1666e1e495630a4b0571fbe4731},
	doi = {10.1109/PETPES47060.2019.9003821},
	abstract = {Paralysis (paraplegia) and motor-impairments impact the autonomy of elder people while performing their tasks independently. To fill the gap between machines and humans, gesture plays a significant role. This work focuses on Human-Robot Interaction (HRI), designed for assistance of wheel chair bound people with the help of mobile robots. People will get their work done faster and effortlessly with the help of mobile robots. This paper explains the interaction of mobile robot based on user gestures and assists using a 2-DoF manipulator for pick and place of the objects. Gestures are recognized by camera at real-time and robot (Firebird V-LPC2148) is controlled wirelessly through XBee radio module interface in the given environment. © 2019 IEEE.},
	booktitle = {1st {International} {Conference} on {Power} {Electronics} {Applications} and {Technology} in {Present} {Energy} {Scenario}, {PETPES} 2019 - {Proceedings}},
	author = {Sriram, K.N.V. and Palaniswamy, S.},
	year = {2019},
	keywords = {1scopus, 1search1, 1include\_search1},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/KX6SRL87/Sriram and Palaniswamy - 2019 - Mobile robot assistance for disabled and senior ci.pdf:application/pdf},
}

@inproceedings{meghdari_real-time_2017,
	title = {The real-time facial imitation by a social humanoid robot},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017504593&doi=10.1109%2fICRoM.2016.7886797&partnerID=40&md5=2dd2199aa0b3509d4e0a494cbb384958},
	doi = {10.1109/ICRoM.2016.7886797},
	abstract = {Facial expression imitation with applications in the design of human robot interaction (HRI) systems is an active area of research. In this study, we propose an approach for real-time imitation of human facial expression by a humanoid social robot 'Alice'. Artificial neural network (ANN) and Kinect sensor are used for recognition and classifying of the facial expressions like happiness, sadness, fear, anger and surprise; with the Alice humanoid robot imitating the comprehended expressions. Results and experiments demonstrate the effectiveness of the approach. © 2016 IEEE.},
	booktitle = {4th {RSI} {International} {Conference} on {Robotics} and {Mechatronics}, {ICRoM} 2016},
	author = {Meghdari, A. and Shouraki, S.B. and Siamy, A. and Shariati, A.},
	year = {2017},
	keywords = {1scopus},
	pages = {524--529},
	annote = {cited By 9},
	file = {Full Text:/Users/tid010/Zotero/storage/CL48SJ6G/Meghdari et al. - 2017 - The real-time facial imitation by a social humanoi.pdf:application/pdf},
}

@article{sisbot_synthesizing_2010,
	title = {Synthesizing robot motions adapted to human presence: {A} planning and control framework for safe and socially acceptable robot motions},
	volume = {2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857639464&doi=10.1007%2fs12369-010-0059-6&partnerID=40&md5=f56f958bd1e48f0e9c978eae43eff736},
	doi = {10.1007/s12369-010-0059-6},
	abstract = {With robotics hardware becoming more and more safe and compliant, robots are not far from entering our homes. The robot, that will share the same environment with humans, will be expected to consider the geometry of the interaction and to perform intelligent space sharing.In this case, even the simplest tasks, e.g. handing over an object to a person, raise important questions such as: where the task should be achieved?; how to place the robot relatively to the human in order to ease the human action?; how to hand over an object?; and more generally, how to move in a relatively constrained environment in the presence of humans?In this paper we present an integrated motion synthesis framework from planning to execution that is especially designed for a robot that interacts with humans. This framework, composed of Perspective Placement, Human Aware Manipulation Planner and Soft Motion Trajectory Planner, generates robot motions by taking into account human's safety; his vision field and his perspective; his kinematics and his posture along with the task constraints. © Springer Science \& Business Media BV 2010.},
	number = {3},
	journal = {International Journal of Social Robotics},
	author = {Sisbot, E.A. and Marin-Urias, L.F. and Broquère, X. and Sidobre, D. and Alami, R.},
	year = {2010},
	keywords = {1scopus},
	pages = {329--343},
	annote = {cited By 79},
	file = {Full Text:/Users/tid010/Zotero/storage/FFP44GRF/Sisbot et al. - 2010 - Synthesizing robot motions adapted to human presen.pdf:application/pdf},
}

@inproceedings{manigandan_wireless_2010,
	title = {Wireless vision based mobile robot control using hand gesture recognition through perceptual color space},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956149313&doi=10.1109%2fACE.2010.69&partnerID=40&md5=303a421221d0cbb9df3409e13a64755d},
	doi = {10.1109/ACE.2010.69},
	abstract = {In this paper we have implemented a wireless vision based mobile robot control through hand gesture recognition based on perceptual color space such as HIS, HSV/HSB, HSL. Vision-based hand gesture recognition is an important problem in the field of human-computer interaction, since hand motions and gestures could potentially be used to interact with computers in more natural ways. The robot control was done purely based on the orientation histograms a simple and fast algorithm on the system which would recognize static hand gestures with HSV color spaces as major parameters. The wireless based mobile robot system using hand gestures is a new innovative user interface that resolves the complications of using numerous remote controls for various applications. Based on one unified set of hand gestures, this system interprets the user hand gestures into pre-defined commands to control the remote robot. The experimental results are very encouraging as the system produces real-time responses and highly accurate recognition towards various gestures under different lighting conditions. © 2010 IEEE.},
	booktitle = {{ACE} 2010 - 2010 {International} {Conference} on {Advances} in {Computer} {Engineering}},
	author = {Manigandan, M. and Jackin, I.M.},
	year = {2010},
	keywords = {1scopus},
	pages = {95--99},
	annote = {cited By 25},
	file = {Full Text:/Users/tid010/Zotero/storage/XN5VL45M/Manigandan and Jackin - 2010 - Wireless vision based mobile robot control using h.pdf:application/pdf},
}

@inproceedings{gong_research_2018,
	title = {Research on human-robot interaction {Security} {Strategy} of {Movement} authorization for service robot {Based} on people's attention monitoring},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059072005&doi=10.1109%2fIISR.2018.8535908&partnerID=40&md5=71d6d2acb03a897105ea31438836675c},
	doi = {10.1109/IISR.2018.8535908},
	abstract = {This paper puts forward a kind of movement behavior planning method of service robot based on the responsibility analysis of accident responsibility and designs a set of security interactive strategy based on the human's attention to robot's action. In order to effectively avoid the human physical damage caused by the robot movement without realizing the dangerous behavior of the robot, we proposed a detecting method of people's awareness. The strategy in this paper adopts the method of real time on-line detection of human activity based on IMU, measuring the static attitude of human, at the mean time we use computer vision to evaluate people's attention to robot's behavior and carry out safety warning. Through strategic choice, it can stop the movement in a warning state and apply to a person for their authorization to effectively avoid harm people. © 2018 IEEE.},
	booktitle = {2018 {International} {Conference} on {Intelligence} and {Safety} for {Robotics}, {ISR} 2018},
	author = {Gong, J. and Wang, H. and Lu, Z. and Feng, N. and Hu, F.},
	year = {2018},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {521--526},
	annote = {cited By 2},
	file = {Full Text:/Users/tid010/Zotero/storage/7Q2WSPR2/Gong et al. - 2018 - Research on human-robot interaction Security Strat.pdf:application/pdf},
}

@article{alvarez-santos_feature_2012,
	title = {Feature analysis for human recognition and discrimination: {Application} to a person-following behaviour in a mobile robot},
	volume = {60},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862696354&doi=10.1016%2fj.robot.2012.05.014&partnerID=40&md5=8e76dd61cff2c85e63d780118fbacef6},
	doi = {10.1016/j.robot.2012.05.014},
	abstract = {One of the most important abilities that personal robots need when interacting with humans is the ability to discriminate amongst them. In this paper, we carry out an in-depth study of the possibilities of a colour camera placed on top of a robot to discriminate between humans, and thus get a reliable person-following behaviour on the robot. In particular we have reviewed and analysed the possibility of using the most popular colour and texture features used in object and texture recognition, to identify and model the target (person being followed). Nevertheless, the real-time restrictions make necessary the selection of a reduced subset of these features to reduce the computational burden. This subset of features was selected after carrying out a redundancy analysis, and considering how these features perform when discriminating amongst similar human torsos. Finally, we also describe several scoring functions able to dynamically adjust the relevance of each feature considering the particular conditions of the environment where the robot moves, together with the characteristics of the clothes worn by the persons that are in the scene. The results of this in-depth study have been implemented in a novel and adaptive system (described in this paper), which is able to discriminate between humans to get reliable person-following behaviours in a mobile robot. The performance of our proposal is clearly shown through a set of experimental results obtained with a real robot working in real and difficult scenarios. © 2012 Elsevier B.V. All rights reserved.},
	number = {8},
	journal = {Robotics and Autonomous Systems},
	author = {Alvarez-Santos, V. and Pardo, X.M. and Iglesias, R. and Canedo-Rodriguez, A. and Regueiro, C.V.},
	year = {2012},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {1021--1036},
	annote = {cited By 40},
}

@article{zhu_robust_2017,
	title = {Robust {Regression}-{Based} {Motion} {Perception} for {Online} {Imitation} on {Humanoid} {Robot}},
	volume = {9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035104475&doi=10.1007%2fs12369-017-0416-9&partnerID=40&md5=f982e8e5c7774b75b96ac2d49c0a9537},
	doi = {10.1007/s12369-017-0416-9},
	abstract = {Kinect is frequently used as a capture device for perceiving human motion in human–robot interaction. However, the Kinect’s principle of capture makes it possible for outliers to be present in the raw 3D joint position data, yielding an unsatisfying motion imitation by a humanoid robot. To eliminate these outliers and improve the precision of motion perception, we are inspired from the principle of signal restoration and propose a robust regression-based refining algorithm. We made contributions mainly in designing an Arc Tangent Square function to estimate the tendency of motion trajectories, and constructing a stepwise robust regression strategy to successively refine the outliers hidden in the motion capture data. The motion trajectories refined by the proposed algorithm are 40, 10, and 30\% better than the raw motion capture data on spatial similarity, temporal similarity, and smoothness, respectively. In the online implementation on a humanoid robot NAO, the imitated motions of the human’s upper limbs are synchronous and accurate. The proposed robust regression-based refining algorithm realizes high-performance motion perception for online imitation of the humanoid robot. © 2017, Springer Science+Business Media B.V.},
	number = {5},
	journal = {International Journal of Social Robotics},
	author = {Zhu, T. and Zhao, Q. and Wan, W. and Xia, Z.},
	year = {2017},
	keywords = {1scopus},
	pages = {705--725},
	annote = {cited By 2},
	file = {Full Text:/Users/tid010/Zotero/storage/KPUFQNB6/Zhu et al. - 2017 - Robust Regression-Based Motion Perception for Onli.pdf:application/pdf},
}

@inproceedings{yang_study_2013,
	title = {A study of the human-robot synchronous control system based on skeletal tracking technology},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898827105&doi=10.1109%2fROBIO.2013.6739794&partnerID=40&md5=03916c04d48ac2d9b5c64cea136464c9},
	doi = {10.1109/ROBIO.2013.6739794},
	abstract = {The purpose of this study is to develop a new system to synchronously control the simulated robot and humanoid robot based on the skeletal tracking technology. This system is a human-robot interaction system. Human can control the robot through the skeletal tracking technology without placing any marker on the subject's body. We use the Microsoft Kinect to get the skeleton framework information of the subject and input the data into the program. Therefore, the experiment has no demand on color to distinguish the subject's motions, and the subject can just perform in the front of the Kinect without wearing any control device. After processing the data, the computer controls the simulated robot to regenerate the subject's motions synchronously. In this study, we can control the simulated robot to do three different exercises. To verify the validity of the control system, we did a series of experiments to control a humanoid robot NAO. The experimental results show that the synchronous control system can distinguish the movements of human upper limb efficiently, and the cost is lower than that of the previous studies. This system controls both robot model and humanoid robot NAO based on human motion to achieve the human-machine interaction efficiently and the success rate is more than 90\%. © 2013 IEEE.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Robotics} and {Biomimetics}, {ROBIO} 2013},
	author = {Yang, N. and Duan, F. and Wei, Y. and Liu, C. and Tan, J.T.C. and Xu, B. and Zhang, J.},
	year = {2013},
	keywords = {1scopus},
	pages = {2191--2196},
	annote = {cited By 15},
	file = {Full Text:/Users/tid010/Zotero/storage/XHCJGYVP/Yang et al. - 2013 - A study of the human-robot synchronous control sys.pdf:application/pdf},
}

@inproceedings{cazzato_real-time_2019,
	title = {Real-time human head imitation for humanoid robots},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076607664&doi=10.1145%2f3348488.3348501&partnerID=40&md5=a7f56456fa64020d5fbd206a9f96c772},
	doi = {10.1145/3348488.3348501},
	abstract = {The ability of the robots to imitate human movements has been an active research study since the dawn of the robotics. Obtaining a realistic imitation is essential in terms of perceived quality in human-robot interaction, but it is still a challenge due to the lack of effective mapping between human movements and the degrees of freedom of robotics systems. If high-level programming interfaces, software and simulation tools simplified robot programming, there is still a strong gap between robot control and natural user interfaces. In this paper, a system to reproduce on a robot the head movements of a user in the field of view of a consumer camera is presented. The system recognizes the presence of a user and its head pose in real-time by using a deep neural network, in order to extract head position angles and to command the robot head movements consequently, obtaining a realistic imitation. At the same time, the system represents a natural user interface to control the Aldebaran NAO and Pepper humanoid robots with the head movements, with applications in human-robot interaction. © 2019 Association for Computing Machinery.},
	booktitle = {{ACM} {International} {Conference} {Proceeding} {Series}},
	author = {Cazzato, D. and Cimarelli, C. and Sanchez-Lopez, J.L. and Olivares-Mendez, M.A. and Voos, H.},
	year = {2019},
	keywords = {1scopus},
	pages = {65--69},
	annote = {cited By 1},
	file = {Submitted Version:/Users/tid010/Zotero/storage/RKG95MTJ/Cazzato et al. - 2019 - Real-time human head imitation for humanoid robots.pdf:application/pdf},
}

@inproceedings{bayram_audio-visual_2016,
	title = {Audio-visual multi-person tracking for active robot perception},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963716023&doi=10.1109%2fSII.2015.7405043&partnerID=40&md5=a5bf99ee6a5c76b064398c99e906dae9},
	doi = {10.1109/SII.2015.7405043},
	abstract = {Integration of signals acquired by the sensors of different modalities is known to enhance robot perception. This work presents a multimodal system consisting of audition, vision and motion modalities for human tracking with navigational tasks in a real environment having obstacles and noises affecting robot perception. The main goal of the system is to track the human who interacts with the robot. In the audition modality, multiple signal classification based on generalized eigenvalue decomposition method is utilized for sound source localization and the complementary vision modality incorporates face detection including a method combining feature-based and color-based techniques and tracking facilities to provide continuity in tracking the human. The other perceptive ability of the robot is to detect obstacles while tracking and approaching the human and to avoid them without losing the target. The aim of the successive motion modality is to enable a robot having intelligent and human-like behaviors by using the output from the sensor fusion framework and obstacle detection. To evaluate the performances of robot perception and behaviors, a mobile robot is utilized in both single and multi-person experiments in a real environment with obstacles. © 2015 IEEE.},
	booktitle = {2015 {IEEE}/{SICE} {International} {Symposium} on {System} {Integration}, {SII} 2015},
	author = {Bayram, B. and Ince, G.},
	year = {2016},
	keywords = {1scopus},
	pages = {575--580},
	annote = {cited By 1},
	file = {Bayram and Ince - 2016 - Audio-visual multi-person tracking for active robo.pdf:/Users/tid010/Zotero/storage/KTS6AMK5/Bayram and Ince - 2016 - Audio-visual multi-person tracking for active robo.pdf:application/pdf},
}

@article{gorer_autonomous_2017,
	title = {An autonomous robotic exercise tutor for elderly people},
	volume = {41},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979584462&doi=10.1007%2fs10514-016-9598-5&partnerID=40&md5=33634c98067f624459debe8aa2dec3aa},
	doi = {10.1007/s10514-016-9598-5},
	abstract = {Ambient assisted living proposes to utilize technological solutions to sustain the well being of elderly people. In accordance with the vision of successful aging, we describe in this study an autonomous robotic exercise tutor for elderly people. The robot learns a set of physical exercises from a human demonstrator in an imitation framework, and performs these motions in an exercise scenario, while monitoring the elderly person to provide verbal feedback. We developed an exercise program in collaboration with a nursing home, and tested our system in a real world scenario with visitors of a day care center, over multiple sessions. We provide a detailed description of the system implementation, as well as our observations for the exercise program. For the study held in the day care center, video annotations and user self-assessments are evaluated to measure the overall performance of the system and to validate our approach. The analysis revealed that elderly people can successfully exercise with the assistance of the robot, while staying engaged with the system over multiple sessions. © 2016, Springer Science+Business Media New York.},
	number = {3},
	journal = {Autonomous Robots},
	author = {Görer, B. and Salah, A.A. and Akın, H.L.},
	year = {2017},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {657--678},
	annote = {cited By 28},
	file = {Full Text:/Users/tid010/Zotero/storage/RW6CKXB8/Görer et al. - 2017 - An autonomous robotic exercise tutor for elderly p.pdf:application/pdf},
}

@article{ferraguti_safety_2020,
	title = {Safety barrier functions and multi-camera tracking for human–robot shared environment},
	volume = {124},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075861338&doi=10.1016%2fj.robot.2019.103388&partnerID=40&md5=a2b9823c11fe19f2394a30586751edb2},
	doi = {10.1016/j.robot.2019.103388},
	abstract = {A new vision in human–robot collaboration has allowed to place robots nearby human operators, working close to each other in industrial environments. As a consequence, human safety has become a dominant issue, together with production efficiency. In this paper we propose an optimization-based control algorithm that allows robots to avoid obstacles (like human operators) while minimizing the difference between the nominal acceleration input and the commanded one. Control Barrier Functions are exploited to build safety barriers around each robot link, to guarantee collision-free trajectories along the whole robot body. Human accelerations and velocities are computed by means of a bank of Kalman filters. To solve obstruction problems, two RGB-D cameras are used and the measured skeleton data are processed and merged using the mentioned bank of Kalman filters. The algorithm is implemented on an Universal Robots UR5 in order to validate the proposed approach. © 2019 Elsevier B.V.},
	journal = {Robotics and Autonomous Systems},
	author = {Ferraguti, F. and Talignani Landi, C. and Costi, S. and Bonfè, M. and Farsoni, S. and Secchi, C. and Fantuzzi, C.},
	year = {2020},
	keywords = {1scopus},
	annote = {cited By 3},
	file = {Ferraguti et al. - 2020 - Safety barrier functions and multi-camera tracking.pdf:/Users/tid010/Zotero/storage/7R8I75UT/Ferraguti et al. - 2020 - Safety barrier functions and multi-camera tracking.pdf:application/pdf},
}

@article{ju_integrative_2017,
	title = {An integrative framework of human hand gesture segmentation for human-robot interaction},
	volume = {11},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030484444&doi=10.1109%2fJSYST.2015.2468231&partnerID=40&md5=d329cfafcfb133ea5b8bd3144b29b56e},
	doi = {10.1109/JSYST.2015.2468231},
	abstract = {This paper proposes a novel framework to segment hand gestures in RGB-depth (RGB-D) images captured by Kinect using humanlike approaches for human-robot interaction. The goal is to reduce the error of Kinect sensing and, consequently, to improve the precision of hand gesture segmentation for robot NAO. The proposed framework consists of two main novel approaches. First, the depth map and RGB image are aligned by using the genetic algorithm to estimate key points, and the alignment is robust to uncertainties of the extracted point numbers. Then, a novel approach is proposed to refine the edge of the tracked hand gestures in RGB images by applying a modified expectation-maximization (EM) algorithm based on Bayesian networks. The experimental results demonstrate that the proposed alignment method is capable of precisely matching the depth maps with RGB images, and the EM algorithm further effectively adjusts the RGB edges of the segmented hand gestures. The proposed framework has been integrated and validated in a system of human-robot interaction to improve NAO robot's performance of understanding and interpretation. © 2007-2012 IEEE.},
	number = {3},
	journal = {IEEE Systems Journal},
	author = {Ju, Z. and Ji, X. and Li, J. and Liu, H.},
	year = {2017},
	keywords = {1scopus},
	pages = {1326--1336},
	annote = {cited By 26},
	file = {Submitted Version:/Users/tid010/Zotero/storage/BCRUAHB8/Ju et al. - 2017 - An integrative framework of human hand gesture seg.pdf:application/pdf},
}

@article{ferrer_robot_2013,
	title = {Robot {Interactive} {Learning} through {Human} {Assistance}},
	volume = {48},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885609111&doi=10.1007%2f978-3-642-35932-3_11&partnerID=40&md5=9b3df414eeacabc8c8be00dd912f5cab},
	doi = {10.1007/978-3-642-35932-3_11},
	abstract = {This chapter presents some real-life examples using the interactive multimodal framework; in this work, the robot is capable of learning through human assistance. The basic idea is to use the human feedback to improve the learning behavior of the robot when it deals with human beings.We show two different prototypes that have been developed for the following topics: interactive motion learning for robot companion; and on-line face learning using robot vision. On the one hand, the objective of the first prototype is to learn how a robot has to approach to a pedestrian who is going to a destination, minimizing the disturbances to the expected person's path. On the other hand, the objectives of the second prototype are twofold, first, the robot invites a person to approach the robot to initiate a dialogue, and second, the robot learns the face of the person that is invited for a dialogue. The two prototypes have been tested in real-life conditions and the results are very promising. © Springer-Verlag Berlin Heidelberg 2013.},
	journal = {Intelligent Systems Reference Library},
	author = {Ferrer, G. and Garrell, A. and Villamizar, M. and Huerta, I. and Sanfeliu, A.},
	year = {2013},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {185--203},
	annote = {cited By 8},
	file = {Submitted Version:/Users/tid010/Zotero/storage/Q2BTQ62X/Ferrer et al. - 2013 - Robot Interactive Learning through Human Assistanc.pdf:application/pdf},
}

@article{zhang_interactive_2018,
	title = {An interactive control system for mobile robot based on cloud services},
	volume = {30},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055119655&doi=10.1002%2fcpe.4983&partnerID=40&md5=b790e9092a5e7c4efee558ef31b381d5},
	doi = {10.1002/cpe.4983},
	abstract = {Visual interaction is one of the most important ways for mobile robots to serve the elderly in the future. Mobile robots need to recognize human movements to follow human movements or evade actions. An indoor mobile robot interaction system based on cloud servers is presented. In a human-machine coexistence environment, the mobile robot recognizes human motion, which represents the control intention for robot. In addition, if the person does not issue a control command, the robot follows the operator moving trajectory as a follower. A mobile robot with four Mecanum wheels is used in experiments. In the robot, a 3D camera is used to capture human motion and recognizes the language control commands. Then, the mobile robot executes the command action after translating the human body language to its motion direction control mode. Moreover, a 2D camera in the robot is used to autonomously capture 2D human multi-view images and combines facial recognition to achieve specific human target to follow-up. Some experiments are presented to verify the effectiveness of the whole Interaction control system. © 2018 John Wiley \& Sons, Ltd.},
	number = {24},
	journal = {Concurrency Computation},
	author = {Zhang, L. and Zhang, K.},
	year = {2018},
	keywords = {1scopus},
	annote = {cited By 4},
	file = {Full Text:/Users/tid010/Zotero/storage/3Z45C65R/Zhang and Zhang - 2018 - An interactive control system for mobile robot bas.pdf:application/pdf},
}

@inproceedings{yun_robotic_2013,
	title = {Robotic person-tracking with modified multiple instance learning},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889562018&doi=10.1109%2fROMAN.2013.6628445&partnerID=40&md5=7aba09aedf8a42e99b43d7c91db06c70},
	doi = {10.1109/ROMAN.2013.6628445},
	abstract = {Robotic person-following is an essential component for natural human robot interaction. To follow a person, the robot should track the target person robustly and in real time. Object tracking algorithms in the computer vision field typically require abundant features and heavy computing power, and thus cannot be directly applied to person-following robots due to the problems arising in practical robotic environments. This paper proposes a robotic person-tracking algorithm based on modified multiple instance learning. In order to resolve the problems raised by the rearward view of the target person, the tracker is modified to be guided by color histogram back-projection. Additionally, the search area model is modified from circle to ellipse and the number of features is reduced so that the tracker should adapt the robotic environment in real-time. The algorithm is validated through system integration and experiments. © 2013 IEEE.},
	booktitle = {Proceedings - {IEEE} {International} {Workshop} on {Robot} and {Human} {Interactive} {Communication}},
	author = {Yun, W.-H. and Cho, Y.-J. and Kim, D. and Lee, J. and Yoon, H. and Kim, J.},
	year = {2013},
	keywords = {1scopus},
	pages = {198--203},
	annote = {cited By 2},
	file = {Full Text:/Users/tid010/Zotero/storage/BDM9AR4U/Yun et al. - 2013 - Robotic person-tracking with modified multiple ins.pdf:application/pdf},
}

@inproceedings{ratul_gesture_2016,
	title = {Gesture based wireless shadow robot},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007232619&doi=10.1109%2fICIEV.2016.7760024&partnerID=40&md5=f8c0781b18832117855f93e10eb75322},
	doi = {10.1109/ICIEV.2016.7760024},
	abstract = {In this project, an application of gesture mimic human-robot interaction is presented which has been developed using a Kinect sensor. The gesture recognition method combines depth information and traditional joints tracking algorithm using kinect to employ skeleton angular motion in dynamic gesture mimicking. A Master/Slave structured robot control application for master Bluetooth in PC and paired with slave module for the Arduino receiver is developed. The wireless setup provides a favorable function for gesture commands that uses Kinect sensor for controlling of shadow robot. Training session has been utilized for storing the serial string array sets in a Local database of the application which can later be transmitted to the slave at the time it is commanded. A Non-feedback static walking sequence is also been developed for bipedal forward walking of the humanoid robot. Experiment results validate the feasibility and usefulness of the application system. © 2016 IEEE.},
	booktitle = {2016 5th {International} {Conference} on {Informatics}, {Electronics} and {Vision}, {ICIEV} 2016},
	author = {Ratul, A.U. and Ali, M.T. and Ahasan, R.},
	year = {2016},
	keywords = {1scopus},
	pages = {351--355},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/9FJR6CTB/Ratul et al. - 2016 - Gesture based wireless shadow robot.pdf:application/pdf},
}

@inproceedings{yang_real-time_2015,
	title = {Real-time human-robot interaction in complex environment using kinect v2 image recognition},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960856552&doi=10.1109%2fICCIS.2015.7274606&partnerID=40&md5=ab2700f66d6a825a7f083e8d0204ecdc},
	doi = {10.1109/ICCIS.2015.7274606},
	abstract = {This paper presents real-time interaction between 7-DOF KUKA robotic arm and any untrained human operator using Kinect V2. Kinect sensor is utilized to detect human body joints and mono color object to be grasped, using HSL-XYZ algorithm. By moving hand holding simple object such as a ball, operator can make KUKA robot follow the expected trajectory and fulfill pass-and-place tasks. One hand of operator is followed by KUKA, while the pose of the other arm commands the gripper to move, grasp, release and place. Experiments proved our advantages that Human-Robot Interaction is more robust, easier and more intuitive for human, with lower requirements for sensor; and is a novel solution for industries and life. Client - server application using UDP protocol is performed to transmit and receive real-time control and feedback data. © 2015 IEEE.},
	booktitle = {Proceedings of the 2015 7th {IEEE} {International} {Conference} on {Cybernetics} and {Intelligent} {Systems}, {CIS} 2015 and {Robotics}, {Automation} and {Mechatronics}, {RAM} 2015},
	author = {Yang, Y. and Yan, H. and Dehghan, M. and Ang, M.H.},
	year = {2015},
	keywords = {1scopus},
	pages = {112--117},
	annote = {cited By 8},
	file = {Full Text:/Users/tid010/Zotero/storage/9YL94VTE/Yang et al. - 2015 - Real-time human-robot interaction in complex envir.pdf:application/pdf},
}

@inproceedings{lima_real-time_2019,
	title = {Real-time hand pose tracking and classification for natural human-robot control},
	volume = {5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068209535&doi=10.5220%2f0007384608320839&partnerID=40&md5=19be6f3b0b9db770a4f98d0a2cb816c9},
	doi = {10.5220/0007384608320839},
	abstract = {We present a human-robot natural interaction approach based on teleoperation through body gestures. More specifically, we propose an interface where the user can use his hand to intuitively control the position and status (open/closed) of a robotic arm gripper. In this work, we employ a 6-DOF (six degrees-of-freedom) industrial manipulator which mimics user movements in real-time, positioning the end effector as if the individual was looking into a mirror, entailing a natural and intuitive interface. The controlling hand of the user is tracked using body skeletons acquired from a Microsoft Kinect sensor, while a Convolutional Neural Network recognizes whether the hand is opened or closed using depth data. The network was trained on hand images collected from several individuals, in different orientations, resulting in a robust classifier that performs well regardless of user location or orientation. There is no need for wearable devices, such as gloves or wristbands. We present results of experiments that reveal high performance of the proposed approach to recognize both the user hand position and its status (open/closed); and experiments to demonstrate the robustness and applicability of the proposed approach to industrial tasks. Copyright © 2019 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved},
	booktitle = {{VISIGRAPP} 2019 - {Proceedings} of the 14th {International} {Joint} {Conference} on {Computer} {Vision}, {Imaging} and {Computer} {Graphics} {Theory} and {Applications}},
	author = {Lima, B. and Júnior, G.L.N. and Amaral, L. and Vieira, T. and Ferreira, B. and Vieira, T.},
	year = {2019},
	keywords = {1scopus},
	pages = {832--839},
	annote = {cited By 0},
	file = {Lima et al. - 2019 - Real-time hand pose tracking and classification fo.pdf:/Users/tid010/Zotero/storage/HAF4EPWZ/Lima et al. - 2019 - Real-time hand pose tracking and classification fo.pdf:application/pdf},
}

@article{li_inferring_2019,
	title = {Inferring user intent to interact with a public service robot using bimodal information analysis},
	volume = {33},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063883693&doi=10.1080%2f01691864.2019.1599727&partnerID=40&md5=22781757ddca8e1fc24f75cccf3fd856},
	doi = {10.1080/01691864.2019.1599727},
	abstract = {Achieving polite service with a public service robot requires it to proactively ascertain who will interact with it in human-populated environments. Enlightened by interactive inference of intentions among humans, we investigate a novel and practical interactive intention-predicting method for people using bimodal information analysis for a public service robot. Different from the traditional research, only the visual cues are used to analyze the user's attention, this method combines the RGB-D camera and laser information to perceive the user, which realizes the 360-degree range perception, and compensates for the lack of perspective using the RGB-D camera. In addition, seven kinds of interactive intent features were extracted, and a random forest regression model was trained to score the interaction intentions of the people in the field of view. Considering the inference order of two different sensors, a priority rule for intention inference is also designed. The algorithm is implemented into a robot operation system (ROS) and evaluated on our public service robot. Extensive experimental results illustrate that the proposed method enables public service robots to achieve a higher level of politeness than the traditional, passive interactivity approach in which robots wait for commands from users. © 2019, © 2019 Informa UK Limited, trading as Taylor \& Francis Group and The Robotics Society of Japan.},
	number = {7-8},
	journal = {Advanced Robotics},
	author = {Li, K. and Sun, S. and Zhao, X. and Wu, J. and Tan, M.},
	year = {2019},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {369--387},
	annote = {cited By 2},
	file = {Full Text:/Users/tid010/Zotero/storage/2LA533JS/Li et al. - 2019 - Inferring user intent to interact with a public se.pdf:application/pdf},
}

@inproceedings{abidi_human_2013,
	title = {Human pointing as a robot directive},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875757975&doi=10.1109%2fHRI.2013.6483504&partnerID=40&md5=fffa220113c091f230e8897e2d92dfa1},
	doi = {10.1109/HRI.2013.6483504},
	abstract = {People are accustomed to directing other people's attention using pointing gestures. People enact and interpret pointing commands often and effortlessly. If robots understand human intentions (e.g. as encoded in pointing-gestures), they can reach higher levels of engagement with people. This paper explores methods that robots can use to allow people to direct them to move to a specific location, using an inexpensive Kinect sensor. The joint positions of the pointing human's right arm and hand in 3D-space are extracted and used by the robot to identify the direction of user's pointing gesture. We evaluated the proposed approach on a PR2 robot whose task was to move to a location that a human pointed to on the ground. This method enables the robot to follow human pointing gestures on the fly and in real-time. It will be deployed on a PR2 in the wild in a new building environment where the robot will be expected to interact with people and interpret their human pointing behaviors. © 2013 IEEE.},
	booktitle = {{ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	author = {Abidi, S. and Williams, M. and Johnston, B.},
	year = {2013},
	keywords = {1scopus},
	pages = {67--68},
	annote = {cited By 14},
	file = {Full Text:/Users/tid010/Zotero/storage/HSIA25QF/Abidi et al. - 2013 - Human pointing as a robot directive.pdf:application/pdf},
}

@article{moreno_path_2016,
	title = {Path optimization planning for human-robot interaction},
	volume = {11},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002736320&partnerID=40&md5=f881e78233bc762de3474a62d985513c},
	abstract = {This article presents the development of a human-machine interaction system which sets an optimization algorithm for collaborative work between a person and a robotic arm, so that the person can enter the workspace of the robot and, the latter, anchor their displacement relative to the path that follows and with respect to the movement of the person by means of a machine vision system. The optimization algorithm corresponds to external penalty method determined by its variant of quadratic penalty, taking as restricting the location of the hand of the person plus a safe distance. A kinetic sensor is used as the acquisition system of the spatial location of the person to determine the distance between the person and the robotic arm. Interaction testing shows that the implemented algorithm achieved efficiently divert the path of the arm relative to the location of the person, avoiding collisions and ensuring the person integrity. © Research India Publications.},
	number = {22},
	journal = {International Journal of Applied Engineering Research},
	author = {Moreno, R.J. and Mauledoux, M. and Avilés, O.F.},
	year = {2016},
	keywords = {1scopus},
	pages = {10822--10827},
	annote = {cited By 1},
	file = {Moreno et al. - 2016 - Path optimization planning for human-robot interac.pdf:/Users/tid010/Zotero/storage/M2NJJCK6/Moreno et al. - 2016 - Path optimization planning for human-robot interac.pdf:application/pdf},
}

@inproceedings{igorevich_behavioral_2011,
	title = {Behavioral synchronization of human and humanoid robot},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857277771&doi=10.1109%2fURAI.2011.6145902&partnerID=40&md5=4cb986de6c8dc10afad4916fa6325a41},
	doi = {10.1109/URAI.2011.6145902},
	abstract = {This paper explores interaction design and real time implementation of behavioral synchronization of Human and Humanoid robot. Human postures are captured into a 3D skeleton using Kinect sensor. Single 3D coordinates of joints can be derived from the skeleton. For the behavioral synchronization primitive humanoid robot with 16 servo motors are used. Human's joint coordinates are extracted from the standing posture. Using mechanism suggested in this paper, the joints information converted and mapped to the humanoid robot. The overall architecture was designed and implemented in expandable way. This gives opportunity to control humanoid robot locally as well as remotely. Suggested behavioral synchronization method from this work is implemented and validated. Time delay issues are also considered as a significant topic in instance implementation. Behavior synchronization of human and robot have totally 400 millisecond time delays, where 50\% is related to humanoid's internal latency. Additionally suggested architecture provides another mode, where user can use alternative controllers. © 2011 IEEE.},
	booktitle = {{URAI} 2011 - 2011 8th {International} {Conference} on {Ubiquitous} {Robots} and {Ambient} {Intelligence}},
	author = {Igorevich, R.R. and Ismoilovich, E.P. and Min, D.},
	year = {2011},
	keywords = {1scopus},
	pages = {655--660},
	annote = {cited By 12},
	file = {Full Text:/Users/tid010/Zotero/storage/YXKDN6NN/Igorevich et al. - 2011 - Behavioral synchronization of human and humanoid r.pdf:application/pdf},
}

@inproceedings{zhang_optimal_2016,
	title = {Optimal robot selection by gaze direction in multi-human multi-robot interaction},
	volume = {2016-November},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006474706&doi=10.1109%2fIROS.2016.7759745&partnerID=40&md5=0b36db74e565dc3878423d02c6ad1e82},
	doi = {10.1109/IROS.2016.7759745},
	abstract = {This paper presents a computer vision based system for interaction between multiple humans and multiple robots. Each human can "select" (obtain the undivided attention of) a robot by simply looking directly at it. This extends previous work whereby a single human can select one or more robots from a population. Each robot optimally assigns human identities to tracked faces in its camera view using a local Hungarian algorithm. The gaze-direction and location of the faces are estimated via vision, and a score for each robotface pair is assigned. Then the system finds the global optimal allocation of robot-to-human selections using a centralized Hungarian algorithm. A useful feature of this method is that robots can be selected by people they cannot see. This is the first demonstration of optimal many-to-many robot-selection HRI. © 2016 IEEE.},
	booktitle = {{IEEE} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	author = {Zhang, L. and Vaughan, R.},
	year = {2016},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {5077--5083},
	annote = {cited By 6},
	file = {Full Text:/Users/tid010/Zotero/storage/LCCLJ8LJ/Zhang and Vaughan - 2016 - Optimal robot selection by gaze direction in multi.pdf:application/pdf},
}

@inproceedings{tseng_multi-human_2014,
	title = {Multi-human spatial social pattern understanding for a multi-modal robot through nonverbal social signals},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937549717&doi=10.1109%2fROMAN.2014.6926307&partnerID=40&md5=2e3bf59f87b7c11105b89e782a20b2f4},
	doi = {10.1109/ROMAN.2014.6926307},
	abstract = {For service robots to be able to enter a multi-human office environment, it is important to find a group of human users' social patterns and then to provide a proper service to them in time. Usually, human users' social patterns are represented in terms of nonverbal social signals. In this paper, a new integrated approach on recognizing multi-human social signals is proposed. Specifically, the nonverbal social signals are detected by a laser range finder and a RGB-D camera and are processed to find the multi-human (spatial) social patterns. Those recognized patterns are then applied to human-to-human, human-to-robot or multi-human-to-robot interactive formation. Experimental results shows that our robot successfully recognizes the aforementioned users' social patterns followed by appropriate services. © 2014 IEEE.},
	booktitle = {{IEEE} {RO}-{MAN} 2014 - 23rd {IEEE} {International} {Symposium} on {Robot} and {Human} {Interactive} {Communication}: {Human}-{Robot} {Co}-{Existence}: {Adaptive} {Interfaces} and {Systems} for {Daily} {Life}, {Therapy}, {Assistance} and {Socially} {Engaging} {Interactions}},
	author = {Tseng, S.-H. and Hsu, Y.-H. and Chiang, Y.-S. and Wu, T.-Y. and Fu, L.-C.},
	year = {2014},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {531--536},
	annote = {cited By 3},
	file = {Full Text:/Users/tid010/Zotero/storage/7SHNFS5U/Tseng et al. - 2014 - Multi-human spatial social pattern understanding f.pdf:application/pdf},
}

@article{chen_approaches_2010,
	title = {Approaches to robotic vision control using image pointing recognition techniques},
	volume = {67 LNEE},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867368290&doi=10.1007%2f978-3-642-12990-2_36&partnerID=40&md5=6b09b8f96b27ce6fcf22499157a96e51},
	doi = {10.1007/978-3-642-12990-2_36},
	abstract = {Intelligent robot human-machine interactive technology will be incorporated into our daily lives and industrial production. This paper presents an autonomous mobile robot control system for human-machine interaction, The use of computer vision, 3D reconstruction of the two-dimensional image information of a specific coordinate point to point system. The use of a known point to the appearance characteristics of objects, by vision recognition algorithm, the original color image data for target screening and recognition. Allows users to easily through simple body movements, issuing commands to the robot. And by support vector machine(SVG) to classify non-linear non-separable type of data, accept user input and recognition actions to improve the robot vision system for target identification accuracy, and thus to achieve the goal of human-computer interaction. © 2010 Springer-Verlag Berlin Heidelberg.},
	journal = {Lecture Notes in Electrical Engineering},
	author = {Chen, T.-D.},
	year = {2010},
	keywords = {1scopus},
	pages = {321--328},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/N4H94RFU/Chen - 2010 - Approaches to robotic vision control using image p.pdf:application/pdf},
}

@inproceedings{zhang_human_2020,
	title = {Human {Motion} {Capture} {Based} on {Kinect} and {IMUs} and {Its} {Application} to {Human}-{Robot} {Collaboration}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092669471&doi=10.1109%2fICARM49381.2020.9195342&partnerID=40&md5=ccebe4c2e546439daf64a36d15249967},
	doi = {10.1109/ICARM49381.2020.9195342},
	abstract = {In this paper, a nonlinear optimization solver is proposed to establish the human upper body motion capture system, including the orientation of each bone joint and the global position of the hip bone. 5 IMUs and one Kinect are used, the rotation data and acceleration data of IMUs, Kinect skeleton position data and human pose prior are fused in a non-linear optimization way. The method proposed in this paper combines the characteristics of Kinect data with high accuracy and IMU data with stability, which can make up for the accuracy of human pose capture in dynamic, magnetically disturbed and occlusion conditions. In order to verify the practicability of the human motion capture system proposed in this paper, the system is firstly applied to real-time human motion capture, then a simple human-robot collaboration (HRC) case is implemented, which is very common in modern collaborative medical field. The experimental results show that the proposed human motion capture system can achieve stable human motion measurement, which can meet the needs of HRC scenarios. © 2020 IEEE.},
	booktitle = {{ICARM} 2020 - 2020 5th {IEEE} {International} {Conference} on {Advanced} {Robotics} and {Mechatronics}},
	author = {Zhang, J. and Li, P. and Zhu, T. and Zhang, W.-A. and Liu, S.},
	year = {2020},
	keywords = {1scopus},
	pages = {392--397},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/6RK8L37W/Zhang et al. - 2020 - Human Motion Capture Based on Kinect and IMUs and .pdf:application/pdf},
}

@article{munaro_fast_2014,
	title = {Fast {RGB}-{D} people tracking for service robots},
	volume = {37},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905730835&doi=10.1007%2fs10514-014-9385-0&partnerID=40&md5=3bfb89284c5369b00db0b9d12d2d5725},
	doi = {10.1007/s10514-014-9385-0},
	abstract = {Service robots have to robustly follow and interact with humans. In this paper, we propose a very fast multi-people tracking algorithm designed to be applied on mobile service robots. Our approach exploits RGB-D data and can run in real-time at very high frame rate on a standard laptop without the need for a GPU implementation. It also features a novel depth-based sub-clustering method which allows to detect people within groups or even standing near walls. Moreover, for limiting drifts and track ID switches, an online learning appearance classifier is proposed featuring a three-term joint likelihood. We compared the performances of our system with a number of state-of-the-art tracking algorithms on two public datasets acquired with three static Kinects and a moving stereo pair, respectively. In order to validate the 3D accuracy of our system, we created a new dataset in which RGB-D data are acquired by a moving robot. We made publicly available this dataset which is not only annotated by hand, but the ground-truth position of people and robot are acquired with a motion capture system in order to evaluate tracking accuracy and precision in 3D coordinates. Results of experiments on these datasets are presented, showing that, even without the need for a GPU, our approach achieves state-of-the-art accuracy and superior speed. © 2014 Springer Science+Business Media New York.},
	number = {3},
	journal = {Autonomous Robots},
	author = {Munaro, M. and Menegatti, E.},
	year = {2014},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {227--242},
	annote = {cited By 114},
	file = {Full Text:/Users/tid010/Zotero/storage/3HPDCAA9/Munaro and Menegatti - 2014 - Fast RGB-D people tracking for service robots.pdf:application/pdf},
}

@article{martin_estimation_2010,
	title = {Estimation of pointing poses for visually instructing mobile robots under real world conditions},
	volume = {58},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-75149153253&doi=10.1016%2fj.robot.2009.09.013&partnerID=40&md5=48661286b99c0a7217eadad98ad49d5b},
	doi = {10.1016/j.robot.2009.09.013},
	abstract = {In this paper, we present an approach for directing a mobile robot under real-world conditions into a target position by means of pointing poses only. Because one important objective of our work is the development of a low-cost platform, only monocular vision at web-cam level should be employed. Our previous approach presented in Gross et al. (2006) [1], Richarz et al. (2007) [2] has been improved by several additional processing steps. Finally, a background subtraction technique and a histogram equalization have been integrated in the preprocessing stage to be able to work in environments with structured backgrounds and under variable lighting conditions. Furthermore, a discriminant analysis was used to find the most relevant input features for the pointing pose estimator. The contribution of this paper is, however, not only the presentation of an approach to estimating pointing poses in a demanding real-world scenario on a mobile robot, but also the detailed and evaluative comparison between different image-preprocessing techniques, alternative feature extraction methods, and several function approximators with the same set of test- and training data. Reasonable combinations of the different methods are tested, and for each component on the processing chain the effect on the accuracy of the target estimation is quantized. The approach presented in this paper has been implemented on the mobile interaction robot Horos to determine the performance and estimation accuracy under real-world conditions. Furthermore, we compared the accuracy of our approach with that of humans performing the same estimation task, and achieved very comparable results for the best estimator. © 2009 Elsevier B.V. All rights reserved.},
	number = {2},
	journal = {Robotics and Autonomous Systems},
	author = {Martin, C. and Steege, F.-F. and Gross, H.-M.},
	year = {2010},
	keywords = {1scopus},
	pages = {174--185},
	annote = {cited By 8},
	file = {Martin et al. - 2010 - Estimation of pointing poses for visually instruct.pdf:/Users/tid010/Zotero/storage/7NYBSFFJ/Martin et al. - 2010 - Estimation of pointing poses for visually instruct.pdf:application/pdf},
}

@inproceedings{lambrecht_spatial_2012,
	title = {Spatial programming for industrial robots based on gestures and {Augmented} {Reality}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872297670&doi=10.1109%2fIROS.2012.6385900&partnerID=40&md5=e3b29ed396f66d15b9ad7c0cef5062d9},
	doi = {10.1109/IROS.2012.6385900},
	abstract = {The presented spatial programming system provides an assistance system for online programming of industrial robots. A handheld device and a motion tracking system establish the basis for a modular 3D programming approach corresponding to different phases of robot programming: definition, evaluation and adaption. Static and dynamic gestures enable the program definition of poses, trajectories and tasks. The spatial evaluation is done using an Augmented Reality application on a handheld device. Therefore, the programmer is able to move freely within the robot cell and define the program spatially through gestures. The camera image of the handheld is simultaneously enhanced by virtual objects representing the robot program. Based on 3D motion tracking of human movements and a mobile Augmented Reality application, we introduce a novel kind of interaction for the adaption of robot programs. The programmer is enabled to interact with virtual program components through bare-hand gestures. Such sample forms of interaction include translation and rotation applicable to poses, trajectories or tasks representations. Finally, the program is adapted according to the gestural changes and can be transferred from the handheld device directly to the robot controler. © 2012 IEEE.},
	booktitle = {{IEEE} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	author = {Lambrecht, J. and Kruger, J.},
	year = {2012},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {466--472},
	annote = {cited By 29},
	file = {Full Text:/Users/tid010/Zotero/storage/MR5GJA4K/Lambrecht and Kruger - 2012 - Spatial programming for industrial robots based on.pdf:application/pdf},
}

@article{zhang_vision-based_2019,
	title = {Vision-based target-following guider for mobile robot},
	volume = {66},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070534192&doi=10.1109%2fTIE.2019.2893829&partnerID=40&md5=761ea3a526a7ab0d51ff7b94c850a7aa},
	doi = {10.1109/TIE.2019.2893829},
	abstract = {A vision-based target-following guider for mobile robot is presented in this paper. It consists of three parts: the visual tracking part; the target redetection part; and the visual servo part. In the visual tracking part, the target contour band is proposed for irregular sampling, which can improve the performance of the correlation filter-based methods. In the target redetection part, potential targets are searched by an offline trained detector. Then an online training module is used to determine the real target and achieve accurate positioning. The interaction matrix of point features is used in the visual servo part for motion control to maintain the relative pose between the target and the robot. The visual tracking part and the target redetection part are tested respectively on many videos, which proved to work well. To show the effectiveness of our method, some state-of-the-art methods are also tested. The target-following guider is evaluated on mobile robots. In our experiments, the robot can robustly follow the human target over a long distance, which strongly proved the validity of our target-following guider. © 1982-2012 IEEE.},
	number = {12},
	journal = {IEEE Transactions on Industrial Electronics},
	author = {Zhang, M. and Liu, X. and Xu, D. and Cao, Z. and Yu, J.},
	year = {2019},
	keywords = {1scopus},
	pages = {9360--9371},
	annote = {cited By 12},
	file = {Full Text:/Users/tid010/Zotero/storage/QSHVFJ2K/Zhang et al. - 2019 - Vision-based target-following guider for mobile ro.pdf:application/pdf},
}

@inproceedings{luo_human-robot_2019,
	title = {A {Human}-{Robot} {Interaction} for a {Mecanum} {Wheeled} {Mobile} {Robot} with {Real}-{Time} {3D} {Two}-{Hand} {Gesture} {Recognition}},
	volume = {1267},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069965108&doi=10.1088%2f1742-6596%2f1267%2f1%2f012056&partnerID=40&md5=50fd35d2dbdc92d00cd4b36a32d37911},
	doi = {10.1088/1742-6596/1267/1/012056},
	abstract = {Human interaction with mobile robot becomes a popular research area and its applications are widely used in industrial, commercial and military fields. A two-hand gesture recognition method with depth camera is presented for real-time controlling the mecanum wheeled mobile robot. Seven different gestures could be recognized from one hand for mobile robot navigation and three gestures could be recognized from the other hand for controlling the gripper installed on the robot. Under the proposed control scheme, the mobile robot system can be navigated and can be operated at the same time for achieving missions by two different groups of hand gestures. The accuracy of the gesture recognition is about 94\%. During mobile robot control experiment, the system works timely, accurately and stably for certain tasks such as directional movement, grasping and cleaning obstacles. © Published under licence by IOP Publishing Ltd.},
	booktitle = {Journal of {Physics}: {Conference} {Series}},
	author = {Luo, X. and Amighetti, A. and Zhang, D.},
	year = {2019},
	note = {Issue: 1},
	keywords = {1scopus},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/FTISY7LQ/Luo et al. - 2019 - A Human-Robot Interaction for a Mecanum Wheeled Mo.pdf:application/pdf},
}

@inproceedings{chen_integrated_2010,
	title = {An integrated color and hand gesture recognition approach for an autonomous mobile robot},
	volume = {5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650544460&doi=10.1109%2fCISP.2010.5647930&partnerID=40&md5=ac870aa4015ca886e35e82e2539e0db2},
	doi = {10.1109/CISP.2010.5647930},
	abstract = {The purpose of this paper is to develop a real-time human-robot interaction with hand gesture recognition which can be constructed combining color and shape cues. There are two major problems that need to be resolved in this study. Firstly, due to variation in the apparent color of targets under varying imaging conditions, we propose a color multithresholding method with thresholds self-tuning mechanism to overcome the variation of target color suitable for indoor and outdoor environments. Secondly, the feature of the hand gesture is determined through the boundary extraction algorithm and the Fourier descriptors without having the results being affected by the rotation or size of hand gestures. Subsequently, outline feature for the nonlinear non-separable type of data was classified by using the support vector machines. In this paper, the accuracy of hand gesture recognition for the robot vision system has been enhanced by processing of these two methods as described above, and the human-robot interaction control of the robot has been successfully implemented under varying lighting conditions. ©2010 IEEE.},
	booktitle = {Proceedings - 2010 3rd {International} {Congress} on {Image} and {Signal} {Processing}, {CISP} 2010},
	author = {Chen, K.-Y. and Chien, C.-C. and Chang, W.-L. and Teng, J.-T.},
	year = {2010},
	keywords = {1scopus},
	pages = {2496--2500},
	annote = {cited By 8},
	file = {Full Text:/Users/tid010/Zotero/storage/WGG53URD/Chen et al. - 2010 - An integrated color and hand gesture recognition a.pdf:application/pdf},
}

@inproceedings{faudzi_real-time_2012,
	title = {Real-time hand gestures system for mobile robots control},
	volume = {41},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901060139&doi=10.1016%2fj.proeng.2012.07.246&partnerID=40&md5=506b3c2b4d7b06eb1bf609c833f34815},
	doi = {10.1016/j.proeng.2012.07.246},
	abstract = {Autonomous mobile robot navigation in an indoor environment using vision sensor is receiving a considerable attention in current robotics research activities. In this paper, a robot controlled by real-time hand gesture is proposed. This system includes an image pre-processing and feature of extraction state that consists of bounding box and Center-Of-Mass based computation. Through the feature of extraction state, the object's Center-Of-Mass and bounding box attributes are extracted to be applied for gesture sign control. This system could be used in gesture recognition for robot control applications. The result shows the developed mobile robots could be controlled successfully through hand gestures that facilitate the process of human-robot interaction. © 2012 The Authors.},
	booktitle = {Procedia {Engineering}},
	author = {Faudzi, A.A.M. and Ali, M.H.K. and Azman, M.A. and Ismail, Z.H.},
	year = {2012},
	keywords = {1scopus},
	pages = {798--804},
	annote = {cited By 11},
	file = {Full Text:/Users/tid010/Zotero/storage/WLJ5MEUK/Faudzi et al. - 2012 - Real-time hand gestures system for mobile robots c.pdf:application/pdf},
}

@inproceedings{choudhary_real_2015,
	title = {Real time robotic arm control using hand gestures},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925423366&doi=10.1109%2fICHPCA.2014.7045349&partnerID=40&md5=0f7bb0144c9c502e8056c25531938e9d},
	doi = {10.1109/ICHPCA.2014.7045349},
	abstract = {In this paper we propose a way to accomplish Human Computer Interface absolutely in electronic way (without mechanical sensors). The idea is to extirpate old techniques of controlling Robotic arm using joysticks, buttons and supersede with more intuitive technique ie., to control robotic arm by hand motion or gesture. Here we propound an approach to achieve the aforementioned idea employing Image processing technique using web camera. We detect the vital features of hand: fingers by computational geometry calculation enabling real time interaction between hand gestures and Robot. Our system can meticulously locate fingers even when fore-arm is involved. And the system can sustain a certain rotation of palm and fore-arm, which augments the freedom of use in palm center estimation. © 2014 IEEE.},
	booktitle = {2014 {International} {Conference} on {High} {Performance} {Computing} and {Applications}, {ICHPCA} 2014},
	author = {Choudhary, G.B. and Chethan, R.B.V.},
	year = {2015},
	keywords = {1scopus},
	annote = {cited By 7},
	file = {Full Text:/Users/tid010/Zotero/storage/XT8ECAGE/Choudhary and Chethan - 2015 - Real time robotic arm control using hand gestures.pdf:application/pdf},
}

@article{gupta_robust_2015,
	title = {A {Robust} {Visual} {Human} {Detection} {Approach} with {UKF}-{Based} {Motion} {Tracking} for a {Mobile} {Robot}},
	volume = {9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960400729&doi=10.1109%2fJSYST.2014.2317777&partnerID=40&md5=84d5020a7dcce135b903728c36958ca5},
	doi = {10.1109/JSYST.2014.2317777},
	abstract = {Robust tracking of a human in a video sequence is an essential prerequisite to an increasing number of applications, where a robot needs to interact with a human user or operates in a human-inhabited environment. This paper presents a robust approach that enables a mobile robot to detect and track a human using an onboard RGB-D sensor. Such robots could be used for security, surveillance, and assistive robotics applications. The proposed approach has real-time computation power through a unique combination of new ideas and well-established techniques. In the proposed method, background subtraction is combined with depth segmentation detector and template matching method to initialize the human tracking automatically. A novel concept of head and hand creation based on depth of interest is introduced in this paper to track the human silhouette in a dynamic environment, when the robot is moving. To make the algorithm robust, a series of detectors (e.g., height, size, and shape) is utilized to distinguish target human from other objects. Because of the relatively high computation time of the silhouette-matching-based method, a confidence level is defined, which allows using the matching-based method only where it is imperative. An unscented Kalman filter is used to predict the human location in the image frame to maintain the continuity of the robot motion. The efficacy of the approach is demonstrated through a real experiment on a mobile robot navigating in an indoor environment. © 2007-2012 IEEE.},
	number = {4},
	journal = {IEEE Systems Journal},
	author = {Gupta, M. and Behera, L. and Subramanian, V.K. and Jamshidi, M.M.},
	year = {2015},
	keywords = {1scopus},
	pages = {1363--1375},
	annote = {cited By 22},
	file = {Full Text:/Users/tid010/Zotero/storage/3S5L9WTK/Gupta et al. - 2015 - A Robust Visual Human Detection Approach with UKF-.pdf:application/pdf},
}

@inproceedings{vasconcelos_socially_2016,
	title = {Socially {Acceptable} {Robot} {Navigation} in the {Presence} of {Humans}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964378509&doi=10.1109%2fLARS-SBR.2015.14&partnerID=40&md5=969c8c6a43cd37e77d013fef15b1b736},
	doi = {10.1109/LARS-SBR.2015.14},
	abstract = {Considering the widespread use of mobile robots in different parts of society, it is important to provide them with the capability to behave in a socially acceptable manner. Therefore, a research topic of great importance recently has been the study of Human-Robot Interaction (HRI). In this work we propose a methodology to dynamically adapt the robot's behavior during its navigation considering a possible encounter with humans in the environment. The method is divided into two basic steps. The first one is based upon Computer Vision techniques and executes the recognition and analysis of the scene, considering characteristics such as the presence of humans, quantity, and distance to the robot. Considering the information from the previous stage, the methodology decides whether the navigation should undergo some modification. Among the possible adaptation are changing the current trajectory and reduction in speed. Different trials on a real-world scenario were executed, providing a thorough evaluation and validation of the methodology. © 2015 IEEE.},
	booktitle = {Proceedings - 12th {LARS} {Latin} {American} {Robotics} {Symposium} and 3rd {SBR} {Brazilian} {Robotics} {Symposium}, {LARS}-{SBR} 2015 - {Part} of the {Robotics} {Conferences} 2015},
	author = {Vasconcelos, P.A.A. and Pereira, H.N.S. and Macharet, D.G. and Nascimento, E.R.},
	year = {2016},
	keywords = {1scopus},
	pages = {222--227},
	annote = {cited By 2},
	file = {Full Text:/Users/tid010/Zotero/storage/NV78G7ZT/Vasconcelos et al. - 2016 - Socially Acceptable Robot Navigation in the Presen.pdf:application/pdf},
}

@inproceedings{haghighi_integration_2019,
	title = {Integration of {Camera} and {Inertial} {Measurement} {Unit} for {Entire} {Human} {Robot} {Interaction} {Using} {Machine} {Learning} {Algorithm}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075635898&doi=10.1109%2fSSD.2019.8893167&partnerID=40&md5=42e0453fce960ea80bbf3fb9ccb8a54c},
	doi = {10.1109/SSD.2019.8893167},
	abstract = {Importance of robots in industrial applications is a well-known fact. There are many tasks in industries, which can't be done alone by a robot or alone by a worker. Therefore, there is a need to establish a reliable and safe interaction environment between robots and workers. To do so, some information about the worker should be conveyed to the robot. This article focuses on industrial Human-Robot Interaction. For a safe and efficient Human-Robot Interaction, a robot needs to know about worker's position, posture and gesture. This paper proposes integration of an Inertial Measurement Unit (IMU) and a 3D camera as an image sensor to eliminate each other drawbacks and applies machine learning algorithm to detect postures and gestures of worker. Some experimental results during the interaction with heavy-duty robot will be presented © 2019 IEEE.},
	booktitle = {16th {International} {Multi}-{Conference} on {Systems}, {Signals} and {Devices}, {SSD} 2019},
	author = {Haghighi, A. and Bdiwi, M. and Putz, M.},
	year = {2019},
	keywords = {1scopus},
	pages = {741--746},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/U5JTDJWS/Haghighi et al. - 2019 - Integration of Camera and Inertial Measurement Uni.pdf:application/pdf},
}

@inproceedings{waskito_wheeled_2020,
	title = {Wheeled {Robot} {Control} with {Hand} {Gesture} based on {Image} {Processing}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091976882&doi=10.1109%2fIAICT50021.2020.9172032&partnerID=40&md5=7a5176d84771a31480df73dfef43862e},
	doi = {10.1109/IAICT50021.2020.9172032},
	abstract = {Computer vision based on shape recognition has a lot of potential in human and computer interaction. Hand gestures can be used as symbols of human interaction with computers which are preferred in the use of various hand gestures in sign language. Various tasks can be used to set remote control functions, control robots, and so on. The process of processing images or hand drawings using computer vision is called image processing. In this paper, a wheeled robot control system can be moved according to the given hand gesture commands. There are 6 forms of hand gestures that are made as input, and each hand gesture gives one command for the movement of a wheeled robot. The method used to classify each hand gesture, namely Convolutional Neural Network (CNN). CNN is a branch of the Artificial Neural Network (ANN) that can perform extraction features and create desired categories. The results of the classification will be carried out and sent to a wireless robot to run a movement. The result of this system is the movement of the wheeled robot following the given hand gestures. Variables that affect this system are training parameters and environmental parameters which include the amount of light intensity, distance, and tilt angle. The accuracy of the entire system obtained is 91.33\%. © 2020 IEEE.},
	booktitle = {Proceedings - 2020 {IEEE} {International} {Conference} on {Industry} 4.0, {Artificial} {Intelligence}, and {Communications} {Technology}, {IAICT} 2020},
	author = {Waskito, T.B. and Sumaryo, S. and Setianingsih, C.},
	year = {2020},
	keywords = {1scopus},
	pages = {48--54},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/QIXC4BZY/Waskito et al. - 2020 - Wheeled Robot Control with Hand Gesture based on I.pdf:application/pdf},
}

@article{raajan_hand_2013,
	title = {Hand posture recognition with application to robot control},
	volume = {5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878394139&partnerID=40&md5=a4e7b99617f93670b4db6e543075dda9},
	abstract = {In recent years, several researches are being done to improve the means by which humans interact with machines. Having developed a variety of input devices, we still are not completely comfortable with the present human-machine interaction processes. This stirred up the efforts undertaken to make the machines adapt to the human's natural means of communication which are speech and body language. The objective of this paper is to implement a real-time vision system which offers better comfort to humans while interacting with machines. In our paper, we showed a simple but efficient method to implement a hand posture recognition system and by means of which we control a bot wirelessly through Bluetooth. The simplicity of our method enables fast recognition of the hand postures shown and therefore achieves the real-time continuous control over the bot.},
	number = {2},
	journal = {International Journal of Engineering and Technology},
	author = {Raajan, N.R. and Raghuraman, S. and Vignesh, T.},
	year = {2013},
	keywords = {1scopus},
	pages = {1115--1119},
	annote = {cited By 0},
	file = {Raajan et al. - 2013 - Hand posture recognition with application to robot.pdf:/Users/tid010/Zotero/storage/CR866R63/Raajan et al. - 2013 - Hand posture recognition with application to robot.pdf:application/pdf},
}

@inproceedings{liu_omnidirectional_2010,
	title = {Omnidirectional vision for mobile robot human body detection and localization},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78751563037&doi=10.1109%2fICSMC.2010.5641683&partnerID=40&md5=8d7c1e422ae05fcdd5b8ececdf9b0afe},
	doi = {10.1109/ICSMC.2010.5641683},
	abstract = {Human body detection and localization is an essential capability of an autonomous mobile robot which works in the human-robot interaction (HRI) environments. However, due to field of view (FOV) limitations, it is hard to detect all human bodies around a mobile robot by using a conventional camera, and distances between robots and human bodies are also difficult to estimate. In this paper, we propose a novel omnidirectional visual system to locate positions of human bodies for an autonomous mobile robot. Firstly, a handy fitting shape based method (FSM) is presented to remap a omnidirectional image to a bird's eye view image. A new bird's eye view image segmentation algorithm, which is inspired by image pyramids, is used to split obstacle objects and ground plane. Secondly, a shape-based human body detector is implemented in unwrapped omnidirectional images to locate regions of human bodies. These human body detection results are combined with bird's eye view image segmentation to distinguish human bodies from other obstacle objects. Experiments show that our system performs well in human-robot interaction environments. ©2010 IEEE.},
	booktitle = {Conference {Proceedings} - {IEEE} {International} {Conference} on {Systems}, {Man} and {Cybernetics}},
	author = {Liu, H. and Huo, Z. and Yang, G.},
	year = {2010},
	keywords = {1scopus},
	pages = {2186--2191},
	annote = {cited By 5},
	file = {Full Text:/Users/tid010/Zotero/storage/BE3U4IW6/Liu et al. - 2010 - Omnidirectional vision for mobile robot human body.pdf:application/pdf},
}

@inproceedings{yun_robust_2010,
	title = {Robust robot's attention for human based on the multi-modal sensor and robot behavior},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870051809&doi=10.1109%2fARSO.2010.5680037&partnerID=40&md5=6f061f4919c9b3e81b513cfea5a22cfc},
	doi = {10.1109/ARSO.2010.5680037},
	abstract = {In this paper, we propose the robust robot's attention for human based on the multi-modal sensor and robot behavior. All of the robot components for attention are operating on the intelligent robot software architecture. Human search collect the human information from the sensing information for vision and voice in various illumination change and dynamic environments. And human tracker follows the face trajectory with efficiency and safety. Unlike common belief, the biggest obstacle and competitive factor in robotics is expected to be human robot interaction. Since robot intelligence is not yet at a practical level, the creation of a general interaction manager using an intelligence system will not be realized for some time. Instead, it focuses on one-way speaking and expressing based on emotion to human. Experimental results show that the proposed scheme works successfully in real environments. ©2010 IEEE.},
	booktitle = {Proceedings of {IEEE} {Workshop} on {Advanced} {Robotics} and its {Social} {Impacts}, {ARSO}},
	author = {Yun, S. and Kim, C.G. and Kim, M. and Choi, M.-T.},
	year = {2010},
	keywords = {1scopus},
	pages = {117--122},
	annote = {cited By 3},
	file = {Full Text:/Users/tid010/Zotero/storage/ARXK2NLU/Yun et al. - 2010 - Robust robot's attention for human based on the mu.pdf:application/pdf},
}

@inproceedings{katsuki_high-speed_2015,
	title = {High-speed {Human}/{Robot} {Hand} {Interaction} {System}},
	volume = {02-05-March-2015},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969285275&doi=10.1145%2f2701973.2701984&partnerID=40&md5=62e5562e524dda1c958312afeed81488},
	doi = {10.1145/2701973.2701984},
	abstract = {We propose an entirely new human hand/robot hand interaction system designed with a focus on high speed. The speed of this system, from input via a high-speed vision system to output by a high-speed multifingered robot hand, exceeds the visual recognition speed of humans. Therefore, the motion of the interaction system cannot be recognized by the human eye. As an application, we created a system called "Rock-Paper-Scissors robot system with 100\% winning rate', based on this interaction system. This system always beats human players in the Rock-Paper-Scissors game due to the high speed of our interaction system.We also discuss the future possibilities of this system. © 2015 Authors.},
	booktitle = {{ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	author = {Katsuki, Y. and Yamakawa, Y. and Ishikawa, M.},
	year = {2015},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {117--118},
	annote = {cited By 9},
	file = {Full Text:/Users/tid010/Zotero/storage/9UC6UWS6/Katsuki et al. - 2015 - High-speed HumanRobot Hand Interaction System.pdf:application/pdf},
}

@inproceedings{wu_accompanist_2012,
	title = {Accompanist detection and following for wheelchair robots with fuzzy controller},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869774498&partnerID=40&md5=cf2b7d1a68212e98c613c1440ef11df6},
	abstract = {Human-robot interaction is an important issue for service robots. In this paper, we propose to utilize the multisensory data fusion to autonomously detect and track an accompanist as a target person in the surroundings of the mobile robot. First, we use laser range finder to detect the target person features which is characterized using distances and orientations from range sensor readings. Based on the sensor information and the desired tracking position, a fuzzy based tracking controller is proposed to follow the accompanist. In addition, the obstacle avoidance strategy is also embedded into the following controller to mitigate disturbances or interferences in dynamical environment when robot is performing a following task simultaneously. If the accompanist is missed, a recognition process based on a pan-tilt-zoom camera and observed images will be used to search the target person again. Experimental results demonstrated the validation and feasibility of the proposed system. © 2012 Intl Jrnal Advanced Mech.},
	booktitle = {2012 {International} {Conference} {onAdvanced} {Mechatronic} {Systems}, {ICAMechS} 2012},
	author = {Wu, B.-F. and Jen, C.-L. and Tsou, T.-Y. and Li, W.-F. and Tseng, P.-Y.},
	year = {2012},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {638--643},
	annote = {cited By 5},
}

@inproceedings{quintero_visual_2015,
	title = {Visual pointing gestures for bi-directional human robot interaction in a pick-and-place task},
	volume = {2015-November},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954063152&doi=10.1109%2fROMAN.2015.7333604&partnerID=40&md5=5c1fdbc59e4f9ab22ecab2c4148ec810},
	doi = {10.1109/ROMAN.2015.7333604},
	abstract = {This paper explores visual pointing gestures for two-way nonverbal communication for interacting with a robot arm. Such non-verbal instruction is common when humans communicate spatial directions and actions while collaboratively performing manipulation tasks. Using 3D RGBD we compare human-human and human-robot interaction for solving a pick-and-place task. In the human-human interaction we study both pointing and other types of gestures, performed by humans in a collaborative task. For the human-robot interaction we design a system that allows the user to interact with a 7DOF robot arm using gestures for selecting, picking and dropping objects at different locations. Bi-directional confirmation gestures allow the robot (or human) to verify that the right object is selected. We perform experiments where 8 human subjects collaborate with the robot to manipulate ordinary household objects on a tabletop. Without confirmation feedback selection accuracy was 70-90\% for both humans and the robot. With feedback through confirmation gestures both humans and our vision-robotic system could perform the task accurately every time (100\%). Finally to illustrate our gesture interface in a real application, we let a human instruct our robot to make a pizza by selecting different ingredients. © 2015 IEEE.},
	booktitle = {Proceedings - {IEEE} {International} {Workshop} on {Robot} and {Human} {Interactive} {Communication}},
	author = {Quintero, C.P. and Tatsambon, R. and Gridseth, M. and Jagersand, M.},
	year = {2015},
	keywords = {1scopus},
	pages = {349--354},
	annote = {cited By 7},
	file = {Full Text:/Users/tid010/Zotero/storage/TY2TQ8DK/Quintero et al. - 2015 - Visual pointing gestures for bi-directional human .pdf:application/pdf},
}

@inproceedings{pentiuc_drive_2018,
	title = {"{Drive} me": {A} interaction system between human and robot},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050282891&doi=10.1109%2fDAAS.2018.8396087&partnerID=40&md5=4b5720126cc77526d229c6316a7ae3e6},
	doi = {10.1109/DAAS.2018.8396087},
	abstract = {This paper presents a new interaction system between human and robot, called "Drive Me", a system that allows users to "drive" a electric robot using natural and dynamic gestures. The user stands in front of the depth camera and performs the gestures that practically move and "drive" the electric robot. The naturalness and the facility of performing the gestures let us know that "Drive Me" is a robust and easy to use interaction system between human and robot. Gesture recognition has a high accuracy rate and that makes of "Drive Me" a system easy to use by different users, system that does not depend of the ambient light conditions. We also made a performance evaluation of the "Drive Me" interaction system and we calculated the accuracy rate, the error rate, the precision, the recall, the sensitivity and the specificity of gesture recognition. © 2018 IEEE.},
	booktitle = {2018 14th {International} {Conference} on {Development} and {Application} {Systems}, {DAS} 2018 - {Proceedings}},
	author = {Pentiuc, S.-G. and Vultur, O.-M.},
	year = {2018},
	keywords = {1scopus},
	pages = {144--149},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/W3VFFJ9F/Pentiuc and Vultur - 2018 - Drive me A interaction system between human and.pdf:application/pdf},
}

@inproceedings{lee_learning_2017,
	title = {Learning robot activities from first-person human videos using convolutional future regression},
	volume = {2017-September},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041949068&doi=10.1109%2fIROS.2017.8205953&partnerID=40&md5=76afdcf7cf1da6d67e274a89c654cc8b},
	doi = {10.1109/IROS.2017.8205953},
	abstract = {We design a new approach that allows robot learning of new activities from unlabeled human example videos. Given videos of humans executing the same activity from a human's viewpoint (i.e., first-person videos), our objective is to make the robot learn the temporal structure of the activity as its future regression network, and learn to transfer such model for its own motor execution. We present a new deep learning model: We extend the state-of-the-art convolutional object detection network for the representation/estimation of human hands in training videos, and newly introduce the concept of using a fully convolutional network to regress (i.e., predict) the intermediate scene representation corresponding to the future frame (e.g., 1-2 seconds later). Combining these allows direct prediction of future locations of human hands and objects, which enables the robot to infer the motor control plan using our manipulation network. We experimentally confirm that our approach makes learning of robot activities from unlabeled human interaction videos possible, and demonstrate that our robot is able to execute the learned collaborative activities in real-time directly based on its camera input. © 2017 IEEE.},
	booktitle = {{IEEE} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	author = {Lee, J. and Ryoo, M.S.},
	year = {2017},
	keywords = {1scopus},
	pages = {1497--1504},
	annote = {cited By 4},
	file = {Submitted Version:/Users/tid010/Zotero/storage/FQCXHTTD/Lee and Ryoo - 2017 - Learning robot activities from first-person human .pdf:application/pdf},
}

@article{tolgyessy_foundations_2017,
	title = {Foundations of {Visual} {Linear} {Human}–{Robot} {Interaction} via {Pointing} {Gesture} {Navigation}},
	volume = {9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028298106&doi=10.1007%2fs12369-017-0408-9&partnerID=40&md5=9e53c30544361bbc078d50183d80a672},
	doi = {10.1007/s12369-017-0408-9},
	abstract = {This paper presents a human–robot interaction method for controlling an autonomous mobile robot with a referential pointing gesture. A human user points to a specific location, robot detects the pointing gesture, computes its intersection with surrounding planar surface and moves to the destination. A depth camera mounted on the chassis is used. The user does not need to wear any extra clothing or markers. The design includes necessary mathematical concepts such as transformations between coordinate systems and vector abstraction of features needed for simple navigation, which other current research works misses. We provide experimental evaluation with derived probability models. We term this approach “Linear HRI” and define 3 laws of Linear HRI. © 2017, Springer Science+Business Media Dordrecht.},
	number = {4},
	journal = {International Journal of Social Robotics},
	author = {Tölgyessy, M. and Dekan, M. and Duchoň, F. and Rodina, J. and Hubinský, P. and Chovanec, L.},
	year = {2017},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {509--523},
	annote = {cited By 10},
	file = {Full Text:/Users/tid010/Zotero/storage/GHJSKYGS/Tölgyessy et al. - 2017 - Foundations of Visual Linear Human–Robot Interacti.pdf:application/pdf},
}

@inproceedings{fahn_real-time_2010,
	title = {Real-time face tracking techniques used for the interaction between humans and robots},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956052758&doi=10.1109%2fICIEA.2010.5514736&partnerID=40&md5=763e574d4da28da710b2801bd4e60d3e},
	doi = {10.1109/ICIEA.2010.5514736},
	abstract = {Owing to the demand of more efficient and friendly human-computer interfaces, the researches on face processing have been rapidly grown in recent years. In addition to offering some kinds of service for human being, one of the most important characteristics of a favorable system is to autonomously interact with people. In view of the above-mentioned facts, an automatic real-time face tracking system installed on a person following robot is presented in this paper. In the face tracking procedure, we employ an improved particle filter to dynamically locate a human face. Since we have considered the hair color information of a human head, the particle filter will keep tracking even if the person is back to the sight of a camera. We further adopt both the motion and color cues as the features to alleviate the influence of the background as low as possible. According to the position of the human face in an image, we issue a series of commands (moving forward, turning left or turning right) to drive the motors of wheels on a robot, and judge the distance between the robot and a person with the aid of three ultrasonic sensors to issue a set of commands (stop or turn backward) until the robot follows to a suitable distance from the person. Experimental results reveal that the face tracking rate is more than 95\% in general situations and over 88\% when the face suffers from temporal occlusion. Besides this, the efficiency of system execution is very satisfactory. © 2010 IEEE.},
	booktitle = {Proceedings of the 2010 5th {IEEE} {Conference} on {Industrial} {Electronics} and {Applications}, {ICIEA} 2010},
	author = {Fahn, C.-S. and Lin, Y.-T.},
	year = {2010},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {12--17},
	annote = {cited By 1},
	file = {Full Text:/Users/tid010/Zotero/storage/JV7DNB3G/Fahn and Lin - 2010 - Real-time face tracking techniques used for the in.pdf:application/pdf},
}

@inproceedings{miyoshi_above_2014,
	title = {Above your hand: {Direct} and natural interaction with aerial robot},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905855762&doi=10.1145%2f2614066.2614086&partnerID=40&md5=1ae71b088f60a6c9b847a400425f9bc0},
	doi = {10.1145/2614066.2614086},
	abstract = {"Above Your Hand" is a new application of interactive aerial robot. We explore direct and natural interaction with autonomous aerial robots and its applications. The uniqueness of our approach is to use no external equipment such as controllers, motion tracking system or wireless control system. Our original palm-sized quadcopter (Figure 1, right) flies above your hand wearing a glove of a particular color. It is capable of following your hand using two onboard cameras (Figure 1, middle). One camera is attached horizontally to the quadcopter and the other vertically. When it does not detect hands, it keeps hovering at its current location by processing the feature points of the obtained images of its environment without particular landmarks [Konomura et al. 2013, 2014]. Since all of this processing is executed inside the onboard Linux-based microcontroller, it requires no external computational control at all. This is the world-first success in both the smallness and full autonomy within the onboard computer. The smallness enables more active and closer interaction in indoor environments. Conversely our robot does not perform well in outdoor or windy environments yet. Owing to the simple design of the interaction, it is easy for multiple people to be involved in the interaction at the same time. For example you can "pass" the flying robot to another person from hand to hand (Figure 1, left). There have been previous works on natural interaction with aerial robots. Ng made methods to interact with Parrot's AR.Drone using Microsoft's Kinect [Ng et al. 2011]. Sanna presented a NUI framework for quadcopter control [Sanna et al. 2013]. Lementec used multiple orientation sensors to classify gestures [Lementec et al. 2004]. Since all of these methods require stationary equipment, there are naturally limitations in the available area of the aerial robots. Our approach has much potential to remove this limitation. © 2014 held by the Owner/Author.},
	booktitle = {{ACM} {SIGGRAPH} 2014 {Emerging} {Technologies}, {SIGGRAPH} 2014},
	author = {Miyoshi, K. and Konomura, R. and Hori, K.},
	year = {2014},
	keywords = {1scopus},
	annote = {cited By 9},
	file = {Full Text:/Users/tid010/Zotero/storage/NWGHENVS/Miyoshi et al. - 2014 - Above your hand Direct and natural interaction wi.pdf:application/pdf},
}

@inproceedings{cheng_multiple-robot_2013,
	title = {A multiple-robot system for home service},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897693987&doi=10.1109%2fCACS.2013.6734111&partnerID=40&md5=7d17741672b3b43edaa6ea5082e48674},
	doi = {10.1109/CACS.2013.6734111},
	abstract = {This paper focuses on applying visual servoing techniques for a multiple-robot system to cooperate. Each individual robot has its own video camera and the images captured are used both for the feedback control and for mutual interaction. Furthermore, we utilize a centralized computing kernel to coordinate total information to speed up the communication between robots and simplify the connection. The robot tasks are achieved mainly through computer vision. For example, the security monitoring task uses human face image detection, the service of providing water uses images to make sure proper robot and human interaction, and the system uses images for robots to cooperate for the floor surface exploring and object transporting. © 2013 IEEE.},
	booktitle = {2013 {CACS} {International} {Automatic} {Control} {Conference}, {CACS} 2013 - {Conference} {Digest}},
	author = {Cheng, C.-Y. and Zhuo, Y.-Y. and Kuo, G.-H.},
	year = {2013},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {79--84},
	annote = {cited By 1},
	file = {Full Text:/Users/tid010/Zotero/storage/ZPM8523B/Cheng et al. - 2013 - A multiple-robot system for home service.pdf:application/pdf},
}

@inproceedings{nguyen_using_2016,
	title = {Using {Hand} {Postures} for {Interacting} with {Assistant} {Robot} in {Library}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964744498&doi=10.1109%2fKSE.2015.18&partnerID=40&md5=6766c216341e945de7ff7ca7cc41be56},
	doi = {10.1109/KSE.2015.18},
	abstract = {Visual interpretation of hand gesture for human-system interaction in general and human-robot interaction in particular is becoming a hot topic in computer vision and robotics fields. Hand gestures provide very intuitive and efficient means and enhance the flexibility in communication. Even a number of works have been proposed for hand gesture recognition, the use of these works for real human-robot interaction is still limited. Based on our previous works for hand detection and hand gesture recognition, we have built a fully automatic hand gesture recognition system and have applied it in a human-robot interaction application: service robot in library. In this paper, we describe in detail this application from the user requirement analysis to system deployment and experiments with end-users. © 2015 IEEE.},
	booktitle = {Proceedings - 2015 {IEEE} {International} {Conference} on {Knowledge} and {Systems} {Engineering}, {KSE} 2015},
	author = {Nguyen, V.-T. and Tran, T.-H. and Le, T.-L. and Mullot, R. and Courboulay, V.},
	year = {2016},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {354--359},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/U2IVWWTT/Nguyen et al. - 2016 - Using Hand Postures for Interacting with Assistant.pdf:application/pdf},
}

@inproceedings{arumbakkam_multi-modal_2010,
	title = {A multi-modal architecture for human robot communication},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79851489286&doi=10.1109%2fICHR.2010.5686337&partnerID=40&md5=129f5fc216e4a0c074c143da603e5f5e},
	doi = {10.1109/ICHR.2010.5686337},
	abstract = {In this paper we present a human-friendly control framework and an associated system architecture for performing compliant trajectory tracking of multimodal human gesture information on a position controlled humanoid robot in realtime. The contribution of this paper includes a system architecture and control methodology that enables real-time compliant control of humanoid robots from demonstrated human motion and speech inputs. The human motion consists of the body and head pose. The human body motion, represented by a set of Cartesian space motion descriptors, is captured using a single depth camera marker-less vision processing module. The human head pose , represented by two degrees of freedom, is estimated and tracked using a single CCD camera. The architecture also enables fine motion control through human speech commands processed by a dedicated speech processing system. Motion description from the three input modes are synchronized and retargeted to the joint space coordinates of the humanoid robot in real-time. The retargeted motion adheres to the robot's kinematic constraints and represents the reference joint motion that is subsequently executed by a model based compliant control framework through a torque to position transformation system. The compliant and low gain tracking performed by this framework renders the system physically safe and therefore friendly to humans interacting with the robot. Experiments were performed on the Honda humanoid robot and the results are presented here. ©2010 IEEE.},
	booktitle = {2010 10th {IEEE}-{RAS} {International} {Conference} on {Humanoid} {Robots}, {Humanoids} 2010},
	author = {Arumbakkam, A.K. and Yoshikawa, T. and Dariush, B. and Fujimura, K.},
	year = {2010},
	keywords = {1scopus},
	pages = {639--646},
	annote = {cited By 2},
	file = {Full Text:/Users/tid010/Zotero/storage/8HB7JIYZ/Arumbakkam et al. - 2010 - A multi-modal architecture for human robot communi.pdf:application/pdf},
}

@inproceedings{yoshida_evaluation_2011,
	title = {Evaluation of pointing navigation interface for mobile robot with spherical vision system},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053063424&doi=10.1109%2fFUZZY.2011.6007673&partnerID=40&md5=93ca464cea79784a882afae95d61a190},
	doi = {10.1109/FUZZY.2011.6007673},
	abstract = {In human robot interaction, intuitive interface is necessary. A specific interaction device, for instance, a joystick or a teaching pendant, is not usually intuitive and needs trainings for a general user. Instruction by gesture is one of the intuitive interfaces and a potential user does not need any training for showing a gesture. Pointing is one of the simplest gestures. Hibino et. al.[1] proposed a simple human pointing recognition system for a mobile robot that has an upward directed camera and recognizes human pointing and navigate itself to the place a user is pointing by simple visual feedback control. This paper shows improvement of the method and investigates the validity and usefulness of the proposed method with questionnaire investigations with the proposed and conventional user interfaces. © 2011 IEEE.},
	booktitle = {{IEEE} {International} {Conference} on {Fuzzy} {Systems}},
	author = {Yoshida, K. and Hibino, F. and Takahashi, Y. and Maeda, Y.},
	year = {2011},
	keywords = {1scopus},
	pages = {721--726},
	annote = {cited By 5},
	file = {Full Text:/Users/tid010/Zotero/storage/4MUSGQQ4/Yoshida et al. - 2011 - Evaluation of pointing navigation interface for mo.pdf:application/pdf},
}

@article{yuan_natural_2020,
	title = {A natural immersive closed-loop interaction method for human–{Robot} “{Rock}–{Paper}–{Scissors}” {Game}},
	volume = {1031 AISC},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077532173&doi=10.1007%2f978-981-13-9406-5_14&partnerID=40&md5=746161c8d0a837801907215ce1995ec0},
	doi = {10.1007/978-981-13-9406-5_14},
	abstract = {The existing Kinect somatosensory games and other human–computer interaction are lack of natural interaction experience. We made research on a natural immersive closed-loop interaction method for human–robot “rock–paper–scissors” game. Kinect was used to make somatosensory dynamic recognition. Mechanical arm was used to make gestures to interact with human and to show the result of the game. System through the development of Modbus master and Modbus slave based on Arduino to control the mechanical arm, and used the multithread concurrent to achieve a better real-time experience. The PC was programmed to determine the outcome and record the result. The natural immersive closed-loop interaction method has the advantages of natural interaction, fast response, and strong expansibility. © Springer Nature Singapore Pte Ltd. 2020.},
	journal = {Advances in Intelligent Systems and Computing},
	author = {Yuan, X. and Dai, S. and Fang, Y.},
	year = {2020},
	keywords = {1scopus},
	pages = {103--111},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/4SWRZEP8/Yuan et al. - 2020 - A natural immersive closed-loop interaction method.pdf:application/pdf},
}

@article{qian_visually_2013,
	title = {Visually gesture recognition for an interactive robot grasping application},
	volume = {8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878477530&partnerID=40&md5=983edd9a03f6a4690e16308da763716e},
	abstract = {Gesture based natural human-robot interaction paradigm has few physical requirements, and thus can be deployed in many restrictive and challenging environments. In this paper, we propose a robot vision based approach to recognizing intentional arm-pointing gestures of human for an object grasping application. To overcome the limitation of robot onboard vision quality and background cluttering in natural indoor environment, a multi-cue human detection method is proposed. Human body is detected and verified by merging appearance and color features with robust head-shoulder based shape matching for reducing the false detection rate. Then intentional dynamic arm-pointing gestures of a person are identified using Dynamic Time Warping (DTW) technique, whilst unconscious motions of arm and head are rejected. Implementation of a gesture-guided robot grasping task in an indoor environment is given to demonstrate this approach, in which a fast and reliable recognition of pointing gesture recognition is achieved.},
	number = {3},
	journal = {International Journal of Multimedia and Ubiquitous Engineering},
	author = {Qian, K. and Hu, C.},
	year = {2013},
	keywords = {1scopus},
	pages = {189--196},
	annote = {cited By 1},
	file = {Qian and Hu - 2013 - Visually gesture recognition for an interactive ro.pdf:/Users/tid010/Zotero/storage/NGUA4VJV/Qian and Hu - 2013 - Visually gesture recognition for an interactive ro.pdf:application/pdf},
}

@article{li_visual_2015,
	title = {Visual {Perception} {Based} {Engagement} {Awareness} for {Multiparty} {Human}-{Robot} {Interaction}},
	volume = {12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948569788&doi=10.1142%2fS021984361550019X&partnerID=40&md5=9e10cc5b8a6733661e13ffc312cc0c46},
	doi = {10.1142/S021984361550019X},
	abstract = {Computational systems for human-robot interaction (HRI) could benefit from visual perceptions of social cues that are commonly employed in human-human interactions. However, existing systems focus on one or two cues for attention or intention estimation. This research investigates how social robots may exploit a wide spectrum of visual cues for multiparty interactions. It is proposed that the vision system for social cue perception should be supported by two dimensions of functionality, namely, vision functionality and cognitive functionality. A vision-based system is proposed for a robot receptionist to embrace both functionalities for multiparty interactions. The module of vision functionality consists of a suite of methods that computationally recognize potential visual cues related to social behavior understanding. The performance of the models is validated by the ground truth annotation dataset. The module of cognitive functionality consists of two computational models that (1) quantify users' attention saliency and engagement intentions, and (2) facilitate engagement-aware behaviors for the robot to adjust its direction of attention and manage the conversational floor. The performance of the robot's engagement-aware behaviors is evaluated in a multiparty dialog scenario. The results show that the robot's engagement-aware behavior based on visual perceptions significantly improve the effectiveness of communication and positively affect user experience. © 2015 World Scientific Publishing Company.},
	number = {4},
	journal = {International Journal of Humanoid Robotics},
	author = {Li, L. and Xu, Q. and Wang, G.S. and Yu, X. and Tan, Y.K. and Li, H.},
	year = {2015},
	keywords = {1scopus},
	annote = {cited By 0},
	file = {Li et al. - 2015 - Visual Perception Based Engagement Awareness for M.pdf:/Users/tid010/Zotero/storage/RCSQUGTX/Li et al. - 2015 - Visual Perception Based Engagement Awareness for M.pdf:application/pdf},
}

@inproceedings{stipancic_programming_2012,
	title = {Programming an industrial robot by demonstration},
	volume = {1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896948058&partnerID=40&md5=6e496dc214a79d3349c1c4326736d866},
	abstract = {The constant need for reducing production costs and faster arrival tothe market of new products requires changes in product design,material selection, and production processes. While applying the changes, a level of complexity of contemporary industrial systems is undoubtedly increasing. Despite the fact that industrial robotic systems are becoming more and more complex, they should become user-friendlier and easily accessible for human operators at the same time.Such systems should have a smaller number of welltrained and highly educated employees who manage the production.This paper present an approach in programming a robot for doing assembly tasks based on Programming by Demonstration principles. The system tracks gestures and movements of the human operator by using Microsoft Kinect ® sensor.To enable an interaction between the system and the human operator,a simple gesture command list is developed. This methodology could help non-expert users to teach robots how to perform new assembly tasks in more human like ways. The command list present a very first step in a development of more powerfulgesture language that should provide a simple and an intuitive way forexchaniging information between people and machines.},
	booktitle = {23rd {DAAAM} {International} {Symposium} on {Intelligent} {Manufacturing} and {Automation} 2012},
	author = {Stipancic, T. and Jerbic, B. and Bucevic, A. and Curkovic, P.},
	year = {2012},
	keywords = {1scopus},
	pages = {15--18},
	annote = {cited By 8},
	file = {Stipancic et al. - 2012 - Programming an industrial robot by demonstration.pdf:/Users/tid010/Zotero/storage/9RMRCNSL/Stipancic et al. - 2012 - Programming an industrial robot by demonstration.pdf:application/pdf},
}

@article{du_online_2018,
	title = {Online robot teaching with natural human-robot interaction},
	volume = {65},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045193829&doi=10.1109%2fTIE.2018.2823667&partnerID=40&md5=0a1a788b32b046d948f7ac84f02386c2},
	doi = {10.1109/TIE.2018.2823667},
	abstract = {With the development of Industry 4.0, robots tend to be intelligent and collaborative. For one, robots can interact naturally with humans. For another, robots can work collaboratively with humans in a common area. The traditional teaching method is no longer suitable for the production mode with human-robot collaboration. Since the traditional teaching processes are complicated, they need highly skilled staffs. This paper focuses on the natural way of online teaching, which can be applied to the tasks such as welding, painting, and stamping. This paper presents an online teaching method with the fusion of speech and gesture. A depth camera (Kinect) and an inertial measurement unit are used to capture the speech and gesture of the human. Interval Kalman filter and improved particle filter are employed to estimate the gesture. To integrate speech and gesture information more deeply, a novel method of audio-visual fusion based on text is proposed, which can extract the most useful information from the speech and gestures by transforming them into text. Finally, a maximum entropy algorithm is employed to deal with the fusion text into the corresponding robot instructions. The practicality and effectiveness of the proposed approach were validated by five subjects without robot teaching skills. The results indicate that the online robot teaching system can successfully teach robot manipulators. © 1982-2012 IEEE.},
	number = {12},
	journal = {IEEE Transactions on Industrial Electronics},
	author = {Du, G. and Chen, M. and Liu, C. and Zhang, B. and Zhang, P.},
	year = {2018},
	keywords = {1scopus},
	pages = {9571--9581},
	annote = {cited By 21},
	file = {Full Text:/Users/tid010/Zotero/storage/NWZHYKEQ/Du et al. - 2018 - Online robot teaching with natural human-robot int.pdf:application/pdf},
}

@article{condes_person_2019,
	title = {Person {Following} {Robot} {Behavior} {Using} {Deep} {Learning}},
	volume = {855},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057859226&doi=10.1007%2f978-3-319-99885-5_11&partnerID=40&md5=4c12446c62580bc88b7142405ce35467},
	doi = {10.1007/978-3-319-99885-5_11},
	abstract = {Human-robot interaction (HRI) is a field with growing impact as robot applications are entering into homes, supermarkets and general human environments. Person following is an interesting capability in HRI. This paper presents a new system for a robust person following behavior inside a robot. Its perception module addresses the person detection on images using a pretrained TensorFlow SSD Convolutional Neural Network which provides robustness even on tough lighting conditions. It also includes a face detector and a FaceNet CNN to reidentify the target person. Care has been put to allow real-time operation. The control module implements two PID controllers for a reactive smooth response, moving the robot towards the target person without distracting with other people around. The entire system has been experimentally validated on a real TurtleBot2 robot, with an Asus Xtion RGBD camera. © 2019, Springer Nature Switzerland AG.},
	journal = {Advances in Intelligent Systems and Computing},
	author = {Condés, I. and Cañas, J.M.},
	year = {2019},
	keywords = {1scopus},
	pages = {147--161},
	annote = {cited By 1},
	file = {Full Text:/Users/tid010/Zotero/storage/GK4SQLKG/Condés and Cañas - 2019 - Person Following Robot Behavior Using Deep Learnin.pdf:application/pdf},
}

@inproceedings{gao_humanoid_2015,
	title = {Humanoid robot locomotion control by posture recognition for human-robot interaction},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964502045&doi=10.1109%2fROBIO.2015.7418995&partnerID=40&md5=c37d21b15475bb630c2b14646946d0e2},
	doi = {10.1109/ROBIO.2015.7418995},
	abstract = {During human-robot interaction, it is often necessary for human users to command the robot's locomotion. In this paper, we design an intuitive interaction mechanism for users to command a NAO robot's locomotion with predefined postures. Based on images taken by its onboard monocular RGB camera, the robot can localize the human user's head, torso and arms, and recognize the posture displayed by the user with the k-NN algorithm. Then the robot can move according to the locomotion command (forward/backward/left/right) associated with the posture. We test these postures in an HRI experiment and get good recognition performance. © 2015 IEEE.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Robotics} and {Biomimetics}, {IEEE}-{ROBIO} 2015},
	author = {Gao, X. and Zheng, M. and Meng, M.Q.-H.},
	year = {2015},
	keywords = {1scopus},
	pages = {1572--1577},
	annote = {cited By 1},
	file = {Full Text:/Users/tid010/Zotero/storage/2LYGBFY7/Gao et al. - 2015 - Humanoid robot locomotion control by posture recog.pdf:application/pdf},
}

@inproceedings{nguyen_audio-visual_2014,
	title = {Audio-visual integration for human-robot interaction in multi-person scenarios},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946688704&doi=10.1109%2fETFA.2014.7005303&partnerID=40&md5=d2dfacb6114b01f01ff20aeb46721450},
	doi = {10.1109/ETFA.2014.7005303},
	abstract = {This paper presents the integration of audio-visual perception components for human robot interaction in the Robot Operating System (ROS). Visual-based nodes consist of skeleton tracking and gesture recognition using a depth camera, and face recognition using an RGB camera. Auditory perception is based on sound source localization using a microphone array. We present an integration framework of these nodes using a top-down hierarchical messaging protocol. On the top of the integration, a message carries information about the number of persons and their corresponding states (who, what, where), which are updated from many low-level perception nodes. The top message is passed to a planning node to make a reaction of the robot, according to the perception about surrounding people. This paper demonstrates human-robot interaction in multi-persons scenario where robot pays its attention to the speaking or waving hand persons. Moreover, this modularization architecture enables reusing modules for other applications. To validate this approach, two sound source localization algorithms are evaluated in real-time where ground-truth localization is provided by the face recognition module. © 2014 IEEE.},
	booktitle = {19th {IEEE} {International} {Conference} on {Emerging} {Technologies} and {Factory} {Automation}, {ETFA} 2014},
	author = {Nguyen, Q. and Yun, S.-S. and Choi, J.},
	year = {2014},
	keywords = {1scopus},
	annote = {cited By 4},
	file = {Full Text:/Users/tid010/Zotero/storage/J9GHR646/Nguyen et al. - 2014 - Audio-visual integration for human-robot interacti.pdf:application/pdf},
}

@article{arai_service_2020,
	title = {Service robot arm controlled just by sight},
	volume = {69},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062915242&doi=10.1007%2f978-3-030-12388-8_38&partnerID=40&md5=05f6407b933e9622ea0de5d9c2bd8e07},
	doi = {10.1007/978-3-030-12388-8_38},
	abstract = {Robot arm controlled by Eye Based Human-Computer Interaction: EBHCI is proposed. The proposed system allows disabled person to select desirable food from the meal tray by their eyes only as an example. Robot arm which is used for retrieving the desirable food is controlled by human eye. At the tip of the robot arm, tiny camera is equipped. Disabled person wears a glass of which a single Head Mount Display: HMD and tiny camera is mounted so that disabled person can look at the desired food and retrieve it by looking at the food displayed onto HMD. This is just an example. There are a plenty of available services as a magic arm. Experimental results show that disabled person can retrieve the desired food successfully. It also is confirmed that robot arm control by EBHCI is much faster than that by hands. © Springer Nature Switzerland AG 2020.},
	journal = {Lecture Notes in Networks and Systems},
	author = {Arai, K.},
	year = {2020},
	keywords = {1include\_search1, 1scopus, 1search1},
	pages = {535--545},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/GG76Y7LK/Arai - 2020 - Service robot arm controlled just by sight.pdf:application/pdf},
}

@article{weber_follow_2018,
	title = {Follow me: {Real}-time in the wild person tracking application for autonomous robotics},
	volume = {11175 LNAI},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053880253&doi=10.1007%2f978-3-030-00308-1_13&partnerID=40&md5=59ad6f337dff44435a41f6fe0c7ed4ea},
	doi = {10.1007/978-3-030-00308-1_13},
	abstract = {In the last 20 years there have been major advances in autonomous robotics. In IoT (Industry 4.0), mobile robots require more intuitive interaction possibilities with humans in order to expand its field of applications. This paper describes a user-friendly setup, which enables a person to lead the robot in an unknown environment. The environment has to be perceived by means of sensory input. For realizing a cost and resource efficient Follow Me application we use a single monocular camera as low-cost sensor. For efficient scaling of our Simultaneous Localization and Mapping (SLAM) algorithm, we integrate an inertial measurement unit (IMU) sensor. With the camera input we detect and track a person. We propose combining state of the art deep learning with Convolutional Neural Network (CNN) and SLAM algorithms functionality on the same input camera image. Based on the output robot navigation is possible. This work presents the specification, workflow for an efficient development of the Follow Me application. Our application’s delivered point clouds are also used for surface construction. For demonstration, we use our platform SCITOS G5 equipped with the afore mentioned sensors. Preliminary tests show the system works robustly in the wild (This work is partially supported by a grant of the BMBF FHprofUnt program, no. 03FH049PX5). © Springer Nature Switzerland AG 2018.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Weber, T. and Triputen, S. and Danner, M. and Braun, S. and Schreve, K. and Rätsch, M.},
	year = {2018},
	keywords = {1scopus},
	pages = {156--167},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/LQSHE4RE/Weber et al. - 2018 - Follow me Real-time in the wild person tracking a.pdf:application/pdf},
}

@inproceedings{hong_interactive_2018,
	title = {Interactive humanoid robot arm imitation system using human upper limb motion tracking},
	volume = {2018-January},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049920911&doi=10.1109%2fROBIO.2017.8324706&partnerID=40&md5=71d564132bfb9ea5b35ae1710abb2574},
	doi = {10.1109/ROBIO.2017.8324706},
	abstract = {Robot arm imitation based on human upper limb motion tracking possesses huge potential application value, especially with the addition of hazardous environments. It provides an intuitive way of teaching a complicated humanoid robot arm to perform human-like behaviors. In this paper, we build an interactive imitation system composed of Kinect and a self-built humanoid robot arm, which could mimic human upper limb motions in real time. The Kinect device captures the positions and joint orientations of the participant's upper limb. A method of determining the credibility of estimated joints is put forward, and along with Savitzky-Golay smoothing filter, we decrease jitter caused by measurement errors of Kinect. Experiments indicate that the robot arm system has a certain flexibility to accomplish a variety of human upper limb motions. © 2017 IEEE.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Robotics} and {Biomimetics}, {ROBIO} 2017},
	author = {Hong, C. and Chen, Z. and Zhu, J. and Zhang, X.},
	year = {2018},
	keywords = {1scopus},
	pages = {2746--2751},
	annote = {cited By 1},
	file = {Full Text:/Users/tid010/Zotero/storage/BIUFNVWJ/Hong et al. - 2018 - Interactive humanoid robot arm imitation system us.pdf:application/pdf},
}

@article{han_human_2017,
	title = {Human robot interaction method by using hand gesture recognition},
	volume = {448},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019671922&doi=10.1007%2f978-981-10-5041-1_17&partnerID=40&md5=ccd4ea717228ac96a33931e447f2ed86},
	doi = {10.1007/978-981-10-5041-1_17},
	abstract = {Recently, human robot interaction technique is limelight in which controlling robot by using natural user interface such as using body gesture, eye tracking or human voice recognition. In this paper, hand gesture based robot controlling method is proposed. In the present study, a new natural user interface model is hierachically designed. The hand gesture is recognized based on Kinect V2. The recognized gesture information is send to the robot controlling robot through Bluetooth 4.0 interface. The developed robot controlling interface can be adopted into the field of disaster area in order to restore or surveillance the area of human inavailable. Experimental result showed that the proposed method can be effectively used for controlling robot. © Springer Nature Singapore Pte Ltd. 2017.},
	journal = {Lecture Notes in Electrical Engineering},
	author = {Han, J. and Jang, W. and Jung, D. and Lee, E.C.},
	year = {2017},
	keywords = {1scopus},
	pages = {97--102},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/KQYHIQCZ/Han et al. - 2017 - Human robot interaction method by using hand gestu.pdf:application/pdf},
}

@inproceedings{ali_human_2013,
	title = {Human detection and following by a mobile robot using {3D} features},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887895676&doi=10.1109%2fICMA.2013.6618174&partnerID=40&md5=f6ce9ddc6a5c9d568e32f05d183ee959},
	doi = {10.1109/ICMA.2013.6618174},
	abstract = {Human-robot interaction is one of the most basic requirements for service robots. In order to provide the desired service, these robots are required to detect and track human beings in the environment. This paper presents a novel approach for classifying a target person in a crowded environment. The system used the approaches for human detection and following by implementing the multi-sensor data fusion technique using stereo camera and laser range finder (LRF). Our system tracks human being by gathering features of human upper body and face in 3D space from stereo camera and uses laser rangefinder to get legs data. Using these data our system classifies the target person from other human beings in the environment. We used Haar cascade classifiers for the detection of upper body and face, and used stereo camera for getting dimensions in 3D space. The approach for gathering legs data is based on the recognition of legs pattern extracted from laser scan. Tracking of target person is done using Cam Shift theorem. Using all these techniques we present a novel approach for target person classification and tracking. Our approach is feasible for mobile robots with an identical device arrangement. © 2013 IEEE.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Mechatronics} and {Automation}, {IEEE} {ICMA} 2013},
	author = {Ali, B. and Iqbal, K.F. and Ayaz, Y. and Muhammad, N.},
	year = {2013},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {1714--1719},
	annote = {cited By 6},
	file = {Full Text:/Users/tid010/Zotero/storage/KS9SBLI5/Ali et al. - 2013 - Human detection and following by a mobile robot us.pdf:application/pdf},
}

@inproceedings{zhang_automating_2018,
	title = {Automating {Robotic} {Furniture} with {A} {Collaborative} {Vision}-based {Sensing} {Scheme}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058102008&doi=10.1109%2fROMAN.2018.8525783&partnerID=40&md5=993418baa209bf4ada7e163c51526a3c},
	doi = {10.1109/ROMAN.2018.8525783},
	abstract = {Automating teleoperated robots is an essential task for transforming human-robot interaction design into practical applications. In this paper, we present a Collaborative Vision-based Sensing Scheme (CVSS) for automating mobile robotic furniture in the household environment. Using multiple cameras to perceive users' spatial information and capture their body postures respectively, our sensing scheme can provide formerly teleoperated robots with sufficient situational awareness of the users' surroundings and enable them to understand the interactive willingness of humans. As an application instance, we introduce the design of a furniture-type robot, called Automan, whose prototype is a teleoperated mechanical ottoman reported in [1]. Utilizing CVSS, we enable Automan the similar functions as teleoperated ottoman to offer and withdraw services for humans. To evaluate our automation method, we conducted the subjective experiments with 20 participants to verify the effectiveness of it in comparison with the teleoperated ottomans in terms of interactive experience. And the result of paired samples t test indicates that the participants can't significantly distinguish whether the interactive ottoman is autonomous or teleoperated (p»0.05). Furthermore, we also explored and analyzed users' satisfaction for different behavior styles between autonomous and teleoperated ottomans. © 2018 IEEE.},
	booktitle = {{RO}-{MAN} 2018 - 27th {IEEE} {International} {Symposium} on {Robot} and {Human} {Interactive} {Communication}},
	author = {Zhang, Z. and Chen, Z. and Li, W.},
	year = {2018},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {719--725},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/AHHS7T95/Zhang et al. - 2018 - Automating Robotic Furniture with A Collaborative .pdf:application/pdf},
}

@inproceedings{cheng_design_2012,
	title = {Design and implementation of human-robot interactive demonstration system based on {Kinect}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866698912&doi=10.1109%2fCCDC.2012.6242992&partnerID=40&md5=c3a4ece707f1b3ca77e72db4854b949b},
	doi = {10.1109/CCDC.2012.6242992},
	abstract = {With the development of technology, humanoid robots gradually enter our life, not only for education, but helping people with housework and many other tasks, some of which are inevitable for human. But only few people know how to control and interact with a humanoid robot, which hinders the development of humanoid robot. So a human-robot interactive demonstration system is designed to help non-expert users to control the humanoid robot, Aldebaran humanoid robot Nao in this case. Users just need to use the most natural body gestures to interact with the robot. Microsoft Kinect is applied in this system to recognize different body gestures and generate visual Human-Robot interaction interface, then the controlling signals of different body gesture modules are sent to Nao through wifi, which can stimulate Nao to complete tasks. This kind of system aims to enrich the interactive way between human and robots and help non-expert users to control the robot freely, making Human-Robot interaction much easier. © 2012 IEEE.},
	booktitle = {Proceedings of the 2012 24th {Chinese} {Control} and {Decision} {Conference}, {CCDC} 2012},
	author = {Cheng, L. and Sun, Q. and Su, H. and Cong, Y. and Zhao, S.},
	year = {2012},
	keywords = {1scopus},
	pages = {971--975},
	annote = {cited By 55},
	file = {Full Text:/Users/tid010/Zotero/storage/QURAYCU4/Cheng et al. - 2012 - Design and implementation of human-robot interacti.pdf:application/pdf},
}

@inproceedings{christiernin_interacting_2016,
	title = {Interacting with industrial robots - {A} motion-based interface},
	volume = {07-10-June-2016},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977068107&doi=10.1145%2f2909132.2926073&partnerID=40&md5=e95420f391911b23b741190f546da2ad},
	doi = {10.1145/2909132.2926073},
	abstract = {Collaborative industrial robot cells are becoming more and more interesting for industry through the new Industrie 4.0 initiative. In this paper we report early work on motion-based interaction with industrial robots. Human motion is tracked by a Kinect camera and translated into robot code. A group of tests subjects are asked to interact with the system and their activities are observed. Lessons learned on interaction challenges in a robot cell are reported. © 2016 Copyright held by the owner/author(s).},
	booktitle = {Proceedings of the {Workshop} on {Advanced} {Visual} {Interfaces} {AVI}},
	author = {Christiernin, L.G. and Augustsson, S.},
	year = {2016},
	keywords = {1scopus},
	pages = {310--311},
	annote = {cited By 1},
	file = {Full Text:/Users/tid010/Zotero/storage/NDQCV7Q7/Christiernin and Augustsson - 2016 - Interacting with industrial robots - A motion-base.pdf:application/pdf},
}

@inproceedings{shariatee_safe_2017,
	title = {Safe collaboration of humans and {SCARA} robots},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017500504&doi=10.1109%2fICRoM.2016.7886809&partnerID=40&md5=30603bb3cd454423ae99d2fc00b8cff4},
	doi = {10.1109/ICRoM.2016.7886809},
	abstract = {This paper presents a method to determine distance between human operator and SCARA robot using computer vision in order to provide a safe workstation for human robot collaboration. Kinect sensor is used as the input device to the system. Kinect has four streams of data among which depth data is effectively used in this approach. The measured distance is used to calculate danger index. Online trajectory generation allows the robot to perform appropriate action. The results of applying this system to FUM SCARA is presented. © 2016 IEEE.},
	booktitle = {4th {RSI} {International} {Conference} on {Robotics} and {Mechatronics}, {ICRoM} 2016},
	author = {Shariatee, M. and Khosravi, H. and Fazl-Ersi, E.},
	year = {2017},
	keywords = {1scopus},
	pages = {589--594},
	annote = {cited By 3},
	file = {Full Text:/Users/tid010/Zotero/storage/WT6ZMG7W/Shariatee et al. - 2017 - Safe collaboration of humans and SCARA robots.pdf:application/pdf},
}

@article{ali_improved_2015,
	title = {Improved method for stereo vision-based human detection for a mobile robot following a target person},
	volume = {26},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84935422209&doi=10.7166%2f26-1-891&partnerID=40&md5=ef1898e34047dfb696e7a85e4ed7865b},
	doi = {10.7166/26-1-891},
	abstract = {Interaction between humans and robots is a fundamental need for assistive and service robots. Their ability to detect and track people is a basic requirement for interaction with human beings. This article presents a new approach to human detection and targeted person tracking by a mobile robot. Our work is based on earlier methods that used stereo vision-based tracking linked directly with Hu moment-based detection. The earlier technique was based on the assumption that only one person is present in the environment – the target person – and it was not able to handle more than this one person. In our novel method, we solved this problem by using the Haar-based human detection method, and included a target person selection step before initialising tracking. Furthermore, rather than linking the Kalman filter directly with human detection, we implemented the tracking method before the Kalman filter-based estimation. We used the Pioneer 3AT robot, equipped with stereo camera and sonars, as the test platform. © 2015, South African Institute of Industrial Engineering. All rights reserved.},
	number = {1},
	journal = {South African Journal of Industrial Engineering},
	author = {Ali, B. and Ayaz, Y. and Jamil, M. and Gilani, S.O. and Muhammad, N.},
	year = {2015},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {102--119},
	annote = {cited By 4},
	file = {Full Text:/Users/tid010/Zotero/storage/SMY5AIEE/Ali et al. - 2015 - Improved method for stereo vision-based human dete.pdf:application/pdf},
}

@article{chen_human-following_2019,
	title = {A {Human}-{Following} {Mobile} {Robot} {Providing} {Natural} and {Universal} {Interfaces} for {Control} with {Wireless} {Electronic} {Devices}},
	volume = {24},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074267031&doi=10.1109%2fTMECH.2019.2936395&partnerID=40&md5=f76b47991ce721520de61ef8e86f7501},
	doi = {10.1109/TMECH.2019.2936395},
	abstract = {This article presents the development of a mobile robot using a human-following algorithm based on human path prediction and provides a natural gesture interaction for an operator to control wireless electronic devices using a low-cost red-green-blue-depth (RGB-D) sensor. The overall experimental setup consists of a skid-steered mobile robot, Kinect sensor, laptop, red-green-blue (RGB) camera, and two lamps. OpenNI middleware is used to process the depth data from the Kinect sensor, and OpenCV is used to process data from the RGB camera. The human-following control system consists of two feedback control loops for linear and rotational motions, respectively. A lead-lag and proportional-integral derivative controllers are developed for the linear and rotational motion control loops, respectively. There are small delays (0.3 s for linear motion and 0.2 s for rotational motion) of the system's response. However, the delays are acceptable since they do not cause the tracking distance or angle out of our desirable range (±0.05 m and ±10° of the reference input). A new human-position prediction algorithm based on human orientation is proposed for human following. Experimental results show that the tracking algorithm reduces the distance and angular errors by 40\% and 50\%, respectively. There are four gestures designed for the operator to control the robot. Success rates of gesture recognition are more than 90\% within the detectable range of the Kinect sensor. © 2019 IEEE.},
	number = {5},
	journal = {IEEE/ASME Transactions on Mechatronics},
	author = {Chen, J. and Kim, W.-J.},
	year = {2019},
	keywords = {1scopus},
	pages = {2377--2385},
	annote = {cited By 2},
	file = {Full Text:/Users/tid010/Zotero/storage/JJGGMPPG/Chen and Kim - 2019 - A Human-Following Mobile Robot Providing Natural a.pdf:application/pdf},
}

@article{nishiyama_human_2013,
	title = {Human intervention for searching targets using mobile agents in a multi-robot environment},
	volume = {254},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896883602&doi=10.3233%2f978-1-61499-262-2-154&partnerID=40&md5=c679e94088bde8090533ee4ad7fbc8c2},
	doi = {10.3233/978-1-61499-262-2-154},
	abstract = {We propose an intelligent interface for the mobile software agents that we have developed. The interface should have two roles. One is to visualize the mobile software agents using augmented reality and the other is to give a human user the means to control the mobile software agents by gesture using a motion capture camera. Through the interface we human beings can intuitively grasp the activities of the mobile agents, i.e. through augmented reality. In order to provide proactive inputs from the user, we utilize the Kinect motion capture cameras to capture the human users' will. The Kinect motion capture camera is mounted on a mobile robot that is near the human user. The robot acts as a mediator that recognizes the human user's will and convey it to mobile software agents that control the mobile multiple robots. A mobile software agent is searching a target, and uses a mobile robot to seize the target when it finds the target. When the user points at the target or a mobile robot, the monitoring software captures the will of the user and conveys instructions to the mobile agent based on the information from the Kinect. The mobile agent migrates from one robot to another to look for the searching mobile agents and hands the instruction to which robot it should move. The agent migration is represented by an image's moving to the robot that was pointed. This paper reports the intelligent user interface that provides the interaction between the human user and the mobile agents as the first step toward the complete intelligent human computer interface. We demonstrate the usefulness through preliminary experiments. © 2013 The authors and IOS Press.},
	journal = {Frontiers in Artificial Intelligence and Applications},
	author = {Nishiyama, T. and Takimoto, M. and Kambayashi, Y.},
	year = {2013},
	keywords = {1scopus},
	pages = {154--163},
	annote = {cited By 0},
	file = {Nishiyama et al. - 2013 - Human intervention for searching targets using mob.pdf:/Users/tid010/Zotero/storage/X578UFP3/Nishiyama et al. - 2013 - Human intervention for searching targets using mob.pdf:application/pdf},
}

@article{jevtic_comparison_2015,
	title = {Comparison of {Interaction} {Modalities} for {Mobile} {Indoor} {Robot} {Guidance}: {Direct} {Physical} {Interaction}, {Person} {Following}, and {Pointing} {Control}},
	volume = {45},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959471947&doi=10.1109%2fTHMS.2015.2461683&partnerID=40&md5=fae590ee89573e87a8c7d583892bf0fa},
	doi = {10.1109/THMS.2015.2461683},
	abstract = {Three advanced natural interaction modalities for mobile robot guidance in an indoor environment were developed and compared using two tasks and quantitative metrics to measure performance and workload. The first interaction modality is based on direct physical interaction requiring the human user to push the robot in order to displace it. The second and third interaction modalities exploit a 3-D vision-based human-skeleton tracking allowing the user to guide the robot by either walking in front of it or by pointing toward a desired location. In the first task, the participants were asked to guide the robot between different rooms in a simulated physical apartment requiring rough movement of the robot through designated areas. The second task evaluated robot guidance in the same environment through a set of waypoints, which required accurate movements. The three interaction modalities were implemented on a generic differential drive mobile platform equipped with a pan-tilt system and a Kinect camera. Task completion time and accuracy were used as metrics to assess the users' performance, while the NASA-TLX questionnaire was used to evaluate the users' workload. A study with 24 participants indicated that choice of interaction modality had significant effect on completion time (F(2,61) = 84.874, p {\textless} 0.001), accuracy (F(2,29) = 4.937, p = 0.016), and workload (F(2,68) = 11.948, p {\textless} 0.001). The direct physical interaction required less time, provided more accuracy and less workload than the two contactless interaction modalities. Between the two contactless interaction modalities, the person-following interaction modality was systematically better than the pointing-control one: The participants completed the tasks faster with less workload. © 2015 IEEE.},
	number = {6},
	journal = {IEEE Transactions on Human-Machine Systems},
	author = {Jevtić, A. and Doisy, G. and Parmet, Y. and Edan, Y.},
	year = {2015},
	keywords = {1scopus},
	pages = {653--663},
	annote = {cited By 24},
	file = {Submitted Version:/Users/tid010/Zotero/storage/GXX7IZ9W/Jevtić et al. - 2015 - Comparison of Interaction Modalities for Mobile In.pdf:application/pdf},
}

@article{chen_person_2012,
	title = {Person following of a mobile robot using kinect through features detection based on {SURF}},
	volume = {542-543},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868363112&doi=10.4028%2fwww.scientific.net%2fAMR.542-543.779&partnerID=40&md5=b69c56ebcdab57195aab340cb6518a7a},
	doi = {10.4028/www.scientific.net/AMR.542-543.779},
	abstract = {Following a person is an important task for domestic service robots in applications in which human-robot interaction is a primary requirement. Two steps will be completed if the robot needs to achieves this task. It includes detecting the target person and following it. Thus, the robot needs applicable algorithm and specific sensor. In this paper features detection and following technology based on SURF (Speed Up Robust Features) algorithm is used for person following in domestic environments. The vision system of robot obtains very good features of the target person through the RGB camera of kinect (Kinect sensor device) using SURF algorithm. And the depth camera of kinect helps the robot obtain the accurate information about the position of the target person in the environment. It uses SURF algorithm to extract the features of the target person, and match them in following frames. The proposed method is programmed in high speed hardware system and using small zone person following method in order to meet the real time requirement. Experimental results are provided to demonstrate the effectiveness of the proposed approach. © (2012) Trans Tech Publications, Switzerland.},
	journal = {Advanced Materials Research},
	author = {Chen, W. and Guo, S.},
	year = {2012},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {779--784},
	annote = {cited By 2},
}

@inproceedings{xu_skeleton_2020,
	title = {Skeleton {Guided} {Conflict}-{Free} {Hand} {Gesture} {Recognition} for {Robot} {Control}},
	doi = {10.1109/iCAST51195.2020.9319483},
	abstract = {The skeleton analysis introduced by Kinect has been an efficient way for interaction. Skeleton analysis interaction is more intuitive and aligns better with human natural behaviors compared with traditional approaches. However, skeleton analysis often has the problem of producing conflict gesture identifications if two interaction movements are similar. Additionally, it always mistakenly recognizes some unconscious or intentional body movements as positive gestures. To this end, we proposed a new interaction method enhanced by both vision algorithms and deep learning. An improved residual neural network is employed to recognize gestures which are then used for distinguishing similar body movements. A combined human-computer interaction scheme is proposed which includes three main components: (a) a hand shape segmentation approach enhanced by skin color detection and skeleton joint tracking, (b) the deep learning augmented detection for changes of gestures and (c) a deep learning-based gesture command recognition for robot control. Experiments are conducted using the proposed method for robot interaction. The results demonstrate that unconscious body movements can be accurately identified. Similar body movements can also be distinguished robustly. The proposed method can run in real-time with competitive performance.},
	booktitle = {2020 11th {International} {Conference} on {Awareness} {Science} and {Technology} ({iCAST})},
	author = {Xu, J. and Li, J. and Zhang, S. and Xie, C. and Dong, J.},
	month = dec,
	year = {2020},
	note = {ISSN: 2325-5994},
	keywords = {deep learning, robot control, Robots, Human computer interaction, Gesture recognition, Kinect, Image segmentation, gesture recognition, Skeleton, Image color analysis, Skin, skeleton-based, skin detection, 1xplore},
	pages = {1--6},
	file = {Full Text:/Users/tid010/Zotero/storage/JRMU53SH/Xu et al. - 2020 - Skeleton Guided Conflict-Free Hand Gesture Recogni.pdf:application/pdf},
}

@article{chalvatzaki_learn_2019,
	title = {Learn to {Adapt} to {Human} {Walking}: {A} {Model}-{Based} {Reinforcement} {Learning} {Approach} for a {Robotic} {Assistant} {Rollator}},
	volume = {4},
	issn = {2377-3766},
	doi = {10.1109/LRA.2019.2929996},
	abstract = {In this letter, we tackle the problem of adapting the motion of a robotic assistant rollator to patients with different mobility status. The goal is to achieve a coupled human–robot motion in a front-following setting as if the patient was pushing the rollator himself/herself. To this end, we propose a novel approach using model-based reinforcement learning (MBRL) for adapting the control policy of the robotic assistant. This approach encapsulates our previous work on human tracking and gait analysis from RGB-D and laser streams into a human-in-the-loop decision making strategy. We use long short-term memory (LSTM) networks for designing a human motion intention model and a coupling parameters forecast model, leveraging on the outcome of human gait analysis. An initial LSTM-based policy network was trained via imitation learning from human demonstrations in a motion capture setup. This policy is then fine-tuned with the MBRL framework using tracking data from real patients. A thorough evaluation analysis proves the efficiency of the MBRL approach as a user-adaptive controller.},
	number = {4},
	journal = {IEEE Robotics and Automation Letters},
	author = {Chalvatzaki, G. and Papageorgiou, X. S. and Maragos, P. and Tzafestas, C. S.},
	month = oct,
	year = {2019},
	keywords = {Legged locomotion, Navigation, Learning systems, Robot kinematics, Robot sensing systems, learning and adaptive systems, Predictive models, Adaptive systems, automation in life sciences: biotechnology, Biotechnology, Human-centered robotics, pharmaceutical and health care, 1xplore},
	pages = {3774--3781},
	file = {Full Text:/Users/tid010/Zotero/storage/XUM7UTFS/Chalvatzaki et al. - 2019 - Learn to Adapt to Human Walking A Model-Based Rei.pdf:application/pdf},
}

@inproceedings{milligan_selecting_2011,
	title = {Selecting and commanding groups in a multi-robot vision based system},
	doi = {10.1145/1957656.1957809},
	abstract = {We present a novel method for a human user to select groups of robots without using any external instruments. We use computer vision techniques to read hand gestures from a user and use the hand gesture information to select single or multiple robots from a population and assign them to a task. To select robots the user simply draws a circle in the air around the robots that the user wants to command. Once the user selects the group of robots, he or she can send them to a location by pointing to a target location. To achieve this we use cameras mounted on mobile robots to find the user's face and then track his or her hand. Our method exploits an observation from human-robot interaction on pointing, which found a human's target when pointing is best inferred using the line from the human's eyes to the user's extended hand[1]. When circling robots the projected eye-to-hand lines forms a cone-like shape that envelops the selected robots. From a 2D camera mounted on the robot, this cone is seen with the user's face as the vertex and the hand movements as a circular slice of the cone. We show in the video how the robots can tell if they have been selected by testing to see if the face is within the circle made by the hand. If the face is within the circle then the robot was selected, if the face is outside the circle it was not selected. Following selection the robots then read a command by looking for a pointing gesture, which is detected by an outreached hand. From the pointing gesture the robots collectively infer which target is pointing at by calculating the distance and direction that the hand moved to relative to the face. The selected robots then travel to the target, and unselected robots can then be selected and commanded as desired. The robots communicate their state to the user through LED lights on the robots chassis. When a robot is searching for the user's face the LEDs flash to get the user's attention (as it is easiest to find frontal faces). When the robots find the users face the lights become a solid yellow to indicate that they are ready to be selected. When selected, the robots' LEDs turn blue to indicate they can now be commanded. Once robots are sent off to a location, remaining robots can then be selected and assigned another task. We demonstrate this method working on low powered Atom Netbooks and off the shelf USB web cameras. This shows the first working implementation of a system that allows a human to select and command groups of robots with out using any external instruments.},
	booktitle = {2011 6th {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction} ({HRI})},
	author = {Milligan, B. and Mori, G. and Vaughan, R.},
	month = mar,
	year = {2011},
	note = {ISSN: 2167-2148},
	keywords = {robot vision, mobile robots, Humans, Cameras, Robot vision systems, Robot kinematics, Face, human-robot interaction, hand gestures, gesture recognition, computer vision techniques, multi-robot systems, image sensors, video signal processing, Educational institutions, Computer Vision, pointing gesture, 2D camera, circling robots, cone-like shape, external instruments, group commanding, group selection, LED lights, low powered atom netbooks, Multiple Robots, multirobot vision based system, Pointing, projected eye-to-hand lines, robot LED, robots chassis, Selection, shelf USB Web cameras, Task allocation and coordination, User Feedback, video, 1xplore},
	pages = {415--415},
	file = {Full Text:/Users/tid010/Zotero/storage/GMV5D58G/Milligan et al. - 2011 - Selecting and commanding groups in a multi-robot v.pdf:application/pdf},
}

@inproceedings{petric_online_2014,
	title = {Online approach for altering robot behaviors based on human in the loop coaching gestures},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929192146&doi=10.1109%2fICRA.2014.6907557&partnerID=40&md5=afa80707d4bac5f028839035d3aa3943},
	doi = {10.1109/ICRA.2014.6907557},
	abstract = {The creation and adaptation of motor behaviors is an important capability for autonomous robots. In this paper we propose an approach for altering existing robot behaviors online, where a human coach interactively changes the robot motion to achieve the desired outcome. Using hand gestures, the human coach can specify the desired modifications to the previously acquired behavior. To preserve a natural posture while performing the task, the movement is encoded in the robot's joint space using periodic dynamic movement primitives. The coaching gestures are mapped to the robot joint space via robot Jacobian and used to create a virtual force field affecting the movement. A recursive least squares technique is used to modify the existing movement with respect to the virtual force field. The proposed approach was evaluated on a simulated three degrees of freedom planar robot and on a real humanoid robot, where human coaching gestures were captured by an RGB-D sensor. Although our focus was on rhythmic movements, the developed approach is also applicable to discrete (point-to-point) movements. © 2014 IEEE.},
	booktitle = {Proceedings - {IEEE} {International} {Conference} on {Robotics} and {Automation}},
	author = {Petric, T. and Gams, A. and Zlajpah, L. and Ude, A. and Morimoto, J.},
	year = {2014},
	keywords = {1scopus},
	pages = {4770--4776},
	annote = {cited By 11},
	file = {Full Text:/Users/tid010/Zotero/storage/CKHGBQGG/Petric et al. - 2014 - Online approach for altering robot behaviors based.pdf:application/pdf},
}

@inproceedings{kalidolda_towards_2018,
	title = {Towards {Interpreting} {Robotic} {System} for {Fingerspelling} {Recognition} in {Real} {Time}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045263087&doi=10.1145%2f3173386.3177085&partnerID=40&md5=f82ad8422f6f3be395a1b2d0b258d936},
	doi = {10.1145/3173386.3177085},
	abstract = {Hearing-impaired communities around the world communicate via a sign language. The focus of this work is to develop an interpreting human-robot interaction system that could act as a sign language interpreter in public places. This paper presents an ongoing work, which aims to recognize fingerspelling gestures in real time. To this end, we utilize a deep learning method for classification of 33 gestures used for fingerspelling by the local deaf-mute community. In order to train and test the performance of the recognition system, we utilize previously collected dataset of motion RGB-D data of 33 manual gestures. After applying it to a deep learning method, we achieved an offline result of an average accuracy of 75\% for a complete alphabet recognition. In real time, the result was only 24.72\%. In addition, we integrated a form of auto-correction in order to perform spell-checking on the recognized letters. Among 35 tested words, four words were recognized correctly (11.4\%). Finally, we conducted an exploratory study inviting ten deaf individuals to interact with our sign language interpreting robotic system. © 2018 Authors.},
	booktitle = {{ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	author = {Kalidolda, N. and Sandygulova, A.},
	year = {2018},
	keywords = {1scopus},
	pages = {141--142},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/Z25EF2FS/Kalidolda and Sandygulova - 2018 - Towards Interpreting Robotic System for Fingerspel.pdf:application/pdf},
}

@article{koustoumpardis_human_2016,
	title = {Human robot collaboration for folding fabrics based on force/{RGB}-{D} feedback},
	volume = {371},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983168571&doi=10.1007%2f978-3-319-21290-6_24&partnerID=40&md5=639e360a68ae9e129563351783825240},
	doi = {10.1007/978-3-319-21290-6_24},
	abstract = {In this paper, the human-robot collaboration for executing complicated handling tasks for folding non-rigid objects is investigated. A hierarchical control system is developed for the co-manipulation task of folding sheets like fabrics/cloths. The system is based on force and RGB-D feedback in both higher and lower control levels of the process. In the higher level, the perception of the human’s intention is used for deciding the robot’s action; in the lower level the robot reacts to the force/RGB-D feedback to follow human guidance. The proposed approach is tested in folding a rectangular piece of fabric. Experiments showed that the developed robotic system is able to track the human’s movement in order to help her/him to accomplish the folding co-manipulation task. © Springer International Publishing Switzerland 2016.},
	journal = {Advances in Intelligent Systems and Computing},
	author = {Koustoumpardis, P.N. and Chatzilygeroudis, K.I. and Synodinos, A.I. and Aspragathos, N.A.},
	year = {2016},
	keywords = {1scopus},
	pages = {235--243},
	annote = {cited By 9},
	file = {Full Text:/Users/tid010/Zotero/storage/NN9X7QM5/Koustoumpardis et al. - 2016 - Human robot collaboration for folding fabrics base.pdf:application/pdf},
}

@inproceedings{chen_stereovision-only_2014,
	title = {Stereovision-only based interactive mobile robot for human-robot face-to-face interaction},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919934212&doi=10.1109%2fICPR.2014.322&partnerID=40&md5=58c3d82ff292bf8dd2f560159dacb3ad},
	doi = {10.1109/ICPR.2014.322},
	abstract = {In this paper, we present a stereovision-only based interactive mobile robot for supporting human-robot face-to-face interaction in the real world. A three-level architecture, which consists of sensor level, perception level and behavior level, is designed for the robot in order to perceive, understand and react to the human activity during interaction based only on visual information. A high performance stand-alone stereovision system (RGBD imager), developed in our lab, is applied to obtain the composite of color (RGB) images and dense disparity (D) maps at video rate. The RGBD imager allows the robot a human-like 3-D visual perception ability to (1) autonomously detect the human of interest whom the robot could interact with using the offline learning approaches, and (2) focus exclusively on the target human while both the human and the robot are moving during interaction using on-line learning approaches. We demonstrate and evaluate the performance of our interactive mobile robot in an office environment. The experimental results show that a reliable and dynamic face-to-face interaction is achieved, so that the target human face is always kept in the field of view and at a suitable social distance from the robot. © 2014 IEEE.},
	booktitle = {Proceedings - {International} {Conference} on {Pattern} {Recognition}},
	author = {Chen, L. and Dong, Z. and Gao, S. and Yuan, B. and Pei, M.},
	year = {2014},
	keywords = {1scopus},
	pages = {1840--1845},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/XXTNX4CQ/Chen et al. - 2014 - Stereovision-only based interactive mobile robot f.pdf:application/pdf},
}

@inproceedings{alonso-mora_human_2014,
	title = {Human - {Robot} swarm interaction for entertainment: {From} animation display to gesture based control},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897001353&doi=10.1145%2f2559636.2559645&partnerID=40&md5=c406ba9d41341203ee82974fe6a3e50c},
	doi = {10.1145/2559636.2559645},
	abstract = {This work shows experimental results with three systems that take real-time user input to direct a robot swarm formed by tens of small robots. These are: real-time drawing, gesture based interaction with an RGB-D sensor and control via a hand-held tablet computer.},
	booktitle = {{ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	author = {Alonso-Mora, J. and Siegwart, R. and Beardsley, P.},
	year = {2014},
	keywords = {1scopus},
	pages = {98},
	annote = {cited By 7},
	file = {Full Text:/Users/tid010/Zotero/storage/T6PH3MBB/Alonso-Mora et al. - 2014 - Human - Robot swarm interaction for entertainment.pdf:application/pdf},
}

@inproceedings{lichtenstern_prototyping_2012,
	title = {A prototyping environment for interaction between a human and a robotic multi-agent system},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859949210&doi=10.1145%2f2157689.2157747&partnerID=40&md5=b70fab00e14d070c7c5dd91ac09d95b2},
	doi = {10.1145/2157689.2157747},
	abstract = {In this paper we describe our prototyping environment to study concepts for empowering a single user to control robotic multi-agent systems. We investigate and validate these concepts by experiments with a fleet of hovering robots. Specifically, we report on a first experiment in which one robot is equipped with an RGB-D sensor through which the user is enabled to directly interact with a multi-agent system without the need to carry any device. © 2012 Authors.},
	booktitle = {{HRI}'12 - {Proceedings} of the 7th {Annual} {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	author = {Lichtenstern, M. and Frassl, M. and Perun, B. and Angermann, M.},
	year = {2012},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {185--186},
	annote = {cited By 26},
	file = {Full Text:/Users/tid010/Zotero/storage/ZAXYHJTA/Lichtenstern et al. - 2012 - A prototyping environment for interaction between .pdf:application/pdf},
}

@article{maurtua_natural_2017,
	title = {Natural multimodal communication for human-robot collaboration},
	volume = {14},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027442420&doi=10.1177%2f1729881417716043&partnerID=40&md5=e0b6f55211dd4253209abd2f35460e71},
	doi = {10.1177/1729881417716043},
	abstract = {This article presents a semantic approach for multimodal interaction between humans and industrial robots to enhance the dependability and naturalness of the collaboration between them in real industrial settings. The fusion of several interaction mechanisms is particularly relevant in industrial applications in which adverse environmental conditions might affect the performance of vision-based interaction (e.g. poor or changing lighting) or voice-based interaction (e.g. environmental noise). Our approach relies on the recognition of speech and gestures for the processing of requests, dealing with information that can potentially be contradictory or complementary. For disambiguation, it uses semantic technologies that describe the robot characteristics and capabilities as well as the context of the scenario. Although the proposed approach is generic and applicable in different scenarios, this article explains in detail how it has been implemented in two real industrial cases in which a robot and a worker collaborate in assembly and deburring operations. © The Author(s) 2017.},
	number = {4},
	journal = {International Journal of Advanced Robotic Systems},
	author = {Maurtua, I. and Fernández, I. and Tellaeche, A. and Kildal, J. and Susperregi, L. and Ibarguren, A. and Sierra, B.},
	year = {2017},
	keywords = {1scopus},
	pages = {1--12},
	annote = {cited By 14},
	file = {Full Text:/Users/tid010/Zotero/storage/5V3RTRB7/Maurtua et al. - 2017 - Natural multimodal communication for human-robot c.pdf:application/pdf},
}

@article{tarbouriech_bi-objective_2020,
	title = {Bi-objective {Motion} {Planning} {Approach} for {Safe} {Motions}: {Application} to a {Collaborative} {Robot}},
	volume = {99},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075242575&doi=10.1007%2fs10846-019-01110-1&partnerID=40&md5=f28e1890282d75fcc5659428be1695de},
	doi = {10.1007/s10846-019-01110-1},
	abstract = {This paper presents a new bi-objective safety-oriented path planning strategy for robotic manipulators. Integrated into a sampling-based algorithm, our approach can successfully enhance the task safety by guiding the expansion of the path towards the safest configurations. Our safety notion consists of avoiding dangerous situations, e.g. being very close to the obstacles, human awareness, e.g. being as much as possible in the human vision field, as well as ensuring human safety by being as far as possible from human with hierarchical priority between human body parts. Experimental validations are conducted in simulation and on the real Baxter research robot. They revealed the efficiency of the proposed method, mainly in the case of a collaborative robot sharing the workspace with humans. © 2019, Springer Nature B.V.},
	number = {1},
	journal = {Journal of Intelligent and Robotic Systems: Theory and Applications},
	author = {Tarbouriech, S. and Suleiman, W.},
	year = {2020},
	keywords = {1scopus},
	pages = {45--63},
	annote = {cited By 0},
	file = {Accepted Version:/Users/tid010/Zotero/storage/UWBQD27T/Tarbouriech and Suleiman - 2020 - Bi-objective Motion Planning Approach for Safe Mot.pdf:application/pdf},
}

@article{hwang_interactions_2020,
	title = {Interactions between {Specific} {Human} and {Omnidirectional} {Mobile} {Robot} {Using} {Deep} {Learning} {Approach}: {SSD}-{FN}-{KCF}},
	volume = {8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082023753&doi=10.1109%2fACCESS.2020.2976712&partnerID=40&md5=f8c1e3708809ee393985caa87047e910},
	doi = {10.1109/ACCESS.2020.2976712},
	abstract = {To fulfill the tasks of human-robot interaction (HRI), how to detect the specific human (SH) becomes paramount. In this paper, the deep learning approach by the integration of Single-Shot Detection, FaceNet, and Kernelized Correlation Filter (SSD-FN-KCF) is developed. From the outset, the SSD is employed to detect the human up to 8m using the RGB-D camera with 320× 240 resolution. Afterward the omnidirectional mobile robot (ODMR) is driven to the neighborhood of 2.5∼ 3.0m such that the depth image can accurately estimate the detected human's pose. Subsequently, the ODMR is commanded to the vicinity of 1.0m and the orientation inside-6060° with respect to the optical axis to identify whether he/she is the SH by the FaceNet. To reduce the computation time of the FaceNet and extend the SH's tracking, the KCF is employed to achieve the task of HRI (e.g., human following). Based on the image processing result, the required pose for searching or tracking (specific) human is accomplished by the image-based adaptive finite-time hierarchical constraint control. Finally, the experiment with the SH, who is far from and on the backside of the ODMR, validates the effectiveness and robustness of the proposed approach. © 2013 IEEE.},
	journal = {IEEE Access},
	author = {Hwang, C.-L. and Wang, D.-S. and Weng, F.-C. and Lai, S.-L.},
	year = {2020},
	keywords = {1scopus},
	pages = {41186--41200},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/APFAT7C8/Hwang et al. - 2020 - Interactions between Specific Human and Omnidirect.pdf:application/pdf},
}

@inproceedings{mead_probabilistic_2012,
	title = {A probabilistic framework for autonomous proxemic control in situated and mobile human-robot interaction},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859938200&doi=10.1145%2f2157689.2157751&partnerID=40&md5=8289355e149acd5e9ccad15c91432d87},
	doi = {10.1145/2157689.2157751},
	abstract = {In this paper, we draw upon insights gained in our previous work on human-human proxemic behavior analysis to develop a novel method for human-robot proxemic behavior production. A probabilistic framework for spatial interaction has been developed that considers the sensory experience of each agent (human or robot) in a co-present social encounter. In this preliminary work, a robot attempts to maintain a set of human body features in its camera field-of-view. This methodology addresses the functional aspects of proxemic behavior in human-robot interaction, and provides an elegant connection between previous approaches. © 2012 Authors.},
	booktitle = {{HRI}'12 - {Proceedings} of the 7th {Annual} {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	author = {Mead, R. and Mataric, M.J.},
	year = {2012},
	keywords = {1scopus},
	pages = {193--194},
	annote = {cited By 16},
	file = {Submitted Version:/Users/tid010/Zotero/storage/5QN2P6VD/Mead and Mataric - 2012 - A probabilistic framework for autonomous proxemic .pdf:application/pdf},
}

@article{vaufreydaz_starting_2016,
	title = {Starting engagement detection towards a companion robot using multimodal features},
	volume = {75},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948585868&doi=10.1016%2fj.robot.2015.01.004&partnerID=40&md5=edd7cef936287b615709e44028fd6c4d},
	doi = {10.1016/j.robot.2015.01.004},
	abstract = {Recognition of intentions is a subconscious cognitive process vital to human communication. This skill enables anticipation and increases the quality of interactions between humans. Within the context of engagement, non-verbal signals are used to communicate the intention of starting the interaction with a partner. In this paper, we investigated methods to detect these signals in order to allow a robot to know when it is about to be addressed. Originality of our approach resides in taking inspiration from social and cognitive sciences to perform our perception task. We investigate meaningful features, i.e. human readable features, and elicit which of these are important for recognizing someone's intention of starting an interaction. Classically, spatial information like the human position and speed, the human-robot distance are used to detect the engagement. Our approach integrates multimodal features gathered using a companion robot equipped with a Kinect. The evaluation on our corpus collected in spontaneous conditions highlights its robustness and validates the use of such a technique in a real environment. Experimental validation shows that multimodal features set gives better precision and recall than using only spatial and speed features. We also demonstrate that 7 selected features are sufficient to provide a good starting engagement detection score. In our last investigation, we show that among our full 99 features set, the space reduction is not a solved task. This result opens new researches perspectives on multimodal engagement detection. © 2015 Elsevier B.V. All rights reserved.},
	journal = {Robotics and Autonomous Systems},
	author = {Vaufreydaz, D. and Johal, W. and Combe, C.},
	year = {2016},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {4--16},
	annote = {cited By 18},
	file = {Submitted Version:/Users/tid010/Zotero/storage/E4R8Q6AW/Vaufreydaz et al. - 2016 - Starting engagement detection towards a companion .pdf:application/pdf},
}

@article{prediger_robot-supported_2014,
	title = {Robot-supported pointing interaction for intelligent environments},
	volume = {8530 LNCS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901611714&doi=10.1007%2f978-3-319-07788-8_17&partnerID=40&md5=56c9bc378a248ff5891d346e103b92bf},
	doi = {10.1007/978-3-319-07788-8_17},
	abstract = {A natural interaction with appliances in smart environment is a highly desired form of controlling the surroundings using intuitively learned interpersonal means of communication. Hand and arm gestures, recognized by depth cameras, are a popular representative of this interaction paradigm. However they usually require stationary units that limit applicability in larger environments. To overcome this problem we are introducing a self-localizing mobile robot system that autonomously follows the user in the environment, in order to recognize performed gestures independent from the current user position. We have realized a prototypical implementation using a custom robot platform and evaluated the system with various users. © 2014 Springer International Publishing Switzerland.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Prediger, M. and Braun, A. and Marinc, A. and Kuijper, A.},
	year = {2014},
	keywords = {1scopus},
	pages = {172--183},
	annote = {cited By 2},
	file = {Full Text:/Users/tid010/Zotero/storage/7BBXAT2L/Prediger et al. - 2014 - Robot-supported pointing interaction for intellige.pdf:application/pdf},
}

@inproceedings{luo_real-time_2019,
	title = {A {Real}-time {Moving} {Target} {Following} {Mobile} {Robot} {System} with {Depth} {Camera}},
	volume = {491},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063805494&doi=10.1088%2f1757-899X%2f491%2f1%2f012004&partnerID=40&md5=0cee9e3903bf2a25dbccc8b17c551ce4},
	doi = {10.1088/1757-899X/491/1/012004},
	abstract = {An intelligent system that could obtain a certain moving target human and could follow the target in real-time by a mobile robot could be widely applied in manufacturing as assistant or service industry to provide a human robot interaction experience. In this work, an improved efficient human following algorithm for a mobile robotic system is proposed. The system consists by a mobile robot and a depth camera which overcomes target scale variation, missing and occlusion. An improved three-dimensional tracking method is presented with the depth camera. Besides, the depth camera also provides information to avoid obstacles in robot's path. The omnidirectional mobile robot in the system provides a flexible and fast motion responding solution. In the experiments, the proposed system is tested in different scenes and the results show the reliability of the 3D tracking system in terms of accuracy and robustness to the environment. © Published under licence by IOP Publishing Ltd.},
	booktitle = {{IOP} {Conference} {Series}: {Materials} {Science} and {Engineering}},
	author = {Luo, X. and Zhang, D. and Jin, X.},
	year = {2019},
	note = {Issue: 1},
	keywords = {1scopus, 1search1, 1include\_search1},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/5PHZW5F9/Luo et al. - 2019 - A Real-time Moving Target Following Mobile Robot S.pdf:application/pdf},
}

@article{valle_personalized_2019,
	title = {Personalized {Robot} {Assistant} for {Support} in {Dressing}},
	volume = {11},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044325404&doi=10.1109%2fTCDS.2018.2817283&partnerID=40&md5=2b84223ca304c6a96cda2f5003fc05b3},
	doi = {10.1109/TCDS.2018.2817283},
	abstract = {Robot-assisted dressing is performed in close physical interaction with users who may have a wide range of physical characteristics and abilities. Design of user adaptive and personalized robots in this context is still indicating limited, or no consideration, of specific user-related issues. This paper describes the development of a multimodal robotic system for a specific dressing scenario-putting on a shoe, where users' personalized inputs contribute to a much improved task success rate. We have developed: 1) user tracking, gesture recognition, and posture recognition algorithms relying on images provided by a depth camera; 2) a shoe recognition algorithm from RGB and depth images; and 3) speech recognition and text-to-speech algorithms implemented to allow verbal interaction between the robot and user. The interaction is further enhanced by calibrated recognition of the users' pointing gestures and adjusted robot's shoe delivery position. A series of shoe fitting experiments have been performed on two groups of users, with and without previous robot personalization, to assess how it affects the interaction performance. Our results show that the shoe fitting task with the personalized robot is completed in shorter time, with a smaller number of user commands, and reduced workload. © 2019 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.},
	number = {3},
	journal = {IEEE Transactions on Cognitive and Developmental Systems},
	author = {Valle, A.F. and Alenyà, G. and Chance, G. and Caleb-Solly, P. and Dogramadzi, S. and Torras, C.},
	year = {2019},
	keywords = {1scopus},
	pages = {363--374},
	annote = {cited By 4},
	file = {Full Text:/Users/tid010/Zotero/storage/IFUBESAU/Valle et al. - 2019 - Personalized Robot Assistant for Support in Dressi.pdf:application/pdf},
}

@inproceedings{nair_3d_2011,
	title = {{3D} {Position} based multiple human servoing by low-level-control of 6 {DOF} industrial robot},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860773974&doi=10.1109%2fROBIO.2011.6181732&partnerID=40&md5=4414e75e0bbbb795b0adaccd49200e5c},
	doi = {10.1109/ROBIO.2011.6181732},
	abstract = {In this paper, we present a new vision-based multiple human tracking system. This novel 3D visual tracking system is capable of automatically identifying, labeling and tracking multiple humans in real-time even when they occlude each other. Furthermore, the multiple human tracker was implemented in a vision driven robot system for human robot interaction. The distributed system comprises of 4 subsystems: a)Multiple Human Tracking System, b) Robot Control System, c) 3D Visualization System and d) Remote Interface System. The Visual Tracking System performs real-time detection and tracking of humans in 3D within a large workspace. The Robot System uses the 3D position data of the targets obtained from the vision system to interact with the humans. The visual information is also used to monitor safe interaction within humans and robot. The Robot System is a 6DOF Stäubli TX90 industrial arm, controlled in real-time through a low-level interface. A real-time representation of the actual environment is rendered in 3D by the 3D Visualization System. The individual subsystems communicate with each other over a common communication engine based on TCP/IP. The complete system can be controlled and monitored through a wireless device. © 2011 IEEE.},
	booktitle = {2011 {IEEE} {International} {Conference} on {Robotics} and {Biomimetics}, {ROBIO} 2011},
	author = {Nair, S. and Dean-Leon, E. and Knoll, A.},
	year = {2011},
	keywords = {1scopus},
	pages = {2816--2823},
	annote = {cited By 0},
	file = {Submitted Version:/Users/tid010/Zotero/storage/EU2Q4E7Z/Nair et al. - 2011 - 3D Position based multiple human servoing by low-l.pdf:application/pdf},
}

@inproceedings{ke_vision_2016,
	title = {Vision system of facial robot {SHFR}- {III} for human-robot interaction},
	volume = {2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013041566&doi=10.5220%2f0005994804720478&partnerID=40&md5=3e7f3ea80d9d3bb9bdc365191b404d37},
	doi = {10.5220/0005994804720478},
	abstract = {The improvement of human-robot interaction is an inevitable trend for the development of robots. Vision is an important way for a robot to get the information from outside. Binocular vision model is set up on the facial expression robot SHFR- III, this paper develops a visual system for human-robot interaction, including face detection, face location, gender recognition, facial expression recognition and reproduction. The experimental results show that the vision system can conduct accurate and stable interaction, and the robot can carry out human-robot interaction. Copyright © 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.},
	booktitle = {{ICINCO} 2016 - {Proceedings} of the 13th {International} {Conference} on {Informatics} in {Control}, {Automation} and {Robotics}},
	author = {Ke, X. and Zhu, Y. and Yang, Y. and Xing, J. and Luo, Z.},
	year = {2016},
	keywords = {1scopus},
	pages = {472--478},
	annote = {cited By 0},
	file = {Submitted Version:/Users/tid010/Zotero/storage/GU6ZNXJI/Ke et al. - 2016 - Vision system of facial robot SHFR- III for human-.pdf:application/pdf},
}

@article{tao_multilayer_2013,
	title = {A multilayer hidden markov models-based method for human-robot interaction},
	volume = {2013},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886685293&doi=10.1155%2f2013%2f384865&partnerID=40&md5=177f0ce77ccb37f6ab7df6303230af87},
	doi = {10.1155/2013/384865},
	abstract = {To achieve Human-Robot Interaction (HRI) by using gestures, a continuous gesture recognition approach based on Multilayer Hidden Markov Models (MHMMs) is proposed, which consists of two parts. One part is gesture spotting and segment module, the other part is continuous gesture recognition module. Firstly, a Kinect sensor is used to capture 3D acceleration and 3D angular velocity data of hand gestures. And then, a Feed-forward Neural Networks (FNNs) and a threshold criterion are used for gesture spotting and segment, respectively. Afterwards, the segmented gesture signals are respectively preprocessed and vector symbolized by a sliding window and a K-means clustering method. Finally, symbolized data are sent into Lower Hidden Markov Models (LHMMs) to identify individual gestures, and then, a Bayesian filter with sequential constraints among gestures in Upper Hidden Markov Models (UHMMs) is used to correct recognition errors created in LHMMs. Five predefined gestures are used to interact with a Kinect mobile robot in experiments. The experimental results show that the proposed method not only has good effectiveness and accuracy, but also has favorable real-time performance. © 2013 Chongben Tao and Guodong Liu.},
	journal = {Mathematical Problems in Engineering},
	author = {Tao, C. and Liu, G.},
	year = {2013},
	keywords = {1scopus},
	annote = {cited By 1},
	file = {Full Text:/Users/tid010/Zotero/storage/2LGHCM6L/Tao and Liu - 2013 - A multilayer hidden markov models-based method for.pdf:application/pdf},
}

@article{chien_navigating_2019,
	title = {Navigating a service robot for indoor complex environments},
	volume = {9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061032208&doi=10.3390%2fapp9030491&partnerID=40&md5=c9ff553e93a32dbb4100cf3058b87f21},
	doi = {10.3390/app9030491},
	abstract = {This paper investigates the use of an autonomous service robot in an indoor complex environment, such as a hospital ward or a retirement home. This type of service robot not only needs to plan and find paths around obstacles, but must also interact with caregivers or patients. This study presents a type of service robot that combines the image from a 3D depth camera with infrared sensors, and the inputs from multiple sonar sensors in an Adaptive Neuro-Fuzzy Inference System (ANFIS)-based approach in path planning. In personal contacts, facial features are used to perform person recognition in order to discriminate between staff, patients, or a stranger. In the case of staff, the service robot can perform a follow-me function if requested. The robot can also use an additional feature which is to classify the person's gender. The purpose of facial and gender recognition includes helping to present choices for suitable destinations to the user. Experiments were done in cramped but open spaces, as well as confined passages scenarios, and in almost all cases, the autonomous robots were able to reach their destinations. © 2019 by the authors.},
	number = {3},
	journal = {Applied Sciences (Switzerland)},
	author = {Chien, J.-C. and Dang, Z.-Y. and Lee, J.-D.},
	year = {2019},
	keywords = {1scopus, 1search1, 1include\_search1},
	annote = {cited By 3},
	file = {Full Text:/Users/tid010/Zotero/storage/BKRK65DL/Chien et al. - 2019 - Navigating a service robot for indoor complex envi.pdf:application/pdf},
}

@article{mazhar_real-time_2019,
	title = {A real-time human-robot interaction framework with robust background invariant hand gesture detection},
	volume = {60},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066259834&doi=10.1016%2fj.rcim.2019.05.008&partnerID=40&md5=2daa5d897a3b09137e94d4f6327e83c9},
	doi = {10.1016/j.rcim.2019.05.008},
	abstract = {In the light of factories of the future, to ensure productive and safe interaction between robot and human coworkers, it is imperative that the robot extracts the essential information of the coworker. We address this by designing a reliable framework for real-time safe human-robot collaboration, using static hand gestures and 3D skeleton extraction. OpenPose library is integrated with Microsoft Kinect V2, to obtain a 3D estimation of the human skeleton. With the help of 10 volunteers, we recorded an image dataset of alpha-numeric static hand gestures, taken from the American Sign Language. We named our dataset OpenSign and released it to the community for benchmarking. Inception V3 convolutional neural network is adapted and trained to detect the hand gestures. To augment the data for training the hand gesture detector, we use OpenPose to localize the hands in the dataset images and segment the backgrounds of hand images, by exploiting the Kinect V2 depth map. Then, the backgrounds are substituted with random patterns and indoor architecture templates. Fine-tuning of Inception V3 is performed in three phases, to achieve validation accuracy of 99.1\% and test accuracy of 98.9\%. An asynchronous integration of image acquisition and hand gesture detection is performed to ensure real-time detection of hand gestures. Finally, the proposed framework is integrated in our physical human-robot interaction library OpenPHRI. This integration complements OpenPHRI by providing successful implementation of the ISO/TS 15066 safety standards for “safety rated monitored stop” and “speed and separation monitoring” collaborative modes. We validate the performance of the proposed framework through a complete teaching by demonstration experiment with a robotic manipulator. © 2019},
	journal = {Robotics and Computer-Integrated Manufacturing},
	author = {Mazhar, O. and Navarro, B. and Ramdani, S. and Passama, R. and Cherubini, A.},
	year = {2019},
	keywords = {1scopus},
	pages = {34--48},
	annote = {cited By 11},
	file = {Submitted Version:/Users/tid010/Zotero/storage/P82ZPKUC/Mazhar et al. - 2019 - A real-time human-robot interaction framework with.pdf:application/pdf},
}

@article{ghandour_human_2017,
	title = {Human robot interaction for hybrid collision avoidance system for indoor mobile robots},
	volume = {2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069497874&doi=10.25046%2faj020383&partnerID=40&md5=311e6d03d68816a9de3cd06d5ef71b19},
	doi = {10.25046/aj020383},
	abstract = {In this paper, a novel approach for collision avoidance for indoor mobile robots based on human-robot interaction is realized. The main contribution of this work is a new technique for collision avoidance by engaging the human and the robot in generating new collision-free paths. In mobile robotics, collision avoidance is critical for the success of the robots in implementing their tasks, especially when the robots navigate in crowded and dynamic environments, which include humans. Traditional collision avoidance methods deal with the human as a dynamic obstacle, without taking into consideration that the human will also try to avoid the robot, and this causes the people and the robot to get confused, especially in crowded social places such as restaurants, hospitals, and laboratories. To avoid such scenarios, a reactive-supervised collision avoidance system for mobile robots based on human-robot interaction is implemented. In this method, both the robot and the human will collaborate in generating the collision avoidance via interaction. The person will notify the robot about the avoidance direction via interaction, and the robot will search for the optimal collision-free path on the selected direction. In case that no people interacted with the robot, it will select the navigation path autonomously and select the path that is closest to the goal location. The humans will interact with the robot using gesture recognition and Kinect sensor. To build the gesture recognition system, two models were used to classify these gestures, the first model is Back-Propagation Neural Network (BPNN), and the second model is Support Vector Machine (SVM). Furthermore, a novel collision avoidance system for avoiding the obstacles is implemented and integrated with the HRI system. The system is tested on H20 robot from DrRobot Company (Canada) and a set of experiments were implemented to report the performance of the system in interacting with the human and avoiding collisions. © 2017 Advances in Science, Technology and Engineering Systems. All Rights Reserved.},
	number = {3},
	journal = {Advances in Science, Technology and Engineering Systems},
	author = {Ghandour, M. and Liu, H. and Stoll, N. and Thurow, K.},
	year = {2017},
	keywords = {1scopus},
	pages = {650--657},
	annote = {cited By 1},
	file = {Full Text:/Users/tid010/Zotero/storage/79RA4R7F/Ghandour et al. - 2017 - Human robot interaction for hybrid collision avoid.pdf:application/pdf},
}

@inproceedings{weinrich_appearance-based_2013,
	title = {Appearance-based {3D} upper-body pose estimation and person re-identification on mobile robots},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893534813&doi=10.1109%2fSMC.2013.748&partnerID=40&md5=e717ee3f91124ced6a44d841a2cbc19b},
	doi = {10.1109/SMC.2013.748},
	abstract = {In the field of human-robot interaction (HRI), detection, tracking and re-identification of humans in a robot's surroundings are crucial tasks, e. g. for socially compliant robot navigation. Besides the 3D position detection, the estimation of a person's upper-body orientation based on monocular camera images is a challenging problem on a mobile platform. To obtain real-time position tracking as well as upper-body orientation estimations, the proposed system comprises discriminative detectors whose hypotheses are tracked by a Kalman filter-based multihypotheses tracker. For appearance-based person recognition, a generative approach, based on a 3D shape model, is used to refine these tracked hypotheses. This model evaluates edges and color-based discrimination from the background. Furthermore, for each person the texture of his or her upper-body is learned and used for person re-identification. When computational resources are limited, the update rate of the model-based optimization reduces itself automatically. Thereby the estimation accuracy decreases, but the system keeps tracking the persons around the robot in real-time. The person's 3D pose is tracked up to a distance of 5.0 meters with an average Euclidean error of 18 cm. The achieved motion independent average upper-body orientation error is 22°. Furthermore, the upper-body texture is learned on-line which allowed a stable person re-identification in our experiments. © 2013 IEEE.},
	booktitle = {Proceedings - 2013 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics}, {SMC} 2013},
	author = {Weinrich, C. and Volkhardt, M. and Gross, H.-M.},
	year = {2013},
	keywords = {1scopus},
	pages = {4384--4390},
	annote = {cited By 7},
	file = {Submitted Version:/Users/tid010/Zotero/storage/6MY3M8TD/Weinrich et al. - 2013 - Appearance-based 3D upper-body pose estimation and.pdf:application/pdf},
}

@inproceedings{van_den_bergh_real-time_2011,
	title = {Real-time {3D} hand gesture interaction with a robot for understanding directions from humans},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053044858&doi=10.1109%2fROMAN.2011.6005195&partnerID=40&md5=40dc11174e4d4f314af533685e1a6ecc},
	doi = {10.1109/ROMAN.2011.6005195},
	abstract = {This paper implements a real-time hand gesture recognition algorithm based on the inexpensive Kinect sensor. The use of a depth sensor allows for complex 3D gestures where the system is robust to disturbing objects or persons in the background. A Haarlet-based hand gesture recognition system is implemented to detect hand gestures in any orientation, and more in particular pointing gestures while extracting the 3D pointing direction. The system is integrated on an interactive robot (based on ROS), allowing for real-time hand gesture interaction with the robot. Pointing gestures are translated into goals for the robot, telling him where to go. A demo scenario is presented where the robot looks for persons to interact with, asks for directions, and then detects a 3D pointing direction. The robot then explores his vicinity in the given direction and looks for a new person to interact with. © 2011 IEEE.},
	booktitle = {Proceedings - {IEEE} {International} {Workshop} on {Robot} and {Human} {Interactive} {Communication}},
	author = {Van Den Bergh, M. and Carton, D. and De Nijs, R. and Mitsou, N. and Landsiedel, C. and Kuehnlenz, K. and Wollherr, D. and Van Gool, L. and Buss, M.},
	year = {2011},
	keywords = {1scopus},
	pages = {357--362},
	annote = {cited By 127},
	file = {Full Text:/Users/tid010/Zotero/storage/8EQSGT93/Van Den Bergh et al. - 2011 - Real-time 3D hand gesture interaction with a robot.pdf:application/pdf},
}

@inproceedings{li_real-time_2019,
	title = {Real-{Time} {Human}-{Robot} {Interaction} for a {Service} {Robot} {Based} on {3D} {Human} {Activity} {Recognition} and {Human}-{Mimicking} {Decision} {Mechanism}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064983594&doi=10.1109%2fCYBER.2018.8688272&partnerID=40&md5=eb5c9608c29c5ad5972a2b1fef9fb8f2},
	doi = {10.1109/CYBER.2018.8688272},
	abstract = {In this paper, we present a real-time Human-Robot Interaction (HRI)system for a service robot based on 3D human activity recognition and human-mimicking decision mechanism. A three-layer Long-Short-Term Memory (LSTM)network is trained to recognize human activity. The input of this network is the 3D skeleton joints information captured by Kinect V1 and the output of this network is the class label of human activity. Moreover, the human-mimicking decision mechanism is also fused into the online test process of human activity recognition, which allows the robot to instinctively decide whether to interrupt the current task according to task priority. The robot can make appropriate responses based on aforementioned analytical results. The system framework is realized on the Robot Operating System (ROS). The real-life activity interaction between our service robot and the user was conducted to demonstrate the effectiveness of developed HRI system. © 2018 IEEE.},
	booktitle = {8th {Annual} {IEEE} {International} {Conference} on {Cyber} {Technology} in {Automation}, {Control} and {Intelligent} {Systems}, {CYBER} 2018},
	author = {Li, K. and Wu, J. and Zhao, X. and Tan, M.},
	year = {2019},
	keywords = {1scopus},
	pages = {498--503},
	annote = {cited By 3},
	file = {Submitted Version:/Users/tid010/Zotero/storage/7Q8Z33XA/Li et al. - 2019 - Real-Time Human-Robot Interaction for a Service Ro.pdf:application/pdf},
}

@article{cherubini_unified_2015,
	title = {A unified multimodal control framework for human-robot interaction},
	volume = {70},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929505391&doi=10.1016%2fj.robot.2015.03.002&partnerID=40&md5=4499620bfcaf68211c160f20f697b27d},
	doi = {10.1016/j.robot.2015.03.002},
	abstract = {In human-robot interaction, the robot controller must reactively adapt to sudden changes in the environment (due to unpredictable human behaviour). This often requires operating different modes, and managing sudden signal changes from heterogeneous sensor data. In this paper, we present a multimodal sensor-based controller, enabling a robot to adapt to changes in the sensor signals (here, changes in the human collaborator behaviour). Our controller is based on a unified task formalism, and in contrast with classical hybrid visicn-force-position control, it enables smooth transitions and weighted combinations of the sensor tasks. The approach is validated in a mock-up industrial scenario, where pose, vision (from both traditional camera and Kinect), and force tasks must be realized either exclusively or simultaneously, for human-robot collaboration. © 2015 Elsevier B.V. All rights reserved.},
	journal = {Robotics and Autonomous Systems},
	author = {Cherubini, A. and Passama, R. and Fraisse, P. and Crosnier, A.},
	year = {2015},
	keywords = {1scopus},
	pages = {106--115},
	annote = {cited By 27},
	file = {Submitted Version:/Users/tid010/Zotero/storage/AXZQ2WLR/Cherubini et al. - 2015 - A unified multimodal control framework for human-r.pdf:application/pdf},
}

@inproceedings{couture-beil_selecting_2010,
	title = {Selecting and commanding individual robots in a vision-based multi-robot system},
	doi = {10.1109/HRI.2010.5453167},
	abstract = {This video presents a computer vision based system for interaction between a single human and multiple robots. Face contact and motion-based gestures are used as two different non-verbal communication channels; a user first selects a particular robot by simply looking at it, then assigns it a task by waving his or her hand.},
	booktitle = {2010 5th {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction} ({HRI})},
	author = {Couture-Beil, A. and Vaughan, R. T. and Mori, G.},
	month = mar,
	year = {2010},
	note = {ISSN: 2167-2148},
	keywords = {robot vision, Cognitive robotics, Human robot interaction, Computer vision, Cameras, Robot vision systems, face recognition, human-robot interaction, Robot sensing systems, gesture recognition, face detection, Face detection, multi-robot systems, computer vision, multi-robot system, Multirobot systems, distributed system, gesture-controlled robot, task assignment, Communication channels, Infrared sensors, motion-based gestures, 1xplore, 1search1, 1include\_search1},
	pages = {355--356},
	file = {Submitted Version:/Users/tid010/Zotero/storage/A2N7RL7Q/Couture-Beil et al. - 2010 - Selecting and commanding individual robots in a vi.pdf:application/pdf},
}

@article{gao_hand_2020,
	title = {Hand gesture recognition using multimodal data fusion and multiscale parallel convolutional neural network for human–robot interaction},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078064988&doi=10.1111%2fexsy.12490&partnerID=40&md5=68a33aadaf893294e532de07da9c540d},
	doi = {10.1111/exsy.12490},
	abstract = {Hand gesture recognition plays an important role in human–robot interaction. The accuracy and reliability of hand gesture recognition are the keys to gesture-based human–robot interaction tasks. To solve this problem, a method based on multimodal data fusion and multiscale parallel convolutional neural network (CNN) is proposed in this paper to improve the accuracy and reliability of hand gesture recognition. First of all, data fusion is conducted on the sEMG signal, the RGB image, and the depth image of hand gestures. Then, the fused images are generated to two different scale images by downsampling, which are respectively input into two subnetworks of the parallel CNN to obtain two hand gesture recognition results. After that, hand gesture recognition results of the parallel CNN are combined to obtain the final hand gesture recognition result. Finally, experiments are carried out on a self-made database containing 10 common hand gestures, which verify the effectiveness and superiority of the proposed method for hand gesture recognition. In addition, the proposed method is applied to a seven-degree-of-freedom bionic manipulator to achieve robotic manipulation with hand gestures. © 2020 John Wiley \& Sons, Ltd.},
	journal = {Expert Systems},
	author = {Gao, Q. and Liu, J. and Ju, Z.},
	year = {2020},
	keywords = {1scopus},
	annote = {cited By 2},
	file = {Full Text:/Users/tid010/Zotero/storage/B6CUT64J/Gao et al. - 2020 - Hand gesture recognition using multimodal data fus.pdf:application/pdf},
}

@article{koppula_anticipating_2016,
	title = {Anticipating {Human} {Activities} {Using} {Object} {Affordances} for {Reactive} {Robotic} {Response}},
	volume = {38},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961588783&doi=10.1109%2fTPAMI.2015.2430335&partnerID=40&md5=8b695548eb29e2ac583f1cde7c8b423d},
	doi = {10.1109/TPAMI.2015.2430335},
	abstract = {An important aspect of human perception is anticipation, which we use extensively in our day-to-day activities when interacting with other humans as well as with our surroundings. Anticipating which activities will a human do next (and how) can enable an assistive robot to plan ahead for reactive responses. Furthermore, anticipation can even improve the detection accuracy of past activities. The challenge, however, is two-fold: We need to capture the rich context for modeling the activities and object affordances, and we need to anticipate the distribution over a large space of future human activities. In this work, we represent each possible future using an anticipatory temporal conditional random field (ATCRF) that models the rich spatial-temporal relations through object affordances. We then consider each ATCRF as a particle and represent the distribution over the potential futures using a set of particles. In extensive evaluation on CAD-120 human activity RGB-D dataset, we first show that anticipation improves the state-of-the-art detection results. We then show that for new subjects (not seen in the training set), we obtain an activity anticipation accuracy (defined as whether one of top three predictions actually happened) of 84.1, 74.4 and 62.2 percent for an anticipation time of 1, 3 and 10 seconds respectively. Finally, we also show a robot using our algorithm for performing a few reactive responses. © 1979-2012 IEEE.},
	number = {1},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Koppula, H.S. and Saxena, A.},
	year = {2016},
	keywords = {1scopus},
	pages = {14--29},
	annote = {cited By 210},
	file = {Submitted Version:/Users/tid010/Zotero/storage/XVH9YR7F/Koppula and Saxena - 2016 - Anticipating Human Activities Using Object Afforda.pdf:application/pdf},
}

@article{yang_novel_2018,
	title = {A novel gesture recognition system for intelligent interaction with a nursing-care assistant robot},
	volume = {8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057127800&doi=10.3390%2fapp8122349&partnerID=40&md5=be5390fd6b4187a38f74ad145e41d6df},
	doi = {10.3390/app8122349},
	abstract = {The expansion of nursing-care assistant robots in smart infrastructure has provided more applications for homecare services, which has raised new demands for smart and natural interaction between humans and robots. This article proposed an innovative hand motion trajectory (HMT) gesture recognition system based on background velocity features. Here, a new wearable wrist-worn camera prototype for gesture's video collection was designed, and a new method for the segmentation of continuous gestures was shown. Meanwhile, a nursing-care assistant robot prototype was designed for assisting the elderly, which is capable of carrying the elderly with omnidirectional motion and grabbing the specified object at home. In order to evaluate the performance of the gesture recognition system, 10 special gestures were defined as the move commands for interaction with the robot, and 1000 HMT gesture samples were obtained from five subjects for leave-one-subject-out (LOSO) cross-validation classification with an average recognition accuracy of up to 97.34\%. Moreover, the performance and practicability of the proposed system were further demonstrated by controlling the omnidirectional movement of the nursing-care assistant robot using the predefined gesture commands. © 2018 by the authors. Licensee MDPI, Basel, Switzerland.},
	number = {12},
	journal = {Applied Sciences (Switzerland)},
	author = {Yang, G. and Lv, H. and Chen, F. and Pang, Z. and Wang, J. and Yang, H. and Zhang, J.},
	year = {2018},
	keywords = {1scopus, 1search1, 1include\_search1},
	annote = {cited By 10},
	file = {Full Text:/Users/tid010/Zotero/storage/UPCUNDT9/Yang et al. - 2018 - A novel gesture recognition system for intelligent.pdf:application/pdf},
}

@inproceedings{mollaret_multi-modal_2016,
	title = {A multi-modal perception based architecture for a non-intrusive domestic assistant robot},
	volume = {2016-April},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964864735&doi=10.1109%2fHRI.2016.7451816&partnerID=40&md5=2c05f79672ce7e7f53247a48a4d6c01e},
	doi = {10.1109/HRI.2016.7451816},
	abstract = {We present a multi-modal perception based architecture to realize a non-intrusive domestic assistant robot. The realized robot is non-intrusive in that it only starts interaction with a user when it detects the user's intention to do so automatically. All the robot's actions are based on multi-modal perceptions, which include: user detection based on RGB-D data, user's intention-for-interaction detection with RGB-D and audio data, and communication via speech recognition. The utilization of multi-modal cues in different parts of the robotic activity paves the way to successful robotic runs. © 2016 IEEE.},
	booktitle = {{ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	author = {Mollaret, C. and Mekonnen, A.A. and Pinquier, J. and Lerasle, F. and Ferrane, I.},
	year = {2016},
	keywords = {1scopus},
	pages = {481--482},
	annote = {cited By 3},
	file = {Submitted Version:/Users/tid010/Zotero/storage/PQQ3XDRR/Mollaret et al. - 2016 - A multi-modal perception based architecture for a .pdf:application/pdf},
}

@article{kogkas_free-view_2019,
	title = {Free-{View}, {3D} {Gaze}-{Guided} {Robotic} {Scrub} {Nurse}},
	volume = {11768 LNCS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075688399&doi=10.1007%2f978-3-030-32254-0_19&partnerID=40&md5=a1e6b32733bff5988273b0b403e6c561},
	doi = {10.1007/978-3-030-32254-0_19},
	abstract = {We introduce a novel 3D gaze-guided robotic scrub nurse (RN) and test the platform in simulated surgery to determine usability and acceptability with clinical teams. Surgeons and trained scrub nurses performed an ex vivo task on pig colon. Surgeons used gaze via wearable eye-tracking glasses to select surgical instruments on a screen, in turn initiating RN to deliver the instrument. Comparison was done between human- and robot-assisted tasks (HT vs RT). Real-time gaze-screen interaction was based on a framework developed with synergy of conventional wearable eye-tracking, motion capture system and RGB-D cameras. NASA-TLX and Van der Laan’s technology acceptance questionnaires were collected and analyzed. 10 teams of surgical trainees (ST) and scrub nurses (HN) participated. Overall, NASA-TLX feedback was positive. ST and HN revealed no statistically significant difference in overall task load. Task performance feedback was unaffected. Frustration was reported by ST. Overall, Van der Laan’s scores showed positive usefulness and satisfaction scores following RN use. There was no significant difference in task interruptions across HT vs RT. Similarly, no statistical difference was found in duration to task completion in both groups. Quantitative and qualitative feedback was positive. The source of frustration has been understood. Importantly, there was no significant difference in task workflow or operative time, with overall perceptions towards task performance remaining unchanged in HT vs RT. © 2019, Springer Nature Switzerland AG.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Kogkas, A. and Ezzat, A. and Thakkar, R. and Darzi, A. and Mylonas, G.},
	year = {2019},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {164--172},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/QFQ6MRYU/Kogkas et al. - 2019 - Free-View, 3D Gaze-Guided Robotic Scrub Nurse.pdf:application/pdf},
}

@inproceedings{budiharto_indoor_2010,
	title = {Indoor navigation using adaptive neuro fuzzy controller for servant robot},
	volume = {1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952628595&doi=10.1109%2fICCEA.2010.119&partnerID=40&md5=ac87ba21774d448ca73a001ab62f3ee9},
	doi = {10.1109/ICCEA.2010.119},
	abstract = {We present our ongoing work on the development of Adaptive Neuro Fuzzy Inference System (ANFIS) Controller for humanoid servant robot designed for navigation based on vision. In this method, black line on the landmark used as a track for robot's navigation using webcam as line sensor. We proposed architecture of ANFIS controller for servant robot based on mapping method, 3 input and 3 output applied to the controller. Only 45 training data used for navigation and best error starting at epoch 62. Each of the components are described in the paper and experimental results are presented. Humanoid servant robot also equipped with 4DOF arm robot, face recognition and text to speech processor. In order to demonstrate and measure the usefulness of such technologies for human-robot interaction, all components have been integrated and have been used for a servant robot named Srikandi I. Based on experiments, ANFIS controller succesfully implemented as controller for robot's navigation. © 2010 IEEE.},
	booktitle = {2010 2nd {International} {Conference} on {Computer} {Engineering} and {Applications}, {ICCEA} 2010},
	author = {Budiharto, W. and Jazidie, A. and Purwanto, D.},
	year = {2010},
	keywords = {1scopus},
	pages = {582--586},
	annote = {cited By 19},
	file = {Full Text:/Users/tid010/Zotero/storage/Z3ZU88DL/Budiharto et al. - 2010 - Indoor navigation using adaptive neuro fuzzy contr.pdf:application/pdf},
}

@article{phong_vietnamese_2020,
	title = {Vietnamese service robot based on artificial intelligence},
	volume = {9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084409952&doi=10.18178%2fijmerr.9.5.701-708&partnerID=40&md5=f6ce05677c0de4a17f4f59f578b7ba12},
	doi = {10.18178/ijmerr.9.5.701-708},
	abstract = {Service robot have been designed and developed for different objectives and requirements. Since robotic revolution, Vietnam is one of the most influenced among South-east Asia countries in developing artificial intelligence. People believe that robot will replace all blue-collars in almost company, even hospital and school in next few decades, therefore developers are trying to improve natural language interaction between human and robot, especially helping new vision for Vietnamese robotics. We have built a robot application that capable of understand Vietnamese natural language, there are four tasks which was mentioned solving the problems, even more AI method. Deep learning, on the other hand, is a sub-field of machine learning. In this paper, with these algorithms, artificial intelligent supported is much complex, people can communicate naturally with robot, not only English but also Vietnamese at well. In this paper, we will introduce the specification and intelligent interaction processing in naturally Vietnamese. © 2020 by the authors.},
	number = {5},
	journal = {International Journal of Mechanical Engineering and Robotics Research},
	author = {Phong, N.T.T. and Nam, L.H.T. and Thinh, N.T.},
	year = {2020},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {701--708},
	annote = {cited By 0},
}

@article{zhang_indoor_2018,
	title = {Indoor omni-directional mobile robot that track independently},
	volume = {29},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047059475&doi=10.3966%2f199115992018042902013&partnerID=40&md5=967a7a21fc5f953c4b292c3eaf1b887b},
	doi = {10.3966/199115992018042902013},
	abstract = {Service robots want to achieve intelligent interaction, self-tracking is the foundation. At present, the tracking of indoor robots is mainly for human body tracking or object tracking with auxiliary markers, only one type of target can be tracked. In order to solve the above shortcomings, this paper designed an automatic omni-directional robot, using a common camera, according the different target using different strategies to achieve the tracking of any tracking, regardless of their status. The tracking strategy can be divided into two broad categories. One is tracking the human. Using based classifier face tracking and feature point tracking, Set the face tracking as a high priority of tracking, and the feature point tracking is added to ensure the tracking integrity when face blind spots appear. Another is the tracking of objects in any state. In this case, the feature points are used to track the target feature points, using Shi-Tomasi algorithm and Lucas-Kanade algorithm. The 2D coordinates of target sub-pixel level accuracy are obtained. Combined with laser range finder, build three-dimensional coordinate positioning. Finally, this paper verifies the feasibility of the system through experiments. The success rate of tracking is over 90\%, which has high reliability. © 2018 Computer Society of the Republic of China. All Rights Reserved.},
	number = {2},
	journal = {Journal of Computers (Taiwan)},
	author = {Zhang, K. and Zhang, L.},
	year = {2018},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {118--135},
	annote = {cited By 1},
}

@article{mronga_constraint-based_2020,
	title = {A constraint-based approach for human–robot collision avoidance},
	volume = {34},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079162309&doi=10.1080%2f01691864.2020.1721322&partnerID=40&md5=8754d51df7b55e874e9e7fd9b5df34ca},
	doi = {10.1080/01691864.2020.1721322},
	abstract = {In this paper, we present a software-based approach for collision avoidance that can be applied in human–robot collaboration scenarios. One of the contributions is a method for converting clustered 3D sensor data into computationally efficient convex hull representations used for robot-obstacle distance computation. Based on the computed distance vectors, we generate collision avoidance motions using a potential field approach and integrate them with other simultaneously running robot tasks in a constraint-based control framework. In order to improve control performance, we apply evolutionary techniques for parameter optimization within this framework based on selected quality criteria. Experiments are performed on a dual-arm robotic system equipped with several depth cameras. The approach is able to generate task-compliant avoidance motions in dynamic environments with high performance. © 2020, © 2020 Informa UK Limited, trading as Taylor \& Francis Group and The Robotics Society of Japan.},
	number = {5},
	journal = {Advanced Robotics},
	author = {Mronga, D. and Knobloch, T. and de Gea Fernández, J. and Kirchner, F.},
	year = {2020},
	keywords = {1scopus},
	pages = {265--281},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/JY83MW66/Mronga et al. - 2020 - A constraint-based approach for human–robot collis.pdf:application/pdf},
}

@inproceedings{vasquez_deep_2017,
	title = {Deep {Detection} of {People} and their {Mobility} {Aids} for a {Hospital} {Robot}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040735155&doi=10.1109%2fECMR.2017.8098665&partnerID=40&md5=9f1bc2c4211f3ff9a577d940ea970bcb},
	doi = {10.1109/ECMR.2017.8098665},
	abstract = {Robots operating in populated environments encounter many different types of people, some of whom might have an advanced need for cautious interaction, because of physical impairments or their advanced age. Robots therefore need to recognize such advanced demands to provide appropriate assistance, guidance or other forms of support. In this paper, we propose a depth-based perception pipeline that estimates the position and velocity of people in the environment and categorizes them according to the mobility aids they use: pedestrian, person in wheelchair, person in a wheelchair with a person pushing them, person with crutches and person using a walker. We present a fast region proposal method that feeds a Region-based Convolutional Network (Fast R- CNN [1]). With this, we speed up the object detection process by a factor of seven compared to a dense sliding window approach. We furthermore propose a probabilistic position, velocity and class estimator to smooth the CNN's detections and account for occlusions and misclassifications. In addition, we introduce a new hospital dataset with over 17,000 annotated RGB-D images. Extensive experiments confirm that our pipeline successfully keeps track of people and their mobility aids, even in challenging situations with multiple people from different categories and frequent occlusions. © 2017 IEEE.},
	booktitle = {2017 {European} {Conference} on {Mobile} {Robots}, {ECMR} 2017},
	author = {Vasquez, A. and Kollmitz, M. and Eitel, A. and Burgard, W.},
	year = {2017},
	keywords = {1scopus, 1search1, 1include\_search1},
	annote = {cited By 6},
	file = {Submitted Version:/Users/tid010/Zotero/storage/ZBWXZKLS/Vasquez et al. - 2017 - Deep Detection of People and their Mobility Aids f.pdf:application/pdf},
}

@inproceedings{lavanya_gesture_2018,
	title = {Gesture controlled robot},
	volume = {2018-January},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046554296&doi=10.1109%2fICEECCOT.2017.8284549&partnerID=40&md5=a2846b5261084dcd6fb0af8db7878d35},
	doi = {10.1109/ICEECCOT.2017.8284549},
	abstract = {This project is a real time monitoring system by which humans interacts with robots through gestures. This is an immense aid for people for whom mobility is a great challenge. There is a dire need for vision based interface over speech recognition as it failed to mandate the robots because of modulation and varying frequency. Gesture recognition consists of three stages: capturing of image, image processing and data extraction. The implementation is achieved by navigation of the robot through various gestures. By the impact of this project, life of physically challenged people becomes less challenging. From further research it will benefit various areas including applications in military and high security bases. © 2017 IEEE.},
	booktitle = {International {Conference} on {Electrical}, {Electronics}, {Communication} {Computer} {Technologies} and {Optimization} {Techniques}, {ICEECCOT} 2017},
	author = {Lavanya, K.N. and Shree, D.R. and Nischitha, B.R. and Asha, T. and Gururaj, C.},
	year = {2018},
	keywords = {1scopus},
	pages = {465--469},
	annote = {cited By 4},
	file = {Full Text:/Users/tid010/Zotero/storage/PH2PU6V6/Lavanya et al. - 2018 - Gesture controlled robot.pdf:application/pdf},
}

@article{boucher_i_2012,
	title = {I reach faster when i see you look: {Gaze} effects in human-human and human-robot face-to-face cooperation},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861880897&doi=10.3389%2ffnbot.2012.00003&partnerID=40&md5=93de47ff728474a4d3c42e1ce9ea2bc9},
	doi = {10.3389/fnbot.2012.00003},
	abstract = {Human-human interaction in natural environments relies on a variety of perceptual cues. Humanoid robots are becoming increasingly refined in their sensorimotor capabilities, and thus should now be able to manipulate and exploit these social cues in cooperation with their human partners. Previous studies have demonstrated that people follow human and robot gaze, and that it can help them to cope with spatially ambiguous language. Our goal is to extend these findings into the domain of action, to determine how human and robot gaze can influence the speed and accuracy of human action. We report on results from a human-human cooperation experiment demonstrating that an agent's vision of her/his partner's gaze can significantly improve that agent's performance in a cooperative task. We then implement a heuristic capability to generate such gaze cues by a humanoid robot that engages in the same cooperative interaction. The subsequent human-robot experiments demonstrate that a human agent can indeed exploit the predictive gaze of their robot partner in a cooperative task. This allows us to render the humanoid robot more human-like in its ability to communicate with humans. The long term objectives of the work are thus to identify social cooperation cues, and to validate their pertinence through implementation in a cooperative robot. The current research provides the robot with the capability to produce appropriate speech and gaze cues in the context of human-robot cooperation tasks. Gaze is manipulated in three conditions: Full gaze (coordinated eye and head), eyes hidden with sunglasses, and head fixed. We demonstrate the pertinence of these cues in terms of statistical measures of action times for humans in the context of a cooperative task, as gaze significantly facilitates cooperation as measured by human response times. © 2012 Boucher, Pattacini, Lelong, Bailly, Elisei, Fagel, Dominey and Ventre-Dominey.},
	number = {MAY},
	journal = {Frontiers in Neurorobotics},
	author = {Boucher, J.-D. and Pattacini, U. and Lelong, A. and Bailly, G. and Elisei, F. and Fagel, S. and Dominey, P.F. and Ventre-Dominey, J.},
	year = {2012},
	keywords = {1scopus},
	annote = {cited By 56},
	file = {Full Text:/Users/tid010/Zotero/storage/CUM9AKJK/Boucher et al. - 2012 - I reach faster when i see you look Gaze effects i.pdf:application/pdf},
}

@inproceedings{song_towards_2017,
	title = {Towards robust ego-centric hand gesture analysis for robot control},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018679118&doi=10.1109%2fSIPROCESS.2016.7888345&partnerID=40&md5=99a12501d4dbfc201bac711d197bf9bc},
	doi = {10.1109/SIPROCESS.2016.7888345},
	abstract = {Wearable device with an ego-centric camera would be the next generation device for human-computer interaction such as robot control. Hand gesture is a natural way of ego-centric human-computer interaction. In this paper, we present an ego-centric multi-stage hand gesture analysis pipeline for robot control which works robustly in the unconstrained environment with varying luminance. In particular, we first propose an adaptive color and contour based hand segmentation method to segment hand region from the ego-centric viewpoint. We then propose a convex U-shaped curve detection algorithm to precisely detect positions of fingertips. And parallelly, we utilize the convolutional neural networks to recognize hand gestures. Based on these techniques, we combine most information of hand to control the robot and develop a hand gesture analysis system on an iPhone and a robot arm platform to validate its effectiveness. Experimental result demonstrates that our method works perfectly on controlling the robot arm by hand gesture in real time. © 2016 IEEE.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Signal} and {Image} {Processing}, {ICSIP} 2016},
	author = {Song, H. and Feng, W. and Guan, N. and Huang, X. and Luo, Z.},
	year = {2017},
	keywords = {1scopus},
	pages = {661--666},
	annote = {cited By 1},
	file = {Full Text:/Users/tid010/Zotero/storage/249RH7TK/Song et al. - 2017 - Towards robust ego-centric hand gesture analysis f.pdf:application/pdf},
}

@inproceedings{lee_visual_2020,
	title = {Visual {Perception} {Framework} for an {Intelligent} {Mobile} {Robot}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094322333&doi=10.1109%2fUR49135.2020.9144932&partnerID=40&md5=0f8157ddfbec2bf8be249cc5c41158e0},
	doi = {10.1109/UR49135.2020.9144932},
	abstract = {Visual perception is a fundamental capability necessary for intelligent mobile robots to interact properly and safely with the humans in the real-world. Recently, the world has seen revolutionary advances in deep learning has led to some incredible breakthroughs in vision technology. However, research integrating diverse visual perception methods into robotic systems is still in its infancy and lacks validation in real-world scenarios. In this paper, we present a visual perception framework for an intelligent mobile robot. Based on the robot operating system middleware, our framework integrates a broad set of advanced algorithms capable of recognising people, objects and human poses, as well as describing observed scenes. In several challenge scenarios of international robotics competitions using two mobile service robots, the performance and acceptability of the proposed framework are evaluated. © 2020 IEEE.},
	booktitle = {2020 17th {International} {Conference} on {Ubiquitous} {Robots}, {UR} 2020},
	author = {Lee, C.-Y. and Lee, H. and Hwang, I. and Zhang, B.-T.},
	year = {2020},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {612--616},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/GPQYWYRB/Lee et al. - 2020 - Visual Perception Framework for an Intelligent Mob.pdf:application/pdf},
}

@inproceedings{wang_collision-free_2017,
	title = {Collision-free trajectory planning in human-robot interaction through hand movement prediction from vision},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044448781&doi=10.1109%2fHUMANOIDS.2017.8246890&partnerID=40&md5=10434a1266321214b8cc74406c466bcf},
	doi = {10.1109/HUMANOIDS.2017.8246890},
	abstract = {We present a framework from vision based hand movement prediction in a real-world human-robot collaborative scenario for safety guarantee. We first propose a perception submodule that takes in visual data solely and predicts human collaborator's hand movement. Then a robot trajectory adaptive planning submodule is developed that takes the noisy movement prediction signal into consideration for optimization. To validate the proposed systems, we first collect a new human manipulation dataset that can supplement the previous publicly available dataset with motion capture data to serve as the ground truth of hand location. We then integrate the algorithm with a six degree-of-freedom robot manipulator that can collaborate with human workers on a set of trained manipulation actions, and it is shown that such a robot system outperforms the one without movement prediction in terms of collision avoidance. We verify the effectiveness of the proposed motion prediction and robot trajectory planning approaches in both simulated and physical experiments. To the best of the authors' knowledge, it is the first time that a deep model based movement prediction system is utilized and is proven effective in human-robot collaboration scenario for enhanced safety. © 2017 IEEE.},
	booktitle = {{IEEE}-{RAS} {International} {Conference} on {Humanoid} {Robots}},
	author = {Wang, Y. and Ye, X. and Yang, Y. and Zhang, W.},
	year = {2017},
	keywords = {1scopus},
	pages = {305--310},
	annote = {cited By 10},
	file = {Full Text:/Users/tid010/Zotero/storage/ZQGI4PDZ/Wang et al. - 2017 - Collision-free trajectory planning in human-robot .pdf:application/pdf},
}

@article{vysocky_interaction_2019,
	title = {Interaction with {Collaborative} {Robot} {Using} {2D} and {TOF} {Camera}},
	volume = {11472 LNCS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064044137&doi=10.1007%2f978-3-030-14984-0_35&partnerID=40&md5=9fb3a8ff787279039433a413f1f19c49},
	doi = {10.1007/978-3-030-14984-0_35},
	abstract = {With increasing count of applications with collaborative robots it is important to be able to set up conditions for controlling the robot and its interaction with human operator. This article describes a quick response system detecting hands of the operator using 2D and TOF camera in the shared operator-robot workspace. The technology provides data about position of hands and gestures which is used to instant reactions of the robot on presence of the operator and to control the robot. Interaction with robot with gestures is for the operator more intuitive with no need of expertise knowledge. Operator can use both hands, so the operation is more efficient and faster. This system was tested on prototype workplace and results are evaluated in this article. © 2019, Springer Nature Switzerland AG.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Vysocký, A. and Pastor, R. and Novák, P.},
	year = {2019},
	keywords = {1scopus},
	pages = {477--489},
	annote = {cited By 3},
	file = {Full Text:/Users/tid010/Zotero/storage/E86EQYVY/Vysocký et al. - 2019 - Interaction with Collaborative Robot Using 2D and .pdf:application/pdf},
}

@inproceedings{potdar_learning_2016,
	title = {Learning by demonstration from multiple agents in humanoid robots},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981295017&doi=10.1109%2fSCEECS.2016.7509324&partnerID=40&md5=c921d078c39428c65277d919e35b30ca},
	doi = {10.1109/SCEECS.2016.7509324},
	abstract = {In this paper we present a novel hand gesture recognition system for adaptive learning of human interactions for intuitive human-robot interactions. The proposed system tracks upper body hand gestures of two interacting human agents simultaneously with the help of a motion tracking camera. High dimensional spatio-temporal gesture data is modelled onto low dimensional latent space by means of principal component analysis (PCA) yet maintaining maximum spatial information. Mapping between these low dimensional motion data is learned by non-linear learning algorithms like Radial Basis Function Neural Network (RBFN) and Long Short Term Memory Neural Network (LSTM). Finally learned interaction models can be used by robot to perform similar interactions with a human partner. In order to evaluate accuracy of gestures performed by the robot, the paper presents novel approach to find optimum number of neurons required in the hidden layer of a three layered artificial neural network to produce the desired gesture. Gestures are performed by both, a simulated virtual robot and a physical humanoid robot platform. © 2016 IEEE.},
	booktitle = {2016 {IEEE} {Students}' {Conference} on {Electrical}, {Electronics} and {Computer} {Science}, {SCEECS} 2016},
	author = {Potdar, S. and Sawarkar, A. and Kazi, F.},
	year = {2016},
	keywords = {1scopus},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/9C6F3TPR/Potdar et al. - 2016 - Learning by demonstration from multiple agents in .pdf:application/pdf},
}

@article{liu_dynamic_2020,
	title = {Dynamic risk assessment and active response strategy for industrial human-robot collaboration},
	volume = {141},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078452968&doi=10.1016%2fj.cie.2020.106302&partnerID=40&md5=d29fd256c7c66ea3a855ac356475e02b},
	doi = {10.1016/j.cie.2020.106302},
	abstract = {To enhance flexibility and sustainability, human-robot collaboration is becoming a major feature of next-generation robots. The safety assessment strategy is the first and crucial issue that needs to be considered due to the removal of the safety barrier. This paper determined the set of safety indicators and established an assessment model based on the latest safety-related ISO standards and manufacturing conditions. A dynamic modified SSM (speed and separation monitoring) method is presented for ensuring the safety of human-robot collaboration while maintaining productivity as high as possible. A prototype system including dynamic risk assessment and safe motion control is developed based on the virtual model of the robot and human skeleton point data from the vision sensor. The real-time risk status of the working robot can be known and the risk field around the robot which is visualized in an augmented reality environment so as to ensure safe human-robot collaboration. This system is experimentally validated on a human-robot collaboration cell using an industrial robot with six degrees of freedom. © 2020 Elsevier Ltd},
	journal = {Computers and Industrial Engineering},
	author = {Liu, Z. and Wang, X. and Cai, Y. and Xu, W. and Liu, Q. and Zhou, Z. and Pham, D.T.},
	year = {2020},
	keywords = {1scopus},
	annote = {cited By 0},
	file = {Liu et al. - 2020 - Dynamic risk assessment and active response strate.pdf:/Users/tid010/Zotero/storage/K4CDDIUW/Liu et al. - 2020 - Dynamic risk assessment and active response strate.pdf:application/pdf},
}

@article{yoo_gaze_2017,
	title = {Gaze control of humanoid robot for learning from demonstration},
	volume = {447},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978496765&doi=10.1007%2f978-3-319-31293-4_21&partnerID=40&md5=91aadc0ea5ea4eb162411696f0cc2236},
	doi = {10.1007/978-3-319-31293-4_21},
	abstract = {Robots can learn knowledge by observing demonstration of humans. As tutees, robots need to not only observe human behaviors, but also make proper feedbacks for human tutors because learning is an interactive process in which information is delivered in bidirectional ways between humans and robots. Gaze is an adequate method for robots to provide human tutors with feedbacks that robots are concentrating on current learning because gaze directly represents where they are paying attention to. This paper proposes a gaze control algorithm with a state machine in learning from demonstration. A human tutor shows demonstration in front of a robot tutee, and the robot tutee observes the demonstration for learning. The robot tutee perceives external environment through its camera, recognizes a human and objects, and figures out a state at which the robot tutee is situated. Then, the robot tutee gazes at proper targets that are predefined by the state machine. The human tutor also adjusts the demonstration to make learning more effectively according to the robot tutee’s feedbacks. The effectiveness of the proposed method is demonstrated through the experiments with a robotic head with 17 degrees of freedom, developed in the RIT Lab., KAIST. © Springer international publishing switzerland 2017.},
	journal = {Advances in Intelligent Systems and Computing},
	author = {Yoo, B.-S. and Kim, J.-H.},
	year = {2017},
	keywords = {1scopus},
	pages = {263--270},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/D5UKARRT/Yoo and Kim - 2017 - Gaze control of humanoid robot for learning from d.pdf:application/pdf},
}

@article{xu_online_2014,
	title = {Online {Dynamic} {Gesture} {Recognition} for {Human} {Robot} {Interaction}},
	volume = {77},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924223417&doi=10.1007%2fs10846-014-0039-4&partnerID=40&md5=26030c05b21a29842581e7a9222cae7a},
	doi = {10.1007/s10846-014-0039-4},
	abstract = {This paper presents an online dynamic hand gesture recognition system with an RGB-D camera, which can automatically recognize hand gestures against complicated background. For background subtraction, we use a model-based method to perform human detection and segmentation in the depth map. Since a robust hand tracking approach is crucial for the performance of hand gesture recognition, our system uses both color information and depth information in the process of hand tracking. To extract spatio-temporal hand gesture sequences in the trajectory, a reliable gesture spotting scheme with detection on change of static postures is proposed. Then discrete HMMs with Left-Right Banded (LRB) topology are utilized to model and classify gestures based on multi-feature representation and quantization of the hand gesture sequences. Experimental evaluations on two self-built databases of dynamic hand gestures show the effectiveness of the proposed system. Furthermore, we develop a human-robot interactive system, and the performance of this system is demonstrated through interactive experiments in the dynamic environment. © 2014, Springer Science+Business Media Dordrecht.},
	number = {3-4},
	journal = {Journal of Intelligent and Robotic Systems: Theory and Applications},
	author = {Xu, D. and Wu, X. and Chen, Y.-L. and Xu, Y.},
	year = {2014},
	keywords = {1scopus},
	pages = {583--596},
	annote = {cited By 43},
	file = {Full Text:/Users/tid010/Zotero/storage/XMSI325F/Xu et al. - 2014 - Online Dynamic Gesture Recognition for Human Robot.pdf:application/pdf},
}

@inproceedings{saegusa_cognitive_2011,
	title = {Cognitive robotics-active perception of the self and others},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79961174482&doi=10.1109%2fHSI.2011.5937403&partnerID=40&md5=d43cb015d09eee6bf2dffdd44e5cd302},
	doi = {10.1109/HSI.2011.5937403},
	abstract = {The paper describes constructive approach of active perception for anthropomorphic robots. The key idea is that a robot tries to identify a human's action as an own action based on the observation of action effects for objects. In the proposed framework, the active perception are decomposed into the three phases; (1) a robot voluntarily generates actions to discover the own body and other objects. Here the object is defined as what the robot can interact with. (2) the robot characterizes its own action with its effect for objects. (3) the robot identifies the human action as the own action. The mirrored perception of the own action and the human's action allows the robot to share motor intelligence with humans. The approach was evaluated in experiments with a humanoid robot equipped with multi sensory modalities such as vision, tactile, and proprioceptive sensing. © 2011 IEEE.},
	booktitle = {4th {International} {Conference} on {Human} {System} {Interaction}, {HSI} 2011},
	author = {Saegusa, R. and Natale, L. and Metta, G. and Sandini, G.},
	year = {2011},
	keywords = {1scopus},
	pages = {419--426},
	annote = {cited By 5},
	file = {Full Text:/Users/tid010/Zotero/storage/UXA49V78/Saegusa et al. - 2011 - Cognitive robotics-active perception of the self a.pdf:application/pdf},
}

@inproceedings{yao_monocular_2017,
	title = {Monocular vision-based human following on miniature robotic blimp},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027963253&doi=10.1109%2fICRA.2017.7989369&partnerID=40&md5=7ae330b49dc52904ca0d971a4a9c225a},
	doi = {10.1109/ICRA.2017.7989369},
	abstract = {We present an approach that allows the Georgia Tech Miniature Autonomous Blimp (GT-MAB) to detect and follow a human. This accomplishment is the first Human Robot Interaction (HRI) demonstration between an uninstrumented human and a robotic blimp. GT-MAB is an ideal platform for HRI missions because it is safe to humans and can support sufficient flight time for HRI experiments. However, due to complex aerodynamic influence on the blimp, the human following task for GT-MAB with a single on-board camera is a challenging problem. We integrate Haar face detector and KLT feature tracker to achieve robust human tracking. After a human face is detected in the real-time video stream, we estimated the 3D positions of the human with respect to GT-MAB. Visionbased PID controllers are designed based on estimated relative position and the motion primitives of GT-MAB such that it can achieve stable and continuous human following behavior. Experimental results are presented to demonstrate the human following capability on GT-MAB. © 2017 IEEE.},
	booktitle = {Proceedings - {IEEE} {International} {Conference} on {Robotics} and {Automation}},
	author = {Yao, N. and Anaya, E. and Tao, Q. and Cho, S. and Zheng, H. and Zhang, F.},
	year = {2017},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {3244--3249},
	annote = {cited By 22},
	file = {Full Text:/Users/tid010/Zotero/storage/VAIQ7F67/Yao et al. - 2017 - Monocular vision-based human following on miniatur.pdf:application/pdf},
}

@article{park_real-time_2011,
	title = {Real-time {3D} pointing gesture recognition for mobile robots with cascade {HMM} and particle filter},
	volume = {29},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957697336&doi=10.1016%2fj.imavis.2010.08.006&partnerID=40&md5=4d2c26fb412c0f8ff71cdf91ad086020},
	doi = {10.1016/j.imavis.2010.08.006},
	abstract = {In this paper, we present a real-time 3D pointing gesture recognition algorithm for mobile robots, based on a cascade hidden Markov model (HMM) and a particle filter. Among the various human gestures, the pointing gesture is very useful to human-robot interaction (HRI). In fact, it is highly intuitive, does not involve a-priori assumptions, and has no substitute in other modes of interaction. A major issue in pointing gesture recognition is the difficultly of accurate estimation of the pointing direction, caused by the difficulty of hand tracking and the unreliability of the direction estimation. The proposed method involves the use of a stereo camera and 3D particle filters for reliable hand tracking, and a cascade of two HMMs for a robust estimate of the pointing direction. When a subject enters the field of view of the camera, his or her face and two hands are located and tracked using particle filters. The first stage HMM takes the hand position estimate and maps it to a more accurate position by modeling the kinematic characteristics of finger pointing. The resulting 3D coordinates are used as input into the second stage HMM that discriminates pointing gestures from other types. Finally, the pointing direction is estimated for the pointing state. The proposed method can deal with both large and small pointing gestures. The experimental results show gesture recognition and target selection rates of better than 89\% and 99\% respectively, during human-robot interaction. © 2010 Elsevier B.V. All rights reserved.},
	number = {1},
	journal = {Image and Vision Computing},
	author = {Park, C.-B. and Lee, S.-W.},
	year = {2011},
	keywords = {1scopus},
	pages = {51--63},
	annote = {cited By 51},
	file = {Park and Lee - 2011 - Real-time 3D pointing gesture recognition for mobi.pdf:/Users/tid010/Zotero/storage/R8V5WMB3/Park and Lee - 2011 - Real-time 3D pointing gesture recognition for mobi.pdf:application/pdf},
}

@inproceedings{sorostinean_activity_2018,
	title = {Activity {Recognition} {Based} on {RGB}-{D} and {Thermal} {Sensors} for {Socially} {Assistive} {Robots}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060817685&doi=10.1109%2fICARCV.2018.8581349&partnerID=40&md5=4e356a7ef5e3b40d1343a97836cd3cae},
	doi = {10.1109/ICARCV.2018.8581349},
	abstract = {For socially assistive robots, being able to recognize basic human actions is an important capability. The sensors, which are frequently mounted on most recent robots, such as RGB-D and thermal cameras, as well as the advances in deep learning have enabled the research on activity recognition to grow. In this paper, we collected our own dataset of actions in a home-like scenario, which contains thermal imagery in addition to RGB-D data and we proposed a method based on Long-term Recurrent Convolutional Networks (LRCN). We showed that our method has an accuracy comparable with the state-of-the-art. We also proved that thermal information can improve the recognition accuracy. Furthermore, we tested the real-time capability of our system and conducted a real-time experiment with a robot (Pepper robot from Softbank Robotics) so as to investigate the effect of a robot enabled with action recognition capability in a human-robot interaction. © 2018 IEEE.},
	booktitle = {2018 15th {International} {Conference} on {Control}, {Automation}, {Robotics} and {Vision}, {ICARCV} 2018},
	author = {Sorostinean, M. and Tapus, A.},
	year = {2018},
	keywords = {1scopus},
	pages = {1298--1304},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/4QNBSE6Y/Sorostinean and Tapus - 2018 - Activity Recognition Based on RGB-D and Thermal Se.pdf:application/pdf},
}

@inproceedings{cech_active-speaker_2015,
	title = {Active-speaker detection and localization with microphones and cameras embedded into a robotic head},
	volume = {2015-February},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937845770&doi=10.1109%2fHUMANOIDS.2013.7029977&partnerID=40&md5=fb32d2616b1a23d5d3ea7acf6d262116},
	doi = {10.1109/HUMANOIDS.2013.7029977},
	abstract = {In this paper we present a method for detecting and localizing an active speaker, i.e., a speaker that emits a sound, through the fusion between visual reconstruction with a stereoscopic camera pair and sound-source localization with several microphones. Both the cameras and the microphones are embedded into the head of a humanoid robot. The proposed statistical fusion model associates 3D faces of potential speakers with 2D sound directions. The paper has two contributions: (i) a method that discretizes the two-dimensional space of all possible sound directions and that accumulates evidence for each direction by estimating the time difference of arrival (TDOA) over all the microphone pairs, such that all the microphones are used simultaneously and symmetrically and (ii) an audio-visual alignment method that maps 3D visual features onto 2D sound directions and onto TDOAs between microphone pairs. This allows to implicitly represent both sensing modalities into a common audiovisual coordinate frame. Using simulated as well as real data, we quantitatively assess the robustness of the method against noise and reverberations, and we compare it with several other methods. Finally, we describe a real-time implementation using the proposed technique and with a humanoid head embedding four microphones and two cameras: this enables natural human-robot interactive behavior. © 2013 IEEE.},
	booktitle = {{IEEE}-{RAS} {International} {Conference} on {Humanoid} {Robots}},
	author = {Cech, J. and Mittal, R. and Deleforge, A. and Sanchez-Riera, J. and Alameda-Pineda, X. and Horaud, R.},
	year = {2015},
	note = {Issue: February},
	keywords = {1scopus},
	pages = {203--210},
	annote = {cited By 9},
	file = {Submitted Version:/Users/tid010/Zotero/storage/ASXH7LHX/Cech et al. - 2015 - Active-speaker detection and localization with mic.pdf:application/pdf},
}

@article{ding_optimizing_2011,
	title = {Optimizing motion of robotic manipulators in interaction with human operators},
	volume = {7101 LNAI},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855416359&doi=10.1007%2f978-3-642-25486-4_52&partnerID=40&md5=0aa2f57ea759d981fab527c470863c04},
	doi = {10.1007/978-3-642-25486-4_52},
	abstract = {Recently, the problem of how to manipulate industrial robots that interact with human operators attracts a lot of attention in robotics research. This interest stems from the insight that the integration of human operators into robot based manufacturing systems may increase productivity by combining the abilities of machines with those of humans. In such a Human-Robot-Interaction (HRI) setting, the challenge is to manipulate the robots both safely and efficiently. This paper proposes an online motion planning approach for robotic manipulators with HRI based on model predictive control (MPC) with embedded mixed-integer programming. Safety-relevant regions, which are potentially occupied by the human operators, are generated online using camera data and a knowledge-base of typical human motion patterns. These regions serve as constraints of the optimization problem solved online to generate control trajectories for the robot. As described in the last part of the paper, the proposed method is realized for a HRI scenario. © 2011 Springer-Verlag.},
	number = {PART 1},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Ding, H. and Wijaya, K. and Reißig, G. and Stursberg, O.},
	year = {2011},
	keywords = {1scopus},
	pages = {520--531},
	annote = {cited By 3},
	file = {Full Text:/Users/tid010/Zotero/storage/MARBG93E/Ding et al. - 2011 - Optimizing motion of robotic manipulators in inter.pdf:application/pdf},
}

@inproceedings{van_den_broek_ergonomic_2020,
	title = {Ergonomic adaptation of robotic movements in human-robot collaboration},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083279267&doi=10.1145%2f3371382.3378304&partnerID=40&md5=b6d1403a5e93d222615697a12983cd90},
	doi = {10.1145/3371382.3378304},
	abstract = {Musculoskeletal Disorders (MSDs) are common occupational diseases. An interesting research question is whether collaborative robots actively can minimise the risk of MSDs during collaboration. In this work ergonomic adaptation of robotic movements during human-robot collaboration is explored in a first test case, namely, adjustment of work sureface height. Vision based markerless posture estimation is used as input in combination with ergonomic assessment methods to adapt robotic movements in order to facilitate better ergonomic conditions for the human worker. © 2020 ACM.},
	booktitle = {{ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	author = {Van Den Broek, M.K. and Moeslund, T.B.},
	year = {2020},
	keywords = {1scopus},
	pages = {499--501},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/P4YVZ522/Van Den Broek and Moeslund - 2020 - Ergonomic adaptation of robotic movements in human.pdf:application/pdf},
}

@inproceedings{lang_research_2020,
	title = {Research on human-robot natural interaction algorithm based on body potential perception},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082169504&doi=10.1145%2f3379247.3379256&partnerID=40&md5=dc7e132f1b350b6ae9d15e8bd29f2b41},
	doi = {10.1145/3379247.3379256},
	abstract = {At present, human-computer interaction has become a research hotspot in the field of computer. In the field of vision of humancomputer interaction, identifying and tracking human activities is still a highly concerned task for researchers. Sensors and communication technologies are being used to capture human motion and provide an interactive way for human-computer cooperation. However, using intelligent computing method to identify human activities in different scenes is still a field rarely studied by researchers. In this paper, we make full use of an intelligent calculation method to recognize other people's activities and carry out human-computer interaction. By detecting the information of human joints, we calculate the movement rate of joints, and then effectively identify simple human actions. Through experiments, paper robot can accurately respond to human actions. © 2020 ACM International Conference Proceeding Series. All rights reserved.},
	booktitle = {{ACM} {International} {Conference} {Proceeding} {Series}},
	author = {Lang, X. and Feng, Z. and Yang, X.},
	year = {2020},
	keywords = {1scopus},
	pages = {260--264},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/ATGFK65P/Lang et al. - 2020 - Research on human-robot natural interaction algori.pdf:application/pdf},
}

@inproceedings{yuan_development_2018,
	title = {Development of a human-friendly robot for socially aware human-robot interaction},
	volume = {2018-January},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050746359&doi=10.1109%2fICARM.2017.8273138&partnerID=40&md5=3044374569519247f83cb032a38c872c},
	doi = {10.1109/ICARM.2017.8273138},
	abstract = {One of the fundamental issues for service robots is human-robot interaction. In order to provide desired services, these robots need audiovisual perception of humans and to make appropriate feedback. In this paper, we present our system for spontaneous speech recognition, localization and identification of the user, recognition of the user's gestures, so as to perform the corresponding tasks like mapping, following the user and heading to designated location meanwhile avoiding the obstacles on the way. The system employs a new method of multi-feature detection for robot self-localization based on the recognition of artificial and natural features extracted from laser scans and RGB-D images, and they are quite discriminative in cluttered environments. An angle potential field (APF) method was used for obstacle avoidance. Experiments demonstrate the feasibility and effectiveness of the strategies in the paper. © 2017 IEEE.},
	booktitle = {2017 2nd {International} {Conference} on {Advanced} {Robotics} and {Mechatronics}, {ICARM} 2017},
	author = {Yuan, W. and Li, Z.},
	year = {2018},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {76--81},
	annote = {cited By 4},
	file = {Full Text:/Users/tid010/Zotero/storage/IUPHTJNN/Yuan and Li - 2018 - Development of a human-friendly robot for socially.pdf:application/pdf},
}

@article{castellano_context-sensitive_2014,
	title = {Context-sensitive affect recognition for a robotic game companion},
	volume = {4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983580321&doi=10.1145%2f2622615&partnerID=40&md5=54f92fb79665b7d2794692e706de17b5},
	doi = {10.1145/2622615},
	abstract = {Social perception abilities are among the most important skills necessary for robots to engage humans in natural forms of interaction. Affect-sensitive robots are more likely to be able to establish and maintain believable interactions over extended periods of time. Nevertheless, the integration of affect recognition frameworks in real-time human-robot interaction scenarios is still underexplored. In this article, we propose and evaluate a context-sensitive affect recognition framework for a robotic game companion for children. The robot can automatically detect affective states experienced by children in an interactive chess game scenario. The affect recognition framework is based on the automatic extraction of task features and social interaction-based features. Vision-based indicators of the children's nonverbal behaviour are merged with contextual features related to the game and the interaction and given as input to support vector machines to create a context-sensitive multimodal system for affect recognition. The affect recognition framework is fully integrated in an architecture for adaptive human-robot interaction. Experimental evaluation showed that children's affect can be successfully predicted using a combination of behavioural and contextual data related to the game and the interaction with the robot. It was found that contextual data alone can be used to successfully predict a subset of affective dimensions, such as interest toward the robot. Experiments also showed that engagement with the robot can be predicted using information about the user's valence, interest and anticipatory behaviour. These results provide evidence that social engagement can be modelled as a state consisting of affect and attention components in the context of the interaction. © 2014 ACM.},
	number = {2},
	journal = {ACM Transactions on Interactive Intelligent Systems},
	author = {Castellano, G. and Leite, I. and Pereira, A. and Martinho, C. and Paiva, A. and McOwan, P.W.},
	year = {2014},
	keywords = {1scopus, 1search1, 1include\_search1},
	annote = {cited By 14},
	file = {Full Text:/Users/tid010/Zotero/storage/GRHHDMMY/Castellano et al. - 2014 - Context-sensitive affect recognition for a robotic.pdf:application/pdf},
}

@inproceedings{zardykhan_collision_2019,
	title = {Collision {Preventing} {Phase}-{Progress} {Control} for {Velocity} {Adaptation} in {Human}-{Robot} {Collaboration}},
	volume = {2019-October},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082653866&doi=10.1109%2fHumanoids43949.2019.9035065&partnerID=40&md5=8ab723f79586df840401dc0916d054d9},
	doi = {10.1109/Humanoids43949.2019.9035065},
	abstract = {As robots are leaving dedicated areas on the factory floor and start to share workspaces with humans, safety of such collaboration becomes a major challenge. In this work, we propose new approaches to robot velocity modulation: While the robot is on a path prescribed by the task, it predicts possible collisions with the human and gradually slows down, proportionally to the danger of collision. Two principal approaches are developed-Impulse Orb and Prognosis Window-That dynamically determine the possible robot-induced collisions and apply a novel velocity modulating approach, in which the phase progress of the robot trajectory is modulated while the desired robot path remains intact. The methods guarantee that the robot will halt before contacting the human, but they are less conservative and more flexible than solutions using reduced speed and complete stop only, thereby increasing the effectiveness of human-robot collaboration. This approach is especially useful in constrained setups where the robot path is prescribed. Speed modulation is smooth and does not lead to abrupt motions, making the behavior of the robot also better understandable for the human counterpart. The two principal methods under different parameter settings are experimentally validated in a human-robot interaction scenario with the Franka Emika Panda robot, an external RGB-D camera, and human keypoint detection using OpenPose. © 2019 IEEE.},
	booktitle = {{IEEE}-{RAS} {International} {Conference} on {Humanoid} {Robots}},
	author = {Zardykhan, D. and Svarny, P. and Hoffmann, M. and Shahriari, E. and Haddadin, S.},
	year = {2019},
	keywords = {1scopus},
	pages = {266--273},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/SHTJQ3ZK/Zardykhan et al. - 2019 - Collision Preventing Phase-Progress Control for Ve.pdf:application/pdf},
}

@article{shieh_fuzzy_2014,
	title = {Fuzzy visual detection for human-robot interaction},
	volume = {31},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84914166846&doi=10.1108%2fEC-11-2012-0292&partnerID=40&md5=14985345134e1bbb1d9a8dfdfe620b4c},
	doi = {10.1108/EC-11-2012-0292},
	abstract = {Purpose - The purpose of this paper is to propose a fast object detection algorithm based on structural light analysis, which aims to detect and recognize human gesture and pose and then to conclude the respective commands for human-robot interaction control. Design/methodology/approach - In this paper, the human poses are estimated and analyzed by the proposed scheme, and then the resultant data concluded by the fuzzy decision-making system are used to launch respective robotic motions. The RGB camera and the infrared light module aim to do distance estimation of a body or several bodies. Findings - The modules not only provide image perception but also objective skeleton detection. In which, a laser source in the infrared light module emits invisible infrared light which passes through a filter and is scattered into a semi-random but constant pattern of small dots which is projected onto the environment in front of the sensor. The reflected pattern is then detected by an infrared camera and analyzed for depth estimation. Since the depth of object is a key parameter for pose recognition, one can estimate the distance to each dot and then get depth information by calculation of distance between emitter and receiver. Research limitations/implications - Future work will consider to reduce the computation time for objective estimation and to tune parameters adaptively. Practical implications - The experimental results demonstrate the feasibility of the proposed system. Originality/value - This paper achieves real-time human-robot interaction by visual detection based on structural light analysis. © Emerald Group Publishing Limited.},
	number = {8},
	journal = {Engineering Computations (Swansea, Wales)},
	author = {Shieh, M.-Y. and Hsieh, C.-Y. and Hsieh, T.-M.},
	year = {2014},
	keywords = {1scopus},
	pages = {1709--1719},
	annote = {cited By 3},
	file = {Full Text:/Users/tid010/Zotero/storage/KGXKVNS6/Shieh et al. - 2014 - Fuzzy visual detection for human-robot interaction.pdf:application/pdf},
}

@inproceedings{deepan_raj_static_2017,
	title = {Static gesture recognition based precise positioning of 5-{DOF} robotic arm using {FPGA}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034841422&doi=10.1109%2fTIMA.2017.8064804&partnerID=40&md5=69c2b52e2e84606ea0250b60fc96d842},
	doi = {10.1109/TIMA.2017.8064804},
	abstract = {Human machine interactions plays a vital role in numerous applications such as biomedical, communication systems, automobiles and also in commercial electronic equipment. During surgeries, utilizing a robotic arm to perform the task will help in maintaining a non-infectious environment, thereby aid in the speedy recovery of patients. During industrial inspection of materials or products, usage of robotic arm will speed up the process and results in an increase in throughput. However, in order to perform these tasks effectively and efficiently, precise positioning of the robotic arm is essential. Also the robotic arm needs to be controlled dynamically from a remote location. In this research work, static gesture based recognition system and precise positioning of a robotic arm using field programmable gate arrays is proposed. User hand gestures are recognized using machine vision algorithms implement in Odroid-XU4. These gestures are used to interact with the 5 DOF-robotic arm via field programmable gate arrays (FPGA). FPGAs are used for simultaneous controlling the actuators (Parallel controlling) in different regions of the robotic arm. Hence, by speeding up the process of positioning the robotic arm at a precise position. © 2017 IEEE.},
	booktitle = {Proceedings - {TIMA} 2017: 9th {International} {Conference} on {Trends} in {Industrial} {Measurement} and {Automation}},
	author = {Deepan Raj, M. and Gogul, I. and Thangaraja, M. and Kumar, V.S.},
	year = {2017},
	keywords = {1scopus},
	annote = {cited By 5},
	file = {Full Text:/Users/tid010/Zotero/storage/RM5RWGCU/Deepan Raj et al. - 2017 - Static gesture recognition based precise positioni.pdf:application/pdf},
}

@article{hu_design_2014,
	title = {Design of sensing system and anticipative behavior for human following of mobile robots},
	volume = {61},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887313680&doi=10.1109%2fTIE.2013.2262758&partnerID=40&md5=7f55d61c689f2c7babdb9912cfaa28d9},
	doi = {10.1109/TIE.2013.2262758},
	abstract = {The human-following behavior design for mobile robots is considered in this paper. In particular, the issue should be addressed from the perspective of human-robot interaction since humans are aware of the following actions. This makes the problem quite different from human tracking where recognition and location accuracy are the main concerns. An anticipative humanfollowing behavior is proposed by incorporating the human model. The human model is constructed using relevant scientific studies about human walk and social interaction, allowing the robot to predict the human trajectory and to take preemptive action. To realize the idea, it is necessary to have a robust sensing system that is capable of tracking the human location persistently. In this paper, we also propose a sensing system based on a novel 3-D meanshift algorithm on RGBD camera. The system performance is assessed through experimental evaluation of three specific humanfollowing scenarios: following from behind, following on the side, and following in front. Each of these scenarios has its particularities and applications, thus providing insight about the effectiveness and usability of anticipative behavior. © 2013 IEEE.},
	number = {4},
	journal = {IEEE Transactions on Industrial Electronics},
	author = {Hu, J.-S. and Wang, J.-J. and Ho, D.M.},
	year = {2014},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {1916--1927},
	annote = {cited By 55},
	file = {Full Text:/Users/tid010/Zotero/storage/F5CN5IG9/Hu et al. - 2014 - Design of sensing system and anticipative behavior.pdf:application/pdf},
}

@article{wu_improved_2019,
	title = {An improved method of optical flow using human body-following wheeled robot},
	volume = {10},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061725711&doi=10.1142%2fS179396231950003X&partnerID=40&md5=685ab13df4f90ebe2c0932f3b63a6e4b},
	doi = {10.1142/S179396231950003X},
	abstract = {The theory and technology of human-machine coordination and natural interaction have a wide range of application prospect in future smart factories. This paper elaborates on the design and implementation of a body-following wheeled robot system based on Kinect, as well as the use of gesture recognition function to enhance the interactive performance. An improved optical flow method is put forward to obtain the direction and speed of the target movement. The smoothing parameters in traditional optical flow are replaced by variables. The new smoothing parameter is related to the local gradient value. Compared with the traditional optical flow method, it can reflect the status of moving objects more clearly, reduce noise and ensure real-time performance, solving the problem of tracking state oscillation caused by the skeleton node drifts when the target is occluded. The experiment on the wheeled robot confirms that the system can accomplish the tracking task in a preferable way. © 2019 World Scientific Publishing Company.},
	number = {2},
	journal = {International Journal of Modeling, Simulation, and Scientific Computing},
	author = {Wu, Y. and Yang, Q. and Zhou, X.},
	year = {2019},
	keywords = {1scopus},
	annote = {cited By 0},
	file = {Wu et al. - 2019 - An improved method of optical flow using human bod.pdf:/Users/tid010/Zotero/storage/N7VGJPG6/Wu et al. - 2019 - An improved method of optical flow using human bod.pdf:application/pdf},
}

@inproceedings{moh_gesture_2019,
	title = {Gesture {Recognition} and {Effective} {Interaction} {Based} {Dining} {Table} {Cleaning} {Robot}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078065867&doi=10.1109%2fRITAPP.2019.8932802&partnerID=40&md5=cc13aae5f44d7288f1e715dec4bc863a},
	doi = {10.1109/RITAPP.2019.8932802},
	abstract = {We present a framework for dining table cleaning robot, which enables the robot to detect the cleaning target and perform cleaning task correspondingly to the given instruction, without needing prior information of the cleaning target. A cleaning robot should be able to detect the object efficiently. In order to enable object detection without prior information, the background subtraction method is employed, which is based on the 3D point group data taken by a RGB-D camera. In addition to object detection, a cleaning robot should be able to modify its movement in accordance with the user's instructions. Therefore, we propose an interaction system which allows the user to use gesture to provide instructions to the robot. A pointing gesture is used to specify the cleaning target. When the information needed for the cleaning task is insufficient, the robot will ask for further information from the user. If multiple objects are detected, the robot will rank all the objects according to their distance from the pointed coordinate. The user can re-designate the cleaning target with preregistered gesture commands. Once the robot has collected enough information for its duty, it will execute the cleaning task specified by the user. © 2019 IEEE.},
	booktitle = {2019 7th {International} {Conference} on {Robot} {Intelligence} {Technology} and {Applications}, {RiTA} 2019},
	author = {Moh, J.J. and Kijima, T. and Zhang, B. and Lim, H.-O.},
	year = {2019},
	keywords = {1scopus},
	pages = {72--77},
	annote = {cited By 1},
	file = {Full Text:/Users/tid010/Zotero/storage/C6JPBC3G/Moh et al. - 2019 - Gesture Recognition and Effective Interaction Base.pdf:application/pdf},
}

@article{avioz-sarig_robotic_2020,
	title = {Robotic {System} for {Physical} {Training} of {Older} {Adults}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091774621&doi=10.1007%2fs12369-020-00697-y&partnerID=40&md5=cafd1bc2d8be5d903c38a6c05577b679},
	doi = {10.1007/s12369-020-00697-y},
	abstract = {Physical exercise has many physical, psychological and social health benefits leading to improved life quality. This paper presents a robotic system developed as a personal coach for older adults aiming to motivate older adults to participate in physical activities. The robot instructs the participants, demonstrates the exercises and provides real-time corrective and positive feedback according to the participant’s performance as monitored by an RGB-D camera. Two robotic systems based on two different humanoid robots (Nao, toy-like and Poppy, mechanical-like) were developed and implemented using the Python programming language. Experimental studies with 32 older adults were conducted, to determine the preferable mode and timing of the feedback provided to the user to accommodate user preferences, motivate the users and improve their interaction with the system. Additionally, user preferences with regards to the two different humanoid robots used were explored. The results revealed that the system motivated the older adults to engage more in physical exercises. The type and timing of feedback influenced this engagement. Most of these older adults also perceived the system as very useful, easy to use, had a positive attitude towards the system and noted their intention to use it. Most users preferred the more mechanical looking robot (Poppy) over the toy-like robot (Nao). © 2020, The Author(s).},
	journal = {International Journal of Social Robotics},
	author = {Avioz-Sarig, O. and Olatunji, S. and Sarne-Fleischmann, V. and Edan, Y.},
	year = {2020},
	keywords = {1scopus},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/MZHNW3GW/Avioz-Sarig et al. - 2020 - Robotic System for Physical Training of Older Adul.pdf:application/pdf},
}

@inproceedings{kobayashi_people_2010,
	title = {People tracking using integrated sensors for human robot interaction},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954397137&doi=10.1109%2fICIT.2010.5472444&partnerID=40&md5=87753bd7f9570d5c4190be21ffea4600},
	doi = {10.1109/ICIT.2010.5472444},
	abstract = {In human-human interaction, position and orientation of participants' bodies and faces play an important role. Thus, robots need to be able to detect and track human bodies and faces, and obtain human positions and orientations to achieve effective human-robot interaction. It is difficult, however, to robustly obtain such information from video cameras alone in complex environments. Hence, we propose to use integrated sensors that are composed of a laser range sensor and an omni-directional camera. A Rao-Blackwellized particle filter framework is employed to track the position and orientation of both bodies and heads of people based on the distance data and panorama images captured from the laser range sensor and the omni-directional camera. In addition to the tracking techniques, we present two applications of our integrated sensor system. One is a robotic wheelchair moving with a caregiver; the sensor system detects and tracks the caregiver and the wheelchair moves with the caregiver based on the tracking results. The other is a museum guide robot that explains exhibits to multiple visitors; the position and orientation data of visitors' bodies and faces enable the robot to distribute its gaze to each of multiple visitors to keep their attention while talking. ©2010 IEEE.},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Industrial} {Technology}},
	author = {Kobayashi, Y. and Kuno, Y.},
	year = {2010},
	keywords = {1scopus},
	pages = {1617--1622},
	annote = {cited By 22},
	file = {Full Text:/Users/tid010/Zotero/storage/I2FWBR47/Kobayashi and Kuno - 2010 - People tracking using integrated sensors for human.pdf:application/pdf},
}

@article{ikai_robot_2016,
	title = {Robot control using natural instructions via visual and tactile sensations},
	volume = {12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984855492&doi=10.3844%2fjcssp.2016.246.254&partnerID=40&md5=aeaac14791e81f3b2ef5781bc9b28365},
	doi = {10.3844/jcssp.2016.246.254},
	abstract = {Stress-free interaction between humans and robots is necessary to support humans in daily life. In order to achieve this, we anticipate the development of new robots equipped with tactile and vision sensors for receiving human instructions. In this article, we focus on spontaneous movements that do not require training, such as pointing and force adjustment and that are suitable for daily care. These movements, which we call natural instructions, involve the transmission of human instructions to robots. In this experiment, we examine a robot equipped with vision and tactile sensors capable of receiving natural instructions. Our new robot accomplishes a retrieving and passing task using the natural instructions of finger pointing and tapping with the palm. © 2016 Takuya Ikai, Shota Kamiya and Masahiro Ohka.},
	number = {5},
	journal = {Journal of Computer Science},
	author = {Ikai, T. and Kamiya, S. and Ohka, M.},
	year = {2016},
	keywords = {1scopus},
	pages = {246--254},
	annote = {cited By 7},
	file = {Full Text:/Users/tid010/Zotero/storage/TWKJDWSD/Ikai et al. - 2016 - Robot control using natural instructions via visua.pdf:application/pdf},
}

@article{chen_wristcam_2019,
	title = {{WristCam}: {A} {Wearable} {Sensor} for {Hand} {Trajectory} {Gesture} {Recognition} and {Intelligent} {Human}-{Robot} {Interaction}},
	volume = {19},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055678617&doi=10.1109%2fJSEN.2018.2877978&partnerID=40&md5=711d03def60c70115cf0ea31710b4c2d},
	doi = {10.1109/JSEN.2018.2877978},
	abstract = {As a promising component for body sensor networks, the wearable sensors for hand gesture recognition have increasingly received great attention in recent years. By interpreting human intentions through hand gestures, the natural human-robot interaction can be realized in the smart home where the youth and the elderly can perform hand gestures to control the household robot or the robotic wheelchair. Here, a wearable wrist-worn camera sensor was shown to recognize hand trajectory gestures. The moving velocity of the user's hand was deduced from the matched speeded up robust features keypoints of the moving background of the video sequence. Furthermore, the segmentation of continuous gestures was achieved by detecting the predefined gesture starting signal from the hand region of the image, which was segmented by the lazy snapping algorithm. In this paper, 10 types of gestures and 1350 gesture samples collected from 15 subjects at three different scenes were classified by the dynamic time warping algorithm and the results achieved an average recognition accuracy up to 97.6\%. Moreover, the practicability of the proposed system was further demonstrated by controlling a cooperative robot to draw letters on paper. © 2001-2012 IEEE.},
	number = {19},
	journal = {IEEE Sensors Journal},
	author = {Chen, F. and Lv, H. and Pang, Z. and Zhang, J. and Hou, Y. and Gu, Y. and Yang, H. and Yang, G.},
	year = {2019},
	keywords = {1scopus},
	pages = {8441--8451},
	annote = {cited By 8},
	file = {Full Text:/Users/tid010/Zotero/storage/SHA28H3I/Chen et al. - 2019 - WristCam A Wearable Sensor for Hand Trajectory Ge.pdf:application/pdf},
}

@inproceedings{zhuang_learning_2016,
	title = {Learning by showing: {An} end-to-end imitation leaning approach for robot action recognition and generation},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016723025&doi=10.1109%2fROBIO.2016.7866317&partnerID=40&md5=16377b5630a7b3b351706699ac179f7a},
	doi = {10.1109/ROBIO.2016.7866317},
	abstract = {Human action recognition and generation for imitation learning are very important topic of the robot-human interaction research field. In this paper, we present a novel approach for human action recognition and robot action generation based on Kinect motion captured data using Hidden Markov Models (HMMs). The robot recognizes the captured human actions using HMMs, and generates the similar actions by the identical learned HMMs. Different from the traditional robot action generation methods, our system generates the robot action and its parameters only from the HMM which is learned from the recognition phase. In this paper, it is a very important point that the robot can recognize and generate action using an identical HMM, and the robot do not need to record any trajectory data for the action generation using transitional method, such as Dynamic Movement Primitives (DMPs). Since the robot action and its parameters are generated from HMMs, we can adjust the parameters to change the robot action speed. In order to improve the action accuracy, we employ an Augmented Lagrange Multiplier method (ALM) to fine-tune the trajectory of the generated action. So that, the fine-tuned action adjusts its trajectory to accurately reach the target point, and keep the similar style to the original action roughly. © 2016 IEEE.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Robotics} and {Biomimetics}, {ROBIO} 2016},
	author = {Zhuang, C. and Zhou, H. and Sakane, S.},
	year = {2016},
	keywords = {1scopus},
	pages = {173--178},
	annote = {cited By 4},
	file = {Full Text:/Users/tid010/Zotero/storage/SZ4MVASS/Zhuang et al. - 2016 - Learning by showing An end-to-end imitation leani.pdf:application/pdf},
}

@article{morato_toward_2014,
	title = {Toward safe human robot collaboration by using multiple kinects based real-time human tracking},
	volume = {14},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893067977&doi=10.1115%2f1.4025810&partnerID=40&md5=d50d99386b8cb4f0ebabd0768045d2f2},
	doi = {10.1115/1.4025810},
	abstract = {We present a multiple Kinects based exteroceptive sensing framework to achieve safe human-robot collaboration during assembly tasks. Our approach is mainly based on a real-time replication of the human and robot movements inside a physics-based simulation of the work cell. This enables the evaluation of the human-robot separation in a 3D Euclidean space, which can be used to generate safe motion goals for the robot. For this purpose, we develop an N-Kinect system to build an explicit model of the human and a roll-out strategy, in which we forward-simulate the robot's trajectory into the near future. Now, we use a precollision strategy that allows a human to operate in close proximity with the robot, while pausing the robot's motion whenever an imminent collision between the human model and any part of the robot is detected. Whereas most previous range based methods analyzed the physical separation based on depth data pertaining to 2D projections of robot and human, our approach evaluates the separation in a 3D space based on an explicit human model and a forward physical simulation of the robot. Real-time behavior (≈ 30 Hz) observed during experiments with a 5 DOF articulated robot and a human safely collaborating to perform an assembly task validate our approach. © 2014 by ASME.},
	number = {1},
	journal = {Journal of Computing and Information Science in Engineering},
	author = {Morato, C. and Kaipa, K.N. and Zhao, B. and Gupta, S.K.},
	year = {2014},
	keywords = {1scopus},
	annote = {cited By 93},
	file = {Submitted Version:/Users/tid010/Zotero/storage/WATEP6MD/Morato et al. - 2014 - Toward safe human robot collaboration by using mul.pdf:application/pdf},
}

@inproceedings{efthymiou_multi-_2018,
	title = {Multi- {View} {Fusion} for {Action} {Recognition} in {Child}-{Robot} {Interaction}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062904452&doi=10.1109%2fICIP.2018.8451146&partnerID=40&md5=e5f837a6908738f0d2ea18773c48f217},
	doi = {10.1109/ICIP.2018.8451146},
	abstract = {Answering the challenge of leveraging computer vision methods in order to enhance Human Robot Interaction (HRI) experience, this work explores methods that can expand the capabilities of an action recognition system in such tasks. A multi-view action recognition system is proposed for integration in HRI scenarios with special users, such as children, in which there is limited data for training and many state-of-the-art techniques face difficulties. Different feature extraction approaches, encoding methods and fusion techniques are combined and tested in order to create an efficient system that recognizes children pantomime actions. This effort culminates in the integration of a robotic platform and is evaluated under an alluring Children Robot Interaction scenario. © 2018 IEEE.},
	booktitle = {Proceedings - {International} {Conference} on {Image} {Processing}, {ICIP}},
	author = {Efthymiou, N. and Koutras, P. and Filntisis, P.P. and Potamianos, G. and Maragos, P.},
	year = {2018},
	keywords = {1scopus},
	pages = {455--459},
	annote = {cited By 4},
	file = {Full Text:/Users/tid010/Zotero/storage/XY4V9BTS/Efthymiou et al. - 2018 - Multi- View Fusion for Action Recognition in Child.pdf:application/pdf},
}

@inproceedings{bothe_effective_2018,
	title = {Effective {Use} of {Lightweight} {Robots} in {Human}-{Robot} {Workstations} with {Monitoring} {Via} {RGBD}-{Camera}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056463239&doi=10.1109%2fMMAR.2018.8486036&partnerID=40&md5=ea49c81560b46d1b73e34fadc0d0fd29},
	doi = {10.1109/MMAR.2018.8486036},
	abstract = {The new robot generation no longer requires physical separation between humans and robots. Moreover, in order to optimize production processes, humans and robots are expected to interact. This article describes the integration and implementation of technologies intended to increase the flexibility and security of human-robot cooperation. Furthermore, a possible model for managing this transition is described in detail, involving the use of an RGBD camera. With this camera, it should be possible to detect state and position changes of the people at a human-robot workstation and consequently adapt the movements of the robot. Overall, the essential aim of this paper is to suggest ways of increasing economic efficiency within assembly processes, while also increasing security. © 2018 IEEE.},
	booktitle = {2018 23rd {International} {Conference} on {Methods} and {Models} in {Automation} and {Robotics}, {MMAR} 2018},
	author = {Bothe, K. and Winkler, A. and Goldhahn, L.},
	year = {2018},
	keywords = {1scopus},
	pages = {698--702},
	annote = {cited By 0},
	file = {Full Text:/Users/tid010/Zotero/storage/F638ZHZI/Bothe et al. - 2018 - Effective Use of Lightweight Robots in Human-Robot.pdf:application/pdf},
}

@inproceedings{shieh_gesture_2014,
	title = {Gesture motion based interactive control system for humanoid robots},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896635764&partnerID=40&md5=8db7bbbf4078a4da6b8992f3b407a07c},
	abstract = {The paper proposes an interactive control system for humanoid robots based on gesture motion detection and analysis schemes. It involves image recognition and skeleton analysis to determine the motion and its meanings on robot interactive controls. The images detected by a Kinect consists of a 2D image and a depth image, where the 2D image is adopted as the data of facial recognition and motion detection, and the depth image is offered for motion analysis. From experimental results, it's seen that the proposed system is just like a transformer for human-robot interaction and plays good performance on interactive controls. © 2014 Taylor \& Francis Group.},
	booktitle = {Innovation, {Communication} and {Engineering} - {Proceedings} of the 2nd {International} {Conference} on {Innovation}, {Communication} and {Engineering}, {ICICE} 2013},
	author = {Shieh, M.Y. and Chiou, J.S. and Pai, N.S. and Li, J.H.},
	year = {2014},
	keywords = {1scopus},
	pages = {329--332},
	annote = {cited By 0},
	file = {Shieh et al. - 2014 - Gesture motion based interactive control system fo.pdf:/Users/tid010/Zotero/storage/3YI95MBT/Shieh et al. - 2014 - Gesture motion based interactive control system fo.pdf:application/pdf},
}

@inproceedings{torres_implementation_2012,
	title = {Implementation of interactive arm playback behaviors of social robot {Zeno} for autism spectrum disorder therapy},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871952994&doi=10.1145%2f2413097.2413124&partnerID=40&md5=4804665e2578994dd0473781be199cd1},
	doi = {10.1145/2413097.2413124},
	abstract = {In this paper, we describe control algorithms accomplishing human-robot interaction through mimicking behaviors between the humanoid robot Zeno and humans. Specifically, arm and torso motions of the robot follow closely those of the human, this mimicking behavior, can be used for clinical treatment and diagnosis during robot therapy of subjects suffering from Autism Spectrum Disorders (ASD). In this paper, we describe algorithms and results of implementing simple position control schemes on Zeno via visual feedback from Kinect data. The behavior can be used by therapists to achieve desired poses of the robot that may be beneficial to children with ASD by enhancing their motor skills as well as social interaction. Results show that in this case, simple actuators, sensors and control schemes can generate smooth and responsive robot trajectories.},
	booktitle = {{ACM} {International} {Conference} {Proceeding} {Series}},
	author = {Torres, N.A. and Clark, N. and Ranatunga, I. and Popa, D.},
	year = {2012},
	keywords = {1scopus},
	annote = {cited By 10},
	file = {Full Text:/Users/tid010/Zotero/storage/V4SI6CT3/Torres et al. - 2012 - Implementation of interactive arm playback behavio.pdf:application/pdf},
}

@inproceedings{lalejini_evaluation_2015,
	title = {Evaluation of supervisory control interfaces for mobile robot integration with tactical teams},
	volume = {2015-January},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937392110&doi=10.1109%2fARSO.2014.7020971&partnerID=40&md5=c3c1ac4b952f39431a8ef89b264ce835},
	doi = {10.1109/ARSO.2014.7020971},
	abstract = {As robotic systems become more sophisticated, they are increasingly called upon to accompany humans in high-stress environments. This research was conducted to support the integration of robotic systems into tactical teams operating in challenging and stressful environments. Robotic systems used to assist tactical teams will need to support some form of autonomy; these systems must be capable of providing operators supervisory control in cases of unpredictable real-time events. An evaluation of the relative effectiveness of three different methods of supervisory control of an autonomously operated mobile robot system was conducted: (1) hand gestures using a Microsoft Kinect, (2) an interactive Android application on a hand-held mobile device, and (3) verbal commands issued through a headset. These methods of supervisory control were compared to a teleoperated robot using a gamepad controller. The results from this pilot study determined that the touchscreen device was the easiest interface to use to override the robot's next intended movement (L2(3,23)=11.413, p=.003, d=1.58) and was considered the easiest interface to use overall (L2(3,23)=8.078, p=.044, d=.93). The results also indicate that the touchscreen device provided the most enjoyable, satisfying, and engaging interface of the four user interfaces evaluated. © 2014 IEEE.},
	booktitle = {Proceedings of {IEEE} {Workshop} on {Advanced} {Robotics} and its {Social} {Impacts}, {ARSO}},
	author = {Lalejini, A. and Duckworth, D. and Sween, R. and Bethel, C.L. and Carruth, D.},
	year = {2015},
	note = {Issue: January},
	keywords = {1scopus},
	pages = {1--6},
	annote = {cited By 2},
	file = {Full Text:/Users/tid010/Zotero/storage/TQYAV3FX/Lalejini et al. - 2015 - Evaluation of supervisory control interfaces for m.pdf:application/pdf},
}

@article{wang_vision-guided_2013,
	title = {Vision-guided active collision avoidance for human-robot collaborations},
	volume = {1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892776649&doi=10.1016%2fj.mfglet.2013.08.001&partnerID=40&md5=b9986aaaa63828dcaa8b3af6543320af},
	doi = {10.1016/j.mfglet.2013.08.001},
	abstract = {This paper reports a novel methodology of real-time active collision avoidance in an augmented environment, where virtual 3D models of robots and real camera images of operators are used for monitoring and collision detection. A prototype system is developed and linked to robot controllers for adaptive robot control, with zero robot programming for end users. According to the result of collision detection, the system can alert an operator, stop a robot, or modify the robot's trajectory away from an approaching operator. Through a case study, it shows that this method can be applied to real-world applications such as human-robot collaborative assembly to safeguard human operators. © 2013 Society of Manufacturing Engineers (SME).},
	number = {1},
	journal = {Manufacturing Letters},
	author = {Wang, L. and Schmidt, B. and Nee, A.Y.C.},
	year = {2013},
	keywords = {1scopus},
	pages = {5--8},
	annote = {cited By 41},
	file = {Wang et al. - 2013 - Vision-guided active collision avoidance for human.pdf:/Users/tid010/Zotero/storage/49E6WWB2/Wang et al. - 2013 - Vision-guided active collision avoidance for human.pdf:application/pdf},
}

@inproceedings{do_human-robot_2014,
	title = {Human-robot collaboration in a {Mobile} {Visual} {Sensor} {Network}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929224248&doi=10.1109%2fICRA.2014.6907163&partnerID=40&md5=b76f3181b4da4d12e68c5e5815f5158a},
	doi = {10.1109/ICRA.2014.6907163},
	abstract = {This paper proposes and implements a framework for human-robot collaboration in a Mobile Visual Sensor Network (MVSN). A collaborative architecture for the proposed human-integrated MVSN was developed to allow the human operator and robots to collaborate to perform surveillance tasks. We successfully implemented the MVSN so the user can control the deployment of the mobile sensors through his head movement. We also explored using computer vision techniques and navigation techniques on the robot nodes to conduct active human target detection. The robot nodes, therefore, are able to detect human faces while exploring the unknown environment, and then relay the face images to the operator for target recognition. In this way, humans and robots can complement each other to accomplish surveillance tasks. Our experimental results validated the proposed framework. © 2014 IEEE.},
	booktitle = {Proceedings - {IEEE} {International} {Conference} on {Robotics} and {Automation}},
	author = {Do, H.M. and Mouser, C. and Liu, M. and Sheng, W.},
	year = {2014},
	keywords = {1scopus, 1search1, 1include\_search1},
	pages = {2203--2208},
	annote = {cited By 2},
	file = {Full Text:/Users/tid010/Zotero/storage/X7Q9RQWN/Do et al. - 2014 - Human-robot collaboration in a Mobile Visual Senso.pdf:application/pdf},
}

@inproceedings{dometios_real-time_2017,
	title = {Real-time end-effector motion behavior planning approach using on-line point-cloud data towards a user adaptive assistive bath robot},
	volume = {2017-September},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041964348&doi=10.1109%2fIROS.2017.8206387&partnerID=40&md5=b12bef66e988cdd2965b78bc84050232},
	doi = {10.1109/IROS.2017.8206387},
	abstract = {Elderly people have particular needs in performing bathing activities, since these tasks require body flexibility. Our aim is to build an assistive robotic bath system, in order to increase the independence and safety of this procedure. Towards this end, the expertise of professional carers for bathing sequences and appropriate motions has to be adopted, in order to achieve natural, physical human - robot interaction. In this paper, a real-time end-effector motion planning method for an assistive bath robot, using on-line Point-Cloud information, is proposed. The visual feedback obtained from Kinect depth sensor is employed to adapt suitable washing paths to the user's body part motion and deformable surface. We make use of a navigation function-based controller, with guarantied globally uniformly asymptotic stability, and bijective transformations for the adaptation of the paths. Experiments were conducted with a rigid rectangular object for validation purposes, while a female subject took part to the experiment in order to evaluate and demonstrate the basic concepts of the proposed methodology. © 2017 IEEE.},
	booktitle = {{IEEE} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	author = {Dometios, A.C. and Papageorgiou, X.S. and Arvanitakis, A. and Tzafestas, C.S. and Maragos, P.},
	year = {2017},
	keywords = {1scopus},
	pages = {5031--5036},
	file = {Full Text:/Users/tid010/Zotero/storage/5Y28KW3U/Dometios et al. - 2017 - Real-time end-effector motion behavior planning ap.pdf:application/pdf},
}

@inproceedings{hafiane_3d_2013,
	title = {{3D} hand recognition for telerobotics},
	doi = {10.1109/ISCI.2013.6612390},
	abstract = {This paper presents a system for recognition of 3D hand gestures for the purpose controlling and manipulating robots. This objective of the work is to allow the robot to mimic or imitate the recognized gesture which can be used for remote manipulation of a robotic arm to perform complex task (teleoperation). Telerobotics systems rely on computer vision to create the human-machine interface. In this project, hand tracking was used as an intuitive control interface because it represents a natural interaction medium. The system tracks the hand of the operator and the gesture it represents, and relays the appropriate signal to the robot to perform the respective action in real time. The study focuses on two gestures, open hand, and closed hand, as the NAO robot is not equipped with a dexterous hand. SURF features points have been used to represent the hand gesture and face to hand distance was used to gauge the depth of the hand. This system has been test with Aldebaran NAO robot for performing different gesture imitation task for picking and placing objects.},
	booktitle = {2013 {IEEE} {Symposium} on {Computers} {Informatics} ({ISCI})},
	author = {Hafiane, Saad and Salih, Yasir and Malik, Aamir S.},
	month = apr,
	year = {2013},
	keywords = {1final\_includes, 1include\_search6, 2D hand gestures, Cameras, Face, Feature extraction, human-machine interface, Instruction sets, NAO robot, Robustness, SURF, telerobotics, Telerobotics},
	pages = {132--137},
	file = {Full Text:/home/brendan/Zotero/storage/38KETC3B/Hafiane et al. - 2013 - 3D hand recognition for telerobotics.pdf:application/pdf}
}

@inproceedings{uribe_mobile_2011,
	title = {Mobile robotic teleoperation using gesture-based human interfaces},
	doi = {10.1109/LARC.2011.6086812},
	abstract = {In this work a natural interaction framework for programming a mobile robot with gestures is developed using two low-cost human interface devices available on the market. The use of natural motion has been growing among user interface researches and developers for offering comfortable ways of interacting with the devices around us. Some examples can be seen in how touch screens, accelerometers and image processing have influenced our interactions with cellphones, gaming devices and computers, thus, allowing us to take advantage of our body ergonomics. The objective of this work is to integrate an intuitive tool for teleoperating a mobile robot through gestures as an alternative input for encouraging non-experts to relate with robotics and ease navigation tasks. For accomplishing the objective of this work, the mobile robots programming architecture is studied and integrated with those of the user interfaces for allowing the teleoperation of the robotics device. To check how the implemented framework impacts the user interaction, a navigation scenario with obstacles is used for validating the suitability of the gesture-based navigation.},
	booktitle = {{IX} {Latin} {American} {Robotics} {Symposium} and {IEEE} {Colombian} {Conference} on {Automatic} {Control}, 2011 {IEEE}},
	author = {Uribe, Alvaro and Alves, Silas and Rosário, João M. and Filho, Humberto Ferasoli and Pérez-Gutiérrez, Byron},
	month = oct,
	year = {2011},
	keywords = {1final\_includes, 1include\_search6, Collision avoidance, Computer architecture, Educational robots, Mobile robots, Navigation, Robot sensing systems},
	pages = {1--6},
	file = {Full Text:/home/brendan/Zotero/storage/AYSJLRSY/Uribe et al. - 2011 - Mobile robotic teleoperation using gesture-based h.pdf:application/pdf}
}

@inproceedings{devanne_co-design_2018,
	title = {A {Co}-design {Approach} for a {Rehabilitation} {Robot} {Coach} for {Physical} {Rehabilitation} {Based} on the {Error} {Classification} of {Motion} {Errors}},
	doi = {10.1109/IRC.2018.00074},
	abstract = {The rising number of the elderly incurs growing concern about healthcare, and in particular rehabilitation healthcare. Assistive technology and assistive robotics in particular may help to improve this process. We develop a robot coach capable of demonstrating rehabilitation exercises to patients, watch a patient carry out the exercises and give him feedback so as to improve his performance and encourage him. The HRI of the system is based on our study with a team of rehabilitation therapists and with the target population. The system relies on human motion analysis. We develop a method for learning a probabilistic representation of ideal movements from expert demonstrations. A Gaussian Mixture Model is employed from position and orientation features captured using a Microsoft Kinect v2. For assessing patients' movements, we propose a real-time multi-level analysis to both temporally and spatially identify and explain body part errors. This analysis combined with a classification algorithm allows the robot to provide coaching advice to make the patient improve his movements. The evaluation on three rehabilitation exercises shows the potential of the proposed approach for learning and assessing kinaesthetic movements.},
	booktitle = {2018 {Second} {IEEE} {International} {Conference} on {Robotic} {Computing} ({IRC})},
	author = {Devanne, Maxime and Nguyen, Sao Mai and Remy-Neris, Olivier and Le Gals-Garnett, Beatrice and Kermarrec, Gilles and Thepaut, Andre},
	month = jan,
	year = {2018},
	keywords = {1final\_includes, 1include\_search6, Back, clinical tests, Gaussian Mixture Model, Hidden Markov models, human motion analysis, Manifolds, movement analysis, Pain, physical rehabilitation, rehabilitation robotics, robot coach, Robots, Skeleton, Statistics},
	pages = {352--357},
	file = {Submitted Version:/home/brendan/Zotero/storage/DF9SVQPK/Devanne et al. - 2018 - A Co-design Approach for a Rehabilitation Robot Co.pdf:application/pdf}
}

@inproceedings{guo_control_2016,
	title = {A control system of human-computer interaction based on kinect somatosensory equipment},
	doi = {10.1109/CCDC.2016.7531921},
	abstract = {The requirement of human-robot interaction has become increasingly high with the improvement of the robot technology. This paper presents a robot control system which is based on the Kinect. The system through the body movements and gestures to control the robot. This system is divided into two parts: information collection system and action execution system. The human action information is collected by Kinect somatosensory equipment' after the PC analysis parse out the appropriate action' the information is transmitted to the actions performed robot through the bluetooth wireless module. Information will be received by the robot and the robot controller will control the robot to make the corresponding action' this enables it to achieve real-time human imitation.},
	booktitle = {2016 {Chinese} {Control} and {Decision} {Conference} ({CCDC})},
	author = {Guo, Liang and Liu, Chenxi and Wen, Xiaoyan and Chen, Haohua and Zhang, Jianghui},
	month = may,
	year = {2016},
	note = {ISSN: 1948-9447},
	keywords = {1final\_includes, 1include\_search6, Cameras, Elbow, Human computer interaction, Human-Computer Interaction, Kinect, Robot Control System, Robot kinematics, Servomotors, Skeleton},
	pages = {5170--5175},
	file = {Full Text:/home/brendan/Zotero/storage/AV3PFA67/Guo et al. - 2016 - A control system of human-computer interaction bas.pdf:application/pdf}
}

@article{li_cyber-physical_2013,
	title = {A cyber-physical management system for delivering and monitoring surgical instruments in the or},
	volume = {20},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880320220&doi=10.1177%2f1553350612459109&partnerID=40&md5=d6e68220c396a43fb329d0e4da91567d},
	doi = {10.1177/1553350612459109},
	abstract = {Background. The standard practice in the operating room (OR) is having a surgical technician deliver surgical instruments to the surgeon quickly and inexpensively, as required. This human "in the loop" system may result in mistakes (eg, missing information, ambiguity of instructions, and delays). Objective. Errors can be reduced or eliminated by integrating information technology (IT) and cybernetics into the OR. Gesture and voice automatic acquisition, processing, and interpretation allow interaction with these new systems without disturbing the normal flow of surgery. Methods. This article describes the development of a cyber-physical management system (CPS), including a robotic scrub nurse, to support surgeons by passing surgical instruments during surgery as required and recording counts of surgical instruments into a personal health record (PHR). The robot used responds to hand signals and voice messages detected through sophisticated computer vision and data mining techniques. Results. The CPS was tested during a mock surgery in the OR. The in situ experiment showed that the robot recognized hand gestures reliably (with an accuracy of 97\%), it can retrieve instruments as close as 25 mm, and the total delivery time was less than 3 s on average. Conclusions. This online health tool allows the exchange of clinical and surgical information to electronic medical record-based and PHR-based applications among different hospitals, regardless of the style viewer. The CPS has the potential to be adopted in the OR to handle surgical instruments and track them in a safe and accurate manner, releasing the human scrub tech from these tasks. © The Author(s) 2012.},
	number = {4},
	journal = {Surgical Innovation},
	author = {Li, Y.-T. and Jacob, M. and Akingba, G. and Wachs, J.P.},
	year = {2013},
	keywords = {1final\_includes, 1include\_search6},
	pages = {377--384},
	annote = {cited By 11},
	file = {Full Text:/home/brendan/Zotero/storage/WG927PAG/Li et al. - 2013 - A cyber-physical management system for delivering .pdf:application/pdf}
}

@article{sanna_kinect-based_2012,
	title = {A kinect-based natural interface for quadrotor control},
	volume = {78 LNICST},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869593677&doi=10.1007%2f978-3-642-30214-5_6&partnerID=40&md5=2d9c96e2b08a77d2e6240ea0f28f9c76},
	doi = {10.1007/978-3-642-30214-5_6},
	abstract = {The evolution of input device technologies led to identification of the natural user interface (NUI) as the clear evolution of the human-machine interaction, following the shift from command-line interfaces (CLI) to graphical user interfaces (GUI). The design of user interfaces requires a careful mapping of complex user "actions" in order to make the human-computer interaction (HCI) more intuitive, usable, and receptive to the user's needs: in other words, more user-friendly and, why not, fun. NUIs constitute a direct expression of mental concepts and the naturalness and variety of gestures, compared with traditional interaction paradigms, can offer unique opportunities also for new and attracting forms of human-machine interaction. In this paper, a kinect-based NUI is presented; in particular, the proposed NUI is used to control the Ar.Drone quadrotor. © 2012 ICST Institute for Computer Science, Social Informatics and Telecommunications Engineering.},
	journal = {Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering},
	author = {Sanna, A. and Lamberti, F. and Paravati, G. and Henao Ramirez, E.A. and Manuri, F.},
	year = {2012},
	keywords = {1final\_includes, 1include\_search6},
	pages = {48--56},
	annote = {cited By 6},
	file = {Full Text:/home/brendan/Zotero/storage/ACTGSTAU/Sanna et al. - 2012 - A kinect-based natural interface for quadrotor con.pdf:application/pdf}
}

@article{muller_multi-modal_2020,
	title = {A multi-modal person perception framework for socially interactive mobile service robots},
	volume = {20},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078858351&doi=10.3390%2fs20030722&partnerID=40&md5=3ee8f1181d01ab2317abbec6e7cc9051},
	doi = {10.3390/s20030722},
	abstract = {In order to meet the increasing demands of mobile service robot applications, a dedicated perception module is an essential requirement for the interaction with users in real-world scenarios. In particular, multi sensor fusion and human re-identification are recognized as active research fronts. Through this paper we contribute to the topic and present a modular detection and tracking system that models position and additional properties of persons in the surroundings of a mobile robot. The proposed system introduces a probability-based data association method that besides the position can incorporate face and color-based appearance features in order to realize a re-identification of persons when tracking gets interrupted. The system combines the results of various state-of-the-art image-based detection systems for person recognition, person identification and attribute estimation. This allows a stable estimate of a mobile robot’s user, even in complex, cluttered environments with long-lasting occlusions. In our benchmark, we introduce a new measure for tracking consistency and show the improvements when face and appearance-based re-identification are combined. The tracking system was applied in a real world application with a mobile rehabilitation assistant robot in a public hospital. The estimated states of persons are used for the user-centered navigation behaviors, e.g., guiding or approaching a person, but also for realizing a socially acceptable navigation in public environments. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.},
	number = {3},
	journal = {Sensors (Switzerland)},
	author = {Müller, S. and Wengefeld, T. and Trinh, T.Q. and Aganian, D. and Eisenbach, M. and Gross, H.-M.},
	year = {2020},
	keywords = {1final\_includes, 1include\_search6},
	annote = {cited By 4},
	file = {Full Text:/home/brendan/Zotero/storage/MLGDVF2W/Müller et al. - 2020 - A multi-modal person perception framework for soci.pdf:application/pdf}
}

@article{kawasaki_multimodal_2020,
	title = {A {Multimodal} {Path} {Planning} {Approach} to {Human} {Robot} {Interaction} {Based} on {Integrating} {Action} {Modeling}},
	volume = {100},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090143317&doi=10.1007%2fs10846-020-01244-7&partnerID=40&md5=9ad3ac3d592943f1cbc831b396b8b14b},
	doi = {10.1007/s10846-020-01244-7},
	abstract = {To complete a task consisting of a series of actions that involve human-robot interaction, it is necessary to plan a motion that considers each action individually as well as in relation to the following action. We then focus on the specific action of “approaching a group of people” in order to accurately obtain human data that is used to make the performance of tasks involving interactions with multiple people more smooth. The movement depends on the characteristics of the important sensors used for the task and on the placement of people at and around the destination. Considering the multiple tasks and placement of people, the pre-calculation of the destinations and paths is difficult. This paper thus presents a system of navigation that can accurately obtain human data based on sensor characteristics, task content, and real-time sensor data for processes involving human-robot interaction (HRI); this method does not navigate specifically toward a previously determined static point. Our goal was achieved by using a multimodal path planning based on integration of action modeling by considering both voice and image sensing of interacting people as well as obstacle avoidance. We experimentally verified our method by using a robot in a coffee shop environment. © 2020, Springer Nature B.V.},
	number = {3-4},
	journal = {Journal of Intelligent and Robotic Systems: Theory and Applications},
	author = {Kawasaki, Y. and Yorozu, A. and Takahashi, M. and Pagello, E.},
	year = {2020},
	keywords = {1final\_includes, 1include\_search6},
	pages = {955--972},
	annote = {cited By 1},
	file = {Full Text:/home/brendan/Zotero/storage/WNGXAAXI/Kawasaki et al. - 2020 - A Multimodal Path Planning Approach to Human Robot.pdf:application/pdf}
}

@inproceedings{costanzo_multimodal_2019,
	title = {A {Multimodal} {Perception} {System} for {Detection} of {Human} {Operators} in {Robotic} {Work} {Cells}},
	doi = {10.1109/SMC.2019.8914519},
	abstract = {Workspace monitoring is a critical hw/sw component of modern industrial work cells or in service robotics scenarios, where human operators share their workspace with robots. Reliability of human detection is a major requirement not only for safety purposes but also to avoid unnecessary robot stops or slowdowns in case of false positives. The present paper introduces a novel multimodal perception system for human tracking in shared workspaces based on the fusion of depth and thermal images. A machine learning approach is pursued to achieve reliable detection performance in multi-robot collaborative systems. Robust experimental results are finally demonstrated on a real robotic work cell.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Systems}, {Man} and {Cybernetics} ({SMC})},
	author = {Costanzo, Marco and De Maria, Giuseppe and Lettera, Gaetano and Natale, Ciro and Perrone, Dario},
	month = oct,
	year = {2019},
	note = {ISSN: 2577-1655},
	keywords = {1final\_includes, 1include\_search6, Calibration, Cameras, Robot vision systems, Safety, Service robots},
	pages = {692--699},
	file = {Full Text:/home/brendan/Zotero/storage/6A7C32IH/Costanzo et al. - 2019 - A Multimodal Perception System for Detection of Hu.pdf:application/pdf}
}

@article{batista_probabilistic_2015,
	title = {A {Probabilistic} {Approach} for {Fusing} {People} {Detectors}},
	volume = {26},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946019514&doi=10.1007%2fs40313-015-0202-6&partnerID=40&md5=c74c1fe468cf56fe34a350887bf3abe8},
	doi = {10.1007/s40313-015-0202-6},
	abstract = {Automatic detection of people is essential for automated systems that interact with persons and perform complex tasks in an environment with humans. To detect people efficiently, in this article it is proposed the use of high-level information from several people detectors, which are combined using probabilistic techniques. The detectors rely on information from one or more sensors, such as cameras and laser rangefinders. The detectors’ combination allows the prediction of the position of the persons inside the sensors’ fields of view and, in some situations, outside them. Also, the fusion of the detector’s output can make people detection more robust to failures and occlusions, yielding in more accurate and complete information than the one given by a single detector. The methodology presented in this paper is based on a recursive Bayes filter, whose prediction and update models are specified in function of the detectors used. Experiments were executed with a mobile robot that collects real data in a dynamic environment, which, in our methodology, is represented by a local semantic grid that combines three different people detectors. Results indicate the improvements brought by the approach in relation to a single detector alone. © 2015, Brazilian Society for Automatics–SBA.},
	number = {6},
	journal = {Journal of Control, Automation and Electrical Systems},
	author = {Batista, N.C. and Pereira, G.A.S.},
	year = {2015},
	keywords = {1final\_includes, 1include\_search6},
	pages = {616--629},
	annote = {cited By 5},
	file = {Full Text:/home/brendan/Zotero/storage/2SVVEW9A/Batista and Pereira - 2015 - A Probabilistic Approach for Fusing People Detecto.pdf:application/pdf}
}

@inproceedings{cid_real_2013,
	title = {A real time and robust facial expression recognition and imitation approach for affective human-robot interaction using {Gabor} filtering},
	doi = {10.1109/IROS.2013.6696662},
	abstract = {Facial expressions are a rich source of communicative information about human behavior and emotion. This paper presents a real-time system for recognition and imitation of facial expressions in the context of affective Human Robot Interaction. The proposed method achieves a fast and robust facial feature extraction based on consecutively applying filters to the gradient image. An efficient Gabor filter is used, along with a set of morphological and convolutional filters to reduce the noise and the light dependence of the image acquired by the robot. Then, a set of invariant edge-based features are extracted and used as input to a Dynamic Bayesian Network classifier in order to estimate a human emotion. The output of this classifier updates a geometric robotic head model, which is used as a bridge between the human expressiveness and the robotic head. Experimental results demonstrate the accuracy and robustness of the proposed approach compared to similar systems.},
	booktitle = {2013 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	author = {Cid, Felipe and Prado, José Augusto and Bustos, Pablo and Núñez, Pedro},
	month = nov,
	year = {2013},
	note = {ISSN: 2153-0866},
	keywords = {1final\_includes, 1include\_search6, Bayes methods, Face recognition, Facial features, Feature extraction, Image edge detection, Robustness},
	pages = {2188--2193},
	file = {Full Text:/home/brendan/Zotero/storage/V2LTLPUZ/Cid et al. - 2013 - A real time and robust facial expression recogniti.pdf:application/pdf}
}

@inproceedings{nazari_simplified_2015,
	title = {A simplified method in human to robot motion mapping schemes},
	doi = {10.1109/ICRoM.2015.7367842},
	abstract = {In this paper, a simplified method in human to robot motion mapping schemes is proposed and a prototype has been implemented. The proposed design contains two arms, a sample arm and a teach pendant robotic arm which will be taught to replicate the desired position. The camera starts the process by getting live images of the sample arm and sending it to the computer. After doing some calculations, the robotic arm moves to get the same shape as the sample arm. Two image processing techniques including Hough transform and color detection are used during the process. A comparison of the two methods and the advantages and disadvantages of each one is presented and evaluated using the implemented design.},
	booktitle = {2015 3rd {RSI} {International} {Conference} on {Robotics} and {Mechatronics} ({ICROM})},
	author = {Nazari, Samira and Charmi, Mostafa and Hassani, Maryam and Ahmadi, Ghazale},
	month = oct,
	year = {2015},
	keywords = {1final\_includes, 1include\_search6, C\#, Data acquisition, Hough transform, image processing, Mechatronics, motion detection, robotic arm, Robots},
	pages = {545--550},
	file = {Full Text:/home/brendan/Zotero/storage/ASFP8UEV/Nazari et al. - 2015 - A simplified method in human to robot motion mappi.pdf:application/pdf}
}

@inproceedings{jindai_small-size_2010,
	title = {A small-size handshake robot system based on a handshake approaching motion model with a voice greeting},
	doi = {10.1109/AIM.2010.5695738},
	abstract = {We previously proposed a model for a handshake approaching motion with a voice greeting and developed a handshake robot system that applied the proposed model for embodied interaction with humans. The handshake robot system was fabricated by considering the average size of a human arm. However, small-size embodied interaction robots are required for use in homes. Therefore, in this study, a small-size handshake robot system is developed for use in the home. Furthermore, a hand position recognition method that involves a combination of an image processing and a 3D model of a human arm is proposed, and the method is adopted in the developed robot system to recognize the position of a human hand without requiring prior contact or any restrictions on humans. The effectiveness of the handshake approaching motion model for a small-size robot and the developed small-size handshake robot system are demonstrated by sensory evaluation.},
	booktitle = {2010 {IEEE}/{ASME} {International} {Conference} on {Advanced} {Intelligent} {Mechatronics}},
	author = {Jindai, Mitsuru and Watanabe, Tomio},
	month = jul,
	year = {2010},
	note = {ISSN: 2159-6255},
	keywords = {1final\_includes, 1include\_search6, Cameras, Embodied Interaction, Handshake, Human Emotion, Humans, Image recognition, Robot kinematics, Robot sensing systems, Robot-Human System, Three dimensional displays},
	pages = {521--526},
	file = {Full Text:/home/brendan/Zotero/storage/TETUZB4L/Jindai and Watanabe - 2010 - A small-size handshake robot system based on a han.pdf:application/pdf}
}

@inproceedings{simul_support_2016,
	title = {A support vector machine approach for real time vision based human robot interaction},
	doi = {10.1109/ICCITECHN.2016.7860248},
	abstract = {Today humanoid robots are being exhibited to redact various task as a personal assistant of a human. To be an assistant, a robot needs to interact with human as a human. For this reason robot needs to understand the human gender, facial expression, facial gesture in real time. Ribo - A humanoid robot build in RoboSUST lab which has the ability to communicate in Bangla with the people speaking in Bengali. In this article the authors show the implementation of theoretical knowledge of the recognition of real time facial expression, detection of human gender and yes / no from facial gesture in Ribo. Real time facial expression and gender detection can be performed using Support Vector Machine (SVM). A prepared dataset containing the facial landmarks leveled as five different expression: sad, angry, smile, surprise and normal, is given to SVM to construct a classifier. For the prediction of any expression, facial images are taken in real time and provided the facial landmarks data to SVM. Local Binary Pattern(LBP) algorithm is used for extracting features from face images. These features leveled as male and female are responsible to build the classifier. The face gesture for detecting `yes/no' is performed by tracking the movement of face in a certain time. After those implementations the principal results will make a framework that will be used in Ribo to recognize human facial expression, facial gesture movement and detect human gender.},
	booktitle = {2016 19th {International} {Conference} on {Computer} and {Information} {Technology} ({ICCIT})},
	author = {Simul, Nishikanto Sarkar and Ara, Nusrat Mubin and Islam, Md. Saiful},
	month = dec,
	year = {2016},
	keywords = {1final\_includes, 1include\_search6, Computers, Face, Face detection, Face recognition, Facial gesture, Human robot interaction, Landmarks, LBP, Machine Learning, Real time facial expression, Real-time systems, Ribo, Support vector machines, SVM},
	pages = {496--500},
	file = {Full Text:/home/brendan/Zotero/storage/R9JUTV8X/Simul et al. - 2016 - A support vector machine approach for real time vi.pdf:application/pdf}
}

@inproceedings{hasanuzzaman_adaptation_2010,
	title = {Adaptation to new user interactively using dynamically calculated principal components for user-specific human-robot interaction},
	doi = {10.1109/SII.2010.5708319},
	abstract = {This paper presents an algorithm for interactive adaptation to new user using dynamically calculated principal components. In this algorithm, new user is adapted based on the matching score of face recognition method that use dynamically calculated eigenvectors and eigenvalues from known training face dataset. User adaptation method measures the trueness of known and unknown person using the recognition result from specific number of consecutive face images. If the value of trueness for un-known person is greater than specific threshold then the robot informs the person is unknown and asks the person name and culture information to preserve in the knowledge-base through interaction. In case of known person the system greets with that person based on his/her predefined culture. The system increments the training face dataset by including any unknown face image and subsequently recalculates the eigenvectors and eigenvalues to form new PCA. The algorithm is tested by implementing a human-robot greeting scenario with a mobile robot.},
	booktitle = {2010 {IEEE}/{SICE} {International} {Symposium} on {System} {Integration}},
	author = {Hasanuzzaman, Md. and Inamura, Tetsunari},
	month = dec,
	year = {2010},
	keywords = {1final\_includes, 1include\_search6, Eigenvalues and eigenfunctions, Face, Face detection, Face recognition, Image recognition, Principal component analysis, Training},
	pages = {164--169},
	file = {Full Text:/home/brendan/Zotero/storage/TRI87REN/Hasanuzzaman and Inamura - 2010 - Adaptation to new user interactively using dynamic.pdf:application/pdf}
}

@article{zhang_adaptive_2015,
	title = {Adaptive facial point detection and emotion recognition for a humanoid robot},
	volume = {140},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941733865&doi=10.1016%2fj.cviu.2015.07.007&partnerID=40&md5=18d07f3728f045b5f2b0c32c0f8d1ff4},
	doi = {10.1016/j.cviu.2015.07.007},
	abstract = {Automatic perception of facial expressions with scaling differences, pose variations and occlusions would greatly enhance natural human robot interaction. This research proposes unsupervised automatic facial point detection integrated with regression-based intensity estimation for facial action units (AUs) and emotion clustering to deal with such challenges. The proposed facial point detector is able to detect 54 facial points in images of faces with occlusions, pose variations and scaling differences using Gabor filtering, BRISK (Binary Robust Invariant Scalable Keypoints), an Iterative Closest Point (ICP) algorithm and fuzzy c-means (FCM) clustering. Especially, in order to effectively deal with images with occlusions, ICP is first applied to generate neutral landmarks for the occluded facial elements. Then FCM is used to further reason the shape of the occluded facial region by taking the prior knowledge of the non-occluded facial elements into account. Post landmark correlation processing is subsequently applied to derive the best fitting geometry for the occluded facial element to further adjust the neutral landmarks generated by ICP and reconstruct the occluded facial region. We then conduct AU intensity estimation respectively using support vector regression and neural networks for 18 selected AUs. FCM is also subsequently employed to recognize seven basic emotions as well as neutral expressions. It also shows great potential to deal with compound and newly arrived novel emotion class detection. The overall system is integrated with a humanoid robot and enables it to deal with challenging real-life facial emotion recognition tasks. © 2015 Elsevier Inc. All rights reserved.},
	journal = {Computer Vision and Image Understanding},
	author = {Zhang, L. and Mistry, K. and Jiang, M. and Chin Neoh, S. and Hossain, M.A.},
	year = {2015},
	keywords = {1final\_includes, 1include\_search6},
	pages = {93--114},
	annote = {cited By 38},
	file = {Zhang et al. - 2015 - Adaptive facial point detection and emotion recogn.pdf:/home/brendan/Zotero/storage/9HIB5V4J/Zhang et al. - 2015 - Adaptive facial point detection and emotion recogn.pdf:application/pdf}
}

@inproceedings{liu_interactive_2016,
	title = {An interactive training system of motor learning by imitation and speech instructions for children with autism},
	doi = {10.1109/HSI.2016.7529609},
	abstract = {This paper presents an interactive training platform of motor learning using movement imitation and synchronous speech instruction. This platform enables a child with autism spectrum disorder (ASD) and a robot to imitate each other. A robot can ask a child to copy its action and instruct human how to adjust his/her action to match its action. A robot can also ask a child to coach it, which is able to elicit children's response to increase their communication. The platform is built up by a NAO humanoid robot that demonstrates actions, and a depth camera that captures child's actions. We scaled the skeleton tracking data in order to evaluate the consistence of actions between human and robot. The pilot tests on both children with and without ASD have shown that our framework is flexible and convenient for assisting intervention, and that the synchronous speech instructions to some extend facilitate children with ASD to perform their actions for motor learning.},
	booktitle = {2016 9th {International} {Conference} on {Human} {System} {Interactions} ({HSI})},
	author = {Liu, Xiaofeng and Zhou, Xu and Liu, Ce and Wang, Jianmin and Zhou, Xiaoqin and Xu, Ning and Jiang, Aimin},
	month = jul,
	year = {2016},
	keywords = {1final\_includes, 1include\_search6, Autism, Pediatrics, Robot kinematics, Robot sensing systems, Speech, Variable speed drives},
	pages = {56--61},
	file = {Full Text:/home/brendan/Zotero/storage/58KN6EER/Liu et al. - 2016 - An interactive training system of motor learning b.pdf:application/pdf}
}

@inproceedings{yan_optimization_2020,
	address = {New York, NY, USA},
	series = {{ICIT} 2020},
	title = {An {Optimization} {Method} for {Human}-{Robot} {Collaboration} in a {Production} {Unit} in the {Context} of {Intelligent} {Manufacturing}},
	isbn = {978-1-4503-8855-9},
	url = {https://doi-org.ezp01.library.qut.edu.au/10.1145/3446999.3447640},
	doi = {10.1145/3446999.3447640},
	abstract = {With the increasing application of robots in manufacturing industry, the problem of collaboration between human and robot and the efficiency of collaboration between multiple robots becomes more and more prominent in complex production scenes. An optimization method of operators and robots using depth vision sensors and intelligence trajectory planning algorithm is proposed in human-robot system. The research is expected to improve the manufacturing efficiency by reducing the waste of time with better time-consuming stability performance to the maximum extent and guarantee the safety of operators.},
	booktitle = {2020 {The} 8th {International} {Conference} on {Information} {Technology}: {IoT} and {Smart} {City}},
	publisher = {Association for Computing Machinery},
	author = {Yan, Jihong and Chen, Chao and Wang, Zipeng and Zhao, Lizhong and Li, Dianguo},
	year = {2020},
	note = {event-place: Xi'an, China},
	keywords = {1final\_includes, 1include\_search6, Action recognition, Human-robot collaboration, Manufacturing efficiency, Welding spots planning},
	pages = {244--250},
	file = {Full Text:/home/brendan/Zotero/storage/66MPIBHN/Yan et al. - 2020 - An Optimization Method for Human-Robot Collaborati.pdf:application/pdf}
}

@inproceedings{maraj_application_2016,
	title = {Application interface for gesture recognition with {Kinect} sensor},
	doi = {10.1109/ICKEA.2016.7803000},
	abstract = {Technology is becoming more natural and intuitive. People already use gestures and speech to interact with their PCs and other devices. In this paper, we have developed an application interface to recognize human body gestures and reflect these gestures through Kinect sensor to Lego robot. The main contribution of this paper is implementation process of the system and development of application interface. Visual Studio 2013 in c\# will be used to control a Lego Robot gestures detected through API Kinect. At the end we will test the system and we will represent the results graphically.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Knowledge} {Engineering} and {Applications} ({ICKEA})},
	author = {Maraj, Dardan and Maraj, Arianit and Hajzeraj, Adhurim},
	month = sep,
	year = {2016},
	keywords = {1final\_includes, 1include\_search6, C\# languages, gesture, kinect, lego, Programming, recognition, robot, Robot kinematics, Robot sensing systems, sensor, Visualization},
	pages = {98--102},
	file = {Full Text:/home/brendan/Zotero/storage/NXQMUWDD/Maraj et al. - 2016 - Application interface for gesture recognition with.pdf:application/pdf}
}

@inproceedings{das_attracting_2013,
	title = {Attracting attention and establishing a communication channel based on the level of visual focus of attention},
	doi = {10.1109/IROS.2013.6696663},
	abstract = {Recent research in HRI has emphasized the need to design affective interaction systems equipped with social intelligence. A robot's awareness of its social role encompasses the ability to behave in a socially acceptable manner, the ability to communicate appropriately according to the situation, and the ability to detect the feelings of interactive partners, as humans do with one another. In this paper, we propose an intelligent robotic method of attracting a target person's attention in a way congruent to satisfying these social requirements. If the robot needs to initiate communication urgently, such as in the case of reporting an emergency, it does not need to consider the current situation of the person it is addressing. Otherwise, the robot should observe the person to ascertain who or what s/he is looking at (VFOA), and how attentively s/he is doing so (VFOA level). Moreover, the robot must identify an appropriate time at which to attract the target person's attention so as to not interfere with his/her work. We have realized just such a robotic system by developing computer vision methods to detect a target person's VFOA and its level, and testing the system's effectiveness in a series of experiments.},
	booktitle = {2013 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	author = {Das, Dipankar and Kobayashi, Yoshinori and Kuno, Yoshinori},
	month = nov,
	year = {2013},
	note = {ISSN: 2153-0866},
	keywords = {1final\_includes, 1include\_search6, Communication channels, Face, Magnetic heads, Robots, Visualization, Writing},
	pages = {2194--2201},
	file = {Full Text:/home/brendan/Zotero/storage/62Y3YLLE/Das et al. - 2013 - Attracting attention and establishing a communicat.pdf:application/pdf}
}

@article{shakev_autonomous_2018,
	title = {Autonomous flight control and precise gestural positioning of a small quadrotor},
	volume = {756},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045264998&doi=10.1007%2f978-3-319-75181-8_9&partnerID=40&md5=1dbb8c1ad353ae7e0f76b7f46502de63},
	doi = {10.1007/978-3-319-75181-8_9},
	abstract = {Precise gestural positioning interface of a small quadrotor, presented here, is an advance continuation (ending stage) of intelligent autonomous flight control strategy. It is based on gestures and visual computing techniques and ensures intuitive way of prepositioning (fine movements in flight’s end point proximity) in absence of GPS signal or when human interaction is crucial. Therefore, a human operator could control the implementation of various maneuvers during the flight of the rotorcraft via specific gestures and body postures. A Parrot AR. Drone quadrotor and a Microsoft Kinect sensor have been used to implement and evaluate the proposed autonomous and semi-autonomous flight control. © Springer International Publishing AG, part of Springer Nature 2018.},
	journal = {Studies in Computational Intelligence},
	author = {Shakev, N.G. and Ahmed, S.A. and Topalov, A.V. and Popov, V.L. and Shiev, K.B.},
	year = {2018},
	keywords = {1final\_includes, 1include\_search6},
	pages = {179--197},
	annote = {cited By 2},
	file = {Full Text:/home/brendan/Zotero/storage/L2VFQWGQ/Shakev et al. - 2018 - Autonomous flight control and precise gestural pos.pdf:application/pdf}
}

@article{sanchez-matilla_benchmark_2020,
	title = {Benchmark for {Human}-to-{Robot} {Handovers} of {Unseen} {Containers} {With} {Unknown} {Filling}},
	volume = {5},
	issn = {2377-3766},
	doi = {10.1109/LRA.2020.2969200},
	abstract = {The real-time estimation through vision of the physical properties of objects manipulated by humans is important to inform the control of robots for performing accurate and safe grasps of objects handed over by humans. However, estimating the 3D pose and dimensions of previously unseen objects using only RGB cameras is challenging due to illumination variations, reflective surfaces, transparencies, and occlusions caused both by the human and the robot. In this letter, we present a benchmark for dynamic human-to-robot handovers that do not rely on a motion capture system, markers, or prior knowledge of specific objects. To facilitate comparisons, the benchmark focuses on cups with different levels of transparencies and with an unknown amount of an unknown filling. The performance scores assess the overall system as well as its components in order to help isolate modules of the pipeline that need improvements. In addition to the task description and the performance scores, we also present and distribute as open source a baseline implementation for the overall pipeline to enable comparisons and facilitate progress.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Sanchez-Matilla, Ricardo and Chatzilygeroudis, Konstantinos and Modas, Apostolos and Duarte, Nuno Ferreira and Xompero, Alessio and Frossard, Pascal and Billard, Aude and Cavallaro, Andrea},
	month = apr,
	year = {2020},
	keywords = {1final\_includes, 1include\_search6, Benchmark testing, Handover, human-robot handover, perception for manipulation, Performance evaluation and benchmarking, Robot sensing systems, Task analysis, Three-dimensional displays},
	pages = {1642--1649},
	file = {Full Text:/home/brendan/Zotero/storage/AADXYSYM/Sanchez-Matilla et al. - 2020 - Benchmark for Human-to-Robot Handovers of Unseen C.pdf:application/pdf}
}

@inproceedings{nascimento_collision_2020,
	title = {Collision {Avoidance} in {Human}-{Robot} {Interaction} {Using} {Kinect} {Vision} {System} {Combined} {With} {Robot}’s {Model} and {Data}},
	doi = {10.1109/IROS45743.2020.9341248},
	abstract = {Human-Robot Interaction (HRI) is a largely ad-dressed subject today. Collision avoidance is one of main strategies that allow space sharing and interaction without contact between human and robot. It is thus usual to use a 3D depth camera sensor which may involves issues related to occluded robot in camera view. While several works overcame this issue by applying infinite depth principle or increasing the number of cameras, we developed in the current work a new and an original approach based on the combination of a 3D depth sensor (Microsoft® Kinect V2) and the proprioceptive robot position sensors. This method uses a principle of limited safety contour around the obstacle to dynamically estimate the robot-obstacle distance, and then generate the repulsive force that controls the robot. For validation, our approach is applied in real time to avoid collision between dynamical obstacles (humans or objects) and the end-effector of a real 7-dof Kuka LBR iiwa collaborative robot.Several strategies based on distancing and its combination with dodging were tested. Results have shown a reactive and efficient collision avoidance, by ensuring a minimum obstacle-robot distance (of ≈ 240mm), even when the robot is in an occluded zone in the Kinect camera view.},
	booktitle = {2020 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Nascimento, Hugo and Mujica, Martin and Benoussaad, Mourad},
	month = oct,
	year = {2020},
	note = {ISSN: 2153-0866},
	keywords = {1final\_includes, 1include\_search6, Cameras, Collision avoidance, Human-robot interaction, Robot vision systems, Robots, Safety, Three-dimensional displays},
	pages = {10293--10298},
	file = {Accepted Version:/home/brendan/Zotero/storage/BRG6I4CN/Nascimento et al. - 2020 - Collision Avoidance in Human-Robot Interaction Usi.pdf:application/pdf}
}

@inproceedings{azari_commodifying_2019,
	title = {Commodifying {Pointing} in {HRI}: {Simple} and {Fast} {Pointing} {Gesture} {Detection} from {RGB}-{D} {Images}},
	doi = {10.1109/CRV.2019.00031},
	abstract = {We present and characterize a simple and reliable method for detecting pointing gestures suitable for human-robot interaction applications using a commodity RGB-D camera. We exploit an existing Deep CNN model to robustly find hands and faces in RGB images, then examine the corresponding depth channel pixels to obtain full 3D pointing vectors. We test several methods of estimating the hand end-point of the pointing vector. The system runs at better than 30Hz on commodity hardware: exceeding the frame rate of typical RGB-D sensors. An estimate of the absolute pointing accuracy is found empirically by comparison with ground-truth data from a VICON motion-capture system, and the useful interaction volume established. Finally, we show an end-to-end test where a robot estimates where the pointing vector intersects the ground plane, and report the accuracy obtained. We provide source code as a ROS node, with the intention of contributing a commodity implementation of this common component in HRI systems.},
	booktitle = {2019 16th {Conference} on {Computer} and {Robot} {Vision} ({CRV})},
	author = {Azari, Bita and Lim, Angelica and Vaughan, Richard},
	month = may,
	year = {2019},
	keywords = {1final\_includes, 1include\_search6, Cameras, Gesture Detection, Hidden Markov models, Human Robot Interaction, Reliability, Robotics, Robots, Sensors, Three-dimensional displays, Two dimensional displays},
	pages = {174--180},
	file = {Submitted Version:/home/brendan/Zotero/storage/C5GUYCX2/Azari et al. - 2019 - Commodifying Pointing in HRI Simple and Fast Poin.pdf:application/pdf}
}

@inproceedings{fallahinia_comparison_2020,
	title = {Comparison of {Constrained} and {Unconstrained} {Human} {Grasp} {Forces} {Using} {Fingernail} {Imaging} and {Visual} {Servoing}},
	doi = {10.1109/ICRA40945.2020.9196963},
	abstract = {Fingernail imaging has been proven to be effective in prior works [1], [2] for estimating the 3D fingertip forces with a maximum RMS estimation error of 7\%. In the current research, fingernail imaging is used to perform unconstrained grasp force measurement on multiple fingers to study human grasping. Moreover, two robotic arms with mounted cameras and a visual tracking system have been devised to keep the human fingers in the camera frame during the experiments. Experimental tests have been conducted for six human subjects under both constrained and unconstrained grasping conditions, and the results indicate a significant difference in force collaboration among the fingers between the two grasping conditions. Another interesting result according to the experiments is that in comparison to constrained grasping, unconstrained grasp forces are more evenly distributed over the fingers and there is less force variation (more steadiness) in each finger force. These results validate the importance of measuring grasp forces in an unconstrained manner in order to study how humans naturally grasp objects.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Fallahinia, Navid and Mascaro, Stephen A.},
	month = may,
	year = {2020},
	note = {ISSN: 2577-087X},
	keywords = {1final\_includes, 1include\_search6, Cameras, Estimation, Force, Force measurement, Grasping, Mathematical model},
	pages = {2668--2674},
	file = {Submitted Version:/home/brendan/Zotero/storage/BPV56HNB/Fallahinia and Mascaro - 2020 - Comparison of Constrained and Unconstrained Human .pdf:application/pdf}
}

@inproceedings{vignolo_computational_2017,
	title = {Computational vision for social intelligence},
	volume = {SS-17-01 - SS-17-08},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028705748&partnerID=40&md5=9b703a5d5e1eac69dd73d2883ecaf39f},
	abstract = {A fundamental trait of human intelligence is represented by social intelligence, which enables natural and fruitful interaction since very early during infancy. The ability to collaborate is also a key challenge for today's robotics, which could benefit from the design of computational models supporting the understanding of social intelligence for the future of humanrobot interaction. Our research focuses on these topics from the perspective of computational vision. In particular we aim at understanding how social intelligence develops in presence of the very limited sensory-motor skills and prior knowledge common to babies. As a starting point we consider the natural predisposition of newborns to notice potential interacting partners in their surroundings, which is manifested by a preference for biological motion over other types of motion. To model this skill, we propose a video-based computational method for biological motion detection inspired by the Two-Thirds Power Law, a well-known invariant of human movements. In particular, we address the problem by recruiting machine learning framework, leveraging a binary classification to discriminate biological from non-biological stimuli from rather coarse motion models extracted from video measurements. After evaluating the performance of the method and its generalization power to complex scenarios in an offline test, the method is engineered to work online on a robot, the humanoid iCub. The integration with the attentional module of the robot enables it to direct its gaze toward human activity in the scene. We posit that the possibility for a robotic system to orient the attention toward potential interacting agents, as a human infant would, represents one of the first stages of social intelligence, on top of which more complex skills, as action and intention understanding, could emerge. © Copyright 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
	booktitle = {{AAAI} {Spring} {Symposium} - {Technical} {Report}},
	author = {Vignolo, A. and Sciutti, A. and Rea, F. and Noceti, N. and Odone, F. and Sandini, G.},
	year = {2017},
	keywords = {1final\_includes, 1include\_search6},
	pages = {647--651},
	annote = {cited By 0},
	file = {Vignolo et al. - 2017 - Computational vision for social intelligence.pdf:/home/brendan/Zotero/storage/VJR62C9P/Vignolo et al. - 2017 - Computational vision for social intelligence.pdf:application/pdf}
}

@inproceedings{schmidt_contact-less_2013,
	title = {Contact-less and programming-less human-robot collaboration},
	volume = {7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883860879&doi=10.1016%2fj.procir.2013.06.030&partnerID=40&md5=12efe8f260f2b9a7c47868628826b843},
	doi = {10.1016/j.procir.2013.06.030},
	abstract = {In today's manufacturing environment, safe human-robot collaboration is of paramount importance, to improve efficiency and flexibility. Targeting the safety issue, this paper presents an approach for human-robot collaboration in a shared workplace in close proximity, where real data driven 3D model of a robot and multiple depth images of the workplace are used for monitoring and decision-making to perform a task. The strategy for robot control depends on the current task and the information about the operator's presence and position. A case study of assembly is carried out in a robotic assembly cell with human collaboration. The results show that this approach can be applied in real-world applications such as human-robot collaborative assembly with human operators safeguarded at all time. © 2013 The Authors.},
	booktitle = {Procedia {CIRP}},
	author = {Schmidt, B. and Wang, L.},
	year = {2013},
	keywords = {1final\_includes, 1include\_search6},
	pages = {545--550},
	annote = {cited By 20},
	file = {Schmidt and Wang - 2013 - Contact-less and programming-less human-robot coll.pdf:/home/brendan/Zotero/storage/HFSJDQG6/Schmidt and Wang - 2013 - Contact-less and programming-less human-robot coll.pdf:application/pdf}
}

@inproceedings{saffar_context-based_2015,
	title = {Context-based intent understanding using an {Activation} {Spreading} architecture},
	doi = {10.1109/IROS.2015.7353791},
	abstract = {In this paper, we propose a new approach for recognizing intentions of humans by observing their activities with an RGB-D camera. Activities and goals are modeled as a distributed network of inter-connected nodes in an Activation Spreading Network (ASN). Inspired by a formalism in hierarchical task networks, the structure of the network captures the hierarchical relationship between high-level goals and low-level activities that realize these goals. Our approach can detect intentions before they are realized and it can work in real-time. We also extend the formalism of ASNs to incorporate contextual information into intent recognition. A fully functioning system is developed for experimental evaluation. We implemented a robotic system that uses our intent recognition to naturally interact with the user. Our ASN based intent recognizer is tested against two different scenarios involving everyday activities performed by a subject, and our results show that the proposed approach is able to detect low-level activities and recognize high-level intentions effectively in real-time. Further analysis shows that contextual ASN is able to discriminate between otherwise ambiguous goals.},
	booktitle = {2015 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Saffar, Mohammad Taghi and Nicolescu, Mircea and Nicolescu, Monica and Rekabdar, Banafsheh},
	month = sep,
	year = {2015},
	keywords = {1final\_includes, 1include\_search6, Cameras, Electronic mail, Feature extraction, Hidden Markov models, Joining processes, Real-time systems, Streaming media},
	pages = {3002--3009},
	file = {Full Text:/home/brendan/Zotero/storage/RLNWPK62/Saffar et al. - 2015 - Context-based intent understanding using an Activa.pdf:application/pdf}
}

@inproceedings{lathuiliere_deep_2018,
	title = {Deep {Reinforcement} {Learning} for {Audio}-{Visual} {Gaze} {Control}},
	doi = {10.1109/IROS.2018.8594327},
	abstract = {We address the problem of audio-visual gaze control in the specific context of human-robot interaction, namely how controlled robot motions are combined with visual and acoustic observations in order to direct the robot head towards targets of interest. The paper has the following contributions: (i) a novel audio-visual fusion framework that is well suited for controlling the gaze of a robotic head; (ii) a reinforcement learning (RL) formulation for the gaze control problem, using a reward function based on the available temporal sequence of camera and microphone observations; and (iii) several deep architectures that allow to experiment with early and late fusion of audio and visual data. We introduce a simulated environment that enables us to learn the proposed deep RL model without the need of spending hours of tedious interaction. By thoroughly experimenting on a publicly available dataset and on a real robot, we provide empirical evidence that our method achieves state-of-the-art performance.},
	booktitle = {2018 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Lathuilière, Stéphane and Massé, Benoit and Mesejo, Pablo and Horaud, Radu},
	month = oct,
	year = {2018},
	note = {ISSN: 2153-0866},
	keywords = {1final\_includes, 1include\_search6, Cameras, Reinforcement learning, Robot kinematics, Robot vision systems, Visualization},
	pages = {1555--1562},
	file = {Submitted Version:/home/brendan/Zotero/storage/9LA5M83Z/Lathuilière et al. - 2018 - Deep Reinforcement Learning for Audio-Visual Gaze .pdf:application/pdf}
}

@inproceedings{chen_design_2020,
	title = {Design of a real-time human-robot collaboration system using dynamic gestures},
	volume = {2B-2020},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101287128&doi=10.1115%2fIMECE2020-23650&partnerID=40&md5=445740f2cb6df8e9c5b72572dd4d4f30},
	doi = {10.1115/IMECE2020-23650},
	abstract = {With the development of industrial automation and artificial intelligence, robotic systems are developing into an essential part of factory production, and the human-robot collaboration (HRC) becomes a new trend in the industrial field. In our previous work, ten dynamic gestures have been designed for communication between a human worker and a robot in manufacturing scenarios, and a dynamic gesture recognition model based on Convolutional Neural Networks (CNN) has been developed. Based on the model, this study aims to design and develop a new real-time HRC system based on multi-threading method and the CNN. This system enables the real-time interaction between a human worker and a robotic arm based on dynamic gestures. Firstly, a multi-threading architecture is constructed for high-speed operation and fast response while schedule more than one task at the same time. Next, A real-time dynamic gesture recognition algorithm is developed, where a human worker's behavior and motion are continuously monitored and captured, and motion history images (MHIs) are generated in real-time. The generation of the MHIs and their identification using the classification model are synchronously accomplished. If a designated dynamic gesture is detected, it is immediately transmitted to the robotic arm to conduct a real-time response. A Graphic User Interface (GUI) for the integration of the proposed HRC system is developed for the visualization of the real-time motion history and classification results of the gesture identification. A series of actual collaboration experiments are carried out between a human worker and a six-degree-of-freedom (6 DOF) Comau industrial robot, and the experimental results show the feasibility and robustness of the proposed system. © 2020 The Author(s). This is an Open Access article under the CC BY license.},
	booktitle = {{ASME} {International} {Mechanical} {Engineering} {Congress} and {Exposition}, {Proceedings} ({IMECE})},
	author = {Chen, H. and Leu, M.C. and Tao, W. and Yin, Z.},
	year = {2020},
	keywords = {1final\_includes, 1include\_search6},
	annote = {cited By 0},
	file = {Full Text:/home/brendan/Zotero/storage/W8DJGTUZ/Chen et al. - 2020 - Design of a real-time human-robot collaboration sy.pdf:application/pdf}
}

@article{mendez-polanco_detection_2010,
	title = {Detection of multiple people by a mobile robot in dynamic indoor environments},
	volume = {6433 LNAI},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649984937&doi=10.1007%2f978-3-642-16952-6_53&partnerID=40&md5=548ac1adb90ecc22305aaa49b4bef0c8},
	doi = {10.1007/978-3-642-16952-6_53},
	abstract = {Detection of multiple people is a key element for social robot design and it is a requirement for effective human-robot interaction. However, it is not an easy task, especially in complex real world scenarios that commonly involve unpredictable motion of people. This paper focuses on detecting multiple people with a mobile robot by fusing information from different sensors over time. The proposed approach applies a segmentation method that uses the distance to the objects to separate possible people from the background and a novel adaptive contour people model to obtain a probability of detecting people. A probabilistic skin model is also applied to the images and both evidences are merged and used over time with a Bayesian scheme to detect people. We present experimental results that demonstrate how the proposed method is able to detect people who is standing, sitting and leaning sideways using a mobile robot in cluttered real world scenarios. © 2010 Springer-Verlag.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Méndez-Polanco, J.A. and Muñoz-Meléndez, A. and Morales-Manzanares, E.F.},
	year = {2010},
	keywords = {1final\_includes, 1include\_search6},
	pages = {522--531},
	annote = {cited By 0},
	file = {Full Text:/home/brendan/Zotero/storage/W5CWIZZN/Méndez-Polanco et al. - 2010 - Detection of multiple people by a mobile robot in .pdf:application/pdf}
}

@inproceedings{celik_development_2012,
	title = {Development of a robotic-arm controller by using hand gesture recognition},
	doi = {10.1109/INISTA.2012.6246985},
	abstract = {This paper deals with the robotic arm controller using image processing in the field of Human-Machine Interaction (HMI). There are two different methods used to analyze to control the robotic arm, the main aim of them is getting the hand gesture information without using tool that helps the system to extract data easier (ex. glove or wrist band). After segmentation of the hand, the first method is comparing of all pre-stored data in the database at the Template Matching Algorithm, the second method is Signature Signal, distance signal between edge of the hand and center of hand, Signature Signal is used to find where the fingertips are and to count the number of them. Both methods have enough calculation speed to be used in continuous frame capturing sequence.},
	booktitle = {2012 {International} {Symposium} on {Innovations} in {Intelligent} {Systems} and {Applications}},
	author = {Celik, Ibrahim Baran and Kuntalp, Mehmet},
	month = jul,
	year = {2012},
	keywords = {1final\_includes, 1include\_search6, Computer vision, Feature extraction, Gesture recognition, Grippers, Human machine interaction, Humans, Image edge detection, Image Proccessing, Robot Controlling, Robots, Vectors},
	pages = {1--5},
	file = {Full Text:/home/brendan/Zotero/storage/IZ82TPL2/Çelik and Kuntalp - 2012 - Development of a robotic-arm controller by using h.pdf:application/pdf}
}

@article{yu_efficiency_2019,
	title = {Efficiency and learnability comparison of the gesture-based and the mouse-based telerobotic systems},
	volume = {28},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069780255&doi=10.24846%2fv28i2y201909&partnerID=40&md5=a919928cb2c01cecfe30ba68a842bd64},
	doi = {10.24846/v28i2y201909},
	abstract = {Telerobotic systems enable operators to interact with remote manipulators. Such systems have often been used in hazardous environments. This paper presents a gesture-based telerobotic system using Kinect. Kinect is an essential component of the Natural User Interface (NUI) capable of translating human body motion into telemanipulation commands. In comparison to traditional mouse-based control, the gesture-based system led to faster completion of the remote object moving task and was also easier to learn. Thus, it is possible to conclude that the gesture-based NUI proved a more intuitive and efficient usage than the traditional user interfaces. © 2012-2019 ICI Bucharest.},
	number = {2},
	journal = {Studies in Informatics and Control},
	author = {Yu, J. and Paik, W.},
	year = {2019},
	keywords = {1final\_includes, 1include\_search6},
	pages = {213--220},
	annote = {cited By 1},
	file = {Full Text:/home/brendan/Zotero/storage/MYQW5ZFZ/Yu and Paik - 2019 - Efficiency and learnability comparison of the gest.pdf:application/pdf}
}

@article{pang_efficient_2020,
	title = {Efficient {Hybrid}-{Supervised} {Deep} {Reinforcement} {Learning} for {Person} {Following} {Robot}},
	volume = {97},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067673916&doi=10.1007%2fs10846-019-01030-0&partnerID=40&md5=7c5ddf36e27a79f7081e5686e7aba15e},
	doi = {10.1007/s10846-019-01030-0},
	abstract = {Traditional person following robots usually need hand-crafted features and a well-designed controller to follow the assigned person. Normally it is difficult to be applied in outdoor situations due to variability and complexity of the environment. In this paper, we propose an approach in which an agent is trained by hybrid-supervised deep reinforcement learning (DRL) to perform a person following task in end-to-end manner. The approach enables the robot to learn features autonomously from monocular images and to enhance performance via robot-environment interaction. Experiments show that the proposed approach is adaptive to complex situations with significant illumination variation, object occlusion, target disappearance, pose change, and pedestrian interference. In order to speed up the training process to ensure easy application of DRL to real-world robotic follower controls, we apply an integration method through which the agent receives prior knowledge from a supervised learning (SL) policy network and reinforces its performance with a value-based or policy-based (including actor-critic method) DRL model. We also utilize an efficient data collection approach for supervised learning in the context of person following. Experimental results not only verify the robustness of the proposed DRL-based person following robot system, but also indicate how easily the robot can learn from mistakes and improve performance. © 2019, Springer Nature B.V.},
	number = {2},
	journal = {Journal of Intelligent and Robotic Systems: Theory and Applications},
	author = {Pang, L. and Zhang, Y. and Coleman, S. and Cao, H.},
	year = {2020},
	keywords = {1final\_includes, 1include\_search6},
	pages = {299--312},
	annote = {cited By 2},
	file = {Full Text:/home/brendan/Zotero/storage/ZMM5378P/Pang et al. - 2020 - Efficient Hybrid-Supervised Deep Reinforcement Lea.pdf:application/pdf}
}

@inproceedings{barz_evaluating_2017,
	address = {New York, NY, USA},
	series = {{HRI} '17},
	title = {Evaluating {Remote} and {Head}-{Worn} {Eye} {Trackers} in {Multi}-{Modal} {Speech}-{Based} {HRI}},
	isbn = {978-1-4503-4885-0},
	url = {https://doi-org.ezp01.library.qut.edu.au/10.1145/3029798.3038367},
	doi = {10.1145/3029798.3038367},
	abstract = {Gaze is known to be a dominant modality for conveying spatial information, and it has been used for grounding in human-robot dialogues. In this work, we present the prototype of a gaze-supported multi-modal dialogue system that enhances two core tasks in human-robot collaboration: 1) our robot is able to learn new objects and their location from user instructions involving gaze, and 2) it can instruct the user to move objects and passively track this movement by interpreting the user's gaze. We performed a user study to investigate the impact of different eye trackers on user performance. In particular, we compare a head-worn device and an RGB-based remote eye tracker. Our results show that the head-mounted eye tracker outperforms the remote device in terms of task completion time and the required number of utterances due to its higher precision.},
	booktitle = {Proceedings of the {Companion} of the 2017 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Barz, Michael and Poller, Peter and Sonntag, Daniel},
	year = {2017},
	note = {event-place: Vienna, Austria},
	keywords = {1final\_includes, 1include\_search6, eye tracking, human-centred computing, human-robot interaction, machine learning, multi-modal interaction},
	pages = {79--80},
	file = {Full Text:/home/brendan/Zotero/storage/X2PYXDA8/Barz et al. - 2017 - Evaluating Remote and Head-Worn Eye Trackers in Mu.pdf:application/pdf}
}

@inproceedings{pfeil_exploring_2013,
	title = {Exploring {3D} gesture metaphors for interaction with unmanned aerial vehicles},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875831758&doi=10.1145%2f2449396.2449429&partnerID=40&md5=844d533272cf20ff2b9e2c0394dc547f},
	doi = {10.1145/2449396.2449429},
	abstract = {We present a study exploring upper body 3D spatial interaction metaphors for control and communication with Unmanned Aerial Vehicles (UAV) such as the Parrot AR Drone. We discuss the design and implementation of five interaction techniques using the Microsoft Kinect, based on metaphors inspired by UAVs, to support a variety of flying operations a UAV can perform. Techniques include a first-person interaction metaphor where a user takes a pose like a winged aircraft, a game controller metaphor, where a user's hands mimic the control movements of console joysticks, "proxy" manipulation, where the user imagines manipulating the UAV as if it were in their grasp, and a pointing metaphor in which the user assumes the identity of a monarch and commands the UAV as such. We examine qualitative metrics such as perceived intuition, usability and satisfaction, among others. Our results indicate that novice users appreciate certain 3D spatial techniques over the smartphone application bundled with the AR Drone. We also discuss the trade-offs in the technique design metrics based on results from our study. Copyright © 2013 ACM.},
	booktitle = {International {Conference} on {Intelligent} {User} {Interfaces}, {Proceedings} {IUI}},
	author = {Pfeil, K.P. and Koh, S.L. and LaViola Jr., J.J.},
	year = {2013},
	keywords = {1final\_includes, 1include\_search6},
	pages = {257--266},
	annote = {cited By 67},
	file = {Full Text:/home/brendan/Zotero/storage/THU8FYTD/Pfeil et al. - 2013 - Exploring 3D gesture metaphors for interaction wit.pdf:application/pdf}
}

@inproceedings{belo_facial_2019,
	title = {Facial {Recognition} {Experiments} on a {Robotic} {System} {Using} {One}-{Shot} {Learning}},
	doi = {10.1109/LARS-SBR-WRE48964.2019.00020},
	abstract = {One of the crucial tasks during the interaction human-robot is the face recognition task on digital images. Firstly, the task of recognition requires face detection and a continuing procedure for extracting face characteristics and dealing with the brightness of the environment's faces positioned at different angles and sometimes occlusion problems. In this paper, the aim is to explore the One-shot learning technique, which considers only one image of each person for face detection and uses information extracted from other image databases for this. One of its modifications, Face Recognition algorithm, is applied to recognize people during sessions of interaction with a humanoid robot. This algorithm uses the Dlib capabilities to detect, extract and recognize faces through bases with a singular face image. The results of the experiments performed are presented and show how useful is this kind of learning technique for interactions of a robot with humans.},
	booktitle = {2019 {Latin} {American} {Robotics} {Symposium} ({LARS}), 2019 {Brazilian} {Symposium} on {Robotics} ({SBR}) and 2019 {Workshop} on {Robotics} in {Education} ({WRE})},
	author = {Belo, José Pedro Ribeiro and Sanches, Felipe Padula and Romero, Roseli Aparecida Francelin},
	month = oct,
	year = {2019},
	note = {ISSN: 2643-685X},
	keywords = {1final\_includes, 1include\_search6, deep-learning, Face, Face detection, Face recognition, facial recognition, Image recognition, one-shot learning, pepper robot, Task analysis, Tools},
	pages = {67--73},
	file = {Full Text:/home/brendan/Zotero/storage/DC3F2KQ8/Belo et al. - 2019 - Facial Recognition Experiments on a Robotic System.pdf:application/pdf}
}

@article{hartmann_feasibility_2013,
	title = {Feasibility of touch-less control of operating room lights},
	volume = {8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878885454&doi=10.1007%2fs11548-012-0778-2&partnerID=40&md5=b34d2651055912d8be6fd00ddbe1135a},
	doi = {10.1007/s11548-012-0778-2},
	abstract = {Purpose: Today's highly technical operating rooms lead to fairly complex surgical workflows where the surgeon has to interact with a number of devices, including the operating room light. Hence, ideally, the surgeon could direct the light without major disruption of his work. We studied whether a gesture tracking-based control of an automated operating room light is feasible. Methods: So far, there has been little research on control approaches for operating lights. We have implemented an exemplary setup to mimic an automated light controlled by a gesture tracking system. The setup includes a articulated arm to position the light source and an off-the-shelf RGBD camera to detect the user interaction. We assessed the tracking performance using a robot-mounted hand phantom and ran a number of tests with 18 volunteers to evaluate the potential of touch-less light control. Results: All test persons were comfortable with using the gesture-based system and quickly learned how to move a light spot on flat surface. The hand tracking error is direction-dependent and in the range of several centimeters, with a standard deviation of less than 1 mm and up to 3.5 mm orthogonal and parallel to the finger orientation, respectively. However, the subjects had no problems following even more complex paths with a width of less than 10 cm. The average speed was 0.15 m/s, and even initially slow subjects improved over time. Gestures to initiate control can be performed in approximately 2 s. Two-thirds of the subjects considered gesture control to be simple, and a majority considered it to be rather efficient. Conclusions: Implementation of an automated operating room light and touch-less control using an RGBD camera for gesture tracking is feasible. The remaining tracking error does not affect smooth control, and the use of the system is intuitive even for inexperienced users. © 2012 CARS.},
	number = {2},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {Hartmann, F. and Schlaefer, A.},
	year = {2013},
	keywords = {1final\_includes, 1include\_search6},
	pages = {259--268},
	annote = {cited By 16},
	file = {Full Text:/home/brendan/Zotero/storage/5R4AQG97/Hartmann and Schlaefer - 2013 - Feasibility of touch-less control of operating roo.pdf:application/pdf}
}

@inproceedings{manitsaris_fingers_2016,
	address = {New York, NY, USA},
	series = {{MOCO} '16},
	title = {Fingers {Gestures} {Early}-{Recognition} with a {Unified} {Framework} for {RGB} or {Depth} {Camera}},
	isbn = {978-1-4503-4307-7},
	url = {https://doi-org.ezp01.library.qut.edu.au/10.1145/2948910.2948947},
	doi = {10.1145/2948910.2948947},
	abstract = {This paper presents a unified framework computer vision approach for finger gesture early recognition and interaction that can be applied on sequences of either RGB or depth images without any supervised skeleton extraction. Either RGB or time-of-flight cameras can be used to capture finger motions. The hand detection is based on a skin color model for color images or distance slicing for depth images. A unique hand model is used for the finger detection and identification. Static (fingerings) and dynamic (sequence and/or combination of fingerings) patterns can be early-recognized based on one-shot learning approach using a modified Hidden Markov Models approach. The recognition accuracy is evaluated in two different applications: musical and robotic interaction. In the first case standardized basic piano-like finger gestures (ascending/descending scales, ascending/descending arpeggio) are used to evaluate the performance of the system. In the second case, both standardized and user-defined gestures (driving, waypoints etc.) are recognized and used to interactively control an automated guided vehicle.},
	booktitle = {Proceedings of the 3rd {International} {Symposium} on {Movement} and {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Manitsaris, Sotiris and Tsagaris, Apostolos and Glushkova, Alina and Moutarde, Fabien and Bevilacqua, Frédéric},
	year = {2016},
	note = {event-place: Thessaloniki, GA, Greece},
	keywords = {1final\_includes, 1include\_search6, computer vision, early recognition, Finger motion patterns, standardized interaction, unified framework, user-defined interaction},
	file = {Submitted Version:/home/brendan/Zotero/storage/UWH9QB9V/Manitsaris et al. - 2016 - Fingers Gestures Early-Recognition with a Unified .pdf:application/pdf}
}

@inproceedings{bras_gesture_2018,
	title = {Gesture recognition from skeleton data for intuitive human-machine interaction},
	volume = {7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057981149&doi=10.3233%2f978-1-61499-898-3-271&partnerID=40&md5=fa004ce51e405604b8af223ae5027202},
	doi = {10.3233/978-1-61499-898-3-271},
	abstract = {Human gesture recognition has assumed a capital role in industrial applications, such as Human-Machine Interaction. We propose an approach for segmentation and classification of dynamic gestures based on a set of handcrafted features, which are drawn from the skeleton data provided by the Kinect sensor. The module for gesture detection relies on a feedforward neural network which performs framewise binary classification. The method for gesture recognition applies a sliding window, which extracts information from both the spatial and temporal dimensions. Then we combine windows of varying durations to get a multi-temporal scale approach and an additional gain in performance. Encouraged by the recent success of Recurrent Neural Networks for time series domains, we also propose a method for simultaneous gesture segmentation and classification based on the bidirectional Long Short-Term Memory cells, which have shown ability for learning the temporal relationships on long temporal scales. We evaluate all the different approaches on the dataset published for the ChaLearn Looking at People Challenge 2014. The most effective method achieves a Jaccard index of 0.75, which suggests a performance almost on pair with that presented by the state-of-the-art techniques. At the end, the recognized gestures are used to interact with a collaborative robot. © 2018 The authors and IOS Press.},
	booktitle = {Advances in {Transdisciplinary} {Engineering}},
	author = {Brás, A. and Simão, M. and Neto, P.},
	year = {2018},
	keywords = {1final\_includes, 1include\_search6},
	pages = {271--280},
	annote = {cited By 0},
	file = {Full Text:/home/brendan/Zotero/storage/UNH2QZ7P/Brás et al. - 2018 - Gesture recognition from skeleton data for intuiti.pdf:application/pdf}
}

@inproceedings{saichinmayi_gesture_2015,
	title = {Gesture signals processing for a silent spybot},
	doi = {10.1109/SPIN.2015.7095406},
	abstract = {In the modern era, robots have become an integral part of human life. In a world where humans and robots need to coexist, it is important to evolve more natural and easy communication mechanisms for human-machine interaction. The communication mechanism needs to be `easy' more for humans, than machines. One such mechanism can be developed by using natural gestures of the humans. In this paper, a prototype silent spybot is developed that operates by processing the gesture commands signal. The prototype uses video processing with object tracking algorithm to understand the gesture commands. The spybot is programmed to interpret the gesture command signal and navigate according to hand gestures, sensed by a video camera operating within a short range. The gesture commands can be used for controlling the spybot's functions such as movement of the robot, or other operations of the robot, silently. Performance evaluation is carried out through various environments, with encouraging results. As a possible application, a spy robot that captures images from target places silently and sends the spied data to a host computer, is also demonstrated.},
	booktitle = {2015 2nd {International} {Conference} on {Signal} {Processing} and {Integrated} {Networks} ({SPIN})},
	author = {SaiChinmayi, N. and Hasitha, Ch. and Sravya, B. and Mittal, V. K.},
	month = feb,
	year = {2015},
	keywords = {1final\_includes, 1include\_search6, Arduino Uno microcontroller board, Bluetooth, gesture controlled robot, gesture signal processing, Prototypes, Robot kinematics, Robot sensing systems, Signal processing, spybot},
	pages = {756--761},
	file = {Full Text:/home/brendan/Zotero/storage/JW22FLR2/SaiChinmayi et al. - 2015 - Gesture signals processing for a silent spybot.pdf:application/pdf}
}

@inproceedings{mohaimenianpour_hands_2018,
	title = {Hands and {Faces}, {Fast}: {Mono}-{Camera} {User} {Detection} {Robust} {Enough} to {Directly} {Control} a {UAV} in {Flight}},
	doi = {10.1109/IROS.2018.8593709},
	abstract = {We present a robust real-time system for simultaneous detection of hands and faces in RGB and gray-scale images, and a novel dataset used for training. Our goal is to provide a robust sensor front-end suitable for real-time human-robot interaction using face-engagement and gestures. Using hand-labelled videos obtained from real human-UAV interaction experiments, we re-trained the YOLOv2 Deep Convolutional Neural Network to detect only hands and faces. This model was then used to automatically label several much larger third-party datasets. After manual correction of these results, we modified and re-trained the model on all this labelled data. We obtain qualitatively good detection results at 60Hz on a commodity GPU: our simultaneous hand-and-face detector gives state of the art accuracy and speed in a hand detection benchmark and competitive results in a face detection benchmark. To demonstrate its effectiveness for human-robot interaction we describe its use as the input to a simple but practical gestural human-UAV interface for entertainment or industrial applications. All software, training and test data are freely available.},
	booktitle = {2018 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {MohaimenianPour, Sepehr and Vaughan, Richard},
	month = oct,
	year = {2018},
	note = {ISSN: 2153-0866},
	keywords = {1final\_includes, 1include\_search6, Cameras, Detectors, Face detection, Feature extraction, Object detection, Proposals, Training},
	pages = {5224--5231},
	file = {Full Text:/home/brendan/Zotero/storage/47DRYGBI/MohaimenianPour and Vaughan - 2018 - Hands and Faces, Fast Mono-Camera User Detection .pdf:application/pdf}
}

@article{angani_human_2020,
	title = {Human and robotic fish interaction controlled using hand gesture image processing},
	volume = {32},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096299482&doi=10.18494%2fSAM.2020.2925&partnerID=40&md5=54d4b3a22e2703e0a3c63c7aa101334e},
	doi = {10.18494/SAM.2020.2925},
	abstract = {This paper is about the control of robotic fish movement in an aquarium via human hand gestures detected by image sensors attached in the aquarium. In this study, sensors actively interact with humans and robotic fish. Image and radio frequency sensors are used to identify the position and color of robotic fish. Recently, we have studied human interactive control based on hand gesture recognition. Image sensors send the input signals of hand gestures obtained from real-time video images processed using tracking control algorithms, such as color mark, stop zone, and lead-lag tracking algorithms, to robotic fish. The movement of robotic fish is controlled via the movement of the two hands, where the left hand is for the fish to be controlled and the right hand is for controlling the movement of the robotic fish. Hand gesture recognition consists of hand feature segmentation and gesture recognition from the hand features. Our results show that interactive human control using hand gestures successfully controls the movement of robotic fish. © 2020 M Y U Scientific Publishing Division. All rights reserved.},
	number = {10},
	journal = {Sensors and Materials},
	author = {Angani, A. and Lee, J.-W. and Talluri, T. and Lee, J.-y. and Shin, K.J.},
	year = {2020},
	keywords = {1final\_includes, 1include\_search6},
	pages = {3479--3490},
	annote = {cited By 4},
	file = {Full Text:/home/brendan/Zotero/storage/WFQZYWM5/Angani et al. - 2020 - Human and robotic fish interaction controlled usin.pdf:application/pdf}
}

@inproceedings{gu_human_2012,
	title = {Human gesture recognition through a {Kinect} sensor},
	doi = {10.1109/ROBIO.2012.6491161},
	abstract = {Gesture recognition can be applied to many research areas, such as vision-based interface, communication and human robot interaction (HRI). This paper implements a non-intrusive, real-time gesture recognition system using a depth sensor. Related features are obtained from the human skeleton model generated by the Kinect sensor. Hidden Markov Models (HMMs) are used to model the dynamics of the gestures. We conducted offline experiments to check the accuracy and robustness of the system. Online experiments were also performed to verify the real-time requirement. Final results indicate that the average recognition accuracy is around 85\% for the subject who provides the training data and 73\% for the other subject who does not. The system was also used to interact with a mobile robot through gestures. This application indicates that it is robust to work in real-time.},
	booktitle = {2012 {IEEE} {International} {Conference} on {Robotics} and {Biomimetics} ({ROBIO})},
	author = {Gu, Ye and Do, Ha and Ou, Yongsheng and Sheng, Weihua},
	month = dec,
	year = {2012},
	keywords = {1final\_includes, 1include\_search6},
	pages = {1379--1384},
	file = {Full Text:/home/brendan/Zotero/storage/RY7D7WBC/Gu et al. - 2012 - Human gesture recognition through a Kinect sensor.pdf:application/pdf}
}

@article{lee_human_2012,
	title = {Human tracking with an infrared camera using curve matching framework},
	volume = {2012},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872904085&doi=10.1186%2f1687-6180-2012-99&partnerID=40&md5=769d68aee600cd968ff889ffddbf224d},
	doi = {10.1186/1687-6180-2012-99},
	abstract = {The Kalman filter (KF) has been improved for a mobile robot to human tracking. The proposed algorithm combines a curve matching framework and KF to enhance prediction accuracy of target tracking. Compared to other methods using normal KF, the Curve Matched Kalman Filter (CMKF) method predicts the next movement of the human by taking into account not only his present motion characteristics, but also the previous history of target behavior patterns-the CMKF provides an algorithm that acquires the motion characteristics of a particular human and provides a computationally inexpensive framework of human-tracking system. The proposed method demonstrates an improved target tracking using a heuristic weighted mean of two methods, i.e., the curve matching framework and KF prediction. We have conducted the experimental test in an indoor environment using an infrared camera mounted on a mobile robot. Experimental results validate that the proposed CMKF increases prediction accuracy by more than 30\% compared to normal KF when the characteristic patterns of target motion are repeated in the target trajectory. © 2012 Lee et al.},
	number = {1},
	journal = {Eurasip Journal on Advances in Signal Processing},
	author = {Lee, S.J. and Shah, G. and Bhattacharya, A.A. and Motai, Y.},
	year = {2012},
	keywords = {1final\_includes, 1include\_search6},
	annote = {cited By 8},
	file = {Full Text:/home/brendan/Zotero/storage/FN6Q6R2S/Lee et al. - 2012 - Human tracking with an infrared camera using curve.pdf:application/pdf}
}

@inproceedings{granata_human_2013,
	title = {Human whole body motion characterization from embedded {Kinect}},
	doi = {10.1109/CogInfoCom.2013.6719228},
	abstract = {Non-verbal communications such as kinesthetics, or body language and posture are important codes used to establish and maintain interpersonal relationships. They can also be utilized for safe and efficient human robot interactions. A correct interpretation of the human activity through the analysis of certain spatio-temporal and dynamic parameters represent an outstanding benefit for the quality of human machine communication in general. This paper presents an effective markerless motion capture system provided by a mobile robot for sensing human activity, in non-invasive fashion. We present a physical model based method exploiting the embedded Kinect. Its performances are evaluated first comparing the results to those obtained with a precise 3D motion capture marker based system and to data obtained from a dynamic posturography platform. Then an experiment in real life conditions is performed to assess the system sensitivity to some gait disturbances.},
	booktitle = {2013 {IEEE} 4th {International} {Conference} on {Cognitive} {Infocommunications} ({CogInfoCom})},
	author = {Granata, Consuelo and Salini, Joseph and Ady, Ragou and Bidaud, Philippe},
	month = dec,
	year = {2013},
	keywords = {1final\_includes, 1include\_search6, Calibration, Joints, Simultaneous localization and mapping},
	pages = {133--138},
	file = {Submitted Version:/home/brendan/Zotero/storage/9CF8LWGR/Granata et al. - 2013 - Human whole body motion characterization from embe.pdf:application/pdf}
}

@article{mateus_human-aware_2016,
	title = {Human-aware navigation using external omnidirectional cameras},
	volume = {417},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951731476&doi=10.1007%2f978-3-319-27146-0_22&partnerID=40&md5=8be976880be125fd15ada50dce0f1801},
	doi = {10.1007/978-3-319-27146-0_22},
	abstract = {If robots are to invade our homes and offices, they will have to interact more naturally with humans. Natural interaction will certainly include the ability of robots to plan their motion, accounting for the social norms enforced. In this paper we propose a novel solution for Human-AwareNavigation resorting to external omnidirectional static cameras, used to implement a vision-based person tracking system. The proposed solution was tested in a typical domestic indoor scenario in four different experiments. The results show that the robot is able to cope with human-aware constraints, defined after common proxemics rules. © Springer International Publishing Switzerland 2016.},
	journal = {Advances in Intelligent Systems and Computing},
	author = {Mateus, A. and Miraldo, P. and Lima, P.U. and Sequeira, J.},
	year = {2016},
	keywords = {1final\_includes, 1include\_search6},
	pages = {283--295},
	annote = {cited By 2},
	file = {Full Text:/home/brendan/Zotero/storage/UYMKG98H/Mateus et al. - 2016 - Human-aware navigation using external omnidirectio.pdf:application/pdf}
}

@article{medeiros_human-drone_2020,
	title = {Human-{Drone} {Interaction}: {Using} {Pointing} {Gesture} to {Define} a {Target} {Object}},
	volume = {12182 LNCS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088742548&doi=10.1007%2f978-3-030-49062-1_48&partnerID=40&md5=d9ab1cb7e73e9d91209229c2aec1674a},
	doi = {10.1007/978-3-030-49062-1_48},
	abstract = {This paper focuses on exploring the optimal gesture interface for Human-Drone Interaction in a firefighting scenario. For this purpose, we conducted a Preliminary Interview and User Study with seven subjects from the Kobe Firefighting Brigade, Japan. As a drone’s flight and locomotion properties significantly affect the user’s mental and physical expectations, differently compared to other grounded robots, a careful investigation of user-defined design preferences and interactions is required. This work proposes an examination and discussion, with experienced firefighters, about Human-Drone Interactions when relying solely on the drone’s monocular camera, without relying on other devices such as GPS or external cameras. The User Study had three main elements: A drone, a building, and the volunteering firefighters. During the Study, each firefighter should specify a window to the drone. The drone would theoretically use that window to enter the building and perform some designed tasks, like information gathering, thus saving the firefighter time and effort. Results show that the subjects always chose Pointing Gestures and voice commands as a means to communicate to the drone a target window. We built A prototype application with the resulting gesture from this investigation. In the prototype, a drone can understand to which object the user is pointing. © 2020, Springer Nature Switzerland AG.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Medeiros, A.C.S. and Ratsamee, P. and Uranishi, Y. and Mashita, T. and Takemura, H.},
	year = {2020},
	keywords = {1final\_includes, 1include\_search6},
	pages = {688--705},
	annote = {cited By 1},
	file = {Full Text:/home/brendan/Zotero/storage/9J8JGR9C/Medeiros et al. - 2020 - Human-Drone Interaction Using Pointing Gesture to.pdf:application/pdf}
}

@inproceedings{yue_human-robot_2020,
	title = {Human-{Robot} {Teaming} and {Coordination} in {Day} and {Night} {Environments}},
	doi = {10.1109/ICARCV50220.2020.9305408},
	abstract = {As robots are sharing work spaces with human, human-robot teamwork is becoming increasingly important. It is foreseeable that the daily work team will be composed of human and robots. The integration of the appropriate decision-making process is an essential part to design and develop the team. If robots can understand the activities and intents of human, it is convenient for a person to cooperate with robots in a natural manner. This paper proposes a system that enables robots to understand human pose and execute given command. The system provides two options for different hardware systems: the first one is suitable for powerful computational units; the second model is compact and efficient on a normal robot platform. In order to enrich application scenarios, we propose a method to extract human pose from thermal images so that our system can be used in all-weather scenario. In addition, we collected extensive training data and trained a MLP neural network to classify several human poses. The experimental results show the accuracy and efficiency of the proposed MLP neural network in day and night environments.},
	booktitle = {2020 16th {International} {Conference} on {Control}, {Automation}, {Robotics} and {Vision} ({ICARCV})},
	author = {Yue, Yufeng and Liu, Xiangyu and Wang, Yuanzhe and Zhang, Jun and Wang, Danwei},
	month = dec,
	year = {2020},
	keywords = {1final\_includes, 1include\_search6, Cameras, Convolution, Robot kinematics, Robot vision systems, Robots, Skeleton, Training data},
	pages = {375--380},
	file = {Full Text:/home/brendan/Zotero/storage/84W36RQ6/Yue et al. - 2020 - Human-Robot Teaming and Coordination in Day and Ni.pdf:application/pdf}
}

@inproceedings{li_id-match_2016,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '16},
	title = {{ID}-{Match}: {A} {Hybrid} {Computer} {Vision} and {RFID} {System} for {Recognizing} {Individuals} in {Groups}},
	isbn = {978-1-4503-4082-3},
	url = {https://doi-org.ezp01.library.qut.edu.au/10.1145/2851581.2889430},
	doi = {10.1145/2851581.2889430},
	abstract = {Technologies that allow autonomous robots and computer systems to quickly recognize and interact with individuals in a group setting has the potential to enable a wide range of personalized experiences. We present ID-Match, a hybrid computer vision and RFID system that uses a novel reverse synthetic aperture technique to recover the relative motion paths of a RFID tags worn by people and correlate that to physical motion paths of individuals as measured with a 3D depth camera. Results show that our real-time system is capable of simultaneously recognizing and correctly assigning IDs to individuals within 4 seconds with 96.6\% accuracy and groups of five people in 7 seconds with 95\% accuracy. In order to test the effectiveness of this approach in realistic scenarios, groups of five participants play an interactive quiz game with an autonomous robot, resulting in an ID assignment accuracy of 93.3\%.},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} {Extended} {Abstracts} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Li, Hanchuan and Zhang, Peijin and Al Moubayed, Samer and Patel, Shwetak N. and Sample, Alanson P.},
	year = {2016},
	note = {event-place: San Jose, California, USA},
	keywords = {1final\_includes, 1include\_search6, computer vision, human-robot interaction, recognition, RFID, sensor fusion, synthetic aperture},
	pages = {7},
	file = {id-match.pdf:/home/brendan/Zotero/storage/FJ2AMZRZ/id-match.pdf:application/pdf}
}

@inproceedings{valipour_incremental_2017,
	title = {Incremental learning for robot perception through {HRI}},
	doi = {10.1109/IROS.2017.8206106},
	abstract = {Visual scene understanding is a crucial skill for robots, yet difficult to achieve. Recently, Convolutional Neural Networks (CNN), have shown success in this task. However, there is still a gap between their performance on image datasets and real-world robotics scenarios. In particular, a-priori training is on a bounded set of object categories, while in many unstructured tasks new objects are encountered. We present a novel paradigm for incrementally improving a robot's visual perception through active human-robot interaction. In this paradigm, the user introduces novel objects to the robot by means of pointing and voice commands. Given this information, the robot visually explores the object and adds images from it to re-train the perception module. Our method leverages state of the art Convolutional Neutal Networks - CNNs from offline batch learning, human guidance, robot exploration and incremental on-line learning.},
	booktitle = {2017 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Valipour, Sepehr and Perez, Camilo and Jagersand, Martin},
	month = sep,
	year = {2017},
	note = {ISSN: 2153-0866},
	keywords = {1final\_includes, 1include\_search6, Feature extraction, Machine learning, Object recognition, Proposals, Robots, Training},
	pages = {2772--2777},
	file = {Submitted Version:/home/brendan/Zotero/storage/9MQ288GY/Valipour et al. - 2017 - Incremental learning for robot perception through .pdf:application/pdf}
}

@inproceedings{de_luca_integrated_2012,
	title = {Integrated control for {pHRI}: {Collision} avoidance, detection, reaction and collaboration},
	doi = {10.1109/BioRob.2012.6290917},
	abstract = {We present an integrated control framework for safe physical Human-Robot Interaction (pHRI) based on a hierarchy of consistent behaviors. Safe human robot coexistence is achieved with a layered approach for coping with undesired collisions and intended contacts. A collision avoidance algorithm based on depth information of the HRI scene is used in the first place. Since collision avoidance cannot be guaranteed, it is supported by a physical collision detection/reaction method based on a residual signal which needs only joint position measures. On top of this layer, safe human-robot collaboration tasks can be realized. Collaboration phases are activated and ended by human gestures or voice commands. Intentional physical interaction is enabled and exchanged forces are estimated by integrating the residual with an estimation of the contact point obtained from depth sensing. During the collaboration, only the human parts that are designated as collaborative are allowed to touch the robot while, consistently to the lower layers, all other contacts are considered undesired collisions. Preliminary experimental results with a KUKA LWR-IV and a Kinect sensor are presented.},
	booktitle = {2012 4th {IEEE} {RAS} {EMBS} {International} {Conference} on {Biomedical} {Robotics} and {Biomechatronics} ({BioRob})},
	author = {De Luca, Alessandro and Flacco, Fabrizio},
	month = jun,
	year = {2012},
	note = {ISSN: 2155-1782},
	keywords = {1final\_includes, 1include\_search6, Collaboration, Collision avoidance, Humans, Joints, Robot kinematics, Robot sensing systems},
	pages = {288--295},
	file = {Full Text:/home/brendan/Zotero/storage/HDF4AI3U/De Luca and Flacco - 2012 - Integrated control for pHRI Collision avoidance, .pdf:application/pdf}
}

@article{zhao_interactive_2013,
	title = {Interactive indoor environment mapping through visual tracking of human skeleton},
	volume = {20},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888614192&doi=10.1504%2fIJMIC.2013.057565&partnerID=40&md5=fec81026ac518210f233ae3aa84e2970},
	doi = {10.1504/IJMIC.2013.057565},
	abstract = {This paper presents a novel human-robot interaction approach to grid mapping of an indoor environment based on a 3D kinect sensor and the grid-based mapping algorithm. It mainly includes three modules: skeleton tracking, robot control and GMapping. Firstly, the skeleton tracking module builds a human skeleton model, extracts the skeleton joints' position information from 3D visual data and generates digital signals through identifying some simple motions and events. Then according to different digital signals and joints' position information, the robot control module enables the robot to take different actions such as following the person, stop and so on. Finally, the grid map of the environment is built through GMapping algorithm based on odometry and laser data, which is improved by Rao-Blackwellised particle filters. The proposed approach has been implemented successfully in several different buildings and can be applied to service robots. Compared with traditional roaming for mapping, human guiding the robot for mapping is more efficient and takes less time in a complicated environment. Meanwhile, compared with wearable motion sensors attached to the human body, this approach is more convenient and make the user more comfortable. Copyright © 2013 Inderscience Enterprises Ltd.},
	number = {4},
	journal = {International Journal of Modelling, Identification and Control},
	author = {Zhao, C. and Pan, W. and Hu, H.},
	year = {2013},
	keywords = {1final\_includes, 1include\_search6},
	pages = {319--328},
	annote = {cited By 3},
	file = {Zhao et al. - 2013 - Interactive indoor environment mapping through vis.pdf:/home/brendan/Zotero/storage/FMTYB4U4/Zhao et al. - 2013 - Interactive indoor environment mapping through vis.pdf:application/pdf}
}

@article{lee_interactive_2012,
	title = {Interactive multi-resolution display using a projector mounted mobile robot in intelligent space},
	volume = {9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870399262&doi=10.5772%2f54232&partnerID=40&md5=709f68bdbc8c21950090279cd26efa53},
	doi = {10.5772/54232},
	abstract = {In this paper, we propose a novel interactive multi-resolution display system. The proposed system is based on a projector-mounted mobile robot in an Intelligent Space. The Intelligent Space can calculate the location of the region of interest (ROI) by recognizing the user's pointing gesture. The steerable projector mounted on the mobile robot can improve the brightness and resolution of the ROI of a large image projected by a stationary projector installed in the Intelligent Space. In the proposed system, the user is not required to hold any apparatuses for interacting with the display. Additionally, the proposed system is easy to use because it is designed with the natural and intuitive hand movement of user in mind. In the experiments, we demonstrate the feasibility of the proposed system. © 2012 Lee et al.},
	journal = {International Journal of Advanced Robotic Systems},
	author = {Lee, J.-E. and Park, J. and Kim, G.-S. and Lee, J.-H. and Kim, M.-H.},
	year = {2012},
	keywords = {1final\_includes, 1include\_search6},
	annote = {cited By 1},
	file = {Full Text:/home/brendan/Zotero/storage/73RJ5GPU/Lee et al. - 2012 - Interactive multi-resolution display using a proje.pdf:application/pdf}
}

@article{yu_interactive_2019,
	title = {Interactive {Robot} {Learning} for {Multimodal} {Emotion} {Recognition}},
	volume = {11876 LNAI},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076569737&doi=10.1007%2f978-3-030-35888-4_59&partnerID=40&md5=65c512432d4bdb6d70e39b74ea2f14fb},
	doi = {10.1007/978-3-030-35888-4_59},
	abstract = {Interaction plays a critical role in skills learning for natural communication. In human-robot interaction (HRI), robots can get feedback during the interaction to improve their social abilities. In this context, we propose an interactive robot learning framework using multimodal data from thermal facial images and human gait data for online emotion recognition. We also propose a new decision-level fusion method for the multimodal classification using Random Forest (RF) model. Our hybrid online emotion recognition model focuses on the detection of four human emotions (i.e., neutral, happiness, angry, and sadness). After conducting offline training and testing with the hybrid model, the accuracy of the online emotion recognition system is more than 10\% lower than the offline one. In order to improve our system, the human verbal feedback is injected into the robot interactive learning. With the new online emotion recognition system, a 12.5\% accuracy increase compared with the online system without interactive robot learning is obtained. © 2019, Springer Nature Switzerland AG.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Yu, C. and Tapus, A.},
	year = {2019},
	keywords = {1final\_includes, 1include\_search6},
	pages = {633--642},
	annote = {cited By 4},
	file = {Full Text:/home/brendan/Zotero/storage/P29R5EF9/Yu and Tapus - 2019 - Interactive Robot Learning for Multimodal Emotion .pdf:application/pdf}
}

@inproceedings{benabdallah_kinect-based_2015,
	title = {Kinect-based {Computed} {Torque} {Control} for lynxmotion robotic arm},
	doi = {10.1109/ICMIC.2015.7409416},
	abstract = {This paper focuses on a Kinect-based real-time-interactive control system implementation. Based on LabVIEW integrated development environment (IDE), a developed human machine-interface (HMI) allows user to control in real time a Lynxmotion robotic arm. The Kinect software development kit (SDK) provides a tool to keep track of human body skeleton and abstract it into 3 dimensions coordinates. Therefore, Kinect sensor is integrated into our control system to detect the different user joints coordinates. An inverse kinematic calculation algorithm-like method is proposed to calculate different user joints positions, velocities and accelerations. First, the dynamic model of the lynxmotion robot has been developed. Then, a real time computed-torque control algorithm has been implemented. Experimental results prove the effectiveness of the control approach.},
	booktitle = {2015 7th {International} {Conference} on {Modelling}, {Identification} and {Control} ({ICMIC})},
	author = {Benabdallah, Ismail and Bouteraa, Yassine and Boucetta, Rahma and Rekik, Chokri},
	month = dec,
	year = {2015},
	keywords = {1final\_includes, 1include\_search6, Computational modeling, Computed Torque Control, Dynamics, gesture recognition, Heuristic algorithms, Human-Machine-Interface, manipulator robot, Mathematical model, Microsoft Kinect, Robot kinematics, Robot sensing systems},
	pages = {1--6},
	file = {Full Text:/home/brendan/Zotero/storage/XS2RRC93/Benabdallah et al. - 2015 - Kinect-based Computed Torque Control for lynxmotio.pdf:application/pdf}
}

@inproceedings{paetzel_let_2019,
	address = {New York, NY, USA},
	series = {{HAI} '19},
	title = {Let {Me} {Get} {To} {Know} {You} {Better}: {Can} {Interactions} {Help} to {Overcome} {Uncanny} {Feelings}?},
	isbn = {978-1-4503-6922-0},
	url = {https://doi-org.ezp01.library.qut.edu.au/10.1145/3349537.3351894},
	doi = {10.1145/3349537.3351894},
	abstract = {With an ever increasing demand for personal service robots and artificial assistants, companies, start-ups and researchers aim to better understand what makes robot platforms more likable. Some argue that increasing a robot's humanlikeness leads to a higher acceptability. Others, however, find that extremely humanlike robots are perceived as uncanny and are consequently often rejected by users. When investigating people's perception of robots, the focus of the related work lies almost solely on the first impression of these robots, often measured based on images or video clips of the robots alone. Little is known about whether these initial positive or negative feelings persist when giving people the chance to interact with the robot. In this paper, 48 participants were gradually exposed to the capabilities of a robot and their perception of it was tracked from their first impression to after playing a short interactive game with it. We found that initial uncanny feelings towards the robot were significantly decreased after getting to know it better, which further highlights the importance of using real interactive scenarios when studying people's perception of robots. In order to elicit uncanny feelings, we used the 3D blended embodiment Furhat and designed four different facial textures for it. Our work shows that a blended platform can cause different levels of discomfort towards it depending on the facial texture and may thus be an interesting tool for further research on the uncanny valley.},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Human}-{Agent} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Paetzel, Maike and Castellano, Ginevra},
	year = {2019},
	note = {event-place: Kyoto, Japan},
	keywords = {1final\_includes, 1include\_search6, embodiment, human-robot interaction, uncanny valley},
	pages = {59--67},
	file = {Full Text:/home/brendan/Zotero/storage/3W2ZVNBB/Paetzel and Castellano - 2019 - Let Me Get To Know You Better Can Interactions He.pdf:application/pdf}
}

@inproceedings{terreran_low-cost_2020,
	title = {Low-cost scalable people tracking system for human-robot collaboration in industrial environment},
	volume = {51},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099814621&doi=10.1016%2fj.promfg.2020.10.018&partnerID=40&md5=785fb609b49c94305e08d9346fba1c25},
	doi = {10.1016/j.promfg.2020.10.018},
	abstract = {Human-robot collaboration is one of the key elements in the Industry 4.0 revolution, aiming to a close and direct collaboration between robots and human workers to reach higher productivity and improved ergonomics. The first step toward such kind of collaboration in the industrial context is the removal of physical safety barriers usually surrounding standard robotic cells, so that human workers can approach and directly collaborate with robots. Anyway, human safety must be granted avoiding possible collisions with the robot. In this work, we propose the use of a people tracking algorithm to monitor people moving around a robot manipulator and recognize when a person is too close to the robot while performing a task. The system is implemented by a camera network system positioned around the robot workspace, and thoroughly evaluated in different industry-like settings in terms of both tracking accuracy and detection delay. © 2020 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/) Peer-review under responsibility of the scientific committee of the FAIM 2021.},
	booktitle = {Procedia {Manufacturing}},
	author = {Terreran, M. and Lamon, E. and Michieletto, S. and Pagello, E.},
	year = {2020},
	keywords = {1final\_includes, 1include\_search6},
	pages = {116--124},
	annote = {cited By 0},
	file = {Terreran et al. - 2020 - Low-cost scalable people tracking system for human.pdf:/home/brendan/Zotero/storage/KHAQ9MKC/Terreran et al. - 2020 - Low-cost scalable people tracking system for human.pdf:application/pdf}
}

@inproceedings{silva_mirroring_2016,
	title = {Mirroring emotion system - on-line synthesizing facial expressions on a robot face},
	doi = {10.1109/ICUMT.2016.7765359},
	abstract = {Social skills are an important issue throughout human life. Therefore, systems that can synthesize emotions, for example virtual characters (avatars) and robotic platforms, are gaining special attention in the literature. In particular, those systems may be important tools in order to promote social and emotional competences in children (or adults) that have some communication/interaction impairments. The present paper proposes a mirroring emotion system that uses the recent Intel RealSense 3D sensor along with a humanoid robot. The system extracts the user's facial Action Units (AUs) and head motion data. Then, it sends the information to the robot allowing on-line imitation. The first tests were conducted in a laboratorial environment using the software FaceReader in order to verify its correct functioning. Next, a perceptual study was performed to verify the similarity between the expressions of a performer and those of the robot using a quiz distributed to 59 respondents. Finally, the system was evaluated with typically developing children with 6 to 9 years old. The robot mimicked the children's emotional facial expressions. The results point out that the present system can on-line and accurately map facial expressions of a user onto the robot.},
	booktitle = {2016 8th {International} {Congress} on {Ultra} {Modern} {Telecommunications} and {Control} {Systems} and {Workshops} ({ICUMT})},
	author = {Silva, Vinicius and Soares, Filomena and Esteves, João Sena},
	month = oct,
	year = {2016},
	note = {ISSN: 2157-023X},
	keywords = {1final\_includes, 1include\_search6, Facial Expressions, Intel RealSense, Mimicking Emotions, Zeno robot},
	pages = {213--218},
	file = {Full Text:/home/brendan/Zotero/storage/93RF9AZ8/Silva et al. - 2016 - Mirroring emotion system - on-line synthesizing fa.pdf:application/pdf}
}

@inproceedings{pustianu_mobile_2011,
	title = {Mobile robot control using face recognition algorithms},
	abstract = {In this paper is presented an algorithm to detect human faces using the orthogonal projection principle PCA (Principal Component Analysis) and a neural network classifier trained offline with a set of training images acquired from a webcam as well as from the CBLC and Manchester databases. The algorithm for the detection and tracking of faces will be implemented in a Visual C + + application, which will be tested on the mobile robot. The purpose of this paper is to implement an image processing algorithm in robotics thus making a human-robot interaction. The experimental results have been achieved on the PeopleBot, a two-wheeled differently driven mobile robot with a castor wheel for stability.},
	booktitle = {15th {International} {Conference} on {System} {Theory}, {Control} and {Computing}},
	author = {Pustianu, Alexandru Ionut and Serbencu, Adriana and Cernega, Daniela Cristina},
	month = oct,
	year = {2011},
	keywords = {1final\_includes, 1include\_search6, Algorithm design and analysis, Face, Face detection, Mobile robots, Principal component analysis, Vectors},
	pages = {1--6},
	file = {Pustianu et al. - 2011 - Mobile robot control using face recognition algori.pdf:/home/brendan/Zotero/storage/WQEJ3WPH/Pustianu et al. - 2011 - Mobile robot control using face recognition algori.pdf:application/pdf}
}

@inproceedings{zambelli_multimodal_2016,
	title = {Multimodal imitation using self-learned sensorimotor representations},
	doi = {10.1109/IROS.2016.7759582},
	abstract = {Although many tasks intrinsically involve multiple modalities, often only data from a single modality are used to improve complex robots acquisition of new skills. We present a method to equip robots with multimodal learning skills to achieve multimodal imitation on-the-fly on multiple concurrent task spaces, including vision, touch and proprioception, only using self-learned multimodal sensorimotor relations, without the need of solving inverse kinematic problems or explicit analytical models formulation. We evaluate the proposed method on a humanoid iCub robot learning to interact with a piano keyboard and imitating a human demonstration. Since no assumptions are made on the kinematic structure of the robot, the method can be also applied to different robotic platforms.},
	booktitle = {2016 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Zambelli, Martina and Demiris, Yiannis},
	month = oct,
	year = {2016},
	note = {ISSN: 2153-0866},
	keywords = {1final\_includes, 1include\_search6, Kinematics, Manipulators, Robot kinematics, Robot sensing systems, Trajectory, Visualization},
	pages = {3953--3958},
	file = {Submitted Version:/home/brendan/Zotero/storage/HNL8B6NN/Zambelli and Demiris - 2016 - Multimodal imitation using self-learned sensorimot.pdf:application/pdf}
}

@article{anzalone_multimodal_2013,
	title = {Multimodal people engagement with {iCub}},
	volume = {196 AISC},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870779018&doi=10.1007%2f978-3-642-34274-5_16&partnerID=40&md5=8bf847c663dd0cde0164fe7a48ab80f9},
	doi = {10.1007/978-3-642-34274-5_16},
	abstract = {In this paper we present an engagement system for the iCub robot that is able to arouse in human partners a sense of "co-presence" during human-robot interaction. This sensation is naturally triggered by simple reflexes of the robot, that speaks to the partners and gazes the current "active partner" (e.g. the talking partner) in interaction tasks. The active partner is perceived through a multimodal approach: a commercial rgb-d sensor is used to recognize the presence of humans in the environment, using both 3d information and sound source localization, while iCub's cameras are used to perceive his face. © 2013 Springer-Verlag.},
	journal = {Advances in Intelligent Systems and Computing},
	author = {Anzalone, S.M. and Ivaldi, S. and Sigaud, O. and Chetouani, M.},
	year = {2013},
	keywords = {1final\_includes, 1include\_search6},
	pages = {59--64},
	annote = {cited By 2},
	file = {Full Text:/home/brendan/Zotero/storage/P7CU3GZ6/Anzalone et al. - 2013 - Multimodal people engagement with iCub.pdf:application/pdf}
}

@inproceedings{ozgur_natural_2014,
	title = {Natural user interface for {Roombots}},
	doi = {10.1109/ROMAN.2014.6926223},
	abstract = {Roombots (RB) are self-reconfigurable modular robots designed to study robotic reconfiguration on a structured grid and adaptive locomotion off grid. One of the main goals of this platform is to create adaptive furniture inside living spaces such as homes or offices. To ease the control of RB modules in these environments, we propose a novel and more natural way of interaction with the RB modules on a RB grid, called the Natural Roombots User Interface. In our method, the user commands the RB modules using pointing gestures. The user's body is tracked using multiple Kinects. The user is also given real-time visual feedback of their physical actions and the state of the system via LED illumination electronics installed on both RB modules and the grid. We demonstrate how our interface can be used to efficiently control RB modules on simple point-to-point grid locomotion and conclude by discussing future extensions.},
	booktitle = {The 23rd {IEEE} {International} {Symposium} on {Robot} and {Human} {Interactive} {Communication}},
	author = {Özgur, Ayberk and Bonardi, Stéphane and Vespignani, Massimo and Möckel, Rico and Ijspeert, Auke J.},
	month = aug,
	year = {2014},
	note = {ISSN: 1944-9437},
	keywords = {1final\_includes, 1include\_search6, Color, Light emitting diodes, Lighting, Robot sensing systems, Three-dimensional displays, Visualization},
	pages = {12--17},
	file = {Submitted Version:/home/brendan/Zotero/storage/A2QTLGGS/Özgur et al. - 2014 - Natural user interface for Roombots.pdf:application/pdf}
}

@article{chen_online_2019,
	title = {Online control programming algorithm for human–robot interaction system with a novel real-time human gesture recognition method},
	volume = {16},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068878408&doi=10.1177%2f1729881419861764&partnerID=40&md5=f9cb0b6c02f52869229a2f9123633aa4},
	doi = {10.1177/1729881419861764},
	abstract = {This article proposes an online control programming algorithm for human–robot interaction systems, where robot actions are controlled by the recognition results of gestures performed by human operators based on visual images. In contrast to traditional robot control systems that use pre-defined programs to control a robot where the robot cannot change its tasks freely, this system allows the operator to train online and replan human–robot interaction tasks in real time. The proposed system is comprised of three components: an online personal feature pretraining system, a gesture recognition system, and a task replanning system for robot control. First, we collected and analyzed features extracted from images of human gestures and used those features to train the recognition program in real time. Second, a multifeature cascade classifier algorithm was applied to guarantee both the accuracy and real-time processing of our gesture recognition method. Finally, to confirm the effectiveness of our algorithm, we selected a flight robot as our test platform to conduct an online robot control experiment based on the visual gesture recognition algorithm. Through extensive experiments, the effectiveness and efficiency of our method has been confirmed. © The Author(s) 2019.},
	number = {4},
	journal = {International Journal of Advanced Robotic Systems},
	author = {Chen, B. and Hua, C. and Dai, B. and He, Y. and Han, J.},
	year = {2019},
	keywords = {1final\_includes, 1include\_search6},
	annote = {cited By 4},
	file = {Full Text:/home/brendan/Zotero/storage/X8NLBCDY/Chen et al. - 2019 - Online control programming algorithm for human–rob.pdf:application/pdf}
}

@article{hegger_people_2013,
	title = {People detection in 3d point clouds using local surface normals},
	volume = {7500 LNAI},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883352267&doi=10.1007%2f978-3-642-39250-4_15&partnerID=40&md5=3346b2f3ef9d9a7c092f781845774215},
	doi = {10.1007/978-3-642-39250-4_15},
	abstract = {The ability to detect people in domestic and unconstrained environments is crucial for every service robot. The knowledge where people are is required to perform several tasks such as navigation with dynamic obstacle avoidance and human-robot-interaction. In this paper we propose a people detection approach based on 3d data provided by a RGB-D camera. We introduce a novel 3d feature descriptor based on Local Surface Normals (LSN) which is used to learn a classifier in a supervised machine learning manner. In order to increase the systems flexibility and to detect people even under partial occlusion we introduce a top-down/bottom-up segmentation. We deployed the people detection system on a real-world service robot operating at a reasonable frame rate of 5Hz. The experimental results show that our approach is able to detect persons in various poses and motions such as sitting, walking, and running. © 2013 Springer-Verlag.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Hegger, F. and Hochgeschwender, N. and Kraetzschmar, G.K. and Ploeger, P.G.},
	year = {2013},
	keywords = {1final\_includes, 1include\_search6},
	pages = {154--164},
	annote = {cited By 9},
	file = {Full Text:/home/brendan/Zotero/storage/JL2NWWWA/Hegger et al. - 2013 - People detection in 3d point clouds using local su.pdf:application/pdf}
}

@inproceedings{costante_personalizing_2014,
	title = {Personalizing vision-based gestural interfaces for {HRI} with {UAVs}: a transfer learning approach},
	doi = {10.1109/IROS.2014.6943024},
	abstract = {Following recent works on HRI for UAVs, we present a gesture recognition system which operates on the video stream recorded from a passive monocular camera installed on a quadcopter. While many challenges must be addressed for building a real-time vision-based gestural interface, in this paper we specifically focus on the problem of user personalization. Different users tend to perform the same gesture with different styles and speed. Thus, a system trained on visual sequences depicting some users may work poorly when data from other people are available. On the other hand, collecting and annotating many user-specific data is time consuming. To avoid these issues, in this paper we propose a personalized gestural interface. We introduce a novel transfer learning algorithm which, exploiting both data downloaded from the web and gestures collected from other users, permits to learn a set of person-specific classifiers. We integrate the proposed gesture recognition module into a HRI system with a flying quadrotor robot. In our system first the UAV localizes a person and individuates her identity. Then, when a user performs a specific gesture, the system recognizes it adopting the associated user-specific classifier and the quadcopter executes the corresponding task. Our experimental evaluation demonstrates that the proposed personalized gesture recognition solution is advantageous with respect to generic ones.},
	booktitle = {2014 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	author = {Costante, Gabriele and Bellocchio, Enrico and Valigi, Paolo and Ricci, Elisa},
	month = sep,
	year = {2014},
	note = {ISSN: 2153-0866},
	keywords = {1final\_includes, 1include\_search6, Cameras, Feature extraction, Gesture recognition, Kernel, Robots, Vectors, Visualization},
	pages = {3319--3326},
	file = {Full Text:/home/brendan/Zotero/storage/MPEEI9U7/Costante et al. - 2014 - Personalizing vision-based gestural interfaces for.pdf:application/pdf}
}

@inproceedings{konda_real_2012,
	address = {New York, NY, USA},
	series = {{HRI} '12},
	title = {Real {Time} {Interaction} with {Mobile} {Robots} {Using} {Hand} {Gestures}},
	isbn = {978-1-4503-1063-5},
	url = {https://doi-org.ezp01.library.qut.edu.au/10.1145/2157689.2157743},
	doi = {10.1145/2157689.2157743},
	abstract = {We developed a robust real time hand gesture based interaction system to effectively communicate with a mobile robot which can operate in an outdoor environment. The system enables the user to operate a mobile robot using hand gesture based commands. In particular the system offers direct on site interaction providing better perception of environment to the user. To overcome the illumination challenges in outdoors, the system operates on depth images. Processed depth images are given as input to a convolutional neural network which is trained to detect static hand gestures.The system is evaluated in real world experiments on a mobile robot to show the operational efficiency in outdoor environment.},
	booktitle = {Proceedings of the {Seventh} {Annual} {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Konda, Kishore Reddy and Königs, Achim and Schulz, Hannes and Schulz, Dirk},
	year = {2012},
	note = {event-place: Boston, Massachusetts, USA},
	keywords = {1final\_includes, 1include\_search6, gesture detection, human-robot interaction, neural network},
	pages = {177--178},
	file = {Full Text:/home/brendan/Zotero/storage/XGKKTLLG/Konda et al. - 2012 - Real Time Interaction with Mobile Robots Using Han.pdf:application/pdf}
}

@inproceedings{barros_real-time_2014,
	title = {Real-time gesture recognition using a humanoid robot with a deep neural architecture},
	doi = {10.1109/HUMANOIDS.2014.7041431},
	abstract = {Dynamic gesture recognition is one of the most interesting and challenging areas of Human-Robot-Interaction (HRI). Problems like image segmentation, temporal and spatial feature extraction and real time recognition are the most promising issues to name in this context. This work proposes a deep neural model to recognize dynamic gestures with minimal image preprocessing and real time recognition in an experimental set up using a humanoid robot. We conducted two experiments with command gestures in an offline fashion and for demonstration in a Human-Robot-Interaction (HRI) scenario. Our results showed that the proposed model achieves high classification rates of the gestures executed by different subjects, who perform them with varying speed. With our additional audio feedback we demonstrate that our system performs in real time.},
	booktitle = {2014 {IEEE}-{RAS} {International} {Conference} on {Humanoid} {Robots}},
	author = {Barros, Pablo and Parisi, German I. and Jirak, Doreen and Wermter, Stefan},
	month = nov,
	year = {2014},
	note = {ISSN: 2164-0580},
	keywords = {1final\_includes, 1include\_search6, Computer architecture, Feature extraction, Gesture recognition, Kernel, Real-time systems, Robots, Training},
	pages = {646--651},
	file = {Full Text:/home/brendan/Zotero/storage/37BMQZFT/Barros et al. - 2014 - Real-time gesture recognition using a humanoid rob.pdf:application/pdf}
}

@inproceedings{moe_real-time_2013,
	title = {Real-time hand guiding of industrial manipulator in 5 {DOF} using {Microsoft} {Kinect} and accelerometer},
	doi = {10.1109/ROMAN.2013.6628421},
	abstract = {Human computer interaction is currently developing toward more intuitive and natural ways of communication. The main goal of research presented in this paper is the development of a system for controlling a robot manipulator in 5 DOF using human motions. The developed system reads and interprets sensor data from a Microsoft Kinect and an accelerometer embedded in a smartphone and use this data and the robot kinematics to generate a position and orientation reference to the robot controller allowing real-time interaction with the robot. The system has been implemented and tested on a Universal Robot manipulator arm with 6 joints. This robot has an embedded controller able to receive a reference in both Cartesian and joint space and calculate and track trajectories that fulfills this reference. The main result of this research is the architecture of an industrially oriented hand guiding system.},
	booktitle = {2013 {IEEE} {RO}-{MAN}},
	author = {Moe, Signe and Schjølberg, Ingrid},
	month = aug,
	year = {2013},
	note = {ISSN: 1944-9437},
	keywords = {1final\_includes, 1include\_search6, Computers, Cybernetics, Manipulators},
	pages = {644--649},
	file = {Full Text:/home/brendan/Zotero/storage/3SDIDLTH/Moe and Schjølberg - 2013 - Real-time hand guiding of industrial manipulator i.pdf:application/pdf}
}

@inproceedings{zhu_real-time_2016,
	title = {Real-time imitation framework for humanoid robots based on posture classification},
	volume = {2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021216149&doi=10.1109%2fICMLC.2016.7872936&partnerID=40&md5=4ae38916815be63f0f785e3280855567},
	doi = {10.1109/ICMLC.2016.7872936},
	abstract = {A safe real-time imitation framework for humanoid robots to human beings action is proposed. Postures of an actor are divided into three categories: robust postures, normal postures and dangerous postures. To deal with the robust postures, direct mapping from skeleton data gathered by 3D cameras to each angle of robot joint motors is taken because the postures are robust enough for imitation in high speed. When it comes to a normal posture, one specific safe robot posture is designed to correspond to each part of separated human postures. As the actor changes his posture from one to another, the robot will also switch from the corresponding posture to another in the designed way. Meanwhile, when there is no need to switch, the direct mapping is taken to fulfill the task. The imitation strategy for dangerous postures is the same as the normal postures except that the robot can not use direct mapping when there is no switch task because some changes of the designed posture may lead to tumble. © 2016 IEEE.},
	booktitle = {Proceedings - {International} {Conference} on {Machine} {Learning} and {Cybernetics}},
	author = {Zhu, M.-D. and Xia, L.-X. and Su, J.-B.},
	year = {2016},
	keywords = {1final\_includes, 1include\_search6},
	pages = {489--494},
	annote = {cited By 0},
	file = {Full Text:/home/brendan/Zotero/storage/D58U2Q7B/Zhu et al. - 2016 - Real-time imitation framework for humanoid robots .pdf:application/pdf}
}

@inproceedings{hsu_real-time_2020,
	title = {Real-{Time} {Interaction} {System} of {Human}-{Robot} with {Hand} {Gestures}},
	doi = {10.1109/ECICE50847.2020.9301957},
	abstract = {With the declining birthrate and the aging population, the shortage of labor has become more obvious. Therefore, the teleoperation of the robot as the substitute of human labour have become popular. In this study, an application of real-time gesture-based human-robot interaction is proposed using a Kinect sensor for elderly care. By employing gesture-based teleoperation of robots, demands, and conditions of the elderly such as sudden falling and the abnormality can be quickly recognized. The robot used in this study, iRobot Create, is a popular mobile robot for indoor usage which will recognize the following gestures: right-hand raising, left-hand raising, right-hand waving, and left hand waving, in corresponding to the control the robot's moving forward, backward, right turn, and left turn, respectively. Experimental results shown that the four types of gestures can be recognized by the iRobot Create with human gestures captured by the Kinect sensor. The corresponding actions in real-time were realized through the designed gesture-based remote human-robot interaction system. The experimental results are also can be seen from the Youtube channel.},
	booktitle = {2020 {IEEE} {Eurasia} {Conference} on {IOT}, {Communication} and {Engineering} ({ECICE})},
	author = {Hsu, Roy Chaoming and Su, Po-Cheng and Hsu, Jia-Le and Wang, Chi-Yong},
	month = oct,
	year = {2020},
	keywords = {1final\_includes, 1include\_search6, elderly care, gesture recognition, Human-robot interaction, Keyboards, Real-time systems, Robot kinematics, Robot sensing systems, Robots, Senior citizens, teleoperation of robot},
	pages = {396--398},
	file = {Full Text:/home/brendan/Zotero/storage/MCX2VIW4/Hsu et al. - 2020 - Real-Time Interaction System of Human-Robot with H.pdf:application/pdf}
}

@article{farulla_real-time_2014,
	title = {Real-{Time} {Single} {Camera} {Hand} {Gesture} {Recognition} {System} for {Remote} {Deaf}-{Blind} {Communication}},
	volume = {8853},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918576783&doi=10.1007%2f978-3-319-13969-2_3&partnerID=40&md5=3781c9ef14c2648d6c50aed467b5188c},
	doi = {10.1007/978-3-319-13969-2_3},
	abstract = {This paper presents a fast approach for marker-less Full-DOF hand tracking, leveraging only depth information from a single depth camera. This system can be useful in many applications, ranging from tele-presence to remote control of robotic actuators or interaction with 3D virtual environment. We applied the proposed technology to enable remote transmission of signs from Tactile Sing Languages (i.e., Sign Languages with Tactile feedbacks), allowing non-invasive remote communication not only among deaf-blind users, but also with deaf, blind and hearing with proficiency in Sign Languages. We show that our approach paves the way to a fluid and natural remote communication for deaf-blind people, up to now impossible. This system is a first prototype for the PARLOMA project, which aims at designing a remote communication system for deaf-blind people. © Springer International Publishing Switzerland 2014.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Farulla, G.A. and Russo, L.O. and Pintor, C. and Pianu, D. and Micotti, G. and Salgarella, A.R. and Camboni, D. and Controzzi, M. and Cipriani, C. and Oddo, C.M. and Rosa, S. and Indaco, M.},
	year = {2014},
	keywords = {1final\_includes, 1include\_search6},
	pages = {35--52},
	annote = {cited By 8},
	file = {Full Text:/home/brendan/Zotero/storage/VZ7CFLAT/Farulla et al. - 2014 - Real-Time Single Camera Hand Gesture Recognition S.pdf:application/pdf}
}

@article{maher_realtime_2017,
	title = {Realtime {Human}-{UAV} {Interaction} {Using} {Deep} {Learning}},
	volume = {10568 LNCS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032690344&doi=10.1007%2f978-3-319-69923-3_55&partnerID=40&md5=3ae959b4ba9131bf4855f373068c53b5},
	doi = {10.1007/978-3-319-69923-3_55},
	abstract = {In this paper, we propose a realtime human gesture identification for controlling a micro UAV in a GPS denied environment. Exploiting the breakthrough of deep convolution network in computer vision, we develop a robust Human-UAV Interaction (HUI) system that can detect and identify a person gesture to control a micro UAV in real time. We also build a new dataset with 23 participants to train or fine-tune the deep neural networks for human gesture detection. Based on the collected dataset, the state-of-art YOLOv2 detection network is tailored to detect the face and two hands locations of a human. Then, an interpreter approach is proposed to infer the gesture from detection results, in which each interpreted gesture is equivalent to a UAV flying command. Real flight experiments performed by non-expert users with the Bebop 2 micro UAV have approved our proposal for HUI. The gesture detection deep model with a demo will be publicly available to aid the research work. © 2017, Springer International Publishing AG.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Maher, A. and Li, C. and Hu, H. and Zhang, B.},
	year = {2017},
	keywords = {1final\_includes, 1include\_search6},
	pages = {511--519},
	annote = {cited By 6},
	file = {Full Text:/home/brendan/Zotero/storage/GIP4Z29Q/Maher et al. - 2017 - Realtime Human-UAV Interaction Using Deep Learning.pdf:application/pdf}
}

@inproceedings{baron_remote_2013,
	title = {Remote control of the artificial arm model using {3D} hand tracking},
	doi = {10.1109/SELM.2013.6562954},
	abstract = {Modern sensing technologies create new possibilities to control mobile robots without any dedicated manipulators. In this article authors present a novel method that enables driving of the Mindstorms NXT artificial arm with Microsoft Kinect, using gesture recognition. To imitate movement of an artificial robotic arm, an algorithm of the human-computer interaction is employed using skeleton tracking and gesture control in 3D space.},
	booktitle = {2013 {International} {Symposium} on {Electrodynamic} and {Mechatronic} {Systems} ({SELM})},
	author = {Baron, G. and Czekalski, P. and Malicki, D. and Tokarz, K.},
	month = may,
	year = {2013},
	keywords = {1final\_includes, 1include\_search6, Actuators, Buildings, gesture control, Java, mobile robots, remote control, Robot sensing systems, Tracking},
	pages = {9--10},
	file = {Full Text:/home/brendan/Zotero/storage/3PEUQUA4/Baron et al. - 2013 - Remote control of the artificial arm model using 3.pdf:application/pdf}
}

@inproceedings{obo_robot_2015,
	title = {Robot posture generation based on genetic algorithm for imitation},
	doi = {10.1109/CEC.2015.7256938},
	abstract = {Human-like-motion performed by robots can have a contribution to exert a strong influence on human-robot interaction, because bodily expressions convey important and effective information. If the robots could adapt the features of human behavior to their motions and skills, the communication would become more smooth and natural. In this paper, we develop a posture measurement system for a robot imitation using a 3D image sensor. This paper proposes a method of robot posture generation based on a steady-state genetic algorithm (SSGA). SSGA is one of evolutionary optimization methods using selection, mutation, and crossover operators. Since SSGA is a simplified model, it is easy to implement into a real-time processing. Furthermore, we apply a continuous model of generation for an adaptive search in dynamical environment.},
	booktitle = {2015 {IEEE} {Congress} on {Evolutionary} {Computation} ({CEC})},
	author = {Obo, Takenori and Loo, Chu Kiong and Kubota, Naoyuki},
	month = may,
	year = {2015},
	note = {ISSN: 1941-0026},
	keywords = {1final\_includes, 1include\_search6, 3D image sensor, Elbow, Genetic algorithms, imitation, Joints, Kinematics, posture generation, Robot sensing systems, Shoulder, steady-state genetic algorithm},
	pages = {552--557},
	file = {Full Text:/home/brendan/Zotero/storage/S2C3RJDQ/Obo et al. - 2015 - Robot posture generation based on genetic algorith.pdf:application/pdf}
}

@article{chao_robotic_2014,
	title = {Robotic free writing of chinese characters via human-robot interactions},
	volume = {11},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898450328&doi=10.1142%2fS0219843614500078&partnerID=40&md5=0f1574e72692698b71bf69c9c2fe789a},
	doi = {10.1142/S0219843614500078},
	abstract = {Implementation of robotic writing ability is recognized as a difficult task, which involves complicated image processing and robotic control algorithms. This paper introduces a novel approach to robotic writing by using human-robot interactions. The method applies a motion sensing input device to capture a human demonstrator's arm trajectories, uses a gesture determination algorithm to extract a Chinese character's strokes from these trajectories, and employs noise filtering and curve fitting methods to optimize the strokes. The approach displays real-time captured trajectories to the human demonstrator; therefore, the human demonstrator is able to adjust his/her gesture to achieve a better character writing effect. Then, our robot writes the human-gestured character by using the robotic arm's joint values. The inverse kinematics algorithm generates the joint values from the stroke trajectories. Experimental analysis shows that the proposed approach can allow a human to naturally and conveniently control the robot in order to write many Chinese characters. Additionally, this approach allows the robot to achieve a satisfactory writing quality for characters with a simple structure, with the potential to write more complex characters. © 2014 World Scientific Publishing Company.},
	number = {1},
	journal = {International Journal of Humanoid Robotics},
	author = {Chao, F. and Chen, F. and Shen, Y. and He, W. and Sun, Y. and Wang, Z. and Zhou, C. and Jiang, M.},
	year = {2014},
	keywords = {1final\_includes, 1include\_search6},
	annote = {cited By 16},
	file = {Chao et al. - 2014 - Robotic free writing of chinese characters via hum.pdf:/home/brendan/Zotero/storage/P2MXK2F6/Chao et al. - 2014 - Robotic free writing of chinese characters via hum.pdf:application/pdf}
}

@inproceedings{weiss_robots_2010,
	series = {{HRI} '10},
	title = {Robots {Asking} for {Directions}: {The} {Willingness} of {Passers}-by to {Support} {Robots}},
	isbn = {978-1-4244-4893-7},
	abstract = {This paper reports about a human-robot interaction field trial conducted with the autonomous mobile robot ACE (Autonomous City Explorer) in a public place, where the ACE robot needs the support of human passers-by to find its way to a target location. Since the robot does not possess any prior map knowledge or GPS support, it has to acquire missing information through interaction with humans. The robot thus has to initiate communication by asking for the way, and retrieves information from passers-by showing the way by gestures (pointing) and marking goal positions on a still image on the touch screen of the robot. The aims of the field trial where threefold: (1) Investigating the aptitude of the navigation architecture, (2) Evaluating the intuitiveness of the interaction concept for the passers-by, (3) Assessing people's willingness to support the ACE robot in its task, i.e. assessing the social acceptability. The field trial demonstrates that the architecture enables successful autonomous path finding without any prior map knowledge just by route directions given by passers-by. An additional street survey and observational data moreover attests the intuitiveness of the interaction paradigm and the high acceptability of the ACE robot in the public place.},
	booktitle = {Proceedings of the 5th {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {IEEE Press},
	author = {Weiss, Astrid and Igelsböck, Judith and Tscheligi, Manfred and Bauer, Andrea and Kühnlenz, Kolja and Wollherr, Dirk and Buss, Martin},
	year = {2010},
	note = {event-place: Osaka, Japan},
	keywords = {1final\_includes, 1include\_search6, autonomous mobile robot, field trial, human-robot interaction, social acceptance},
	pages = {23--30},
	file = {Weiss et al. - 2010 - Robots Asking for Directions The Willingness of P.pdf:/home/brendan/Zotero/storage/E7CR4N3P/Weiss et al. - 2010 - Robots Asking for Directions The Willingness of P.pdf:application/pdf}
}

@inproceedings{voisan_ros-based_2015,
	title = {{ROS}-based robot navigation and human interaction in indoor environment},
	doi = {10.1109/SACI.2015.7208244},
	abstract = {This paper sets the basis of a robot collaborative application framework that is able to perform basic interactions with a human partner, namely patrol and monitor a designated area, followed by interaction with a human participant when this is detected. The application is developed using an adaptation of ROS to Corobot robot and image processing techniques that involve OpenCV face detection.},
	booktitle = {2015 {IEEE} 10th {Jubilee} {International} {Symposium} on {Applied} {Computational} {Intelligence} and {Informatics}},
	author = {Voisan, Emil-Ioan and Paulis, Bogdan and Precup, Radu-Emil and Dragan, Florin},
	month = may,
	year = {2015},
	keywords = {1final\_includes, 1include\_search6, Face detection, Navigation, Robot kinematics, Simultaneous localization and mapping},
	pages = {31--36},
	file = {Full Text:/home/brendan/Zotero/storage/VH6JKB78/Voisan et al. - 2015 - ROS-based robot navigation and human interaction i.pdf:application/pdf}
}

@inproceedings{saveriano_safe_2014,
	title = {Safe motion generation and online reshaping using dynamical systems},
	doi = {10.1109/URAI.2014.7057407},
	abstract = {Summary form only given. In the future, robotic platforms will be integrated in social environment to actively cooperate with humans during their daily-life. A feasible human-robot co-working requires close and safe interaction, in which the partners can understand and quickly adapt their mutual behaviors. In this work, we present a real-time approach to generate safe (velocity scaling) and feasible (collision avoidance, goal adaptation) motions in a human-robot interaction scenario. The basic assumption is that robot's motion is generated using a first-order, asymptotically stable Dynamical System (DS): x = f(x), (1) where x and x represent the robot end-effector position and velocity respectively. Driving robots with DS has several advantages in terms of robustness to external perturbations, such as unexpected contacts or changes in the goal/initial position. The DS structure allows us to easily implement a human-based velocity scaling algorithm, by modifying Eq. (1) as: x = αhf(x), (2) where 0 {\textless}; αmin ≤ αh ≤ 1 is a scalar function inversely proportional to the human-robot distance. When the human, tracked using an RGB-D camera, enters in the robot's workspace αh = αmin, while the human goes away, αh linearly goes to 1. Being αh strictly positive, this scaling will not affect the equilibrium of the DS, in other words, the task will always be correctly executed. During tasks execution, unforeseen obstacle can appear in the robot workspace. To avoid possible collisions, saving the convergence properties of the DS, authors propose to modulate a generic first-order DS as [1]: x = M(x)(f(x)-o) + o, (3) where o is the obstacle velocity and M(x) is the so-called modulation matrix, used to reduce the end-effector velocity in the obstacle normal direction [2]. To avoid collisions with the robot's links, we calculate the closest point on the robot to the obstacle and use the approach in Eq. (3) to drive away that point. This task is projected in the manipulator Jacobian null-space to not affect the end-effector motion. The described approaches for velocity scaling and obstacle avoidance are integrated, together with an online reshaping method used to modify the robot's goal position, in a two levels hierarchical architecture (see Fig. 1). The higher level of the architecture is used to recognize human activities and to select/reshape the robot motion according with the current activity. The Human Activities Interpreter determines the user status (far/interaction) and the goals status (free/occluded). Rough depth data from a RGB-D sensor are first filtered to remove the points belonging to the robot surface and then used to track human and obstacles. Using this information, the Robot Behaviour Selection/Reshaping module to select the right behavior from a database of Motion Primitives, scaling down the velocity and changing the goal position when needed. The selected motion primitive is passed to the Collision Avoidance level, that implements the real-time, reactive collision avoidance algorithm in Eq. (3). The related video shows an application of the proposed approach in a human-robot interaction scenario. The robot moves counterclockwise towards four goal positions, while it avoids possible collisions. When the user enters in the workspace, the velocity of the robot is scaled down. If the user hides with his hands the goal, the next free goal position becomes the equilibrium point of the DS.},
	booktitle = {2014 11th {International} {Conference} on {Ubiquitous} {Robots} and {Ambient} {Intelligence} ({URAI})},
	author = {Saveriano, Matteo and Lee, Dongheui},
	month = nov,
	year = {2014},
	keywords = {1final\_includes, 1include\_search6, Collision avoidance, Jacobian matrices, Modulation, Real-time systems, Robot motion, Robot sensing systems},
	pages = {45--45},
	file = {Full Text:/home/brendan/Zotero/storage/IVAN3HZ9/Saveriano and Lee - 2014 - Safe motion generation and online reshaping using .pdf:application/pdf}
}

@inproceedings{svarny_safe_2019,
	title = {Safe physical {HRI}: {Toward} a unified treatment of speed and separation monitoring together with power and force limiting},
	doi = {10.1109/IROS40897.2019.8968463},
	abstract = {So-called collaborative robots are a current trend in industrial robotics. However, they still face many problems in practical application such as reduced speed to ascertain their collaborativeness. The standards prescribe two regimes: (i) speed and separation monitoring and (ii) power and force limiting, where the former requires reliable estimation of distances between the robot and human body parts and the latter imposes constraints on the energy absorbed during collisions prior to robot stopping. Following the standards, we deploy the two collaborative regimes in a single application and study the performance in a mock collaborative task under the individual regimes, including transitions between them. Additionally, we compare the performance under “safety zone monitoring” with keypoint pair-wise separation distance assessment relying on an RGB-D sensor and skeleton extraction algorithm to track human body parts in the workspace. Best performance has been achieved in the following setting: robot operates at full speed until a distance threshold between any robot and human body part is crossed; then, reduced robot speed per power and force limiting is triggered. Robot is halted only when the operator's head crosses a predefined distance from selected robot parts. We demonstrate our methodology on a setup combining a KUICA LBR iiwa robot, Intel RealSense RGB-D sensor and OpenPose for human pose estimation.},
	booktitle = {2019 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Svarny, Petr and Tesar, Michael and Behrens, Jan Kristof and Hoffmann, Matej},
	month = nov,
	year = {2019},
	note = {ISSN: 2153-0866},
	keywords = {1final\_includes, 1include\_search6},
	pages = {7580--7587},
	file = {Submitted Version:/home/brendan/Zotero/storage/37QFNX26/Svarny et al. - 2019 - Safe physical HRI Toward a unified treatment of s.pdf:application/pdf}
}

@inproceedings{fuad_skeleton_2015,
	title = {Skeleton based gesture to control manipulator},
	doi = {10.1109/ICAMIMIA.2015.7508010},
	abstract = {The key aspect to control the motion of manipulator intuitively is ability to map both joints on human and machine. This research proposes a method to interact with arm robot by using gesture based on skeleton image of user that is read from Kinect sensor. Velocity of each joint is defined by measuring angle rotation of every bone with respect to previous linked bone. Experiments has been carried out to manipulate SCORBOT-ER 9Pro with ability to handle 4 of 5 axises.},
	booktitle = {2015 {International} {Conference} on {Advanced} {Mechatronics}, {Intelligent} {Manufacture}, and {Industrial} {Automation} ({ICAMIMIA})},
	author = {Fuad, Muhammad},
	month = oct,
	year = {2015},
	keywords = {1final\_includes, 1include\_search6, Erbium, Gesture, Interaction, Joints, Kinect sensor, Manipulators, Robot sensing systems, SCORBOT-ER 9Pro, Shoulder, Skeleton Image},
	pages = {96--101},
	file = {Full Text:/home/brendan/Zotero/storage/D68SM3XM/Fuad - 2015 - Skeleton based gesture to control manipulator.pdf:application/pdf}
}

@article{ali_smart_2019,
	title = {Smart {Wheelchair} {Maneuvering} {Among} {People}},
	volume = {11645 LNAI},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070707610&doi=10.1007%2f978-3-030-26766-7_4&partnerID=40&md5=40192a26fb99c6f9b26315ed9f7fad45},
	doi = {10.1007/978-3-030-26766-7_4},
	abstract = {The advancement of technology is increasing with several applications on robotics like smart wheelchairs providing autonomous functions for severely impaired users with less need of caregiver support. One of the main issues in robotic wheelchair research is autonomous pedestrian avoidance for safety and smooth maneuvering. However, this is difficult because most of the pedestrians change their motion abruptly. Thus we need a fully autonomous smart wheelchair that can avoid collisions with individual or multiple pedestrians safely and with user comfort in mind for crowded environments like train stations. This paper presents a method for our smart wheelchair to maneuver through individual and multiple pedestrians by detecting and analyzing their interactions and predicted intentions with the wheelchair. Our smart wheelchair can obtain head and body orientations of pedestrians by using OpenPose. Using a single camera, we infer the walking directions or next movements of pedestrians by combining face pose and body posture estimation with our collision avoidance strategy in real-time. For an added layer of safety, we also use a LiDAR sensor for detection of any obstacles in the surrounding area to avoid collisions in advance. © 2019, Springer Nature Switzerland AG.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Ali, S. and Lam, A. and Fukuda, H. and Kobayashi, Y. and Kuno, Y.},
	year = {2019},
	keywords = {1final\_includes, 1include\_search6},
	pages = {32--42},
	annote = {cited By 0},
	file = {Full Text:/home/brendan/Zotero/storage/LBVGPS6U/Ali et al. - 2019 - Smart Wheelchair Maneuvering Among People.pdf:application/pdf}
}

@article{rehman_target_2017,
	title = {Target detection and tracking using intelligent wheelchair},
	volume = {18},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039946803&doi=10.5013%2fIJSSST.a.18.01.04&partnerID=40&md5=a6ca569a0d51955e6f6ff019eba61275},
	doi = {10.5013/IJSSST.a.18.01.04},
	abstract = {Human-robot interaction is one of the most basic requirements for service robots. In order to provide the desired service, these robots are required to detect and track people in human cluttered environment. Once the target to be followed has been recognized by robot, next step is navigation while taking care of obstacles. This paper presents a novel approach for the target detection to make the robot able of target tracking in a cluttered environment. Using data from Hough transform our system classifies the target person from other human beings in the environment. Our system tracks human being by gathering details of his position and velocity, and then converting this data into corresponding linear and angular velocity of robot. System used in the project is an intelligent wheelchair with partial AI and partial human command based working architecture with LRF and stereo camera mounted on it. © 2017, UK Simulation Society. All rights reserved.},
	number = {1},
	journal = {International Journal of Simulation: Systems, Science and Technology},
	author = {Rehman, S. and Ashraf, T. and Umair, M. and Zubair, U. and Ayaz, Y. and Khan, H.},
	year = {2017},
	keywords = {1final\_includes, 1include\_search6},
	pages = {4.1--4.8},
	annote = {cited By 0},
	file = {Rehman et al. - 2017 - Target detection and tracking using intelligent wh.pdf:/home/brendan/Zotero/storage/EYRS97BY/Rehman et al. - 2017 - Target detection and tracking using intelligent wh.pdf:application/pdf}
}

@article{agrigoroaie_enrichme_2016,
	title = {The {ENRICHME} project: {Lessons} learnt from a first interaction with the elderly},
	volume = {9979 LNAI},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992518980&doi=10.1007%2f978-3-319-47437-3_72&partnerID=40&md5=1be2447e73f3d4036c8b45fd1066c23e},
	doi = {10.1007/978-3-319-47437-3_72},
	abstract = {The main purpose of the ENRICHME European project is to develop a socially assistive robot that can help the elderly, adapt to their needs, and has a natural behavior. In this paper, we present some of the lessons learnt from the first interaction between the robot and two elderly people from one partner care facility (LACE Housing Ltd, UK). The robot interacted with the two participants for almost one hour. A tremendous amount of sensory data was recorded from the multi-sensory system (i.e., audio data, RGB-D data, thermal images, and the data from the skeleton tracker) for better understanding the interaction, the needs, the reactions of the users, and the context. This data was processed offline. Before the interaction between the two elderly residents and the robot, a demo was shown to all the residents of the facility. The reactions of the residents were positive and they found the robot useful. The first lessons learnt from this interaction between Kompaï robot and the elderly are reported. © Springer International Publishing AG 2016.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Agrigoroaie, R. and Ferland, F. and Tapus, A.},
	year = {2016},
	keywords = {1final\_includes, 1include\_search6},
	pages = {735--745},
	annote = {cited By 6},
	file = {Full Text:/home/brendan/Zotero/storage/KQ4ARGN4/Agrigoroaie et al. - 2016 - The ENRICHME project Lessons learnt from a first .pdf:application/pdf}
}

@inproceedings{de_schepper_towards_2020,
	title = {Towards robust human-robot mobile co-manipulation for tasks involving the handling of non-rigid materials using sensor-fused force-torque, and skeleton tracking data},
	volume = {97},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100858223&doi=10.1016%2fj.procir.2020.05.245&partnerID=40&md5=4e9871f651486975ff0a5fb49937b9c3},
	doi = {10.1016/j.procir.2020.05.245},
	abstract = {Over the past decades, robots have been extensively deployed in multiple industries. More recently, industrial robots have been taken out of their cages, being more present in dynamic, uncertain environments interacting in the vicinity of humans. Traditionally, robots have been mainly developed to perform pre-programmed tasks. However, some tasks are too complex or expensive to be performed by a robotic system alone. One of these hard tasks is the handling of large fibre sheets in composite part production. This paper presents a force-based approach towards human-robot co-manipulation of tasks involving the handling of non-rigid materials, such as composite fibres. Our approach fuses the data of a force-torque sensor and skeleton tracking data from a 2D camera to control the mobile manipulator in an intuitive manner, by using the intelligence of the operator as much as possible. By using this approach, tools such as path planning, high-level task planning, or modelling of objects to be manipulated are not essential to obtain the results. The overall approach is illustrated with a co-manipulation proof-of-concept experiment in which a mobile manipulator robot handles a flexible textile sheet together with a human operator. © 2020 The Author(s).},
	booktitle = {Procedia {CIRP}},
	author = {De Schepper, D. and Moyaers, B. and Schouterden, G. and Kellens, K. and Demeester, E.},
	year = {2020},
	keywords = {1final\_includes, 1include\_search6},
	pages = {325--330},
	annote = {cited By 0},
	file = {De Schepper et al. - 2020 - Towards robust human-robot mobile co-manipulation .pdf:/home/brendan/Zotero/storage/MM6T4N7Z/De Schepper et al. - 2020 - Towards robust human-robot mobile co-manipulation .pdf:application/pdf}
}

@inproceedings{monajjemi_uav_2015,
	title = {{UAV}, do you see me? {Establishing} mutual attention between an uninstrumented human and an outdoor {UAV} in flight},
	doi = {10.1109/IROS.2015.7353882},
	abstract = {We present the first demonstration of establishing mutual attention between an outdoor UAV in autonomous normal flight and an uninstrumented human user. We use the familiar periodic waving gesture as a signal to attract the UAV's attention. The UAV can discriminate this gesture from human walking and running that appears similarly periodic. Once a signaling person is observed and tracked, the UAV acknowledges that the user has its attention by hovering and performing a “wobble” behavior. Both parties are now ready for further interaction. The system works on-board the UAV using a single camera for input and is demonstrated working reliably in real-robot trials.},
	booktitle = {2015 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Monajjemi, Mani and Bruce, Jake and Sadat, Seyed Abbas and Wawerla, Jens and Vaughan, Richard},
	month = sep,
	year = {2015},
	keywords = {1final\_includes, 1include\_search6, Cameras, Computer vision, Feature extraction, Motion estimation, Real-time systems, Tracking},
	pages = {3614--3620},
	file = {Full Text:/home/brendan/Zotero/storage/BAJXULZW/Monajjemi et al. - 2015 - UAV, do you see me Establishing mutual attention .pdf:application/pdf}
}

@article{gemerek_video-guided_2019,
	title = {Video-guided {Camera} {Control} for {Target} {Tracking} and {Following}},
	volume = {51},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061302011&doi=10.1016%2fj.ifacol.2019.01.062&partnerID=40&md5=a185a40712e8c7d9efc0e8e15f7766c4},
	doi = {10.1016/j.ifacol.2019.01.062},
	abstract = {This paper considers the problem of controlling a nonholonomic mobile ground robot equipped with an onboard camera characterized by a bounded field-of-view, tasked with detecting and following a potentially moving human target using onboard computing and video processing in real time. Computer vision algorithms have been recently shown highly effective at object detection and classification in images obtained by vision sensors. Existing methods typically assume a stationary camera and/or use pre-recorded image sequences that do not provide a causal relationship with future images. The control method developed in this paper seeks to improve the performance of the computer vision algorithms, by planning the robot/camera trajectory relative to the moving target based on the desired size and position of the target in the image plane, without the need to estimate the target's range. The method is tested and validated using a highly realistic and interactive game programming environment, known as Unreal Engine™, that allows for closed-loop simulations of the robot-camera system. Results are further validated through physical experiments using a Clearpath™ Jackal robot equipped with a camera which is capable of following a human target for long time periods. Both simulation and experimental results show that the proposed vision-based controller is capable of stabilizing the target object size and position in the image plane for extended periods of time. © 2019},
	number = {34},
	journal = {IFAC-PapersOnLine},
	author = {Gemerek, J. and Ferrari, S. and Wang, B.H. and Campbell, M.E.},
	year = {2019},
	keywords = {1final\_includes, 1include\_search6},
	pages = {176--183},
	annote = {cited By 4},
	file = {Gemerek et al. - 2019 - Video-guided Camera Control for Target Tracking an.pdf:/home/brendan/Zotero/storage/5YUJGS3Q/Gemerek et al. - 2019 - Video-guided Camera Control for Target Tracking an.pdf:application/pdf}
}

@inproceedings{sun_visual_2019,
	title = {Visual {Hand} {Tracking} on {Depth} {Image} using 2-{D} {Matched} {Filter}},
	doi = {10.1109/UCET.2019.8881866},
	abstract = {Hand detection has been the central attention of human-machine interaction in recent researches. In order to track hand accurately, traditional methods mostly involve using machine learning and other available libraries, which requires a lot of computational resource on data collection and processing. This paper presents a method of hand detection and tracking using depth image which can be conveniently and manageably applied in practice without the huge data analysis. This method is based on the two-dimensional matched filter in image processing to precisely locate the hand position through several underlying codes, cooperated with a Delta robot. Compared with other approaches, this method is comprehensible and time-saving, especially for single specific gesture detection and tracking. Additionally, it is friendly-programmed and can be used on variable platforms such as MATLAB and Python. The experiments show that this method can do fast hand tracking and improve accuracy by selecting the proper hand template and can be directly used in the applications of human-machine interaction. In order to evaluate the performance of gesture tracking, a recorded video on depth image model is used to test theoretical design, and a delta parallel robot is used to follow the moving hand by the proposed algorithm, which demonstrates the feasibility in practice.},
	booktitle = {2019 {UK}/ {China} {Emerging} {Technologies} ({UCET})},
	author = {Sun, Yongdian and Liang, Xiangpeng and Fan, Hua and Imran, Muhammad and Heidari, Hadi},
	month = aug,
	year = {2019},
	keywords = {1final\_includes, 1include\_search6, Convolution, Delta robot, Depth Image, Hand Detection and Tracking, Human-machine interaction, Image edge detection, Man-machine systems, Matched Filter, Matched filters, Robot kinematics, Robot sensing systems},
	pages = {1--4},
	file = {Accepted Version:/home/brendan/Zotero/storage/6MJ3R56Q/Sun et al. - 2019 - Visual Hand Tracking on Depth Image using 2-D Matc.pdf:application/pdf}
}

@inproceedings{nagi_wisdom_2015,
	title = {Wisdom of the swarm for cooperative decision-making in human-swarm interaction},
	doi = {10.1109/ICRA.2015.7139432},
	abstract = {Human-swarm interaction (HSI) is a developing field of research in which the problem of gesture-based control has been attracting an increasing attention, being at the same time a natural form of interaction and an effective way to point and select individual or groups of robots in the swarm. Gesture-based interaction usually requires vision-based recognition and classification of the gesture from the swarm. At this aim, existing methods for cooperative sensing and recognition make use of distributed consensus algorithms, which include for instance averaging and frequency counting. In this work we present a distributed consensus protocol that allows robot swarms to learn efficiently gestures from online interactions with a human teacher. The protocol also facilitates the integration of different consensus algorithms. Experiments have been performed in emulation using on real data acquired by a swarm of robots. The results indicate that effectively exploiting the collective decision-making of the swarm is a viable way to rapidly achieve good learning performance.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Nagi, J. and Ngo, H. and Gambardella, L. M. and Di Caro, Gianni A.},
	month = may,
	year = {2015},
	note = {ISSN: 1050-4729},
	keywords = {1final\_includes, 1include\_search6, Accuracy, Multi-robot systems, Prediction algorithms, Protocols, Robot sensing systems},
	pages = {1802--1808},
	file = {Full Text:/home/brendan/Zotero/storage/P6SDVKQM/Nagi et al. - 2015 - Wisdom of the swarm for cooperative decision-makin.pdf:application/pdf}
}

@article{bouteraa_gesture-based_2017,
	title = {A gesture-based telemanipulation control for a robotic arm with biofeedback-based grasp},
	volume = {44},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028454676&doi=10.1108%2fIR-12-2016-0356&partnerID=40&md5=1103971d7038aa8b043806b99379ba65},
	doi = {10.1108/IR-12-2016-0356},
	abstract = {Purpose - The idea is to exploit the natural stability and performance of the human arm during movement, execution and manipulation. The purpose of this paper is to remotely control a handling robot with a low cost but effective solution. Design/methodology/approach - The developed approach is based on three different techniques to be able to ensure movement and pattern recognition of the operator's arm as well as an effective control of the object manipulation task. In the first, the methodology works on the kinect-based gesture recognition of the operator's arm. However, using only the vision-based approach for hand posture recognition cannot be the suitable solution mainly when the hand is occluded in such situations. The proposed approach supports the vision-based system by an electromyography (EMG)-based biofeedback system for posture recognition. Moreover, the novel approach appends to the vision system-based gesture control and the EMG-based posture recognition a force feedback to inform operator of the real grasping state. Findings - The main finding is to have a robust method able to gesture-based control a robot manipulator during movement, manipulation and grasp. The proposed approach uses a real-time gesture control technique based on a kinect camera that can provide the exact position of each joint of the operator's arm. The developed solution integrates also an EMG biofeedback and a force feedback in its control loop. In addition, the authors propose a high-friendly human-machine-interface (HMI) which allows user to control in real time a robotic arm. Robust trajectory tracking challenge has been solved by the implementation of the sliding mode controller. A fuzzy logic controller has been implemented to manage the grasping task based on the EMG signal. Experimental results have shown a high efficiency of the proposed approach. Research limitations/implications - There are some constraints when applying the proposed method, such as the sensibility of the desired trajectory generated by the human arm even in case of random and unwanted movements. This can damage the manipulated object during the teleoperation process. In this case, such operator skills are highly required. Practical implications - The developed control approach can be used in all applications, which require real-time human robot cooperation. Originality/value - The main advantage of the developed approach is that it benefits at the same time of three various techniques: EMG biofeedback, vision-based system and haptic feedback. In such situation, using only vision-based approaches mainly for the hand postures recognition is not effective. Therefore, the recognition should be based on the biofeedback naturally generated by the muscles responsible of each posture. Moreover, the use of force sensor in closed-loop control scheme without operator intervention is ineffective in the special cases in which the manipulated objects vary in a wide range with different metallic characteristics. Therefore, the use of human-in-the-loop technique can imitate the natural human postures in the grasping task. © Emerald Publishing Limited.},
	number = {5},
	journal = {Industrial Robot},
	author = {Bouteraa, Y. and Abdallah, I.B.},
	year = {2017},
	keywords = {1final\_includes, 1include\_search6, 1scopus},
	pages = {575--587},
	annote = {cited By 11},
	file = {Bouteraa and Abdallah - 2017 - A gesture-based telemanipulation control for a rob.pdf:/home/brendan/Zotero/storage/M4V3HJTD/Bouteraa and Abdallah - 2017 - A gesture-based telemanipulation control for a rob.pdf:application/pdf}
}

@inproceedings{zhao_intuitive_2016,
	title = {An intuitive human robot interface for tele-operation},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010073109&doi=10.1109%2fRCAR.2016.7784072&partnerID=40&md5=41abc1ba6a2959b085d650f78dd2591f},
	doi = {10.1109/RCAR.2016.7784072},
	abstract = {This paper proposed an intuitive human robot interface for real-time tele-operation, where human operator can operate the Baxter robot to implement complicated tasks in unstructured and uncertain environment intuitively and efficiently. Firstly, In this paper a new method for building an human robot interaction interface is proposed. In addition, workspace mapping between master and slave manipulator is a key problem of human robot interaction (HRI) when there are huge difference in size and structure of human and robot's manipulators. So, the workspace mapping method between human arm and Baxter Robot manipulator and the inverse kinematics approach for solving 7-DOF redundant manipulator by using Kinect sensor were also discussed in detail. In the end, an experiment was employed to validate the performance of proposed interface. © 2016 IEEE.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Real}-{Time} {Computing} and {Robotics}, {RCAR} 2016},
	author = {Zhao, L. and Liu, Y. and Wang, K. and Liang, P. and Li, R.},
	year = {2016},
	keywords = {1final\_includes, 1include\_search6, 1scopus},
	pages = {454--459},
	annote = {cited By 5},
	file = {Full Text:/home/brendan/Zotero/storage/F8CJSZVU/Zhao et al. - 2016 - An intuitive human robot interface for tele-operat.pdf:application/pdf}
}

@inproceedings{abiddin_development_2019,
	title = {Development of robot-human imitation program for telerehabilitation system},
	volume = {2018-September},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063132549&doi=10.1109%2fDeSE.2018.00045&partnerID=40&md5=d41faf27f0a34cdbd930146cc3a3b635},
	doi = {10.1109/DeSE.2018.00045},
	abstract = {This paper presents the development of a robothuman imitation program which will be used in a telerehabilitation system for children with disabilities. The purpose of this project is to provide an easier and more efficient way of providing a rehabilitation process for children with disabilities, especially for autistic children. The number of autistic patients needing rehabilitation is increasing rapidly and with the problem of shortage of professional therapist, this can be an alternative solution to assist the therapist during rehabilitation period. Therefore, robot-human imitation program is developed consisting of autonomous programmable humanoid robot NAO and Microsoft Kinect as the motionsensing input device. The imitation program is conducted by sets of instructions given to the patients to imitate certain pose done by humanoid robot NAO. Microsoft Kinect Sensor is used to determine whether the imitation follows the required criteria or not. Matlab software interacts with both humanoid robot NAO and Microsoft Kinect Sensor to perform the rehabilitation program. The number of trials needed to achieve the required imitation will be translated into points and these points will be uploaded into online database for record and evaluation process. © 2018 IEEE.},
	booktitle = {Proceedings - {International} {Conference} on {Developments} in {eSystems} {Engineering}, {DeSE}},
	author = {Abiddin, W.Z.B.W.Z. and Jailani, R. and Omar, A.R.},
	year = {2019},
	keywords = {1final\_includes, 1include\_search6, 1scopus},
	pages = {198--201},
	annote = {cited By 0},
	file = {Full Text:/home/brendan/Zotero/storage/F6B6AP6C/Abiddin et al. - 2019 - Development of robot-human imitation program for t.pdf:application/pdf}
}

@article{xiao_human-robot_2014,
	title = {Human-{Robot} {Interaction} by {Understanding} {Upper} {Body} {Gestures}},
	volume = {23},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905504222&doi=10.1162%2fPRES_a_00176&partnerID=40&md5=5c20fbf3b7c99d4982abb03a4d449dff},
	doi = {10.1162/PRES_a_00176},
	abstract = {In this paper, a human-robot interaction system based on a novel combination of sensors is proposed. It allows one person to interact with a humanoid social robot using natural body language. The robot understands the meaning of human upper body gestures and expresses itself by using a combination of body movements, facial expressions, and verbal language. A set of 12 upper body gestures is involved for communication. This set also includes gestures with human-object interactions. The gestures are characterized by head, arm, and hand posture information. The wearable Immersion CyberGlove II is employed to capture the hand posture. This information is combined with the head and arm posture captured from Microsoft Kinect. This is a new sensor solution for human-gesture capture. Based on the posture data from the CyberGlove II and Kinect, an effective and real-time human gesture recognition method is proposed. The gesture understanding approach based on an innovative combination of sensors is the main contribution of this paper. To verify the effectiveness of the proposed gesture recognition method, a human body gesture data set is built. The experimental results demonstrate that our approach can recognize the upper body gestures with high accuracy in real time. In addition, for robot motion generation and control, a novel online motion planning method is proposed. In order to generate appropriate dynamic motion, a quadratic programming (QP)-based dual-arms kinematic motion generation scheme is proposed, and a simplified recurrent neural network is employed to solve the QP problem. The integration of a handshake within the HRI system illustrates the effectiveness of the proposed online generation method. © 2014 by the Massachusetts Institute of Technology.},
	number = {2},
	journal = {Presence: Teleoperators and Virtual Environments},
	author = {Xiao, Y. and Zhang, Z. and Beck, A. and Yuan, J. and Thalmann, D.},
	year = {2014},
	keywords = {1final\_includes, 1include\_search6, 1scopus},
	pages = {133--154},
	annote = {cited By 47},
	file = {Full Text:/home/brendan/Zotero/storage/4J2SWLYY/Xiao et al. - 2014 - Human-Robot Interaction by Understanding Upper Bod.pdf:application/pdf}
}

@inproceedings{quintero_interactive_2014,
	title = {Interactive teleoperation interface for semi-autonomous control of robot arms},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902241902&doi=10.1109%2fCRV.2014.55&partnerID=40&md5=dbda4a1ed6309e9d357bbaa70fc436f4},
	doi = {10.1109/CRV.2014.55},
	abstract = {We propose and develop an interactive semi-autonomous control of robot arms. Our system controls two interactions: (1) A user can naturally control a robot arm by a direct linkage to the arm motion from the tracked human skeleton. (2) An autonomous image-based visual servoing routine can be triggered for precise positioning. Coarse motions are executed by human teleoperation and fine motions by image-based visual servoing. A successful application of our proposed interaction is presented for a WAM arm equipped with an eye-in-hand camera. © 2014 IEEE.},
	booktitle = {Proceedings - {Conference} on {Computer} and {Robot} {Vision}, {CRV} 2014},
	author = {Quintero, C.P. and Fomena, R.T. and Shademan, A. and Ramirez, O. and Jagersand, M.},
	year = {2014},
	keywords = {1final\_includes, 1include\_search6, 1scopus},
	pages = {357--363},
	annote = {cited By 2},
	file = {Full Text:/home/brendan/Zotero/storage/8CTLB8TR/Quintero et al. - 2014 - Interactive teleoperation interface for semi-auton.pdf:application/pdf}
}

@inproceedings{bai_kinect-based_2018,
	title = {Kinect-based hand tracking for first-person-perspective robotic arm teleoperation},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072337950&doi=10.1109%2fICInfA.2018.8812561&partnerID=40&md5=82b9b6f09d6c511286395359c4be85f9},
	doi = {10.1109/ICInfA.2018.8812561},
	abstract = {This paper deals with the man-machine interaction of robotic arm teleoperation by the developed Kinect and first-person-perspective follow-up technologies. Kinect is used to collect and preprocess the depth information to determine the hand position vector. With the help of the virtual robotic arm set up by Processing, the relationship between the hand and shoulder is mapped to the one between the robotic arm's chassis and its gripper, then it can be output to the Arduino lower computer, so that the corresponding motion of the robotic arm is controlled. The first-person-perspective follow-up obtains the position information of the head by the positioning chip fixed on the VR glasses. After data transmission and processing, the angles of pan-tilt platform, i.e., the pitch and yaw angle of the camera, are changed to keep the orientation of the camera consistent with the user's eyes. The VR glasses provide real-time view of the first-person perspective. Compared with the existing ones, the proposed tele-operating robotic arm enhanced by Kinect and binocular camera has the advantages of better immersion experience and intuitive operation. © 2018 IEEE.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Information} and {Automation}, {ICIA} 2018},
	author = {Bai, X. and Li, C. and Chen, K. and Feng, Y. and Yu, Z. and Xu, M.},
	year = {2018},
	keywords = {1final\_includes, 1include\_search6, 1scopus},
	pages = {684--691},
	annote = {cited By 0},
	file = {Full Text:/home/brendan/Zotero/storage/ASXSUJEL/Bai et al. - 2018 - Kinect-based hand tracking for first-person-perspe.pdf:application/pdf}
}

@inproceedings{mohammad_tele-operation_2013,
	title = {Tele-operation of robot using gestures},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892557552&doi=10.1109%2fAMS.2013.15&partnerID=40&md5=60bc54d9f2d71a872ba87f8967c4c93d},
	doi = {10.1109/AMS.2013.15},
	abstract = {This paper proposes an idea to develop a tele-operated humanoid robot, which has capability to perform assigned task at a long distance based on the gestures given by the human. Tele-operation of robots is the field that bridges the transition between manual control and complete autonomous control of robots. Controlling robots at anyplace in the world by accessing the sensory data from the robot gives the humans the capability to perform several tasks. To start with this idea a framework is implemented in which the user can send commands to control a humanoid from anywhere in the world over Transmission control protocol. An extension is being developed to control the robot remotely using gestures. A communication channel to transfer image feedback data from the robot via a TCP socket is also being developed. Users just need to use the most natural body gestures to interact with the robot. Microsoft Kinect is applied in this system to recognize different body gestures and generate visual Human-Robot interaction interface, then the controlling signals of different body gesture modules are sent to the humanoid robot thorough a infrared transmitter, which can stimulate the robot to complete tasks. © 2013 IEEE.},
	booktitle = {Proceedings - {Asia} {Modelling} {Symposium} 2013: 7th {Asia} {International} {Conference} on {Mathematical} {Modelling} and {Computer} {Simulation}, {AMS} 2013},
	author = {Mohammad, F. and Sudini, K.R. and Puligilla, V. and Kapula, P.R.},
	year = {2013},
	keywords = {1final\_includes, 1include\_search6, 1scopus},
	pages = {67--71},
	annote = {cited By 1},
	file = {Full Text:/home/brendan/Zotero/storage/DCYZ6NHE/Mohammad et al. - 2013 - Tele-operation of robot using gestures.pdf:application/pdf}
}

@inproceedings{sripada_teleoperation_2019,
	title = {Teleoperation of a {Humanoid} {Robot} with {Motion} {Imitation} and {Legged} {Locomotion}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061498072&doi=10.1109%2fICARM.2018.8610719&partnerID=40&md5=f79a9664d7b164a2ce70f30f56e2b919},
	doi = {10.1109/ICARM.2018.8610719},
	abstract = {This work presents a teleoperated humanoid robot system that can imitate human motions, walk and turn. To capture human motions, a Microsoft Kinect Depth sensor is used. Unlike the cumbersome motion capture suits, the sensor makes the system more comfortable to interact with. The skeleton data is extracted from the Kinect which is processed to generate the robot's joint angles. Robot Operating System (ROS) is used for the communication between the various parts of the code to achieve minimal latency. The lower body motions of the user are captured and processed and used to make the robot walk forward, backward and to make it turn right or left thus enabling a walking teleoperated humanoid robot system. The system has been tested on a 20DOF, 50cm tall humanoid robot and the results are produced. © 2018 IEEE.},
	booktitle = {{ICARM} 2018 - 2018 3rd {International} {Conference} on {Advanced} {Robotics} and {Mechatronics}},
	author = {Sripada, A. and Asokan, H. and Warrier, A. and Kapoor, A. and Gaur, H. and Patel, R. and Sridhar, R.},
	year = {2019},
	keywords = {1final\_includes, 1include\_search6, 1scopus},
	pages = {375--379},
	annote = {cited By 4},
	file = {Full Text:/home/brendan/Zotero/storage/D4YPB34E/Sripada et al. - 2019 - Teleoperation of a Humanoid Robot with Motion Imit.pdf:application/pdf}
}
@article{zhang_gesture-based_2019,
	title = {Gesture-based human-robot interface for dual-robot with hybrid sensors},
	volume = {46},
	issn = {0143-991X, 0143-991X},
	url = {https://www.emerald.com/insight/content/doi/10.1108/IR-11-2018-0245/full/html},
	doi = {10.1108/IR-11-2018-0245},
	abstract = {Purpose – The purpose of this paper is the research of a novel gesture-based dual-robot collaborative interaction interface, which achieves the gesture recognition when both hands overlap. This paper designs a hybrid-sensor gesture recognition platform to detect the both-hand data for dual-robot control.},
	language = {en},
	number = {6},
	urldate = {2021-06-22},
	journal = {Industrial Robot: the international journal of robotics research and application},
	author = {Zhang, Bo and Du, Guanglong and Shen, Wenming and Li, Fang},
	month = oct,
	year = {2019},
	keywords = {1final\_includes, 1include\_search6},
	pages = {800--811},
	file = {Zhang et al. - 2019 - Gesture-based human-robot interface for dual-robot.pdf:/home/brendan/Zotero/storage/LWXB5U7Q/Zhang et al. - 2019 - Gesture-based human-robot interface for dual-robot.pdf:application/pdf}
}