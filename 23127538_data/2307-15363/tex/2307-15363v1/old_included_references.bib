@InProceedings{10.1007/978-3-030-62056-1_44,
author="Khavas, Zahra Rezaei
and Ahmadzadeh, S. Reza
and Robinette, Paul",
editor="Wagner, Alan R.
and Feil-Seifer, David
and Haring, Kerstin S.
and Rossi, Silvia
and Williams, Thomas
and He, Hongsheng
and Sam Ge, Shuzhi",
title="Modeling Trust in Human-Robot Interaction: A Survey",
booktitle="Social Robotics",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="529--541",
abstract="As the autonomy and capabilities of robotic systems increase, they are expected to play the role of teammates rather than tools and interact with human collaborators in a more realistic manner, creating a more human-like relationship. Given the impact of trust observed in human-robot interaction (HRI), appropriate trust in robotic collaborators is one of the leading factors influencing the performance of human-robot interaction. Team performance can be diminished if people do not trust robots appropriately by disusing or misusing them based on limited experience. Therefore, trust in HRI needs to be calibrated properly, rather than maximized, to let the formation of an appropriate level of trust in human collaborators. For trust calibration in HRI, trust needs to be modeled first. There are many reviews on factors affecting trust in HRI[22], however, as there are no reviews concentrated on different trust models, in this paper, we review different techniques and methods for trust modeling in HRI. We also present a list of potential directions for further research and some challenges that need to be addressed in future work on human-robot trust modeling.",
isbn="978-3-030-62056-1"
}

@inproceedings{monajjemi_integrating_2014,
	title = {Integrating {Multi}-{Modal} {Interfaces} to {Command} {UAVs} [{Video} {Abstract}]},
	abstract = {We present an integrated human-robot interaction system that enables a user to select and command a team of two Unmanned Aerial Vehicles (UAV) using voice, touch, face engagement and hand gestures. This system integrates multiple human [multi]-robot interaction interfaces as well as a navigation and mapping algorithm in a coherent semirealistic scenario. The task of the UAVs is to explore and map a simulated Mars environment. To initiate a mission, the user needs to select a robot. To do this, We used the“Touch-To-Name”selection and naming interface [3]. In this method, the user first announces the desired number of robot(s) (e.g “You” or “You Two”), then gently moves intended robot(s) iteratively. Robots compare their accelerometer readings over Wi-Fi to agree on which one is selected. Once selected, the user names the selected robot using verbal commands (e.g “You are Green”). These names are then used to command the robots (e.g., “Green Takeoff”) [4]. Here, we use this interface with maximum group size set to one. After taking off and while hovering, robot looks for human faces in its camera feed. When user's face is detected, the robot continuously controls its altitude and heading direction to face the user. A hand wave gesture (left or right) assigns an exploration task to the robot in the indicated direction. We used the method described in [2] for face tracking and gesture recognition. While exploring, each robot performs vision-based Simultaneous Localization and Mapping (SLAM) using their onboard monocular camera [1]. We used the“Feature-rich path planning algorithm” introduced in [5] to robustly navigate a UAV while exploring an unknown environment. To terminate the mission, the user commands each robot to come back home (e.g “Green come back”). To come back, robots use the same algorithm to plan a feature-rich path to their takeoff position. Finally, The user asks robots to land. (e.g., “Green land”).The system provides two types of feedback to the user during interaction sessions and mission execution. Robots change the color and blinking pattern of their LED lights to inform the user about their state (e.g., “tracking user's face”, “exploring” or “being idle”). In addition, a text-to-speech (TTS) engine provides verbal feedback to the user whenever a robot's state changes. As an example, when the Green robot is asked by the user to comeback, it acknowledges by saying“Green is coming back”. The TTS is embedded within a general purpose web-based robot monitoring dashboard.We used Parrot AR-Drone 2.0 quadrocopter as UAV platform in our system. All described software components run off-board on two commodity Intel Core i7 notebooks (one dedicated to each robot). The computers are connected to UAVs via Wi-Fi connection.The video shows a complete run-through of a two robot exploration mission in which the HRI worked perfectly.},
	booktitle = {2014 9th {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction} ({HRI})},
	author = {Monajjemi, V. and Pourmehr, S. and Sadat, S. A. and Zhan, F. and Wawerla, J. and Mori, G. and Vaughan, R.},
	month = mar,
	year = {2014},
	note = {ISSN: 2167-2121},
	keywords = {robot vision, Robots, path planning, mobile robots, human-robot interaction, Task analysis, human faces, Human-robot interaction, Face, human-robot interaction system, Navigation, SLAM (robots), gesture recognition, hand gestures, cameras, navigation, interactive systems, face tracking, multiple human-robot interaction, verbal commands, autonomous aerial vehicles, unmanned aerial vehicles, UAV, remotely operated vehicles, face engagement, general purpose web-based robot monitoring dashboard, Green robot, mapping algorithm, multimodal interfaces, naming interface, robot exploration mission, text-to-speech engine, Touch-To-Name selection, Unmanned aerial vehicles, Wireless fidelity, 1bt\_include},
	pages = {106--106},
	file = {Monajjemi et al. - 2014 - Integrating Multi-Modal Interfaces to Command UAVs.pdf:/home/brendan/Zotero/storage/MHV4QDAN/Monajjemi et al. - 2014 - Integrating Multi-Modal Interfaces to Command UAVs.pdf:application/pdf}
}

@inproceedings{ali_human_2013,
	title = {Human tracking by a mobile robot using {3D} features},
	doi = {10.1109/ROBIO.2013.6739841},
	abstract = {Detection and Tracking of human being is a very important problem in Computer Vision. Human robot interaction is a very essential need for service robots where robots are required to detect and track human beings in order to provide the required service. In this paper we present an improved novel approach for tracking a target person in crowded environment. We used multi-sensor data fusion approach by combining the data of stereo camera and laser rangefinder (LRF) to perform human tracking. The system gathers the features of human upper body, face and legs in the target person selection phase and then the robot will start following the target person. Camera is used for upper body and face detection while laser rangefinder is used for gathering legs data. Template matching and triangulation is done in order to get the dimensions of upper body and face. Target person tracking is done using Cam shift tracker. Thus our method presents a novel approach that uses all these techniques to track a target person in a crowded environment.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Robotics} and {Biomimetics} ({ROBIO})},
	author = {Ali, B. and Qureshi, A. H. and Iqbal, K. F. and Ayaz, Y. and Gilani, S. O. and Jamil, M. and Muhammad, N. and Ahmed, F. and Muhammad, M. S. and Kim, W. and Ra, M.},
	month = dec,
	year = {2013},
	keywords = {robot vision, Legged locomotion, mobile robots, human-robot interaction, Service robots, Cameras, computer vision, face recognition, object detection, Robot vision systems, object tracking, Face, mobile robot, service robots, human robot interaction, service robot, camera, face detection, sensor fusion, LRF, human detection, image matching, template matching, Target tracking, target tracking, stereo image processing, human tracking, stereo camera, Lasers, 3D features, body detection, Cam shift, cam shift tracker, Haar classifiers, laser rangefinder, leg detection, multisensor data fusion, target person tracking, triangulation, upper body detection, 1bt\_include},
	pages = {2464--2469},
	file = {Ali et al. - 2013 - Human tracking by a mobile robot using 3D features.pdf:/home/brendan/Zotero/storage/K9DJ5F5A/Ali et al. - 2013 - Human tracking by a mobile robot using 3D features.pdf:application/pdf}
}

@inproceedings{ali_human_2013-1,
	title = {Human detection and following by a mobile robot using {3D} features},
	doi = {10.1109/ICMA.2013.6618174},
	abstract = {Human-robot interaction is one of the most basic requirements for service robots. In order to provide the desired service, these robots are required to detect and track human beings in the environment. This paper presents a novel approach for classifying a target person in a crowded environment. The system used the approaches for human detection and following by implementing the multi-sensor data fusion technique using stereo camera and laser range finder (LRF). Our system tracks human being by gathering features of human upper body and face in 3D space from stereo camera and uses laser rangefinder to get legs data. Using these data our system classifies the target person from other human beings in the environment. We used Haar cascade classifiers for the detection of upper body and face, and used stereo camera for getting dimensions in 3D space. The approach for gathering legs data is based on the recognition of legs pattern extracted from laser scan. Tracking of target person is done using Cam Shift theorem. Using all these techniques we present a novel approach for target person classification and tracking. Our approach is feasible for mobile robots with an identical device arrangement.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Mechatronics} and {Automation}},
	author = {Ali, B. and Iqbal, K. F. and Ayaz, Y. and Muhammad, N.},
	month = aug,
	year = {2013},
	note = {ISSN: 2152-744X},
	keywords = {Legged locomotion, mobile robots, image sensors, human-robot interaction, image classification, Service robots, feature extraction, Feature extraction, Cameras, object detection, Face, mobile robot, service robots, cameras, face detection, sensor fusion, laser range finder, laser ranging, LRF, Target tracking, target tracking, stereo image processing, Haar cascade classifiers, 3D space, stereo camera, Lasers, 3D features, body detection, laser rangefinder, leg detection, target person tracking, upper body detection, Haar transforms, Cam Shift theorem, crowded environment, human being detection, human being tracking, human upper body features, laser scan, leg pattern recognition, multisensor data fusion technique, target person classification, 1bt\_include},
	pages = {1714--1719},
	file = {Ali et al. - 2013 - Human detection and following by a mobile robot us.pdf:/home/brendan/Zotero/storage/K4BBZZ9V/Ali et al. - 2013 - Human detection and following by a mobile robot us.pdf:application/pdf}
}

@inproceedings{bdiwi_handing-over_2013,
	title = {Handing-over model-free objects to human hand with the help of vision/force robot control},
	doi = {10.1109/SSD.2013.6564138},
	abstract = {Handing-over objects from robot to humans is essential step to perform different tasks especially those requiring physical interaction between the robot and the human, e.g. service robots can help elderly, blind and disabled people or human-robot teamwork could work together in factories. This work will propose a new robot system which combines visual servoing and force control in order to hand over model-free objects from undefined place to human hand. This work will present: 1. vision algorithms which help the robot system to detect and to segment the objects without any information about their model. 2. The possibility for visual tracking of human hand with the help of Kinect camera. 3. The importance of fusion vision and force control in order to ensure the safety during the human-robot interaction. 4. How the robot will deliver the objects to the human hand with the help of vision and force control. This work will be supported with experimental results.},
	booktitle = {10th {International} {Multi}-{Conferences} on {Systems}, {Signals} {Devices} 2013 ({SSD13})},
	author = {Bdiwi, M. and Suchý, J. and Winkler, A.},
	month = mar,
	year = {2013},
	keywords = {robot vision, Force control, Force, force control, human-robot interaction, Robot kinematics, Feature extraction, Cameras, Human-robot interaction, object tracking, visual servoing, service robots, cameras, image processing, Image segmentation, visual tracking, Kinect camera, robot system, human hand, dexterous manipulators, object segmentation, vision algorithms, force robot control, fusion vision, human-robot physical interaction, human-robot teamwork, model-free objects, 1bt\_include, 1vision},
	pages = {1--6},
	file = {Bdiwi et al. - 2013 - Handing-over model-free objects to human hand with.pdf:/home/brendan/Zotero/storage/H4ZAP5LZ/Bdiwi et al. - 2013 - Handing-over model-free objects to human hand with.pdf:application/pdf}
}

@inproceedings{zhang_egocentric_2013,
	title = {An egocentric vision based assistive co-robot},
	doi = {10.1109/ICORR.2013.6650473},
	abstract = {We present the prototype of an egocentric vision based assistive co-robot system. In this co-robot system, the user is wearing a pair of glasses with a forward looking camera, and is actively engaged in the control loop of the robot in navigational tasks. The egocentric vision glasses serve for two purposes. First, it serves as a source of visual input to request the robot to find a certain object in the environment. Second, the motion patterns computed from the egocentric video associated with a specific set of head movements are exploited to guide the robot to find the object. These are especially helpful for quadriplegic individuals who do not have needed hand functionality for interaction and control with other modalities (e.g., joystick). In our co-robot system, when the robot does not fulfill the object finding task in a pre-specified time window, it would actively solicit user controls for guidance. Then the users can use the egocentric vision based gesture interface to orient the robot towards the direction of the object. After that the robot will automatically navigate towards the object until it finds it. Our experiments validated the efficacy of the closed-loop design to engage the human in the loop.},
	booktitle = {2013 {IEEE} 13th {International} {Conference} on {Rehabilitation} {Robotics} ({ICORR})},
	author = {Zhang, J. and Zhuang, L. and Wang, Y. and Zhou, Y. and Meng, Y. and Hua, G.},
	month = jun,
	year = {2013},
	note = {ISSN: 1945-7901},
	keywords = {closed loop systems, robot vision, control system synthesis, path planning, image motion analysis, Robot kinematics, medical robotics, Cameras, Robot vision systems, gesture recognition, Humans, Robotics, cameras, closed-loop design, egocentric vision based assistive co-robot system, egocentric vision based gesture interface, forward looking camera, Glass, navigational tasks, user controls, Image Processing, active learning, Algorithms, co-robot, Computer-Assisted, control loop, Eyeglasses, glasses, head movements, Histograms, human in the loop, Man-Machine Systems, motion patterns, quadriplegic individuals, Self-Help Devices, visual input, Visual Perception, 1bt\_include, 1vision},
	pages = {1--7},
	file = {Zhang et al. - 2013 - An egocentric vision based assistive co-robot.pdf:/home/brendan/Zotero/storage/IINULF47/Zhang et al. - 2013 - An egocentric vision based assistive co-robot.pdf:application/pdf}
}

@inproceedings{fahn_real-time_2010,
	title = {Real-time face tracking techniques used for the interaction between humans and robots},
	doi = {10.1109/ICIEA.2010.5514736},
	abstract = {Owing to the demand of more efficient and friendly human-computer interfaces, the researches on face processing have been rapidly grown in recent years. In addition to offering some kinds of service for human being, one of the most important characteristics of a favorable system is to autonomously interact with people. In view of the above-mentioned facts, an automatic real-time face tracking system installed on a person following robot is presented in this paper. In the face tracking procedure, we employ an improved particle filter to dynamically locate a human face. Since we have considered the hair color information of a human head, the particle filter will keep tracking even if the person is back to the sight of a camera. We further adopt both the motion and color cues as the features to alleviate the influence of the background as low as possible. According to the position of the human face in an image, we issue a series of commands (moving forward, turning left or turning right) to drive the motors of wheels on a robot, and judge the distance between the robot and a person with the aid of three ultrasonic sensors to issue a set of commands (stop or turn backward) until the robot follows to a suitable distance from the person. Experimental results reveal that the face tracking rate is more than 95\% in general situations and over 88\% when the face suffers from temporal occlusion. Besides this, the efficiency of system execution is very satisfactory.},
	booktitle = {2010 5th {IEEE} {Conference} on {Industrial} {Electronics} and {Applications}},
	author = {Fahn, C. and Lin, Y.},
	month = jun,
	year = {2010},
	note = {ISSN: 2158-2297},
	keywords = {robot vision, Robot sensing systems, human-robot interaction, face recognition, Human robot interaction, Human-robot interaction, Face, Mobile robots, particle filtering (numerical methods), Real time systems, Robotics and automation, face tracking, Turning, color cue, Hair, human-computer interfaces, motion cue, particle filter, Particle filters, Particle tracking, real-time face tracking technique, ultrasonic sensors, 1bt\_include},
	pages = {12--17},
	annote = {cited By 1},
	file = {IEEE Xplore Full Text PDF:/home/brendan/Zotero/storage/2YNUV2MA/Fahn and Lin - 2010 - Real-time face tracking techniques used for the in.pdf:application/pdf;Fahn and Lin - 2010 - Real-time face tracking techniques used for the in.pdf:/home/brendan/Zotero/storage/HR2IV2B7/Fahn and Lin - 2010 - Real-time face tracking techniques used for the in.pdf:application/pdf}
}

@article{scheggi_human-robot_2014,
	title = {Human-{Robot} {Formation} {Control} via {Visual} and {Vibrotactile} {Haptic} {Feedback}},
	volume = {7},
	issn = {2329-4051},
	doi = {10.1109/TOH.2014.2332173},
	abstract = {In this paper we present a new visuo-haptic interaction mechanism for human-robot formation control. The formation setup consists of a human leader and multiple follower robots. The mobile robots are equipped only with RGB-D cameras, and they should maintain a desired distance and orientation to the leader at all times. Mechanical limitations common to all the robots limit the possible trajectories that the human can take. In this regard, vibrotactile feedback provided by a haptic bracelet guides the human along trajectories that are feasible for the team by warning her/him when the formation constraints are being violated. Psychophysical tests on the bracelet together with real-world experiments conducted with a team of Pioneer robots show the effectiveness of the proposed visuo-haptic paradigm for the coordination of mixed human-robot teams.},
	number = {4},
	journal = {IEEE Transactions on Haptics},
	author = {Scheggi, S. and Morbidi, F. and Prattichizzo, D.},
	month = oct,
	year = {2014},
	keywords = {robot vision, Humanoid robots, mobile robots, trajectory control, human-robot interaction, Robot kinematics, multi-robot systems, Cameras, Robot vision systems, Mobile robots, Humans, image colour analysis, Robotics, haptic interfaces, Haptic interfaces, Algorithms, vibrotactile feedback, RGB-D cameras, autonomous vehicles, Adult, Male, design for wearability, Feedback, Female, follower robots, formation constraints, haptic bracelet, Haptic I/O, Human Engineering, human leader, human-robot formation control, human-robot team, mechanical limitations, Middle Aged, mixed human-robot teams, Photic Stimulation, Physiological, Pioneer robots, psychophysical tests, Touch, trajectories, Vibration, vibrotactile haptic feedback, visual haptic feedback, visuo-haptic interaction mechanism, visuo-haptic paradigm, Young Adult, 1bt\_include},
	pages = {499--511},
	file = {Scheggi et al. - 2014 - Human-Robot Formation Control via Visual and Vibro.pdf:/home/brendan/Zotero/storage/YJI88I6C/Scheggi et al. - 2014 - Human-Robot Formation Control via Visual and Vibro.pdf:application/pdf}
}

@inproceedings{tseng_multi-human_2014,
	title = {Multi-human spatial social pattern understanding for a multi-modal robot through nonverbal social signals},
	doi = {10.1109/ROMAN.2014.6926307},
	abstract = {For service robots to be able to enter a multi-human office environment, it is important to find a group of human users' social patterns and then to provide a proper service to them in time. Usually, human users' social patterns are represented in terms of nonverbal social signals. In this paper, a new integrated approach on recognizing multi-human social signals is proposed. Specifically, the nonverbal social signals are detected by a laser range finder and a RGB-D camera and are processed to find the multi-human (spatial) social patterns. Those recognized patterns are then applied to human-to-human, human-to-robot or multi-human-to-robot interactive formation. Experimental results shows that our robot successfully recognizes the aforementioned users' social patterns followed by appropriate services.},
	booktitle = {The 23rd {IEEE} {International} {Symposium} on {Robot} and {Human} {Interactive} {Communication}},
	author = {Tseng, S. and Hsu, Y. and Chiang, Y. and Wu, T. and Fu, L.},
	month = aug,
	year = {2014},
	note = {ISSN: 1944-9437},
	keywords = {robot vision, image sensors, Robot sensing systems, human-robot interaction, Face, image colour analysis, RGB-D camera, service robots, Skeleton, laser range finder, laser ranging, Fuses, human-to-human interactive formation, human-to-robot interactive formation, Lasers, multihuman office environment, multihuman social signals, multihuman spatial social pattern understanding, multihuman-to-robot interactive formation, multimodal robot, nonverbal social signals, 1bt\_include},
	pages = {531--536},
	file = {06926307.pdf:/home/brendan/Zotero/storage/TQMSKMD5/06926307.pdf:application/pdf}
}

@inproceedings{zhang_automating_2018,
	title = {Automating {Robotic} {Furniture} with {A} {Collaborative} {Vision}-based {Sensing} {Scheme}},
	doi = {10.1109/ROMAN.2018.8525783},
	abstract = {Automating teleoperated robots is an essential task for transforming human-robot interaction design into practical applications. In this paper, we present a Collaborative Vision-based Sensing Scheme (CVSS) for automating mobile robotic furniture in the household environment. Using multiple cameras to perceive users' spatial information and capture their body postures respectively, our sensing scheme can provide formerly teleoperated robots with sufficient situational awareness of the users' surroundings and enable them to understand the interactive willingness of humans. As an application instance, we introduce the design of a furniture-type robot, called Automan, whose prototype is a teleoperated mechanical ottoman reported in [1]. Utilizing CVSS, we enable Automan the similar functions as teleoperated ottoman to offer and withdraw services for humans. To evaluate our automation method, we conducted the subjective experiments with 20 participants to verify the effectiveness of it in comparison with the teleoperated ottomans in terms of interactive experience. And the result of paired samples t test indicates that the participants can't significantly distinguish whether the interactive ottoman is autonomous or teleoperated (p≫0.05). Furthermore, we also explored and analyzed users' satisfaction for different behavior styles between autonomous and teleoperated ottomans.},
	booktitle = {2018 27th {IEEE} {International} {Symposium} on {Robot} and {Human} {Interactive} {Communication} ({RO}-{MAN})},
	author = {Zhang, Z. and Chen, Z. and Li, W.},
	month = aug,
	year = {2018},
	note = {ISSN: 1944-9437},
	keywords = {robot vision, mobile robots, human-robot interaction, Cameras, Robot vision systems, human-robot interaction design, Visualization, service robots, cameras, telerobotics, Shoulder, application instance, Automan, automating mobile robotic furniture, automation method, collaborative vision-based sensing scheme, furniture, furniture-type robot, household environment, interactive ottoman, interactive willingness, teleoperated mechanical ottoman, teleoperated ottomans, teleoperated robots, users satisfaction, users spatial information, utilizing CVSS, 1bt\_include, 1vision},
	pages = {719--725},
	file = {Zhang et al. - 2018 - Automating Robotic Furniture with A Collaborative .pdf:/home/brendan/Zotero/storage/D8NCZILA/Zhang et al. - 2018 - Automating Robotic Furniture with A Collaborative .pdf:application/pdf}
}

@inproceedings{cheng_multiple-robot_2013,
	title = {A multiple-robot system for home service},
	doi = {10.1109/CACS.2013.6734111},
	abstract = {This paper focuses on applying visual servoing techniques for a multiple-robot system to cooperate. Each individual robot has its own video camera and the images captured are used both for the feedback control and for mutual interaction. Furthermore, we utilize a centralized computing kernel to coordinate total information to speed up the communication between robots and simplify the connection. The robot tasks are achieved mainly through computer vision. For example, the security monitoring task uses human face image detection, the service of providing water uses images to make sure proper robot and human interaction, and the system uses images for robots to cooperate for the floor surface exploring and object transporting.},
	booktitle = {2013 {CACS} {International} {Automatic} {Control} {Conference} ({CACS})},
	author = {Cheng, C. and Zhuo, Y. and Kuo, G.},
	month = dec,
	year = {2013},
	keywords = {feedback, path planning, mobile robots, Robot kinematics, Service robots, multi-robot systems, Cameras, computer vision, Robot vision systems, Mobile robots, visual servoing, service robots, Mobile handsets, feedback control, floor surface exploring, home service, human face image detection, multiple-robot system, mutual interaction, object transporting, security monitoring task, visual servoing techniques, 1bt\_include},
	pages = {79--84},
	file = {Cheng et al. - 2013 - A multiple-robot system for home service.pdf:/home/brendan/Zotero/storage/83YBSEJT/Cheng et al. - 2013 - A multiple-robot system for home service.pdf:application/pdf}
}

@inproceedings{bellarbi_social_2016,
	title = {A social planning and navigation for tour-guide robot in human environment},
	doi = {10.1109/ICMIC.2016.7804186},
	abstract = {The biggest challenges in modern robotics is service robots, which are able to execute many tasks for humans in their presence. This perspective naturally causes the problem of social navigation of mobile robots as well as human-robot interaction. In this article, we present the implementation of a navigation method on a mobile robot in indoor environment; we detail the algorithm and the implementation of human-robot interaction social rules. The principle is to define a goal for the robot, which plans a path that drives it to its goal, choosing the shortest, the smoothest, and the safety way, avoiding socially the dynamic obstacles. For this purpose, the robot uses a laser sensor for building environments maps, localization, and detection of new obstacles, and an RGB-D camera (Kinect sensor) for social avoidance.},
	booktitle = {2016 8th {International} {Conference} on {Modelling}, {Identification} and {Control} ({ICMIC})},
	author = {Bellarbi, A. and Kahlouche, S. and Achour, N. and Ouadah, N.},
	month = nov,
	year = {2016},
	keywords = {robot vision, path planning, mobile robots, Planning, Trajectory, image sensors, Robot sensing systems, human-robot interaction, Collision avoidance, Mobile robots, Navigation, SLAM (robots), collision avoidance, RGB-D camera, service robots, Kinect sensor, indoor environment, indoor navigation, localization, environment maps, human environment, human-robot interaction social rules, laser sensor, mobile robotic, obstacle detection, obstacles avoidance, sensor placement, social navigation, social planning, tour-guide robot, 1bt\_include},
	pages = {622--627},
	file = {Bellarbi et al. - 2016 - A social planning and navigation for tour-guide ro.pdf:/home/brendan/Zotero/storage/E7DFI4X3/Bellarbi et al. - 2016 - A social planning and navigation for tour-guide ro.pdf:application/pdf}
}

@inproceedings{luo_human_2010,
	title = {Human tracking and following using sound source localization for multisensor based mobile assistive companion robot},
	doi = {10.1109/IECON.2010.5675451},
	abstract = {We have developed a mobile assistive companion robot by combining a vision sensor and a laser range sensor to track and follow a target person. Although it works well in most cases, robot might lose target occasionally due to external factors such as bad view conditions or unconstructed environments. To solve this problem, we develop a speech system and sound source detection system to achieve sound source localization and speech interaction between users and robot. When robot gets lost during the tracking and following process, it will inform the user, and wait for a clapping sound from the user to re-localize user's location. The proposed method integrates human robot interaction based on speech system and sound source detection to retrieve target person's location when robot get lost, which is significantly different from other solutions which use motion model or Bayesian filters such as Kalman filters or particle filters to estimate user's location when robot is losing target. In this paper, we have demonstrated the success of the proposed method experimentally.},
	booktitle = {{IECON} 2010 - 36th {Annual} {Conference} on {IEEE} {Industrial} {Electronics} {Society}},
	author = {Luo, R. C. and Huang, C. H. and Lin, T. T.},
	month = nov,
	year = {2010},
	note = {ISSN: 1553-572X},
	keywords = {robot vision, path planning, mobile robots, image sensors, Robot sensing systems, human-robot interaction, Robot kinematics, Feature extraction, Humans, speech recognition, human robot interaction, sensor fusion, laser ranging, Speech, vision sensor, target tracking, Microphones, sound source localization, filtering theory, laser range sensor, mobile assistive companion robot, sound source detection system, speech system, 1bt\_include},
	pages = {1552--1557},
	annote = {cited By 10},
	file = {Luo et al. - 2010 - Human tracking and following using sound source lo.pdf:/home/brendan/Zotero/storage/2IZW743Y/Luo et al. - 2010 - Human tracking and following using sound source lo.pdf:application/pdf}
}

@inproceedings{monajjemi_hri_2013,
	title = {{HRI} in the sky: {Creating} and commanding teams of {UAVs} with a vision-mediated gestural interface},
	doi = {10.1109/IROS.2013.6696415},
	abstract = {Extending our previous work in real-time vision-based Human Robot Interaction (HRI) with multi-robot systems, we present the first example of creating, modifying and commanding teams of UAVs by an uninstrumented human. To create a team the user focuses attention on an individual robot by simply looking at it, then adds or removes it from the current team with a motion-based hand gesture. Another gesture commands the entire team to begin task execution. Robots communicate among themselves by wireless network to ensure that no more than one robot is focused, and so that the whole team agrees that it has been commanded. Since robots can be added and removed from the team, the system is robust to incorrect additions. A series of trials with two and three very low-cost UAVs and off-board processing demonstrates the practicality of our approach.},
	booktitle = {2013 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	author = {Monajjemi, V. M. and Wawerla, J. and Vaughan, R. and Mori, G.},
	month = nov,
	year = {2013},
	note = {ISSN: 2153-0866},
	keywords = {Robot sensing systems, human-robot interaction, Robot kinematics, HRI, multi-robot systems, Cameras, Face, gesture recognition, human robot interaction, Optical imaging, autonomous aerial vehicles, UAV, wireless network, multirobot systems, Aircraft, motion-based hand gesture, uninstrumented human, vision-mediated gestural interface, 1bt\_include, 1vision},
	pages = {617--623},
	file = {Monajjemi et al. - 2013 - HRI in the sky Creating and commanding teams of U.pdf:/home/brendan/Zotero/storage/3RZL7H5I/Monajjemi et al. - 2013 - HRI in the sky Creating and commanding teams of U.pdf:application/pdf}
}

@inproceedings{nguyen_using_2015,
	title = {Using {Hand} {Postures} for {Interacting} with {Assistant} {Robot} in {Library}},
	doi = {10.1109/KSE.2015.18},
	abstract = {Visual interpretation of hand gesture for human-system interaction in general and human-robot interaction in particular is becoming a hot topic in computer vision and robotics fields. Hand gestures provide very intuitive and efficient means and enhance the flexibility in communication. Even a number of works have been proposed for hand gesture recognition, the use of these works for real human-robot interaction is still limited. Based on our previous works for hand detection and hand gesture recognition, we have built a fully automatic hand gesture recognition system and have applied it in a human-robot interaction application: service robot in library. In this paper, we describe in detail this application from the user requirement analysis to system deployment and experiments with end-users.},
	booktitle = {2015 {Seventh} {International} {Conference} on {Knowledge} and {Systems} {Engineering} ({KSE})},
	author = {Nguyen, V. and Tran, T. and Le, T. and Mullot, R. and Courboulay, V.},
	month = oct,
	year = {2015},
	keywords = {robot vision, human-robot interaction, Service robots, Feature extraction, Cameras, computer vision, Human-robot interaction, Robot vision systems, gesture recognition, service robots, service robot, hand postures, assistant robot, fully automatic hand gesture recognition system, hand detection, Hand posture recognition, human-system interaction, Kernel descriptor, Libraries, library, library automation, Viola-Jones detector, Visual based human-robot interaction, visual hand gesture interpretation, 1bt\_include},
	pages = {354--359},
	file = {Nguyen et al. - 2015 - Using Hand Postures for Interacting with Assistant.pdf:/home/brendan/Zotero/storage/S5LTN77M/Nguyen et al. - 2015 - Using Hand Postures for Interacting with Assistant.pdf:application/pdf}
}

@inproceedings{limin_medical_2017,
	title = {The medical service robot interaction based on kinect},
	doi = {10.1109/ITCOSP.2017.8303077},
	abstract = {In view of human body motion recognition technology, a gesture recognition method based on LabVIEW is proposed, which is applied to the control of medical service robot. The 3D somatosensory camera of Kinect is used to track the human skeleton points, and then capture human actions in real time on the LabVIEW platform, identify different actions of the body. Finally, the motion commands are sent to the robot through the Bluetooth communication to complete the robot forward, turning and wheelchair mode conversion, etc. After testing, the method of hand gesture recognition based on the LabVIEW, can be very good for tracking the human body and identifying the body's actions. The human computer interaction of the medical service robot is realized, which provides convenience for the assistant doctor to complete the rehabilitation and nursing of the patients.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Intelligent} {Techniques} in {Control}, {Optimization} and {Signal} {Processing} ({INCOS})},
	author = {Limin, M. and Peiyi, Z.},
	month = mar,
	year = {2017},
	keywords = {Robot sensing systems, Three-dimensional displays, medical robotics, Cameras, gesture recognition, human computer interaction, hand gesture recognition, Manipulators, service robots, patient rehabilitation, Human-computer interaction, Skeleton, Kinect, Bluetooth, Robot, human actions, 3D somatosensory camera, Bluetooth communication, gesture recognition method, human body motion recognition technology, human skeleton points, kinect, LabVIEW platform, medical service robot interaction, Medical services, motion commands, Motion recognition, 1bt\_include},
	pages = {1--7},
	file = {Limin and Peiyi - 2017 - The medical service robot interaction based on kin.pdf:/home/brendan/Zotero/storage/XFP836SR/Limin and Peiyi - 2017 - The medical service robot interaction based on kin.pdf:application/pdf}
}

@article{erol_toward_2020,
	title = {Toward {Artificial} {Emotional} {Intelligence} for {Cooperative} {Social} {Human}–{Machine} {Interaction}},
	volume = {7},
	issn = {2329-924X},
	doi = {10.1109/TCSS.2019.2922593},
	abstract = {The aptitude to identify the emotional states of others and response to exposed emotions is an important aspect of human social intelligence. Robots are expected to be prevalent in society to assist humans in various tasks. Human-robot interaction (HRI) is of critical importance in the assistive robotics sector. Smart digital assistants and assistive robots fail quite often when a request is not well defined verbally. When the assistant fails to provide services as desired, the person may exhibit an emotional response such as anger or frustration through expressions in their face and voice. It is critical that robots understand not only the language, but also human psychology. A novel affection-based perception architecture for cooperative HRIs is studied in this paper, where the agent is expected to recognize human emotional states, thus encourages a natural bonding between the human and the robotic artifact. We propose a method to close the loop using measured emotions to grade HRIs. This metric will be used as a reward mechanism to adjust the assistant's behavior adaptively. Emotion levels from users are detected through vision and speech inputs processed by deep neural networks (NNs). Negative emotions exhibit a change in performance until the user is satisfied.},
	number = {1},
	journal = {IEEE Transactions on Computational Social Systems},
	author = {Erol, B. A. and Majumdar, A. and Benavidez, P. and Rad, P. and Choo, K. R. and Jamshidi, M.},
	month = feb,
	year = {2020},
	keywords = {neural nets, robot vision, supervisory control, Humanoid robots, human-robot interaction, humanoid robot, Task analysis, Cameras, Robot vision systems, speech recognition, emotion recognition, Emotion recognition, psychology, Assistive robotics, smart home, assistive robots, deep neural networks, affective computing, affection-based perception architecture, artificial emotional intelligence, assistant behavior, assistive robotics sector, cooperative HRI, cooperative social human-machine interaction, emotion levels, emotional response, Emotional responses, human emotional state recognition, human psychology, human social intelligence, human–machine interactions, Internet of robotic things, negative emotions, robotic artifact, smart digital assistants, speech input, vision input, 1bt\_include},
	pages = {234--246},
	file = {Erol et al. - 2020 - Toward Artificial Emotional Intelligence for Coope.pdf:/home/brendan/Zotero/storage/TNDEEVD4/Erol et al. - 2020 - Toward Artificial Emotional Intelligence for Coope.pdf:application/pdf}
}

@inproceedings{sriram_mobile_2019,
	title = {Mobile {Robot} {Assistance} for {Disabled} and {Senior} {Citizens} {Using} {Hand} {Gestures}},
	doi = {10.1109/PETPES47060.2019.9003821},
	abstract = {Paralysis (paraplegia) and motor-impairments impact the autonomy of elder people while performing their tasks independently. To fill the gap between machines and humans, gesture plays a significant role. This work focuses on Human-Robot Interaction (HRI), designed for assistance of wheel chair bound people with the help of mobile robots. People will get their work done faster and effortlessly with the help of mobile robots. This paper explains the interaction of mobile robot based on user gestures and assists using a 2-DoF manipulator for pick and place of the objects. Gestures are recognized by camera at real-time and robot (Firebird V-LPC2148) is controlled wirelessly through XBee radio module interface in the given environment.},
	booktitle = {2019 {International} {Conference} on {Power} {Electronics} {Applications} and {Technology} in {Present} {Energy} {Scenario} ({PETPES})},
	author = {Sriram, K. N. V. and Palaniswamy, S.},
	month = aug,
	year = {2019},
	keywords = {control engineering computing, manipulators, mobile robots, Robot sensing systems, human-robot interaction, Robot kinematics, medical robotics, assisted living, Mobile robots, Navigation, gesture recognition, hand gestures, Gesture, Gesture recognition, handicapped aids, Manipulators, camera, geriatrics, 2DoF manipulator, Camera, disabled citizens, elder people, Fire-Bird V, Firebird V-LPC2148, Manipulator, mobile robot assistance, motor-impairments, objects pick and place, Obstacle avoidance, paralysis, Self-localization, senior citizens, Servo motor, user gestures, wheel chair bound people, XBee, XBee radio module interface, 1bt\_include},
	pages = {1--6},
	file = {Sriram and Palaniswamy - 2019 - Mobile Robot Assistance for Disabled and Senior Ci.pdf:/home/brendan/Zotero/storage/XCEK2FSD/Sriram and Palaniswamy - 2019 - Mobile Robot Assistance for Disabled and Senior Ci.pdf:application/pdf}
}

@inproceedings{droeschel_towards_2011,
	title = {Towards joint attention for a domestic service robot - person awareness and gesture recognition using {Time}-of-{Flight} cameras},
	doi = {10.1109/ICRA.2011.5980067},
	abstract = {Joint attention between a human user and a robot is essential for effective human-robot interaction. In this work, we propose an approach to person awareness and to the perception of showing and pointing gestures for a domestic service robot. In contrast to previous work, we do not require the person to be at a predefined position, but instead actively approach and orient towards the communication partner. For perceiving showing and pointing gestures and for estimating the pointing direction a Time-of-Flight camera is used. Estimated pointing directions and shown objects are matched to objects in the robot's environment. Both the perception of showing and pointing gestures as well as the accurary of estimated pointing directions have been evaluated in a set of different experiments. The results show that both gestures are adequatly perceived by the robot. Furthermore, our system achieves a higher accuracy in estimating the pointing direction than is reported in the literature for a stereo-based system. In addition, the overall system has been successfully tested in two international RoboCup@Home competitions and the 2010 ICRA Mobile Manipulation Challenge.},
	booktitle = {2011 {IEEE} {International} {Conference} on {Robotics} and {Automation}},
	author = {Droeschel, D. and Stückler, J. and Holz, D. and Behnke, S.},
	month = may,
	year = {2011},
	note = {ISSN: 1050-4729},
	keywords = {human-robot interaction, Cameras, Robot vision systems, Face, gesture recognition, Humans, service robots, Image segmentation, domestic service robot person awareness, stereo-based system, time-of-flight cameras, 1bt\_include},
	pages = {1205--1210},
	file = {Droeschel et al. - 2011 - Towards joint attention for a domestic service rob.pdf:/home/brendan/Zotero/storage/6G9BBRTD/Droeschel et al. - 2011 - Towards joint attention for a domestic service rob.pdf:application/pdf}
}

@inproceedings{gong_research_2018,
	title = {Research on human-robot interaction {Security} {Strategy} of {Movement} authorization for service robot {Based} on people's attention monitoring},
	doi = {10.1109/IISR.2018.8535908},
	abstract = {This paper puts forward a kind of movement behavior planning method of service robot based on the responsibility analysis of accident responsibility and designs a set of security interactive strategy based on the human's attention to robot's action. In order to effectively avoid the human physical damage caused by the robot movement without realizing the dangerous behavior of the robot, we proposed a detecting method of people's awareness. The strategy in this paper adopts the method of real time on-line detection of human activity based on IMU, measuring the static attitude of human, at the mean time we use computer vision to evaluate people's attention to robot's behavior and carry out safety warning. Through strategic choice, it can stop the movement in a warning state and apply to a person for their authorization to effectively avoid harm people.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Intelligence} and {Safety} for {Robotics} ({ISR})},
	author = {Gong, J. and Wang, H. and Lu, Z. and Feng, N. and Hu, F.},
	month = aug,
	year = {2018},
	keywords = {robot vision, motion control, control system synthesis, mobile robots, human-robot interaction, Service robots, Cameras, Robot vision systems, Face, service robots, service robot, robot movement, Security, accident responsibility, human physical damage, human-robot interaction security strategy, movement authorization, movement behavior, responsibility analysis, security interactive strategy, time on-line detection, 1bt\_include},
	pages = {521--526},
	file = {Gong et al. - 2018 - Research on human-robot interaction Security Strat.pdf:/home/brendan/Zotero/storage/ZMWXBAPZ/Gong et al. - 2018 - Research on human-robot interaction Security Strat.pdf:application/pdf}
}

@inproceedings{jean-pierre_artist_2012,
	title = {The artist robot: {A} robot drawing like a human artist},
	doi = {10.1109/ICIT.2012.6209985},
	abstract = {An artist robot that draws portraits like a human artist is presented in this paper. This application concerns entertainement; it was developed at the PPRIME Institute in the ROBIOSS team and a patent [1] was deposited for this invention in 2007. The Artist Robot was installed in 2006 in the Futuroscope Park in France : the “artist robot” draws every day the portraits of the visitors by using a camera and a pen attached to end-effector. It has been developed first in the context of a collaboration with the Futuroscope Park. A second version of the “artist robot” was designed for being used in international shows or exhibitions. Because of the “human like” behavior, the artist robot meets a real success with the public and its ability to reproduce a human portrait by using in the same time robot motion control and image processing provides realistic result for the portrait rendering. Based on the specifications, the whole application is detailed. The robot and its environment are described; and sofware engineering and image processing are discussed. Finally results illustrate the efficiency and the success of the Artist Robot; and an analysis of how the task is carried out is provided.},
	booktitle = {2012 {IEEE} {International} {Conference} on {Industrial} {Technology}},
	author = {Jean-Pierre, G. and Saïd, Z.},
	month = mar,
	year = {2012},
	keywords = {robot vision, motion control, Planning, Cameras, Face, end effectors, camera, image processing, Software, end-effector, entertainment, robot motion control, artist robot, drawing robot, entertainement, exhibitions, France, Futuroscope Park, human artist, human like behavior, human portrait, international shows, invention, pen, portraits, PPRIME Institute, ROBIOSS team, sofware engineering, 1bt\_include},
	pages = {486--491},
	file = {Jean-Pierre and Saïd - 2012 - The artist robot A robot drawing like a human art.pdf:/home/brendan/Zotero/storage/SR45Q9EP/Jean-Pierre and Saïd - 2012 - The artist robot A robot drawing like a human art.pdf:application/pdf}
}

@article{gao_user_2020,
	title = {User {Modelling} {Using} {Multimodal} {Information} for {Personalised} {Dressing} {Assistance}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.2978207},
	abstract = {Assistive robots in home environments are steadily increasing in popularity. Due to significant variabilities in human behaviour, as well as physical characteristics and individual preferences, personalising assistance poses a challenging problem. In this paper, we focus on an assistive dressing task that involves physical contact with a human's upper body, in which the goal is to improve the comfort level of the individual. Two aspects are considered to be significant in improving a user's comfort level: having more natural postures and exerting less effort. However, a dressing path that fulfils these two criteria may not be found at one time. Therefore, we propose a user modelling method that combines vision and force data to enable the robot to search for an optimised dressing path for each user and improve as the human-robot interaction progresses. We compare the proposed method against two single-modality state-of-the-art user modelling methods designed for personalised assistive dressing by user studies (31 subjects). Experimental results show that the proposed method provides personalised assistance that results in more natural postures and less effort for human users.},
	journal = {IEEE Access},
	author = {Gao, Y. and Chang, H. J. and Demiris, Y.},
	year = {2020},
	keywords = {optimisation, Hidden Markov models, Robots, Force, human-robot interaction, Task analysis, assisted living, feature extraction, Human-robot interaction, service robots, interactive systems, human behaviour, human users, Adaptation models, assistive dressing, assistive robots, personalised dressing assistance, user modelling, home environments, Data models, multimodal information, physical contact, dressing path optimisation, human-robot interaction progresses, humans upper body, Multimodal user modelling, natural postures, personalised assistive dressing, physical characteristics, user modelling method, users comfort level, vision and force fusion, 1bt\_include},
	pages = {45700--45714},
	file = {Full Text:/home/brendan/Zotero/storage/ML27H6JZ/Gao et al. - 2020 - User Modelling Using Multimodal Information for Pe.pdf:application/pdf}
}

@inproceedings{vasquez_deep_2017,
	title = {Deep {Detection} of {People} and their {Mobility} {Aids} for a {Hospital} {Robot}},
	doi = {10.1109/ECMR.2017.8098665},
	abstract = {Robots operating in populated environments encounter many different types of people, some of whom might have an advanced need for cautious interaction, because of physical impairments or their advanced age. Robots therefore need to recognize such advanced demands to provide appropriate assistance, guidance or other forms of support. In this paper, we propose a depth-based perception pipeline that estimates the position and velocity of people in the environment and categorizes them according to the mobility aids they use: pedestrian, person in wheelchair, person in a wheelchair with a person pushing them, person with crutches and person using a walker. We present a fast region proposal method that feeds a Region-based Convolutional Network (Fast R- CNN [1]). With this, we speed up the object detection process by a factor of seven compared to a dense sliding window approach. We furthermore propose a probabilistic position, velocity and class estimator to smooth the CNN's detections and account for occlusions and misclassifications. In addition, we introduce a new hospital dataset with over 17,000 annotated RGB-D images. Extensive experiments confirm that our pipeline successfully keeps track of people and their mobility aids, even in challenging situations with multiple people from different categories and frequent occlusions.},
	booktitle = {2017 {European} {Conference} on {Mobile} {Robots} ({ECMR})},
	author = {Vasquez, A. and Kollmitz, M. and Eitel, A. and Burgard, W.},
	month = sep,
	year = {2017},
	keywords = {robot vision, Hidden Markov models, Robots, mobile robots, image sensors, convolution, object detection, image colour analysis, handicapped aids, Image segmentation, Detectors, RGB-D images, Pipelines, Proposals, Hospitals, class estimator, CNN detections, dense sliding window approach, depth-based perception pipeline, Fast R- CNN, fast region proposal method, hospital robot, mobility aids, object detection process, physical impairments, Region-based Convolutional Network, robots operating, wheelchair, 1bt\_include},
	pages = {1--7},
	file = {Submitted Version:/home/brendan/Zotero/storage/4HKHGI8C/Vasquez et al. - 2017 - Deep Detection of People and their Mobility Aids f.pdf:application/pdf}
}

@inproceedings{wu_accompanist_2012,
	title = {Accompanist detection and following for wheelchair robots with fuzzy controller},
	abstract = {Human-robot interaction is an important issue for service robots. In this paper, we propose to utilize the multisensory data fusion to autonomously detect and track an accompanist as a target person in the surroundings of the mobile robot. First, we use laser range finder to detect the target person features which is characterized using distances and orientations from range sensor readings. Based on the sensor information and the desired tracking position, a fuzzy based tracking controller is proposed to follow the accompanist. In addition, the obstacle avoidance strategy is also embedded into the following controller to mitigate disturbances or interferences in dynamical environment when robot is performing a following task simultaneously. If the accompanist is missed, a recognition process based on a pan-tilt-zoom camera and observed images will be used to search the target person again. Experimental results demonstrated the validation and feasibility of the proposed system.},
	booktitle = {The 2012 {International} {Conference} on {Advanced} {Mechatronic} {Systems}},
	author = {Wu, B. and Jen, C. and Tsou, T. and Li, W. and Tseng, P.},
	month = sep,
	year = {2012},
	note = {ISSN: 2325-0690},
	keywords = {robot vision, robot dynamics, position control, mobile robots, Robot sensing systems, human-robot interaction, object detection, Tracking, mobile robot, Mobile robots, collision avoidance, service robots, Image recognition, fuzzy control, sensor fusion, laser range finder, laser ranging, Mobile communication, Lasers, accompanist detection, disturbances mitigation, dynamical environment, following controller, fuzzy based tracking controller, interferences mitigation, multisensory data fusion, observed images, obstacle avoidance strategy, pan-tilt-zoom camera, position tracking, range sensor readings, recognition process, target person feature detection, wheelchair robots, 1bt\_include},
	pages = {638--643},
	file = {Wu et al. - 2012 - Accompanist detection and following for wheelchair.pdf:/home/brendan/Zotero/storage/FFMRUGNR/Wu et al. - 2012 - Accompanist detection and following for wheelchair.pdf:application/pdf}
}

@inproceedings{lambrecht_spatial_2012,
	title = {Spatial programming for industrial robots based on gestures and {Augmented} {Reality}},
	doi = {10.1109/IROS.2012.6385900},
	abstract = {The presented spatial programming system provides an assistance system for online programming of industrial robots. A handheld device and a motion tracking system establish the basis for a modular 3D programming approach corresponding to different phases of robot programming: definition, evaluation and adaption. Static and dynamic gestures enable the program definition of poses, trajectories and tasks. The spatial evaluation is done using an Augmented Reality application on a handheld device. Therefore, the programmer is able to move freely within the robot cell and define the program spatially through gestures. The camera image of the handheld is simultaneously enhanced by virtual objects representing the robot program. Based on 3D motion tracking of human movements and a mobile Augmented Reality application, we introduce a novel kind of interaction for the adaption of robot programs. The programmer is enabled to interact with virtual program components through bare-hand gestures. Such sample forms of interaction include translation and rotation applicable to poses, trajectories or tasks representations. Finally, the program is adapted according to the gestural changes and can be transferred from the handheld device directly to the robot controler.},
	booktitle = {2012 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	author = {Lambrecht, J. and Krüger, J.},
	month = oct,
	year = {2012},
	note = {ISSN: 2153-0866},
	keywords = {Trajectory, Robot sensing systems, image motion analysis, Robot kinematics, industrial robots, Service robots, Tracking, gesture recognition, cameras, robot programming, static gestures, augmented reality, dynamic gestures, human movements, gait analysis, Programming, camera image, motion tracking system, 3D motion tracking, assistance system, bare-hand gestures, handheld device, interactive programming, mobile augmented reality application, modular 3D programming approach, online programming, program definition, robot cell, robot controler, robot programs, spatial programming, spatial programming system, virtual objects, virtual program components, 1bt\_include},
	pages = {466--472},
	file = {Kai-Tai Song and Wei-Jyun Chen - 2011 - Human activity recognition using a mobile camera.pdf:/home/brendan/Zotero/storage/EVJQQDNN/Kai-Tai Song and Wei-Jyun Chen - 2011 - Human activity recognition using a mobile camera.pdf:application/pdf}
}

@inproceedings{yuan_development_2017,
	title = {Development of a human-friendly robot for socially aware human-robot interaction},
	doi = {10.1109/ICARM.2017.8273138},
	abstract = {One of the fundamental issues for service robots is human-robot interaction. In order to provide desired services, these robots need audiovisual perception of humans and to make appropriate feedback. In this paper, we present our system for spontaneous speech recognition, localization and identification of the user, recognition of the user's gestures, so as to perform the corresponding tasks like mapping, following the user and heading to designated location meanwhile avoiding the obstacles on the way. The system employs a new method of multi-feature detection for robot self-localization based on the recognition of artificial and natural features extracted from laser scans and RGB-D images, and they are quite discriminative in cluttered environments. An angle potential field (APF) method was used for obstacle avoidance. Experiments demonstrate the feasibility and effectiveness of the strategies in the paper.},
	booktitle = {2017 2nd {International} {Conference} on {Advanced} {Robotics} and {Mechatronics} ({ICARM})},
	author = {Yuan, W. and Li, Z.},
	month = aug,
	year = {2017},
	keywords = {robot vision, human-robot interaction, Robot kinematics, feature extraction, Cameras, Human-robot interaction, Robot vision systems, user identification, gesture recognition, image colour analysis, collision avoidance, speech recognition, Speech recognition, service robots, angle potential field, Companion robots, human-friendly robot, Multimodal perception, obstacle avoidance, RGB-D images, robot self-localization, socially aware human-robot interaction, spontaneous speech recognition, 1bt\_include},
	pages = {76--81},
	file = {Yuan and Li - 2017 - Development of a human-friendly robot for socially.pdf:/home/brendan/Zotero/storage/FW7FNV4G/Yuan and Li - 2017 - Development of a human-friendly robot for socially.pdf:application/pdf}
}

@inproceedings{fareed_gesture_2015,
	title = {Gesture {Based} {Wireless} {Single}-{Armed} {Robot} in {Cartesian} {3D} {Space} {Using} {Kinect}},
	doi = {10.1109/CSNT.2015.86},
	abstract = {Human Machine Interaction (HMI) has always played an important role in everybody's life motivating research in the area of intelligent service robots. Conventional methods such as remote controllers or wearables cannot cater the high demands in some scenarios. To overcome this situation, the challenge is to develop vision-based gesture recognition techniques. This paper describes our work of controlling an Arduino based wheeled, one armed robot, used as a prototype, controlled through various gestures of the arms and legs. For gesture recognition, we make use of skeletal tracing ability of Kinect – a product of Microsoft. Bluetooth is used to make the controls wireless. Since it is not line of sight operation, the robot also captures the environment video and transmits it over radio frequency in real-time and displayed on the screen. On the user end, according to the received video, the operator guides the robot and uses the arm to pick and place objects with the help of predetermined gestures.},
	booktitle = {2015 {Fifth} {International} {Conference} on {Communication} {Systems} and {Network} {Technologies}},
	author = {Fareed, M. M. F. M. and Akram, Q. I. and Anees, S. B. A. and Fakih, A. H.},
	month = apr,
	year = {2015},
	keywords = {Robot sensing systems, human-robot interaction, Collision avoidance, Robot kinematics, Tracking, gesture recognition, Gesture recognition, Joints, intelligent service robots, Arduino, Arm control, Bluetooth, cartesian 3D space, gesture based wireless single-armed robot, human machine interaction, intelligent robots, Kinect Sensor, Motion control, Robotic arm, skeletal tracing ability, Skeletal Tracking, 1bt\_include},
	pages = {1210--1215},
	file = {Fareed et al. - 2015 - Gesture Based Wireless Single-Armed Robot in Carte.pdf:/home/brendan/Zotero/storage/RKE8NQCM/Fareed et al. - 2015 - Gesture Based Wireless Single-Armed Robot in Carte.pdf:application/pdf}
}

@inproceedings{lee_visual_2020,
	title = {Visual {Perception} {Framework} for an {Intelligent} {Mobile} {Robot}},
	doi = {10.1109/UR49135.2020.9144932},
	abstract = {Visual perception is a fundamental capability necessary for intelligent mobile robots to interact properly and safely with the humans in the real-world. Recently, the world has seen revolutionary advances in deep learning has led to some incredible breakthroughs in vision technology. However, research integrating diverse visual perception methods into robotic systems is still in its infancy and lacks validation in real-world scenarios. In this paper, we present a visual perception framework for an intelligent mobile robot. Based on the robot operating system middleware, our framework integrates a broad set of advanced algorithms capable of recognising people, objects and human poses, as well as describing observed scenes. In several challenge scenarios of international robotics competitions using two mobile service robots, the performance and acceptability of the proposed framework are evaluated.},
	booktitle = {2020 17th {International} {Conference} on {Ubiquitous} {Robots} ({UR})},
	author = {Lee, C. and Lee, H. and Hwang, I. and Zhang, B.},
	month = jun,
	year = {2020},
	note = {ISSN: 2325-033X},
	keywords = {deep learning, learning (artificial intelligence), robot vision, mobile robots, Robot sensing systems, Task analysis, Service robots, visual perception, pose estimation, Mobile robots, service robots, object recognition, middleware, Detectors, intelligent robots, human pose recognition, Visual perception, mobile service robots, intelligent mobile robot, international robotics competitions, people recognition, robot operating system middleware, vision technology, visual perception framework, 1bt\_include},
	pages = {612--616},
	file = {Lee et al. - 2020 - Visual Perception Framework for an Intelligent Mob.pdf:/home/brendan/Zotero/storage/JNV88PB6/Lee et al. - 2020 - Visual Perception Framework for an Intelligent Mob.pdf:application/pdf}
}

@inproceedings{farulla_novel_2015,
	title = {A {Novel} {Architectural} {Pattern} to {Support} the {Development} of {Human}-{Robot} {Interaction} ({HRI}) {Systems} {Integrating} {Haptic} {Interfaces} and {Gesture} {Recognition} {Algorithms}},
	doi = {10.1109/ISVLSI.2015.112},
	abstract = {Haptic and robotic interfaces are recently gaining momentum to be pervasively integrated in modern everyday life. In fact, they can be employed in several different fields, ranging from manipulation of small and dangerous objects to rehabilitation, assistive and service technologies, and are also integrated in mission critical systems. Modern research is rapidly shifting to investigate novel and more intuitive ways of controlling these interfaces. In particular, gesture-based control is one of the most interesting scenario for Human-Robot Interaction (HRI), since we human perceive gestures as a natural way of interaction with the external world. In this work we present a novel architectural pattern, entirely based on the Robotic Operating System (ROS), to support the development of applications and systems where computer vision techniques are applied to control robotic interfaces. As case study, the presented pattern is used to develop and assess the overall PARLOMA system. PARLOMA project aims at developing a system to enable remote communication between deaf-blind subjects. The system is designed to send, remotely and in real-time, messages in tactile Sign Language from a sender to a deaf-blind recipient (or many recipients) by integrating hand tracking and gesture recognition algorithms coupled with bio-mimetic haptic interfaces.},
	booktitle = {2015 {IEEE} {Computer} {Society} {Annual} {Symposium} on {VLSI}},
	author = {Farulla, G. A. and Russo, L. O. and Gallifuoco, V. and Indaco, M.},
	month = jul,
	year = {2015},
	note = {ISSN: 2159-3477},
	keywords = {control engineering computing, robot vision, ROS, human-robot interaction, Cameras, computer vision, Robot vision systems, gesture recognition, Gesture recognition, Joints, Human-Robot Interaction, Gesture Recognition, haptic interfaces, architectural pattern, assistive technology, biomimetic hap tic interfaces, deaf-blind subjects, gesture recognition algorithms, gesture-based control, hand tracking, Haptic interfaces, Haptic Interfaces, HRI systems, human-robot interaction systems, operating systems (computers), PARLOMA system, Peer-to-peer computing, rehabilitation, remote communication, robotic interface control, robotic interfaces, robotic operating system, Service Robotics, service technology, tactile sign language, 1bt\_include},
	pages = {386--391},
	file = {Farulla et al. - 2015 - A Novel Architectural Pattern to Support the Devel.pdf:/home/brendan/Zotero/storage/6WNYJ5SY/Farulla et al. - 2015 - A Novel Architectural Pattern to Support the Devel.pdf:application/pdf}
}

@inproceedings{wang_wheeled_2013,
	title = {Wheeled robot control based on gesture recognition using the {Kinect} sensor},
	doi = {10.1109/ROBIO.2013.6739488},
	abstract = {Human Machine Interaction (HMI) plays a very important role in intelligent service robot research. Traditional HMI methods such as keyboard and mouse cannot satisfy the high demands in some environments. To solve this problem, many researchers pay attention to vision-based gesture recognition research recently. This paper proposes a simple method to control the movement of a robot based on Kinect which provides skeleton data with low computation, acceptable performance and financial cost. The method can recognize eleven defined gestures by using the coordinates of joints, which are obtained from the skeleton model provided by the Kinect SDK. A Khepera III robot is used as a prototype control object, to verify the effectiveness of the proposed method. The experimental results show that the success rate of gesture recognition is over 96\%. The proposed method is robust to work in real-time.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Robotics} and {Biomimetics} ({ROBIO})},
	author = {Wang, Y. and Song, G. and Qiao, G. and Zhang, Y. and Zhang, J. and Wang, W.},
	month = dec,
	year = {2013},
	keywords = {robot vision, motion control, Hidden Markov models, mobile robots, image sensors, Robot sensing systems, human-robot interaction, Robot kinematics, gesture recognition, Gesture recognition, Joints, service robots, Kinect sensor, human machine interaction, intelligent robots, financial cost, HMI method, intelligent service robot, joints coordination, Khepera III robot, Kinect SDK, prototype control object, robot movement control, skeleton model, vision-based gesture recognition, wheeled robot control, wheels, 1bt\_include},
	pages = {378--383},
	file = {Wang et al. - 2013 - Wheeled robot control based on gesture recognition.pdf:/home/brendan/Zotero/storage/F76BHWK2/Wang et al. - 2013 - Wheeled robot control based on gesture recognition.pdf:application/pdf}
}

@inproceedings{por_olivia_2010,
	title = {Olivia 2.0 @ {TechFest} 09: {Receptionist} robot impressed visitors with lively interactions},
	doi = {10.1109/HRI.2010.5453169},
	abstract = {Olivia 2.0 is a Social Robot designed to interact and serve in office environment as a Robotic Receptionist. This is the forth model of Service Robot developed by A*STAR Robotics Team in Singapore. For a start, the occupation \& background story of Olivia as a receptionist has set a common ground between human and robot for interaction around topics fitting to the job. The use of vision technology enabled Olivia to detect the presence of a visitor standing in front of her so that she will initiate a dialogue. Through speech recognition technology and careful designed dialogue management system, visitors are able to converse with Olivia to know more about the amenities in Fusionopolis building as well as to engage in small talk. Taking the persona of a 5 years old kid, with a cute face and child voice, coupled with nice decorations has set the stage for a fun interaction time, as Olivia proceeds to engage the visitor to play a simple game that showcased object recognition and tracking capability. Olivia is built with advanced Mechatronics design with 13 degree of freedom for head, body and hands motions. The advance motion control algorithm and imitation learning software trained her well to display humanlike hand gestures and upper body movements. We noticed the lively gestures coupled with expressive robotic voice are very crucial to draw attention from human for the continuous engagement with Social Robot. We knew that this would be a valuable field trial opportunity whereby many visitors were having first encountering with service robot. We took advantage to study social acceptance by taking video recording for every human-robot interactions, follow by inviting visitors to participate in on-site feedback gathering with questionnaires. Of more than 100 questionnaires completed, 62\% gave an overall rating of good and above, several expressed that the response of the robot is slow and 75.5\% found that the robot is able to recognize their speech without any difficulties. The top 3 robot features that people would like to have in the robot are: fast response; clear way of talking; delivering relevant information. At the end of this technology exhibition over two days, more than 100 visitors interacted with Olivia for information enquiry and playing games. They were greatly impressed by her capabilities and above all they had a lot of fun interacting with her.},
	booktitle = {2010 5th {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction} ({HRI})},
	author = {Por, L. T. C. and Tay, A. and Limbu, D. K.},
	month = mar,
	year = {2010},
	note = {ISSN: 2167-2148},
	keywords = {robot vision, humanoid robots, human-robot interaction, hand gestures, speech recognition, service robots, Social robotics, service robot, object recognition, interactive systems, tracking, social robot, receptionist robot, video recording, dialogue management system, learning software, motion control algorithm, Olivia 2.0, 1bt\_include},
	pages = {351--351},
	file = {Full Text:/home/brendan/Zotero/storage/DQHVF86X/Por et al. - 2010 - Olivia 2.0 @ TechFest 09 Receptionist robot impre.pdf:application/pdf}
}

@inproceedings{wang_see_2020,
	address = {New York, NY, USA},
	series = {{HRI} '20},
	title = {See {What} {I} {See}: {Enabling} {User}-{Centric} {Robotic} {Assistance} {Using} {First}-{Person} {Demonstrations}},
	isbn = {978-1-4503-6746-2},
	url = {https://doi.org/10.1145/3319502.3374820},
	doi = {10.1145/3319502.3374820},
	abstract = {We explore first-person demonstration as an intuitive way of producing task demonstrations to facilitate user-centric robotic assistance. First-person demonstration directly captures the human experience of task performance via head-mounted cameras and naturally includes productive viewpoints for task actions. We implemented a perception system that parses natural first-person demonstrations into task models consisting of sequential task procedures, spatial configurations, and unique task viewpoints. We also developed a robotic system capable of interacting autonomously with users as it follows previously acquired task demonstrations. To evaluate the effectiveness of our robotic assistance, we conducted a user study contextualized in an assembly scenario; we sought to determine how assistance based on a first-person demonstration (user-centric assistance) versus that informed only by the cover image of the official assembly instruction (standard assistance) may shape users' behaviors and overall experience when working alongside a collaborative robot. Our results show that participants felt that their robot partner was more collaborative and considerate when it provided user-centric assistance than when it offered only standard assistance. Additionally, participants were more likely to exhibit unproductive behaviors, such as using their non-dominant hand, when performing the assembly task without user-centric assistance.},
	booktitle = {Proceedings of the 2020 {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Yeping and Ajaykumar, Gopika and Huang, Chien-Ming},
	year = {2020},
	note = {event-place: Cambridge, United Kingdom},
	keywords = {human-robot interaction, 1bt\_include, collaborative robotics, first-person demonstration, programming by demonstration},
	pages = {639--648},
	file = {Wang et al. - 2020 - See What I See Enabling User-Centric Robotic Assi.pdf:/home/brendan/Zotero/storage/7M9VSMSM/Wang et al. - 2020 - See What I See Enabling User-Centric Robotic Assi.pdf:application/pdf}
}

@article{castellano_context-sensitive_2014,
	title = {Context-{Sensitive} {Affect} {Recognition} for a {Robotic} {Game} {Companion}},
	volume = {4},
	issn = {2160-6455},
	url = {https://doi.org/10.1145/2622615},
	doi = {10.1145/2622615},
	abstract = {Social perception abilities are among the most important skills necessary for robots to engage humans in natural forms of interaction. Affect-sensitive robots are more likely to be able to establish and maintain believable interactions over extended periods of time. Nevertheless, the integration of affect recognition frameworks in real-time human-robot interaction scenarios is still underexplored. In this article, we propose and evaluate a context-sensitive affect recognition framework for a robotic game companion for children. The robot can automatically detect affective states experienced by children in an interactive chess game scenario. The affect recognition framework is based on the automatic extraction of task features and social interaction-based features. Vision-based indicators of the children’s nonverbal behaviour are merged with contextual features related to the game and the interaction and given as input to support vector machines to create a context-sensitive multimodal system for affect recognition. The affect recognition framework is fully integrated in an architecture for adaptive human-robot interaction. Experimental evaluation showed that children’s affect can be successfully predicted using a combination of behavioural and contextual data related to the game and the interaction with the robot. It was found that contextual data alone can be used to successfully predict a subset of affective dimensions, such as interest toward the robot. Experiments also showed that engagement with the robot can be predicted using information about the user’s valence, interest and anticipatory behaviour. These results provide evidence that social engagement can be modelled as a state consisting of affect and attention components in the context of the interaction.},
	number = {2},
	journal = {ACM Trans. Interact. Intell. Syst.},
	author = {Castellano, Ginevra and Leite, Iolanda and Pereira, André and Martinho, Carlos and Paiva, Ana and Mcowan, Peter W.},
	month = jun,
	year = {2014},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {human-robot interaction, Affect recognition, contextual information, robot companions, 1bt\_include, multimodal video corpus},
	file = {Castellano et al. - 2014 - Context-Sensitive Affect Recognition for a Robotic.pdf:/home/brendan/Zotero/storage/RWCXYTXQ/Castellano et al. - 2014 - Context-Sensitive Affect Recognition for a Robotic.pdf:application/pdf}
}

@article{chen_person_2012,
	title = {Person following of a mobile robot using kinect through features detection based on {SURF}},
	volume = {542-543},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868363112&doi=10.4028%2fwww.scientific.net%2fAMR.542-543.779&partnerID=40&md5=b69c56ebcdab57195aab340cb6518a7a},
	doi = {10.4028/www.scientific.net/AMR.542-543.779},
	abstract = {Following a person is an important task for domestic service robots in applications in which human-robot interaction is a primary requirement. Two steps will be completed if the robot needs to achieves this task. It includes detecting the target person and following it. Thus, the robot needs applicable algorithm and specific sensor. In this paper features detection and following technology based on SURF (Speed Up Robust Features) algorithm is used for person following in domestic environments. The vision system of robot obtains very good features of the target person through the RGB camera of kinect (Kinect sensor device) using SURF algorithm. And the depth camera of kinect helps the robot obtain the accurate information about the position of the target person in the environment. It uses SURF algorithm to extract the features of the target person, and match them in following frames. The proposed method is programmed in high speed hardware system and using small zone person following method in order to meet the real time requirement. Experimental results are provided to demonstrate the effectiveness of the proposed approach. © (2012) Trans Tech Publications, Switzerland.},
	journal = {Advanced Materials Research},
	author = {Chen, W. and Guo, S.},
	year = {2012},
	keywords = {1bt\_include},
	pages = {779--784},
	annote = {cited By 2}
}

@article{ferrer_robot_2013,
	title = {Robot {Interactive} {Learning} through {Human} {Assistance}},
	volume = {48},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885609111&doi=10.1007%2f978-3-642-35932-3_11&partnerID=40&md5=9b3df414eeacabc8c8be00dd912f5cab},
	doi = {10.1007/978-3-642-35932-3_11},
	abstract = {This chapter presents some real-life examples using the interactive multimodal framework; in this work, the robot is capable of learning through human assistance. The basic idea is to use the human feedback to improve the learning behavior of the robot when it deals with human beings.We show two different prototypes that have been developed for the following topics: interactive motion learning for robot companion; and on-line face learning using robot vision. On the one hand, the objective of the first prototype is to learn how a robot has to approach to a pedestrian who is going to a destination, minimizing the disturbances to the expected person's path. On the other hand, the objectives of the second prototype are twofold, first, the robot invites a person to approach the robot to initiate a dialogue, and second, the robot learns the face of the person that is invited for a dialogue. The two prototypes have been tested in real-life conditions and the results are very promising. © Springer-Verlag Berlin Heidelberg 2013.},
	journal = {Intelligent Systems Reference Library},
	author = {Ferrer, G. and Garrell, A. and Villamizar, M. and Huerta, I. and Sanfeliu, A.},
	year = {2013},
	keywords = {1bt\_include},
	pages = {185--203},
	annote = {cited By 8},
	file = {Submitted Version:/home/brendan/Zotero/storage/RXCDZYEC/Ferrer et al. - 2013 - Robot Interactive Learning through Human Assistanc.pdf:application/pdf}
}

@article{li_inferring_2019,
	title = {Inferring user intent to interact with a public service robot using bimodal information analysis},
	volume = {33},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063883693&doi=10.1080%2f01691864.2019.1599727&partnerID=40&md5=22781757ddca8e1fc24f75cccf3fd856},
	doi = {10.1080/01691864.2019.1599727},
	abstract = {Achieving polite service with a public service robot requires it to proactively ascertain who will interact with it in human-populated environments. Enlightened by interactive inference of intentions among humans, we investigate a novel and practical interactive intention-predicting method for people using bimodal information analysis for a public service robot. Different from the traditional research, only the visual cues are used to analyze the user's attention, this method combines the RGB-D camera and laser information to perceive the user, which realizes the 360-degree range perception, and compensates for the lack of perspective using the RGB-D camera. In addition, seven kinds of interactive intent features were extracted, and a random forest regression model was trained to score the interaction intentions of the people in the field of view. Considering the inference order of two different sensors, a priority rule for intention inference is also designed. The algorithm is implemented into a robot operation system (ROS) and evaluated on our public service robot. Extensive experimental results illustrate that the proposed method enables public service robots to achieve a higher level of politeness than the traditional, passive interactivity approach in which robots wait for commands from users. © 2019, © 2019 Informa UK Limited, trading as Taylor \& Francis Group and The Robotics Society of Japan.},
	number = {7-8},
	journal = {Advanced Robotics},
	author = {Li, K. and Sun, S. and Zhao, X. and Wu, J. and Tan, M.},
	year = {2019},
	keywords = {1bt\_include},
	pages = {369--387},
	annote = {cited By 1},
	file = {Li et al. - 2019 - Inferring user intent to interact with a public se.pdf:/home/brendan/Zotero/storage/8CMVCCYW/Li et al. - 2019 - Inferring user intent to interact with a public se.pdf:application/pdf}
}

@article{arai_service_2020,
	title = {Service robot arm controlled just by sight},
	volume = {69},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062915242&doi=10.1007%2f978-3-030-12388-8_38&partnerID=40&md5=05f6407b933e9622ea0de5d9c2bd8e07},
	doi = {10.1007/978-3-030-12388-8_38},
	abstract = {Robot arm controlled by Eye Based Human-Computer Interaction: EBHCI is proposed. The proposed system allows disabled person to select desirable food from the meal tray by their eyes only as an example. Robot arm which is used for retrieving the desirable food is controlled by human eye. At the tip of the robot arm, tiny camera is equipped. Disabled person wears a glass of which a single Head Mount Display: HMD and tiny camera is mounted so that disabled person can look at the desired food and retrieve it by looking at the food displayed onto HMD. This is just an example. There are a plenty of available services as a magic arm. Experimental results show that disabled person can retrieve the desired food successfully. It also is confirmed that robot arm control by EBHCI is much faster than that by hands. © Springer Nature Switzerland AG 2020.},
	journal = {Lecture Notes in Networks and Systems},
	author = {Arai, K.},
	year = {2020},
	keywords = {1bt\_include},
	pages = {535--545},
	annote = {cited By 0},
	file = {Arai - 2020 - Service robot arm controlled just by sight.pdf:/home/brendan/Zotero/storage/XVFMU2EV/Arai - 2020 - Service robot arm controlled just by sight.pdf:application/pdf}
}

@article{kollmitz_deep_2019,
	title = {Deep {3D} perception of people and their mobility aids},
	volume = {114},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060737586&doi=10.1016%2fj.robot.2019.01.011&partnerID=40&md5=efd73acbfab5d00a1b6f1f30efc2817c},
	doi = {10.1016/j.robot.2019.01.011},
	abstract = {Robots operating in populated environments, such as hospitals, office environments or airports, encounter a large variety of people with some of them having an advanced need for cautious interaction because of their advanced age or motion impairments. To provide appropriate assistance and support robot helpers require the ability to recognize people and their potential requirements. In this article, we present a people detection framework that distinguishes people according to the mobility aids they use. Our framework uses a deep convolutional neural network for detecting people in image data. For human-aware robots it is necessary to know where people are in a 3D world reference frame instead of only locating them in a 2D image, therefore we add a 3D centroid regression output to the network to predict the Cartesian position of people. We further use a probabilistic class, position and velocity tracker to account for false detections and occlusions. Our framework comes in two variants: The depth only variant targets high privacy demands, while the RGB only framework provides improved detection performance for non-critical applications. Both variants do not require additional geometric information about the environment. We demonstrate our approach using a dedicated dataset acquired with the support of a mobile robotic platform. The dataset contains five classes: pedestrian, person in wheelchair, pedestrian pushing a person in a wheelchair, person using crutches and person using a walking frame. Our framework achieves an mAP of 0.87 for RGB and 0.79 for depth images at a detection distance threshold of 0.5m on our dataset, with a runtime of 53ms per image. The annotated dataset is publicly available and our framework is made open source as a ROS people detector. © 2019 Elsevier B.V.},
	journal = {Robotics and Autonomous Systems},
	author = {Kollmitz, M. and Eitel, A. and Vasquez, A. and Burgard, W.},
	year = {2019},
	keywords = {1bt\_include},
	pages = {29--40},
	annote = {cited By 6},
	file = {Kollmitz et al. - 2019 - Deep 3D perception of people and their mobility ai.pdf:/home/brendan/Zotero/storage/KPX6IGSH/Kollmitz et al. - 2019 - Deep 3D perception of people and their mobility ai.pdf:application/pdf}
}

@article{masmoudi_expressive_2011,
	title = {Expressive robot to support elderly},
	volume = {29},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865460136&doi=10.3233%2f978-1-60750-814-4-557&partnerID=40&md5=510ee58a412a14c3e5bf659d38199518},
	doi = {10.3233/978-1-60750-814-4-557},
	abstract = {Objective. Handibio, in their partnership with Robopec, aims to create a robot equipped with emotional intelligence to support the elderly suffering from slight cognitive disabilities. The originality of this work lies in the development of hardware and software technologies while taking into account the benefit / risk / cost. Main content. The elderly affected by light cognitive disabilities wish to carry on living at home, but usually suffer from loneliness and isolation, both causes of depression. These people need services, particularly in the form of cognitive training and enhancement of socialization that the information and communication technologies (ICT) can provide. However, these ICT are not always accepted as they are not adapted well enough to the needs of these people. Our idea is that a robot serving a cognitive disable person, adapted to the difficulties of this same person and controlled by them, could contribute to their support at home providing with several types of services, from the most simple such as politely reminding this person to take his/her medication, to cognitive and psychological support. To make the relationship more human and intuitive, the robot should have emotion expressions and could make use of speech so that people could interact directly with the robot. This study presents an experimental protocol in order to assess the interaction between the robot and the elderly. It consists in carrying out both abilities test to recognize facial expression to define the difficulties caused by the specificities and variabilities of the environment, and ability evaluates of the robot's to transmit emotion to these persons. Results. Reeti is a "PCBot", that is to say the combination of a PC and an expressive robot. Its flexible "skin" and its 15-degree range give an infinity of facial expression possibilities to it. Its two mobile eyes equipped with two HD video cameras enable it to have an acute vision and a 3D perception of its surroundings. If connected to the Internet, it can send email alerts or simply display a monitoring interface with a remote control. Doing so, it shows a huge potential as an experimental platform for researchers in several fields such as vision, human-robot interaction, therapy assisted with robotics, behavior analysis, sociology and psychology in order to support the elderly and cognitive disable people. The robot has the ability to recognize people and their emotions through the automatic interpretation of their facial expressions. Moreover, it is able to reproduce some basic facial expressions in order to exchange with people. Conclusion. This work highlights the importance of human computer interaction in order to assist the elderly. Facial emotion recognition is necessary in the human-robot interaction, hence the need to overcome technical obstacles in the field of facial expression recognition, especially on elderly subjects. © 2011 The authors and IOS Press. All rights reserved.},
	journal = {Assistive Technology Research Series},
	author = {Masmoudi, R. and Bouchouicha, M. and Gorce, P.},
	year = {2011},
	keywords = {1bt\_include},
	pages = {557--564},
	annote = {cited By 1}
}

@article{kogkas_free-view_2019,
	title = {Free-{View}, {3D} {Gaze}-{Guided} {Robotic} {Scrub} {Nurse}},
	volume = {11768 LNCS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075688399&doi=10.1007%2f978-3-030-32254-0_19&partnerID=40&md5=a1e6b32733bff5988273b0b403e6c561},
	doi = {10.1007/978-3-030-32254-0_19},
	abstract = {We introduce a novel 3D gaze-guided robotic scrub nurse (RN) and test the platform in simulated surgery to determine usability and acceptability with clinical teams. Surgeons and trained scrub nurses performed an ex vivo task on pig colon. Surgeons used gaze via wearable eye-tracking glasses to select surgical instruments on a screen, in turn initiating RN to deliver the instrument. Comparison was done between human- and robot-assisted tasks (HT vs RT). Real-time gaze-screen interaction was based on a framework developed with synergy of conventional wearable eye-tracking, motion capture system and RGB-D cameras. NASA-TLX and Van der Laan’s technology acceptance questionnaires were collected and analyzed. 10 teams of surgical trainees (ST) and scrub nurses (HN) participated. Overall, NASA-TLX feedback was positive. ST and HN revealed no statistically significant difference in overall task load. Task performance feedback was unaffected. Frustration was reported by ST. Overall, Van der Laan’s scores showed positive usefulness and satisfaction scores following RN use. There was no significant difference in task interruptions across HT vs RT. Similarly, no statistical difference was found in duration to task completion in both groups. Quantitative and qualitative feedback was positive. The source of frustration has been understood. Importantly, there was no significant difference in task workflow or operative time, with overall perceptions towards task performance remaining unchanged in HT vs RT. © 2019, Springer Nature Switzerland AG.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Kogkas, A. and Ezzat, A. and Thakkar, R. and Darzi, A. and Mylonas, G.},
	year = {2019},
	keywords = {1bt\_include},
	pages = {164--172},
	annote = {cited By 0},
	file = {Full Text:/home/brendan/Zotero/storage/KPYBRAVT/Kogkas et al. - 2019 - Free-View, 3D Gaze-Guided Robotic Scrub Nurse.pdf:application/pdf}
}

@article{burger_two-handed_2012,
	title = {Two-handed gesture recognition and fusion with speech to command a robot},
	volume = {32},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862114714&doi=10.1007%2fs10514-011-9263-y&partnerID=40&md5=af76768e476b2086073af14be163518c},
	doi = {10.1007/s10514-011-9263-y},
	abstract = {Assistance is currently a pivotal research area in robotics, with huge societal potential. Since assistant robots directly interact with people, finding natural and easy-to-use user interfaces is of fundamental importance. This paper describes a flexible multimodal interface based on speech and gesture modalities in order to control our mobile robot named Jido. The vision system uses a stereo head mounted on a pan-tilt unit and a bank of collaborative particle filters devoted to the upper human body extremities to track and recognize pointing/symbolic mono but also bi-manual gestures. Such framework constitutes our first contribution, as it is shown, to give proper handling of natural artifacts (self-occlusion, camera out of view field, hand deformation) when performing 3D gestures using one or the other hand even both. A speech recognition and understanding system based on the Julius engine is also developed and embedded in order to process deictic and anaphoric utterances. The second contribution deals with a probabilistic and multi-hypothesis interpreter framework to fuse results from speech and gesture components. Such interpreter is shown to improve the classification rates of multimodal commands compared to using either modality alone. Finally, we report on successful live experiments in human-centered settings. Results are reported in the context of an interactive manipulation task, where users specify local motion commands to Jido and perform safe object exchanges. © 2011 Springer-Verlag.},
	number = {2},
	journal = {Autonomous Robots},
	author = {Burger, B. and Ferrané, I. and Lerasle, F. and Infantes, G.},
	year = {2012},
	keywords = {1bt\_include},
	pages = {129--147},
	annote = {cited By 54},
	file = {Burger et al. - 2012 - Two-handed gesture recognition and fusion with spe.pdf:/home/brendan/Zotero/storage/3V2SBA9V/Burger et al. - 2012 - Two-handed gesture recognition and fusion with spe.pdf:application/pdf}
}

@article{stuckler_dynamaid_2011,
	title = {Dynamaid, an anthropomorphic robot for research on domestic service applications [{Dynamaid}, čovjekoliki robot za istraživanje uslužnih djelatnosti u kućanstvima]},
	volume = {52},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855874333&doi=10.1080%2f00051144.2011.11828422&partnerID=40&md5=f12fa8ea6656537b9f48f31305947f47},
	doi = {10.1080/00051144.2011.11828422},
	abstract = {Domestic tasks require three main skills from autonomous robots: robust navigation, object manipulation, and intuitive communication with the users. Most robot platforms, however, support only one or two of the above skills. In this paper we present Dynamaid, a robot platform for research on domestic service applications. For robust navigation, Dynamaid has a base with four individually steerable differential wheel pairs, which allow omnidirectional motion. For mobile manipulation, Dynamaid is additionally equipped with two anthropomorphic arms that include a gripper, and with a trunk that can be lifted as well as twisted. For intuitive multimodal communication, the robot has a microphone, stereo cameras, and a movable head. Its humanoid upper body supports natural interaction. It can perceive persons in its environment, recognize and synthesize speech. We developed software for the tests of the RoboCup@Home competitions, which serve as benchmarks for domestic service robots. With Dynamaid and our communication robot Robotinho, our team Nimbro@Home took part in the RoboCup German Open 2009 and RoboCup 2009 competitions in which we came in second and third, respectively. We also won the innovation award for innovative robot design, empathic behaviors, and robot-robot cooperation.},
	number = {3},
	journal = {Automatika},
	author = {Stückler, J. and Behnke, S.},
	year = {2011},
	keywords = {1bt\_include},
	pages = {233--243},
	annote = {cited By 3},
	file = {Full Text:/home/brendan/Zotero/storage/JKSA8JTT/Stückler and Behnke - 2011 - Dynamaid, an anthropomorphic robot for research on.pdf:application/pdf}
}

@article{sun_rgb-d_2019,
	title = {{RGB}-{D} {Sensor} {Based} {Human} {Comfortable} {Following} {Behavior} for {Service} {Robots} in {Indoor} {Environments} [基于{RGB}-{D} 传感器的室内服务机器人舒适跟随方法]},
	volume = {41},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076858203&doi=10.13973%2fj.cnki.robot.180717&partnerID=40&md5=ac30b0abe44d04232a782091132d8e11},
	doi = {10.13973/j.cnki.robot.180717},
	abstract = {RGB-D Sensor Based Human Comfortable Following Behavior for Service Robots in Indoor Environments [基于RGB-D 传感器的室内服务机器人舒适跟随方法]},
	number = {6},
	journal = {Jiqiren/Robot},
	author = {Sun, Y. and Liu, J.},
	year = {2019},
	keywords = {1bt\_include},
	pages = {823--833},
	annote = {cited By 1}
}

@article{gorer_autonomous_2017,
	title = {An autonomous robotic exercise tutor for elderly people},
	volume = {41},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979584462&doi=10.1007%2fs10514-016-9598-5&partnerID=40&md5=33634c98067f624459debe8aa2dec3aa},
	doi = {10.1007/s10514-016-9598-5},
	abstract = {Ambient assisted living proposes to utilize technological solutions to sustain the well being of elderly people. In accordance with the vision of successful aging, we describe in this study an autonomous robotic exercise tutor for elderly people. The robot learns a set of physical exercises from a human demonstrator in an imitation framework, and performs these motions in an exercise scenario, while monitoring the elderly person to provide verbal feedback. We developed an exercise program in collaboration with a nursing home, and tested our system in a real world scenario with visitors of a day care center, over multiple sessions. We provide a detailed description of the system implementation, as well as our observations for the exercise program. For the study held in the day care center, video annotations and user self-assessments are evaluated to measure the overall performance of the system and to validate our approach. The analysis revealed that elderly people can successfully exercise with the assistance of the robot, while staying engaged with the system over multiple sessions. © 2016, Springer Science+Business Media New York.},
	number = {3},
	journal = {Autonomous Robots},
	author = {Görer, B. and Salah, A.A. and Akın, H.L.},
	year = {2017},
	keywords = {1bt\_include},
	pages = {657--678},
	annote = {cited By 28},
	file = {Görer et al. - 2017 - An autonomous robotic exercise tutor for elderly p.pdf:/home/brendan/Zotero/storage/8C7G8SFD/Görer et al. - 2017 - An autonomous robotic exercise tutor for elderly p.pdf:application/pdf}
}

@article{molina_collaborative_2018,
	title = {A collaborative approach for surface inspection using aerial robots and computer vision},
	volume = {18},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044254949&doi=10.3390%2fs18030893&partnerID=40&md5=6776b9df8b843543cecd0a4fe838217b},
	doi = {10.3390/s18030893},
	abstract = {Aerial robots with cameras on board can be used in surface inspection to observe areas that are difficult to reach by other means. In this type of problem, it is desirable for aerial robots to have a high degree of autonomy. A way to provide more autonomy would be to use computer vision techniques to automatically detect anomalies on the surface. However, the performance of automated visual recognition methods is limited in uncontrolled environments, so that in practice it is not possible to perform a fully automatic inspection. This paper presents a solution for visual inspection that increases the degree of autonomy of aerial robots following a semi-automatic approach. The solution is based on human-robot collaboration in which the operator delegates tasks to the drone for exploration and visual recognition and the drone requests assistance in the presence of uncertainty. We validate this proposal with the development of an experimental robotic system using the software framework Aerostack. The paper describes technical challenges that we had to solve to develop such a system and the impact on this solution on the degree of autonomy to detect anomalies on the surface. © 2018 by the authors. Licensee MDPI, Basel, Switzerland.},
	number = {3},
	journal = {Sensors (Switzerland)},
	author = {Molina, M. and Frau, P. and Maravall, D.},
	year = {2018},
	keywords = {1bt\_include, 1vision},
	annote = {cited By 7},
	file = {Full Text:/home/brendan/Zotero/storage/L62I53T6/Molina et al. - 2018 - A collaborative approach for surface inspection us.pdf:application/pdf}
}

@article{hsieh_development_2015,
	title = {The development of a robot-based learning companion: a user-centered design approach},
	volume = {23},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928277679&doi=10.1080%2f10494820.2013.765895&partnerID=40&md5=a0062044e5e334a124b4beadacdbb290},
	doi = {10.1080/10494820.2013.765895},
	abstract = {A computer-vision-based method is widely employed to support the development of a variety of applications. In this vein, this study uses a computer-vision-based method to develop a playful learning system, which is a robot-based learning companion named RobotTell. Unlike existing playful learning systems, a user-centered design (UCD) approach is applied to assess the interface design of the RobotTell System so that the human–robot interaction can be enhanced during the design process. More specifically, the interface of the RobotTell system is assessed based on Nielsen’s heuristics. Based on the results of Nielsen’s heuristic evaluation, three versions of the RobotTell system were produced. The implications for the design of these three versions of the RobotTell system are discussed. © 2013, © 2013 Taylor \& Francis.},
	number = {3},
	journal = {Interactive Learning Environments},
	author = {Hsieh, Y.-Z. and Su, M.-C. and Chen, S.Y. and Chen, G.-D.},
	year = {2015},
	keywords = {1bt\_include},
	pages = {356--372},
	annote = {cited By 2},
	file = {Hsieh et al. - 2015 - The development of a robot-based learning companio.pdf:/home/brendan/Zotero/storage/QI4MCZY7/Hsieh et al. - 2015 - The development of a robot-based learning companio.pdf:application/pdf}
}

@article{wu_toward_2020,
	title = {Toward {Design} of a {Drip}-{Stand} {Patient} {Follower} {Robot}},
	volume = {2020},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082745126&doi=10.1155%2f2020%2f9080642&partnerID=40&md5=1b1465430bbf1cd3892f558727ea0171},
	doi = {10.1155/2020/9080642},
	abstract = {A person following robot is an application of service robotics that primarily focuses on human-robot interaction, for example, in security and health care. This paper explores some of the design and development challenges of a patient follower robot. Our motivation stemmed from common mobility challenges associated with patients holding on and pulling the medical drip stand. Unlike other designs for person following robots, the proposed design objectives need to preserve as much as patient privacy and operational challenges in the hospital environment. We placed a single camera closer to the ground, which can result in a narrower field of view to preserve patient privacy. Through a unique design of artificial markers placed on various hospital clothing, we have shown how the visual tracking algorithm can determine the spatial location of the patient with respect to the robot. The robot control algorithm is implemented in three parts: (a) patient detection; (b) distance estimation; and (c) trajectory controller. For patient detection, the proposed algorithm utilizes two complementary tools for target detection, namely, template matching and colour histogram comparison. We applied a pinhole camera model for the estimation of distance from the robot to the patient. We proposed a novel movement trajectory planner to maintain the dynamic tipping stability of the robot by adjusting the peak acceleration. The paper further demonstrates the practicality of the proposed design through several experimental case studies. © 2020 Zewen Wu and Shahram Payandeh.},
	journal = {Journal of Robotics},
	author = {Wu, Z. and Payandeh, S.},
	year = {2020},
	keywords = {1bt\_include},
	annote = {cited By 0},
	file = {Full Text:/home/brendan/Zotero/storage/3FFFGIK9/Wu and Payandeh - 2020 - Toward Design of a Drip-Stand Patient Follower Rob.pdf:application/pdf}
}

@article{almonfrey_flexible_2018,
	title = {A flexible human detection service suitable for {Intelligent} {Spaces} based on a multi-camera network},
	volume = {14},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044780931&doi=10.1177%2f1550147718763550&partnerID=40&md5=0126fd712e881063a40f7099e7d82e1a},
	doi = {10.1177/1550147718763550},
	abstract = {The research field of the Intelligent Spaces has experienced increasing attention in the last decade. As an instance of the ubiquitous computing paradigm, the general idea is to extract information from the ambient and use it to interact and provide services to the actors present in the environment. The sensory analysis is mandatory in this area and humans are usually the principal actors involved. In this sense, we propose a human detector to be used in an Intelligent Space based on a multi-camera network. Our human detector is implemented in the same paradigm of our Intelligent Space. As a contribution of the present work, the human detector is designed to be a service that is scalable, reliable and parallelizable. It is also a concern of our service to be flexible, less structured as possible, attending different Intelligent Space applications and services, as well as their requirements. As it can be found in different everyday environments, a multi-camera system is used to overcome some difficulties traditionally faced by existing human detection approaches. To validate our approach, we implement three different applications that are proof of concept of many day-to-day real tasks. Two of these applications involve human–robot interaction. With respect to time and detection performance requirements, our human detection service has proved to be suitable for interacting with the other services of our Intelligent Space, in order to successfully complete the tasks of each application. © 2018, © The Author(s) 2018.},
	number = {3},
	journal = {International Journal of Distributed Sensor Networks},
	author = {Almonfrey, D. and do Carmo, A.P. and de Queiroz, F.M. and Picoreti, R. and Vassallo, R.F. and Salles, E.O.T.},
	year = {2018},
	keywords = {1bt\_include},
	annote = {cited By 3},
	file = {Full Text:/home/brendan/Zotero/storage/T2YSBFN3/Almonfrey et al. - 2018 - A flexible human detection service suitable for In.pdf:application/pdf}
}

@inproceedings{burke_multimodal_2013,
	title = {Multimodal interaction for human-robot teams},
	volume = {8741},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881182929&doi=10.1117%2f12.2016334&partnerID=40&md5=09abb1a0af68f43305d3f1bc36dce362},
	doi = {10.1117/12.2016334},
	abstract = {Unmanned ground vehicles have the potential for supporting small dismounted teams in mapping facilities, maintaining security in cleared buildings, and extending the team's reconnaissance and persistent surveillance capability. In order for such autonomous systems to integrate with the team, we must move beyond current interaction methods using headsdown teleoperation which require intensive human attention and affect the human operator's ability to maintain local situational awareness and ensure their own safety. This paper focuses on the design, development and demonstration of a multimodal interaction system that incorporates naturalistic human gestures, voice commands, and a tablet interface. By providing multiple, partially redundant interaction modes, our system degrades gracefully in complex environments and enables the human operator to robustly select the most suitable interaction method given the situational demands. For instance, the human can silently use arm and hand gestures for commanding a team of robots when it is important to maintain stealth. The tablet interface provides an overhead situational map allowing waypoint-based navigation for multiple ground robots in beyond-line-ofsight conditions. Using lightweight, wearable motion sensing hardware either worn comfortably beneath the operator's clothing or integrated within their uniform, our non-vision-based approach enables an accurate, continuous gesture recognition capability without line-of-sight constraints. To reduce the training necessary to operate the system, we designed the interactions around familiar arm and hand gestures. © 2013 SPIE.},
	booktitle = {Proceedings of {SPIE} - {The} {International} {Society} for {Optical} {Engineering}},
	author = {Burke, D. and Schurr, N. and Ayers, J. and Rousseau, J. and Fertitta, J. and Carlin, A. and Dumond, D.},
	year = {2013},
	keywords = {1bt\_include},
	annote = {cited By 5},
	file = {Burke et al. - 2013 - Multimodal interaction for human-robot teams.pdf:/home/brendan/Zotero/storage/G699P3A7/Burke et al. - 2013 - Multimodal interaction for human-robot teams.pdf:application/pdf}
}

@article{chien_navigating_2019,
	title = {Navigating a service robot for indoor complex environments},
	volume = {9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061032208&doi=10.3390%2fapp9030491&partnerID=40&md5=c9ff553e93a32dbb4100cf3058b87f21},
	doi = {10.3390/app9030491},
	abstract = {This paper investigates the use of an autonomous service robot in an indoor complex environment, such as a hospital ward or a retirement home. This type of service robot not only needs to plan and find paths around obstacles, but must also interact with caregivers or patients. This study presents a type of service robot that combines the image from a 3D depth camera with infrared sensors, and the inputs from multiple sonar sensors in an Adaptive Neuro-Fuzzy Inference System (ANFIS)-based approach in path planning. In personal contacts, facial features are used to perform person recognition in order to discriminate between staff, patients, or a stranger. In the case of staff, the service robot can perform a follow-me function if requested. The robot can also use an additional feature which is to classify the person's gender. The purpose of facial and gender recognition includes helping to present choices for suitable destinations to the user. Experiments were done in cramped but open spaces, as well as confined passages scenarios, and in almost all cases, the autonomous robots were able to reach their destinations. © 2019 by the authors.},
	number = {3},
	journal = {Applied Sciences (Switzerland)},
	author = {Chien, J.-C. and Dang, Z.-Y. and Lee, J.-D.},
	year = {2019},
	keywords = {1bt\_include},
	annote = {cited By 3},
	file = {Full Text:/home/brendan/Zotero/storage/U8RDTCPA/Chien et al. - 2019 - Navigating a service robot for indoor complex envi.pdf:application/pdf}
}

@article{zhao_identification_2014,
	title = {Identification method of interaction intention based on head and eye behaviors},
	volume = {35},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911469624&partnerID=40&md5=938180c71615cd66e6c184514fb6728d},
	abstract = {Human-service robot natural interaction plays an important role for service robot application, and the correct judgment of interactive intention is the foundation to improve interaction effect and reliability. This paper proposes a human-interactive collaboration interface-mobile service robot system; through detecting the head and eye behaviors based on machine vision approach, combining the feedback of robot operating scene, the user's head facing region is identified as the preliminary region of interest. Moreover, according to the direction of eye gaze the specific gazing target is determined; by this way, the user's interested target can be followed and the interaction intention can be identified. Finally, we established a test platform and some experiments were conducted; the results suggest that the proposed method can effectively identify the user's interactive intention, which provides a new way for the further study of the collaboration between service robot and human. ©, 2014, Science Press. All right reserved.},
	number = {10},
	journal = {Yi Qi Yi Biao Xue Bao/Chinese Journal of Scientific Instrument},
	author = {Zhao, Q. and Shao, H. and Lu, J.},
	year = {2014},
	keywords = {1bt\_include},
	pages = {2313--2320},
	annote = {cited By 5}
}

@inproceedings{schrader_gnc_2018,
	title = {{GNC} system design for the crew interactive mobile companion ({CIMON})},
	volume = {2018-October},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065329347&partnerID=40&md5=9efd84cc4d816d9e6958781e3b7efa23},
	abstract = {CIMON is a technology demonstration mission for the ISS launched in June 2018. Designed as a social companion for astronauts, the flying ball shall perform supporting crew activities like displaying procedures. Furthermore it is designed for social interaction and trained in small talk. CIMON consists of two parts: The free flying ball â€œFree Flyerâ€ on-board the ISS and the artificial intelligence located on ground. The guidance, navigation, and control system of the Free Flyer is designed in Matlab / Simulink and auto-coded as a software node for the Robot Operation System (ROS), which serves as a middleware for communication between the software in orbit and on ground. The GNC software runs on the Free Flyer's on-board computer and provides several operation modes that are triggered by the artificial intelligence. CIMON is designed to navigate autonomously through the Columbus module of the ISS. Vision based navigation is provided by a backwards pointing stereo camera and supported by an inertial measurement unit. Both relative and absolute navigation are available, allowing CIMON to follow commands like â€œCome closerâ€. The Free Flyer features 14 small fans arranged in seven tubes for attitude and position control. Providing a level of agility that satisfies a human counterpart despite severe thrust restrictions is a major challenge of the GNC system. CIMON may respond to the astronaut by turning around utilizing its voice tracking and face detection capabilities. If required, it may also approach the astronaut and place and hold itself in a convenient reading and chatting position. For verification, different test environments were implemented. Copyright © 2018 by the International Astronautical Federation (IAF).},
	booktitle = {Proceedings of the {International} {Astronautical} {Congress}, {IAC}},
	author = {SchrÃ¶der, V. and Regele, R. and Sommer, J. and Eisenberg, T. and Karrasch, C.},
	year = {2018},
	keywords = {1bt\_include},
	annote = {cited By 1}
}

@inproceedings{luo_real-time_2019,
	title = {A {Real}-time {Moving} {Target} {Following} {Mobile} {Robot} {System} with {Depth} {Camera}},
	volume = {491},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063805494&doi=10.1088%2f1757-899X%2f491%2f1%2f012004&partnerID=40&md5=0cee9e3903bf2a25dbccc8b17c551ce4},
	doi = {10.1088/1757-899X/491/1/012004},
	abstract = {An intelligent system that could obtain a certain moving target human and could follow the target in real-time by a mobile robot could be widely applied in manufacturing as assistant or service industry to provide a human robot interaction experience. In this work, an improved efficient human following algorithm for a mobile robotic system is proposed. The system consists by a mobile robot and a depth camera which overcomes target scale variation, missing and occlusion. An improved three-dimensional tracking method is presented with the depth camera. Besides, the depth camera also provides information to avoid obstacles in robot's path. The omnidirectional mobile robot in the system provides a flexible and fast motion responding solution. In the experiments, the proposed system is tested in different scenes and the results show the reliability of the 3D tracking system in terms of accuracy and robustness to the environment. © Published under licence by IOP Publishing Ltd.},
	booktitle = {{IOP} {Conference} {Series}: {Materials} {Science} and {Engineering}},
	author = {Luo, X. and Zhang, D. and Jin, X.},
	year = {2019},
	note = {Issue: 1},
	keywords = {1bt\_include},
	annote = {cited By 0},
	file = {Full Text:/home/brendan/Zotero/storage/XLKSXARU/Luo et al. - 2019 - A Real-time Moving Target Following Mobile Robot S.pdf:application/pdf}
}

@article{kahlouche_human_2021,
	title = {Human {Activity} {Recognition} {Based} on {Ensemble} {Classifier} {Model}},
	volume = {682},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092719860&doi=10.1007%2f978-981-15-6403-1_78&partnerID=40&md5=20976e09fe5f4eb202a7e9a741ea1ebb},
	doi = {10.1007/978-981-15-6403-1_78},
	abstract = {In this work, we address the problem of Human Activity Recognition (HAR), applied to service robot. In addition, a real time HRI system able to understand some common interactive human activities is designed. To classify activities into different classes, a combination of three supervised machine-learning algorithms: Support Vector Machine (SVM), Decision Tree (DT) and Artificial Neural Network (ANN) is used, based on the idea that a set of classifiers improve machine learning results. Our approach uses as input a view invariant 3D data of skeleton joints, which are rich body movement information recorded from a single Microsoft Kinect camera to create specific dataset of six interactive activities. The algorithm was able to successfully classify and recognize activities being performed in front of the camera. The system framework is realized on the Robot Operating System (ROS), and real-life activity interaction between our service robot and the user was conducted to demonstrate the effectiveness of the developed HRI system. © 2021, Springer Nature Singapore Pte Ltd.},
	journal = {Lecture Notes in Electrical Engineering},
	author = {Kahlouche, S. and Belhocine, M.},
	year = {2021},
	keywords = {1bt\_include},
	pages = {1121--1132},
	annote = {cited By 0},
	file = {Kahlouche and Belhocine - 2021 - Human Activity Recognition Based on Ensemble Class.pdf:/home/brendan/Zotero/storage/9G48G8V9/Kahlouche and Belhocine - 2021 - Human Activity Recognition Based on Ensemble Class.pdf:application/pdf}
}

@article{munaro_fast_2014,
	title = {Fast {RGB}-{D} people tracking for service robots},
	volume = {37},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905730835&doi=10.1007%2fs10514-014-9385-0&partnerID=40&md5=3bfb89284c5369b00db0b9d12d2d5725},
	doi = {10.1007/s10514-014-9385-0},
	abstract = {Service robots have to robustly follow and interact with humans. In this paper, we propose a very fast multi-people tracking algorithm designed to be applied on mobile service robots. Our approach exploits RGB-D data and can run in real-time at very high frame rate on a standard laptop without the need for a GPU implementation. It also features a novel depth-based sub-clustering method which allows to detect people within groups or even standing near walls. Moreover, for limiting drifts and track ID switches, an online learning appearance classifier is proposed featuring a three-term joint likelihood. We compared the performances of our system with a number of state-of-the-art tracking algorithms on two public datasets acquired with three static Kinects and a moving stereo pair, respectively. In order to validate the 3D accuracy of our system, we created a new dataset in which RGB-D data are acquired by a moving robot. We made publicly available this dataset which is not only annotated by hand, but the ground-truth position of people and robot are acquired with a motion capture system in order to evaluate tracking accuracy and precision in 3D coordinates. Results of experiments on these datasets are presented, showing that, even without the need for a GPU, our approach achieves state-of-the-art accuracy and superior speed. © 2014 Springer Science+Business Media New York.},
	number = {3},
	journal = {Autonomous Robots},
	author = {Munaro, M. and Menegatti, E.},
	year = {2014},
	keywords = {1bt\_include},
	pages = {227--242},
	annote = {cited By 112},
	file = {Munaro and Menegatti - 2014 - Fast RGB-D people tracking for service robots.pdf:/home/brendan/Zotero/storage/PCP445CZ/Munaro and Menegatti - 2014 - Fast RGB-D people tracking for service robots.pdf:application/pdf}
}

@article{lima_socrobhome_2019,
	title = {{SocRob}@{Home}: {Integrating} {AI} {Components} in a {Domestic} {Robot} {System}},
	volume = {33},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088042613&doi=10.1007%2fs13218-019-00618-w&partnerID=40&md5=ffcd63d8ebb424e61bd8a0c2565bb59f},
	doi = {10.1007/s13218-019-00618-w},
	abstract = {This paper describes the SocRob@Home robot system, consisting of a mobile robot (MBOT) equipped with several sensors and actuators, including a manipulator arm, and several software modules that provide the skills and capability to perform domestic tasks while interacting with humans in a domestic environment. We describe the whole system holistically, explaining how it integrates the contributing modules, and then we focus on the most relevant sub-systems, pointing out the original contributions of our research and development on the system in the last 5 years. The robot system includes metric and semantic mapping, several navigation modes (way-point navigation, person following and multi-sensor obstacle detection and avoidance), vision-based object detection, recognition, servoing and grasping, speech understanding, task planning and task execution. The robot system is mostly activated by speech commands from a human, and these commands, after being interpreted, are executed by the robot sub-systems, coordinated by a task executor. Lessons learned during the development and use of this system, which are useful as guidelines for the development of similar robot systems, are provided. MBOT’s performance is assessed using the task benchmarks scoring system of the European Robotics League competitions on Consumer Service robots. © 2019, Gesellschaft für Informatik e.V. and Springer-Verlag GmbH Germany, part of Springer Nature.},
	number = {4},
	journal = {KI - Kunstliche Intelligenz},
	author = {Lima, P.U. and Azevedo, C. and Brzozowska, E. and Cartucho, J. and Dias, T.J. and Gonçalves, J. and Kinarullathil, M. and Lawless, G. and Lima, O. and Luz, R. and Miraldo, P. and Piazza, E. and Silva, M. and Veiga, T. and Ventura, R.},
	year = {2019},
	keywords = {1bt\_include},
	pages = {343--356},
	annote = {cited By 0},
	file = {Lima et al. - 2019 - SocRob@Home Integrating AI Components in a Domest.pdf:/home/brendan/Zotero/storage/XS4YQXEG/Lima et al. - 2019 - SocRob@Home Integrating AI Components in a Domest.pdf:application/pdf}
}

@inproceedings{liu_deep_2019,
	title = {Deep learning-based human motion prediction considering context awareness for human-robot collaboration in manufacturing},
	volume = {83},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070551266&doi=10.1016%2fj.procir.2019.04.080&partnerID=40&md5=1998c98f019b6e1cecd543b931accd02},
	doi = {10.1016/j.procir.2019.04.080},
	abstract = {The interest of human-robot collaboration (HRC) for intelligent manufacturing service system is gradually increasing. Fluent human-robot coexistence in manufacturing requires accurate estimation of the human motion intention so that the efficiency and safety of HRC can be guaranteed. Human motion is mainly defined as the sequential positions of the joints of human skeletons among traditional motion prediction solutions, which lead to a deficiency of tools or product components holding in hand. Context awareness based temporal processing is the key to evaluating human motion before the accomplishment of it, so as to save time as well as recognize the intention of the human. In this paper, a deep learning system combing convolutional neural network (CNN) and long short-term memory network (LSTM) towards vision signals is explored to predict human motion accurately. Creatively, this paper utilizes LSTM to extract temporal patterns of human motion automatically outputting the prediction result before motion takes place. Not only does it avoid complex feature extraction due to its end-to-end characteristic, but provide a natural interaction between human and robot without wearable devices or tags that may become a burden for the former. A case study of desktop computer product disassembly is executed to demonstrate the feasibility of the recommended method. Experimental performance proves that our method outperforms the other three optimization algorithms on the prediction accuracy. © 2019 The Authors. Published by Elsevier B.V.},
	booktitle = {Procedia {CIRP}},
	author = {Liu, Z. and Liu, Q. and Xu, W. and Liu, Z. and Zhou, Z. and Chen, J.},
	year = {2019},
	keywords = {1bt\_include},
	pages = {272--278},
	annote = {cited By 4},
	file = {Liu et al. - 2019 - Deep learning-based human motion prediction consid.pdf:/home/brendan/Zotero/storage/QNKLJIQU/Liu et al. - 2019 - Deep learning-based human motion prediction consid.pdf:application/pdf}
}

@article{bousquet-jette_fast_2017,
	title = {Fast scene analysis using vision and artificial intelligence for object prehension by an assistive robot},
	volume = {63},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019041198&doi=10.1016%2fj.engappai.2017.04.015&partnerID=40&md5=823b5b73597cf97e6952a6ab197315e1},
	doi = {10.1016/j.engappai.2017.04.015},
	abstract = {Robotic assistance for people affected by motor deficits is a fast growing field. In this context, two major challenges remain in terms of automated scene analysis and automated object prehension. More specifically, the most robust of current segmentation methods are still computationally intensive, preventing the automation of objects prehension from being fast enough to be considered acceptable as an everyday technical-aid. The objective of this study is to develop a fast scene analysis using vision and artificial intelligence for object prehension by an assistive robot. The solution developed in this paper aims at facilitating human-machine interaction by enabling users to easily communicate their needs to the technical aid. To achieve this, this paper proposes several novelties in three interconnected domains: scene segmentation, prehension and recognition of 3D objects. A novel technic, inspired by mechanical probing, is developed for scenes probing to detect objects. A simple, fast and effective decision tree is proposed for object prehension. Finally, the physical characteristics of the 3D objects are directly used in the neural network without using discriminants features descriptors. The results obtained in this paper have shown that scene analysis for robotic object prehension in cooperation with a user can be performed with effective promptness. Indeed, the system requires on average 0.6 s to analyze an object in a scene. With the JACO robotic assistance arm, the system can pick up a requested object in 15 s while moving at 50 mm/s, which may be greatly improved upon using a faster robot. The system performance averages 83\% accuracy for object recognition and is able to use a decision tree to select a simple approach path for the robot end-effector towards a desired object. This system, in combination with an assistive robot, has great potential for providing users suffering from musculoskeletal disorders with improved autonomy and independence, and for encouraging sustained usage of this type of technical aids. © 2017 Elsevier Ltd},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Bousquet-Jette, C. and Achiche, S. and Beaini, D. and Law-Kam Cio, Y.S. and Leblond-Ménard, C. and Raison, M.},
	year = {2017},
	keywords = {1bt\_include, 1vision},
	pages = {33--44},
	annote = {cited By 7},
	file = {Bousquet-Jette et al. - 2017 - Fast scene analysis using vision and artificial in.pdf:/home/brendan/Zotero/storage/RJMSA2QF/Bousquet-Jette et al. - 2017 - Fast scene analysis using vision and artificial in.pdf:application/pdf}
}

@inproceedings{foukarakis_robot-based_2016,
	title = {A robot-based application for physical exercise training},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979500650&doi=10.5220%2f0005800300450052&partnerID=40&md5=433373a41ab62a0c9326c1805a6e0969},
	doi = {10.5220/0005800300450052},
	abstract = {According to studies, performing physical exercise is beneficial for reducing the risk of falling in the elderly and prolonging their stay at home. In addition, regular exercising helps cognitive function and increases positive behaviour for seniors with cognitive impairment and dementia. In this paper, a fitness application integrated into a service robot is presented. Its aim is to motivate the users to perform physical training by providing relevant exercises and useful feedback on their progress. The application utilizes the robot vision system to track and recognize user movements and activities and supports multimodal interaction with the user. The paper describes the design challenges, the system architecture, the user interface and the human motion capturing module. Additionally, it discusses some results from user testing in laboratory and home-based trials. Copyright © 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.},
	booktitle = {{ICT4AWE} 2016 - 2nd {International} {Conference} on {Information} and {Communication} {Technologies} for {Ageing} {Well} and e-{Health}, {Proceedings}},
	author = {Foukarakis, M. and Adami, I. and Ioannidi, D. and Leonidis, A. and Michel, D. and Qammaz, A. and Papoutsakis, K. and Antona, M. and Argyros, A.},
	year = {2016},
	keywords = {1bt\_include},
	pages = {45--52},
	annote = {cited By 4},
	file = {Foukarakis et al. - 2016 - A robot-based application for physical exercise tr.pdf:/home/brendan/Zotero/storage/ZBZK2IJY/Foukarakis et al. - 2016 - A robot-based application for physical exercise tr.pdf:application/pdf}
}

@article{haddadin_towards_2014,
	title = {Towards the robotic co-worker},
	volume = {90},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927673518&doi=10.1007%2f978-3-642-40308-8_8&partnerID=40&md5=a6172d577cd8f531a5eb6d69780a2c53},
	doi = {10.1007/978-3-642-40308-8_8},
	abstract = {Various human-friendly motion control methods were presented and analyzed. These are independently useful tools for numerous applications as they open up entirely new robot behaviors. However, due to their complex interrelationship in this chapter it is discussed how to integrate the presented methods into a more general hybrid state-based control architecture. Even though the focus is on robotic co-workers, the elaborated schemes are also applicable to service robots. The implementation of such a sensor-based robotic co-worker that brings robots closer to humans in industrial settings and achieve close cooperation is currently a challenging goal in robotics. Pioneering examples of intimate collaboration between human and robot, whose origin can be found in [10], are Intelligent Assist Devices (IADs), as the skill assist described in [18]. In 1983 a method was proposed at DLR for allowing immediate “programming by touch” of a robot through a force/torquesensor- ball [8], see Fig. 8.1 (left). Despite being a common vision in robotics the robotic co-worker has not become reality yet, as there are various open questions still to be answered. Apart from the control and safety aspects, the architectural level also poses significant challenges. © Springer-Verlag Berlin Heidelberg 2014.},
	journal = {Springer Tracts in Advanced Robotics},
	author = {Haddadin, S.},
	year = {2014},
	keywords = {1bt\_include},
	pages = {195--215},
	annote = {cited By 1},
	file = {Accepted Version:/home/brendan/Zotero/storage/QMNBGGEK/Haddadin - 2014 - Towards the robotic co-worker.pdf:application/pdf}
}

@article{phong_vietnamese_2020,
	title = {Vietnamese service robot based on artificial intelligence},
	volume = {9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084409952&doi=10.18178%2fijmerr.9.5.701-708&partnerID=40&md5=f6ce05677c0de4a17f4f59f578b7ba12},
	doi = {10.18178/ijmerr.9.5.701-708},
	abstract = {Service robot have been designed and developed for different objectives and requirements. Since robotic revolution, Vietnam is one of the most influenced among South-east Asia countries in developing artificial intelligence. People believe that robot will replace all blue-collars in almost company, even hospital and school in next few decades, therefore developers are trying to improve natural language interaction between human and robot, especially helping new vision for Vietnamese robotics. We have built a robot application that capable of understand Vietnamese natural language, there are four tasks which was mentioned solving the problems, even more AI method. Deep learning, on the other hand, is a sub-field of machine learning. In this paper, with these algorithms, artificial intelligent supported is much complex, people can communicate naturally with robot, not only English but also Vietnamese at well. In this paper, we will introduce the specification and intelligent interaction processing in naturally Vietnamese. © 2020 by the authors.},
	number = {5},
	journal = {International Journal of Mechanical Engineering and Robotics Research},
	author = {Phong, N.T.T. and Nam, L.H.T. and Thinh, N.T.},
	year = {2020},
	keywords = {1bt\_include},
	pages = {701--708},
	annote = {cited By 0},
	file = {Phong et al. - 2020 - Vietnamese service robot based on artificial intel.pdf:/home/brendan/Zotero/storage/53U7WEWV/Phong et al. - 2020 - Vietnamese service robot based on artificial intel.pdf:application/pdf}
}

@article{zhang_indoor_2018,
	title = {Indoor omni-directional mobile robot that track independently},
	volume = {29},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047059475&doi=10.3966%2f199115992018042902013&partnerID=40&md5=967a7a21fc5f953c4b292c3eaf1b887b},
	doi = {10.3966/199115992018042902013},
	abstract = {Service robots want to achieve intelligent interaction, self-tracking is the foundation. At present, the tracking of indoor robots is mainly for human body tracking or object tracking with auxiliary markers, only one type of target can be tracked. In order to solve the above shortcomings, this paper designed an automatic omni-directional robot, using a common camera, according the different target using different strategies to achieve the tracking of any tracking, regardless of their status. The tracking strategy can be divided into two broad categories. One is tracking the human. Using based classifier face tracking and feature point tracking, Set the face tracking as a high priority of tracking, and the feature point tracking is added to ensure the tracking integrity when face blind spots appear. Another is the tracking of objects in any state. In this case, the feature points are used to track the target feature points, using Shi-Tomasi algorithm and Lucas-Kanade algorithm. The 2D coordinates of target sub-pixel level accuracy are obtained. Combined with laser range finder, build three-dimensional coordinate positioning. Finally, this paper verifies the feasibility of the system through experiments. The success rate of tracking is over 90\%, which has high reliability. © 2018 Computer Society of the Republic of China. All Rights Reserved.},
	number = {2},
	journal = {Journal of Computers (Taiwan)},
	author = {Zhang, K. and Zhang, L.},
	year = {2018},
	keywords = {1bt\_include},
	pages = {118--135},
	annote = {cited By 1},
	file = {Zhang and Zhang - 2018 - Indoor omni-directional mobile robot that track in.pdf:/home/brendan/Zotero/storage/WIMI26VY/Zhang and Zhang - 2018 - Indoor omni-directional mobile robot that track in.pdf:application/pdf}
}

@article{yang_novel_2018,
	title = {A novel gesture recognition system for intelligent interaction with a nursing-care assistant robot},
	volume = {8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057127800&doi=10.3390%2fapp8122349&partnerID=40&md5=be5390fd6b4187a38f74ad145e41d6df},
	doi = {10.3390/app8122349},
	abstract = {The expansion of nursing-care assistant robots in smart infrastructure has provided more applications for homecare services, which has raised new demands for smart and natural interaction between humans and robots. This article proposed an innovative hand motion trajectory (HMT) gesture recognition system based on background velocity features. Here, a new wearable wrist-worn camera prototype for gesture's video collection was designed, and a new method for the segmentation of continuous gestures was shown. Meanwhile, a nursing-care assistant robot prototype was designed for assisting the elderly, which is capable of carrying the elderly with omnidirectional motion and grabbing the specified object at home. In order to evaluate the performance of the gesture recognition system, 10 special gestures were defined as the move commands for interaction with the robot, and 1000 HMT gesture samples were obtained from five subjects for leave-one-subject-out (LOSO) cross-validation classification with an average recognition accuracy of up to 97.34\%. Moreover, the performance and practicability of the proposed system were further demonstrated by controlling the omnidirectional movement of the nursing-care assistant robot using the predefined gesture commands. © 2018 by the authors. Licensee MDPI, Basel, Switzerland.},
	number = {12},
	journal = {Applied Sciences (Switzerland)},
	author = {Yang, G. and Lv, H. and Chen, F. and Pang, Z. and Wang, J. and Yang, H. and Zhang, J.},
	year = {2018},
	keywords = {1bt\_include},
	annote = {cited By 7},
	file = {Full Text:/home/brendan/Zotero/storage/4PYG2KGC/Yang et al. - 2018 - A novel gesture recognition system for intelligent.pdf:application/pdf}
}

@article{zillich_software_2012,
	title = {A software integration framework for cognitive systems},
	volume = {336 CCIS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868376569&doi=10.1007%2f978-3-642-34781-8_9&partnerID=40&md5=8a7329fa816c3d29111a15c9d1461a78},
	doi = {10.1007/978-3-642-34781-8_9},
	abstract = {Handling complex tasks with increasing levels of autonomy requires robotic systems to incorporate a large number of different functionalities at various levels of abstraction, such as localisation, navigation, object detection and tracking, human robot interaction including speech and gesture recognition as well as high level reasoning and planning. The interaction between functionalities in these cognitive robotics systems not only requires integration at a technical level but more importantly at an organisational and semantic level. Within this contribution, these cognitive functionalities are encapsulated in software components with the objective to provide clearly specified interfaces to allow reuse in other cognitive vision or robotics systems. To arrive at the level of building a system from these functionalities, it is considered essential to provide a framework that coordinates the components. Two principles organise the components: (1) the service principle uses a "yellow pages" directory to announce its capabilities and to select other components, and (2) the hierarchy principle orders components along data abstraction from signal to symbolic levels and ascertains that system response is reactive. The proposed system is demonstrated in a context-oriented system for activity interpretation involving functionalities such as tracking, object and gesture recognition, spatio-temporal object relationships and reasoning to extract symbolic activity descriptions. © 2012 Springer-Verlag.},
	journal = {Communications in Computer and Information Science},
	author = {Zillich, M. and Ponweiser, W. and Vincze, M.},
	year = {2012},
	keywords = {1bt\_include},
	pages = {121--135},
	annote = {cited By 0},
	file = {Zillich et al. - 2012 - A software integration framework for cognitive sys.pdf:/home/brendan/Zotero/storage/Z6HN2XNK/Zillich et al. - 2012 - A software integration framework for cognitive sys.pdf:application/pdf}
}

@article{tan_safety_2010,
	title = {Safety strategy for human-robot collaboration: {Design} and development in cellular manufacturing},
	volume = {24},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951841739&doi=10.1163%2f016918610X493633&partnerID=40&md5=dd2a40fe6091c2799e897aabe09a04e7},
	doi = {10.1163/016918610X493633},
	abstract = {Our research aims to design and develop a safety strategy for a human-robot collaboration system. Although robotic assistance in a cellular manufacturing system is promising, safety is the uppermost consideration before it can be materialized. Five main safety designs are developed in this work. (i) Safe working areas for humans and robots. (ii) To control the behavior of the robot based on the collaboration requirements, light curtains defined safe collaborative working zones. (iii) Additionally, the robot system was developed using safe mechanical design and Dual Check Safety control strategies in terms of robot speed and travel area to minimize collaboration risks. (iv) A vision system using IP cameras was developed to monitor operator safety conditions by measuring the body posture and position of the operator. (v) The operation control system coordinated the collaborative flow between the operator and robot system. Apart from these developments, risk assessments were conducted to evaluate the safety design of the system, and a mental safety study was performed to investigate robot motion speed and working distance on the operator's physiological effects. Our findings demonstrate the feasibility of the prototype system to safely perform assembly operations. © 2010 Koninklijke Brill NV, Leiden and The Robotics Society of Japan.},
	number = {5-6},
	journal = {Advanced Robotics},
	author = {Tan, J.T.C. and Duan, F. and Kato, R. and Arai, T.},
	year = {2010},
	keywords = {1bt\_include},
	pages = {839--860},
	annote = {cited By 20},
	file = {Tan et al. - 2010 - Safety strategy for human-robot collaboration Des.pdf:/home/brendan/Zotero/storage/9XC4TUBS/Tan et al. - 2010 - Safety strategy for human-robot collaboration Des.pdf:application/pdf}
}

@article{aranda_friendly_2010,
	title = {Friendly human-machine interaction in an adapted robotized kitchen},
	volume = {6179 LNCS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954902636&doi=10.1007%2f978-3-642-14097-6_50&partnerID=40&md5=3010231c1bb97cfa8546e2eb024056aa},
	doi = {10.1007/978-3-642-14097-6_50},
	abstract = {The concept and design of a friendly human-machine interaction system for an adapted robotized kitchen is presented. The kitchen is conceived in a modular way in order to be adaptable to a great diversity in level and type of assistance needs. An interaction manager has been developed which assist the user to control the system actions dynamically according to the given orders and the present state of the environment. Real time enhanced perception of the scenario is achieved by means of a 3D computer vision system. The main goal of the present project is to provide this kitchen with the necessary intelligent behavior to be able to actuate efficiently by interpreting the users' will. © 2010 Springer-Verlag Berlin Heidelberg.},
	number = {PART 1},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Aranda, J. and Vinagre, M. and Martín, E.X. and Casamitjana, M. and Casals, A.},
	year = {2010},
	keywords = {1bt\_include},
	pages = {312--319},
	annote = {cited By 3},
	file = {Aranda et al. - 2010 - Friendly human-machine interaction in an adapted r.pdf:/home/brendan/Zotero/storage/ZNJ2DVU3/Aranda et al. - 2010 - Friendly human-machine interaction in an adapted r.pdf:application/pdf}
}

@article{giuliani_design_2010,
	title = {Design principles for safety in human-robot interaction},
	volume = {2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857625100&doi=10.1007%2fs12369-010-0052-0&partnerID=40&md5=1e0b3124361f249d526b1cf8d87be6c8},
	doi = {10.1007/s12369-010-0052-0},
	abstract = {The interaction of humans and robots has the potential to set new grounds in industrial applications as well as in service robotics because it combines the strengths of humans, such as flexibility and adaptability, and the strengths of robots, such as power and precision. However, for a successful interaction the safety of the human has to be guaranteed at all times. This goal can be reached by the use of specialised robot hardware but we argue that safety in human-robot interaction can also be done with regular industrial robots, if they are equipped with additional sensors to track the human's position and to analyse the human's verbal and non-verbal utterances, and if the software that is controlling the robot is especially designed towards safety in the interaction. For this reason, we propose three design principles for an increased safety in robot architectures and any other software component that controls a robot for human-robot interaction: robustness, fast reaction time, and context awareness. We present a robot architecture that is based on these principles and show approaches for speech processing, vision processing, and robot control that also follow these guidelines. © Springer Science \& Business Media BV 2010.},
	number = {3},
	journal = {International Journal of Social Robotics},
	author = {Giuliani, M. and Lenz, C. and Müller, T. and Rickert, M. and Knoll, A.},
	year = {2010},
	keywords = {1bt\_include},
	pages = {253--274},
	annote = {cited By 26},
	file = {Giuliani et al. - 2010 - Design principles for safety in human-robot intera.pdf:/home/brendan/Zotero/storage/2KW9THMP/Giuliani et al. - 2010 - Design principles for safety in human-robot intera.pdf:application/pdf}
}