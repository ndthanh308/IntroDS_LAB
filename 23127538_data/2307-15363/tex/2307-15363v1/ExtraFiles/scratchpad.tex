All papers related to gesture recognition are listed here: ~\cite{luo_tracking_2011,milligan_selecting_2011,lichtenstern_prototyping_2012, nishiyama_human_2013, alonso-mora_human_2014, canal_gesture_2015, fareed_gesture_2015, song_towards_2017,lima_real-time_2019, martin_real-time_2019, zhang_interactive_2018, chen_human-following_2019, miller_self-driving_2019, droeschel_towards_2011, canal_gesture_2015, moh_gesture_2019, sriram_mobile_2019, kalidolda_towards_2018,paulo_vision-based_2012,lam_real-time_2011, cicirelli_kinect-based_2015,fareed_gesture_2015,lalejini_evaluation_2015, ghandour_human_2017, long_kinect-based_2018,pentiuc_drive_2018, zhang_interactive_2018,chen_human-following_2019, miller_self-driving_2019, li_real-time_2019, chen_approaches_2010,manigandan_wireless_2010,faudzi_real-time_2012, lavanya_gesture_2018, waskito_wheeled_2020, xu_skeleton_2020,park_real-time_2011, van_den_bergh_real-time_2011, yoshida_evaluation_2011, abidi_human_2013, prediger_robot-supported_2014, chen_stereovision-only_2014, jevtic_comparison_2015, tolgyessy_foundations_2017,wang_wheeled_2013,fang_vehicle-mounted_2019,fujii_gesture_2014,ghandour_human_2017,yoshida_evaluation_2011,mao_medical_2018} (Only half done). 
\\
Test for importing citations:
\\
\cite{arumbakkam_multi-modal_2010,budiharto_indoor_2010,chen_integrated_2010,chen_approaches_2010,couture-beil_selecting_2010,fahn_real-time_2010,kobayashi_people_2010,manigandan_wireless_2010,martin_estimation_2010,sisbot_synthesizing_2010,sosnowski_mirror_2010,yun_robust_2010,tan_safety_2010,droeschel_towards_2011,luo_human_2010,masmoudi_expressive_2011,broccia_gestural_2011,ding_optimizing_2011,igorevich_behavioral_2011,jia_autonomous_2011,lam_real-time_2011,luo_tracking_2011,milligan_selecting_2011,nair_3d_2011,park_real-time_2011,saegusa_cognitive_2011,van_den_bergh_real-time_2011,yoshida_evaluation_2011,alvarez-santos_feature_2012,burger_two-handed_2012,chen_person_2012,cheng_design_2012,csapo_multimodal_2012,faudzi_real-time_2012,foster_two_2012,gori_all_2012,lambrecht_spatial_2012,lichtenstern_prototyping_2012,mead_probabilistic_2012,paulo_vision-based_2012,stipancic_programming_2012,torres_implementation_2012,wu_accompanist_2012,bdiwi_handing-over_2013,abidi_human_2013,cheng_multiple-robot_2013,ferrer_robot_2013,indrajit_development_2013,nishiyama_human_2013,pereira_human-robot_2013,qian_visually_2013,tao_multilayer_2013,wang_vision-guided_2013,wang_wheeled_2013,weinrich_appearance-based_2013,werner_evaluation_2013,yang_study_2013,yun_robotic_2013,alonso-mora_human_2014,castellano_context-sensitive_2014,chen_stereovision-only_2014,do_human-robot_2014,fujii_gesture_2014,hu_design_2014,miyoshi_above_2014,morato_toward_2014,munaro_fast_2014,nguyen_audio-visual_2014,petric_online_2014,prediger_robot-supported_2014,scheggi_human-robot_2014,shieh_fuzzy_2014,taheri_social_2014,tseng_multi-human_2014,xu_online_2014,ali_improved_2015,canal_gesture_2015,cech_active-speaker_2015,cherubini_unified_2015,choudhary_real_2015,cicirelli_kinect-based_2015,fareed_gesture_2015,gao_humanoid_2015,gupta_robust_2015,jevtic_comparison_2015,katsuki_high-speed_2015,lalejini_evaluation_2015,li_visual_2015,quintero_visual_2015,saleh_nonverbal_2015,vircikova_teach_2015,yang_real-time_2015,bayram_audio-visual_2016,christiernin_interacting_2016,ehlers_human-robot_2016,hassan_computationally_2016,ikai_robot_2016,kahily_real-time_2016,ke_vision_2016,koppula_anticipating_2016,koustoumpardis_human_2016,mollaret_multi-modal_2016,moreno_path_2016,potdar_learning_2016,ratul_gesture_2016,talebpour-board_2016,vasconcelos_socially_2016,vaufreydaz_starting_2016,zhang_optimal_2016,angonese_multiple_2017,arenas_deep_2017,bellarbi_social_2017,bilac_gaze_2017,deepan_raj_static_2017,dometios_real-time_2017,ghandour_human_2017,gorer_autonomous_2017,han_human_2017,ju_integrative_2017,lee_learning_2017,maurtua_natural_2017,meghdari_real-time_2017,shariatee_safe_2017,song_towards_2017,tolgyessy_foundations_2017,wang_collision-free_2017,yao_monocular_2017,yoo_gaze_2017,zhu_robust_2017,vasquez_deep_2017,araiza-lllan_dynamic_2018,bolano_towards_2018,bothe_effective_2018,du_online_2018,efthymiou_multi-_2018,gong_research_2018,gui_teaching_2018,hong_interactive_2018,kalidolda_towards_2018,lavanya_gesture_2018,li_learning_2018,long_kinect-based_2018,mao_medical_2018,pasinetti_development_2018,pentiuc_drive_2018,sorostinean_activity_2018,weber_follow_2018,yuan_development_2018,zhang_indoor_2018,zhang_interactive_2018,zhang_automating_2018,cazzato_real-time_2019,chalvatzaki_learn_2019,chen_wristcam_2019,chen_human-following_2019,chien_navigating_2019,condes_person_2019,fang_vehicle-mounted_2019,haghighi_integration_2019,jarosz_detecting_2019,kogkas_free-view_2019,landi_prediction_2019,li_inferring_2019,li_real-time_2019,li_cnn_2019,lima_real-time_2019,luo_human-robot_2019,luo_real-time_2019,martin_real-time_2019,mazhar_real-time_2019,miller_self-driving_2019,moh_gesture_2019,scimmi_experimental_2019,sriram_mobile_2019,valle_personalized_2019,vysocky_interaction_2019,wu_improved_2019,yang_socially-aware_2019,zardykhan_collision_2019,zhang_vision-based_2019,anuradha_human_2020,augello_towards_2020,avioz-sarig_robotic_2020,bingol_practical_2020,chan_collision-free_2020,ferraguti_safety_2020,gao_hand_2020,hwang_interactions_2020,lang_research_2020,lee_visual_2020,lee_real-time_2020,liu_dynamic_2020,lu_research_2020,mronga_constraint-based_2020,phong_vietnamese_2020,tarbouriech_bi-objective_2020,van_den_broek_ergonomic_2020,waskito_wheeled_2020,wu_toward_2020,xu_skeleton_2020,yuan_natural_2020,zhang_human_2020,gao_user_2020,almonfrey_flexible_2018,monajjemi_hri_2013,jindai_small-size_2010,hasanuzzaman_adaptation_2010,mendez-polanco_detection_2010,weiss_robots_2010,pustianu_mobile_2011,sanna_kinect-based_2012,celik_development_2012,gu_human_2012,lee_human_2012,de_luca_integrated_2012,lee_interactive_2012,konda_real_2012,li_cyber-physical_2013,cid_real_2013,das_attracting_2013,schmidt_contact-less_2013,pfeil_exploring_2013,hartmann_feasibility_2013,granata_human_2013,zhao_interactive_2013,anzalone_multimodal_2013,hegger_people_2013,moe_real-time_2013,baron_remote_2013,ozgur_natural_2014,costante_personalizing_2014,barros_real-time_2014,farulla_real-time_2014,chao_robotic_2014,saveriano_safe_2014,batista_probabilistic_2015,nazari_simplified_2015,zhang_adaptive_2015,saffar_context-based_2015,saichinmayi_gesture_2015,benabdallah_kinect-based_2015,obo_robot_2015,voisan_ros-based_2015,fuad_skeleton_2015,monajjemi_uav_2015,nagi_wisdom_2015,guo_control_2016,simul_support_2016,liu_interactive_2016,maraj_application_2016,manitsaris_fingers_2016,mateus_human-aware_2016,li_id-match_2016,silva_mirroring_2016,zambelli_multimodal_2016,zhu_real-time_2016,agrigoroaie_enrichme_2016,vignolo_computational_2017,barz_evaluating_2017,valipour_incremental_2017,maher_realtime_2017,rehman_target_2017,devanne_co-design_2018,shakev_autonomous_2018,lathuiliere_deep_2018,bras_gesture_2018,mohaimenianpour_hands_2018,costanzo_multimodal_2019,azari_commodifying_2019,yu_efficiency_2019,belo_facial_2019,yu_interactive_2019,paetzel_let_2019,chen_online_2019,svarny_safe_2019,ali_smart_2019,gemerek_video-guided_2019,sun_visual_2019,muller_multi-modal_2020,kawasaki_multimodal_2020,yan_optimization_2020,sanchez-matilla_benchmark_2020,nascimento_collision_2020,fallahinia_comparison_2020,chen_design_2020,pang_efficient_2020,angani_human_2020,medeiros_human-drone_2020,yue_human-robot_2020,terreran_low-cost_2020,hsu_real-time_2020,de_schepper_towards_2020,uribe_mobile_2011,hafiane_3d_2013,mohammad_tele-operation_2013,xiao_human-robot_2014,quintero_interactive_2014,zhao_intuitive_2016,bouteraa_gesture-based_2017,bai_kinect-based_2018,abiddin_development_2019,sripada_teleoperation_2019}



%This is instead in the graph above - Gesture recognition was paired with different camera systems: depth aware cameras ($N=54$, Microsoft Kinect ($N=48$)), single camera set-ups ($N=21$), stereo cameras ($N=4$), and omni-directional cameras ($N=1$). 
%Gestures often involved pointing ($N=24$, ~\cite{martin_estimation_2010, luo_tracking_2011, milligan_selecting_2011, abidi_human_2013, prediger_robot-supported_2014,jevtic_comparison_2015, maurtua_natural_2017, tolgyessy_foundations_2017}), making a fist ($N=13$), waving ($N=8$, ~\cite{couture-beil_selecting_2010, cheng_design_2012,tao_multilayer_2013, fujii_gesture_2014,canal_gesture_2015}), finger count ($N=11$), thumbs up ($N=3$,  ~\cite{manigandan_wireless_2010, faudzi_real-time_2012, vysocky_interaction_2019}), 'stop' signal~\cite{faudzi_real-time_2012, ehlers_human-robot_2016} and other gestures ($N=21$, including gestures from body pose). 




Identified domains included manufacturing and warehouses, field (e.g. human-robot teaming /  human-supervisor [operator] - include above or below ground/water, indoor/domestic, (e.g. social robots, humanoid robots)

Application areas: Handling, Welding, Assembly 
RV Techniques: Image Processing, Feature Detection, Segmentation, 

Introduction to the concept and topic. What is it, why is it important, papers that describe gesture recognition (with or without robot). 
What are some of the main use cases or domain areas that gesture recognition is used - (Operation and Communication). 
This stage can include papers that are broader in domain. Mention some reviews but briefly if they are older than 10 years. 
1-3 sentences per paper at this stage (unless the paper is highly comprehensive and describes exactly what you want to say, then feel free to add more than 3). 

What are the differences between state of the art from computer vision conferences (for example) and what is being used in HRC to date? 
How can this be improved to make better robots that can utilize gesture recognition? 


Kinesics (Movements: Actions, Facial Macro, Facial Micro, Proxemics (Distance), Haptics (Touch), Emotion (Recognition), Chronemics (Time). 

Six people with different heights performed 5 gestures 50 times each~\cite{han_human_2017}. 

From the total 1500 gestures, there were 11 errors, with no significance in accuracy between short, medium and tall people. 

Five people with different heights perform gestures to control a mobile robot~\cite{ghandour_human_2017}. In the total 90 gestures issued, two were misclassified~\cite{ghandour_human_2017}. From the failures, 78.9\% were related to distance and rotational speed of the robot. 


Task type was identified as a high level overview (i.e. walking assistant, search and rescue, etc). Task criticality referred to when failure could affect the life of a person, and it was divided into three categories: high, medium and low. Robot morphology was classified as physical appearance based on three categories: anthropomorphic (i.e. human-like), zoomorphic (i.e. animal-like) and functional (i.e. neither, but focusing on the function of the robot). Ratio of robot to people was calculated the number of humans over the number of robots. Composition of robot teams involved whether robot teams contained the same or different type of robot, classified as homogeneous or heterogeneous. Level of shared interaction among teams involved the formation of the human-robot team. Interaction role was one of the five following classifications: supervisor, operator, mechanic/programmer, teammate and bystander. Type of human-robot physical proximity included passing, following, avoiding, approaching, touching and none of the above. Space/time taxonomy classified if robots and humans were used at different times (asynchronus), same time (synchronous), while in the same location (collocated) or different location (non-collocated) \cite{yanco2004classifying}. Level of robot autonomy in HRI/C reported using \cite{beer2014toward}'s 10-point taxonomy: 1) manual teleoperation, 2) action support, 3) assisted teleoperation, 4) batch processing, 5) decision support, 6) shared control with human initiative, 7) shared control with robot initiative, 8) supervisory control, 9) executive control, and 10) full autonomy.