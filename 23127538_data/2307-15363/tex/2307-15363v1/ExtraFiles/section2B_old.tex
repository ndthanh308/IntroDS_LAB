
The following section provides a detailed review for robotic vision techniques used in each application area. In this section, we present information on the process of robotic vision, including algorithms, data sets, cameras and methods to allow robots to provide information, support or actions via robotic vision. 


% ==============================================================
\subsection{Gesture Recognition}
% ==============================================================

Gesture recognition via robotic vision involves being able to identify hand and body gestures from video streams. Robotic vision for gesture recognition is commonly performed using a two-step process. First, there is an extraction step in which the hand, body or head is identified in an image. Second, a classification step is performed to determine what gesture is being demonstrated by the human. Here we will discuss the two steps in detail and common techniques used within each domain for gesture recognition. 

%\subsubsection{Step 1 - Extraction Step - Process, Techniques, Methods:} 
\subsubsection{Overview:} 

To start the process of gesture recognition, the first step is often to first extract the hand, body or head from the image. This includes finding the location of the human in the image, and seeking to detect a human face in the image or video stream. To detect a human face, the Viola-Jones method~\cite{viola_robust_2004} was commonly used for this step ~\cite{couture-beil_selecting_2010, martin_estimation_2010, droeschel_towards_2011, lam_real-time_2011,  van_den_bergh_real-time_2011,pereira_human-robot_2013, monajjemi_hri_2013}. The Viola-Jones method involves using Haar filters to extract features from an image, and AdaBoost to make predictions based on a weighted sum of features. Finally, a cascade of classifiers makes the algorithm more efficient by discarding false detections early. Face location can also be used as a method to help determine where in the image the algorithm should focus on to increase the chance of detecting the location of the persons hands ~\cite{droeschel_towards_2011, luo_tracking_2011, milligan_selecting_2011}. Colour segmentation was often used to find a hand in an image. This included where the human was required to wear a coloured item (for example a coloured glove~\cite{chen_integrated_2010, yoshida_evaluation_2011, faudzi_real-time_2012}, a piece of coloured clothing~\cite{hassan_computationally_2016}, or with coloured tape around specific fingers~\cite{lavanya_gesture_2018}). Colour segmentation has also been used to segmenting skin tone~\cite{chen_approaches_2010, manigandan_wireless_2010, luo_tracking_2011, park_real-time_2011}. Segmentation is typically performed in HSV~\cite{manigandan_wireless_2010, lambrecht_spatial_2012, raajan_hand_2013, sriram_mobile_2019}, or YCrCb~\cite{luo_tracking_2011, choudhary_real_2015, xu_skeleton_2020} colour spaces, using hand coded threshold values~\cite{chen_approaches_2010, luo_tracking_2011, raajan_hand_2013, choudhary_real_2015}, or by using histogram analysis from a skin sample~\cite{manigandan_wireless_2010, lambrecht_spatial_2012, xu_skeleton_2020}. Segmentation is also performed with the aid of the depth sensors~\cite{droeschel_towards_2011, paulo_vision-based_2012, mazhar_real-time_2019, xu_skeleton_2020}. Droschel et al~\cite{droeschel_towards_2011} apply 3D region growing using the centroid of the head as a seeding point. Paulo et al~\cite{paulo_vision-based_2012} segment hand gestures by setting minimum and maximum distances to the sensor. Mazhar et al~\cite{mazhar_real-time_2019} first extract hand position using skeleton information, then find the bounding region of the hand using depth. Xu et al~\cite{xu_skeleton_2020} perform colour segmentation thresholds tuned for human skin tone, and incorporate depth to remove segmentation errors. Skeleton tracking was commonly used to extract human features from an image, using RGB-D cameras (example papers include~\cite{broccia_gestural_2011, lichtenstern_prototyping_2012, cicirelli_kinect-based_2015,fang_vehicle-mounted_2019, chen_human-following_2019, kahlouche_human_2019}, and RGB cameras (for example with OpenPose~\cite{Cao_2017_CVPR})~\cite{martin_real-time_2019, mazhar_real-time_2019, miller_self-driving_2019}. 

% Skeleton information was then used to extract image regions for hand gesture detection~\cite{lima_real-time_2019}, or to classify body gestures from human joint positions. or by using a secondary detector to first identify the location of the human and sampling coloured pixels from this region (for example by first detecting the head)~\cite{}

%\subsubsection{Step 2 - Classification Step - Process, Techniques, Methods:}

Once the region of interest is extracted, hand and body gestures are classified through a variety of methods depending on the dynamic or static nature of the gestures being performed. For static hand gestures, classification was often performed from segmented images by determining the contours (boundary pixels of a region) and the convex hull (smallest convex polygon to contain the region) as a method of counting the number of fingers being held up~\cite{lambrecht_spatial_2012, raajan_hand_2013, choudhary_real_2015, deepan_raj_static_2017,  sriram_mobile_2019, xu_skeleton_2020}. The distance of each contour point to the centroid of the segmented image was also used for finger counting~\cite{manigandan_wireless_2010, luo_tracking_2011}. From skeleton information, gestures were often classified from the 3D joint positions of the human skeleton using the angular configuration of the joints~\cite{cheng_design_2012, lichtenstern_prototyping_2012, nishiyama_human_2013, wang_vision-guided_2013, fujii_gesture_2014, yang_real-time_2015, fareed_gesture_2015, han_human_2017, miller_self-driving_2019, moh_gesture_2019}, or by using add-on SDK's (such as Visual Gesture Builder~\cite{long_kinect-based_2018}, OpenNI User Generator~\cite{yuan_development_2018}, and the supporting SDK for the Intel Realsense~\cite{vysocky_interaction_2019}). Ghandour et al~\cite{ghandour_human_2017} train a neural network classifier for body pose gestures, using joint positions as input that were first extracted from skeletal information. Li et al~\cite{li_real-time_2019} provide an LSTM network with joint positions to determine the users intention.

% other
% and using classification algorithms such as support vector machines (SVM)~\cite{ehlers_human-robot_2016} and neural networks~\cite{cicirelli_kinect-based_2015, ghandour_human_2017}. 

% Histograms of Oriented Gradients (HOG)~\cite{gori_all_2012}, , SIFT

% Historgram of flow~\cite{gori_all_2012}?. 

% Parallel Tracking and Mapping (PTAM)~\cite{monajjemi_hri_2013}

Dynamic gestures that require a sequence of gestures for classification use multiple image frames. Common algorithms used to track and classify dynamic gestures include Hidden Markov Models (HMM)~\cite{park_real-time_2011, droeschel_towards_2011, burger_two-handed_2012, tao_multilayer_2013, fujii_gesture_2014, xu_online_2014}, optic flow for hand wave detection~\cite{monajjemi_hri_2013}, particle filters for tracking face and hands~\cite{park_real-time_2011, burger_two-handed_2012, prediger_robot-supported_2014, du_online_2018}, and Dynamic Time Warping (DTW)~\cite{qian_visually_2013, canal_gesture_2015, vircikova_teach_2015, pentiuc_drive_2018, chen_wristcam_2019}. Tao et al~\cite{tao_multilayer_2013} use a neural network to segment 3D acceleration and 3D angular
velocity data of hand gestures into a binary gesture indication value, and use a HMM to classify hand waves in various directions. Chen et al~\cite{chen_wristcam_2019} compare SURF features between frames to determine the background velocity from a wrist mounted camera, then perform DTW to classify dynamic hand gestures. Using the joint positions and a fast Fourier transform (FFT) to extract a sequence of feature vectors, Cicirelli et al~\cite{cicirelli_kinect-based_2015} train a separate neural network for each of the ten recognised body gestures. The input to this network is three features (elbow angle, and shoulder angles in two planes) for 60 frames. 

Other classification methods used traditional machine learning techniques, such as k-nearest neighbors (k-NN)~\cite{pereira_human-robot_2013, gao_humanoid_2015, chen_wristcam_2019}, support vector machines (SVM)~\cite{chen_integrated_2010, chen_approaches_2010, gori_all_2012, ehlers_human-robot_2016}, and multi-layered perceptrons (MLP)~\cite{martin_estimation_2010, tao_multilayer_2013}. k-NN was used to determine if a query sample was close to a known set of gestures, where sample features included joint positions,~\cite{gao_humanoid_2015}, Hu moments~\cite{pereira_human-robot_2013}, and velocity vectors found with dynamic time warping (DTW)~\cite{chen_wristcam_2019}. Features classified using an SVM include segmented images~\cite{chen_approaches_2010}, fourier descriptors of image boundaries~\cite{chen_integrated_2010}, and joint position vectors~\cite{ehlers_human-robot_2016} . Other methods use feature extractors such as Haar filters ~\cite{van_den_bergh_real-time_2011} or Gabor filters~\cite{martin_estimation_2010}. Van den bergh~\cite{van_den_bergh_real-time_2011} perform Average Neighborhood Margin Maximization (ANMM) to reduce the dimensionality of of features extracted using Haar filters, and then perform an nearest neighbours comparison to a database of hand gestures. Martin et al train a multi-layer perceptron using features from Gabor filters to recognise pointing gestures~\cite{martin_estimation_2010}. 

Deep neural networks have been adopted for gesture classification with increased popularity in recent years. Convolutional neural networks (CNN's) were used to classify gestures directly from pixels~\cite{arenas_deep_2017, mazhar_real-time_2019, xu_skeleton_2020}, depth images~\cite{kalidolda_towards_2018, lima_real-time_2019}, or from a combination of input channels~\cite{gao_hand_2020}. Neural networks have also been used to perform gesture classification from the skeletal information~\cite{cicirelli_kinect-based_2015, ghandour_human_2017, kahlouche_human_2019, li_real-time_2019}, and other sensory inputs~\cite{martin_estimation_2010, tao_multilayer_2013}. Examples of papers that investigate their own network architecture include~\cite{cicirelli_kinect-based_2015, arenas_deep_2017, ghandour_human_2017, lima_real-time_2019}, and examples where existing network architectures are used include~\cite{mazhar_real-time_2019, waskito_wheeled_2020, gao_hand_2020, xu_skeleton_2020}. Papers that used existing network architectures start with pre-trained models that are then fine tuned on task specific data~\cite{mazhar_real-time_2019}, or train from scratch with a custom dataset~\cite{gao_hand_2020, xu_skeleton_2020, waskito_wheeled_2020}. Mazhar et al~\cite{mazhar_real-time_2019} use a pre-trained Inception V3~\cite{szegedy_rethinking_2016} and fine tune with collected data of hand gestures. Waskito et al~\cite{waskito_wheeled_2020} classify hand gestures from a small amount of collected data (1000 images) by first extracting the binary image of the hand region. Gao et al~\cite{gao_hand_2020} train two Inception V4~\cite{szegedy_incept_2017} networks in parallel, where the input to one network is the full resolution input, and the input to the second network is a low resolution form of the input. This work utilises an Myo arm band, and fuses the RGB, depth, and sEMG data creating a 5 channel image as input. Xu et al~\cite{xu_skeleton_2020} train a ResNet~\cite{he_deep_2016} to learn the minimum convex hull of a hand gesture to determine the number of fingers held up to the robot. Kahlouche et al~\cite{kahlouche_human_2019} fuse multiple classification methods (SVM, decision tree, and a MLP) using a voting process from an ensemble of classifiers. Body pose classifiers with neural networks are used for static~\cite{ghandour_human_2017} and dynamic~\cite{cicirelli_kinect-based_2015, li_cnn_2019} body poses.


\subsubsection{Cameras:}
Gesture recognition can be performed using a variety of different camera systems. Depth aware cameras were used in 59 of the 88 gesture recognition applications identified. Of the 59 papers that used depth aware cameras, 53 of these were the Microsoft Kinect. Single camera set-ups were found in 26 papers. Stereo cameras were used in 5 papers, and omni-directional vision was used in one example. 


\subsubsection{Datasets/Models:} 
Gesture recognition often uses large datasets that involve people in frame performing common and unique gestures for training. Datasets involved videos with xxx and xxx. Common high-quality data sets used in the training session involved xxx, xxx and xxx, which contained xxx(s) video samples that has been pre-cleaning using xxx, and xxx. 

Datasets were most often collected by the researchers using the robotic platform being investigated (key examples include~\cite{martin_estimation_2010, droeschel_towards_2011,park_real-time_2011, pereira_human-robot_2013, tao_multilayer_2013, cicirelli_kinect-based_2015, arenas_deep_2017, ghandour_human_2017, lima_real-time_2019, kahlouche_human_2019, chen_wristcam_2019, xu_skeleton_2020}). Several papers built on previous work for gesture recognition models (the details of datasets used in training were not provided)~\cite{couture-beil_selecting_2010, quintero_visual_2015, ehlers_human-robot_2016, maurtua_natural_2017, martin_real-time_2019, li_real-time_2019}. Papers with datasets made publicly available include~\cite{mazhar_real-time_2019, lima_real-time_2019}. Mazhar et al~\cite{mazhar_real-time_2019} made their dataset of hand gestures `OpenSign' available to others (10 volunteers who recorded 10 static gestures). Lima et al~\cite{lima_real-time_2019} make available their dataset of 160,000 samples from men and women in different poses performing open and closed hand gestures. 

% Do we care about custom dataset sizes? particularly if not released?
% Collected dataset sizes (where reported): 6,000 images~\cite{chen_integrated_2010}, 1,710 images,~\cite{martin_estimation_2010}, 1656 and 348 pointing gestures are used for training and testing.~\cite{park_real-time_2011}, 2082 gesture frames and 61 gestures, 27 of them being static gestures and the other 34 dynamic gestures."~\cite{canal_gesture_2015}, 180 images (for kNN classifier)~\cite{gao_humanoid_2015}, 600 images single background + 1400 images different background~\cite{arenas_deep_2017}, 448 images."~\cite{ghandour_human_2017}, 10,000 training images and 3,500 testing images, all the images are 96*96 gray level images. 5 gestures~\cite{song_towards_2017}. 6000 images (6 gestures).~\cite{waskito_wheeled_2020}. 2500 gesture images.~\cite{xu_skeleton_2020}. 1000 gestures images.~\cite{yang_novel_2018}. 160,000 samples from 20 individuals (15 men and 5 women), hand poses: open and closed hand. ~\cite{lima_real-time_2019}. 30 different people performing 33 manual gestures once. We additionally collected customized data where one person performed the same 33 gestures ten times.~\cite{kalidolda_towards_2018}

% Datasets that use video~\cite{chen_wristcam_2019}. 1350 gestures (135 videos × 10 gestures/video. 15 subjects of 7 females and 8 males. Their ages ranged from 20 to 65 years~\cite{chen_wristcam_2019}. 


\subsubsection{Multiple-Sensors/Sensor Fusion:} Robotic vision for gesture recognition was also paired with other sensor modalities to improve interaction and collaborative processes. Sensors that improve the robot perception of the interaction include speech recognition~\cite{burger_two-handed_2012, fujii_gesture_2014, vysocky_interaction_2019, yuan_development_2018, du_online_2018, maurtua_natural_2017}, surface electromyographic (sEMG) sensor (the Myo gesture control arm band)~\cite{gao_hand_2020}, Leap Motion sensor~\cite{kalidolda_towards_2018}, IMU~\cite{du_online_2018}, and wearable sensors such as a camera attached to a persons wrist to detect hand gestures are used to control a dual arm robot~\cite{chen_wristcam_2019}. Other modalities provide feedback to the human, including speech response~\cite{kahlouche_human_2019,mazhar_real-time_2019, avioz-sarig_robotic_2020}, LED's that change colour in response to commands~\cite{milligan_selecting_2011, alonso-mora_human_2014}, and a display to provide decision support for operators~\cite{lambrecht_spatial_2012, sriram_mobile_2019, lalejini_evaluation_2015}, such as robot state information~\cite{chen_human-following_2019}. 

% ==================================================
\subsection{Action Recognition}
% ==================================================

Action recognition via robotic vision involves being able to identify human motion from video streams. Similar to gesture recognition, robotic vision for action recognition is commonly performed using a two-step process. First, there is an extraction step in which the body pose is identified in an image. Second, a classification step is performed to determine what action is being demonstrated by the human. Here we will discuss the two steps in detail and common techniques used within each domain for action recognition. 

%\subsubsection{Step 1 - Extraction Step - Process, Techniques, Methods:} 

Actions are recognised by first extracting the relevant human information from the image. Like to gesture recognition, skeleton information is often extracted first, then classification is performed using the position of the extracted joints~\cite{werner_evaluation_2013, gorer_autonomous_2017, avioz-sarig_robotic_2020, lang_research_2020}, or by utilising joint position information with other classification methods~\cite{potdar_learning_2016, sorostinean_activity_2018, chalvatzaki_learn_2019, lang_research_2020}. Alternatively, regions of interest can be acquired using modern deep learning techniques such as faster R-CNN~\cite{renNIPS15fasterrcnn} (used by ~\cite{kollmitz_deep_2019}). Colour segmentation is also used in action recognition to identify where a hand is in an image~\cite{wang_collision-free_2017}. 

%\subsubsection{Step 2 - Classification Step - Process, Techniques, Methods:}

Gorer et al~\cite{gorer_autonomous_2017} compare the joint position of  motion identifier joints and those from a human demonstrator, to identify the disparity from an elderly user performing an exercise pose. The robot also performs face tracking (from on board software) and eye blinking (with changes to eye LED's).

Kollmitz et al\cite{kollmitz_deep_2019} use a Kalman filter to track regions of interest (extracted with faster R-CNN~\cite{renNIPS15fasterrcnn}), and a hidden markov model (HMM) to classify the class label, 3D position and velocity of a person using a walking aid. A ResNet-50~\cite{he_deep_2016} was pre-trained on ImageNet data, using RGB images as input.

Chalvatzaki et al~\cite{chalvatzaki_learn_2019} show that a neural network policy with LSTM cells trained using model based reinforcement learning outperforms several alternative methods (inverse reinforcement learning, model predictive control, and kinematic solutions) when predicting human motion when walking behind a robotic rollator. The policy utilises leg information from laser data, and upper body joint positions extracted from an RGB-D image using OpenPose~\cite{Cao_2017_CVPR}.

Potdar et al~\cite{potdar_learning_2016} reduce the dimensionality of the extracted skeletal information from 60 to 7 using principle component analysis (PCA), then train a LSTM neural network classifier. 

Wang et al use a pre-trained CNN (VGG-16~\cite{simonyan_very_2015}) to extract visual features from an image region and predict hand movement (change in x,y position) using an LSTM model~\cite{wang_collision-free_2017}.

Dense trajectories~\cite{wang_action_2011} are should to provide better features (resulting in more accurate action classifications), than CNN's where there is a mismatch between training and testing data (for example, when identifying the actions of children when large action recognition datasets are of adults)~\cite{efthymiou_multi-_2018}. 

Gui et al train a Generative Adverserial Network (GAN) from motion input (sequence of skeletal data)~\cite{gui_teaching_2018}. Skeleton information is first extracted using OpenPose~\cite{Cao_2017_CVPR} from RGB data. Depth information is then introduced to determine the 3D joint locations. A GAN framework with a single GRU cell for each the encoder and decoder is utilised to generate motion predictions.

Sorostinean et al~\cite{sorostinean_activity_2018} pre-train a LRCN network (Long-term Recurrent Convolutional Network) using the NTU RGB-D dataset~\cite{shahroudy_ntu_2016}, and fine tune with collected data that includes thermal information. First, joint positions were extracted using the Kinect API, then the LRCN network was used to extract features from skeleton information and the depth image. Features from face temperature were also tracked between frames. Features from the depth, skeletal, and thermal information were then used by a support vector machine (SVM) model with a radial bases function (RBF) kernel to classify the actions.

Zhang et al~\cite{zhang_automating_2018} predict if a person is sitting or standing, facing the robot, and if the person is exhibiting a welcoming action (such as waving or stretching). The Microsoft SDK is used to extract skeleton information, then sitting is determined if the skeleton height is less than the height of the standing skeleton, body orientation is determined from shoulder positions, and template matching is used to determine a welcoming pose.

From the skeleton information (extracted from Baidu AI), Lang et al~\cite{lang_research_2020} find the average of 7 key locations (top of the head, left/right elbow, wrist and ankle). These average values were collated to determine an identifier score, with each action corresponding to a unique identifier score.

In the work by Lee et al~\cite{lee_real-time_2020} joint pose information was first extracted from OpenPose~\cite{Cao_2017_CVPR} from RGB data. An image is then create mapping joint positions $x,y,z$ into RGB colour space (each pixel represents a joint location, normalised to be between 0 and 255). Each image column becomes the location of each joint position, and has a row for each camera frame. This compressed joint information image is then passed into a convolutional neural network (CNN) and trained to predict actions on the NTU RGB-D dataset.



\subsubsection{Cameras Used for Action Recognition:}

From the 15 identified papers that used action recognition, the most common camera type was the RGB-D sensor ($n=13$), followed by monocular RGB cameras ($n=2$). In the 13 papers that used an RGB-D camera, 12 used the Microsoft Kinect. In addition to RGB-D cameras, Sorostinean et al~\cite{sorostinean_activity_2018} also utilise thermal cameras in an action recognition scenario.

\subsubsection{Datasets}

Action recognition classification often required the usage of a large dataset of actions~\cite{wang_collision-free_2017, gui_teaching_2018, sorostinean_activity_2018, kollmitz_deep_2019, lee_real-time_2020}. Two papers~\cite{sorostinean_activity_2018, lee_real-time_2020} used the NTU RGB-D dataset~\cite{shahroudy_ntu_2016}, containing 60 action classes from 56,880 video samples with RGB videos, depth map sequences, 3D skeletal data, and infrared (IR) videos for each sample. The manipulation action dataset (MAD)~\cite{fermuller_prediction_2018} was used by~\cite{wang_collision-free_2017} and contains five different objects with five distinct actions. Each action is repeated five times (a total number of 625 recordings)~\cite{wang_collision-free_2017}. The H3.6M dataset~\cite{ionescu_dataset_2014} has 3.6 million 3D human poses (5 female, 6 male from 4 view points) and was used by~\cite{gui_teaching_2018} to train a motion prediction GAN. Kollmitz et al~\cite{kollmitz_deep_2019} make available their dataset of various mobility levels (wheelchair, walker, walking stick) with over 17,000 annotated RGB-D images.

% For the MBRL training we have employed 5000 frames of tracking data from four patients, while for testing the controller’s performance we have kept 2000 data from one patient unseen to the training set.~\cite{chalvatzaki_learn_2019}



% Efthymiou et al~\cite{efthymiou_multi-_2018} use a pre-trained model on the Sports1M corpus, which is then fine-tune to classify the actions in our database. In addition, since we have limited data, we split each of our videos in 16-long frame clips with the 15 frames overlapping and use thse to fine-tune the network~\cite{efthymiou_multi-_2018}. Other database mentions from~\cite{efthymiou_multi-_2018}: the ActivityNet database [29] contains 15,410 videos of 200 classes for training, Sports1M [30] includes over 1 million videos of 487 classes, and UCF101 [31] contains 13,320 videos from 101 classes.

% ==================================================
\subsection{Robot Movement in Human Spaces}
% ==================================================
TODO ~\cite{bellarbi_social_2017}

Mobile robots that operate in human spaces are required to track people through multiple frames, responding as the person or robot moves through the world. The robot must first detect the person/s and then track their location. In some cases the robot is able to re-acquire a target person when they are no longer in view.

% 12 faces, 8 body pose, 3 ssd/
Face detection is one of the more common approaches to detecting if an image is exclusively a person, and the most common method (explained in detail in Sec 2B: Gesture Recognition) is the Viola-Jones method~\cite{viola_robust_2004}, example usage includes~\cite{budiharto_indoor_2010, luo_human_2010, kobayashi_people_2010, lam_real-time_2011, wu_accompanist_2012, ferrer_robot_2013, pereira_human-robot_2013, do_human-robot_2014, ali_improved_2015, bayram_audio-visual_2016, yao_monocular_2017, condes_person_2019}. Luo et al~\cite{luo_human_2010} train the Viola-Jones method to detect the humans from the upwards view of a low mobile robot (not only to detect faces). Another popular solution for person detection is to use skeleton extraction (for example from the Microsoft SDK, OpenNI, or OpenPose)~\cite{ghandour_human_2017, long_kinect-based_2018, yuan_development_2018, chen_human-following_2019, miller_self-driving_2019, wu_improved_2019, yang_socially-aware_2019, anuradha_human_2020}. This method is particularly useful if gestures are also used in part of the system. The single shot detector CNN~\cite{liu_ssd_2016} was used in more recent papers to extract people from a scene~\cite{condes_person_2019, hwang_interactions_2020}, and faces directly~\cite{weber_follow_2018}. Condes et al~\cite{condes_person_2019} utilise the SSD network~\cite{liu_ssd_2016} to first detect the human, then the Viola-Jones method to detect the face within this region. Weber et al~\cite{weber_follow_2018} used a CNN (VGG-16~\cite{simonyan_very_2015}) with single shot detection (SSD~\cite{liu_ssd_2016}) trained with the HollywoodHeads dataset~\cite{vu15heads} for face detection. Detections were then tracked with LSD SLAM~\cite{fleet_lsd-slam_2014} and by fitting of faces to a template face model (3D morphable face model 3DMM)~\cite{weber_follow_2018}. FaceNet~\cite{hwang_interactions_2020} was used to detect faces~\cite{condes_person_2019}, and to recognise a specific persons face~\cite{hwang_interactions_2020}. Angonese et al~\cite{angonese_multiple_2017} use a pre-trained GoogLeNet~\cite{szegedy_going_2015} CNN that was fine tuned on a custom dataset of specific instances of people, such that detections could be integrated as landmarks in a SLAM algorithm for robot navigation. 

Other detection methods use appearance models to match regions of the image to a model~\cite{weinrich_appearance-based_2013, yun_robotic_2013, chen_stereovision-only_2014}. Weinrich et al~\cite{weinrich_appearance-based_2013} use a laser for leg detection, then use HOG with a sliding window with a multi-resolution pyramid of the input image to identify, and re-identify, the upper body of a detected person compared with an appearance model (a manually constructed model of an upper body). Yun et al~\cite{yun_robotic_2013} use online multiple instance learning (MIL~\cite{babenko_visual_2009}) to track a person from the back appearance, training a colour histogram score from online images using both positive and negative examples. Chen et al~\cite{chen_stereovision-only_2014} also utilise MIL to perform segmentation with skin colour to detect a person, updating the adaptive appearance model of the person and the local image background. 3D voxelisation was used extract human shapes from depth images~\cite{munaro_fast_2014, kahily_real-time_2016}. Munro et al~\cite{munaro_fast_2014} created a voxel grid filter from an RGB-D image to remove the ground, and clustered groups of voxels based on 3D proximity and geometric shape to a person. Kahily et al~\cite{kahily_real-time_2016} relied on the Microsoft SDK to extract skeleton information when the robot was not moving, but found voxelisation, segmentation, and 3D clustering to be the preferred person detection method when the sensor was moving~\cite{kahily_real-time_2016}. Images that are first converted to a local binary patterns (LBP)~\cite{ojala_lbp_2002} can be more robust to lighting~\cite{yun_robust_2010}. Alvarez-Santos et al~\cite{alvarez-santos_feature_2012} use a laser for leg detection, then use weighted image features that include LBP~\cite{ojala_lbp_2002}, canny edge detection~\cite{canny_computational_1986}, and histogram of gradients (HOG~\cite{dalal_histograms_2005}) features for robust person detection. HOG features were also used in support vector machines (SVM) to detect legs~\cite{talebpour-board_2016}, and coupled with height and distance information for robust people detection~\cite{vasconcelos_socially_2016}. Jia et al~\cite{jia_autonomous_2011} use the disparity image from a stereo camera, and a canny edge detector to extract a human shape. Head segmentation is the performed using geometric relationships, and compared to a samples from a model image using Hu moments~\cite{jia_autonomous_2011}. Gupta et al~\cite{gupta_robust_2015} first subtract the background, then perform template matching on the segmented image to detect a person. Ferrer et al~\cite{ferrer_robot_2013} use online random ferns to compares pixel intensities to learn a particular persons face. This method also utilises human feedback (through a keyboard and touch screen) to improve the classifier when the predictions are uncertain~\cite{ferrer_robot_2013}. Almonfrey et al~\cite{almonfrey_flexible_2018} developed a vision network using four static cameras to create an intelligent work space, where a mobile robot can identify and avoid people.

Once a human feature (for example the face) is detected, it must be followed between image frames. Tracking is often performed using a particle filter~\cite{fahn_real-time_2010, kobayashi_people_2010, ali_improved_2015, prediger_robot-supported_2014}, or through optic flow~\cite{wu_improved_2019}. Fahn et al~\cite{fahn_real-time_2010} begin the interaction with a face is in the center of the screen, and track the colour spectrum of the skin and hair using a particle filter. SURF features~\cite{bay_speeded-up_2008} are another method for tracking image features through frames~\cite{wu_accompanist_2012}, where image samples can be provided a priori (for example by showing the robot a pattern the target person is wearing~\cite{chen_person_2012}) or by first detecting the human (for example through face detection), then identifying their clothing for tracking~\cite{wu_accompanist_2012}. Kernalized correlation filters (KCF)~\cite{henriques_high-speed_2015} were also used for tracking~\cite{luo_real-time_2019, hwang_interactions_2020}. Human motion prediction~\cite{hu_design_2014, gupta_robust_2015, chen_human-following_2019}, and robot kinematic models~\cite{zhang_vision-based_2019} were used to perform better tracking. Hu et al~\cite{hu_design_2014} use a model of a person walking to anticipate human movement and more accurately perform person following. Zhang et al~\cite{zhang_vision-based_2019} develop a person detection and tracking method that uses the contours of the target in the image (target contour band). The robot follows the target using image based visual servo control using a kinematic model of the robot and the bounding corners of the target in the image~\cite{zhang_vision-based_2019}. Kalman filters were a common method for minimising tracking errors from noisy sensors and odometry~\cite{gupta_robust_2015, talebpour-board_2016, luo_real-time_2019, long_kinect-based_2018}.

% Re-identification of a lost target was considered by several papers~\cite{luo_human_2010, wu_accompanist_2012, weinrich_appearance-based_2013, condes_person_2019, zhang_vision-based_2019}.
% Re-identify targets section todo


\subsubsection{Cameras Used for Robot Movement in Human Spaces:}

From the 48 identified papers where a robot was required to move in human spaces, the most common camera type was the RGB-D sensor ($n=25$), followed by monocular RGB cameras ($n=16$). In the 25 papers that used an RGB-D camera, 19 used the Microsoft Kinect. Other camera types used were stereo cameras ($n=4$) and omni-directional cameras ($n=3$).


\subsubsection{Datasets/Models:} 

Munaro et al~\cite{munaro_fast_2014} release the Kinect Tracking Precision (KTP) dataset, collected from the Microsoft Kinect and containing 8,475 frames with a total of 14,766 instances of people.

Weber et al~\cite{weber_follow_2018} train a face detection model on the HollywoodHeads Dataset~\cite{vu15heads}, containing annotated head bounding box regions for 224,740 video frames from Hollywood movies.

Ferrer et al~\cite{ferrer_robot_2013} collect a dataset to verify their face detection method (12 sequences of 6 different persons).

Almonfrey et al~\cite{almonfrey_flexible_2018} train a person detectors using the INRIA dataset~\cite{dalal_histograms_2005}, containing over 1800 annotated human images. 
% Almonfrey et al~\cite{almonfrey_flexible_2018} also release a dataset of 6000 annotated images, unclear if this is true (mentioned only in the introduction?).

% ==================================================
\subsection{Object Handover and Collaborative Actions}
% ==================================================
% ===============

The most common person extraction technique used for handovers and collaborative actions was by extracting human joint information, from an RGB-D camera~\cite{christiernin_interacting_2016, wang_vision-guided_2013, morato_toward_2014, cherubini_unified_2015, koustoumpardis_human_2016, moreno_path_2016, araiza-lllan_dynamic_2018, bothe_effective_2018, hong_interactive_2018, landi_prediction_2019, scimmi_experimental_2019, valle_personalized_2019, liu_dynamic_2020, zhang_human_2020, gao_user_2020, chan_collision-free_2020, ferraguti_safety_2020}, or with an RGB camera using OpenPose~\cite{zardykhan_collision_2019, van_den_broek_ergonomic_2020}. More than one RGB-D sensor was used to improve the accuracy of human position identification~\cite{wang_vision-guided_2013, morato_toward_2014, pasinetti_development_2018,  scimmi_experimental_2019, ferraguti_safety_2020}. Filters were also used improve the quality of the detected joints~\cite{koustoumpardis_human_2016, hong_interactive_2018}. Face detection was used in one paper, using the Viola-Jones method to control the heading of a humanoid robot using the head position of a human operator~\cite{arumbakkam_multi-modal_2010}. Attaching coloured markers to a person was used in two papers for person detection and tracking in a shared workspace, where multiple cameras were used to localise the person in 3D space~\cite{tan_safety_2010, ding_optimizing_2011}. Other methods for detecting a person relied on them being a known distance from an RGB-D sensor and applying a distance threshold~\cite{shariatee_safe_2017, bolano_towards_2018}. From segmented images, morphology operations and edge detection was performed to extract the person from the image~\cite{shariatee_safe_2017}. Adaptive thresholding was used to find the pupil position in an eye centered image to detect gaze direction~\cite{arai_service_2020}. HOG descriptors were used with an SVM for person detection in a shared manufacturing cell~\cite{pasinetti_development_2018}.

Modelling of robot motion~\cite{wang_vision-guided_2013} and human motion~\cite{morato_toward_2014, landi_prediction_2019, gao_user_2020} improved the ability for controllers to operate safely in collaborative tasks. Landi et al~\cite{landi_prediction_2019} use a semi-adaptable neural network to model the hand position of the human that minimises jerk and improve collision detection. Gao et al~\cite{gao_user_2020} use a Gaussian mixture model to predict the motion of a human in an assisted dressing task~\cite{gao_user_2020}. Inverse kinematics were used to plan a trajectory that avoids the human~\cite{moreno_path_2016, zardykhan_collision_2019}. Voxelisation of an RGB-D images was also used to help avoid collisions with a person~\cite{mronga_constraint-based_2020, tarbouriech_bi-objective_2020}. Mronga et al~\cite{mronga_constraint-based_2020} use kinematic continuous collision detection (KCCD) to plan a path that avoids contact. Tarbouriech et al~\cite{tarbouriech_bi-objective_2020} use bi-directional RRT with a 3d occupancy grid to plan for human avoidance, and awareness. Visual servoing was used as method to direct a robot to a grasp location in an collaborative manipulation task~\cite{koustoumpardis_human_2016}, or towards the human hand in a handover task ~\cite{bdiwi_handing-over_2013, rosenberger_object-independent_2020}.

Particle filters were used for person tracking between image frames~\cite{arumbakkam_multi-modal_2010, nair_3d_2011, morato_toward_2014}. Nair et al~\cite{nair_3d_2011} track multiple people in an shared workspace by first segmenting the background from a static background model, then matching a fixed size 3D bounding box to segmented region for detection. To reduce computational load with multiple person detections, a global particle set is divided between the number of detections~\cite{nair_3d_2011}. Kalman filters were used~\cite{morato_toward_2014}

Kogkas et al~\cite{kogkas_free-view_2019} use a motion capture system to detect head pose, and eye-tracking glasses for gaze estimation. In a surgical task, a surgeon selects an instrument by looking at the item on a screen for a fixed period~\cite{kogkas_free-view_2019}. Dometrios et al~\cite{dometios_real-time_2017} use a depth camera to extract the surface shape of a human back for a person washing application with a soft robot.

Recent methods use convolutional neural networks (CNN's) for human detection in collaborative tasks. Bingol et al~\cite{bingol_practical_2020} utilises two CNN's: one that takes an image as input to determine the proximity of the person to the robot work area, and one that takes a torque feature matrix to determine if there has been contact made with the robot. Rosenberger et al~\cite{rosenberger_object-independent_2020} use a YOLOv3~\cite{} network for object detection, a fine tuned ResNet50~\cite{} network to segment a human hand from grasping locations on an object, and GG-CNN~\cite{} to to detect grasp locations of the segmented object.

% ==================================================

\subsubsection{Cameras Used for Object Handover and Collaborative Actions:}

From the 38 identified papers where a robot collaborated with a human in an an object handover or action, the most common camera type was the RGB-D sensor ($n=30$), followed by monocular RGB cameras ($n=10$). In the 30 papers that used an RGB-D camera, 22 used the Microsoft Kinect. Four papers ($n=3$) used stereo cameras.


\subsubsection{Datasets/Models:} 

The Ego hand dataset~\cite{} (4800 images of hands) was used to train a hand detector~\cite{rosenberger_object-independent_2020}.



% ==================================================
\subsection{Learning from Demonstration}
% ==================================================

Learning from demonstration tasks often used segmented human joint positions from an RGB-D camera~\cite{stipancic_programming_2012, petric_online_2014, potdar_learning_2016, li_learning_2018, du_online_2018}, or from an RGB camera using OpenPose~\cite{mazhar_real-time_2019}. The Viola-Jones face detection method was used by Yoo et al~\cite{yoo_gaze_2017} for face tracking, and segmentation by pixel values was used by Saegusa et al~\cite{saegusa_cognitive_2011} to determine hand position. Gestures were used to determine the commencement and completion of demonstrations~\cite{stipancic_programming_2012, petric_online_2014, du_online_2018, mazhar_real-time_2019}(methods described in the previous sections). 

Petric et al~\cite{petric_online_2014} control a humanoid end-effector using periodic dynamic movement primitives (DMP~\cite{ijspeert_dynamical_2013}). A person points to the part of the robot movement that needs to adapt, with the gesture defining the direction and magnitude of the new motion, and the robot adjusts its movements in response~\cite{petric_online_2014}. Potdar et al~\cite{potdar_learning_2016} use data collected from an interaction between two people to train an LSTM neural network to learn the complementary robot response to the actions of the person. Li et al~\cite{li_learning_2018} use Gaussian mixture models (GMM) and Gaussian mixture regression (GMR) to model the joint trajectories of a person performing a peg in the hole task. Inverse kinematics is used to translate the trajectories into the robot joint space for an imNEU humanoid robot~\cite{li_learning_2018}. Yoo et al~\cite{yoo_gaze_2017} follows a sequence of actions using gaze for a plant watering task. The robot first detects and tracks the human face and hands, and then uses depth thresholding and a CNN for object detection to determine the next object in the sequence~\cite{yoo_gaze_2017}.

% TODO: more on ~\cite{saegusa_cognitive_2011}.

% ==================================================

\subsubsection{Cameras Used for Learning from Demonstration:}

From the 7 papers identified that required the robot to learn from demonstrations, the most common camera type was the RGB-D sensor ($n=6$), of which 5 where the Microsoft Kinect. One paper ($n=1$) used a monocular RGB camera.

\subsubsection{Datasets/Models:} 


% ==================================================
\subsection{Social Classification and Communication} 
% ==================================================

% TODO fix up ones that have been moved
Joint positions extracted from RGB-D sensor~\cite{igorevich_behavioral_2011, torres_implementation_2012, csapo_multimodal_2012, indrajit_development_2013, yang_study_2013, taheri_social_2014, tseng_multi-human_2014, mollaret_multi-modal_2016, meghdari_real-time_2017, augello_towards_2020, lu_research_2020}

Viola-Jones method for face detection~\cite{csapo_multimodal_2012, saleh_nonverbal_2015}. Saleh et al~\cite{saleh_nonverbal_2015} classify head movements, such as nodding and shaking, using the direction and magnitude pattern (DMP) of depth pixels as features classified with an SVM~\cite{saleh_nonverbal_2015}.

% ==================================================

 iCub gaze controller~\cite{boucher_i_2012}


colour segmentation + blob detection and tracking ~\cite{foster_two_2012}

two cameras and colour segmentation for little uav~\cite{miyoshi_above_2014}.

moving average and butterworth filter for joints from kinect~\cite{taheri_social_2014}.

approaches people after detecting TODO~\cite{tseng_multi-human_2014}

Face Detector [27], Identity Recognition (identity, age, and gender) [28], [29], Face Tracking [30], Sound Recognition [25], Sound Localization and AV Fusion (described in this paper).~\cite{cech_active-speaker_2015}.


Opencv haar features viola-jones and cascades, trained for eye, nose, mouth. Gabor filters, PCA, and SVM~\cite{ke_vision_2016}

Random forests for real time 3D face analysis for line of sight OpenNi for body pose. Detects desire to interact with head/shoulder pose, speech. Voice Activity Detection (VAD). particle swarm optimization inspired tracker, tracking and voice are fused for HMM.~\cite{mollaret_multi-modal_2016}

Minimum Redundancy Maximum Relevance (MRMR)~\cite{vaufreydaz_starting_2016}. 29 interactions with the robot,made by 15 different participants, 5 classes for interact, leave interact etc.. 158000 frames

ROS face recognition package - Viola-jones but using depth to remove false positives based on realistic size of face.~\cite{zhang_optimal_2016}

Train filled pause detector (speech), when detected use peppers gaze detector for 1.5s, make a decision whos turn to speak~\cite{bilac_gaze_2017}

MS SDK + Neural network~\cite{meghdari_real-time_2017}

Velocity (MV) Algorithm and RVM, robust regress-based refining. I think they use an sdk~\cite{zhu_robust_2017}

OpenCV, no details of method used for face detection~\cite{gui_teaching_2018}.

This is in action rec as well~\cite{sorostinean_activity_2018}. pepper robot detects activity and responds with questions and change in state.CNN + LSTM using skeleton data (SDK)

ResNet-10 SSD + OpenFace, pre-trained networks~\cite{cazzato_real-time_2019}.

"Fuzzy combination of depth and sonar. penCV’s
OpenCV’s LPBH (Local Binary Patterns Histogram) face recognizer method"~\cite{chien_navigating_2019}

custom gaze detection  - detection face, detect landmarks on face, eye regions, head pose, pupil centre~\cite{jarosz_detecting_2019}

SSD for face detection, NN- facial recognition model. random forest regression, ~\cite{li_inferring_2019}

HAAr features for detecting face region, CNN and LSTM (custom network, search through parameters)~\cite{li_cnn_2019}

OpenNI~\cite{augello_towards_2020}

From extracted features, classification of gestures was performed with traditional machine learning methods such support vector machines (SVM)~\cite{saleh_nonverbal_2015, ke_vision_2016}. SVM's facial expressions~\cite{sosnowski_mirror_2010, castellano_context-sensitive_2014, ke_vision_2016}.  Other methods use feature extractors such as Gabor filters~\cite{ke_vision_2016}. 
Neural network layers that utilise time-series (such as LSTM) are also used to classify dynamic gestures~\cite{li_cnn_2019}.


Recent works use convolutional neural networks to detect the face region ~\cite{li_inferring_2019, li_cnn_2019}. The face location can be used to then classify facial expressions~\cite{li_cnn_2019},

~\cite{lee_visual_2020}semantic slam

OpenNI~\cite{lu_research_2020},

MS SDK for head pose detection~\cite{phong_vietnamese_2020}

OpenCV 2 cascade face detector, which is based on the Viola and Jones and openCV for locating facial features. Emotion classification ~\cite{masmoudi_expressive_2011}

Deep neural networks have been adopted for gesture classification in recent years. The most common usage was convolutional neural networks (CNN's), classifying gestures directly from pixels. Alternatives perform classification from the extracted skeletal information~\cite{}. architectures and use pre-trained models that are then fine tuned on task specific data~\cite{mazhar_real-time_2019, waskito_wheeled_2020}, or trained with a custom dataset~\cite{gao_hand_2020}. Examples of papers that investigate their own network architecture include~\cite{li_cnn_2019}. Li et al~\cite{li_inferring_2019} TODO: SSD for face detection + other things. Li et al~\cite{li_cnn_2019} use a CNN with LSTM to detect facial expression changes.

\subsubsection{Cameras Used for Social Classification and Communication:}

From the 41 papers identified that required the robot to communicate with a human in an social interaction, the most common camera type was the RGB-D sensor ($n=24$), followed by monocular RGB cameras ($n=15$). In the 24 papers that used an RGB-D camera, 23 used the Microsoft Kinect. Two papers ($n=2$) used a stereo camera.

\subsubsection{Datasets/Models:} 

Papers that used datasets from external sources were most commonly used for facial expression recognition~\cite{sosnowski_mirror_2010, castellano_context-sensitive_2014, ke_vision_2016, li_inferring_2019, li_cnn_2019}. The Cohn-Kanade dataset~\cite{kanade_comprehensive_2000} and its extended form (CK+)~\cite{lucey_extended_2010} was used to train facial expression models~\cite{sosnowski_mirror_2010, ke_vision_2016, li_cnn_2019}. Other datasets include FERET, AffectNet, MMI Face Database, and the Inter-ACT corpus. Several papers built on previous work for gesture recognition models (the details of datasets used in training were not provided)~\cite{saleh_nonverbal_2015}. 

the AffectNet Database contains approx- imately 450,000 + Cohn-Kanade (CK+) dataset includes 123 subjects and 593 image sequences. ~\cite{li_cnn_2019}

Datasets that use video~\cite{meghdari_real-time_2017}
"10 persons, 6 males and 4 females Kinect data output was captured at least for 3 seconds with 30 frame per second rate for each facial expression with different head orientation. Though, each person has a dataset that each frame of data is an array with 17 members."~\cite{meghdari_real-time_2017}

Trained on: Aberdeen Facial Database, GUFD Facial Database, Utrecht ECVP Facial Database + images of laboratory personel using openCV’s LPBH~\cite{chien_navigating_2019}.

WIDER FACE dataset + Kaggle facial expres- sion recognition challenge dataset~\cite{li_inferring_2019}
