
% =============================================================================
Cameras used in gesture recognition
% =============================================================================
monocular
rgb-d
kinect



% =============================================================================
% =============================================================================
DATASETS for GESTURE RECOGNITION:
% =============================================================================
~\cite{chen_integrated_2010,martin_estimation_2010,droeschel_towards_2011,park_real-time_2011,pereira_human-robot_2013,tao_multilayer_2013,fujii_gesture_2014,xu_online_2014,canal_gesture_2015, cicirelli_kinect-based_2015, gao_humanoid_2015, potdar_learning_2016, arenas_deep_2017, ghandour_human_2017, meghdari_real-time_2017, song_towards_2017, yang_novel_2018, chen_wristcam_2019, lima_real-time_2019, kahlouche_human_2019, xu_skeleton_2020, waskito_wheeled_2020}
Required datasets:
Each gesture contains 1,000 data, with a total of 6,000 data~\cite{chen_integrated_2010}

"training set of 19 persons
including 1,710 samples."~\cite{martin_estimation_2010}

2004 pointing gestures are captured for the first stage HMM and 1656 and 348 pointing gestures are used for training and testing, respectively.~\cite{park_real-time_2011}

Custom database of gestures for matching~\cite{pereira_human-robot_2013}

We respectively recorded 10 sets of data from three experimenters for training and testing~\cite{tao_multilayer_2013}

A number of example data for each gesture were used for training of HMM~\cite{fujii_gesture_2014}

Custom dynamic gesture database~\cite{xu_online_2014}

"2082 gesture
frames and 61 gestures, 27 of them being static gestures and
the other 34 dynamic gestures."~\cite{canal_gesture_2015}

10 gestures performed by 10 different people~\cite{cicirelli_kinect-based_2015}

Training data for KNN classification, about 30 per gesture (6 gestures)~\cite{gao_humanoid_2015}

5 gestures from 2 humans, unknown how much data~\cite{potdar_learning_2016}

600 images single background + 1400 images different background~\cite{arenas_deep_2017}

"The training data is collected from four
people with different heights [152-187] cm. Each person is asked
to do 112 gestures with distances [1.7, 4] m from the Kinect, with
deviation angle [-40, 40]. Thus the total training set is 448."~\cite{ghandour_human_2017}

"10 persons, 6 males and 4 females Kinect data output was captured at least for 3 seconds
with 30 frame per second rate for each facial expression with
different head orientation. Though, each person has a dataset
that each frame of data is an array with 17 members."~\cite{meghdari_real-time_2017}

10,000 training images and 3,500 testing images, all the images are 96*96 gray level images. 5 gestures~\cite{song_towards_2017}

Custom: data collected from 5 subjects1000 gestures. Dynamic time warping algorithm for gesture classification? DTW~\cite{yang_novel_2018}

Dataset collected (For kNN?)  1350 gestures (135 videos × 10 gestures/video. 15 subjects of 7 females and 8 males. Their ages ranged from 20 to 65 years~\cite{chen_wristcam_2019}

160,000 samples from 20 individuals (15 men and 5 women). Individuals per- formed the predefined hand poses: open and closed hand. ~\cite{lima_real-time_2019}

 It was collected from 30 different people performing 33 manual gestures once. We additionally collected customized data where one person performed the same 33 gestures ten times.~\cite{kalidolda_towards_2018}

datasets used is 1000 images for each hand gesture and the number of validation data is 100 images for each hand gesture.~\cite{waskito_wheeled_2020}

Custom: OpenNI used to collect pose dataset dataset contains six interactive activitivities (4 static, 2 dynamic)~\cite{kahlouche_human_2019}


2500 gesture images are created which includes 300 images for each of ”1”- ”5” gestures, 200 images for each of ”6”-”10” gestures. There are 1200 gesture images in the test set including 120 images for each of ”1”-”10” gestures.~\cite{xu_skeleton_2020}


Trained hmm model from participant actions.~\cite{droeschel_towards_2011}


Trained in previous work:
previous work~\cite{couture-beil_selecting_2010}
Training outlined in previous work~\cite{saleh_nonverbal_2015}
Training occurs, unclear as to the database, previous work in [3]~\cite{ehlers_human-robot_2016}
previous work~\cite{maurtua_natural_2017}
Trained in previous work~\cite{martin_real-time_2019}
Unknown, possibly custom~\cite{gao_hand_2020}

Provided dataset online:

We named our dataset OpenSign~\cite{mazhar_real-time_2019} 

Available datasets:

"Cohn-Kanade
Facial Expression Database [6] and MMI Face Database [7]"~\cite{sosnowski_mirror_2010}

faceAPI, a real-time face tracking toolkit from Seeing Machines, The Inter-ACT corpus used for data, SVM smile detector~\cite{castellano_context-sensitive_2014}


FERET (Phillips et al., 1998) face database is selected for the establishment of gender identification, CK+ (Lucey et al., 2010) facial expression database is selected for the establishment of facial expression recognition~\cite{ke_vision_2016}

WIDER FACE dataset + Kaggle facial expres- sion recognition challenge dataset~\cite{li_inferring_2019}

the AffectNet Database contains approx- imately 450,000 + Cohn-Kanade (CK+) dataset includes 123 subjects and 593 image sequences. ~\cite{li_cnn_2019}






% Gesture recognition is particularly useful in loud environments such as industrial settings~\cite{}, or when the vision sensor can be used for additional components of the robots sensing ability such as object detection~\cite{}, where other human sensing modalities require an additional sensing module, such as to detect speech).


% Robots can be controlled with gestures, where a human operator must learn how to interact with the robot, or in the case that the human is a user, and the robot must learn how to interact with the human. Tasks that require gesture recognition can have a low level of autonomy where discrete gestures trigger an action from a set of pre-defined behaviours~\cite{fujii_gesture_2014}, though can also be used to input higher level human intuition, for example take the left path for an otherwise autonomous mobile robot~\cite{lalejini_evaluation_2015}. Gesture recognition can also be used to control part of the system, for example a robot arm can mimic human joint positions and hand gestures can be used to open and close a gripper~\cite{lima_real-time_2019, martin_real-time_2019}. Head recognition~\cite{saleh_nonverbal_2015} and facial expression recognition~\cite{li_cnn_2019} are examples of the robot learning the behaviours that a human performs naturally, and allow the robot to engage with the human without the human explicitly learning how to interact with the robot. We will now discuss some of the uses of gesture recognition in the three identified configurations (hand, body, and head).

% Gesture recognition has become more prevalent in robotics applications due to cheap RGB-D sensors, such as the Microsoft Kinect, and the accompanying off-the-shelf software development kits (SDK's)~\cite{}. The MS Kinect  was the most used sensor for HRI found in our search (107 papers used the MS Kinect, where RGB-D sensors overall had 127, 

% Off-the-shelf algorithms were found in more than half of the papers included in our search, including SDK's, OpenNI (open natural interaction), OpenNITE, OpenCV (open computer vision), pre-trained neural networks YOLO, OpenPose.  Off-the-shelf gesture recognition examples:



\begin{enumerate}
    \item What are the most common forms and domain areas of robotic vision in HRC/I? 
    \item What is the current state-of-the-art applications of robotic vision for HRC/I? 
    \item What is the functional impact of robotic vision in HRC/I?
    \item What are the upcoming challenges for robotic vision in HRC/I?
\end{enumerate}

The following section will provide a brief breakdown of each of the main application areas that require robotic vision, including the purpose, intended use, current level of performance, and detailed examples of its propose use in action. 

action recognition:


Gesture recognition:

purpose: to interact with a robot with unique actions from the hand, body, or face.
Intended use: For control - of a mobile or manipulator robot, or to provide a service to the human as user
current level of performance: 
detailed examples.


robot movement in human spaces:


handovers and collaborative actions:

social communication:




\technique list:

\textbf{Off the shelf}:
For practitioners looking to integrate robotic vision into their HRC/I project, off the shelf techniques are becoming more common and more powerful. 

can be used with low cost sensors like the MS Kinect~\ref{}, or with any RGB camera with . 

Available software libraries include:
OpenNI (Open Natural Interaction), which is used in this
paper, NITE Primesense, libfreenect, CL NUI, Microsoft
Kinect SDK and Evoluce SDK.

OpenFace

OpenCV~\ref{} is library that  commonly used as an interface with such as:

Viola-Jones~\cite{}, HAAR features, cascades, with adaBoosting for face detection, examples include OpenCV.

Pre-trained neural networks: OpenPose

Segmentation 

HOG features

SURF and SIFT

Other commonly used methods: SVM's, kNN

Dynamic Time Warping.

 SVM, kNN


Deep neural networks: CNN's, LSTM's, GAN's,

Non-vision techniques? (motion tracking) Kalman filter, particle filter, GMM,




% =============================================================
% =============================================================
% =============================================================



Task	
Grasp Assistant 	Moving something to a human (by grasping)
Gesture Control	Drones, anything that is controlled through gesture
Mobility Assistant	Moving a human
Social Mobile	Any mobile platform with the primary task of mobility (following, avoiding, approaching human). Only robots that don't interact at a higher social level
Social Gesture	Mobile robots that are a bit more sophisticated than just mobile. Exercise robots, robots that engage physically
Social Emotive	Robots that converse with people with face gesture recognition
Other	e.g. infrustructure inspection, drip stand robot (add to to this list, then can consider new classes)
	
Robot	
Mobile	
Fixed Manipulator	
Mobile Manipulator	
Drone	
Social	Mobile social robot should be classified by primary task (mobile or social)
	
Domain	
Field	Outside applications
Industrial	Including manufacturing and warehouse
Domestic	Home, can include healthcare (targeted at home)
Urban	Shopping centre, school, restaurant, can include healthcare (hospital)


	
Robotic Vision	
Gaze estimation	Includes eye tracking and head pose tracking
Human pose tracking	No classification, just is there a person, and where
Human pose recognition	Includes disability classification and pose estimation / identification / classification
Hand tracking	No classification, just is there a hand, and where
Hand gesture recognition	Includes gesture classification
Face tracking	Includes face detection and head tracking
Face recognition	Detecting gender, or specific person
Face gesture recognition	Includes emotion recognition
Other	Human not present in vision part (consider excluding paper?)

-

List of robots commonly used, a list of tasks commonly performed
HRC robots:
\begin{itemize}
    \item Robot arms (Jaco, Kuka) 
    \item Humanoids (Nao, Pepper)
    \item Mobile bases (Pioneer)
    \item Mobile base with arms (PR2, robot arm mounted to wheelchair)
    \item Social (Humanoids, FurHat)
    \item Aerial and underwater
\end{itemize}

HRC tasks:
\begin{itemize}
    \item Reaching/manipulation assistance
    \item Person following
    \item Instruction providing
    \item 
\end{itemize}

How is vision used
HRC vision sensor types:
\begin{itemize}
    \item RGB
    \item Depth
    \item RGB+D e.g. Kinect (RBG + D, or TOF)
    \item 
\end{itemize}

HRC vision algorithms:
\begin{itemize}
    \item 
\end{itemize}

Gesture recognition

With or without robot (robot response/action is not considered)

High speed single camera techniques are emerging. ~\cite{katsuki_high_speed_2015} use a single high speed camera predict the configuration of human hand. This could enhance efficiency in production processes by cooperating with humans on tasks swiftly.

~\cite{xia_vision-based_2019} binocular vision for hand gesture segmentation and extraction. \textbf{No robot} 

~\cite{gori_multitype_2016} develops an algorithm for activity recognition from multiple people in a robots view (RGBD). Computation is performed in real time, though currently limited to two people. Autonomous mobile robot with a MS Kinect is used to collect data from a natural environment with students.

~\cite{jevtic_comparison_2015}assesses three control modalities including push, following, and pointing to guide a mobile platform to specific rooms in domestic setting. For service robot applications where the robot is guided to a location by a human. Uses off the shelf person detection and skeleton joint tracking (Microsoft Kinect SDK). Concludes that pointing is worse (time taken, accuracy, stress on the human) than pushing, but there are applications where pushing might not be possible. 

~\cite{shukla_probabilistic_2015} looks at finger pointing, in particular creates a probabilistic method for estimating the pose of a pointing finger (or pointing tool) to identify a target object, such that a human can point to an object and a robot can pick it up. Uses RBGD, and a grasping robot arm (couldn't see any details about the robot). Pointing is determined independently of body pose, and is invariant to size and scale of pointing gesture. In a followup work in simulation and on a real platform, ~\citep{shukla_proactive_2017} demonstrates the human can point to an object, signal "give me", and handover of the object. This is demonstrated the task of assembling furniture, where the robot hands the human the legs to a table. 


Reverse gesture recognition? Robot provides the gesture ~\cite{admoni_modeling_2016}


% ExClude papers that investigate the social outcomes? ~\cite{mead_proxemics_2015} investigate the distance of a mobile robot from a human providing instructions, either by speech or by pointing. This study looks at the experience of the human subject (robot competence, anthropomorphism, engagement, likability, and technology adoption), as well as the performance of the speech and gesture recognition over at different distances from the human (speech and gesture recognition degrade with distance). The robot is mobile with a humanoid upper torso. Might be a better fit in an another place (proxemics is the main priority).