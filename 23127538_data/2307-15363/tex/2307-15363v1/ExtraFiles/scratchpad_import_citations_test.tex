%%
%% This is file `sample-acmsmall.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `acmsmall')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-acmsmall.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%%%  
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[acmsmall]{acmart}
%% NOTE that a single column version is required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{10.1145/1122445.1122456}


%%
%% These commands are for a JOURNAL article.
\acmJournal{JACM}
\acmVolume{37}
\acmNumber{4}
\acmArticle{111}
\acmMonth{8}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}
% \usepackage{booktabs, multirow} % for borders and merged ranges
% \usepackage{soul}% for underlines
% \usepackage[table]{xcolor} % for cell colors
% \usepackage{changepage,threeparttable} % for wide tables
\usepackage{float}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{enumitem} % for variations in list formatting
% \usepackage{pgfplotstable,filecontents}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.9}% supress warning
%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Robotic Vision for Human-Robot Interaction and Collaboration: A Survey and Systematic Review}
%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Nicole Robinson}
\email{nicole.robinson@monash.edu}
\orcid{0000-0002-7144-3082}
%\authornotemark[1]
\affiliation{%
  \institution{Australian Research Council Centre of Excellence for Robotic Vision, Faculty of Engineering, Turner Institute for Brain and Mental Health, Monash University and Queensland University of Technology}
  \streetaddress{18 Alliance Lane}
  \city{Clayton}
  \state{Victoria}
  \country{Australia}
  \postcode{3800}
}

\author{Brendan Tidd}
\email{brendan.tidd@hdr.qut.edu.au}
\orcid{0000-0002-7721-7799}
\affiliation{%
  \institution{Australian Research Council Centre of Excellence for Robotic Vision, Science and Engineering Faculty, School of Electrical Engineering \& Robotics, Queensland University of Technology}
  \streetaddress{2 George Street}
  \city{Brisbane}
  \state{Queensland}
  \country{Australia}
  \postcode{4000}
}

\author{Dylan Campbell}
\email{dylan@robots.ox.ac.uk}
%\orcid{0000-0002-7721-7799}
\affiliation{%
 % \institution{Australian Research Council Centre of Excellence for Robotic Vision, Science and Engineering Faculty, School of Electrical Engineering \& Robotics, Queensland University of Technology}
  %\streetaddress{2 George Street}
  %\city{Brisbane}
  %\state{Queensland}
  %\country{Australia}
  %\postcode{4000}
}

\author{Dana Kuli{\'c}}
\email{dana.kulic@monash.edu}
\orcid{0000-0002-4169-2141}
%\authornotemark[1]
\affiliation{%
  \institution{Australian Research Council Centre of Excellence for Robotic Vision, Faculty of Engineering, Monash University}
  \streetaddress{18 Alliance Lane}
  \city{Clayton}
  \state{Victoria}
  \country{Australia}
  \postcode{3800}
}

\author{Peter Corke}
\email{peter.corke@qut.edu.au}
\orcid{0000-0001-6650-367X}
\affiliation{%
  \institution{Australian Research Council Centre of Excellence for Robotic Vision, Science and Engineering Faculty, School of Electrical Engineering \& Robotics, Queensland University of Technology}
  \streetaddress{2 George Street}
  \city{Brisbane}
  \state{Queensland}
  \country{Australia}
  \postcode{4000}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Robinson et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Robotic vision for human-robot interaction and collaboration is critical given the rapid improvements computer vision can confer on vision-related robot functions, leading to robots that better adapt to human needs and preferences. This survey and systematic review presents a comprehensive report on robotic vision in human-robot interaction and collaboration, including trends, prevalence, interaction taxonomies, vision techniques and models, training datasets, participant samples, evaluation of robotic vision, and challenges for the field. Systematic extraction and evaluation identified 302 papers for robots that had a mobile, manipulation and/or non-verbal communication capacity in the last 10 years that had direct benefit or value to a person's work or lifestyle. The review explored general trends, common application and domain areas, relevant taxonomies, datasets, models, state-of-the-art algorithms, participant samples and upcoming challenges for the field.  \end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002944.10011122.10002945</concept_id>
       <concept_desc>General and reference~Surveys and overviews</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10003120.10003121.10003126</concept_id>
       <concept_desc>Human-centered computing~HCI theory, concepts and models</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010178.10010224.10010225.10010233</concept_id>
       <concept_desc>Computing methodologies~Vision for robotics</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{General and reference~Surveys and overviews}
\ccsdesc[300]{Human-centered computing~HCI theory, concepts and models}
\ccsdesc[500]{Computing methodologies~Vision for robotics}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{robotic vision, human-robot interaction, gesture recognition, robot movement in human spaces, object handover,  collaborative actions, learning from demonstration, social classification and communication}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle
\section{Introduction}

\cite{arumbakkam_multi-modal_2010,budiharto_indoor_2010,chen_integrated_2010,chen_approaches_2010,couture-beil_selecting_2010,fahn_real-time_2010,kobayashi_people_2010,manigandan_wireless_2010,martin_estimation_2010,sisbot_synthesizing_2010,sosnowski_mirror_2010,yun_robust_2010,tan_safety_2010,droeschel_towards_2011,luo_human_2010,masmoudi_expressive_2011,broccia_gestural_2011,ding_optimizing_2011,igorevich_behavioral_2011,jia_autonomous_2011,lam_real-time_2011,luo_tracking_2011,milligan_selecting_2011,nair_3d_2011,park_real-time_2011,saegusa_cognitive_2011,van_den_bergh_real-time_2011,yoshida_evaluation_2011,alvarez-santos_feature_2012,burger_two-handed_2012,chen_person_2012,cheng_design_2012,csapo_multimodal_2012,faudzi_real-time_2012,foster_two_2012,gori_all_2012,lambrecht_spatial_2012,lichtenstern_prototyping_2012,mead_probabilistic_2012,paulo_vision-based_2012,stipancic_programming_2012,torres_implementation_2012,wu_accompanist_2012,bdiwi_handing-over_2013,abidi_human_2013,cheng_multiple-robot_2013,ferrer_robot_2013,indrajit_development_2013,nishiyama_human_2013,pereira_human-robot_2013,qian_visually_2013,tao_multilayer_2013,wang_vision-guided_2013,wang_wheeled_2013,weinrich_appearance-based_2013,werner_evaluation_2013,yang_study_2013,yun_robotic_2013,alonso-mora_human_2014,castellano_context-sensitive_2014,chen_stereovision-only_2014,do_human-robot_2014,fujii_gesture_2014,hu_design_2014,miyoshi_above_2014,morato_toward_2014,munaro_fast_2014,nguyen_audio-visual_2014,petric_online_2014,prediger_robot-supported_2014,scheggi_human-robot_2014,shieh_fuzzy_2014,taheri_social_2014,tseng_multi-human_2014,xu_online_2014,ali_improved_2015,canal_gesture_2015,cech_active-speaker_2015,cherubini_unified_2015,choudhary_real_2015,cicirelli_kinect-based_2015,fareed_gesture_2015,gao_humanoid_2015,gupta_robust_2015,jevtic_comparison_2015,katsuki_high-speed_2015,lalejini_evaluation_2015,li_visual_2015,quintero_visual_2015,saleh_nonverbal_2015,vircikova_teach_2015,yang_real-time_2015,bayram_audio-visual_2016,christiernin_interacting_2016,ehlers_human-robot_2016,hassan_computationally_2016,ikai_robot_2016,kahily_real-time_2016,ke_vision_2016,koppula_anticipating_2016,koustoumpardis_human_2016,mollaret_multi-modal_2016,moreno_path_2016,potdar_learning_2016,ratul_gesture_2016,talebpour-board_2016,vasconcelos_socially_2016,vaufreydaz_starting_2016,zhang_optimal_2016,angonese_multiple_2017,arenas_deep_2017,bellarbi_social_2017,bilac_gaze_2017,deepan_raj_static_2017,dometios_real-time_2017,ghandour_human_2017,gorer_autonomous_2017,han_human_2017,ju_integrative_2017,lee_learning_2017,maurtua_natural_2017,meghdari_real-time_2017,shariatee_safe_2017,song_towards_2017,tolgyessy_foundations_2017,wang_collision-free_2017,yao_monocular_2017,yoo_gaze_2017,zhu_robust_2017,vasquez_deep_2017,araiza-lllan_dynamic_2018,bolano_towards_2018,bothe_effective_2018,du_online_2018,efthymiou_multi-_2018,gong_research_2018,gui_teaching_2018,hong_interactive_2018,kalidolda_towards_2018,lavanya_gesture_2018,li_learning_2018,long_kinect-based_2018,mao_medical_2018,pasinetti_development_2018,pentiuc_drive_2018,sorostinean_activity_2018,weber_follow_2018,yuan_development_2018,zhang_indoor_2018,zhang_interactive_2018,zhang_automating_2018,cazzato_real-time_2019,chalvatzaki_learn_2019,chen_wristcam_2019,chen_human-following_2019,chien_navigating_2019,condes_person_2019,fang_vehicle-mounted_2019,haghighi_integration_2019,jarosz_detecting_2019,kogkas_free-view_2019,landi_prediction_2019,li_inferring_2019,li_real-time_2019,li_cnn_2019,lima_real-time_2019,luo_human-robot_2019,luo_real-time_2019,martin_real-time_2019,mazhar_real-time_2019,miller_self-driving_2019,moh_gesture_2019,scimmi_experimental_2019,sriram_mobile_2019,valle_personalized_2019,vysocky_interaction_2019,wu_improved_2019,yang_socially-aware_2019,zardykhan_collision_2019,zhang_vision-based_2019,anuradha_human_2020,augello_towards_2020,avioz-sarig_robotic_2020,bingol_practical_2020,chan_collision-free_2020,ferraguti_safety_2020,gao_hand_2020,hwang_interactions_2020,lang_research_2020,lee_visual_2020,lee_real-time_2020,liu_dynamic_2020,lu_research_2020,mronga_constraint-based_2020,phong_vietnamese_2020,tarbouriech_bi-objective_2020,van_den_broek_ergonomic_2020,waskito_wheeled_2020,wu_toward_2020,xu_skeleton_2020,yuan_natural_2020,zhang_human_2020,gao_user_2020,almonfrey_flexible_2018,monajjemi_hri_2013,jindai_small-size_2010,hasanuzzaman_adaptation_2010,mendez-polanco_detection_2010,weiss_robots_2010,pustianu_mobile_2011,sanna_kinect-based_2012,celik_development_2012,gu_human_2012,lee_human_2012,de_luca_integrated_2012,lee_interactive_2012,konda_real_2012,li_cyber-physical_2013,cid_real_2013,das_attracting_2013,schmidt_contact-less_2013,pfeil_exploring_2013,hartmann_feasibility_2013,granata_human_2013,zhao_interactive_2013,anzalone_multimodal_2013,hegger_people_2013,moe_real-time_2013,baron_remote_2013,ozgur_natural_2014,costante_personalizing_2014,barros_real-time_2014,farulla_real-time_2014,chao_robotic_2014,saveriano_safe_2014,batista_probabilistic_2015,nazari_simplified_2015,zhang_adaptive_2015,saffar_context-based_2015,saichinmayi_gesture_2015,benabdallah_kinect-based_2015,obo_robot_2015,voisan_ros-based_2015,fuad_skeleton_2015,monajjemi_uav_2015,nagi_wisdom_2015,guo_control_2016,simul_support_2016,liu_interactive_2016,maraj_application_2016,manitsaris_fingers_2016,mateus_human-aware_2016,li_id-match_2016,silva_mirroring_2016,zambelli_multimodal_2016,zhu_real-time_2016,agrigoroaie_enrichme_2016,vignolo_computational_2017,barz_evaluating_2017,valipour_incremental_2017,maher_realtime_2017,rehman_target_2017,devanne_co-design_2018,shakev_autonomous_2018,lathuiliere_deep_2018,bras_gesture_2018,mohaimenianpour_hands_2018,costanzo_multimodal_2019,azari_commodifying_2019,yu_efficiency_2019,belo_facial_2019,yu_interactive_2019,paetzel_let_2019,chen_online_2019,svarny_safe_2019,ali_smart_2019,gemerek_video-guided_2019,sun_visual_2019,muller_multi-modal_2020,kawasaki_multimodal_2020,yan_optimization_2020,sanchez-matilla_benchmark_2020,nascimento_collision_2020,fallahinia_comparison_2020,chen_design_2020,pang_efficient_2020,angani_human_2020,medeiros_human-drone_2020,yue_human-robot_2020,terreran_low-cost_2020,hsu_real-time_2020,de_schepper_towards_2020,uribe_mobile_2011,hafiane_3d_2013,mohammad_tele-operation_2013,xiao_human-robot_2014,quintero_interactive_2014,zhao_intuitive_2016,bouteraa_gesture-based_2017,bai_kinect-based_2018,abiddin_development_2019,sripada_teleoperation_2019}




\section{Acknowledgments}
This work was supported by a Research Grant from the State of Queensland acting through the Department of Science, Information Technology and Innovation. 

% Not sure how to add a reference with the correct style?
\bibliographystyle{acm}
% \bibliography{references, includes, included_references}
\bibliography{references, includes}
\end{document}
\endinput
%%
%% End of file `sample-acmsmall.tex'.
