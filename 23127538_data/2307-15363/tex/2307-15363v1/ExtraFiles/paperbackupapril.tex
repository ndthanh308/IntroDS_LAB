%%
%% This is file `sample-acmsmall.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `acmsmall')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-acmsmall.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[acmsmall]{acmart}
%% NOTE that a single column version is required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{10.1145/1122445.1122456}


%%
%% These commands are for a JOURNAL article.
\acmJournal{JACM}
\acmVolume{37}
\acmNumber{4}
\acmArticle{111}
\acmMonth{8}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}
% \usepackage{booktabs, multirow} % for borders and merged ranges
% \usepackage{soul}% for underlines
% \usepackage[table]{xcolor} % for cell colors
% \usepackage{changepage,threeparttable} % for wide tables
\usepackage{float}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{xcolor}
% \usepackage{pgfplotstable,filecontents}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.9}% supress warning
%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Robotic Vision for Human-Robot Interaction and Collaboration: A Comprehensive Survey and Systematic Review}
%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Nicole Robinson}
\email{nicole.robinson@monash.edu}
\orcid{0000-0002-7144-3082}
\authornotemark[1]
\affiliation{%
  \institution{Australian Research Council Centre of Excellence for Robotic Vision, Faculty of Engineering, Turner Institute for Brain and Mental Health, Monash University and Queensland University of Technology}
  \streetaddress{18 Alliance Lane}
  \city{Clayton}
  \state{Victoria}
  \country{Australia}
  \postcode{3800}
}

\author{Brendan Tidd}
\email{brendan.tidd@hdr.qut.edu.au}
\orcid{0000-0002-7721-7799}
\affiliation{%
  \institution{Australian Research Council Centre of Excellence for Robotic Vision, Science and Engineering Faculty, School of Electrical Engineering \& Robotics, Queensland University of Technology}
  \streetaddress{2 George Street}
  \city{Brisbane}
  \state{Queensland}
  \country{Australia}
  \postcode{4000}
}

\author{Dana Kuli{\'c}}
\email{dana.kulic@monash.edu}
\orcid{0000-0002-4169-2141}
\authornotemark[1]
\affiliation{%
  \institution{Faculty of Engineering, Monash University}
  \streetaddress{18 Alliance Lane}
  \city{Clayton}
  \state{Victoria}
  \country{Australia}
  \postcode{3800}
}

\author{Peter Corke}
\email{peter.corke@qut.edu.au}
\orcid{0000-0001-6650-367X}
\affiliation{%
  \institution{Australian Research Council Centre of Excellence for Robotic Vision, Science and Engineering Faculty, School of Electrical Engineering \& Robotics, Queensland University of Technology}
  \streetaddress{2 George Street}
  \city{Brisbane}
  \state{Queensland}
  \country{Australia}
  \postcode{4000}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Robinson et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
This survey provides a comprehensive overview on the use of on robotic vision for human-robot collaboration. For scope, this systematic review focuses on in-depth review of papers from the past 10 years to demonstrate the most current techniques and applications to understand and respond to human behaviour. This survey has reviewed papers to answer key questions about robotic vision in human-robot collaboration, which involves what are the most common forms of vision, domains areas, algorithm benchmarks such as speed, performance and accuracy. This paper intends to present a synthesis of results on the functional impact of human-robot collaboration via robotic vision, and upcoming challenges for the field. This will conclude on a set of recommendations for engineering domain areas for improvement, and domains from the human-robot collaboration field that call for better use of vision in real-world applications
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002944.10011122.10002945</concept_id>
       <concept_desc>General and reference~Surveys and overviews</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10003120.10003121.10003126</concept_id>
       <concept_desc>Human-centered computing~HCI theory, concepts and models</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010178.10010224.10010225.10010233</concept_id>
       <concept_desc>Computing methodologies~Vision for robotics</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{General and reference~Surveys and overviews}
\ccsdesc[300]{Human-centered computing~HCI theory, concepts and models}
\ccsdesc[500]{Computing methodologies~Vision for robotics}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
%\keywords{datasets, neural networks, gaze detection, text tagging}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle
\section{Introduction}
This survey presents a comprehensive review on the use of robotic vision for human-robot collaboration and interaction. It includes information on how robotic vision has been applied to common human-machine interaction problems, such as information exchange, control, social communication, and task execution. This survey provides scoping information on common domains of human-robot interaction in which robotic vision has helped to make notable impact, such as gesture recognition, robot movement in human spaces, object handover and collaboration action, learning from demonstration, and social classification and communication. This survey also explores how robotic vision has been used to improve human-robot collaboration such as team performance, role definition, team formation, and task completion. To answer research questions about the impact of robotic vision in human-robot collaboration and interaction, we present a systematic review that extracts key papers from the last decade used defined search criteria. We identify emergent trends, patterns and conclusions on the use and functional impact of robotic vision. To conclude, this paper discusses upcoming challenges for robotic vision in human-robot collaboration and interaction work. This includes how research and engineering work from computer vision can be applied to improve human-robot collaboration, and how human-robot collaboration can help to inform what are critical elements for computer vision research when it comes to humans in robot spaces. The survey concludes with a discussion on the implications of robotic vision work for human-robot collaboration, and key future research goals for the field. 

\section{Computer Vision and Human Actions} 
Computer vision run through robots has been a key perception channel for robots in relation to improving human-robot interaction and collaboration. Computer vision aims to identify and understand information in digital images and videos. From this information, the robotic system can make an informed decision based on visual information from the scene, object or human, helping the robot to act in the most useful way to the person. A vision system that informs the actions of a robot is often identified as robotic vision. Common sensors in robotic vision include cameras and depth sensors which provide rich information about the world, such as being able to recognise and respond to different objects, environments, scenes and actions, including from humans. Examples of computer vision applications include being able to use image detection to localise where a person is in the scene, image classification to determine what it is in the image, and segmentation to classify what part of the image belongs to an area of interest in the scene, such as a person. Computer vision can allow a robot to understand more information from a scene compared to other sensors, such as to create a 3D image from a 2D image, providing more detailed sensory data to create smart robot collaboration. Robotic vision provides significant advantages such as real-time processing, low expense of cameras as a sensor, rich sensory data from images such as colours, and ability to extract detailed information, which all works to improve robots behaviour with and around humans. Robotic vision has seen significant improvements stemming from the computer vision research community. This has been in large from the rise in large data-sets, computing power, complex algorithms and scientific method. This was paired with the emergence key progress developments in related areas, such as the use of large data sets to improve recognition efforts in computer vision. 

To build human-centred applications, visual information can be utilised by a wide range of computer vision algorithms. Computer vision also allows individuals to interact with machines using a modality that is more natural and intuitive to humans. For instance, computer vision can help to remove the need for using terminals to communicate to the robot, the need to wear physical apparatus, the learning curve to use teach pendants to signal information. This also includes being able to provide real-time information in a way that does not inhibit natural actions or reactions from humans, given that requiring people to enter data in a terminal or use a teach pendant can alter their behaviour in a way that is unproductive to the task. This can provide an autonomous platform with greater capacity to respond in an intelligent way. Some examples of vision applications to improve HRC/I include gesture recognition, scene classification, and object segmentation, expanding the ability for the robot to provide a more functional service to the human. Common applications include pose estimation, which can detect and track the location of a person or object in the scene. However, robotic vision and computer vision applications is still emerging for humans. Reasons include that people are unpredictable, human-based data-sets can be more challenging to collect with the need for informed consent for their use, and emergent capacity of computer vision to capture and interpret human data in near real-time. 

\subsubsection{Gesture and Action Recognition}
Gesture recognition is a communication method used in human-robot collaboration to translate the pose of the human into instructions for a robot. Gesture recognition is a preferred communication method over alternatives such as speech recognition in domains that are noisy (industrial, outdoor). Gesture recognition is often performed using an RGB-D sensor~\cite{katsuki_high_speed_2015,gori_multitype_2016,tsiami_multi3_2018,jevtic_comparison_2015,shukla_probabilistic_2015}, or stereo cameras~\cite{xia_vision-based_2019}. Off the shelf sensors such as the Microsoft Kinect provide a gesture recognition SDK for practitioners to provide the functionality to suitable robotics platforms~\cite{jevtic_comparison_2015}. OpenNI is an alternative commonly used off the shelf software. Mitra et al~\cite{mitra_gesture_2007} classify three types of gestures: hand and arm, head and face, and body. Body gestures: full body actions or motions, Hand and arm gestures: arm poses, hand gestures, and Head and facial gestures: nodding or shaking head, winking lips.

Gesture recognition is an important tool for HRC, where gestures can be used to signal which object to interact with~\cite{shukla_probabilistic_2015, shukla_proactive_2017}, where a robot should move~\cite{jevtic_comparison_2015}, and level of human engagement~\cite{tsiami_multi3_2018}. Robots that have been used with gesture recognition include social platforms (Furhat, NAO, Zeno)~\cite{tsiami_multi3_2018}, mobile platforms~\cite{jevtic_comparison_2015}, and grasping aids~\cite{shukla_probabilistic_2015, shukla_proactive_2017}. Xia et al~\cite{xia_vision-based_2019} provide survey on vision-based hand gesture recognition for HRC. This work identifies hand gestures as an effective way to interact with humans in an industrial environment. Accuracy and rapidity of hand gesture recognition directly affects the fluency and naturalness of HRC. They found that most research assumes hand detection background is simple, but this is not the case industrial applications. They also highlight hard real-time most important requirement. Taking users as the core, building the system around the human to make collaboration, easily and accurately convey desired information. This work identifies four key components to gesture recognition: sensor technologies, gesture identification, gesture tracking, and gesture classification. Also discussed is the importance of hard real-time gesture recognition systems for the safety of the human in HRC manufacturing teams. With the current trends in sensor usage, the best system performance can be achieved with multiple sensor modalities in the same system. Similarly, combinations of algorithms can improve efficiency. Algorithms have been developed for identifying hand gestures~\cite{xia_vision-based_2019} and activity recognition from multiple people in a robots view (RGBD)~\cite{gori_multitype_2016}. Action recognition application use cases include vision-based fallen person detection \cite{Solbach_2017_ICCV}. 

% Action recognition is the identification and classification of human movements. The use of action recognition in HRI/C allows robots to predict what a human is doing so they may provide functional assistance to the person. The actions performed by humans typically do not have the intention of robot control, and robots must be able to anticipate the movements of the person so they may provide assistance. 

\subsubsection{Robot Movement in Human Spaces}

\subsubsection{Object Handover and Collaborative Actions}

Visual information can support robot handover operations by perceiving and interpreting humans and objects in the scene, reach ability, motion consistency and collision avoidance \cite{8955665}. 

\subsubsection{Learning from Demonstration}

\subsubsection{Social Classification and Communication}

\section{Human-Robot Collaboration and Interaction} 

Human-robot interaction is directed around interactivity between humans and robots, aiming to develop robots that can interact with people in different everyday environments. Human-robot interaction often involves work to create socially interactive robotic systems that can address and respond to the complexities of dynamic human behaviour to improve the effectiveness and efficacy of the collaboration. For instance, human-robot interaction work might focus on improving the social acuity of the robot to respond to human-based visual information, or improve robot motion to better integrate into human-dense environments \cite{bartneck2020human}. Human-robot collaboration instead addresses more research around human and robots working together to form a team with the intention to achieve shared goals with a common purpose and goal towards a directed outcome \cite{bauer2008human}. Human-robot collaboration facilitates interaction and shared work, pairing together the advances of machines in routine work with the work environment with humans. These types are robots can also be called cobots, robots that are designed to physically interact with humans in a shared work space to complete a set of tasks. In the process of a human and robot working together, it is common for the human to create the intended goal, and the robot to assist or support the human to reach the proposed outcome \cite{bauer2008human}. Human-robot interaction and collaboration can also include different team variants, including individual one-to-one interaction, or team compositions, which can influence how likely individuals are to interact with robots, levels of attention given to robots, and and can be characterised by psychological elements such as perception differences for in-group membership with robot teams \cite{sebo2020robots}. The future pathway for improved human-robot integration is to develop robotic platforms that can become more capable in the less structured domestic setting where the domain is human-related.

Human-robot collaboration is suited for applications where a human component is still required. Autonomous systems that have no ability to make decisions can have significant setbacks in limited applications for humans to work alongside these systems. Specific tasks that have been at the forefront of human-robot collaboration include object handover \cite{8955665}. Some key examples of collaborative actions with humans involves humans using their hands, voice or gestures to operate the robot, or direct the robot to initiate an action. Support from robots can also help to reduce repetition fatigue, injuries and human errors. The future aim is to help with physical interaction, such as reducing lifting fatigue or item assembly in conjunction with the person. In these examples, collaboration with a robot can enhance the speed, productivity or safety of the human as the robot complements or adds value to the objective. There are also a wide variety of factors that can influence the efficacy of humans working with robots that are outside of engineering, such as the formation and trust repair between humans and machines \cite{10.1007/978-3-030-62056-1_44}. For instance, trust can influence misuse of the system, or disuse as a result of an under-trust in the system \cite{10.1007/978-3-030-62056-1_44}.

Common robots used in human-robot interaction and collaboration work include mobile platforms (Pioneer, Turtlebot), robotic arms that are able to manipulate objects (Kinova Jaco, Kuka), or robots designed to provide socially interactive  support through other movements in a social setting (NAO, Pepper). Domains that have more frequently investigated the use of human-robot interaction and collaboration have included human-central fields such as agricultural use \cite{VASCONEZ201935}, domestic and urban settings, such as public shopping spaces \cite{10.1007/978-3-319-47437-3_74}, healthcare clinics or hospitals \cite{RobinsonSys} and educational settings \cite{belpaeme2018social}.

It is important that autonomous systems such as collaborative robots are able to identify and understand the intention of human collaborators and respond with effective actions. For the robot to behave in desirable ways for the person, the robot must be able to sense, perceive and respond to human actions, intentions and states. Important elements often addressed in human-robot interaction and collaboration across multiple domains include productivity, efficiency, safety and adaptability of the robot with the human \cite{VASCONEZ201935}. For example, key issues around the use of robots with humans is the safety of the human \cite{HaddadinSami2009RfSR}. Robots are therefore programmed to use action such as co-ordination in the shared space to avoid collision, reducing the chance of harm coming to the person \cite{VillaniValeria2018Sohc, robla2017working}. This can be performed by programming the robot to sense and act on information provided by the human and the environment. 

\section{Systematic Review and Intended Outcomes}
To understand the field of robotic vision for human-robot interaction/collaboration, there is a need to conduct a thorough examination of research papers in a systematic way using search criteria and a methodological process. Such a systematic review will help to detail patterns and trends, providing a clear picture of the overall field in the last 10 years. This includes being able to identify and extract information about common use cases such as domain, application area, and key tasks that use robotic vision. This type of review can work to clearly deduct systematic and robust conclusions about prevalence and functional impact of robotic vision in HRC/I. This can involve making conclusions such as what percentage of work involves different types of cameras, the most common robotic vision techniques, and how human-robot team performance is impacted when computer vision is central to the interaction. 

A gold-standard reporting method used to conduct such a systematic review is the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines \cite{moher2009preferred}. The PRISMA method for conducting a systematic review has been cited over 80,000 times in the last 15 years, showing the impact the methodology has to answer research questions through systematic synthesis of information. This method involves a standard checklist and reporting methodology for systematic evaluation of research, allowing clear conclusions to be drawn across a large pool of studies. The PRISMA method also intends to reduce the risk of reporting bias, given that studies found using the criteria are included into the systematic review for analysis \cite{moher2009preferred}. This PRISMA-style systematic review will explore different disciplines related to robotic vision, which include collaborative tasks and communication behaviours involved in human-robot collaboration and interaction. Communicative behaviours not in the scope of this paper include verbal communication (paralanguage), including the use of voice even when paired with non-verbal communication to portray messages or intended cues using techniques such as pitch, volume, intonation, and prosody. It is intended that this review will inform readers about the current capacity in robotic vision to interpret and respond to human actions, activities/tasks, states and emotions. We investigated papers that had robots with a mobile, manipulation and/or non-verbal communication capacity that could provide direct benefit or value to the persons work or lifestyle. From literature and research trials from the last 10 years, this survey will answer the presented research questions:

\begin{enumerate}
    \item What is the overview for robotic vision in HRC/I in the last 10 years?  
    \item What is the current state-of-the-art applications of robotic vision for HRC/I? 
    \item What is the functional impact of robotic vision in HRC/I?
    \item What are the upcoming challenges for robotic vision in HRC/I?
\end{enumerate}

\section{Methodology and PRISMA Review Protocol}
This systematic review protocol followed the Preferred Reporting Items for Systematic Reviews and Meta-Analyses guidelines for search, screening, and evaluation. The databases chosen for search included IEEE Xplore, ACM Library, and Scopus. Search terms were as follows: 
(("Document Title": Robot*) AND ("Abstract":Vision OR "Abstract":Camera OR "Abstract":RGB) AND ("Abstract":Human OR "Abstract":Person OR "Abstract":User) AND ("Abstract":Collaborat* OR "Abstract":Interact* OR "Abstract":"Human-in-the-Loop")). The search terms were chosen to be broad enough to capture papers across multiple disciplines, but also scoped to the topic of interest: robotics, vision, and humans. The choice to include 'Robot' in the title was to ensure identified papers were those in which authors referred to their system as a robot. This also meant that it did not require external evaluation for each paper on what is considered a robot. This method would also exclude machines that were intended to be tele-operated or limited with no sensing capacity. The following inclusion criteria were chosen for the review:

\begin{enumerate}
    \item There must include at least one physically embodied robot that can perceive through a vision system
    \item The robot(s) must be capable of at least one closed-loop interaction or information exchange between the human and the robot(s), where the robot(s) vision system is utilised in the exchange, and a human is the focus of the vision system
    \item The robot(s) must be able to make a decision and/or action based on visual input that is real time or at least fast enough for an interactive channel to occur between the human and the robot (i.e. ~60 seconds). This excludes papers where the camera/robot acted as a sensor, and did not perform actions in response to its perception.
\end{enumerate} 

Exclusion criteria involved non-embodied agents, such as digital avatars and conversational agents, which would necessitate their own systematic review protocol. Exclusion involved those with no closed-loop interaction, in which robotic vision did not substantiate interaction if only one signal was received. Others included those where robotic vision is not a central component to the system. An example of this involved the main interaction component being involved in speech recognition commands, and the camera system operating but not determining the actions or behaviour of the robot in the closed loop interaction. Other forms of robotic vision applications that were excluded involved low-functionality robotic devices (i.e. entertainment or children's toys) and other non-robotic devices with no autonomous actuation (terminals, computers, smartphones, tele-operated devices etc). Other exclusion criteria involved papers with limited to no experimental testing, such as design papers without research data on system performance with the human in a human-robot interaction or collaboration task. This included robots in challenge scenarios that only reported competition scores rather than experimental data in the form of a user study. Other search exclusion criteria involved terms related to surgical robots that are tele-operated by surgeons, as well as other types of robot tele-operation via remote control. Inclusion and exclusion criteria were reviewed and approved by subject matter experts to confirm their ability to accurately capture papers relevant to robotic vision for human-robot collaboration. Papers needed to have been published between 2010 to 2020 in a peer-reviewed academic venue such as a journal, conference, or listed online in a research repository as a completed piece of work with some form of peer-review. In the instance of papers reporting multiple versions of the work, the most complete version was chosen for inclusion.  

% \textcolor{red}{Exclusion considerations:
% \begin{itemize}
%     \item Criteria 1: Can the robot be replaced? (e.g. not just used to collect data with a camera, person moving a camera to collect data, or computer screen)
%     \item Criteria 2: Can the human be replaced? e.g. not just dynamic obstacle detection
%     \item An option if we have too many: The human must be part of the vision sensing i.e. human sensing with vision, human-vision? (excludes human operating through vision e.g. AR or GUI), though would also exclude robots where other sensors (speech) are used for human interaction and robotic vision is used for the task. Would also exclude grasp assistant robots where vision isn't used for object selection (though there could be other uses for human-vision in this case e.g. handover).
% \end{itemize}}

\section{Results}
\subsection{Selected Articles}
The initial search conducted yielded 2252 papers, 671 of which were duplicate records. A total of xxx papers were then screened for titles and abstracts for reviewing eligibility in the initial exclusion phase, resulting in xxx papers omitted on exclusion for reasons such as being a review (n = xxx), a book chapter (n = xxx), a thesis (n = xxx) or other type of non-related work (n = xxx). A total of xxx were excluded based on title for focusing on surgical (n = xxx) or teleoperation (n = xxx). The remaining xxx papers were assessed in detail by reviewing full text, resulting in xxx papers omitted on exclusion Criteria 1-3: xxx on C1, xxx on C2 and xxx on C3, leaving xx papers that met full inclusion criteria. Lastly, xxx records were excluded at the full text phase for not having robotic vision as the central component (n = xxx), robotic toys (n = xxx) or no experimental testing (n = xxx). See Figure ~\ref{fig:consort_chart} for the CONSORT chart. The following sections will provide a brief breakdown of each of the main application areas that require robotic vision, including the purpose, intended use, current level of performance, and detailed examples of its propose use in action. 

% Figure environment removed

% =============================================
% ---------------------------------------------
\subsection{Excluded Work}
% ---------------------------------------------
% =============================================
A total of 47 papers were excluded on C1. Common trends included no robot~\cite{ueno2014efficient, zhao2016intuitive, hacinecipoglu2020pose, mcfassel2018prototyping}, a camera on its own such as a motorized camera system~\cite{moladande2019implicit}. Others included simulation only~\cite{yamamoto20112d, papadopoulos2019advanced}, such as a virtual robot~\cite{du2014markerless, ben2016kinect}. Others did not involve vision with the robot capabilities, such as using leap motion \cite{pop2019control} and IMUs~\cite{mendes2017human}. A total of 30 papers were excluded on C2. Excluded papers included ones that more focus on other aspects, such as speech \cite{liu2018learning} or visual servoing \cite{jiang2018personalize}. Others included no humans \cite{del2011interaction}, such as the human not being involved or required in the vision process \cite{haddadin2011towards, randelli2013knowledge,cicconet2013human, khatib2017visual, yamakawa2018human}. A total of 66 papers were excluded on C3. Excluded papers often involved ones that did not have a robot action based on the vision with the robot acting only as a sensor \cite{tornow2013multi, mckeague2013hand, zhang2013real, massardi2020parc, gonzalez2020audiovisual}. Few exclusions involved the robot being operated using a Wizard of Oz response \cite{rehm2013negative} or did not have a near real-time information exchange \cite{chen2020human}.

\section{Research Question 1. What is the overview of robotic vision in human-robot collaboration and interaction in the last 10 years?}

Here we present an overview for prevalence and trends in research papers published in the past 10 years. All presented papers (218) met the proposed systematic review criteria of robotic vision in HRC/I tasks. Results from included papers found an increased interest in the usage of robotic vision for HRC/I tasks (See Fig~\ref{fig:total_papers}). Data findings revealed clear trends towards increased volume of papers published within the robotic vision field for HRI/HRC in the past 5 years (See Fig~\ref{fig:total_papers}). This included more experimental study and use case scenarios emerging over time. Recent papers from 2020/2021 reported notable difficulties to continue research during the COVID-19 pandemic, impacting capacity to test robotic vision with human participants. Geographical origin for each research paper was higher for the United States of America, Europe and Asia, with the location determined based on the first authors location listed on the publication. (See Fig~\ref{fig:total_papers}.b and Fig~\ref{fig:papers_per_continent}).

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

The most common application areas and domain areas of robotic vision are listed here: Action recognition, gesture recognition, robot movement in human spaces, object handover and collaborative actions, learning from demonstration and social classification and communication (See  Fig~\ref{fig:pi_breakdown}). The most common domain areas for the use of robotic vision in HRI/HRC involved field and outdoor, industrial settings such as including manufacturing and warehouses, domestic and home use cases, and public spaces such as shopping centres, schools, restaurants, and hotels (See  Fig~\ref{fig:pi_breakdown}). The most popular robot was a mobile robot, followed by a fixed manipulator, social robot, mobile manipulator and unmanned aerial vehicle (UAV) (See Fig~\ref{fig:pi_breakdown}). For camera type, RGD-B cameras were the most popular, including the Kinect holding a large portion of camera type. This was followed by monocular, stero and omni-directional cameras (See Fig~\ref{fig:pi_breakdown}). Trends could be seen with the increase and rapid uptake of RGB-D cameras between 2010 and 2020 (See Fig~\ref{fig:camera_type})

%For each section:
%\begin{itemize}
%   \item Overview and break down of papers:
%    \begin{itemize}
%        \item action: types of actions, role of the human
%        \item gesture form (hand, body, head) + specific gestures + role of the human (e.g. robot control))
%        \item robot movement (approaching, following, avoiding) + how does the human interact, common usage (person as a user, or controller), 
%        \item manipulation (handover, shared workspace) + how does the human interact + role of human 
%        \item social (...)
%    \end{itemize}
%    \item How was vision used:
%    \begin{itemize}
%        \item action
%        \item gestures: hand/body classification
%        \item movement: human pose, how was tracking performed?
%        \item handovers: tracking of human pose/hand
%        \item social: 
%    \end{itemize}
%    \item Specific tasks (robot action how and why). These should be clustered somehow, into task area? Don't need every paper just a few examples for each task area?:
%    \begin{itemize}
%        \item actions:
%        \item gestures:
%        \item movement:
%        \item handovers:
%        \item social:
%    \end{itemize}
%\end{itemize}
%a safety precaution, environmental awareness in relation to the human, and social interaction to facilitate a functional goal between a human and a robot. [Examples]

% ==============================================================
% --------------------------------------------------------------
\section{Research Question 2A. What are the most common application areas and domains of robotic vision in HRC/I?}
% --------------------------------------------------------------
% ==============================================================

The following section provides a breakdown of the main application areas that used robotic vision: gesture recognition, action recognition, robot movement in human spaces, object handover and collaborative actions, learning from demonstration, and social classification and communication. This included describing the purpose, intended use, current level of performance, and detailed examples of the task in action. If a paper had more than one application area, each application area was included in the final score. Total paper count for all application areas is greater than the total paper count. 

% ==============================================================
\subsection{Gesture Recognition}
% ==============================================================

Gesture recognition was used as a robotic vision method to interact with robots in diverse applications. In the 217 papers, 88 papers (43\%) described at least one form of gesture recognition that used robotic vision. From the 88 papers, 61 (29\%) involved gestures from body pose, 27 (13\%) robot implementations involved a hand gesture, and 6 (3\%) used a combination of head gestures or facial expressions. Specific gesture recognition actions included pointing ($n=22$), making a fist ($n=12$), waving ($n=7$), shaking ($n=1$), thumbs up ($n=3$), and other forms of gestures ($n=xxx$). A large majority of gestures were static ($n=75$), which meant that no visual detection of movement was required for gesture recognition. Static gestures often involved a single gesture from a stationary human pose with a smaller portion of gesture recognition being dynamic ($n=16$) and requiring multiple frames to classify the gesture. Static gestures could also signify the start of more dynamic gestures~\cite{xu_online_2014}. Gesture recognition often involved hand waves~\cite{couture-beil_selecting_2010, cheng_design_2012,tao_multilayer_2013, fujii_gesture_2014,canal_gesture_2015}, pointing~\cite{martin_estimation_2010, luo_tracking_2011, milligan_selecting_2011, abidi_human_2013, prediger_robot-supported_2014,jevtic_comparison_2015, maurtua_natural_2017, tolgyessy_foundations_2017}, and the thumbs up gesture ~\cite{manigandan_wireless_2010, faudzi_real-time_2012, vysocky_interaction_2019}. A frequent hand gesture was the `stop' signal to direct the robot to cease further action. This hand gesture was often commanded by the human using a flat palm facing directly towards the robot, e.g. ~\cite{faudzi_real-time_2012, ehlers_human-robot_2016}. Hand gestures often took little effort for the human to perform for it to be detected (for example making a fist, or performing a thumbs up~\cite{manigandan_wireless_2010, faudzi_real-time_2012, vysocky_interaction_2019,pereira_human-robot_2013, petric_online_2014, waskito_wheeled_2020}). Gestures were also used with teams of robots~\cite{milligan_selecting_2011,lichtenstern_prototyping_2012, nishiyama_human_2013, monajjemi_hri_2013, alonso-mora_human_2014, canal_gesture_2015}, and by multiple humans~\cite{luo_tracking_2011}. Common use cases for visually detecting a gesture involved for robot control ($n=xxx$), to signal for the robot to change its state ($n=xxx$), teaching ($n=xxx$) and to engage in social interactivity ($n=xxx$). % 28, 43, 3 
The most common use for robotic vision in gesture recognition was a non-contact method to control the robot from a short distance ($n=74$, xxx\%). Gesture recognition to control robots was often used for industrial robot arms, e.g. ~\cite{fareed_gesture_2015, song_towards_2017,lima_real-time_2019, martin_real-time_2019}, 
mobile robots, e.g. ~\cite{zhang_interactive_2018, chen_human-following_2019, miller_self-driving_2019}, mobile robots with a manipulator, e.g. ~\cite{droeschel_towards_2011, canal_gesture_2015, moh_gesture_2019, sriram_mobile_2019}, social robots, e.g. ~\cite{kalidolda_towards_2018}, and unmanned aerial vehicle, e.g. ~\cite{lichtenstern_prototyping_2012, monajjemi_hri_2013}. To control the robot, body gestures (often including hand gestures) were often used as the control signal to instruct the robot to perform an action ($n=43$). Continuous control was used in some papers,  such as mobile robots with hand position 
~\cite{paulo_vision-based_2012}, and small mobile robots with head positions ~\cite{lam_real-time_2011}. 

\subsubsection{Control (Mobile):} Common instructed actions included directional movement for mobile robots, such as hand gestures to signal moving forward, left, right, or stop, e.g. ~\cite{lam_real-time_2011, cicirelli_kinect-based_2015,fareed_gesture_2015,lalejini_evaluation_2015, ghandour_human_2017, long_kinect-based_2018, pentiuc_drive_2018, zhang_interactive_2018, chen_human-following_2019, miller_self-driving_2019, li_real-time_2019, chen_approaches_2010,manigandan_wireless_2010,faudzi_real-time_2012, raajan_hand_2013, lavanya_gesture_2018, waskito_wheeled_2020, xu_skeleton_2020}. Pointing has been used for directing mobile robots to a specified location~\cite{park_real-time_2011, van_den_bergh_real-time_2011, yoshida_evaluation_2011, abidi_human_2013, prediger_robot-supported_2014, chen_stereovision-only_2014, jevtic_comparison_2015, tolgyessy_foundations_2017}. Mobile robot have been summoned by waving, and directed to a specific location by pointing~\cite{kahlouche_human_2019}. More refined human movements were also used the control signal, such as shoulder angle to control the heading of a mobile robot using discrete angle, such as 45 degree increments~\cite{wang_wheeled_2013}, or head position~\cite{lam_real-time_2011}. Dynamic gestures such as hand waving have been used to signify a follow-me or bye-bye command for a mobile robot~\cite{fujii_gesture_2014}. Dynamic motions were also used to control a tracked robot forward, backwards, left and right by moving an arm up and down, left to right, or in circles, e.g. ~\cite{fang_vehicle-mounted_2019}. Lastly, body gestures were used to control an otherwise autonomous mobile robot with instructions such as turn and stop from by moving arms up or down~\cite{ghandour_human_2017}. A spherical vision system is used with a mobile robot with three omni-directional wheels to detect pointing gestures from a person wearing a red coat with a blue glove ~\cite{yoshida_evaluation_2011}. The robot moves on a soccer field in the direction of the pointing gesture. The velocity of a wheelchair robot was commanded by human shoulder position, and its configuration was controlled with arm gestures (transitioning between bed mode and sitting mode)~\cite{mao_medical_2018}.

\subsubsection{Control (Manipulators):} Robots were controlled using hand gestures for actions such as opening or closing the gripper~\cite{fareed_gesture_2015, lima_real-time_2019, martin_real-time_2019}, such as using an open palm ~\cite{fareed_gesture_2015, lima_real-time_2019, martin_real-time_2019}), lifting or lowering the arm~\cite{fareed_gesture_2015, deepan_raj_static_2017, song_towards_2017, luo_human-robot_2019}, rotate the arm~\cite{choudhary_real_2015, song_towards_2017}, to place an object into an open palm, return to position when the palm is closed~\cite{arenas_deep_2017}, set positions for lifting, lowering~\cite{fareed_gesture_2015, deepan_raj_static_2017, luo_human-robot_2019}, and rotation~\cite{choudhary_real_2015, song_towards_2017}. Pointing was often used for selecting an object for grasping~\cite{quintero_visual_2015, moh_gesture_2019, sriram_mobile_2019}, such as the robot arm confirming object selection by also pointing at the object. al~\cite{quintero_visual_2015}. Hand gestures were paired with other body movements, such as to robot arms responding to open and closed hand gestures to open or close the gripper by following shoulder and elbow positions~\cite{lima_real-time_2019, martin_real-time_2019}. Other examples with robotic arms used collaborative robot actions to drop a small object into the users palm~\cite{arenas_deep_2017}. Actions included the robot helping to cook, such as dropping confirmed toppings over a pizza base~\cite{quintero_visual_2015}. A robot equipped with two arms, stereo vision, and tactile sensors picks up an object (sponge cube, wooden cube, ping-pong ball) determined by a hand pointing gesture from a human operator~\cite{ikai_robot_2016}. The robot releases the object on the palm of the operator.

\subsubsection{Control (Mobile Manipulators, Social and Unmanned Aerial Vehicles):} In mobile robots with a manipulator, a pointing gesture was used to select a desired object the robot must pick up~\cite{droeschel_towards_2011, qian_visually_2013, canal_gesture_2015, moh_gesture_2019, sriram_mobile_2019}. A mobile robot with a manipulator named Jido responds to gestures that involve both hands, and user speech, to identify, fetch, and handover objects (such as a water bottle)~\cite{burger_two-handed_2012}. In another example with a mobile base with arms, the robot waves back to waving person, and performs behaviours from dynamic gestures (such as circling when a circle is drawn in the air)~\cite{li_real-time_2019}

\subsubsection{State Change:} Gestures were used to signal the robot to change its state ~\cite{pereira_human-robot_2013, lalejini_evaluation_2015, ehlers_human-robot_2016, chen_human-following_2019}. Some examples of state change included to initiate person guiding or following~\cite{pereira_human-robot_2013}, path direction (left or right) of an otherwise autonomous navigating robot~\cite{lalejini_evaluation_2015, ehlers_human-robot_2016}. Hand gestures were used to start or stop walking for a small humanoid~\cite{ratul_gesture_2016}. Body gestures were used as well, such as to start or stop following a person in an indoor environment (two hands up)~\cite{long_kinect-based_2018} or use of a raised left or right arm raised to change between robot following or parking behaviour~\cite{miller_self-driving_2019}. An autonomous mobile navigation robot explores a laboratory, asking humans for directions (when a human is detected) and translated hand gesture pointing to goals into the robot's map~\cite{van_den_bergh_real-time_2011}.

\subsubsection{Learning from Demonstration:} Gestures were used to program industrial robot arms in learning from demonstration tasks~\cite{stipancic_programming_2012, mazhar_real-time_2019}. A pre-determined gesture (hand gesture~\cite{mazhar_real-time_2019} or body gestures~\cite{stipancic_programming_2012}) often signified for the task to be recorded. To demonstrate, ~\cite{mazhar_real-time_2019} programmed the robot to become compliant once the command has been received. Therefore, the human programmer could manually move the end effector to perform a desired action. A second gesture then determined the completion of programming, and for the robot to begin executing a new task. Work by Stipancic et al~\cite{stipancic_programming_2012} showed the person starting the demonstration by lifting one left off the ground, performing a sequence of body gestures that include actions such as opening and closing the gripper, and storing the current tool centre point position. Other examples of human demonstrations use pointing and speech to show an industrial arm where to work~\cite{du_online_2018}. An industrial robot arm tracks hand position with a Kinect sensor and an inertial measurement unit (IMU) worn by the user, while learning from online demonstrations. When the robot end effector gets close to the desired position, fine tuning is performed by speech and static hand gesture pointing.~\cite{du_online_2018}. In another example, the tool centre point of an industrial arm moves in front of a work piece to be machined, as pointed to by a human operator~\cite{maurtua_natural_2017}. 

\subsubsection{Teams:} A team of four mobile robots respond to gestures from a human operator~\cite{milligan_selecting_2011}. The operator selects a group of robots at a time by drawing a circle around the desired group. A robot is selected if the operators face is detected within the cone created by the circle gesture. The selected robots then go to a location pointed to by the operator. Each robot has an LED to provide feedback when the robots are searching for the operator's face, and when they are selected~\cite{milligan_selecting_2011}. Body pose (for example arms out front, or above the persons head) was used to command a swarm of small mobile robots to move into a set configuration (such as making a smiling face)~\cite{alonso-mora_human_2014}. Pointing is also used to select a particular robot in a team of flying~\cite{lichtenstern_prototyping_2012}, and to command a selected group of mobile robots~\cite{milligan_selecting_2011}. Hand waving has also been used to select a specific unmanned aerial vehicle from a team~\cite{monajjemi_hri_2013}. For instance, multiple unmanned aerial vehicles could detect a persons face direction, followed by robot selection when the person waves to a robot they are looking at directly. Deselection then occurs by waving with the opposite hand. The selected robots then respond to a command (e.g both hands waving down to land) ~\cite{monajjemi_hri_2013}. Luo et al~\cite{luo_tracking_2011} demonstrate an interaction with two people and a humanoid robot head. The robot detects and tracks the face of a person in view. A pointing gesture indicates to the robot that another person is present, the robot moves its head and begins tracking the second person. Nguyen et al~\cite{nguyen_audio-visual_2014} also demonstrate a multi-person interaction. A mobile robot identifies a person by localising from an audio source, and then determines which person to track when they wave.

\subsubsection{Social Action and Interactivity:} Gestures have been performed by anthropomorphic robots to mimic human gestures for social interaction ~\cite{gao_hand_2020}. This included copying of human hand gestures on an astronaut assistant robot hand~\cite{gao_hand_2020}. Waving was also used by a humanoid robot on a mobile base to wave in response to a human wave, before moving to an object that has been pointed to~\cite{canal_gesture_2015}. Kalidolda et al~\cite{kalidolda_towards_2018} used humanoid robots (Pepper and NAO) to perform finger spelling gestures to communicate with deaf-mute individuals at a public service centre. Social interactivity also involved gesture-based games, such as paper-scissors-rock which required the robot to first perform a human-like action then classify the user pose to determine the result~\cite{katsuki_high_speed_2015, yuan_natural_2020}. 

% ==================================================
\subsection{Action Recognition}
% ==================================================

%Walking style prediction~\cite{kollmitz_deep_2019, chalvatzaki_learn_2019}, exercise robot~\cite{werner_evaluation_2013, gorer_autonomous_2017, avioz-sarig_robotic_2020}, home service robots~\cite{koppula_anticipating_2016} and robot arms~\cite{lee_learning_2017, wang_collision-free_2017}. Use cases for visual detection were often actions to directly assist a human, such as with a robotic walker~\cite{chalvatzaki_learn_2019}, playing a game~\cite{efthymiou_multi-_2018}, clearing a table~\cite{lee_learning_2017}, or opening a fridge~\cite{koppula_anticipating_2016}). Additional feedback provided by the robot, such as blinking and face tracking, provided the user with a more pleasant experience~\cite{gorer_autonomous_2017, avioz-sarig_robotic_2020}.

In the 215 papers, 15 papers (7.4\%) described the recognition of human actions for the purpose of HRI/C. Recognised actions included activities such as eating ($n=3$), exercise poses ($n=3$), and recognition of their walking (gait) motion ($n=2$). Use cases for visual detection were often actions to assist the person~\cite{chalvatzaki_learn_2019, efthymiou_multi-_2018,lee_learning_2017,koppula_anticipating_2016}. Others included deciding when to provide assistance based on states and actions, such as when a person is tired~\cite{zhang_automating_2018}, and if the person had fallen to the ground~\cite{sorostinean_activity_2018}). Other examples included the prediction of human actions, such as the usage of tools to avoid collisions~\cite{wang_collision-free_2017}. Humanoid robots were most often used in the action recognition loop: NAO~\cite{werner_evaluation_2013, gorer_autonomous_2017, efthymiou_multi-_2018, avioz-sarig_robotic_2020}, Pepper~\cite{gui_teaching_2018, sorostinean_activity_2018, lang_research_2020}, other humanoids~\cite{potdar_learning_2016, avioz-sarig_robotic_2020}, mobile robots~\cite{zhang_automating_2018, kollmitz_deep_2019, lee_real-time_2020}, Baxter~\cite{lee_learning_2017} and UR5~\cite{wang_collision-free_2017}), PR2 service robot~\cite{koppula_anticipating_2016} and a robotic walking aid~\cite{chalvatzaki_learn_2019}.

A walking mobility classification allowed a service robot to guide people of different walking capacity (i.e. wheelchairs, crutches, or walkers) to an entrance at a hospital that was either stairs or an elevator~\cite{kollmitz_deep_2019}. An autonomous furniture robot (an ottoman) was used to approached a human when it predicts the person is tired and needed a seat to rest~\cite{zhang_automating_2018}. A walking assistant robot predicted the motion of the human user to better track their position~\cite{chalvatzaki_learn_2019}. Humanoid exercise robots performed a desired exercise and provided feedback to a user about the quality of the performed pose~\cite{werner_evaluation_2013, gorer_autonomous_2017, avioz-sarig_robotic_2020}. Avioz et al~\cite{avioz-sarig_robotic_2020} recorded a count of poses being performed and changed the exercise when the person waved their hand. Gorer et al~\cite{gorer_autonomous_2017} had the robot learn exercises by imitating a human demonstrator, and then performing the exercises back to a person. A score was then provided at the end of the session to track performance. Service robots used action recognition to determine when to provide domestic chore assistance, such as filling a glass of water, opening a fridge door~\cite{koppula_anticipating_2016}, clearing a table, or pushing a trivet (object placed between a table and a hot serving dish) towards the person~\cite{lee_learning_2017}. A small humanoid robot learned to respond to a human that was handing over a bottle~\cite{potdar_learning_2016}. In a shared workspace, a UR5 robot arm avoided making contact with a person using an object (for example using a hammer or reaching for a cup)~\cite{wang_collision-free_2017}. In a turn taking game, a child and humanoid robot took turns to perform and recognise a pantomime action (such as swimming, painting a wall, or digging a hole)~\cite{efthymiou_multi-_2018}. A Pepper robot interacted with a person by predicting human motion (i.e. walking, eating, smoking, and discussion), then to demonstrate the remaining motion sequence after the camera was occluded~\cite{gui_teaching_2018}. Pepper could detect when a person is eating or drinking, going to sleep, has fallen to the ground, or has back pain with a state change based on the response paired with delivering dialogue to engage the person, such as asking if the robot should call an ambulance~\cite{sorostinean_activity_2018}. A Pepper robot also detected the actions of a person (shake head, wave hand, embrace, kick, bow, clap, high 5), and performed a corresponding behaviour~\cite{lang_research_2020}. Lastly, a mobile robot recognised several actions, such as eating, brushing teeth, and making a phone call), and performed person tracking~\cite{lee_real-time_2020}. 

% ========================


% With action recognition the robot should recognise what a human is doing to provide assistance without specific human instruction. In comparison to the other application areas, action recognition is a newer area of research (most papers are from the past few years). Accurate classification of human actions is a difficult vision problem, and requires large datasets to train time series classifiers (such as the use of Long Short Term Memory (LSTM)~\cite{HochSchm97} or Gated Rectified Unit (GRU)~\cite{cho-etal-2014-learning} blocks).

% Example methods: Papers that use LSTM:~\cite{potdar_learning_2016, wang_collision-free_2017, sorostinean_activity_2018, chalvatzaki_learn_2019, li_real-time_2019}, papers that use GRU:~\cite{gui_teaching_2018}. Other methods: Hidden Markov Models (HMMs)~\cite{zhuang_learning_2016}, change of position over time (between frames)~\cite{lang_research_2020}, stacked frames (16 images) from multiple kinects fed through a CNN~\cite{efthymiou_multi-_2018}. Not really sure, something to do with segmentation~\cite{saegusa_cognitive_2011}.


% ==================================================
\subsection{Robot Movement in Human Spaces}
% ==================================================

In the 215 papers included in this search 64 (29\%) involved a mobile robot moving in human spaces. Items in this application area were categorised based on if the paper included a mobile robot and part of the interaction did not require the human to perform a set pose (the human was detected without performing a gesture or particular action). The most common task required a robot to follow a person (61 papers or 29\%), with 61 (29\%) papers involved avoiding people, and 61 (29\%) required the robot to approach a person or a group of people. Many robots were required to be highly autonomous (the human was a user, or bystander in the exchange), or required minimal intervention through gesture commands (n=10). The most commonly used mobile platform was from the Pioneer range of mobile robots ($n=10$). Others included the SCITOS G5 ($n=2$) and the iRobot create ($n=2$). 


From the included papers, body pose was the most common method for tracking a person (n=), followed by head tracking (n=), and multimodal, where laser or ultrasonic range sensors were used to identify the person (by detecting legs ($n=4$) or shoulders ($n=1$)). Mobile robots benefit from laser range sensors for people detection, obstacle avoidance, and for navigation using SLAM. Sound can also be used to localise a person when not in view~\cite{nguyen_audio-visual_2014, bayram_audio-visual_2016}.

Laser for leg detection~\cite{alvarez-santos_feature_2012, pereira_human-robot_2013, weinrich_appearance-based_2013, hu_design_2014}, or shoulder detection~\cite{kobayashi_people_2010}.

Laser for obstacle avoidance~\cite{yun_robotic_2013, ali_improved_2015, yang_socially-aware_2019}.

Laser for navigation using SLAM~\cite{do_human-robot_2014, angonese_multiple_2017, yuan_development_2018, miller_self-driving_2019}. Miller et al~\cite{miller_self-driving_2019} follow a human by giving the robot position goals in its occupancy map that are a fixed offset from the tracked human.

A depth image can also be used for obstacle avoidance~\cite{bayram_audio-visual_2016}.

Ultra-sonic sensor for person following, where the person must hold a coloured marker~\cite{hassan_computationally_2016}. Sonar for navigation~\cite{chien_navigating_2019}.


Medical vs service

Multi-human environment~\cite{yang_socially-aware_2019}

Re-identification:
~\cite{wu_accompanist_2012, weinrich_appearance-based_2013, condes_person_2019, zhang_vision-based_2019}


% up to:
Unsure: ~\cite{zhang_optimal_2016}

% ==========================================

Pioneer:

Pioneer robot detects and follows a person~\cite{jia_autonomous_2011, alvarez-santos_feature_2012, pereira_human-robot_2013, yun_robotic_2013, hu_design_2014, munaro_fast_2014, chen_stereovision-only_2014, ali_improved_2015, gupta_robust_2015, angonese_multiple_2017, chien_navigating_2019}. 

Face tracking:
~\cite{fahn_real-time_2010, cheng_multiple-robot_2013, bayram_audio-visual_2016,  weber_follow_2018}

body tracking (using extracted skeleton:
~\cite{}

Other forms of tracking (e.g. through SURF features, or histogram analysis):
~\cite{chen_person_2012, wu_accompanist_2012,  weinrich_appearance-based_2013 }

% Following =============================
% ==========================================

Person following using face tracking~\cite{fahn_real-time_2010}

Person following for wheelchair and museum robot, laser to get shoulder contours, then head position~\cite{kobayashi_people_2010}

A mobile R2D2 robot with arms, carries a smaller mobile robot with a gripper~\cite{cheng_multiple-robot_2013}. The mother robot detects a persons face and waves. The robot can also deliver a drink based on the distance to the target persons face. Lastly, the mother robot deploys a smaller robot with a gripper to pick up small objects in its path.

Person following with mobile robot using SURF features~\cite{chen_person_2012}.

Wheelchair robot follows accompanist using laser for leg detection and obstacle avoidance, face detection, and then SURF features to match clothing and re-identify when a person returns to view~\cite{wu_accompanist_2012}.

Other mobile platforms included the SCITOS G5 mobile robot~\cite{weinrich_appearance-based_2013, weber_follow_2018}. Weinrich et al~\cite{weinrich_appearance-based_2013} use lasers to detect a persons legs, and an omni-directional camera to detect, track, and re-identify a person . Weber et al use perform person following using face tracking with a monocular camera~\cite{weber_follow_2018}. 


Bayram et al~\cite{bayram_audio-visual_2016} use audio localisation to allow a robot to find a person when they are not in view. Once the robot can see the person, it performs face tracking and is able to detect multiple people. Nguyen et al~\cite{nguyen_audio-visual_2014} also localise a person that is out of view using sound, and once in view, a target person (from multiple people) is tracked when they wave.

Person following with mobile robot~\cite{kahily_real-time_2016}.

Person following with mobile robot~\cite{talebpour_-board_2016}.

Yao et al~\cite{yao_monocular_2017} use a monocular camera attached to an autonomous blimp robot to detect and track a persons face.

Medical nurse robot follows with skeleton tracking~\cite{long_kinect-based_2018}

Turtlebot detects, tracks and follows human~\cite{condes_person_2019}. Re-identifies specific target face when.

Mecanum mobile robot follows human~\cite{luo_real-time_2019}.

Wheelchair follows person~\cite{wu_improved_2019}.

Person following with vision~\cite{zhang_vision-based_2019}.

Person following with vision~\cite{anuradha_human_2020}

Person following with vision~\cite{hwang_interactions_2020}

% Approaching =============================

Line following robot, approaches person when face detected~\cite{budiharto_indoor_2010}.

Tibi robot (mobile base with anthropomorphic body and head) approaches people searching for an interaction~\cite{ferrer_robot_2013}.


Chen et al~\cite{chen_stereovision-only_2014} use a pioneer robot to detect and approach a persons face using stereo vision. 


% Avoiding =============================

Mobile robot detects people and adjusts velocity~\cite{vasconcelos_socially_2016}

Autonomous people avoidance, people can also use gestures to signal the preferred path of the robot.~\cite{ghandour_human_2017}

Avoid people using a social force model that allows for appropriate social distance when passing~\cite{yang_socially-aware_2019}. Laser for obstacles.

% Other ===============================

A team of iRobot create platforms navigate through an urban environment detecting human faces and reporting back to a supervisory person~\cite{do_human-robot_2014}. The robots have an omni-directional camera for person detection, and a laser that is used to navigate through the environment (using SLAM).

Multiple people are detected and used as landmarks for integration into a SLAM solution for a mobile pioneer robot following a path~\cite{angonese_multiple_2017}.




% social acceptance considerations =========================

Jido robot approaches human and hands over object~\cite{sisbot_synthesizing_2010}. Trajectories are synthesised based on safe and socially acceptable distances.

A PR2 robot approaches a person using a real-time proxemic controller~\cite{mead_probabilistic_2012}.


% -- With gestures:
% Following
Robots that must move in human spaces can also be controlled with gestures, where gestures are used to change the robot between states (for example to switch to person following)~\cite{pereira_human-robot_2013, fujii_gesture_2014, prediger_robot-supported_2014, long_kinect-based_2018, yuan_development_2018, chen_human-following_2019, miller_self-driving_2019}, or where a person can guide an people avoiding robot towards a particular path~\cite{ghandour_human_2017}.

% ==================================================
\subsection{Object Handover and Collaborative Actions}
% ==================================================

ASIMO humanoid robot follows the head pose of an operator for direction while mimicking hand position with its end effector to handover an object~\cite{arumbakkam_multi-modal_2010}. 

Other examples of human demonstrations show the Tool Center Point (TCP) following human hand position from body pose recognition~\cite{christiernin_interacting_2016, arumbakkam_multi-modal_2010},

Programming an industrial robot arm with gestures~\cite{stipancic_programming_2012}

clusters From Valencio's revew paper
\begin{itemize}
    \item robot 2 human, or human to robot
    \item pre-handover - communication, grasping, motion planning perception
    \item handover location (fixed, pre-planned, or online)
    \item Physical handover: grip force, error handling
    \item post handover task
    \item metrics: task performance, user experience
    \item number of objects
\end{itemize}

% ==================================================
\subsection{Learning from Demonstration}
% ==================================================
Programming an industrial robot arm with gestures~\cite{stipancic_programming_2012}


Hold, place, take~\cite{saegusa_cognitive_2011}

% ==================================================
\subsection{Social Classification and Communication} 
% ==================================================

Gestures such as nodding or shaking of the head were used in social interactivity, including to evaluate human engagement~\cite{saleh_nonverbal_2015}. For instance, facial expression and gaze detection were used to determine children engagement in an educational program~\cite{castellano_context-sensitive_2014}. Facial expressions and body pose were used to evaluate if a mobile robot should approach a human~\cite{li_inferring_2019}.

Lastly, facial expressions were used to control an anthropomorphic social robot head by mimicking human expressions, such as the  Eddie~\cite{sosnowski_mirror_2010}, Reeti~\cite{masmoudi_expressive_2011}, and Alice~\cite{meghdari_real-time_2017} robot. 

A mobile social robot determines the appropriate time to approach a human after inferring user intent through body gestures and facial expressions~\cite{li_inferring_2019}. For these demonstrations there is often more than one human in the robot field of view, and the robot must at all times maintain appropriate social distance.

Molleret et al~\cite{mollaret_multi-modal_2016} use a PR2 robot to approach, and interact with a person when it detects intention for interaction.

Chien et al perform gender classification and person identification to follow a person~\cite{chien_navigating_2019}. The robot also has a sonar sensor for obstacle avoidance.



% ==============================================================
% --------------------------------------------------------------
\section{Research Question 2B. What are the most effective techniques of robotic vision in HRC/I?}
% --------------------------------------------------------------
% ==============================================================

The following section provides a detailed review for robotic vision techniques used in each application area. In this section, we present information on the process of robotic vision, including algorithms, data sets, cameras and methods to allow robots to provide information, support or actions via robotic vision. 

% ==============================================================
\subsection{Gesture Recognition}
% ==============================================================

Gesture recognition via robotic vision involves being able to identify hand and body gestures from video streams. Robotic vision for gesture recognition is commonly performed using a two-step process. First, there is an extraction step in which the hand, body or head is identified in an image. Second, a classification step is performed to determine what gesture is being demonstrated by the human. Here we will discuss the two steps in detail and common techniques used within each domain for gesture recognition. 

%\subsubsection{Step 1 - Extraction Step - Process, Techniques, Methods:} 
\subsubsection{Overview:} 

To start the process of gesture recognition, the first step is often to first extract the hand, body or head from the image. This includes finding the location of the human in the image, and seeking to detect a human face in the image or video stream. To detect a human face, the Viola-Jones method~\cite{viola_robust_2004} was commonly used for this step ~\cite{couture-beil_selecting_2010, martin_estimation_2010, droeschel_towards_2011, lam_real-time_2011,  van_den_bergh_real-time_2011,pereira_human-robot_2013, monajjemi_hri_2013}. The Viola-Jones method involves using Haar filters to extract features from an image, and AdaBoost to make predictions based on a weighted sum of features. Finally, a cascade of classifiers makes the algorithm more efficient by discarding false detections early. Face location can also be used as a method to help determine where in the image the algorithm should focus on to increase the chance of detecting the location of the persons hands ~\cite{droeschel_towards_2011, luo_tracking_2011, milligan_selecting_2011}. Colour segmentation was often used to find a hand in an image. This included where the human was required to wear a coloured item (for example a coloured glove~\cite{chen_integrated_2010, yoshida_evaluation_2011, faudzi_real-time_2012}, a piece of coloured clothing~\cite{hassan_computationally_2016}, or with coloured tape around specific fingers~\cite{lavanya_gesture_2018}). Colour segmentation has also been used to segmenting skin tone~\cite{chen_approaches_2010, manigandan_wireless_2010, luo_tracking_2011, park_real-time_2011}. Segmentation is typically performed in HSV~\cite{manigandan_wireless_2010, lambrecht_spatial_2012, raajan_hand_2013, sriram_mobile_2019}, or YCrCb~\cite{luo_tracking_2011, choudhary_real_2015, xu_skeleton_2020} colour spaces, using hand coded threshold values~\cite{chen_approaches_2010, luo_tracking_2011, raajan_hand_2013, choudhary_real_2015}, or by using histogram analysis from a skin sample~\cite{manigandan_wireless_2010, lambrecht_spatial_2012, xu_skeleton_2020}. Segmentation is also performed with the aid of the depth sensors~\cite{droeschel_towards_2011, mazhar_real-time_2019, xu_skeleton_2020}. Droschel et al~\cite{droeschel_towards_2011} apply 3D region growing using the centroid of the head as a seeding point. Mazhar et al~\cite{mazhar_real-time_2019} first extract hand position using skeleton information, then find the bounding region of the hand using depth. Xu et al~\cite{xu_skeleton_2020} perform colour segmentation thresholds tuned for human skin tone, and incorporate depth to remove segmentation errors. Skeleton tracking was commonly used to extract human features from an image, using RGB-D cameras (example papers include~\cite{broccia_gestural_2011, lichtenstern_prototyping_2012, cicirelli_kinect-based_2015,fang_vehicle-mounted_2019, chen_human-following_2019, kahlouche_human_2019}, and RGB cameras (for example with OpenPose~\cite{Cao_2017_CVPR})~\cite{martin_real-time_2019, mazhar_real-time_2019, miller_self-driving_2019}. 

% Skeleton information was then used to extract image regions for hand gesture detection~\cite{lima_real-time_2019}, or to classify body gestures from human joint positions. 

% or by using a secondary detector to first identify the location of the human and sampling coloured pixels from this region (for example by first detecting the head)~\cite{}

%\subsubsection{Step 2 - Classification Step - Process, Techniques, Methods:}

Once the region of interest is extracted, hand and body gestures are classified through a variety of methods depending on the dynamic or static nature of the gestures being performed. For static hand gestures, classification was often performed from segmented images by determining the contours (boundary pixels of a region) and the convex hull (smallest convex polygon to contain the region) as a method of counting the number of fingers being held up~\cite{lambrecht_spatial_2012, raajan_hand_2013, choudhary_real_2015, deepan_raj_static_2017,  sriram_mobile_2019, xu_skeleton_2020}. The distance of each contour point to the centroid of the segmented image was also used for finger counting~\cite{manigandan_wireless_2010, luo_tracking_2011}. From skeleton information, gestures were often classified from the 3D joint positions of the human skeleton using the angular configuration of the joints~\cite{cheng_design_2012, lichtenstern_prototyping_2012, nishiyama_human_2013, wang_vision-guided_2013, fujii_gesture_2014, yang_real-time_2015, fareed_gesture_2015, han_human_2017, miller_self-driving_2019, moh_gesture_2019}, or by using add-on SDK's (such as Visual Gesture Builder~\cite{long_kinect-based_2018}, OpenNI User Generator~\cite{yuan_development_2018}, and the supporting SDK for the Intel Realsense~\cite{vysocky_interaction_2019}). Ghandour et al~\cite{ghandour_human_2017} train a neural network classifier for body pose gestures, using joint positions as input that were first extracted from skeletal information. Li et al~\cite{li_real-time_2019} provide an LSTM network with joint positions to determine the users intention.

% and using classification algorithms such as support vector machines (SVM)~\cite{ehlers_human-robot_2016} and neural networks~\cite{cicirelli_kinect-based_2015, ghandour_human_2017}. 

% Histograms of Oriented Gradients (HOG)~\cite{gori_all_2012}, , SIFT

% Historgram of flow~\cite{gori_all_2012}?. 

% Parallel Tracking and Mapping (PTAM)~\cite{monajjemi_hri_2013}

Dynamic gestures that require a sequence of gestures for classification use multiple image frames. Common algorithms used to track and classify dynamic gestures include Hidden Markov Models (HMM)~\cite{park_real-time_2011, droeschel_towards_2011, burger_two-handed_2012, tao_multilayer_2013, fujii_gesture_2014, xu_online_2014}, optic flow for hand wave detection~\cite{monajjemi_hri_2013}, particle filters for tracking face and hands~\cite{park_real-time_2011, burger_two-handed_2012, prediger_robot-supported_2014, du_online_2018}, and Dynamic Time Warping (DTW)~\cite{qian_visually_2013, canal_gesture_2015, vircikova_teach_2015, pentiuc_drive_2018, chen_wristcam_2019}. Tao et al~\cite{tao_multilayer_2013} use a neural network to segment 3D acceleration and 3D angular
velocity data of hand gestures into a binary gesture indication value, and use a HMM to classify hand waves in various directions. Chen et al~\cite{chen_wristcam_2019} compare SURF features between frames to determine the background velocity from a wrist mounted camera, then perform DTW to classify dynamic hand gestures. Using the joint positions and a fast Fourier transform (FFT) to extract a sequence of feature vectors, Cicirelli et al~\cite{cicirelli_kinect-based_2015} train a separate neural network for each of the ten recognised body gestures. The input to this network is three features (elbow angle, and shoulder angles in two planes) for 60 frames. 

Other classification methods used traditional machine learning techniques, such as k-nearest neighbors (k-NN)~\cite{pereira_human-robot_2013, gao_humanoid_2015, chen_wristcam_2019}, support vector machines (SVM)~\cite{chen_integrated_2010, chen_approaches_2010, gori_all_2012, ehlers_human-robot_2016}, and multi-layered perceptrons (MLP)~\cite{martin_estimation_2010, tao_multilayer_2013}. k-NN was used to determine if a query sample was close to a known set of gestures, where sample features included joint positions,~\cite{gao_humanoid_2015}, Hu moments~\cite{pereira_human-robot_2013}, and velocity vectors found with dynamic time warping (DTW)~\cite{chen_wristcam_2019}. Features classified using an SVM include segmented images~\cite{chen_approaches_2010}, fourier descriptors of image boundaries~\cite{chen_integrated_2010}, and joint position vectors~\cite{ehlers_human-robot_2016} . Other methods use feature extractors such as Haar filters ~\cite{van_den_bergh_real-time_2011} or Gabor filters~\cite{martin_estimation_2010}. Van den bergh~\cite{van_den_bergh_real-time_2011} perform Average Neighborhood Margin Maximization (ANMM) to reduce the dimensionality of of features extracted using Haar filters, and then perform an nearest neighbours comparison to a database of hand gestures. Martin et al train a multi-layer perceptron using features from Gabor filters to recognise pointing gestures~\cite{martin_estimation_2010}. 

Deep neural networks have been adopted for gesture classification with increased popularity in recent years. Convolutional neural networks (CNN's) were used to classify gestures directly from pixels~\cite{arenas_deep_2017, mazhar_real-time_2019, xu_skeleton_2020}, depth images~\cite{kalidolda_towards_2018, lima_real-time_2019}, or from a combination of input channels~\cite{gao_hand_2020}. Neural networks have also been used to perform gesture classification from the skeletal information~\cite{cicirelli_kinect-based_2015, ghandour_human_2017, kahlouche_human_2019, li_real-time_2019}, and other sensory inputs~\cite{martin_estimation_2010, tao_multilayer_2013}. Examples of papers that investigate their own network architecture include~\cite{cicirelli_kinect-based_2015, arenas_deep_2017, ghandour_human_2017, lima_real-time_2019}, and examples where existing network architectures are used include~\cite{mazhar_real-time_2019, waskito_wheeled_2020, gao_hand_2020, xu_skeleton_2020}. Papers that used existing network architectures start with pre-trained models that are then fine tuned on task specific data~\cite{mazhar_real-time_2019}, or train from scratch with a custom dataset~\cite{gao_hand_2020, xu_skeleton_2020, waskito_wheeled_2020}. Mazhar et al~\cite{mazhar_real-time_2019} use a pre-trained Inception V3~\cite{szegedy_rethinking_2016} and fine tune with collected data of hand gestures. Waskito et al~\cite{waskito_wheeled_2020} classify hand gestures from a small amount of collected data (1000 images) by first extracting the binary image of the hand region. Gao et al~\cite{gao_hand_2020} train two Inception V4~\cite{szegedy_incept_2017} networks in parallel, where the input to one network is the full resolution input, and the input to the second network is a low resolution form of the input. This work utilises an Myo arm band, and fuses the RGB, depth, and sEMG data creating a 5 channel image as input. Xu et al~\cite{xu_skeleton_2020} train a ResNet~\cite{he_deep_2016} to learn the minimum convex hull of a hand gesture to determine the number of fingers held up to the robot. Kahlouche et al~\cite{kahlouche_human_2019} fuse multiple classification methods (SVM, decision tree, and a MLP) using a voting process from an ensemble of classifiers. Body pose classifiers with neural networks are used for static~\cite{ghandour_human_2017} and dynamic~\cite{cicirelli_kinect-based_2015, li_cnn_2019} body poses.


\subsubsection{Cameras:}
Gesture recognition can be performed using a variety of different camera systems. Depth aware cameras were used in 59 of the 88 gesture recognition applications identified. Of the 59 papers that used depth aware cameras, 53 of these were the Microsoft Kinect. Single camera set-ups were found in 26 papers. Stereo cameras were used in 5 papers, and omni-directional vision was used in one example. 


\subsubsection{Datasets/Models:} 
Gesture recognition often uses large datasets that involve people in frame performing common and unique gestures for training. Datasets involved videos with xxx and xxx. Common high-quality data sets used in the training session involved xxx, xxx and xxx, which contained xxx(s) video samples that has been pre-cleaning using xxx, and xxx. 

Datasets were most often collected by the researchers using the robotic platform being investigated (key examples include~\cite{martin_estimation_2010, droeschel_towards_2011,park_real-time_2011, pereira_human-robot_2013, tao_multilayer_2013, cicirelli_kinect-based_2015, arenas_deep_2017, ghandour_human_2017, lima_real-time_2019, kahlouche_human_2019, chen_wristcam_2019, xu_skeleton_2020}). Several papers built on previous work for gesture recognition models (the details of datasets used in training were not provided)~\cite{couture-beil_selecting_2010, quintero_visual_2015, ehlers_human-robot_2016, maurtua_natural_2017, martin_real-time_2019, li_real-time_2019}. Papers with datasets made publicly available include~\cite{mazhar_real-time_2019, lima_real-time_2019}. Mazhar et al~\cite{mazhar_real-time_2019} made their dataset of hand gestures `OpenSign' available to others (10 volunteers who recorded 10 static gestures). Lima et al~\cite{lima_real-time_2019} make available their dataset of 160,000 samples from men and women in different poses performing open and closed hand gestures. 

\subsubsection{Multiple-Sensors/Sensor Fusion:} Robotic vision for gesture recognition was also paired with other sensor modalities to improve interaction and collaborative processes. Sensors that improve the robot perception of the interaction include speech recognition~\cite{burger_two-handed_2012, fujii_gesture_2014, vysocky_interaction_2019, yuan_development_2018, du_online_2018, maurtua_natural_2017}, surface electromyographic (sEMG) sensor (the Myo gesture control arm band)~\cite{gao_hand_2020}, Leap Motion sensor~\cite{kalidolda_towards_2018}, IMU~\cite{du_online_2018}, and wearable sensors such as a camera attached to a persons wrist to detect hand gestures are used to control a dual arm robot~\cite{chen_wristcam_2019}. Other modalities provide feedback to the human, including speech response~\cite{kahlouche_human_2019,mazhar_real-time_2019, avioz-sarig_robotic_2020}, LED's that change colour in response to commands~\cite{milligan_selecting_2011, alonso-mora_human_2014}, and a display to provide decision support for operators~\cite{lambrecht_spatial_2012, sriram_mobile_2019, lalejini_evaluation_2015}, such as robot state information~\cite{chen_human-following_2019}. 

% \subsubsection{Classification results with different methods}
% Deep neural networks have shown improved classification than other methods for gesture recognition. of 
% Total average accuracy 


% Typically deep neural networks require training, and datasets that used to train neural network gesture recognition classifiers are..

% Papers that used neural networks:~\cite{martin_esitimation_2010, tao_multilayer_2013, cicirelli_kinect-based_2015, potdar_learning_2016, arenas_deep_2017, ghandour_human_2017, mazhar_real-time_2019, lima_real-time_2019, waskito_wheeled_2020, xu_skeleton_2020, gao_hand_2020}

% Collected dataset sizes 600 images single background + 1400 images different background~\cite{arenas_deep_2017}, 448 images."~\cite{ghandour_human_2017}, 6000 images (6 gestures).~\cite{waskito_wheeled_2020}. 2500 gesture images.~\cite{xu_skeleton_2020}. 160,000 samples from 20 individuals (15 men and 5 women), hand poses: open and closed hand. ~\cite{lima_real-time_2019}. 30 different people performing 33 manual gestures once. We additionally collected customized data where one person performed the same 33 gestures ten times.~\cite{kalidolda_towards_2018}

% Datasets that use video~\cite{chen_wristcam_2019}. 1350 gestures (135 videos  10 gestures/video. 15 subjects of 7 females and 8 males. Their ages ranged from 20 to 65 years~\cite{chen_wristcam_2019}. 

% Collected dataset sizes (where reported): 6,000 images~\cite{chen_integrated_2010}, 1,710 images,~\cite{martin_estimation_2010}, 1656 and 348 pointing gestures are used for training and testing.~\cite{park_real-time_2011}, 2082 gesture frames and 61 gestures, 27 of them being static gestures and the other 34 dynamic gestures."~\cite{canal_gesture_2015}, 180 images (for kNN classifier)~\cite{gao_humanoid_2015}, 600 images single background + 1400 images different background~\cite{arenas_deep_2017}, 448 images."~\cite{ghandour_human_2017}, 10,000 training images and 3,500 testing images, all the images are 96*96 gray level images. 5 gestures~\cite{song_towards_2017}. 6000 images (6 gestures).~\cite{waskito_wheeled_2020}. 2500 gesture images.~\cite{xu_skeleton_2020}. 1000 gestures images.~\cite{yang_novel_2018}. 160,000 samples from 20 individuals (15 men and 5 women), hand poses: open and closed hand. ~\cite{lima_real-time_2019}. 30 different people performing 33 manual gestures once. We additionally collected customized data where one person performed the same 33 gestures ten times.~\cite{kalidolda_towards_2018}


% ==================================================
\subsection{Action Recognition}
% ==================================================

% Similar to robotic vision usage in the gesture recognition section above, the first step in action recognition is to extract the human from the image. 

Action recognition via robotic vision involves being able to identify human motion from video streams. Similar to gesture recognition, robotic vision for action recognition is commonly performed using a two-step process. First, there is an extraction step in which the body pose is identified in an image. Second, a classification step is performed to determine what action is being demonstrated by the human. Here we will discuss the two steps in detail and common techniques used within each domain for action recognition. 

%\subsubsection{Step 1 - Extraction Step - Process, Techniques, Methods:} 

Actions are recognised by first extracting the relevant human information from the image. Like to gesture recognition, skeleton information is often extracted first, then classification is performed using the position of the extracted joints~\cite{werner_evaluation_2013, gorer_autonomous_2017, avioz-sarig_robotic_2020, lang_research_2020}, or by utilising joint position information with other classification methods~\cite{potdar_learning_2016, sorostinean_activity_2018, chalvatzaki_learn_2019, lang_research_2020}. Alternatively, regions of interest can be acquired using modern deep learning techniques such as faster R-CNN~\cite{renNIPS15fasterrcnn} (used by ~\cite{kollmitz_deep_2019}). Colour segmentation is also used in action recognition to identify where a hand is in an image~\cite{wang_collision-free_2017}. 

%\subsubsection{Step 2 - Classification Step - Process, Techniques, Methods:}

Gorer et al~\cite{gorer_autonomous_2017} compare the joint position of  motion identifier joints and those from a human demonstrator, to identify the disparity from an elderly user performing an exercise pose. The robot also performs face tracking (from on board software) and eye blinking (with changes to eye LED's).

Kollmitz et al\cite{kollmitz_deep_2019} use a Kalman filter to track regions of interest (extracted with faster R-CNN~\cite{renNIPS15fasterrcnn}), and a hidden markov model (HMM) to classify the class label, 3D position and velocity of a person using a walking aid. A ResNet-50~\cite{he_deep_2016} is pre-trained on ImageNet data, using RGB images as input.

Chalvatzaki et al~\cite{chalvatzaki_learn_2019} show that a neural network policy with LSTM cells trained using model based reinforcement learning outperforms several alternative methods (inverse reinforcement learning, model predictive control, and kinematic solutions) when predicting human motion when walking behind a robotic rollator. The policy utilises leg information from laser data, and upper body joint positions extracted from an RGB-D image using OpenPose~\cite{Cao_2017_CVPR}.

Potdar et al~\cite{potdar_learning_2016} reduce the dimensionality of the extracted skeletal information from 60 to 7 using principle component analysis (PCA), then train a LSTM neural network classifier. 

Wang et al use a pre-trained CNN (VGG-16~\cite{simonyan_very_2015}) to extract visual features from an image region and predict hand movement (change in x,y position) using an LSTM model~\cite{wang_collision-free_2017}.

Dense trajectories~\cite{wang_action_2011} are should to provide better features (resulting in more accurate action classifications), than CNN's where there is a mismatch between training and testing data (for example, when identifying the actions of children when large action recognition datasets are of adults)~\cite{efthymiou_multi-_2018}. 

Gui et al train a Generative Adverserial Network (GAN) from motion input (sequence of skeletal data)~\cite{gui_teaching_2018}. Skeleton information is first extracted using OpenPose~\cite{Cao_2017_CVPR} from RGB data. Depth information is then introduced to determine the 3D joint locations. A GAN framework with a single GRU cell for each the encoder and decoder is utilised to generate motion predictions.

Sorostinean et al~\cite{sorostinean_activity_2018} pre-train a LRCN network (Long-term Recurrent Convolutional Network) using the NTU RGB-D dataset~\cite{shahroudy_ntu_2016}, and fine tune with collected data that includes thermal information. First, joint positions were extracted using the Kinect API, then the LRCN network was used to extract features from skeleton information and the depth image. Features from face temperature were also tracked between frames. Features from the depth, skeletal, and thermal information were then used by a support vector machine (SVM) model with a radial bases function (RBF) kernel to classify the actions.

Zhang et al~\cite{zhang_automating_2018} predict if a person is sitting or standing, facing the robot, and if the person is exhibiting a welcoming action (such as waving or stretching). The Microsoft SDK is used to extract skeleton information, then sitting is determined if the skeleton height is less than the height of the standing skeleton, body orientation is determined from shoulder positions, and template matching is used to determine a welcoming pose.

From the skeleton information (extracted from Baidu AI), Lang et al~\cite{lang_research_2020} find the average of 7 key locations (top of the head, left/right elbow, wrist and ankle). These average values were collated to determine an identifier score, with each action corresponding to a unique identifier score.

In the work by Lee et al~\cite{lee_real-time_2020} joint pose information was first extracted from OpenPose~\cite{Cao_2017_CVPR} from RGB data. An image is then create mapping joint positions $x,y,z$ into RGB colour space (each pixel represents a joint location, normalised to be between 0 and 255). Each image column becomes the location of each joint position, and has a row for each camera frame. This compressed joint information image is then passed into a convolutional neural network (CNN) and trained to predict actions on the NTU RGB-D dataset.

% ==========TODO:
% More details about~\cite{efthymiou_multi-_2018}, though I think they are out of scope:
% For the first pipeline of our single- view action recognition system, the state-of-the-art Dense Trajecto- ries (DT) [21] features are combined with the Bag of Visual Words (BoVW) encoding framework. In each video frame, dense points are sampled and tracked through time based on a dense optical flow field. The features that are computed along each trajectory are: the Trajectory descriptor [21], Histograms of Oriented Gradients (HOG) [22], Histograms of Optical Flow (HOF) [22], and Motion Boundary Histograms (MBH) [21] computed on both axes (MBHx, MBHy). Encoding of the features using the BoVW and assignment to K=4000 clusters follows in order to form a representation of each video. Videos are classified based on their BoVW representation, using non-linear Support Vector Machines (SVMs)


\subsubsection{Cameras Used for Action Recognition:}

From the 15 identified papers that used action recognition, the most common camera type was the RGB-D sensor ($n=13$), followed by monocular RGB cameras ($n=2$). In the 13 papers that used an RGB-D camera, 12 used the Microsoft Kinect. In addition to RGB-D cameras, Sorostinean et al~\cite{sorostinean_activity_2018} also utilise thermal cameras in an action recognition scenario.

\subsubsection{Datasets}

Action recognition classification often required the usage of a large dataset of actions~\cite{wang_collision-free_2017, gui_teaching_2018, sorostinean_activity_2018, kollmitz_deep_2019, lee_real-time_2020}. Two papers~\cite{sorostinean_activity_2018, lee_real-time_2020} used the NTU RGB-D dataset~\cite{shahroudy_ntu_2016}, containing 60 action classes from 56,880 video samples with RGB videos, depth map sequences, 3D skeletal data, and infrared (IR) videos for each sample. The manipulation action dataset (MAD)~\cite{fermuller_prediction_2018} was used by~\cite{wang_collision-free_2017} and contains five different objects with five distinct actions. Each action is repeated five times (a total number of 625 recordings)~\cite{wang_collision-free_2017}. The H3.6M dataset~\cite{ionescu_dataset_2014} has 3.6 million 3D human poses (5 female, 6 male from 4 view points) and was used by~\cite{gui_teaching_2018} to train a motion prediction GAN. Kollmitz et al~\cite{kollmitz_deep_2019} make available their dataset of various mobility levels (wheelchair, walker, walking stick) with over 17,000 annotated RGB-D images.

% For the MBRL training we have employed 5000 frames of tracking data from four patients, while for testing the controllers performance we have kept 2000 data from one patient unseen to the training set.~\cite{chalvatzaki_learn_2019}



% Efthymiou et al~\cite{efthymiou_multi-_2018} use a pre-trained model on the Sports1M corpus, which is then fine-tune to classify the actions in our database. In addition, since we have limited data, we split each of our videos in 16-long frame clips with the 15 frames overlapping and use thse to fine-tune the network~\cite{efthymiou_multi-_2018}. Other database mentions from~\cite{efthymiou_multi-_2018}: the ActivityNet database [29] contains 15,410 videos of 200 classes for training, Sports1M [30] includes over 1 million videos of 487 classes, and UCF101 [31] contains 13,320 videos from 101 classes.

% ==================================================
\subsection{Robot Movement in Human Spaces}
% ==================================================

Mobile robots that operate in human spaces are required to track people through multiple frames, responding as the person or robot moves through the world. The robot must first detect the person/s and then track their location. In some cases the robot is able to re-acquire a target person when they are no longer in view.

Face detection is one of the more common approaches to detecting if an image is exclusively a person, and the most common method (explained in detail in Sec 2B: Gesture Recognition) is the Viola-Jones method~\cite{viola_robust_2004}, example usage includes~\cite{budiharto_indoor_2010, lam_real-time_2011, wu_accompanist_2012, ferrer_robot_2013, pereira_human-robot_2013, do_human-robot_2014, yao_monocular_2017, condes_person_2019}. Weber et al~\cite{weber_follow_2018} used a CNN for face detections. Another popular solution for person detection is to use skeleton extraction (for example from the Microsoft SDK, OpenNI, OpenPose)~\cite{ghandour_human_2017, long_kinect-based_2018, yuan_development_2018, chen_human-following_2019, miller_self-driving_2019, wu_improved_2019, yang_socially-aware_2019, anuradha_human_2020}. This method is particularly useful if gestures are used in part of the system.

Once a human feature (for example the face) is detected, it must be followed between image frames. Tracking is often performed using a particle filter~\cite{fahn_real-time_2010}, or through optic flow~\cite{wu_improved_2019}. SURF features~\cite{bay_speeded-up_2008} are another method for tracking a image features through frames~\cite{wu_accompanist_2012}, where images features can be provided a priori (for example by showing the robot a pattern a person is wearing~\cite{~\cite{chen_person_2012}}) or by first detecting the human (for example through face detection), then identifying their clothing for tracking~\cite{wu_accompanist_2012}. Kalman filters are a common method for minimising tracking error from noisy sensors and odometry~\cite{}.

Online learning (MIL), and template matching (appearance models).

Local binary patterns are improve robustness to lighting conditions.

human motion prediction~\cite{chen_human-following_2019}

% Examples ======================================================
Face detection with Viola-Jones~\cite{budiharto_indoor_2010, lam_real-time_2011}.

A particle filter is used to track a face~\cite{fahn_real-time_2010}. Face is in the center of the screen at the beginning of the interaction, the colour spectrum of the skin and hair are determined and tracked using a particle filter.

Laser information to get the contours of the shoulder, viola-jones to get face information. Tracked with particle filter.~\cite{kobayashi_people_2010}

Images are first converted to a local binary pattern~\cite{ojala_lbp_2002} to be robust to lighting~\cite{yun_robust_2010}. 

Laser for legs, weighted features for human detection including LBP~\cite{ojala_lbp_2002}, canny edge detector~\cite{canny_computational_1986}, and histogram of gradients (HOG~\cite{dalal_histograms_2005})~\cite{alvarez-santos_feature_2012}

Laser for leg detection, face detection with viola-jones, and then SURF features to match clothing~\cite{wu_accompanist_2012}.

SURF features~\cite{bay_speeded-up_2008} are used to track a person wearing a known pattern~\cite{chen_person_2012}.

Disparity image is taken from a stereo camera, a canny edge detector is used to extract a human shape, the head is segmented using geometry, and compared to a samples from a model image using Hu moments~\cite{jia_autonomous_2011}

Skeleton information is extracted from the Microsoft Kinect SDK, then Bayesian inference is used to sample over all possible robot poses best suited for an interaction~\cite{mead_probabilistic_2012}.

Unknown techniques:
Trajectories are generated based on an ideal interaction area in front of the detected person~\cite{sisbot_synthesizing_2010}.

Unknown face detection method~\cite{cheng_multiple-robot_2013}

Face regions are detected with the Viola-Jones method from OpenCV, then an online random ferns method compares pixel intensities to learn a particular persons face~\cite{ferrer_robot_2013}. This method also utilises human feedback (through a keyboard and touch screen) to improve the classifier when the predictions are uncertain.

Weinrich et al~\cite{weinrich_appearance-based_2013} use a laser for leg detect, then use HOG with a sliding window with a multi-resolution pyramid of the input image to identify, and re-identify, the upper body of a detected person compared with an appearance model (manually constructed model of an upper body).


Yun et al~\cite{yun_robotic_2013} use online multiple instance learning (MIL~\cite{babenko_visual_2009}) to track a person from the back appearance, training a colour histogram score from online images using both positive and negative examples. Chen et al~\cite{chen_stereovision-only_2014} also utilise MIL to perform segmentation with skin colour to detect a person, updating the adaptive appearance model of the person and the local image background. 

Weber et al~\cite{weber_follow_2018} used a CNN (VGG-16~\cite{simonyan_very_2015}) with single shot detection (SSD~\cite{liu_ssd_2016}) for head detection. Detections were then tracked with LSD SLAM~\cite{fleet_lsd-slam_2014} and through the fitting of faces to a template face model (3D morphable face model 3DMM).

Face detection is perform using the Viola-Jones method~\cite{do_human-robot_2014}. 

Hu et al use a model of a person walking to anticipate human movement and more accurately perform person following~\cite{hu_design_2014}.

Munaro et al~\cite{munaro_fast_2014} create a voxel grid filter from an RGB-D image to remove the ground, and cluster groups of voxels based on 3D proximity and shape to detect and track people.


Ali et al~\cite{ali_improved_2015} uses the Viola-Jones method for face detection, and a particle filter for target tracking. Laser for leg obstacle avoidance.

Gupta et al~\cite{gupta_robust_2015} first subtract the background, then perform template matching on the segmented image to detect a person. A motion model and Kalman filter result in smooth tracking.

Bayram et al~\cite{bayram_audio-visual_2016} also use the Viola-Jones for face detection, then perform color-based tracking in YCbCr colour space using Camshift in OpenCV.

Hassan et al~\cite{hassan_computationally_2016} requires a person hold a coloured marker. Colour segmentation is performed and following occurs with the aid of a ultra-sonic range sensor.

Voxelisation, segmentation and 3D clustering is used to detect a person when the sensor is moving~\cite{kahily_real-time_2016}.

HOG features into an SVM to determine if there is a leg. Kalman filter for tracking~\cite{talebpour_-board_2016}

People are detected using HOG features into an SVM, and confirmed using height and distance~\cite{vasconcelos_socially_2016}.

Angonese et al~\cite{angonese_multiple_2017} train a GoogLeNet~\cite{szegedy_going_2015} CNN to detect specific instances of people, such that they can ben integrated into a SLAM algorithm for robot navigation. Features from the CNN were used with an SVM. This required a custom dataset of people to be collected. The CNN was pre-trained on ImageNet

Viola-jones for face detection and tracking for blimp~\cite{yao_monocular_2017}.

Laser for slam, skeleton for person tracking (OpenNI)~\cite{yuan_development_2018}

Condes et al~\cite{condes_person_2019} first use a single shot detector (SSD) network~\cite{liu_ssd_2016} to detect a person, then use the Viola-Jones method to determine the region of the persons face. Tracking is performed by determining if a detection in a new frame is near the pixel location in a previous frame. Finally, FaceNet~\cite{schroff_facenet_2015}, a CNN that outputs a vector embedding of a face, is used to determine if the face detected is of a known face, and is used for re-identification when a target person is lost.

Luo et al~\cite{luo_real-time_2019} use a kernalised correlation filter (KCF) to track a target accross image frames. An extended kalman filter is used to account for the noise of the predictions.

Skeleton information obtained with Kinect SDK, tracking is performed with optic flow~\cite{wu_improved_2019}

OpenPose for skeleton~\cite{yang_socially-aware_2019}, social force model for navigation adhering to social conventions. Laser for obstacles.

A person is detected and tracked using the contours of the target in the image (target contour band)~\cite{zhang_vision-based_2019}. The robot follows the target using image based visual servo control using a kinematic model of the robot and the bounding corners of the target in the image.

SSD for person, FaceNet to detect specific person, KCF for tracking~\cite{hwang_interactions_2020}.

With gestures:
Laser for leg detection, Viola-Jones for face detection and tracking~\cite{pereira_human-robot_2013}.

Body tracking using skeleton extracted from the Microsoft Kinect SDK~\cite{fujii_gesture_2014}

Prediger et al~\cite{prediger_robot-supported_2014} extract a skeleton tracking with particle filter.

Skeleton extraction, Gesture Builder, and Kalman filter~\cite{long_kinect-based_2018}

OpeNI for joint angles, and motion tracking with human motion prediction~\cite{chen_human-following_2019}.

OpenPose for gestures and skeleton~\cite{miller_self-driving_2019}, goals are given in the robot occupancy map that are a fixed offset from the tracked human.

Kinect SDK for skeleton and following~\cite{anuradha_human_2020}.


\subsubsection{Cameras Used for Robot Movement in Human Spaces:}

From the 64 identified papers where a robot was required to move in human spaces, the most common camera type was the RGB-D sensor ($n=35$), followed by monocular RGB cameras ($n=20$). In the 35 papers that used an RGB-D camera, 25 used the Microsoft Kinect. Other camera types used were stereo cameras ($n=6$) and omni-directional cameras ($n=3$).



\subsubsection{Datasets/Models:} 

Munaro et al~\cite{munaro_fast_2014} release the Kinect Tracking Precision (KTP) dataset, collected from the Microsoft Kinect and containing 8,475 frames with a total of 14,766 instances of people.

Weber et al~\cite{weber_follow_2018} train a face detection on the HollywoodHeads Dataset~\cite{vu15heads}. The dataset contains annotated head bounding box regions for 224,740 video frames from Hollywood movies.


% ==================================================
\subsection{Object Handover and Collaborative Actions}
% ==================================================


Later works used neural networks to train per-pixel hand segmentation (each pixel is classified as a hand or not a hand)~\cite{rosenberger_object-independent_2020} 
% : per-pixel classification of hand/not hand. 50 layers and dilated network strategy is utilized along with a pyramid pooling module using ResNet 50, that is also used to segment human skin


\subsubsection{Cameras Used for Object Handover and Collaborative Actions:}

From the 63 identified papers where a robot collaborated with a human in an an object handover or action, the most common camera type was the RGB-D sensor ($n=42$), followed by monocular RGB cameras ($n=17$). In the 42 papers that used an RGB-D camera, 32 used the Microsoft Kinect. Four papers ($n=4$) used stereo cameras.


\subsubsection{Datasets/Models:} 


Pre-trained classifiers such as YOLO~\cite{rosenberger_object-independent_2020}

Ferrer et al~\cite{ferrer_robot_2013} collect a dataset to verify their face detection method (12 sequences of 6 different persons).

% ==================================================
\subsection{Learning from Demonstration}
% ==================================================

\subsubsection{Cameras Used for Learning from Demonstration:}

From the 7 papers identified that required the robot to learn from demonstrations, the most common camera type was the RGB-D sensor ($n=6$), of which 5 where the Microsoft Kinect. One paper ($n=1$) used a monocular RGB camera.

\subsubsection{Datasets/Models:} 


% ==================================================
\subsection{Social Classification and Communication} 
% ==================================================

From extracted features, classification of gestures was performed with traditional machine learning methods such support vector machines (SVM)~\cite{saleh_nonverbal_2015, ke_vision_2016}. SVM's facial expressions~\cite{sosnowski_mirror_2010, castellano_context-sensitive_2014, ke_vision_2016}. Head gestures from direction and magnitude pattern of depth pixels were classified with a SVM~\cite{saleh_nonverbal_2015}. Other methods use feature extractors such as Gabor filters~\cite{ke_vision_2016}. 
Neural network layers that utilise time-series (such as LSTM) are also used to classify dynamic gestures~\cite{li_cnn_2019}.

Head gestures from direction and magnitude pattern of depth pixels were classified with a SVM~\cite{saleh_nonverbal_2015}.

Recent works use convolutional neural networks to detect the face region ~\cite{li_inferring_2019, li_cnn_2019}. The face location can be used to then classify facial expressions~\cite{li_cnn_2019},


Deep neural networks have been adopted for gesture classification in recent years. The most common usage was convolutional neural networks (CNN's), classifying gestures directly from pixels. Alternatives perform classification from the extracted skeletal information~\cite{}. architectures and use pre-trained models that are then fine tuned on task specific data~\cite{mazhar_real-time_2019, waskito_wheeled_2020}, or trained with a custom dataset~\cite{gao_hand_2020}. Examples of papers that investigate their own network architecture include~\cite{li_cnn_2019}. Li et al~\cite{li_inferring_2019} TODO: SSD for face detection + other things. Li et al~\cite{li_cnn_2019} use a CNN with LSTM to detect facial expression changes.

\subsubsection{Cameras Used for Social Classification and Communication:}

From the 41 papers identified that required the robot to communicate with a human in an social interaction, the most common camera type was the RGB-D sensor ($n=24$), followed by monocular RGB cameras ($n=15$). In the 24 papers that used an RGB-D camera, 23 used the Microsoft Kinect. Two papers ($n=2$) used a stereo camera.

\subsubsection{Datasets/Models:} 

Papers that used datasets from external sources were most commonly used for facial expression recognition~\cite{sosnowski_mirror_2010, castellano_context-sensitive_2014, ke_vision_2016, li_inferring_2019, li_cnn_2019}. The Cohn-Kanade dataset~\cite{kanade_comprehensive_2000} and its extended form (CK+)~\cite{lucey_extended_2010} was used to train facial expression models~\cite{sosnowski_mirror_2010, ke_vision_2016, li_cnn_2019}. Other datasets include FERET, AffectNet, MMI Face Database, and the Inter-ACT corpus. Several papers built on previous work for gesture recognition models (the details of datasets used in training were not provided)~\cite{saleh_nonverbal_2015}. 

Datasets that use video~\cite{meghdari_real-time_2017}
"10 persons, 6 males and 4 females Kinect data output was captured at least for 3 seconds
with 30 frame per second rate for each facial expression with
different head orientation. Though, each person has a dataset
that each frame of data is an array with 17 members."~\cite{meghdari_real-time_2017}

% =============================================
% ---------------------------------------------
\section{Research Question 3. What is the current state-of-the-art of robotic vision for HRC/I?}
% ---------------------------------------------
% =============================================
A detailed evaluation of the current state-of-the-art systems for robotic vision in HRC/I is discussed to demonstrate how computer vision techniques have performed in action. This includes demonstrating the impact of speed, accuracy and performance from the last year alone to showcase the capacity for robots to interact with humans based on visual information. 

%\textbf{From PRISMA criteria - last year or so}. Benchmarks, potentially our own benchmark.

\subsection{Gesture Recognition}

\subsubsection{Hand gestures} 
The dataset OpenSign (custom dataset)~\cite{mazhar_real-time_2019}, control kuka arm with hand gestures, detect 5 gestures in a row at 40Hz == 250ms per detection. ``Inception V3 convolutional neural network is adapted and trained to detect the hand gestures. To augment the data for training the hand gesture detector, we use OpenPose to localize the hands in the dataset images and segment the backgrounds of hand images, by exploiting the Kinect V2 depth map. Then, the backgrounds are substituted with random patterns and indoor architecture templates. Fine-tuning of Incep- tion V3 is performed in three phases, to achieve validation accuracy of 99.1\% and test accuracy of 98.9\%.," 10 gestures performed with labels from American Sign Language : fist, peace sign, single index finger up, index finger up and thumb out,  f, and several gestures representing `None'. Dataset required: RGB-D, 10 gestures performed by 10 people. These include 8646 original images, and 12304 synthetic images obtained by substituting the background.

For gesture recognition, classification accuracy is paramount to the functional benefit of the system. For hand gesture classification, Gao et al~\cite{gao_hand_2020} show the accuracy of their parallel convolutional neural network architecture is better than using a single network (92.45\% compared with 88.89\%). They also show that multimodal input (from RGB, depth, and sEMG) also produces the more accurate classification than using a single channel input. For real-time systems, the timing of classification algorithms can be a bottleneck for the user experience. Gao et al~\cite{gao_hand_2020} also show there parallel CNN network runs at 32ms on a GTX1060 GPU.

Waskito et al~\cite{waskito_wheeled_2020} test the robustness of their hand gesture classifier by providing the accuracy as the hand is rotated, or when lighting conditions are varied. They provide an average total accuracy for their system at 96.67\%, and test the timing for each gesture with an average response time of 0.141s.

Dynamic hand gestures:
Camera mounted to wrist, and hand only does fist (gesture is from background movement) unsure if its a fair SOTA~\cite{chen_wristcam_2019}, 1350, dynamic hand gestures using SURF features to determine background velocity, 98\% accuracy.


Pointing gestures from hand:


\subsubsection{Body gestures} 
Static body pose:
100 percent classification accuracy on training set, out of 90 test experiments it mis-classifed two gestures. ~\cite{ghandour_human_2017}. 5 people performing 18 gestures, 7 possible gestures: arms down, left/right arm raised 90 degrees, both arms 90 degrees, both arms up, left/right arm up. Alternative:~\cite{chen_human-following_2019}, only 4 gestures, use joint positions to classify from Kinect + OpenNI, 10 of each gestures, confusing task (insert light into UI and change its colour).

Dynamic body pose:
Seem to have good results (>96\% classification accuracy on 300 tests), but very sparse on the details, seem to get the skeleton from ms sdk, but unknown what they do after that~\cite{fang_vehicle-mounted_2019}. 6 gestures from moving an arm left/right/up/down, making circular motions. Alternative: ~\cite{pentiuc_drive_2018}, DTW performed on skeleton data from Kinect, 5 gestures left/right arm up or to the side, 500 data points, accuracy given (> 86\%).

Pointing gestures from body pose:

\subsubsection{Final Recommendation:} 
To implement robotic vision for gesture recognition, common steps include xxx. Relevant packages include xxx. High performing data sets include xxx. What are the steps to get this to work, datasets, training, off the shelf usability.

% Where possible use off-the-shelf algorithms to locate head position, perform segmentation, or extract skeleton information. Collect own dataset from the robot's sensors. Train a classifier by fine-tuning a pre-trained network.

\subsection{Action Recognition}
Gui et al~\cite{gui_teaching_2018} achieve a lower prediction error (euclidean distance between the prediction and the ground truth in the angle space) than several other methods when evaluating sequences between 80 and 1000 milliseconds from the H3.6M dataset~\cite{ionescu_dataset_2014}.

Kollmitz et al\cite{kollmitz_deep_2019} found that RGB data performed better than DepthJet (colour-encoded depth image), and found that a VGG-CNN-M~\cite{chatfield_return_2014} provided a trade off between classification accuracy and processing speed (compared to ResNet-50~\cite{he_deep_2016} and GoogLeNet~\cite{szegedy_going_2015}) when classifying people with walking aids.

Chalvatzaki et al~\cite{chalvatzaki_learn_2019} show their model based reinforcement learning method obtains a smaller tracking error than several other control methods for a robotic rollator~\cite{chalvatzaki_learn_2019}. 

For recognising actions from a child~\cite{efthymiou_multi-_2018}, using dense trajectories from multi-view fusion. Speed accuracy performance.

Lee et al~\cite{lee_real-time_2020} achieve an accuracy of 71\% from an RGB camera on the NTU RGB-D dataset (75\% from Kinect) at 15fps. 


\subsubsection{Final Recommendation:} 
To implement robotic vision for gesture recognition, common steps include xxx. Relevant packages include xxx. High performing data sets include xxx. What are the steps to get this to work, datasets, training, off the shelf usability.

\subsection{Robot Movement in Human Spaces}

Webeer et al~\cite{weber_follow_2018} achieve 59.7\% Mean Average Precision (mPA) on the HollywoodHeads dataset~\cite{vu15heads}.

Zhang et al~\cite{zhang_vision-based_2019} compare their detection and tracking method using target contour bands to several others using videos from the object tracking benchmark (OTB) dataset. Their method was the most accurate (94\%) overall, with the fastest processing time (34fps).



\subsubsection{Final Recommendation:} 
To implement robotic vision for robot movement in human spaces, common steps include xxx. Relevant packages include xxx. High performing data sets include xxx. What are the steps to get this to work, datasets, training, off the shelf usability.

\subsection{Object Handover and Collaborative Actions}

\subsubsection{Final Recommendation:} 
To implement robotic vision for object handover and collaborative actions, common steps include xxx. Relevant packages include xxx. High performing data sets include xxx. What are the steps to get this to work, datasets, training, off the shelf usability.


\subsection{Learning from Demonstration}

\subsubsection{Final Recommendation:} 
To implement robotic vision for learning from demonstration, common steps include xxx. Relevant packages include xxx. High performing data sets include xxx. What are the steps to get this to work, datasets, training, off the shelf usability.


\subsection{Social Classification and Communication} 



\textbf{Facial expression recognition:} 
~\cite{li_cnn_2019} - HAAR features for detecting face region, then CNN and LSTM. Use fine-tuning from pre-trained network.

\subsubsection{Final Recommendation:} 
To implement robotic vision for social clarification and communication, common steps include xxx. Relevant packages include xxx. High performing data sets include xxx. What are the steps to get this to work, datasets, training, off the shelf usability.

%\vspace{5mm} 
%Insert Table 2 Here - Author Reference, Year, Application Area, Domain Area, Type of Robot, Speed, Accuracy. Performance. 
%\vspace{5mm} 


% =============================================
% ---------------------------------------------
\section{Research Question 4. What is the impact of robots with robotic vision in HRC/I in human-robot teams?}
% ---------------------------------------------
% =============================================

In this section, we discuss the impact of robots with robotic vision for human participants. This involves a detailed description and presentation of characteristics of people who have participated in user studies around robotic vision. This is followed by reviewing user study information to determine how robots with vision capabilities had been applied to tasks or roles, and how did they perform in relation to human-robot team performance. A standardization for metrics to benchmark human-machine teams has been generated to help evaluate the impact of human-robot interaction in core areas \cite{damacharla2018common}. These benchmark metrics will be used to evaluate the impact of robotic vision on HRC/I in the areas of mission, efficiency, safety and timing metrics. 

\subsection{Participant Overview:}

In the 215 papers, only 46 (20\%) reported any form of a user study with human participants. There was a total of 635 participants from the 44 studies. User studies had an average of 14 people per study (Range = 2 - 50, $SD = 13$), except one paper that reported a user study did not provide details of participant numbers~\cite{christiernin_interacting_2016}. In studies that reported participant age (xxx\%), they were on average (xxx) years old (Range =  12 - 60, $SD = 34$). In studies that reported country of origin (xxx), participants were most often from (xxx) countries. 
 
In the 88 papers, 26 (xx\%) papers reported a user study with gesture recognition with a total of 350 participants. There was a mean average of 13 participants per study (range = 2 - 32, SD = xxx) testing out gestures for human-robot interaction and collaboration. In the studies that reported age (xxx\%), participants were on average 45 (xxx) years of age (range =  12 - 60, SD = 34). In studies that reported country of origin, (xxx), they were most often from European ($n=13$) and Asian ($n=10$) countries. 



%Users validated accuracy of perception system:
%Gather a baseline of pointing accuracy from human users i.e. a human was asked to determine the pointing location where pointing was performed by another human~\cite{martin_estimation_2010}. 



%ChaLearn Gesture Recognition Challenge evaluation protocol was applied in a gesture recognition game with iCub humanoid, from 30 users without prior knowledge of the gestures~\cite{gori_all_2012}. The robot won 75\%. 15\% were lost due to out of distribution gestures performed by the participants, 10\% due to classification errors.

%The accuracy of a gesture recognition system is reported from three users interacting with a NAO robot performing gesture recognition in a social scenario~\cite{gao_humanoid_2015}.


%Questionaires: (Godspeed [11],PANAS [12]) and specifically introduced ad-hoc questionnaires [13] based on Heerink et al. [14]) ~\cite{werner_evaluation_2013}.

%questionnaire using Likert-scales from 1 to
%9.~\cite{prediger_robot-supported_2014}

% PARADISE framework, and GODSPEED questions~\cite{foster_two_2012}.

%bipolar Likert-
%type seven-point scales.~\cite{scheggi_human-robot_2014}

%Raw NASA-TLX Questionnaire~\cite{jevtic_comparison_2015}
% godspeed~\cite{zhang_automating_2018}

\subsection{Mission Metrics}
Robotic vision had important impact on improving the capacity to collaborate during missions for human-robot interaction. Example mission metrics include plan execution, situation coverage, task difficulty and success ~\cite{damacharla2018common}.


Kalidolda et al~\cite{kalidolda_towards_2018} use humanoid robots Pepper and NAO to perform fingerspelling gestures to communicate with deaf-mute individuals at a public service centre, with nine out of ten people preferring the embodied robot over a display.

From a study with 30 elderly residents, 77\% preferred an exercise robot (NAO) compared to a training video~\cite{werner_evaluation_2013}. From a further 32 elderly persons, 92\% indicated they would like to continue an exercise program with a robot~\cite{avioz-sarig_robotic_2020}. Over a 3 week period, Gorer et al~\cite{gorer_autonomous_2017} showed an increase exercise success rates each week with 12 elderly users.

From evaluation of 11 young adults, pointing provided more amusement and was the favourite method for controlling a mobile robot compared to a joystick or gamepad~\cite{yoshida_evaluation_2011}.

Participants in an interaction with a NAO robot performing natural human motions (face tracking, body and hand gestures) during a conversation found the experience engaging and generally liked the interaction~\cite{csapo_multimodal_2012}

An anthropomorphic bar tender robot was perceived as likable, intelligent, and safe from a study with 31 participants. The robot served drinks successfully with high accuracy (100\% for single person scenario), with reasonable response time (658ms) and an expected interaction duration (49.4s).

In an interaction that compared pointing styles to steer a PR-2 robot (the pointing vector is made following: the elbow to the finger, or the eye to the finger). From 8 users, 62\% of participants preferred the elbow to finger pointing method, 38\% preferred the eye to finger method~\cite{abidi_human_2013}. Users were not aware of how the robot would respond to their pointing methods, and the response of the robot was randomised between the two methods.

In a human following task, from an evaluation with 13 users generally found the robot's behaviour appropriate, though 11 out of 13 users were a little disturbed by the robot~\cite{prediger_robot-supported_2014}. Six people were willing to adapt their living environment to accommodate the robot, ten did not want to adjust their walking speed.

Users wore a haptic bracelet while leading multiple robots, where the robots would detect user deviance from a set trajectory and activate the bracelet~\cite{scheggi_human-robot_2014}. From a 7 point scale, 14 users rated an average of 5.6 for comfort of the device, and 6.3 when asked about the informativeness of the bracelet. 

Four autistic children interacted with a NAO robot, each child was eager to participate and engage with a robot (no statistics are supplied)~\cite{taheri_social_2014}.

From a study with 24 people performing waving and pointing gestures to a NAO robot on a mobile platform, 79\% thought the waving and pointing gestures were natural and easy to perform~\cite{canal_gesture_2015}.

For a guiding a mobile robot to specific rooms and waypoints in an domestic setting, from 24 participants, direct physical interaction was shown to be the least mentally demanding, and was the most accurately performed~\cite{jevtic_comparison_2015}. Of two contactless modalities, following and pointing, following required less mental effort and resulted in more accurate guidance of the robot.

In a maze navigation task for an autonomous robot, 23 participants were tasked with providing high level directional support to take a left or right turn at key locations~\cite{lalejini_evaluation_2015}. When assessed on the interface used convey the signal (gesture, handheld device, voice), 19 out of 23 participants rated a handheld device the easiest. Participants reported interactions with all methods of control as fun, enjoyable, and exciting.

Twenty students interacted with a robotic ottoman, and compared autonomous operation to teleoperation by an unseen human~\cite{zhang_automating_2018}. The majority of the participants (13 out of 20) were unable to tell if the robot was autonomously operated. While the autonomous robot was perceived as more intelligent, the teleoperated robot was overall more likeable.

Sorostinean et al~\cite{sorostinean_activity_2018} assess the trust placed in a home service robot (Pepper) when detecting the actions of a person (including back pain and falling to the ground). 75\% of people considered action recognition important for a home service robot, and people had more trust in the system the more accurate the classifications were.

From a study of 50 volunteers interacting with a Pepper robot, more than 40 users reported a satisfaction level of at least 4 (out of 5)~\cite{lang_research_2020}.


\subsection{These papers had users studies, but they only assessed the accuracy of the system:}

Twelve users perform pointing gestures for testing of pointing recognition accuracy (over 89\% accuracy of pointing, and 67.9\% accuracy of moving to the location)~\cite{park_real-time_2011}.

The accuracy of an action recognition system was evaluated with 25 children performing 12 actions, and compared to the accuracy of 14 adult participants~\cite{efthymiou_multi-_2018}. 

Experiments with 16 people show that pointing gestures made to a mobile robot from the eye to the hand were more accurate than from the elbow to the hand~\cite{droeschel_towards_2011}. Errors increase the further the person is from the robot.

Six people with different heights perform 5 gestures 50 times each~\cite{han_human_2017}. From the total 1500 gestures, there were 11 errors, with no significance in accuracy between short, medium and tall people.

Five people with different heights perform gestures to control a mobile robot~\cite{ghandour_human_2017}. Of the total 90 gestures issued, two were misclassified.

From two users with no HRI experience, a mobile robot was successfully controlled in 77.4\% of interactions~\cite{ehlers_human-robot_2016}. From the failures 78.9\% were related to distance and rotational speed of the robot.

% ===================================
Up to: ~\cite{li_visual_2015}, done gesture and action.
% ===================================

\subsection{Efficiency Metrics}
User studies that focused on robotic vision for improving team efficiency found that vision did play a role to help the human-robot team reach an end goal. Examples of efficiency metrics include recognition or decision accuracy, plan execution and team productivity ~\cite{damacharla2018common}.

\subsection{Safety Metrics}
User studies that focused on robotic vision for safety demonstrated how vision could help improve the safety of humans, including humans in robot spaces. Examples of safety metrics included risk to the human, hazards, fatigue, stress, or situational awareness~\cite{damacharla2018common}.

\subsection{Timing Metrics}
User studies that focused on robotic vision had notable impact in relation to time allocation of human-robot interaction, including for the operator, human, and the shared mission task. Time metric examples include autonomous operation time, productive time, mean time between failures and time to complete team task~\cite{damacharla2018common}.

% =============================================
% ---------------------------------------------
\section{Research Question 5. What are the upcoming challenges for robotic vision in HRC/I?}
% ---------------------------------------------
% =============================================

Here we present a general review of limitations as written in the conclusions and limitations section of the reviewed papers, as well as challenge papers for future works. Limitations are addressed for the computing and robotics field as it relates to robotic vision, key application areas and integration of robotic vision into improving the functionality and utility of human-robot interaction. 

\subsection{Software Challenges}
Robotic vision is heavily dependant on computer vision progress and advancements, given that algorithms run through robots are only as effective as they are in computer systems. Robotic Vision is limited by the current SOTA in computer vision, a rapidly expanding field, and the adaption of processed output to robotic tasks. Research has shown that deep neural networks can fail to generalise with a reduction in accuracy when tested outside of benchmark data sets \cite{recht2019imagenet}. 

\subsection{Hardware Challenges}
Robotic vision has its own unique challenges above and beyond computer vision, which includes the need to integrate computer vision into robotic hardware. Therefore, not only is the adoption of robotic vision tied with the progress of computer vision, but also limitations of current robotic platforms. This includes the challenges of on-board processing compared to cloud processing. Other challenges include translating signals from computer vision into actionable robot functions. 

\subsection{Sim-to-Real Challenges}
Robots are also often deployed in more unpredictable settings, and required to respond to more real-world scenarios as they arise which may have not arisen in the training set. 

\subsection{Human-Related Challenges}
Human actions are not always intentional or with meaning, whereas the successful use of robotic vision requires meaning to be placed on each action for the robot to appropriately respond to the user. In addition, user studies are not often representative of the general population, so benchmark metrics are being reported on smaller sample diversity. 


% =============================================
% ---------------------------------------------
\section{Future Work and Next Steps}
% ---------------------------------------------
% =============================================
Here we will do a review of computer vision work only that has not yet been applied to HRC/I and discuss key domains of computer vision work that are emergent, but not yet applied or explored in detail in HRC/I.

% =============================================
% ---------------------------------------------
\section{Discussion}
% ---------------------------------------------
% =============================================


% =============================================
\subsection{Implications for Robotic Vision in Human-Robot Collaboration}
% =============================================
Robot working with and around humans occurs most predominantly in the domestic and urban environments, corresponding to where robots are likely to bring utility to the lives of humans. There also is a correlation between task difficulty and domain, where less difficult tasks are more commonly investigated (people following), where harder tasks are less well represented. Mobile robots make up a significant portion of the papers we encountered, where these robots are suited generally in urban and domestic environments, largely because they are inexpensive. 

With the introduction of the Microsoft Kinect sensor in 2010, we have seen an increased percentage of papers using RGB-D cameras. Fig~\ref{fig:camera_type} shows the breakdown of camera usage. Sensors like the Kinect are an inexpensive source of depth information (compared with expensive alternatives like laser range sensors). Furthermore, the release of software development kits (SDK) and open source libraries based around this technology provide developers with human skeleton tracking information, several years before deep learning allowed this technology on monocular cameras~\cite{Cao_2017_CVPR}. The increasing trends in HRC/I could in part be attributed to the accessibility of humans to robots through inexpensive sensors and open source libraries.

% =============================================
\subsection{Use of Robotic Vision to Improve Future Human-Robot Collaboration}
% =============================================


% =============================================
% ---------------------------------------------
\section{Conclusion}
% ---------------------------------------------
% =============================================
This comprehensive review provided an extensive analysis on the use of robotic vision in human-robot collaboration and interaction. This survey investigated the most common domains, areas and performance metrics for robotic vision in human-robot tasks. This survey also provided a detailed overview of the current challenges and future impact that robotic vision could have on future human-robot interactions. 

\section{Acknowledgments}
This work was supported by a Research Grant from the State of Queensland acting through the Department of Science, Information Technology and Innovation.

% Not sure how to add a reference with the correct style?
\bibliographystyle{acm}
% \bibliography{references, includes, included_references}
\bibliography{references, includes}
\end{document}
\endinput
%%
%% End of file `sample-acmsmall.tex'.
