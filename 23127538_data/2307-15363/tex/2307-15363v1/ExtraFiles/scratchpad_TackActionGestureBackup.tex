\subsubsection{Task/Action of the Robot:}

% ARMS ====================================
A small robot arm with a gripper identifies when a persons hand is open, and moves to drop a small object into the users palm~\cite{arenas_deep_2017}. When the persons hand is closed the robot returns to its initial position.

A WAM robot arm (7DOF) makes a pizza following human direction to by moving ingredients that are pointed at by the human~\cite{quintero_visual_2015}. The human point to an object, the robot confirms the location the object by also pointing (robot to human feedback). The human performs a confirmation gesture and instructs the robot to drop the ingredients over the pizza base. 

An industrial robot arm tracks hand position with a Kinect sensor and an inertial measurement unit (IMU) worn by the user, while learning from online demonstrations~\cite{du_online_2018}. When the robot end effector gets close to the desired position, fine tuning is performed by speech and static hand gesture pointing.

The tool centre point of an industrial arm moves in front of a work piece to be machined, as pointed to by a human operator~\cite{maurtua_natural_2017}. 

A robot equipped with two arms, stereo vision, and tactile sensors picks up an object (sponge cube, wooden cube, ping-pong ball) determined by a hand pointing gesture from a human operator~\cite{ikai_robot_2016}. The robot releases the object on the palm of the operator.

% Mobile ====================================
A spherical vision system is used with a mobile robot with three omni-directional wheels to detect pointing gestures from a person wearing a red coat with a blue glove ~\cite{yoshida_evaluation_2011}. The robot moves on a soccer field in the direction of the pointing gesture.

The velocity of a wheelchair robot was commanded by human shoulder position, and its configuration was controlled with arm gestures (transitioning between bed mode and sitting mode)~\cite{mao_medical_2018}. 

A autonomous mobile navigation robot explores a laboratory, asking humans for directions (when a human is detected) and translating hand gesture pointing to goals on the robot's map~\cite{van_den_bergh_real-time_2011}.

A mobile robot with a manipulator named Jido responds to gestures that involve both hands, and user speech, to identify, fetch, and handover objects (such as a water bottle)~\cite{burger_two-handed_2012}. 

A team of four mobile robots respond to gestures from a human operator~\cite{milligan_selecting_2011}. The operator selects a group of robots at a time by drawing a circle around the desired group. A robot is selected if the operators face is detected within the cone created by the circle gesture. The selected robots then go to a location pointed to by the operator. Each robot has an LED to provide feedback when the robots are searching for the operator's face, and when they are selected.

% Social ====================================
A humanoid exercise robot (NAO and Poppy) demonstrated a pose for a human to perform and provide feedback on the quality of the exercise~\cite{avioz-sarig_robotic_2020}. The robot keeps track of the number of exercises correctly performed, changing to a new exercises when the user raises their hand. Elderly users were motivated to exercise by the interaction.

Body pose (for example arms out front, or above the persons head) was used to command a swarm of small mobile robots to move into a set configuration (such as making a smiling face)~\cite{alonso-mora_human_2014}. 

A mobile social robot determines the appropriate time to approach a human after inferring user intent through body gestures and facial expressions~\cite{li_inferring_2019}. For these demonstrations there is often more than one human in the robot field of view, and the robot must at all times maintain appropriate social distance.

Luo et al~\cite{luo_tracking_2011} demonstrate an interaction with two people and a humanoid robot head. The robot detects and tracks the face of a person in view. A pointing gesture indicates to the robot that another person is present, the robot moves its head and begins tracking the second person. 

% Additional (could be in the section above?) =================================
Lastly, robotic vision for gesture recognition was also paired with other sensor modalities to improve interaction and collaborative processes. Sensors that improve the robot perception of the interaction include speech recognition~\cite{burger_two-handed_2012, fujii_gesture_2014, vysocky_interaction_2019, yuan_development_2018, du_online_2018, maurtua_natural_2017}, surface electromyographic (sEMG) sensor (the Myo gesture control arm band)~\cite{gao_hand_2020}, Leap Motion sensor~\cite{kalidolda_towards_2018}, IMU~\cite{du_online_2018}, and wearable sensors such as a camera attached to a persons wrist to detect hand gestures are used to control a wheelchair robot~\cite{yang_novel_2018} and a dual arm robot~\cite{chen_wristcam_2019}. Other modalities provide feedback to the human, including speech response~\cite{kahlouche_human_2019,mazhar_real-time_2019, avioz-sarig_robotic_2020}, LED's that change colour in response to commands~\cite{milligan_selecting_2011, alonso-mora_human_2014}, and a display to provide decision support for operators~\cite{lambrecht_spatial_2012, sriram_mobile_2019, lalejini_evaluation_2015}, such as robot state information~\cite{chen_human-following_2019}, 

% Other sensors improved the robots actions e.g. laser~\cite{yuan_development_2017, miller_self-driving_2019}, tactile~\cite{ikai_robot_2016}.